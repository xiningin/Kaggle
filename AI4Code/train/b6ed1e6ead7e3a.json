{"cell_type":{"c75063c1":"code","d5c82f84":"code","4f384ed9":"code","7b9da88f":"code","5894405a":"code","f05a691b":"code","4d8bd7ca":"code","0c84bc8d":"code","a8188507":"code","3d0ccb67":"code","cc40964c":"code","fc1b12e5":"code","2a7641f7":"code","5c7ea144":"code","e1cbdd91":"code","5266b78d":"code","214c3a14":"code","57fed6ab":"code","e00147e8":"code","2c50a863":"code","aa359168":"code","9eb90662":"code","19e5c38f":"code","a3d24e6d":"code","6d971ce5":"code","f37ff3e1":"code","ae523613":"code","3dadab1a":"code","cf1248d5":"code","c900727e":"code","ca726be2":"code","94ed8d18":"code","4c77f397":"code","8a3a6a5d":"code","7ef1834d":"markdown","9b310672":"markdown","747ca11d":"markdown"},"source":{"c75063c1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings \n\nwarnings.filterwarnings('ignore')\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d5c82f84":"train = pd.read_csv('..\/input\/tweets-with-sarcasm-and-irony\/train.csv')\ntrain.head()","4f384ed9":"test = pd.read_csv('..\/input\/tweets-with-sarcasm-and-irony\/test.csv')\ntest.head()","7b9da88f":"# let's visualize the classes \ntrain['class'].value_counts()","5894405a":"test['class'].value_counts()","f05a691b":"# let's import some visualization Library\nimport seaborn as sns \nimport matplotlib.pyplot as plt \n\n","4d8bd7ca":"fig, ax = plt.subplots(1, 2, figsize=(22, 6))\nsns.countplot(train['class'], ax = ax[0])\nsns.countplot(test['class'], ax = ax[1])\nax[0].set_title('Training', size=19)\nax[1].set_title('Testing', size=19)\nplt.show()","0c84bc8d":"# let's check the shape of the dataset \n\nprint(f'Shape of the training dataset is : {train.shape}')\nprint(f'Shape of the testing dataset is : {test.shape}')","a8188507":"# let's check for the null values in dataste \nprint(f'Null values : \\n{train.isna().sum()}')\nprint(f'Null values : \\n{test.isna().sum()}')","3d0ccb67":"# two null values present in the test dataset .. we will drop these null values \n\ntest.dropna(axis=0, inplace=True)","cc40964c":"# let's check again the null values\nprint(f'Null values : \\n{test.isna().sum()}')","fc1b12e5":"s = train['tweets'][0]","2a7641f7":"!pip install emoji","5c7ea144":"\nimport re \nfrom lxml import html\nfrom emoji import demojize\n\nimport nltk\nnltk.download('stopwords')\n\n\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\n\nstemmer = PorterStemmer()\nstop = stopwords.words('english')\n\ndef clean_text(text):\n    \n    # Remove Hyperlinks\n    text = re.sub('http\\S+', ' ', text)\n\n    # Remove non alphabets\n    text = re.sub('[^a-zA-Z ]+', ' ', text)\n\n    # Lowercase and split\n    text = text.lower().split()\n\n    # Remove stopwords and short words\n    text = [stemmer.stem(word) for word in text if word not in stop and len(word) > 2]\n\n    # Join and Return\n    return ' '.join(text)\n\ndef clean_text2(tweet):\n    # Special characters\n    tweet = re.sub(r\"\\x89\u00db_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00d2\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00d3\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00cfWhen\", \"When\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00cf\", \"\", tweet)\n    tweet = re.sub(r\"China\\x89\u00db\u00aas\", \"China's\", tweet)\n    tweet = re.sub(r\"let\\x89\u00db\u00aas\", \"let's\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00f7\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00aa\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00a2\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00a2\u00e5\u00ca\", \"\", tweet)\n    tweet = re.sub(r\"from\u00e5\u00cawounds\", \"from wounds\", tweet)\n    tweet = re.sub(r\"\u00e5\u00ca\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c8\", \"\", tweet)\n    tweet = re.sub(r\"Jap\u00cc_n\", \"Japan\", tweet)    \n    tweet = re.sub(r\"\u00cc\u00a9\", \"e\", tweet)\n    tweet = re.sub(r\"\u00e5\u00a8\", \"\", tweet)\n    tweet = re.sub(r\"Suru\u00cc\u00a4\", \"Suruc\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c7\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5\u00a33million\", \"3 million\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c0\", \"\", tweet)\n    \n    #emojis\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    tweet =  emoji_pattern.sub(r'', tweet)\n    \n    # usernames mentions like \"@abc123\"\n    ment = re.compile(r\"(@[A-Za-z0-9]+)\")\n    tweet =  ment.sub(r'', tweet)\n    \n    # Contractions\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89\u00db\u00aat\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aam\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aave\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89\u00db\u00aas\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89\u00db\u00aat\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89\u00db\u00aas\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89\u00db\u00aas\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aave\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89\u00db\u00aat\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89\u00db\u00aat\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89\u00db\u00aas\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89\u00db\u00aare\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89\u00db\u00aat\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89\u00db\u00aat\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aall\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aad\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n    tweet = re.sub(r\"don\u00e5\u00abt\", \"do not\", tweet)   \n            \n    # Character entity references\n    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n    \n    # html tags\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    tweet = re.sub(html, '', tweet)\n    \n    # Urls\n    tweet = re.sub(r\"https?:\\\/\\\/t.co\\\/[A-Za-z0-9]+\", \"\", tweet)\n    tweet = re.sub(r'https?:\/\/\\S+|www\\.\\S+','', tweet)\n        \n    #Punctuations and special characters\n    \n    tweet = re.sub('[%s]' % re.escape(string.punctuation),'',tweet)\n    \n    tweet = tweet.lower()\n    \n    splits = tweet.split()\n    splits = [word for word in splits if word not in set(nltk.corpus.stopwords.words('english'))]\n    tweet = ' '.join(splits)\n    \n    return tweet\n    ","e1cbdd91":"print(f'Original String: {s}')\nprint(f'Cleaned String: {clean_text2(s)}')","5266b78d":"text = train['tweets'].apply(clean_text2)","214c3a14":"text[:10]","57fed6ab":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\nle = le.fit(train['class'])","e00147e8":"labels = le.transform(train['class'])","2c50a863":"text[:10], labels[:10]","aa359168":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer()","9eb90662":"texts_train = tfidf.fit_transform(text).todense()\n","19e5c38f":"# same for test data\ntest_text = test['tweets'].apply(clean_text)\ntest_labels = le.transform(test['class'])\n\ntexts_test = tfidf.transform(test_text).todense()\n\n","a3d24e6d":"from sklearn.naive_bayes import MultinomialNB\n\nclf = MultinomialNB()","6d971ce5":"clf.fit(texts_train, labels)","f37ff3e1":"from sklearn.metrics import classification_report\nprint(classification_report(test_labels, clf.predict(texts_test)))","ae523613":"# looks like our model is able to predict class 2 better than any other class \n","3dadab1a":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(test_labels, clf.predict(texts_test))","cf1248d5":"from sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\ncount_vect.fit(text)\n\n# transform the training and test data using count vectorizer object\ntextcv =  count_vect.transform(text)\n","c900727e":"textcv.todense()","ca726be2":"testcv = count_vect.transform(test_text)","94ed8d18":"clf.fit(textcv, labels)","4c77f397":"print(classification_report(test_labels, clf.predict(testcv)))","8a3a6a5d":"confusion_matrix(test_labels, clf.predict(testcv))","7ef1834d":"# Text Preprocessing ","9b310672":"# TFIDF","747ca11d":"# COUNT VECTORIZER "}}