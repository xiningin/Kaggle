{"cell_type":{"82691667":"code","49271e80":"code","0f110739":"code","5027840c":"code","21bd7017":"code","f6d698fb":"code","739b9692":"code","f9554660":"code","94a9abca":"code","8e3ea50b":"code","d2551401":"code","2c15646b":"code","fa74762f":"code","58f21e24":"code","954c989b":"code","66dbaed4":"code","e1acced6":"code","f08bdef5":"code","f5b42b41":"code","afc6bb40":"code","63060850":"code","98771268":"code","47861f3e":"code","221dea41":"code","51670d7f":"code","6e37d782":"code","560fe540":"code","632fa580":"code","2a115818":"code","8e87fe20":"code","7a90dc77":"code","6aa00261":"code","ad763f7c":"markdown","e43056f0":"markdown","34ff6561":"markdown","630a6ac3":"markdown","24e1832d":"markdown","1b092d11":"markdown","f313e180":"markdown","31d952b8":"markdown","fc7b940a":"markdown","fa873738":"markdown","99d264d1":"markdown","d3a84a81":"markdown","09141aa6":"markdown","e34a102d":"markdown","2f88023b":"markdown"},"source":{"82691667":"##Importing Libraries\n\n#Libraries for Dataset Loading and Data Wrangling\nimport pandas as pd\nimport numpy as np \n\n#Data Visualization\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\n\n#Model Selection & Model Optimization\nfrom sklearn.model_selection import train_test_split,RandomizedSearchCV\nfrom sklearn.feature_selection import SequentialFeatureSelector \n\n#Statistical Inference Analyis\nimport statsmodels.api as sm\nimport scipy.stats as stats\n\n#Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n#Validation\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\n#Filter's Warnings \nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","49271e80":"data = pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\nprint(f\"Number of Rows: {data.shape[0]}\\nNumber of Columns: {data.shape[1]}\")\ndata.head()","0f110739":"data.isna().sum()","5027840c":"data.describe()","21bd7017":"data.info()","f6d698fb":"labels = list(data['output'].unique())\nvalues = list(data['output'].value_counts())\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.5)])\nfig.show()","739b9692":"data.info()","f9554660":"data.describe()","94a9abca":"data.columns","8e3ea50b":"sns.set(rc = {'figure.figsize':(15,10)})\nsns.heatmap(data.corr(),annot=True,linewidths=0.4,\n    linecolor='Black',\n    cbar=False,cmap = 'plasma')","d2551401":"sns.set(rc = {'figure.figsize':(15,10)})\nsns.heatmap(data.isnull(),cmap = 'plasma',yticklabels = False)","2c15646b":"labels = list(data['sex'].unique())\nvalues = list(data['sex'].value_counts())\nfig = go.Figure(data=[go.Pie(labels=labels, values=values)])\nfig.show()","fa74762f":"data['cp'].value_counts(normalize=True).plot.bar(color=['green','blue','purple','red'],edgecolor='black',title='Major Vessels')","58f21e24":"sns.countplot(data=data,x='age')\nplt.title( 'Age Count')","954c989b":"sns.histplot(data,x = data['age'],bins = 10,)","66dbaed4":"X = data.drop('output',axis = 1)\ny = data.output","e1acced6":"Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size = 0.25,shuffle = True)","f08bdef5":"log_reg = sm.Logit(ytrain,Xtrain).fit()","f5b42b41":"print(log_reg.summary())","afc6bb40":"SFS = SequentialFeatureSelector(LogisticRegression(random_state=0),n_features_to_select=7,\n    direction='backward',\n    scoring='roc_auc',\n    cv=5).fit(Xtrain,ytrain)","63060850":"SFS_results = pd.DataFrame({'Variable':Xtrain.columns,\n                             'Chosen':SFS.get_support()})\nSFS_results.head(13)","98771268":"SFS_Variable = SFS_results[SFS_results['Chosen'] == True]['Variable']\nlog_reg = sm.Logit(ytrain, Xtrain[SFS_Variable]).fit()","47861f3e":"print(log_reg.summary())","221dea41":"Perfomance_df = pd.DataFrame(columns=['Model','Feature_Selection','Accuracy','Log_Loss','Roc'])\nPerfomance_df.head()","51670d7f":"#Pruning\nalphas = DecisionTreeClassifier(random_state=0).cost_complexity_pruning_path(Xtrain, ytrain)['ccp_alphas']\n                    \n#Pools of Parameters\nrandom_parameters = {'n_estimators': [10,100,1000],\n                     'criterion':['gini','entropy'],\n                     'max_depth': [10,100,1000],\n                     'max_features':[\"auto\",\"sqrt\", \"log2\"],\n                     'bootstrap' :[True,False],\n                     'class_weight': [\"balanced\", \"balanced_subsample\"], \n                     'ccp_alpha': alphas\n                    }\n\n#Randomized Cross Validation for Hyperparameters Tuning\nRFC = RandomizedSearchCV(RandomForestClassifier(), \n                         param_distributions = random_parameters,\n                         n_iter = 100,\n                         scoring = 'accuracy',\n                         n_jobs = 10,\n                         cv = 3,\n                         verbose = 2,\n                         random_state=0,\n                         return_train_score = True)\nRFC.fit(Xtrain, ytrain)","6e37d782":"Best_Parameter = RFC.best_params_\n\n\n#Test the Preforamnce of Best Parameters \n\nRFC = RandomForestClassifier(n_estimators = Best_Parameter['n_estimators'],\n                             criterion = Best_Parameter['criterion'],\n                             max_depth = Best_Parameter['max_depth'],\n                             max_features = Best_Parameter['max_features'],\n                             bootstrap = Best_Parameter['bootstrap'],\n                             class_weight = Best_Parameter['class_weight'],\n                             ccp_alpha = Best_Parameter['ccp_alpha']\n                            )\nRFC.fit(Xtrain, ytrain)","560fe540":"#Validation\npred = RFC.predict(Xtest)\nPerformance_df = Perfomance_df.append(pd.DataFrame([['RFC', 'Full', accuracy_score(ytest, pred), log_loss(ytest, pred), roc_auc_score(ytest, pred)]],\n                                                    columns = ['Model', 'Feature Selection','Accuracy', 'Log Loss', 'ROC']), sort = False)\n\nprint('Accuracy:', accuracy_score(ytest, pred))\nprint('Log Loss:', log_loss(ytest, pred))\nprint('ROC Accuracy:', roc_auc_score(ytest, pred))\nprint('Confusion Matrix:\\n', \n       confusion_matrix(ytest, pred))","632fa580":"plot_roc_curve(RFC, Xtest, ytest)","2a115818":"#Find the Alpha\nalphas = DecisionTreeClassifier(random_state=0).cost_complexity_pruning_path(Xtrain[SFS_Variable], ytrain)['ccp_alphas']\n\n#Pools of Parameters        \nrandom_parameters = {'n_estimators': [10,100,1000],\n                     'criterion':['gini','entropy'],\n                     'max_depth': [10,100,1000],\n                     'max_features':[\"auto\",\"sqrt\", \"log2\"],\n                     'bootstrap' :[True,False],\n                     'class_weight': [\"balanced\", \"balanced_subsample\"], \n                     'ccp_alpha': alphas\n                    }\n  \n#Randomized Cross Validation for Hyperparameters Tuning\nRFC = RandomizedSearchCV(RandomForestClassifier(), \n                         param_distributions = random_parameters,\n                         n_iter = 100,\n                         scoring = 'accuracy',\n                         n_jobs = 10,\n                         cv = 3,\n                         verbose = 2,\n                         random_state=0,\n                         return_train_score = True)\nRFC.fit(Xtrain[SFS_Variable], ytrain)","8e87fe20":"Best_Parameter = RFC.best_params_\n\n#Test the Preforamnce of Best Parameters \nRFC = RandomForestClassifier(n_estimators = Best_Parameter['n_estimators'],\n                             criterion = Best_Parameter['criterion'],\n                             max_depth = Best_Parameter['max_depth'],\n                             max_features = Best_Parameter['max_features'],\n                             bootstrap = Best_Parameter['bootstrap'],\n                             class_weight = Best_Parameter['class_weight'],\n                             ccp_alpha = Best_Parameter['ccp_alpha']\n                            )\nRFC.fit(Xtrain[SFS_Variable], ytrain)","7a90dc77":"#Validation\npred = RFC.predict(Xtest[SFS_Variable])\nPerformance_df = Performance_df.append(pd.DataFrame([['RFC', 'Selected', accuracy_score(ytest, pred), log_loss(ytest, pred), roc_auc_score(ytest, pred)]],\n                                                    columns = ['Model', 'Feature Selection','Accuracy', 'Log Loss', 'ROC']), sort = False)\n\nprint('Accuracy:', accuracy_score(ytest, pred))\nprint('Log Loss:', log_loss(ytest, pred))\nprint('ROC Accuracy:', roc_auc_score(ytest, pred))\nprint('Confusion Matrix:\\n', \n       confusion_matrix(ytest, pred))","6aa00261":"plot_roc_curve(RFC,Xtest[SFS_Variable], ytest)","ad763f7c":"1. Positivily Correlated Variables:\n* thalachh\n* cp\n* sip\n* restecg\n2. Negativily Correalated Variables:\n* exng \n* oldpeak\n* caa\n* thall","e43056f0":"> **RandomForestClassifier(for all dependent Variables):**","34ff6561":"**Note:**\nage, trtbps, chol, fbs, restecg shows high p-values, under which we cannot reject the null hypothesis that it is insignificant to the dependent variables, which should be removed at first. ","630a6ac3":"> **RandomForest(for Selected Dependent Variables):**","24e1832d":"# Univariate analysis","1b092d11":"# About this dataset\n## I) Dependant Variables\n1. Age : Age of the patient\n2. Sex : Sex of the patient\n3. exang: exercise induced angina (1 = yes; 0 = no)\n4. ca: number of major vessels (0-3)\n5. cp : Chest Pain type chest pain type\n     * Value 1: typical angina\n     * Value 2: atypical angina\n     * Value 3: non-anginal pain\n     * Value 4: asymptomatic\n6. trtbps : resting blood pressure (in mm Hg)\n7. chol : cholestoral in mg\/dl fetched via BMI sensor\n8. fbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n9. rest_ecg : resting electrocardiographic results\n     * Value 0: normal\n     * Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n     * Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n     * thalach : maximum heart rate achieved\n\n\n## II) Independent Variables:\ntarget : \n         0= less chance of heart attack \n         1= more chance of heart attack","f313e180":"The number of Male(68.3%) is greater than Female(31.7%)","31d952b8":"# Statistical Inference & Feature Selection ","fc7b940a":"**Notes:**\n\nThrough Logistic Regression and Sequential Feature Selection, we could further consolidate our findings and hypotheses from privous Pearson Correlation heatmap.\n\nThe outcome reveals that age, trtbps, chol, fbs, restecg are the significant variables for prediction and the remaining will be conceived as insignificant noise and removed accordingly.","fa873738":"The Datasets contains Zero Nulls values","99d264d1":"# Splitting the Data","d3a84a81":"# Importing Libraries","09141aa6":"# Model Evalution","e34a102d":"# Data Exploration","2f88023b":"The DataSet is Balanced"}}