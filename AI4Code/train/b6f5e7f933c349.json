{"cell_type":{"2b80b32c":"code","0b725d81":"code","773d3d96":"code","c1f52e63":"code","ea851090":"code","3518b6f9":"code","eaa81182":"code","6ca342d1":"code","95f77225":"code","e10b4617":"code","7c7e31c2":"code","92d007b0":"code","f2566360":"code","a8760451":"code","80fef1ac":"code","7235e9f2":"code","c53051f4":"code","c58aa085":"code","fb1040f7":"code","0ee450e4":"code","3cb0d3ee":"code","f1626581":"code","0a79178b":"code","57432abc":"code","96af345e":"code","8c003460":"code","7d808959":"code","5fc5b53a":"code","77911158":"code","ac1a309a":"code","027d0905":"markdown","a58691a2":"markdown","86687c02":"markdown","a137f6b7":"markdown","472fbfdd":"markdown","615d4a59":"markdown","721f4548":"markdown","44781362":"markdown","37833f90":"markdown","ac7813fb":"markdown","23b200e0":"markdown","b59cb326":"markdown","54e95fa2":"markdown","f2938c38":"markdown","fa00d824":"markdown","6104df67":"markdown","22b2f072":"markdown","d521ad97":"markdown","3024dfef":"markdown","63d992c1":"markdown","6a2887df":"markdown","1885814e":"markdown","4f1bd65e":"markdown"},"source":{"2b80b32c":"!pip install keras\n!pip install pydot\n!pip install graphviz","0b725d81":"# IPython display functions\nimport IPython\nfrom IPython.display import display, HTML, SVG, Image\n\n# General Plotting\nimport matplotlib.pyplot as plt\n\nplt.style.use('seaborn-paper')\nplt.rcParams['figure.figsize'] = [10, 6] ## plot size\nplt.rcParams['axes.linewidth'] = 2.0 #set the value globally\n\n## notebook style and settings\ndisplay(HTML(\"<style>.container { width:90% !important; }<\/style>\"))\ndisplay(HTML(\"<style>.output_png { display: table-cell; text-align: center; vertical-align: middle; } <\/style>\"))\ndisplay(HTML(\"<style>.MathJax {font-size: 100%;}<\/style>\"))\n\n# For changing background color\ndef set_background(color):\n    script = ( \"var cell = this.closest('.code_cell');\" \"var editor = cell.querySelector('.input_area');\" \"editor.style.background='{}';\" \"this.parentNode.removeChild(this)\" ).format(color)\n    display(HTML('<img src onerror=\"{}\">'.format(script)))","773d3d96":"import os\nimport sys\nimport random\nimport numpy as np\nimport pandas as pd\nfrom os import walk\n\n# Metrics\nfrom sklearn.metrics import *\n\n# Keras library for deep learning\n# https:\/\/keras.io\/\nimport tensorflow as tf\nimport keras\nfrom keras.datasets import mnist # MNIST Data set\nfrom keras.models import Sequential # Model building\nfrom keras.layers import * # Model layers\nfrom keras.preprocessing.image import *\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","c1f52e63":"import os\nimport glob\nimport h5py\nimport shutil\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mimg\nfrom os import listdir, makedirs, getcwd, remove\nfrom os.path import isfile, join, abspath, exists, isdir, expanduser\nfrom PIL import Image\nfrom pathlib import Path\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom keras.models import Sequential, Model\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, SeparableConv2D\nfrom keras.layers import GlobalMaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.merge import Concatenate\nfrom keras.models import Model\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom keras import backend as K\ncolor = sns.color_palette()\n%matplotlib inline","ea851090":"def displayConfusionMatrix(confusionMatrix, precisionNegative, precisionPositive, recallNegative, recallPositive, title):\n    # Set font size for the plots. You can ignore this line.\n    PLOT_FONT_SIZE = 14\n    \n    # Set plot size. Please ignore this line\n    plt.rcParams['figure.figsize'] = [5, 5]\n    \n    # Transpose of confusion matrix to align the plot with the actual precision recall values. Please ignore this as well.\n    confusionMatrix = np.transpose(confusionMatrix)\n    \n    # Plotting the confusion matrix\n    plt.imshow(confusionMatrix, interpolation='nearest',cmap=plt.cm.Blues, vmin=0, vmax=100)\n    \n    \n    # Setting plot properties. You should ignore everything from here on.\n    xticks = np.array([-0.5, 0, 1,1.5])\n    plt.gca().set_xticks(xticks)\n    plt.gca().set_yticks(xticks)\n    plt.gca().set_xticklabels([\"\", \"Healthy\\nRecall=\" + str(recallNegative) , \"Pneumonia\\nRecall=\" + str(recallPositive), \"\"], fontsize=PLOT_FONT_SIZE)\n    plt.gca().set_yticklabels([\"\", \"Healthy\\nPrecision=\" + str(precisionNegative) , \"Pneumonia\\nPrecision=\" + str(precisionPositive), \"\"], fontsize=PLOT_FONT_SIZE)\n    plt.ylabel(\"Predicted Class\", fontsize=PLOT_FONT_SIZE)\n    plt.xlabel(\"Actual Class\", fontsize=PLOT_FONT_SIZE)\n    plt.title(title, fontsize=PLOT_FONT_SIZE)\n        \n    # Add text in heatmap boxes\n    for i in range(2):\n        for j in range(2):\n            text = plt.text(j, i, confusionMatrix[i][j], ha=\"center\", va=\"center\", color=\"white\", size=15) ### size here is the size of text inside a single box in the heatmap\n            \n    plt.show()","3518b6f9":"def calculateMetricsAndPrint(predictions, predictionsProbabilities, actualLabels):\n    # Convert label format from [0,1](label 1) and [1,0](label 0) into single integers: 1 and 0.\n    actualLabels = [item[1] for item in actualLabels]\n    \n    # Get probabilities for the class with label 1. That is all we need to compute AUCs. We don't need probabilities for class 0.\n    predictionsProbabilities = [item[1] for item in predictionsProbabilities]\n    \n    # Calculate metrics using scikit-learn functions. The round function is used to round the numbers up to 2 decimal points.\n    accuracy = round(accuracy_score(actualLabels, predictions) * 100, 2)\n    precisionNegative = round(precision_score(actualLabels, predictions, average = None)[0] * 100, 2)\n    precisionPositive = round(precision_score(actualLabels, predictions, average = None)[1] * 100, 2)\n    recallNegative = round(recall_score(actualLabels, predictions, average = None)[0] * 100, 2)\n    recallPositive = round(recall_score(actualLabels, predictions, average = None)[1] * 100, 2)\n    auc = round(roc_auc_score(actualLabels, predictionsProbabilities) * 100, 2)\n    confusionMatrix = confusion_matrix(actualLabels, predictions)\n    \n    # Print metrics. .%2f prints a number upto 2 decimal points only.\n    print(\"------------------------------------------------------------------------\")\n    print(\"Accuracy: %.2f\\nPrecisionNegative: %.2f\\nPrecisionPositive: %.2f\\nRecallNegative: %.2f\\nRecallPositive: %.2f\\nAUC Score: %.2f\" % \n          (accuracy, precisionNegative, precisionPositive, recallNegative, recallPositive, auc))\n    print(\"------------------------------------------------------------------------\")\n    \n    print(\"+ Printing confusion matrix...\\n\")\n    # Display confusion matrix\n    displayConfusionMatrix(confusionMatrix, precisionNegative, precisionPositive, recallNegative, recallPositive, \"Confusion Matrix\")\n    \n    print(\"+ Printing ROC curve...\\n\")\n    # ROC Curve\n    plt.rcParams['figure.figsize'] = [16, 8]\n    FONT_SIZE = 16\n    falsePositiveRateDt, truePositiveRateDt, _ = roc_curve(actualLabels, predictionsProbabilities)\n    plt.plot(falsePositiveRateDt, truePositiveRateDt, linewidth = 5, color='black')\n    plt.xticks(fontsize=FONT_SIZE)\n    plt.yticks(fontsize=FONT_SIZE)\n    plt.xlabel(\"False Positive Rate\", fontsize=FONT_SIZE)\n    plt.ylabel(\"True Positive Rate\", fontsize=FONT_SIZE)\n    plt.show()\n    \n    return auc","eaa81182":"def getKagglePredictions(model, kaggleData, filename):\n    print(\"+ Writing kaggle test results in : ..\/input\/output\/%s...\" % filename)\n    predictions = model.predict(kaggleData)\n    predictionProbs = [item[1] for item in predictions]\n        \n    # Store predictions for kaggle\n    outputFile = open(\"..\/input\/output\/\" + str(filename), \"w\")\n    outputFile.write(\"Id,Prediction\\n\")\n    for i in range(0, len(predictionProbs)):\n        outputFile.write(str(i + 1) + \",\" + str(predictionProbs[i]) + \"\\n\")\n    \n    outputFile.close()","6ca342d1":"def calculateClasswiseTopNAccuracy(actualLabels, predictionsProbs, TOP_N):\n    \"\"\"\n    TOP_N is the top n% predictions you want to use for each class\n    \"\"\"\n\n    discreteActualLabels = [1 if item[1] > item[0] else 0 for item in actualLabels]\n    discretePredictions = [1 if item[1] > item[0] else 0 for item in predictionsProbs]\n    predictionProbsTopNHealthy, predictionProbsTopNPneumonia = [item[0] for item in predictionsProbs], [item[1] for item in predictionsProbs]\n    predictionProbsTopNHealthy = list(reversed(sorted(predictionProbsTopNHealthy)))[:int(len(predictionProbsTopNHealthy) * TOP_N \/ 100)][-1]\n    predictionProbsTopNPneumonia = list(reversed(sorted(predictionProbsTopNPneumonia)))[:int(len(predictionProbsTopNPneumonia) * TOP_N \/ 100)][-1]\n\n    # Calculate accuracy for both classes\n    accuracyHealthy = []\n    accuracyPneumonia = []\n    for i in range(0, len(discretePredictions)):\n        if discretePredictions[i] == 1:\n            # Pneumonia\n            if predictionsProbs[i][1] > predictionProbsTopNPneumonia:\n                accuracyPneumonia.append(int(discreteActualLabels[i]) == 1)\n        else:\n            # Healthy\n            if predictionsProbs[i][0] > predictionProbsTopNHealthy:\n                accuracyHealthy.append(int(discreteActualLabels[i]) == 0)\n\n    accuracyHealthy = round((accuracyHealthy.count(True) * 100) \/ len(accuracyHealthy), 2)\n    accuracyPneumonia = round((accuracyPneumonia.count(True) * 100) \/ len(accuracyPneumonia), 2)\n    return accuracyHealthy, accuracyPneumonia","95f77225":"# Load normal images\nnormalImagesPath = \"..\/input\/deep-image-normal\/normal\"\nnormalImageFiles = []\nfor(_,_,files) in walk(normalImagesPath):\n    normalImageFiles.extend(files)\n\n\n# Load pneumonia images\npneumoniaImagesPath = \"..\/input\/deep-image-normal\/pneumonia\/pneumonia\"\npneumoniaImageFiles = []\nfor(_,_,files) in walk(pneumoniaImagesPath):\n    pneumoniaImageFiles.extend(files)\n    \nrandom.shuffle(pneumoniaImageFiles)\npneumoniaImageFiles = pneumoniaImageFiles[:len(normalImageFiles)]\nprint(\"Normal X-ray images: %d\\nPneumonia X-ray images: %d\" % (len(normalImageFiles), len(pneumoniaImageFiles)))","e10b4617":"imagesData = []\nimagesLabels = []\n\nfor file in normalImageFiles:\n    fullPath = normalImagesPath + \"\/\" + file\n    if os.path.exists(fullPath) == False:\n            continue\n    imageData = load_img(normalImagesPath + \"\/\" + file, color_mode = \"grayscale\") # load_img function comes from keras library when we do \"from keras.preprocessing.image import *\"\n    imageArray = img_to_array(imageData) \/ 255.0\n    \n    imagesData.append(imageArray)\n    imagesLabels.append(0)\n    \n\nfor file in pneumoniaImageFiles:\n    fullPath = pneumoniaImagesPath + \"\/\" + file\n    if os.path.exists(fullPath) == False:\n            continue\n            \n    imageData = load_img(pneumoniaImagesPath + \"\/\" + file, color_mode = \"grayscale\") # load_img function comes from keras library when we do \"from keras.preprocessing.image import *\"\n    imageArray = img_to_array(imageData) \/ 255.0\n    \n    imagesData.append(imageArray)\n    imagesLabels.append(1)\n\nimagesData = np.array(imagesData)\nimagesLabels = keras.utils.to_categorical(imagesLabels)\nprint(\"Input data shape: %s\" % (imagesData.shape,))","7c7e31c2":"testImagesPath = \"d..\/input\/deep-image-normal\/test\/test\"\ntestImageFiles = []\nfor(_,_,files) in walk(testImagesPath):\n    testImageFiles.extend(files)\ntestImageFiles = list(sorted(testImageFiles))\n    \nkaggleTestImages = []\nfor file in testImageFiles:\n    fullPath = testImagesPath + \"\/\" + file\n    if os.path.exists(fullPath) == False:\n        continue\n    imageData = load_img(testImagesPath + \"\/\" + file, color_mode = \"grayscale\") # load_img function comes from keras library when we do \"from keras.preprocessing.image import *\"\n    imageArray = img_to_array(imageData) \/ 255.0\n    \n    kaggleTestImages.append(imageArray)\n    \nkaggleTestImages = np.array(kaggleTestImages)\nprint(\"Number of test images: %d\" % len(kaggleTestImages))","92d007b0":"def trainTestSplit(data, labels):\n    \"\"\"\n    80-20 train-test data split\n    \"\"\"\n    trainData, trainLabels, testData, testLabels = [], [], [], []\n    for i in range(0, len(data)):\n        if i % 5 == 0:\n            testData.append(data[i])\n            testLabels.append(labels[i])\n        else:\n            trainData.append(data[i])\n            trainLabels.append(labels[i])\n            \n    return np.array(trainData), np.array(testData), np.array(trainLabels), np.array(testLabels)","f2566360":"# Split data into 80% training and 20% testing\ntrainData, testData, trainLabels, testLabels = trainTestSplit(imagesData, imagesLabels)","a8760451":"def createParameterizedConvolutionalNeuralNetwork(trainImages, numLayers, numFilters, kernelSize, maxPooling, dropoutValue, learningRate, numClasses):\n    # Create model object\n    model = Sequential()\n    \n    model.add(Conv2D(numFilters, kernel_size=(kernelSize, kernelSize),\n                       activation='relu', padding = 'same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(MaxPooling2D(pool_size=(maxPooling, maxPooling)))\n    model.add(Dropout(dropoutValue))\n    \n    while numLayers > 1:\n        model.add(Conv2D(numFilters, kernel_size=(kernelSize, kernelSize),\n                     activation='relu', padding = 'same'))\n        model.add(MaxPooling2D(pool_size=(maxPooling, maxPooling)))\n        model.add(Dropout(dropoutValue))\n        \n        numLayers = numLayers - 1\n        \n    # Flatten & Final dropout\n    model.add(Flatten())\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(dropoutValue))\n    model.add(Dense(numClasses, activation='softmax'))\n\n    # Compile model.\n    model.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer=keras.optimizers.Adam(lr=learningRate),\n                  metrics=['accuracy'])\n    # Return model\n    return model\n","80fef1ac":"def createNuancedConvolutionalNeuralNetwork(trainImages, numClasses):\n\n        # Create model object\n    model = Sequential()\n    \n\n    model.add(Conv2D(filters = 32, kernel_size=(3, 3),\n                     activation='relu', padding = 'Valid',\n                     input_shape=trainImages.shape[1:]))\n    model.add(MaxPooling2D(pool_size=(2, 2),strides=2))\n    model.add(Dropout(0.2))\n    \n    model.add(Conv2D(filters = 64, kernel_size=(5, 5),\n                     activation='relu', padding = 'Valid',\n                     input_shape=trainImages.shape[1:]))\n    model.add(Conv2D(filters = 64, kernel_size=(5, 5),\n                     activation='relu', padding = 'Valid',\n                     input_shape=trainImages.shape[1:]))\n    model.add(MaxPooling2D(pool_size=(2, 2),strides=2))\n    model.add(Dropout(0.3))\n     \n    # Second layer with diffefiltersrent parameters\n    model.add(Conv2D(filters = 128, kernel_size=(7, 7),\n                     activation='relu', padding = 'Valid',\n                     input_shape=trainImages.shape[1:]))\n    model.add(Conv2D(filters = 128, kernel_size=(7, 7),\n                     activation='relu', padding = 'Valid',\n                     input_shape=trainImages.shape[1:]))\n    model.add(MaxPooling2D(pool_size=(2, 2),strides=2))\n    model.add(Dropout(0.5))\n  \n    # Flatten & Final dropout\n    model.add(Flatten())\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.7))\n    model.add(Dense(numClasses, activation='softmax'))\n\n    # Compile model. \n    model.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer=keras.optimizers.Adam(lr=0.001),\n                  metrics=['accuracy'])\n    \n    # Return model\n    return model","7235e9f2":"def cnn0(trainImages, numClasses):\n\n        # Create model object\n    model = Sequential()\n    \n\n    model.add(Conv2D(filters = 16, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(Conv2D(filters = 16, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    \n    \n\n    model.add(SeparableConv2D(filters = 32, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(SeparableConv2D(filters = 32, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    ##model.add(Dropout(0.3))\n    \n\n    model.add(SeparableConv2D(filters = 64, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(SeparableConv2D(filters = 64, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    ##model.add(Dropout(0.3))\n    \n\n    model.add(SeparableConv2D(filters = 128, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(SeparableConv2D(filters = 128, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.2))\n    \n    \n\n    model.add(SeparableConv2D(filters = 256, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(SeparableConv2D(filters = 256, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.2))\n    \n  \n    # Flatten & Final dropout\n    model.add(Flatten())\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.7))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(numClasses, activation='softmax'))\n\n    # Compile model. You can skip this line.\n    model.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer=keras.optimizers.Adam(lr=0.0001, decay=1e-5),\n                  metrics=['accuracy'])\n                  \n    # Return model\n    return model\n\n","c53051f4":"def cnn1(trainImages, numClasses):\n\n        # Create model object\n    model = Sequential()\n    \n    # Conv Layers\n    model.add(Conv2D(filters = 64, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(Conv2D(filters = 64, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    \n    \n    model.add(SeparableConv2D(filters = 128, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(SeparableConv2D(filters = 128, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.3))\n    \n    model.add(SeparableConv2D(filters = 256, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(BatchNormalization())\n    model.add(SeparableConv2D(filters = 256, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(BatchNormalization())\n    model.add(SeparableConv2D(filters = 256, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.3))\n    \n    model.add(SeparableConv2D(filters = 512, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(BatchNormalization())\n    model.add(SeparableConv2D(filters = 512, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(BatchNormalization())\n    model.add(SeparableConv2D(filters = 512, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.3))\n    \n  \n    # Flatten & Final dropout\n    model.add(Flatten())\n    model.add(Dense(1024, activation='relu'))\n    model.add(Dropout(0.7))\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(numClasses, activation='softmax'))\n\n    # Compile model. \n    model.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer=keras.optimizers.Adam(lr=0.0001, decay=1e-5),\n                  metrics=['accuracy'])\n                  \n    # Return model\n    return model","c58aa085":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils.vis_utils import plot_model","fb1040f7":"def cnn2(trainImages, numClasses):\n\n        # Create model object\n    model = Sequential()\n    \n    # Add the first layer with dropout\n    model.add(Conv2D(filters = 16, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(Conv2D(filters = 16, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    \n    \n    model.add(SeparableConv2D(filters = 32, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(SeparableConv2D(filters = 32, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    \n    model.add(SeparableConv2D(filters = 64, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(SeparableConv2D(filters = 64, kernel_size=(5, 5),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.2))\n    \n    model.add(SeparableConv2D(filters = 128, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(SeparableConv2D(filters = 128, kernel_size=(5, 5),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.3))\n    \n    \n    model.add(SeparableConv2D(filters = 256, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(SeparableConv2D(filters = 256, kernel_size=(5, 5),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(SeparableConv2D(filters = 256, kernel_size=(7, 7),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.3))\n    \n    model.add(SeparableConv2D(filters = 512, kernel_size=(3, 3),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(SeparableConv2D(filters = 512, kernel_size=(5, 5),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(SeparableConv2D(filters = 512, kernel_size=(7, 7),\n                     activation='relu', padding = 'Same',\n                     input_shape=trainImages.shape[1:]))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.3))\n    \n  \n    # Convolutional layers are done\n    model.add(Flatten())\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.7))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(numClasses, activation='softmax'))\n\n    # Compile model. You can skip this line.\n    model.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer=keras.optimizers.Adam(lr=0.0001, decay=1e-5),\n                  metrics=['accuracy'])\n                  \n    # Return model\n    return model","0ee450e4":"set_background('#fce53a')\n\ndataAugmentation = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0,\n    height_shift_range=0.2,\n    horizontal_flip=True,\n    vertical_flip=True,\n    shear_range=0,\n    zoom_range=0.2)","3cb0d3ee":"set_background('#fce53a')\n\n\nnumLayers = 5 \nnumFilters = 32\nkernelSize = 5 \ndropoutValue = 0.4\nmaxPooling = 2 \nnumClasses = 2 \nbatchSize = 16 \nlearningRate = .00001 \nepochs = 1 \nUSE_DATA_AUGMENTATION = False \n\n\n\n\ndataAugmentation.fit(trainData) ","f1626581":"# Create model\nparameterizedModel = createParameterizedConvolutionalNeuralNetwork(trainData, numLayers, numFilters, kernelSize, maxPooling, dropoutValue, learningRate, numClasses = 2)\nprint(\"+parameterized model has been created...\")","0a79178b":"cnn0 = cnn0(imagesData, numClasses = 2)\ncnn1 = cnn1(imagesData, numClasses = 2)\ncnn2 = cnn2(imagesData, numClasses = 2)\nprint(\"+parameterized model has been created...\")","57432abc":"model = cnn2","96af345e":"bestAcc = 0.0\nbestEpoch = 0\nbestAccPredictions, bestAccPredictionProbabilities = [], []\n\nprint(\"+ Starting training\" )\nprint(\"-----------------------------------------------------------------------\\n\")\nfor epoch in range(epochs):\n    \n    #################################################### Model Training ###############################################################\n    if USE_DATA_AUGMENTATION == True:\n        # Use data augmentation in alternate epochs\n        if epoch % 2 == 0:\n            # Alternate between training with and without augmented data. \n            model.fit_generator(dataAugmentation.flow(trainData, trainLabels, batch_size=batchSize),\n                        steps_per_epoch=len(trainData) \/ batchSize, epochs=1, verbose = 2)\n        else:\n            model.fit(trainData, trainLabels, batch_size=batchSize, epochs=1, verbose = 2)\n    else:\n        # Do not use data augmentation\n        model.fit(trainData, trainLabels, batch_size=batchSize, epochs=1, verbose = 2)\n    \n    \n    #################################################### Model Testing ###############################################################\n    # Calculate test accuracy\n    accuracy = round(model.evaluate(testData, testLabels)[1] * 100, 3)\n    predictions = model.predict(testData)\n    print(\"+ Test accuracy at epoch %d is: %.2f\" % (epoch, accuracy))\n    \n    if accuracy > bestAcc:\n        bestEpoch = epoch\n        bestAcc = accuracy\n        bestAccPredictions = [1 if item[1] > item[0] else 0 for item in predictions]\n        bestAccPredictionProbabilities = predictions\n        \n        ##################################### Store predictions for kaggle ###########################################################\n        #kaggleResultsFileName = \"epoch-\" + str(epoch) + \"-resultscnn0.csv\"\n        #getKagglePredictions(model, kaggleTestImages, kaggleResultsFileName)\n        ##############################################################################################################################\n    print('\\n')\nprint(\"------------------------------------------------------------------------\")\n\n\n##################################################### Printing best metrics ##########################################################\n# Get more metrics for the best performing epoch\nprint(\"\\n*** Printing our best validation results that we obtained in epoch %d ...\" % bestEpoch)\ncalculateMetricsAndPrint(bestAccPredictions, bestAccPredictionProbabilities, testLabels)","8c003460":"#model.save('my_model.h5')","7d808959":"# Different Output view for PPT\n!pip install keras_sequential_ascii\nfrom keras_sequential_ascii import keras2ascii\nkeras2ascii(cnn2)","5fc5b53a":"%load_ext tensorboard","77911158":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nfrom packaging import version\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nprint(\"TensorFlow version: \", tf.__version__)\nassert version.parse(tf.__version__).release[0] >= 2, \\\n    \"This notebook requires TensorFlow 2.0 or above.\"\n","ac1a309a":"################################## Does not work with Sample Dataset ###########################\ntopNValues = [5, 10, 20, 30]\n##############################################################################################################\n\n#accuraciesHealthy, accuraciesPneumonia = [], []\n#for topn in topNValues:\n#    accuracyHealthy, accuracyPneumonia = calculateClasswiseTopNAccuracy(testLabels, bestAccPredictionProbabilities, topn)\n#    accuraciesHealthy.append(accuracyHealthy)\n#    accuraciesPneumonia.append(accuracyPneumonia)\n    \n#    print(\"+ Accuracy for top %d percent predictions for healthy: %.2f, pneumonia: %.2f\" % (topn, accuracyHealthy, accuracyPneumonia))\n    \n# Plot results\n#x = np.arange(len(accuraciesHealthy))\n#plt.plot(x, accuraciesHealthy, linewidth = 3, color = '#e01111')\n#scatterHealthy = plt.scatter(x, accuraciesHealthy, marker = 's', s = 100, color = '#e01111')\n#plt.plot(x, accuraciesPneumonia, linewidth = 3, color = '#0072ff')\n#scatterPneumonia = plt.scatter(x, accuraciesPneumonia, marker = 'o', s = 100, color = '#0072ff')\n#plt.xticks(x, topNValues, fontsize = 15)\n#plt.yticks(fontsize = 15)\n#plt.xlabel(\"Top N%\", fontsize = 15)\n#plt.ylabel(\"Accuracy\", fontsize = 15)\n#plt.legend([scatterHealthy, scatterPneumonia], [\"Accuracy for Healthy\", \"Accuracy for Pneumonia\"], fontsize = 17)\n#plt.ylim(0, 110)\n#plt.show()\n","027d0905":"## 4.3 Training and Validation","a58691a2":"## 1.1 Confusion Matrix","86687c02":"## 1.3 Kaggle Predictions","a137f6b7":"## 4.4 Top n% Predictions","472fbfdd":"## 1.4 Top n% accuracy","615d4a59":"Library Imports","721f4548":"# 2. Data Loading\n## 2.1 Loading File Paths","44781362":"![image.png](attachment:image.png)","37833f90":"# 3. Deep Learning Models\n\n\n## 3.1 Parameterized Convolutional Neural Networks\n","ac7813fb":"I improved the efficiency of this model through the use of depthwise separable convolution which reduces the number of multiplications and parameters per layer. It works by splitting the convolution into a depthwise step where it runs a filter of depth 1 over the layer and point wise step that convolves a single channel at a time then multiplies it once by the depth rather than doing this for every channel as in standard convolution.\n\nAdding separable conv2d layers to the model consistently improved the Kaggle AUC score which was around .996 with standard layers and around .998 with separable layers. This also improved runtime relative to the number of layers and the number of epochs that the model could run for without overfitting. It did significantly reduce validation performance in the 5-6 epochs but outperformed the standard models after 20 epochs and started scoring over .998 after about 40 epochs. ","23b200e0":"![image.png](attachment:image.png)","b59cb326":"## 4.2 Model Parameters","54e95fa2":"## 3.2 More Nuanced Convolutional Neural Networks.","f2938c38":"# 4. Model Training\n## 4.1 Data Augmentation\n","fa00d824":"## 2.3 Data Splitting into Training and Validation","6104df67":"### 4.3.2 Model Training and Validation","22b2f072":"### 2.2.2 Kaggle Testing Data","d521ad97":"## 2.2 Loading Image Data\n### 2.2.1 Training and Validation","3024dfef":"### 4.3.1 Model Instantiation","63d992c1":"For the .9993 score (in diagram below) and 1.0 on leaderboard (cnn 0 in notebook) I started with a standard convolutional layer for low level features then increased the number of filters in each set of separable convolutional layers to capture the more abstract white spots in edge cases.\n\nI was getting good results by stepping up the filter sizes in later layers to 5x5 & 7x7 for standard convolutional layers but this did not translate well to separable layers so I kept the filters at a constant 3x3.\n\nI relied primarily on batch normalization to reduce overfitting in the first few layers and gradually increased dropout towards the end, especially between fully connected layers.\n","6a2887df":"## 1.2 Metrics Calculation","1885814e":"Notebook Styling","4f1bd65e":"# 1. Helper Functions"}}