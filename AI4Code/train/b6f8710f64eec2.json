{"cell_type":{"4f168ad3":"code","d16f2fea":"code","03fc4e80":"code","7adba688":"code","081a6a94":"code","fa80f9a1":"code","0eb1a4c7":"code","ef919e79":"code","58498d34":"code","431b543c":"code","2dc58d87":"code","69c46644":"code","f4fb358b":"code","c47446f1":"code","00c63311":"markdown","2f516f9b":"markdown","15630175":"markdown","6e409053":"markdown","03599509":"markdown","48b501f2":"markdown","70024afb":"markdown","df349bbc":"markdown","cfe00676":"markdown","974e45f2":"markdown","08967517":"markdown"},"source":{"4f168ad3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom pandas import datetime\nimport math, time\nimport itertools\nfrom sklearn import preprocessing\nimport datetime\nfrom operator import itemgetter\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers.recurrent import LSTM\nfrom keras.models import load_model\nimport keras\nimport h5py\nimport requests\nimport os\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d16f2fea":"df = pd.read_csv('\/kaggle\/input\/brent-oil-prices\/BrentOilPrices.csv')\ndf.Date = pd.to_datetime(df.Date)\ndf.set_index('Date', inplace=True)\ndf.head()","03fc4e80":"df.isnull().sum()","7adba688":"import matplotlib.pyplot as plt\nimport seaborn as sns # using seaborn because the charts are more visually pleasing\n\nsns.set_theme(style=\"darkgrid\")\nplt.figure(figsize=(20,5))\nsns.lineplot(x=\"Date\", y=\"Price\", data=df)\nplt.title('Daily historical Brent Oil Prices available on the U.S. Energy Information Admin', fontsize=14)\nplt.ylabel('Dollars per Barrel')\nplt.show()","081a6a94":"df.describe()","fa80f9a1":"# Creating a simple moving average for 7 and 21 days\ndf['ma7'] = df.Price.rolling(window=7).mean()\ndf['ma21'] = df.Price.rolling(window=21).mean()\n\n# Creating the EMA\ndf['ema12'] = df.Price.ewm(span=12).mean().fillna(0)\ndf['ema26'] = df.Price.ewm(span=26).mean().fillna(0)\ndf['macd'] = df.ema12 - df.ema26\n\n#The variables below are used for Bollinger Bands.\nwindow=21\nno_std = 2\nrolling_mean = df.Price.rolling(window).mean()\nrolling_std = df.Price.rolling(window).std()\ndf['bollinger_low'] = (rolling_mean - (rolling_std * no_std)).fillna(0)\ndf['bollinger_high'] = (rolling_mean + (rolling_std * no_std)).fillna(0)\ndf['ema'] = df.Price.ewm(com=0.5).mean()\ndf['momentum'] =  df.Price - 1\n\ndf.head()","0eb1a4c7":"min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\ndataset = min_max_scaler.fit_transform(df.Price.values.reshape(-1, 1))\ndataset[0:10]","ef919e79":"# split into train and test sets\ntrain_size = int(len(dataset) * 0.7)\ntest_size = len(dataset) - train_size\ntrain, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\nf'Dataset size: {len(df)} >> Train length: {len(train)} || Test Length: {len(test)}'","58498d34":"# convert an array of values into a dataset matrix\ndef create_dataset(dataset, look_back=15):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    return np.array(dataX), np.array(dataY)","431b543c":"x_train, y_train = create_dataset(train, look_back=15)\nx_test, y_test = create_dataset(test, look_back=15)\nf'X_train: {x_train.shape} || \\\ny_train: {y_train.shape} || \\\nX_test: {x_test.shape} || \\\ny_test: {y_test.shape}'","2dc58d87":"x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\nx_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\nf'X_train: {x_train.shape} || \\\ny_train: {y_train.shape} || \\\nX_test: {x_test.shape} || \\\ny_test: {y_test.shape}'","69c46644":"# create and fit the LSTM network\nlook_back = 15\nmodel = Sequential()\nmodel.add(LSTM(20, input_shape=(1, look_back)))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(x_train, y_train, epochs=20, batch_size=1, verbose=2)","f4fb358b":"trainPredict = model.predict(x_train)\ntestPredict = model.predict(x_test)\n# invert predictions\ntrainPredict = min_max_scaler.inverse_transform(trainPredict)\ntrainY = min_max_scaler.inverse_transform([y_train])\ntestPredict = min_max_scaler.inverse_transform(testPredict)\ntestY = min_max_scaler.inverse_transform([y_test])\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","c47446f1":"# shift train predictions for plotting\ntrainPredictPlot = np.empty_like(dataset)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n\n# shift test predictions for plotting\ntestPredictPlot = np.empty_like(dataset)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n\n# plot baseline and predictions\nplt.figure(figsize=(20,5))\nplt.plot(trainPredictPlot, color='black', label='Train data')\nplt.plot(testPredictPlot, color='blue', label='Prediction',)\nplt.plot(min_max_scaler.inverse_transform(dataset),label='baseline', alpha=0.4, linewidth=5)\nplt.title('Daily historical Brent Oil Prices available on the U.S. Energy Information Admin', fontsize=14)\nplt.ylabel('Dollars per Barrel')\nplt.legend()\nplt.show()","00c63311":"### 3. Machine learning\nNow the actual prediction using machine learning begins to take shape. As stated earlier, machine learning do not handle missing values very well, but it also has a problem with outliers. To Fix dis we will use a scaler.\n\n### 3.1. Data Normalization\nA scaler, in machine learning is a tool that uses a range and set all values that you pass to it within data range.\nThe code belows handles that.","2f516f9b":"#### 1.3. Data quality check\nChecking the dataset as a whole using summary statistics.","15630175":"## About this notebook\n\nI am using the available data to predict Brent Oil Prices. Predicting on stochastic data--randomly determined--while incredibly hard can be optmize to a certain point using NN. While the accuracy is not very high further optimization in its accuracy can be achieved by a deeper understanding of the variables that compose the price of this dataset.\n \n### Steps taken towards the prediction:\n    1. Load the dataset and check its data\n        1.1. Verify any possible missing data and use the average price to fill it.\n        1.2. Visual inspection of the data using matplotlib\n        1.3. Data quality check\n    2. Build of technical indicators\n        2.1. MA - Moving Average\n        2.2. EMA - Exponential Moving Average\n        2.3. MACD - Moving Average convergence-divergence\n        2.4. Bollinger Bands\n    3. Machine Learning\n        3.1. Data Normalization\n        3.3. Split of the data into training and testing\n        3.3. Recurrent Neural Network Model & training\n        3.4. Visual inspection of the data & predicted data using matplotlib\n        3.5. Score\n    4. Conclussion","6e409053":"#### 1.2. Visual inspection of the data using matplotlib\nVisualy analyzing the data to check for ouliers that might skew the predictions.","03599509":"The function belows handles the data transformation of the training and test data so we can load it into our model, since LTMS prefere matrixes.","48b501f2":"# LSTM","70024afb":"#### 1.1. Verify any possible missing data and use the average price to fill it.\nMost machine learning models do not handle missing values very well so it is common practise to check for it before going any further.","df349bbc":"### 1. Load the dataset and check its data","cfe00676":"#### 3.3. Split of the data into training and testing\nIn order to test the effectiveness of our model we need to test it in the data that we already have.\nTo do so, you usually uses a *train_test_split* approach where 70% of our data is used for training and 30% for testing.","974e45f2":"### 2. Build of technical indicators\nThese indicators are a set of mathematical calculation that will be based on our 'oil price' they will help us to predict--hopefully--where the price is going.\n\n##### 2.2. MA - Moving Average\nA Moving average is, often called *Simple Moving Average* or SMA is a average that considers a time period to its calculation. The formula is as follows:$$MA=\\frac{A_{1}+A_{2}+A_{3}+...A_{n}}{n}$$ \nWhere: A=Average in period *n* and *n* = Number of periods\n\n***\n##### 2.2. EMA - Exponential Moving Average\nThis indicator is called Exponential Moving Average, what set its appart is that the most recent days have a higher importance to the formulas. This is often use to predict buy\/sell in the stock market.\n$$EMA_{Today} = \\begin{pmatrix}Value_{Today} * \\begin{pmatrix}\\frac{smoothing} {1+Days}\\end{pmatrix}\\end{pmatrix} \n                +EMA_{Yesterday} * \\begin{pmatrix}1-\\begin{pmatrix}\\frac{smoothing} {1+Days}\\end{pmatrix}\\end{pmatrix}\n                $$For the smoothing factor we will set its default value: 2.\n***\n#### 2.3. MACD - Moving Average convergence-divergence \nMACD the difference two EMA a short-period (fast) EMA and a long-period (slow)  EMA:$$MACD = EMA_{1} - EMA_{2}$$\nThe usual values for EMA 1 is 12 and the EMA 2 is 26.\n***\n#### 2.4. Bollinger Bands\nThis bands are useful to predict the volatily of a given item, allow us to see if there is anything that goes outside what is considere \"normal behavior\".\nThere are two bollinger bands, one in the upper part of the chart, called *upper band* and one in the lower part called *lower band*:$$Upper band: MA_{21 days} + (\\sigma MA_{21 days}x * 2)$$\n$$Upper band: MA_{21 days} - (\\sigma MA_{21 days}x * 2)$$ Where: x is the variable you are using as a predictor.","08967517":"We can now check our reshaped dataset."}}