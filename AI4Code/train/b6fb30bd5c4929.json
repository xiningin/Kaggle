{"cell_type":{"fee3dfe6":"code","3ecaaf1e":"code","e0a74850":"code","5a16343b":"code","fbea8b8f":"code","eb642751":"code","803019de":"code","7a2dd65e":"code","fd597dc5":"code","aaf73e5a":"code","e7688595":"code","d35d8bcc":"code","b06ddc92":"code","0bc2c9ac":"code","3216d84d":"code","c98b3eb8":"code","6e48d6f0":"code","b629bb8a":"code","3e846d1a":"code","d6a8e3b1":"code","005ce11b":"code","6a151172":"code","7920af86":"code","a91e39bf":"code","784f1863":"code","b9862379":"code","4cfb2069":"code","38e76cca":"code","1ea99fa2":"code","03f64b4a":"code","4d793461":"code","efcbf7af":"code","927c6ae2":"code","a2328776":"code","6c19592c":"code","da2ccfad":"code","316695ed":"code","30409202":"code","8abf70fe":"code","dec65b32":"code","73ec5e27":"code","9c3678a6":"code","a5ea37cc":"code","69f14c44":"code","7188c240":"code","d91b5574":"code","80a79065":"code","89a1ffb4":"code","fbd06519":"code","2eb9650f":"code","83537d97":"code","5cb08d36":"code","9b900778":"code","f8daea25":"code","682771d8":"code","a46d84ed":"markdown","84f131c0":"markdown","b756ca76":"markdown","18c4878b":"markdown","e18aea8e":"markdown","7cd2474b":"markdown","ca316be8":"markdown","d5440697":"markdown","d2dbe422":"markdown","12fa58d1":"markdown","3e0d567a":"markdown","7f70fc21":"markdown","9aea9543":"markdown","0b7a7e13":"markdown","6fd6997f":"markdown","d3f8bf91":"markdown","e080818e":"markdown","9e719e72":"markdown","5f5c7218":"markdown","40dcb900":"markdown","9cff9d0d":"markdown","e4fe9dc6":"markdown","f8a4d4e0":"markdown","43b0298f":"markdown","f2cdf5a8":"markdown","db85767f":"markdown","730dbe0f":"markdown","7e6219ca":"markdown","fbaf593e":"markdown","ef67ae90":"markdown","af423529":"markdown","2e7e9fdf":"markdown","3bff2a22":"markdown","163c18b5":"markdown","182ad272":"markdown","dd62fbe5":"markdown","c9b97f61":"markdown","8f339839":"markdown","beb89bf2":"markdown","a3a701bd":"markdown","7cb056c1":"markdown","b997eb2f":"markdown","7394d155":"markdown","42511a4c":"markdown","bb2c371b":"markdown","d0e43e6c":"markdown","a573c313":"markdown","2654a949":"markdown","b031f01c":"markdown","a115972e":"markdown","1f6b59fd":"markdown","e4d4b761":"markdown","8f29d6ff":"markdown","6b23baf5":"markdown","3f30272e":"markdown","4b2afcda":"markdown","b92197c0":"markdown","1d3536c3":"markdown","d7b194d7":"markdown","5d37a37d":"markdown","bfec2cba":"markdown","20bf21ce":"markdown"},"source":{"fee3dfe6":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.model_selection  import cross_val_score\n\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import plot_roc_curve","3ecaaf1e":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","e0a74850":"#observe the different feature type present in the data\ndf.shape\n","5a16343b":"df.info()","fbea8b8f":"#Changing the data type of Class\n\ndf['Class'] = df['Class'].astype('category')\n\n#Renaming the classes\ndf['Class'] = df['Class'].cat.rename_categories({1:'Fraudulent',0:'Non_Fraudulent'})\n\ndf['Class']","eb642751":"classes=df['Class'].value_counts()\nnormal_share=classes[0]\/df['Class'].count()*100\nprint(normal_share)\nfraud_share=classes[1]\/df['Class'].count()*100\nprint(fraud_share)","803019de":"#Creating a df for percentage of each class\nclass_share = {'Class':['fraudulent','non_fraudulent'],'Percentage':[fraud_share,normal_share]}\nclass_share = pd.DataFrame(class_share)\nclass_share.head()","7a2dd65e":"# Create a bar plot for the number and percentage of fraudulent vs non-fraudulent transcations\nsns.set_palette(\"muted\")\nplt.figure(figsize=(14,6))\nplt.subplot(121)\nsns.countplot('Class',data=df)\nplt.title('No. of fraudulent vs non-fraudulent')\n\nplt.subplot(122)\nsns.barplot(x='Class', y='Percentage',data=class_share)\nplt.title('% of fraudulent vs non-fraudulent')\nplt.show()\n","fd597dc5":"# Create a scatter plot to observe the distribution of classes with time\n#sns.set_palette(\"muted\")\nplt.figure(figsize=(10,6))\nsns.stripplot(x= 'Class', y= 'Time',data=df)\nplt.title('Distribution of Classes with Time\\n (0: Non-Fraudulent || 1: Fraudulent)')\nplt.show()","aaf73e5a":"# Create a scatter plot to observe the distribution of classes with Amount\nplt.figure(figsize=(10,6))\nsns.stripplot(x= 'Class', y= 'Amount',data=df)\nplt.title('Distribution of Classes with Amount\\n (0: Non-Fraudulent || 1: Fraudulent)')\nplt.show()","e7688595":"# Drop unnecessary columns\n# Dropping the column 'Time' since it does not have any impact on deciding a fraud transaction\n\ndf=df.drop('Time',axis=1)\ndf.shape","d35d8bcc":"#Plotting heatmap to check the coorelation\n\nplt.figure(figsize=(8,6))\n\nsns.heatmap(df.corr(),linewidths=0.5,cmap='YlGnBu')\n\nplt.show()","b06ddc92":"y= df.iloc[:,-1] #class variable\nX = df.iloc[:,:-1]\n","0bc2c9ac":"from sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0,stratify=y)\n\n#Using stratify=y so that proportion of each class is same in both train and test set","3216d84d":"print('Total count for each class:\\n', y.value_counts())\nprint(\"\\nCount of each class in train data:\\n\",y_train.value_counts())\nprint(\"\\nCount of each class in test data:\\n\",y_test.value_counts())","c98b3eb8":"# plot the histogram of a variable from the dataset to see the skewness\n# ploting distribution plot for all columns to check the skewness\n\n#Loop for creating distplot.\n\ncollist = list(X_train.columns)\n\nc = len(collist)\nm = 1\nn = 0\n\nplt.figure(figsize=(20,30))\n\nfor i in collist:\n  if m in range(1,c+1):\n    plt.subplot(8,4,m)\n    sns.distplot(X_train[X_train.columns[n]])\n    m=m+1\n    n=n+1\n\nplt.show()\n\n\n","6e48d6f0":"# - Apply : preprocessing.PowerTransformer(copy=False) to fit & transform the train & test data\n# Using \u2018yeo-johnson\u2019 method since it works with positive and negative values. It is used to improve normality or symmetry\n\n\nfrom sklearn.preprocessing import power_transform\n\nX_train = power_transform(X_train,method='yeo-johnson')\nX_test = power_transform(X_test,method='yeo-johnson')","b629bb8a":"# Converting X_train & X_test back to dataframe\ncols = X.columns\n\nX_train = pd.DataFrame(X_train)\nX_train.columns = cols\n\nX_test = pd.DataFrame(X_test)\nX_test.columns = cols\n","3e846d1a":"# plot the histogram of a variable from the dataset again to see the result \n# Plotting same set of variables as earlier to identify the difference.\n\n#Loop for creating distplot.\n\ncollist = list(X_train.columns)\n\nc = len(collist)\nm = 1\nn = 0\n\nplt.figure(figsize=(20,30))\n\nfor i in collist:\n  if m in range(1,c+1):\n    plt.subplot(8,4,m)\n    sns.distplot(X_train[X_train.columns[n]])\n    m=m+1\n    n=n+1\n\nplt.show()\n\n","d6a8e3b1":"# Function to plot ROC curve and classification score which will be used for each model\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\n\ndef plot_roc(fpr,tpr):\n    plt.plot(fpr, tpr, color='green', label='ROC')\n    plt.plot([0, 1], [0, 1], color='yellow', linestyle='--')\n    plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.legend()\n    plt.show()\n\ndef clf_score(clf):\n    prob = clf.predict_proba(X_test)\n    prob = prob[:, 1]\n    auc = roc_auc_score(y_test, prob)    \n    print('AUC: %.2f' % auc)\n    fpr, tpr, thresholds = roc_curve(y_test,prob, pos_label='Non_Fraudulent')\n    plot_roc(fpr,tpr)\n    predicted=clf.predict(X_test)\n    report = classification_report(y_test, predicted)\n    print(report)\n    return auc\n","005ce11b":"# Logistic Regression\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression #import the package\nfrom sklearn.model_selection import GridSearchCV\n","6a151172":"num_C = [0.001,0.01,0.1,1,10,100] #--> list of values\n\nfor cv_num in num_C:\n  clf = LogisticRegression(penalty='l2',C=cv_num,random_state = 0)\n  clf.fit(X_train, y_train)\n  print('C:', cv_num)\n  print('Coefficient of each feature:', clf.coef_)\n  print('Training accuracy:', clf.score(X_train, y_train))\n  print('Test accuracy:', clf.score(X_test, y_test))\n  print('')","7920af86":"#perform cross validation\n\ngrid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l2\"]}  # l2 ridge\n\nlsr = LogisticRegression()\nclf_lsr_cv = GridSearchCV(lsr,grid,cv=3,scoring='roc_auc')\nclf_lsr_cv.fit(X_train,y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",clf_lsr_cv.best_params_)\nprint(\"accuracy :\",clf_lsr_cv.best_score_)\n\n#perform hyperparameter tuning\n\n\n\n#print the optimum value of hyperparameters","a91e39bf":"# Fitting the model with best parameters .\n\nlsr_best = LogisticRegression(penalty='l2',C=0.01,random_state = 0)\nlsr_clf = lsr_best.fit(X_train,y_train)\nclf_score(lsr_clf)","784f1863":"#K-Nearest Neighbor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection  import cross_val_score\nfrom sklearn.metrics import accuracy_score, mean_squared_error\n\n# Taking only odd integers as K values to apply the majority rule. \nk_range = np.arange(1, 20, 2)\nscores = [] #to store cross val score for each k\nk_range","b9862379":"# Finding the best k with stratified K-fold method. \n# We will use cv=3 in cross_val_score to specify the number of folds in the (Stratified)KFold.\n\nfor k in k_range:\n  knn_clf = KNeighborsClassifier(n_neighbors=k)\n  knn_clf.fit(X_train,y_train)\n  score = cross_val_score(knn_clf, X_train, y_train, cv=3, n_jobs = -1)\n  scores.append(score.mean())\n\n#Storing the mean squared error to decide optimum k\nmse = [1-x for x in scores]\n","4cfb2069":"#Plotting a line plot to decide optimum value of K\n\nplt.figure(figsize=(20,8))\nplt.subplot(121)\nsns.lineplot(k_range,mse,markers=True,dashes=False)\nplt.xlabel(\"Value of K\")\nplt.ylabel(\"Mean Squared Error\")\nplt.subplot(122)\nsns.lineplot(k_range,scores,markers=True,dashes=False)\nplt.xlabel(\"Value of K\")\nplt.ylabel(\"Cross Validation Accuracy\")\n\nplt.show()\n","38e76cca":"#Fitting the best parameter to the model\n# 3 fold cross validation with K=3\n\nknn = KNeighborsClassifier(n_neighbors=3)\n\nknn_clf = knn.fit(X_train,y_train)\n\n","1ea99fa2":"# Checking AUC \n\nclf_score(knn_clf)\n","03f64b4a":"#importing libraries\n\nfrom sklearn import tree\nfrom pprint import pprint\n","4d793461":"# 5 fold cross validation for getting best parameter\n\ndepth_score=[]\ndep_rng = [x for x in range(1,20)]\nfor i in dep_rng:\n  clf = tree.DecisionTreeClassifier(max_depth=i)\n  score_tree = cross_val_score(estimator=clf, X=X_train, y=y_train, cv=5, n_jobs=-1)\n  depth_score.append(score_tree.mean())\nprint(depth_score)","efcbf7af":"#Plotting depth against score\n\nplt.figure(figsize=(8,6))\nsns.lineplot(dep_rng,depth_score,markers=True,dashes=False)\nplt.xlabel(\"Depth\")\nplt.ylabel(\"Cross Validation Accuracy\")\n\nplt.show()","927c6ae2":"#Fitting the model with depth=5 and plotting ROC curve\n\ndt = tree.DecisionTreeClassifier(max_depth = 5)\ndt_clf = dt.fit(X_train,y_train)\n\n#Plotting ROC\nclf_score(dt_clf)","a2328776":"#Import libraries\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n","6c19592c":"# Using grid search cv to find the best parameters.\n\nparam = {'n_estimators': [10, 20, 30, 40, 50], 'max_depth': [2, 3, 4, 7, 9]}\nrfc = RandomForestClassifier()\nclf_rfc_cv = GridSearchCV(rfc, param, cv=5,scoring='roc_auc', n_jobs=-1)\nclf_rfc_cv.fit(X_train,y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",clf_rfc_cv.best_params_)\nprint(\"accuracy :\",clf_rfc_cv.best_score_)\n\n","da2ccfad":"#Fitting model and plotting ROC\n\nrf = RandomForestClassifier(max_depth=9, n_estimators=30)\nRFC_clf = rf.fit(X_train,y_train)\n\n#Plotting ROC\nclf_score(RFC_clf)\n","316695ed":"#import libraries\n\nfrom xgboost import XGBClassifier\nfrom scipy import stats","30409202":"# Using grid search cv to find the best parameters.\n\nxgbst = XGBClassifier()\n\nparam_xgb = {'n_estimators': [130,140,150],\n              'max_depth': [3, 5, 7],\n               'min_child_weight':[1,2,3]\n             } \n\nclf_xgb_cv = GridSearchCV(xgbst, param_xgb, cv=3,scoring='roc_auc', n_jobs=-1)\nclf_xgb_cv.fit(X_train,y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",clf_xgb_cv.best_params_)\nprint(\"accuracy :\",clf_xgb_cv.best_score_)\n\n","8abf70fe":"#Fitting the model with best parameters.\n\nxgbst = XGBClassifier(n_estimators=150,max_depth=5,min_child_weight=3)\n\nxgb_clf = xgbst.fit(X_train,y_train)\n\n#Plotting ROC\nclf_score(xgb_clf)\n","dec65b32":"clf = XGBClassifier(n_estimators=150,max_depth=5,min_child_weight=3)  #initialise the model with optimum hyperparameters\nclf.fit(X_train, y_train)\n\n# print the evaluation score on the X_test by choosing the best evaluation metric\nclf_score(clf)","73ec5e27":"#importing SMOTE\n\nfrom imblearn.over_sampling import SMOTE\n\nsm = SMOTE()\nX_sm, y_sm = sm.fit_resample(X_train, y_train)","9c3678a6":"#CHecking shape and class count after smote\nfrom collections import Counter\n\nprint('Resampled dataset shape %s' % Counter(y_sm))\nprint(X_sm.shape)\nprint(y_sm.shape)","a5ea37cc":"# importing ADASYN\n\nfrom imblearn.over_sampling import ADASYN\n\nada = ADASYN()\nX_ada, y_ada = ada.fit_resample(X_train, y_train)","69f14c44":"# CHecking shape and class count after ADASYN\nfrom collections import Counter\n\nprint('Resampled dataset shape %s' % Counter(y_ada))\nprint(X_ada.shape)\nprint(y_ada.shape)","7188c240":"# Using the best parameters that we got from the cross validation on imbalanced data.\n\nlsr_best = LogisticRegression(penalty='l2',C=0.01,random_state = 0)\nlsr_sm = lsr_best.fit(X_sm,y_sm)\n\n# Printing ROC curve and accuracy scores\nclf_score(lsr_sm)","d91b5574":"lsr_ada = lsr_best.fit(X_ada,y_ada)\n\n# Printing ROC curve and accuracy scores\nclf_score(lsr_ada)","80a79065":"# KNN with SMOTE re-sampled data\n\nknn = KNeighborsClassifier(n_neighbors=3)\n\nknn_sm = knn.fit(X_sm,y_sm)\n\n#Printing ROC \n\nclf_score(knn_sm)","89a1ffb4":"# KNN with ADASYN re-sampled data\n\nknn = KNeighborsClassifier(n_neighbors=3)\n\nknn_ada = knn.fit(X_ada,y_ada)\n\n#Printing ROC \n\nclf_score(knn_ada)","fbd06519":"# Building model with SMOTE\n\ndt = tree.DecisionTreeClassifier(max_depth = 5)\ndt_sm = dt.fit(X_sm,y_sm)\n\n#Plotting ROC\nclf_score(dt_sm)","2eb9650f":"# Building model with ADASYN\n\ndt = tree.DecisionTreeClassifier(max_depth = 5)\ndt_ada = dt.fit(X_ada,y_ada)\n\n#Plotting ROC\nclf_score(dt_ada)","83537d97":"#Building Random forest with best parameters on SMOTE\nrf = RandomForestClassifier(max_depth=9, n_estimators=30)\nRFC_sm = rf.fit(X_sm,y_sm)\n\n#Plotting ROC\nclf_score(RFC_sm)\n","5cb08d36":"#Building Random forest with best parameters on ADASYN\nrf = RandomForestClassifier(max_depth=9, n_estimators=30)\nRFC_ada = rf.fit(X_ada,y_ada)\n\n#Plotting ROC\nclf_score(RFC_ada)\n","9b900778":"# Since X_sm and X_ada are arrays, we need to covert them to dataframes to avoid feature mismatch error \nX_sm = pd.DataFrame(X_sm)\nX_sm.columns = cols\n\nX_ada = pd.DataFrame(X_ada)\nX_ada.columns = cols","f8daea25":"#Fitting the XGBoost model with best parameters on SMOTE\n\nxgbst = XGBClassifier(n_estimators=150,max_depth=5,min_child_weight=3)\n\nxgb_sm = xgbst.fit(X_sm,y_sm)\n\n#Plotting ROC\nclf_score(xgb_sm)","682771d8":"#Fitting the XGBoost model with best parameters on ADASYN\n\nxgbst = XGBClassifier(n_estimators=150,max_depth=5,min_child_weight=3)\n\nxgb_ada = xgbst.fit(X_ada,y_ada)\n\n#Plotting ROC\nclf_score(xgb_ada)","a46d84ed":"### Splitting the data into train & test data","84f131c0":"## Credit Card Fraud Detection\n\nIn this project I have compared various classification models to check which model gives the best result.\n\nModels were built on the imbalanced data and hyperparameters were tuned. Then SMOTE and ADASYN techniques were used to balance the data. Models was tried on both SMOTE and ADASYN data to see which one is producing better result.\n\nUsed the following classification models.\n\n* Logistic Regression\n* KNN\n* Decision Tree\n* Random forest\n* XGBoost","b756ca76":"#### Proceed with the model which shows the best result \n","18c4878b":"#### Logistic regression with SMOTE","e18aea8e":"## Exploratory data analysis","7cd2474b":"Here we will observe the distribution of our classes","ca316be8":"**Note**:\n- Due to time constraint we have choosen few parameters to tune in each model.\n\n- For cross validation, GridsearchCV and Stratified Kfold (cross_val_score) has been used.","d5440697":"### Random Forest Classifier","d2dbe422":"#### Class balancing with **SMOTE**:","12fa58d1":"- Random Forest performs better on SMOTE.\n- Both AUC and Recall for Fraud transactions are better on ADASYN sampled data, but Precision is extremely low.\n- Where as in SMOTE we have a fair precision with good recall resulting in a fair f1-score(0.57). ","3e0d567a":"#### XGBoost with ADASYN","7f70fc21":"\n\n* The score for depth=5 is the highest. We will use this in our model.\n\n\n","9aea9543":"\n**Insight:**\n*   From the above plot we observe that there isn't any\nparticular time interval at which fraudulent transactions happen. It can happen at any time. \n* The Time column is evenly distributed for fraudulent transactions and doesn't seem to have any role in deciding whether a transaction is fraud or not.","0b7a7e13":"### If there is skewness present in the distribution use:\n- <b>Power Transformer<\/b> package present in the <b>preprocessing library provided by sklearn<\/b> to make distribution more gaussian","6fd6997f":"#### Random Forest on SMOTE ","d3f8bf91":"## Model Building on the imbalanced dataset\n","e080818e":"#### XGBoost with SMOTE","9e719e72":"\n\n*   The best C value is the one for which the difference between train and test score is the least.\n*   In our case the best value of C=0.1\n\n","5f5c7218":"* We are getting very good precision(0.97) for Faudulent class which is very good along with the AUC of 0.97","40dcb900":"- KNN gives same recall(0.88) on both SMOTE and ADASYN.\n- But on SMOTE, the AUC & f1-score are slightly better. So, KNN performs better on SMOTE. \n","9cff9d0d":"- AUC is higher in SMOTE by a small margin but Recall is better in ADASYN than SMOTE.\n- The Precision is extremely low in both, resulting in low f1-score. So the model is not good enough.","e4fe9dc6":"## Choosing the Best Model.\n\n- To save banks from high-value fraudulent transactions, we have to focus on a high recall in order to detect actual fraudulent transactions but we can not have a very low precision.\n\n- The top two models giving better AUC are KNN (with SMOTE) & Random Forest(with SMOTE).\n\n- Scores of Random Forest model:\n\n          AUC : 0.98 \n\n          Recall: 0.88\n\n          Precision: 0.42\n\n          f1-Score : 0.57\n\n- Scores of KNN model:\n\n          AUC : 0.94 \n\n          Recall: 0.88\n\n          Precision: 0.61\n\n          f1-Score : 0.72\n\n- Comparing both we can see that the Random forest model has more AUC score than KNN but the KNN model has a better f1-score (Which is a result of better precision and recall)\n\n- Though the recall is same in both, having a better precision at a little trade off with AUC score will help the model generalize better. Having a good precision will help preventing a fair transaction being called fraudaulent.\n\n- So the KNN model with SMOTE oversampling is our final model.","f8a4d4e0":"#### Decision Tree on Adasyn","43b0298f":"#### KNN on ADASYN","f2cdf5a8":"- AUC & Recall both are better on SMOTE. \n- But the f1-score is extremely low. Model is overfitting.","db85767f":"### Logistic Regression","730dbe0f":"#### KNN on SMOTE","7e6219ca":"* Out of the 5 models XGBoost performed the best with AUC of 0.98 and Recall of 0.78.","fbaf593e":"### **Model performance parameter:**\n\n\n*   We will use ROC curve and find AUC Score as the performance matrix for the models.\n*  ROC curve meausres the performance of the model at different thresholds which will help us find the optimum threshold for the model.\n\n","ef67ae90":"**Insight:** \n\n*   The fraudulent transactions do not have any high amount transactions. The maximum amount for a fraudulent transaction is somewhere around $2500. \n \n\n","af423529":"### Logistic Regression","2e7e9fdf":"* The KNN model with imbalanced data gives AUC of 0.94  which is pretty good but recall is 0.77 which is the score we should look to improve in this case.","3bff2a22":"## Model Building with Balanced data\n- We will use our tuned models which was built on imbalanced data, with both SMOTE and ADASYN technique and see which one gives the best result.","163c18b5":"* As seen above the count of each class is same after SMOTE resampling.","182ad272":"* We are getting AUC of 0.98 with f1-score of 0.86 which is good.\n* Recall is 0.78 which is better than our other models.","dd62fbe5":"* The AUC score for decision tree is only 0.88 which is not satisfactory. The precison and recall are also lower than KNN and logistic regression model.","c9b97f61":"* We got the best parameters for XGboost as following.\n\ntuned hpyerparameters : {'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 150}\nAUC : 0.9850958755280601","8f339839":"* V7 and V20 seem to have positive correlation with the feature 'Amount'. Since this is a PCA converted data, there isn't much to conclude from the heatmap.","beb89bf2":"The no. of fraudulent cases are quite insignificant compared to the non-fraudulent cases. This is a highly unbalnced dataset.","a3a701bd":"### Decision Tree Classifier\n\n","7cb056c1":"#### Decision Tree on Smote","b997eb2f":"### K-Nearest Neighbours","7394d155":"### Random Forest Classifier","42511a4c":"## Handling Class Imbalance\n\n* We will use two balancing techniques, SMOTE & ADASYN.\n* In SMOTE a subset of data is taken from the minority class as an example and then new synthetic similar instances are created.\n* ADASYN (Adaptive Synthetic) is an algorithm that generates synthetic data, and its greatest advantages are not copying the same minority data, and generating more data for \u201charder to learn\u201d examples.\n\n","bb2c371b":"**Insight:**\n\n\n*   We plotted distribution plots for all the variables and it is clearly that there are some variables which are skewed either towards left or right.\n*   This means all variables are not normally distributed as expected even if this is a PCA transformed dataset.\n\n* We must transform the data to remove the skewness.\n\n","d0e43e6c":"\n\n\n*   From the above plot optimum K value is 3 for KNN\n\n\n\n","a573c313":"### XGBoost Classifier","2654a949":"**Insight:**\n\n\n*   After the Power transformation the variables are more gaussian like.\n*   Changes in V1, V12, V26 and Amount coulmn are quite evident. Skewness has been removed to some extent.\n\n","b031f01c":"### Plotting the distribution of a variable","a115972e":"### KNN","1f6b59fd":"* We will use these parameters for Random forest  {'max_depth': 9, 'n_estimators': 30}. The Accuracy is 0.97 which is very good.","e4d4b761":"#### Logistic regression with ADASYN","8f29d6ff":"The data is imbalanced. Only 0.172% of total cases are positive. ","6b23baf5":"\n\n*   The AUC score is 0.98 but the data is clearly overfitting due to the imbalanced data.\n\n\n","3f30272e":"### XGBoost Classifier","4b2afcda":"### Decision Tree","b92197c0":"**Insight:**\n* Since the data is PCA transformed all features are expected tobe normally distributed. The data does not have any null value. Each feature has 284807 observations.\n* The datatype of the 'Class' variable is int. Since we know that the Class vaiable should be categorical (0: non fraud & 1:fraud), we must change the datatype.","1d3536c3":"\n\n*   Best parameters :  {'C': 0.01, 'penalty': 'l2'}\n\n\n","d7b194d7":"##### Preserve X_test & y_test to evaluate on the test data once you build the model","5d37a37d":"*  AUC is similar in both resampled data scenarios. \n* With SMOTE XGBoost gives a better Recall but both have a low precision & f1-score.","bfec2cba":"#### Class balancing with **ADASYN**:","20bf21ce":"#### Random Forest on ADASYN "}}