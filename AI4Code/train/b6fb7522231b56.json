{"cell_type":{"4fdc09a2":"code","5f772c2c":"code","ae5a610a":"code","96695dc1":"code","856760b8":"code","e252ee12":"code","3a4d0262":"code","a527f5b6":"code","b4b40371":"code","54b57672":"code","9aa73a5e":"code","629dcdc4":"code","dab35dc7":"code","1d842421":"code","c3753c18":"code","a4ebe135":"markdown","c6c7fab8":"markdown","54c00364":"markdown","063d8cef":"markdown","4b00b3fc":"markdown","652cebe2":"markdown","a5cf2674":"markdown","6a7324a5":"markdown","f6bcc298":"markdown"},"source":{"4fdc09a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5f772c2c":"import glob\nimport random\nimport os\nimport numpy as np\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom torch import nn\nfrom matplotlib import pyplot as plt\n\n#from pytorch_msssim import ssim, ms_ssim, SSIM, MS_SSIM\n\nimport argparse\nimport os\nimport numpy as np\nimport math\nimport itertools\nimport sys\n\nimport torchvision.transforms as transforms\nfrom torchvision.utils import save_image, make_grid\n\nfrom torch.utils.data import DataLoader\nfrom torch.autograd import Variable\n\n#from models import *\n#from datasets import *\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n\n\nfrom torchvision.models import vgg19\n\nfrom skimage import color\nfrom IPython import embed","ae5a610a":"def load_img(img_path):\n\tout_np = np.asarray(Image.open(img_path))\n\tif(out_np.ndim==2):\n\t\tout_np = np.tile(out_np[:,:,None],3)\n\treturn out_np\n\ndef resize_img(img, HW=(256,256), resample=3):\n\treturn np.asarray(Image.fromarray(img).resize((HW[1],HW[0]), resample=resample))\n\ndef preprocess_img(img_rgb_orig, HW=(256,256), resample=3):\n\t# return original size L and resized L as torch Tensors\n\timg_rgb_rs = resize_img(img_rgb_orig, HW=HW, resample=resample)\n\t\n\timg_lab_orig = color.rgb2lab(img_rgb_orig)\n\timg_lab_rs = color.rgb2lab(img_rgb_rs)\n\n\timg_l_orig = img_lab_orig[:,:,0]\n\timg_l_rs = img_lab_rs[:,:,0]\n\n\ttens_orig_l = torch.Tensor(img_l_orig)[None,None,:,:]\n\ttens_rs_l = torch.Tensor(img_l_rs)[None,None,:,:]\n\n\treturn (tens_orig_l, tens_rs_l)\n\ndef postprocess_tens_new(tens_orig_l, out_ab, mode='bilinear'):\n\t# tens_orig_l \tBatchsize x 1 x H_orig x W_orig\n\t# out_ab \t\tBatchsize x 2 x H x W\n    Batchsize = tens_orig_l.shape[0]\n\n    output_ = []\n    for i in range(Batchsize):\n        tens_orig_l_i = tens_orig_l[i][np.newaxis, :, :, :]\n        out_ab_i  = out_ab[i][np.newaxis, :, :, :]\n        HW_orig_i = tens_orig_l_i.shape[2:]\n        HW_i = out_ab_i.shape[2:]\n\n        # call resize function if needed\n        if(HW_orig_i[0]!=HW_i[0] or HW_orig_i[1]!=HW_i[1]):\n            out_ab_orig_i = F.interpolate(out_ab_i, size=HW_orig_i, mode='bilinear')\n        else:\n            out_ab_orig_i = out_ab_i\n\n        out_lab_orig_i = torch.cat((tens_orig_l_i, out_ab_orig_i), dim=1)\n        #output_.append(color.lab2rgb(out_lab_orig_i.data.cpu().numpy()[0,...].transpose((1,2,0))))\n        output_.append(color.lab2rgb(out_lab_orig_i.data.cpu().numpy()[0,...].transpose((1,2,0))).transpose((2,0,1)))\n    return np.array(output_)","96695dc1":"class BaseColor(nn.Module):\n\tdef __init__(self):\n\t\tsuper(BaseColor, self).__init__()\n\n\t\tself.l_cent = 50.\n\t\tself.l_norm = 100.\n\t\tself.ab_norm = 110.\n\n\tdef normalize_l(self, in_l):\n\t\treturn (in_l-self.l_cent)\/self.l_norm\n\n\tdef unnormalize_l(self, in_l):\n\t\treturn in_l*self.l_norm + self.l_cent\n\n\tdef normalize_ab(self, in_ab):\n\t\treturn in_ab\/self.ab_norm\n\n\tdef unnormalize_ab(self, in_ab):\n\t\treturn in_ab*self.ab_norm\n\n\nclass ECCVGenerator(BaseColor):\n    def __init__(self, norm_layer=nn.BatchNorm2d):\n        super(ECCVGenerator, self).__init__()\n\n        model1=[nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=True),]\n        model1+=[nn.ReLU(True),]\n        model1+=[nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=True),]\n        model1+=[nn.ReLU(True),]\n        model1+=[norm_layer(64),]\n\n        model2=[nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=True),]\n        model2+=[nn.ReLU(True),]\n        model2+=[nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1, bias=True),]\n        model2+=[nn.ReLU(True),]\n        model2+=[norm_layer(128),]\n\n        model3=[nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n        model3+=[nn.ReLU(True),]\n        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n        model3+=[nn.ReLU(True),]\n        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1, bias=True),]\n        model3+=[nn.ReLU(True),]\n        model3+=[norm_layer(256),]\n\n        model4=[nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n        model4+=[nn.ReLU(True),]\n        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n        model4+=[nn.ReLU(True),]\n        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n        model4+=[nn.ReLU(True),]\n        model4+=[norm_layer(512),]\n\n        model5=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n        model5+=[nn.ReLU(True),]\n        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n        model5+=[nn.ReLU(True),]\n        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n        model5+=[nn.ReLU(True),]\n        model5+=[norm_layer(512),]\n\n        model6=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n        model6+=[nn.ReLU(True),]\n        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n        model6+=[nn.ReLU(True),]\n        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n        model6+=[nn.ReLU(True),]\n        model6+=[norm_layer(512),]\n\n        model7=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n        model7+=[nn.ReLU(True),]\n        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n        model7+=[nn.ReLU(True),]\n        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n        model7+=[nn.ReLU(True),]\n        model7+=[norm_layer(512),]\n\n        model8=[nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=True),]\n        model8+=[nn.ReLU(True),]\n        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n        model8+=[nn.ReLU(True),]\n        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n        model8+=[nn.ReLU(True),]\n\n        model8+=[nn.Conv2d(256, 313, kernel_size=1, stride=1, padding=0, bias=True),]\n\n        self.model1 = nn.Sequential(*model1)\n        self.model2 = nn.Sequential(*model2)\n        self.model3 = nn.Sequential(*model3)\n        self.model4 = nn.Sequential(*model4)\n        self.model5 = nn.Sequential(*model5)\n        self.model6 = nn.Sequential(*model6)\n        self.model7 = nn.Sequential(*model7)\n        self.model8 = nn.Sequential(*model8)\n\n        self.softmax = nn.Softmax(dim=1)\n        self.model_out = nn.Conv2d(313, 2, kernel_size=1, padding=0, dilation=1, stride=1, bias=False)\n        self.upsample4 = nn.Upsample(scale_factor=4, mode='bilinear')\n\n    def forward(self, input_l):\n        conv1_2 = self.model1(self.normalize_l(input_l))\n        conv2_2 = self.model2(conv1_2)\n        conv3_3 = self.model3(conv2_2)\n        conv4_3 = self.model4(conv3_3)\n        conv5_3 = self.model5(conv4_3)\n        conv6_3 = self.model6(conv5_3)\n        conv7_3 = self.model7(conv6_3)\n        conv8_3 = self.model8(conv7_3)\n        out_reg = self.model_out(self.softmax(conv8_3))\n\n        return self.unnormalize_ab(self.upsample4(out_reg))\n\ndef eccv16(pretrained=True):\n\tmodel = ECCVGenerator()\n\tif(pretrained):\n\t\timport torch.utils.model_zoo as model_zoo\n\t\tmodel.load_state_dict(model_zoo.load_url('https:\/\/colorizers.s3.us-east-2.amazonaws.com\/colorization_release_v2-9b330a0b.pth',map_location='cpu',check_hash=True))\n\treturn model\n","856760b8":"class FeatureExtractor(nn.Module):\n    def __init__(self):\n        super(FeatureExtractor, self).__init__()\n        vgg19_model = vgg19(pretrained=True)\n        self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18])\n\n    def forward(self, img):\n        return self.feature_extractor(img)","e252ee12":"class Discriminator(nn.Module):\n    def __init__(self, input_shape):\n        super(Discriminator, self).__init__()\n\n        self.input_shape = input_shape\n        in_channels, in_height, in_width = self.input_shape\n        patch_h, patch_w = int(in_height \/ 2 ** 4), int(in_width \/ 2 ** 4)\n        self.output_shape = (1, patch_h, patch_w)\n\n        def discriminator_block(in_filters, out_filters, first_block=False):\n            layers = []\n            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n            if not first_block:\n                layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n            layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        layers = []\n        in_filters = in_channels\n        for i, out_filters in enumerate([64, 128, 256, 512]):\n            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n            in_filters = out_filters\n\n        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, img):\n        return self.model(img)","3a4d0262":"batch_size = 4\nn_cpu = 0\nout_channels = 3\n\nclass ImageDataset(Dataset):\n    def __init__(self, root, transform): #, shape):\n        #height, width = shape\n        self.transform = transform\n\n        self.files = sorted(glob.glob(root + \"\/*.*\"))\n\n    def __getitem__(self, index):\n        \n        black_path = self.files[index % len(self.files)]\n        color_path = black_path.replace('black','color')\n        \n        img_black = np.asarray(Image.open(black_path))\n        if(img_black.ndim==2):\n            img_black = np.tile(img_black[:,:,None],3)\n        (tens_l_orig, tens_l_rs) = preprocess_img(img_black, HW=(400, 400))\n        #img_bw = postprocess_tens(tens_l_orig, torch.cat((0*tens_l_orig,0*tens_l_orig),dim=1))\n        #img_black = self.transform(img_black)\n        \n        \n        img_color = Image.open(color_path)\n        img_color = self.transform(img_color)\n\n        return {\"black\": tens_l_rs.squeeze(0), 'orig': tens_l_orig.squeeze(0), \"color\": img_color}\n\n\n    def __len__(self):\n        return len(self.files)\n\nroot = '..\/input\/image-colorization-dataset\/data\/train_black\/'\ntransform = transforms.Compose(\n            [\n                #transforms.Resize((shape[0], shape[1]), Image.BICUBIC),\n                transforms.ToTensor(),\n                #transforms.Normalize(mean, std),\n            ]\n        )\ndataloader = DataLoader(\n    ImageDataset(root, transform),\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=n_cpu,\n)","a527f5b6":"dataiter = iter(dataloader)\ndata = dataiter.next()\n\ni = 1\nimg = data['black'][i].permute(1, 2, 0)\nprint(img.shape)\nt = np.zeros((400, 400, 3))\nt[..., 0] = img[..., 0]\nt = color.lab2rgb(t)\nplt.imshow(t)\nplt.show()\n\nimg = data['orig'][i].permute(1, 2, 0)\nprint(img.shape)\nt = np.zeros((400, 400, 3))\nt[..., 0] = img[..., 0]\nt = color.lab2rgb(t)\nplt.imshow(t)\nplt.show()\n\nprint(img.shape)\nimg = data['color'][i].permute(1, 2, 0)\nplt.imshow(img)\nplt.show()","b4b40371":"class color_ecv(nn.Module):\n    def __init__(self, in_channels):\n        super(color_ecv, self).__init__()\n        \n        self.model = eccv16(pretrained=True)\n    \n    def forward(self, x):\n        ecv_output = self.model(x)\n        return ecv_output","54b57672":"os.makedirs(\"colorit_gan\/images\", exist_ok=True)\nos.makedirs(\"colorit_gan\/saved_models\", exist_ok=True)","9aa73a5e":"start_epoch = 0\nn_epochs= 50\n\nlr = 0.0002\nb1 = 0.5\nb2 = 0.999\ndecay_epoch = 100\nin_channels = 1\nout_channels = 3\nsample_interval = 100\ncheckpoint_interval = 1\n\ncuda = torch.cuda.is_available()","629dcdc4":"shape = (400, 400)\n\n# Initialize generator and discriminator\ngenerator = color_ecv(in_channels = 3)\ndiscriminator = Discriminator(input_shape=(out_channels, *shape))\nfeature_extractor = FeatureExtractor()\n\n# Set feature extractor to inference mode\nfeature_extractor.eval()\n\n# Losses\ncriterion_GAN = torch.nn.MSELoss()\ncriterion_content = torch.nn.L1Loss()\n\nif cuda:\n    generator = generator.cuda()\n    discriminator = discriminator.cuda()\n    feature_extractor = feature_extractor.cuda()\n    criterion_GAN = criterion_GAN.cuda()\n    criterion_content = criterion_content.cuda()","dab35dc7":"if start_epoch != 0:\n    # Load pretrained models\n    generator.load_state_dict(torch.load(\"colorit_gan\/saved_models\/generator_\"+str(start_epoch-1)+\".pth\"))\n    discriminator.load_state_dict(torch.load(\"colorit_gan\/saved_models\/discriminator_\"+str(start_epoch-1)+\".pth\"))","1d842421":"# Optimizers\noptimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n\nTensor = torch.cuda.FloatTensor if cuda else torch.Tensor","c3753c18":"# ----------\n#  Training\n# ----------\n\nfor epoch in range(start_epoch, n_epochs):\n    for i, imgs in enumerate(dataloader):\n\n        # imgs_black : Light Channel of the Image in Model Shape\n        # imgs_black_orig : Light Channel of the Image in Original Image Shape\n        # imgs_color : Colourfull Image Provided \n        # gen_ab : A and B channel generated by the generator\n        # gen_color : Colourfull Image generated by the Generator after postprocessing\n        \n        \n        # Configure model input\n        imgs_black = Variable(imgs[\"black\"].type(Tensor))\n        imgs_black_orig = Variable(imgs[\"orig\"].type(Tensor))\n        imgs_color = Variable(imgs[\"color\"].type(Tensor))\n\n        # Adversarial ground truths\n        valid = Variable(Tensor(np.ones((imgs_black.size(0), *discriminator.output_shape))), requires_grad=False)\n        fake = Variable(Tensor(np.zeros((imgs_black.size(0), *discriminator.output_shape))), requires_grad=False)\n\n        # ------------------\n        #  Train Generators\n        # ------------------\n\n        optimizer_G.zero_grad()\n\n        # Generate a high resolution image from low resolution input\n        gen_ab = generator(imgs_black)\n        gen_color = postprocess_tens_new(imgs_black_orig, gen_ab)\n        if cuda:\n            gen_color = torch.from_numpy(gen_color).to('cuda')\n\n        # Adversarial loss\n        loss_GAN = criterion_GAN(discriminator(gen_color), valid)\n\n        # Content loss\n        gen_features = feature_extractor(gen_color.detach())\n        real_features = feature_extractor(imgs_color.detach())\n        loss_content = criterion_content(gen_features, real_features.detach())\n\n        # Total loss\n        loss_G = loss_content + 1e-3 * loss_GAN\n\n        loss_G.backward()\n        optimizer_G.step()\n\n        # ---------------------\n        #  Train Discriminator\n        # ---------------------\n\n        optimizer_D.zero_grad()\n\n        # Loss of real and fake images\n        loss_real = criterion_GAN(discriminator(imgs_color), valid)\n        loss_fake = criterion_GAN(discriminator(gen_color.detach()), fake)\n\n        # Total loss\n        loss_D = (loss_real + loss_fake) \/ 2\n\n        loss_D.backward()\n        optimizer_D.step()\n\n        # --------------\n        #  Log Progress\n        # --------------\n\n        sys.stdout.write(\n            \"[Epoch %d\/%d] [Batch %d\/%d] [D loss: %f] [G loss: %f]\"\n            % (epoch, n_epochs, i, len(dataloader), loss_D.item(), loss_G.item())\n        )\n\n        batches_done = epoch * len(dataloader) + i\n        if batches_done % sample_interval == 0:\n            # Save image grid with upsampled inputs and SRGAN outputs\n            #imgs_lr = nn.functional.interpolate(imgs_lr, scale_factor=4)\n            gen_color = make_grid(gen_color.detach(), nrow=1, normalize=True)\n            imgs_black_orig = make_grid(imgs_black_orig, nrow=1, normalize=True)\n            img_grid = torch.cat((imgs_black_orig, gen_color), -1)\n            save_image(img_grid, \".\/colorit_gan\/images\/%d.png\" % batches_done, normalize=False)\n\n    if checkpoint_interval != -1 and epoch % checkpoint_interval == 0:\n        # Save model checkpoints\n        torch.save(generator.state_dict(), \".\/colorit_gan\/saved_models\/generator_%d.pth\" % epoch)\n        torch.save(discriminator.state_dict(), \".\/colorit_gan\/saved_models\/discriminator_%d.pth\" % epoch)","a4ebe135":"# Models\n## ECV16\nThis Model is based on the paper Colorful Image Colorization https:\/\/arxiv.org\/abs\/1603.08511 which we will train using a GAN with the ECV Model (A normal residual convnet model proposed in the paper) as the generator and a discriminator that we will design.","c6c7fab8":"## Util","54c00364":"## Feature Extractor\nThis is a noval approach to compare to images and use that comparison as a loss to train a model.\nWe will use a pre-trained VGG19 model. Two images are passed in it and the activations of the 18th layers are taken for both the images and then this activations are used to calculate the loss which can be calculated using RMSE, MSE etc between the two activations.","063d8cef":"# See Detailed code at https:\/\/github.com\/aayush9753\/ColorIt","4b00b3fc":"## Discriminator\nTakes an image as input and converts it into a single no. after passing through several convolutional blocks.","652cebe2":"# Image Dataset","a5cf2674":"# Training","6a7324a5":"## Importing Libraries","f6bcc298":"# Initialising Models and Training Params"}}