{"cell_type":{"23ed0b25":"code","b673c8aa":"code","fe4ea3a4":"code","f3c6049d":"code","2caa5d06":"code","292b84d3":"code","e20bffab":"code","9bf59119":"code","8e50e50a":"code","e880111b":"code","224dc990":"code","3bb75ce0":"code","be2ca307":"code","517cce19":"code","8b4c7cd2":"code","2f2adb8c":"code","3aaccc29":"code","18953ff5":"code","0447478a":"code","818d0d3b":"code","e778ee8a":"code","64b65f5b":"code","7f7e19fe":"code","695d2c64":"code","5d8954e7":"code","0dfee253":"code","612543f4":"code","79bb2d27":"code","7901a3dd":"code","03f0eebd":"code","939ffed4":"code","efe4b48d":"code","22e7f49a":"code","a886f0f7":"code","3806741f":"code","56abda57":"code","0dfb3073":"code","6ccdb90e":"code","390eea8c":"code","ddb20b0c":"code","c5adf115":"code","4fb0302a":"code","cbe91ab4":"code","c0aa7e69":"code","8c0f6d15":"code","ccaaec53":"code","63b5aadf":"code","57256bfc":"code","5381bdf4":"code","c98b8f3b":"code","6878d880":"code","70a41398":"code","bc8d55ca":"code","d502c36f":"code","771dd870":"code","a96df805":"code","361a6d1f":"code","5c53bfc6":"code","fd528f6a":"code","264dca2b":"code","de20aad8":"code","dbaa27a4":"code","0723d1ec":"code","8f810ae5":"code","cc0a4d4a":"code","bdcb10d7":"code","c9c57561":"code","3540d0cd":"code","63c309b1":"code","48c6cfc9":"code","9f314abe":"code","b1a057a7":"code","355394e8":"code","98a596f8":"code","09691f87":"code","d6a59e0d":"code","301d4a90":"code","803404db":"code","f7853179":"code","cc308396":"code","c8948a45":"code","a9af556b":"code","b32714da":"code","af82fdc4":"code","11125b36":"code","eee2d6f4":"code","ead3311b":"code","e60fd140":"code","e2a4890e":"code","3caaf09a":"code","7755e994":"code","f697aacd":"code","14409a75":"markdown","968a767b":"markdown","ebc22ae9":"markdown","f686c16f":"markdown","ded727d7":"markdown","4e0a2d8c":"markdown","31eb20cf":"markdown","5a7aa1c6":"markdown","b3cd7b40":"markdown","6430f927":"markdown","1fb34da5":"markdown","ec92a417":"markdown","4cdfdeea":"markdown","a919dd3c":"markdown","27375b48":"markdown","b81e7d8f":"markdown","2af4b685":"markdown","676e7dfa":"markdown","a7c09ec4":"markdown","81a85979":"markdown","d4188746":"markdown","5c50071d":"markdown","d4d00c0a":"markdown","3daf04db":"markdown","5194e54f":"markdown","930c66ec":"markdown","0e363628":"markdown","c8465bb7":"markdown","7731cc22":"markdown","78a40e44":"markdown","6fdfe79e":"markdown","648de250":"markdown","16990702":"markdown","7f79c9cd":"markdown","f830e5a0":"markdown","c1c0950b":"markdown","9c818ce3":"markdown","7048e4f6":"markdown","555c9f05":"markdown","e5401498":"markdown","ae5d9d50":"markdown","5a44c4fc":"markdown","9574c983":"markdown","0ac1f5ff":"markdown"},"source":{"23ed0b25":"#from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n#from xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\n\n\nimport numpy as np\n#import pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets.samples_generator import make_blobs\nfrom sklearn.cluster import KMeans\n\n#Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","b673c8aa":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport math\nfrom scipy.stats import kurtosis\nfrom scipy.stats import skew\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom numpy.fft import *\nfrom sklearn.model_selection import cross_val_score,train_test_split\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.preprocessing import StandardScaler\nfrom mlxtend.classifier import StackingClassifier\nfrom mlxtend.classifier import StackingCVClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nimport seaborn as sns\nimport matplotlib.style as style \nimport eli5\nfrom skopt import BayesSearchCV\nfrom eli5.sklearn import PermutationImportance\nstyle.use('ggplot')\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn import datasets\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split, KFold\nfrom sklearn.metrics import recall_score, roc_auc_score\n\nfrom sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=3, random_state=None)\n# from xgboost import XGBClassifier\n# model = XGBClassifier()\nfrom sklearn.metrics import f1_score,classification_report\n\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\n\nfrom sklearn import svm, datasets\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom scipy import interp\nfrom sklearn.metrics import roc_auc_score\nfrom eli5.sklearn import PermutationImportance\n\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score,classification_report\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV","fe4ea3a4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f3c6049d":"sample = pd.read_csv('\/kaggle\/input\/datavidia2019\/sample_submission.csv')\nhotel = pd.read_csv('\/kaggle\/input\/datavidia2019\/hotel.csv')\nflight = pd.read_csv('\/kaggle\/input\/datavidia2019\/flight.csv')\ntest = pd.read_csv('\/kaggle\/input\/datavidia2019\/test.csv')","2caa5d06":"flight.info()","292b84d3":"flight.shape","e20bffab":"hotel.info()","9bf59119":"hotel.shape","8e50e50a":"flight['gender'].value_counts() \n# terdapat sebanyak 24 data yang tidak diketahui gendernya.","e880111b":"flight['trip'].value_counts()","224dc990":"flight['service_class'].value_counts()","3bb75ce0":"flight['is_tx_promo'].value_counts()","be2ca307":"flight['airlines_name'].value_counts()","517cce19":"flight['route'].value_counts()","8b4c7cd2":"flight.groupby(['is_tx_promo', 'gender']).count()","2f2adb8c":"# is_tx_promo\nprint('female NO  :', (29539\/(29539+38074))*100)\nprint('male NO    :', (38074\/(38074+29539))*100)\nprint('female YES :', (27347\/(22962+27347))*100)\nprint('male YES   :', (22963\/(27347+22962))*100)","3aaccc29":"flight_gender = flight[flight['gender'] == 'None'] # filtering DataFrame untuk kolom fitur Gender == None\nflight_gender.account_id.unique()","18953ff5":"a1 = flight[flight['account_id'] == 'eaa8ec58eb416c13dd6f9d53ff88b2f2'] # 5 kali perjalanan tanpa promo M\na2 = flight[flight['account_id'] == '044eb7e13934d8dda219a7bc297e907d'] # 12 kali perjalanan            F\na3 = flight[flight['account_id'] == 'af59e51233e2da3877e9b520aae7cfbb'] # 3 kali perjalanan             F\na4 = flight[flight['account_id'] == '02789604494e3f45ebe767d538ff4fbc'] # 2 kali perjalanan             F\na5 = flight[flight['account_id'] == 'bbcc88153dbac780d0630f8ab4b39708'] # 1 kali perjalanan             F *terdapat data dengan price, is_tx_promo yang sama \na6 = flight[flight['account_id'] == 'd9313670821c35b4f774dd482edc37ea'] # 1 kali perjalanan             M","0447478a":"for i in flight.loc[flight['account_id'] == 'eaa8ec58eb416c13dd6f9d53ff88b2f2'].index :\n    flight['gender'][i] = 'M'\n\nfor i in flight.loc[flight['account_id'] == '044eb7e13934d8dda219a7bc297e907d'].index :\n    flight['gender'][i] = 'F'\n    \nfor i in flight.loc[flight['account_id'] == 'af59e51233e2da3877e9b520aae7cfbb'].index :\n    flight['gender'][i] = 'F'\n\nfor i in flight.loc[flight['account_id'] == '02789604494e3f45ebe767d538ff4fbc'].index :\n    flight['gender'][i] = 'F'\n\nfor i in flight.loc[flight['account_id'] == 'bbcc88153dbac780d0630f8ab4b39708'].index :\n    flight['gender'][i] = 'F'\n    \nfor i in flight.loc[flight['account_id'] == 'd9313670821c35b4f774dd482edc37ea'].index :\n    flight['gender'][i] = 'M'","818d0d3b":"flight.gender.value_counts()","e778ee8a":"# Membuat list Feature Target\niscrosssell = list()\nfor i in range(len(flight)):\n    if flight.iloc[i,]['hotel_id'] == 'None':\n        iscrosssell.append('no')\n    else:\n        iscrosssell.append('yes')\n        \n# Membuat kolom baru di flight dataframe\nflight['is_cross_sell'] = pd.Series(iscrosssell)\nflight.is_cross_sell.replace(['yes', 'no'], [1,0], inplace = True)\n","64b65f5b":"data_awal = pd.concat([flight, test])","7f7e19fe":"# 1 --> YES\n# 0 --> NO\ndata_awal.is_tx_promo.replace(['YES', 'NO'], [1,0], inplace = True)\n    \n# 1 --> Male\n# 0 --> Female\ndata_awal.gender.replace(['M','F'], [1,0], inplace=True)\n\n# 1 --> Economy\n# 0 --> Business\ndata_awal.service_class.replace(['ECONOMY','BUSINESS'], [1,0], inplace=True)\n\n# 0 --> trip\n# 1 --> roundtrip\n# 2 --> round\ndata_awal.trip.replace(['trip','roundtrip','round'], [0,1,2], inplace=True)","695d2c64":"# Menyimpan dataframe sebelum dirubah nama maskapai\ndata_train = data_awal\n# Mengubah nama maskapai\ndata_awal.airlines_name.replace(['6c483c0812c96f8ec43bb0ff76eaf716','33199710eb822fbcfd0dc793f4788d30', '0a102015e48c1f68e121acc99fca9a05',\n                             'ad5bef60d81ea077018f4d50b813153a', '74c5549aa99d55280a896ea50068a211','e35de6a36d385711a660c72c0286154a'\n                                ,'9855a1d3de1c46526dde37c5d6fb758c','6872b49542519aea7ae146e23fab5c08'],\n                            [1,2,3,4,5,6,7,8], inplace =True)","5d8954e7":"flight = data_awal.iloc[:117946]\ntest = data_awal.iloc[117946:]","0dfee253":"flight.shape, test.shape","612543f4":"# Mencari account_id yang sama di dataset flight dan test.\nkolom_account_id = list()\n\nfor i in test.account_id.unique():\n    if i in flight.account_id.unique():\n        kolom_account_id.append(i)\n        \n# Mengubah list ke dalam bentuk dataframe        \nkolom_account_id = pd.DataFrame(pd.Series(kolom_account_id))\nkolom_account_id.columns = ['Perpotongan_Akun']\nkolom_account_id.head()","79bb2d27":"# Membuat DataFrame Data_flight\nData_flight = list()\nfor i in kolom_account_id.Perpotongan_Akun:\n    for i in flight.loc[flight.account_id == i].index:\n        Data_flight.append(tuple(flight.iloc[i]))\nData_flight = pd.DataFrame(Data_flight, columns = flight.columns)","7901a3dd":"# Membuat DataFrame Data_flight_no\nData_flight_no = flight.copy()\nfor i in kolom_account_id.Perpotongan_Akun:\n    Data_flight_no = Data_flight_no[Data_flight_no['account_id'] != i]","03f0eebd":"# Membuat DataFrame Data_test\nData_test = list()\ntest_ = test.reset_index()\ntest_ = test_.drop(columns='index')\nfor i in kolom_account_id.Perpotongan_Akun:\n    for i in test_.loc[test_.account_id == i].index:\n        Data_test.append(tuple(test_.iloc[i]))\nData_test = pd.DataFrame(Data_test, columns = test_.columns)","939ffed4":"# Membuat DataFrame Data_test_no\nData_test_no = test_.copy()\nfor i in kolom_account_id.Perpotongan_Akun:\n    Data_test_no = Data_test_no[Data_test_no['account_id'] != i]","efe4b48d":"# Melihat Ukuran Baris dan Kolom dari setiap dataframe\nData_flight.shape, Data_flight_no.shape, Data_test.shape, Data_test_no.shape","22e7f49a":"# Melihat apakah ada account_id yang sama atau tidak sama pada ke-4 dataframe\nlen(Data_flight.account_id.unique()), len(Data_flight_no.account_id.unique()), len(Data_test.account_id.unique()), len(Data_test_no.account_id.unique())","a886f0f7":"fig, ax = plt.subplots(2,4 , figsize= (12,8))\n\nsns.distplot(Data_flight.price, ax = ax[0,0])\nax[0,0].set_title('Distribusi Data_flight')\n\nsns.distplot(Data_flight_no.price, ax = ax[0,1])\nax[0,1].set_title('Distribusi Data_flight_no')\n\n\nsns.distplot(Data_test.price, ax = ax[0,2])\nax[0,2].set_title('Distribusi Data_test ')\n\n\nsns.distplot(Data_test_no.price, ax = ax[0,3])\nax[0,3].set_title('Distribusi Data_test_no')\n\n\nsns.distplot(Data_flight.member_duration_days, ax = ax[1,0])\nax[1,0].set_title('Distribusi Data_flight')\n\nsns.distplot(Data_flight_no.member_duration_days, ax = ax[1,1])\nax[1,1].set_title('Distribusi Data_flight_no')\n\n\nsns.distplot(Data_test.member_duration_days, ax = ax[1,2])\nax[1,2].set_title('Distribusi Data_test ')\n\n\nsns.distplot(Data_test_no.member_duration_days, ax = ax[1,3])\nax[1,3].set_title('Distribusi Data_test_no')\n\n\n\nplt.tight_layout()\nplt.show()","3806741f":"Data_flight.price.median(), Data_flight_no.price.median(),  Data_test.price.median(), Data_test_no.price.median()","56abda57":"fig, ax = plt.subplots(2,4 , figsize= (15,10))\n\nsns.countplot(Data_flight.is_tx_promo, ax = ax[0,0])\nax[0,0].set_title('Distribusi Data_flight')\n\nsns.countplot(Data_flight_no.is_tx_promo ,ax = ax[0,1])\nax[0,1].set_title('Distribusi Data_flight_no')\n\n\nsns.countplot(Data_test.is_tx_promo, ax = ax[0,2])\nax[0,2].set_title('Distribusi Data_test ')\n\n\nsns.countplot(Data_test_no.is_tx_promo, ax = ax[0,3])\nax[0,3].set_title('Distribusi Data_test_no')\n\n\nsns.countplot(Data_flight.no_of_seats, ax = ax[1,0])\nax[1,0].set_title('Distribusi Data_flight')\n\nsns.countplot(Data_flight_no.no_of_seats, ax = ax[1,1])\nax[1,1].set_title('Distribusi Data_flight_no')\n\n\nsns.countplot(Data_test.no_of_seats, ax = ax[1,2])\nax[1,2].set_title('Distribusi Data_test ')\n\n\nsns.countplot(Data_test_no.no_of_seats, ax = ax[1,3])\nax[1,3].set_title('Distribusi Data_test_no')\n\n\n\nplt.tight_layout()\nplt.show()","0dfb3073":"fig, ax = plt.subplots(2,4 , figsize= (15,10))\n\nsns.countplot(Data_flight.airlines_name, ax = ax[0,0])\nax[0,0].set_title('Distribusi Data_flight')\n\nsns.countplot(Data_flight_no.airlines_name ,ax = ax[0,1])\nax[0,1].set_title('Distribusi Data_flight_no')\n\n\nsns.countplot(Data_test.airlines_name, ax = ax[0,2])\nax[0,2].set_title('Distribusi Data_test ')\n\n\nsns.countplot(Data_test_no.airlines_name, ax = ax[0,3])\nax[0,3].set_title('Distribusi Data_test_no')\n\n\nsns.countplot(Data_flight.trip, ax = ax[1,0])\nax[1,0].set_title('Distribusi Data_flight')\n\nsns.countplot(Data_flight_no.trip, ax = ax[1,1])\nax[1,1].set_title('Distribusi Data_flight_no')\n\n\nsns.countplot(Data_test.trip, ax = ax[1,2])\nax[1,2].set_title('Distribusi Data_test ')\n\n\nsns.countplot(Data_test_no.trip, ax = ax[1,3])\nax[1,3].set_title('Distribusi Data_test_no')\n\n\n\nplt.tight_layout()\nplt.show()","6ccdb90e":"fig, ax = plt.subplots(2,2 , figsize= (20,20))\n\n\nsns.countplot(x = 'is_cross_sell' , hue = 'gender', data= data_train, ax = ax[0,0])\nsns.countplot(x = 'is_cross_sell', hue = 'trip', data = data_train, ax = ax[0,1])\nsns.countplot(x = 'is_cross_sell', hue = 'service_class', data = data_train, ax = ax[1,0])\nsns.countplot(x = 'is_cross_sell', hue = 'is_tx_promo', data = data_train, ax = ax[1,1])","390eea8c":"fig, ax = plt.subplots(2 ,2 , figsize = (20, 20))\n\nsns.countplot(x = 'is_cross_sell', hue = 'no_of_seats', data = data_train, ax = ax[0,0])\nsns.countplot(x = 'is_cross_sell', hue = 'route', data = data_train, ax = ax[0,1])\nsns.countplot(x ='is_cross_sell', hue = 'visited_city', data = data_train , ax = ax[1,0])","ddb20b0c":"fig, ax = plt.subplots(2,2 , figsize = (20,20))\n\n\nsns.distplot(data_train['price'], ax = ax[0,0])\nsns.distplot(data_train['member_duration_days'], ax = ax[0,1])\nsns.countplot(x = 'airlines_name', data = data_train, ax = ax[1,0])\nsns.countplot(x = 'airlines_name', hue = 'is_cross_sell', data = data_train, ax = ax[1,1])","c5adf115":"data_train['visited'] = data_train['visited_city'].apply(lambda x : x.split(\"[\")[1])\ndata_train['visited'] = data_train['visited'].apply(lambda x : x.split(\"]\")[0])\ndata_train['con_visited'] = data_train['visited'].apply(lambda x : x.count(\",\") + 1)","4fb0302a":"data_train['con_log'] = data_train['log_transaction'].apply(lambda x : x.count(\",\") +1)","cbe91ab4":"data = data_train[['account_id', 'order_id']]\ndata_group = data['account_id'].value_counts().index.to_frame()\ndata_group['jumlah_transaksi'] = data['account_id'].value_counts().values\ndata_group = data_group.rename(columns = {0:'account_id'})\ndata_train = pd.merge(data_train, data_group , how = 'left', on= 'account_id')","c0aa7e69":"\ndata_train['kota1'] = data_train['visited'].apply(lambda x : x.split(\",\")[0])\ndata_train['kota2'] = data_train['visited'].apply(lambda x : x.split(\",\")[1])\ndata_train['kota3'] = data_train['visited'].apply(lambda x : x.split(\",\")[2])\n\ndata_train['kota4'] = 'None'\nfor i in range(len(data_train)):\n    try :\n        data_train['kota4'][i] = data_train['visited'][i].split(\",\")[3]\n    except:\n        continue\n    \n\ndata_train['kota5'] = 'None'\nfor i in range(len(data_train)):\n    try:\n        data_train['kota5'][i] = data_train['visited'][i].split(\",\")[4]\n    except:\n        continue\n    \n\n    \nimport re\n\nfor i in range(len(data_train)):\n    data_train['kota1'][i] = \"\".join(re.findall(\"[a-zA-Z]\", data_train['kota1'][i]))    \n    data_train['kota2'][i] = \"\".join(re.findall(\"[a-zA-Z]\", data_train['kota2'][i]))\n    data_train['kota3'][i] = \"\".join(re.findall(\"[a-zA-Z]\", data_train['kota3'][i]))\n    data_train['kota4'][i] = \"\".join(re.findall(\"[a-zA-Z]\", data_train['kota4'][i]))\n    data_train['kota5'][i] = \"\".join(re.findall(\"[a-zA-Z]\", data_train['kota5'][i]))\n\n\ndata_train['Semarang'] = 0 \nfor i in range(len(data_train)):\n    if data_train['kota1'][i] == 'Semarang':\n        data_train['Semarang'][i] = 1\n    elif data_train['kota2'][i] == 'Semarang':\n        data_train['Semarang'][i] = 1\n    elif data_train['kota3'][i] == 'Semarang':\n        data_train['Semarang'][i] = 1\n    elif data_train['kota4'][i] == 'Semarang':\n        data_train['Semarang'][i] = 1  \n    elif data_train['kota5'][i] == 'Semarang':\n        data_train['Semarang'][i] = 1\n\n\ndata_train['Jogjakarta'] = 0 \nfor i in range(len(data_train)):\n    if data_train['kota1'][i] == 'Jogjakarta':\n        data_train['Jogjakarta'][i] = 1\n    elif data_train['kota2'][i] == 'Jogjakarta':\n        data_train['Jogjakarta'][i] = 1\n    elif data_train['kota3'][i] == 'Jogjakarta':\n        data_train['Jogjakarta'][i] = 1\n    elif data_train['kota4'][i] == 'Jogjakarta':\n        data_train['Jogjakarta'][i] = 1  \n    elif data_train['kota5'][i] == 'Jogjakarta':\n        data_train['Jogjakarta'][i] = 1\n   \ndata_train['Aceh'] = 0 \nfor i in range(len(data_train)):\n    if data_train['kota1'][i] == 'Aceh':\n        data_train['Aceh'][i] = 1\n    elif data_train['kota2'][i] == 'Aceh':\n        data_train['Aceh'][i] = 1\n    elif data_train['kota3'][i] == 'Aceh':\n        data_train['Aceh'][i] = 1\n    elif data_train['kota4'][i] == 'Aceh':\n        data_train['Aceh'][i] = 1  \n    elif data_train['kota5'][i] == 'Aceh':\n        data_train['Aceh'][i] = 1\n\n","8c0f6d15":"facet = sns.FacetGrid(data_train, hue=\"is_cross_sell\",aspect=4)\nfacet.map(sns.kdeplot,'price',shade= True)\nfacet.set(xlim=(0, data_train['price'].max()))\nfacet.add_legend()\n \nplt.xlim(4000000,1000000) \n","ccaaec53":"data_train.loc[data_train['price'] <= 840000 , 'bin_price'] = 'pr_1'\ndata_train.loc[(data_train['price'] > 840000) & (data_train['price']<=1100000), 'bin_price' ] = 'pr_2'\ndata_train.loc[(data_train['price'] >1100000)& (data_train['price']<1350000), 'bin_price']= 'pr_3'\ndata_train.loc[(data_train['price'] > 1350000)& (data_train['price']<1760000),'bin_price']= 'pr_4'\ndata_train.loc[(data_train['price']>1760000), 'bin_price'] = 'pr_5'","63b5aadf":"facet = sns.FacetGrid(data_train, hue=\"is_cross_sell\",aspect=4)\nfacet.map(sns.kdeplot,'member_duration_days',shade= True)\nfacet.set(xlim=(0, data_train['member_duration_days'].max()))\nfacet.add_legend()\n ","57256bfc":"data_train.loc[data_train['member_duration_days'] <= 340, \"bin_member_duration_days\"] = 'duration_1',\ndata_train.loc[(data_train['member_duration_days'] > 340) & (data_train['member_duration_days'] <= 735), \"bin_member_duration_days\" ] = 'duration_2',\ndata_train.loc[data_train['member_duration_days'] > 735, \"bin_member_duration_days\"] = 'duration_3',","5381bdf4":"def _kurtosis(x):\n    return kurtosis(x)\n\ndef CPT5(x):\n    den = len(x)*np.exp(np.std(x))\n    return sum(np.exp(x))\/den\n\ndef skewness(x):\n    return skew(x)\n\ndef SSC(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1 \n    xn_i1 = x[0:len(x)-2]  # xn-1\n    ans = np.heaviside((xn-xn_i1)*(xn-xn_i2),0)\n    return sum(ans[1:]) \n\ndef wave_length(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1 \n    return sum(abs(xn_i2-xn))\n    \ndef norm_entropy(x):\n    tresh = 3\n    return sum(np.power(abs(x),tresh))\n\ndef SRAV(x):    \n    SRA = sum(np.sqrt(abs(x)))\n    return np.power(SRA\/len(x),2)\n\ndef mean_abs(x):\n    return sum(abs(x))\/len(x)\n\ndef zero_crossing(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1\n    return sum(np.heaviside(-xn*xn_i2,0))\n\ndef mean_change_of_abs_change(x):\n    return np.mean(np.diff(np.abs(np.diff(x))))","c98b8f3b":"# Membuat fitur berdasarkan mean,sum,median,quartile,max,min, dan beberapa statistik lainnya dari log_transaction.\n\ndata_train['log_transaction'] = data_train['log_transaction'].apply(lambda x : x.split(\"[\")[1])\ndata_train['log_transaction'] = data_train['log_transaction'].apply(lambda x : x.split(\"]\")[0])\n\ndata_train['list_log'] = data_train['log_transaction'].apply(lambda x : [float(x.split(\",\")[i]) for i in range(len(x.split(\",\")))] )\n\ndata_train['mean_log'] = data_train['list_log'].apply(lambda x : np.mean(x))\ndata_train['sum_log'] = data_train['list_log'].apply(lambda x :np.sum(x))\ndata_train['median_log'] = data_train['list_log'].apply(lambda x : np.percentile(x, 50))\ndata_train['quartile_1'] = data_train['list_log'].apply(lambda x : np.percentile(x, 25))\ndata_train['qurttile_3'] = data_train['list_log'].apply(lambda x :np.percentile(x , 75))\ndata_train['quartile_4'] = data_train['list_log'].apply(lambda x : np.percentile(x, 95))\ndata_train['max_log'] = data_train['list_log'].apply(lambda x :np.max(x))\ndata_train['min_log'] = data_train['list_log'].apply(lambda x :np.min(x))\n\n\ndata_train['range_log'] = data_train['list_log'].apply(lambda x : np.max(x) - np.min(x))\ndata_train['maxtomin_log'] = data_train['list_log'].apply(lambda x : np.max(x) \/ np.min(x))\ndata_train['mean_abs_chg_log'] = data_train['list_log'].apply( lambda x : np.mean(np.abs(np.diff(x))))\ndata_train['mean_change_of_abs_change'] = data_train['list_log'].apply(lambda x : mean_change_of_abs_change(x))\n\ndata_train['abs_max_log'] = data_train['list_log'].apply(lambda x : np.max(np.abs(x)))\ndata_train['abs_min_log'] = data_train['list_log'].apply(lambda x : np.min(np.abs(x)))\ndata_train['abs_avg_log'] = (data_train['abs_max_log'] + data_train['abs_min_log'])\/2\n\n\ndata_train['iqr_log'] =  data_train['qurttile_3'] - data_train['quartile_1'] \ndata_train['scc_log'] = data_train['list_log'].apply(lambda x : SSC(x))\ndata_train['skewness_log'] = data_train['list_log'].apply(lambda x : skewness(x))\n\ndata_train['wave_length_log'] = data_train['list_log'].apply(lambda x : wave_length(x))\ndata_train['kurtosis_log'] = data_train['list_log'].apply(lambda x : _kurtosis(x))\ndata_train['zero_crossing'] = data_train['list_log'].apply(lambda x : zero_crossing(x))","6878d880":"# Membuat fitur berdasarkan mean,sum,median,quartile,max,min, dan beberapa statistik lainnya dari price.\n\ncol = 'price'\nagg = pd.DataFrame()\n\nagg[str(col)+'_mean'] = data_train.groupby(['account_id'])[col].mean()\nagg[str(col)+'_median'] = data_train.groupby(['account_id'])[col].median()\nagg[str(col)+'_max'] = data_train.groupby(['account_id'])[col].max()\nagg[str(col)+'_min'] = data_train.groupby(['account_id'])[col].min()\nagg[str(col) + '_maxtoMin'] = agg[str(col) + '_max'] \/ agg[str(col) + '_min']\nagg[str(col) + '_abs_max'] = data_train.groupby(['account_id'])[col].apply(lambda x: np.max(np.abs(x)))\nagg[str(col) + '_abs_min'] = data_train.groupby(['account_id'])[col].apply(lambda x: np.min(np.abs(x)))\nagg[str(col) + '_abs_avg'] = (agg[col + '_abs_min'] + agg[col + '_abs_max'])\/2\nagg[str(col)+'_mad'] = data_train.groupby(['account_id'])[col].mad()\nagg[str(col)+'_q25'] = data_train.groupby(['account_id'])[col].quantile(0.25)\nagg[str(col)+'_q75'] = data_train.groupby(['account_id'])[col].quantile(0.75)\nagg[str(col)+'_q95'] = data_train.groupby(['account_id'])[col].quantile(0.95)\nagg[str(col)+'_ssc'] = data_train.groupby(['account_id'])[col].apply(SSC)\nagg[str(col)+'_mean_abs'] = data_train.groupby(['account_id'])[col].apply(mean_abs)\nagg[str(col)+'_norm_entropy'] = data_train.groupby(['account_id'])[col].apply(norm_entropy)\nagg[str(col)+'_SRAV'] = data_train.groupby(['account_id'])[col].apply(SRAV)\nagg[str(col)+'_kurtosis'] = data_train.groupby(['account_id'])[col].apply(_kurtosis)\n","70a41398":"data_train = pd.merge( data_train, agg , how = \"left\", on = \"account_id\") ","bc8d55ca":"# Membuat fitur berdasarkan mean,sum,median,quartile,max,min, dan beberapa statistik lainnya dari price.\ncol = 'member_duration_days'\nagg = pd.DataFrame()\n\nagg[str(col)+'_mean'] = data_train.groupby(['account_id'])[col].mean()\nagg[str(col)+'_median'] = data_train.groupby(['account_id'])[col].median()\nagg[str(col)+'_max'] = data_train.groupby(['account_id'])[col].max()\nagg[str(col)+'_min'] = data_train.groupby(['account_id'])[col].min()\nagg[str(col) + '_maxtoMin'] = agg[str(col) + '_max'] \/ agg[str(col) + '_min']\nagg[str(col) + '_abs_max'] = data_train.groupby(['account_id'])[col].apply(lambda x: np.max(np.abs(x)))\nagg[str(col) + '_abs_min'] = data_train.groupby(['account_id'])[col].apply(lambda x: np.min(np.abs(x)))\nagg[str(col) + '_abs_avg'] = (agg[col + '_abs_min'] + agg[col + '_abs_max'])\/2\nagg[str(col)+'_mad'] = data_train.groupby(['account_id'])[col].mad()\nagg[str(col)+'_q25'] = data_train.groupby(['account_id'])[col].quantile(0.25)\nagg[str(col)+'_q75'] = data_train.groupby(['account_id'])[col].quantile(0.75)\nagg[str(col)+'_q95'] = data_train.groupby(['account_id'])[col].quantile(0.95)\nagg[str(col)+'_ssc'] = data_train.groupby(['account_id'])[col].apply(SSC)\nagg[str(col)+'_mean_abs'] = data_train.groupby(['account_id'])[col].apply(mean_abs)\nagg[str(col)+'_norm_entropy'] = data_train.groupby(['account_id'])[col].apply(norm_entropy)\nagg[str(col)+'_SRAV'] = data_train.groupby(['account_id'])[col].apply(SRAV)\nagg[str(col)+'_kurtosis'] = data_train.groupby(['account_id'])[col].apply(_kurtosis)\n","d502c36f":"data_train = pd.merge( data_train, agg , how = \"left\", on = \"account_id\") ","771dd870":"\ndata_train['Harga_Satuan_Bangku'] = (data_train.price) \/ (data_train.no_of_seats)\ndata_train['member_duration_week'] = (data_train.member_duration_days) \/ 7\ndata_train['member_duration_month'] = (data_train.member_duration_days) \/ 30\ndata_train['member_duration_year'] = (data_train.member_duration_days) \/ 365\n\ndata_train['durasi_kunjungan'] = data_train.member_duration_days \/ data_train.con_visited","a96df805":"clustering = data_train[['order_id', 'member_duration_days', 'price']]","361a6d1f":"wcs = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=1000, n_init=20, random_state=0)\n    kmeans.fit(clustering[['price', 'member_duration_days']])\n    wcs.append(kmeans.inertia_)\nprint(wcs)","5c53bfc6":"plt.plot(range(1, 11), wcs)\nplt.title('Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","fd528f6a":"kmeans = KMeans(n_clusters=5, init='k-means++', max_iter=300, n_init=10, random_state=0)\npred_y = kmeans.fit_predict(clustering[['price', 'member_duration_days']])\nclustering['clustering_price_duration'] = pred_y.tolist()\n\nclustering['clustering_price_duration'].value_counts()","264dca2b":"wc = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n    kmeans.fit(clustering[['price']])\n    wc.append(kmeans.inertia_)\n","de20aad8":"plt.plot(range(1, 11), wc)\nplt.title('Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","dbaa27a4":"kmeans = KMeans(n_clusters=5, init='k-means++', max_iter=300, n_init=10, random_state=0)\npred_price = kmeans.fit_predict(clustering[['price']])\n\n\nclustering['clustering_price'] = pred_price.tolist()\nclustering['clustering_price'].value_counts()","0723d1ec":"wc = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n    kmeans.fit(clustering[['member_duration_days']])\n    wc.append(kmeans.inertia_)\n","8f810ae5":"plt.plot(range(1, 11), wc)\nplt.title('Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","cc0a4d4a":"kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300, n_init=10, random_state=0)\npred_price = kmeans.fit_predict(clustering[['member_duration_days']])\n\nclustering['clustering_duration'] = pred_price.tolist()\nclustering['clustering_duration'].value_counts()\n\n","bdcb10d7":"# Satukan data\n\nclustering = clustering[['order_id', 'clustering_price_duration', 'clustering_duration']]\ndata_train = pd.merge(data_train, clustering, how = 'left', on = 'order_id')\ndata_train.head()","c9c57561":"data_train['real_female'] = 0\ndata_train.loc[(data_train['gender'] == 0)  & (data_train['is_tx_promo'] == 1), 'real_female'] = 1\n\ndata_train['real_female'].value_counts()","3540d0cd":"data_train['rich_female'] = 0\ndata_train.loc[(data_train['gender'] == 0 ) &(data_train['service_class'] == 0), 'rich_female'] = 1\n\ndata_train.rich_female.value_counts()","63c309b1":"data_train['rich_male'] = 0\ndata_train.loc[(data_train['gender']  == 1 ) & (data_train['service_class'] == 0 ), 'rich_male'] = 1\n\ndata_train.rich_male.value_counts()","48c6cfc9":"data_train['rich_trip'] = 0\ndata_train.loc[(data_train['trip_trip']  == 1 ) & (data_train['service_class'] == 0 ), 'rich_trip'] = 1\n\ndata_train.rich_trip.value_counts()","9f314abe":"data_train['rich_roundtrip'] = 0 \ndata_train.loc[(data_train['trip_roundtrip']  == 1 ) & (data_train['service_class'] == 0 ), 'rich_roundtrip'] = 1\n\ndata_train.rich_roundtrip.value_counts()","b1a057a7":"data_train.head()","355394e8":"## feature indvidu , pasangan , keluarga\ndata_train.loc[data_train['no_of_seats'] == 1 , 'berangkat_dengan'] = 'individu'\ndata_train.loc[data_train['no_of_seats'] == 2 , 'berangkat_dengan'] = 'couple'\ndata_train.loc[data_train['no_of_seats'] > 2 , 'berangkat_dengan'] = 'family'","98a596f8":"facet = sns.FacetGrid(data_train, hue=\"is_cross_sell\",aspect=4)\nfacet.map(sns.kdeplot,'jumlah_transaksi',shade= True)\nfacet.set(xlim=(0, data_train['jumlah_transaksi'].max()))\nfacet.add_legend()\n","09691f87":"## feature indvidu , pasangan , keluarga\ndata_train.loc[data_train['jumlah_transaksi'] <= 25 , 'berlangganan'] = 'yes'\ndata_train.loc[ (data_train['jumlah_transaksi'] > 25) & (data_train['jumlah_transaksi'] >= 118) , 'berlangganan'] = 'no'\ndata_train.loc[ (data_train['jumlah_transaksi'] > 118) & (data_train['jumlah_transaksi'] > 132) , 'berlangganan'] = 'yes'\ndata_train.loc[data_train['jumlah_transaksi'] >=  132 , 'berlangganan'] = 'no'\n","d6a59e0d":"facet = sns.FacetGrid(data_train, hue=\"is_cross_sell\",aspect=4)\nfacet.map(sns.kdeplot,'con_log',shade= True)\nfacet.set(xlim=(0, data_train['con_log'].max()))\nfacet.add_legend()\nplt.xlim(0,30)","301d4a90":"data_train.loc[data_train['con_log'] <= 15 , 'con_log_bin'] = 'yes'\ndata_train.loc[data_train['con_log'] > 15 , 'con_log_bin'] = 'no'\n","803404db":"# Membuat dummies fitur yang tidak ordinal.\nfitur = pd.get_dummies(data_train[['trip','airlines_name','bin_price', 'bin_member_duration_days']])\nvis = pd.get_dummies(data_train['visited_city'], prefix = 'visited_city')\n\ndata_train = pd.concat([data_train, fitur], axis = 1)\ndata_train = pd.concat([data_train, vis], axis = 1)\ndata_train = data_train.drop(['visited_city'], axis = 1)\n","f7853179":"trip_with = pd.get_dummies(data_train[['berangkat_dengan', 'berlangganan', 'con_log_bin']], prefix = ['berangkat_dengan', 'berlangganan', 'con_log_bin'])\n\ndata_train = pd.concat([data_train, trip_with ], axis = 1)\ndata_train = data_train.drop(['berangkat_dengan', 'berlangganan', 'con_log_bin'], axis = 1)\n","cc308396":"col_con = data_train.columns.to_list()\nexc = []\nfor column in col_con:\n    try:\n        data_train[column] = data_train[column].astype('int64')\n    except:\n        exc.append(column)\n        \nexc","c8948a45":"data_train.head()","a9af556b":"flight__ = data_train.iloc[:117946]\ntest__ = data_train.iloc[117946:]","b32714da":"test = data_train[data_train['is_cross_sell'] == 'None']\ntrain = data_train[data_train['is_cross_sell'] != 'None']\n\ntrain['is_cross_sell'] = train['is_cross_sell'].astype('int64')\n\ny = train['is_cross_sell']\nX = train.drop(['is_cross_sell','order_id'], axis = 1)\n\nX.shape , y.shape\n","af82fdc4":"kf = KFold(n_splits=3 , random_state=42, shuffle=False)\n\nsmoter = SMOTE(random_state = 42)\n    \nX_train_cv_upsample = []\ny_train_cv_upsample = []\n    \n    \nX_val_cv = []\ny_val_cv = []\n    \n    \nfor train_fold_index , val_fold_index in kf.split(X , y):\n        \n    X_train_fold , y_train_fold = X.iloc[train_fold_index], y[train_fold_index]\n        \n    X_val_fold , y_val_fold = X.iloc[val_fold_index], y[val_fold_index]\n        \n    X_train_fold_upsample , y_train_fold_upsample = smoter.fit_resample(X_train_fold.values, y_train_fold.values)\n        \n    # masukan data train yang sudah di oversampling\n    X_train_cv_upsample.append(X_train_fold_upsample)\n    y_train_cv_upsample.append(y_train_fold_upsample)\n        \n        \n    #masukan data validation\n    X_val_cv.append(X_val_fold)\n    y_val_cv.append(y_val_fold)\n\n    \nX_train_1 = X_train_cv_upsample[0]\ny_train_1 = y_train_cv_upsample[0]\n\nX_val_1 = X_val_cv[0]\ny_val_1 = y_val_cv[0]","11125b36":"result = []\nresult_cr  = []\nresult_auc = []\n\nfor index in range(len(X_train_cv_upsample)):\n    X_train = X_train_cv_upsample[index]\n    y_train = y_train_cv_upsample[index]\n                   \n    X_val = X_val_cv[index]\n    \n    y_val = y_val_cv[index]\n                   \n    mod = ensemble.ExtraTreesClassifier(random_state = 42).fit(X_train , y_train)\n    \n    # make prediction\n    #pred = mod.predict(X_val)\n    pred_prob = mod.predict_proba(X_val)\n    \n    #control threshold \n    threshold = 0.25 # threshold we set where the probability prediction must be above this to be classified as a '1'\n    classes = pred_prob[:,1] # say it is the class in the second column you care about predictint\n    classes[classes>=threshold] = 1\n    classes[classes<threshold] = 0\n    \n    #cek \n    \n    f1 = f1_score(y_val, classes)\n    cr = classification_report(y_val , classes)\n    auc = roc_auc_score(y_val, classes)\n    \n    result.append(f1)\n    result_cr.append(cr)\n    result_auc.append(auc)","eee2d6f4":"result, np.mean(result)","ead3311b":"for cr in result_cr:\n    print(cr)","e60fd140":"print(result_auc), np.mean(result_auc)","e2a4890e":"X__ = flight__.drop(columns = ['is_cross_sell', 'order_id','account_id'])\ny_ = flight__.is_cross_sell","3caaf09a":"for train_index, test_index in skf.split(X__,y_):\n    print('Train:', train_index, 'Validation:', test_index)\n    X_train, X_test = X__.iloc[train_index], X__.iloc[test_index] \n    y_train, y_test = y_.iloc[train_index], y_.iloc[test_index]\n    \n    \n    # applying SMOTE to our data and checking the class counts\n    X_resampled_3, y_resampled_3 = SMOTE().fit_resample(X_train, y_train)\n    X_train_res = pd.DataFrame(X_resampled_3)\n    X_train_res.columns = X_train.columns\n    model.fit(X_train_res, y_resampled_3)\n    \n    y_pred = model.predict(X_test)\n    probs = model.predict_proba(X_test) # prediction on a new dataset X_new\n\n    threshold = 0.25 # threshold we set where the probability prediction must be above this to be classified as a '1'\n    classes = probs[:,1] # say it is the class in the second column you care about predictint\n    classes[classes>=threshold] = 1\n    classes[classes<threshold] = 0\n    \n    print()\n    print(f1_score(y_test, classes, average='micro'))\n    print('Report : ')\n    print(classification_report(y_test, classes))\n    print()\n    print(roc_auc_score(y_test,classes))","7755e994":"# import lightgbm as lgb\n# MLA = [\n#     #Ensemble Methods\n#     ensemble.ExtraTreesClassifier(),\n#     ensemble.GradientBoostingClassifier(),\n#     ensemble.RandomForestClassifier(),\n\n    \n#     #Navies Bayes\n#     naive_bayes.BernoulliNB(),\n#     naive_bayes.GaussianNB(),\n    \n#     #Nearest Neighbor\n#     neighbors.KNeighborsClassifier(),\n    \n    \n#     #Trees    \n#     tree.DecisionTreeClassifier(),\n#     tree.ExtraTreeClassifier(),\n  \n#     #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n#     XGBClassifier()  \n#     lgb()\n    \n#     ]\n# evaluasi = pd.DataFrame(columns = ['model'])\n\n# evaluasi['model'] = MLA\n","f697aacd":"# hasil = []\n\n# for model in MLA:\n#     # model\n#     try:\n#         mod = model.fit(X_train_1 , y_train_1)\n    \n#         # make prediction\n#         pred = mod.predict(X_val_1)\n#         f1 = f1_score(y_val_1, pred)\n#         hasil.append(f1)\n#     except:\n#         print(model)\n    \n# evaluasi['f1_score'] = hasil\n","14409a75":"#### Binning Price","968a767b":"Pada dataset flight.csv, pengisian kategori 'None' pada gender didasarkan atas is_tx_promo. Persentase wanita lebih besar dibandingkan laki - laki dalam hal penggunaan promo untuk pembelian tiket pesawat. Sehingga data dengan is_tx_promo == YES diklasifikasikan sebagai F dan data dengan is_ix promo == NO diklasifikasikan sebagai M.\n\nflight.csv belum memiliki variable target is_cross_sell. is_cross_sell == 1 ketika terdapat id pada kolom fitur hotel_id dan is_cross_sell == 0 ketika kolom fitur hotel_id bernilai 'None'.","ebc22ae9":"- Dapat dilihat pada distribusi price untuk ke-4 dataframe memiliki pola yang sama, namun pada Data_flight dan Data_flight_no memiliki harga yang melebihi 40 juta sedangkan pada Data_test serta Data_test_no tidak.\n- Distribusi member_duration_days untuk ke-4 dataframe memiliki pola yang sama.","f686c16f":"Membuar beberapa fitur, yaitu harga satuan bangku, member_duration_week, member_duration_month, member_duration_year dan perbandingan antara member_duration_days dengan jumlah total kota yang pernah dikunjungi (con_visited). Perbandingan member_duration_days dengan con_visited dilakukan untuk melihat pengaruh apakah membuat account_id yang lebih cepat cenderung untuk sering berpergian atau tidak.","ded727d7":"## Validation","4e0a2d8c":"### CHECK FLIGHT__","31eb20cf":"## Dummies Feature","5a7aa1c6":"# Modeling","b3cd7b40":"Apakah semua **account_id** di dalam dataset flight dan test sama ? atau adakah **account_id** yang tidak tercantum pada dataset flight, namun ada pada dataset test ? atau mungkin sebaliknya ?\n\nHal ini perlu dilakukan untuk melihat perilaku **account_id**. Jika ada account_id yang tidak terdapat pada dataset train, namun ada pada dataset test, kemungkinan pola dari setiap **account_id** pada dataset flight dan test berbeda.\n\nMaka dari itu, dilakukan filtering terhadap dataset flight dan test. Filtering dilakukan berdasarkan **account_id** yang sama pada dataset flight dan test.","6430f927":"Jika codingan dibawah dijalankan dibutuhkan waktu berjam-jam. Kesimpulan yang bisa didapatkan berupa model terbaik yang digunakan adalah ExtratreeClassifier().","1fb34da5":"Checking data terhadap train data flight.csv dan hotel.csv dilakukan untuk mengetahui apakah terdapat missing value (NaN) dalam kolom fitur. Checking tersebut dilakukan menggunakan method .info() dimana method ini akan memberikan output berupa jumlah data non-null dan tipe data pada setiap kolom fitur di dataset. Sementara .shape merupakan salah satu atributyang digunakan untuk mengetahui dimensi data dalam (baris, kolom).\n\nDari dua data train flight.csv dan hotel.csv tidak terdapat missing values (NaN) pada setiap kolom fitur. flight.csv memiliki dimensi data (117946, 14), yaitu sebanyak 117946 baris dan 14 kolom. hotel.csv memiliki dimensi data (2962, 6), yaitu sebanyak 2692 baris dan 6 kolom. Terdapat satu fitur kolom yang sama dari kedua dataset yaitu hotel_id.","ec92a417":"## Binning con_log","4cdfdeea":"### Grid Model","a919dd3c":"## Exploratory Data Analysis","27375b48":"## Feature Extraction ","b81e7d8f":"Dapat dilihat **account_id** yang sama pada dataset flight dan test adalah sebanyak 3905, sedangkan **account_id** pada dataset flight dan tidak tercantum pada dataset test sebanyak 70891, lalu jumlah **account_id** pada dataset test dan tidak tercantum pada dataset flight sebanyak 4819","2af4b685":"### Clustering member_duration_days","676e7dfa":"## Base Model","a7c09ec4":"## Clustering Feature Continu","81a85979":"### Clustering price and member_duration_days","d4188746":"## Binning no_of_seats","5c50071d":"### Aggregation Price","d4d00c0a":"## Binning Jumlah Transaksi","3daf04db":"\nMembuat 4 dataframe yang terdiri dari Data_flight, Data_flight_no, Data_test, Data_test_no.\n\n1. Data_flight : merupakan dataset flight yang memiliki account_id yang sama pada dataset test.\n2. Data_flight_no : merupakan dataset flight yang tidak memiliki account_id yang sama pada dataset test.\n3. Data_test : merupakan dataset test yang memiliki account_id yang sama pada dataset flight.\n4. Data_test_no : merupakan dataset test yang tidak memiliki account_id yang sama pada dataset flight.","5194e54f":"## Aggregation By account_id","930c66ec":"### Clustering Price","0e363628":"Dapat dilihat pada grafik, bahwa jumlah harga mengalami fluktuasi sehingga dapat dilakukan binning yang nantinya akan di dummies","c8465bb7":"## Data Preprocessing","7731cc22":"### Mengubah Nilai Fitur Kategorik ke dalam Angka","78a40e44":"### Visited_city\nSetiap account_id pernah berkunjung ke kota Medan, Bali, dan Jakarta. Namun, tidak semua account_id pernah berkunjung ke kota Semarang, Jogjakarta, dan Aceh. Sehingga dibuat 3 fitur yang merepresentasikan pernah atau tidak berkunjung ke-3 kota tersebut. Jika seorang customer pernah berkunjung ke-3 kota tersebut mungkin customer tersebut adalah seorang traveler sehingga cenderung untuk melakukan cross_selling.","6fdfe79e":"Selanjutnya dilakukan visualisasi data untuk melihat apakah **account_id** yang tidak sama memiliki pola dengan **account_id** yang sama ?","648de250":"### Binning member_duration_days","16990702":"### EDA FULL DATASET","7f79c9cd":"#### Total Harga Transaksi\nJika seorang customer sering berpergian berarti customer tersebut sering melakukan transaksi, sehingga customer cenderung untuk melakukan cross_selling sehingga dibuat fitur Jumlah total harga transaksi yang pernah dilakukan. ","f830e5a0":"### Aggregation member_duration_days","c1c0950b":"### FITUR STATISTIK (MEAN,MIN,MAX,QUARTILE, AND DLL)","9c818ce3":"## Feature Engineering","7048e4f6":"Setiap kolom fitur memiliki distribusi data yang berbeda. Selanjutnya, checking distribusi data pada kolom fitur dengan menggunakan ['nama fitur'].value_counts() akan memberikan output berupa jumlah distribusi pada setiap jenis data tersebut. Sementara ['nama fitur'].unique() dapat menjadi alternatif lain jika hanya ingin mengetahui banyak jenis data dalam satu kolom fitur.\n\n1. flight.csv : gender, trip, service_class, is_tx_promo, no_of_seats, airlines_name, route\n1. hotel.csv  : starRating, city, free_wifi, pool_access, free_breakfast","555c9f05":"#### Jumlah Transaksi\nMembuat fitur jumlah transaksi dari jumlah order_id yang dilakukan selama satu tahun untuk tiap account_id. Jumlah transaksi mungkin mempengaruhi terjadinya cross_selling.","e5401498":"Dari checking data, tersebut diperoleh informasi bahwa seluruh rute penerbangan adalah dari Jakarta menuju Denpasar (CGK-DPS) dengan penyedia layanan sebanyak 6 maskapai penerbangan (airlines_names). Jenis tiket penerbangan berupa trip, roundtrip, dan round (trip) dalam dua kelas, BUSINESS dan ECONOMY (service_class) yang dibeli dengan atau tanpa menggunakan promo (is_tx_promo) dengan banyak pembelian mulai dari 1 hingaa 11 kursi penumpang.","ae5d9d50":"### Binning Fitur kontinu","5a44c4fc":"Dapat dilihat pada grafik, bahwa member_duration_days mengalami fluktuasi sehingga dapat dilakukan binning yang nantinya akan di dummies","9574c983":"## Importing Data\nDataset terdiri dari 4 bagian, yaitu train data *flight.csv*, *hotel.csv*, dan test data *test.csv*. Penjelasan mengenai fitur - fitur yang terdapat dalam dataset dijelaskan dalam *Data Dictionary.pdf*. ","0ac1f5ff":"#### Total Visited_City\nMembuat fitur jumlah total kota yang pernah dikunjungi oleh customer. Dapat diasumsikan bahwa semakin banyak kota yang dikunjungi oleh customer maka customer mungkin seorang traveler sehingga cenderung untuk melakukan cross_selling."}}