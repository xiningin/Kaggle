{"cell_type":{"8736bb15":"code","29c4317e":"code","d2e54224":"code","ba592b9a":"code","ca8d18dc":"code","e43c4fe7":"code","6fb06391":"code","1f6c2406":"code","7e446b2c":"code","d8e02437":"code","4747f67d":"code","4b122c73":"code","f9912a42":"code","65c9d94a":"code","3db52897":"code","858d331f":"code","c62faa51":"code","4f829be4":"code","c6c23997":"code","7a48e7ee":"code","c2f73deb":"code","a67160c6":"code","0ee09a1e":"code","1fbb7143":"code","9b68e9c7":"code","55dc2a6c":"code","bed6d8a4":"code","95c00a0c":"code","dc39b6ba":"code","8d3f3058":"code","5a15be7b":"code","ae8153d5":"code","a1e83b85":"code","3c88b929":"code","f50cea8d":"code","37bed47f":"code","b0ec912a":"code","b1dbcbe6":"code","5021663c":"code","a6bd9e5c":"code","a0afd06e":"code","d8c6525c":"code","eb508936":"code","39b3bbaa":"code","53d0641b":"markdown","8463516d":"markdown","5d94ad06":"markdown","2976f084":"markdown","18b9ec5b":"markdown","d83e4d91":"markdown","d161fb5d":"markdown","c7c651d0":"markdown","ad8e9df3":"markdown","21e9abfb":"markdown","80820e40":"markdown","fc1a253c":"markdown","b72e41ca":"markdown","0ce1109d":"markdown","7097af12":"markdown","918a3c4f":"markdown","ef020f76":"markdown","5d7eba74":"markdown","29d5ad3b":"markdown","4df1e85b":"markdown","372154e2":"markdown","50284709":"markdown","153d5504":"markdown"},"source":{"8736bb15":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n\n# Function to perform data standardization \nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\n\n# SMOTE technique\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score \n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom mlxtend.plotting import plot_confusion_matrix\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","29c4317e":"df = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","d2e54224":"df.head()","ba592b9a":"# Last 5 rows\ndf.tail()","ca8d18dc":"df.shape","e43c4fe7":"df.columns","6fb06391":"# Checknig the types of columns\ndf.dtypes","1f6c2406":"df.info()","7e446b2c":"# Checking Null Values!\ndf.isnull().sum().max()","d8e02437":"# Checking class distribution of the classes\nprint(\"Number of No fraud transactions = \", df[df['Class'] == 0].shape[0])\nprint(\"Number of fraud transactions = \", df[df['Class'] == 1].shape[0])\nprint('No Frauds', round(df['Class'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')","4747f67d":"plt.figure(figsize = (10, 8))\ncolors = [\"yellow\", \"blue\"]\n\nsns.countplot('Class', data=df, palette=colors)\nplt.xticks(np.arange(2), ['No Fraud', 'Fraud'])\nplt.xlabel(\"Classes\", fontsize=12)\nplt.ylabel(\"Count of transaction\", fontsize=12)\nplt.title('Distribution of target variable.', fontsize=20)","4b122c73":"plt.figure(figsize = (10, 8))\n\nsns.distplot(df['Amount'].values, color='b')\nplt.xlim([min(df['Amount']), max(df['Amount'])])\nplt.title('Distribution of Transaction amount.', fontsize=20)","f9912a42":"plt.figure(figsize = (10, 8))\n\nsns.distplot(df['Time'].values, color='b')\nplt.xlim([min(df['Time']), max(df['Time'])])\nplt.title('Distribution of Transaction time.', fontsize=20)","65c9d94a":"a = StandardScaler()","3db52897":"df['Amount'] = a.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['Time'] = a.fit_transform(df['Time'].values.reshape(-1,1))","858d331f":"df.head()","c62faa51":"y = df['Class']\nX = df.drop(['Class'], axis = 1)","4f829be4":"print(\"Shape of X\", X.shape)\nprint(\"Shape of y\",y.shape)","c6c23997":"# Split X and y into train and test sets: 80-30\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)\n\nprint(\"Number transactions X_train dataset: \", X_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions X_test dataset: \", X_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)","7a48e7ee":"# Printing number of samples before oversampling\nprint(\"Before OverSampling the count of label 1 (Fraud): {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling the count of label 0 (Non Fraud): {}\".format(sum(y_train==0)))","c2f73deb":"sm = SMOTE(random_state=2)\nX_train, y_train = sm.fit_sample(X_train, y_train.ravel())","a67160c6":"# Printing number of samples after oversampling\nprint(\"After OverSampling the count of label 1 (Fraud): {}\".format(sum(y_train==1)))\nprint(\"After OverSampling the count of label 0 (Non Fraud): {}\".format(sum(y_train==0)))","0ee09a1e":"print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","1fbb7143":"df.corr()","9b68e9c7":"plt.figure(figsize=(15,10))\nsns.heatmap(df.corr(), cmap=\"Greens\")","55dc2a6c":"log_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)","bed6d8a4":"y_pred = log_reg.predict(X_test)\nacc = accuracy_score(y_test, y_pred, normalize=True) * float(100)\nprint(\"Accuracy for the logistic regression model is\", acc, \"%.\")\nprint(\"Precision for the logistic regression model is {}\".format(precision_score(y_test, y_pred)))\nprint(\"Recall for the logistic regression model is {}\".format(recall_score(y_test, y_pred)))","95c00a0c":"confusion_matrix(y_test,y_pred).T","dc39b6ba":"cm = confusion_matrix(y_test,y_pred)\nplot_confusion_matrix(cm, figsize = (10, 5 ), cmap = 'tab20c_r')\nplt.xticks(range(2), ['Non Fraud', 'Fraud'], fontsize=16, rotation = 90)\nplt.yticks(range(2), ['Non Fraud', 'Fraud'], fontsize=16)\nplt.show()","8d3f3058":"gb = GradientBoostingClassifier()\ngb.fit(X_train, y_train)","5a15be7b":"y_pred = gb.predict(X_test)\nacc = accuracy_score(y_test, y_pred, normalize=True) * float(100)\nprint(\"Accuracy for the gradient boosting classifier is\", acc, \"%.\")\nprint(\"Precision for the gradient boosting classifier is {}\".format(precision_score(y_test, y_pred)))\nprint(\"Recall for the gradient boosting classifier is {}\".format(recall_score(y_test, y_pred)))","ae8153d5":"cm = confusion_matrix(y_test,y_pred)\nplot_confusion_matrix(cm, figsize = (10, 5 ), cmap = 'tab20c_r')\nplt.xticks(range(2), ['Non Fraud', 'Fraud'], fontsize=16, rotation = 90)\nplt.yticks(range(2), ['Non Fraud', 'Fraud'], fontsize=16)\nplt.show()","a1e83b85":"rf = RandomForestClassifier()\nrf.fit(X_train, y_train)","3c88b929":"y_pred = rf.predict(X_test)\nacc = accuracy_score(y_test, y_pred, normalize=True) * float(100)\nprint(\"Accuracy for the random forest classifier is\", acc, \"%.\")\nprint(\"Precision for the random forest classifier is {}\".format(precision_score(y_test, y_pred)))\nprint(\"Recall for the random forest classifier is {}\".format(recall_score(y_test, y_pred)))","f50cea8d":"cm = confusion_matrix(y_test,y_pred)\nplot_confusion_matrix(cm, figsize = (10, 5 ), cmap = 'tab20c_r')\nplt.xticks(range(2), ['Non Fraud', 'Fraud'], fontsize=16, rotation = 90)\nplt.yticks(range(2), ['Non Fraud', 'Fraud'], fontsize=16)\nplt.show()","37bed47f":"tree = DecisionTreeClassifier(random_state=0)\ntree.fit(X_train, y_train)","b0ec912a":"y_pred = tree.predict(X_test)\nacc = accuracy_score(y_test, y_pred, normalize=True) * float(100)\nprint(\"Accuracy for the decision tree is\", acc, \"%.\")\nprint(\"Precision for the decision tree is {}\".format(precision_score(y_test, y_pred)))\nprint(\"Recall for the decision tree is {}\".format(recall_score(y_test, y_pred)))","b1dbcbe6":"cm = confusion_matrix(y_test,y_pred)\nplot_confusion_matrix(cm, figsize = (10, 5 ), cmap = 'tab20c_r')\nplt.xticks(range(2), ['Non Fraud', 'Fraud'], fontsize=16, rotation = 90)\nplt.yticks(range(2), ['Non Fraud', 'Fraud'], fontsize=16)\nplt.show()","5021663c":"knn = KNeighborsClassifier()\nknn.fit(X_train, y_train)","a6bd9e5c":"y_pred = knn.predict(X_test)\nacc = accuracy_score(y_test, y_pred, normalize=True) * float(100)\nprint(\"Accuracy for KNN is\", acc, \"%.\")\nprint(\"Precision for KNN is {}\".format(precision_score(y_test, y_pred)))\nprint(\"Recall for KNN is {}\".format(recall_score(y_test, y_pred)))","a0afd06e":"cm = confusion_matrix(y_test,y_pred)\nplot_confusion_matrix(cm, figsize = (10, 5 ), cmap = 'tab20c_r')\nplt.xticks(range(2), ['Non Fraud', 'Fraud'], fontsize=16, rotation = 90)\nplt.yticks(range(2), ['Non Fraud', 'Fraud'], fontsize=16)\nplt.show()","d8c6525c":"xgb = XGBClassifier()\nxgb.fit(X_train, y_train)","eb508936":"y_pred = xgb.predict(X_test)\nacc = accuracy_score(y_test, y_pred, normalize=True) * float(100)\nprint(\"Accuracy for XGBoost is\", acc, \"%.\")\nprint(\"Precision for XGBoost is {}\".format(precision_score(y_test, y_pred)))\nprint(\"Recall for XGBoost is {}\".format(recall_score(y_test, y_pred)))","39b3bbaa":"cm = confusion_matrix(y_test,y_pred)\nplot_confusion_matrix(cm, figsize = (10, 5 ), cmap = 'tab20c_r')\nplt.xticks(range(2), ['Non Fraud', 'Fraud'], fontsize=16, rotation = 90)\nplt.yticks(range(2), ['Non Fraud', 'Fraud'], fontsize=16)\nplt.show()","53d0641b":"#### 5. k-Nearest Neighbors","8463516d":"We have around 2.8 lakhs rows and 31 columns in the dataset.","5d94ad06":"#### Separating target feature.","2976f084":"#### Data standardization\n* In Data Standardization we perform zero mean centring and unit scaling; i.e. we make the mean of all the features as zero and the standard deviation as 1.\n* Thus we use **mean** and **standard deviation** of each feature.\n* It is very important to save the **mean** and **standard deviation** for each of the feature from the **training set**, because we use the same mean and standard deviation in the test set.\n\n\nSince our data is already scaled except amount and time. So we will only standardize these columns.","18b9ec5b":"#### 4. Decision Tree","d83e4d91":"Displaying first 5 rows.","d161fb5d":"## Exploratory Data Analysis\n#### Reading CSV file data.\n","c7c651d0":"#### 6. XGBoost","ad8e9df3":"As you can see in the dataset, the features are scaled in one range and the names of the features are not shown due to privacy reasons of banking data.\n\nPrinting the shape of the data.","21e9abfb":"As we can we have 351 labels in the Fraud class and 199K labels in the non fraud class. In the next cell we are applying the SMOTE technique to balance the number of labels.","80820e40":"#### Observations:\n\n * V17, V14, V12 and V10 are negatively correlated. Notice how the lower these values are, the more likely the end result will be a fraud transaction.\n * V2, V4, V11, and V19 are positively correlated. Notice how the higher these values are, the more likely the end result will be a fraud transaction.","fc1a253c":"#### 3. Random Forest","b72e41ca":"#### 2. Gradient Boosting","0ce1109d":"The classes are heavily skewed so we need to solve this issue later.","7097af12":"### Machine Learning Models\n\n#### 1. Logistic Regression","918a3c4f":"There is no any null value in our dataset which is good.","ef020f76":"By seeing the above plot we can see distributions are skewed for target features, we can also see further distributions of the other features. There are few techniques that can help the distributions be less skewed or to balance dataset which will be implemented in further section.","5d7eba74":"#### SMOTE for imbalanced data","29d5ad3b":"#### Columns of the dataset","4df1e85b":"#### Correlations with the imbalanced data\n* Finally, let's take a look at the relationships between numeric features and other numeric features.\n* ***Correlation*** is a value between -1 and 1 that represents how closely values for two separate features move in unison.\n* Positive correlation means that as one feature increases, the other increases; eg. a child's age and her height.\n* Negative correlation means that as one feature increases, the other decreases; eg. hours spent studying and number of parties attended.\n* Correlations near -1 or 1 indicate a strong relationship.\n* Those closer to 0 indicate a weak relationship.\n* 0 indicates no relationship.","372154e2":"So in the dataset we have all the numerical features only. So we dont have to convert any feature from categorical to numerical. ","50284709":"#### Train and validation split\n\nAs we have to developed the model to predict the sales of alcohol so we will build the regression model and accordingly also do the splitting of the data.","153d5504":"#### Importing the necessary libraries. "}}