{"cell_type":{"8821374c":"code","2dcf1f24":"code","f06261c7":"code","5ae0121d":"code","39495362":"code","b18b5d01":"code","fc4739d4":"code","8ad2e0d8":"code","2eb699fb":"code","a319104f":"code","4a3c4cdd":"code","e0de5d83":"code","a2e1d77f":"code","c495f893":"code","be4df721":"code","b86e0d90":"code","1a3f2572":"code","8b984003":"code","7418493c":"code","2dfb2a12":"code","126462c0":"code","02ca268b":"code","2cefcd66":"code","51befed2":"code","1178aee7":"code","3f0bef32":"code","60486bd9":"code","64a6ac95":"code","1b41b008":"code","4098af7e":"code","967de6f4":"code","a0f18699":"code","1f6df140":"code","259152a2":"code","37f598a4":"code","bcc60ab7":"code","f3e18a68":"code","562b41e9":"code","adffb869":"code","c5aa7ca0":"code","409f956d":"code","5ae3bacb":"code","5684a489":"code","af18ac45":"code","1d194c2d":"code","3c34c12c":"code","9167a9f1":"code","d5aa32bf":"code","e5834afc":"code","f3e0610c":"code","4c231e79":"markdown","d647f298":"markdown","cdb33484":"markdown","007291c7":"markdown","9110160b":"markdown","bdb56bfc":"markdown","966fc35d":"markdown","c59319e1":"markdown","845dc213":"markdown","b4027dcd":"markdown","2ee935af":"markdown","54648be9":"markdown","a84c3918":"markdown","8b0f0a1d":"markdown","b8823f53":"markdown","e128634c":"markdown","abf032d0":"markdown","0a52414f":"markdown","543469fa":"markdown","1f39f911":"markdown","65cda5be":"markdown","47245b89":"markdown","60d6406f":"markdown","d36796e7":"markdown","6a18ad53":"markdown","d31763e4":"markdown","9bca7517":"markdown","e250bd86":"markdown","eacdd79a":"markdown","a88cc68c":"markdown","a64ac4dc":"markdown","b14ffd1e":"markdown","29bfbc94":"markdown","2743e041":"markdown","b726ed76":"markdown","57571d63":"markdown","9604a480":"markdown","bf1f22fc":"markdown","069fd656":"markdown","bc4869b3":"markdown","1b9e7f4e":"markdown","b045226a":"markdown","ab16e5af":"markdown","6f6ff873":"markdown","af81c522":"markdown","82b81a7b":"markdown","48cf3bbe":"markdown","68219347":"markdown","d1e5a916":"markdown","fef08323":"markdown","883b648c":"markdown","676010dc":"markdown","e1272e22":"markdown","3fff80e3":"markdown","0d6fd4d0":"markdown","05b9a09a":"markdown","aaf19f2c":"markdown","0e53a879":"markdown","302978ec":"markdown","782e3e16":"markdown","407b4b71":"markdown","9d9f49e2":"markdown","da9e9487":"markdown","3ebb307f":"markdown","5c4d744b":"markdown","a8a65703":"markdown","8153a9dd":"markdown","f0775cfe":"markdown","71b2ced3":"markdown","12c23587":"markdown","49617d7a":"markdown","6a43644b":"markdown","09f503d6":"markdown","f0a7a4b2":"markdown","f8ffacf0":"markdown","b25d80f3":"markdown","7083fcba":"markdown","11dbb988":"markdown","868908a2":"markdown","4e486a7c":"markdown","4ff5577d":"markdown","64795543":"markdown","76a7a429":"markdown","8678c407":"markdown","70c8bb06":"markdown"},"source":{"8821374c":"# libraries\nimport os\nimport numpy as np \nimport pandas as pd \nfrom sklearn import preprocessing\nfrom tqdm import tqdm_notebook #gives a progress bar\nimport seaborn as sns\n\n# Matplot\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nplt.style.use('default')\nimport matplotlib.image as mpimg\n\n# NLP\/WordCloud\nimport nltk\nfrom collections import Counter\nimport regex as re\nfrom wordcloud import WordCloud\nfrom fuzzywuzzy import process, fuzz\nfrom nltk.corpus import stopwords\nimport string\n\n# spacy for basic processing, optional, can use nltk as well(lemmatisation etc.)\nimport spacy\nspacy.cli.download(\"en\")\nfrom nltk.stem import WordNetLemmatizer \n\n# Topic Modelling\n#gensim for LDA\nimport gensim\nfrom gensim import corpora, models\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel, HdpModel\n!pip install lda\nimport lda\n\n#plotting tools\nimport pyLDAvis\n#import pyLDAvis.gensim_models as gensimvis\nimport pyLDAvis.gensim_models\n\n\n#Images\nfrom PIL import Image\n\n# Additional Libraries\nimport warnings\nimport matplotlib.cbook\nwarnings.filterwarnings(\"ignore\")\nfrom scipy import stats\n\n# text\nimport textwrap\nclass color:\n   BOLD = '\\033[1m' + '\\033[93m'\n   END = '\\033[0m'","2dcf1f24":"# MakeupAlley\nMA_df = pd.read_csv('..\/input\/johnson-johnson-ogx-product-reviews\/MakeupAlley_Reviews.csv')\n\n# Ulta Beauty\nUlta_df = pd.read_csv('..\/input\/johnson-johnson-ogx-product-reviews\/Ulta_Reviews.csv');","f06261c7":"print(\"MakeUpAlley: \", MA_df.shape)\nMA_df.head()","5ae0121d":"print(\"Ulta: \", Ulta_df.shape)\nUlta_df.head()","39495362":"print('\\033[1m' + \"MakeupAlley: \"+'\\033[0m'\"\\n\", MA_df.dtypes)\n\nprint('\\033[1m' + \"\\nUlta: \"+'\\033[0m'\"\\n\", Ulta_df.dtypes)","b18b5d01":"Ulta_df['date'] = pd.to_datetime((Ulta_df['date']))\nUlta_df['date'] = Ulta_df[\"date\"].dt.strftime(\"%Y-%m\")\nUlta_df.head()","fc4739d4":"print(\"MakeupAlley: \", MA_df['product'].nunique())\n\nprint(\"Ulta: \", Ulta_df['product'].nunique())","8ad2e0d8":"MA_prods = MA_df['product'].unique()\nprod_df = Ulta_df[Ulta_df['product'].isin(MA_prods)]\nprod_both = prod_df['product'].unique()\nprint(prod_both[:20])","2eb699fb":"MA_prod = MA_df['product'].unique().tolist()\nsorted(MA_prod)[:10]","a319104f":"Ulta_prod = Ulta_df['product'].unique().tolist()\nsorted(Ulta_prod[:10])","4a3c4cdd":"print(\"MakeupAlley: \", MA_df['review'].nunique())\n\nprint(\"Ulta: \", Ulta_df['review'].nunique())","e0de5d83":"print(\"Ulta Shape:\", Ulta_df.shape)\n\ndup = Ulta_df.copy()\ndup['review'] = dup.duplicated(subset='review')\ndup[[\"date\", \"rating\",\"product\"]] = False\n\nplt.figure(figsize = (16, 10))\nsns.heatmap(dup,xticklabels=True)\nplt.title(\"Duplicate values in Ulta Data\", size=20);","a2e1d77f":"Ulta_df.drop_duplicates(inplace=True)\n\nprint(Ulta_df.shape)","c495f893":"# FuzzyWuzzy\nwuzzy_score = [(x,) + i\n             for x in Ulta_prod \n             for i in process.extract(x, Ulta_prod, \n                                      scorer=fuzz.token_sort_ratio)]\n\nwuzzy_df = pd.DataFrame(wuzzy_score, columns=['Ulta_prod','similar','fuzzy_score'])\n\n\n# create a new column to get rid of reverse duplicates\nwuzzy_df['sorted'] = np.minimum(wuzzy_df['Ulta_prod'], wuzzy_df['similar'])\n\nwuzzy_df = wuzzy_df[(wuzzy_df['fuzzy_score'] >= 83) &\n                    (wuzzy_df['Ulta_prod'] != wuzzy_df['similar'])&\n                    (wuzzy_df['sorted'] != wuzzy_df['similar'])\n                   ] \n\nwuzzy_high_score = wuzzy_df.drop('sorted',axis=1).copy()\nprint(wuzzy_high_score.shape)\nwuzzy_high_score.head(20)","be4df721":"name_replace = dict(zip(wuzzy_high_score['Ulta_prod'], wuzzy_high_score['similar']))\n\nfor row in Ulta_df['product']:\n    for key, value in name_replace.items():\n        if row == key:\n            Ulta_df['product'].replace(row, value, inplace=True)\n            \nprint(Ulta_df.shape)\nUlta_df.head()","b86e0d90":"# Product name being replaced\nmy_check = Ulta_df[Ulta_df['product']=='Renewing + Argan Oil of Morocco Extra Penetrating Oil']\nprint(my_check.shape)\nmy_check.head()","1a3f2572":"Ulta_prods = Ulta_df['product'].unique().tolist()\n\nfuzzy_score = [(x,) + i\n             for x in Ulta_prods \n             for i in process.extract(x, MA_prod, \n                                      scorer=fuzz.token_sort_ratio)]\n\nfuzzy_df = pd.DataFrame(fuzzy_score, columns=['Ulta_prod','MA_prod','fuzzy_score'])\n\nfuzzy_df = fuzzy_df[(fuzzy_df['fuzzy_score'] >= 80) &\n                    (fuzzy_df['Ulta_prod'] != fuzzy_df['MA_prod'])\n                   ]\n\n# replace names\ndata_replace = dict(zip(fuzzy_df['Ulta_prod'], fuzzy_df['MA_prod']))\n\nfor i in MA_df['product']:\n    for key, value in data_replace.items():\n        if i == value:\n            MA_df['product'].replace(i, key, inplace=True)\n            \nprint(MA_df.shape)\nMA_df.head()","8b984003":"frames = [Ulta_df, MA_df]\nOGX_df = pd.concat(frames)\n\nprint(OGX_df.shape)","7418493c":"OGX_df.drop_duplicates(inplace=True)\nprint(OGX_df.shape)","2dfb2a12":"dup_df = OGX_df.groupby(['date','rating','review'],as_index=False).agg({'product':','.join})\nprint(dup_df.shape)","126462c0":"#dup_df = OGX_df.groupby(['date','rating','review'],as_index=False).agg({'product':','.join})\n\ndup = dup_df['product'].value_counts()\ndup = dup.sort_values(ascending=False).reset_index()\ndup = pd.DataFrame(dup).rename(columns={\"index\": \"product\", 'product': \"count\"})\n#print(dup.shape)\ndup.head(10)","02ca268b":"MA_dist = dup_df['date'].value_counts()\nMA_dist = MA_dist.sort_index().reset_index()\nMA_dist = pd.DataFrame(MA_dist).rename(columns={\"index\": \"dates\", 'date': \"count\"})\n\ntickvalues = range(0,len(MA_dist['dates']))\nlabels = MA_dist['dates'].to_list()\n\n# plot\nMA_dist.plot(kind='bar', x='dates',y='count',color='gold',figsize=(14,9))\nplt.xlabel(\"Year\", fontsize=12)\nplt.xticks(ticks = tickvalues[::9] ,labels = labels[::9],rotation=45)\nplt.ylabel(\"# of Reviews\", fontsize=14)\nplt.title(\"OGX Reviews Distribution Over Time\", fontsize=20)\nplt.style.use('default');","2cefcd66":"# distribution of ratings\nrate_dist = dup_df['rating'].value_counts()\nrate_dist = rate_dist.sort_index()\n\n#plot\nplt.subplots(figsize=(12,10))\n#MA_df.rating.value_counts().plot(kind='bar', color='gold')\nrate_dist.plot(kind='bar', color='gold')\nplt.xlabel(\"Rating\", fontsize=16)\nplt.ylabel(\"# of Reviews\", fontsize=16)\nplt.title(\"OGX Review Ratings\", fontsize=20);","51befed2":"MA_rate = pd.DataFrame({'count' : dup_df.groupby(\"rating\")['date'].value_counts()}).reset_index()\n\ntrain_df = MA_rate.pivot_table(index=['date'], columns=['rating'], values='count')\ntrain_df = train_df.reset_index()\n\ntrain_df['date'] = pd.to_datetime(train_df['date'])\ntrain_df['date'] = train_df['date'].dt.strftime(\"%Y\")\nrate_sum = train_df.groupby('date').sum().reset_index()\n\nrate_sum.plot(x='date', kind='bar', stacked=True, figsize = (13,8))\nplt.xlabel(\"Year\", fontsize=15)\n#plt.xticks(ticks = tick[::8] ,labels = labl[::8],rotation=45)\nplt.ylabel(\"Ratings\", fontsize=15)\nplt.title(\"OGX Ratings Distributions Over Time\", fontsize=20);","1178aee7":"Top_Size = dup.loc[0:30]\nx = Top_Size['count']\nlabel = Top_Size['product']\n\n#plt.subplots(figsize=(14,7))\nTop_Size.plot(kind='barh', color='blue',figsize=(13,9))\n\nfor i, v in enumerate(x):\n    plt.text(v, i, v,fontweight='bold',\n            Bbox = dict(facecolor = 'green', alpha =.2))\nplt.xlabel(\"Number of Reviews\", fontsize=14)\nplt.ylabel(\"Product\", fontsize=14)\n#plt.yticks(ticks=x,labels=label)\nplt.gca().invert_yaxis()\nplt.title(\"Number of Reviews per Product\", fontsize=20);","3f0bef32":"y_df = dup_df[dup_df['rating'] < 3]\n\nnew_df = pd.DataFrame({'bad_reviews' : y_df.groupby('product')['rating'].count().sort_values(ascending=False)}).reset_index()\n\n# combine\nBad_Ratings_per_product = pd.merge(dup, new_df, on='product', how='left')\nBad_Ratings_per_product['ratio'] = (Bad_Ratings_per_product['bad_reviews']\/Bad_Ratings_per_product['count'])\nBad_Ratings_per_product.head()","60486bd9":"def tokenit(data):\n\n    # the \\w+ numeric will find all words, meaning any sequence of letters without a space\n    tokens = re.findall('\\w+', data)\n    \n    # take out some of these common and useless words to get a better understanding of topics\n    common_words = ['i','I','shampoo','conditioner','product','hair','quot','it','this','the']\n    \n    #remove stop words\n    stop_words = nltk.corpus.stopwords.words('english') + list(string.punctuation)+common_words\n\n    best_rev = []\n    for word in tokens:\n        if word not in stop_words:\n            best_rev.append(word)\n        \n    # lowercase\n    rev_words = []\n    for w in best_rev:\n        rev_words.append(w.lower())\n        \n    return rev_words\n\n\n#make list of tokenized reviews\ntok_revs = []\nfor row in dup_df['review']:\n    toks = tokenit(row)\n    tok_revs.append(toks) \n    \n    \n\n# Remove certain words\ndef lose_words(data):\n    \n    no_words = ['it','this','the','like']\n    \n    best_rev = []\n    for word in data:\n        if word not in no_words:\n            best_rev.append(word)\n            \n    return best_rev\n\n\nnew_toks=[]\nfor i in tok_revs:\n    tok = lose_words(i)\n    new_toks.append(tok)","64a6ac95":"# add list as column in dataset\ndup_df['token_reviews'] = new_toks\n\n# lemmatize\ndef lemmatize_text(text):\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(w) for w in text]\n\ndup_df['lemmatized_reviews'] = dup_df.token_reviews.apply(lemmatize_text)\n\ndup_df.head()","1b41b008":"dup_baddf = dup_df[dup_df['rating']<4]\n\n# create dictionary and corpus\nid2word=corpora.Dictionary(dup_baddf['lemmatized_reviews'])\n\n#create corpus\ncorpus=[id2word.doc2bow(text) for text in dup_baddf['lemmatized_reviews']]\n","4098af7e":"#model\nhdpmodel = HdpModel(corpus=corpus, id2word=id2word)\nhdptopics = hdpmodel.show_topics(formatted=False)\nlen(hdptopics)","967de6f4":"# Build LDA model\nlda_model= gensim.models.ldamodel.LdaModel(corpus=corpus,id2word=id2word,num_topics=20,random_state=500,\\\n                                          update_every=1,chunksize=100,passes=10,alpha='auto',per_word_topics=True)\n","a0f18699":"from IPython.core.display import display, HTML\n\ndisplay(HTML(\"<style>.container { max-width:100% !important; }<\/style>\"))\ndisplay(HTML(\"<style>.output_result { max-width:100% !important; }<\/style>\"))\ndisplay(HTML(\"<style>.output_area { max-width:100% !important; }<\/style>\"))\ndisplay(HTML(\"<style>.input_area { max-width:100% !important; }<\/style>\"))\n\n# visulaise the topics\npyLDAvis.enable_notebook()\nvis=pyLDAvis.gensim_models.prepare(lda_model,corpus,id2word, sort_topics=False)\n#vis\npyLDAvis.display(vis)","1f6df140":"rows=5\ncols=4\n\nfig, ax = plt.subplots(rows, cols, figsize=(10.5,8.5))\n\nrow=0 \ncol=0 \n\nfor t in range(lda_model.num_topics):\n    word_p_list = lda_model.show_topic(t)\n    topic_word_dict = {p[0]:p[1] for p in word_p_list}\n    ax[row][col].imshow(WordCloud(background_color='white',max_words = 100).fit_words(topic_word_dict))\n    ax[row][col].set_title(\"Topic #\" + str(t+1))\n    ax[row][col].xaxis.set_visible(False)\n    ax[row][col].yaxis.set_visible(False)\n    row=row+1\n    if row==rows:\n        row=0\n        col=col+1\n    plt.axis(\"off\")\n    plt.tight_layout()\n    plt.grid(b=None)\n\nplt.show() ","259152a2":"MA_bdf = dup_df[dup_df['rating']<4]","37f598a4":"hair_loss = MA_bdf[MA_bdf['review'].str.contains(r'^(?=.*hair)(?=.*\\bloss\\b)') | MA_bdf['review'].str.contains(r'^(?=.*\\blosing\\b)(?=.*hair)')]\nhair_damage = MA_bdf[MA_bdf['review'].str.contains(r'^(?=.*hair)(?=.*damag)') | MA_bdf['review'].str.contains(r'^(?=.*hair)(?=.*strip)')]\nbrittle_hair = MA_bdf[MA_bdf['review'].str.contains(r'^(?=.*hair)(?=.*brittle)') | MA_bdf['review'].str.contains(r'^(?=.*hair)(?=.*straw)')]\nhair_lose = MA_bdf[MA_bdf['review'].str.contains(r'^(?=.*\\bhair\\b)(?=.*\\blose\\b)') | MA_bdf['review'].str.contains(r'^(?=.*\\bhair\\b)(?=.*\\blost\\b)')]\nhair_frizz = MA_bdf[MA_bdf['review'].str.contains(r'^(?=.*hair)(?=.*frizz)') | MA_bdf['review'].str.contains(r'^(?=.*hair)(?=.*frizz)')]","bcc60ab7":"# put dataframes in list\nhair_list = [hair_loss, \nhair_damage,\nbrittle_hair,\nhair_lose,\nhair_frizz]\n\n# concatenate lists\nhair_df = pd.concat(hair_list)\nhair_df.reset_index(inplace=True)\nhair_df = hair_df.drop('index', axis=1)\n\n\nnew_hair = hair_df.drop_duplicates(subset=['review'])\nnew_hair.reset_index(inplace=True)\nnew_hair = new_hair.drop('index', axis=1)\n#print(new_hair.shape)","f3e18a68":"MA_neghair = pd.DataFrame({'count' : new_hair[\"product\"].value_counts()}).reset_index()\nprint(MA_neghair.shape)\nMA_neghair.head()","562b41e9":"scalp_damage = MA_bdf[MA_bdf['review'].str.contains(r'^(?=.*scalp)(?=.*itch)') | MA_bdf['review'].str.contains(r'^(?=.*skin)(?=.*irrita)')]\nskin_damage = MA_bdf[MA_bdf['review'].str.contains(r'^(?=.*skin)(?=.*itch)') | MA_bdf['review'].str.contains(r'^(?=.*scalp)(?=.*irrita)')]\nburn_damage = MA_bdf[MA_bdf['review'].str.contains(r'^(?=.*skin)(?=.*burn)') | MA_bdf['review'].str.contains(r'^(?=.*scalp)(?=.*burn)')]\nallergy_damage = MA_bdf[MA_bdf['review'].str.contains(r'^(?=.*aller)(?=.*reaction)') | MA_bdf['review'].str.contains(r'^(?=.*skin)(?=.*react)')]\nsore_damage = MA_bdf[MA_bdf['review'].str.contains(r'^(?=.*scalp)(?=.*sore)') | MA_bdf['review'].str.contains(r'^(?=.*skin)(?=.*sore)')]\ndry_damage = MA_bdf[MA_bdf['review'].str.contains(r'^(?=.*scalp)(?=.*dry)') | MA_bdf['review'].str.contains(r'^(?=.*skin)(?=.*dry)')]","adffb869":"# put dataframes in list\nskin_list = [scalp_damage,\nskin_damage,\nburn_damage,\nallergy_damage,\nsore_damage,\ndry_damage]\n\n# concatenate lists\nskin_df = pd.concat(skin_list)\nskin_df.reset_index(inplace=True)\nskin_df = skin_df.drop('index', axis=1)\n#print(skin_df.shape)\n\nnew_skin = skin_df.drop_duplicates(subset=['review'])\nnew_skin.reset_index(inplace=True)\nnew_skin = new_skin.drop('index', axis=1)\n#print(new_skin.shape)\n#new_skin.head()","c5aa7ca0":"print(new_skin.shape)\nnew_skin.head()","409f956d":"MAskin_neg = pd.DataFrame({'count' : new_skin[\"product\"].value_counts()}).reset_index()\nprint(MAskin_neg.shape)\nMAskin_neg.head()","5ae3bacb":"def graph_areas(data):\n    # the data stands for prod_1\n    # hair damage\n    Top_prod1 = new_hair[new_hair['product']== data].reset_index()\n    Top_prod1 = Top_prod1.drop(columns='index')\n    Top_prod1['date'] = pd.to_datetime(Top_prod1['date'])\n    Top_prod1['date'] = Top_prod1['date'].dt.strftime(\"%Y\")\n    Hair_rev = Top_prod1['review']\n    # skin damage\n    Top_prod2 = new_skin[new_skin['product']== data].reset_index()\n    Top_prod2 = Top_prod2.drop(columns='index')\n    Top_prod2['date'] = pd.to_datetime(Top_prod2['date'])\n    Top_prod2['date'] = Top_prod2['date'].dt.strftime(\"%Y\")\n    Skin_rev = Top_prod2['review']\n    \n    #distribution for line graph\n    years = ['2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021']\n    # hair\n    Hair_line = Top_prod1['date'].value_counts()\n    Hair_line = Hair_line.sort_index().reset_index()\n    Hair_line = pd.DataFrame(Hair_line).rename(columns={\"index\": \"dates\", 'date': \"count\"})\n    for i in years:\n        if i not in Hair_line.values:\n            new_row = {'dates': i,'count': 0}\n            Hair_line = Hair_line.append(new_row, ignore_index=True)\n    Hair_line['dates'] = pd.to_datetime(Hair_line['dates'])\n    Hair_line['dates'] = Hair_line['dates'].dt.strftime(\"%Y\")\n    Hair_line = Hair_line.sort_values(by='dates').reset_index()\n    Hair_line = Hair_line.drop(columns='index')\n\n    #skin\n    Skin_line = Top_prod2['date'].value_counts()\n    Skin_line = Skin_line.sort_index().reset_index()\n    Skin_line = pd.DataFrame(Skin_line).rename(columns={\"index\": \"dates\", 'date': \"count\"})\n    for i in years:\n        if i not in Skin_line.values:\n            new_row = {'dates': i,'count': 0}\n            Skin_line = Skin_line.append(new_row, ignore_index=True)\n    Skin_line['dates'] = pd.to_datetime(Skin_line['dates'])\n    Skin_line['dates'] = Skin_line['dates'].dt.strftime(\"%Y\")\n    Skin_line = Skin_line.sort_values(by='dates').reset_index()\n    Skin_line = Skin_line.drop(columns='index')\n    \n    \n    \n    \n###--------------------------------------------------------------------------------------------###\n    #Average Rating\n    mean = dup_df[dup_df['product']== data]\n    prod_mean = round(mean['rating'].mean(),1)\n    \n    \n    \n    \n###--------------------------------------------------------------------------------------------###\n    \n    # % Bad Reviews\n    BR = Bad_Ratings_per_product.loc[Bad_Ratings_per_product['product'] == data, 'ratio'].iloc[0]\n    \n    return Hair_rev, Skin_rev, Hair_line, Skin_line, prod_mean, BR","5684a489":"# First product\nprod_1= MA_neghair['index'].loc[0]\nh_review1, s_review1, graph_1h, graph_1s, graph_1b, graph_1d = graph_areas(prod_1)\n\n# Second Product\nprod_2 = MA_neghair['index'].loc[1]\nh_review2, s_review2, graph_2h, graph_2s, graph_2b, graph_2d = graph_areas(prod_2)\n\n# Third Product\nprod_3= MA_neghair['index'].loc[2]\nh_review3, s_review3, graph_3h, graph_3s, graph_3b, graph_3d = graph_areas(prod_3)\n\n# Fourth Product\nprod_4= MA_neghair['index'].loc[3]\nh_review4, s_review4, graph_4h, graph_4s, graph_4b, graph_4d = graph_areas(prod_4)\n","af18ac45":"plt.style.use('seaborn-white')\n\n\nfig = plt.figure(constrained_layout=True,figsize=(10,6))\naxx = fig.add_gridspec(4, 4)\ntitle = textwrap.fill(str(prod_1),78)\n#fig.suptitle(title, size = 12, weight='bold')\n#fig.suptitle(str(n_prod), size = 10, weight='bold')\n\n\n\n\n####---------------------------------------------------------####\n# donut plot\naxd1 = fig.add_subplot(axx[0:3, 0:1])\n#ax1.set_title('% Bad Reviews',fontsize=11)\n\naxd1.set_title('% Bad Reviews',fontsize=12)\n\nBR1 = graph_1d*100\nratio = [BR1, 100-BR1]\ncolors = ['red','lightgray']\naxd1.pie(ratio, startangle=270, colors=colors)\nlabels = \"{:.2%}\".format(graph_1d)\n# add a circle at the center to transform it in a donut chart\nmy_circle1=plt.Circle( (0,0), 0.85, color='white')\np=plt.gcf()\np.gca().add_artist(my_circle1)\naxd1.add_patch(my_circle1)\nlabel = axd1.annotate(labels, xy=(0,0), fontsize=15, ha=\"center\", color='red')\n\n#line\naxl1 = fig.add_subplot(axx[0:3, 1:4])\n\nxh1=graph_1h['dates']\nyh1=graph_1h['count']\naxl1.plot(xh1,yh1, color='red',label='Hair Damage')\nplt.fill_between(xh1, yh1, color='red', alpha=0.3)\nxs1=graph_1s['dates']\nys1=graph_1s['count']\naxl1.plot(xs1,ys1,color='blue',label='Skin Damage')\nplt.fill_between(xs1, ys1, color='blue', alpha=0.3)\naxl1.tick_params(axis=\"y\",direction=\"in\", pad=-10)\nplt.xticks(rotation=45)\nplt.legend()\nplt.title(\"Damage Reviews Over Time\", fontsize=14)\n#plt.title(str(prod_1), fontsize=13,weight='bold')\n\n#bar\naxb1 = fig.add_subplot(axx[3:4, 0:4])\n\nx=[graph_1b]\naxb1.barh(y=1,width=graph_1b,color='blue', alpha=0.6)\nfor i, v in enumerate(x):\n    plt.text(v+0.06, i+.9, str(v),fontsize=14,fontweight='bold')\nplt.ylim(0.5, 1.5)\nplt.xlim(0, 5)\n#plt.axis(\"off\")\nplt.tight_layout()\n#plt.ylabel(\"Avg. Rating\", fontsize=10.5, rotation='horizontal')\nplt.grid(False)\naxb1.tick_params(left=False,labelleft=False)\nplt.title(\"Avg. Product Rating\", fontsize=10)\n\nplt.show();\n\n\nprint(color.BOLD + \"Review 1:\"+color.END, h_review1.loc[0], \"\\n\")\nprint(color.BOLD + \"Review 2:\"+color.END, h_review1.loc[1], \"\\n\")\nprint(color.BOLD + \"Review 3:\"+color.END, h_review1.loc[2])\n\n#print(color.BOLD + \"Review 1:\"+color.END, textwrap.fill(h_review1.loc[0],100),\"\\n\")\n#print(color.BOLD + \"Review 2:\"+color.END, textwrap.fill(h_review1.loc[1],100),\"\\n\")\n#print(color.BOLD + \"Review 3:\"+color.END, textwrap.fill(h_review1.loc[2],100))\n","1d194c2d":"plt.style.use('seaborn-white')\n\n\nfig = plt.figure(constrained_layout=True,figsize=(10,6))\naxx = fig.add_gridspec(4, 4)\ntitle = textwrap.fill(str(prod_2),78)\n#fig.suptitle(title, size = 12, weight='bold')\n#fig.suptitle(str(n_prod), size = 10, weight='bold')\n\n\n\n\n####---------------------------------------------------------####\n# donut plot\naxd2 = fig.add_subplot(axx[0:3, 0:1])\n#ax1.set_title('% Bad Reviews',fontsize=11)\n\naxd2.set_title('% Bad Reviews',fontsize=12)\n\nBR2 = graph_2d*100\nratio = [BR2, 100-BR2]\ncolors = ['red','lightgray']\naxd2.pie(ratio, startangle=270, colors=colors)\nlabels = \"{:.2%}\".format(graph_2d)\n# add a circle at the center to transform it in a donut chart\nmy_circle2=plt.Circle( (0,0), 0.85, color='white')\np=plt.gcf()\np.gca().add_artist(my_circle2)\naxd2.add_patch(my_circle2)\nlabel = axd2.annotate(labels, xy=(0,0), fontsize=15, ha=\"center\", color='red')\n\n#line\naxl2 = fig.add_subplot(axx[0:3, 1:4])\n\nxh2=graph_2h['dates']\nyh2=graph_2h['count']\naxl2.plot(xh2,yh2, color='red',label='Hair Damage')\nplt.fill_between(xh2, yh2, color='red', alpha=0.3)\nxs2=graph_2s['dates']\nys2=graph_2s['count']\naxl2.plot(xs2,ys2,color='blue',label='Skin Damage')\nplt.fill_between(xs2, ys2, color='blue', alpha=0.3)\naxl2.tick_params(axis=\"y\",direction=\"in\", pad=-10)\nplt.xticks(rotation=45)\nplt.legend()\nplt.title(\"Damage Reviews Over Time\", fontsize=14)\n#plt.title(str(prod_2), fontsize=13,weight='bold')\n\n#bar\naxb2 = fig.add_subplot(axx[3:4, 0:4])\n\nx=[graph_2b]\naxb2.barh(y=1,width=graph_2b,color='blue', alpha=0.6)\nfor i, v in enumerate(x):\n    plt.text(v+0.06, i+.9, str(v),fontsize=14,fontweight='bold')\nplt.ylim(0.5, 1.5)\nplt.xlim(0, 5)\n#plt.axis(\"off\")\nplt.tight_layout()\n#plt.ylabel(\"Avg. Rating\", fontsize=10.5, rotation='horizontal')\nplt.grid(False)\naxb2.tick_params(left=False,labelleft=False)\nplt.title(\"Avg. Product Rating\", fontsize=10)\n\nplt.show();\n\n\nprint(color.BOLD + \"Review 1:\"+color.END, h_review2.loc[0],\"\\n\")\nprint(color.BOLD + \"Review 2:\"+color.END, h_review2.loc[5],\"\\n\")\nprint(color.BOLD + \"Review 3:\"+color.END, s_review2.loc[9])\n","3c34c12c":"plt.style.use('seaborn-white')\n\n\nfig = plt.figure(constrained_layout=True,figsize=(10,6))\naxx = fig.add_gridspec(4, 4)\ntitle = textwrap.fill(str(prod_3),78)\n#fig.suptitle(title, size = 12, weight='bold')\n#fig.suptitle(str(n_prod), size = 10, weight='bold')\n\n\n\n\n####---------------------------------------------------------####\n# donut plot\naxd3 = fig.add_subplot(axx[0:3, 0:1])\n#ax1.set_title('% Bad Reviews',fontsize=11)\n\naxd3.set_title('% Bad Reviews',fontsize=12)\n\nBR3 = graph_3d*100\nratio = [BR3, 100-BR3]\ncolors = ['red','lightgray']\naxd3.pie(ratio, startangle=270, colors=colors)\nlabels = \"{:.2%}\".format(graph_3d)\n# add a circle at the center to transform it in a donut chart\nmy_circle3=plt.Circle( (0,0), 0.85, color='white')\np=plt.gcf()\np.gca().add_artist(my_circle3)\naxd3.add_patch(my_circle3)\nlabel = axd3.annotate(labels, xy=(0,0), fontsize=15, ha=\"center\", color='red')\n\n#line\naxl3 = fig.add_subplot(axx[0:3, 1:4])\n\nxh3=graph_3h['dates']\nyh3=graph_3h['count']\naxl3.plot(xh3,yh3, color='red',label='Hair Damage')\nplt.fill_between(xh3, yh3, color='red', alpha=0.3)\nxs3=graph_3s['dates']\nys3=graph_3s['count']\naxl3.plot(xs3,ys3,color='blue',label='Skin Damage')\nplt.fill_between(xs3, ys3, color='blue', alpha=0.3)\naxl3.tick_params(axis=\"y\",direction=\"in\", pad=-10)\nplt.xticks(rotation=45)\nplt.legend()\nplt.title(\"Damage Reviews Over Time\", fontsize=14)\n#plt.title(str(prod_3), fontsize=13,weight='bold')\n\n#bar\naxb3 = fig.add_subplot(axx[3:4, 0:4])\n\nx=[graph_3b]\naxb3.barh(y=1,width=graph_3b,color='blue', alpha=0.6)\nfor i, v in enumerate(x):\n    plt.text(v+0.06, i+.9, str(v),fontsize=14,fontweight='bold')\nplt.ylim(0.5, 1.5)\nplt.xlim(0, 5)\n#plt.axis(\"off\")\nplt.tight_layout()\n#plt.ylabel(\"Avg. Rating\", fontsize=10.5, rotation='horizontal')\nplt.grid(False)\naxb3.tick_params(left=False,labelleft=False)\nplt.title(\"Avg. Product Rating\", fontsize=10)\n\nplt.show();\n\n\nprint(color.BOLD + \"Review 1:\"+color.END, h_review3.loc[0],\"\\n\")\nprint(color.BOLD + \"Review 2:\"+color.END, h_review3.loc[12],\"\\n\")\nprint(color.BOLD + \"Review 3:\"+color.END, h_review3.loc[10])\n","9167a9f1":"plt.style.use('seaborn-white')\n\n\nfig = plt.figure(constrained_layout=True,figsize=(10,6))\naxx = fig.add_gridspec(4, 4)\n#title = textwrap.fill(str(prod_4),78)\n#fig.suptitle(title, size = 12, weight='bold')\n#fig.suptitle(str(n_prod), size = 10, weight='bold')\n\n\n\n\n####---------------------------------------------------------####\n# donut plot\naxd4 = fig.add_subplot(axx[0:3, 0:1])\n#ax1.set_title('% Bad Reviews',fontsize=11)\n\naxd4.set_title('% Bad Reviews',fontsize=12)\n\nBR4 = graph_4d*100\nratio = [BR4, 100-BR4]\ncolors = ['red','lightgray']\naxd4.pie(ratio, startangle=270, colors=colors)\nlabels = \"{:.2%}\".format(graph_4d)\n# add a circle at the center to transform it in a donut chart\nmy_circle4=plt.Circle( (0,0), 0.85, color='white')\np=plt.gcf()\np.gca().add_artist(my_circle4)\naxd4.add_patch(my_circle4)\nlabel = axd4.annotate(labels, xy=(0,0), fontsize=15, ha=\"center\", color='red')\n\n#line\naxl4 = fig.add_subplot(axx[0:3, 1:4])\n\nxh4=graph_4h['dates']\nyh4=graph_4h['count']\naxl4.plot(xh4,yh4, color='red',label='Hair Damage')\nplt.fill_between(xh4, yh4, color='red', alpha=0.3)\nxs4=graph_4s['dates']\nys4=graph_4s['count']\naxl4.plot(xs4,ys4,color='blue',label='Skin Damage')\nplt.fill_between(xs4, ys4, color='blue', alpha=0.3)\naxl4.tick_params(axis=\"y\",direction=\"in\", pad=-10)\nplt.xticks(rotation=45)\nplt.legend()\nplt.title(\"Damage Reviews Over Time\", fontsize=14)\n#plt.title(str(prod_4), fontsize=13,weight='bold')\n\n#bar\naxb4 = fig.add_subplot(axx[3:4, 0:4])\n\nx=[graph_4b]\naxb4.barh(y=1,width=graph_4b,color='blue', alpha=0.6)\nfor i, v in enumerate(x):\n    plt.text(v+0.06, i+.9, str(v),fontsize=14,fontweight='bold')\nplt.ylim(0.5, 1.5)\nplt.xlim(0, 5)\n#plt.axis(\"off\")\nplt.tight_layout()\n#plt.ylabel(\"Avg. Rating\", fontsize=10.5, rotation='horizontal')\nplt.grid(False)\naxb4.tick_params(left=False,labelleft=False)\nplt.title(\"Avg. Product Rating\", fontsize=10)\n\nplt.show();\n\n\nprint(color.BOLD + \"Review 1:\"+color.END, h_review4.loc[0],\"\\n\")\nprint(color.BOLD + \"Review 2:\"+color.END, h_review4.loc[1],\"\\n\")\nprint(color.BOLD + \"Review 3:\"+color.END, s_review4.loc[2])","d5aa32bf":"# change date to just year\nnew_hair['date'] = pd.to_datetime(new_hair['date'])\nnew_hair['date'] = new_hair['date'].dt.strftime(\"%Y\")\n\n# newhair groupby date\nallhair = pd.DataFrame({'count' : new_hair[\"date\"].value_counts()}).reset_index()\nallhair = allhair.sort_values(by='index').reset_index()\n#allhair = allhair.drop(columns='level_O')\nmyhair_ = allhair.drop(allhair.columns[i], axis=1)\n\n\nnew_skin['date'] = pd.to_datetime(new_skin['date'])\nnew_skin['date'] = new_skin['date'].dt.strftime(\"%Y\")\n\n# newhair groupby date\nallskin = pd.DataFrame({'count' : new_skin[\"date\"].value_counts()}).reset_index()\nallskin = allskin.sort_values(by='index').reset_index()\n#allhair = allhair.drop(columns='level_O')\nmyskin_ = allskin.drop(allskin.columns[i], axis=1)","e5834afc":"# basically right, but fix up\nplt.style.use('seaborn-whitegrid')\n\nfig = plt.figure(figsize=(12,8))\nax = plt.axes()\n\nxh=myhair_['index']\nyh=myhair_['count']\nax.plot(xh,yh, color='red',label='Hair Damage')\n\nxs=myskin_['index']\nys=myskin_['count']\nax.plot(xs,ys,color='blue',label='Skin Damage')\n\nplt.xticks(rotation=45)\nplt.legend()\nplt.title(\"Damage\", fontsize=18)\nplt.show();","f3e0610c":"print('\\033[1m' + \"Number of Hair Damage Reviews: \"+'\\033[0m', len(new_hair.index), \"\\n\")\n\nprint('\\033[1m'+\"Number of Skin Damage Reviews: \"+'\\033[0m', len(new_skin.index), \"\\n\")\n\nprint('\\033[1m'+\"Total Number of Reviews: \"+'\\033[0m', len(dup_df.index))","4c231e79":"#### Can this type of analysis be used to identify consumer harm in other product?\n\nWe think yes. Although the collection and analysis of these reviews poses obstacles, any company that values consumer feedback could easily muster the resources necessary to see what problems their consumers are dealing with. \n\nOf course, the tasks in this project were made substantially easier by having the knowledge of the lawsuit and product harms in advance. The real difficulty with expanding a project like this is being able to identify common themes among multiple products when you have no idea what you are looking for. However, I believe it is possible for consumer rights advocates, attorneys, and even the companies themselves to make use of the abundant data provided by consumers on the web to analyze and identify what kind of risks, if any, products pose to the general public in a much shorter time frame than traditionally expected.\n\nThere is no reason these issues should avoid being addressed (legally or otherwise) for more than 10 years after people began identifying the problems online. ","d647f298":"## 3. Date","cdb33484":"\u2b07\ufe0f  **Drop duplicates again because we will now have more after changing product names.** ","007291c7":"## \ud83d\udcc8 Graph Complaints","9110160b":"## Thick & Full Biotin & Collagen Shampoo, Thick & Full Biotin & Collagen Conditioner, Biotin & Collagen Extra Volume Extra Strength Conditioner","bdb56bfc":"Further, we can examine the number of hair and skin damage reviews in the dataset:","966fc35d":"\ud83d\udccd **Reviews by product**","c59319e1":"Topic Modeling is a type of statistical modeling used to understand large corpuses of texts through topic extraction. The Latent Dirchlet Allocation (LDA) and Hierarchical Dirichlet Process (HDP) are two forms of topic modeling that are commonly used. Both hava advantages and disadvantages, but the main difference between the two is that LDA requires the user to input a specific number of topics and HDP does not. The reason for using both of them will be explained in the next note. \n\nThe main thing to know is that it is difficult to fine tune an LDA model and that means it's hard to always find the most accurate topics. We already have an idea of what we are looking for so that makes this a little easier. In order to perform the modeling the data must be processed, which is what our tokenization and lemmatization process was above. Now we are going to extract topics from our dataset and visualize them in order to find relevant terms and patterns. ","845dc213":">These are the topics that we have grouped the reviews into. The words listed are the words that are the words in each topic that are most frequent in all the topics. ","b4027dcd":"# Resources\n\n1. https:\/\/towardsdatascience.com\/fuzzywuzzy-find-similar-strings-within-one-column-in-a-pandas-data-frame-99f6c2a0c212\n2. https:\/\/medium.com\/airy-science\/gain-insights-from-customers-review-using-topic-modelling-2ae632be202b\n3. https:\/\/towardsdatascience.com\/the-complete-guide-for-topics-extraction-in-python-a6aaa6cedbbc\n4. https:\/\/towardsdatascience.com\/dont-be-afraid-of-nonparametric-topic-models-part-2-python-e5666db347a\n5. https:\/\/towardsdatascience.com\/unsupervised-nlp-topic-models-as-a-supervised-learning-input-cf8ee9e5cf28\n6. https:\/\/www.kaggle.com\/andradaolteanu\/i-commonlit-explore-xgbrf-repeatedfold-model\n7. https:\/\/medium.com\/analytics-vidhya\/the-ultimate-markdown-guide-for-jupyter-notebook-d5e5abf728fd\n8. https:\/\/stackoverflow.com\/questions\/62823331\/lda-visualisation-in-jupyter-notebook ","2ee935af":"# **Class Action: OGX Product Reviews**","54648be9":"# Data","a84c3918":"> **Now that we have just one large (semi) clean dataset, lets explore some of the other features it contains.**","8b0f0a1d":"## Takeaways\n\n- Reviews related to hair loss, skin irritation, and skin damage were posted online from 2007 onward\n- Although not overwhelming, there was evidence of a pattern of damage caused by multiple products\n- These reviews were public and available to Johnson & Johsnon who could have viewed them any time over the past 14 years\n- After years of these reviews being posted on multiple public websites Johnson & Johnson continued selling these products with DMDM hydantoin in them\n- The complaint alleges damages caused by OGX Biotin + Collagen Shampoo and Conditioner, Renewing Argan Oil of Morocco Shampoo and Conditioner, OGX Extra Strength Hydrate & Repair and Argan Oil of Morocco Shampoo and Conditioner, and OGX Quenching + Coconut Curls Shampoo, all of which are included in the analysis above, as well as a multitude of other products. ","b8823f53":"\ud83d\udccd **Reviews by product**","e128634c":"**Libraries below \u2b07**","abf032d0":"#### 5.1 Number of reviews per product\n","0a52414f":"<h1><center><b>Part I<\/b><\/center><\/h1>\n<h1><center><b>Overview<\/b><\/center><\/h1>","543469fa":"**From an initial glance** it looks like the two datasets only share 4 products with the same name. But upon further investigation we can conclude that this number is so small because each website formats the product names in a slightly different way.","1f39f911":"***","65cda5be":"The first thing we need to do is use Natural Language Processing (NLP) to make the text of the reviews readable for our analysis. This process is called tokenization. ","47245b89":"\n\n>**Problem:** Our first problem is that there is an inflated number of products because the website formatted a lot of product names differently. Some products that are the same vary slightly based on an extra descriptor or letter case differences. \n>\n>\n>**Solution:** Use the FuzzyWuzzy python package to combine rows where products have similar names.\n","60d6406f":"**Change Date Types to Match**","d36796e7":"**Replace Ulta Dataset Product Names** \ud83d\udd03","6a18ad53":"## Nourishing + Coconut Milk Conditioner, Nourishing + Coconut Milk Shampoo","d31763e4":"## 1. Explore Data ","9bca7517":">We have reviews ranging from 2007-2021. That's 13 years of reviews. It looks like the majority of our reviews are from 2011-2015. ","e250bd86":"### Terms\n\n**1. Hair Damage\/Loss**\n\n    * hair damage \n    * hair loss \n    * dry hair\n    * stripped hair\n    * straw\/strawlike\n    * brittle\n    * frizzle\n\n\n**2. Skin\/Scalp Irritation**\n\n    * Skin irritation\n    * skin burning\n    * breakouts\n    * scalp burning\n    * scalp irritation\n    * flaking\n    * dry skin\/scalp\n    * itching\/itchie scalp\n    * allergic reaction","eacdd79a":"\ud83d\udea8 **What's Happening?** \ud83d\udea8\n\n\n    \n>It appears that there are a large number of duplicate reviews in the Ulta dataset. When I went back and checked the webiste I was able to determine that the Ulta website lists many of the same reviews for similarly named products. I'm not sure why this is a feature on the website, or if it is bug they need to fix, but it made things a little more difficult. To see what I'm talking about, I'll give you a little example. If you click on  \"Argon Penatrating Oil\" you would find the same reviews for that product as you would for \"Argon Extra Penatrating.\" \n>    \n>The next portion of the notebook will address the duplicate issues:\n    \n","a88cc68c":"***","a64ac4dc":"### 2.5 Combine Products for Duplicate Reviews","b14ffd1e":"## ARGAN OIL MOROCCO CONDITONER, Renewing + Argan Oil Of Morocco Shampoo, Renewing + Argan Oil of Morocco Hydrating Conditioner","29bfbc94":"## Renewing + Argan Oil of Morocco Penetrating Oil, Renewing Argan Oil Of Morocco Weightless Healing Dry Oil","2743e041":"**Data Types** \ud83d\udcbe","b726ed76":"Now we can identify what percentage of the reviews are related to injuries claimed in the lawsuit:\n* Percentage of Skin Damage Reviews:  1.23%\n* Percentage of Hair Damage Reviews:  5.25%\n\nAnd in total, around **6.5%** of the Reviews we collected were related to damages and injuries alleged in the lawsuit. This may seem small if we were merely looking at the number of bad reviews, but this number is only representative of reviews that claim hair or skin damage. This means 6.5% of all the reviews we collected were related to specific pattern of injuries caused by a product. That's a large enough percentage to cause alarm to a company that is monitoring consumer reviews. ","57571d63":">Next, we want examine what the distribution of poor product reviews is across products. We declare a product review with a rating of 1 or 2 stars as being a poor rating. ","9604a480":"**Check names** \u2705\n\n> For our values we are replacing we should get a return of zero.","bf1f22fc":"## 4. Ratings","069fd656":"## \ud83c\udf10 Overview\n\n\nWe can see above that some products, even though they have many high ratings, have a significant and sustained number of reviews claiming hair loss and skin damage. These reviews are pretty consistent over time, and there are enough of them to demonstrate a patter of injuries caused by OGX product. Below we can see that both skin and hair damage claims have been made in reviews every year since 2007. ","bc4869b3":">Each review of a product is accompanied by a rating of between 1 and 5 stars. Five stars normally would indicate satisfaction with the product and one star would signal dissapointment. ","1b9e7f4e":"Our dataset comes from two sources:\n \n\n1. [MakeupAlley](https:\/\/www.makeupalley.com\/), a website specifically for users to review personal care and beauty products; and \n\n2. [Ulta](https:\/\/www.ulta.com\/), the company website for Ulta Beauty, which contains thousands of reviews on products spanning many markets. \n\nI know there are many other data sources out there, but I believe these two represent a good cross-section of the website review domain and they both appear to be reputable websites.\n\nLike all data science problems, most of the work here was in the data preparation and cleaning. There were some challenges in our dataset due to the configuration of the website we scraped. This presented significant challenges that I did not anticipate and required a little creative thinking that I will try to outline below. Because of these challenges I will be a little more descriptive and technical in this section as it relates to the code and data preparation. First-off, lets load the data:","b045226a":"## 2. Reviews\n\nNext, we come to the meat of our dataset--the reviews.\n\nLets get an idea of how many unique reviews there are in each dataset:\n    ","ab16e5af":"\n>**Here is the big problem:** We are left with a dataset where similarly sounding products share a large number of reviews. This means we know that some of the reviews (specifically Ulta reviews) reflect a product, but we don't know exactly which one they are talking about. \n>\n>\n>**Solution:** Group Products by Review and combine the product names. \n","6f6ff873":"### <span style=\"color:#CC5500;\"> Fuzzy Wuzzy <\/span>\n\nHere's where we dive into our use of FuzzyWuzzy. \n\n    \n>FuzzyWuzzy is a python package that looks at the string values in different columns and rates them on similarity. Here we use it to identify products that are the same but whose names have different formatting. Then we will standardize the product name formatting across our dataset. \n>\n>The goal of this first pass is to correct any instances where two products are really the same, but the names have been formatted slightly differently on the website. These slight differences may not affect how a reader would view the reviews, but they will affect our ability to group products for our analysis. \n    \n    \nLook at this [article](https:\/\/towardsdatascience.com\/fuzzywuzzy-find-similar-strings-within-one-column-in-a-pandas-data-frame-99f6c2a0c212) for reference on FuzzyWuzzy and the code we used below.","af81c522":"![image.ping](https:\/\/www.mcnicholaslaw.com\/wp-content\/uploads\/2020\/01\/Class-Action.jpg)","82b81a7b":"***","48cf3bbe":"\ud83d\udccc **Our new total number of reviews is 9,824.**","68219347":"### 2.1 Duplicates\n\nIdentify the duplicate values","d1e5a916":"<h1><center><b>Part II<\/b><\/center><\/h1>\n<h1><center><b>Analysis of Reviews<\/b><\/center><\/h1>","fef08323":"> \u2b07\ufe0f  Drop rows that are complete duplicates:","883b648c":"# 2. Topic Modeling","676010dc":"> \ud83d\udccc **Note:** Dropped about 5,000 reviews","e1272e22":"\ud83d\udea8 **Note on HDP:**\n    \nIn this [article](https:\/\/towardsdatascience.com\/unsupervised-nlp-topic-models-as-a-supervised-learning-input-cf8ee9e5cf28) by Marc Kelechava he outlines a solution to the topic selection problem involved with LDA models, which involves running a HDP model on the corpus first and seeing what number of topics the HDP chooses. \n\n\nThe HDP model seeks to learn the correct number of topics from the data (corpus) so you don't need to provide it with a fixed number of topics. This turned out to be a great idea, so I implemented it below and decided to use the fixed number of 20 for my topics input in the LDA model. ","3fff80e3":"\nIn June 2021, consumers began filing class action lawsuits against Johnson & Johnson alleging damages caused by products in their OGX line. The OGX product line consists of mainly hair products including shampoos, conditioners, and oils. The product line was owned by Vogue International until Johnson & Johnson acquired vogue international in 2016. The [complaint](https:\/\/www.classaction.org\/media\/whipple-v-johnson-and-johnson-consumer-inc.pdf) alleges that the products contain **DMDM hydantoin**, a preservative known to slowly leach formaldehyde when it comes into contact with water. Formaldehyde is a known human carcinogen that can cause harmful skin reactions. Further, the complaint alleges that Johnson & Johnson has known for a decade that DMDM hydantoin can cause hair loss and skin reactions and even [announced plans](http:\/\/www.nytimes.com\/2012\/08\/16\/business\/johnson-johnson-to-remove-formaldehyde-from-products.html) to remove these chemicals from their products by 2015. However, the lawsuit alleges that J&J failed to remove formaldehyde from their OGX products after the acquisition in 2016. \n\nThe case is still in its early stages but after reading the filing I decided to try to find some customer reviews of the products. To my surprise I saw a few customer reviews mentioning the same types of injuries alleged in the lawsuit. This gave me the idea to review the online product reviews to determine if these injuries and this lawsuit could have been identified earlier. \n\nThe complaint alleges that Johnson & Johnson knew about the dangers of these chemicals but still included DMDM hydantoin in their products, its even listed as an ingredient on a number of these products. We've asked the question about whether they knew the chemicals were dangerous, but now let's ask the question of whether they knew their customers were being injured by their products. In order to answer this question we will need some data, and I decided the best type of data could be customer reviews. Public online product reviews are accessible to anyone (even Johnson & Johnson)and provide consumers a place to air their grievances and warn other consumers about dangers in products. \n\nThis notebook aims to analyze customer reviews of the OGX product line owned by Johnson & Johnson. The goal is two-fold: First, to discover if there is a history of injuries like the ones named in the lawsuit and whether this information was accessible to Johnson & Johnson. If there were reviews over the previous 10 years that indicate OGX products cause hair loss, skin damage, and other injuries related to formaldehyde, then an inference can be made that Johnson & Johnson didn't just know the chemicals were dangerous, but knew or should have known the actual products were causing harm to its customers. Second, we want to explore the possibilities of using online product reviews to determine if a product is dangerous. Traditionally, we find out about dangerous products by either waiting for enough people to get injured so that it is obvious and action is finally taken, or we wait for some kind of ruling by a government agency or product recall by a company. But what if there was another way. What if we could look at the data people explicitly make public (i.e. online reviews on public websites) to determine if there is a trend of certain injuries or problems that would allow us to identify a dangerous product. \n\nI plan on walking the reader through the data I have collected and my analysis on it. We will investigate the customer review data and seek to determine topcs of interest and common phrases shared by reviews identifying injuries. I will show that the customer review data has given us more than a mere scintilla of evidence that consumers shared common injuries resulting from OGX product use, and that these injuries have been mentioned in product reviews for close to a decade. Additionally, I will identify products that have received a large share of these damaging reviews, and we will discuss what role data anlysis can play in consumer protection and lawsuits.  ","0d6fd4d0":"### 2.2 Fix Ulta Products","05b9a09a":"Here we use the terms we've identified above to filter rows that include the desired reviews. We pull any reviews that include the combination of terms below. This is a way to locate reviews that talk about hair damage caused by OGX Products. It's not 100% accurate, so it will miss some reviews and include some that probably shouldn't be included, but overall it should give us a picture of the opinion of the product. ","aaf19f2c":"> \ud83d\udccc **Note:** That didn't really do much because there are not many complete duplicates in the Ulta dataset. There are duplicate reviews, but as we stated above they have different product names. So we'll need to be a little more creative. ","0e53a879":"## 3. Visualize Reviews","302978ec":"***","782e3e16":"## 5. Product","407b4b71":"### B). Skin","9d9f49e2":"### \u2601\ufe0f WordCloud","da9e9487":"# TOC\n\n#### **Part I Overview**\n1. [Explore Data](#1.-Explore-Data)\n2. [Reviews](#2.-Reviews)\n3. [Date](#3.-Date)\n4. [Ratings](#4.-Ratings)\n5. [Product](#5.-Product)\n\n#### **Part II Analysis**\n1. [Tokenize](#1.-Tokenize)\n2. [Topic Modeling](#2.-Topic-Modeling)\n3. [Visualize](#3.-Visualize-Reviews)\n\n#### **Conclusion**","3ebb307f":"# Conclusion","5c4d744b":"### A). Hair","a8a65703":"#### 5.2 Number of Bad Reviews per Product","8153a9dd":"> \ud83d\udccc **Note:** Limit to reviews with a 3-star rating or lower","f0775cfe":"In this section we will visualize the trends in the customer reviews we have collected:\n    \n- Graph on the four products with the highest number of hair or skin injury reviews \n- Graph distribution of all complaints over time\n- Recap scope of damaging reviews\n\n\nHopefully this section will allow you to grasp the trends and scope of damages caused by these products. ","71b2ced3":"### 2.4 Combine Datasets","12c23587":"## 1. Tokenize","49617d7a":"#### \ud83d\udce3 **Now comes the main point of our project. We want to identify any patterns among the reviews, particularly any patterns of injury or product defect.** \n\n>In order to accomplish this we are going to analyze the text of the actual reviews to gain insight into what customers are saying about the OGX product line. We want to understand if there is significant negative feedback and if there are any common problems among large numbers of customers. This should give us insight into whether we can discern possible future lawsuits or product recalls from publicly available customer reviews in real time. ","6a43644b":"Below we can get an idea of the distribution of product reviews.","09f503d6":"<div class = \"alert alert-block alert-info\">\nYou can sift through the topics if you want and explore the dataset. In general, a large portion of the reviews are based around topics that have to do with hair, smell, dryness, use of product, etc. Then there is a litany of small topics that reviews are grouped into. But for a broader overview lets examine the topics with a wordcloud below.","f0a7a4b2":"It looks like there are more than 3x as many products in the MakeupAlley data than in the Ulta. However, the Ulta dataset provides us with a much larger number of actual reviews. Lets see how similar the two datasets are by determining if they share any of the same products.","f8ffacf0":"In order to deal with these differing formats we are going to use the **FuzzyWuzzy** python package to align the review names. But we'll address that more below. For now let's just put all of the unique product names into a list that we can use later: ","b25d80f3":"### 2.3 Replace MakeupAlley Product Names","7083fcba":"<div class=\"alert alert-block alert-info\">\n\n<b>Tip:<\/b> Dry hair is a big one but we decided to leave it out for this analysis because the word \"dry\" is so common and it is used in instances which aren't specifically describing damaged hair. Additionally, terms such as \"brittle\", \"straw like\", and \"frizzle\" should cover most of the dry hair related damage.<br>\n  \nIt's just one area that is too broad (other keywords will contain some reviews that aren't necessarily describing hair damage and they will miss some reviews that are describing hair damage; this portion is really meant to be an analysis that gets most of the reviews describing hair damage or loss. Further identification of each damaging review would be needed for more in depth and time consuming analysis).","11dbb988":"Now we can use the same process we used above to address differences between the two datasets, instead of just one dataset. So here we will adjust the product names in the MakeupAlley dataset so that they are formatted the same way as the ones in the Ulta dataset. ","868908a2":"![image.ping](https:\/\/i0.wp.com\/brandingforum.org\/wp-content\/uploads\/2016\/06\/Johnson-Johnson-Vogue.jpg?resize=696%2C390&ssl=1)","4e486a7c":"\u26a1\ufe0f **After reviewing the wordcloud and the Topic Model, I have come up with a list of words that might be identifying of injuries related to the lawsuit** ","4ff5577d":"\ud83c\udf1f **Note:** The most common rating is 5 star, followed by 4 star, then 1 star ratings. This means most people likely enjoy the product. But we also have a situation where there could be some review inflation. As pointed out in this [FTC order](https:\/\/www.ftc.gov\/news-events\/press-releases\/2021\/10\/ftc-puts-hundreds-businesses-notice-about-fake-reviews-other), many large companies flood the internet with positive reviews that they pay for. This distorts the picture consumers get of the product they are searching for. But there's no evidence that has happened here, we can only look at the reviews we gathered and if they contain patterns. \n","64795543":"How many of the same products do the two datasets share?","76a7a429":"**It works!**","8678c407":"So we can see in the big years the 5 star ratings kind of dominate, followed by 4 stars and 1 star. But as we move on through 2018 and onward the 5 star ratings drop off, while the other star ratings don't change by as much. This could signal dissatisfaction recently with the products as more facts have come to light about their damaging nature. Possibly more and more people read online reviews and realized the product was the cause of their hair or skin problems. ","70c8bb06":"If you have any comments, questions, or anything else to add please don't hesitate to make it known below. Thanks for reading! And I hope this notebook was useful. "}}