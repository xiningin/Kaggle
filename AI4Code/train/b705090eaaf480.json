{"cell_type":{"d13a9f5a":"code","687fec51":"code","9ff2e0f8":"code","5078849b":"code","6837e7b7":"code","eb66662e":"code","903fa3aa":"code","2fc849c6":"code","1660da40":"code","b976a688":"code","29a36ffb":"code","730b622d":"code","532fccfa":"code","b31aa18e":"code","16e02a46":"code","3b27c6e9":"code","0e498fd9":"code","7b7719c8":"code","8bf1d34b":"code","10f3ecd8":"code","90033103":"code","2d8fefd8":"code","a5ec0e53":"code","4fb23698":"code","4ececc40":"code","622088b7":"code","f4307d47":"code","7363a4d8":"code","0a3f6d6d":"code","e80b010b":"code","30c8abd8":"code","bc7d74c3":"code","5275f070":"code","b9c709c8":"code","7199b356":"markdown","36085405":"markdown","a13ec107":"markdown","1b422500":"markdown","972d3412":"markdown","d8b8e423":"markdown"},"source":{"d13a9f5a":"#Importing libraries\nimport nltk, re, pprint\nimport numpy as np\nimport pandas as pd\nimport requests\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pprint, time\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize","687fec51":"# reading the Treebank tagged sentences\nwsj = list(nltk.corpus.treebank.tagged_sents())","9ff2e0f8":"# first few tagged sentences\nprint(wsj[:40])","5078849b":"# Splitting into train and test\nrandom.seed(1234)\ntrain_set, test_set = train_test_split(wsj,test_size=0.3)\n\nprint(len(train_set))\nprint(len(test_set))\nprint(train_set[:40])","6837e7b7":"# Getting list of tagged words\ntrain_tagged_words = [tup for sent in train_set for tup in sent]\nlen(train_tagged_words)","eb66662e":"# tokens \ntokens = [pair[0] for pair in train_tagged_words]\ntokens[:10]","903fa3aa":"# vocabulary\nV = set(tokens)\nprint(len(V))","2fc849c6":"# number of tags\nT = set([pair[1] for pair in train_tagged_words])\nlen(T)","1660da40":"print(T)","b976a688":"# computing P(w\/t) and storing in T x V matrix\nt = len(T)\nv = len(V)\nw_given_t = np.zeros((t, v))","29a36ffb":"# compute word given tag: Emission Probability\ndef word_given_tag(word, tag, train_bag = train_tagged_words):\n    tag_list = [pair for pair in train_bag if pair[1]==tag]\n    count_tag = len(tag_list)\n    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n    count_w_given_tag = len(w_given_tag_list)\n    \n    return (count_w_given_tag, count_tag)","730b622d":"# examples\n\n# large\nprint(\"\\n\", \"large\")\nprint(word_given_tag('large', 'JJ'))\nprint(word_given_tag('large', 'VB'))\nprint(word_given_tag('large', 'NN'), \"\\n\")\n\n# will\nprint(\"\\n\", \"will\")\nprint(word_given_tag('will', 'MD'))\nprint(word_given_tag('will', 'NN'))\nprint(word_given_tag('will', 'VB'))\n\n# book\nprint(\"\\n\", \"book\")\nprint(word_given_tag('book', 'NN'))\nprint(word_given_tag('book', 'VB'))","532fccfa":"# compute tag given tag: tag2(t2) given tag1 (t1), i.e. Transition Probability\n\ndef t2_given_t1(t2, t1, train_bag = train_tagged_words):\n    tags = [pair[1] for pair in train_bag]\n    count_t1 = len([t for t in tags if t==t1])\n    count_t2_t1 = 0\n    for index in range(len(tags)-1):\n        if tags[index]==t1 and tags[index+1] == t2:\n            count_t2_t1 += 1\n    return (count_t2_t1, count_t1)","b31aa18e":"# examples\nprint(t2_given_t1(t2='NNP', t1='JJ'))\nprint(t2_given_t1('NN', 'JJ'))\nprint(t2_given_t1('NN', 'DT'))\nprint(t2_given_t1('NNP', 'VB'))\nprint(t2_given_t1(',', 'NNP'))\nprint(t2_given_t1('PRP', 'PRP'))\nprint(t2_given_t1('VBG', 'NNP'))","16e02a46":"#Please note P(tag|start) is same as P(tag|'.')\nprint(t2_given_t1('DT', '.'))\nprint(t2_given_t1('VBG', '.'))\nprint(t2_given_t1('NN', '.'))\nprint(t2_given_t1('NNP', '.'))\n","3b27c6e9":"# creating t x t transition matrix of tags\n# each column is t2, each row is t1\n# thus M(i, j) represents P(tj given ti)\n\ntags_matrix = np.zeros((len(T), len(T)), dtype='float32')\nfor i, t1 in enumerate(list(T)):\n    for j, t2 in enumerate(list(T)): \n        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]\/t2_given_t1(t2, t1)[1]","0e498fd9":"tags_matrix","7b7719c8":"# convert the matrix to a df for better readability\ntags_df = pd.DataFrame(tags_matrix, columns = list(T), index=list(T))","8bf1d34b":"tags_df","10f3ecd8":"tags_df.loc['.', :]","90033103":"# heatmap of tags matrix\n# T(i, j) means P(tag j given tag i)\nplt.figure(figsize=(18, 12))\nsns.heatmap(tags_df)\nplt.show()\n","2d8fefd8":"# frequent tags\n# filter the df to get P(t2, t1) > 0.5\ntags_frequent = tags_df[tags_df>0.5]\nplt.figure(figsize=(18, 12))\nsns.heatmap(tags_frequent)\nplt.show()","a5ec0e53":"len(train_tagged_words)","4fb23698":"# Viterbi Heuristic\ndef Viterbi(words, train_bag = train_tagged_words):\n    state = []\n    T = list(set([pair[1] for pair in train_bag]))\n    \n    for key, word in enumerate(words):\n        #initialise list of probability column for a given observation\n        p = [] \n        for tag in T:\n            if key == 0:\n                transition_p = tags_df.loc['.', tag]\n            else:\n                transition_p = tags_df.loc[state[-1], tag]\n                \n            # compute emission and state probabilities\n            emission_p = word_given_tag(words[key], tag)[0]\/word_given_tag(words[key], tag)[1]\n            state_probability = emission_p * transition_p    \n            p.append(state_probability)\n            \n        pmax = max(p)\n        # getting state for which probability is maximum\n        state_max = T[p.index(pmax)] \n        state.append(state_max)\n    return list(zip(words, state))\n\n","4ececc40":"# Running on entire test dataset would take more than 3-4hrs. \n# Let's test our Viterbi algorithm on a few sample sentences of test dataset\n\nrandom.seed(1234)\n\n# choose random 5 sents\nrndom = [random.randint(1,len(test_set)) for x in range(5)]\n\n# list of sents\ntest_run = [test_set[i] for i in rndom]\n\n# list of tagged words\ntest_run_base = [tup for sent in test_run for tup in sent]\n\n# list of untagged words\ntest_tagged_words = [tup[0] for sent in test_run for tup in sent]\ntest_run","622088b7":"# tagging the test sentences\nstart = time.time()\ntagged_seq = Viterbi(test_tagged_words)\nend = time.time()\ndifference = end-start","f4307d47":"print(\"Time taken in seconds: \", difference)\nprint(tagged_seq)\n#print(test_run_base)","7363a4d8":"# accuracy\ncheck = [i for i, j in zip(tagged_seq, test_run_base) if i == j] ","0a3f6d6d":"accuracy = len(check)\/len(tagged_seq)","e80b010b":"accuracy","30c8abd8":"incorrect_tagged_cases = [[test_run_base[i-1],j] for i, j in enumerate(zip(tagged_seq, test_run_base)) if j[0]!=j[1]]","bc7d74c3":"incorrect_tagged_cases","5275f070":"## Testing\nsentence_test = 'Twitter is the best networking social site. Man is a social animal. Data science is an emerging field. Data science jobs are high in demand.'\nwords = word_tokenize(sentence_test)\n\nstart = time.time()\ntagged_seq = Viterbi(words)\nend = time.time()\ndifference = end-start","b9c709c8":"print(tagged_seq)\nprint(difference)","7199b356":"## 2. POS Tagging Algorithm - HMM\n\nWe'll use the HMM algorithm to tag the words. Given a sequence of words to be tagged, the task is to assign the most probable tag to the word. \n\nIn other words, to every word w, assign the tag t that maximises the likelihood P(t\/w). Since P(t\/w) = P(w\/t). P(t) \/ P(w), after ignoring P(w), we have to compute P(w\/t) and P(t).\n\n\nP(w\/t) is basically the probability that given a tag (say NN), what is the probability of it being w (say 'building'). This can be computed by computing the fraction of all NNs which are equal to w, i.e. \n\nP(w\/t) = count(w, t) \/ count(t). \n\n\nThe term P(t) is the probability of tag t, and in a tagging task, we assume that a tag will depend only on the previous tag. In other words, the probability of a tag being NN will depend only on the previous tag t(n-1). So for e.g. if t(n-1) is a JJ, then t(n) is likely to be an NN since adjectives often precede a noun (blue coat, tall building etc.).\n\n\nGiven the penn treebank tagged dataset, we can compute the two terms P(w\/t) and P(t) and store them in two large matrices. The matrix of P(w\/t) will be sparse, since each word will not be seen with most tags ever, and those terms will thus be zero. \n","36085405":"## POS Tagging, HMMs, Viterbi\n\nLet's learn how to do POS tagging by Viterbi Heuristic using tagged Treebank corpus. Before going through the code, let's first understand the pseudo-code for the same. \n\n1. Tagged Treebank corpus is available (Sample data to training and test data set)\n   - Basic text and structure exploration\n2. Creating HMM model on the tagged data set.\n   - Calculating Emission Probabaility: P(observation|state)\n   - Calculating Transition Probability: P(state2|state1)\n3. Developing algorithm for Viterbi Heuristic\n4. Checking accuracy on the test data set\n\n\n## 1. Exploring Treebank Tagged Corpus","a13ec107":"### Emission Probabilities","1b422500":"## 4. Evaluating on Test Set","972d3412":"### Transition Probabilities","d8b8e423":"## 3. Viterbi Algorithm\n\nLet's now use the computed probabilities P(w, tag) and P(t2, t1) to assign tags to each word in the document. We'll run through each word w and compute P(tag\/w)=P(w\/tag).P(tag) for each tag in the tag set, and then assign the tag having the max P(tag\/w).\n\nWe'll store the assigned tags in a list of tuples, similar to the list 'train_tagged_words'. Each tuple will be a (token, assigned_tag). As we progress further in the list, each tag to be assigned will use the tag of the previous token.\n\nNote: P(tag|start) = P(tag|'.') "}}