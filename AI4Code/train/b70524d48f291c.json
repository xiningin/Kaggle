{"cell_type":{"cac3a8c1":"code","92ed8450":"code","fbb3909b":"code","7072c14d":"code","cf042127":"code","74225bf7":"code","eca8708f":"code","5b6c6651":"code","18f0885e":"code","38754bb2":"code","49025087":"code","be4f6f3a":"code","71bb3e38":"code","e692abe3":"code","00fcf3b2":"code","1ebe6bc1":"code","66b960ca":"code","ee08c438":"code","5b45bd51":"code","4b687e49":"code","ff386fc7":"code","953cd2b9":"code","233f7834":"code","353410d1":"code","65e406a2":"code","a1d63088":"code","2d96f12c":"code","a7d6f121":"code","7fedee47":"code","f32aee41":"code","d5832287":"code","d05436c1":"code","44d6b3b9":"code","ace6881f":"code","2e88a0fe":"code","ee5ab0ca":"code","8d9421d5":"code","4c83b03e":"code","b321ab39":"code","6df8b1c4":"code","131d1c4f":"code","e7c68264":"code","2c39c4ff":"code","8a7b05e9":"code","a8c248db":"markdown","b1072135":"markdown","5d0f98b3":"markdown","699758eb":"markdown","c83a2c00":"markdown","cf1acd7d":"markdown","1275722b":"markdown","6a6858af":"markdown","8d67847a":"markdown","ceb47a5a":"markdown","66d86034":"markdown","481ff228":"markdown","eb61c33f":"markdown","5df18484":"markdown","118f006b":"markdown","a9c35ce3":"markdown","fa3efec3":"markdown","3f1e60fb":"markdown","5471f630":"markdown","ed30c37f":"markdown","b377b587":"markdown","46f775aa":"markdown","e1f04931":"markdown"},"source":{"cac3a8c1":"# set up environment\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set(style='whitegrid')\nimport time\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom matplotlib import cm\nfrom tqdm import tqdm\nfrom Bio.Seq import Seq\n\n# Sklearn Module\nfrom sklearn.model_selection import KFold,GroupKFold,GridSearchCV,StratifiedKFold\nfrom sklearn.metrics import confusion_matrix,balanced_accuracy_score\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer,RobustScaler\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\nfrom sklearn.model_selection import learning_curve,ShuffleSplit\nfrom sklearn.feature_selection import SelectKBest,f_regression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn import preprocessing\nfrom imblearn.over_sampling import SMOTE, SMOTENC, SMOTEN\n\n# Machine Learning Models\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom catboost import CatBoostClassifier,CatBoostRegressor\nfrom xgboost import XGBRegressor,XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nimport os,warnings;warnings.filterwarnings(\"ignore\")","92ed8450":"import plotly.express as px\nimport plotly.graph_objects as go\nfrom collections import Counter\n\ndef dict_sum(dictlist):\n  outdic = {}\n  for d in dictlist:\n    for k in d.keys():\n      outdic[k] = 0\n  for d in dictlist:\n    for k in d.keys():\n      outdic[k]+=d[k]\n  return outdic\n\n# Class for Sequence Operations \nclass SQ: \n    \n    def __init__ (self, seq=None, seq_type = \"DNA\"): \n        self.seq = seq.upper()\n        self.seq_type = seq_type\n          \n    # class instance operations\n    def __len__(self):\n        return len(self.seq)\n    def __getitem__(self, n):\n        return self.seq[n]\n    def __getslice__(self, i, j):\n        return self.seq[i:j]\n    def __str__(self):\n        return self.seq\n    \n    # Frequency of Sybols \n    def freq(self,compare=None,show_id='perc',fheight=None,fwidth=None):\n        \n        if(compare is not None):\n            if(self.seq_type != compare.seq_type):\n                print('sequences are not of same type')\n                return None\n            \n        c1 = dict(Counter(self.seq))  # abc counter for s1\n        if(compare is not None):\n            c2 = dict(Counter(compare))  # abc counter for s2\n            \n        abc = list(self.abc())\n        count = Counter(abc)\n        abc_c = dict(Counter({x:0 for x in count}))\n        \n        c_all1 = dict_sum([c1,abc_c])\n        if(compare is not None):\n            c_all2 = dict_sum([c2,abc_c])    \n\n        lst = []\n        for i in c_all1.keys():\n           if(self.seq_type == 'DNA' or self.seq_type == 'mRNA'):\n               lst.append(dic_map('iupac_nucleotide',i))\n           elif(self.seq_type == 'PROTEIN'):\n               lst.append(dic_map('iupac_amino',i))\n                \n        if(compare is not None):\n            lst2 = []\n            for i in c_all2.keys():\n               if(self.seq_type == 'DNA' or self.seq_type == 'mRNA'):\n                   lst2.append(dic_map('iupac_nucleotide',i))\n               elif(self.seq_type == 'PROTEIN'):\n                   lst2.append(dic_map('iupac_amino',i))\n          \n        perc = [round(x \/ len(self.seq),3) for x in [*c_all1.values()]]\n        if(show_id is 'perc'):\n            show1 = lst; show2 = perc\n        elif(show_id is 'count'):\n            show1 = lst; show2 = [*c_all1.values()]\n        fig = go.Figure(go.Bar(y=show1,x=show2,\n                               marker_color='rgb(26, 118, 255)',\n                               orientation='h',text=show2,\n                               textposition='outside',name='SEQ1'))\n        if(compare is not None):\n            perc = [round(x \/ len(compare),3) for x in [*c_all2.values()]]\n            if(show_id is 'perc'):\n                show1 = lst2; show2 = perc\n            elif(show_id is 'count'):\n                show1 = lst2; show2 = [*c_all2.values()]\n            fig.add_trace(go.Bar(y=show1,x=show2,marker_color='rgb(55, 83, 109)',\n                                 orientation='h',text=show2,\n                                 textposition='outside',name='SEQ2'))\n        fig.update_layout(template='plotly_white',height=fheight,width=fwidth,\n                         title=f'<b>{self.seq_type} SEQUENCE CONTENT<\/b>',\n                         font=dict(family='sans-serif',size=12),\n                         margin=dict(l=40, r=40, t=50, b=10))\n        fig.show()\n\n    # Return % GC Nucleotides\n    def gc(self):\n        if (self.seq_type == \"DNA\" or self.seq_type == \"mRNA\"):\n            ii = 0\n            for s in self.seq:\n                if(s in \"GCgc\"):\n                    ii += 1\n            return round(ii \/ len(self.seq),4)\n        else:\n            return None\n        \n    # General Sequence Info\n    def get_seq_biotype (self):\n        return self.seq_type\n    def info(self):\n        print (f\"SEQ: {self.seq}\" +\" \"+ f\"TYPE: {self.seq_type}\")\n        \n    # Get ABC\n    def abc(self):\n        if(self.seq_type==\"DNA\"): \n          return \"ACGT\"\n        elif(self.seq_type==\"mRNA\"):\n          return \"ACGU\"\n        elif (self.seq_type==\"PROTEIN\"): \n          return \"ACDEFGHIKLMNPQRSTVWY\"\n        else: \n          return None\n        \n    # Check Validity\n    def validate(self,verbose=False):\n        alp = self.abc()\n        res = True; i = 0\n        while (res and i < len(self.seq)):\n            if self.seq[i] not in alp: \n                res = False\n            else: i += 1\n        if(res):\n            if(verbose):\n                print(f'{self.seq_type} is valid')\n            return res\n        else:\n            if(verbose):\n                print(f'{self.seq_type} is invalid')\n            return res\n        \n    # Transcription \n    def transcription(self):\n        if (self.seq_type == \"DNA\"):\n            return SQ(self.seq.replace(\"T\",\"U\"), \"mRNA\")\n        else:\n            return None\n    \n    # Reverse Compliment\n    def reverse_comp(self):\n        \n        if (self.seq_type != \"DNA\"): \n            print('input not DNA')\n            return None\n    \n        lst_seq = ['A','T','G','C']\n        lst_comp = ['T','A','C','G']\n            \n        comp = ''\n        for char in self.seq:\n            ii=-1\n            for c in lst_seq:\n                ii+=1\n                if(char == c ):\n                    comp = lst_comp[ii] + comp\n            \n        return SQ(comp, \"DNA\")\n        \n    # Translate \n    @staticmethod\n    def translate(seq,p0=0):\n        seq_aa = \"\"\n        for pos in range(p0,len(seq)-2,3):\n            cod = seq[pos:pos+3]\n            seq_aa += dic_map(map_id='codon',tid=cod)\n        return seq_aa\n    \n    '''Get All Possible open reading frames (ORF)'''\n    # store all possible collections of amino acid groups \n    # in all 6 frames\n    def frames(self):\n        res = []\n        for i in range(0,3):\n            res.append(self.translate(self.seq,i))\n        rc = self.reverse_comp()\n        for i in range(0,3):\n            res.append(self.translate(rc,i)) \n        return res\n    \n    '''Computes all possible proteins in an amino acid sequence in reading frame '''\n    # using the knowledge that it starts with M and ends with _, \n    # filter out rule breaking ORFs\n    @staticmethod\n    def all_proteins_RF(aa_seq):\n        # aa_seq -> converted ORF\n        current_prot = []\n        proteins = []\n        for aa in aa_seq:\n            if(aa == \"_\"):\n                if current_prot:\n                    for p in current_prot:\n                        proteins.append(p)\n                    current_prot = []\n            else:\n                if(aa == \"M\"):\n                    current_prot.append(\"\")\n                for i in range(len(current_prot)):\n                    current_prot[i] += aa\n        return proteins\n    \n    '''Computes all possible proteins for all ORF'''\n    # and sort them based on size\n    def ORF_protein(self, mins = 0):\n        \n        # order \n        def insert_prot_ord (prot, list_prots):\n            i = 0\n            while i < len(list_prots) and len(prot) < len(list_prots[i]):        \n                i += 1\n            list_prots.insert(i, prot)\n        \n        rfs = self.frames()  # get all ORF conversions\n        res = []\n        for rf in rfs:\n            print(rf)\n            prots = self.all_proteins_RF(rf) # return only protein cases\n            # additionally sort based on protein size\n            for p in prots: \n                if len(p) > mins: \n                    insert_prot_ord(p, res)\n        return res\n    \n# Plot Correlation to Target Variable only\ndef corrMat(df,target='demand',figsize=(9,0.5),ret_id=False):\n    \n    corr_mat = df.corr().round(2);shape = corr_mat.shape[0]\n    corr_mat = corr_mat.transpose()\n    corr = corr_mat.loc[:, df.columns == target].transpose().copy()\n    \n    if(ret_id is False):\n        f, ax = plt.subplots(figsize=figsize)\n        sns.heatmap(corr,vmin=-0.3,vmax=0.3,center=0, \n                     cmap=cmap,square=False,lw=2,annot=True,cbar=False)\n        plt.title(f'Feature Correlation to {target}')\n    \n    if(ret_id):\n        return corr","fbb3909b":"import pandas as pd\nimport numpy as np\nimport os\n\n''' Align Metadata Target values w\/ Unitig File & Compile Feature Matrix '''\nclass get_unitigs:\n    \n    def __init__(self,verbose=True):\n        self.df = pd.read_csv('..\/input\/gono-unitigs\/metadata.csv', index_col=0) # metadata\n        self.meta_names = self.df.columns \n        self.target_name = None\n        self.verbose = verbose\n    \n    # Get Unitig Feature matrix & Target Vector\n    def get_case(self,phenotype=None):\n    \n        self.target_name = phenotype\n        _metadata = self.df\n        if(self.verbose):\n            print(f'Target Antibiotic: {self.target_name}')\n            print(f'Metadata df: {_metadata.shape}')\n        \n        # remove those that don't contain target values\n        _metadata = _metadata.dropna(subset=[phenotype])\n        self.metadata = _metadata.copy()\n        \n        if(self.verbose):\n            print(f'Metadata df after na() removal {_metadata.shape}')\n        _metadata = _metadata[phenotype] # choose target variable \n        \n        prefix = '..\/input\/gono-unitigs\/'\n        suffix = '_gwas_filtered_unitigs.Rtab'\n        \n        if(self.verbose):\n            print('\\nCombining Metadata & Unitigs')\n        \n        # unitig feature matrix for phenotype\n        tdf = pd.read_csv(prefix + phenotype + suffix, sep=\" \", \n                          index_col=0, low_memory=False)\n        # align column data w\/ metadata df (pattern_id = sample_idd)\n        tdf = tdf.T \n        # keep only common rows, ie. that have resistence measure]\n        tdf = tdf[tdf.index.isin(_metadata.index)] \n        \n        train = tdf\n        target = _metadata[_metadata.index.isin(tdf.index)]\n\n        self.X = pd.concat([train,target],axis=1)\n        if(self.verbose):\n            print(f'Unitig Matrix (+target): {self.X.shape}')","7072c14d":"# load meta data & look at the first 5 samples\ncase_cip = get_unitigs()\ndisplay(case_cip.df.T.iloc[:,:7])","cf042127":"case_cip.get_case(phenotype='cip_sr')\ncase_cip.X.iloc[:,:2].head()","74225bf7":"target = case_cip.X[case_cip.target_name]\nprint(target[:5])","eca8708f":"''' Ciprofloxacin '''\n# 8873 Unitigs\n\ncase = get_unitigs()\ncase.get_case('cip_sr')\nprint(case.X[case.target_name].value_counts())","5b6c6651":"''' Azithromycin '''\n# 515 unitigs\n\ncase = get_unitigs()\ncase.get_case('azm_sr')\nprint(case.X[case.target_name].value_counts())","18f0885e":"''' Cefixime '''\n# 384 unitigs\n\ncase = get_unitigs()\ncase.get_case('cfx_sr')\nprint(case.X[case.target_name].value_counts())","38754bb2":"''' Feature Matrix Upsampling Modification (+Target) '''\n# Model based approach to upsample minor class in target variable\n\nclass mod_unitigs():\n    \n    def __init__(self,unitigs):\n        self.X = unitigs.X # input data class\n        self.target_name = unitigs.target_name\n        self.verbose = True\n\n    ''' Downsampling Class 0 using .sample & recompile '''\n    # If there's too much of the dominant class, just downsample\n    \n    def split_case(self,frac_id=0.5):\n        \n        X = self.train\n        y = pd.Series(self.target,name=self.target_name)\n        XX = pd.concat([X,y],axis=1)\n        \n        lst_temp = dict(tuple(XX.groupby(self.target_name))) # divide classes\n        ratio = lst_temp[0].shape[0]\/lst_temp[1].shape[0] # get class ratio\n        \n        # Sample approach for downsizing majority class\n        X_red = lst_temp[0].sample(frac=frac_id)\n        X_all = pd.concat([X_red,lst_temp[1]],axis=0)\n        \n        if(self.verbose):\n            print(f'Class 0 : {lst_temp[0].shape}')\n            print(f'Class 1 : {lst_temp[1].shape}')\n            print(f'Class Ratio: {round(ratio,4)}')\n            print(f'Reduced Training Matrix: {X_all.shape}')\n        \n        # Redefine .train, .target\n        self.target = X_all[self.target_name].copy()\n        X_all.drop(self.target_name, inplace=True, axis=1)\n        self.train = X_all\n        \n    ''' SMOTE UPSAMPLING '''\n    # For unbalanced problems, synthetically\/model new data\n        \n    def smote(self,smote_id = 'smotenc',\n                   smote_strat=0.5,\n                   k_neighbours=5):\n        \n        self.smote_id = smote_id\n        self.smote_strat = smote_strat\n        self.smote_nbr = k_neighbours\n        \n        y = self.X[self.target_name].copy()\n        X = self.X.drop([self.target_name],axis=1).copy()\n    \n        # smote for contin, smotenc for category\n        if(self.smote_id is 'smote'):\n            model_id = SMOTE(sampling_strategy=self.smote_strat,\n                             k_neighbors=self.smote_nbr)\n        elif(self.smote_id is 'smotenc'):\n            model_id = SMOTENC(sampling_strategy=self.smote_strat,\n                               k_neighbors=self.smote_nbr,\n                               categorical_features=[0,1])\n        \n        X_mod, y_mod = model_id.fit_resample(X,y)\n        self.X = pd.concat([X_mod,y_mod],axis=1)\n        \n        if(self.verbose):\n            print(f'\\nSMOTE Upsampling: {self.X.shape}')\n            print(f'Target Value Counts: \\n{pd.Series(y_mod).value_counts()}')\n        self.X = pd.concat([X_mod,y_mod],axis=1)","49025087":"lst_azm = ['Year','Continent','Beta.lactamase','azm_mic','Azithromycin','azm_sr']\nlst_cip = ['Year','Continent','Beta.lactamase','cip_mic','Ciprofloxacin','cip_sr']\nlst_cfx = ['Year','Continent','Beta.lactamase','cfx_mic','Cefixime','cfx_sr']\nlst_antibio = [lst_azm,lst_cip,lst_cfx]\n\n# Plot Parallel Categories Plot\ndef plot_pp(lst,colour='ghostwhite'):\n    tdf = get_unitigs().df[lst]\n    tdf.dropna(inplace=True)\n    tdf.sort_values(by=lst[-3],inplace=True,ascending=False)\n    tdf['Year'] = tdf['Year'].astype(str)\n    fig = px.parallel_categories(tdf)\n    fig.update_traces(patch={\"line\": {\"color\":colour,'shape':'hspline'}})\n    fig.update_layout(title=f'Bacteria Resistance to {lst[-2]}')\n    fig.update_layout(margin=dict(t=60,b=10),height=400)\n    fig.show()","be4f6f3a":"for i in lst_antibio:\n    plot_pp(i,'mistyrose')","71bb3e38":"# For the purposes of plotting; require standard codes for plotly chloropeth\ncountry_map = {'UK':'GBR','Canada':'CAN','France':'FRA','Spain':'SPA','Austria':'AUT',\n               'Italy':'ITA','Slovakia':'SVK','Norway':'NWY','Slovenia':'SVN',\n               'Hungary':'HUN','The Netherlands':'NED','Belgium':'BEL','Greece':'GRC',\n               'Sweden':'SWE','Denmark':'DNK','Germany':'GER','Latvia':'LAT',\n               'Iceland':'ICL','Cyprus':'CYP','Malta':'MLT','Portugal':'PRT',\n               'Brasil':'BRA','Chile':'CHL','Ecuador':'ECU','USA':'USA','Australia':'AUS',\n               'Caribbean':'XXX','Cuba':'CUB','Poland':'POL','Romania':'ROU',\n               'Russia':'RUS','Turkey':'TUR','Jamaica':'JAM','Lithuania':'LTU',\n               'Scotland':'SCO','Argentina':'ARG','Thailand':'THA','Japan':'JPN',\n               'New_Zealand':'NZL','India':'IND','China':'CHN','Indonesia':'IDN',\n               'Philippines':'PHL','Vietnam':'VNM','Pakistan':'PAK','Bhutan':'BTN',\n               'Estonia':'EST','Belarus':'BLR','Armenia':'ARM','Bulgaria':'BUL',\n               'Finland':'FIN','HongKong':'HKG','Malaysia':'MYS','Gambia':'GMB',\n               'South_Africa':'ZAF','Tanzania':'TZA','CaboVerde':'CPV','Marocko':'MAR',\n               'Uganda':'UGA','Angola':'AGO','Guinea':'GIN','GuineaBissau':\n               'GNB','Saudi_Arabia':'SAU','Ivory_Coast':'CIV','Ireland':'IRL'}","e692abe3":"lst_azm = ['Year','Country','Continent','Beta.lactamase','azm_mic','Azithromycin','azm_sr']\nlst_cip = ['Year','Country','Continent','Beta.lactamase','cip_mic','Ciprofloxacin','cip_sr']\nlst_cfx = ['Year','Country','Continent','Beta.lactamase','cfx_mic','Cefixime','cfx_sr']\nlst_antibio = [lst_azm,lst_cip,lst_cfx]\n\ndef plot_geomean(lst):\n    \n    global country_map\n\n    tdf = get_unitigs().df[lst]\n    tdf.dropna(inplace=True)\n    tdf.sort_values(by=lst[-3],inplace=True,ascending=False)\n    tdf['Year'] = tdf['Year'].astype(str)\n    \n    tdf['Country'] = tdf['Country'].map(country_map)\n    tdf2 = tdf.groupby(['Country']).mean()\n\n    fig = go.Figure(data=go.Choropleth(\n        locations = tdf2.index,\n        z = tdf2[lst[-1]],\n        colorscale = 'magenta',\n        autocolorscale=False,\n        reversescale=False,\n        marker_line_color='black',\n        marker_line_width=0.5,\n        colorbar_title = f'{lst[-1]}'))\n\n    fig.update_layout(title=f'Bacteria Resistance to {lst[-2]}',\n                      geo=dict(showframe=False,showcoastlines=False,\n                               projection_type='equirectangular'))\n    fig.update_layout(margin=dict(t=60,b=10),height=400)\n    fig.show()","00fcf3b2":"# Plot Choropleth Map\nfor i in lst_antibio:\n    plot_geomean(i)","1ebe6bc1":"def get_mic():\n    \n    lst_cases = ['azm_mic','cip_mic','cfx_mic']\n    rtabs = ['azm_sr','cip_sr','cfx_sr']\n    lst_temp = []\n    \n    ii=-1\n    for case in lst_cases:\n        \n        ii+=1\n        case_id = get_unitigs(verbose=False)\n        case_id.get_case(rtabs[ii])\n        \n        X_all = pd.concat([case_id.X,case_id.metadata],axis=1)\n        \n        new_df = X_all[case].value_counts().rename_axis(case).reset_index(name='counts')\n        new_df = new_df.rename(columns={new_df.columns[0]: 'mic'})\n        new_df['case'] = case                \n        lst_temp.append(new_df)\n        \n    X_counts = pd.concat([lst_temp[0],lst_temp[1],lst_temp[2]],axis=0)\n    X_counts.sort_values(by='mic',inplace=True,ascending=True)\n    X_counts['mic'] = X_counts['mic'].astype(str)\n\n    fig = px.bar(X_counts, x='mic',y='counts',color='case')\n    fig.update_layout(template='plotly_white',height=300)\n    fig.show()","66b960ca":"get_mic()","ee08c438":"column_values = case.X[case.target_name].values.ravel()\nprint(pd.unique(column_values))","5b45bd51":"case_unitigs = case.X.columns.tolist()\n\nii=-1\nfor i in case_unitigs:\n    if(',' in i):\n        ii+=1;print(f'{ii} | {i}')","4b687e49":"# Using SQ() Class we can define sequences\n\nlst_SQ = []\nfor unitig in case_unitigs:\n    lst_SQ.append(SQ(unitig,'DNA'))\n    \nprint(type(lst_SQ[4]))\n\n# Using BioPython we can define sequences\n\nlst_bSQ = []\nfor unitig in case_unitigs:\n    lst_bSQ.append(Seq(unitig))\n    \nprint(type(lst_bSQ[4]))","ff386fc7":"# models use in kfold\nmodels = []\nmodels.append(\n    ('LDA', LinearDiscriminantAnalysis()))  \n\n# models use in gscv\ncv_lda = {'solver':\n              ['svd', 'lsqr', 'eigen']}\n\nmodels = []\nmodels.append(\n    ('LDA', LinearDiscriminantAnalysis(),cv_lda))  ","953cd2b9":"class class_eval():\n    \n    def __init__(self,data=None,models=None,nfold = 4, \n                 gsfold=3, verbose=True,\n                 shuffle_id=False,random_state=22):\n\n        self.shuffle = shuffle_id\n\n        if(self.shuffle is False):\n            self.rs = None\n        else:\n            self.rs = random_state # random state\n        \n        if(data is not None):\n            X_all = data.X\n            self.y = data.X[data.target_name].copy()\n            self.X = data.X.drop([data.target_name],axis=1).copy()\n        else:\n            print('input case data')\n\n        self.models = models # Tuple \/w (name,model)\n        self.nfold = nfold       # number of cross validation folds \n        self.gsfold = gsfold    # number of grid search folds in every fold\n        self.target_name = data.target_name  # target name\n        \n    ''' Grid-Search Standard Kfold Cross Validation '''\n    # For each fold, a grid search cv is applied to find best params\n    \n    def gscv(self,type_id='kfold'):\n        \n        self.store_models = {} # Store models of Kfolds\n        self.dic_tr = {}\n        self.dic_te = {}\n        \n        for model in self.models:\n            \n            if(type_id is 'skfold'):\n                kfold = StratifiedKFold(n_splits=self.nfold,\n                                        shuffle=self.shuffle,\n                                        random_state=self.rs)\n            elif(type_id is 'kfold'):\n                kfold = KFold(n_splits=self.nfold,\n                              shuffle=self.shuffle,\n                              random_state=self.rs)\n        \n            lst_temp = []; lst_temp2 = []; kfold_id = -1\n            for train_index, test_index in kfold.split(self.X,self.y):\n    \n                kfold_id+=1;print(f'\\nkfold {kfold_id}')\n                # split data into train\/test sets\n                X_train = self.X.iloc[train_index]\n                y_train = self.y[train_index]\n                X_test = self.X.iloc[test_index]\n                y_test = self.y[test_index]\n\n                # perform grid search to identify best hyper-parameters\n                gs_clf = GridSearchCV(model[1],param_grid=model[2], \n                                      cv=self.gsfold, n_jobs=-1, \n                                      scoring='balanced_accuracy')\n                \n                # Train using the best model \n                gs_clf.fit(X_train, y_train)\n                best_model = gs_clf.best_estimator_ # best cv model\n                self.store_models[f'GS_{model[0]}_{kfold_id}'] = best_model\n                \n                # Predict using best model\n                ym_tr = gs_clf.predict(X_train)\n                ym_te = gs_clf.predict(X_test)\n                score_tr = balanced_accuracy_score(y_train,ym_tr)\n                score_te = balanced_accuracy_score(y_test,ym_te)\n                lst_temp.append(score_tr); lst_temp2.append(score_te)\n            \n                print(f'Train Score: {round(score_tr,4)} - Test Score: {round(score_te,4)}')\n                print('Best hyperparameters for this fold')\n                print(gs_clf.best_params_)\n                print(f\"Test : Confusion matrix Fold {kfold_id}\")\n                print(confusion_matrix(y_test, ym_te))\n                \n            self.dic_tr[model[0]] = lst_temp\n            self.dic_te[model[0]] = lst_temp2\n            test_mean = round(sum(lst_temp2)\/len(lst_temp2),4)\n            print(f'\\n {model[0]} - Test Mean Score: {test_mean}')\n        \n    '''K-Fold Cross Validation'''\n    # w\/ type_id option; statified used to make sure classes are balanced in folds\n    \n    def cv(self,type_id='kfold'):\n        \n        print(f'type_id set to: {type_id}')\n        self.store_models = {} # Store models of Kfolds\n        self.dic_tr = {}\n        self.dic_te = {}\n    \n        # Cycle though all tuple model settings\n        for model in self.models:\n            \n            t0 = time.time()\n            if(type_id is 'skfold'):\n                kfold = StratifiedKFold(n_splits=self.nfold,\n                                        shuffle=self.shuffle,\n                                        random_state=self.rs)\n            elif(type_id is 'kfold'):\n                kfold = KFold(n_splits=self.nfold,\n                              shuffle=self.shuffle,\n                              random_state=self.rs)\n                \n            lst_temp = []; lst_temp2 = []; kfold_id = -1\n            for train_index, test_index in kfold.split(self.X,self.y):\n\n                kfold_id+=1              \n                # split data into train\/test sets\n                X_train = self.X.iloc[train_index]\n                y_train = self.y[train_index]\n                X_test = self.X.iloc[test_index]\n                y_test = self.y[test_index]\n\n                # Fit, Stopre & Predict Kfold models\n                model[1].fit(X_train, y_train)\n                self.store_models[f'{model[0]}_{kfold_id}'] = model[1]\n                ym_tr = model[1].predict(X_train)\n                ym_te = model[1].predict(X_test)\n                \n                score_tr = balanced_accuracy_score(y_train,ym_tr)\n                score_te = balanced_accuracy_score(y_test,ym_te)\n                lst_temp.append(score_tr); lst_temp2.append(score_te)\n\n            self.dic_tr[model[0]] = lst_temp\n            self.dic_te[model[0]] = lst_temp2\n            test_mean = round(sum(lst_temp2)\/len(lst_temp2),4)\n            print(f'{model[0]} - Test Mean Score: {test_mean} - Total Time: {round(time.time() - t0,4)}')\n            \n    ''' unitig visual functions only '''\n    \n    def col_trans(self,sel_id=0):\n        \n        # unitig to abrev\n        if(sel_id is 0):\n            self.X_names = self.X.columns.tolist()\n            temp_names = self.X_names.copy()\n\n            unitigs = self.X_names.copy()\n            del unitigs[-1]\n\n            lst_abr = []\n            ii=-1\n            for unitig in range(0,len(unitigs)):\n                ii+=1;lst_abr.append(f'u{ii}')\n            lst_abr.append(self.target_name)\n            self.X.columns = lst_abr\n            self.dicabr = dict(zip(lst_abr,self.X_names))\n            \n        # abreb to unitig\n        elif(sel_id is 1):\n            self.X.columns = self.X_names\n        \n    ''' PLOT KFOLD RESULTS '''\n    \n    def fold_plot(self):\n            \n        df_tr = pd.DataFrame(self.dic_tr)\n        df_te = pd.DataFrame(self.dic_te)\n\n        # Make Plots\n        fig,ax = plt.subplots(2,2,figsize=(14,6))\n        sns.stripplot(data=df_tr, orient='h',linewidth=1,ax=ax[0,0])\n        sns.heatmap(data=df_tr,annot=True,cbar=False,cmap=\"plasma_r\",\n                    vmax=1,vmin=0.5,ax=ax[0,1])\n        sns.stripplot(data=df_te, orient='h',linewidth=1,ax=ax[1,0])\n        sns.heatmap(data=df_te,annot=True,cbar=False,cmap=\"plasma_r\",\n                    vmax=1,vmin=0.5,fmt='.3g',ax=ax[1,1])\n        ax[0,0].set_xlim([0.5,1.1]);ax[1,0].set_xlim([0.5,1.1])\n        ax[0,0].set_title('train kfold') \n        ax[0,1].set_title('train kfold heatmap')\n        ax[1,0].set_title('test kfold')\n        ax[1,1].set_title('test kfold heatmap')\n        sns.despine(bottom=True, left=True)\n        plt.tight_layout()","233f7834":"''' Define Models used for Testing '''\nmodels = []\nn_est = 10\n\n# Unsupervised Learning Models\nmodels.append(('LDA', LinearDiscriminantAnalysis())) \nmodels.append(('KNN', KNeighborsClassifier()))  \nmodels.append(('NB', GaussianNB()))\n\n# # Supervised Learning Models\nmodels.append(('SVC',SVC()))\nmodels.append(('TREE', DecisionTreeClassifier())) # Supervised Model\nmodels.append(('GBM', GradientBoostingClassifier(n_estimators=n_est)))\n\n# Desirable for Feature Importance Evaluation\nmodels.append(('XGB',XGBClassifier(n_estimators=n_est,verbosity = 0)))\nmodels.append(('CAT',CatBoostClassifier(silent=True,n_estimators=n_est)))\nmodels.append(('RF', RandomForestClassifier(n_estimators=n_est)))","353410d1":"# Get case; (Dataset Feature Class)\ncase = get_unitigs()\ncase.get_case('cip_sr')\n\n# Standard KFOLD evaluation\neval1 = class_eval(data=case, # input the case class\n                   nfold = 4, # 4 fold kfold\n                   models=models) # global models tuple list)\n\n# Evaluate kfold using selected models \neval1.cv(type_id='kfold')  # standard kfold evaluation","65e406a2":"eval1.fold_plot()  # plot kfold results","a1d63088":"''' Tree Based Feature Importance '''\n# requires evaluation class input w\/ at least one of RF, CatBoost & XGB moedls\n# models stored in .store_models are required from eval class\n\nclass fi:\n    \n    def __init__(self,data=None, # evaluation class\n                      sort_by='RF', # show most important features\n                      max_features=10 # limit unitigs to \n                ):\n        \n        if(data is None):\n            print('Enter Evaluation class w\/ CAT,RF,XGB')\n        else:\n            evals = data\n            # check which models are present\n            lst_models = list(evals.store_models.keys())\n            temp = []\n            for i in lst_models:\n                if('CAT' in i):\n                    temp.append('CAT')\n                if('RF' in i):\n                    temp.append('RF')\n                if('XGB' in i):\n                    temp.append('XGB')\n                    \n            # input contains gscv data\n            if('GS' in lst_models[0]):\n                self.gs_id = True\n            else:\n                self.gs_id = False\n                \n        self.lst_tree_models = list(set(temp))\n        \n        self.evals = data  # evaluation class\n        self.lst_Seqs = []  # list of important unitigs\n        self.max_features = max_features # show top n features\n        self.sort_by = sort_by # sort by particualr model fi, other mods show this index only\n        self.abr_feat = False # activate if unitig names are too big for figure\n\n    # Compile all Tree based feature importance results\n    def get(self):\n\n        # USL scaling\n        min_max_scaler = preprocessing.MinMaxScaler()\n        \n        # Recall Model & Get Feature Importance from data class\n        # unless gridsearched, all kfolds are the same model\n        \n        ii=-1\n        # if randomforest models are present\n        if('RF' in self.lst_tree_models):\n            \n            if(self.gs_id):\n                \n                # fold names\n                tlst_models = [f'GS_RF_{i}' for i in range(0,self.evals.nfold)]\n                \n                # stack all fold results\n                for kfold_id in tlst_models: \n                    ii+=1\n                    rf_model = self.evals.store_models[kfold_id]\n                    imp_rf = rf_model.feature_importances_\n                    rf_sc = min_max_scaler.fit_transform(imp_rf[:,None])\n                    ldf = pd.DataFrame(rf_sc,index=self.evals.X.columns,columns=[kfold_id])\n                    if(ii is 0):\n                        df = ldf.copy()\n                    else:\n                        df = pd.concat([df,ldf],axis=1)\n                    \n            else:\n            \n                ii+=1\n                rf_model = self.evals.store_models['RF_1']\n                imp_rf = rf_model.feature_importances_\n                rf_sc = min_max_scaler.fit_transform(imp_rf[:,None])\n                ldf = pd.DataFrame(rf_sc,index=self.evals.X.columns,columns=['RF'])\n                if(ii is 0):\n                    df = ldf.copy()\n                else:\n                    df = pd.concat([df,ldf],axis=1)\n                \n        # if catboost models are present\n        if('CAT' in self.lst_tree_models):\n            \n            if(self.gs_id):\n                \n                # fold names\n                tlst_models = [f'GS_CAT_{i}' for i in range(0,self.evals.nfold)]\n                \n                # stack all fold results\n                for kfold_id in tlst_models: \n                    ii+=1\n                    cb_model = self.evals.store_models[kfold_id]\n                    imp_cb = cb_model.get_feature_importance()\n                    cb_sc = min_max_scaler.fit_transform(imp_cb[:,None])\n                    ldf = pd.DataFrame(cb_sc,index=self.evals.X.columns,columns=[kfold_id])\n                    if(ii is 0):\n                        df = ldf.copy()\n                    else:\n                        df = pd.concat([df,ldf],axis=1)\n                    \n            else:\n                ii+=1\n                cb_model = self.evals.store_models['CAT_1']\n                imp_cb = cb_model.get_feature_importance()\n                cb_sc = min_max_scaler.fit_transform(imp_cb[:,None])\n                ldf = pd.DataFrame(cb_sc,index=self.evals.X.columns,columns=['CB'])\n                if(ii is 0):\n                    df = ldf.copy()\n                else:\n                    df = pd.concat([df,ldf],axis=1)\n                \n            \n        if('XGB' in self.lst_tree_models):\n            \n            if(self.gs_id):\n\n                # fold names\n                tlst_models = [f'GS_XGB_{i}' for i in range(0,self.evals.nfold)]\n\n                # stack all fold results\n                for kfold_id in tlst_models: \n                    ii+=1\n                    xg_model = self.evals.store_models[kfold_id]\n                    imp_xg = xg_model.feature_importances_\n                    xg_sc = min_max_scaler.fit_transform(imp_xg[:,None])\n                    ldf = pd.DataFrame(xg_sc,index=self.evals.X.columns,columns=[kfold_id])\n                    if(ii is 0):\n                        df = ldf.copy()\n                    else:\n                        df = pd.concat([df,ldf],axis=1)\n\n            else:\n\n                ii+=1\n                xg_model = self.evals.store_models['XGB_1']\n                imp_xg = xg_model.feature_importances_\n                xg_sc = min_max_scaler.fit_transform(imp_xg[:,None])\n                ldf = pd.DataFrame(rf_sc,index=self.evals.X.columns,columns=['XGB'])\n                \n                if(ii is 0):\n                    df = ldf.copy()\n                else:\n                    df = pd.concat([df,ldf],axis=1)\n\n        # change to abbrev if names are too long to display\n        if(self.abr_feat):\n            self.evals.col_trans(0)\n        \n        # Sort by one of the available columns\n        df.sort_values(by=self.sort_by,ascending=False,inplace=True)\n\n        if(self.abr_feat):\n                self.evals.col_trans(1)\n\n        # show only most critical features in FI\n        subset = df[:self.max_features]\n        \n#       Store the most important features\n        for i in subset.index.tolist():\n            self.lst_Seqs.append(Seq(i))\n        \n        # Plot features \n        fig = px.bar(subset,orientation='h')\n        fig.update_traces(width=0.5)\n        fig.update_layout(height=400,template='plotly_white',\n                          title=f\"<b>FEATURE IMPORTANCE<\/b> | Sorted by {self.sort_by}()\")\n        fig.show()","2d96f12c":"crit_unitigs = fi(data=eval1,sort_by='CB')\ncrit_unitigs.get()","a7d6f121":"crit_unitigs.lst_Seqs","7fedee47":"# Show default parameters used in catboost model\neval1.store_models['CAT_0'].get_all_params()","f32aee41":"# Define Grid used in Cross Validation\nparams = {'n_estimators':[10,25,40],\n         'learning_rate':[0.01,0.05,0.5]}\n\n# Define Model (just the one) \nmodels = []\nmodels.append(('CAT', CatBoostClassifier(silent=True,\n                                         task_type=\"GPU\"),params))\n\n# Get Dataset Features\ncase2 = get_unitigs()\ncase2.get_case('cip_sr')\n\neval2 = class_eval(data=case,\n                   models=models)\neval2.gscv()","d5832287":"crit_unitigs = fi(data=eval2,sort_by='GS_CAT_0')\ncrit_unitigs.get()","d05436c1":"crit_unitigs.lst_Seqs","44d6b3b9":"case = get_unitigs()\ncase.get_case('cip_sr')\n\nprint(case.X.groupby('cip_sr')['GTGCGACAGCAAAGTCCAAACCAGCGTCCCCGCC'].mean())\nprint(case.X.groupby('cip_sr')['GCGCAGCCGCAAATCTTGTTTTCCCATTCCGCC'].mean())\nprint(case.X.groupby('cip_sr')['AAATTGCGGATCGATGCGCGAAGGGTCGAATGC'].mean())\n\nprint(case.X.groupby('cip_sr')['GGCATCCCGAAGCCGAATACGGCAACGGCAAGCG'].mean())","ace6881f":"''' Define Models used for Testing '''\nmodels = []\nmodels.append(('SVC',SVC(kernel = 'linear')))\n\n# Get case; (Dataset Feature Class)\ncase = get_unitigs()\ncase.get_case('cip_sr')\n\n# Standard KFOLD evaluation\neval_svc1 = class_eval(data=case, # input the case class\n                   nfold = 4, # 4 fold kfold\n                   models=models) # global models tuple list)\n\n# Evaluate kfold using selected models \neval_svc1.cv(type_id='kfold')  # standard kfold evaluation","2e88a0fe":"''' Function plots & returns highest weighted features '''\n# for SVC linear covariance function model in SQ sequence format\n\ndef fi_svc(classifier, feature_names, top_features=5,verbose=False):\n    \n    coef = classifier.coef_.ravel()\n    top_positive_coefficients = np.argsort(coef)[-top_features:]\n    top_negative_coefficients = np.argsort(coef)[:top_features]\n    top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n\n    # plt.title(\"Feature Importances (Support Vector Machine) - Ciprofloxacin Resistance\", y=1.08)\n    colors = ['crimson' if c < 0 else 'cornflowerblue' for c in coef[top_coefficients]]\n    feature_names = np.array(feature_names)\n    lser = pd.Series(data=coef[top_coefficients],index=feature_names[top_coefficients])\n    fig = px.bar(lser,orientation='h')\n    fig.update_traces(width=0.5)\n    fig.update_layout(height=350,template='plotly_white',showlegend=False,\n                        title=f\"<b>FEATURE IMPORTANCE<\/b> | SVC\")\n    fig.show()\n    \n    # if we print the unitigs, we can then look at what genes they relate to\n    top_negative_coefficients = np.argsort(coef)[:5]\n    neg_predictors = np.asarray(feature_names)[top_negative_coefficients]\n    top_positive_coefficients = np.argsort(coef)[-5:]\n    pos_predictors = np.asarray(feature_names)[top_positive_coefficients]\n    if(verbose):\n        print(\"Top negative predictors: \",neg_predictors)\n        print(\"Top positive predictors: \",pos_predictors)\n    \n    # Store the most important features\n    top_negSeq = []; top_posSeq = []\n    for i in range(0,top_features):\n        top_negSeq.append(Seq(neg_predictors[i]))\n        top_posSeq.append(Seq(pos_predictors[i]))\n        \n    return top_negSeq, top_posSeq ","ee5ab0ca":"eval_svc1.store_models","8d9421d5":"neg_predictors,pos_predictors = fi_svc(eval_svc1.store_models['SVC_3'], list(eval_svc1.X.columns))","4c83b03e":"# return list of sequences list ordered from most to least important\nneg_predictors ","b321ab39":"# return list of sequences ordered from least to most important\npos_predictors ","6df8b1c4":"svm_params = {\n    'C': [0.01],\n    'gamma': [1e-06, 1e-05],\n    'kernel': ['linear']\n}\n\n# Define Model (just the one) \nsvm = SVC(class_weight='balanced')\nmodels = []\nmodels.append(('SVM',svm,svm_params))\n\n# Get Dataset Features\ncase = get_unitigs()\ncase.get_case('cip_sr')\n\neval_svc2 = class_eval(data=case,\n                   models=models)\neval_svc2.gscv()","131d1c4f":"eval_svc2.store_models","e7c68264":"neg_predictors,pos_predictors = fi_svc(eval_svc2.store_models['GS_SVM_0'], list(eval_svc2.X.columns))","2c39c4ff":"# return list of sequences list ordered from most to least important\nneg_predictors ","8a7b05e9":"# return list of sequences ordered from least to most important\npos_predictors ","a8c248db":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#323232;\n       font-size:220%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:white;\">\n        <b>3.3 | MIC VALUES OF ALL SAMPLES<\/b>\n    <\/p>\n<\/div>\n\n- <b><span style='color:#E888BB'>MIC<\/span><\/b> : a measure of the <b><span style='color:#E888BB'>concentration of antibiotic<\/span><\/b> bacteria can tolerate before it impairs their growth.\n- We can definitely note a correlation to <b>_mic<\/b> features for <b>antibiotic resistance<\/b> in this graph alone:\n   > - Only small quantities of <b>Cefixime<\/b> are required to impair bacterial growth.\n   > - Compared to <b>Azithromycin<\/b> & <b>Ciprofloxacin<\/b>, which concentrate higher values of <b>_mic<\/b> as well \n   > - Hinting that it's less effective or that the bacteria is tending to become more resistant to the antibiotic & larger quantities are required to affect its function.","b1072135":"<div style=\"color:white;display:fill;border-radius:5px;background-color:#323232;\n       font-size:220%;font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 10px;color:white;\"><b>3.2 | COUNTRY BASED BACTERIA RESISTANCE<\/b><\/p>\n<\/div>\n\n### **<span style='color:#E888BB'>LEAST EFFECTIVE TREATMENT LOCATIONS<\/span>**\n\nThe result sample pool may not be very balanced to make specific conclusions, but resistance to specific anibiotics has some <b>geographic variation<\/b>.\n\n- For <b><span style='color:#E888BB'>Azithromycin<\/span><\/b>, samples from the <b>US<\/b>, <b>Sweden<\/b> & <b>China<\/b> are shown to be only countries with unsuccesful treatment cases.\n- Globally, <b><span style='color:#E888BB'>Ciprofloxacin<\/span><\/b> has become an innefective treatment. <b>Chile<\/b>, <b>Finland<\/b>, <b>Vietnam<\/b>, <b>China<\/b> are amonst the least effective locations.\n- Aside from <b>France<\/b>, <b><span style='color:#E888BB'>Cefixime<\/span><\/b> has been the most effective treatment globally.","5d0f98b3":"# <b>5 <span style='color:#E888BB'>|<\/span> CIPROFLOXACIN RESISTANCE MODELS<\/b>\n\n<div style=\"color:white;display:fill;border-radius:5px;background-color:#323232;\n            font-size:220%;font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 10px;color:white;\"><b>5.1 | MODEL SUMMARY<\/b><\/p>\n<\/div>\n\n- **<span style='color:#E888BB'>Kfold Resistance Models<\/span>** - Set benchmark model scores, outline promising models, quite different models are chosen to see which approaches work best for this problem\n- **<span style='color:#E888BB'>GridSearchCV Resistance Models<\/span>** - As one of the more well performing models that has <b>feature importance<\/b> as well, Let's try to improve the <b>CatBoost<\/b> model accuracy & check how kfold models differ when it comes to feature importance\n- **<span style='color:#E888BB'>SVC Based Resistance Models<\/span>** - SVC offers feature importance similar to <code>LinearRegression()<\/code>, we'll be able to know which top features favours which particular classes in terms of weighting, so if we can make sure the model performs well, we'll have some more unique insight on top of the tree based approach, so we'll try to tune the model so its as accurate as it can be so we have more confidence in the result\n\n<div style=\"color:white;display:fill;border-radius:5px;background-color:#323232;\n            font-size:220%;font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 10px;color:white;\"><b>5.2 | KFOLD RESISTANCE MODELS<\/b><\/p>\n<\/div>\n\n### **<span style='color:#E888BB'>CHOOSING MODELS FOR EVALUATION<\/span>**\n- We add models we would like to use for cross validation both <b>unsupervised<\/b> & <b>supervised<\/b> learning models.\n- <b>hyperparameter selections<\/b> are quite arbitrary, tree based methods use <b>n_estimators = 10<\/b>, which doesn't seem like a bad place to start.\n","699758eb":"### **<span style='color:#E888BB'>TREE BASED FEATURE IMPORTANCE<\/span>**\n- Using <code>RandomForest<\/code>,<code>CatBoost<\/code>,<code>XGBRegressor<\/code> we have access to the model's <b>relative feature importance<\/b>\n- It's useful to compare the <b>feature importance<\/b> of multiple models together. We have over 8000 unitigs; let's pick the <b>top 10<\/b> most influential.\n- The most critical <b>unitigs<\/b> to the determination of antibiotic resistance (according to the model) are saved in <b>BioPython<\/b> <code>Seq<\/code> format & can be used for further analyses.","c83a2c00":"# <b>3 <span style='color:#E888BB'>|<\/span> EXPLORATORY DATA ANALYSIS<\/b>\n\n<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#323232;\n       font-size:220%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:white;\">\n        <b>3.1 | PARALLEL CATEGORIES<\/b>\n    <\/p>\n<\/div>\n\n### **<span style='color:#E888BB'>EXPLORING THE METADATA<\/span>**\n\n- Aside from the <b><span style='color:#E888BB'>unitig<\/span><\/b> data, the <b><span style='color:#E888BB'>Metadata Dataset<\/span><\/b> contains some interesting info about each <b>Sample_ID<\/b> as well.\n- Having sorted by the <b>X_mic<\/b> column, <code>log2_X_mic<\/code> has the same ordering as <code>X_mic<\/code> & <code>antibiotic column name<\/code>, so it was not included to reduce clutter in the figures.\n- <b><span style='color:#E888BB'>Beta.lactamase<\/span><\/b> relation to antibiotic resistance doesn't seem to exhibit any particular patterns, other than <b>Penicillin<\/b>, for which the <b>R<\/b> (resistant) type almost exclsively indicates that the antibiotic will not be effective.\n- Most bacteria <b>Sample_ID<\/b> are also shown to be of <b>S<\/b> (sensitive) type.\n- If we highlight the uttermost right column, <b>_sr<\/b>, we can also note that resistance of these bacteria to antibiotics tends to be rising over time.\n- These figures clearly indicate that <b><span style='color:#E888BB'>cefixime<\/span><\/b> by far is the most effective treatment out of the tree antibiotics.\n- <b><span style='color:#E888BB'>Ciprofloxacin<\/span><\/b> on the otherhand has not been very effective treatment against the bacteria.","cf1acd7d":"### **<span style='color:#E888BB'>GROUPED UNITIGS<\/span>**\nSome columns also contain multiple **<span style='color:#E888BB'>unitigs<\/span>** which we can note below as well, perhaps indicating that all grouped <b>unitigs<\/b> must be present in a given <b>Sample_ID<\/b>.","1275722b":"<div style=\"color:white;display:fill;border-radius:5px;background-color:#323232;\n            font-size:220%;font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 10px;color:white;\"><b>5.4 | SVC MODEL RESISTANCE MODELS<\/b><\/p>\n<\/div>\n\n### **<span style='color:#E888BB'>SVC SUBOPTIMAL MODEL <\/span>**\n- In **Section 5.2**, we used the **<span style='color:#E888BB'>SVC<\/span>** model, using the default hyperparameters, which uses <code>kernel='rbf'<\/code>\n- **<span style='color:#E888BB'>SVC<\/span>** gives us the option to evaluate the feature importance of weights, if we use <code>kernel='linear'<\/code>\n- Unlike the **tree based feature importance**, we will know towards which class the particular feature leans towards (importance wise)\n- Let's try a default model first, followed by a **gridsearchcv** optimised model, so we can see if there is any difference in feature importance\n- <code>fi_svc<\/code> is a slightly adjusted function from **@nwheeler443**'s notebook [ML Workshop Extension Notebook - Ciprofloxacin](https:\/\/www.kaggle.com\/nwheeler443\/ml-workshop-extension-notebook-ciprofloxacin); uses Plotly & returns sequence **SQ** based features","6a6858af":"# <b>3 <span style='color:#E888BB'>|<\/span> FEATURE MATRIX MODIFICATION<\/b>\n\n<div style=\"color:white;display:fill;border-radius:5px;\n            background-color:#323232;font-size:220%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 10px;color:white;\"><b>3.1 | MODIFYING CASE DATA<\/b><\/p>\n<\/div>\n\n- In **Section 2.3**, we saw that for some antibiotics, there are very few resistant samples present , mainly **<span style='color:#E888BB'>Cefixime<\/span>**\n- This results in a very tricky situation if we want to use cross validation, we may not even have enough samples for the model to learn anything meaningful \n- Creating models **<span style='color:#E888BB'>Azithromycin<\/span>** & **<span style='color:#E888BB'>Ciprofloxacin<\/span>** should not have this issue, as we have sufficient number of samples, even if an imbalance is present\n\nWe can try two approaches:\n- Downsample the dominant class (not resistant) <code>.split_case<\/code>\n- Utilise SMOTE based upsampling strategy <code>smote<\/code>","8d67847a":"<div style=\"color:white;display:fill;border-radius:5px;\n            background-color:#323232;font-size:220%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 10px;color:white;\"><b>2.3 | TARGET VARIABLE DISTRIBUTION <\/b><\/p>\n<\/div>\n\n### **<span style='color:#E888BB'>TARGET DISTRIBUTIONS & UNITIG COUNT<\/span>**\n- It's useful to look at the class distributions for all three cases we'll be making models for.\n- As we can see below, the unitig distributions are quite different as well, so our feature matrix will vary significantly for each case.","ceb47a5a":"![](https:\/\/images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com\/f\/8cc1eeaa-4046-4c4a-ae93-93d656f68688\/deu4ysw-578b0695-86dc-4699-9de9-cb4abead217d.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcLzhjYzFlZWFhLTQwNDYtNGM0YS1hZTkzLTkzZDY1NmY2ODY4OFwvZGV1NHlzdy01NzhiMDY5NS04NmRjLTQ2OTktOWRlOS1jYjRhYmVhZDIxN2QuanBnIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.48HPCBPfeD5YQERVHZIKkQBPwGTmY4pRYNWv7Bko8Ek)\n[Kras-Driven Lung Cancer @nci (unsplash)](https:\/\/unsplash.com\/photos\/zoFbfT0M_BU)\n\n# <b>1 <span style='color:#E888BB'>|<\/span> INTRODUCTION<\/b>\n\n<div style=\"color:white;display:fill;border-radius:5px;\n            background-color:#323232;font-size:220%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 10px;color:white;\"><b>1.1 | BACKGROUND<\/b><\/p>\n<\/div>\n\n### **<span style='color:#E888BB'>RESISTANCE OF BACTERIA<\/span>**\n\nFrom <b>ML Workshop Starter<\/b> | [Notebook](https:\/\/www.kaggle.com\/nwheeler443\/ml-workshop-starter-notebook)\n\n> We will be focussing on a species called <b><span style='color:#E888BB'>Neisseria gonorrhoeae<\/span><\/b>, the <b>bacteria which cause gonorrhoea<\/b>. <br>\n> Gonorrhoea is the second most common sexually transmitted infection (STI) in Europe, after chlamydia. <br>\n> Rates of gonorrhoea infection are on the rise, with a 26% increase reported from 2017-2018 in the UK.\n\n> Many people who are infected (especially women) experience no symptoms, helping the disease to spread. <br>\n> If the infection is left untreated, it can lead to infertility in women, and can occasionally spread to <br>\n> other parts of the body such as your joints, heart valves, brain or spinal cord. <br>\n> Resistance of these bacteria to antibiotics is rising over time, making infections hard to treat. <br>\n\n- In the past, patients were treated with an antibiotic called <b><span style='color:#E888BB'>ciprofloxaxcin<\/span><\/b>. \n- Doctors had to stop using this antibiotic because <b>resistance to the drug became too common<\/b>, causing treatments of infections to fail. \n- Until very recently, the recommended treatment was two drugs - <code>ceftriaxone<\/code> and <b><span style='color:#E888BB'>azithromycin<\/span><\/b>. \n- <b><span style='color:#E888BB'>Azithromycin<\/span><\/b> was removed from recommendations because of concern over <b>rising resistance to the antibiotic<\/b>.\n- In February 2018, the first ever reported case of resistance to treatment with <code>ceftriaxone<\/code> and <b><span style='color:#E888BB'>azithromycin<\/span><\/b>, as well as resistance to the last-resort treatment <code>spectinomycin<\/code>, was reported. \n- Currently in the UK, patients are only treated with <code>ceftriaxone<\/code>.\n\n### **<span style='color:#E888BB'>ANTIBIOTICS<\/span>**\n\nThree antibiotics and associated <b>unitigs<\/b> are used to make a model in this problem.\n\n<b>Azithromycin<\/b>\n\n> Azithromycin is an antibiotic used to treat various types of infections of the respiratory tract, ear, skin and eye in adults and children. It is also effective in typhoid fever and some sexually transmitted diseases like gonorrhea.\n\n<b>Ciprofloxacin<\/b>\n\n> Ciprofloxacin is an antibiotic, used in the treatment of bacterial infections. It is also used in treating infections of the urinary tract, nose, throat, skin and soft tissues and lungs (pneumonia). It prevents the bacterial cells from dividing and repairing, thereby killing them.\n\n<b>Cefixime<\/b>\n\n> Cefixime is an antibiotic medicine used to treat a variety of bacterial infections. It is effective in infections of the respiratory tract (eg. pneumonia), urinary tract, ear, nasal sinus, throat, and some sexually transmitted diseases.\n\n### **<span style='color:#E888BB'>UNITIGS<\/span>**\n\nIn our dataset, we will come across features data that will convey the presence or absence of a particular <b>nucleotide<\/b> sequence in the <b>Bacteria's DNA<\/b>\n\nFrom <b>ML Workshop Starter<\/b> | [Notebook](https:\/\/www.kaggle.com\/nwheeler443\/ml-workshop-starter-notebook)\n\n> For this analysis, we're using <b><span style='color:#E888BB'>unitigs<\/span><\/b>, stretches of DNA (in string format) shared by a subset of the strains in our study. <br>\n> Unitigs are an efficient but flexible way of representing DNA variation in bacteria. <br>\n> If you'd like to learn more about unitigs, and how this dataset was constructed, have a look at this [paper](https:\/\/journals.plos.org\/plosgenetics\/article?id=10.1371\/journal.pgen.1007758). <br>\n> The full dataset consists of 584,362 unitigs, which takes a long time to train models on, so for this exercise <br>\n> we will be using a set that has been filtered for unitigs associated with resistance.\n\n- We will look through these <b>unitigs<\/b> in the <b>EDA<\/b> section, which are located in <b>.Ttab<\/b> files based on available & past treatment data.\n\n<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#323232;\n       font-size:220%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:white;\">\n        <b>1.2 | TARGET VARIABLES<\/b>\n    <\/p>\n<\/div>\n\n<b><span style='color:#E888BB'>Neisseria gonorrhoeae<\/span><\/b> is either resistant (<code>target=1<\/code>) to a particular treatment or not resistant (<code>target=0<\/code>) for a particular.\n\n### **<span style='color:#E888BB'>AVAILABLE UNITIG DATA<\/span>**\n\nWe will be choosing one of the following <b><span style='color:#E888BB'>antibiotic<\/span><\/b> cases below:\n\n> - Bacteria resistance to <code>Azithromycin<\/code>; <code>azm_sr<\/code>\n> - Bacteria resistance to <code>Ciprofloxacin<\/code>; <code>cip_sr<\/code>\n> - Bacteria resistance to <code>Cefixime<\/code>; <code>cfx_sr<\/code>\n\n### **<span style='color:#E888BB'>UNAVAILABLE UNITIG DATA<\/span>**\n\nWe don't currently have <b><span style='color:#E888BB'>unitig<\/span><\/b> data for the following antibiotics, so we can't check these cases:\n\n> - Bacteria resistance to <code>Ceftriaxone<\/code>; <code>cro_sr<\/code>\n> - Bacteria resistance to <code>Tetracycline<\/code>; <code>tet_sr<\/code>\n> - Bacteria reistance to <code>Penicillin<\/code>; <code>pen_sr<\/code>\n\n<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#323232;\n       font-size:220%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:white;\">\n        <b>1.3 | NOTEBOOK AIM<\/b>\n    <\/p>\n<\/div>\n\n- Our aim will be to build a model(s) that will be distinguish whether the bacteria <b><span style='color:#E888BB'>is resistant<\/span><\/b> (<code>target=1<\/code>) to a particular antibiotic or not (<code>target=0<\/code>).\n- We will limit outselves to creating models for the target variables for which we have <b>unitig<\/b> sequence data only (<b>Ceftriaxone<\/b>,<b>Ciprofloxacin<\/b> & <b>Cefixime<\/b>)\n\n<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#323232;\n       font-size:220%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:white;\">\n        <b>1.4 | IMBALANCED DATA<\/b>\n    <\/p>\n<\/div>\n\n- Highly <b><span style='color:#E888BB'>imbalanced datasets<\/span><\/b> don't always cause issues for models, having chosen an appropriate metric we can monitor how well the model predicts both classes.\n- However, there are thresholds at which very extreme ratios will lead to completely <b>inaccurate models<\/b>.\n- If our <b>target variable<\/b> is extremely biased towards one class, which ultimately one of our <b>target variable<\/b> cases is, we need to <b><span style='color:#E888BB'>modify our feature matrix<\/span><\/b>, if no extra minority class instances are available.\n\n### **<span style='color:#E888BB'>WHY WE SHOULD PAY ATTENTION TO UPSAMPLING<\/span>**\n- This clearly presents a slight issue, since creation & usage of such resistance models is beneficial at early stages (ie. very few <code>target=1<\/code> cases).\n- Upsampling or just **modification of the feature matrix** can be utilised to create additional sample via unsupervised learning, including the standard <code>SMOTE<\/code> approach which we'll try.","66d86034":"### **<span style='color:#E888BB'>WORKING WITH SEQUENCES<\/span>**\n- Unitigs are composed of <b><span style='color:#E888BB'>nucleotides<\/span><\/b>, which means we can use the <b><span style='color:#E888BB'>SQ()<\/span><\/b> class (included in this notebook) from a [previous notebook](https:\/\/www.kaggle.com\/shtrausslearning\/working-with-sequences-protein-identification) and store the sequence data.\n- Alternatively, we can use the <b><span style='color:#E888BB'>BioPython<\/span><\/b> module as well, storing the sequence data in <b>Seq<\/b> instances. You can look at the [Biopython Basics](https:\/\/www.kaggle.com\/shtrausslearning\/biopython-bioinformatics-basics) notebook. ","481ff228":"<div style=\"color:white;display:fill;border-radius:5px;background-color:#323232;\n            font-size:220%;font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 10px;color:white;\"><b>5.3 | GRIDSEARCHCV RESISTANCE MODEL<\/b><\/p>\n<\/div>\n\n### **<span style='color:#E888BB'>CATBOOST HYPERPARAMETER TUNING <\/span>**\n- We saw that <code>CatboostClassifier<\/code> had already has quite good results on the <b>test set<\/b>, let's try to optimise the <b>hyperparameters<\/b>, if we can.\n- <b>Catboost<\/b> does optimise certain parameters that aren't self defined, so we may not get any improvement, however we should get <b>different hyerparameter combinations<\/b> on different folds, which will be handy to check if there is any <b>feature importance variation<\/b>.\n\n### **<span style='color:#E888BB'>GETTING ALL MODELS PARAMETERS <\/span>**\n- We can call <code>.get_all_params()<\/code> to display all the parameters that were set in <b>CatBoost<\/b>, on top of the <b>n_estimators<\/b> that we set ourselves.\n- The <b>learning rate<\/b> & <b>n_estimators<\/b>\/<b>iterations<\/b> are two hyperparameters we can try to tune.","eb61c33f":"# <b>2 <span style='color:#E888BB'>|<\/span> THE DATASET<\/b>\n\n<div style=\"color:white;display:fill;border-radius:5px;\n            background-color:#323232;font-size:220%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 10px;color:white;\"><b>2.1 | CREATING CASE DATA<\/b><\/p>\n<\/div>\n\n### **<span style='color:#E888BB'>GET_UNITIGS CLASS<\/span>**\n\n- We need to do some data wrangling & combine two data files together; <code>.get_case<\/code>\n- In this problem, a case type format is used (grouped feature matrix & target vector)\n\nThe feature matrix (stored within <code>.X<\/code>) will contain:\n- All collected samples from across various databses, corresponding to the bacteria's DNA (which was cut into segments of 31-mers)\n- These segments are called unitigs | [Dataset Construction Information](https:\/\/journals.plos.org\/plosgenetics\/article?id=10.1371\/journal.pgen.1007758)\n> unitigs - stretches of DNA shared by a subset of the strains in our study\n- When a particular DNA segment (unitig) is present in the bacterial sample DNA, it's value is set to 1 & 0 if it's not present\n\n**<span style='color:#E888BB'>MERGING RTAB & METADATA<\/span>**\n- Filtered Unitig data is located in .Rtab files, corresponding to the same **sample_id** as that of the metadata\n- We need read the .Rtab files & merge the two dataframes based on the **sample_id** index; using <code>.get_case()<\/code>","5df18484":"### **<span style='color:#E888BB'>CASE INPUT<\/span>**\n- Using the unmodified feature matrix of <code>get_unitigs().get_case()<\/code>\n- Using a standard <b>4-fold<\/b> cross validation strategy, let's see how well the models perfom on each fold.","118f006b":"\n### **<span style='color:#E888BB'>TARGET VARIABLE<\/span>**\n- The target vector, is also stored in <code>.X<\/code>, and represents the resistance property of the sample\n- The <b>Sample_ID<\/b> is either 1.0 (resistant) or 0.0 (non-resistant) to a particular antibiotic in question.","a9c35ce3":"### **<span style='color:#E888BB'>PLOTTING KFOLD MODEL FEATURE IMPORTANCES <\/span>**\n- Passing on an <code>eval<\/code> class instance into class <code>fi<\/code>, we have access to <code>store_models<\/code>, which when using the <b>gridsearchcv<\/b> option in class <code>eval<\/code> stores the fold models.\n- Unlike <code>.cv()<\/code>, we probably will have slightly different model hyperparameters for each fold, <b>feature importance<\/b> of these models can also be visualised.","fa3efec3":"<div style=\"color:white;display:fill;border-radius:5px;background-color:#323232;\n       font-size:220%;font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 10px;color:white;\"><b>3.4 | UNITIG<\/b><\/p>\n<\/div>\n\n### **<span style='color:#E888BB'>UNIQUE VALUES OF ALL UNITIGS<\/span>**\nAll columns in the feature matrix <code>.X<\/code>, used for training contains only values for <b><span style='color:#E888BB'>present (1)<\/span><\/b> or <b><span style='color:#E888BB'>not present (0)<\/span><\/b> for each <b>Sample_ID<\/b>.","3f1e60fb":"# <b>4 <span style='color:#E888BB'>|<\/span> CREATING RESISTANCE MODELS<\/b>\n\n<div style=\"color:white;display:fill;border-radius:5px;background-color:#323232;\n       font-size:220%;font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 10px;color:white;\"><b>4.1 | LIST OF TUPLE MODELS<\/b><\/p>\n<\/div>\n\n### **<span style='color:#E888BB'>ADDING MODELS TO LIST<\/span>** `models`\n\n- We define a <b>list of tuples<\/b> from which we can access the case (<b>Model Name<\/b>, <b>Model Instance<\/b> & <b>CV Parameters<\/b>) & use them in the evaluation class.\n- A dictionary contaning <b>CV Parameters<\/b> (third term) is only required when calling <code>.gscv()<\/code>","5471f630":"<div style=\"color:white;display:fill;border-radius:5px;\n            background-color:#323232;font-size:220%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 10px;color:white;\"><b>2.2 | CASE PREVIEW - Ciprofloxacin <\/b><\/p>\n<\/div>\n\n- In this notebook, we'll be looking at bacterial resistance for three different cases, let's check one case first & load the other when required\n\n### **<span style='color:#E888BB'>METADATA CONTENT<\/span>**\n- <b>Metadata<\/b> is stored in <code>.df<\/code>, which contains our <b>target variable<\/b> (one of the <b>_sr<\/b> columns) for a specific <b>Sample_ID<\/b>\n- As we can see, we don't have all the data, some data is missing in our <b>metadata<\/b>, we'll only be dropping <b>Nan<\/b> cases for the target variable (<b>_sr<\/b>) we're concerned with.\n\n### **<span style='color:#E888BB'>FEATURE MATRIX<\/span>**\n- The feature matrix is created from one of the <b>.Rtab<\/b> files, depending on which case we are testing.\n- Each column in the feature matrix is called a <b><span style='color:#E888BB'>unitig<\/span><\/b> (pattern_id); is treated as a feature for the specific <b>Sample_ID<\/b>, which is our index.","ed30c37f":"### **<span style='color:#E888BB'>REVISITING UNITIG DISTRIBUTIONS <\/span>**\n- Having identified the key features (<b>unitigs<\/b>) which have the most weight in its respective model\n- With <b>tree based model<\/b> feature importance, we ultimately get <b>only positive values<\/b> & don't have an indication about which class the result is leaning towards\n- However knowing which features features (<b>unitigs<\/b>) to look at now, we can use <code>group_by<\/code> to get some idea since they are all of <b>One Hot Encoding<\/b> type","b377b587":"### **<span style='color:#E888BB'>SVC OPTIMISED MODEL <\/span>**\n- The linear covariance function model performed a little worse than those tested in the previous section\n- Let's see if we can optimise the model a little bit & check if the same features will remain the most important in the model\n- Let's try a variation in the **gamma** hyperparameter for the search grid to keep things quite simple","46f775aa":"### **<span style='color:#E888BB'>DEFINING GRID & GRIDSEARCHCV <\/span>**\n- We define a <b>parameter grid<\/b>, which will be used in <code>GridSearchCV<\/code>; keeping it simple, looking at only <code>n_estimators<\/code> (iters) & the <code>learning rate<\/code>.\n- Defining a standard <b>4-fold<\/b> cross validation strategy once again, we should get 4 slightly different models this time, which are stored in <code>.store_models()<\/code>.\n- Catboost is of course compatible with the <b>GPU<\/b>, so we can set <b>task_type = \"GPU\"<\/b>.","e1f04931":"<div style=\"color:white;display:fill;border-radius:5px;background-color:#323232;\n            font-size:220%;font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 10px;color:white;\"><b>4.2 | MODEL EVALUATION CLASS<\/b><\/p>\n<\/div>\n\n### **<span style='color:#E888BB'>CLASS INSTANCE INPUTS<\/span>**\n\n- We'll define a class, <code>class_eval<\/code> that will be used for <b>evaluation<\/b> purposes. \n- We can instantiate <code>class_eval<\/code> having defined:\n    - **<span style='color:#E888BB'>Data\/Case Class<\/span>** containing feature & target vector (<code>get_unitifs<\/code> or <code>mod_unitigs<\/code>)\n    - **<span style='color:#E888BB'>List of models<\/span>** (containing tuples)\n    - **<span style='color:#E888BB'>nfold<\/span>** (number of kfold splits)\n    - **<span style='color:#E888BB'>gsfold<\/span>** (number of gridsearch folds)\n\n### **<span style='color:#E888BB'>EVALUATION OPTIONS<\/span>**\n\nUsing the <code>self.models<\/code> defined during instantiation: \n\n- <code>.cv(type_id)<\/code> : <b>kfold<\/b> Cross validation is done with the selected <code>model(s)<\/code> with the set <b>hyperparameters<\/b>.\n- <code>.gscv(type_id)<\/code> : <b>kfold<\/b> cross validation but with <b>Grid Search<\/b>, best mean <b>balanced_accuracy<\/b> score hyperparamers used for fitting on whole kfold subset.\n\nBoth functions require kfold type input <code>type_id<\/code>: <b>Standard<\/b> (kfold) \/ <b>Stratified<\/b> (skfold)\n\n#### **gscv**\nFor all input models, defined with <b>grid-search<\/b> hyperparameters (third term):\n- We split the dataset into <code>nfold<\/code> groups containing both **training** & **test data**\n- For each train segment data, we use <code>GridSearchCV<\/code> to find the best scoring model using the provided parameter dictionary. \n- Using the <b>best scoring model<\/b> on the training set, we evaluate both training & test scoring using the <b>evaluation metric<\/b>.\n- For <b>outputs<\/b>, we recall the best scoring <b>hyperparameters<\/b> & <b>confusion matrix<\/b> for each fold.\n\n#### **cv**\nFor all input models:\n- We split the dataset into <code>nfold<\/code> groups containing both **training** & **test data**\n- For each nfold, we have a <b>training<\/b> & <b>test<\/b> set. On this data we evaluate our model w\/ predefined hyperparameters.\n- Results for each fold are stored in <code>.dic_tr<\/code> & <code>.dic_te<\/code> which can be visualised by calling <code>.fold_plot()<\/code>\n\n### **<span style='color:#E888BB'>EVALUATION METRIC<\/span>**\n- As we saw in <b>Section 2&3<\/b>, the **<span style='color:#E888BB'>target variable<\/span>** in our binary classification problem can be <b>very one sided<\/b>\n- Let's use **<span style='color:#E888BB'>balanced_accuracy<\/span>**, which is suitable for <b>imbalanced class<\/b> problems."}}