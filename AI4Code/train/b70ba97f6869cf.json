{"cell_type":{"6a54e244":"code","1efdfc74":"code","7e6ddca2":"code","33dbae8c":"code","22e88b00":"code","e77ffce1":"code","8bcf6b52":"code","01f69ef0":"code","2d88dd6f":"code","56eef26c":"code","4ebf11cb":"code","f96500b0":"code","1686c981":"code","d053081f":"code","0c03a400":"code","1a1ada4a":"code","4c1675aa":"code","0b324f68":"code","4d74bb52":"code","9b17a115":"code","abfe9116":"markdown","35a5292e":"markdown","e997e431":"markdown","8b4dff2c":"markdown","123c9499":"markdown","36132579":"markdown","648903b2":"markdown","d4808503":"markdown","3224aa1e":"markdown","921518f9":"markdown","c4ed7e32":"markdown","cd78d638":"markdown","661a6462":"markdown","d221c699":"markdown","46165a31":"markdown"},"source":{"6a54e244":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport warnings\nwarnings.filterwarnings(\"ignore\")","1efdfc74":"plt.rcParams['figure.figsize'] = (12,7)","7e6ddca2":"df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ndf.head()","33dbae8c":"sns.countplot(df['target'])\nsns.despine()","22e88b00":"df['keyword'].value_counts().head(20).plot.barh()\nsns.despine()","e77ffce1":"df['location'].value_counts().head(20).plot.barh()\nsns.despine()","8bcf6b52":"fig, ax = plt.subplots(figsize=(12,7))\nfor label, group in df.groupby('target'):\n    sns.distplot(group['text'].str.len(), label=str(label), ax=ax)\nplt.xlabel('# of characters')\nplt.ylabel('density')\nplt.legend()\nsns.despine()","01f69ef0":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom umap import UMAP\n\nfeatures = []\nfor i in range(1, 11):\n    X_dim = CountVectorizer(min_df=i, stop_words='english').fit_transform(df['text'])\n    features.append(X_dim.shape[1])\nplt.plot(range(1, 11), features)\nplt.xlabel('min df')\nplt.ylabel('# of features')\nsns.despine()","2d88dd6f":"from sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom umap import UMAP\n\ndim_red = make_pipeline(\n    CountVectorizer(min_df=2, stop_words='english'),\n    UMAP()\n)\nX_dim = dim_red.fit_transform(df['text'])","56eef26c":"%%capture\n!pip install altair notebook vega # needs internet in settings (right panel)\nimport altair as alt\nalt.renderers.enable('kaggle')","4ebf11cb":"alt.Chart(pd.DataFrame({\n    'x0': X_dim[:,0],\n    'x1': X_dim[:,1],\n    'text': df['text'],\n    'keyword': df['keyword'],\n    'location': df['location'],\n    'target': df['target']\n}).sample(5000, random_state=42)).mark_point().encode(\n    x='x0',\n    y='x1',\n    color='target:N',\n    tooltip='keyword'\n).properties(\n    title='Based on text',\n    width=500,\n    height=500\n).interactive()","f96500b0":"from sklearn.preprocessing import OneHotEncoder\n\nenc = make_pipeline(\n    OneHotEncoder(),\n    UMAP(metric='cosine', random_state=42)\n)\nX_onehot = enc.fit_transform(df[['keyword']].fillna(''))","1686c981":"alt.Chart(pd.DataFrame({\n    'x0': X_onehot[:,0],\n    'x1': X_onehot[:,1],\n    'text': df['text'],\n    'keyword': df['keyword'],\n    'location': df['location'],\n    'target': df['target']\n}).sample(5000, random_state=42)).mark_point().encode(\n    x='x0',\n    y='x1',\n    color='target:N',\n    tooltip='text'\n).properties(\n    title='Based on keywords',\n    width=500,\n    height=500\n).interactive()","d053081f":"keywords = df.groupby('keyword').agg({\n    'target': 'mean'\n})","0c03a400":"keywords['target'].sort_values(ascending=False).head(10).plot.barh()\nplt.xlabel('p(target=1)')\nsns.despine()","1a1ada4a":"keywords['target'].sort_values().head(10).plot.barh()\nplt.xlabel('p(target=1)')\nsns.despine()","4c1675aa":"for index, row in df[df['keyword'] == 'body%20bags'].sample(10).iterrows():\n    print('Label: {} | {}'.format(row.target, row.text))","0b324f68":"keywords.query('target > .45 and target < .55').sort_values('target').plot.barh()\nplt.xlabel('p(target=1)')\nsns.despine()","4d74bb52":"for index, row in df[df['keyword'] == 'hail'].sample(10).iterrows():\n    print('Label: {} | {}'.format(row.target, row.text))","9b17a115":"for index, row in df[df['keyword'] == 'bombed'].sample(10).iterrows():\n    print('Label: {} | {}'.format(row.target, row.text))","abfe9116":"The previous figure shows that we can significantly reduce the dimensionality by using the minimum document frequency of 2. Hopefully, this will help us understand the data better.","35a5292e":"On the other hand, the following figure shows the keywords that people use more frequently in daily conversations, not to describe catastrophic events.","e997e431":"It turns out, you can see the separation better this way! Those tiny islands are now in the same color, i.e. same labels. Will this be a good predictor then?","8b4dff2c":"Let's see some examples containing the word `hail` and `bombed`.","123c9499":"In terms of the word length, we can see that the \"real\" tweets are slightly longer.","36132579":"Notice that while the clusters still contain 0s and 1s based on the text alone. The keywords are also jumbled. *What if we do it the other way around?*","648903b2":"In this kernel, we would like to understand more about the content of the tweets. Which words can explain the classes better?","d4808503":"The majority of the tweets are located in the US, e.g. `USA`, `New York`, `United States`, `Los Angeles, CA`.","3224aa1e":"We can see that the classes are slightly imbalanced which might impact our classifier performance later on.","921518f9":"# Feature Selection\n\nOne of the easiest ways to extract features from text data is to use bag-of-words or TF-IDF. However, this might result in high-dimensional data. One of the causes is from tokens that only appear once in the whole dataset. On the other hand, there might be stop words that will appear frequently but carry little to no meaning, such as prepositions. Thus, we will remove the stop words first and then pick the minimum number of documents that a particular token should appear in to reduce the dimensionality.","c4ed7e32":"The following figure shows that keywords like `derailment`, `wreckage`, `debris`, `outbreak`, or `typhoon` are more likely to describe a real event. As we can see later on, though `bombed` is used in daily conversations figuratively, people use `suicide bombing` or `suicide bomb` sparingly.","cd78d638":"# Dimensionality Reduction\n\nThough we have reduced the number of dimensions from around 20k to 6k (~70% reduction), it is still hard to understand the clusters in the data without visualising them in 2D. *Thus, [UMAP](https:\/\/pair-code.github.io\/understanding-umap\/) to the rescue!*\n\nIn the following code, we try to project the data into 2D from the raw tweets. To make it easier to evaluate the \"clusters\", we can visualise them using an interactive library called [Altair](https:\/\/altair-viz.github.io\/). This declarative visualisation library can help us to put tooltips and interact with each data point to understand our data.","661a6462":"# Indistinctive keywords\n\nThe main problems are in these indistinctive keywords. They are more likely what form the big island in the middle of the UMAP projection based on keywords in the last scatter plot.","d221c699":"# Top distinctive keywords\n\nIf we can separate the data with keywords, which of them are actually distinctive?","46165a31":"In the following figure, you can see the top 20 keywords that are used in the data."}}