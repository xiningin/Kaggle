{"cell_type":{"cf8f56a2":"code","336cb05f":"code","a8361901":"code","5606b25d":"code","b4a806ec":"code","263e2682":"code","8b1ba2c4":"code","3799c375":"code","9d812b8f":"code","5bd287a2":"code","ef563535":"code","f5f78f39":"code","2221a729":"code","9ddf9151":"code","961acbfe":"code","b79cedb2":"code","3a957ef7":"code","41005637":"code","9dfe08a4":"code","5fdc309b":"code","ccd5247c":"code","633ab5d0":"code","9983e48b":"code","9ed7fb0b":"code","6dd26fd5":"code","a4f16045":"code","8a6c1de8":"code","25b0e42d":"code","34f8120d":"code","3400b474":"code","681f5154":"code","3e2dac63":"code","ad171104":"code","17bad30a":"code","9fd149af":"code","ef2946f9":"code","34664235":"code","01b377c0":"code","46afa9bd":"code","709db32c":"code","53d3a041":"code","eb5a4e83":"code","6057f130":"code","ee96e3a9":"code","8c53364e":"code","877c301e":"code","3801bd83":"code","a5b6ab03":"code","6acddc9f":"code","c1d1585c":"code","d467043a":"code","ee400625":"code","2a791972":"code","df16dac6":"code","74e3a67a":"code","6741b696":"code","4c50a9e6":"code","506c2a96":"code","cb213bae":"code","30f98a45":"code","7c631880":"code","e81c59db":"code","0c01cddd":"code","9457d367":"code","e441f3de":"code","7cfc66cb":"code","74f5e0a1":"code","1022caa9":"code","9a90cd77":"code","665cd91e":"code","6b01c3f8":"code","3b6d7b48":"code","2739a50e":"code","ad422de2":"code","4cc3611f":"code","98b81a50":"code","8a15c9d1":"code","88c6bd66":"code","b3488e73":"code","424ffd4b":"code","7bcfadee":"markdown","0c14429f":"markdown","23fbc6b6":"markdown","437154d6":"markdown","b7139dec":"markdown","d991401e":"markdown","1a39f8c3":"markdown","18f2bb30":"markdown","e0198845":"markdown","94711f8f":"markdown","f68abe08":"markdown","9f7013ce":"markdown","6ae7ca01":"markdown","52ac49f5":"markdown","14151fa4":"markdown","03c7845f":"markdown","dcfc4a1b":"markdown","149c24f7":"markdown","f867cfa7":"markdown","831db9da":"markdown","50ef1a3d":"markdown","bdbcc1a2":"markdown","632e1172":"markdown","a248c4e5":"markdown","50450485":"markdown","6b3d9db6":"markdown","0202634e":"markdown","606b14fb":"markdown","f44ffdbf":"markdown","ee701c32":"markdown","1f34ffe2":"markdown"},"source":{"cf8f56a2":"# Importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set up matplotlib style \nplt.style.use('ggplot')\n\n# Libraries for wordcloud making and image importing\nfrom wordcloud import WordCloud, ImageColorGenerator\nfrom PIL import Image\n\n# And libraries for data transformation\nimport datetime\nfrom string import punctuation","336cb05f":"# Import data and transform tsv file\ndata = pd.read_csv('..\/input\/amazon-alexa-reviews\/amazon_alexa.tsv', delimiter='\\t')","a8361901":"# Data overlook\ndata.head()","5606b25d":"# Transform string data and remove punctuation\ndata['verified_reviews'] = data.verified_reviews.apply(lambda x: x.lower())\ndata['verified_reviews'] = data.verified_reviews.apply(lambda x: ''.join([c for c in x if c not in punctuation]))","b4a806ec":"# Get length of review for EDA\ndata['review_length'] = data.verified_reviews.apply(lambda x: len(x))","263e2682":"# Check the data again\ndata.head()","8b1ba2c4":"# Take a look at the mean, standard deviation, and maximum\nprint('The mean for the length of review:',data['review_length'].mean())\nprint('The standard deviation for the length of reviews:',data['review_length'].std())\nprint('The maximum for the length of reviews:',data['review_length'].max())","3799c375":"# And take a look at the distribution of the length\ndata['review_length'].hist(bins=20)\nplt.title('Distribution of review length')","9d812b8f":"# Transform date to datetime data type\ndata['date'] = data.date.apply(lambda x:  datetime.datetime.strptime(x, '%d-%b-%y'))","5bd287a2":"data.info()","ef563535":"A = np.array(Image.open('..\/input\/amazon-logo\/amazon-logo-white.png'))","f5f78f39":"np.random.seed(321)\nsns.set(rc={'figure.figsize':(14,8)})\nreviews = ' '.join(data['verified_reviews'].tolist())\n\nwordcloud = WordCloud(mask=A,background_color=\"black\").generate(reviews)\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.margins(x=0, y=0)\nplt.title('Reviews',size=20)\nplt.show()","2221a729":"sns.set(rc={'figure.figsize':(10,6)})\nsns.countplot(data.variation,\n              order = data['variation'].value_counts().index)\nplt.xticks(rotation=90)\nplt.title('Counts of each variation')","9ddf9151":"data.rating.value_counts()","961acbfe":"data5 = data[data.rating == 5]\ndata_not_5 = data[data.rating != 5]\ndata1 = data[data.rating == 1]","b79cedb2":"sns.set(rc={'figure.figsize':(14,8)})\nreviews = ' '.join(data5['verified_reviews'].tolist())\n\nwordcloud = WordCloud(mask=A,background_color=\"black\").generate(reviews)\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.margins(x=0, y=0)\nplt.title('Reviews of rating 5',size=20)\nplt.show()","3a957ef7":"pd.options.display.max_colwidth = 200\n\ndata5[data5['verified_reviews'].str.contains('prime')]['verified_reviews'][:3]","41005637":"data5[data5['verified_reviews'].str.contains('time ')]['verified_reviews'][:3]","9dfe08a4":"data5[data5['verified_reviews'].str.contains('easy')]['verified_reviews'][:3]","5fdc309b":"sns.set(rc={'figure.figsize':(14,8)})\nreviews = ' '.join(data1['verified_reviews'].tolist())\n\nwordcloud = WordCloud(mask=A,background_color=\"black\").generate(reviews)\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.margins(x=0, y=0)\nplt.title('Reviews of rating 1',size=20)\nplt.show()","ccd5247c":"data1[data1['verified_reviews'].str.contains('useless')]['verified_reviews'][:3]","633ab5d0":"reviews = ' '.join(data_not_5['verified_reviews'].tolist())\n\nwordcloud = WordCloud(mask=A,background_color=\"black\").generate(reviews)\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.margins(x=0, y=0)\nplt.title('Reviews of negative reviews',size=20)\nplt.show()","9983e48b":"sns.boxplot(data.variation, data.rating)\nplt.xticks(rotation = 90)","9ed7fb0b":"data.head()","6dd26fd5":"data_date = data.groupby('date').count()","a4f16045":"data_date.rating.plot()","8a6c1de8":"sns.boxplot('rating','review_length',data=data)","25b0e42d":"data['log_review_length'] = data.review_length.apply(lambda x: (np.log(x)+1))","34f8120d":"sns.boxplot('rating','log_review_length',data=data)","3400b474":"sns.boxplot('variation','log_review_length',data=data)\nplt.xticks(rotation = 90)","681f5154":"from collections import Counter\n\ntext = ' '.join(data['verified_reviews'].tolist())\nreview_word = text.split(' ')\nall_reviews = ' '.join(review_word)\nwords = all_reviews.split()\n\n# words wrong datatype\ncounts = Counter(words)\nvocab = sorted(counts, key=counts.get, reverse=True)\nvocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n\nreviews_ints = []\nfor review in review_word:\n    reviews_ints.append([vocab_to_int[word] for word in review.split()])","3e2dac63":"print('Unique words: ', len((vocab_to_int)))","ad171104":"counts.most_common(20)","17bad30a":"data1['rating'].value_counts()","9fd149af":"text1 = ' '.join(data1['verified_reviews'].tolist())\nreview_word1 = text1.split(' ')\nall_reviews1 = ' '.join(review_word1)\nwords1 = all_reviews1.split()\n\n# words wrong datatype\ncounts1 = Counter(words1)\nvocab1 = sorted(counts1, key=counts1.get, reverse=True)\nvocab_to_int1 = {word: ii for ii, word in enumerate(vocab1, 1)}\n\nreviews_ints1 = []\nfor review in review_word1:\n    reviews_ints1.append([vocab_to_int1[word] for word in review.split()])","ef2946f9":"counts1.most_common(20)","34664235":"data1 = pd.read_csv('..\/input\/amazon-alexa-reviews\/amazon_alexa.tsv', delimiter='\\t')","01b377c0":"import spacy\nnlp = spacy.load('en')\n\ndef explain_text_entities(text):\n    doc = nlp(text)\n    for ent in doc.ents:\n        print(f'Entity: {ent}, Label: {ent.label_}, {spacy.explain(ent.label_)}')","46afa9bd":"for i in range(15,20):\n    one_sentence = data1['verified_reviews'][i]\n    doc = nlp(one_sentence)\n    spacy.displacy.render(doc, style='ent',jupyter=True)","709db32c":"for i in ['en_core_web_sm','en_core_web_lg']:\n    nlp = spacy.load(i)\n    print('This is model:',i)\n    for i in range(17,22):\n        one_sentence = data1['verified_reviews'][i]\n        doc = nlp(one_sentence)\n        spacy.displacy.render(doc, style='ent',jupyter=True)","53d3a041":"data.head()","eb5a4e83":"data['positive'] = 0\ndata.loc[data['rating'] ==5, 'positive'] = 1\n\ny = data['positive']","6057f130":"from nltk.tokenize import word_tokenize \n\nword_tokenize(data.verified_reviews[0])","ee96e3a9":"from nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \n\nstop_words = set(stopwords.words('english')) \n\ndata['cleaned_reviews'] = data.verified_reviews.apply(lambda x: word_tokenize(x))\n\ndata['cleaned_reviews'] = data.cleaned_reviews.apply(lambda x: [w for w in x if w not in stop_words])\n\ndata['cleaned_reviews'] = data.cleaned_reviews.apply(lambda x: ' '.join(x))","8c53364e":"data.head()","877c301e":"data.info()","3801bd83":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nX_train, X_test, y_train, y_test = train_test_split(data[\"cleaned_reviews\"], y, test_size=0.33\n                                    ,random_state=53)\n\n# Initialize a CountVectorizer object: count_vectorizer\ncount_vectorizer = CountVectorizer(stop_words=\"english\")\n\n# Transform the training data using only the 'text' column values: count_train \ncount_train = count_vectorizer.fit_transform(X_train)\n\ny_train = np.asarray(y_train.values)\n\nch2 = SelectKBest(chi2, k = 300)\n\nX_new = ch2.fit_transform(count_train, y_train)\n\n# Transform the test data using only the 'text' column values: count_test \ncount_test = count_vectorizer.transform(X_test)\n\nX_test_new = ch2.transform(X=count_test)","a5b6ab03":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Initialize a TfidfVectorizer object: tfidf_vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.7)\n\n# Transform the training data: tfidf_train \ntfidf_train = tfidf_vectorizer.fit_transform(X_train)\n\n# Transform the test data: tfidf_test \ntfidf_test = tfidf_vectorizer.transform(X_test)","6acddc9f":"# Create the CountVectorizer DataFrame: count_df\ncount_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n\n# Create the TfidfVectorizer DataFrame: tfidf_df\ntfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n\n# Print the head of count_df\nprint(count_df.head())\n\n# Print the head of tfidf_df\nprint(tfidf_df.head())\n\n# Calculate the difference in columns: difference\ndifference = set(count_df.columns) - set(tfidf_df.columns)\nprint(difference)\n\n# Check whether the DataFrames are equal\nprint(count_df.equals(tfidf_df))\n","c1d1585c":"# Import the necessary modules\nfrom sklearn import metrics\nfrom sklearn.naive_bayes import MultinomialNB\n\n\n# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\nnb_classifier = MultinomialNB()\n\n# Fit the classifier to the training data\nnb_classifier.fit(X_new, y_train)\n\n# Create the predicted tags: pred\npred = nb_classifier.predict(X_test_new)\n\n# Calculate the accuracy score: score\nscore = metrics.accuracy_score(y_test, pred)\nprint('Accuracy is:',score)\nf1 = metrics.f1_score(y_test, pred)\nprint('F score is:',f1)","d467043a":"sns.heatmap(metrics.confusion_matrix(pred,y_test),annot=True,fmt='2.0f')","ee400625":"# Create a Multinomial Naive Bayes classifier: nb_classifier\nnb_classifier = MultinomialNB()\n\n# Fit the classifier to the training data\nnb_classifier.fit(tfidf_train,y_train)\n\n# Create the predicted tags: pred\npred = nb_classifier.predict(tfidf_test)\n\n# Calculate the accuracy score: score\nscore = metrics.accuracy_score(y_test, pred)\nprint('Accuracy is:',score)\nf1 = metrics.f1_score(y_test, pred)\nprint('F score is:',f1)","2a791972":"sns.heatmap(metrics.confusion_matrix(pred,y_test),annot=True,fmt='2.0f')","df16dac6":"from sklearn.feature_extraction.text import TfidfTransformer\ndef TextPreprocessing(train,train_y,test):\n    \n    CountVectorizer(stop_words=\"english\")\n    count = count_vectorizer.fit_transform(train)\n    count_test = count_vectorizer.transform(X_test)\n    \n    tfidf_vectorizer = TfidfTransformer()\n    tfidf = tfidf_vectorizer.fit_transform(count)\n    tfidf_test = tfidf_vectorizer.transform(count_test)\n    \n    ch2 = SelectKBest(chi2, k = 300)\n    \n    train_new = ch2.fit_transform(tfidf, y_train)\n    test_new = ch2.transform(tfidf_test)\n    \n    return train_new, test_new","74e3a67a":"train, test = TextPreprocessing(X_train,y_train,X_test)","6741b696":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(random_state=51)\n# Fit the classifier to the training data\nclf.fit(train, y_train)\n\n# Create the predicted tags: pred\npred = clf.predict(test)\n\n# Calculate the accuracy score: score\nscore = metrics.accuracy_score(y_test, pred)\nprint('Accuracy is:',score)\nf1 = metrics.f1_score(y_test, pred)\nprint('F score is:',f1)","4c50a9e6":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(random_state=51)\n# Fit the classifier to the training data\nclf.fit(X_new, y_train)\n\n# Create the predicted tags: pred\npred = clf.predict(X_test_new)\n\n# Calculate the accuracy score: score\nscore = metrics.accuracy_score(y_test, pred)\nprint('Accuracy is:',score)\nf1 = metrics.f1_score(y_test, pred)\nprint('F score is:',f1)\n","506c2a96":"sns.heatmap(metrics.confusion_matrix(pred,y_test),annot=True,fmt='2.0f')","cb213bae":"clf = RandomForestClassifier(random_state=51)\n# Fit the classifier to the training data\nclf.fit(tfidf_train, y_train)\n\n# Create the predicted tags: pred\npred = clf.predict(tfidf_test)\n\n# Calculate the accuracy score: score\nscore = metrics.accuracy_score(y_test, pred)\nprint('Accuracy is:',score)\nf1 = metrics.f1_score(y_test, pred)\nprint('F score is:',f1)\n","30f98a45":"sns.heatmap(metrics.confusion_matrix(pred,y_test),annot=True,fmt='2.0f')","7c631880":"from sklearn.model_selection import GridSearchCV\n\nclf = RandomForestClassifier()\n\nscorer = metrics.make_scorer(metrics.fbeta_score, beta=0.5)\n\nparameters = {'n_estimators': [150, 180, 250], 'max_features': [120,150], 'max_depth': [120,135,150],\n              'min_samples_split':[3,5],'min_samples_leaf':[1,3,5]}\n\ngrid_obj = GridSearchCV(clf, parameters, scoring=scorer)\n\ngrid_fit = grid_obj.fit(X_new, y_train)\n\n# Get the estimator\nbest_clf = grid_fit.best_estimator_\n\nbest_predictions = best_clf.predict(X_test_new)\n\nscore = metrics.accuracy_score(y_test, best_predictions)\nprint('Accuracy is:',score)\nf1 = metrics.f1_score(y_test, best_predictions)\nprint('F score is:',f1)","e81c59db":"best_clf","0c01cddd":"sns.heatmap(metrics.confusion_matrix(pred,best_predictions),annot=True,fmt='2.0f')","9457d367":"from itertools import compress\n\nfeatures = count_vectorizer.get_feature_names()\nmask = ch2.get_support()\nfeatures = list(compress(features, mask))\nimportances = best_clf.feature_importances_\nindices = np.argsort(importances)\n\nsns.set(rc={'figure.figsize':(11,50)})\n\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","e441f3de":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nX_test_new = X_test_new.toarray()\n\nperm = PermutationImportance(best_clf, random_state=1).fit(X_test_new, y_test)\neli5.show_weights(perm, feature_names = features)","7cfc66cb":"best_clf.fit(count_train,y_train)","74f5e0a1":"eli5.show_prediction(best_clf, doc=X_train[20], vec=count_vectorizer)","1022caa9":"from sklearn.tree import DecisionTreeClassifier\n\ntree_model = DecisionTreeClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(X_new, y_train)","9a90cd77":"from sklearn import tree\nimport graphviz\n\ntree_graph = tree.export_graphviz(tree_model, feature_names=features)\ngraphviz.Source(tree_graph)","665cd91e":"df = pd.DataFrame(X_test_new,columns = features)\ndf.head()","6b01c3f8":"from matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\n\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=tree_model, dataset=df, model_features=features, feature='love')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'love')\nplt.show()","3b6d7b48":"feature_to_plot = 'time'\npdp_dist = pdp.pdp_isolate(model=tree_model, dataset=df, model_features=features, feature=feature_to_plot)\n\npdp.pdp_plot(pdp_dist, feature_to_plot)\nplt.show()","2739a50e":"best_clf.fit(X_new, y_train)","ad422de2":"# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=best_clf, dataset=df, model_features=features, feature='use')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'use')\nplt.show()","4cc3611f":"# Similar to previous PDP plot except we use pdp_interact instead of pdp_isolate and pdp_interact_plot instead of pdp_isolate_plot\nfeatures_to_plot = ['love', 'good']\ninter2  =  pdp.pdp_interact(model=best_clf, dataset=df, model_features=features, features=features_to_plot)\n\npdp.pdp_interact_plot(pdp_interact_out=inter2, feature_names=features_to_plot, plot_type='contour')\nplt.show()","98b81a50":"row_to_show = 5\ndata_for_prediction = df.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\ndata_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n\n\nbest_clf.predict_proba(data_for_prediction_array)","8a15c9d1":"import shap  # package used to calculate Shap values\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(best_clf)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(data_for_prediction)","88c6bd66":"shap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)","b3488e73":"# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(best_clf)\n\n# calculate shap values. This is what we will plot.\n# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.\nshap_values = explainer.shap_values(df)\n\n# Make plot. Index of [1] is explained in text below.\nshap.summary_plot(shap_values[1], df)","424ffd4b":"shap.dependence_plot('love', shap_values[1], df, interaction_index=\"great\")","7bcfadee":"Here you can see how each word effect the prediction. But since I already do some preprocessing, the sentence may not make too many sense","0c14429f":"We cab see that most of the values in each columns are only effective between 1 and -1","23fbc6b6":"This result is lovely! It has a balance between positive result and negative result since I use f score for scorer in the model tuning. This is what I want!","437154d6":"### <a id=\"MT\">Model Tuning<\/a>","b7139dec":"It's weird that there is a peak in the end of July. I'll do some research for now.","d991401e":"# <a id=\"1\">Counter<\/a>\n\n## I'll start do some preprocessing here for further modeling here first. This kernel is still in progress and has not been organized. I'll make it more ready for readying!","1a39f8c3":"The more work show up in the review, the less possible it's a positive review. And it contributes at most about -0.375 to the prediction","18f2bb30":"I can see that for the people who use Alexa and give rating 5 think:\n\n* Alexa is easy to use (set up)\n* Frequently used by customers\n* The service of connect to prime day is good and popular","e0198845":"The only different I found between rating 1 data most common words is \"but\".\n\n## <a id=4>SpaCy<\/a>\n\nI'll implement Spacy from here. This is a great NLP library. I'm still getting used to it.","94711f8f":"I can see some normal words and some positive reviews.\n\nFor positive parts, I saw:\n\n* love\/ new\n* works great\/ better\/ good","f68abe08":"### Summary\n\nI try to demonstrate some points in this work:\n\n* To do analysis on text data like this one   \n* Trying SpaCy libraries to identify entities\n* How can we preprocess text data for modeling\n* Tuning model for specific goals\n* How can we interpret our model in terms of variables or frequency or how they effect each other\n\nI try my best to evaluate and explain my final model. I think it's good to know more how the model predict and value each words and make the deicision. It literally let me know my models are no longer the black box technology to me. And interpret the model can allow me to explain the insights to non-tech audience. Moreover, knowing how the model make the prediction can let me apply the knowledge of these nodes to the business. I'll bring these to my work and make more kernels like this. Thank you so much for your support! Happy coding!","9f7013ce":"### <a id=\"p\">Text Preprocessing<\/a>\n\nSince there are some difference between the ways people put reviews, let me do some transform before my analysis","6ae7ca01":"This one is more interesting. I go back to use the Random Forest Classifier model to see how it think about 'use'. For the review with no more than 1 'use', it means nothing. We can easily see this phenomenon in the graph!","52ac49f5":"This is seriously biased! Most reviews are rating 5. Those rating 1 and 2 are considered outliers in each product variation!","14151fa4":"Using TfidfVectorizer is even worse. It made it to get all positive reviews, but missed most of the negative reviews, which is not our goal for the most of time!\n\nBut how about putting them together?","03c7845f":"Well, this is more obvious.","dcfc4a1b":"Since we can see that the first node is love smaller than 0.5 or not, it means that either there is a love in the review or not. It contributes about 0.27 percent to the model","149c24f7":"It's not obvious but rating 5 has lower distribution and median.","f867cfa7":"I use SelectKBest library to keep only 2000 words for training. However, it performs worse for the Naive Bayes models, but better for the random forest classifier.","831db9da":"## As many of us might share same doubt: Are these voice assistants really improving our lives?\nLots of people think they will only make us lazier. While some of us think they can have more time for works or other things with the help of voice assistants. I'll analyze these Alexa reviews to see if the users really think Alexa make their lives better.   \n   \nI'll start with words cloud to see whate words show up frequently and then dive into the reviews to summarize the positive review and negative reviews.\n\nThe ultimate goal is to find the pain points for those customers who don't like Alexa. And to build a NLP model predicting the rating by review!\n\n* [Exploratory & Feature Engineer](#0)\n - [Text preprocessing](#p)\n* [Counter](#1)   \n* [SpaCy](#4)      \n* [Modeling](#2)    \n - [Preprocessing](#pp)\n - [Model Tuning](#MT)\n - [Model Explanation and Evaluation](#ME)","50ef1a3d":"Here we can see that love is where the model start with, followed by work, buy, times, sound, and odd in the second layer. The other nodes, we can see how a simple decision tree model will classify the data into positive reviews or not.","bdbcc1a2":"Well, this is more straightforward. We can see the probibility is high on the left top and descreasing toward right bottom side.","632e1172":"Obviously, it's easy to predict positive (rating 5). But since the labels (positive or not) is pretty biased, the negative ratings are hard to catch. Only half of them are successfully predicted. Let's dive into more about it!\n\n2019-1-23 update: I remove some meaningless words from the review and make it to predict 3 reivew correctly. I'll dive deeper to see how can I preprocess the text","a248c4e5":"## <a id=\"ME\">Model Explaination and evaluation<\/a>","50450485":"# <a id=\"0\">Exploratory<\/a>","6b3d9db6":"## We can see the entities they identify are different. And we need to find a right one for your analysis\n\nI'll leave it here and move on to the modeling","0202634e":"### So the Spacy can find out the specific words for you. However, it depends on the package you load in to be accurate or not\n\nI'll demonstrate three different models below:","606b14fb":"We can tell that the NLP model is really complicated. Even though *love* is the main words for predicting the target, it doesn't really occupy the most of the factors. Only  some of the reviews with 1 or 2 *love* have darker dots and somehow effect the prediction. ","f44ffdbf":"### <a id=\"pp\">Preprocessing<\/a>","ee701c32":"The bad reviews are most about:\n\n* Not fully-enabled service\n* Echo works worse than other Amazon products","1f34ffe2":"## <a id=\"2\">Modeling<\/a>\n\n## NLP start from here!"}}