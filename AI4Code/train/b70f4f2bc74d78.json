{"cell_type":{"dcfaa0fa":"code","7e95636e":"code","da73de86":"code","b1504a36":"code","6d43ac00":"code","67d7459d":"markdown","f709ff84":"markdown","b6d7e292":"markdown","0c69e8b2":"markdown","e187405a":"markdown"},"source":{"dcfaa0fa":"doc_a = 'this document is first document'\ndoc_b = 'this document is the second document'\n\nbag_of_words_a = doc_a.split(' ')\nbag_of_words_b = doc_b.split(' ')\n\nunique_words_set = set(bag_of_words_a).union(set(bag_of_words_b))\nprint(unique_words_set)\n\n# Now create a dictionary of words and their occurence for each document in the corpus (collection of documents).\n\ndict_a = dict.fromkeys(unique_words_set, 0)\n# print(dict_a) # {'this': 0, 'document': 0, 'second': 0, 'is': 0, 'the': 0}\n\nfor word in bag_of_words_a:\n    dict_a[word] += 1\n\nprint(dict_a)\n# {'this': 1, 'document': 2, 'second': 1, 'is': 1, 'the': 1}\n\n# similarly\n\ndict_b = dict.fromkeys(unique_words_set, 0)\n\nfor word in bag_of_words_b:\n    dict_b[word] += 1\n\nprint(dict_b)\n# {'this': 1, 'document': 2, 'second': 1, 'is': 1, 'the': 1}","7e95636e":"def compute_term_frequency(word_dictionary, bag_of_words):\n    term_frequency_dictionary = {}\n    length_of_bag_of_words = len(bag_of_words)\n\n    for word, count in word_dictionary.items():\n        term_frequency_dictionary[word] = count \/ float(length_of_bag_of_words)\n\n    return term_frequency_dictionary\n\n# Implementation\n\nprint(compute_term_frequency(dict_a, bag_of_words_a))","da73de86":"import math\n\ndef compute_inverse_document_frequency(full_doc_list):\n    idf_dict = {}\n    length_of_doc_list = len(full_doc_list)\n\n    idf_dict = dict.fromkeys(full_doc_list[0].keys(), 0)\n    for word, value in idf_dict.items():\n        idf_dict[word] = math.log(length_of_doc_list \/ (float(value) + 1))\n\n    return idf_dict\n\nfinal_idf_dict = compute_inverse_document_frequency([dict_a, dict_b])\nprint(final_idf_dict)","b1504a36":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\n\ncorpus_1 = [\n     'this is sunny morning',\n     'yesterday was a gloomy morning',\n     'tomorrow will be rainly morning'\n]\n\nvectorizer.fit(corpus_1)\n\n","6d43ac00":"skl_tf_idf_vectorized = vectorizer.transform(corpus_1)\n\n# As the final output of sklearn tf-idf vectorizer is a sparse matrix to save storage space\n# To visually understand the output better, we need to convert the sparse output matrix to dense matrix with toarray()\nprint(skl_tf_idf_vectorized.toarray())\n# print(skl_tf_idf_vectorized[0])\n\n# As above Even more clear way to visually inspect the output is to convert it to a pandas dataframe\n# So below I will convert that to a dataframe and then use todense()\nskl_tfdf_output = skl_tf_idf_vectorized[0]\ndf_tfdf_sklearn = pd.DataFrame(skl_tfdf_output.T.todense(), index=vectorizer.get_feature_names(), columns=['tf-idf'])\ndf_tfdf_sklearn.sort_values(by=[\"tf-idf\"], ascending=True)\ndf_tfdf_sklearn","67d7459d":"## TF-IDF Model\n\nThere are some potential problems that might arise with the Bag of Words model when it is used on large corpora. Since the feature vectors are based on absolute term frequencies, there might be some terms that occur frequently across all documents and these may tend to overshadow other terms in the feature set. Especially words that don\u2019t occur as frequently, but might be more interesting and effective as features to identify specific categories. This is where TF-IDF comes into the picture. TF-IDF stands for term frequency-inverse document frequency. It\u2019s a combination of two metrics, term frequency (tf ) and inverse document frequency (idf ). This technique was originally developed as a\nmetric for ranking search engine results based on user queries and has come to be a part of information retrieval and text feature extraction.\n\n#### Suppose the query is \u201calbert einstein\u201c. Then, score(article) = TFIDF(article, \u201calbert\u201d) + TFIDF(article, \u201ceinstein\u201d). Our search engine will display the articles sorted by score.\n\nLet\u2019s formally define TF-IDF now and look at the mathematical representations before diving into its implementation.\n\n#### Mathematically, TD-IDF is the product of two metrics and can be represented as follows:\n\n\n###  $tfidf = tf  * idf$\n\n\nwhere term frequency (tf) and inverse-document frequency (idf) represent the two metrics we just talked about.\n\n## Term Frequency (TF)\n\nTerm frequency in any document vector is denoted by the raw frequency value of that term in a particular document. Mathematically it can be represented as follows:\n\n### $tf (w ,D ) = f_{wD} $\n\nwhere $f_{wD}$ denoted frequency for word w in document D, which becomes the term frequency (tf ). Sometimes you can also normalize the absolute raw frequency using logarithms or averaging the frequency. We use the raw frequency in our computations.\n\n\nSo, in other words, The number of times a word appears in a document divded by the total number of words in the document. Every document has its own term frequency.\n\n![img](https:\/\/i.imgur.com\/zdAyxl8.png)\n\nFollowing Python code can compute **Term Frequency**","f709ff84":"## TF-IDF Applications\n\n- Information retrieval: by calculating the TF-IDF score of a user query against the whole document set we can figure out how relevant a document is to that given query. Many say, most search engines use some form of TF-IDF implementation.\n\n- Keywords extraction: The highest ranking words for a document in terms of TF-IDF score can very well represent the keywords of that document(as they make that document stand out from the other documents). So we can very easily use some sort of TF-IDF score computation to extract the keywords from a text.\n\n\n## In general whats exactly fit and transform method do in-scikit-learn\n\nWhat `fit()` method does is create a model that extracts the various parameters from your training samples to do the neccessary transformation later on. transform() on the other hand is doing the actual transformation to the data itself returning a standardized or scaled form.\n\n`fit_transform()` is just a faster way of doing the operations of fit() and transform() consequently.\n\nLet us take an example for Scaling values in a dataset:\n\nHere the fit method, when applied to the training dataset, learns the model parameters (for example, mean and standard deviation). We then need to apply the transform method on the training dataset to get the transformed (scaled) training dataset. We could also perform both of this steps in one step by applying fit_transform on the training dataset.\n\nHence, every sklearn's transform's fit() just calculates the relevant parameters (e.g. \u03bc and \u03c3 in case of StandardScaler) and saves them as an internal object's state. Afterwards, you can call its transform() method to apply the transformation to any particular set of examples.\n\n#### Then why do we need 2 separate methods - fit and transform ?\n\n##### We use fit_transform() on the train data so that we learn the parameters of scaling on the train data and in the same time we scale the train data. We only use transform() on the test data because we use the scaling paramaters learned on the train data to scale the test data.\n\nAnd here's why we do like that in detail\n\nIn practice we need to have a separate training and testing dataset and that is where having a separate fit and transform method helps. We apply fit on the training dataset and use the transform method on both - the training dataset and the test dataset. Thus the training as well as the test dataset are then transformed(scaled) using the model parameters that were learnt on applying the fit method the training dataset.\n\nImportant thing here is that when you divide your dataset into train and test sets what you are trying to achieve is somewhat simulate a real world application. In a real world scenario you will only have training data and you will develop a model according to that and predict unseen instances of similar data.\n\nIf you transform the entrire data with fit_transform() and then split to train test you violate that simulation approach and do the transformation according to the unseen examples as well. Which will inevatibly result in an optimistic model as you already somewhat prepared your model by the unseen samples metrics as well.\n\nIf you split the data to train test and apply fit_transform() to both you will also be mistaken as your first transformation of train data will be done by train splits metrics only and your second will be done by test metrics only.\n\nThe right way to do these preprocessings is to train any transformer with train data only and do the transformations to the test data. Because only then you can be sure that your resulting model represents a real world solution.\n\nExample Code:\n\n```python\nscaler = preprocessing.StandardScaler().fit(X_train)\nscaler.transform(X_train)\nscaler.transform(X_test)\n```\n\nNote that mean and std obtained from the training set are used for scaling all training dataset values. And we should not compute a separate mean and std on the test set to scale the test set values instead we have to use the ones obtained using fit on the training set. We have to ensure identical operation on test set.\n\nThe idea is, once we executed `t.fit(train_data)`, t is fitted, so you can safely use\n\n`t.transform(test_data)`\n\n## Now implementation with sklearn","b6d7e292":"In this post, I shall go over TF-IDF Model and its implementation with Scikit-learn.\n\n\n## Traditional Feature Engineering Models\n\nTraditional (count-based) feature engineering strategies for textual data belong to a family of models popularly known as the Bag of Words model. This includes term frequencies, TF-IDF (term frequency-inverse document frequency), N-grams, topic models, and so on. While they are effective methods for extracting features from text, due to the inherent nature of the model being just a bag of unstructured words, we lose\nadditional information like the semantics, structure, sequence, and context around nearby words in each text document.\n\n## Bag of Words Model\n\nThis is perhaps the most simple vector space representational model for unstructured text. A vector space model is simply a mathematical model to represent unstructured text (or any other data) as numeric vectors, such that each dimension of the vector is a specific feature\/attribute. The Bag of Words model represents each text document as a numeric vector where each dimension is a specific word from the corpus and the value could be its frequency in the document, occurrence (denoted by 1 or 0), or even weighted values. The model\u2019s name is such because each document is represented literally as a bag of its own words, disregarding word order, sequences, and grammar.","0c69e8b2":"#### The IDF is computed once for all documents applying following formulae\n\n### $idfs = compute_inverse_document_frequency([num_of_words_a, num_of_words_b])$\n\nLastly, the TF-IDF is simply the TF multiplied by IDF.\n\n\n## the TF-IDF is simply the TF multiplied by IDF\n\n![img](https:\/\/i.imgur.com\/OCRidLj.png)\n\n#### So then TF-IDF is a score which is applied to every word in every document in our dataset. And for every word, the TF-IDF value increases with every appearance of the word in a document, but is gradually decreased with every appearance in other documents.\n\nNow suppose the word Python doesn\u2019t appear in any of our other blog posts. We draw the conclusion that the word Python isn\u2019t very relevant to most of our blog posts, but very relevant to post #2, where it appeared dozens of times. The inverse document frequency (IDF) tells us how important a term is to a collection of documents. A good example of how IDF comes into play is for the word \u201cthe.\u201d We know that just about every document contains \u201cthe,\u201d so the term isn\u2019t really special anymore, thereby producing a very low IDF. Now let\u2019s contrast \u201cthe\u201d with \u201cPython\u201d in our example. \u201cPython\u201d appears rarely in the other posts, so its IDF should be high. In fact, \u201cPython\u201d now carries a weight signaling that in any document in which it appears, it is important to that document.\n\n#### When we multiply TF and IDF, we observe that the larger the number, the more important a term in a document is to that document. We can then compute the TF-IDF for each word in each document and create a vector.\n\n---\n\n## [sk-learn's implementation of tf-idf](https:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html#text-feature-extraction)\n\n- Sklearn does couple of tweaks in its implementation of TF-IDF vectorizer, so to replicate the exact results our custom TF-DF would need to add following\n\n- Sklearn formula of idf is different from the standard textbook formula. Here the constant \"1\" is added to the numerator and denominator of the idf as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions.\n\n- Sklearn has its vocabulary generated from idf sorted in alphabetical order\n\n- The final output of sklearn tf-idf vectorizer is a sparse matrix\n\n![img](https:\/\/i.imgur.com\/U14jzLo.png)\n\n[Source](https:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html#text-feature-extraction)\n\n[The below from Sklearn's source code](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/9780abda5fd54a491a6a98cd542da02094f912a5\/sklearn\/feature_extraction\/text.py#L1314)\n\n\"The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.\n\n    The formula that is used to compute the tf-idf for a term t of a document d in a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is computed as **idf(t) = log [ n \/ df(t) ] + 1** (if ``smooth_idf=False``), where\n    n is the total number of documents in the document set and df(t) is the document frequency of t; the document frequency is the number of documents in the document set that contain the term t. The effect of adding \"1\" to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely\n    ignored.\n\n    (Note that the idf formula above differs from the standard textbook\n    notation that defines the idf as  idf(t) = log [ n \/ (df(t) + 1) ]).\n\n    If ``smooth_idf=True`` (the default), the constant \"1\" is added to the numerator and denominator of the idf as if an extra document was seen containing every term in the collection exactly once, which prevents\n    zero divisions: idf(t) = log [ (1 + n) \/ (1 + df(t)) ] + 1.\"\n\n---\n\n## A note on normalization - and why sklearn normalize the final tf-idf matrix derived from its  TfidfVectorizer class\n\n![img](https:\/\/i.imgur.com\/DxlL7sm.png)\n\n#### What does it mean to normalize an array?\n\nData normalization is used in machine learning to make model training less sensitive to the scale of features. This allows our model to converge to better weights and, in turn, leads to a more accurate model. Normalization makes the features more consistent with each other, which allows the model to predict outputs more accurately.\n\nTo normalize a vector in math means to divide each of its elements to some value V so that the length\/norm of the resulting vector is 1. Turns out the needed V is equal the length (the length of the vector).\n\nSo this is basically the norm calculations\n\n![img](https:\/\/i.imgur.com\/4kPSrDI.png)\n\nFor a vector x having N components, the L\u00b9 just adds up the components. Since we would like our magnitude to always be positive, we take the absolute value of the components. The L\u00b2 norm takes the sum of the squared values, taking the square root at the end. \n\nSay you have this array.\n\n`[-3, +4]`\n\nIts length (in Euclid metric) is: `V = sqrt((-3)^2 + (+4)^2) = 5`\n\nSo its corresponding normalized vector is:\n\n`[-3\/5, +4\/5]`\n\nIts length is now: `sqrt ( (-3\/5)^2 + (+4\/5)^2 )` which is 1.\n\nYou can use another metric (e.g. I think Manhattan distance)\nbut the idea is the same. Divide each element of your array\nby `V` where `V = || your_vector || = norm (your_vector)`.\n\n[Read further](https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html#preprocessing-normalization)\n\nWhile deriving TF-DF, the final TF-IDF metric that we will be using is a normalized version of the tfidf matrix that we get from the product of tf and idf. We will normalize the tfidf matrix by dividing it by the L2 norm of the matrix, also known as the Euclidean norm, which is the square root of the sum of the square of each term\u2019s tfidf weight. Mathematically we can represent the final tfidf feature vector as follows:\n\n ### $tfidf= tfidf \/ || tfidf ||$\n\nwhere \u2225tfidf\u2225 represents the Euclidean L2 norm for the tfidf matrix. There are multiple variants of this model but they all end up with similar results.\n\nInformally speaking, the norm is a generalization of the concept of (vector) length; from the [Wikipedia entry][1]:\n\n> In linear algebra, functional analysis, and related areas of mathematics, a **norm** is a function that assigns a strictly positive *length* or *size* to each vector in a vector space.\n\nThe [L2-norm][2] is the usual Euclidean length, i.e. the square root of the sum of the squared vector elements.\n\nThe [L1-norm][3] is the sum of the absolute values of the vector elements.\n\n---\n\n## Normalizing a Vector\n\nFirst the absolute basics of what norm is -\n\n![img](https:\/\/i.imgur.com\/AgFmtNY.png)\n\nSo, now, calculate the norm of the vector u\u20d7 =(3,4).\n\nWe first note that u\u20d7 \u2208 R2, and we will thus use the formula\n\n![img](https:\/\/i.imgur.com\/3MH9JXD.png)\n\nWhen we substitute our values in, we obtain that \u2225u\u20d7 \u2225=\u221a25 = 5. Thus our vector u\u20d7  has length \/ norm of 5.\n\n![img](https:\/\/i.imgur.com\/bbYMkwl.png)\n\nMathematically a norm is a total size or length of all vectors in a vector space or matrices. And after we calculate the Norm, then we can normalize a vector. By definition a norm on a vector space\u2014over the real or complex field\u2014is an assignment of a non-negative real number to a vector. The norm of a vector is its length, and the length of a vector must always be positive (or zero). A negative length makes no sense.\n\nWhen we think of geometric vectors, i.e., directed line segments that start at the origin, then intuitively the length of a vector is the distance of the \u201cend\u201d of this directed line segment from the origin.\n\nTaking any vector and reducing its magnitude to 1.0 while keeping its direction is called normalization. Normalization is performed by dividing the x and y (and z in 3D) components of a vector by its magnitude:\n\nFor any vector V = (x, y, z),\n\nwe know the magnitude |V| = sqrt(x*x + y*y + z*z) which gives the length of the vector.\n\nWhen we normalize a vector, we actually calculate\n\n#### V\/|V| = (x\/|V|, y\/|V|, z\/|V|).\n\n#### Lets look at an example\n\n![img](https:\/\/i.imgur.com\/2PCb8nh.png)\n\nCan do some basic calculation to see that a normalized vector has length 1. This is because:\n\n(In first line below I to bring the sqrt outside of the braces, I am multiplying x\/|V| with x\/|V| and so on )\n\n```\n| V\/|V| | = sqrt((x\/|V|)*(x\/|V|) + (y\/|V|)*(y\/|V|) + (z\/|V|)*(z\/|V|))\n          = sqrt(x*x + y*y + z*z) \/ |V|\n          = |V| \/ |V|\n          = 1\n```\n\n\n## Differences between Norm of a Vector and distance between two points\n\n#### Key point to remember - Distance are always between two points and Norm are always for a Vector.\n\n#### That means Euclidean Distance between 2 points x1 and x2 is nothing but the L2 norm of vector (x1 - x2)\n\nBy definition L2 Norm of a vector = Euclidian distance of that point vector from origin.\n\nIn other words, the distance(metric) between any two vectors can be defined as the norm of the difference those vectors.\n\n![img](https:\/\/i.imgur.com\/cYxlXSv.png)","e187405a":"## Inverse document frequency\nInverse document frequency denoted by idf is the inverse of the document frequency for each term and is computed by dividing the total number of documents in our corpus by the document frequency for each term and then applying logarithmic scaling to the result.\n\n\nIn some implementation, people will be adding 1 to the document frequency for each term to indicate that we also have one more document in our corpus, which essentially has every term in the vocabulary. This is to prevent potential division by zero errors and smoothen the inverse document frequencies. This is to avoid ignoring terms that might have zero idf.\n\n #### Inverse Document Frequency, measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones\n\nMathematically, our implementation for idf can be represented as follows:\n\n![img](https:\/\/i.imgur.com\/LBoEWr8.png)\n\nThe log of the number of documents divided by the number of documents that contain the word w. Inverse data frequency determines the weight of rare words across all documents in the corpus.\n\nConsider a document containing 100 words wherein the word cat appears 3 times. The term frequency (i.e., tf) for cat is then (3 \/ 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 \/ 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.\n\nFollowing Python code can compute **Inverse document frequency**"}}