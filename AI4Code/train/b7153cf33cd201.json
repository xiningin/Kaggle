{"cell_type":{"d936459c":"code","0e3a8237":"code","2247f7bc":"code","52138a99":"code","fdc22bee":"code","ccce1462":"code","dba1571b":"code","081d4a68":"code","d8ed5a8d":"code","84ebca8c":"code","0b6e5a3e":"code","9304c9a1":"code","76fdf15f":"code","c821cfc2":"code","ae15982c":"code","3a083c5e":"code","351c2403":"code","004b5775":"code","0741ea22":"code","e7d3357c":"code","a31ff3bf":"code","9d3ffe96":"code","9eabb09b":"code","6c007189":"code","ef268e27":"code","d0d876fc":"code","301c6f09":"code","b8ffbeea":"code","27cb15b0":"code","b8c42746":"code","f5fd9212":"code","69fb0ee2":"code","1a2bcaaf":"code","abdc7de6":"code","45280762":"code","68ac2db3":"code","7cef8a5a":"code","2e75c462":"code","d03d5315":"code","1a0a7d2d":"code","06490dc9":"code","4a323d16":"code","1c28e595":"code","7eb34cfe":"code","67198655":"code","e5035047":"code","68c282a0":"code","1e22536e":"code","710d72cc":"code","860d345e":"markdown","48ecc4d5":"markdown","273849f8":"markdown","a618c63f":"markdown","be2a33ed":"markdown","aea47096":"markdown","2996496a":"markdown","07531b41":"markdown","6f8c2a01":"markdown","df194b92":"markdown","73a9220a":"markdown","f4f2edec":"markdown","4be344a1":"markdown","704b2951":"markdown","1e726f68":"markdown","e0d86d27":"markdown","d067c3b5":"markdown","26f5e9ab":"markdown","9a732521":"markdown","d1116d03":"markdown","ee846ae1":"markdown"},"source":{"d936459c":"import pandas as pd","0e3a8237":"#MODEL_PATH = \".\/YOLOX_outputs\/cots_config\/best_ckpt.pth\"\n!ls \/kaggle\/working\/YOLOX\/YOLOX_outputs\/cots_config","2247f7bc":"df = pd.read_csv(\"\/kaggle\/input\/tensorflow-great-barrier-reef\/train.csv\")\ndf.head(5)","52138a99":"print(\"\\n video_id\", \"\\n max: \", df.video_id.max(), \"\\n min: \", df.video_id.min(), \"\\n count: \", len(set(df.video_id)))\nprint(\"\\n sequence: \", \"\\n max: \", df.sequence.max(), \"\\n min: \", df.sequence.min(), \"\\n count: \", len(set(df.sequence)))\nprint(\"\\n sequence_frame\", \"\\n max: \", df.sequence_frame.max(), \"\\n min: \", df.sequence_frame.min(), \"\\n count: \", len(set(df.sequence_frame)))\n\n\nprint(\"\\n video_frame\", \"\\n max: \", df.video_frame.max(), \"\\n min: \", df.video_frame.min(), \"\\n count: \", len(set(df.video_frame)))\nprint(\"\\n image_id\", \"\\n max: \", df.image_id.max(), \"\\n min: \", df.image_id.min(), \"\\n count: \", len(set(df.image_id)))\nprint(\"\\n annotations\", \"\\n max: \", df.annotations.max(), \"\\n min: \", df.annotations.min(), \"\\n count: \", len(set(df.annotations)))","fdc22bee":"# for each video_id: 0,1,2; \n# input row: can be a pandas dataframe; where we have video_id & video_frame columns\nTRAIN_PATH = '\/kaggle\/input\/tensorflow-great-barrier-reef'\n\ndef get_path(row):\n    row['image_path'] = f'{TRAIN_PATH}\/train_images\/video_{row.video_id}\/{row.video_frame}.jpg'\n    return row\n\ndef get_bbox(annots):\n    bbox = [list(annot.values()) for annot in annots]\n    return bbox","ccce1462":"df[\"num_bbox\"] = df['annotations'].apply(lambda k: str.count(k, 'x'))\nprint(df[\"num_bbox\"].max(), df[\"num_bbox\"].min())\ndf[df[\"num_bbox\"]==df[\"num_bbox\"].max()][\"annotations\"]","dba1571b":"df_train = df[df[\"num_bbox\"]>0]\ndf.shape, df_train.shape","081d4a68":"from tqdm.notebook import tqdm\ntqdm.pandas()\nimport ast","d8ed5a8d":"df_train['annotations'] = df_train['annotations'].progress_apply(lambda x: ast.literal_eval(x))","84ebca8c":"df_train['bboxes'] = df_train.annotations.progress_apply(get_bbox)","0b6e5a3e":"df_train[df_train[\"num_bbox\"]==df_train[\"num_bbox\"].max()].head","9304c9a1":"df_train[\"width\"] = 1280\ndf_train[\"height\"] = 720","76fdf15f":"df_train","c821cfc2":"df_train = df_train.progress_apply(get_path, axis=1)","ae15982c":"print(df_train['image_path'].iloc[0])","3a083c5e":"from sklearn.model_selection import GroupKFold","351c2403":"kf = GroupKFold(n_splits = 5) \ndf_train = df_train.reset_index(drop=True)\ndf_train['fold'] = -1","004b5775":"for fold, (train_idx, val_idx) in enumerate(kf.split(df_train, y = df_train.video_id.tolist(), groups=df_train.sequence)):\n    df_train.loc[val_idx, 'fold'] = fold\n\ndf_train.head(5)","0741ea22":"from collections import Counter\nCounter(df_train.fold)","e7d3357c":"HOME_DIR = '\/kaggle\/working\/' \nDATASET_PATH = 'dataset\/images'\n\n!mkdir {HOME_DIR}dataset\n!mkdir {HOME_DIR}{DATASET_PATH}\n!mkdir {HOME_DIR}{DATASET_PATH}\/train2017\n!mkdir {HOME_DIR}{DATASET_PATH}\/val2017\n!mkdir {HOME_DIR}{DATASET_PATH}\/annotations","a31ff3bf":"from shutil import copyfile\n\nSELECTED_FOLD = 4\n\nfor i in tqdm(range(len(df_train))):\n    row = df_train.loc[i]\n    if row.fold != SELECTED_FOLD:\n        copyfile(f'{row.image_path}', f'{HOME_DIR}{DATASET_PATH}\/train2017\/{row.image_id}.jpg')\n    else:\n        copyfile(f'{row.image_path}', f'{HOME_DIR}{DATASET_PATH}\/val2017\/{row.image_id}.jpg') ","9d3ffe96":"import os\n\nprint(f'Number of training files: {len(os.listdir(f\"{HOME_DIR}{DATASET_PATH}\/train2017\/\"))}')\nprint(f'Number of validation files: {len(os.listdir(f\"{HOME_DIR}{DATASET_PATH}\/val2017\/\"))}')","9eabb09b":"def save_annot_json(json_annotation, filename):\n    with open(filename, 'w') as f:\n        output_json = json.dumps(json_annotation)\n        f.write(output_json)","6c007189":"annotion_id = 0","ef268e27":"def dataset2coco(df, dest_path):\n    \n    global annotion_id\n    \n    annotations_json = {\n        \"info\": [],\n        \"licenses\": [],\n        \"categories\": [],\n        \"images\": [],\n        \"annotations\": []\n    }\n    \n    info = {\n        \"year\": \"2021\",\n        \"version\": \"1\",\n        \"description\": \"COTS dataset - COCO format\",\n        \"contributor\": \"\",\n        \"url\": \"https:\/\/kaggle.com\",\n        \"date_created\": \"2021-11-30T15:01:26+00:00\"\n    }\n    annotations_json[\"info\"].append(info)\n    \n    lic = {\n            \"id\": 1,\n            \"url\": \"\",\n            \"name\": \"Unknown\"\n        }\n    annotations_json[\"licenses\"].append(lic)\n\n    classes = {\"id\": 0, \"name\": \"starfish\", \"supercategory\": \"none\"}\n\n    annotations_json[\"categories\"].append(classes)\n\n    \n    for ann_row in df.itertuples():\n            \n        images = {\n            \"id\": ann_row[0],\n            \"license\": 1,\n            \"file_name\": ann_row.image_id + '.jpg',\n            \"height\": ann_row.height,\n            \"width\": ann_row.width,\n            \"date_captured\": \"2021-11-30T15:01:26+00:00\"\n        }\n        \n        annotations_json[\"images\"].append(images)\n        \n        bbox_list = ann_row.bboxes\n        \n        for bbox in bbox_list:\n            b_width = bbox[2]\n            b_height = bbox[3]\n            \n            # some boxes in COTS are outside the image height and width\n            if (bbox[0] + bbox[2] > 1280):\n                b_width = bbox[0] - 1280 \n            if (bbox[1] + bbox[3] > 720):\n                b_height = bbox[1] - 720 \n                \n            image_annotations = {\n                \"id\": annotion_id,\n                \"image_id\": ann_row[0],\n                \"category_id\": 0,\n                \"bbox\": [bbox[0], bbox[1], b_width, b_height],\n                \"area\": bbox[2] * bbox[3],\n                \"segmentation\": [],\n                \"iscrowd\": 0\n            }\n            \n            annotion_id += 1\n            annotations_json[\"annotations\"].append(image_annotations)\n        \n        \n    print(f\"Dataset COTS annotation to COCO json format completed! Files: {len(df)}\")\n    return annotations_json","d0d876fc":"import json\n\n# Convert COTS dataset to JSON COCO\ntrain_annot_json = dataset2coco(df_train[df_train.fold != SELECTED_FOLD], f\"{HOME_DIR}{DATASET_PATH}\/train2017\/\")\nval_annot_json = dataset2coco(df_train[df_train.fold == SELECTED_FOLD], f\"{HOME_DIR}{DATASET_PATH}\/val2017\/\")\n\n# Save converted annotations\nsave_annot_json(train_annot_json, f\"{HOME_DIR}{DATASET_PATH}\/annotations\/train.json\")\nsave_annot_json(val_annot_json, f\"{HOME_DIR}{DATASET_PATH}\/annotations\/valid.json\")","301c6f09":"NANO = True\n\nif NANO:\n    config_file_template = '''\n\n#!\/usr\/bin\/env python3\n# -*- coding:utf-8 -*-\n# Copyright (c) Megvii, Inc. and its affiliates.\n\nimport os\n\nimport torch.nn as nn\n\nfrom yolox.exp import Exp as MyExp\n\n\nclass Exp(MyExp):\n    def __init__(self):\n        super(Exp, self).__init__()\n        self.depth = 0.33\n        self.width = 0.25\n        self.input_size = (416, 416)\n        self.mosaic_scale = (0.5, 1.5)\n        self.random_size = (10, 20)\n        self.test_size = (416, 416)\n        self.exp_name = os.path.split(\n            os.path.realpath(__file__))[1].split(\".\")[0]\n        self.enable_mixup = False\n\n        # Define yourself dataset path\n        self.data_dir = \"\/kaggle\/working\/dataset\/images\"\n        self.train_ann = \"train.json\"\n        self.val_ann = \"valid.json\"\n\n        self.num_classes = 1\n\n        self.max_epoch = $max_epoch\n        self.data_num_workers = 2\n        self.eval_interval = 1\n\n    def get_model(self, sublinear=False):\n        def init_yolo(M):\n            for m in M.modules():\n                if isinstance(m, nn.BatchNorm2d):\n                    m.eps = 1e-3\n                    m.momentum = 0.03\n\n        if \"model\" not in self.__dict__:\n            from yolox.models import YOLOX, YOLOPAFPN, YOLOXHead\n            in_channels = [256, 512, 1024]\n            # NANO model use depthwise = True, which is main difference.\n            backbone = YOLOPAFPN(self.depth,\n                                 self.width,\n                                 in_channels=in_channels,\n                                 depthwise=True)\n            head = YOLOXHead(self.num_classes,\n                             self.width,\n                             in_channels=in_channels,\n                             depthwise=True)\n            self.model = YOLOX(backbone, head)\n\n        self.model.apply(init_yolo)\n        self.model.head.initialize_biases(1e-2)\n        return self.model\n\n'''","b8ffbeea":"!git clone https:\/\/github.com\/Megvii-BaseDetection\/YOLOX -q\n\n%cd YOLOX\n!pip install -U pip && pip install -r requirements.txt\n!pip install -v -e . ","27cb15b0":"!pip install 'git+https:\/\/github.com\/cocodataset\/cocoapi.git#subdirectory=PythonAPI'","b8c42746":"from string import Template\n\nPIPELINE_CONFIG_PATH='cots_config.py'\n\npipeline = Template(config_file_template).substitute(max_epoch = 2)\n\nwith open(PIPELINE_CONFIG_PATH, 'w') as f:\n    f.write(pipeline)","f5fd9212":"# .\/yolox\/data\/datasets\/coco_classes.py\n\ncoco_cls = '''\nCOCO_CLASSES = (\n  \"starfish\",\n)\n'''\nwith open('.\/yolox\/data\/datasets\/coco_classes.py', 'w') as f:\n    f.write(coco_cls)\n\n# check if everything is ok    \n!more .\/yolox\/data\/datasets\/coco_classes.py","69fb0ee2":"sh = 'wget https:\/\/github.com\/Megvii-BaseDetection\/storage\/releases\/download\/0.0.1\/yolox_s.pth'\nMODEL_FILE = 'yolox_s.pth'\n\nif NANO:\n    sh = '''\n    wget https:\/\/github.com\/Megvii-BaseDetection\/storage\/releases\/download\/0.0.1\/yolox_nano.pth\n    '''\n    MODEL_FILE = 'yolox_nano.pth'\n\nwith open('script.sh', 'w') as file:\n    file.write(sh)\n\n!bash script.sh","1a2bcaaf":"!cp .\/tools\/train.py .\/","abdc7de6":"!python train.py \\\n    -f cots_config.py \\\n    -d 1 \\\n    -b 32 \\\n    --fp16 \\\n    -o \\\n    -c {MODEL_FILE}   # Remember to chenge this line if you take different model eg. yolo_nano.pth, yolox_s.pth or yolox_m.pth","45280762":"!cp .\/YOLOX_outputs\/cots_config\/best_ckpt.pth \/kaggle\/working\/YOLOX\/YOLOX_outputs\/cots_config\/best_ckpt.pth","68ac2db3":"!ls \/kaggle\/working\/YOLOX\/YOLOX_outputs\/cots_config","7cef8a5a":"!ls \/kaggle\/working\/YOLOX\/YOLOX_outputs\/cots_config","2e75c462":"%cp ..\/..\/input\/yolox-kaggle-fix-for-demo-inference\/demo.py tools\/demo.py\nTEST_IMAGE_PATH = \"\/kaggle\/working\/dataset\/images\/val2017\/0-4614.jpg\"\nMODEL_PATH = \".\/YOLOX_outputs\/cots_config\/best_ckpt.pth\"\n\n!python tools\/demo.py image \\\n    -f cots_config.py \\\n    -c {MODEL_PATH} \\\n    --path {TEST_IMAGE_PATH} \\\n    --conf 0.1 \\\n    --nms 0.45 \\\n    --tsize 960 \\\n    --save_result \\\n    --device gpu","d03d5315":"from yolox.utils import postprocess\nfrom yolox.data.data_augment import ValTransform\nimport importlib\nimport torch\n\nCOCO_CLASSES = (\n  \"starfish\",\n)\n\n# get YOLOX experiment\ncurrent_exp = importlib.import_module('cots_config')\nexp = current_exp.Exp()\n\n# set inference parameters\ntest_size = (960, 960)\nnum_classes = 1\nconfthre = 0.1\nnmsthre = 0.45\n\n\n# get YOLOX model\nmodel = exp.get_model()\nmodel.cuda()\nmodel.eval()\n\n# get custom trained checkpoint\nckpt_file = \".\/YOLOX_outputs\/cots_config\/best_ckpt.pth\"\nckpt = torch.load(ckpt_file, map_location=\"cpu\")\nmodel.load_state_dict(ckpt[\"model\"])","1a0a7d2d":"def yolox_inference(img, model, test_size): \n    bboxes = []\n    bbclasses = []\n    scores = []\n    \n    preproc = ValTransform(legacy = False)\n\n    tensor_img, _ = preproc(img, None, test_size)\n    tensor_img = torch.from_numpy(tensor_img).unsqueeze(0)\n    tensor_img = tensor_img.float()\n    tensor_img = tensor_img.cuda()\n\n    with torch.no_grad():\n        outputs = model(tensor_img)\n        outputs = postprocess(\n                    outputs, num_classes, confthre,\n                    nmsthre, class_agnostic=True\n                )\n\n    if outputs[0] is None:\n        return [], [], []\n    \n    outputs = outputs[0].cpu()\n    bboxes = outputs[:, 0:4]\n\n    bboxes \/= min(test_size[0] \/ img.shape[0], test_size[1] \/ img.shape[1])\n    bbclasses = outputs[:, 6]\n    scores = outputs[:, 4] * outputs[:, 5]\n    \n    return bboxes, bbclasses, scores","06490dc9":"def draw_yolox_predictions(img, bboxes, scores, bbclasses, confthre, classes_dict):\n    for i in range(len(bboxes)):\n            box = bboxes[i]\n            cls_id = int(bbclasses[i])\n            score = scores[i]\n            if score < confthre:\n                continue\n            x0 = int(box[0])\n            y0 = int(box[1])\n            x1 = int(box[2])\n            y1 = int(box[3])\n\n            cv2.rectangle(img, (x0, y0), (x1, y1), (0, 255, 0), 2)\n            cv2.putText(img, '{}:{:.1f}%'.format(classes_dict[cls_id], score * 100), (x0, y0 - 3), cv2.FONT_HERSHEY_PLAIN, 0.8, (0,255,0), thickness = 1)\n    return img","4a323d16":"import cv2 \nfrom PIL import Image\n\nTEST_IMAGE_PATH = \"\/kaggle\/working\/dataset\/images\/val2017\/0-4614.jpg\"\nimg = cv2.imread(TEST_IMAGE_PATH)\n\n# Get predictions\nbboxes, bbclasses, scores = yolox_inference(img, model, test_size)\n\n# Draw predictions\nout_image = draw_yolox_predictions(img, bboxes, scores, bbclasses, confthre, COCO_CLASSES)\n\n# Since we load image using OpenCV we have to convert it \nout_image = cv2.cvtColor(out_image, cv2.COLOR_BGR2RGB)\ndisplay(Image.fromarray(out_image))","1c28e595":"import greatbarrierreef\n\nenv = greatbarrierreef.make_env()   # initialize the environment\niter_test = env.iter_test()  ","7eb34cfe":"submission_dict = {\n    'id': [],\n    'prediction_string': [],\n}\n\nfor (image_np, sample_prediction_df) in iter_test:\n \n    bboxes, bbclasses, scores = yolox_inference(image_np, model, test_size)\n    \n    predictions = []\n    for i in range(len(bboxes)):\n        box = bboxes[i]\n        cls_id = int(bbclasses[i])\n        score = scores[i]\n        if score < confthre:\n            continue\n        x_min = int(box[0])\n        y_min = int(box[1])\n        x_max = int(box[2])\n        y_max = int(box[3])\n        \n        bbox_width = x_max - x_min\n        bbox_height = y_max - y_min\n        \n        predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n    \n    prediction_str = ' '.join(predictions)\n    sample_prediction_df['annotations'] = prediction_str\n    env.predict(sample_prediction_df)\n\n    print('Prediction:', prediction_str)","67198655":"sub_df = pd.read_csv('submission.csv')\nsub_df.head()","e5035047":"!ls","68c282a0":"!pwd & cd .. & pwd","1e22536e":"!ls \/kaggle\/input\/yolox-kaggle-fix-for-demo-inference","710d72cc":"df = pd.read_csv(\"\/kaggle\/input\/tensorflow-great-barrier-reef\/train.csv\")\nTRAIN_PATH = '\/kaggle\/input\/tensorflow-great-barrier-reef'","860d345e":"### inference bounding boxes ","48ecc4d5":"# LOAD LIBRARIES","273849f8":"# FIGURE","a618c63f":"# ii. CREATE TRAIN AND VALIDATION SETS","be2a33ed":"# vi. Inference ","aea47096":"#### create a row with width and height ","2996496a":"# i. LOAD DATA","07531b41":"## Hierarchy of data: 1) video 2) sequence 3) video_frame 4) image_id = (video_id + video_frame)","6f8c2a01":"## LOAD THE IMAGES","df194b92":"#### create a list that contains all the bounding boxes within the image","73a9220a":"#### ast.literal_eval raises an exception if the input isn't a valid Python datatype, so the code won't be executed if it's not.","f4f2edec":"### draw results","4be344a1":"# v. Install YOLOX","704b2951":"Configuration files for Yolox:\n- [YOLOX-nano](https:\/\/github.com\/Megvii-BaseDetection\/YOLOX\/blob\/main\/exps\/default\/nano.py)\n- [YOLOX-s](https:\/\/github.com\/Megvii-BaseDetection\/YOLOX\/blob\/main\/exps\/default\/yolox_s.py)\n- [YOLOX-m](https:\/\/github.com\/Megvii-BaseDetection\/YOLOX\/blob\/main\/exps\/default\/yolox_m.py)\n\nBelow you can find two (yolox-s and yolox-nano) configuration files for our COTS dataset training.\n\n<div align=\"center\"><img  width=\"800\" src=\"https:\/\/github.com\/Megvii-BaseDetection\/YOLOX\/raw\/main\/assets\/git_fig.png\"\/><\/div>","1e726f68":"### 3. Process annotations","e0d86d27":"# RUN INFERENCE","d067c3b5":"# vii. Submit","26f5e9ab":"# iv. PREPARE CONGIGURATION FILES AND COMPARE THE SOTA OBJECT DETECTION METHODS","9a732521":"### SETUP model","d1116d03":"# iii. CREATE ANNOTATION FILES IN COCO FORMAT SO THAT WE CAN USE THE PRETRAINED MODEL YOLOX","ee846ae1":"### 2. Take only images with bounding boxes"}}