{"cell_type":{"5d889092":"code","96091412":"code","bee620c5":"code","43087c99":"code","61c20089":"code","25d3a5ed":"code","3e589e5f":"code","a735d900":"code","560de174":"code","e0f7b0c5":"code","1fafd458":"code","4e7339f8":"code","ef349913":"code","bf57d9b8":"code","a473e321":"code","a37a2a4e":"code","8f1b7fb1":"code","745c6a00":"code","04a1f07c":"code","95f0a229":"code","305a634f":"code","4c8b6ec7":"code","b8f6ce11":"code","f11aaa2c":"code","d7ef30c5":"code","b9b71014":"code","802ae0a4":"code","32566e55":"code","ff05a203":"code","2b0c5ea8":"code","65283a11":"code","05417f59":"code","0979acf1":"code","91cc8cb2":"code","a586c45f":"code","dcd70f8f":"code","001bafa0":"code","7769f27e":"code","20cfff45":"code","bc8bc3d0":"code","071a5e88":"code","30194664":"code","0f4f10b1":"code","27594394":"code","45e2b7e1":"code","6fd6433e":"code","957b6870":"code","79988a01":"code","64512eb7":"code","2e601a11":"code","825dc510":"code","fc1cafe5":"code","fb1afd80":"code","a46b4a5a":"code","36bb6335":"code","c922a4f3":"code","ec59c8cf":"code","d570a25b":"code","a9b0c405":"code","8e9f7581":"code","27e9b7a8":"code","fffe8bbf":"code","222e683c":"code","026baf1f":"code","c3c3f811":"code","d41a2e5a":"code","60a9a418":"markdown","3d7832ed":"markdown","2df5b998":"markdown","a78e4f70":"markdown","97202278":"markdown","185fb82a":"markdown","6fb1ab0b":"markdown","277054ad":"markdown","83975fb4":"markdown","970740d9":"markdown","780db605":"markdown","343404f4":"markdown","3553eb45":"markdown","55373207":"markdown","4ccefd9c":"markdown","46422a24":"markdown","3a2ca383":"markdown","47790420":"markdown","5f2b538d":"markdown","c19a3fa7":"markdown","09f12a3b":"markdown"},"source":{"5d889092":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport missingno as msno\nimport seaborn as sns\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestRegressor, BaggingRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error, r2_score, mean_absolute_error  # for regression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingRegressor, VotingClassifier\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","96091412":"df = pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","bee620c5":"df.shape","43087c99":"df.columns","61c20089":"df.info()","25d3a5ed":"# no null or Nan values.\ndf.isnull().sum()","3e589e5f":"# just to visualize. no missing values.\nmsno.matrix(df, color=(0, 0, 0))","a735d900":"df.describe(include='all')","560de174":"sns.factorplot(data=df, kind='box', size=10, aspect=2.5)","e0f7b0c5":"df.hist(bins=10, figsize=(20, 20))\nplt.show()","1fafd458":"# corelation matrix.\ncor_mat = df.corr()\nmask = np.array(cor_mat)\nmask[np.tril_indices_from(mask)] = False\nfig = plt.gcf()\nfig.set_size_inches(30, 12)\nsns.heatmap(data=cor_mat, mask=mask, square=True, annot=True, cbar=True)","4e7339f8":"def plot(feature_x, target='quality'):\n    sns.factorplot(x=target, y=feature_x, data=df,\n                   kind='bar', size=5, aspect=1)\n    sns.factorplot(x=target, y=feature_x, data=df,\n                   kind='violin', size=5, aspect=1)\n    sns.factorplot(x=target, y=feature_x, data=df,\n                   kind='swarm', size=5, aspect=1)","ef349913":"# for fixed acidity.\nplot('fixed acidity', 'quality')","bf57d9b8":"# for alcohol.\nplot('alcohol', 'quality')","a473e321":"bins = (2, 6.5, 8)\ngroup_names = ['bad', 'good']\ndf['quality'] = pd.cut(df['quality'], bins=bins, labels=group_names)","a37a2a4e":"label_quality = LabelEncoder()","8f1b7fb1":"df.quality = label_quality.fit_transform(df.quality)","745c6a00":"X = df.drop('quality', axis=1)\ny = df.quality","04a1f07c":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=42)","95f0a229":"models = [LinearSVC(), SVC(kernel='rbf'), KNeighborsClassifier(), RandomForestClassifier(),\n          DecisionTreeClassifier(), GradientBoostingClassifier(), GaussianNB()]\nmodel_names = ['LinearSVM', 'rbfSVM', 'KNearestNeighbors', 'RandomForestClassifier', 'DecisionTree',\n               'GradientBoostingClassifier', 'GaussianNB']\n\nacc = []\n\nfor model in range(len(models)):\n    clf = models[model]\n    clf.fit(X_train, y_train)\n    pred = clf.predict(X_test)\n    acc.append(accuracy_score(pred, y_test))\n\nmodels = {'Modelling Algo': model_names, 'Accuracy': acc}","305a634f":"models_df = pd.DataFrame(models)","4c8b6ec7":"models_df","b8f6ce11":"sns.barplot(y='Modelling Algo', x='Accuracy', data=models_df)","f11aaa2c":"def feature_scaling(X_train, X_test, y_train, y_test, name_scaler):\n    models = [LinearSVC(), SVC(kernel='rbf'), KNeighborsClassifier(), RandomForestClassifier(),\n              DecisionTreeClassifier(), GradientBoostingClassifier(), GaussianNB()]\n    acc_sc = []\n    for model in range(len(models)):\n        clf = models[model]\n        clf.fit(X_train, y_train)\n        pred = clf.predict(X_test)\n        acc_sc.append(accuracy_score(pred, y_test))\n    models_df[name_scaler] = np.array(acc_sc)","d7ef30c5":"scalers = [MinMaxScaler(), StandardScaler()]\nnames = ['Acc_Min_Max_Scaler', 'Acc_Standard_Scaler']\nfor scale in range(len(scalers)):\n    scaler = scalers[scale]\n    scaler.fit(df)\n    scaled_df = scaler.transform(df)\n    X = scaled_df[:, 0:11]\n    Y = df.quality.to_numpy()\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.20, random_state=42)\n    feature_scaling(X_train, X_test, y_train, y_test, names[scale])","b9b71014":"models_df","802ae0a4":"sns.barplot(y='Modelling Algo', x='Accuracy', data=models_df)","32566e55":"sns.barplot(y='Modelling Algo', x='Acc_Min_Max_Scaler', data=models_df)","ff05a203":"sns.barplot(y='Modelling Algo', x='Acc_Standard_Scaler', data=models_df)","2b0c5ea8":"# preparing the features by using a StandardScaler as it gave better results.\nscaler = StandardScaler()\nX = df.drop('quality', axis=1)\ny = df.quality\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42)","65283a11":"X_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","05417f59":"model, test_accuracy = [], []","0979acf1":"params_dict = {'n_neighbors': [i+1 for i in range(50)], 'n_jobs': [-1]}\nknn_clf = GridSearchCV(estimator=KNeighborsClassifier(),\n                       param_grid=params_dict, scoring='accuracy', cv=10)\n\nknn_clf.fit(X_train, y_train)","91cc8cb2":"knn_clf.best_params_","a586c45f":"# the best accuracy obtained by Grid search on the train set.\nknn_clf.best_score_","dcd70f8f":"pred = knn_clf.predict(X_test)\naccuracy = accuracy_score(y_test, pred)\n\nmodel.append('KNN')\ntest_accuracy.append(accuracy)\nprint(\"KNN Accuracy:\", accuracy)","001bafa0":"params_dict = {'C': [0.98, 1.0, 1.2, 1.5, 2.0, 5.0], 'gamma': [\n    0.50, 0.60, 0.70, 0.80, 0.90, 1.00], 'kernel': ['linear', 'rbf']}\nsvm_clf = GridSearchCV(\n    estimator=SVC(), param_grid=params_dict, scoring='accuracy', cv=10)\n\nsvm_clf.fit(X_train, y_train)","7769f27e":"svm_clf.best_params_","20cfff45":"svm_clf.best_score_","bc8bc3d0":"pred = svm_clf.predict(X_test)\naccuracy = accuracy_score(y_test, pred)\n\nmodel.append('SVM')\ntest_accuracy.append(accuracy)\nprint(\"SVM Accuracy:\", accuracy)","071a5e88":"param_dict = {'criterion': ['gini', 'entropy'], 'max_depth': [\n    4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 20, 30, 40, 50, 70, 90, 120, 150]}\ndt_clf = GridSearchCV(DecisionTreeClassifier(),\n                      param_grid=param_dict, scoring='accuracy', cv=10)\n\ndt_clf.fit(X_train, y_train)","30194664":"dt_clf.best_params_","0f4f10b1":"dt_clf.best_score_","27594394":"pred = dt_clf.predict(X_test)\naccuracy = accuracy_score(y_test, pred)\n\nmodel.append('Decision Tree')\ntest_accuracy.append(accuracy)\nprint(\"Decision Tree Accuracy:\", accuracy)","45e2b7e1":"params_dict = {'n_estimators': [100, 200, 300, 400,\n                                500], 'max_features': ['auto', 'sqrt', 'log2']}\nrf_clf = GridSearchCV(estimator=RandomForestClassifier(\n    n_jobs=-1), param_grid=params_dict, scoring='accuracy', cv=10)\n\nrf_clf.fit(X_train, y_train)","6fd6433e":"rf_clf.best_params_","957b6870":"rf_clf.best_score_","79988a01":"pred = rf_clf.predict(X_test)\naccuracy = accuracy_score(y_test, pred)\n\nmodel.append('Random Forest')\ntest_accuracy.append(accuracy)\nprint(\"Random Forest Accuracy:\", accuracy)","64512eb7":"gb_clf = GridSearchCV(estimator=GradientBoostingClassifier(),\n                      cv=10, param_grid=dict({'n_estimators': [100, 200, 300, 400, 500, 600, 700]}))\n\ngb_clf.fit(X_train, y_train)","2e601a11":"gb_clf.best_params_","825dc510":"gb_clf.best_score_","fc1cafe5":"pred = gb_clf.predict(X_test)\naccuracy = accuracy_score(y_test, pred)\n\nmodel.append('Gradient Boosting')\ntest_accuracy.append(accuracy)\nprint(\"Gradient Boosting Accuracy:\", accuracy)","fb1afd80":"param_dict = {'n_estimators': list(range(1, 201, 20))}\n\nadaboost_clf = GridSearchCV(AdaBoostClassifier(DecisionTreeClassifier(\n    criterion='gini', max_depth=1000),  algorithm='SAMME.R'),\n    param_grid=param_dict)\n\nadaboost_clf.fit(X_train, y_train)","a46b4a5a":"adaboost_clf.best_params_","36bb6335":"adaboost_clf.best_score_","c922a4f3":"pred = adaboost_clf.predict(X_test)\naccuracy = accuracy_score(y_test, pred)\n\nmodel.append('Adaboost')\ntest_accuracy.append(accuracy)\nprint(\"Adaboost Accuracy:\", accuracy)","ec59c8cf":"h_voting_clf = VotingClassifier([\n    ('logistic regression', LogisticRegression(C=0.1, penalty='l2')),\n    ('knn', KNeighborsClassifier(n_jobs=-1, n_neighbors=4)),\n    ('svm', SVC(C=2.0, gamma=0.7, kernel='rbf')),\n    ('random forests', RandomForestClassifier(\n        max_features='auto', n_estimators=300)),\n    ('gradient boosting', GradientBoostingClassifier(n_estimators=600)),\n], voting='hard')","d570a25b":"h_voting_clf.fit(X_train, y_train)","a9b0c405":"h_voting_clf.score(X_train, y_train)","8e9f7581":"pred = h_voting_clf.predict(X_test)\naccuracy = accuracy_score(y_test, pred)\n\nmodel.append('Hard Voting')\ntest_accuracy.append(accuracy)\nprint(\"Hard Voting Accuracy:\", accuracy)","27e9b7a8":"s_voting_clf = VotingClassifier([\n    ('logistic regression', LogisticRegression(C=0.1, penalty='l2')),\n    ('knn', KNeighborsClassifier(n_jobs=-1, n_neighbors=4)),\n    ('svm', SVC(C=2.0, gamma=0.7, kernel='rbf', probability=True)),\n    ('random forests', RandomForestClassifier(\n        max_features='auto', n_estimators=300)),\n    ('gradient boosting', GradientBoostingClassifier(n_estimators=600)),\n], voting='soft')","fffe8bbf":"s_voting_clf.fit(X_train, y_train)","222e683c":"s_voting_clf.score(X_train, y_train)","026baf1f":"pred = s_voting_clf.predict(X_test)\naccuracy = accuracy_score(y_test, pred)\n\nmodel.append('Soft Voting')\ntest_accuracy.append(accuracy)\nprint(\"Soft Voting Accuracy:\", accuracy)","c3c3f811":"final = pd.DataFrame({\n    'Model': model,\n    'Test Accuracy': test_accuracy\n})","d41a2e5a":"final","60a9a418":"# Red Wine Quality","3d7832ed":"## Modelling the data","2df5b998":"**Inderences from the heat map**\n\n* The quality of wine is highly related to volatile acidity (negative correlation).\n* Also the quality of wine is highly corelated to alcohol (positice correlation).\n* pH and citric acid \/ fixed acidity are highly inversely related as all of us know that acids have smaller pH values.\n","a78e4f70":"### Adaboost Classifier","97202278":"### Random Forest","185fb82a":"## Importing the dataset","6fb1ab0b":"### Decision Tree","277054ad":"### Voting Classifier","83975fb4":"## Importing the libraries","970740d9":"#### Soft Voting","780db605":"#### Hard Voting","343404f4":"## Final Evaluation","3553eb45":"## Feature Scaling","55373207":"### SVM","4ccefd9c":"## Correlation between different features","46422a24":"## So finally we select *Hard Voting Classifier*!","3a2ca383":"## Visualize the distribution of variables i.e univariate analysis","47790420":"### Gradient Boosting","5f2b538d":"## How quality varies from different numeric features","c19a3fa7":"## Parameter Tuning and Model Selection","09f12a3b":"### K-Nearest Neighbors (KNN)"}}