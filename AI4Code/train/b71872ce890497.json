{"cell_type":{"b06310b7":"code","ea7f2a64":"code","40994810":"code","f3dd2e60":"code","6765ebc3":"code","a3d91024":"code","21e5abee":"code","92a43250":"code","63fa3c79":"code","7776e67a":"code","192ead0b":"code","0ab2c8ca":"code","9c196801":"code","73a8b6d8":"code","f689b02d":"code","d20661d1":"code","6b318b3f":"code","2ad72004":"code","8c6e96b9":"code","320ed115":"code","83ffbb22":"code","b9f748d9":"code","6635b59b":"code","2d0a586a":"code","b00af0f1":"code","ebbaa2df":"code","37329d35":"code","4101fbb2":"code","66f90141":"code","ea4f5b45":"code","f430ca75":"code","548dc46d":"code","371724cb":"code","dfe64ac8":"code","fc3a35a5":"code","1c976d2b":"code","6442e046":"markdown","88721e8c":"markdown","d5679d99":"markdown","f3671db4":"markdown","b15b2997":"markdown","555bfefa":"markdown","91c6f7ac":"markdown","ba300f4d":"markdown","84ccfb84":"markdown","94b1c821":"markdown","1c4c51dd":"markdown","b58d47c0":"markdown","3254698e":"markdown","85e6fc04":"markdown","3e4c9078":"markdown","cf1dd03b":"markdown","ef0d5ce9":"markdown","cb825abb":"markdown","2aac43fb":"markdown","d223aeaa":"markdown","0959df33":"markdown","e5805ece":"markdown","c6bea1fb":"markdown","56688206":"markdown","c7b1aa47":"markdown"},"source":{"b06310b7":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if filename.endswith('.jpg'):\n            break\n        print(os.path.join(dirname, filename))","ea7f2a64":"df_sample = pd.read_csv('..\/input\/herbarium-2020-fgvc7\/sample_submission.csv')\ndisplay(df_sample)","40994810":"import json\nwith open(\"..\/input\/herbarium-2020-fgvc7\/nybg2020\/train\/metadata.json\", 'r',\n                 encoding='utf-8', errors='ignore') as f:\n    train_meta = json.load(f)\n    \nwith open(\"..\/input\/herbarium-2020-fgvc7\/nybg2020\/test\/metadata.json\", 'r',\n                 encoding='utf-8', errors='ignore') as f:\n    test_meta = json.load(f)","f3dd2e60":"display(train_meta.keys())","6765ebc3":"train_id = pd.DataFrame(train_meta['annotations'])\ndisplay(train_id)","a3d91024":"train_cat = pd.DataFrame(train_meta['categories'])\ntrain_cat.columns = ['family', 'genus', 'category_id', 'category_name']\ndisplay(train_cat)","21e5abee":"train_img = pd.DataFrame(train_meta['images'])\ntrain_img.columns = ['file_name', 'height', 'image_id', 'license', 'width']\ndisplay(train_img)","92a43250":"train_reg = pd.DataFrame(train_meta['regions'])\ntrain_reg.columns = ['region_id', 'region_name']\ndisplay(train_reg)","63fa3c79":"train_df = train_id.merge(train_cat, on='category_id', how='outer')\ntrain_df = train_df.merge(train_img, on='image_id', how='outer')\ntrain_df = train_df.merge(train_reg, on='region_id', how='outer')","7776e67a":"print(train_df.info())\ndisplay(train_df)","192ead0b":"bools_img_path = train_df['file_name'].isna()\nkeep = [x for x in range(train_df.shape[0]) if not bools_img_path[x]]\ntrain_df = train_df.iloc[keep]","0ab2c8ca":"dtypes = ['int32', 'int32', 'int32', 'int32', 'object', 'object', 'object', 'object', 'int32', 'int32', 'int32', 'object']\nfor n, col in enumerate(train_df.columns):\n    train_df[col] = train_df[col].astype(dtypes[n])\nprint(train_df.info())\ndisplay(train_df)","9c196801":"test_meta.keys()","73a8b6d8":"test_df = pd.DataFrame(test_meta['images'])\ntest_df.columns = ['file_name', 'height', 'image_id', 'license', 'width']\nprint(test_df.info())\ndisplay(test_df)","f689b02d":"train_df.to_csv('full_train_data.csv', index=False)\ntest_df.to_csv('full_test_data.csv', index=False)","d20661d1":"import matplotlib.pyplot as plt","6b318b3f":"_train = train_df['category_name'].value_counts()\nprint(len(_train),'\u7a2e\u985e')\nfig = plt.figure(figsize=(14,3))\nax1 = fig.add_subplot(1, 2, 1)\nax1.bar(_train[0:11].index,_train[0:11].values)\nplt.xticks(rotation=90)\nplt.title('top10')\nax2 = fig.add_subplot(1, 2, 2)\nax2.bar(_train[-10:-1].index,_train[-10:-1].values)\nplt.xticks(rotation=90)\nplt.title('worst10')\nfig.savefig(\"category_top10_worst_10.png\",bbox_inches=\"tight\")\nplt.show()","2ad72004":"_train = train_df['family'].value_counts()\nprint(len(_train),'family')\nfig = plt.figure(figsize=(14,3))\nax1 = fig.add_subplot(1, 2, 1)\nax1.bar(_train[0:11].index,_train[0:11].values)\nplt.xticks(rotation=90)\nplt.title('top10')\nax2 = fig.add_subplot(1, 2, 2)\nax2.bar(_train[-10:-1].index,_train[-10:-1].values)\nplt.xticks(rotation=90)\nplt.title('worst10')\nfig.savefig(\"family_top10_worst_10.png\",bbox_inches=\"tight\")\nplt.show()","8c6e96b9":"_train = train_df['genus'].value_counts()\nprint(len(_train),'genus')\nfig = plt.figure(figsize=(14,3))\nax1 = fig.add_subplot(1, 2, 1)\nax1.bar(_train[0:11].index,_train[0:11].values)\nplt.xticks(rotation=90)\nplt.title('top10')\nax2 = fig.add_subplot(1, 2, 2)\nax2.bar(_train[-10:-1].index,_train[-10:-1].values)\nplt.xticks(rotation=90)\nplt.title('worst10')\nfig.savefig(\"genus_top10_worst_10.png\",bbox_inches=\"tight\")\nplt.show()","320ed115":"print(\"Total Unique Values for each columns:\")\nprint(\"{0:10s} \\t {1:10d}\".format('train_df', len(train_df)))\nfor col in train_df.columns:\n    print(\"{0:10s} \\t {1:10d}\".format(col, len(train_df[col].unique())))","83ffbb22":"family = train_df[['family', 'genus', 'category_name']].groupby(['family', 'genus']).count()\ndisplay(family.describe())","b9f748d9":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(featurewise_center=False,\n                                     featurewise_std_normalization=False,\n                                     rotation_range=180,\n                                     width_shift_range=0.1,\n                                     height_shift_range=0.1,\n                                     zoom_range=0.2)","6635b59b":"m = train_df[['file_name', 'family', 'genus', 'category_id']]\nfam = m.family.unique().tolist()\nm.family = m.family.map(lambda x: fam.index(x))\ngen = m.genus.unique().tolist()\nm.genus = m.genus.map(lambda x: gen.index(x))\ndisplay(m)","2d0a586a":"os.chdir('..\/input')","b00af0f1":"from efficientnet.efficientnet.keras import EfficientNetB3 \n#from efficientnet.efficientnet.model import EfficientNetB3\nfrom keras.models import Model\nfrom keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten, BatchNormalization, Input, concatenate\nfrom keras.optimizers import Adam\nfrom keras.utils import plot_model\nfrom sklearn.model_selection import train_test_split as tts","ebbaa2df":"def fg_model(shape,lr):\n    \n    \n    actual_shape = shape\n    i = Input(actual_shape)\n    x = EfficientNetB3(weights='imagenet', include_top=False, input_shape=actual_shape, pooling='max')(i)\n    #x = Flatten()(x)\n    o1 = Dense(310, name=\"family\", activation='softmax')(x)\n    o2 = concatenate([x,o1])\n    o2 = Dense(3678, name=\"genus\", activation='softmax')(o2)\n    o3 = concatenate([x,o2])\n    o3 = Dense(32093, name=\"category_id\", activation='softmax')(o3)\n    model = Model(inputs=i,outputs=[o1,o2,o3])\n    \n    #model.layers[1].trainable = False\n    #model.get_layer('genus').trainable = False\n    \n    opt = Adam(lr=lr, amsgrad=True)\n    model.compile(optimizer=opt, loss=['sparse_categorical_crossentropy', \n                                   'sparse_categorical_crossentropy','sparse_categorical_crossentropy'],\n                 metrics=['accuracy'])\n\n    \n    return model","37329d35":"model = fg_model((300,300,3), 0.01) #Efficientnet B3 was designed for image size 300x300\nmodel.summary()","4101fbb2":"\ntrain, verif = tts(m, test_size=0.2, shuffle=True, random_state=17)\ntrain = train[:40000]\nverif = verif[:10000]\nshape = (120, 120, 3)\nepochs = 1\nbatch_size = 32\n\nmodel = fg_model(shape, 0.007)\nopt = Adam(lr=0.007, amsgrad=True)\n#Disable the last two output layers for training the Family\nmodel.get_layer('genus').trainable = True\nmodel.get_layer('family').trainable = True\nmodel.get_layer('category_id').trainable = True\n\nmodel.compile(optimizer=opt, loss=['sparse_categorical_crossentropy', \n                               'sparse_categorical_crossentropy','sparse_categorical_crossentropy'],\n             metrics=['accuracy'])\n#Train Family for 2 epochs\nmodel.fit_generator(train_datagen.flow_from_dataframe(dataframe=train,\n                                                      directory='..\/input\/herbarium-2020-fgvc7\/nybg2020\/train\/',\n                                                      x_col=\"file_name\",\n                                                      y_col=[\"family\", \"genus\", \"category_id\"],\n                                                      target_size=(120, 120),\n                                                      batch_size=batch_size,\n                                                      class_mode='multi_output'),\n                    validation_data=train_datagen.flow_from_dataframe(\n                        dataframe=verif,\n                        directory='..\/input\/herbarium-2020-fgvc7\/nybg2020\/train\/',\n                        x_col=\"file_name\",\n                        y_col=[\"family\", \"genus\", \"category_id\"],\n                        target_size=(120, 120),\n                        batch_size=batch_size,\n                        class_mode='multi_output'),\n                    epochs=epochs,\n                    steps_per_epoch=len(train)\/\/batch_size,\n                    validation_steps=len(verif)\/\/batch_size,\n                    verbose=1,\n                    workers=8,\n                    use_multiprocessing=True)\n\n#Reshuffle the inputs\ntrain, verif = tts(m, test_size=0.2, shuffle=True, random_state=18)\ntrain = train[:40000]\nverif = verif[:10000]\n\n#Make the Genus layer Trainable\nmodel.get_layer('genus').trainable = True\nmodel.get_layer('family').trainable = False\nmodel.get_layer('category_id').trainable = True\nmodel.compile(optimizer=opt, loss=['sparse_categorical_crossentropy', \n                               'sparse_categorical_crossentropy','sparse_categorical_crossentropy'],\n             metrics=['accuracy'])\n\n#Train Family and Genus for 2 epochs\nmodel.fit_generator(train_datagen.flow_from_dataframe(dataframe=train,\n                                                      directory='..\/input\/herbarium-2020-fgvc7\/nybg2020\/train\/',\n                                                      x_col=\"file_name\",\n                                                      y_col=[\"family\", \"genus\", \"category_id\"],\n                                                      target_size=(120, 120),\n                                                      batch_size=batch_size,\n                                                      class_mode='multi_output'),\n                    validation_data=train_datagen.flow_from_dataframe(\n                        dataframe=verif,\n                        directory='..\/input\/herbarium-2020-fgvc7\/nybg2020\/train\/',\n                        x_col=\"file_name\",\n                        y_col=[\"family\", \"genus\", \"category_id\"],\n                        target_size=(120, 120),\n                        batch_size=batch_size,\n                        class_mode='multi_output'),\n                    epochs=epochs,\n                    steps_per_epoch=len(train)\/\/batch_size,\n                    validation_steps=len(verif)\/\/batch_size,\n                    verbose=1,\n                    workers=8,\n                    use_multiprocessing=True)\n\n#Reshuffle the inputs\ntrain, verif = tts(m, test_size=0.2, shuffle=True, random_state=19)\ntrain = train[:40000]\nverif = verif[:10000]\n\n#Make the category_id layer Trainable\nmodel.get_layer('genus').trainable = True\nmodel.get_layer('family').trainable = True\nmodel.get_layer('category_id').trainable = True\nmodel.compile(optimizer=opt, loss=['sparse_categorical_crossentropy', \n                               'sparse_categorical_crossentropy','sparse_categorical_crossentropy'],\n             metrics=['accuracy'])\n#Train them all for 2 epochs\nmodel.fit_generator(train_datagen.flow_from_dataframe(dataframe=train,\n                                                      directory='..\/input\/herbarium-2020-fgvc7\/nybg2020\/train\/',\n                                                      x_col=\"file_name\",\n                                                      y_col=[\"family\", \"genus\", \"category_id\"],\n                                                      target_size=(120, 120),\n                                                      batch_size=batch_size,\n                                                      class_mode='multi_output'),\n                    validation_data=train_datagen.flow_from_dataframe(\n                        dataframe=verif,\n                        directory='..\/input\/herbarium-2020-fgvc7\/nybg2020\/train\/',\n                        x_col=\"file_name\",\n                        y_col=[\"family\", \"genus\", \"category_id\"],\n                        target_size=(120, 120),\n                        batch_size=batch_size,\n                        class_mode='multi_output'),\n                    epochs=epochs,\n                    steps_per_epoch=len(train)\/\/batch_size,\n                    validation_steps=len(verif)\/\/batch_size,\n                    verbose=1,\n                    workers=8,\n                    use_multiprocessing=True)","66f90141":"model.save('..\/working\/fg_model.h5')","ea4f5b45":"model.history.history","f430ca75":"del train, verif, m, train_df, fam, gen, _train\nbatch_size = 32\n\n# generator = test_datagen.flow_from_dataframe(\n#         dataframe = test_df,#.iloc[:10000], \n#         directory = '..\/input\/herbarium-2020-fgvc7\/nybg2020\/test\/',\n#         x_col = 'file_name',\n#         target_size=(120, 120),\n#         batch_size=batch_size,\n#         class_mode=None,  # only data, no labels\n#         shuffle=False)\n\n# family, genus, category = model.predict_generator(generator, verbose=1)","548dc46d":"\ncategories = []\nfor i in range(1,len(test_df)):\n    if i % 10000 == 0:\n        test_datagen = ImageDataGenerator(featurewise_center=False,\n                                  featurewise_std_normalization=False)\n        generator = test_datagen.flow_from_dataframe(\n            dataframe = test_df.iloc[i-10000:i], \n            directory = '..\/input\/herbarium-2020-fgvc7\/nybg2020\/test\/',\n            x_col = 'file_name',\n            target_size=(120, 120),\n            batch_size=batch_size,\n            class_mode=None,  # only data, no labels\n            shuffle=False)\n\n        family, genus, category = model.predict_generator(generator, verbose=1,max_queue_size=10)\n        categories.append(np.argmax(category, axis=1))\n        last = i # \u6700\u5f8c\u306eindex\u3092\u4fdd\u5b58\n        del test_datagen, generator, family,genus\n    elif i == (len(test_df)-1):\n        test_datagen = ImageDataGenerator(featurewise_center=False,\n                                  featurewise_std_normalization=False)        \n        generator = test_datagen.flow_from_dataframe(\n            dataframe = test_df.iloc[last:i+1], \n            directory = '..\/input\/herbarium-2020-fgvc7\/nybg2020\/test\/',\n            x_col = 'file_name',\n            target_size=(120, 120),\n            batch_size=batch_size,\n            class_mode=None,  # only data, no labels\n            shuffle=False)\n\n        family, genus, category = model.predict_generator(generator, verbose=1,max_queue_size=10)\n        categories.append(np.argmax(category, axis=1))\n        del test_datagen, generator,family,genus\n#categories = np.concatenate(categories,axis=0)","371724cb":"np.concatenate(categories[0:2])","dfe64ac8":"sub = pd.DataFrame()\nsub['Id'] = test_df.image_id\nsub['Id'] = sub['Id'].astype('int32')\nsub['Predicted'] = np.concatenate(categories)\nsub['Predicted'] = sub['Predicted'].astype('int32')\ndisplay(sub)\nsub.to_csv('..\/working\/submission.csv', index=False)","fc3a35a5":"# end_time = time.time()\n# total = end_time - start_time\n# h = total\/\/3600\n# m = (total%3600)\/\/60\n# s = total%60\n# print(\"Total time spent: %i hours, %i minutes, and %i seconds\" %(h, m, s))","1c976d2b":"# in_out_size = (120*120) + 3 #We will resize the image to 120*120 and we have 3 outputs\n# def xavier(shape, dtype=None):\n#     return np.random.rand(*shape)*np.sqrt(1\/in_out_size)\n\n# def fg_model(shape, lr=0.001):\n#     '''Family-Genus model receives an image and outputs two integers indicating both the family and genus index.'''\n#     i = Input(shape)\n    \n#     x = Conv2D(3, (3, 3), activation='relu', padding='same', kernel_initializer=xavier)(i)\n#     x = Conv2D(3, (5, 5), activation='relu', padding='same', kernel_initializer=xavier)(x)\n#     x = MaxPool2D(pool_size=(3, 3), strides=(3,3))(x)\n#     x = BatchNormalization()(x)\n#     x = Dropout(0.5)(x)\n#     x = Conv2D(16, (5, 5), activation='relu', padding='same', kernel_initializer=xavier)(x)\n#     #x = Conv2D(16, (5, 5), activation='relu', padding='same', kernel_initializer=xavier)(x)\n#     x = MaxPool2D(pool_size=(5, 5), strides=(5,5))(x)\n#     x = BatchNormalization()(x)\n#     x = Dropout(0.5)(x)\n#     x = Flatten()(x)\n    \n#     o1 = Dense(310, activation='softmax', name='family', kernel_initializer=xavier)(x)\n    \n#     o2 = concatenate([o1, x])\n#     o2 = Dense(3678, activation='softmax', name='genus', kernel_initializer=xavier)(o2)\n    \n#     o3 = concatenate([o1, o2, x])\n#     o3 = Dense(32094, activation='softmax', name='category_id', kernel_initializer=xavier)(o3)\n    \n#     x = Model(inputs=i, outputs=[o1, o2, o3])\n    \n#     opt = Adam(lr=lr, amsgrad=True)\n#     x.compile(optimizer=opt, loss=['sparse_categorical_crossentropy', \n#                                    'sparse_categorical_crossentropy', \n#                                    'sparse_categorical_crossentropy'],\n#                  metrics=['accuracy'])\n#     return x\n\n# model = fg_model((120, 120, 3))\n# model.summary()\n# plot_model(model, to_file='full_model_plot.png', show_shapes=True, show_layer_names=True)","6442e046":"# Submission\n\nNext, we'll save the predicted values under `predictions` into the specified format for submissions. Remember that our `predictions` is a `list` of 3-outputs, namely: `family`, `genus`, `category_id` in that order.","88721e8c":"# Predict\n\nNow, we will do our prediction. We may as well skip doing a confusion-matrix for our model because it's not even fully trained, so we go straight to our submission.\n\nSimilar to the above reason, we will be limiting the `predictions` to the first `10,000` items due to RAM limitations.","d5679d99":"# Data Generator\ndata generator\u3092\u4f5c\u308b\u3002\n\u524d\u51e6\u7406\u7b49\u3057\u3066\u304a\u304f\u306b\u306f\u30c7\u30fc\u30bf\u306e\u5bb9\u91cf\u304c\u5927\u304d\u3059\u304e\u308b\u3002\u30d0\u30c3\u30c1\u3054\u3068\u306b\u51e6\u7406\u3092\u3059\u308b\u306e\u3067\u3001dataGenerator\u306b\u4efb\u305b\u308b\u3053\u3068\u3068\u3059\u308b\u3002\n\n\u51e6\u7406\u6642\u9593\u53c2\u8003\nhttps:\/\/hironsan.hatenablog.com\/entry\/2017\/09\/09\/130608","f3671db4":"Followed by the `image properties`:","b15b2997":"After selecting the `non-NaN` items, we now reiterate on their file types. We need to save on memory, as we reached `102+ MB` for this DataFrame Only.","555bfefa":"# Model Creation\n\u4e00\u5ea6\u306bcategory\u3092\u5f53\u3066\u306b\u884c\u304f\u3053\u3068\u3082\u3067\u304d\u308b\u304c\u3001\u4e8b\u524d\u60c5\u5831\u3068\u3057\u3066\u4e0e\u3048\u3089\u308c\u3066\u3044\u308bfamily\u3084genus\u306e\u60c5\u5831\u3092\u751f\u304b\u3057\u3066\u3001\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3059\u308b\u30a2\u30d7\u30ed\u30fc\u30c1\u3092\u3068\u308b\u3002(\u305d\u306e\u307e\u307e\u4e88\u6e2c\u3059\u308b\u624b\u6cd5\u306f\u307b\u304b\u30e1\u30f3\u30d0\u30fc\u304c\u5b9f\u65bd\u3002)\n\nfamily\u3084genus\u3092category\u306e\u4e88\u6e2c\u306b\u751f\u304b\u3059\u6642\u3001family\u3084genus\u3092\u4e88\u6e2c\u3059\u308b\u5206\u985e\u5668\u3092\u4f5c\u3063\u3066\u3001\u7279\u5b9a\u306e\u5c64\u306e\u91cd\u307f\u3092\u5b66\u7fd2\u3055\u305b\u306a\u3044\u3053\u3068\u304c\u3067\u304d\u308b\u3002tensorFlow\u3067\u3042\u308c\u3070\u3001\u30ec\u30a4\u30e4\u30fc\u306btrainable\u5c5e\u6027\u304c\u5b58\u5728\u3057\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5b66\u7fd2\u3055\u305b\u305f\u304f\u306a\u3044\u5c64\u3084\u30e2\u30c7\u30eb\u306b\u3064\u3044\u3066\u3001trainable\u5c5e\u6027\u3092False\u3068\u3059\u308b\u3053\u3068\u3067\u3001\u5b66\u7fd2\u3092\u304b\u3051\u306a\u3044\u3067\u304a\u304f\u3053\u3068\u304c\u3067\u304d\u308b\u3002(\u305f\u3060\u3057\u5b66\u7fd2\u524d\u306e\u6700\u5f8c\u306bcompile()\u3092\u5b9f\u884c\u3057\u306a\u3044\u3068\u5c5e\u6027\u306e\u5909\u66f4\u304c\u7e41\u6804\u3055\u308c\u306a\u3044\u306e\u3067\u6ce8\u610f)\n\n\u53c2\u8003\u306eURL\u3067\u306f\n> summary()\u306e\u51fa\u529b\u7d50\u679c\u306b\u306f\u53cd\u6620\u3055\u308c\u3066\u3044\u308b\u304c\u3001\u5b9f\u969b\u306b\u8a2d\u5b9a\u3092\u6709\u52b9\u306b\u3059\u308b\u306b\u306fcompile()\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u306e\u3067\u6ce8\u610f\u3002compile()\u306e\u3042\u3068\u3067trainable\u3092\u5909\u66f4\u3057\u305f\u5834\u5408\u3001\u518d\u5ea6compile()\u3057\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\u3002\n\n\u3068\u3057\u3066\u3044\u308b\u305f\u3081\u3001\u3053\u306enotebook\u3067\u306ftrainable\u5c5e\u6027\u306e\u5909\u66f4\u3092\u3057\u3066\u3044\u306a\u3044\u307e\u307e\u5b66\u7fd2\u3092\u304b\u3051\u3066\u3044\u308b\u3053\u3068\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u306f\u306a\u3044\u304b\uff1f\n\n\n\u53c2\u8003\nhttps:\/\/note.nkmk.me\/python-tensorflow-keras-trainable-freeze-unfreeze\/\n","91c6f7ac":"Looking closer, there's a line with `NaN` values there. We need to remove rows with `NaN`s so we proceed to the next line:","ba300f4d":"Now, we will be unifying the metadata from the `*.json` files. We will first work with the `train` data.","84ccfb84":"# Train","94b1c821":"Next is for `plant categories`:","1c4c51dd":"# Train\n\nNow, we will begin the training.","b58d47c0":"## CNN\u306e\u69cb\u9020\u306e\u6c7a\u3081\u65b9\u306b\u3064\u3044\u3066\n\u8abf\u3079\u3066\u3082\u3053\u3046\u3044\u3046\u6642\u306f\u3053\u3046\u3059\u308b\u307f\u305f\u3044\u306a\u5404\u4e8b\u4f8b\u3054\u3068\u306e\u5177\u4f53\u4f8b\u304c\u51fa\u3066\u304f\u308b\u306e\u307f\u3002\u307e\u305f\u6700\u8fd1\u306f\u300c\u3053\u308c\u304c\u6d41\u884c\u308a\u300d\u306e\u3088\u3046\u306a\u30e2\u30c7\u30eb\u69cb\u9020\u306e\u9686\u76db\u307e\u3067\u3042\u308b\u3063\u307d\u3044\u3002\u7d50\u5c40\u3069\u3046\u3059\u308a\u3083\u3044\u3044\u304b\u308f\u304b\u3089\u306a\u3044\u304b\u3089\u3001Efficientnet\u306b\u4efb\u305b\u3066\u3057\u307e\u3046\u3053\u3068\u306b\u3057\u305f\u3002(AutoML\u306e\u8003\u3048\u65b9\u304b\u3089)\n\n\n\u53c2\u8003\nhttps:\/\/qiita.com\/icoxfog417\/items\/5fd55fad152231d706c2","3254698e":"Now, we will transform the `family` and `genus` to ids.","85e6fc04":"With some proper `image_data_augmentation` we can make up for the small number of samples for some images (first quartile).","3e4c9078":"# overview\nSee input data and output format ","cf1dd03b":"Then, we will merge all the DataFrames and see what we got:","ef0d5ce9":"And lastly, the `region`:","cb825abb":"# Data Exploration\n\nWe will now start the data exploration and see what we can do with this dataset.","2aac43fb":"Finally, for our `test` dataset. Since it only contains one key, `images`:","d223aeaa":"Load json files","0959df33":"# Finish\n\nThere you have it! A working model for predicting the `Category` of the plants. I hope that this kernel helped you on your journey in unraveling the mysteries of this dataset! Please upvote before forking___________3-(^_^ )","e5805ece":"First, we access the `annotations` list and convert it to a df.","c6bea1fb":"Here, we can see that other than the `category_id`, there's also the `family`, `genus`, `category_name`, `region_id` and `region_name` for the other probable targets. `category_id` and `category_name` are one and the same, similar to `region_id` and `region_name`.\n\nA possible approach for this kernel is to use a `CNN` to predict `family` and `genus` (we will ignore `region` for now). Then, using the `family` and `genus`, we will predict the `category_id` for the image.","56688206":"Perfect!\n\nNow, we can go ahead and save this dataframe as a `*.csv` file for future use!","c7b1aa47":"Since there's a lot of images included there, we only checked non-image files and got the three above. Next, we will load the sample submission and check."}}