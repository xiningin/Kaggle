{"cell_type":{"620d4531":"code","bb16fdb8":"code","f7dc98ad":"code","53e71082":"code","e6498b16":"code","20390461":"code","0a20ce17":"code","c981d7d6":"code","ef7d8952":"code","0cb0dde4":"code","04d9143c":"code","802860fb":"code","3ff8a1a7":"code","153e9760":"code","94199217":"code","aeea146b":"code","894c4f46":"code","859cf5e0":"code","1ac37533":"code","94cb2266":"code","f8a85f44":"code","9d49fd71":"code","120c73f7":"code","e646ec5c":"code","39ae6aed":"code","ca65b85d":"code","0c0ceb97":"code","06a70e3c":"code","58d1a310":"code","36e35d07":"code","c7af7918":"code","5dc5bce9":"code","047c0a2a":"code","99270ea1":"code","023e0b5b":"code","d1cd363a":"code","97307d16":"code","b585c764":"code","53dfb3c5":"code","dcb2739f":"code","b2e19a01":"code","823a156f":"code","ae6083e7":"code","5001e9f4":"code","947faf78":"code","fe66cceb":"code","46e3a880":"code","7678b977":"code","e510a39a":"code","43af353b":"code","2290f4fc":"code","70b09b0a":"code","98b399fe":"code","a7e479c7":"code","2e9f710c":"code","43981e72":"code","f640edc8":"code","980c0593":"code","238365d6":"code","1a148f9b":"code","254b94ef":"code","ed95e047":"code","6954c2f7":"code","6559460f":"code","79aad4b1":"code","7c4e34e6":"code","62743489":"code","bf9fc563":"code","29742850":"code","5dbb6a61":"code","f3784399":"code","e45913ca":"code","8af65eb4":"code","cebcaf1d":"code","f85e5565":"code","6b88112b":"code","b102c0c1":"code","e383c3a4":"code","bd8f2490":"code","5414eb38":"code","9a6c1a53":"code","e6288d85":"code","286db17b":"code","4371b9dc":"markdown","5011b576":"markdown","c2448e44":"markdown","748dd285":"markdown","5be31779":"markdown","6dca0254":"markdown","70965493":"markdown","6d7512ab":"markdown","e8e992e1":"markdown","70d21891":"markdown","4049d0de":"markdown","b2bb5ab9":"markdown","e98f7702":"markdown","d60b4d0e":"markdown","5d8447c1":"markdown","47ee9227":"markdown","246748bb":"markdown","11879449":"markdown","af1e3880":"markdown","2c00268d":"markdown","0891544f":"markdown","97d47858":"markdown","a2318d68":"markdown","017f3bdb":"markdown","2d1a46ba":"markdown","f955b94c":"markdown","ba51d52c":"markdown","c27f9a73":"markdown","f6fbf5f7":"markdown","b32d2262":"markdown","502dba72":"markdown","258958aa":"markdown","694f32bb":"markdown","e8dea3ce":"markdown","303b7ab6":"markdown","fcbad8d3":"markdown","7a9ab42f":"markdown"},"source":{"620d4531":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport sys\nimport numpy as np\nimport random as rn\nimport pandas as pd\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bb16fdb8":"import pandas as pd\n\nreview_df=pd.read_csv(\"..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\").iloc[:10000, :]\n\nreview_df.head(10)","f7dc98ad":"print(review_df['sentiment'].value_counts())","53e71082":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(6,5))\nsns.set(style = \"darkgrid\" , font_scale = 1.2)\nsns.countplot(review_df.sentiment)","e6498b16":"positive_values = review_df[(review_df.review .notnull()) & (review_df.sentiment == \"positive\")]\n\npositive_values.head(15)","20390461":"from wordcloud import WordCloud,STOPWORDS \nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nwordcloud = WordCloud(width=500,height=250, max_font_size=80, max_words=150, background_color=\"white\").generate(positive_values.review[0])\nf = plt.figure() \nf.set_figwidth(15) \nf.set_figheight(10) \nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.margins(x=0, y=0)\nplt.show()\n","0a20ce17":"negative_values=review_df[(review_df.review .notnull()) & (review_df.sentiment == \"negative\")]\n\nnegative_values.head(15)","c981d7d6":"wordcloud = WordCloud(width=500,height=250, max_font_size=80, max_words=150, background_color=\"white\").generate(negative_values.review[11])\n\n\nf = plt.figure() \nf.set_figwidth(15) \nf.set_figheight(10) \n\n \nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.margins(x=0, y=0)\n \nplt.show()\n","ef7d8952":"from sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\nreview_df['sentiment'] = labelencoder.fit_transform(review_df['sentiment'])\n\nreview_df['sentiment'].head(10)\n","0cb0dde4":"%%time\n\nimport re\nimport string\npattern = re.compile(r'<br\\s*\/><br\\s*\/>>*|(\\-)|(\\\\)|(\\\/)')\ndef preprocess_reviews(reviews):\n    reviews = [pattern.sub(\" \",item) for item in reviews]\n    return reviews\nclean = preprocess_reviews(review_df['review'])\n\nreview_df['review'] = clean\n\ndef remove_punctuation(input):\n    table = str.maketrans('','',string.punctuation)\n    return input.translate(table)\nreview_df['review'] = review_df['review'].apply(remove_punctuation)\n\nreview_df['review'].head(15)","04d9143c":"review_df['review'] = review_df['review'].str.lower()","802860fb":"review_df.head(15)","3ff8a1a7":"%%time\nreview_df['review']=review_df['review'].apply(str)\ndef remove_linebreaks(input):\n    text = re.compile(r'\\n')\n    return text.sub(r' ',input)\nreview_df['review'] = review_df['review'].apply(remove_linebreaks)\n","153e9760":"%%time\nfrom nltk.tokenize import word_tokenize\nreview_df['review'] = review_df['review'].apply(word_tokenize)\n","94199217":"review_df.head(15)","aeea146b":"%%time\n\nfrom nltk.corpus import stopwords\ndef remove_stopwords(input1):\n    words = []\n    for word in input1:\n            if word not in stopwords.words('english'):\n             words.append(word)\n    return words\nreview_df['review'] = review_df['review'].apply(remove_stopwords)\n\n\nreview_df['review'].head(10)","894c4f46":"%%time\nfrom nltk.stem import WordNetLemmatizer\nlem = WordNetLemmatizer()\ndef lemma_wordnet(input):\n    return [lem.lemmatize(w) for w in input]\nreview_df['review'] = review_df['review'].apply(lemma_wordnet)","859cf5e0":"review_df.head(15)","1ac37533":"%%time\ndef combine_text(input):\n    combined = '  '.join(input)\n    return combined\nreview_df['review'] = review_df['review'].apply(combine_text)","94cb2266":"review_df['review'].head(15)","f8a85f44":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn-darkgrid')\nplt.figure(figsize=(10,5))\nsns.distplot(review_df['review'].apply(len),kde=False,color='red',hist=True)\nplt.xlabel(\"Rivew Length\",size=15)\nplt.ylabel(\"Frequency\",size=15)\nplt.title(\"Length Histogram\",size=15)","9d49fd71":"X=review_df.iloc[:,0:1].values\ny=review_df.iloc[:,-1].values\n","120c73f7":"# from sklearn.feature_extraction.text import CountVectorizer\n# cv=CountVectorizer(max_features=1000)\n# X=cv.fit_transform(review_df['review']).toarray()","e646ec5c":"# import sklearn\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1,2))\n# X = tfidf.fit_transform(review_df.review).toarray()","39ae6aed":"from sklearn.feature_extraction.text import HashingVectorizer\nhv = HashingVectorizer(n_features=1000)\nX=hv.fit_transform(review_df['review']).toarray()","ca65b85d":"from sklearn.model_selection import train_test_split\n\nX_train, X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","0c0ceb97":"from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.ensemble import RandomForestClassifier as RFC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\n\n#Gaussian Naive Bayes model\nclf1=GaussianNB()\n\n#Bernoulli model\nclf2=BernoulliNB()\n\n#Ridge Classifier model\nclf3=RidgeClassifier()\n\n#Random Forest Classifier model \nclf4 = RFC(n_jobs = 2, random_state = 0)\n\n#Support Vector Machine Classifier model\nclf5 = SVC(kernel='linear')","06a70e3c":"clf1.fit(X_train,y_train)\nclf2.fit(X_train,y_train)\nclf3.fit(X_train,y_train)\nclf4.fit(X_train,y_train)\nclf5.fit(X_train,y_train)","58d1a310":"y_pred1=clf1.predict(X_test)\ny_pred2=clf2.predict(X_test)\ny_pred3=clf3.predict(X_test)\ny_pred4=clf4.predict(X_test)\ny_pred5=clf5.predict(X_test)\n\nacc1=accuracy_score(y_test,y_pred1)\nacc2=accuracy_score(y_test,y_pred2)\nacc3=accuracy_score(y_test,y_pred3)\nacc4=accuracy_score(y_test,y_pred4)\nacc5=accuracy_score(y_test,y_pred5)","36e35d07":"#Get acuracy score results for trained models :\nprint(\"Gaussian\",acc1,\"%\")\nprint(\"Bernaulli\",acc2,\"%\")\nprint(\"RidgeClassifier\",acc3,\"%\")\nprint(\"Random Forest\",acc4,\"%\")\nprint(\"Support Vector Machine\",acc5,\"%\")\n","c7af7918":"import matplotlib.pyplot as plt\nfig = plt.figure(figsize=(7,5))\nax = fig.add_axes([0,0,1,1])\nmodels = ['Gaussian ', 'Bernaulli ', 'Ridge ', 'Random Forest ', 'SVM']\naccurisy = [acc1*100 ,acc2*100,acc3*100,acc4*100,acc5*100]\nax.bar(models,accurisy,color = 'bgmrc',width = 0.8)\nplt.xlabel(\"ML model\",size=15)\nplt.ylabel(\"Accuracy\",size=15)\n \nplt.show()","5dc5bce9":"from sklearn.metrics import classification_report,confusion_matrix ","047c0a2a":"from sklearn.model_selection import TimeSeriesSplit\nfrom yellowbrick.datasets import load_occupancy\nfrom yellowbrick.classifier import classification_report\nclas = ['Bad Reviews','Good Reviews']\nvisualizer = classification_report(\n    clf1, X_train, y_train, X_test, y_test, classes=clas, support=True\n)","99270ea1":"Co_Mat = confusion_matrix(y_test,y_pred1)\nprint(Co_Mat)","023e0b5b":"import seaborn as sns\nimport matplotlib.pyplot as plt\ncm = confusion_matrix(y_test,y_pred1)\nplt.figure(figsize = (5,5))\nsns.heatmap(cm,cmap= \"YlOrRd\", \n            linecolor = 'black', \n            linewidth = 1, \n            annot = True, \n            fmt='', \n            xticklabels = ['Bad Reviews','Good Reviews'], \n            yticklabels = ['Bad Reviews','Good Reviews'])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")","d1cd363a":"from sklearn.model_selection import TimeSeriesSplit\nfrom yellowbrick.datasets import load_occupancy\nfrom yellowbrick.classifier import classification_report\nclas = ['Bad Reviews','Good Reviews']\nvisualizer = classification_report(\n    clf2, X_train, y_train, X_test, y_test, classes=clas, support=True\n)","97307d16":"Co_Mat = confusion_matrix(y_test,y_pred2)\nprint(Co_Mat)","b585c764":"import seaborn as sns\nimport matplotlib.pyplot as plt\ncm = confusion_matrix(y_test,y_pred2)\nplt.figure(figsize = (5,5))\nsns.heatmap(cm,cmap= \"YlOrRd\", \n            linecolor = 'black', \n            linewidth = 1, \n            annot = True, \n            fmt='', \n            xticklabels = ['Bad Reviews','Good Reviews'], \n            yticklabels = ['Bad Reviews','Good Reviews'])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")","53dfb3c5":"from sklearn.model_selection import TimeSeriesSplit\nfrom yellowbrick.datasets import load_occupancy\nfrom yellowbrick.classifier import classification_report\nclas = ['Bad Reviews','Good Reviews']\nvisualizer = classification_report(\n    clf3, X_train, y_train, X_test, y_test, classes=clas, support=True\n)\n","dcb2739f":"Co_Mat = confusion_matrix(y_test,y_pred3)\nprint(Co_Mat)","b2e19a01":"import seaborn as sns\nimport matplotlib.pyplot as plt\ncm = confusion_matrix(y_test,y_pred3)\nplt.figure(figsize = (5,5))\nsns.heatmap(cm,cmap= \"YlOrRd\", \n            linecolor = 'black', \n            linewidth = 1, \n            annot = True, \n            fmt='', \n            xticklabels = ['Bad Reviews','Good Reviews'], \n            yticklabels = ['Bad Reviews','Good Reviews'])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")","823a156f":"# print(\"Rabdom Forest  classification report is :\\n\")\n# print(classification_report(y_test,y_pred4, target_names = ['Bad Reviews','Good Reviews']))\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom yellowbrick.datasets import load_occupancy\nfrom yellowbrick.classifier import classification_report\nclas = ['Bad Reviews','Good Reviews']\nvisualizer = classification_report(\n    clf4, X_train, y_train, X_test, y_test, classes=clas, support=True\n)","ae6083e7":"Co_Mat = confusion_matrix(y_test,y_pred4)\nprint(Co_Mat)","5001e9f4":"import seaborn as sns\nimport matplotlib.pyplot as plt\ncm = confusion_matrix(y_test,y_pred4)\nplt.figure(figsize = (5,5))\nsns.heatmap(cm,cmap= \"YlOrRd\", \n            linecolor = 'black', \n            linewidth = 1, \n            annot = True, \n            fmt='', \n            xticklabels = ['Bad Reviews','Good Reviews'], \n            yticklabels = ['Bad Reviews','Good Reviews'])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")","947faf78":"# print(\"SVM  classification report is :\\n\")\n# print(classification_report(y_test,y_pred5, target_names = ['Bad Reviews','Good Reviews']))\n\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom yellowbrick.datasets import load_occupancy\nfrom yellowbrick.classifier import classification_report\nclas = ['Bad Reviews','Good Reviews']\nvisualizer = classification_report(\n    clf5, X_train, y_train, X_test, y_test, classes=clas, support=True\n)","fe66cceb":"Co_Mat = confusion_matrix(y_test,y_pred5)\nprint(Co_Mat)","46e3a880":"import seaborn as sns\nimport matplotlib.pyplot as plt\ncm = confusion_matrix(y_test,y_pred5)\nplt.figure(figsize = (5,5))\nsns.heatmap(cm,cmap= \"YlOrRd\", \n            linecolor = 'black', \n            linewidth = 1, \n            annot = True, \n            fmt='', \n            xticklabels = ['Bad Reviews','Good Reviews'], \n            yticklabels = ['Bad Reviews','Good Reviews'])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")","7678b977":"pip install pytorch_pretrained_bert","e510a39a":"import torch\nfrom pytorch_pretrained_bert import BertModel\nfrom torch import nn\n# from torchnlp.datasets import imdb_dataset      # --> We are using our own uploaded dataset.\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom torch.optim import Adam\nfrom torch.nn.utils import clip_grad_norm_\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\n%matplotlib inline","43af353b":"rn.seed(321)\nnp.random.seed(321)\ntorch.manual_seed(321)\ntorch.cuda.manual_seed(321)","2290f4fc":"path = '..\/input\/imdb-50k-movie-reviews-test-your-bert\/'\n\n# experimenting here with a sample of dataset, to avoid memory overflow.\n\ntrain_data = pd.read_csv(path + 'train.csv')\ntest_data = pd.read_csv(path + 'test.csv')\n\n\n\n\n# experimenting here with a sample of dataset, to avoid memory overflow.\ntrain_data = train_data[:7500]\ntest_data = test_data[:2500]\n\n\n# Convert the DataFrame to a dictionary , like [{'col1': 1.0, 'col2': 0.5}, {'col1': 2.0, 'col2': 0.75}]\ntrain_data = train_data.to_dict(orient='records')\ntest_data = test_data.to_dict(orient='records')","70b09b0a":"train_texts, train_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), train_data)))\ntest_texts, test_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), test_data)))\n\nlen(train_texts), len(train_labels), len(test_texts), len(test_labels)","98b399fe":"train_texts[0]","a7e479c7":"# pytorch_pretained_bert already available in kaggle conda env.\n!pip install pytorch-nlp","2e9f710c":"from pytorch_pretrained_bert import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","43981e72":"tokenizer.tokenize('Here is nashaat jouda welcome ')","f640edc8":"train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], train_texts))\ntest_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], test_texts))\n\nlen(train_tokens), len(test_tokens)","980c0593":"train_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, train_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\ntest_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, test_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n\ntrain_tokens_ids.shape, test_tokens_ids.shape","238365d6":"train_y = np.array(train_labels) == 'pos'\ntest_y = np.array(test_labels) == 'pos'\ntrain_y.shape, test_y.shape, np.mean(train_y), np.mean(test_y)","1a148f9b":"train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\ntest_masks =  [[float(i > 0) for i in ii] for ii in test_tokens_ids]","254b94ef":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import classification_report","ed95e047":"# baseline_model = make_pipeline(CountVectorizer(ngram_range=(1,3)), LogisticRegression()).fit(train_texts, train_labels)","6954c2f7":"# baseline_predicted = baseline_model.predict(test_texts)","6559460f":"# print(classification_report(test_labels, baseline_predicted))","79aad4b1":"class BertBinaryClassifier(nn.Module):\n    def __init__(self, dropout=0.1):\n        super(BertBinaryClassifier, self).__init__()\n\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n\n        self.dropout = nn.Dropout(dropout)\n        self.linear = nn.Linear(768, 1)\n        self.sigmoid = nn.Sigmoid()\n    \n    def forward(self, tokens, masks=None):\n        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n        dropout_output = self.dropout(pooled_output)\n        linear_output = self.linear(dropout_output)\n        proba = self.sigmoid(linear_output)\n        return proba","7c4e34e6":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","62743489":"str(torch.cuda.memory_allocated(device)\/1000000 ) + 'M'","bf9fc563":"bert_clf = BertBinaryClassifier()\nbert_clf = bert_clf.cuda()     # running BERT on CUDA_GPU","29742850":"str(torch.cuda.memory_allocated(device)\/1000000 ) + 'M'","5dbb6a61":"x = torch.tensor(train_tokens_ids[:3]).to(device)\ny, pooled = bert_clf.bert(x, output_all_encoded_layers=False)\nx.shape, y.shape, pooled.shape","f3784399":"y = bert_clf(x)\ny.cpu().detach().numpy()        # kinda Garbage Collector to free up used and cache space","e45913ca":"# Cross- checking CUDA GPU Memory to ensure GPU memory is not overflowing.\nstr(torch.cuda.memory_allocated(device)\/1000000 ) + 'M'","8af65eb4":"y, x, pooled = None, None, None\ntorch.cuda.empty_cache()     # Clearing Cache space for fresh Model run\nstr(torch.cuda.memory_allocated(device)\/1000000 ) + 'M'","cebcaf1d":"# Setting hyper-parameters\n\nBATCH_SIZE = 4\nEPOCHS = 3","f85e5565":"train_tokens_tensor = torch.tensor(train_tokens_ids)\ntrain_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n\ntest_tokens_tensor = torch.tensor(test_tokens_ids)\ntest_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n\ntrain_masks_tensor = torch.tensor(train_masks)\ntest_masks_tensor = torch.tensor(test_masks)\n\nstr(torch.cuda.memory_allocated(device)\/1000000 ) + 'M'","6b88112b":"train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\ntrain_sampler = RandomSampler(train_dataset)\ntrain_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n\ntest_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\ntest_sampler = SequentialSampler(test_dataset)\ntest_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)","b102c0c1":"param_optimizer = list(bert_clf.sigmoid.named_parameters()) \noptimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]","e383c3a4":"optimizer = Adam(bert_clf.parameters(), lr=3e-6)","bd8f2490":"torch.cuda.empty_cache()   # Clearing Cache space for a fresh Model run","5414eb38":"for epoch_num in range(EPOCHS):\n    bert_clf.train()\n    train_loss = 0\n    for step_num, batch_data in enumerate(train_dataloader):\n        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n        print(str(torch.cuda.memory_allocated(device)\/1000000 ) + 'M')\n        logits = bert_clf(token_ids, masks)\n        \n        loss_func = nn.BCELoss()\n\n        batch_loss = loss_func(logits, labels)\n        train_loss += batch_loss.item()\n        \n        \n        bert_clf.zero_grad()\n        batch_loss.backward()\n        \n\n        clip_grad_norm_(parameters=bert_clf.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        clear_output(wait=True)\n        print('Epoch: ', epoch_num + 1)\n        print(\"\\r\" + \"{0}\/{1} loss: {2} \".format(step_num, len(train_data) \/ BATCH_SIZE, train_loss \/ (step_num + 1)))","9a6c1a53":"bert_clf.eval()\nbert_predicted = []\nall_logits = []\nwith torch.no_grad():\n    for step_num, batch_data in enumerate(test_dataloader):\n\n        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n\n        logits = bert_clf(token_ids, masks)\n        loss_func = nn.BCELoss()\n        loss = loss_func(logits, labels)\n        numpy_logits = logits.cpu().detach().numpy()\n        \n        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n        all_logits += list(numpy_logits[:, 0])","e6288d85":"np.mean(bert_predicted)","286db17b":"print(classification_report(test_y, bert_predicted))","4371b9dc":"ploting wordcloud for negative values :","5011b576":"# BERT Model","c2448e44":"A. Bag of words ","748dd285":"ploting bar plot flowshart (comparing number of positive and negative records) :","5be31779":"determine X:review , y:seniment (label and target feature of dataset)to use it later in ML models : ","6dca0254":"4. Remove line breaks","70965493":"visualizing one of the sentences from train set","6d7512ab":"3. Convert all text to lowercase","e8e992e1":"6. Remove stopword","70d21891":"Evaluating our trined machine learning models :","4049d0de":"# Analyzing machine learningModels( classification models )","b2bb5ab9":"build and prepare models :","e98f7702":"# Data understanding and Overview","d60b4d0e":"ploting wordcloud for positive values :","5d8447c1":"spliting data into train and test set by using sklearn\u2019s train_test_split :","47ee9227":"# STEP 1: Preprocess Data and Build a Transformer Model","246748bb":"Preparing Token embeddings","11879449":"chose just negative reviews of our data frame ,,,to use it in wordcloud visualizing :","af1e3880":"8. Combine individual words","2c00268d":"show simple describtion of dataset number of positive and negative records:","0891544f":"As we see RidgeClassifier  model gave the highest accuricy , so lits show it\u2018s classification report:","97d47858":"chose just positve reviews of our data frame ,,,to use it in wordcloud visualizing :","a2318d68":"**Since we have reviews as text and we want to run a mathematical model we need a method to convert the text to numbers**","017f3bdb":"2. Remove none text and special character\n","2d1a46ba":"7. Lemmatization","f955b94c":"# Word embedding techniques (Text  features extraction) ","ba51d52c":"**Plotting Models Accuaracy scores :**","c27f9a73":"Preparing Token Ids..","f6fbf5f7":"Train models on our dataset :","b32d2262":"5. Tokenization","502dba72":"Read & view dataset :","258958aa":"C. Hashing ","694f32bb":"B. TF-IDF **","e8dea3ce":"# Data preprocessing phase","303b7ab6":"Get evaluation metrics of ML models (confusion_matrix ,accuracy , precision,recall,f1-score ,support ) :","fcbad8d3":"1. Turn sentiment into categorical values :","7a9ab42f":"Fine Tune BERT.."}}