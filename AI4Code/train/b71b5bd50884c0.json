{"cell_type":{"c6867a9f":"code","23fe1c8e":"code","8abee9e4":"code","912956b6":"code","b8d9e838":"code","cc19d654":"code","7155e4ec":"code","56f14db2":"code","5b522480":"code","90146710":"code","cb4cff0b":"code","2670fa3a":"code","4f614076":"code","f90aedd0":"code","121d869e":"code","047d6756":"code","4d67708f":"code","07f65339":"code","f8ea4ff3":"code","b5bccaab":"code","f018d3e4":"code","090152e1":"code","8426479f":"code","5ab34b39":"code","5611c703":"code","d295f70f":"code","eb8c3d8a":"code","40e076a2":"code","e7bbe49e":"code","b74f1995":"code","9697f361":"code","f37f03cd":"code","901e125b":"code","358d3e38":"code","dd7949dc":"code","fe5ac536":"code","8293db1b":"code","d0e4843f":"code","6e19d306":"code","f0603085":"code","2a43c9c7":"code","75f2fe96":"code","b929dcd4":"code","dd2e91ae":"code","f0a80518":"code","7c23ca06":"code","36cfd535":"code","81f4ce73":"code","2e2fee11":"code","7567968d":"code","5528d98b":"code","a4e9984e":"code","e96b9a15":"code","3fb29aab":"code","2afb71cb":"code","38b4c6d4":"code","99b67b95":"code","fd00d138":"code","1c7699ca":"code","967441c0":"code","e8336977":"code","aa3e75d4":"code","f1911166":"code","6d0ffd13":"code","05aeed67":"code","1a9ba560":"code","c995c8c1":"code","6a431298":"code","0c43f260":"code","038a7ea1":"code","aabca8a9":"code","c32b04c5":"markdown","9235b59b":"markdown","447274b1":"markdown","818455c2":"markdown","9ff71c23":"markdown","d5cf83ca":"markdown","05c76f03":"markdown","8d43ade5":"markdown","4f9bbe51":"markdown","933e1582":"markdown","7f0d61ed":"markdown","d585ee7d":"markdown","541b1b2a":"markdown","8fb9c858":"markdown","fbc457d9":"markdown","771a35cd":"markdown","2f2dd80b":"markdown","a16d9a2c":"markdown","5668d962":"markdown","dc2a2aba":"markdown","d7b87221":"markdown","468b2cf5":"markdown","ef26342d":"markdown","cae7ab47":"markdown","9c7cd041":"markdown","964b6964":"markdown","ee1b6a27":"markdown","d2e27346":"markdown","768c849f":"markdown","159874e3":"markdown","9951661a":"markdown","46ade761":"markdown","82798ecd":"markdown","0fc1d53f":"markdown","3e1d081c":"markdown","5bc8b04f":"markdown","0316379f":"markdown","87de10d0":"markdown","53bfdf6b":"markdown","b7395bf9":"markdown","7ed0d299":"markdown","0fb2f0a1":"markdown","2c187b43":"markdown","85e47976":"markdown","e4913ca5":"markdown","e4434d07":"markdown","e2f746dc":"markdown","6e9e4e3f":"markdown","64bee3a4":"markdown","02dcea08":"markdown","66aa6c69":"markdown","de80026f":"markdown"},"source":{"c6867a9f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","23fe1c8e":"train_df=pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\ntrain_df.head()","8abee9e4":"len(train_df),train_df.index.shape[-1]","912956b6":"import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing,metrics,manifold\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV,cross_val_predict\nfrom imblearn.over_sampling import ADASYN,SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nimport collections\nimport matplotlib.patches as mpatches\nfrom sklearn.metrics import accuracy_score\n%matplotlib inline\nfrom sklearn.preprocessing import RobustScaler\nimport xgboost\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import classification_report,roc_auc_score,roc_curve,r2_score,recall_score,confusion_matrix,precision_recall_curve\nfrom collections import Counter\nfrom sklearn.model_selection import StratifiedKFold,KFold,StratifiedShuffleSplit\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD,SparsePCA\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom nltk.tokenize import word_tokenize\nfrom collections import defaultdict\nfrom collections import Counter\nimport seaborn as sns\nfrom wordcloud import WordCloud,STOPWORDS\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go","b8d9e838":"## Assess the shape of the data\nprint(\"The Shape of the Dataset\".format(),train_df.shape)","cc19d654":"good_reviews=train_df[train_df['sentiment']=='positive']['review']\nbad_reviews=train_df[train_df['sentiment']=='negative']['review']\nprint(\"First 10 samples of good reviews\\n\".format(),good_reviews[:10])\nprint(\"First 10 samples of bad reviews\\n\".format(),bad_reviews[:10])","7155e4ec":"#Count of good and bad reviews\ncount=train_df['sentiment'].value_counts()\nprint('Total Counts of both sets'.format(),count)\n\nprint(\"==============\")\n#Creating a function to plot the counts using matplotlib\ndef plot_counts(count_good,count_bad):\n    plt.rcParams['figure.figsize']=(6,6)\n    plt.bar(0,count_good,width=0.6,label='Positive Reviews',color='Green')\n    plt.legend()\n    plt.bar(2,count_bad,width=0.6,label='Negative Reviews',color='Red')\n    plt.legend()\n    plt.ylabel('Count of Reviews')\n    plt.xlabel('Types of Reviews')\n    plt.show()\n    \ncount_good=train_df[train_df['sentiment']=='positive']\ncount_bad=train_df[train_df['sentiment']=='negative']\nplot_counts(len(count_good),len(count_bad))","56f14db2":"#Analyse the count of words in each segment- both positive and negative reviews\n#Function for checking word length\ndef cal_len(data):\n    return len(data)\n\n#Create generic plotter with Seaborn\ndef plot_count(count_ones,count_zeros,title_1,title_2,subtitle):\n    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n    sns.distplot(count_zeros,ax=ax1,color='Blue')\n    ax1.set_title(title_1)\n    sns.distplot(count_ones,ax=ax2,color='Red')\n    ax2.set_title(title_2)\n    fig.suptitle(subtitle)\n    plt.show()    \n\n\n\n\ncount_good_words=count_good['review'].str.split().apply(lambda z:cal_len(z))\ncount_bad_words=count_bad['review'].str.split().apply(lambda z:cal_len(z))\nprint(\"Positive Review Words:\" + str(count_good_words))\nprint(\"Negative Review Words:\" + str(count_bad_words))\nplot_count(count_good_words,count_bad_words,\"Positive Review\",\"Negative Review\",\"Reviews Word Analysis\")","5b522480":"#Count Punctuations\/Stopwords\/Codes and other semantic datatypes\n#We will be using the \"generic_plotter\" function.\n\ncount_good_punctuations=count_good['review'].apply(lambda z: len([c for c in str(z) if c in string.punctuation]))\ncount_bad_punctuations=count_bad['review'].apply(lambda z:len([c for c in str(z) if c in string.punctuation]))\nplot_count(count_good_punctuations,count_bad_punctuations,\"Positive Review Punctuations\",\"Negative Review Punctuations\",\"Reviews Word Punctuation Analysis\")","90146710":"#Analyse Stopwords\n\ndef plot_count_1(count_ones,count_zeros,title_1,title_2,subtitle):\n    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n    sns.distplot(count_zeros,ax=ax1,color='Blue')\n    ax1.set_title(title_1)\n    sns.distplot(count_ones,ax=ax2,color='Orange')\n    ax2.set_title(title_2)\n    fig.suptitle(subtitle)\n    plt.show()    \n\n\nstops=set(stopwords.words('english'))\ncount_good_stops=count_good['review'].apply(lambda z : np.mean([len(z) for w in str(z).split()]))\ncount_bad_stops=count_bad['review'].apply(lambda z : np.mean([len(z) for w in str(z).split()]))\nplot_count_1(count_good_stops,count_bad_stops,\"Positive Reviews Stopwords\",\"Negative Reviews Stopwords\",\"Reviews Stopwords Analysis\")","cb4cff0b":"## Checking number of Urls\ncount_good_urls=count_good['review'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\ncount_bad_urls=count_bad['review'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n\nplot_count_1(count_good_stops,count_bad_stops,\"Positive Reviews URLs\",\"Negative Reviews URLs\",\"Reviews URLs Analysis\")","2670fa3a":"#WordCloud Visualizations\n#Method for creating wordclouds\nfrom PIL import Image\ndef display_cloud(data,img_path,color):\n    plt.subplots(figsize=(10,10))\n    mask = np.array(Image.open(img_path))\n    wc = WordCloud(stopwords=STOPWORDS, \n                   mask=mask, background_color=\"white\", contour_width=2, contour_color=color,\n                   max_words=2000, max_font_size=256,\n                   random_state=42, width=mask.shape[1],\n                   height=mask.shape[0])\n    wc.generate(' '.join(data))\n    plt.imshow(wc, interpolation=\"bilinear\")\n    plt.axis('off')\n    plt.show()\n    \ndisplay_cloud(train_df['review'],'..\/input\/avenger-image-1\/captain-america__silo.png','red')","4f614076":"#Wordlcouds for good reviews\ndisplay_cloud( count_good['review'],'..\/input\/avenger-image\/avengers-endgame-imax-poster-crop.png','blue')","f90aedd0":"#Simplified counter function\ndef create_corpus(word):\n    corpus=[]\n    \n    for x in train_df[train_df['sentiment']==word]['review'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus\n\ncorpus=create_corpus('positive')\ncounter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:100]:\n    if (word not in stops) :\n        x.append(word)\n        y.append(count)\nsns.barplot(x=y,y=x)","121d869e":"#Gram analysis on Training set- Bigram and Trigram\nstopword=set(stopwords.words('english'))\ndef gram_analysis(data,gram):\n    tokens=[t for t in data.lower().split(\" \") if t!=\"\" if t not in stopword]\n    ngrams=zip(*[tokens[i:] for i in range(gram)])\n    final_tokens=[\" \".join(z) for z in ngrams]\n    return final_tokens\n\n\n#Create frequency grams for analysis\n    \ndef create_dict(data,grams):\n    freq_dict=defaultdict(int)\n    for sentence in data:\n        for tokens in gram_analysis(sentence,grams):\n            freq_dict[tokens]+=1\n    return freq_dict\n\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"n_gram_words\"].values[::-1],\n        x=df[\"n_gram_frequency\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n\n\ndef create_new_df(freq_dict,):\n    freq_df=pd.DataFrame(sorted(freq_dict.items(),key=lambda z:z[1])[::-1])\n    freq_df.columns=['n_gram_words','n_gram_frequency']\n    #print(freq_df.head())\n    #plt.barh(freq_df['n_gram_words'][:20],freq_df['n_gram_frequency'][:20],linewidth=0.3)\n    #plt.show()\n    trace=horizontal_bar_chart(freq_df[:20],'orange')\n    return trace\n    \ndef plot_grams(trace_zero,trace_one):\n    fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of positive reviews\", \n                                          \"Frequent words of negative reviews\"])\n    fig.append_trace(trace_zero, 1, 1)\n    fig.append_trace(trace_ones, 1, 2)\n    fig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\n    py.iplot(fig, filename='word-plots')\n    \n    \ntrain_df_zero=count_bad['review']\ntrain_df_ones=count_good['review']\n\nprint(\"Bi-gram analysis\")\nfreq_train_df_zero=create_dict(train_df_zero[:200],2)\n#print(freq_train_df_zero)\ntrace_zero=create_new_df(freq_train_df_zero)\nfreq_train_df_ones=create_dict(train_df_ones[:200],2)\n#print(freq_train_df_zero)\ntrace_ones=create_new_df(freq_train_df_ones)\nplot_grams(trace_zero,trace_ones)\nprint(\"Tri-gram analysis\")\nfreq_train_df_zero=create_dict(train_df_zero[:200],3)\n#print(freq_train_df_zero)\ntrace_zero=create_new_df(freq_train_df_zero)\nfreq_train_df_ones=create_dict(train_df_ones[:200],3)\n#print(freq_train_df_zero)\ntrace_ones=create_new_df(freq_train_df_ones)\nplot_grams(trace_zero,trace_ones)","047d6756":"#Lets fo a Penta-Gram analysis to get an idea how the vectorization will be performed\nprint(\"Penta-gram analysis\")\nfreq_train_df_zero=create_dict(train_df_zero[:200],5)\n#print(freq_train_df_zero)\ntrace_zero=create_new_df(freq_train_df_zero)\nfreq_train_df_ones=create_dict(train_df_ones[:200],5)\n#print(freq_train_df_zero)\ntrace_ones=create_new_df(freq_train_df_ones)\nplot_grams(trace_zero,trace_ones)\n","4d67708f":"%%time\nimport re\n#Removes Punctuations\ndef remove_punctuations(data):\n    punct_tag=re.compile(r'[^\\w\\s]')\n    data=punct_tag.sub(r'',data)\n    return data\n\n#Removes HTML syntaxes\ndef remove_html(data):\n    html_tag=re.compile(r'<.*?>')\n    data=html_tag.sub(r'',data)\n    return data\n\n#Removes URL data\ndef remove_url(data):\n    url_clean= re.compile(r\"https:\/\/\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n\n#Removes Emojis\ndef remove_emoji(data):\n    emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    data=emoji_clean.sub(r'',data)\n    url_clean= re.compile(r\"https:\/\/\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n\ntrain_df['review']=train_df['review'].apply(lambda z: remove_punctuations(z))\n\ntrain_df['review']=train_df['review'].apply(lambda z: remove_html(z))\ntrain_df['review']=train_df['review'].apply(lambda z: remove_url(z))\ntrain_df['review']=train_df['review'].apply(lambda z: remove_emoji(z))","07f65339":"## In this case, we will be replacing some abbreviated pronouns with full forms (example:\"you've\"->you have\")\ndef remove_abb(data):\n    data = re.sub(r\"he's\", \"he is\", data)\n    data = re.sub(r\"there's\", \"there is\", data)\n    data = re.sub(r\"We're\", \"We are\", data)\n    data = re.sub(r\"That's\", \"That is\", data)\n    data = re.sub(r\"won't\", \"will not\", data)\n    data = re.sub(r\"they're\", \"they are\", data)\n    data = re.sub(r\"Can't\", \"Cannot\", data)\n    data = re.sub(r\"wasn't\", \"was not\", data)\n    data = re.sub(r\"don\\x89\u00db\u00aat\", \"do not\", data)\n    data= re.sub(r\"aren't\", \"are not\", data)\n    data = re.sub(r\"isn't\", \"is not\", data)\n    data = re.sub(r\"What's\", \"What is\", data)\n    data = re.sub(r\"haven't\", \"have not\", data)\n    data = re.sub(r\"hasn't\", \"has not\", data)\n    data = re.sub(r\"There's\", \"There is\", data)\n    data = re.sub(r\"He's\", \"He is\", data)\n    data = re.sub(r\"It's\", \"It is\", data)\n    data = re.sub(r\"You're\", \"You are\", data)\n    data = re.sub(r\"I'M\", \"I am\", data)\n    data = re.sub(r\"shouldn't\", \"should not\", data)\n    data = re.sub(r\"wouldn't\", \"would not\", data)\n    data = re.sub(r\"i'm\", \"I am\", data)\n    data = re.sub(r\"I\\x89\u00db\u00aam\", \"I am\", data)\n    data = re.sub(r\"I'm\", \"I am\", data)\n    data = re.sub(r\"Isn't\", \"is not\", data)\n    data = re.sub(r\"Here's\", \"Here is\", data)\n    data = re.sub(r\"you've\", \"you have\", data)\n    data = re.sub(r\"you\\x89\u00db\u00aave\", \"you have\", data)\n    data = re.sub(r\"we're\", \"we are\", data)\n    data = re.sub(r\"what's\", \"what is\", data)\n    data = re.sub(r\"couldn't\", \"could not\", data)\n    data = re.sub(r\"we've\", \"we have\", data)\n    data = re.sub(r\"it\\x89\u00db\u00aas\", \"it is\", data)\n    data = re.sub(r\"doesn\\x89\u00db\u00aat\", \"does not\", data)\n    data = re.sub(r\"It\\x89\u00db\u00aas\", \"It is\", data)\n    data = re.sub(r\"Here\\x89\u00db\u00aas\", \"Here is\", data)\n    data = re.sub(r\"who's\", \"who is\", data)\n    data = re.sub(r\"I\\x89\u00db\u00aave\", \"I have\", data)\n    data = re.sub(r\"y'all\", \"you all\", data)\n    data = re.sub(r\"can\\x89\u00db\u00aat\", \"cannot\", data)\n    data = re.sub(r\"would've\", \"would have\", data)\n    data = re.sub(r\"it'll\", \"it will\", data)\n    data = re.sub(r\"we'll\", \"we will\", data)\n    data = re.sub(r\"wouldn\\x89\u00db\u00aat\", \"would not\", data)\n    data = re.sub(r\"We've\", \"We have\", data)\n    data = re.sub(r\"he'll\", \"he will\", data)\n    data = re.sub(r\"Y'all\", \"You all\", data)\n    data = re.sub(r\"Weren't\", \"Were not\", data)\n    data = re.sub(r\"Didn't\", \"Did not\", data)\n    data = re.sub(r\"they'll\", \"they will\", data)\n    data = re.sub(r\"they'd\", \"they would\", data)\n    data = re.sub(r\"DON'T\", \"DO NOT\", data)\n    data = re.sub(r\"That\\x89\u00db\u00aas\", \"That is\", data)\n    data = re.sub(r\"they've\", \"they have\", data)\n    data = re.sub(r\"i'd\", \"I would\", data)\n    data = re.sub(r\"should've\", \"should have\", data)\n    data = re.sub(r\"You\\x89\u00db\u00aare\", \"You are\", data)\n    data = re.sub(r\"where's\", \"where is\", data)\n    data = re.sub(r\"Don\\x89\u00db\u00aat\", \"Do not\", data)\n    data = re.sub(r\"we'd\", \"we would\", data)\n    data = re.sub(r\"i'll\", \"I will\", data)\n    data = re.sub(r\"weren't\", \"were not\", data)\n    data = re.sub(r\"They're\", \"They are\", data)\n    data = re.sub(r\"Can\\x89\u00db\u00aat\", \"Cannot\", data)\n    data = re.sub(r\"you\\x89\u00db\u00aall\", \"you will\", data)\n    data = re.sub(r\"I\\x89\u00db\u00aad\", \"I would\", data)\n    data = re.sub(r\"let's\", \"let us\", data)\n    data = re.sub(r\"it's\", \"it is\", data)\n    data = re.sub(r\"can't\", \"cannot\", data)\n    data = re.sub(r\"don't\", \"do not\", data)\n    data = re.sub(r\"you're\", \"you are\", data)\n    data = re.sub(r\"i've\", \"I have\", data)\n    data = re.sub(r\"that's\", \"that is\", data)\n    data = re.sub(r\"i'll\", \"I will\", data)\n    data = re.sub(r\"doesn't\", \"does not\",data)\n    data = re.sub(r\"i'd\", \"I would\", data)\n    data = re.sub(r\"didn't\", \"did not\", data)\n    data = re.sub(r\"ain't\", \"am not\", data)\n    data = re.sub(r\"you'll\", \"you will\", data)\n    data = re.sub(r\"I've\", \"I have\", data)\n    data = re.sub(r\"Don't\", \"do not\", data)\n    data = re.sub(r\"I'll\", \"I will\", data)\n    data = re.sub(r\"I'd\", \"I would\", data)\n    data = re.sub(r\"Let's\", \"Let us\", data)\n    data = re.sub(r\"you'd\", \"You would\", data)\n    data = re.sub(r\"It's\", \"It is\", data)\n    data = re.sub(r\"Ain't\", \"am not\", data)\n    data = re.sub(r\"Haven't\", \"Have not\", data)\n    data = re.sub(r\"Could've\", \"Could have\", data)\n    data = re.sub(r\"youve\", \"you have\", data)  \n    data = re.sub(r\"don\u00e5\u00abt\", \"do not\", data)  \ntrain_df['review']=train_df['review'].apply(lambda z: remove_abb(z))\n","f8ea4ff3":"train_df['review'][:5]","b5bccaab":"count_good=train_df[train_df['sentiment']=='positive']\ncount_bad=train_df[train_df['sentiment']=='negative']","f018d3e4":"#Apply Gram Analysis\ntrain_df_zero=count_bad['review']\ntrain_df_ones=count_good['review']\nprint(\"Tri-gram analysis\")\nfreq_train_df_zero=create_dict(train_df_zero[:200],3)\n#print(freq_train_df_zero)\ntrace_zero=create_new_df(freq_train_df_zero)\nfreq_train_df_ones=create_dict(train_df_ones[:200],3)\n#print(freq_train_df_zero)\ntrace_ones=create_new_df(freq_train_df_ones)\nplot_grams(trace_zero,trace_ones)","090152e1":"#Check with wordclouds again just to be sure!\ndisplay_cloud(train_df['review'],'..\/input\/avenger-image-1\/captain-america__silo.png','blue')","8426479f":"#Lemmatize the dataset\nfrom nltk.stem import WordNetLemmatizer\n\n\ndef lemma_traincorpus(data):\n    lemmatizer=WordNetLemmatizer()\n    out_data=\"\"\n    for words in data:\n        out_data+= lemmatizer.lemmatize(words)\n    return out_data\n\ntrain_df['review']=train_df['review'].apply(lambda z: lemma_traincorpus(z))\n","5ab34b39":"#check a sample from the lemmatized dataset\ntrain_df['review'][5:10]","5611c703":"#For example let us try to stem them and check  a sample\n\nfrom nltk.stem import *\ndef stem_traincorpus(data):\n    stemmer = PorterStemmer()\n    out_data=\"\"\n    for words in data:\n        out_data+= stemmer.stem(words)\n    return out_data\n\nsample_train_df=train_df[5:10]\nsample_train_df['review']=sample_train_df['review'].apply(lambda z: stem_traincorpus(z))\nsample_train_df['review']","d295f70f":"%%time\ntfidf_vect=TfidfVectorizer(stop_words='english',ngram_range=(1,3))\ntrain_tfidf=tfidf_vect.fit_transform(train_df['review'].values.tolist())\ntrain_tfidf.shape","eb8c3d8a":"## Outputs from the TF-IDF transformed data\nprint(train_tfidf)","40e076a2":"%%time\ntrain_li=[]\nfor i in range(len(train_df)):\n    if (train_df['sentiment'][i]=='positive'):\n        train_li.append(1)\n    else:\n        train_li.append(0)\ntrain_df['Binary']=train_li\ntrain_df.head()","e7bbe49e":"#Count Vectorization\nimport matplotlib\nimport matplotlib.pyplot as plt\ndef vectorize(data):\n    cv=CountVectorizer()\n    fit_data_cv=cv.fit_transform(data)\n    return fit_data_cv,cv\n\n#Tfidf vectorization from sklearn\ndef tfidf(data):\n    tfidfv=TfidfVectorizer()\n    fit_data_tfidf=tfidfv.fit_transform(data)\n    return fit_data_cv,tfidfv\n\ndef dimen_reduc_plot(test_data,test_label,option):\n    tsvd= TruncatedSVD(n_components=2,algorithm=\"randomized\",random_state=42)\n    tsne=TSNE(n_components=2,random_state=42) #not recommended instead use PCA\n    pca=SparsePCA(n_components=2,random_state=42)\n    if(option==1):\n        tsvd_result=tsvd.fit_transform(test_data)\n        plt.figure(figsize=(10,8))\n        colors=['orange','red']\n        \n        sns.scatterplot(x=tsvd_result[:,0],y=tsvd_result[:,1],hue=test_label        )\n        \n        plt.show()\n        plt.figure(figsize=(10,10))\n        plt.scatter(tsvd_result[:,0],tsvd_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n        color_red=mpatches.Patch(color='red',label='Negative Review')\n        color_orange=mpatches.Patch(color='orange',label='Positive Review')\n        plt.legend(handles=[color_orange,color_red])\n        plt.title(\"TSVD\")\n        plt.show()\n    if(option==2):\n        tsne_result=tsne.fit_transform(test_data)\n        plt.figure(figsize=(10,8))\n        colors=['orange','red']\n        sns.scatterplot(x=tsne_result[:,0],y=tsne_result[:,1],hue=test_label)\n        plt.show()\n        plt.figure(figsize=(10,10))\n        plt.scatter(x=tsne_result[:,0],y=tsne_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n        color_red=mpatches.Patch(color='red',label='Negative Review')\n        color_orange=mpatches.Patch(color='orange',label='Positive Review')\n        plt.legend(handles=[color_orange,color_red])\n        plt.title(\"PCA\")\n        plt.show() \n    if(option==3):\n        pca_result=pca.fit_transform(test_data.toarray())\n        plt.figure(figsize=(10,8))\n        colors=['orange','red']\n        sns.scatterplot(x=pca_result[:,0],y=pca_result[:,1],hue=test_label)\n        plt.show()\n        plt.figure(figsize=(10,10))\n        plt.scatter(x=pca_result[:,0],y=pca_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n        color_red=mpatches.Patch(color='red',label='Negtive Review')\n        color_orange=mpatches.Patch(color='orange',label='Positive Review')\n        plt.legend(handles=[color_orange,color_red])\n        plt.title(\"TSNE\")\n        plt.show()\ntrain_data=train_df       \ndata_vect=train_data['review'].values\ndata_vect_good=count_good['review'].values\ntarget_vect=train_data['Binary'].values\ntarget_data_vect_good=train_df[train_df['sentiment']=='positive']['Binary'].values\ndata_vect_bad=count_bad['review'].values\ntarget_data_vect_bad=train_df[train_df['sentiment']=='positive']['Binary'].values\ntrain_data_cv,cv= vectorize(data_vect)\nreal_review_train_data_cv,cv=vectorize(data_vect_good)\n\nprint(train_data.head())\ndimen_reduc_plot(train_data_cv,target_vect,1)\ndimen_reduc_plot(real_review_train_data_cv,target_data_vect_good,1)\ndimen_reduc_plot(real_review_train_data_cv,target_data_vect_bad,1)\n# dimen_reduc_plot(train_data_cv,target_vect,3)\n# dimen_reduc_plot(real_review_train_data_cv,target_data_vect_good,3)\n# dimen_reduc_plot(train_data_cv,target_vect,2)\n# dimen_reduc_plot(real_review_train_data_cv,target_data_vect_good,2)\n","b74f1995":"#TSNE visualization on first 1000 samples\ntrain_data=train_df[:1000]       \ndata_vect=train_data['review'].values\ndata_vect_good=count_good['review'].values\ntarget_vect=train_data['Binary'].values\ntarget_data_vect_good=train_df[train_df['sentiment']=='positive']['Binary'].values\ndata_vect_bad=count_bad['review'].values\ntarget_data_vect_bad=train_df[train_df['sentiment']=='positive']['Binary'].values\ntrain_data_cv,cv= vectorize(data_vect)\nreal_review_train_data_cv,cv=vectorize(data_vect_good)\ndimen_reduc_plot(train_data_cv,target_vect,3)\n","9697f361":"check_df=list(train_df['review'].str.split())\n","f37f03cd":"%%time\n## Load word2vec algorithm from gensim\nfrom gensim.models import Word2Vec,KeyedVectors\n\nmodel=Word2Vec(check_df,min_count=1)\nword_li=list(model.wv.vocab)\nprint(word_li[:5])","901e125b":"#View the Tensor\nprint(model)\nprint(model['reviewers'])","358d3e38":"#View the Embedding Word Vector\nplt.plot(model['reviewers'])\nplt.show()","dd7949dc":"##save the modeled words produced from Word2Vec\nmodel.save('word2vec_model.bin')\nloaded_model=KeyedVectors.load('word2vec_model.bin')\nprint(loaded_model)","fe5ac536":"#Measure Cosine distance\ndistance=model.similarity('reviewers','injustice')\nprint(distance)","8293db1b":"# PCA transform in 2D for visualization of embedded words\nfrom matplotlib import pyplot\npca = PCA(n_components=2)\ntransformation_model=loaded_model[loaded_model.wv.vocab]\nresult = pca.fit_transform(transformation_model[:50])\n# create a scatter plot of the projection\npyplot.scatter(result[:, 0], result[:, 1])\nwords = list(loaded_model.wv.vocab)\nfor i, word in enumerate(words[:50]):\n    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\npyplot.show()","d0e4843f":"#Using Google News Embeddings For our corpus\ngoogle_news_embed='..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin'\ngoogle_loaded_model=KeyedVectors.load_word2vec_format(google_news_embed,binary=True)\nprint(google_loaded_model)","6e19d306":"#Visualize the Word Vectors\nplt.plot(google_loaded_model['reviews'])\nplt.plot(google_loaded_model['injustice'])\nplt.show()","f0603085":"# PCA transform in 2D for visualization of google news embedded words\nfrom matplotlib import pyplot\npca = PCA(n_components=2)\ntransformation_model=google_loaded_model[google_loaded_model.wv.vocab]\nresult = pca.fit_transform(transformation_model[:50])\n# create a scatter plot of the projection\npyplot.scatter(result[:, 0], result[:, 1])\nwords = list(google_loaded_model.wv.vocab)\nfor i, word in enumerate(words[:50]):\n    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\npyplot.show()","2a43c9c7":"from gensim.scripts.glove2word2vec import glove2word2vec\n\nglove_file='..\/input\/glove-global-vectors-for-word-representation\/glove.6B.50d.txt'\nword2vec_output_file = 'glove.6B.100d.txt.word2vec'\nglove_loaded=glove2word2vec(glove_file, word2vec_output_file)\nprint(glove_loaded)","75f2fe96":"glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\nplt.plot(glove_model['reviews'])\nplt.plot(glove_model['injustice'])\nplt.show()","b929dcd4":"# PCA transform in 2D for visualization of glove embedded words\nfrom matplotlib import pyplot\npca = PCA(n_components=2)\ntransformation_model=glove_model[glove_model.wv.vocab]\nresult = pca.fit_transform(transformation_model[:50])\n# create a scatter plot of the projection\npyplot.scatter(result[:, 0], result[:, 1])\nwords = list(glove_model.wv.vocab)\nfor i, word in enumerate(words[:50]):\n    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\npyplot.show()","dd2e91ae":"from gensim.models import Word2Vec,KeyedVectors\n\nfasttext_file=\"..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec\"\nprint(fasttext_file)","f0a80518":"fasttext_model = KeyedVectors.load_word2vec_format(fasttext_file, binary=False)\nplt.plot(fasttext_model['reviews'])\nplt.plot(fasttext_model['injustice'])\nplt.show()","7c23ca06":"# PCA transform in 2D for visualization of glove embedded words\nfrom matplotlib import pyplot\npca = PCA(n_components=2)\ntransformation_model=fasttext_model[fasttext_model.wv.vocab]\nresult = pca.fit_transform(transformation_model[:50])\n# create a scatter plot of the projection\npyplot.scatter(result[:, 0], result[:, 1])\nwords = list(fasttext_model.wv.vocab)\nfor i, word in enumerate(words[:50]):\n    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\npyplot.show()","36cfd535":"#Creating Embedding Matrix\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nmaxlen=1000\nmax_features=5000 \nembed_size=300\n\ntrain_sample=train_df['review']\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_sample))\ntrain_sample=tokenizer.texts_to_sequences(train_sample)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_sample=pad_sequences(train_sample,maxlen=maxlen)\n\n\n\nEMBEDDING_FILE = '..\/input\/wikinews300d1msubwordvec\/wiki-news-300d-1M-subword.vec'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\nplt.plot(embedding_matrix[20])\nplt.show()","81f4ce73":"!pip install tensorflow==2.3.1\n!pip install \"tensorflow_hub>=0.6.0\"\n!pip3 install tensorflow_text==1.15\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\n\n","2e2fee11":"#Convert the textual reviews to list for analysing sentences(sentence vectors)\nz=train_df['review'].tolist()","7567968d":"##Tensorflow Hub ELMO-2\nimport tensorflow_hub as hub\nimport tensorflow as tf\n\nelmo = hub.load(\"https:\/\/tfhub.dev\/google\/elmo\/2\")\n\ndef create_elmo_embeddings(data):\n    embed=elmo(data,signature=\"default\",as_dict=True)[\"elmo\"]\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.tables_initializer())\n        out_x=sess.run(embed)\n        #out_y=ses.run(tf.reduce_mean(embed,1))\n        return out_x\nelmo_input=z[:2]\nelmo_output=create_elmo_embeddings(elmo_input)\n","5528d98b":"#tokenize and encode the inputs\n\nimport transformers\nfrom transformers import BertTokenizer,TFBertModel\ntokenizer = transformers.BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)\nbert_model = transformers.TFBertModel.from_pretrained('bert-large-uncased')\ndef bert_encode(data,maximum_length) :\n    input_ids = []\n    attention_masks = []\n  \n\n    for i in range(len(data)):\n        encoded = tokenizer.encode_plus(\n        \n          data[i],\n          add_special_tokens=True,\n          max_length=maximum_length,\n          pad_to_max_length=True,\n        \n          return_attention_mask=True,\n        \n        )\n      \n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n    return np.array(input_ids),np.array(attention_masks)\n\ntrain_input_ids,train_attention_masks = bert_encode(train_df['review'][:5],1000)","a4e9984e":"#Visualize the attention masks and input ids.\ntrain_attention_masks,train_input_ids\n","e96b9a15":"\n#Build a miniature model for extracting the embeddings\nimport tensorflow as tf\nfrom keras.models import Sequential,Model\nfrom tensorflow.keras.layers import LSTM, Dense,Flatten,Conv2D,Conv1D,GlobalMaxPooling1D\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom tensorflow.keras import layers\ninput_ids = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\ninput_masks_ids = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32')\nbert_output=bert_model([input_ids,input_masks_ids])[0]\nbert_output.shape\nbert_output[:,0,:]\nmodel=Model(inputs=[input_ids,input_masks_ids],outputs=[bert_output])\nmodel.summary()","3fb29aab":"#Use the tokenizer and model  from the Transformers and determine the output features from the last hidden layer.\nimport tensorflow as tf\nfrom transformers import BertTokenizer, TFBertModel\n\ndef get_embeddings(model_name,tokenizer,name,inp):\n    tokenizer = tokenizer.from_pretrained(name)\n    model = model_name.from_pretrained(name)\n    input_ids = tf.constant(tokenizer.encode(inp))[None, :]  # Batch size 1\n    outputs = model(input_ids)\n    last_hidden_states = outputs[0]\n    cls_token=last_hidden_states[0]\n    return cls_token\ncls_token=get_embeddings(TFBertModel,BertTokenizer,'bert-base-uncased',z[0])\ncls_token","2afb71cb":"# For visualizing the embeddings \nprint(cls_token.shape)\nplt.plot(cls_token[0])\nplt.plot(cls_token[1])\nplt.show()","38b4c6d4":"#Distil BERT Embeddings\nimport numpy as np\nfrom transformers import AutoTokenizer, pipeline, TFDistilBertModel\nfrom scipy.spatial.distance import cosine\ndef transformer_embedding(name,inp,model_name):\n\n    model = model_name.from_pretrained(name)\n    tokenizer = AutoTokenizer.from_pretrained(name)\n    pipe = pipeline('feature-extraction', model=model, \n                tokenizer=tokenizer)\n    features = pipe(inp)\n    features = np.squeeze(features)\n    return features\nembedding_features1=transformer_embedding('distilbert-base-uncased',z[0],TFDistilBertModel)\nembedding_features2=transformer_embedding('distilbert-base-uncased',z[1],TFDistilBertModel)\ndistance=1-cosine(embedding_features1[0],embedding_features2[0])\nprint(distance)","99b67b95":"#Visualize embeddings\nplt.plot(embedding_features1[0])\nplt.plot(embedding_features2[0])","fd00d138":"#BERT embeddings\nfrom transformers import AutoTokenizer, pipeline, TFBertModel\nbert_features1=transformer_embedding('bert-base-uncased',z[0],TFBertModel)\nbert_features2=transformer_embedding('bert-base-uncased',z[1],TFBertModel)\ndistance=1-cosine(bert_features1[0],bert_features2[0])\nprint(distance)\nplt.plot(bert_features1[0])\nplt.plot(bert_features2[0])\nplt.show()","1c7699ca":"##Roberta Embeddings\nfrom transformers import AutoTokenizer, pipeline, TFRobertaModel\nroberta_features1=transformer_embedding('roberta-base',z[0],TFRobertaModel)\nroberta_features2=transformer_embedding('roberta-base',z[1],TFRobertaModel)\ndistance=1-cosine(roberta_features1[0],roberta_features2[0])\nprint(distance)\nplt.plot(roberta_features1[0])\nplt.plot(roberta_features2[0])\nplt.show()","967441c0":"from transformers import AutoTokenizer, pipeline, TFXLNetModel\nxlnet_features1=transformer_embedding('xlnet-base-cased',z[0],TFXLNetModel)\nxlnet_features2=transformer_embedding('xlnet-base-cased',z[1],TFXLNetModel)\ndistance=1-cosine(xlnet_features1[0],xlnet_features2[0])\nprint(distance)\nplt.plot(xlnet_features1[0])\nplt.plot(xlnet_features2[0])\nplt.show()","e8336977":"from transformers import AutoTokenizer, pipeline, BartModel\nbart_features1=transformer_embedding('facebook\/bart-base',z[0],BartModel)\nbart_features2=transformer_embedding('facebook\/bart-base',z[1],BartModel)\ndistance=1-cosine(bart_features1[0],bart_features2[0])\nprint(distance)\nplt.plot(bart_features1[0])\nplt.plot(bart_features2[0])\nplt.show()","aa3e75d4":"from transformers import AutoTokenizer, pipeline, TFAlbertModel\nalbert_features1=transformer_embedding('albert-base-v1',z[0],TFAlbertModel)\nalbert_features2=transformer_embedding('albert-base-v1',z[1],TFAlbertModel)\ndistance=1-cosine(albert_features1[0],albert_features2[0])\nprint(distance)\nplt.plot(albert_features1[0])\nplt.plot(albert_features2[0])\nplt.show()","f1911166":"#sophisticated variants of BERT\nfrom transformers import AutoTokenizer, pipeline, FlaubertModel\nflaubert_features1=transformer_embedding('flaubert\/flaubert_base_cased',z[0],FlaubertModel)\nflaubert_features2=transformer_embedding('flaubert\/flaubert_base_cased',z[1],FlaubertModel)\ndistance=1-cosine(flaubert_features1[0],flaubert_features2[0])\nprint(distance)\nplt.plot(flaubert_features1[0])\nplt.plot(flaubert_features2[0])\nplt.show()","6d0ffd13":"#GPT embeddings\nfrom transformers import AutoTokenizer, pipeline, TFOpenAIGPTModel\ndef transformer_gpt_embedding(name,inp,model_name):\n\n    model = model_name.from_pretrained(name)\n    tokenizer = AutoTokenizer.from_pretrained(name)\n    tokenizer.pad_token = \"[PAD]\"\n    pipe = pipeline('feature-extraction', model=model, \n                tokenizer=tokenizer)\n    features = pipe(inp)\n    features = np.squeeze(features)\n    return features\ngpt_features1=transformer_gpt_embedding('openai-gpt',z[0],TFOpenAIGPTModel)\ngpt_features2=transformer_gpt_embedding('openai-gpt',z[1],TFOpenAIGPTModel)\ndistance=1-cosine(gpt_features1[0],gpt_features2[0])\nprint(distance)\nplt.plot(gpt_features1[0])\nplt.plot(gpt_features2[0])\nplt.show()","05aeed67":"#GPT-2\nfrom transformers import AutoTokenizer, pipeline, TFGPT2Model\n\ngpt2_features1=transformer_gpt_embedding('openai-gpt',z[0],TFGPT2Model)\ngpt2_features2=transformer_gpt_embedding('openai-gpt',z[1],TFGPT2Model)\ndistance=1-cosine(gpt2_features1[0],gpt2_features2[0])\nprint(distance)\nplt.plot(gpt2_features1[0])\nplt.plot(gpt2_features2[0])\nplt.show()","1a9ba560":"#Electra\n\nfrom transformers import AutoTokenizer, pipeline, TFElectraModel\nelectra_features1=transformer_embedding('google\/electra-small-discriminator',z[0],TFElectraModel)\nelectra_features2=transformer_embedding('google\/electra-small-discriminator',z[1],TFElectraModel)\ndistance=1-cosine(electra_features1[0],electra_features2[0])\nprint(distance)\nplt.plot(electra_features1[0])\nplt.plot(electra_features2[0])\nplt.show()","c995c8c1":"#Longformer\nfrom transformers import AutoTokenizer, pipeline, TFLongformerModel\nlongformer_features1=transformer_embedding('allenai\/longformer-base-4096',z[0],TFLongformerModel)\nlongformer_features2=transformer_embedding('allenai\/longformer-base-4096',z[1],TFLongformerModel)\ndistance=1-cosine(longformer_features1[0],longformer_features2[0])\nprint(distance)\nplt.plot(longformer_features1[0])\nplt.plot(longformer_features2[0])\nplt.show()","6a431298":"#Import BERT and the variables\nimport os\nBERT_MODEL = 'bert-base-uncased'\nCASED = 'uncased' in BERT_MODEL\nINPUT = '..\/input\/'\nTEXT_COL = 'comment_text'\nMAXLEN = 250\nos.system('pip install --no-index --find-links=\"..\/input\/pytorchpretrainedbert\/\" pytorch_pretrained_bert')","0c43f260":"#Error Cause\nos.system('pip install --no-index --find-links=\"..\/input\/pytorchpretrainedbert\/\" pytorch_pretrained_bert')","038a7ea1":"#Load  from pytorch pretrained model- weights\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom pytorch_pretrained_bert.modeling import BertModel\n\n#BERT_FP = '..\/input\/torch-bert-weights\/bert-base-uncased\/bert-base-uncased\/'\n#Function for creating BERT embeddings-matrix\ndef bert_embedding_matrix():\n    bert = BertModel.from_pretrained('bert-base-uncased')\n    print(bert)\n    bert_embeddings = list(bert.children())[0]\n    bert_word_embeddings = list(bert_embeddings.children())[0]\n    mat = bert_word_embeddings.weight.data.numpy()\n    return mat\nembedding_matrix = bert_embedding_matrix()\nprint(embedding_matrix.shape)","aabca8a9":"plt.plot(embedding_matrix[0])\nplt.plot(embedding_matrix[1])\nplt.plot(embedding_matrix[2])\nplt.show()","c32b04c5":"## Using ELMO Embeddings\n\nTraditionally, Elmo embeddings could be used from [tensorflow hub](https:\/\/tfhub.dev\/). The original implementation can be found in [AllenNLP](https:\/\/allennlp.org\/elmo). Some resources for using ELMO :\n\n- [TFHub](https:\/\/tfhub.dev\/google\/elmo\/1)\n- [ELMO-Good article](https:\/\/www.analyticsvidhya.com\/blog\/2019\/03\/learn-to-use-elmo-to-extract-features-from-text\/)\n- [ELMO-article](https:\/\/towardsdatascience.com\/elmo-contextual-language-embedding-335de2268604)\n","9235b59b":"## Static Word Embeddings\n\nThese embeddings are pre-trained on large corpuses like Wikipedia, News corpuses.etc. However, the base of these algorithms rely on 2 important techniques:\n\n- [Skipgram](http:\/\/mccormickml.com\/2016\/04\/19\/word2vec-tutorial-the-skip-gram-model\/)\n\n\n<img src=\"https:\/\/miro.medium.com\/max\/1580\/0*xqhh7Gd64VAQ1Sny.png\">\n\n\n- [Common Bag of Words Model](https:\/\/analyticsindiamag.com\/the-continuous-bag-of-words-cbow-model-in-nlp-hands-on-implementation-with-codes\/)\n\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*UVe8b6CWYykcxbBOR6uCfg.png\">\n\n\nThis blog by [Lilian](https:\/\/lilianweng.github.io\/lil-log\/2017\/10\/15\/learning-word-embedding.html) is a great introduction.\n\nThe Skipgram approach tries to predict contextual words and phrases given a single word, whereas the CBOW approach tries to predict a single correct word given the context.\n\n[Jay's blog](http:\/\/jalammar.github.io\/mit-analytics-lab-talk\/) is a great resource for more learning!\n","447274b1":"## Dynamic Embeddings\n\nDeep contextual embeddings and sentence\/word vectors falls under dynamic embeddings. These embeddings are current SOTA and these are deep contextual embeddings ,implying that there is a need for robust Neural NEtwork models for these architectures.\n\nSince we will not be going into depth about the model architectures of each of these, we will be brushing over the concetps required for creating these embedding models. The following models lie within the scope of these embeddings:\n\n- [ELMO](https:\/\/arxiv.org\/abs\/1802.05365)\n- [Transformers](https:\/\/arxiv.org\/abs\/1706.03762)\n\nBoth these papers are essentially important for their contributions to contextual deep embeddings.","818455c2":"## Importance of Data Preprocessing and Cleaning\n\nThe afore mentioned phase is one of the most important phase. If the textual data is not properly cleaned or processed, incorrect words\/puncutations\/urls and associated redundancies get added to the data. This impacts the performance when we will be creating static\/dynamic embeddings and analysing the sentence\/word vectors. In the context of embeddings,(and subsequently models), we will find that if we donot remove these inconsistencies, the vectors will not be properly placed. For example, if we apply a SOTA language embedding such as GPT-2 on unclean data containing such redundancies, the tokenizer will create separate tokens for them; which will lead the model to associate certain weights for these. These add to the redundancy and increase complexity of the model. When we will be extracting individual entries (word\/sentence vectors),then these redundancies get added to the vector space and interfere with different metrics such as semantic similarity or classification\/question answers etc.\n\nIn real world, data is much more unclean as in most of the cases, data scientists work with unstructured data coming from MonogoDb or other databases. In some cases, the data may be scraped from websites which picks up a lot of inconsistencies- specially pdf, which when converted to textual format(using beautifulsoup library or similar),may contain some urls\/tags.\nThis paper provides a good [analysis](https:\/\/arxiv.org\/abs\/1808.00024).Cleaning is hence really important!\n\n","9ff71c23":"## NLP Workshop:Dataset Preparation\n\n\nAuthored by [abhilash1910](https:\/\/www.kaggle.com\/abhilash1910)\n\n### Movie Reviews!!\n\nThis is the first step in the learning curriculum where we will be exploring strategies and initial benchmark analysis for our dataset and derive important information from it. In this context, we will be focussing on preparing the dataset, eliminating the redundancies such as punctuations ,stopwords and estimating the meaningfulness of the data.\n\nThe steps to be followed can be proposed:\n\n- Loading the dataset\n- Preprocessing of data\n    - Lemmatizing\n    - Tokenizing\n- Cleaning\n    - Stopword\n    - punctuations\n    - Common words\n    - URLs\n    - HTML tags\n    - Emojis\n    - Expanding abbreviations\n\nThese steps are fundamental for any pipeline related to NLP , and this produces the transformed data which is suitable for analysis. This data without redundancies is fit for passing to any deep learning model  or statistical model or can be finetuned with more transformations. This would rather input more \"semantic\" sense to the data at hand.\n\n\n\nLets get started!!\n\n<img src=\"https:\/\/www.denofgeek.com\/wp-content\/uploads\/2019\/07\/endgame-recap-scaled.jpeg?fit=2560%2C1440\">","d5cf83ca":"## XLNet Embeddings\n\nThis [paper](https:\/\/arxiv.org\/abs\/1906.08237) provides an important outline of the modifications made on top of BERT for producing XLNet. It applies an autoregressive language model and has the 2 most important points:\n\n- Enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order\n- Overcomes the limitations of BERT thanks to its autoregressive formulation.\n\nIt is a permutation language model and a pictorial representation can be :\n\n<img src=\"https:\/\/zdnet2.cbsistatic.com\/hub\/i\/r\/2019\/06\/21\/2a4e6548-9dee-491d-b638-8cfae9bbb2fe\/resize\/1200x900\/ab279544c2631111754a357ada50ef29\/google-xlnet-architecture-2019.png\">\n\n\nResources:\n- [Blog](https:\/\/mlexplained.com\/2019\/06\/30\/paper-dissected-xlnet-generalized-autoregressive-pretraining-for-language-understanding-explained\/)\n- [Xlnet](https:\/\/www.borealisai.com\/en\/blog\/understanding-xlnet\/)","05c76f03":"## Using the Transfomer Method\n\n\nIn this case , we will be using the HuggingFace Transformer method for extracting sentence embeddings. This is a rather simpler method as we only need to extract the last layer from from the model output. The model in this case is bert-base-uncased (12-layer, 768-hidden, 12-heads, 110M parameters.Trained on lower-cased English text.).This code segment is model agnostic and can be used for any variats of BERT (except T5, GPT variants).\n","8d43ade5":"## GPT-Generative Pretraining\n\n[This](https:\/\/s3-us-west-2.amazonaws.com\/openai-assets\/research-covers\/language-unsupervised\/language_understanding_paper.pdf) is a different model from BERT and its variants built primarily for NLG (generative modelling).  GPT has the following important points:\n\n\n- GPT is a model with absolute position embeddings so it\u2019s usually advised to pad the inputs on the right rather than the left.\n\n- GPT was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be observed in the run_generation.py example script.\n\nSome resources are helpful:\n\n- [GPT](https:\/\/medium.com\/dataseries\/openai-gpt-generative-pre-training-for-language-understanding-bbbdb42b7ff4)\n- [Openai](https:\/\/openai.com\/blog\/better-language-models\/)\n- [Imgae GPT](https:\/\/openai.com\/blog\/image-gpt\/)\n\n<img src=\"https:\/\/www.topbots.com\/wp-content\/uploads\/2019\/04\/cover_GPT_web.jpg\">","4f9bbe51":"## Statistical Analysis-II\n\nIn this context , we will be exploring further into the analysis part. This would allow us to have a better idea which part of the data requires removal and which part can be transformed before applying any model on it.\n\nHere we will be looking into:\n\n- [Gram Statistics](https:\/\/albertauyeung.github.io\/2018\/06\/03\/generating-ngrams.html)\n\nGram analysis is an essential tool which forms the base of preparing a common bag of words model containing relevant data. This process implies that we are taking into consideration which words are present in conjunction with other words with a maximum frequency in the dataset. Grams can be n-ary implying that we can have many gram analysis taking n-words together.For example: a Ternary Gram Analysis(Tri-gram) includes analysing sentences which have 3 words occuring together at a higher frequency.\n\nA detailed image of a ternary gram analysis using a famous example is provided:\n\n<img src=\"https:\/\/miro.medium.com\/max\/536\/1*vZhxrBkCz-yN_rzZBqSKiA.png\">\n\nAnother example is also provided:\n\n\n<img src=\"https:\/\/images.deepai.org\/glossary-terms\/867de904ba9b46869af29cead3194b6c\/8ARA1.png\">\n\n","933e1582":"## End of Dataset Preparation\n\nAt this stage , we have covered the dataset preparation part of the pipeline. At this stage we have analysed the dataset , got an initial estimate about the words in the corpus. We performed cleaning, statistical analysis as well as lemmatization to prepare the dataset for EDA and successive steps. ","7f0d61ed":"## Time for some Cleaning!\n\nBefore we move ahead , let us clean the dataset and remove the redundancies.This includes\n\n- HTML codes\n- URLs\n- Emojis\n- Stopwords\n- Punctuations\n- Expanding Abbreviations\n\nThese will be sufficient for cleaning the corpus! \n\n[Regex](https:\/\/docs.python.org\/3\/howto\/regex.html) is a very good tool which will help us to do this cleaning.","d585ee7d":"## BART Model\n\n[This](https:\/\/arxiv.org\/abs\/1910.13461) is alternate SOTA model to denoise sentence2 sentence pretraining for natural language generation,comprehension etc. The most important points can be summarized as:\n\n\n- Bart uses a standard seq2seq\/machine translation architecture with a bidirectional encoder (like BERT) and a left-to-right decoder (like GPT).\n\n- The pretraining task involves randomly shuffling the order of the original sentences and a novel in-filling scheme, where spans of text are replaced with a single mask token.\n\n- BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n\nThe architecture contains these encoder -decoder modules :\n<img src=\"https:\/\/miro.medium.com\/max\/3138\/1*Qss9gtS1nw_sgcG1pMAM2A.png\">\n\n<img src=\"https:\/\/miseciara.files.wordpress.com\/2013\/11\/bart.gif\">\n\n\nSome resources:\n- [Blog](https:\/\/medium.com\/dair-ai\/bart-are-all-pretraining-techniques-created-equal-e869a490042e)\n- [Blog-BART](https:\/\/medium.com\/analytics-vidhya\/revealing-bart-a-denoising-objective-for-pretraining-c6e8f8009564)","541b1b2a":"## BERT Embeddings with Alternate Strategy\n\nHere we see the new methodology applied for BERT using the Pipeline module of Trasnformers.","8fb9c858":"## Data Columns\n\nThe \"review\" column contains the textual information(input features) and the \"sentiment\" column contains the output labels. The task of any classifier is to correctly predict the \"sentiment\" given any \"review\" or textual column. Hence we have to apply our data cleaning, transformation steps to the \"review\" column.","fbc457d9":"## Importing libraries \n\nThe significant aspect is to import libraries for our use statistical analysis cases. Some of these include:\n\n- [Sklearn](https:\/\/scikit-learn.org\/stable\/)\n- [Matplotlib](https:\/\/matplotlib.org\/)\n- [Seaborn](https:\/\/seaborn.pydata.org\/)\n- [NLTK](https:\/\/www.nltk.org\/)\n- wordcloud\n\nThese libraries and frameworks are efficient in handling data which can be used for initial analysis. As we progress, we will be including more libraries.\n\n","771a35cd":"## Transformers\n\n\n\n<img src=\"https:\/\/static01.nyt.com\/images\/2007\/07\/02\/arts\/Trans1600.jpg?quality=75&auto=webp&disable=upscale\">\n\n\nWe come to Transformer Embeddings  for which the most important aspect is the Transformer architecture. Since we will be diving in depth into architectures in the Machine LEarning Training session (model building), it is safe to have a glimpse of a traditional Transformer Architecture.\n\n<img src=\"https:\/\/i0.wp.com\/esciencegroup.com\/wp-content\/uploads\/2020\/02\/01.png?resize=506%2C641&ssl=1\">\n\nWe will be working with the [HuggingFace](https:\/\/huggingface.co\/) repository as it contains all SOTA Transformer models. In this context, it is useful to mention some important resources:\n\n- [Transformer Keras](https:\/\/keras.io\/examples\/nlp\/text_classification_with_transformer\/)\n- [Kaggle Kernel](https:\/\/www.kaggle.com\/suicaokhoailang\/lstm-attention-baseline-0-652-lb)\n\n\nHowever in this case, since we would be using the models just for extracting embeddings or features, it is important to know the intermediate layers which should be chosen. Since Transformer architectures are really huge, (BERT\/GPT variants), it is very complicated to fully understand which layer should be extracted for the features. While BERT, the first Transformer, relies on 2 tokens ([CLS] and [SEP]) ,extracting the sentence embedding vectors are done after extracting the last output layer. However , different models have different number of layers, and in this case, we will exploring a model agnostic way to extract sentence embeddings and performing similarity check with all of the models.\n\n\nIt is recommended to follow this [article](http:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/) before going further\n\n<img src=\"http:\/\/jalammar.github.io\/images\/bert-next-sentence-prediction.png\">","2f2dd80b":"## Vectorization - TFIDF and Count\n\nWe will move ahead with TFIDF and Count vectorization strategies and will be going in further sections.\n\n- [TF-IDF Vectorization](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html): This works by applying a logarithmic term to inverse document frequency (IDF) part other than determining the \"TF\" or term freqency part. The formulation can be shown as follows:\n\n<img src=\"https:\/\/plumbr.io\/app\/uploads\/2016\/06\/tf-idf.png\">\n\n- [Count Vectorization](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html): This is a simpler vectorization technique which relies on frequency of occurence of a particular term in a document or corpus.\n\nA pictorial representation about the way in which vectorization occurs is provided:\n\n<img src=\"https:\/\/www.oreilly.com\/library\/view\/applied-text-analysis\/9781491963036\/assets\/atap_0408.png\">\n\nLet us now go ahead and vectorize the corpus and test its dimensionality.","a16d9a2c":"## BERT Embeddings\n\n[BERT](https:\/\/arxiv.org\/abs\/1810.04805) is a traditional SOTA transformer architecture published by Google Research which uses bidirectional pretraining . The importance of using BERT is that it has 2 important aspects:\n\n- Msked Language Model (MLM)\n- Next Sentence Prediction(NSP)\n\nThe bidirectional pre-training is essentially helpful to be used for any tasks. The [Huggingface](https:\/\/miro.medium.com\/max\/876\/0*ViwaI3Vvbnd-CJSQ.png) implementation is helpful for fine-tuning BERT for any language modelling task. The BERT architecture falls under an encoder-decoder(Transformer) model as follows:\n\n<img src=\"https:\/\/miro.medium.com\/max\/876\/0*ViwaI3Vvbnd-CJSQ.png\">\n\n\nFor fine-tuning and pre-training for different downstream tasks like Q\/A, Classification, Language Modelling, Multiple Choice, NER etc. different layers of the BERT are used. \n\n<img src=\"https:\/\/d3i71xaburhd42.cloudfront.net\/df2b0e26d0599ce3e70df8a9da02e51594e0e992\/15-Figure4-1.png\">\n\n","5668d962":"## ELMO - Brief Overview\n\nELMO is a contextualised deep embedding model, which is dynamic and semi supervised.\nWord representations are functions of entire input sequence,and are computed on top of 2 bidirectional LSTM with character convolutions. The standard architecture for ELMO is as follows:\n\n<img src=\"https:\/\/jalammar.github.io\/images\/Bert-language-modeling.png\">\n\nThe most important aspect is tasks specific combinations of intermediate layer representations of the Bilstm which allows retention of the words\/long sentence sequences in the embedding space .\n\n[Jay's blog provides a good walkthrough](http:\/\/jalammar.github.io\/illustrated-bert\/).\n\nBidirectional transfer learning (BiLSTM architecture) is important in this aspect.\n\n<img src=\"http:\/\/jalammar.github.io\/images\/elmo-forward-backward-language-model-embedding.png\">\n","dc2a2aba":"## Data Cleaning is completed!\n\nWe have completed the cleaning step and gained significant insights about the corpus.\n\n<img src=\"https:\/\/filmschoolrejects.com\/wp-content\/uploads\/2019\/01\/Spider-Man-Far-From-Home-700x500.png\">\n","d7b87221":"## Validating the number of entries\n\nThis includes the number of entries we have in the dataset. Also we can have an analysis on the statistical aspects of the data which we will be exploring further through graphs and charts.","468b2cf5":"## Alternate Strategy With Transformers-One For All\n\nSentence Vectors can be determined with the help of [Pipeline in Transformers](https:\/\/huggingface.co\/transformers\/main_classes\/pipelines.html). This is a robust and efficient way to generate sentence vectors and compute corrspoinding distances between those vectors. It is a faster way which applies to almost all transformers.\n\n<img src=\"https:\/\/www.sideshow.com\/wp\/wp-content\/uploads\/2019\/05\/InfinityStones-Infographic-01.jpg\">","ef26342d":"## Word2Vec and its variants\n\n[Word2Vec](https:\/\/arxiv.org\/abs\/1301.3781) is one of the traditional algorithms which was emphasized based on Heirarchical Softmax as well as with simplistic RNNs. [Gensim](https:\/\/radimrehurek.com\/gensim\/models\/word2vec.html) provides a great way to use and start with Word2Vec. \nThe Word2Vec algorithm builds by using the Skipgram model as well as the Common Bag of Words Model. Both the models are described in the links.\n\nSome references:\n\n- [Nathan's blog](https:\/\/nathanrooy.github.io\/posts\/2018-03-22\/word2vec-from-scratch-with-python-and-numpy\/)\n- [Illya's paper](https:\/\/arxiv.org\/abs\/1310.4546)\n- [Jason's Blog](https:\/\/machinelearningmastery.com\/develop-word-embeddings-python-gensim\/)\n- [Advanced Resource](https:\/\/adventuresinmachinelearning.com\/word2vec-keras-tutorial\/)\n\nAn overview of hte sipgram process:\n\n<img src=\"https:\/\/nathanrooy.github.io\/posts\/2018-03-22\/word2vec-from-scratch-with-python-and-numpy\/skip-gram-architecture.png\">\n\n\n\n\nHuffman Algorithm forms the base of Hierarchical softmax based [Word2vec algorithm](https:\/\/leimao.github.io\/article\/Hierarchical-Softmax\/). This uses the huffman coding algorithm  for compression.\n\n<img src=\"https:\/\/miro.medium.com\/proxy\/1*a4idodtq60y2U5HqpB_MTQ.png\">\n\n[This resource](https:\/\/towardsdatascience.com\/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281) is also helpful.","cae7ab47":"## BERT Embeddings From Pytorch\n\nSince we have explored Transformer Embeddings, we will be looking into using pretrained BERT embedding from [Pytorch](https:\/\/pytorch.org\/hub\/huggingface_pytorch-transformers\/).\nWe will be downloading BERT from [Pytorch Pretrained BERT](https:\/\/pypi.org\/project\/pytorch-pretrained-bert\/).\nThis is one of the initial starting libraries for Huggingface.\n\n\nSome important resources:\n\n- [Notebook](https:\/\/www.kaggle.com\/christofhenkel\/bert-embeddings-lstm)\n- [Blog](https:\/\/medium.com\/@aniruddha.choudhury94\/part-2-bert-fine-tuning-tutorial-with-pytorch-for-text-classification-on-the-corpus-of-linguistic-18057ce330e1)\n\n\n<img src=\"https:\/\/miro.medium.com\/max\/875\/1*Pvyx8-QWdo_WBghNOd-zGA.png\">","9c7cd041":"## End of Static Embeddings\n\nThus we saw the power of traditional static embeddings and also the way to create embedding matrices from those word vectors. The Word2Vec and its variant algorithms are hence very powerful for  initial benchmarking for any classification tasks. These old SOTA models are hence very useful for us in NLP.\n\n<img src=\"https:\/\/ftw.usatoday.com\/wp-content\/uploads\/sites\/90\/2019\/04\/captain-america.jpg\">\n","964b6964":"## The data is cleaned!\n\nLet us apply the Gram Statistics on the cleaned dataset!","ee1b6a27":"## Converting the labels to Binary Numerics\n\nHere we convert the labels into binary (1,0) values , which will be helpful when we apply tensor compression or dimensionality reduction algorithms for visualizing the vectors.","d2e27346":"## Transforming the Corpus!!\n\nNow at this stage the data is successfully cleaned and all redundant noises are removed. These steps are generic to any NLP pipeline which reduces the dimension of the data. Once the data is cleaned , we can again prune some words to their base form and reduce the sentence lengths. This is important because when we are applying any model (statistical, deep learning,  transformers,graphs), 2 different words from the same base word are encoded and tokenized in a different manner. For instance, the word \"watched\" and \"watching\" have the same root word \"watch\", however they are encoded separately with respect to any Tokenizer.\n\n\nTo alleviate this issue, it is recommended to perform lemmatization on the text corpus so that the words can be reduced to their root semantic word. Morphological transformations such as \"watched\" and \"watching\", are converted to their base form through this method. Stemming , although can be used , is not recommended as it does not take into consideration the semantics of the sentence or the surrounding words which are present around it.Stemming also produces words which are not present in the vocabulary.\n\nFor an in depth study of the same, please refer to the [Stanford documentation](https:\/\/nlp.stanford.edu\/IR-book\/html\/htmledition\/stemming-and-lemmatization-1.html)\n\nFor trying out stemming, you can use my porter stemmer [library](https:\/\/github.com\/abhilash1910\/Classic_Stemmer)\n\nBut for now, we will be using NLTK for our lemmatization purposes. So [lets, get started!](http:\/\/www.nltk.org\/api\/nltk.stem.html?highlight=lemmatizer)\n\n","768c849f":"## Important Links for Reduction\n\nThese are some important links for the reduction strategies:\n\n- [TSNE](https:\/\/www.datacamp.com\/community\/tutorials\/introduction-t-sne)\n- [Dimension Reduction](https:\/\/machinelearningmastery.com\/dimensionality-reduction-for-machine-learning\/#:~:text=Dimensionality%20reduction%20refers%20to%20techniques,input%20variables%20in%20a%20dataset.&text=Large%20numbers%20of%20input%20features,the%20number%20of%20input%20features.)\n- [PCA & TSNE-Good article](https:\/\/medium.com\/analytics-vidhya\/a-complete-guide-on-dimensionality-reduction-62d9698013d2)\n- [TFIDF and Reduction](https:\/\/towardsdatascience.com\/2-latent-methods-for-dimension-reduction-and-topic-modeling-20ff6d7d547)\n- [Berkely Course Advanced](https:\/\/people.eecs.berkeley.edu\/~jordan\/courses\/294-fall09\/lectures\/dimensionality\/)\n- [SVD](https:\/\/machinelearningmastery.com\/singular-value-decomposition-for-machine-learning\/)\n- [U-V matrix decomposition- SVD](https:\/\/blog.statsbot.co\/singular-value-decomposition-tutorial-52c695315254)","159874e3":"## Working with Glove Embeddings\n\nGlove embeddings rely on global vector representations mechanism, which is an unsupervised algorithm. This captures both the global corpus statistics as well as local semantic information. Glove vectors used here can be converted from \"txt\" format to Word2Vec format by using scripts provided in the Gensim library.This allows us to manipulate the glove embeddings in a manner similar to Word2Vec and apply the similarity metric.The loss function for the glove relies on logistic regression of the log co-occurence probabilities.\n![image.png](attachment:image.png)","9951661a":"## Inference from Analysis - II\n\nIn this section, we have analysed based on positional features of words in a corpus\/sentence\/paragraph. The Gram analysis,particularly the pentagram analysis provides an idea which sentences occur more often in the corpus. And in most of the cases, these bag of words are the ones picked up by any frequency vectorization technique. \n\nThus this provides an outline as to the frequency of the conjuction of words which are occuring at the highest frequency. Another important aspect is that,there is a presence of certain html tags and punctuations which have to be removed as these are adding noise to the review corpus. This will be taken up in the cleaning phase.","46ade761":"## Model agnostic procedure to create Embedding Matrix\n\nFrom the embeddings discussed above, all of these can be used to create Embedding Matrix . The importance of building a Embedding matrix is to have mutual co-occurence embedding probabilities (vectors) of all the words present in the corpus. This is the format in which neural network frameworks like Keras, Tensorflow uses. These embedding vectors are then passed through deep learning layers. The flexibility of these static embeddings is that ,they provide a good benchmark on the initial task (classification) in our case.These also provide a very good approximation to the amount of percentage accuracy or loss which can be achieved with a coupled Encoder-Decoder\/Transformer like architectures.\n\nIn this case, we will be using the Keras\/Tensorflow framework to build the matrix. Though we will not be building neural networks yet, this provides an insight as to how to build the Embedding Layer of a Keras Neural Network architecture.\n\n<img src=\"https:\/\/miro.medium.com\/max\/688\/1*zR61FG9RUd6ul4ecXA_euQ.jpeg\">\n\n\nSome tutorials for starting with NLP in KEras:\n\n- [Resource](https:\/\/keras.io\/examples\/nlp\/)","82798ecd":"## Roberta Model\n\n[Roberta Model](https:\/\/huggingface.co\/transformers\/model_doc\/roberta.html) is a robust and large model built by [Facebook Research](https:\/\/github.com\/pytorch\/fairseq\/tree\/master\/examples\/roberta), to alleviate undertrained nature of BERT. It trains in much larger mini-batch sizes. [This](https:\/\/cloud.google.com\/tpu\/docs\/tutorials\/roberta-pytorch) provides a good model of how to train Roberta on Google cloud.The original paper can be found [here](https:\/\/arxiv.org\/abs\/1907.11692), and the model architecture is provided.\n\n<img src=\"https:\/\/camo.githubusercontent.com\/f5c0d05eb0635cdd0e17e137265af23fa825b1d4\/68747470733a2f2f646c2e666261697075626c696366696c65732e636f6d2f584c4d2f786c6d5f6669677572652e6a7067\">\n\n\nResources:\n\n- [Blog](https:\/\/medium.com\/towards-artificial-intelligence\/a-robustly-optimized-bert-pretraining-approach-f6b6e537e6a6)\n- [Blog-2](https:\/\/medium.com\/analytics-vidhya\/using-roberta-with-fastai-for-nlp-7ed3fed21f6c)","0fc1d53f":"## DistilBERT\n\n[This](https:\/\/huggingface.co\/transformers\/model_doc\/distilbert.html) is a distilled version of pretraining BERT to produce a lightweight version of it. It is analogous to teacher supervision of a neural network learning to optimize tis weights. [DistilBERT Paper](https:\/\/arxiv.org\/abs\/1910.01108) provides an insight why it is 40% smaller but preserves 95% of BERT's weights for transfer learning.\n\n<img src=\"https:\/\/storage.googleapis.com\/groundai-web-prod\/media%2Fusers%2Fuser_14%2Fproject_391208%2Fimages%2FKD_figures%2Ftransformer_distillation.png\">\n\n\nA very neat representation on the model workflow is provided here:\n\n<img src=\"http:\/\/jalammar.github.io\/images\/distilBERT\/bert-input-to-output-tensor-recap.png\">","3e1d081c":"## Visualize the word Vector\n\nIn this case, we will be visualizing the word \"reviewers\" vector projection as well as its corresponding tensor.","5bc8b04f":"## Why Reduction?\n\nContextual word vectors are very large and they are determined with respect to the surrounding context.Visualizing large dimension tensors or vectors are computatinally very difficult. By lowering the dimension, the implication is that there is a tensor decomposition based on the above mentioned algorithms. Tensor decomposition is based on rank specifications of a tensor or matrix. Based on rank, these reductions rely on reducing the effective rank based on the number of components we would want to visualize. When we specify the number of components we would want the tensor to be compressed into, the effective rank reduction takes place by Eigen vector decomposition (which is a numerical method). \n\nThe TSNE, being a gradient descent based algorithm, takes up a lot of computation space when tried with large amounts of data. This leads to exhaustive memory consumption.In this case, a better solution would be to batch process the data, before passing it to the TSNE compressor. Tensorflow provides [Projector](https:\/\/projector.tensorflow.org\/) which is really helpful for visualizing different embedding vectors which we will be going into. A visualization of the same is provided:\n\n![image.png](attachment:image.png)\n","0316379f":"## Inference From Analysis -I\n\nThe following can be inferred from the data:\n\n- The dataset is balanced.\n- The dataset contains equal number of semantics for reviews of both polarity.\n- The dataset contains redundant words and html syntaxes.\n- Punctuations\/stopwords are present in a equal distribution in the dataset.\n\nThis tells us that we have to do lots of cleaning!\n\n\n<img src=\"https:\/\/i.pinimg.com\/originals\/c4\/84\/34\/c484342c69562e5960fc2a8951d87c74.png\">\n","87de10d0":"## Dimension reduction of the embedding vectors\n\nLet us try to visualize the compressed and decomposed embedding space based on first 50 entries in the dataset.Once we plot using the Word2Vec Skipgram\/CBOW model, we can visualize the relative positioning of the words close to each other. The famous example using \"kings\",\"queens\" is provided below:\n\n<img src=\"https:\/\/miro.medium.com\/max\/327\/1*keqyBCQ5FL6A7DZLrXamvQ.png\">\n\nCosine Distance measurement is one metric which determines closeness of the 2 word vectors.When we plot vectors using matplotlib, we will be seeing graphs like ECG(electro-cardiograms) diagrams. The reason being each word is analysed on the basis of the number of words in its surrounding words.\n\n![image.png](attachment:image.png)\n","53bfdf6b":"## Realize the Size of the BERT Model\n\nThe size of the simple NN model built with BERT as intermediate Embedding Layer can be observed. Bert-large-uncased has 24-layer, 1024-hidden, 16-heads, 336M parameters and trained on lower-cased English text.","b7395bf9":"## Semantic Embeddings\n\nIn this context, we will be looking into semantic embeddings. These include embeddings which can either by static and dynamic. Word Embeddings fall under this category. \n\nWord Embeddings: These are vector space transformations of the words present in the corpus. When converted to vectors, several metrics can be applied like finding similarity, distance measurement between the vectors, numerical transforms of the vectors. With word vectors, we can specify semantic similarity between different words or collection of words. A pictorial representation of word vectors compressed with Dimension reduction methods is [provided below](https:\/\/www.tensorflow.org\/tutorials\/text\/word_embeddings):\n\n<img src=\"https:\/\/github.com\/tensorflow\/docs\/blob\/master\/site\/en\/tutorials\/text\/images\/embedding.jpg?raw=1\">\n\n\nIn this scenario, we will be focussing on all embedding algorithms. Primarily we be starting with Word2Vec and will be understanding advanced BERT\/GPT architectures.","7ed0d299":"## GPT-2\n\n<img src=\"http:\/\/jalammar.github.io\/images\/gpt2\/openAI-GPT-2-3.png\">\n\nIt is a [robust model](https:\/\/cdn.openai.com\/better-language-models\/language_models_are_unsupervised_multitask_learners.pdf). GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset[1] of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.Some important aspects:\n\n- GPT-2 is a model with absolute position embeddings so it\u2019s usually advised to pad the inputs on the right rather than the left.\n\n- GPT-2 was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be observed in the run_generation.py example script.\n\nResources:\n\n- [GPT-2](http:\/\/jalammar.github.io\/illustrated-gpt2\/)\n- [Source Code](https:\/\/github.com\/openai\/gpt-2)\n- [Blog](https:\/\/www.analyticsvidhya.com\/blog\/2019\/07\/openai-gpt2-text-generator-python\/)\n- [Blog](https:\/\/towardsdatascience.com\/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8)\n\nIt is important to note the effect of Attention and masking in GPT-2 model. These are represented in the diagram:\n\n<img src=\"http:\/\/jalammar.github.io\/images\/gpt2\/gpt2-self-attention-qkv-1-2.png\">\n<img src=\"http:\/\/jalammar.github.io\/images\/gpt2\/gpt2-self-attention-qkv-3-2.png\">\n\nSelf Attention:\n\n<img src=\"http:\/\/jalammar.github.io\/images\/gpt2\/gpt2-self-attention-split-attention-heads-1.png\">\n","0fb2f0a1":"## Embeddings Conclusion and Key Takeaways\n\nThis Notebook has provided an idea of using a code segment for multiple Transformer based embedding models . For this it is to be kept in mind that most of these embedding models are very big architectures-Transformers - which we will be taking up in the next session on Model building. Since the codes are mostly in Tensorflow Keras, some additional resources are provided for running them in Pytorch:\n\n- [BERT Embeddings Pytorch](https:\/\/www.kaggle.com\/abhilash1910\/bertsimilarity-library)\n- [NLP Workshop](https:\/\/www.kaggle.com\/abhilash1910\/nlp-workshop-2-ml-india)\n- [Workshop1](https:\/\/www.kaggle.com\/abhilash1910\/nlp-workshop-ml-india)\n\nWill be updating with more reseources. We have explored embeddings (static and dynamic) and also had an idea on the entire NLP pipeline till embeddings!! Now we know the power of model agnostic code  in NLP.\n\n<img src=\"https:\/\/img.cinemablend.com\/filter:scale\/quill\/2\/0\/d\/8\/1\/6\/20d81656d25ffbc06d6b2e382241661c629972e1.jpg?mw=600\">\n\n\nNext module- [Notebook](https:\/\/www.kaggle.com\/colearninglounge\/nlp-end-to-end-cll-nlp-workshop-2)","2c187b43":"## Load the Dataset\n\nThis is the primary step of the entire pipeline. In this case, we have to load the dataset using pandas.\nIn this case, we will be exploring different datasets for our use case. We will be using the [IMDB Movie Reviews Dataset](https:\/\/www.kaggle.com\/lakshmi25npathi\/imdb-dataset-of-50k-movie-reviews) primarily for the initial use case. \n\nThe data is collated from [Stanford Dataset](http:\/\/ai.stanford.edu\/~amaas\/data\/sentiment\/) and the sentiment of the text corpus is either positive or negative.\n\nWe will be analysing the data first in terms of the columns which it has.","85e47976":"## The Cosine Distance Metric\n\n\nIn this context, we will be using Cosine similarity metric from [Scipy](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.spatial.distance.cosine.html). But we can use them after we extract the last hidden layer from the model output(similar to BERT). ","e4913ca5":"## Vectorization and Embeddings\n\nIn this context, we will be vectorizing our dataset. This would allow us to convert our data to higher dimensional containers (matrices). These vectorization strategies allow the word corpus to be properly suitable for advanced semantic analysis.\n\nHere there are 2 variants of transforming the textual corpus to a numerical vector:\n\n- Vectorize without semantics \n- Retain Semantic Importance\n\nIn the first case, vectorization strategy is used to provide a co-occurence probabilistic distribution for vectorization. Methods like TF-IDF,Count vectorization\/One hot vectorization, falls under this criteria.These methods leverage statistical co-occurence probabilities and log likelihoods for determining the frequently occuring sentences or group of words in a corpus.\n\nThe second case, relies on applying vectors with respect to semantic importance. Embeddings fall under this category. Embeddings are largely of 2 kinds \n\n- Static Embeddings: Word2Vec, Glove, Fasttext, Paragram \n- Dynamic Embeddings: ELMO, BERT & its variants, XLNet\/Transformer-XL\n\nAll of these embeddings rely on pretrained word vectors where a probabilistic score is attributed to each word in the corpus. These probabilities are plotted in a low dimensional plane and the \"meaning\" of the words are inferred from these vectors. Generally speaking cosine distance is taken as the major metric of similarity measurement between word and sentence vectors to infer similarity.","e4434d07":"## Restrictions with Tensorflow TF 2.0\n\n\nPreviously there used to be no issues, working with ELMO from tf hub with tensorflow version <2.0. With TF versions morethan 2.0, there are some issues with loading the embeddings (as eager execution during graph computation fails). Hence it is recommended to use ELMO-2 with Tensorflow <2.0 (favourably 1.15).\n\nELMO-3 can be used with Tensorflow 2.0 ","e2f746dc":"## Convert Input DataFrame to a List\n\nThis phase is helpful if we would like to investigate individual word embeddings or sentence embeddings. Differentiating the individual rows of text makes it easier to pass into static and dynamic embedding models.","6e9e4e3f":"## Finetuning BERT for Embeddings\n\nFor finetuning, it is to be kept in mind, there are many ways to do this. We are using BERT from Huggingface repository while it can also be used from [TF-HUB](https:\/\/tfhub.dev\/s?module-type=text-embedding) or from [Google-Research repository](https:\/\/github.com\/google-research\/bert). The reason for using HuggingFace is that the same codebase is applicable for all language models. The 3 most important input features that any language model asks for is:\n\n- input_ids\n- attention_masks\n- token_ids\n\nLet us first try to analyse and understand how BERT  tokenizers, and model can be used in this context. The [BERT](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html) documentation provides an outline of how to use BERT tokenizers and also modify it for downstream tasks.\n\nGenerally by virtue of transfer learning through weight transfer, we use pretrained [BERT models](https:\/\/huggingface.co\/transformers\/pretrained_models.html) from the list. This allows us to finetune it to extract only the embeddings. Since we are using Keras, we have to build up a small model containing an Input Layer and apply the tokenized(encoded) input ids, attention masks as input to the pretrained and loaded BERT model.This is very similar to creating a very own classification model for BERT using Keras\/Tensorflow, but since we will be needing only the Embeddings it is safe to extract only the sentence vectors in the last layer of the model output. In most of the cases , we will see that the dimensions of the output vector is (x,768) where x depends on the number of tokenized input features. For this we extract the [CLS] tokenized feature from the ouput to just extract the sentence embeddings.\n\n<img src=\"http:\/\/jalammar.github.io\/images\/distilBERT\/bert-output-tensor-selection.png\">\n\n\nSome important resources which may be helpful:\n\n- [Blog](https:\/\/towardsdatascience.com\/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a)\n- [Extensive Nice Blog](https:\/\/towardsdatascience.com\/nlp-extract-contextualized-word-embeddings-from-bert-keras-tf-67ef29f60a7b)\n- [Good Kernel](https:\/\/www.kaggle.com\/shirishsharma\/nlp-from-embeddings-and-rnns-to-bert)\n\n","64bee3a4":"## Variants of Word2Vec algorithms\n\nThese variants include :\n\n- [FastText](https:\/\/github.com\/facebookresearch\/fastText)\n- [Glove](https:\/\/nlp.stanford.edu\/projects\/glove\/)\n- [Google News Vectors](https:\/\/code.google.com\/archive\/p\/word2vec\/)\n\nSome important links:\n\n- [FastText Models](https:\/\/fasttext.cc\/docs\/en\/english-vectors.html)\n- [Resource](https:\/\/www.analyticsvidhya.com\/blog\/2017\/07\/word-representations-text-classification-using-fasttext-nlp-facebook\/)\n- [Glove Source](https:\/\/github.com\/stanfordnlp\/GloVe)\n\n\nAll of these are traditional static embeddings . However they are very powerful on their own terms. These can be used with any neural network or classifier model with correct activations to suit our purpose. Word2Vec is the first SOTA model which relies on Softmax and probabilistic log likelihood of softmax to generate the predicted outputs. Word2vec has laid the foundation of all the algorithms which we saw here as well as variants in Node2Vec, Doc2Vec etc.\n\n\nAn interactive view of these vector embeddings can be seen below:\n\n","02dcea08":"## Statistical Analysis-I\n\nThis is the start of the analysis phase where we will first check the amount of data present in either of the sentiments. \nWe will follow this up with some pictorial representations related to the words and frequency mappings.\n\n","66aa6c69":"## Visualizing the  Vector Space\n\nAs words and sentences are vectorized, the dimensions of the vector space becomes significantly large to be accomodated in a model. For any computation system it is recommended to keep the dimensions of a tensor (matrix) as small as possible and maintain its regularity.  For tensors with larger dimensions and irregular shapes, it is difficult for the system to perform any operation (matrix \/tensor multiplication etc.). Complex operations like tensor differentiation (Jacobian) or numerical approximation is another difficult thing to do for large matrices. The rank plays an important aspect for these operations.\n\nNow, we have to reduce the dimensions ,else the kernel will run out of memory. For this  we wmploy 3 different decomposition techniques:\n\n- [PCA](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html)\n- [SVD](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.TruncatedSVD.html)\n- [TSNE](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.manifold.TSNE.html)\n\nThese algorithms rely on Eigen vector decomposition and Eigen matrices for creating smaller matrices. These reduced matrices are well-fitted to perform any numerical approximation tasks from differentiation to higher order non linear dynamics. [PCA](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis) in general is a well known method and forms the base of all decomposition techniques.  Pictorially it operates as follows with the help of orthogonal Eigen vectors:\n\n<img src=\"https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_ica_vs_pca_thumb.png\">\n\n\nTSNE is a more sophisticated [method](https:\/\/lvdmaaten.github.io\/publications\/papers\/AISTATS_2009.pdf)  which uses a non convex optimization along with gradient descent. This is different than Eigen Vector (convex optimization) method of PCA and hence different results may be obtained in different iterations. It is a memory intensive method and is often powerful at the expense of longer execution time.\n\n<img src=\"https:\/\/miro.medium.com\/max\/685\/1*njEd7PiqBW-zW38E23Ho9w.png\">\n","de80026f":"## Albert Model\n\n[This](https:\/\/arxiv.org\/abs\/1909.11942) is a lighter version of BERT which splits the embedding matrix into 2 smaller matrices and uses repeated splitting in between the transformer layers.Some important points:\n\n\n- ALBERT is a model with absolute position embeddings so it\u2019s usually advised to pad the inputs on the right rather than the left.\n\n- ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains similar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same number of (repeating) layers.\n\nResources:\n-[Source code](https:\/\/github.com\/google-research\/ALBERT)\n- [Blog](https:\/\/medium.com\/@lessw\/meet-albert-a-new-lite-bert-from-google-toyota-with-state-of-the-art-nlp-performance-and-18x-df8f7b58fa28)\n- [Blog](https:\/\/medium.com\/doxastar\/from-bert-to-albert-pre-trained-langaug-models-5865aa5c3762)\n\n<img src=\"data:image\/jpeg;base64,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\">"}}