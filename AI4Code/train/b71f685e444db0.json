{"cell_type":{"835370b6":"code","e9d0bd39":"code","47b1086a":"code","064f3ab5":"code","37be01ab":"code","13aa7128":"code","563713b2":"code","a2a1a6a5":"code","14f2e870":"code","9443dd51":"code","ba08db1e":"code","d4c285b2":"code","250ea5fb":"code","d9fb95cc":"code","80850c57":"code","86642db4":"code","6e985feb":"code","f6112c56":"code","b566f430":"code","2af10d85":"code","4c9385f2":"code","808084e5":"code","b8bd891e":"code","fb0e5836":"code","ebadadef":"code","30121fc0":"code","e4d566f4":"code","d1ef97c4":"code","a5add0e9":"code","1d31d571":"code","ea1b9edc":"code","f322ae96":"code","1bca038c":"code","9788affb":"code","203665df":"code","d1aee7ea":"code","9ab66b0e":"code","44d0c727":"code","2bcc7955":"code","bdbcbbad":"code","f519fda0":"code","dce712b5":"code","785e66f8":"code","c83a2d60":"code","0d6d12a8":"code","855646a5":"code","3ef8d79f":"code","b0a2cb9e":"code","1e47b3c7":"code","3d4b5feb":"code","345ef31d":"code","2b564950":"code","221770fb":"code","efb06e57":"code","0b9d4f50":"code","c32d07fe":"code","cf61eb08":"code","533120e4":"markdown","aa15bed7":"markdown","86bd89f8":"markdown","1425b9d8":"markdown","f1aec9e7":"markdown","49bc5d47":"markdown","51132d76":"markdown","8d57f25e":"markdown","28f27dda":"markdown","8f94374f":"markdown","af643bb9":"markdown","8d9624f2":"markdown","97adf669":"markdown","33e79cfc":"markdown","d724d5cb":"markdown","56e75960":"markdown","7db894b5":"markdown","bec8d25e":"markdown","a36318be":"markdown","022c8dff":"markdown","5bcd4d0e":"markdown","00d13499":"markdown","be323e2d":"markdown","ad05e1d0":"markdown","56497b8f":"markdown","0c7e7a55":"markdown","50d33320":"markdown","b36a17f4":"markdown","c846a4f1":"markdown","875d83cd":"markdown","53713380":"markdown","0bce718c":"markdown"},"source":{"835370b6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e9d0bd39":"#Importing required Libraries\n#data analysis\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n#visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#machine learning models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier","47b1086a":"#acquiring the training and testing datasets \ntrain_set = pd.read_csv('..\/input\/train.csv')\ntest_set = pd.read_csv('..\/input\/test.csv')\ncomplete_set = [train_set, test_set]","064f3ab5":"#preview training data\ntrain_set.head()","37be01ab":"train_set.columns","13aa7128":"train_set.info()","563713b2":"train_set.describe()","a2a1a6a5":"#preveiw test data\ntest_set.head()","14f2e870":"test_set.info()","9443dd51":"test_set.describe()","ba08db1e":"#Describing the correlation between the columns\n# Set figure size with matplotlib\nplt.figure(figsize=(10,6))\nsns.heatmap(train_set.corr(),annot=True)","d4c285b2":"sns.countplot(x=\"Pclass\",hue=\"Sex\" ,data=train_set)","250ea5fb":"#Analyzing for Pclass\nsns.barplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=train_set);","d9fb95cc":"#Pivioting features\ntrain_set[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean()","80850c57":"sns.boxplot(x=\"Survived\", y=\"Age\", hue=\"Sex\", data=train_set);","86642db4":"X = sns.FacetGrid(train_set, col='Survived')\nX.map(plt.hist, 'Age', bins=20)","6e985feb":"sns.countplot(x=\"Sex\" ,data=train_set)","f6112c56":"sns.barplot(x=\"Sex\", y=\"Survived\", data=train_set);","b566f430":"#Pivoting for Sex\ntrain_set[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean()","2af10d85":"#Analyzing for SibSp\nsns.barplot(x=\"SibSp\", y=\"Survived\", hue=\"Sex\", data=train_set);","4c9385f2":"#Analyzing for SibSp\ntrain_set[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean()","808084e5":"#Analyzing for SibSp\nsns.barplot(x=\"Parch\", y=\"Survived\", data=train_set);","b8bd891e":"#Analyzing for Parch\ntrain_set[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean()","fb0e5836":"sns.countplot(x=\"Embarked\",data=train_set);","ebadadef":"sns.barplot(x=\"Embarked\", y=\"Survived\", data=train_set);","30121fc0":"sns.barplot(x=\"Embarked\", y=\"Survived\",hue=\"Sex\", data=train_set);","e4d566f4":"#Analyzing the realtion between every column with \"Survived\"\n#Analyzing for Embarked\ntrain_set[[\"Embarked\", \"Survived\"]].groupby(['Embarked'], as_index=False).mean()","d1ef97c4":"#Analyzing the realtion between every column with \"Survived\"\n#Analyzing for Fare\nsns.swarmplot(x=\"Survived\", y=\"Fare\",hue='Pclass', data=train_set);","a5add0e9":"y = sns.FacetGrid(train_set, col='Survived')\ny.map(plt.hist, 'Fare', bins=10)","1d31d571":"#Total Nuber of missing Values in train _set\ntrain_set.isnull().sum()","ea1b9edc":"    #complete missing age with median\n    train_set['Age'].fillna(train_set['Age'].median(), inplace = True)","f322ae96":"train_set.Age.isnull().sum()","1bca038c":"#complete embarked with mode\ntrain_set['Embarked'].fillna(train_set['Embarked'].mode()[0], inplace = True)","9788affb":"train_set.Embarked.isnull().sum()","203665df":"test_set.isnull().sum()","d1aee7ea":"#complete missing age with median\ntest_set['Age'].fillna(test_set['Age'].median(), inplace = True)\n#complete embarked with mode\ntest_set['Embarked'].fillna(test_set['Embarked'].mode()[0], inplace = True)\n#complete missing fare with median\ntest_set['Fare'].fillna(test_set['Fare'].median(), inplace = True)","9ab66b0e":"#Total Nuber of missing Values in test_set\ntest_set.isnull().sum()","44d0c727":"#Creating new features in both train_set and test_set\nfor dataset in complete_set:    \n    #Two features combined in one\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 #1 means passanger is travelling Alone\n     # now update value of IsAlone wherever FamilySize is greater than 1\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 \n   \n\n    #Regular Expression to split title from name\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n\n    #Continuous variable bins; qcut vs cut: https:\/\/stackoverflow.com\/questions\/30211923\/what-is-the-difference-between-pandas-qcut-and-pandas-cut\n    #Fare Bins\/Buckets using qcut or frequency bins: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.qcut.html\n    #Age Bins\/Buckets using cut or value bins: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.cut.html\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n\n\n    \n#Cleanup Title Column from less frequent Titles\nfor dataset in complete_set:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\n#preview data again\ntrain_set.info()\ntest_set.info()\n\n#for the warning below\n#https:\/\/stackoverflow.com\/questions\/20625582\/how-to-deal-with-settingwithcopywarning-in-pandas","2bcc7955":"#Backup data before converting it to numerical value\ntrain_backup=train_set\ntest_backup=test_set\ncomplete_set_backup=complete_set\n#Convert objects to category using Label Encoder for train_set and test_set\nfrom sklearn.preprocessing import LabelEncoder\n#code categorical data\nlabel = LabelEncoder()\nfor dataset in complete_set:    \n    dataset['Sex'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin'] = label.fit_transform(dataset['FareBin'])","bdbcbbad":"train_set.head()","f519fda0":"test_set.head()","dce712b5":"print(train_set.shape)\nprint(test_set.shape)","785e66f8":"train_model=train_set[['Survived','Pclass','Sex','Embarked','IsAlone','Title','FareBin','AgeBin']]\ntrain_model.head()","c83a2d60":"test_model=test_set[['Pclass','Sex','Embarked','IsAlone','Title','FareBin','AgeBin']]\ntest_model.head()","0d6d12a8":"X_train = train_model[['Pclass','Sex','Embarked','IsAlone','Title','FareBin','AgeBin']]\nY_train = train_model[\"Survived\"]\nX_test  = test_model\nX_train.shape, Y_train.shape, X_test.shape","855646a5":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","3ef8d79f":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","b0a2cb9e":"#K nearest neighbours\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","1e47b3c7":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","3d4b5feb":"# Perceptron\nfrom sklearn.linear_model import Perceptron\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","345ef31d":"# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","2b564950":"# Stochastic Gradient Descent\nfrom sklearn import linear_model\nsgd = linear_model.SGDClassifier(max_iter=1000)\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd","221770fb":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","efb06e57":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","0b9d4f50":"#Comparing the scores\nmodels = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","c32d07fe":"submission = pd.DataFrame({\n        \"PassengerId\": test_set[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })","cf61eb08":"#Submit output as CSV File\nsubmission.to_csv('submission.csv', index=False)","533120e4":"Analyzing the training set","aa15bed7":"Analyzing the realtion between every column with \"Survived\" using plots","86bd89f8":"Input test_set for a model ","1425b9d8":"Analyzing the realtion of SibSp and Parch column","f1aec9e7":"**Model and Prediction**\nNow we are ready to train a model and predict the required solution. We will use the following regression algorithms:\n\n* Logistic Regression\n* KNN or k-Nearest Neighbors\n* Support Vector Machines\n* Naive Bayes classifier\n* Decision Tree\n* Random Forrest\n* Perceptron \n*  Linear SVC\n* Stochastic Gradient Descent\n* Decision Tree\n* Random Forest","49bc5d47":"Here we can see the fare differece of each class ","51132d76":"Most passengers embarked from port 'S' i.e Southampton","8d57f25e":"Dealing with missing values in Age","28f27dda":"**Introduction**\n\nThis is my first notebook of machine learning. This kernel is inspired from [Titanic Data Science Solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)","8f94374f":"From the heatmap we can conclude the correlation ","af643bb9":"First we identify the columns which contains the missing values","8d9624f2":"Dealing with missing values in Test Set","97adf669":"Analyzing Test Set","33e79cfc":"**Creating New Features **\n\nWe will create some new features from the exisiting ones in order to represent our data well, and provide a relevant data to our models.\n1. Create FamilySize feature by adding SibSp and Parch\n2. Create IsAlone Feature from FamilySize, i.e if FamilySize is 1 then the person is traveling alone\n3. Create bins for Age and Fare Columns\n4. Create Title feature by extracting title from Name column","d724d5cb":"**Cleaning the Data**","56e75960":"Analyzing the realtion with Embarked Column","7db894b5":"From this barplot we can see that survival rate is dependent on Classs (as most of the females survived from Class 1).\nTo confirm this relation we will analyze by pivoting the PClass column","bec8d25e":"Analyzing the realtion of Sex column","a36318be":"Here we conclude that most number of people survived from classs 1 and least from class 3","022c8dff":"As we can see from the above barplot female survival rate is much greater than male survival rate.\nWe will group Sex and Survived columns to get a general idea of survival for each gender.","5bcd4d0e":"#Analyzing the realtion with Parch Column","00d13499":"Test set has missing values in Age and Cabin columns","be323e2d":"Analyzing the realtion with Embarked Column","ad05e1d0":"Input train_set for a model","56497b8f":"Analyzing the relation with Age column","0c7e7a55":"**Analyzing the Data**","50d33320":"So we conclude from the above garph that we have to divide age into bins (chnage from continous to discreete type) ","b36a17f4":"All missing values replaced with median in  Age column of the train_set DataFrame","c846a4f1":"**Convert Formats**\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.","875d83cd":"We will divide age into bins for getting more valuable insights from the data","53713380":"We ignore Name, Ticket and Cabin Columns for now, we will deal with them later","0bce718c":"Taining data has missing values in Age,Cabin and Embarked Columns"}}