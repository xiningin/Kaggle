{"cell_type":{"97d4c660":"code","be15fb95":"code","701db637":"code","65080249":"code","8bb62314":"code","bbd76503":"code","29d8a29e":"code","597b53e1":"code","a88e889f":"code","829dd14a":"code","20182068":"code","5e6ded38":"markdown","d23b196b":"markdown","74070c98":"markdown","d1f5c02d":"markdown","39afb666":"markdown","f4f04bb3":"markdown","18501d0a":"markdown","80d4ddd2":"markdown","51b29dd3":"markdown"},"source":{"97d4c660":"import json\nimport random\nimport numpy as np\nimport pandas as pd\nfrom igraph import Graph\nimport igraph\nfrom pprint import pprint\n#\nimport plotly.io as pio\nimport plotly.graph_objects as go\n#\nfrom xgboost import XGBClassifier\nfrom xgboost import XGBModel\nfrom xgboost import Booster\n#\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.utils import shuffle\n\ntry:\n    from PDSUtilities.xgboost import plot_importance\n    from PDSUtilities.xgboost import plot_tree\nexcept ImportError as e:\n    try:\n        !pip install PDSUtilities --upgrade\n        from PDSUtilities.xgboost import plot_importance\n        from PDSUtilities.xgboost import plot_tree\n    except ImportError as e:\n        raise ImportError(\"You must install PDSUtilities to plot importance and trees...\") from e\n\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk(\"\/kaggle\/input\"):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# # Colorblindness friendly colours...\n# # It is important to make our work\n# # as accessible as possible...\n# COLORMAP = [\"#005ab5\", \"#DC3220\"]\n# Labels for plotting...\nLABELS = {\n    \"Sex\": \"Sex\",\n    \"Age\": \"Age\",\n    \"MaxHR\": \"Max HR\",\n    \"OldPeak\": \"Old Peak\",\n    \"STSlope\": \"ST Slope\",\n    \"RestingBP\": \"Rest. BP\",\n    \"FastingBS\": \"Fast. BS\",\n    \"RestingECG\": \"Rest. ECG\",\n    \"Cholesterol\": \"Cholesterol\",\n    \"HeartDisease\": \"Heart Disease\",\n    \"ChestPainType\": \"Chest Pain\",\n    \"ExerciseAngina\": \"Ex. Angina\",\n}\n# Random seed for determinism...\nSEED = 395147\n\n# # You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","be15fb95":"# Loading the data from the csv file...\ndf = pd.read_csv(\"\/kaggle\/input\/heart-failure-prediction\/heart.csv\")\ndf.head()","701db637":"# Fix the egregious column naming error...\ndf = df.rename(columns = {\"ST_Slope\": \"STSlope\", \"Oldpeak\": \"OldPeak\"})\n\n# Always test these things...\nassert len(df[\"STSlope\"]) > 0, \"Ruh roh! ST_Slope is still terribly mistaken!\"\nassert len(df[\"OldPeak\"]) > 0, \"Ruh roh! Oldpeak is still terribly mistaken!\"\n\n# Convert target to categorical\ntarget = pd.Categorical(df[\"HeartDisease\"])\ndf[\"HeartDisease\"] = target.codes\n\nprint(\"Datatypes\")\nprint(\"---------\")\nprint(df.dtypes)","65080249":"# Break the columns into two groupings...\ncategorical_columns = [column for column in df.columns if df[column].dtypes == np.object]\nnumerical_columns   = [column for column in df.columns if df[column].dtypes != np.object]\n\nif \"HeartDisease\" in numerical_columns:\n    numerical_columns.remove(\"HeartDisease\")\n\nassert \"HeartDisease\" not in numerical_columns, \"Ruh roh! HeartDisease is still in numerical_columns!\"\n\nprint(\"Categorical Columns: \", categorical_columns)\nprint(\"  Numerical Columns: \", numerical_columns)","8bb62314":"def set_column_value_to_normal_distribution(df, column, value):\n    # Compute the column's mean and standard deviation\n    # after removing rows whose column matches value...\n    mean_value = df[df[column] != value][column].mean()\n    std_value  = df[df[column] != value][column].std()\n    # Create a random number generator...\n    rng = np.random.default_rng(SEED)\n    # Now set the column of those rows to a\n    # random sample from a normal distribution...\n    df[column] = df[column].apply(\n        lambda x : rng.normal(mean_value, std_value) if x == value else x\n    )\n    return df\n\ndf = set_column_value_to_normal_distribution(df, \"RestingBP\"  , 0)\ndf = set_column_value_to_normal_distribution(df, \"Cholesterol\", 0)\n\n# Always test...\nassert len(df[df[\"RestingBP\"  ] == 0]) == 0, \"Ruh roh! One or more patients has crashed again!\"\nassert len(df[df[\"Cholesterol\"] == 0]) == 0, \"Ruh roh! One or more patients has crashed again!\"","bbd76503":"PERFORM_GRID_SEARCH = True","29d8a29e":"PERFORM_GRID_SEARCH = False","597b53e1":"PERFORM_HUGE_GRID_SEARCH = False","a88e889f":"# Split the dataset into training and test...\nxt, xv, yt, yv = train_test_split(\n    df.drop(\"HeartDisease\", axis = 1),\n    df[\"HeartDisease\"],\n    test_size = 0.2,\n    random_state = 42,\n    shuffle = True,\n    stratify = df[\"HeartDisease\"]\n)\n\n# Define the data preparation, feature\n# selection and classification pipeline\npipeline = Pipeline(steps = [\n    (\"transform\", ColumnTransformer(\n            transformers = [\n                (\"cat\", OrdinalEncoder(), categorical_columns),\n                (\"num\", MinMaxScaler(), numerical_columns)\n            ]\n        )\n    ),\n    (\"features\", SelectKBest()),\n    (\"classifier\", XGBClassifier(\n            objective = \"binary:logistic\", eval_metric = \"auc\", use_label_encoder = False\n        )\n    )\n])\n\nif PERFORM_GRID_SEARCH:\n    # Define our search space for grid search...\n    # Short search over gamma as a quick example...\n    search_space = [{\n        \"classifier__n_estimators\": [60],\n        \"classifier__learning_rate\": [0.1],\n        \"classifier__max_depth\": [4],\n        \"classifier__colsample_bytree\": [0.2],\n        \"classifier__gamma\": [i \/ 10.0 for i in range(3, 7)],\n        \"features__score_func\": [chi2],\n        \"features__k\": [10],\n    }]\n    if PERFORM_HUGE_GRID_SEARCH:\n        # Define our search space for grid search...\n        # This is a real search but takes hours...\n        search_space = [{\n            \"classifier__n_estimators\": [i*10 for i in range(1, 10)],\n            \"classifier__learning_rate\": [0.01, 0.05, 0.1, 0.2],\n            \"classifier__max_depth\": range(1, 10, 1),\n            \"classifier__colsample_bytree\": [i\/20.0 for i in range(7)],\n            \"classifier__gamma\": [i \/ 10.0 for i in range(3, 7)],\n            \"features__score_func\": [chi2],\n            \"features__k\": [10],\n        }]\n    # Define grid search...\n    grid = GridSearchCV(\n        pipeline,\n        param_grid = search_space,\n        # Define cross validation...\n        cv = KFold(n_splits = 10, random_state = 917, shuffle = True),\n        # Define AUC and accuracy as score...\n        scoring = {\n            \"AUC\": \"roc_auc\",\n            \"Accuracy\": make_scorer(accuracy_score)\n        },\n        refit = \"AUC\",\n        verbose = 1,\n        n_jobs = -1\n    )\n    # Fit grid search\n    grid_model = grid.fit(xt, yt)\n    yp = grid_model.predict(xv)\n    #\n    print(f\"Best AUC Score: {grid_model.best_score_}\")\n    print(f\"Accuracy: {accuracy_score(yv, yp)}\")\n    print(\"Confusion Matrix: \", confusion_matrix(yv, yp))\n    print(\"Best Parameters: \", grid_model.best_params_)","829dd14a":"# Use the new best parameters if they were computed\n# else use the previously computed best parameters.\n# These produced an AUC score of 0.9244531360448315\n# and an accuracy of 0.8858695652173914.\nparameters = {\n    \"classifier__colsample_bytree\": 0.2,\n    \"classifier__gamma\": 0.4,\n    \"classifier__learning_rate\": 0.1,\n    \"classifier__max_depth\": 4,\n    \"classifier__n_estimators\": 60,\n    \"features__k\": 10,\n    \"features__score_func\": chi2\n}\nif PERFORM_GRID_SEARCH:\n    parameters = grid_model.best_params_\n\npipeline.set_params(**parameters)\n\nmodel = pipeline.fit(xt, yt)\nyp = model.predict(xv)\n\nprint(f\"Accuracy: {accuracy_score(yv, yp)}\")\nprint(\"Confusion Matrix: \", confusion_matrix(yv, yp))\nprint(\"Prediction: \", yp)","20182068":"classifier = pipeline[\"classifier\"]\ntrees = [tree for tree in classifier.get_booster()]\n# print(pio.templates[\"presentation\"])\nfeatures = [LABELS[column] for column in df.drop(\"HeartDisease\", axis = 1).columns]\n\nfig = plot_importance(classifier, features = features)\nfig.update_layout(template = \"presentation\")\nfig.update_layout(width = 700, height = 600)\nfig.update_layout(\n    margin = { 'l': 150 }, #, 'r': 10, 't': 50, 'b': 10 },\n)\n# This is literally the dumbest thing I've seen in years...\n# This puts space between the ticks and tick labels. SMFH.\nfig.update_yaxes(ticksuffix = \"  \")\n\nfig.show()\n\nbooster = classifier.get_booster()\nprint(\"Plotting the first five trees:\")\nfor tree in range(0, np.minimum(5, len(trees))):\n    title = f\"Tree {tree}\"\n    grayscale = tree % 2 == 0\n    edge_labels = { 'Yes\/Missing': \"Yes\" }\n    fig = plot_tree(booster, tree, features = features, grayscale = grayscale, edge_labels = edge_labels)\n    fig.update_layout(\n        margin = { 'l': 10, 'r': 10, 't': 50, 'b': 10 },\n        title = { 'text': title, 'x': 0.5, 'xanchor': \"center\" },\n    )\n    fig.show()\n","5e6ded38":"## Randomly Redistribute Missing Values\nThe columns `RestingBP` and `Cholesterol` have records with value 0. Hypothesising that these\nrepresent missing values, we set them to random values drawn from a normal distribution fit to\nthe rest of the values in each of those columns.","d23b196b":"## Load the Data","74070c98":"## Build the model...\n\nRun this cell if you want to perform a very limited grid search:","d1f5c02d":"## Description\nSimple notbook showing how to use `xgboost` to analyse the Heart Disease dataset.\n\nNotebook also shows how to use **`PDSUtilities.xgboost.plot_importance()`** and **`PDSUtilities.xgboost.plot_tree()`**, my Plotly-based replacements for `xgboost.plot_importance()` and `xgboost.plot_tree()`.\n\nCheck them out at [https:\/\/github.com\/DrJohnWagner\/PDSUtilities](https:\/\/github.com\/DrJohnWagner\/PDSUtilities).","39afb666":"Change `PERFORM_HUGE_GRID_SEARCH` to `True` if you want to do an extensive grid search.\n\nBe aware: this takes many hours on a four-core CPU!","f4f04bb3":"Run this cell if you **do not** want to perform grid search:","18501d0a":"## Grab the Column Names","80d4ddd2":"## Fix the Column Names\nColumns `ST_Slope` and `Oldpeak` do not use the same naming convention as the other columns.\n\nWe also convert `HeartDisease` to a categorical variable (`int64` to `int8`)...","51b29dd3":"## Plotting the Importance and First Five Trees\nHere I demonstrate the use of **`PDSUtilities.xgboost.plot_importance()`** and **`PDSUtilities.xgboost.plot_tree()`** on the `xgboost` model above..."}}