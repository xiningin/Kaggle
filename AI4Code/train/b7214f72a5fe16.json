{"cell_type":{"fd960ca6":"code","75e89a95":"code","8cc89bb7":"code","958ee315":"code","58991e8a":"code","12d21c5e":"code","938663bd":"code","36df58ff":"markdown","2d31a563":"markdown","83aa3ea0":"markdown","d0720278":"markdown","c52d2ce8":"markdown"},"source":{"fd960ca6":"import numpy as np                # linear algebra\nimport pandas as pd               # data frames\nimport seaborn as sns             # visualizations\nimport matplotlib.pyplot as plt   # visualizations\nimport scipy.stats                # statistics\nimport time\nfrom sklearn.preprocessing import power_transform\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\nfrom scipy.stats import uniform\n\nimport os\nprint(os.listdir(\"..\/input\"))","75e89a95":"df = pd.read_csv(\"..\/input\/pulsar_stars.csv\")\ndf_scale = df.copy()\ncolumns =df.columns[:-1]\ndf_scale[columns] = power_transform(df.iloc[:,0:8],method='yeo-johnson')\ndf_scale.head()","8cc89bb7":"# Create feature and target arrays\nX = df_scale.iloc[:,0:8]\ny = df_scale.iloc[:,-1]\n\n# Split into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=123, stratify=y)\nprint('X_train:', X_train.shape)\nprint('X_test:', X_test.shape)","958ee315":"# Import necessary modules\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {\"max_depth\": [3, None],\n              \"max_features\": randint(1, 9),\n              \"min_samples_leaf\": randint(1, 9),\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# Instantiate a Decision Tree classifier: tree\nsingle = DecisionTreeClassifier()\n\n# Instantiate the Grid Search\nsingle_cv = RandomizedSearchCV(single, param_dist, cv=5, scoring='accuracy')\n\n# Fit it to the data\nstart = time.time()\nsingle_cv.fit(X_train, y_train)\nsingle_t = time.time() - start\n\n\n# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(single_cv.best_params_))\nprint(\"Best score is {}\".format(single_cv.best_score_))\nprint(\"Run in (s) {}\".format(single_t) )","58991e8a":"# Import necessary modules\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 500, num = 10)],\n              \"max_depth\": [3, None],\n              \"max_features\": ['auto', 'sqrt'],\n              'min_samples_split': randint(2, 9),\n              'min_samples_leaf': randint(1, 9),\n              'bootstrap': [True, False]}\n\n# Instantiate a Random Forest Classifier\nbagging_tree = RandomForestClassifier()\n\n# Instantiate the Grid Search\nbagging_cv = RandomizedSearchCV(bagging_tree, param_dist, cv=5, scoring='accuracy')\n\n# Fit it to the data\nstart = time.time()\nbagging_cv.fit(X_train, y_train)\nbagging_t = time.time() - start\n\n\n# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(bagging_cv.best_params_))\nprint(\"Best score is {}\".format(bagging_cv.best_score_))\nprint(\"Run in (s) {}\".format(bagging_t) )","12d21c5e":"# Import necessary modules\nimport xgboost as xgb\n\n# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {'n_estimators': randint(100, 300),\n              'learning_rate': uniform(0.01, 0.6),\n              'subsample': [0.3, 0.9],\n              'max_depth': [3, 4, 5, 6, 7, 8, 9],\n              'colsample_bytree': [0.5, 0.8],\n              'min_child_weight': [1, 2, 3, 4]\n             }\n\n# Instantiate a XGBoost Classifier\nboosting_tree = xgb.XGBClassifier(objective = 'binary:logistic')\n\n# Instantiate the Grid Search\nboosting_cv = RandomizedSearchCV(boosting_tree, param_dist, cv=5, scoring='accuracy')\n\n# Fit it to the data\nstart = time.time()\nboosting_cv.fit(X_train, y_train)\nboosting_t = time.time() - start\n\n# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(boosting_cv.best_params_))\nprint(\"Best score is {}\".format(boosting_cv.best_score_))\nprint(\"Run in (s) {}\".format(boosting_t) )","938663bd":"results = {'Algorithm': ['single', 'bagging', 'boosting'],\n           'Acc': [single_cv.best_score_, bagging_cv.best_score_, boosting_cv.best_score_],\n           'time': [single_t, bagging_t, boosting_t]}\ndf = pd.DataFrame.from_dict(results)\n\nsns.scatterplot(x=\"time\", y=\"Acc\",\n                     hue=\"Algorithm\", sizes=(10, 200),\n                     data=df)","36df58ff":"For more about [Exploratory Data Analysis and preprocessing](https:\/\/www.kaggle.com\/camiloemartinez\/a-new-pulsar-star-supervised-machine-learning) explore this notebook in the link.","2d31a563":"## Objective\n\nThe idea is to compare different decision tree models in order to understand how different it performs. Those models vary from simple to a higher level of complexity. The first model is a single **Decision Tree**, the second model is a bagging model **Random Forest** and the last one is boosting with the **Gradient Boosting** model.","83aa3ea0":"## 1. Single Decision Tree\n\n[Single Models tunning](https:\/\/www.kaggle.com\/camiloemartinez\/a-new-pulsar-star-supervised-machine-learning) which is performed in this notebook and bring it here as a benchmark for more complex models.","d0720278":"## 2. Bagging Decision Tree \n\nFor this model it uses Random Forest tunning its parameters.","c52d2ce8":"## 3. Boosting Decision Tree \n\nFor this model it uses Xgboost tunning its parameters."}}