{"cell_type":{"afe4f7a3":"code","73916d03":"code","749b1850":"code","6df46579":"code","e0da7d69":"code","43ded95d":"code","88e00ce1":"code","62a238d6":"code","6b3147ad":"code","9dcf2d93":"code","3a739c2e":"code","fec5aa91":"code","731597ae":"code","65621c8a":"code","5eb9d14d":"code","4e2e8cb7":"code","ada3cf07":"code","64abaddf":"code","dcc9adc4":"code","26cc855a":"code","2d5f07b4":"code","092acd14":"code","53608b4a":"code","045ed7ab":"code","b8a6d2bd":"code","da5f9d2f":"code","0f7e2a61":"code","14ac985f":"code","249f734d":"code","16333d0e":"code","ec107045":"code","65ebcf57":"code","1f40abf0":"code","f73c93ac":"code","1a7e25b2":"code","6ad3a513":"markdown","f85c2cd0":"markdown","6300751f":"markdown","54127bde":"markdown","3ce6b852":"markdown","9c9afbef":"markdown","5ee84e2f":"markdown","63574326":"markdown","f0f4f581":"markdown","6bb9424f":"markdown","f687b91b":"markdown","c7070d51":"markdown","1a336f41":"markdown","ee59a69e":"markdown","f48c9f06":"markdown","01330a6e":"markdown","616eabf2":"markdown","d064eec3":"markdown","75fa73be":"markdown","f2f7fc63":"markdown","a2fe68d8":"markdown","f9eda28d":"markdown","5ce7329d":"markdown","619a4d93":"markdown","4fc62ee4":"markdown"},"source":{"afe4f7a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport pandas_profiling as pp\nimport seaborn as sns\n%matplotlib inline\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.","73916d03":"train= pd.read_csv('..\/input\/learn-together\/train.csv')\ntest= pd.read_csv('..\/input\/learn-together\/test.csv')","749b1850":"#pp.ProfileReport(train)","6df46579":"train.info()","e0da7d69":"test.info()","43ded95d":"from scipy.stats import ks_2samp\nfrom tqdm import tqdm\nks_values =[]\np_values  = []\ntrain_columns = train.columns[1:55]\nfor i in tqdm(train_columns):\n    ks_values.append(ks_2samp(test[i] , train[i])[0])\n    p_values.append(ks_2samp(test[i] , train[i])[1])\np_values_series = pd.Series(p_values, index = train_columns) ","88e00ce1":"colpercount = pd.DataFrame()\nfor col in train.columns[1:54]:    \n    unique_values =train[col].nunique()\n    colpercount = colpercount.append({'feature' : col,'unique_value_num' : unique_values},ignore_index= True)\nsns.set(rc={'figure.figsize':(16,8)})\nplot1=colpercount.plot(x='feature',y='unique_value_num',kind='bar')\nfor p in plot1.patches[1:]:\n    h = p.get_height()\n    x = p.get_x()+p.get_width()\/2.\n    if h != 0:\n        plot1.annotate(\"%g\" % p.get_height(), xy=(x,h), xytext=(0,4), rotation=90, \n                   textcoords=\"offset points\", ha=\"center\", va=\"bottom\")\nplot1.set(ylabel='Count')\nplot1= plot1.set(title='Number of unique values in each columns')","62a238d6":"sns.set(rc={'figure.figsize':(16,30)})\nx=1\nfor num, alpha in enumerate(train.columns[1:11]):\n    plt.subplot(6, 4, num+x)\n    #plt.hist(diffcheck(alpha)[0], alpha=0.75, label='train', color='g')\n    #plt.hist(diffcheck(alpha)[1], alpha=0.25, label='test', color='r')\n    #plt.legend(loc='upper right')\n    train[alpha].plot(kind='hist',color='forestgreen')\n    plt.title(alpha +('-train'))\n    plt.subplot(6, 4, num+x+1)\n    \n    x=x+1\n    test[alpha].plot(kind='hist',color='salmon')    \n    plt.title(alpha +('-test'))\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","6b3147ad":"sns.set(rc={'figure.figsize':(16,40)})\nx=1\nfor num, alpha in enumerate(train.columns[11:54]):\n    plt.subplot(22, 4, num+x)\n    #plt.hist(diffcheck(alpha)[0], alpha=0.75, label='train', color='g')\n    #plt.hist(diffcheck(alpha)[1], alpha=0.25, label='test', color='r')\n    #plt.legend(loc='upper right')\n    train[alpha].plot(kind='hist',color='forestgreen')\n    plt.title(alpha +('-train'))\n    plt.subplot(22, 4, num+x+1)\n    \n    x=x+1\n    test[alpha].plot(kind='hist',color='salmon')    \n    plt.title(alpha +('-test'))\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","9dcf2d93":"sns.set(rc={'figure.figsize':(8,4)})\nplot1=sns.countplot(train.Cover_Type)\nfor p in plot1.patches[1:]:\n    h = p.get_height()\n    x = p.get_x()+p.get_width()\/2.\n    if h != 0:\n        plot1.annotate(\"%g\" % p.get_height(), xy=(x,h), xytext=(0,4), rotation=90, \n                   textcoords=\"offset points\", ha=\"center\", va=\"bottom\")","3a739c2e":"sns.set(rc={'figure.figsize':(16,30)})\nfor num,alpha in enumerate(train.columns[1:11]):\n    ax1= plt.subplot(6,2,num+1)\n    sns.boxplot(data=train,y=alpha,x='Cover_Type',ax=ax1)\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","fec5aa91":"#sns.set(rc={'figure.figsize':(16,30)})\nfig = plt.figure(figsize=(16,40))\nfor num,alpha in enumerate(train.columns[1:11]):\n    ax1= plt.subplot(10,1,num+1)\n    #train[alpha].plot(kind='hist')\n    plot1=train.pivot(columns='Cover_Type')[alpha].plot(kind='hist',stacked=True,ax=ax1,legend=False)\n    if num==1:\n        handles, labels = plot1.get_legend_handles_labels()\n    plot1=plot1.set_title(alpha)    \nfig.legend(handles,labels,loc='upper right')\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n","731597ae":"#sns.set(rc={'figure.figsize':(16,40)})\nfig = plt.figure(figsize=(16,40))\nfor num,alpha in enumerate(train.columns[11:55]):\n    plt.subplot(15,3,num+1)\n    plot1=sns.countplot(x=train[alpha], hue =train.Cover_Type)\n    if num==1:\n        handles, labels = plot1.get_legend_handles_labels()\n    plot1.legend_.remove()\n    for p in plot1.patches[1:]:\n        h = p.get_height()\n        x = p.get_x()+p.get_width()\/2.\n        if h != 0:\n            plot1.annotate(\"%g\" % p.get_height(), xy=(x,h), xytext=(0,4), rotation=90, \n                   textcoords=\"offset points\", ha=\"center\", va=\"bottom\")    \nfig.legend(handles,labels,loc='lower right')#, bbox_to_anchor=(0.5, -0.12), ncol=3)\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","65621c8a":"sns.set(rc={'figure.figsize':(12,8)})\nplot1=sns.scatterplot(data=train,x='Elevation',y='Slope',hue='Aspect')","5eb9d14d":"plot2=sns.scatterplot(data=train,x='Hillshade_Noon',y='Hillshade_9am',hue='Aspect')","4e2e8cb7":"plot3=sns.scatterplot(data=train,x='Hillshade_Noon',y='Hillshade_3pm',hue='Aspect')","ada3cf07":"plot4=sns.scatterplot(data=train,x='Hillshade_9am',y='Hillshade_3pm',hue='Aspect')","64abaddf":"plot5=sns.scatterplot(data=train,x='Elevation',y='Vertical_Distance_To_Hydrology',hue='Cover_Type')","dcc9adc4":"plot6=sns.scatterplot(data=train,x='Horizontal_Distance_To_Hydrology',y='Horizontal_Distance_To_Roadways',hue='Cover_Type')","26cc855a":"X=train.copy()","2d5f07b4":"X['asp_cosine'] =  np.cos(np.radians(X.Aspect))\nX['slope_cosine'] = X.Slope * X['asp_cosine']\nX['asp_sine'] = np.sin(np.radians(X.Aspect))\nX['slope_sine'] = X.Slope * X['asp_sine']\ntest['asp_cosine'] =  np.cos(np.radians(test.Aspect))\ntest['slope_cosine'] = test.Slope * test['asp_cosine']\ntest['asp_sine'] = np.sin(np.radians(test.Aspect))\ntest['slope_sine'] = test.Slope * test['asp_sine']\nX['elevation_sq'] = X.Elevation **2\nX['log_elevation'] = np.log(X.Elevation+1)\ntest['elevation_sq'] = test.Elevation **2\ntest['log_elevation'] = np.log(test.Elevation+1)\nX['elev_slope']= X.Elevation\/X.Slope\ntest['elev_slope']= test.Elevation\/test.Slope\nX['dist_hydrolgy']= (X.Vertical_Distance_To_Hydrology**2 +X.Horizontal_Distance_To_Hydrology**2)**0.5\ntest['dist_hydrolgy']= (test.Vertical_Distance_To_Hydrology**2+test.Horizontal_Distance_To_Hydrology**2)**0.5\nX['elev_verthydrolgy'] = X.Elevation - X.Vertical_Distance_To_Hydrology\ntest['elev_verthydrolgy'] = test.Elevation - test.Vertical_Distance_To_Hydrology\nX['shade3_to_noon']= X.Hillshade_Noon-X.Hillshade_3pm\nX['shade9_to_noon']= X.Hillshade_Noon-X.Hillshade_9am\nX['shade9_to_3']= X.Hillshade_9am-X.Hillshade_3pm\ntest['shade3_to_noon']= test.Hillshade_Noon-test.Hillshade_3pm\ntest['shade9_to_noon']= test.Hillshade_Noon-test.Hillshade_9am\ntest['shade9_to_3']= test.Hillshade_9am-test.Hillshade_3pm\nX['shade_mean']= X.loc[:,['Hillshade_Noon','Hillshade_9am','Hillshade_3pm']].mean(axis=1)\nX['shade_std']= X.loc[:,['Hillshade_Noon','Hillshade_9am','Hillshade_3pm']].std(axis=1)\ntest['shade_mean']= test.loc[:,['Hillshade_Noon','Hillshade_9am','Hillshade_3pm']].mean(axis=1)\ntest['shade_std']= test.loc[:,['Hillshade_Noon','Hillshade_9am','Hillshade_3pm']].std(axis=1)\nX['distance_mean']= X.loc[:,['Horizontal_Distance_To_Hydrology','Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points']].mean(axis=1)\nX['distance_std']= X.loc[:,['Horizontal_Distance_To_Hydrology','Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points']].std(axis=1)\ntest['distance_mean']= test.loc[:,['Horizontal_Distance_To_Hydrology','Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points']].mean(axis=1)\ntest['distance_std']= test.loc[:,['Horizontal_Distance_To_Hydrology','Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points']].std(axis=1)","092acd14":"y= X.Cover_Type\nX.drop(['Id','Cover_Type'],axis=1,inplace=True)\ntest_id = test.Id\ntest.drop('Id',axis=1,inplace=True)\nX.drop(['Elevation','Slope','Aspect'],axis=1,inplace=True)\ntest.drop(['Elevation','Slope','Aspect'],axis=1,inplace=True)","53608b4a":"from lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)\nlgbmodel= LGBMClassifier(n_estimators=400,\n                           metric='multi_error',\n                           num_leaves=100,\n                           learning_rate=0.05,\n                           verbosity=1,\n                           random_state=1,\n                           n_jobs=-1)\nlgbmodel.fit(X_train,y_train,eval_metric='multi_error')","045ed7ab":"from sklearn.metrics import accuracy_score,classification_report,roc_auc_score\npredict_test = lgbmodel.predict(X_test)\nprint(classification_report(y_test,predict_test))","b8a6d2bd":"feature_importance_df = pd.DataFrame({'feature': X.columns,'importance': lgbmodel.feature_importances_})\nplt.figure(figsize=(14,25))\nsns.barplot(x=\"importance\",\n             y=\"feature\",\n             data=feature_importance_df.sort_values(by=\"importance\",\n                                            ascending=False))\nplt.title('LightGBM Features and importance')\nplt.tight_layout()","da5f9d2f":"predictions= lgbmodel.predict(test)","0f7e2a61":"params1 = {\n          \"objective\" : \"multiclass\",\n          \"num_class\" : 8,\n          \"num_leaves\" : 100,\n          'n_jobs' : -1,\n          \"max_depth\": -1,\n          \"learning_rate\" : 0.01,\n          'metric': 'multi_error',\n          \"bagging_fraction\" : 0.9,  # subsample\n          \"feature_fraction\" : 0.9,  # colsample_bytree\n          \"bagging_freq\" : 5,        # subsample_freq\n          \"bagging_seed\" : 1337,\n          \"verbosity\" : -1 }","14ac985f":"import lightgbm as lgb\nfrom sklearn.model_selection import KFold,StratifiedKFold, RepeatedKFold\nfeatures = [x for x in X.columns]\nfolds = RepeatedKFold(n_splits=3,n_repeats=2, random_state=0)\narr1=np.empty(shape=(len(X),8))\noof1 = np.zeros(len(X))\npred_arr1=np.empty(shape=(len(test),8))\npredictions1 = np.zeros(len(test))\nfeature_importance_df1 = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X.values, y.values)):\n    print(\"fold n\u00b0{}\".format(fold_))\n    trn_data = lgb.Dataset(X.iloc[trn_idx][features], label=y.iloc[trn_idx])\n    val_data = lgb.Dataset(X.iloc[val_idx][features], label=y.iloc[val_idx])\n\n    num_round = 1000\n    clf1 = lgb.train(params1, trn_data, num_boost_round=num_round,valid_sets = [trn_data, val_data], verbose_eval=-1, early_stopping_rounds = 100)\n    arr1[val_idx] = clf1.predict(X.iloc[val_idx][features], num_iteration=clf1.best_iteration)\n    oof1[val_idx] = np.argmax(arr1[val_idx],axis=1)\n    fold_importance_df1 = pd.DataFrame()\n    fold_importance_df1[\"feature\"] = features\n    fold_importance_df1[\"importance\"] = clf1.feature_importance()\n    fold_importance_df1[\"fold\"] = fold_ + 1\n    feature_importance_df1 = pd.concat([feature_importance_df1, fold_importance_df1], axis=0)\n    pred_arr1    += clf1.predict(test[features], num_iteration=clf1.best_iteration) \/ (3*2)\n    \npredictions1 = np.argmax(pred_arr1,axis=1)\nprint(classification_report(y,oof1))","249f734d":"cols = (feature_importance_df1[[\"feature\", \"importance\"]]\n         .groupby(\"feature\")\n         .mean()\n         .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\nbest_features = feature_importance_df1.loc[feature_importance_df1.feature.isin(cols)]\n\nplt.figure(figsize=(14,25))\nsns.barplot(x=\"importance\",\n             y=\"feature\",\n             data=best_features.sort_values(by=\"importance\",\n                                            ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()","16333d0e":"import xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nxgb1 = XGBClassifier(\n learning_rate =0.05,\n n_estimators=1000,\n max_depth=9,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'multi:softmax',\n metric='merror',\n num_class=7,\n scale_pos_weight=1,\n seed=1337)\n#X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)\nxgb1.fit(X_train,y_train)","ec107045":"predict_test1 = xgb1.predict(X_test)\nprint(classification_report(y_test,predict_test1))","65ebcf57":"predictions2= xgb1.predict(test)","1f40abf0":"feature_importance_df2 = pd.DataFrame({'feature': X.columns,'importance': xgb1.feature_importances_})\nplt.figure(figsize=(14,25))\nsns.barplot(x=\"importance\",\n             y=\"feature\",\n             data=feature_importance_df2.sort_values(by=\"importance\",\n                                            ascending=False))\nplt.title('XBoost Model  Features and importance')\nplt.tight_layout()","f73c93ac":"import statistics\nfinal_pred = np.array([])\nfor i in range(0,len(test)):\n    if len(set([predictions[i], predictions1[i], predictions2[i]])) == len([predictions[i], predictions1[i], predictions2[i]]):\n        final_pred = np.append(final_pred,predictions[i])\n    else:\n        final_pred = np.append(final_pred, statistics.mode([predictions[i], predictions1[i], predictions2[i]]))\nfinal_pred = final_pred.astype(int)","1a7e25b2":"\nsubmit = pd.DataFrame({'Id': test_id,\n                       'Cover_Type': final_pred})\nsubmit.to_csv('submission.csv', index=False)","6ad3a513":"The research paper mentions about slope * cosine(aspect), slope * sin(aspect),ln(Elevation+1),Square of elevation as important features which determine forest growth","f85c2cd0":"# Exploring Train and Test data <a><\/a>","6300751f":"Wilderness Area features show some interesting patterns . For example forest cover type 4 has only 0 values for Wildnerness_Area3 wheras it has only value in Wilderness_Area4. These features will help segregating forest cover tpes 3 & 4.","54127bde":"**Xgboost Model**","3ce6b852":"**Light GBM Models **\n","9c9afbef":"# Results <a><\/a>","5ee84e2f":"Let's now explore the 44 categorical features.","63574326":"Interesting to note that almost all data in Soil-type features is 0 in both test and train data sets. Whether these features have any importance while developingg models need to be loooked into.","f0f4f581":"LGBM model with repeated Kfolds","6bb9424f":"From the above histograms there seems to be no major difference in the distribution of data between train and test","f687b91b":"# Exploring Forest Cover Types <a><\/a>","c7070d51":"The train dataset has only 15120 rows of data and 56 columns. There are no null values in any of the features \/columns","1a336f41":"Aspect plays an important role in the Hill shade as seen from the plot below.","ee59a69e":"Selcting the final prediction based on the mode of predictions from all 3 models","f48c9f06":"From the above plot it's clear that the first 11 features are numerical and the last 44 features are categorical or boolean in nature.\n\nLet's look at histograms of the first 11 features which are numerical in nature. The data distribution in train and test datasets for each feature is shown side by side.","01330a6e":"Surprisingly test data has 565892 rows . It's unusual in a machine learning problem to have a very high number of test data compared to train data. This raises a very pertinent question . Are the training and test data similiar. \n\nLet's look at the number of unique values in each of the features in train dataset to understand the type of features\/columns\n\n","616eabf2":"Let's now look at how the different forest cover types appear in the categorical features","d064eec3":"Intersting difference in data distribution exists between the different types of forest covers across various features. It looks like it will be difficlt to seperate 1 and 2 cover types as the data follows identical distribution across features.","75fa73be":"Albert R. Stage and Christian Salas have done research on Interactions of Elevation, Aspect, and Slope in Models of Forest Species Composition and Productivity.\n\nASPECT,SLOPE,AND ELEVATION have been demon-strated to be useful surrogates for the spatial and temporal distribution of factors such as radiation,precipitation, and temperature that influence species com-position and productivity and hence type of forest covers.","f2f7fc63":"# Feature Engineering <a><\/a>","a2fe68d8":"Boxplots below show how within the numerical columns values are distributed across different forest cover types","f9eda28d":"# Models <a><\/a>","5ce7329d":"-[Exploring Train and Test data](#2)\n\n-[Exploring Forest Cover Types](#3)\n\n-[Understanding Features](#4)\n\n-[Feature Engineering](#5)\n\n-[Models](#6)\n\n-[Results](#7)","619a4d93":"# Understanding Features <a><\/a>","4fc62ee4":"\n\nAll the seven Forest cover types are equally represented in the train data as shown below"}}