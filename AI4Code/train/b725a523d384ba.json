{"cell_type":{"f7c5b66d":"code","23ae0b28":"code","80786db0":"code","0dca9c91":"code","38d1fe9a":"code","4f32801f":"code","8583ec35":"code","84bd8b22":"code","ade7068a":"code","efe20a44":"code","d3cb9bba":"code","0276c80c":"code","218e85ea":"code","7f8ef7d7":"code","db1920df":"code","0f27627e":"code","5c29883a":"code","ec8a8a84":"code","74f31050":"code","66f408ac":"code","5f7fdb14":"code","8173b167":"code","df57a76d":"code","12b30142":"code","3f23e14d":"code","f8ce3a32":"code","831de50c":"code","caed9cd4":"code","6d0a28c4":"code","1cf2c777":"code","ae25de77":"code","87b5af5d":"markdown","8e1cbf96":"markdown","40a82dab":"markdown","d9dea547":"markdown","fe920e4c":"markdown","6576cfb9":"markdown","cab9bd25":"markdown","f8e7d4ae":"markdown","e1eb8ea2":"markdown","5ccd381d":"markdown","bc76affb":"markdown","56aa8ef0":"markdown","0945eadc":"markdown","00a17916":"markdown","09ca5347":"markdown","cedefa6d":"markdown","6f3935ed":"markdown","834bc16b":"markdown","af191ee4":"markdown"},"source":{"f7c5b66d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport random\n\nfrom matplotlib.lines import Line2D\n\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.ensemble import IsolationForest\n\nimport optuna\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\n# import warnings\n# warnings.simplefilter(action='ignore', category=UserWarning)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('..\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","23ae0b28":"# Read the data\ntrain = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv', index_col='id')\ntest = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv', index_col='id')\nsample = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv', index_col='id')","80786db0":"train.head()","0dca9c91":"test.head()\n","38d1fe9a":"# Colors to be used for plots\n\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","4f32801f":"fig, ax = plt.subplots(figsize=(5, 5))\npie = ax.pie([len(train), len(test)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"salmon\", \"teal\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","8583ec35":"train.dtypes","84bd8b22":"train.describe().T","ade7068a":"test.describe().T","efe20a44":"train.nunique().sort_values().head(10)","d3cb9bba":"(train.claim ==1).sum()","0276c80c":"fig, ax = plt.subplots(figsize=(5, 5))\npie = ax.pie([(train.claim ==0).sum(), (train.claim ==1).sum()],\n             labels=[\"0\", \"1\"],\n             colors=[\"orange\", \"blue\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Target distribuition\\n\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","218e85ea":"test.columns == train.drop(columns=\"claim\").columns\nnum_attribs = test.columns","7f8ef7d7":"df = pd.concat([train.drop(\"claim\", axis=1), \n                test], axis=0)\ncolumns = num_attribs\n\ncols = 3\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,rows*3), sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            hist1 = axs[r, c].hist(train[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"deepskyblue\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\")\n            hist2 = axs[r, c].hist(test[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"palevioletred\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Test Dataset\")\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=13)\n            axs[r, c].tick_params(axis=\"x\", labelsize=13)\n            axs[r, c].grid(axis=\"y\")\n            axs[r, c].legend(fontsize=13)\n                                  \n        i+=1\n#plt.suptitle(\"Feature values distribution in both datasets\", y=0.99)\nplt.show();","db1920df":"# Plot dataframe\ndf = train.corr().round(5)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(16,16))\nax = sns.heatmap(df, annot=False, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"bold\", \"fontsize\":13})\nax.set_title(\"Feature correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"normal\")\nplt.setp(ax.get_yticklabels(), weight=\"normal\",\n         rotation_mode=\"anchor\", rotation=0, ha=\"right\")\nplt.show();","0f27627e":"print(\"(train, test) na --> \",(train.isna().sum().sum(), test.isna().sum().sum()))","5c29883a":"print(\"% Train NA for each feature, min, max\")\ntrain_na = train.drop(columns=\"claim\").isna().sum()\ntest_na = test.isna().sum()\n\nprint(f\"There are a minimum of {train_na.min()\/len(train)*100} % of NA Values in each features in TRAIN df \")\nprint(f\"There are a maximum of {train_na.max()\/len(train)*100} % of NA Values in each features in TRAIN df \")\nprint(\"Test NA for each feature, min, max\")\nprint(f\"There are a minimum of {test_na.min()\/len(test)*100} % of NA Values in each features in TEST df \")\nprint(f\"There are a maximum of {test_na.max()\/len(test)*100} % of NA Values in each features in TEST df \")","ec8a8a84":"# Checking if there are concentrated NA values in each id for Train and Test df\n\nna_counts_train = train.isna().sum(axis=1).sort_values(ascending = False).value_counts()\n\nfig, ax = plt.subplots(figsize=(16, 8))\n\nbars = ax.bar(na_counts_train.index,\n              na_counts_train.values,\n              color=colors,\n              edgecolor=\"black\")\nax.set_title(\"N\u00b0 missing NA for each id in TRAIN db\", fontsize=20, pad=15)\nax.set_ylabel(\"Missing Values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"N\u00b0of missing values for each row\", fontsize=14, labelpad=10)\nax.bar_label(bars, [f\"{x:2.2f}%\" for x in na_counts_train.values\/(len(train)\/100)],\n                 padding=5, fontsize=10, rotation=90)\nax.margins(0.025, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","74f31050":"na_counts_test = test.isna().sum(axis=1).sort_values(ascending = False).value_counts()\n\n\nfig, ax = plt.subplots(figsize=(16, 8))\n\nbars = ax.bar(na_counts_test.index,\n              na_counts_test.values,\n              color=colors,\n              edgecolor=\"black\")\nax.set_title(\"N\u00b0 missing NA for each id in TEST db\", fontsize=20, pad=15)\nax.set_ylabel(\"Missing Values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"N\u00b0 of id\", fontsize=14, labelpad=10)\nax.bar_label(bars, [f\"{x:2.2f}%\" for x in na_counts_test.values\/(len(test)\/100)],\n                 padding=5, fontsize=10, rotation=90)\nax.margins(0.025, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","66f408ac":"print(\"Rows with more then 1 NA in TRAIN df:\")\nprint((train.isna().sum(axis=1)>0).value_counts())\nprint((train.isna().sum(axis=1)>0).value_counts()\/len(train)*100)\nprint(\"\\nRows with more then 1 NA in TEST df:\")\nprint((test.isna().sum(axis=1)>0).value_counts())\nprint((test.isna().sum(axis=1)>0).value_counts()\/len(test)*100)","5f7fdb14":"X = train.drop(columns = \"claim\")\ny = train.claim\nX_test = test.copy()","8173b167":"x_Mm_scaler = MinMaxScaler()\nX = pd.DataFrame(x_Mm_scaler.fit_transform(train.drop(\"claim\", axis=1)),\n                 columns=train.drop(\"claim\", axis=1).columns)\nX_test = pd.DataFrame(x_Mm_scaler.transform(test), columns=test.columns)\n","df57a76d":"from sklearn.model_selection import StratifiedShuffleSplit","12b30142":"xgb_params = {\n    'n_estimators': 10000, \n 'learning_rate': 0.1, \n 'subsample': 0.6, \n 'colsample_bytree': 0.5, \n 'max_depth': 6, \n 'booster': 'gbtree', \n 'tree_method': 'gpu_hist', \n 'random_state': 42, \n 'n_jobs': 4}","3f23e14d":"X.head()","f8ce3a32":"%%time\nsplits = 6\nskf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\noof_preds = np.zeros((X.shape[0],))\npreds = 0\nmodel_fi = 0\ntotal_mean_rmse = 0\n\nfor fold, (train_indicies, valid_indicies) in enumerate(skf.split(X,y)):\n    \n    X_train, X_valid = X.loc[train_indicies], X.loc[valid_indicies]\n    y_train, y_valid = y.loc[train_indicies], y.loc[valid_indicies]\n    print(fold, f\"X_train = {X_train.shape} - y_train: {y_train.shape}\")\n    print(fold, f\"X_valid = {X_valid.shape} - y_valid: {y_valid.shape}\")\n    model = XGBClassifier(**xgb_params)\n    model.fit(X_train, y_train,\n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_metric=\"rmse\",\n              early_stopping_rounds=100,\n              verbose=False)\n    print(\"fitted\")\n    preds += model.predict(X_test) \/ splits\n    print(preds.shape)\n    print(\"preds ok\")\n    model_fi += model.feature_importances_\n    print(\"model_fi ok\")\n    oof_preds[valid_indicies] = model.predict(X_valid)\n    print(oof_preds)\n    oof_preds[oof_preds < 0] = 0\n#     fold_rmse = np.sqrt(mean_squared_error(y_scaler.inverse_transform(np.array(y_valid).reshape(-1,1)), y_scaler.inverse_transform(np.array(oof_preds[valid_idx]).reshape(-1,1))))\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_indicies]))\n    print(f\"Fold {fold} RMSE: {fold_rmse}\")\n#         print(f\"Trees: {model.tree_count_}\")\n    total_mean_rmse += fold_rmse \/ splits\nprint(f\"\\nOverall RMSE: {total_mean_rmse}\") ","831de50c":"# xgb public Score: \npredictions = pd.DataFrame()\npredictions[\"id\"] = test.index\npredictions[\"claim\"] = preds\n\npredictions.to_csv('submission_xgb_no_NA.csv', index=False, header=predictions.columns)\npredictions.head()","caed9cd4":"fig, ax = plt.subplots(figsize=(5, 5))\npie = ax.pie([(predictions.claim < 0.5).sum(), (predictions.claim >= 0.5).sum()],\n             labels=[\"0\", \"1\"],\n             colors=[\"orange\", \"blue\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Target distribuition\\n\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","6d0a28c4":"%%time\nsplits = 6\nskf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\noof_preds = np.zeros((X.shape[0],))\npreds = 0\nmodel_fi = 0\ntotal_mean_rmse = 0\n\nfor fold, (train_indicies, valid_indicies) in enumerate(skf.split(X,y)):\n    \n    X_train, X_valid = X.loc[train_indicies], X.loc[valid_indicies]\n    y_train, y_valid = y.loc[train_indicies], y.loc[valid_indicies]\n    print(fold, f\"X_train = {X_train.shape} - y_train: {y_train.shape}\")\n    print(fold, f\"X_valid = {X_valid.shape} - y_valid: {y_valid.shape}\")\n    model = XGBClassifier(**xgb_params)\n    model.fit(X_train, y_train,\n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_metric=\"rmse\",\n              early_stopping_rounds=100,\n              verbose=False)\n    print(\"fitted\")\n    preds += model.predict(X_test) \/ splits\n    print(preds.shape)\n    print(\"preds ok\")\n    model_fi += model.feature_importances_\n    print(\"model_fi ok\")\n    oof_preds[valid_indicies] = model.predict(X_valid)\n    print(oof_preds)\n    oof_preds[oof_preds < 0] = 0\n#     fold_rmse = np.sqrt(mean_squared_error(y_scaler.inverse_transform(np.array(y_valid).reshape(-1,1)), y_scaler.inverse_transform(np.array(oof_preds[valid_idx]).reshape(-1,1))))\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_indicies]))\n    print(f\"Fold {fold} RMSE: {fold_rmse}\")\n#         print(f\"Trees: {model.tree_count_}\")\n    total_mean_rmse += fold_rmse \/ splits\nprint(f\"\\nOverall RMSE: {total_mean_rmse}\") ","1cf2c777":"from sklearn.impute import SimpleImputer","ae25de77":"imputer_zeros = SimpleImputer(strategy=\"constant\", fill_value = 0)","87b5af5d":"# Checking for Categorical Features","8e1cbf96":"There are a very low correlation between features","40a82dab":"Let's take a look at the data","d9dea547":"Visualizing wow many Na values for each column","fe920e4c":"## Checking how many na values in train and test","6576cfb9":"Train and test dataset are quite well balanced","cab9bd25":"Only the 37% of id have a full stack o features with no NA both in Train and Test set","f8e7d4ae":"Let's check the distribuition of the Target:","e1eb8ea2":"# Correlations","5ccd381d":"# XGBC with Zeros  NA values","bc76affb":"Quite a lot Na Values to handle. This table require a data cleaning\n## Data cleaning","56aa8ef0":"# EDA\nEDA base on Notebook https:\/\/www.kaggle.com\/maximkazantsev\/tps-08-21-xgboost Thanks to @maximkazantsev","0945eadc":"# Libraries and Data import","00a17916":"*****************\nWORK IN PROGRESS\n*****************\n## Filling NA values\nThere are some ML alghorithms that doesn't support the presence of NA values in dataframe. \nI'd like to verify the efficiency of several methods applying a untuned XGBR:\n\n* Filling all NA qith zeros\n* Filling all NA wth the mean value \n* Filling all NA with the median value for each features\n* Filling all NA with the median value for each features\n* Applying a ML algorith to search an appropriate Value [is it worth it???]\n\nXGBC can handle a DF with NA, so let's try to fit it without NA handler","09ca5347":"No features looks like to be categorical","cedefa6d":"# Scaling data","6f3935ed":"# XGBC without NA handling","834bc16b":"There are some id with a maximum of 14 missing feature. ","af191ee4":"We will use sklearn \"Simple Imputer\".\nWe will fit the SimpleImputer only in the Train Set. We will aplly it to both Train and Test set to avoid \nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.SimpleImputer.html"}}