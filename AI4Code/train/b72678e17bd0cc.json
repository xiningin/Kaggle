{"cell_type":{"738ce924":"code","4daaccc8":"code","5a590ee4":"code","09139487":"code","df9157f2":"code","aad02980":"code","967ca0e3":"code","7fa32f72":"code","503249a5":"code","561a4165":"code","f0e804f9":"code","47b47f3b":"code","89a4e5a8":"code","5ad65fc4":"code","365527e8":"code","8fa5d3c5":"code","17df3d9c":"code","f8704f7c":"code","cc2d5d99":"code","f7381350":"code","fc4b957d":"code","62771647":"code","621df55a":"code","b89cc90a":"code","f3a5109d":"code","0bce341e":"code","31e80cc1":"code","5dfc0bb8":"code","8cd185fe":"code","c055e3f0":"code","2aed8156":"code","b70f86fa":"code","70c9ae24":"code","e129a43a":"code","a90bed86":"code","048d15d9":"code","3ec5fd2e":"code","5c6a2bfe":"code","6cf1c9f6":"code","52a9cc8b":"code","ddf64d30":"code","cdf314e9":"code","f439f42e":"code","7628fdb4":"code","4283341f":"code","7b4e0f9f":"code","2c6456d4":"code","8ddfd001":"code","9bd27003":"code","29de8291":"code","0f1544d0":"code","099579e1":"markdown","6c9cd8d1":"markdown","595a7312":"markdown","e0f6c48f":"markdown","d5e31c7f":"markdown","fc624f21":"markdown","057e88d8":"markdown","2927d19f":"markdown","c55f1ca2":"markdown","5164bee7":"markdown","abdd7912":"markdown","41f1e1cc":"markdown","8f746259":"markdown","97a037df":"markdown","a54e983b":"markdown","222b34bb":"markdown","9a2ba985":"markdown","6056f71d":"markdown","aa89a152":"markdown","2f258ad0":"markdown","16f6f6c9":"markdown","8977fa07":"markdown","ca7f3792":"markdown","84119217":"markdown"},"source":{"738ce924":"## REQUIRED LIBRARIES\n# For data wrangling \nimport numpy as np\nimport pandas as pd\n\n# For visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\npd.options.display.max_rows = None\npd.options.display.max_columns = None","4daaccc8":"# Read the data frame\ndf = pd.read_csv('..\/input\/Churn_Modelling.csv', delimiter=',')\ndf.shape","5a590ee4":"# the initial 5 rows of the original dataset\ndf.head()","09139487":"# Check columns list and missing values\ndf.isnull().sum()","df9157f2":"# Get unique count for each variable\ndf.nunique()","aad02980":"# Drop the columns as explained above\ndf = df.drop([\"RowNumber\", \"CustomerId\", \"Surname\"], axis = 1)","967ca0e3":"# Review the top rows of what is left of the data frame\ndf.head()","7fa32f72":"# Check variable data types\ndf.dtypes","503249a5":"labels = 'Exited', 'Retained'\nsizes = [df.Exited[df['Exited']==1].count(), df.Exited[df['Exited']==0].count()]\nexplode = (0, 0.1)\nfig1, ax1 = plt.subplots(figsize=(10, 8))\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax1.axis('equal')\nplt.title(\"Proportion of customer churned and retained\", size = 20)\nplt.show()","561a4165":"    # We first review the 'Status' relation with categorical variables\n    fig, axarr = plt.subplots(2, 2, figsize=(20, 12))\n    sns.countplot(x='Geography', hue = 'Exited',data = df, ax=axarr[0][0])\n    sns.countplot(x='Gender', hue = 'Exited',data = df, ax=axarr[0][1])\n    sns.countplot(x='HasCrCard', hue = 'Exited',data = df, ax=axarr[1][0])\n    sns.countplot(x='IsActiveMember', hue = 'Exited',data = df, ax=axarr[1][1])","f0e804f9":"    # Relations based on the continuous data attributes\n    fig, axarr = plt.subplots(3, 2, figsize=(20, 12))\n    sns.boxplot(y='CreditScore',x = 'Exited', hue = 'Exited',data = df, ax=axarr[0][0])\n    sns.boxplot(y='Age',x = 'Exited', hue = 'Exited',data = df , ax=axarr[0][1])\n    sns.boxplot(y='Tenure',x = 'Exited', hue = 'Exited',data = df, ax=axarr[1][0])\n    sns.boxplot(y='Balance',x = 'Exited', hue = 'Exited',data = df, ax=axarr[1][1])\n    sns.boxplot(y='NumOfProducts',x = 'Exited', hue = 'Exited',data = df, ax=axarr[2][0])\n    sns.boxplot(y='EstimatedSalary',x = 'Exited', hue = 'Exited',data = df, ax=axarr[2][1])","47b47f3b":"# Split Train, test data\n# So here I've decided to make a split of 70\\30\ndf_train = df.sample(frac=0.7,random_state=300)\ndf_test = df.drop(df_train.index)\nprint(len(df_train))\nprint(len(df_test))","89a4e5a8":"df_train['BalanceSalaryRatio'] = df_train.Balance\/df_train.EstimatedSalary\nsns.boxplot(y='BalanceSalaryRatio',x = 'Exited', hue = 'Exited',data = df_train)\nplt.ylim(-1, 5)","5ad65fc4":"# Given that tenure is a 'function' of age, we introduce a variable aiming to standardize tenure over age:\ndf_train['TenureByAge'] = df_train.Tenure\/(df_train.Age)\nsns.boxplot(y='TenureByAge',x = 'Exited', hue = 'Exited',data = df_train)\nplt.ylim(-1, 1)\nplt.show()","365527e8":"'''Lastly we introduce a variable to capture credit score given age to take into account credit behaviour visavis adult life\n:-)'''\ndf_train['CreditScoreGivenAge'] = df_train.CreditScore\/(df_train.Age)","8fa5d3c5":"# Resulting Data Frame\ndf_train.head()","17df3d9c":"# Arrange columns by data type for easier manipulation\ncontinuous_vars = ['CreditScore',  'Age', 'Tenure', 'Balance','NumOfProducts', 'EstimatedSalary', 'BalanceSalaryRatio',\n                   'TenureByAge','CreditScoreGivenAge']\ncat_vars = ['HasCrCard', 'IsActiveMember','Geography', 'Gender']\ndf_train = df_train[['Exited'] + continuous_vars + cat_vars]\ndf_train.head()","f8704f7c":"'''For the one hot variables, we change 0 to -1 so that the models can capture a negative relation \nwhere the attribute in inapplicable instead of 0'''\ndf_train.loc[df_train.HasCrCard == 0, 'HasCrCard'] = -1\ndf_train.loc[df_train.IsActiveMember == 0, 'IsActiveMember'] = -1\ndf_train.head()","cc2d5d99":"# One hot encode the categorical variables\nlst = ['Geography', 'Gender']\nremove = list()\nfor i in lst:\n    if (df_train[i].dtype == np.str or df_train[i].dtype == np.object):\n        for j in df_train[i].unique():\n            df_train[i+'_'+j] = np.where(df_train[i] == j,1,-1)\n        remove.append(i)\ndf_train = df_train.drop(remove, axis=1)\ndf_train.head()","f7381350":"# minMax scaling the continuous variables\nminVec = df_train[continuous_vars].min().copy()\nmaxVec = df_train[continuous_vars].max().copy()\ndf_train[continuous_vars] = (df_train[continuous_vars]-minVec)\/(maxVec-minVec)\ndf_train.head()","fc4b957d":"# data prep pipeline for test data\ndef DfPrepPipeline(df_predict,df_train_Cols,minVec,maxVec):\n    # Add new features\n    df_predict['BalanceSalaryRatio'] = df_predict.Balance\/df_predict.EstimatedSalary\n    df_predict['TenureByAge'] = df_predict.Tenure\/(df_predict.Age - 18)\n    df_predict['CreditScoreGivenAge'] = df_predict.CreditScore\/(df_predict.Age - 18)\n    \n    # Reorder the columns\n    continuous_vars = ['CreditScore','Age','Tenure','Balance','NumOfProducts','EstimatedSalary','BalanceSalaryRatio',\n                   'TenureByAge','CreditScoreGivenAge']\n    cat_vars = ['HasCrCard','IsActiveMember',\"Geography\", \"Gender\"] \n    df_predict = df_predict[['Exited'] + continuous_vars + cat_vars]\n    \n    # Change the 0 in categorical variables to -1\n    df_predict.loc[df_predict.HasCrCard == 0, 'HasCrCard'] = -1\n    df_predict.loc[df_predict.IsActiveMember == 0, 'IsActiveMember'] = -1\n    \n    # One hot encode the categorical variables\n    lst = [\"Geography\", \"Gender\"]\n    remove = list()\n    for i in lst:\n        for j in df_predict[i].unique():\n            df_predict[i+'_'+j] = np.where(df_predict[i] == j,1,-1)\n        remove.append(i)\n    df_predict = df_predict.drop(remove, axis=1)\n    \n    # Ensure that all one hot encoded variables that appear in the train data appear in the subsequent data\n    L = list(set(df_train_Cols) - set(df_predict.columns))\n    for l in L:\n        df_predict[str(l)] = -1        \n    \n    # MinMax scaling coontinuous variables based on min and max from the train data\n    df_predict[continuous_vars] = (df_predict[continuous_vars]-minVec)\/(maxVec-minVec)\n    \n    # Ensure that The variables are ordered in the same way as was ordered in the train set\n    df_predict = df_predict[df_train_Cols]\n    return df_predict","62771647":"# Support functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom scipy.stats import uniform\n\n# Fit models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier # I've added this algorithm\n\n# Scoring functions\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve","621df55a":"# Function to give best model score and parameters\ndef best_model(model):\n    print(model.best_score_)    \n    print(model.best_params_)\n    print(model.best_estimator_)\ndef get_auc_scores(y_actual, method,method2):\n    auc_score = roc_auc_score(y_actual, method); \n    fpr_df, tpr_df, _ = roc_curve(y_actual, method2); \n    return (auc_score, fpr_df, tpr_df)","b89cc90a":"# Fit primal logistic regression\nparam_grid = {'C': [0.1,0.5,1,10,50,100], 'max_iter': [250], 'fit_intercept':[True],'intercept_scaling':[1],\n              'penalty':['l2'], 'tol':[0.00001,0.0001,0.000001]}\nlog_primal_Grid = GridSearchCV(LogisticRegression(solver='lbfgs'),param_grid, cv=10, refit=True, verbose=0)\nlog_primal_Grid.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)\nbest_model(log_primal_Grid)\n%time","f3a5109d":"# Fit logistic regression with degree 2 polynomial kernel\nparam_grid = {'C': [0.1,10,50], 'max_iter': [300,500], 'fit_intercept':[True],'intercept_scaling':[1],'penalty':['l2'],\n              'tol':[0.0001,0.000001]}\npoly2 = PolynomialFeatures(degree=2)\ndf_train_pol2 = poly2.fit_transform(df_train.loc[:, df_train.columns != 'Exited'])\nlog_pol2_Grid = GridSearchCV(LogisticRegression(solver = 'liblinear'),param_grid, cv=5, refit=True, verbose=0)\nlog_pol2_Grid.fit(df_train_pol2,df_train.Exited)\nbest_model(log_pol2_Grid)\n%time","0bce341e":"# Fit SVM with RBF Kernel\nparam_grid = {'C': [0.5,100,150], 'gamma': [0.1,0.01,0.001],'probability':[True],'kernel': ['rbf']}\nSVM_grid = GridSearchCV(SVC(), param_grid, cv=3, refit=True, verbose=0)\nSVM_grid.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)\nbest_model(SVM_grid)\n%time","31e80cc1":"# Fit SVM with pol kernel\nparam_grid = {'C': [0.5,1,10,50,100], 'gamma': [0.1,0.01,0.001],'probability':[True],'kernel': ['poly'],'degree':[2,3] }\nSVM_grid = GridSearchCV(SVC(), param_grid, cv=3, refit=True, verbose=0)\nSVM_grid.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)\nbest_model(SVM_grid)\n%time","5dfc0bb8":"# Fit random forest classifier\nparam_grid = {'max_depth': [3, 5, 6, 7, 8], 'max_features': [2,4,6,7,8,9],'n_estimators':[50,100],'min_samples_split': [3, 5, 6, 7]}\nRanFor_grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, refit=True, verbose=0)\nRanFor_grid.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)\nbest_model(RanFor_grid)\n%time","8cd185fe":"# Fit Extreme Gradient boosting classifier\nparam_grid = {'max_depth': [5,6,7,8], 'gamma': [0.01,0.001,0.001],'min_child_weight':[1,5,10], 'learning_rate': [0.05,0.1, 0.2, 0.3], 'n_estimators':[5,10,20,100]}\nxgb_grid = GridSearchCV(XGBClassifier(), param_grid, cv=5, refit=True, verbose=0)\nxgb_grid.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)\nbest_model(xgb_grid)","c055e3f0":"# Fit AdaBoostClassifier\nparam_grid = {\"base_estimator\": [None], \"n_estimators\": [50,100,150], \"learning_rate\": [0.8, 0.9, 1.0], \"algorithm\": ['SAMME.R'], \"random_state\": [None]}\nada_grid = GridSearchCV(AdaBoostClassifier(), param_grid, cv=5,refit=True, verbose=0)\nada_grid.fit(df_train.iloc[:, df_train.columns != \"Exited\"], df_train.Exited)\nbest_model(ada_grid)\n%time","2aed8156":"# Fit primal logistic regression\nlog_primal = LogisticRegression(C=50, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=250, multi_class='warn',\n          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n          tol=1e-05, verbose=0, warm_start=False)\n\nlog_primal.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)","b70f86fa":"# Fit logistic regression with pol 2 kernel\npoly2 = PolynomialFeatures(degree=2)\ndf_train_pol2 = poly2.fit_transform(df_train.loc[:, df_train.columns != 'Exited'])\nlog_pol2 = LogisticRegression(C=50, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=300, multi_class='warn',\n          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n          tol=0.0001, verbose=0, warm_start=False)\n\nlog_pol2.fit(df_train_pol2,df_train.Exited)","70c9ae24":"# Fit SVM with RBF Kernel\nSVM_RBF = SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n  max_iter=-1, probability=True, random_state=None, shrinking=True,\n  tol=0.001, verbose=False)\n\nSVM_RBF.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)","e129a43a":"# Fit SVM with Pol Kernel\nSVM_POL = SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape='ovr', degree=2, gamma=0.1, kernel='poly',\n  max_iter=-1, probability=True, random_state=None, shrinking=True,\n  tol=0.001, verbose=False)\n\nSVM_POL.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)","a90bed86":"# Fit Random Forest classifier\nRF = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=8, max_features=9, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=6,\n            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\n\nRF.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)","048d15d9":"# Fit Extreme Gradient Boost Classifier\nXGB = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=1, gamma=0.01, learning_rate=0.1, max_delta_step=0,\n       max_depth=5, min_child_weight=10, missing=None, n_estimators=100,\n       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n       silent=True, subsample=1)\n\nXGB.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)","3ec5fd2e":"# Fit AdaBoostClassifier\nADA = AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n          learning_rate=0.8, n_estimators=50, random_state=None)\nADA.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)","5c6a2bfe":"print(classification_report(df_train.Exited, log_primal.predict(df_train.loc[:, df_train.columns != 'Exited'])))","6cf1c9f6":"print(classification_report(df_train.Exited,  log_pol2.predict(df_train_pol2)))","52a9cc8b":"print(classification_report(df_train.Exited,  SVM_RBF.predict(df_train.loc[:, df_train.columns != 'Exited'])))","ddf64d30":"print(classification_report(df_train.Exited,  SVM_POL.predict(df_train.loc[:, df_train.columns != 'Exited'])))","cdf314e9":"print(classification_report(df_train.Exited,  RF.predict(df_train.loc[:, df_train.columns != 'Exited'])))","f439f42e":"print(classification_report(df_train.Exited,  XGB.predict(df_train.loc[:, df_train.columns != 'Exited'])))","7628fdb4":"print(classification_report(df_train.Exited, ADA.predict(df_train.loc[:, df_train.columns != 'Exited'])))","4283341f":"y = df_train.Exited\nX = df_train.loc[:, df_train.columns != 'Exited']\nX_pol2 = df_train_pol2\nauc_log_primal, fpr_log_primal, tpr_log_primal = get_auc_scores(y, log_primal.predict(X),log_primal.predict_proba(X)[:,1])\nauc_log_pol2, fpr_log_pol2, tpr_log_pol2 = get_auc_scores(y, log_pol2.predict(X_pol2),log_pol2.predict_proba(X_pol2)[:,1])\nauc_SVM_RBF, fpr_SVM_RBF, tpr_SVM_RBF = get_auc_scores(y, SVM_RBF.predict(X),SVM_RBF.predict_proba(X)[:,1])\nauc_SVM_POL, fpr_SVM_POL, tpr_SVM_POL = get_auc_scores(y, SVM_POL.predict(X),SVM_POL.predict_proba(X)[:,1])\nauc_RF, fpr_RF, tpr_RF = get_auc_scores(y, RF.predict(X),RF.predict_proba(X)[:,1])\nauc_XGB, fpr_XGB, tpr_XGB = get_auc_scores(y, XGB.predict(X),XGB.predict_proba(X)[:,1])\nauc_ADA, fpr_ADA, tpr_ADA = get_auc_scores(y, ADA.predict(X),ADA.predict_proba(X)[:,1])","7b4e0f9f":"plt.figure(figsize = (12,6), linewidth= 1)\nplt.plot(fpr_log_primal, tpr_log_primal, label = 'log primal Score: ' + str(round(auc_log_primal, 5)))\nplt.plot(fpr_log_pol2, tpr_log_pol2, label = 'log pol2 score: ' + str(round(auc_log_pol2, 5)))\nplt.plot(fpr_SVM_RBF, tpr_SVM_RBF, label = 'SVM RBF Score: ' + str(round(auc_SVM_RBF, 5)))\nplt.plot(fpr_SVM_POL, tpr_SVM_POL, label = 'SVM POL Score: ' + str(round(auc_SVM_POL, 5)))\nplt.plot(fpr_RF, tpr_RF, label = 'RF score: ' + str(round(auc_RF, 5)))\nplt.plot(fpr_XGB, tpr_XGB, label = 'XGB score: ' + str(round(auc_XGB, 5)))\nplt.plot(fpr_ADA, tpr_ADA, label = 'ADA score: ' + str(round(auc_ADA, 5)))\n\nplt.plot([0,1], [0,1], 'k--', label = 'Random: 0.5')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC Curve')\nplt.legend(loc='best')\n#plt.savefig('roc_results_ratios.png')\nplt.show()","2c6456d4":"# Make the data transformation for test data\ndf_test = DfPrepPipeline(df_test,df_train.columns,minVec,maxVec)\ndf_test = df_test.mask(np.isinf(df_test))\ndf_test = df_test.dropna()\ndf_test.shape","8ddfd001":"# Ada Boost\nprint(classification_report(df_test.Exited,  ADA.predict(df_test.loc[:, df_test.columns != 'Exited'])))","9bd27003":"# Random Forest\nprint(classification_report(df_test.Exited,  RF.predict(df_test.loc[:, df_test.columns != 'Exited'])))","29de8291":"# Ada Boost\nauc_ADA_test, fpr_ADA_test, tpr_ADA_test = get_auc_scores(df_test.Exited, ADA.predict(df_test.loc[:, df_test.columns != 'Exited']),\n                                                       ADA.predict_proba(df_test.loc[:, df_test.columns != 'Exited'])[:,1])\nplt.figure(figsize = (12,6), linewidth= 1)\nplt.plot(fpr_ADA_test, tpr_ADA_test, label = 'ADA score: ' + str(round(auc_ADA_test, 5)))\nplt.plot([0,1], [0,1], 'k--', label = 'Random: 0.5')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC Curve')\nplt.legend(loc='best')\n#plt.savefig('roc_results_ratios.png')\nplt.show()","0f1544d0":"# Random Forest\nauc_RF_test, fpr_RF_test, tpr_RF_test = get_auc_scores(df_test.Exited, RF.predict(df_test.loc[:, df_test.columns != 'Exited']),\n                                                       RF.predict_proba(df_test.loc[:, df_test.columns != 'Exited'])[:,1])\nplt.figure(figsize = (12,6), linewidth= 1)\nplt.plot(fpr_RF_test, tpr_RF_test, label = 'RF score: ' + str(round(auc_RF_test, 5)))\nplt.plot([0,1], [0,1], 'k--', label = 'Random: 0.5')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC Curve')\nplt.legend(loc='best')\n#plt.savefig('roc_results_ratios.png')\nplt.show()","099579e1":"We aim to accomplist the following for this study:\n\n1. Identify and visualize which factors contribute to customer churn:\n    \n2. Build a prediction model that will perform the following:\n    * Classify if a customer is going to churn or not\n    * Preferably and based on model performance, choose a model that will attach a probability to the churn to make it easier for customer service to target low hanging fruits in their efforts to prevent churn","6c9cd8d1":" ## 1. Introduction","595a7312":"## !Warning. This section takes a loooooong time to run so you have the option to skip to the next section where I fit the better models from this section.","e0f6c48f":"### Review best model fit accuracy : Keen interest is on the performance in predicting 1's (Customers who churn)","d5e31c7f":"So we moslty have categorical variables and 5 continuous variables","fc624f21":"We note the following:\n* Majority of the data is from persons from France. However, the proportion of churned customers is with inversely related to the population of customers alluding to the bank possibly having a problem (maybe not enough customer service resources allocated) in the areas where it has fewer clients.\n* The proportion of female customers churning is also greater than that of male customers\n* Interestingly, majority of the customers that churned are those with credit cards. Given that majority of the customers have credit cards could prove this to be just a coincidence. \n* Unsurprisingly the inactive members have a greater churn. Worryingly is that the overall proportion of inactive mebers is quite high suggesting that the bank may need a program implemented to turn this group to active customers as this will definately have a positive impact on the customer churn.\n","057e88d8":"## 3. Exploratory Data Analysis\nHere our main interest is to get an understanding as to how the given attributes relate too the 'Exit' status.","2927d19f":"## 5. Data prep for model fitting","c55f1ca2":"### Fit best Models","5164bee7":"## 4. Feature engineering\nWe seek to add features that are likely to have an impact on the probability of churning. We first split the train and test sets","abdd7912":"### From the above results, my main aim is to predict the customers that will possibly churn so they can be put in some sort of scheme to prevent churn hence the recall measures on the 1's is of more importance to me than the overall accuracy score of the model.\n\n### Given that in the data we only had 20% of churn, a recall greater than this baseline will already be an improvement but we want to get as high as possible while trying to maintain a high precision so that the bank can train its resources effectively towards clients highlighted by the model without wasting too much resources on the false positives.\n\n### From the review of the fitted models above, the best model that gives a decent balance of the recall and precision is the random forest where according to the fit on the training set, with a precision score on 1's of 0.88, out of all customers that the model thinks will churn, 88% do actually churn and with the recall score of 0.53 on the 1's, the model is able to highlight 53% of all those who churned.\n\n### Nevertheless, I've decided to put ADA model here just for the represnation purposes.","41f1e1cc":"From the above, a couple of question linger:\n1. The data appears to be a snapshot as some point in time e.g. the balance is for a given date which leaves a lot of questions:\n    * What date is it and of what relevance is this date\n    * Would it be possible to obtain balances over a period of time as opposed to a single date.\n2. There are customers who have exited but still have a balance in their account! What would this mean? Could they have exited from a product and not the bank?\n3. What does being an active member mean and are there difference degrees to it? Could it be better to provide transaction count both in terms of credits and debits to the account instead?\n4. A break down to the products bought into by a customer could provide more information topping listing of product count\n\nFor this exercise, we proceed to model without context even though typically having context and better understanding of the data extraction process would give better insight and possibly lead to better and contextual results of the modelling process","8f746259":"## 7. Conclusion","97a037df":"Well isn't that a rare find; no missing values! ","a54e983b":"### Test model prediction accuracy on test data","222b34bb":"## 6. Model fitting and selection\nFor the model fitting, I will try out the following\n* Logistic regression in the primal space and with different kernels\n* SVM in the primal and with different Kernels\n* Ensemble models","9a2ba985":"So about 20% of the customers have churned. So the baseline model could be to predict that 20% of the customers will churn.\nGiven 20% is a small number,  we need to ensure that the chosen model does predict with great accuracy this 20% as it is of interest to the bank to identify and keep this bunch as opposed to accurately predicting the customers that are retained.","6056f71d":"From the above, we will not require the first 2 attributes as the are specific to a customer. It is borderline with the surname as this would result to profiling so we exclude this as well.","aa89a152":"we have seen that the salary has little effect on the chance of a customer churning. However as seen above, the ratio of the bank balance and the estimated salary indicates that customers with a higher balance salary ratio churn more which would be worrying to the bank as this impacts their source of loan capital.","2f258ad0":"The Df has 1000 rows with 14 attributes. We review this further to identify what attributes will be necessary and what data manipulation needs to be carried out before Exploratory analysis and prediction modelling","16f6f6c9":"We note the following:\n* There is no significant difference in the credit score distribution between retained and churned customers. \n* The older customers are churning at more than the younger ones alluding to a difference in service preference in the age categories. The bank may need to review their target market or review the strategy for retention between the different age groups\n* With regard to the tenure, the clients on either extreme end (spent little time with the bank or a lot of time with the bank) are more likely to churn compared to those that are of average tenure.\n*  Worryingly, the bank is losing customers with significant bank balances which is likely to hit their available capital for lending.\n* Neither the product nor the salary has a significant effect on the likelihood to churn.","8977fa07":"<img src=\"https:\/\/s16353.pcdn.co\/wp-content\/uploads\/2018\/06\/Churn.png\" style=\"float: left;\" width=\"400\" height=\"100\" \/>\n# PREVENTING BANK CUSTOMER CHURN!\n# \" Exploratory Data Analysis  & Prediction!\"\n\n# 2018\n\n[** by_Keldine Malit** ]","ca7f3792":"The precision of the model on previousy unseen test data is slightly higher with regard to predicting 1's i.e. those customers that churn. However, in as much as the model has a high accuracy, it still misses about half of those who end up churning. This could be imprved by providing retraining the model with more data over time while in the meantime working with the model to save the 41% that would have churned :-)","84119217":"## 2. Data set review & preparation\n\nIn this section we will seek to explore the structure of our data: \n1. To understand the input space the data set\n2. And to prepare the sets for exploratory and prediction tasks as described in section 1"}}