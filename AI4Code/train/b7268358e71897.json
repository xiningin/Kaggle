{"cell_type":{"b910a918":"code","d1ac9def":"code","34138ad4":"code","7ccdad56":"code","29114c9f":"code","a16f3350":"code","8535dac9":"code","5a62039e":"code","ef43e1f7":"code","97e0fad9":"code","c9283ff4":"markdown","4651a86b":"markdown","3a1eab2b":"markdown","56c59987":"markdown","4a01cb7b":"markdown","c0588717":"markdown","e87fd4fb":"markdown","0e66e063":"markdown","ea4e748a":"markdown"},"source":{"b910a918":"from tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport imagehash\nimport PIL\nimport os","d1ac9def":"class CFG():\n    \n    threshold = .9\n    img_size = 600\n    seed = 42","34138ad4":"#\u0111\u1ecdc \u1ea3nh v\u00e0o\nroot = '\/kaggle\/input\/plant-pathology-2021-fgvc8\/train_images'\n#l\u1ea5y path c\u1ee7a file jpg\npaths = os.listdir(root)\n\ndf = pd.read_csv('\/kaggle\/input\/plant-pathology-2021-fgvc8\/train.csv', index_col='image')\n\n#decode \u1ea3nh, chuy\u1ec3n \u1ea3nh sang d\u1ea1ng 600X600X3 (rgb), cast sang unit8\nfor path in tqdm(paths, total=len(paths)):\n    image = tf.io.read_file(os.path.join(root, path))\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, [CFG.img_size, CFG.img_size])\n    image = tf.cast(image, tf.uint8).numpy()\n    plt.imsave(path, image)","7ccdad56":"hash_functions = [#grey scale, down scale sang 8x8\n    imagehash.average_hash,#t\u00ednh trung b\u00ecnh c\u1ed9ng t\u1ea5t c\u1ea3 c\u00e1c \u0111i\u1ec3m \u1ea3nh r\u1ed3i so s\u00e1nh t\u1eebng \u0111i\u1ec3m \u1ea3nh, >tb ->1, <tb -> 0\n    imagehash.phash,#\n    imagehash.dhash,\n    imagehash.whash]\n#m\u1ed7i h\u00e0m hash \u0111\u01b0a ra t\u1eadp 64 bit 0,1\n\nimage_ids = []\nhashes = []#k\u1ebft qu\u1ea3 sau khi \u00e1p d\u1ee5ng 4 c\u00e1ch hash cho \u1ea3nh.\n\npaths = tf.io.gfile.glob('.\/*.jpg')\n\nfor path in tqdm(paths, total=len(paths)):\n\n    image = PIL.Image.open(path)\n\n    hashes.append(np.array([x(image).hash for x in hash_functions]).reshape(-1,))\n    image_ids.append(path.split('\/')[-1])#th\u00eam id \u1ea3nh\n    \nhashes = np.array(hashes)\nimage_ids = np.array(image_ids)","29114c9f":"#so s\u00e1nh c\u00e1c \u1ea3nh\nduplicate_ids = []\n# \u1ea3nh \u0111\u1ea7u: 110 ->ttf\n#so sanh \u1ea3nh a2: 100>tff\n#similarity: 101 -> mean(similarity) = 2\/3\nfor i in tqdm(range(len(hashes)), total=len(hashes)):\n    similarity = (hashes[i] == hashes).mean(axis=1)#m\u1ea3ng \u0111\u1ed9 t\u01b0\u01a1ng \u0111\u1ed3ng c\u1ee7a c\u00e1c \u0111i\u1ec3m \u1ea3nh\n    duplicate_ids.append(list(image_ids[similarity > CFG.threshold]))#n\u1ebfu \u0111\u1ed9 gi\u1ed1ng nhau > 0.9 th\u00ec coi l\u00e0 gi\u1ed1ng nhau\nduplicate_ids","a16f3350":"duplicates = [frozenset([x] + y) for x, y in zip(image_ids, duplicate_ids)]\nduplicates = set([x for x in duplicates if len(x) > 1])#do sau khi so s\u00e1nh duplicate_id c\u00f3 1 ph\u1ea7n t\u1eed l\u00e0 ch\u00ednh \u1ea3nh \u0111\u00f3\nlen(duplicates)","8535dac9":"#cho ra m\u1ed9t file csv \u0111\u1ec3 d\u00f9ng cho note book sau\nprint(f'Found {len(duplicates)} duplicate pairs:')\nfor row in duplicates:\n    print(', '.join(row))","5a62039e":"print('Writing duplicates to \"duplicates.csv\".')\nwith open('duplicates.csv', 'w') as file:\n    for row in duplicates:\n        file.write(','.join(row) + '\\n')","ef43e1f7":"for row in duplicates:\n    \n    figure, axes = plt.subplots(1, len(row), figsize=[5 * len(row), 5])\n\n    for i, image_id in enumerate(row):\n        image = plt.imread(os.path.join('..\/input\/plant-pathology-2021-fgvc8\/train_images', image_id))\n        axes[i].imshow(image)\n\n        axes[i].set_title(f'{image_id} - {df.loc[image_id, \"labels\"]}')\n        axes[i].axis('off')\n\n    plt.show()","97e0fad9":"for file in tf.io.gfile.glob('.\/*.jpg'):\n    os.remove(file)","c9283ff4":"### Summary\nNote book n\u00e0y \u0111\u01b0\u1ee3c sao ch\u00e9p v\u00e0 ch\u1ec9nh s\u1eeda l\u1ea1i t\u1eeb project c\u1ee7a [Nick Kuzmenkov](https:\/\/www.kaggle.com\/nickuzmenkov).\n\nVi\u1ec7c tr\u00f9ng l\u1eb7p \u1ea3nh g\u00e2y kh\u00f3 kh\u0103n cho vi\u1ec7c hu\u1ea5n luy\u1ec7n d\u1eef li\u1ec7u. \u1ea2nh tr\u00f9ng l\u1eb7p nh\u01b0ng c\u00f9ng nh\u00e3n g\u00e2y hi\u1ec7n t\u01b0\u01a1ng data leakage, trong khi \u1ea3nh tr\u00f9ng l\u1eb7p nh\u01b0ng kh\u00e1c nh\u00e3n g\u00e2y nhi\u1ec5u.\nNote book n\u00e0y s\u1eed d\u1ee5ng \u0111\u1ec3 t\u00ecm c\u00e1ch \u1ea3nh tr\u00f9ng l\u1eb7p. K\u1ebft qu\u1ea3 t\u00ecm \u0111\u01b0\u1ee3c 56 c\u1eb7p \u1ea3nh tr\u00f9ng l\u1eb7p.\n### C\u00e1c note book kh\u00e1c c\u1ee7a nh\u00f3m:\n\n1. [Revealing Duplicates notebook](https:\/\/www.kaggle.com\/nvlinhh\/int3414-22-n11-revealing-duplicate)\n2. [Preprocessing notebook](https:\/\/www.kaggle.com\/congnguyen8201\/int3414-22-n11-preprocessing)\n3. [Training notebook](https:\/\/www.kaggle.com\/congnguyen8201\/int3414-22-n11-training)","4651a86b":"## 1. Saving downscaled images to boost performance\nComputing hash over original images of very high quality would take nearly 5 hours, thus we downscaling first.","3a1eab2b":"### Imports","56c59987":"### Clear working folder to avoid output pollution","4a01cb7b":"### Acknowledgements\n\n* This work is Copy&Edit form @appian **[notebook](https:\/\/www.kaggle.com\/appian\/let-s-find-out-duplicate-images-with-imagehash)** with a lot of changes, but still highly inspired. If you find this notebook useful, please, upvote his work too.\n* Thanks to @kingofarmy for spotting more duplicates in **[his thread](https:\/\/www.kaggle.com\/c\/plant-pathology-2021-fgvc8\/discussion\/229851)**.","c0588717":"## 4. Let's see what is found","e87fd4fb":"Here we add some of the duplicates spotted by @kingofarmy in the corresponding **[discussion](https:\/\/www.kaggle.com\/c\/plant-pathology-2021-fgvc8\/discussion\/229851)**:","0e66e063":"## 3. Run search across hashed images\nWe firstly compare each image hash with all the hashes and then leave only unique pairs of matches","ea4e748a":"## 2. Hash computation"}}