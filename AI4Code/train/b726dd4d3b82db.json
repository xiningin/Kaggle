{"cell_type":{"c8c84e72":"code","9be9b092":"code","59c19157":"code","bc7b3037":"code","ac8155cc":"code","03df66b6":"code","6ad2c62a":"code","7c090b58":"code","707668e1":"code","de394761":"code","096dfeda":"code","0e940ecc":"code","efbce633":"code","c261fae6":"code","57293648":"code","e7149de0":"code","eaf74f2b":"code","18ff9224":"code","f5548964":"code","1aa4d904":"code","3a15bffa":"code","13eb5e93":"code","7766fec7":"code","4aa98008":"code","63ffc31e":"markdown","e34da5a4":"markdown","cd58fa33":"markdown","c9ad9c60":"markdown","5a3ac3ab":"markdown","5c13a342":"markdown","1a3f04d0":"markdown","59db3bdc":"markdown","63f03323":"markdown","72e7eae5":"markdown","f1491b31":"markdown","7bed3e32":"markdown","d9ddf84b":"markdown","34258f34":"markdown","db2d0528":"markdown","52d3650d":"markdown","3209decd":"markdown","4d40cf4c":"markdown","39895050":"markdown","d7726c8a":"markdown","aa00e904":"markdown","5b667402":"markdown","8906071b":"markdown","38ceb7ed":"markdown","01d49e2b":"markdown"},"source":{"c8c84e72":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9be9b092":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","59c19157":"file = '..\/input\/BlackFriday.csv'\ndf = pd.read_csv(file)","bc7b3037":"df.shape","ac8155cc":"df.head(10)","03df66b6":"df.describe()","6ad2c62a":"df.isnull().sum()","7c090b58":"age = df['Age'].value_counts()\noccupation = df['Occupation'].value_counts()\ngender = df['Gender'].value_counts()\n\nlabels_1 = df['Age'].unique()\nlabels_2 = df['Occupation'].unique()\nlabels_3 = df['Gender'].unique()\n\nf,(ax1,ax2,ax3) = plt.subplots(3,1,figsize=(12,40))\nax1.pie(age,labels=labels_1,autopct='%1.1f%%')\nax1.set_title('Age Breakdown')\nax2.pie(occupation,labels=labels_2,autopct='%1.1f%%')\nax2.set_title('Occupation Breakdown')\nax3.pie(gender,labels=labels_3,autopct='%1.1f%%')\nax3.set_title('Gender Breakdown')\nplt.show()","707668e1":"# Purchase distribution\nfrom scipy.stats import norm\nprice = df.groupby('User_ID')['Purchase'].agg('sum')\nplt.figure(figsize=(10,10))\nsns.distplot(price)\nplt.title('Mean Purchase Price Distribution')\nplt.show()","de394761":"# Marital Status Ratio\ndf['Marital_Status'].unique()\ndf['Marital_Status'].value_counts()\n#Ratio per city\nplt.figure(figsize=(10,8))\nsns.countplot(x='City_Category',hue='Marital_Status',data=df)\nplt.show()","096dfeda":"df[['Product_Category_2','Product_Category_3']] = df[['Product_Category_2','Product_Category_3']].fillna(0).astype(int) \nprint('Category 1:',sorted(df['Product_Category_1'].unique()))\nprint('Category 2:',sorted(df['Product_Category_2'].unique()))\nprint('Category 3:',sorted(df['Product_Category_3'].unique()))","0e940ecc":"f, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(18,10))\nsns.countplot(x='Product_Category_1',data=df,ax=ax1,color='r')\nsns.countplot(x='Product_Category_2',data=df,ax=ax2,color='b')\nsns.countplot(x='Product_Category_3',data=df,ax=ax3,color='g')\nplt.show()\n","efbce633":"# Let set a function for this\ndef age_group(age):\n    if (age == '0-17') or (age=='18-25') or (age == '26-35'):\n        return 'Young'\n    if (age == '36-45') or (age=='46-50') or (age == '51-55'):\n        return 'Middle'\n    else:\n        return 'Old'\n\ndf['Age_group'] = df['Age'].apply(age_group)\n\n# Young population product study\nf, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(20,8))\nyoung_1 = df[df.Age_group == 'Young']['Product_Category_1'].value_counts()[:3].plot(kind='bar',ax=ax1)\nax1.set_xlabel('Product 1 Sub-Category')\nax1.set_ylabel('Frequency')\nax1.set_title('Young Population')\nyoung_2 = df[df.Age_group == 'Young']['Product_Category_2'].value_counts()[:3].plot(kind='bar',ax=ax2)\nax2.set_xlabel('Product 2 Sub-Category')\nax2.set_ylabel('Frequency')\nax2.set_title('Young Population')\nyoung_3 = df[df.Age_group == 'Young']['Product_Category_3'].value_counts()[:3].plot(kind='bar',ax=ax3)\nax3.set_xlabel('Product 3 Sub-Category')\nax3.set_ylabel('Frequency')\nax3.set_title('Young Population')\n\nf, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(20,8))\nmiddle_1 = df[df.Age_group == 'Middle']['Product_Category_1'].value_counts()[:3].plot(kind='bar',ax=ax1)\nax1.set_xlabel('Product 1 Sub-Category')\nax1.set_ylabel('Frequency')\nax1.set_title('Middle Aged Population')\nmiddle_2 = df[df.Age_group == 'Middle']['Product_Category_2'].value_counts()[:3].plot(kind='bar',ax=ax2)\nax2.set_xlabel('Product 2 Sub-Category')\nax2.set_ylabel('Frequency')\nax2.set_title('Middle Aged Population')\nmiddle_3 = df[df.Age_group == 'Middle']['Product_Category_3'].value_counts()[:3].plot(kind='bar',ax=ax3)\nax3.set_xlabel('Product 3 Sub-Category')\nax3.set_ylabel('Frequency')\nax3.set_title('Middle Aged Population')\n\nf, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(20,8))\nold_1 = df[df.Age_group == 'Old']['Product_Category_1'].value_counts()[:3].plot(kind='bar',ax=ax1)\nax1.set_xlabel('Product 1 Sub-Category')\nax1.set_ylabel('Frequency')\nax1.set_title('Old Age Population')\nold_2 = df[df.Age_group == 'Old']['Product_Category_2'].value_counts()[:3].plot(kind='bar',ax=ax2)\nax2.set_xlabel('Product 1 Sub-Category')\nax2.set_ylabel('Frequency')\nax2.set_title('Old Age Population')\nold_3 = df[df.Age_group == 'Old']['Product_Category_3'].value_counts()[:3].plot(kind='bar',ax=ax3)\nax3.set_xlabel('Product 1 Sub-Category')\nax3.set_ylabel('Frequency')\nax3.set_title('Old Aged Population')\n\nplt.show()","c261fae6":"# Correlation matrix between features.\ncorr = df[['Gender','Age','Occupation','City_Category','Stay_In_Current_City_Years','Marital_Status','Product_Category_1','Product_Category_2','Product_Category_3','Purchase']].corr()\nsns.heatmap(corr, xticklabels=corr.columns.values,yticklabels=corr.columns.values, annot=True)\nplt.show()","57293648":"from sklearn.preprocessing import LabelEncoder\nlabel_enc = LabelEncoder()\nproduct_enc = LabelEncoder()\ndf['User_ID'] = label_enc.fit_transform(df.User_ID)\ndf['Product_ID'] = product_enc.fit_transform(df.Product_ID)\n\n# One Hot Encoding Age, Stay in Current City Years, City_Category\ndf_Age = pd.get_dummies(df.Age)\ndf_city = pd.get_dummies(df.City_Category)\ndf_staycity = pd.get_dummies(df.Stay_In_Current_City_Years)\ndf_Gender = pd.Series(np.where(df.Gender == 'M',1,0), name='Gender')\ndf_Agegroup = pd.get_dummies(df.Age_group)\n\ndf_new = pd.concat([df,df_Gender,df.Age_group, df_Age, df_city, df_staycity], axis=1)\ndf_new.drop(['Age','Gender','City_Category','Age_group','Stay_In_Current_City_Years'], axis=1, inplace=True)\ndf_new = df_new.rename(columns={'0':'Gender'})","e7149de0":"# let's sample only half the df_new data\ndf_sample = df_new.sample(frac=0.05, random_state=100)\nX2 = df_sample.drop(['Purchase'], axis=1)\ny2 = df_sample.Purchase","eaf74f2b":"# Linear Model\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\nmod = LinearRegression()\n\nscoring = 'neg_mean_squared_error'\nlinear_cv = cross_val_score(mod, X2,y2, cv=5, scoring=scoring)\nprint((-1*linear_cv.mean())**0.5)","18ff9224":"import warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import learning_curve\ntrain_sizes, train_scores, valid_scores = learning_curve(mod, X2, y2, cv=3, scoring='neg_mean_squared_error')\n\ntrain_scores = (-1*train_scores)**0.5\nvalid_scores = (-1*valid_scores)**0.5\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\nvalid_scores_mean = np.mean(valid_scores, axis=1)\nvalid_scores_std = np.std(valid_scores, axis=1)\n\nplt.figure()\nplt.plot(train_sizes,valid_scores_mean,label='valid')\nplt.plot(train_sizes,train_scores_mean,label='train')\nplt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.3,color=\"g\")\nplt.fill_between(train_sizes, valid_scores_mean - valid_scores_std,valid_scores_mean + valid_scores_std, alpha=0.3, color=\"b\")\nplt.xlabel('Number of samples')\nplt.ylabel('RMSE')\nplt.legend()\nplt.show()","f5548964":"# Random Forest Regressor \nfrom sklearn.ensemble import RandomForestRegressor\nmod = RandomForestRegressor()\nscoring = 'neg_mean_squared_error'\nRF_cv = cross_val_score(mod, X2,y2, cv=5, scoring=scoring)\nprint((-1*RF_cv.mean())**0.5)","1aa4d904":"# Random Forest Regressor Learning Curve\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import learning_curve\ntrain_sizes, train_scores, valid_scores = learning_curve(RandomForestRegressor(), X2, y2, cv=3, scoring='neg_mean_squared_error')\n\ntrain_scores = (-1*train_scores)**0.5\nvalid_scores = (-1*valid_scores)**0.5\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\nvalid_scores_mean = np.mean(valid_scores, axis=1)\nvalid_scores_std = np.std(valid_scores, axis=1)\n\nplt.figure()\nplt.plot(train_sizes,valid_scores_mean,label='valid')\nplt.plot(train_sizes,train_scores_mean,label='train')\nplt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.3,color=\"g\")\nplt.fill_between(train_sizes, valid_scores_mean - valid_scores_std,valid_scores_mean + valid_scores_std, alpha=0.3, color=\"b\")\nplt.xlabel('Number of samples')\nplt.ylabel('RMSE')\nplt.legend()\nplt.show()","3a15bffa":"rf = RandomForestRegressor(n_estimators=100).fit(X2,y2)\nf_im = rf.feature_importances_.round(3)\nser_rank = pd.Series(f_im,index=X2.columns).sort_values(ascending=False)\nplt.figure()\nsns.barplot(y=ser_rank.index,x=ser_rank.values,palette='deep')\nplt.xlabel('relative importance')","13eb5e93":"X2 = df_sample.drop(['User_ID','Product_ID','Purchase'], axis=1)\ny2 = df_sample.Purchase\n\n# Random Forest Regressor \nfrom sklearn.ensemble import RandomForestRegressor\nmod = RandomForestRegressor()\nscoring = 'neg_mean_squared_error'\nRF_cv = cross_val_score(mod, X2,y2, cv=5, scoring=scoring)\nprint((-1*RF_cv.mean())**0.5)","7766fec7":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nX_train, X_test, y_train, y_test = train_test_split(X2, y2, test_size=0.20, random_state=123)\nxg_reg = xgb.XGBRegressor()\nxg_reg.fit(X_train,y_train)\npreds = xg_reg.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test, preds))\nprint(\"RMSE: %f\" % (rmse))","4aa98008":"DM_train = xgb.DMatrix(X_train,y_train)\nDM_test =  xgb.DMatrix(X_test,y_test)\nparams = {\"booster\":\"gblinear\", \"objective\":\"reg:linear\"}\nxg_reg = xgb.train(params = params, dtrain=DM_train, num_boost_round=5)\npreds = xg_reg.predict(DM_test)\nrmse = np.sqrt(mean_squared_error(y_test,preds))\nprint(\"RMSE: %f\" % (rmse))","63ffc31e":"# Age, Occupation, Gender Breakdowns\n","e34da5a4":"# Gradient Boosting (XGBOOST)\n\nNow, we'll try another ensemble tree learning method known as Gradient Boosting. XGBoost has two types of learners, linear based learner and a tree based learner. We will try both and see which one performs better. The default learner is the tree-based learner which I will use in conjunction with train_test_split and evaluate the error of our model.","cd58fa33":"Product Category 1 looks like the most important feature in helping shape how much a product costs! That followed by the actual user and product ID. However, since we know that the user doesn't really purchase based on their own user ID and product ID, these 2 variables may be causing an over-fitting issue. It's most likely that user ID and product ID is random amongst the population of buyers and there probably isn't a clear trend. Let's remove these two variables and see if the model performs better.","c9ad9c60":"From this graph, I noticed that as the model fits more and more training data, the rmse goes up (which it naturally should as there will always be a residual error between actual and predicted value based on training data). However, when studying the validation test-set, it doesn't seem like the model is doing a good job at predicting the output. This seems like there is a **high bias**, where the model isn't complex enough to fit the training data and predict test data. We need a more complex model - one that has a low bias and low variance. Variance is the model's ability to see the generalized trend without overfitting on the training-data. Let's try a Random Forest Regressor, which is known to be a complex model but one that doesn't overfit.","5a3ac3ab":"Let's first import all the necessary packages\n","5c13a342":"It looks like the only meaningful column we can study from this data-set is the Purchase column. The mean purchase price was 9333 with a standard deviation of approximately 4981. It looks like there could be a few outliers with the max going up to 23,961. We will need to keep this mind when creating our machine learning algorithms because outliers can affect the accuracy of our model. ","1a3f04d0":"From the looks of it, the model seems to be **over-fitting** where the training score is way lower than the test score (which means there is high variance in the model and we need to generalize it further). We need to generalize the model a little more. Let's study the feature importances of the random forest regressor and see if we can eliminate any features. ","59db3bdc":"It looks like in product category 1, sub-category 1, 5 and 8 were the most popular in the story. In sub-category 2, 0 was the largest but because we populated NaNs with 0, this just means there were a lot of unknown product 2 sub-categories under product category 2. The only known popular sub-categories were 2, 8 and 14. The same process applies for Product Category 3 where there were a lot of unknowns populated as 0 there as well. It looks like product category 1 will probably be the more useful column in helping predicting purchase price as we have a lot of known information on this. \n\nI'm curious to see if Age plays a crucial role in how popular product sub-categories are. I would think that people who are relatively young vs. old will have different preferences in what they buy, and that will accordingly affect prices. ","63f03323":"We can see from the first graph that the majority of people in the store were between the ages 0 and 17. There was also a significant portion of people 55+ and between 26-35. Taking ages 35 and below to be within the 'Young' Age-group, we can see that they make a total of 60% of the population. It looks like this store definitely has stuff that caters to the young population, so we probably expect to see a correlation between age and spending. In terms of occupation, jobs 16,15 & 10 were the top 3. Studying gender, we can see that the overwhelming gender is Male. ","72e7eae5":"# XGBOOST (Linear base learner)","f1491b31":"# Correlation Matrix\n\nLet's study the correlation matrix between the different features and see if we can find a high correlation between variables. The correlation function will automatically find the correlation between numerical columns which is pretty convenient!","7bed3e32":"It looks like we should stick with our tree-based learner and xgboost performs a little better than Random Forest. Now we can try optimizing this model and also analyze whether the algorithm is being over-fitted or not. My next steps in optimizing this XGBoost would be:\n\n- try different learning rates \n- try varying the sample of data and number of feature columns used to train XGBoost decision trees\n","d9ddf84b":"Looks like the error went down a little, but it didn't change that much. Maybe we can try another ensemble learning known as XGBOOST. ","34258f34":"# Purchase Price Distribution\n\nHere, we want to study every the spending distribution of every user. Because each user may be multiple items, we will want to get the total spending price of each user at the store before plotting the distribution. From the graph below, we can see that the distribution is very left skewed with a few users spending largely (tail) as possible outliers. ","db2d0528":"# Age vs. Product Category\n\nSince we previously saw how there was a huge young population in and out of this store, I'm curious to see if there is a trend in the types of products bought vs. their age (after all, there's got to be something these ages like in this store to come shop in it). I'm going to do a little feature engineering first by setting some group labels based on age ranges:\n\n- [**0-17, 18-25, 26-35] : Young**\n- [**36-45, 46-50, 51-55] : Middle-Aged**\n- [**51+] : Old Age**","52d3650d":"# Marital Status\n\nLet's study the amount of people married vs. not married and study this for each city category. We can see that in each city, the number of unmarried people were higher than married people at this store, with city C having the most number of unmarried and married people overall. This is obviously absolute values and so it doesn't make sense to compare these numbers when the total number of customers from each city is different. ","3209decd":"# Ending Note\n\nThat's the best I could do as far as algorithm tuning to get the error down. It just might be that we need more info on the product sub-categories of products 2 & 3 in order to develop better models. If anyone has suggestions for me on how I can better my EDA and modelling, I would highly appreciate it! But I hope I was able to provide some insight and help a few of you kagglers out there. I'm still pretty new to kaggle but I hope to improve on this analysis in the future with your help! \n\nStay Tuned!\n\nCheers. ","4d40cf4c":"A few things I can take-away from this correlation matrix:\n\n- There seems to be a relatively strong** negative correlation** between **product category 1 and purchase price**. \n- There also seems to be a relatively **positive correlation** between** product category 3 and purchase price** \n- There is a **negative correlation** between product category 1 and product category 3** (we should watch out for this as it could possibly over-fit our machine learning model). ","39895050":"Wow, it looks like there doesn't seem to be a huge correlation between age group and the categories of products bought, except for a few differences seen. I expected to see a difference in product categories from Young & middle but it looks like the top 3 product sub-categories for product category 1 & 2 were the same. For the old age, looks like product category 8 topped the charts. It looks like age-group probably won't be highly correlated with purchase price since we see that product categories are similar. ","d7726c8a":"# A Brief Overview \n\nHere, we will start off with studying some basics of the data. We will look at some of the data structures of the data and its formats. We will then study some descriptive statistics on the columns. It's also important to check for null values to see if we can drop\/correct it before developing applying machine learning models. ","aa00e904":"# Product Category Study\n\nIt looks like each product falls into 3 main categories. In each category, there are various sub-categories. Let's:\n\n1. Fill all NaN with 0, and convert float columns to int\n2. Study the sub-category ranges for each product category\n3. Let's study the count plot of the different categories","5b667402":"# Black Friday EDA (with a little Machine Learning towards the end)","8906071b":"# Table of Contents\n\n1. Brief Overview \n2. Age, Gender, Occupation Breakdown\n3. Purchase Price Distribution\n4. Marital Status Breakdown\n5. Product Category Study\n6. Age vs. Product Category Study\n7. Correlations Matrix\n8. Machine Learning Modelling ","38ceb7ed":"# Machine Learning Models\n\nMachine learning models always work better with numerical values instead of categorical values, so we can help code these categories into unique numerical identifiers using the pandas dummy function (pd.get_dummies()). This will encode all the different labels into numerical vectors.  \n\n\nHere, I'm to try and follow this procedure outlined below to pave way to the right model:\n\n-  Pre-process the data using pd.get_dummies() and LabelEncoder(). I use label encoder on specific columns that have too many unique categories because using dummies will make the data too large. \n2. Split the data between input features and the output variable which is the purchase price\n3. Run linear regression, random forest regressor and XGboost regressor","01d49e2b":"It looks like we have reduced our mean squared errror from around 4000 to around 3000, a positive sign! Let's have a look at the learning curve performance of the Random Forest Regressor, to see if the model has a high variance or bias. "}}