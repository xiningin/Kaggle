{"cell_type":{"0450afbb":"code","47e1903d":"code","095899b4":"code","3f2d0a8c":"code","19af5800":"code","3e1715af":"code","1866d92f":"code","179c01cd":"code","5637754f":"code","de9db7e5":"code","94576266":"code","7f792f82":"code","d2717317":"code","73e5ff15":"code","0ac205bd":"code","6fb1e200":"code","27d750ad":"code","17e28e7b":"code","becb79b9":"code","584e6aed":"code","51664cac":"code","9baf7c7d":"code","1718cfde":"code","1505c1df":"code","c00b74ed":"code","086e45ae":"code","b0b8cb34":"code","dad154b8":"code","0bc94cc4":"code","bb652dca":"code","f51e632e":"markdown","fbfe57b2":"markdown","a7079031":"markdown","67be8ac9":"markdown","b183b8d1":"markdown","dea864c2":"markdown"},"source":{"0450afbb":"!pip install ..\/input\/sacremoses\/sacremoses-master\/ > \/dev\/null\n\nimport os\nimport sys\nimport glob\nimport torch\n\nsys.path.insert(0, \"..\/input\/transformers\/transformers-master\/\")\nimport transformers\nfrom transformers import *","47e1903d":"import sys\npackage_dir_a = \"..\/input\/ppbert\/pytorch-pretrained-bert\/pytorch-pretrained-BERT\"\nsys.path.insert(0, package_dir_a)\n\nfrom pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch","095899b4":"\nfrom transformers import *\nimport shutil\n# Translate model from tensorflow to pytorch\nWORK_DIR = \"..\/working\/\"\nBERT_MODEL_PATH ='..\/input\/bert-roberta\/'\n# BERT_MODEL_PATH ='..\/input\/roberta-large\/'\nconvert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n    BERT_MODEL_PATH + 'bert_model.ckpt',\nBERT_MODEL_PATH + 'bert_config.json',\nWORK_DIR + 'pytorch_model.bin')\n\nshutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'config.json')\n\nprint('yes')","3f2d0a8c":"import pandas as pd\nimport numpy as np\nimport json\nimport random\nimport copy\nimport re\nfrom tqdm import tqdm\nimport collections\nfrom random import shuffle\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ntqdm.pandas()\n\n#import torch.utils.data as data\nfrom torchvision import datasets, models, transforms\nfrom transformers import *\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import f1_score\nimport random\n\n!pip install pytorch-crf","19af5800":"class Do_data():\n    def __init__(self,doc_id,content,enents,key_word=None,content_split_off=0):\n        self.doc_id=doc_id\n        self.content=content\n        self.enents=enents\n        self.key_word=key_word\n        self.content_split_off=content_split_off\n    def __str__(self):\n        return self.__repr__()\n    def __repr__(self):\n        s = \"\"\n        s += \"doc_id: %s\\n\" % (str(self.doc_id))\n        s += \"content: %s\\n\" % (str(self.content))\n        s += \"enents: %s\\n\" % (str(self.enents))\n        s += \"key_word: %s\\n\" % (str(self.key_word))\n        s += \"content_split_off: %s\\n\" % (str(self.key_word))\n        return s\n\n\ndef get_shama(path):\n    shama=collections.defaultdict(list)\n    with open(path) as f:\n        for l in f:\n            l = json.loads(l)\n            event_list=[] #\u4e8b\u4ef6\u96c6\u5408\n            content=l['content']\n            docid=l['doc_id']\n            enents=[]\n            for event in l['events']:\n                typ=event['event_type']\n                v=list(event.keys())\n                v.remove('event_type')\n                v.remove('event_id')\n                shama[typ].extend(v)\n                shama[typ]=list(set(shama[typ]))\n    return shama\n\ndef load_data(path,is_test=False):\n    with open(path) as f:\n        D=[]\n        for l in f:\n            l = json.loads(l)\n            event_list=[] #\u4e8b\u4ef6\u96c6\u5408\n            content=l['content']\n            docid=l['doc_id']\n            enents=[]\n            if not is_test:\n                for event in l['events']:\n                    event_dict={}\n                    roles=shama[event['event_type']]\n                    for role in roles:\n                        if role in event:\n                            event_dict[role]=event[role]\n                    enents.append([event['event_type'],event_dict])\n            data=Do_data(docid,content,enents)\n#             print(data)\n            D.append(data)\n    return D\ndata_path='..\/input\/ccks42\/'\nshama=get_shama(data_path+'event_element_train_data_label.txt')\ntrain_data=load_data(data_path+'event_element_train_data_label.txt')\ntest_data=load_data(data_path+'event_element_dev_data.txt',is_test=True)\ns={}\nfor t in train_data:\n    if t.enents[0][0] not in s:\n        s[t.enents[0][0]]=0\n    s[t.enents[0][0]]+=1\n    if t.enents[0][0] in ['\u80a1\u4e1c\u51cf\u6301', '\u80a1\u4e1c\u589e\u6301', '\u80a1\u6743\u51bb\u7ed3', '\u80a1\u6743\u8d28\u62bc']:\n        train_data.remove(t)\nprint(s)\nprint(len(train_data))\nprint(len(test_data))","3e1715af":"shama_single={}\ni=0\nfor k,v in shama.items():\n    if k not in ['\u80a1\u4e1c\u51cf\u6301', '\u80a1\u4e1c\u589e\u6301', '\u80a1\u6743\u51bb\u7ed3', '\u80a1\u6743\u8d28\u62bc']:\n        for vi in v:\n            if k+'-'+vi not in shama_single:\n                shama_single[k+'-'+vi]=i\n                i+=1\nshama_single","1866d92f":"shama","179c01cd":"import pickle\nval_recall_data={}\ntest_recall_data_all=collections.defaultdict(list)\nfor i in range(1,6):\n    with open('..\/input\/recall-new\/val_preds{}.pkl'.format(i),'rb') as f:\n        d=pickle.load(f)\n        for k,v in d.items():\n            val_recall_data[k]=v\n    with open('..\/input\/recall-new\/test_preds{}.pkl'.format(i),'rb') as f:\n        d=pickle.load(f)\n        for k,v in d.items():\n            test_recall_data_all[k].append(v)\n            \ntest_recall_data={}\nfor k,v in test_recall_data_all.items():\n    v=collections.Counter(v).most_common(1)[0][0]\n    test_recall_data[k]=v\ntest_recall_data\n","5637754f":"MAX_OR_TEXT_LEN=-1\nMAX_TEXT_LEN = 485\nMAX_QUESTION_LEN=15\nMAX_LEN=512\nSEP_TOKEN_ID = 102\nDEVICE = 'cuda'\n\n\n\n\ndef seed_everything(seed):\n    \"\"\"\n    Seeds basic parameters for reproductibility of results\n    \n    \n    Arguments:\n        seed {int} -- Number of the seed\n    \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\nSEED=2020\nseed_everything(SEED)\n","de9db7e5":"import unicodedata\ndef _is_control(ch):\n    \"\"\"\u63a7\u5236\u7c7b\u5b57\u7b26\u5224\u65ad\n    \"\"\"\n    return unicodedata.category(ch) in ('Cc', 'Cf')\n\ndef _is_special(ch):\n    \"\"\"\u5224\u65ad\u662f\u4e0d\u662f\u6709\u7279\u6b8a\u542b\u4e49\u7684\u7b26\u53f7\n    \"\"\"\n    return bool(ch) and (ch[0] == '[') and (ch[-1] == ']')\n\ndef stem(token):\n    \"\"\"\u83b7\u53d6token\u7684\u201c\u8bcd\u5e72\u201d\uff08\u5982\u679c\u662f##\u5f00\u5934\uff0c\u5219\u81ea\u52a8\u53bb\u6389##\uff09\n    \"\"\"\n    if token[:2] == '##':\n        return token[2:]\n    else:\n        return token\ndef rematch(text,tokens,_do_lower_case=True):\n    '''\n    \u8fd4\u56de\u7684\u662ftoken\u540e\u6807\u7b7e\u4e0e\u539f\u59cb\u7684\u6620\u5c04\n    '''\n    normalized_text, char_mapping = '', []\n    #\u89c4\u8303\u5316\u6837\u672c\n    for i, ch in enumerate(text):\n        if _do_lower_case:\n            ch = unicodedata.normalize('NFD', ch)\n            ch = ''.join([c for c in ch if unicodedata.category(c) != 'Mn'])\n            ch = ch.lower()\n        ch = ''.join([\n                c for c in ch\n                if not (ord(c) == 0 or ord(c) == 0xfffd or _is_control(c))\n            ])\n        normalized_text += ch\n        char_mapping.extend([i] * len(ch))\n\n    text, token_mapping, offset = normalized_text, [], 0\n    for i,token in enumerate(tokens):\n        if _is_special(token):\n            token_mapping.append([offset])\n            offset+=1\n        else:\n            token = stem(token)\n            #none\u8868\u793a\u6709\u9519\n            if text[offset:].find(token)==-1:\n                return None\n            start = text[offset:].index(token) + offset\n            end = start + len(token)\n            token_mapping.append(char_mapping[start:end])\n            offset = end\n\n    return token_mapping\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None)\ntext='\u96c0\u5de2\u88c1\u54584000\u4eba\uff1a\u65f6\u4ee3\u629b\u5f03\u4f60\u65f6\uff0c\u8fde\u62db\u547c\u90fd\u4e0d\u4f1a\u6253\uff01'\ntokens=tokenizer.tokenize(text)\nprint(tokens)\nmapping=rematch(text,tokens)","94576266":"def search(pattern, sequence):\n    \"\"\"\u4ecesequence\u4e2d\u5bfb\u627e\u5b50\u4e32pattern\n    \u5982\u679c\u627e\u5230\uff0c\u8fd4\u56de\u7b2c\u4e00\u4e2a\u4e0b\u6807\uff1b\u5426\u5219\u8fd4\u56de-1\u3002\n    \"\"\"\n    n = len(pattern)\n    for i in range(len(sequence)):\n        if sequence[i:i + n] == pattern:\n            return i\n    return -1\n\ndef search_list(pattern, sequence):\n    \"\"\"\u4ecesequence\u4e2d\u5bfb\u627e\u5b50\u4e32pattern\n    \u5982\u679c\u627e\u5230\uff0c\u8fd4\u56de\u7b2c\u4e00\u4e2a\u4e0b\u6807\uff1b\u5426\u5219\u8fd4\u56de-1\u3002\n    \"\"\"\n    n = len(pattern)\n    ans=[]\n    for i in range(len(sequence)):\n        if sequence[i:i + n] == pattern:\n            ans.append(i)\n    return ans\n\nclass Feature(object):\n    def __init__(self,item_id,or_text,mapping,mapping_off,event_type_role,token_ids,question,labels,enenttype_enent):\n        self.item_id=item_id\n        self.or_text=or_text\n        self.mapping=mapping\n        self.mapping_off=mapping_off\n        self.event_type_role=event_type_role\n        self.token_ids=token_ids\n        self.question=question\n        self.labels=labels\n        self.enenttype_enent=enenttype_enent\n    def __str__(self):\n        return self.__repr__()\n\n    def __repr__(self):\n        s = \"\"\n        s += \"id: %s\\n\" % (str(self.item_id))\n        s += \"or_text: %s\\n\" % (str(self.or_text))\n        s += \"mapping: %s\\n\" % (str(self.mapping))\n        s += \"mapping_off: %s\\n\" % (str(self.mapping_off))\n        s += \"event_type_role: %s\\n\" % (str(self.event_type_role)) \n        s += \"token_ids: %s\\n\" % (str(self.token_ids)) \n        s += \"question: %s\\n\" % (str(self.question))\n        s += \"labels: %s\\n\" % (str(self.labels))\n        s += \"enenttype_enent: %s\\n\" % (str(self.enenttype_enent))\n        return s\n\nclass zy_DataSet(torch.utils.data.Dataset):\n    def __init__(self, data_list,train_mode=False, val_mode=False,test_mode=False,labeled=True,recall=False):\n        self.tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None)\n        self.train_mode = train_mode\n        self.val_mode=val_mode\n        self.test_mode=test_mode\n        self.labeled = labeled\n        self.recall=recall\n        if recall and self.val_mode:\n            print(recall)\n            self.features =self.get_val_recall_features(data_list)\n        elif recall and self.test_mode:\n            self.features =self.get_test_recall_features(data_list)\n        elif self.train_mode:\n            self.features =self.get_train_features(data_list)\n        elif self.val_mode:\n            self.features =self.get_train_features(data_list,train_mode=False)\n        elif self.test_mode:\n            self.features =self.get_train_features(data_list)\n        else:\n            print('no features !!!')\n    def get_test_recall_features(self,data_list):\n        neg=0\n        features=[]\n        all_shama_single=list(shama_single.keys())\n        ###\u6ed1\u52a8\u7a97\u53e3\n        print('or_cnt',len(data_list))\n        split_data_list=[]\n        for index in range(len(data_list)):\n            data=data_list[index]\n            content=data.content #\u6587\u672c\n            docid=data.doc_id #\u6587\u672cid\n            enents=['null',{}]\n            #\u957f\u5ea6\u5c0f\u4e8e\u6700\u5927\u957f\u5ea6\n            if len(content)<=MAX_TEXT_LEN-2:\n                split_data_list.append(Do_data(docid,content,enents))\n                continue\n            #\u5927\u4e8e\u6700\u5927\u957f\u5ea6\uff0c\u7528\u6ed1\u52a8\u7a97\u53e3\u5207\u5206\n            windows=300\n            split_off=0\n            while len(content)>MAX_TEXT_LEN-2:\n                split_content=content[:MAX_TEXT_LEN-2]\n                split_data_list.append(Do_data(docid,split_content,enents,split_off))\n                content=content[windows:]\n            if len(content)>0:\n                split_data_list.append(Do_data(docid,content,enents,split_off))\n        print('split_cnt',len(split_data_list))\n        print('*'*50)\n        for data in tqdm(split_data_list):\n            content=data.content #\u6587\u672c\n            docid=data.doc_id #\u6587\u672cid\n            enent_type=test_recall_data[docid]#\u4e8b\u4ef6\u7c7b\u578b\n            enenttype_enent=[{},{}]\n            ennet_roles=copy.deepcopy(shama[enent_type]) #\u8fd9\u4e2a\u4e8b\u4ef6\u53ef\u80fd\u7684roles,\u4f46\u662f\u4e0d\u8981\u516c\u544a\u65f6\u95f4\uff0c\u56e0\u4e3a\u516c\u544a\u65f6\u95f4\u603b\u662f\u5728\u6587\u672c\u7684\u6700\u540e\n            if enent_type in ['\u80a1\u4e1c\u51cf\u6301', '\u80a1\u4e1c\u589e\u6301', '\u80a1\u6743\u51bb\u7ed3', '\u80a1\u6743\u8d28\u62bc']:\n                continue\n            if '\u516c\u544a\u65f6\u95f4' in ennet_roles:\n                ennet_roles.remove('\u516c\u544a\u65f6\u95f4')\n            #\u6587\u672ctoken\n            t_tokens=self.tokenizer.tokenize(content)\n            assert len(t_tokens)<=MAX_TEXT_LEN+10\n            mapping=rematch(content,t_tokens) #\u83b7\u5f97\u539f\u59cb\u7684map\n            if mapping==None:\n                continue\n            text_token_ids = self.tokenizer.convert_tokens_to_ids(t_tokens+['[SEP]'])\n            #\u5bf9\u6bcf\u4e00\u4e2a\u53ef\u80fd\u7684role\u5efa\u7acb\u4e00\u4e2a\u6837\u672c\n            for role in ennet_roles:\n                question=enent_type+'-'+role\n                q_tokens= self.tokenizer.convert_tokens_to_ids(['[CLS]']+self.tokenizer.tokenize(question)+['[SEP]'])\n                if len(q_tokens)>MAX_QUESTION_LEN:\n                    print('big')\n                    print(len(q_tokens))\n                    print(question)\n                token_ids=q_tokens+text_token_ids\n                assert len(token_ids)<=MAX_LEN\n                if len(token_ids) < MAX_LEN:\n                    token_ids += [0] * (MAX_LEN- len(token_ids))\n                \n                mapping_off=len(q_tokens)\n                labels=[0]*len(token_ids)\n                feature=Feature(item_id=docid,\n                            or_text=content,\n                            mapping=mapping,\n                            mapping_off=mapping_off,\n                            event_type_role=(enent_type,role),\n                            token_ids=token_ids,\n                            question=question,\n                            labels=labels,\n                            enenttype_enent=enenttype_enent)\n                features.append(feature)\n        #\u518d\u6dfb\u52a0\u516c\u544a\u65f6\u95f4\n        print('or_data:',len(data_list))\n        print(len(features))\n        for data in data_list:\n            content=data.content[-MAX_TEXT_LEN-2:] #\u6587\u672c\n#             print(len(content))\n            docid=data.doc_id #\u6587\u672cid\n            enent_type=test_recall_data[docid]#\u4e8b\u4ef6\u7c7b\u578b\n            enenttype_enent=[{},{}]\n            if enent_type in ['\u80a1\u4e1c\u51cf\u6301', '\u80a1\u4e1c\u589e\u6301', '\u80a1\u6743\u51bb\u7ed3', '\u80a1\u6743\u8d28\u62bc']:\n                continue\n            ennet_roles=copy.deepcopy(shama[enent_type]) #\u8fd9\u4e2a\u4e8b\u4ef6\u53ef\u80fd\u7684roles,\u4f46\u662f\u4e0d\u8981\u516c\u544a\u65f6\u95f4\uff0c\u56e0\u4e3a\u516c\u544a\u65f6\u95f4\u603b\u662f\u5728\u6587\u672c\u7684\u6700\u540e\n            if '\u516c\u544a\u65f6\u95f4' not in ennet_roles:\n                continue\n            ennet_roles=['\u516c\u544a\u65f6\u95f4']\n            #\u6587\u672ctoken\n            t_tokens=self.tokenizer.tokenize(content)\n            assert len(t_tokens)<=MAX_TEXT_LEN+10\n            mapping=rematch(content,t_tokens) #\u83b7\u5f97\u539f\u59cb\u7684map\n            if mapping==None:\n                continue\n            text_token_ids = self.tokenizer.convert_tokens_to_ids(t_tokens+['[SEP]'])\n            #\u5bf9\u6bcf\u4e00\u4e2a\u53ef\u80fd\u7684role\u5efa\u7acb\u4e00\u4e2a\u6837\u672c\n            for role in ennet_roles:\n                question=enent_type+'-'+role\n                q_tokens= self.tokenizer.convert_tokens_to_ids(['[CLS]']+self.tokenizer.tokenize(question)+['[SEP]'])\n                if len(q_tokens)>MAX_QUESTION_LEN:\n                    print('big')\n                    print(len(q_tokens))\n                    print(question)\n                token_ids=q_tokens+text_token_ids\n                assert len(token_ids)<=MAX_LEN\n                if len(token_ids) < MAX_LEN:\n                    token_ids += [0] * (MAX_LEN- len(token_ids))\n                \n                mapping_off=len(q_tokens)\n                labels=[0]*len(token_ids)\n                feature=Feature(item_id=docid,\n                            or_text=content,\n                            mapping=mapping,\n                            mapping_off=mapping_off,\n                            event_type_role=(enent_type,role),\n                            token_ids=token_ids,\n                            question=question,\n                            labels=labels,\n                            enenttype_enent=enenttype_enent)\n                features.append(feature)\n        print(len(features))\n        return features    \n    \n    def get_val_recall_features(self,data_list):\n        neg=0\n        features=[]\n        all_shama_single=list(shama_single.keys())\n        ###\u6ed1\u52a8\u7a97\u53e3\n        print('or_cnt',len(data_list))\n        split_data_list=[]\n        for index in range(len(data_list)):\n            data=data_list[index]\n            content=data.content #\u6587\u672c\n            docid=data.doc_id #\u6587\u672cid\n            enents=data.enents\n            #\u957f\u5ea6\u5c0f\u4e8e\u6700\u5927\u957f\u5ea6\n            if len(content)<=MAX_TEXT_LEN-2:\n                split_data_list.append(Do_data(docid,content,enents))\n                continue\n            #\u5927\u4e8e\u6700\u5927\u957f\u5ea6\uff0c\u7528\u6ed1\u52a8\u7a97\u53e3\u5207\u5206\n            windows=300\n            split_off=0\n            while len(content)>MAX_TEXT_LEN-2:\n                split_content=content[:MAX_TEXT_LEN-2]\n                split_data_list.append(Do_data(docid,split_content,enents,split_off))\n                content=content[windows:]\n            if len(content)>0:\n                split_data_list.append(Do_data(docid,content,enents,split_off))\n        print('split_cnt',len(split_data_list))\n        print('*'*50)\n        for data in tqdm(split_data_list):\n            content=data.content #\u6587\u672c\n            docid=data.doc_id #\u6587\u672cid\n            enent_type=val_recall_data[docid]#\u4e8b\u4ef6\u7c7b\u578b\n            enenttype_enent=data.enents[0][1]\n            ennet_roles=copy.deepcopy(shama[enent_type]) #\u8fd9\u4e2a\u4e8b\u4ef6\u53ef\u80fd\u7684roles,\u4f46\u662f\u4e0d\u8981\u516c\u544a\u65f6\u95f4\uff0c\u56e0\u4e3a\u516c\u544a\u65f6\u95f4\u603b\u662f\u5728\u6587\u672c\u7684\u6700\u540e\n            if enent_type in ['\u80a1\u4e1c\u51cf\u6301', '\u80a1\u4e1c\u589e\u6301', '\u80a1\u6743\u51bb\u7ed3', '\u80a1\u6743\u8d28\u62bc']:\n                continue\n            if '\u516c\u544a\u65f6\u95f4' in ennet_roles:\n                ennet_roles.remove('\u516c\u544a\u65f6\u95f4')\n            #\u6587\u672ctoken\n            t_tokens=self.tokenizer.tokenize(content)\n            assert len(t_tokens)<=MAX_TEXT_LEN+10\n            mapping=rematch(content,t_tokens) #\u83b7\u5f97\u539f\u59cb\u7684map\n            if mapping==None:\n                continue\n            text_token_ids = self.tokenizer.convert_tokens_to_ids(t_tokens+['[SEP]'])\n            #\u5bf9\u6bcf\u4e00\u4e2a\u53ef\u80fd\u7684role\u5efa\u7acb\u4e00\u4e2a\u6837\u672c\n            for role in ennet_roles:\n                question=enent_type+'-'+role\n                q_tokens= self.tokenizer.convert_tokens_to_ids(['[CLS]']+self.tokenizer.tokenize(question)+['[SEP]'])\n                if len(q_tokens)>MAX_QUESTION_LEN:\n                    print('big')\n                    print(len(q_tokens))\n                    print(question)\n                token_ids=q_tokens+text_token_ids\n                assert len(token_ids)<=MAX_LEN\n                if len(token_ids) < MAX_LEN:\n                    token_ids += [0] * (MAX_LEN- len(token_ids))\n                \n                mapping_off=len(q_tokens)\n                labels=[0]*len(token_ids)\n                feature=Feature(item_id=docid,\n                            or_text=content,\n                            mapping=mapping,\n                            mapping_off=mapping_off,\n                            event_type_role=(enent_type,role),\n                            token_ids=token_ids,\n                            question=question,\n                            labels=labels,\n                            enenttype_enent=enenttype_enent)\n                features.append(feature)\n        #\u518d\u6dfb\u52a0\u516c\u544a\u65f6\u95f4\n        print('or_data:',len(data_list))\n        print(len(features))\n        for data in data_list:\n            content=data.content[-MAX_TEXT_LEN-2:] #\u6587\u672c\n#             print(len(content))\n            docid=data.doc_id #\u6587\u672cid\n            enent_type=val_recall_data[docid]#\u4e8b\u4ef6\u7c7b\u578b\n            enenttype_enent=data.enents[0][1]\n            if enent_type in ['\u80a1\u4e1c\u51cf\u6301', '\u80a1\u4e1c\u589e\u6301', '\u80a1\u6743\u51bb\u7ed3', '\u80a1\u6743\u8d28\u62bc']:\n                continue\n            ennet_roles=copy.deepcopy(shama[enent_type]) #\u8fd9\u4e2a\u4e8b\u4ef6\u53ef\u80fd\u7684roles,\u4f46\u662f\u4e0d\u8981\u516c\u544a\u65f6\u95f4\uff0c\u56e0\u4e3a\u516c\u544a\u65f6\u95f4\u603b\u662f\u5728\u6587\u672c\u7684\u6700\u540e\n            if '\u516c\u544a\u65f6\u95f4' not in ennet_roles:\n                continue\n            ennet_roles=['\u516c\u544a\u65f6\u95f4']\n            #\u6587\u672ctoken\n            t_tokens=self.tokenizer.tokenize(content)\n            assert len(t_tokens)<=MAX_TEXT_LEN+10\n            mapping=rematch(content,t_tokens) #\u83b7\u5f97\u539f\u59cb\u7684map\n            if mapping==None:\n                continue\n            text_token_ids = self.tokenizer.convert_tokens_to_ids(t_tokens+['[SEP]'])\n            #\u5bf9\u6bcf\u4e00\u4e2a\u53ef\u80fd\u7684role\u5efa\u7acb\u4e00\u4e2a\u6837\u672c\n            for role in ennet_roles:\n                question=enent_type+'-'+role\n                q_tokens= self.tokenizer.convert_tokens_to_ids(['[CLS]']+self.tokenizer.tokenize(question)+['[SEP]'])\n                if len(q_tokens)>MAX_QUESTION_LEN:\n                    print('big')\n                    print(len(q_tokens))\n                    print(question)\n                token_ids=q_tokens+text_token_ids\n                assert len(token_ids)<=MAX_LEN\n                if len(token_ids) < MAX_LEN:\n                    token_ids += [0] * (MAX_LEN- len(token_ids))\n                \n                mapping_off=len(q_tokens)\n                labels=[0]*len(token_ids)\n                feature=Feature(item_id=docid,\n                            or_text=content,\n                            mapping=mapping,\n                            mapping_off=mapping_off,\n                            event_type_role=(enent_type,role),\n                            token_ids=token_ids,\n                            question=question,\n                            labels=labels,\n                            enenttype_enent=enenttype_enent)\n                features.append(feature)\n        print(len(features))\n        return features\n    \n    def get_train_features(self,data_list,recall=None,train_mode=True):\n        neg=0\n        features=[]\n        all_shama_single=list(shama_single.keys())\n        ###\u6ed1\u52a8\u7a97\u53e3\n        print('or_cnt',len(data_list))\n        split_data_list=[]\n        for index in range(len(data_list)):\n            data=data_list[index]\n            content=data.content #\u6587\u672c\n            docid=data.doc_id #\u6587\u672cid\n            enents=data.enents\n            #\u957f\u5ea6\u5c0f\u4e8e\u6700\u5927\u957f\u5ea6\n            if len(content)<=MAX_TEXT_LEN-2:\n                split_data_list.append(Do_data(docid,content,enents))\n                continue\n            #\u5927\u4e8e\u6700\u5927\u957f\u5ea6\uff0c\u7528\u6ed1\u52a8\u7a97\u53e3\u5207\u5206\n            windows=300\n            split_off=0\n            while len(content)>MAX_TEXT_LEN-2:\n                split_content=content[:MAX_TEXT_LEN-2]\n                split_data_list.append(Do_data(docid,split_content,enents,split_off))\n                content=content[windows:]\n            if len(content)>0:\n                split_data_list.append(Do_data(docid,content,enents,split_off))\n        print('split_cnt',len(split_data_list))\n        print('*'*50)\n        for data in tqdm(split_data_list):\n            content=data.content #\u6587\u672c\n#             print(len(content))\n            docid=data.doc_id #\u6587\u672cid\n            enenttype_enent=data.enents[0] #\u8fd9\u4e9b\u53ea\u6709\u4e00\u4e2a\u4e8b\u4ef6\n            enent_type=enenttype_enent[0] #\u4e8b\u4ef6\u7c7b\u578b\n            enent_dict=enenttype_enent[1] #role:argument\n            ennet_roles=copy.deepcopy(shama[enent_type]) #\u8fd9\u4e2a\u4e8b\u4ef6\u53ef\u80fd\u7684roles,\u4f46\u662f\u4e0d\u8981\u516c\u544a\u65f6\u95f4\uff0c\u56e0\u4e3a\u516c\u544a\u65f6\u95f4\u603b\u662f\u5728\u6587\u672c\u7684\u6700\u540e\n            if '\u516c\u544a\u65f6\u95f4' in ennet_roles:\n                ennet_roles.remove('\u516c\u544a\u65f6\u95f4')\n            #\u6587\u672ctoken\n            t_tokens=self.tokenizer.tokenize(content)\n            assert len(t_tokens)<=MAX_TEXT_LEN+10\n            mapping=rematch(content,t_tokens) #\u83b7\u5f97\u539f\u59cb\u7684map\n            if mapping==None:\n                continue\n            text_token_ids = self.tokenizer.convert_tokens_to_ids(t_tokens+['[SEP]'])\n            #\u5bf9\u6bcf\u4e00\u4e2a\u53ef\u80fd\u7684role\u5efa\u7acb\u4e00\u4e2a\u6837\u672c\n            for role in ennet_roles:\n                question=enent_type+'-'+role\n                q_tokens= self.tokenizer.convert_tokens_to_ids(['[CLS]']+self.tokenizer.tokenize(question)+['[SEP]'])\n                if len(q_tokens)>MAX_QUESTION_LEN:\n                    print('big')\n                    print(len(q_tokens))\n                    print(question)\n                token_ids=q_tokens+text_token_ids\n                assert len(token_ids)<=MAX_LEN\n                if len(token_ids) < MAX_LEN:\n                    token_ids += [0] * (MAX_LEN- len(token_ids))\n                \n                mapping_off=len(q_tokens)\n                labels=[0]*len(token_ids)\n                if role in enent_dict and len(enent_dict[role])>0 and train_mode:\n                    argument=enent_dict[role]\n                    a_token_ids=self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(argument))\n                    start_indexs=search_list(a_token_ids,token_ids) #\u6240\u6709\u7b54\u6848\n                    for start_index in start_indexs:\n                        labels[start_index]=1\n                        for i in range(1,len(a_token_ids)):\n                            labels[start_index+i]=2\n                \n                feature=Feature(item_id=docid,\n                            or_text=content,\n                            mapping=mapping,\n                            mapping_off=mapping_off,\n                            event_type_role=(enent_type,role),\n                            token_ids=token_ids,\n                            question=question,\n                            labels=labels,\n                            enenttype_enent=enenttype_enent)\n                features.append(feature)\n        #\u518d\u6dfb\u52a0\u516c\u544a\u65f6\u95f4\n        print('or_data:',len(data_list))\n        print(len(features))\n        for data in data_list:\n            content=data.content[-MAX_TEXT_LEN-2:] #\u6587\u672c\n#             print(len(content))\n            docid=data.doc_id #\u6587\u672cid\n            enenttype_enent=data.enents[0] #\u8fd9\u4e9b\u53ea\u6709\u4e00\u4e2a\u4e8b\u4ef6\n            enent_type=enenttype_enent[0] #\u4e8b\u4ef6\u7c7b\u578b\n            enent_dict=enenttype_enent[1] #role:argument\n            ennet_roles=copy.deepcopy(shama[enent_type]) #\u8fd9\u4e2a\u4e8b\u4ef6\u53ef\u80fd\u7684roles,\u4f46\u662f\u4e0d\u8981\u516c\u544a\u65f6\u95f4\uff0c\u56e0\u4e3a\u516c\u544a\u65f6\u95f4\u603b\u662f\u5728\u6587\u672c\u7684\u6700\u540e\n            if '\u516c\u544a\u65f6\u95f4' not in ennet_roles:\n                continue\n            ennet_roles=['\u516c\u544a\u65f6\u95f4']\n            #\u6587\u672ctoken\n            t_tokens=self.tokenizer.tokenize(content)\n            assert len(t_tokens)<=MAX_TEXT_LEN+10\n            mapping=rematch(content,t_tokens) #\u83b7\u5f97\u539f\u59cb\u7684map\n            if mapping==None:\n                continue\n            text_token_ids = self.tokenizer.convert_tokens_to_ids(t_tokens+['[SEP]'])\n            #\u5bf9\u6bcf\u4e00\u4e2a\u53ef\u80fd\u7684role\u5efa\u7acb\u4e00\u4e2a\u6837\u672c\n            for role in ennet_roles:\n                question=enent_type+'-'+role\n                q_tokens= self.tokenizer.convert_tokens_to_ids(['[CLS]']+self.tokenizer.tokenize(question)+['[SEP]'])\n                if len(q_tokens)>MAX_QUESTION_LEN:\n                    print('big')\n                    print(len(q_tokens))\n                    print(question)\n                token_ids=q_tokens+text_token_ids\n                assert len(token_ids)<=MAX_LEN\n                if len(token_ids) < MAX_LEN:\n                    token_ids += [0] * (MAX_LEN- len(token_ids))\n                \n                mapping_off=len(q_tokens)\n                labels=[0]*len(token_ids)\n                if role in enent_dict and len(enent_dict[role])>0 and train_mode:\n                    argument=enent_dict[role]\n                    a_token_ids=self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(argument))\n                    start_indexs=search_list(a_token_ids,token_ids) #\u6240\u6709\u7b54\u6848\n                    for start_index in start_indexs:\n                        labels[start_index]=1\n                        for i in range(1,len(a_token_ids)):\n                            labels[start_index+i]=2\n                \n                feature=Feature(item_id=docid,\n                            or_text=content,\n                            mapping=mapping,\n                            mapping_off=mapping_off,\n                            event_type_role=(enent_type,role),\n                            token_ids=token_ids,\n                            question=question,\n                            labels=labels,\n                            enenttype_enent=enenttype_enent)\n                features.append(feature)\n        print(len(features))\n        return features\n                        \n    \n    def __len__(self):\n        return len(self.features)\n    def select_tokens(self, tokens, max_num):\n        if len(tokens) <= max_num:\n            return tokens\n        return tokens[:max_num]\n    def get_seg_ids(self, ids):\n        seg_ids = torch.zeros_like(ids)\n        seg_idx = 0\n        for i, e in enumerate(ids):\n            seg_ids[i] = seg_idx\n            if e == SEP_TOKEN_ID:\n                seg_idx += 1\n        max_idx = torch.nonzero(seg_ids == seg_idx)\n        seg_ids[max_idx] = 0\n        return seg_ids\n    def __getitem__(self,index):\n        feature=self.features[index]\n        token_ids=torch.tensor(feature.token_ids)\n        seg_ids=self.get_seg_ids(token_ids)\n        labels=torch.tensor(np.array(feature.labels).astype(np.float32)).long()\n        \n        return token_ids,seg_ids,labels\n    \n    def collate_fn(self, batch):\n        token_ids = torch.stack([x[0] for x in batch])\n        seg_ids = torch.stack([x[1] for x in batch])\n        labels=torch.stack([x[2] for x in batch])\n        return token_ids, seg_ids, labels\n    \n    \ndef get_loader(df,batch_size=16,train_mode=False,val_mode=False,test_mode=False,train_val=True,recall=False):\n    ds_df = zy_DataSet(copy.deepcopy(df),train_mode=train_mode,val_mode=val_mode,test_mode=test_mode,labeled=train_val,recall=recall)\n    loader = torch.utils.data.DataLoader(ds_df, batch_size=batch_size, shuffle=train_mode, num_workers=0, collate_fn=ds_df.collate_fn, drop_last=train_mode)\n    loader.num = len(ds_df)\n    \n    return loader,ds_df.features\n\ndef debug_loader(df):\n    loader,features=get_loader(train_data[:64],train_mode=True)\n    for token_ids, seg_ids,labels in loader:\n        print(token_ids)\n        print(seg_ids)\n        print(labels)\n        break\n    print(len(features))","7f792f82":"train_data[:1]","d2717317":"train_loader,train_features=get_loader(train_data[:1],train_mode=True)\nlen(train_features)","73e5ff15":"train_features[0]","0ac205bd":"debug_loader(train_loader)","6fb1e200":"from transformers import *\nfrom torchcrf import CRF\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport time\nfrom tqdm import tqdm_notebook\n\nfrom transformers import *\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport collections\nimport time\nfrom tqdm import tqdm_notebook","27d750ad":"class PositionalWiseFeedForward(nn.Module):\n\n    def __init__(self, model_dim=768, ffn_dim=2048, dropout=0.0):\n        super(PositionalWiseFeedForward, self).__init__()\n\n\n        self.w1 = nn.Conv1d(model_dim, ffn_dim, 1)\n        self.w2 = nn.Conv1d(ffn_dim, model_dim, 1)\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(model_dim)\n\n\n\n    def forward(self, x):\n        #[b,e,s]\n        output = x.transpose(1, 2)\n\n        output = self.w2(F.relu(self.w1(output)))\n        output = self.dropout(output.transpose(1, 2))\n\n        # add residual and norm layer\n        output = self.layer_norm(x + output)\n        return output\n\n\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self,attention_dropout=0.0):\n        super(ScaledDotProductAttention, self).__init__()\n        self.dropout = nn.Dropout(attention_dropout)\n        self.softmax = nn.Softmax(dim=2)\n\n    def forward(self,q,k,v,scale=None,attn_mask=None):\n        \"\"\"\n\n        :param q: [B,Lq,D_q]\n        :param k:[B,Lk,D_k]\n        :param v:[B,Lv,D_v]\n        :param scale: \u7f29\u653e\u56e0\u5b50\n        :param attn_mask:[B,Lq,Lk]\n        :return:\u4e0a\u4e0b\u6587\u5f20\u91cf\uff0c\u548cattention\u5f20\u91cf\n        \"\"\"\n\n\n        #[B,s,s]\n        attention=torch.bmm(q,k.transpose(1,2))\n\n        if scale:\n            attention=attention*scale\n        #attn_mask:[B,sq,sk]\n        if attn_mask!=None:\n            attention=attention.masked_fill_(attn_mask,-np.inf)\n\n        attention=self.softmax(attention)\n\n        attention=self.dropout(attention)\n\n        #[b,s,d]\n        context=torch.bmm(attention,v)\n\n        return context,attention\n\nclass zy_Model(nn.Module):\n    def __init__(self):\n        num_labels=3\n        super(zy_Model, self).__init__()\n        self.model_name = 'zy_Model'\n        self.bert_model = BertModel.from_pretrained(\"..\/working\",cache_dir=None,output_hidden_states=True)\n        self.zy_hidden_fc= nn.Sequential(nn.Linear(768, num_labels),nn.ReLU(True))\n        self.crf=CRF(num_labels,batch_first=True)\n    \n    def mask_mean(self,x,mask):\n        mask_x=x*(mask.unsqueeze(-1))\n        x_sum=torch.sum(mask_x,dim=1)\n        re_x=torch.div(x_sum,torch.sum(mask,dim=1).unsqueeze(-1))\n        return re_x\n    def forward(self,ids,seg_ids,labels,is_test=False):\n        attention_mask = (ids > 0)\n        last_seq,pooled_output,hidden_state=self.bert_model(input_ids=ids,token_type_ids=seg_ids,attention_mask=attention_mask)\n        \n\n        emissions=self.zy_hidden_fc(last_seq)\n    \n        \n#         ans_out=self.zy_hidden_ans_label(pooled_output).sigmoid()\n#         left_right_out=self.zy_left_right(pooled_output)\n        if not is_test:\n#             ans_crit=nn.BCELoss()\n#             c_left_right=nn.CrossEntropyLoss()\n            loss=-self.crf(emissions, labels, mask=attention_mask,reduction='mean')\n#             ans_loss=ans_crit(ans_out,ans_label)+c_left_right(left_right_out,left_right)\n            return loss\n        else:\n            decode=self.crf.decode(emissions,attention_mask)\n            return decode\n\ndef debug_label():\n    loader,features=get_loader(train_data[:64],train_mode=True,batch_size=2)\n    model=zy_Model()\n    for token_ids, seg_ids,labels,in loader:\n        print(token_ids.size())\n        y = model(token_ids, seg_ids,labels ,is_test=False)\n        print(y)\n        y = model(token_ids, seg_ids,labels ,is_test=True)\n        print(y)\n        print(len(y))\n        break\n\n        ","17e28e7b":"# debug_label()","becb79b9":"# model=zy_Model(num_labels)\n# list(model.named_parameters())","584e6aed":"def get_text_f1(pred_text,text):\n    common_number=0\n    for ch in pred_text:\n        if ch in text:\n            common_number+=1\n    p_len=len(pred_text)\n    t_len=len(text)\n    P=common_number\/p_len if p_len>0 else 0.\n    R=common_number\/t_len if t_len>0 else 0.\n    \n    return (2*P*R)\/(P+R) if (P+R)>0 else 0.\n    \ndef metric_fn(results,features):\n    totol_number=0\n    predict_number=0\n    predict_score=0\n    for index,feat in enumerate(features):\n        true_dicts=feat.answers #\u771f\u5b9e\u7684answers\n        pred_dicts=results[index] #\u9884\u6d4b\u7684answers\n        totol_number+=len(set(list(true_dicts.values()))) #\u53bb\u6389\u91cd\u590d\u7684\n        predict_number+=len(set(list(pred_dicts.values())))\n        \n        #\u4e3a\u4e86\u591a\u4e2a\u8868\u8ff0\uff0c\u6539\u4e3adict list\n        true_dicts_list=collections.defaultdict(list)\n        predict_dicts_list=collections.defaultdict(list)\n        for answer,key in pred_dicts.items():\n            predict_dicts_list[key].append(answer)\n        for answer,key in true_dicts.items():\n            true_dicts_list[key].append(answer)\n        #\u8ba1\u7b97\u8bba\u6587f1\uff0c\u5982\u679c\u6709\u591a\u4e2a\u8bba\u5143\uff0c\u9009\u62e9\u5206\u6570\u6700\u9ad8\u90a3\u4e00\u4e2a\n        for key,answer in predict_dicts_list.items():\n            if key in true_dicts_list:\n                true_list=true_dicts_list[key]\n                ans_list=answer\n                s=0.\n                for t in true_list:\n                    for a in ans_list:\n                        s=max(s,get_text_f1(a,t))\n                predict_score+=s\n    P=predict_score\/predict_number if predict_number>0 else 0.\n    R=predict_score\/totol_number if totol_number>0 else 0.\n    \n    f1=(2*P*R)\/(P+R) if (P+R)>0 else 0.\n    \n    return f1,P,R\n    \n    \ndef compute_list_score(preds,trues):\n    score_dict={}\n    for i in range(len(preds)):\n        for j in range(len(trues)):\n            score_dict[(i,j)]=get_text_f1(preds[i],trues[j])\n    number=min(len(preds),len(trues))\n    score_dict= sorted(score_dict.items(), key=lambda d:d[1], reverse = True)\n    aready1={}\n    aready2={}\n    s=0.\n    for k,v in score_dict:\n        if number>0:\n            if k[0] not in aready1 and k[1] not in aready2:\n                s+=v\n                aready1[k[0]]=''\n                aready2[k[1]]=''\n                number-=1\n        else:\n            break\n    return s\n            \n    \n    \ndef metric_fn_qa(results,label_results):\n    totol_number=0\n    predict_number=0\n    predict_score=0\n    for item_id,feat in tqdm_notebook(label_results.items()):\n        #feat p_feat\u662f\u4e00\u4e2a\u5b57\u5178\n        p_feat=results[item_id]\n        #\u83b7\u53d6\u771f\u5b9e\u6807\u7b7e\n        p_feat_str={}\n        for k,v in p_feat.items():\n            p_feat_str[k]=collections.Counter(v).most_common(1)[0][0]\n        predict_number+=len(p_feat_str)\n#         print('predict:',p_feat)\n#         print('real:',feat)\n#         print('predict_real:',p_feat_str)\n#         print('*'*30)\n        for e_role,answer in feat.items():\n            if len(answer)!=0:\n                totol_number+=1\n                if e_role in p_feat_str and p_feat_str[e_role]==answer:\n                    predict_score+=1\n            \n    print(predict_number)\n    print(totol_number)\n    P=predict_score\/predict_number if predict_number>0 else 0.\n    R=predict_score\/totol_number if totol_number>0 else 0.\n    \n    f1=(2*P*R)\/(P+R) if (P+R)>0 else 0.\n    \n    return f1,P,R\n\ndef validation_fn(model,val_loader,val_features,is_test=False,val=True):\n    model.eval()\n    predicts=[]\n    p_labels=[]\n    ans_labels=[]\n    bar = tqdm_notebook(val_loader)\n    for i,(ids,seg_ids,labels) in enumerate(bar):\n        p_label= model(ids.cuda(DEVICE),seg_ids.cuda(DEVICE),labels.cuda(DEVICE),is_test=True)\n        p_labels.extend(p_label)\n    results=collections.defaultdict(dict) #\u9884\u6d4b\u7684\u7ed3\u679c\n    label_results=collections.defaultdict(dict) #\u771f\u5b9e\u7684\u7ed3\u679c\n    for index in range(len(p_labels)):\n        preds=p_labels[index] #\u9884\u6d4b\u7ed3\u679c\n        feat=val_features[index]\n        item_id=feat.item_id\n        text=feat.or_text\n        off=feat.mapping_off\n        mapping=feat.mapping #\u6620\u5c04\n        event_type_role=feat.event_type_role #(enent_type,role)\n        enenttype_enent=feat.enenttype_enent\n        arguments=[]\n        starting=False\n        p_ans=[]\n        for i,label in enumerate(preds[off:len(mapping)+off]):\n            if label > 0 and label<=2:\n                ch = text[mapping[i][0]:mapping[i][-1] + 1] #\u5f53\u524d\u9884\u6d4b\u7684\u5b57\n                #\u662f\u5b9e\u4f53\u5f00\u59cb\n                if label==1:\n                    starting = True\n                    p_ans.append([i]) #\u4e8b\u4ef6\u7c7b\u578b\n                elif starting:\n                    p_ans[-1].append(i) #\u662f\u9884\u6d4b\u4e2d\u95f4\u7684\u5b57\n                else:\n                    starting = False\n            else:\n                starting=False\n        #\u83b7\u53d6\u9884\u6d4b\u7ed3\u679c\n        p_ans_text=[]\n        for w in p_ans:\n            p_ans_text.append(text[mapping[w[0]][0]:mapping[w[-1]][-1] + 1])\n        \n        if len(p_ans_text)>0:\n            #\u5df2\u7ecf\u521b\u5efa\u4e86\u7b54\u6848list\n            if len(results[item_id])>0:\n                results[item_id][event_type_role].extend(p_ans_text)\n            #\u6ca1\u6709\u521b\u5efa\u7b54\u6848\u5b57\u5178\n            else:\n                dict_list=collections.defaultdict(list)\n                dict_list[event_type_role]=p_ans_text\n                results[item_id]=dict_list\n        #\u83b7\u53d6\u771f\u5b9e\u7ed3\u679c\n        if len(label_results[item_id])==0 and val:\n            dict_list=collections.defaultdict(str)\n            for role,argument in enenttype_enent[1].items():\n                if len(argument)!=0:\n                    dict_list[(enenttype_enent[0],role)]=str(argument) #i[0]\u662findex\n            label_results[item_id]=dict_list\n    if not is_test:\n        res=metric_fn_qa(results,label_results)\n        return res[0],res[1],res[2],label_results\n    else:\n        return results\n        \n        \n    \ndef train_model(model,train_loader,val_loader,val_features,val_loader_recall=None,val_features_recall=None,accumulation_steps=2,early_stop_epochs=2,epochs=4,model_save_path='pytorch_zy_model_true.pkl'):  \n    \n    losses=[]\n    ########\u68af\u5ea6\u7d2f\u8ba1\n    batch_size = accumulation_steps*32\n    \n    ########\u65e9\u505c\n    early_stop_epochs=2\n    no_improve_epochs=0\n    \n    ########\u4f18\u5316\u5668 \u5b66\u4e60\u7387\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    \n    crf_p=[n for n, p in param_optimizer if str(n).find('crf')!=-1]\n#     crf_p_bias=[n for n, p in param_optimizer if (str(n).find('crf')!=-1 or str(n).find('zy')!=-1) and str(n).find('bias')!=-1 ]\n    print(crf_p)\n#     print(crf_p_bias)\n    optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) and n not in crf_p], 'weight_decay': 0.8},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay) and n not in crf_p], 'weight_decay': 0.0},\n            {'params': [p for n, p in param_optimizer if n in crf_p], 'lr': 1e-4, 'weight_decay': 0.8},\n#             {'params': [p for n, p in param_optimizer if n in crf_p_bias], 'lr': 1e-4,'weight_decay': 0.0}\n            ]   \n    \n    optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\n    \n    train_len=len(train_loader)\n    \n    best_vmetric=-np.inf\n    tloss = []\n    ans_losses=[]\n    for epoch in range(1,epochs+1):\n        model.train()\n        bar = tqdm_notebook(train_loader)\n        for i,(ids,seg_ids,labels) in enumerate(bar):\n            loss= model(ids.cuda(DEVICE),seg_ids.cuda(DEVICE),labels.cuda(DEVICE),is_test=False)\n            sloss=loss\n            sloss.backward()\n            tloss.append(loss.item())\n            ans_losses.append(loss.item())\n            if (i+1) % accumulation_steps == 0 or (i+1)==train_len:\n                optimizer.step()\n                optimizer.zero_grad()\n            bar.set_postfix(loss=np.array(tloss).mean(),ans_loss=np.array(ans_losses).mean())\n        \n        #val\n        val_f1,val_p,val_recall,val_results=validation_fn(model,val_loader,val_features)\n        if val_features_recall:\n            s=validation_fn(model,val_loader_recall,val_features_recall,is_test=True,val=False)\n            print('recall f1:',metric_fn_qa(s,val_results))\n#             losses.append(str(s))\n        losses.append( 'train_loss:%.5f, f1: %.5f, precision: %.5f, recall: %.5f, best f1: %.5f\\n' %\n            (np.array(tloss).mean(),val_f1, val_p, val_recall, best_vmetric))\n        print(losses[-1])\n        if val_f1>=best_vmetric:\n            torch.save(model.state_dict(),model_save_path)\n            best_vmetric=val_f1\n            no_improve_epochs=0\n            print('improve save model!!!')\n        else:\n            no_improve_epochs+=1\n        if no_improve_epochs==early_stop_epochs:\n            print('no improve score !!! stop train !!!')\n            break\n    return losses","51664cac":"# train_loader,train_features=get_loader(train_data,batch_size=16,train_mode=True,train_val=True)\n# val_loader,val_features=get_loader(valid_data,batch_size=8,val_mode=True,train_val=True,recall=False)\n# val_loader_recall,val_features_recall=get_loader(valid_data,batch_size=8,val_mode=True,train_val=True,recall=True)","9baf7c7d":"# print(len(train_features))\n# print(len(val_features))\n# print(len(val_features_recall))","1718cfde":"# model=zy_Model(num_labels).cuda(DEVICE)\n# losses=train_model(model,train_loader,val_loader,val_features,val_loader_recall,val_features_recall,accumulation_steps=2,early_stop_epochs=2,epochs=7,model_save_path='right_model_ans_label.pkl')","1505c1df":"# for l in losses:\n#     print(l)","c00b74ed":"# model=zy_Model(num_labels).cuda(DEVICE)\n# model_save_path='..\/input\/ee-model\/right_model_ans_label.pkl'\n# # model_save_path='right_model_ans_label.pkl'\n# model.load_state_dict(torch.load(model_save_path))","086e45ae":"test_loader,test_features=get_loader(test_data,batch_size=8,test_mode=True,train_val=False,recall=True)\nlen(test_features)","b0b8cb34":"def predict_to_file_qa(results,out_file):\n    \"\"\"\u9884\u6d4b\u7ed3\u679c\u5230\u6587\u4ef6\uff0c\u65b9\u4fbf\u63d0\u4ea4\n    \"\"\"\n    fw =open(out_file, 'w', encoding='utf-8')\n    for item_id,v in results.items():\n        l={}\n        l['doc_id']=item_id\n        events=[]\n        single_dict={}\n        for k,vi in v.items():\n#             vi=sorted(vi,key=lambda x:len(x),reverse=True)\n#             print(vi)\n#             v_r=collections.Counter(vi).most_common(1)[0][0]\n            single_dict['event_type']=k[0]\n            single_dict[k[1]]=collections.Counter(vi).most_common(1)[0][0]\n        events.append(single_dict)\n        l['events']=events\n#         print(l)\n        l = json.dumps(l, ensure_ascii=False)\n        fw.write(l + '\\n')\n    fw.close()","dad154b8":"from sklearn.model_selection import KFold\nimport pickle\nFOLD=5\nkf = KFold(n_splits=FOLD, shuffle=True,random_state=2019)\nlog_losses=[]\n\nfor i,(train_index , test_index) in enumerate(kf.split(train_data)):\n    print(str(i+1),'*'*50)\n    if i!=2:\n        continue\n    tra=[train_data[i] for i in train_index]\n    valid=[train_data[i] for i in test_index]\n    print(len(tra))\n    print(len(valid))\n    \n    tra_loader,tra_features=get_loader(tra,batch_size=16,train_mode=True,train_val=True)\n    valid_loader,valid_features=get_loader(valid,batch_size=8,val_mode=True,train_val=True)\n    valid_loader_recall,valid_features_recall=get_loader(valid,batch_size=8,val_mode=True,train_val=True,recall=True)\n    print(len(valid_features))\n    print(len(valid_features_recall))\n    model=zy_Model().cuda(DEVICE)\n    losses=train_model(model,tra_loader,valid_loader,valid_features,accumulation_steps=2,early_stop_epochs=2,epochs=7,model_save_path='cv_recall{}.pkl'.format(i+1))\n    log_losses.extend(losses)\n    #\u52a0\u8f7d\u6700\u597d\u6a21\u578b\n#     model_save_path='..\/input\/ccks42-crf1\/cv_recall{}.pkl'.format(i+1)\n    model_save_path='cv_recall{}.pkl'.format(i+1)\n    model.load_state_dict(torch.load(model_save_path))\n    \n    \n#     f1,p,r,val_label=validation_fn(model,valid_loader,valid_features,is_test=False)\n#     print(f1)\n#     recall_p=validation_fn(model,valid_loader_recall,valid_features_recall,is_test=True,val=False)\n#     print('off_score:',metric_fn_qa(recall_p,val_label))\n#     with open('val_preds{}.pkl'.format(i+1),'wb') as f:\n#         pickle.dump(val_preds,f)\n        \n    test_preds=validation_fn(model,test_loader,test_features,is_test=True,val=False)\n    with open('test_preds{}.pkl'.format(i+1),'wb') as f:\n        pickle.dump(test_preds,f)\n    \n    print(str(i+1),'-'*50)\n    log_losses.append('*'*50)\n    torch.cuda.empty_cache()\n    \n    predict_to_file_qa(test_preds,'base_crf1_predict{}.json'.format(i+1))\n#     break\n#     if i+1>=3:\n#         break\n#     break","0bc94cc4":"#     test_preds=validation_fn(model,test_loader,test_features,is_test=True,val=False)\n#     with open('test_preds{}.pkl'.format(i+1),'wb') as f:\n#         pickle.dump(test_preds,f)","bb652dca":"for l in log_losses:\n    print(l)","f51e632e":"# train","fbfe57b2":"### cv ","a7079031":"### load data","67be8ac9":"### model","b183b8d1":"### dataloader","dea864c2":"### add "}}