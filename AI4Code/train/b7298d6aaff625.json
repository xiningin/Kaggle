{"cell_type":{"2a6acb7c":"code","c9944925":"code","0b5ea0bf":"code","a0cdf718":"code","beb345cb":"code","bd068300":"code","c158a19f":"code","86061bab":"code","63ca0626":"code","73376cfd":"code","0e4d12e4":"code","c83ea039":"code","5f2c7adf":"code","2b82f13f":"code","df893f0c":"code","1cb50277":"code","205759ba":"code","c0680a42":"code","27a7c313":"code","aede341c":"markdown","0d1b1719":"markdown","f2a148fe":"markdown","262175f3":"markdown","2e2c9f86":"markdown","95aadd4f":"markdown","8433863a":"markdown","65f491c6":"markdown","59d4db4c":"markdown"},"source":{"2a6acb7c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c9944925":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection  import train_test_split","0b5ea0bf":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","a0cdf718":"train.head()","beb345cb":"train = train.astype(float)\ntest = test.astype(float)","bd068300":"x= train.drop(columns = ['label'])\ny= train.iloc[:,0]","c158a19f":"x_train,x_test , y_train ,  y_test = train_test_split(x,y,random_state =14, test_size = 0.2) ","86061bab":"x_train = x_train\/255.0\nx_test = x_test\/255.0\ntest = test\/255.0","63ca0626":"x_train = x_train.values.reshape(-1,28,28,1)\nx_test = x_test.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1) ","73376cfd":"y_train = np.array(y_train)\ny_test = np.array(y_test)","0e4d12e4":"model = tf.keras.models.Sequential([\n  tf.keras.layers.Conv2D(64, (3,3), activation='relu',input_shape=(28,28,1)),\n  tf.keras.layers.MaxPooling2D(2, 2),\n  tf.keras.layers.Conv2D(128,(3,3),activation='relu'),\n  tf.keras.layers.MaxPooling2D(2,2),\n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(1024, activation='relu'),\n  tf.keras.layers.Dense(10, activation='softmax')\n])\n","c83ea039":"model.summary()","5f2c7adf":"model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","2b82f13f":"history = model.fit(x_train,y_train ,epochs = 50,validation_data=(x_test,y_test))","df893f0c":"pred = model.predict(test)","1cb50277":"pred=[np.argmax(i) for i in pred]","205759ba":"Predictions = pd.DataFrame({'ImageId' : (i+1 for i in range(len(pred))) , 'Label' : pred})","c0680a42":"Predictions.head()","27a7c313":"Submission_1 = Predictions.to_csv('Sub_1.csv',index=False)","aede341c":"### Scaling Data ","0d1b1719":"### Segregating Target & Features","f2a148fe":"### Importing Important Libraries ","262175f3":"## Building Model ","2e2c9f86":"#### Splitting Data in Train & Test Sets ","95aadd4f":"## Digit Recognizer \n\nThis is my 2nd Notebook with just a few tweaks from the last notebook so I'm not gonna add more comments but just the code.\n\nProblem Statement & Data background :- The Bacis Digit Recognization using the pouplar Dataset MNIST the AKA Hello world of Computer vision.\n\nThe Data have Handwritten Digits from 0-9.\n\nMy approach :- I'll be using Keras basics CNN model using 2 layers of Convolutions. \nChanges from the last notebook :- In the last notebook I used the Adam optimizer and got an accuracy of around 98.14 on the validation Data since I used a callback and cancelled training at 99% accuraacy on training Data. \n<br>So in this notebook I'll try to improve my accuracy by using the RMSprop optimizer and having 50 epochs with batch size 32(Default)","8433863a":"### Importing Data ","65f491c6":"## Making Predictions on Test Data","59d4db4c":"### Reshaping our Data "}}