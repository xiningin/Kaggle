{"cell_type":{"aa319325":"code","33e8098e":"code","6d634b48":"code","afd0a532":"code","f031b1e9":"code","91fcbd6d":"code","44cddaee":"code","22b7358e":"code","15580670":"code","addc35f9":"code","c277861c":"code","243d451e":"code","dda2e3ba":"code","7e79df45":"code","a4bbdc56":"code","137c62a0":"code","6b70f030":"code","f49fafdf":"code","968586a8":"code","c9a564a1":"code","57ae7b16":"code","aad61372":"code","b848f7bd":"code","23b65921":"code","16ea8e1c":"code","5cd082bc":"code","5fbf6b05":"code","c9db55be":"code","6a535352":"code","461ed9be":"code","d7f93a26":"code","09ffa769":"code","a6db698d":"code","89292621":"markdown","3e7838d5":"markdown","eb7f3a7c":"markdown","d64e4908":"markdown","269a27c8":"markdown","293c88d5":"markdown","264c0a4a":"markdown","dfd4f336":"markdown","2e0db597":"markdown","c9a99156":"markdown"},"source":{"aa319325":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","33e8098e":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing import image\nimport tensorflow as tf\nfrom tensorflow.keras import datasets ,layers,models\nimport matplotlib.pyplot as plt\nfrom keras.applications import VGG19\nfrom keras.applications.vgg19 import preprocess_input\nfrom tensorflow.keras.applications import *\nfrom keras.callbacks import ReduceLROnPlateau\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization","6d634b48":"num_classes = 1\ninput_shape = (224, 224, 3)\n","afd0a532":"img = image.load_img(\"..\/input\/chest-xray-pneumonia\/chest_xray\/test\/NORMAL\/IM-0001-0001.jpeg\")\nimg","f031b1e9":"plt.imshow(img)\n","91fcbd6d":"\nimport cv2\nimg_num = cv2.imread(\"..\/input\/chest-xray-pneumonia\/chest_xray\/test\/NORMAL\/IM-0001-0001.jpeg\")\nimg_num.shape","44cddaee":"img_num","22b7358e":"### train = ImageDataGenerator(rescale = 1\/255)\ntrain = ImageDataGenerator(rescale=1.\/255,\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.2, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip = True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\n\nvalid = ImageDataGenerator(rescale=1.\/255,\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.2, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip = True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images \n\n\n\n","15580670":"train_dataset = train.flow_from_directory(\n        '..\/input\/chest-xray-pneumonia\/chest_xray\/train\/',\n        target_size=(224, 224),  \n        class_mode='binary')\nvalidation_dataset = valid.flow_from_directory(\n        '..\/input\/chest-xray-pneumonia\/chest_xray\/val\/',\n        target_size=(224, 224),\n        class_mode='binary')","addc35f9":"train_dataset.class_indices\nimg_size = 224","c277861c":"\nconv_base = EfficientNetB6(input_shape=(img_size,img_size,3),include_top=False,weights=\"imagenet\")\n","243d451e":"cnn4 = Sequential()\ncnn4.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(224,224,3)))\ncnn4.add(BatchNormalization())\n\ncnn4.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\ncnn4.add(BatchNormalization())\ncnn4.add(MaxPooling2D(pool_size=(2, 2)))\ncnn4.add(Dropout(0.25))\n\ncnn4.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\ncnn4.add(BatchNormalization())\ncnn4.add(Dropout(0.25))\n\ncnn4.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\ncnn4.add(BatchNormalization())\ncnn4.add(MaxPooling2D(pool_size=(2, 2)))\ncnn4.add(Dropout(0.25))\n\ncnn4.add(Flatten())\n\ncnn4.add(Dense(512, activation='relu'))\ncnn4.add(BatchNormalization())\ncnn4.add(Dropout(0.5))\n\ncnn4.add(Dense(128, activation='relu'))\ncnn4.add(BatchNormalization())\ncnn4.add(Dropout(0.5))\n\ncnn4.add(Dense(1, activation='sigmoid'))\n\n\ncnn4.summary()","dda2e3ba":"opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy',\n                                            patience = 2,\n                                            verbose=1,\n                                            factor=0.1,\n                                            min_lr=0.000001)\ncnn4.compile(optimizer = opt, loss='binary_crossentropy', metrics=['accuracy'])\n\n\n","7e79df45":"history = cnn4.fit(train_dataset, validation_data=validation_dataset, epochs = 15, callbacks = [learning_rate_reduction])\nhistory","a4bbdc56":"test_dataset = valid.flow_from_directory(\n        '..\/input\/chest-xray-pneumonia\/chest_xray\/test\/',\n        target_size=(224, 224),\n        class_mode='binary')","137c62a0":"print(\"Evaluate on test data\")\nresults = cnn4.evaluate(test_dataset)\nprint(\"test loss, test acc:\", results)\nprint(\"Accuracy of the model is - \" , results[1]*100 , \"%\")\n","6b70f030":"# Create the base model of VGG19\nvgg19 = VGG19( include_top=False, input_shape = (224, 224, 3) , weights = 'imagenet')\n","f49fafdf":"vgg19.summary()","968586a8":"model = Sequential()\n\nfor layer in vgg16.layers[:-1]: # this is where I changed your code\n    model.add(layer)    \n# Freeze the layers \n#for layer in model.layers:\n#    layer.trainable = False\n","c9a564a1":"model.add(Flatten())\nmodel.add(Dense(64 , activation = 'relu'))\nmodel.add(Dense(32 , activation = 'relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n","57ae7b16":"model.summary()","aad61372":"opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\nmodel.compile(optimizer = opt, loss='binary_crossentropy', metrics=[tf.keras.metrics.BinaryCrossentropy()])\n","b848f7bd":"history = model.fit(train_dataset, validation_data=validation_dataset, epochs = 5, batch_size = 64)\nhistory","23b65921":"test_dataset = valid.flow_from_directory(\n        '..\/input\/chest-xray-pneumonia\/chest_xray\/test\/',\n        target_size=(224,224),\n        class_mode='binary')","16ea8e1c":"print(\"Evaluate on test data\")\nresults = model.evaluate(test_dataset)\nprint(\"test loss, test acc:\", results)\nprint(\"Accuracy of the model is - \" , results[1]*100 , \"%\")\n","5cd082bc":"from matplotlib import pyplot\n\n# plot loss during training\npyplot.subplot(211)\npyplot.title('Loss')\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\n# plot accuracy during training\npyplot.subplot(212)\npyplot.title('Accuracy')\npyplot.plot(history.history['accuracy'], label='train')\npyplot.plot(history.history['val_accuracy'], label='test')\npyplot.legend()\npyplot.show()","5fbf6b05":"learning_rate = 0.001\nweight_decay = 0.0001\nbatch_size = 64\nnum_epochs = 100\nimage_size = 224  # We'll resize input images to this size\npatch_size = 6  # Size of the patches to be extract from the input images\nnum_patches = (image_size \/\/ patch_size) ** 2\nprojection_dim = 64\nnum_heads = 4\ntransformer_units = [\n    projection_dim * 2,\n    projection_dim,\n]  # Size of the transformer layers\ntransformer_layers = 1\nmlp_head_units = [64, 32]  # Size of the dense layers of the final classifier","c9db55be":"def mlp(x, hidden_units, dropout_rate):\n    for units in hidden_units:\n        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n        x = layers.Dropout(dropout_rate)(x)\n    return x","6a535352":"class Patches(layers.Layer):\n    def __init__(self, patch_size):\n        super(Patches, self).__init__()\n        self.patch_size = patch_size\n\n    def call(self, images):\n        batch_size = tf.shape(images)[0]\n        patches = tf.image.extract_patches(\n            images=images,\n            sizes=[1, self.patch_size, self.patch_size, 1],\n            strides=[1, self.patch_size, self.patch_size, 1],\n            rates=[1, 1, 1, 1],\n            padding=\"VALID\",\n        )\n        patch_dims = patches.shape[-1]\n        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n        return patches","461ed9be":"class PatchEncoder(layers.Layer):\n    def __init__(self, num_patches, projection_dim):\n        super(PatchEncoder, self).__init__()\n        self.num_patches = num_patches\n        self.projection = layers.Dense(units=projection_dim)\n        self.position_embedding = layers.Embedding(\n            input_dim=num_patches, output_dim=projection_dim\n        )\n\n    def call(self, patch):\n        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n        encoded = self.projection(patch) + self.position_embedding(positions)\n        return encoded","d7f93a26":"def create_vit_classifier():\n    inputs = layers.Input(shape=input_shape)\n    # Create patches.\n    patches = Patches(patch_size)(inputs)\n    # Encode patches.\n    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n\n    # Create multiple layers of the Transformer block.\n    for _ in range(transformer_layers):\n        # Layer normalization 1.\n        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n        # Create a multi-head attention layer.\n        attention_output = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n        )(x1, x1)\n        # Skip connection 1.\n        x2 = layers.Add()([attention_output, encoded_patches])\n        # Layer normalization 2.\n        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n        # MLP.\n        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n        # Skip connection 2.\n        encoded_patches = layers.Add()([x3, x2])\n\n    # Create a [batch_size, projection_dim] tensor.\n    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n    representation = layers.Flatten()(representation)\n    representation = layers.Dropout(0.5)(representation)\n    # Add MLP.\n    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n    # Classify outputs.\n    logits = layers.Dense(1, activation='sigmoid')(features)\n    # Create the Keras model.\n    model = keras.Model(inputs=inputs, outputs=logits)\n    \n    return model","09ffa769":"def run_experiment(model):\n    opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n\n    model.compile(\n        optimizer=opt,\n        loss='binary_crossentropy',\n        metrics=[ 'accuracy' ],\n    )\n\n    checkpoint_filepath = \"\/tmp\/checkpoint\"\n    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n        checkpoint_filepath,\n        monitor=\"val_accuracy\",\n        save_best_only=True,\n        save_weights_only=True,\n    )\n\n    history = model.fit(train_dataset, validation_data=validation_dataset, epochs = 5, batch_size = 64,callbacks=[checkpoint_callback])\n    model.load_weights(checkpoint_filepath)\n    return history\n\n\nvit_classifier = create_vit_classifier()\nprint(vit_classifier.summary())\n","a6db698d":"history = run_experiment(vit_classifier)","89292621":"# # **base-line Model**","3e7838d5":"# # Compile, train, and evaluate the mode\n","eb7f3a7c":"# # Implement multilayer perceptron (MLP)\n","d64e4908":"# # Implement patch creation as a layer\n","269a27c8":"# # Implement the patch encoding layer\nThe PatchEncoder layer will linearly transform a patch by projecting it into a vector of size projection_dim. In addition, it adds a learnable position embedding to the projected vector.","293c88d5":"# # Build the ViT model\n","264c0a4a":"###### ","dfd4f336":"#    **load pre-trained EfficientNetB6 baseline model with the imagNet Dataset **","2e0db597":"# # **VGG16 Model**","c9a99156":"# VIT model "}}