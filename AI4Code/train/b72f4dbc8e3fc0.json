{"cell_type":{"7e4e90e5":"code","ee78b5aa":"code","99e2df6e":"code","e8c685e6":"code","bd434c0b":"code","9ac34995":"code","d2d0f6c6":"code","44cc0588":"code","d5d6bf9b":"code","e6f8784d":"code","16c9adc7":"code","e98b66b0":"code","3054234c":"markdown","090f1d1b":"markdown","e4c1024a":"markdown","7493ea72":"markdown","2cb9d3f4":"markdown"},"source":{"7e4e90e5":"import numpy as np # what im i supposed to say?\nimport pandas as pd # data processing\npd.set_option('mode.chained_assignment', None) # Nasty, i know...\n\nimport torch # To light the way\nimport matplotlib.pyplot as plt # xes and ys\n\nfrom copy import deepcopy # Memory Things","ee78b5aa":"def expands_variables(df, cats, conts, aggs=['mean', 'std', 'max', 'min'], powers=4, put_dummies=True):\n    \"\"\"\n    This functions expands categorical fields by aggregating them and appending data as new rows\n      this method also puts raises the continuous variables to powers if powers is not None\n      It also creates dummies if you ask it to!\n    \"\"\"\n    use_powers = (not powers==None) # Should use powers?\n    groups = cats + list(set([tuple(sorted([x,y]))  for x in cats for y in cats if x!=y ])) # Permutes categorical to create combinations \n    cont_filters = {key:aggs for key in conts} # Agg dict\n    temp = df.copy() # Create a copy from DataFrame\n    \n    # For each group do groupby and merge DataFrame\n    for idx in range(len(groups)):\n        g = groups[idx]\n        # To get rid of annoying warnnings \n        if type(g) == tuple:\n            g = list(g)\n        gb = temp.groupby(g).agg(cont_filters) # GroupBy\n        gb.columns = [\"_\".join([x[0],x[1],str(idx)]) for x in list(gb.columns)] # Rename columns so they don't overlap\n        temp = pd.merge(temp, gb, on=g, how=\"left\") # Merge DataFrame\n    \n    if use_powers: # If you desire to use powers\n        for x in conts:\n            for pw in range(powers-1):\n                temp[x+'_pow_'+str(pw+2)] = temp[x]**(pw+2) # Raise them powers and put to their respective names\n    if put_dummies:# If you desire to use dummies\n        for x in cats: \n            temp = temp.join(pd.get_dummies(temp[x], dtype=float), how='left') # Get them dummies\n    return temp # return transformed DataFrame","99e2df6e":"dataset = pd.read_csv('..\/input\/diabetes-dataset\/diabetes2.csv') # Load\ndataset = expands_variables(dataset, [], dataset.drop(['Outcome'], axis=1).columns) # Sprinkle Some Info\ndataset = dataset.sample(dataset.count()[0]) # Randomize\n\n#Split\nfrom sklearn.model_selection import train_test_split\ntrain_df, test_df = train_test_split(dataset, test_size=0.2)","e8c685e6":"def normalize_data(data, ignore=None):\n    df = deepcopy(data)\n    describe = dataset.describe()\n    mapping = {}\n    for x in df.columns:\n        if x not in ignore:\n            desc = dataset.describe().T[['mean','std']]\n            mean = desc.loc[desc.index==x]['mean'].values[0]\n            std = desc.loc[desc.index==x]['std'].values[0]\n            mapping[x] = {'mean':mean,'std':std}\n            df[x] = df[x].apply(lambda x: (x-mean)\/std)\n    return df, mapping\n\ndef denormalize_data(data, mapping):\n    df = deepcopy(data)\n    for x in mapping.keys():\n        df[x] = df[x].apply(lambda u: (u*mapping[x]['std'])+mapping[x]['mean'])\n    return df, mapping","bd434c0b":"train_df_norm, normal_mapping = normalize_data(train_df, ignore=['Outcome'])\ntest_df_norm, normal_mapping = normalize_data(test_df, ignore=['Outcome'])","9ac34995":"X_train, Y_train = train_df_norm.drop(['Outcome'], axis=1).values, train_df_norm[['Outcome']].values\nX_test, Y_test = test_df_norm.drop(['Outcome'], axis=1).values, test_df_norm[['Outcome']].values\nprint(X_train.shape)\nprint(X_test.shape)","d2d0f6c6":"class LogisticRegressionModel(torch.nn.Module):\n    def __init__(self, feature_count):\n        super(LogisticRegressionModel, self).__init__()\n        self.linear = torch.nn.Linear(feature_count, 1)\n    def forward(self, x):\n        return torch.sigmoid(self.linear(x))\n    def fit_model(self, X, Y, \n                  learning_rate_a= 0.0001,  learning_rate_b= 0.00001, cut_learning_rate=0.45,\n                  epochs=20000, \n                  lambda1 = 0.5, lambda2 = 0.01):\n        \n        x = torch.autograd.Variable(torch.tensor(X).type(torch.FloatTensor))\n        y = torch.autograd.Variable(torch.tensor(Y).type(torch.FloatTensor))\n        criterion = torch.nn.BCELoss()\n        optimizer = torch.optim.Adamax(\n            self.parameters(), lr=learning_rate_a, weight_decay=1e-6)\n        losses = []\n        using_LR = \"A\"\n        \n        for epoch in range(epochs):\n            optimizer.zero_grad()\n            # ===================forward=====================\n            output = self.forward(x)\n            loss = criterion(output, y)\n            # Some Regularization\n            all_linear_params = torch.cat([x.view(-1) for x in self.linear.parameters()])\n            l1_regularization = lambda1 * torch.norm(all_linear_params, 1)\n            l2_regularization = lambda2 * torch.norm(all_linear_params, 2)\n            #loss += l1_regularization + l2_regularization\n            # ===================backward====================\n            loss.backward()\n            optimizer.step()\n            # ===================log========================\n            if (loss.data.item() <= cut_learning_rate):\n                using_LR = 'B'\n                for param_group in optimizer.param_groups:\n                    param_group['lr'] = learning_rate_b\n            else:\n                using_LR = 'A'\n                for param_group in optimizer.param_groups:\n                    param_group['lr'] = learning_rate_a\n\n            print('Using LR_'+using_LR+' epoch [{}\/{}], loss:{:.8f}'\n                  .format(epoch+1, epochs, loss.data.item()), end='\\r')\n            losses.append(loss.data.item())\n            \n        q = [x for x in range(len(losses))]\n        plt.plot(q, losses)\n        ","44cc0588":"modelA = LogisticRegressionModel(X_train.shape[1])\nmodelA.fit_model(X_train, Y_train)\n# Create Torch Tensor\nx = torch.autograd.Variable(torch.tensor(X_test).type(torch.FloatTensor))\n# Get Results\nmodelA_out = modelA(x).detach().numpy()","d5d6bf9b":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import roc_curve\n\nprecisionA, recallA, thresholdsA = precision_recall_curve(Y_test, modelA_out)\nfpr_A, tpr_A, threshold_A = roc_curve(Y_test, modelA_out)\n\nfig, axes = plt.subplots(nrows=1, ncols=2, constrained_layout=True, figsize=(10,10))\nfig.suptitle('Metrics', fontsize=16)\n\nax = axes[0]\nax.plot(recallA, precisionA, 'k--', label='Logistic Regression')\n\nax.set_title(\"Precision Recall\")\nax.set_xlabel(\"Recall\")\nax.set_ylabel(\"Precision\")\nax.set_ylim([0.0, 1.05])\nax.set_xlim([0.0, 1.0])\nax.set_aspect(1.0)\nax.grid()\n\n\nax = axes[1]\nax.plot(fpr_A, tpr_A, 'k--', label='Logistic Regression')\nax.set_title(\"ROC\")\nax.set_xlabel('False Positive Rate')\nax.set_ylabel('True Positive Rate')\nax.set_ylim([0.0, 1.05])\nax.set_xlim([0.0, 1.0])\nax.set_aspect(1.0)\n\nax.grid()\nplt.show()","e6f8784d":"th = threshold_A[np.where(fpr_A >= 0.3)[0][0]]\nprint('Selected Threshold',th)\ntest_results = test_df[['Outcome']]\ntest_results.columns = ['Actual']\ntest_results['Model Porba'] = modelA_out\ntest_results['Model Prediction'] = test_results['Model Porba'].apply(lambda x: 1 if (x > th) else 0)\ntest_results.head(10)","16c9adc7":"pd.crosstab(test_results['Model Prediction'].apply(lambda x: \"Yes\" if x == 1 else \"No\"), \n            test_results.Actual.apply(lambda x: \"Yes\" if x == 1 else \"No\"))","e98b66b0":"from sklearn.metrics import accuracy_score\nprint(\"Model Accuracy\", str(\"%.2f\"%(100*accuracy_score(test_results.Actual, test_results['Model Prediction']))))","3054234c":"## Basics 2 - Logistic Regression\n\nLogistic Regression For Fun :D","090f1d1b":"## Select Prediction Cut\n\nOkay so we are dealing with medical data here.. a desease in this case, quite a responsability don't you think?\nWe can't just have a bunch of false negatives right? \nSo we may set the **False Positive Rate** quite **high**, that is, our **threshold** will be quite **low**.\n\nIn the case of a false positive, a trained professional might be able to handle a machine miss classification..","e4c1024a":"## Metrics","7493ea72":"## Prepara Data","2cb9d3f4":"## Define and Train Model"}}