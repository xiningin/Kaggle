{"cell_type":{"fd8c451c":"code","0ae38917":"code","32be8516":"code","3b6573ff":"code","600f6efc":"code","93c4fbcc":"code","86422832":"code","5a47419b":"code","26248ed9":"code","98f21be8":"code","c12fbe51":"code","70ad87b0":"code","2bbad958":"code","8ff3d383":"code","5980d40c":"code","d1a0502b":"code","c266c954":"code","b5da2ae1":"code","9d838e5d":"code","5bbf152b":"code","01943d47":"code","99b25153":"code","1bc1a7d8":"code","2de4898c":"code","9d111981":"code","be9c3fc9":"code","7e29e32e":"code","cb21b304":"code","e539056c":"code","83d1526a":"code","b91772e7":"code","61fc253d":"code","7fbe0d5f":"code","5b53fb49":"code","a68ad3bb":"code","b2a29cad":"code","cd49f6b8":"code","b6b08406":"code","60ec8341":"code","dec3e50a":"code","b539e385":"code","1dee1a35":"code","5e49d0d9":"code","8451a7ca":"code","418f5c9e":"code","41070c4b":"code","46a7d37b":"code","7d75964e":"code","9f0dc72c":"code","e9434a50":"code","928f5eca":"code","3b502842":"code","7f455464":"code","dcb22ebd":"code","a4a1b9de":"code","b900bd37":"code","def47d7d":"code","11a96046":"code","f779892d":"code","84692cc9":"code","957551a3":"code","a2b76da1":"code","7d1c0ef4":"code","a7cf8928":"code","23ea72af":"code","3eb3a571":"code","3599de39":"code","82302e33":"code","69653b33":"code","b0810be8":"code","c0a814a1":"code","c70ecf13":"code","5be02f3a":"code","917b2c31":"code","c428ba5e":"code","97344c85":"code","e95de379":"code","78315d0e":"code","4203d8ee":"code","0d62a8b1":"code","67b51949":"code","e84caac5":"code","efbe530f":"code","84fe91fc":"code","d7e607cf":"code","29aa2c80":"code","073a6499":"code","2f81d60b":"code","580d126c":"code","51f78107":"code","ebdeffd5":"code","c3cca81c":"code","60f92c25":"code","faf61f37":"code","b8706be7":"code","8b449f4c":"code","e2c9e92d":"code","0e5f98e0":"code","75f48636":"code","411eeb21":"code","754d52be":"code","746bd584":"code","bc7fa22a":"code","7f244c65":"code","0d31651d":"code","1f3570b9":"code","b53dac17":"code","68805341":"code","59e68675":"code","f911c778":"code","9f96703d":"code","d49f5289":"code","fb162b38":"code","9fd2da8b":"code","e1b07eae":"code","41ab64a5":"code","b73a24ed":"code","34e13947":"code","a37b014f":"code","64b45661":"code","a64da2f4":"code","8a2b682c":"code","a0d069dc":"code","ef5a8293":"code","35123d9b":"code","fe507387":"code","6ea59be2":"code","c47467f9":"code","6da1bc54":"code","48263314":"code","e0beea84":"code","824148d4":"code","61c8f8e6":"code","1d668505":"code","0aa72e57":"code","4bd89e4d":"code","d8dafd45":"code","080d2481":"code","46ec48da":"code","d19b1e6d":"code","661415cc":"code","914c2915":"code","00e56c80":"code","b158032b":"code","a0f9a072":"code","59d1d35e":"code","8bd226be":"code","079a7b40":"code","e8e40133":"code","b7e1571f":"code","08e128d8":"code","f3bcacec":"code","cb351b54":"code","dea87bc1":"code","e8c5ed93":"code","28e5e4a4":"code","e8483089":"code","0f4a992b":"code","ee99b4ba":"code","8fd08229":"code","fe8d625d":"code","b9800fe9":"code","19a5e500":"code","5772360e":"code","990342a9":"code","2a0c192a":"code","a67fa092":"code","b1c9c2be":"code","d960fc30":"markdown","07be3c6b":"markdown","09d94aee":"markdown","b6232403":"markdown","e7bc6472":"markdown","f8d0da40":"markdown","8f5256c2":"markdown","da191b03":"markdown","f7b098cb":"markdown","57816151":"markdown","4e1f74de":"markdown","ee83c84f":"markdown","e8baaf03":"markdown","5f5810d8":"markdown","ec7e3da1":"markdown","c54e714f":"markdown","71658cde":"markdown","c4f82951":"markdown","9299895f":"markdown","cef79550":"markdown","691638d4":"markdown","f275c4fe":"markdown","34ff8f7e":"markdown","319ca52f":"markdown","70ae790e":"markdown","25d11545":"markdown","8f281727":"markdown","74547bb4":"markdown","b9bff9ba":"markdown","c70b4359":"markdown","cc8d2ded":"markdown","c3aa3cfc":"markdown","1b9c1270":"markdown","69e7edff":"markdown","b4136359":"markdown","9baa2591":"markdown","eb5ea67c":"markdown","8364ab4c":"markdown","4cd914dc":"markdown","0b215e7a":"markdown","def88901":"markdown","29052c4c":"markdown","9fce9737":"markdown","37371c1e":"markdown","9f011f12":"markdown","741b23fb":"markdown","5b8a9724":"markdown","f24c2e86":"markdown","4dec05ea":"markdown","8ceb52af":"markdown","d524db46":"markdown","6cfd738e":"markdown","a877716a":"markdown","7676cf8e":"markdown","78848efa":"markdown","b57935ca":"markdown","41b73110":"markdown","d979aaa9":"markdown","6857a47d":"markdown","a120879e":"markdown","11257798":"markdown","9235d38d":"markdown","84346097":"markdown","2c6409d5":"markdown","8dd8476b":"markdown","8179e1ba":"markdown"},"source":{"fd8c451c":"#pip install -U scikit-learn","0ae38917":"# Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Importing Pandas and NumPy\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nimport numpy as np\n\n# Importing visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns","32be8516":"#statsmodels\nimport statsmodels\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","3b6573ff":"#sklearn\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n# Importing classification report and confusion matrix from sklearn metrics\nfrom sklearn.metrics import classification_report,confusion_matrix, accuracy_score\n# Imputer from sklearn.impute \nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n# RFE import\nfrom sklearn.feature_selection import RFE","600f6efc":"# Importing all datasets\nchurn_data = pd.read_csv(\"..\/input\/telecom-churn-dataset\/telecom_churn_data.csv\")\nchurn_data.head()","93c4fbcc":"churn_data.shape","86422832":"churn_data.info()","5a47419b":"# Checking Null values%\nround(100*(churn_data.isnull().sum()\/len(churn_data.index)),2)","26248ed9":"churn_data['avg_total_rech_amt_6_7'] = churn_data[['total_rech_amt_6', 'total_rech_amt_7']].mean(axis=1)","98f21be8":"churn_data = churn_data[churn_data.avg_total_rech_amt_6_7 >= churn_data.avg_total_rech_amt_6_7.quantile(.70)]","c12fbe51":"# Average columns will be added later\nchurn_data.drop(['avg_total_rech_amt_6_7'],inplace=True,axis=1)\nchurn_data.shape","70ad87b0":"churn_data['churn'] = np.where(  (churn_data.total_ic_mou_9 == 0)\n                               & (churn_data.total_og_mou_9 == 0)\n                               & (churn_data.vol_2g_mb_9 == 0)\n                               & (churn_data.vol_3g_mb_9 == 0), 1, 0)","2bbad958":"churn_data['churn'].value_counts()","8ff3d383":"churn_percent = (sum(churn_data['churn'])\/len(churn_data.index))*100\nprint(churn_percent)\n\nplt.figure(figsize=(12, 6))\ncolors = [\"#3791D7\", \"#D72626\"]\nlabels = \"Loyal Customers\", \"Churn Customers\"\nchurn_data[\"churn\"].value_counts().plot.pie(explode=[0,0.2], autopct='%1.2f%%', shadow=True, colors=colors, labels=labels, fontsize=12, startangle=70)\nplt.ylabel('% of Customers', fontsize=14)\nplt.show()","5980d40c":"churn_data.shape","d1a0502b":"# Rename the vbc columns for consistency with other columns\nchurn_data.rename(columns = {'jun_vbc_3g':'vbc_3g_6' , 'jul_vbc_3g':'vbc_3g_7', \n                             'aug_vbc_3g':'vbc_3g_8' , 'sep_vbc_3g':'vbc_3g_9' }, inplace = True)\n\n# Rename the last_day_rch_amt columns to last_day_rech_amt for consistency\nchurn_data.rename(columns = {'last_day_rch_amt_6':'last_day_rech_amt_6' , 'last_day_rch_amt_7':'last_day_rech_amt_7',\n                             'last_day_rch_amt_8':'last_day_rech_amt_8' , 'last_day_rch_amt_9':'last_day_rch_amt_9'},\n                  inplace = True)","c266c954":"churn_data = churn_data.loc[:,~churn_data.columns.str.endswith('_9')]\n\nchurn_data.shape","b5da2ae1":"# Checking Null values%\nround(100*(churn_data.isnull().sum()\/len(churn_data.index)),2)","9d838e5d":"# Drop features\/columns that have more than 60% NULL values\nchurn_data = churn_data.dropna(thresh=churn_data.shape[0]*0.6, how='all', axis=1)","5bbf152b":"# Drop the date columns as they don't seem to hold significance\nfor col in churn_data.columns:\n    if 'date' in col:\n        churn_data.drop(col,inplace=True,axis=1)\n\nchurn_data.shape","01943d47":"# Drop features that have only one unique value\nfor col in churn_data.columns:\n    if (churn_data[col].nunique() == 1):\n        print(col)\n        churn_data.drop(col,inplace=True,axis=1)\n\nchurn_data.shape","99b25153":"# No duplicate mobile number rows\nprint(churn_data.mobile_number.nunique())\n# Drop mobile number column\nchurn_data.drop('mobile_number',inplace=True,axis=1)","1bc1a7d8":"churn_data.describe(percentiles=[.99,.95,.9,.75,.25,.1,.05,.01])","2de4898c":"for col in churn_data.columns:\n    if(col != \"churn\"):\n        Q90 = churn_data[col].quantile(0.9)  \n        churn_data[col] = np.clip(churn_data[col], 0, Q90)","9d111981":"churn_data.describe(percentiles=[.99,.95,.9,.75,.25,.1,.05,.01])","be9c3fc9":"# Checking Null values%\nround(100*(churn_data.isnull().sum()\/len(churn_data.index)),2)","7e29e32e":"# All missing values are in mou columns\n# Impute with value 0 on all other missing values\nchurn_data = churn_data.fillna(0)","cb21b304":"# Checking Null values%\nround(100*(churn_data.isnull().sum()\/len(churn_data.index)),2)","e539056c":"for first_col in churn_data.columns:\n    if first_col.endswith(\"_6\"):\n        second_col = first_col.replace('_6','_7')\n        third_col  = first_col.replace('_6','_8')\n        avg_col = \"avg_\"+first_col+\"_7\"\n        #print(\"Derive \", avg_col, \"from \",first_col, second_col)        \n        churn_data[avg_col]  = churn_data[[first_col, second_col]].mean(axis=1)\n        churn_data.drop([first_col, second_col],inplace=True,axis=1)","83d1526a":"churn_data.describe()","b91772e7":"print(churn_data.shape)","61fc253d":"#Identifying customers who use roaming\nchurn_data[\"roaming_user\"] = np.where( (churn_data.avg_roam_ic_mou_6_7 != 0) |                                       \n                                       (churn_data.avg_roam_og_mou_6_7 != 0) |\n                                       (churn_data.roam_ic_mou_8 != 0) |                                       \n                                       (churn_data.roam_og_mou_8 != 0),\n                                       1,0\n                                    )\n\n#Identifying customers who use std\nchurn_data[\"std_user\"] = np.where( (churn_data.avg_std_ic_mou_6_7 != 0) |                                       \n                                   (churn_data.avg_std_og_mou_6_7 != 0) |\n                                   (churn_data.std_ic_mou_8 != 0) |                                       \n                                   (churn_data.std_og_mou_8 != 0),\n                                   1,0)\n\n#Identifying customers who use internet\nchurn_data[\"internet_user\"] = np.where(  (churn_data.avg_vol_2g_mb_6_7 != 0) |\n                                          (churn_data.avg_vol_3g_mb_6_7 != 0) |\n                                          (churn_data.avg_sachet_2g_6_7 !=0) |                                          \n                                          (churn_data.avg_vbc_3g_6_7 !=0) |\n                                          (churn_data.vol_2g_mb_8 != 0) |\n                                          (churn_data.vol_3g_mb_8 != 0) |\n                                          (churn_data.sachet_2g_8 !=0)  |\n                                          (churn_data.vbc_3g_8 !=0),\n                                          1,0\n                                       )\n\ndf_roaming_user = churn_data.loc[(churn_data[\"roaming_user\"] == 1),:]\ndf_std_user = churn_data.loc[(churn_data[\"std_user\"] == 1),:]\ndf_internet_user = churn_data.loc[(churn_data[\"internet_user\"] == 1),:]","7fbe0d5f":"print(churn_data.groupby(\"churn\").roaming_user.value_counts())\n\ndf_churn = churn_data.loc[(churn_data[\"churn\"] == 1),:]\ndf_non_churn = churn_data.loc[(churn_data[\"churn\"] == 0),:]\n\nplt.figure(figsize=(14,8))\nplt.subplot(1, 2, 1)\nlabels = \"Roaming Users\", \"Non-Roaming Users\"\ndf_churn.roaming_user.value_counts().plot.pie(labels = labels, autopct=\"%1.2f%%\")\nplt.ylabel(\"Partition of Churn users\")\n\nplt.subplot(1, 2, 2)\nlabels = \"Non-Roaming Users\", \"Roaming Users\"\ndf_non_churn.roaming_user.value_counts().plot.pie(labels = labels, autopct=\"%1.2f%%\")\nplt.ylabel(\"Partition of Non Churn users\")\nplt.show()","5b53fb49":"cond = (((churn_data.avg_std_ic_mou_6_7 != 0) &\n         (churn_data.std_ic_mou_8 == 0)) | \\\n        ((churn_data.avg_std_og_mou_6_7 != 0) &\n         (churn_data.std_og_mou_8 == 0))\n        )\n\nchurn_data['std_churn_b'] = np.where(cond, 1, 0)\nprint(churn_data.groupby([\"churn\"]).std_churn_b.value_counts())","a68ad3bb":"print(churn_data.groupby(\"churn\").std_churn_b.value_counts())\n\ndf_churn = churn_data.loc[(churn_data[\"churn\"] == 1),:]\ndf_non_churn = churn_data.loc[(churn_data[\"churn\"] == 0),:]\n\nplt.figure(figsize=(14,8))\nplt.subplot(1, 2, 1)\nlabels = \"STD Users\", \"OK\"\ndf_churn.std_churn_b.value_counts().plot.pie(labels = labels, autopct=\"%1.2f%%\")\nplt.ylabel(\"Partition of Churn users\")\n\nplt.subplot(1, 2, 2)\nlabels = \"OK\", \"STD Users\"\ndf_non_churn.std_churn_b.value_counts().plot.pie(labels = labels, autopct=\"%1.2f%%\")\nplt.ylabel(\"Partition of Non Churn users\")\nplt.show()","b2a29cad":"# Boxplot of churned customers vs non-churned customers based on no of days they are with the network.\nprint(churn_data.groupby(\"churn\").aon.describe())\nsns.boxplot(x=churn_data.churn, y=churn_data.aon)","cd49f6b8":"churn_data['customer_new'] = churn_data[\"aon\"].apply(lambda x : 1 if x<1000 else 0)","b6b08406":"df_churn = churn_data.loc[(churn_data[\"churn\"] == 1),:]\ndf_non_churn = churn_data.loc[(churn_data[\"churn\"] == 0),:]\nprint(churn_data.groupby(\"churn\").customer_new.value_counts())\n\nplt.figure(figsize=(12,6))\nplt.subplot(1, 2, 1)\nlabels = \"New Customers\",\"Old Customers\"\ndf_churn.customer_new.value_counts().plot.pie(labels = labels, autopct=\"%1.2f%%\")\nplt.ylabel(\"Partition of Churn users\")\n\nplt.subplot(1, 2, 2)\nlabels = \"Old Customers\",\"New Customers\"\ndf_non_churn.customer_new.value_counts().plot.pie(labels = labels, autopct=\"%1.2f%%\")\nplt.ylabel(\"Partition of Non Churn users\")\nplt.show()","60ec8341":"churn_data.drop(['aon'], axis = 1, inplace = True)","dec3e50a":"plt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nsns.boxplot(x=churn_data.churn,y=churn_data.avg_arpu_6_7)\nplt.subplot(1,2,2)\nsns.boxplot(x=churn_data.churn,y=churn_data.arpu_8)\nplt.show()\nprint(churn_data.groupby(\"churn\").avg_arpu_6_7.describe())\nprint(churn_data.groupby(\"churn\").arpu_8.describe())","b539e385":"plt.figure(figsize=(12,5))\nplt.subplot(2,2,1)\nsns.distplot(df_non_churn.avg_arpu_6_7)\nplt.subplot(2,2,2)\nsns.distplot(df_non_churn.arpu_8)\nplt.subplot(2,2,3)\nsns.distplot(df_churn.avg_arpu_6_7)\nplt.subplot(2,2,4)\nsns.distplot(df_churn.arpu_8)\nplt.show()","1dee1a35":"plt.figure(figsize=(14, 6))\nplt.subplot(1,2,1)\nax1 = sns.boxplot(x=\"churn\",y=\"avg_total_rech_num_6_7\", data = churn_data)\nplt.subplot(1,2,2)\nax1 = sns.boxplot(x=\"churn\",y=\"total_rech_num_8\", data = churn_data)\nplt.show()\nprint(churn_data.groupby(\"churn\").avg_total_rech_num_6_7.describe())\nprint(churn_data.groupby(\"churn\").total_rech_num_8.describe())","5e49d0d9":"plt.figure(figsize=(12,5))\nplt.subplot(2,2,1)\nsns.distplot(df_non_churn.avg_total_rech_num_6_7)\nplt.subplot(2,2,2)\nsns.distplot(df_non_churn.total_rech_num_8)\nplt.subplot(2,2,3)\nsns.distplot(df_churn.avg_total_rech_num_6_7)\nplt.subplot(2,2,4)\nsns.distplot(df_churn.total_rech_num_8)\nplt.show()","8451a7ca":"plt.figure(figsize=(14, 6))\nplt.subplot(1,2,1)\nax1 = sns.boxplot(x=\"churn\",y=\"avg_total_rech_amt_6_7\", data = churn_data)\nplt.subplot(1,2,2)\nax1 = sns.boxplot(x=\"churn\",y=\"total_rech_amt_8\", data = churn_data)\nplt.show()\nprint(churn_data.groupby(\"churn\").avg_total_rech_amt_6_7.describe())\nprint(churn_data.groupby(\"churn\").total_rech_amt_8.describe())","418f5c9e":"plt.figure(figsize=(12,5))\nplt.subplot(2,2,1)\nsns.distplot(df_non_churn.avg_total_rech_amt_6_7)\nplt.subplot(2,2,2)\nsns.distplot(df_non_churn.total_rech_amt_8)\nplt.subplot(2,2,3)\nsns.distplot(df_churn.avg_total_rech_amt_6_7)\nplt.subplot(2,2,4)\nsns.distplot(df_churn.total_rech_amt_8)\nplt.show()","41070c4b":"plt.figure(figsize=(14, 6))\nplt.subplot(1,2,1)\nax1 = sns.boxplot(x=\"churn\",y=\"avg_max_rech_amt_6_7\", data = churn_data)\nplt.subplot(1,2,2)\nax1 = sns.boxplot(x=\"churn\",y=\"max_rech_amt_8\", data = churn_data)\nplt.show()\nprint(churn_data.groupby(\"churn\").avg_max_rech_amt_6_7.describe())\nprint(churn_data.groupby(\"churn\").max_rech_amt_8.describe())","46a7d37b":"plt.figure(figsize=(12,5))\nplt.subplot(2,2,1)\nsns.distplot(df_non_churn.avg_max_rech_amt_6_7)\nplt.subplot(2,2,2)\nsns.distplot(df_non_churn.max_rech_amt_8)\nplt.subplot(2,2,3)\nsns.distplot(df_churn.avg_max_rech_amt_6_7)\nplt.subplot(2,2,4)\nsns.distplot(df_churn.max_rech_amt_8)\nplt.show()","7d75964e":"plt.figure(figsize=(14, 6))\nplt.subplot(1,2,1)\nax1 = sns.boxplot(x=\"churn\",y=\"avg_last_day_rech_amt_6_7\", data = churn_data)\nplt.subplot(1,2,2)\nax1 = sns.boxplot(x=\"churn\",y=\"last_day_rech_amt_8\", data = churn_data)\nplt.show()\nprint(churn_data.groupby(\"churn\").avg_last_day_rech_amt_6_7.describe())\nprint(churn_data.groupby(\"churn\").last_day_rech_amt_8.describe())","9f0dc72c":"plt.figure(figsize=(12,5))\nplt.subplot(2,2,1)\nsns.distplot(df_non_churn.avg_last_day_rech_amt_6_7)\nplt.subplot(2,2,2)\nsns.distplot(df_non_churn.last_day_rech_amt_8)\nplt.subplot(2,2,3)\nsns.distplot(df_churn.avg_last_day_rech_amt_6_7)\nplt.subplot(2,2,4)\nsns.distplot(df_churn.last_day_rech_amt_8)\nplt.show()","e9434a50":"cond = (  ((churn_data['avg_last_day_rech_amt_6_7']!=0) & (churn_data['last_day_rech_amt_8']==0)) | \\\n          ((churn_data['avg_max_rech_amt_6_7']!=0) & (churn_data['max_rech_amt_8']==0)) | \\\n          ((churn_data['avg_total_rech_amt_6_7']!=0) & (churn_data['total_rech_amt_8']==0))\n       )\n\nchurn_data['rech_churn_b'] = np.where(cond, 1, 0)\nprint(churn_data.groupby([\"churn\"]).rech_churn_b.value_counts())","928f5eca":"print(churn_data.groupby(\"churn\").rech_churn_b.value_counts())\n\ndf_churn = churn_data.loc[(churn_data[\"churn\"] == 1),:]\ndf_non_churn = churn_data.loc[(churn_data[\"churn\"] == 0),:]\n\nplt.figure(figsize=(14,8))\nplt.subplot(1, 2, 1)\nlabels = \"Recharge\", \"OK\"\ndf_churn.rech_churn_b.value_counts().plot.pie(labels = labels, autopct=\"%1.2f%%\")\nplt.ylabel(\"Partition of Churn users\")\n\nplt.subplot(1, 2, 2)\nlabels = \"OK\", \"Recharge\"\ndf_non_churn.rech_churn_b.value_counts().plot.pie(labels = labels, autopct=\"%1.2f%%\")\nplt.ylabel(\"Partition of Non Churn users\")\nplt.show()","3b502842":"plt.figure(figsize=(14, 6))\nplt.subplot(2,2,1)\nax1 = sns.boxplot(x=\"churn\",y=\"avg_vol_2g_mb_6_7\", data = df_internet_user)\nplt.subplot(2,2,2)\nax1 = sns.boxplot(x=\"churn\",y=\"vol_2g_mb_8\", data = df_internet_user)\n\nplt.figure(figsize=(14, 6))\nplt.subplot(2,2,3)\nax1 = sns.boxplot(x=\"churn\",y=\"avg_vol_3g_mb_6_7\", data = df_internet_user)\nplt.subplot(2,2,4)\nax1 = sns.boxplot(x=\"churn\",y=\"vol_3g_mb_8\", data = df_internet_user)\nplt.show()","7f455464":"plt.figure(figsize=(14, 6))\nplt.subplot(2,2,1)\nsns.boxplot(x=\"churn\",y=\"avg_monthly_3g_6_7\", data = df_internet_user)\nplt.subplot(2,2,2)\nsns.boxplot(x=\"churn\",y=\"monthly_3g_8\", data = df_internet_user)\nplt.subplot(2,2,3)\nsns.boxplot(x=\"churn\",y=\"avg_monthly_2g_6_7\", data = df_internet_user)\nplt.subplot(2,2,4)\nsns.boxplot(x=\"churn\",y=\"monthly_2g_8\", data = df_internet_user)\nplt.show()","dcb22ebd":"plt.figure(figsize=(14, 6))\nplt.subplot(2,2,1)\nsns.boxplot(x=\"churn\",y=\"avg_sachet_3g_6_7\", data = df_internet_user)\nplt.subplot(2,2,2)\nsns.boxplot(x=\"churn\",y=\"sachet_3g_8\", data = df_internet_user)\nplt.subplot(2,2,3)\nsns.boxplot(x=\"churn\",y=\"avg_sachet_2g_6_7\", data = df_internet_user)\nplt.subplot(2,2,4)\nsns.boxplot(x=\"churn\",y=\"sachet_2g_8\", data = df_internet_user)\nplt.show()","a4a1b9de":"plt.figure(figsize=(14, 6))\nplt.subplot(1,2,1)\nsns.boxplot(x=\"churn\",y=\"avg_vbc_3g_6_7\", data = df_internet_user)\nplt.subplot(1,2,2)\nsns.boxplot(x=\"churn\",y=\"vbc_3g_8\", data = df_internet_user)\nplt.show()","b900bd37":"cond = ( ((churn_data['avg_vol_3g_mb_6_7']!=0) & (churn_data['vol_3g_mb_8']==0)) | \\\n         ((churn_data['avg_sachet_2g_6_7']!=0) & (churn_data['sachet_2g_8']==0)) | \\\n         ((churn_data['avg_monthly_3g_6_7']!=0) & (churn_data['monthly_3g_8']==0)) | \\\n         ((churn_data['avg_vbc_3g_6_7']!=0) & (churn_data['vbc_3g_8']==0)) | \\\n         ((churn_data['avg_vol_2g_mb_6_7']!=0) & (churn_data['vol_2g_mb_8']==0)))\n\nchurn_data['internet_churn_b'] = np.where(cond, 1, 0)\nprint(churn_data.groupby([\"churn\",\"internet_user\"]).internet_churn_b.value_counts())","def47d7d":"df_internet_churn = churn_data.loc[(churn_data[\"churn\"] == 1) & (churn_data[\"internet_user\"] == 1),:]\ndf_internet_non_churn = churn_data.loc[(churn_data[\"churn\"] == 0) & (churn_data[\"internet_user\"] == 1),:]\n\nplt.figure(figsize=(14,8))\nplt.subplot(1, 2, 1)\nlabels = \"Change in internet plans\",\"OK\"\ndf_internet_churn.internet_churn_b.value_counts().plot.pie(labels = labels, autopct=\"%1.2f%%\")\nplt.ylabel(\"Partition of Churn Internet users\")\n\nplt.subplot(1, 2, 2)\nlabels = \"OK\",\"Change in internet plans\"\ndf_internet_non_churn.internet_churn_b.value_counts().plot.pie(labels = labels, autopct=\"%1.2f%%\")\nplt.ylabel(\"Partition of Non Churn Internet users\")\nplt.show()","11a96046":"### Dropping few insignifanct features\nchurn_data.drop(['avg_monthly_2g_6_7','monthly_2g_8',\n                 'avg_sachet_3g_6_7','sachet_3g_8'], axis = 1, inplace = True)","f779892d":"print(churn_data.groupby(\"churn\").avg_og_others_6_7.describe())\nprint(churn_data.groupby(\"churn\").og_others_8.describe())\nprint(churn_data.groupby(\"churn\").avg_ic_others_6_7.describe())\nprint(churn_data.groupby(\"churn\").ic_others_8.describe())","84692cc9":"# Dropping as most values are 0\nchurn_data.drop(['avg_og_others_6_7','og_others_8'],inplace=True,axis=1)\nchurn_data.drop(['avg_ic_others_6_7','ic_others_8'],inplace=True,axis=1)","957551a3":"print(churn_data.groupby(\"churn\").avg_spl_ic_mou_6_7.describe())\nprint(churn_data.groupby(\"churn\").spl_ic_mou_8.describe())\nprint(churn_data.groupby(\"churn\").avg_spl_og_mou_6_7.describe())\nprint(churn_data.groupby(\"churn\").spl_og_mou_8.describe())\nplt.figure(figsize=(14, 6))\nplt.subplot(2,2,1)\nsns.boxplot(x=\"churn\",y=\"avg_spl_ic_mou_6_7\", data = churn_data)\nplt.subplot(2,2,2)\nsns.boxplot(x=\"churn\",y=\"spl_ic_mou_8\", data = churn_data)\nplt.subplot(2,2,3)\nsns.boxplot(x=\"churn\",y=\"avg_spl_og_mou_6_7\", data = churn_data)\nplt.subplot(2,2,4)\nsns.boxplot(x=\"churn\",y=\"spl_og_mou_8\", data = churn_data)\nplt.show()","a2b76da1":"# Dropping as most values are 0; and both churn and non churn users show a drop in spl og\nchurn_data.drop(['avg_spl_ic_mou_6_7','spl_ic_mou_8'],inplace=True,axis=1)","7d1c0ef4":"print(churn_data.groupby(\"churn\").avg_loc_og_t2c_mou_6_7.describe())\nprint(churn_data.groupby(\"churn\").loc_og_t2c_mou_8.describe())\nplt.figure(figsize=(12, 5))\nplt.subplot(1,2,1)\nsns.boxplot(x=\"churn\",y=\"avg_loc_og_t2c_mou_6_7\", data = churn_data)\nplt.subplot(1,2,2)\nsns.boxplot(x=\"churn\",y=\"loc_og_t2c_mou_8\", data = churn_data)\nplt.show()","a7cf8928":"churn_data.drop(['avg_loc_og_t2c_mou_6_7','loc_og_t2c_mou_8'],inplace=True,axis=1)","23ea72af":"print(churn_data.groupby(\"churn\").avg_onnet_mou_6_7.describe())\nprint(churn_data.groupby(\"churn\").onnet_mou_8.describe())\nprint(churn_data.groupby(\"churn\").avg_offnet_mou_6_7.describe())\nprint(churn_data.groupby(\"churn\").offnet_mou_8.describe())\nplt.figure(figsize=(14, 6))\nplt.subplot(2,2,1)\nsns.boxplot(x=\"churn\",y=\"avg_onnet_mou_6_7\", data = churn_data)\nplt.subplot(2,2,2)\nsns.boxplot(x=\"churn\",y=\"onnet_mou_8\", data = churn_data)\nplt.subplot(2,2,3)\nsns.boxplot(x=\"churn\",y=\"avg_offnet_mou_6_7\", data = churn_data)\nplt.subplot(2,2,4)\nsns.boxplot(x=\"churn\",y=\"offnet_mou_8\", data = churn_data)\nplt.show()","3eb3a571":"print(churn_data.groupby(\"churn\").avg_loc_ic_t2t_mou_6_7.describe())\nprint(churn_data.groupby(\"churn\").loc_ic_t2t_mou_8.describe())\nprint(churn_data.groupby(\"churn\").avg_loc_ic_t2m_mou_6_7.describe())\nprint(churn_data.groupby(\"churn\").loc_ic_t2m_mou_8.describe())\nprint(churn_data.groupby(\"churn\").avg_loc_ic_t2f_mou_6_7.describe())\nprint(churn_data.groupby(\"churn\").loc_ic_t2f_mou_8.describe())\nprint(churn_data.groupby(\"churn\").avg_loc_ic_mou_6_7.describe())\nprint(churn_data.groupby(\"churn\").loc_ic_mou_8.describe())","3599de39":"plt.figure(figsize=(14, 6))\nplt.subplot(1,2,1)\nsns.boxplot(x=\"churn\",y=\"avg_loc_ic_mou_6_7\", data = churn_data)\nplt.subplot(1,2,2)\nsns.boxplot(x=\"churn\",y=\"loc_ic_mou_8\", data = churn_data)\nplt.show()","82302e33":"print(churn_data.groupby(\"churn\").avg_loc_og_t2t_mou_6_7.describe())\nprint(churn_data.groupby(\"churn\").loc_og_t2t_mou_8.describe())\nprint(churn_data.groupby(\"churn\").avg_loc_og_t2m_mou_6_7.describe())\nprint(churn_data.groupby(\"churn\").loc_og_t2m_mou_8.describe())\nprint(churn_data.groupby(\"churn\").avg_loc_og_t2f_mou_6_7.describe())\nprint(churn_data.groupby(\"churn\").loc_og_t2f_mou_8.describe())\nprint(churn_data.groupby(\"churn\").avg_loc_og_mou_6_7.describe())\nprint(churn_data.groupby(\"churn\").loc_og_mou_8.describe())","69653b33":"plt.figure(figsize=(14, 6))\nplt.subplot(1,2,1)\nsns.boxplot(x=\"churn\",y=\"avg_loc_og_mou_6_7\", data = churn_data)\nplt.subplot(1,2,2)\nsns.boxplot(x=\"churn\",y=\"loc_og_mou_8\", data = churn_data)\nplt.show()","b0810be8":"print(churn_data.groupby(\"churn\").avg_std_ic_t2t_mou_6_7.describe())\nprint(churn_data.groupby(\"churn\").std_ic_t2t_mou_8.describe())\nprint(churn_data.groupby(\"churn\").avg_std_ic_t2m_mou_6_7.describe())\nprint(churn_data.groupby(\"churn\").std_ic_t2m_mou_8.describe())\nprint(churn_data.groupby(\"churn\").avg_std_ic_t2f_mou_6_7.describe())\nprint(churn_data.groupby(\"churn\").std_ic_t2f_mou_8.describe())\nprint(churn_data.groupby(\"churn\").avg_std_ic_mou_6_7.describe())\nprint(churn_data.groupby(\"churn\").std_ic_mou_8.describe())","c0a814a1":"plt.figure(figsize=(14, 6))\nplt.subplot(1,2,1)\nsns.boxplot(x=\"churn\",y=\"avg_std_ic_mou_6_7\", data = churn_data)\nplt.subplot(1,2,2)\nsns.boxplot(x=\"churn\",y=\"std_ic_mou_8\", data = churn_data)\nplt.show()","c70ecf13":"print(churn_data.groupby(\"churn\").avg_std_og_t2t_mou_6_7.describe())\nprint(churn_data.groupby(\"churn\").std_og_t2t_mou_8.describe())\nprint(churn_data.groupby(\"churn\").avg_std_og_t2m_mou_6_7.describe())\nprint(churn_data.groupby(\"churn\").std_og_t2m_mou_8.describe())\nprint(churn_data.groupby(\"churn\").avg_std_og_t2f_mou_6_7.describe())\nprint(churn_data.groupby(\"churn\").std_og_t2f_mou_8.describe())\nprint(churn_data.groupby(\"churn\").avg_std_og_mou_6_7.describe())\nprint(churn_data.groupby(\"churn\").std_og_mou_8.describe())","5be02f3a":"plt.figure(figsize=(14, 6))\nplt.subplot(1,2,1)\nsns.boxplot(x=\"churn\",y=\"avg_std_og_mou_6_7\", data = churn_data)\nplt.subplot(1,2,2)\nsns.boxplot(x=\"churn\",y=\"std_og_mou_8\", data = churn_data)\nplt.show()","917b2c31":"print(churn_data.groupby(\"churn\").avg_isd_og_mou_6_7.describe())\nprint(churn_data.groupby(\"churn\").isd_og_mou_8.describe())\nprint(churn_data.groupby(\"churn\").avg_isd_ic_mou_6_7.describe())\nprint(churn_data.groupby(\"churn\").isd_ic_mou_8.describe())","c428ba5e":"plt.figure(figsize=(14, 6))\nplt.subplot(2,2,1)\nsns.boxplot(x=\"churn\",y=\"avg_isd_ic_mou_6_7\", data = churn_data)\nplt.subplot(2,2,2)\nsns.boxplot(x=\"churn\",y=\"isd_ic_mou_8\", data = churn_data)\nplt.subplot(2,2,3)\nsns.boxplot(x=\"churn\",y=\"avg_isd_og_mou_6_7\", data = churn_data)\nplt.subplot(2,2,4)\nsns.boxplot(x=\"churn\",y=\"isd_og_mou_8\", data = churn_data)\nplt.show()","97344c85":"churn_data.drop(['avg_isd_og_mou_6_7','isd_og_mou_8'],inplace=True,axis=1)\nchurn_data.drop(['avg_isd_ic_mou_6_7','isd_ic_mou_8'],inplace=True,axis=1)","e95de379":"print(churn_data.groupby(\"churn\").avg_total_ic_mou_6_7.describe())\nprint(churn_data.groupby(\"churn\").total_ic_mou_8.describe())","78315d0e":"print(churn_data.groupby(\"churn\").avg_total_og_mou_6_7.describe())\nprint(churn_data.groupby(\"churn\").total_og_mou_8.describe())","4203d8ee":"plt.figure(figsize=(14, 6))\nplt.subplot(2,2,1)\nsns.boxplot(x=\"churn\",y=\"avg_total_ic_mou_6_7\", data = churn_data)\nplt.subplot(2,2,2)\nsns.boxplot(x=\"churn\",y=\"total_ic_mou_8\", data = churn_data)\nplt.subplot(2,2,3)\nsns.boxplot(x=\"churn\",y=\"avg_total_og_mou_6_7\", data = churn_data)\nplt.subplot(2,2,4)\nsns.boxplot(x=\"churn\",y=\"total_og_mou_8\", data = churn_data)\nplt.show()","0d62a8b1":"# Dropping roaming mou columns as we already derived roaming user\nfor col in churn_data.columns:\n    if (\"roam_\" in col):\n        churn_data.drop(col, axis=1, inplace=True)\n\n# Dropping std_user column as we already derived std_churn_b\nchurn_data.drop(['std_user'],inplace=True,axis=1)","67b51949":"churn_data.describe()","e84caac5":"churn_data_pca = churn_data.copy()\nchurn_data_pca.shape","efbe530f":"df_mou = pd.DataFrame()\nfor col in churn_data.columns:\n    if (\"mou_6_7\" in col):\n        df_mou[col] = churn_data[col]\n\nplt.figure(figsize=(20,10))\nsns.heatmap(df_mou.corr(), annot = True, cmap=\"YlGnBu\")","84fe91fc":"# Dropping high correlated features\nchurn_data.drop(['avg_loc_ic_mou_6_7'], axis=1, inplace=True)\nchurn_data.drop(['avg_loc_og_mou_6_7'], axis=1, inplace=True)\nchurn_data.drop(['avg_std_ic_mou_6_7'], axis=1, inplace=True)\nchurn_data.drop(['avg_total_ic_mou_6_7'], axis=1, inplace=True)","d7e607cf":"df_mou = pd.DataFrame()\nfor col in churn_data.columns:\n    if (\"mou_8\" in col):\n        df_mou[col] = churn_data[col]\n\nplt.figure(figsize=(20,10))\nsns.heatmap(df_mou.corr(), annot = True, cmap=\"YlGnBu\")","29aa2c80":"# Dropping high correlated features\nchurn_data.drop(['loc_og_mou_8'], axis=1, inplace=True)\nchurn_data.drop(['loc_ic_mou_8'], axis=1, inplace=True)\nchurn_data.drop(['std_ic_mou_8'], axis=1, inplace=True)\nchurn_data.drop(['total_ic_mou_8'], axis=1, inplace=True)","073a6499":"df_mou = pd.DataFrame()\nfor col in churn_data.columns:\n    if (\"mou\" in col):\n        df_mou[col] = churn_data[col]\n\nplt.figure(figsize=(20,10))\nsns.heatmap(df_mou.corr(), annot = True, cmap=\"YlGnBu\")","2f81d60b":"churn_data.drop(['onnet_mou_8'], axis=1, inplace=True)\nchurn_data.drop(['avg_onnet_mou_6_7'], axis=1, inplace=True)","580d126c":"df_others = pd.DataFrame()\nfor col in churn_data.columns:\n    if (\"mou\" not in col):\n        df_others[col] = churn_data[col]\n\nplt.figure(figsize=(20,10))\nsns.heatmap(df_others.corr(), annot = True, cmap=\"YlGnBu\")","51f78107":"# Highly Correlated with arpu\nchurn_data.drop(['total_rech_amt_8'], axis=1, inplace=True)\nchurn_data.drop(['avg_total_rech_amt_6_7'], axis=1, inplace=True)","ebdeffd5":"df_others = pd.DataFrame()\nfor col in churn_data.columns:\n    if (\"mou\" not in col):\n        df_others[col] = churn_data[col]\n\nplt.figure(figsize=(20,10))\nsns.heatmap(df_others.corr(), annot = True, cmap=\"YlGnBu\")","c3cca81c":"print(churn_data.shape)","60f92c25":"churn_data.head()","faf61f37":"from sklearn.model_selection import train_test_split\n\n# Putting feature variable to X\nX = churn_data.drop(['churn'],axis=1)\n\n# Putting response variable to y\ny = churn_data['churn']\n\ny.head()","b8706be7":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.7,test_size=0.3,random_state=100,stratify=y)","8b449f4c":"scaler = StandardScaler()\n\ncategory_cols = ['roaming_user','customer_new','internet_user','internet_churn_b','std_churn_b','rech_churn_b']  \n\nX_train_category = X_train[category_cols]\nX_train = X_train.drop(category_cols,axis=1)\n\nX_test_category = X_test[category_cols]\nX_test = X_test.drop(category_cols,axis=1)\n\nnumeric_cols = X_train.columns\n\n# Apply fit_transform on train data\nX_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n\n# Apply transform on test data\nX_test[numeric_cols]  = scaler.transform(X_test[numeric_cols])\n\n# Concatenate numerical transformed data and categorical data columns\nX_train = pd.concat([X_train_category, X_train], axis=1)\nX_test = pd.concat([X_test_category, X_test], axis=1)\n\nprint(X_train.shape)\nprint(X_test.shape)","e2c9e92d":"X_test.head()","0e5f98e0":"def df_predictions(y, y_pred, prob_boundary):\n    y_df = pd.DataFrame(y)\n    y_df['CustID'] = y_df.index\n\n    y_pred_df = pd.DataFrame(y_pred)\n\n    # Removing index for both dataframes to append them side by side \n    y_df.reset_index(drop=True, inplace=True)\n    y_pred_df.reset_index(drop=True, inplace=True)\n\n    y_pred_final = pd.concat([y_df,y_pred_df],axis=1)\n\n    # Renaming the column\n    y_pred_final= y_pred_final.rename(columns={ 0 : 'churn_Prob'})\n\n    # Rearranging the columns\n    #y_pred_final = y_pred_final.reindex_axis(['CustID','churn','churn_Prob'], axis=1)\n\n    # Creating new column 'predicted' with 1 if churn_Prob>prob_boundary else 0\n    y_pred_final['churn_predicted'] = y_pred_final.churn_Prob.map( lambda x: 1 if x >prob_boundary else 0)\n   \n    return y_pred_final","75f48636":"def model_eval(y_pred_final):\n    # Confusion matrix \n    confusion = metrics.confusion_matrix(y_pred_final.churn, y_pred_final.churn_predicted)\n    TP = confusion[1,1] # true positive \n    TN = confusion[0,0] # true negatives\n    FP = confusion[0,1] # false positives\n    FN = confusion[1,0] # false negatives\n    print(\"Confusion Matrix -> \")\n    print(\"# Predicted\",\"\\t\",\"notchurn\",\"\\t\",\"churn\")\n    print(\"# Actual\")\n    print(\"# not_churn\\t\",confusion[0,0],\"\\t\\t\",confusion[0,1])        \n    print(\"# churn\\t\\t\",confusion[1,0],\"\\t\\t\",confusion[1,1])\n    # Let's check the report of our model\n    print(\"\\nClassification Report -> \")\n    print(classification_report(y_pred_final.churn, y_pred_final.churn_predicted))    \n    print(\"\\nSensitivity (True Positive rate OR Recall of Churn Label) -> \")\n    print(round(TP \/ (TP+FN),2))\n    print(\"\\nSpecificity (True Negative rate OR Recall of Non Churn Label) -> \")\n    print(round(TN \/ (TN+FP),2))\n    auc_score = metrics.roc_auc_score( y_pred_final.churn, y_pred_final.churn_Prob)\n    print(\"\\nAUC Score -> \", auc_score)    ","411eeb21":"#Draws the ROC\ndef draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","754d52be":"def plot_cutoff_df(y_train_pred_final):\n    # Let's create columns with different probability cutoffs \n    numbers = [float(x)\/10 for x in range(10)]\n    for i in numbers:\n        y_train_pred_final[i]= y_train_pred_final.churn_Prob.map(lambda x: 1 if x > i else 0)\n    # Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\n    cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n    # TP = confusion[1,1] # true positive \n    # TN = confusion[0,0] # true negatives\n    # FP = confusion[0,1] # false positives\n    # FN = confusion[1,0] # false negatives\n\n    num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n    for i in num:\n        cm1 = metrics.confusion_matrix(y_train_pred_final.churn, y_train_pred_final[i] )\n        total1=sum(sum(cm1))\n        accuracy = (cm1[0,0]+cm1[1,1])\/total1    \n        speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n        sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n        cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\n    print(cutoff_df)\n\n    # Let's plot accuracy sensitivity and specificity for various probabilities.\n    cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\n    plt.show()","746bd584":"#Prints VIF of all the features\ndef calculateVif(x_train_sm): \n    vif = pd.DataFrame()\n    vif['Features'] = x_train_sm.columns\n    vif['VIF'] = [variance_inflation_factor(x_train_sm.values, i) for i in range(x_train_sm.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    print(vif)","bc7fa22a":"#returns the stats metric\ndef getLogisticRegStatSummary():\n    logm2 = sm.GLM(y_train,A_train_sm, family = sm.families.Binomial())\n    res = logm2.fit()\n    print(res.summary())\n    calculateVif(X_train_sm)\n    return res","7f244c65":"#Initialising the model\nlogreg = LogisticRegression(class_weight='balanced')","0d31651d":"rfe = RFE(logreg, 12)\nrfe = rfe.fit(X_train, y_train)","1f3570b9":"col = X_train.columns[rfe.support_]","b53dac17":"## Building the model in bottom backward approach\n#X_train_sm = sm.add_constant(X_train[col])\nX_train_sm = (X_train[col])\nX_train_sm.shape","68805341":"#res = getLogisticRegStatSummary(X_train_sm)\nres = logreg.fit(X_train_sm,y_train)\ncalculateVif(X_train_sm)","59e68675":"#Lets drop the column with highest VIF\nX_train_sm.drop(\"internet_user\", axis=1, inplace=True)\n#res = getLogisticRegStatSummary(X_train_sm)\nres = logreg.fit(X_train_sm,y_train)\ncalculateVif(X_train_sm)","f911c778":"#Lets drop the column with highest VIF\nX_train_sm.drop(\"loc_og_t2m_mou_8\", axis=1, inplace=True)\n#res = getLogisticRegStatSummary(X_train_sm)\nres = logreg.fit(X_train_sm,y_train)\ncalculateVif(X_train_sm)","9f96703d":"#Co-efficients of the attributes.\npd_coefficients = pd.concat([pd.DataFrame(X_train_sm.columns), pd.DataFrame(np.transpose(logreg.coef_))], axis=1)\npd_coefficients.columns = [\"Feature\", \"Co-efficient\"]\npd_coefficients","d49f5289":"##Lets stop converging since both p-value and vif are in acceptable range\n#y_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred = res.predict_proba(X_train_sm)[:,1]\ny_train_pred[:10]","fb162b38":"y_train_pred_final = df_predictions(y_train, y_train_pred, 0.5)\n\n# Let's see the head\nprint(y_train_pred_final[y_train_pred_final['churn']==0].head())\nprint(y_train_pred_final[y_train_pred_final['churn']==1].head())\n\ndraw_roc(y_train_pred_final.churn, y_train_pred_final.churn_Prob)","9fd2da8b":"# Let's plot accuracy sensitivity and specificity for various probabilities.\nplot_cutoff_df(y_train_pred_final)","e1b07eae":"final_cutoff = 0.45\ny_train_pred_final['churn_predicted'] = y_train_pred_final.churn_Prob.map( lambda x: 1 if x > final_cutoff else 0)\ny_train_pred_final.head()","41ab64a5":"model_eval(y_train_pred_final)","b73a24ed":"X_train_sm.columns","34e13947":"#Predicting on test data \n#X_test_sm = sm.add_constant(X_test)\nX_test_sm = X_test[X_train_sm.columns]\n#y_test_pred = res.predict(X_test).values.reshape(-1)\ny_test_pred = res.predict_proba(X_test_sm)[:,1]\ny_test_pred","a37b014f":"y_test_pred_final = df_predictions(y_test, y_test_pred, final_cutoff)\n\n# Let's see the head\nprint(y_test_pred_final[y_test_pred_final['churn']==0].head())\nprint(y_test_pred_final[y_test_pred_final['churn']==1].head())","64b45661":"model_eval(y_test_pred_final)","a64da2f4":"from sklearn.decomposition import PCA\nfrom sklearn.decomposition import IncrementalPCA\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.pipeline import Pipeline","8a2b682c":"# create folds\nfolds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 0)\n\nLogistic_grid = GridSearchCV(\n    Pipeline([\n        ('reduce_dim', PCA()),\n        ('classify', LogisticRegression(class_weight='balanced'))\n        ]),\n    param_grid=[\n        {\n            'reduce_dim__n_components': range(20,40,4),\n            'classify__C': np.logspace(-4, 4, 4)\n        }\n    ],\n    cv=folds, scoring='roc_auc')","a0d069dc":"Logistic_grid.fit(X_train, y_train)","ef5a8293":"print(\"PCA \",Logistic_grid.best_estimator_.named_steps['classify'])\nprint(\"\\nBest params \",Logistic_grid.best_params_)\nprint(\"\\nBest score (CV score=%0.3f):\" % Logistic_grid.best_score_)","35123d9b":"var_cumu = np.cumsum(Logistic_grid.best_estimator_.named_steps['reduce_dim'].explained_variance_ratio_)\nfig = plt.figure(figsize=[12,8])\nplt.vlines(x=36, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\nplt.hlines(y=0.96, xmax=45, xmin=0, colors=\"g\", linestyles=\"--\")\nplt.plot(var_cumu)\nplt.ylabel(\"Cumulative variance explained\")\nplt.show()","fe507387":"pca = IncrementalPCA(n_components=36)\ndf_train_pca = pca.fit_transform(X_train)\ndf_test_pca = pca.transform(X_test)","6ea59be2":"print(df_train_pca.shape)\nprint(df_test_pca.shape)\nprint(pca.components_)\npca.explained_variance_ratio_","c47467f9":"corrmat = np.corrcoef(df_train_pca.transpose())\nprint(corrmat.shape)\nplt.figure(figsize=[15,15])\nsns.heatmap(corrmat, annot=True)","6da1bc54":"learner_pca = LogisticRegression(class_weight='balanced')\nmodel_pca = learner_pca.fit(df_train_pca, y_train)","48263314":"y_train_pred = model_pca.predict_proba(df_train_pca)[:,1]\n\ny_train_pred_final = df_predictions(y_train, y_train_pred, 0.5)\n\ndraw_roc(y_train_pred_final.churn, y_train_pred_final.churn_Prob)","e0beea84":"# Let's plot accuracy sensitivity and specificity for various probabilities.\nplot_cutoff_df(y_train_pred_final)","824148d4":"final_cutoff = 0.45\ny_train_pred_final['churn_predicted'] = y_train_pred_final.churn_Prob.map( lambda x: 1 if x > final_cutoff else 0)\n# Let's see the head\nprint(y_train_pred_final[y_train_pred_final['churn']==0].head())\nprint(y_train_pred_final[y_train_pred_final['churn']==1].head())","61c8f8e6":"model_eval(y_train_pred_final)","1d668505":"y_test_pred = model_pca.predict_proba(df_test_pca)[:,1]\ny_test_pred","0aa72e57":"\"{:2.2}\".format(metrics.roc_auc_score(y_test, y_test_pred))","4bd89e4d":"y_test_pred_final = df_predictions(y_test, y_test_pred, final_cutoff)\n\n# Let's see the head\nprint(y_test_pred_final[y_test_pred_final['churn']==0].head())\nprint(y_test_pred_final[y_test_pred_final['churn']==1].head())","d8dafd45":"model_eval(y_test_pred_final)","080d2481":"# Importing random forest classifier from sklearn library\nfrom sklearn.ensemble import RandomForestClassifier\n#from imblearn.ensemble import BalancedRandomForestClassifier","46ec48da":"# create folds\nfolds = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 0)\n\nrf_grid = GridSearchCV(\n    Pipeline([\n        ('reduce_dim', PCA()),\n        ('classify', RandomForestClassifier(class_weight='balanced_subsample'))\n        ]),\n    param_grid=[\n        {\n            'reduce_dim__n_components': range(20,40,4),\n            'classify__max_depth': [6,8],\n            'classify__min_samples_leaf': range(200,400,100),\n            'classify__min_samples_split': range(300,500,100),\n            'classify__n_estimators': [10,20,30],\n            'classify__criterion':[\"gini\",\"entropy\"]\n        }\n    ],\n    cv=folds, scoring='roc_auc')","d19b1e6d":"rf_grid.fit(X_train, y_train)","661415cc":"print(\"PCA \",rf_grid.best_estimator_.named_steps['classify'])\nprint(\"\\n\",rf_grid.best_params_)\nprint(\"\\nBest parameter (CV score=%0.3f):\" % rf_grid.best_score_)","914c2915":"pca = IncrementalPCA(n_components=36)\ndf_train_pca = pca.fit_transform(X_train)\ndf_test_pca = pca.transform(X_test)","00e56c80":"rfc = RandomForestClassifier(bootstrap=True,\n                             class_weight='balanced_subsample',\n                             max_depth=6,\n                             criterion='gini',\n                             min_samples_leaf=200, \n                             min_samples_split=400,\n                             n_estimators=30)","b158032b":"# fit\nrfc.fit(df_train_pca,y_train)","a0f9a072":"y_train_pred = rfc.predict_proba(df_train_pca)[:,1]\n\ny_train_pred_final = df_predictions(y_train, y_train_pred, 0.5)\n\ndraw_roc(y_train_pred_final.churn, y_train_pred_final.churn_Prob)","59d1d35e":"# Let's plot accuracy sensitivity and specificity for various probabilities.\nplot_cutoff_df(y_train_pred_final)","8bd226be":"final_cutoff = 0.4\ny_train_pred_final['churn_predicted'] = y_train_pred_final.churn_Prob.map( lambda x: 1 if x > final_cutoff else 0)\n# Let's see the head\nprint(y_train_pred_final[y_train_pred_final['churn']==0].head())\nprint(y_train_pred_final[y_train_pred_final['churn']==1].head())","079a7b40":"model_eval(y_train_pred_final)","e8e40133":"y_test_pred = rfc.predict_proba(df_test_pca)[:, 1]\ny_test_pred","b7e1571f":"y_test_pred_final = df_predictions(y_test, y_test_pred, final_cutoff)\n\n# Let's see the head\nprint(y_test_pred_final[y_test_pred_final['churn']==0].head())\nprint(y_test_pred_final[y_test_pred_final['churn']==1].head())","08e128d8":"model_eval(y_test_pred_final)","f3bcacec":"#Installinh XGBoost:Comment out if already installed\n#pip install xgboost","cb351b54":"#Importing xgboost package\nimport xgboost as xgb","dea87bc1":"# create folds\nfolds = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 0)\n\nxgBoost_grid = GridSearchCV(\n    Pipeline([\n        ('reduce_dim', PCA()),\n        ('classify', xgb.XGBClassifier(random_state=42, scale_pos_weight = 11))\n        ]),\n    param_grid=[\n        {\n         'reduce_dim__n_components': range(20,40,4),  \n         'classify__objective':['binary:logistic'],\n         'classify__learning_rate': [0.001,0.05,0.1, 10], \n         'classify__max_depth': [2,3],\n         'classify__min_child_weight': [35],\n         'classify__subsample': [0.8],\n         'classify__colsample_bytree': [0.7],\n         'classify__n_estimators': [35]}\n    ],\n    cv=folds, scoring='roc_auc')\n","e8c5ed93":"xgBoost_grid.fit(X_train, y_train)","28e5e4a4":"print(\"PCA \",xgBoost_grid.best_estimator_.named_steps['classify'])\nprint(\"\\n\",xgBoost_grid.best_params_)\nprint(\"\\nBest parameter (CV score=%0.3f):\" % xgBoost_grid.best_score_)","e8483089":"pca = IncrementalPCA(n_components=36)\ndf_train_pca = pca.fit_transform(X_train)\ndf_test_pca = pca.transform(X_test)","0f4a992b":"xgb_model = xgb.XGBClassifier(objective = 'binary:logistic',\n              learning_rate= 0.1, \n              max_depth= 3,\n              min_child_weight= 35,\n              subsample= 0.8,\n              colsample_bytree= 0.7,\n              n_estimators= 35,\n              random_state= 42,\n              scale_pos_weight = 11)","ee99b4ba":"xgb_model.fit(df_train_pca,y_train)","8fd08229":"y_train_pred = xgb_model.predict_proba(df_train_pca)[:,1]\n\ny_train_pred_final = df_predictions(y_train, y_train_pred, 0.5)\n\ndraw_roc(y_train_pred_final.churn, y_train_pred_final.churn_Prob)","fe8d625d":"# Let's plot accuracy sensitivity and specificity for various probabilities.\nplot_cutoff_df(y_train_pred_final)","b9800fe9":"final_cutoff = 0.45\ny_train_pred_final['churn_predicted'] = y_train_pred_final.churn_Prob.map( lambda x: 1 if x > final_cutoff else 0)\n# Let's see the head\nprint(y_train_pred_final[y_train_pred_final['churn']==0].head())\nprint(y_train_pred_final[y_train_pred_final['churn']==1].head())","19a5e500":"model_eval(y_train_pred_final)","5772360e":"y_test_pred = xgb_model.predict_proba(df_test_pca)[:, 1]\ny_test_pred","990342a9":"##### Create a dataframe to make predictions for PCA+XGBoost Forest","2a0c192a":"y_test_pred_final = df_predictions(y_test, y_test_pred, final_cutoff)\n\n# Let's see the head\nprint(y_test_pred_final[y_test_pred_final['churn']==0].head())\nprint(y_test_pred_final[y_test_pred_final['churn']==1].head())","a67fa092":"model_eval(y_test_pred_final)","b1c9c2be":"pd_coefficients","d960fc30":"## Data Modeling\n\nBuild models to predict churn. The predictive model that you\u2019re going to build will serve two purposes:\n\nIt will be used to predict whether a high-value customer will churn or not, in near future (i.e. churn phase). By knowing this, the company can take action steps such as providing special plans, discounts on recharge etc.\n\nIt will be used to identify important variables that are strong predictors of churn. These variables may also indicate why customers choose to switch to other networks.","07be3c6b":"#### Based upon above scree plot, consider \"n\" PCA components","09d94aee":"### Rename few of the attributes for consistency","b6232403":"1. Roaming and STD users are more likely to churn. This was clearly highlighted in the EDA and the logistic regression model coverged down to include these features with a positive co-efficients .\n\n2. Reduced usage of internet volume and internet plans are also one of the main reasons that why users are churning. Reduced or usage values=0 in the 8th month as compared to last 2 months are also causes of concern.\n\n3. New customers are more likely to churn.Customers whose aon(No of days with the network) is low are the ones churning more when compared to customers who are with the network since long.The observation was captured in the EDA.\n\n4. Reduced or recharges values=0 in the 8th month as compared to last 2 months are also causes of concern.\n\n5. Reduced minutes of usages across all categories are also causes of concern.\n\n6. There can be seen a negative coefficient in logistic regression internet vol attributes indicating a decrease in internet   consumption in the 8 month contributing to the churn.\n\n","e7bc6472":"##### Model Evaluation for Logistic Regression on Train Data","f8d0da40":"#### Analysis of minutes of usage columns","8f5256c2":"##### Model Evaluation for PCA+XGBoost on test data","da191b03":"## Understanding and Defining Churn\n\nThere are two main models of payment in the telecom industry - postpaid (customers pay a monthly\/annual bill after using the services) and prepaid (customers pay\/recharge with a certain amount in advance and then use the services).\n\nIn the postpaid model, when customers want to switch to another operator, they usually inform the existing operator to terminate the services, and you directly know that this is an instance of churn.\n\nHowever, in the prepaid model, customers who want to switch to another network can simply stop using the services without any notice, and it is hard to know whether someone has actually churned or is simply not using the services temporarily (e.g. someone may be on a trip abroad for a month or two and then intend to resume using the services again).\n\nThus, churn prediction is usually more critical (and non-trivial) for prepaid customers, and the term \u2018churn\u2019 should be defined carefully.  Also, prepaid is the most common model in India and southeast Asia, while postpaid is more common in Europe in North America.\n\nThis project is based on the Indian and Southeast Asian market.","f7b098cb":"##### Model Evaluation for Logistic Regression on Test Data","57816151":"### Data Modeling using Logistic Regression without PCA","4e1f74de":"#### Derive average value columns of 6th and 7th months (action months) and diff value columns\n\nAlso drop the 6th and 7th months columns since they are depicted by a corresponding single column.\n\nFor example derive avg_arpu_6_7 from arpu_6 and arpu_7 and then drop arpu_6 and arpu_7.\n\nDiff column = arpu_8 - avg_arpu_6_7","ee83c84f":"#### Analyze aon (age on network) feature","e8baaf03":"## Reading and Understanding the Data\n","5f5810d8":"## Understanding Customer Behaviour During Churn\n\nCustomers usually do not decide to switch to another competitor instantly, but rather over a period of time (this is especially applicable to high-value customers). In churn prediction, we assume that there are three phases of customer lifecycle :\n\nThe \u2018good\u2019 phase: In this phase, the customer is happy with the service and behaves as usual.\n\nThe \u2018action\u2019 phase: The customer experience starts to sore in this phase, for e.g. he\/she gets a compelling offer from a  competitor, faces unjust charges, becomes unhappy with service quality etc. In this phase, the customer usually shows different behaviour than the \u2018good\u2019 months. Also, it is crucial to identify high-churn-risk customers in this phase, since some corrective actions can be taken at this point (such as matching the competitor\u2019s offer\/improving the service quality etc.)\n\nThe \u2018churn\u2019 phase: In this phase, the customer is said to have churned. You define churn based on this phase. Also, it is important to note that at the time of prediction (i.e. the action months), this data is not available to you for prediction. Thus, after tagging churn as 1\/0 based on this phase, you discard all data corresponding to this phase.\n\n \n\nIn this case, since you are working over a four-month window, the first two months are the \u2018good\u2019 phase, the third month is the \u2018action\u2019 phase, while the fourth month is the \u2018churn\u2019 phase.","ec7e3da1":"All the distribution plots are right skewed","c54e714f":"## High Value Churn\n\nIn the Indian and the southeast Asian market, approximately 80% of revenue comes from the top 20% customers (called high-value customers). Thus, if we can reduce churn of the high-value customers, we will be able to reduce significant revenue leakage.\n\nIn this project, we will define high-value customers based on a certain metric (mentioned later below) and predict churn only on high-value customers.","71658cde":"#### Apply XG Boost on PCA components","c4f82951":"## Understanding the dataset\n\nThe dataset contains customer-level information for a span of four consecutive months - June, July, August and September. The months are encoded as 6, 7, 8 and 9, respectively. \n\nThe business objective is to predict the churn in the last (i.e. the ninth) month using the data (features) from the first three months. To do this task well, understanding the typical customer behaviour during churn will be helpful.","9299895f":"### Removing high NULL value features\/columns\n\nDrop features\/columns that have more than 60% NULL values","cef79550":"## Definitions of Churn\n\nThere are various ways to define churn, such as:\n\nRevenue-based churn: Customers who have not utilised any revenue-generating facilities such as mobile internet, outgoing calls, SMS etc. over a given period of time. One could also use aggregate metrics such as \u2018customers who have generated less than INR 4 per month in total\/average\/median revenue\u2019. \nThe main shortcoming of this definition is that there are customers who only receive calls\/SMSes from their wage-earning counterparts, i.e. they don\u2019t generate revenue but use the services. For example, many users in rural areas only receive calls from their wage-earning siblings in urban areas. \n\nUsage-based churn: Customers who have not done any usage, either incoming or outgoing - in terms of calls, internet etc. over a period of time. \nA potential shortcoming of this definition is that when the customer has stopped using the services for a while, it may be too late to take any corrective actions to retain them. For e.g., if you define churn based on a \u2018two-months zero usage\u2019 period, predicting churn could be useless since by that time the customer would have already switched to another operator.\n\nIn this project, we will use the usage-based definition to define churn.","691638d4":"#### Analyze all the recharge columns","f275c4fe":"#### Fitting the model with best parameters obtained from grid search","34ff8f7e":"## Recommend strategies to manage customer churn\n","319ca52f":"Inference ->\n\nAmong the churn internet users, around 75% churn when internet usage changes to 0 as compared to previous months.\n\nWhereas among the non churn internet users, 40% internet usage becomes 0.","70ae790e":"1. As seen from the above potential areas causing churn include STD users and roaming users , therefore a more focus in the area    of STD and roaming in lines of pricing\/offer is required.\n2. It is very clear from the models and EDA that new customers are tending more to churn, hence new customers to the network        should be focused more and should be given better offers.\n3. It is also observed that the volume consumption in both 2g and 3g are dropping in the action month of churn users ,              indicating the customers disatisfaction in the internet services offered to them. To the potential customers a better  \n   internet plan can be offered to make the customer stay back.","25d11545":"##### Model Evaluation for PCA+Logistic Regression on Test Data","8f281727":"### Tag churners\n\nNow tag the churned customers (churn=1, else 0) based on the fourth month as follows: Those who have not made any calls (either incoming or outgoing) AND have not used mobile internet even once in the churn phase. The attributes you need to use to tag churners are:\n\ntotal_ic_mou_9\ntotal_og_mou_9\nvol_2g_mb_9\nvol_3g_mb_9","74547bb4":"### Splitting Data into Training and Test Sets","b9bff9ba":"#### Analyze arpu (average revenue per user) column","c70b4359":"### Checking the correlation between the features","cc8d2ded":"### Remove all the attributes corresponding to the churn phase\n\nAfter tagging churners, remove all the attributes corresponding to the churn phase (all attributes having \u2018 _9\u2019, etc. in their names).","c3aa3cfc":"### Data Modeling using PCA","1b9c1270":"## Data Preparation and Cleaning\n","69e7edff":"#### Strategies to withold the customers","b4136359":"##### Fitting the final model with the best parameters obtained from grid search.","9baa2591":"#### Analyze Internet Usage columns","eb5ea67c":"##### Create a dataframe to make predictions for PCA+Random Forest","8364ab4c":"### Common functions for all models","4cd914dc":"### Filter high-value customers\n\nWe need to predict churn only for the high-value customers. \nDefine high-value customers as follows: Those who have recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months (the good phase).","0b215e7a":"### Normalize the dataset","def88901":"### Drop date columns","29052c4c":"#### Apply Random Forest on PCA components","9fce9737":"##### Based upon above graph; decide the probability cutoff for churn label","37371c1e":"#### Apply Logistic Regression on PCA components","9f011f12":"##### Based upon above graph; decide the probability cutoff for churn label","741b23fb":"#### Analyze roaming mou columns","5b8a9724":"##### Model Evaluation for PCA+Random Forest on Train Data","f24c2e86":"#### Potential reasons for churn","4dec05ea":"Inference ->\n\nAmong the churn users, almost 74% are new customers whose aon is less than ~1000 days.\n\nWhereas in non churn users, around 51% are new customers whose aon is less than ~1000 days.\n\nDrop aon as the derived customer_category will be used for modeling","8ceb52af":"##### Based upon above graph; decide the probability cutoff for churn label","d524db46":"## Imports\n","6cfd738e":"### Outlier treatment","a877716a":"Inference ->\n\nAmong the churn users, around 56% churn when STD usage is 0 as compared to previous months.\n\nWhereas among the non churn users, 15% user STD usage becomes 0.","7676cf8e":"Inference ->\n\nAmong the churn users, around 69% are roaming users.\n\nWhereas among the non churn users, around 37% are roaming users.","78848efa":"##### Model Evaluation for PCA+XGBoost on Train Data","b57935ca":"#### Analyze STD Usage minutes","41b73110":"#### Check your derived variables","d979aaa9":"#### Making a scree plot for the explained variance and components","6857a47d":"##### Model Evaluation for PCA+Random Forest on test data","a120879e":"### Drop columns which have only one unique value","11257798":"Inference ->\n\nAmong the churn users, around 70% churn when recharges are 0 as compared to previous months.\n\nWhereas among the non churn users, 20% recharge usage becomes 0.","9235d38d":"### Derived Features along with Univariate and Bivariate Analysis","84346097":"## Business Problem Overview\n\nIn the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n\nFor many incumbent operators, retaining high profitable customers is the number one business goal.\n\nTo reduce customer churn, telecom companies need to predict which customers are at high risk of churn.\n\nIn this project, you will analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn.","2c6409d5":"##### Model Evaluation for Logistic Regression on Train Data","8dd8476b":"##### Based upon above graph; decide the probability cutoff for churn label","8179e1ba":"### Impute missing values"}}