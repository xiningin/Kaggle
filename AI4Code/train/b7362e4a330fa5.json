{"cell_type":{"814b27d3":"code","3eddb110":"code","c3cf9f62":"code","924f3e17":"code","eb4237ee":"code","792371e5":"code","a78a4342":"code","78d5e005":"code","b6bf5288":"code","c070e011":"code","2ddd284f":"markdown","dab28ccc":"markdown"},"source":{"814b27d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3eddb110":"from sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt","c3cf9f62":"dataset=make_blobs(n_samples=200, n_features=2, centers=3, cluster_std=1.5, random_state=42)","924f3e17":"dataset #Randomly Generated Dataset wherein we are intersted in Features and not the target variable","eb4237ee":"dp=dataset[0] #Considering only Features\ndp","792371e5":"import scipy.cluster.hierarchy as sch \nfrom sklearn.cluster import AgglomerativeClustering\nimport plotly.express as px\nimport cufflinks as cf\nfrom plotly.offline import init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\ncf.go_offline()","a78a4342":"#Creating a Dendrogram\naxes,fig=plt.subplots(figsize=[15,10])\ndend=sch.dendrogram(sch.linkage(dp,method='ward'))","78d5e005":"px.scatter(dp[:,0],dp[:,1])","b6bf5288":"HC=AgglomerativeClustering(n_clusters=4, affinity='euclidean',linkage='ward')\nclust=HC.fit_predict(dp)","c070e011":"axes,fig=plt.subplots(figsize=[15,10])\nplt.scatter(dp[clust==0,0],dp[clust==0,1],s=50,c='blue')\nplt.scatter(dp[clust==1,0],dp[clust==1,1],s=50,c='yellow')\nplt.scatter(dp[clust==2,0],dp[clust==2,1],s=50,c='red')\nplt.scatter(dp[clust==3,0],dp[clust==3,1],s=50,c='green')\n\nplt.show()","2ddd284f":"## Importing Libraries for Hierarchical Clustering and building Dendograms","dab28ccc":"### The make_blobs function is a part of sklearn.datasets.samples_generator. All methods in the package, help us to generate data samples or datasets. In machine learning, which scikit-learn all about, datasets are used to evaluate performance of machine learning models.  n_features determined how many columns or features the generated datasets will have. In machine learning, features correspond to numerical characteristics data. For example, in Iris Dataset, there are 4 features (Sepal Length, Sepal Width, Petal Length and Petal Width) so there are 4 numerical columns in the dataset. So by increasing n_features in make_blobs, we are adding more features hence increase the complexity of generated dataset. centers parameter corresponds to number of classes generated in the data."}}