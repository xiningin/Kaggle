{"cell_type":{"2daf5e62":"code","c5325989":"code","fea5f014":"code","c056039d":"code","e448ddaa":"code","f6037156":"code","4802cdb1":"code","9f72f3c0":"code","40b184aa":"code","bfdb0458":"code","e2833150":"code","0e33df2d":"code","ec224df9":"code","7259ae6e":"code","01af5a4c":"code","bed091d2":"code","24302758":"code","a41dc2bb":"code","e3e6b7ee":"code","09df9b96":"code","bbea2032":"code","dcc8177a":"code","e6d477c3":"code","bbfebd95":"code","a92909d3":"code","22934700":"code","028398ad":"code","72040ef7":"code","b22187c0":"code","48d36857":"code","90705a9b":"code","da008f16":"code","c87ed0b5":"code","0ca1523f":"code","00b133f2":"code","5945654c":"code","776466b4":"code","ff962b2b":"code","9f86f3ec":"code","779c639e":"code","05ab5fba":"code","db77c663":"code","236caed6":"code","993d791f":"code","8c554633":"code","e562a54a":"code","94cc6fb4":"code","62a0baf2":"code","0f2abaf4":"markdown","9dcea2dc":"markdown","4b04f710":"markdown","53db615e":"markdown","7c2426ad":"markdown","7921f87e":"markdown","ab1bb4ae":"markdown","9c097496":"markdown","34d56216":"markdown","02fbaa44":"markdown","cbc3dd05":"markdown","a5e55911":"markdown","1815f51c":"markdown","407b662d":"markdown","a708d867":"markdown","4c25992c":"markdown","63e15c97":"markdown","9e91b7b9":"markdown","eaa25dd4":"markdown","42b19375":"markdown","754a7026":"markdown","e874dc24":"markdown","358e5b72":"markdown","4f85ccdd":"markdown","cb1d90a9":"markdown"},"source":{"2daf5e62":"# Make sure to install torch and torchvisionif running locally on jupyter notebook","c5325989":"!pip install opendatasets \nimport os\nimport torch\nimport torchvision\nfrom torchvision.datasets.utils import download_url\nfrom torch.utils.data import random_split\nimport opendatasets as od","fea5f014":"# uncomment if running on platform other than kaggle then would be required to enter kaggle credentials\n# also then remove '\/kaggle\/input\/' from data_dir\n# # Dowload the dataset\n# od.download('https:\/\/www.kaggle.com\/c\/fake-image-classification-challenge')","c056039d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e448ddaa":"os.listdir('\/kaggle\/input\/fake-image-classification-challenge')","f6037156":"os.listdir('\/kaggle\/input\/fake-image-classification-challenge\/data')","4802cdb1":"# uncomment this if running on any other platform\n# data_dir = 'fake-image-classification-challenge\/data'\ndata_dir = '\/kaggle\/input\/fake-image-classification-challenge\/data'\n\nprint(os.listdir(data_dir))\nclasses = os.listdir(data_dir + \"\/train\")\nprint(classes)","9f72f3c0":"fake_face_files = os.listdir(data_dir + \"\/train\/fake\")\nprint('No. of training examples for fake faces:', len(fake_face_files))\nprint(fake_face_files[:5])","40b184aa":"real_face_files = os.listdir(data_dir + \"\/train\/real\")\nprint('No. of training examples for real faces:', len(real_face_files))\nprint(real_face_files[:5])","bfdb0458":"from torchvision.datasets import ImageFolder\nimport torchvision.transforms as tt\n\ndataset = ImageFolder(data_dir+'\/train', tt.Compose([tt.Resize(142), \n                                                     tt.RandomHorizontalFlip(), \n                                                      #tt.RandomResizedCrop(64, scale=(0.5,0.9), ratio=(1, 1)), \n                                            #tt.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n                                            #tt.RandomCrop(64, padding=4, padding_mode='reflect'),\n                                            tt.ToTensor(),\n                                            tt.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n                                            ]))","e2833150":"img, label = dataset[0]\nprint(img.shape, label)\nimg","0e33df2d":"classes_here=dataset.classes\nprint(classes_here)","ec224df9":"import matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nmatplotlib.rcParams['figure.facecolor'] = '#ffffff'","7259ae6e":"def show_example(img, label):\n    print('Label: ', dataset.classes[label], \"(\"+str(label)+\")\")\n    plt.imshow(img.permute(1, 2, 0))","01af5a4c":"# used *arg here for -> img,label=dataset[0] -> shortcut\nshow_example(*dataset[0])","bed091d2":"show_example(*dataset[1099])","24302758":"show_example(*dataset[17999])","a41dc2bb":"random_seed = 43\ntorch.manual_seed(random_seed);","e3e6b7ee":"val_size = 1800\ntrain_size = len(dataset) - val_size\n\ntrain_ds, val_ds = random_split(dataset, [train_size, val_size])\nlen(train_ds), len(val_ds)","09df9b96":"from torch.utils.data.dataloader import DataLoader\n\nbatch_size=85","bbea2032":"train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\nval_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)","dcc8177a":"from torchvision.utils import make_grid\nstats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n\n# to remove normalized pixels\ndef denormalize(images, means, stds):\n    means = torch.tensor(means).reshape(1, 3, 1, 1)\n    stds = torch.tensor(stds).reshape(1, 3, 1, 1)\n    return images * stds + means\n\ndef show_batch(dl):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(12, 12))\n        ax.set_xticks([]); ax.set_yticks([])\n        denorm_images = denormalize(images, *stats)\n        ax.imshow(make_grid(denorm_images[:64], nrow=8).permute(1, 2, 0).clamp(0,1))\n        break","e6d477c3":"show_batch(train_dl)","bbfebd95":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","a92909d3":"device = get_default_device()\ndevice","22934700":"train_dl = DeviceDataLoader(train_dl, device)\nval_dl = DeviceDataLoader(val_dl, device)","028398ad":"import torch.nn as nn\nimport torch.nn.functional as F","72040ef7":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))","b22187c0":"# def conv_block(in_channels, out_channels, pool=False):\n#     layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n#               nn.BatchNorm2d(out_channels), \n#               nn.ReLU(inplace=True)]\n#     if pool: layers.append(nn.MaxPool2d(2))\n#     return nn.Sequential(*layers)\n\n# class ResNet9(ImageClassificationBase):\n#     def __init__(self, in_channels, num_classes):\n#         super().__init__()\n        \n#         self.conv1 = conv_block(in_channels, 64) # output: 64 x 128 x 128\n#         self.conv2 = conv_block(64, 128, pool=True) # output: 128 x 64 x 64 ; as pool is true\n#         self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n        \n#         self.conv3 = conv_block(128, 256, pool=True)  # output: 256 x 32 x 32 \n#         self.conv4 = conv_block(256, 512, pool=True)  # output: 512 x 16 x 16 \n#         self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n        \n#         self.classifier = nn.Sequential(nn.AdaptiveMaxPool2d(1), \n#                                         nn.Flatten(), \n#                                         nn.Dropout(0.2),\n#                                         nn.Linear(512, num_classes)) # output: 512 x 1 x 1 \n        \n#     def forward(self, xb):\n#         out = self.conv1(xb)\n#         out = self.conv2(out)\n#         out = self.res1(out) + out\n#         out = self.conv3(out)\n#         out = self.conv4(out)\n#         out = self.res2(out) + out\n#         out = self.classifier(out)\n#         return out","48d36857":"def conv_block(in_channels, out_channels, pool=False):\n    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n              nn.BatchNorm2d(out_channels), \n              nn.ReLU(inplace=True)]\n    if pool: layers.append(nn.MaxPool2d(2))\n    return nn.Sequential(*layers)\n\nclass ResNet9(ImageClassificationBase):\n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n        \n        self.conv1 = conv_block(in_channels, 34) # output: 64 x 128 x 128\n        self.conv2 = conv_block(34, 64, pool=True) # output: 128 x 64 x 64 ; as pool is true\n        self.res1 = nn.Sequential(conv_block(64, 64), conv_block(64, 64))\n        \n        self.conv3 = conv_block(64, 128)\n        self.conv4 = conv_block(128,256, pool=True)\n        self.res2 = nn.Sequential(conv_block(256,256), conv_block(256,256))\n        \n        self.conv5 = conv_block(256,512)  # output: 256 x 32 x 32 \n        self.conv6 = conv_block(512, 512, pool=True)  # output: 512 x 16 x 16 \n        self.res3 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n        \n        self.classifier = nn.Sequential(nn.AdaptiveMaxPool2d(1), \n                                        nn.Flatten(), \n                                        nn.Dropout(0.2),\n                                        nn.Linear(512, num_classes)) # output: 512 x 1 x 1 \n        \n    def forward(self, xb):\n        out = self.conv1(xb)\n        out = self.conv2(out)\n        out = self.res1(out) + out\n        out = self.conv3(out)\n        out = self.conv4(out)\n        out = self.res2(out) + out\n        out = self.conv5(out)\n        out = self.conv6(out)\n        out = self.res3(out) + out\n        out = self.classifier(out)\n        return out","90705a9b":"model = to_device(ResNet9(3, 2), device)\nmodel","da008f16":"torch.cuda.empty_cache()\nfor images, labels in train_dl:\n    print('images.shape:', images.shape)\n    out = model(images)\n    print('out.shape:', out.shape)\n    print('out[0]:', out[0])\n    break","c87ed0b5":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n    torch.cuda.empty_cache()\n    history = []\n    \n    # Set up cutom optimizer with weight decay\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    # Set up one-cycle learning rate scheduler\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n                                                steps_per_epoch=len(train_loader))\n    \n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        lrs = []\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            \n            # Gradient clipping\n            if grad_clip: \n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n            \n            optimizer.step()\n            optimizer.zero_grad()\n            \n            # Record & update learning rate\n            lrs.append(get_lr(optimizer))\n            sched.step()\n        \n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","0ca1523f":"history = [evaluate(model, val_dl)]\nhistory","00b133f2":"epochs = 12\nmax_lr = 0.001 # best1 for lr=0.0001 and size: 64 x 64 and batch size=50 over 99% GPU usage\ngrad_clip = 0.1\nweight_decay = 1e-4\nopt_func = torch.optim.Adam","5945654c":"%%time\nhistory += fit_one_cycle(epochs, max_lr, model, train_dl, val_dl, grad_clip=grad_clip, weight_decay=weight_decay, opt_func=opt_func)\n# time= 20min 30s","776466b4":"# %%time\n# history += fit_one_cycle(12, 0.00001, model, train_dl, val_dl, grad_clip=grad_clip, weight_decay=weight_decay, opt_func=opt_func)","ff962b2b":"%%time\nhistory += fit_one_cycle(25, max_lr, model, train_dl, val_dl, grad_clip=grad_clip, weight_decay=weight_decay, opt_func=opt_func)","9f86f3ec":"# %%time\n# history += fit_one_cycle(12, 0.0001, model, train_dl, val_dl, grad_clip=grad_clip, weight_decay=weight_decay, opt_func=opt_func)","779c639e":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');","05ab5fba":"plot_accuracies(history)","db77c663":"def plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');","236caed6":"plot_losses(history)","993d791f":"import numpy as np\ndef plot_lrs(history):\n    lrs = np.concatenate([x.get('lrs', []) for x in history])\n    plt.plot(lrs)\n    plt.xlabel('Batch no.')\n    plt.ylabel('Learning rate')\n    plt.title('Learning Rate vs. Batch no.');\nplot_lrs(history)","8c554633":"def predict_image(img, model):\n    # Convert to a batch of 1\n    xb = to_device(img.unsqueeze(0), device)\n    # Get predictions from model\n    yb = model(xb)\n    # Pick index with highest probability\n    _, preds  = torch.max(yb, dim=1)\n    # Retrieve the class label\n    return dataset.classes[preds[0].item()]","e562a54a":"img, label = val_ds[0]\nplt.imshow(img.permute(1, 2, 0).clamp(0, 1))\nprint('Label:', classes_here[label], ', Predicted:', predict_image(img, model))","94cc6fb4":"%config Completer.use_jedi = False","62a0baf2":"import pandas as pd\nfrom PIL import Image\n\n# test images directory\ntest_dir=data_dir+'\/test'\nsubmission=pd.read_csv(data_dir+\"\/sample_submission.csv\")\nfinal_submission=submission.copy()\n\n# to trabsform image\ntransforms_test=tt.Compose([tt.Resize(142), \n                            tt.RandomHorizontalFlip(), \n                             #tt.RandomResizedCrop(64, scale=(0.5,0.9), ratio=(1, 1)), \n                             #tt.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n                             #tt.RandomCrop(64, padding=4, padding_mode='reflect'),\n                             tt.ToTensor(),\n                             tt.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n                                            ])\n\nfor img in os.listdir(test_dir):\n    img_str=img\n    address=test_dir+\"\/\"+str(img)\n    img=Image.open(address)\n    img=transforms_test(img)\n    prediction=predict_image(img, model)\n    final_submission.loc[final_submission['path']=='data\/test\/'+str(img_str),'label']=str(prediction)\n    \n    # to print image uncomment this\n    #plt.imshow(img.permute(1, 2, 0).clamp(0, 1))\n    #print('Label:', classes_here[label], ', Predicted:', predict_image(img, model))\n    \n    \nfinal_submission.to_csv('Submit_me.csv', index=False)\n\n\n\n\n\n\n#test_dataset = ImageFolder(data_dir+'\/test', )","0f2abaf4":"***AIM: To classfiy given test image as real(1) or fake(0).***","9dcea2dc":"## Training and Validation Datasets\n\nWhile building real world machine learning models, it is quite common to split the dataset into 3 parts:\n\n1. **Training set** - used to train the model i.e. compute the loss and adjust the weights of the model using gradient descent.\n2. **Validation set** - used to evaluate the model while training, adjust hyperparameters (learning rate etc.) and pick the best version of the model.\n3. **Test set** - used to compare different models, or different types of modeling approaches, and report the final accuracy of the model.\n\nSince there's no predefined validation set, we can set aside a small portion (18000 images) of the training set to be used as the validation set. We'll use the `random_split` helper method from PyTorch to do this. To ensure that we always create the same validation set, we'll also set a seed for the random number generator.","4b04f710":"The above directory structure (one folder per class) is used by many computer vision datasets, and most deep learning libraries provide utilites for working with such datasets. We can use the `ImageFolder` class from `torchvision` to load the data as PyTorch tensors.","53db615e":"We can now wrap our training and validation data loaders using `DeviceDataLoader` for automatically transferring batches of data to the GPU (if available), and use `to_device` to move our model to the GPU (if available).","7c2426ad":"![](https:\/\/3.bp.blogspot.com\/-AADRULtzSHc\/XHzFyxMxx4I\/AAAAAAAAB3c\/Yl-4CDVSuwQKwQwsCkrWWlYJ9lzTwghigCLcBGAs\/s1600\/ai-images-news.jpg)","7921f87e":"The initial accuracy is around 50%, which is what one might expect from a randomly intialized model (since it has a 1 in 2 chance of getting a label right by guessing randomly).\n\nWe'll use the following *hyperparmeters* (learning rate, no. of epochs, batch_size etc.) to train our model. As an exercise, you can try changing these to see if you have achieve a higher accuracy in a shorter time. ","ab1bb4ae":"### ***How to run the code***\n**To run the code you would be required to enter your kaggle credentials in order the data set from kaggle using opendatasets module in python otherwise the notebook won't run!**","9c097496":"## Model with Residual Blocks and Batch Normalization\n\nOne of the key changes to our CNN model this time is the addition of the resudial block, which adds the original input back to the output feature map obtained by passing the input through one or more convolutional layers.\n\n![](https:\/\/miro.medium.com\/max\/1140\/1*D0F3UitQ2l5Q0Ak-tjEdJg.png)\n\nSo, lets build our model now:","34d56216":"We can now create data loaders for training and validation, to load the data in batches","02fbaa44":"## Testing with individual images\n\nWhile we have been tracking the overall accuracy of a model so far, it's also a good idea to look at model's results on some sample images. Let's test out our model with some images from the predefined validation dataset of 9000 images.\n","cbc3dd05":"# *Fake Face Image Classification Challenge*\n\n*Is this Face image fake or real?*","a5e55911":"**Downloading and preparing data by applying transforms on PIL images to convert them to required tensors.**","1815f51c":"Before we begin training, let's instantiate the model once again and see how it performs on the validation set with the initial set of parameters.","407b662d":"Let's look at a couple of images from the dataset. As we can tell, the 142x142 px images are quite difficult to identify as fake or real, even for the human eye. Try changing the indices below to view different images.","a708d867":"To seamlessly use a GPU, if one is available, we define a couple of helper functions (`get_default_device` & `to_device`) and a helper class `DeviceDataLoader` to move our model & data to the GPU as required.","4c25992c":"We can look at batches of images from the dataset using the `make_grid` method from `torchvision`. Each time the following code is run, we get a different bach, since the sampler shuffles the indices before creating batches.\n\nLet's take a look at some sample images from the training dataloader. To display the images, we'll need to _denormalize_ the pixels values to bring them back into the range `(0,1)`.","63e15c97":"### ***Introduction***\n\n\n***About*** \n\nThis competition is being organised by the Heuristics Club (a sub-club of KamandPrompt), IIT Mandi for Utkarsh 2021.\n\n\n***Goal of the Competition***\n\nIn the challenge, images of human faces are provided. The goal of this challenge is to classify whether a given image has a real face or a fake face.\n\n\n***Context***\n\nNowadays, with the advent of Generative Adversarial Networks (GANs), there are lots of fake images and videos of people on the internet. These algorithms can generate very accurate and realistic looking images and videos. This is a big issue because it can be used to attack public figures, spread fake news, etc. So, to identify whether any particular piece of content is fake or not is an important problem that needs to be tackled.\n\nSince no algorithm is perfect in this task as of now, so this is an interesting problem to solve with deep learning.\n\n![](https:\/\/www.iuemag.com\/may2013\/images\/iu-emagazine-best-fake-face-dancing-musical-tunes-silly-crowd-smile-joke-happiness-deliver.png)","9e91b7b9":"We can view the image using `matplotlib`, but we need to change the tensor dimensions to `(142,142,3)`. Let's create a helper function to display an image and its label.","eaa25dd4":"*Importing important modules*","42b19375":"After running the model for about 20-30 minutes we extract the best possible accuracy on validation set which is : ~98%.\nNow, lets try to plot how accuracy vs epochs and learning rate vs epoch as shown below:","754a7026":"The list of classes is stored in the `.classes` property of the dataset. The numeric label for each element corresponds to index of the element's label in the list of classes.","e874dc24":"# ***Preparing Final Submission***\n\nMaking prediction on test set and saving prediction to 'submit_me.csv' file for final submission.","358e5b72":"### ***So Lets start***","4f85ccdd":"Let's look at a sample element from the training dataset. Each element is a tuple, containing a image tensor and a label. Since the data consists of 142x142 px color images with 3 channels (RGB), each image tensor has the shape `(3,142,142)`.","cb1d90a9":"## ***Technologies used:***\nIn this notebook, we'll use the following techniques to train a state-of-the-art model in less than 5 minutes to achieve over 97% accuracy in  classifying images from the given dataset:\n\n- Data normalization\n- Data augmentation\n- Residual connections\n- Batch normalization\n- Learning rate scheduling\n- Weight Decay\n- Gradient clipping\n- Adam optimizer"}}