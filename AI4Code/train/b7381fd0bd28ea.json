{"cell_type":{"7203e035":"code","9328f2a3":"code","26e8950b":"code","b433040c":"code","f7d0d3cb":"code","ae679d55":"code","64ade168":"code","153f7db8":"code","bedb57ad":"code","d6cb1071":"code","530f0002":"code","bf80595d":"code","3fd0ae7d":"code","9774ec75":"code","87e13d0e":"code","eaadc5bc":"code","d5d35a89":"code","3d3b8637":"code","3790ac24":"code","758504c7":"code","b1ffca53":"code","a2919066":"code","96b123ff":"code","870618a1":"code","388fac5a":"code","618251f6":"code","873f7f2d":"code","bf5433c1":"code","ce401557":"code","4dc1f2fe":"code","39f7ec69":"code","be61fc4b":"code","2e820421":"code","17863d0b":"code","ec96563f":"code","45698fd3":"code","4abeb9a0":"code","ce65679e":"code","7a3c2fa9":"code","6c3fc6e6":"code","aac40404":"code","e0ad3003":"code","ec150fd2":"code","b1bc8641":"code","8ba25559":"code","06753aca":"code","fbb7d3d0":"code","cd5b6c47":"code","978f1570":"code","894f900d":"code","a5196e11":"code","2226e1b4":"code","e2208df7":"code","27eea1a3":"code","4bd6331b":"code","cdd1007b":"code","440b0c41":"code","2a3bbd9d":"code","64391273":"markdown","d85bcefe":"markdown","ff926e71":"markdown","d332a74e":"markdown","5a614e52":"markdown","04e7bf70":"markdown","14461d34":"markdown","b90bd778":"markdown","ffe1bba2":"markdown","58ae14ad":"markdown","5d810eef":"markdown","3ed1b491":"markdown","bb6c376a":"markdown","74c04927":"markdown","fcca19fe":"markdown","1c62d10d":"markdown","5595d082":"markdown","6a491082":"markdown","f68d3710":"markdown"},"source":{"7203e035":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nltk\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom nltk import PorterStemmer, SnowballStemmer, WordNetLemmatizer \nfrom nltk.corpus import wordnet\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.preprocessing import  MinMaxScaler, RobustScaler, StandardScaler, LabelEncoder as le\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import scale\nfrom sklearn.decomposition import PCA\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV, cross_val_score, StratifiedKFold, learning_curve\n\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","9328f2a3":"train_data = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_data = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsample_submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\n\ntrain_data","26e8950b":"# list down some elements of test data\ntest_data.head(5)","b433040c":"sample_submission.head(5)","f7d0d3cb":"# using the pandas groupby function to calculate samples per labels\n\ncount_data = train_data[['text','target']].groupby('target').count().reset_index()\ncount_data","ae679d55":"# plotting instances per labels\n\nsns.barplot(x = 'target',y = 'text', data = count_data)\nplt.title('no. of instances vs labels')","64ade168":"## merge train and test data to perform common operations on both\n## due to merging an extra column of target will form, which contains nan values\n\ndataset = pd.concat([train_data, test_data])\nprint(dataset.shape)\ndataset.head(5)","153f7db8":"# checking null values in both entire dataset and train data\nprint(dataset.isnull().sum())\nprint('-'*100)\nprint(train_data.isnull().sum())","bedb57ad":"# checking null values in both entire dataset and train data\nprint(dataset.info())\nprint('-'*100)\nprint(train_data.info())","d6cb1071":"dataset['len_letters'] = dataset['text'].apply(len)\ntrain_data['len_letters'] = train_data['text'].apply(len)\ndataset.head(5)","530f0002":"fig,axes = plt.subplots(ncols = 2)\nsns.distplot(train_data['len_letters'][train_data['target'] == 1 ],label = 'disaster',color = 'r' ,ax = axes[0]) ## denoting disaster tweets length\nsns.distplot(train_data['len_letters'][train_data['target'] == 0 ],label = 'non-disaster',color = 'g', ax= axes[1] ) ## denoting not disaster tweets length","bf80595d":"# getting length of data corresponding to every label\ntrain_data[train_data['target']==1].describe()  # for disastorous label","3fd0ae7d":"\ntrain_data[train_data['target']==0].describe()  # for non-disastorous label","9774ec75":"# filling every unknown keyword with not_known\ndataset['keyword'] = dataset['keyword'].fillna('not_known')","87e13d0e":"dataset['text'][dataset['keyword'] == 'blaze'] ","eaadc5bc":"train_data['location'].isnull().sum()","d5d35a89":"# now individual label by label\nprint('For disastrous, unknown locations : ',train_data['location'][train_data['target']==1].isnull().sum())\nprint('For non disastrous, unknown locations : ',train_data['location'][train_data['target']==0].isnull().sum())\n","3d3b8637":"train_data['location'][train_data['target']==1].sample(5) # randomly seen some samples origin location","3790ac24":"# filling localation with unknown\ndataset['location'].fillna('unknown', inplace = True)\ntrain_data['location'].fillna('unknown', inplace = True)","758504c7":"train_data['location'].sample(5)","b1ffca53":"# now encoding labels using their frequency wise ratio for locations, may be the no. of times a place occurs may affect the tweet\nnum_locations = Counter(dataset['location'])\nnum_locations\n\n","a2919066":"# label encoding\ndataset['location'] = le().fit_transform(dataset['location'])\ndataset","96b123ff":"#now time for analysing the text\n\n#converting the text into lower case\ndataset['text'] = dataset['text'].map(lambda x: x.lower())\ntrain_data['text'] = train_data['text'].map(lambda x: x.lower())\n\ndataset['text']","870618a1":"# dividing training data text on the basis of labels and then find common words in it\n\ndis_text = train_data['text'][train_data['target']==1]\ndis_text\n","388fac5a":"ndis_text = train_data['text'][train_data['target']==0]\nndis_text","618251f6":"dis_count = dis_text.map(lambda x: nltk.word_tokenize(x)) # for disastrous \nndis_count = ndis_text.map(lambda x : nltk.word_tokenize(x)) # for non disastrous\ndataset['text'] = dataset['text'].map(lambda x : nltk.word_tokenize(x)) # for whole dataset tokenize\nndis_count","873f7f2d":"# let's investigate which words are more common in each cases\nfrom nltk import FreqDist\n# for target =1\ncollection_words_dist = []\nfor i in list(dis_count):\n    collection_words_dist.extend(i)\n    \nmap_1 = FreqDist(collection_words_dist)\nmap_1","bf5433c1":"# plot some of them\nplt.figure(figsize = (8,8))\nsns.barplot(x = list(dict(map_1.most_common(15)).keys()), y = list(dict(map_1.most_common(15)).values())) ","ce401557":"# for target =0\ncollection_words_ndist = []\nfor i in list(ndis_count):\n    collection_words_ndist.extend(i)\n    \nmap_0 = FreqDist(collection_words_ndist)\nmap_0","4dc1f2fe":"# plot some of them\nplt.figure(figsize = (8,8))\nsns.barplot(x = list(dict(map_0.most_common(15)).keys()), y = list(dict(map_0.most_common(15)).values())) ","39f7ec69":"# lematiing the text to obtain root level text\n## lematization is beneficial if appropriate pos tags is used \n# function to map nltk tags with wordnet tags\nlemmatizer = WordNetLemmatizer()\ndef nltk_tags_2_word_tags(nltk_tag):\n    if nltk_tag.startswith('J'):\n        return wordnet.ADJ\n    elif nltk_tag.startswith('V'):\n        return wordnet.VERB\n    elif nltk_tag.startswith('N'):\n        return wordnet.NOUN\n    elif nltk_tag.startswith('R'):\n        return wordnet.ADV\n    else:          \n        return None\n    \n# function to create lemmatized sentences from tokenized words\ndef lemmatized_sentences(tokenized_sentence):\n    pos = nltk.pos_tag(tokenized_sentence)  # returns a tuple of words with their nltk tags\n    #tuple of (token, wordnet_tag)\n    wordnet_tagged = map(lambda x: (x[0], nltk_tags_2_word_tags(x[1])), pos)\n    \n    lemmatized_sentence = []\n    for word, tag in wordnet_tagged:\n        if tag is None:\n            #if there is no available tag, append the token as is\n            lemmatized_sentence.append(word)\n        else:        \n            #else use the tag to lemmatize the token\n            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n    return \" \".join(lemmatized_sentence)\n    \n  \n","be61fc4b":"# now converting our tokenized lowered words into lemmatized sentences\n\ndis_lem = dis_count.apply(lemmatized_sentences)\nndis_lem = ndis_count.apply(lemmatized_sentences)\ndataset['text'] = dataset['text'].apply(lemmatized_sentences)\n\n# finally dataset is lemmatized let's see\ndataset\n","2e820421":"dis_lem ","17863d0b":"import re   #regex library \n\n# function to remove patterns\ndef remove_pattern(input_txt, pattern):\n    reg_obj = re.compile(pattern)\n    input_txt = reg_obj.sub(r'', input_txt)\n        \n    return input_txt   \n","ec96563f":"dataset['text'] = dataset['text'].apply(lambda x: remove_pattern(x,\"@[\\w]*\"))","45698fd3":"# Reference : https:\/\/www.kaggle.com\/shahules\/tweets-complete-eda-and-basic-modeling\n\n\ndataset['text'] = dataset['text'].apply(lambda x: remove_pattern(x,'https?:\/\/\\S+|www\\.\\S+'))\ndataset['text'] = dataset['text'].apply(lambda x: remove_pattern(x,'<.*?>'))\n    ","4abeb9a0":"dataset['text'] = dataset['text'].apply(lambda x: remove_pattern(x,\"[^a-zA-Z# ]\"))","ce65679e":"# now using tf-idf to create new words using bigram + unigram\n\ntfidf = TfidfVectorizer(ngram_range = (1,1),max_df=0.90, min_df=2,stop_words = 'english')\ntext_set = tfidf.fit_transform(dataset['text'])\ntext_set","7a3c2fa9":"from scipy.sparse import hstack\ndataset_dtm = hstack((text_set,np.array(dataset['location'])[:,None]))\n","6c3fc6e6":"dataset_dtm=text_set","aac40404":"dataset_dtm","e0ad3003":"dataset_dtm = dataset_dtm.tocsr()  # converting to sparse row format\nx_train = dataset_dtm[0:len(train_data)]\nx_test = dataset_dtm[len(train_data):]\nx_train.shape","ec150fd2":"x_train","b1bc8641":"y_train = train_data['target']\nlen(y_train)","8ba25559":"kfold = StratifiedKFold(n_splits = 5 )\n","06753aca":"# Modeling step Test differents algorithms \nrandom_state = 2\nclassifiers = []\nclassifiers.append(SVC(kernel = 'rbf',probability = True))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\n#classifiers.append(LGBMClassifier(objective='classification', random_state=random_state))\n#classifiers.append(AdaBoostClassifier(ExtraTreesClassifier(random_state=2,max_depth = None,min_samples_split= 2,min_samples_leaf = 1,bootstrap = False,n_estimators =320), random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression())\nclassifiers.append(XGBClassifier(random_state=random_state))\n#classifiers.append(LinearDiscriminantAnalysis())\n\"\"\"\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(-cross_val_score(classifier,x_train, y = y_train, scoring = 'accuracy', cv = kfold , n_jobs =-1))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVR\",\"DecisionTree\",\"lgbm\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"KNeighboors\",\"LogisticRegression\",\"xgboost\",\"LDA\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")\ncv_res\n\"\"\"\n","fbb7d3d0":"parameter = {'solver':['liblinear','lbfgs'],\n            'max_iter':[200,400]}\n\nLogis_clf = LogisticRegression()\n\nlreg = GridSearchCV(Logis_clf, param_grid = parameter, cv = 3, verbose=True, n_jobs=-1)\nlreg.fit(x_train, y_train) # training the model\n\nlreg_best = lreg.best_estimator_\n\nprint(lreg.best_score_)\nprint(lreg.best_params_)","cd5b6c47":"# feeding raw models into stacking ensemble as the metal model will extract tht best out of each one\nfrom vecstack import stacking\nfrom sklearn.metrics import accuracy_score,f1_score\n\nS_train, S_test = stacking(classifiers,                   \n                           x_train, y_train, x_test,   \n                           regression= False,\n                          \n     \n                           mode='oof_pred_bag', \n       \n                           needs_proba=True,\n         \n                           save_dir=None, \n             \n    \n                           n_folds=5, \n                 \n                           stratified=True,\n            \n                           shuffle=True,  \n            \n                           random_state=0,    \n         \n                           verbose=2)","978f1570":"S_train","894f900d":"S_train.shape","a5196e11":"argmax_train = []\nargmax_test = []\nfor i in range(0,S_train.shape[1],2):\n    argmax_train.append( np.argmax(S_train[:,i:i+2],axis=1))\n    argmax_test.append( np.argmax(S_test[:,i:i+2],axis=1))","2226e1b4":"argmax_train = np.array(argmax_train,dtype= np.int64).T\nargmax_test = np.array(argmax_test,dtype= np.int64).T\n","e2208df7":"argmax_train","27eea1a3":"argmax_train.shape","4bd6331b":"# here using overall probabilities for meta model\n## from sklearn.metrics import f1_score\nmodelc = LogisticRegression()\n    \nmodel1c = modelc.fit(S_train, y_train)\ny_pred1c = model1c.predict_proba(S_train)\ny_predc = model1c.predict_proba(S_test)\n\nprint('Final test prediction score: [%.8f]' % accuracy_score(y_train, np.argmax(y_pred1,axis=1)))\nprint('Final f1-score test prediction: [%.8f]' % f1_score(y_train, np.argmax(y_pred1,axis=1)))\n","cdd1007b":"# here using predictions for metal model\n## from sklearn.metrics import f1_score\nmodel = XGBClassifier(random_state=2, objective = 'reg:linear', n_jobs=-1, learning_rate= 0.5, \n                      n_estimators=30, max_depth=20)\n    \nmodel1 = model.fit(argmax_train, y_train)\ny_pred1 = model1.predict_proba(argmax_train)\ny_pred = model1.predict_proba(argmax_test)\n\nprint('Final test prediction score: [%.8f]' % accuracy_score(y_train, np.argmax(y_pred1,axis=1)))\nprint('Final f1-score test prediction: [%.8f]' % f1_score(y_train, np.argmax(y_pred1,axis=1)))\n","440b0c41":"## checking the distribution of prediction\nsns.distplot(y_pred)\nsns.distplot(y_predc)\n             ","2a3bbd9d":"sample_submission['target'] = np.argmax(y_pred+y_predc,axis=1)\n\nsample_submission.to_csv('submission_with_stacking.csv', index = False)","64391273":" ## Tokenize","d85bcefe":"### Removing https type symbol","ff926e71":" ### Firstly,Applying lematization","d332a74e":"Assumption is mostly correct","5a614e52":"If you find this notebook helpful **Please Upvote**","04e7bf70":"*Adding location to this sparse matrix*","14461d34":"## Data Analysis ","b90bd778":"### text cleaning","ffe1bba2":"### stacking base models\n\nhere we are feeding raw models into stacking as we except that the meta model will extract best out of everyone","58ae14ad":"## MODEL BUILDING","5d810eef":"### slicing back into train and test","3ed1b491":"### Remove punctuations, special characters, numbers","bb6c376a":"look like no relation of frequency, hence using label encoding for it","74c04927":"it seems from above that the word blaze doesn't add any information due to different semantic meanings in different samples, let's n investigate about location as it may provide some info","fcca19fe":"## Key takeaways \n1. most of the frequent occuring words are common except some words like I, hence we need to keep i as a tweet related to personal intrested may be mostly non-disastrous\n2. we need to lemmatize the text to create further root level meaning\n3. all the symbols will be assumed to add some type of information hence keeping it also.\n4. but we need to remove some patterns as it will common in both and may deviate the tweets pattern","1c62d10d":"*kfolds for cross validation*","5595d082":"HUGE!! Unknown locations, let's diving deeper,\nMay be most unknown locations were related to non disaster ones","6a491082":"## Loading files","f68d3710":"### Removing twitter handles"}}