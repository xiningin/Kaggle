{"cell_type":{"938ca981":"code","d21f905a":"code","f71e203d":"code","c39c9232":"code","d0775397":"code","936176d2":"code","d8f323fb":"code","727aa426":"code","d8da84dd":"code","7de7586f":"code","65c319da":"code","eefc30f5":"code","7afee938":"code","e3256967":"code","3590e410":"code","e6184515":"code","1847d5fe":"code","b56cff64":"code","2c4d0de5":"code","27ec9ba1":"code","325b7cd0":"code","ef0cda7e":"code","93b1d465":"code","62704630":"code","1a8d1d90":"code","2268c901":"code","9b482927":"code","b1ac3849":"code","09a17809":"code","52ce7f75":"code","391ba2ff":"code","b54a9c98":"code","7025a62b":"code","5cafe70f":"code","29931d0e":"code","aa8ebe06":"code","9ce33c66":"code","e851ee65":"code","1a682a9c":"code","9128667a":"code","38448fcc":"code","cf28802a":"code","8512d232":"code","fc69ba76":"code","7aaf3633":"markdown","e5f49eb2":"markdown","85b057af":"markdown","a073744b":"markdown","15e59ac9":"markdown","f38bb4ef":"markdown","f7351634":"markdown","64c29e92":"markdown","95383438":"markdown","744e210e":"markdown","7726605a":"markdown","7c4fb0f7":"markdown","e48541b1":"markdown","0ccea4ee":"markdown","bb774459":"markdown","77e8678b":"markdown","1a79fb22":"markdown"},"source":{"938ca981":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as pyplot\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport functools\nimport tensorflow as tf\nimport datetime\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# tf.enable_eager_execution()\n\n# Any results you write to the current directory are saved as output.","d21f905a":"tf.__version__","f71e203d":"TRAIN_PATH = '..\/input\/digit-recognizer\/train.csv'\nTEST_PATH = '..\/input\/digit-recognizer\/test.csv'\nBATCH_SIZE = 1000","c39c9232":"train_df = pd.read_csv(TRAIN_PATH)\ntrain_df.shape","d0775397":"train_df.info()","936176d2":"labels = train_df['label']\ntrain_df.drop('label', axis=1, inplace=True)","d8f323fb":"train_df.columns","727aa426":"X_train, X_test, y_train, y_test = train_test_split(train_df.values, labels.values, test_size=0.2, random_state=42)\n# X_train = train_df\n# y_train = labels","d8da84dd":"X_img = X_train\nX_img.shape","7de7586f":"X_img = X_img.reshape(X_img.shape[0], 28, 28)","65c319da":"pyplot.imshow(X_img[0])\npyplot.title(y_train[0])","eefc30f5":"pca = PCA(169)\nclass SqueezeDataset(BaseEstimator, TransformerMixin):\n    def __init__(self, expected_variance_ratio = 0.95):\n        self.variance_ratio = expected_variance_ratio\n        self.pca = pca\n    \n    def fit(self, X, y=None):\n        self.pca.fit(X)\n        return self\n    \n    def transform(self, X, y=None):\n        X_transformed = self.pca.transform(X)\n        return X_transformed","7afee938":"train_data_pipeline = Pipeline([\n    ('scale_dataset', StandardScaler()),\n    ('squeeze_dataset', SqueezeDataset())\n])","e3256967":"X_processed = train_data_pipeline.fit_transform(X_train)","3590e410":"X_processed.shape","e6184515":"x_inverse_trans = pca.inverse_transform(X_processed)","1847d5fe":"x_inverse_trans = x_inverse_trans.reshape(x_inverse_trans.shape[0], 28, 28)","b56cff64":"x_inverse_trans.shape","2c4d0de5":"fig, ax = pyplot.subplots(1, 3)\nax[0].imshow(x_inverse_trans[0])\nax[0].set_title('inverse transform')\nax[1].imshow(X_processed[0].reshape(13, 13))\nax[1].set_title('processed image')\nax[2].imshow(X_img[0])\nax[2].set_title('original')","27ec9ba1":"def prepare_dataset(features, labels):\n    print(features.shape)\n    dataset = (\n        tf.data.Dataset.from_tensor_slices((features, labels))\n            .shuffle(len(labels))\n            .repeat()\n            .batch(BATCH_SIZE)\n            .prefetch(1)\n    )\n    iterator = dataset.make_one_shot_iterator()\n    return iterator","325b7cd0":"Input = tf.keras.Input\nConv2D = functools.partial(\n        tf.keras.layers.Conv2D,\n        activation='elu',\n        padding='same'\n    )\nBatchNormalization = tf.keras.layers.BatchNormalization\nDense = tf.keras.layers.Dense\nAveragePooling2D = tf.keras.layers.AveragePooling2D\nDropout = tf.keras.layers.Dropout\nFlatten = tf.keras.layers.Flatten\nL2 = functools.partial(\n        tf.keras.regularizers.l2,\n        l=0.2\n    )","ef0cda7e":"def prepare_model():\n    input = Input(shape=(28,28,1,))\n    conv1 = Conv2D(8, (3, 3))(input)\n    conv2 = Conv2D(16, (3, 3))(conv1)\n    batch_norm1 = BatchNormalization(axis=3)(conv2)\n    conv3 = Conv2D(32, (5, 5))(batch_norm1)\n    dropout = Dropout(0.3)(conv3)\n#     avg_pool_1 = AveragePooling2D((2, 2))(dropout)\n    conv4 = Conv2D(32, (5, 5))(dropout)\n    batch_norm2 = BatchNormalization(axis=3)(conv4)\n#     avg_pool_2 = AveragePooling2D(2, 2)(conv4)\n    conv5 = Conv2D(16, (7, 7))(batch_norm2)\n    conv6 = Conv2D(8, (7, 7))(conv5)\n    flt1 = Flatten()(conv6)\n    output = Dense(10, activation='softmax')(flt1)\n    model = tf.keras.Model(input, output)\n    model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy']) # Since labels are not one hot encoded using just categorial_crossentropy dosen't make sence here.\n    model.summary()\n    return model","93b1d465":"iterator = prepare_dataset(X_train.reshape(X_train.shape[0], 28, 28, 1), y_train)","62704630":"val_iterator = prepare_dataset(X_test.reshape(X_test.shape[0], 28, 28, 1), y_test)","1a8d1d90":"model = prepare_model()","2268c901":"logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\ntensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint(filepath='..\/input\/weights.hdf5', verbose=1, save_best_only=True),\n    tensorboard_callback,\n    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n]","9b482927":"history = model.fit(iterator, steps_per_epoch=34, epochs=50,  verbose=1, callbacks=callbacks,\n                   validation_data=val_iterator, validation_steps=9)","b1ac3849":"pyplot.imshow(X_test[0].reshape(28, 28))\npyplot.title(y_test[0])","09a17809":"# Check how model is learning internally\ninputs = model.input\noutputs = [layer.output for layer in model.layers][3:]\nfunctor = tf.keras.backend.function([inputs, tf.keras.backend.learning_phase()], outputs)","52ce7f75":"# Testing\ntest = X_test[0].reshape(1, 28, 28, 1)\nlayer_outs = functor([test, 1.])\nprint(layer_outs)","391ba2ff":"layer_outs[0].shape","b54a9c98":"sample = np.squeeze(layer_outs[0])\nsample = np.transpose(sample)\nsample[0].shape","7025a62b":"pyplot.imshow(sample[10])","5cafe70f":"%load_ext tensorboard.notebook\n%tensorboard --logdir logs","29931d0e":"test_df = pd.read_csv(TEST_PATH)","aa8ebe06":"test_df.shape","9ce33c66":"test_df = test_df.values.reshape(test_df.shape[0], 28, 28, 1)","e851ee65":"test_df.shape","1a682a9c":"predictions = model.predict(test_df)","9128667a":"predictions = np.argmax(predictions, axis = 1)","38448fcc":"submission_df = pd.concat([pd.Series(range(1, 28001), name=\"ImageId\"), pd.Series(predictions, name=\"Label\")], axis = 1)","cf28802a":"submission_df.head(5)","8512d232":"pyplot.imshow(test_df[3].reshape(28, 28))","fc69ba76":"submission_df.to_csv(\"cnn_mnist_datagen.csv\", index=False)","7aaf3633":"## Training model\nTrain model using both training and valiadion iterator so that it will train and validate itself in every epoch. Set epochs to 50, if there is no improvement in learning for 3 epoch model will stop training as mentioned in callbacks.","e5f49eb2":"Visualize some testing data","85b057af":"## Creating data pipline\nIn this dataset we have pixel densities as training data ranging from 0-255. For better performance of our model let's do scaling first and then we will apply dimentionality reduction.Since we need to apply dimentionality reduction to all of the data, lets create a pipeline todo the same. Here we are using scikit learn's PCA to do the same. 95% of variance will give upto 153 dimentions, so let's take 169 dimentions which will be helpful for visualization as well. Using transform method of estimators, apply inverse trasform of PCA data, to get an approximation of training dataset.","a073744b":"Combining both estimator to create a full pipeline and transforming data","15e59ac9":"## Visualize model performance with tensorboard","f38bb4ef":"Visialize approximation of training data","f7351634":"## Callbacks to model\nCreate callbacks to save model, visualize its performance with tensorboard, and for early stoping to avoid vanishing gradient problem","64c29e92":"## Check how model is learning during training\nLet's check how model is learning during training. get outputs of 3rd layer from the model and try to visulaze it.","95383438":"## Image Classification with dimentionality reduction\n    * Load data and visualize it \n    * Create Scikit learn data pipline\n    * Create tensorflow dataset\n    * Create tensorflow model\n    * Create Callbacks for the model\n    * Train model\n    * Check how model is learning during training\n    * Visualize model performance with tensorboard\n    * Do inferance on created model","744e210e":"Get iterator from prepare dataset function defined above for both training and validation datasets.","7726605a":"## Visualize data\nSince we are dealing with images let's plot some images from dataset","7c4fb0f7":"## Creating CNN with tensorflow","e48541b1":"## Load data\nLoad the input data with pandas and seperate labels from it. It good to have training set and validation dataset, so with scikit learn ```train_test_split``` we are creating training and validation dataset with 80, 20 percent for input training data.","0ccea4ee":"Appying inverse transform to get an approximation of given training data after PCA","bb774459":"Declare some layers with common variables to avoid repeating of code.","77e8678b":"## Inferance\nAfter all training and understanding how model learns and performs, let's do inferance on our model with test dataset. Load test dataset and send it to model to predict labels and write it to CSV file.","1a79fb22":"## Create tensorflow dataset\nWith the approximation we have, let's create a dataset and an iterator, which can do shuffling, batching on out dataset.\nCreate a one shot iterator to iterate over data."}}