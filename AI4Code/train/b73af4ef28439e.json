{"cell_type":{"595d6a7b":"code","809015db":"code","aa3f61ce":"code","7a87a9a1":"code","e9c6c326":"code","1e149e20":"code","66c5ee9f":"code","278ac8d2":"code","b0b62a45":"code","350817c8":"code","5d390c82":"code","ad67ff08":"code","197e0eaa":"code","3642fa94":"code","565610da":"code","8c526e85":"code","51aef028":"code","cf4e4c83":"code","52a8b428":"code","ed76ff96":"code","d32d215c":"code","d19ad444":"code","ede2c1f5":"code","a7a74bd0":"markdown","0430ca90":"markdown","c23eaa91":"markdown","5c11cf0b":"markdown","b0471ce8":"markdown","0fad7d73":"markdown","1b040726":"markdown","dc4b3adf":"markdown","e52a15ec":"markdown","aaa4abf8":"markdown","bc67d1a2":"markdown","58187d08":"markdown","5b734ae4":"markdown","9fa0902b":"markdown","bf36b1e2":"markdown","ed09ff90":"markdown","2ffefaac":"markdown","08d53d19":"markdown","9865a996":"markdown"},"source":{"595d6a7b":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\nimport plotly.express as px\n#px.offline.init_notebook_mode(connected=True)\nfrom IPython.core.display import HTML\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport tensorflow as tf\n\n!pip install googletrans\nfrom googletrans import Translator\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nimport torch\nimport random\n","809015db":"# directory\nDATA_DIR = \"\/kaggle\/input\/contradictory-my-dear-watson\"\n\n# read train and test csv files\ntrain_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\ntest_df = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n#print(f\"train data {train_df.shape} test data {test_df.shape}\")","aa3f61ce":"dataset_size = {\"train set\":train_df.shape[0], \"test set\":test_df.shape[0]}\nfig = px.bar(y = list(dataset_size.keys()), x = list(dataset_size.values()), \n             title=\"Distribution of train and test\", text= list(dataset_size.values()))\nfig.update_layout(\n    xaxis_title=\"No of samples\",\n    yaxis_title=\"Dataset\")\nfig.show()","7a87a9a1":"# show list of columns available\n\nhtml = '<h3>Available columns in the Training data:<\/h3><\/br><center><table style=\"width:50%; border: 1px solid black; \"><tr style=\"border: 1px solid black\"><th style=\"border: 1px solid black\">Column name<\/th><th style=\"border: 1px solid black\">Desc<\/th><\/tr>'\ncols = train_df.columns.tolist()\ncols_desc = [\"A unique identifier\",\n             \"Actual sentence\",\n             \"Hypothesis sentence\",\n             \"Language abbreviation code\",\n             \"Language\",           \n             \"Classification of the relationship between the premise and hypothesis (0 for entailment, 1 for neutral, 2 for contradiction)\"]\ncols_desc = list(zip(cols, cols_desc))\nhtml += \" \".join([f\"<tr style='border: 1px solid black;'><td style='border: 1px solid black'>{col}<td style='border: 1px solid black'>{desc}<\/td><\/tr>\" for col, desc in cols_desc])\nhtml += '<\/table><\/center>'\ndisplay(HTML(html))","e9c6c326":"# show list of languages available\n\nhtml = '''<head><style>\ntable, th, td {\n  border: 1px solid black;\n  border-collapse: collapse;\n}\nth, td {\n  padding: 5px;\n  text-align: left;\n}\n<\/style><\/head><body><h3>This dataset contains premise-hypothesis pairs in 15 different languages:<\/h3><\/br><center><table style=\"width:25%\"><tr><th>Languages<\/th><th>Languages code<\/th><\/tr>\n'''\nlanguage_codes = train_df[[\"language\", \"lang_abv\"]].drop_duplicates().set_index('language').to_dict()[\"lang_abv\"]\n\nhtml += \" \".join([f\"<tr><td>{lang}<\/td><td>{language_codes[lang]}<\/td><\/tr>\" for lang in language_codes])\nhtml += '<\/table><\/center><\/body>'\ndisplay(HTML(html))\n\n","1e149e20":"language_distribution_class = train_df[[\"id\", \"language\",\"label\"]].groupby([\"language\",\"label\"])[\"id\"].count().reset_index().rename(columns={\"id\":\"count\",\"label\":'class'})\nlanguages_count_dict = language_distribution_class.groupby(\"language\")[\"count\"].sum().to_dict()\n# total count of samples for each language\nlanguage_distribution_class[\"total_count\"] = language_distribution_class[\"language\"].map(languages_count_dict)\nlanguage_distribution_class[\"percent\"] = round(language_distribution_class[\"count\"] \/ language_distribution_class[\"total_count\"], 2)\nlanguage_distribution_class[\"class_count\"] = language_distribution_class[\"class\"].astype(str) + \", \"+ language_distribution_class[\"percent\"].astype(str) + \"%\"\nfig = px.pie(language_distribution_class, values='count', names='language',hover_data=[\"class\"], title='Distribution of language of text')\nfig1 = px.sunburst(language_distribution_class, path=['language', 'class_count'], values='count', hover_data=[\"count\"], title='Distribution of language with labels')\nfig.show()\nfig1.show()","66c5ee9f":"label_distribution = train_df[\"label\"].value_counts() \nlabel_distribution.index = [\"entailment\", \"neutral\", \"contradiction\"]\n\nfig = px.bar(label_distribution, title=\"Label (target variable) distribution\")\nfig.update_layout(\n    xaxis_title=\"Labels\",\n    yaxis_title=\"Frequency\")\nfig.show()","278ac8d2":"stopwords = set(STOPWORDS)\n\nurl = \"https:\/\/raw.githubusercontent.com\/amueller\/word_cloud\/master\/examples\/a_new_hope.png\"\nresponse = requests.get(url)\nimg = Image.open(BytesIO(response.content))\n\nwordcloud = WordCloud(stopwords=stopwords, contour_width=3, contour_color='steelblue', background_color=\"white\", max_words=500).generate(\",\".join(train_df[train_df[\"language\"].str.contains(\"English\")][\"premise\"].tolist()))\nmask = np.array(img)\n\nplt.figure(figsize = (20,15))\nplt.imshow(mask, cmap=plt.cm.gray, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title(\"English word distribution\",fontdict={\"fontsize\":20}, pad=2)\nplt.show()","b0b62a45":"# instantiate translator\ntranslator = Translator()\n\n# function to translate sentence\ndef translate_sentence(sentence, src_lang):\n    if \"en\" == src_lang:        \n        return sentence\n    \n    src_lang = \"zh-cn\" if \"zh\" in src_lang else src_lang\n    translated_sentence = translator.translate(sentence, src=src_lang, dest=\"en\")    \n    return translated_sentence.text\n\ndef generate_translate_for_other_language():\n    ## translation for training data \n    for index, row in tqdm(train_df.iterrows()): \n        # translate premise sentence train set\n        train_df.loc[index, \"premise_translated\"] = translate_sentence(row['premise'], row[\"lang_abv\"])\n        # translate hypothesis sentence train set\n        train_df.loc[index, \"hypothesis_translated\"] = translate_sentence(row['hypothesis'], row[\"lang_abv\"])\n\n\n    file_name = r'translated_text.csv'\n    ## save translated dataframe\n    train_df.to_csv(file_name, index=False)\n    print(f\"Translated train dataset saved in {file_name} csv file\")\n    ## translation for testing data \n    for index, row in tqdm(test_df.iterrows()): \n        # translate premise sentence train set\n        test_df.loc[index, \"premise_translated\"] = translate_sentence(row['premise'], row[\"lang_abv\"])\n        # translate hypothesis sentence train set\n        test_df.loc[index, \"hypothesis_translated\"] = translate_sentence(row['hypothesis'], row[\"lang_abv\"])\n\n\n    file_name = r'translated_test_dataset.csv'\n    ## save translated dataframe\n    train_df.to_csv(file_name, index=False)\n    print(f\"Translated train dataset saved in {file_name} csv file\")\n\nPROCESSED_OUTPUT_DIR = \"..\/input\/my-dear-watson-translated-text\"\nif os.path.exists(PROCESSED_OUTPUT_DIR):\n    train_df[['premise_translated', 'hypothesis_translated']] = pd.read_csv(os.path.join(PROCESSED_OUTPUT_DIR, \"translated_train_dataset.csv\"))[['premise_translated', 'hypothesis_translated']]\n    test_df[['premise_translated', 'hypothesis_translated']] = pd.read_csv(os.path.join(PROCESSED_OUTPUT_DIR, \"translated_test_dataset.csv\"))[['premise_translated', 'hypothesis_translated']]\n\nprint(train_df[train_df[\"lang_abv\"] != 'en'][['premise_translated', 'hypothesis_translated', 'label']].head(3))","350817c8":"FOLD = 5\nk_fold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=42)\nfor fold, (train_idx, val_idx) in enumerate(k_fold.split(train_df, y=train_df[\"label\"])):\n    print(fold, train_idx.shape, val_idx.shape)\n    train_df.loc[val_idx,'fold'] = fold","5d390c82":"## setting up TPU\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\nprint('Number of replicas:', strategy.num_replicas_in_sync)","ad67ff08":"from transformers import BertTokenizer, TFBertModel\nimport tensorflow as tf","197e0eaa":"model_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name)","3642fa94":"def encode_sentence(sentence):\n    tokens = list(tokenizer.tokenize(sentence))\n    tokens.append('[SEP]')\n    return tokenizer.convert_tokens_to_ids(tokens)","565610da":"def bert_encoder(hypotheses, premises, tokenizer):\n    \n    num_examples = len(hypotheses)\n\n    sentence1 = tf.ragged.constant([\n      encode_sentence(s)\n      for s in np.array(hypotheses)])\n    sentence2 = tf.ragged.constant([\n      encode_sentence(s)\n       for s in np.array(premises)])\n\n    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n    input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n\n    input_mask = tf.ones_like(input_word_ids).to_tensor()\n\n    type_cls = tf.zeros_like(cls)\n    type_s1 = tf.zeros_like(sentence1)\n    type_s2 = tf.ones_like(sentence2)\n    input_type_ids = tf.concat(\n      [type_cls, type_s1, type_s2], axis=-1).to_tensor()\n\n    inputs = {\n      'input_word_ids': input_word_ids.to_tensor(),\n      'input_mask': input_mask,\n      'input_type_ids': input_type_ids}\n\n    return inputs","8c526e85":"!pip install nlpaug\nimport nlpaug.augmenter.word as naw","51aef028":"aug = naw.SynonymAug(aug_src='wordnet')\naugmented_text = aug.augment(train_df['premise_translated'][0])\nprint(f\"Orginial: \\n{train_df['premise_translated'][0]}\\nAugmented Text: \\n{augmented_text}\")","cf4e4c83":"## augment text\ndef agument_text(data_df, col_name):\n    aug = naw.SynonymAug(aug_src='wordnet')\n    \n    for index, row in tqdm(enumerate(data_df[col_name])):\n        data_df.loc[index, col_name] = aug.augment(row)\n    \n    return data_df[col_name]","52a8b428":"max_len = 75\n\ndef build_model():\n    bert_model = TFBertModel.from_pretrained(model_name)\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n    \n    embedding = bert_model([input_word_ids, input_mask, input_type_ids])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","ed76ff96":"## test set input\ntest_input = bert_encoder(test_df['premise_translated'].values, test_df['hypothesis_translated'].values, tokenizer) \n\n## storing prediction results\ntest_prediction = np.zeros((test_df.shape[0], 3))\n\n## iterate each fold\nfor fold in range(FOLD):\n    train_df_fold = train_df[train_df[\"fold\"] != fold]\n    val_df = train_df[train_df[\"fold\"] == fold]\n    \n    ## agument 5% of training data\n    sample_selection = train_df_fold.sample(frac=0.05).reset_index(drop=True)\n    \n    sample_selection[\"premise_translated\"] = agument_text(sample_selection, \"premise_translated\")\n    sample_selection[\"hypothesis_translated\"] = agument_text(sample_selection, \"hypothesis_translated\")\n    \n    train_df_fold = pd.concat([train_df_fold, sample_selection])\n    train_input = bert_encoder(train_df_fold['premise_translated'].values, train_df_fold['hypothesis_translated'].values, tokenizer) \n    val_input = bert_encoder(val_df['premise_translated'].values, val_df['hypothesis_translated'].values, tokenizer) \n    \n    with strategy.scope():\n        model = build_model()\n        model.fit(train_input, train_df_fold.label.values, epochs = 6, verbose = 1, batch_size = 64, validation_data = val_input)        \n        \n        ## predict for unseen data\n        predictions = model.predict(test_input)\n        \n        test_prediction += predictions \n        \n## take mean of the prediction\ntest_prediction = test_prediction \/ FOLD\n## take the maximum probability value \ntest_prediction = np.argmax(test_prediction, axis=1)","d32d215c":"test_df[\"prediction\"] = test_prediction","d19ad444":"## submission file\ntest_df[[\"id\", \"prediction\"]].head()","ede2c1f5":"test_df[[\"id\", \"prediction\"]].to_csv(\"submission.csv\", index = False)","a7a74bd0":"There are various ways that you can do word augument.\n- Word embeddings\n- TF-IDF\n- Contextual Word Embeddings\n- Synonym\n\nWill experiment with Synonym augment. Will do substitute the word by **WordNet's synonym**","0430ca90":"Label distribution almost balanced for each class.","c23eaa91":"There is **56.7%** English text available in which **35%, 32% and 33%** data distributed for each label (0,1, and 2).\n\nLikewise, other language text vary from 3.39% to 2.89% distribution. This is highly unbalanced dataset with respect to each language but we would need to check with the distribution of each labels.","5c11cf0b":"#### Thanks for reading the basic exploration notebook with BERT model. ","b0471ce8":"## Tranlate Non-english to English for train and test set.","0fad7d73":"### Let's build Transformer model","1b040726":"The above is the sample sentence where augmented a word by **Wordnet** synonym.\n\nLet's randomly augment text for **5% of training data** for each fold. This number is randomly taken.","dc4b3adf":"## Task:\n\nWe have two sentences, there are three ways they could be related: \n- one could entail the other (entailment)\n- one could contradict the other (contradiction) or\n- they could be unrelated (neutral)\n\nNatural Language Inferencing (NLI) is a popular NLP problem that involves determining how pairs of sentences (consisting of a premise and a hypothesis) are related. \n\nIn this notebook, Will explore the [Contradictory, My Dear Watson](https:\/\/www.kaggle.com\/c\/contradictory-my-dear-watson\/overview) challange.","e52a15ec":"## Stratified KFold","aaa4abf8":"![image.png](attachment:image.png)\n\nBasic idea is, we have seen there are 15 different languages available in the training data. Hence, doing the experimentation of translating the non-English to English input.","bc67d1a2":"The label (target variable) distribution is close to balanced. Based on the distribution of language is highly different than distribution of each class. \n\nHence, we can conclude following observation:\n- Multiclass problem\n    - 3 classes available to classify\n- Multi-lingual\n    - Have 15 different languages\n- Unbalanced with distribution of language\n    - This may cause issue with the lesser vocabulary size in differenct language.","58187d08":"Input text contains multiple language. This could be real challanging itself to handle **Multi-lingual** data.","5b734ae4":"Input tokens should be tokenized in [CLS] Hello, my dog is cute [SEP] your dog too. [SEP] format.","9fa0902b":"## Augument NLP data","bf36b1e2":"The above Word cloud is for English language. If you make an Word cloud with entire corpus we might see other languages frequency would be very less.","ed09ff90":"We would be ideally spliting the dataset in ratio of 70-30% or 80-20% for train and test split. Likewise, here also we could see 70-30% split for train and test available already.\n\nWe have 17315 samples for train and test. This is descent enough amount of data for any problem. ","2ffefaac":"Total 7 columns available including with label (target variable). As we could see, the label contains 3 different values that needs to be classified.\n\nHence, the problem type is **Multi-class classification** since it has more than 2 classes.\n\nThe primary input for this challange is:\n- premise (Actual sentence)\n- hypothesis (Hypothesis sentence)\n\nAlso, language code help us to identify what kind of language it contains in our input. Otherise, We would need to identify what kind of language our input contains without this information.\n","08d53d19":"## Submittion of Predictions","9865a996":"Stratified K-fold model training"}}