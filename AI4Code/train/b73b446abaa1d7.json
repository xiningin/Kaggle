{"cell_type":{"c4b03d8e":"code","15ccb5e0":"code","14c4b401":"code","688e7009":"code","491de495":"code","058aeeeb":"code","f551d3c3":"code","0d229592":"code","9d6a3c58":"code","74bedbff":"code","a5447941":"code","8c3de76c":"code","8f229f27":"code","6f9b3e87":"code","5be2a88a":"code","6d7ea88b":"code","d966190c":"code","8bef618a":"code","70772db8":"code","f07f0faa":"code","5b9d4a19":"code","ade3e67a":"code","4bf4da45":"code","5c8d5921":"code","99ad184d":"markdown","35f69385":"markdown","202d7c99":"markdown","aadce57f":"markdown","f8d5aa48":"markdown","4af30041":"markdown","2a67b0e0":"markdown","34e14b52":"markdown","82654a7e":"markdown","507b74db":"markdown","5326fb97":"markdown","c7df063a":"markdown","e9d7dc4f":"markdown","6df0c5b3":"markdown","11647e26":"markdown","6c571b59":"markdown","b0b24b4e":"markdown","e34fddc8":"markdown","dfe4117f":"markdown","4ccaabb6":"markdown","cfa1c43d":"markdown","e1042304":"markdown","38641afd":"markdown","8f761bd8":"markdown","273eae1c":"markdown","a21db6a6":"markdown","c6accbad":"markdown"},"source":{"c4b03d8e":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_rows', 80)\npd.set_option('display.max_columns', 80)\npd.set_option('display.width', 1000)","15ccb5e0":"games = pd.read_csv('..\/input\/csgo-ratings\/csgo_games_ratings.csv')\n\n# I left the resulting rating changes for each game in the data, but we won't need this.\ngames.drop(['t1_rating_change', 't2_rating_change'], axis=1, inplace=True)\ngames.shape","14c4b401":"games.head(10)","688e7009":"t1_wins = games[games['t1_win'] == 1]\nt2_wins = games[games['t1_win'] == 0]\n\nprint('Number of t1 wins: ', len(t1_wins))\nprint('Number of t2 wins: ', len(t2_wins))","491de495":"def equalize_wins(games):\n    t1_wins = games[games['t1_win'] == 1]\n    t2_wins = games[games['t1_win'] == 0]\n\n    min_wins = min(len(t1_wins), len(t2_wins))\n    max_wins = max(len(t1_wins), len(t2_wins))\n\n    if len(t2_wins) < len(t1_wins):\n        reduced_t1_wins = t1_wins.sample(frac=(min_wins \/ max_wins), random_state=1)\n        games = pd.concat([reduced_t1_wins, t2_wins], axis=0)\n    else:\n        reduced_t2_wins = t2_wins.sample(frac=(min_wins \/ max_wins), random_state=1)\n        games = pd.concat([reduced_t2_wins, t1_wins], axis=0)\n\n    return games.sample(frac=1, random_state=1)\n    \ngames = equalize_wins(games)","058aeeeb":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 7))\n\nax1.hist(games['rating_difference'], bins=300)\nax1.set_ylabel('Number of Games', {'fontsize': 12})\nax1.set_xlabel('Rating Difference', {'fontsize': 12})\nax1.set_title('All Games', {'fontsize': 16})\n\nnon_zero = games[games['rating_difference'] != 0]\n\nax2.hist(non_zero['rating_difference'], bins=300)\nax2.set_ylabel('Number of Games', {'fontsize': 12})\nax2.set_xlabel('Rating Difference', {'fontsize': 12})\nax2.set_title('Games With Non-Zero Rating Difference', {'fontsize': 16})\nplt.show()","f551d3c3":"zero_games = games[games['rating_difference'] == 0]\nprint('Win rate for zero difference games:', \n      round((zero_games['t1_win'].sum() \/ len(zero_games)), 4))","0d229592":"def win_rates(history):\n    t1_better_t2 = history[history['rating_difference'] >= 0]\n    t1_better_percentage = t1_better_t2['t1_win'].sum() \/ len(t1_better_t2)\n\n    return round(t1_better_percentage, 4)\n\nnon_zero_games = games[games['rating_difference'] != 0].copy()\nprint('Win rate for all games:', win_rates(games))\nprint('Win rate for non-zero rating_difference games:', win_rates(non_zero_games))","9d6a3c58":"# Returns a dataframe containing the win rates for games with a greater absolute rating_difference than i, along with the number of games in the sample\ndef win_rate(games):\n    rating_win_rate = []\n    games['rating_difference'] = games['rating_difference'].astype(int)\n    for i in range(0, games['rating_difference'].abs().max()):\n        t1_better_t2 = games[games['rating_difference'] > i]\n        t1_worse_t2 = games[games['rating_difference'] < -i]\n\n        # We are interested in the t1_worse_t2 games where t1_win is 0, this is captured below.\n        wins = t1_better_t2['t1_win'].sum() + (len(t1_worse_t2) - t1_worse_t2['t1_win'].sum())\n        num_games = len(t1_better_t2) + len(t1_worse_t2)\n        win_rate = wins \/ num_games\n\n        rating_win_rate.append({'rating_difference': i, \n                                'win_rate': win_rate, \n                                'num_games': num_games})\n\n    return pd.DataFrame(rating_win_rate)","74bedbff":"def plot_win_rating(games):\n    fig, ax1 = plt.subplots(figsize=(7, 7))\n    ax1.set_title('Win Rate vs Rating Difference', {'fontsize': 16})\n\n    ax1.bar(x=games['rating_difference'], \n            height=games['num_games'], \n            width=2,\n            alpha=0.4)\n    ax1.set_xlabel('Rating difference', {'fontsize': 12})\n    ax1.set_ylabel('Number of Games', {'fontsize': 12})\n\n    ax2 = fig.add_subplot(sharex=ax1, frameon=False)\n    ax2.plot(games['win_rate'], color='r')\n    ax2.yaxis.tick_right()\n    ax2.yaxis.set_label_position('right')\n    ax2.set_ylabel('Win Rate', {'fontsize': 12})\n\n    plt.show()","a5447941":"rating_win_rate = win_rate(games)\nplot_win_rating(rating_win_rate)","8c3de76c":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import scale\n\nfrom scipy import stats\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nresults = []","8f229f27":"def decision_tree_classifier(games):\n    x = np.array(games['rating_difference']).reshape(-1, 1)\n    y = games['t1_win']\n\n    x = scale(x)\n\n    hyperparameters = {\n        'max_depth': [2, 3, 4, 5],\n        'min_samples_split': [2, 3, 4, 5],\n        'min_samples_leaf': [1, 2, 3, 4, 5],\n        'max_leaf_nodes': [5, 6, 7, 8, 9]\n\n    }\n    dtc = DecisionTreeClassifier()\n    grid = GridSearchCV(dtc, param_grid=hyperparameters, cv=5)\n    grid.fit(x, y)\n\n    return grid.best_params_, grid.best_score_\n  \ninclude_zero = decision_tree_classifier(games)\nnon_zero = decision_tree_classifier(non_zero_games)\nprint('With zero rating difference \\nBest accuracy: ', round(include_zero[1], 4), \n      '\\nBest parameters: ', include_zero[0])\nprint('\\nWithout zero rating difference \\nBest accuracy: ', round(non_zero[1], 4), \n      '\\nBest parameters: ', non_zero[0])\n\nresults.append({'model': 'Decision Tree', \n                'non_zero': round(non_zero[1], 4), \n                'include_zero': round(include_zero[1], 4)}\n              )","6f9b3e87":"def k_nearest_neighbors(games):    \n    x = np.array(games['rating_difference']).reshape(-1, 1)\n    y = games['t1_win']\n\n    x = scale(x)\n\n    hyperparameters = {\n        'n_neighbors': [10, 20, 30, 40, 50, 60, 70],\n        'weights': ['uniform', 'distance'],\n        'algorithm': ['brute', 'auto'],\n        'p': [1, 2]\n\n    }\n    knc = KNeighborsClassifier()\n    grid = GridSearchCV(knc, param_grid=hyperparameters, cv=5)\n    grid.fit(x, y)\n\n    return grid.best_params_, grid.best_score_\n    \ninclude_zero = k_nearest_neighbors(games)\nnon_zero = k_nearest_neighbors(non_zero_games)\nprint('With zero rating difference \\nBest accuracy: ', round(include_zero[1], 4), \n      '\\nBest parameters: ', include_zero[0])\nprint('\\nWithout zero rating difference \\nBest accuracy: ', round(non_zero[1], 4), \n      '\\nBest parameters: ', non_zero[0])\n\nresults.append({'model': 'K Neighbors', \n                'non_zero': round(non_zero[1], 4), \n                'include_zero': round(include_zero[1], 4)}\n              )","5be2a88a":"def random_forest_classifier(games):    \n    x = np.array(games['rating_difference']).reshape(-1, 1)\n    y = games['t1_win']\n\n    x = scale(x)\n\n    hyperparameters = {\n        'n_estimators': [10, 15, 20, 25],\n        'max_depth': [2, 3, 4],\n        'min_samples_split': [5, 10, 15, 20],\n        'max_leaf_nodes': [20, 25, 30, 35],\n        'random_state': [1]\n\n    }\n    rfc = RandomForestClassifier()\n    grid = GridSearchCV(rfc, param_grid=hyperparameters, cv=5)\n    grid.fit(x, y)\n\n    return grid.best_params_, grid.best_score_\n    \ninclude_zero = random_forest_classifier(games)\nnon_zero = random_forest_classifier(non_zero_games)\nprint('With zero rating difference \\nBest accuracy: ', round(include_zero[1], 4), \n      '\\nBest parameters: ', include_zero[0])\nprint('\\nWithout zero rating difference \\nBest accuracy: ', round(non_zero[1], 4), \n      '\\nBest parameters: ', non_zero[0])\n\nresults.append({'model': 'Random Forest', \n                'non_zero': round(non_zero[1], 4), \n                'include_zero': round(include_zero[1], 4)}\n              )","6d7ea88b":"def cvc(games):\n    x = np.array(games['rating_difference']).reshape(-1, 1)\n    y = games['t1_win']\n\n    x = scale(x)\n\n    hyperparameters = {}\n\n    svc = SVC()\n    grid = GridSearchCV(svc, param_grid=hyperparameters, cv=10)\n    grid.fit(x, y)\n\n    return grid.best_params_, grid.best_score_\n    \ninclude_zero = cvc(games)\nnon_zero = cvc(non_zero_games)\nprint('With zero rating difference \\nBest accuracy: ', round(include_zero[1], 4))\nprint('\\nWithout zero rating difference \\nBest accuracy: ', round(non_zero[1], 4))\n\nresults.append({'model': 'CVC', \n                'non_zero': round(non_zero[1], 4), \n                'include_zero': round(include_zero[1], 4)}\n              )","d966190c":"def sgd(games):\n    x = np.array(games['rating_difference']).reshape(-1, 1)\n    y = games['t1_win']\n\n    x = scale(x)\n\n    hyperparameters = {\n        'loss': ['log', 'hinge', 'squared_loss'],\n        'max_iter': [10000],\n        'shuffle': [False]\n    }\n    sgdc = SGDClassifier()\n    grid = GridSearchCV(sgdc, param_grid=hyperparameters, cv=10)\n    grid.fit(x, y)\n\n    return grid.best_params_, grid.best_score_\n    \ninclude_zero = sgd(games)\nnon_zero = sgd(non_zero_games)\n\nprint('With zero rating difference \\nBest accuracy: ', round(include_zero[1], 4), \n      '\\nBest parameters: ', include_zero[0])\nprint('\\nWithout zero rating difference \\nBest accuracy: ', round(non_zero[1], 4), \n      '\\nBest parameters: ', non_zero[0])\n\nresults.append({'model': 'SGD', \n                'non_zero': round(non_zero[1], 4), \n                'include_zero': round(include_zero[1], 4)}\n              )","8bef618a":"results_df = pd.DataFrame(results)\n\nfig, ax = plt.subplots(figsize=(7, 7))\n\nax.bar(x=results_df['model'], \n       height=results_df['non_zero'], \n       label='Excluding Zero Difference')\nax.bar(x=results_df['model'], \n       height=results_df['include_zero'], \n       label='Including Zero Difference')\n\nax.set_ylim(bottom=0.5)\nax.set_title('Accuracy of Models', {'fontsize':16})\nax.set_ylabel('Accuracy', {'fontsize':12})\nax.set_xlabel('Model', {'fontsize':12})\nax.legend(loc='upper right')\n\nplt.show()","70772db8":"def create_model():\n\n    model = keras.Sequential()\n    model.add(keras.layers.Dense(2, activation=\"softmax\"))\n\n    opt = tf.keras.optimizers.Adam()\n\n    model.compile(loss='sparse_categorical_crossentropy',\n                  optimizer=opt,\n                  metrics=['accuracy'])\n\n    return model\n\n# Don't turn x into an nparray yet, \n# so we can track the non scaled values during the shuffle to keep comparisons intuitive.\nx = non_zero_games['rating_difference']\nnon_zero_games['scaled_x'] = scale(x)\ny = np.array(non_zero_games['t1_win'])\n\ntrain_x, test_x, train_y, test_y = train_test_split(non_zero_games[['rating_difference', \n                                                                    'scaled_x']], \n                                                    y, \n                                                    test_size=0.2, \n                                                    random_state=1, \n                                                    shuffle=True\n                                                   )\n\nmodel = create_model()\nmodel.fit(np.array(train_x['scaled_x']), \n          train_y, \n          validation_data=(np.array(test_x['scaled_x']), \n          test_y), \n          epochs=100, \n          verbose=2\n         )","f07f0faa":"# This function matches the nparrays we fed into our model with their dataframe counterparts \ndef prob_details(x, y):    \n    probabilities = model.predict(np.array(x['scaled_x']))\n    with_ratings = pd.concat([pd.DataFrame(probabilities, \n                                           columns=['t2_confidence', 't1_confidence']),\n                              x['rating_difference'].reset_index(drop=True),\n                              pd.DataFrame(y, columns=['t1_win'])], \n                             axis=1\n                            )\n    return with_ratings","5b9d4a19":"test_details = prob_details(test_x, test_y)\n\nfig, ax1 = plt.subplots(figsize=(7, 7))\nax1.set_title('Confidence vs Rating Difference', {'fontsize': 16})\nax1.bar(test_details['rating_difference'], test_details['t1_confidence'])\nax1.set_xlabel('Rating Difference', {'fontsize': 12})\nax1.set_ylabel('Confidence', {'fontsize': 12})\n\nplt.plot()","ade3e67a":"def plot_confidence_win_rate(games):\n    plt.figure(figsize=(7, 7))\n    \n    bin_means, bin_edges, binnumber = stats.binned_statistic(games['rating_difference'], \n                                                             games['t1_confidence'], \n                                                             'mean', \n                                                             bins=10)\n    plt.hlines(bin_means, \n               bin_edges[:-1], \n               bin_edges[1:], \n               colors='r', \n               alpha=0.8, \n               label='Average Confidence')\n    \n    bin_means, bin_edges, binnumber = stats.binned_statistic(games['rating_difference'], \n                                                             games['t1_win'], \n                                                             'mean', \n                                                             bins=10)\n    plt.hlines(bin_means, \n               bin_edges[:-1], \n               bin_edges[1:], \n               colors='b', \n               alpha=0.8, \n               label='Average Win Rate')\n    \n    plt.legend(loc='upper left')\n    plt.title('Win Rates and Model Confidence', {'fontsize':16})\n    plt.ylabel('Confidence \/ Win Rate', {'fontsize':12})\n    plt.xlabel('Rating Difference', {'fontsize':12})\n    plt.show()","4bf4da45":"plot_confidence_win_rate(test_details)","5c8d5921":"train_details = prob_details(train_x, train_y)\nplot_confidence_win_rate(train_details)","99ad184d":"## Results of Regular Machine Learning","35f69385":"## C-Support Vector Classification","202d7c99":"We have a new winner! 56.86%!","aadce57f":"## Expected Win Rates\n\nI expect t1 to win more often than not if t1_rating > t2_rating. But the question is how often? We can get a brief understanding of what kind of model accuracies to expect by looking at the data.","f8d5aa48":"# Conclusion\n\nThis was meant to be a small introduction to the process and potential of applying machine learning to new applications. It is by no means enough to start bookmaking for CS:GO games on it's own. But by adding more proven features, you could build a model robust enough to profitably make books with a large enough vig.\n\nOther interesting ideas to explore:\n\n* Scraping more intricate team statistics and adding these to the model. \n* Adding historical betting odds to the model (This would be enormously helpful if you could find a good dataset).\n* Cutting out extreme rating differences from consideration.\n\nI'll keep working on this notebook for the next little while. I would like to upgrade most of the graphs and explore more ways of presenting relationships in the data. I'm also still curious about the effectiveness of removing extreme rating differences from consideration. \n\nThanks for taking a look at my notebook!","4af30041":"As we expected, excluding games with zero rating difference does increase the accuracy of our model. \n\nEven just using a decision tree classifier, we're able to get very close to the accuracy we expected from earlier (56.02% vs 56.53%). \n\nMore surprisingly, the model we ran including games with zero rating difference actually performed marginally better than the win rate we saw earlier (55.02% vs 54.54%).","2a67b0e0":"## \"Neural Network\"\n\nNow for the big guns! The world's smallest neural network!","34e14b52":"Looks pretty reasonable.\n\nIt should however be noted that the edge cases have fewer games in their respective sample sizes. We saw earlier how the linear relationship between the rating difference and the win rate deteriorated as the absolute rating difference increased. So we should put more stock into the middle section of these graphs, because the majority of the data exists in the portion between -100 and 100.","82654a7e":"As far as regular machine learning goes, the random forest classifier produced the best results with an accuracy of 56.86%.","507b74db":"# Introduction\n\nThe goal of this project was to produce win probabilities for any two counterstrike teams. The intention was to compare these probabilities with odds being offered by betting sites and place bets if their odds seem favorable when compared to my model. I went into this with a good deal of skepticism over potential profitability, since I was essentially trying to outperform the bookmakers at their own game. But I figured it would be a good exercise in a real world application of machine learning. \n\n## Back Story\n\nInitially, I only had about 600 games and their details pulled from the API at www.pandascore.co. I first changed the teams into dummy variables, then trained a model on these dummy variables, with game outcomes as labels. I managed to get some results from this, but figured if I could include more comprehensive stats, that would help a lot. So with a bit of feature engineering, I managed to include statistics like win rate, number of wins, and number of losses up until the game played. This worked out better than just the dummy variables. But I started playing with individual features and trying to find the single best predictive statistic to feed a neural network. Not all features are created equally. The best way to build a model is to start with the best individual features. \n\nEventually I scraped the team ratings from www.gosugamers.net and matched this to my game history data. Team ratings are a way of comparing teams in the global league. Teams win points by winning games, and lose points by losing games or being inactive for long enough. The higher the team rating, the better the team. Using team ratings worked out exceptionally well. In fact, suspiciously well... I was worried about how much data leakage was influencing my outcomes. Current ratings were undoubtedly leaking information that hadn't happened yet at the time of the games I was predicting. The dataset I was working with was also quite small, so I was hoping to work with a more extensive set.\n\nThis led me to scrape data from each team's history on the same site. Each team has a graph showing how their rating has changed over time, and the games that affected the change. So it was a matter of scraping data from as many of these graphs as possible, then matching up said data to the opponent's information for the same game. This dataset is the result of that scraping.\n\nI have already cleaned the data and dropped any tie games. It's a very simple dataset, but this is meant to serve as an introduction to analyzing individual features and their predictive power. The end goal would likely be to combine a number of these features to further enhance predictive power. But before throwing all possible features at a model for training, it's good to investigate their individual utility. ","5326fb97":"Nothing to write home about here.","c7df063a":"The distribution of rating_difference seems to be reasonably normal. But we will still normalize our data before feeding it into a network. This should reduce overfitting on the somewhat noisy training set. We may however want to consider dropping games with no rating difference to see how that affects our results.","e9d7dc4f":"Not bad, but not great!","6df0c5b3":"This graph shows how often t1 or t2 wins if their rating difference is greater than x. We can see that initially, the win rate increases linearly in relation to the difference in rating. But this relationship deteriorates as the sample of games decreases. It may be worth playing around with the dataset and dropping games with a rating difference too large to have a significant sample size. ","11647e26":"While this isn't the biggest imbalance, including the zero difference games could adversely affect our training, especially since there are so many games with no rating difference. We'll keep this in mind as we explore the expected win rates.","6c571b59":"## Initial Hiccups\n\nI started by feeding a deep neural network t1_rating and t2_rating. But the network was picking up on nuance in the data that it shouldn't have. Certain matchups of ratings had very small sample sizes. For example, the network was learning that a matchup of 923 vs 1124 was actually favorable for the 923 rating, since there was only one game of that matchup, and 923 had won.\n\nTo remedy this problem, I simplified the input data by reducing t1_rating and t2_rating into one statistic called rating_difference. I speculated that this would capture the important relationship between the two, while forcing our model to stop picking up on patterns between t1_rating and t2_rating that shouldn't be significant.\n\nI assumed the network wouldn't need a lot of neuron's to capture what I imagined as a linear relationship. So I stripped the network down to just one neuron. Since this isn't exactly harnessing the amazing powers of neural networks, I've decided to back up a step and see how some regular machine learning models compare to the ultimately shallow neural network.\n\n## Balancing The Data\n\nBefore we get started, we should do a little exploration of the data, and apply some preprocessing. For instance, if there are more t1 wins than losses, we'll want to balance the data so our model isn't influenced by the imbalance.\n","b0b24b4e":"There does seem to be at least a loose relationship between the model's confidence rates and the actual win rates of those games.\n\nThe relationship may make more sense if we compare predictions on the training data to the actual win rate. Normally this wouldn't be advised, since overfitting can make any model seem smarter than it is. However, the accuracy and loss between the training set and the test set are reasonably similar. Also, since there is only one neuron, the model doesn't really have the complexity to overfit on the training data. This all suggests that overfitting is relatively insubstantial, and we can look at our performance on the training set with a pinch of salt.","e34fddc8":"Not quite as good as the decision tree classifier. ","dfe4117f":"## Random Forest Classifier","4ccaabb6":"This gives us a pretty intuitive understanding of the linear relationship the model comes up with between confidence and the rating difference. \n\nNow let's see how the model's confidence compares to the actual win rates of the data.","cfa1c43d":"Since the model is only one neuron, the model should be creating a simple linear relationship between rating_difference and t1_win. We can see this is the case by looking at the graph of t1_confidence.","e1042304":"## Decision Tree Classifier","38641afd":"While this isn't a huge imbalance, it would likely bias the model somewhat into assuming t2 wins more than t1.","8f761bd8":"## K Nearest Neighbors","273eae1c":"## Stochastic Gradient Descent","a21db6a6":"# Applying Machine Learning","c6accbad":"As expected, the win rate increases if we remove zero difference games from consideration. But even in this case, t1 wins only 56.5% of the time when their rating is better than t2, and conversely loses about 43.5% of the time if their rating is worse than t2. This is far from amazing predictive power. But we would expect that win rate to go up in relation to the difference in rank. "}}