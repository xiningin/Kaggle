{"cell_type":{"4599767e":"code","583280ce":"code","b5b870b3":"code","d621cf97":"code","991996c9":"code","9a161242":"code","26eab0d5":"code","d3e2acf9":"code","5e6406d3":"code","24f6e26c":"code","4dfdc828":"code","10b861ba":"markdown","769ee0b8":"markdown","ceb00e0e":"markdown","1ce5ffdb":"markdown","284f3fd1":"markdown","c8a75ebc":"markdown","45dddc6d":"markdown","3657b1a6":"markdown","ff92491e":"markdown","4830ac0d":"markdown"},"source":{"4599767e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","583280ce":"import pandas as pd\n\nfn = r\"\/kaggle\/input\/twitter-product-sentiment-analysis\/Twitter Product Sentiment Analysis.csv\"\ndf = pd.read_csv(fn)\n\nprint(df.shape)\nprint(df.head(3))","b5b870b3":"import nltk\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import sentiwordnet as swn\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nnltk.download('wordnet')\nnltk.download('sentiwordnet')\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nlemmatizer = WordNetLemmatizer()","d621cf97":"!pip install tweet-preprocessor\nimport preprocessor as p\np.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.SMILEY, p.OPT.EMOJI)","991996c9":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\ncleantweets = []\npostags = []\nimport string\ntable = str.maketrans('','', string.punctuation)\n\nfor tweet in df['tweet']:\n    try:\n        tweet = p.clean(tweet)\n        #tokenize + lower case\n        words2 = word_tokenize(tweet.lower())\n        #remove puncts\n        words3 = [w.translate(table) for w in words2]\n        #remove stopwords\n        words4 = [word for word in words3 if word not in stop_words]\n        #applying lemmatization\n        words5 = [lemmatizer.lemmatize(word) for word in words4]\n                \n        #combining all words\n        cleantweets.append((\" \".join(words5)).strip())\n    except:\n        cleantweets.append(tweet)\n        continue\n\nprint(len(cleantweets))\n\ndf['clean_Tweets'] = cleantweets","9a161242":"pos=neg=obj=count=0\n\npostagging = []\n\nfor tweet in df['clean_Tweets']:\n    list = word_tokenize(tweet)\n    postagging.append(nltk.pos_tag(list))\n\ndf['pos_tags'] = postagging","26eab0d5":"# Convert between the PennTreebank tags to simple Wordnet tags\ndef penn_to_wn(tag):\n    if tag.startswith('J'):\n        return wn.ADJ\n    elif tag.startswith('N'):\n        return wn.NOUN\n    elif tag.startswith('R'):\n        return wn.ADV\n    elif tag.startswith('V'):\n        return wn.VERB\n    return None","d3e2acf9":"# Returns list of pos-neg and objective score. But returns empty list if not present in senti wordnet.\ndef get_sentiment(word,tag):\n    wn_tag = penn_to_wn(tag)\n    \n    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n        return []\n\n    #Lemmatization\n    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n    if not lemma:\n        return []\n\n    #Synset is a special kind of a simple interface that is present in NLTK to look up words in WordNet. \n    #Synset instances are the groupings of synonymous words that express the same concept. \n    #Some of the words have only one Synset and some have several.\n    synsets = wn.synsets(word, pos=wn_tag)\n    if not synsets:\n        return []\n\n    # Take the first sense, the most common\n    synset = synsets[0]\n    swn_synset = swn.senti_synset(synset.name())\n\n    return [synset.name(), swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score()]","5e6406d3":"pos=neg=obj=count=0\nsenti_score = []\n\nfor pos_val in df['pos_tags']:\n    senti_val = [get_sentiment(x,y) for (x,y) in pos_val]\n    for score in senti_val:\n        try:\n            pos = pos + score[1]  #positive score is stored at 2nd position\n            neg = neg + score[2]  #negative score is stored at 3rd position\n        except:\n            continue\n    senti_score.append(pos - neg)\n    pos=neg=0    \n    \ndf['senti_score'] = senti_score","24f6e26c":"df.head","4dfdc828":"df.to_csv(\"Sentiment_predited_result.csv\")","10b861ba":"As we have directly used scores provided to words in SentiWordNet library, We missed one important factor to take into consoderation here.\n\n**=> Context** <br>\n\npos_tag of NLTK is not context sensitive. Hence, Whatever pos_tag we got for Tweet words are also not based on context in which word is used. As you can refer above image, <br>\n\n<1> We went to the river bank <br>\n<2> I need to go to bank to make a deposit. <br>\n\nHere **bank** word is common in both sentences, but it is used in different context. <br>\n\nTo take into consideration the context of the word and Improve the Sentiment Score accuracy, We have to introduce NLP Word Embeddings like **ELMO** and **BERT** etc. to perform Sentiment Analysis.","769ee0b8":"### **SentiWordNet** is a library created by NLP community engineers for sentiment analysis. <br>\n\n### It contains POS Tags, Positivity score, Negativity score and SynsetTerms ie. in common language synonyms of a perticular words. <br>\n\n### As scores related to words are already define in SentiWordNet based on some rules [and we are not considering context or sequence of word in which a word is used], This approach is called as Rule Based Sentiment Analysis Approach. <br>\n\n### A screenshot of how SentiWordNet library looks like is given below:","ceb00e0e":"# **Text PreProcessing**\n\n* Removing Punctuations\n* Change Case\n* Tokenization\n* Removing Stop Words\n* Lemmatization","1ce5ffdb":"## **Functions for Sentiment Scoring**","284f3fd1":"![image.png](attachment:image.png)","c8a75ebc":"## **POS Tagging**","45dddc6d":"# **Importing Required Libraries**","3657b1a6":"![image.png](attachment:image.png)","ff92491e":"![image.png](attachment:image.png)","4830ac0d":"# **Why it is called as Rule Based Approach ?**"}}