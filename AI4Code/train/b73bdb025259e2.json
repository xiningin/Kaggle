{"cell_type":{"66345834":"code","4814b25c":"code","c0299b3d":"code","115aea61":"code","5353c683":"code","26856d98":"code","30b3c261":"code","2db1f950":"code","494d8648":"code","84628bad":"code","5b030f01":"code","b9ba6f30":"code","4fa292d8":"code","94f67e1f":"code","2e2d48b4":"code","3ea42df9":"code","63fba939":"code","2290cd33":"code","454712e7":"code","630e629a":"code","93bf0555":"code","6075023b":"code","45f8e4c8":"code","5b3afdfc":"code","3772f368":"code","f52f3d18":"code","2cbd191e":"code","e491f961":"code","0e1e3ee5":"code","31603171":"code","2e40c91f":"code","279c508f":"code","f82a126c":"code","ba559b9c":"markdown","076d7e3b":"markdown","9b6a1088":"markdown","fbd5c7e0":"markdown","da785243":"markdown","30f57f9e":"markdown","61aafc91":"markdown","998a95bb":"markdown","608ee7b6":"markdown","37b22e29":"markdown","3e8b35cb":"markdown","33a97705":"markdown"},"source":{"66345834":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport time\nimport requests\nimport re\nimport seaborn as sns\nfrom pandas.api.types import CategoricalDtype\nimport sys  \nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly import tools\nfrom plotly.offline import iplot, init_notebook_mode, plot\ninit_notebook_mode(connected=True)\nfrom itertools import combinations\n\n#Import the ML Libraries\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import linear_model as LM\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier as RFC\nfrom sklearn.ensemble import GradientBoostingClassifier as GBC\nfrom sklearn import svm\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import confusion_matrix, roc_curve, accuracy_score\nsns.set(style=\"whitegrid\", palette=\"muted\")\npd.options.display.float_format = '{:,.2f}'.format\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n%matplotlib inline","4814b25c":"#initialize data frames\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ndfset = [train, test]","c0299b3d":"Married_Dict = {\"Mrs\": 1,\"Mme\": 1, \"Lady\": 1, \"Mlle\": 1, \"th\": 1, \"Dr\": 1, \"Dona\": 1,\n                   \"Miss\":0, \"Ms\": 0}\nfemsub_list = []\n\n#Functions to pull out middle names of males and females\ndef middle_name(lst):\n    try:\n        remaining = lst[2:]\n    except:\n        pass\n    \n    return \" \".join(remaining)\n\ndef fmiddle_name(lst):\n    try:\n        remaining = lst[1:]\n    except:\n        pass\n    \n    return \" \".join(remaining)\n\n#Following are Regex patterns to be used later\npattern1 = r'\\(\\w+\\s(\\w+\\s)?\\w+\\)' #Pattern to find Wife name\npattern2 = r'\\w+\\s' #Pattern to find Husband Name\n\n#We try to make all changes in one for-loop pass\nfor df in dfset:\n    #Rename the Name Column and separate Name components\n    df.rename(columns={\"Name\":\"Full_Name\"}, inplace = True)\n    df[\"Last_Name\"] = df.Full_Name.str.split(',').apply(lambda x: x[0])\n    df[\"First_Name\"] = df.Full_Name.str.split(',').apply(lambda x: x[1]).str.slice(1)\n    \n    #Pull out titles\n    df[\"Title\"] = df.First_Name.str.split(\" \").\\\n                                apply(lambda x: x[0]).str.slice(0,-1)\n    \n    #Use female titles to determine whether married\n    \n    df[\"Is_Married_Bin\"] = df.Title.loc[df.Sex==\"female\"].map(Married_Dict).astype(\"int\")\n    \n    #create a set of married femalse\n    femsub = df.loc[df.Sex==\"female\"].Is_Married_Bin\n    femsub_list.append(femsub)\n    \n    #Try to track and match the Married Male pairs\n    #First create some dummy series to manipulate\n    married_firstname, married_lastname = df.loc[(df.Is_Married_Bin == 1) & (df.Sex == 'female'), 'First_Name'] ,\\\n    df.loc[(df.Is_Married_Bin == 1) & (df.Sex == 'female'), 'Last_Name'] #We split the women's names into 2 series\n    df['Spouse_First'] = np.nan #We Initialize empty series for the Spouse First name\n    df['Middle_Name'] = np.nan #We Initialize emptys series for middle names\n    df['Couple_Survived'] = np.nan\n    df['Spouse_Present'] = 0\n    df['Cabin_Rooms'] = 0 #To see how many rooms each person has\n    df['Cabin_Group'] = np.nan\n    df['Cabin_Number'] = np.nan\n    try:\n        df.loc[df.Sex=='male','Middle_Name'] = df.loc[df.Sex=='male','First_Name'].str.split(\" \").apply(middle_name) #This pulls out the male middle names\n    except:\n        pass\n    df.loc[df.Sex=='male','First_Name'] = df.loc[df.Sex=='male','First_Name'].str.split(\" \").apply(lambda x: x[1]) #This pulls out male first name\n    \n    \n    #Next create for loop to pull out the wife first name and husband names\n    for e in married_firstname.index:\n        try:\n            wife_first = (re.search(pattern1,married_firstname[e])[0]) \n            wife_first = wife_first[1:-1] #Use regex pattern to strip out name and then remove parenthesis\n            husband_first = (re.search(pattern2,married_firstname[e]))[0]\n            df.loc[e,['First_Name','Spouse_First']] = wife_first, husband_first.replace(' ','')\n\n        except:\n            pass\n    \n    try:\n        df.loc[df.Sex=='female', 'Middle_Name'] = df.loc[df.Sex=='female','First_Name'].str.split(\" \").apply(fmiddle_name)\n    except:\n        pass\n    df.loc[df.Sex=='female','First_Name'] = df.loc[df.Sex=='female','First_Name'].str.split(\" \").apply(lambda x: x[0])\n    \n    #Now we should have the females matched to males...or atleast a clue\n    \n    #We will create a list of potential men who can be married\n    married_men_candidate = {name: [] for name in married_lastname.unique()}\n    \n    #For Men women, we will create a dictionary of last names and number of married women\n    married_women = {name: married_lastname.value_counts()[i] for i, name in enumerate(married_lastname.value_counts().index)}\n    \n    #Fill in the empty married_men_candidates with potential first names\n    for e in married_lastname.unique():\n        married_men_candidate[e] = list(df.loc[(df.Last_Name == e) & (df.Sex=='male'), 'First_Name'])\n        if len(married_men_candidate[e]) == 0: #Remove keys with empty lists\n            del married_men_candidate[e]\n    \n    #The following For Loop wil map the male candidates to their wives\n    #If the male candidate and wife match, we must flip the Spouse_Present Parameter to \"1\", and update the male \"Spouse_First\"\n    for lastname in married_men_candidate: #Pull out each last name\n        for m_name in married_men_candidate[lastname]: #Loop through each first name of Last name\n            for f_name in df.loc[(df.Last_Name == lastname) & (df.Sex == 'female'),'Spouse_First']: #Pull up Spouse candidates\n                try:\n                    if m_name == f_name: #If the spouse first name and male first name match\n                        female_name = df.loc[(df.Last_Name==lastname)&(df.Sex =='female') & (df.Spouse_First==f_name),'First_Name']\n                        df.loc[(df.Sex=='male') & (df.First_Name == m_name) & (df.Last_Name ==lastname),'Spouse_First'] = female_name[female_name.index[0]]\n                        df.loc[(df.Sex=='male') & (df.First_Name == m_name) & (df.Last_Name ==lastname),'Spouse_Present'] = 1\n                        df.loc[(df.Sex=='female') & (df.Spouse_First == f_name) & (df.Last_Name== lastname),'Spouse_Present'] = 1\n\n                except:\n                    pass\n\n    \n    #Convert Pclass to ordered Categories\n    cat_type_class = CategoricalDtype(categories=[3,2,1], ordered=True)\n    df[\"Pclass_cat\"] = df.Pclass.astype(cat_type_class)\n    \n    #Round up Fares to clean up the data\n    df['Fare'] = np.ceil(df['Fare'])\n    \n    df['Is_Married_Bin'].fillna(0, inplace=True)\n    df.loc[df.Age.isnull(),'Age'] = np.random.randint(15,45)\n    \n    #Lets perform a For-Loop through the cabins to extract relevant features\n    #Pull out number of rooms, Cabin group and Cabin room number\n    for room in df.loc[df.Cabin.notnull(),'Cabin'].index:\n        try:\n            rooms = df.loc[room,'Cabin'].split(' ') #First split cabin string into list of different rooms\n            df.loc[room, 'Cabin_Rooms'] = len(rooms)\n            for room1 in rooms: #Split individual rooms into string units to pull out extra info\n                df.loc[room, 'Cabin_Group'] = room1[0]\n                df.loc[room, 'Cabin_Number'] = room1[1:]\n        except:\n            df.loc[room,'Cabin_Rooms'] = 0\n            \n\n\n","115aea61":"#Implement Dummy variables for Sex that can't be done with For-Loop :(\nsex_male = pd.get_dummies(train.Sex, prefix=\"Sex\", drop_first=True)\nembarking = pd.get_dummies(train.Embarked, prefix='Embarked', drop_first=True)\ncabin_grp = pd.get_dummies(train.Cabin_Group, prefix='Cabin_Grp', drop_first=True)\ntrain = pd.concat([train, sex_male, embarking, cabin_grp], axis=1)\n\nsex_male = pd.get_dummies(test.Sex, prefix='Sex', drop_first=True)\nembarking = pd.get_dummies(test.Embarked, prefix='Embarked', drop_first=True)\ncabin_grp = pd.get_dummies(test.Cabin_Group, prefix='Cabin_Grp', drop_first=True)\ntest = pd.concat([test, sex_male, embarking, cabin_grp], axis=1)","5353c683":"Null_Hypo = ['Not Survied', 'Survied']\n\nprint(Null_Hypo[np.argmax([1-train.Survived.mean(), train.Survived.mean()])], '{:.2f}%'.format(max(1-train.Survived.mean(), train.Survived.mean())*100))\n\n","26856d98":"first = train.groupby('Cabin_Rooms')['Fare', 'Survived','Pclass'].mean()\nsecond = train.groupby('Cabin_Rooms')['Full_Name'].count()\ncabins = pd.concat([first, second], axis=1)\ncabin_df = pd.DataFrame(cabins)\ncabin_df.rename(columns={'Full_Name':'Num_People'}, inplace=True)\ncabin_df['Approx_Dead'] = round((1-cabin_df['Survived'])*cabin_df['Num_People'])\n\nbar_fig = {'data': [],\n          'layout': {}}\n\nbar_fig['layout'] = go.Layout(\n    autosize = False, width = 750, height=500, \n    title = 'Cabin Perspective',\n    xaxis = dict(title = 'Cabin Rooms',\n                 showline = True,\n                 dtick = 1,\n                 showgrid = False,\n                domain=[0,0.8]),\n    yaxis = dict(title='Fares',\n                 showline = True,\n                 showgrid = False\n                ),\n    yaxis2 = dict(title = 'Number of People',\n                 showline = True,\n                 overlaying = 'y', #Allows for second axis\n                 side = 'right',\n                 anchor = 'x'),\n    \n    yaxis3 = dict(title = 'Average Pclass',\n                  showline = False,\n                  overlaying = 'y',\n                  side = 'right',\n                  anchor = 'free',\n                  position = 1\n                 ),\n    legend = dict(x = 1.15, y = 1),\n    hovermode = 'closest'\n\n)\n\nbar_data = go.Bar(x= cabin_df.index, \n                  y=cabin_df.Fare, \n                  name='Fare'\n                 )\nline_data = go.Scatter(x=cabin_df.index, \n                       y=cabin_df.Approx_Dead, \n                       yaxis = 'y2', \n                       name='Approx Dead',\n                       mode='lines'\n                      )\nline2_data = go.Scatter(x=cabin_df.index,\n                      y=cabin_df.Num_People,\n                      name='Number of People',\n                      mode='lines',\n                      yaxis = 'y2',\n                     )\ndot_data = go.Scatter(x=cabin_df.index,\n                     y=cabin_df.Pclass,\n                     name='Avareage Pclass',\n                     mode='markers',\n                     yaxis = 'y3')\n\nbar_fig['data'].append(bar_data)\nbar_fig['data'].append(line_data)\nbar_fig['data'].append(line2_data)\nbar_fig['data'].append(dot_data)\n\niplot(bar_fig)","30b3c261":"# Lets visualize a little\n\nsurv_set = train.loc[train.Survived==1]\n\nfacplot = sns.FacetGrid(surv_set, col='Embarked', row='Pclass', col_order=['C','S','Q'])\nfacplot.map(sns.kdeplot, 'Sex_male')\nfacplot.fig.suptitle('Gender Survival Density Distribution', y=1.1)","2db1f950":"# Lets visualize a little\n\ndeath_set = train.loc[train.Survived==0]\n\nfacplot = sns.FacetGrid(death_set, col='Embarked', row='Pclass', col_order=['C','S','Q'])\nfacplot.map(sns.kdeplot, 'Sex_male')\nfacplot.fig.suptitle('Gender Death Density Distribution', y=1.1)","494d8648":"#Create some plots which help distinguish trends based on age and class based on sex\nsns.lmplot(x=\"Pclass_cat\", y=\"Survived\", hue=\"Sex_male\", height=7, palette = ['r','y'], data=train)\nsns.lmplot(x='Age', y='Survived', data=train, hue=\"Sex_male\", palette = ['r','y'], height = 7)","84628bad":"#Create a graph which plots Number of Female survival victims split by Pclass and age. \nfemaleSet = train.loc[train.Sex==\"female\"]\nf_pop_dist = femaleSet.Pclass.value_counts(normalize=True)\nsns.lmplot(x=\"Is_Married_Bin\", y = \"Survived\",  scatter=True, hue=\"Pclass\",\\\n           palette = ('r','c','y'), data=femaleSet, logistic=True) \n#sns.lmplot(data=femaleSet, y = \"Survived\", x=\"Age\",  palette = ('r','c','y'), hue='Pclass', logistic=True)","5b030f01":"#Lets get a better picture picture on fare vs Pclass\n\nfigure = {'data': [],\n         'layout': {}}\n\nfigure['layout'] = go.Layout(\n    autosize = False, width = 500, height=500, \n    title = 'Pclass vs Fares',\n    xaxis = dict(title = 'Pclass',\n                 showline = True,\n                 dtick = 1,\n                 showgrid = False),\n    yaxis = dict(title='Fares',\n                 showline = True,\n                 showgrid = False\n                ),\n    hovermode = 'closest'\n)\n\nfor cls in train.Pclass.unique():\n    pset = train.loc[train.Pclass == cls]\n    y = pset['Fare']\n    x = [cls]*len(pset['Fare'])\n    name = 'Pclass {}'.format(cls)\n    text_list = []\n    for ix in pset['Fare'].index:\n        text = 'Person: {pr}<br>Age: {ag}<br>Port: {pt}'.format(pr = pset.loc[ix,'Full_Name'], \n                                                            ag = pset.loc[ix,'Age'], \n                                                            pt = pset.loc[ix,'Embarked'])\n        text_list.append(text)\n    data = go.Scatter(x = x, y = y, name = name, mode = 'markers', text = text_list)\n    figure['data'].append(data)\n\niplot(figure)","b9ba6f30":"# To kick off our ML side of the excercise, we're going to build a function to give us an accuracy breakdown\n\ndef c_matrix(y_true,y_pred):\n    scoring = confusion_matrix(y_true,y_pred) #Following creates array for confusion matrix\n    tn, fp, fn, tp = scoring.ravel() #Create individual parameters of confusino matrix\n    \n    test = pd.DataFrame(scoring, columns = [('Actual', 'Negative'), ('Actual', 'Positive')], \n                        index = [('Predicted', 'Negative'), ('Predicted', 'Positive')])\n    \n    #Accuracy variables\n    accu_rate = accuracy_score(y,y_pred) \n    \n    recall = tp\/(tp+fn)\n    precision = tp\/(tp+fp)\n    specificity = tn\/(tn+fp)\n    npv = tn\/(tn+fn)\n    \n    return test, accu_rate, recall, precision, specificity, npv\n\n\n","4fa292d8":"#Next we will create a group of features to later split into feature groups in the feature set\ncols_set = ['Pclass', 'Is_Married_Bin','Spouse_Present','Cabin_Rooms', \n           'Sex_male', 'Embarked_Q', 'Embarked_S', 'Cabin_Grp_B', 'Cabin_Grp_C', 'Cabin_Grp_D', \n            'Cabin_Grp_E','Cabin_Grp_F', 'Cabin_Grp_G', 'Cabin_Grp_T']\n\n\n#The fuction creates combinations...so it kinda becomes a bit like a random tree regressor\n#We will use the logistic regression class to see if we can get some different error sets\ndef feature_create(feature_set):\n    two_group = []\n    three_group = []\n    four_group = []\n    five_group = []\n    for item in combinations(feature_set, 2):\n        two_group.append(list(item))\n    for item in combinations(feature_set, 3):\n        three_group.append(list(item))\n    #for item in combinations(feature_set, 4):\n       # four_group.append(list(item))\n    #for item in combinations(feature_set, 5):\n       # five_group.append(list(item))\n    \n    grp_list = [two_group, three_group]#, four_group, five_group]\n    \n    return grp_list\n\nfeature_set = feature_create(cols_set)\n\ntrain[cols_set].isnull().sum()","94f67e1f":"#Start setting up your Machine Learning Parameters, begin by testing logistic regression\n\n\nlog_confus = []\nlog_accu = []\nlog_recall = []\nlog_precise = []\nlog_spec = []\nlog_npv = []\nlog_feat = []\n\ny = train.Survived\nlog = LogisticRegression()\nrand_state = range(20)\nlog_param_grid = dict(random_state=rand_state)\nfor feat_grp in feature_set:\n    for feat_cols in feat_grp:\n        X = train[feat_cols]\n        gridLog = GridSearchCV(log, log_param_grid, cv=2, scoring = 'accuracy')\n        gridLog.fit(X,y)\n        best_log = gridLog.best_estimator_\n        y_pred = best_log.predict(X)\n        confus_matrix, accu, recall, precise, spec, npv = c_matrix(y,y_pred)\n        log_confus.append(confus_matrix), log_accu.append(accu), log_recall.append(recall)\n        log_precise.append(precise), log_spec.append(spec), log_npv.append(npv) \n        \n        feat_text = \",\".join(feat_cols)\n        \n        log_text = \"Feature Cols: {}\"\n        feat_text = \",\".join(feat_cols)\n        log_text = \"Feature Cols: {}\".format(feat_text)\n        log_feat.append(log_text)\n\n        ","2e2d48b4":"def confus_fig(accu, recall, precise, feat, spec, npv, title):\n    fig = {'data': [],\n              'layout': {}}\n\n    fig['data'].append(go.Scatter(x=accu, \n                                      y=precise, \n                                      text=feat, \n                                      mode='markers', \n                                      name = 'Precision vs Accuracy'\n                                     ))\n    fig['data'].append(go.Scatter(x=accu, \n                                      y=recall, \n                                      text=feat, \n                                      mode='markers', \n                                      name='Recall vs Accuracy'\n                                     ))\n    fig['data'].append(go.Scatter(x=accu,\n                                      y=npv,\n                                      text=feat,\n                                      mode='markers',\n                                      name='NPV vs Accuracy'))\n    fig['data'].append(go.Scatter(x=accu,\n                                      y=spec,\n                                      text=feat,\n                                      mode='markers',\n                                      name='Specifity vs Accuracy'\n                                     ))\n    fig['layout']['title'] = '{} Training Error Results'.format(title)\n    fig['layout']['hovermode'] = 'closest'\n    fig['layout']['xaxis'] = dict(title='Accuracy',\n                                  range=[0.5,.9],\n                                     showline = True,\n                                     showgrid = False)\n    fig['layout']['yaxis'] = dict(title='Positive: Precision & Recall',\n                                  range=[0.5,1],\n                                     showline = True,\n                                     showgrid = False)\n    fig['layout']['yaxis2'] = dict(title='Negative: Specifity & NPV',\n                                   range=[0.5,1],\n                                      showline = True,\n                                      showgrid = False)\n    \n    return fig\n\nlog_fig = confus_fig(log_accu, log_recall, log_precise, log_feat, log_spec, log_npv, \"Logistic Regression\")\niplot(log_fig)","3ea42df9":"top5_accu = log_accu.copy()\ntop5_accu = list(set(top5_accu))\ntop5_accu.sort(reverse=True)\ntop5_accu = top5_accu[0:5]\nfor i, a in enumerate(top5_accu):\n    log_index = log_accu.index(a)\n    print(i+1, log_feat[log_index], \", Accuracy {:,.2f}%\".format(a*100))","63fba939":"col_set2 = ['Pclass', 'Is_Married_Bin','Spouse_Present','Cabin_Rooms', 'Fare',\n           'Sex_male', 'Embarked_Q', 'Embarked_S', 'Cabin_Grp_B', 'Cabin_Grp_C', 'Cabin_Grp_D', \n            'Cabin_Grp_E','Cabin_Grp_F', 'Cabin_Grp_G', 'Cabin_Grp_T']\nfeature_set2 = feature_create(col_set2)","2290cd33":"svc_confus = []\nsvc_accu = []\nsvc_recall = []\nsvc_precise = []\nsvc_spec = []\nsvc_npv = []\nsvc_feat = []\n\nsvc = LinearSVC()\n\nsvc_param_grid1 = {'penalty':['l1','l2'],\n                 'random_state':[i for i in range(1,20)],\n                   'dual': [False]\n                 }\nsvc_param_grid2 = {'penalty': ['l2'],\n                   'loss': ['hinge'],\n                  'random_state': [i for i in range(1,20)]\n                  }\nfor feat_grp in feature_set2:\n    for feat_cols in feat_grp:\n        X = train[feat_cols]\n        grid_svc = GridSearchCV(svc,[svc_param_grid1, svc_param_grid2], cv=2, scoring='accuracy')\n        grid_svc.fit(X,y)\n        best_svc = grid_svc.best_estimator_\n        ypred = best_svc.predict(X)\n        confus_matrix, accu, recall, precise, spec, npv = c_matrix(y,ypred)\n        svc_confus.append(confus_matrix), svc_accu.append(accu), svc_recall.append(recall)\n        svc_precise.append(precise), svc_spec.append(spec), svc_npv.append(npv)\n        \n        feat_text = ','.join(feat_cols)\n        svc_text = \"Feature Cols: {}\".format(feat_text)\n        svc_feat.append(svc_text)\n\n        \n        \n        \n        \n\n","454712e7":"svc_fig = confus_fig(svc_accu, svc_recall, svc_precise, svc_feat, svc_spec, svc_npv, \"Linear SVC Regression\")\niplot(svc_fig)","630e629a":"print(\"The Linear SVC best Test Score: {:.2f}%\".format(grid_svc.cv_results_['mean_test_score'].max()*100))\nprint('The Linear SVC best Train Score: {:.2f}%'.format(np.asarray(svc_accu).max()*100))\n\ntop5_accu = svc_accu.copy()\ntop5_accu = list(set(top5_accu))\ntop5_accu.sort(reverse=True)\ntop5_accu = top5_accu[0:5]\nfor i, a in enumerate(top5_accu):\n    svc_index = svc_accu.index(a)\n    print(i+1, svc_feat[svc_index], \", Accuracy {:,.2f}%\".format(a*100))","93bf0555":"#Next we are gonna try out the Random Tree Classifier\ntree = RFC()\n\ntree_param_grid = dict(n_estimators= [i for i in range(1,10)], \n                   max_depth= [None]+[i for i in range(1,10)], \n                   max_features= ['auto']+[i for i in range(2,5)]\n                      )\n\n\ngridTree = GridSearchCV(tree, tree_param_grid, cv=10, scoring='accuracy' )\n\n","6075023b":"X = train[col_set2]\ngridTree.fit(X,y)\n\n","45f8e4c8":"#Now that we have a fitted tree, lets create some dictionaries to later produce charts\ndef grid_output(gridobj):\n    rank_dict = {}\n    top5_testerror = {}\n    top5_param = {}\n    top5_trainerror = {}\n    rank = 1\n    while len(rank_dict)<5:\n        #Create dictionary of top 5 performing trees\n        ranks = gridobj.cv_results_['rank_test_score']\n        rank_dict[rank] = np.where(ranks == rank)\n        top5_testerror[rank] = []\n        top5_param[rank] = []\n        top5_trainerror[rank] = []\n        if len(rank_dict[rank][0]) == 0:\n            del rank_dict[rank] \n            del top5_testerror[rank] \n            del top5_trainerror[rank]\n            del top5_param[rank]\n        rank +=1\n\n    for rnk in rank_dict:\n        for lst in rank_dict[rnk]:\n            for ix in lst:\n                top5_param[rnk].append(gridobj.cv_results_['params'][ix])\n                top5_testerror[rnk].append(gridobj.cv_results_['mean_test_score'][ix])\n                top5_trainerror[rnk].append(gridobj.cv_results_['mean_train_score'][ix])        \n                \n    return rank_dict, top5_testerror, top5_trainerror, top5_param\n\nrank_dict, top5_testerror, top5_trainerror, top5_param = grid_output(gridTree)","5b3afdfc":"#Create an output to show the test and training error of the\n\nrandtree_fig = {'data': [],\n               'layout': {}}\nrandtree_fig['layout']['title'] = 'Random Tree Forest Training\/Test Error Results'\nrandtree_fig['layout']['hovermode'] = 'closest'\nrandtree_fig['layout']['xaxis'] = dict(title='Max depth',\n                                       dtick=1,\n                                       showline = True,\n                                       showgrid = False)\nrandtree_fig['layout']['yaxis'] = dict(title='Testing Accuracy Rate',\n                                       range = [0.80,.90],\n                                 showline = True,\n                                 showgrid = False)\nrandtree_fig['layout']['yaxis2'] = dict(title='Training Accuracy Rate',\n                                        range = [0.80,.90],\n                                        showline = True,\n                                        showgrid = False, \n                                        side = 'right',\n                                       overlaying = 'y')\nrandtree_fig['layout']['legend'] = dict(x= 1.1, y = 1)\nrandtree_fig['layout']['updatemenus'] = list([\n    dict(type='buttons',\n         active=-1,\n         buttons = list([\n             {'args': [{'visible': [True, False]},\n                       {'title': 'Random Forest Test Error Results'}],\n              'label': 'Test Error',\n              'method': 'update'},\n             {'args': [{'visible': [False,True]},\n                      {'title': 'Random Forest Train Error Results'}],\n             'label': 'Train Error',\n             'method':'update'},\n             {'args': [{'visible':[True,True]},\n                      {'title': 'Random Forest Train\/Test Error Results'}],\n             'label': 'Both',\n             'method': 'update'}\n             ]),\n         direction = 'left',\n         pad = {'r': 10, 't': 10},\n         showactive = True,\n         x = 0,\n         xanchor = 'left',\n         y = 1.05,\n         yanchor = 'top'\n        )\n])\n\n\n\n\n\n\n\nfor rnk in rank_dict:\n    train_ylist = []\n    test_ylist =[]\n    xlist = []\n    slist = []\n    tlist = []\n    for lst in top5_testerror[rnk]:\n        test_ylist.append(lst)\n    for lst in top5_trainerror[rnk]:\n        train_ylist.append(lst)\n    for lst in top5_param[rnk]:\n        xlist.append(lst['max_depth'])\n        slist.append(lst['n_estimators']*5)\n        tlist.append(\"Max Features: {}\".format(lst['max_features']))\n    test_data = go.Scatter(x = xlist,\n                           y = test_ylist,\n                           mode = 'markers',\n                           text = tlist,\n                           name = \"Rank {} Test Data\".format(rnk),\n                           marker = dict(size = slist,\n                                         opacity = .9\n                                        )     \n                          )\n    train_data = go.Scatter(x = xlist,\n                            y = train_ylist,\n                            yaxis = 'y2',\n                            mode = 'markers',\n                            text = tlist,\n                            name = \"Rank {} Train Data\".format(rnk),\n                            marker = dict(size = slist,\n                                         opacity = .9\n                                        )\n                           )\n    \n    randtree_fig['data'].append(test_data)\n    randtree_fig['data'].append(train_data)\n    \n    #for err in top5_testerror[rnk]:\n\niplot(randtree_fig)                        ","3772f368":"print(\"The RFC best Test Score: {:.2f}%\".format(gridTree.cv_results_['mean_test_score'].max()*100))\nprint('The RFC best Train Score: {:.2f}%'.format(gridTree.cv_results_['mean_train_score'].max()*100))\n\n","f52f3d18":"gbmTree = GBC()\n\ngbm_param_grid = {'learning_rate': [0.05,.1,.2,.5],\n                  'n_estimators': [80,90,100,110,120,150],\n                  'max_depth': [2,3,4,5]}\n\ngrid_gbmTree = GridSearchCV(gbmTree,gbm_param_grid, cv=10, scoring='accuracy')","2cbd191e":"grid_gbmTree.fit(X,y)","e491f961":"rank_dict, top5_testerror, top5_trainerror, top5_param = grid_output(grid_gbmTree)","0e1e3ee5":"#Create an output to show the test and training error of the\n\ngbm_fig = {'data': [],\n               'layout': {}}\ngbm_fig['layout']['title'] = 'Gradient Boosting Training\/Test Error Results'\ngbm_fig['layout']['hovermode'] = 'closest'\ngbm_fig['layout']['xaxis'] = dict(title='Number of Estimators',\n                                       dtick=10,\n                                       showline = True,\n                                       showgrid = False)\ngbm_fig['layout']['yaxis'] = dict(title='Testing Accuracy Rate',\n                                       range = [0.80,.95],\n                                 showline = True,\n                                 showgrid = False)\ngbm_fig['layout']['yaxis2'] = dict(title='Training Accuracy Rate',\n                                        range = [0.80,.95],\n                                        showline = True,\n                                        showgrid = False, \n                                        side = 'right',\n                                       overlaying = 'y')\ngbm_fig['layout']['legend'] = dict(x= 1.1, y = 1)\ngbm_fig['layout']['updatemenus'] = list([\n    dict(type='buttons',\n         active=-1,\n         buttons = list([\n             {'args': [{'visible': [True, False]},\n                       {'title': 'Gradient Boosting Test Error Results'}],\n              'label': 'Test Error',\n              'method': 'update'},\n             {'args': [{'visible': [False,True]},\n                      {'title': 'Gradient Boosting Train Error Results'}],\n             'label': 'Train Error',\n             'method':'update'},\n             {'args': [{'visible':[True,True]},\n                      {'title': 'Gradient Boosting Train\/Test Error Results'}],\n             'label': 'Both',\n             'method': 'update'}\n             ]),\n         direction = 'left',\n         pad = {'r': 10, 't': 10},\n         showactive = True,\n         x = 0,\n         xanchor = 'left',\n         y = 1.05,\n         yanchor = 'top'\n        )\n])\n\n\n\n\n\n\n\nfor rnk in rank_dict:\n    train_ylist = []\n    test_ylist =[]\n    xlist = []\n    slist = []\n    tlist = []\n    for lst in top5_testerror[rnk]:\n        test_ylist.append(lst)\n    for lst in top5_trainerror[rnk]:\n        train_ylist.append(lst)\n    for lst in top5_param[rnk]:\n        xlist.append(lst['n_estimators'])\n        slist.append(lst['max_depth']*10)\n        tlist.append(\"Learning Rate: {}\".format(lst['learning_rate']))\n    test_data = go.Scatter(x = xlist,\n                           y = test_ylist,\n                           mode = 'markers',\n                           text = tlist,\n                           name = \"Rank {} Test Data\".format(rnk),\n                           marker = dict(size = slist,\n                                         opacity = .9\n                                        )     \n                          )\n    train_data = go.Scatter(x = xlist,\n                            y = train_ylist,\n                            yaxis = 'y2',\n                            mode = 'markers',\n                            text = tlist,\n                            name = \"Rank {} Train Data\".format(rnk),\n                            marker = dict(size = slist,\n                                         opacity = .9\n                                        )\n                           )\n    \n    gbm_fig['data'].append(test_data)\n    gbm_fig['data'].append(train_data)\n    \n    #for err in top5_testerror[rnk]:\n\niplot(gbm_fig)   \n\n","31603171":"grid_gbmTree.cv_results_['mean_test_score'].max()\nprint(\"The GBM best Test Score: {:.2f}%\".format(grid_gbmTree.cv_results_['mean_test_score'].max()*100))\nprint('The GBM best Train Score: {:.2f}%'.format(grid_gbmTree.cv_results_['mean_train_score'].max()*100))\n\n\n","2e40c91f":"best_gbmTree = grid_gbmTree.best_estimator_","279c508f":"test['Fare'] = test['Fare'].fillna(0)\ntest['Cabin_Grp_T'] = 0\ntest_X = test[col_set2]\ny_test = best_gbmTree.predict(test_X)","f82a126c":"kaggle_submit = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': y_test})\n\n#kaggle_submit.to_csv('kaggle_submission.csv')","ba559b9c":"With this we have a Null rate of 61.5% - i.e I should expect to get ~61.5% of my predictions correct if I just pick \"not survived\" as my prediction","076d7e3b":"### What do we learn from the Facet Grids?\n\nNothing earth shattering, other than that people who left Queenstown clearly weren't very rich (as they were only in the 3rd class tickets...which isn't hugely helpful). Not the most helpful image...let's try out luck with a few other things","9b6a1088":"### What does this tell us?\n\nThis is a simple check on whether \"Fare\" is a material parameter - and it seems it isn't. Given the presence of inconsistent values, we see Fare tends to be less useful than Pclass but sometimes useful when the value is very large (hinting at specialness). This characteristic hints that a tree method might be useful (or use of non-linearity)","fbd5c7e0":"### What does this tell us?\n\nOur test using the Linear Support Vector Classifier from the Support Vector Machine Library did not yield substantially better results than the logistic regression. Overall, there isn't a substantialy difference in result thought it is curious that Fare is appearing as a relevant variable this time - let's try our luck using Fare in the test with a Random Forest Classifier","da785243":"### Summary\n\nThe graph shows the spectrum of accuracies accross depth. The size of the dot relates to the number of estimators. Hover over the dot to check the number of max features.\n\nThe above graph proves the adage that adding complexity can lead to overfitting. The graph shows the full spectrum and we can see the max test score and and max train score. From the charts we can see that adding to the max depth helps in improving the the training data but doesn't necessarily produce the best results with the test data. The current bench mark of the Random Tree Classifier beats the  Logistic Regression Train Error by a healthy margin. Even though the below best estmator attribute shows us that the gridsearchcv operation favors a max depth of 6, I think it's worth considering the max depth of 7 with smaller number of estimators as well.","30f57f9e":"### What does this tell us?\n\nWe can see that there is a clear divergence to being a female and that the not only is the probability of survival vastly different, but its sensitivity to Pclass is also a lot higher than the male group. Also, interestingly, the male group and female group have divergent trends as a function of age - this hints that a simple logistic regression. We see a correlation between Pclass and surival quite clearly","61aafc91":"### What does this tell us?\n\nSo far we have exceeded our estimate using the Null Hypothesis by using a healthy margin and that it is beneficial to mix the gender in all the predictions with mixed results when introducing whether spouse is present or cabin groups. \n\nSurprisingly, hovering over the dots shows that \"embarked\" performed quite poorly for accuracy prediction. The results of the confusion matrix show that narrowing down the features to presnet atleas tthe Spouse Present, Gender and evidence of a cabin group seem to present the highest probability of survival","998a95bb":"### Data Clean up and Formatting\n\n- [ ] We need to make uniform changes to the Train and Test sets so that the model we train can work on both sets\n- [ ] Our first task to clarify on the individual parameters\n    - Are people married?\n        - Find a way to connect married people\n    - Do they have special Titles?\n    - Do they have any children?\n    - Do they have any other type of family with them\n- [ ] Pclass and Cabin connection\n    - Also how do cabins play a role?\n    - Is there any pattern in the ticket prefixes","608ee7b6":"### Summary\n\nThe conclusion of this excercise (atleast for now) yields that the Gradient Boosting Method seems to be yielding the most accurate prediction. Though it is clear that we are at a higher risk of overfitting to the training data with the Gradient Boosting method, there doesn't seem to be material downside to using it vs the Random Forest Classifier.","37b22e29":"### What does this tell us?\n\nFrom what we can see above, there is clearly a correlation between survival and the number of rooms that you have. Further investigation needed on the Pclass trend breakdown","3e8b35cb":"# Titanic Survivors Kaggle Submission\n\n## Abstract\nThe purpose of my submission for the Kaggle ML competition is to determine which ML algo will most accurately predict whether passengers listed in a test.csv file survived the Titanic. The algo will be trained using data from a file called \"Train.CSV\" and tested using a \"Train-Test-Split\" method to measure the accuracy of the algo.\n\n**Procedures**\nFirst Steps...\n- Import, clean and recategorize data\n    - Test for Null\/Bad Values cross every column\n    - Name Column\n        - Use distinctions of Name to determine family orientation, relationship to port of departure\/class\n    - Sex Column\n        - break into dummy variables \n        - Use Female\/Name cross-section to determine \"Marriage\" title\n    - Convert the Pclass, Survived and Sex Columns into Category Types\n        \n        \nIf anyone comes across my first attempt, feel free to leave a comment! ","33a97705":"### Visualization\n\nNow that we have most of the formating out of the way (finally!) we can make some visualizations to start understanding the trends and key parameters"}}