{"cell_type":{"51acd8f1":"code","fd9c7141":"code","35105977":"code","2f01a4c5":"code","29bcc59f":"code","20e889c3":"code","556d7f81":"code","19e19b74":"code","f7c18173":"code","30d7b20d":"code","87c12627":"code","22a611e3":"code","9da8b0f0":"code","f52e4657":"code","e04699dd":"code","3f988c82":"code","724fcb4d":"code","a26afd2d":"code","b511efb1":"code","015940cd":"code","57a3feaa":"code","8cf951df":"code","c2ddea94":"code","fac1111f":"code","619417ee":"code","3e1b3e31":"code","19cd59a5":"code","f1da27d7":"code","c2dbdde2":"markdown","cfd07306":"markdown","2b0db9e0":"markdown","0834d3cc":"markdown","9da0e4d0":"markdown","59a8cf8d":"markdown","7d9686c0":"markdown"},"source":{"51acd8f1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fd9c7141":"# Loading neccesary packages. \nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\nfrom scipy import stats \nfrom scipy.stats import skew, boxcox_normmax, norm \nfrom scipy.special import boxcox1p \n \nimport matplotlib.gridspec as gridspec \nfrom matplotlib.ticker import MaxNLocator \n \nimport statsmodels\n    \nimport warnings \npd.options.display.max_columns = 250 \npd.options.display.max_rows = 250 \nwarnings.filterwarnings('ignore') ","35105977":"%matplotlib inline\nsns.set_style('whitegrid')\nplt.style.use('seaborn-paper')","2f01a4c5":"#Load Data\ntrain_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","29bcc59f":"#Preview data\ntrain_data.head()","20e889c3":"train_data.describe()","556d7f81":"# plt.figure(figsize=(16,6))\n# _ = sns.pairplot(train_data)\n# plt.show()","19e19b74":"# Percent Nulls\nfor col in train_data.columns:\n    length = len(train_data)\n    NA = train_data[col].isna().sum()\n    perc = NA\/length*100\n    print(col,\": \\t\",perc,\"\\t Percent Null\")","f7c18173":"# Remove any columns with low information (over 40% NULL) and fill NA's.\nfor col in train_data.columns:\n    length = len(train_data)\n    NA = train_data[col].isna().sum()\n    perc = NA\/length*100\n    #print(col,\": \\t\",perc,\"\\t Percent Null\")\n    if perc > 40:\n        del train_data[col]\n        print(f\"Deleted {col} due to low information\")\n    elif perc >0 and train_data[col].dtype != 'object':\n        train_data[col] = train_data[col].fillna(train_data[col].mean(skipna=True))\n    else:\n        pass","30d7b20d":"train_data.dtypes","87c12627":"# Get rid of string data (no dummies for now)\ntrain_data = train_data.select_dtypes(exclude = 'object')","22a611e3":"# Do the same to the test data set\n# Remove any columns with low information (over 40% NULL) and fill NA's.\nfor col in test_data.columns:\n    length = len(test_data)\n    NA = test_data[col].isna().sum()\n    perc = NA\/length*100\n    #print(col,\": \\t\",perc,\"\\t Percent Null\")\n    if perc > 40:\n        del test_data[col]\n        print(f\"Deleted {col} due to low information\")\n    elif perc >0 and test_data[col].dtype != 'object':\n        test_data[col] = test_data[col].fillna(test_data[col].mean(skipna=True))\n    else:\n        pass\n\n    \n    # Get rid of string data (no dummies for now)\ntest_data = test_data.select_dtypes(exclude = 'object')","9da8b0f0":"from sklearn.model_selection import train_test_split\n\ny = train_data['SalePrice']\nX = train_data.drop(['SalePrice'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.4, random_state = 123)","f52e4657":"from sklearn.linear_model import LinearRegression\n\nmodel_LR = LinearRegression()\nmodel_LR.fit(X_train, y_train)\nmodel_LR.predict(X_test)\nmodel_LR.score(X_test, y_test)","e04699dd":"#test_data = test_data.drop(['SalePrice'],axis=1)","3f988c82":"test_predictions_LR = model_LR.predict(test_data)","724fcb4d":"# Write output\noutput = pd.DataFrame({'Id': test_data['Id'], 'SalePrice': test_predictions_LR})\n#output = output.iloc[:1459,:]\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","a26afd2d":"sample_submission.tail()","b511efb1":"output.tail()","015940cd":"# Getting Feature importance\nimportance = model_LR.coef_\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print(f'Feature: {X_train.columns[i]}, Score: {v:.5f}')\n# plot feature importance\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()","57a3feaa":"#Re-Load Data\ntrain_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')","8cf951df":"# Using our importance from our starter model, we will remove all non-string fields except for those\nstrings = train_data.select_dtypes('object').columns\nimportant_features = ['OverallQual','GarageCars','KitchenAbvGr','BedroomAbvGr']\ntrain_data = train_data[list(strings) + list(important_features)]\ntrain_data.columns","c2ddea94":"train_data.head()","fac1111f":"# Alley == NaN could just be a missing feature. Let's fill these with 0's\ntrain_data['Alley'] = train_data['Alley'].fillna(0)\ntrain_data.head()","619417ee":"# Fence == NaN could just be a missing feature. Let's fill these with 0's\ntrain_data['Fence'] = train_data['Fence'].fillna(0)","3e1b3e31":"# Remove any columns with low information (over 40% NULL) and fill NA's.\nfor col in train_data.columns:\n    length = len(train_data)\n    NA = train_data[col].isna().sum()\n    perc = NA\/length*100\n    print(col,\": \\t\",perc,\"\\t Percent Null\")\n    if perc > 40:\n        del train_data[col]\n        print(f\"\\nDeleted {col} due to low information\\n\\n\")\n    elif perc >0 and train_data[col].dtype != 'object':\n        train_data[col] = train_data[col].fillna(train_data[col].mean(skipna=True))\n    else:\n        pass","19cd59a5":"# If there's any nulls left, let's fill with most frequent values\nfrom sklearn.impute import SimpleImputer\n\nimp_frequent = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n\n#train_data = imp_frequent.transform(train_data)\n\n\nfor col in train_data.columns:\n    length = len(train_data)\n    NA = train_data[col].isna().sum()\n    perc = NA\/length*100\n    print(col,\": \\t\",perc,\"\\t Percent Null\")\n    if perc > 0:\n        imp_frequent.fit(train_data[[col]])\n        train_data[col] = imp_frequent.transform(train_data[[col]])\n        print(f\"\\n\\n{col} has been imputed!\\n\\n\")\n    else:\n        pass","f1da27d7":"from sklearn.preprocessing import OneHotEncoder\n\nenc = OneHotEncoder(handle_unknown='ignore')\nenc.fit_transform(train_data[strings]) #get dummies and add to DF","c2dbdde2":"# Let's One Hot Encode!","cfd07306":"## Initial Model","2b0db9e0":"## What to do next:\n\n[ ] Fill in NA's intelligently (not with just average)\n\n[ ] One Hot Encode Categorical data\n\n[ ] Trim down total columns","0834d3cc":"# 80% Accuracy is the baseline to beat!\n\n- Looking at important features at a glance, it appears that Overall Quality and GarageCars are positive features for price (meaning, they will drive the price up), while KitchenAboveGround and BedroomAboveGround are negative features. Since we have a lot of features, the ones that are close to zero importance can be removed with little-to-no impact.","9da0e4d0":"## Load Data","59a8cf8d":"## Clean up data for initial model","7d9686c0":"## Train\/Test Datasets"}}