{"cell_type":{"540ff027":"code","33236fff":"code","ce759101":"code","bcc6587b":"code","1f2ca91d":"code","e081b457":"code","9e146505":"code","172efe6e":"code","9a28cfa8":"code","ad155a02":"code","8a9055b3":"code","626ee51a":"code","54f33531":"code","995231cc":"code","00d67dac":"code","e0040b98":"code","400b6cef":"code","0bb8de72":"code","3495ed02":"code","d10a03da":"code","3131f312":"code","67fd4482":"code","5b00446b":"code","8a67309f":"code","b3225d3d":"code","c430221f":"code","07ae8d50":"code","f8e35d8f":"code","d8d93360":"code","239e64e4":"code","298337f8":"code","ac3021d7":"code","9807d665":"markdown","a63ee62f":"markdown","33b3647b":"markdown","ef2e1c6f":"markdown","ead556e5":"markdown","bb2bd283":"markdown","77447517":"markdown","866d7534":"markdown","58fc735a":"markdown","aebcc0de":"markdown","6b18e226":"markdown","918f7cc7":"markdown","891215f9":"markdown","27e4f2cb":"markdown","2aeb8910":"markdown","eecf075e":"markdown","9a56da22":"markdown","ec555c40":"markdown","eb0e79e4":"markdown","5bfa72ae":"markdown","835ebed3":"markdown"},"source":{"540ff027":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline ","33236fff":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel","ce759101":"from wordcloud import WordCloud","bcc6587b":"import os\nmovie = pd.read_csv('..\/input\/movie-recommendation-engine\/movies.csv')\nratings = pd.read_csv('..\/input\/movie-recommendation-engine\/ratings.csv')","1f2ca91d":"movie.head()","e081b457":"ratings.head()","9e146505":"movie.info()\nprint(\"___________.......................______________\")\nratings.info()","172efe6e":"# Dimension of datasets:\nprint(\"Dimension of Movie Dataset is\")\nmovie.shape","9a28cfa8":"print(\"Dimension of Ratings Dataset is\")\nratings.shape","ad155a02":"movie.describe()","8a9055b3":"ratings.describe()","626ee51a":"genres=[]\nfor genre in movie.genres:\n    \n    x=genre.split('|')\n    for i in x:\n         if i not in genres:\n            genres.append(str(i))\ngenres=str(genres)    \nmovie_title=[]\nfor title in movie.title:\n    movie_title.append(title[0:-7])\nmovie_title=str(movie_title)    ","54f33531":"# Format of both the cloud \nwordcloud_genre=WordCloud(width=1500,height=800,background_color='black',min_font_size=2,min_word_length=3).generate(genres)\nwordcloud_title=WordCloud(width=1500,height=800,background_color='cyan',min_font_size=2,min_word_length=3).generate(movie_title)","995231cc":"plt.figure(figsize=(30,15))\nplt.axis('off')\nplt.title('WORDCLOUD for Movies Genre',fontsize=30)\nplt.imshow(wordcloud_genre)","00d67dac":"plt.figure(figsize=(30,15))\nplt.axis('off')\nplt.title('WORDCLOUD for Movies title',fontsize=30)\nplt.imshow(wordcloud_title)","e0040b98":"df = pd.merge(ratings,movie, how='left', on='movieId')\ndf.head()","400b6cef":"df1=df.groupby(['title'])[['rating']].sum()\ndf1.head(10)","0bb8de72":"high_rated=df1.nlargest(20,'rating') # by default first 20 rated movies\nhigh_rated.head()","3495ed02":"plt.figure(figsize=(30,10))\nplt.bar(high_rated.index,high_rated['rating'])\nplt.ylabel('ratings', fontsize=20)\nplt.xticks(fontsize=20,rotation=90)\nplt.xlabel('Movie Title', fontsize=20)\nplt.yticks(fontsize=15)","d10a03da":"# after viewing the sum let's check the count of the movies rating how many time movies are rated:\ndf2=df.groupby('title')[['rating']].count()\nrating_count_20=df2.nlargest(20,'rating')\nrating_count_20.head()","3131f312":"plt.figure(figsize=(30,10))\nplt.title('Top 20 movies with highest number of ratings',fontsize=30)\nplt.xticks(fontsize=25,rotation=90)\nplt.yticks(fontsize=25)\nplt.xlabel('movies title',fontsize=30)\nplt.ylabel('Ratings Count',fontsize=30)\n\nplt.bar(rating_count_20.index,rating_count_20.rating,color='red')","67fd4482":"#term frequency inverse document frequency:\n#Define a TF-IDF Vectorizer Object\nCV=TfidfVectorizer()\n\n#Construct the required TF-IDF matrix by fitting and transforming the data\ntfidf_matrix=CV.fit_transform(movie['genres'])","5b00446b":"#Output the shape of tfidf_matrix\ntfidf_matrix.shape","8a67309f":"#Creating a pivot table array for our customize view:\nmovie_user = df.pivot_table(index='userId',columns='title',values='rating')\nmovie_user.head()","b3225d3d":"# Compute the cosine similarity matrix\ncosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)","c430221f":"#Construct a reverse map of indices and movie titles\nindices=pd.Series(movie.index,index=movie['title']).drop_duplicates()","07ae8d50":"indices","f8e35d8f":"# Function that takes in movie title as input and outputs most similar movies\ndef get_recommendations(title, cosine_sim=cosine_sim):\n    # Get the index of the movie that matches the title\n    idx = indices[title]\n\n    # Get the pairwsie similarity scores of all movies with that movie\n    sim_scores = list(enumerate(cosine_sim[idx]))\n\n    # Sort the movies based on the similarity scores\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n\n    # Get the scores of the 10 most similar movies\n    sim_scores = sim_scores[1:11]\n\n    # Get the movie indices\n    movie_indices = [i[0] for i in sim_scores]\n\n    # Return the top 10 most similar movies\n    return movie['title'].iloc[movie_indices]\n\n# without explaination:\n\n#titles=movies['title']\n#def recommendations(title):\n #   idx = indices[title]\n  #  sim_scores = list(enumerate(cosine_sim[idx]))\n   # sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n   # sim_scores = sim_scores[1:21]\n    #movie_indices = [i[0] for i in sim_scores]\n    #return titles.iloc[movie_indices]","d8d93360":"# # Top 10 Similar movies to Toy Story (1995):\nget_recommendations('Toy Story (1995)')","239e64e4":"# Top 10 Similar movies to Pulp Fiction (1994):\nget_recommendations('Pulp Fiction (1994)')","298337f8":"# Top 10 Similar movies to Jumanji 1995:\nget_recommendations('Jumanji (1995)')","ac3021d7":"get_recommendations('Casino (1995)')","9807d665":"## Thank You","a63ee62f":"# Content Based Filtering\u00b6","33b3647b":"# To generate wordcloud seperate each string for its count into a list:","ef2e1c6f":"## Now we will gonna merge both the dataset:","ead556e5":"### We are going to define a function that takes in a movie title as an input and outputs a list of the 10 most similar movies. Firstly, for this, we need a reverse mapping of movie titles and DataFrame indices. In other words, we need a mechanism to identify the index of a movie in our metadata DataFrame, given its title.","bb2bd283":"\nContent Based Filtering\n\nIn this recommender system the content of the movie (genres, cast, crew, keyword, tagline etc) is used to find its similarity with other movies. Then the movies that are most likely to be similar are recommended.","77447517":"# Recommendation System for Movielens Dataset - Kaggle","866d7534":"\n\n### There are basically three types of recommender systems:-\n\n#### Demographic Filtering- \nThey offer generalized recommendations to every user, based on movie popularity and\/or genre. The System recommends the same movies to users with similar demographic features. Since each user is different , this approach is considered to be too simple. The basic idea behind this system is that movies that are more popular and critically acclaimed will have a higher probability of being liked by the average audience.\n\n#### Content Based Filtering- \nThey suggest similar items based on a particular item. This system uses item metadata, such as genre, director, description, actors, etc. for movies, to make these recommendations. The general idea behind these recommender systems is that if a person liked a particular item, he or she will also like an item that is similar to it.\n\n#### Collaborative Filtering- \nThis system matches persons with similar interests and provides recommendations based on this matching. Collaborative filters do not require item metadata like its content-based counterparts.\n\n\n\nIn this notebook we are going to implement content based recommendation system","58fc735a":"### We are now in a good position to define our recommendation function. These are the following steps we'll follow :-\n\n    -Get the index of the movie given its title.\n    -Get the list of cosine similarity scores for that particular movie with all movies. Convert it into a list of tuples  where the first element is its position and the second is the similarity score.\n    -Sort the aforementioned list of tuples based on the similarity scores; that is, the second element.\n    -Get the top 10 elements of this list. Ignore the first element as it refers to self (the movie most similar to a particular movie is the movie itself).\n    -Return the titles corresponding to the indices of the top elements.\n\n","aebcc0de":"### Importing essential libraries","6b18e226":"\nThe data consists of 105339 ratings applied over 10329 movies.\n\nThe movies.csv dataset contains three columns:\n\n    movieId: the ID of the movie\n    title: movies title\n    genres: movies genres\n\nThe ratings.csv dataset contains four columns:\n\n    userId: the ID of the user who rated the movie.\n    movieId: the ID of the movie\n    ratings: ratings given by each user (from 0 to 5)\n    Timstamp: The time the movie was rated.\n\n","918f7cc7":"### The Age of Recommender Systems\n\nThe rapid growth of data collection has led to a new era of information. Data is being used to create more efficient systems and this is where Recommendation Systems come into play. Recommendation Systems are a type of information filtering systems as they improve the quality of search results and provides items that are more relevant to the search item or are realted to the search history of the user.\n\nThey are used to predict the rating or preference that a user would give to an item. Almost every major tech company has applied them in some form or the other: Amazon uses it to suggest products to customers, YouTube uses it to decide which video to play next on autoplay, and Facebook uses it to recommend pages to like and people to follow. Moreover, companies like Netflix and Spotify depend highly on the effectiveness of their recommendation engines for their business and sucees.\n\n#### In this kernel we'll be building a baseline Movie Recommendation System using Movielens Dataset.\n","891215f9":"![image.png](attachment:image.png)","27e4f2cb":"### Plot description based Recommender","2aeb8910":"For any of you who has done even a bit of text processing before knows we need to convert the word vector of each overview. Now we'll compute Term Frequency-Inverse Document Frequency (TF-IDF) vectors for each overview.\n\nNow if you are wondering what is term frequency , it is the relative frequency of a word in a document and is given as (term instances\/total instances). Inverse Document Frequency is the relative count of documents containing the term is given as log(number of documents\/documents with term) The overall importance of each word to the documents in which they appear is equal to TF * IDF\n\nThis will give you a matrix where each column represents a word in the overview vocabulary (all the words that appear in at least one document) and each row represents a movie, as before.This is done to reduce the importance of words that occur frequently in plot overviews and therefore, their significance in computing the final similarity score.\n\nFortunately, scikit-learn gives you a built-in TfIdfVectorizer class that produces the TF-IDF matrix in a couple of lines. That's great, isn't it","eecf075e":"Suppose a user wants to watch a movie similar to burbs, The (1989) then we can recommend the user by calculating the cosine similarity between burbs, The (1989) and other movies. So we have to first find the cosine similarity between the movies of data.\n\nWith this matrix in hand, we can now compute a similarity score. There are several candidates for this; such as the euclidean, the Pearson and the cosine similarity scores. There is no right answer to which score is the best. Different scores work well in different scenarios and it is often a good idea to experiment with different metrics.\n\n#### We will be using the cosine similarity to calculate a numeric quantity that denotes the similarity between two movies. We use the cosine similarity score since it is independent of magnitude and is relatively easy and fast to calculate. Mathematically, it is defined as follows:","9a56da22":"## As you can see, our engine has done a decent job for finding movies with similar plot descriptions.","ec555c40":"Since we have used the TF-IDF vectorizer, calculating the dot product will directly give us the cosine similarity score. Therefore, we will use sklearn's linear_kernel() instead of cosine_similarities() since it is faster.","eb0e79e4":"From the above table we can conclue that in the give dataset:\n\n    The average rating is 3.50 and minimum and maximum rating is 0.5 and 5 respectively.\n    There are 668 user who has given their ratings for 149532 movies.\n\n","5bfa72ae":"### Loading the Dataset:","835ebed3":"### Sum of rating of all the rated movies and top 20 high rated movies table and diagram:"}}