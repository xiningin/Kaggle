{"cell_type":{"a6ea6aa8":"code","69cc03f8":"code","18e12b6b":"code","7b06fafd":"code","82dd5e24":"code","be324c39":"code","763ced58":"code","b339d34a":"code","8bd6c938":"code","9e9f49d5":"code","3c568b81":"code","cce44854":"code","7d048207":"code","0110635b":"code","fe2478a7":"code","941b52f8":"code","5f853b17":"code","f5608d68":"code","b50f8421":"code","e5ab96f5":"code","95b237f1":"markdown","18fa9cad":"markdown","15233fce":"markdown","6b16848f":"markdown","c6f21af2":"markdown","af8a690f":"markdown","30a85055":"markdown","7b384e36":"markdown","3d3b6f86":"markdown"},"source":{"a6ea6aa8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","69cc03f8":"#for data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#for text pre-processing\nimport re, string\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\n\n\n#for model-building\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\n\n# bag of words\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#for word embedding\nimport gensim\nfrom gensim.models import Word2Vec","18e12b6b":"## Load data set into dataframe\ndf_train=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nprint(df_train.shape)\ndf_train.head(3)","7b06fafd":"df_test.head(3)","82dd5e24":"## find missing values in df\ndf_train.isna().sum()","be324c39":"# CLASS distribution\nx=df_train['target'].value_counts()\nprint(x)\nsns.barplot(x.index,x)","763ced58":"##Count number of words in a tweet\ndf_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\nprint(\"Number of words in Disaster tweets:\",df_train[df_train['target']==1]['word_count'].mean()) \nprint(\"Number of words in Non-Disaster tweets:\",df_train[df_train['target']==0]['word_count'].mean()) \n\n\n#Plot word-count per tweet\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,4))\ndf_train_words=df_train[df_train['target']==1]['word_count']\nax1.hist(df_train_words,color='red')\nax1.set_title('Disaster tweets')\ndf_train_words=df_train[df_train['target']==0]['word_count']\nax2.hist(df_train_words,color='green')\nax2.set_title('Non-disaster tweets')\nfig.suptitle('Words per tweet')\nplt.show()","b339d34a":"##Count number of character\ndf_train['char_count'] = df_train['text'].apply(lambda x: len(str(x)))\nprint(\"Number of characters in Disaster tweets:\",df_train[df_train['target']==1]['char_count'].mean()) \nprint(\"Number of characters in Non-Disaster tweets:\",df_train[df_train['target']==0]['char_count'].mean()) \n\n#Plot word-count per tweet\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,4))\ndf_train_words=df_train[df_train['target']==1]['char_count']\nax1.hist(df_train_words,color='red')\nax1.set_title('Disaster tweets')\ndf_train_words=df_train[df_train['target']==0]['char_count']\nax2.hist(df_train_words,color='green')\nax2.set_title('Non-disaster tweets')\nfig.suptitle('Character count per tweet')\nplt.show()","8bd6c938":"##Count number of unique word count\ndf_train['unique_word_count'] = df_train['text'].apply(lambda x: len(set(str(x).split())))\nprint(\"Number of unique words in Disaster tweets:\",df_train[df_train['target']==1]['unique_word_count'].mean()) \nprint(\"Number of unique words in Non-Disaster tweets:\",df_train[df_train['target']==0]['unique_word_count'].mean()) ","9e9f49d5":"## Simple text cleaning processe :- Remove punctuations, special characters, URLs, hashtag, leading, trailing & extra white spaces\/tabs,typos, slangs are corrected.\ntext = \"   This is a message to be cleaned. It may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     .  \"\n\n#convert to lowercase and remove punctuations and characters and then strip\ndef preprocess(text):\n    text = text.lower() #lowercase text\n    text=text.strip()  #remove leading\/trailing whitespace \n    text=re.compile('<.*?>').sub('', text) #Remove HTML tags\/markups\n    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  #Replace punctuation with space. \n    text = re.sub('\\s+', ' ', text)  #Remove extra space and tabs\n    text = re.sub(r'\\[[0-9]*\\]',' ',text) #[0-9] matches any digit (0 to 10000...)\n    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n    text = re.sub(r'\\d',' ',text) #matches any digit from 0 to 100000..., \\D matches non-digits\n    text = re.sub(r'\\s+',' ',text) #\\s matches any whitespace, \\s+ matches multiple whitespace, \\S matches non-whitespace \n    \n    return text\n\ntext=preprocess(text)\nprint(text)  #text is a string","3c568b81":"## LEXICON-BASED Text Processing\n\n#1. STOP-WORD Removal\ndef stopword(string):\n    a= [i for i in string.split() if i not in stopwords.words('english')]\n    return ' '.join(a)\n\ntext=stopword(text)\nprint(text)\n\n#2. STEMMING\n \n# Initialize the stemmer\nsnow = SnowballStemmer('english')\ndef stemming(string):\n    a=[snow.stem(i) for i in word_tokenize(string) ]\n    return \" \".join(a)\ntext=stemming(text)\nprint(text)\n\n#3. LEMMATIZATION\n# Initialize the lemmatizer\nwl = WordNetLemmatizer()\n \n# Helper function to map NTLK position tags\ndef get_wordnet_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\n# Tokenize the sentence\ndef lemmatizer(string):\n    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word\/token\n    return \" \".join(a)\n\ntext = lemmatizer(text)\nprint(text)","cce44854":"def finalpreprocess(string):\n    return lemmatizer(stopword(preprocess(string)))\ndf_train['clean_text'] = df_train['text'].apply(lambda x: finalpreprocess(x))\ndf_train.head()","7d048207":"# create Word2vec model\n\n#convert preprocessed sentence to tokenized sentence\ndf_train['clean_text_tok']=[nltk.word_tokenize(i) for i in df_train['clean_text']] \n\n#min_count=1 means word should be present at least across all documents,\nmodel = Word2Vec(df_train['clean_text_tok'],min_count=1)  \n\n#combination of word and its vector\nw2v = dict(zip(model.wv.index_to_key, model.wv.vectors))  \n\n#for converting sentence to vectors\/numbers from word vectors result by Word2Vec\nclass MeanEmbeddingVectorizer(object):\n    def __init__(self, word2vec):\n        self.word2vec = word2vec\n        # if a text is empty should return a vector of zeros\n        self.dim = len(next(iter(word2vec.values())))\n\n    def fit(self, X, y):\n        return self\n\n    def transform(self, X):\n        return np.array([\n            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n                    or [np.zeros(self.dim)], axis=0)\n            for words in X\n        ])","0110635b":"##Split the training dataset into test and train data\n\nX_train, X_test, y_train, y_test = train_test_split(df_train[\"clean_text\"],df_train[\"target\"],test_size=0.2,shuffle=True)","fe2478a7":"# Word2Vec runs on tokenized sentences\nX_train_tok= [nltk.word_tokenize(i) for i in X_train]  \nX_test_tok= [nltk.word_tokenize(i) for i in X_test]","941b52f8":"#TF-IDF\n# Convert x_train to vector since model can only run on numbers and not words- Fit and transform\ntfidf_vectorizer = TfidfVectorizer(use_idf=True)\n#tfidf runs on non-tokenized sentences unlike word2vec\nX_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n# Only transform x_test (not fit and transform)\nX_val_vectors_tfidf = tfidf_vectorizer.transform(X_test)\n\n#Word2vec\n# Fit and transform\nmodelw = MeanEmbeddingVectorizer(w2v)\nX_train_vectors_w2v = modelw.transform(X_train_tok)\nX_val_vectors_w2v = modelw.transform(X_test_tok)","5f853b17":"## Classification Model using Logistic Regression(tf-idf)\n\nlr_tfidf=LogisticRegression(solver = 'liblinear', C=1, penalty = 'l2')\nlr_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n\n#Predict y value for test dataset\ny_predict = lr_tfidf.predict(X_val_vectors_tfidf)\ny_prob = lr_tfidf.predict_proba(X_val_vectors_tfidf)[:,1]\n\nprint(classification_report(y_test,y_predict))\nprint('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n \nfpr, tpr, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(fpr, tpr)\nprint('Area under the curve (AUC) :', roc_auc)","f5608d68":"## Classification Model using Naive Bayes(tf-idf)\n\nnb_tfidf = MultinomialNB()\nnb_tfidf.fit(X_train_vectors_tfidf, y_train)  \n\n#Predict y value for test dataset\ny_predict = nb_tfidf.predict(X_val_vectors_tfidf)\ny_prob = nb_tfidf.predict_proba(X_val_vectors_tfidf)[:,1]\n\nprint(classification_report(y_test,y_predict))\nprint('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n \nfpr, tpr, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(fpr, tpr)\nprint('AUC:', roc_auc)","b50f8421":"## Classification Model using Logistic Regression (W2v)\nlr_w2v=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\nlr_w2v.fit(X_train_vectors_w2v, y_train)  #model\n\n#Predict y value for test dataset\ny_predict = lr_w2v.predict(X_val_vectors_w2v)\ny_prob = lr_w2v.predict_proba(X_val_vectors_w2v)[:,1]\n \nprint(classification_report(y_test,y_predict))\nprint('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n \nfpr, tpr, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(fpr, tpr)\nprint('AUC:', roc_auc)","e5ab96f5":"## Testing with the best model\n\n#Preprocess the data\ndf_test['clean_text'] = df_test['text'].apply(lambda x: finalpreprocess(x)) \nX_test=df_test['clean_text'] \nX_vector=tfidf_vectorizer.transform(X_test) \n\n## Use the trained model on X_vector\ny_predict = nb_tfidf.predict(X_vector)      \ny_prob = nb_tfidf.predict_proba(X_vector)[:,1]\ndf_test['predict_prob']= y_prob\ndf_test['target']= y_predict\nprint(df_test.head())\nfinal=df_test[['id','target']].reset_index(drop=True)\nfinal.to_csv('NLP_submission.csv')","95b237f1":"### Testing Model on Test Dataset","18fa9cad":"## NLP Text Classification Model\n\nIt is a supervised machine learning model that predicts which Tweets are about real disasters and which one\u2019s aren\u2019t.\n\nSteps involved in development of text classification model are below :-\n\n1. Importing Libraries\n2. Loading the data set & perform Exploratory Data Analysis\n3. Text pre-processing\n4. Split the train dataset\n4. Extracting vectors from text (Vectorization) using Bag-of-Words(with Tf-Idf) and Word2Vec\n5. ML Model algorithms\n6. Testing ML models on test dataset\n\n","15233fce":"### ML Models Development","6b16848f":"### Text Pre-Processing","c6f21af2":"### Split the data","af8a690f":"#### Word2Vec model\nBag-of-Words (BoW) and Word Embedding (with Word2Vec) are two well-known methods for converting text data to numerical data.","30a85055":"### Vectorization using Bag-of-Words (with Tf-Idf ) and Word2Vec","7b384e36":"### EDA","3d3b6f86":"### Import packages"}}