{"cell_type":{"912ad2b4":"code","cf022ea3":"code","d4566892":"code","eb0bb50b":"code","e6af103e":"code","3845933b":"code","d9390d4e":"code","9807d6df":"code","3074194b":"code","cccc88f3":"markdown","f94c30c9":"markdown","8cecc7e6":"markdown","a0bc1351":"markdown","17feae92":"markdown"},"source":{"912ad2b4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nfrom scipy import stats\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n# Any results you write to the current directory are saved as output.","cf022ea3":"from kmodes.kmodes import KModes\ndf_allData=pd.read_csv('..\/input\/BlackFriday.csv')\nprint(df_allData.sample(n=2))","d4566892":"groupByUserData=df_allData.groupby(['User_ID'])\n\ntimes=df_allData['User_ID'].value_counts()\ntimes=times.sort_index()\n\n#get the mean\nmeanData=groupByUserData.mean()\n\n#get the mode\nmodeData=groupByUserData.agg(lambda x: stats.mode(x)[0][0])\n\nmean_mode_data={'Gender':modeData['Gender'],'Occupation':modeData['Occupation'],'Age':modeData['Age'],'City_Category':modeData['City_Category'],'Marital_Status':modeData['Marital_Status'],'Product_CateGory_1':modeData['Product_Category_1'],'Stay_In_Current_City_Years':modeData['Stay_In_Current_City_Years']}\nmean_mode_data=pd.DataFrame(mean_mode_data)\nmean_mode_data['times']=times\nmean_mode_data['Gender_M']=pd.get_dummies(mean_mode_data['Gender'])['M']\nmean_mode_data=mean_mode_data.drop(['Gender'],axis=1)\nmean_mode_data['Purchase']=meanData['Purchase']\n\nprint (mean_mode_data.sample(2))","eb0bb50b":"X=pd.DataFrame({'Gender':modeData['Gender'],'Occupation':modeData['Occupation'],'Age':modeData['Age'],'City_Category':modeData['City_Category'],'Marital_Status':modeData['Marital_Status'],'Product_CateGory_1':modeData['Product_Category_1'],\"Stay_In_Current_City_Years\":modeData[\"Stay_In_Current_City_Years\"]})\n\none_hot_city=pd.get_dummies(mean_mode_data['City_Category'])\none_hot_age=pd.get_dummies(mean_mode_data['Age'])\none_hot_occupation=pd.get_dummies(mean_mode_data['Occupation'])\none_hot_years=pd.get_dummies(mean_mode_data['Stay_In_Current_City_Years'])\none_hot_product=pd.get_dummies(mean_mode_data['Product_CateGory_1'])\nXX=pd.concat([one_hot_age,one_hot_city,one_hot_occupation,one_hot_years,one_hot_product],axis=1)\nXX['Gender_M']=mean_mode_data['Gender_M']\nXX['Marital_Status']=mean_mode_data['Marital_Status']\n\nprint (\"categorical data:\")\nprint(X.sample(2))\nprint(\"one-hot encoding data:\")\nprint(XX.sample(2))","e6af103e":"from sklearn.metrics import jaccard_similarity_score\necArr=[]\njcArr=[]\njcXArr=[]\nfor i in range(2,10):\n    km=KModes(n_clusters=i)\n    y=km.fit_predict(X)\n    tempArrjc=[]\n    tempArrec=[]\n    tempArrjcX=[]\n    for j in range(i):\n        #print(sum(y==j))\n        #print(XX[y==j].mode())\n        jcscore=[]\n        ecscore=[]\n        jcXscore=[]\n        for k in XX[y==j].T:\n            try:\n                #jcscore.append(jaccard_similarity_score(XX.loc[k],XX[y==j].mode().T[0]))\n                \n                ecscore.append(np.linalg.norm(np.array(XX.loc[k])-np.array(XX[y==j].mode().T[0])))\n                \n                jcXscore.append(jaccard_similarity_score(list(X.loc[k]),list(X[y==j].mode().T[0])))\n\n            except:\n                #print(XX.loc[k].T)\n                #print(XX[y==j].mode())\n                print(k)\n                break;\n        #print(np.mean(jcscore))\n        #tempArrjc.append(np.mean(jcscore))\n        #tempArrec.append(np.mean(ecscore))\n        tempArrjcX.append(np.mean(jcXscore))\n\n    print(\"n_cluster =\",i,\":\",np.mean(tempArrjcX))\n    #jcArr.append(np.mean(tempArrjc))\n    #ecArr.append(np.mean(tempArrec))\n    jcXArr.append(np.mean(tempArrjcX))","3845933b":"XXXX=X.drop(['Marital_Status','Product_CateGory_1','Stay_In_Current_City_Years','Age'],axis=1)\nprint(XXXX.sample(2))\nfrom sklearn.metrics import jaccard_similarity_score\necArr=[]\njcArr=[]\njcXArr=[]\nfor i in range(10,11):\n    km=KModes(n_clusters=i)\n    y=km.fit_predict(XXXX)\ndis_jc=[]\ndis_ec=[]\nfor i in range(10):\n    dis_jc.append(jaccard_similarity_score(list(XXXX[y==i].mode().T[0]),list(XXXX[y!=i].mode().T[0])))\n    \nprint(\"average jc distance in selected features:\",np.mean(dis_jc))\n    \nfor i in range(10):\n    dis_ec.append(np.linalg.norm((np.array(XX[y==i].mode().T[0])-np.array(XX[y!=i].mode().T[0]))))\n    \nprint(\"average ec distance in all one-hot features:\",np.mean(dis_ec))   ","d9390d4e":"purchase_y=pd.DataFrame({\"y\":y,\"Purchase\":mean_mode_data[\"Purchase\"]})\nplt.scatter(purchase_y['y'],purchase_y['Purchase'])\nfor i in range(10):\n    plt.scatter(i,purchase_y[purchase_y['y']==i].Purchase.mean(),c='r')","9807d6df":"XXXXX=X.drop(['Stay_In_Current_City_Years'],axis=1)\nprint(XXXXX.sample(2))\necArr=[]\njcArr=[]\njcXArr=[]\nfor i in range(10,11):\n    km=KModes(n_clusters=i)\n    y=km.fit_predict(XXXXX)\n\ndis_jc=[]\ndis_ec=[]\nfor i in range(10):\n    dis_jc.append(jaccard_similarity_score(list(XXXXX[y==i].mode().T[0]),list(XXXXX[y!=i].mode().T[0])))\n    \nfor i in range(10):\n    dis_ec.append(np.linalg.norm((np.array(XX[y==i].mode().T[0])-np.array(XX[y!=i].mode().T[0]))))\n    \nprint(\"average jc distance in selected features:\",np.mean(dis_jc))\nprint(\"average ec distance in all one-hot features:\",np.mean(dis_ec))","3074194b":"purchase_y=pd.DataFrame({\"y\":y,\"Purchase\":mean_mode_data[\"Purchase\"]})\nplt.scatter(purchase_y['y'],purchase_y['Purchase'])\nfor i in range(10):\n    plt.scatter(i,purchase_y[purchase_y['y']==i].Purchase.mean(),c='r')","cccc88f3":"## conclusion\n\nThought the process is hard, I think I get something surpringly at the end.\n\nI paid too much attentation on the cluster number n and the distance between a cluster but I haven't had a great evaluation way.\n\nBut when I campared different features as input to k-modes with the same n, I got some pretty things.\n\nThe change in average euclidean distance showed the feature 'Marital_Status' , 'Product_CateGory_1', 'Age' do influence on the whole cluster performance.\n\nWhat't more, the 'Purchase' would really reflect the performance of cluster in some way.\n\nWith this conclusion, it means we can do further work about how different categorial features influence the cluster. The best feature may have the biggest influence on average euclidean distance.","f94c30c9":"In our opinion, we think our model would do cluster on different people, so our key is people.\n\nBut the dataset includes different records of one-single people. So at the beginning we use gruopby to get each people's whole records.\n\nThen we try to get the mode of each one's \"Product_Category_1\" to represent the main product category and get the mean of one people's whole \"Purchase\" as a feature of \"average purchase\". (We are not sure weather this is a good way buy we have to do this because we cannot keep all the data to train)\n\nWhat's more, we change the \"Gender\" attribute to 0-1 attribute.","8cecc7e6":"At first, I think the key is finding the best clusters number n, but I don't have ideas about how to evalue the output. I just thought maybe different cluster's average price can reflect some difference.\n\nI try to use jaccard distance and eulicdean distance, but the jaccard distance inside a cluster would alway decrease when n increase.","a0bc1351":"To cluster the categorical attributes, we came up with ideas.\n\n1. do one-hot encoding on the discrete attributes\n\n1. use k-modes or k-prototype model\n\n1. drop those categorical attributes\n\n","17feae92":"Lack of the ways to evaluate better n, I decide to focus on features more rather than n.\n\nThen I camp up with an idea that I can calculate the eulidean distance between a cluster's mode point and the other point's mode point.\n\nSo I do some tries."}}