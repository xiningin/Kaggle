{"cell_type":{"2645606c":"code","aea5570c":"code","1976e17f":"code","fd212cc2":"code","ae158c8a":"code","ecd7bccc":"code","7638724d":"code","90c1ea33":"code","ed257fc3":"code","3002a813":"code","8e2725ab":"code","4c53b5ec":"code","3c66a733":"code","5d29123c":"code","b3536070":"code","9b14d07f":"code","6079dd20":"code","b828fdb7":"code","91d8a409":"code","da6872ea":"code","a53096a1":"code","f8c43aa6":"code","82b23ea3":"code","04fd860b":"code","ddd77bcb":"code","6b7d4aa3":"code","d99f6f8d":"code","ef80db80":"code","8c7aad7b":"code","4b2ea408":"code","494cd8fb":"code","aad68b68":"code","b795af46":"code","a7910234":"code","7f722255":"code","7e190ed7":"code","b279b8f4":"code","0101e437":"code","683cfc85":"code","ba46ff02":"code","77142785":"code","e59b30ae":"code","7daa38a2":"code","e64e62b1":"code","df354f94":"code","ef12de63":"code","98c314d1":"code","7bdb31eb":"code","6524887c":"code","3bdcf43a":"code","56ba66dc":"code","ffa98fcf":"markdown","8267fad7":"markdown","c262d1f2":"markdown","d4e4d8db":"markdown","7b23afda":"markdown","9adfe046":"markdown","46fd75bf":"markdown","c3fa7008":"markdown","708f2a91":"markdown","6bbfba6d":"markdown","e8fe8e2c":"markdown","34c1428f":"markdown","b9b66949":"markdown","757b40b6":"markdown","b2f7db43":"markdown","bcc0464a":"markdown","48de4f02":"markdown","29177da9":"markdown","fdba8007":"markdown","70307d09":"markdown","c16ee76d":"markdown","b0233032":"markdown","ebe1b57f":"markdown","3f4a9875":"markdown","8b8a69e6":"markdown","0cfb2e43":"markdown","8a03757b":"markdown","49e124b5":"markdown","09b65fb0":"markdown","447f7ab9":"markdown","9d304c6c":"markdown","66343a93":"markdown","2ea1969a":"markdown","2ebc2eaa":"markdown","bcc3ff73":"markdown"},"source":{"2645606c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aea5570c":"##importing required libraries\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n##Model\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n##Performance metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, recall_score,accuracy_score, make_scorer\n","1976e17f":"##reading the bank data\ndf=pd.read_csv('\/kaggle\/input\/personal-loan-modeling\/Bank_Personal_Loan_Modelling.csv')\ndf.head()","fd212cc2":"df.tail()","ae158c8a":"df.info()","ecd7bccc":"#Removing ID column which is of no relevance\ndf1= df.drop(columns =['ID', 'ZIP Code'])","7638724d":"df1.isnull().sum()##checking missing values","90c1ea33":"df1.describe()","ed257fc3":"df1.loc[df1.Experience < 0, ['Experience']] = 0\ndf1.describe()","3002a813":"##Lets see the distribution of target column- Personal Loan\nprint(df1.groupby('Personal Loan').size())\nsns.countplot(df1['Personal Loan'],label=\"Count\")\nplt.title(\"Distribution of Target Variable\")\nplt.show()","8e2725ab":"sns.distplot(df1[\"Age\"])","4c53b5ec":"sns.distplot(df1[\"Experience\"])","3c66a733":"plt.figure(figsize=(15,10))\nplt.subplot(2,3,1)\ndf1.groupby('Personal Loan')['Income'].mean().plot(kind='bar',title='Income')\nplt.subplot(2,3,2)\ndf1.groupby('Personal Loan')['CCAvg'].mean().plot(kind='bar', title='Average CC Spend')\nplt.subplot(2,3,3)\ndf1.groupby('Personal Loan')['Age'].mean().plot(kind='bar', title='Age')\nplt.subplot(2,3,4)\ndf1.groupby('Personal Loan')['Experience'].mean().plot(kind='bar', title='Experience')\nplt.subplot(2,3,5)\ndf1.groupby('Personal Loan')['Mortgage'].mean().plot(kind='bar', title='Mortagage')","5d29123c":"sns.heatmap(df1.corr())\nplt.show()","b3536070":"sns.lmplot(x='Income',y='CCAvg',data=df1,fit_reg=False,hue='Personal Loan') \nsns.lmplot(x='Income',y='Mortgage',data=df1,fit_reg=False,hue='Personal Loan') \nplt.show()","9b14d07f":"pd.crosstab(df1['Securities Account'],df['Personal Loan']).plot(kind='bar',stacked=True,title='Securities')\npd.crosstab(df1['CD Account'],df['Personal Loan']).plot(kind='bar',stacked=True,title='CD Account')","6079dd20":"pd.crosstab(df1['Online'],df1['Personal Loan']).plot(kind='bar',stacked=True,title='Online')\npd.crosstab(df1['CreditCard'],df1['Personal Loan']).plot(kind='bar',stacked=True,title='Credit Card')","b828fdb7":"##Plotting family\nedu=pd.crosstab(df1['Family'],df1['Personal Loan'])\nedu.div(edu.sum(1).astype(float),axis=0).plot(kind='bar',\n                                              stacked=True,title='% Family')","91d8a409":"##Plotting education\nedu=pd.crosstab(df1['Education'],df1['Personal Loan'])\nedu.div(edu.sum(1).astype(float),axis=0).plot(kind='bar',\n                                              stacked=True,title='% Education')","da6872ea":"#splitting the data into train-test in 80-20 ratio\nX_train, X_test, Y_train, Y_test = train_test_split(df1.loc[:, df1.columns != 'Personal Loan'], df1['Personal Loan'], \n                                                    stratify=df1['Personal Loan'], \n                                                    random_state=66, test_size =0.2)\nprint(\"Training Data: \",X_train.shape, Y_train.shape)\nprint(\"Test Data: \",X_test.shape, Y_test.shape)","a53096a1":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nprint(\"Accuracy on test data: \",acc_log)","f8c43aa6":"coeff_df = pd.DataFrame(df1.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","82b23ea3":"##Predicting on test data\npred_log = logreg.predict(X_test)\nauc_log = round(roc_auc_score(Y_test, pred_log)*100,2)\nrecall_log = round(recall_score(Y_test, pred_log)*100,2)\nprint(\"AUC: \",  auc_log)\nprint(\"Recall: \",  recall_log)","04fd860b":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\n## checking the metrics of model on test data\nacc_svc = round(svc.score(X_test, Y_test)*100, 2)\nprint(\"Accuracy on test set: \",acc_svc)","ddd77bcb":"##Predicting on test data\npred_svc = svc.predict(X_test)\nauc_svc = round(roc_auc_score(y_test, pred_svc)*100,2)\nrecall_svc = round(recall_score(y_test, pred_svc)*100,2)\nprint(\"AUC: \",  auc_svc)\nprint(\"Recall: \",  recall_svc)","6b7d4aa3":"knn = KNeighborsClassifier(n_neighbors = 4)\nknn.fit(X_train, Y_train)\n## checking the metrics of model on test data\nacc_knn = round(knn.score(X_test, Y_test)*100, 2)\nprint(\"Accuracy on test data: \",acc_knn)","d99f6f8d":"### Predicting on test data and checking the metrics\npred_knn = knn.predict(X_test)\nauc_knn = round(roc_auc_score(Y_test, pred_knn)*100,2)\nrecall_knn = round(recall_score(Y_test, pred_knn)*100,2)\nprint(\"AUC: \",  auc_knn)\nprint(\"Recall: \",  recall_knn)","ef80db80":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nacc_gaussian = round(gaussian.score(X_test, Y_test) * 100, 2)\nacc_gaussian","8c7aad7b":"pred_nb = gaussian.predict(X_test)\nauc_nb = round(roc_auc_score(Y_test, pred_nb)*100,2)\nrecall_nb = round(recall_score(Y_test, pred_nb)*100,2)\nprint(\"AUC: \",  auc_nb)\nprint(\"Recall: \",  recall_nb)","4b2ea408":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nacc_p = round(perceptron.score(X_test, Y_test) * 100, 2)\nacc_p","494cd8fb":"### Predicting on test data\npred_p = gaussian.predict(X_test)\nauc_p = round(roc_auc_score(Y_test, pred_p)*100,2)\nrecall_p = round(recall_score(Y_test, pred_p)*100,2)\nprint(\"AUC: \",  auc_p)\nprint(\"Recall: \",  recall_p)","aad68b68":"#Building the model\nmodel_ct = DecisionTreeClassifier(criterion='gini',random_state=1)\nmodel_ct.fit(X_train,Y_train) ## training the model\n## checking the metrics of model on test data\nacc_ct=round(model_ct.score(X_test, Y_test)*100,2)\nprint(\"Accuracy on test data: \",acc_ct)","b795af46":"model_ct = DecisionTreeClassifier(criterion='gini',random_state=1, max_depth=5)\nmodel_ct.fit(X_train,Y_train) ## training the model\n## checking the metrics of model on test data\nacc_ct=round(model_ct.score(X_test, Y_test)*100,2)\nprint(\"Accuracy on test data: \",acc_ct)","a7910234":"### Predicting on test data\npred_ct = model_ct.predict(X_test)\nauc_ct = round(roc_auc_score(Y_test, pred_ct)*100,2)\nrecall_ct = round(recall_score(Y_test, pred_ct)*100,2)\nprint(\"AUC: \",  auc_ct)\nprint(\"Recall: \",  recall_ct)","7f722255":"rf = RandomForestClassifier(n_estimators=101, random_state=1)\nrf.fit(X_train, Y_train)\n## checking the metrics of model on test data\nacc_rf=round(rf.score(X_test, Y_test)*100,2)\nprint(\"Accuracy on test set: \",acc_rf)","7e190ed7":"### Predicting on test data\npred_rf = rf.predict(X_test)\nauc_rf = round(roc_auc_score(Y_test, pred_rf)*100,2)\nrecall_rf = round(recall_score(Y_test, pred_rf)*100,2)\nprint(\"AUC: \",  auc_rf)\nprint(\"Recall: \",  recall_rf)","b279b8f4":"gb = GradientBoostingClassifier(random_state=1)\ngb.fit(X_train, Y_train)\n## checking the metrics of model on test data\nacc_gb=round(gb.score(X_test, Y_test)*100,2)\nprint(\"Accuracy on test set: \",acc_gb)","0101e437":"### Predicting on test data\npred_gb = gb.predict(X_test)\nauc_gb = round(roc_auc_score(Y_test, pred_gb)*100,2)\nrecall_gb = round(recall_score(Y_test, pred_gb)*100,2)\nprint(\"AUC: \",  auc_gb)\nprint(\"Recall: \",  recall_gb)","683cfc85":"import xgboost as xgb\nxgb = xgb.XGBClassifier(random_state=1)\nxgb.fit(X_train, Y_train)\n## checking the metrics of model on test data\nacc_xgb=round(xgb.score(X_test, Y_test)*100,2)\nprint(\"Accuracy on test data: \",acc_xgb)","ba46ff02":"### Predicting on test data\npred_xgb = xgb.predict(X_test)\nauc_xgb = round(roc_auc_score(Y_test, pred_xgb)*100,2)\nrecall_xgb = round(recall_score(Y_test, pred_xgb)*100,2)\nprint(\"AUC: \",  auc_xgb)\nprint(\"Recall: \",  recall_xgb)","77142785":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\n\n## checking the metrics of model on test data\nacc_sgd=round(sgd.score(X_test, Y_test)*100,2)\nprint(\"Accuracy on test set: \",acc_sgd)","e59b30ae":"### Predicting on test data\npred_sgd = sgd.predict(X_test)\nauc_sgd = round(roc_auc_score(Y_test, pred_sgd)*100,2)\nrecall_sgd = round(recall_score(Y_test, pred_sgd)*100,2)\nprint(\"AUC: \",  auc_sgd)\nprint(\"Recall: \",  recall_sgd)","7daa38a2":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Gradient Boosting', \n              'Decision Tree','XGBoosting'],\n    'Accuracy': [acc_svc, acc_knn, acc_log, \n              acc_rf, acc_sgd, acc_p, \n              acc_sgd, acc_gb, acc_ct, acc_xgb],\n    'AUC' : [auc_svc, auc_knn, auc_log, \n              auc_rf, auc_sgd, auc_p, \n              auc_sgd, auc_gb, auc_ct,auc_xgb],\n    'Recall': [recall_svc, recall_knn, recall_log, \n              recall_rf, recall_sgd, recall_p, \n              recall_sgd, recall_gb, recall_ct,recall_xgb]})\nmodels.sort_values(by='Accuracy', ascending=False)","e64e62b1":"rf_importances = pd.Series(rf.feature_importances_, index=X_train.columns)\nxgb_importances = pd.Series(xgb.feature_importances_, index=X_train.columns)\nplt.figure(figsize=(20,5))\nplt.subplot(1,2,1)\nrf_importances.nlargest(25).sort_values(ascending=True).plot(kind='barh', title='Importance by Rf')\nplt.subplot(1,2,2)\nxgb_importances.nlargest(25).sort_values(ascending=True).plot(kind='barh',title='Importance by XGB')","df354f94":"##lets try to do some evaluation for random forest model using cross validation.\nrfc_eval = cross_val_score(estimator = rf, X = X_train, y = Y_train, cv = 10)\nrfc_eval.mean()","ef12de63":"##lets try to find best hyperparameter for random forest model using GridSearchCV.\nparam_grid = {'n_estimators': [101,201,251], 'max_features': [6,7,8], 'max_depth':[7,8,9]}\nrf1 = GridSearchCV(RandomForestClassifier(), param_grid, cv=10, \n                   scoring=make_scorer(accuracy_score))\nrf1.fit(X_train, Y_train)\nprint(\"Accuracy :\",round(rf1.score(X_test, Y_test)*100,2))","98c314d1":"best=rf1.best_params_\nprint(best)","7bdb31eb":"##Tuning the Decision Tree model\nmodel_t = DecisionTreeClassifier(random_state=1,max_depth=5)\nmodel_t.fit(X_train,Y_train) ## training the model\n## checking the accuracy of model on test data\nacc_t=round(model_t.score(X_test, Y_test)*100,2)\nprint(\"Accuracy on test data: \",acc_t)","6524887c":"##Predicting on test data\n\npred_t = model_t.predict(X_test)\nprint(\"=== Confusion Matrix ===\")\nprint(confusion_matrix(Y_test, pred_t))\nprint('\\n')\nprint(\"=== Classification Report ===\")\nprint(classification_report(Y_test, pred_t))\nprint('\\n')\nauc_t = round(roc_auc_score(Y_test, pred_t)*100,2)\nprint(\"AUC: \",  auc_t)\nrecall_t = round(recall_score(Y_test, pred_t)*100,2)\nprint(\"Recall: \",  recall_t)","3bdcf43a":"##Plotting the tree\nplt.figure(figsize=(25,10))\na= plot_tree(model_t, \n             feature_names=X_train.columns,\n             filled=True, \n              rounded=True, \n              fontsize=14)","56ba66dc":"print('---Comparison Of Best 2 Models---')\nprint('Decision Tree Model Accuracy:',acc_t,',Auc:',auc_t,',Recall:',recall_t)\nprint(' XGBoosting Model Accuracy:',acc_xgb,',Auc:',auc_xgb,',Recall:',recall_xgb)","ffa98fcf":"* Experience and Age mean are similar for personal Loan -ve and +ve\n* Higher income, cc spent and mortgage have responded +ve to loan","8267fad7":"After tuning Decision Tree has given best accuracy. Lets see confusion report after this model","c262d1f2":"1. Correlation Matrix","d4e4d8db":"## Further steps- Tuning models for better performance","7b23afda":"10. Stochastic Gradient Descent","9adfe046":"## Performance Conclusion\nFrom the above table we can see that Gradient Boosting, Random Forest, and XGBoosting have given best accuracy. Since we have highly imbalanced data so we cant only rely on Accuracy. Thats why we considered Recall also. Among those models, XGBoosting is giving best recall score.","46fd75bf":"## Summary for EDA \nSo to summarize our basic EDA we can conclude the below strategy for the bank to select the target audience\n\n* Higher Income more loan\n* Lower mortgage has more chances for personal loan. Exception-Exclude zero Mortgage candidates\n* Age and Experience do not much effect loan preference.\n* In all Education levels, maximum population located in 20 to 100 income range\n* Low Income and Low Mortgage-Less loan- New scheme for such peoples\n* Higher income and higher mortgage have better conversion ratio-Different marketing for easy pickers\n* Higher Credit spend and higher Income-more chances of conversion\n* Good income but less Credit spend(Income 50k~100K, and CCavg<2500)- Bright spot to increase the loan\n* The distribution of No personal loan compared to education reveals that we have mostly equal distribution of no-loan takers in all the 3 education levels. Higher eductaion has responding slighly more +ves but nor very big difference.","c3fa7008":"## Feature Importance","708f2a91":"5. Perceptron","6bbfba6d":"From the data we can say that the variables types are- Numeric: Age, Experience, Income, CCAvg, Mortgage Categorical: Family, Personal Loan, Securities Account, CD Account, Online, Education,Credit Card","e8fe8e2c":"Personal Loan is the feature we are going to predict. 0 means gave -ve response to the campaign, 1 means took personal loan as result of the campaign. We have to check what features influence 1. In the dataset we have only 480 (~9.6%), highly imbalance dataset.","34c1428f":"1. Logistic Regression","b9b66949":"7. Random Forest","757b40b6":"## Model Building","b2f7db43":"From the above table experience has -ve values which is not the correct value. We'll remove the -ve values from this variable and make those 0.","bcc0464a":"## Model Evaluation","48de4f02":"3. Finding distribution of Categorical variable with respect to Personal Loan","29177da9":"3. KNN","fdba8007":"8. Gradient Boosting","70307d09":"We can use Logistic Regression to validate our assumptions and decisions for feature creating and completing goals. This can be done by calculating the coefficient of the features in the decision function.\n\nPositive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).","c16ee76d":"6. Decision Tree","b0233032":"### Data Splitting","ebe1b57f":"## EDA","3f4a9875":"Now we are ready to train a model and predict the required solution. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification. We want to identify relationship between output (Loan yes or no) with other variables or features. We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification, we can narrow down our choice of models to a few. These include:\n\n* Logistic Regression\n* KNN or k-Nearest Neighbors\n* Support Vector Machines\n* Naive Bayes classifier\n* Decision Tree\n* Random Forrest\n* Perceptron\n* Artificial neural network","8b8a69e6":"2. Understanding relationships of numeric variables with Personal Loan","0cfb2e43":"2. SVM ","8a03757b":"There are no missing values. Lets proceed with EDA","49e124b5":"Conclusion: Age and experience are highly correlated, quite obviously. Income and CC average spent are also significantly correlated.","09b65fb0":"Similar pattern we can see from this graph also, higher Income+ CC spent and higher Income+Mortgage have responded +ve to loan.","447f7ab9":"Conclusions:\nCD Account is highest positivie coefficient, implying as CD Account is 1, the probability of Loan =1 increases the most.","9d304c6c":"### Bivariate Analysis","66343a93":"## Final Models","2ea1969a":"9. XGBoosting","2ebc2eaa":"4. Naive Bayes","bcc3ff73":"We are not able to increase the accuracy of Rf using cross validation and Grid Search any further."}}