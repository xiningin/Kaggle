{"cell_type":{"178c048e":"code","3527ebd9":"code","418b919c":"code","9256e0cc":"code","d7a2feef":"code","b5e5b24b":"code","288cd602":"code","ab115773":"code","2ebadf8e":"code","4298d2b0":"code","d72dac86":"code","7212b9d3":"code","db47e15d":"code","3b0edc7f":"code","154d7d66":"code","01b648a5":"code","00ab6729":"code","7e2b2619":"code","d981befe":"code","d093cb8a":"code","de2f62af":"code","c808d49f":"code","34e48e38":"code","14aa4814":"code","11146edf":"code","c53d46ed":"code","2ab5563a":"code","0eb0c0a6":"code","930406e2":"code","bcdae740":"code","ba514c17":"code","87d9a511":"code","a352a2bf":"code","2ebad335":"code","df919ee8":"code","21bd3ba2":"code","78e17ba7":"code","b581cb2d":"code","01d49f84":"code","34aee9dc":"code","54070821":"code","720fefdf":"code","5078604b":"code","d2e69cbe":"code","f3d003f4":"code","60c77548":"code","114421cf":"code","71e88b0b":"code","9f24b395":"code","d49d786e":"markdown","7c29d08d":"markdown","cfed0fc4":"markdown","c9f786ab":"markdown","7ceef6b2":"markdown","85d1c1f9":"markdown","d77d42e0":"markdown","16f1b480":"markdown","b102abdd":"markdown","8f409e11":"markdown","e1527b0d":"markdown","b8ffd329":"markdown","a2db4f9b":"markdown","9585d3e7":"markdown","a0107b79":"markdown","b5ae9241":"markdown","f1040371":"markdown","df47a307":"markdown","e79fe695":"markdown"},"source":{"178c048e":"#Import libraries\nimport numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)","3527ebd9":"#reading data\ntrain= pd.read_csv('\/kaggle\/input\/hackerearth-ml-challenge-pet-adoption\/train.csv')\ntest= pd.read_csv('\/kaggle\/input\/hackerearth-ml-challenge-pet-adoption\/test.csv')\n\nprint(\"Train Shape: \",train.shape)\nprint(\"Test Shape: \", test.shape)","418b919c":"# Check for columns\nprint(train.columns)\nprint(test.columns)","9256e0cc":"# Checking the data\ntrain.head()","d7a2feef":"test.head()","b5e5b24b":"#check for datatypes\nprint(train.dtypes)\nprint('*'*30)\nprint(test.dtypes)","288cd602":"print('Var1: Breed Category')\nprint(train['breed_category'].value_counts())\nprint()\nprint('Var2: Pet Category')\nprint(train['pet_category'].value_counts())","ab115773":"# train\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","2ebadf8e":"# test\ntotal_test = test.isnull().sum().sort_values(ascending=False)\npercent_test = (test.isnull().sum()\/test.isnull().count()).sort_values(ascending=False)\nmissing_data_test = pd.concat([total_test, percent_test], axis=1, keys=['Total', 'Percent'])\nmissing_data_test","4298d2b0":"# Col1: pet_id\nprint(train.shape)\nprint(train.pet_id.nunique())\n\nprint()\n\nprint(test.shape)\nprint(test.pet_id.nunique())","d72dac86":"train.sort_values(by=['pet_id']).head()","7212b9d3":"test.sort_values(by=['pet_id']).head()","db47e15d":"train.sort_values(by=['issue_date']).head()","3b0edc7f":"# feature engg\n# getting substring from pet_id for new feature\ntrain['nf1_pet_id'] = train['pet_id'].str[:6]\ntrain['nf2_pet_id'] = train['pet_id'].str[:7]","154d7d66":"# check for new feature-1\nprint(train.nf1_pet_id.nunique())\nprint(train.nf1_pet_id.value_counts())","01b648a5":"# check for new feature-2\nprint(train.nf2_pet_id.nunique())\nprint(train.nf2_pet_id.value_counts())","00ab6729":"train.groupby(['nf1_pet_id', 'pet_category']).size()","7e2b2619":"test['pet_id'].str[:6].value_counts()","d981befe":"# Col2-3: issue_data and listing_date \n\n#anomoly detection datetime- train\ntrain['issue_date']= pd.to_datetime(train['issue_date'])\ntrain['listing_date']= pd.to_datetime(train['listing_date'])\n\ntrain['duration_days'] = (train['listing_date'] - train['issue_date']).dt.days\ntrain.loc[train['listing_date'] < train['issue_date']]","d093cb8a":"#anomoly detection datetime- test\ntest['issue_date']= pd.to_datetime(test['issue_date'])\ntest['listing_date']= pd.to_datetime(test['listing_date'])\ntest.loc[test['listing_date'] < test['issue_date']]","de2f62af":"# Col4: condition\ntrain = train.fillna(-99)\ntest = test.fillna(-99)\nprint(train['condition'].value_counts())\nprint()\nprint(test['condition'].value_counts())","c808d49f":"train.groupby(['condition','pet_category']).size()","34e48e38":"train.columns","14aa4814":"train.groupby(['condition','X1','X2','breed_category']).size()","11146edf":"# Col5: color_type\nprint(train['color_type'].value_counts())\nprint('*'*40)\nprint(test['color_type'].value_counts())","c53d46ed":"train.groupby(['color_type', 'pet_category']).size()","2ab5563a":"train.groupby(['color_type','breed_category']).size()","0eb0c0a6":"print(train['color_type'].nunique())\nprint(test['color_type'].nunique())","930406e2":"#to find which two color types not present in test\nset(train.color_type) - set(test.color_type)","bcdae740":"set(test.color_type) - set(train.color_type)","ba514c17":"# Col6-7: length(m) and height(cm)\nsns.distplot(train['length(m)'])","87d9a511":"df=train[['length(m)','height(cm)']]\ndf['length(cm)'] = df['length(m)']*100\ndf[['length(cm)','height(cm)']].boxplot()","a352a2bf":"train.describe()","2ebad335":"print(len(train[train['length(m)'] == 0]))\nprint(len(test[test['length(m)']==0]))","df919ee8":"#convert length(m) to length(cm)\ntrain['length(cm)'] = train['length(m)'].apply(lambda x: x*100)\ntest['length(cm)'] = test['length(m)'].apply(lambda x: x*100)","21bd3ba2":"train.drop('length(m)', axis=1, inplace=True)\ntest.drop('length(m)', axis=1, inplace=True)","78e17ba7":"train[train['length(cm)']==0].groupby(['length(cm)','pet_category']).size()","b581cb2d":"test['length(cm)'].mean()","01d49f84":"# replace all 0 length with mean of lengths\nval = train['length(cm)'].mean()\ntrain['length(cm)'] = train['length(cm)'].replace(to_replace=0, value=val)\ntest['length(cm)'] = test['length(cm)'].replace(to_replace=0, value=val)","34aee9dc":"# check again for 0 length\nprint(len(train[train['length(cm)'] == 0]))\nprint(len(test[test['length(cm)']==0]))","54070821":"train[['length(cm)','height(cm)']].describe()","720fefdf":"#new feature\ntrain['ratio_len_height'] = train['length(cm)']\/train['height(cm)']","5078604b":"#relation between ratio and pet_category\nsns.catplot(x='pet_category',y='ratio_len_height',data=train)","d2e69cbe":"sns.catplot(x='breed_category',y='ratio_len_height',data=train)","f3d003f4":"sns.catplot(x='pet_category',y='duration_days',data=train)","60c77548":"sns.boxplot(x='breed_category',y='height(cm)',data=train)","114421cf":"# Col8-9: X1, X2 \n#X1\nprint(train['X1'].value_counts())\nprint('*'*30)\nprint(test['X1'].value_counts())","71e88b0b":"#X2\nprint(train['X2'].value_counts())\nprint('*'*30)\nprint(test['X2'].value_counts())","9f24b395":"#correlation matrix\nplt.subplots(figsize=(10,8))\nsns.heatmap(train.corr(), annot= True)","d49d786e":"* We have only one column condition with missing values both in train and test.","7c29d08d":"* I also found duration_days to be very useful.","cfed0fc4":"* Tried sorting values by id and issue_date in order to understand how data split was made in order to choose validation split. It didn't work.\n* var pet_id is unique for each rows in train and test.\n* Possibility of new feature generation from alphanumeric col pet_id.","c9f786ab":"**Yaaa!!! Looks like we hit a jackpot here.**\n\n*TODO: Modelling*\n\nGenerate 3 binary features for -99, 0.0, 1.0 condition types.","7ceef6b2":"#### With better feature engineering and a careful validation strategy, It easy to score better. \n#### I hope this notebook was helpful. Will keep updating and publish my Modelling notebook soon. \n\n### Thank you, kindly Upvote and Happy learning :)","85d1c1f9":"**93 rows in train and 44 column in test have length column zero**","d77d42e0":"* Ratio of length and height is somewhat distinctive feature. Useful.","16f1b480":"#### Other variables analysis and relation with targets","b102abdd":"* We have two target lables to predict: breed_category and pet_category.","8f409e11":"*TODO: Modelling*\n1. Research and try predicting the anonomized features X1 and X2 with their distribution.","e1527b0d":"* After creating lots of features, correlation matrix can help us in effective feature selection. We can remove those features which has strong correlation between them (corr > 0.9)\n* Mild correlation between X1 and X2\n\n#### Additional Notes\n\n1. For feature selection use any of univariate feature selection mechanism f_classif, chi2, mutual_info_classif. \n\n**I used ANOVA-F value f_classif in my case.**\n\n2. I have not used any kind of model stacking or blending approach yet. XGB seems to be giving good results as always. \n3. For categorical variables encoding go for One Hot Encoding, I also tried mean target encoding technique with StratifiedKFold approach and regularization parameter. It gave similar results to One Hot Encoding.\n","b8ffd329":"*TODO: Modelling*\n\n1. Generate multiple datetime features from issue_date and listing_date.\n2. Correct 2 detected anomolies in train.","a2db4f9b":"**Many pets have length zero**","9585d3e7":"#### Missing Values","a0107b79":"### Hackerearth ML Challenge 2020 - Adopt a buddy\n\n#### Problem type: Multitarget Multiclass Classification\n\nThis is an ongoing ML competition on Hackerearth (Jul 30, 2020 - Aug 23, 2020). We are required to build an model to determine type and breed of the animal based on its physical attributes and other factors. The evaluation metric being used is (the average of both f1_scores * 100).\n\nFor this competition the key to get on the top of leaderboard is **data Analysis and generating new features** which I have covered in this notebook. After end of the competition on Aug 23rd, I have secured Rank 9th with the public leaderboard score of 91.32278 \n\n**Kindly upvote if you find it interesting\/helpful and comment your suggestions or any queries.**","b5ae9241":"*TODO: Modelling*\n1. generate new ratio feature based on length and height, research pets length height correlation and possibility of more features.\n2. check for anomoly and correct","f1040371":"**Another cool feature found**\n\n*TODO: Modelling*\n\n* Generate new features based on grouped color_type variables. Particulary useful for predicting pet_category.","df47a307":"#### Target Variable Analysis","e79fe695":"**The first thing we can notice is the imbalaced classes in target variables. Due to imbalanced class distribution, we need to be very careful while choosing any validation strategy. StratifiedKFold validation will be good. We can take a note of few things here:**\n* There are 3 classes in breed category -> 0, 1, 2\n* there are 4 classes in pet category   -> 0, 1, 2, 4 \n* No class labelled 3 in pet category.\n"}}