{"cell_type":{"223c8a59":"code","84af881e":"code","272362d3":"code","a694ba49":"code","cd9e9809":"code","b88ef42e":"code","76da1701":"code","10d25d5d":"markdown","ac6053f9":"markdown","5fa6528c":"markdown","d516fb17":"markdown","defda7e6":"markdown","e395714a":"markdown","50a71315":"markdown"},"source":{"223c8a59":"#the necessary imports\nfrom bs4 import BeautifulSoup\nimport requests\nimport nltk","84af881e":"#this gets the book from a certain website\nr = requests.get('http:\/\/www.gutenberg.org\/files\/5200\/5200-h\/5200-h.htm')\n\n#whenever you have a book you need to set the encoding correctly, most of the cases this is 'utf-8'\nr.encoding = 'utf-8'\n\n# Now lets extract the text from the html which is placed in our variable r\nhtml = r.text\n\n# Lets do a sanity and check the first 1000 words\nprint(html[:1000])","272362d3":"# Lets first create the soup from our HTML file\nsoup = BeautifulSoup(html)\n\n# Then we're getting the text out of it\ntext = soup.get_text()\n\n# Lets print a random area to make sure that everything is working fine\nprint(text[10000:11000])","a694ba49":"# First we create the tokenizer (\\w+) means all non-word characters\ntokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n\n# Then we will fill in the text\ntokens = tokenizer.tokenize(text)\n\n# Lets do a sanity check again\nprint(tokens[:4])","cd9e9809":"# Lets make a new list with lowercase words\nwords = []\n\n# Looping through the words and appending them in the new list.\nfor word in tokens:\n    words.append(word.lower())\n\n# Sanity, sanity, sanity\nprint(words[:4])","b88ef42e":"#For the stopwords we have to install nltk.corpus, else you will get an error. Lets place the stopwords in sw\nfrom nltk.corpus import stopwords\nsw = stopwords.words('english')\n\n# We create a new list without any stopword, called words_ns\nwords_ns = []\n\n# Appending to words_ns all words that are in words but not in sw\nfor word in words:\n    if word not in sw:\n        words_ns.append(word)\n\n# Lets make sure that the stopwords are gone, as you can see 'by' is now gone\nprint(words_ns[:4])","76da1701":"# Displays figures inline in Jupyter Notebooks\n%matplotlib inline\n\n# Create a frequency distribution\nfreqdist = nltk.FreqDist(words_ns)\n\n# Lets plot and see if it has paid off!\nfreqdist.plot\n\n\n# The word found most is 'Gregor'","10d25d5d":"## Task 5:\n\nWe're making good progress. The next step is to make everything lowercase so that won't cause any difficulties when python is going through the text. I did this as follows.","ac6053f9":"## Task 4:\n\nWell that seems about right although there is still a lot of text at the beginning and at the end. Since it's not that much we just leave it as it is. Now it's time to use the nltk- natural language tool kit. First we will remove whitespaces and dots and commas. Else this could be seen by Python as a word. This is called tokenizing, bellow is the way I have done it.","5fa6528c":"## Task 6:\n    \nWe don't want to have any stop words since they will probably be the first ones, we are more interested in the real content. So lets remove the stopwords, luckily there is a package for that in nltk.","d516fb17":"## Task 7:\n\nAs last we need to figure out how to the first words. We are going to use matplotlib inline, this might differ when you're not using Jupyter Notebooks. ","defda7e6":"## Task 2:\n\nWe need to find a book that we like in HTML format. Gutenberg.org is a website with a bunch of free books, so this is a good start.\n\nPersonally I am fond of Kafka's \"Metamorphosis\", so I pick this one. The link to my book would be as follows 'http:\/\/www.gutenberg.org\/files\/5200\/5200-h\/5200-h.htm'. Once picked lets get it into Python by a request.","e395714a":"## Task 1: \n\n\nLets start with importing the necessary modules, so we will import the three main modules.","50a71315":"## Task 3:\n    \nWell that all seems to work. There are a few things that we should take into consideration, for example you can ssee the HTML code at the top. We don't want to keep this so we need to look for a solution, and BeautifulSoup comes in handy on that matter. BeautifulSoup creates from HTML soup humanly readable soup."}}