{"cell_type":{"9663da13":"code","504e019f":"code","dce1c68c":"code","a3d92628":"code","71397d7f":"code","67473454":"code","76530838":"code","3d145daa":"code","99b3c7d8":"code","cd9d13d9":"code","8729445e":"code","15b8af89":"code","acdd6a5e":"code","9d3bbe21":"code","a118cabe":"code","2a53e23e":"code","e403e6b2":"code","a91581c7":"code","b1c042d8":"code","ce0cc1c9":"code","ddea569a":"code","8bba3d99":"code","aad4d213":"code","3929e2d3":"code","47b40ba4":"code","7fa41880":"code","c61e2798":"code","6188cd24":"code","f61d2c6c":"code","4c5e68cf":"code","abf2efe9":"code","a05c13d7":"code","d35a2a32":"code","1446e9da":"code","8141a6d6":"code","8d3a9ebb":"code","3de34f0e":"code","512d3ccb":"code","a782196b":"code","db720da3":"code","e4b4417b":"code","8dacc3f9":"code","c8ce604c":"code","ef99a9a0":"code","07d8e0d4":"code","82dfaf88":"code","5a75a836":"code","2a3107d2":"code","4d3ae85b":"code","41b74f12":"code","4b943ef2":"code","0e52634a":"code","e0060ffb":"code","f94c13a5":"code","d5b4a3ec":"code","beb4c9ba":"code","16344638":"markdown","de12ad38":"markdown","bb92b810":"markdown","8d3401c2":"markdown"},"source":{"9663da13":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n#Ignore Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","504e019f":"# explore the algorithm wrapped by RFE\n#from sklearn.feature_selection import RFE\n#from sklearn.feature_selection import RFECV\nfrom sklearn.feature_selection import SelectKBest\n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif\n\n#Calculate Accuracy\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\n\n#Stat test\nfrom scipy.stats import f_oneway\nfrom scipy.stats import ttest_ind\n\nfrom pprint import pprint\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport scipy.stats as stats\n\nfrom matplotlib import pyplot\n\nimport matplotlib.pylab as plt\n%matplotlib inline\n\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 8\n\n#This is similar to pd.DataFrame\nimport dask.dataframe as dd\n\n#This is an API to call for local Dask Cluster\nfrom dask.distributed import Client, LocalCluster\n\nfrom sklearn.model_selection import RandomizedSearchCV","dce1c68c":"#Function to reduce memory usage\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64','float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                else:\n                    df[col] = df[col].astype(np.float32)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\n# feature selection with score_func indicated\ndef select_features(X_train, y_train, score_func):\n    # configure to select all features\n\tfs = SelectKBest(score_func=score_func, k='all')\n\t# learn relationship from training data\n\tfs.fit(X_train, y_train)\n\t# transform train input data\n\tX_train_fs = fs.transform(X_train)\n\t\n\treturn X_train_fs, fs\n ","a3d92628":"#Taking 2 samples from dataset for training and validation\ntrain_sample = reduce_mem_usage(pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/train.csv\", nrows=10000))\n#train = reduce_mem_usage(pd.read_csv(\"train.csv\", nrows=10000))\ntest_sample = reduce_mem_usage(pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/test.csv\", nrows=10000))","71397d7f":"#Base assumption is I can infer to the population with samples.\n#Pull a sample from the data and sub sample to see if it makes \n#sense.\n\nrow = list()\nmeans = list()\nn=1000\n\nfor count in range(1,n):\n        row.append(count)\n        means.append(train_sample.sample(n=500).target.mean())\ncurve = pd.Series(means,index=row)\n\nfor count in range(1,n):\n        row.append(count)\n        means.append(train_sample.sample(n=100).target.mean())\ncurve2 = pd.Series(means,index=row)\n","67473454":"#Check if train db outcome can be infer with test \npop_mean = train_sample.target.mean()\nstats.ttest_1samp(curve, pop_mean)","76530838":"#null hypothesis is that they are equal, alternative is they are not.\ndef plot_distribution(inp, n=0):\n    plt.figure()\n    ax=sns.displot(inp)\n    \n    plt.axvline(np.mean(inp),color='k',linestyle='dashed', linewidth=5)\n    _, max_ = plt.ylim()\n    plt.text(inp.mean()+inp.mean()\/10, max_- max_ \/ 10, \"Mean: {:.2f}\".format(inp.mean()),\n            )\n    plt.title(str(n) + 'samples')\n    \n    return plt.figure\n\nplot_distribution(curve, 500)\nplot_distribution(curve2, 100)","3d145daa":"fig = plt.figure(figsize=(20,10))\nsns.distplot(curve,hist=False, rug=True)\nsns.distplot(curve2,hist=False, rug=True)\nplt.axvline(np.mean(curve),color='green',linestyle='dashed',linewidth=3)\nplt.axvline(np.mean(curve2),color='orange',linestyle='dashed',linewidth=3)\nplt.show()","99b3c7d8":"del curve, curve2","cd9d13d9":"X = train_sample.drop(columns=['id','target']).select_dtypes(include='float16')\ny = train_sample.target\n\n# feature selection\nX_train_fs, fs = select_features(X, y, mutual_info_classif)\n\n# what are scores for the features\n#for i in range(len(fs.scores_)):\n#\tprint('Feature %d: %f' % (i, fs.scores_[i]))\n# plot the scores\npyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)\npyplot.show()\n","8729445e":"#Feature list\nmi_list  = list() \nfor i in range(len(fs.scores_)):\n    if (fs.scores_[i] > 0):\n        mi_list.append(i)\n\nmi_data = train_sample[X.columns[mi_list]]","15b8af89":"mi_data.head()","acdd6a5e":"# load the dataset\nchi_train = train_sample.drop(columns=['id','target'])\nchi_train = chi_train.select_dtypes(include=['int8','int32'])\n\n# feature selection\nX_train_fx, fx = select_features(chi_train, y,chi2)\n\n# what are scores for the features\n#for i in range(len(fx.scores_)):\n#\tprint('Feature %d: %f' % (i, fx.scores_[i]))\n\n# plot the scores\npyplot.bar([i for i in range(len(fx.scores_))], fx.scores_)\npyplot.show()","9d3bbe21":"#Feature list\nchi2_list  = list() \nfor i in range(len(fx.scores_)):\n    if (fx.scores_[i] > 0.1):\n        chi2_list.append(i)\nchi_data = train_sample[chi_train.columns[chi2_list]]","a118cabe":"chi_data","2a53e23e":"new_df = chi_data.join(mi_data)","e403e6b2":"def modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=10):\n    \n    dtrain_y = train_sample['target'].values\n    eval_metric = [\"auc\",\"error\"]\n    if useTrainCV:\n        xgb_param = alg.get_xgb_params()\n        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain_y)\n        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n        alg.set_params(n_estimators=cvresult.shape[0])\n    \n    #Fit the algorithm on the data\n    alg.fit(dtrain[predictors], train_sample['target'],eval_metric=eval_metric)\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain[predictors])\n    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n        \n    #Print model report:\n    print (\"\\nModel Report\")\n    print (\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain_y, dtrain_predictions))\n    print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(train_sample['target'], dtrain_predprob))\n               \n    feat_imp = pd.Series(alg.get_booster().get_fscore()).sort_values(ascending=False)\n    feat_imp.plot(kind='bar', title='Feature Importances')\n    plt.ylabel('Feature Importance Score')\n    ","a91581c7":"trainb = new_df.copy()\n\ntarget = 'target'\nIDcol = 'id'\npredictors = [x for x in trainb.columns if x not in ['target', 'id']]\n\nxgb1 = XGBClassifier(\n learning_rate = 0.1,\n n_estimators = 1000,\n max_depth = 5,\n min_child_weight = 1,\n gamma = 0,\n subsample = 0.8,\n colsample_bytree = 0.8,\n objective = 'binary:logistic',\n nthread = 4,\n scale_pos_weight = 1,\n seed = 27)\n\nmodelfit(xgb1, trainb, predictors)","b1c042d8":"X = train_sample.loc[:,new_df.columns]\n\nX_train, X_validation, y_train, y_validation = train_test_split(X, train_sample.target, test_size=0.3)\nxgb1.fit(X_train, y_train,eval_set=[(X_train, y_train), (X_validation, y_validation)], early_stopping_rounds=10) ","ce0cc1c9":"results = xgb1.evals_result()\n\nplt.figure(figsize=(10,7))\nplt.plot(results['validation_0'][\"logloss\"], label=\"Training loss\")\nplt.plot(results['validation_1'][\"logloss\"], label=\"Validation loss\")\nplt.axvline(xgb1.best_ntree_limit, color=\"gray\", label=\"Optimal tree number\")\nplt.xlabel(\"Number of trees\")\nplt.ylabel(\"Loss\")\nplt.legend()","ddea569a":"bntl = xgb1.best_ntree_limit","8bba3d99":"#Performing Coordinate Descent and tune depth and child weight\nparam_test1 = {\n 'max_depth':range(3,10,2),\n 'min_child_weight':range(1,8,2)\n}\n\ngsearch1 = GridSearchCV(estimator = XGBClassifier(seed=27), param_grid = param_test1, scoring='roc_auc',n_jobs=-1, cv=5)","aad4d213":"gsearch1.fit(trainb[predictors],train_sample[target], verbose=2)\n","3929e2d3":"print(gsearch1.best_params_,gsearch1.best_score_)","47b40ba4":"#Further tuning of depth and child weight\nparam_test2 = {\n 'max_depth':[2,3,4],\n 'min_child_weight':[4,5,6]\n}\ngsearch2 = GridSearchCV(estimator = XGBClassifier(seed=27), \n param_grid = param_test2, scoring='roc_auc',n_jobs=-1, cv=5)\n\ngsearch2 = GridSearchCV(estimator = XGBClassifier(seed=27), param_grid = param_test2, scoring='roc_auc',n_jobs=-1, cv=5)\n","7fa41880":"gsearch2.fit(trainb[predictors],train_sample[target], verbose=2)","c61e2798":"gsearch2.best_params_","6188cd24":"param_test2b = {\n 'min_child_weight':[2,3,4,5]\n}\ngsearch2b = GridSearchCV(estimator = XGBClassifier(max_depth=2,seed=27), \n param_grid = param_test2b, scoring='roc_auc',n_jobs=4, cv=5)\n\ngsearch2b.fit(trainb[predictors],train_sample[target], verbose=2)","f61d2c6c":"gsearch2b.best_params_\ngsearch2b.best_score_","4c5e68cf":"param_test3 = {\n 'gamma':[i\/10.0 for i in range(0,50)]\n}\ngsearch3 = GridSearchCV(estimator = XGBClassifier(max_depth=2,\n min_child_weight=4, seed=27), \n param_grid = param_test3, scoring='roc_auc',n_jobs=4, cv=5)\n\ngsearch3.fit(trainb[predictors],train_sample[target], verbose=2)\n#gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_","abf2efe9":"print(gsearch3.best_params_, gsearch3.best_score_)","a05c13d7":"param_test4 = {\n 'subsample':[i\/10.0 for i in range(1,10)],\n 'colsample_bytree':[i\/10.0 for i in range(1,10)]\n}\ngsearch4 = GridSearchCV(estimator = xgb.XGBClassifier(max_depth=2,\n min_child_weight=4, gamma=4.9, seed=27), \n param_grid = param_test4, scoring='roc_auc',n_jobs=4, cv=5)\ngsearch4.fit(trainb[predictors],train_sample[target], verbose=2)\n","d35a2a32":"print(gsearch4.best_params_, gsearch4.best_score_)","1446e9da":"param_test5 = {\n 'subsample':[i\/100.0 for i in range(85,95,1)],\n 'colsample_bytree':[i\/100.0 for i in range(5,15,1)]\n}\n\ngsearch5 = GridSearchCV(estimator = XGBClassifier(max_depth=2,\n min_child_weight=4, gamma=4.9, seed=27), \n param_grid = param_test5, scoring='roc_auc',n_jobs=4, cv=5)\n\ngsearch5.fit(trainb[predictors],train_sample[target], verbose=2)\n","8141a6d6":"print(gsearch5.best_params_, gsearch5.best_score_)","8d3a9ebb":"param_test6 = {\n 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n}\n\ngsearch6 = GridSearchCV(estimator = XGBClassifier(max_depth=2,\n min_child_weight=4, gamma=4.9, colsample_bytree=0.14,subsample=0.88,seed=27), \n param_grid = param_test6, scoring='roc_auc',n_jobs=4, cv=5)\n\ngsearch6.fit(trainb[predictors],train_sample[target], verbose=2)\n","3de34f0e":"print(gsearch6.best_params_, gsearch6.best_score_)","512d3ccb":"param_test7 = {\n 'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]\n}\n\ngsearch7 = GridSearchCV(estimator = XGBClassifier(max_depth=2,\n min_child_weight=4, gamma=4.9, colsample_bytree=0.14,subsample=0.88,seed=27), \n param_grid = param_test7, scoring='roc_auc',n_jobs=4, cv=5)\n\ngsearch7.fit(trainb[predictors],train_sample[target], verbose=2)","a782196b":"print(gsearch7.best_params_, gsearch7.best_score_)","db720da3":"#Tuning the lamda reduces accuracy\n#param_test8 = {\n# 'reg_lambda':[0.1, 1.0, 5.0, 10.0, 50.0, 100.0]\n#}\n\n#gsearch8 = GridSearchCV(estimator = XGBClassifier(max_depth=2,\n# min_child_weight=4, gamma=4.9, colsample_bytree=0.95,\n# subsample=0.26,reg_alpha= 0.05,seed=27), \n# param_grid = param_test8, scoring='roc_auc',n_jobs=4, cv=5)\n\n#gsearch8.fit(trainb[predictors],train_sample[target], verbose=2)","e4b4417b":"xgb1 = XGBClassifier(\n learning_rate = 0.01,\n n_estimators = 2000,\n max_depth=2,\n min_child_weight=4, \n gamma=4.9, \n colsample_bytree=0.95,\n subsample=0.26,\n reg_alpha= 0.05,\n seed=27)\n\nxgb1.fit(X_train, y_train,eval_set=[(X_train, y_train), (X_validation, y_validation)], early_stopping_rounds=10) \nresults = xgb1.evals_result()\n\nplt.figure(figsize=(10,7))\nplt.plot(results['validation_0'][\"logloss\"], label=\"Training loss\")\nplt.plot(results['validation_1'][\"logloss\"], label=\"Validation loss\")\nplt.axvline(xgb1.best_ntree_limit, color=\"gray\", label=\"Optimal tree number\")\nplt.xlabel(\"Number of trees\")\nplt.ylabel(\"Loss\")\nplt.legend()\n","8dacc3f9":"bntl = xgb1.best_ntree_limit","c8ce604c":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 100)]\n#max_depth [0,\u221e]\nmax_depth = [int(x) for x in np.linspace(0, 15, 1)]\n#min_child_weight [0,\u221e]\nmin_child_weight= [int(x) for x in np.linspace(0, 15, 1)] \n#colsample_by[0, 1]\ncolsample_bytree = [x\/100 for x in range(50,100 ,1)]\n#subsample 0 to 1\nsubsample = [x\/100 for x in range(0,50, 1)]\n#gamma: [0,\u221e]\ngamma = [x\/100 for x in range(0,100, 1)]\n#reg_alpha, [0,\u221e]\nreg_alpha = [1e-5, 1e-2, 0.1, 1, 100]\n#reg_lambda [0,\u221e]\nreg_lambda = [1e-5, 1e-2, 0.1, 1, 100]\n#learning_rate [0,1]\nlearning_rate = [1e-5, 1e-2, 0.1, 1, 100]\n\n#random_grid = {\n#               'n_estimators': n_estimators,\n#               'max_depth': max_depth,\n#               'min_child_weight': min_child_weight,\n#               'colsample_bytree': colsample_bytree,\n#               'subsample': subsample,\n#               'gamma': gamma,\n#               'reg_alpha' : reg_alpha,\n#               'reg_lambda' : reg_lambda,\n#               'learning_rate' : learning_rate,\n#                }\n\n#print(random_grid)","ef99a9a0":"xgb2 = XGBClassifier(\n     n_estimators = 739,\n     max_depth = 2,\n     min_child_weight = 4, \n     gamma = 4.9, \n     colsample_bytree = 0.95,\n     subsample = 0.26,\n     objective='binary:logistic',\n     seed=27)\n#xgb_random = RandomizedSearchCV(estimator = xgb2, param_distributions = random_grid, scoring='roc_auc', n_iter = 10, cv = 5, verbose=2, random_state=27, n_jobs = -1)\n#xgb_random.fit(X_train,y_train)\nparams = {\n        #'min_child_weight': [1, 5, 10],\n        #'gamma': [0.5, 1, 1.5, 2, 5],\n        #'subsample': [0.6, 0.8, 1.0],\n        #'colsample_bytree': [0.6, 0.8, 1.0],\n        #'max_depth': [3, 4, 5],\n        'reg_alpha' : reg_alpha,\n        'reg_lambda' : reg_lambda,\n        'learning_rate' : learning_rate,\n        }\n\nfolds = 5\nparam_comb = 100\n\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 27)\n\nrandom_search = RandomizedSearchCV(xgb2, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=-1, cv=skf.split(X_train,y_train), verbose=3, random_state=27 )\nrandom_search.fit(X_train,y_train)","07d8e0d4":"print(random_search.best_params_,random_search.best_score_)","82dfaf88":"xgb2 = XGBClassifier(\n     n_estimators = 739,\n     max_depth = 2,\n     min_child_weight = 4, \n     gamma = 4.9, \n     colsample_bytree = 0.95,\n     subsample = 0.26,\n     objective='binary:logistic',\n     reg_alpha = 0.01,\n     reg_lambda = 1e-05,\n     learning_rate = 0.01,\n    seed=27)","5a75a836":"modelfit(xgb2, trainb, predictors)","2a3107d2":"xgb2.fit(X_train, y_train,eval_set=[(X_train, y_train), (X_validation, y_validation)], early_stopping_rounds=20) \nresults = xgb2.evals_result()\n\nplt.figure(figsize=(10,7))\nplt.plot(results['validation_0'][\"logloss\"], label=\"Training loss\")\nplt.plot(results['validation_1'][\"logloss\"], label=\"Validation loss\")\nplt.axvline(xgb1.best_ntree_limit, color=\"gray\", label=\"Optimal tree number\")\nplt.xlabel(\"Number of trees\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\n","4d3ae85b":"#xgb1.save_model(\"my_xgboost.json\")\nimport pickle\nxgb2.n_estimators=80\npickle.dump(xgb2,open('model.pkl','wb'))\n\n# load\n#new_xgb = xgb.XGBClassifier()\n#new_xgb.load_model(\".\/submission.csv\/my_xgboost.json\")\n\n# check optimal number of trees of loaded model\n#new_xgb.best_ntree_limit\n","41b74f12":"cluster = LocalCluster(n_workers = 2)\nclient = Client(cluster)\n\ntrain_dask = dd.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv')#,blocksize=64e6)\n","4b943ef2":"train_dask.persist()\nX = train_dask.loc[:,new_df.columns]\ny = train_dask['target']\n\ndtrain = xgb.dask.DaskDMatrix(client,X,y)","0e52634a":"params = {\n 'n_estimators' : 80,\n 'max_depth': 2,\n 'min_child_weight' : 4, \n 'gamma' : 4.9, \n 'colsample_bytree' : 0.95,\n 'subsample' : 0.26,\n 'nthread' : 4,\n 'reg_alpha': 0.01,   \n 'reg_lambda' : 1e-05,\n 'learning_rate' : 0.01,\n}\n\n# train the model\n#%%time \noutput = xgb.dask.train(\n    client, params, dtrain, num_boost_round = 100,\n    evals=[(dtrain, 'accuracy'),(dtrain, 'auc_roc')], early_stopping_rounds = 10\n)\n\nbooster = output['booster']  # booster is the trained model\nhistory = output['history']  # A dictionary containing evaluation \n","e0060ffb":"test_dask = dd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv')\ntest_dask.persist()\n\ntest_id = test_dask['id']\n\n#test = xgb.DMatrix(test_dask.loc[:,new_df.columns])\n","f94c13a5":"test = test_dask.loc[:,new_df.columns]\npreds = xgb.dask.predict(client, output, test)\n#prediction = xgb.dask.predict(client, output, X)\ny_test = preds.compute()\nsubmission = pd.DataFrame(list(zip(test_id, y_test)), columns =['id', 'target'])\nsubmission.to_csv('submission.csv', index=False)","d5b4a3ec":"client.close()","beb4c9ba":"submission.head()","16344638":"**Final Model Training with Dask using selected features and parameters**","de12ad38":"**Randomized Grid Search**","bb92b810":"**Save final model in pickle \/ other formats**","8d3401c2":"**Coordinate Descent**"}}