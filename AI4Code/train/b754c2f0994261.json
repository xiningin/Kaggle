{"cell_type":{"5fb0498d":"code","5c76b7bd":"code","b4f1d3dc":"code","d7b14f0b":"code","c6fd4eaf":"code","234121d3":"code","ee1fbe02":"code","e8e14154":"code","b9569a24":"code","3f9179ba":"code","0313ee91":"code","08b333f2":"code","4b71dac1":"code","d8faaad6":"code","7688c601":"code","6259176f":"code","f8076813":"code","e3747680":"code","7440a3c1":"code","c2d10b9c":"code","96ee8f24":"code","e67084d9":"code","a1663aac":"code","4d050dda":"code","a1aa5606":"code","8a1ad64e":"code","33fdaa12":"code","de5d1543":"code","b8a9d86c":"code","905e401f":"code","6bade139":"code","0f2700bb":"code","b69baaa4":"code","abd9f356":"code","53f900e7":"code","7c54c744":"code","7dbd118a":"code","afce811b":"code","c332add6":"code","7580b2a2":"code","2a3a7a55":"code","cfbe59ef":"code","98094d2a":"code","ba4337e7":"code","934eacfa":"code","31f1955c":"markdown","a92aa203":"markdown","ffbcdea0":"markdown","ff7f7270":"markdown","a59ba7b0":"markdown","c3902419":"markdown","9196383a":"markdown","f9dc0bff":"markdown","3151a87f":"markdown","e0082b4a":"markdown","7b37aa20":"markdown","72c0d6ff":"markdown","f9fb44f5":"markdown","b3b516a3":"markdown","c9370504":"markdown","9b7bec03":"markdown","66d967b8":"markdown","5667dad6":"markdown","70c34aea":"markdown","5a05797a":"markdown","2e4982bb":"markdown","978902ab":"markdown","40da2773":"markdown","81b0ce8b":"markdown","3e298810":"markdown","620a7596":"markdown","afd2eb9f":"markdown","b9c49acf":"markdown","69f359e5":"markdown","f9fac55e":"markdown","35cf0154":"markdown","6864c7d8":"markdown","c754ab2b":"markdown","c1b23dbf":"markdown","52f42ad0":"markdown","94d5d49d":"markdown","1d4dfcb0":"markdown","712fb9dd":"markdown","e1a41f66":"markdown","9fe86819":"markdown","4a207437":"markdown","b80e9af3":"markdown","87e1f237":"markdown","d366bdaf":"markdown","98833738":"markdown","341faaec":"markdown","ccf4d836":"markdown","10c0b167":"markdown"},"source":{"5fb0498d":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport tensorflow as tf\nimport keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nimport pickle\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport shutil\nimport random","5c76b7bd":"with open(\"..\/input\/datacleaningglassesnoglasses\/glasses.txt\", \"rb\") as fp:\n    glasses = pickle.load(fp)\nplt.figure(figsize=(12, 12))\nran_num = []\nfor i in range(0,9):\n    n = random.randint(0,len(glasses))\n    ran_num.append(n)\nfor i in range(9):\n    ax= plt.subplot(3, 3, i + 1)\n    plt.imshow(mpimg.imread(glasses[ran_num[i]]))\n    plt.title(\"glasses\")\n    plt.axis(\"off\")","b4f1d3dc":"with open(\"..\/input\/datacleaningglassesnoglasses\/no_glasses.txt\", \"rb\") as fp: \n    no_glasses = pickle.load(fp)\nplt.figure(figsize=(12, 12))\nran_num = []\nfor i in range(0,9):\n    n = random.randint(0,len(no_glasses))\n    ran_num.append(n)\nfor i in range(9):\n    ax= plt.subplot(3, 3, i + 1)\n    plt.imshow(mpimg.imread(no_glasses[ran_num[i]]))\n    plt.title(\"no_glasses\")\n    plt.axis(\"off\")","d7b14f0b":"with open(\"..\/input\/datacleaningglassesnoglasses\/no_clear.txt\", \"rb\") as fp: \n    no_clear = pickle.load(fp)\nplt.figure(figsize=(12, 12))\nran_num = []\nfor i in range(0,9):\n    n = random.randint(0,len(no_clear))\n    ran_num.append(n)\nfor i in range(9):\n    ax= plt.subplot(3, 3, i + 1)\n    plt.imshow(mpimg.imread(no_clear[ran_num[i]]))\n    plt.title(\"no_clear\")\n    plt.axis(\"off\")","c6fd4eaf":"print(\"The length of the different groups:\" + \"-Glasses: \" + str(len(glasses)) + \" -No glasses: \" + str(len(no_glasses)) + \" -No clear: \" + str(len(no_clear)))","234121d3":"tf.random.set_seed(123456)","ee1fbe02":"BATCH_SIZE = 32\nIMG_SIZE = (160, 160)","e8e14154":"all_images= glasses + no_glasses","b9569a24":"data_dir= \"\/kaggle\/input\/datacleaningglassesnoglasses\/Images\/Images\/\"","3f9179ba":"train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    validation_split=0.3,\n    subset=\"training\",\n    shuffle=True,\n    seed=123456,\n    image_size= IMG_SIZE,\n    batch_size=BATCH_SIZE)","0313ee91":"validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    validation_split=0.3,\n    subset=\"validation\",\n    shuffle=True,\n    seed=123456,\n    image_size= IMG_SIZE,\n    batch_size=BATCH_SIZE)","08b333f2":"class_names = train_dataset.class_names\nprint(class_names)","4b71dac1":"class_names = train_dataset.class_names\n\nplt.figure(figsize=(12, 12))\nfor images, labels in train_dataset.take(1):\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")","d8faaad6":"val_batches = tf.data.experimental.cardinality(validation_dataset)\ntest_dataset = validation_dataset.take(val_batches \/\/ 5)\nvalidation_dataset = validation_dataset.skip(val_batches \/\/ 5)","7688c601":"print('Number of training batches: %d' % tf.data.experimental.cardinality(train_dataset))\nprint('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\nprint('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))","6259176f":"AUTOTUNE = tf.data.AUTOTUNE\n\ntrain_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\nvalidation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)","f8076813":"data_augmentation = tf.keras.Sequential([\n  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n])","e3747680":"for image, _ in train_dataset.take(1):\n    plt.figure(figsize=(12, 12))\n    first_image = image[0]\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n        plt.imshow(augmented_image[0] \/ 255)\n        plt.axis('off')","7440a3c1":"preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input","c2d10b9c":"rescale = tf.keras.layers.experimental.preprocessing.Rescaling(1.\/127.5, offset= -1)","96ee8f24":"IMG_SHAPE = IMG_SIZE + (3,)\nbase_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')","e67084d9":"image_batch, label_batch = next(iter(train_dataset))\nfeature_batch = base_model(image_batch)\nprint(feature_batch.shape)","a1663aac":"base_model.trainable = False","4d050dda":"global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\nfeature_batch_average = global_average_layer(feature_batch)\nprint(feature_batch_average.shape)","a1aa5606":"prediction_layer = tf.keras.layers.Dense(1)\nprediction_batch = prediction_layer(feature_batch_average)\nprint(prediction_batch.shape)","8a1ad64e":"inputs = tf.keras.Input(shape=(160, 160, 3))\nx = data_augmentation(inputs)\nx = preprocess_input(x)\nx = base_model(x, training=False)\nx = global_average_layer(x)\nx = tf.keras.layers.Dropout(0.2)(x)\noutputs = prediction_layer(x)\nmodel = tf.keras.Model(inputs, outputs)","33fdaa12":"model.summary()","de5d1543":"base_learning_rate = 0.0001\nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])","b8a9d86c":"len(model.trainable_variables)","905e401f":"initial_epochs = 10\nloss0, accuracy0 = model.evaluate(validation_dataset)","6bade139":"print(\"initial loss: {:.2f}\".format(loss0))\nprint(\"initial accuracy: {:.2f}\".format(accuracy0))","0f2700bb":"model_fit = model.fit(train_dataset,\n                    epochs= initial_epochs,\n                    validation_data= validation_dataset)","b69baaa4":"acc = model_fit.history['accuracy']\nval_acc = model_fit.history['val_accuracy']\nloss_ = model_fit.history['loss']\nval_loss_ = model_fit.history['val_loss']","abd9f356":"## 3.8) Results","53f900e7":"plt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.ylim([0.8, 1])\nplt.plot([initial_epochs-1,initial_epochs-1],\n          plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss_, label='Training Loss')\nplt.plot(val_loss_, label='Validation Loss')\nplt.ylim([0, 1.0])\nplt.plot([initial_epochs-1,initial_epochs-1],\n         plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","7c54c744":"loss, accuracy = model.evaluate(test_dataset)\nloss, accuracy1 = model.evaluate(train_dataset)\nprint('Test accuracy :', accuracy)\nprint('Train accuracy :', accuracy1)","7dbd118a":"base_model.trainable = True","afce811b":"print(\"Number of layers in the base model: \", len(base_model.layers))\nfine_tune_at = 100\nfor layer in base_model.layers[:fine_tune_at]:\n    layer.trainable =  False","c332add6":"model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate\/10),\n              metrics=['accuracy'])","7580b2a2":"len(model.trainable_variables)","2a3a7a55":"fine_tune_epochs = 5\ntotal_epochs =  initial_epochs + fine_tune_epochs\n\nmodel_fit_fine = model.fit(train_dataset,\n                         epochs= total_epochs,\n                         initial_epoch= model_fit.epoch[-1],\n                         validation_data= validation_dataset)","cfbe59ef":"acc += model_fit_fine.history['accuracy']\nval_acc += model_fit_fine.history['val_accuracy']\nloss_ += model_fit_fine.history['loss']\nval_loss_ += model_fit_fine.history['val_loss']","98094d2a":"plt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.ylim([0.8, 1])\nplt.plot([initial_epochs-1,initial_epochs-1],\n          plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss_, label='Training Loss')\nplt.plot(val_loss_, label='Validation Loss')\nplt.ylim([0, 1.0])\nplt.plot([initial_epochs-1,initial_epochs-1],\n         plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","ba4337e7":"loss, accuracy = model.evaluate(test_dataset)\nloss, accuracy1 = model.evaluate(train_dataset)\nloss, accuracy2 = model.evaluate(validation_dataset)\nprint('Test accuracy :', accuracy)\nprint('Train accuracy :', accuracy1)\nprint('Validation accuracy :', accuracy2)","934eacfa":"image_batch, label_batch = test_dataset.as_numpy_iterator().next()\npredictions = model.predict_on_batch(image_batch).flatten()\n\npredictions = tf.nn.sigmoid(predictions)\npredictions = tf.where(predictions < 0.5, 0, 1)\n\nprint('Predictions:\\n', predictions.numpy())\nprint('Labels:\\n', label_batch)\n\nplt.figure(figsize=(12, 12))\nfor i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(image_batch[i].astype(\"uint8\"))\n    plt.title(class_names[predictions[i]])\n    plt.axis(\"off\")","31f1955c":"### As per the results, it looks like that the `model classified perfectly the images`, it can be seen that the array of `Predictions` is the same as `Labels`","a92aa203":"## 3.5) Adding classification head","ffbcdea0":"## Currently we have the `train_dataset` and `validation_dataset` created, however it is important to have also a `small split to test the model`, test_dataset. This will be `20% of the validation_dataset`, this means, more or less `6%`","ff7f7270":"## After selecting the parameters, we are ready to split the data, in this case it will be separated as per below:\n- 70% Training data\n- 30% Validation data","a59ba7b0":"## No clear:","c3902419":"# 4) Model with fine tunning","9196383a":"### As it can be seen in the graphs the `accuracy` along the `10 epochs` was `really similar` for the `validation` and `training` samples\n### However, it seems that for some epochs the model works slighty better in the `validation sample` than in the `training sample`. This probably happened due to the application of some layers like `tf.keras.layers.BatchNormalization` and `tf.keras.layers.Dropout`, which are applied during the `training process`","f9dc0bff":"### In this case the `Model 2 with fine tuning` will be applied, as the performance of the same was better. The prediction is going to be done in the `test sample`","3151a87f":"## No glasses:","e0082b4a":"## The Dataset.prefetch() function used in the three splits, overlaps data preprocessing and model execution during the training process","7b37aa20":"### This model is a continuation of the `Model 1`, however, a `fine tunning`process will be applied in order to `increase the performance`\n### During the previos process the `weights of the pre-trained network` were `not updated` during the training.\n### However, it is possible to increase the performance applying these `weights`","72c0d6ff":"## It seems the train_dataset contains 4920 images, and the validation_datset 1476, out of 4920 files","f9fb44f5":"### This model expects `pixel values between -1 and 1`, hence the images should be `preprocessed`","b3b516a3":"### The layer `tf.keras.layers.GlobalAveragePooling2D` is going to be used, in order to convert the features in a `1280-element vector`, per each image","c9370504":"### It is time to apply the previous process to the model:\n- `Data augmentation`\n- `Rescaling`\n- `Basel model`\n- `Feature extractor`","9b7bec03":"## 3.7) Training the model","66d967b8":"### The base model `MobileNet V2` was developed by Google\n### It contains a large dataset with `1.4 million of images` and `1000 classes`\n### It is important to include `include_top=False` because the classification layers previously created should be included","5667dad6":"## 3.6) Compilation of the model:","70c34aea":"# 1) Data cleaning:","5a05797a":"### The data is created `artificially`, hence, in some photos it was not clear wheter the photos belong to the class `glasses` or `no_glasses`","2e4982bb":"## Below we can see `some images from the train_dataset`, as we can observe there are from the two desired classes:","978902ab":"### It seems the data is `imbalance` there are `much more glasses images than no glasses images`. On the other hand, `77 images` are not going to be used in the model because are `not clear`","40da2773":"### There are `two variable objects`. Divided between around `2.5 million of MobilNet` parameters which are `frozen`, and `1.2 thousend` of trainable parameter in the `Dense layer`","81b0ce8b":"## Glasess:","3e298810":"## This configuration allows the model to speed up the training process\u00b6","620a7596":"## In this project a `classification model` will be created in order to classify images of people with glasses and no glasses","afd2eb9f":"## 3.2) Rescale pixel values:\u00b6","b9c49acf":"### As it can be seen in the `training` and `test`samples the accuracy is really high, greater than 0.99, hence this `model is robust` and `works really well`","69f359e5":"### Firstly, the application `MobileNetV2` will be downloaded, which is going to be used as a base for the model. This is a way of performing transfer learning, which consists in using a training learning from a pre-trained network","f9fac55e":"## The `random seed` is going to be applied, it will be used along all the model. In this way we will be sure that the same random seed is applied when it is optional to be called","35cf0154":"## 3.4) Feature extraction","6864c7d8":"# 5) Prediction","c754ab2b":"### First of all, we should `freeze the convolutional base` created from the previous step, because it is going to be used as a `feature extractor`","c1b23dbf":"### This feature extractor converts the images from `160x160x3` to `5x5x1280`","52f42ad0":"## 4.4) Results","94d5d49d":"## The next model is based on Tensorflow\/Learn\/Tutorials\/Images\/Transfer learning and fine-tuning - https:\/\/www.tensorflow.org\/tutorials","1d4dfcb0":"# 3) Preparing base model:","712fb9dd":"### Data augmentation is an optional step which introduces several artificial observations to the training sample.\n### In this model we are going to introduce two data augmentations:\u00b6","e1a41f66":"## Firstly, all the images were changed from `png format to jpg format`, however, the updated folder `datacleaningglassesnoglasses` contains already this mentioned change","9fe86819":"## 3.1) Data augmentation\u00b6","4a207437":"## To this end several parameteres are going to be selected, firstly the `batch size equal to 32`, and the `image size (height and width) equal to 160`","b80e9af3":"## 4.1) Unfreeze the top layers of the model","87e1f237":"## 4.3) Training the model","d366bdaf":"# 2) Data processing:","98833738":"### `tf.keras.layers.Dense` is a layer that converts the features into a `single prediction`","341faaec":"## 3.3) Creating base model from MobileNet V2","ccf4d836":"## 4.2) Compile the model","10c0b167":"### Below we can find the results of the data augmentation:"}}