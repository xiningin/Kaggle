{"cell_type":{"79c5d00f":"code","2e988fd6":"code","c24da23d":"code","b8817fa0":"code","731d10cc":"code","95f8305a":"code","991e7973":"code","eccc4380":"code","b4271c1a":"code","ce7d21c7":"code","f0e06978":"code","2ac86330":"code","3b8111e8":"code","013bc7f6":"code","af57015f":"code","3725584c":"code","686f60a8":"code","0517cefa":"code","12e9fc4b":"code","1d599d99":"code","52ad78c1":"code","7f0f0510":"code","5744237d":"code","a75559f6":"code","c310544f":"code","c7d75ac6":"code","aa951c06":"code","ed72650b":"code","5979cf7f":"code","e0bccd88":"code","02cc41ac":"code","829eb619":"code","304db987":"markdown","58816c6a":"markdown","4040cbdf":"markdown","43c669a8":"markdown","335617cf":"markdown","773c35ff":"markdown","37c0134d":"markdown","9eda76ba":"markdown","50f18592":"markdown","7f6f1954":"markdown","a13f7d3a":"markdown","01e2e1ac":"markdown","d7a0fee3":"markdown"},"source":{"79c5d00f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ntrain = pd.read_csv('..\/input\/cars-train.csv')\nval = pd.read_csv('..\/input\/cars-test.csv')\n\ntrain.head()","2e988fd6":"train = train.drop(['car.id'], axis=1)\nval = val.drop(['car.id'], axis=1)","c24da23d":"x_train = train.drop(['class'], axis = 1)\ny_train = train['class']\nx_val = val.drop(['class'], axis = 1)\ny_val = val['class']","b8817fa0":"train.describe(include='all')","731d10cc":"def plot_all_hists_df(df):\n    size = df.shape[1]\n    col_num = 4\n    row_num = int(np.ceil(size\/4))\n    fig = plt.figure(figsize=(9,5))\n    for i, name in enumerate(df):\n        ax=fig.add_subplot(row_num,col_num,i+1)\n        plt.hist(list(df[name]))\n        ax.set_title(name)\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n    plt.show()\n\nplot_all_hists_df(train)\n","95f8305a":"plot_all_hists_df(val)","991e7973":"print('Total number of each times each category of class shows up training set:')\nprint(train['class'].value_counts())\nprint('*****')\nprint('Total number of each times each category of class shows up validation set:')\nprint(val['class'].value_counts())","eccc4380":"class_counts_train = [680, 216, 38, 36]\nclass_counts_val = [227, 72, 13, 12]","b4271c1a":"total_tuples = set()\nfor index, row in train.iterrows():\n    total_tuples.add((row[0],row[1],row[2],row[3],row[4],row[5]))\nprint('Total number of unique combinations of independent variables: {}'.format(len(total_tuples)))","ce7d21c7":"train_num = train.replace({'unacc': 0, 'acc':1, 'good':2, 'vgood': 3, 'low':0, 'med':1, 'high':2, 'vhigh':3, 'small':0, 'big':2, \"2\":0, \"3\":1, \"4\":2, \"more\":3, \"5more\":3})\nval_num = val.replace({'unacc': 0, 'acc':1, 'good':2, 'vgood': 3, 'low':0, 'med':1, 'high':2, 'vhigh':3, 'small':0, 'big':2, \"2\":0, \"3\":1, \"4\":2, \"more\":3, \"5more\":3})","f0e06978":"train_num.head()","2ac86330":"varXclass = list()\ndepvar_name = [('class X '+name) for name in train_num]\nname = [x for x in train_num]\ndepvar_name = depvar_name[:6]\nfor i in range(6): varXclass.append(list())\nfor index, row in train_num.iterrows():\n    for i in range(6):\n        varXclass[i].append('class: '+str(row[6])+' X '+name[i]+' '+str(row[i]))","3b8111e8":"for i, x in enumerate(varXclass): \n    print(depvar_name[i]+' unique combos: {}'.format(len(set(x))))\n    varXclass[i].sort()","013bc7f6":"def combos_to_hist(list_of_sets, names):\n    fig = plt.figure(figsize=(15,10))\n    num_cols = 4\n    num_rows = int(np.ceil(len(list_of_sets)\/4))\n    for i, sets in enumerate(list_of_sets):\n        ax=fig.add_subplot(num_rows,num_cols,i+1)\n        plt.hist(sets)\n        plt.xticks(fontsize = 12,rotation='vertical')\n        ax.set_title(names[i])\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n    plt.show()\n    \ncombos_to_hist(varXclass, depvar_name)","af57015f":"def freq_dict(in_list):\n    return {i:in_list.count(i) for i in set(in_list)}\n\nfreq_dict_list=[]\n\nfor combo in varXclass:\n    combo_freq_dict = freq_dict(combo)\n    for pair in combo_freq_dict:\n        norm = class_counts_train[int(pair[7])]\n        combo_freq_dict[pair] = combo_freq_dict[pair]\/norm\n    freq_dict_list.append(combo_freq_dict)\n\ndef dict_to_two_lists(input_dict):\n    key_list = []\n    value_list = []\n    for key, value in input_dict.items():\n        key_list.append(key)\n        value_list.append(value)\n    return key_list, value_list\n\ndef freq_dict_list_hist(input_list, names):\n    size = len(input_list)\n    col_num = 4\n    row_num = int(np.ceil(size\/4))\n    fig = plt.figure(figsize=(15,10))\n    for i, freq_dict in enumerate(input_list):\n        ax=fig.add_subplot(row_num,col_num,i+1)\n        key_list, value_list = dict_to_two_lists(freq_dict)\n        plt.bar(key_list, value_list)\n        plt.xticks(fontsize = 12,rotation='vertical')\n        ax.set_title(names[i])\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n    plt.show()\n    \nfreq_dict_list_hist(freq_dict_list, depvar_name)","3725584c":"x_train = pd.get_dummies(x_train)\ny_train = pd.get_dummies(y_train)\nx_val = pd.get_dummies(x_val)\ny_val = pd.get_dummies(y_val)\n\nx_train_num = train_num.drop(['class'], axis=1)\ny_train_num = train_num['class']\nx_val_num = val_num.drop(['class'], axis=1)\ny_val_num = val_num['class']","686f60a8":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\n\nclf = RandomForestClassifier(n_estimators = 500)\nclf.fit(x_train, y_train)\nprint(classification_report(y_train, clf.predict(x_train)))\nprint(classification_report(y_val, clf.predict(x_val)))\nprint(accuracy_score(y_val, clf.predict(x_val)))","0517cefa":"clf = RandomForestClassifier(n_estimators = 500)\nclf.fit(x_train_num, y_train_num)\nprint(classification_report(y_train_num, clf.predict(x_train_num)))\nprint(classification_report(y_val_num, clf.predict(x_val_num)))\nprint(accuracy_score(y_val_num, clf.predict(x_val_num)))","12e9fc4b":"from sklearn.naive_bayes import MultinomialNB\n\nclf = MultinomialNB()\nclf.fit(x_train, y_train_num)\nprint(classification_report(y_train_num, clf.predict(x_train)))\nprint(classification_report(y_val_num, clf.predict(x_val)))\nprint(accuracy_score(y_val_num, clf.predict(x_val)))","1d599d99":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression()\nclf.fit(x_train, y_train_num)\nprint(classification_report(y_train_num, clf.predict(x_train)))\nprint(classification_report(y_val_num, clf.predict(x_val)))\nprint(accuracy_score(y_val_num, clf.predict(x_val)))","52ad78c1":"from sklearn.neighbors import KNeighborsClassifier\n\nclf = KNeighborsClassifier()\nclf.fit(x_train, y_train)\nprint(classification_report(y_train, clf.predict(x_train)))\nprint(classification_report(y_val, clf.predict(x_val)))\nprint(accuracy_score(y_val, clf.predict(x_val)))","7f0f0510":"clf = KNeighborsClassifier()\nclf.fit(x_train_num, y_train_num)\nprint(classification_report(y_train_num, clf.predict(x_train_num)))\nprint(classification_report(y_val_num, clf.predict(x_val_num)))\nprint(accuracy_score(y_val_num, clf.predict(x_val_num)))","5744237d":"from xgboost import XGBClassifier\n\nclf = XGBClassifier(n_estimators = 1000)\nclf.fit(x_train_num, y_train_num)\nprint(classification_report(y_train_num, clf.predict(x_train_num)))\nprint(classification_report(y_val_num, clf.predict(x_val_num)))\nprint(accuracy_score(y_val_num, clf.predict(x_val_num)))","a75559f6":"n_range = []\nacc_score = []\nfor n in range(100, 3000, 100):\n    n_range.append(n)\n    clf = XGBClassifier(n_estimators = n)\n    clf.fit(x_train_num, y_train_num)\n    acc_score.append(accuracy_score(y_val_num, clf.predict(x_val_num)))\n\nplt.title(\"N estimators to accuracy\")\nplt.plot(n_range, acc_score, ls='-', marker='o', color='red', label='One-hot encoded')\nplt.legend()","c310544f":"from xgboost import XGBClassifier\n\nclf = XGBClassifier(n_estimators = 650)\nclf.fit(x_train_num, y_train_num)\nprint(classification_report(y_train_num, clf.predict(x_train_num)))\nprint(classification_report(y_val_num, clf.predict(x_val_num)))\nprint(accuracy_score(y_val_num, clf.predict(x_val_num)))","c7d75ac6":"test = pd.read_csv('..\/input\/cars-final-prediction.csv')\ntest.head()","aa951c06":"test_num = test.replace({'low':0, 'med':1, 'high':2, 'vhigh':3, 'small':0, 'big':2, \"2\":0, \"3\":1, \"4\":2, \"more\":3, \"5more\":3})\ntest_num.head()","ed72650b":"test_num['num_predictions']=clf.predict(test_num.drop(['car.id'], axis=1))","5979cf7f":"test_num['class']=test_num['num_predictions'].replace({0:'unacc', 1:'acc', 2:'good', 3:'vgood'})\ntest_num.head()","e0bccd88":"comp_output=test_num[['car.id', 'class']]\ncomp_output.head()","02cc41ac":"plt.hist(list(comp_output['class']))\ncomp_output['class'].value_counts()","829eb619":"comp_output.to_csv('cars-submission.csv', index=False)","304db987":"So all dependent variables seem to be relatively evenly distributed and the target variable is heavily skewed towards unacceptable.  Having such evenly distributed independent variables but skewed dependent variables does not look good at first glance but it could also work out well.  Also, there are only 4\\*4\\*4\\*3\\*3\\*3\\*3\\*4 = 5184 total combinations of *all* variables and only only 1728 total combinations of all independent variables.  Since we have 970 total rows of data, if each row has a unique combination of dependent variables and each unique combination to dependent variables maps to a single unique independent variable, then a trained model should be able to achieve high accuracy on the validation and test sets due to the fact that we *know* for sure over half of all possible data points and there is no ambiguity in the data.  The below code calculates the total number of unique combinations of independent variables in our data.\n","58816c6a":"It looks pretty good.  Decently similar distribution to the test set with 'good' and 'vgood' slightly underrepresented.  Since those are the two with the least data, I'm guessing this model is misclassifying a few of them.","4040cbdf":"**Now histograms for our validation data**","43c669a8":"We'll need this to normalize a histogram later on, so below we save it to a list.","335617cf":"## UMUC DATA 650 Kaggle","773c35ff":"From these histograms, it looks like there is a relationship between our independent variables and the class variable.  However, the class variable is extremely skewed so it might be better to normalize the counts in the histogram by their respective target variable count.","37c0134d":"Next we pair the values of each variable with it's corresponding class value so that we can get an idea of how each variable affects class","9eda76ba":"LOL.  I was going to draft a few models in keras for fun but that just seems stupid now.  My intuition before starting this was that numerically encoding the categorical variables and applying an ensemble decision tree model (my guess was random forest but I guess gradient boosted takes the cake this time) would work well but 100% accuracy on the test set is pretty ridiculous.","50f18592":"From above, we can see there is no missing data to worry about.  Cool!","7f6f1954":"**Ouch.**  Let's hope there is a more distinct relationship between multiple combinations of the independent variables and their class.  Next we'll get into actually building and testing models.","a13f7d3a":"Since there are 970 total rows of data, the above tells us that all 970 rows of data for our independent variables (so, excluding the 'class' variable) are all unique.  So, despite the imbalanced data set it should be possible to achieve 100% accuracy on the training set just using decision trees--although such a model will likely be very overfitting.  It gives hope that we can get a decent, generalizable model from this data, however, especially since the histogram plots all looks relatively evenly distributed outside of the target variable.\n\nBelow we create a copy of our train\/validation dataframes where we use integers to order ever variables categories from least to most.  Since there is a natural ordering to *all* our categorical variables, we can preserve their relationship to each other by giving them integer values ranking them from lowest to highest.  This is done to give a natural ordering to our next histogram charts and also for model building as there are a ton of classifiers in sklearn that require ordinal variables for the target variable to even train.","01e2e1ac":"Welp.  I guess that's it boys.  Not sure how I'll do much better than that.  Guess I'll tune the parameters below just to maximize it before submitting.","d7a0fee3":"**Below are histograms for each variable in the training data**"}}