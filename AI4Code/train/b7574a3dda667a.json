{"cell_type":{"e153c707":"code","70ad491e":"code","270cff2e":"code","3c26da35":"code","a3434335":"code","0df8c10a":"code","5e8f1552":"code","27611c3b":"code","4292db93":"code","92efaeaf":"code","926092fb":"code","d6bcbde9":"code","3ddd4afe":"code","04165e88":"markdown","c298dbeb":"markdown","fe1a09a4":"markdown","f9ddad93":"markdown","071456cd":"markdown","1e64c2d4":"markdown","c3ce950a":"markdown","bde5fe24":"markdown","f50d68bc":"markdown","d32779c5":"markdown","d1d8a326":"markdown","82487996":"markdown","f04a515a":"markdown","c0796061":"markdown","e5924b58":"markdown","1a265360":"markdown","d41ff754":"markdown","805e8d59":"markdown"},"source":{"e153c707":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n# Import warnings\nimport warnings\n# filter warnings\nwarnings.filterwarnings('ignore')\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","70ad491e":"# Load data set\nx_l = np.load('..\/input\/Sign-language-digits-dataset\/X.npy')\ny_l = np.load('..\/input\/Sign-language-digits-dataset\/Y.npy')\nimg_size = 64  # pixel size\n\n # for sign zero\nplt.subplot(1,2,1)  \nplt.imshow(x_l[260])  # Get 260th index\nplt.axis(\"off\")\n\n# for sign one\nplt.subplot(1,2,2)\nplt.imshow(x_l[900])  # Get 900th index\nplt.axis(\"off\")","270cff2e":"# From 0 to 204 zero sign, from 205 to 410 is one sign\nX = np.concatenate((x_l[204:409], x_l[822:1027] ), axis=0)\n\n# We will create their labels. After that, we will concatenate on the Y.\nz = np.zeros(205)\no = np.ones(205)\nY = np.concatenate((z,o), axis=0).reshape(X.shape[0],1)\nprint(\"X shape: \", X.shape)\nprint(\"Y shape: \", Y.shape)","3c26da35":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.2, random_state=42)\n# random_state = Use same seed while randomizing\nprint(x_train.shape)\nprint(y_train.shape)","a3434335":"x_train_flatten = x_train.reshape(x_train.shape[0], x_train.shape[1] * x_train.shape[2])\nx_test_flatten = x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2])\nprint('x_train_flatten: {} \\nx_test_flatten: {} '.format(x_train_flatten.shape, x_test_flatten.shape))","0df8c10a":"# Here we will change the location of our samples and features. '(328,4096) -> (4096,328)' \nx_train = x_train_flatten.T\nx_test = x_test_flatten.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"x train: \", x_train.shape)\nprint(\"x test: \", x_test.shape)\nprint(\"y train: \", y_train.shape)\nprint(\"y test: \", y_test.shape)","5e8f1552":"# Now let's create the parameter and sigmoid function. \n# So what we need is dimension 4096 that is number of pixel as a parameter for our initialize method(def)\n\ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension, 1), 0.01)\n    b = 0.0\n    return w,b\n\n# Sigmoid function\n# z = np.dot(w.T, x_train) +b\ndef sigmoid(z):\n    y_head = 1\/(1 + np.exp(-z))  # sigmoid function finding formula\n    return y_head\nsigmoid(0)  # o should result in 0.5","27611c3b":"w,b = initialize_weights_and_bias(4096)\nprint(w)\nprint(\"----------\")\nprint(b)","4292db93":"# In backward propagation we will use y_head that found in forward progation\n# Therefore instead of writing backward propagation method, lets combine forward propagation and backward propagation\ndef forward_backward_propagation(w, b, x_train, y_train):\n    # Forward propagation\n    z = np.dot(w.T, x_train) + b\n    y_head = sigmoid(z)\n    loss = -(1 - y_train) * np.log(1 - y_head) - y_train * np.log(y_head)\n    cost = (np.sum(loss)) \/ x_train.shape[1]   # x_train.shape[1] is for scaling\n    \n    # Backward propagation\n    derivative_weight = (np.dot(x_train, ((y_head - y_train).T))) \/ x_train.shape[1] # x_train.shape[1] is for \n    derivative_bias = np.sum(y_head - y_train) \/ x_train.shape[1] # x_train.shape[1] is for     \n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    return cost, gradients","92efaeaf":"def update(w, b, x_train, y_train, learning_rate, number_of_iteration):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # updating(learning) parameters is number_of_iteration times\n    for i in range(number_of_iteration):\n        # make forward and backward propagation and find cost gradients\n        cost, gradients = forward_backward_propagation(w, b, x_train, y_train)\n        cost_list.append(cost)  # adding costs to cost_list\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]        \n        if i % 10 == 0:\n            cost_list2.append(cost)   # adding costs to cost_list2\n            index.append(i) # Adds a cost to the index in every 10 steps\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n        \n        # we update (learn) parameters weights and bias\n    \n    parameters = {\"weight\": w, \"bias\":b}\n    plt.plot(index, cost_list2)\n    plt.xticks(index, rotation='vertical')\n    plt.xlabel(\"Number of iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","926092fb":"# Let's create prediction parameter\n\ndef predict(w,b,x_test):\n    # x_test is a input for forward prapagation\n    z = sigmoid(np.dot(w.T, x_test) +b)\n    y_prediction = np.zeros((1, x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head = 1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head = 0),      \n    for i in range(z.shape[1]):\n        if z[0,i] <= 0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n    return y_prediction\n","d6bcbde9":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate, n_iterations):\n    # Initialize\n    dimension = x_train.shape[0]   # that is 4096\n    w, b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate, n_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"], parameters[\"bias\"], x_test)\n    y_prediction_train = predict(parameters[\"weight\"], parameters[\"bias\"], x_train) \n    \n    # print train \/ test errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))    \n\nlogistic_regression(x_train, y_train, x_test, y_test, learning_rate = 0.01, n_iterations = 170)","3ddd4afe":"from sklearn import linear_model\nlr_sl = linear_model.LogisticRegression(random_state=42, max_iter = 150)\n\nprint(\"test accuracy: {}\".format(lr_sl.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)))","04165e88":"* We need our model to be able to prediction\n* We need x_test to make prediction.","c298dbeb":"Now lets put them all together.","fe1a09a4":"<a id=\"0\"><\/a> <br>\n## 1) Introduction\n* We will be working on this kernel Sign Language data. We'll introduce 80% of the sign language we have, and we will try to predict the remaining 20%.\n* In this Kernel we will do the Logistic Regression step by step. Then we will learn how to do it very easily with Sklearn.\n* Let's start by creating our libraries\n","f9ddad93":"<a id=\"5\"><\/a> <br>\n## 6) Prediction Parameter","071456cd":"<a id=\"1\"><\/a> <br>\n## 2) Preparing Dataset\n---\n* Now we'll upload our library and then let's see the 0 and 1 signs we'll work on.","1e64c2d4":"  * Since our data in X is 3D, we need to flatten it to 2D to use Deep Learning.\n  * Since our data in Y is 2D, we don't need to flatten.","c3ce950a":"<a id=\"6\"><\/a> <br>\n##  7) Logistic Regression\n","bde5fe24":"* Now we reserve 80% of the values as 'train' and 20% as 'test'.\n* Then let's create x_train, y_train, x_test, y_test arrays\n\n","f50d68bc":"<a id=\"8\"><\/a> <br>\n> # CONCLUSION \n* If you want a more detailed kernel. Check out DATAI TEAM's Deep Learning Tutorial for Beginners Kernel.  https:\/\/www.kaggle.com\/kanncaa1\/deep-learning-tutorial-for-beginners\n---\n<br> **Thank you for your votes and comments.**                                                                                                                                             \n<br>**If you have any suggest, May you write for me, I will be happy to hear it.**","d32779c5":"* Now we will concatenate our pictures consisting of 0 and 1.\n* We have image of 255 one sign, 255 zero sign","d1d8a326":"# Cihan Yatbaz\n###  29 \/ 11 \/ 2018\n\n\n\n1.  [Introduction:](#0)\n2. [Preparing Dataset :](#1)\n3. [Creating Parameters :](#2)\n4. [Forward and Backward Propagation  :](#3)\n5. [Updating Parameter :](#4)\n6. [Prediction Parameter :](#5)\n7. [ Logistic Regression :](#6)\n8. [Logistec Regression with Sklearn  :](#7)\n9. [CONCLUSION :](#8)","82487996":"* The shape of the X is (410, 64, 64)\n    * 410 means that we have 410 images (zero and one signs)\n    * 64 means that our image size is 64x64 (64x64 pixels)\n* The shape of the Y is (410,1)\n    * 410 means that we have 410 labels (0 and 1)","f04a515a":"<a id=\"4\"><\/a> <br>\n## 5) Updating(Learning) Parameters\n---\n* Now let's apply Updating Parameter \n","c0796061":"<a id=\"7\"><\/a> <br>\n## 8) Logistec Regression with Sklearn","e5924b58":"* With the Sklearn library, we can find the result you found above in a much easier way.","1a265360":"<a id=\"3\"><\/a> <br>\n## 4) Forward and Backward Propagation\n* To reduce our cost function, we create parameters w, b.\n    * b = b - learning_rate(gradient of b) \n    * w = w - learning_rate(gradient of w) \n* Our cost function is equal to the sum of the losses of each image.\n* To reduce losses, we need to update our cost with the Gradient Descent Method.\n* To do this, we'll create our Forward propagation and Backward propagation parameters.\n\n   \n","d41ff754":"* Now x and y 2D\n* 4096 = 64 * 64","805e8d59":"<a id=\"2\"><\/a> <br>\n## 3) Creating Parameters\n\n* Parameters are weight and bias.\n* Our parameters are \"w\" and \"b\" standing for \"Weights\" and \"Bias\"\n* z = (w.t)x + b => z equals to (transpose of weights times input x) + bias\n* In an other saying => z = b + px1w1 + px2w2 + ... + px4096*w4096\n* y_head = sigmoid(z)\n* Sigmoid function makes z between zero and one so that is probability.\n"}}