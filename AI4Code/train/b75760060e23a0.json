{"cell_type":{"052c4b7f":"code","678d5246":"code","d3260e5c":"code","7d67a7fa":"code","f90f759b":"code","d7078114":"code","2aee12b3":"code","3edb5d05":"code","c673451a":"code","db2e93a5":"code","cb24c82f":"code","e6e71320":"code","9cb19d13":"code","1b24c917":"code","9724c931":"code","50a6e09b":"code","9dc65841":"code","06697d4f":"code","b3eb3a1f":"code","7be42a09":"code","20abeb76":"code","6a42dc79":"code","a1410397":"code","90e6c9f7":"code","1374c49d":"code","15912437":"markdown","accd55d1":"markdown","101cb423":"markdown","750e4b1d":"markdown","335039f5":"markdown","0898fed1":"markdown","3c450357":"markdown","cd735434":"markdown","ef0eb4fa":"markdown","740b3c49":"markdown","884604e3":"markdown","58025f1d":"markdown","887887f2":"markdown","ef9a576f":"markdown","6f3754b4":"markdown","2c6301e3":"markdown","7d446a71":"markdown","e81e8a73":"markdown","ba08510b":"markdown","912cc767":"markdown","e29e6750":"markdown","665ea7ec":"markdown","efd63ccf":"markdown","fa4f5950":"markdown","f1c86afa":"markdown","024c192e":"markdown","4b970a93":"markdown","ffb59567":"markdown","05a6dd62":"markdown"},"source":{"052c4b7f":"from sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nimport random\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm\n\nimport plotly.graph_objects as go\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, accuracy_score, roc_curve\nfrom lightgbm import LGBMClassifier","678d5246":"df_train = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv').drop('sig_id', axis=1)\ndf_target = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv').drop('sig_id', axis=1)","d3260e5c":"GC = [col for col in df_train if ('g-' in col) | ('c-' in col)]","7d67a7fa":"rocs = {}\nfeature_importance = {}\n\nfor target in tqdm(df_target.columns):\n    \n    X = df_train.loc[df_target[target]==1, GC]\n    y = df_train.loc[df_target[target]==1, 'cp_dose']\n    lenc = LabelEncoder()\n    y = lenc.fit_transform(y)\n    \n    if (len(X)>= 10) & (len(np.unique(y)) == 2):\n        kf = StratifiedKFold(n_splits=5)\n        preds = []\n        feat_imp = []\n        yy = []\n        for train_ind, test_ind in kf.split(X,y):\n            \n            Xtrain, Xtest, ytrain, ytest = X.iloc[train_ind], X.iloc[test_ind], y[train_ind], y[test_ind]\n            model = LGBMClassifier(n_estimators=100)\n            model.fit(Xtrain, ytrain)\n            p = model.predict_proba(Xtest)[:,1]\n            preds.append(p)\n            yy.append(ytest)\n            feat_imp.append(model.feature_importances_)\n            \n        yy = np.hstack(yy)\n        preds = np.hstack(preds)\n        rocs[target] = roc_auc_score(yy, preds)\n        feature_importance[target] = np.mean(feat_imp,axis=0)","f90f759b":"rocs_serie = pd.Series(rocs).sort_values(ascending = False)\nfig = go.Figure(\n    go.Bar(\n        x = rocs_serie.index,\n        y = rocs_serie.values\n    )\n)\nfig.update_layout(template = 'presentation', title = 'AUC Score')\nfig.show()","d7078114":"import plotly.express as px\nfrom sklearn.decomposition import PCA, KernelPCA\n\nacc_cols = rocs_serie[rocs_serie>0.8].index\nfi_dose = pd.DataFrame(feature_importance, index = GC)\nfi_dose = fi_dose[acc_cols]\n\nfor col in fi_dose.columns:\n    \n    fig = go.Figure(\n        go.Bar(\n            x = fi_dose.index,\n            y = fi_dose[col]\n        )\n    )\n    fig.update_layout(template = 'presentation', title = col, height = 300)\n    fig.show()","2aee12b3":"top_fi = fi_dose.copy()\ntop_fi[top_fi<=10] = 0\ntop_fi[top_fi!=0] = 1\ntop_fi = top_fi.sum(axis=1)\/len(top_fi.columns)\ntop_fi = top_fi[top_fi>0].sort_values(ascending = False)[:40]\nfig = go.Figure(\n    go.Bar(\n        x = top_fi.index,\n        y = top_fi.values\n    )\n)\nfig.update_layout(template = 'presentation', title = 'Top Features to predict cp_dose', height = 300)\nfig.show()","3edb5d05":"X = df_train.loc[df_train.cp_type != 'trt_cp', GC]\ny = df_train.loc[df_train.cp_type != 'trt_cp', 'cp_dose']\nlenc = LabelEncoder()\ny = lenc.fit_transform(y)\n\ntarget = 'no_target'\nif (len(X)> 10) & (len(np.unique(y)) == 2):\n\n    Xtrain, Xtest, ytrain, ytest = train_test_split(X,y, test_size = 0.2, stratify = y)\n    model = LGBMClassifier(n_estimators=100)\n    model.fit(Xtrain, ytrain)\n    p = model.predict_proba(Xtest)[:,1]\n    roc = roc_auc_score(ytest, p)\n    \nprint(roc)","c673451a":"print(roc_auc_score(ytest, p))\nprint(accuracy_score(ytest, np.round(p)))\n\nfi = pd.Series(model.feature_importances_, index = GC)\nfi = fi[fi>10] # keep only high importance features\nfi = fi.sort_values(ascending = False)\nfig = go.Figure(\n    go.Bar(\n        x = fi.index,\n        y = fi.values\n    )\n)\nfig.update_layout(template = 'presentation', title = 'feature importance for cp_type != trt_cp')\nfig.show()","db2e93a5":"# Remove high feature importances\nbfi = fi[fi>10].index\nGC2 = [elmt for elmt in GC if elmt not in bfi]\nprint(len(GC))\nprint(len(GC2))","cb24c82f":"rocs = {}\n\nfeature_importance = {}\n\nfor target in tqdm(df_target.columns):\n    \n    X = df_train.loc[df_target[target]==1, GC2]\n    y = df_train.loc[df_target[target]==1, 'cp_dose']\n    lenc = LabelEncoder()\n    y = lenc.fit_transform(y)\n    \n    if (len(X)>= 10) & (len(np.unique(y)) == 2):\n        kf = StratifiedKFold(n_splits=5)\n        preds = []\n        feat_imp = []\n        yy = []\n        for train_ind, test_ind in kf.split(X,y):\n            \n            Xtrain, Xtest, ytrain, ytest = X.iloc[train_ind], X.iloc[test_ind], y[train_ind], y[test_ind]\n            model = LGBMClassifier(n_estimators=100)\n            model.fit(Xtrain, ytrain)\n            p = model.predict_proba(Xtest)[:,1]\n            preds.append(p)\n            yy.append(ytest)\n            feat_imp.append(model.feature_importances_)\n            \n        yy = np.hstack(yy)\n        preds = np.hstack(preds)\n        rocs[target] = roc_auc_score(yy, preds)\n        feature_importance[target] = np.mean(feat_imp,axis=0)","e6e71320":"rocs_serie = pd.Series(rocs).sort_values(ascending = False)\nfig = go.Figure(\n    go.Bar(\n        x = rocs_serie.index,\n        y = rocs_serie.values\n    )\n)\nfig.update_layout(template = 'presentation', title = 'AUC Score')\nfig.show()","9cb19d13":"rep = df_target[rocs_serie.index].sum(axis=1).apply(lambda x:1 if x>0 else 0)\nrep = rep[df_train.cp_type == 'trt_cp']\nrep = pd.DataFrame(rep.groupby(rep).count()).rename(columns = {0: 'count'})\nrep['names'] = ['with AUC < 0.8', 'with AUC > 0.8']\nfig = px.pie(rep, values = 'count', names='names')\nfig.update_layout(title = 'Repartion of samples by AUC for target cp_dose', template = 'presentation')","1b24c917":"import plotly.express as px\nfrom sklearn.decomposition import PCA, KernelPCA\n\nacc_cols = rocs_serie[rocs_serie>0.8].index\nfi_dose = pd.DataFrame(feature_importance, index = GC2)\nfi_dose = fi_dose[acc_cols]\n\nfor col in fi_dose.columns:\n    \n    fig = go.Figure(\n        go.Bar(\n            x = fi_dose.index,\n            y = fi_dose[col]\n        )\n    )\n    fig.update_layout(template = 'presentation', title = col, height = 300)\n    fig.show()","9724c931":"top_fi = fi_dose.copy()\ntop_fi[top_fi<=10] = 0\ntop_fi[top_fi!=0] = 1\ntop_fi = top_fi.sum(axis=1)\/len(top_fi.columns)\ntop_fi = top_fi[top_fi>0].sort_values(ascending = False)[:40]\nfig = go.Figure(\n    go.Bar(\n        x = top_fi.index,\n        y = top_fi.values\n    )\n)\nfig.update_layout(template = 'presentation', title = '% of time a feature is important for the remaining targets', height = 300)\nfig.show()","50a6e09b":"X = df_train.loc[df_train.cp_type != 'trt_cp', GC]\ny = df_train.loc[df_train.cp_type != 'trt_cp', 'cp_time']\nlenc = LabelEncoder()\ny = lenc.fit_transform(y)\n\ntarget = 'no_target'\nif (len(X)> 10) & (len(np.unique(y)) == 3):\n    Xtrain, Xtest, ytrain, ytest = train_test_split(X,y, test_size = 0.2, stratify = y)\n    model = LGBMClassifier()\n    model.fit(Xtrain, ytrain)\n    p = model.predict_proba(Xtest)\n    roc = roc_auc_score(ytest, p, multi_class = 'ovr')\n    feature_importance = model.feature_importances_\n    \nprint(roc)","9dc65841":"fi = pd.Series(model.feature_importances_, index = GC)\nfi = fi.sort_values(ascending = False)\nfig = go.Figure(\n    go.Bar(\n        x = fi.index,\n        y = fi.values\n    )\n)\nfig.show()","06697d4f":"# Remove high feature importances\nbfi = fi[fi>10].index\nGC2 = [elmt for elmt in GC if elmt not in bfi]\nprint(len(GC))\nprint(len(GC2))","b3eb3a1f":"rocs = {}\n\nfeature_importance = {}\n\nfor target in tqdm(df_target.columns):\n    \n    X = df_train.loc[df_target[target]==1, GC2]\n    y = df_train.loc[df_target[target]==1, 'cp_time']\n    lenc = LabelEncoder()\n    y = lenc.fit_transform(y)\n    \n    if (len(X)>= 15) & (len(np.unique(y)) == 3):\n        kf = StratifiedKFold(n_splits=5)\n        preds = []\n        feat_imp = []\n        yy = []\n        for train_ind, test_ind in kf.split(X,y):\n            \n            Xtrain, Xtest, ytrain, ytest = X.iloc[train_ind], X.iloc[test_ind], y[train_ind], y[test_ind]\n            model = LGBMClassifier(n_estimators=100)\n            model.fit(Xtrain, ytrain)\n            p = model.predict_proba(Xtest)\n            preds.append(p)\n            yy.append(ytest)\n            feat_imp.append(model.feature_importances_)\n            \n        yy = np.hstack(yy)\n        preds = np.vstack(preds)\n        rocs[target] = roc_auc_score(yy, preds, multi_class='ovr')\n        feature_importance[target] = np.mean(feat_imp,axis=0)","7be42a09":"rocs_serie = pd.Series(rocs).sort_values(ascending = False)\nfig = go.Figure(\n    go.Bar(\n        x = rocs_serie.index,\n        y = rocs_serie.values\n    )\n)\nfig.update_layout(template = 'presentation', title = 'AUC Score')\nfig.show()","20abeb76":"import plotly.express as px\nfrom sklearn.decomposition import PCA, KernelPCA\n\nacc_cols = rocs_serie[rocs_serie>0.8].index\nfi_time = pd.DataFrame(feature_importance, index = GC2)\nfi_time = fi_time[acc_cols]\n\nfor col in fi_time.columns:\n    \n    fig = go.Figure(\n        go.Bar(\n            x = fi_time.index,\n            y = fi_time[col]\n        )\n    )\n    fig.update_layout(template = 'presentation', title = col, height = 300)\n    fig.show()","6a42dc79":"top_fi = fi_time.copy()\ntop_fi[top_fi<=10] = 0\ntop_fi[top_fi!=0] = 1\ntop_fi = top_fi.sum(axis=1)\/len(top_fi.columns)\ntop_fi = top_fi[top_fi>0].sort_values(ascending = False)[:40]\nfig = go.Figure(\n    go.Bar(\n        x = top_fi.index,\n        y = top_fi.values\n    )\n)\nfig.update_layout(template = 'presentation', title = '% of time a feature is important for the remaining targets', height = 300)\nfig.show()","a1410397":"print(f'total number of targets for target = dose : {len(fi_dose.columns)}')\nprint(fi_dose.columns)\nprint('\\n')\nprint(f'total number of targets for target = time : {len(fi_time.columns)}')\nprint(fi_time.columns)\nprint('\\n')\ncommon = list(set(fi_dose.columns).intersection(set(fi_time.columns)))\nprint('\\n')\nprint('targets in common in both sets:')\nprint(common)","90e6c9f7":"for col in common:\n    fig = go.Figure()\n    fig.add_trace(\n        go.Bar(\n            x = fi_dose.index,\n            y = fi_dose[col],\n            name = 'dose'\n        )   \n    )\n    fig.add_trace(\n        go.Bar(\n            x = fi_time.index,\n            y = fi_time[col],\n            name = 'time'\n        )   \n    )\n    fig.update_layout(template = 'presentation', height = 300, title = col)\n    fig.show()","1374c49d":"for target in fi_dose.columns:\n    try:\n        plt.figure(figsize = (25,5))\n        plt.subplot(1,2,1)\n        plt.title(target+ 'importance > 10')\n        test = fi_dose[target]\n        features = test[test>10].index\n\n        sub = df_train.loc[(df_target[target]==1), features]\n        pca = PCA(2)\n        pca.fit(sub)\n\n        \n        for d in df_train.cp_dose.unique():\n            sub_df = df_train.loc[(df_target[target]==1) & (df_train.cp_dose==d), features]\n            xpca = pca.transform(sub_df)\n            plt.scatter(xpca[:,0],xpca[:,1], label = d)\n        plt.legend()\n        plt.subplot(1,2,2)\n        test = fi_dose[target]\n        features = test[test>5].index\n\n        sub = df_train.loc[(df_target[target]==1), features]\n        pca = PCA(2)\n        pca.fit(sub)\n\n        plt.title(target + 'importance > 5')\n        for d in df_train.cp_dose.unique():\n            sub_df = df_train.loc[(df_target[target]==1) & (df_train.cp_dose==d), features]\n            xpca = pca.transform(sub_df)\n            plt.scatter(xpca[:,0],xpca[:,1], label = d)\n        plt.legend()\n        plt.show()\n    except:\n        pass","15912437":"#### Feature Importance","accd55d1":"So it appears here that the featuress g-307 and g-370 are actually very important, even when there is no actual drugs in the samples.\nWe will rerun the analysis on the targets an remove those \"obvious\" targets","101cb423":"### Predict cp_time for real targets","750e4b1d":"### Trying to predict each target separately\n\nIn this part, I take only samples related to one target, and I try to predict the dose based on the other training features","335039f5":"#### What features comes back the more ?\n\nWe see here that after removing the \"obvious\" features, the frequency of apparition of top features is much lower, which mean that each target has its own feature importance.\nThose might be well related to the the target themselves","0898fed1":"#### Feature importance (ctr_vehicle)","3c450357":"#### Removing features","cd735434":"#### Model Training","ef0eb4fa":"### No target (cp_type == ctr_vehicle)","740b3c49":"#### Feature importance","884604e3":"#### How many samples are included in that batch of high AUC? \n\nThe below figure shows that this analysis might be worth for more than 60% of the total dataset","58025f1d":"### Active targets\n\nNow that we removed features good at predicting **ctr_vehicle** samples, we try to predict cp_dose for the samples of each target separately","887887f2":"Let's visualise the PCA of the features selected by our cp_dose classifier for each target","ef9a576f":"### Removing \"obvious\" features\n\nFrom the figure above, we see that some features are presents for all targets.\nTo make sure that we keep only features relevant to each target, we run the same approach on all samples tagged as **cp_type = ctr_vehicle** and we will remove the active features from our training set","6f3754b4":"#### Feature importance\n\nWe select the sets of samples with ROC > 0.8 and plot the feature importance.\nWe notice that the important features are not the same for each target, giving a potential clue that these are the features reacting to the target.","2c6301e3":"# Using this method to remove outliers ?","7d446a71":"#### Top feature importance","e81e8a73":"## Predict cp_dosage","ba08510b":"#### AUC Scores","912cc767":"## Are the features extracted the same ?","e29e6750":"#### ROC","665ea7ec":"## Are the same targets detected ?\n\nVariations in **cp_dosage** seems easier to predict compared to **cp_time**\nOn the other end, we see that targets with high ROC score for **cp_time** are all included in targets with high ROC for **cp_dosage**","efd63ccf":"#### Feature importance for targets with AUC > 0.8","fa4f5950":"## Predict cp_time\n\nWe restart as above, but this time trying to predict the cp_time feature. I start directly by removing \"obvious\" targets for cp_time","f1c86afa":"#### Removing features","024c192e":"#### ROCs\n\nAbout 30 set of samples reach a ROC > 0.8. For those samples, it is possible to predict correctly cp_dosage based only on the other training_features ","4b970a93":"#### What features comes back the more ?\n\nWe see here that after removing the \"obvious\" features, the frequency of apparition of top features is much lower, which mean that each target has its own feature importance.\nThose might be well related to the the target themselves","ffb59567":"# Introduction\n\nThis notebook aims at exploring the possibilities offers to make feature selection based on the features cp_dose and cp_time.\n\nThe idea is that for a given target, dose and time shall be somehow correlated to active features for that target.","05a6dd62":"#### Models training"}}