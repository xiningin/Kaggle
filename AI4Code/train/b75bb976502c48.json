{"cell_type":{"b66c2819":"code","79c3a455":"code","366b04f2":"code","76d5569a":"code","2c8bcb5a":"code","ccea46a7":"code","89280d8e":"code","66760f38":"code","97442129":"code","79db1191":"markdown","ee45ed58":"markdown","53f8b64f":"markdown","157e4d22":"markdown","4763cb3c":"markdown"},"source":{"b66c2819":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import preprocessing as pp\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn import metrics\n\ncolor = sns.color_palette()","79c3a455":"PATH_TO_FILE = \"\/kaggle\/input\/psykose\/schizophrenia-features.csv\"\n\n_PARAMS_LORGREG = {\n    \"penalty\": \"l2\", \"C\": 1.0, \"class_weight\": \"balanced\",\n    \"random_state\": 2018, \"solver\": \"liblinear\", \"n_jobs\": 1\n}\n\n_PARAMS_RFC = {\n    \"n_estimators\": 10,\n    \"max_features\": \"auto\", \"max_depth\": None,\n    \"min_samples_split\": 2, \"min_samples_leaf\": 1,\n    \"min_weight_fraction_leaf\": 0.0,\n    \"max_leaf_nodes\": None, \"bootstrap\": True,\n    \"oob_score\": False, \"n_jobs\": -1, \"random_state\": 2018,\n    \"class_weight\": \"balanced\"\n}\n\n_PARAMS_XGB = {\n    \"nthread\":16, \"learning_rate\": 0.3, \"gamma\": 0, \"max_depth\": 6, \"verbosity\": 0,\n    \"min_child_weight\": 1, \"max_delta_step\": 0, \"subsample\": 1.0, \"colsample_bytree\": 1.0,\n    \"objective\":\"binary:logistic\", \"num_class\":1, \"eval_metric\":\"logloss\", \"seed\":2018,\n}\n\n_PARAMS_LIGHTGB = {\n    \"task\": \"train\", \"num_class\":1, \"boosting\": \"gbdt\", \"verbosity\": -1,\n    \"objective\": \"binary\", \"metric\": \"binary_logloss\", \"metric_freq\":50, \"is_training_metric\":False,\n    \"max_depth\":4, \"num_leaves\": 31, \"learning_rate\": 0.01, \"feature_fraction\": 1.0, \"bagging_fraction\": 1.0,\n    \"bagging_freq\": 0, \"bagging_seed\": 2018, \"num_threads\":16\n}","366b04f2":"data = pd.read_csv(PATH_TO_FILE)\n\ndataX = data.copy().drop([\"class\", \"class_str\", \"userid\"], axis=1)\ndataY = data[\"class\"].copy()\n\nscaler = pp.StandardScaler(copy=True)\n\ndataX.loc[:, dataX.columns] = scaler.fit_transform(dataX[dataX.columns])\n\nX_TRAIN, X_TEST, Y_TRAIN, Y_TEST = train_test_split(\n    dataX,\n    dataY,\n    test_size=0.10,\n    random_state=2019,\n    stratify=dataY\n)","76d5569a":"def plot_prc_curve(y_preds, y_trues, title=None):\n    \n    precision, recall, _ = metrics.precision_recall_curve(\n        y_trues,\n        y_preds\n    )\n\n    average_precision = metrics.average_precision_score(\n        y_trues,\n        y_preds\n    )\n\n    print(\"Average Precision = %.2f\" % average_precision)\n    \n    plt.step(recall, precision, color=\"k\", alpha=0.7, where=\"post\")\n    plt.fill_between(recall, precision, step=\"post\", alpha=0.3, color=\"k\")\n\n    if title is None:\n        title = \"PRC: Average Precision = %.2f\" % average_precision\n\n    plt.title(title)\n\n    plt.xlabel(\"Recall\")\n    plt.ylabel(\"Precision\")\n    plt.ylim([0.0, 1.05])\n    plt.xlim([0.0, 1.0])\n\n    plt.show()\n\ndef plot_roc_curve(y_preds, y_trues, title=None):\n    \n    fpr, tpr, _ = metrics.roc_curve(y_trues, y_preds)\n\n    auc_roc = metrics.auc(fpr, tpr)\n\n    print(\"AUCROC = %.2f\" % auc_roc)\n\n    if title is None:\n        title = \"AUCROC = %.2f\" % auc_roc\n\n    plt.title(title)\n\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel(\"FPR\")\n    plt.ylabel(\"TPR\")\n    \n    plt.plot(fpr, tpr, color=\"r\", lw=2, label=\"ROC curve\")\n    plt.plot([0, 1], [0, 1], color=\"k\", lw=2, linestyle=\"--\")\n\n    plt.show()\n\n\ndef model_predict_k_fold( train_func, pred_func, model=None, n_splits=10, shuffle=True, random_state=2018 ):\n     \n    y_preds = []\n    y_trues = []\n\n    k_fold = StratifiedKFold(\n        n_splits=n_splits,\n        shuffle=shuffle,\n        random_state=random_state\n    )\n\n    for train_index, fold_index in k_fold.split(np.zeros(len(X_TRAIN)), Y_TRAIN.ravel()):\n        \n        x_fold_train, x_fold_test = X_TRAIN.iloc[train_index, :], X_TRAIN.iloc[fold_index, :]\n        y_fold_train, y_fold_test = Y_TRAIN.iloc[train_index], Y_TRAIN.iloc[fold_index]\n\n        model = train_func( model, x_fold_train, y_fold_train, x_fold_test, y_fold_test )\n        y_pred = pred_func( model, x_fold_test )\n\n        y_preds.extend( list(y_pred) )\n        y_trues.extend( list(y_fold_test) )\n\n    return model, np.array(y_preds), np.array(y_trues)\n","2c8bcb5a":"def logreg_train_func(model, x_train, y_train, x_test, y_test):\n    model.fit(x_train, y_train)\n    return model\n\ndef logreg_pred_func(model, data):\n    return model.predict_proba(data)[:, 1]\n\nlogreg = LogisticRegression( **_PARAMS_LORGREG )\n\nlogreg, logreg_y_preds, logreg_y_trues = model_predict_k_fold( logreg_train_func, logreg_pred_func, logreg )\nlogreg_test_preds = logreg_pred_func( logreg, X_TEST )\n\nplot_prc_curve( logreg_y_preds, logreg_y_trues, \"LogReg Cross-Vaidation PRC\" )\nplot_roc_curve( logreg_y_preds, logreg_y_trues, \"LogReg Cross-Vaidation ROC\" )\n\nplot_prc_curve( logreg_test_preds, Y_TEST, \"LogReg Testing PRC\" )\nplot_roc_curve( logreg_test_preds, Y_TEST, \"LogReg Testing ROC\" )","ccea46a7":"def rfc_train_func(model, x_train, y_train, x_test, y_test):\n    model.fit(x_train, y_train)\n    return model\n\ndef rfc_pred_func(model, data):\n    return model.predict_proba(data)[:, 1]\n\nrfc = RandomForestClassifier( **_PARAMS_RFC )\n\nrfc, rfc_y_preds, rfc_y_trues = model_predict_k_fold( rfc_train_func, rfc_pred_func, rfc )\nrfc_test_preds = rfc_pred_func( rfc, X_TEST )\n\nplot_prc_curve( rfc_y_preds, rfc_y_trues, \"RFC Cross-Vaidation PRC\" )\nplot_roc_curve( rfc_y_preds, rfc_y_trues, \"RFC Cross-Vaidation ROC\" )\n\nplot_prc_curve( rfc_test_preds, Y_TEST, \"RFC Testing PRC\" )\nplot_roc_curve( rfc_test_preds, Y_TEST, \"RFC Testing ROC\" )","89280d8e":"def xgb_train_func(model, x_train, y_train, x_test, y_test):\n    dtrain = xgb.DMatrix(data=x_train, label=y_train)\n\n    bst = xgb.cv(_PARAMS_XGB,\n        dtrain,\n        num_boost_round=2000,\n        nfold=5,\n        early_stopping_rounds=200,\n        verbose_eval=50\n    )\n\n    best_rounds = np.argmin(bst[\"test-logloss-mean\"])\n    bst = xgb.train(_PARAMS_XGB, dtrain, best_rounds)\n    return bst\n\ndef xgb_pred_func(model, data):\n    data = xgb.DMatrix(data=data)\n    pred = model.predict(data)\n    return pred\n\nxgb_model, xgb_y_preds, xgb_y_trues = model_predict_k_fold( xgb_train_func, xgb_pred_func )\nxgb_test_preds = xgb_pred_func( xgb_model, X_TEST )\n\nplot_prc_curve( xgb_y_preds, xgb_y_trues, \"XGB Cross-Vaidation PRC\" )\nplot_roc_curve( xgb_y_preds, xgb_y_trues, \"XGB Cross-Vaidation ROC\" )\n\nplot_prc_curve( xgb_test_preds, Y_TEST, \"XGB Testing PRC\" )\nplot_roc_curve( xgb_test_preds, Y_TEST, \"XGB Testing ROC\" )","66760f38":"def gbm_train_func(model, x_train, y_train, x_test, y_test):\n    lgb_train = lgb.Dataset(x_train, y_train)\n    lgb_eval = lgb.Dataset(x_test, y_test, reference=lgb_train)\n\n    gbm = lgb.train(\n        _PARAMS_LIGHTGB,\n        lgb_train,\n        verbose_eval=False,\n        num_boost_round=2000,\n        valid_sets=lgb_eval,\n        early_stopping_rounds=200\n    )\n\n    return gbm\n\ndef gbm_pred_func(model, data):\n    return model.predict(data, num_iteration=model.best_iteration)\n\ngbm, gbm_y_preds, gbm_y_trues = model_predict_k_fold( gbm_train_func, gbm_pred_func )\ngbm_test_preds = gbm_pred_func( gbm, X_TEST )\n\nplot_prc_curve( gbm_y_preds, gbm_y_trues, \"GBM Cross-Vaidation PRC\" )\nplot_roc_curve( gbm_y_preds, gbm_y_trues, \"GBM Cross-Vaidation ROC\" )\n\nplot_prc_curve( gbm_test_preds, Y_TEST, \"GBM Testing PRC\" )\nplot_roc_curve( gbm_test_preds, Y_TEST, \"GBM Testing ROC\" )","97442129":"def ensemble_train_func(model, x_train, y_train, x_test, y_test):\n    lgb_train = lgb.Dataset(x_train, y_train)\n    lgb_eval = lgb.Dataset(x_test, y_test, reference=lgb_train)\n\n    ensemble = lgb.train(\n        _PARAMS_LIGHTGB,\n        lgb_train,\n        verbose_eval=False,\n        num_boost_round=2000,\n        valid_sets=lgb_eval,\n        early_stopping_rounds=200\n    )\n\n    return ensemble\n\ndef ensemble_pred_func(model, data):\n    return model.predict(data, num_iteration=model.best_iteration)\n\nensemble, ensemble_y_preds, ensemble_y_trues = model_predict_k_fold( ensemble_train_func, ensemble_pred_func )\nensemble_test_preds = ensemble_pred_func( ensemble, X_TEST )\n\nplot_prc_curve( ensemble_y_preds, ensemble_y_trues, \"Ensemble Cross-Vaidation PRC\" )\nplot_roc_curve( ensemble_y_preds, ensemble_y_trues, \"Ensemble Cross-Vaidation ROC\" )\n\nplot_prc_curve( ensemble_test_preds, Y_TEST, \"Ensemble Testing PRC\" )\nplot_roc_curve( ensemble_test_preds, Y_TEST, \"Ensemble Testing ROC\" )","79db1191":"# XGBoost","ee45ed58":"# Logistic Regression","53f8b64f":"# LightGBM","157e4d22":"# Random Forest","4763cb3c":"# Ensemble"}}