{"cell_type":{"cdcb3de9":"code","a3499fe7":"markdown"},"source":{"cdcb3de9":"from gensim.models import ldamodel\nimport gensim.corpora;\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer;\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n\n\ndef load_data(filename):\n    reviews = list()\n    labels = list()\n    with open(filename, encoding='utf-8') as file:\n        file.readline()\n        for line in file:\n            line = line.strip().split(' ',1)\n            labels.append(line[0])\n            reviews.append(line[1])\n\n    return reviews\n\ndef display_topics(model, feature_names, no_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        print (\"Topic %d:\" % (topic_idx))\n        print (\" \".join([feature_names[i]\n                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n\nf = open('..\/input\/corpuscomentarios\/corpuscomentarios.csv', 'r')\ndata = f.readlines()\nf.close()\n        \nno_features = 1000\n\ntf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\ntf = tf_vectorizer.fit_transform(data)\ntf_feature_names = tf_vectorizer.get_feature_names()\n\nq_topicos = 10\n\nlda = LatentDirichletAllocation(n_components=q_topicos, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n\nno_top_words = 8\n\ndisplay_topics(lda, tf_feature_names, no_top_words)\n ","a3499fe7":"# An\u00e1lisis por LDA de comentarios de edutubers de julioprofe\n\nEl **topic modeling** es una t\u00e9cnica no supervisada de NLP, capaz de detectar y **extraer de manera autom\u00e1tica relaciones sem\u00e1nticas latentes** de grandes vol\u00famenes de informaci\u00f3n.\n\nEstas relaciones son los llamados t\u00f3picos, que son un conjunto de palabras que suelen aparecer juntas en los mismos contextos y nos permiten observar relaciones que ser\u00edamos incapaces de observar a simple vista.\n\nExisten diversas t\u00e9cnicas que pueden ser usadas para obtener estos t\u00f3picos. El principal algoritmo y que adem\u00e1s ser\u00e1 el que utilizaremos en esta publicaci\u00f3n, es el modelo **latent dirichlet allocation (LDA)**, propuesto por David Blei, que nos devuelve por un lado los diferentes t\u00f3picos que componen la colecci\u00f3n de documentos y por otro lado cu\u00e1nto de cada t\u00f3pico est\u00e1 presente en cada documento. Los t\u00f3picos consisten en una distribuci\u00f3n de probabilidades de aparici\u00f3n de las distintas palabras del vocabulario.\n\nPaper base: https:\/\/www.jmlr.org\/papers\/volume3\/blei03a\/blei03a.pdf"}}