{"cell_type":{"c509dbbe":"code","4f9f3d7d":"code","0c8bf820":"code","7fa9b6a1":"code","5e8dc02a":"code","0a722107":"code","8f6d834a":"code","93d1bcce":"code","4fba7047":"code","0aad4aea":"code","8b99a641":"code","291868ae":"code","0177e8db":"code","70d41d15":"code","2502445e":"code","46b18184":"code","1cfc0e67":"code","240601d3":"code","53e781e6":"code","258cbd92":"code","1576f6ba":"code","8953c227":"code","299292c5":"code","c5ed2cda":"code","5ef187ad":"code","a2229bbb":"code","19d25221":"markdown","2177bae1":"markdown","469abec6":"markdown","e47b0987":"markdown","079b4c05":"markdown","c0ccdc3b":"markdown","1801e1fd":"markdown","3d4156a9":"markdown","2a0df282":"markdown","c4f2a711":"markdown","988ce87f":"markdown","9c4a30ee":"markdown","0b5d6fdc":"markdown","7f552a57":"markdown","1c783cfe":"markdown","42241379":"markdown","af2a2a64":"markdown","82fae931":"markdown","addf7f48":"markdown","a5f1b5fe":"markdown","4be83468":"markdown"},"source":{"c509dbbe":"#!pip install yfinance","4f9f3d7d":"%%capture\nimport umap\nimport pandas as pd\nimport numpy as np\n# import yfinance as yf\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom glob import glob\nfrom joblib import Parallel, delayed\nfrom sklearn.manifold import TSNE, SpectralEmbedding\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\n%config InlineBackend.figure_format = 'retina'\nsns.set_theme('notebook', 'white', font_scale=1.2, palette='tab10')","0c8bf820":"data_dir = '\/kaggle\/input\/optiver-realized-volatility-prediction'\ndf_files = pd.DataFrame({'book_path': glob(f'{data_dir}\/book_train.parquet\/**\/*.parquet')}) \\\n    .assign(stock_id=lambda x: x.book_path.str.extract(\"stock_id=(\\d+)\").astype('int')) \\\n    .sort_values('stock_id')\ndf_target_train = pd.read_csv(f'{data_dir}\/train.csv')\ndf_volatility_train = df_target_train.groupby('time_id').target.mean()","7fa9b6a1":"def rmspe(y_true, y_pred):\n    return ((((y_true - y_pred) \/ y_true)) ** 2).mean() ** 0.5\n\ndef plot_price(stock_id, time_id, price_name, kind, ax):\n    r = df_files.query(f'stock_id == {stock_id}').iloc[0]\n    df = pd.read_parquet(r.book_path, columns=['time_id', 'seconds_in_bucket', price_name])\n    df = df.query(f'time_id == {time_id}').drop(columns='time_id').set_index('seconds_in_bucket').reindex(np.arange(600), method='ffill')\n    min_diff = np.nanmin(abs(df[price_name].diff().where(lambda x: x > 0)))\n    if kind == 'price_norm':\n        df[price_name].plot.line(legend=False, ax=ax)\n        ax.set_title(f'stock_id={stock_id}, time_id={time_id}: {price_name} normalized')\n    elif kind == 'price_change':\n        df = df[price_name].diff().reset_index()\n        df.plot.bar(x='seconds_in_bucket', y=price_name, color=np.where(df[price_name] > 0, 'g', 'r'), legend=False, edgecolor='none', width=1, ax=ax)\n        ax.set_title(f'stock_id={stock_id}, time_id={time_id}: {price_name} change')\n        ax.yaxis.set_major_locator(mpl.ticker.MultipleLocator(min_diff))\n    elif kind == 'ticks_change':\n        df = df[price_name].diff().div(min_diff).reset_index()\n        df.plot.bar(x='seconds_in_bucket', y=price_name, color=np.where(df[price_name] > 0, 'g', 'r'), legend=False, edgecolor='none', width=1, ax=ax)\n        ax.set_title(f'stock_id={stock_id}, time_id={time_id}: {price_name} change (ticks)')\n        ax.yaxis.set_major_locator(mpl.ticker.MultipleLocator(1))\n    elif kind == 'price_original':\n        df[price_name] = 0.01 \/ min_diff * df[price_name]\n        df[price_name].plot.line(legend=False, ax=ax)\n        ax.set_title(f'stock_id={stock_id}, time_id={time_id}: {price_name} original')\n\n    ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(30))\n    ax.xaxis.set_tick_params(rotation=0)\n    ax.set_axisbelow(True)\n    ax.grid(axis='y', linestyle='--')\n    ax.set_xlim(0, 600)\n    \ndef plot_emb(emb, color, name, kind='volatility', fig=None, ax=None):\n    if fig is None or ax is None:\n        fig, ax = plt.subplots(figsize=(7, 7))\n    if kind == 'volatility':\n        norm = mpl.colors.LogNorm()\n        ticks = mpl.ticker.LogLocator(2)\n        formatter = mpl.ticker.ScalarFormatter()\n    elif kind == 'date':\n        norm = None\n        ticks = None\n        formatter = mpl.dates.AutoDateFormatter(mpl.dates.MonthLocator())\n    plot = ax.scatter(emb[:, 0], emb[:, 1], s=3, c=color, edgecolors='none', cmap='jet', norm=norm);\n    divider = make_axes_locatable(ax)\n    cax = divider.append_axes('right', size='5%', pad=0.2)    \n    cb = fig.colorbar(plot, label=kind, format=formatter,\n                      ticks=ticks, cax=cax)\n    cb.ax.minorticks_off()\n    ax.set_title(f'{name}')","5e8dc02a":"f, ax = plt.subplots(4, 1, sharex=True, figsize=(13, 16))        \nplot_price(1, 16, 'ask_price1', 'price_norm', ax[0])\nplot_price(1, 16, 'ask_price1', 'price_change', ax[1])\nplot_price(1, 16, 'ask_price1', 'ticks_change', ax[2])\nplot_price(1, 16, 'ask_price1', 'price_original', ax[3])","0a722107":"def calc_price(df):\n    diff = abs(df.diff())\n    min_diff = np.nanmin(diff.where(lambda x: x > 0))\n    n_ticks = (diff \/ min_diff).round()\n    return 0.01 \/ np.nanmean(diff \/ n_ticks)\n\ndef calc_prices(r):\n    df = pd.read_parquet(r.book_path, columns=['time_id', 'ask_price1', 'ask_price2', 'bid_price1', 'bid_price2'])\n    df = df.groupby('time_id').apply(calc_price).to_frame('price').reset_index()\n    df['stock_id'] = r.stock_id\n    return df","8f6d834a":"df_prices_denorm = pd.concat(Parallel(n_jobs=-1, verbose=0)(delayed(calc_prices)(r) for _, r in df_files.iterrows()))","93d1bcce":"df_prices_denorm = df_prices_denorm.pivot('time_id', 'stock_id', 'price')\ndf_prices_denorm","4fba7047":"plt.figure(figsize=(15, 20))\nax = sns.stripplot(data=df_prices_denorm, orient='h', alpha=0.3, s=2, jitter=0.2,\n                   order=df_prices_denorm.median().sort_values().index[::-1].tolist(), \n                   palette='Spectral')\nax.tick_params(axis='y', which='major', labelsize=10)\nplt.xlabel('price')\nplt.title('Denormalized price distribution by stock');","0aad4aea":"# SP100_tickers = pd.read_html('https:\/\/en.wikipedia.org\/wiki\/S%26P_100')[2].Symbol\n# df_prices_real = yf.download(SP100_tickers.to_list(), start='2020-01-01', end='2021-06-01', interval='1h')\ndf_prices_real = pd.read_hdf('\/kaggle\/input\/orvp-dataset\/prices_real.hdf', key='prices')","8b99a641":"df_volatility_real = 1 \/ 2 * np.log(df_prices_real.High \/ df_prices_real.Low) ** 2 - \\\n    (2 * np.log(2) - 1) * np.log(df_prices_real.Close \/ df_prices_real.Open) ** 2\ndf_volatility_real = df_volatility_real.mean(axis=1)\n\ndf_prices_real = df_prices_real.Open.fillna(df_prices_real.Open.mean()).dropna(axis=1).sample(frac=1)\ndf_volatility_real = df_volatility_real.loc[df_prices_real.index]\ndf_prices_real","291868ae":"df_prices_denorm_scaled = df_prices_denorm.fillna(df_prices_denorm.mean())\ndf_prices_denorm_scaled = pd.DataFrame(minmax_scale(df_prices_denorm_scaled), index=df_prices_denorm.index)\n\ndf_prices_real_scaled = df_prices_real.fillna(df_prices_real.mean())\ndf_prices_real_scaled = pd.DataFrame(minmax_scale(df_prices_real_scaled), index=df_prices_real.index)","0177e8db":"f, ax = plt.subplots(1, 2, figsize=(14, 6))        \nemb = PCA(n_components=2)\nemb_denorm = emb.fit_transform(df_prices_denorm_scaled)\nemb_real = emb.fit_transform(df_prices_real_scaled)\nplot_emb(emb_denorm, df_volatility_train, 'denormalized prices', 'volatility', f, ax[0])\nplot_emb(emb_real, df_volatility_real, 'real prices', 'volatility', f, ax[1])\nf.suptitle('PCA embeddings')\nplt.tight_layout()","70d41d15":"f, ax = plt.subplots(1, 2, figsize=(14, 6))        \nemb = TSNE(n_components=2, perplexity=40, learning_rate=50, \n           verbose=1, init='pca', n_iter=2000,\n           early_exaggeration=12)\nemb_denorm = emb.fit_transform(df_prices_denorm_scaled)\nemb_real = emb.fit_transform(df_prices_real_scaled)\nplot_emb(emb_denorm, df_volatility_train, 'denormalized prices', 'volatility', f, ax[0])\nplot_emb(emb_real, df_volatility_real, 'real prices', 'volatility', f, ax[1])\nf.suptitle('TSNE embeddings')\nplt.tight_layout()","2502445e":"f, ax = plt.subplots(1, 2, figsize=(14, 6))        \nemb = umap.UMAP(n_neighbors=60, min_dist=0.1, target_metric='euclidean', \n                init='spectral',  low_memory=False, verbose=True, \n                spread=0.5, local_connectivity=1, repulsion_strength=1, \n                negative_sample_rate=5)\nemb_denorm = emb.fit_transform(df_prices_denorm_scaled)\nemb_real = emb.fit_transform(df_prices_real_scaled)\nplot_emb(emb_denorm, df_volatility_train, 'denormalized prices', 'volatility', f, ax[0])\nplot_emb(emb_real, df_volatility_real, 'real prices', 'volatility', f, ax[1])\nf.suptitle('UMAP embeddings')\nplt.tight_layout()","46b18184":"f, ax = plt.subplots(1, 2, figsize=(14, 6))        \nemb = SpectralEmbedding(random_state=2)\nemb_denorm = emb.fit_transform(df_prices_denorm_scaled)\nemb_real = emb.fit_transform(df_prices_real_scaled)\nplot_emb(emb_denorm, df_volatility_train, 'denormalized prices', 'volatility', f, ax[0])\nplot_emb(emb_real, df_volatility_real, 'real prices', 'volatility', f, ax[1])\nf.suptitle('Spectral embeddings')\nplt.tight_layout()","1cfc0e67":"plot_emb(emb_real, [mpl.dates.date2num(i) for i in df_volatility_real.index], 'real prices', 'date')","240601d3":"df_prices_denorm_ordered = df_prices_denorm.iloc[np.argsort(-emb_denorm[:, 0])]","53e781e6":"df_prices_denorm_ordered.reset_index(drop=True).rolling(10).mean(). \\\n    plot(subplots=True, layout=(-1, 5), figsize=(15, 60), sharex=True, lw=1)\nplt.suptitle('Denormalized prices in recovered time order')\nplt.subplots_adjust(top=0.97, wspace=0.3);","258cbd92":"df_prices_real.plot(subplots=True, layout=(-1, 5), figsize=(15, 60), sharex=True, lw=1);\nplt.xticks([])\nplt.suptitle('Real prices')\nplt.subplots_adjust(top=0.97, wspace=0.3);","1576f6ba":"_, ax = plt.subplots(1, 1, figsize=(15, 5))\ndf_prices_real['AMZN'].sort_index().to_frame().set_index(np.linspace(0, 1, len(df_prices_real))).plot(lw=1, ax=ax)\ndf_prices_denorm_ordered[61].rolling(10).mean().to_frame().set_index(np.linspace(0.02, 0.86, len(df_prices_denorm_ordered))).plot(lw=1, ax=ax);","8953c227":"_, ax = plt.subplots(1, 1, figsize=(15, 5))\ndf_prices_real['GE'].sort_index().to_frame().set_index(np.linspace(0, 1, len(df_prices_real))).plot(lw=1, ax=ax)\ndf_prices_denorm_ordered[31].rolling(10).mean().to_frame().set_index(np.linspace(0, 0.88, len(df_prices_denorm_ordered))).plot(lw=1, ax=ax);","299292c5":"def calc_rv(r):\n    df = pd.read_parquet(r.book_path)\n    df['wap'] = (df.ask_price1 * df.bid_size1 + df.bid_price1 * df.ask_size1) \/ (df.ask_size1 + df.bid_size1)\n    df = df.groupby('time_id').wap.apply(lambda x: (np.log(x).diff() ** 2).sum() ** 0.5).reset_index()\n    df.rename(columns={'wap': 'rv'}, inplace=True)\n    df['stock_id'] = r.stock_id\n    return df\n    \ndf_rv_train = pd.concat(Parallel(n_jobs=-1, verbose=0)(\n        delayed(calc_rv)(r) for _, r in df_files.iterrows()))","c5ed2cda":"nn = NearestNeighbors(algorithm='brute')\nnn.fit(df_prices_denorm_scaled)\nnn_ind = nn.kneighbors(df_prices_denorm_scaled, n_neighbors=10, return_distance=False)\n\nresults = []\nfor nn_count in range(1, 11):\n    time_ind = df_target_train.time_id.factorize()[0]\n    stock_ind = df_target_train.stock_id.factorize()[0]\n    df_train = df_rv_train.pivot('time_id', 'stock_id', 'rv')\n    \n    # a bit of advanced indexing to gather nearest rv\n    df_train = pd.DataFrame(df_train.values[\n            nn_ind[time_ind, :nn_count],\n            stock_ind[:, None]], \n        index=df_target_train.index)\n    df_train.rename(lambda x: f'rv_{x}', axis=1, inplace=True)\n    df_train.fillna(df_train.mean(), inplace=True)\n\n    model = LinearRegression()\n    scores = cross_val_score(model, df_train, df_target_train.target, \n                             groups=df_target_train.time_id, \n                             scoring=make_scorer(rmspe),\n                             fit_params={'sample_weight': 1 \/ df_target_train.target ** 2})\n    \n    results.append({'nn_count': nn_count, 'score': scores.mean()})\n    \ndf_results = pd.DataFrame(results)\nax = df_results.plot(x='nn_count', y='score', style='-C0o', figsize=(7, 5), legend=False)\nbest = df_results.iloc[df_results.score.argmin()]\nplt.plot(best.nn_count, best.score, 'C3o', markersize=10)\nplt.annotate(f'{best.score:.5f}', xy=(best.nn_count, best.score), xytext=(0, 10), \n             fontsize=12, textcoords='offset points', ha='center')\nax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(1))\nplt.ylabel('CV score')\nplt.title('How many nearest time_ids to take');","5ef187ad":"nn_count = 6\n\ntime_ind = df_target_train.time_id.factorize()[0]\nstock_ind = df_target_train.stock_id.factorize()[0]\ndf_train = df_rv_train.pivot('time_id', 'stock_id', 'rv')\n\ndf_train = pd.DataFrame(df_train.values[\n        nn_ind[time_ind, :nn_count],\n        stock_ind[:, None]], \n    index=df_target_train.index)\ndf_train.rename(lambda x: f'rv_{x}', axis=1, inplace=True)\ndf_train.fillna(df_train.mean(), inplace=True)\n\nmodel = LinearRegression()\nmodel.fit(df_train, df_target_train.target, sample_weight=1 \/ df_target_train.target ** 2)\n\nplt.bar(x=df_train.columns, height=model.coef_);\nplt.title('Feature weights');","a2229bbb":"df_files = pd.DataFrame({'book_path': glob(f'{data_dir}\/book_test.parquet\/**\/*.parquet')}) \\\n    .assign(stock_id=lambda x: x.book_path.str.extract(\"stock_id=(\\d+)\").astype('int')) \\\n    .sort_values('stock_id')\n\ndf_submission = pd.read_csv(f'{data_dir}\/test.csv')\n\ndf_prices_denorm_test = pd.concat(Parallel(n_jobs=-1, verbose=0)(delayed(calc_prices)(r) for _, r in df_files.iterrows()))\ndf_prices_denorm_test = df_prices_denorm_test.pivot('time_id', 'stock_id', 'price')\ndf_prices_denorm_scaled_test = df_prices_denorm_test.fillna(df_prices_denorm_test.mean())\ndf_prices_denorm_scaled_test = pd.DataFrame(minmax_scale(df_prices_denorm_scaled_test), index=df_prices_denorm_test.index)\n\ndf_rv_test = pd.concat(Parallel(n_jobs=-1, verbose=0)(\n        delayed(calc_rv)(r) for _, r in df_files.iterrows()))\n\nif len(df_prices_denorm_scaled_test) == 1:\n    nn_ind = np.zeros((3, nn_count), dtype='int')\nelse:\n    nn = NearestNeighbors(algorithm='brute')\n    nn.fit(df_prices_denorm_scaled_test)\n    nn_ind = nn.kneighbors(df_prices_denorm_scaled_test, n_neighbors=10, return_distance=False)\n\ntime_ind = df_submission.time_id.factorize()[0]\nstock_ind = df_submission.stock_id.factorize()[0]\ndf_test = df_rv_test.pivot('time_id', 'stock_id', 'rv')\n\ndf_test = pd.DataFrame(df_test.values[\n        nn_ind[time_ind, :nn_count],\n        stock_ind[:, None]], \n    index=df_submission.index)\ndf_test.rename(lambda x: f'rv_{x}', axis=1, inplace=True)\ndf_test.fillna(df_test.mean(), inplace=True)\n\ndf_submission['target'] = model.predict(df_test)\ndf_submission = df_submission[['row_id', 'target']]\ndf_submission.to_csv('submission.csv', index=False)\ndf_submission.head()","19d25221":"Looks pretty reasonable - you can easily spot 2020 coronavirus crash with following upwards trend on many plots. \n\nLet's look at real prices for comparison:","2177bae1":"Now we'll train a few models varying number of neighours we provide as input to select an optimal number based on CV.","469abec6":"*Note: this idea is based on assumption that tick size is constant for all time_id\/stock_id and is equal to \\\\$0.01, otherwise the resulting prices might be different.*","e47b0987":"Cool! We can see, that spectral embedding indeed sorts all points by date. So we now can use X-coordinate of embeddings to approximately recover time order of time_ids in training data.","079b4c05":"To better understand the idea let's plot some charts for selected stock_id\/time_id:\n1. First chart is normalized to 1 price as it is in training dataset\n2. Second chart - is normalized price changes `price.diff()`. You can clearly see that they are all integer multiples of some real value.\n3. Third chart is just the same as second, but divided by min price change `price.diff() \/ min(price.diff())`, thus we get price changes in ticks. Here you can see it even better that price changes by integer number of ticks.\n4. Tha last chart is reconstructed price `price * 0.01 \/ min(price.diff())`","c0ccdc3b":"In the plot above nn_count=1 effectively means we use only one feature - current realized volatility as input, and you can see that providing realized volatility from neighbour time_ids greatly improves the score from *0.27* to *0.23*, and it plateaus after nn_count > 6.","1801e1fd":"PLB of this submission was **0.21379**","3d4156a9":"Finally let's train model using optimal nn_count=6 and use it for predicting on test.","2a0df282":"Poor General Electric, it really having a bad time in last years.\n\n<img src=\"https:\/\/user-images.githubusercontent.com\/4602302\/135102010-fb45f4d9-4d00-4cdd-b571-47dc17f84ac3.png\" width=\"50%\"\/>\n<img src=\"https:\/\/user-images.githubusercontent.com\/4602302\/135102047-67745529-ef46-4cd2-b784-f79b41504168.png\" width=\"50%\"\/>","c4f2a711":"Now let's recover original prices for each stock_id\/time_id and explore them.","988ce87f":"Well, there a few interesting insights from the plots above:\n1. There are definetely a lot of similarities between embeddings of denorm prices vs real prices - both shapes and colors. This can confirm that price denormalization works correctly.\n2. We indeed see long continuous segments as it was expected.\n3. There is a single outstanding cluster of high volatility, and as we know that real data includes period of [2020 stock market crash](https:\/\/en.wikipedia.org\/wiki\/2020_stock_market_crash) we can pretty confidently assume that this is the same cluster in training data.\n4. Spectral embeddings look very nice - there is single connected line and we can easily order all points from left to right.","9c4a30ee":"In this notebook I'm trying to recover time_id order based on denormalized prices using minimum tick size idea from [The Leak](https:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/discussion\/256725) discussion, and after use this information to improve model performance.\n\n**Update:**\n\nAs the discussion was deleted, I will briefly explain the idea here, though whole credit for finding it belongs to @nquay3. Actually it is quite simple and is based on 2 facts:\n1. Stock prices are changing not by arbitrarty real value but by multiples of minimum possible price change called *tick size*, which is usually equals to \\\\$0.01, though it might be different for certain stocks and exchanges.\n2. From the host [answer](https:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/discussion\/249752#1384739) we know that prices were normalized by dividing them by the price (WAP to be precise) at seconds_in_bucket=0 for each stock_id\/time_id. So tick size will also be divided by this price.\n\nThen if we find normalized tick size by taking minimum of all price changes for each stock_id\/time_id, and assuming real tick size=\\\\$0.01, we can restore original price as: \n\n$S_0=\\frac{0.01}{normalized\\_tick\\_size}$.","0b5d6fdc":"Let's also download some real stock prices for comparison from approximately the same period as training data","7f552a57":"Calculating real prices volatility from OLHC data using GARMAN-KLASS estimator (thanks @lucasmorin)","1c783cfe":"*TODO: Use [Dynamic Time Warping](https:\/\/tslearn.readthedocs.io\/en\/stable\/user_guide\/dtw.html) to match other stocks.*","42241379":"Here are recovered prices:","af2a2a64":"### Can we recover time_id order from denormalized prices?\n\n*I found this task an intersting exercise alongside with the main competition goal, I worked on it when I was stuck with other normal ideas.*\n\nWe can treat each time_id row as a point in a 112 dimensional space, where temporally close time_ids should be close in that space. Ideally, there should be a continuous 1d manifold\/curve. So lets try different dimensionality reduction methods on both denormalized and real prices for comparison.","82fae931":"From denormalized prices distribution plot we can see that the most expensive stock (61) prices are in range 1800-3500, which matches AMZN prices for 2020-2021(half). Let's plot both of them in a single plot.","addf7f48":"Another outstanding stock is 31 which has largest RMSPE and lowest price range 6-14. Looking through available real stocks charts we can notice that GE prices look suspiciously similar to it.","a5f1b5fe":"To confirm our hypothesis that spectral embedding indeed sorts points by date, let's show the same plot but colorcode it by date, as we know it for real prices.","4be83468":"### Can we utilize this additional information to improve our model score?\n\nI can't say I found the best way to utilize it with maximum gain, considering that this time ordering method while recovering global order might be quite noisy locally. But one simple idea could be to feed into our model features from nearest time_ids besides current one.\n\nLet's try it with the most basic linear model with one feature: current realized volatility."}}