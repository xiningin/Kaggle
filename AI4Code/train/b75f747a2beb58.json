{"cell_type":{"be444d02":"code","5f7a8eb7":"code","f509fa74":"code","72d56a3f":"code","57b9da9d":"code","78f52579":"code","3c791b8b":"code","15c9d0ce":"code","c3e643be":"code","23dfc2a8":"code","fdea01c4":"code","6ae200f3":"code","2a3cfd3b":"code","4c127ba6":"code","56787a37":"code","e205b09c":"code","4810d811":"code","eaceb762":"code","7dde7417":"code","ab13f6b4":"code","83221bff":"code","da60ab8d":"code","ddd7293c":"code","4ddfb9d8":"code","e6f44b71":"code","438dee32":"code","3b3e0552":"code","9ee0fdf1":"code","a4074127":"code","4f06611f":"code","1a298f87":"code","8a4bc92c":"code","3c58efc3":"code","264b1773":"code","f7583203":"code","38c8197e":"code","a552c9f0":"code","77e84d8c":"code","f0d7b83f":"code","f12eec8e":"code","3a109b94":"code","80aef21c":"code","85146ddb":"code","f29e6cf1":"code","147a9a4f":"code","9cf85bab":"code","fbac2210":"code","f0b100c3":"code","c935a492":"code","9da38125":"code","c9ac81f4":"code","c8bf9452":"code","a3b09897":"code","4b57bdc5":"code","7a0ce82a":"code","86a7ada5":"code","44592bff":"code","0924e5a7":"code","9e45f725":"code","e92c6133":"code","1f4bf87a":"code","eaefcd3d":"code","1cf5acc6":"code","87d63dfc":"code","640e6631":"code","7438aa83":"markdown","bde72b31":"markdown","fedc9dbd":"markdown","40d5ff3e":"markdown","a9516b26":"markdown","ef271ced":"markdown","4ec2e442":"markdown","564842c9":"markdown","abddfe7f":"markdown","5d7f6095":"markdown","ab298a7b":"markdown","0ffdf251":"markdown","fd1e73ac":"markdown","03e8c983":"markdown","3ead7d76":"markdown","5a3c6769":"markdown","adae481c":"markdown","c0c50bff":"markdown","9a54001d":"markdown","c066a65b":"markdown","eb566ece":"markdown","b85fe59f":"markdown","894cce7a":"markdown","a1c6edeb":"markdown","67ea01f1":"markdown","8abfd4fd":"markdown","c8d96e94":"markdown","a2ed1ed3":"markdown","51c1ee9e":"markdown","e02307c4":"markdown","7117e44b":"markdown","f8c6aa5f":"markdown","5b115c44":"markdown","cdc3e49d":"markdown","dbb4a6b3":"markdown"},"source":{"be444d02":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport statsmodels as stats\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import linear_model\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom statsmodels.formula.api import ols\nfrom sklearn.linear_model import LinearRegression","5f7a8eb7":"store = pd.read_csv(\"..\/input\/store.csv\", index_col  = 'Store')\ntrain = pd.read_csv(\"..\/input\/train.csv\", index_col  = 'Store')\nstore.info()\ntrain.info()\ntrain.head(5)","f509fa74":"%env JOBLIB_TEMP_FOLDER=\/tmp","72d56a3f":"train_store_joined = train.join(store)","57b9da9d":"# replacing null values with 0\ncleaned_data=train_store_joined.fillna(0)","78f52579":"cleaned_data.head(5)","3c791b8b":"cleaned_data.describe()","15c9d0ce":"# checking linearity beetween customers visiting store vs sales\nplt.scatter(cleaned_data['Customers'],cleaned_data['Sales'])\nplt.grid(True)\nplt.title('Customers Vs Sales', fontsize=14)\nplt.xlabel('Customers', fontsize=14)\nplt.ylabel('Sales', fontsize=14)\nplt.show()","c3e643be":"# The relationship between a store type and its respective assortment type\nStoretypeXAssortment = sns.countplot(x=\"StoreType\",hue=\"Assortment\",order=[\"a\",\"b\",\"c\",\"d\"],\n                                     data=cleaned_data,palette=sns.color_palette(\"Set2\", n_colors=3)).set_title(\"Number of Different Assortments per Store Type\")","23dfc2a8":"# when are the stores open during the week?\nax = sns.countplot(x='Open', hue='DayOfWeek', data=cleaned_data, palette='Set1')","fdea01c4":"cleaned_data['StateHoliday'] = cleaned_data.StateHoliday.replace('0', 0)\ncleaned_data = pd.get_dummies(cleaned_data, columns=['StateHoliday','StoreType','Assortment'], drop_first=True)\ncleaned_data.info()\ncleaned_data.head(5)","6ae200f3":"stats.graphics.gofplots.qqplot(cleaned_data['Promo'], line='r')\nstats.graphics.gofplots.qqplot(cleaned_data['Customers'], line='r')\nstats.graphics.gofplots.qqplot(cleaned_data['Promo2SinceYear'], line='r')\n","2a3cfd3b":"features = ['Customers','Open','Promo','SchoolHoliday','CompetitionDistance',\n            'CompetitionOpenSinceMonth','CompetitionOpenSinceYear',\n            'Promo2','Promo2SinceWeek','Promo2SinceYear','StateHoliday_a','StateHoliday_b',\n            'StateHoliday_c','StoreType_b','StoreType_c','StoreType_d','Assortment_b','Assortment_c','Sales']\n\nmask = np.zeros_like(cleaned_data[features].corr(), dtype=np.bool) \nmask[np.triu_indices_from(mask)] = True \n\nf, ax = plt.subplots(figsize=(16, 12))\nplt.title('Pearson Correlation Matrix',fontsize=25) \n\nsns.heatmap(cleaned_data[features].corr(),linewidths=0.25,vmax=1.0,square=True,cmap=\"YlGnBu\", \n            linecolor='w',annot=True,mask=mask,cbar_kws={\"shrink\": .75})","4c127ba6":"# defining generic fuctions used for building and evaluating model\ndef calculate_cv_error(X,Y):\n    \"\"\"\n    Calculates cross validation error of model\n    :param X: independent variables i.e predictors contributing model\n    :param Y: dependent variable i.e target in model\n    :return: float value returns mean squared error\n    \"\"\"\n    regr = linear_model.LinearRegression()\n    ms_errors= cross_val_score(regr, X, Y, cv=5, scoring = make_scorer(mean_squared_error))\n    rms_errors = np.sqrt(ms_errors)\n    mean_rms_error = rms_errors.mean()\/1000\n    return mean_rms_error\n\ndef build_OLS_model(features,Y):\n    \"\"\"\n    Build OLS linear regression model\n    :param features: independent variables i.e predictors needs to be included while building model\n    :param Y: dependent variable i.e target in model\n    :return: Linear regression model\n    \"\"\"\n    X = cleaned_data[features]\n    X = sm.add_constant(X)\n    model = sm.OLS(Y, X)\n    return model","56787a37":"# building model1\nfeatures = ['Customers','Open','Promo','SchoolHoliday','CompetitionDistance',\n            'CompetitionOpenSinceMonth','CompetitionOpenSinceYear',\n            'Promo2','Promo2SinceWeek','Promo2SinceYear','StateHoliday_a','StateHoliday_b',\n            'StateHoliday_c','StoreType_b','StoreType_c','StoreType_d','Assortment_b','Assortment_c']\nX = np.array(cleaned_data[features], dtype=pd.Series)\nY = np.array(cleaned_data['Sales'], dtype=pd.Series)\nprint(\"Cross Validation Error: \"+str(calculate_cv_error(X,Y)))\nY = cleaned_data['Sales']\nmodel1 = build_OLS_model(features,Y)\nresults1 = model1.fit()\nresults1.summary()","e205b09c":"# building model2\n# As CompetitionOpenSinceYear and CompetitionOpenSinceMonth are strongly correlated removing CompetitionOpenSinceMonth\n# As Promo2SinceWeek and Promo2SinceYear are strongly correlated removing Promo2SinceYear\n# As Promo2 and Promo2SinceWeek are strongly correlated removing Promo2\nfeatures = ['Customers','Open','Promo','SchoolHoliday','CompetitionDistance',\n            'CompetitionOpenSinceYear',\n            'Promo2SinceWeek','StateHoliday_a','StateHoliday_b',\n            'StateHoliday_c','StoreType_b','StoreType_c','StoreType_d','Assortment_b','Assortment_c']\nX = np.array(cleaned_data[features], dtype=pd.Series)\nY = np.array(cleaned_data['Sales'], dtype=pd.Series)\nprint(\"Cross Validation Error: \"+str(calculate_cv_error(X,Y)))\nY = cleaned_data['Sales']\nmodel2 = build_OLS_model(features,Y)\nresults2 = model2.fit()\nresults2.summary()","4810d811":"# building model3\n# As Open and StateHoliday are strongly correlated removing StateHoliday\nfeatures = ['Customers','Open','Promo','CompetitionDistance',\n            'CompetitionOpenSinceYear',\n            'Promo2SinceWeek','StoreType_b','StoreType_c','StoreType_d','Assortment_b','Assortment_c']\nX = np.array(cleaned_data[features], dtype=pd.Series)\nX = sm.add_constant(X)\nY = np.array(cleaned_data['Sales'], dtype=pd.Series)\nprint(\"Cross Validation Error: \"+str(calculate_cv_error(X,Y)))\nY = cleaned_data['Sales']\nmodel3 = build_OLS_model(features,Y)\nresults3 = model3.fit()\nresults3.summary()","eaceb762":"# building model4\n# After removing StateHoliday correlation term error increases so let go ahead with model2 predictores and add interaction term in it.\ncleaned_data[\"Promo2SinceWeek*Promo\"]=cleaned_data['Promo2SinceWeek']*cleaned_data['Promo']\ncleaned_data[\"CompetitionDistance*CompetitionOpenSinceYear\"]=cleaned_data['CompetitionDistance']*cleaned_data['CompetitionOpenSinceYear']\ncleaned_data[\"inf\"]=cleaned_data['Promo2SinceWeek']*cleaned_data['Promo']\nfeatures = ['Customers','Open','Promo','SchoolHoliday','CompetitionDistance',\"CompetitionDistance*CompetitionOpenSinceYear\",\n            'CompetitionOpenSinceYear',\"Promo2SinceWeek*Promo\",\n            'Promo2SinceWeek','StateHoliday_a','StateHoliday_b',\n            'StateHoliday_c','StoreType_b','StoreType_c','StoreType_d','Assortment_b','Assortment_c']\nX = np.array(cleaned_data[features], dtype=pd.Series)\nY = np.array(cleaned_data['Sales'], dtype=pd.Series)\nprint(\"Cross Validation Error: \"+str(calculate_cv_error(X,Y)))\nY = cleaned_data['Sales']\nmodel4 = build_OLS_model(features,Y)\nresults4 = model4.fit()\nresults4.summary()","7dde7417":"features = ['Sales','Customers','Open','Promo','SchoolHoliday','CompetitionDistance',\"CompetitionDistance*CompetitionOpenSinceYear\",\n            'CompetitionOpenSinceYear',\"Promo2SinceWeek*Promo\",\n            'Promo2SinceWeek','StateHoliday_a','StateHoliday_b',\n            'StateHoliday_c','StoreType_b','StoreType_c','StoreType_d','Assortment_b','Assortment_c']\n\ndataSet=cleaned_data[features]","ab13f6b4":"sales_mean= np.mean(dataSet['Sales'])","83221bff":"##Changing data into categorical by assigning value of sales above mean as 1 and below mean as 0\ndat_classification= dataSet\ndat_classification.loc[ dat_classification['Sales'] <= sales_mean, 'Sales'] = 0\ndat_classification.loc[ dat_classification['Sales'] > sales_mean, 'Sales'] = 1","da60ab8d":"dat_classificationY= dat_classification['Sales']\ndat_classificationX= dat_classification.drop(['Sales'],axis=1)","ddd7293c":"## Splitting the data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(dat_classificationX, dat_classificationY, test_size = 0.2,random_state=0)","4ddfb9d8":"from sklearn.ensemble import RandomForestClassifier  \nrandomForestCLassifier = RandomForestClassifier()  \nrandomForestCLassifier.fit(X_train, y_train) ","e6f44b71":"y_pred = randomForestCLassifier.predict(X_test) ","438dee32":"def classification_Parameters(y_test, y_pred):\n    from sklearn.metrics import confusion_matrix \n    from sklearn.metrics import accuracy_score \n    from sklearn.metrics import classification_report \n    results = confusion_matrix(y_test, y_pred) \n    print (\"Confusion Matrix :\")\n    print(results) \n    print ('Accuracy Score :',accuracy_score(y_test, y_pred))\n    print ('Report : ')\n    print (classification_report(y_test, y_pred)) ","3b3e0552":"classification_Parameters(y_test, y_pred)","9ee0fdf1":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\ndef RocCurve(model):\n    logit_roc_auc = roc_auc_score(y_test, model.predict(X_test))\n    fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])\n    plt.figure()\n    plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.savefig('Log_ROC')\n    plt.show()","a4074127":"RocCurve(randomForestCLassifier)","4f06611f":"from sklearn.model_selection import RandomizedSearchCV\nfrom pprint import pprint\n#Let us see what default parameters our model used\nprint('Default Parameters of :\\n')\npprint(randomForestCLassifier.get_params())","1a298f87":"from sklearn.model_selection import GridSearchCV\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [70, 100],\n    'max_features': [3, 5],\n    'min_samples_leaf': [2, 4],\n    'min_samples_split': [7, 10],\n    'n_estimators': [200, 400]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","8a4bc92c":"#Fit the grid search to the data\ngrid_search.fit(X_train, y_train)\ngrid_search.best_params_","3c58efc3":"optimizedRandom=RandomForestClassifier(n_estimators=200,max_depth=100, min_samples_split=10, min_samples_leaf=4, max_features=5,bootstrap=True)\noptimizedRandom.fit(X_train,y_train)\ny_pred = optimizedRandom.predict(X_test)  \nclassification_Parameters(y_test, y_pred)","264b1773":"RocCurve(optimizedRandom)","f7583203":"from xgboost import XGBClassifier\n","38c8197e":"boostClassModel= XGBClassifier()\nboostClassModel.fit(X_train, y_train)","a552c9f0":"y_pred = boostClassModel.predict(X_test) \nclassification_Parameters(y_test, y_pred)","77e84d8c":"RocCurve(boostClassModel)","f0d7b83f":"#Check Default parameters\nboostClassModel.get_params","f12eec8e":"learning_rates = [0.5, 0.75, 1]\nn_estimators=[20,30,40]\nmax_depths=[2,3,4]\nmax_features=[2,3,4]\nfor i in range(3):\n    xgb = XGBClassifier(n_estimators=n_estimators[i], learning_rate = learning_rates[i], max_features=max_features[i], max_depth = max_depths[i], random_state = 0)\n    xgb.fit(X_train, y_train)\n    print(\"Accuracy score (training): {0:.3f}\".format(xgb.score(X_train, y_train)))\n    print(\"Accuracy score (validation): {0:.3f}\".format(xgb.score(X_test, y_test)))\n    print()","3a109b94":"#So best is \n#learning_rates = 1\n#n_estimators=40\n#max_depths=4\n#max_features=4\nxgb_optimized = XGBClassifier(n_estimators=40, learning_rate = 1, max_features=4, max_depth = 4, random_state = 0)\nxgb_optimized.fit(X_train,y_train)","80aef21c":"y_pred = xgb_optimized.predict(X_test)  \nclassification_Parameters(y_test, y_pred)","85146ddb":"RocCurve(xgb_optimized)","f29e6cf1":"from sklearn.linear_model import LogisticRegression\nLRclassifier= LogisticRegression()\nLRclassifier.fit(X_train,y_train)\ny_pred = LRclassifier.predict(X_test)","147a9a4f":"classification_Parameters(y_test, y_pred)","9cf85bab":"RocCurve(LRclassifier)","fbac2210":"def regessionEvaluation(y_test, y_pred):  \n    from sklearn import metrics  \n    print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n    print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n    print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","f0b100c3":"data_regression=dataSet","c935a492":"y= data_regression['Sales']\ndataX= data_regression.drop(['Sales'],axis=1)\nX_train, X_test, y_train, y_test = train_test_split(dataX,Y, test_size=0.2, random_state=0) ","9da38125":"from sklearn.ensemble import RandomForestRegressor  \nmodelRegrRF = RandomForestRegressor()  \nmodelRegrRF.fit(X_train, y_train)  ","c9ac81f4":"y_pred = modelRegrRF.predict(X_test) ","c8bf9452":"regessionEvaluation(y_test,y_pred)","a3b09897":"#Let us see what default parameters our model used\nprint('Parameters currently in use:\\n')\npprint(modelRegrRF.get_params())","4b57bdc5":"from sklearn.model_selection import GridSearchCV\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4],\n    'min_samples_split': [8, 10],\n    'n_estimators': [100, 200]\n}\n# Create a based model\nrandomRegressor = RandomForestRegressor()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = randomRegressor, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\n","7a0ce82a":"# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)","86a7ada5":"#grid_search.best_params_","44592bff":"randomForestOptimized=RandomForestRegressor(n_estimators=200,max_depth=100, min_samples_split=10, min_samples_leaf=4, max_features=5,bootstrap=True)","0924e5a7":"randomForestOptimized.fit(X_train,y_train)\ny_pred = randomForestOptimized.predict(X_test)","9e45f725":"regessionEvaluation(y_test,y_pred)","e92c6133":"from xgboost import XGBRegressor","1f4bf87a":"xgBoostRegressor = XGBRegressor()\nxgBoostRegressor.fit(X_train, y_train)","eaefcd3d":"y_pred = xgBoostRegressor.predict(X_test) \nregessionEvaluation(y_test,y_pred)","1cf5acc6":"learning_rates = [0.5, 0.75, 1]\nn_estimators=[20,30,40]\nmax_depths=[2,3,4]\nmax_features=[2,3,4]\nfor i in range(3):\n    xgb = XGBRegressor(n_estimators=n_estimators[i], learning_rate = learning_rates[i], max_features=max_features[i], max_depth = max_depths[i], random_state = 0)\n    xgb.fit(X_train, y_train)\n    print(\"Accuracy score (training): {0:.3f}\".format(xgb.score(X_train, y_train)))\n    print(\"Accuracy score (validation): {0:.3f}\".format(xgb.score(X_test, y_test)))\n    print()","87d63dfc":"#So best is \n#learning_rates = 1\n#n_estimators=40\n#max_depths=4\n#max_features=4\nxgbRegressor_optimized = XGBRegressor(n_estimators=40, learning_rate = 1, max_features=4, max_depth = 4, random_state = 0)\nxgbRegressor_optimized.fit(X_train,y_train)","640e6631":"y_pred = xgbRegressor_optimized.predict(X_test) \nregessionEvaluation(y_test,y_pred)","7438aa83":"Gradient boosting is one of the most powerful techniques for building predictive models.\nHow Gradient Boosting Works\n**Gradient boosting involves three elements:**\n- A loss function to be optimized.\n- A weak learner to make predictions.\n- An additive model to add weak learners to minimize the loss function.\n\nGradient Boosting gets its name from Gradient Descent. Given the predetermined loss function, Gradient Descent is utilized to find the parameters which minimize this loss function. Initially, gradient descent uses some parameters to looks at each point along the loss function, and find the negative derivative of that point. As gradient descent continues along the loss function, it continuously tunes the parameters until the minimum point is found. The goal is to find the optimal parameters which have the biggest decrease on the loss function. This is how Gradient Boosting attempts to minimize error. By sequentially minimizing our loss function (meaning we are sequentially minimizing the amount of error with each weak learner), our model gets stronger and stronger until a final predictor is found.\n\n**XGBoosting**\nIn the realm of data science, machine learning algorithms, and model building, the ultimate goal is to build the strongest predictive model while accounting for computational efficiency as well. This is where XGBoosting comes into play. XGBoost (eXtreme Gradient Boosting) is a direct application of Gradient Boosting for decision trees. There are a myriad of resources that dive into the mathematical backing and systematic functions of XGBoost, but the main advantages are as follows:\n\n1. Easy to use\n2. Computational efficiency\n3. Model Accuracy\n4. Feasibility\u200a\u2014\u200aeasy to tune parameters and modify objectives.","bde72b31":"## Importing required libraries ","fedc9dbd":"## AUC for models\n1. Random Forest:\n    - AUC: 0.95\n2. XGBClassifier(Optimized) \n    - AUC: 0.94\n3. Logistic regression \n    - AUC: 0.80\n \n So as per evaluation criteria **Random forest** works best for thus classification problem\n    ","40d5ff3e":"Random forests is a supervised learning algorithm. It can be used both for classification and regression. It is also the most flexible and easy to use algorithm. A forest is comprised of trees. It is said that the more trees it has, the more robust a forest is. Random forests creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting. It also provides a pretty good indicator of the feature importance.\n![](http:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1526467744\/voting_dnjweq.jpg)","a9516b26":"### XGBoosting","ef271ced":"### Hyper parameter Tuning of Random Forest","4ec2e442":"## Comparison of Different Classification Models","564842c9":"# Part A - Classification with Trees.","abddfe7f":"### Checking Multi-colinearity ","5d7f6095":"## Building models","ab298a7b":"    # Comparison of Different Regression Models","0ffdf251":"***Confusion matrix ***\n\nIt is a performance measurement for machine learning classification problem where output can be two or more classes. It is a table with 4 different combinations of predicted and actual values. gives us a matrix as output and describes the complete performance of the model. \n![image.png](attachment:image.png)\n**Accuracy**: Overall, how often is the classifier correct? \n\n(TP+TN)\/total \n\n**Misclassification** Rate: Overall, how often is it wrong? \n\n(FP+FN)\/total  \n\nequivalent to 1 minus Accuracy also known as \"Error Rate\" \n\nTrue Positive Rate: When it's actually yes, how often does it predict yes? \n\nTP\/TP+ FN \u202f\u202falso known as \"Sensitivity\" or \"Recall\" \n\nFalse Positive Rate: When it's actually no, how often does it predict yes? \n\nFP\/TN+ FP  \n\n \n**\nArea Under\u202fCurve **\n\nArea Under Curve(AUC) is one of the most widely used metrics for evaluation. It is used for binary classification problem. AUC of a classifier is equal to the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example. Before defining AUC, let us understand two basic terms\u202f: \n\nTrue Positive Rate (Sensitivity)\u202f: True Positive Rate is defined as\u202fTP\/ (FN+TP). True Positive Rate corresponds to the proportion of positive data points that are correctly considered as positive, with respect to all positive data points. \n\n \n\nFalse Positive Rate (Specificity)\u202f: False Positive Rate is defined as\u202fFP \/ (FP+TN). False Positive Rate corresponds to the proportion of negative data points that are mistakenly considered as positive, with respect to all negative data points. \n\n \n\nFalse Positive Rate and True Positive Rate both have values in the range [0, 1].. AUC is the area under the curve of plot False Positive Rate vs True Positive Rate at different points in [0, 1]. \n\nArea under ROC curve is often used as a measure of quality of the classification models. A random classifier has an area under the curve of 0.5, while AUC for a perfect classifier is equal to 1. In practice, most of the classification models have an AUC between 0.5 and 1 \n\n ![image.png](attachment:image.png)\n\nAs evident, AUC has a range of [0, 1]. The greater the value, the better is the performance of our model. \n\n \n\nWhen a classifier cannot distinguish between the two groups, the area will be equal to 0.5 (the ROC curve will coincide with the diagonal). When there is a perfect separation of the two groups, i.e., no overlapping of the distributions, the area under the ROC curve reaches to 1 (the ROC curve will reach the upper left corner of the pl \n\n","fd1e73ac":"## Boosting based tree algorithm","03e8c983":"## Evaluating Criteria","3ead7d76":"Algorithms with there metrics value:\n\n**Linear Regression**\n    - Cross Validation Error: 1.242288014816511\n\n**Random Forest**\n    - Mean Absolute Error: 405.7327874520134\n    - Mean Squared Error: 435275.179031254\n    - Root Mean Squared Error: 659.7538776174446\n\n**Boosting:**\n    - Mean Absolute Error: 597.3932168285128\n    - Mean Squared Error: 715220.9126263337\n    - Root Mean Squared Error: 845.7073445503082\n   \n ","5a3c6769":"## Random Forest","adae481c":"## Joining two dataset","c0c50bff":"### Hyper Parameters tuning of XGB classifier\n\nParameters of XGB Classifier\n\n**[Tuning Parameters](https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/)**\n\nGeneral Approach for Parameter Tuning\nWe will use an approach similar to that of GBM here. The various steps to be performed are:\n\n1. Choose a relatively high learning rate. Generally a learning rate of 0.1 works but somewhere between 0.05 to 0.3 should work for different problems. Determine the optimum number of trees for this learning rate. XGBoost has a very useful function called as \u201ccv\u201d which performs cross-validation at each boosting iteration and thus returns the optimum number of trees required.\n2. Tune tree-specific parameters ( max_depth, min_child_weight, gamma, subsample, colsample_bytree) for decided learning rate and number of trees. Note that we can choose different parameters to define a tree and I\u2019ll take up an example here.\n3.Tune regularization parameters (lambda, alpha) for xgboost which can help reduce model complexity and enhance performance.\n4. Lower the learning rate and decide the optimal parameters .","9a54001d":"## By seeing Z score and p value of intercation terms we can say that they are strongly contributing the model and so model4 is the model we can go ahead with prediction.","c066a65b":"## Loading the data","eb566ece":"# Rossmann Store Sales: Forecast sales using store, promotions and its competitors information\n![](https:\/\/www.magnapolonia.org\/wp-content\/uploads\/2017\/08\/rossmann.jpg)","b85fe59f":"## Cleaning dataset","894cce7a":"### Bagging based tree algorithm (e.g. Random Forest)","a1c6edeb":"# Regression\nEvaluation Criteria\n![](https:\/\/i.stack.imgur.com\/83BUy.png)","67ea01f1":"### Hyper parameter tuning of Random forest regressor","8abfd4fd":"## Creating dummy variables","c8d96e94":"## Exploratory data analysis","a2ed1ed3":"Hyper Parameters of Random Forest\n- n_estimators = number of trees in the foreset\n- max_features = max number of features considered for splitting a node\n- max_depth = max number of levels in each decision tree\n- min_samples_split = min number of data points placed in a node before the node is split\n- min_samples_leaf = min number of data points allowed in a leaf node\n- bootstrap = method for sampling data points (with or without replacement)","51c1ee9e":"## Linear Assumptions","e02307c4":"# Part C - Regression with Trees","7117e44b":"# Part B - Classification with Logistic Regression\nLogistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.). In other words, the logistic regression model predicts P(Y=1) as a function of X.\nThe activation function that is used is known as the sigmoid function. The plot of the sigmoid function looks like\n![alt](https:\/\/cdn-images-1.medium.com\/max\/1200\/1*yKvimZ3MCAX-rwMX2n87nw.png)\nWe can see that the value of the sigmoid function always lies between 0 and 1. The value is exactly 0.5 at X=0. We can use 0.5 as the probability threshold to determine the classes. If the probability is greater than 0.5, we classify it as Class-1(Y=1) or else as Class-0(Y=0).","f8c6aa5f":"#### Implementation of XGboost","5b115c44":"## Boosting","cdc3e49d":"### Adding interaction term","dbb4a6b3":"A regression tree is built through a process known as binary recursive partitioning, which is an iterative process that splits the data into partitions or branches, and then continues splitting each partition into smaller groups as the method moves up each branch.\n\nInitially, all records in the Training Set (pre-classified records that are used to determine the structure of the tree) are grouped into the same partition. The algorithm then begins allocating the data into the first two partitions or branches, using every possible binary split on every field. The algorithm selects the split that minimizes the sum of the squared deviations from the mean in the two separate partitions. This splitting rule is then applied to each of the new branches. This process continues until each node reaches a user-specified minimum node size and becomes a terminal node. (If the sum of squared deviations from the mean in a node is zero, then that node is considered a terminal node even if it has not reached the minimum size.)"}}