{"cell_type":{"eb2b7dc9":"code","b7b62c5e":"code","faced462":"code","08e08e5f":"code","7d01f236":"code","16ca6ab0":"code","1dcabb1a":"code","239ba1e0":"code","06c42562":"code","4ba90eb6":"code","2f3a3f42":"code","de0299b6":"code","627a069c":"code","894ec782":"code","9aea1fc5":"code","32413eca":"code","46933dcd":"code","63df8feb":"code","d6336f1c":"code","55fc62ea":"code","79e37e73":"code","c93bf0ee":"code","741bb946":"code","d4bd9deb":"code","bf144fe8":"code","27e23e77":"code","2047cd90":"code","7b725305":"code","a0020a49":"code","b03104e4":"code","9421566f":"code","1ccafc4c":"markdown","fc96871b":"markdown","0d74376c":"markdown","39761139":"markdown","2f5e1cb9":"markdown","491d7566":"markdown","2ae32ec5":"markdown","490f2f25":"markdown","d2b361ca":"markdown","42caf293":"markdown","eec06f49":"markdown","c536748a":"markdown","433ca11a":"markdown","46c24c45":"markdown","79d7480c":"markdown","d759c568":"markdown","b41fbcd6":"markdown","2b1330ab":"markdown","065c8301":"markdown","bf45c685":"markdown"},"source":{"eb2b7dc9":"# Import the numerical algebra libs\nimport pandas as pd\nimport numpy as np\n\n# Import visualization libs\nimport seaborn as sns\nimport matplotlib.pyplot as plt","b7b62c5e":"import warnings\nwarnings.filterwarnings('ignore')","faced462":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","08e08e5f":"#BASE_PATH = \"\/content\/drive\/My Drive\/Colab Notebooks\/datasets\/\"\nBASE_PATH = \"\/kaggle\/input\/concrete-compressive-strength-data-set\/\"\nfile_name = \"compresive_strength_concrete.csv\"","7d01f236":"data = pd.read_csv(BASE_PATH + file_name)\nprint(f\"The given dataset contains {data.shape[0]} rows and {data.shape[1]} columns\")\nprint(f\"The given dataset contains {data.isna().sum().sum()} Null value\")","16ca6ab0":"data.head()","1dcabb1a":"data.rename(columns=dict(zip(data.columns, ['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg',\n       'fineagg', 'age', 'strength'])), inplace=True)","239ba1e0":"data.info()","06c42562":"data.describe().transpose()","4ba90eb6":"plt.subplots(figsize=(12, 6))\nax = sns.boxplot(data=data)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45);","2f3a3f42":"import itertools\n\ncols = [i for i in data.columns if i != 'strength']\n\nfig = plt.figure(figsize=(15, 20))\n\nfor i,j in itertools.zip_longest(cols, range(len(cols))):\n    plt.subplot(4,2,j+1)\n    ax = sns.distplot(data[i],color='green',rug=True)\n    plt.axvline(data[i].mean(),linestyle=\"dashed\",label=\"mean\", color='black')\n    plt.legend()\n    plt.title(i)\n    plt.xlabel(\"\")","de0299b6":"fig = plt.figure(figsize=(12, 6))\nplt.axvline(data.strength.mean(),linestyle=\"dashed\",label=\"mean\", color='blue')\nsns.distplot(data.strength);","627a069c":"# Build a pair plot\n\ng = sns.PairGrid(data)\ng.map_upper(plt.scatter)\ng.map_lower(sns.lineplot)\ng.map_diag(sns.kdeplot, lw=3, legend=True);","894ec782":"plt.subplots(figsize=(12, 6))\ncorr = data.corr('spearman')\n\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n\nax = sns.heatmap(data=corr, cmap='mako', annot=True, mask=mask)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45);","9aea1fc5":"from mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(14,11))\n\nax  = fig.gca(projection = \"3d\")\n#plt.subplot(111,projection = \"3d\") \n\nplot =  ax.scatter(data[\"cement\"],\n           data[\"strength\"],\n           data[\"superplastic\"],\n           linewidth=1,edgecolor =\"k\",\n           c=data[\"age\"],s=100,cmap=\"cool\")\n\nax.set_xlabel(\"cement\")\nax.set_ylabel(\"strength\")\nax.set_zlabel(\"superplastic\")\n\nlab = fig.colorbar(plot,shrink=.5,aspect=5)\nlab.set_label(\"AGE\",fontsize = 15)\n\nplt.title(\"3D plot for cement,compressive strength and super plasticizer\",color=\"navy\")\nplt.show()","32413eca":"from sklearn.cluster import KMeans\n\n# Create a copy of the data\ndata_copy = data.copy(deep=True)\ntmp_df = data_copy[['ash', 'superplastic', 'slag']]\n\ncluster_range = range( 2, 15 ) \ncluster_errors = []\nfor num_clusters in cluster_range:\n  clusters = KMeans( num_clusters, n_init = 5)\n  clusters.fit(tmp_df)\n  labels = clusters.labels_\n  centroids = clusters.cluster_centers_\n  cluster_errors.append( clusters.inertia_ )\nclusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )\nclusters_df[0:15]","46933dcd":"plt.figure(figsize=(12,6))\nplt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" );","63df8feb":"from sklearn.cluster import KMeans\nfrom scipy.stats import zscore\n\ntmp_df_z = tmp_df.apply(zscore)\n\ncluster = KMeans( n_clusters = 5, random_state = 2354 )\ncluster.fit(tmp_df_z)\n\nprediction=cluster.predict(tmp_df_z)\ndata_copy[\"group\"] = prediction ","d6336f1c":"for col in data_copy.columns[:-2]:\n  sns.lmplot(data=data_copy, x=col, y='strength', hue='group')","55fc62ea":"# Helper classes\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, cross_val_score\n\nfrom sklearn.impute import SimpleImputer\n\nimport pandas as pd\nimport numpy as np\n\nclass Regressor(object):\n    \"\"\"\n    Class representing a regressor. \n    Based on the parameters supplied in the constructor, this class constructs a pipeline object.\n    The constructed pipeline adds \n    - Standard scalar if the scale parameter is passed as True\n    - Polynomial Feature transformations if the include_polymomial flag is set as True\n    \"\"\"\n    def __init__(self, name, model, scale=True, include_polynomial=False, degree=2):\n        self.name = name\n        self.model = model\n        steps = []\n        if scale:\n            steps.append(('scaler', StandardScaler()))\n        if include_polynomial:\n            steps.append(('poly_features', PolynomialFeatures(degree=degree)))\n        steps.append(('model', model))\n        self.steps = steps\n\n    def get_name(self):\n        return self.name\n\n    def get_model(self):\n        return self.model\n\n    def get(self):\n        return Pipeline(steps=self.steps)\n\n    def feature_imp(self):\n        try:\n            return self.model.feature_importances_\n        except AttributeError:\n            try:\n                return self.model.coef_\n            except AttributeError:\n                return None\n\n\nclass ModelsBuilder(object):\n    '''\n    This class is responsible for building the model and constructing a results dataframe.\n    It accepts several regressor objects.\n    '''\n    def __init__(self, regressors, data, target, test_size=0.3, seed=42):\n        self.regressors = regressors\n        self.split_data = train_test_split(data.drop(target, axis=1), data[target], test_size=test_size, random_state=seed)\n        self.data = data\n        self.target = target\n\n    def build(self, k_fold_splits=10):\n        results = pd.DataFrame(columns=['model', 'training_score', 'test_score', 'k_fold_mean', 'k_fold_std'])\n        for regressor in self.regressors:\n            regressor.get().fit(self.split_data[0], self.split_data[2])\n            cross_vals = cross_val_score(regressor.get(), self.data.drop(self.target, axis=1), self.data[self.target], cv=KFold(n_splits=k_fold_splits))\n            mean = round(cross_vals.mean(), 3)\n            std = round(cross_vals.std(), 3)\n            results = results.append({\n                'model': regressor.get_name(),\n                'training_score': round(regressor.get().score(self.split_data[0], self.split_data[2]), 3),\n                'test_score': round(regressor.get().score(self.split_data[1], self.split_data[3]),3), \n                'k_fold_mean': mean, \n                'k_fold_std': std, \n                '95% confidence intervals': str(round(mean-(1.96*std),3)) + ' <-> ' + str(round(mean+(1.96*std),3))\n                }, ignore_index=True)\n\n        return results\n\n\nclass OutliersImputer(SimpleImputer):\n    '''\n    This class extends the functionality of SimpleImputer to handle outliers.\n    '''\n    def __init__(self, strategy='mean'):\n        self.strategy = strategy\n        super().__init__(strategy=strategy)\n\n    def fit(self, X, y=None):\n        for i in X.columns:\n            q1, q2, q3 = X[i].quantile([0.25,0.5,0.75])\n            IQR = q3 - q1\n            a = X[i] > q3 + 1.5*IQR\n            b = X[i] < q1 - 1.5*IQR\n            X[i] = np.where(a | b, np.NaN, X[i])  \n        return super().fit(X, y)","79e37e73":"from sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\n\nregressors = [\n    Regressor('Linear Regression', LinearRegression(), scale=True), \n    Regressor('Linear Regression degree 2', LinearRegression(), \n              scale=True, include_polynomial=True, degree=2),\n    Regressor('Linear Regression degree 3', LinearRegression(), \n              scale=True, include_polynomial=True, degree=3), \n    Regressor('Ridge', Ridge(random_state=42), scale=True), \n    Regressor('Ridge degree 2', Ridge(random_state=42), \n              scale=True, include_polynomial=True, degree=2),\n    Regressor('Ridge degree 3', Ridge(random_state=42), \n              scale=True, include_polynomial=True, degree=3),\n    Regressor('Lasso', Lasso(random_state=42), scale=True), \n    Regressor('Lasso degree 2', Lasso(random_state=42), \n              scale=True, include_polynomial=True, degree=2),\n    Regressor('Lasso degree 3', Lasso(random_state=42), \n              scale=True, include_polynomial=True, degree=3), \n    Regressor('Decision Tree', DecisionTreeRegressor(random_state=42, max_depth=4), scale=True),\n    Regressor('Ada boosting', AdaBoostRegressor(random_state=42), scale=True),\n    Regressor('Random forest', RandomForestRegressor(random_state=42, max_depth=4), scale=True),\n    Regressor('Gradient boosting', GradientBoostingRegressor(random_state=42, max_depth=4), scale=True),\n    Regressor('KNN', KNeighborsRegressor(n_neighbors=3), scale=True),\n    Regressor('SVR', SVR(gamma='auto'), scale=True),\n]","c93bf0ee":"# Iteration 1 - Use all data\nresult = ModelsBuilder(regressors, data, 'strength').build()\ntmp_best = result.sort_values(['k_fold_mean'], ascending=False).head(1)\ntmp_best['model'] = 'Best Model = ' + tmp_best['model']\nresult = result.append(tmp_best, ignore_index=True)\nresult","741bb946":"# Iteration 2 - Ouliers treatment\n\n# Count outliers\nq1= data.quantile(0.25)\nq3= data.quantile(0.75)\nIQR = q3-q1\noutliers = pd.DataFrame(((data > (q3+1.5*IQR)) | (data < (q1-IQR*1.5))).sum(axis=0), columns=['No. of outliers'])\noutliers['Percentage of outliers'] = round(outliers['No. of outliers']*100\/len(data), 2)\noutliers","d4bd9deb":"data[['age','superplastic']] = OutliersImputer().fit_transform(data[['age','superplastic']])","bf144fe8":"result_outliers_treatment = ModelsBuilder(regressors, data, 'strength').build()\ntmp_best = result_outliers_treatment.sort_values(['k_fold_mean'], ascending=False).head(1)\ntmp_best['model'] = 'Best Model = ' + tmp_best['model']\nresult_outliers_treatment = result_outliers_treatment.append(tmp_best, ignore_index=True)\nresult_outliers_treatment","27e23e77":"# Iteration 3 - Remove features based on k-means clustering\nresult_feature_engg = ModelsBuilder(regressors, data.drop(['ash', 'coarseagg', 'fineagg'], axis=1), 'strength').build()\ntmp_best = result_feature_engg.sort_values(['k_fold_mean'], ascending=False).head(1)\ntmp_best['model'] = 'Best Model = ' + tmp_best['model']\nresult_feature_engg = result_feature_engg.append(tmp_best, ignore_index=True)\nresult_feature_engg","2047cd90":"columns = data.drop(['ash', 'coarseagg', 'fineagg', 'strength'], axis=1).columns\nfeature_imp = pd.DataFrame(index=columns)\nfor r in regressors:\n    fi = r.feature_imp()\n    if fi is not None and len(fi) == len(columns):\n        feature_imp[r.get_name()] = fi\n\n\nplt.figure(figsize=(12, 20))\nfor i, col in enumerate(feature_imp.columns):\n  plt.subplot(4, 2, i+1)\n  ax = sns.barplot(x=feature_imp.index, y=feature_imp[col])\n  ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n  plt.subplots_adjust(hspace=0.4, wspace=0.4)","7b725305":"# Split data\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(data.drop(['ash', 'coarseagg', 'fineagg', 'strength'], axis=1), \n                                                   data['strength'], \n                                                   test_size = 0.2, \n                                                   random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","a0020a49":"# Prepare parameter grid\nparam_grid = {\n    'criterion': ['mse', 'mae', 'friedman_mse'], \n    'learning_rate': [0.05, 0.1, 0.15, 0.2], \n    'max_depth': [2, 3, 4, 5], \n    'max_features': ['sqrt', None], \n    'max_leaf_nodes': list(range(2, 10)),\n    'n_estimators': list(range(50, 500, 50)),\n    'subsample': [0.8, 0.9, 1.0]\n}","b03104e4":"# Perform hyper parameter tuning using GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrs = RandomizedSearchCV(estimator=GradientBoostingRegressor(random_state=42), param_distributions=param_grid, \n                 return_train_score= True, n_jobs=-1, verbose=2, cv = 10, n_iter=500)\nrs.fit(X_train, y_train)","9421566f":"mean = rs.best_score_\nstd = rs.cv_results_['mean_test_score'].std()\n\nprint(f\"Mean training score: {rs.cv_results_['mean_train_score'].mean()}\")\nprint(f\"Mean validation score: {mean}\")\nprint(f\"Validation standard deviation: {std}\")\nprint(f\"95% confidence interval: {str(round(mean-(1.96*std),3)) + ' <-> ' + str(round(mean+(1.96*std),3))}\")\nprint(f\"Best parameters: {rs.best_params_}\")\nprint(f\"Test score: {rs.score(X_test, y_test)}\")","1ccafc4c":"# Exploratory data analysis\n\n## Box plots","fc96871b":"# First look at the data","0d74376c":"# Conclusion\n\nIn the above study, we find that in order to predict the strength of concrete\n1. The features that affect the strength are cement, slag, water, superplastic and age\n2. The best model is Gradient boosting\n3. Using the above model, we can predict the strength accurately between 83% to 100% with 95% confidence.","39761139":"## Pair plot","2f5e1cb9":"### Observations\n\n* As expected, cement and age have strong correlation with strength\n* Super plastic has mild positive correlation with strength\n* As expected, water and superplastic have strong correlation","491d7566":"# Model building","2ae32ec5":"### Observations\n\n1.   All of the data in the dataset is numerical\n2.   No null\/NAN data\n3.   Age data appears to have outliers because max value is very large as compared to 3rd IQR value","490f2f25":"## Heat map","d2b361ca":"# Hyper parameter tuning\n\n<p> At this point we can conclude that Gradient boosting is perfect fit to predict the strength of concrete based on the given dataset. <\/p>\n<p> Now we can tune the hyper parameters for gradient boosting <\/p>\n\n#### Steps:\n1. Split the data into training and testing set (80-20)\n2. Use the training data to perform cross validation hyper parameter tuning using Grid Search\n3. Check the best performing model against the test set","42caf293":"## 3D graph between 3 most important features and target","eec06f49":"### Observations\n\n* Age column appears to be having maximum number of outliers\n* Slag, Water, superplastic, fineagg features have some outliers\n* All features except age and strength have same units(kg in m3 mixture) but have different scales. Thus we might need to scale the data so as to avoid bias in algorithms","c536748a":"# Dataset description\n\n\n\n*   Cement (cement) -- quantitative -- kg in a m3 mixture -- Input Variable\n*   Blast Furnace Slag (slag) -- quantitative -- kg in a m3 mixture -- Input Variable\n*   Fly Ash (ash) -- quantitative -- kg in a m3 mixture -- Input Variable\n*   Water (water) -- quantitative -- kg in a m3 mixture -- Input Variable\n*   Superplasticizer (superplastic) -- quantitative -- kg in a m3 mixture -- Input Variable\n*   Coarse Aggregate (coarseagg) -- quantitative -- kg in a m3 mixture -- Input Variable\n*   Fine Aggregate (fineagg) -- quantitative -- kg in a m3 mixture -- Input Variable\n*   Age(age) -- quantitative -- Day (1~365) -- Input Variable\n*   Concrete compressive strength(strength) -- quantitative -- MPa -- Output Variable\n\n\n\n\n\n\n\n","433ca11a":"### Observations\n\n1. Outliers treatment improves the overall performace on most of the models\n2. Removing the features (ash, coarseagg and fineagg) does not affect the models. \n3. Gradient boosting is clearly the best model. It provides better complexity with 95% confidence interval between 88% and 95%","46c24c45":"# K-Means clustering\n\nAsh, Superplastic, slag appear to have 2 gaussian","79d7480c":"### Observations\n\n#### Diagonal analysis + dist plots analysis\n* Distribution of cement appears nearly normal\n* Slag and ash has 2 gaussians and is skewed\n* Water and Superplastic have near normal distributions\n* Age data has long tail which confirms the presence of outliers\n* Strength is normally distributed\n\n#### Off-diagonal analysis with strength\n* Cement has strong correlation with strength\n* Slag is a very weak predictor because the distribution is like a cloud\n* ash, coarseagg and fineagg are also weak predictors\n* Water appears to have a negative correlation with strength\n* Superplastic appears to have positive correlation with strength\n* age also has strong correlation with strength\n\n#### Off-diagonal analysis between other features\n* Cement and slag have strong correlation\n* Water and super plastic have strong negative correlation","d759c568":"# Steps:\n1. Import the libs\n2. First look at the data\n3. Exploratory data analysis <br \/>\n    3.1. IQR and ouliers analysis (box plot)<br \/>\n    3.2. Distribution of independent variables <br \/>\n    3.3. Pair plots <br \/>\n    3.4. Heat map analysis <br \/>\n4. K-Means clustering - deal with multiple gaussians\n5. Model building - Iteration 1 --> Use data as is\n6. Model building - Iteration 2 --> Include the results of outlier treatment\n7. Model building - Iteration 3 --> Include the results of K-means clustering\n8. Model building - Iteration 4 --> Normalize\/Standardize the data before model building\n9. Hyper parameter training on best model from all iterations","b41fbcd6":"### Observations\n\nFrom the above analysis, it can be seen\n1. Water, superplastic, age and cement are the most important attributes for strength prediction\n2. ash, coarseagg and fineagg are not strong predictors. Thus we can try removing them\n3. slag is mildly important predictor","2b1330ab":"# Import the libs","065c8301":" We will try the following algorithms:\n1. Linear Regression\n2. Linear Regression with Polynomial features of degree 2\n3. Linear Regression with Polynomial features of degree 3\n4. Ridge \n5. Ridge with polynomial features of degree 2\n6. Ridge with polynomial features of degree 3\n7. Lasso \n8. Lasso with polynomial features of degree 2\n9. Lasso with polynomial features of degree 3\n10. Decision Trees\n11. Random forest\n12. Ada boosting\n13. Gradient boosting\n14. KNN\n15. Support Vector machines\n\nWe will always scale the data as confirmed in EDA\n\nwe will perform the analysis in following iterations\n1. Build the models and perform analysis on all data\n2. Treat outliers and build models again\n3. Consider the results of Feature engineering (k-means clustering)\n4. Discuss and compare all the results","bf45c685":"## Distribution of independent variables\n"}}