{"cell_type":{"ab0371ef":"code","a11c5d58":"code","4718da8d":"code","567266d6":"code","bca5d3f0":"code","f5058227":"code","bdf70b51":"code","fbb1926e":"code","9d7934f1":"code","e1efd740":"code","edbc248d":"code","2dd96c95":"code","c3f73cd2":"code","68568e69":"code","c2ab15f6":"code","4e741cc5":"code","4a095add":"code","1969e8a1":"code","b94e0b87":"code","50938411":"code","085940d1":"code","a571bfb1":"code","ec00fc01":"code","693c6e76":"code","2c570873":"code","9e0b7940":"code","f44aaab5":"code","a4d7f870":"code","ba49fa42":"code","4dd5eb99":"code","c169b646":"code","e37c5ee3":"code","41c2371c":"code","721ca008":"code","b8990b5f":"code","5126c346":"code","f78a0791":"code","be48fcdf":"code","daeb184f":"code","e3ad0c9a":"markdown","af4f327d":"markdown","ea544cff":"markdown","5d9ff237":"markdown","6c3f0b12":"markdown","21ef1a04":"markdown","4ab1c069":"markdown","81f77542":"markdown","ca361839":"markdown","bb143784":"markdown","6c828f8e":"markdown","707a2bd4":"markdown","918df9d4":"markdown","451c0db0":"markdown","d0979be3":"markdown","674a6dbc":"markdown","4bf07cd1":"markdown","803c82d5":"markdown","e58186e3":"markdown","27400c20":"markdown","00a384cf":"markdown","d45ea948":"markdown","ea8f7599":"markdown","c7743347":"markdown","e42161da":"markdown"},"source":{"ab0371ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a11c5d58":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix,plot_confusion_matrix,classification_report\nfrom sklearn.model_selection import StratifiedShuffleSplit,cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom tensorflow.keras import models,layers","4718da8d":"train=pd.read_csv('..\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv')\ntrain.head()","567266d6":"print(train.shape)","bca5d3f0":"train.describe()","f5058227":"train.info()","bdf70b51":"plt.figure(figsize=(10,7))\nsns.countplot(x='Loan_Status',data=train)\nprint('Proportion of loan applications approved: ',train.Loan_Status.value_counts()[0]\/len(train)*100)\nprint('Proportion of loan applications rejected: ',train.Loan_Status.value_counts()[1]\/len(train)*100)","fbb1926e":"plt.figure(figsize=(10,7))\nsns.distplot(train['ApplicantIncome'],color='Blue')","9d7934f1":"plt.figure(figsize=(10,7))\nsns.histplot(x='ApplicantIncome',data=train,color='Red')","e1efd740":"plt.figure(figsize=(10,7))\nsns.distplot(train['LoanAmount'],color='purple')","edbc248d":"plt.figure(figsize=(10,7))\nsns.histplot(data=train,x='LoanAmount',color='pink')","2dd96c95":"sns.relplot(x='ApplicantIncome',y='LoanAmount',data=train,hue='Loan_Status',height=7.0)\nplt.xlabel('Applicant Income ')\nplt.ylabel('Loan Amount (in thousands)')\nplt.show()","c3f73cd2":"plt.figure(figsize=(10,7))\nsns.countplot(x='Gender',data=train,hue='Loan_Status',palette='flare_r')","68568e69":"grid=sns.FacetGrid(data=train,col='Loan_Status',height=5.0)\ngrid.map(sns.countplot,'Married',palette='jet_r')","c2ab15f6":"plt.figure(figsize=(10,7))\nsns.countplot('Dependents',data=train,hue='Loan_Status',palette='magma')","4e741cc5":"plt.figure(figsize=(10,7))\nsns.countplot('Education',data=train,hue='Loan_Status',palette='inferno_r')","4a095add":"grid=sns.FacetGrid(data=train,col='Loan_Status',height=5.0)\ngrid.map(sns.countplot,'Self_Employed',palette='crest_r')","1969e8a1":"plt.figure(figsize=(15,8))\nsns.countplot('Loan_Amount_Term',data=train,hue='Loan_Status',palette='Dark2')\nplt.xlabel('Loan_Amount_Term (weeks)')\nplt.show()","b94e0b87":"grid=sns.FacetGrid(data=train,col='Loan_Status',height=5.0)\ngrid.map(sns.countplot,'Credit_History',palette='gist_earth');","50938411":"grid=sns.FacetGrid(data=train,col='Loan_Status',height=5.0)\ngrid.map(sns.countplot,'Property_Area',palette='CMRmap_r');","085940d1":"sns.heatmap(train.corr(),annot=True)","a571bfb1":"train.isnull().sum()","ec00fc01":"train.Gender.fillna(train.Gender.mode()[0],inplace=True)\ntrain.Married.fillna(train.Married.mode()[0],inplace=True)\ntrain.Dependents.fillna(train.Dependents.mode()[0],inplace=True)\ntrain.Self_Employed.fillna(train.Self_Employed.mode()[0],inplace=True)\ntrain.LoanAmount.fillna(train.LoanAmount.median(),inplace=True)\ntrain.Loan_Amount_Term.fillna(train.Loan_Amount_Term.mode()[0],inplace=True)\ntrain.Credit_History.fillna(train.Credit_History.mode()[0],inplace=True)","693c6e76":"train.isnull().sum()","2c570873":"train.dtypes","9e0b7940":"object_col=train.select_dtypes('object').columns\nle=LabelEncoder()\nfor col in object_col:\ntrain[col]=le.fit_transform(train[col])","f44aaab5":"train.dtypes","a4d7f870":"train.head()","ba49fa42":"plt.figure(figsize=(12,7))\nsns.heatmap(train.corr(),annot=True)","4dd5eb99":"train.drop('Loan_ID',axis=1,inplace=True)\ntrain.drop('Property_Area',axis=1,inplace=True)","c169b646":"y_train=train.Loan_Status\ntrain.drop('Loan_Status',axis=1,inplace=True)\nx_train=train","e37c5ee3":"sss=StratifiedShuffleSplit(n_splits=1,test_size=0.3,random_state=42)\nfor train,test in sss.split(x_train,y_train):\n  x_train,x_test=x_train.iloc[train],x_train.iloc[test]\n  y_train,y_test=y_train.iloc[train],y_train.iloc[test]\n\nprint(f'Shape of x_train is {x_train.shape}')\nprint(f'Shape of y_train is {y_train.shape}')\nprint(f'Shape of x_test is {x_test.shape}')\nprint(f'Shape of y_test is {y_test.shape}')","41c2371c":"print('Proportion of 0 in y_train :',y_train.value_counts()[0]\/ len(y_train))\nprint('Proportion of 0 in y_test :',y_test.value_counts()[0]\/ len(y_test))\nprint('Proportion of 1 in y_train :',y_train.value_counts()[1]\/ len(y_train))\nprint('Proportion of 1 in y_test :',y_test.value_counts()[1]\/ len(y_test))","721ca008":"model=LogisticRegression(max_iter=200)\nmodel.fit(x_train,y_train)\ncross_val=(cross_val_score(model,x_train,y_train,cv=5))\ny_pred=model.predict(x_test)\ncf_r=classification_report(y_test,y_pred)\nprint('Mean cross validation score ',np.mean(cross_val))\nprint('Accuracy on test data ', model.score(x_test,y_test))\nprint('Accuracy on train data ', model.score(x_train,y_train))\nprint(plot_confusion_matrix(model,x_test,y_test,values_format='0.3g'))\nprint('Classification Report :\\n',cf_r)","b8990b5f":"for i in range(1,100):\n  best_n=[]\n  model=KNeighborsClassifier(n_neighbors=i)\n  model.fit(x_train,y_train)\n  cross_val=(cross_val_score(model,x_train,y_train,cv=5))\n  best_n.append( model.score(x_test,y_test)) \ny_pred=model.predict(x_test) \nprint('Mean cross validation score ',np.mean(cross_val))\nprint('Accuracy on test data ', model.score(x_test,y_test))\nprint(plot_confusion_matrix(model,x_test,y_test,values_format='0.3g'))\nprint('Accuracy on train data ', model.score(x_train,y_train))","5126c346":"model=DecisionTreeClassifier(random_state=42)\nmodel.fit(x_train,y_train)\ncross_val=(cross_val_score(model,x_train,y_train,cv=5))\ny_pred=model.predict(x_test)\ncf_r=classification_report(y_test,y_pred)  \nprint('Mean cross validation score ',np.mean(cross_val))\nprint('Accuracy on test data ', model.score(x_test,y_test))\nprint('Accuracy on train data ', model.score(x_train,y_train))\nprint(plot_confusion_matrix(model,x_test,y_test,values_format='0.3g'))\nprint('Classification Report :\\n',cf_r)","f78a0791":"model=XGBClassifier()\nmodel.fit(x_train,y_train)\ncross_val=(cross_val_score(model,x_train,y_train,cv=5))\ny_pred=model.predict(x_test)\ncf_r=classification_report(y_test,y_pred)  \nprint('Mean cross validation score ',np.mean(cross_val))\nprint('Accuracy on test data ', model.score(x_test,y_test))\nprint('Accuracy on train data ', model.score(x_train,y_train))\nprint(plot_confusion_matrix(model,x_test,y_test,values_format='0.3g'))\nprint('Classification Report :\\n',cf_r)","be48fcdf":"model=models.Sequential()\nmodel.add(layers.Dense(32,activation='relu',input_shape=x_train.shape))\nmodel.add(layers.Dropout(0.3))\nmodel.add(layers.Dense(16,activation='relu'))\nmodel.add(layers.Dense(8,activation='relu'))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(1,activation='sigmoid'))\nmodel.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\nmodel.fit(x_train,y_train,epochs=300,batch_size=2,verbose=0)","daeb184f":"print('Accuracy on test data ', model.evaluate(x_test,y_test))\ny_pred=model.predict(x_test)\ny_pred=np.where(y_pred>0.5,1,0)\nc_m=confusion_matrix(y_test,y_pred)\nprint('Confusion Matrix : \\n',c_m)","e3ad0c9a":"**The following 2 distribution plots indicate that majority of those people have submitted an application for loan who have income between 0 & 10000($).**","af4f327d":"# Exploratory Data Analysis","ea544cff":"**We can see that there is equal proportion of target variables in both datasets.This makes the data balanced,so easier and more effective for our model to make predictions.**","5d9ff237":"# Conclusion\nOut of all the ML\/DL models I have tried, Logistic Regression was the one which performed the best with test accuracy of 85%. So if anyone is not married,is graduated,has no dependents, is not self employed,has good income and has good credit hisotry will have good chance of loan application being accepted.","6c3f0b12":"**Those features who had only 2 unique values, I got their null values filled by their mode value. Whereas I got LoanAmount feature's null values filled by its median, because it may have outliers and taking average could divert our model from correctly predicting.**","21ef1a04":"In this notebook, I am going to show you that how different ML & DL models perform on a credit risk modeling dataset, where we will predict which of the customers will have their loan approved.\n\nThis is going to be a binary classification problem where the model will learn to predict the Loan_Status of a person, based on information available.\n\nThe dataset has 614 rows and 13 different features, including the target variable(Loan_Status). The data contains following features in it:\n\n**Loan_ID**: A unique loan id\n\n**Gender**: Male\/Female\n**Married**:Yes\/No\n\n**Dependents**:Number of poeple depending on applicant\n\n**Education**:Applicant's education--Graduate\/Not Gradudate\n\n**Self_Employed**:Yes\/No\n\n**AppicantIncome**: Income of applicant($)\n\n**CoapplicantIncome**:Income of co-applicant($)\n\n**LoanAmount**:Loan amount($ thousands)\n\n**Loan_Amount_Term**:Term for borrowing money(weeks)\n\n**Credit_History**:Applicant's credit history\n\n**Property_Area**:Urban\/Rural\/Semi\n\n**Loan_Status**:Loan Approved (Yes\/No)\n\nFirst I am going to import some important libraries, then I will do some exploratory data analysis, then a bit of feature engineering followed by creating models and evaluating them on test set.","4ab1c069":"**The following plot shows the relationship between credit history and loan status. It suggests that those who had bad credit history didnt get their loan application approved compared to those who had god credit history. So if anyone has bad credit history, that person might have to face disappointment.**","81f77542":"**Having a look at heatmap gives relationship between different integer\/float datatype features. At this moment, it is not showing other features as they have object datatype.Later in this notebook we will see it in more detail.**","ca361839":"The following illustration illustrates that from the data provided almost 69% of the loan applications were approved.","bb143784":"**The following plot suggests that those people's loan application was more accepted who had borrowed it for 360(weeks) compared to those who had borrowed it for less term.So if one borrows it for this much time period, that person has greater chance for loan to be approved.**","6c828f8e":"**The following plot gives an idea about the people's education. It explains that those people who were graduate, had greater chance for their loan to be approved.**","707a2bd4":"# Feature Engineering\n**From the data, I am going to remove the Loan_ID and Property_Area feature because the heatmap suggests that these two features dont have any strong effect on any other feature.**","918df9d4":"**Our model only accepts tensors\/numeric data, so I will use LabelEncoder module of sklearn library to encode the object datatype features. This will change their datatype to integer as well.**","451c0db0":"**The following illustration indicates that there were more Male candidates for loan application as compared to Females, but the loan status is not much affected by it as there is almost same proportion of rejections in both the cases.**","d0979be3":"# Cleaning Data\nThere are alot of null values in the dataset, so we will have to deal with them.","674a6dbc":"**This count plot illustrates the property area applicants have. This feature is not showing any relation with loan status as people who had different property area got their applications approved. Through property area it cannot be distinguised that who's application has more chance of being approved.**","4bf07cd1":"# Creating ML\/DL models\nLogistic Regression","803c82d5":"**The following figure indicates that those who had no dependents got their loan approved compared to those who had. So if someone has no dependents, that person has greater chance for loan to be approved.**","e58186e3":"**Now we will have a detailed look at heatmap diagram. It contains all features in it now because we recently encoded the object datatypes and they are converted to integer.\nThe heatmap suggests that their is high positive corelation between applicant's income and loan amount. The more the applicant's income is, the more loan amount he wants to have approved.\nThis also shows high positive corelation between loan status and credit history. The more good the credit history a person has, there is high chance for his loan to be approved. We also found this insight previously in countplot diagram of credit history and loan status.**\n\n**The more darker the color is, there is more negative correlation. The more lighter it is, there is more positive correlation between the features.**\n","27400c20":"**Now I am going to split my data into train data(for training models) and test data(for evaluating models) with the help of sklearn's module, StratifiedShuffleSplit. I did not use train_test_split because it doesnt split the target variable in a balanced way. StratifiedShuffleSplit will make sure that both train and test data have equal proportion of target variables. I will be splitting data in to 60 40 ratio. 60% for training and 40% for testing.**","00a384cf":"**The following illustration illustrates that mainly those applicant's application for loan was approved who were not married. So if someone is not married, that person has greater chance for loan to be approved.**","d45ea948":"**The following figure suggests that people who were not self employed were given more preference over those who were self employed. If someone is not self employed, that person has more chance for getting loan approved.**","ea8f7599":"# KNeighborsClassifier\n\nHere I have used loop to get the best value for nearest neighbors.","c7743347":"**The following distribution plots show that majority of people wanted a loan between 100,000 & 200,000($).**","e42161da":"The following scatterplot between applicant's income and loan amount indicates that mainly people wanted to have a lesser loan. This is shown by the distribution of points at left bottom of the figure. We can also see that loan status is not affected by the amount of loan or applicant's income. It varies through out the data."}}