{"cell_type":{"798cdf02":"code","c051ddd6":"code","18531e01":"code","0e613d9d":"code","9809439e":"code","80272d35":"code","80e3644e":"code","8b82d205":"code","a1375f36":"code","483b06a5":"code","0a702056":"code","56bfaf4b":"code","769dd6a9":"code","f963fc0c":"code","c5c05814":"code","28d892cd":"code","7c2c79de":"code","f30a0bb8":"code","eb858fa0":"code","6783e064":"code","9faca6b7":"code","6ca57352":"code","97a21648":"code","88f82287":"markdown","be1d5439":"markdown","2c3de2d7":"markdown","11561241":"markdown","c1833378":"markdown","ee03a861":"markdown","a941e1c0":"markdown","4fab4534":"markdown","9d91698e":"markdown","56304f34":"markdown","ff5ee99d":"markdown","6ef5306c":"markdown","3086c1b1":"markdown","f6bc2a1c":"markdown","cf45c9e4":"markdown","a1592266":"markdown","23e12ab3":"markdown","0b3da6b4":"markdown","588b78aa":"markdown","8d2befdd":"markdown","e198fcf2":"markdown","f1385de0":"markdown","1263b488":"markdown","524937ff":"markdown","2fa599d6":"markdown"},"source":{"798cdf02":"!pip install pyspark ","c051ddd6":"from pyspark import SparkContext\nimport pandas as pd","18531e01":"sc = SparkContext() ","0e613d9d":"%%writefile example.txt\n#Id       Timestamp    Customer   State    ServiceID    Amount\n201       10\/13\/2017      100       NY       131          100.00\n204       10\/18\/2017      700       TX       129          450.00\n202       10\/15\/2017      203       CA       121          200.00\n206       10\/19\/2017      202       CA       131          500.00\n203       10\/17\/2017      101       NY       173          750.00\n205       10\/19\/2017      202       TX       121          200.00\n203       10\/17\/2017      101       NY       173          50.00","9809439e":"print ( open(\"example.txt\",\"r\").read() ) ","80272d35":"example = sc.textFile(\"example.txt\")","80e3644e":"example.first()","8b82d205":"example.count()","a1375f36":"example.collect()","483b06a5":"example.take(5)","0a702056":"filtered = example.filter(lambda x : \"204\" in x)","56bfaf4b":"filtered.collect()","769dd6a9":"out = example.flatMap(lambda x : x.split())\nout.take(10)","f963fc0c":"cleaned = example.map(lambda x : x[1:] if \"#Id\" in x else x)","c5c05814":"cleaned.collect()","28d892cd":"split_lines = example.map(lambda x : x.split())\nsplit_lines.collect()","7c2c79de":"pd.DataFrame(data = split_lines.collect()[1:], columns=split_lines.collect()[0])","f30a0bb8":"split_lines.filter(lambda x : x[0] in \"204\").collect()","eb858fa0":"## print out what we have so far\nsplit_lines.collect()","6783e064":"# get State and Amount as a typle \nstate_amt = split_lines.map(lambda x: (x[3],x[5]))\nstate_amt.collect()","9faca6b7":"# remember to change the values to floats, else they will be considered strings \ngrouped_state = state_amt.reduceByKey(lambda x,y : float(x) + float(y))\ngrouped_state.collect()","6ca57352":"grouped_state = grouped_state.filter(lambda x : not x[0]==\"State\")\ngrouped_state.collect()","97a21648":"grouped_state.sortBy(lambda x : x[1], ascending=False).collect()","88f82287":"### reduceByKey \nCan we get the total amounts by each state - i.e. sum  all ***Amounts***, grouped by the **State**\n\nTo do this we need to:\n* Create a tuple of just the State and Amount columns\n* Run ReduceByKey on the new typle\n","be1d5439":"### Count ","2c3de2d7":"# Create an RDD <a class=\"anchor\" id=\"4\"><\/a>\nWe will set the text file to be distrubuted across the pyspark cluster","11561241":"#### Split the data example \nAnother important map transformation is to split the data on spaces (or any other delimiter) \\\nThis will create a list of lists which easier to work with as it gives defined rows and columns \n\nLets split the text by the spaces to get a list of lists ","c1833378":"<a class=\"anchor\" id=\"0\"><\/a>\n\n# **Table of Contents**\n\n\n1.\t[Installation](#1)\n2.\t[Libraries](#2)\n3.\t[Create our test data](#3)\n    - 2.2   [Machine learning in Pyspark](#2.2)\n4.\t[Create a RDD](#4)\n5.\t[Apply actions to RDD](#5)\n6.\t[Apply Transformations to RDD ](#6)\n7.\t[Examples](#7)\n8.\t[Conclusion](#8)\n","ee03a861":"# Libraries & Instantiate pyspark <a class=\"anchor\" id=\"2\"><\/a>\n\nAs this is a basic look at pyspark we will only need two libraries","a941e1c0":"#### For numerical lists we can use **.parrallelize()**\n\nie. \nsc.parallelize([1,2,3,4,5])","4fab4534":"### Flatmap\ncreate a 1-dimensional list ","9d91698e":"reduceByKey groups by the key which is the first column **State** \\\nWe then apply a lambda function to sum all the values that were grouped (Amount)","56304f34":"### Map \n\nApplying a transformation to the RDD, this is dependent on the function \n\n#### Remove hashtag example \n\nIf we look at the data we can see that there is a hashtag in from of Id, which is not present in any of the other headings \\\nLets remove the hashtag","ff5ee99d":"### Take\nGet the top n rows of the RDD","6ef5306c":"#### Get row of id = 204","3086c1b1":"# Intro to PySpark Actions & Transformations \n\n\n![pyspark](https:\/\/miro.medium.com\/max\/1400\/1*nPcdyVwgcuEZiEZiRqApug.jpeg)\n\nIn this kernel we will use pyspark to perform basic queries and transformations on a resilient distributed dataset (RDD). A number of these eamples have been taken from the udemy course [Python for Data Science and Machine Learning Bootcamp](https:\/\/www.udemy.com\/course\/python-for-data-science-and-machine-learning-bootcamp)\n\n\nBefore we begin lets go over some common terms  \n\nTerms                  |Descriptions \n----                   |-------\nRDD                    |Resilient Distributed Dataset\nTransformation         |Spark operation that outputs an RDD\nAction                 |Spark operation that outputs a result \nSpark Job              |Sequence of transformations on data with a final action\n\n### About RDD's \nAn RDD is a distributed collection of objects that is resilient( fault tolerent) and immutable. Immutable meaning that once and RDD us created, you cannot alter it. Resilient meaning that each record in an RDD is split into logical partitions and can be computed on different nodes of a pyspark cluster.\n\nAll RDD data is in a tuple format with a key and value \ni.e. (key, value) ","f6bc2a1c":"# Conclusion <a class=\"anchor\" id=\"8\"><\/a>\nThis was inial introductory guide to pyspark to get you familiar with some basic actions and transformation \\\nOnce you are confortable with the above, try you hand at more complex notebooks \n\n* [analysis with basic pyspark ](https:\/\/www.kaggle.com\/hackspyder\/analysis-with-basic-pyspark)\n* [pyspark ml tutorial for begginners](https:\/\/www.kaggle.com\/fatmakursun\/pyspark-ml-tutorial-for-beginners) ","cf45c9e4":"# Create a basic text file <a class=\"anchor\" id=\"3\"><\/a>\nWe will create a basic text file with some lines of text \\\nThis will be our test file to apply pyspark actions and transformations on ","a1592266":"##### Pandas cross over\nOnce data is split, it is in a list of list - due to splitting \\\nIf we want this can be put into a Dataframe\n\n***note*** this is for reference only, usually we would not want to use pandas as our data would be very large","23e12ab3":"### First\nGet first row of data","0b3da6b4":"# Apply Transformations to RDD <a class=\"anchor\" id=\"6\"><\/a>\nBasic transformations include: \n\n\nTransformation   |Example                   |Result\n--------         |  ----------                               |-------\nfilter            |`filter(lambda x: x % 2 == 0)`           |Discard non-even elements\nmap |`map(lambda x: x.split())`               |Split each string into words\nflatmap |`flatMap(lambda x: x.split())`           |Split each string into words and flatten sequence\nreduceByKey |`reduceByKey(lambda x,y: x+y)`           |sum values by key - NB needs to be in tuple \nsample|`sample(withReplacement=True,0.25)`      |Create sample of 25% of elements with replacement\nunion |`union(rdd)`                             |Append `rdd` to existing RDD\ndistinct |`distinct()`                             |Remove duplicates in RDD\nsortBy|`sortBy(lambda x: x, ascending=False)`   |Sort elements in descending order\n\n##### NOTE: Most transformations will use a function, such as [lambda](https:\/\/towardsdatascience.com\/lambda-functions-with-practical-examples-in-python-45934f3653a8)","588b78aa":"# Examples <a class=\"anchor\" id=\"7\"><\/a>\nWe will go over a slightly more advanced use cases, as well as use reducebykey()","8d2befdd":"### Collect\nGet all rows in the RDD ","e198fcf2":"### Remove column names i.e. remove State and Amount row","f1385de0":"# PySpark installation <a class=\"anchor\" id=\"1\"><\/a>\nRequirements: \n* Java ---- pre-installed on kaggle kernels\n* Scala ---- pre-installed on kaggle kernels\n* py4j library ---- pre-installed on kaggle kernels\n* PySpark ","1263b488":"# Apply actions to RDD <a class=\"anchor\" id=\"5\"><\/a>\nBasic actions include:\n\nAction                    | Description \n----                      |-------\n.first( 3 )               | get first 3 elements\n.count()                  | count of elements\n.take( 3 )                | get top 3 elements\n.collect()                | output RDD to a list \n.sum()                    | summation of elements \n.mean()                   | mean of elements\n.stdev()                  | standard deviation of elements","524937ff":"### Sort Values ","2fa599d6":"### Filter\nFilter the RDD by a function "}}