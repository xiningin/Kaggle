{"cell_type":{"49226622":"code","e128f7fa":"code","7470e763":"code","105943fb":"code","d536518d":"code","af3c8e95":"code","6b66f039":"code","b05a5846":"code","856ea826":"code","cb629460":"code","694dfe3e":"code","310b019f":"code","186e8856":"code","56452430":"code","56d659ae":"code","2cac10a8":"code","787214fe":"code","eb803dfc":"code","04f7a15c":"code","ec3004b8":"code","1d181abf":"code","0d624aa7":"code","d2d3e902":"code","84a6c285":"code","146c758a":"code","d3ae57dd":"code","57072de3":"code","3723ca99":"code","76bd7208":"code","b0a9986b":"code","f68b1130":"code","260d7e3f":"code","db2c01fb":"code","c8c65a5c":"code","5dbae678":"code","058bf4d2":"code","b556d32f":"code","ae117f68":"code","95e39d74":"code","23ae34b7":"code","7e4be82a":"code","d5889a71":"code","73964b66":"code","b415d237":"code","d5df126c":"code","d2caeef8":"code","80291f2e":"code","fb721009":"code","b4c623ac":"code","7a849dbf":"code","b48ff256":"code","7e32f56d":"code","0f766256":"code","04a72ca8":"code","a7aa2929":"code","be8b750f":"markdown","f76c22fb":"markdown","a7e248cd":"markdown","393aa05c":"markdown","53d213f3":"markdown","7df5fb33":"markdown","38e62f90":"markdown","6700d987":"markdown","20fa56ea":"markdown","e654451c":"markdown","2694fd17":"markdown","142a3efe":"markdown","9f476e30":"markdown","0d35c4f3":"markdown","9b4d5299":"markdown","8ee4fe26":"markdown","7a44aa4f":"markdown","e1daeabc":"markdown","b66101f3":"markdown"},"source":{"49226622":"from IPython.display import Image\nImage(url = \"https:\/\/storage.ning.com\/topology\/rest\/1.0\/file\/get\/3780584426?profile=original\",width = 1000, height=800)","e128f7fa":"!pip install pickle5","7470e763":"import re\nimport string \nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pickle5 as pickle\nimport scipy.sparse\n\n#preprocessing and scoring\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, f1_score, accuracy_score, recall_score, precision_score\nfrom nltk.stem import WordNetLemmatizer \n\n#models and algos\nfrom textblob import TextBlob\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\n##customer pipeline function\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n#Feature Extraction\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#Hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.model_selection import StratifiedKFold","105943fb":"print(stopwords.words(\"english\"))","d536518d":"train = pd.read_csv(\"..\/input\/sentiment140-dataset-1600000-tweets\/training.1600000.processed.noemoticon.csv\",encoding=\"Latin-1\" ,names=[\"polarity\",\"id\", \"date\",\"query\", \"user\", \"tweet\"])\nsentiment140  = pd.read_csv(\"..\/input\/sentiment140-dataset-1600000-tweets\/testdata.manual.2009.06.14.csv\",encoding=\"Latin-1\" ,names=[\"polarity\",\"id\", \"date\",\"query\", \"user\", \"tweet\"])\napple = pd.read_csv(\"..\/input\/apple-twitter-sentiment-crowdflower\/Apple-Twitter-Sentiment-DFE.csv\", encoding=\"Latin-1\")\ntwitter_reddit = pd.read_csv(\"..\/input\/twitter-and-reddit-sentimental-analysis-dataset\/Twitter_Data.csv\")\nus_airlines =  pd.read_csv(\"..\/input\/twitter-airline-sentiment\/Tweets.csv\")","af3c8e95":"#remove unnecessary columns\ntrain = train[[\"polarity\", \"tweet\"]]\nsentiment140 = sentiment140[[\"polarity\", \"tweet\"]]\napple = apple[[\"sentiment\",\"text\"]]\nus_airlines = us_airlines[[\"airline_sentiment\", \"text\"]]\ntwitter_reddit = twitter_reddit[[\"category\", \"clean_text\"]]","6b66f039":"#rename columns\napple.columns = [\"polarity\",\"tweet\"]\nsentiment140.columns = [\"polarity\",\"tweet\"]\nus_airlines.columns = [\"polarity\",\"tweet\"]\ntwitter_reddit.columns = [\"polarity\",\"tweet\"]","b05a5846":"#Replace values to have -1 negative, 0 neutral, 1 postive\ntrain[\"polarity\"]  = train[\"polarity\"].replace(4,1)\ntrain[\"polarity\"]  = train[\"polarity\"].replace(0,-1)\n\nsentiment140[\"polarity\"]  = sentiment140[\"polarity\"].replace(4,1)\nsentiment140[\"polarity\"]  = sentiment140[\"polarity\"].replace(0,-1)\nsentiment140[\"polarity\"]  = sentiment140[\"polarity\"].replace(2,0)\n\napple[\"polarity\"] = apple[\"polarity\"].replace(\"1\",-1)\napple[\"polarity\"] = apple[\"polarity\"].replace(\"3\",0)\napple[\"polarity\"] = apple[\"polarity\"].replace(\"5\",1)\n\nus_airlines[\"polarity\"] =us_airlines[\"polarity\"].replace(\"negative\",-1)\nus_airlines[\"polarity\"] =us_airlines[\"polarity\"].replace(\"neutral\",0)\nus_airlines[\"polarity\"] =us_airlines[\"polarity\"].replace(\"positive\",1)","856ea826":"apple = apple[apple[\"polarity\"]!= \"not_relevant\"]\napple[\"polarity\"] = apple[\"polarity\"].astype(int) ","cb629460":"twitter_reddit.dropna(inplace= True)","694dfe3e":"print(\"train: \", len(train))\nprint(\"sentiment140: \", len(sentiment140))\nprint(\"apple: \", len(apple))\nprint(\"twitter_reddit: \", len(twitter_reddit))\nprint(\"us_airlines: \", len(us_airlines))","310b019f":"tweets_df = pd.concat([train,sentiment140,apple,us_airlines,twitter_reddit],axis=0)","186e8856":"sns.countplot(data = tweets_df , x = \"polarity\")","56452430":"tweets_df[\"tweet\"] = tweets_df[\"tweet\"].astype(str)\ntweets_df.reset_index(drop = True,inplace=True)","56d659ae":"# Length  \ntweets_df[\"length\"] = tweets_df[\"tweet\"].apply(len)","2cac10a8":"tweets_df.groupby(\"polarity\")[\"length\"].describe()","787214fe":"plt.figure(figsize=(15,5))\n\nsns.displot(data = tweets_df, x= \"length\", hue= \"polarity\" ,palette={-1:\"r\",0:\"b\",1:\"g\"}, bins = 30,aspect= 4, alpha = 0.8)","eb803dfc":"def clean_text(text):  \n    pat1 = r'@[^ ]+'                   #@signs and value\n    pat2 = r'https?:\/\/[A-Za-z0-9.\/]+'  #links\n    pat3 = r'\\'s'                      #floating s's\n    pat4 = r'\\#\\w+'                     # hashtags and value\n    pat5 = r'&amp '\n    pat6 = r'[^A-Za-z\\s]'         #remove non-alphabet\n    combined_pat = r'|'.join((pat1, pat2,pat3,pat4,pat5, pat6))\n    text = re.sub(combined_pat,\"\",text).lower()\n    return text.strip()","04f7a15c":"#clean\ntweets_df[\"cleaned_tweet\"] = tweets_df[\"tweet\"].apply(clean_text)","ec3004b8":"#drop empty \ntweets_df = tweets_df [ ~(tweets_df[\"cleaned_tweet\"] ==\"\")]","1d181abf":"lem = WordNetLemmatizer()\n\ndef tokenize_lem(sentence):\n    outlist= []\n    token = sentence.split()\n    for tok in token:\n        outlist.append(lem.lemmatize(tok))\n    return \" \".join(outlist)","0d624aa7":"tweets_df[\"cleaned_tweet\"] = tweets_df[\"cleaned_tweet\"].apply(tokenize_lem)","d2d3e902":"tweets_df.head()","84a6c285":"tweets_df.info()","146c758a":"X_train, X_test, y_train, y_test = train_test_split(tweets_df[[\"cleaned_tweet\",\"length\"]], tweets_df[\"polarity\"], test_size=0.1, random_state=42)","d3ae57dd":"tfidf = TfidfVectorizer()","57072de3":"tfidf.fit(X_train[\"cleaned_tweet\"])","3723ca99":"X_train_v = tfidf.transform(X_train[\"cleaned_tweet\"])\nX_test_v = tfidf.transform(X_test[\"cleaned_tweet\"])","76bd7208":"print(X_train_v.shape)\nprint(X_test_v.shape)","b0a9986b":"scaler = MinMaxScaler()\nscaler2 = MinMaxScaler()","f68b1130":"scaler.fit([X_train[\"length\"]])\nscaler2.fit([X_test[\"length\"]])","260d7e3f":"X_train_len = scaler.transform([X_train[\"length\"]])\nX_train_len = X_train_len.reshape( X_train_v.shape[0], 1)\n\nX_train = scipy.sparse.hstack([X_train_v,X_train_len], format = \"csr\")","db2c01fb":"X_test_len = scaler2.transform([X_test[\"length\"]])\nX_test_len = X_test_len.reshape(X_test_v.shape[0], 1)\n\nX_test = scipy.sparse.hstack([X_test_v,X_test_len], format = \"csr\")","c8c65a5c":"#Parameters tested outside Kaggle \n'''gen_params = {\"alpha\":[0.001,0.01,1,3,4,5]}\n\nxgb_params={'eta': [0.5, 1, 2, 3], \n            'max_depth': [None, 3, 5, 7, 9],\n                         \n            'n_estimators': [50, 100, 150, 200, 300, 500]\n           }\nlnSVC_params = {\n    \"C\": [0.01,1,3,4,5,10]\n}\nlogr_params = {\n    \"penalty\": [\"l2\" ,\"l1\", \"none\"],\n    \"C\": [1,3,4,5],\n    \"max_iter\": [5000,10000]\n}\n\nforest_params = {\"n_estimators\": [50, 100],\n                 \"max_depth\" : [1,5, 10]\n}'''","5dbae678":"kfold =StratifiedKFold(n_splits=5,shuffle=True,random_state=42)","058bf4d2":"model_score = pd.DataFrame(columns=[\"model_f1_train\",\"params_used\", \"f1\",\"precision\",\"recall\"])\n\n\ndef model_prediction(model, params):\n    \n    model = GridSearchCV(model, param_grid= params, cv= kfold)\n    \n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    print (\"Model and params: \", model.best_estimator_, model.best_params_) \n    print(\"\\n\")\n    print(\"Train score: \", model.best_score_)\n    print(\"test score: \",accuracy_score(y_test,y_pred))\n    print(\"\\n\")\n    print(\"Test Report:\")\n    print(classification_report(y_test,y_pred))\n    return y_pred, model","b556d32f":"def model_scoring(y_pred, model):\n    global y_test\n    global model_score\n    \n    df = pd.DataFrame(data = [[model.best_score_,\n                           model.best_params_,\n                           f1_score(y_test,y_pred,average=\"macro\"),\n                           precision_score(y_test,y_pred,average=\"macro\"),\n                           recall_score(y_test,y_pred,average=\"macro\")\n                          ]] , \n                  columns =model_score.columns, \n                  index=[str(model.best_estimator_)])\n    model_score = model_score.append ( df )  ","ae117f68":"#Set Parameters\ngen_params = {\"alpha\":[1,3]}\nxgb_params = {\n        'n_estimators': [200],\n        'max_depth': [9],\n        'eta': [0.5],\n}\nlnSVC_params = {\n    \"C\": [1]\n}\nlogr_params = {\n    \"penalty\": [\"l2\"],\n    \"C\": [5],\n    \"max_iter\": [10000]\n}\nforest_params = {\"n_estimators\": [100],\n                 \"max_depth\" : [8]\n}","95e39d74":"#Instantiate\nvader = SentimentIntensityAnalyzer()\n#textblob does not required instantiation \n\nlogr_i = LogisticRegression(solver=\"sag\")\nridge_i = RidgeClassifier()                 # L2 regularization\nlnSVC_i = LinearSVC()\nnaivemulti_i = MultinomialNB()\nnaivebern_i = BernoulliNB()\nxgb_i = XGBClassifier(#tree_method='gpu_hist'\n                     )\nrf_i = RandomForestClassifier()","23ae34b7":"log_pred , logr_m = model_prediction(logr_i,logr_params)\nridge_pred, ridge_m = model_prediction(ridge_i,{\"alpha\":[3]})\nlinSVC_pred, lnSVC_m = model_prediction(lnSVC_i, lnSVC_params)\nnaivemulti_pred, naivemulti_m = model_prediction(naivemulti_i, gen_params)\nnaivebern_pred, naivebern_m = model_prediction(naivebern_i, gen_params)\n#xgb_pred, xgb_m = model_prediction(xgb_i, xgb_params)\n#rf_pred, rf_m = model_prediction(rf_i,forest_params)","7e4be82a":"model_scoring(log_pred, logr_m)\nmodel_scoring(ridge_pred, ridge_m)\nmodel_scoring(linSVC_pred, lnSVC_m)\nmodel_scoring(naivemulti_pred, naivemulti_m)\nmodel_scoring(naivebern_pred, naivebern_m)\n#model_scoring(xgb_pred, xgb_m)\n#model_scoring(rf_pred, rf_m)","d5889a71":"model_score","73964b66":"def out_box_textblob(x):\n    x = TextBlob(x).sentiment[0]\n    if x >0:\n        x = 1\n    elif x<0:\n        x = -1\n    else:\n        x= 0\n    return x","b415d237":"def out_box_vader(x):\n    x = vader.polarity_scores(x)[\"compound\"]\n    if x >0:\n        x = 1\n    elif x<0:\n        x = -1\n    else:\n        x= 0\n    return x","d5df126c":"def out_box_score(y_true, prediction, name):\n    global model_score\n    df = pd.DataFrame(data = [[0,\n                               0,\n                               f1_score(y_true,prediction,average=\"macro\"),\n                               precision_score(y_true,prediction,average=\"macro\"),\n                               recall_score(y_true,prediction,average=\"macro\")\n                              ]] ,\n                      columns=model_score.columns, \n                      index=[name])\n    model_score = model_score.append ( df )","d2caeef8":"textblob_pred = tweets_df[\"cleaned_tweet\"].apply(out_box_textblob)\nvader_pred = tweets_df[\"cleaned_tweet\"].apply(out_box_vader)\n\nout_box_score(tweets_df[\"polarity\"],vader_pred, \"Vader\")\nout_box_score(tweets_df[\"polarity\"],textblob_pred,\"Textblob\")","80291f2e":"model_score","fb721009":"model_dict = {\n    logr_m : \"logr_m\",\n    ridge_m:\"ridge_m\",lnSVC_m:\"lnSVC_m\", naivemulti_m:\"naivemulti_m\", naivebern_m:\"naivebern_m\"\n    #,xgb_m:\"xgb_m\"\n    #,rf_m: \"rf_m\"\n             }\nfor m in model_dict.items():\n    file = open(f'{m[1]}.pickle','wb')\n    pickle.dump(m[0], file)\n    file.close()","b4c623ac":"## save vocabulary \nwith open(\"vocabulary\",\"wb\") as f:\n    pickle.dump(tfidf.vocabulary_,f)\n    f.close()","7a849dbf":"\"\"\"#from sklearn.ensemble import VotingClassifier\nvc = VotingClassifier([(\"logr_m\",logr_m), (\"ridge_m\",ridge_m), (\"lnSVC_m\",lnSVC_m), \n                       #(\"ridge_m\",ridge_m),\n                       (\"naivemulti_m\",naivemulti_m),\n                       (\"naivebern_m\",naivebern_m),\n                       (\"rf_m\",xgb_m)],\n                      voting = \"hard\")\"\"\"","b48ff256":"from sklearn.pipeline import Pipeline\n\nX = tweets_df[\"cleaned_tweet\"]\ny = tweets_df[\"polarity\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","7e32f56d":"pipe = Pipeline(steps = [\n    (\"tfidf\", TfidfVectorizer()),\n    (\"xgb\" , GridSearchCV(xgb_i, param_grid= xgb_params, cv= kfold))\n]\n               )","0f766256":"pipe.fit(X,y)","04a72ca8":"print(classification_report(y_test, pipe.predict(X_test)))","a7aa2929":"file = open('pipemodel_model.pickle','wb')\npickle.dump(pipe, file)\nfile.close()","be8b750f":"## GridSearchCV\nDue to the time some of these models took to run, the tuning for the below models were was done outside Kaggle and as a result some parameter iterations were reduced:\n\n#### params per model\n* LogisticRegression(C=5, max_iter=10000, solver='sag')\n* LinearSVC(C=1)\n* RidgeClassifier(alpha=3)\n* BernoulliNB(alpha=1)\n* MultinomialNB(alpha=0.01)\n* XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\\n colsample_bynode=1, colsample_bytree=1, eta=0.5, gamma=0,\\n gpu_id=-1, importance_type='gain', interaction_constraints='',\\n learning_rate=0.5, max_delta_step=0, max_depth=9,\\n min_child_weight=1, missing=nan, monotone_constraints='()',\\n n_estimators=500, n_jobs=4, num_parallel_tree=1,\\n objective='multi:softprob', random_state=0, reg_alpha=0,\\n reg_lambda=1, scale_pos_weight=None, subsample=1,\\n tree_method='exact', validate_parameters=1, verbosity=None)","f76c22fb":"# Pipeline","a7e248cd":"Due to the sentiment140 training data not including neutral tweets we have a imbalanced dataset ","393aa05c":"### Run GridSearchCV","53d213f3":"# Project Summary\nTrain model to predict sentiment analysis. This model will the be used to predict Tweets that reference one of the top 5 banks in South Africa. \n\nTraining: I used mutiple datasets to do this with Sentiment 140 being the main contributor of tweets (1.6 million) \nHowever this was automatically labled (using emoticons) and doesnt have neutral tweets labled \n\n**Note** Proof of concept version was completed using a [pretrained model](https:\/\/www.kaggle.com\/slythe\/sentiment-analysis-with-twint-textblob-poc) (Textblob) \n\n## Datasets used\n1. [Sentiment140](https:\/\/www.kaggle.com\/milobele\/sentiment140-dataset-1600000-tweets)\n1. [Twitter and Reddit](https:\/\/www.kaggle.com\/cosmos98\/twitter-and-reddit-sentimental-analysis-dataset) \n1. [US airlines](https:\/\/www.kaggle.com\/crowdflower\/twitter-airline-sentiment) \n1.[Apple sentiment](https:\/\/www.kaggle.com\/slythe\/apple-twitter-sentiment-crowdflower) \n\n## Known issues with sentiment analysis:\n* Sarcasm - \"thanks FNB, now I cant open my account cause its frozen\" \n* Comparison of entities  - \"Capitec is the worst, you should use Standard Bank\" \n* Training data on non-South African tweets  - Jargon and lingo is different\n* Language usage - multiple languages are used in South Africa\n\nReference notebooks:\n* [Full sentiment analysis](https:\/\/www.kaggle.com\/paoloripamonti\/twitter-sentiment-analysis\/notebook) \n* [beginners notebook](https:\/\/www.kaggle.com\/stoicstatic\/twitter-sentiment-analysis-for-beginners)","7df5fb33":"# Cleaning ","38e62f90":"Due to the time take to run the below models (+9hrs) I have excluded random forest and XGBoost\n\nNote thet XGBoost has the highest accruacy and f1 score of all the models used (see pipeline further below)","6700d987":"# Additional Features\nThere are a number of additional features that can be added however this can be its own notebook. We will look only at length for now","20fa56ea":"#### Add TFIDF to tweets vector and Scaling","e654451c":"## Save pipeline","2694fd17":"#### Split the data","142a3efe":"## Save all models & vocab","9f476e30":"### TF-IDF \nvectorise the tweets ","0d35c4f3":"### Lemmetization ","9b4d5299":"# Out-the box models\n* Textblob\n* Vader","8ee4fe26":"## Read and setup datasets","7a44aa4f":"### Algorithms used:\n* Random Forest\n* LinearSVC\n* Naive Bayes (Bernoulini and Multinomial)\n* XGBoost\n* Logistic Regression\n* Ridge \n\n### Out-the-box models for comparison:\n1. Textblob \n1. Vader","e1daeabc":"## Voting Classifier \nOption ensemble ","b66101f3":"## Note on stopwords\nRemoval of stop words seems to do more harm than good, many of the sentences lose their meaning. \"not\" and \"nor\" are particularly an issue as well as many of the prepositions \\ \n\n**ie.**\n> what is Apple doing, they won't do well  -->>  Apple well \\\n> I'm against Samsung  -->>  Samsung"}}