{"cell_type":{"4a6e6de9":"code","4c36cad5":"code","2d74954f":"code","8d6d1a9d":"code","d764bab5":"code","73de96a6":"code","604b2f2a":"code","9769fdd3":"code","040f0fa3":"code","c542b215":"code","016791f8":"code","4241246b":"code","9e50c177":"code","3adde64b":"code","8397804d":"code","90a6b85f":"code","ea66726e":"code","6e630c43":"code","0c9dbad6":"code","414818a0":"code","fbf44177":"code","9f242665":"code","6206541d":"markdown","e5200923":"markdown"},"source":{"4a6e6de9":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nimport numpy as np\nimport math\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets, transforms\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n%matplotlib inline","4c36cad5":"class SimpleModel(torch.nn.Module):\n    def __init__(self, dropout_rate, decay):\n        super(SimpleModel, self).__init__()\n        self.dropout_rate = dropout_rate\n        self.decay = decay\n        self.f = torch.nn.Sequential(\n            torch.nn.Linear(1,20),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(p=self.dropout_rate),\n            torch.nn.Linear(20, 20),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(p=self.dropout_rate),\n            torch.nn.Linear(20,1)\n        )\n    def forward(self, X):        \n        return self.f(X)\n    \nSimpleModel(0.5, 1e-3)","2d74954f":"def uncertainity_estimate(x, model, num_samples, l2):\n    \"\"\"\n    MC dropout\u3067\u4e88\u6e2c\u306e\u4e0d\u78ba\u5b9f\u6027(Epistemic uncertainty)\u8a08\u7b97\n    Args:\n        x: \u4e88\u6e2c\u3057\u305f\u3044\u8aac\u660e\u5909\u6570\n        model: Dropout\u3001L2\u6b63\u5247\u5316\u3092\u4f7f\u3063\u3066\u5b66\u7fd2\u3057\u305fDL\u30e2\u30c7\u30eb\n        num_samples: \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u56de\u6570\n        l2: MC dropout\u306e\u5206\u6563\u51fa\u3059\u305f\u3081\u306el2\u6b63\u5247\u5316\u4fc2\u6570\n    \"\"\"\n    outputs = np.hstack([model(x).cpu().detach().numpy() for i in range(num_samples)]) # n\u56de inference, output.shape = [20, N]\n    y_mean = outputs.mean(axis=1)  # n\u56de\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u306e\u5e73\u5747\n    y_variance = outputs.var(axis=1)  # n\u56de\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u306e\u5206\u6563\n    # \u5206\u6563\u306f\u3001\u300c\u666e\u901a\u306e\u5206\u6563\u300d+\u300c\u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u7387\u3001L2\u6b63\u5247\u5316\u4fc2\u6570\u3001\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6570\u7b49\u306b\u95a2\u9023\u3057\u305f\u91cf\u300d\u3067\u6c42\u3081\u308b\n    tau = l2 * (1. - model.dropout_rate) \/ (2. * N * model.decay)\n    y_variance += (1. \/ tau)\n    y_std = np.sqrt(y_variance)\n    return y_mean, y_std","8d6d1a9d":"# \u56de\u5e30\u554f\u984c\u3092\u8003\u3048\u308b\nx = np.linspace(-3, 3, 100).reshape(100, 1)\ny = x**3\n\nN = 200 ##  number of points\n\nx_obs = np.linspace(-2, 2, N)\nnoise = np.random.normal(loc=0, scale=3., size=N)\ny_obs =  x_obs**3 + noise\nprint(f\"x_obs: {x_obs.shape}, y_obs: {y_obs.shape}\")\nprint(x_obs[:5])\nprint(y_obs[:5])\n\nx_test = np.linspace(-3, 3, N)\ny_test = x_test**3 + noise\nprint(f\"x_test: {x_test.shape}, x_test: {x_test.shape}\")\n\n## Normalise data:\n#x_mean, x_std = x_obs.mean(), x_obs.std()\n#y_mean, y_std = y_obs.mean(), y_obs.std()\n#x_obs = (x_obs - x_mean) \/ x_std\n#y_obs = (y_obs - y_mean) \/ y_std\n#x_test = (x_test - x_mean) \/ x_std\n#y_test = (y_test - y_mean) \/ y_std","d764bab5":"plt.figure(figsize=(12,6))\nplt.plot(x, y, ls=\"--\", color=\"r\", label='ground truth: $y=x^3$')\nplt.plot(\n    x_obs,\n    y_obs,\n    \"or\",\n    marker=\"o\",\n    color=\"0.1\",\n    alpha=0.8,\n    label=\"data points\",\n)\nplt.grid()\nplt.legend()\nplt.show()","73de96a6":"model = SimpleModel(dropout_rate=0.5, decay=1e-6).to(device)\ncriterion  = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=model.decay)","604b2f2a":"# \u5b66\u7fd2\nfor iter in range(20000):\n    y_pred = model(torch.Tensor(x_obs).view(-1,1).to(device))\n    y_true = Variable(torch.Tensor(y_obs).view(-1,1).to(device))\n    optimizer.zero_grad()\n    loss = criterion(y_pred, y_true)\n    loss.backward()\n    optimizer.step()\n    \n    if iter % 2000 == 0:\n        print(\"Iter: {}, Loss: {:.4f}\".format(iter, loss.item()))","9769fdd3":"y_pred = model(torch.Tensor(x_obs).view(-1, 1).to(device))\n\nplt.figure(figsize=(12, 6))\nplt.plot(x, y, ls=\"--\", color=\"r\", label=\"ground truth $y=x^3$\")\nplt.plot(\n    x_obs, y_obs, ls=\"none\", marker=\"o\", color=\"0.1\", alpha=0.8, label=\"data points\"\n)\nplt.plot(x_obs, y_pred.cpu().detach().numpy(), ls=\"-\", color=\"b\", label=\"MLP (MSE)\")\nplt.legend()\nplt.grid()\nplt.show()","040f0fa3":"# n\u56de\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u3066\u4e0d\u78ba\u5b9f\u6027\u8a08\u7b97\n\niters_uncertainty = 200\n\nlengthscale = 0.01\nn_std = 2  # number of standard deviations to plot  \u03c3\u306e\u6570\u5206\u8272\u5909\u3048\u308b\ny_mean, y_std = uncertainity_estimate(\n    torch.Tensor(x_test).view(-1, 1).to(device), model, iters_uncertainty, lengthscale\n)\n\nplt.figure(figsize=(12, 6))\nplt.plot(x, y, ls=\"--\", color=\"r\", label=\"ground truth $y=x^3$\")\nplt.plot(x_obs, y_obs, ls=\"none\", marker=\"o\", color=\"0.1\", alpha=0.8, label=\"data points\")\nplt.plot(x_test, y_mean, ls=\"-\", color=\"b\", label=\"MC Dropout mean\")  # \u9752\u7dda\u304c\u4e88\u6e2c\u306e\u5e73\u5747\n#plt.plot(x_test, y_test, ls=\"--\", color=\"r\", label=\"true\")  # \u8d64\u7dda\u304c\u6b63\u89e3\nfor i in range(n_std):\n    plt.fill_between(\n        x_test,\n        y_mean - y_std * ((i + 1.0)),\n        y_mean + y_std * ((i + 1.0)),\n        color=\"b\",\n        alpha=0.1,\n        label=f\"epistemic uncertainty {i+1}$\\sigma$\",\n    )\nplt.legend()\nplt.grid()\nplt.show()","c542b215":"# MNIST\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3067\u304d\u306a\u304f\u306a\u3063\u305f\u5bfe\u7b56 20210304 https:\/\/github.com\/pytorch\/vision\/issues\/1938\nfrom six.moves import urllib\nopener = urllib.request.build_opener()\nopener.addheaders = [('User-agent', 'Mozilla\/5.0')]\nurllib.request.install_opener(opener)\n\n# Training dataset\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(root='.', train=True, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])), batch_size=64, shuffle=True, num_workers=2)\n# Test dataset\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(root='.', train=False, transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])), batch_size=64, shuffle=True, num_workers=2)","016791f8":"class CNN_Model(nn.Module):\n    def __init__(self):\n        super(CNN_Model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(F.dropout(self.conv1(x), training=True), 2))\n        x = F.relu(F.max_pool2d(F.dropout(self.conv2(x), training=True), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=True)\n        x = self.fc2(x)\n        return x","4241246b":"model = CNN_Model().to(device)","9e50c177":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\nmodel.train()","3adde64b":"# train\nfor epoch in range(3):\n    for batch_idx, (data, target) in enumerate(train_loader):        \n        data, target = data.to(device), target.to(device)    \n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 300 == 0:\n            print('Train Epoch: {} [{}\/{} ({:.0f}%)] \\tLoss: {:.6f}'\n                  .format(epoch, batch_idx * len(data),\n                          len(train_loader.dataset),\n                          100. * batch_idx \/ len(train_loader),\n                          loss.item()))","8397804d":"# normal predict(dropout\u975e\u6d3b\u6027\u306b\u3057\u3066\u4e88\u6e2c)\ndef test(model):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += criterion(output, target)  # sum up batch loss\n            pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    test_loss \/= len(test_loader.dataset)\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}\/{} ({:.2f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct \/ len(test_loader.dataset)))","90a6b85f":"test(model)","ea66726e":"# MC dropout\ndef mcdropout_test(model):\n    model.train()\n    test_loss = 0\n    correct = 0\n    T = 50\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            ########### n\u56de\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0 ##########\n            output_list = []\n            for i in range(T):\n                output_list.append(torch.unsqueeze(model(data), 0))\n            output_mean = torch.cat(output_list, 0).mean(0)\n            #####################################\n            test_loss += criterion(output_mean, target)  # sum up batch loss\n            pred = output_mean.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    test_loss \/= len(test_loader.dataset)\n    print('\\nMC Dropout Test set: Average loss: {:.4f}, Accuracy: {}\/{} ({:.2f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct \/ len(test_loader.dataset)))","6e630c43":"mcdropout_test(model)","0c9dbad6":"\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(root='.', train=False, transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])), batch_size=1, shuffle=True, num_workers=2)","414818a0":"# \u753b\u50cf\u306b\u56de\u8ee2\u3064\u3051\u308b\nr = 60\nrotation_matrix = torch.Tensor([[[math.cos(r\/360.0*2*math.pi), -math.sin(r\/360.0*2*math.pi), 0],\n                                 [math.sin(r\/360.0*2*math.pi), math.cos(r\/360.0*2*math.pi), 0]]]).to(device)\nprint(rotation_matrix)","fbf44177":"# \u753b\u50cf\u306b\u56de\u8ee2\u3064\u3051\u3066MC dropout\u3067\u4e0d\u78ba\u5b9f\u6027\u8a08\u7b97\ndef uncertainty_test(model):\n    model.train()\n    T = 50\n    rotation_list = range(0, 360, 10)\n    with torch.no_grad():\n        for idx, (data, target) in enumerate(test_loader):\n            data, target = data.to(device), target.to(device)\n            ########## \u753b\u50cf\u306b\u56de\u8ee2\u3064\u3051\u308b ##########\n            image_list = []\n            unct_list = []\n            predict_list =[]\n            for r in rotation_list:\n                output_list = []\n                rotation_matrix = torch.Tensor([[[math.cos(r\/360.0*2*math.pi), -math.sin(r\/360.0*2*math.pi), 0],\n                                                 [math.sin(r\/360.0*2*math.pi), math.cos(r\/360.0*2*math.pi), 0]]]).to(device)\n                grid = F.affine_grid(rotation_matrix, data.size())\n                data_rotate = F.grid_sample(data, grid)\n                image_list.append(data_rotate)\n                \n                ########## \u56de\u8ee2\u3064\u3051\u305f\u753b\u50cfn\u56de\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0 ##########\n                for i in range(T):\n                    out = torch.unsqueeze(F.softmax(model(data_rotate), dim=1), dim=0)\n                    output_list.append(out)\n                    #print(out)\n                output_mean = torch.cat(output_list, 0).mean(dim=0)  # n\u56de\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u305f\u78ba\u4fe1\u5ea6\u306e\u5e73\u5747\n                output_variance = torch.cat(output_list, 0).var(dim=0).mean().item()  # n\u56de\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u305f\u78ba\u4fe1\u5ea6\u306e\u5206\u6563\u306e\u5e73\u5747\n                #print(output_mean)\n                #################################################\n                confidence = output_mean.data.cpu().numpy().max()  # \u78ba\u4fe1\u5ea6\u306e\u6700\u5927\u5024\n                predict = output_mean.data.cpu().numpy().argmax()  # \u78ba\u4fe1\u5ea6\u6700\u5927\u306e\u30af\u30e9\u30b9\u3092\u4e88\u6e2c\u30af\u30e9\u30b9\u3068\u3059\u308b\n                print(\"confidence:\", confidence)\n                print(\"predict:\", predict)\n                predict_list.append(predict)\n                unct_list.append(output_variance)\n#                 print ('rotation degree', str(r).ljust(3), 'Uncertainty : {:.4f} Predict : {} Softmax : {:.2f}'.format(output_variance, predict, confidence))\n\n            # 10\u5ea6\u5358\u4f4d\u3067\u56de\u8ee2\u3064\u3051\u305f\u753b\u50cf\u53ef\u8996\u5316.\u4e0b\u306e\u6570\u5b57\u304c\u4e0d\u78ba\u5b9f\u6027\uff08n\u56de\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u305f\u78ba\u4fe1\u5ea6\u306e\u5206\u6563\u306e\u5e73\u5747\uff09\n            plt.figure(figsize=(24, 6))\n            for i in range(len(rotation_list)):\n                ax = plt.subplot(2, len(rotation_list)\/2, i+1)\n                plt.text(0.5, -0.5, \"{0:.3f}\".format(unct_list[i]),\n                         size=14, ha=\"center\", transform=ax.transAxes)\n                plt.axis('off')\n                plt.gca().set_title('pred:'+str(predict_list[i]), size=14)\n                plt.imshow(image_list[i][0, 0, :, :].data.cpu().numpy())\n            plt.show()\n            if idx > 10:\n                break","9f242665":"uncertainty_test(model)","6206541d":"Monte Carlo(MC) Dropout\n- https:\/\/github.com\/cpark321\/uncertainty-deep-learning\/blob\/master\/02.%20Dropout%20as%20a%20Bayesian%20Approximation.ipynb","e5200923":"MC-Dropout with MNIST"}}