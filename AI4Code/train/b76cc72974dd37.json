{"cell_type":{"e2537788":"code","25d7e3b5":"code","49ba2a82":"code","44b012a8":"code","d0cba33b":"code","ab4ef36d":"code","6d829d1a":"code","3a7911d8":"code","729d3b2a":"code","4ed82dc9":"code","021ba08b":"code","2f0bfa25":"code","b695a0c0":"code","98f7f716":"code","c60070ff":"code","8f899fc1":"code","34e57584":"code","86354775":"code","127cfa42":"code","40f4bacc":"code","0a731cae":"code","b5d62d48":"code","5c99cc55":"code","bbc0a3bc":"code","9c31f8b5":"code","81e6d43e":"code","b1b0eee8":"code","dafa85c5":"markdown","2fd4382f":"markdown","e574f183":"markdown","b701a72d":"markdown","b9aea93d":"markdown","29fc288c":"markdown","0e6228a8":"markdown","436dfcfa":"markdown","5d86a57e":"markdown","2f3e63e7":"markdown","308daab7":"markdown","eb31dd4e":"markdown","33dbc1ba":"markdown","d79fdc5b":"markdown","13bcd8ec":"markdown","797418f7":"markdown","9ac65dea":"markdown"},"source":{"e2537788":"# data imports\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tsfresh as tsf\nsns.set(style=\"darkgrid\")\n\n\nimport os\nimport random\nrandom.seed(42)\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","25d7e3b5":"# load data\ntrain=pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv\")","49ba2a82":"train = train[(train.item_price < 300000 )& (train.item_cnt_day < 1000)]\ntrain = train[train.item_price > 0].reset_index(drop = True)\ntrain.loc[train.item_cnt_day < 1, \"item_cnt_day\"] = 0","44b012a8":"from itertools import product\nimport time\nts = time.time()\nmatrix = []\ncols  = [\"date_block_num\", \"shop_id\", \"item_id\"]\ntrain = train.sample(5000)  # comment out if you want to run for the whole data set\nfor i in range(34):\n    matrix.append( np.array(list( product( [i], train.shop_id.unique(), train.item_id.unique() ) ), dtype = np.int16) )\n\nmatrix = pd.DataFrame( np.vstack(matrix), columns = cols )\nmatrix[\"date_block_num\"] = matrix[\"date_block_num\"].astype(np.int8)\nmatrix[\"shop_id\"] = matrix[\"shop_id\"].astype(np.int8)\nmatrix[\"item_id\"] = matrix[\"item_id\"].astype(np.int16)\nmatrix.sort_values( cols, inplace = True )\ntime.time()- ts","d0cba33b":"ts = time.time()\ngroup = train.groupby( [\"date_block_num\", \"shop_id\", \"item_id\"] ).agg( {\"item_cnt_day\": [\"sum\"]} )\ngroup.columns = [\"item_cnt_month\"]\ngroup.reset_index( inplace = True)\nmatrix = pd.merge( matrix, group, on = cols, how = \"left\" )\nmatrix[\"item_cnt_month\"] = matrix[\"item_cnt_month\"].fillna(0).astype(np.float16)\ntime.time() - ts","ab4ef36d":"test[\"date_block_num\"] = 34\ntest[\"date_block_num\"] = test[\"date_block_num\"].astype(np.int8)\ntest[\"shop_id\"] = test.shop_id.astype(np.int8)\ntest[\"item_id\"] = test.item_id.astype(np.int16)","6d829d1a":"ts = time.time()\n\nmatrix = pd.concat([matrix, test.drop([\"ID\"],axis = 1)], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna( 0, inplace = True )\ntime.time() - ts","3a7911d8":"ts = time.time()\n\n\nmatrix['shop_id_even'] = matrix.shop_id.astype(str).str.zfill(2)\nmatrix['item_id_even'] = matrix.item_id.astype(str).str.zfill(5)\nmatrix['id'] = matrix.shop_id_even + matrix.item_id_even\n\ntime.time() - ts","729d3b2a":"shops = random.sample(list(matrix.shop_id.unique()), k = 15)  # comment out if you want to run for the whole data set\nitems = random.sample(list(matrix.item_id.unique()), k = 25)  # comment out if you want to run for the whole data set\n\nmatrix_small = matrix.loc[(matrix.shop_id.isin(shops)) & (matrix.item_id.isin(items))]\ndf = matrix_small[['id', 'date_block_num', 'item_cnt_month']]\ndf.info()","4ed82dc9":"from tsfresh.utilities.dataframe_functions import roll_time_series\ndf_rolled = roll_time_series(df, \"id\", \"date_block_num\", max_timeshift=6)","021ba08b":"df_rolled.head()","2f0bfa25":"from tsfresh import extract_features\ndf_rolled['item_cnt_month'] = df_rolled.item_cnt_month.astype(float) # somehow tsfresh cannot cope with float16 values\ndf_features = extract_features(df_rolled, column_id=\"id\", column_sort=\"date_block_num\")\ndf_features.shape","b695a0c0":"print(*df_features.columns, sep=\"\\n\")","98f7f716":"df_features.isna().sum().sort_values()","c60070ff":"# Therefore, we eliminate all features that are filled with less than 90% \ndf_features = df_features.loc[:, df_features.isnull().mean() < .1]\ndf_features.shape","8f899fc1":"df_features.reset_index(inplace=True)\ndf_features.head()","34e57584":"# lets split the id back into shop id and item id so that we can merge it to the original dataframe\ndf_features['shop_id'] = df_features['level_0'].str[:2]\ndf_features['item_id'] = df_features['level_0'].str[2:]\n\ndf_features['item_id'] = df_features.item_id.astype(int)\ndf_features['shop_id'] = df_features.shop_id.astype(int)\n\ndf_features = df_features.rename(columns={'level_1': 'date_block_num'})\ndf_features.drop(columns=['level_0'], inplace=True)","86354775":"# this will generate the features as a lag feature\ndf_features['date_block_num'] += 1","127cfa42":"matrix_small.drop(columns=['item_id_even', 'id', 'shop_id_even'], inplace=True)\ndata = matrix_small.merge(df_features, on=[\"item_id\", \"shop_id\", \"date_block_num\"], how=\"left\")","40f4bacc":"import gc\nimport pickle\nfrom xgboost import XGBRegressor\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4","0a731cae":"data[data[\"date_block_num\"]==34].shape","b5d62d48":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","5c99cc55":"Y_train = Y_train.clip(0, 20)\nY_valid = Y_valid.clip(0, 20)","bbc0a3bc":"del data\ngc.collect();","9c31f8b5":"ts = time.time()\n\nmodel = XGBRegressor(\n    n_estimators=1000\n)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 20)\n\ntime.time() - ts","81e6d43e":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nX_test['pred'] = Y_test\ntest = test.merge(X_test, on=['shop_id', 'item_id'], how=\"left\")\n\nsubmission = pd.DataFrame({\n    \"ID\": test.ID, \n    \"item_cnt_month\": test['pred']\n})\nsubmission.to_csv('xgb_submission.csv', index=False)","b1b0eee8":"import shap\nshap.initjs()\n\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_valid, approximate=True)\n\nshap.summary_plot(shap_values, X_valid)","dafa85c5":"# Modelling","2fd4382f":"We'll remove the obvious outliers in the dataset - the items that sold more than 1000 in one day and the item with price greater than 300,000.","e574f183":"Create a test set for month 34.","b701a72d":"# Introduction\n\nThis notebook is based on the superb notebook by Gordon Henderson: https:\/\/www.kaggle.com\/gordotron85\/future-sales-xgboost-top-3 . Unlike him we will save the time for laborious feature engineering and use an automatic feature extraction for time series data provided by tsfresh (https:\/\/tsfresh.readthedocs.io\/en\/latest\/).\n\nDue to resource limitations in kaggle I will only showcase how to generate the features based on a subsample of the data. When I tried to run the rolling_frame function on the complete train set it took forever. Maybe dask dataframes could be an option here and I plan to explore this in future versions -> https:\/\/tsfresh.readthedocs.io\/en\/latest\/text\/large_data.html","b9aea93d":"Next, we slide a window of size 6 over the entire dataframe. Tsfresh provides a very useful method that generates a new dataframe for us. This method generates a new id that is a tuple of a) the id we created and b) the maximum date_block_num considered in the window. ","29fc288c":"# Load Data","0e6228a8":"# 1. Data Cleaning\n\nWe'll remove outliers, clean up some of the raw data and add some new variables to it.","436dfcfa":"Arguably, that are too many columns and also not all features contain values as we look at very short time windows. ","5d86a57e":"Next, let's downsample our data and only look at 15 different shops and 25 items","2f3e63e7":"# Feature engineering \n\nFor feature engineering we will use the features generated by the tsfresh package. As this notebook is designed to only show the basic approach of using this package and as we deal with quite a lot of data here, we will sample a subset of the whole data to show how to use the package. ","308daab7":"# Preprocessing\n\nCreate a matrix df with every combination of month, shop and item in order of increasing month. Item_cnt_day is summed into an item_cnt_month.\n\nWith tsfresh we will later slide a window over every time series to create feature for that window. This means we have to create one entry for every shop_id and item_id for every date_block_num so that the windows contain even time spaces. ","eb31dd4e":"Concatenate train and test sets.","33dbc1ba":"Use month 34 as validation for training.","d79fdc5b":"First we have to bring the data into the format that tsfresh requires. \n\nThat is, we need to have a dataframe with at least three columns:\n- id column identifiying one timeseries, in our case a combination from shop and item id\n- time column identifying the time step, in our case date block num\n- observation value: in our case the sales of the month\n\nLet's create ad ID column that distinguishes an item_id shop_id combination unambigously. ","13bcd8ec":"# Remove outliers","797418f7":"That leaves us with 208 features. Not bad.\n\nNext, we will now merge this data again onto our dataframe to create the training dataframe","9ac65dea":"Next, we let tsfresh run its magic and create the features for the time series. In total, 779 features are created for our timeseries. "}}