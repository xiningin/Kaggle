{"cell_type":{"28a6204a":"code","0e13bc95":"code","26fcec19":"code","cea6c10c":"code","d1facb55":"code","65db342d":"code","21a324c3":"code","a1d1ac47":"code","1443ab90":"code","33ba207f":"code","64636808":"code","f0af70da":"code","fc795f50":"code","b4757da1":"code","206c8404":"code","761d63df":"code","d1e1b4b9":"code","33f41041":"code","1445cb4a":"code","c81b5e0f":"code","b69e82c6":"markdown","e841a2a8":"markdown","5f9ead06":"markdown","4f1ec004":"markdown","5c6fc219":"markdown"},"source":{"28a6204a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0e13bc95":"df = pd.read_csv(\"\/kaggle\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\/5802.csv\", low_memory=False)\nprint(df.shape)\ndf.head()","26fcec19":"#Code by Parul Pandey  https:\/\/www.kaggle.com\/parulpandey\/a-guide-to-handling-missing-values-in-python\n\n\nfrom sklearn.impute import SimpleImputer\ndf_most_frequent = df.copy()\n#setting strategy to 'mean' to impute by the mean\nmean_imputer = SimpleImputer(strategy='most_frequent')# strategy can also be mean or median \ndf_most_frequent.iloc[:,:] = mean_imputer.fit_transform(df_most_frequent)","cea6c10c":"df_most_frequent.isnull().sum()","d1facb55":"# Data processing, metrics and modeling\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom datetime import datetime\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\nfrom sklearn import metrics\n# Lgbm\nimport lightgbm as lgb\nimport catboost\nfrom catboost import Pool\nimport xgboost as xgb\n\n# Suppr warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")","65db342d":"#fill in mean for floats\nfor c in df_most_frequent.columns:\n    if df_most_frequent[c].dtype=='float16' or  df_most_frequent[c].dtype=='float32' or  df_most_frequent[c].dtype=='float64':\n        df_most_frequent[c].fillna(df[c].mean())\n\n#fill in -999 for categoricals\ndf_most_frequent = df_most_frequent.fillna(-999)\n# Label Encoding\nfor f in df_most_frequent.columns:\n    if df_most_frequent[f].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(df_most_frequent[f].values))\n        df_most_frequent[f] = lbl.transform(list(df_most_frequent[f].values))\n        \nprint('Labelling done.')","21a324c3":"import seaborn as sns\nsns.distplot(df_most_frequent['time'])\nplt.title('Time Distribution');","a1d1ac47":"print('skew',df_most_frequent['time'].skew())\nprint('kurtosis',df_most_frequent['time'].kurtosis())","1443ab90":"ax = sns.heatmap(df.corr(), annot=True, fmt=\".4f\")","33ba207f":"y = df_most_frequent['time']\ndf_most_frequent = df_most_frequent.drop(['time'], axis=1)","64636808":"#Code by Josu\u00e9 Nascimento https:\/\/www.kaggle.com\/josutk\/eda-stacking-regularization\/notebook\n\ndef plot_predict(pred, true):\n    indexs = []\n    for i in range(len(pred)):\n        indexs.append(i)\n        \n\n    fig = go.Figure()\n\n    fig.add_trace(go.Line(\n        x=indexs,\n        y=pred,\n        name=\"Predict\"\n    ))\n\n    fig.add_trace(go.Line(\n        x=indexs,\n        y=true,\n        name=\"Test\"\n    ))\n\n    fig.show()","f0af70da":"from sklearn.ensemble import StackingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import LinearSVR\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Lasso \nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nX_train, X_test, y_train, y_test = train_test_split(\n    df_most_frequent, y, random_state=42\n)\n   ","fc795f50":"#Code by Josu\u00e9 Nascimento https:\/\/www.kaggle.com\/josutk\/eda-stacking-regularization\/notebook\n\nlasso_params = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]}\nlasso = Lasso(random_state=42)\nclf_lasso = GridSearchCV(lasso, lasso_params, cv=5, scoring='neg_mean_squared_error', n_jobs= 4, verbose = 1)\nclf_lasso.fit(df_most_frequent, y)\nprint(clf_lasso.best_estimator_)\nprint(clf_lasso.best_score_)","b4757da1":"#Code by Josu\u00e9 Nascimento https:\/\/www.kaggle.com\/josutk\/eda-stacking-regularization\/notebook\n\nimport lightgbm as lgb\nlightgbm_params ={'learning_rate':[0.0001, 0.001, 0.003, 0.01, 0.1],\n                  'n_estimators':[10,20, 50, 100],\n                 'max_depth':[4, 6, 10, 15, 20, 50]}\ngbm = lgb.LGBMRegressor(random_state = 42)\nclf_gbm = GridSearchCV(gbm, lightgbm_params, cv=5,  scoring='neg_mean_squared_error',n_jobs= 4, verbose = 1)\nclf_gbm.fit(df_most_frequent, y)\nprint(clf_gbm.best_estimator_)\nprint(clf_gbm.best_score_)\n# (learning_rate=0.001, max_depth=6, n_estimators=50, random_state=42)","206c8404":"scores = {}\n\ngbm = lgb.LGBMRegressor(random_state = 42, learning_rate=0.001, max_depth=6, n_estimators=50)\nmodel = gbm.fit(df_most_frequent, y)\npred = model.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_test, pred)))\nscore = 100* max(0, 1-mean_squared_error(y_test, pred))\nprint(score)\nscores['LGBM'] = score","761d63df":"#Code by Josu\u00e9 Nascimento https:\/\/www.kaggle.com\/josutk\/eda-stacking-regularization\/notebook\n\nfrom sklearn.ensemble import AdaBoostRegressor\nadam_boosting_params = {'learning_rate':[0.0001, 0.001, 0.003, 0.01, 0.1,1],\n                        'n_estimators':[10,20, 50, 100]}\nada = AdaBoostRegressor(random_state=42)\nclf_ada = GridSearchCV(ada, adam_boosting_params, cv=5,  scoring='neg_mean_squared_error',n_jobs= 4, verbose = 1)\nclf_ada.fit(df_most_frequent, y)\nprint(clf_ada.best_estimator_)\nprint(clf_ada.best_score_)\n# (learning_rate=0.0001, n_estimators=100, random_state=42)","d1e1b4b9":"#Code by Josu\u00e9 Nascimento https:\/\/www.kaggle.com\/josutk\/eda-stacking-regularization\/notebook\n\nada = AdaBoostRegressor(random_state=42, learning_rate=0.0001, n_estimators=100)\nmodel = ada.fit(X_train, y_train)\npred = model.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_test, pred)))\nscore = 100* max(0, 1-mean_squared_error(y_test, pred))\nprint(score)\nscores['ADA'] = score","33f41041":"#Code by Josu\u00e9 Nascimento https:\/\/www.kaggle.com\/josutk\/eda-stacking-regularization\/notebook\n\nfrom sklearn.svm import LinearSVR\n\nsvr_params = {'C':[0.0001, 0.001,0.01, 0.1, 1 , 10, 100]}\nsvr = LinearSVR(random_state=42)\nclf_svr = GridSearchCV(svr, svr_params, cv=5, scoring='neg_mean_squared_error', n_jobs=4, verbose=1)\nclf_svr.fit(df_most_frequent, y)\nprint(clf_svr.best_estimator_)\nprint(clf_svr.best_score_)\n# (C=0.001, random_state=42)","1445cb4a":"#Code by Josu\u00e9 Nascimento https:\/\/www.kaggle.com\/josutk\/eda-stacking-regularization\/notebook\n\nlvr = LinearSVR(C=0.001, random_state=42)\nmodel = svr.fit(X_train, y_train)\npred = model.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_test, pred)))\nscore = 100* max(0, 1-mean_squared_error(y_test, pred))\nprint(score)\nscores['SVR'] = score","c81b5e0f":"#Code by Josu\u00e9 Nascimento https:\/\/www.kaggle.com\/josutk\/eda-stacking-regularization\/notebook\n\nresult = pd.DataFrame([])\nresult['model'] = list(scores.keys())\nresult['score'] = list(scores.values())\nresult = result.sort_values(['score'], ascending=False)\nresult.head()","b69e82c6":"#I changed max_features to 2. In the original code was 4. Due to the number of columns\/features float (not integer)","e841a2a8":"#Lasso","5f9ead06":"#Label Encoding","4f1ec004":"It was suppose to have Random Forest and XGBoost in my Stacking attempt. However, I had some issues with the snippets.  Maybe I'll try in another Notebook ","5c6fc219":"![](https:\/\/mlfromscratch.com\/content\/images\/2020\/01\/model_stacking_overview-4.png)mlfromscratch.com"}}