{"cell_type":{"f75e0726":"code","7d715c23":"code","ebf26132":"code","e7155e22":"code","0c3fb95e":"code","dcb2112c":"code","72ba4f3f":"code","10f12478":"code","2e8a0fd6":"code","351c670d":"code","82d5a540":"code","66a85ded":"code","d5746415":"code","fab8f189":"code","ef1ed5f9":"code","78134ccd":"code","55d76eac":"code","ecb74368":"code","0a08ac9a":"code","0cc76181":"code","c8a37881":"code","37d3cdaf":"code","bb43a289":"code","84e8e6cf":"code","f70e6ff1":"code","94716e6d":"code","64581327":"code","7cc14d9c":"code","c6300099":"code","72e4e8ad":"code","b0705476":"markdown","8f149938":"markdown","1e3fdc8e":"markdown","8cb9a358":"markdown","07e6c1ef":"markdown","28a8de82":"markdown","81ef9746":"markdown","5fde08ac":"markdown","4e5a357d":"markdown","f69812c6":"markdown","4b692900":"markdown","92159681":"markdown","0b1622d2":"markdown"},"source":{"f75e0726":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7d715c23":"## Import required libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport IPython.display as ipd\n\n'''Librosa is a special library used for audio analysis'''\nimport librosa\nimport librosa.display","ebf26132":"#let's take a random audio file from the data.\naudio_file_path='..\/input\/urbansound8k\/fold1\/101415-3-0-2.wav'\n\n#let's view the waveplot \nplt.figure(figsize=(14,3))\ny, sr = librosa.load(audio_file_path)\nlibrosa.display.waveplot(y, sr = sr)\nipd.Audio(audio_file_path)","e7155e22":"print('Time series data :- ',y)\nprint('Sample rate :- ',sr)","0c3fb95e":"# Load the meta data\nmetadata = pd.read_csv('..\/input\/urbansound8k\/UrbanSound8K.csv')\nmetadata.head()","dcb2112c":"# check whether the dataset is imbalanced\nmetadata['class'].value_counts()","72ba4f3f":"mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\nprint(mfccs.shape)","10f12478":"# Extracting MFCC's for every audio file\nimport pandas as pd\nimport os\nimport librosa\n\naudio_dataset_path = '..\/input\/urbansound8k'\nmetadata = pd.read_csv('..\/input\/urbansound8k\/UrbanSound8K.csv')\nmetadata.head()","2e8a0fd6":"def features_extractor(file):\n    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n    mfccs_scaled_features = np.mean(mfccs_features.T, axis=0)\n    return mfccs_scaled_features","351c670d":"import numpy as np\nfrom tqdm import tqdm\n## Now we iterate through every audio file and extract features\n## using MeL-Frequency cepstral Coefficients\nextracted_features=[]\nfor index_num, row in tqdm(metadata.iterrows()):\n    file_name = os.path.join(os.path.abspath(audio_dataset_path),'fold'+str(row[\"fold\"])+'\/', str(row['slice_file_name']))\n    final_class_labels = row['class']\n    data = features_extractor(file_name)\n    extracted_features.append([data,final_class_labels])","82d5a540":"## Converting extracted_features to pandas dataframe\nextracted_features_df = pd.DataFrame(extracted_features, columns=['feature','class'])\nextracted_features_df.head()","66a85ded":"## Split the dataset into independent and dependent dataset\nX=np.array(extracted_features_df['feature'].tolist())\ny= np.array(extracted_features_df['class'].tolist())","d5746415":"X.shape","fab8f189":"## Label Encoder\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\ny = to_categorical(labelencoder.fit_transform(y))","ef1ed5f9":"y.shape","78134ccd":"## Train Test Split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test= train_test_split(X,y,test_size=0.2,random_state=42)","55d76eac":"X_train","ecb74368":"y_train","0a08ac9a":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","0cc76181":"import tensorflow as tf\nprint(tf.__version__)","c8a37881":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn import metrics","37d3cdaf":"### No of classes\nnum_labels = y.shape[1]","bb43a289":"model = Sequential()\n\n### first Layer\nmodel.add(Dense(100, input_shape=(40,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n### second Layer\nmodel.add(Dense(200))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n### third Layer\nmodel.add(Dense(100))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n\n### final Layer\nmodel.add(Dense(num_labels))\nmodel.add(Activation('softmax'))","84e8e6cf":"model.summary()","f70e6ff1":"model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')","94716e6d":"## Training my model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom datetime import datetime\n\nnum_epochs = 100\nnum_batch_size = 32\n\ncheckpointer = ModelCheckpoint(filepath='.\/', verbose = 1, save_best_only=True)\nstart = datetime.now()\n\nmodel.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data = (X_test, y_test), callbacks=[checkpointer])\n\nduration = datetime.now() - start\nprint('Training completed in time: ', duration)","64581327":"test_accuracy = model.evaluate(X_test, y_test, verbose=0)\nprint(test_accuracy[1])","7cc14d9c":"filename = '..\/input\/urbansound8k\/fold6\/108638-9-0-1.wav'\naudio, sample_rate = librosa.load(filename, res_type='kaiser_fast')\nmfccs_features = librosa.feature.mfcc(y=audio,sr=sample_rate, n_mfcc=40)\nmfccs_scaled_features = np.mean(mfccs_features.T, axis=0)\n\nprint(mfccs_scaled_features)\nmfccs_scaled_features = mfccs_scaled_features.reshape(1,-1)\nprint(mfccs_scaled_features)\nprint(mfccs_scaled_features.shape)","c6300099":"predicted_label = model.predict_classes(mfccs_scaled_features)\nprint(predicted_label)\nprediction_class = labelencoder.inverse_transform(predicted_label)\nprediction_class","72e4e8ad":"plt.figure(figsize=(14,5))\ndata, sample_rate = librosa.load(filename)\nlibrosa.display.waveplot(data, sr = sample_rate)\nipd.Audio(filename)","b0705476":"## Data Understanding and Exploration","8f149938":"The output of librosa.feature.mfcc function is the matrix, which is a numpy.ndarray of shape (n_mfcc, T) where T denotes the track duration in frames. ","1e3fdc8e":"It is a sound of dog barking.","8cb9a358":"#### Model Creation","07e6c1ef":"So, model predicted it correctly. It is street music.","28a8de82":"so, this model give 74.81% accuracy on test data.","81ef9746":"## Data Preprocessing","5fde08ac":"# Audio Classification\nObjective : Use a simple Neural Network to classify audio samples in their category based on features extracted using LIBROSA.","4e5a357d":"A `time series` is a series of data points indexed in time order.\nHere, time series of an audio signal represented as a one-dimensional numpy.ndarray of floating-point values. y[t] corresponds to amplitude of the waveform at sample t.\n\n\nA `sample rate` or sampling rate defines how many times per second a sound is sampled. The default sampling rate used by Librosa is 22050","f69812c6":"##### Feature Extration\n\nLet's extract the Mel-frequency cepstral coefficients from the raw signal y","4b692900":"#### Evaluate the model","92159681":"### Testing some test audio data\nsteps:\n- Preprocess the new audio data\n- predict the classes\n- Inverse transform your Predicted Label","0b1622d2":"This model predicted given audio file as `street_music`. Let's check out what audio it is."}}