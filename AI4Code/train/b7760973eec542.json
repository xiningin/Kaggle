{"cell_type":{"298a8c19":"code","1534a7b0":"code","ea4826ca":"code","003c450c":"code","35933739":"code","f58c0375":"code","71430ef9":"code","09eab262":"code","8ac5eef0":"code","49e7047c":"code","2cee1ff5":"code","4c8c53e5":"code","2b3d12a4":"code","08111161":"code","11f1c65c":"code","fc1a5457":"code","5c742ef6":"code","aaffc686":"code","1d1f00a7":"code","4395cabe":"code","a11f57da":"code","8dc84493":"code","80889c38":"code","9b16e780":"code","12f4e0f0":"code","02b478df":"code","808131ee":"code","f0e5b0d2":"code","c6424a16":"code","245fdf84":"markdown","ee31980d":"markdown","120691a4":"markdown","ef55c3d1":"markdown","3629bf92":"markdown","2ad85c0a":"markdown","f1bd67ab":"markdown","329632dc":"markdown","5f071f2a":"markdown","f8aa6141":"markdown"},"source":{"298a8c19":"#Define constant values here\nLR = 0.003\nBATCH_SIZE = 64\nCLASSES = 10\nEPOCHS = 100\nWEIGHT_DECAY = 0.001\nMOMENTUM = 0.9\nPRINT_ON = False\n\nimport torch\nimport numpy as np\nfrom torch.utils.data import SubsetRandomSampler\n\n#Start simple: when we know nothing about the data, may be best to try a simple Linear Classifier!\nclass Baseline(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer = torch.nn.Linear(100, CLASSES)\n    \n    def forward(self, inputs):\n        return self.layer(inputs)\n\n#The data is provided as .npy files. Hence, we need to create a Dataset class, \n#so we can use a Dataloader to create batches from our data.\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, X, y):\n        assert len(X) == len(y)\n        self.X = X\n        self.y = y.astype(int)\n    \n    def __getitem__(self, index):\n        return self.X[index], self.y[index]\n    \n    def __len__(self):\n        return len(self.y)\n\n#Use a GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n#Cross Entropy Loss to train the model as this is a 10 Class Classification problem\ncriterion = torch.nn.CrossEntropyLoss()","1534a7b0":"from sklearn.model_selection import train_test_split\n#Read the training data provided. y is a 1-D array with values from 0 to 9\nx = np.load(\"\/kaggle\/input\/aicore-classification\/X_train.npy\")\ny = np.load(\"\/kaggle\/input\/aicore-classification\/y_train.npy\")\ny = y.astype(int)\nprint(x.shape)\n\n#Create Datasets - one for val and another for train\ndata = Dataset(x, y)\nval = Dataset(x, y)\n\n#Get indices from train dataset for stratified sampling (on y). \n#These indices are used in the dataloders for train and validation respectively\ntrain_indices, val_indices = train_test_split(\nnp.arange(len(data)),\ntest_size=0.2,\nshuffle=True,\nstratify=y)\n\n#Create dataloaders for train and validation - indices used from the split\ndataloaders = {\n    \"train\": torch.utils.data.DataLoader(\n        data,\n        batch_size = BATCH_SIZE,\n        pin_memory = torch.cuda.is_available(),\n        sampler=SubsetRandomSampler(train_indices)\n    ),\n    \"validation\": torch.utils.data.DataLoader(\n        val,\n        batch_size = BATCH_SIZE,\n        pin_memory = torch.cuda.is_available(),\n        sampler=SubsetRandomSampler(val_indices)\n    ),\n}","ea4826ca":"#This function computes the accuracy of prediction between logits from model output and true labels\ndef accuracy(logits, y):\n    return torch.mean((torch.argmax(logits, dim = 1) == y).float())\n\n#This function computes the cross entropy loss\ndef loss(logits, y):\n    return torch.nn.functional.cross_entropy(logits, y, reduction=\"mean\")\n\n#This function computes the confusion matrix. X-axis labels are the predicted labels, \n#Y-axis labels are True labels\ndef conf_matrix(logits, y):\n    y = y.to(\"cpu\")\n    y_np = y.numpy()\n    ypred = torch.argmax(logits, dim=1)\n    ypred = ypred.to(\"cpu\")\n    ypred_np = ypred.numpy()\n    confusion_mat = np.zeros((CLASSES,CLASSES))\n    for i,j in enumerate(range(len(y_np))):\n        confusion_mat[ypred_np[i],y_np[j]] +=1\n    return confusion_mat.astype(int)\n\n\ndef train(dataloader, device, model, criterion, optimizer):\n    model.train()\n    counter = 0\n    accuracy_train = 0\n    loss_train = 0\n    for (X, y) in dataloader:\n        counter += 1\n        X = X.to(device)\n        y = y.to(device)\n        outputs = model(X)\n        loss_val = criterion(outputs, y)\n        loss_val.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        loss_val.detach()\n        accuracy_train += accuracy(outputs, y)\n        loss_train += loss(outputs, y)\n    if PRINT_ON:\n        print((accuracy_train\/counter).cpu().numpy());\n    return accuracy_train\/counter, loss_train\n\ndef evaluate(dataloader, device, model, criterion, optimizer):\n    model.eval()\n    with torch.no_grad():\n        counter = 0\n        accuracy_val = 0\n        loss_val = 0\n        conf_val = np.zeros((CLASSES,CLASSES))\n        for (X, y) in dataloader:\n            counter += 1\n            X = X.to(device)\n            y = y.to(device)\n            outputs = model(X)\n            conf_val += conf_matrix(outputs, y)\n            accuracy_val += accuracy(outputs, y)\n            loss_val += loss(outputs, y)\n        if PRINT_ON:\n            print((accuracy_val\/counter).cpu().numpy());\n        return accuracy_val\/counter, conf_val, loss_val","003c450c":"def RunCode(model, optimizer):\n    prev_acc = 0\n    acc_lst = []\n    loss_lst = []\n    loss_train_lst = []\n    for epoch in range(EPOCHS):\n        if PRINT_ON:\n            print(f\"===================== EPOCH {epoch} ===================\")\n            print(f\"=================== TRAINING ====================\")\n        acc_train, loss_train = train(dataloaders[\"train\"],device, model, criterion, optimizer)\n        loss_train_lst.append(loss_train)\n        if PRINT_ON:\n            print(f\"================== VALIDATION ===================\")\n        acc, conf, loss_eval = evaluate(dataloaders[\"validation\"],device, model, criterion, optimizer)\n        acc_lst.append(acc)\n        loss_lst.append(loss_eval)\n        if(acc > prev_acc):\n            prev_acc = acc\n            best_epoch = epoch\n            torch.save(model.state_dict(), \"model\")\n\n    model.load_state_dict(torch.load(\"model\"))\n    acc, conf, loss_chk = evaluate(dataloaders[\"validation\"],device, model, criterion, optimizer)\n    print(f\"Best Model metrics: Epoch no: {best_epoch} \") \n    print(f\"Accuracy:\", acc)\n    print(conf)\n","35933739":"model = Baseline().to(device)\n#SGD optimizer - seems to be the most common first choice\noptimizer = torch.optim.SGD(model.parameters(), lr=LR)\nRunCode(model, optimizer)","f58c0375":"#Accuracy of about 30% -- model is possibly too simple. \n#So we move to a slightly more complex network.\nclass NeuralNw(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = torch.nn.Sequential(\n            torch.nn.Linear(100, 50),\n            torch.nn.ReLU(),\n            torch.nn.BatchNorm1d(50),\n            torch.nn.Linear(50, 25),\n            torch.nn.ReLU(),\n            torch.nn.BatchNorm1d(25),\n            torch.nn.Linear(25, 10),\n        )\n    def forward(self, inputs):\n        return self.model(inputs)","71430ef9":"model = NeuralNw().to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=LR)\nRunCode(model, optimizer)","09eab262":"#Good improvement in accuracy\n#Trying with an Adam regularizer\nmodel = NeuralNw().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\nRunCode(model, optimizer)","8ac5eef0":"#Good improvement again - improved training accuracy and test accuracy. \n#Lot of discrepancy between the training and val accuracy. Hence trying a regularizer (L2)\nmodel = NeuralNw().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay = WEIGHT_DECAY)\nRunCode(model, optimizer)","49e7047c":"#I next try to use XGBoost for a variation in my approach\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nxgb_model = xgb.XGBClassifier(objective='multi:softmax',\n                          base_score=0.5, \n                          booster='gbtree', \n                          colsample_bytree=0.3, \n                          gamma=1,\n                          learning_rate=0.03, \n                          max_depth=8,\n                          n_estimators=1000,\n                          reg_alpha=0, reg_lambda=1, \n                          subsample=0.8\n                          )\nxgb_model.fit(x[train_indices], y[train_indices])\n\ny_pred = xgb_model.predict(x[val_indices])\n    \nprint(confusion_matrix(y[val_indices], y_pred))\nprint(accuracy_score(y[val_indices], y_pred))","2cee1ff5":"#Do we get some insight from the max and min values?\nprint(f\"Max value in the data is {np.max(x)}\")\nprint(f\"Min value in the data is {np.min(x)}\")","4c8c53e5":"#What does the boxplot say?\nimport matplotlib.pyplot as plt \nplt.boxplot(x);\nplt.show()\nplt.boxplot(x[:,0:30]);\nplt.show()","2b3d12a4":"#Could it have been an image? NO - from the plots below\nf, axarr = plt.subplots(1,2)\naxarr[0].imshow(np.reshape(x[0],(10,10)))\naxarr[1].imshow(np.reshape(x[10],(10,10)))\nplt.show()","08111161":"f, axarr = plt.subplots(2,2)\naxarr[0,0].plot(x[2])\naxarr[0,1].plot(x[14])\naxarr[1,0].plot(x[0])\naxarr[1,1].plot(x[17])\nplt.show()\nprint(f'Data class for sample 2: {y[2]}, for sample 14: {y[14]}')\nprint(f'Data class for sample 0: {y[0]}, for sample 17: {y[17]}')","11f1c65c":"!pip -q install sktime","fc1a5457":"from sklearn.linear_model import RidgeClassifierCV\nfrom sklearn.pipeline import make_pipeline\nfrom sktime.transformations.panel.rocket import Rocket\nfrom sklearn.metrics import confusion_matrix\n\n#For time series models, the format generally is (batch_size, time_step, features). \n#Hence reshaping inputs to that format here.\nX_timeseries = np.reshape(x, (-1,1,100))\n\nrocket = Rocket()  # by default, ROCKET uses 10,000 kernels\nX_train_data = X_timeseries[train_indices].astype(int)\ny_train_data = y[train_indices].astype(int)\nrocket.fit(X_train_data)\nX_train_transform = rocket.transform(X_train_data)\n\nclassifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\nclassifier.fit(X_train_transform, y_train_data)\n\nX_val_data = X_timeseries[val_indices].astype(int)\ny_val_data = y[val_indices].astype(int)\nX_val_transform = rocket.transform(X_val_data)\ny_pred = classifier.predict(X_val_transform)\n\nprint(classifier.score(X_val_transform, y_val_data))\nprint(confusion_matrix(y_val_data, y_pred))","5c742ef6":"#Checking how the spectrogram looks\nfrom scipy import signal\ng, axarr = plt.subplots(2,2)\nplt.ylabel('Time [sec]')\nplt.xlabel('Frequency [Hz]')\nsample = 2\nf, t, Sxx = signal.spectrogram(x[sample], nperseg=50,noverlap=23)\naxarr[0,0].pcolormesh(f, t, Sxx.T, shading='gouraud')\n\nsample = 14\nf, t, Sxx = signal.spectrogram(x[sample], nperseg=50,noverlap=23)\naxarr[0,1].pcolormesh(f, t, Sxx.T, shading='gouraud')\n\nsample = 0\nf, t, Sxx = signal.spectrogram(x[sample], nperseg=50,noverlap=23)\naxarr[1,0].pcolormesh(f, t, Sxx.T, shading='gouraud')\n\nsample = 17\nf, t, Sxx = signal.spectrogram(x[sample], nperseg=50,noverlap=23)\naxarr[1,1].pcolormesh(f, t, Sxx.T, shading='gouraud')\n\n\nplt.show()\nprint(f'Data class for sample 2: {y[2]}, for sample 14: {y[14]}')\nprint(f'Data class for sample 0: {y[0]}, for sample 17: {y[17]}')\n\nprint(f'The number of frequency steps: {f.shape[0]}, number of time steps: {t.shape[0]}')\nprint(f'Frequency steps are: {f},\\n Time steps are: {t}')\n\n#In order to get an equal number of steps in the time and frequency domain, as I am not sure which\n#one could be more important, I have modified to use the same number of steps here.\n\ng, axarr = plt.subplots(2,2)\nplt.ylabel('Time [sec]')\nplt.xlabel('Frequency [Hz]')\nsample = 2\nf, t, Sxx = signal.spectrogram(x[sample], nperseg=28,noverlap=23)\naxarr[0,0].pcolormesh(f, t, Sxx.T, shading='gouraud')\n\nsample = 14\nf, t, Sxx = signal.spectrogram(x[sample], nperseg=28,noverlap=23)\naxarr[0,1].pcolormesh(f, t, Sxx.T, shading='gouraud')\n\nsample = 0\nf, t, Sxx = signal.spectrogram(x[sample], nperseg=28,noverlap=23)\naxarr[1,0].pcolormesh(f, t, Sxx.T, shading='gouraud')\n\nsample = 17\nf, t, Sxx = signal.spectrogram(x[sample], nperseg=28,noverlap=23)\naxarr[1,1].pcolormesh(f, t, Sxx.T, shading='gouraud')\n\n\nplt.show()\nprint(f'Data class for sample 2: {y[2]}, for sample 14: {y[14]}')\nprint(f'Data class for sample 0: {y[0]}, for sample 17: {y[17]}')\n\nprint(f'The number of frequency steps: {f.shape[0]}, number of time steps: {t.shape[0]}')\nprint(f'Frequency steps are: {f},\\n Time steps are: {t}')","aaffc686":"#2D CNN Using Resnet18 for the classification\nfrom sklearn.model_selection import train_test_split\n\ndata_list = []\nfor sample in range(x.shape[0]):\n    f, t, Sxx = signal.spectrogram(x[sample], nperseg=28,noverlap=23)\n    data_list.append(Sxx)\ndata_array = np.array(data_list)\nXSpectro = np.reshape(data_array, (-1, 1, 15,15))\n#Create Datasets - one for val and another for train\ndata = Dataset(XSpectro, y)\nval = Dataset(XSpectro, y)\n\n#Get indices from train dataset for stratified sampling (on y). \n#These indices are used in the dataloders for train and validation respectively\ntrain_indices, val_indices = train_test_split(\nnp.arange(len(data)),\ntest_size=0.2,\nshuffle=True,\nstratify=y)\n\n#Create dataloaders for train and validation - indices used from the split\ndataloaders = {\n    \"train\": torch.utils.data.DataLoader(\n        data,\n        batch_size = BATCH_SIZE,\n        pin_memory = torch.cuda.is_available(),\n        sampler=SubsetRandomSampler(train_indices)\n    ),\n    \"validation\": torch.utils.data.DataLoader(\n        val,\n        batch_size = BATCH_SIZE,\n        pin_memory = torch.cuda.is_available(),\n        sampler=SubsetRandomSampler(val_indices)\n    ),\n}","1d1f00a7":"import torchvision\nEPOCHS = 15 # Used lower number of epochs as this one takes very long to run\nLR = 0.0008\nmodel = torch.nn.Sequential(\n    torch.nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, padding=1),\n    torchvision.models.resnet18(num_classes=10),\n).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay = WEIGHT_DECAY)\n\nRunCode(model, optimizer)","4395cabe":"validation_percent = 0.2\nBATCH_SIZE = 32\nEPOCHS = 20 \noptimizers = \"Adam\"\nWEIGHT_DECAY = 0.001\nMOMENTUM = 0.9\nLR = 0.0009\nCLASSES = 10","a11f57da":"import torch.nn.functional as F\n\nclass Rnn(torch.nn.Module):\n    def __init__(self):\n        super(Rnn,self).__init__()\n\n        self.rnn = torch.nn.LSTM(\n            input_size=100,\n            hidden_size=256,\n            num_layers=3,\n            batch_first = True,  # when(time_step, batch, input) is Ture\n        )\n        self.out = torch.nn.Sequential(torch.nn.Linear(256,64),\n                                       torch.nn.Linear(64,10))\n\n    def forward(self, x):\n        r_out,(h_n,h_c) = self.rnn(x,None)  # x (batch,time_step,input_size)\n        out = self.out(r_out[:,-1,:]) #(batch,time_step,input)\n        return out\n\nmodel = Rnn().to(device)\n\nif optimizers == \"SGD\":\n    optimizer = torch.optim.SGD(model.parameters(), lr=LR, weight_decay = WEIGHT_DECAY, momentum = MOMENTUM)\nelse:\n    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay = WEIGHT_DECAY)\n\ncriterion = torch.nn.CrossEntropyLoss()","8dc84493":"from sklearn.model_selection import train_test_split\nxrnn = np.reshape(x, (-1, 1, 100))\n#Create Datasets - one for val and another for train\ndata = Dataset(xrnn, y)\nval = Dataset(xrnn, y)\n\n#Get indices from train dataset for stratified sampling (on y). \n#These indices are used in the dataloders for train and validation respectively\ntrain_indices, val_indices = train_test_split(\nnp.arange(len(data)),\ntest_size=0.2,\nshuffle=True,\nstratify=y)\n\n#Create dataloaders for train and validation - indices used from the split\ndataloaders = {\n    \"train\": torch.utils.data.DataLoader(\n        data,\n        batch_size = BATCH_SIZE,\n        pin_memory = torch.cuda.is_available(),\n        sampler=SubsetRandomSampler(train_indices)\n    ),\n    \"validation\": torch.utils.data.DataLoader(\n        val,\n        batch_size = BATCH_SIZE,\n        pin_memory = torch.cuda.is_available(),\n        sampler=SubsetRandomSampler(val_indices)\n    ),\n}","80889c38":"RunCode(model, optimizer)","9b16e780":"validation_percent = 0.2\nBATCH_SIZE = 32\nEPOCHS = 100\noptimizers = \"Adam\"\nWEIGHT_DECAY = 0.001\nMOMENTUM = 0.9\nLR = 0.0009\nCLASSES = 10","12f4e0f0":"import torch.nn.functional as F\n\nclass ConvNet(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_layers = torch.nn.Sequential(\n            torch.nn.Conv1d(1, 16, kernel_size=3,stride=1,bias = True),\n            torch.nn.ReLU(),\n            torch.nn.Conv1d(16, 16, kernel_size=3,stride=1, bias = True),\n            torch.nn.ReLU(),\n            torch.nn.AvgPool1d(2, stride=2),\n        )\n        self.fc_layers = torch.nn.Sequential(\n            torch.nn.Linear(768, 512),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(p=0.1),\n            torch.nn.Linear(512, 128),\n            torch.nn.ReLU(),\n            torch.nn.Linear(128, 10),\n        )\n        \n    def forward(self, x):\n        x = self.conv_layers(x)# pass through conv layers\n        #print(x.shape)\n        #x = torch.nn.functional.avg_pool1d(x, 4)\n        x = x.view(x.shape[0], -1)# flatten output ready for fully connected layer\n        x = self.fc_layers(x)# pass through fully connected layer\n        #x = F.softmax(x, dim=1)# softmax activation function on outputs\n        return x\n    \nmodel = ConvNet().to(device)\n\nif optimizers == \"SGD\":\n    optimizer = torch.optim.SGD(model.parameters(), lr=LR, weight_decay = WEIGHT_DECAY, momentum = MOMENTUM)\nelse:\n    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay = WEIGHT_DECAY)\n\ncriterion = torch.nn.CrossEntropyLoss()","02b478df":"from sklearn.model_selection import train_test_split\nxorg = np.load(\"\/kaggle\/input\/aicore-classification\/X_train.npy\")\ny = np.load(\"\/kaggle\/input\/aicore-classification\/y_train.npy\")\ny = y.astype(int)\n\n#mean = np.mean(xorg, axis = 0, keepdims = True)\n#std = np.std(xorg, axis = 0, keepdims = True)\n#x = (xorg - mean)\/ std\nminval = np.min(xorg)\nmaxval = np.max(xorg)\nx = (xorg - minval)\/(maxval-minval)\nx = xorg #Comment this line to run with the normalization enabled\n#The data for a Conv Net is of the form (batch_size, number of layers, number of features)\n#Hence reshape to the required format\nx_1D = np.reshape(x, (-1, 1, 100))\n\ndata = Dataset(x_1D, y)\nval = Dataset(x_1D, y)\n\ntrain_indices, val_indices = train_test_split(\nnp.arange(len(data)),\ntest_size=0.2,\nstratify=y)\n\ndataloaders = {\n    \"train\": torch.utils.data.DataLoader(\n        data,\n        batch_size = BATCH_SIZE,\n        pin_memory = torch.cuda.is_available(),\n        sampler=SubsetRandomSampler(train_indices)\n    ),\n    \"validation\": torch.utils.data.DataLoader(\n        val,\n        batch_size = BATCH_SIZE,\n        pin_memory = torch.cuda.is_available(),\n        sampler=SubsetRandomSampler(val_indices)\n    ),\n}","808131ee":"prev_acc = 0\nacc_lst = []\nloss_lst = []\nloss_train_lst = []\nlr_updated = LR\nfor epoch in range(EPOCHS):\n    if PRINT_ON:\n        print(f\"===================== EPOCH {epoch} ===================\")\n        print(f\"=================== TRAINING ====================\")\n    acc_train, loss_train = train(dataloaders[\"train\"],device, model, criterion, optimizer)\n    loss_train_lst.append(loss_train)\n    if PRINT_ON:\n        print(f\"================== VALIDATION ===================\")\n    acc, conf, loss_eval = evaluate(dataloaders[\"validation\"],device, model, criterion, optimizer)\n    acc_lst.append(acc)\n    loss_lst.append(loss_eval)\n    if(acc > prev_acc):\n        prev_acc = acc\n        best_epoch = epoch\n        torch.save(model.state_dict(), \"model\")\n    \n    if epoch%40 == 0 and epoch != 0:\n        lr_updated = lr_updated \/10\n        for param_group in optimizer.param_groups:\n            if PRINT_ON:\n                print(f\"Learning rate updated to {lr_updated}\")\n            param_group['lr'] = lr_updated\nmodel.load_state_dict(torch.load(\"model\"))\nacc, conf, loss_chk = evaluate(dataloaders[\"validation\"],device, model, criterion, optimizer)\nprint(f\"Best Model metrics: Epoch no: {best_epoch} \") \nprint(f\"Accuracy:\", acc)\nprint(conf)\n","f0e5b0d2":"import matplotlib.pyplot as plt\nplt.plot(acc_lst);\nplt.show()\n\nplt.plot(loss_lst);\nplt.show()\nplt.plot(loss_train_lst);\nplt.show()","c6424a16":"t = np.load(\"\/kaggle\/input\/aicore-classification\/X_test.npy\")\n#t = (t-minval)\/(maxval-minval)\nt = t.reshape((-1,1,100))\ntest = torch.from_numpy(t)\ntest = test.to(device)\ny_test = model(test)\ny_test = torch.argmax(y_test, dim=1)\ny_test = y_test.cpu().numpy()\n\nimport pandas as pd\npd.DataFrame({\"Id\": np.arange(len(y_test)), \"Category\": y_test}).astype(int).to_csv(\n                \"solution.csv\", index=False)","245fdf84":"Tried standardization and normalization - normalization worked better.\nThe training process for this model is a little weird. By including the standardization, the training is not able to start, that is, the model predicts any one class randomly and gets stuck at around 10% accuracy (since there are 10 classes, 1 class right is 10%). In order to break this jinx, I first trained the network without the normalization code. Next, I run the below cell by including normalization and I was able to get 0.47606 accuracy on the public leaderboard (with this method).\nSo, my understanding is that the key was the starting point, the initialization of weights. The random initialization caused the gradient descent to never reach the minima. When I had the weights initialized differently by running without normalization, the network was able to get to the minima.","ee31980d":"At this point I played around with the Learning rate, weight decay, batch size, used Momentum,and came up with the most optimum settings for the network","120691a4":"Accuracy too low. I next explored converting the data to frequency domain. \nA spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. (Wikipedia). I tried several compinations of time steps and frequency steps to understand what may be a good representation of the possible frequency components in the data. I next used a 2D CNN to classify the generated spectrograms.","ef55c3d1":"No major change in the accuracy with XGBoost as well. The above model was a result of hyperparameter tuning based on my reading on XGBoost, not grid search.\nHere's where I start to wonder:<br> \n* What could the data have been?\n* Should I add more layers? \n* Should I only tune my hyperparameters for this one?\n* Or maybe I have my architecture all wrong","3629bf92":"Accuracy not good. Could be best to try 1D CNN to capture any kind of relationship between the features, even if it is not time-series.\nHere, I also tuned the hyperparameters (LR, Momentum, Batch Size, Weight Decay) and used a step LR scheduler. I also varied the steps for changing LR, factor to divide the LR at each step. The resulting code is provided below.","2ad85c0a":"# **Analysis of the AiCore Classification problem - Part 1**# \nThis in-class AiCore Classification competition was part of the Machine Learning Course offered by the AiCore.\n\nData:\nTraining: 10000 labelled samples, 100 features<br>\nTest: 40000 samples split as 10000 (Public), 30000 (Private) <br><br>\nSource of the data: Unknown <br>\nWhat is the data\/ generating distribution: Unknown <br>\n\nUsed GPUs from Google Collab and Paperspace for training the different models in this Notebook\n","f1bd67ab":"Also, in all of the above codes, I reduced the validation split to only be 0.01% instead of the 20% used in the codes above, so as to use more training data. This resulted in at least 1% higher accuracy than the original code. I will next post a part 2 Notebook where I will discuss the tuning that was done to increase accuracy on the leaderboard.","329632dc":"The next code is to use an LSTM model for the classification.","5f071f2a":"The first 2 plots are for class 9 and second 2 for class 6. I could observe some similarity and similar frequency components in the two. Could it have been some summation of several sinusoids and noise? So, I felt the next best try would be Time Series models. <br>\nThe internet says ROCKET transform can work magic on Time Series data. So I try that first, on the lookout of magic\nhttps:\/\/medium.com\/towards-artificial-intelligence\/rocket-fast-and-accurate-time-series-classification-f54923ad0ac9","f8aa6141":"The plots above show some definite trend. There is some relationship between the different features. I guess this to either be a Time Series relationship or an image, or some other relationship between the pixels. Some more plots next, an attempt to uncover this relationship."}}