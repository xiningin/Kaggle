{"cell_type":{"bd51d943":"code","995e2d26":"code","d42ea08c":"code","36a62495":"code","1a2be54f":"code","a8954495":"code","f167453b":"code","0686ee9e":"code","fd99fcd7":"code","2be47980":"code","4dd45ed5":"markdown","87e31062":"markdown","33c6cf60":"markdown","805c4a32":"markdown","33c9e24b":"markdown","e9c826c7":"markdown","501daf09":"markdown","05801ad6":"markdown","e45e93b4":"markdown"},"source":{"bd51d943":"#basic tools \nimport os\nimport numpy as np\nimport pandas as pd\nimport warnings\n\n#tuning hyperparameters\nfrom bayes_opt import BayesianOptimization\nfrom skopt  import BayesSearchCV \n\n#graph, plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#building models\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nimport time\nimport sys\n\n#metrics \nfrom sklearn.metrics import roc_auc_score, roc_curve\nimport shap\nwarnings.simplefilter(action='ignore', category=FutureWarning)","995e2d26":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","d42ea08c":"%%time\ntrain= reduce_mem_usage(pd.read_csv(\"..\/input\/santander-customer-transaction-prediction-dataset\/train.csv\"))\ntest= reduce_mem_usage(pd.read_csv(\"..\/input\/santander-customer-transaction-prediction-dataset\/test.csv\"))\nprint(\"Shape of train set: \",train.shape)\nprint(\"Shape of test set: \",test.shape)","36a62495":"y=train['target']\nX=train.drop(['ID_code','target'],axis=1)","1a2be54f":"%%time\n\ndef bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=3, random_seed=6,n_estimators=10000, output_process=False):\n    # prepare data\n    train_data = lgb.Dataset(data=X, label=y, free_raw_data=False)\n    # parameters\n    def lgb_eval(learning_rate,num_leaves, feature_fraction, bagging_fraction, max_depth, max_bin, min_data_in_leaf,min_sum_hessian_in_leaf,subsample):\n        params = {'application':'binary', 'metric':'auc'}\n        params['learning_rate'] = max(min(learning_rate, 1), 0)\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        params['max_depth'] = int(round(max_depth))\n        params['max_bin'] = int(round(max_depth))\n        params['min_data_in_leaf'] = int(round(min_data_in_leaf))\n        params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n        params['subsample'] = max(min(subsample, 1), 0)\n        \n        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=['auc'])\n        return max(cv_result['auc-mean'])\n     \n    lgbBO = BayesianOptimization(lgb_eval, {'learning_rate': (0.01, 1.0),\n                                            'num_leaves': (24, 80),\n                                            'feature_fraction': (0.1, 0.9),\n                                            'bagging_fraction': (0.8, 1),\n                                            'max_depth': (5, 30),\n                                            'max_bin':(20,90),\n                                            'min_data_in_leaf': (20, 80),\n                                            'min_sum_hessian_in_leaf':(0,100),\n                                           'subsample': (0.01, 1.0)}, random_state=200)\n\n    \n    #n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n    #init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.\n    \n    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n    \n    model_auc=[]\n    for model in range(len( lgbBO.res)):\n        model_auc.append(lgbBO.res[model]['target'])\n    \n    # return best parameters\n    return lgbBO.res[pd.Series(model_auc).idxmax()]['target'],lgbBO.res[pd.Series(model_auc).idxmax()]['params']\n\nopt_params = bayes_parameter_opt_lgb(X, y, init_round=5, opt_round=10, n_folds=3, random_seed=6,n_estimators=10000)","a8954495":"opt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\nopt_params[1]['max_depth'] = int(round(opt_params[1]['max_depth']))\nopt_params[1]['min_data_in_leaf'] = int(round(opt_params[1]['min_data_in_leaf']))\nopt_params[1]['max_bin'] = int(round(opt_params[1]['max_bin']))\nopt_params[1]['objective']='binary'\nopt_params[1]['metric']='auc'\nopt_params[1]['is_unbalance']=True\nopt_params[1]['boost_from_average']=False\nopt_params=opt_params[1]\nopt_params\n","f167453b":"%%time \n\ntarget=train['target']\nfeatures= [c for c in train.columns if c not in ['target','ID_code']]\n\n\nfolds = StratifiedKFold(n_splits=10, shuffle=True, random_state=31416)\noof = np.zeros(len(train))\npredictions = np.zeros(len(test))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 15000\n    clf = lgb.train(opt_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=500, early_stopping_rounds = 250)\n    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))\n","0686ee9e":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:20].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(20,28))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged\/folds)')\nplt.tight_layout()\nplt.savefig('Feature_Importance.png')","fd99fcd7":"explainer = shap.TreeExplainer(clf)\nshap_values = explainer.shap_values(X)\n\nshap.summary_plot(shap_values, X)","2be47980":"#tree visualization\ngraph = lgb.create_tree_digraph(clf, tree_index=3, name='Tree3' )\ngraph.graph_attr.update(size=\"110,110\")\ngraph","4dd45ed5":"<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Understanding the Model Better<\/center><\/h3>\n\n\n\nTo get an overview of which features are most important for a model, we can plot the SHAP values of every feature for every sample. The plot below sorts features by the sum of SHAP value magnitudes over all samples, and uses SHAP values to show the distribution of the impacts each feature has on the model output. [(source)](https:\/\/github.com\/slundberg\/shap)  ","87e31062":"Here is my optimal parameter for LightGBM. ","33c6cf60":"<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Loading Library and Dataset<\/center><\/h3>\n","805c4a32":"\n\n\n1. [Loading Library and Dataset](#2)\n2. [Bayesian Optimization with LightGBM](#3)\n3. [Training LightGBM](#4)\n4. [Understanding the Model Better](#5)\n","33c9e24b":"<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Bayesian Optimization with LightGBM<\/center><\/h3>\n\nNow I am going to prepare data for modeling and a Baysian Optimization function. You can put more parameters (ex. lambda_l1 and lambda_l2) into the function.  ","e9c826c7":"We can also plot a tree from the model and see each tree! ","501daf09":"![](https:\/\/static.packt-cdn.com\/products\/9781788479042\/graphics\/7bfa3d4d-e715-4306-9335-0b5f8b72924d.png)","05801ad6":"<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Training LightGBM<\/center><\/h3>\n\n\n\nBased on the parameter from the previous step, I am going to train LightGBM. ","e45e93b4":"<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Bayesian Optimization with LightGBM<\/center><\/h3>\n\n\n\n"}}