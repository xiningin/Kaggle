{"cell_type":{"47de2808":"code","985ffa6b":"code","60a26afc":"code","46be9626":"code","18d9003b":"code","b39f59e2":"code","b86989cd":"code","fe4c974d":"code","4c683512":"code","bda4feca":"code","89443f52":"code","5454252f":"code","b465bddf":"code","7484e076":"code","a0dd4fac":"code","f0dac93f":"code","faaeba9d":"code","0f5a7644":"code","c8e2ce43":"code","d6a2b38e":"code","3cd8000a":"code","041a6707":"code","e109bb3f":"code","29354e9a":"markdown","bbe6d7f6":"markdown","ea6543ab":"markdown","7cb53a95":"markdown","f17d354b":"markdown","ba29efae":"markdown","a6e1eda6":"markdown","d43042d0":"markdown","480d1840":"markdown","a625485d":"markdown","bab8bf9d":"markdown","2e06c3d9":"markdown","d73f3bb3":"markdown","2e74aab5":"markdown","7ae4e689":"markdown","b0f917ae":"markdown","0314cc9b":"markdown","e38e1018":"markdown"},"source":{"47de2808":"import numpy as np\nimport pandas as pd\nimport torch\nimport re\nimport random\nfrom numba import cuda\nimport gc\ngc.enable()\n\nimport torch.nn as nn                    \nimport torch.nn.functional as F          \nimport torch.optim as optim               \nfrom torch.optim import lr_scheduler\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n\nimport transformers\nimport tokenizers\n\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom transformers import (GPT2Config,\n                          GPT2Tokenizer,\n                          GPT2Model,\n                          AdamW, \n                          get_cosine_schedule_with_warmup,\n                          logging)\n\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.utils.data import Dataset, DataLoader\nscaler = torch.cuda.amp.GradScaler()\n\nlogging.set_verbosity_warning()\nimport warnings\nwarnings.simplefilter('ignore')\n\n%matplotlib inline\nimport matplotlib.pyplot as plt","985ffa6b":"# Look for gpu to use. Will use `cpu` by default if no gpu found.\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","60a26afc":"MAX_LENGTH = 315\nTRAIN_BATCH_SIZE = 4\nTEST_BATCH_SIZE = 2\nEPOCHS = 4","46be9626":"train_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n\ntrain_df['excerpt'] = train_df['excerpt'].astype(str)\ntest_df['excerpt'] = test_df['excerpt'].astype(str)","18d9003b":"PATH = \"..\/input\/hugging-face-gpt2\/gpt2\"\n\ntokenizer = GPT2Tokenizer(\n            vocab_file = f'{PATH}\/vocab.json', \n            merges_file= f'{PATH}\/merges.txt', \n            add_prefix_space = True,\n            lowercase=True,\n            bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n\n# default to right padding\ntokenizer.padding_side = \"right\"","b39f59e2":"pd.DataFrame({'Token meaning': ['Beginning of sequence (BOS) token',\n                                'End of sequence (EOS) token', 'Padding token']},\n             index=['<|startoftext|>', '<|endoftext|>', '<|pad|>'])","b86989cd":"# Define PAD Token = EOS Token = 50256\ntokenizer.pad_token = tokenizer.eos_token","fe4c974d":"print(tokenizer.eos_token)","4c683512":"print(tokenizer.pad_token)","bda4feca":"print(tokenizer.bos_token)","89443f52":"class TextDataset(Dataset):\n    def __init__(self, df, tokenizer = tokenizer, max_length = MAX_LENGTH):\n        '''PyTorch Dataset class for loading data.\n          This is where the data parsing happens.\n          This class is built with reusability in mind: it can be used as is as.\n        '''\n        \n        self.df = df\n        self.target = \"target\" in df  \n        self.tokenizer = tokenizer\n        self.max_length = self.tokenizer.model_max_length if max_length is None else max_length\n        \n    def __len__(self):\n        '''When used `len` return the number of examples. '''\n        return len(self.df)\n    \n    def get_data(self, row):    \n        excerpt = \" \".join(row.excerpt.lower().split())                \n        encoded_input = self.tokenizer('<|startoftext|>'+ excerpt + '<|endoftext|>', truncation=True, \n                       max_length=self.max_length, padding=\"max_length\")\n        input_ids = torch.tensor(encoded_input['input_ids'], dtype=torch.long)\n        attention_mask = torch.tensor(encoded_input['attention_mask'], dtype=torch.long)\n        \n        return input_ids, attention_mask\n    \n    def __getitem__(self, index):\n        '''Given an index return an example from the position. '''\n        \n        data = {}\n        row = self.df.iloc[index]        \n        input_ids, attention_mask = self.get_data(row)\n        data['input_ids'] = input_ids\n        data['attention_mask'] = attention_mask\n        if self.target:\n            data['target'] = row.target\n        return data\n            ","5454252f":"e = TextDataset(train_df)\ne[1]","b465bddf":"def train_val_dataloaders(df, train_idx, val_idx, batch_size):\n        \n    train_df = df.iloc[train_idx]\n    val_df = df.iloc[val_idx]\n    train_loader = torch.utils.data.DataLoader(\n        TextDataset(train_df), \n        batch_size=batch_size, \n        shuffle=True, #Should give better results particularly when you are running for more epochs like in training\n        num_workers=2,  \n        drop_last=True, pin_memory=True)\n    val_loader = torch.utils.data.DataLoader(\n        TextDataset(val_df),\n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=2, pin_memory=False) \n    #The num_workers attribute tells the data loader instance how many sub-processes to use for data loading\n    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n    return dataloaders_dict\n\ndef test_loader(df, batch_size=TEST_BATCH_SIZE):\n    loader = torch.utils.data.DataLoader(\n        TextDataset(test_df), \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=2, pin_memory=False)    \n    return loader","7484e076":"class TextModel(nn.Module):\n     \n    def __init__(self):\n        super(TextModel, self).__init__()\n        \n        # Get model configuration\n        config = GPT2Config.from_pretrained(\n            '..\/input\/hugging-face-gpt2\/gpt2\/config.json', \n            n_layer=12, n_head=12, num_labels=1, layer_norm_epsilon = 1e-7, \n            output_hidden_states=True, output_attentions = False)    \n        self.gpt = GPT2Model.from_pretrained(\n            '..\/input\/hugging-face-gpt2\/gpt2\/pytorch_model.bin', config=config)\n        \n        # resize model embedding to match new tokenizer\n        self.gpt.resize_token_embeddings(len(tokenizer)) #resize the dictionary size of the embedding layer        \n        # fix model padding token id\n        self.gpt.config.pad_token_id = self.gpt.config.eos_token_id\n        for param in self.gpt.parameters():\n            param.requires_grad = True    \n            \n        self.drop = nn.Dropout(config.attn_pdrop)\n         \n        self.attention = nn.Sequential(  \n            nn.LayerNorm(config.hidden_size),\n            nn.Linear(config.n_embd, config.n_layer),            \n            nn.ReLU(),                       \n            nn.Linear(config.n_layer, 1),\n            nn.Softmax(dim=1)\n            )         \n        \n        self.regressor = nn.Sequential(     \n            nn.LayerNorm(config.n_embd),\n            nn.Linear(config.n_embd, 1)                        \n            )\n        \n    def forward(self, input_ids, attention_mask, past_key_values=None): \n        # Type: torch tensor\n        outputs = self.gpt(input_ids =input_ids,past_key_values =past_key_values,\n                           attention_mask= attention_mask) \n        \n        # There are a total of 12-layer, 768-hidden, 12-heads and 117M parameters.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_state = outputs.last_hidden_state\n        \n        # Using the dropout\n        last_layer_hidden_state = self.drop(last_layer_hidden_state)\n        \n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for gpt2).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_state)\n        \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_state, dim=1) \n        out = self.regressor(context_vector)\n        \n        return out","a0dd4fac":"def set_seed(seed):\n    # Set random seed for reproducibility\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n        \nseed = 9999","f0dac93f":"train_loss = []\nval_loss = []","faaeba9d":"def train(model, dataloaders_dict, optimizer, num_epochs, scheduler, device, filename): \n    # Load model to defined device\n    model.to(device) \n    \n    for epoch in range(num_epochs):\n        for key in ['train', 'val']:\n            if key == 'train':\n                model.train()\n                dataloaders = dataloaders_dict['train']\n            else:\n                model.eval()\n                dataloaders = dataloaders_dict['val']\n\n            #total loss for this epoch.\n            epoch_loss = 0.0\n            \n            # Set tqdm to add loading screen and set the length\n            # Evaluate data for one epoch\n            loader = tqdm(dataloaders, total=len(dataloaders))\n            \n            # Tracking variables\n            all_targets = []\n            all_predictions = []  \n\n            rmse = 0.0           \n            # loop over the data iterator, and feed the inputs to the network\n            # Train the model on each batch\n            \n            for (idx, data) in enumerate(loader):\n                input_ids = data['input_ids']\n                attention_mask = data['attention_mask']\n                # Add original target labels - use later for evaluation\n                target = data['target']\n\n                # Always clear any previously calculated gradients before performing a backward pass\n                model.zero_grad() # Reset gradients tensors\n                optimizer.zero_grad()\n\n                # move batch values to device\n                input_ids = input_ids.to(device, dtype=torch.long, non_blocking=True) # Overlapping transfer if pinned memory\n                attention_mask = attention_mask.to(device, dtype=torch.long, non_blocking=True)\n                target = target.to(device, dtype=torch.float, non_blocking=True)\n\n                with torch.set_grad_enabled(key == 'train'): \n                    \n                    with torch.cuda.amp.autocast():\n                        output = model(input_ids, attention_mask).flatten()\n                        loss = nn.MSELoss()(output, target)  # defining loss function\n                        \n                        # Move targets and outputs to CPU\n                        all_predictions.append(output.detach().cpu().numpy().tolist())\n                        all_targets.append(target.detach().cpu().numpy().tolist())\n\n                    # Accumulate the training loss over all of the batches so that we can\n                    # calculate the average loss at the end. `loss` is a Tensor containing a\n                    # single value; the `.item()` function just returns the Python value from the tensor.\n                    epoch_loss += loss.item()\n                    \n                    if key == 'train':\n                        scaler.scale(loss).backward() # backwards of loss\n                        scaler.step(optimizer) # Update optimizer\n                        scaler.update() # scaler update\n                        scheduler.step() # Update learning rate schedule\n\n                        # Clip the norm of the gradients to 1.0.\n                        # This is to help prevent the \"exploding gradients\" problem.\n                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   \n\n                    del target, output, loss  # delete these variables to free the GPU memory\n                    torch.cuda.empty_cache() # Releases all unoccupied cached memory currently held by the caching allocator\n            \n            # Calculate the average loss over the training data.\n            epoch_loss = epoch_loss \/ len(dataloaders.dataset) \n            \n            # Return all true labels and prediciton for future evaluations\n            all_targets = np.concatenate(all_targets) \n            all_predictions = np.concatenate(all_predictions)\n                        \n            # Score with rmse\n            rmse = np.sqrt(mean_squared_error(all_targets,all_predictions)) \n\n            if key == 'train':\n                train_loss.append(epoch_loss)\n                losses = np.mean(train_loss)\n                print('Epoch {}\/{} | {:^5} | Loss: {:.4f} | RMSE: {:.4f}'.format(\n                    epoch + 1, num_epochs, key, losses, rmse))   \n\n            else:\n                val_loss.append(epoch_loss)\n                losses = np.mean(val_loss)\n                print('Epoch {}\/{} | {:^5} | Loss: {:.4f} | RMSE: {:.4f}'.format(\n                    epoch + 1, num_epochs, key, losses, rmse)) \n                \n\n    torch.save(model.state_dict(), filename)\n    \n    del model, optimizer, scheduler\n    gc.collect()   # performs a blocking garbage collection of all generations \n    torch.cuda.empty_cache()","0f5a7644":"kfold = KFold(n_splits=5,shuffle=True,random_state=seed)","c8e2ce43":"%%time\n\nfor fold, (idxTrain, idxVal) in enumerate(kfold.split(train_df)):\n    \n    print('#'*10)\n    print('### FOLD %i'%(fold + 1))\n    print('#'*10)\n    \n    set_seed(seed + fold)\n    model = TextModel()\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n    # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n    # The 'W' stands for 'Weight Decay fix\"\n    optimizer = AdamW(optimizer_parameters, lr=3e-7, eps=1e-8, correct_bias=False) \n    dataloaders_dict = train_val_dataloaders(train_df, idxTrain, idxVal, batch_size=TRAIN_BATCH_SIZE)\n    \n    # Total number of training steps is number of batches * number of epochs.\n    # `train_dataloader` contains batched data so `len(train_dataloader)` gives \n    # us the number of batches.\n    num_training_steps = int(len(dataloaders_dict) \/ EPOCHS * TRAIN_BATCH_SIZE)\n    scheduler = get_cosine_schedule_with_warmup(\n                  optimizer,\n                  num_warmup_steps=0, \n                  num_training_steps=num_training_steps\n                )    # learning rate scheduler\n    train(\n        model, \n        dataloaders_dict, \n        optimizer, \n        EPOCHS,\n        scheduler,\n        device,\n        f'gpt_fold{fold}.pth')","d6a2b38e":"%%time\n\nt_loader = test_loader(test_df)\npredictions = []\nmodels = []\nfor fold in range(kfold.n_splits):\n    model = TextModel()\n    model.to(device)\n    model.load_state_dict(torch.load(f'.\/gpt_fold{fold}.pth'))\n    model.eval()\n    models.append(model)\n    \nloader = tqdm(t_loader, total=len(t_loader))\nfor model in models:\n    preds = []\n    for (idx, data) in enumerate(loader):\n        input_ids = data['input_ids']\n        attention_mask = data['attention_mask']\n\n        input_ids = input_ids.to(device, dtype=torch.long) \n        attention_mask = attention_mask.to(device, dtype=torch.long)\n\n        with torch.no_grad():\n            output = model(input_ids, attention_mask).flatten() \n            output = output.detach().cpu().numpy()\n            preds.append(output)\n    preds = np.concatenate(preds)\n    predictions.append(preds)\n    torch.cuda.empty_cache()","3cd8000a":"predictions = pd.DataFrame(predictions) \npredictions = predictions.T \npredictions = predictions.mean(axis=1) \npredictions = predictions.values","041a6707":"submission_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\nsubmission_df.target = predictions\nsubmission_df.to_csv(\"submission.csv\", index=False)","e109bb3f":"submission_df","29354e9a":"## 5.2: Training Preparation\nWe now activate the training mode of our model \u2014 and finally, initialize our optimizer.\n\n#### Set seed for reproducibility\nA random seed is used to ensure that results of our model are reproducible. In other words, the use of this parameter ensures that anyone who runs your code will get the exact same outputs. In data science reproducibility is extremely important.","bbe6d7f6":"And now we can see that we created our tensors \u2014 we will be training our model through masked-language modeling (MLM). So, we need three tensors:\n- `input_ids` \u2014 our token_ids with ~15% of tokens masked using the mask token <mask>.\n- `attention_mask` \u2014 a tensor of 1s and 0s, marking the position of \u2018real\u2019 tokens\/padding tokens \u2014 used in attention calculations.\n- `target` \u2014 our token_ids with no masking.    ","ea6543ab":"Just printing out the special tokens.","7cb53a95":"## 5.3: Setting parameters for the training\n\nWe create optimizer and scheduler use by PyTorch in training, and use the most common parameters used by transformers models. Then we loop through the number of defined epochs and call the train and validation functions.","f17d354b":"Our `attention_mask` and `target` tensors are simply extracted from our batch. The `input_ids` tensors require more attention however, for this tensor we mask ~15% of the tokens \u2014 assigning them the token IDs. In the final output, we can see part of an encoded `input_ids` tensor. The very first token ID is 1 \u2014 the `<|startoftext|>` token. Dotted around the tensor we have several token IDs \u2014 these are our newly added masked tokens.\n\n\n## 4.2: Building the DataLoader\nNext, we define our Dataset class \u2014 which we use to initialize our three encoded tensors as PyTorch torch.utils.data.Dataset objects.","ba29efae":"<p style=\"text-align:center;\"> <span style=\"font-size:30px;\"> <b> OpenAI GPT2 implementation: using Pytorch  <\/b> <\/span> <\/p>\n\n![GPT2](https:\/\/www.vyrazu.com\/wp-content\/uploads\/2021\/01\/Gpt2.png)","a6e1eda6":"## **Thank you so much for reading! Please do upvote if you liked it. \ud83d\ude42**","d43042d0":"# Step 4: Creating the Input Pipeline\nThe input pipeline of our training process is the more complex part of the entire process. It consists of us taking our raw training data, transforming it, and loading it into a DataLoader ready for training.\n\n## 4.1: Data Preprocessing\nNo preprocessing step is required for GPT2. For example, lower casing, tokenization and other step are skipped as it is believed that these pre-processing step restrict the capability of the model and it is able evaluate all language model benchmark.\n\nThere are three main parts of this PyTorch Dataset class:\n\n- _**init()**_ where we read in the dataset and transform text and labels into numbers.\n- _**len()**_ where we need to return the number of examples we read in. This is used when calling len(MovieReviewsDataset()).\n- _**getitem()**_ always takes as an input an int value that represents which example from our examples to return from our dataset. If a value of 3 is passed, we will return the example form our dataset at position 3.","480d1840":"Finally, our dataset is loaded into a PyTorch DataLoader object \u2014 which we use to load our data into our model during training.\n\n\n\n# Step 5: Training the Model\nWe need two things for training, our DataLoader and a model. The DataLoader we have \u2014 but no model.\n\n## 5.1: Initializing the Model\nFor training, we need a pre-trained GPT2Model. To create that, we first need to create a GPT2 config object to describe the parameters we\u2019d like to initialize GPT2 with. \n\n_**Config (GPT2Config)** \u2013 Model configuration class with all the parameters of the model. Initializing with a config file does not load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model weights._\n\nThen, we import and initialize our GPT2 model, which is a transformer outputting raw hidden-states without any specific head on top.\n\nLoading the three essential parts of the pretrained GPT2 transformer: `configuration`, `tokenizer` and`model`.\n\nWhile creating the model_config(`config`), we will mention the number of labels we need for our regression task, which is only need one label for num_labels.","a625485d":"### Some configurations for the model","bab8bf9d":"# Step 6: Evaluate\nThis is the last step before submission. Just gather all the models we got from our training, and try evaluating the RMSE score.","2e06c3d9":"# Step 1: About GPT-2\n\nOpenAI GPT-2 model was proposed in Language Models are Unsupervised Multitask Learners by Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei and Ilya Sutskever. It\u2019s a causal (unidirectional) transformer pretrained using language modeling on a very large corpus of ~40 GB of text data.\n\nThe abstract from the paper is the following:\n\n_GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset[1] of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data._\n\n\n## Understanding the architecture\n\nGPT-2 is based on Transformer architecture which was first proposed by the team of researchers at Google in their paper ['Attention is all You Need'](https:\/\/arxiv.org\/abs\/1706.03762). The paper described an encoder-decoder based architecture that used concepts like multi heads and self-attention.\n\nTransformer architecture is an improvement over RNN based architectures like LSTM and GRU due to various reasons:\n- Transformers can achieve parallelization of tokens (which are basically parts of a text) within an input.\n- Transformers require constant O(1) number of operations to learn dependency between two tokens independently of their position distance in a sequence. This makes transformers better at capturing long-term dependencies.\n- With the help of multi-head attention, the model can capture various aspects of the input and improve its expressive ability.\n\nGPT-2 is essentially a decoder-only transformer. The model is built by stacking up the transformer decoder blocks.\n<img src=\"https:\/\/images.contentstack.io\/v3\/assets\/blt71da4c740e00faaa\/blt1c4150bcaddeae45\/601c8d573e70bb4c12c6feea\/Vision-Transformer-Model-Architecture-1024x746.jpg\" width=\"600px\">\n\nUnlike the self-attention that transformers use, GPT-2 uses **masked self-attention**. A normal self-attention block allows a position to peak at tokens to its right. Masked self-attention prevents that from happening, which means that they only use the left context to predict the next word.\n\nLet's now try to understand the process step-by-step.\n\n# Step 2: Importing useful libraries","d73f3bb3":"#### Using k-Fold Cross-Validation\nK-Folds technique is a popular and easy to understand, it generally results in a less biased model compare to other methods. Because it ensures that every observation from the original dataset has the chance of appearing in training and test set. This is one among the best approach if we have a limited input data, like this dataset.","2e74aab5":"But we want to define the PAD token as the EOS token. \n\n**Why?** Since for getting GPT2 to work, we will need to update the tokenizer's pad token to be the eos token. Adding the EOS token as PAD token to avoid warnings","7ae4e689":"Number of batches - depending on the max sequence length and GPU memory. For 256 sequence length batch of 8\/4 works without cuda memory issues. For small sequence length can try batch of 32 or higher.\n\nThis is because we know that increasing the batch size will directly result in increasing the required GPU memory. Often times, not having enough GPU memory prevent us from increasing the batch size. And for that reason, keeping a batch size below 16 is recommended for this notebook.\n\n\n### Loading the data","b0f917ae":"We need to pad the sentences because GPT-2 is a model with absolute position embeddings so it\u2019s usually advised to pad the inputs on the right rather than the left.\n\nSo, make sure to include them within the special_tokens parameter of our tokenizer\u2019s train method call.\n\nHere's how the GPT2 special tokens look like.","0314cc9b":"Append predictions to the submission.csv","e38e1018":"# Step 3: Preparing the data\n\nThe transformer has already been trained with a specific vocabulary, which means we need to train with the exact same vocabulary and also tokenize our data in the same way that the transformer did when it was initially trained.\n\n\n### What is GPT2Tokenizer?\nGPT-2 tokenizer, based on byte-level Byte-Pair-Encoding (BPE), has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will be encoded differently whether it is at the beginning of the sentence (without space) or not.\n\n#### What is BPE?\nBPE is a simple form of data compression in which the most common pair of consecutive bytes of data is replaced with a byte that does not occur within that data. BPE is a middle ground between character and word-level encodings which helps it in managing the vocabulary of large corpora. This behavior also enables the encoding of any rare words in the vocabulary with appropriate subword tokens without introducing any \u201cunknown\u201d tokens.\n\n#### Choice of the GPT2 tokenizer: \nWe could have chosen another pre-trained BBPE tokenizer for this study. The key point is to use BBPE tokenizers trained on huge corpus because they can thus tokenize any word of any language without using the unknown token.\n\n\n### Initializing the Tokenizer\nWe first initialize the tokenizer using the two files we built before \u2014 using a simple from_pretrained. i.e., the vocab and the merges files."}}