{"cell_type":{"bf28bf86":"code","17a81b62":"code","63cc3fe3":"code","28bee18a":"code","d8248ea8":"code","e5514a14":"code","d685f26c":"code","22ecb630":"code","23dc8dc3":"code","12173eae":"code","7785fa2f":"code","cabda9f1":"code","03574360":"code","8d2f028f":"code","0f7cd9f2":"code","52fabfb1":"code","63760391":"code","8c7591a9":"code","3b58f2d5":"code","59306aad":"code","c072c355":"code","5c3e4088":"code","fc9fe36b":"code","4907564c":"code","f3fc69ee":"markdown","a6b79610":"markdown","41001f8c":"markdown","4d069a7d":"markdown","496b0c6a":"markdown","cce7f904":"markdown","a833b699":"markdown","b197a8fe":"markdown","40e10946":"markdown","73f65cf8":"markdown","945674b6":"markdown","af803973":"markdown","8b269272":"markdown","d0df3d97":"markdown","b1c8a48b":"markdown","9c31ed19":"markdown","36918cc4":"markdown","a87fd863":"markdown","306487b4":"markdown","b3bbaa84":"markdown","949f88ac":"markdown","695a5ad1":"markdown"},"source":{"bf28bf86":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords \nfrom unicodedata import normalize\nfrom nltk.stem import SnowballStemmer\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","17a81b62":"def csv_len(data) :\n    \n    words = []\n    letters = []\n    sentiments = []\n    tweets = []\n    \n    for index, tweet in data.iterrows():\n        tweet_split = tweet.Tweet.split()\n        \n        sentiments.append(tweet.Sentiment)\n        tweets.append(tweet.Tweet)\n        letters.append(len(tweet.Tweet))\n        words.append(len(tweet_split))\n    \n    data['Tweet'] = tweets\n    data['Sentiment'] = sentiments\n    data['Words'] = words\n    data['Letters'] = letters\n    return data","63cc3fe3":"def graphic(data_len) :\n    \n    fig,ax = plt.subplots(figsize=(5,5))\n    plt.boxplot(data_len)\n    plt.show()","28bee18a":"def preprocessing(data) :\n    \n    tweets = []\n    sentiment = []\n\n    for index, tweet in data.iterrows():\n        words_cleaned=\"\"\n        tweet_clean = tweet.Tweet.lower()\n    \n        words_cleaned =\" \".join([word for word in tweet_clean.split()\n            if 'http:\/\/' not in word\n            and 'https:\/\/'not in word\n            and '.com' not in word\n            and '.es' not in word\n            and not word.startswith('@')\n            and not word.startswith('#')\n            and word != 'rt'])\n        \n        \n        tweet_clean = re.sub(r'\\b([jh]*[aeiou]*[jh]+[aeiou]*)*\\b',\"\",words_cleaned)\n        tweet_clean = re.sub(r'(.)\\1{2,}',r'\\1',tweet_clean)\n        tweet_clean = re.sub(\n            r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\", \n            normalize( \"NFD\", tweet_clean), 0, re.I)\n        tweet_clean = re.sub(\"[^a-zA-Z]\",\" \",tweet_clean)\n        tweet_clean = re.sub(\"\\t\", \" \", tweet_clean)\n        tweet_clean = re.sub(\" +\", \" \",tweet_clean) \n        tweet_clean = re.sub(\"^ \", \"\", tweet_clean)\n        tweet_clean = re.sub(\" $\", \"\", tweet_clean)\n        tweet_clean = re.sub(\"\\n\", \"\", tweet_clean)\n        \n        words_cleaned=\"\"\n        stemmed =\"\"\n        \n        stop_words = set(stopwords.words('english'))\n        stemmer = SnowballStemmer('english')\n        \n        tokens = word_tokenize(tweet_clean)\n        \n        words_cleaned =[word for word in tokens if not word in stop_words]\n        stemmed = \" \".join([stemmer.stem(word) for word in words_cleaned])\n        \n        \n    \n        sentiment.append(tweet.Sentiment)\n        tweets.append(stemmed)\n    \n    data['Tweet'] = tweets\n    data['Sentiment'] = sentiment\n    data.loc[:,['Sentiment','Tweet']]\n    \n    return data","d8248ea8":"train = pd.read_csv('\/kaggle\/input\/semevaldatadets\/semeval-2017-train.csv', delimiter='\t')\ntrain.columns = ['Sentiment', 'Tweet']\ntrain.rename(columns={'label': 'Sentiment','text' : 'Tweet'})","e5514a14":"test = pd.read_csv('\/kaggle\/input\/semevaldatadets\/semeval-2017-test.csv', delimiter='\t')\ntest.columns = ['Sentiment', 'Tweet']\ntest.rename(columns={'label': 'Sentiment','text' : 'Tweet'})","d685f26c":"train.Sentiment.value_counts()","22ecb630":"test.Sentiment.value_counts()","23dc8dc3":"train = csv_len(train)\ntrain","12173eae":"graphic(train['Words'])","7785fa2f":"graphic(train['Letters'])","cabda9f1":"test = csv_len(test)\ntest","03574360":"graphic(test['Words'])","8d2f028f":"graphic(test['Letters'])","0f7cd9f2":"train_cleaned = preprocessing(train)\ntrain_cleaned.loc[:,['Sentiment','Tweet']]","52fabfb1":"test_cleaned = preprocessing(test)\ntest_cleaned.loc[:,['Sentiment','Tweet']]","63760391":"train_final = train_cleaned.loc[:,['Sentiment','Tweet']]\ntrain_final\ntrain_final.to_csv('semevalTrain.csv',index=False)","8c7591a9":"test_final = test_cleaned.loc[:,['Sentiment','Tweet']]\ntest_final\ntest_final.to_csv('semevalTest.csv',index=False)","3b58f2d5":"train_cleaned = csv_len(train_cleaned)\ntrain_cleaned","59306aad":"graphic(train_cleaned['Words'])","c072c355":"graphic(train_cleaned['Letters'])","5c3e4088":"test_cleaned = csv_len(test_cleaned)\ntest_cleaned","fc9fe36b":"graphic(test_cleaned['Words'])","4907564c":"graphic(test_cleaned['Letters'])","f3fc69ee":"En primer lugar obtenemos el dataframe de entrenamiento.","a6b79610":"En segundo lugar obtenemos el dataframe de predicci\u00f3n.","41001f8c":"Para posteriormente poder comprobar la reducci\u00f3n de palabras y letras despu\u00e9s del preprocesado ***a\u00f1adimos dos columnas*** al dataframe inicial, donde la ***primera*** corresponde al ***n\u00famero de palabras*** que hay por l\u00ednea y la ***segunda*** el ***n\u00famero de caracteres***.","4d069a7d":"Obtenemos la ***palabras y letras por tweet*** del dataframe train antes del preprocesado.","496b0c6a":"Mostramos el ***gr\u00e1fico*** asociado a las ***palabras*** por tweet antes del preprocesamiento.","cce7f904":"Mostramos el ***gr\u00e1fico*** asociado a las ***letra*** por tweet despu\u00e9s del preprocesamiento.","a833b699":"***En el preprocesamiento: *** \n\n1. Eliminaremos ****enlaces*** (comienzan por http o https y terminan por .com o .es), ***menciones***, ***hashtags*** y ***retweets***.\nEl siguiente paso ser\u00eda eliminar: \n1. Los ***caracteres duplicados***, en este caso se contempla que si un caracter se repite 3 veces se sustituir\u00e1 por una sola ocurrencia de este.\n2. Las ***risas*** (hahaha, jajaja) contemplando que puedan ser con cualquier vocal.\n3. Eliminamos las tildes o caracteres de raros que se encuentren en las letras.\n4. Eliminamos cualquier ***caracter que no sean letras***, ya que estos no tienen ning\u00fan valor.\n5. Se sustituyen los ***tabuladores*** por espacios en blanco.\n6. Si hay varios ***espacios en blanco*** se sustituyen por ***un solo espacio***.\n7. ***Eliminamos espacios*** al comienzo del tweet y al final.\n8. ***Eliminamos los saltos de l\u00ednea*** que puedan existir en cualquier parte de la cadena.\n9. Eliminamos ***Stopwords***: es decir eliminamos la palabras vac\u00edas que por s\u00ed solas no tienen ning\u00fan tipo de significado, sino que sirven para acompa\u00f1ar o modificar a otras palabras.\n10. Reducimos las palabras presentes a su ***ra\u00edz*** correspondiente.","b197a8fe":"# **PROCESO DE ENTRENAMIENTO DE ALGORITMOS**\n\nAntes de entrenar un algoritmo, ***recabaremos informaci\u00f3n del csv*** para comparar los datos iniciales con los finales. \nDebemos limpiar los datos que con los que este va a trabajar y presentarlos en el formato adecuado para este algoritmo, pero antes, los datos preprocesados deben formar parte de la bolsa de palabras con la que trabajar\u00e1 el algoritmo.","40e10946":"**2. Preprocesamiento de datos.** \n\nLimpiamos los datos con los que trabajar\u00e1 el algoritmo utilizando la funci\u00f3n ***\"preprocessing\"***.","73f65cf8":"Mostramos el ***gr\u00e1fico*** asociado a las ***letras*** por tweet antes del preprocesamiento.","945674b6":"Mostramos el ***gr\u00e1fico*** asociado a las ***palabras*** por tweet antes del preprocesamiento.","af803973":"Mostramos el ***gr\u00e1fico*** asociado a las ***letra*** por tweet despu\u00e9s del preprocesamiento.","8b269272":"# **Corpus: *Sem-Eval ***","d0df3d97":"**1. Recogida de informaci\u00f3n.**\n\nComprobamos los diferentes valores de sentimientos que hay tanto en el dataframe test como en el train. En este caso hay tres (-1, 0, 1) donde cada uno tiene un valor de filas asignado. ","b1c8a48b":"Mostramos el ***gr\u00e1fico*** asociado a las ***letras*** por tweet antes del preprocesamiento.","9c31ed19":"Mostramos el ***gr\u00e1fico*** asociado a las ***palabras*** por tweet despu\u00e9s del preprocesamiento.","36918cc4":"**2. Guardamos el preprocesaiento.** \n\nGuardamos los datos preprocesados en un csv para poder descargarlos y utilizarlos en un notebook para entrenar los clasificadores.","a87fd863":"Mostramos el ***gr\u00e1fico*** asociado a las ***palabras*** por tweet despu\u00e9s del preprocesamiento.","306487b4":"Obtenemos la ***palabras y letras por tweet*** del dataframe test antes del preprocesado.","b3bbaa84":"Obtenemos la ***palabras y letras por tweet*** del dataframe train despu\u00e9s del preprocesado.","949f88ac":"A trav\u00e9s de los datos recogidos en la funci\u00f3n csv_len ***mostraremos un gr\u00e1fico*** donde ***muestra los rangos de cantidad de palabras y letras*** con las que estamos trabajando, haciendo as\u00ed m\u00e1s visuales los datos recogidos.","695a5ad1":"Obtenemos la ***palabras y letras por tweet*** del dataframe test despu\u00e9s del preprocesado."}}