{"cell_type":{"4e0604b7":"code","69dbdcc9":"code","292af2db":"code","e86e4570":"code","962c2430":"code","7035e741":"code","e4e2d8e3":"code","25100d82":"code","7d5fb9cf":"code","e4346f79":"code","0ba72975":"code","b48530e2":"code","0a0fdc56":"code","fdd0cbe9":"code","d4d55bb3":"code","73c37175":"markdown","f3e94fe0":"markdown","49b98dc2":"markdown","a7b8dd7c":"markdown","6d03894b":"markdown","d5466a5b":"markdown","98f9e923":"markdown"},"source":{"4e0604b7":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\n%matplotlib inline\n\nimport cv2\nfrom tqdm import tqdm_notebook, tnrange\nfrom glob import glob\nfrom itertools import chain\nfrom skimage.io import imread, imshow, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom skimage.color import rgb2gray\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.models import Model, load_model, save_model\nfrom tensorflow.keras.layers import Input, Activation, BatchNormalization, Dropout, Lambda, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint","69dbdcc9":"#Set Parameters\nim_width = 256\nim_height = 256\n","292af2db":"train_files = []\nmask_files = glob('..\/input\/lgg-mri-segmentation\/kaggle_3m\/*\/*_mask*')\n\nfor i in mask_files:\n    train_files.append(i.replace('_mask',''))\n\nprint(train_files[:10])\nprint(mask_files[:10])","e86e4570":"#Lets plot some samples\nrows,cols=3,3\nfig=plt.figure(figsize=(10,10))\nfor i in range(1,rows*cols+1):\n    fig.add_subplot(rows,cols,i)\n    img_path=train_files[i]\n    msk_path=mask_files[i]\n    img=cv2.imread(img_path)\n    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    msk=cv2.imread(msk_path)\n    plt.imshow(img)\n    plt.imshow(msk,alpha=0.4)\nplt.show()","962c2430":"df = pd.DataFrame(data={\"filename\": train_files, 'mask' : mask_files})\ndf_train, df_test = train_test_split(df,test_size = 0.1)\ndf_train, df_val = train_test_split(df_train,test_size = 0.2)\nprint(df_train.values.shape)\nprint(df_val.values.shape)\nprint(df_test.values.shape)","7035e741":"# From: https:\/\/github.com\/zhixuhao\/unet\/blob\/master\/data.py\ndef train_generator(data_frame, batch_size, aug_dict,\n        image_color_mode=\"rgb\",\n        mask_color_mode=\"grayscale\",\n        image_save_prefix=\"image\",\n        mask_save_prefix=\"mask\",\n        save_to_dir=None,\n        target_size=(256,256),\n        seed=1):\n    '''\n    can generate image and mask at the same time use the same seed for\n    image_datagen and mask_datagen to ensure the transformation for image\n    and mask is the same if you want to visualize the results of generator,\n    set save_to_dir = \"your path\"\n    '''\n    image_datagen = ImageDataGenerator(**aug_dict)\n    mask_datagen = ImageDataGenerator(**aug_dict)\n    \n    image_generator = image_datagen.flow_from_dataframe(\n        data_frame,\n        x_col = \"filename\",\n        class_mode = None,\n        color_mode = image_color_mode,\n        target_size = target_size,\n        batch_size = batch_size,\n        save_to_dir = save_to_dir,\n        save_prefix  = image_save_prefix,\n        seed = seed)\n\n    mask_generator = mask_datagen.flow_from_dataframe(\n        data_frame,\n        x_col = \"mask\",\n        class_mode = None,\n        color_mode = mask_color_mode,\n        target_size = target_size,\n        batch_size = batch_size,\n        save_to_dir = save_to_dir,\n        save_prefix  = mask_save_prefix,\n        seed = seed)\n\n    train_gen = zip(image_generator, mask_generator)\n    \n    for (img, mask) in train_gen:\n        img, mask = adjust_data(img, mask)\n        yield (img,mask)\n\ndef adjust_data(img,mask):\n    img = img \/ 255\n    mask = mask \/ 255\n    mask[mask > 0.5] = 1\n    mask[mask <= 0.5] = 0\n    \n    return (img, mask)","e4e2d8e3":"smooth=100\n\ndef dice_coef(y_true, y_pred):\n    y_truef=K.flatten(y_true)\n    y_predf=K.flatten(y_pred)\n    And=K.sum(y_truef* y_predf)\n    return((2* And + smooth) \/ (K.sum(y_truef) + K.sum(y_predf) + smooth))\n\ndef dice_coef_loss(y_true, y_pred):\n    return -dice_coef(y_true, y_pred)\n\ndef iou(y_true, y_pred):\n    intersection = K.sum(y_true * y_pred)\n    sum_ = K.sum(y_true + y_pred)\n    jac = (intersection + smooth) \/ (sum_ - intersection + smooth)\n    return jac\n\ndef jac_distance(y_true, y_pred):\n    y_truef=K.flatten(y_true)\n    y_predf=K.flatten(y_pred)\n\n    return - iou(y_true, y_pred)","25100d82":"def conv_bn_relu_block(num_kernel,input_layer):\n    x = Conv2D(num_kernel, (3,3), padding='same')(input_layer)\n    x = BatchNormalization(axis=3)(x)\n    x = Activation('relu')(x)\n    x = Conv2D(num_kernel, (3,3), padding='same')(x)\n    x = BatchNormalization(axis=3)(x)\n    x = Activation('relu')(x)\n    return x\ndef maxpool_conv_bn_relu_block(num_kernel, input_layer):\n    x = MaxPooling2D(pool_size=(2,2))(input_layer)\n    x = conv_bn_relu_block(num_kernel,x)\n    return x\ndef upconv_conv_bn_relu_block(num_kernel,input_layer,concat_layer):\n    x = concatenate([Conv2DTranspose(num_kernel, (2, 2), strides=(2, 2), padding='same')(input_layer), concat_layer], axis=3)\n    x = conv_bn_relu_block(num_kernel,x)\n    return x\n    ","7d5fb9cf":"def unet(input_size = (256,256,3)):\n    inputs = Input(input_size)\n    \n    conv_bn_relu_block_64 = conv_bn_relu_block(64,inputs)\n    maxpool_conv_bn_relu_128 = maxpool_conv_bn_relu_block(128,conv_bn_relu_block_64)\n    maxpool_conv_bn_relu_256 = maxpool_conv_bn_relu_block(256,maxpool_conv_bn_relu_128)\n    maxpool_conv_bn_relu_512 = maxpool_conv_bn_relu_block(512,maxpool_conv_bn_relu_256)\n    maxpool_conv_bn_relu_1024 = maxpool_conv_bn_relu_block(1024,maxpool_conv_bn_relu_512)\n    \n    upconv_conv_bn_relu_512 = upconv_conv_bn_relu_block(512,maxpool_conv_bn_relu_1024,maxpool_conv_bn_relu_512)\n    upconv_conv_bn_relu_256 = upconv_conv_bn_relu_block(256,upconv_conv_bn_relu_512,maxpool_conv_bn_relu_256)\n    upconv_conv_bn_relu_128 = upconv_conv_bn_relu_block(128,upconv_conv_bn_relu_256,maxpool_conv_bn_relu_128)\n    upconv_conv_bn_relu_64 = upconv_conv_bn_relu_block(64,upconv_conv_bn_relu_128,conv_bn_relu_block_64)\n    \n    last_conv = Conv2D(1, (1, 1), activation='sigmoid')(upconv_conv_bn_relu_64)\n\n    return Model(inputs=[inputs], outputs=[last_conv])\n    \n    ","e4346f79":"EPOCHS = 150\nBATCH_SIZE = 16\nlearning_rate = 1e-4","0ba72975":"train_generator_args = dict(rotation_range=0.2,\n                            width_shift_range=0.05,\n                            height_shift_range=0.05,\n                            shear_range=0.05,\n                            zoom_range=0.05,\n                            horizontal_flip=True,\n                            fill_mode='nearest')\ntrain_gen = train_generator(df_train, BATCH_SIZE,\n                                train_generator_args,\n                                target_size=(im_height, im_width))\n    \ntest_gener = train_generator(df_val, BATCH_SIZE,\n                                dict(),\n                                target_size=(im_height, im_width))\n    \nmodel = unet(input_size=(im_height, im_width, 3))\n\n\n\ndecay_rate = learning_rate \/ EPOCHS\nopt = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=decay_rate, amsgrad=False)\nmodel.compile(optimizer=opt, loss=dice_coef_loss, metrics=[\"binary_accuracy\", iou, dice_coef])\n\ncallbacks = [ModelCheckpoint('unet_brain_mri_seg.hdf5', verbose=1, save_best_only=True)]\n\nhistory = model.fit(train_gen,\n                    steps_per_epoch=len(df_train) \/ BATCH_SIZE, \n                    epochs=EPOCHS, \n                    callbacks=callbacks,\n                    validation_data = test_gener,\n                    validation_steps=len(df_val) \/ BATCH_SIZE)","b48530e2":"a = history.history\n\n\nlist_trainjaccard = a['iou']\nlist_testjaccard = a['val_iou']\n\nlist_trainloss = a['loss']\nlist_testloss = a['val_loss']\nplt.figure(1)\nplt.plot(list_testloss, 'b-',label = \"Val loss\")\nplt.plot(list_trainloss,'r-', label = \"Train loss\")\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Loss graph', fontsize = 15)\nplt.figure(2)\nplt.plot(list_trainjaccard, 'r-',label = \"Train IOU\")\nplt.plot(list_testjaccard, 'b-',label = \"Val IOU\" )\nplt.xlabel('Iteration')\nplt.ylabel('Accuracy')\nplt.title('Accuracy graph', fontsize = 15)\nplt.show()","0a0fdc56":"model = load_model('unet_brain_mri_seg.hdf5', custom_objects={'dice_coef_loss': dice_coef_loss, 'iou': iou, 'dice_coef': dice_coef})","fdd0cbe9":"test_gen = train_generator(df_test, BATCH_SIZE,\n                                dict(),\n                                target_size=(im_height, im_width))\nresults = model.evaluate(test_gen, steps=len(df_test) \/ BATCH_SIZE)\nprint(\"Test lost: \",results[0])\nprint(\"Test IOU: \",results[2])\nprint(\"Test Dice Coefficent: \",results[3])","d4d55bb3":"for i in range(30):\n    index=np.random.randint(1,len(df_test.index))\n    img = cv2.imread(df_test['filename'].iloc[index])\n    img = cv2.resize(img ,(im_height, im_width))\n    img = img \/ 255\n    img = img[np.newaxis, :, :, :]\n    pred=model.predict(img)\n\n    plt.figure(figsize=(12,12))\n    plt.subplot(1,3,1)\n    plt.imshow(np.squeeze(img))\n    plt.title('Original Image')\n    plt.subplot(1,3,2)\n    plt.imshow(np.squeeze(cv2.imread(df_test['mask'].iloc[index])))\n    plt.title('Original Mask')\n    plt.subplot(1,3,3)\n    plt.imshow(np.squeeze(pred) > .5)\n    plt.title('Prediction')\n    plt.show()","73c37175":"# **Define loss function and metrics**","f3e94fe0":"# **Create data frame and split data on train set, validation set and test set**","49b98dc2":"# **Load image's path and mask's path**","a7b8dd7c":"# **Training**","6d03894b":"# **Data Visualization**","d5466a5b":"# **Define Unet**","98f9e923":"# **Data genertator, data augmentation and adjust data**"}}