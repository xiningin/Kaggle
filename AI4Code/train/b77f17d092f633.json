{"cell_type":{"0fcd8b70":"code","e6ac2dda":"code","60081994":"code","2876643e":"code","8e209cae":"code","e5e63e4d":"code","fda23683":"code","bee1cd91":"code","b2428d7f":"code","1f507a96":"code","4506c061":"code","bf0f88db":"code","e8d4234a":"code","b0c1e129":"code","d3331a5b":"code","ac50c1e0":"code","9a7f2071":"code","e383e0a5":"code","88de0310":"markdown","262c21b6":"markdown","032c7af5":"markdown","195906be":"markdown","a61ae963":"markdown","e8d8bdf7":"markdown","0872f789":"markdown","3e5a4d2f":"markdown","27a39c93":"markdown"},"source":{"0fcd8b70":"import os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom IPython.display import clear_output\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms as T\nfrom torchvision.io import read_image\nfrom torchvision.models import resnet18\nfrom torchvision.transforms.functional import to_tensor\n\nimport warnings\nwarnings.simplefilter('ignore')","e6ac2dda":"TRAIN_IMAGES_FODLER = '\/kaggle\/input\/aaa-ml-2021\/numbers_reading_train\/'  # \u0434\u0430\u043d\u043d\u044b\u0435 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\nTEST_IMAGES_FODLER = '\/kaggle\/input\/aaa-ml-2021\/numbers_reading_test\/'  # \u0434\u0430\u043d\u043d\u044b\u0435 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  \n\nTRAIN_FILE = '\/kaggle\/input\/aaa-ml-2021\/train_images.csv'  # \u0434\u0430\u043d\u043d\u044b\u0435 \u0441 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u043e\u0439 \u0438 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435\u043c\u0438 \u0444\u0430\u0439\u043b\u043e\u0432\nTEST_FILE = '\/kaggle\/input\/aaa-ml-2021\/test_images.csv'  # \u0434\u0430\u043d\u043d\u044b\u0435 \u0441 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435\u043c\u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u0444\u0430\u0439\u043b\u043e\u0432\n\n\nMEAN = np.array([0.485, 0.456, 0.406])\nSTD = np.array([0.229, 0.224, 0.225])","60081994":"train_data = pd.read_csv(TRAIN_FILE)\ntrain_data.head()","2876643e":"def read_numpy_gray(im):\n    im = torch.from_numpy(im)\n    im = im.unsqueeze(0)\n    im = im.repeat(3, 1, 1)\n    im = im.permute(1, 2, 0)\n    return im.numpy()","8e209cae":"im = plt.imread(os.path.join(TRAIN_IMAGES_FODLER, '10008.jpg'))\nplt.imshow(read_numpy_gray(im))\n\nfor i in range(1, 9):\n    plt.plot([i * im.shape[1] \/\/ 9, i * im.shape[1] \/\/ 9], [0, 55]);","e5e63e4d":"def plot_samples(images, labels, nrows=3, ncols=3):\n\n    f, axes = plt.subplots(nrows=nrows, ncols=nrows,)\n    f.set_figheight(9)\n    f.set_figwidth(9)\n\n    for im_file, num, ax in zip(images, labels, axes.flatten()):\n        im_path = os.path.join(TRAIN_IMAGES_FODLER, im_file)\n        im = plt.imread(im_path)\n        ax.imshow(im, cmap='gray')\n        ax.set_title(num)\n        ax.axis('off')","fda23683":"sample = train_data.sample(9, random_state=1)\nplot_samples(sample['image'], sample['number']);","bee1cd91":"def plot_progress(lrs, train_losses, train_accs, train_accs_agg, test_loss, test_accs, test_accs_agg):\n    clear_output(True)\n    \n    f, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=1, ncols=4)\n    f.set_figheight(6)\n    f.set_figwidth(20)\n    \n    ax1.plot(train_losses, label='train loss')\n    ax1.plot(test_loss, label='val loss')\n    ax1.plot(np.zeros_like(train_losses), '--', label='zero')\n    ax1.set_title('Loss')\n    ax1.set_ylabel('Loss')\n    ax1.set_xlabel('Batch number')\n    ax1.legend()\n    \n    ax2.plot(train_accs, label='train CER')\n    ax2.plot(test_accs, label='val CER')\n    ax2.plot(np.ones_like(train_accs), '--', label='100% accuracy')\n    ax2.set_title('Char error rate')\n    ax2.set_ylabel('CER')\n    ax2.set_xlabel('Batch number')\n    ax2.legend()\n    \n    ax3.plot(train_accs_agg, label='train accuracy')\n    ax3.plot(test_accs_agg, label='val accuracy')\n    ax3.plot(np.ones_like(test_accs_agg), '--', label='100% accuracy')\n    ax3.set_title('Accuracy')\n    ax3.set_ylabel('Accuracy')\n    ax3.set_xlabel('Batch number')\n    ax3.legend()\n    \n    ax4.plot(lrs, label='learning rate')\n    ax4.set_title('Learing rate')\n    ax4.set_xlabel('Batch number')\n    ax4.legend()\n\n    plt.show()","b2428d7f":"letters = set(''.join(train_data.number))\nn_letters = len(letters)\n\nletter_index_map = {l: i for i, l in enumerate(letters)}\nindex_letter_map = {i: l for l, i in letter_index_map.items()}\n\nclass PlateNumberDataset(Dataset):\n    \n    def __init__(self, image_folder, images, labels, mode='fit'):\n        self.image_folder = image_folder\n        self.images = images\n        self.labels = labels\n        self.mode = mode\n        self.norm = T.Normalize(MEAN, STD)\n    \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        \n        image = read_image(os.path.join(self.image_folder, self.images[idx]))\n        image = image.repeat(3, 1, 1)\n        \n#        if self.mode =='fit':\n#            AUGMENTATIONS HERE\n        \n        image = self.norm(image.float() \/ 255.)\n        labels = self.transform_labels(self.labels[idx])\n        \n        return image, labels, idx\n    \n    @staticmethod\n    def transform_labels(label_text):\n        return torch.tensor([letter_index_map[l] for l in label_text])","1f507a96":"class PlateReader(nn.Module):\n    \n    def __init__(self, ):\n        super(PlateReader, self).__init__()\n        self.resnet = nn.Sequential(*(list(resnet18().children())[:-2]))\n        self.cnn = nn.Conv1d(in_channels=512, kernel_size=3, padding=1, out_channels=n_letters)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.resnet(x)\n        x = x.mean(axis=2)\n        x = self.cnn(x)\n        return x","4506c061":"X_train, X_val, y_train, y_val = train_test_split(\n    train_data.image, train_data.number, test_size=0.1, \n)\n\ntrain_dataset = PlateNumberDataset(image_folder=TRAIN_IMAGES_FODLER, images=X_train.values, labels=y_train.values)\nval_dataset = PlateNumberDataset(image_folder=TRAIN_IMAGES_FODLER, images=X_val.values, labels=y_val.values, mode='predict')\n\ntrain_sequence_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_sequence_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)","bf0f88db":"def schedule(step_number, breaking_step1=125, breaking_step2=400):\n    \n    if step_number < breaking_step1:\n        return step_number \/ breaking_step1\n    elif step_number < breaking_step2:\n        return 1\n    else:\n        return (0.99 ** (step_number - breaking_step2))\n\nn_steps = 800\nplt.plot(np.arange(n_steps), [schedule(step) for step in np.arange(n_steps)])\nplt.title('LR schedule')\nplt.xlabel('learing rate')\nplt.ylabel('batch number');","e8d4234a":"model = PlateReader()\nmodel.to(DEVICE)\n\nfor i, param in enumerate(model.parameters()):\n    param.requires_grad = True\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, schedule)\nloss_func = nn.CrossEntropyLoss()\n\nn_epochs = 30\n\nlosses = []\naccs = []\naccs_agg = []\n\nval_losses = []\nval_accs = []\nval_accs_agg = []\n\nlrs = []\n\n\nfor i in range(n_epochs):    \n    for j, (x_train, y_train, _) in enumerate(train_sequence_dataloader):\n        \n        x_train = x_train.to(DEVICE)\n        y_train = y_train.to(DEVICE)\n\n        model.train()\n        preds = model(x_train)\n        train_loss = loss_func(preds, y_train)\n        train_acc = (torch.argmax(preds, dim=1) == y_train).float().mean()\n        train_acc_agg = (torch.argmax(preds, dim=1) == y_train).all(dim=1).float().mean()\n        \n        optimizer.zero_grad()\n        train_loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        lrs.append(optimizer.param_groups[0][\"lr\"])\n        model.eval()\n\n        x_val, y_val, _ = next(iter(val_sequence_dataloader))\n        \n        x_val = x_val.to(DEVICE)\n        y_val = y_val.to(DEVICE)\n        \n        model.eval()\n        \n        with torch.no_grad():\n            val_preds = model(x_val)\n            val_loss = loss_func(val_preds, y_val)\n            val_acc = (torch.argmax(val_preds, dim=1) == y_val).float().mean()\n            val_acc_agg = (torch.argmax(val_preds, dim=1) == y_val).all(dim=1).float().mean()\n        \n        \n        if j > 10:\n            losses.append(train_loss.item())\n            accs.append(train_acc.item())\n            accs_agg.append(train_acc_agg.item())\n\n            val_losses.append(val_loss.item())\n            val_accs.append(val_acc.item())\n            val_accs_agg.append(val_acc_agg.item())\n\n            plot_progress(lrs, losses, accs, accs_agg, val_losses, val_accs, val_accs_agg)\n","b0c1e129":"image_augmentations = T.Compose([\n    T.Lambda(lambda x: x),\n    # AUGMENTATIONS TRANSFORMES HERE\n])\n\ndef augmented_numpy_gray(im):\n    im = torch.from_numpy(im)\n    im = im.unsqueeze(0)\n    im = im.repeat(3, 1, 1)\n    \n    # AUGMENTATIONS HERE\n    \n    im = im.permute(1, 2, 0)\n    \n    return im.numpy()","d3331a5b":"def plot_samples_with_augs(images, labels, transform=None, nrows=3, ncols=3):\n\n    f, axes = plt.subplots(nrows=nrows, ncols=nrows,)\n    f.set_figheight(9)\n    f.set_figwidth(9)\n\n    for im_file, num, ax in zip(images, labels, axes.flatten()):\n        im_path = os.path.join(TRAIN_IMAGES_FODLER, im_file)\n        im = plt.imread(im_path)\n        \n        if transform:\n            im = transform(im)\n            \n        ax.imshow(im, cmap='gray')\n        ax.set_title(num)\n        ax.axis('off')","ac50c1e0":"sample = train_data.sample(9)\nplot_sample(sample['image'], sample['number'], transform=);","9a7f2071":"class PlateNumberWithAugmentationsDataset(Dataset):\n    \n    def __init__(self, image_folder, images, labels, mode='fit'):\n        self.image_folder = image_folder\n        self.images = images\n        self.labels = labels\n        self.mode = mode\n        self.norm = T.Normalize(MEAN, STD)\n    \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        \n        image = read_image(os.path.join(self.image_folder, self.images[idx]))\n        image = image.repeat(3, 1, 1)\n        \n#        if self.mode =='fit':\n#            AUGMENTATIONS HERE\n        \n        image = self.norm(image.float() \/ 255.)\n        labels = self.transform_labels(self.labels[idx])\n        \n        return image, labels, idx\n    \n    @staticmethod\n    def transform_labels(label_text):\n        return torch.tensor([letter_index_map[l] for l in label_text])","e383e0a5":"### training code here","88de0310":"## \u0420\u0430\u0431\u043e\u0442\u0430 \u043d\u0430 \u0441\u0435\u043c\u0438\u043d\u0430\u0440\u0435: \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0434\u0430\u043d\u043d\u044b\u0445","262c21b6":"\u0420\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e, \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u044e\u0449\u0443\u044e \u043e\u0442\u0440\u0438\u0441\u043e\u0432\u0430\u0442\u044c \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u0441 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u043e\u0439 \u0432 title:","032c7af5":"\u0414\u043e\u043c\u0430\u0448\u043d\u0435\u0435 \u0437\u0430\u0434\u0430\u043d\u0438\u0435:\n* \u041f\u0440\u043e\u0431\u0438\u0442\u044c \u043f\u043e\u0440\u043e\u0433\u043e accuracy > 0.95","195906be":"\u0418\u0434\u0435\u0438 \u0434\u043b\u044f \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u044f:\n1. \u0411\u043e\u043b\u044c\u0448\u0435 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438!;\n2. \u0421\u0438\u043d\u0442\u0435\u0437 \u0434\u0430\u043d\u043d\u044b\u0445;\n3. \u0414\u0440\u0443\u0433\u043e\u0439 backbone \u0434\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 (\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e ResNet \u044d\u0442\u043e overkill);\n4. \u0414\u0440\u0443\u0433\u0430\u044f \u0433\u043e\u043b\u043e\u0432\u0430 \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438;\n5. \u0423\u043c\u0435\u043d\u0448\u0438\u0442\u044c \u0440\u0430\u0437\u043c\u0435\u0440 \u0441\u043b\u043e\u0432\u0430\u0440\u044f \u2013 \u0447\u0442\u043e\u0431\u044b \u043d\u0435 \u043f\u0443\u0442\u0430\u0442\u044c \"o\" \u0438 0;1) \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u0442\u0435 \u043a\u0430\u043a\u0438\u0435 \u0441\u043f\u043e\u0441\u043e\u0431\u044b \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0435\u0441\u0442\u044c \u0432 torchvision.transforms, \u043f\u043e\u0445\u043e\u0434\u044f\u0442 \u043b\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0434\u043b\u044f \u043d\u0430\u0448\u0435\u0439 \u0437\u0430\u0434\u0430\u0447\u0438? \u0411\u044b\u0432\u0430\u044e\u0442 \u043b\u0438 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u044f\u0442 \u0434\u043b\u044f \u043e\u0434\u043d\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0438 \u0438 \u043f\u043e\u0434\u0445\u043e\u0434\u044f\u0442 \u0434\u043b\u044f \u0434\u0440\u0443\u0433\u043e\u0439?\n6. \u0420\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u044f.","a61ae963":"\u041a\u043b\u0430\u0441\u0441 \u043c\u043e\u0434\u0435\u043b\u0438: \n1. ResNet18 \u0431\u0435\u0437 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0445 \u0434\u0432\u0443\u0445 \u0441\u043b\u043e\u0451\u0432\n2. Conv1d c \u0432\u044b\u0445\u043e\u0434\u043e\u043c \u043d\u0430 \u0440\u0430\u0437\u043c\u0435\u0440 \u0441\u043b\u043e\u0432\u0430\u0440\u044f\n","e8d8bdf7":"<img src=\"http:\/\/labelimages.avito.ru\/mlcourse_week4_augmentation.png\" style=\"width: 1200px;\">","0872f789":"\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443. \u0414\u043e\u0440\u0438\u0441\u0443\u0435\u043c \u043b\u0438\u043d\u0438\u0439 \u0440\u0430\u0437\u0434\u0435\u043b\u044f\u044e\u0449\u0438\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443 \u043d\u0430 \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u044b\u0435 \u0443\u0447\u0430\u0441\u0442\u043a\u0438:","3e5a4d2f":"\u0421\u0434\u0435\u043b\u0430\u0435\u043c \u0440\u0430\u0441\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0434\u043b\u044f \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f: \u0441\u043d\u0430\u0447\u0430\u043b\u043e \u0431\u0443\u0434\u0435\u043c \u043f\u043b\u0430\u0432\u043d\u043e \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0432\u0430\u0442\u044c \u043e\u0442 0 \u0434\u043e lr, \u043f\u043e\u0442\u043e\u043c \u043f\u043e\u0441\u0442\u0435\u043f\u0435\u043d\u043d\u043e \u0443\u043c\u0435\u043d\u044c\u0448\u0430\u0442\u044c.","27a39c93":"1) \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u0442\u0435 \u043a\u0430\u043a\u0438\u0435 \u0441\u043f\u043e\u0441\u043e\u0431\u044b \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0435\u0441\u0442\u044c \u0432 torchvision.transforms, \u043f\u043e\u0445\u043e\u0434\u044f\u0442 \u043b\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0434\u043b\u044f \u043d\u0430\u0448\u0435\u0439 \u0437\u0430\u0434\u0430\u0447\u0438? \u0411\u044b\u0432\u0430\u044e\u0442 \u043b\u0438 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u044f\u0442 \u0434\u043b\u044f \u043e\u0434\u043d\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0438 \u0438 \u043f\u043e\u0434\u0445\u043e\u0434\u044f\u0442 \u0434\u043b\u044f \u0434\u0440\u0443\u0433\u043e\u0439?"}}