{"cell_type":{"eac116e5":"code","99d08c05":"code","5da7f7ff":"code","74455e7b":"code","e6fb2dab":"code","59767b73":"code","5975f12a":"code","eb01c423":"code","4f459b18":"code","470a64ec":"code","32eda7fd":"code","ff913a1f":"code","9684d42c":"code","0fa337bb":"code","1aafb6c6":"code","aa8258ef":"code","c803e5e6":"code","8200af7c":"markdown","c468a5b3":"markdown","91aed28d":"markdown","0e0833ab":"markdown","98148907":"markdown","0abf4bb7":"markdown","65114484":"markdown","6d258335":"markdown","b3430915":"markdown","9aeb0cb1":"markdown","9ac4302a":"markdown","eb644137":"markdown","802bd711":"markdown","73dcaf12":"markdown","92648a2a":"markdown","f3e745f1":"markdown","8804b96b":"markdown","3d5631e8":"markdown"},"source":{"eac116e5":"import numpy as np\nimport pandas as pd\nimport time\nimport matplotlib.pyplot as plt\nplt.cla()   # Clear axis\nplt.clf()   # Clear figure\nplt.close('all')\n\ndf = pd.read_csv('..\/input\/mobile-health\/mhealth_raw_data.csv')\nprint(df.info())","99d08c05":"def median_filter(df,target, window):\n    median = df.rolling(window).median().drop(np.arange(window-1))\n    target = target.drop(np.arange(window-1))\n    return median, target","5da7f7ff":"def show_plot (df,df_target,plot_size,time_point, plt_title):\n    plt.figure()\n    if time_point < plot_size\/2:\n        left_point = 0\n    else:\n        left_point  = int(time_point-plot_size\/2)\n    if time_point > len(df) - plot_size\/2:\n        right_point = len(df)\n    else:\n        right_point = int(time_point+plot_size\/2)\n\n    x_c = np.arange(left_point,right_point)\n    \n    plt.plot(x_c, df['glz'][left_point:right_point],label ='arx',  color='c')\n    plt.plot(x_c, df['gry'][left_point:right_point],label ='arx',  color='blue')\n    plt.plot(x_c, df['alx'][left_point:right_point],label ='arx',  color='green')\n    plt.plot(x_c, df['arx'][left_point:right_point],label ='arx',  color='orange')\n    \n    plt.plot(x_c, 2*np.sign(df_target[left_point:right_point] -1), label ='arx', color='tab:red')\n    plt.title(plt_title)\n    plt.legend([\"glz\", \"gry\", \"alx\", \"arx\", \"Activity\"], loc =\"upper left\")\n    plt.show()","74455e7b":"df_target = df.Activity\ndf = df.drop(columns=[\"Activity\",\"subject\"])\n\nplot_size = 250\ntp = 739585\nplt_title = \"Raw data\"\nshow_plot (df,df_target,plot_size,tp,plt_title)\n","e6fb2dab":"window=5\ndf_filter, df_target = median_filter(df,df_target,window)\n\n\ndf_filter['Activity'] = df_target\ndf_filter.index = range(len(df_filter.index))\ndf_target = df_filter.Activity\n\nplt_title = \"Filtered data\"\nshow_plot (df_filter,df_target,plot_size,tp,plt_title)","59767b73":"Activity_modes = pd.DataFrame(columns=['activity','begin','end', 'duration'])\nbegin =0\nfor i in range(1,len(df_target)):\n    if df_target[i] != df_target[i-1]:\n        row = {'activity': df_target[i-1],\"begin\": begin, \"end\": i-1, \"duration\": i-1-begin}\n        Activity_modes = Activity_modes.append( row, ignore_index=True )\n        begin = i\nprint(\"list of activites:\\n\",Activity_modes)","5975f12a":"plot_size = 2000\nprint(\"Between change activity mode and change the lable 'activity' exsist gap of ~1000 samples (~20sec in sample rate 50Hz)\")\ntp =Activity_modes['begin'][143]\nplt_title = \"lable 'Activity' and real activity change \"\nshow_plot (df_filter,df_target,plot_size,tp,plt_title)\n\nprint(\"data size df_filter\",len(df_filter))\n\nprint(np.min(Activity_modes[Activity_modes['activity']==12]))\nprint(np.min(Activity_modes[Activity_modes['activity']==5]))\nprint(\"Activities  5 (Climbing stairs) and 12 (Jump front & back) has very short time periods\")\n\nprint(np.min(Activity_modes[Activity_modes['activity']==0]))\nprint(\"In several places activity #0 (Standing still ) has very short time periods\")\n","eb01c423":"Min_Activity0_time = 2000\na0 = Activity_modes[Activity_modes['activity']==0 ] \na0 = a0[a0 ['duration'] < Min_Activity0_time]\na0.index = range(len(a0.index))\nfor i in range(0,len(a0)):\n    list1 = list(np.arange(a0['begin'][i],a0['end'][i]+1))\n    df_filter=df_filter.drop(index=  list1)\n\ndf_filter.index = range(len(df_filter.index))\n#create new target vector\ndf_target = df_filter.Activity\nprint(\"size of df_filter after removing short activity #0:\",len(df_filter))\n","4f459b18":"for i in range(1,len(df_target)):\n    if df_target[i] != df_target[i-1]:\n        #print(df_target[i-1],df_target[i])\n        if df_target[i-1] == 0: #most popular activity #0 \n            left_drop = Min_Activity0_time\/2  \n        elif df_target[i-1] == 5 or df_target[i-1] == 12: #short activities 5 and 12\n            left_drop = 0\n        else:\n            left_drop = 600\n            \n        if df_target[i] == 0:\n            right_drop = Min_Activity0_time\/2\n        elif df_target[i] == 5 or df_target[i] == 12:\n            right_drop = 0\n        else:\n            right_drop = 600\n        list1 = list(np.arange(i-left_drop,i+right_drop))  \n        df_filter=df_filter.drop(index=  list1)\n        \nprint(\"size of df_filter after remove transition states \",len(df_filter))\n","470a64ec":"from sklearn.utils import resample\n \nprint(\"Downsample all activities to equal amount\") \n\neach_act =   2000 # number of indexes of each activity type\nprint(\"Size of each activity is\", each_act) \nTmax     = 125 # maximum simulation time (sec)\none_activities_max = min(df_filter.Activity.value_counts())\nif each_act > one_activities_max:\n    print(\"Error. Activity sample size exceed maximum value(\",one_activities_max,\")\")\n\ndf_mini = resample(df_filter[df_filter.Activity== 0],n_samples=each_act, random_state=42)\nfor i in range(1,max(df_filter.Activity)+1):\n    df_one  = resample(df_filter[df_filter.Activity== i],n_samples=each_act, random_state=42)\n    df_mini = pd.concat([df_mini, df_one])\n\n\ndf_mini.index = range(len(df_mini.index))\nprint(\"resampled list of activites:\\n\",df_mini.Activity.value_counts())\ndf_mini_target = df_mini[\"Activity\"]\ndf_mini = df_mini.drop(columns=[\"Activity\"])","32eda7fd":"#data normalization\ndf_mini=(df_mini-df_mini.min())\/(df_mini.max()-df_mini.min())\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_mini, df_mini_target, test_size=0.3)\n","ff913a1f":"from sklearn.neighbors import KNeighborsClassifier\nn_neighbors = 3\nTstart = time.perf_counter()\nKNeighbors = KNeighborsClassifier(n_neighbors = int(n_neighbors) )\nKNeighbors.fit(X_train, y_train)\ny_pred=KNeighbors.predict(X_test)\ndiff_vec = y_pred != y_test\nerr = int(1000*sum(diff_vec)\/len(y_pred))\/10\nTend = time.perf_counter()\nprint(\"KNeighborsC: N\", int(n_neighbors), \"error\", err, \"%, Tcalc=\", int(Tend - Tstart), \"sec\")    \n  ","9684d42c":"from sklearn.linear_model import LogisticRegression\nC =1000\nlogistic_regression= LogisticRegression(max_iter=50000, C=C)\nlogistic_regression.fit(X_train,y_train)\ny_pred=logistic_regression.predict(X_test)\ndiff_vec = y_pred != y_test\nerr = int(1000*sum(diff_vec)\/len(y_pred))\/10\nprint(\"LogisticRegression: C\", C, \"error\", err, \"%\")   ","0fa337bb":"from sklearn import svm\nC = 1000\ngamma = 10\nSVM_type = 'rbf'      \nprint(SVM_type, \" calculation\")\nTstart = time.perf_counter()\nmodel = svm.SVC(kernel=SVM_type, gamma=gamma, C=C)\nmodel = model.fit(X_train, y_train)\ny_pred = model.predict(X_test)\ndiff = np.array(y_pred) != y_test #).flatten()\nerr = int(1000*sum(diff)\/len(y_test))\/10\nTend = time.perf_counter()\nprint(\"SVC: C\", C, \"gamma\",gamma, \"error\", err, \"%, Tcalc=\", int(Tend - Tstart), \"sec\")\n","1aafb6c6":"from sklearn.ensemble import RandomForestClassifier\nn_est = 800\nmax_d = 7\nTstart = time.perf_counter()\nreg = RandomForestClassifier(n_estimators=int(n_est), max_depth=int(max_d), random_state=0)\nreg.fit(X_train, y_train)\ny_pred =  reg.predict(X_test)\ndiff_vec = y_pred != y_test\nerr = int(1000*sum(diff_vec)\/len(y_pred))\/10\nTend = time.perf_counter()\nprint(\"RandomForestC\", \"n_est\", n_est,\"max_d\", max_d ,\"error\", err, \"%, Tcalc=\", int(Tend - Tstart), \"sec\")\n","aa8258ef":"from sklearn.ensemble import GradientBoostingClassifier\nn_est = 55\nmax_d = 7\nTstart = time.perf_counter()\nreg = GradientBoostingClassifier(n_estimators=int(n_est), max_depth=int(max_d), random_state=0)\nreg.fit(X_train, y_train)\ny_pred =  reg.predict(X_test)\ndiff_vec = y_pred != y_test\nerr = int(1000*sum(diff_vec)\/len(y_pred))\/10\nTend = time.perf_counter()\nprint(\"GradientBC\", \"n_est\", n_est,\"max_d\", max_d ,\"error\", err, \"%, Tcalc=\", int(Tend - Tstart), \"sec\")\n","c803e5e6":"error_investigation = pd.DataFrame(columns=['test', 'pred', 'err'])\nerror_investigation['test'] = y_test\nerror_investigation['pred'] = y_pred\nerror_investigation[ 'err'] = diff_vec\n\nFailed = error_investigation[error_investigation[ 'err'] == True]\nFailed.index = range(len(Failed))\nactivity_labels = {\n    0: \"None\",\n    1: \"Standing still (1 min)\",\n    2: \"Sitting and relaxing (1 min)\",\n    3: \"Lying down (1 min)\",\n    4: \"Walking (1 min)\",\n    5: \"Climbing stairs (1 min)\",\n    6: \"Waist bends forward (20x)\",\n    7: \"Frontal elevation of arms (20x)\",\n    8: \"Knees bending (crouching) (20x)\",\n    9: \"Cycling (1 min)\",\n    10: \"Jogging (1 min)\",\n    11: \"Running (1 min)\",\n    12: \"Jump front & back (20x)\"\n}\nFtest = Failed.test.value_counts()\nprint(\"Error summary:\")\nfor i in Ftest.index :\n    proc = int(1000*Ftest[i]\/each_act)\/10\n    print(proc,\"%\", \" \", \"(#%2d\" %i,\")\",  activity_labels[i] )","8200af7c":"**Gradient Boost**","c468a5b3":"**Random Forest**","91aed28d":"Let's check what kind of errors we have","0e0833ab":"create list of short activites #0 and remove them","98148907":"![](http:\/\/)","0abf4bb7":"Remove indexes around the point of activity change:\n* Activity 0 - 1000 on each side\n* Activities 5 and 12 (short activities) - no drop\n* others - 600 in each side","65114484":"Let's calculate Activity timing. ","6d258335":"**Plot demonstration function.**\n* Red line - activity function. \n>     '0' - activity #0, \n>     '1'- other activities\n* Other lines - sensors data","b3430915":"# **Used functions**\n**Median filter function.** \nAfter median calculation first rows became NaN and were deleted","9aeb0cb1":"Data normalization and Train\/Test separation","9ac4302a":"After median filtered data became more \"quet\". The large window makes the \"jumps\" smaller and increases the predictive success. But let's take a small window (size 5) for an interesting challenge","eb644137":"Take plot of the random plase in data set. It can be seen that raw data has a lot of strong \"jumps\". The median filter will remove them.","802bd711":"# Activity state prediction\nThe stages of this work are:\n1. Investigate the data and remove irrational measurements\n2. Run several predictional models and find the best one\n3. Prediction time should be less than one minute\n4. Check prediction errors","73dcaf12":"Downsample all activities to equal amount\n","92648a2a":"Check features quality\n* Activity label is not coherent with the real activiy measurement.\n* Activities #5 and #12 are very short","f3e745f1":"**SVC**","8804b96b":"# Data processing\n**KNeighbors**","3d5631e8":"**Logistic regression**\n* Really bad results for all C parameters of Logistic Regression method\n"}}