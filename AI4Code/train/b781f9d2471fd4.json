{"cell_type":{"78a982e4":"code","66c74b9c":"code","84ffd0b3":"code","de878c54":"code","da1d04a1":"code","3caf3271":"code","dcc12901":"code","e446fd48":"code","556620b7":"code","9c97fa89":"code","2039931a":"code","a2110dd5":"code","2b936894":"code","68b9504a":"code","0fbc4fb0":"code","b7dc4d9f":"code","5f1f948e":"code","93ffaa1f":"code","52f98f86":"code","95eb8d51":"code","79a4b632":"code","54f240d1":"code","16b72212":"code","c80b79a4":"code","0ee08783":"code","4110ee55":"code","a580c95f":"code","17bb13dd":"code","51c344fc":"code","bf20a447":"code","d3b9dfdc":"code","ff1119dc":"code","684d5436":"markdown","a16594ee":"markdown","dac1a1e7":"markdown","15426515":"markdown","596882c9":"markdown","14096cfe":"markdown","cb6d1bf4":"markdown","9a8cbabd":"markdown"},"source":{"78a982e4":"import warnings\nwarnings.filterwarnings(\"ignore\")","66c74b9c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","84ffd0b3":"import re\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n    \nprint(tf.__version__)","de878c54":"AUTOTUNE=tf.data.experimental.AUTOTUNE\nGCS_PATH=KaggleDatasets().get_gcs_path()\nBATCH_SIZE=16*strategy.num_replicas_in_sync\nIMAGE_SIZE=[180,180]\nEPOCHS=25","da1d04a1":"filenames=tf.io.gfile.glob(str(GCS_PATH+'\/chest_xray\/train\/*\/*'))\nfilenames.extend(tf.io.gfile.glob(str(GCS_PATH+'\/chest_xray\/val\/*\/*')))\n\ntrain_filenames,val_filenames=train_test_split(filenames,test_size=0.2)","3caf3271":"COUNT_NORMAL=len([filename for filename in train_filenames if \"NORMAL\" in filename])\nprint(\"Normal images count in training set: \"+ str(COUNT_NORMAL))\n\nCOUNT_PNEUMONIA=len([filename for filename in train_filenames if \"PNEUMONIA\" in filename])\n\n\nprint(\"Pneumonia images count in training set:\"+str(COUNT_PNEUMONIA))\n","dcc12901":"train_list_ds=tf.data.Dataset.from_tensor_slices(train_filenames)\nval_list_ds=tf.data.Dataset.from_tensor_slices(val_filenames)\nfor f in train_list_ds.take(5):\n    print(f.numpy())","e446fd48":"TRAIN_IMG_COUNT=tf.data.experimental.cardinality(train_list_ds).numpy()\nprint(\"Training images: \",str(TRAIN_IMG_COUNT))\nVAL_IMG_COUNT=tf.data.experimental.cardinality(val_list_ds).numpy()\nprint(\"Vlaidating images: \",str(VAL_IMG_COUNT))","556620b7":"CLASS_NAMES=np.array([str(tf.strings.split(item,os.path.sep)\n                         [-1].numpy())[2:-1]\n                     for item in tf.io.gfile.glob(str(GCS_PATH+\"\/chest_xray\/train\/*\"))])\nCLASS_NAMES","9c97fa89":"def get_label(file_path):\n    parts=tf.strings.split(file_path,os.path.sep)\n    return parts[-2]==\"PNEUMONIA\"","2039931a":"def decode_img(img):\n    img=tf.image.decode_jpeg(img,channels=3)\n    img=tf.image.convert_image_dtype(img,tf.float32)\n    return tf.image.resize(img,IMAGE_SIZE)","a2110dd5":"def process_path(file_path):\n    label=get_label(file_path)\n    img=tf.io.read_file(file_path)\n    img=decode_img(img)\n    return img, label","2b936894":"train_ds=train_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\nval_ds=val_list_ds.map(process_path,num_parallel_calls=AUTOTUNE)","68b9504a":"for image, label in train_ds.take(1):\n    print(\"Image shape: \",image.numpy().shape)\n    print(\"Label\", label.numpy())","0fbc4fb0":"test_list_ds=tf.data.Dataset.list_files(str(GCS_PATH+'\/chest_xray\/test\/*\/*'))\nTEST_IMAGE_COUNT=tf.data.experimental.cardinality(test_list_ds).numpy()\ntest_ds=test_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\ntest_ds=test_ds.batch(BATCH_SIZE)\n\nTEST_IMAGE_COUNT","b7dc4d9f":"def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n    if cache:\n        if isinstance(cache,str):\n            ds=ds.cache(cache)\n        else:\n            ds=ds.cache()\n    ds=ds.shuffle(buffer_size=shuffle_buffer_size)\n    \n    ds=ds.repeat()\n    \n    ds=ds.batch(BATCH_SIZE)\n        \n    ds=ds.prefetch(buffer_size=AUTOTUNE)\n    return ds\n    \n    \n            ","5f1f948e":"train_ds=prepare_for_training(train_ds)\nval_ds=prepare_for_training(val_ds)\n\nimage_batch, label_batch=next(iter(train_ds))","93ffaa1f":"def show_batch(image_batch, label_batch):\n    plt.figure(figsize=(10,10))\n    for n in range(25):\n        ax=plt.subplot(5,5,n+1)\n        plt.imshow(image_batch[n])\n        if label_batch[n]:\n            plt.title(\"PNEUMONIA\")\n        else:\n            plt.title(\"NORMAL\")\n        plt.axis(\"off\")","52f98f86":"show_batch(image_batch.numpy(),label_batch.numpy())","95eb8d51":"def conv_block(filters):\n    block=tf.keras.Sequential([\n        tf.keras.layers.SeparableConv2D(filters,3,activation='relu',padding='same'),\n        tf.keras.layers.SeparableConv2D(filters,3,activation='relu',padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.MaxPool2D()\n    ])\n    return block","79a4b632":"def dense_block(units,dropout_rate):\n    block=tf.keras.Sequential([\n        tf.keras.layers.Dense(units, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(dropout_rate)\n    ])\n    return block","54f240d1":"def build_model():\n    model=tf.keras.Sequential([\n        tf.keras.Input(shape=(IMAGE_SIZE[0],IMAGE_SIZE[1],3)),\n        tf.keras.layers.Conv2D(16,3,activation='relu',padding='same'),\n        tf.keras.layers.Conv2D(16,3,activation='relu',padding='same'),\n        tf.keras.layers.MaxPool2D(),\n        \n        conv_block(32),\n        conv_block(64),\n        \n        conv_block(128),\n        \n        tf.keras.layers.Dropout(0.2),\n        \n        conv_block(256),\n        \n        tf.keras.layers.Dropout(0.2),\n        \n        tf.keras.layers.Flatten(),\n        dense_block(512,0.7),\n        dense_block(128,0.5),\n        dense_block(64,0.3),\n        \n        tf.keras.layers.Dense(1,activation='sigmoid')\n        \n    ])\n    \n    return model","16b72212":"initial_bias=np.log([COUNT_PNEUMONIA\/COUNT_NORMAL])\ninitial_bias","c80b79a4":"weight_for_0=(1\/COUNT_NORMAL)* (TRAIN_IMG_COUNT)\/2.0\nweight_for_1=(1\/COUNT_PNEUMONIA)* (TRAIN_IMG_COUNT)\/2.0\n\nclass_weight={0:weight_for_0,1:weight_for_1}\n\nprint('weigth for class 0:{:.2f}'.format(weight_for_0))\nprint('weigth for class 1:{:.2f}'.format(weight_for_1))","0ee08783":"with strategy.scope():\n    model=build_model()\n    \n    METRICS=[\n        'accuracy',\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall')\n        \n    ]\n    \n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=METRICS)\n    \n    \n    \n    ","4110ee55":"history=model.fit(\n    train_ds,\n    steps_per_epoch=TRAIN_IMG_COUNT \/\/ BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_data=val_ds,\n    validation_steps=VAL_IMG_COUNT \/\/ BATCH_SIZE,\n    class_weight=class_weight\n)","a580c95f":"# model finetuning","17bb13dd":"checkpoint_cb=tf.keras.callbacks.ModelCheckpoint(\"xray_model.h5\",save_best_only=True)\nearly_stopping_cb=tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)","51c344fc":"def exponential_decay(lr0,s):\n    def exponential_decay_fn(epoch):\n        return lr0*0.1**(epoch\/s)\n    return exponential_decay_fn\n\nexponential_decay_fn=exponential_decay(0.01,20)\n\nlr_scheduler=tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)","bf20a447":"history=model.fit(\n    train_ds,\n    steps_per_epoch=TRAIN_IMG_COUNT\/\/BATCH_SIZE,\n    epochs=100,\n    validation_data=val_ds,\n    validation_steps=VAL_IMG_COUNT\/\/BATCH_SIZE,\n    class_weight=class_weight,\n    callbacks=[checkpoint_cb, early_stopping_cb,lr_scheduler]\n)","d3b9dfdc":"fig,ax=plt.subplots(1,4,figsize=(20,5))\nax=ax.ravel()\n\nfor i, met in enumerate(['precision','recall', 'accuracy', 'loss']):\n    ax[i].plot(history.history[met])\n    ax[i].plot(history.history['val_'+met])\n    ax[i].set_title('Model {}'.format(met))\n    ax[i].set_xlabel('epochs' )\n    ax[i].set_ylabel( met)\n    ax[i].legend(['train','val'] )","ff1119dc":"loss,acc, prec,rec=model.evaluate(test_ds)","684d5436":"# CNN","a16594ee":"Visualise Dataset","dac1a1e7":"<!-- learning rate tuning -->","15426515":"Training","596882c9":"# visualising model performance","14096cfe":"Correction of data imbalance","cb6d1bf4":"learning rate finetuning","9a8cbabd":"Predict, evaluate results"}}