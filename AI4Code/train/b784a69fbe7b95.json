{"cell_type":{"8b8be204":"code","fb0f5ff0":"code","c78ffeea":"code","d0743860":"code","6e7478c7":"code","96ff1b4f":"code","e6bd190f":"code","e22816ec":"code","4457b26b":"code","9ef6557f":"code","b3cea76f":"code","3f8e932b":"code","cc84e540":"code","f06db45b":"code","f197085e":"code","c0d8f977":"code","ab9a15dc":"code","0fb6c494":"code","d34d30b3":"code","be83687e":"code","03f5d6f9":"code","7afd52d3":"code","95dd4151":"code","6dc0a16c":"code","283bddf1":"code","ad028416":"code","1b5840d5":"code","2751bedc":"code","a80553ea":"code","b86a0951":"code","fbfe5c9d":"code","3fc74ab4":"code","5f4be348":"code","6fdef4e8":"code","64231156":"code","0c01d891":"code","4f9a5daa":"code","c93f9d9b":"code","12e391d6":"code","1a25736a":"code","7937d089":"code","6cf4b7f2":"code","e5e7d7e2":"code","f87577f5":"markdown","42a93a89":"markdown","fc6488f0":"markdown","674b019b":"markdown","46fd4dfa":"markdown","6868c04c":"markdown","a87db8c6":"markdown","a620539b":"markdown","a5f410df":"markdown","01084401":"markdown"},"source":{"8b8be204":"import numpy as np \nimport pandas as pd \nimport os\nfrom glob import glob\nimport cv2\nimport ast\nfrom tqdm.notebook import tqdm\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport time\n\nimport random\nsns.set_style('whitegrid')","fb0f5ff0":"df= pd.read_csv('..\/input\/global-wheat-detection\/train.csv')\nprint('Totall Traning data: {} with {} unique images'.format(df.shape[0], len(df.image_id.unique())))\ndf.head()","c78ffeea":"def add_path(label):\n    path='..\/input\/global-wheat-detection\/train'\n    return os.path.join(path, label+'.jpg')\n\ndf['image_name']= df.image_id.apply(add_path)\ndf.head()","d0743860":"df.info()\n#bbox -- String","6e7478c7":"df.bbox=df.bbox.apply(lambda x: ast.literal_eval(x))","96ff1b4f":"df['x_min']= df.bbox.apply(lambda x: x[0])\ndf['y_min']= df.bbox.apply(lambda x: x[1])\ndf['x_max']= df.bbox.apply(lambda x: x[0]+ x[2])\ndf['y_max']= df.bbox.apply(lambda x: x[1]+ x[3])\ndf['prct_area_cov']= df.bbox.apply(lambda x: ((x[2]*x[3])\/(1024.0*1024.0))*100)\ndf=df.drop('source', 1)\ndf.head()","e6bd190f":"plt.figure(figsize=(20,6))\nsns.kdeplot(df.prct_area_cov.values, shade=True)\nplt.xlabel('Percent Area covered by Bounding Box', size=15); plt.ylabel('Probability Density', size=15)\nplt.title('Area per BBox Distribution', size=20)","e22816ec":"print('Rows we lost:', (df[(df.prct_area_cov<8) & (df.prct_area_cov>0.4)].shape[0]))\ndf=df[(df.prct_area_cov<6) & (df.prct_area_cov>0.3)]\ndf=df.reset_index()","4457b26b":"plt.figure(figsize=(20,6))\nsns.kdeplot(df.prct_area_cov.values, shade=True)\nplt.xlabel('Percent Area covered by Bounding Box', size=15); plt.ylabel('Probability Density', size=15)\nplt.title('Area per BBox Distribution', size=20)","9ef6557f":"def load(path, resize=False, gray=False):\n    img= cv2.imread(path)\n    if resize:\n        img= cv2.resize(img, (500,500))\n    if gray:\n        img= cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    else:\n        img= cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return img\n\ndef draw_rec(img, boxes):\n    for box in boxes:\n        x,y,w,h= box\n        x=int(x); y=int(y); w=int(w); h= int(h)\n        img= cv2.rectangle(img, (x,y), (x+w, y+h), color=(16, 228, 214), thickness=3)\n    return img","b3cea76f":"def draw_rand():\n    index= random.randint(0, (df.shape[0]))\n    label= df.image_id[index]\n    data= df[df.image_id== label]\n    path= '..\/input\/global-wheat-detection\/train'\n    path= os.path.join(path, (label+'.jpg'))\n    img= load(path)\n    img2= img.copy()\n    \n    img2= draw_rec(img2, data.bbox.values)\n    f, ax= plt.subplots(1,2, figsize=(25,12))\n    ax[0].imshow(img, aspect='auto'); ax[0].grid(False)\n    ax[1].imshow(img2, aspect='auto'); ax[1].grid(False)\n\n    plt.show()","3f8e932b":"draw_rand()","cc84e540":"draw_rand()","f06db45b":"df['class_name']= 'wheat_head'\ndf.head()","f197085e":"print('Original dataframe shape',df.shape)\ntest=df.image_id.unique()[-10:]\ntest","c0d8f977":"df_test= pd.DataFrame([])\n\nfor _id in tqdm(test):\n    df_2=df[df.image_id==_id]\n    df_test= pd.concat([df_test, df_2])\n    \ndf_train= df\nfor _id in tqdm(test):\n    df_train=df_train[df_train.image_id!=_id]\n    \ndf_train=df_train.reset_index()\ndf_test=df_test.reset_index()","ab9a15dc":"df_test.shape, df_train.shape","0fb6c494":"df_train= df_train[['image_name','x_min', 'y_min', 'x_max', 'y_max', 'class_name']]\nprint(df_train.shape)\ndf_train.head()","d34d30b3":"df_train.iloc[:,1:-1]=df_train.iloc[:,1:-1].astype('int32')\ndf_train.image_name= df_train.image_name.apply(lambda x: '..\/'+x)\ndf_train.head()","be83687e":"!git clone https:\/\/github.com\/fizyr\/keras-retinanet.git","03f5d6f9":"%cd keras-retinanet\/\n!pip install .","7afd52d3":"!python setup.py build_ext --inplace","95dd4151":"import tensorflow\nfrom keras_retinanet import models\nfrom keras_retinanet.utils.image import read_image_bgr ,preprocess_image, resize_image\nfrom keras_retinanet.utils.visualization import draw_box, draw_caption\nfrom keras_retinanet.utils.colors import label_color\n\nimport requests\nimport urllib","6dc0a16c":"df_train.to_csv('annotations.csv', index=False, header=None)","283bddf1":"with open(\"classes.csv\",\"w\") as file:\n    file.write(\"wheat_head,0\")","ad028416":"PRETRAINED_MODEL = 'snapshots\/_pretrained_model.h5'\n\nURL_MODEL = 'https:\/\/github.com\/fizyr\/keras-retinanet\/releases\/download\/0.5.1\/resnet50_coco_best_v2.1.0.h5'\nurllib.request.urlretrieve(URL_MODEL, PRETRAINED_MODEL)\n\nprint('Downloaded pretrained model to ' + PRETRAINED_MODEL)","1b5840d5":"!keras_retinanet\/bin\/train.py --freeze-backbone \\\n  --random-transform \\\n  --weights {PRETRAINED_MODEL} \\\n  --batch-size 8 \\\n  --steps 200 \\\n  --epochs 9 \\\n  csv annotations.csv classes.csv","2751bedc":"!ls snapshots","a80553ea":"model_path = os.path.join('snapshots', sorted(os.listdir('snapshots'), reverse=True)[0])\n\nmodel = models.load_model(model_path, backbone_name='resnet50')\nmodel = models.convert_model(model)","b86a0951":"%cd ..\/","fbfe5c9d":"def perd_from_model(path, th=0.5, box_only=False):\n    # load image\n    image = read_image_bgr(path)\n\n    # copy to draw on\n    draw = image.copy()\n    draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n\n    # preprocess image for network\n    image = preprocess_image(image)\n    image, scale = resize_image(image)\n    print('scale', scale)\n\n    # process image\n    start = time.time()\n    boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\n    print(\"processing time: \", time.time() - start)\n\n    # correct for image scale\n    boxes \/= scale\n    \n    if box_only:\n        return scores, boxes\n\n    # visualize detections\n    for box, score, label in zip(boxes[0], scores[0], labels[0]):\n    # scores are sorted so we can break\n        if score < 0.5:\n            break\n\n        color = label_color(label)\n\n        b = box.astype(int)\n        draw_box(draw, b, color=color)\n    return draw\n        \n    ","3fc74ab4":"def visu_test(df_test):\n    label=test[np.random.randint(0,9)]\n    data= df_test[df_test.image_id== label]\n    path= '..\/input\/global-wheat-detection\/train'\n    path= os.path.join(path, (label+'.jpg'))\n    img= load(path)\n    img2= img.copy()\n    \n    img2= draw_rec(img2, data.bbox.values)\n    perd= perd_from_model(path, 0.3)\n    \n    f, ax= plt.subplots(1,3, figsize=(35,12))\n    ax[0].imshow(img, aspect='auto'); ax[0].grid(False)\n    ax[1].imshow(img2, aspect='auto'); ax[1].grid(False)\n    ax[2].imshow(perd, aspect='auto'); ax[2].grid(False)\n    \n    ax[0].set_title('Original Image', size=24)\n    ax[1].set_title('Original Image with B.boxes', size=24)\n    ax[2].set_title('Predicted B.boxes with Image', size=24)\n    plt.show()\n    ","5f4be348":"visu_test(df_test)","6fdef4e8":"visu_test(df_test)","64231156":"visu_test(df_test)","0c01d891":"visu_test(df_test)","4f9a5daa":"sub=pd.read_csv('..\/input\/global-wheat-detection\/sample_submission.csv')\nsub","c93f9d9b":"sub.PredictionString[0]","12e391d6":"def perdict(label):\n    string= ''\n    path='..\/input\/global-wheat-detection\/test'\n    path= os.path.join(path, (label+'.jpg'))\n    score, boxes= perd_from_model(path, box_only=True, th=0.3)\n    \n    string=''\n    for s, b in zip(score[0], boxes[0]):\n        if s <0.3:\n            break\n        string+= '{} {} {} {} {} '.format(s, int(b[0]), int(b[1]), int(b[2]-b[0]), int(b[3]-b[1]))\n    return string\n    ","1a25736a":"sub['PredictionString']= sub.image_id.apply(perdict)","7937d089":"sub","6cf4b7f2":"sub.PredictionString= sub.PredictionString.apply(lambda x: x[:-1])","e5e7d7e2":"sub.to_csv('\/kaggle\/working\/submission.csv',index=False)","f87577f5":"## Mis-matched B.boxes fixection\n### We have encountered with Mis-matched Large Bounding Boxes in certain images for our EDA NOTEBOOK...\n\n","42a93a89":"## Image Data Visulization","fc6488f0":"# RetinaNet for Global Wheat\ud83c\udf31 Head Detection\n\n> ### This Notebook is for Training of **Keras RetinaNet** for wheat head detection \ud83d\udd0d\n\n<hr>\n\n### For EDA and Image Data analysis\ud83d\udcca visit:\n### https:\/\/www.kaggle.com\/akhileshdkapse\/global-wheat-detection-comprehensive-eda\n\n### Table of contents\n* Data Loading\n* Mis-matched B.boxes fixection\n* Image Data Visulization\n* Data Per-Processing\n* Model Traning\n* Trained model Analysis\/Visulization of test image data\n* Submission\n","674b019b":"## Data Per-Processing \n\n### Keras RetinaNet  https:\/\/github.com\/fizyr\/keras-retinanet","46fd4dfa":"## Data Loading","6868c04c":"## Submission","a87db8c6":"## Trained model Analysis\/Visulization of test image data","a620539b":"### B.B0x Area outliers:\n* We can see most of the **Bounding Boxes are coverd less than 3% of totall area** on Actuall Image.\n* **Outliers** are those BBoxes which have **coverd more than 5%** of tatall area of Image, as WHEAT HEAD covers less space in an image.","a5f410df":"## Model Traning","01084401":"![](https:\/\/www.kaggleusercontent.com\/kf\/41590794\/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..wNU8w8SLUJMdER2nLmV0uA.bE7UWnqdhT5glaFaCMq4ReTOPK-XOCw2dZ_GRP_qdx-xLst2Df5bzmX3dvAiscihs5m0Dv8N5uZwVn_L8GPwXNfCliZgPsnBBxoJU2Ubxz0j7hdI1_H4BxpWM1aFKohGmuL7vh1kgunH3UDua8i_VHxmamOT3RE3ucyNAkH567mMcNIn3P-Nil4_5QuUtbZXzLh-ovee_nvMd1rPU_KDJqap8d5-gkixkxUMlX4oyb2j1qP3-9Dx5hM6RaaHo15eLUPevPZcNxdgNTj_xJbJ2fkxQzKfe8qaYAPoDwMh16qQUzKyq2eGf9o2MSWoF97F-pjOjHf_R8ONGrdnzD0vAVkCanJjOgUJ_uU8kfV83koFj63IpNld9SwO_OwBLQjANvdfA2ifq3Ek78UGfxPxh_cX6C0hYbZmB0GTpIYkhZjEBYQ3cq6M6GFoRzP6VcwU86ZnOa23KcFSGUnlV4DQxvfBaPf4e2dqEdyy80xLShyyP0PW7VgHXRVcIGKLiceLMMiW68pR4WaAEGFfDUHUF4tzVhD5-UU8cXKhCl5z--2q3bV2Hay5sZodpU61CS0UgdLkBqQjqcyH3F6HS_Iq2Xbso-IMHAdXtsk03yXNrN1ekn32tkulIGLA_PSd6f3S6zbgdIP1Hz7c8ZBSnSlXLyCWM-wOponEhFc7r1e_9rU.OQH__iA9D8ymBfeK7ornKg\/__results___files\/__results___38_2.png)\n\n\n> ### Let's fix it "}}