{"cell_type":{"32f6c740":"code","3e26e813":"code","5145e7ec":"code","34e8c602":"code","7a011506":"code","317c0cf8":"code","906415f5":"code","c0486c0e":"code","916c26ae":"code","19428633":"code","8bc3938d":"code","fb1b586b":"code","e9898b99":"code","abf1d341":"code","55c0f81c":"code","2ad1f649":"code","7132914a":"code","2c09af09":"code","512eae43":"code","9044342a":"code","1e2d945c":"code","ab0dbc70":"code","8ff28a8a":"markdown","cbe0c93c":"markdown"},"source":{"32f6c740":"import os\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\n\nimport cv2\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nimport torchvision\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader, Dataset","3e26e813":"import pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint","5145e7ec":"!pip install timm\nimport timm","34e8c602":"def seed_everything(seed):\n    # random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(42)","7a011506":"csv_path = \"..\/input\/dogbreedclassification128x128\/labels.csv\"\ndf = pd.read_csv(csv_path)\n\nimg_folder = \"..\/input\/dogbreedclassification128x128\/train\"\n\nmodel_name = \"resnet200d\"\nbatch_size = 128\nn_classes = 120","317c0cf8":"# first checking the counts\ndf.groupby('breed').count()","906415f5":"test_df = df.groupby('breed').sample(frac=0.18)\ntest_df.groupby('breed').count()","c0486c0e":"train_df = pd.concat([df,test_df]).drop_duplicates(keep=False)\ntrain_df.groupby('breed').count()","916c26ae":"class_string = \"\"\"affenpinscher\nafghan_hound\nafrican_hunting_dog\nairedale\namerican_staffordshire_terrier\nappenzeller\naustralian_terrier\nbasenji\nbasset\nbeagle\nbedlington_terrier\nbernese_mountain_dog\nblack-and-tan_coonhound\nblenheim_spaniel\nbloodhound\nbluetick\nborder_collie\nborder_terrier\nborzoi\nboston_bull\nbouvier_des_flandres\nboxer\nbrabancon_griffon\nbriard\nbrittany_spaniel\nbull_mastiff\ncairn\ncardigan\nchesapeake_bay_retriever\nchihuahua\nchow\nclumber\ncocker_spaniel\ncollie\ncurly-coated_retriever\ndandie_dinmont\ndhole\ndingo\ndoberman\nenglish_foxhound\nenglish_setter\nenglish_springer\nentlebucher\neskimo_dog\nflat-coated_retriever\nfrench_bulldog\ngerman_shepherd\ngerman_short-haired_pointer\ngiant_schnauzer\ngolden_retriever\ngordon_setter\ngreat_dane\ngreat_pyrenees\ngreater_swiss_mountain_dog\ngroenendael\nibizan_hound\nirish_setter\nirish_terrier\nirish_water_spaniel\nirish_wolfhound\nitalian_greyhound\njapanese_spaniel\nkeeshond\nkelpie\nkerry_blue_terrier\nkomondor\nkuvasz\nlabrador_retriever\nlakeland_terrier\nleonberg\nlhasa\nmalamute\nmalinois\nmaltese_dog\nmexican_hairless\nminiature_pinscher\nminiature_poodle\nminiature_schnauzer\nnewfoundland\nnorfolk_terrier\nnorwegian_elkhound\nnorwich_terrier\nold_english_sheepdog\notterhound\npapillon\npekinese\npembroke\npomeranian\npug\nredbone\nrhodesian_ridgeback\nrottweiler\nsaint_bernard\nsaluki\nsamoyed\nschipperke\nscotch_terrier\nscottish_deerhound\nsealyham_terrier\nshetland_sheepdog\nshih-tzu\nsiberian_husky\nsilky_terrier\nsoft-coated_wheaten_terrier\nstaffordshire_bullterrier\nstandard_poodle\nstandard_schnauzer\nsussex_spaniel\ntibetan_mastiff\ntibetan_terrier\ntoy_poodle\ntoy_terrier\nvizsla\nwalker_hound\nweimaraner\nwelsh_springer_spaniel\nwest_highland_white_terrier\nwhippet\nwire-haired_fox_terrier\nyorkshire_terrier\"\"\"\n\nclass_lbl_list = class_string.split(\"\\n\")\n\nassert len(class_lbl_list) == 120, \"mismatch, missing class labels\"","19428633":"# use tpu later\n# GPU settings\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","8bc3938d":"img_transform = transforms.Compose(\n                    [transforms.ToTensor(),\n                     transforms.Normalize([0.5], [0.5])]\n                    )","fb1b586b":"class DogsDataset(Dataset):\n\n    def __init__(self, input_df, root_dir, transform=None):\n        \"\"\"\n        args:\n            csv_file (string): Path to the csv file with classes.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.df = df.sample(frac=1).reset_index(drop=True) # shuffling\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        \n        img_name = os.path.join(self.root_dir, str(self.df.iloc[idx, 0]) + \".jpg\")\n        image = cv2.imread(img_name)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n        image = np.array(image)\n        class_lbl = self.df.iloc[idx, 1]\n        class_lbl = torch.tensor([class_lbl_list.index(class_lbl)])\n\n        # one hot encoding\n        one_hot_lbl = F.one_hot(class_lbl, num_classes=n_classes)\n        one_hot_lbl = torch.squeeze(one_hot_lbl)\n\n        if self.transform:\n            transformed_image = self.transform(image)\n\n        return transformed_image, one_hot_lbl","e9898b99":"train_ds = DogsDataset(train_df, img_folder, transform = img_transform)\ntest_ds = DogsDataset(test_df, img_folder, transform = img_transform)","abf1d341":"train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\ntest_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)","55c0f81c":"img, lbls = next(iter(train_loader))","2ad1f649":"img[0].shape, lbls[0].shape","7132914a":"img_grid = torchvision.utils.make_grid(img[:25], nrow=5, normalize=True)\n\nplt.figure(figsize=(8, 8))\nplt.imshow(img_grid.permute(1, 2, 0))\nplt.show()","2c09af09":"class ResNet200DModel(pl.LightningModule):\n    def __init__(self, model_name='resnet200d', n_classes = n_classes):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=True)\n        \n        n_features = self.model.fc.in_features\n        \n        self.model.global_pool = nn.Identity() # remove global pooling\n        self.model.fc = nn.Identity() # remove existing fc layers\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        \n        # the new fc layers\n        self.fc = nn.Sequential(\n            nn.Linear(n_features, n_features \/\/ 2),\n            nn.ReLU(),\n            nn.Linear(n_features \/\/ 2, n_features \/\/ 4),\n            nn.ReLU(),\n            nn.Linear(n_features \/\/ 4, n_features \/\/ 8),\n            nn.ReLU(),\n            nn.Linear(n_features \/\/ 8, n_features \/\/ 32),\n            nn.ReLU(),\n            nn.Linear(n_features \/\/ 32, n_classes),\n            # note that I have not added sigmoid here! keep this mind while selecting loss\n        )\n        \n        # freeze layers\n        for param in self.model.parameters():\n            param.requires_grad = False\n        \n        self.criterion = nn.BCEWithLogitsLoss()\n\n    def forward(self, x):\n        bs = x.size(0)\n        features = self.model(x)\n        pooled_features = self.pooling(features).view(bs, -1)\n        output = self.fc(pooled_features)\n        return output\n    \n    def configure_optimizers(self):\n        optimizer = Adam(net.parameters(), lr=0.001, weight_decay = 1e-6, amsgrad = False)\n        return optimizer\n    \n    def training_step(self, batch, batch_idx):\n        images, labels = batch\n        outputs = self(images)\n        loss = self.criterion(outputs, labels.float())\n        return {\n                'loss': loss,\n                'y': labels.detach(), \n                'y_hat': torch.sigmoid(outputs).detach() # because no sigmoid layer in the model\n                }\n    \n    def training_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['loss'] for x in outputs]).mean().item()\n        y = torch.cat([x['y'] for x in outputs])\n        y_hat = torch.cat([x['y_hat'] for x in outputs])\n        \n        # getting the class with highest probs\n        y = torch.argmax(y, dim = 1)\n        y_hat = torch.argmax(y_hat, dim = 1)\n        \n        acc = (y_hat == y).float().mean().item()\n        print(f\"train epoch {self.current_epoch} acc:{acc:.3f} avg_train_loss:{avg_loss:.3f}\")\n        return None # on_trained_end must return None\n        \n    \n    def validation_step(self, batch, batch_idx):\n        images, labels = batch\n        outputs = self(images)\n        loss = self.criterion(outputs, labels.float())\n        return {\n                'val_loss': loss,\n                'y': labels.detach(), \n                'y_hat': torch.sigmoid(outputs).detach() # because no sigmoid layer in the model\n                }\n    \n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean().item()\n        y = torch.cat([x['y'] for x in outputs])\n        y_hat = torch.cat([x['y_hat'] for x in outputs])\n        \n        # getting the class with highest probs\n        y = torch.argmax(y, dim = 1)\n        y_hat = torch.argmax(y_hat, dim = 1)\n        \n        acc = (y_hat == y).float().mean().item()\n        print(f\"valid epoch {self.current_epoch} acc:{acc:.4} avg_val_loss:{avg_loss:.4f}\")\n        tensorboard_logs = {'val_loss': avg_loss, 'val_acc': acc}\n        return {'avg_val_loss': avg_loss,\n                'val_acc': acc,\n                'log': tensorboard_logs}","512eae43":"net = ResNet200DModel()","9044342a":"early_stop = EarlyStopping(\n    monitor='val_loss',\n    patience = 4,\n    strict = False,\n    verbose = False,\n    mode = 'min'\n)\n\ncheckpoint_callback = ModelCheckpoint(\n    monitor=\"val_loss\",\n    filename=\"resnet200d-{epoch:02d}-{val_loss:.2f}\",\n    save_top_k=3,\n    mode=\"min\",\n)","1e2d945c":"trainer = pl.Trainer(gpus=1, callbacks = [early_stop, checkpoint_callback], max_epochs=40)\ntrainer.fit(net, train_loader, test_loader)","ab0dbc70":"def make_folder(path):\n    if not os.path.exists(path):\n        os.makedirs(path, exist_ok=True)\n        \nmake_folder(\".\/saved_models\")","8ff28a8a":"## creating a validation dataset","cbe0c93c":"## useful flags\n* `min_epochs` and `max_epochs`\n* `min_steps` and `max_steps`, when both are used steps are priortized\n* `check_val_every_n_epochs`, pretty self-explanatory - default = 1\n* `val_check_interval` for long epochs, int for steps and float for percentage\n* `num_sanity_val_steps`, lifesaver\n* `limit_<type>_batches`\n\n### gpu training\n* `gpus=X`\n* `auto_select_gpus=<bool>`\n* `log_gpu_memory` - expensive as it calls nvidia-smi, options = ['all', 'min_max']\n* `benchmark` - speedups?\n* `deterministic` - removes randomness, may slow down training\n\n### dataloaders\n* `reload_dataloaders_every_epoch` - for continuously changing datasets or production systems\n\n### callbacks\n* `callbacks` = [`<add callbacks here>`, ...]"}}