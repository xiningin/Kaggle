{"cell_type":{"ef1545f0":"code","9781e4d6":"code","7d7dc26b":"code","3f65da19":"code","3ef32666":"code","2350239c":"code","e2182c82":"code","0b208496":"code","4e7df499":"code","7ea55319":"code","4f9e5a38":"code","8b1562cd":"code","e1f97224":"code","f53fe286":"code","2fde8232":"code","1652f389":"code","0d07ec2d":"code","72ab5f44":"code","52bca483":"code","ec531577":"code","6cba19bf":"code","1fc95cfb":"code","418a117c":"code","7a75b3d0":"code","6a627b92":"code","327b8d67":"code","153c06a7":"code","981af146":"code","a20842b0":"code","6e6b9a7c":"code","dabdae2a":"code","73cac310":"code","35cade1c":"code","cba34baa":"code","9c1c87bd":"code","7d23a3fb":"code","f9a83bd2":"code","62145fd6":"code","2c1c6bbe":"code","c602aa7f":"code","83e9981d":"code","cb267e10":"code","ff184a3e":"code","840da85e":"code","77612984":"code","5abc6bfa":"code","f197382c":"code","f8d6c9d6":"code","5ff403b2":"code","634d791c":"code","439327a9":"code","0ab4e419":"code","1cc773a0":"code","4386b92a":"code","ef63cd5c":"code","2a998463":"code","efc5a2e5":"code","0db0d69b":"code","ee99174c":"code","e2258bbd":"code","721ebd2e":"code","463cf25a":"code","c26847fd":"code","bbdebb8e":"code","5946ce73":"code","16a2136d":"code","24e1a412":"code","2584f485":"code","58b3b4af":"code","3a2da0ac":"code","53b391fa":"code","f86e5da6":"code","a7056fc8":"code","315fe742":"code","13e2768a":"code","b28c4903":"code","e0268854":"code","12ef90dc":"code","5b5cd38a":"code","242bb274":"code","62b4f2b6":"code","417bce99":"markdown","66f7f2f6":"markdown","987f9960":"markdown","90ea432c":"markdown","6954f58a":"markdown","d5ab59ae":"markdown","8e60ded6":"markdown","661f6db9":"markdown","ceae987c":"markdown","1f324569":"markdown","4497dfc1":"markdown","9b5b8b9f":"markdown","1f8d0e4d":"markdown","8a554e0c":"markdown","a16e8573":"markdown","782a549e":"markdown","fb4f7c61":"markdown","4e71afdb":"markdown","3f6101e7":"markdown","81da6ce8":"markdown","0395301d":"markdown","aa36c105":"markdown","07b93548":"markdown","ec8210c5":"markdown","79b934f3":"markdown","42a52aef":"markdown","706ea006":"markdown","ae04bfaf":"markdown","fd1dd52c":"markdown","ac760004":"markdown","8378bb17":"markdown","2fc2dd86":"markdown","d84e7159":"markdown","cc655816":"markdown","055b3293":"markdown","074e19d7":"markdown","1e85ba93":"markdown","82dc4e6e":"markdown","64ce0baa":"markdown","b5862f0a":"markdown","e4a7969a":"markdown","83863efa":"markdown","cfcaa297":"markdown","5e2e37df":"markdown","af17ea35":"markdown","0c60dcee":"markdown","0f669a8d":"markdown","19771478":"markdown","e3ecf1cf":"markdown","c69267a3":"markdown"},"source":{"ef1545f0":"!pip install TPOT","9781e4d6":"!pip install pycaret","7d7dc26b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.express as px\n\nimport cufflinks as cf \n\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import  RadiusNeighborsClassifier\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.svm import NuSVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nimport xgboost as xgb\nfrom sklearn.linear_model import RidgeClassifier\nfrom catboost import Pool, CatBoostClassifier, cv\nimport lightgbm as lgb\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\ninit_notebook_mode(connected=True)  \nimport plotly.figure_factory as ff\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import PCA\nimport random\n","3f65da19":"def random_colors(number_of_colors):\n    color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n                 for i in range(number_of_colors)]\n    return color","3ef32666":"train = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")","2350239c":"table = ff.create_table(train.head().round(3))\niplot(table,filename='jupyter-table1')","e2182c82":"train.columns\n","0b208496":"train.shape\n","4e7df499":"iplot(ff.create_table(train.dtypes.to_frame().reset_index().round(3)),filename='jupyter-table2')","7ea55319":"iplot(ff.create_table(train.describe().reset_index().round(3)),filename='jupyter-table2')","4f9e5a38":"train.isnull().sum()","8b1562cd":"msno.bar(train, color = 'b', figsize = (10,8))","e1f97224":"msno.matrix(train)","f53fe286":"species_count = train['Outcome'].value_counts()\ndata = [go.Bar(\n    x = species_count.index,\n    y = species_count.values,\n    marker = dict(color = random_colors(3),line=dict(color='#000000', width=2))\n)]\n\nlayout = go.Layout(\n   {\n      \"title\":\"Healthy VS DIabetic\",\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","2fde8232":"trace = go.Pie(labels = list(train.Outcome.unique()), values = list(train.Outcome.value_counts()),\n                            hole = 0.2,\n               marker=dict(colors = random_colors(3), \n                           line=dict(color='#000000', width=2)\n                           ))\ndata = [trace]\nlayout = go.Layout(\n   {\n      \"title\":\"Healthy VS Diabetic\",\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","1652f389":"data = [go.Heatmap(z = np.array(train.corr().values),\n                   x = np.array(train.corr().columns),\n                   y = np.array(train.corr().columns),\n                     colorscale='Viridis',)\n       ]\nlayout = go.Layout(dict(title = 'Correlation Matrix for variables',\n                            #autosize = False,\n                            #height  = 1400,\n                            #width   = 1600,\n                            margin  = dict(r = 0 ,l = 100,\n                                           t = 0,b = 100,\n                                         ),\n                            yaxis   = dict(tickfont = dict(size = 9)),\n                            xaxis   = dict(tickfont = dict(size = 9)),\n                           )\n                      )\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","0d07ec2d":"trace0 = go.Box(\n    name = 'Pregnancies',\n    y = train[\"Pregnancies\"]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":\"Pregnancies \",\n   }\n)\n\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","72ab5f44":"Diabetic = train[(train['Outcome'] != 0)]\nHealthy = train[(train['Outcome'] == 0)]\n\ntmp1 = Diabetic[\"Pregnancies\"]\ntmp2 = Healthy[\"Pregnancies\"]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['diabetic', 'healthy']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = \"Pregnancies\")\n\npy.iplot(fig, filename = 'Density plot')","52bca483":"fig = go.Figure(\n    data=[go.Histogram(x=train['Pregnancies'])],layout_title_text=' Pregnancies Distribution ',layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","ec531577":"fig = px.violin(train, y=\"Pregnancies\", box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","6cba19bf":"cols = \"Glucose\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","1fc95cfb":"\nDiabetic = train[(train['Outcome'] != 0)]\nHealthy = train[(train['Outcome'] == 0)]\n\ntmp1 = Diabetic[cols]\n\ntmp2 = Healthy[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['diabetic', 'healthy']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","418a117c":"fig = go.Figure(\n    data=[go.Histogram(x=train[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","7a75b3d0":"fig = px.violin(train, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","6a627b92":"cols = \"BloodPressure\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","327b8d67":"\nDiabetic = train[(train['Outcome'] != 0)]\nHealthy = train[(train['Outcome'] == 0)]\n\ntmp1 = Diabetic[cols]\n\ntmp2 = Healthy[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['diabetic', 'healthy']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","153c06a7":"fig = go.Figure(\n    data=[go.Histogram(x=train[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","981af146":"fig = px.violin(train, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","a20842b0":"cols = \"SkinThickness\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","6e6b9a7c":"\nDiabetic = train[(train['Outcome'] != 0)]\nHealthy = train[(train['Outcome'] == 0)]\n\ntmp1 = Diabetic[cols]\n\ntmp2 = Healthy[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['diabetic', 'healthy']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","dabdae2a":"fig = go.Figure(\n    data=[go.Histogram(x=train[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","73cac310":"fig = px.violin(train, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","35cade1c":"cols = \"Insulin\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","cba34baa":"\nDiabetic = train[(train['Outcome'] != 0)]\nHealthy = train[(train['Outcome'] == 0)]\n\ntmp1 = Diabetic[cols]\n\ntmp2 = Healthy[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['diabetic', 'healthy']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","9c1c87bd":"fig = go.Figure(\n    data=[go.Histogram(x=train[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","7d23a3fb":"fig = px.violin(train, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","f9a83bd2":"cols = \"BMI\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","62145fd6":"\nDiabetic = train[(train['Outcome'] != 0)]\nHealthy = train[(train['Outcome'] == 0)]\n\ntmp1 = Diabetic[cols]\n\ntmp2 = Healthy[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['diabetic', 'healthy']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","2c1c6bbe":"fig = go.Figure(\n    data=[go.Histogram(x=train[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","c602aa7f":"fig = px.violin(train, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","83e9981d":"cols = \"DiabetesPedigreeFunction\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","cb267e10":"\nDiabetic = train[(train['Outcome'] != 0)]\nHealthy = train[(train['Outcome'] == 0)]\n\ntmp1 = Diabetic[cols]\n\ntmp2 = Healthy[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['diabetic', 'healthy']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","ff184a3e":"fig = go.Figure(\n    data=[go.Histogram(x=train[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","840da85e":"fig = px.violin(train, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","77612984":"cols = \"Age\"\ntrace0 = go.Box(\n    name = cols,\n    y = train[cols]\n)\n\n\ndata = [trace0]\n\n\nlayout = go.Layout(\n   {\n      \"title\":cols,\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","5abc6bfa":"\nDiabetic = train[(train['Outcome'] != 0)]\nHealthy = train[(train['Outcome'] == 0)]\n\ntmp1 = Diabetic[cols]\n\ntmp2 = Healthy[cols]\nhist_data = [tmp1, tmp2]\n    \ngroup_labels = ['diabetic', 'healthy']\ncolors = random_colors(2)\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = 0, curve_type='kde')\n    \nfig['layout'].update(title = cols)\n\npy.iplot(fig, filename = 'Density plot')","f197382c":"fig = go.Figure(\n    data=[go.Histogram(x=train[cols])],layout_title_text=cols,layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","f8d6c9d6":"fig = px.violin(train, y=cols, box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","5ff403b2":"X = train.iloc[:,:-1].values\ny = train.iloc[:,-1].values\nencoder = LabelEncoder()\ny = encoder.fit_transform(y)\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)","634d791c":"## Logistic Regression\n\nModel = LogisticRegression()\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","439327a9":"# K-Nearest Neighbours\n\nModel = KNeighborsClassifier(n_neighbors=8)\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","0ab4e419":"## Decision Tree\n\nModel = DecisionTreeClassifier()\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","1cc773a0":"## Naive Bayes\n\nModel = GaussianNB()\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","4386b92a":"# Linear Discriminant Analysis\n\nModel = LinearDiscriminantAnalysis()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","ef63cd5c":"## Light GBM\n\nparams = {'objective':'binary', 'metric':'accuracy'}\n  \n# create dataset for lightgbm\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)\n\nModel = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_test], verbose_eval=10)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred.round()))\n","2a998463":"\nModel=CatBoostClassifier(eval_metric='Accuracy',use_best_model=True,random_seed=42)\nModel.fit(X_train,y_train,eval_set=(X_test,y_test))","efc5a2e5":"## CatBoost\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","0db0d69b":"## XGBoost\n\nModel=xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","ee99174c":"## Ridge Classifier\n\nModel=RidgeClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","e2258bbd":"## Quadratic Discriminant Analysis\n\nModel=QuadraticDiscriminantAnalysis()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","721ebd2e":"## Bagging Classifier\n\nModel=BaggingClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","463cf25a":"## MLPClassifier\n\nModel=MLPClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\n# Summary of the predictions\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","c26847fd":"## Linear Support Vector Classification\n \nModel = LinearSVC()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","bbdebb8e":"## Nu-Support Vector Classification\n\nModel = NuSVC()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","5946ce73":"## BernoulliNB\n\nModel = BernoulliNB()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","16a2136d":"## Passive Aggressive Classifier\n\nModel = PassiveAggressiveClassifier()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","24e1a412":"## Gradient Boosting Machine\nModel = GradientBoostingClassifier(n_estimators=100, random_state=9)\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))\n","2584f485":"## Adaboost\n\nModel = AdaBoostClassifier(n_estimators=100, random_state=9)\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))\n","58b3b4af":"## Extra Trees\n\nModel = ExtraTreesClassifier(n_estimators=100, max_features=3)\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))\n","3a2da0ac":"\n## Random Forest\n\nModel = RandomForestClassifier(n_estimators=100, max_features=3)\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","53b391fa":"## Support Vector Machine\n\nModel = SVC()\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","f86e5da6":"import h2o\nfrom h2o.automl import H2OAutoML\nh2o.init(max_mem_size='16G')","a7056fc8":"data = h2o.import_file('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\n\n# Identify predictors and response\nx = data.columns\ny = \"Outcome\"\nx.remove(y)\n\ndata[y] = data[y].asfactor()\n\naml = H2OAutoML(max_models=20, max_runtime_secs=1500, seed=1)\naml.train(x=x, y=y, training_frame=data)","315fe742":"lb = aml.leaderboard\nlb.head()","13e2768a":"from tpot import TPOTClassifier\nfrom tpot import TPOTRegressor\n\ntpot = TPOTClassifier(generations=5, verbosity=2)\ntpot.fit(X_train,y_train)\n","b28c4903":"y_pred=tpot.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","e0268854":"! pip install -U pycaret # Quite large depencies to install !","12ef90dc":"from pycaret.classification import *\n\nclf1 = setup(data = train, \n             target = 'Outcome',\n             silent = True)\n","5b5cd38a":"compare_models()\n","242bb274":"lgbm  = create_model('catboost')      ","62b4f2b6":"plot_model(estimator = lgbm, plot = 'learning')\n","417bce99":"<a id=\"5.18\"><\/a>\n<font color=\"blue\" size=+2.5><b> Stochastic Gradient Boosting <\/b><\/font>\n\n*Stochastic Gradient Boosting (also called Gradient Boosting Machines) are one of the most sophisticated ensemble techniques. It is also a technique that is proving to be perhaps one of the best techniques available for improving performance via ensembles. You can construct a Gradient Boosting model for classification using the GradientBoostingClassifier class*","66f7f2f6":"<a id=\"5.14\"><\/a>\n<font color=\"blue\" size=+2.5><b> Nu-Support Vector Classification <\/b><\/font>\n\n*Similar to SVC but uses a parameter to control the number of support vectors.*","987f9960":"<a id=\"5.20\"><\/a>\n<font color=\"blue\" size=+2.5><b> Extra Trees <\/b><\/font>\n\n*Extra Trees are another modification of bagging where random trees are constructed from samples of the training dataset. You can construct an Extra Trees model for classification using the ExtraTreesClassifier class*","90ea432c":"<a id=\"5.25\"><\/a>\n<font color=\"blue\" size=+2.5><b> Pycaret <\/b><\/font>\n","6954f58a":"<a id=\"3.46\"><\/a>\n<font color=\"blue\" size=+2.5><b> BMI Variable Analysis <\/b><\/font>","d5ab59ae":"<a id=\"4.1\"><\/a>\n<font color=\"blue\" size=+2.5><b> Data Handling and Preparation <\/b><\/font>\n","8e60ded6":"<a id=\"3.41\"><\/a>\n<font color=\"blue\" size=+2.5><b> Pregnancies Variable Analysis <\/b><\/font>","661f6db9":"<font size=\"+2\" color=blue ><b>Please Upvote my kernel and keep it in your favourite section if you think it is helpful.<\/b><\/font>","ceae987c":"<a id=\"1.1\"><\/a>\n<font color=\"blue\" size=+2.5><b> About Data<\/b><\/font>\n<br\/>\n<br\/>\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n## The columns in this dataset are:\n* Pregnancies: Number of times pregnant\n* Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n* BloodPressure: Diastolic blood pressure (mm Hg)\n* SkinThickness: Triceps skin fold thickness (mm)\n* Insulin: 2-Hour serum insulin (mu U\/ml)\n* BMI: Body mass index (weight in kg\/(height in m)^2)\n* DiabetesPedigreeFunction: Diabetes pedigree function\n* Age: Age (years)\n* Outcome: Class variable (0 or 1)","1f324569":"<a id=\"5.5\"><\/a>\n<font color=\"blue\" size=+2.5><b> Linear Discriminant Analysis <\/b><\/font>\n\n*Linear Discriminant Analysis or LDA is a statistical technique for binary and multiclass\nclassification. It too assumes a Gaussian distribution for the numerical input variables. You can\nconstruct an LDA model using the LinearDiscriminantAnalysis class.*","4497dfc1":"<a id=\"3.42\"><\/a>\n<font color=\"blue\" size=+2.5><b> Glucose Variable Analysis <\/b><\/font>","9b5b8b9f":"<a id=\"5.16\"><\/a>\n<font color=\"blue\" size=+2.5><b> Passive Aggressive Classifier <\/b><\/font>\n\n*Like MultinomialNB, this classifier is suitable for **discrete data**. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary\/boolean features.*\nThe Passive-Aggressive algorithms are a family of Machine learning algorithms that are not very well known by beginners and even intermediate Machine Learning enthusiasts. However, they can be very useful and efficient for certain applications.","1f8d0e4d":"![image.png](attachment:image.png)","8a554e0c":"<a id=\"3\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center>  Exploratory Data Analysis <\/center><\/h2>","a16e8573":"<font size=\"+3\" color=blue><b> <center><u> PIMA + EDA + Plotly + (25+) Models For Beginners <\/u><\/center><\/b><\/font>","782a549e":"<a id=\"5.9\"><\/a>\n<font color=\"blue\" size=+2.5><b> Ridge Classifier <\/b><\/font>\n\n*Classifier using Ridge regression. This classifier first converts the target values into {-1, 1} and then treats the problem as a \nregression task (multi-output regression in the multiclass case).*","fb4f7c61":"<a id=\"5.22\"><\/a>\n<font color=\"blue\" size=+2.5><b> Support Vector Machine <\/b><\/font>\n\n*Support Vector Machines (or SVM) seek a line that best separates two classes. Those data instances that are closest to the line that best separates the classes are called support vectors and influence where the line is placed. SVM has been extended to support multiple classes Of particular importance is the use of different kernel functions via the kernel parameter .A powerful Radial Basis Function is used by default. You can construct an SVM model using the SVC class.*","4e71afdb":"<a id=\"5\"><\/a>\n<font color=\"blue\" size=+2.5><b> Model Training <\/b><\/font>\n","3f6101e7":"<a id=\"5.19\"><\/a>\n<font color=\"blue\" size=+2.5><b> \nAdaBoost <\/b><\/font>\n\n*AdaBoost was perhaps the first successful boosting ensemble algorithm. It generally works by weighting instances in the dataset by how easy or difficult they are to classify, allowing the algorithm to pay or less attention to them in the construction of subsequent models. You can construct an AdaBoost model for classification using the AdaBoostClassifier class*","81da6ce8":"<a id=\"2.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Import Dataset <\/b><\/font>\n","0395301d":"<a id=\"5.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> K-Nearest Neighbours <\/b><\/font>\n\n*The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems*","aa36c105":"<a id=\"5.10\"><\/a>\n<font color=\"blue\" size=+2.5><b> Quadratic Discriminant Analysis <\/b><\/font>\n\n\n*A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule.\nThe model fits a **Gaussian** density to each class.*","07b93548":"<a id=\"5.6\"><\/a>\n<font color=\"blue\" size=+2.5><b> LightGBM <\/b><\/font>\n\n*LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:*\n\nFaster training speed and higher efficiency.\nLower memory usage.\nBetter accuracy.\nSupport of parallel and GPU learning.\nCapable of handling large-scale data.","ec8210c5":"<a id=\"5.12\"><\/a>\n<font color=\"blue\" size=+2.5><b> MLPClassifier  <\/b><\/font>\n\n*MLPClassifier stands for Multi-layer Perceptron classifier which in the name itself connects to a Neural Network. Unlike other classification algorithms such as Support Vectors or Naive Bayes Classifier, MLPClassifier relies on an underlying Neural Network to perform the task of classification.*","79b934f3":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center>Objective  <\/center><\/h2>\n\nGoal of this kernel is following:\n- Basic Exploratory Data Analysis.\n- Beginners guide on PIMA India Diabetes Dataset.\n- Feature Analysis\n- Modelling on 25+ Models","42a52aef":"<a id=\"5.3\"><\/a>\n<font color=\"blue\" size=+2.5><b> Decision Tree (CART) <\/b><\/font>\n\n*Classification and Regression Trees (CART or just decision trees) construct a binary tree from\nthe training data. Split points are chosen greedily by evaluating each attribute and each value\nof each attribute in the training data in order to minimize a cost function (like the Gini index).\nYou can construct a CART model using the DecisionTreeClassifier class*","706ea006":"<a id=\"5.8\"><\/a>\n<font color=\"blue\" size=+2.5><b> XGBoost <\/b><\/font>\n\n*XGBoost stands for Extreme Gradient Boosting, it is a performant machine learning library based on the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman. XGBoost implements a Gradient Boosting algorithm based on decision trees.*","ae04bfaf":"<a id=\"5.13\"><\/a>\n<font color=\"blue\" size=+2.5><b> Linear Support Vector Classification  <\/b><\/font>\n\n*Similar to **SVC** with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.*","fd1dd52c":"<a id=\"3.43\"><\/a>\n<font color=\"blue\" size=+2.5><b> BloodPressure Variable Analysis <\/b><\/font>","ac760004":"<a id=\"5.1\"><\/a>\n<font color=\"blue\" size=+2.5><b> Logistic Regression <\/b><\/font>\n\n*Logistic regression assumes a Gaussian distribution for the numeric input variables and can\nmodel binary classification problems. You can construct a logistic regression model using the\nLogisticRegression class*","8378bb17":"<a id=\"3.48\"><\/a>\n<font color=\"blue\" size=+2.5><b> Age Variable Analysis <\/b><\/font>","2fc2dd86":"<a id=\"5.11\"><\/a>\n<font color=\"blue\" size=+2.5><b> Bagging classifier  <\/b><\/font>\n\n*A Bagging classifier is an ensemble **meta-estimator** that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.*","d84e7159":"<a id=\"5.24\"><\/a>\n<font color=\"blue\" size=+2.5><b> TPOT <\/b><\/font>","cc655816":"<a id=\"3.47\"><\/a>\n<font color=\"blue\" size=+2.5><b> DiabetesPedigreeFunction Variable Analysis <\/b><\/font>","055b3293":"<a id=\"5.21\"><\/a>\n<font color=\"blue\" size=+2.5><b> Random Forest <\/b><\/font>\n\n*Random Forests is an extension of bagged decision trees. Samples of the training dataset are taken with replacement, but the trees are constructed in a way that reduces the correlation between individual classifiers. Specifically, rather than greedily choosing the best split point in the construction of each tree, only a random subset of features are considered for each split. You can construct a Random Forest model for classification using the RandomForestClassifier class.*","074e19d7":"<a id=\"5.7\"><\/a>\n<font color=\"blue\" size=+2.5><b> CatBoost <\/b><\/font>\n\n*CatBoost is an algorithm for gradient boosting on decision trees. It is developed by Yandex researchers and engineers, and is used for search, recommendation systems, personal assistant, self-driving cars, weather prediction and many other tasks at Yandex and in other companies, including CERN, Cloudflare, Careem taxi. It is in open-source and can be used by anyone.*","1e85ba93":"<a id=\"3.44\"><\/a>\n<font color=\"blue\" size=+2.5><b> SkinThickness Variable Analysis <\/b><\/font>","82dc4e6e":"\n\n<a id=\"1\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center>  Introduction  <\/center><\/h2>","64ce0baa":"<a id=\"2.1\"><\/a>\n<font color=\"blue\" size=+2.5><b> Import Libraries <\/b><\/font>\n","b5862f0a":"<a id=\"2\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center>  Load and Check Data  <\/center><\/h2>","e4a7969a":"<a id=\"5.15\"><\/a>\n<font color=\"blue\" size=+2.5><b> BernoulliNB <\/b><\/font>\n\n*Like MultinomialNB, this classifier is suitable for **discrete data**. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary\/boolean features.*","83863efa":"<a id=\"5.23\"><\/a>\n<font color=\"blue\" size=+2.5><b> H2O <\/b><\/font>","cfcaa297":"# Upvote The Kernel If you like my work","5e2e37df":"<a id=\"3.62\"><\/a>\n<font color=\"blue\" size=+2.5><b> Correlation <\/b><\/font>\n","af17ea35":"<a id=\"5.4\"><\/a>\n<font color=\"blue\" size=+2.5><b> Naive Bayes <\/b><\/font>\n\n*Naive Bayes calculates the probability of each class and the conditional probability of each class\ngiven each input value. These probabilities are estimated for new data and multiplied together,\nassuming that they are all independent (a simple or naive assumption). When working with\nreal-valued data, a Gaussian distribution is assumed to easily estimate the probabilities for\ninput variables using the Gaussian Probability Density Function. You can construct a Naive\nBayes model using the GaussianNB class*","0c60dcee":"<a id=\"3.45\"><\/a>\n<font color=\"blue\" size=+2.5><b> Insulin Variable Analysis <\/b><\/font>","0f669a8d":"<a id=\"3.1\"><\/a>\n<font color=\"blue\" size=+2.5><b> Missing Value Analysis <\/b><\/font>\n","19771478":"<a id=\"3.4\"><\/a>\n<font color=\"blue\" size=+2.5><b> Non-Target Variable Analysis <\/b><\/font>\n","e3ecf1cf":"<a id=\"top\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"> <center> Table of content <\/center><\/h2>\n\n<font color=\"blue\" size=+1><b>Introduction<\/b><\/font>\n* [About Data ](#1.1)\n* [Data Dictionary ](#1.3)\n* [Data Variable](#1.4)\n    \n<font color=\"blue\" size=+1><b> Load and Check Data <\/b><\/font>\n* [Importing Library](#2.1)\n* [Load Dataset](#2.2)\n\n<font color=\"blue\" size=+1><b> Exploratory Data Analysis <\/b><\/font>\n* [Missing Value Analysis](#3.1)\n* [Target Variable Analysis](#3.2)    \n* [Non-Target Variable Analysis](#3.4)   \n    *     [Pregnancies](#3.41)\t\n    *     [Glucose](#3.42)\n    *     [BloodPressure](#3.43)\n    *     [SkinThickness](#3.44)\n    *     [Insulin](#3.45)\n    *     [BMI](#3.46)\n    *     [DiabetesPedigreeFunction](#3.47)\n    *     [Age](#3.48)\n \n<font color=\"blue\" size=+1><b> Data Handling and Preparation <\/b><\/font>\n* [Handing Missing Data ](#4.1)\n* [Train Test Split ](#4.2)\n\n<font color=\"blue\" size=+1><b> Model Training <\/b><\/font>\n* [Logistic Regression ](#5.1)\n* [K-Nearest Neighbours ](#5.2)    \n* [Decision Tree ](#5.3)\n* [Naive Bayes ](#5.4)    \n* [Linear Discriminant Analysis ](#5.5)\n* [LightLGM ](#5.6)    \n* [CatBoost ](#5.7)\n* [XGBoost ](#5.8)    \n* [Ridge Classifier ](#5.9)\n* [Quadratic Discriminant Analysis ](#5.10)    \n* [Bagging classifier ](#5.11)\n* [MLPClassifier](#5.12)    \n* [Linear Support Vector Classification ](#5.13)\n* [Nu-Support Vector Classification ](#5.14)    \n* [BernoulliNB ](#5.15)\n* [Passive Aggressive Classifier ](#5.16)    \n* [Stochastic Gradient Boosting ](#5.18)    \n* [AdaBoost ](#5.19)\n* [Extra Trees ](#5.20)    \n* [Random Forest ](#5.21)\n* [SVC ](#5.22)    \n* [H2O ](#5.23)\n* [TPOT ](#5.24)    \n* [PyCaret ](#5.25)\n","c69267a3":"<a id=\"3.2\"><\/a>\n<font color=\"blue\" size=+2.5><b> Target Variable Analysis <\/b><\/font>\n"}}