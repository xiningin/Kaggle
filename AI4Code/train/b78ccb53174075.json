{"cell_type":{"554b3734":"code","cbf0fb42":"code","0d4c07e4":"code","2d3260b5":"code","8b8d1910":"code","f7790c8c":"code","fa0b19a7":"code","a67ec464":"code","e5875ffc":"code","11e5766a":"code","bcd36594":"code","cdfc5f6d":"code","47af665e":"code","99d8f00c":"code","dd7c08ca":"code","42aeda3b":"code","990ac611":"code","ac70e6f2":"code","c6561d1d":"code","26cd59de":"code","ea78513f":"code","3a8f8a19":"code","8049db90":"code","162b689b":"code","a0ce7161":"code","5925b323":"code","dc28f7be":"markdown","517dfd76":"markdown","dbc1a428":"markdown","5a98ce2a":"markdown","c14edc1d":"markdown","a7945752":"markdown","a76b586c":"markdown"},"source":{"554b3734":"import pandas as pd\nimport numpy as np\n\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom sklearn.preprocessing import LabelEncoder\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\nfrom catboost import CatBoostClassifier, Pool\n \nfrom sklearn import model_selection\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression,LinearRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score,accuracy_score\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom wordcloud import WordCloud\n\nimport tensorflow.keras\nimport sklearn\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D\n\nimport warnings\nwarnings.filterwarnings('ignore')","cbf0fb42":"train1=pd.read_csv(\"..\/input\/its-controversial\/The_Rohingya_Genocide.csv\",names=['text','target'],header=0)\ntrain2=pd.read_csv(\"..\/input\/its-controversial\/Black_Sox.csv\",names=['text','target'],header=0)\ntrain3=pd.read_csv(\"..\/input\/its-controversial\/Enron.csv\",names=['text','target'],header=0)\ntrain4=pd.read_csv(\"..\/input\/its-controversial\/Equifax_Data_Breach.csv\",names=['text','target'],header=0)\ntrain5=pd.read_csv(\"..\/input\/its-controversial\/NSA_spying.csv\",names=['text','target'],header=0)\ntrain6=pd.read_csv(\"..\/input\/its-controversial\/Privacy_Leak.csv\",names=['text','target'],header=0)\ntrain7=pd.read_csv(\"..\/input\/its-controversial\/WeWorkIPO.csv\",names=['text','target'],header=0)\ntrain8=pd.read_csv(\"..\/input\/its-controversial\/cambridge_analytica.csv\",names=['text','target'],header=0)\ntrain=pd.concat([train1,train2,train3,train4,train5,train6,train7,train8], ignore_index=True)\ntest=pd.read_csv(\"..\/input\/its-controversial\/Black_lives_Eric.csv\",names=['text','target'],header=0)","0d4c07e4":"train.head(6)","2d3260b5":"train.head()","8b8d1910":"print(\"Train Dataset missing data:\\n\",train.isnull().sum(),\"\\n\")","f7790c8c":"# lowering the text\ntrain.text=train.text.apply(lambda x:x.lower() )\ntest.text=test.text.apply(lambda x:x.lower())\n#removing square brackets\ntrain.text=train.text.apply(lambda x:re.sub('\\[.*?\\]', '', x) )\ntest.text=test.text.apply(lambda x:re.sub('\\[.*?\\]', '', x) )\ntrain.text=train.text.apply(lambda x:re.sub('<.*?>+', '', x) )\ntest.text=test.text.apply(lambda x:re.sub('<.*?>+', '', x) )\n#removing hyperlink\ntrain.text=train.text.apply(lambda x:re.sub('https?:\/\/\\S+|www\\.\\S+', '', x) )\ntest.text=test.text.apply(lambda x:re.sub('https?:\/\/\\S+|www\\.\\S+', '', x) )\n#removing puncuation\ntrain.text=train.text.apply(lambda x:re.sub('[%s]' % re.escape(string.punctuation), '', x) )\ntest.text=test.text.apply(lambda x:re.sub('[%s]' % re.escape(string.punctuation), '', x) )\ntrain.text=train.text.apply(lambda x:re.sub('\\n' , '', x) )\ntest.text=test.text.apply(lambda x:re.sub('\\n', '', x) )\n#remove words containing numbers\ntrain.text=train.text.apply(lambda x:re.sub('\\w*\\d\\w*' , '', x) )\ntest.text=test.text.apply(lambda x:re.sub('\\w*\\d\\w*', '', x) )\n\ntrain.text.head()","fa0b19a7":"negetive_tweets = train[train['target']==\"negative\"]['text']\npositive_tweets = train[train['target']==\"positive\"]['text']","a67ec464":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[16, 8])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(positive_tweets))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Positive Tweets',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(negetive_tweets))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negetive Tweets',fontsize=40);","e5875ffc":"#Tokenizer\ntoken=nltk.tokenize.RegexpTokenizer(r'\\w+')\n#applying token\ntrain.text=train.text.apply(lambda x:token.tokenize(x))\ntest.text=test.text.apply(lambda x:token.tokenize(x))\n#view\ndisplay(train.text.head())","11e5766a":"#removing stop words\ntrain.text=train.text.apply(lambda x:[w for w in x if w not in stopwords.words('english')])\ntest.text=test.text.apply(lambda x:[w for w in x if w not in stopwords.words('english')])\n#view\ntrain.text.head()","bcd36594":"#stemmering the text and joining\nstemmer = nltk.stem.PorterStemmer()\ntrain.text=train.text.apply(lambda x:\" \".join(stemmer.stem(token) for token in x))\ntest.text=test.text.apply(lambda x:\" \".join(stemmer.stem(token) for token in x))\n#View\ntrain.text.head()","cdfc5f6d":"count_vectorizer = CountVectorizer()\ntrain_vectors_count = count_vectorizer.fit_transform(train['text'])\ntest_vectors_count = count_vectorizer.transform(test[\"text\"])\n\n","47af665e":"CLR = LogisticRegression(C=2)\nscores = model_selection.cross_val_score(CLR, train_vectors_count, train[\"target\"], cv=6, scoring=\"accuracy\")\nscores","99d8f00c":"tfidf = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    ngram_range=(1, 1),\n    max_features=10000)\ntrain_tfidf = tfidf.fit_transform(train['text'])\ntest_tfidf = tfidf.transform(test[\"text\"])","dd7c08ca":"NB_Vec = MultinomialNB()\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\nscores = model_selection.cross_val_score(NB_Vec, train_vectors_count, train[\"target\"], cv=cv, scoring=\"accuracy\")\nscores","42aeda3b":"NB_TF = MultinomialNB()\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\nscores = model_selection.cross_val_score(NB_TF, train_tfidf, train[\"target\"], cv=cv, scoring=\"accuracy\")\nscores","990ac611":"from catboost import CatBoostClassifier\ncat=LogisticRegression(C=4)\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\nscores = model_selection.cross_val_score(cat, train_tfidf, train[\"target\"], cv=cv, scoring=\"accuracy\")\nscores","ac70e6f2":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm, tree","c6561d1d":"cat=RandomForestClassifier()\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\nscores = model_selection.cross_val_score(cat, train_tfidf, train[\"target\"], cv=cv, scoring=\"accuracy\")\nscores","26cd59de":"cat=svm.SVC()\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\nscores = model_selection.cross_val_score(cat, train_tfidf, train[\"target\"], cv=cv, scoring=\"accuracy\")\nscores","ea78513f":"cat=tree.DecisionTreeClassifier()\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\nscores = model_selection.cross_val_score(cat, train_tfidf, train[\"target\"], cv=cv, scoring=\"accuracy\")\nscores","3a8f8a19":"cat=XGBClassifier()\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\nscores = model_selection.cross_val_score(cat, train_tfidf, train[\"target\"], cv=cv, scoring=\"accuracy\")\nscores","8049db90":"cat1=RandomForestClassifier()\ncat=XGBClassifier()\ncat.fit(train_tfidf,train.target)","162b689b":"pred=cat.predict(test_tfidf)","a0ce7161":"accuracy_score(test.target,pred)","5925b323":"test['predict']=pred\ntest.head()","dc28f7be":"## Tokenizeing text","517dfd76":"## Text Processing","dbc1a428":"## Model Testing","5a98ce2a":"## word cloud","c14edc1d":"## Datasets","a7945752":"## Importing","a76b586c":"## Best Model"}}