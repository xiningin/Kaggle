{"cell_type":{"93ab8dac":"code","1f83e794":"code","289f0ff3":"code","574faa80":"code","536e24a8":"code","a729aab1":"code","7379401f":"code","f3e5ce6e":"code","11419aaf":"code","e354313b":"code","ec2508b8":"code","5cf278a3":"code","5090c5ad":"code","cf615d20":"code","d627b4c9":"code","98c26135":"code","e241e925":"code","55b91976":"code","d20f1982":"code","e89a376a":"code","a88e9c68":"code","71dc21cd":"code","ef54178b":"code","949a7607":"code","a187cc81":"code","7fc0fdda":"code","5fd4820f":"code","9cf0ffac":"code","4e7d0227":"markdown","7524eb60":"markdown","2fb8c609":"markdown","d242f6f2":"markdown","13f13f6c":"markdown","5bdde960":"markdown","f2ee5156":"markdown","568149d0":"markdown","d985b89e":"markdown","c4ba17d1":"markdown","193dbee6":"markdown","dfd9a0e4":"markdown","669b7fdd":"markdown","3881c995":"markdown","87d38aa4":"markdown","f7d0367b":"markdown","98393049":"markdown","b72eea96":"markdown","ec4ea7c4":"markdown","cb610527":"markdown","77f75f35":"markdown","3046ffad":"markdown","e8666de6":"markdown","8be70328":"markdown","41433e6e":"markdown","acb1020e":"markdown","dde1242a":"markdown","9ff56c70":"markdown","287ebe27":"markdown","958b3775":"markdown","334b99fa":"markdown","5dfcb1d1":"markdown","781cf42e":"markdown","1bd8653b":"markdown","85e1f33f":"markdown","5ccca472":"markdown","a231391b":"markdown","41d6abb1":"markdown","13c3c00c":"markdown"},"source":{"93ab8dac":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split,cross_val_score,KFold,GridSearchCV\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score\nimport scikitplot as skplt\n","1f83e794":"dataset=pd.read_csv(\"..\/input\/adult.csv\")","289f0ff3":"print(dataset.isnull().sum())\nprint(dataset.dtypes)","574faa80":"dataset.head()","536e24a8":"#removing '?' containing rows\ndataset = dataset[(dataset != '?').all(axis=1)]\n#label the income objects as 0 and 1\ndataset['income']=dataset['income'].map({'<=50K': 0, '>50K': 1})","a729aab1":"sns.catplot(x='education.num',y='income',data=dataset,kind='bar',height=6)\nplt.show()","7379401f":"#explore which country do most people belong\nplt.figure(figsize=(38,14))\nsns.countplot(x='native.country',data=dataset)\nplt.show()","f3e5ce6e":"#marital.status vs income\nsns.factorplot(x='marital.status',y='income',data=dataset,kind='bar',height=8)\nplt.show()","11419aaf":"#relationship vs income\nsns.factorplot(x='relationship',y='income',data=dataset,kind='bar',size=7)\nplt.show()","e354313b":"#we can reformat marital.status values to single and married\ndataset['marital.status']=dataset['marital.status'].map({'Married-civ-spouse':'Married', 'Divorced':'Single', 'Never-married':'Single', 'Separated':'Single', \n'Widowed':'Single', 'Married-spouse-absent':'Married', 'Married-AF-spouse':'Married'})","ec2508b8":"for column in dataset:\n    enc=LabelEncoder()\n    if dataset.dtypes[column]==np.object:\n         dataset[column]=enc.fit_transform(dataset[column])","5cf278a3":"plt.figure(figsize=(14,10))\nsns.heatmap(dataset.corr(),annot=True,fmt='.2f')\nplt.show()","5090c5ad":"dataset=dataset.drop(['relationship','education'],axis=1)","cf615d20":"dataset=dataset.drop(['occupation','fnlwgt','native.country'],axis=1)","d627b4c9":"print(dataset.head())","98c26135":"X=dataset.iloc[:,0:-1]\ny=dataset.iloc[:,-1]\nprint(X.head())\nprint(y.head())\nx_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.33,shuffle=False)","e241e925":"clf=GaussianNB()\ncv_res=cross_val_score(clf,x_train,y_train,cv=10)\nprint(cv_res.mean()*100)","55b91976":"clf=DecisionTreeClassifier()\ncv_res=cross_val_score(clf,x_train,y_train,cv=10)\nprint(cv_res.mean()*100)","d20f1982":"clf=RandomForestClassifier(n_estimators=100)\ncv_res=cross_val_score(clf,x_train,y_train,cv=10)\nprint(cv_res.mean()*100)","e89a376a":"'''\n---USED GRIDSEARCH FOR HYPERPARAMETER TUNING-----\nclf=RandomForestClassifier()\nkf=KFold(n_splits=3)\nmax_features=np.array([1,2,3,4,5])\nn_estimators=np.array([25,50,100,150,200])\nmin_samples_leaf=np.array([25,50,75,100])\nparam_grid=dict(n_estimators=n_estimators,max_features=max_features,min_samples_leaf=min_samples_leaf)\ngrid=GridSearchCV(estimator=clf,param_grid=param_grid,cv=kf)\ngres=grid.fit(x_train,y_train)\nprint(\"Best\",gres.best_score_)\nprint(\"params\",gres.best_params_)\n\n----------------OUTPUT------------------------\nBest 0.810471100554236\nparams {'max_features': 5, 'min_samples_leaf': 50, 'n_estimators': 50}\n'''","a88e9c68":"clf=RandomForestClassifier(n_estimators=50,max_features=5,min_samples_leaf=50)\nclf.fit(x_train,y_train)","71dc21cd":"pred=clf.predict(x_test)\npred","ef54178b":"print(\"Accuracy: %f \" % (100*accuracy_score(y_test, pred)))","949a7607":"import xgboost as xgb\nxgb.__version__","a187cc81":"dmat=xgb.DMatrix(x_train,y_train)\ntest_dmat=xgb.DMatrix(x_test)","7fc0fdda":"from skopt import BayesSearchCV \nimport warnings\nwarnings.filterwarnings('ignore', message='The objective has been evaluated at this point before.')\n\nparams={'min_child_weight': (0, 10),\n        'max_depth': (0, 30),\n        'subsample': (0.5, 1.0, 'uniform'),\n        'colsample_bytree': (0.5, 1.0, 'uniform'),\n        'n_estimators':(50,100),\n        'reg_lambda':(1,100,'log-uniform'),\n        }\n\nbayes=BayesSearchCV(estimator=xgb.XGBClassifier(objective='binary:logistic',eval_metric='error',eta=0.1),search_spaces=params,n_iter=50,scoring='accuracy',cv=5)\nres=bayes.fit(x_train,y_train)\nprint(res.best_params_)\nprint(res.best_score_)","5fd4820f":"final_p={'colsample_bytree': 1.0, 'max_depth': 3, 'min_child_weight': 0,'subsample': 0.5,'reg_lambda': 100.0,'objective':'binary:logistic','eta': 0.1,'n_estimators':50, \"silent\": 1}\ncv_res=xgb.cv(params=final_p,dtrain=dmat,num_boost_round=1000,early_stopping_rounds=100,metrics=['error'],nfold=5)\ncv_res.tail()","9cf0ffac":"final_clf=xgb.train(params=final_p,dtrain=dmat,num_boost_round=837)\npred=final_clf.predict(test_dmat)\nprint(pred)\npred[pred > 0.5 ] = 1\npred[pred <= 0.5] = 0\nprint(pred)\nprint(accuracy_score(y_test,pred)*100)\n","4e7d0227":"## Feature Engineering","7524eb60":"Here most people are from the USA,so we can drop this column as it creates unnecessary bias.","2fb8c609":"## Finalize the Model","d242f6f2":" ### *Label encoding*","13f13f6c":"## Load libraries","5bdde960":"## **XGBoost**","f2ee5156":"# **ADULT CENSUS INCOME PREDICTION**","568149d0":"Grid and random search are completely uninformed by past evaluations, and as a result, often spend a significant amount of time evaluating \u201cbad\u201d hyperparameters.But bayesain optimization are informed of previous evaluations.A very lucid explanation of it,that has helped me a lot, is given by William Koehrsen [here](https:\/\/towardsdatascience.com\/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f).\nThe library that i have used here is scikit-optimization([skopt](https:\/\/scikit-optimize.github.io\/)).This library provides us with the BayesSearchCV method.","d985b89e":"## Model Tuning","c4ba17d1":"### Gridsearch","193dbee6":"Random forest follows the concept of bagging,the other method is boosting.In Boosting algorithms each classifier is trained on data, taking into account the previous classifiers\u2019 success. After each training step, the weights are redistributed. Misclassified data increases its weights to emphasise the most difficult cases. In this way, subsequent learners will focus on them during their training.\n\nFew boosting algorihtms are adaboost,gradientboosting,XGBoost.XGBoost is one of the most popular machine learning algorithm these days. \n\n[XGBoost](https:\/\/xgboost.readthedocs.io\/en\/latest\/) (Extreme Gradient Boosting) belongs to a family of boosting algorithms and uses the gradient boosting (GBM) framework at its core.\n\nTo install XGBoost on your system using [conda](https:\/\/anaconda.org\/conda-forge\/xgboost).","dfd9a0e4":"Higher the value ,higher the probability of income greater than 50k(Obviously!)","669b7fdd":"Split the dataset into predictors and target and make training and testing sets","3881c995":"  Model Tuning is defined as tuning the parameters(hyperparameters) of our model so as to increase the performance of our classifier","87d38aa4":"Now after finding the rounds required we train our final model with the tuned parameters and the rounds.We then validate it on our test set","f7d0367b":"### *Evaluation metrics*","98393049":"XGBoost has an inbuilt cvmethod which helps us to find the rounds using early stopping to prevent overfitting","b72eea96":" Dataset after preprocessing","ec4ea7c4":"As you can see we got a difference of  almost  0.6% ,this is very small but it may help in winning competitions and also I have not taken all parameters into the tuning process(gamma,alpha,etc.).Thus I can say with further tuning we can improve this model even more.I would VERY much appreciate any suggestions for improvments that I could make.","cb610527":"## **Bayesian Optimization**","77f75f35":"## Analyze data","3046ffad":"## Overview","e8666de6":"### *Correlation using heatmap*","8be70328":"### *Make predictions*","41433e6e":" We can also drop \"occupation\" as \"workclass\" is sufficient.\nFurthermore,\"fnlwgt\" is not useful to us as it refers to only the sampling in the census conducted and has no practical effect on the label.Also we  drop the \"native.country\" as more are from single country(USA) which can cause bias.","acb1020e":"## Load Data","dde1242a":" We use cross validation(CV) to select which model to use.In k-fold CV a model is trained using k-1  of the folds as training data.Then the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).","9ff56c70":"Aha! You can clearly see that \"relationship\" and \"marital.status\", look similar i.e. tell us the same thing.\n\nI will prove this now,first let us do some changes to data so it is simpler to understand.","287ebe27":"In this dataset ,just by looking at the columns \"education\" and \"education.num\" you could say that they bot convey the same meaning,one just specifies the degree name and the other specifies a numerical value for that degree ,we could drop any one of these. Numerical data is preferrable so lets keep \"education.num\" and we can remove \"education\".The same could be said about \"marital.status\" and \"relationship\",here, generally one would assume income levels whether a person is married or not.\"relationship\" indirectly conveys the same husband ,wife indirectly means the person is married others like child,etc says that person is single.Hence we can drop any one of these.\n\nI will prove these in the following sections.","958b3775":"#### Note:- This  cell is commented out as it takes long time to compute(15-20 mins)","334b99fa":"In this Notebook ,I am going to show my work on the Adult Census Income dataset and how boosting can help increase our accuracy(even by small amount) ,especially using the XGBoost algorithm.\n\nI am a beginner ,so please keep in mind that I may have not gone into greater depths of analysis and model tuning.I would greatly appreciate any suggestions so as to improve my model.","5dfcb1d1":"### *Fit the model with tuned parameters*","781cf42e":"As we have already done feature engineering etc. We can move on to tune the hyperparameters.Parameters can be found in the XGboost documetation.\n\nBut before that we have to convert data into Dmatrix (XGBoost uses data only in this format).","1bd8653b":" Instead of manually changing each parameter and comparing results,sklearn provides us with Gridsearch which uses crossvalidation to internally check all the parameters and compare results and gives us the best parameters as output.\n This is a time taking process.","85e1f33f":" As we can see from the heatmap \"education\" and \"education.num\" are highly correlated, same can be said about the \"marital.status\" and \"relationship\" ,thus,we can drop \"relationship\" and \"education\".","5ccca472":"### *Check for null values and show the datatypes*","a231391b":"*Prediction task is to determine whether a person makes over 50K or less in a year.*\n \n **Attributes**:\n\n  *income*: >50K, <=50K\n\n  *age*: continuous\n\n  *workclass*: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov,\n  State-gov, Without-pay, Never-worked\n\n  *fnlwgt*: continuous\n\n  *education*: Bachelors, Some-college, 11th, HS-grad, Prof-school, \n  Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, \n  Doctorate, 5th-6th, Preschool\n  education-num: continuous\n  \n  *marital-status*: Married-civ-spouse, Divorced, Never-married, Separated, \n  Widowed, Married-spouse-absent, Married-AF-spouse\n\n  *occupation*: Tech-support, Craft-repair, Other-service, Sales, \n  Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct,\n  Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, \n  Protective-serv, Armed-Forces\n\n  *relationship*: Wife, Own-child, Husband, Not-in-family, Other-relative, \n  Unmarried\n\n  *race*: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black\n\n  *sex*: Female, Male\n\n  *capital-gain*: continuous\n\n  *capital-loss*: continuous\n\n  *hours-per-week*: continuous\n\n  *native-country*: United-States, Cambodia, England, Puerto-Rico, \n  Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, \n  Greece, South, China, Cuba, Iran, Honduras, Philippines, \n  Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland,\n  France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti,\n  Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand,\n  Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands","41d6abb1":"### *Look at data*","13c3c00c":"We use random forest as it is known for its robustness and less sensitivity.It is a bagging algorithm."}}