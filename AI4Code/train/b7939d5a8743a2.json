{"cell_type":{"bacd6f50":"code","b686f855":"code","9e535d44":"code","6202b6de":"code","ea103995":"code","96dab0d9":"code","4d6d15aa":"markdown","547916ab":"markdown","f788e6fd":"markdown","3801e1d7":"markdown","5d50f068":"markdown","5ade32db":"markdown"},"source":{"bacd6f50":"import pandas as pd\nimport numpy  as np\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import explained_variance_score","b686f855":"s1 = pd.read_csv(\"..\/input\/very-simple-neural-network-regression\/submission.csv\")\ns2 = pd.read_csv(\"..\/input\/gaussian-process-regression-sample-script\/submission.csv\")\ns3 = pd.read_csv(\"..\/input\/random-forest-regression-minimalist-script\/submission.csv\")\ns4 = pd.read_csv(\"..\/input\/very-simple-xgboost-regression\/submission.csv\")\ns5 = pd.read_csv(\"..\/input\/catboost-regression-minimalist-script\/submission.csv\")\n\n\nn_submission_files = 5\n# also create a placeholder dataFrame\ns_final = pd.read_csv(\"..\/input\/very-simple-xgboost-regression\/submission.csv\")","9e535d44":"solution   = pd.read_csv('..\/input\/house-prices-advanced-regression-solution-file\/submission.csv')\ny_true     = solution[\"SalePrice\"]","6202b6de":"from scipy.optimize import minimize\n\ntmp_scores = []\ntmp_weights = []\npredictions = []\npredictions.append( s1[\"SalePrice\"] )\npredictions.append( s2[\"SalePrice\"] )\npredictions.append( s3[\"SalePrice\"] )\npredictions.append( s4[\"SalePrice\"] )\npredictions.append( s5[\"SalePrice\"] )\n\ndef scoring_function(weights):\n    final_prediction = 0\n    for weight, prediction in zip(weights, predictions):\n            final_prediction += weight*prediction\n    return np.sqrt(mean_squared_log_error(y_true, final_prediction))\n\nfor i in range(150):\n    starting_values = np.random.uniform(size=n_submission_files)\n    bounds = [(0,1)]*len(predictions)\n    result = minimize(scoring_function, \n                      starting_values, \n                      method='L-BFGS-B', \n                      bounds=bounds, \n                      options={'disp': False, 'maxiter': 10000})\n    tmp_scores.append(result['fun'])\n    tmp_weights.append(result['x'])\n\nbestWeight = tmp_weights[np.argmin(tmp_scores)]\nprint('Best weights', bestWeight)","ea103995":"s_final[\"SalePrice\"] = s1[\"SalePrice\"]*bestWeight[0] + s2[\"SalePrice\"]*bestWeight[1] +  s3[\"SalePrice\"]*bestWeight[2] +  s4[\"SalePrice\"]*bestWeight[3] +  s5[\"SalePrice\"]*bestWeight[4]\n\nprint(\"The new score is %.5f\" % np.sqrt( mean_squared_log_error(y_true, s_final[\"SalePrice\"]) ) )\nprint(\"The new explained variance is %.5f\" % explained_variance_score(y_true, s_final[\"SalePrice\"]) )","96dab0d9":"s_final.to_csv('submission.csv', index=False)","4d6d15aa":"we can see that the best combination is a mix consisting of 2.73% Gaussian process, 7.3% random forest, 17.7% XGBoost and finally 72% CatBoost. Let us now take a look at the results","547916ab":"read in my `submission.csv` files","f788e6fd":"We now use [scipy.optimize.minimize](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.optimize.minimize.html) to find the lowest score using the evaluation metric of the House Prices competition, which in this case is the root of the [mean squared logarithmic error regression loss](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.mean_squared_log_error.html)","3801e1d7":"we shall also read in the [ground truth (correct) target values](https:\/\/www.kaggle.com\/carlmcbrideellis\/house-prices-advanced-regression-solution-file):","5d50f068":"# Combining my submission.csv files for a better score\nOver time I have have experimented with various machine learning techniques applied to the [House Prices: Advanced Regression Techniques competition](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques), each time submitting a `submission.csv` file. Here is a table of my results:\n\n| Technique | Score | Explained variance |\n| :--- | --- | --- |\n| [neural network](https:\/\/www.kaggle.com\/carlmcbrideellis\/very-simple-neural-network-regression)| 0.23181 | 0.69091 |\n| [Gaussian process](https:\/\/www.kaggle.com\/carlmcbrideellis\/gaussian-process-regression-sample-script) | 0.21004 | 0.76409 |\n| [Random forest](https:\/\/www.kaggle.com\/carlmcbrideellis\/random-forest-regression-minimalist-script) | 0.17734 | 0.86514 |\n| [XGBoost](https:\/\/www.kaggle.com\/carlmcbrideellis\/very-simple-xgboost-regression) | 0.15617 | 0.90148 |\n| [CatBoost](https:\/\/www.kaggle.com\/carlmcbrideellis\/catboost-regression-minimalist-script) | 0.15270 | 0.90096 |\n\nI thought it would be fun to find a [linear combination](https:\/\/en.wikipedia.org\/wiki\/Linear_combination) of my `submission.csv` files that gives a better leaderboard score than any of the individual submissions, as well as having a better [explained variance score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.explained_variance_score.html). To do this I make use of the excellent notebook [\"Finding Ensemble Weights\"](https:\/\/www.kaggle.com\/hsperr\/finding-ensamble-weights) written by [Henning Sperr](https:\/\/www.kaggle.com\/hsperr).","5ade32db":"# Success!\nIt looks like we were able to find a judicious combination of weights that does indeed result in a better `submission.csv` than any of the component `submission.csv` files. Let us now submit this new solution to the competition:"}}