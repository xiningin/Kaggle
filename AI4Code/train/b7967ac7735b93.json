{"cell_type":{"c0f4eb81":"code","5d60ce75":"code","35f7dfe1":"code","9d0f1ff1":"code","2f539beb":"code","7bcaceb5":"code","e432342a":"code","0cf10ee7":"code","d400aa8a":"code","f2c25b66":"code","645ba5e5":"code","6ed8acd0":"code","f5526458":"code","686e13fc":"code","cde7f25a":"code","3f19e4c8":"code","78b57990":"code","60ade09b":"code","c65cf5eb":"code","524b6efd":"code","2dea27a7":"code","a0dde494":"code","69d74641":"code","fae0b375":"code","f4733b6e":"code","2c3bea47":"code","cf17b58a":"code","266b6cf8":"code","afbce7a1":"code","34a645bd":"code","f0b8df97":"code","8f033783":"code","75bb86ed":"markdown","59c34d0f":"markdown","73a8fe1a":"markdown","2cd7d784":"markdown","cbe70eef":"markdown","b48b3d69":"markdown","131d5c04":"markdown","3182a21f":"markdown","65232e74":"markdown","ef2d82ee":"markdown","78a06921":"markdown","15cfbdf0":"markdown","c6fbb87d":"markdown"},"source":{"c0f4eb81":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5d60ce75":"train_data = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")","35f7dfe1":"EMBEDDING_FILE = '..\/input\/glove6b100dtxt\/glove.6B.100d.txt'\n\nembeddings = {}\nfor o in open(EMBEDDING_FILE):\n    word = o.split(\" \")[0]\n    # print(word)\n    embd = o.split(\" \")[1:]\n    embd = np.asarray(embd, dtype='float32')\n    # print(embd)\n    embeddings[word] = embd","9d0f1ff1":"def get_feature_vectors(sentence):\n    words = sentence.split()\n    feature_vec = np.zeros((100,),dtype=\"float32\")\n    i=0\n    for word in words:\n        try:\n            feature_vec = np.add(feature_vec, embeddings.get(word))\n        except:\n            i = i + 1\n    if len(words) > 0:\n        feature_vec = np.divide(feature_vec, len(words)- i)\n    return feature_vec","2f539beb":"excerpts = train_data[\"excerpt\"]\ntarget = train_data[\"target\"]\ntest_excerpts = test_data[\"excerpt\"]","7bcaceb5":"excerpts = excerpts.str.lower()\ntest_excerpts = test_excerpts.str.lower()","e432342a":"from nltk.stem import PorterStemmer\nps = PorterStemmer()\nexcerpts = excerpts.apply(ps.stem)\ntest_excerpts = test_excerpts.apply(ps.stem)","0cf10ee7":"from nltk.stem import WordNetLemmatizer\nwnl = WordNetLemmatizer()\nexcerpts = excerpts.apply(wnl.lemmatize)\ntest_excerpts = test_excerpts.apply(wnl.lemmatize)","d400aa8a":"from nltk.corpus import stopwords\n\", \".join(stopwords.words('english'))\nSTOPWORDS = set(stopwords.words('english'))\n\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\nexcerpts = excerpts.apply(lambda text: remove_stopwords(text))\ntest_excerpts = test_excerpts.apply(lambda text: remove_stopwords(text))","f2c25b66":"excerpts_full = []\nfor index, value in excerpts.items():\n    excerpts_full.append(value)","645ba5e5":"y_full = []\nfor index, value in target.items():\n    y_full.append(value)","6ed8acd0":"excerpts_test = []\nfor index, value in test_excerpts.items():\n    excerpts_test.append(value)","f5526458":"from sklearn.model_selection import train_test_split\nexcerpts_train, excerpts_val, y_train, y_val = train_test_split(excerpts_full, y_full, test_size=0.20)","686e13fc":"train_vectors = np.array([get_feature_vectors(sentence) for sentence in excerpts_train])\nval_vectors = np.array([get_feature_vectors(sentence) for sentence in excerpts_val])\ntest_vectors = np.array([get_feature_vectors(sentence) for sentence in excerpts_test])\ny_train = np.array(y_train)\ny_val = np.array(y_val)\nfull_vectors = np.array([get_feature_vectors(sentence) for sentence in excerpts_full])\ny_full = np.array(y_full)","cde7f25a":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\nmodel = keras.Sequential([\n    layers.Dense(units=64, activation='relu', input_shape=[100]),\n    layers.Dense(units=32, activation='relu'),\n    layers.Dense(units=8, activation='relu'),\n    # the linear output layer \n    layers.Dense(units=1),\n])","3f19e4c8":"# Define callbacks\nearly_stopping = callbacks.EarlyStopping(\n    patience=5,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\n\nrlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=100)","78b57990":"model.compile(\n    optimizer=Adam(lr=0.01), \n    loss='mae',\n)","60ade09b":"history = model.fit(\n    train_vectors, y_train,\n    validation_data=(val_vectors, y_val),\n    batch_size=64,\n    epochs=50,\n    callbacks=[early_stopping, rlrop]\n)","c65cf5eb":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()","524b6efd":"from sklearn.metrics import mean_absolute_error\nval_predictions_nn = model.predict(val_vectors)\nnn_val_mae = mean_absolute_error(y_val,val_predictions_nn)\nprint(\"Validation MAE for Random Forest Model: {}\".format(nn_val_mae))","2dea27a7":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.metrics import mean_squared_error\nrf_model = RandomForestRegressor(random_state=42)\nrf_model.fit(train_vectors,y_train)","a0dde494":"val_predictions_rf = rf_model.predict(val_vectors)\nrf_val_mae = mean_absolute_error(y_val,val_predictions_rf)\nprint(\"Validation MAE for Random Forest Model: {}\".format(rf_val_mae))","69d74641":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Define the model\nxgb_model = XGBRegressor(n_estimators = 500, learning_rate=0.05)\n\n# Fit the model\nxgb_model.fit(train_vectors,y_train)","fae0b375":"# Get predictions for the validation set\nval_predictions_xgb = xgb_model.predict(val_vectors)\n\n# Calculate MAE\nxgb_val_mae = mean_absolute_error(val_predictions_xgb,y_val) \nprint(\"Validation MAE for XGB Model: {}\".format(xgb_val_mae))\nprint(val_predictions_xgb[0])","f4733b6e":"ensemble_preds = []\nfor i in range(len(val_predictions_xgb)):\n    ensemble_preds.append(float((val_predictions_nn[i][0] + val_predictions_rf[i] + val_predictions_xgb[i])\/3))","2c3bea47":"# Calculate MAE\nensemble_val_mae = mean_absolute_error(ensemble_preds,y_val) \nprint(\"Validation MAE for Ensemble Model: {}\".format(ensemble_val_mae))","cf17b58a":"model.fit(\n    full_vectors, y_full,\n    batch_size=64,\n    epochs=50,\n    callbacks=[early_stopping, rlrop]\n)","266b6cf8":"rf_model.fit(full_vectors,y_full)","afbce7a1":"xgb_model.fit(full_vectors,y_full)","34a645bd":"test_preds = rf_model.predict(test_vectors) #model.predict(test_vectors) for NN, rf_model.predict(test_vectors) for RF\ntest_preds = test_preds.flatten()","f0b8df97":"ensemble_test_preds = []\nnn_preds = model.predict(test_vectors)\nrf_preds = rf_model.predict(test_vectors)\nxgb_preds = xgb_model.predict(test_vectors)\nfor i in range(len(rf_preds)):\n    ensemble_test_preds.append(float((nn_preds[i][0] + rf_preds[i] + xgb_preds[i])\/3))","8f033783":"sample_submission = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\nsample_submission.target = ensemble_test_preds #test_preds \nsample_submission.to_csv('submission.csv',index=False)","75bb86ed":"## XGB Regressor","59c34d0f":"## Obtain the feature vectors for each dataset as NumPy array","73a8fe1a":"## Prepare submission format","2cd7d784":"## Splitting train and validation sets","cbe70eef":"## Obtain test predictions","b48b3d69":"## Random forest model","131d5c04":"## Re-train models on full data","3182a21f":"## Check validation score with a combination of the three models","65232e74":"## Neural network model ","ef2d82ee":"## Separating and pre-processing data ","78a06921":"## Define function to obtain feature vector for each datapoint","15cfbdf0":"## Loading train and test data","c6fbb87d":"## Populating GLoVe embeddings"}}