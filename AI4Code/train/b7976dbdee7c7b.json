{"cell_type":{"d527d20b":"code","7f5eacfc":"code","1ced9ed5":"code","a91c45f7":"code","d49e530d":"code","1abddcf7":"code","b7d54a45":"code","6c6a659f":"code","5835a0f5":"code","4811c38b":"code","67fdc878":"code","7daede38":"code","55867ca9":"code","06912938":"code","7033cf01":"code","4ba72077":"code","d75fd3ec":"code","f1f24290":"code","e5eafc19":"code","069000a9":"code","ae8107b4":"code","2257baab":"code","4c674cfc":"code","7d418764":"code","5b168e19":"code","909302f3":"code","0581b406":"code","b520e267":"code","e860f360":"code","6a53b750":"code","ebfe96b8":"code","e6829e39":"code","cfff6e90":"code","168d3951":"code","1b99700b":"code","f4f270b7":"code","4e450c7b":"code","4e7050c5":"code","044c0a7d":"code","8b104061":"code","d87e6cb5":"code","54bea605":"code","927efccb":"code","71e9ae0b":"code","f3130ab6":"code","3ef65da9":"code","5bdeca52":"code","6d5fbbb2":"markdown","7a8073ea":"markdown","6efd64dc":"markdown","49ee103a":"markdown","b8da6bb0":"markdown","58dc286c":"markdown","0f3d7ef1":"markdown","67590938":"markdown","35b49df0":"markdown","ae8b220a":"markdown","ec29ef0e":"markdown","fc7aa984":"markdown","d181e2cd":"markdown","a3b7b583":"markdown","bb1b5f77":"markdown","5093ac4a":"markdown","5c9d5bda":"markdown","6238cb01":"markdown","9f4e04a9":"markdown","334764c2":"markdown"},"source":{"d527d20b":"import pandas as pd\nfrom pandas.plotting import scatter_matrix\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVR\nimport optuna\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nimport plotly.graph_objects as go\nimport statistics","7f5eacfc":"df = pd.read_csv('..\/input\/same-old-creating-folds\/train_5folds.csv')\ndf.shape","1ced9ed5":"df.head()","a91c45f7":"X = df.drop(['Id', 'Pawpularity', 'kfold'], axis=1)\ny = df['Pawpularity']","d49e530d":"correlations = X.corr()\nfig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(111)\ncax = ax.matshow(correlations, vmin=-1, vmax=1, cmap='RdPu')\nfig.colorbar(cax)\nticks = np.arange(0,12,1)\nax.set_xticks(ticks)\nax.set_yticks(ticks)\nax.set_xticklabels(X.columns)\nax.set_yticklabels(X.columns)\nplt.show()","1abddcf7":"model = RandomForestRegressor()\nmodel.fit(X, y)\nimportance1 = model.feature_importances_\n\nmodel = ExtraTreesRegressor()\nmodel.fit(X, y)\nimportance2 = model.feature_importances_\n\nmodel = XGBRegressor()\nmodel.fit(X, y)\nimportance3 = model.feature_importances_","b7d54a45":"importance = (importance1+importance2+importance3)\/3\nd = pd.DataFrame()\nd['imp'] = importance\nd['f'] = X.columns\n\nd=d.sort_values('imp', ascending=False)\n\nfig, ax = plt.subplots(figsize=(20,7))\n\nax.barh(d.f, d.imp, color='crimson')\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\nax.xaxis.set_tick_params(pad=5)\nax.yaxis.set_tick_params(pad=10)\nfor s in ['top','bottom','left','right']:\n    ax.spines[s].set_visible(False)\n    \nax.grid(b=True, color='grey', linestyle='-.', linewidth=0.5, alpha=0.2)\nax.invert_yaxis()\nplt.show()\n","6c6a659f":"rmse = 0\n\nfor fold in range(5):\n\n    train = df[df['kfold']!=fold]\n    valid = df[df['kfold']==fold]\n    \n    xtrain = train.drop(['Id', 'Pawpularity', 'kfold'], axis=1).values\n    xtest = valid.drop(['Id', 'Pawpularity', 'kfold'], axis=1)\n    ytrain = train['Pawpularity'].values\n    ytest = valid['Pawpularity'].values\n    \n\n    lr = LinearRegression()\n    \n    lr.fit(xtrain, ytrain)\n    \n    \n    ypred = lr.predict(xtest)\n    \n    \n    folddf = valid.copy()\n    folddf['Pawpularity_pred'] = ypred\n    rmse_fold = mean_squared_error(ytest, ypred, squared=False)\n    \n    rmse += rmse_fold\/5\nprint('LR rmse = ', rmse)","5835a0f5":"rmse = 0\n\nfor fold in range(5):\n\n    train = df[df['kfold']!=fold]\n    valid = df[df['kfold']==fold]\n    \n    xtrain = train.drop(['Id', 'Pawpularity', 'kfold'], axis=1).values\n    xtest = valid.drop(['Id', 'Pawpularity', 'kfold'], axis=1)\n    ytrain = train['Pawpularity'].values\n    ytest = valid['Pawpularity'].values\n    \n    #### Using PCA ####\n    pca = PCA(n_components=2)\n    xtrain = pca.fit_transform(xtrain)\n    xtest = pca.transform(xtest)\n    \n    lr = LinearRegression()\n    \n    lr.fit(xtrain, ytrain)\n    \n    \n    ypred = lr.predict(xtest)\n    \n    \n    folddf = valid.copy()\n    folddf['Pawpularity_pred'] = ypred\n    rmse_fold = mean_squared_error(ytest, ypred, squared=False)\n    \n    rmse += rmse_fold\/5\nprint('LR rmse = ', rmse)","4811c38b":"rmse = 0\n\nfor fold in range(5):\n\n    train = df[df['kfold']!=fold]\n    valid = df[df['kfold']==fold]\n    \n    xtrain = train.drop(['Id', 'Pawpularity', 'kfold'], axis=1).values\n    xtest = valid.drop(['Id', 'Pawpularity', 'kfold'], axis=1)\n    ytrain = train['Pawpularity'].values\n    ytest = valid['Pawpularity'].values\n    \n    svr = SVR()\n    \n    svr.fit(xtrain, ytrain)\n    \n    ypred = svr.predict(xtest)\n      \n    folddf = valid.copy()\n    folddf['Pawpularity_pred'] = ypred\n    rmse_fold = mean_squared_error(ytest, ypred, squared=False)\n    \n    rmse += rmse_fold\/5\n\nprint('='*100)\nprint('SVR rmse = ', rmse)\nprint('='*100)","67fdc878":"rmse = 0\n\nfor fold in range(5):\n\n    train = df[df['kfold']!=fold]\n    valid = df[df['kfold']==fold]\n    \n    xtrain = train.drop(['Id', 'Pawpularity', 'kfold'], axis=1).values\n    xtest = valid.drop(['Id', 'Pawpularity', 'kfold'], axis=1)\n    ytrain = train['Pawpularity'].values\n    ytest = valid['Pawpularity'].values\n    \n    # Using PCA\n    pca = PCA(n_components=2)\n    xtrain = pca.fit_transform(xtrain)\n    xtest = pca.transform(xtest)\n    \n    svr = SVR()\n    \n    svr.fit(xtrain, ytrain)\n    \n    \n    \n    \n    ypred = svr.predict(xtest)\n    \n    \n    folddf = valid.copy()\n    folddf['Pawpularity_pred'] = ypred\n    rmse_fold = mean_squared_error(ytest, ypred, squared=False)\n    \n    rmse += rmse_fold\/5\n    \nprint('='*100)\nprint('SVR rmse = ', rmse)\nprint('='*100)","7daede38":"rmse = 0\n\nfor fold in range(5):\n\n    train = df[df['kfold']!=fold]\n    valid = df[df['kfold']==fold]\n    \n    xtrain = train.drop(['Id', 'Pawpularity', 'kfold'], axis=1).values\n    xtest = valid.drop(['Id', 'Pawpularity', 'kfold'], axis=1)\n    ytrain = train['Pawpularity'].values\n    ytest = valid['Pawpularity'].values\n    \n    rf = RandomForestRegressor()\n    et = ExtraTreesRegressor()\n    xgb = XGBRegressor()\n    lr = LinearRegression()\n    \n    rf.fit(xtrain, ytrain)\n    et.fit(xtrain, ytrain)\n    xgb.fit(xtrain, ytrain)\n    lr.fit(xtrain, ytrain)\n    \n    ypred1 = rf.predict(xtest)\n    ypred2 = et.predict(xtest)\n    ypred3 = xgb.predict(xtest)\n    ypred4 = lr.predict(xtest)\n    ypred = (ypred1+ypred2+ypred3+ypred4)\/4\n\n\n    rmse_fold = mean_squared_error(ytest, ypred, squared=False)\n    \n    rmse += rmse_fold\/5\n    \nprint('='*100)\nprint('Ensemble rmse = ', rmse)\nprint('='*100)","55867ca9":"rmse = 0\n\nfor fold in range(5):\n    y_pred_all = []\n    train = df[df['kfold']!=fold]\n    valid = df[df['kfold']==fold]\n\n    xtrain = train.drop(['Id', 'Pawpularity', 'kfold'], axis=1).values\n    xtest = valid.drop(['Id', 'Pawpularity', 'kfold'], axis=1)\n    ytrain = train['Pawpularity'].values\n    ytest = valid['Pawpularity'].values\n    \n    pca = PCA(n_components=5)\n\n    xtrain = pca.fit_transform(xtrain)\n\n    xtest = pca.transform(xtest)\n\n    rf = RandomForestRegressor()\n    et = ExtraTreesRegressor()\n    xgb = XGBRegressor()\n    lr = LinearRegression()\n    \n    rf.fit(xtrain, ytrain)\n    et.fit(xtrain, ytrain)\n    xgb.fit(xtrain, ytrain)\n    lr.fit(xtrain, ytrain)\n    \n    ypred1 = rf.predict(xtest)\n    ypred2 = et.predict(xtest)\n    ypred3 = xgb.predict(xtest)\n    ypred4 = lr.predict(xtest)\n    ypred = (ypred1+ypred2+ypred3+ypred4)\/4\n\n    rmse_fold = mean_squared_error(ytest, ypred, squared=False)\n\n    rmse += rmse_fold\/5\n    \nrmse_i = rmse\n    \nprint('='*100)\nprint('Ensemble rmse with PCA = ', rmse)\nprint('='*100)","06912938":"def objective(trial):\n    \n    components = trial.suggest_int('n_components', 1,10,1)\n    \n    rmse = 0\n\n    for fold in range(5):\n        y_pred_all = []\n        train = df[df['kfold']!=fold]\n        valid = df[df['kfold']==fold]\n\n        xtrain = train.drop(['Id', 'Pawpularity', 'kfold'], axis=1).values\n        xtest = valid.drop(['Id', 'Pawpularity', 'kfold'], axis=1)\n        ytrain = train['Pawpularity'].values\n        ytest = valid['Pawpularity'].values\n        \n        pca = PCA(n_components=2)\n        xtrain = pca.fit_transform(xtrain)\n        xtest = pca.transform(xtest)\n\n        rf = RandomForestRegressor()\n        et = ExtraTreesRegressor()\n        xgb = XGBRegressor()\n        lr = LinearRegression()\n\n        rf.fit(xtrain, ytrain)\n        et.fit(xtrain, ytrain)\n        xgb.fit(xtrain, ytrain)\n        lr.fit(xtrain, ytrain)\n\n        ypred1 = rf.predict(xtest)\n        ypred2 = et.predict(xtest)\n        ypred3 = xgb.predict(xtest)\n        ypred4 = lr.predict(xtest)\n        ypred = (ypred1+ypred2+ypred3+ypred4)\/4\n\n        rmse_fold = mean_squared_error(ytest, ypred, squared=False)\n\n        rmse += rmse_fold\/5\n        \n    return rmse","7033cf01":"optuna.logging.set_verbosity(optuna.logging.WARNING)\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)","4ba72077":"best_params = study.best_params\nn = list(best_params.values())[0]\nprint(f'Optimun number of n_components for PCA in the ensemble is : {n}')","d75fd3ec":"test_df = pd.read_csv('..\/input\/petfinder-pawpularity-score\/test.csv')\ntest_df = test_df.drop(['Id'], axis=1)","f1f24290":"rmse = 0\npreds = []\npseudo_labels = pd.DataFrame()\n\n\nfor fold in range(5):\n    folddf = pd.DataFrame()\n\n    train = df[df['kfold']!=fold]\n    valid = df[df['kfold']==fold]\n    xpred = test_df.values\n\n    xtrain = train.drop(['Id', 'Pawpularity', 'kfold'], axis=1).values\n    xtest = valid.drop(['Id', 'Pawpularity', 'kfold'], axis=1)\n    ytrain = train['Pawpularity'].values\n    ytest = valid['Pawpularity'].values\n    \n    pca = PCA(n_components=n)\n    xtrain = pca.fit_transform(xtrain)\n    xtest = pca.transform(xtest)\n    xpred = pca.transform(xpred)\n\n    rf = RandomForestRegressor()\n    et = ExtraTreesRegressor()\n    xgb = XGBRegressor()\n    lr = LinearRegression()\n    \n    rf.fit(xtrain, ytrain)\n    et.fit(xtrain, ytrain)\n    xgb.fit(xtrain, ytrain)\n    lr.fit(xtrain, ytrain)\n    \n    ypred1 = rf.predict(xtest)\n    ypred2 = et.predict(xtest)\n    ypred3 = xgb.predict(xtest)\n    ypred4 = lr.predict(xtest)\n    ypred = (ypred1+ypred2+ypred3+ypred4)\/4\n    \n    ypred1_ = rf.predict(xpred)\n    ypred2_ = et.predict(xpred)\n    ypred3_ = xgb.predict(xpred)\n    ypred4_ = lr.predict(xpred)\n    ypred_ = (ypred1_+ypred2_+ypred3_+ypred4_)\/4\n    \n    preds.append(np.hstack(ypred_))\n    folddf = valid.copy()\n    folddf['Pawpularity_pred'] = ypred\n    folddf['pseudo_labels'] = ypred\/100\n    \n    rmse_fold = mean_squared_error(ytest, ypred, squared=False)\n    pseudo_labels = pd.concat([pseudo_labels, folddf])\n    rmse += rmse_fold\/5\n    \nprint('='*100)\nprint('Ensemble rmse with PCA = ', rmse)\nprint('='*100)","e5eafc19":"test_df['pseudo_labels'] = sum(ypred_)\/(len(ypred_)*100)","069000a9":"test_df.head()","ae8107b4":"pseudo_labels.head()","2257baab":"multi_table = pd.DataFrame()\nmulti_table['Orginal Pawpularity'] = pseudo_labels['Pawpularity'].describe()\nmulti_table['Predicted Pawpularity'] = pseudo_labels['Pawpularity_pred'].describe()","4c674cfc":"from IPython.core.display import HTML\n\ndef multi_table(table_list):\n    ''' Acceps a list of IpyTable objects and returns a table which contains each IpyTable in a cell\n    '''\n    return HTML(\n        '<table style=\"margin: 0px auto;><tr style=\"background-color:ash;\">' + \n        ''.join(['<td>' + table._repr_html_() + '<\/td>' for table in table_list]) +\n        '<\/tr><\/table>')\n\nmulti_table([pd.DataFrame(pseudo_labels['Pawpularity'].describe()), pd.DataFrame(pseudo_labels['Pawpularity_pred'].describe())])","7d418764":"fig = go.Figure()\nfig.add_trace(go.Box(y=pseudo_labels['Pawpularity'], name = 'True',))\nfig.add_trace(go.Box(y=pseudo_labels['Pawpularity_pred'], name = 'Predicted'))\n\nfig.show()","5b168e19":"layout = go.Layout(\n    autosize=False,\n    width=1200,\n    height=700)\n\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=pseudo_labels['Id'], y=pseudo_labels['Pawpularity'], name='Pawpularity', mode='lines', opacity=0.5, marker={'size':3}))\nfig.add_trace(go.Scatter(x=pseudo_labels['Id'], y=pseudo_labels['Pawpularity_pred'], name='Prediction', mode='lines', opacity=0.6, marker={'size':3, 'color':'crimson'}))\nfig.add_trace(go.Scatter(x=pseudo_labels['Id'], y=[np.mean(pseudo_labels['Pawpularity'])]*len(pseudo_labels), name='Pawpularity Mean', mode='lines'))\nfig.add_trace(go.Scatter(x=pseudo_labels['Id'], y=[np.mean(pseudo_labels['Pawpularity_pred'])]*len(pseudo_labels), name='Pred Mean', mode='lines', marker={'color':'black', 'line': dict(width=200)}))\n\nfig.add_trace(go.Scatter(x=pseudo_labels['Id'], y=[statistics.median(pseudo_labels['Pawpularity'])]*len(pseudo_labels), name='Pawpularity Median', mode='lines'))\nfig.add_trace(go.Scatter(x=pseudo_labels['Id'], y=[statistics.median(pseudo_labels['Pawpularity_pred'])]*len(pseudo_labels), name='Pred Median', mode='lines'))\n\nfig.update_xaxes(visible=False, showticklabels=False)\n\nfig.show()","909302f3":"layout = go.Layout(\n    autosize=False,\n    width=1200,\n    height=700)\n\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=pseudo_labels['Id'], y=pseudo_labels['Pawpularity'], name='Pawpularity', mode='markers', opacity=0.7, marker={'size':3}))\nfig.add_trace(go.Scatter(x=pseudo_labels['Id'], y=pseudo_labels['Pawpularity_pred'], name='Prediction', mode='markers', opacity=0.9, marker={'size':3, 'color':'crimson'}))\n\nfig.update_xaxes(visible=False, showticklabels=False)\n\nfig.show()","0581b406":"pseudo_labels.head()","b520e267":"from sklearn import model_selection","e860f360":"def create_folds(data, num_splits):\n    data[\"kfold\"] = -1\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n\n    data.loc[:, \"bins\"] = pd.cut(data[\"pseudo_labels\"], bins=num_bins, labels=False)\n\n    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n    \n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    data = data.drop(\"bins\", axis=1)\n\n    return data","6a53b750":"test_df_folds = create_folds(test_df, 5)","ebfe96b8":"test_df_folds.head()","e6829e39":"rmse = 0\nrmse_r = 0\nfinal = pd.DataFrame()\n\nfor fold in range(5):\n    folddf = pd.DataFrame()\n    train = test_df_folds[test_df_folds['kfold']!=fold]\n    valid = test_df_folds[test_df_folds['kfold']==fold]\n\n    xtrain = train.drop(['pseudo_labels', 'kfold'], axis=1).values\n    xtest = valid.drop(['pseudo_labels', 'kfold'], axis=1)\n    ytrain = train['pseudo_labels'].values\n    ytest = (valid['pseudo_labels'].values)*100\n    \n    xgbc = XGBClassifier(verbosity=0, silent=True)\n    \n    xgbc.fit(xtrain, ytrain)\n    ypred = xgbc.predict(xtest)\n\n    ypred = ypred*100\n    folddf = valid.copy()\n    y_pred_rounded = [np.round(y) for y in ypred]\n    folddf['Pawpularity_pred'] = [y for y in ypred]\n    rmse_fold = mean_squared_error(ytest, ypred, squared=False)\n    rmse_fold_r = mean_squared_error(ytest, y_pred_rounded, squared=False)\n    final = pd.concat([final, folddf])\n\n    rmse += rmse_fold\/5\n    rmse_r += rmse_fold_r\/5\n    \nprint('='*100)\nprint('XGBoost RMSE trained on pseudo-lables = ', rmse)\nprint('='*100)\nprint('XGBoost RMSE trained on pseudo-lables (rounded) = ', rmse_r)\nprint('='*100)","cfff6e90":"final.head()","168d3951":"# fig = go.Figure()\n# fig.add_trace(go.Box(y=pseudo_labels['Pawpularity'], name = 'True',))\n# fig.add_trace(go.Box(y=pseudo_labels['Pawpularity_pred'], name = 'Level 1'))\n# fig.add_trace(go.Box(y=final['Pawpularity_pred'], name = 'Level 2'))\n\n# fig.show()","1b99700b":"# layout = go.Layout(\n#     autosize=False,\n#     width=1200,\n#     height=700)\n\n# fig = go.Figure(layout=layout)\n\n# fig.add_trace(go.Scatter(x=pseudo_labels['Id'], y=pseudo_labels['Pawpularity'], name='True', mode='markers', opacity=0.6, marker={'size':3}))\n# fig.add_trace(go.Scatter(x=pseudo_labels['Id'], y=pseudo_labels['Pawpularity_pred'], name='Level 1', mode='markers', opacity=0.8, marker={'size':3}))\n# fig.add_trace(go.Scatter(x=final['Id'], y=final['Pawpularity_pred'], name='Level 2', mode='markers', opacity=1, marker={'size':3}))\n\n# fig.update_xaxes(visible=False, showticklabels=False)\n\n# fig.show()","f4f270b7":"# layout = go.Layout(\n#     autosize=False,\n#     width=1200,\n#     height=700)\n\n# fig = go.Figure(layout=layout)\n\n# fig.add_trace(go.Scatter(x=pseudo_labels['Id'], y=pseudo_labels['Pawpularity'], name='True', mode='lines', opacity=0.6, marker={'size':3}))\n# fig.add_trace(go.Scatter(x=pseudo_labels['Id'], y=pseudo_labels['Pawpularity_pred'], name='Level 1', mode='lines', opacity=0.8, marker={'size':3}))\n# fig.add_trace(go.Scatter(x=final['Id'], y=final['Pawpularity_pred'], name='Level 2', mode='lines', opacity=1, marker={'size':3}))\n\n# fig.update_xaxes(visible=False, showticklabels=False)\n\n# fig.show()","4e450c7b":"# from IPython.core.display import HTML\n\n# def multi_table(table_list):\n#     ''' Acceps a list of IpyTable objects and returns a table which contains each IpyTable in a cell\n#     '''\n#     return HTML(\n#         '<table style=\"margin: 0px auto;><tr style=\"background-color:ash;\">' + \n#         ''.join(['<td>' + table._repr_html_() + '<\/td>' for table in table_list]) +\n#         '<\/tr><\/table>')\n\n# multi_table([pd.DataFrame(pseudo_labels['Pawpularity'].describe()), pd.DataFrame(pseudo_labels['Pawpularity_pred'].describe()), pd.DataFrame(final['Pawpularity_pred'].describe())])","4e7050c5":"rmse = 0\nrmse_r = 0\nfinal2 = pd.DataFrame()\n\nfor fold in range(5):\n    folddf = pd.DataFrame()\n\n    train = test_df_folds[test_df_folds['kfold']!=fold]\n    valid = test_df_folds[test_df_folds['kfold']==fold]\n\n    xtrain = train.drop(['pseudo_labels', 'kfold'], axis=1).values\n    xtest = valid.drop(['pseudo_labels', 'kfold'], axis=1)\n    ytrain = train['pseudo_labels'].values\n    ytest = (valid['pseudo_labels'].values)*100\n    \n    pca = PCA(n_components=n)\n    xtrain = pca.fit_transform(xtrain)\n    xtest = pca.transform(xtest)\n    \n    xgbc = XGBClassifier(verbosity=0, silent=True)\n    \n    xgbc.fit(xtrain, ytrain)\n    ypred = xgbc.predict(xtest)\n\n    ypred = ypred*100\n    folddf = valid.copy()\n    y_pred_rounded = [np.round(y) for y in ypred]\n    folddf['Pawpularity_pred'] = [np.round(y) for y in ypred]\n    rmse_fold = mean_squared_error(ytest, ypred, squared=False)\n    rmse_fold_r = mean_squared_error(ytest, y_pred_rounded, squared=False)\n    final2 = pd.concat([final2, folddf])\n\n    rmse += rmse_fold\/5\n    rmse_r += rmse_fold_r\/5\n    \nprint('='*100)\nprint('XGBoost (PCA) RMSE trained on pseudo-lables = ', rmse)\nprint('='*100)\nprint('XGBoost (PCA) RMSE trained on pseudo-lables (rounded) = ', rmse_r)\nprint('='*100)","044c0a7d":"rmse = 0\nfinal3 = pd.DataFrame()\n\nxtrain = test_df_folds.drop(['kfold', 'pseudo_labels'], axis=1).values\nxtest = xtrain.copy()\n\nytrain = test_df_folds['pseudo_labels'].values*100\nytest = (test_df_folds['pseudo_labels'].values)*100\n\nxgbc = XGBRegressor()\n\nxgbc.fit(xtrain, ytrain)\nypred = xgbc.predict(xtest)\n\n\nfinal3 = test_df_folds.copy()\n# ypred =  ypred * 100\nfinal3['Pawpularity_pred'] = ypred\nrmse = mean_squared_error(ytest, ypred, squared=False)\n\nprint('='*100)\nprint('XGBoost RMSE trained on pseudo-lables (with leakage) = ', rmse)\nprint('='*100)","8b104061":"final_pred_1 = final['Pawpularity_pred'].values\nfinal_pred_2 = final2['Pawpularity_pred'].values\nfinal_pred_3 = final3['Pawpularity_pred'].values","d87e6cb5":"# print(mean_squared_error(ytest, final_pred_1, squared=False))\n# print(mean_squared_error(ytest, final_pred_2, squared=False))\n# print(mean_squared_error(ytest, final_pred_3, squared=False))","54bea605":"super_final = (final_pred_1+final_pred_2+final_pred_3)\/3\nsuper_final_weighted = (final_pred_1+2*final_pred_2+final_pred_3)\/4","927efccb":"test_df['Pawpularity'] = super_final","71e9ae0b":"test_df = test_df.drop(['kfold', 'pseudo_labels', 'bins'], axis=1)","f3130ab6":"test_df.to_csv('submission.csv', index=False)","3ef65da9":"# final_rmse = mean_squared_error(ytest, super_final, squared=False)\n# final_rmse_w = mean_squared_error(ytest, super_final_weighted, squared=False)\n\n# print('='*100)\n# print('Final RMSE = ', final_rmse)\n# print('='*100)\n# print('Final RMSE (weighted) = ', final_rmse_w)\n# print('='*100)","5bdeca52":"# print('='*100)\n# print(f'The raw ensemble for level 1 models scored an RMSE of {rmse_i}')\n# print(f'The final ensemble after level 2 models scored an RMSE of {final_rmse_w}')\n\n# print('='*100)\n# if rmse_i<final_rmse_w:\n#     print(f'The RMSE increased by {final_rmse_w-rmse_i}')\n# else:\n#     print(f'Level 2 ensemble: The RMSE decreased by {np.round(rmse_i-final_rmse_w, 2)}')\n#     print('='*100)\n#     print(f'Level 2 PCA XGBoost: The RMSE decreased by {np.round(rmse_i-mean_squared_error(ytest, final_pred_2, squared=False), 2)}')\n# print('='*100)","6d5fbbb2":"# Ensembling with PCA","7a8073ea":"# Using SVR","6efd64dc":"### Please do <span style=\"color:red\">upvote<\/span>. if you like. \ud83d\udcab\n### That gives a hell lot of motivation to create kernels and do experiments. \ud83d\udcab\n### Happy kaggling!! \ud83d\udcab","49ee103a":"# Conclusion\n\n* The level 1 predictions have lower variance and lesser outliers than the true values\n* The level 2 predictions have even lower variance than the level 1 predictions\n* Rounded off values increases the rsme","b8da6bb0":"# Using PCA and Pseudo Labels to reduce RMSE\n\nUsing only ***meta features*** and PCA to predict the OOF values of Pawpularity.\n\nI have used different ***regressors*** to ensemble as a ***level 1*** model and generate the Pawpularity Score.\n\nUsed those Pawpularity scores as ***Pseudo Labels*** to train a classifier model and generate labels. \n\nCompared the the change in RMSE between Level1 and Level2 models.\n","58dc286c":"# Linear Regression with PCA","0f3d7ef1":"# Now let's use PCA and see what happens","67590938":"# Predicting with pseudo labels with no leak\n\nUsing the classifier instead of the Regressor models. \nWe are scaling the **Pawpularity** in the range of **0** and **1**.","35b49df0":"# Using SVR with PCA","ae8b220a":"# Using optimum ***n_components*** to create Pseudo labels","ec29ef0e":"# With Leakage","fc7aa984":"### Visualisation of the pseudolabels","d181e2cd":"# Ensembling","a3b7b583":"## Important Observations\n\n* The predicted output has less variance and mostly scattered near the mean\n* This draws to a conclusion that **meta-features** aren't enough to get the best results\n\n## Let's dive into using * **pseudo-labels** * for training and see if it actually decreases the RMSE","bb1b5f77":"# Comparison\n\nLet us now visually compare the **true values** with the **level 1** predictions and the **level 2** predictions","5093ac4a":"#### This step is gonna take some time. Don't worry :)       ","5c9d5bda":"# Linear Regression","6238cb01":"### Conclusions\n\nI ran these experiments a few times and the results are pretty consistent. I can see that there is a certain decrease in the RMSE by using the following methods:\n\n* Using K-folds\n* Using PCA over simple regressor\/classifier\n* Using Pseudo Labels for level 2 model\n* Using PCA and pseudo labelling simultaneously\n* Only meta features arent enough","9f4e04a9":"## Please <span style=\"color:red\">upvote<\/span> if you like :)\nAnd feel free to comment your opinion and anything you want :) <br>\nThis will motivate me to do more experiments :)","334764c2":"## Choosing the optimal ***n_components*** of PCA for the final ensemble \n\nUsing optuna to optimise the number of components of PCA"}}