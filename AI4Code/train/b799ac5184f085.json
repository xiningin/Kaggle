{"cell_type":{"00b82b72":"code","a93b849a":"code","08b68cde":"code","7aae9fd1":"code","4dd43f19":"code","2beebc74":"code","6cc65ec1":"code","2432a2f6":"code","91400959":"code","c85265d5":"code","66ca07b6":"code","5f146e2d":"code","d6c8f584":"code","26feb376":"code","7f259761":"code","85252905":"code","d5de8392":"code","2c1eea5b":"code","c811c865":"code","67094da2":"code","857da731":"code","44088819":"code","f145a176":"code","d37eda14":"code","b0b1cf4d":"code","5cfff836":"code","b7bbed2a":"code","c2575feb":"code","61b5bc0e":"code","154039e8":"code","1971eee8":"code","7edd3cd1":"code","37e4f00b":"code","fa5f9b76":"code","aa6efba3":"code","f7478f4c":"code","7d2a789f":"code","cc6e6cea":"code","49fb6f58":"code","66689eb2":"code","0d4cc2fa":"code","fd2bc391":"code","8681e8ba":"code","d6476bd7":"code","c3cbd405":"code","51d2a860":"code","a1031690":"code","7bb0ecba":"code","ca79f3cd":"code","7251a540":"code","cb572cbe":"markdown","f09deb09":"markdown","6ed00aa3":"markdown","4a61bd43":"markdown","3fc7cd4f":"markdown","a488b144":"markdown","605af2ee":"markdown","884b538f":"markdown","4ecbc5fb":"markdown","79191477":"markdown","7dd31c29":"markdown","c9dd7b37":"markdown","36291392":"markdown","71cd4805":"markdown","8e1edfbb":"markdown","d559ef5a":"markdown","12cbe573":"markdown","6dbfa1a8":"markdown","8aebc902":"markdown","f3a085e4":"markdown","3cdfcaa9":"markdown","67a06482":"markdown","788a0583":"markdown","f6cbccb7":"markdown","7d97d40d":"markdown","425477fe":"markdown","d6e962a1":"markdown"},"source":{"00b82b72":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import Imputer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.offline\nfrom plotly.offline import init_notebook_mode\nimport plotly.graph_objs as go","a93b849a":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntrain['set'] = 'train'\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ntest['Survived'] = 0\ntest['set'] = 'test'","08b68cde":"train = train[['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp','Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'set']]\ntest = test[['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp','Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'set']]\n\ndf = train.append(test)\ndf.head()","7aae9fd1":"df[\"Name\"] = df[\"Name\"].str.replace(r\"\\\"\", \"\")","4dd43f19":"cabin_dummies = df['Cabin'].str.extract(r'(?P<cabin_prefx>[a-zA-Z]*?)(?P<cabin_sufx>[0-9]+)$')\nname_dummies = df['Name'].str.extract(r'^(?P<Family_Name>[a-zA-Z]+)\\s*,\\s*(?P<pronoun>[a-zA-Z]+)\\s*\\.\\s*(?P<First_Name>[\\s*\\w()]*)')\ndf[\"prefix_ticket\"] = df[\"Ticket\"].str.extract(r\"^([A-Za-z.-\\\/ ]+) ?\")\ndf[\"prefix_ticket\"] = df[\"prefix_ticket\"].str.replace(\"[\\\/., ]\", \"\")\ndf[\"prefix_ticket\"] = df[\"prefix_ticket\"].str.upper()\n\ndf = pd.concat([df,cabin_dummies], axis=1)\ndf = pd.concat([df,name_dummies], axis=1)","2beebc74":"family_size = pd.DataFrame(df['Family_Name'].value_counts()).reset_index()\nfamily_size.columns = [\"Family_Name\", \"Family_Size\"]\ndf = df.merge(family_size, how=\"left\", left_on='Family_Name', right_on='Family_Name')","6cc65ec1":"df = df.fillna(0)","2432a2f6":"from sklearn import preprocessing\ndef create_dummies( df, colname ):\n    col_dummies = pd.get_dummies(df[colname], prefix=colname)\n    col_dummies.drop(col_dummies.columns[0], axis=1, inplace=True)\n    df = pd.concat([df, col_dummies], axis=1)\n    df.drop( colname, axis = 1, inplace = True )\n    return df","91400959":"dummizar = ['Pclass','Sex','Parch','pronoun','cabin_prefx', 'Family_Name', 'prefix_ticket']\n\ndf_antigo = df\n\nfor i in dummizar:\n    df = create_dummies(df, i)","c85265d5":"df.head()","66ca07b6":"df_antigo_test = df_antigo[df_antigo[\"set\"] == \"test\"]\ndf_antigo = df_antigo[df_antigo[\"set\"] == \"train\"]\n\ndf_test = df[df[\"set\"] == \"test\"]\ndf = df[df[\"set\"] == \"train\"]","5f146e2d":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64','uint8']\ndf_features = df.select_dtypes(include=numerics)\ndisplay(df_features.head())\n\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64','uint8']\ndf_features_test = df_test.select_dtypes(include=numerics)\ndisplay(df_features_test.head())","d6c8f584":"plt.figure(figsize=(15,8))\nsns.countplot(df_antigo[\"Family_Size\"], hue=df_antigo[\"Survived\"], orient=\"h\")","26feb376":"plt.figure(figsize=(20,10))\nsns.countplot(y=df_antigo[\"pronoun\"], hue=df_antigo[\"Survived\"], orient=\"h\")","7f259761":"plt.figure(figsize=(15,10))\nsns.countplot(y=df_antigo[\"prefix_ticket\"], hue=df_antigo[\"Survived\"], orient=\"h\")","85252905":"plt.clf()\nfig, axes = plt.subplots(1, 2, figsize=(20,5))\n\ndf_antigo_male = df_antigo[df_antigo[\"Sex\"] == \"male\"]\ndf_antigo_female = df_antigo[df_antigo[\"Sex\"] == \"female\"]\n\nsns.distplot( df_antigo_male[df_antigo_male[\"Survived\"]==1][\"Age\"], hist=False, color=\"skyblue\", label=\"Male Survived\", ax=axes[0])\nsns.distplot( df_antigo_male[df_antigo_male[\"Survived\"]==0][\"Age\"], hist=False, color=\"red\", label=\"Male Died\", ax=axes[0])\n\nsns.distplot( df_antigo_female[df_antigo_female[\"Survived\"]==0][\"Age\"] , hist=False, color=\"red\", label=\"Female Died\", ax=axes[1])\nsns.distplot( df_antigo_female[df_antigo_female[\"Survived\"]==1][\"Age\"] , hist=False, color=\"skyblue\", label=\"Female Survived\", ax=axes[1])\n\nplt.legend()\n","d5de8392":"fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n\nboxen = sns.boxenplot(x='Survived', y='Age', hue='Sex', data=df_antigo, ax=axes[0]);\nplt.setp(boxen.artists, alpha=.5, linewidth=2, edgecolor=\"k\");\n\nstrip = sns.stripplot(x='Survived', y='Age', hue='Sex', data=df_antigo, ax=axes[1]);\nplt.setp(strip.artists, alpha=.5, linewidth=2, edgecolor=\"k\");\n\nplt.xticks(rotation=45);","2c1eea5b":"quantitative_features_list = [\"Sex_male\",\"pronoun_Mr\",\"Pclass_3\",\"Fare\",\"pronoun_Mrs\",\"pronoun_Miss\",\"Age\",\"SibSp\",\"Parch_1\",\"cabin_prefx_B\",\"cabin_prefx_D\",\"cabin_prefx_E\",\"Pclass_2\",\"Parch_2\",\"pronoun_Master\",\"cabin_prefx_C\",\"prefix_ticket_A\",\"prefix_ticket_FCC\",\"prefix_ticket_PC\"]\ndf_quantitative_values = df[quantitative_features_list]\ndf_quantitative_values.head()","c811c865":"corr = df_quantitative_values.corr().round(1)\nplt.figure(figsize=(len(corr)\/2, len(corr)\/2))\n\nsns.heatmap(corr, cmap='BrBG', vmax=1.0, vmin=-1.0, center=0, annot=True, square=True, cbar=False);","67094da2":"df_features.head()","857da731":"X_train = df_features.iloc[:, 2:]\ny_train = df_features.iloc[:, 1]","44088819":"display(X_train.head())\ndisplay(y_train.head())","f145a176":"from sklearn.model_selection import StratifiedKFold\nkf = StratifiedKFold(n_splits=10, shuffle=True)\nkf.get_n_splits(X_train)\nprint(kf)","d37eda14":"x_train_list, y_train_list, x_test_list, y_test_list = list(), list(), list(), list()\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\naccuracy_scores = []\nprecision_scores = []\nrecall_scores = []\nf1_scores = []\nfeature_importances = []","b0b1cf4d":"x_train_list, y_train_list, x_test_list, y_test_list = list(), list(), list(), list()\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\naccuracy_scores = []\nprecision_scores = []\nrecall_scores = []\nf1_scores = []\nfeature_importances = []\n\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\n\nfor train_index,test_index in kf.split(X_train,y_train):\n    \n    X_train_kf, y_train_kf = X_train.iloc[train_index], y_train.iloc[train_index]\n    X_test_kf, y_test_kf = X_train.iloc[test_index], y_train.iloc[test_index]\n\n    res = classifier.fit(X_train_kf, y_train_kf)\n    y_pred_kf = classifier.predict(X_test_kf)\n    \n    accuracy_scores.append(accuracy_score(y_test_kf, y_pred_kf))\n    precision_scores.append(precision_score(y_test_kf, y_pred_kf))\n    recall_scores.append(recall_score(y_test_kf, y_pred_kf))\n    f1_scores.append(f1_score(y_test_kf, y_pred_kf))","5cfff836":"fig, axes = plt.subplots(1, 1, figsize=(20,7))\naxes.set_title('Logistic Regression')\naxes.set(ylim=(0.5, 1))\ndf_results = pd.DataFrame(list(zip(accuracy_scores, precision_scores, recall_scores, f1_scores)), columns=[\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"])\ndisplay(df_results.describe())\ndisplay(sns.lineplot(data=df_results, palette=\"husl\", linewidth=3))","b7bbed2a":"x_train_list, y_train_list, x_test_list, y_test_list = list(), list(), list(), list()\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\naccuracy_scores = []\nprecision_scores = []\nrecall_scores = []\nf1_scores = []\nfeature_importances = []\n\nfrom sklearn.naive_bayes import BernoulliNB\nnb = BernoulliNB()\n\nfor train_index,test_index in kf.split(X_train,y_train):\n    X_train_kf, y_train_kf = X_train.iloc[train_index], y_train.iloc[train_index]\n    X_test_kf, y_test_kf = X_train.iloc[test_index], y_train.iloc[test_index]\n    \n    res = nb.fit(X_train_kf, y_train_kf)\n#    importances = list(nb.feature_importances_)\n    y_pred_kf = nb.predict(X_test_kf)\n    \n#    feature_importances.append(importances)\n    accuracy_scores.append(accuracy_score(y_test_kf, y_pred_kf))\n    precision_scores.append(precision_score(y_test_kf, y_pred_kf))\n    recall_scores.append(recall_score(y_test_kf, y_pred_kf))\n    f1_scores.append(f1_score(y_test_kf, y_pred_kf))","c2575feb":"fig, axes = plt.subplots(1, 1, figsize=(20,7))\naxes.set_title('Naive Bayes')\naxes.set(ylim=(0.5, 1))\ndf_results = pd.DataFrame(list(zip(accuracy_scores, precision_scores, recall_scores, f1_scores)), columns=[\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"])\ndisplay(df_results.describe())\ndisplay(sns.lineplot(data=df_results, palette=\"husl\", linewidth=3))","61b5bc0e":"x_train_list, y_train_list, x_test_list, y_test_list = list(), list(), list(), list()\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\naccuracy_scores = []\nprecision_scores = []\nrecall_scores = []\nf1_scores = []\nfeature_importances = []\n\n# Import the model we are using\nfrom sklearn.ensemble import RandomForestClassifier\n# Instantiate model with 1000 decision trees\nrf = RandomForestClassifier(n_estimators = 2000, max_depth=5)\n\nfor train_index,test_index in kf.split(X_train,y_train):\n    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    \n    X_train_kf, y_train_kf = X_train.iloc[train_index], y_train.iloc[train_index]\n    X_test_kf, y_test_kf = X_train.iloc[test_index], y_train.iloc[test_index]\n    \n    res = rf.fit(X_train_kf, y_train_kf)\n    importances = list(rf.feature_importances_)\n    y_pred_kf = rf.predict(X_test_kf)\n    \n    feature_importances.append(importances)\n    accuracy_scores.append(accuracy_score(y_test_kf, y_pred_kf))\n    precision_scores.append(precision_score(y_test_kf, y_pred_kf))\n    recall_scores.append(recall_score(y_test_kf, y_pred_kf))\n    f1_scores.append(f1_score(y_test_kf, y_pred_kf))","154039e8":"fig, axes = plt.subplots(1, 1, figsize=(20,7))\naxes.set_title('Random Forest')\naxes.set(ylim=(0.5, 1))\ndf_results = pd.DataFrame(list(zip(accuracy_scores, precision_scores, recall_scores, f1_scores)), columns=[\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"])\ndisplay(df_results.describe())\ndisplay(sns.lineplot(data=df_results, palette=\"husl\", linewidth=3))","1971eee8":"fig, axes = plt.subplots(1, 1, figsize=(15,10))\nfeature_list = X_train.columns\ndf_features_importances = pd.DataFrame(list(zip(feature_list, importances)), columns=[\"feature\", \"importance\"])\ndf_features_importances = df_features_importances.sort_values(\"importance\", ascending=False)\nplot_features = df_features_importances.head(20)\nsns.barplot(y=plot_features[\"feature\"], x=plot_features[\"importance\"], palette=\"rocket\")","7edd3cd1":"relevant_features = df_features_importances.iloc[:20, 0].values","37e4f00b":"x_train_list, y_train_list, x_test_list, y_test_list = list(), list(), list(), list()\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\naccuracy_scores = []\nprecision_scores = []\nrecall_scores = []\nf1_scores = []\nfeature_importances = []\n\nX_train_fi = X_train[relevant_features]\n\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 2000, max_depth=5)\n\nfor train_index,test_index in kf.split(X_train_fi,y_train):\n    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    \n    X_train_kf, y_train_kf = X_train_fi.iloc[train_index], y_train.iloc[train_index]\n    X_test_kf, y_test_kf = X_train_fi.iloc[test_index], y_train.iloc[test_index]\n    \n    res = rf.fit(X_train_kf, y_train_kf)\n    importances = list(rf.feature_importances_)\n    y_pred_kf = rf.predict(X_test_kf)\n    \n    feature_importances.append(importances)\n    accuracy_scores.append(accuracy_score(y_test_kf, y_pred_kf))\n    precision_scores.append(precision_score(y_test_kf, y_pred_kf))\n    recall_scores.append(recall_score(y_test_kf, y_pred_kf))\n    f1_scores.append(f1_score(y_test_kf, y_pred_kf))","fa5f9b76":"fig, axes = plt.subplots(1, 1, figsize=(20,7))\naxes.set_title('Random Forest')\naxes.set(ylim=(0.5, 1))\ndf_results = pd.DataFrame(list(zip(accuracy_scores, precision_scores, recall_scores, f1_scores)), columns=[\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"])\ndisplay(df_results.describe())\ndisplay(sns.lineplot(data=df_results, palette=\"husl\", linewidth=3))","aa6efba3":"fig, axes = plt.subplots(1, 1, figsize=(15,10))\nfeature_list = X_train.columns\ndf_features_importances = pd.DataFrame(list(zip(feature_list, importances)), columns=[\"feature\", \"importance\"])\ndf_features_importances = df_features_importances.sort_values(\"importance\", ascending=False)\nsns.barplot(y=df_features_importances[\"feature\"], x=df_features_importances[\"importance\"], palette=\"rocket\")","f7478f4c":"x_train_list, y_train_list, x_test_list, y_test_list = list(), list(), list(), list()\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\naccuracy_scores = []\nprecision_scores = []\nrecall_scores = []\nf1_scores = []\nfeature_importances = []\n\nX_train_fi = X_train\n\nfrom sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier(n_estimators=1000, max_depth=2)\n\nfor train_index,test_index in kf.split(X_train_fi,y_train):\n    X_train_kf, y_train_kf = X_train_fi.iloc[train_index], y_train.iloc[train_index]\n    X_test_kf, y_test_kf = X_train_fi.iloc[test_index], y_train.iloc[test_index]\n    \n    res = gb.fit(X_train_kf, y_train_kf)\n    importances = list(gb.feature_importances_)\n    y_pred_kf = gb.predict(X_test_kf)\n    \n    feature_importances.append(importances)\n    accuracy_scores.append(accuracy_score(y_test_kf, y_pred_kf))\n    precision_scores.append(precision_score(y_test_kf, y_pred_kf))\n    recall_scores.append(recall_score(y_test_kf, y_pred_kf))\n    f1_scores.append(f1_score(y_test_kf, y_pred_kf))","7d2a789f":"fig, axes = plt.subplots(1, 1, figsize=(20,7))\naxes.set_title('Gradient Boosting')\naxes.set(ylim=(0.5, 1))\ndf_results = pd.DataFrame(list(zip(accuracy_scores, precision_scores, recall_scores, f1_scores)), columns=[\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"])\ndisplay(df_results.describe())\ndisplay(sns.lineplot(data=df_results, palette=\"husl\", linewidth=3))","cc6e6cea":"fig, axes = plt.subplots(1, 1, figsize=(15,10))\nfeature_list = X_train.columns\n\ndf_vamos_ver = pd.DataFrame(feature_importances, columns=feature_list)\ndf_features_importances = pd.DataFrame(df_vamos_ver.describe().iloc[1,:]).reset_index()\ndf_features_importances.columns=[\"feature\", \"importance\"]\n\ndf_features_importances = df_features_importances.sort_values(\"importance\", ascending=False)\nplot_features = df_features_importances.head(20)\nsns.barplot(y=plot_features[\"feature\"], x=plot_features[\"importance\"], palette=\"rocket\")","49fb6f58":"relevant_features = df_features_importances.iloc[:10, 0].values\nprint(relevant_features)","66689eb2":"x_train_list, y_train_list, x_test_list, y_test_list = list(), list(), list(), list()\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\naccuracy_scores = []\nprecision_scores = []\nrecall_scores = []\nf1_scores = []\nfeature_importances = []\n\nX_train_fi = X_train[relevant_features]\n\nfrom sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier(n_estimators=1000, max_depth=2, random_state=50)\n\nfor train_index,test_index in kf.split(X_train_fi,y_train):\n    X_train_kf, y_train_kf = X_train_fi.iloc[train_index], y_train.iloc[train_index]\n    X_test_kf, y_test_kf = X_train_fi.iloc[test_index], y_train.iloc[test_index]\n    \n    res = gb.fit(X_train_kf, y_train_kf)\n    importances = list(gb.feature_importances_)\n    y_pred_kf = gb.predict(X_test_kf)\n    \n    feature_importances.append(importances)\n    accuracy_scores.append(accuracy_score(y_test_kf, y_pred_kf))\n    precision_scores.append(precision_score(y_test_kf, y_pred_kf))\n    recall_scores.append(recall_score(y_test_kf, y_pred_kf))\n    f1_scores.append(f1_score(y_test_kf, y_pred_kf))","0d4cc2fa":"fig, axes = plt.subplots(1, 1, figsize=(20,7))\naxes.set_title('Gradient Boosting Filtered 10 best features')\naxes.set(ylim=(0.5, 1))\ndf_results = pd.DataFrame(list(zip(accuracy_scores, precision_scores, recall_scores, f1_scores)), columns=[\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"])\ndisplay(df_results.describe())\ndisplay(sns.lineplot(data=df_results, palette=\"husl\", linewidth=3))","fd2bc391":"fig, axes = plt.subplots(1, 1, figsize=(15,10))\nfeature_list = X_train_fi.columns\ndf_features_importances = pd.DataFrame(list(zip(feature_list, importances)), columns=[\"feature\", \"importance\"])\ndf_features_importances = df_features_importances.sort_values(\"importance\", ascending=False)\nplot_features = df_features_importances.head(20)\nsns.barplot(y=plot_features[\"feature\"], x=plot_features[\"importance\"], palette=\"rocket\")","8681e8ba":"print(relevant_features)","d6476bd7":"X_train_fi = X_train[relevant_features]\nX_train_fi.head()","c3cbd405":"classifier = gb\nclassifier.fit(X_train_fi, y_train)","51d2a860":"X_valid = df_features_test[relevant_features]\nIds = df_features_test.iloc[:, 0].reset_index()","a1031690":"X_valid.head()","7bb0ecba":"len(X_valid)","ca79f3cd":"y_pred = classifier.predict(X_valid)\ny_pred_df = pd.DataFrame(y_pred)\n\nsubmit_base = pd.DataFrame(pd.concat([Ids[\"PassengerId\"],y_pred_df], axis=1, ignore_index=True))\nsubmit_base.columns = [\"PassengerId\",\"Survived\"]\nsubmit_base = submit_base.sort_values(\"PassengerId\")\ndisplay(submit_base.head())","7251a540":"submit_base.to_csv(\"submission.csv\", index=False)","cb572cbe":"## missing values","f09deb09":"## filter the best 10 features","6ed00aa3":"## validation - measuring results","4a61bd43":"## Feature Importances","3fc7cd4f":"## results","a488b144":"## Feature importance reanalysed","605af2ee":"### restart measuring variables","884b538f":"### descriptive new features","4ecbc5fb":"# Validation base - predict and submit","79191477":"### separate the numeric values","7dd31c29":"# Gradient Boosting (filtered)","c9dd7b37":"## Analyse result","36291392":"## separating numeric values","71cd4805":"# Gradient Boosting","8e1edfbb":"# Naive Bayes","d559ef5a":"# Randon Forest *20 most relevant features*","12cbe573":"## creating dummies","6dbfa1a8":"# Feature Engineering (extraction)","8aebc902":"# Data Prep","f3a085e4":"# Random Forest\n\n## kfold (all predictors)","3cdfcaa9":"## feature importance analysis","67a06482":"## stratified kfold (train e test)","788a0583":"# Separate Train and Test (kaggle dataset)","f6cbccb7":"## hold-out (train e validation)","7d97d40d":"# cross-validation","425477fe":"# logistic regression","d6e962a1":"## Analyse result"}}