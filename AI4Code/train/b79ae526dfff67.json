{"cell_type":{"feb026ae":"code","5adff1c8":"code","03b44ac8":"code","9cd726ea":"code","c9d887d7":"code","e9204430":"code","490b14ca":"code","11efd38e":"code","621180c8":"code","331f0a50":"code","62bd878c":"code","c000f04f":"code","6cb84fd1":"code","2a176529":"code","c669d0b2":"markdown","a4251770":"markdown","ed99c746":"markdown","47bdebab":"markdown","66f02b1c":"markdown","b4c0ef36":"markdown","287709b7":"markdown","0e16d994":"markdown","c9f34c73":"markdown","0a6a2db0":"markdown","8bcc04ef":"markdown","50115b60":"markdown"},"source":{"feb026ae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nibabel as nib #for loading nii files\nimport matplotlib.pyplot as plt\nimport cv2\nimport tensorflow as tf #for deep learning","5adff1c8":"data = pd.read_csv('..\/input\/covid19-ct-scans\/metadata.csv')\ndata.head()","03b44ac8":"def read_nii(filepath):\n    '''\n    Reads .nii file and returns pixel array\n    '''\n    ct_scan = nib.load(filepath)\n    array   = ct_scan.get_fdata()\n    array   = np.rot90(np.array(array))\n    return(array)","9cd726ea":"# Read sample\nsample_ct   = read_nii(data.loc[1,'ct_scan'])\nsample_lung = read_nii(data.loc[1,'lung_mask'])\nsample_infe = read_nii(data.loc[1,'infection_mask'])\nsample_all  = read_nii(data.loc[1,'lung_and_infection_mask'])","c9d887d7":"fig = plt.figure(figsize = (18,15))\nplt.subplot(1,4,1)\nplt.imshow(sample_ct[..., 150], cmap = 'bone')\nplt.title('Original Image')\n\nplt.subplot(1,4,2)\nplt.imshow(sample_ct[..., 150], cmap = 'bone')\nplt.imshow(sample_lung[..., 150],alpha = 0.5, cmap = 'nipy_spectral')\nplt.title('Lung Mask')\n\nplt.subplot(1,4,3)\nplt.imshow(sample_ct[..., 150], cmap = 'bone')\nplt.imshow(sample_infe[..., 150], alpha = 0.5, cmap = 'nipy_spectral')\nplt.title('Infection Mask')\n\nplt.subplot(1,4,4)\nplt.imshow(sample_ct[..., 150], cmap = 'bone')\nplt.imshow(sample_all[..., 150], alpha = 0.5, cmap = 'nipy_spectral')\nplt.title('Lung and Infection Mask')","e9204430":"lungs = []\ninfections = []\nimg_size = 128\n\nfor i in range(len(data)):\n    ct = read_nii(data['ct_scan'][i])\n    infect = read_nii(data['infection_mask'][i])\n    \n    for ii in range(ct.shape[0]):\n        lung_img = cv2.resize(ct[ii], dsize = (img_size, img_size),interpolation = cv2.INTER_AREA).astype('uint8')\n        infec_img = cv2.resize(infect[ii],dsize=(img_size, img_size),interpolation = cv2.INTER_AREA).astype('uint8')\n        lungs.append(lung_img[..., np.newaxis])\n        infections.append(infec_img[..., np.newaxis])","490b14ca":"lungs = np.array(lungs)\ninfections = np.array(infections)","11efd38e":"from sklearn.model_selection import train_test_split\nlung_train, lung_test, infect_train, infect_test = train_test_split(lungs, infections, test_size = 0.1)","621180c8":"IMG_HEIGHT = 128\nIMG_WIDTH = 128\nIMG_CHANNELS = 1","331f0a50":"#Build the model\ninputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\ns = tf.keras.layers.Lambda(lambda x: x \/ 255)(inputs)\n\n#Contraction path\nc1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\nc1 = tf.keras.layers.Dropout(0.1)(c1)\nc1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\np1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n\nc2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\nc2 = tf.keras.layers.Dropout(0.1)(c2)\nc2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\np2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n \nc3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\nc3 = tf.keras.layers.Dropout(0.2)(c3)\nc3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\np3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n \nc4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\nc4 = tf.keras.layers.Dropout(0.2)(c4)\nc4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\np4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n \nc5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\nc5 = tf.keras.layers.Dropout(0.3)(c5)\nc5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n\n#Expansive path \nu6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\nu6 = tf.keras.layers.concatenate([u6, c4])\nc6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\nc6 = tf.keras.layers.Dropout(0.2)(c6)\nc6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n \nu7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\nu7 = tf.keras.layers.concatenate([u7, c3])\nc7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\nc7 = tf.keras.layers.Dropout(0.2)(c7)\nc7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n \nu8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\nu8 = tf.keras.layers.concatenate([u8, c2])\nc8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\nc8 = tf.keras.layers.Dropout(0.1)(c8)\nc8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n \nu9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\nu9 = tf.keras.layers.concatenate([u9, c1], axis=3)\nc9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\nc9 = tf.keras.layers.Dropout(0.1)(c9)\nc9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n \noutputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\nmodel = tf.keras.Model(inputs=[inputs], outputs=[outputs])\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()","62bd878c":"history = model.fit(lung_train, infect_train, epochs = 10, validation_data = (lung_test, infect_test))","c000f04f":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Accuracy vs Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(['Train', 'Val'], loc = 'upper left')\nplt.show()","6cb84fd1":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss vs Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(['Train', 'Val'], loc = 'upper right')\nplt.show()\n","2a176529":"predicted = model.predict(lung_test)\nfig = plt.figure(figsize = (18,15))\n\n\nfor i in range(len(lung_test)):\n    plt.subplot(1,3,1)\n    plt.imshow(lung_test[i][...,0], cmap = 'bone')\n    plt.title('original lung')\n\n    plt.subplot(1,3,2)\n    plt.imshow(lung_test[i][...,0], cmap = 'bone')\n    plt.imshow(infect_test[i][...,0],alpha = 0.5, cmap = \"nipy_spectral\")\n    plt.title('original infection mask')\n\n    plt.subplot(1,3,3)\n    plt.imshow(lung_test[i][...,0], cmap = 'bone')\n    plt.imshow(predicted[i][...,0],alpha = 0.5,cmap = \"nipy_spectral\")\n    plt.title('predicted infection mask')","c669d0b2":"## Reading the chest ct scan data","a4251770":"## Converting the data into array for training model","ed99c746":"## Using our trained model for prediction","47bdebab":"## Building the U-Net architecture for the segmentation","66f02b1c":"## Performing test and train split","b4c0ef36":"## Plotting the sample images","287709b7":"## Defining function for readiing .nii files","0e16d994":"## Training the model","c9f34c73":"## Importing important libraries","0a6a2db0":"## Plotting the performance metrics","8bcc04ef":"**Introduction**:\n\n![image.png](attachment:8ecbb069-d658-4319-bba1-7eb21d481208.png)![image.png](attachment:7ae186ca-6d28-4a87-9dad-f51dcaf2b258.png)\n\nU-Net is a convolutional neural network that was developed for biomedical image segmentation at the Computer Science Department of the University of Freiburg, Germany. The network is based on the fully convolutional network and its architecture was modified and extended to work with fewer training images and to yield more precise segmentations. Segmentation of a 512 \u00d7 512 image takes less than a second on a modern GPU.\n\nThe U-Net architecture stems from the so-called \u201cfully convolutional network\u201d first proposed by Long and Shelhamer.\n\nThe main idea is to supplement a usual contracting network by successive layers, where pooling operations are replaced by upsampling operators. Hence these layers increase the resolution of the output. What's more, a successive convolutional layer can then learn to assemble a precise output based on this information.\n\nOne important modification in U-Net is that there are a large number of feature channels in the upsampling part, which allow the network to propagate context information to higher resolution layers. As a consequence, the expansive path is more or less symmetric to the contracting part, and yields a u-shaped architecture. The network only uses the valid part of each convolution without any fully connected layers. To predict the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image. This tiling strategy is important to apply the network to large images, since otherwise the resolution would be limited by the GPU memory.\n\nSource: https:\/\/en.wikipedia.org\/wiki\/U-Net\n\n","50115b60":"## Reading the metadata"}}