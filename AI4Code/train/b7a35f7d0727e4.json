{"cell_type":{"0df193bd":"code","aacd70ce":"code","9252eb31":"code","b057e050":"code","4a112c8f":"code","97550deb":"code","da701302":"code","32c5eb12":"code","b269fe5e":"code","a6615df1":"code","1f03483b":"code","3216da11":"code","cf9ec502":"code","f593ca20":"code","50342b1e":"code","e743ea17":"code","7d9a773e":"code","67deb01d":"code","8736b29b":"code","de9c0947":"code","a900121b":"markdown","00afdbba":"markdown","3ce748da":"markdown","0317b39a":"markdown","a2e3d61c":"markdown","cb98cd22":"markdown","e66112ac":"markdown","51133b1e":"markdown","5f6f0248":"markdown","8e33d826":"markdown","ca9e6b08":"markdown","2419101c":"markdown","16f2621c":"markdown","e88e3afb":"markdown","f05ef555":"markdown","3cfbb4e2":"markdown","c506ecc3":"markdown","15e696fa":"markdown","11f76365":"markdown","9d692255":"markdown","a5dca100":"markdown","29667e19":"markdown"},"source":{"0df193bd":"## Import Libraries\nimport numpy as np \nimport pandas as pd \nimport os\n\n# Read Data\ndata=pd.read_csv(\"..\/input\/googleplaystore_user_reviews.csv\",encoding=\"latin1\")\n","aacd70ce":"data.head()    # Show information about our data. ","9252eb31":"data=pd.concat([data.Translated_Review,data.Sentiment],axis=1)\ndata.dropna(axis=0,inplace=True)  # For drop nan values. It makes confuse for our model.\ndata.tail()","b057e050":"data.Sentiment.unique() ","4a112c8f":"data.Sentiment=[0 if i==\"Positive\" else 1 if i== \"Negative\" else 2 for i in data.Sentiment]\n\n#visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.countplot(data.Sentiment)\nplt.title(\"Count of Sentiments\")","97550deb":"data.Sentiment.value_counts()","da701302":"import re ## Regular expression for deleting characters which are not letters.\nfirst_sample = data.Translated_Review[9] \nsample = re.sub(\"[^a-zA-Z]\",\" \",first_sample)\nsample = sample.lower()\nprint(\"[{}] convert to \\n[{}]\".format(data.Translated_Review[9],sample))","32c5eb12":"## import libraries\n\nimport nltk  ## Natural Language Tool Kit\nfrom nltk.corpus import stopwords \n\nsample=nltk.word_tokenize(sample)\nprint(sample)","b269fe5e":"sample = [word for word in sample if not word in set(stopwords.words(\"english\"))]\nprint(sample)   ## drop unnecesarry words like it, I, you.","a6615df1":"lemma = nltk.WordNetLemmatizer()  ##We have already imported nltk.\nsample = [ lemma.lemmatize(word) for word in sample]\nsample = \" \".join(sample)\n## for this example there is no paragoge I cant show you but if there is -ed or -s or something like these,\n## lemmatizer will drop them and returns stem of word","1f03483b":"text_list=[]\nfor i in data.Translated_Review:\n    text=re.sub(\"[^a-zA-Z]\",\" \",i)\n    text=text.lower()\n    text=nltk.word_tokenize(text)\n    lemma=nltk.WordNetLemmatizer()\n    text=[lemma.lemmatize(word) for word in text]\n    text=\" \".join(text)\n    text_list.append(text)","3216da11":"text_list[:5]","cf9ec502":"from sklearn.feature_extraction.text import CountVectorizer\nmax_features=1000\ncou_vec=CountVectorizer(max_features=max_features) # stop_words=\"english\" you can add but we have already applied it.\nsparce_matrix=cou_vec.fit_transform(text_list).toarray()\nall_words=cou_vec.get_feature_names()\nprint(\"Most used 50 words: \",all_words[0:50])","f593ca20":"y = data.iloc[:,1].values\nx= sparce_matrix\n\nfrom sklearn.model_selection import train_test_split\n\nxtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=0.2,random_state=1)","50342b1e":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(xtrain,ytrain)\nprint(\"acc : \", nb.score(xtest,ytest))","e743ea17":"y_pred=nb.predict(xtest)\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nnames=[\"Positive\",\"Negative\",\"Neutral\"]\ncm=confusion_matrix(ytest,y_pred)\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidth=.5,linecolor=\"r\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nax.set_xticklabels(names)\nax.set_yticklabels(names)\nplt.show()","7d9a773e":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nrf = RandomForestClassifier(n_estimators = 10, random_state=42)\nrf.fit(xtrain,ytrain)\nprint(\"acc: \",rf.score(xtest,ytest))","67deb01d":"y_pred=rf.predict(xtest)\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nnames=[\"Positive\",\"Negative\",\"Neutral\"]\ncm=confusion_matrix(ytest,y_pred)\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidth=.5,linecolor=\"r\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nax.set_xticklabels(names)\nax.set_yticklabels(names)\nplt.show()","8736b29b":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\nlr.fit(xtrain,ytrain)\nprint(\"Logistic Regression accuracy: \",lr.score(xtest,ytest))","de9c0947":"y_pred=lr.predict(xtest)\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nnames=[\"Positive\",\"Negative\",\"Neutral\"]\ncm=confusion_matrix(ytest,y_pred)\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidth=.5,linecolor=\"r\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nax.set_xticklabels(names)\nax.set_yticklabels(names)\nplt.show()","a900121b":"Here is our data. We need to classify reviews according to Sentiment. So Translated_Review is our x column and Sentiment is our y column which we will predict.\n* Now Lets learn our sentiments.","00afdbba":"59%... Not a good score lets try other models","3ce748da":"*See we removed  ' ! ' from sentence.","0317b39a":"### *Drop Unnecessary Words","a2e3d61c":"** Split data to train and test","cb98cd22":"## *For giving example I will work on only one sample. Then works for all","e66112ac":"# Lets apply it for all !","51133b1e":"We have 3 values:\n* Posivitive (0)\n* Negative  (1)\n* Neutral     (2)\n \nWe can accept it like this. Lets convert these to our values.","5f6f0248":"## Here our Preprocessing Side\n\n** So we need to preprocess our data which means clean data for model.\n\n** First we need to remove characters which are not letters.","8e33d826":"# 90%, A little bit better.","ca9e6b08":"# We have bag words, clean and relevant words.","2419101c":"# - What is Lemmatazation\n**  With lemmatization we can convert words to stem. For example; Liked and Like. \n** Is it important ?\n    - Yes, because in your \tperspective liked and like seem like same thing but for machine they both are different. We need to make easier it for machine.","16f2621c":"### This table is confusion matrix and show us predict of models. Y coordinates is y_true, x coordinates is y_pred. \n#### In this table if we look neutral, our model predict 38 positive and 58 negative instead of neutral","e88e3afb":"## *Now StopWords Turn","f05ef555":"# Lets try with Naive Bayes :","3cfbb4e2":"# RandomForest Classifier","c506ecc3":"# Our data is ready for models. Its time to choose best one !","15e696fa":"Translated_Review &  Sentiment columns are the columns which we are interested. So lets take them.","11f76365":"## What is Natural Language Processing (NPL) ?\nNatural language processing (NLP) is a subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\n\n* In this kernel we will learn how we classify types of review. \n* Our steps are:\n        ** Read Data\n        ** PreProcess Data\n        ** Stopwords\n        ** Lemmatazation\n","9d692255":"### *Tokenize provides us split the sentence.","a5dca100":"## 89% is good score, but lets try \n\n## -Logistic Regression","29667e19":"# Conclusion\n\n### We used npl on google play store apps data set. We follow basic npl processes which are PreProcess Data, remove Stopwords, Lemmatazation, create bag of words and finally use our model.\n\n### **We split our data test and train for model.\n\n### ** We used random forest, naive bayes and finally logistic regression.\n\n###  ** According to score table Logistic Regression gives us best accurancy like 90%.\n\n###  ** If you have any question please feel free to ask. And if you like it please upvote to motivate :)"}}