{"cell_type":{"df897a44":"code","a5efef32":"code","dbf14bb4":"code","a84a91b3":"code","8afa381f":"code","1e8a46a1":"code","0805c2b0":"code","ecbedd38":"code","7a6deeab":"code","7474839e":"code","706d8607":"code","f1c45023":"code","9c5cf001":"code","756ed84b":"code","d805c196":"markdown","45c78d6a":"markdown","e3dfbbc1":"markdown","6a0a962a":"markdown","f867393f":"markdown","6bc72b25":"markdown","de7baa01":"markdown","7d8d9fe2":"markdown","2961c024":"markdown","4ade27ba":"markdown"},"source":{"df897a44":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport csv\nfrom PIL import Image\nimport torch\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a5efef32":"!python --version\n!pip freeze | grep torch","dbf14bb4":"class Steel_Data(torch.utils.data.Dataset):\n    def __init__(self, csv_file, mode='train', transform=None):\n        \n        self.mode = mode # 'train', 'val' or 'test'\n        self.data_list = []\n        self.category = []\n        self.transform = transform\n        \n        with open(csv_file, newline='') as csvfile:\n            reader = csv.DictReader(csvfile) #\u7528dictionary\u7684\u65b9\u5f0f\u8b80\u53d6csv\u7684\u8cc7\u6599\n            for row in reader:\n                self.data_list.append(row['ImageId']) #\u5c07key \u70ba file_path\u7684value\u8b80\u9032data_list\n                if mode != 'test':\n                    self.category.append(int(row['ClassId'])-1)\n        if mode == 'train':\n            self.data_list = self.data_list[0:6000]\n            self.category = self.category[0:6000]\n        if mode == 'val':\n            self.data_list = self.data_list[6000:7096]\n            self.category = self.category[6000:7096] \n\n    def __getitem__(self, index):\n\n        data = Image.open('..\/input\/severstal-steel-defect-detection\/train_images\/'+ self.data_list[index])\n        if self.transform is not None:\n            data = self.transform(data)\n        if self.mode == 'test':\n            return data\n        category = torch.tensor(int(self.category[index]))\n        return data, category\n\n    def __len__(self):\n        return len(self.data_list)","a84a91b3":"from torchvision import transforms\n# For TRAIN\n########################################################################\n#  TODO: use transforms.xxx method to do some data augmentation        #\n#  This one is for training, find the composition to get better result #\n########################################################################\ntransforms_train = transforms.Compose([\ntransforms.Resize((256, 256)),         #\u5c07\u7167\u7247\u56fa\u5b9a\u70ba196x196\u7684\u5927\u5c0f\ntransforms.RandomCrop((224, 224)),      #\u5c07\u7167\u7247\u96a8\u6a5f\u88c1\u6e1b\u70ba224x224\u7684\u5927\u5c0f\ntransforms.RandomHorizontalFlip(p=0.5), #0.5\u7684\u6a5f\u7387\u662f\u5426\u6c34\u5e73\u7ffb\u8f49\ntransforms.RandomVerticalFlip(p=0.5),   #0.5\u7684\u6a5f\u7387\u662f\u5426\u5782\u76f4\u7ffb\u8f49\ntransforms.RandomRotation(degrees=(-90, 90)),  #\u96a8\u6a5f\u5730\u5728-90~90\u5ea6\u9593\u65cb\u8f49\ntransforms.ToTensor(),  #\u5c07\u7167\u7247\u8f49\u6210tensor \u4e26\u4e14\u5c07\u6578\u503c\u90fd\u8f49\u63db\u62100~1 \ntransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n]) #\u6a19\u6e96\u5316 \n########################################################################\n#                           End of your code                           #\n########################################################################\n\n# For VAL, TEST\n########################################################################\n#  TODO: use transforms.xxx method to do some data augmentation        #\n#  This one is for validate and test,                                  #\n#  NOTICE some operation we usually not use in this part               #\n########################################################################\ntransforms_test = transforms.Compose([\ntransforms.Resize((256, 256)),\ntransforms.CenterCrop((224, 224)),\ntransforms.ToTensor(),\ntransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n########################################################################\n#                           End of your code                           #\n########################################################################","8afa381f":"dataset_train = Steel_Data('..\/input\/severstal-steel-defect-detection\/train.csv', mode='train',transform=transforms_train)\ndataset_val = Steel_Data('..\/input\/severstal-steel-defect-detection\/train.csv', mode='val', transform=transforms_train)","1e8a46a1":"print(\"The first image's shape in dataset_train :\", dataset_train.__getitem__(0)[0].size()) #[0]means data\nprint(\"There are\", dataset_train.__len__(), \"images in dataset_train.\")\nprint('-'*50)\n\nprint(\"The first image's shape in dataset_val :\", dataset_val.__getitem__(0)[0].size()) #[0]means data\nprint(\"There are\", dataset_val.__len__(), \"images in dataset_val.\")\n\n# 224x224 because of transformation","0805c2b0":"from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(dataset_train, batch_size=32, shuffle=True)\nval_loader = DataLoader(dataset_val, batch_size=32, shuffle=False)","ecbedd38":"import torch.nn as nn \nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.models as models\nresnet101=torchvision.models.resnet101(pretrained=True)\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net,self).__init__()\n        self.cnn_model = resnet101\n        self.fc1 = nn.Sequential(\n            nn.Linear(1000, 500),\n            nn.ReLU(),\n            nn.BatchNorm1d(500),\n            nn.Dropout(p=0.4),\n            nn.Linear(500, 4))\n    def forward(self, x):\n        x = self.cnn_model(x)\n        out = self.fc1(x)\n        return out","7a6deeab":"model = Net()\ndevice = torch.device('cuda')\nmodel = model.to(device)","7474839e":"x = torch.rand(32, 3, 224, 224) # generate fake data\nx = x.to(device)\nout = model(x) # output of category and attribute\nprint(out)\nprint(out.shape)\n_,p=torch.max(out.data,1)\nprint(p)","706d8607":"import torch.nn as nn\nimport torch.optim as optim\ncriterion = nn.CrossEntropyLoss() # CrossEntropyLoss function combines both a SoftMax activation \n                                  # and a cross entropy loss function in the same function \n                                  # \u9019\u6b63\u662f\u70ba\u4ec0\u9ebc\u6211\u5011\u6c92\u6709\u5728\u6700\u5f8c\u7528 SoftMax\u8f49\u63db\u7684\u539f\u56e0\noptimizer = torch.optim.SGD(model.parameters(), lr=0.005,momentum=0.9)\ncriterion = criterion.to(device)","f1c45023":"def train(input_data, model, criterion, optimizer):\n    '''\n    Argement:\n    input_data -- iterable data, typr torch.utils.data.Dataloader is prefer\n    model -- nn.Module, model contain forward to predict output\n    criterion -- loss function, used to evaluate goodness of model\n    optimizer -- optmizer function, method for weight updating\n    '''\n    model.train()\n    total_count = 0\n    acc_count = 0\n    total_run = 0\n    total_f1_score = 0\n    for i, data in enumerate(input_data, 0):\n        images, categorys = data[0].to(device), data[1].to(device)\n        \n        ########################################################################\n        # TODO: Forward, backward and optimize                                 #\n        # 1. zero the parameter gradients                                      #\n        # 2. process input through the network                                 #\n        # 3. compute the loss                                                  #\n        # 4. propagate gradients back into the network\u2019s parameters            #\n        # 5. Update the weights of the network                                 #\n        ########################################################################\n        # Run the forward \n        outputs = model(images)\n        loss = criterion(outputs, categorys) \n\n        # Backward and perform optimization\n        optimizer.zero_grad() #\u5c07\u68af\u5ea6\u521d\u59cb\u5316\u70ba0\uff0c\u9019\u6b65\u5f88\u95dc\u9375\uff0c\u56e0\u70ba\u6bcf\u6b21\u6211\u5011\u4f7f\u7528\u7684batch\u4e0d\u540c\uff0c\u5c0e\u81f4loss\u4e0d\u540c\uff0c\u56e0\u6b64\u68af\u5ea6\u51fd\u6578\u4e0d\u540c\n        loss.backward() #\u9032\u884c\u53cd\u5411\u50b3\u64ad\n        optimizer.step() #\u85c9\u7531\u53cd\u5411\u50b3\u64ad\u7684\u7d50\u679c\u8a08\u7b97\u68af\u5ea6\n        ########################################################################\n        #                           End of your code                           #\n        ########################################################################\n\n\n        ########################################################################\n        # TODO: Get the counts of correctly classified images                  #\n        # 1. get the model predicted result                                    #\n        # 2. sum the number of this batch predicted images                     #\n        # 3. sum the number of correctly classified                            #\n        # 4. save this batch's loss into loss_list                             #\n        # dimension of outputs: [batch_size, number of classes]                #\n        # Hint 1: use outputs.data to get no auto_grad                         #\n        # Hint 2: use torch.max()                                              #\n        ########################################################################\n        _, predicted = torch.max(outputs.data,1) #\u8fd4\u56de\u6bcf\u4e00\u884c\u4e2d\u6700\u5927\u503c\u7684\u90a3\u4e2a\u5143\u7d20\uff0c\u4e14\u8fd4\u56de\u5176\u7d22\u5f15\n        total_count += categorys.size(0) #x.size(0)\u6307\u7684\u662fbatch size (\u76ee\u524d\u8a2d\u5b9a\u70ba32)\n        acc_count += (predicted == categorys).sum().item() #\u5206\u985e\u6b63\u78ba\u7684\u7e3d\u6578\u91cf\n        total_run += 1\n        ########################################################################\n        #                           End of your code                           #\n        ########################################################################\n\n    # Compute this epoch accuracy and loss\n    acc = acc_count \/ total_count\n\n    return acc","9c5cf001":"def val(input_data, model, criterion, optimizer):\n    model.eval()\n    \n    total_count = 0\n    acc_count = 0\n    total_run = 0\n    total_f1_score = 0\n    with torch.no_grad():\n        for data in input_data:\n            images, categorys = data[0].to(device), data[1].to(device)\n\n            ####################################################################\n            # TODO: Get the predicted result and loss                          #\n            # 1. process input through the network                             #\n            # 2. compute the loss                                              #\n            # 3. get the model predicted result                                #\n            # 4. get the counts of correctly classified images                 #\n            # 5. save this batch's loss into loss_list                         #\n            ####################################################################\n            outputs_val = model(images)\n            loss = criterion(outputs_val, categorys)\n\n            _, predicted = torch.max(outputs_val.data,1)\n            total_count += categorys.size(0)\n            acc_count += (predicted == categorys).sum().item()\n            total_run += 1\n            ####################################################################\n            #                         End of your code                         #\n            ####################################################################\n\n    acc = acc_count \/ total_count\n    return acc","756ed84b":"################################################################################\n# You can adjust those hyper parameters to loop for max_epochs times           #\n################################################################################\nmax_epochs = 50\nlog_interval = 1 # print acc and loss in per log_interval time\n################################################################################\n#                               End of your code                               #\n################################################################################\ntrain_acc_list = []\nval_acc_list = []\n\nfor epoch in range(1, max_epochs + 1):\n    print('=' * 20, 'Epoch', epoch, '=' * 20)\n    train_acc = train(train_loader, model, criterion, optimizer)\n    val_acc = val(val_loader, model, criterion, optimizer)\n\n    train_acc_list.append(train_acc)\n    val_acc_list.append(val_acc)\n    if epoch % log_interval == 0:\n        print('Train Acc: {:.6f}'.format(train_acc))\n        print('Val Acc: {:.6f}'.format(val_acc))","d805c196":"# 2. Implement CNN using PyTorch","45c78d6a":"### 2.3  Define loss and optimizer","e3dfbbc1":"### 1.2 Instantiate dataset","6a0a962a":"### 3.1 Train function\nLet's define train function.  \nIt will iterate input data 1 epoch and update model with optmizer.  \nFinally, calculate mean loss and total accuracy.\n\nHint: [torch.max()](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.max.html#torch-max)","f867393f":"### 1.3 DataLoader \n\n`torch.utils.data.DataLoader` define how to sample from `dataset` and some other function like:\n+ `shuffle` : set to `True` to have the data reshuffled at every epoch\n+ `batch_size` : how many samples per batch to load\n\nSee [torch.utils.data.DataLoader](https:\/\/pytorch.org\/docs\/stable\/data.html#torch.utils.data.DataLoader) for more details","6bc72b25":"## 3. Train the model","de7baa01":"### 2.1 Define a Convolutional Neural Network\n\nTry to design and train a deep convolutional network from scratch to predict the class label of a flower image. \n\nYou can refer to last assignment about image_classifier, and try to go deep and use more method for better model.","7d8d9fe2":"### 1.1 Data augmentation \n\nData augmentation are techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data.\n\nPytorch use `torchvision.transforms` to do data augmentation.\n[You can see all function here.](https:\/\/pytorch.org\/docs\/stable\/torchvision\/transforms.html)\n\n**NOTICE**: There are some operations may not be necessary for predict, so we should write one for train and one for others.\n\n(**Slide.07 page.49**)","2961c024":"### 2.2 Debug your code","4ade27ba":"## 1. Loading the dataset"}}