{"cell_type":{"700c48f6":"code","4a49bb5f":"code","a1e7fee6":"code","9eff098d":"code","68af1d06":"code","633f50d8":"code","1b375f49":"code","f973972a":"code","5d223ba1":"code","1fc86670":"code","3e13fe8e":"code","f29df812":"code","03096b2a":"code","55fd165f":"code","cd866869":"code","32a05fcc":"code","e3440dee":"code","e8a3c182":"code","2d0b9592":"code","1bb19d09":"code","7617737a":"code","d0421659":"code","c8a1a998":"code","956e46da":"code","a7e266a8":"code","3facf654":"code","201bc252":"code","6ded15f6":"code","10ec7cb5":"code","d7a435c5":"code","dec4d9c7":"code","c5d24733":"code","03fe733e":"code","d1271fdf":"code","9c7940a9":"code","805c60e8":"code","96f7febe":"code","642cee66":"code","ba88cc23":"code","2e9eaa6d":"code","9f87d934":"code","6a3ba1fa":"code","29f8d71e":"code","f5a9880d":"code","8ee8c4ae":"code","04f7ac3c":"code","53b15add":"code","39dad8ba":"code","cdd4b3df":"code","cea717dd":"code","36073699":"code","621fd408":"code","d2ddb753":"code","6f7d7db5":"code","c3101473":"code","b72d33de":"code","88177b43":"code","0a8bbaa0":"code","a173ea5d":"markdown","4e017efd":"markdown","9083e77b":"markdown","91800a29":"markdown","924159ee":"markdown","ea6a0854":"markdown","b4bdab26":"markdown","2a65a347":"markdown","f4479e29":"markdown","f5a5af37":"markdown","20e172c1":"markdown","fde7ab2e":"markdown","567f4a0a":"markdown","6f85b098":"markdown","cea5ba9c":"markdown","90bc9337":"markdown","324f2983":"markdown","d1516f55":"markdown","871a2744":"markdown","c74c64ee":"markdown","0222045a":"markdown","65afc11e":"markdown","f58271d5":"markdown","efbc8c27":"markdown","b80bcc6b":"markdown","20395daf":"markdown","aa0eb8fa":"markdown","6c7b5db2":"markdown","afd3ac76":"markdown","2a238ef9":"markdown","5b795120":"markdown","8e75b8a8":"markdown","80318ebd":"markdown","a78a4626":"markdown","35e71fbd":"markdown","3fe9ef81":"markdown","5da28247":"markdown","b35baf17":"markdown","5bf6b20c":"markdown","03d2c4bb":"markdown","a7a9482b":"markdown","eb9fb737":"markdown","24f46f1c":"markdown","b899bf9c":"markdown","2dda38d8":"markdown","88a2d149":"markdown","3986a53a":"markdown","bbfdf3c6":"markdown","9e0266bf":"markdown","b28f8a12":"markdown","6a836bab":"markdown","e7154e5d":"markdown","dfef180c":"markdown","82303e26":"markdown","6a4b8d67":"markdown","0c74ac4d":"markdown","94549d85":"markdown","e23fd6ef":"markdown","4a9720b0":"markdown","5fd9979c":"markdown","56109d02":"markdown","21f9e859":"markdown","c00e6930":"markdown","dd2b7e10":"markdown","da8e00dd":"markdown","f9d62545":"markdown","e7df2847":"markdown","761d5218":"markdown","e27140dc":"markdown","6d316e11":"markdown","0adf6ef8":"markdown","db223a77":"markdown","71697562":"markdown","9371209b":"markdown","43a5111b":"markdown","5309f66c":"markdown","abee32e2":"markdown"},"source":{"700c48f6":"#Load libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Load dataset\ndf = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')","4a49bb5f":"#Get to know the dataset and display all columns\npd.set_option('display.max_columns', None)\ndf.head()","a1e7fee6":"#Determine number of rows and columns\ndf.shape","9eff098d":"#Check features, datatypes and null values\ndf.info()","68af1d06":"#Remove rows with null values\ndf = df.dropna(how='any',axis=0) ","633f50d8":"#Determine number of rows and columns\ndf.shape","1b375f49":"plt.figure(figsize=(3,5))\ncountplot_stroke = sns.countplot(data=df,x='stroke')\nplt.title(\"Number of stroke and no stroke patients in dataset\")\nfor p in countplot_stroke.patches: \n    countplot_stroke.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.ylim(0, 6000)","f973972a":"x,y = 'stroke', 'gender'\n(df.groupby(x)[y].value_counts(normalize=True).mul(100).rename('percent').reset_index().pipe((sns.catplot,'data'), x=x,y='percent',hue=y,kind='bar'))","5d223ba1":"x,y = 'stroke', 'hypertension'\n(df.groupby(x)[y].value_counts(normalize=True).mul(100).rename('percent').reset_index().pipe((sns.catplot,'data'), x=x,y='percent',hue=y,kind='bar'))","1fc86670":"x,y = 'stroke', 'heart_disease'\n(df.groupby(x)[y].value_counts(normalize=True).mul(100).rename('percent').reset_index().pipe((sns.catplot,'data'), x=x,y='percent',hue=y,kind='bar'))","3e13fe8e":"x,y = 'stroke', 'work_type'\n(df.groupby(x)[y].value_counts(normalize=True).mul(100).rename('percent').reset_index().pipe((sns.catplot,'data'), x=x,y='percent',hue=y,kind='bar'))","f29df812":"x,y = 'stroke', 'Residence_type'\n(df.groupby(x)[y].value_counts(normalize=True).mul(100).rename('percent').reset_index().pipe((sns.catplot,'data'), x=x,y='percent',hue=y,kind='bar'))","03096b2a":"x,y = 'stroke', 'smoking_status'\n(df.groupby(x)[y].value_counts(normalize=True).mul(100).rename('percent').reset_index().pipe((sns.catplot,'data'), x=x,y='percent',hue=y,kind='bar'))","55fd165f":"plt.title(\"Age distribution of stroke and no stroke patients\")\nsns.kdeplot(df['age'], data = df, hue = 'stroke', fill=True)","cd866869":"ax = sns.boxplot(x=\"ever_married\", y=\"age\", data=df)","32a05fcc":"plt.title(\"Age distribution and marital status\")\nsns.kdeplot(df['age'], data = df, hue = 'ever_married', fill=True)","e3440dee":"ax = sns.boxplot(x=\"stroke\", y=\"bmi\", data=df)","e8a3c182":"ax = sns.boxplot(x=\"stroke\", y=\"avg_glucose_level\", data=df)","2d0b9592":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib\ndf_corr = df.drop(columns=['id', 'hypertension', 'heart_disease']).select_dtypes(include=np.number)\ndf_corr = df_corr.corr()\nplt.figure(figsize=(7,7))\nsns.heatmap(df_corr,annot=True)","1bb19d09":"df.drop('id', axis='columns', inplace=True)","7617737a":"df['stroke'].value_counts()","d0421659":"from sklearn.utils import resample\n\n#Upsampling minority class: stroke = 1\ndf_majority = df[df['stroke']==0]\ndf_minority = df[df['stroke']==1]\n\ndf_minority_oversampled = resample(df_minority, replace = True, n_samples=4700, random_state=21)\n\ndf_oversampled = pd.concat([df_majority, df_minority_oversampled])\n\ndf_oversampled['stroke'].value_counts()","c8a1a998":"def create_dummies(df,column_name):\n    dummies = pd.get_dummies(df[column_name],prefix=column_name)\n    df = pd.concat([df,dummies],axis=1)\n    return df","956e46da":"df_oversampled = create_dummies(df_oversampled,\"gender\")\ndf_oversampled = create_dummies(df_oversampled,\"ever_married\")\ndf_oversampled = create_dummies(df_oversampled,\"work_type\")\ndf_oversampled = create_dummies(df_oversampled,\"Residence_type\")\ndf_oversampled = create_dummies(df_oversampled,\"smoking_status\")\ndf_oversampled.head()\n","a7e266a8":"df_oversampled.columns","3facf654":"columns = ['age', 'hypertension', 'heart_disease', 'avg_glucose_level', 'bmi',\n       'gender_Female', 'gender_Male', 'gender_Other',\n       'ever_married_No', 'ever_married_Yes', 'work_type_Govt_job',\n       'work_type_Never_worked', 'work_type_Private',\n       'work_type_Self-employed', 'work_type_children', 'Residence_type_Rural',\n       'Residence_type_Urban', 'smoking_status_Unknown',\n       'smoking_status_formerly smoked', 'smoking_status_never smoked',\n       'smoking_status_smokes']\n\nX = df_oversampled[columns]\ny = df_oversampled['stroke']\n","201bc252":"from sklearn.model_selection import train_test_split\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3,random_state=0)","6ded15f6":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_scaled = sc.fit_transform(train_X)\nX_test_scaled = sc.transform (test_X)\n","10ec7cb5":"from sklearn.linear_model import LogisticRegression\nlr_1 = LogisticRegression()\nlr_1.fit(X_train_scaled, train_y)\nprediction_lr_1 = lr_1.predict(X_test_scaled)\n","d7a435c5":"from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nprint(confusion_matrix(test_y, prediction_lr_1))\nprint(classification_report(test_y, prediction_lr_1))","dec4d9c7":"roc_lr_1 = roc_auc_score(test_y, prediction_lr_1) \nroc_lr_1","c5d24733":"from sklearn.model_selection import RandomizedSearchCV\n\nparam_grid_lr = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'penalty': ['l1', 'l2'], 'max_iter': list(range(100,800,100)), 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n\nlr_2 = LogisticRegression()\nlr_2_model = RandomizedSearchCV(lr_2, param_grid_lr, cv = 5)\nlr_2_model.fit(X_train_scaled, train_y)\nprediction_lr_2 = lr_2_model.best_estimator_.predict(X_test_scaled)\n\nprint(\"Tuned Logistic Regression Parameters: {}\".format(lr_2_model.best_params_)) \nprint(\"Best score is {}\".format(lr_2_model.best_score_))\n\nprint(confusion_matrix(test_y,prediction_lr_2))\nprint(classification_report(test_y,prediction_lr_2))\n\n","03fe733e":"roc_lr_2 = roc_auc_score(test_y, prediction_lr_2) \nroc_lr_2","d1271fdf":"from sklearn.tree import DecisionTreeClassifier \nfrom sklearn import metrics \n\ndtc_1 = DecisionTreeClassifier()\ndtc_1 = dtc_1.fit(train_X, train_y)\nprediction_dtc_1 = dtc_1.predict(test_X)","9c7940a9":"from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(test_y, prediction_dtc_1))\nprint(classification_report(test_y, prediction_dtc_1))","805c60e8":"roc_dtc_1 = roc_auc_score(test_y, prediction_dtc_1) \nroc_dtc_1","96f7febe":"from scipy.stats import randint\nparam_grid_dtc = {\"max_depth\": [3,None], \"max_features\":randint(1,5), \"min_samples_leaf\":randint(1,9), \"criterion\": [\"gini\", \"entropy\"]}\ndtc_2 = DecisionTreeClassifier()\ndtc_2_model = RandomizedSearchCV(dtc_2, param_grid_dtc, cv = 5)\ndtc_2_model.fit(train_X, train_y)\nprediction_dtc_2 = dtc_2_model.best_estimator_.predict(test_X)\n\nprint(\"Tuned Decision Tree Parameters: {}\".format(dtc_2_model.best_params_)) \nprint(\"Best score is {}\".format(dtc_2_model.best_score_))\n\nprint(confusion_matrix(test_y, prediction_dtc_2))\nprint(classification_report(test_y,prediction_dtc_2))\n","642cee66":"roc_dtc_2 = roc_auc_score(test_y, prediction_dtc_2) \nroc_dtc_2","ba88cc23":"from sklearn.ensemble import RandomForestClassifier\nrfc_1=RandomForestClassifier()\nrfc_1.fit(train_X, train_y)\nprediction_rfc_1=rfc_1.predict(test_X)\n\nprint(confusion_matrix(test_y, prediction_rfc_1))\nprint(classification_report(test_y, prediction_rfc_1))\n","2e9eaa6d":"roc_rfc_1 = roc_auc_score(test_y, prediction_rfc_1) \nroc_rfc_1","9f87d934":"param_grid_rfc = {'bootstrap': [True, False], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'max_features': ['auto', 'sqrt'], 'min_samples_leaf': [1, 2, 4], 'min_samples_split': [2, 5, 10], 'n_estimators': [130, 180, 230]}\nrfc_2 =RandomForestClassifier()\nrfc_2_model = RandomizedSearchCV(rfc_2, param_grid_rfc, cv = 5)\nrfc_2_model.fit(train_X, train_y)\nprediction_rfc_2 = rfc_2_model.best_estimator_.predict(test_X)\n\nprint(\"Tuned RFC Parameters: {}\".format(rfc_2_model.best_params_)) \nprint(\"Best score is {}\".format(rfc_2_model.best_score_))\n\nprint(confusion_matrix(test_y, prediction_rfc_2))\nprint(classification_report(test_y,prediction_rfc_2))","6a3ba1fa":"roc_rfc_2 = roc_auc_score(test_y, prediction_rfc_2) \nroc_rfc_2","29f8d71e":"from sklearn import svm\nsvc_1 = svm.SVC()\nsvc_1.fit(X_train_scaled, train_y)\nprediction_svc_1 = svc_1.predict(X_test_scaled)\n\nprint(confusion_matrix(test_y, prediction_svc_1))\nprint(classification_report(test_y,prediction_svc_1))","f5a9880d":"roc_svc_1 = roc_auc_score(test_y, prediction_svc_1) \nroc_svc_1","8ee8c4ae":"param_grid_svc = {'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001]} \n\nsvc_2 = svm.SVC()\nsvc_2_model = RandomizedSearchCV(svc_2, param_grid_svc, cv = 5)\nsvc_2_model.fit(X_train_scaled, train_y)\nprediction_svc_2 = svc_2_model.best_estimator_.predict(X_test_scaled)\n\nprint(\"Tuned RFC Parameters: {}\".format(svc_2_model.best_params_)) \nprint(\"Best score is {}\".format(svc_2_model.best_score_))\n\nprint(confusion_matrix(test_y, prediction_svc_2))\nprint(classification_report(test_y,prediction_svc_2))","04f7ac3c":"roc_svc_2 = roc_auc_score(test_y, prediction_svc_2) \nroc_svc_2","53b15add":"from sklearn.neighbors import KNeighborsClassifier\nknn_1 = KNeighborsClassifier()\nknn_1.fit(X_train_scaled, train_y)\nprediction_knn_1 = knn_1.predict(X_test_scaled)\n\nprint(confusion_matrix(test_y, prediction_knn_1))\nprint(classification_report(test_y,prediction_knn_1))","39dad8ba":"roc_knn_1 = roc_auc_score(test_y, prediction_knn_1) \nroc_knn_1","cdd4b3df":"param_grid_knn = {'n_neighbors': [3,5,11, 13, 15, 17, 19], 'weights': ['uniform', 'distance'], 'metric': ['euclidean', 'manhatten']}\nknn_2 = KNeighborsClassifier()\nknn_2_model = RandomizedSearchCV(knn_2, param_grid_knn, cv = 5)\nknn_2_model.fit(X_train_scaled, train_y)\nprediction_knn_2 = knn_2_model.best_estimator_.predict(X_test_scaled)\n\nprint(\"Tuned KNN Parameters: {}\".format(knn_2_model.best_params_)) \nprint(\"Best score is {}\".format(knn_2_model.best_score_))\n\nprint(confusion_matrix(test_y, prediction_knn_2))\nprint(classification_report(test_y,prediction_knn_2))","cea717dd":"roc_knn_2 = roc_auc_score(test_y, prediction_knn_2) \nroc_knn_2","36073699":"from sklearn.ensemble import GradientBoostingClassifier\ngbc_1 = GradientBoostingClassifier()\ngbc_1.fit(train_X, train_y)\nprediction_gbc_1 = gbc_1.predict(test_X)\n\nprint(confusion_matrix(test_y, prediction_gbc_1))\nprint(classification_report(test_y,prediction_gbc_1))","621fd408":"roc_gbc_1 = roc_auc_score(test_y, prediction_gbc_1) \nroc_gbc_1","d2ddb753":"param_grid_gbc = {'n_estimators':[10, 100, 1000], 'learning_rate': [0.001, 0.01, 0.1], 'subsample': [0.5, 0.7, 1.0], 'max_depth': [3, 7, 9]}\ngbc_2 = GradientBoostingClassifier()\ngbc_2_model = RandomizedSearchCV(gbc_2, param_grid_gbc, cv = 5)\ngbc_2_model.fit(train_X, train_y)\nprediction_gbc_2 = gbc_2_model.best_estimator_.predict(test_X)\n\nprint(\"Tuned GBC Parameters: {}\".format(gbc_2_model.best_params_)) \nprint(\"Best score is {}\".format(gbc_2_model.best_score_))\n\nprint(confusion_matrix(test_y, prediction_gbc_2))\nprint(classification_report(test_y,prediction_gbc_2))","6f7d7db5":"roc_gbc_2 = roc_auc_score(test_y, prediction_gbc_2) \nroc_gbc_2","c3101473":"from sklearn.ensemble import RandomForestClassifier\nrfc_best=RandomForestClassifier(n_estimators = 130, min_samples_split = 2, min_samples_leaf = 1, max_features = 'sqrt', max_depth = 20, bootstrap = False)\nrfc_best.fit(X_train_scaled, train_y)\nprediction_rfc_best=rfc_best.predict(X_test_scaled)\n\nprint(confusion_matrix(test_y, prediction_rfc_best))\nprint(classification_report(test_y, prediction_rfc_best))","b72d33de":"roc_rfc_best = roc_auc_score(test_y, prediction_rfc_best) \nroc_rfc_best","88177b43":"feature_importances = pd.DataFrame(rfc_best.feature_importances_, index = X.columns, columns=['importance']).sort_values('importance', ascending=False)\nfeature_importances","0a8bbaa0":"feature_importances.plot(kind='bar')","a173ea5d":"### K-nearest neighbor (sensitive to range)","4e017efd":"### Split dataset into trainingset and testset ","9083e77b":"### Age and stroke","91800a29":"Evaluating feature importance in the random forest model on original data. ","924159ee":"The dataset is imbalanced with only few stroke patients. This needs to be addressed during data preprocessing before building the predictive models.","ea6a0854":"### Gradient Boosting Classifier (not sensitive to range of data)","b4bdab26":"Feature \"bmi\" contains 201 null values. This is only a few values and can therefore be dropped from the dataset.","2a65a347":"* age correlates with bmi and hypertension, avg_glucose_level \n* stroke correlates weakly with age and average glucose level\n","f4479e29":"* resident type does not seem to be a relevant influential factor for stroke in this cohort of patients","f5a5af37":"Difficult to draw conclusions due to large number of \"Unknown\" and low number of stroke patients in dataset:\n* former smokers more prevalent in stroke patients","20e172c1":"## Residence type of stroke patients","fde7ab2e":"Age, average glucose level and bmi are the most important features for the random forest classifier model.","567f4a0a":"* average glucose level higher in stroke patients","6f85b098":"To address the imbalance within this dataset, oversampling will be used to increase the size of  the minority class stroke.","cea5ba9c":"### Addressing imbalance within dataset of classes stroke \/ no stroke","90bc9337":"### BMI and stroke","324f2983":"### Glucose level and stroke","d1516f55":"### Pearson correlation matrix of dichotomous categorical variable stroke and a continuous variables age, bmi and average glucose level","871a2744":"Trying to improve performance of the knn model using hyperparameter tuning.","c74c64ee":"* approx. 800 more female patients than male patients in the dataset\n* ratio female\/male is smaller in stroke patients vs. non-stroke patients, suggesting that stroke is more prevalent in male patients.\n\n","0222045a":"### Best performing model","65afc11e":"## Data preprocessing","f58271d5":"### Support vector machine (sensitive to range of data)","efbc8c27":"This is the list of columns inlcuding the new columns created during encoding. ","b80bcc6b":"Fit the model with scaled data and default setting and get performance results.","20395daf":"Fit the model with default settings.","aa0eb8fa":"* Stroke\u00a0is a\u00a0medical condition\u00a0in which poor\u00a0blood flow\u00a0to the\u00a0brain\u00a0causes\u00a0cell death. Sudden bleeding in the brain can also cause a stroke if it damages brain cells. (1)\n* There are over 13.7 million new strokes each year 3. Globally, one in four people over age 25 will have a stroke in their lifetime. (2) \n* Worldwide, cerebrovascular accidents (stroke) are the second leading cause of death and the third leading cause of disability. (3)\n* Each year, 52% of all strokes occur in men and 48% in women. Metabolic factors (high systolic blood pressure, high BMI, high fasting plasma glucose, high total cholesterol, and   low glomerular filtration rate), as well as behavioural factors (smoking, poor diet, and low physical activity) are risk factors for stroke. (2) \n","6c7b5db2":"Trying to improve the performance of the logistic regression using hyperparameter tuning. RandomSearchCV uses randomized search over parameters from the parameter grid.","afd3ac76":"## Hypertension in stroke patients","2a238ef9":"* stroke is more prevalent in patients ever married","5b795120":"## Heart disease in stroke patients","8e75b8a8":"### Logistic regression (sensitive to range of data)","80318ebd":"The dataset will be transformed using Standard Scaler to achieve distribution with a mean value of 0 and standard deviation of 1 to prepare data for models that are sensitive to range, like like logistic regression, support vector machine and  k-nearest neighbor.","a78a4626":"Hyperparameter tuning did not improve the performance of the logistic regression model.","35e71fbd":"### Feature scaling to account for models sensitive to range of data","3fe9ef81":"Feature will be dropped, because it has no predictive value.","5da28247":"* dataset with good age distribution\n* stroke is more frequent in older age","b35baf17":"Trying to improve the model using hyperparameter tuning.","5bf6b20c":"Fit the model with default settings and get performance results.","03d2c4bb":"* heart disease more prevalent in stroke patients compared to no stroke patients","a7a9482b":"Trying to improve performance of decision tree model using hyperparametertuning with RandomSearchCV.","eb9fb737":"### Gender of stroke and no stroke patients","24f46f1c":"Difficult to draw conclusions due to small number of stroke patients:\n* class \"children\" less prevalent in stroke patients\n* work type \"private\" and \"self-employed\" more prevalent in stroke patients","b899bf9c":"The dataset is split into a training set (70%) to train the models and test set (30%) to evaluate the performance of the models.","2dda38d8":"# Predicting stroke \u00b6","88a2d149":"## EDA","3986a53a":"### Random Forest Classifier (not sensitive to range of data)","bbfdf3c6":"* hypertension more prevalent in stroke patients compared to no stroke patients","9e0266bf":"## Conclusions\n\nThe random forest classifier resulted in the most accurate prediction of stroke with highest recall and precision:\n* using hyperparameters 'n_estimators': 230, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 60, 'bootstrap': False \n* and age, average glucose level and bmi being the most important features\n\nInterestingly, point-biserial correlation coefficient of bmi and stroke was very low.\n\nHyperparameter tuning using RandomSearchCV resulted:\n* in better performance for Random Forest Classifier, Support Vector Machine, K-nearest neighbor and Gradient Boosting classifier\n* same performance for Logistic Regression\n* worse performance for Decision Tree Classifier\n\n","b28f8a12":"## Aim\nBuild a model to predict whether a patient is likely to get stroke based on the parameters: id, gender, age, hypertension, heart_disease, ever_married, work_type, residence_type, avg_glucose_level, bmi, smoking_status. This is a binary classification problem.\n","6a836bab":"### Proportion of stroke patients in dataset","e7154e5d":"### Smoking status and stroke","dfef180c":"Fit the model with scaled data and default settings and get performance results.","82303e26":"Trying to improve performance of the random forest model using hyperparametertuning with RandomSearchCV. Trying to improve the model with hyperparameter tuning.\n\n","6a4b8d67":"## Work type of stroke patients","0c74ac4d":"Hyperparameter tuning slightly improved the performance of the decision tree classifier model.","94549d85":"### Get to know the dataset","e23fd6ef":"Get performance results of the decision tree model","4a9720b0":"Hyperparameter tuning improved the performance of the support vector machine model.","5fd9979c":"### Define target variable","56109d02":"Machine learning models require input and output variables to be numeric. The categorical features in this dataset will therefore be encoded to numbers before fitting and evaluating a model.","21f9e859":"* stroke is more prevalent in patients ever married, but  ever married patients are also older contributing to an increased risk for stroke","c00e6930":"Fit the model with scaled data and default settings.","dd2b7e10":"Get performance results of the logistic regression model.","da8e00dd":"Hyperparameter tuning improved the performance of the random forest classifier.","f9d62545":"### Marital status of stroke patients","e7df2847":"### Remove null values from dataset","761d5218":"Hyperparameter tuning improved the k-nearest neighbor model.","e27140dc":"### Dropping feature 'id\"","6d316e11":"This is a binary classification problem and the classifier is supposed to output a stroke or no stroke event.","0adf6ef8":"Hyperparameter tuning improved the gradient boosting model.","db223a77":"### Evaluation and importance of features for best performing model (Random Forest Classfier)","71697562":"## Resources\n1. NIH: National Heart, Lung and Blood institute: https:\/\/www.nhlbi.nih.gov\/health-topics\/stroke\n2. World Stroke Organization (WSO): Global Stroke Fact Sheet 2019: https:\/\/www.world-stroke.org\/assets\/downloads\/WSO_Fact-sheet_15.01.2020.pdf\n3. Bulletin of the World Health Organization 2016;94:634-634A. doi: http:\/\/dx.doi.org\/10.2471\/BLT.16.181636\n","9371209b":"Both, random forest classifier and gradient boosting classifier performed very well. Hyperparameter tuning further improved the performance of both models, with random forest classifier providing the best performance of predicting stroke. ","43a5111b":"### Deal with categorical data","5309f66c":"* bmi of stroke patients is slightly higher compared to no stroke patients\n* no stroke class with more outliers ","abee32e2":"### Decision tree classifier (not sensitive to range of data)"}}