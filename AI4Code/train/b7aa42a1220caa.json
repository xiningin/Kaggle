{"cell_type":{"fb0a8b57":"code","74613adf":"code","1ea0fc5d":"code","4809ecba":"code","5b6c2150":"code","f799042b":"code","b9b0d09b":"code","0d79148d":"code","ba23a908":"code","fbab4a72":"code","ec2e326d":"code","1ebdf931":"code","a25c3496":"code","d750cd82":"code","26eb4bbe":"markdown","3c4c72dc":"markdown","4b9fa5ee":"markdown","a01134ef":"markdown","9de18f93":"markdown"},"source":{"fb0a8b57":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","74613adf":"# Step 1: get the data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nCalendarDF=pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv\", header=0)\nPricesDF=pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv\", header=0)\nSalesDF=pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv\", header=0)\nSubmissionDF=pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv\", header=0)","1ea0fc5d":"CalendarDF.info()","4809ecba":"PricesDF.info()","5b6c2150":"SalesDF.info()","f799042b":"SubmissionDF.info()","b9b0d09b":"import os, psutil  \n\npid = os.getpid()\npy = psutil.Process(pid)\nmemory_use = py.memory_info()[0] \/ 2. ** 30\nprint ('memory GB:' + str(np.round(memory_use, 2)))","0d79148d":"del PricesDF\ndel SubmissionDF\n\nimport gc\ngc.collect()","ba23a908":"TX_1_Sales = SalesDF[['TX_1' in x for x in SalesDF['store_id'].values]]\nTX_1_Sales = TX_1_Sales.reset_index(drop = True)\nTX_1_Sales.info()","fbab4a72":"# Generate MultiIndex for easier aggregration.\nTX_1_Indexed = pd.DataFrame(TX_1_Sales.groupby(by = ['cat_id','dept_id','item_id']).sum())\nTX_1_Indexed.info()","ec2e326d":"# Aggregate total sales per day for each sales category\nFood = pd.DataFrame(TX_1_Indexed.xs('FOODS').sum(axis = 0))\nHobbies = pd.DataFrame(TX_1_Indexed.xs('HOBBIES').sum(axis = 0))\nHousehold = pd.DataFrame(TX_1_Indexed.xs('HOUSEHOLD').sum(axis = 0))\nFood.info()","1ebdf931":"## Merge the aggregated sales data to the calendar dataframe based on date\nCalendarDF = CalendarDF.merge(Food, how = 'left', left_on = 'd', right_on = Food.index)\nCalendarDF = CalendarDF.rename(columns = {0:'Food'})\nCalendarDF = CalendarDF.merge(Hobbies, how = 'left', left_on = 'd', right_on = Hobbies.index)\nCalendarDF = CalendarDF.rename(columns = {0:'Hobbies'})\nCalendarDF = CalendarDF.merge(Household, how = 'left', left_on = 'd', right_on = Household.index)\nCalendarDF = CalendarDF.rename(columns = {0:'Household'})\nCalendarDF.head(10)","a25c3496":"# Store a copy of the new data frame for my future use. Delete the SalesDF to save memory usage.\nCalendarDF.to_csv(\"CalendarDF.csv\", index = False)\ndel SalesDF\ngc.collect()\nmemory_use = py.memory_info()[0] \/ 2. ** 30\nprint ('memory GB:' + str(np.round(memory_use, 2)))","d750cd82":"import matplotlib.pyplot as plt\nplt.style.use('bmh')\n\nCalendarDF['date'] = pd.to_datetime(CalendarDF.date)\n## graph daily sales data for each year \nis_2011 = CalendarDF.year == 2011\nis_2012 = CalendarDF.year == 2012\nis_2013 = CalendarDF.year == 2013\nis_2014 = CalendarDF.year == 2014\nis_2015 = CalendarDF.year == 2015\nis_2016 = CalendarDF.year == 2016\n\ntemp = CalendarDF[is_2011]\ntemp2 = CalendarDF[is_2012]\ntemp3 = CalendarDF[is_2013]\ntemp4 = CalendarDF[is_2014]\ntemp5 = CalendarDF[is_2015]\ntemp6 = CalendarDF[is_2016]\n\nfig, axs = plt.subplots(3 , 2, figsize = (14,10))\naxs[0,0].title.set_text('2011')\naxs[0 , 1].title.set_text('2012')\naxs[1 , 0].title.set_text('2013')\naxs[1 , 1].title.set_text('2014')\naxs[2 , 0].title.set_text('2015')\naxs[2 , 1].title.set_text('2016')\n\naxs[0,0].plot(temp.date, temp.Food)\naxs[0,0].plot(temp.date, temp.Hobbies)\naxs[0,0].plot(temp.date, temp.Household)\n\ndf = temp[temp.event_name_1.notnull()]\ndf.reset_index(drop=True)\n\nl1 = list(df['date'])\nl2 = list(df['event_name_1'])\n\nstart, end = axs[0,0].get_ylim()[0], axs[0,0].get_ylim()[1]\n\naxs[0,0].vlines(l1, start, end, linestyles = '--', color = 'r', alpha = .5)\nfor i in range(0,len(l2)):\n    if l2[i] is not None:\n       axs[0,0].text(l1[i],end,l2[i])\n        \naxs[0,1].plot(temp2.date, temp2.Food)\naxs[0,1].plot(temp2.date, temp2.Hobbies)\naxs[0,1].plot(temp2.date, temp2.Household)\n\naxs[1,0].plot(temp3.date, temp3.Food)\naxs[1,0].plot(temp3.date, temp3.Hobbies)\naxs[1,0].plot(temp3.date, temp3.Household)\n\naxs[1,1].plot(temp4.date, temp4.Food)\naxs[1,1].plot(temp4.date, temp4.Hobbies)\naxs[1,1].plot(temp4.date, temp4.Household)\n\naxs[2,0].plot(temp5.date, temp5.Food)\naxs[2,0].plot(temp5.date, temp5.Hobbies)\naxs[2,0].plot(temp5.date, temp5.Household)\n\naxs[2,1].plot(temp6.date, temp6.Food)\naxs[2,1].plot(temp6.date, temp6.Hobbies)\naxs[2,1].plot(temp6.date, temp6.Household)\n\nplt.show()","26eb4bbe":"# Prepare my dataset for TX_1 and aggregate to the product category level","3c4c72dc":"# Load the data and take a glance at the data","4b9fa5ee":"# M5 Forecast Accuracy Research\n\nThe M5 Forecasting competition provides a real life data set to perform business analysis and forecast research. My goal, as a Stern freshman student, is to learn the various time series forecasting algorithms, and compare their accuracy in different types of use cases, similar to the M4 research published here. https:\/\/mofc.unic.ac.cy\/m4\/.\n\nMany thanks to Professor Makridakis and the MOFC team for making the data easily available and pulling the talents from all over the world to explore and develop the wonderful world of telling the future!\n\nDuring the next month, I plan to compare the following commonly used forecasting algorithms, using the Root Mean Squared Scaled Error (RMSSE) as the measurement:\n\n* Autoregressive Integrated Moving Average (ARIMA)\n* Seasonal ARIMA (SARIMA)\n* Seasonal ARIMA with Excogenous Regressors (SARIMAX)\n* Simple Exponential Smoothing (SES)\n* Holt Winter's Exponential Smoothing (Holt)\n\nThe algorithms are explained in this Jason Brownlee here https:\/\/machinelearningmastery.com\/time-series-forecasting-methods-in-python-cheat-sheet\/\n\nAs my goal is not to compete, but to learn and compare the algorithms, I plan to analyze only one store (TX_1) and aggregate at the product category level (Food, Hobbies and Household). I will skip the price dataset but will see how the events helps the forecast accuracy. ","a01134ef":"# I don't plan to use the price data. Deleting the PriceDF and SubmissionDF.","9de18f93":"# Do some graphing to look at the data.\nI referenced this notebook for the graphing. https:\/\/www.kaggle.com\/risheshg\/m5-accuracy-starter-data-exploration. There are a lot more to learn in plotting the data. But for now, I will pause.\n\nFor next week, I plan to start use the CalendarDF to do time-series analysis. "}}