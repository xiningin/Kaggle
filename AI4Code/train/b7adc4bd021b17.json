{"cell_type":{"71c1d76a":"code","c8a9ca18":"code","03524122":"code","46d280bf":"code","555d07e6":"code","cbcc5cb3":"code","b29282b9":"code","c4328f2e":"code","c70779f0":"code","b8857ee9":"code","bf94062a":"code","41b0c2c4":"code","b73720b8":"code","7326e0ba":"code","04ed72a4":"code","0654f397":"code","b1030111":"code","8c5d05cc":"code","531db4b1":"code","8af34067":"code","0196545f":"code","ddf0925f":"code","0f3be469":"code","c3226ef1":"code","93edbaeb":"code","64dd9c03":"code","19739c69":"code","5017412c":"code","ee771448":"code","717f7cf5":"code","34e5b18b":"code","49ea5251":"code","f3fefd6d":"code","9fc8c83f":"code","a99e2fbd":"code","a3e82390":"code","586cd84e":"code","79a1cbb3":"code","c7b36acf":"code","faadb0ac":"code","9ce3c13a":"code","868bc606":"code","9f6ad28b":"code","110a6515":"code","2df7811f":"code","00946a5a":"code","4a8e0548":"code","58324a90":"code","0f569be2":"code","cf4e1ee4":"code","e2350f9f":"code","30d73a11":"code","926e9cb7":"code","3016a2be":"code","b02ab754":"code","aa3a3273":"code","08e99cf9":"code","bb261774":"code","faef862c":"code","10a51105":"code","a2d320c4":"code","c9f1a6c7":"code","8be90301":"code","45294e42":"code","81f2cfb4":"code","ea33155a":"code","2edd90ae":"code","e89a462e":"code","59117983":"markdown","0eb010d8":"markdown","ea2e249b":"markdown","7f93c06b":"markdown","ae72ff57":"markdown","2f7af8be":"markdown","472a9ed5":"markdown","50b75011":"markdown","10a00e61":"markdown","89e417d6":"markdown","de9934e5":"markdown","63cfe27c":"markdown","5d3b3aaf":"markdown","d662390f":"markdown","66b9468b":"markdown","1b0fac1e":"markdown","1899d156":"markdown","13058578":"markdown","1b234be4":"markdown","052b9843":"markdown","1376858b":"markdown","31f61fbe":"markdown","5ec56b24":"markdown","f4e519e5":"markdown","ee1e0b69":"markdown","47fa1f2f":"markdown","f3002383":"markdown","de0452ed":"markdown","4d804473":"markdown","42b956a5":"markdown","6c36ace8":"markdown","c1ec0556":"markdown","670cfaab":"markdown","9f2f549c":"markdown","4ea73555":"markdown","e8eea8e0":"markdown","6fb48fd3":"markdown","6afede04":"markdown","e59e27a1":"markdown","4d6aa98c":"markdown","f6cf577e":"markdown","7040f260":"markdown","5774e060":"markdown","a74fc68a":"markdown","c86eff23":"markdown","ad040466":"markdown","3ff4ed73":"markdown","8e8733a8":"markdown","369b999f":"markdown"},"source":{"71c1d76a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.utils import resample\n# Data Feature Selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n# Data Splitting\nfrom sklearn.model_selection import train_test_split\n# Data Scaling\nfrom sklearn.preprocessing import MinMaxScaler\n#Data Modelling\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, classification_report\n# Hyperparameter Tuning\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n#Ensembling\nfrom mlxtend.classifier import StackingCVClassifier\n","c8a9ca18":"#data overview\ndf_data = pd.read_csv(r\"..\/input\/heart-diseases\/datasets_4123_6408_framingham.csv\")\ndf_data.head(20)","03524122":"df_data.shape","46d280bf":"df_data.info()","555d07e6":"df_data.isnull().sum()","cbcc5cb3":"df_data.duplicated().sum()","b29282b9":"print((df_data[\"glucose\"].mode())[0])","c4328f2e":"df_data[\"glucose\"].fillna((df_data[\"glucose\"].mode())[0], inplace=True)","c70779f0":"df_data.dropna(inplace=True)\ndf_data.isnull().sum()","b8857ee9":"plt.figure(figsize=(40,15), facecolor='w')\nsns.boxplot(data=df_data)\nplt.show()","bf94062a":"df_data['totChol'].max()","41b0c2c4":"df_data['sysBP'].max()","b73720b8":"df_data = df_data[df_data['totChol']<600.0]\ndf_data = df_data[df_data['sysBP']<295.0]\ndf_data.shape","7326e0ba":"df_data.describe()","04ed72a4":"#Checking relationship between variables\ncor=df_data.corr()\nplt.figure(figsize=(15,15), facecolor='w')\nsns.heatmap(cor,xticklabels=cor.columns,yticklabels=cor.columns,annot=True, cmap ='Blues' )\nplt.title(\"Correlation among all the Variables of the Dataset\", size=10)\ncor","0654f397":"categorical_features = ['age', 'education', 'currentSmoker', 'BPMeds', 'prevalentStroke', 'prevalentHyp', 'diabetes']","b1030111":"for feature in categorical_features:\n    print(feature,':')\n    print(df_data[feature].value_counts())\n    print(\"-----------------\")","8c5d05cc":"num_plots = len(categorical_features)\ntotal_cols = 2\ntotal_rows = num_plots\/\/total_cols + 1\nfig, axs = plt.subplots(nrows=total_rows, ncols=total_cols,\n                        figsize=(5*total_cols, 5*total_rows), facecolor='w', constrained_layout=True)\nfor i, var in enumerate(categorical_features):\n    row = i\/\/total_cols\n    pos = i % total_cols\n    plot = sns.countplot(x=var, data=df_data, ax=axs[row][pos])","531db4b1":"#numeric_features\nplt.figure(figsize=(23,15))\nplt.subplots_adjust(wspace=0.3, hspace=0.3)\n\nplt.subplot(2, 3, 1)\nsns.distplot(df_data['glucose'] , color='blue')\nplt.title('Distribution of Glucose')\n\nplt.subplot(2, 3, 2)\nsns.distplot(df_data['totChol'], color='orange')\nplt.title('Distribution of Total Cholesterol')\n\nplt.subplot(2, 3, 3)\nsns.distplot(df_data['sysBP'], color='r')\nplt.title('Distribution of Systolic BP')\n\nplt.subplot(2, 3, 4)\nsns.distplot(df_data['diaBP'] , color='purple')\nplt.title('Distribution of Dia. BP')\n\nplt.subplot(2, 3, 5)\nsns.distplot(df_data['BMI'], color='g')\nplt.title('Distribution of BMI')\n\nplt.subplot(2, 3, 6)\nsns.distplot(df_data['heartRate'], color='lime')\nplt.title('Distribution of HeartRate')","8af34067":"numeric_features = ['cigsPerDay', 'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']\nnum_plots = len(numeric_features)\ntotal_cols = 2\ntotal_rows = num_plots\/\/total_cols + 1\ncolor = ['v', 'i', 'b', 'g', 'y', 'o', 'r']\nfig, axs = plt.subplots(nrows=total_rows, ncols=total_cols,\n                        figsize=(7*total_cols, 7*total_rows), facecolor='w', constrained_layout=True)\nfor i, var in enumerate(numeric_features):\n    row = i\/\/total_cols\n    pos = i % total_cols\n    plot = sns.violinplot(y=var, data=df_data, ax=axs[row][pos], linewidth=3)","0196545f":"#Target variable\n#Distribution of outcome variable, Heart Disease\nplt.figure(figsize=(10, 8), facecolor='w')\nplt.subplots_adjust(right=1.5)\nplt.subplot(121)\nsns.countplot(x=\"TenYearCHD\", data=df_data)\nplt.title(\"Count distribution of TenYearCHD\", size=10)\nplt.subplot(122)\nlabels=[0,1]\nplt.pie(df_data[\"TenYearCHD\"].value_counts(),autopct=\"%1.1f%%\",labels=labels,colors=[\"green\",\"red\"])\nplt.show()","ddf0925f":"#Relationship between education and cigsPerDay\n#Grouping education and cigsPerDay\ngraph_1 = df_data.groupby(\"education\", as_index=False).cigsPerDay.mean()\nplt.figure(figsize=(10,8), facecolor='w')\nsns.regplot(x=graph_1[\"education\"], y=graph_1[\"cigsPerDay\"])\nplt.title(\"Graph showing cigsPerDay in every level of education.\", size=15)\nplt.xlabel(\"education\", size=15)\nplt.ylabel(\"cigsPerDay\", size=15)\nplt.xticks(size=10)\nplt.yticks(size=10)","0f3be469":"#checking for which gender has more risk of coronary heart disease CHD\ngraph_2 = df_data.groupby(\"age\", as_index=False).TenYearCHD.sum()\n#Ploting the above values\nplt.figure(figsize=(10,8), facecolor='w')\nsns.barplot(x=graph_2[\"age\"], y=graph_2[\"TenYearCHD\"])\nplt.title(\"Graph showing which gender has more risk of coronary heart disease CHD\", size=15)\nplt.xlabel(\"Gender\\n0 is female and 1 is male\",size=15)\nplt.ylabel(\"TenYearCHD cases\", size=15)\nplt.xticks(size=10)\nplt.yticks(size=10)","c3226ef1":"#Distribution of current smokers with respect to age\nplt.figure(figsize=(20,10), facecolor='w')\nsns.countplot(x=\"age\",data=df_data,hue=\"currentSmoker\")\nplt.title(\"Graph showing which age group has more smokers.\", size=15)\nplt.xlabel(\"age\", size=15)\nplt.ylabel(\"age Count\", size=15)\nplt.xticks(size=10)\nplt.yticks(size=10)","93edbaeb":"#Relation between cigsPerDay and risk of coronary heart disease.\nplt.figure(figsize=(30,12), facecolor='w')\nsns.countplot(x=\"TenYearCHD\",data=df_data,hue=\"cigsPerDay\")\nplt.legend(title='cigsPerDay', fontsize='large')\nplt.title(\"Graph showing the relation between cigsPerDay and risk of coronary heart disease.\", size=15)\nplt.xlabel(\"Risk of TenYearCHD\", size=15)\nplt.ylabel(\"Count of TenYearCHD\", size=15)\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.show()","64dd9c03":"#Relation between sysBP and risk of CHD\n# Grouping up the data and ploting it\ngraph_3 = df_data.groupby(\"TenYearCHD\", as_index=False).sysBP.mean()\n\nplt.figure(figsize=(10,8), facecolor='w')\nsns.barplot(x=graph_3[\"TenYearCHD\"], y=graph_3[\"sysBP\"])\nplt.title(\"Graph showing the relation between sysBP and risk of CHD\", size=15)\nplt.xlabel(\"Risk of CHD\", size=15)\nplt.ylabel(\"sysBP\", size=15)\nplt.xticks(size=10)\nplt.yticks(size=10)","19739c69":"plt.figure(figsize=(10,8), facecolor='w')\nsns.regplot(x=graph_3[\"TenYearCHD\"], y=graph_3[\"sysBP\"])\nplt.title(\"Distribution of sysBP in relation to the risk of CHD\", size=15)\nplt.xticks(size=10)\nplt.yticks(size=10)","5017412c":"# Grouping up the data and ploting it\n# Relation between diaBP and risk of CHD\ngraph_4 = df_data.groupby(\"TenYearCHD\", as_index=False).diaBP.mean()\n\nplt.figure(figsize=(12,8), facecolor='w')\nsns.barplot(x=graph_4[\"TenYearCHD\"], y=graph_4[\"diaBP\"])\nplt.title(\"Graph showing the relation between diaBP and risk of CHD\", size=15)\nplt.xlabel(\"Risk of CHD\", size=15)\nplt.ylabel(\"diaBP\", size=15)\nplt.xticks(size=10)\nplt.yticks(size=10)","ee771448":"plt.figure(figsize=(10,8), facecolor='w')\nsns.regplot(x=graph_4[\"TenYearCHD\"], y=graph_4[\"diaBP\"])\nplt.title(\"Distribution of diaBP in relation to the risk of CHD\", size=15)\nplt.xticks(size=10)\nplt.yticks(size=10)","717f7cf5":"#elation between age and totChol\ngraph_5 = df_data.groupby(\"TenYearCHD\", as_index=False).totChol.mean()\n\nplt.figure(figsize=(10,8), facecolor='w')\nsns.barplot(x=graph_5[\"TenYearCHD\"], y=graph_5[\"totChol\"])\nplt.title(\"Graph showing the relation between age and totChol\", size=15)\nplt.xlabel(\"age\", size=15)\nplt.ylabel(\"totChol\", size=15)\nplt.xticks(size=10)\nplt.yticks(size=10)","34e5b18b":"plt.figure(figsize=(10,8), facecolor='w')\nsns.regplot(x=graph_5[\"TenYearCHD\"], y=graph_5[\"totChol\"])\nplt.title(\"Distribution of age with respect to totChol\", size=15)\nplt.xticks(size=10)\nplt.yticks(size=10)","49ea5251":"#Relationship between age and cigsPerDay, totChol, glucose.\nplt.figure(figsize=(20,10), facecolor='w')\nsns.boxplot(x=\"age\",y=\"totChol\",data=df_data)\nplt.title(\"Distribution of age with respect to totChol\", size=20)\nplt.show()\nplt.figure(figsize=(20,10), facecolor='w')\nsns.boxplot(x=\"age\",y=\"cigsPerDay\",data=df_data)\nplt.title(\"Distribution of age with respect to cigsPerDay\", size=20)\nplt.show()\nplt.figure(figsize=(20,10), facecolor='w')\nsns.boxplot(x=\"age\",y=\"glucose\",data=df_data)\nplt.title(\"Distribution of age with respect to glucose\", size=15)\nplt.show()","f3fefd6d":"#Distribution of sysBP vs diaBP with respect to currentSmoker and male attributes\n\n#sysBP vs diaBP with respect to currentSmoker and male attributes\nplt.figure(figsize=(9, 9), facecolor='w')\nsns.lmplot('sysBP', 'diaBP', \n           data=df_data,\n           hue=\"TenYearCHD\",\n           col=\"age\",row=\"currentSmoker\")\nplt.show()","9fc8c83f":"target1=df_data[df_data['TenYearCHD']==1]\ntarget0=df_data[df_data['TenYearCHD']==0]","a99e2fbd":"target1=resample(target1,replace=True,n_samples=len(target0),random_state=40)\ntarget=pd.concat([target0,target1])\ntarget['TenYearCHD'].value_counts()","a3e82390":"df_data=target\nnp.shape(df_data)","586cd84e":"plt.figure(figsize=(12, 10), facecolor='w')\nplt.subplots_adjust(right=1.5)\nplt.subplot(121)\nsns.countplot(x=\"TenYearCHD\", data=df_data)\nplt.title(\"Count of TenYearCHD column\", size=15)\nplt.subplot(122)\nlabels=[0,1]\nplt.pie(df_data[\"TenYearCHD\"].value_counts(),autopct=\"%1.1f%%\",labels=labels,colors=[\"crimson\",\"seagreen\"])\nplt.show()","79a1cbb3":"X = df_data.iloc[:,0:15]  \ny = df_data.iloc[:,-1]    \n\nbestfeatures = SelectKBest(score_func=chi2, k=10)\nfit = bestfeatures.fit(X,y)\ndf_scores = pd.DataFrame(fit.scores_)\ndf_columns = pd.DataFrame(X.columns)\n\nfeatureScores = pd.concat([df_columns,df_scores],axis=1)\nfeatureScores.columns = ['Specs','Score']  \nprint(featureScores.nlargest(11,'Score'))","c7b36acf":"featureScores = featureScores.sort_values(by='Score', ascending=False)\nfeatureScores\n","faadb0ac":"#Visualization of Feature Selection:\n\nplt.figure(figsize=(20,5))\nsns.barplot(x='Specs', y='Score', data=featureScores, palette = \"Blues_r\")\nplt.box(False)\nplt.title('Feature importance', fontsize=15)\nplt.xlabel('\\n Features', fontsize=15)\nplt.ylabel('Importance \\n', fontsize=15)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()","9ce3c13a":"features_list = featureScores[\"Specs\"].tolist()[:10]\nfeatures_list\n","868bc606":"#A new dataset with the most important features is created.\ndf = df_data[['sysBP', 'glucose','age','totChol','cigsPerDay','diaBP','prevalentHyp','diabetes','BPMeds','TenYearCHD']]\ndf.head()","9f6ad28b":"#Final Correlation Check:\nsns.set_context('talk')\nplt.figure(figsize=(22,10))\nsns.heatmap(df_data.corr()*100, annot=True, cmap='Blues')","110a6515":"scaler = MinMaxScaler(feature_range=(0,1))\ndf_scaled = pd.DataFrame(scaler.fit_transform(df_data), columns=df_data.columns)\ndf_scaled.describe()","2df7811f":"y = df['TenYearCHD']\nX = df.drop(['TenYearCHD'], axis=1)\ntrain_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.4, random_state=1)","00946a5a":"scaler = MinMaxScaler()\ntrain_x = scaler.fit_transform(train_x)\ntest_x = scaler.transform(test_x)\nprint ('Train set:', train_x.shape,  train_y.shape) \nprint ('Test set:', test_x.shape,  test_y.shape) ","4a8e0548":"#evaluation and accuracy\nm1 = 'Logistic Regression'\nlogreg = LogisticRegression() \nlogreg.fit(train_x, train_y) \npred_y = logreg.predict(test_x)\n\nfrom sklearn.metrics import jaccard_score  \nprint('Accuracy of the model in jaccard similarity score is = ',  \n      jaccard_score(test_y, pred_y))","58324a90":"#confusion matrix\n  \ncm = confusion_matrix(test_y, pred_y) \nconf_matrix = pd.DataFrame(data = cm,  \n                           columns = ['Predicted:0', 'Predicted:1'],  \n                           index =['Actual:0', 'Actual:1']) \nplt.figure(figsize = (8, 5)) \nsns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = \"Blues\") \nplt.show() \n  \nprint('The details for confusion matrix is =') \nprint (classification_report(test_y, pred_y)) ","0f569be2":"#Evaluation And Accuracy\nm2 = 'KNeighborsClassifier'\nknn = KNeighborsClassifier(n_neighbors=1)\nmodel = knn.fit(train_x, train_y)\nknn_predict = knn.predict(test_x)\nknn_acc_score = accuracy_score(test_y, knn_predict)\n\nfrom sklearn.metrics import jaccard_score \nprint('Accuracy of the model in jaccard similarity score is = ',  \n      jaccard_score(test_y, knn_predict))","cf4e1ee4":"#Confusion Matrix\ncm = confusion_matrix(test_y, knn_predict) \nconf_matrix = pd.DataFrame(data = cm,  \n                           columns = ['Predicted:0', 'Predicted:1'],  \n                           index =['Actual:0', 'Actual:1']) \nplt.figure(figsize = (8, 5)) \nsns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = \"Blues_r\") \nplt.show() \n  \nprint('The details for confusion matrix is =') \nprint (classification_report(test_y, knn_predict)) ","e2350f9f":"#Evaluation and Accuracy\nm3 = 'DecisionTreeClassifier'\ndt = DecisionTreeClassifier(criterion = 'entropy',random_state=0,max_depth = 30)\ndt.fit(train_x,train_y)\ndt_predict = dt.predict(test_x)\n\nfrom sklearn.metrics import jaccard_score \nprint('Accuracy of the model in jaccard similarity score is = ',  \n      jaccard_score(test_y, dt_predict))","30d73a11":"#Confusion Matrix\ncm = confusion_matrix(test_y, dt_predict) \nconf_matrix = pd.DataFrame(data = cm,  \n                           columns = ['Predicted:0', 'Predicted:1'],  \n                           index =['Actual:0', 'Actual:1']) \nplt.figure(figsize = (8, 5)) \nsns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = \"winter\")\nplt.show() \n  \nprint('The details for confusion matrix is =') \nprint (classification_report(test_y, dt_predict)) ","926e9cb7":"#Evaluation And Accuracy\nm4 = 'Random Forest Classfier'\nrf = RandomForestClassifier(n_estimators=200, random_state=0,max_depth=12)\nrf.fit(train_x,train_y)\nrf_predict = rf.predict(test_x)\n\nfrom sklearn.metrics import jaccard_score \nprint('Accuracy of the model in jaccard similarity score is = ',  \n      jaccard_score(test_y, rf_predict))","3016a2be":"#Confusion Matrix\ncm = confusion_matrix(test_y, rf_predict) \nconf_matrix = pd.DataFrame(data = cm,  \n                           columns = ['Predicted:0', 'Predicted:1'],  \n                           index =['Actual:0', 'Actual:1']) \nplt.figure(figsize = (8, 5)) \nsns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = \"Greens\") \nplt.show() \n  \nprint('The details for confusion matrix is =') \nprint (classification_report(test_y, rf_predict)) ","b02ab754":"m5 = 'Gradient Boosting Classifier'\ngbc =  GradientBoostingClassifier()\ngbc.fit(train_x,train_y)\ngbc_predict = gbc.predict(test_x)\n\nfrom sklearn.metrics import jaccard_score \nprint('Accuracy of the model in jaccard similarity score is = ',  \n      jaccard_score(test_y, gbc_predict))","aa3a3273":"#Confusion Matrix\ncm = confusion_matrix(test_y, gbc_predict) \nconf_matrix = pd.DataFrame(data = cm,  \n                           columns = ['Predicted:0', 'Predicted:1'],  \n                           index =['Actual:0', 'Actual:1']) \nplt.figure(figsize = (8, 5)) \nsns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = \"Greens_r\")\nplt.show() \n  \nprint('The details for confusion matrix is =') \nprint (classification_report(test_y, gbc_predict)) ","08e99cf9":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","bb261774":"# Use the random grid to search for best hyperparameters\nrf = RandomForestClassifier()\n\n# Random search of parameters, using 3 fold cross validation, \nrf_random = RandomizedSearchCV(estimator = rf, \n                               param_distributions = random_grid, \n                               n_iter = 150, \n                               cv = 2, \n                               verbose=2, \n                               random_state=7, \n                               n_jobs = -1)\n\n# Fit the random search model\nrf_random.fit(train_x,train_y)","faef862c":"rf_hyper = rf_random.best_estimator_\nrf_hyper.fit(train_x,train_y)\nprint(\"Accuracy on training set is : {}\".format(rf_hyper.score(train_x,train_y)))\nprint(\"Accuracy on validation set is : {}\".format(rf_hyper.score(test_x, test_y)))\nrf_predict = rf_hyper.predict(test_x)\nprint(\"Accuracy of Hyper-tuned Random Forest Classifier:\",jaccard_score(test_y, rf_predict))\nprint(classification_report(test_y, rf_predict))","10a51105":"#Number of trees\nn_estimators = [int(i) for i in np.linspace(start=100,stop=1000,num=10)]\n#Number of features to consider at every split\nmax_features = ['auto','sqrt']\n#Maximum number of levels in tree\nmax_depth = [int(i) for i in np.linspace(10, 100, num=10)]\nmax_depth.append(None)\n#Minimum number of samples required to split a node\nmin_samples_split=[2,5,10]\n#Minimum number of samples required at each leaf node\nmin_samples_leaf = [1,2,4]\n\n#Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}","a2d320c4":"gb=GradientBoostingClassifier(random_state=0)\n#Random search of parameters, using 3 fold cross validation, \n#search across 100 different combinations\ngb_random = RandomizedSearchCV(estimator=gb, param_distributions=random_grid,\n                              n_iter=150, scoring='f1', \n                              cv=2, verbose=2, random_state=0, n_jobs=-1,\n                              return_train_score=True)\n\n# Fit the random search model\ngb_random.fit(train_x,train_y)","c9f1a6c7":"gb_hyper = gb_random.best_estimator_\ngb_hyper.fit(train_x,train_y)\nprint(\"Accuracy on training set is : {}\".format(gb_hyper.score(train_x,train_y)))\nprint(\"Accuracy on validation set is : {}\".format(gb_hyper.score(test_x, test_y)))\ngbc_predict = gb_hyper.predict(test_x)\ngbc_acc_score = accuracy_score(test_y, gbc_predict)\nprint(\"Accuracy of Hyper-tuned Gradient Boosting Classifier:\",gbc_acc_score*100,'\\n')\nprint(classification_report(test_y, gbc_predict))","8be90301":"lr_false_positive_rate,lr_true_positive_rate,lr_threshold = roc_curve(test_y,pred_y)\nknn_false_positive_rate,knn_true_positive_rate,knn_threshold = roc_curve(test_y,knn_predict)                                                             \ndt_false_positive_rate,dt_true_positive_rate,dt_threshold = roc_curve(test_y,dt_predict)\nrf_false_positive_rate,rf_true_positive_rate,rf_threshold = roc_curve(test_y,rf_predict)\ngbc_false_positive_rate,gbc_true_positive_rate,gbc_threshold = roc_curve(test_y,gbc_predict)\n\n\nsns.set_style('whitegrid')\nplt.figure(figsize=(15,8), facecolor='w')\nplt.title('Reciever Operating Characterstic Curve')\nplt.plot(lr_false_positive_rate,lr_true_positive_rate,label='Logistic Regression')\nplt.plot(knn_false_positive_rate,knn_true_positive_rate,label='K-Nearest Neighbor')\nplt.plot(dt_false_positive_rate,dt_true_positive_rate,label='Desion Tree')\nplt.plot(rf_false_positive_rate,rf_true_positive_rate,label='Random Forest')\nplt.plot(gbc_false_positive_rate,gbc_true_positive_rate,label='Gradient Boosting Classifier')\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.legend()\nplt.show()","45294e42":"model_ev = pd.DataFrame({'Model': ['Logistic Regression','K-Nearest Neighbour','Decision Tree',\n                                   'Random Forest','Gradient Boosting'], 'Accuracy': [jaccard_score(test_y, pred_y), jaccard_score(test_y, knn_predict), \n                                                                                     jaccard_score(test_y, dt_predict), jaccard_score(test_y, rf_predict),jaccard_score(test_y, gbc_predict)]})\nmodel_ev","81f2cfb4":"colors = ['red','green','blue','gold','silver']\nplt.figure(figsize=(20,15), facecolor='w')\nplt.title(\"Barplot Representing Accuracy of different models\")\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Models\")\nplt.bar(model_ev['Model'],model_ev['Accuracy'],color = colors)\nplt.show()","ea33155a":"scv=StackingCVClassifier(classifiers=[rf_hyper, gb_hyper, knn, dt], meta_classifier= rf)\ntrain_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.4, random_state=1)\nscv.fit(train_x.values,train_y.values)\nscv_predict = scv.predict(test_x)\nscv_acc_score = accuracy_score(test_y, scv_predict)\nprint(\"Accuracy of StackingCVClassifier:\",scv_acc_score*100,'\\n')","2edd90ae":"cm = confusion_matrix(test_y, scv_predict) \nconf_matrix = pd.DataFrame(data = cm,  \n                           columns = ['Predicted:0', 'Predicted:1'],  \n                           index =['Actual:0', 'Actual:1']) \nplt.figure(figsize = (8, 5)) \nsns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = \"Blues_r\")\nplt.show() \n  \nprint('The details for confusion matrix is =') \nprint (classification_report(test_y, scv_predict)) ","e89a462e":"model_ev = model_ev.append({\"Model\":\"Stacking Ensemble\", \"Accuracy\":scv_acc_score*100}, ignore_index=True)\nmodel_ev","59117983":"The missing data has been handled and all other rows containing missing values has been removed.","0eb010d8":"## Resampling imbalanced dataset by oversampling positive case","ea2e249b":"## Hyperparameter Tuning for best Classifier","7f93c06b":"* Mid-age groups ranging from the age of 38 - 46 have more number of currentSmokers.\n* No currentSmokers observed below the age of 32.\n* maximum age for a currentSmokers is 70.","ae72ff57":"## Predictive Modelling","2f7af8be":"* Individual Best Model: Gradient Boosting( 96.6% )\n* Overall Best Model: Stacking Ensemble Classification( 97% )","472a9ed5":"## Conclusion","50b75011":"### * Logistic Regression","10a00e61":"The columns removed are:\n\n* Education: Due to irrelevance to outcome variable and being out of subject.\n* CurrentSmoker: Due to presence of a more informative similar variable(CigsPerDay).\n* PrevalentStroke: Due to high imbalance caused by this variable.\n* BMI: Due to unimpactful effect on outcome variable.\n* HeartRate: Due to the prediction made by sklearn algorithm.","89e417d6":"* Low cigsPerDay comes with lower risk of CHD.\n* Those who don't smoke, i.e., with a cigsPerDay of 0.0 has a really low risk of contracting the disease\n* Although that is the case, low cigsPerDay doesn't actually guarantee a much lower risk of CHD","de9934e5":"## Bivariate Analysis","63cfe27c":"We can observe that almost all features have a strong correlation to the output variable.","5d3b3aaf":"### Gradient Boosting Classifier","d662390f":"It shows the number of np.nan or null values or missing values are present in the dataset:\n\n   * education: 105\n   * cigsPerDay: 29\n   * BPMeds: 53\n   * totChol: 50\n   * BMI: 19\n   * heartRate: 1\n   * glucose: 388","66b9468b":"* aged people have more cholesterol\n* bad cholesterol in general","1b0fac1e":"### Random Forest","1899d156":"Among the categorical features:\n* BPmeds, prevalentStroke and diabetes are highly imbalanced.\n* BPMeds, currentSmoker, diabetes, male, prevalentHyp, and prevalentStroke are binary variable features of the dataset.\n* The number of Smokers and non-Smokers in currentSmoker is almost the same","13058578":"The above graph plots the relationship between systolic blood pressure and diastolic blood pressure for patients based on their gender and whether they are current smokers or not and plots the best fit line","1b234be4":"this shows an evweview of the Columns, non-null count and the data types of the dataset","052b9843":"# Feature Selection","1376858b":"### Decision tree","31f61fbe":"The distribution is highly imbalanced. As in, the number of negative cases outweigh the number of positive cases.\nThis would lead to class imbalance problem while fitting our models. \nTherefore, this problem needs to be addressed and taken care of.","5ec56b24":"* Handling missing and duplicate data.\n* Univariate, Bivariate and Multivariate Analysis.\n* Individual Features descriptive statistics.\n* Visualizing Target attribute shows if the dataset is imbalanced.\n* Resampling imbalanced dataset by oversampling positive case.\n* Feature Selection\n","f4e519e5":"## Ensembling\n* In order to increase the accuracy of the model we use ensembling. Here we use stacking technique. We stack the 4 highest accuracy yielding models to create an ensembled model","ee1e0b69":"## Training And Testing Data","47fa1f2f":"Continuos value features analysis:\n\n* Age : We can see that Min. age of subject found in given records is 32 while Max. being 70. So our values are ranging from 32 to 70.\n* cigsPerDay : Subject smoking Cig. per day is as low as nill while we have 70 Cigs. per day making the Peak.\n* totChol : Min. Cholesterol level recorded in our dataset is 107 while Max. is 696.\n* sysBP : Min. Systolic Blood Pressure observed in Subject is 83 while Max. is 295.\n* diaBP : Min. Diastolic Blood Pressure observed in Subject is 48 while Max. is 142.\n* BMI : Body Mass Index in our dataset ranges from 15.54 to 57 .\n* heartRate : Observed Heartrate in our case study is 44 to 143.\n* glucose : Glucose sugar level range is 40 to 394.","f3002383":"According to this dataset, males have shown a slighly higher risk of coronary heart disease TenYearCHD.","de0452ed":"* Minor relation of higher risk of TenYearCHD found with higher sysBP\n* Majority of people with sysBP ranging from 72 - 130 has lower chance of contracting the disease.","4d804473":"## EXPLORATORY DATA ANALYSIS","42b956a5":"* Glucose, Total Cholestrol, SysBP, and BMI are right skewed.\n* DiaBP and HeartRate are almost close to normal distibution.","6c36ace8":"* The X train and test tables contain all the features and their values.\n* The y train and test tables contain all the standalone features without their values.","c1ec0556":" there are no Duplicated Values present in the dataset.","670cfaab":"### missing data handling","9f2f549c":"### K Nearest Neighbors","4ea73555":"* Removable Outliers are detected in totChol and sysBP columns of our dataset. Outliers in all other numerical columns are important and thus cannot be removed.\n*  the missing values, outliers and duplicate values are dealt with, now we perform EDA.","e8eea8e0":"The dataset contains 4240 redords and 16 columns including the target column.","6fb48fd3":"## Feature Spliting And Scaling","6afede04":"# Coronary-Heart-Disease-Prediction","e59e27a1":"These are the 10 best features that can be used to predict the output variable:\n\n* Systolic Blood Pressure\n* Glucose\n* Age\n* Cholesterin\n* Cigarettes per Day\n* Diastolic Blood Pressure\n* Hypertensive\n* Diabetes\n* Blood Pressure Medication\n* Gender","4d6aa98c":"The number of positive and negative cases are equal.\nHence the classes are now balanced for model fitting","f6cf577e":"### Gradient Boosting Classifier","7040f260":"## Univariate Analysis","5774e060":"### Multivariate Analysis","a74fc68a":"The Pearson correlation between the attributes provides information to deduce if a feature is usefull or not.\n\n* currentSmoker and cigsPerDay has a strong Correlation of 77.\n* prevalentHyp vs sysBP \/ diaBP are having Positive Correlation of 70 and 62.\n* glucose and diabetes are postively Correlated alongside sysBP and diaBP.\n* The column education has a negative correlatio with the outcom variable 'TenYearCHD'. In practicality distinguishing between patients by using a feature such an education will be subjective and will not effect the prediction.","c86eff23":"### Random Forest","ad040466":"* cigsPerDay has a highly uneven distribution with the most data present in 0.\n* The majority portions of the following columns lie in the range:\n * totChol: 150 to 300\n * sysBP: 100 to 150\n * diaBP: 60 to 100\n * BMI: 20 to 30\n * heartRate: 50 to 100\n* glucose: 50 to 150","3ff4ed73":"* Minor relation found between higher risk of TenYearCHD with higher diaBP similar to the previous one\n* Majority of people with diaBP ranging upto 80.0 has lower chance of contracting the disease.","8e8733a8":"## Model Evaluation","369b999f":"* There is a minor relation between totChol and glucose.\n* totChol has a steep, linear and inverse graph for lower ranges of age\n* cigsPerDay has a fairly parallel relationship with age"}}