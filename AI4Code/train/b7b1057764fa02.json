{"cell_type":{"6e3f8383":"code","047433ce":"code","539d87e9":"code","2836aedd":"code","3dc9b7d4":"code","88c1e822":"code","f573848e":"code","64189ab2":"code","ad05a78a":"code","f0c81c79":"code","d4c8a9ba":"code","c1bf1f03":"code","94fa93a0":"code","a96abfdf":"code","602122be":"code","bba90ed9":"code","ff132222":"code","0ea9615c":"code","4279584f":"markdown","e081a5f5":"markdown","8d9b1dc0":"markdown","57f557f5":"markdown","18a24d65":"markdown","be731f07":"markdown","fc8e16de":"markdown","8b0983dc":"markdown","6a74aff9":"markdown","7e426c82":"markdown","141b6b61":"markdown","bfac7ace":"markdown","48ebd66e":"markdown","1fac38b9":"markdown","39b4946d":"markdown","7b7abbd9":"markdown","0e8eb809":"markdown","ab3483b4":"markdown","6f42a419":"markdown"},"source":{"6e3f8383":"import numpy as np\nnp.random.seed(5) \nimport tensorflow as tf\ntf.set_random_seed(2)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nimport cv2\n\ntrain_dir = \"..\/input\/asl-alphabet\/asl_alphabet_train\/asl_alphabet_train\"\neval_dir = \"..\/input\/asl-alphabet-test\/asl-alphabet-test\"","047433ce":"#Helper function to load images from given directories\ndef load_images(directory):\n    images = []\n    labels = []\n    for idx, label in enumerate(uniq_labels):\n        for file in os.listdir(directory + \"\/\" + label):\n            filepath = directory + \"\/\" + label + \"\/\" + file\n            image = cv2.resize(cv2.imread(filepath), (64, 64))\n            images.append(image)\n            labels.append(idx)\n    images = np.array(images)\n    labels = np.array(labels)\n    return(images, labels)","539d87e9":"import keras\n\nuniq_labels = sorted(os.listdir(train_dir))\nimages, labels = load_images(directory = train_dir)\n\nif uniq_labels == sorted(os.listdir(eval_dir)):\n    X_eval, y_eval = load_images(directory = eval_dir)","2836aedd":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(images, labels, test_size = 0.1, stratify = labels)\n\nn = len(uniq_labels)\ntrain_n = len(X_train)\ntest_n = len(X_test)\n\nprint(\"Total number of symbols: \", n)\nprint(\"Number of training images: \" , train_n)\nprint(\"Number of testing images: \", test_n)\n\neval_n = len(X_eval)\nprint(\"Number of evaluation images: \", eval_n)","3dc9b7d4":"#Helper function to print images\ndef print_images(image_list):\n    n = int(len(image_list) \/ len(uniq_labels))\n    cols = 8\n    rows = 4\n    fig = plt.figure(figsize = (24, 12))\n\n    for i in range(len(uniq_labels)):\n        ax = plt.subplot(rows, cols, i + 1)\n        plt.imshow(image_list[int(n*i)])\n        plt.title(uniq_labels[i])\n        ax.title.set_fontsize(20)\n        ax.axis('off')\n    plt.show()","88c1e822":"y_train_in = y_train.argsort()\ny_train = y_train[y_train_in]\nX_train = X_train[y_train_in]\n\nprint(\"Training Images: \")\nprint_images(image_list = X_train)","f573848e":"y_test_in = y_test.argsort()\ny_test = y_test[y_test_in]\nX_test = X_test[y_test_in]\n\nprint(\"Testing images: \")\nprint_images(image_list = X_test)","64189ab2":"print(\"Evaluation images: \")\nprint_images(image_list = X_eval)","ad05a78a":"y_train = keras.utils.to_categorical(y_train)\ny_test = keras.utils.to_categorical(y_test)\ny_eval = keras.utils.to_categorical(y_eval)","f0c81c79":"print(y_train[0])\nprint(len(y_train[0]))","d4c8a9ba":"X_train = X_train.astype('float32')\/255.0\nX_test = X_test.astype('float32')\/255.0\nX_eval = X_eval.astype('float32')\/255.0","c1bf1f03":"from keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Conv2D, Dense, Dropout, Flatten\nfrom keras.layers import Flatten, Dense\nfrom keras.models import Sequential\n\nmodel = Sequential()\nmodel.add(Conv2D(filters = 64, kernel_size = 5, padding = 'same', activation = 'relu', \n                 input_shape = (64, 64, 3)))\nmodel.add(Conv2D(filters = 64, kernel_size = 5, padding = 'same', activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = (4, 4)))\nmodel.add(Dropout(0.5))\nmodel.add(Conv2D(filters = 128 , kernel_size = 5, padding = 'same', activation = 'relu'))\nmodel.add(Conv2D(filters = 128 , kernel_size = 5, padding = 'same', activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = (4, 4)))\nmodel.add(Dropout(0.5))\nmodel.add(Conv2D(filters = 256 , kernel_size = 5, padding = 'same', activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Flatten())\nmodel.add(Dense(29, activation='softmax'))\n\nmodel.summary()","94fa93a0":"model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])","a96abfdf":"hist = model.fit(X_train, y_train, epochs = 5, batch_size = 64)","602122be":"score = model.evaluate(x = X_test, y = y_test, verbose = 0)\nprint('Accuracy for test images:', round(score[1]*100, 3), '%')\nscore = model.evaluate(x = X_eval, y = y_eval, verbose = 0)\nprint('Accuracy for evaluation images:', round(score[1]*100, 3), '%')","bba90ed9":"#Helper function to plot confusion matrix\ndef plot_confusion_matrix(y, y_pred):\n    y = np.argmax(y, axis = 1)\n    y_pred = np.argmax(y_pred, axis = 1)\n    cm = confusion_matrix(y, y_pred)\n    plt.figure(figsize = (24, 20))\n    ax = plt.subplot()\n    plt.imshow(cm, interpolation = 'nearest', cmap = plt.cm.Purples)\n    plt.colorbar()\n    plt.title(\"Confusion Matrix\")\n    tick_marks = np.arange(len(uniq_labels))\n    plt.xticks(tick_marks, uniq_labels, rotation=45)\n    plt.yticks(tick_marks, uniq_labels)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    ax.title.set_fontsize(20)\n    ax.xaxis.label.set_fontsize(16)\n    ax.yaxis.label.set_fontsize(16)\n    limit = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], 'd'), horizontalalignment = \"center\",color = \"white\" if cm[i, j] > limit else \"black\")\n    plt.show()","ff132222":"from sklearn.metrics import confusion_matrix\nimport itertools\n\ny_test_pred = model.predict(X_test, batch_size = 64, verbose = 0)\nplot_confusion_matrix(y_test, y_test_pred)","0ea9615c":"y_eval_pred = model.predict(X_eval, batch_size = 64, verbose = 0)\nplot_confusion_matrix(y_eval, y_eval_pred)","4279584f":"Notice how the training images are all in a similar environment, with a combination of lights and shadows. We will address these issues later.\n\nLet us now print the testing images.","e081a5f5":"Once I have loaded the data, it is time to begin an understanding of it. The code below gives us some basic details about the data.\n\n**Note**: In the train-test split I have used the `stratify` argument on the labels. This argument ensures that the data is split evenly along all labels i.e. the proportion of each label in the testing data is the same as the proportion of the testing data itself - 10%.\n\nNotice that I have two sets of testing images here. This is because the first test set is taken from the same data as the training set - that is, they have a similar look, while the evaluation data, taken from a separate library, has a completely different look. This is done so that I can test the model on both types of data and see how robust it is in the presence of changing backgrounds and colours. The differences between the two testing data will become clear when we look at their images.","8d9b1dc0":"We're ready to fit it to the training data. The values `acc` will give us the accuracy of the model after each epoch. I will run the model for a total of 5 epochs. Despite our high number of parameters and large dataset, the model runs rather quickly as `keras` is able to engage the GPU in its functioning.","57f557f5":"With our helper function now ready, we begin to print the images. The code lines above the print command: \n\n`y_train_in = y_train.argsort()\ny_train = y_train[y_train_in]\nX_train = X_train[y_train_in]` \n\nsort the data according to the symbols, making it easier for us to deal with it, and ensures that we do not run into any mismatches of data image and its symbol.","18a24d65":"Now that we have our helper function, we are ready to use it to extract data. This exaction takes a long time due to the large dataset. Even though I am using GPU services here, it does not help the time as these functions are not able to take advantage of the GPU.\n\nAn alternate method for dealing with such large datasets is to use generators - for the images, and the model. Generators are much faster than this manual, almost brute-force method, but they do not allow a similar easy access to the data. Which method to use is a judgement call made on the basis of time and space constraints. Since I have no such meaningful constraints here, I have used the longer method.","be731f07":"We see that the evaluation images do not, in fact, look at all similar to the training images. They have different hues and vastly different backgrounds. Indeed they are a much better embodiment of 'real-world data'. Therefore these will make a good test for our model's performance on 'real-world' ASL images.\n\n**Note**: I did not use any sort on the evalutation arrays before printing. This is because the evaluation arrays did not go through a `train-test-split`, and therefore their sorting was never disturbed.\n\n# 4. Preprocessing: One-hot enconding the data\n\nI briefly covered label encoding earlier, where each letter of the labels is associated with a number:\n\n`A` is encoded as `0`\n`B` is encoded as `1`\n`C` is encoded as `2`\n`D` is encoded as `3`\n ...\n `nothing` is encoded as `28`\n \nSo, currently, our labels for each of the letters are encoded as categorical integers, where `'A', 'B' and 'C'` are encoded as `0, 1, and 2`, respectively. However, `keras` models do not accept labels in this format, and we must first one-hot encode the labels before supplying them to a `keras` model.\n\nThis conversion will turn the one-dimensional array of labels into a two-dimensional array. Each row in the two-dimensional array of one-hot encoded labels corresponds to a different label. The row has a `1` in the column that corresponds to the correct label, and 0 elsewhere.\n\nFor instance,\n\n*    `0` is encoded as `[1, 0, 0]`,\n*    `1` is encoded as `[0, 1, 0]`, and\n*    `2` is encoded as `[0, 0, 1]`.\n\nThe image below summarizes these concepts:\n\n<img src=\"https:\/\/i.imgur.com\/CtSIsMP.jpg\" alt='Label encoding and One-hot encoding' style=\"width: 800px;\"\/>\n\nOne-hot encoding is easy to do using `keras`:","fc8e16de":"This is a much more interesting confusion matrix, with a lot of information. Our model performed best with images of P and F. We also see that the model got none of the images for A correct, and that it mistakenly labeled a signficant number of Vs as Ks. Studying this confusion matrix gives us a detailed and interesting view into the workings of our model on the evaluation images.","8b0983dc":"Having loaded, understood and pre-processed our data, it is finally time to turn to the model.\n\n# 5. Define and run the model\n\nI will define a deep learning model, a convolutional neural network (CNN) to classify the data. CNNs are a specific kind of artificial neural network that is very effective for image classification because they are able to take into account the spatial coherence of the image, i.e., that pixels close to each other are often related. This network accepts an image of an American Sign Language letter as input. The output layer returns the network's predicted probabilities that the image belongs in each category.\n\nBuilding a CNN begins with specifying the model type. In our case, I'll use a Sequential model, which is a linear stack of layers. I'll then add two convolutional layers. To understand convolutional layers, imagine a flashlight being shown over the top left corner of the image and slowly sliding across all the areas of the image, moving across the image in the same way your eyes move across words on a page. Convolutional layers pass a kernel (a sliding window) over the image and perform element-wise matrix multiplication between the kernel values and the pixel values in the image.\n\nHowever, those are not the only layers that we need to perform our task. A complete neural network architecture will have a number of other layers that are designed to play a specific role in the overall functioning of the network. Much deep learning research is about how to structure these layers into coherent systems.\n\nWe'll add the following layers:\n\n*     MaxPooling. This passes a (4, 4) moving window over the image and downscales the image by outputting the maximum value within the window.\n*     Dropout. This prevents the model from overfitting, i.e. perfectly remembering each image, by randomly setting a percentage of the input units to 0 at each update during training.\n*     Conv2D. Add further convolutional layers since deeper models, i.e. models with more convolutional layers, are better able to learn features from images.\n*     Flatten. As its name suggests, this flattens the output from the convolutional part of the CNN into a one-dimensional feature vector which can be passed into the following fully connected layers.\n*     Dense. Fully connected layer where every input is connected to every output.\n\n<img src=\"https:\/\/i.imgur.com\/6Crxeo5.png\" alt=\"Fully connected layer vs Convolutional Layer\" >\n\nTo take a look at how it all stacks up, we'll print the model summary. Notice that our model has a whopping `1,660,253` paramaters. These are the different weights that the model learns through training and what are used to generate predictions on a new image.","6a74aff9":"With our function now ready we can plot the first confusion matrix. This will be the matrix for the testing data, which gave a high accuracy. We expect to find the diagonal elements to have large values with some values distributed in non-diagonal elements. Note that our the matrix is not normalized, and the total number of testing images per label were 300.","7e426c82":"The testing images look rather similar to the training images - this is because they have both been taken from the same dataset. Therefore, we can already expect that a model that does well with the training images will also do well with these testing images.\n\nDo the evaluation images look similar to these? Let us have a look.","141b6b61":"We see that we have a total of 29 symbols. These are the letters A to Z, and symbols for delete, space and nothing. Our total images from the first dataset, 87000 in number, have been split into training images - 78300, and testing images - 8700. We also have an additional 870 evaluation images from a different directory.\n\n# 3. Printing images\n\nWe are now ready to print the images for the symbols. I will print one image for each symbol, from each dataset - train, test and evaluation. Since the same operation of printing images is done thrice, I will now write a helper function for this purpose. This function will create a grid of 8x4 images and fill 29 images in the first 29 of the 32 spaces.","bfac7ace":"# 7. Test the model\n\nWe can now test the model on our testing and evaluation images. We are looking to note the difference between how the model performs with the testing images vs the evaluation or real-world images.","48ebd66e":"We see that our model does rather well with the testing images which came from the same batch as the training data. this is unsurprising given that our model performed well with the training data. On the evaluation images however, our model does rather poorly with less than 50% accuracy. This tells us that we need to include more varied data in our analysis to get better results with real-world images. A great way to visualize the successes and errors of our model is to plot confusion matrices, which I will do next.\n\n# 8. Confusion Matrices\n\nA confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. Each row of the matrix represents the instances in an actual class while each column represents the instances in an predicted class (or vice versa). The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabeling one as another).\n\nI will plot confusion matrices for both the testing and evaluation data. Since I have to go through the same code twice, it makes sense to first define a function that helps us plot these matrices. The function below does exactly that. The code `y = np.argmax(y, axis = 1)` converts the 2D array y into a 1D array with number labels for each image. I do this for both the true and the predicted values. Additionally, plotting the confusion matrix with a `cmap` automatically fills it with a colour gradient showing how many of the values were correctly predicted. The `for` loop at the end of the function fills each cell of the matrix with the number of predictions that correspond to that cell.","1fac38b9":"# 2. Loading the data\n\nSince the data is held in multiple folders in the directories, it is necessary to extract it and store it in arrays. Since I will be doing this operation twice - once on the training data, and again on the evaluation data, it makes sense to write a function that helps with this exaction. This is the function given below, which takes each individual image, resizes it to a convenient size and adds it to an array. It also adds the corresponding `label code` to a `labels` array. A `label code` is simply a number that we associate with each label. For example, `A` goes to `0`, `B`:`1`, `C`:`2` and so on. This is known as label encoding and I will cover it in a little more detail in section 4.","39b4946d":"Now we can have a look at one of the labels to see if it is indeed one-hot encoded:","7b7abbd9":"We see that we have successfully one-hot encoded the labels. The length is 29, to make room for the 29 labels `A` to `nothing` in our data.\n\n# 5. Preprocessing - Normalize RGB values\n\nNow let us look at how the image data is stored. There are three components for each image - one component each for the Red, Green, and Blue (RGB) channels. The component values are stored as integer numbers in the range 0 to 255, the range that a single 8-bit byte can offer. \n\nIf, however, we divide by 255 the range can be described with a 0.0-1.0 where 0.0 means 0 (0x00 in hex) and 1.0 means 255 (0xFF in hex). Normalization will help us remove distortions caused by lights and shadows in an image. This is a good idea for our dataset as we have seen that our images have a lot of different light and shadow areas.\n\nThe following image is an example of what this normalization of RGB values does:\n\n<img src=\"http:\/\/aishack.in\/static\/img\/tut\/normalized-rgb.jpg\" alt=\"RGB vs Normalized RGB\" style=\"width:300px\">\n\nA more detailed explanation can be found [here](http:\/\/aishack.in\/tutorials\/normalized-rgb\/).","0e8eb809":"# 1. American Sign Language (ASL)\u00b6\n\nAmerican Sign Language (ASL) is the primary language used by many deaf individuals in North America, and it is also used by hard-of-hearing and hearing individuals. The language is as rich as spoken languages and employs signs made with the hand, along with facial gestures and bodily postures.\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/7\/7d\/American_Sign_Language_ASL.svg\" alt='\"ASL\" spelled out in American Sign Language fingerspelling' style=\"width: 600px;\"\/>\n\nA lot of recent progress has been made towards developing computer vision systems that translate sign language to spoken language. This technology often relies on complex neural network architectures that can detect subtle patterns in streaming video. However, as a first step, towards understanding how to build a translation system, we can reduce the size of the problem by translating individual letters, instead of sentences.\n\nIn this notebook, I will train a convolutional neural network to classify images of American Sign Language (ASL) letters. After loading, examining, and preprocessing the data, I will train the network and test its performance.\n\nIn the code cell below, I load the necessary libraries, and training and test data directories.","ab3483b4":"After having defined a neural network in `keras`, the next step is to compile it.","6f42a419":"Note that most of diagonal elements have values 300, and the non-diagonal elements have values 0, indicating that the model labeled most of our data correctly.\n\nNext I will plot the confusion matrix for the evaluation images. Once again, the matrix is not normalized so it is important to note that for every label we had 30 evaluation images."}}