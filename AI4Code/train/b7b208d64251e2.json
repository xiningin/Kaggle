{"cell_type":{"5be6be0f":"code","3280916b":"code","2138b122":"code","8934b295":"code","896f9b9b":"code","1c0f4b8d":"code","c62decdb":"code","1ef22add":"code","2c533890":"code","8553c54c":"code","9031febb":"code","0030c7f0":"code","8905aa90":"code","67f26a37":"code","2a579f54":"code","35d630fa":"markdown","9fdad99d":"markdown","fa3cf26c":"markdown","15fcdfdc":"markdown","87c42b49":"markdown","3b2db412":"markdown","e2adb833":"markdown","53609f96":"markdown","1460a7d2":"markdown","4d8ca12b":"markdown","18f21136":"markdown"},"source":{"5be6be0f":"import numpy as np \nimport pandas as pd\nfrom sklearn import *\nfrom sklearn.metrics import f1_score\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nimport xgboost as xgb\nfrom catboost import Pool,CatBoostRegressor\nimport datetime\nimport gc\n\nsns.set_style(\"whitegrid\")\n\nfrom sklearn.model_selection import KFold","3280916b":"#Constants\nMODEL_TYPE = 'lgb'  # available types 'lgb', 'xgb', 'cat'\nTRAINING = True\nENSEMBLE = False\nGROUP_BATCH_SIZE = 4000\nWINDOWS = [10, 50]\n\n\nBASE_PATH = '\/kaggle\/input\/liverpool-ion-switching'\nDATA_PATH = '\/kaggle\/input\/data-without-drift'\nRFC_DATA_PATH = '\/kaggle\/input\/ion-shifted-rfc-proba'\nMODELS_PATH = '\/kaggle\/input\/ensemble-models'","2138b122":"# create folds\n\nimport pandas as pd\nfrom sklearn import model_selection\n\n\ndf = pd.read_csv(f\"{DATA_PATH}\/train_clean.csv\")\ndf = df.dropna().reset_index(drop=True)\n\ndf[\"kfold\"] = -1\n\n# df = df.sample(frac=1).reset_index(drop=True)\n\nkf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (trn_, val_) in enumerate(kf.split(X=df, y=df.open_channels.values)):\n    print(len(trn_), len(val_))\n    df.loc[val_, 'kfold'] = fold\n\ndf.to_csv(\"train_folds.csv\", index=False)","8934b295":"%%time\n\ndef create_rolling_features(df):\n    for window in WINDOWS:\n        df[\"rolling_mean_\" + str(window)] = df['signal'].rolling(window=window).mean()\n        df[\"rolling_std_\" + str(window)] = df['signal'].rolling(window=window).std()\n        df[\"rolling_var_\" + str(window)] = df['signal'].rolling(window=window).var()\n        df[\"rolling_min_\" + str(window)] = df['signal'].rolling(window=window).min()\n        df[\"rolling_max_\" + str(window)] = df['signal'].rolling(window=window).max()\n        df[\"rolling_min_max_ratio_\" + str(window)] = df[\"rolling_min_\" + str(window)] \/ df[\"rolling_max_\" + str(window)]\n        df[\"rolling_min_max_diff_\" + str(window)] = df[\"rolling_max_\" + str(window)] - df[\"rolling_min_\" + str(window)]\n\n    df = df.replace([np.inf, -np.inf], np.nan)    \n    df.fillna(0, inplace=True)\n    return df\n\n\ndef create_features(df, batch_size):\n    \n    df['group'] = df.groupby(df.index\/\/batch_size, sort=False)['signal'].agg(['ngroup']).values\n    df['group'] = df['group'].astype(np.uint16)\n    for window in WINDOWS:    \n        df['signal_shift_pos_' + str(window)] = df.groupby('group')['signal'].shift(window).fillna(0)\n        df['signal_shift_neg_' + str(window)] = df.groupby('group')['signal'].shift(-1 * window).fillna(0)\n        \n    df['signal_2'] = df['signal'] ** 2\n    return df   ","896f9b9b":"## reading data\ntrain = pd.read_csv(f'train_folds.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\ntest  = pd.read_csv(f'{DATA_PATH}\/test_clean.csv', dtype={'time': np.float32, 'signal': np.float32})\nsub  = pd.read_csv(f'{BASE_PATH}\/sample_submission.csv', dtype={'time': np.float32})\n\n# loading and adding shifted-rfc-proba features\ny_train_proba = np.load(f\"{RFC_DATA_PATH}\/Y_train_proba.npy\")\ny_test_proba = np.load(f\"{RFC_DATA_PATH}\/Y_test_proba.npy\")\n\nfor i in range(11):\n    train[f\"proba_{i}\"] = y_train_proba[:, i]\n    test[f\"proba_{i}\"] = y_test_proba[:, i]\n\n    \ntrain = create_rolling_features(train)\ntest = create_rolling_features(test)   \n    \n## normalizing features\ntrain_mean = train.signal.mean()\ntrain_std = train.signal.std()\ntrain['signal'] = (train.signal - train_mean) \/ train_std\ntest['signal'] = (test.signal - train_mean) \/ train_std\n\n\nprint('Shape of train is ',train.shape)\nprint('Shape of test is ',test.shape)","1c0f4b8d":"## create features\n\nbatch_size = GROUP_BATCH_SIZE\n\ntrain = create_features(train, batch_size)\ntest = create_features(test, batch_size)\n\ncols_to_remove = ['time','signal','batch','batch_index','batch_slices','batch_slices2', 'group']\ncols = [c for c in train.columns if c not in cols_to_remove]\ncols_test = [c for c in test.columns if c not in cols_to_remove]\n\nX = train[cols]\ny = train['open_channels']\nX_test = test[cols_test]","c62decdb":"del train\ndel test\ngc.collect()","1ef22add":"def f1_score_calc(y_true, y_pred):\n    return f1_score(y_true, y_pred, average=\"macro\")\n\ndef lgb_Metric(preds, dtrain):\n    labels = dtrain.get_label()\n    preds = np.round(np.clip(preds, 0, 10)).astype(int)\n    score = f1_score(labels, preds, average=\"macro\")\n    return ('KaggleMetric', score, True)","2c533890":"def train_model(X, X_test, params, model_type='lgb', eval_metric='f1score',\n                               columns=None, plot_feature_importance=False, model=None,\n                               verbose=50, early_stopping_rounds=200, n_estimators=2000):\n    \n    # to set up scoring parameters\n    metrics_dict = {'f1score': {'lgb_metric_name': lgb_Metric,}}\n    results = []    \n    oof = np.zeros(len(X) )    \n    prediction = np.zeros((len(X_test)))    \n    feature_importance = pd.DataFrame()\n            \n    if True:        \n        for fold in range(0,5):\n            result_dict = {}\n            X_train = X[X.kfold != fold].reset_index(drop=True)\n            X_valid = X[X.kfold == fold].reset_index(drop=True)\n            y_train = X_train.open_channels\n            y_valid = X_valid.open_channels\n            \n            X_train = X_train.drop(['kfold', 'open_channels'], axis=1)\n            X_valid = X_valid.drop(['kfold', 'open_channels'], axis=1)\n            \n#             X_train, X_valid, y_train, y_valid = model_selection.train_test_split(X, y, test_size=0.3, random_state=7)    \n\n            if model_type == 'lgb':\n                model = lgb.train(params, lgb.Dataset(X_train, y_train),\n                                  n_estimators,  lgb.Dataset(X_valid, y_valid),\n                                  verbose_eval=verbose, early_stopping_rounds=early_stopping_rounds, feval=lgb_Metric)\n\n                preds = model.predict(X_valid, num_iteration=model.best_iteration) #model.predict(X_valid) \n                y_pred_valid = np.round(np.clip(preds, 0, 10)).astype(int)\n                y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n\n            if model_type == 'xgb':\n                train_set = xgb.DMatrix(X_train, y_train)\n                val_set = xgb.DMatrix(X_valid, y_valid)\n                model = xgb.train(params, train_set, num_boost_round=2222, evals=[(train_set, 'train'), (val_set, 'val')], \n                                         verbose_eval=verbose, early_stopping_rounds=early_stopping_rounds)\n\n                preds = model.predict(xgb.DMatrix(X_valid)) #model.predict(X_valid) \n                y_pred_valid = np.round(np.clip(preds, 0, 10)).astype(int)\n                y_pred = model.predict(xgb.DMatrix(X_test))\n\n\n            print(f'FINAL score fold {fold}: {f1_score_calc(y_valid, y_pred_valid)}')\n            print('*'*100)\n\n            result_dict['pred_valid'] = preds\n            result_dict['pred_test'] = y_pred\n            result_dict['model'] = model\n            results.append(result_dict)\n    return results","8553c54c":"## training model\n\nif TRAINING and MODEL_TYPE == 'lgb':\n    params = {'learning_rate': 0.1, 'max_depth': 7, 'num_leaves':2**7+1, 'metric': 'rmse', 'random_state': 7, 'n_jobs':-1}\n\n    result_dict_lgb = train_model(X=X, X_test=X_test, params=params, model_type=MODEL_TYPE, eval_metric='f1score', plot_feature_importance=False,\n                                                          verbose=50, early_stopping_rounds=150, n_estimators=3000)","9031febb":"if TRAINING and MODEL_TYPE == 'xgb':\n    params_xgb = {'colsample_bytree': 0.375,'learning_rate': 0.1,'max_depth': 10, 'subsample': 1, 'objective':'reg:squarederror',\n              'eval_metric':'rmse'}\n\n    result_dict_xgb = train_model(X=X, X_test=X_test, params=params_xgb, model_type=MODEL_TYPE, eval_metric='f1score', plot_feature_importance=False,\n                                                          verbose=50, early_stopping_rounds=250)","0030c7f0":"if TRAINING and MODEL_TYPE == 'lgb':\n    for fold in range(0,1):\n        booster = result_dict_lgb[fold]['model']\n        fi = pd.DataFrame()\n        fi['importance'] = booster.feature_importance(importance_type='gain')\n        fi['feature'] = booster.feature_name()\n        best_features = fi.sort_values(by='importance', ascending=False)[:20]\n        plt.figure(figsize=(16, 12));\n        sns.barplot(x=\"importance\", y=\"feature\", data=best_features);\n        plt.title('LGB Features fold {fold} (avg over folds)');","8905aa90":"import joblib\n\nif TRAINING:\n    if MODEL_TYPE == 'lgb':\n        for fold in range(0,5):\n            filename = f'lgb_v{fold}.sav'\n            joblib.dump(result_dict_lgb[fold]['model'], filename)\n    elif MODEL_TYPE == 'xgb':\n        for fold in range(0,5):\n            filename = f'xgb_{fold}.sav'\n            joblib.dump(result_dict_xgb[fold]['model'], filename)","67f26a37":"def get_prediction(test, model, model_type):\n    if model_type == 'xgb':\n        y_pred = model.predict(xgb.DMatrix(test))\n    else:\n        y_pred = model.predict(test, num_iteration=model.best_iteration)\n    y_pred = np.round(np.clip(y_pred, 0, 10)).astype(int)\n    return y_pred","2a579f54":"if TRAINING:\n    sub  = pd.read_csv(f'{BASE_PATH}\/sample_submission.csv', dtype={'time': np.float32})    \n    if MODEL_TYPE == 'lgb':\n        preds = []\n        for fold in range(0,5):\n            preds.append(result_dict_lgb[fold][f'pred_test'])\n        preds = np.average(preds, axis=0)\n        y_pred = np.round(np.clip(preds, 0, 10)).astype(int)\n\n        sub['open_channels'] =  np.array(np.round(y_pred,0), np.int)\n    elif MODEL_TYPE == 'xgb':\n        preds = []\n        for fold in range(0, 5):\n            preds.append(result_dict_xgb[fold][f'pred_test'])\n        preds = np.average(preds, axis=0)\n        y_pred = np.round(np.clip(preds, 0, 10)).astype(int)\n        sub['open_channels'] =  np.array(np.round(y_pred,0), np.int)\n\n    sub.to_csv('submission.csv', index=False, float_format='%.4f')\n    print(sub.head(10))\nelse:\n    if ENSEMBLE:\n        model_lgb = joblib.load(MODELS_PATH + 'lgb_0.sav')\n        model_xgb = joblib.load(MODELS_PATH + 'xgb_0.sav')\n        y_pred_lgb = get_prediction(X_test, model_lgb, 'lgb')\n        y_pred_xgb = get_prediction(X_test, model_xgb, 'xgb')\n        y_pred = 0.50 * y_pred_lgb + 0.50 * y_pred_xgb\n    else:\n        if MODEL_TYPE == 'lgb':\n            model_lgb = joblib.load(MODELS_PATH + 'lgb_0.sav')\n            y_pred = get_prediction(X_test, model_lgb, MODEL_TYPE)\n        elif MODEL_TYPE == 'xgb':\n            model_xgb = joblib.load(MODELS_PATH + 'xgb_1.sav')\n            y_pred = get_prediction(X_test, model_xgb, MODEL_TYPE)\n            \n    sub  = pd.read_csv(f'{BASE_PATH}\/sample_submission.csv', dtype={'time': np.float32})\n\n    sub['open_channels'] =  np.array(np.round(y_pred,0), np.int)\n    sub.to_csv('submission.csv', index=False, float_format='%.4f')\n    print(sub.head(10))","35d630fa":"# 1. Create folds","9fdad99d":"# 4. Training Model","fa3cf26c":"## 6. Saving models","15fcdfdc":"## 2. Create Features","87c42b49":"## 7. Importing saved models and create submission","3b2db412":"#### Whats new in this version\n* Added code to train model for 5 folds and then ensemble.\n* added [data-without-drift](https:\/\/www.kaggle.com\/cdeotte\/data-without-drift)\n* removed old features as they were taking too much time\n* added new features mostly from [here](https:\/\/www.kaggle.com\/nxrprime\/wavenet-with-shifted-rfc-proba-and-cbr)\n\n#### What's next\n* Try Ensemble using GroupKFold\n* Add few top features from old version \n","e2adb833":"<a id=\"id5\"><\/a><br> \n## **5. Plotting feature importance top 20 Features**","53609f96":"<a id=\"ref\"><\/a>\n# **8. References** \n\n[1] Deep-Channel uses deep neural networks to detect\nsingle-molecule events from patch-clamp data https:\/\/www.nature.com\/articles\/s42003-019-0729-3.pdf\n\n[2] The Patch Clamp Method: https:\/\/www.youtube.com\/watch?v=mVbkSD5FHOw\n\n[3] Electophysiology: Patch clamp method https:\/\/www.youtube.com\/watch?v=CvfXjGVNbmw\n\n[4] The Action Potential https:\/\/www.youtube.com\/watch?v=HYLyhXRp298\n\n[5] https:\/\/www.kaggle.com\/pestipeti\/eda-ion-switching\n\n[6] https:\/\/www.kaggle.com\/kmat2019\/u-net-1d-cnn-with-keras\n\n[7] https:\/\/www.kaggle.com\/cdeotte\/one-feature-model-0-930\n\n[8] https:\/\/www.kaggle.com\/nxrprime\/wavenet-with-shifted-rfc-proba-and-cbr","1460a7d2":"* > **Note**: If you like my work, please, upvote \u263a <br>","4d8ca12b":"The model can score 0.940 on LB without further optimization. Maybe with some GroupKField Cross Validation, Ensamble or more features engineering, it's possible to get > 0.940","18f21136":"<a id=\"id5\"><\/a> <br> \n# **3. Model**\n"}}