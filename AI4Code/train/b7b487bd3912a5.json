{"cell_type":{"d8af2d84":"code","9b436fbb":"code","0ff4880e":"code","8a05db95":"code","ce1d3c17":"code","9af0ad3a":"code","783c6913":"code","de23c965":"code","b4baf547":"code","be76426d":"code","d5599c11":"code","aabde88b":"code","ec2f00f7":"code","b145b278":"code","eb1b693c":"code","0b9625e4":"code","57220085":"code","f10d1e6d":"code","0bcd7d2f":"code","e5fe0480":"code","4f0d5d9d":"code","dfa38f3c":"code","d60a102e":"code","3a478857":"code","2964ff37":"code","fa327507":"code","028c1bd1":"code","5c9801ae":"code","2f5a26e5":"markdown","8c08cef5":"markdown","304e444d":"markdown","b3813e0e":"markdown","a0997260":"markdown","c383aae2":"markdown","db0eb7ff":"markdown","80847a8f":"markdown","e5d9403a":"markdown","73433d5c":"markdown"},"source":{"d8af2d84":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n  #  for filename in filenames:\n   #     print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9b436fbb":"import cv2\nimport matplotlib.pyplot as plt\nimport cudf, cuml, cupy\n\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom tensorflow.keras.applications import EfficientNetB3\n\nimport gc   # garbage collect","0ff4880e":"LIMIT = 1\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  try:\n    tf.config.experimental.set_virtual_device_configuration(\n        gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n    \n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    \n  except RuntimeError as e:\n    print(e)","8a05db95":"COMPUTE_CV = True\n\ntest = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\n\nif(len(test) > 3): COMPUTE_CV = False\nelse: print('This submission notebook will compute CV score, but commit notebook will not')","ce1d3c17":"train = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\n\ntemp = train.groupby('label_group').posting_id.agg('unique').to_dict()\n\ntrain['target'] = train.label_group.map(temp)\n\ntrain.head()","9af0ad3a":"def getMetric(col):\n    def f1score(row):\n        num_intersect = len(np.intersect1d(row['target'], row[col]))\n        return 2 * num_intersect \/ (len(row['target']) + len(row[col]))\n    return f1score","783c6913":"temp = train.groupby('image_phash').posting_id.agg('unique').to_dict()\n\ntrain['oof'] = train.image_phash.map(temp)","de23c965":"train['f1'] = train.apply(getMetric('oof'), axis = 1)\n\nprint('Baseline CV score : {}'.format(train['f1'].mean()))","b4baf547":"if COMPUTE_CV:\n    test = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\n    test_cdf = cudf.DataFrame(test)\n    print('Commit is On, i.e using train as test\\n')\nelse:\n    test = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\n    test_cdf = cudf.read_csv('..\/input\/shopee-product-matching\/test.csv')\n    print('Submission is On\\n')\n    \n \nprint('Shape : {}'.format(test_cdf.shape))","be76426d":"test_cdf.head()","d5599c11":"if COMPUTE_CV:\n    base = '..\/input\/shopee-product-matching\/train_images\/'\nelse:\n    base = '..\/input\/shopee-product-matching\/test_images\/'\n","aabde88b":"# model = EfficientNetB3(weights = 'imagenet', include_top = False, pooling = 'avg', input_shape = None)","ec2f00f7":"# model.save_weights('modelweights.h5')","b145b278":"model = EfficientNetB3(weights = None, include_top = False, pooling = 'avg', input_shape = None)","eb1b693c":"model.load_weights('..\/input\/efficientnetb3-imagenet-weights\/modelweights(1).h5')","0b9625e4":"import math\n\nclass DataGenerator(tf.keras.utils.Sequence):\n    # Generates data for keras'\n    \n    def __init__(self, df, img_size = 256, batch_size = 32, path = ''):\n        self.df = df\n        self.img_size = img_size\n        self.batch_size = batch_size\n        self.path = path\n        self.indices = np.arange(len(self.df))\n        \n    def __len__(self):                     # Denotes the number of batches per epoch\n        return math.ceil(len(self.df) \/ self.batch_size)\n    \n    def __getitem__(self, index):   # Generates one batch of data\n        \n        indices = self.indices[ index*self.batch_size : min((index+1)*self.batch_size, len(self.df))]\n        X = np.zeros((len(indices), self.img_size, self.img_size, 3))\n        df = self.df.iloc[indices]\n        \n        for i , (index, row) in enumerate(df.iterrows()):\n            img = cv2.imread(self.path + row.image)\n            X[i,] = cv2.resize(img, (self.img_size, self.img_size))\n        \n        return X\n            \n        \n","57220085":"embeds = []\nchunk_size = 1024*4\n\nnum_chunks = math.ceil(len(test) \/ chunk_size)\n\nfor i in range(num_chunks):\n    low = i*chunk_size\n    high = min((i+1)*chunk_size , len(test))\n    \n    print('chunk : {} - {}'.format(low, high))\n    \n    test_gen = DataGenerator(test.iloc[low : high], path = base)\n    \n    image_embeddings = model.predict(test_gen, verbose = 1, use_multiprocessing = True, workers = 4)\n    \n    embeds.append(image_embeddings)\n    \n\nimage_embeddings = np.concatenate(embeds)\n\nprint('image embeddings shape : ',image_embeddings.shape)\n","f10d1e6d":"del model   # model will delete it but the TF graph will have no changes.\n\n_ = gc.collect()  ","0bcd7d2f":"KNN = 50\n\nif(len(test) == 3): KNN = 2\n\nmodel = NearestNeighbors(n_neighbors = KNN)\n\nmodel.fit(image_embeddings)\n","e5fe0480":"preds = []\nchunk_size = 1024*4\n\nnum_chunks = math.ceil(len(image_embeddings)\/chunk_size)\n\nfor i in range(num_chunks):\n    \n    low = i * chunk_size\n    high = min((i+1) * chunk_size , len(image_embeddings))\n    \n    print('chunk : {} - {}'.format(low, high))\n    \n    distances, indices = model.kneighbors(image_embeddings[low:high])\n    \n    for k in range(high-low):\n        ind = np.where(distances[k,] < 6.0)[0]\n        ids = indices[k, ind]\n        sim_img_ids = test.iloc[ids].posting_id.values\n        preds.append(sim_img_ids)\n        \ndel model, distances, indices, embeds, image_embeddings\n_ = gc.collect()","4f0d5d9d":"test['pred2'] = preds\ntest.head()","dfa38f3c":"model = TfidfVectorizer(stop_words = 'english', binary = True, max_features = 25000)\n\ntext_embeddings = model.fit_transform(test_cdf.title).toarray()\n\nprint('text embeddings shape : ',text_embeddings.shape)","d60a102e":"preds = []\nchunk_size = 1024*4\n\nnum_chunks = math.ceil(len(test) \/ chunk_size)\n\nfor i in range(num_chunks):\n    low = i * chunk_size\n    high = min((i+1)*chunk_size, len(test))\n    \n    print('chunk : {} - {}'.format(low, high))\n    \n    distances = cupy.matmul(text_embeddings, text_embeddings[low:high].T).T\n    \n    for k in range(high-low):\n        ind = cupy.where(distances[k,] > 0.7)[0]\n        #sim_titles = test.iloc[ind].posting_id.values\n        sim_titles = test.iloc[cupy.asnumpy(ind)].posting_id.values\n        preds.append(sim_titles)\n        \ndel model, text_embeddings\n_ = gc.collect()","3a478857":"test['preds'] = preds\ntest.head()","2964ff37":"temp = test.groupby('image_phash').posting_id.agg('unique').to_dict()\n\ntest['preds3']  = test['image_phash'].map(temp)\n\ntest.head()","fa327507":"def combine_for_sub(row):\n    x = np.concatenate([row.preds, row.pred2, row.preds3])\n    return \" \".join(np.unique(x))\n\ndef combine_for_cv(row):\n    x = np.concatenate([row.preds, row.pred2, row.preds3])\n    return np.unique(x)","028c1bd1":"if COMPUTE_CV:\n    temp = test.groupby('label_group').posting_id.agg('unique').to_dict()\n    test['target'] = test['label_group'].map(temp)\n    test['oof'] = test.apply(combine_for_cv, axis = 1)\n    test['f1'] = test.apply(getMetric('oof'), axis = 1)\n    print('CV score : ',test['f1'].mean())\n    \ntest['matches'] = test.apply(combine_for_sub,axis=1)","5c9801ae":"test[['posting_id', 'matches']].to_csv('submission.csv',index=False)\n\nsub = pd.read_csv('submission.csv')\n\nsub.head()","2f5a26e5":"### Using phash feature","8c08cef5":"#### We will find similar images with RAPIDS cuML KNN in chunks","304e444d":"### Restrict TensorFlow to 1GB OF GPU RAM so that we have 15GB RAM for RAPIDS","b3813e0e":"### Custom DataGenrator for generating Data","a0997260":"\n#### To prevent memory errors, we will compute image embeddings in chunks ","c383aae2":"## Using Image Embeddings","db0eb7ff":"## Use Text Embeddings","80847a8f":"### Compute CV score","e5d9403a":"## Compute Baseline CV score","73433d5c":"## Compute RAPIDS Model CV"}}