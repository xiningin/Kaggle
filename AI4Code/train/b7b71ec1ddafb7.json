{"cell_type":{"e0f2f8a7":"code","7e88b7e9":"code","4db0cc58":"code","af341fc3":"code","bf5d0cdc":"code","4189f0e2":"code","b373cdd3":"code","315dae34":"code","d3a84b36":"code","ffffbfd9":"code","a0d997e0":"code","064b7992":"code","bba0253f":"code","43abed84":"code","3e02a287":"code","2aeef1d3":"code","dc04c056":"code","b815a4b2":"code","0bedb069":"code","21156e45":"code","d2b88b34":"code","3ba30f5e":"code","cdf7f758":"code","0293393f":"code","97c67aa9":"code","24606e5a":"code","45401e42":"code","ed39fbf6":"code","5b37ba9c":"code","454b090b":"code","3aec8c08":"code","ef3fbd63":"code","36622a57":"code","6af47a08":"code","c76c089b":"code","070e8947":"code","5f924b9e":"code","55debfa7":"code","f4aa2a96":"code","2f393f8b":"code","0ee75794":"code","31f5bd9d":"code","8aec89e6":"code","78255000":"code","01a45b84":"code","34cc84b3":"code","aa7ea301":"code","75d7f338":"code","d0413199":"code","e2e8fa11":"code","ec2718a5":"code","2c2cebb5":"code","44cd5e87":"code","ba6b306a":"markdown","cceaf722":"markdown","31decda2":"markdown","230cc103":"markdown","31e360e8":"markdown"},"source":{"e0f2f8a7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input', topdown = True):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7e88b7e9":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly as py\nimport plotly.express as px\nimport plotly.graph_objs as pgo\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nfrom wordcloud import WordCloud\n\ninit_notebook_mode(connected=True) \n\nimport warnings\nwarnings.filterwarnings('ignore')","4db0cc58":"import pandas as pd\npath = \"\/kaggle\/input\/data-science-bowl-2019\"\ndf_sample_submission = pd.read_csv(f\"{path}\/sample_submission.csv\")\ndf_specs = pd.read_csv(f\"{path}\/specs.csv\")\ndf_test = pd.read_csv(f\"{path}\/test.csv\")\ndf_train = pd.read_csv(f\"{path}\/train.csv\")\ndf_train_labels = pd.read_csv(f\"{path}\/train_labels.csv\")","af341fc3":"def data_description(data):\n    return (data.describe())\n\ndef data_info(data):\n    return (data.info())\n\ndef data_head(data):\n    return (data.head())\n\ndef data_column(data):\n    return (data.column)","bf5d0cdc":"data_description(df_train)\ndata_info(df_train)\ndata_head(df_train)","4189f0e2":"print (\"Train Size: {0} \\n Test Size: {1}\" .format(df_train.shape, df_test.shape))","b373cdd3":"df_train_labels.head()\ndf_train_labels.shape","315dae34":"pd.set_option(\"max_colwidth\", 150)\ndata_head(df_test)","d3a84b36":"pd.set_option(\"max_colwidth\", 150)\ndata_head(df_specs)","ffffbfd9":"pd.set_option(\"max_colwidth\", 150)\ndata_head(df_train_labels)","a0d997e0":"print(f\"df_train_installation_id: {df_train.installation_id.nunique()}\")\nprint(f\"df_train_labels_installation_id: {df_train_labels.installation_id.nunique()}\")\nprint(f\"df_test_installation_id: {df_test.installation_id.nunique()}\")\nprint(f\"test&submission_installation_id_check: {set(df_test.installation_id.unique()) == set(df_sample_submission.installation_id.unique())}\")\nprint(f\"train&test titles_check: {set(df_test.title.unique()) == set(df_train.title.unique())}\")","064b7992":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (total\/data.isnull().count()*100)\n    missing_columns = pd.concat([total, percent], axis = 1, keys = [\"Total\", \"Percent\"])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    missing_columns[\"Types\"] = types\n    return(np.transpose(missing_columns))","bba0253f":"missing_data(df_train)","43abed84":"missing_data(df_test)","3e02a287":"missing_data(df_specs)","2aeef1d3":"missing_data(df_train_labels)","dc04c056":"print(f\"Counting Rows in train set: {df_train.shape[0]}\")\nprint(f\"Counting Rows in train_labels set: {df_train_labels.shape[0]}\")\nprint(f\"Counting Rows in test set: {df_test.shape[0]}\")\nprint(f\"Counting Rows in specs set: {df_specs.shape[0]}\")\n","b815a4b2":"for column in df_train.columns.values:\n    print(f\"Counts of Unique values in train_set \u00b4{column}`: {df_train[column].nunique()}\")","0bedb069":"for column in df_train_labels.columns.values:\n    print(f\"Counts of Unique values in train_labels \u00b4{column}`: {df_train_labels[column].nunique()}\")","21156e45":"df_train_labels['title'].apply(lambda x: x if x != {} else 0).value_counts()","d2b88b34":"for column in df_test.columns.values:\n    print(f\"Counts of Unique values in test_set \u00b4{column}`: {df_test[column].nunique()}\")","3ba30f5e":"for column in df_specs.columns.values:\n    print(f\"Counts of Unique values in Specs \u00b4{column}`: {df_specs[column].nunique()}\")","cdf7f758":"import plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\ntrace = go.Pie(labels = df_train['title'].value_counts().index,\n              values = df_train['title'].value_counts().values,\n              domain = {'x':[0.20,1]})\n\ndata = [trace]\nlayout = go.Layout(title = 'PieChat Distribution of titles in train_labels')\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","0293393f":"trace = go.Pie(labels = df_train_labels['title'].value_counts().index,\n              values = df_train_labels['title'].value_counts().values,\n              domain = {'x':[0.20,1]})\n\ndata = [trace]\nlayout = go.Layout(title = 'PieChat Distribution of Titles in train_labels')\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","97c67aa9":"def plot_count(feature, title, df, size=1):\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:30], palette='Set3')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    if(size > 2):\n        plt.xticks(rotation=90, size=8)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n    plt.show() ","24606e5a":"plot_count('title', 'title (first most frequent 30 values - train_labels)', df_train_labels, size=6)","45401e42":"trace = go.Pie(labels = df_train['type'].value_counts().index,\n              values = df_train['type'].value_counts().values,\n              domain = {'x':[0.20,1]})\n\ndata = [trace]\nlayout = go.Layout(title = 'PieChat Distribution of type in train')\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","ed39fbf6":"trace = go.Pie(labels = df_test['type'].value_counts().index,\n              values = df_test['type'].value_counts().values,\n              domain = {'x':[0.20,1]})\n\ndata = [trace]\nlayout = go.Layout(title = 'PieChat Distribution of type in test')\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","5b37ba9c":"trace = go.Pie(labels = df_train['world'].value_counts().index,\n              values = df_train['world'].value_counts().values,\n              domain = {'x':[0.20,1]})\n\ndata = [trace]\nlayout = go.Layout(title = 'PieChat Distribution of world in train')\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","454b090b":"trace = go.Pie(labels = df_test['world'].value_counts().index,\n              values = df_test['world'].value_counts().values,\n              domain = {'x':[0.20,1]})\n\ndata = [trace]\nlayout = go.Layout(title = 'PieChat Distribution of world in test')\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","3aec8c08":"extracted_samples_train = df_train.sample(100000)\ndata_head(extracted_samples_train)","ef3fbd63":"extracted_samples_train.iloc[0].event_data\n","36622a57":"import json\n\nextracted_event_data_samples_train = pd.io.json.json_normalize(extracted_samples_train.event_data.apply(json.loads))","6af47a08":"print(f\"extracted_event_data_shape: {extracted_event_data_samples_train.shape}\")\ndata_head(extracted_event_data_samples_train)","c76c089b":"missing_data(extracted_event_data_samples_train)","070e8947":"extracted_event_data_samples_train.columns\n","5f924b9e":"def existing_data(data):\n    total = data.isnull().count() - data.isnull().sum()\n    percent = 100 - (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    tt = pd.DataFrame(tt.reset_index())\n    return(tt.sort_values(['Total'], ascending=False))","55debfa7":"stat_extracted_event_data_samples_train = existing_data(extracted_event_data_samples_train)\n\nplt.figure(figsize=(10, 10))\nsns.set(style='whitegrid')\nax = sns.barplot(x='Percent', y='index', data=stat_extracted_event_data_samples_train.head(50), color='blue')\nplt.title('Most frequent features in extracted_event_data_samples_train')\nplt.ylabel('features')","f4aa2a96":"df_specs.args[0]","2f393f8b":"\nextracted_specs_args = pd.DataFrame()\nfor i in range(0, df_specs.shape[0]): \n    for arg_item in json.loads(df_specs.args[i]) :\n        new_df = pd.DataFrame({'event_id': df_specs['event_id'][i],\\\n                               'info':df_specs['info'][i],\\\n                               'args_name': arg_item['name'],\\\n                               'args_type': arg_item['type'],\\\n                               'args_info': arg_item['info']}, index=[i])\n        extracted_specs_args = extracted_specs_args.append(new_df)","0ee75794":"print(f\"extracted args from specs: {extracted_specs_args.shape}\")\n\ndata_head(extracted_specs_args)","31f5bd9d":"tmp = extracted_specs_args.groupby(['event_id'])['info'].count()\ndf = pd.DataFrame({'event_id':tmp.index, 'count': tmp.values})\nplt.figure(figsize=(6,4))\nsns.set(style='whitegrid')\nax = sns.distplot(df['count'],kde=True,hist=False, bins=40)\nplt.title('Distribution of number of arguments per event_id')\nplt.xlabel('Number of arguments'); plt.ylabel('Density'); plt.show()","8aec89e6":"plot_count('args_name', 'args_name (first most frequent 30 values - specs)', extracted_specs_args, size=6)","78255000":"from typing import Any\nimport re\n\ndef add_datepart(df: pd.DataFrame, field_name: str,\n                 prefix: str = None, drop: bool = True, time: bool = True, date: bool = True):\n    \"\"\"\n    Helper function that adds columns relevant to a date in the column `field_name` of `df`.\n    from fastai: https:\/\/github.com\/fastai\/fastai\/blob\/master\/fastai\/tabular\/transform.py#L55\n    \"\"\"\n    field = df[field_name]\n    prefix = ifnone(prefix, re.sub('[Dd]ate$', '', field_name))\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Is_month_end', 'Is_month_start']\n    if date:\n        attr.append('Date')\n    if time:\n        attr = attr + ['Hour', 'Minute']\n    for n in attr:\n        df[prefix + n] = getattr(field.dt, n.lower())\n    if drop:\n        df.drop(field_name, axis=1, inplace=True)\n    return df\n\n\ndef ifnone(a: Any, b: Any) -> Any:\n    \"\"\"`a` if `a` is not None, otherwise `b`.\n    from fastai: https:\/\/github.com\/fastai\/fastai\/blob\/master\/fastai\/core.py#L92\"\"\"\n    return b if a is None else a","01a45b84":"from joblib import Parallel, delayed\nfrom collections import defaultdict\nimport copy\n\n\nclass FeatureGenerator(object):\n    def __init__(self, n_jobs=1, df=None, dataset: str = 'df_train'):\n        self.n_jobs = n_jobs\n        self.df = df\n        self.dataset = dataset\n\n    def read_chunks(self):\n        for id, user_sample in self.df.groupby('installation_id', sort=False):\n            yield id, user_sample\n\n    def get_features(self, row):\n        \"\"\"\n        Gets three groups of features: from original data and from read and imaginary parts of FFT.\n        \"\"\"\n        return self.features(row)\n\n    def features(self, id, user_sample):\n        user_data = []\n\n        accuracy_mapping = {0: 0, 1: 3, 0.5: 2}\n\n        user_stats = defaultdict(int)\n        user_stats['installation_id'] = user_sample['installation_id'].unique()[0]\n        user_stats['world'] = user_sample['world'].unique()[0]\n        user_stats['timestamp'] = user_sample['timestamp'].unique()[0]\n\n        temp_dict = defaultdict(int)\n        another_temp_dict = {}\n        another_temp_dict['durations'] = []\n        another_temp_dict['all_durations'] = []\n        another_temp_dict['durations_with_triers'] = []\n        another_temp_dict['mean_action_time'] = []\n        title_data = defaultdict(dict)\n\n        for i, session in user_sample.groupby('game_session', sort=False):\n            user_stats['last_ass_session_game_time'] = another_temp_dict['durations'][-1] if len(another_temp_dict['durations']) > 0 else 0\n            user_stats['last_session_game_time'] = another_temp_dict['all_durations'][-1] if len(another_temp_dict['all_durations']) > 0 else 0\n\n            # calculate some user_stats and append data\n            if session['trier'].sum() > 0 or self.dataset == 'df_test':\n                user_stats['session_title'] = session['title'].values[0]\n                accuracy = np.nan_to_num(session['correct'].sum() \/ session['trier'].sum())\n                if accuracy in accuracy_mapping.keys():\n                    user_stats['accuracy_group'] = accuracy_mapping[accuracy]\n                else:\n                    user_stats['accuracy_group'] = 1\n                user_stats['accumulated_accuracy_group'] = temp_dict['accumulated_accuracy_group'] \/ user_stats['counter'] if user_stats['counter'] > 0 else 0\n                temp_dict['accumulated_accuracy_group'] += user_stats['accuracy_group']\n                user_data.append(copy.copy(user_stats))\n\n            user_stats[session['type'].values[-1]] += 1\n            user_stats['accumulated_correct_attempts'] += session['correct'].sum()\n            user_stats['accumulated_uncorrect_attempts'] += session['trier'].sum() - session['correct'].sum()\n            event_code_counts = session['event_code'].value_counts()\n            for i, j in zip(event_code_counts.index, event_code_counts.values):\n                user_stats[i] += j\n\n            temp_dict['assessment_counter'] += 1\n            if session['title'].values[-1] in title_data.keys():\n                pass\n            else:\n                title_data[session['title'].values[-1]] = defaultdict(int)\n\n            title_data[session['title'].values[-1]]['duration_all'] += session['game_time'].values[-1]\n            title_data[session['title'].values[-1]]['counter_all'] += 1\n            #user_stats['duration'] += (session['timestamp'].values[-1] - session['timestamp'].values[0]) \/ np.timedelta64(1, 's')\n\n            user_stats['duration'] = (session.iloc[-1,2] - session.iloc[0,2]).seconds\n            if session['type'].values[0] == 'Assessment' and (len(session) > 1 or self.dataset == 'df_test'):\n                another_temp_dict['durations'].append(user_stats['duration'])\n                accuracy = np.nan_to_num(session['correct'].sum() \/ session['trier'].sum())\n                user_stats['accumulated_accuracy_'] += accuracy\n                user_stats['counter'] += 1\n                if user_stats['counter'] == 0:\n                    user_stats['accumulated_accuracy'] = 0\n                else:\n                    user_stats['accumulated_accuracy'] = user_stats['accumulated_accuracy_'] \/ user_stats['counter']\n\n                accuracy = np.nan_to_num(session['correct'].sum() \/ session['trier'].sum())\n\n                if accuracy in accuracy_mapping.keys():\n                    user_stats[accuracy_mapping[accuracy]] += 1\n                else:\n                    user_stats[1] += 1\n\n                user_stats['accumulated_actions'] += len(session)\n\n                if session['trier'].sum() > 0:\n                    user_stats['sessions_with_trier'] += 1\n                    another_temp_dict['durations_with_triers'].append(user_stats['duration'])\n\n                if session['correct'].sum() > 0:\n                    user_stats['sessions_with_correct_trier'] += 1\n                    \n                user_stats['title_duration'] = title_data[session['title'].values[-1]]['duration']\n                user_stats['title_counter'] = title_data[session['title'].values[-1]]['counter']\n                user_stats['title_mean_duration'] = user_stats['title_duration'] \/ user_stats['title_mean_duration']  if user_stats['title_mean_duration'] > 0 else 0\n\n                user_stats['title_duration_all'] = title_data[session['title'].values[-1]]['duration_all']\n                user_stats['title_counter_all'] = title_data[session['title'].values[-1]]['counter_all']\n                user_stats['title_mean_duration_all'] = user_stats['title_duration_all'] \/ user_stats['title_mean_duration_all']  if user_stats['title_mean_duration_all'] > 0 else 0\n                \n                title_data[session['title'].values[-1]]['duration'] += session['game_time'].values[-1]\n                title_data[session['title'].values[-1]]['counter'] += 1\n\n            elif (len(session) > 1 or self.dataset == 'df_test'):\n                another_temp_dict['all_durations'].append(user_stats['duration'])\n\n\n            if user_stats['duration'] != 0:\n                temp_dict['nonzero_duration_assessment_counter'] += 1\n            #user_stats['duration_mean'] = user_stats['duration'] \/ max(temp_dict['nonzero_duration_assessment_counter'], 1)\n            # stats from assessment sessions\n            user_stats['duration_mean'] = np.mean(another_temp_dict['durations'])\n            user_stats['duration_trier'] = np.mean(another_temp_dict['durations_with_triers'])\n\n            # stats from all sessions\n            user_stats['all_duration_mean'] = np.mean(another_temp_dict['all_durations'])\n            user_stats['all_accumulated_actions'] += len(session)\n            user_stats['mean_action_time'] = np.mean(another_temp_dict['mean_action_time'])\n            another_temp_dict['mean_action_time'].append(session['game_time'].values[-1] \/ len(session))\n\n\n        if self.dataset == 'df_test':\n            user_data = [user_data[-1]]\n\n        return user_data\n\n    def generate(self):\n        feature_list = []\n#         res = Parallel(n_jobs=self.n_jobs, backend='threading')(delayed(self.features)(id, user_sample)\n#                                                                 for id, user_sample in tqdm(self.read_chunks(),\n#                                                                                             total=self.df[\n#                                                                                                 'installation_id'].nunique()))\n        res = Parallel(n_jobs=self.n_jobs, backend='threading')(delayed(self.features)(id, user_sample)\n                                                                for id, user_sample in self.read_chunks())\n        for r in res:\n            for r1 in r:\n                feature_list.append(r1)\n        return pd.DataFrame(feature_list)","34cc84b3":"\ndf_train[\"trier\"] = 0\ndf_train.loc[(df_train[\"title\"]==\"Bird Measurer (Assessment)\")&(df_train[\"event_code\"]==4110),\"trier\"] = 1\ndf_train.loc[(df_train[\"type\"]==\"Assessment\")&(df_train[\"title\"]!=\"Bird Measurer (Assessment)\")&(df_train[\"event_code\"]==4100),\"trier\"] = 1","aa7ea301":"from category_encoders.ordinal import OrdinalEncoder\n\ntitle_encode = OrdinalEncoder()\ntitle_encode.fit(list(set(df_train['title'].unique()).union(set(df_test['title'].unique()))));\nworld_encode = OrdinalEncoder()\nworld_encode.fit(list(set(df_train['world'].unique()).union(set(df_test['world'].unique()))));","75d7f338":"df_train['correct'] = None\ndf_train.loc[(df_train['trier'] == 1) & (df_train['event_data'].str.contains('\"correct\":true')), 'correct'] = True\ndf_train.loc[(df_train['trier'] == 1) & (df_train['event_data'].str.contains('\"correct\":false')), 'correct'] = False","d0413199":"df_train['title'] = title_encode.transform(df_train['title'].values)\ndf_train['world'] = world_encode.transform(df_train['world'].values)\ndf_train = df_train.loc[df_train['installation_id'].isin(df_train_labels['installation_id'].unique())]","e2e8fa11":"%%time\nFG = FeatureGenerator(n_jobs=2, df=df_train)\ntrain_set = FG.generate()\ntrain_set = train_set.fillna(0)","ec2718a5":"data_head(train_set)","2c2cebb5":"df_train[\"timestamp\"] = pd.to_datetime(df_train[\"timestamp\"])\n\nextracted_train_timestamp = add_datepart(df_train, \"timestamp\", drop = False)","44cd5e87":"data_head(extracted_train_timestamp)","ba6b306a":"**Writing a function on timestamps in the training set**","cceaf722":"**Visualizing the distribution of args in each rows**","31decda2":"**Modifying the missing data**","230cc103":"**extracting datas from df_specs**\nEach rows is a dictionary containg keys. these keys are name, type and info. we will generate new columns for this keys","31e360e8":"**Creating a Sample as extracted sample of about 100000 rows**\nWe will then use json package to normalize the json files in the event_data column. The value in the column will be values associated with the keys"}}