{"cell_type":{"55309c5c":"code","fed5dabe":"code","8acf121e":"code","56ae4d6e":"code","1707f5e3":"code","82d8d28c":"code","06392238":"code","8d1b58da":"code","1e79da81":"code","3e9bc161":"code","6418b11f":"code","5785b629":"code","40f6ab07":"code","d941dd6d":"code","13706862":"code","016ee3f3":"code","3f0264a9":"code","acaae054":"code","218f6575":"code","5b749425":"code","ed1ff903":"code","d98fd6fa":"code","8e2bdf41":"code","23b301ff":"code","29cf8b78":"code","cab37013":"code","4c065681":"code","d327a122":"code","a5227b5e":"code","72b23501":"code","25c5fc60":"code","18ce47a4":"code","2ef89d1f":"code","d74e22df":"code","ac3d89b5":"code","8cee2093":"code","f588b9d8":"code","7b15c0ca":"code","39965efe":"code","c4e8a7ef":"code","e394f91f":"code","e3c12a03":"code","5303ded8":"code","5105afce":"code","2125924b":"code","3409af1c":"code","9e57e165":"code","1665c648":"code","ddc92276":"code","f8e79fb3":"code","1a6c7bdc":"code","129dc876":"code","60304f2d":"code","ffcc833e":"code","9525e585":"code","f3c217f0":"code","853a9e42":"code","f162b0e0":"code","4826da3c":"code","37e69dc8":"code","714a21b7":"markdown","04a8f274":"markdown","d5fb58df":"markdown","fe8a5ace":"markdown","59a67b3b":"markdown","cb01903d":"markdown","e0bc5e49":"markdown","baf6ece5":"markdown","2ab37ecb":"markdown","5c82d7b3":"markdown","2e2fe804":"markdown","6e0f2cd1":"markdown","4582253e":"markdown","d64ea410":"markdown","8b3810dc":"markdown"},"source":{"55309c5c":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom pandas import read_csv\nimport seaborn as sns\nimport time\n\nfrom collections import Counter\nfrom scipy import stats\nimport random\n","fed5dabe":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:#tqdm(df.columns):\n        col_type = df[col].dtypes\n\n        if col_type=='object':\n            df[col] = df[col].astype('category')\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n","8acf121e":"import pandas as pd\nrootfolder = '\/kaggle\/input\/competitive-data-science-predict-future-sales'\nitems_df = pd.read_csv(f'{rootfolder}\/items.csv')\nshops_df = pd.read_csv(f'{rootfolder}\/shops.csv')\nicats_df = pd.read_csv(f'{rootfolder}\/item_categories.csv')\nsales_train = pd.read_csv(f'{rootfolder}\/sales_train.csv')\nsmpsb_df = pd.read_csv(f'{rootfolder}\/sample_submission.csv')\ntest  = pd.read_csv(f'{rootfolder}\/test.csv')","56ae4d6e":"sales_train.head()\n","1707f5e3":"test.head()","82d8d28c":"shops_df.info()","06392238":"items_df.info()","8d1b58da":"icats_df.info()\n","1e79da81":"\n\nplt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=sales_train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(sales_train.item_price.min(), sales_train.item_price.max()*1.1)\nsns.boxplot(x=sales_train.item_price)\n\n","3e9bc161":"sns.scatterplot(x=sales_train.date,y=sales_train.date_block_num)","6418b11f":"\nsales_train = sales_train[sales_train.item_price<100000]\nsales_train = sales_train[sales_train.item_cnt_day<1001]","5785b629":"median = sales_train[(sales_train.shop_id==32)&(sales_train.item_id==2973)&(sales_train.date_block_num==4)&(sales_train.item_price>0)].item_price.median()\nsales_train.loc[sales_train.item_price<0, 'item_price'] = median","40f6ab07":"\nplt.figure(figsize=(35,10))\nsns.countplot(x='date_block_num', data=sales_train);\n\n","d941dd6d":"plt.figure(figsize=(35,10))\nsns.countplot(x='shop_id', data=sales_train)","13706862":"items_categories_merged = pd.merge(icats_df,items_df,on='item_category_id',how='left')","016ee3f3":"def exclude_preprositions(x):\n    x = x.split(' ')\n    x = ' '.join(i for i in x if not i in prepositions_to_exclude).strip()\n    return x\n","3f0264a9":"from sklearn.feature_extraction.text import TfidfVectorizer\nitems_categories_merged['type_of_category']=items_categories_merged['item_category_name'].apply(lambda x: x.split(' ')[0].strip())\ndict_types = dict(items_categories_merged['type_of_category'].value_counts())\ncat, _ = zip(*sorted(dict_types.items(),key=lambda x: x[1])[::-1][:5])\nprint('Most frequent types of categories : {0}'.format(cat))\nnum_features = 10\nsymbols_to_exclude = ['[',']','!','.',',','*','(',')','\"',':']\nprepositions_to_exclude = ['\u0432','\u043d\u0430','\u0443','the','a','an','of','\u0434\u043b\u044f']\nfor symbol in symbols_to_exclude:\n    items_categories_merged['item_name'] = items_categories_merged['item_name'].str.replace(symbol,'')\nitems_categories_merged['item_name'] = items_categories_merged['item_name'].str.lower()\nitems_categories_merged['item_name'] = items_categories_merged['item_name'].str.replace('-',' ')\nitems_categories_merged['item_name'] = items_categories_merged['item_name'].str.replace('\/',' ')\nitems_categories_merged['item_name'] = items_categories_merged['item_name'].str.strip()\nitems_categories_merged['item_name'] = items_categories_merged['item_name'].apply(exclude_preprositions)\nvectorizer = TfidfVectorizer(max_features=num_features)\nres = vectorizer.fit_transform(items_categories_merged['item_name'])\nprint('Top {0} features of tfidf : {1}'.format(num_features,vectorizer.get_feature_names()))\ncount_vect_df = pd.DataFrame(res.todense(), columns=vectorizer.get_feature_names())\nitems_categories_merged = pd.concat([items_categories_merged,count_vect_df],axis=1)\n","acaae054":"items_categories_merged.drop(columns=['item_name','item_category_name'],inplace=True)","218f6575":"\n\nimport re\ndef create_city_name(x):\n    for i in not_city:\n        if i in x:\n            return 'unk_city'\n    return x.split(' ')[0].strip()\ndef create_shop_type(x):\n    to_return = 'unk_type'\n    for i in type_of_shops:\n        regex = re.compile(i)\n        if re.search(regex,x):\n                to_return = i \n    return to_return\n\nnot_city = ['\u0412\u044b\u0435\u0437\u0434\u043d\u0430\u044f \u0422\u043e\u0440\u0433\u043e\u0432\u043b\u044f','\u0418\u043d\u0442\u0435\u0440\u043d\u0435\u0442-\u043c\u0430\u0433\u0430\u0437\u0438\u043d','\u0426\u0438\u0444\u0440\u043e\u0432\u043e\u0439 \u0441\u043a\u043b\u0430\u0434 1\u0421-\u041e\u043d\u043b\u0430\u0439\u043d']\ntype_of_shops = ['\u0422\u0420\u0426', '\u0422\u0426','\u0422\u0420\u041a','\u0422\u041a','\u041c\u0422\u0420\u0426']+not_city\nshops_df['city_name'] = shops_df['shop_name'].apply(create_city_name)\nshops_df['shop_type'] = shops_df['shop_name'].apply(create_shop_type)\n\n","5b749425":"\nshops_df.drop(columns='shop_name',inplace=True)\n\n","ed1ff903":"shops_df.goup()","d98fd6fa":"train_sales = sales_train","8e2bdf41":"    train_sales[\"date\"] = pd.to_datetime(train_sales[\"date\"], format=\"%d.%m.%Y\") # seting the column as pandas datetime\n    train_sales[\"month\"] = train_sales['date'].dt.day # extracting month\n    train_sales.info()","23b301ff":"\n\ny_hat_mean = (1.41241**2-1.25011**2-1)\/-2\nprint('Mean of target values in public leaderboard is : {0}'.format(y_hat_mean))\n\n","29cf8b78":"\n\nlen(list(set(test.item_id) - set(test.item_id).intersection(set(test.item_id)))), len(list(set(test.item_id))), len(test)\n\n","cab37013":"\nmean = sales_train.groupby(['date_block_num','shop_id','item_id'])['item_cnt_day'].sum().mean()\nprint('Mean of target value in train data : {0}'.format(mean))\nif np.abs(mean-y_hat_mean)<0.2:\n    print('The mean of train and test targets is aligned!')\nelse:\n    print('The mean of train and test targets is not aligned!')\n\n","4c065681":"from itertools import product\nmatrix = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = train_sales[train_sales.date_block_num==i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)","d327a122":"train_sales['revenue'] = train_sales['item_price'] *  train_sales['item_cnt_day']","a5227b5e":"group = train_sales.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float16))","72b23501":"train_sales['shop_id'] = train_sales['shop_id'].astype(np.int8)\ntrain_sales['item_id'] = train_sales['item_id'].astype(np.int16)\ntrain_sales['date_block_num'] = train_sales['date_block_num'].astype(np.int8)","25c5fc60":"test['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)\n\n","18ce47a4":"matrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True) # 34 month","2ef89d1f":"matrix.head()","d74e22df":"matrix = pd.merge(matrix, shops_df, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, items_categories_merged, on=['item_id'], how='left')","ac3d89b5":"matrix","8cee2093":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","f588b9d8":"matrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')","7b15c0ca":"matrix.head()","39965efe":"group = matrix.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num'], how='left')\nmatrix['date_avg_item_cnt'] = matrix['date_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_avg_item_cnt')\nmatrix.drop(['date_avg_item_cnt'], axis=1, inplace=True)","c4e8a7ef":"group = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_cnt'] = matrix['date_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_item_avg_item_cnt')\nmatrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)","e394f91f":"group = matrix.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_shop_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_avg_item_cnt'] = matrix['date_shop_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_shop_avg_item_cnt')\nmatrix.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)","e3c12a03":"group = matrix.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_cat_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_category_id'], how='left')\nmatrix['date_cat_avg_item_cnt'] = matrix['date_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_cat_avg_item_cnt')\nmatrix.drop(['date_cat_avg_item_cnt'], axis=1, inplace=True)","5303ded8":"group = matrix.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_cat_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\nmatrix['date_shop_cat_avg_item_cnt'] = matrix['date_shop_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_cat_avg_item_cnt')\nmatrix.drop(['date_shop_cat_avg_item_cnt'], axis=1, inplace=True)","5105afce":"matrix['month'] = matrix['date_block_num'] % 12","2125924b":"def fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nmatrix = fill_na(matrix)","3409af1c":"matrix","9e57e165":"matrix.info()","1665c648":"to_encode = ['city_name','shop_type','type_of_category']\nnunique_cat = {}\nfor i in to_encode:\n    matrix[i] = matrix[i].factorize()[0]\n    nunique_cat.update({i:matrix[i].nunique()})\nnunique_cat.update({'shop_id':matrix['shop_id'].nunique()})\nnunique_cat.update({'item_id':matrix['item_id'].nunique()})\nnunique_cat.update({'item_category_id':matrix['item_category_id'].nunique()})\nprint('Factorized all the columns!')","ddc92276":"def downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df\n\ndataset = downcast_dtypes(matrix)","f8e79fb3":"dataset.to_pickle('final_dataset.pkl')","1a6c7bdc":"import pandas as pd\nimport lightgbm as lgb\nfrom lightgbm import plot_importance","129dc876":"dataset = pd.read_pickle('final_dataset.pkl')","60304f2d":"dataset.head()","ffcc833e":"dataset.info()","9525e585":"columnsNames = ['date_block_num',                  \n'shop_id',                          \n'item_id',                          \n'item_cnt_month',                                          \n'item_category_id',                 \n'type_of_category',   \n'city_name',               \n'bd',                               \n'cd',                               \n'dvd',                              \n'jewel',                            \n'mp3',                              \n'pc',                               \n'\u0432\u0435\u0440\u0441\u0438\u044f',                           \n'\u0440\u0435\u0433\u0438\u043e\u043d',                           \n'\u0440\u0443\u0441\u0441\u043a\u0430\u044f',                          \n'\u0446\u0438\u0444\u0440\u043e\u0432\u0430\u044f',  \n'item_cnt_month_lag_1',           \n'item_cnt_month_lag_2',        \n'item_cnt_month_lag_3',         \n'item_cnt_month_lag_6',         \n'item_cnt_month_lag_12',          \n'date_avg_item_cnt_lag_1',                  \n'date_item_avg_item_cnt_lag_1',     \n'date_item_avg_item_cnt_lag_2',     \n'date_item_avg_item_cnt_lag_3',     \n'date_item_avg_item_cnt_lag_6',     \n'date_item_avg_item_cnt_lag_12',    \n'date_shop_avg_item_cnt_lag_1',     \n'date_shop_avg_item_cnt_lag_2',     \n'date_shop_avg_item_cnt_lag_3',     \n'date_shop_avg_item_cnt_lag_6',     \n'date_shop_avg_item_cnt_lag_12',    \n'date_cat_avg_item_cnt_lag_1',      \n'date_shop_cat_avg_item_cnt_lag_1', \n'month']         \n","f3c217f0":"data = dataset[columnsNames]","853a9e42":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)\n\n","f162b0e0":"from xgboost import XGBRegressor\nfrom xgboost import plot_importance","4826da3c":"model = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 10)","37e69dc8":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": X_test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)","714a21b7":"### Encoded features using mean","04a8f274":"Month","d5fb58df":"# Explore target feature\n\n","fe8a5ace":"\n\nThere is one item with price below zero. Fill it with median.\n","59a67b3b":"New feature the total price\n","cb01903d":"### Aggregate features \nWe going aggregate the features of category month and day","e0bc5e49":"# Exploring and Predicting Sales\n\n## Descrition of this competition:\nThis challenge serves as final project for the \"How to win a data science competition\" Coursera course.\n\nIn this competition you will work with a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company. \n\nWe are asking you to predict total sales for every product and store in the next month. By solving this competition you will be able to apply and enhance your data science skills.\n\n<b>Data fields<\/b><br>\n<b>ID<\/b> - an Id that represents a (Shop, Item) tuple within the test set<br>\n<b>shop_id<\/b> - unique identifier of a shop<br>\n<b>item_id<\/b> - unique identifier of a product<br>\n<b>item_category_id<\/b> - unique identifier of item category<br>\n<b>item_cnt_day<\/b> - number of products sold. You are predicting a monthly amount of this measure<br>\n<b>item_price<\/b> - current price of an item<br>\n<b>date<\/b> - date in format dd\/mm\/yyyy<br>\n<b>date_block_num<\/b> - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33<br>\n<b>item_name<\/b> - name of item<br>\n<b>shop_name<\/b> - name of shop<br>\n<b>item_category_name<\/b> - name of item category","baf6ece5":"# Machine learning part","2ab37ecb":"## Extract feature based on Categories\n","5c82d7b3":" ## Aggregate test train data\n\nWe going to aggregate the data.\n","2e2fe804":"Function utils","6e0f2cd1":"## Features based on name:","4582253e":"Shops","d64ea410":"### Feature preproccessing","8b3810dc":"Meged test set"}}