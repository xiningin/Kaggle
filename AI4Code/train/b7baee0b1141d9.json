{"cell_type":{"c292fc78":"code","642f789b":"code","015afa49":"code","db93b258":"code","a33130d4":"code","c4958b92":"code","bd1e1ed7":"code","90589dd8":"code","9f0cdee5":"code","e0bb408d":"code","6ab1047e":"code","257e65a5":"code","ad9fc73a":"code","5cd130a9":"markdown","63f749f5":"markdown","b2291737":"markdown"},"source":{"c292fc78":"!cp -r ..\/input\/efficientnet-keras-dataset\/efficientnet_kaggle .\/\n!pip install -q .\/efficientnet_kaggle\/\n!rm -r .\/efficientnet_kaggle","642f789b":"from PIL import Image\nfrom sklearn.model_selection import train_test_split\n\nimport os\nimport math\nimport pandas as pd, numpy as np, random, os\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport matplotlib.pyplot as plt\nimport tensorflow_hub as hub            #tf hub, \uc88b\uc740 \ubaa8\ub378 \uc27d\uac8c \uc0ac\uc6a9\uac00\ub2a5\ud568.\nimport efficientnet.tfkeras as efn\nimport matplotlib as mpl\nimport sklearn\n\nmpl.rcParams['figure.figsize'] = (12, 10)\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']","015afa49":"#train\uc2dc\uc5d0 \uc6b0\uce21 \uc0c1\ub2e8 \uc804\uc6d0 \ubc84\ud2bc \uc606\uc5d0\uc5d0 gpu \ub20c\ub7ec\ubcf4\uba74 gpu\uac00 \ud37c\uc13c\ud2b8 \uc0ac\uc6a9\uc911\uc778\uc9c0 \ub098\uc635\ub2c8\ub2e4\n#\uadf8\uac70\uc5d0 \ub530\ub77c \ub9ce\uc774 \ub0a8\uac70\ub098 \uc5ec\uc720\ub85c\uc6b0\uba74 batch size\ub97c \ub298\ub9ac\uc2dc\uac70\ub098 \ubaa8\ub378\uc744 \ud0a4\uc6cc\ub3c4 \ubb34\ubc29\ud569\ub2c8\ub2e4\n#\uc624\ub958\uc5d0 OOM\uc774\ub77c\ub294 \uc624\ub958\uac00 \ub098\uc624\uba74 Out of memeory \uc774\ubbc0\ub85c \uc989 gpu \uba54\ubaa8\ub9ac\uac00 \ubd80\uc871\ud558\ub2e4\ub294 \uc18c\ub9ac\ub2c8\uae4c \ubaa8\ub378\uc758 \ud06c\uae30\ub97c \uc904\uc774\uac70\ub098\n#\ubc30\uce58\uc0ac\uc774\uc988\ub97c \uc904\uc5ec\uc57c\ud569\ub2c8\ub2e4.\n#****epochs 1\uc785\ub2c8\ub2e4 \ubc14\uafb8\uc138\uc694!!!\nclass CFG:\n    seed = 7777\n    img_size = 224                     #\uc774\ubbf8\uc9c0 \uc0ac\uc774\uc988\ub294 maximum 300\uae4c\uc9c0 \ud558\uaca0\uc2b5\ub2c8\ub2e4. \ub370\uc774\ud130\uc14b\uc774 \ucee4\uc11c \ud0a4\uc6b0\uba74 \ub290\ub9ac\ub124\uc694\n    batch_size = 32                    #\ubaa8\ub378\uc0ac\uc774\uc988 \ud0a4\uc6b0\uba74 batch size \uc904\uc5ec\uc57c \ud569\ub2c8\ub2e4, \ub300\uc2e0 \ub290\ub9bd\ub2c8\ub2e4. \n    epochs = 1                   #batch size\ub294 2^n \ub2e8\uc704\ub85c \ub9de\ucdb0\uc8fc\ub294\uac8c \uc88b\uc73c\ub098 \uc5ec\uac74\uc774 \uc88b\uc9c0 \uc54a\uc73c\uba74 8\uc774\ub098 10 12 \uc774\ub7f0\uc2dd\uc73c\ub85c \ud574\ub3c4\ub429\ub2c8\ub2e4.\n    classes = 325\n    scheduler = 'exp' # Cosine or Exp      #\ud559\uc2b5 \uc2a4\ucf00\uc974\ub7ec \ubc29\uc2dd\n    dropout_rate = 0.2                 #\uc644\uc804 \uc5f0\uacb0\uce35\uc758 drop out\ube44\uc728\uc744 \uc124\uc815\ud560 \ub54c drop out -> \ub178\ub4dc\ub97c \uc57d\uac04 \ubc84\ub9bc, overfitting \uac10\uc18c\n    channels = 3\n    learning_rate = 1e-4               #\ud559\uc2b5\ub960 \n    file_path = '..\/input\/100-bird-species\/'","db93b258":"df = pd.read_csv('..\/input\/100-bird-species\/birds.csv')\nindex = pd.read_csv('..\/input\/100-bird-species\/class_dict.csv')\nindex.columns = [\"class_index\", \"labels\", \"height\", \"width\", \"scale by\"]\ndf = pd.merge(left = df, right = index, how = \"inner\", on = \"labels\").drop(['data set', 'labels', 'height', 'width', 'scale by'], axis=1)\ntrain_df, valid_df = train_test_split(df, test_size=0.2, random_state=CFG.seed, stratify=df.class_index)\ntrain_df, test_df = train_test_split(train_df, test_size=0.25, random_state=CFG.seed, stratify=train_df.class_index)\ntrain_df.head()","a33130d4":"#\ub108\ubb34 \ub9ce\uc544\uc11c \ub300\ucda9 \uc774\uc815\ub3c4\ub9cc \ud569\ub2c8\ub2e4\ndef display_data(size = 2):\n    w = 50\n    h = 50\n    fig = plt.figure(figsize=(20, 20))\n    columns = 10\n    rows = 10\n    for i in range(0, 90):\n        img = Image.open(CFG.file_path+train_df[train_df.class_index == i].iloc[0].filepaths)\n        fig.add_subplot(rows, columns, i+1)\n        plt.imshow(img)\n    plt.show()\ndisplay_data()","c4958b92":"#random augmentation\uc774 \ub108\ubb34 \ub290\ub9ac\uace0 \uae30\ubcf8\uc801\uc778 \uc99d\uac15\ub9cc \ud558\ub354\ub77c\ub3c4 \uc131\ub2a5\uc774 \uc88b\uc544 \uae30\ubcf8\uc801\uc778 \uc99d\uac15\ub9cc \uc801\uc6a9\ud588\uc2b5\ub2c8\ub2e4\n#\ub354 \ucd94\uac00\ud558\uace0 \uc2f6\uc73c\uc2dc\uba74 \uc5ec\uae30 \ucc38\uace0 https:\/\/www.tensorflow.org\/tutorials\/images\/data_augmentation?hl=ko\n#\uac01 \uc801\uc6a9\uc758 \uacb0\uacfc\uac00 \uad81\uae08\ud558\uc2dc\uba74 \uac80\uc0c9 \u3131\u3131 \ndef data_parser(path, label, training=True, use_augment=True):\n    image = tf.io.read_file(CFG.file_path+path)\n    image = tf.image.decode_jpeg(image, channels=CFG.channels)\n    image = tf.image.resize(image, [CFG.img_size, CFG.img_size])\n    image = tf.cast(image, tf.float32) \/ 255.0\n    #\uc774\ubbf8\uc9c0\uac00  0~255\uc0ac\uc774 \uac12\uc774\ub2c8\uae4c 255\ub85c \ub098\ub220\uc11c \uc815\uaddc\ud654 \ud574\uc900\uac81\ub2c8\ub2e4\n    #\uadf8\ub7ec\ub098 \ubaa8\ub378 \uc911\uc5d0\ub294 \uc815\uaddc\ud654\uac00 \uc785\ub825\uce35\uc5d0 \ub4e4\uc5b4\uc788\ub294 \uac83\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.\n    #ex) tf.keras.applications.efficientnet.EfficientNetB0 \uc774\ub7f0 \ubaa8\ub378\ub4e4\uc740 \uc785\ub825\uce35\uc5d0 \uc815\uaddc\ud654\ub97c \ud574\uc11c \ub4e4\uc5b4\uac00\ub2c8\n    #\uc774\ub7f4 \ub584\ub294 \/ 255\uc9c0\uc6cc\uc8fc\uba74 \ub429\ub2c8\ub2e4. \uadfc\ub370 \uc544\ub9c8 \ub300\ubd80\ubd84 \ubaa8\ub378\uc740 \uc548\uadf8\ub7f4\uaebc\uc5d0\uc694.\n    label = tf.one_hot(label, depth=CFG.classes, dtype=tf.float32)\n    if training:\n        #\uac01 \uc99d\uac15\uc758 \uac15\ub3c4\ub97c \uc62c\ub9ac\uc154\ub3c4 \ub429\ub2c8\ub2e4.\n        #\uc544\ub798 \ud568\uc218 \uadf8\ub300\ub85c \uce58\uc2dc\uba74 \uac01\uac01\uc758 parameter\uac00 \uc758\ubbf8\ud558\ub294 \ubc14\ub97c \uc54c\ub824\uc90d\ub2c8\ub2e4\n        image = tf.image.random_flip_left_right(image)\n        image = tf.image.random_flip_up_down(image)\n        image = tf.image.random_saturation(image, 0.95, 1.05)\n        image = tf.image.random_brightness(image, 0.05)\n        image = tf.image.random_contrast(image, 0.95, 1.05)\n        image = tf.image.random_hue(image, 0.05)\n    return image, label\n    \ndef build_dataset(paths, labels=None, batch_size=CFG.batch_size, shuffle=1024, training=True, use_augment=True):\n    AUTO = tf.data.experimental.AUTOTUNE\n    slices = paths if labels is None else (paths, labels)\n    ds = tf.data.Dataset.from_tensor_slices(slices)\n    ds = ds.map(\n      lambda x, y: data_parser(x, y,training, use_augment),\n      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    ds = ds.shuffle(shuffle, seed=CFG.seed) if shuffle else ds\n    ds = ds.batch(batch_size) \n    ds = ds.prefetch(AUTO)\n    return ds","bd1e1ed7":"def display_batch(batch, size=2):\n    imgs, tars = batch\n    plt.figure(figsize=(size*10, 10))\n    print(tars)\n    for img_idx in range(size):\n        plt.subplot(1, size, img_idx+1)\n        #plt.title(f'{CFG.target_col[0]}: {tars[img_idx].numpy()[0]}', fontsize=15)\n        plt.imshow(imgs[img_idx,:, :, :])\n        plt.xticks([])\n        plt.yticks([])\n    plt.tight_layout()\n    plt.show() \npaths  =  train_df.filepaths.tolist()\nlabels = train_df['class_index'].values#.astype(np.float32)\nds = build_dataset(paths[-1000:], labels[-1000:],batch_size=16,shuffle=True)\nds = ds.unbatch().batch(16)\nbatch = next(iter(ds))\ndisplay_batch(batch, 5)","90589dd8":"METRICS = [\n      tf.keras.metrics.TruePositives(name='tp'),\n      tf.keras.metrics.FalsePositives(name='fp'),\n      tf.keras.metrics.TrueNegatives(name='tn'),\n      tf.keras.metrics.FalseNegatives(name='fn'), \n      tf.keras.metrics.Precision(name='precision'),\n      tf.keras.metrics.Recall(name='recall'),\n      tf.keras.metrics.AUC(name='auc'),\n      tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n]\n\ndef build_model(DIM=CFG.img_size, compile_model=True, include_top=False):\n    #\uc5ec\ub7ec\uac00\uc9c0 \uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378\ub4e4\uc744 \uc0ac\uc6a9\ud558\uae30\uc5d0\ub294 \uc815\ub9d0 \uc26c\uc6c0.\n    #\ub300\ubd80\ubd84 \ucf54\ub529 \ud55c\uc904\uc774\uba74 \uc0ac\uc6a9\uac00\ub2a5\n    #\uc5b4\ub5a4 \ubaa8\ub378\uc774 \uac00\uc7a5 \uc88b\uc744\uae4c?\n    #tensorflow hub\uc5d0 \uc88b\uc740 \ubaa8\ub378\uc774 \ub9ce\uc2b5\ub2c8\ub2e4. tf hub \uc0ac\uc6a9\ubc95\uc740 \uad49\uc7a5\ud788 \uc27d\uc2b5\ub2c8\ub2e4.\n    base = efn.EfficientNetB0(input_shape=(CFG.img_size,CFG.img_size,CFG.channels),weights='..\/input\/efficientnet-keras-dataset\/weights\/efficientnet-b0_noisy-student_notop.h5', include_top=False)\n    base.trainable=True\n    inp = base.inputs\n    out = base.output\n    out = tf.keras.layers.GlobalAveragePooling2D()(out)\n    #\ub9cc\uc57d \uc644\uc804 \uc5f0\uacb0\uce35\uc774\ub098 \ub2e4\ub978 \uce35\uc744 \ucd94\uac00\ud574\uc8fc\uace0 \uc2f6\uc73c\uba74 \uc774\uc804 \uce35\uc5d0 \ucd9c\ub825\uacfc \uc5f0\uacb0\ud574\uc918\uc57c\ud568\n    #ex) out = tf.keras.layer.Dense(50)(out) \uc774\ub7f0\uc2dd\uc73c\ub85c\n    #batch normalization\uce35\uc744 \uc5f0\uacb0\ud574\uc900\ub2e4\uba74 \uc88b\uc744\uae4c??\n    #dense\uc758 \ub274\ub7f0 \uac1c\uc218\ub97c \ub298\ub9ac\uba74 \ud30c\ub77c\ubbf8\ud130\uc758 \uac1c\uc218\uac00 \uae09\uaca9\ud788 \uc99d\uac00\ud558\uae30 \ub54c\ubb38\uc5d0 \uc624\ud788\ub824 \ub354 \uc88b\uc740 \ubaa8\ub378\uc744 \ud0dd\ud558\ub294 \uac83\uc774 \uc88b\uc2b5\ub2c8\ub2e4.\n    out = tf.keras.layers.Dense(512,activation='relu')(out)\n    out = tf.keras.layers.Dropout(CFG.dropout_rate)(out) #drop out\uac15\ub3c4\ub97c \uc62c\ub9b0\ub2e4\uba74 \uc88b\uc544\uc9c8\uae4c?\n    #\ub9c8\uc9c0\ub9c9 \uce35\uc740 softmax \uace0\uc815\n    #\uadf8\uc804\uc5d0 Dense\uce35\uc758 \ud65c\uc131\ud568\uc218\ub294 \uc790\uc720, relu\ubcf4\ub2e4 \uc88b\uc740 \ud65c\uc131 \ud568\uc218\uac00 \uc788\uc744\uc9c0\ub3c4???\n    #\uc644\uc804 \uc5f0\uacb0\uce35\uc744 \ucd94\uac00\ud560 \uc218 \ub3c4 \uc788\uc9c0\ub9cc overfitting\uc774 \ubc1c\uc0dd\ud560 \ud655\ub960 \uc62c\ub77c\uac10.\n    out = tf.keras.layers.Dense(CFG.classes,activation='softmax')(out)\n    model = tf.keras.Model(inputs=inp, outputs=out)\n    if compile_model:\n        #optimizer\n        #adam\ubcf4\ub2e4 \uc88b\uc740 \ud559\uc2b5 \uc54c\uace0\ub9ac\uc998\uc774 \uc788\uc744\uae4c???\n        #\ud559\uc2b5\ub960\uc740 \uc5b4\ub5a8\uae4c? \uc5b4\ub5bb\uac8c \uc124\uc815\ud574\uc57c\ud558\uc9c0  *\uc8fc\uc758* pretrain\ub41c \ubaa8\ub378\uc744 \ud6c8\ub828\ud558\uae30\uc5d0 \uae30\uc874\uc758 \uac00\uc911\uce58\uac00 \uc5c6\ub294 \ubaa8\ub378\uc758 \ud559\uc2b5\ub960\uacfc\ub294 \ucc28\uc774\uac00 \uc874\uc7ac\n        optimizer = tf.keras.optimizers.Adam(learning_rate=CFG.learning_rate)\n        #loss\n        loss = tf.keras.losses.CategoricalCrossentropy()\n        model.compile(optimizer=optimizer,\n                      loss=loss,\n                      metrics=['accuracy',METRICS])\n    return model","9f0cdee5":"tmp = build_model()\ntmp.summary()\ndel(tmp)","e0bb408d":"def plot_metrics(history):\n    metrics = ['loss', 'auc', 'precision', 'recall']\n    for n, metric in enumerate(metrics):\n        name = metric.replace(\"_\",\" \").capitalize()\n        plt.subplot(2,2,n+1)\n        plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n        plt.plot(history.epoch, history.history['val_'+metric],\n             color=colors[0], linestyle=\"--\", label='Val')\n        plt.xlabel('Epoch')\n        plt.ylabel(name)\n        if metric == 'loss':\n            plt.ylim([0, plt.ylim()[1]])\n        elif metric == 'auc':\n            plt.ylim([0.8,1])\n        else:\n            plt.ylim([0,1])\n        plt.legend()","6ab1047e":"#\uc774\uc678\uc5d0\ub3c4 \ub2e4\ub978 \uc2a4\ucf00\uc974\ub7ec\ub4e4\uc774 \ub9ce\uc2b5\ub2c8\ub2e4.\n#google\uc5d0 \uac80\uc0c9\ud558\uc2dc\uba74 \ub098\uc635\ub2c8\ub2e4\ndef get_lr_callback(batch_size=8, plot=False):\n    lr_start   = CFG.learning_rate                  \n    lr_max     = 0.0000125 * batch_size\n    lr_min     = CFG.learning_rate                  \n    lr_ramp_ep = 5                      #\uba87\ubc88\uc9f8 \uc8fc\uae30\ubd80\ud130 \ubcc0\ud654\ub97c \uc904\uac74\uc9c0 \uc124\uc815 \n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) \/ lr_ramp_ep * epoch + lr_start\n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n            \n        elif CFG.scheduler=='exp':\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            \n        elif CFG.scheduler=='cosine':\n            decay_total_epochs = CFG.epochs - lr_ramp_ep - lr_sus_ep + 3\n            decay_epoch_index = epoch - lr_ramp_ep - lr_sus_ep\n            phase = math.pi * decay_epoch_index \/ decay_total_epochs\n            cosine_decay = 0.5 * (1 + math.cos(phase))\n            lr = (lr_max - lr_min) * cosine_decay + lr_min\n        return lr\n    if plot:\n        plt.figure(figsize=(10,5))\n        plt.plot(np.arange(CFG.epochs), [lrfn(epoch) for epoch in np.arange(CFG.epochs)], marker='o')\n        plt.xlabel('epoch'); plt.ylabel('learnig rate')\n        plt.title('Learning Rate Scheduler')\n        plt.show()\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n    return lr_callback\n\n_=get_lr_callback(CFG.batch_size, plot=True )","257e65a5":"# CREATE TRAIN AND VALIDATION SUBSETS\ntrain_paths = train_df.filepaths.values; train_labels = train_df['class_index'].values\nvalid_paths = valid_df.filepaths.values; valid_labels = valid_df['class_index'].values  \n# SHUFFLE IMAGE AND LABELS\n\nindex = np.arange(len(train_paths))\nnp.random.shuffle(index)\ntrain_paths  = train_paths[index]\ntrain_labels = train_labels[index]\n\nmodel = build_model(DIM=CFG.img_size, compile_model=True)\n\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath='.\/train\/cp.ckpt',  #\uac00\uc7a5 \uc88b\uc740 \ubaa8\ub378\ub9cc \uc800\uc7a5\ud574\uc8fc\ub294 callback\n                                                 save_weights_only=True,      #callback -> \ud6c8\ub828\uc911\uc5d0 \uc2e4\ud589\ud574\uc8fc\ub294 \ud568\uc218\n                                                 verbose=1)\nlr_callback = get_lr_callback(CFG.batch_size)  #learning rate scheduler\n##################################################################################\n#early stopping callback                                                         #\n#validation set\uc5d0 \ub300\ud55c accuracy\uac00 \ub354 \uc774\uc0c1 \uc99d\uac00\ud558\uc9c0 \uc54a\uc73c\uba74 \ud6c8\ub828\uc744 \uba48\ucdb0\uc8fc\ub294 callback\uc774 \uc788\ub2e4??  #\n#\uc5ec\ub7ec\ubd84\ub4e4\uc774 \uad73\uc774 \ud655\uc778\ud558\uc9c0 \uc54a\uc544\ub3c4 \uc54c\uc544\uc11c \uba48\ucdb0\uc8fc\uae30 \ub54c\ubb38\uc5d0 \uc544\uc8fc \uc88b\uc2b5\ub2c8\ub2e4                           #\n##################################################################################\ntrain_ds = build_dataset(train_paths, train_labels, batch_size=CFG.batch_size, #dataset \uc0dd\uc131\ud568\uc218\n                shuffle=True, training=True, use_augment=True)\nval_ds   = build_dataset(valid_paths, valid_labels, batch_size=CFG.batch_size, #validation set\uc740 \ub370\uc774\ud130 \uc99d\uac15 \uc0ac\uc6a9 x\n                shuffle=False, training=False, use_augment=False)\n\nprint('#'*25)   \nprint('Training...')\nhistory = model.fit(  #\ubaa8\ub378 fitting \uc2dc\uc791, \ud074\ub798\uc2a4\uac00 \ub9ce\uc544 \ucd08\uae30\uc815\ud655\ub3c4\uac00 \ub0ae\uc2b5\ub2c8\ub2e4. \uc624\ub958\uc544\ub2d9\ub2c8\ub2e4\n        train_ds,  \n        epochs=CFG.epochs, \n        callbacks = [cp_callback, lr_callback],  #\uc0ac\uc6a9\ud558\uace0 \uc2f6\uc740 callback\uc815\uc758\ud558\uace0 \ub9ac\uc2a4\ud2b8\uc5d0 \ucd94\uac00\ud574\uc8fc\uba74 \ub05d\n        batch_size=CFG.batch_size,\n        validation_data=val_ds,\n        verbose=1)\n\n#plot_metrics(history) #\ud6c8\ub828 \uacb0\uacfc \uc2dc\uac01\ud654","ad9fc73a":"test_paths = test_df.filepaths.values; test_labels = test_df['class_index'].values\n\ntest_ds   = build_dataset(test_paths, test_labels, batch_size=CFG.batch_size,\n                shuffle=False, training=False, use_augment=False)\n\ntest_predictions_baseline = model.predict(test_ds, batch_size=CFG.batch_size)\n\nbaseline_results = model.evaluate(test_ds, verbose=0, batch_size=CFG.batch_size) #\uc131\ub2a5\ud3c9\uac00\nfor name, value in zip(model.metrics_names, baseline_results):\n    print(name, ': ', value)\nprint()","5cd130a9":"define data pipe-line","63f749f5":"# Tensorflow","b2291737":"visualize intermiate layer"}}