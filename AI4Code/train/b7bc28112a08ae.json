{"cell_type":{"bd6bec7b":"code","ba6f324d":"code","b88cc76b":"code","f7f64286":"code","3d17ba20":"code","ae863a19":"code","820d7d27":"code","fc3101b1":"code","1d6ca8cf":"code","a307a83d":"code","a9faac4a":"code","d33a5e01":"code","dd144b16":"code","9c0e7d32":"code","10a9bb7b":"code","55c676ef":"code","f58f81e8":"code","999aee4c":"code","cd700675":"code","d79793c1":"code","13dae14f":"code","e12ba2fa":"code","ec4c793e":"code","5aca0cf8":"code","98f4c1a0":"code","ea416123":"code","94eb9dd0":"code","aa7b9eb8":"code","b5dcf20b":"code","c0082fa6":"code","5ac58d93":"code","4f9e3573":"code","4aff5ba8":"markdown","af881dbb":"markdown","94a298b9":"markdown","d7f39b94":"markdown","b987dbb4":"markdown","1c70f8cd":"markdown","5967ebc9":"markdown","13576d9f":"markdown","903f4376":"markdown","095a5ccf":"markdown","5fdbcee4":"markdown","2ccbaa35":"markdown","179584e5":"markdown","12c5caa0":"markdown","dc95a76e":"markdown","4a3a967f":"markdown","e5acd86d":"markdown","1a394480":"markdown","1c42108d":"markdown"},"source":{"bd6bec7b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, make_scorer\nfrom sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve, plot_roc_curve\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder, Binarizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.tree import export_graphviz\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom lightgbm import LGBMRegressor\n\nimport os","ba6f324d":"F1_data = pd.read_csv('..\/input\/f1-race-by-race-19832021\/final.csv')\nF1_data","b88cc76b":"# Check features 'F1_data' contains\nprint('Features: ', F1_data.columns)\nF1_data.head()","f7f64286":"# Check whether 'crimes' contains any Null or NaN values\nF1_data.isnull().sum()","3d17ba20":"# Additional arranging 'F1_data'\nF1_data.sort_values(by=['season', 'round'], ascending=True, inplace=True)\nF1_data.drop('Unnamed: 0', axis=1, inplace=True)\nF1_data.reset_index(drop=True, inplace=True)\nF1_data","ae863a19":"# Create another DataFrame for training\nF1_data_train = F1_data.copy()\nF1_data_train.drop(['season', 'round', 'circuit_id', 'driver', 'nationality', 'constructor'], axis=1, inplace=True)\n\nweathers = ['weather_warm', 'weather_cold', 'weather_dry', 'weather_wet', 'weather_cloudy']\nfor weather in weathers:\n    F1_data_train[weather] = F1_data_train[weather].apply(lambda x : 1 if x == True else 0)\n\nF1_data_train","820d7d27":"# Check corrleation of 'F1_data'\nplt.figure(figsize=(15, 15))\nplt.title('Corrleation Heatmap of F1_data')\nsns.heatmap(F1_data.corr(), annot=True, fmt='.1g', linewidths=.3)","fc3101b1":"features_dec = ['grid', 'podium', 'driver_points', 'driver_standings_pos', 'constructor_points', 'constructor_wins', 'constructor_standings_pos']\nF1_data_scaled = F1_data.copy()\n\nscaler = StandardScaler()\nF1_data_scaled = scaler.fit_transform(F1_data[features_dec])\npca = PCA(n_components=2)\npca.fit(F1_data_scaled)\nprint('Variability by PCA Components: ', pca.explained_variance_ratio_)","1d6ca8cf":"# Set X, y as features and label\nF1_data_scaled = F1_data_train.copy()\n\nX_features = F1_data_scaled.drop('driver_wins', axis=1, inplace=False)\ny_label = F1_data_scaled['driver_wins']\nprint('Shape of X_features: {0} \/ Shape of y_label: {1}'.format(X_features.shape, y_label.shape))","a307a83d":"# Evaluate accuracy score without PCA\nrf_clf = RandomForestClassifier(n_estimators=300, random_state=11)\nscores = cross_val_score(rf_clf, X_features, y_label, scoring='accuracy', cv=3)\n\nprint('Accuracy by each fold: ', scores)\nprint('Average Accuracy: {0:.4f}'.format(np.mean(scores)))","a9faac4a":"# Evaluate accuracy score with PCA\npca = PCA(n_components=7)\ndf_pca = pca.fit_transform(X_features)\nscores_pca = cross_val_score(rf_clf, df_pca, y_label, scoring='accuracy', cv=3)\n\nprint('Accuracy by each fold PCA converted: ', scores_pca)\nprint('Average Accuracy PCA converted: {0:.4f}'.format(np.mean(scores_pca)))","d33a5e01":"# Extract data for plotting distribution\nF1_data_weather = F1_data.iloc[:, [3, 4, 5, 6, 7]]\n\n# Check distribution of each features\nfig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12, 6))\n\nfor i, feature in enumerate(F1_data_weather.columns):\n    row = int(i\/3)\n    col = i%3\n    sns.distplot(F1_data_weather.iloc[:, i], ax=axs[row][col])\n\nplt.suptitle('Distirbution of Weather by Density Plot')\nplt.tight_layout","dd144b16":"# Pie Charts\nfig, axs = plt.subplots(ncols = 5, figsize=(30, 6))\n\nfor i, weather in enumerate(F1_data_weather.columns):\n    cols = i%5\n    pd.value_counts(F1_data_weather[weather]).plot.pie(autopct=\"%.1f%%\", ax=axs[cols])\n\nplt.suptitle('Distribution of Weather by Pie charts')\nplt.tight_layout()","9c0e7d32":"# Create DataFrame for Pie Chart\nF1_data_nation = pd.DataFrame(data=F1_data['nationality'].value_counts())\nF1_data_nation.reset_index(inplace=True)\nF1_data_nation.rename({'index' : 'nationality', 'nationality' : 'count'}, axis=1, inplace=True)\nF1_data_nation.sort_values(by='count', ascending=False, inplace=True)\nF1_data_nation.head()","10a9bb7b":"# Encode features\nle = LabelEncoder()\n\nle.fit(F1_data_nation['nationality'])\nF1_data_nation['nationality_le'] = le.transform(F1_data_nation['nationality'])\nF1_data_nation.head()","55c676ef":"# Pie Chart\npie, ax = plt.subplots(figsize=[10, 10])\nlabels = F1_data_nation['nationality']\nplt.pie(F1_data_nation['count'], autopct=\"%.1f%%\", labels=labels, pctdistance=0.5)\nplt.title(\"Distribution of Nationality\", fontsize=14)","f58f81e8":"# Pie Chart\npie, ax = plt.subplots(figsize=[10, 10])\nlabels = F1_data_nation.iloc[0:10, 0]\nplt.pie(F1_data_nation.iloc[0:10, 1], autopct=\"%.1f%%\", labels=labels, pctdistance=0.5)\nplt.title(\"Distribution of Nationality [Top 10]\", fontsize=14)","999aee4c":"# Plot density plot\nplt.figure(figsize=(10, 6))\nplt.title('Distribution of Age')\nsns.distplot(F1_data['driver_age'])","cd700675":"# Scaling\nfor feature in F1_data_train.columns:\n    scaler = StandardScaler()\n    scaler = scaler.fit(np.array(F1_data_train[feature]).reshape(-1, 1))\n    F1_data_train[feature] = scaler.transform(np.array(F1_data_train[feature]).reshape(-1, 1))\n\nF1_data_train","d79793c1":"# Set X, y for features and label\nX = F1_data_train.drop('driver_wins', axis=1, inplace=False)\ny = F1_data_train['driver_wins'].astype(int)","13dae14f":"# Split datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11)\n\nprint('Shape of X_train: ', X_train.shape)\nprint('Shape of X_test: ', X_test.shape)\nprint('Shape of y_train: ', y_train.shape)\nprint('Shape of y_test: ', y_test.shape)","e12ba2fa":"# Utility Function\ndef get_clf_eval_binary(y_test, pred=None, pred_proba=None):\n    confusion = confusion_matrix(y_test, pred)\n    accuracy = accuracy_score(y_test, pred)\n    precision = precision_score(y_test, pred, average='macro')\n    recall = recall_score(y_test, pred, average='macro')\n    f1 = f1_score(y_test, pred, average='macro')\n    roc_auc = roc_auc_score(y_test, pred_proba, multi_class='ovr', average='macro')\n    print('Confusion Matrix')\n    print(confusion)\n    print('Accuracy: {0:.4f}, Precision: {1:.4f}, Recall {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))","ec4c793e":"# Utility Function\ndef get_clf_eval_multiclass(y_test, pred=None, pred_proba=None):\n    confusion = confusion_matrix(y_test, pred)\n    accuracy = accuracy_score(y_test, pred)\n    precision = precision_score(y_test, pred, average='macro')\n    recall = recall_score(y_test, pred, average='macro')\n    f1 = f1_score(y_test, pred, average='macro')\n    #roc_auc = roc_auc_score(y_test, pred_proba, multi_class='ovo', average='macro')\n    print('Confusion Matrix')\n    print(confusion)\n    print('Accuracy: {0:.4f}, Precision: {1:.4f}, Recall {2:.4f}, F1: {3:.4f}'.format(accuracy, precision, recall, f1))","5aca0cf8":"# Plot Function\ndef precision_recall_curve_plot(y_test, pred_proba_c1):\n    # Extarct ndarray of threshold and ndarray of precision, recall by itself\n    precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_c1)\n\n    # Set X axis for threshold values, Y axis for precision, recall and create plot\n    plt.figure(figsize=(8, 6))\n    threshold_boundary = thresholds.shape[0]\n    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label='precision')\n    plt.plot(thresholds, recalls[0:threshold_boundary], label='recall')\n\n    # Scaling threshold values of 0.1 units on X axis\n    start, end = plt.xlim()\n    plt.xticks(np.round(np.arange(start, end, 0.1), 2))\n\n    # Set labels of X axis, y axis, legend and grid\n    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')\n    plt.legend()\n    plt.grid()\n    plt.show()","98f4c1a0":"# Plot Function\ndef roc_curve_plot(y_test, pred_proba_c1):\n    # Return values of FPR, TPR by thresholds\n    fprs, tprs, thresholds = roc_curve(y_test, pred_proba_c1)\n    # Plot ROC curve\n    plt.plot(fprs, tprs, label='ROC')\n    # Plot diagonal line\n    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n\n    # Scaling threshold values of 0.1 units on X axis(FPR)\n    start, end = plt.xlim()\n    plt.xticks(np.round(np.arange(start, end, 0.1), 2))\n    plt.xlim(0, 1)\n    plt.ylim(0, 1)\n    # Set label of X, Y axis\n    plt.xlabel('FPR(1 - Sensitivity)')\n    plt.ylabel('TPR(Recall)')\n    plt.legend()\n\n    plt.show()","ea416123":"# Create Estimator CLass\nlr_clf = LogisticRegression(solver='liblinear')\n\n# Fitting\nlr_clf.fit(X_train, y_train)\n\n# Prediction\nlr_pred = lr_clf.predict(X_test)\n\n# Pred_Proba\nlr_pred_proba = lr_clf.predict_proba(X_test)\n\n# Evaluation\nget_clf_eval_multiclass(y_test, lr_pred, lr_pred_proba)","94eb9dd0":"# For binary classification, we should make 'driver_wins' as binary values(0, 1)\nbinarizer = Binarizer(threshold=F1_data_train['driver_wins'].median())\nF1_data_train['driver_wins'] = binarizer.fit_transform(np.array(F1_data_train['driver_wins']).reshape(-1, 1))\nF1_data_train","aa7b9eb8":"# Set X, y for features and label\nX = F1_data_train.drop('driver_wins', axis=1, inplace=False)\ny = F1_data_train['driver_wins'].astype(int)","b5dcf20b":"# Split datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11)\n\nprint('Shape of X_train: ', X_train.shape)\nprint('Shape of X_test: ', X_test.shape)\nprint('Shape of y_train: ', y_train.shape)\nprint('Shape of y_test: ', y_test.shape)","c0082fa6":"# Process fitting, prediction and evalution by Logistic Regression\n# Create Estimator CLass\ndt_clf = DecisionTreeClassifier()\nlr_clf = LogisticRegression(solver='liblinear')\nrf_clf = RandomForestClassifier()\n\n# Fitting\ndt_clf.fit(X_train, y_train)\nlr_clf.fit(X_train, y_train)\nrf_clf.fit(X_train, y_train)\n\n# Prediction\ndt_pred = dt_clf.predict(X_test)\nlr_pred = lr_clf.predict(X_test)\nrf_pred = rf_clf.predict(X_test)\n\n# Pred_Proba\ndt_pred_proba = dt_clf.predict_proba(X_test)[:, 1]\nlr_pred_proba = lr_clf.predict_proba(X_test)[:, 1]\nrf_pred_proba = rf_clf.predict_proba(X_test)[:, 1]\n\n# Evaluation\nget_clf_eval_binary(y_test, dt_pred, dt_pred_proba)\nget_clf_eval_binary(y_test, lr_pred, lr_pred_proba)\nget_clf_eval_binary(y_test, rf_pred, rf_pred_proba)","5ac58d93":"# DecisionTreeClassifier\nprecision_recall_curve_plot(y_test, dt_pred_proba)\n# LogisitcRegression\nprecision_recall_curve_plot(y_test, lr_pred_proba)\n# RandomForestClassifier\nprecision_recall_curve_plot(y_test, rf_pred_proba)","4f9e3573":"# Plot ROC-AUC Curve\n# DecisionTreeClassifier\nroc_curve_plot(y_test, dt_pred_proba)\n# LogisticRegression\nroc_curve_plot(y_test, lr_pred_proba)\n# RandomForestClassifier\nroc_curve_plot(y_test, rf_pred_proba)","4aff5ba8":"### Description\nThe distribution of age of drivers were much similiar as Normal Distribution which is compatible for training.","af881dbb":"## Summary\nThe number of columns was decreased from 14 to 7, so the decreased rate is 50%  \nBut, the accuracy score was decreased from 88.99% to 83.44%, which means 6% decreased  ","94a298b9":"### Description\nUsually, countries in Europe were superior to other countries.","d7f39b94":"# Data Preprocessing","b987dbb4":"## Binary Classification","1c70f8cd":"# Import libraries and data","5967ebc9":"## Description\nWe can explain explain the variance of 7 features with 2 PCA components  \nThe total variance is about 73% and the first axis was the highest with 55%","13576d9f":"# Create Datasets","903f4376":"# Visualization","095a5ccf":"# Decomposition","5fdbcee4":"# Classification","2ccbaa35":"## Description\nAs you can see heatmap above, featrues from 'grid' to 'constructor_standing_pos' are heavily related with dataset.  \nTherefore, using PCA, we are gonna process decomposition","179584e5":"## MultiClass Classification","12c5caa0":"### Description\nThat plot shows the distribution of weather.  \nGenerally, it was warm and moderate","dc95a76e":"## Distribution of Nationality","4a3a967f":"## Distribution of Age","e5acd86d":"## Distribution of Weather","1a394480":"## Summary\nThe dataset has already been preprocessed, so there're nothing left to do.  \nBut, it contains 21 features which are relatively more than any other datasets.\n","1c42108d":"## Load merged data"}}