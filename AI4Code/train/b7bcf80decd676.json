{"cell_type":{"fe205b27":"code","f7e90f80":"code","c52f8bf9":"code","404215e0":"code","0de318f5":"code","53c45833":"code","2e7dbea8":"code","bdf3b72f":"code","fd957514":"code","89c5060d":"code","ec5fa993":"code","e806c299":"code","be315e36":"code","7e3201ad":"code","7b4d0488":"code","1ac5df4f":"code","88f0505d":"code","5482467b":"code","2beaf4e6":"code","6da326f9":"code","0d2415b1":"code","9f9bf96c":"code","61b2d272":"code","1fb82056":"code","983604bf":"markdown","d6064956":"markdown","d9f8aede":"markdown","cb6428fd":"markdown","c375bb19":"markdown","2f75a0b9":"markdown","7ccfcc17":"markdown","d4e94c03":"markdown","d7bbef63":"markdown","c7ee20dc":"markdown","fa763d9c":"markdown","a7823230":"markdown","54504f22":"markdown","b843a0f5":"markdown","b6f30fef":"markdown"},"source":{"fe205b27":"import pandas as pd\nimport numpy as np\nfrom keras.models import Model\nfrom keras.preprocessing.image import array_to_img\nfrom keras.layers import (Input, Dense, Dropout, Flatten, Reshape, Conv2D, UpSampling2D, Embedding,\n                          BatchNormalization, Flatten, LeakyReLU, GaussianNoise, Multiply)\nfrom keras.optimizers import Adam, SGD\nfrom keras.initializers import TruncatedNormal\nimport matplotlib.pyplot as plt","f7e90f80":"train_data = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntrain_data.label = train_data.label.astype('category')","c52f8bf9":"img_size = 28\nb_size = 32\nchannels = 1\nnum_of_classes = train_data.label.nunique()\nlatent_dim = 100\nnp.random.seed(2019)\n\ndef getImages(label, size):\n    indexes = np.random.choice(train_data[train_data.label==label].index, size)\n    rows = train_data.drop(columns=['label']).loc[indexes]\n    images = np.reshape(np.array(rows), newshape=(size,img_size,img_size,channels)) # shape in (size, width, height, channels)\n    return np.array(images)","404215e0":"all_data = []\nfor i in range(10):\n    all_data.append((getImages(i, 320+32)-127.5)\/127.5) # scale to [-1, 1]","0de318f5":"noise_input = Input(shape=(latent_dim,))\nc_input = Input(shape=(1,))\n\nembedding = Flatten()(Embedding(num_of_classes+1, num_of_classes*10)(c_input))\nmerge_layer = Multiply()([noise_input, embedding])\n \nx = Dense(7*7*256)(merge_layer)\nx = BatchNormalization()(x)\nx = LeakyReLU(0.2)(x)\nx = Reshape((7,7,256))(x)\nx = UpSampling2D()(x)\n\n# 14x14x128\nx = Conv2D(128, 5, padding='same')(x)\nx = LeakyReLU(0.2)(x)\nx = BatchNormalization()(x)\nx = UpSampling2D()(x)\n\n# 28x28x64\nx = Conv2D(64, 3, padding='same')(x)\nx = LeakyReLU(0.2)(x)\nx = BatchNormalization()(x)\n\ngenerator_output = Conv2D(channels, 3, activation='tanh', padding='same')(x)\ngenerator = Model(inputs=[noise_input, c_input], outputs=generator_output)\ngenerator.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-4, decay=1e-6))\ngenerator.summary()","53c45833":"# initial generated image\nnoise = np.random.normal(0,1,size=latent_dim)\nnoise = np.reshape(noise, newshape=(1,latent_dim))\ncondition = np.array([0])\nimage = generator.predict([noise, condition])\nimage = np.reshape(image, newshape=(img_size,img_size,channels))\nimage = array_to_img(image)\nimage.resize((128,128))","2e7dbea8":"# simple cnn model\ndiscriminator_input = Input(shape=(img_size,img_size,channels))\n\nx = GaussianNoise(0.2)(discriminator_input)\nx = Conv2D(64, 5, padding='same', kernel_initializer=TruncatedNormal(stddev=0.02))(x)\nx = BatchNormalization(momentum=0.5)(x)\nx = LeakyReLU(0.2)(x)\n\nx = GaussianNoise(0.2)(x)\nx = Conv2D(128, 5, padding='same', kernel_initializer=TruncatedNormal(stddev=0.02))(x)\nx = BatchNormalization(momentum=0.5)(x)\nx = LeakyReLU(0.2)(x)\n\nx = Flatten()(x)\nx = Dense(1000)(x)\nx = LeakyReLU(0.2)(x)\nx = Dropout(0.3)(x)\nembedding_d = Flatten()(Embedding(num_of_classes+1, num_of_classes*100)(c_input))\ndiscriminator_merge = Multiply()([x, embedding_d])\nx = Dense(128)(discriminator_merge)\nx = LeakyReLU(0.2)(x)\nx = Dropout(0.3)(x)\ndiscriminator_output = Dense(1, activation=\"sigmoid\")(x)\n\ndiscriminator = Model(inputs=[discriminator_input, c_input], outputs=discriminator_output)\ndiscriminator.trainable = True\ndiscriminator.compile(loss='binary_crossentropy', optimizer=SGD(lr=1e-3,decay=1e-6, momentum=0.9, nesterov=True))\ndiscriminator.summary()","bdf3b72f":"def getConditions(condition):\n    conditions = np.array([condition])\n    return np.reshape(np.array([conditions for i in range(b_size)]), newshape=(b_size, 1))\n\nconditions = np.array([getConditions(i) for i in range(num_of_classes)])","fd957514":"for epoch in range(100):\n    i = int(epoch%10)\n    d_loss_real = discriminator.train_on_batch([all_data[i][:32], conditions[i]], np.ones(b_size))\n    fakes = generator.predict([np.random.normal(0,1,size=(b_size, latent_dim)), conditions[i]])\n    d_loss_fake = discriminator.train_on_batch([fakes, conditions[i]], np.array([0.1 for i in range(b_size)]))\n    if epoch%20==0:\n        print(\"Real Loss:\", d_loss_real)\n        print(\"Fake Loss:\", d_loss_fake)","89c5060d":"gan_output = discriminator([generator_output, c_input])\ngan = Model(inputs=[noise_input, c_input], outputs=gan_output)\ndiscriminator.trainable = False\ngan.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-4, decay=1e-6))","ec5fa993":"def getFakeData(iteration):\n    noise = np.random.normal(0,1,size=(b_size,latent_dim))\n    condition = conditions[iteration]\n    x_fake = generator.predict([noise, condition])\n    y_fake = np.array([0.1 for i in range(len(x_fake))]) #0.1 to represent fake\n    return x_fake, y_fake\n\ndef getNoisyData(y_real): # occasionally flip the labels when training the discriminator\n    if np.random.random_integers(0,1) == 0:\n        return y_real\n    else:\n        max_flips = len(y_real)\/\/8\n        number_of_flips = np.random.randint(0,max_flips)\n        for i in np.random.choice([i for i in range(max_flips)], number_of_flips):\n            y_real[i] = 0.1\n        return y_real\n    \ndef train_discriminator(iteration):\n    x_real, y_real = getRealData(iteration)\n    x_fake, y_fake = getFakeData(iteration)\n    \n    # train discriminator\n    discriminator.trainable = True\n    y_real = getNoisyData(y_real)\n    condition = conditions[iteration]\n    loss_real = discriminator.train_on_batch([x_real, condition], y_real)\n    loss_fake = discriminator.train_on_batch([x_fake, condition], y_fake)\n    return [loss_real, loss_fake]\n    \ndef train_gan(iteration):\n    # train generator\n    discriminator.trainable = False\n    noise = np.random.normal(0,1,size=(b_size,latent_dim))\n    condition = conditions[iteration]\n    y_gen = np.ones(len(noise))\n    g_loss = gan.train_on_batch([noise, condition], y_gen)\n    return g_loss\n\ndef getRealData(iteration):\n    x_real = all_data[iteration][:32]\n    y_real = np.array([(0.8+0.01*iteration) for i in range(len(x_real))]) # label smoothing\n    return x_real, y_real","e806c299":"def plotImage(condition):\n    noise = np.random.normal(0,1,size=latent_dim)\n    noise = np.reshape(noise, newshape=(1,latent_dim))\n    image = generator.predict([noise, conditions[condition]])\n    image = np.reshape(image, newshape=(img_size,img_size,channels))\n    image = array_to_img(image)\n    return image","be315e36":"epoch = 1\nwhile (epoch<=4000):\n    for i in range(10):\n        # train 2x discriminator then 1x generator\n        #train_discriminator(i)\n        loss_real, loss_fake = train_discriminator(i)\n        g_loss = train_gan(i)\n    if epoch%500 == 0:\n        print(\"Epoch \", epoch)\n        print(\"Discriminator Loss (Real):\", loss_real)\n        print(\"Discriminator Loss (Fake):\", loss_fake)\n        d_loss = 0.5*(loss_real+loss_fake)\n        print(\"Discriminator Loss (Total):\", d_loss)\n        print(\"Gen Loss:\", g_loss)\n        fig=plt.figure(figsize=(16, 16))\n        for i in range(1, 11):\n            img = plotImage(i-1)\n            fig.add_subplot(1, 10, i)\n            plt.imshow(img)\n        plt.show()\n    epoch+=1","7e3201ad":"fig=plt.figure(figsize=(16, 16))\nfor i in range(1, 11):\n    img = plotImage(i-1)\n    fig.add_subplot(1, 10, i)\n    plt.imshow(img)\nplt.show()","7b4d0488":"discriminator.save('discriminator.h5')\ngenerator.save('generator.h5')\ngan.save('gan.h5')","1ac5df4f":"#from keras.models import load_model\n#generator = load_model('..\/input\/mnist-generator\/generator.h5') # generator from commit 4","88f0505d":"def createCNN():\n    cnn_input = Input(shape=(img_size,img_size,channels))\n\n    x = Conv2D(32, 3, padding='same', activation='relu')(cnn_input)\n    x = BatchNormalization()(x)\n    x = Conv2D(32, 3, padding='same', activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Conv2D(32, 5, padding='same', activation='relu', strides=2)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n\n    x = Conv2D(64, 3, padding='same', activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Conv2D(64, 3, padding='same', activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Conv2D(64, 5, padding='same', activation='relu', strides=2)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n\n    x = Flatten()(x)\n    x = Dense(128, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n\n    cnn_output = Dense(10, activation='softmax')(x)\n    cnn = Model(inputs=cnn_input, outputs=cnn_output)\n    cnn.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n    return cnn","5482467b":"# val data of 320 images\nval_data = []\nval_labels = []\n\nfor i in range(10):\n    val_data.append(all_data[i][32:])\n    onehot = np.array([0 for i in range(num_of_classes)])\n    onehot[i] = 1\n    val_labels.append(np.array([onehot for y in range(320)]))\n    \nval_labels = np.reshape(np.array(val_labels), newshape=(320*10, 10))\nval_data = np.reshape(np.array(val_data), newshape=(320*10, img_size, img_size, channels))","2beaf4e6":"cnn = createCNN()\nlabels = []\nreal_images = []\nfor i in range(10):\n    onehot = np.array([0 for i in range(num_of_classes)])\n    onehot[i] = 1\n    onehot_labels = np.array([onehot for y in range(32)])\n    labels.append(onehot_labels)\n    real, useless_var = getRealData(i)\n    del useless_var\n    real_images.append(real)\nlabels = np.reshape(np.array(labels), newshape=(320, 10))\n\nreal_images = np.reshape(np.array(real_images), newshape=(320, img_size, img_size, channels))\ncnn.fit(x=real_images, y=labels, batch_size=b_size, epochs=20)\ncnn.evaluate(x=val_data, y=val_labels)","6da326f9":"def getNewData(iteration):\n    # get one-hot labels for the 3200 images\n    onehot = np.array([0 for i in range(num_of_classes)])\n    onehot[iteration] = 1\n    onehot_labels = np.array([onehot for y in range(3200)])\n    \n    # conditions for 3200 images\n    new_conditions = np.array([np.array([iteration]) for y in range(3200)])\n\n    # generate 3200-32 images\n    noise = np.random.normal(0,1,size=(3200-b_size,latent_dim))\n    generated = generator.predict([noise, new_conditions[:3200-b_size]])\n     # add original 32 real images with generated images\n    new_data = np.concatenate((all_data[iteration][:32], generated))\n    \n    return new_data, onehot_labels","0d2415b1":"labels = []\nnew_data = []\n\nfor i in range(10):\n    temp_data, label = getNewData(i)\n    labels.append(label)\n    new_data.append(temp_data)\n    \nlabels = np.reshape(np.array(labels), newshape=(3200*10, 10))\nnew_data = np.reshape(np.array(new_data), newshape=(3200*10, img_size, img_size, channels))","9f9bf96c":"cnn = createCNN()\ncnn.fit(x=new_data, y=labels, batch_size=b_size, epochs=20)\ncnn.evaluate(x=val_data, y=val_labels)","61b2d272":"test_data = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\nimage_id = np.array([i for i in range(1,len(test_data)+1)])\ntest_data = np.reshape(np.array(test_data)\/255, newshape=(len(test_data), img_size, img_size, 1))\npredictions = cnn.predict(test_data)","1fb82056":"predictions = predictions.argmax(axis=1)\nsubmission = pd.DataFrame({\"ImageId\":image_id, \"Label\":predictions})\nsubmission.to_csv(\"gan_cnn.csv\", index=False)","983604bf":"### Training GAN\n\n[Tips for Training Gan](https:\/\/github.com\/soumith\/ganhacks)","d6064956":"### Pre-Train Discriminator\n\n- Train Discriminator for 100 epochs","d9f8aede":"### Getting Validation Data","cb6428fd":"### Getting New Data\n\n- New Data consist of **3200** images per labels","c375bb19":"### Sampling Images\n\n- Assume we only have 32 images of each label\n- Input noise is of dimensions (batch size, latent dimension)\n- Input condition is a number which corresponds to the label from 0-9","2f75a0b9":"### Discriminator","7ccfcc17":"### Creating GAN","d4e94c03":"### Creating CNN\n\nCNN architecture I will be using is borrowed from [here](https:\/\/www.kaggle.com\/cdeotte\/how-to-choose-cnn-architecture-mnist#5.-Advanced-features)","d7bbef63":"# GAN","c7ee20dc":"### Generator","fa763d9c":"# CNN","a7823230":"### Training CNN (Using Generated Images) and Evaluating it\n\nNote: New Data is already scaled to [-1, 1]","54504f22":"## Train and Evaluate CNN based on original 32 Images of each class","b843a0f5":"### Imports and reading Data","b6f30fef":"# Predict \"test.csv\" and submit to Kaggle"}}