{"cell_type":{"6d57d9f8":"code","02ccab6a":"code","7dc49954":"code","c41ab950":"code","674b34e8":"code","c6a27d1a":"code","fb8d8bc3":"code","32e5a2b5":"code","35f68fc4":"code","f05faa6c":"code","01b30533":"code","a3f6a998":"code","ea12b817":"code","b79dbb4b":"code","4002e8e2":"code","77841fb5":"code","5dcccfe4":"code","e5d65437":"code","a1108608":"code","8f16231c":"code","53e5daa3":"code","46d8a683":"code","88ba769a":"code","c390faa2":"code","0d9c6774":"code","5b0b9734":"code","c2b26e15":"code","308ab53f":"code","dab6ed04":"code","edffa81e":"code","86404c75":"code","f8c7d261":"code","a944fa5f":"code","8fd7e35a":"code","322d2112":"code","3ee5fb6d":"code","794a0c14":"code","c9aca385":"code","e3478986":"code","43d9c821":"code","0adf23a5":"code","8f263941":"code","6d6c13c4":"code","65abcd2a":"code","64a3ed04":"code","4ab0cbf9":"code","8c7c4aa0":"code","1a88d4f3":"code","4c4063f8":"code","4082c8f1":"code","037466a6":"code","64f7a573":"code","97a73637":"code","dae53dbb":"code","77b5d7e4":"code","1757aeb1":"code","7d8bfe41":"markdown","a5be3e8c":"markdown","1cd95724":"markdown","cebe1c7b":"markdown","f024c0b0":"markdown","2802f6ac":"markdown","7cb3a07d":"markdown","4c12276b":"markdown","54fbc924":"markdown","ea48f8c3":"markdown","a347215b":"markdown","c5451630":"markdown","b305c636":"markdown","3b30264a":"markdown","1ccfc21c":"markdown","83f0383d":"markdown","7c422481":"markdown","da54bc0e":"markdown","8d6c7451":"markdown","c234641b":"markdown","d80ceb1e":"markdown","5c91368a":"markdown"},"source":{"6d57d9f8":"# core\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# pre-processing\nfrom sklearn.preprocessing import MinMaxScaler\n\n# modelling\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Concatenate\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import ReLU\nfrom tensorflow.keras.optimizers import Adam\n\n# callbacks\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\n# model evaluation\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.utils import plot_model","02ccab6a":"# function to engineer features using available information from a dataframe df\ndef engineer_features(df):\n    df['dow']  = df.date_time.dt.day_of_week # day of week\n    df['woy']  = df.date_time.dt.isocalendar().week.astype('int') # week of the year\n    df['hod'] = df.date_time.dt.hour # hour of the day\n    df['working_hours'] = df.date_time.dt.hour.isin(np.arange(8, 19)).astype('int') # indicator of whether the record was made during working hours\n    return df\n\n# function to calculate RMSLE loss directly\ndef rmsle_(y_true, y_pred):\n    msle = tf.keras.losses.MeanSquaredLogarithmicError()\n    return K.sqrt(msle(y_true, y_pred)) ","7dc49954":"train       = pd.read_csv(filepath_or_buffer = '..\/input\/tabular-playground-series-jul-2021\/train.csv', parse_dates = ['date_time'])\ntest        = pd.read_csv(filepath_or_buffer = '..\/input\/tabular-playground-series-jul-2021\/test.csv', parse_dates = ['date_time'])\nsubmission  = pd.read_csv(filepath_or_buffer = '..\/input\/tabular-playground-series-jul-2021\/sample_submission.csv')","c41ab950":"# adding engineered features to their respective dataframes\ntrain = engineer_features(train)\ntest  = engineer_features(test)","674b34e8":"# getting the target column names\ntarget_columns = [column for column in train.columns if column.startswith('target_')]\n\n# getting the feature column names\nfeature_columns = [column for column in train.columns if column not in ['date_time'] + target_columns]","c6a27d1a":"fig, (ax1, ax2) = plt.subplots(ncols = 2, figsize=(25, 8)) \nsns.heatmap(train.drop(columns = ['date_time']).corr(), cmap = 'RdBu', ax = ax1)\nsns.heatmap(test.drop(columns = ['date_time']).corr(), cmap = 'RdBu', ax = ax2)\nax1.set_title('Correlations on the training set', fontdict = {'size': 14, 'fontweight': 'bold'})\nax2.set_title('Correlations on the test set', fontdict = {'size': 14, 'fontweight': 'bold'})\nplt.show()","fb8d8bc3":"# copying the original dataframe\ndf_mlp_1 = train.copy()\n\n# splitting the targets from the features\nX_mlp_1, y_mlp_1 = df_mlp_1[feature_columns], df_mlp_1[target_columns]\n\n# getting the index of the sensor data and the other features\nsensor_index = [column_index for column_index, column_name in enumerate(X_mlp_1.columns) if column_name.startswith('sensor')]\nother_index = [column_index for column_index in range(X_mlp_1.shape[1]) if column_index not in sensor_index]\n\n# log transforming the target columns\ny_mlp_1 = np.log1p(y_mlp_1)\n\n# instantiating the input scaler\nscaler = MinMaxScaler()\n\n# training the scaler\nscaler.fit(X_mlp_1)\n\n# applying the scaler to the data\nX_mlp_1 = scaler.transform(X_mlp_1)\nX_test  = scaler.transform(test[feature_columns])","32e5a2b5":"# copying the dataframe from mlp_1 dataframe\nX_mlp_2 = X_mlp_1.copy()\n\n## separating each of the targets\ny_CO, y_BE, y_NO = y_mlp_1.target_carbon_monoxide, y_mlp_1.target_benzene, y_mlp_1.target_nitrogen_oxides","35f68fc4":"# copying the dataframe from mlp_1 dataframe\nX_mlp_3 = X_mlp_1.copy()\n\n# getting the indexes for the sensor data\nX_mlp_3_sensor, X_mlp_3_others = X_mlp_3[:, sensor_index], X_mlp_3[:, other_index]","f05faa6c":"## function to split each of the individual sequences for the autoregressive mlp\ndef split_sequences_mlp_4(sequence, n_lagged_inputs):\n    # creating empty lists to store the lagged inputs and the target output\n    lagged_inputs, target_output = list(), list()\n    # looping over the sequence\n    for observation in range(len(sequence)):\n        # defining the last index of this loop step\n        last_idx = observation + n_lagged_inputs\n        # breaking the loop if the index is beyond the number of observations\n        if last_idx > len(sequence) - 1:\n            break\n        # extracting the inputs and output for this loop step\n        inputs, output = sequence[observation:last_idx], sequence[last_idx]\n        # appending the input and output sequences\n        lagged_inputs.append(inputs), target_output.append(output)\n    # returning the inputs and outputs\n    return np.array(lagged_inputs), np.array(target_output)","01b30533":"## extracting the splitted sequences for each of the response variables\nn_ar_inputs = 6\nX_ar_CO_mlp_4, y_ar_CO_mlp_4 = split_sequences_mlp_4(sequence = y_CO, n_lagged_inputs = n_ar_inputs)\nX_ar_BE_mlp_4, y_ar_BE_mlp_4 = split_sequences_mlp_4(sequence = y_BE, n_lagged_inputs = n_ar_inputs)\nX_ar_NO_mlp_4, y_ar_NO_mlp_4 = split_sequences_mlp_4(sequence = y_NO, n_lagged_inputs = n_ar_inputs)\n\n## copying the original data\nX_mlp_4_sensor, X_mlp_4_others = X_mlp_3_sensor.copy(), X_mlp_3_others.copy()\n\n## removing the first 6 rows of data, since these were used to create the autoregressive features\nX_mlp_4_sensor, X_mlp_4_others = X_mlp_4_sensor[n_ar_inputs:, :], X_mlp_4_others[n_ar_inputs:, :]","a3f6a998":"## merging all target in a single \ny_ar_mlp_5 = np.column_stack([y_ar_CO_mlp_4, y_ar_BE_mlp_4, y_ar_NO_mlp_4])","ea12b817":"## function to split each of the individual sequences for the autoregressive mlp\ndef split_feature_sequences(sequence, n_lagged_inputs):\n    # creating empty lists to store the lagged inputs and the target output\n    lagged_inputs = list()\n    # looping over the sequence\n    for observation in range(len(sequence)):\n        # defining the last index of this loop step\n        last_idx = observation + n_lagged_inputs\n        # breaking the loop if the index is beyond the number of observations\n        if last_idx > len(sequence):\n            break\n        # extracting the inputs and output for this loop step\n        inputs = sequence[observation:last_idx]\n        # appending the input and output sequences\n        lagged_inputs.append(inputs)\n    # returning the inputs and outputs\n    return np.array(lagged_inputs)","b79dbb4b":"## getting the temperature and relative humidity data\nX_temp_mlp_6, X_rl_mlp_6 = X_mlp_1[:, 0], X_mlp_1[:, 1]\n\n## creating the lagged inputs for temperatura and relative humidity data\nn_ar_mlp_6 = 12\nX_temp_mlp_6 = split_feature_sequences(sequence = X_temp_mlp_6, n_lagged_inputs = n_ar_mlp_6)\nX_rl_mlp_6   = split_feature_sequences(sequence = X_rl_mlp_6, n_lagged_inputs = n_ar_mlp_6)\n\n## removing the data for the n_ar_mlp_6 observations from the sensor data\nX_mlp_6_sensor = X_mlp_3_sensor.copy()\nX_mlp_6_sensor = X_mlp_6_sensor[n_ar_mlp_6 - 1:, :]\n\n## removing the first 6 elements of the autoregressive input objects\nX_ar_CO_mlp_6, X_ar_BE_mlp_6, X_ar_NO_mlp_6 = X_ar_CO_mlp_4.copy(), X_ar_BE_mlp_4.copy(), X_ar_NO_mlp_4.copy()\nX_ar_CO_mlp_6, X_ar_BE_mlp_6, X_ar_NO_mlp_6 = X_ar_CO_mlp_6[5:, :], X_ar_BE_mlp_6[5:, :], X_ar_NO_mlp_6[5:, :]\n\n## removing the first 6 elements from the target frame so that they are all aligned\ny_ar_mlp_6 = y_ar_mlp_5.copy()\ny_ar_mlp_6 = y_ar_mlp_6[5:, :]","4002e8e2":"# creating the test set autoregressive features\n## seeding the initial list\ntest_ar_TEMP, test_ar_RL = X_mlp_1[-12:-1, 0].tolist(), X_mlp_1[-12:-1, 1].tolist()\n\n## adding the rest of the time series\ntest_ar_TEMP, test_ar_RL = test_ar_TEMP + X_test[:, 0].tolist(), test_ar_RL + X_test[:, 1].tolist()\n\n## creating the lagged feature sequences on the test set\ntest_ar_TEMP = split_feature_sequences(sequence = test_ar_TEMP, n_lagged_inputs = n_ar_mlp_6)\ntest_ar_RL   = split_feature_sequences(sequence = test_ar_RL, n_lagged_inputs = n_ar_mlp_6)","77841fb5":"## extracting the splitted sequences for each of the response variables\nn_ar_inputs_mlp_7 = 25\nX_ar_CO_mlp_7, y_ar_CO_mlp_7 = split_sequences_mlp_4(sequence = y_CO, n_lagged_inputs = n_ar_inputs_mlp_7)\nX_ar_BE_mlp_7, y_ar_BE_mlp_7 = split_sequences_mlp_4(sequence = y_BE, n_lagged_inputs = n_ar_inputs_mlp_7)\nX_ar_NO_mlp_7, y_ar_NO_mlp_7 = split_sequences_mlp_4(sequence = y_NO, n_lagged_inputs = n_ar_inputs_mlp_7)\n\n## merging the targets in a single array\ny_ar_mlp_7 = np.column_stack([y_ar_CO_mlp_7, y_ar_BE_mlp_7, y_ar_NO_mlp_7])\n\n## removing the data for the n_ar_inputs_mlp_7 observations from the sensor data\nX_mlp_7_sensor = X_mlp_3_sensor.copy()\nX_mlp_7_sensor = X_mlp_7_sensor[n_ar_inputs_mlp_7:, :]\n\n## removing the first instance as it gets the historical data from t0 to t12, and we needed it from t1 to t13\nX_temp_mlp_7 = X_temp_mlp_6.copy()\nX_temp_mlp_7 = X_temp_mlp_7[14:, :]\n\n## removing the first instance as it gets the historical data from t0 to t12, and we needed it from t1 to t13\nX_rl_mlp_7 = X_rl_mlp_6.copy()\nX_rl_mlp_7 = X_rl_mlp_7[14:, :]","5dcccfe4":"## creating the layers\n# input layer\ninput_layer = Input(shape = (X_mlp_1.shape[1], ))\n# hidden layers\nhidden_layer = Dense(units = 32, activation = 'relu')(input_layer)\nhidden_layer = Dropout(0.2)(hidden_layer)\nhidden_layer = Dense(units = 32, activation = 'relu')(hidden_layer)\n# output layers\noutput_layer = Dense(units = 3)(hidden_layer)\n\n## creating the model\nmodel = Model(inputs = input_layer, outputs = output_layer)\n\n# compilling the model\nmodel.compile(optimizer = Adam(learning_rate = 0.001), loss = 'mse', metrics = [RootMeanSquaredError()])\n\n# instantiating the early stopping callback\nes = EarlyStopping(monitor = 'val_root_mean_squared_error', patience = 20, min_delta = 0.01, restore_best_weights = True)\n\n# instantiating the learning rate scheduller\nlrs = ReduceLROnPlateau(monitor = 'val_root_mean_squared_error', factor = 0.2, patience = 5)\n\n# fitting the model\nhistory = model.fit(x = X_mlp_1, y = y_mlp_1, validation_split = 0.3, batch_size = 16, epochs = 100, callbacks = [es, lrs])\n\n# training history\nplt.figure(figsize = (10, 6))\nplt.plot(history.history['loss'], label = 'training')\nplt.plot(history.history['val_root_mean_squared_error'], label = 'validation')\nplt.title(label = 'Training over epochs', fontdict = {'size': 14, 'fontweight': 'bold'})\nplt.ylabel(ylabel = 'RMSE', fontdict = {'size': 12, 'fontweight': 'bold'})\nplt.xlabel(xlabel = 'Epochs', fontdict = {'size': 12, 'fontweight': 'bold'})\nplt.legend(fontsize = 12)\nplt.tight_layout()\nplt.show()","e5d65437":"## visualizing the model\nplot_model(model, show_shapes = True)","a1108608":"## extracting the predictions on the test set\ntest_preds = model.predict(X_test)\n\n## putting the test predictions back on the original scale\ntest_preds = np.expm1(test_preds)\n\n## copying the submission df\nsubmission_mlp_1 = submission.copy()\n\n## putting the predictions on the submission dataset\nsubmission_mlp_1[target_columns] = test_preds","8f16231c":"## predictions\nplt.figure(figsize = (25, 6))\nfor idx, target in enumerate(target_columns):\n    plt.subplot(1, 3, idx + 1)\n    plt.plot(range(train.shape[0]), train[target], label = 'Train')\n    plt.plot(range(train.shape[0], train.shape[0] + test.shape[0]), submission_mlp_1[target], label = 'Test')\n    plt.title(label = target, fontdict = {'size': 14, 'fontweight': 'bold'})\n    plt.xlabel(xlabel = 'Timesteps', fontdict = {'size': 14})\n    plt.ylabel(ylabel = 'Value', fontdict = {'size': 14})\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.legend()\n    plt.tight_layout()\nplt.show()","53e5daa3":"## getting the predictions of the model\nmlp_1_predictions = model.predict(X_mlp_1)\n\n## putting the predictions in a dataframe\nmlp_1_predictions = pd.DataFrame(data = mlp_1_predictions, columns = target_columns)\n\n## residuals over time\nplt.figure(figsize = (25, 6))\nfor idx, target in enumerate(target_columns):\n    plt.subplot(1, 3, idx + 1)\n    plt.plot(y_mlp_1.loc[:, target] - mlp_1_predictions.loc[:, target])\n    plt.title(label = target, fontdict = {'size': 14, 'fontweight': 'bold'})\n    plt.xlabel(xlabel = 'Timesteps', fontdict = {'size': 14})\n    plt.ylabel(ylabel = 'Residuals', fontdict = {'size': 14})\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.tight_layout()\nplt.show()\n\n# distribution of residuals\nplt.figure(figsize = (25, 6))\nfor idx, target in enumerate(target_columns):\n    plt.subplot(1, 3, idx + 1)\n    plt.hist(y_mlp_1.loc[:, target] - mlp_1_predictions.loc[:, target], bins = 50)\n    plt.title(label = target, fontdict = {'size': 14, 'fontweight': 'bold'})\n    plt.xlabel(xlabel = 'Residuals', fontdict = {'size': 14})\n    plt.ylabel(ylabel = 'Frequency', fontdict = {'size': 14})\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.tight_layout()\nplt.show()","46d8a683":"## creating the layers\n# input layer\ninput_layer = Input(shape = (X_mlp_2.shape[1],))\n# hidden layers\nhidden_layer = Dense(units = 16, activation = 'relu')(input_layer)\nhidden_layer = Dense(units = 32, activation = 'relu')(hidden_layer)\n# output layers\noutput_CO = Dense(units = 1, name = 'out_CO')(hidden_layer)\noutput_BE = Dense(units = 1, name = 'out_BE')(hidden_layer)\noutput_NO = Dense(units = 1, name = 'out_NO')(hidden_layer)\n\n## creating the model\nmlp_2 = Model(inputs = input_layer, outputs = [output_CO, output_BE, output_NO])\n\n# compilling the model\nmlp_2.compile(optimizer = Adam(learning_rate = 0.001), loss = 'mse', metrics = [RootMeanSquaredError()])\n\n# instantiating the early stopping callback\nes = EarlyStopping(monitor = 'val_loss', patience = 20, min_delta = 0.01, restore_best_weights = True)\n\n# instantiating the learning rate scheduller\nlrs = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 5)\n\n# fitting the model\nhistory = mlp_2.fit(x = X_mlp_2, y = [y_CO, y_BE, y_NO], validation_split = 0.3, batch_size = 8, epochs = 400, callbacks = [es, lrs])\n\n# training history\nplt.figure(figsize = (10, 6))\nplt.plot(history.history['loss'], label = 'training')\nplt.plot(history.history['val_loss'], label = 'validation')\nplt.title(label = 'Training over epochs', fontdict = {'size': 14, 'fontweight': 'bold'})\nplt.ylabel(ylabel = 'MSE', fontdict = {'size': 12, 'fontweight': 'bold'})\nplt.xlabel(xlabel = 'Epochs', fontdict = {'size': 12, 'fontweight': 'bold'})\nplt.legend(fontsize = 12)\nplt.tight_layout()\nplt.show()\n\n# training history for the targets\nplt.figure(figsize = (10, 6))\nplt.plot(history.history['val_out_CO_root_mean_squared_error'], label = 'CO')\nplt.plot(history.history['val_out_BE_root_mean_squared_error'], label = 'Benzene')\nplt.plot(history.history['val_out_NO_root_mean_squared_error'], label = 'NO')\nplt.title(label = 'Training over epochs', fontdict = {'size': 14, 'fontweight': 'bold'})\nplt.ylabel(ylabel = 'RMSE', fontdict = {'size': 12, 'fontweight': 'bold'})\nplt.xlabel(xlabel = 'Epochs', fontdict = {'size': 12, 'fontweight': 'bold'})\nplt.legend(fontsize = 12)\nplt.tight_layout()\nplt.show()","88ba769a":"## visualizing the model\nplot_model(mlp_2, show_shapes = True)","c390faa2":"## extracting the predictions on the test set\ntest_preds = mlp_2.predict(X_test)\n\n## reshaping the predictions\ntest_preds = np.column_stack(test_preds)\n\n## putting the test predictions back on the original scale\ntest_preds = np.expm1(test_preds)\n\n## copying the submission df\nsubmission_mlp_2 = submission.copy()\n\n## putting the predictions on the submission dataset\nsubmission_mlp_2[target_columns] = test_preds","0d9c6774":"## predictions\nplt.figure(figsize = (25, 6))\nfor idx, target in enumerate(target_columns):\n    plt.subplot(1, 3, idx + 1)\n    plt.plot(range(train.shape[0]), train[target], label = 'Train')\n    plt.plot(range(train.shape[0], train.shape[0] + test.shape[0]), submission_mlp_2[target], label = 'Test')\n    plt.title(label = target, fontdict = {'size': 14, 'fontweight': 'bold'})\n    plt.xlabel(xlabel = 'Timesteps', fontdict = {'size': 14})\n    plt.ylabel(ylabel = 'Value', fontdict = {'size': 14})\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.legend()\n    plt.tight_layout()\nplt.show()","5b0b9734":"## getting the predictions of the model\nmlp_2_predictions = mlp_2.predict(X_mlp_2)\n\n## reshaping the predictions\nmlp_2_predictions = np.column_stack(mlp_2_predictions)\n\n## putting the predictions in a dataframe\nmlp_2_predictions = pd.DataFrame(data = mlp_2_predictions, columns = target_columns)\n\n## getting the observed values\nobserved_values = pd.DataFrame(data = np.column_stack([y_CO, y_BE, y_NO]), columns = target_columns)\n\n## residuals over time\nplt.figure(figsize = (25, 6))\nfor idx, target in enumerate(target_columns):\n    plt.subplot(1, 3, idx + 1)\n    plt.plot(observed_values.loc[:, target] - mlp_2_predictions.loc[:, target])\n    plt.title(label = target, fontdict = {'size': 14, 'fontweight': 'bold'})\n    plt.xlabel(xlabel = 'Timesteps', fontdict = {'size': 14})\n    plt.ylabel(ylabel = 'Residuals', fontdict = {'size': 14})\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.tight_layout()\nplt.show()\n\n# distribution of residuals\nplt.figure(figsize = (25, 6))\nfor idx, target in enumerate(target_columns):\n    plt.subplot(1, 3, idx + 1)\n    plt.hist(observed_values.loc[:, target] - mlp_2_predictions.loc[:, target], bins = 50)\n    plt.title(label = target, fontdict = {'size': 14, 'fontweight': 'bold'})\n    plt.xlabel(xlabel = 'Residuals', fontdict = {'size': 14})\n    plt.ylabel(ylabel = 'Frequency', fontdict = {'size': 14})\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.tight_layout()\nplt.show()","c2b26e15":"### creating the layers\n## SENSOR DATA\n# input layer\ninput_layer_1 = Input(shape = (X_mlp_3_sensor.shape[1], ))\n# hidden layers\nhidden_layer_1 = Dense(units = 32, activation = 'relu')(input_layer_1)\nhidden_layer_1 = BatchNormalization()(hidden_layer_1)\nhidden_layer_1 = Dense(units = 16, activation = 'relu')(hidden_layer_1)\n\n## OTHER FEATURES\n# input layer\ninput_layer_2 = Input(shape = (X_mlp_3_others.shape[1], ))\n# hidden layers\nhidden_layer_2 = Dense(units = 32, activation = 'relu')(input_layer_2)\n\n## MERGING THE TWO INPUTS\nmerge_layer = Concatenate()([hidden_layer_1, hidden_layer_2])\n## one more layer\nhidden_layer = Dense(units = 32, activation = 'relu')(merge_layer)\nhidden_layer = Dropout(0.1)(hidden_layer)\n# output layers\noutput_layer = Dense(units = 3)(hidden_layer)\n\n## creating the model\nmlp_3 = Model(inputs = [input_layer_1, input_layer_2], outputs = output_layer)\n\n# compilling the model\nmlp_3.compile(optimizer = Adam(learning_rate = 0.0005), loss = 'mse', metrics = [RootMeanSquaredError()])\n\n# instantiating the early stopping callback\nes = EarlyStopping(monitor = 'val_root_mean_squared_error', patience = 20, min_delta = 0.002, restore_best_weights = True)\n\n# instantiating the learning rate scheduller\nlrs = ReduceLROnPlateau(monitor = 'val_root_mean_squared_error', factor = 0.2, patience = 5)\n\n# fitting the model\nhistory = mlp_3.fit(x = [X_mlp_3_sensor, X_mlp_3_others], y = y_mlp_1, validation_split = 0.3, batch_size = 16, epochs = 100, callbacks = [es, lrs])\n\n# training history\nplt.figure(figsize = (10, 6))\nplt.plot(history.history['loss'], label = 'training')\nplt.plot(history.history['val_root_mean_squared_error'], label = 'validation')\nplt.title(label = 'Training over epochs', fontdict = {'size': 14, 'fontweight': 'bold'})\nplt.ylabel(ylabel = 'RMSE', fontdict = {'size': 12, 'fontweight': 'bold'})\nplt.xlabel(xlabel = 'Epochs', fontdict = {'size': 12, 'fontweight': 'bold'})\nplt.legend(fontsize = 12)\nplt.tight_layout()\nplt.show()","308ab53f":"## visualizing the model\nplot_model(mlp_3, show_shapes = True)","dab6ed04":"## extracting the predictions on the test set\ntest_preds = mlp_3.predict([X_test[:, sensor_index], X_test[:, other_index]])\n\n## putting the test predictions back on the original scale\ntest_preds = np.expm1(test_preds)\n\n## copying the submission df\nsubmission_mlp_3 = submission.copy()\n\n## putting the predictions on the submission dataset\nsubmission_mlp_3[target_columns] = test_preds","edffa81e":"## predictions\nplt.figure(figsize = (25, 6))\nfor idx, target in enumerate(target_columns):\n    plt.subplot(1, 3, idx + 1)\n    plt.plot(range(train.shape[0]), train[target], label = 'Train')\n    plt.plot(range(train.shape[0], train.shape[0] + test.shape[0]), submission_mlp_3[target], label = 'Test')\n    plt.title(label = target, fontdict = {'size': 14, 'fontweight': 'bold'})\n    plt.xlabel(xlabel = 'Timesteps', fontdict = {'size': 14})\n    plt.ylabel(ylabel = 'Value', fontdict = {'size': 14})\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.legend()\n    plt.tight_layout()\nplt.show()","86404c75":"## getting the predictions of the model\nmlp_3_predictions = mlp_3.predict([X_mlp_3_sensor, X_mlp_3_others])\n\n## putting the predictions in a dataframe\nmlp_3_predictions = pd.DataFrame(data = mlp_3_predictions, columns = target_columns)\n\n## residuals over time\nplt.figure(figsize = (25, 6))\nfor idx, target in enumerate(target_columns):\n    plt.subplot(1, 3, idx + 1)\n    plt.plot(y_mlp_1.loc[:, target] - mlp_3_predictions.loc[:, target])\n    plt.title(label = target, fontdict = {'size': 14, 'fontweight': 'bold'})\n    plt.xlabel(xlabel = 'Timesteps', fontdict = {'size': 14})\n    plt.ylabel(ylabel = 'Residuals', fontdict = {'size': 14})\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.tight_layout()\nplt.show()\n\n# distribution of residuals\nplt.figure(figsize = (25, 6))\nfor idx, target in enumerate(target_columns):\n    plt.subplot(1, 3, idx + 1)\n    plt.hist(y_mlp_1.loc[:, target] - mlp_3_predictions.loc[:, target], bins = 50)\n    plt.title(label = target, fontdict = {'size': 14, 'fontweight': 'bold'})\n    plt.xlabel(xlabel = 'Residuals', fontdict = {'size': 14})\n    plt.ylabel(ylabel = 'Frequency', fontdict = {'size': 14})\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.tight_layout()\nplt.show()","f8c7d261":"### creating the layers\n## SENSOR DATA\n# input layer\ninput_layer_1 = Input(shape = (X_mlp_4_sensor.shape[1], ))\n# hidden layers\nhidden_layer_1 = Dense(units = 32, activation = 'relu')(input_layer_1)\nhidden_layer_1 = BatchNormalization()(hidden_layer_1)\nhidden_layer_1 = Dense(units = 16, activation = 'relu')(hidden_layer_1)\n\n## OTHER FEATURES\n# input layer\ninput_layer_2 = Input(shape = (X_mlp_4_others.shape[1], ))\n# hidden layers\nhidden_layer_2 = Dense(units = 32, activation = 'relu')(input_layer_2)\n\n## AUTOREGRESSIVE CO\n# input layer\ninput_layer_CO = Input(shape = (X_ar_CO_mlp_4.shape[1], ))\n# hidden layers\nhidden_layer_CO = Dense(units = 32, activation = 'relu')(input_layer_CO)\n\n## AUTOREGRESSIVE BE\n# input layer\ninput_layer_BE = Input(shape = (X_ar_BE_mlp_4.shape[1], ))\n# hidden layers\nhidden_layer_BE = Dense(units = 32, activation = 'relu')(input_layer_BE)\n\n## AUTOREGRESSIVE NO\n# input layer\ninput_layer_NO = Input(shape = (X_ar_NO_mlp_4.shape[1], ))\n# hidden layers\nhidden_layer_NO = Dense(units = 32, activation = 'relu')(input_layer_NO)\n\n## MERGING THE TWO INPUTS\nmerge_layer = Concatenate()([hidden_layer_1, hidden_layer_2, hidden_layer_CO, hidden_layer_BE, hidden_layer_NO])\n## one more layer\nhidden_layer = Dense(units = 32, activation = 'relu')(merge_layer)\nhidden_layer = Dropout(0.1)(hidden_layer)\n# output layers\n# output layers\noutput_CO = Dense(units = 1, name = 'out_CO')(hidden_layer)\noutput_BE = Dense(units = 1, name = 'out_BE')(hidden_layer)\noutput_NO = Dense(units = 1, name = 'out_NO')(hidden_layer)\n\n## creating the model\nmlp_4 = Model(inputs = [input_layer_1, input_layer_2, input_layer_CO, input_layer_BE, input_layer_NO], outputs = [output_CO, output_BE, output_NO])\n\n# compilling the model\nmlp_4.compile(optimizer = Adam(learning_rate = 0.0005), loss = 'mse', metrics = [RootMeanSquaredError()])\n\n# instantiating the early stopping callback\nes = EarlyStopping(monitor = 'val_loss', patience = 20, min_delta = 0.002, restore_best_weights = True)\n\n# instantiating the learning rate scheduller\nlrs = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 5)\n\n# fitting the model\nhistory = mlp_4.fit(x = [X_mlp_4_sensor, X_mlp_4_others, X_ar_CO_mlp_4, X_ar_BE_mlp_4, X_ar_NO_mlp_4], \n                    y = [y_ar_CO_mlp_4, y_ar_BE_mlp_4, y_ar_NO_mlp_4], \n                    validation_split = 0.3, batch_size = 8, epochs = 100, callbacks = [es, lrs])\n\n# training history\nplt.figure(figsize = (10, 6))\nplt.plot(history.history['loss'], label = 'training')\nplt.plot(history.history['val_loss'], label = 'validation')\nplt.title(label = 'Training over epochs', fontdict = {'size': 14, 'fontweight': 'bold'})\nplt.ylabel(ylabel = 'RMSE', fontdict = {'size': 12, 'fontweight': 'bold'})\nplt.xlabel(xlabel = 'Epochs', fontdict = {'size': 12, 'fontweight': 'bold'})\nplt.legend(fontsize = 12)\nplt.tight_layout()\nplt.show()","a944fa5f":"## visualizing the model\nplot_model(mlp_4, show_shapes = True)","8fd7e35a":"## seeding the model prediction with the last six observations in the train set\ntest_seed_CO, test_seed_BE, test_seed_NO = y_CO[-7:-1].tolist(), y_BE[-7:-1].tolist(), y_NO[-7:-1].tolist()\n\n## looping over each of the rows of the test set and generating the predictions\nfor instance in range(X_test.shape[0]):\n    ## parsing the values to a numpy array\n    test_ar_CO, test_ar_BE, test_ar_NO = np.array(test_seed_CO[-6:]), np.array(test_seed_BE[-6:]), np.array(test_seed_NO[-6:])\n\n    ## extracting the predictions on the test set\n    test_preds = mlp_4.predict([\n        X_test[instance, sensor_index].reshape(1, 5),\n        X_test[instance, other_index].reshape(1, 7),\n        test_ar_CO.reshape(1, n_ar_inputs),\n        test_ar_BE.reshape(1, n_ar_inputs),\n        test_ar_NO.reshape(1, n_ar_inputs)\n    ]\n    )\n    \n    ## reshaping the predictions\n    test_preds = np.reshape(test_preds, (-1, ))\n    \n    ## adding the predictions back to the original lists\n    test_seed_CO.append(test_preds[0]), test_seed_BE.append(test_preds[1]), test_seed_NO.append(test_preds[2])\n    \n    ## printing the progress\n    if instance % 200 == 0:\n        print(f'Calculating the predictions for the time step {instance}.')","322d2112":"## stacking the predictions side by side\ntest_preds = np.column_stack([test_seed_CO, test_seed_BE, test_seed_NO])\n\n## removing the first six observation since they come from the train set\ntest_preds = test_preds[6:, :]\n\n## putting the test predictions back on the original scale\ntest_preds = np.expm1(test_preds)\n\n## copying the submission df\nsubmission_mlp_4 = submission.copy()\n\n## putting the predictions on the submission dataset\nsubmission_mlp_4[target_columns] = test_preds","3ee5fb6d":"## predictions\nplt.figure(figsize = (25, 6))\nfor idx, target in enumerate(target_columns):\n    plt.subplot(1, 3, idx + 1)\n    plt.plot(range(train.shape[0]), train[target], label = 'Train')\n    plt.plot(range(train.shape[0], train.shape[0] + test.shape[0]), submission_mlp_4[target], label = 'Test')\n    plt.title(label = target, fontdict = {'size': 14, 'fontweight': 'bold'})\n    plt.xlabel(xlabel = 'Timesteps', fontdict = {'size': 14})\n    plt.ylabel(ylabel = 'Value', fontdict = {'size': 14})\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.legend()\n    plt.tight_layout()\nplt.show()","794a0c14":"## getting the predictions of the model\nmlp_4_predictions = mlp_4.predict([X_mlp_4_sensor, X_mlp_4_others, X_ar_CO_mlp_4, X_ar_BE_mlp_4, X_ar_NO_mlp_4])\n\n## reshaping the predictions\nmlp_4_predictions = np.column_stack(mlp_4_predictions)\n\n## putting the predictions in a dataframe\nmlp_4_predictions = pd.DataFrame(data = mlp_4_predictions, columns = target_columns)\n\n## getting the observed values\nobserved_values = pd.DataFrame(data = np.column_stack([y_ar_CO_mlp_4, y_ar_BE_mlp_4, y_ar_NO_mlp_4]), columns = target_columns)\n\n## residuals over time\nplt.figure(figsize = (25, 6))\nfor idx, target in enumerate(target_columns):\n    plt.subplot(1, 3, idx + 1)\n    plt.plot(observed_values.loc[:, target] - mlp_4_predictions.loc[:, target])\n    plt.title(label = target, fontdict = {'size': 14, 'fontweight': 'bold'})\n    plt.xlabel(xlabel = 'Timesteps', fontdict = {'size': 14})\n    plt.ylabel(ylabel = 'Residuals', fontdict = {'size': 14})\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.tight_layout()\nplt.show()\n\n# distribution of residuals\nplt.figure(figsize = (25, 6))\nfor idx, target in enumerate(target_columns):\n    plt.subplot(1, 3, idx + 1)\n    plt.hist(observed_values.loc[:, target] - mlp_4_predictions.loc[:, target], bins = 50)\n    plt.title(label = target, fontdict = {'size': 14, 'fontweight': 'bold'})\n    plt.xlabel(xlabel = 'Residuals', fontdict = {'size': 14})\n    plt.ylabel(ylabel = 'Frequency', fontdict = {'size': 14})\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.tight_layout()\nplt.show()","c9aca385":"### creating the layers\n## SENSOR DATA\n# input layer\ninput_layer_1 = Input(shape = (X_mlp_4_sensor.shape[1], ))\n# hidden layers\nhidden_layer_1 = Dense(units = 32, activation = 'relu')(input_layer_1)\nhidden_layer_1 = BatchNormalization()(hidden_layer_1)\nhidden_layer_1 = Dense(units = 16, activation = 'relu')(hidden_layer_1)\n\n## OTHER FEATURES\n# input layer\ninput_layer_2 = Input(shape = (X_mlp_4_others.shape[1], ))\n# hidden layers\nhidden_layer_2 = Dense(units = 32, activation = 'relu')(input_layer_2)\n\n## AUTOREGRESSIVE CO\n# input layer\ninput_layer_CO = Input(shape = (X_ar_CO_mlp_4.shape[1], ))\n# hidden layers\nhidden_layer_CO = Dense(units = 32, activation = 'relu')(input_layer_CO)\n\n## AUTOREGRESSIVE BE\n# input layer\ninput_layer_BE = Input(shape = (X_ar_BE_mlp_4.shape[1], ))\n# hidden layers\nhidden_layer_BE = Dense(units = 32, activation = 'relu')(input_layer_BE)\n\n## AUTOREGRESSIVE NO\n# input layer\ninput_layer_NO = Input(shape = (X_ar_NO_mlp_4.shape[1], ))\n# hidden layers\nhidden_layer_NO = Dense(units = 32, activation = 'relu')(input_layer_NO)\n\n## MERGING THE TWO INPUTS\nmerge_layer = Concatenate()([hidden_layer_1, hidden_layer_2, hidden_layer_CO, hidden_layer_BE, hidden_layer_NO])\n## one more layer\nhidden_layer = Dense(units = 32, activation = 'relu')(merge_layer)\nhidden_layer = Dropout(0.1)(hidden_layer)\n# output layers\noutput_layer = Dense(units = 3)(hidden_layer)\n\n## creating the model\nmlp_5 = Model(inputs = [input_layer_1, input_layer_2, input_layer_CO, input_layer_BE, input_layer_NO], outputs = output_layer)\n\n# compilling the model\nmlp_5.compile(optimizer = Adam(learning_rate = 0.0005), loss = 'mse', metrics = [RootMeanSquaredError()])\n\n# instantiating the early stopping callback\nes = EarlyStopping(monitor = 'val_root_mean_squared_error', patience = 20, min_delta = 0.002, restore_best_weights = True)\n\n# instantiating the learning rate scheduller\nlrs = ReduceLROnPlateau(monitor = 'val_root_mean_squared_error', factor = 0.2, patience = 5)\n\n# fitting the model\nhistory = mlp_5.fit(x = [X_mlp_4_sensor, X_mlp_4_others, X_ar_CO_mlp_4, X_ar_BE_mlp_4, X_ar_NO_mlp_4], \n                    y = y_ar_mlp_5, \n                    validation_split = 0.3, batch_size = 8, epochs = 100, callbacks = [es, lrs])\n\n# training history\nplt.figure(figsize = (10, 6))\nplt.plot(history.history['loss'], label = 'training')\nplt.plot(history.history['val_loss'], label = 'validation')\nplt.title(label = 'Training over epochs', fontdict = {'size': 14, 'fontweight': 'bold'})\nplt.ylabel(ylabel = 'RMSE', fontdict = {'size': 12, 'fontweight': 'bold'})\nplt.xlabel(xlabel = 'Epochs', fontdict = {'size': 12, 'fontweight': 'bold'})\nplt.legend(fontsize = 12)\nplt.tight_layout()\nplt.show()","e3478986":"## visualizing the model\nplot_model(mlp_5, show_shapes = True)","43d9c821":"## seeding the model prediction with the last six observations in the train set\ntest_seed_CO, test_seed_BE, test_seed_NO = y_CO[-7:-1].tolist(), y_BE[-7:-1].tolist(), y_NO[-7:-1].tolist()\n\n## looping over each of the rows of the test set and generating the predictions\nfor instance in range(X_test.shape[0]):\n    ## parsing the values to a numpy array\n    test_ar_CO, test_ar_BE, test_ar_NO = np.array(test_seed_CO[-6:]), np.array(test_seed_BE[-6:]), np.array(test_seed_NO[-6:])\n\n    ## extracting the predictions on the test set\n    test_preds = mlp_5.predict([\n        X_test[instance, sensor_index].reshape(1, 5),\n        X_test[instance, other_index].reshape(1, 7),\n        test_ar_CO.reshape(1, n_ar_inputs),\n        test_ar_BE.reshape(1, n_ar_inputs),\n        test_ar_NO.reshape(1, n_ar_inputs)\n    ]\n    )\n    \n    ## reshaping the predictions\n    test_preds = np.reshape(test_preds, (-1, ))\n    \n    ## adding the predictions back to the original lists\n    test_seed_CO.append(test_preds[0]), test_seed_BE.append(test_preds[1]), test_seed_NO.append(test_preds[2])\n    \n    ## printing the progress\n    if instance % 200 == 0:\n        print(f'Calculating the predictions for the time step {instance}.')","0adf23a5":"## stacking the predictions side by side\ntest_preds = np.column_stack([test_seed_CO, test_seed_BE, test_seed_NO])\n\n## removing the first six observation since they come from the train set\ntest_preds = test_preds[6:, :]\n\n## putting the test predictions back on the original scale\ntest_preds = np.expm1(test_preds)\n\n## copying the submission df\nsubmission_mlp_5 = submission.copy()\n\n## putting the predictions on the submission dataset\nsubmission_mlp_5[target_columns] = test_preds","8f263941":"## predictions\nplt.figure(figsize = (25, 6))\nfor idx, target in enumerate(target_columns):\n    plt.subplot(1, 3, idx + 1)\n    plt.plot(range(train.shape[0]), train[target], label = 'Train')\n    plt.plot(range(train.shape[0], train.shape[0] + test.shape[0]), submission_mlp_5[target], label = 'Test')\n    plt.title(label = target, fontdict = {'size': 14, 'fontweight': 'bold'})\n    plt.xlabel(xlabel = 'Timesteps', fontdict = {'size': 14})\n    plt.ylabel(ylabel = 'Value', fontdict = {'size': 14})\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.legend()\n    plt.tight_layout()\nplt.show()","6d6c13c4":"## getting the predictions of the model\nmlp_5_predictions = mlp_5.predict([X_mlp_4_sensor, X_mlp_4_others, X_ar_CO_mlp_4, X_ar_BE_mlp_4, X_ar_NO_mlp_4])\n\n## putting the predictions in a dataframe\nmlp_5_predictions = pd.DataFrame(data = mlp_5_predictions, columns = target_columns)\n\n## getting the observed values\nobserved_values = pd.DataFrame(data = y_ar_mlp_5, columns = target_columns)\n\n## residuals over time\nplt.figure(figsize = (25, 6))\nfor idx, target in enumerate(target_columns):\n    plt.subplot(1, 3, idx + 1)\n    plt.plot(observed_values.loc[:, target] - mlp_5_predictions.loc[:, target])\n    plt.title(label = target, fontdict = {'size': 14, 'fontweight': 'bold'})\n    plt.xlabel(xlabel = 'Timesteps', fontdict = {'size': 14})\n    plt.ylabel(ylabel = 'Residuals', fontdict = {'size': 14})\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.tight_layout()\nplt.show()\n\n# distribution of residuals\nplt.figure(figsize = (25, 6))\nfor idx, target in enumerate(target_columns):\n    plt.subplot(1, 3, idx + 1)\n    plt.hist(observed_values.loc[:, target] - mlp_5_predictions.loc[:, target], bins = 50)\n    plt.title(label = target, fontdict = {'size': 14, 'fontweight': 'bold'})\n    plt.xlabel(xlabel = 'Residuals', fontdict = {'size': 14})\n    plt.ylabel(ylabel = 'Frequency', fontdict = {'size': 14})\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.tight_layout()\nplt.show()","65abcd2a":"### creating the layers\n## SENSOR DATA\n# input layer\ninput_layer_1 = Input(shape = (X_mlp_6_sensor.shape[1], ))\n# hidden layers\nhidden_layer_1 = Dense(units = 32, activation = 'relu')(input_layer_1)\nhidden_layer_1 = BatchNormalization()(hidden_layer_1)\nhidden_layer_1 = Dense(units = 16, activation = 'relu')(hidden_layer_1)\n\n## TEMPERATURE DATA\n# input layer\ninput_layer_2 = Input(shape = (X_temp_mlp_6.shape[1], ))\n# hidden layers\nhidden_layer_2 = Dense(units = 32, activation = 'relu')(input_layer_2)\n\n## RELATIVE HUMIDITY DATA\n# input layer\ninput_layer_3 = Input(shape = (X_rl_mlp_6.shape[1], ))\n# hidden layers\nhidden_layer_3 = Dense(units = 32, activation = 'relu')(input_layer_3)\n\n## AUTOREGRESSIVE CO\n# input layer\ninput_layer_CO = Input(shape = (X_ar_CO_mlp_6.shape[1], ))\n# hidden layers\nhidden_layer_CO = Dense(units = 32, activation = 'relu')(input_layer_CO)\n\n## AUTOREGRESSIVE BE\n# input layer\ninput_layer_BE = Input(shape = (X_ar_BE_mlp_6.shape[1], ))\n# hidden layers\nhidden_layer_BE = Dense(units = 32, activation = 'relu')(input_layer_BE)\n\n## AUTOREGRESSIVE NO\n# input layer\ninput_layer_NO = Input(shape = (X_ar_NO_mlp_6.shape[1], ))\n# hidden layers\nhidden_layer_NO = Dense(units = 32, activation = 'relu')(input_layer_NO)\n\n## MERGING THE TWO INPUTS\nmerge_layer = Concatenate()([hidden_layer_1, hidden_layer_2, hidden_layer_3, hidden_layer_CO, hidden_layer_BE, hidden_layer_NO])\n## one more layer\nhidden_layer = Dense(units = 32, activation = 'relu')(merge_layer)\nhidden_layer = Dropout(0.1)(hidden_layer)\n# output layers\noutput_layer = Dense(units = 3)(hidden_layer)\n\n## creating the model\nmlp_6 = Model(inputs = [input_layer_1, input_layer_2, input_layer_3, input_layer_CO, input_layer_BE, input_layer_NO], outputs = output_layer)\n\n# compilling the model\nmlp_6.compile(optimizer = Adam(learning_rate = 0.0005), loss = 'mse', metrics = [RootMeanSquaredError()])\n\n# instantiating the early stopping callback\nes = EarlyStopping(monitor = 'val_root_mean_squared_error', patience = 20, min_delta = 0.002, restore_best_weights = True)\n\n# instantiating the learning rate scheduller\nlrs = ReduceLROnPlateau(monitor = 'val_root_mean_squared_error', factor = 0.2, patience = 5)\n\n# fitting the model\nhistory = mlp_6.fit(x = [X_mlp_6_sensor, X_temp_mlp_6, X_rl_mlp_6, X_ar_CO_mlp_6, X_ar_BE_mlp_6, X_ar_NO_mlp_6], \n                    y = y_ar_mlp_6, \n                    validation_split = 0.3, batch_size = 8, epochs = 100, callbacks = [es, lrs])\n\n# training history\nplt.figure(figsize = (10, 6))\nplt.plot(history.history['loss'], label = 'training')\nplt.plot(history.history['val_loss'], label = 'validation')\nplt.title(label = 'Training over epochs', fontdict = {'size': 14, 'fontweight': 'bold'})\nplt.ylabel(ylabel = 'RMSE', fontdict = {'size': 12, 'fontweight': 'bold'})\nplt.xlabel(xlabel = 'Epochs', fontdict = {'size': 12, 'fontweight': 'bold'})\nplt.legend(fontsize = 12)\nplt.tight_layout()\nplt.show()","64a3ed04":"## visualizing the model\nplot_model(mlp_6, show_shapes = True)","4ab0cbf9":"## seeding the model prediction with the last six observations in the train set\ntest_seed_CO, test_seed_BE, test_seed_NO = y_CO[-7:-1].tolist(), y_BE[-7:-1].tolist(), y_NO[-7:-1].tolist()\n\n## looping over each of the rows of the test set and generating the predictions\nfor instance in range(X_test.shape[0]):\n    ## parsing the values to a numpy array\n    test_ar_CO, test_ar_BE, test_ar_NO = np.array(test_seed_CO[-6:]), np.array(test_seed_BE[-6:]), np.array(test_seed_NO[-6:]), \n\n    ## extracting the predictions on the test set\n    test_preds = mlp_6.predict([\n        X_test[instance, sensor_index].reshape(1, X_mlp_6_sensor.shape[1]),\n        test_ar_TEMP[instance, :].reshape(1, X_temp_mlp_6.shape[1]),\n        test_ar_RL[instance, :].reshape(1, X_rl_mlp_6.shape[1]),\n        test_ar_CO.reshape(1, X_ar_CO_mlp_6.shape[1]),\n        test_ar_BE.reshape(1, X_ar_BE_mlp_6.shape[1]),\n        test_ar_NO.reshape(1, X_ar_NO_mlp_6.shape[1])\n    ]\n    )\n    \n    ## reshaping the predictions\n    test_preds = np.reshape(test_preds, (-1, ))\n    \n    ## adding the predictions back to the original lists\n    test_seed_CO.append(test_preds[0]), test_seed_BE.append(test_preds[1]), test_seed_NO.append(test_preds[2])\n    \n    ## printing the progress\n    if instance % 200 == 0:\n        print(f'Calculating the predictions for the time step {instance}.')","8c7c4aa0":"## stacking the predictions side by side\ntest_preds = np.column_stack([test_seed_CO, test_seed_BE, test_seed_NO])\n\n## removing the first six observation since they come from the train set\ntest_preds = test_preds[6:, :]\n\n## putting the test predictions back on the original scale\ntest_preds = np.expm1(test_preds)\n\n## copying the submission df\nsubmission_mlp_6 = submission.copy()\n\n## putting the predictions on the submission dataset\nsubmission_mlp_6[target_columns] = test_preds","1a88d4f3":"## predictions\nplt.figure(figsize = (25, 6))\nfor idx, target in enumerate(target_columns):\n    plt.subplot(1, 3, idx + 1)\n    plt.plot(range(train.shape[0]), train[target], label = 'Train')\n    plt.plot(range(train.shape[0], train.shape[0] + test.shape[0]), submission_mlp_6[target], label = 'Test')\n    plt.title(label = target, fontdict = {'size': 14, 'fontweight': 'bold'})\n    plt.xlabel(xlabel = 'Timesteps', fontdict = {'size': 14})\n    plt.ylabel(ylabel = 'Value', fontdict = {'size': 14})\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.legend()\n    plt.tight_layout()\nplt.show()","4c4063f8":"## getting the predictions of the model\nmlp_6_predictions = mlp_6.predict([X_mlp_6_sensor, X_temp_mlp_6, X_rl_mlp_6, X_ar_CO_mlp_6, X_ar_BE_mlp_6, X_ar_NO_mlp_6])\n\n## putting the predictions in a dataframe\nmlp_6_predictions = pd.DataFrame(data = mlp_6_predictions, columns = target_columns)\n\n## getting the observed values\nobserved_values = pd.DataFrame(data = y_ar_mlp_6, columns = target_columns)\n\n## residuals over time\nplt.figure(figsize = (25, 6))\nfor idx, target in enumerate(target_columns):\n    plt.subplot(1, 3, idx + 1)\n    plt.plot(observed_values.loc[:, target] - mlp_6_predictions.loc[:, target])\n    plt.title(label = target, fontdict = {'size': 14, 'fontweight': 'bold'})\n    plt.xlabel(xlabel = 'Timesteps', fontdict = {'size': 14})\n    plt.ylabel(ylabel = 'Residuals', fontdict = {'size': 14})\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.tight_layout()\nplt.show()\n\n# distribution of residuals\nplt.figure(figsize = (25, 6))\nfor idx, target in enumerate(target_columns):\n    plt.subplot(1, 3, idx + 1)\n    plt.hist(observed_values.loc[:, target] - mlp_6_predictions.loc[:, target], bins = 50)\n    plt.title(label = target, fontdict = {'size': 14, 'fontweight': 'bold'})\n    plt.xlabel(xlabel = 'Residuals', fontdict = {'size': 14})\n    plt.ylabel(ylabel = 'Frequency', fontdict = {'size': 14})\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.tight_layout()\nplt.show()","4082c8f1":"### creating the layers\n## SENSOR DATA\n# input layer\ninput_layer_1 = Input(shape = (X_mlp_7_sensor.shape[1], ))\n# hidden layers\nhidden_layer_1 = Dense(units = 32, activation = 'relu')(input_layer_1)\nhidden_layer_1 = BatchNormalization()(hidden_layer_1)\nhidden_layer_1 = Dense(units = 16, activation = 'relu')(hidden_layer_1)\n\n## TEMPERATURE DATA\n# input layer\ninput_layer_2 = Input(shape = (X_temp_mlp_7.shape[1], ))\n# hidden layers\nhidden_layer_2 = Dense(units = 32, activation = 'relu')(input_layer_2)\n\n## RELATIVE HUMIDITY DATA\n# input layer\ninput_layer_3 = Input(shape = (X_rl_mlp_7.shape[1], ))\n# hidden layers\nhidden_layer_3 = Dense(units = 32, activation = 'relu')(input_layer_3)\n\n## AUTOREGRESSIVE CO\n# input layer\ninput_layer_CO = Input(shape = (X_ar_CO_mlp_7.shape[1], ))\n# hidden layers\nhidden_layer_CO = Dense(units = 64, activation = 'relu')(input_layer_CO)\n\n## AUTOREGRESSIVE BE\n# input layer\ninput_layer_BE = Input(shape = (X_ar_BE_mlp_7.shape[1], ))\n# hidden layers\nhidden_layer_BE = Dense(units = 32, activation = 'relu')(input_layer_BE)\n\n## AUTOREGRESSIVE NO\n# input layer\ninput_layer_NO = Input(shape = (X_ar_NO_mlp_7.shape[1], ))\n# hidden layers\nhidden_layer_NO = Dense(units = 32, activation = 'relu')(input_layer_NO)\n\n## MERGING THE TWO INPUTS\nmerge_layer = Concatenate()([hidden_layer_1, hidden_layer_2, hidden_layer_3, hidden_layer_CO, hidden_layer_BE, hidden_layer_NO])\n## one more layer\nhidden_layer = Dense(units = 32, activation = 'relu')(merge_layer)\nhidden_layer = Dropout(0.1)(hidden_layer)\n# output layers\noutput_layer = Dense(units = 3)(hidden_layer)\n\n## creating the model\nmlp_7 = Model(inputs = [input_layer_1, input_layer_2, input_layer_3, input_layer_CO, input_layer_BE, input_layer_NO], outputs = output_layer)\n\n# compilling the model\nmlp_7.compile(optimizer = Adam(learning_rate = 0.0005), loss = 'mse', metrics = [RootMeanSquaredError()])\n\n# instantiating the early stopping callback\nes = EarlyStopping(monitor = 'val_root_mean_squared_error', patience = 20, min_delta = 0.002, restore_best_weights = True)\n\n# instantiating the learning rate scheduller\nlrs = ReduceLROnPlateau(monitor = 'val_root_mean_squared_error', factor = 0.2, patience = 5)\n\n# fitting the model\nhistory = mlp_7.fit(x = [X_mlp_7_sensor, X_temp_mlp_7, X_rl_mlp_7, X_ar_CO_mlp_7, X_ar_BE_mlp_7, X_ar_NO_mlp_7], \n                    y = y_ar_mlp_7, \n                    validation_split = 0.3, batch_size = 8, epochs = 100, callbacks = [es, lrs])\n\n# training history\nplt.figure(figsize = (10, 6))\nplt.plot(history.history['loss'], label = 'training')\nplt.plot(history.history['val_loss'], label = 'validation')\nplt.title(label = 'Training over epochs', fontdict = {'size': 14, 'fontweight': 'bold'})\nplt.ylabel(ylabel = 'RMSE', fontdict = {'size': 12, 'fontweight': 'bold'})\nplt.xlabel(xlabel = 'Epochs', fontdict = {'size': 12, 'fontweight': 'bold'})\nplt.legend(fontsize = 12)\nplt.tight_layout()\nplt.show()","037466a6":"## visualizing the model\nplot_model(mlp_7, show_shapes = True)","64f7a573":"## seeding the model prediction with the last six observations in the train set\ntest_seed_CO, test_seed_BE, test_seed_NO = y_CO[-26:-1].tolist(), y_BE[-26:-1].tolist(), y_NO[-26:-1].tolist()\n\n## looping over each of the rows of the test set and generating the predictions\nfor instance in range(X_test.shape[0]):\n    ## parsing the values to a numpy array\n    test_ar_CO, test_ar_BE, test_ar_NO = np.array(test_seed_CO[-25:]), np.array(test_seed_BE[-25:]), np.array(test_seed_NO[-25:]), \n\n    ## extracting the predictions on the test set\n    test_preds = mlp_7.predict([\n        X_test[instance, sensor_index].reshape(1, X_mlp_7_sensor.shape[1]),\n        test_ar_TEMP[instance, :].reshape(1, X_temp_mlp_7.shape[1]),\n        test_ar_RL[instance, :].reshape(1, X_rl_mlp_7.shape[1]),\n        test_ar_CO.reshape(1, X_ar_CO_mlp_7.shape[1]),\n        test_ar_BE.reshape(1, X_ar_BE_mlp_7.shape[1]),\n        test_ar_NO.reshape(1, X_ar_NO_mlp_7.shape[1])\n    ]\n    )\n    \n    ## reshaping the predictions\n    test_preds = np.reshape(test_preds, (-1, ))\n    \n    ## adding the predictions back to the original lists\n    test_seed_CO.append(test_preds[0]), test_seed_BE.append(test_preds[1]), test_seed_NO.append(test_preds[2])\n    \n    ## printing the progress\n    if instance % 200 == 0:\n        print(f'Calculating the predictions for the time step {instance}.')","97a73637":"## stacking the predictions side by side\ntest_preds = np.column_stack([test_seed_CO, test_seed_BE, test_seed_NO])\n\n## removing the first six observation since they come from the train set\ntest_preds = test_preds[25:, :]\n\n## putting the test predictions back on the original scale\ntest_preds = np.expm1(test_preds)\n\n## copying the submission df\nsubmission_mlp_7 = submission.copy()\n\n## putting the predictions on the submission dataset\nsubmission_mlp_7[target_columns] = test_preds","dae53dbb":"## predictions\nplt.figure(figsize = (25, 6))\nfor idx, target in enumerate(target_columns):\n    plt.subplot(1, 3, idx + 1)\n    plt.plot(range(train.shape[0]), train[target], label = 'Train')\n    plt.plot(range(train.shape[0], train.shape[0] + test.shape[0]), submission_mlp_7[target], label = 'Test')\n    plt.title(label = target, fontdict = {'size': 14, 'fontweight': 'bold'})\n    plt.xlabel(xlabel = 'Timesteps', fontdict = {'size': 14})\n    plt.ylabel(ylabel = 'Value', fontdict = {'size': 14})\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.legend()\n    plt.tight_layout()\nplt.show()","77b5d7e4":"## getting the predictions of the model\nmlp_7_predictions = mlp_7.predict([X_mlp_7_sensor, X_temp_mlp_7, X_rl_mlp_7, X_ar_CO_mlp_7, X_ar_BE_mlp_7, X_ar_NO_mlp_7])\n\n## putting the predictions in a dataframe\nmlp_7_predictions = pd.DataFrame(data = mlp_7_predictions, columns = target_columns)\n\n## getting the observed values\nobserved_values = pd.DataFrame(data = y_ar_mlp_7, columns = target_columns)\n\n## residuals over time\nplt.figure(figsize = (25, 6))\nfor idx, target in enumerate(target_columns):\n    plt.subplot(1, 3, idx + 1)\n    plt.plot(observed_values.loc[:, target] - mlp_7_predictions.loc[:, target])\n    plt.title(label = target, fontdict = {'size': 14, 'fontweight': 'bold'})\n    plt.xlabel(xlabel = 'Timesteps', fontdict = {'size': 14})\n    plt.ylabel(ylabel = 'Residuals', fontdict = {'size': 14})\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.tight_layout()\nplt.show()\n\n# distribution of residuals\nplt.figure(figsize = (25, 6))\nfor idx, target in enumerate(target_columns):\n    plt.subplot(1, 3, idx + 1)\n    plt.hist(observed_values.loc[:, target] - mlp_7_predictions.loc[:, target], bins = 50)\n    plt.title(label = target, fontdict = {'size': 14, 'fontweight': 'bold'})\n    plt.xlabel(xlabel = 'Residuals', fontdict = {'size': 14})\n    plt.ylabel(ylabel = 'Frequency', fontdict = {'size': 14})\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.tight_layout()\nplt.show()","1757aeb1":"submission_mlp_1.to_csv(path_or_buf = 'submission_mlp_1.csv', index = False)\nsubmission_mlp_2.to_csv(path_or_buf = 'submission_mlp_2.csv', index = False)\nsubmission_mlp_3.to_csv(path_or_buf = 'submission_mlp_3.csv', index = False)\nsubmission_mlp_4.to_csv(path_or_buf = 'submission_mlp_4.csv', index = False)\nsubmission_mlp_5.to_csv(path_or_buf = 'submission_mlp_5.csv', index = False)\nsubmission_mlp_6.to_csv(path_or_buf = 'submission_mlp_6.csv', index = False)\nsubmission_mlp_7.to_csv(path_or_buf = 'submission_mlp_7.csv', index = False)","7d8bfe41":"# Data preparation","a5be3e8c":"## MLP #3 - Multi-input MLP","1cd95724":"## Multi-input (AR targets 12hrs+ AR temperature 12hrs + AR relative humidity 12hrs) and single output MLP - MLP #7","cebe1c7b":"## MLP #2 - Multi-output MLP","f024c0b0":"# Functions used in this notebook","2802f6ac":"## Multi-input (AR targets 6hrs+ AR temperature 12hrs + AR relative humidity 12hrs) and single output MLP - MLP #6","7cb3a07d":"## MLP #1 - Simple MLP","4c12276b":"## For the multi-feature-input single-output MLP - MLP #3","54fbc924":"# Saving the submission file","ea48f8c3":"## For the simplest form of the MLP - MLP #1","a347215b":"## MLP #4 - Multi-input-output autoregressive MLP ","c5451630":"# Feature engineering","b305c636":"# Correlations","3b30264a":"## MLP #6 - Multi-AR inputs (12h lag features + 6h lag targets) and single output MLP ","1ccfc21c":"## MLP #7 - Multi-AR inputs (12h lag features + 12h lag targets) and single output MLP","83f0383d":"## For multi-input (autoregressive targets + features) and multi-output MLP - MLP #4","7c422481":"## For multi-input (autoregressive targets + features) and single output MLP - MLP #5","da54bc0e":"# Loading modules and packages","8d6c7451":"# Modelling","c234641b":"# Loading the data","d80ceb1e":"## MLP #6 - Multi-input single-output MLP on autoregressive targets","5c91368a":"## For the multi-output MLP - MLP #2"}}