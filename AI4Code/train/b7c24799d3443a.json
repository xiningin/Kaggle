{"cell_type":{"c24fb495":"code","c6d273c3":"code","a2ce2533":"code","cd664724":"code","c54d7dcc":"code","4cb4eb37":"code","45b751bd":"code","704b7a73":"code","490cfedf":"code","0f6a7fea":"code","344de983":"code","aa5c609a":"code","bc69b632":"code","3ede5384":"code","22599e7d":"code","4c593436":"code","986e64bb":"markdown","d7ba6cd4":"markdown","bda3cd47":"markdown","1a1bfd0e":"markdown","3c057d87":"markdown","d7d20fab":"markdown","69b97cc6":"markdown","386bd43c":"markdown","e47c89d8":"markdown","b90ea571":"markdown","671e3ff3":"markdown","9966bb34":"markdown","77015478":"markdown","06d23f50":"markdown","a7f5c4a0":"markdown","c33bb554":"markdown","8f4026d9":"markdown","7c123097":"markdown","a45508d8":"markdown","1cb95e4d":"markdown","d092029f":"markdown","1de1e59e":"markdown","60d3e45e":"markdown","03f2c33f":"markdown","66bb54ba":"markdown","b007838f":"markdown","81080d84":"markdown","f440730b":"markdown","dad910db":"markdown","73426dcf":"markdown","b76792ce":"markdown","67e65548":"markdown","027c99a1":"markdown","c227c45e":"markdown","416ab849":"markdown","70023406":"markdown","0ac6a40a":"markdown","1c038301":"markdown","49caf5bc":"markdown","8328dc26":"markdown","16ff5b2d":"markdown"},"source":{"c24fb495":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\n\ndataset = load_breast_cancer()\nX, y = dataset.data, dataset.target\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nprint(dataset.DESCR)","c6d273c3":"from sklearn.svm import SVC\n\nclf = SVC(kernel = 'linear').fit(X_train, y_train)\nclf.score(X_test, y_test)","a2ce2533":"from sklearn.metrics import confusion_matrix\n\ny_predicted = clf.predict(X_test)\nconfusion_matrix(y_test, y_predicted)","cd664724":"from sklearn.metrics import precision_score, recall_score, f1_score\n\n#We will calculate the previously discussed metrics for the model we created before\n#precision_score, recall_score and f1_score are methods that takes the original labels and the predicted labels \n#as parameters and return the desired score\nprint('Precision score is {:.3f}'.format(precision_score(y_test, y_predicted)))\nprint('Recall score is {:.3f}'.format(recall_score(y_test, y_predicted)))\nprint('F1-score is {:.3f}'.format(f1_score(y_test, y_predicted)))","c54d7dcc":"#desicion_function\ny_scores = clf.decision_function(X_test)\ny_scores_list = list(zip(y_test, y_scores))\ny_scores_list[:20]","4cb4eb37":"from sklearn.metrics import precision_recall_curve\n\nprecision, recall, threshold = precision_recall_curve(y_test, y_scores)\nplt.figure(figsize = (8,6), dpi = 120)\nplt.plot(precision, recall)\nplt.xlabel('Precision')\nplt.ylabel('Recall')\nplt.title('Precision-recall curve')\nplt.show()\n#obviously, the best threshold is the curve is near to the top right corner as both precision and recall are high, unless\n#we want to maximize one of them at the expense of the other","45b751bd":"from sklearn.metrics import roc_curve\n\n#like precision-recall curve, roc_curve method returns 3 parameters: fpr (false positives rate), tpr (true positives rate)\n#and thresholds. We will explain the first 2 in detail.\n\nfpr, tpr, thresholds = roc_curve(y_test, y_scores)\nplt.figure(dpi = 130)\nplt.plot(fpr, tpr)\nplt.xlabel('False Positives Rate')\nplt.ylabel('True Positives Rate')\nplt.title('ROC curve')\nplt.plot([0,1], [0,1], color= 'r', linestyle = '--')\nplt.show()","704b7a73":"plt.figure(dpi = 120, figsize = (10, 2.5))\npositive = [19, 20, 23, 24, 26, 27, 28, 29, 30]\nplt.plot(positive, [0] * 9, 'o', markersize = 10, label = 'Positive')\nplt.plot(list(set(range(1,31)) - set(positive)), [0] * 21, 'o', markersize = 10, label = 'Negative')\nplt.hlines(0, 1, 30)\nplt.xlabel('Decision Function Score')\nplt.legend(title = 'Original Labels')\nplt.title('Hypothetical Dataset')\nplt.gca().axes.get_yaxis().set_visible(False)","490cfedf":"scores = np.array(list(range(1, 31)))\nlabels = np.zeros(31)\nlabels[positive] = 1\nfpr, tpr, _ = roc_curve(labels[1:], scores)\nplt.figure(dpi = 130)\nplt.plot(fpr, tpr)\nplt.xlabel('False Positives Rate')\nplt.ylabel('True Positives Rate')\nplt.title('ROC curve for nearly perfect model')\nplt.show()","0f6a7fea":"plt.figure(dpi = 120, figsize = (10, 2.5))\nplt.plot(list(set(range(1,31)) - set(positive)), [0] * 21, 'o', markersize = 10, label = 'Positive')\nplt.plot(positive, [0] * 9, 'o', markersize = 10, label = 'Negative')\nplt.hlines(0, 1, 30)\nplt.xlabel('Decision Function Score')\nplt.legend(title = 'Original Labels')\nplt.title('Hypothetical Dataset')\ntplt.gca().axes.get_yaxis().set_visible(False)","344de983":"scores = np.array(list(range(1, 31)))\nlabels = np.ones(31)\nlabels[positive] = 0\nfpr, tpr, _ = roc_curve(labels[1:], scores)\nplt.figure(dpi = 130)\nplt.plot(fpr, tpr)\nplt.xlabel('False Positives Rate')\nplt.ylabel('True Positives Rate')\nplt.title('ROC curve for a bad model')\nplt.show()","aa5c609a":"random_positive = [1, 2, 4, 7,8,9, 14, 20, 22, 23, 24, 27, 29, 30]\nplt.figure(dpi = 120, figsize = (10, 2.5))\nplt.plot(random_positive, [0] * len(random_positive), 'o', markersize = 10, label = 'Positive')\nplt.plot(list(set(range(1, 31)) - set(random_positive)), [0] * (30 - len(random_positive)),\n         'o', markersize = 10, label = 'Negative')\nplt.hlines(0, 1, 30)\nplt.xlabel('Decision Function Score')\nplt.legend(title = 'Original Labels')\nplt.title('Hypothetical Dataset')\nplt.gca().axes.get_yaxis().set_visible(False)","bc69b632":"scores = np.array(list(range(1, 31)))\nlabels = np.zeros(31)\nlabels[random_positive] = 1\nfpr, tpr, _ = roc_curve(labels[1:], scores)\nplt.figure(dpi = 130)\nplt.plot(fpr, tpr)\nplt.xlabel('False Positives Rate')\nplt.ylabel('True Positives Rate')\nplt.title('ROC curve for a random model')\nplt.show()","3ede5384":"from sklearn.metrics import auc\n\n#We will calculate AUC for the original model we were using\ny_predicted = clf.predict(X_test)\nfpr, tpr, _ = roc_curve(y_test, y_predicted)\n#auc method takes false positives rates and true positives rates as parameters\nauc_score = auc(fpr, tpr)\nprint(auc_score)","22599e7d":"from sklearn.datasets import load_diabetes\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.linear_model import LinearRegression\n\ndataset = load_diabetes()\nX, y = dataset.data, dataset.target\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nlinreg = LinearRegression().fit(X_train, y_train)\ny_predicted = linreg.predict(X_test)\nprint('R2 score is {:.3f}'.format(r2_score(y_test, y_predicted)))\nprint('Mean squared error is {:.3f}'.format(mean_squared_error(y_test, y_predicted)))","4c593436":"from sklearn.model_selection import GridSearchCV\n\ndataset = load_breast_cancer()\nX, y = dataset.data, dataset.target\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nparam_list = {'gamma' : [0.01, 0.1, 1, 10], 'C': [0.1, 1, 10, 100]}\nmodel = GridSearchCV(SVC(kernel = 'rbf'), param_grid = param_list, scoring = 'recall')\nmodel.fit(X_train, y_train)\n#GridSearchCV has attributes called best_params_ and best_score_ that stores the values of parameters that produced\n#the maximum desired score and the score itself respectively\nprint('The best parameters are: ', model.best_params_)\nprint('The best recall score is {:.3f}'.format(model.best_score_))","986e64bb":"* $Precision = \\frac{TP}{TP + FP}$\n* $Recall = \\frac{TP}{TP + FN}$","d7ba6cd4":"There are some easy and straightforward ways in sklearn for revealing the probability scores we talked about. We will cover `desicion_function` method. It gives a high absolute scores in positive for high probabilities and high absolute values in negative for low ones.","bda3cd47":"A natural question that could come up to your mind is: \"What is the relation between those interpretations and the metrics used on the axes?\". We will use some simple visualizations that could illustrate the idea.","1a1bfd0e":"### The issue of skewed data\nHere we get a good accuracy for the test set which gives a clue that this model generalizes well even for the examples that it have not seen. Then, what is the problem of using accuracy alone? Let's consider the following scenario: suppose you are working on a classifier that detects spam emails and the dataset that you have contains **1000** training example and **970** of those examples were labeled as non-spam. In this case, if we use a very naive algorithm that marks any input as non-spam, then this \"non-model\" will get an accuracy score of **97%**! although we are not doing anything. Scenarios and datasets like this exist in real world problems and as we saw, accuracy is not enough in situations like this, so we must resort to other metrics that could be helpful.","3c057d87":"Now, let's calculate precision and recall for the \"non-model\" we discussed before. In this case, $TP = 0$, $TN = 970$, $FP = 0$ & $FN = 30$. Therefore, both precision and recall will be equal to **0**. Unlike accuracy, precision and recall detected the problem in this naive model evidently, so we can notice flaws like this in an early stage using those metrics.","d7d20fab":"Now, let's tackle some elements in this visualization:\n* The y-axis (True Positives Rate): it is just a synonym for \"recall\" which we now know.\n* The x-axis (False Positives Rate): it is another quantity that is derived from the confusion matrix like precision and recall. It is calculated as follows: $FPR = \\frac{FP}{FP + TN}$. It can be interpreted as the answer of this question: \"how many examples were mislabeled as positive from the examples that were originally labeled as negative?\".\n* The ROC curve shows how both FPR and TPR changes while changing the values of threshold descendingly.","69b97cc6":"This model is not very successful in separating positive and negative examples as we see. As a result, decreasing the threshold gradually will increase both FPR and TPR nearly at an equal rate because examples are not separated well. This will cause the ROC curve to look like a diagonal line (near to the red dashed one in the first figure of ROC curves). Let's plot the ROC curve of this model to visualize our conclusions.","386bd43c":"Now we made a lot of theory, let's take a look on some code!","e47c89d8":"As we keep descending, the number of true positives will increase fast until we get all the positve examples correctly (recall will be then 1.0). After that, the number of false positives will increase (because we are predicting more and more examples as positive without being very confident) until both of them become 1.0. The following code plots an ROC curve of this hypothetical model.","b90ea571":"## ROC curves\n**ROC** or **Receiver Operating Characteristic** curves is another performance measure that detects how well the model separates positive and negative examples. ROC curves are little bit tricky to interpret but we will tackle them step by step. I will first plot it for the model we are using and then start explaining some of its details.","671e3ff3":"Now let's imagine that we will start with a very high threshold, for example: 30, and then descend gradually (this is what ROC does). At the first 5 descends: all the examples that will be predicted as positive will be true positives, so the recall (true positives rate) will increase rapidly because we are predicting more positive examples correctly (that is what recall captures). At the same time, there is no \"false negatives\" so the curve will keep being on the left in those descends. The previous sentences explain the \"almost-vertical\" line that exist in the ROC curves of good models.","9966bb34":"Then, how could we get information from ROC curves or how to know a good model from a bad one from ROC?. The answer is relatively easy: the nearer the curve of the model to the top left corner (like the blue one above) and the farther it is from the red dashed line, the better the model is in separating positive and negative examples. The nearer the curve is to the red line, the more it becomes evident that the model randomly guesses the labels (very bad model). The nearer the curve is to the lower right corner, the more it is evident that the model separates positive and negative examples but in opposite! (also a bad model).","77015478":"Now let's move on to the random model scenario.","06d23f50":"I will show the code for generating the confusion matrix and then explain the meaning behind each element in it.","a7f5c4a0":"SVM has parameters like `gamma` and `C` that control regularization and senstivity of the model. Those parameters will be tuned using `GridSearchCV`.","c33bb554":"First, we will load the needed libraries and the dataset and store them in variables we will use and also split our dataset into a training set and a test set.","8f4026d9":"### Area Under the Curve (AUC)\nAfter understanding the intuition behind ROC curves, we can delve into AUC (Area Under the Curve) concept easily. As we explained, the nearer the curve is to the top left corner, the better it is. **AUC** metric captures this property obviously because when the curve is near to the top left corner, the area under it will be larger. Actually, a perfect AUC score is 1.0, while an intermediate value around 0.5 means a random model. A value of AUC that is close to 0 detects an \"opposite-to-good\" model. Here is a code for finding AUC score in sklearn:","7c123097":"A quesion that could come up in your mind is: \"Why could not we simply average precision and recall instead of this weird-looking fomula?\". The answer is that the average does not detect if one of precision or recall was noticeably low. For example, if precision was 0.1 and recall was 0.99 then the average of them is 0.545 which may look like as we have a moderate model, however, by looking at the single values of precision and recall, we could see that there is a serious problem in precision which will be detected easily with F1-score (It will be 0.182 in the previous example).","a45508d8":"`SVC` and nearly all classifiers have a built-in method in sklearn for calculating the accuracy of the model which is called `score`.","1cb95e4d":"## F1-score\nFortunately, there are methods that calculate precision and recall scores even without looking at confusion matrix. Before diving into the code, I would like to mention an important metric that is related to both precision and recall which is **F1-score**. **F1-score** simply captures how much a model is good at both of precision and recall by calculating what is known as the \"harmonic mean\" of them. The formula for **F1-score** is:\n* $F1 = \\frac{2 * precision * recall}{precision + recall}$","d092029f":"As we see, it is a 2\u2716\ufe0f2 matrix in the binary case. each element has a specific meaning as follows:\n* Upper left element: **True Negatives** or **TN**. It is the number of examples that had a true label of negative and were also predicted as negative.\n* Upper right element: **False Positives** or **FP**. It is the number of examples that had a true label of negative and were predicted \"wrongly\" as positive.\n* Lower left element: **False Negatives** or **FN**. It is the number of examples that had a true label of positive and were predicted \"wrongly\" as negative.\n* Lower right element: **True Positives** or **TP**. It is the number of examples that had a true label of positive and were also predicted as positive.","1de1e59e":"# Classification\nEvaluating a classification model is usually a tricky job and the techniques used vary based on the nature of the data. Some metrics become useless in certain cases as we will see in a bit. I will use breast cancer dataset which is available in sklearn for the purpose of illustration. It is a binary classification dataset where each example is labeled as either 0 (malignant) or 1 (benign).","60d3e45e":"## Precision & Recall\nPrecision and recall are widely-used metrics in classification problems and are easily calculated using confusion matrices as we will see. We will show the formulas and then explain the interpretation of each one.","03f2c33f":"# Regression\nEvaluating a regressor is bit easier than evaluating a classifier. Most of the times, R2 score is good for evaluating algorithms like linear regression. It is a statistic measure that captures how much the prediction of the model is correlated to the actual values. Another metric is mean squared error that sums the square differences between the prediction of each model and the real value and divides that by the number of example. We will use the diabetes dataset from sklearn for the purpose of illustration.","66bb54ba":"Another famous classification problem is spam detection. In this problem, false positives is worse than false negatives (spam email is the positive class). In another words, predicting a non-spam email as a spam one is much worse than the opposite case as this may lead to missing important emails when they are sent to spam folder. Therefore, we need to maximize the **precision** score (refer to previous definitions to link with this problem).","b007838f":"## Precision & Recall trade off\nBased on the application or the problem we are working on, we decide what we should be trying to maximize, either precision or recall. For example, in the breast cancer detection problem (like the dataset we are working on), false negatives is much worse than false positives (malignant is the positive class). In another words, It is more dangerous to misclassify a patient who has a malignant tumor and predict his tumor as a benign one than predicting a non-cancerous tumor as a malignant one as he\/she will go through more checks and analysis. Therefore, we need to maximize the **recall** score (refer to previous definitions to link with this problem).","81080d84":"So, how could we control precision or recall and maximize any of them. This leads us to Decision Functions.","f440730b":"The above figure represents a hypothetical and almost perfect classifier. The figure shows the examples plotted against their score in the decision function. I said \"perfect\" because all the positive examples exist above a certain decision score \"threshold\" and negative scores exist below a certain score \"threshold\", except for very few outliers. This means that the classifier has separated the positive and negative examples very successfully and with using the right threshold, it will give excellent results, so how will this reflect on its ROC curve?","dad910db":"## Accuracy\nAccuracy is probably the measure that will come to your mind first when you think of evaluating a machine learning model. Indeed, accuracy is the easiest metric to interpret. Accuracy is simply defined as the proportion of correctly predicted examples. First, let's try to use a classifier on this dataset and calculate its accuracy and then we will discuss the issues of depending completely on accuracy. For example, I will use linear Support Vector Machine (SVM) for the purpose of testing.","73426dcf":"## Confusion Matrix\nConfusion matrix is a well-known and widely-used metric for evaluating both binary and multi-class classification problems. We will explore the binary version but the same idea applies to the multi-class version.","b76792ce":"Then, how could confusion matrix help us in scenarios like the one discussed previously?! This leads us to the next metrics that are used a lot and they are more useful with skewed datasets.","67e65548":"We can also calculate accuracy using confusion matrix! Obviously, the elements on the diagonal are the ones that were predicted correctly by our model, so we can get accuracy by summing the elements on the diagonal and dividing them by the total number of the elements in the whole matrix or in another way using the previous terminology:\n$accuracy = \\frac{TN + TP}{TN + TP + FP + FN}$","027c99a1":"## Decision Functions\nMost classifiers do the following before labeling an example: they calculate a certain probability for this example within a specific range and set a threshold value for this probabilities. If the probability of this example was below the threshold it will be predicted as negative, otherwise it will be predicted as positive. The higher the probability of the example, the more confident the model is that this example is positive and vice versa. By varying this threshold, we could control precision and recall based on our problem as we saw. For example, if we increase the threshold, this means that our model will not predict an example as positive unless it is very confident that this example belongs to the positive class resulting into incresing **precision**. On the other hand, when we decrease the threshold, this leads to predicting a lot of examples as positive (even if it was not very confident) including those examples that were originally labeled as positive resulting into increasing **recall**.","c227c45e":"This tool is called `GridSearchCV` which takes the untrained model, the list of parameters needed to be tuned as parameters and the metric that we desire to maximize. It will return a model with tuned parameters for us that is ready to be fitted directly to the data. We will try to use Support Vector Machine model with Radial Basis Function \"RBF\" kernel with the breast cancer dataset. We will target maximizing the **recall** because this is more important in a problem like this as we discussed before.","416ab849":"## Precision-recall curve\nSklearn provides a nice tool for plotting precision against recall while varying the thresholds which is known as precision-recall curve. The `precision_recall_curve` method takes as parameters: the true labels of the data and the desicion function of the same data and returns 3 lists: precision , recall and threshold values controling the last 2 scores. after getting these lists we can easily plot them against each other using `matplotlib`.","70023406":"# Introduction\nIn this tutorial I will explain some of the famous and important performance metrics that are very important in the stage of the evaluation of a model. I will start with metrics used for classifiers and then we will cover regressors. SciKit Learn library is used in this tutorial to implement the measures covered in an easy way.","0ac6a40a":"## Sources\n1. Applied Machine Learning in Python course on Coursera by Kevyn Collins-Thompson\n2. Machine Learning course on Coursera by Andrew Ng","1c038301":"To make sure that the terminology is clear for the reader, when I say **\"label\"** I mean the label that existed in the original dataset and when I say **\"prediction\"** I mean the label that was predicted by the model.\n* Precision could be easily interpreted as the answer of the following question: \"How many examples were labeled originally as positive from the examples that were predicted as positive?\"\n* Recall answers the following question: \"How many examples were predicted as positive from the examples that were originally labeled as positive?\"","49caf5bc":"This scenario is very similar to the previous one but in opposite. As we decrease the threshold, we predict more and more negative examples as positive ones. Therefore, the FPR increases rapidly until it gets to 1.0. After that, we get more positive examples correctly so the recall increases also until it gets to 1.0. The ROC of this model will look like this:","8328dc26":"# Selecting a model based on evaluation metrics\nA lot of models that we use in classification or regression have some parameters that could be adjusted and tuned to make the model convenient as possible for our application. Luckily, there is tool in sklearn that does this job for us instead of tuning this parameters manually.","16ff5b2d":"Now, let's explore the opposite case before discussing the random models."}}