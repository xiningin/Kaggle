{"cell_type":{"aa6f64df":"code","1207c0da":"code","aaee6456":"code","2533a163":"code","75558a10":"code","88caa532":"code","82bd4f74":"code","22211661":"code","7bc5e83e":"code","cf653615":"code","f885bbbe":"code","66c84839":"code","5a6f8587":"code","0e39d137":"code","07364fb0":"code","59b10dcd":"code","8061e4c3":"code","c9c4f099":"code","4bfdb7e3":"code","84f9d502":"markdown","102acddc":"markdown","eb01b4f1":"markdown","d7b91803":"markdown","7df1d8ae":"markdown","9ace1a93":"markdown","e710555b":"markdown","0a6a4fb1":"markdown","0f9485f9":"markdown","9603bc26":"markdown","313cd110":"markdown","8bdcd464":"markdown","9c44a3cf":"markdown","2f71ca11":"markdown","2a76e12f":"markdown","e3b5a8aa":"markdown","a202b057":"markdown","94f6e143":"markdown","94502ee4":"markdown"},"source":{"aa6f64df":"import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_regression\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom sklearn.linear_model import LinearRegression","1207c0da":"X, y, coef = make_regression(n_samples = 100,\n                n_features = 1,\n                noise = 10.0,\n                bias = 100.0,\n                random_state = 6,\n                coef = True)\nx = X[:,0]\ncoef","aaee6456":"## use sklearn linearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(X,y)\nprint(\"coefficient:\",lin_reg.coef_[0])\nprint(\"intercept:\", lin_reg.intercept_)","2533a163":"sns.scatterplot(x = x, y=y, label= \"data\")\nsns.lineplot(x = x, \n             y = x * lin_reg.coef_[0] + lin_reg.intercept_,\n             color = \"red\",\n             label = \"Sklearn LinearRegression\")","75558a10":"def h(x,a,b):\n    y_pred = a * x + b\n    return y_pred","88caa532":"def loss(x,y,a,b):\n    y_pred = h(x,a,b)\n    loss = np.sum((y-y_pred) ** 2)\n    return loss","82bd4f74":"loss(x,y,1,1)","22211661":"def gradient(x,y,a,b):\n    y_pred = h(x,a,b)\n    derivative_a = np.sum(-2*(y-y_pred)*x)\n    derivative_b = np.sum(-2*(y-y_pred))\n    return derivative_a, derivative_b","7bc5e83e":"def steps(derivative_a,derivative_b, learning_rate = 0.01):\n    step_a = derivative_a*learning_rate\n    step_b = derivative_b*learning_rate\n    return (step_a, step_b)","cf653615":"def update_params(a,b,step_a,step_b):\n    a_new = a - step_a\n    b_new = b - step_b\n    return a_new, b_new","f885bbbe":"a = 1\nb = 1\n\nderivative_a, derivative_b = gradient(x,y,a,b)\n\nstep_a, step_b = steps(derivative_a,derivative_b,learning_rate = 0.01)\n\na_new, b_new = update_params(a,b,step_a,step_b)\n\nprint(\"Updated coef after one epoch:\", a_new)\nprint(\"Updated intercept after one epoch:\", b_new)","66c84839":"def gradient_descent(x,y,a_init=1, b_init=1, learning_rate = 0.001, n_epochs = 100):\n    a = a_init\n    b = b_init\n    loss_history = []\n    a_history = []\n    b_history = []\n    \n    for epoch in range(n_epochs):\n        a_history.append(a)\n        b_history.append(b)\n        loss_epoch = loss(x,y,a,b)\n        loss_history.append(loss_epoch)\n        \n        derivative_a, derivative_b = gradient(x,y,a,b)\n        \n        step_a, step_b = steps(derivative_a, derivative_b, learning_rate)\n        \n        a, b = update_params(a, b, step_a, step_b)\n        \n    history = {\"loss\": loss_history, \"a\": a_history, \"b\": b_history}\n    return a, b, history\n    ","5a6f8587":"a, b, history = gradient_descent(x,y, learning_rate = 0.001)","0e39d137":"print(\"Calculated coefficient:\", a)\nprint(\"Calculated intercept:\", b)","07364fb0":"sns.scatterplot(x = x, y=y, label= \"data\")\nsns.lineplot(x = x, \n             y = x * lin_reg.coef_[0] + lin_reg.intercept_,\n             color = \"red\",\n             label = \"Sklearn LinearRegression\")\n\nsns.lineplot(x = x, y = x * a + b,\n             color = \"green\",\n             ls = \"--\",\n             label = \"Custom Linear Regression\")\nplt.title(\"Compare Sklearn and Custom Regressor\")","59b10dcd":"plt.plot(history[\"loss\"])","8061e4c3":"a_max = max(history[\"a\"])\na_min = min(history[\"a\"])\n\nb_max = max(history[\"b\"])\nb_min = min(history[\"b\"])\n\nrange_a = np.linspace(a_min, a_max, 50)\nrange_b = np.linspace(b_min, b_max, 50)\n\nZ = np.zeros((len(range_a), len(range_b)))\nfor (i,a) in enumerate(range_a):\n    for (j, b) in enumerate(range_b):\n        Z[i,j] = loss(x,y,a,b)","c9c4f099":"plt.contourf(range_a, range_b, Z)\nplt.xlabel('a')\nplt.ylabel('b')\nplt.scatter(history['a'], history['b'], c='r', s=1)\nplt.colorbar();","4bfdb7e3":"import plotly.graph_objects as go\n\nsurface = go.Surface(x=range_a, y=range_b, z=Z)\nscatter = go.Scatter3d(x=history['a'], y=history['b'], z=history['loss'], mode='markers')\nfig = go.Figure(data=[surface, scatter])\n\nfig.update_layout(title='Loss Function', autosize=False, width=500, height=500)\nfig.show()","84f9d502":"$$\n\\hat{y} =  a x + b\n$$","102acddc":"$$\n\\frac{d\\ SSR}{d\\ slope}= \\sum_{i=0}^n -2(y^{(i)} - \\hat{y}^{(i)} )\\times x\n$$\n\n$$\n\\frac{d\\ SSR}{d\\ intercept}= \\sum_{i=0}^n -2(y^{(i)} - \\hat{y}^{(i)} ) \n$$\n","eb01b4f1":"# Gradien Descent","d7b91803":"### Updated Parameters","7df1d8ae":"### Gradient Descent One Epoch","9ace1a93":"## Code by hand","e710555b":"### Loss Function","0a6a4fb1":"First, let's create a dummy dataset of 1000 observations, 100 features.","0f9485f9":"### Compare with Sklearn Ordinary Linear Model","9603bc26":"### Hypothesis Function","313cd110":"$$\nupdated\\ parameter = old\\ parameter\\ value - step\\ size\n$$","8bdcd464":"### Put all functions together","9c44a3cf":"### Check Loss History","2f71ca11":"$$\nstep\\ size = gradient \\times learning\\ rate\n$$","2a76e12f":"## Visualisation","e3b5a8aa":"### Step Size","a202b057":"### Gradient","94f6e143":"$$\nMSE = \\sum_{i=0}^n (y^{(i)} - \\hat{y}^{(i)} )^2\n$$","94502ee4":"## Create linear ergression data"}}