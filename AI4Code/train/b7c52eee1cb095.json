{"cell_type":{"9686b2f1":"code","9275c579":"code","c915d5cc":"code","345a710d":"code","1706672e":"code","99565d13":"code","90f93682":"code","69c84950":"code","7baf3970":"code","c6322e2d":"code","7870faef":"code","a6c0429d":"code","3f81c35d":"code","c9860f60":"code","1bf07f5e":"code","0c709ca7":"code","54b84423":"code","5695136d":"code","93bbc6e4":"code","3761a01f":"code","1c946f98":"code","bc61f4d7":"code","163ae2be":"code","6ec28c27":"code","76300dfc":"code","225b03ab":"code","8e0230dd":"code","2030f0b1":"code","6188774e":"code","1eed0868":"code","a971cd0a":"code","0b6c0e4e":"code","86ca250b":"code","17a60ee9":"code","642642e8":"code","9d921f91":"code","4e83ab71":"code","4cc7c9be":"code","fdba5990":"code","20f72a7b":"code","dde4d474":"code","117f7bcd":"code","3def7e84":"code","0f3ce05b":"code","3e6d876b":"code","e55174e7":"code","4f1ba5a2":"code","a1cd3842":"code","5371a6e7":"code","44b6348f":"code","9967a82e":"code","68b0a628":"code","43b16d24":"code","6e57fb82":"code","f9492be9":"code","3f04d811":"code","fa025ec8":"code","ce276246":"code","6b25762f":"code","67e124cd":"code","ed502f9c":"code","251e8041":"code","79a25aa8":"code","69117871":"code","c972f5cc":"code","7abffd41":"code","4440ae76":"code","33571ad7":"code","61b60b9b":"code","d14d096b":"code","791478c7":"code","61e55544":"code","85e7e737":"code","958e92cd":"markdown","d4da18ba":"markdown","7e1d92e3":"markdown","24f65cbc":"markdown","c5db1cb2":"markdown","5c1fa71b":"markdown","3101b6ea":"markdown","c8de2da7":"markdown","2469b17d":"markdown","cec14bd8":"markdown","ee10f98a":"markdown","a0a7642d":"markdown","3892f9c6":"markdown","68f20dfa":"markdown","bc1fca05":"markdown","f68ea939":"markdown","f02ab929":"markdown","d7039722":"markdown","804baf65":"markdown","0e8a5684":"markdown","78d6e4e1":"markdown","1fb2e644":"markdown","974414f2":"markdown","3bc096da":"markdown","f186703e":"markdown","56964f05":"markdown","c1c81ad0":"markdown","4a64e6a8":"markdown","1eb21b63":"markdown","1c1640d1":"markdown","7484d416":"markdown","3250b1b7":"markdown","83754889":"markdown","69be3d19":"markdown","32d18c98":"markdown","e51572f4":"markdown","d3af890f":"markdown","7ce9fb1e":"markdown","82d0f725":"markdown","5cecb4c5":"markdown","607833ab":"markdown","d81c4993":"markdown","af0b4547":"markdown","ce8a3288":"markdown","3e1f5665":"markdown","b223e941":"markdown","258608ab":"markdown","cd0d1ea7":"markdown","4a57b89f":"markdown","724dc700":"markdown","2dbf388e":"markdown","7bb4ef87":"markdown","8ae6ef2a":"markdown","e2006399":"markdown","4a5d82fe":"markdown","8fab82e6":"markdown"},"source":{"9686b2f1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9275c579":"data = pd.read_csv(\"\/kaggle\/input\/car-price-prediction\/CarPrice_Assignment.csv\")\ndata","c915d5cc":"x = data[[\"wheelbase\",\"carwidth\",\"curbweight\",\"compressionratio\",\"horsepower\",\"highwaympg\"]].values\ny = data[[\"price\"]].values\nx","345a710d":"y","1706672e":"x.size","99565d13":"#s1=pd.DataFrame(data=x, columns=[\"wheelbase\",\"carwidth\",\"curbweight\",\"compressionratio\",\"horsepower\",\"highwaympg\"])\n#s1","90f93682":"#s2=pd.DataFrame(data=y)\n#s2","69c84950":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3,random_state=0)","7baf3970":"x_train","c6322e2d":"x_test","7870faef":"y_train","a6c0429d":"y_test","3f81c35d":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\nX_train = sc.fit_transform(x_train)\nX_test = sc.fit_transform(x_test)\n\nY_train = sc.fit_transform(y_train)\nY_test = sc.fit_transform(y_test)","c9860f60":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(x_train, y_train)\n\npred1 = lr.predict(x_test)","1bf07f5e":"y_test","0c709ca7":"pred1","54b84423":"data","5695136d":"x2=data[[\"curbweight\"]]\nx2","93bbc6e4":"y2 = data[[\"price\"]]\ny2","3761a01f":"from sklearn.model_selection import train_test_split\nx_train2, x_test2, y_train2, y_test2 = train_test_split(x2, y2, test_size=0.33,random_state=0)","1c946f98":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(x_train2,y_train2)\n\npred2 = lr.predict(x_test2)","bc61f4d7":"y_test2","163ae2be":"pred2","6ec28c27":"from sklearn.metrics import r2_score\nprint(\"Simple linear regression R2 score:\", r2_score(y2,lr.predict(x2)))","76300dfc":"x_train2=x_train2.sort_values(by=\"curbweight\")\ny_train2=y_train2.sort_index()\n\nplt.plot(x_train2,y_train2,)\nplt.plot(x_test2,pred2)\n\nplt.title(\"car prices\")\nplt.xlabel(\"curbweight\")\nplt.ylabel(\"price\")","225b03ab":"y_train2","8e0230dd":"data","2030f0b1":"carbody=data[[\"carbody\"]].values\ncarbody","6188774e":"data3=data[[\"horsepower\"]].values\ndata3","1eed0868":"price3=data[[\"price\"]].values\nprice3","a971cd0a":"\"\"\"door=data[[\"doornumber\"]].values\ndoor\"\"\"","0b6c0e4e":"#OneHotEncoder\nfrom sklearn import preprocessing\n\nohe=preprocessing.OneHotEncoder()\ncarbody=ohe.fit_transform(carbody).toarray()\nprint(carbody)","86ca250b":"\"\"\"ohe=preprocessing.OneHotEncoder()\ndoor=ohe.fit_transform(door).toarray()\nprint(door)\"\"\"","17a60ee9":"cb1=pd.DataFrame(data=carbody,index=range(205),columns=[\"convertible\",\"hardtop\",\"hatchback\",\"sedan\",\"wagon\"])\ncb1","642642e8":"cb2=pd.DataFrame(data=data3,index=range(205),columns=[\"horsepower\"])\ncb2","9d921f91":"cb3=pd.DataFrame(data=price3,index=range(205),columns=[\"price3\"])\ncb3","4e83ab71":"concat1=pd.concat([cb1,cb2],axis=1)\nconcat1","4cc7c9be":"from sklearn.model_selection import train_test_split\nx_train3,x_test3,y_train3,y_test3 = train_test_split(concat1,cb3,test_size=0.33, random_state=0)","fdba5990":"from sklearn.linear_model import LinearRegression\nregressor=LinearRegression()\nregressor.fit(x_train3,y_train3)\npred3=regressor.predict(x_test3)","20f72a7b":"pred3","dde4d474":"y_test3","117f7bcd":"data_b=pd.concat([concat1,cb3],axis=1)\ndata_b","3def7e84":"sym=data[[\"symboling\"]].values\nsym","0f3ce05b":"x_train4,x_test4,y_train4,y_test4 = train_test_split(data_b,sym,test_size=0.33,random_state=0)","3e6d876b":"r4 = LinearRegression()\nr4.fit(x_train4,y_train4)\n\npred4 = r4.predict(x_test4)","e55174e7":"pred4","4f1ba5a2":"y_test4","a1cd3842":"data_b.info()","5371a6e7":"import statsmodels.api as sm\n\nx4=np.append(arr=np.ones((205,1)).astype(int),values=data_b,axis=1) #column x0\n\nX4_l = data_b.iloc[:,[0,1,2,3,4,5,6]].values #we get all columns\nX4_l= np.array(X4_l,dtype=float)\nmodel = sm.OLS(sym,X4_l).fit() #generate a report with OLS\nprint(model.summary())","44b6348f":"#new OLS model\nX4_l = data_b.iloc[:,[0,1,2,3,5,6]].values #we get all columns\nX4_l= np.array(X4_l,dtype=float)\nmodel = sm.OLS(sym,X4_l).fit() #generate a report with OLS\nprint(model.summary())","9967a82e":"x=data[[\"wheelbase\"]].values\ny=data[[\"price\"]].values","68b0a628":"linreg1 = LinearRegression()\nlinreg1.fit(x,y)","43b16d24":"from sklearn.preprocessing import PolynomialFeatures\npoly_reg2=PolynomialFeatures(degree=2)\nx_poly2=poly_reg2.fit_transform(x)\nlinreg2=LinearRegression()\nlinreg2.fit(x_poly2,y)","6e57fb82":"x_poly2","f9492be9":"#4th order\npoly_reg4=PolynomialFeatures(degree=4)\nx_poly4=poly_reg4.fit_transform(x)\nlinreg4=LinearRegression()\nlinreg4.fit(x_poly4,y)","3f04d811":"x_poly4","fa025ec8":"plt.scatter(x,y,color='red')\nplt.plot(x,linreg1.predict(x), color = 'blue')\nplt.show()\n\nplt.scatter(x,y,color = 'red')\nplt.plot(x,linreg2.predict(poly_reg2.fit_transform(x)), color = 'blue')\nplt.show()\n\nplt.scatter(x,y,color = 'red')\nplt.plot(x,linreg4.predict(poly_reg4.fit_transform(x)), color = 'blue')\nplt.show()","ce276246":"print(\"lin reg 1st order(linear): \",linreg1.predict([[6.6]]))\nprint(\"lin reg 1st order(linear): \",linreg1.predict([[110]]))\n\nprint(\"lin reg 2nd order(linear): \",linreg2.predict(poly_reg2.fit_transform([[6.6]])))\nprint(\"lin reg 2nd order(linear): \",linreg2.predict(poly_reg2.fit_transform([[110]])))\n\nprint(\"lin reg 4th order(linear): \",linreg4.predict(poly_reg4.fit_transform([[6.6]])))\nprint(\"lin reg 4th order(linear): \",linreg4.predict(poly_reg4.fit_transform([[110]])))","6b25762f":"from sklearn.metrics import r2_score\nprint(\"Polynomial regression R2 score:\", r2_score(y,linreg2.predict(x_poly2)))","67e124cd":"from sklearn.preprocessing import StandardScaler\nsc1=StandardScaler()\nx_scaled = sc1.fit_transform(x)\nsc2=StandardScaler()\ny_scaled = np.ravel(sc2.fit_transform(y.reshape(-1,1)))","ed502f9c":"from sklearn.svm import SVR\n\nsvr_reg = SVR(kernel='rbf')\nsvr_reg.fit(x_scaled,y_scaled)\n\nplt.scatter(x_scaled,y_scaled,color='red')\nplt.plot(x_scaled,svr_reg.predict(x_scaled),color='blue')\nplt.show()","251e8041":"svr_reg2 = SVR(kernel='poly')\nsvr_reg2.fit(x_scaled,y_scaled)\n\nplt.scatter(x_scaled,y_scaled,color='red')\nplt.plot(x_scaled,svr_reg2.predict(x_scaled),color='blue')\nplt.show()","79a25aa8":"print(\"SVR rbf pred:\", svr_reg.predict([[110]]))\nprint(\"SVR rbf pred:\", svr_reg.predict([[6.6]]))","69117871":"print(\"SVR poly pred:\", svr_reg2.predict([[110]]))\nprint(\"SVR poly pred:\", svr_reg2.predict([[6.6]]))","c972f5cc":"print(\"SVR rbf  R2 score:\" ,r2_score(y_scaled,svr_reg.predict(x_scaled)))\nprint(\"SVR poly R2 score:\",r2_score(y_scaled,svr_reg2.predict(x_scaled)))","7abffd41":"from sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor(random_state=0)\ndtr.fit(x,y)\nZ = x + 0.5\nK = y - 0.4\n\nplt.scatter(x,y, color='red')\nplt.plot(x,dtr.predict(x), color='blue')\nplt.plot(x,dtr.predict(Z),color='green')\nplt.plot(x,dtr.predict(K),color='yellow')\nplt.show()","4440ae76":"print(dtr.predict([[110]]))\nprint(dtr.predict([[11]]))\nprint(dtr.predict([[6.6]]))","33571ad7":"print(\"Decision tree regression R2 score:\", r2_score(y,dtr.predict(x)))","61b60b9b":"from sklearn.ensemble import RandomForestRegressor\nrfr=RandomForestRegressor(n_estimators = 10,random_state=0) #We give how many decision trees to draw with #n_estimators\nrfr.fit(x,y.ravel()) #ravel combines individual data. Because we separate the random forest, we reunite it.","d14d096b":"print(rfr.predict([[6.6]]))\nprint(rfr.predict([[11]]))\nprint(rfr.predict([[110]]))","791478c7":"plt.scatter(x,y,color='red')\nplt.plot(x,rfr.predict(x),color='blue')\n\nplt.plot(x,rfr.predict(Z),color='green')\nplt.plot(x,rfr.predict(K),color='yellow')\nplt.show()","61e55544":"print(\"Random forest R2 score\", r2_score(y,rfr.predict(x)))\nprint(\"Random forest R2 score\", r2_score(y,rfr.predict(K)))\nprint(\"Random forest R2 score\", r2_score(y,rfr.predict(Z)))","85e7e737":"print('-----------------------')\nprint('Linear R2 degeri')\nprint(\"Simple linear regression R2 score:\", r2_score(y2,lr.predict(x2)))\n\nprint('\\nPolynomial R2 degeri')\nprint(\"Polynomial regression R2 score:\", r2_score(y,linreg2.predict(x_poly2)))\n\nprint('\\nSVR R2 degeri')\nprint(\"SVR poly R2 score:\",r2_score(y_scaled,svr_reg2.predict(x_scaled)))\n\nprint('\\nDecision Tree R2 degeri')\nprint(\"Decision tree regression R2 score:\", r2_score(y,dtr.predict(x)))\n\nprint('\\nRandom Forest R2 degeri')\nprint(\"Random forest R2 score\", r2_score(y,rfr.predict(x)))\n\nprint(\"\\n\")","958e92cd":"<a id=\"16\"><\/a><br>\n## 6.1. Uploading Data","d4da18ba":"<a id=\"37\"><\/a><br>\n## 8.1. Scaling","7e1d92e3":"<a id=\"41\"><\/a><br>\n# 9. Decision Tree Regression","24f65cbc":"> **rbf** is more convinient than **poly** *(0.42>0.12)*","c5db1cb2":"<a id=\"17\"><\/a><br>\n## 6.2. Dependent And Independent Variables","5c1fa71b":"<a id=\"42\"><\/a><br>\n## 9.1. Decision Tree Regression","3101b6ea":"<a id=\"14\"><\/a><br>\n## 5.5. Visualization For Simple Linear Regression","c8de2da7":"<a id=\"11\"><\/a><br>\n## 5.2. Diving Data For Training And Test","2469b17d":"<a id=\"38\"><\/a><br>\n## 8.2. Support Vector Regression","cec14bd8":"> For more: [https:\/\/www.datarobot.com\/blog\/ordinary-least-squares-in-python\/](https:\/\/www.datarobot.com\/blog\/ordinary-least-squares-in-python\/)","ee10f98a":"<a id=\"7\"><\/a><br>\n## 4.3. Scaling","a0a7642d":"<a id=\"18\"><\/a><br>\n## 6.3. One Hot Encoder","3892f9c6":"<a id=\"40\"><\/a><br>\n## 8.4. $R^2$ Score For SVR","68f20dfa":"<a id=\"45\"><\/a><br>\n# 10. Random Forest Regression","bc1fca05":"<a id=\"1\"><\/a><br>\n# 1. Introduction\nIn statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable and one or more independent variables.\n# $$Y_i = f(X_i,\u03b2) + e_i$$","f68ea939":"<a id=\"15\"><\/a><br>\n# 6. Backward Elimination For Simple Linear Regression","f02ab929":"<a id=\"19\"><\/a><br>\n## 6.4. Transforming Numpy Arrays To Dataframe","d7039722":"<a id=\"47\"><\/a><br>\n## 10.2. Prediction Of Random Forest Regression","804baf65":"<a id=\"36\"><\/a><br>\n# 8. SVR (Support Vector Regression)","0e8a5684":"<a id=\"3\"><\/a><br>\n# 3. Uploading Data","78d6e4e1":"<a id=\"4\"><\/a><br>\n# 4. Multivariate Linear Regression","1fb2e644":"<a id=\"8\"><\/a><br>\n## 4.4. Multivariate Linear Regression and Prediction","974414f2":"<a id=\"49\"><\/a><br>\n## 10.4. $R^2$  Score Of Random Forest Regression","3bc096da":"<a id=\"5\"><\/a><br>\n## 4.1. Dependent And Independent Variables","f186703e":"<a id=\"24\"><\/a><br>\n## 6.9. Information About Data","56964f05":"<a id=\"21\"><\/a><br>\n## 6.6. Diving Data For Training And Test","c1c81ad0":"<a id=\"26\"><\/a><br>\n## 6.11. Creating A New OLS Model According To First OLS Model","4a64e6a8":"<a id=\"39\"><\/a><br>\n## 8.3. Prediction Of SVR","1eb21b63":"<a id=\"48\"><\/a><br>\n## 10.3. Visualization For Random Forest Regression","1c1640d1":"<a id=\"12\"><\/a><br>\n## 5.3. Multivariate Linear Regression and Prediction","7484d416":"<a id=\"22\"><\/a><br>\n## 6.7. Simple Linear Regression And Prediction","3250b1b7":"<a id=\"35\"><\/a><br>\n## 7.6. $R^2$ Score For Polynomial Regression\n","83754889":"<a id=\"23\"><\/a><br>\n## 6.8. Backward Elimination For Simple Linear Regression","69be3d19":"<a id=\"10\"><\/a><br>\n## 5.1. Dependent and Independent Variables","32d18c98":"<a id=\"46\"><\/a><br>\n## 10.1. Random Forest Regression","e51572f4":"<a id=\"50\"><\/a><br>\n# 11. $R^2$ Scores Abstract","d3af890f":"<a id=\"6\"><\/a><br>\n## 4.2. Diving Data For Training And Test","7ce9fb1e":"<a id=\"25\"><\/a><br>\n## 6.10. OLS Regression Results","82d0f725":"<a id=\"32\"><\/a><br>\n### 7.3.2. 4th Order","5cecb4c5":"<a id=\"2\"><\/a><br>\n# 2. Libraries","607833ab":"<a id=\"13\"><\/a><br>\n## 5.4. $R^2$ Score For Simple Linear Regression","d81c4993":"<a id=\"29\"><\/a><br>\n## 7.2. Linear Model (1st Order)","af0b4547":"# CONTENTS\n\n<font color=\"red\">\n\n    \n1. [Introduction](#1)\n2. [Libraries](#2)\n3. [Uploading Data](#3)\n4. [Multivariate Linear Regression](#4)\n    * 4.1. [Dependent And Independent Variables](#5)\n    * 4.2. [Diving Data For Training And Test](#6)\n    * 4.3. [Scaling](#7)\n    * 4.4. [Multivariate Linear Regression and Prediction](#8)\n5. [Simple Linear Regression](#9)\n    * 5.1. [Dependent and Independent Variables](#10)\n    * 5.2. [Diving Data For Training And Test](#11)\n    * 5.3. [Multivariate Linear Regression and Prediction](#12)\n    * 5.4. [$\ud835\udc45^2$  Score For Simple Linear Regression](#13)\n    * 5.5. [Visualization For Simple Linear Regression](#14)\n6. [Backward Elimination For Simple Linear Regression](#15)\n    * 6.1. [Uploading Data](#16)\n    * 6.2. [Dependent and Independent Variables](#17)\n    * 6.3. [One Hot Encoder](#18)\n    * 6.4. [Transforming Numpy Arrays To Dataframe](#19)\n    * 6.5. [Concatinating Dataframes](#20)\n    * 6.6. [Diving Data For Training And Test](#21)\n    * 6.7. [Simple Linear Regression And Prediction](#22)\n    * 6.8. [Backward Elimination For Simple Linear Regression](#23)\n    * 6.9. [Information About Data](#24)\n    * 6.10. [OLS Regression Results](#25)\n    * 6.11. [Creating A New OLS Model According To First OLS Model](#26)\n7. [Polynomial Regression](#27)\n    * 7.1. [Dependent And Independent Variables](#28)\n    * 7.2. [Linear Model(1st Order)](#29)\n    * 7.3. [Nonlinear Model](#30)\n        * 7.3.1. [2nd Order](#31)\n        * 7.3.2. [4th Order](#32)\n    * 7.4. [Visualization Of Polynomial Regression](#33)\n    * 7.5. [Predictions](#34)\n    * 7.6. [$\ud835\udc45^2$ Score For Polynomial Regression](#35)\n8. [SVR (Support Vector Regression](#36)\n    * 8.1. [Scaling](#37)\n    * 8.2. [Support Vector Regression](#38)\n    * 8.3. [Prediction Of SVR](#39)\n    * 8.4. [$\ud835\udc45^2$ Score For SVR](#40)\n9. [Decision Tree Regression](#41)\n    * 9.1. [Decision Tree Regression](#42)\n    * 9.2. [Prediction Of Decision Tree Regression](#43)\n    * 9.3. [$\ud835\udc45^2$  Score Of Decision Tree Regression](#44)\n10. [Random Forest Regression](#45)\n    * 10.1. [Random Forest Regression](#46)\n    * 10.2. [Prediction Of Decision Tree Regression](#47)\n    * 10.3. [Visualization For Random Forest Regression](#48)\n    * 10.4. [$\ud835\udc45^2$  Score Of Random Forest Regression](#49)\n11. [$\ud835\udc45^2$  Scores Abstract](#50)","ce8a3288":"<a id=\"44\"><\/a><br>\n## 9.3. $R^2$ Score Of Decision Tree Regression","3e1f5665":"<a id=\"34\"><\/a><br>\n## 7.5. Predictions","b223e941":"> OLS = Ordinary Least Squares","258608ab":"* $Y_i$ = dependent variable\n* f = function\n* $X_i$ = independent variable\n* \u03b2 = unknown parameters\n* $e_i$ = error terms","cd0d1ea7":"<a id=\"20\"><\/a><br>\n## 6.5. Concatinating Dataframes","4a57b89f":"<a id=\"33\"><\/a><br>\n## 7.4. Visualization Of Polynomial Regression","724dc700":"<a id=\"43\"><\/a><br>\n## 9.2. Prediction Of Decision Tree Regression","2dbf388e":"<a id=\"27\"><\/a><br>\n# 7. Polynomial Regression  ","7bb4ef87":"<a id=\"28\"><\/a><br>\n## 7.1. Dependent And Independent Variables","8ae6ef2a":"<a id=\"31\"><\/a><br>\n### 7.3.1. 2nd Order","e2006399":"<a id=\"30\"><\/a><br>\n## 7.3. Nonlinear Model","4a5d82fe":"> For $x_5$, P>|t| value is large. So, we can delete that column.","8fab82e6":"<a id=\"9\"><\/a><br>\n# 5. Simple Linear Regression"}}