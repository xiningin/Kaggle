{"cell_type":{"3a2371c1":"code","56abc830":"code","d1250fa1":"code","7ec90054":"code","e3e36890":"code","7d88bb17":"code","e43f7b1b":"code","55e7dfc1":"code","95cf52d8":"code","7c38ef1d":"code","c6637288":"code","a130e39c":"code","64315f7a":"code","4f33d2b3":"code","85a8bad4":"code","670768d6":"code","ddc721f3":"code","240d431c":"code","61bfed73":"code","410ec6b2":"code","ac14d4af":"code","38eca142":"code","517e8081":"code","39c818da":"code","7d8a5d1e":"code","4f2ef15e":"code","d4916cb4":"code","eaa84cf1":"code","f5333430":"code","d1e323ea":"code","99c6b2f6":"code","86490b9d":"code","b5b3f2ae":"code","3c84ef58":"code","eb4483e8":"code","dd0c3d33":"code","2388d43b":"markdown","cd403881":"markdown","45be4c4e":"markdown","70de515e":"markdown","37146379":"markdown","ec81255a":"markdown","b54a6acb":"markdown","4a3ec21d":"markdown","6033b388":"markdown","1e2d4cb7":"markdown","75bd900d":"markdown","a73dd3f7":"markdown","5d0106d9":"markdown","67886e79":"markdown","02e6c552":"markdown","012fba67":"markdown","5dfb7997":"markdown","24eda392":"markdown","d3fe941a":"markdown","bab20856":"markdown","ba402004":"markdown","0135553d":"markdown","0a1ec326":"markdown","ea26ae15":"markdown","df93972d":"markdown","9358f534":"markdown","d8013acf":"markdown","cbba3687":"markdown","e5db02b3":"markdown","6a743619":"markdown","8ffeed5b":"markdown","ee542efe":"markdown","f7c0961c":"markdown","3c04df34":"markdown","626cc239":"markdown","8c9d6374":"markdown","4fbd1964":"markdown","87d8ab22":"markdown","7562baeb":"markdown"},"source":{"3a2371c1":"import pdb\nimport pickle\nimport joblib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport warnings\n\nfrom scipy import stats\nfrom scipy.special import boxcox1p\nfrom scipy.stats import norm, skew\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\nfrom sklearn.svm import SVR\nfrom sklearn_pandas import DataFrameMapper\nfrom operator import itemgetter\nimport lightgbm as lgb\n\n\nimport os\n\n%matplotlib inline\nwarnings.filterwarnings('ignore')\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))\n\n# Read the CSV\ntrain_csv_path = '..\/input\/house-prices-advanced-regression-techniques\/train.csv'\ntest_csv_path = '..\/input\/house-prices-advanced-regression-techniques\/test.csv'\ntrain_set = pd.read_csv(train_csv_path)\ntest_set = pd.read_csv(test_csv_path)\n\n# Keep original data clean\ntrain_data = train_set.copy()\ntest_data = test_set.copy()\ntrain_ids = train_data['Id'].copy()\ntest_ids = test_data['Id'].copy()\nprint('Test data original columns: {}'.format(train_data.columns.to_list()))","56abc830":"print('Train data original shape: {}'.format(train_data.shape))\nprint('Test data original shape: {}'.format(test_data.shape))","d1250fa1":"train_data.head(5)","7ec90054":"train_data['SalePrice'].describe()","e3e36890":"plt.figure(figsize=(16, 6))\nsns.distplot(train_data['SalePrice']);","7d88bb17":"fig, ax = plt.subplots(figsize=(23,10))\nax.set(yscale=\"log\")\nsns.barplot(x=\"Neighborhood\", y=\"SalePrice\", data=train_data, estimator=np.mean)\nplt.show()","e43f7b1b":"var = 'GrLivArea'\ndata = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', figsize=(16, 6), ylim=(0,800000));","55e7dfc1":"var = 'OverallQual'\ndata = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 10))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","95cf52d8":"var = 'YearBuilt'\ndata = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\nf, ax = plt.subplots(figsize=(22, 12))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","7c38ef1d":"corrs_matrix = train_data.corr()\nf, ax = plt.subplots(figsize=(12, 12))\nsns.heatmap(corrs_matrix, vmax=.8, square=True);","c6637288":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train_data[cols], size = 2.5)\nplt.show();","a130e39c":"total = train_data.isnull().sum().sort_values(ascending=False)\npercent = (train_data.isnull().sum()\/train_data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","64315f7a":"saleprice_scaled = StandardScaler().fit_transform(train_data['SalePrice'][:,np.newaxis]);\nlow_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\nhigh_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\nprint('Low range of distribution:')\nprint(low_range)\nprint('\\nHigh range of the distribution:')\nprint(high_range)","4f33d2b3":"var = 'GrLivArea'\ndata = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000), figsize=(16, 8));","85a8bad4":"train_data.sort_values(by = 'GrLivArea', ascending = False)[:2]\ntrain_data = train_data.drop(train_data[train_data['Id'] == 1299].index)\ntrain_data = train_data.drop(train_data[train_data['Id'] == 524].index)","670768d6":"sns.distplot(train_data['SalePrice'], fit=stats.norm);\nfig = plt.figure()\nres = stats.probplot(train_data['SalePrice'], plot=plt)","ddc721f3":"train_data['SalePrice'] = np.log(train_data['SalePrice'])\nsns.distplot(train_data['SalePrice'], fit=stats.norm);\nfig = plt.figure()\nres = stats.probplot(train_data['SalePrice'], plot=plt)","240d431c":"sns.distplot(train_data['GrLivArea'], fit=stats.norm);\nfig = plt.figure()\nres = stats.probplot(train_data['GrLivArea'], plot=plt)","61bfed73":"train_data['GrLivArea'] = np.log(train_data['GrLivArea'])\nsns.distplot(train_data['GrLivArea'], fit=stats.norm);\nfig = plt.figure()\nres = stats.probplot(train_data['GrLivArea'], plot=plt)","410ec6b2":"sns.distplot(train_data['TotalBsmtSF'], fit=stats.norm);\nfig = plt.figure()\nres = stats.probplot(train_data['TotalBsmtSF'], plot=plt)","ac14d4af":"# Revert to clean training data\ntrain_data = train_set.copy()\ntrain_index = train_data.shape[0]\ntest_index = test_data.shape[0]\ntarget = train_data.SalePrice.values\nall_data = pd.concat((train_data, test_data)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nall_data.shape","38eca142":"all_data['PoolQC'] = all_data['PoolQC'].fillna('None')\nall_data['MiscFeature'] = all_data['MiscFeature'].fillna('None')\nall_data['Alley'] = all_data['Alley'].fillna('None')\nall_data['Fence'] = all_data['Fence'].fillna('None')\nall_data['FireplaceQu'] = all_data['FireplaceQu'].fillna('FireplaceQu')\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\nall_data['MasVnrType'] = all_data['MasVnrType'].fillna('None')\nall_data['MasVnrArea'] = all_data['MasVnrArea'].fillna(0)\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\n\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\n\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\n\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')","517e8081":"all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","39c818da":"columns = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\nfor col in columns:\n    encoder = LabelEncoder() \n    encoder.fit(list(all_data[col].values)) \n    all_data[col] = encoder.transform(list(all_data[col].values))\n    \nall_data.shape","7d8a5d1e":"sale_year = np.max(all_data['YrSold'])\nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\nall_data.shape","4f2ef15e":"num_features = all_data.dtypes[all_data.dtypes != 'object'].index\nskewed_features = all_data[num_features].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_features})\nskewness.head(10)","d4916cb4":"skewness = skewness[abs(skewness) > 0.75]\nprint('{} skewed features to transform'.format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    all_data[feat] = boxcox1p(all_data[feat], lam)\nall_data = pd.get_dummies(all_data)\nall_data.shape","eaa84cf1":"train_data = all_data[:train_index]\ntest_data = all_data[train_index:]","f5333430":"X_train, X_val, y_train, y_val = train_test_split(\n    train_data, target, test_size=0.20, random_state=42)","d1e323ea":"model = lgb.LGBMRegressor(objective='regression',\n                         num_leaves=5,\n                         learning_rate=0.05,\n                         n_estimators=720,\n                         max_bin = 55,\n                         bagging_fraction = 0.8,\n                         bagging_freq = 5,\n                         feature_fraction = 0.2319,\n                         feature_fraction_seed=9,\n                         bagging_seed=9,\n                         min_data_in_leaf =6,\n                         min_sum_hessian_in_leaf = 11)","99c6b2f6":"n_folds = 5\n\ny_train_scaled = np.log1p(y_train)\ny_val_scaled = np.log1p(y_val)\n\ndef rmse_cv(model, X, y):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X.values)\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=5))\n    return rmse","86490b9d":"X = X_train\ny = y_train_scaled\n\nscore = rmse_cv(model, X, y)\nprint(\"Model score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","b5b3f2ae":"def rmse_cv_val(model, X, y):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=5))\n    return rmse","3c84ef58":"X = X_val\ny = y_val_scaled\n\n\nscore = rmse_cv_val(model, X, y)\nprint(\"Validation score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","eb4483e8":"X = X_train\ny = y_train_scaled\nmodel.fit(X, y)","dd0c3d33":"X = test_data\npredictions = np.exp(model.predict(X))\nresult=pd.DataFrame({'Id':test_ids, 'SalePrice':predictions})\nresult.to_csv('\/kaggle\/working\/submission.csv',index=False)\nprint('done')","2388d43b":"#### Apply Log Transform","cd403881":"# Housing Prices - LGBM","45be4c4e":"### Missing Values","70de515e":"#### Sale Price and Overall Quality","37146379":"Conclusions:\n* 'GrLivArea' and 'TotalBsmtSF' linearly related with 'SalePrice' \n* Both relationships are positive\n* 'OverallQual' and 'YearBuilt' also related to 'SalePrice'","ec81255a":"### Final Fit","b54a6acb":"### Scatterplot with Correlated Variables","4a3ec21d":"#### Sales","6033b388":"Housing Prices Advanced Regression - submission version\n* Data Exploration\n* Feature Engineering\n* Clean Code\n* LGBM","1e2d4cb7":"### Skew","75bd900d":"#### Living Area","a73dd3f7":"### Missing Data","5d0106d9":"### Create Output","67886e79":"### Bivariate Analysis","02e6c552":"#### Sale Price and Living Area","012fba67":"#### Sale Price and Year Built","5dfb7997":"* Linear relationship with living area","24eda392":"### Train Scores","d3fe941a":"The two living are points around 4600 and 5600 don't seem to be obeying the rules. I'm going to delete them","bab20856":"### Scale Sales Data","ba402004":"### Correlations","0135553d":"### Sale Price","0a1ec326":"* Positive skewed distribution\n* Peakedness","ea26ae15":"### Validation Function","df93972d":"## Prepare Data for Training","9358f534":"#### Apply Log Transformation","d8013acf":"### Mean Sale Price by Neighborhood","cbba3687":"### Numeric Transformations","e5db02b3":"### Label Encoding","6a743619":"### Fixing Distributions","8ffeed5b":"## Explore the Data","ee542efe":"### New Features","f7c0961c":"### Model","3c04df34":"## Train The Models","626cc239":"[Data analysis worksheet](https:\/\/docs.google.com\/spreadsheets\/d\/1AGyWYMi1CrMCk2jZ8NrVu7g9kjB_ovynVSNddyK904E\/edit?usp=sharing)\n\nCredit to [Pedro Marcelino's](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) notebook on data exploration.","8c9d6374":"### Total Basement Square Footage","4fbd1964":"## Create the Submission","87d8ab22":"## Validate Model","7562baeb":"### Train - Validation Split"}}