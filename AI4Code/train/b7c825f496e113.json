{"cell_type":{"22263c3c":"code","6c69807f":"code","005b3673":"code","e544d3cc":"code","2fd7db91":"code","6256e3c0":"code","de015e87":"code","3d5eaf73":"code","380f4b3f":"code","72a60d2f":"code","8a958541":"code","ff914503":"code","ba04c255":"code","ac616d55":"code","c0d14bc4":"code","3872d7f8":"markdown","375aecd0":"markdown","c472ddc7":"markdown","953c0365":"markdown","f18f454c":"markdown","f9f19553":"markdown","6b9afb7a":"markdown","31ad622e":"markdown","c002a67f":"markdown","d2987662":"markdown","94fd0651":"markdown","628b4dee":"markdown","e5f147f9":"markdown","ab489c28":"markdown","be9437ab":"markdown","87e26449":"markdown","30ead42e":"markdown"},"source":{"22263c3c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6c69807f":"data = pd.read_csv(\"\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\")","005b3673":"data.head()","e544d3cc":"data.tail()","2fd7db91":"data.info()","6256e3c0":"x_data = data.drop(\"DEATH_EVENT\",axis=1) # It is our x axis data (not normalized)\nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))\nx.head()","de015e87":"# We had created our x array.\ny = data[\"DEATH_EVENT\"].values\n\n# There is a good split function in sklearn library\nfrom sklearn.model_selection import train_test_split\n\nx_train1,x_test1,y_train1,y_test1 = train_test_split(x,y,test_size=0.2,random_state=42)\n\nprint(f\"\"\"\\tX Train's shape is: {x_train1.shape}\n        X Test's shape is {x_test1.shape}\n        Y Train's shape is {y_train1.shape}\n        Y Test's shape is {y_test1.shape}\"\"\")\n","3d5eaf73":"\nx_train1 = x_train1.T\nx_test1 = x_test1.T\ny_train1 = y_train1.T\ny_test1 = y_test1.T\n\nprint(f\"\"\"\\tX Train's shape is: {x_train1.shape}\n        X Test's shape is {x_test1.shape}\n        Y Train's shape is {y_train1.shape}\n        Y Test's shape is {y_test1.shape}\"\"\")\n\n","380f4b3f":"def sigmoid(z):\n    \n    y_head = 1 \/ (1 + np.exp(-z))\n    return y_head\n   \n    ","72a60d2f":"def initializing(dimension):\n    \n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b","8a958541":"\ndef forward_backward_propagation(w, b, x_train, y_train):\n    # forward propagation:\n    z = np.dot(w.T, x_train) + b\n    y_head = sigmoid(z)\n    loss = -(1 - y_train) * np.log(1 - y_head) - y_train * np.log(y_head)     # loss function formula\n    cost = (np.sum(loss)) \/ x_train.shape[1]                               # cost function formula\n    \n    # backward propagation:\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    \n    gradients = {'derivative_weight': derivative_weight, 'derivative_bias': derivative_bias}\n    \n    return cost, gradients","ff914503":"def update(w, b, x_train, y_train, learning_rate, nu_of_iteration):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    for i in range(nu_of_iteration):\n        cost, gradients = forward_backward_propagation(w, b, x_train, y_train)\n        cost_list.append(cost)\n    \n        w = w - learning_rate * gradients['derivative_weight']\n        b = b - learning_rate * gradients['derivative_bias']\n        if i % 20 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print('Cost after iteration %i: %f' %(i,cost))\n    \n    parameters = {'weight': w, 'bias':b}\n    \n    # Visulizatio\n    plt.plot(index, cost_list2)\n    plt.xlabel('Nu of Iteration')\n    plt.ylabel('Cost Function Value')\n    plt.show()\n    \n    return parameters, gradients, cost_list","ba04c255":"def predict(w,b,x_test):\n    \n    z = np.dot(w.T,x_test)+b\n    y_head = sigmoid(z)\n    prediction = np.zeros((1,x_test.shape[1]))\n    \n    for i in range(z.shape[1]):\n        \n        if z[0,i] <= 0.5:\n            prediction[0,i] = 0\n        else:\n            prediction[0,i] = 1\n    \n    return prediction","ac616d55":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    \n    dimension =  x_train.shape[0]  \n    w,b = initializing(dimension)\n    \n    \n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \n","c0d14bc4":"logistic_regression(x_train1, y_train1, x_test1, y_test1,learning_rate = 6, num_iterations = 50)\n","3872d7f8":"## Updating Function\n\nIn last function we have given our gradients. And now we are ready to update weight and bias, for this I am going to define a update function","375aecd0":"# Taking a Look at The Data","c472ddc7":"# Defining Functions\n\nIn this section I am going to define functions that I will need in logistic regression\n\n## Sigmoid Function\n\nYou know, in logistic regression we need a y_head value between 0 and 1, and we use sigmoid function to do this process. We can do this with a library but in this kernel I want to do this with my codes.\n\nBefore the coding, Let's taking a look at sigmoid function's mathametical expression\n\n![An-illustration-of-the-signal-processing-in-a-sigmoid-function.png](attachment:An-illustration-of-the-signal-processing-in-a-sigmoid-function.png)\n\nWe are ready to convert sigmoid function to code.","953c0365":"# Introduction\n\nHello,welcome to my kernel! In this kernel I am going to predict death using heart failure data. In order to prediction I am going to use my logistic regression functions. Let's take a look at the schedule\n\n## Schedule\n1. Importing Libraries and The Data\n1. Taking a Look at The Data\n1. Preparing Dataset\n    * Normalization\n    * Train Test Split \n1. Defining Functions\n    * Sigmoid Function\n    * Initializing Function\n    * Forward-Backward Function\n    * Updating Function\n1. Result\n1. Conclusion\n    ","f18f454c":"## Normalization\n\nYou know, we have to normalize the data because if we do not normalize our data, there will be problems in our model. Let's normalize the data.","f9f19553":"# Result\n\nOur functions are ready. We can do logical regression now, but before this we need one more function. Let's write him.","6b9afb7a":"## Forward - Backward Propagation Function\n\nThere are two sections in Logistic Regression. Forward and Backward propagation. In this section I am going to define a function about them\n","31ad622e":"# Preparing Dataset\n","c002a67f":"This function helps us to do logistic regression","d2987662":"I've imported the libraries. Now I'll add the data.","94fd0651":"We created our array but we need to do one more process to our arrays. We need to reverse our arrays by the shape. \n","628b4dee":"Our splitting process has finished.\n","e5f147f9":"# Conclusion\n\nMy english is not enough for describing everything, so my explanations are not enough I know. However if you want to learn more details, you can examine this kernel. \n\nhttps:\/\/www.kaggle.com\/kanncaa1\/deep-learning-tutorial-for-beginners\n\nI am waiting for your advices, comments and upvotes.\n\n## Special Thanks\n\nI examined your kernel, thanks!\n\nhttps:\/\/www.kaggle.com\/akdagmelih\/rain-prediction-logistic-regression-example\n\n","ab489c28":"## Initializing Function\n\nWe need weight values and bias value in logistic regression training. But we do not have any values. For taking initial weight values and bias I am going to define a function","be9437ab":"## Train - Test Split \n\nIn this section I am going to split x and y into two pieces. Train and Test. With train array, we will train our data and with test array we will test our model. Let's do this!","87e26449":"## Prediction Function\n\nWe are ready to create our model, but we can not predict anything because we do not have a prediction function. Therefore I am going to define a prediction function.","30ead42e":"# Importing Libraries and The Data \n\nIn this section I'll add the libraries and the data that I will use. "}}