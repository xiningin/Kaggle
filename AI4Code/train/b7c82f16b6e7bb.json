{"cell_type":{"418f8a4c":"code","38154d7d":"code","72104432":"code","0dd92414":"code","0e8e980a":"code","f3f0182c":"code","7e44a1f4":"code","c95fee50":"code","4d590163":"code","24defd0e":"code","cb54b948":"code","fc484adf":"code","127120c9":"code","1967c36b":"code","d44ff560":"code","3e8ac8d2":"code","bc6e4d55":"code","7364b51f":"code","c899d283":"code","5f4ceadb":"code","8b2652cb":"code","7c9018b4":"code","6720caaa":"code","47333eba":"code","61898e3d":"code","ee754915":"code","612dd120":"markdown","c7281341":"markdown","864ff82a":"markdown","73f4c181":"markdown","5753484f":"markdown","fb2acac2":"markdown","96f64803":"markdown","3bbddb30":"markdown","d4da792e":"markdown","11c8d126":"markdown","f6c79200":"markdown","de5e8eab":"markdown","26378620":"markdown","e346f104":"markdown","1e0cbf6b":"markdown","50cc09fc":"markdown","d72665ea":"markdown","95878ee6":"markdown","79f7b296":"markdown","ffbe0ba1":"markdown","6c0bc39d":"markdown","3364d009":"markdown","10f8464a":"markdown","7bd5a82c":"markdown"},"source":{"418f8a4c":"# import necessary libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","38154d7d":"# Load the files\ndf = pd.read_csv(\"..\/input\/predicting-divorce\/divorce.csv\")\ndf.shape","72104432":"with open('..\/input\/predicting-divorce\/Divorce_README.txt') as file:\n    lines = file.readlines()\n    lines = [line.rstrip() for line in lines]","0dd92414":"que = [x.split('\\t')[1] for x in lines[2:]]\nque[:5]","0e8e980a":"df.info(verbose=False)","f3f0182c":"# Visualize the missing values in data\nimport missingno as msno\nmsno.matrix(df)","7e44a1f4":"# Descriptive statistics\nvalues = dict()\nque_cols = df.drop('Divorce_Y_N',axis=1).columns\nfor c in que_cols:\n    values[c] = dict(round(df[c].value_counts(normalize=True)*100,2))\n    \nque_rate_df = pd.DataFrame(values).rename(columns=dict(zip(que_cols,que))).T\nque_rate_df = que_rate_df[range(5)]\n\nprint(\" -> Yellow highlighted cells indicates the max percentage of people given that rating\")\nprint(\" -> Green highlighted cells indicates the min percentage of people given that rating\")\n# Highlighting the max & min value\nque_rate_df.style\\\n      .format('{:.1f}%')\\\n      .highlight_max(color = 'yellow',axis=1)\\\n      .highlight_min(color = 'lightgreen',axis=1)","c95fee50":"# Correlation study\nplt.figure(figsize=(15,12))\nsns.heatmap(df.corr(),cmap='Spectral')\nplt.title(\"Correlation value between ratings given for questions\")","4d590163":"!pip install factor_analyzer","24defd0e":"data = df.drop('Divorce_Y_N',axis=1)","cb54b948":"from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\nchi2,p = calculate_bartlett_sphericity(data)\nprint(\"Chi squared value : \",chi2)\nprint(\"p value : \",p)","fc484adf":"from factor_analyzer.factor_analyzer import calculate_kmo\nkmo_vars,kmo_model = calculate_kmo(data)\nprint(kmo_model)","127120c9":"from factor_analyzer import FactorAnalyzer\nn = data.shape[1]\nfa = FactorAnalyzer(rotation = None,impute = \"drop\",n_factors=n)\nfa.fit(data)\nev,_ = fa.get_eigenvalues()\nplt.scatter(range(1,n+1),ev)\n\nplt.plot(range(1,n+1),ev)\nplt.title('Scree Plot')\nplt.xlabel('Factors')\nplt.ylabel('Eigen Value')\nplt.grid()","1967c36b":"fa = FactorAnalyzer(n_factors=5,rotation='varimax')\nfa.fit(data)\nfa_load = pd.DataFrame(fa.loadings_,index=data.columns)","d44ff560":"def highlight_max(s):\n    # Get 5 largest values of the column\n    is_large = s.nlargest(10).values\n    # Apply style is the current value is among the 5 biggest values\n    return ['background-color: yellow' if v in is_large else '' for v in s]\n\nfa_load.sort_values(by=0,ascending=False).style.apply(highlight_max)","3e8ac8d2":"print(pd.DataFrame(fa.get_factor_variance(),index=['Variance','Proportional Var','Cumulative Var']))","bc6e4d55":"X = data.copy()\ny = df['Divorce_Y_N'].copy()\nX.shape, y.shape","7364b51f":"from sklearn.decomposition import FactorAnalysis\ntransformer = FactorAnalysis(n_components=5, random_state=0)\nX_transformed = transformer.fit_transform(X)\nX_transformed.shape","c899d283":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\nk = 5\nmodel = LogisticRegression()\nscores = cross_val_score(model,X_transformed,y,cv=k)\nprint(f\"{k} fold - cross validated scores - {scores}\")\nprint(f\"Average accuracy scores - {scores.mean()}\")","5f4ceadb":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_transformed,y,test_size=0.5,stratify=y,random_state=100)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","8b2652cb":"# Build classification model\nmodel = LogisticRegression(random_state=1)\nmodel.fit(X_train,y_train)","7c9018b4":"# Prediction\ny_pred = model.predict(X_test)","6720caaa":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))","47333eba":"model.coef_","61898e3d":"from sklearn.metrics import accuracy_score\n\ntrain_acc = []\ntest_acc = []\ncv_scores = []\n\nfor f in range(2,54):\n    transformer = FactorAnalysis(n_components=f, random_state=0)\n    X_transformed = transformer.fit_transform(X)\n    \n    model = LogisticRegression()\n    scores = cross_val_score(model,X_transformed,y,cv=5)\n    cv_score = scores.mean()\n    X_train, X_test, y_train, y_test = train_test_split(X_transformed,y,test_size=0.5,stratify=y,random_state=100)\n    \n    model.fit(X_train,y_train)\n    \n    y_pred_tr = model.predict(X_train)\n    y_pred_ts = model.predict(X_test)\n    \n    tr_acc = accuracy_score(y_train,y_pred_tr)\n    ts_acc = accuracy_score(y_test,y_pred_ts)\n    \n    train_acc.append(tr_acc)\n    test_acc.append(ts_acc)\n    cv_scores.append(cv_score)","ee754915":"pd.DataFrame({'factors':range(2,54),\n              'train_acc':train_acc,\n              'test_acc':test_acc,\n              'cv_score':cv_scores}).set_index('factors').plot(figsize=(12,8))\nplt.title(\"Factor analysis - Train acc vs Test acc vs CV score\")","612dd120":"Reference - [Analytics vidhya](https:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/dimensionality-reduction-using-factor-analysis-in-python\/)","c7281341":"### Find the Optimal no of factors based on train-test accuracy","864ff82a":"**The p-value < 0.05, correlation is present among the variables with 95% confidence interval**","73f4c181":"### Why Factor Analysis and not PCA?","5753484f":"Factor loading can tell us how each factor created is explaining each of the variable. More value says that factor explains well about that variable. Sorting the factor load values helps us understand which variables are grouped to represent a new factor.","fb2acac2":"**By looking at the coeff, factor 0 is most significant feature that predicts the divorce**","96f64803":"The data doesnt contain any null values.","3bbddb30":"**KMO score is close to 1, thus applying factor analysis can be effective for the dataset.**","d4da792e":"### KMO Test\n\nKMO Test measures the proportion of variance that might be a common variance among the variables. Larger proportions are expected as it represents more correlation is present among the variables thereby giving way for the application of dimensionality reduction techniques such as Factor Analysis. **KMO score is always between 0 to 1 and values more than 0.6 are much appreciated.**","11c8d126":"**By looking at the graph, the optimal number of factors can be decided between 5 - 14**     \n**As number of factors increases, the difference between train and test accuracy increases, which clearly indicates the overfitting**","f6c79200":"#### Bartlett's test of sphericity\n\nH0 : Correlation matrix is an Identical matrix\n\nH1 : Correlation matrix is not Identical matrix","de5e8eab":"### Test to check for adequacy\n\nThe following tests are used to check the adequacy to apply factor analysis to the dataset. \n\n**1. BARTLETT\u2019S TEST OF SPHERICITY**\n\n**2. KAISER-MEYER-OLKIN (KMO) TEST**","26378620":"1. Both Principal Components Analysis (PCA) and Factor Analysis are dimension reduction techniques.\n2. Principal Component analysis makes the components that are completely orthogonal to each other whereas Factor analysis does not require such the factors to be orthogonal\n3. Principal Components is primarily an exploratory technique to rationalize the number of variables in an analysis (for further causal analysis). Factor analysis, on the other hand, is approached with a prior knowledge or belief that which variables may be similar and therefore should load on a single factor. Factor analysis is thus a popular variable reduction technique in market research.","e346f104":"### Factor Loading","1e0cbf6b":"### Factor Analysis\n\nFactor analysis is a linear statistical model, that is used to explain the variance among the observed variable and condense a set of the observed variable into the unobserved variable called factors. Observed variables are modeled as a linear combination of factors and error terms. Factor or latent variable is associated with multiple observed variables, who have common patterns of responses. Each factor explains a particular amount of variance in the observed variables. It helps in data interpretations by reducing the number of variables. ([Source](https:\/\/www.datacamp.com\/community\/tutorials\/introduction-factor-analysis))","50cc09fc":"#### How to infer from the above summary?\n\nFor last question - **I'm not afraid to tell my spouse about her\/his incompetence** - **33.5% people rated 4 while 8.8% people rated 3**","d72665ea":"Since most of the features are correlated with each other, its better to factor analysis","95878ee6":"The dataset contains the rating given by person to his\/her spouse based on 55 different questions asked related to marriage life. \n\nAnd, the person has to rate from 0 - 4, with 0 being lowest and 4 being highest","79f7b296":"Values in all columns have integer data type","ffbe0ba1":"**END OF NOTEBOOK**","6c0bc39d":"**The new 5 factors created can explain 82.5% variance in the data.**","3364d009":"### Model Prediction","10f8464a":"## Divorce prediction using Factor Analysis","7bd5a82c":"### EDA"}}