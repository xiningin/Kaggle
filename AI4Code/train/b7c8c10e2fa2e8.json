{"cell_type":{"376d3f1a":"code","18d1488e":"code","c3b4dc55":"code","277781e2":"code","a0f889a4":"code","a34dd3eb":"code","b03986fe":"markdown","7703c09c":"markdown","658d2d46":"markdown"},"source":{"376d3f1a":"!pip install --no-index --find-links ..\/input\/pytorchtabnet\/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet","18d1488e":"import sys\nsys.path.append('..\/input\/iterativestratification')\n\nimport numpy as np\nimport random\nimport pandas as pd\nimport os\nimport copy\nimport gc\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nimport optuna\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn.modules.loss import _WeightedLoss\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('max_columns', 2000)\n\nfrom tqdm import tqdm\n\nimport glob\n\ndef sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)\n \ndef prep_data(n_comp_GENES, n_comp_CELLS, VarianceThreshold_for_FS):\n    \n    train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\n    train_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\n    train_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n    test_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\n\n    Xtrain = pd.get_dummies(train_features.drop('sig_id',axis=1), columns = ['cp_dose'],drop_first = True)\n    Xtest = pd.get_dummies(test_features.drop('sig_id',axis=1), columns = ['cp_dose'],drop_first = True)\n\n    target= train_targets_scored.drop('sig_id',axis=1)\n\n    GENES = [col for col in train_features.columns if col.startswith('g-')]\n    CELLS = [col for col in train_features.columns if col.startswith('c-')]\n\n    Xtrain = pd.concat([Xtrain[GENES], Xtrain[CELLS], Xtrain.drop(GENES+CELLS,axis=1)],axis=1)\n    Xtest = pd.concat([Xtest[GENES], Xtest[CELLS], Xtest.drop(GENES+CELLS,axis=1)],axis=1)\n\n    # RankGauss - transform to Gauss\n    for col in tqdm((GENES + CELLS)):\n\n        transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n        Xtrain[col] = transformer.fit_transform(Xtrain[[col]])\n        Xtest[col] = transformer.transform(Xtest[[col]])\n\n    data = pd.concat([Xtrain[GENES], Xtest[GENES]])\n    data2 = PCA(n_components=n_comp_GENES, random_state=42).fit_transform(data)\n\n    Xtrain2 = pd.DataFrame(data2[:train_features.shape[0]], columns = [f'pca_G-{i}' for i in range(n_comp_GENES)])\n    Xtest2 =  pd.DataFrame(data2[train_features.shape[0]:], columns = [f'pca_G-{i}' for i in range(n_comp_GENES)])\n\n    data = pd.concat([Xtrain[CELLS], Xtest[CELLS]])\n    data2 = PCA(n_components=n_comp_CELLS, random_state=42).fit_transform(data)\n\n    Xtrain3 = pd.DataFrame(data2[:train_features.shape[0]], columns = [f'pca_C-{i}' for i in range(n_comp_CELLS)])\n    Xtest3 =  pd.DataFrame(data2[train_features.shape[0]:], columns = [f'pca_C-{i}' for i in range(n_comp_CELLS)])\n\n    Xtrain_tot = pd.concat((Xtrain, Xtrain2, Xtrain3), axis=1)\n    Xtest_tot = pd.concat((Xtest, Xtest2, Xtest3), axis=1)\n\n    var_thresh = VarianceThreshold(VarianceThreshold_for_FS)\n\n    data = Xtrain_tot.append(Xtest_tot)\n    data = data.iloc[:, 2:]\n    data_transformed = var_thresh.fit_transform(data.drop('cp_type',axis=1))\n    var = var_thresh.variances_\n\n    Xtrain_transformed = pd.DataFrame(data_transformed[ : len(Xtrain)], columns = data.drop('cp_type',axis=1).columns[var>VarianceThreshold_for_FS])\n    Xtest_transformed = pd.DataFrame(data_transformed[len(Xtrain) : ], columns = data.drop('cp_type',axis=1).columns[var>VarianceThreshold_for_FS])\n\n    Xtrain_transformed['cp_type'] = Xtrain['cp_type']\n    Xtest_transformed['cp_type'] = Xtest['cp_type']\n\n    Xtrain_transformed['cp_time'] = Xtrain['cp_time']\n    Xtest_transformed['cp_time'] = Xtest['cp_time']\n\n    Xtrain_transformed['cp_dose_D2'] = Xtrain['cp_dose_D2']\n    Xtest_transformed['cp_dose_D2'] = Xtest['cp_dose_D2']\n\n    return Xtrain, Xtest, Xtrain_transformed, Xtest_transformed, target     \n    \n    \nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss\n    \nclass LabelSmoothingLoss(nn.Module):\n    \n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing \/ (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n\ndef score(y, yp):\n    return - np.mean(y*np.log(yp+10**(-8)) + (1-y)*np.log(1-yp+10**(-8)))  \n\ndef run_k_fold(folds, seed, Xtrain, Xtest, target, train):\n\n    oof = []\n    predictions = []\n    models = []\n    for fold in range(len(np.unique(folds))):\n\n        oof_, pred_ , model = run_training(folds, fold, seed, Xtrain, Xtest, target, train)\n\n        predictions.append(pred_)\n        oof.append(oof_)\n\n    oof = pd.concat(oof)\n    predictions = np.mean(predictions,axis=0)\n    return oof, predictions, models\n\ndef run_training(fold, seed, Xtrain, Xtest, target, tabnet_params):\n\n    seed_everything(seed)\n    \n    num_features=Xtrain.shape[1]-1\n    num_targets=target.shape[1]\n    \n    x_train = Xtrain[folds != fold].drop('cp_type',axis=1)\n    x_valid = Xtrain[(folds == fold) & (Xtrain.cp_type == 'trt_cp')].drop('cp_type',axis=1)\n    y_train = target.loc[x_train.index]\n    y_valid = target.loc[x_valid.index]\n    \n    iddx = y_valid.index\n    \n    x_train, x_valid, y_train, y_valid = x_train.values, x_valid.values, y_train.values, y_valid.values\n\n    model = TabNetRegressor(**tabnet_params)\n    model.fit(X_train=x_train,\n            y_train=y_train,\n            eval_set=[(x_valid, y_valid)],\n            eval_name = [\"val\"],\n            eval_metric = [\"logits_ll\"],\n            max_epochs=EPOCHS,\n            patience=20, batch_size=BATCHSIZE, virtual_batch_size=GBATCHSIZE,\n            num_workers=1, drop_last=False,\n            # use binary cross entropy as this is not a regression problem\n            loss_fn=SmoothBCEwLogits(smoothing =0.001))\n    \n    x_test = Xtest.drop('cp_type',axis=1).values\n\n    # save tabnet model\n    oof = model.predict(x_valid)\n    oof = pd.DataFrame(oof, index = iddx)\n    predictions = model.predict(x_test)\n    return oof, predictions\n\ndef run_k_fold(NFOLDS, seed, Xtrain, Xtest, target, tabnet_params):\n\n    oof = []\n    predictions = []\n\n    for fold in np.unique(NFOLDS):\n\n        oof_, pred_ = run_training(fold, seed, Xtrain, Xtest, target, tabnet_params)\n        predictions.append(pred_)\n        oof.append(oof_)\n        \n    oof = pd.concat(oof).sort_index()\n    predictions = np.mean(predictions,axis=0)\n    return oof, predictions\n\ndef make_all_preds(Xtrain_transformed, Xtest_transformed, target, folds, train):\n\n    oof, predictions = run_k_fold(folds, seed, Xtrain_transformed, Xtest_transformed, target, train)\n\n    return oof.sort_index(), predictions\n\nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n\nclass LogitsLogLoss(Metric):\n    \"\"\"\n    LogLoss with sigmoid applied\n    \"\"\"\n\n    def __init__(self):\n        self._name = \"logits_ll\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_pred):\n        \"\"\"\n        Compute LogLoss of predictions.\n\n        Parameters\n        ----------\n        y_true: np.ndarray\n            Target matrix or vector\n        y_score: np.ndarray\n            Score matrix or vector\n\n        Returns\n        -------\n            float\n            LogLoss of predictions vs targets.\n        \"\"\"\n        targets = y_true * (1.0 - 1e-4) + 0.5 * 1e-4\n        logits = 1 \/ (1 + np.exp(-y_pred))\n        aux = (1 - y_true) * np.log(1 - logits + 1e-15) + y_true * np.log(logits + 1e-15)\n        return np.mean(-aux)","c3b4dc55":"n_comp_GENES = 450\nn_comp_CELLS = 65\nVarianceThreshold_for_FS = 0.9\nn_d = 32\nn_a = 256\nn_steps = 1\ngamma = 1.5\nlambda_sparse = 1e-7\nLEARNING_RATE = 8e-3\nwd = 1e-5\ndiv_factor = 10\nEPOCHS = 500\nBATCHSIZE = 248\nGBATCHSIZE = 64\nXtrain, Xtest, Xtrain_transformed, Xtest_transformed, target  = prep_data(n_comp_GENES, n_comp_CELLS, VarianceThreshold_for_FS)\nBATCHES = int(len(Xtrain)\/BATCHSIZE)\nSEED = 0\nn_independent= 1\nn_shared = 0 \n\nGROUP = 'tabnet'\ntabnet_params = dict(\n    n_d=n_d, \n    n_a=n_a, \n    n_steps=n_steps,\n    n_independent = n_independent,\n    gamma=gamma,\n    lambda_sparse=lambda_sparse, \n    n_shared = n_shared,\n    optimizer_fn=torch.optim.Adam,\n    optimizer_params=dict(lr=LEARNING_RATE, weight_decay=wd),\n    mask_type='entmax',\n    scheduler_params=dict(mode = \"min\", patience = 7, min_lr = 1e-5, factor = 0.07),\n    scheduler_fn=optim.lr_scheduler.ReduceLROnPlateau,\n    verbose=10,\n    )","277781e2":"preds_tabnet = []\nGROUP = 'tabnet_4'\nseed = 42\nhidden_size = 0\nNFOLDS = 5\nfor SEED in [1,2,3,4,5]:\n    \n    folds = pd.Series(np.zeros(len(Xtrain)), index = Xtrain.index)\n    mskf = MultilabelStratifiedKFold(n_splits=NFOLDS)\n\n    for f, (t_idx, v_idx) in enumerate(mskf.split(X=Xtrain, y=target)):\n        folds.loc[v_idx] = int(f)  \n        \n    preds = np.zeros((Xtest.shape[0], target.shape[1]))\n    oof, preds = make_all_preds(Xtrain, Xtest, target, folds, train = tabnet_params)\n    preds_tabnet.append(sigmoid(preds))","a0f889a4":"preds_tabnet = np.mean(preds_tabnet,axis=0)","a34dd3eb":"test_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsubmit = pd.DataFrame(preds_tabnet, columns = target.columns, index = Xtest.index)\nsubmit.loc[Xtest.cp_type!='trt_cp'] = 0\nsubmit.index = test_features.sig_id\nsubmit = submit.reset_index()\nsubmit.to_csv('submission.csv', index=False)","b03986fe":"A tabnet architecture that perform correctly. Possible to push LB score to 0.01833 by increasing folds to 7\n\nI also made the training here, but I reach same results with the training outside the kernel with only train + public train set.\n\nCredit: \n\nPCA \/ Rank Gauss : [vbmokin](https:\/\/www.kaggle.com\/vbmokin\/moa-pytorch-rankgauss-pca-nn-upgrade-3d-visual)\n\nInitial Tabnet : [optimo](https:\/\/www.kaggle.com\/optimo\/tabnetregressor-2-0-train-infer)","7703c09c":"## Submission","658d2d46":"# Mechanisms of Action (MoA) Prediction"}}