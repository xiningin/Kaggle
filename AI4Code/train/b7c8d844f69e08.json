{"cell_type":{"2ab6633e":"code","922ebe14":"code","dcdfb3ec":"code","08f0a99a":"code","c1b09815":"code","27a4af4d":"code","2ad3210e":"code","4e415e5a":"code","3c92ca88":"code","1ca0c12f":"code","fcdd4541":"code","de5c9c3e":"code","c95b801e":"code","8317b988":"code","cdc808fe":"code","0f7fc597":"code","804b0f62":"code","9266772a":"code","e256cd83":"code","9a15a10d":"code","673792d6":"code","83b0d5b1":"code","6c13dfac":"code","750b57ea":"code","cd244863":"code","20a7760d":"code","71a91ac8":"code","ae6100d2":"markdown","88739df8":"markdown","92ca9248":"markdown","e44d2b90":"markdown","5580aded":"markdown","435bd06c":"markdown","39acca79":"markdown","10f67893":"markdown","d2524429":"markdown","511eb41f":"markdown","5000002c":"markdown","db91cee0":"markdown","962fa6e5":"markdown","8d1fbd88":"markdown","caf9d1f9":"markdown","c2f68f2a":"markdown","4315e610":"markdown","44c54890":"markdown","793dc184":"markdown","79baabfa":"markdown","f64ea3f8":"markdown"},"source":{"2ab6633e":"#importing libraries\n\nimport spacy\nimport pandas as pd\nimport random","922ebe14":"nlp = spacy.load('en')","dcdfb3ec":"type(nlp)","08f0a99a":"tweets = pd.read_csv(\"..\/input\/5000-justdoit-tweets-dataset\/justdoit_tweets_2018_09_07_2.csv\")","c1b09815":"tweets.head()","27a4af4d":"tweets.shape","2ad3210e":"tweets['tweet_full_text'][:20]","4e415e5a":"#random tweets\nrandom.seed(1024)\n\nrandom_tweets = tweets['tweet_full_text'][random.sample(range(1,5000), 20)]\nrandom_tweets","3c92ca88":"#combined text\n\ncombined_text = str(random_tweets)\ncombined_text\n# len(combined_text)\n# type(combined_text)","1ca0c12f":"doc = nlp(combined_text)\ndoc","fcdd4541":"type(doc)","de5c9c3e":"#tokenization using split as space\ndoc.text.split()","c95b801e":"[token.orth_ for token in doc]","8317b988":"[(token.orth_, token.orth) for token in doc if not token.is_punct | token.is_space | token.is_stop]","cdc808fe":"extracted_tokens = [token.orth_ for token in doc if not token.is_punct | token.is_space | token.is_stop]\nextracted_tokens","0f7fc597":"only_word_tokens = [i for i in extracted_tokens if i.isalpha()]\nonly_word_tokens","804b0f62":"list(doc.sents)","9266772a":"[word.lemma_ for word in doc]","e256cd83":"pos_tag = [(word, word.tag_, word.pos_) for word in doc]\npos_tag","9a15a10d":"[i for i in pos_tag if i[1] == 'POS']","673792d6":"[j for j in pos_tag if j[2] == 'PART']","83b0d5b1":"nouns = list(doc.noun_chunks)\nnouns","6c13dfac":"[(token, token.dep_) for token in doc]","750b57ea":"[i for i in doc.ents]","cd244863":"[(i, i.label_, i.label) for i in doc.ents]","20a7760d":"#named entities along with text labels\nspacy.displacy.render(doc, style='ent', jupyter=True)","71a91ac8":"spacy.displacy.render(doc, style='dep', jupyter=True)","ae6100d2":"### Dependency Parser Visualization","88739df8":"### Visualizing named entities alongwith labels","92ca9248":"### Syntactic dependency between tokens","e44d2b90":"#### Methods with underscore suffix in spaCy returns strings whereas methods without underscore suffix returns numbers","5580aded":"## POS Tagging","435bd06c":"### Entities alongwith their labels","39acca79":"### Noun chunks in the data","10f67893":"### Random 20 tweets","d2524429":"### Naive approach for splitting","511eb41f":"### Paste the following code in command line to download the English version of spaCy. \n\n#python -m spacy download en","5000002c":"#### No 'POS' tags in our data so we do not have the option to exploit the owner and the possession information.","db91cee0":"## Tokenization using spaCy","962fa6e5":"## Lemmatization","8d1fbd88":"## Named Entity Recognition","caf9d1f9":"#### For our NLP task, we are only interested in tweets_full_text.","c2f68f2a":"### Combining text as nlp variable created using spaCy will only use str","4315e610":"#### Tokens after removing punctuations, space and stop words","44c54890":"#### Naive approach just breaks the sentence into parts based on the splitting criteria.","793dc184":"#### 'PART' tag suggests that these words are parts of the previous words.","79baabfa":"### Tokenization based on sentences","f64ea3f8":"## Loading the dataset"}}