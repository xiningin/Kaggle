{"cell_type":{"b4b91f0b":"code","3b032cf0":"code","42706244":"code","4a8770b8":"code","1bbc1785":"code","601138f0":"code","abd6f50b":"code","6a4cb440":"code","a379f54c":"code","ab238e34":"code","7df0aef6":"code","74af1a9b":"code","4ac29b4e":"code","94b3fab3":"code","3f3f0c57":"code","fd9f740a":"code","6ac1b1d7":"code","aa707d82":"code","fb244e9d":"code","9d4024b6":"code","26207e93":"code","3e8a55b0":"code","7fd1ae51":"code","f9544620":"code","ae8286c4":"code","db99ec77":"code","b310e44f":"code","fe2bd128":"code","e293a2ca":"code","ed06389b":"code","90250ed8":"code","0a72c6b2":"code","2a07d29a":"code","5384bd3c":"code","306661f6":"code","bab51fac":"code","7a1bf14b":"code","a12a42be":"markdown","c079cda7":"markdown","3cbba2bc":"markdown","413f72ef":"markdown","3f0637ad":"markdown","f38419f2":"markdown","f5cf946a":"markdown","632f0241":"markdown","e3a9f6ae":"markdown","a0a4efb2":"markdown","e7e2739f":"markdown","693ef685":"markdown","707b305e":"markdown","e5ecb437":"markdown","8f118d31":"markdown","5d7c9bea":"markdown","7986865a":"markdown","0449dba4":"markdown","7585aee0":"markdown","f934bd10":"markdown","65d03654":"markdown","be10cba6":"markdown","34e12d85":"markdown","e9eb30bb":"markdown","9840a307":"markdown","82c16895":"markdown","23b8674a":"markdown","b2106a89":"markdown","7d5d2d3c":"markdown","cb5b89d1":"markdown","51fdff3d":"markdown","9006b59f":"markdown","c9a17a00":"markdown"},"source":{"b4b91f0b":"import numpy as np\nimport pandas as pd\nimport sklearn\nimport xgboost as xgb\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.externals import joblib\n\nimport re\nfrom itertools import product\nimport gc\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns","3b032cf0":"# Check library versions\nfor p in [np, pd, sklearn, xgb]:\n    print (p.__name__, p.__version__)","42706244":"# Load data from csv files\nsalesTrain = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv', dtype={'date': 'str', 'date_block_num': 'int32', 'shop_id': 'int32', 'item_id': 'int32', 'item_price': 'float32', 'item_cnt_day': 'int32'})\ntest = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/\/test.csv', dtype={'ID': 'int32', 'shop_id': 'int32', 'item_id': 'int32'})\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/\/items.csv', dtype={'item_name': 'str', 'item_id': 'int32', 'item_category_id': 'int32'})\nitemCategories = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/\/item_categories.csv', dtype={'item_category_name': 'str', 'item_category_id': 'int32'})\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/\/shops.csv', dtype={'shop_name': 'str', 'shop_id': 'int32'})","4a8770b8":"# Join item_category_id to sales_train data\nsales = pd.merge(salesTrain, items, on='item_id', how='left')\nsales = sales.drop('item_name', axis=1)\nsales.head()","1bbc1785":"# For every month we create a grid from all shops\/items combinations from that month\ngrid = []\nfor blockNum in sales.date_block_num.unique():\n    shopIds = sales.loc[sales.date_block_num == blockNum, 'shop_id'].unique()\n    itemIds = sales.loc[sales.date_block_num == blockNum, 'item_id'].unique()\n    grid.append(np.array(list(product(*[[blockNum], shopIds, itemIds])), dtype='int32'))\ngrid = pd.DataFrame(np.vstack(grid), columns=['date_block_num', 'shop_id', 'item_id'], dtype='int32')","601138f0":"grid.head()","abd6f50b":"# Get aggregated values for (shop_id, item_id, month)\n#   The count and average price of sold items in each shop for a month\nsalesMean = sales.groupby(['date_block_num', 'shop_id', 'item_id']).agg({'item_cnt_day': 'sum', 'item_price':'mean'}).reset_index()\nsalesMean = pd.merge(grid, salesMean, how='left', on=['date_block_num', 'shop_id', 'item_id']).fillna(0)\nsalesMean = pd.merge(salesMean, items, how='left', on=['item_id'])\nsalesMean.columns = ['date_block_num', 'shop_id', 'item_id', 'shop_item_count_sum', 'shop_item_price_mean', 'item_name', 'item_category_id']\nsalesMean.drop(['item_name'], axis=1, inplace=True)\nsalesMean.shop_item_count_sum = salesMean.shop_item_count_sum.astype('int32')\n\nsalesMean.head(10).T","6a4cb440":"plt.figure(figsize=(15, 5))\nplt.ylim(0,200)\nplt.hist(salesMean.shop_item_count_sum, bins=100)\nplt.show()","a379f54c":"# Clip target values into [0, 20] range\nsalesMean.shop_item_count_sum = salesMean.shop_item_count_sum.clip(0, 20)\n\nplt.figure(figsize=(15, 5))\nplt.ylim(0,100000)\nplt.hist(salesMean.shop_item_count_sum, bins=20)\nplt.show()","ab238e34":"plt.figure(figsize=(15, 5))\nplt.ylim(0,10)\n# plt.xlim(0, 50000)\nplt.hist(salesMean.shop_item_price_mean, bins=100)\nplt.show()","7df0aef6":"# Clip item prices into [0, 40000] range\nsalesMean.shop_item_price_mean = salesMean.shop_item_price_mean.clip(0, 40000)\n\nplt.figure(figsize=(15, 5))\nplt.ylim(0,1000)\nplt.hist(salesMean.shop_item_price_mean, bins=20)\nplt.show()","74af1a9b":"def encodeMean(groupColumns, tarnsformColumn, outputColumn):\n    gb = salesMean.groupby(groupColumns)\n    salesMean[outputColumn + '_mean'] = gb[tarnsformColumn].transform('mean').astype('float32')\n\ndef encodeMeanSum(groupColumns, tarnsformColumn, outputColumn):\n    gb = salesMean.groupby(groupColumns)\n    salesMean[outputColumn + '_mean'] = gb[tarnsformColumn].transform('mean').astype('float32')\n    salesMean[outputColumn + '_sum'] = gb[tarnsformColumn].transform('sum').astype('float32')\n\nencodeMean   (['date_block_num', 'shop_id'],          'shop_item_price_mean', 'shop_price')\nencodeMeanSum(['date_block_num', 'shop_id'],          'shop_item_count_sum',  'shop_count')\n\nencodeMean   (['date_block_num', 'item_id'],          'shop_item_price_mean', 'item_price')\nencodeMeanSum(['date_block_num', 'item_id'],          'shop_item_count_sum',  'item_count')\n\nencodeMean   (['date_block_num', 'item_category_id'], 'shop_item_price_mean', 'category_price')\nencodeMeanSum(['date_block_num', 'item_category_id'], 'shop_item_count_sum',  'category_count')\n\nsalesMean.head(10).T","4ac29b4e":"lags = [1, 2, 3]\n\nbaseColumns = ['date_block_num', 'shop_id', 'item_id', 'item_category_id']\nlagColumns = ['shop_item_count_sum', 'shop_item_price_mean', 'shop_price_mean', 'shop_count_mean', 'shop_count_sum', 'item_price_mean',\n              'item_count_mean', 'item_count_sum', 'category_price_mean', 'category_count_mean', 'category_count_sum']\n\ndef addLags(salesOrigin, salesMerged):\n    for lag in lags:\n        s = salesOrigin.copy()\n        s.date_block_num += lag\n        s = s[baseColumns + lagColumns]\n        s.columns = baseColumns + [c + '_' + str(lag) for c in lagColumns]\n        salesMerged = pd.merge(salesMerged, s, how='left', on=baseColumns)\n    return salesMerged","94b3fab3":"medians = salesMean.median()\n\ndef fillOutNan(df):\n    for column in df.columns:\n        if 'count' in column:\n            df[column] = df[column].fillna(0)\n        elif 'price' in column:\n            c = re.sub(r'_[0-9]+$', \"\", column)\n            df[column] = df[column].fillna(medians[c])\n    return df","3f3f0c57":"salesMeanLags = salesMean[baseColumns + ['shop_item_count_sum']]\n\n# Create the lag value of all mean encodings\nsalesMeanLags = addLags(salesMean, salesMeanLags)\n\n# Remove values having no lag valuses\nsalesMeanLags = salesMeanLags[salesMeanLags.date_block_num >= max(lags)]\n\n# Fill NaN with zero and median\nsalesMeanLags = fillOutNan(salesMeanLags)\n\nsalesMeanLags.head(10).T","fd9f740a":"# Show the correlation of all features for shop_item_count_sum\ncorr = salesMeanLags.corr()\nplt.figure(figsize=(25,1))\npc = pd.DataFrame([corr.loc['shop_item_count_sum', :]], columns=corr.index)\nsns.heatmap(pc, annot=True)","6ac1b1d7":"validBlock = salesMeanLags.date_block_num.max()\n\nxAll = salesMeanLags.loc[:, salesMeanLags.columns != 'shop_item_count_sum']\nyAll = salesMeanLags.loc[:, salesMeanLags.columns == 'shop_item_count_sum']\n\nxTrain = xAll.loc[xAll.date_block_num <  validBlock]\nxValid = xAll.loc[xAll.date_block_num == validBlock]\n\nyTrain = yAll.loc[xAll.date_block_num <  validBlock]\nyValid = yAll.loc[xAll.date_block_num == validBlock]","aa707d82":"# Join item_category_id and mean encodings to test data\nxTest = pd.merge(test, items, on='item_id', how='left')\nxTest = xTest.drop(['ID', 'item_name'], axis=1)\nxTest['date_block_num'] = salesMean.date_block_num.max() + 1\nxTest.date_block_num = xTest.date_block_num.astype('int32')","fb244e9d":"xTest = xTest[baseColumns]\n\n# Create the lag value of all mean encodings\nxTest = addLags(salesMean[salesMean.date_block_num > salesMean.date_block_num.max() - max(lags)], xTest)\n\n# Fill NaN with zero and median\nxTest = fillOutNan(xTest)\n\nxTest.head(10).T","9d4024b6":"# Check the columns of test data\nassert sum(xTrain.columns != xTest.columns) == 0","26207e93":"# If true, eanble Grid Searh\ngridSearch = False\n\n# Prediction or Train\/Valid\nprediction = True","3e8a55b0":"del salesTrain\ndel items\ndel itemCategories\ndel shops\ndel sales\ndel grid\ndel salesMean\ndel salesMeanLags\n\n# del gb\ndel corr\ndel pc\n\nif not prediction:\n    del xTrain\n    del xValid\n    del yTrain\n    del yValid\n\n# garbage collect\ngc.collect()","7fd1ae51":"if not prediction:\n    lr = LinearRegression()\n    lr.fit(xTrain.values, yTrain.values)\n\n    predTrainLr = lr.predict(xTrain.values)\n\n    print('Train R-squared for LinearRegression is %f' % r2_score(yTrain, predTrainLr))\n    print('Train Mean Squared Error for LinearRegression is %f' % np.sqrt(mean_squared_error(yTrain, predTrainLr)))\n\n    predValidLr = lr.predict(xValid.values)\n\n    print('Valid R-squared for LinearRegression is %f' % r2_score(yValid, predValidLr))\n    print('Valid Mean Squared Error for LinearRegression is %f' % np.sqrt(mean_squared_error(yValid, predValidLr)))\n\n    # show coeficients\n    plt.figure(figsize=(15,3))\n    plt.bar(np.arange(lr.coef_.shape[1]), lr.coef_[0], tick_label=xTrain.columns)\n    plt.xticks(rotation='vertical')\n    plt.show()","f9544620":"if prediction:\n    lr = LinearRegression()\n    lr.fit(xAll.values, yAll)\n\n    predAllLr = lr.predict(xAll.values)\n\n    print('All R-squared for LinearRegression is %f' % r2_score(yAll, predAllLr))\n    print('All Mean Squared Error for LinearRegression is %f' % np.sqrt(mean_squared_error(yAll, predAllLr)))\n\n    predTestLr = lr.predict(xTest.values)","ae8286c4":"# show coeficients\nplt.figure(figsize=(15,3))\nplt.bar(np.arange(lr.coef_.shape[1]), lr.coef_[0], tick_label=xTest.columns)\nplt.xticks(rotation='vertical')\nplt.show()","db99ec77":"params = {'num_round': 100,\n          'eta': 0.3,\n          'seed': 123,\n          'silent': 1,\n          'eval_metric': 'rmse'}","b310e44f":"if gridSearch:\n    gridParams = {'max_depth': [5, 8, 10], \n                  'min_child_weight': [0.5, 0.75, 1],\n                  'subsample': [0.5, 0.75, 1]}\n\n    cvCount = 30000 # xValid.shape[0]\n    x = xValid.values[: cvCount]\n    y = yValid.values[: cvCount, 0]\n\n    gs = GridSearchCV(xgb.XGBClassifier(**params), gridParams, cv=5)\n    gs.fit(x, y)\n\n    print('The best score is %f' % gs.best_score_)\n    print('The best parameters are %s' % gs.best_params_)\n\n    params.update(gs.best_params_)\nelse:\n    # Use parameters which are searched on the latest calculation\n    params.update({'max_depth': 10,\n                   'min_child_weight': 0.5,\n                   'subsample': 0.5})","fe2bd128":"if not prediction:\n    bst = xgb.XGBClassifier(**params)\n    bst.fit(xTrain.values, yTrain.values[:, 0])\n\n    predTrainXgb = bst.predict(xTrain.values)\n    \n    print('Train R-squared for XGB is %f' % r2_score(yTrain, predTrainXgb))\n    print('Train Mean Squared Error for XGB is %f' % np.sqrt(mean_squared_error(yTrain, predTrainXgb)))\n\n    predValidXgb = bst.predict(xValid.values)\n\n    print('Valid R-squared for XGB is %f' % r2_score(yValid, predValidXgb))\n    print('Valid Mean Squared Error for XGB is %f' % np.sqrt(mean_squared_error(yValid, predValidXgb)))\n\n    # Show feature importances\n    bst.get_fscore()\n    mapper = {'f{0}'.format(i): v for i, v in enumerate(xTrain.columns)}\n    mapped = {mapper[k]: v for k, v in bst.get_fscore().items()}\n    fig, ax = plt.subplots(figsize=(10, 15))\n    xgb.plot_importance(mapped, ax=ax)\n    plt.show()","e293a2ca":"if prediction:\n    dAll = xgb.DMatrix(xAll.values, label = yAll.values)\n    dTest = xgb.DMatrix(xTest.values)\n    \n#     bst = xgb.train(params, dAll)\n    \n    # Save model to file\n#     joblib.dump(bst, \"xgb.dat\")\n    \n    # Load model to file\n    bst = joblib.load(\"..\/input\/xgbdat\/xgb.dat\")\n    \n    predAllXgb = bst.predict(dAll)\n    \n    print('All R-squared for XGB is %f' % r2_score(yAll, predAllXgb))\n    print('All Mean Squared Error for XGB is %f' % np.sqrt(mean_squared_error(yAll, predAllXgb)))\n\n    predTestXgb = bst.predict(dTest)","ed06389b":"# Show feature importances\nbst.get_fscore()\nmapper = {'f{0}'.format(i): v for i, v in enumerate(xTrain.columns)}\nmapped = {mapper[k]: v for k, v in bst.get_fscore().items()}\nfig, ax = plt.subplots(figsize=(10, 15))\nxgb.plot_importance(mapped, ax=ax)\nplt.show()","90250ed8":"if not prediction:\n    predTrainLv1 = pd.DataFrame()\n    predTrainLv1['lr']  = predTrainLr[:,0]\n    predTrainLv1['xgb'] = predTrainXgb\n\n    predValidLv1 = pd.DataFrame()\n    predValidLv1['lr']  = predValidLr[:,0]\n    predValidLv1['xgb'] = predValidXgb\n    \n    predTrainLv1.head(20).T\nelse:\n    predAllLv1 = pd.DataFrame()\n    predAllLv1['lr']  = predAllLr[:,0]\n    predAllLv1['xgb'] = predAllXgb\n\n    predTestLv1 = pd.DataFrame()\n    predTestLv1['lr']  = predTestLr[:,0]\n    predTestLv1['xgb'] = predTestXgb\n\n    predTestLv1.head(20).T","0a72c6b2":"lrLv2 = LinearRegression()\n\nif not prediction:\n    lrLv2.fit(predTrainLv1, yTrain.values)\n\n    predTrainLv2 = lrLv2.predict(predTrainLv1)\n    predValidLv2 = lrLv2.predict(predValidLv1)\n\n    print('Train R-squared for Ensembling is %f' % r2_score(yTrain, predTrainLv2))\n    print('Train Mean Squared Error for Ensembling is %f' % np.sqrt(mean_squared_error(yTrain, predTrainLv2)))\n\n    print('Valid R-squared for Ensembling is %f' % r2_score(yValid, predValidLv2))\n    print('Valid Mean Squared Error for Ensembling is %f' % np.sqrt(mean_squared_error(yValid, predValidLv2)))","2a07d29a":"if not prediction:\n    # Show coefitients\n    plt.figure(figsize=(5,3))\n    plt.bar(np.arange(lrLv2.coef_.shape[1]), lrLv2.coef_[0], tick_label=predTrainLv1.columns)\n    plt.xticks(rotation='vertical')\n    plt.show()","5384bd3c":"if prediction:\n    lrLv2.fit(predAllLv1, yAll.values)\n\n    predAllLv2 = lrLv2.predict(predAllLv1)\n\n    print('All R-squared for Ensembling is %f' % r2_score(yAll, predAllLv2))\n    print('All Mean Squared Error for Ensembling is %f' % np.sqrt(mean_squared_error(yAll, predAllLv2)))\n\n    predTestLv2 = lrLv2.predict(predTestLv1)","306661f6":"if prediction:\n    # Show coefitients\n    plt.figure(figsize=(5,3))\n    plt.bar(np.arange(lrLv2.coef_.shape[1]), lrLv2.coef_[0], tick_label=predTestLv1.columns)\n    plt.xticks(rotation='vertical')\n    plt.show()","bab51fac":"if prediction:\n#     pred = predTestLv2[:,0]\n    pred = predTestXgb\n    \n    pred = pred.clip(0, 20)\n    submission = pd.DataFrame({'ID': test.index, 'item_cnt_month': pred})\n    submission.to_csv('submission.csv',index=False)\n\n    print(submission.head())","7a1bf14b":"if prediction:\n    print(submission.describe())","a12a42be":"## Predict","c079cda7":"## Library versions","3cbba2bc":"# Create submission file","413f72ef":"The target values are clipped into [0, 20] range. so I'll clip values into the range and remove outliers.","3f0637ad":"Shop, item and category ID are encoded in 3 ways:\n- Average price\n- Average count of sold items\n- Total count of sold items","f38419f2":"## Train","f5cf946a":"I'll find the best parameters for XGBoost.","632f0241":"NaN values are filled with zero and median.","e3a9f6ae":"# Mean Encodings","a0a4efb2":"The lag value of all mean encodings are created.","e7e2739f":"## Predict","693ef685":"## Train","707b305e":"Since the competition task is to make a monthly prediction, I need to aggregate the data to montly level before doing any encodings.","e5ecb437":"# Load data","8f118d31":"I'll use a simple linear regression to combine the 1st level models.","5d7c9bea":"# Create train\/validate data","7986865a":"# Prediction","0449dba4":"# Lag Features","7585aee0":"## Predict","f934bd10":"## Free memory area","65d03654":"# XGBoost","be10cba6":"# Dependencies","34e12d85":"# Summary\nI used XGBoost, Linear Regression and ensembling. The most important features were data_block_num, shop_id, item_id and item_category_id. I used python. It took to train our model about an hour.\n\n# Features Selection \/ Engineering\nThe most important features for XGBoost model are data_block_num, shop_id, item_id and item_category_id. At first I had ignored these features and mainly used mean encodings such as monthly item sales and counts. I couldn't get good results with this strategy, however. I added these features on trial and I could get a score under 1 rmse.\n\n# Training Methods\nI used XGBoost and Linear Regression methods as the 1st level models. I ensembled models with a simple linear regression to combine these models. I was looking forward to be improved score with ensembling. However its improvement was a little and sometimes worse.\n\n# Interesting findings\nI had been using only some features to make prediction process faster in the beginning. These features were selected by the correlations between features and target value. Because I supposed it was the key to select important features. However low correlation features could sometimes improve scores. I found out I needed to provide all features and validate on models.\n\n# Model Execution Time\n- Computer specifications: Mac Book Pro 2.9 GHz i5 \/ 16 GB\n- EDA time: 5 minutes\n- Training and Validation time: 2 hours\n- Prediction time: 1 hour (a few minutes, if you use the saved XGBoost parameters.)\n- Grid search time: 10 hours","e9eb30bb":"# Fill NaN values","9840a307":"Linear regression and xgboost model are used as 1st level model to predcit.","82c16895":"# Aggregate data","23b8674a":"## Grid search","b2106a89":"## Train","7d5d2d3c":"# Linear Regression","cb5b89d1":"# Ensembling","51fdff3d":"# Create test data","9006b59f":"I'll free memory area which are no longer needed to make calculation faster.","c9a17a00":"# Clip outliers"}}