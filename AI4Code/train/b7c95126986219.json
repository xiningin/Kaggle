{"cell_type":{"fedcc17d":"code","dd3810e6":"code","1418c33c":"code","cb3d7593":"code","41907574":"code","231bf1ac":"code","c3527138":"code","508304a8":"code","18b64efe":"code","435f8f05":"code","5227af51":"code","23120290":"code","1f53ec9b":"code","629fd30b":"code","2a2f59e0":"markdown","12237db2":"markdown","bb9fc229":"markdown","12199909":"markdown","4cb1b962":"markdown","ea85687b":"markdown","d4d09ad4":"markdown","a91e17fd":"markdown","facce49e":"markdown","06bb360a":"markdown","ec4edbce":"markdown","5fd71989":"markdown"},"source":{"fedcc17d":"import numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom scipy import signal\nfrom sklearn import linear_model\nfrom sklearn.metrics import roc_curve, roc_auc_score, auc\nfrom tensorflow import keras\nimport matplotlib.gridspec as gridspec\n\nmatplotlib.rcParams['pdf.fonttype'] = 42\nmatplotlib.rcParams['svg.fonttype'] = 'none'","dd3810e6":"#%% script params\n\n# input parameters\nnum_axons_FF_cap = 100\ntime_delays_list_IF_cap = [250, 500]\nnum_time_delays_IF = len(time_delays_list_IF_cap) + 1\nnum_axons_IF = num_time_delays_IF * num_axons_FF_cap\n\nstimulus_duration_ms = 10000\nrequested_number_of_output_spikes = 40\nmin_time_between_spikes_ms = 135\n\n# neuron model parameters\nv_reset     = -80\nv_threshold = -55\ncurrent_to_voltage_mult_factor = 3\nrefreactory_time_constant = 15\n\n# F&F neuron model parameters\nconnections_per_axon_FF = 5\nnum_synapses_FF = connections_per_axon_FF * num_axons_FF_cap\n\n# synapse non-learnable parameters\ntau_rise_range_FF  = [1,16]\ntau_decay_range_FF = [8,24]\n\ntau_rise_vec_FF  = np.random.uniform(low=tau_rise_range_FF[0] , high=tau_rise_range_FF[1] , size=(num_synapses_FF, 1))\ntau_decay_vec_FF = np.random.uniform(low=tau_decay_range_FF[0], high=tau_decay_range_FF[1], size=(num_synapses_FF, 1))\n\n# synapse learnable parameters\nsynaptic_weights_vec_FF = np.random.normal(size=(num_synapses_FF, 1))\n\n# I&F neuron model parameters\nconnections_per_axon_IF = 1\nnum_synapses_IF = connections_per_axon_IF * num_axons_IF\n\n# synapse non-learnable parameters\ntau_rise_range_IF  = [1,1]\ntau_decay_range_IF = [24,24]\n\ntau_rise_vec_IF  = np.random.uniform(low=tau_rise_range_IF[0] , high=tau_rise_range_IF[1] , size=(num_synapses_IF, 1))\ntau_decay_vec_IF = np.random.uniform(low=tau_decay_range_IF[0], high=tau_decay_range_IF[1], size=(num_synapses_IF, 1))\n\n# synapse learnable parameters\nsynaptic_weights_vec_IF = np.random.normal(size=(num_synapses_IF, 1))\n\n# book-keeping\nsave_figures = True\nsave_figures = False\nall_file_endings_to_use = ['.png', '.pdf', '.svg']\n\nfigure_folder = '\/kaggle\/working\/'\n","1418c33c":"#%% helper functions\n\ndef create_single_PSP_profile(tau_rise, tau_decay, temporal_filter_length=50):\n\n    safety_factor = 1.5\n    if tau_rise >= (tau_decay \/ safety_factor):\n        tau_decay = safety_factor * tau_rise\n\n    exp_r = signal.exponential(M=temporal_filter_length, center=0, tau=tau_rise , sym=False)\n    exp_d = signal.exponential(M=temporal_filter_length, center=0, tau=tau_decay, sym=False)\n\n    post_syn_potential = exp_d - exp_r\n    post_syn_potential \/= post_syn_potential.max()\n\n    return post_syn_potential\n\n\ndef construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec):\n\n    num_synapses = tau_rise_vec.shape[0]\n    temporal_filter_length = int(4 * tau_decay_vec.max()) + 1\n\n    syn_filter = np.zeros((num_synapses, temporal_filter_length))\n\n    for k, (tau_r, tau_d) in enumerate(zip(tau_rise_vec, tau_decay_vec)):\n        syn_filter[k,:] = create_single_PSP_profile(tau_r, tau_d, temporal_filter_length=temporal_filter_length)\n\n    return syn_filter\n\n\ndef simulate_filter_and_fire_cell_training(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,\n                                           refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):\n\n    temporal_filter_length = int(5 * refreactory_time_constant) + 1\n    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]\n\n    # padd input and get all synaptic filters\n    normlized_syn_filter = np.flipud(construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec))\n    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))\n\n    # calc local currents\n    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape)\n    for k in range(normlized_syn_filter.shape[0]):\n        local_normlized_currents[k] = signal.convolve(padded_input[k], normlized_syn_filter[k], mode='valid')[1:]\n\n    # multiply by weights to get the somatic current\n    soma_current = signal.convolve(local_normlized_currents, synaptic_weights, mode='valid')\n\n    # simulate the cell\n    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()\n    output_spike_times_in_ms = []\n    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)\n    for t in range(len(soma_voltage)):\n        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)\n        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):\n            t_start = t + 1\n            t_end = min(len(soma_voltage), t_start + temporal_filter_length)\n            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]\n            output_spike_times_in_ms.append(t)\n\n    return local_normlized_currents, soma_voltage, output_spike_times_in_ms\n\n\ndef simulate_filter_and_fire_cell_training_long(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,\n                                                refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):\n\n\n    total_duration_ms = presynaptic_input_spikes.shape[1]\n    max_duration_per_call_ms = 40000\n    overlap_time_ms = 500\n\n    if max_duration_per_call_ms >= total_duration_ms:\n        local_normlized_currents, soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_training(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,\n                                                                                                                  refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,\n                                                                                                                  current_to_voltage_mult_factor=current_to_voltage_mult_factor)\n        return local_normlized_currents, soma_voltage, output_spike_times_in_ms\n\n\n    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape, dtype=np.float16)\n    soma_voltage = np.zeros((total_duration_ms,))\n    output_spike_times_in_ms = []\n\n    num_sub_calls = int(np.ceil(total_duration_ms \/ (max_duration_per_call_ms - overlap_time_ms)))\n    end_ind = overlap_time_ms\n    for k in range(num_sub_calls):\n        start_ind = end_ind - overlap_time_ms\n        end_ind = min(start_ind + max_duration_per_call_ms, total_duration_ms)\n\n        curr_loc_norm_c, curr_soma_v, curr_out_sp_t = simulate_filter_and_fire_cell_training(presynaptic_input_spikes[:,start_ind:end_ind], synaptic_weights, tau_rise_vec, tau_decay_vec,\n                                                                                             refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,\n                                                                                             current_to_voltage_mult_factor=current_to_voltage_mult_factor)\n\n        # update fields\n        if k == 0:\n            local_normlized_currents[:,start_ind:end_ind] = curr_loc_norm_c\n            soma_voltage[start_ind:end_ind] = curr_soma_v\n            output_spike_times_in_ms += curr_out_sp_t\n        else:\n            local_normlized_currents[:,(start_ind+overlap_time_ms):end_ind] = curr_loc_norm_c[:,overlap_time_ms:end_ind]\n            soma_voltage[(start_ind+overlap_time_ms):end_ind] = curr_soma_v[overlap_time_ms:]\n            curr_out_sp_t = [x for x in curr_out_sp_t if x >= (overlap_time_ms-1)]\n            output_spike_times_in_ms += [(start_ind + x) for x in curr_out_sp_t]\n\n    return local_normlized_currents, soma_voltage, output_spike_times_in_ms\n\n\ndef simulate_filter_and_fire_cell_inference(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,\n                                            refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):\n\n    temporal_filter_length = int(5 * refreactory_time_constant) + 1\n    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]\n\n    # padd input and get all synaptic filters\n    normlized_syn_filter = construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec)\n    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))\n\n    # calc somatic current\n    weighted_syn_filter  = synaptic_weights * normlized_syn_filter\n    soma_current = signal.convolve(padded_input, weighted_syn_filter, mode='valid')[:,1:]\n\n    # simulate the cell\n    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()\n    output_spike_times_in_ms = []\n    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)\n    for t in range(len(soma_voltage)):\n\n        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)\n        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):\n            t_start = t + 1\n            t_end = min(len(soma_voltage), t_start + temporal_filter_length)\n            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]\n            output_spike_times_in_ms.append(t)\n\n    return soma_voltage, output_spike_times_in_ms\n\n\ndef simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,\n                                                refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):\n\n    total_duration_ms = presynaptic_input_spikes.shape[1]\n    max_duration_per_call_ms = 40000\n    overlap_time_ms = 500\n\n    if max_duration_per_call_ms >= total_duration_ms:\n        soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_inference(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,\n                                                                                         refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,\n                                                                                         current_to_voltage_mult_factor=current_to_voltage_mult_factor)\n        return soma_voltage, output_spike_times_in_ms\n\n\n    soma_voltage = np.zeros((total_duration_ms,))\n    output_spike_times_in_ms = []\n\n    num_sub_calls = int(np.ceil(total_duration_ms \/ (max_duration_per_call_ms - overlap_time_ms)))\n    end_ind = overlap_time_ms\n    for k in range(num_sub_calls):\n        start_ind = end_ind - overlap_time_ms\n        end_ind = min(start_ind + max_duration_per_call_ms, total_duration_ms)\n\n        curr_soma_v, curr_out_sp_t = simulate_filter_and_fire_cell_inference(presynaptic_input_spikes[:,start_ind:end_ind], synaptic_weights, tau_rise_vec, tau_decay_vec,\n                                                                             refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,\n                                                                             current_to_voltage_mult_factor=current_to_voltage_mult_factor)\n\n        # update fields\n        if k == 0:\n            soma_voltage[start_ind:end_ind] = curr_soma_v\n            output_spike_times_in_ms += curr_out_sp_t\n        else:\n            soma_voltage[(start_ind+overlap_time_ms):end_ind] = curr_soma_v[overlap_time_ms:]\n            curr_out_sp_t = [x for x in curr_out_sp_t if x >= (overlap_time_ms-1)]\n            output_spike_times_in_ms += [(start_ind + x) for x in curr_out_sp_t]\n\n    return soma_voltage, output_spike_times_in_ms\n\n\ndef prepare_training_dataset(local_normlized_currents, desired_output_spikes, spike_safety_range_ms=10, negative_subsampling_fraction=0.1):\n\n    # remove all \"negative\" time points that are too close to spikes\n    desired_output_spikes_LPF = signal.convolve(desired_output_spikes, np.ones((spike_safety_range_ms,)), mode='same') > 0.1\n    desired_timepoints = ~desired_output_spikes_LPF\n\n    # massivly subsample the remaining timepoints\n    desired_timepoints[np.random.rand(desired_timepoints.shape[0]) > negative_subsampling_fraction] = 0\n    desired_timepoints[desired_output_spikes > 0.1] = 1\n\n    X = local_normlized_currents.T[desired_timepoints,:]\n    y = desired_output_spikes[desired_timepoints]\n\n    return X, y\n","cb3d7593":"#%% define random input for capacity plot\n\n# generate sample input\naxons_input_spikes_capacity = np.random.rand(num_axons_FF_cap, stimulus_duration_ms) < 0.0016\n\n# F&F\npresynaptic_input_spikes_FF = np.kron(np.ones((connections_per_axon_FF,1)), axons_input_spikes_capacity)\nassert presynaptic_input_spikes_FF.shape[0] == num_synapses_FF, 'number of synapses doesnt match the number of presynaptic inputs'\n\n# I&F\npresynaptic_input_spikes_IF = axons_input_spikes_capacity.copy()\nfor delay_ms in time_delays_list_IF_cap:\n    curr_delay_axons_input_spikes_IF = np.zeros(axons_input_spikes_capacity.shape)\n    curr_delay_axons_input_spikes_IF[:,delay_ms:] = axons_input_spikes_capacity[:,:-delay_ms]\n    presynaptic_input_spikes_IF = np.vstack((presynaptic_input_spikes_IF, curr_delay_axons_input_spikes_IF))\nassert presynaptic_input_spikes_IF.shape[0] == num_synapses_IF, 'number of synapses doesnt match the number of presynaptic inputs'\n\n\n# generate desired pattern of output spikes\ndesired_output_spike_times = min_time_between_spikes_ms * np.random.randint(int(stimulus_duration_ms \/ min_time_between_spikes_ms), size=requested_number_of_output_spikes)\ndesired_output_spike_times = np.sort(np.unique(desired_output_spike_times))\n\ndesired_output_spikes = np.zeros((stimulus_duration_ms,))\ndesired_output_spikes[desired_output_spike_times] = 1.0\n\nprint('number of requested output spikes = %d' %(requested_number_of_output_spikes))","41907574":"#%% fit F&F model to the input\n\n# simulate cell with normlized currents\nlocal_normlized_currents_FF, _, _ = simulate_filter_and_fire_cell_training(presynaptic_input_spikes_FF,\n                                                                           synaptic_weights_vec_FF, tau_rise_vec_FF, tau_decay_vec_FF,\n                                                                           refreactory_time_constant=refreactory_time_constant,\n                                                                           v_reset=v_reset, v_threshold=v_threshold,\n                                                                           current_to_voltage_mult_factor=current_to_voltage_mult_factor)\n\n\n# fit linear model to local currents\nfilter_and_fire_model = linear_model.LogisticRegression(C=30000, fit_intercept=True, penalty='l2', max_iter=3000, solver='liblinear')\n\nspike_safety_range_ms = 1\nnegative_subsampling_fraction = 0.99\n\nX, y = prepare_training_dataset(local_normlized_currents_FF, desired_output_spikes,\n                                spike_safety_range_ms=spike_safety_range_ms,\n                                negative_subsampling_fraction=negative_subsampling_fraction)\n\n# fit model\nfilter_and_fire_model.fit(X,y)\n\nprint('number of data points = %d (%.2f%s positive class)' %(X.shape[0], 100 * y.mean(),'%'))\n\ny_hat = filter_and_fire_model.predict_proba(X)[:,1]\n\n# calculate AUC\ntrain_AUC = roc_auc_score(y, y_hat)\n\nfitted_output_spike_prob_FF = filter_and_fire_model.predict_proba(local_normlized_currents_FF.T)[:,1]\nfull_AUC = roc_auc_score(desired_output_spikes, fitted_output_spike_prob_FF)\n\n# get desired FP threshold\ndesired_false_positive_rate = 0.004\n\nfpr, tpr, thresholds = roc_curve(desired_output_spikes, fitted_output_spike_prob_FF)\n\ndesired_fp_ind = np.argmin(abs(fpr-desired_false_positive_rate))\nif desired_fp_ind == 0:\n    desired_fp_ind = 1\n\nactual_false_positive_rate = fpr[desired_fp_ind]\ntrue_positive_rate         = tpr[desired_fp_ind]\ndesired_fp_threshold       = thresholds[desired_fp_ind]\n\nAUC_score = auc(fpr, tpr)\n\nif AUC_score > 0.9995:\n    desired_fp_threshold = 0.15\n\nprint('F&F fitting AUC = %.4f' %(AUC_score))\nprint('at %.4f FP rate, TP = %.4f' %(actual_false_positive_rate, true_positive_rate))\n\noutput_spikes_after_learning_FF = fitted_output_spike_prob_FF > desired_fp_threshold\n","231bf1ac":"#%% fit I&F to the input with 2 delayed versions of the same input\n\n# simulate cell with normlized currents\nlocal_normlized_currents_IF, _, _ = simulate_filter_and_fire_cell_training(presynaptic_input_spikes_IF,\n                                                                           synaptic_weights_vec_IF, tau_rise_vec_IF, tau_decay_vec_IF,\n                                                                           refreactory_time_constant=refreactory_time_constant,\n                                                                           v_reset=v_reset, v_threshold=v_threshold,\n                                                                           current_to_voltage_mult_factor=current_to_voltage_mult_factor)\n\n\n# fit linear model to local currents\nintegrate_and_fire_model = linear_model.LogisticRegression(C=30000, fit_intercept=True, penalty='l2', max_iter=3000, solver='liblinear')\n\nspike_safety_range_ms = 1\nnegative_subsampling_fraction = 0.99\n\nX, y = prepare_training_dataset(local_normlized_currents_IF, desired_output_spikes,\n                                spike_safety_range_ms=spike_safety_range_ms,\n                                negative_subsampling_fraction=negative_subsampling_fraction)\n\n# fit model\nintegrate_and_fire_model.fit(X,y)\n\nprint('number of data points = %d (%.2f%s positive class)' %(X.shape[0], 100 * y.mean(),'%'))\n\ny_hat = integrate_and_fire_model.predict_proba(X)[:,1]\n\n# calculate AUC\ntrain_AUC = roc_auc_score(y, y_hat)\n\nfitted_output_spike_prob_IF = integrate_and_fire_model.predict_proba(local_normlized_currents_IF.T)[:,1]\nfull_AUC = roc_auc_score(desired_output_spikes, fitted_output_spike_prob_IF)\n\n# get desired FP threshold\ndesired_false_positive_rate = 0.004\n\nfpr, tpr, thresholds = roc_curve(desired_output_spikes, fitted_output_spike_prob_IF)\n\ndesired_fp_ind = np.argmin(abs(fpr - desired_false_positive_rate))\nif desired_fp_ind == 0:\n    desired_fp_ind = 1\n\nactual_false_positive_rate = fpr[desired_fp_ind]\ntrue_positive_rate         = tpr[desired_fp_ind]\ndesired_fp_threshold       = thresholds[desired_fp_ind]\n\nAUC_score = auc(fpr, tpr)\n\nif AUC_score > 0.9995:\n    desired_fp_threshold = 0.15\n\nprint('I&F fitting AUC = %.4f' %(AUC_score))\nprint('at %.4f FP rate, TP = %.4f' %(actual_false_positive_rate, true_positive_rate))\n\noutput_spikes_after_learning_IF = fitted_output_spike_prob_IF > desired_fp_threshold\n","c3527138":"#%% MNIST params\n\nspatial_extent_factor = 5\ntemporal_extent_factor_numerator = 2\ntemporal_extent_factor_denumerator = 1\n\nnum_const_firing_channels = 20\ntemporal_silence_ms = 70\n\npositive_digit = 7\nnum_train_positive_patterns = 1000 # reduced due to memory issues on kaggle notebooks\n\nrelease_probability = 1.0\napply_release_prob_during_train = False\napply_releash_prob_during_test = False\n\noutput_spike_tolorance_window_duration = 20\noutput_spike_tolorance_window_offset   = 5\n\ntime_delays_list_IF = [12,24]\nnum_axons_FF = spatial_extent_factor * 20 + num_const_firing_channels\nnum_time_delays_IF = len(time_delays_list_IF) + 1\nnum_axons_IF = num_time_delays_IF * num_axons_FF\n\nnum_synapses_FF = connections_per_axon_FF * num_axons_FF\nconnections_per_axon_IF = 1\nnum_synapses_IF = connections_per_axon_IF * num_axons_IF\n\n# synapse non-learnable parameters\ntau_rise_range_FF  = [1,16]\ntau_decay_range_FF = [8,24]\n\ntau_rise_vec_FF  = np.random.uniform(low=tau_rise_range_FF[0] , high=tau_rise_range_FF[1] , size=(num_synapses_FF, 1))\ntau_decay_vec_FF = np.random.uniform(low=tau_decay_range_FF[0], high=tau_decay_range_FF[1], size=(num_synapses_FF, 1))\n\n# synapse learnable parameters\nsynaptic_weights_vec_FF = np.random.normal(size=(num_synapses_FF, 1))\n\n# I&F neuron model parameters\nconnections_per_axon_IF = 1\nnum_synapses_IF = connections_per_axon_IF * num_axons_IF\n\n# synapse non-learnable parameters\ntau_rise_range_IF  = [1,1]\ntau_decay_range_IF = [24,24]\n\ntau_rise_vec_IF  = np.random.uniform(low=tau_rise_range_IF[0] , high=tau_rise_range_IF[1] , size=(num_synapses_IF, 1))\ntau_decay_vec_IF = np.random.uniform(low=tau_decay_range_IF[0], high=tau_decay_range_IF[1], size=(num_synapses_IF, 1))\n\n# synapse learnable parameters\nsynaptic_weights_vec_IF = np.random.normal(size=(num_synapses_IF, 1))\n","508304a8":"#%% load MNIST dataset and and transform into spikes\n\n(x_train_original, y_train), (x_test_original, y_test) = keras.datasets.mnist.load_data()\n\n# crop the data and binarize it\nh_crop_range = [4,24]\nw_crop_range = [4,24]\n\npositive_threshold = 150\n\nx_train_original = x_train_original[:,h_crop_range[0]:h_crop_range[1],w_crop_range[0]:w_crop_range[1]] > positive_threshold\nx_test_original  = x_test_original[: ,h_crop_range[0]:h_crop_range[1],w_crop_range[0]:w_crop_range[1]] > positive_threshold\n\n#%% Transform Xs to spatio-temporal spike trains\n\n# extend according to \"temporal_extent_factor\"\nkernel = np.ones((1, spatial_extent_factor, temporal_extent_factor_numerator), dtype=bool)\n\nx_train = x_train_original.copy()\nx_test  = x_test_original.copy()\n\n# reshape X according to what is needed\nx_train = np.kron(x_train, kernel)\nx_test = np.kron(x_test, kernel)\n\n# subsample according to \"temporal_extent_factor_denumerator\"\nx_train = x_train[:,:,::temporal_extent_factor_denumerator]\nx_test = x_test[:,:,::temporal_extent_factor_denumerator]\n\n# padd with ones on top (for \"bias\" learning)\ntop_pad_train = np.ones((1, num_const_firing_channels, x_train.shape[2]), dtype=bool)\ntop_pad_test  = np.ones((1, num_const_firing_channels, x_test.shape[2] ), dtype=bool)\n\n# add a few zero rows for clear seperation for visualization purpuses\ntop_pad_train[:,-5:,:] = 0\ntop_pad_test[:,-5:,:] = 0\n\nx_train = np.concatenate((np.tile(top_pad_train, [x_train.shape[0],1,1]), x_train), axis=1)\nx_test  = np.concatenate((np.tile(top_pad_test , [x_test.shape[0],1,1] ), x_test ), axis=1)\n\n# pad with \"temporal_silence_ms\" zeros in the begining of each pattern (for silence between patterns)\nleft_pad_train = np.zeros((1, x_train.shape[1], temporal_silence_ms), dtype=bool)\nleft_pad_test  = np.zeros((1, x_test.shape[1] , temporal_silence_ms), dtype=bool)\n\nx_train = np.concatenate((np.tile(left_pad_train, [x_train.shape[0],1,1]), x_train), axis=2)\nx_test  = np.concatenate((np.tile(left_pad_test , [x_test.shape[0],1,1] ), x_test ), axis=2)\n\n# add background activity\ndesired_background_activity_firing_rate_Hz = 10\nbackground_activity_fraction = desired_background_activity_firing_rate_Hz \/ 1000\n\nx_train[np.random.rand(x_train.shape[0], x_train.shape[1], x_train.shape[2]) < background_activity_fraction] = 1\nx_test[ np.random.rand(x_test.shape[0] , x_test.shape[1] , x_test.shape[2] ) < background_activity_fraction] = 1\n\n# subsample the input spikes\ndesired_average_input_firing_rate_Hz = 20\nactual_mean_firing_rate_Hz = 1000 * x_train.mean()\n\nfraction_of_spikes_to_eliminate = desired_average_input_firing_rate_Hz \/ actual_mean_firing_rate_Hz\n\nx_train = x_train * (np.random.rand(x_train.shape[0], x_train.shape[1], x_train.shape[2]) < fraction_of_spikes_to_eliminate)\nx_test  = x_test  * (np.random.rand(x_test.shape[0], x_test.shape[1], x_test.shape[2]) < fraction_of_spikes_to_eliminate)\n\nfinal_mean_firing_rate_Hz = 1000 * x_train.mean()\n\n#%% Create \"one-vs-all\" dataset\n\ny_train_binary = y_train == positive_digit\ny_test_binary  = y_test  == positive_digit\n\nnum_train_positive_patterns = min(int(y_train_binary.sum()), num_train_positive_patterns)\n\nnum_train_negative_patterns = int(2.0 * num_train_positive_patterns)\n\npositive_inds = np.where(y_train_binary)[0]\nnegative_inds = np.where(~y_train_binary)[0]\n\nselected_train_positives = np.random.choice(positive_inds, size=num_train_positive_patterns)\nselected_train_negatives = np.random.choice(negative_inds, size=num_train_negative_patterns)\n\nall_selected = np.random.permutation(np.concatenate((selected_train_positives, selected_train_negatives)))\n\nX_train_spikes = x_train[all_selected]\nY_train_spikes = y_train_binary[all_selected]\n\nX_test_spikes = x_test.copy()\nY_test_spikes = y_test_binary.copy()\n\nzero_pred_baseline_accuracy = 100 * (1 - Y_test_spikes.mean())\n","18b64efe":"#%% prepare input spikes for F&F and I&F training\n\naxons_input_spikes = np.concatenate([X_train_spikes[k] for k in range(X_train_spikes.shape[0])],axis=1)\n\n# prepare output spikes\npattern_duration_ms = X_train_spikes[0].shape[1]\noutput_spike_offset = 1\noutput_kernel = np.zeros((pattern_duration_ms,))\noutput_kernel[-output_spike_offset] = 1\n\ndesired_output_spikes_mnist = np.kron(Y_train_spikes, output_kernel)\n\n# F&F\npresynaptic_input_spikes_FF = np.kron(np.ones((connections_per_axon_FF,1)), axons_input_spikes)\nassert presynaptic_input_spikes_FF.shape[0] == num_synapses_FF, 'number of synapses doesnt match the number of presynaptic inputs'\n\n# I&F\npresynaptic_input_spikes_IF = axons_input_spikes.copy()\nfor delay_ms in time_delays_list_IF:\n    curr_delay_axons_input_spikes_IF = np.zeros(axons_input_spikes.shape)\n    curr_delay_axons_input_spikes_IF[:,delay_ms:] = axons_input_spikes[:,:-delay_ms]\n    presynaptic_input_spikes_IF = np.vstack((presynaptic_input_spikes_IF, curr_delay_axons_input_spikes_IF))\nassert presynaptic_input_spikes_IF.shape[0] == num_synapses_IF, 'number of synapses doesnt match the number of presynaptic inputs'\n","435f8f05":"#%% prepare input spikes for F&F and I&F testing\n\nnum_test_patterns = X_test_spikes.shape[0]\nnum_test_patterns = 1000 # reduced due to memory issues on kaggle notebooks\n\n# prepare test outputs\noutput_kernel_test = np.zeros((X_test_spikes[0].shape[1],))\noutput_kernel_test[-output_spike_tolorance_window_duration:] = 1\n\ndesired_output_spikes_test = np.kron(Y_test_spikes[:num_test_patterns], output_kernel_test)\ndesired_output_spikes_test = np.concatenate((np.zeros((output_spike_tolorance_window_offset,)), desired_output_spikes_test[:-output_spike_tolorance_window_offset]))\n\n# prepare test inputs\naxons_input_spikes_test = np.concatenate([X_test_spikes[k] for k in range(num_test_patterns)],axis=1)\n\n# F&F\npresynaptic_input_spikes_test_FF = np.kron(np.ones((connections_per_axon_FF,1)), axons_input_spikes_test)\nassert presynaptic_input_spikes_test_FF.shape[0] == num_synapses_FF, 'number of synapses doesnt match the number of presynaptic inputs'\n\n# I&F\npresynaptic_input_spikes_test_IF = axons_input_spikes_test.copy()\nfor delay_ms in time_delays_list_IF:\n    curr_delay_axons_input_spikes_IF = np.zeros(axons_input_spikes_test.shape)\n    curr_delay_axons_input_spikes_IF[:,delay_ms:] = axons_input_spikes_test[:,:-delay_ms]\n    presynaptic_input_spikes_test_IF = np.vstack((presynaptic_input_spikes_test_IF, curr_delay_axons_input_spikes_IF))\nassert presynaptic_input_spikes_test_IF.shape[0] == num_synapses_IF, 'number of synapses doesnt match the number of presynaptic inputs'\n","5227af51":"#%% simulate F&F cell with normlized currents on train\n\nlocal_normlized_currents_FF, _, _ = simulate_filter_and_fire_cell_training_long(presynaptic_input_spikes_FF,\n                                                                                synaptic_weights_vec_FF, tau_rise_vec_FF, tau_decay_vec_FF,\n                                                                                refreactory_time_constant=refreactory_time_constant,\n                                                                                v_reset=v_reset, v_threshold=v_threshold,\n                                                                                current_to_voltage_mult_factor=current_to_voltage_mult_factor)\n\n#%% fit linear model to local currents\n\nfilter_and_fire_model = linear_model.LogisticRegression(C=10000, fit_intercept=False, penalty='l2', solver='liblinear')\n\nspike_safety_range_ms = 20\nnegative_subsampling_fraction = 0.1 # reduced due to memory issues\n\nX, y = prepare_training_dataset(local_normlized_currents_FF, desired_output_spikes_mnist,\n                                spike_safety_range_ms=spike_safety_range_ms,\n                                negative_subsampling_fraction=negative_subsampling_fraction)\n\n# fit model\nfilter_and_fire_model.fit(X, y)\n\nprint('number of data points = %d (%.2f%s positive class)' %(X.shape[0], 100 * y.mean(),'%'))\n\n# calculate train AUC\ny_hat = filter_and_fire_model.predict_proba(X)[:,1]\ntrain_AUC = roc_auc_score(y, y_hat)\n\nFF_learned_synaptic_weights = np.fliplr(filter_and_fire_model.coef_).T\n\n#%% find the best multiplicative factor for test prediction F&F\n\n# FF_weight_mult_factors_list = [1,2,3,4,5,6,9,12,20,50,120,250]\nFF_weight_mult_factors_list = [0.1,1,3.16,10,100]\nFF_accuracy_list = []\nFF_true_positive_list = []\nFF_false_positive_list = []\nfor weight_mult_factor in FF_weight_mult_factors_list:\n\n    # collect learned synaptic weights\n    synaptic_weights_post_learning = weight_mult_factor * FF_learned_synaptic_weights\n\n    soma_voltage_test, output_spike_times_in_ms_test = simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes_test_FF,\n                                                                                                    synaptic_weights_post_learning, tau_rise_vec_FF, tau_decay_vec_FF,\n                                                                                                    refreactory_time_constant=refreactory_time_constant,\n                                                                                                    v_reset=v_reset, v_threshold=v_threshold,\n                                                                                                    current_to_voltage_mult_factor=current_to_voltage_mult_factor)\n\n\n    output_spikes_test = np.zeros(soma_voltage_test.shape)\n    try:\n        output_spikes_test[np.array(output_spike_times_in_ms_test)] = 1.0\n    except:\n        print('no output spikes created')\n\n\n    # calculate test accuracy\n    compact_desired_output_test = Y_test_spikes[:num_test_patterns]\n\n    compact_desired_output_test2 = np.zeros(compact_desired_output_test.shape, dtype=bool)\n    compact_predicted_output_test = np.zeros(compact_desired_output_test.shape, dtype=bool)\n\n    # go over all patterns and extract the prediction (depends if the spikes are in the desired window)\n    for pattern_ind in range(num_test_patterns):\n        start_ind = pattern_duration_ms * pattern_ind + output_spike_tolorance_window_offset\n        end_ind = start_ind + pattern_duration_ms\n\n        # extract prediction\n        predicted_spike_train_for_pattern = output_spikes_test[start_ind:end_ind]\n        desired_spike_train_for_pattern = desired_output_spikes_test[start_ind:end_ind]\n\n        compact_desired_output_test2[pattern_ind]  = desired_spike_train_for_pattern.sum() > 0.1\n\n        if Y_test_spikes[pattern_ind] == 1:\n            # check if there is a spike in the desired window only\n            compact_predicted_output_test[pattern_ind] = (desired_spike_train_for_pattern * predicted_spike_train_for_pattern).sum() > 0.1\n        else:\n            # check if there is any spike in the full pattern duration\n            compact_predicted_output_test[pattern_ind] = predicted_spike_train_for_pattern.sum() > 0.1\n\n    # small verificaiton\n    assert((compact_desired_output_test == compact_desired_output_test2).sum() == num_test_patterns)\n\n    # display accuracy\n    percent_accuracy = 100 * (compact_desired_output_test == compact_predicted_output_test).mean()\n    true_positive    = 100 * (np.logical_and(compact_desired_output_test == True , compact_predicted_output_test == True).sum() \/ (compact_desired_output_test == True).sum())\n    false_positive   = 100 * (np.logical_and(compact_desired_output_test == False, compact_predicted_output_test == True).sum() \/ (compact_desired_output_test == False).sum())\n\n    print('weights mult factor = %.1f: Accuracy = %.3f%s. (TP, FP) = (%.3f%s, %.3f%s)' %(weight_mult_factor, percent_accuracy,'%',true_positive,'%',false_positive,'%'))\n\n    FF_accuracy_list.append(percent_accuracy)\n    FF_true_positive_list.append(true_positive)\n    FF_false_positive_list.append(false_positive)\n\n#%% make a final prediction on the test set F&F\n\n# get the max accuracy weight matrix\nmax_accuracy_weight_mult_factor = FF_weight_mult_factors_list[np.argsort(np.array(FF_accuracy_list))[-1]]\n\nsynaptic_weights_vec_after_learning_FF = max_accuracy_weight_mult_factor * FF_learned_synaptic_weights\n\n# simulate the max accuracy output after learning\nsoma_voltage_test, output_spike_times_in_ms_test_FF = simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes_test_FF,\n                                                                                                   synaptic_weights_vec_after_learning_FF, tau_rise_vec_FF, tau_decay_vec_FF,\n                                                                                                   refreactory_time_constant=refreactory_time_constant,\n                                                                                                   v_reset=v_reset, v_threshold=v_threshold,\n                                                                                                   current_to_voltage_mult_factor=current_to_voltage_mult_factor)\n\n\noutput_spikes_test_after_learning_full_FF = np.zeros(soma_voltage_test.shape)\ntry:\n    output_spikes_test_after_learning_full_FF[np.array(output_spike_times_in_ms_test)] = 1.0\nexcept:\n    print('no output spikes created')\n","23120290":"#%% simulate I&F cell with normlized currents on train\n\nlocal_normlized_currents_IF, _, _ = simulate_filter_and_fire_cell_training_long(presynaptic_input_spikes_IF,\n                                                                                synaptic_weights_vec_IF, tau_rise_vec_IF, tau_decay_vec_IF,\n                                                                                refreactory_time_constant=refreactory_time_constant,\n                                                                                v_reset=v_reset, v_threshold=v_threshold,\n                                                                                current_to_voltage_mult_factor=current_to_voltage_mult_factor)\n\n#%% fit linear model to local currents\n\nintegrate_and_fire_model = linear_model.LogisticRegression(C=10000, fit_intercept=False, penalty='l2', solver='liblinear')\n\nspike_safety_range_ms = 20\nnegative_subsampling_fraction = 0.1 # reduced due to memory issues\n\nX, y = prepare_training_dataset(local_normlized_currents_IF, desired_output_spikes_mnist,\n                                spike_safety_range_ms=spike_safety_range_ms,\n                                negative_subsampling_fraction=negative_subsampling_fraction)\n\n# fit model\nintegrate_and_fire_model.fit(X, y)\n\nprint('number of data points = %d (%.2f%s positive class)' %(X.shape[0], 100 * y.mean(),'%'))\n\n# calculate train AUC\ny_hat = integrate_and_fire_model.predict_proba(X)[:,1]\ntrain_AUC = roc_auc_score(y, y_hat)\n\nIF_learned_synaptic_weights = np.fliplr(integrate_and_fire_model.coef_).T\n\n#%% find the best multiplicative factor for test prediction I&F\n\n# FF_weight_mult_factors_list = [1,2,3,4,5,6,9,12,20,50,120,250]\nIF_weight_mult_factors_list = [0.1,1,3.16,10,100]\nIF_accuracy_list = []\nIF_true_positive_list = []\nIF_false_positive_list = []\nfor weight_mult_factor in IF_weight_mult_factors_list:\n\n    # collect learned synaptic weights\n    synaptic_weights_post_learning = weight_mult_factor * IF_learned_synaptic_weights\n\n    soma_voltage_test, output_spike_times_in_ms_test = simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes_test_IF,\n                                                                                                    synaptic_weights_post_learning, tau_rise_vec_IF, tau_decay_vec_IF,\n                                                                                                    refreactory_time_constant=refreactory_time_constant,\n                                                                                                    v_reset=v_reset, v_threshold=v_threshold,\n                                                                                                    current_to_voltage_mult_factor=current_to_voltage_mult_factor)\n\n\n    output_spikes_test = np.zeros(soma_voltage_test.shape)\n    try:\n        output_spikes_test[np.array(output_spike_times_in_ms_test)] = 1.0\n    except:\n        print('no output spikes created')\n\n\n    # calculate test accuracy\n    compact_desired_output_test = Y_test_spikes[:num_test_patterns]\n\n    compact_desired_output_test2 = np.zeros(compact_desired_output_test.shape, dtype=bool)\n    compact_predicted_output_test = np.zeros(compact_desired_output_test.shape, dtype=bool)\n\n    # go over all patterns and extract the prediction (depends if the spikes are in the desired window)\n    for pattern_ind in range(num_test_patterns):\n        start_ind = pattern_duration_ms * pattern_ind + output_spike_tolorance_window_offset\n        end_ind = start_ind + pattern_duration_ms\n\n        # extract prediction\n        predicted_spike_train_for_pattern = output_spikes_test[start_ind:end_ind]\n        desired_spike_train_for_pattern = desired_output_spikes_test[start_ind:end_ind]\n\n        compact_desired_output_test2[pattern_ind]  = desired_spike_train_for_pattern.sum() > 0.1\n\n        if Y_test_spikes[pattern_ind] == 1:\n            # check if there is a spike in the desired window only\n            compact_predicted_output_test[pattern_ind] = (desired_spike_train_for_pattern * predicted_spike_train_for_pattern).sum() > 0.1\n        else:\n            # check if there is any spike in the full pattern duration\n            compact_predicted_output_test[pattern_ind] = predicted_spike_train_for_pattern.sum() > 0.1\n\n    # small verificaiton\n    assert((compact_desired_output_test == compact_desired_output_test2).sum() == num_test_patterns)\n\n    # display accuracy\n    percent_accuracy = 100 * (compact_desired_output_test == compact_predicted_output_test).mean()\n    true_positive    = 100 * (np.logical_and(compact_desired_output_test == True , compact_predicted_output_test == True).sum() \/ (compact_desired_output_test == True).sum())\n    false_positive   = 100 * (np.logical_and(compact_desired_output_test == False, compact_predicted_output_test == True).sum() \/ (compact_desired_output_test == False).sum())\n\n    print('weights mult factor = %.1f: Accuracy = %.3f%s. (TP, FP) = (%.3f%s, %.3f%s)' %(weight_mult_factor, percent_accuracy,'%',true_positive,'%',false_positive,'%'))\n\n    IF_accuracy_list.append(percent_accuracy)\n    IF_true_positive_list.append(true_positive)\n    IF_false_positive_list.append(false_positive)\n\n#%% make a final prediction on the test set I&F\n\n# get the max accuracy weight matrix\nmax_accuracy_weight_mult_factor = IF_weight_mult_factors_list[np.argsort(np.array(IF_accuracy_list))[-1]]\n\nsynaptic_weights_vec_after_learning_IF = max_accuracy_weight_mult_factor * IF_learned_synaptic_weights\n\n# simulate the max accuracy output after learning\nsoma_voltage_test, output_spike_times_in_ms_test_IF = simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes_test_IF,\n                                                                                                   synaptic_weights_vec_after_learning_IF, tau_rise_vec_IF, tau_decay_vec_IF,\n                                                                                                   refreactory_time_constant=refreactory_time_constant,\n                                                                                                   v_reset=v_reset, v_threshold=v_threshold,\n                                                                                                   current_to_voltage_mult_factor=current_to_voltage_mult_factor)\n\n\noutput_spikes_test_after_learning_full_IF = np.zeros(soma_voltage_test.shape)\ntry:\n    output_spikes_test_after_learning_full_IF[np.array(output_spike_times_in_ms_test)] = 1.0\nexcept:\n    print('no output spikes created')\n","1f53ec9b":"#%% Build the figure\n\nplt.close('all')\nfig = plt.figure(figsize=(12,22))\ngs_figure = gridspec.GridSpec(nrows=17,ncols=1)\ngs_figure.update(left=0.04, right=0.95, bottom=0.02, top=0.98, wspace=0.45, hspace=0.4)\n\nax_axons          = plt.subplot(gs_figure[:5,:])\nax_FF_spikes      = plt.subplot(gs_figure[5,:])\nax_IF_spikes      = plt.subplot(gs_figure[6,:])\nax_desired_output = plt.subplot(gs_figure[7,:])\n\nax_digits            = plt.subplot(gs_figure[9:11,:])\nax_axons_mnist       = plt.subplot(gs_figure[11:15,:])\nax_learning_outcomes = plt.subplot(gs_figure[15:,:])\n\nFF_color = 'orange'\nIF_color = '0.05'\ntarget_color = 'red'\n\nsyn_activation_time, syn_activation_index = np.nonzero(presynaptic_input_spikes_IF.T)\n\nsyn_activation_time_1_cap, syn_activation_index_1_cap = np.nonzero(axons_input_spikes_capacity.T)\nsyn_activation_time_1_cap = syn_activation_time_1_cap \/ 1000\n\nsyn_activation_time_2_cap = syn_activation_time_1_cap + (time_delays_list_IF_cap[0] \/ 1000)\nsyn_activation_time_3_cap = syn_activation_time_1_cap + (time_delays_list_IF_cap[1] \/ 1000)\nsyn_activation_index_2_cap = syn_activation_index_1_cap + num_axons_FF_cap\nsyn_activation_index_3_cap = syn_activation_index_2_cap + num_axons_FF_cap\n\nmin_time_sec = -0.1\nmax_time_sec = stimulus_duration_ms \/ 1000\ntime_sec = np.linspace(0, max_time_sec, output_spikes_after_learning_FF.shape[0])\n\nax_axons.scatter(syn_activation_time_1_cap, syn_activation_index_1_cap, s=8, c='black');\nax_axons.scatter(syn_activation_time_2_cap, syn_activation_index_2_cap, s=8, c='chocolate');\nax_axons.scatter(syn_activation_time_3_cap, syn_activation_index_3_cap, s=8, c='purple');\nax_axons.set_xlim(min_time_sec, max_time_sec);\nax_axons.set_xticks([])\nax_axons.set_yticks([])\nax_axons.spines['top'].set_visible(False)\nax_axons.spines['bottom'].set_visible(False)\nax_axons.spines['left'].set_visible(False)\nax_axons.spines['right'].set_visible(False)\n\nax_FF_spikes.plot(time_sec, output_spikes_after_learning_FF, c=FF_color, lw=2.5);\nax_FF_spikes.set_title('F&F (M = 5)', fontsize=17, color=FF_color)\nax_FF_spikes.set_xlim(min_time_sec, max_time_sec);\nax_FF_spikes.set_xticks([])\nax_FF_spikes.set_yticks([])\nax_FF_spikes.spines['top'].set_visible(False)\nax_FF_spikes.spines['bottom'].set_visible(False)\nax_FF_spikes.spines['left'].set_visible(False)\nax_FF_spikes.spines['right'].set_visible(False)\n\nax_IF_spikes.plot(time_sec, output_spikes_after_learning_IF, c=IF_color, lw=2.5);\nax_IF_spikes.set_title('I&F (Orig Axons + 2 delayed)', fontsize=17, color=IF_color)\nax_IF_spikes.set_xlim(min_time_sec, max_time_sec);\nax_IF_spikes.set_xticks([])\nax_IF_spikes.set_yticks([])\nax_IF_spikes.spines['top'].set_visible(False)\nax_IF_spikes.spines['bottom'].set_visible(False)\nax_IF_spikes.spines['left'].set_visible(False)\nax_IF_spikes.spines['right'].set_visible(False)\n\nax_desired_output.plot(time_sec, desired_output_spikes, c=target_color, lw=2.5);\nax_desired_output.set_title('Desired Output (num spikes = %d)' %(requested_number_of_output_spikes), fontsize=17, color=target_color);\nax_desired_output.set_xlim(min_time_sec, max_time_sec);\nax_desired_output.set_xticks([]);\nax_desired_output.set_yticks([]);\nax_desired_output.spines['top'].set_visible(False)\nax_desired_output.spines['bottom'].set_visible(False)\nax_desired_output.spines['left'].set_visible(False)\nax_desired_output.spines['right'].set_visible(False)\n\n\n\n##%% organize MNIST part into a figure\n\n# test digit images\nextention_kernel = np.ones((1, spatial_extent_factor, temporal_extent_factor_numerator), dtype=bool)\nx_test_original_extended = np.kron(x_test_original, extention_kernel)\nleft_pad_test  = np.zeros((1, x_test_original_extended.shape[1] , temporal_silence_ms), dtype=bool)\nx_test_original_extended  = np.concatenate((np.tile(left_pad_test , [x_test_original_extended.shape[0],1,1] ), x_test_original_extended ), axis=2)\nx_test_axons_input_spikes = np.concatenate([x_test_original_extended[k] for k in range(x_test_original_extended.shape[0])],axis=1)\n\ntest_set_full_duration_ms = x_test_axons_input_spikes.shape[1]\n\n# select a subset of time to display\nnum_digits_to_display = 9\nstart_time = x_test_original_extended.shape[2] * np.random.randint(int(test_set_full_duration_ms \/ x_test_original_extended.shape[2] - temporal_silence_ms))\nend_time = start_time + 1 + x_test_original_extended.shape[2] * num_digits_to_display + temporal_silence_ms\n\n# make sure we have something decent to show (randomise start and end times untill we do)\noutput_spikes_test_after_learning = output_spikes_test_after_learning_full_FF[start_time:end_time]\nwhile output_spikes_test_after_learning.sum() < 2:\n    start_time = x_test_original_extended.shape[2] * np.random.randint(int(test_set_full_duration_ms \/ x_test_original_extended.shape[2] - temporal_silence_ms))\n    end_time = start_time + 1 + x_test_original_extended.shape[2] * num_digits_to_display + temporal_silence_ms\n    output_spikes_test_after_learning = output_spikes_test_after_learning_full_FF[start_time:end_time]\n\nmin_time_ms = 0\nmax_time_ms = end_time - start_time\n\ntime_sec_mnist = np.arange(min_time_ms, max_time_ms) \/ 1000\nmin_time_sec_mnist = min_time_ms \/ 1000\nmax_time_sec_mnist = max_time_ms \/ 1000\n\n# input digits\nx_test_input_digits = x_test_axons_input_spikes[:,start_time:end_time]\n\nsyn_activation_time_1, syn_activation_index_1 = np.nonzero(axons_input_spikes_test[-x_test_axons_input_spikes.shape[0]:,start_time:end_time].T)\nsyn_activation_time_1 = syn_activation_time_1 \/ 1000\nsyn_activation_index_1 = x_test_axons_input_spikes.shape[0] - syn_activation_index_1\n\nsyn_activation_time_2 = syn_activation_time_1 + (time_delays_list_IF[0] \/ 1000)\nsyn_activation_time_3 = syn_activation_time_1 + (time_delays_list_IF[1] \/ 1000)\nsyn_activation_index_2 = syn_activation_index_1 + num_axons_FF\nsyn_activation_index_3 = syn_activation_index_2 + num_axons_FF\n\n\n# output after learning\noutput_spikes_test_after_learning_FF = output_spikes_test_after_learning_full_FF[start_time:end_time]\noutput_spikes_test_after_learning_IF = output_spikes_test_after_learning_full_IF[start_time:end_time]\n\n# desired output\noutput_kernel_test = np.zeros((X_test_spikes[0].shape[1],))\noutput_kernel_test[-1] = 1\ndesired_output_spikes_test_full = np.kron(Y_test_spikes[:num_test_patterns], output_kernel_test)\ndesired_output_spikes_test_mnist = desired_output_spikes_test_full[start_time:end_time]\n\n\n# build figure\nax_digits.imshow(x_test_input_digits, cmap='gray');\nax_digits.set_xticks([])\nax_digits.set_yticks([])\nax_digits.spines['top'].set_visible(False)\nax_digits.spines['bottom'].set_visible(False)\nax_digits.spines['left'].set_visible(False)\nax_digits.spines['right'].set_visible(False)\n\nax_axons_mnist.scatter(syn_activation_time_1, syn_activation_index_1, s=8, c='black');\nax_axons_mnist.scatter(syn_activation_time_2, syn_activation_index_2, s=8, c='chocolate');\nax_axons_mnist.scatter(syn_activation_time_3, syn_activation_index_3, s=8, c='purple');\nax_axons_mnist.set_xlim(min_time_sec_mnist, max_time_sec_mnist);\nax_axons_mnist.set_xticks([])\nax_axons_mnist.set_yticks([])\nax_axons_mnist.spines['top'].set_visible(False)\nax_axons_mnist.spines['bottom'].set_visible(False)\nax_axons_mnist.spines['left'].set_visible(False)\nax_axons_mnist.spines['right'].set_visible(False)\n\n\nax_learning_outcomes.plot(time_sec_mnist, 2.2 + output_spikes_test_after_learning_FF, c=FF_color, lw=2.5);\nax_learning_outcomes.plot(time_sec_mnist, 1.1 + output_spikes_test_after_learning_IF, c=IF_color, lw=2.5);\nax_learning_outcomes.plot(time_sec_mnist, 0.0 + desired_output_spikes_test_mnist, c=target_color, lw=2.5);\n\nax_learning_outcomes.set_xlim(min_time_sec_mnist, max_time_sec_mnist);\nax_learning_outcomes.set_xticks([])\nax_learning_outcomes.set_yticks([])\nax_learning_outcomes.spines['top'].set_visible(False)\nax_learning_outcomes.spines['bottom'].set_visible(False)\nax_learning_outcomes.spines['left'].set_visible(False)\nax_learning_outcomes.spines['right'].set_visible(False)\n\nax_learning_outcomes.text(0.02,2.5, 'F&F (M=5)', color=FF_color, fontsize=20)\nax_learning_outcomes.text(0.02,1.4, 'I&F (Orig Axons + 2 delayed)', color=IF_color, fontsize=20)\nax_learning_outcomes.text(0.02,0.3, 'Desired Output', color=target_color, fontsize=20)","629fd30b":"save_figures = True\n\n# save figure\nif save_figures:\n    figure_name = 'F&F_Axon_reduction_Figure_5_%d' %(np.random.randint(200))\n    for file_ending in all_file_endings_to_use:\n        if file_ending == '.png':\n            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')\n        else:\n            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')\n","2a2f59e0":"## Prepare input spikes for F&F and I&F training","12237db2":"## Helper functions","bb9fc229":"## Load MNIST dataset and and transform into spikes","12199909":"## Define random input for capacity plot","4cb1b962":"## Script params","ea85687b":"## MNIST part params","d4d09ad4":"## Prepare input spikes for F&F and I&F testing","a91e17fd":"## Create the final figure","facce49e":"## Simulate I&F cell with it's normlized currents on train and make a prediction on test","06bb360a":"## Fit I&F to the input with 2 delayed versions of the same input","ec4edbce":"## Fit F&F model to the input","5fd71989":"## Simulate F&F cell with normlized currents on train and make a prediction on test"}}