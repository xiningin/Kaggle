{"cell_type":{"2d32199c":"code","39d25bb9":"code","eb41ff12":"code","f59f0e30":"code","08c0c610":"code","8ce32226":"code","352fbcff":"code","669ee371":"code","c81c2637":"code","a629c51c":"code","a0973dd5":"code","136bef4e":"markdown","3649d6fa":"markdown"},"source":{"2d32199c":"!pip install gdown\n!pip install torch==1.0.1","39d25bb9":"!pwd\n!gdown https:\/\/drive.google.com\/uc?id=1WzrREEwOtOqBjcDWlk5PSl4D27jayqm0\n!gdown https:\/\/drive.google.com\/uc?id=15nxPJ900Kf2mRKDcCuK-S13YdRYcdNb3","eb41ff12":"import os\n\nos.chdir(\"\/kaggle\/working\/\")\nif not os.path.isdir(\"checkpoint\"):\n    !mkdir checkpoint\nif not os.path.isdir(\"out_sample\"):\n    !mkdir out_sample\n!cp -rf \"..\/input\/10k-motorbike\/utils\/utils\/utils.py\" \"..\/working\/\"\n!cp -rf \"..\/input\/10k-motorbike\/utils\/utils\/torch_utils.py\" \"..\/working\/\"\n# copy our file into the working directory (make sure it has .py suffix)\n# copyfile(src = \"..\/input\/my_functions.py\", dst = \"..\/working\/my_functions.py\")\ndataset_path = \"..\/input\/10k-motorbike\/10k_mortorbike_dataset\/motobike\"\nutils_path =  \".\/utils\/utils.py\"\ncheckpoint_path = '.\/checkpoint'\nrestore_path = '.\/'\nout_path = \".\/out_sample\"\n\n","f59f0e30":"# !ls ..\/input\/10k-motorbike\n!ls","08c0c610":"# import sys\n# sys.path.append(\".\/utils\")\nimport glob\nimport os\nimport warnings\n\nimport torch.nn as nn\nfrom PIL import Image\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import LambdaLR\n\n# from utils.torch_utils import *\n# from utils.utils import *\nimport random\nfrom utils import *\nfrom torch_utils import *","8ce32226":"warnings.filterwarnings('ignore', category=FutureWarning)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nconfig = {'DataLoader': {'batch_size': 64,\n                         'shuffle': True},\n          'Generator': {'latent_dim': 120,\n                        'embed_dim': 32,\n                        'ch': 64,\n                        'num_classes': 1,\n                        'use_attn': True},\n          'Discriminator': {'ch': 64,\n                            'num_classes': 1,\n                            'use_attn': True},\n          'sample_latents': {'latent_dim': 120,\n                             'num_classes': 1},\n          'num_iterations': 50000,\n          'decay_start_iteration': 50000,\n          'd_steps': 1,\n          'lr_G': 2e-4,\n          'lr_D': 4e-4,\n          'betas': (0.0, 0.999),\n          'margin': 1.0,\n          'gamma': 0.1,\n          'ema': 0.999,\n          'seed': 42}","352fbcff":"def _load_resized_image(file, root_images):\n    img = cv2.imread(os.path.join(root_images, file))\n    try:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        transform = A.Compose([A.Resize(128, 128, interpolation=cv2.INTER_AREA),\n                           A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n        # transform = A.Compose([A.Resize(64, 64, interpolation=cv2.INTER_AREA)])\n        img = transform(image=img)['image']\n    except:\n        print(\"This file: {} is error\".format(file))\n\n    \n    return img","669ee371":"seed_everything(config['seed'])\nroot_images = dataset_path\nall_files = os.listdir(root_images)\n# all_files = all_files[0:20]\n\nall_images = []\nfor count, f in enumerate(all_files):\n    if count % 1000 == 0: print(count)\n    img = _load_resized_image(f, root_images)\n    if img is not None:\n        all_images.append(img)\n        \n# all_images = [_load_resized_image(f, root_images) for f in all_files]\nall_images = np.array(all_images)\nprint(\"here1\")\nall_labels = np.zeros(len(all_images))\nprint(\"here2\")\n# train_dataiterator = get_dataiterator(all_images, all_labels, config['DataLoader'], device=device)","c81c2637":"train_dataiterator = get_dataiterator(all_images, all_labels, config['DataLoader'], device=device)\nnetG = Generator(**config['Generator']).to(device, torch.float32)\nnetD = Discriminator(**config['Discriminator']).to(device, torch.float32)\n# Exponential moving average of generator weights works well.\n\nnetGE = Generator(**config['Generator']).to(device, torch.float32)\nnetGE.load_state_dict(netG.state_dict());\noptim_G = Adam(params=netG.parameters(), lr=config['lr_G'], betas=config['betas'])\noptim_D = Adam(params=netD.parameters(), lr=config['lr_D'], betas=config['betas'])\ndecay_iter = config['num_iterations'] - config['decay_start_iteration']\nif decay_iter > 0:\n    lr_lambda_G = lambda x: (max(0, 1 - x \/ decay_iter))\n    lr_lambda_D = lambda x: (max(0, 1 - x \/ (decay_iter * config['d_steps'])))\n    lr_sche_G = LambdaLR(optim_G, lr_lambda=lr_lambda_G)\n    lr_sche_D = LambdaLR(optim_D, lr_lambda=lr_lambda_D)\n\n# auxiliary classifier loss.\n# this loss weighted by gamma (0.1) is added to adversarial loss.\n# coefficient gamma is quite sensitive.\n\ncriterion = nn.NLLLoss().to(device, torch.float32)\n\nstep = 1\n\nresume = False\nif resume:\n    state_G = load_checkpoint('{}\/D_checkpoint_6000.ckpt'.format(restore_path))\n    state_D = load_checkpoint('{}\/D_checkpoint_6000.ckpt'.format(restore_path))\n\n    netG.load_state_dict(state_G['model'])\n    netD.load_state_dict(state_D['model'])\n\n    optim_G.load_state_dict(state_G['optimizer'])\n    optim_D.load_state_dict(state_D['optimizer'])\n    step = state_G['iter']\n    ","a629c51c":"# d_steps = config['d_steps'] - step\nwhile True:\n    # Discriminator\n    for i in range(config['d_steps']):\n        for param in netD.parameters():\n            param.requires_grad_(True)\n\n        optim_D.zero_grad()\n        real_imgs, real_labels = train_dataiterator.__next__()\n        # print(real_labels.shape)\n        batch_size = real_imgs.size(0)\n\n        latents, fake_labels = sample_latents(batch_size, **config['sample_latents'], device=device)\n        # print(latents.shape)\n        # print(fake_labels)\n        # exit(0)\n        fake_imgs = netG(latents, fake_labels).detach()\n        preds_real, preds_real_labels = netD(real_imgs, real_labels)\n        # print(preds_real_labels.shape)\n        # print(real_labels.shape)\n        preds_fake, _ = netD(fake_imgs, fake_labels)\n        loss_D = calc_advloss_D(preds_real, preds_fake, config['margin'])\n        loss_D += config['gamma'] * criterion(preds_real_labels, real_labels)\n        loss_D.backward()\n        optim_D.step()\n\n        if (decay_iter > 0) and (step > config['decay_start_iteration']):\n            lr_sche_D.step()\n\n    # Generator\n    for param in netD.parameters():\n        param.requires_grad_(False)\n\n    optim_G.zero_grad()\n\n    real_imgs, real_labels = train_dataiterator.__next__()\n    batch_size = real_imgs.size(0)\n\n    latents, fake_labels = sample_latents(batch_size, **config['sample_latents'], device=device)\n    fake_imgs = netG(latents, fake_labels)\n\n    preds_real, _ = netD(real_imgs, real_labels)\n    preds_fake, preds_fake_labels = netD(fake_imgs, fake_labels)\n\n    loss_G = calc_advloss_G(preds_real, preds_fake, config['margin'])\n    loss_G += config['gamma'] * criterion(preds_fake_labels, fake_labels)\n    loss_G.backward()\n    optim_G.step()\n\n    if (decay_iter > 0) and (step > config['decay_start_iteration']):\n        lr_sche_G.step()\n\n    # Update Generator Eval\n    for param_G, param_GE in zip(netG.parameters(), netGE.parameters()):\n        param_GE.data.mul_(config['ema']).add_((1 - config['ema']) * param_G.data)\n    for buffer_G, buffer_GE in zip(netG.buffers(), netGE.buffers()):\n        buffer_GE.data.mul_(config['ema']).add_((1 - config['ema']) * buffer_G.data)\n\n    # Save checkpoint\n#     if step % 3000 == 0:\n#         print(\"iter: {}, lr_G: {}\".format(step, get_lr(optim_G)))\n#         save_batch(fake_imgs, path=out_path, iter=step)\n#         save_checkpoints(\"checkpoint\", model=netG, optimizer=optim_G, iter=step, name_file=\"G_checkpoint\")\n#         save_checkpoints(\"checkpoint\", model=netD, optimizer=optim_D, iter=step, name_file=\"D_checkpoint\")\n    if step % 3000 == 0:\n        print(\"iter: {}, lr_G: {}\".format(step, get_lr(optim_G)))\n        save_batch(fake_imgs, path=out_path, iter=step)\n        save_checkpoints(checkpoint_path, model=netG, optimizer=optim_G, iter=step, name_file=\"G_checkpoint\")\n        save_checkpoints(checkpoint_path, model=netD, optimizer=optim_D, iter=step, name_file=\"D_checkpoint\")\n\n    # stopping\n    if step < config['num_iterations']:\n        step += 1\n    else:\n        print('total step: {}'.format(step))\n        break","a0973dd5":"!touch out_sample\/file1.txt\n!ls \n# create_download_link(filename='out_sample\/file1.txt')\nfrom IPython.display import FileLink, FileLinks\n# FileLinks('checkpoint')\nFileLinks('out_sample')\n# FileLink('D_checkpoint_6000.ckpt')","136bef4e":"**Start training jobs**","3649d6fa":"***Configure hyper parameter for training model***"}}