{"cell_type":{"6bddabc7":"code","640d3de6":"code","5550c4a8":"code","adf5b3f0":"code","96e5a56d":"code","8ff2519a":"code","c661d22a":"code","2fd9ac4d":"code","d153425e":"code","ffad8202":"code","df81515e":"code","fb9b7023":"code","372dba60":"code","b5ad26a1":"code","484c4823":"code","1309b3c5":"code","f682afc8":"code","a35ed80f":"code","ca8302a1":"code","fc0ea63e":"code","1ff5c900":"code","71a7fc56":"code","424e93f8":"code","327df540":"code","0af667cb":"code","55a9a2ee":"code","b3a9506d":"code","3b8f8de3":"code","ff835ca4":"code","7ff69fbf":"code","c363a1b9":"code","18b37377":"code","914dc247":"code","22a22847":"code","dd9ddb86":"code","63444689":"code","187ac383":"code","ecfd09af":"code","ad956138":"code","1343e6ec":"code","7c8413c1":"code","b9ee39a4":"code","738cd72c":"code","712bd140":"code","5feb5c58":"code","8af27f43":"code","8a203ec7":"code","d50ac5f2":"code","e3eb739e":"code","a6d572e7":"code","ad35220d":"code","f970dcd9":"code","4ed8323b":"code","7b2e528d":"code","641534f9":"code","e819e762":"code","366c9640":"code","5fc260b1":"code","e8a970ea":"code","0db0a86c":"code","3d32fd94":"code","64f2a317":"code","0e0b0bf2":"code","a4978cc6":"code","bfc74865":"code","01befaa3":"code","98ceb98a":"code","e6d19edd":"code","ab4c2215":"code","31f7b2f1":"code","e9b22818":"code","832900f9":"code","58e0c025":"code","7807d904":"code","ac765fa3":"code","c91dc9a2":"code","76196499":"code","11d23738":"code","5d7154fc":"code","7c447e6e":"code","30404ccf":"code","9afcd4d3":"code","9693d2b0":"code","7c00d66f":"code","ca589591":"code","bbdf9966":"code","aecaf2da":"code","daa3ccb7":"code","7501fe13":"code","55005f91":"code","bdea5610":"code","9fe34df4":"code","e8c11610":"code","faca7240":"code","a92d8b47":"code","9d5bf6ab":"code","d267bef4":"code","944bf91a":"code","2c9182c5":"code","adde80b2":"code","eb534a8b":"code","94a03874":"code","df330865":"code","1cea1549":"code","75f44d16":"code","4a32cec6":"code","b5682330":"code","e5305209":"code","e4669a67":"code","22b23154":"code","e09383e8":"code","aac9f4a0":"code","565003b0":"code","11c0296d":"code","6c2a2af2":"code","d7f58458":"code","66f9328e":"code","3123a51b":"markdown","fc9a0caf":"markdown","d81d3962":"markdown","42147246":"markdown","140594ae":"markdown","82cc3da3":"markdown","016e75d5":"markdown","13e03f84":"markdown","cfa06d34":"markdown","b9a0207c":"markdown","0fc59ed8":"markdown","7c1a8977":"markdown","96677d1c":"markdown","a7d47fcc":"markdown","411faf6c":"markdown","9cdad5de":"markdown","a978ce4e":"markdown","822e8cec":"markdown","3a0e5e62":"markdown","c9258d69":"markdown","4676fea0":"markdown","6ca33f6b":"markdown","63db14d2":"markdown","4c8d9425":"markdown","7876e9b5":"markdown","2d81249f":"markdown","41f78d63":"markdown","ea0176ff":"markdown","1c87b64d":"markdown","9d4ed231":"markdown","c2f39d14":"markdown","fe370929":"markdown","bd7bfe38":"markdown","8abb86ef":"markdown","12c1b073":"markdown","62d9fbb1":"markdown","e4c31126":"markdown","b1c862ba":"markdown","74f5eedb":"markdown","ebc9eb1a":"markdown","4bae3be4":"markdown","4deea5a8":"markdown","0c0047b5":"markdown","05595453":"markdown","53e73ba0":"markdown"},"source":{"6bddabc7":"#for data processing\nimport numpy as np \nimport pandas as pd\n\n#for visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom collections import Counter\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","640d3de6":"#Load the data\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","5550c4a8":"train_data.head(2)","adf5b3f0":"test_data.head(2)","96e5a56d":"#Concatenating train and test for easy EDA\ntrain_data['train_or_test']='train'\ntest_data['train_or_test']='test'\nall=pd.concat([train_data,test_data],sort=False)\n\n#Resetting index, removing old index\nall.reset_index(inplace=True)\nall.drop('index',axis=1,inplace=True)","8ff2519a":"all.head(2)","c661d22a":"all.info()","2fd9ac4d":"all.describe()","d153425e":"#Visualization to check for missing values\nsns.heatmap(all.isnull())","ffad8202":"#Survived\nsns.set_style('whitegrid')\nsns.countplot('Survived',hue='train_or_test',data=all)","df81515e":"#Pclass\nall['Pclass'].value_counts()","fb9b7023":"groupby_df = all[all['train_or_test']=='train'].groupby(['Pclass', 'Survived']).agg({'Survived': 'count'})\ngroupby_pcts = groupby_df.groupby(level=0).apply(lambda x:round(100 * x \/ x.sum(),2))\ngroupby_df,groupby_pcts","372dba60":"#Name\nall['Name'].value_counts()","b5ad26a1":"sum(all['Name'].value_counts()>1)","484c4823":"all[(all['Name']=='Kelly, Mr. James') | (all['Name']=='Connolly, Miss. Kate')]","1309b3c5":"#Sex\nall['Sex'].value_counts()","f682afc8":"groupby_df = all[all['train_or_test']=='train'].groupby(['Sex', 'Survived']).agg({'Survived': 'count'})\ngroupby_pcts = groupby_df.groupby(level=0).apply(lambda x:round(100 * x \/ x.sum(),2))\ngroupby_df,groupby_pcts","a35ed80f":"#Age\nsns.boxplot(all['Age'])","ca8302a1":"sns.distplot(all[all['Survived']==0]['Age'],bins=30,color='blue')\nsns.distplot(all[all['Survived']==1]['Age'],bins=30,color='red')","fc0ea63e":"#SibSp\nsns.countplot('SibSp',data=all)","1ff5c900":"groupby_df = all[all['train_or_test']=='train'].groupby(['SibSp', 'Survived']).agg({'Survived': 'count'})\ngroupby_pcts = groupby_df.groupby(level=0).apply(lambda x:round(100 * x \/ x.sum(),2))\ngroupby_df,groupby_pcts","71a7fc56":"#Parch\nsns.countplot('Parch',data=all)","424e93f8":"groupby_df = all[all['train_or_test']=='train'].groupby(['Parch', 'Survived']).agg({'Survived': 'count'})\ngroupby_pcts = groupby_df.groupby(level=0).apply(lambda x:round(100 * x \/ x.sum(),2))\ngroupby_df,groupby_pcts","327df540":"#Ticket\nall['Ticket'].value_counts()","0af667cb":"sum(all['Ticket'].value_counts()>1)","55a9a2ee":"#Fare\nsns.boxplot(all['Fare'])","b3a9506d":"sns.distplot(all[all['Survived']==0]['Fare'],bins=30,color='blue')\nsns.distplot(all[all['Survived']==1]['Fare'],bins=30,color='red')","3b8f8de3":"#Cabin\nall['Cabin'].value_counts()","ff835ca4":"sum(all['Cabin'].value_counts()>1)","7ff69fbf":"all['Embarked'].value_counts()","c363a1b9":"groupby_df = all[all['train_or_test']=='train'].groupby(['Embarked', 'Survived']).agg({'Survived': 'count'})\ngroupby_pcts = groupby_df.groupby(level=0).apply(lambda x:round(100 * x \/ x.sum(),2))\ngroupby_df,groupby_pcts","18b37377":"all.dtypes","914dc247":"sns.heatmap(all.corr(),annot=True)","22a22847":"sns.jointplot(x='Age',y='Fare',data=all,kind='kde')","dd9ddb86":"#Correlation\nall.corr()['Fare']['Age']","63444689":"from scipy.stats import chi2","187ac383":"def chi_test(df,col1,col2):\n    \n    #Contingency Table\n    contingency_table=pd.crosstab(df[col1],df[col2])\n    #print('contingency_table :-\\n',contingency_table)\n\n    #Observed Values\n    Observed_Values = contingency_table.values \n    #print(\"\\nObserved Values :-\\n\",Observed_Values)\n\n    #Expected Values\n    import scipy.stats\n    b=scipy.stats.chi2_contingency(contingency_table)\n    Expected_Values = b[3]\n    #print(\"\\nExpected Values :-\\n\",Expected_Values)\n\n    #Degree of Freedom\n    no_of_rows=len(contingency_table.iloc[0:2,0])\n    no_of_columns=len(contingency_table.iloc[0,0:2])\n    df=(no_of_rows-1)*(no_of_columns-1)\n    #print(\"\\nDegree of Freedom:-\",df)\n\n    #Significance Level 5%\n    alpha=0.05\n    #print('\\nSignificance level: ',alpha)\n\n    #chi-square statistic - \u03c72\n    chi_square=sum([(o-e)**2.\/e for o,e in zip(Observed_Values,Expected_Values)])\n    chi_square_statistic=chi_square[0]+chi_square[1]\n    #print(\"\\nchi-square statistic:-\",chi_square_statistic)\n\n    #critical_value\n    critical_value=chi2.ppf(q=1-alpha,df=df)\n    #print('\\ncritical_value:',critical_value)\n\n    #p-value\n    p_value=1-chi2.cdf(x=chi_square_statistic,df=df)\n    #print('\\np-value:',p_value)\n\n    #compare chi_square_statistic with critical_value and p-value which is the probability of getting chi-square>0.09 (chi_square_statistic)\n    if chi_square_statistic>=critical_value:\n        print(\"\\nchi_square_statistic & critical_value - significant result, reject null hypothesis (H0), dependent.\")\n    else:\n        print(\"\\nchi_square_statistic & critical_value - not significant result, fail to reject null hypothesis (H0).\")\n\n    if p_value<=alpha:\n        print(\"\\np_value & alpha - significant result, reject null hypothesis (H0), dependent.\")\n    else:\n        print(\"\\np_value & alpha - not significant result, fail to reject null hypothesis (H0), independent.\")","ecfd09af":"#Sex & Pclass\nchi_test(all,'Sex','Pclass')","ad956138":"#Sex & Parch\nchi_test(all,'Sex','Parch')","1343e6ec":"#Sex & SibSp\nchi_test(all,'Sex','SibSp')","7c8413c1":"#Sex & Embarked\nchi_test(all,'Sex','Embarked')","b9ee39a4":"#Pclass & SibSp\nchi_test(all,'Pclass','SibSp')","738cd72c":"#Pclass & Parch\nchi_test(all,'Pclass','Parch')","712bd140":"#Pclass & Embarked\nchi_test(all,'Pclass','Embarked')","5feb5c58":"#SibSp & Parch\nchi_test(all,'SibSp','Parch')","8af27f43":"#SibSp & Embarked\nchi_test(all,'SibSp','Embarked')","8a203ec7":"#Parch & Embarked\nchi_test(all,'Parch','Embarked')","d50ac5f2":"#Pclass & Age\nsns.boxplot(x='Pclass',y='Age',data=all)","e3eb739e":"#Sex & Age\nsns.boxplot(x='Sex',y='Age',data=all)","a6d572e7":"#Parch & Age\nsns.boxplot(x='Parch',y='Age',data=all)","ad35220d":"#SibSp & Age\nsns.boxplot(x='SibSp',y='Age',data=all)","f970dcd9":"#Embarked & Age\nsns.boxplot(x='Embarked',y='Age',data=all)","4ed8323b":"#Pclass & Fare\nsns.boxplot(x='Pclass',y='Fare',data=all)","7b2e528d":"#Sex & Fare\nsns.boxplot(x='Sex',y='Fare',data=all)","641534f9":"#Parch & Fare\nsns.boxplot(x='Parch',y='Fare',data=all)","e819e762":"#SibSp & Fare\nsns.boxplot(x='SibSp',y='Fare',data=all)","366c9640":"#Embarked & Fare\nsns.boxplot(x='Embarked',y='Fare',data=all)","5fc260b1":"all.isnull().sum()","e8a970ea":"#Filling Embarked with most common value\nall['Embarked']=all['Embarked'].fillna('S')\n\n#Filling Fare with mean(Fare)\nall['Fare']=all['Fare'].fillna(all['Fare'].mean())","0db0a86c":"#Imputing Age\nindex_NaN_age = list(all[all[\"Age\"].isnull()][\"Age\"].index)\n\nfor i in index_NaN_age :\n    age_med = all[\"Age\"].median()\n    age_pred = all[((all['SibSp'] == all.iloc[i][\"SibSp\"]) & (all['Parch'] == all.iloc[i][\"Parch\"]) & (all['Pclass'] == all.iloc[i][\"Pclass\"]))][\"Age\"].median()\n    if not np.isnan(age_pred) :\n        all['Age'].iloc[i] = age_pred\n    else :\n        all['Age'].iloc[i] = age_med","3d32fd94":"sns.heatmap(all.isnull())","64f2a317":"# Outlier detection \n\ndef detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(all[all['train_or_test']=='train'],2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])","0e0b0bf2":"all.loc[Outliers_to_drop] # Show the outliers rows","a4978cc6":"all = all.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","bfc74865":"all['Pclass'].value_counts() #No Feature Engineering","01befaa3":"all['Fare'].median() #Before Feature Engineering","98ceb98a":"all['FareBand'] = pd.qcut(all['Fare'], 4)","e6d19edd":"all['FareBand'].value_counts()","ab4c2215":"all.loc[ all['Fare'] <= 7.9, 'Fare'] = 0\nall.loc[(all['Fare'] > 7.9) & (all['Fare'] <= 14.4), 'Fare'] = 1\nall.loc[(all['Fare'] > 14.4) & (all['Fare'] <= 30.5), 'Fare']   = 2\nall.loc[ all['Fare'] > 30.5, 'Fare'] = 3","31f7b2f1":"all['Fare'].value_counts()","e9b22818":"all['Age'].median() #Before Feature Engineering","832900f9":"all['AgeBand'] = pd.cut(all['Age'], 5)","58e0c025":"all['AgeBand'].value_counts()","7807d904":"all.loc[ all['Age'] <= 16, 'Age'] = 0\nall.loc[(all['Age'] > 16) & (all['Age'] <= 32), 'Age'] = 1\nall.loc[(all['Age'] > 32) & (all['Age'] <= 48), 'Age'] = 2\nall.loc[(all['Age'] > 48) & (all['Age'] <= 64), 'Age'] = 3\nall.loc[ all['Age'] > 64, 'Age']=5","ac765fa3":"all['Age'].value_counts()","c91dc9a2":"split_one = all['Name'].str.split('.', n=1, expand = True)\nall['First'] = split_one[0]\nall['Last'] = split_one[1]\nsplit_two = all['First'].str.split(',', n=1, expand = True)\nall['Last Name'] = split_two[0]\nall['Title'] = split_two[1]\nsplit_three = all['Title'].str.split('', n=1, expand = True)\n\nsplit_three\n","76196499":"all['Title'].value_counts()","11d23738":"all.drop(['First','Last','Name','Last Name'],axis = 1,inplace = True)","5d7154fc":"all.replace(to_replace = [ ' Don', ' Rev', ' Dr', ' Mme',\n        ' Major', ' Sir', ' Col', ' Capt',' Jonkheer'], value = ' Honorary(M)', inplace = True)\n\nall.replace(to_replace = [ ' Ms', ' Lady', ' Mlle',' the Countess', ' Dona'], value = ' Honorary(F)', inplace = True)\n\nall['Title'].value_counts()","7c447e6e":"all = pd.get_dummies(all, columns = ['Title'],prefix='Title_',drop_first=True)\nall.head()","30404ccf":"all['Family'] = all['SibSp'] + all['Parch'] + 1","9afcd4d3":"all['Single'] = all['Family'].map(lambda s: 1 if s == 1 else 0)\nall['SmallF'] = all['Family'].map(lambda s: 1 if  s == 2  else 0)\nall['MedF'] = all['Family'].map(lambda s: 1 if 3 <= s <= 4 else 0)\nall['LargeF'] = all['Family'].map(lambda s: 1 if s >= 5 else 0)\nall.head()","9693d2b0":"all = pd.get_dummies(all, columns = ['Embarked'], prefix='Embarked_from_',drop_first=True)\nall.head()","7c00d66f":"all.drop('Cabin',axis=1,inplace=True)","ca589591":"all['Ticket'].unique()","bbdf9966":"all['Ticket'].value_counts()","aecaf2da":"all['Ticket'] = all['Ticket'].astype(str)\nall['Ticket_length'] = all['Ticket'].apply(len)\nall['Ticket_length'].astype(int)\nall['Ticket_length'].unique()","daa3ccb7":"all['Ticket_length'] = np.where(((all['Ticket_length'] == 3) | (all['Ticket_length'] == 4) | (all['Ticket_length'] == 5)),4,all['Ticket_length'])\n\nall['Ticket_length'] = np.where(((all['Ticket_length'] == 6)),5,all['Ticket_length'])\n\nall['Ticket_length'] = np.where(((all['Ticket_length'] == 7) | (all['Ticket_length'] == 8) | (all['Ticket_length'] == 9) | (all['Ticket_length'] == 10) | (all['Ticket_length'] == 13)\n                                 | (all['Ticket_length'] == 17)| (all['Ticket_length'] == 16)| (all['Ticket_length'] == 13)| (all['Ticket_length'] == 12) | (all['Ticket_length'] == 15)\n                                 | (all['Ticket_length'] == 11)| (all['Ticket_length'] == 18)),12,all['Ticket_length'])\n\n","7501fe13":"all['Ticket_length'].unique()","55005f91":"all['Ticket_length'] = all['Ticket_length'].astype(str)\n\nall['Ticket_length'] = np.where(((all['Ticket_length'] == '4')),'Below 6',all['Ticket_length'])\nall['Ticket_length'] = np.where(((all['Ticket_length'] == '5')),'At 6',all['Ticket_length'])\nall['Ticket_length'] = np.where(((all['Ticket_length'] == '12')),'Above 6',all['Ticket_length'])","bdea5610":"all['Ticket_length'].unique()","9fe34df4":"all = pd.get_dummies(all, columns=['Ticket_length'], prefix = 'Ticket_Length_',drop_first=True)\nall.head()","e8c11610":"all.drop(['Ticket'],axis = 1, inplace = True)","faca7240":"all = pd.get_dummies(all, columns = ['Sex'],prefix='Gender_',drop_first=True)\nall.head()","a92d8b47":"all.info()","9d5bf6ab":"all.drop(['SibSp','Parch','Family','FareBand','AgeBand'],axis = 1,inplace = True)","d267bef4":"all.info()","944bf91a":"train_data,test_data=all[all['train_or_test']=='train'],all[all['train_or_test']=='test']\ntrain_data.drop('train_or_test',axis=1,inplace=True)\ntest_data.drop('train_or_test',axis=1,inplace=True)","2c9182c5":"train_data.info()","adde80b2":"test_data.info()","eb534a8b":"train_data.describe()","94a03874":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve","df330865":"#train & test split\nX_train, X_test, y_train, y_test = train_test_split(train_data.drop(['PassengerId','Survived'],axis=1), \n                                                    train_data['Survived'], test_size=0.30, \n                                                    random_state=101)","1cea1549":"#Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=10)","75f44d16":"#Modeling step Test differents algorithms \nrandom_state = 101\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(LinearDiscriminantAnalysis())\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train, y = y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","4a32cec6":"cv_res.sort_values('CrossValMeans',ascending=False)","b5682330":"###META MODELING  WITH ADABOOST, RF, EXTRATREES and GRADIENTBOOSTING\n\n#Adaboost\nDTC = DecisionTreeClassifier()\n\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\n\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsadaDTC.fit(X_train,y_train)\n\nada_best = gsadaDTC.best_estimator_\n\n#Best score\ngsadaDTC.best_score_","e5305209":"#ExtraTrees \nExtC = ExtraTreesClassifier()\n\n\n##Search grid for optimal parameters\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsExtC.fit(X_train,y_train)\n\nExtC_best = gsExtC.best_estimator_\n\n#Best score\ngsExtC.best_score_","e4669a67":"#RFC Parameters tunning \nRFC = RandomForestClassifier()\n\n\n##Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC.fit(X_train,y_train)\n\nRFC_best = gsRFC.best_estimator_\n\n#Best score\ngsRFC.best_score_","22b23154":"#Gradient boosting tunning\n\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsGBC.fit(X_train,y_train)\n\nGBC_best = gsGBC.best_estimator_\n\n#Best score\ngsGBC.best_score_","e09383e8":"#SVC classifier\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsSVMC.fit(X_train,y_train)\n\nSVMC_best = gsSVMC.best_estimator_\n\n#Best score\ngsSVMC.best_score_","aac9f4a0":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(gsRFC.best_estimator_,\"RF mearning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(gsSVMC.best_estimator_,\"SVC learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(gsadaDTC.best_estimator_,\"AdaBoost learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(gsGBC.best_estimator_,\"GradientBoosting learning curves\",X_train,y_train,cv=kfold)","565003b0":"nrows = ncols = 2\nfig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,15))\n\nnames_classifiers = [(\"AdaBoosting\", ada_best),(\"ExtraTrees\",ExtC_best),(\"RandomForest\",RFC_best),(\"GradientBoosting\",GBC_best)]\n\nnclassifier = 0\nfor row in range(nrows):\n    for col in range(ncols):\n        name = names_classifiers[nclassifier][0]\n        classifier = names_classifiers[nclassifier][1]\n        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n        g = sns.barplot(y=X_train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row][col])\n        g.set_xlabel(\"Relative importance\",fontsize=12)\n        g.set_ylabel(\"Features\",fontsize=12)\n        g.tick_params(labelsize=9)\n        g.set_title(name + \" feature importance\")\n        nclassifier += 1","11c0296d":"test_Survived_RFC = pd.Series(RFC_best.predict(test_data.drop(['PassengerId','Survived'],axis=1)), name=\"RFC\")\ntest_Survived_ExtC = pd.Series(ExtC_best.predict(test_data.drop(['PassengerId','Survived'],axis=1)), name=\"ExtC\")\ntest_Survived_SVMC = pd.Series(SVMC_best.predict(test_data.drop(['PassengerId','Survived'],axis=1)), name=\"SVC\")\ntest_Survived_AdaC = pd.Series(ada_best.predict(test_data.drop(['PassengerId','Survived'],axis=1)), name=\"Ada\")\ntest_Survived_GBC = pd.Series(GBC_best.predict(test_data.drop(['PassengerId','Survived'],axis=1)), name=\"GBC\")\n\n# Concatenate all classifier results\nensemble_results = pd.concat([test_Survived_RFC,test_Survived_ExtC,test_Survived_AdaC,test_Survived_GBC, test_Survived_SVMC],axis=1)\n\n\ng= sns.heatmap(ensemble_results.corr(),annot=True)","6c2a2af2":"votingC = VotingClassifier(estimators=[('rfc', RFC_best),('gbc',GBC_best)], voting='soft', n_jobs=4)\nvotingC = votingC.fit(X_train, y_train)","d7f58458":"output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': votingC.predict(test_data.drop(['PassengerId','Survived'],axis=1)).astype('int')})\n\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","66f9328e":"output","3123a51b":"# Feature Engineering","fc9a0caf":"Passengers with 1,2 Siblings\/Spouses have a higher chance of Survival ","d81d3962":"##### Sex","42147246":"## Missing Value Treatment","140594ae":"##### Cabin","82cc3da3":"##### I compared 10 popular classifiers and evaluate the mean accuracy of each of them by a stratified kfold cross validation procedure.\n\n    SVC\n    Decision Tree\n    AdaBoost\n    Random Forest\n    Extra Trees\n    Gradient Boosting\n    Multiple layer perceprton (neural network)\n    KNN\n    Logistic regression\n    Linear Discriminant Analysis\n","016e75d5":"## Bivariate Analysis","13e03f84":"Female has higher chance of survival ~74% vs Male 19%","cfa06d34":"##### Ticket","b9a0207c":"##### Categorical & Categorical","0fc59ed8":"## Outlier Detection","7c1a8977":"Converted Fare into bands","96677d1c":"##### Age","a7d47fcc":"Embarked C has a higher chance of survival","411faf6c":"A collection of several models working together on a single set is called an ensemble. The method is called Ensemble Learning. It is much more useful use all different models rather than any one.","9cdad5de":"<img src='https:\/\/www.maritimecyprus.com\/wp-content\/uploads\/2015\/10\/titanic-infographic-696x431.jpg'>","a978ce4e":"#### Why Bivariate Analysis?\n\nBivariate analysis is one of the simplest forms of quantitative (statistical) analysis. It involves the analysis of two variables (often denoted as X, Y), for the purpose of determining the empirical relationship between them. Bivariate analysis can be helpful in testing simple hypotheses of association.\n\n<a> https:\/\/www.statisticshowto.com\/bivariate-analysis\/ <\/a>\n\n#### Bivariate Analysis Grouped by Nature of Variable\n1. Continuous & Continous\n2. Categorical & Categorical\n3. Categorical & Continuous","822e8cec":"Looking at learning curve to avoid overfitting of model scores","3a0e5e62":"We see passengers with higher Fare have a higher chance of survival","c9258d69":"#### What are outliers, how to treat them?\n\n##### What is an Outlier?\n\nOutlier is a commonly used terminology by analysts and data scientists as it needs close attention else it can result in wildly wrong estimations. Simply speaking, Outlier is an observation that appears far away and diverges from an overall pattern in a sample.\n\n \n##### What are the types of Outliers?\n\nOutlier can be of two types: Univariate and Multivariate. Above, we have discussed the example of univariate outlier. These outliers can be found when we look at distribution of a single variable. Multi-variate outliers are outliers in an n-dimensional space. In order to find them, you have to look at distributions in multi-dimensions.\n\nLet us understand this with an example. Let us say we are understanding the relationship between height and weight. Below, we have univariate and bivariate distribution for Height, Weight. Take a look at the box plot. We do not have any outlier (above and below 1.5*IQR, most common method). Now look at the scatter plot. Here, we have two values below and one above the average in a specific segment of weight and height.\n\n##### Outlier, Multivariate Outlier\n\nWhat causes Outliers?\n\nWhenever we come across outliers, the ideal way to tackle them is to find out the reason of having these outliers. The method to deal with them would then depend on the reason of their occurrence. Causes of outliers can be classified in two broad categories:\n\n   1. Artificial (Error) \/ Non-natural\n   2. Natural.\n\nLet\u2019s understand various types of outliers in more detail:\n\n   * Data Entry Errors:- Human errors such as errors caused during data collection, recording, or entry can cause outliers in data.\n    \n   * Measurement Error: It is the most common source of outliers. This is caused when the measurement instrument used turns out to be faulty. For example: There are 10 weighing machines. 9 of them are correct, 1 is faulty. Weight measured by people on the faulty machine will be higher \/ lower than the rest of people in the group. The weights measured on faulty machine can lead to outliers.\n    \n   * Experimental Error: Another cause of outliers is experimental error. For example: In a 100m sprint of 7 runners, one runner missed out on concentrating on the \u2018Go\u2019 call which caused him to start late. Hence, this caused the runner\u2019s run time to be more than other runners. His total run time can be an outlier.\n    \n   * Intentional Outlier: This is commonly found in self-reported measures that involves sensitive data. For example: Teens would typically under report the amount of alcohol that they consume. Only a fraction of them would report actual value. Here actual values might look like outliers because rest of the teens are under reporting the consumption.\n    \n   * Data Processing Error: Whenever we perform data mining, we extract data from multiple sources. It is possible that some manipulation or extraction errors may lead to outliers in the dataset.\n    \n   * Sampling error: For instance, we have to measure the height of athletes. By mistake, we include a few basketball players in the sample. This inclusion is likely to cause outliers in the dataset.\n    \n   * Natural Outlier: When an outlier is not artificial (due to error), it is a natural outlier. For instance: In my last assignment with one of the renowned insurance company, I noticed that the performance of top 50 financial advisors was far higher than rest of the population. Surprisingly, it was not due to any error. Hence, whenever we perform any data mining activity with advisors, we used to treat this segment separately.\n\n \n##### What is the impact of Outliers on a dataset?\n\nOutliers can drastically change the results of the data analysis and statistical modeling. There are numerous unfavourable impacts of outliers in the data set:\n\n*     It increases the error variance and reduces the power of statistical tests\n*     If the outliers are non-randomly distributed, they can decrease normality\n*     They can bias or influence estimates that may be of substantive interest\n*     They can also impact the basic assumption of Regression, ANOVA and other statistical model assumptions.\n\n##### How to detect Outliers?\n\nMost commonly used method to detect outliers is visualization. We use various visualization methods, like Box-plot, Histogram, Scatter Plot (above, we have used box plot and scatter plot for visualization). Some analysts also various thumb rules to detect outliers. Some of them are:\n\n  * Any value, which is beyond the range of -1.5 x IQR to 1.5 x IQR\n  * Use capping methods. Any value which out of range of 5th and 95th percentile can be considered as outlier\n  * Data points, three or more standard deviation away from mean are considered outlier\n  * Outlier detection is merely a special case of the examination of data for influential data points and it also depends on the business understanding\n  * Bivariate and multivariate outliers are typically measured using either an index of influence or leverage, or distance. Popular indices such as Mahalanobis\u2019 distance and Cook\u2019s D are frequently used to detect outliers.\n  * In SAS, we can use PROC Univariate, PROC SGPLOT. To identify outliers and influential observation, we also look at statistical measure like STUDENT, COOKD, RSTUDENT and others.\n\n##### How to remove Outliers?\n\nMost of the ways to deal with outliers are similar to the methods of missing values like deleting observations, transforming them, binning them, treat them as a separate group, imputing values and other statistical methods. Here, we will discuss the common techniques used to deal with outliers:\n\n* Deleting observations: We delete outlier values if it is due to data entry error, data processing error or outlier observations are very small in numbers. We can also use trimming at both ends to remove outliers.\n\n* Transforming and binning values: Transforming variables can also eliminate outliers. Natural log of a value reduces the variation caused by extreme values. Binning is also a form of variable transformation. Decision Tree algorithm allows to deal with outliers well due to binning of variable. We can also use the process of assigning weights to different observations.\n \n* Variable Transformation, LOG\n \n* Imputing: Like imputation of missing values, we can also impute outliers. We can use mean, median, mode imputation methods. Before imputing values, we should analyse if it is natural outlier or artificial. If it is artificial, we can go with imputing values. We can also use statistical model to predict values of outlier observation and after that we can impute it with predicted values.\n \n* Treat separately: If there are significant number of outliers, we should treat them separately in the statistical model. One of the approach is to treat both groups as two different groups and build individual model for both groups and then combine the output.","4676fea0":"#### Why Univariate Analysis?\n\nUnivariate analysis is the simplest form of analyzing data. \u201cUni\u201d means \u201cone\u201d, so in other words your data has only one variable. It doesn't deal with causes or relationships (unlike regression ) and it's major purpose is to describe; It takes data, summarizes that data and finds patterns in the data.\n\n<a> https:\/\/www.statisticshowto.com\/univariate\/ <\/a>","6ca33f6b":"##### Pclass","63db14d2":"Age, Cabin have significant rows with missing values,while Fare & Embarked have a few rows. Survived missing values from test data.","4c8d9425":"# References :\n1. EDA - https:\/\/www.analyticsvidhya.com\/blog\/2016\/01\/guide-data-exploration\/\n2. Voting Classifier - https:\/\/medium.com\/@sanchitamangale12\/voting-classifier-1be10db6d7a5","7876e9b5":"# Loading The Data","2d81249f":"##### SibSp + Parch = Family Size","41f78d63":"## Univariate Analysis","ea0176ff":"# Building Models","1c87b64d":"All the categorical variables seem to be dependent on each other","9d4ed231":"## Feel free to share feedback, Upvote if you like\/found the notebook useful!","c2f39d14":"##### Name","fe370929":"Till here, we have learnt about steps of data exploration, missing value treatment and techniques of outlier detection and treatment. These 3 stages will make your raw data better in terms of information availability and accuracy. Let\u2019s now proceed to the final stage of data exploration. It is Feature Engineering.","bd7bfe38":"# Final Output","8abb86ef":"Target variable: Survived (1\/0), Potential Predictors: All Others","12c1b073":"Lower age has higher chance of survival and the ages are almost normally distributed. ","62d9fbb1":"Clearly Pclass=1 has higher chance of survival ~63% vs Pclass=2(47%) and Pclass=3(24%)","e4c31126":"##### Categorical & Continuous ","b1c862ba":"# Exploratory Data Analysis","74f5eedb":"##### Fare","ebc9eb1a":"Converted Age into bands","4bae3be4":"# The Challenge - Titanic-Machine Learning from Disaster\n\n> The sinking of the Titanic is one of the most infamous shipwrecks in history.\n> \n> On April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n> \n> While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n> \n> In this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc). ","4deea5a8":"##### Continuous & Continuous","0c0047b5":"1-3 Parents\/Children have higher chance of Survival","05595453":"##### Embarked","53e73ba0":"#### What is Chi Square Test?\n\n\n    Chi-Square Test: This test is used to derive the statistical significance of relationship between the variables. Also, it tests whether the evidence in the sample is strong enough to generalize that the relationship for a larger population as well. Chi-square is based on the difference between the expected and observed frequencies in one or more categories in the two-way table. It returns probability for the computed chi-square distribution with the degree of freedom.\n\nProbability of 0: It indicates that both categorical variable are dependent\n\nProbability of 1: It shows that both variables are independent.\n\nProbability less than 0.05: It indicates that the relationship between the variables is significant at 95% confidence. The chi-square test statistic for a test of independence of two categorical variables is found by:"}}