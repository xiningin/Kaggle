{"cell_type":{"34640743":"code","0741c0b7":"code","5eeb3a0e":"code","ac056de3":"code","495988e9":"code","77a3ce5d":"code","4f00084b":"code","e3484c60":"code","b379d000":"code","fbd12989":"code","94e3e0bc":"code","d6587af1":"code","5adb90bd":"code","2d3e091f":"code","f5b617d3":"code","0ef83415":"code","f6e5c34d":"code","a6dae760":"code","5fbef593":"code","cb460bb2":"code","2f3603cb":"code","392395b3":"code","f70ed523":"code","e8171466":"code","ac2a150d":"code","40395e83":"code","f56ba98f":"code","6c620261":"code","3fb8d1f2":"code","654708d2":"code","75d0fede":"code","44d09a8a":"code","176d1e3e":"markdown","468f2714":"markdown","48f619c3":"markdown","4f4b7031":"markdown","59ca412d":"markdown","6ed40c38":"markdown","93de8d68":"markdown","6269ca87":"markdown","9681dacd":"markdown","ce2584df":"markdown","164b7b8e":"markdown","a3a10d42":"markdown","757ff4f3":"markdown","62b6a95e":"markdown","bf4831ae":"markdown","da0bc40b":"markdown","9e4b8dd0":"markdown","83645403":"markdown","c7a6065e":"markdown","7d06d711":"markdown","6b700eab":"markdown","23f0a3e6":"markdown","222964ed":"markdown","e8a9fb3c":"markdown","7e8773dc":"markdown","8cadb87e":"markdown","a8442789":"markdown","fb3b0255":"markdown","ee09dddc":"markdown"},"source":{"34640743":"import numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import StratifiedKFold,KFold\nfrom collections import Counter\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import classification_report,roc_auc_score,confusion_matrix,accuracy_score,f1_score\n\nimport optuna\nfrom optuna.samplers import TPESampler","0741c0b7":"train = pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/test.csv')\nsample = pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/sample_submission.csv')\n\nsns.set(style='white', context='notebook', palette='deep')","5eeb3a0e":"train.head()","ac056de3":"train.info()","495988e9":"train.describe()","77a3ce5d":"train.isnull().sum()","4f00084b":"train.skew()","e3484c60":"train.dtypes","b379d000":"all_features = pd.concat([train.drop(['id','Response'],axis=1),test.drop('id',axis=1)],axis=0)\ny = train['Response']","fbd12989":"fig = px.pie(train,values=train['Response'].value_counts(),names=['Class 0','Class 1'],hole=0.6,labels={0:'Response = 0'},color_discrete_sequence=px.colors.sequential.Sunset)\nfig.show(showlegend=True)","94e3e0bc":"sns.countplot(train['Response'])\nplt.show()","d6587af1":"sns.barplot(train['Driving_License'],train['Response'])\nplt.show()","5adb90bd":"sns.barplot(train['Previously_Insured'],train['Response'])\nplt.show()","2d3e091f":"sns.barplot(train['Vehicle_Damage'],train['Response'])\nplt.show()","f5b617d3":"sns.barplot(train['Vehicle_Age'],train['Response'])\nplt.show()","0ef83415":"sns.barplot(train['Gender'],train['Response'])\nplt.show()","f6e5c34d":"plt.figure(figsize=(20,10))\nsns.barplot(train['Age'],train['Response'])\nplt.show()","a6dae760":"sns.boxplot(train['Age'])\nplt.show()","5fbef593":"bins = [20, 30, 40, 50, 60, 70,90]\nlabels = ['20-27', '28-39', '40-49', '50-59', '60-69', '70+']\nage_categories = pd.cut(train['Age'], bins, labels = labels,include_lowest = True)","cb460bb2":"sns.barplot(age_categories,train['Response'])\nplt.show()","2f3603cb":"sns.boxplot(train['Annual_Premium'])\nplt.show()","392395b3":"g = sns.distplot(train['Annual_Premium'],label='Skewness: '+str(round(train['Annual_Premium'].skew(),4)))\ng = g.legend(loc='best')\nplt.show()","f70ed523":"plt.figure(figsize=(10,10))\nsns.heatmap(train.corr(),annot=True,cmap='rainbow')\nplt.show()","e8171466":"all_features['Vehicle_Age'] = all_features['Vehicle_Age'].map({'> 2 Years':2,'1-2 Year':1,'< 1 Year':0})\nall_features['Vehicle_Damage'] = all_features['Vehicle_Damage'].map({'Yes':1,'No':0})\nall_features['Gender'] = all_features['Gender'].map({'Male':1,'Female':0}) \nall_features","ac2a150d":"X = all_features.iloc[:len(train),:]\nX_test = all_features.iloc[len(train):,:]\n\nkf = StratifiedKFold(n_splits=12,shuffle=True,random_state=42)","40395e83":"for train_index,val_index in kf.split(X,y):\n    X_train,X_val = X.iloc[train_index],X.iloc[val_index],\n    y_train,y_val = y.iloc[train_index],y.iloc[val_index],","f56ba98f":"rus = RandomOverSampler(random_state=42)\nX_rus,y_rus = rus.fit_sample(X_train,y_train)","6c620261":"lgb_rus = LGBMClassifier(random_state=42)\nlgb_rus.fit(X_rus,y_rus)\nprint(classification_report(y_val,lgb_rus.predict(X_val)))\nprint('ROC AUC Score: ' + str(roc_auc_score(y_val,lgb_rus.predict(X_val))))","3fb8d1f2":"sns.heatmap(confusion_matrix(y_val,lgb_rus.predict(X_val)),cmap='magma',annot=True,fmt='g')\nplt.show()","654708d2":"def create_model(trial):\n    n_estimators = trial.suggest_int('n_estimators',100,500)\n    num_leaves = trial.suggest_int('num_leaves',10,500)\n    max_depth = trial.suggest_int('max_depth',4,20)\n    learning_rate = trial.suggest_uniform('learning_rate',0.0001,1)\n    min_child_samples = trial.suggest_int('min_child_samples',10,50)\n    model = LGBMClassifier(n_estimators=n_estimators,num_leaves=num_leaves,\n    max_depth=max_depth,learning_rate=learning_rate,min_child_samples=min_child_samples)\n    return model\n\ndef objective(trial):\n    model = create_model(trial)\n    model.fit(X_rus,y_rus)\n    score = roc_auc_score(y_val,model.predict(X_val))\n    return score\n\nsampler = TPESampler(seed=42)\nstudy = optuna.create_study(sampler=sampler,direction='maximize')\nstudy.optimize(objective,n_trials=60)","75d0fede":"lgb_params = study.best_params\nlgb_params['random_state'] = 42\nlgb = LGBMClassifier(**lgb_params)\nlgb.fit(X_rus, y_rus)\npreds = lgb.predict(X_val)\nprint(classification_report(y_val,lgb.predict(X_val)))\nprint('ROC AUC Score: ' + str(roc_auc_score(y_val,lgb.predict(X_val))))","44d09a8a":"sns.heatmap(confusion_matrix(y_val,lgb.predict(X_val)),cmap='magma',annot=True,fmt='g')\nplt.show()","176d1e3e":"Here we plot a correlation heatmap and see that there is no strong correlation between any feautures and the target feature","468f2714":"<h1 align='center'>Thanks and make sure to learn!<\/h1>","48f619c3":"Let's fit a vanilla LGBMClassifier on the undersampled data and evaluate it","4f4b7031":"Males are more likely to be interested in getting insured. This could be due to many possible reasons, but one could be just that more males in the dataset.","59ca412d":"<h1 align='center'>Basic LightGBM<\/h1>","6ed40c38":"We can see from the above visualisations that:\n\n* The Data is highly imbalanced, with 87.7% belonging to Class 0 and 12.3% belonging to Class 1. We will use both UnderSampling and OverSampling to balance the data out equally","93de8d68":"We define resplit our train and test set, and set up 12 Fold Stratified Cross Validation. It is important to stratify as this ensures that during our training and validation, we split according to target distribution","6269ca87":"People with vehicle damage are also more likely to be interested in getting insurance, which is self-explanatory","9681dacd":"Here we just map categorical values to their appropriate numerical counterparts","ce2584df":"After splitting the ages into 6 groups, we see that this visualisation confirms our observations from before; the main age groups interested in getting insurance are between 28-50.","164b7b8e":"Annual Premium is also skewed. We might try a log or square root transformation to mitigate the skewness","a3a10d42":"<h1 align='center'>Categorical Features Data Analysis<\/h1>","757ff4f3":"<h1 align='center'>Numerical Features Data Analysis<\/h1>","62b6a95e":"Let's combine the train and test set so our transformations are easier as we don't have to apply them seperately to each set","bf4831ae":"Here, we can see that people who have the a driving license are more likely to be interested in getting insurance","da0bc40b":"<h1 align='center'>Exploratory Data Analysis<\/h1>","9e4b8dd0":"<h1 align='center'>Welcome to my Notebook<\/h1>","83645403":"The reason for this boxplot was to see if there was any outliers(in this case, any extreme cases or accidental ages, e.g a 5 year old interested in insurance!)","c7a6065e":"Now,we see that people who are not previously insured are more likely to be interested in getting insurance, as one does not want to pay for multiple insurances","7d06d711":"<h1 align='center'>Hyperparameter Tuning<\/h1>","6b700eab":"<h1 align='center'>Modelling Using Undersampling<\/h1>","23f0a3e6":"We see that Annual Premium has a wide varietly of values","222964ed":"Our model did suprisingly well with default parameters, but not fantastic. Let's optimize","e8a9fb3c":"We will use `imblearn`'s RandomUnderSampler to undersample from the majority class so that they match","7e8773dc":"In this visualisation, we see what ages are more likely to be interested in insurance. We see the following things:\n1. Ages between 20-27 are not very interested in getting insured. This can be because insurance is not their main priority, and they probably cannot afford it as they are most likely students\n2. Ages 30-55 are more interested in getting insured, as they use the car on a day to day basis to travel for work, so they are more in need to repairs \n3. Age 65+ are less likely to be interested in insurance as this is the retirment age and they do not have a need for a car any more, let alone insurance","8cadb87e":"<h1 align='center'>Modelling<\/h1>","a8442789":"<h1 align='center'>Feature Preprocessing<\/h1>","fb3b0255":"Here we define our validation set","ee09dddc":"Here we see that people with older Vehicles are more likely to be interested. This is obvious, as the longer a vehicle is on the road, the more likely it is to have problems and issues as oppose to new cars"}}