{"cell_type":{"9d501dfb":"code","213d6b62":"code","44055e35":"code","63c52a8f":"code","db45a4b4":"code","3de245f2":"code","df165f0c":"code","e8da6edb":"code","ad1f3100":"code","f7b02124":"code","569dc078":"code","d2b0e1b4":"code","c1829260":"code","c15e70f6":"code","38242c99":"code","da5bb21b":"code","d8667448":"code","e3aa8104":"code","f7a1589a":"code","6a2967f7":"code","7a6a35da":"code","d5a10ac7":"markdown","e475adfb":"markdown"},"source":{"9d501dfb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","213d6b62":"import time\nimport numpy\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom bs4 import BeautifulSoup\nimport string\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import layers, callbacks\nfrom tensorflow.keras import Model, Sequential","44055e35":"d_train = pd.read_csv(\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv\", \n                      encoding='latin1')\nd_test = pd.read_csv(\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv\", \n                     encoding='latin1')","63c52a8f":"d_train.head()","db45a4b4":"d_test.head()","3de245f2":"sns.countplot(y=d_train.Sentiment)\n#degrees = 90\n#plt.xticks(rotation=degrees)\nplt.show()","df165f0c":"# Check Nan\nd_train.isnull().sum()","e8da6edb":"d_train.info()","ad1f3100":"# Remove unused column\nd_train = d_train.drop(['Location','TweetAt','ScreenName'], axis=1)\nd_test = d_test.drop(['Location','TweetAt','ScreenName'], axis=1)\n\nd_train.head()","f7b02124":"# Convert sentiment into Positive = 2 , Neutral = 1 , Negative =  0\ndef convert_Sentiment(label):\n    if label == \"Extremely Positive\":\n        return 2\n    elif label == \"Extremely Negative\":\n        return 0\n    elif label == \"Positive\":\n        return 2\n    elif label == \"Negative\":\n        return 0\n    else:\n        return 1\n    \n# Apply convert_Sentiment function\nd_train.Sentiment = d_train.Sentiment.apply(lambda x : convert_Sentiment(x))\nd_train.head()","569dc078":"sns.countplot(y=d_train.Sentiment)\n#degrees = 90\n#plt.xticks(rotation=degrees)\nplt.show()","d2b0e1b4":"def cleaning_text(text):\n    stop_words = stopwords.words(\"english\")\n\n    text = re.sub(r'http\\S+', \" \", text)    # remove urls\n    text = re.sub(r'@\\w+',' ',text)         # remove mentions\n    text = re.sub(r'#\\w+', ' ', text)       # remove hastags\n    text = re.sub('r<.*?>',' ', text)       # remove html tags\n    \n    # remove stopwords \n    text = text.split()\n    text = \" \".join([word for word in text if not word in stop_words])\n\n    for punctuation in string.punctuation:\n        text = text.replace(punctuation, \"\")\n    \n    return text\n\nd_train['preprocessing_results'] = d_train['OriginalTweet'].apply(lambda x: cleaning_text(x))","c1829260":"for i in range(5):\n    print('----------------------------------------------')\n    random_number=np.random.randint(0,len(d_train)-1)\n    print(d_train.preprocessing_results[random_number])\n    print('----------------------------------------------\\n')","c15e70f6":"# Maximum sentence length\nmax_len_words = max(list(d_train['preprocessing_results'].apply(len)))\nprint(max_len_words)","38242c99":"def tokenizer(x_train, y_train, max_len_word):\n    # because the data distribution is imbalanced, \"stratify\" is used\n    X_train, X_val, y_train, y_val = train_test_split(x_train, y_train, \n                                                      test_size=.2, shuffle=True, \n                                                      stratify=y_train, random_state=0)\n\n    # Tokenizer\n    tokenizer = Tokenizer(num_words=5000)\n    tokenizer.fit_on_texts(X_train)\n    sequence_dict = tokenizer.word_index\n    word_dict = dict((num, val) for (val, num) in sequence_dict.items())\n\n    # Sequence data\n    train_sequences = tokenizer.texts_to_sequences(X_train)\n    train_padded = pad_sequences(train_sequences,\n                                 maxlen=max_len_word,\n                                 truncating='post',\n                                 padding='post')\n    \n    val_sequences = tokenizer.texts_to_sequences(X_val)\n    val_padded = pad_sequences(val_sequences,\n                                maxlen=max_len_word,\n                                truncating='post',\n                                padding='post', )\n    \n    print(train_padded.shape)\n    print(val_padded.shape)\n    print('Total words: {}'.format(len(word_dict)))\n    return train_padded, val_padded, y_train, y_val, word_dict\n\nX_train, X_val, y_train, y_val, word_dict = tokenizer(d_train.preprocessing_results, d_train.Sentiment, 300)","da5bb21b":"num_classes = d_train.Sentiment.nunique()\nprint(num_classes)","d8667448":"model = Sequential([\n    layers.Embedding(5000, 300, input_length=300),\n    layers.Bidirectional(layers.LSTM(64, return_sequences=True, recurrent_dropout=0.4)),\n    #layers.LSTM(64, return_sequences=True, recurrent_dropout=0.4),\n    #layers.BatchNormalization(),\n    layers.GlobalAveragePooling1D(),    # or layers.Flatten()\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.4),\n    layers.Dense(num_classes, activation='softmax')\n])","e3aa8104":"model.summary()","f7a1589a":"model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n              metrics=['accuracy'])","6a2967f7":"start = time.perf_counter()\nearly_stopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n                                         mode =\"min\", patience=3)\n\nhistory = model.fit(X_train, y_train,\n                    epochs=50, \n                    validation_data=(X_val, y_val),\n                    callbacks=[early_stopping], \n                    shuffle=True)\n\nelapsed = time.perf_counter() - start\nprint('Elapsed %.3f seconds.' % elapsed)","7a6a35da":"# Plotting accuracy and val_accuracy\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(1, len(val_acc)+1)\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.xlim(1, len(val_acc)+1)\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.xlim(1, len(val_acc)+1)\nplt.title('Training and Validation Loss')\nplt.show()","d5a10ac7":"### Model","e475adfb":"### NLP"}}