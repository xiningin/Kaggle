{"cell_type":{"06c90bdb":"code","144ab685":"code","74b3c9d0":"code","17796cc4":"code","5dfc8b45":"code","977a4fc4":"code","f0f8fc70":"code","56f8d700":"code","11dbc1eb":"code","ebfa2f58":"code","3bd7158f":"code","ae772cfa":"code","3572a6b4":"code","8065da0d":"code","edc45dd8":"code","c2bc765b":"code","73dba533":"code","bcdd637e":"code","65d8458b":"code","e4e35e81":"code","c1d792fb":"code","03eb79fc":"code","db592f8d":"code","bf05f62b":"code","84104976":"code","249821b4":"code","84354786":"code","2b8afcc5":"code","ca200f20":"code","a16948c4":"code","56623e92":"code","e1394f02":"code","fa703b8d":"code","8477fa0a":"code","51dd1c0b":"code","1bbf3dc0":"code","7c7f09ca":"code","85dba372":"code","73522246":"code","af56809a":"code","dd1638c7":"code","2984895c":"code","fc9e92ea":"code","c9ff35d5":"code","44330201":"code","7864c0a2":"code","42eba2a4":"code","080bf76b":"code","c1305fa8":"code","98053fbe":"code","ad5cd5e2":"code","f56e5502":"code","b960d714":"code","753e037b":"code","0a85b670":"code","4d839086":"code","b9b9d387":"code","0357712d":"code","17523eba":"code","c7f5a016":"code","03c13e4b":"code","b8e37ad0":"code","201f5ca6":"code","87d3b5d3":"code","c28ee909":"code","ca27cb55":"code","4d039ed3":"code","10319949":"code","2d1e537f":"code","801ae8ea":"markdown","c4fe3816":"markdown","ffdb6db8":"markdown","6cd14985":"markdown","ef9a184b":"markdown","1afe797d":"markdown","5b7906dc":"markdown","66be63fe":"markdown","da1842f7":"markdown","4fe6b23d":"markdown","dd255fcb":"markdown","673d1462":"markdown","e115b4bf":"markdown","53bedcdb":"markdown","03175d90":"markdown","ec731897":"markdown","8799d5b7":"markdown","0e1b3e64":"markdown","6adc81dc":"markdown","64aa44a7":"markdown","2bc6e8b3":"markdown","1933b871":"markdown","227f7022":"markdown","43157129":"markdown"},"source":{"06c90bdb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","144ab685":"import zipfile\nimport os\nwith zipfile.ZipFile('\/kaggle\/input\/sentiment-analysis-on-movie-reviews\/train.tsv.zip','r') as zip_ref:\n    zip_ref.extractall(\".\/sentiment-analysis-on-movie-reviews\/\")\nwith zipfile.ZipFile('\/kaggle\/input\/sentiment-analysis-on-movie-reviews\/test.tsv.zip','r') as zip_ref:\n    zip_ref.extractall(\".\/sentiment-analysis-on-movie-reviews\/\")","74b3c9d0":"data_source=pd.read_table(\"\/kaggle\/working\/sentiment-analysis-on-movie-reviews\/train.tsv\",sep='\\t')\ndata_source=data_source[['Phrase','Sentiment']].copy()\ndata_source","17796cc4":"data_source.Sentiment.values","5dfc8b45":"data_source.index.values","977a4fc4":"data_source.Phrase[:10]","f0f8fc70":"# split each sentence in blocks of 10 words which is going to be the size of the tokens.\n\ndff=[len(i.split(\" \")) for i in data_source.Phrase[:10]]\nmax(dff)","56f8d700":"dff","11dbc1eb":"from transformers import TFBertModel,  BertConfig, BertTokenizerFast, TFAutoModel\n\n# Then what you need from tensorflow.keras\nfrom tensorflow.keras.layers import Input, Dropout, Dense\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy\nfrom tensorflow.keras.utils import to_categorical\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# And pandas for data import + sklearn because you allways need sklearn\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split","ebfa2f58":"# train_test_split : Split arrays or matrices into random train and test subsets\nX_train_data, X_validation_data, y_train_data, y_validation_data = train_test_split(data_source.index.values,  # sequence number from 1 to 156060\n                                                  data_source.Sentiment.values,   # values 1 to 5\n                                                  test_size=0.15,    # should be between 0.0 and 1.0  .10 = 10% for testing and 90% for training\n                                                  random_state=42,   # Controls the shuffling applied to the data before applying the split\n                                                  stratify=data_source.Sentiment) # data is split in a stratified fashion, using this as the class labels\n\n","3bd7158f":"print(X_train_data)\nprint(X_validation_data)\nprint(y_train_data)","ae772cfa":"data_source.shape[0]","3572a6b4":"#sns.countplot(data_source)\nplt.xlabel('Review Score')","8065da0d":"# in Data_source file , assigning data-type 'training' to Training data and 'validation' to  validation data\n\ndata_source['data_type'] = ['not_set']*data_source.shape[0]\ndata_source.loc[X_train_data, 'data_type'] = 'training'\ndata_source.loc[X_validation_data, 'data_type'] = 'validation'","edc45dd8":"print(X_train_data.shape)\nprint(X_validation_data)\nprint(y_train_data)","c2bc765b":"data_source.isnull().sum()","73dba533":"X_train_data.shape, X_validation_data.shape\n","bcdd637e":"data_source[data_source.data_type=='validation'].Phrase","65d8458b":"print(data_source.Sentiment.values)\nprint(data_source.Phrase.values)","e4e35e81":"max_token_length = max(dff)+3   # Max length of tokens\nnumber_of_samples = len(data_source)\nbert = 'bert-base-cased'\nconfig = BertConfig.from_pretrained(bert)\nconfig.output_hidden_states = False\ntokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path = bert, config = config)\n","c1d792fb":"data_source[\"Sentiment\"].values","03eb79fc":"data_source['Phrase'].values","db592f8d":"sent_values_array = data_source[\"Sentiment\"].values\nlen(sent_values_array)\n#sent_values_array.max()","bf05f62b":"def map_function(input_ids, masks,labels):    # receives 3 tuples and returns 2 \n    return {'input_ids': input_ids, 'attention_mask': masks},labels","84104976":"# to_categorical : Converts a class vector (integers) to binary class matrix\n# converts training sentiment labels in to a binary  matrix  of shape  #items x # values    140454,  5 \nx_senti = to_categorical(data_source[data_source.data_type=='training'].Sentiment)  # x_senti.shape  140454,5\n\n# Tokenize the input \nx = tokenizer(\n    text=data_source[data_source.data_type=='training'].Phrase.to_list(), # list of all Phrases \n    add_special_tokens=True,\n    max_length=max_token_length,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\n\ntrain_dataset=tf.data.Dataset.from_tensor_slices((x['input_ids'], x['attention_mask'], x_senti)) # creates tensor  of 512,84,5\n\n","249821b4":"train_dataset = train_dataset.map(map_function)","84354786":"batch_size = 32\n# shuffle and batch - dropping any remaining samples that don't cleanly\ntrain_dataset = train_dataset.shuffle(1000).batch(batch_size, drop_remainder=True) #  created object of shape 32x 84  32 is the batch size","2b8afcc5":"x_senti.shape","ca200f20":"train_dataset","a16948c4":"train_dataset","56623e92":"x['attention_mask']","e1394f02":"x_senti.shape","fa703b8d":"x['input_ids']","8477fa0a":"x","51dd1c0b":"len(train_dataset)","1bbf3dc0":"y_senti = to_categorical(data_source[data_source.data_type=='validation'].Sentiment)\n\n# Tokenize the input \nv= tokenizer(\n    text=data_source[data_source.data_type=='validation'].Phrase.to_list(),\n    add_special_tokens=True,\n    max_length=max_token_length,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\n\nvalidation_dataset = tf.data.Dataset.from_tensor_slices((v['input_ids'], v['attention_mask'], y_senti))\nvalidation_dataset = validation_dataset.map(map_function)\nvalidation_dataset = validation_dataset.shuffle(1000).batch(batch_size, drop_remainder=True)","7c7f09ca":"max_token_length","85dba372":"# let's create a much nicer confusion matrix\ndef plot_confusion_matrix(y_test, y_pred, classes=None):\n  import itertools\n  figsize = (20, 20)\n  cm = confusion_matrix(y_test, tf.round(y_pred))\n\n  # let's normalize confusion matrix\n  cm_norm = cm.astype(\"float\") \/ cm.sum(axis=1)[:, np.newaxis]\n  n_classes = cm.shape[0]\n\n  # Let's prettify it\n  fig , ax = plt.subplots(figsize=figsize)\n\n  # create matrix plot\n  cax = ax.matshow(cm, cmap=plt.cm.Blues)\n  fig.colorbar(cax)\n\n  \n  if classes:\n    labels = classes\n  else:\n    labels = np.arange(cm.shape[0])\n\n  # Label the axis\n  ax.set(title=\"Confusion Matrix\",\n          xlabel = \"Predicted Label\",\n          ylabel = \"True Label\",\n          xticks = np.arange(n_classes),\n          yticks = np.arange(n_classes),\n          xticklabels = labels,\n          yticklabels = labels)\n\n  # set X-axis labels to bottom\n  ax.xaxis.set_label_position(\"bottom\")\n  ax.xaxis.tick_bottom()\n\n  # Adjust label size\n  ax.yaxis.label.set_size(20)\n  ax.xaxis.label.set_size(20)\n  ax.title.set_size(20)\n\n\n  # set threshold for different colors\n  threshold = ( cm.max() + cm.min()) \/2. \n\n  # plot text in each cell\n\n  for i,j  in itertools.product(range(cm.shape[0]),  range(cm.shape[1])):\n    plt.text(j, i, f\"{cm[i,j]} ({cm_norm[i,j]*100:.1f}%)\",\n                               horizontalalignment = \"center\",\n                               color = \"white\" if cm[i,j] > threshold else \"black\",\n                               size = 15)","73522246":"# Function to Plot the validation and training data separately\n\ndef plot_loss_curves(history):\n  \"\"\"\n  Returns separate loss curves for training and validation metrics.\n  \"\"\" \n  loss = history.history['loss']\n  val_loss = history.history['val_loss']\n\n  accuracy = history.history['accuracy']\n  val_accuracy = history.history['val_accuracy']\n\n  epochs = range(len(history.history['loss']))\n\n  # Plot loss\n  plt.plot(epochs, loss, label='training_loss')\n  plt.plot(epochs, val_loss, label='val_loss')\n  plt.title('Loss')\n  plt.xlabel('Epochs')\n  plt.legend()\n\n  # Plot accuracy\n  plt.figure()\n  plt.plot(epochs, accuracy, label='training_accuracy')\n  plt.plot(epochs, val_accuracy, label='val_accuracy')\n  plt.title('Accuracy')\n  plt.xlabel('Epochs')\n  plt.legend();","af56809a":"input_ids = tf.keras.Input(shape=(max_token_length,), name='input_ids', dtype='int32') # parameters of input layer\nattention_mask = tf.keras.Input(shape=(max_token_length,), name='attention_mask', dtype='int32')  # parameters of attention mask \ninputs = {'input_ids': input_ids, 'attention_mask': attention_mask}  # inputs will be the input of our model..  \nbert = TFAutoModel.from_pretrained('bert-base-cased')\nembeddings = bert.bert(inputs)[1]  #  access pooled activations with [1]  pool from 3 tensors to expecting  2 tensors\nxis = tf.keras.layers.Dense(1024,activation='relu')(embeddings)   # intermediate dense layer\nyhat = tf.keras.layers.Dense(sent_values_array.max()+1, activation='softmax', name='outputs')(xis)  # final output layer\n","dd1638c7":"input_ids","2984895c":"#model = tf.keras.Model(inputs=[train_dataset,mask], outputs=yhat)\nmodel = tf.keras.Model(inputs= inputs, outputs=yhat)   # inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\nmodel.summary()","fc9e92ea":"optimizer = tf.keras.optimizers.Adam(lr=2e-5, decay=1e-6)    #  lr=1e-5\nloss = tf.keras.losses.CategoricalCrossentropy()\naccuracy = tf.keras.metrics.CategoricalAccuracy('accuracy')","c9ff35d5":"model.compile(optimizer=optimizer, loss=loss, metrics=[accuracy])","44330201":"validation_dataset\n","7864c0a2":"history1 = model.fit(\n    train_dataset,  #train_dataset,\n    validation_data= validation_dataset,  #  validatio dataset\n    epochs=8)","42eba2a4":"plot_loss_curves(history1)","080bf76b":"model.save('.\/sentiment-analysis-on-movie-reviews\/Movie_sentiment_analysis_model')","c1305fa8":"#model.save('Movies_sentiment_analysis_model.h5')","98053fbe":"test=pd.read_table(\"\/kaggle\/working\/sentiment-analysis-on-movie-reviews\/test.tsv\",sep='\\t')","ad5cd5e2":"test","f56e5502":"data_source[data_source.data_type=='validation'].Phrase","b960d714":"x_test = tokenizer(\n    text=test.Phrase.to_list(),\n    add_special_tokens=True,\n    max_length=max_token_length,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)","753e037b":"x_test","0a85b670":"test_items=tf.data.Dataset.from_tensor_slices((x_test['input_ids'],x_test['attention_mask']))","4d839086":"def map_func(input_ids, masks):\n    return {'input_ids': input_ids, 'attention_mask': masks}\n\ntest_items = test_items.map(map_func)\ntest_items = test_items.batch(32)","b9b9d387":"test_items","0357712d":"predictions=model.predict(test_items).argmax(axis=-1)","17523eba":"\nprediction2=model.predict(validation_dataset).argmax(axis=-1)\n","c7f5a016":"prediction2.min()","03c13e4b":"pred=prediction2+1\npred.max()\n","b8e37ad0":"y_validation=y_validation_data[:23392]\ny_validation.min()","201f5ca6":"y_validation.shape, pred.shape","87d3b5d3":"sentiments = [0,1,2,3,4]\n#test_sent_array = test[\"Sentiment\"].values\nfrom sklearn.metrics import confusion_matrix\n\nplot_confusion_matrix(y_test=y_validation,\n                      y_pred=prediction2,\n                      classes=sentiments)","c28ee909":"submission = pd.DataFrame()\nsubmission['PhraseId'] = test['PhraseId']\nsubmission['Sentiment'] = predictions\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()","ca27cb55":"new_model =load_model('.\/sentiment-analysis-on-movie-reviews\/Movie_sentiment_analysis_model')\nnew_model.summary()\n","4d039ed3":"%load_ext tensorboard","10319949":"pip install -U tensorboard-plugin-profile\n","2d1e537f":"%tensorboard --port=5036 --logdir tb_callback_dir","801ae8ea":"### Let's check the parameters of dataset:  x[input_ids], x[attention_mask],  x_senti","c4fe3816":"## Importing libraries and tools","ffdb6db8":"#### This function receives 3 parameters and return two parameters:  One dictionary {'input_ids', 'attention_mask'},  labels\n* Note that 'input_ids and  'attention_mask' will be the model's input data","6cd14985":"## Conclusions and acknowledgements*","ef9a184b":"## Defining layers that will fit the model","1afe797d":"## Tokenizing test dataset","5b7906dc":"### in Data_source file , assigning data-type 'training' to Training data and 'validation' to  validation data\n* I  use X_train_data  and X_validation_data to localise the corresponding records within the file  data_source","66be63fe":"## Creating the submission file","da1842f7":"### Let's take a look at X_train and X_validation data and shape","4fe6b23d":"## Trainig the model\n*  Using Training and validation data","dd255fcb":"##  Making predictions","673d1462":"## Building the model","e115b4bf":"## Note: Data taken from Kaggle dataset,  Ideas taken from kaggle-code, Youtube videos,and  from Neural Network and Deep Learning especialization. Course # 5 - Sequence Models - Transformers ....Dr  Andrew Ng. Table of Content.","53bedcdb":"### Compiling the model","03175d90":"### Defining sentiment values or labels ( 1,2,3,4,5 )","ec731897":"## Defining validation_dataset","8799d5b7":"## Defining parameters for the model\n*  Note that the bert model and tokenizer function is needed tp create the dataset","0e1b3e64":"## Saving the model","6adc81dc":"### Defining Adam optimizer, loss and accuracy functions..","64aa44a7":"### Splitting the data for training and validation\n*  we are going to use 90% for training and 10% for validation\n*  Note that X_train_data, X_validation_fdata, y_train_data and y_validation_data store data's indices","2bc6e8b3":" ## Reading source data\n *  Data_source sotre all the data I am going to work with.\n *  Later on I am going to split the data_source between  train and validation data\n * The content includes index, Phtase and Sentiment ( 1 to 5 )\n * The shape of data_source is   ( 156060, 2 )","1933b871":"## Reading test data","227f7022":"## Movie review sentiment Model.\n","43157129":"## Defining train_dataset\n*  Tokenization is the process of breaking up a string into tokens. Commonly, these tokens are words, numbers, and\/or punctuation. The tensorflow_text package provides a number of tokenizers available for preprocessing text required by your text-based models. By performing the tokenization in the TensorFlow graph, you will not need to worry about differences between the training and inference workflows and managing preprocessing scripts."}}