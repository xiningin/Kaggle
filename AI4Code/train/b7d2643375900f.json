{"cell_type":{"9efff121":"code","47bd3494":"code","501e2877":"code","256bec7c":"code","13ebe99e":"code","86d0ae8e":"code","c30200bf":"code","5c7cea4f":"code","202ff50c":"code","3d61f7dd":"code","ad0b5689":"code","878a0dd8":"code","0428eea1":"code","bd855107":"code","ab6c9921":"code","05ba7d61":"code","003c9731":"code","7b27cd04":"code","420ea3ba":"code","2422bd49":"code","b58a0f7f":"code","26d758bf":"code","c39bbdc8":"code","b9801170":"code","8dd9b415":"code","64c3ab06":"code","5c97303f":"code","b06bcace":"code","c5fd9d68":"code","e1e0e542":"code","023049c4":"code","7ef7b09a":"code","e47bc742":"code","048d46e4":"code","a1c89f53":"code","4d8b8f95":"code","d5910846":"code","42565db8":"code","b5e2ceae":"code","250790f3":"code","83c7046d":"code","f82fd69c":"code","050d8a50":"code","708b4652":"code","bc21d3b7":"code","48475329":"code","04a6efeb":"code","2221ebb6":"code","304bc8af":"code","8668f056":"code","cd676179":"code","03cb5482":"code","98d29ff9":"code","a8ff236c":"code","3a5545b3":"code","a9b4d98b":"code","efcc68c9":"code","99f72265":"code","74c6138b":"code","f24b050c":"code","3a836078":"markdown","88d93dd3":"markdown","5bd86c04":"markdown","7e833e31":"markdown","deb18784":"markdown","8da0494a":"markdown","64013348":"markdown","e2476f69":"markdown","5e224647":"markdown","a072832b":"markdown","f867e2b1":"markdown","f0c3e774":"markdown","2dbca017":"markdown","340f6c7c":"markdown","94d9de9e":"markdown","7e658084":"markdown","90296b4d":"markdown","f5802b9f":"markdown","3ecd8635":"markdown"},"source":{"9efff121":"path = '..\/input\/'\nimport os\nfor path, dirs, files in os.walk(f'{path}'):\n    print(path)\n    for f in files:\n        print(f)","47bd3494":"path = '..\/input\/gensim-word-vectors\/'\nfrom gensim.models import KeyedVectors\n\nGLOVE_TWITTER = f'{path}glove-twitter-100\/glove-twitter-100'\ntwitter_model = KeyedVectors.load_word2vec_format(GLOVE_TWITTER)","501e2877":"GLOVE_WIKI = f'{path}glove-wiki-gigaword-300\/glove-wiki-gigaword-300'\nwiki_model = KeyedVectors.load_word2vec_format(GLOVE_WIKI)","256bec7c":"twitter_model.most_similar(\"arms\")","13ebe99e":"wiki_model.most_similar(\"arms\")","86d0ae8e":"wiki_model.most_similar(\"cloud\")","c30200bf":"twitter_model.most_similar(\"cloud\")","5c7cea4f":"wiki_model.most_similar(\"occupy\")","202ff50c":"twitter_model.most_similar(\"occupy\")","3d61f7dd":"wiki_model.most_similar(\"cluod\")","ad0b5689":"twitter_model.most_similar(\"cluod\")","878a0dd8":"twitter_model.most_similar(\"foriegn\")","0428eea1":"wiki_model.most_similar(\"foriegn\")","bd855107":"POSITIVE_LIST = ['woman', 'king']\nNEGATIVE_LIST = ['man']\ntwitter_model.most_similar(positive=POSITIVE_LIST, negative=NEGATIVE_LIST)","ab6c9921":"wiki_model.most_similar(positive=POSITIVE_LIST, negative=NEGATIVE_LIST)","05ba7d61":"POSITIVE_LIST = ['woman', 'programmer']\nNEGATIVE_LIST = ['man']\ntwitter_model.most_similar(positive=POSITIVE_LIST, negative=NEGATIVE_LIST)","003c9731":"wiki_model.most_similar(positive=POSITIVE_LIST, negative=NEGATIVE_LIST)","7b27cd04":"POSITIVE_LIST = ['woman', 'doctor']\nNEGATIVE_LIST = ['man']\ntwitter_model.most_similar(positive=POSITIVE_LIST, negative=NEGATIVE_LIST)","420ea3ba":"wiki_model.most_similar(positive=POSITIVE_LIST, negative=NEGATIVE_LIST)","2422bd49":"POSITIVE_LIST = ['man', 'doctor']\nNEGATIVE_LIST = ['woman']\ntwitter_model.most_similar(positive=POSITIVE_LIST, negative=NEGATIVE_LIST)","b58a0f7f":"wiki_model.most_similar(positive=POSITIVE_LIST, negative=NEGATIVE_LIST)","26d758bf":"GLOVE_TWITTER_S = f'{path}glove-twitter-25\/glove-twitter-25'\ntwitter_model_s = KeyedVectors.load_word2vec_format(GLOVE_TWITTER_S)","c39bbdc8":"twitter_model.most_similar(\"arms\")","b9801170":"twitter_model_s.most_similar(\"arms\")","8dd9b415":"twitter_model.most_similar(\"cloud\")","64c3ab06":"twitter_model_s.most_similar(\"cloud\")","5c97303f":"twitter_model.most_similar(positive=['woman', 'king'], negative=['man'])","b06bcace":"twitter_model_s.most_similar(positive=['woman', 'king'], negative=['man'])","c5fd9d68":"# Load spaCy and the English language model\nimport spacy\nimport en_core_web_lg\nnlp = en_core_web_lg.load()","e1e0e542":"phrase = \"NLP is so fun!\"\ndoc = nlp(phrase)\nprint(f'spaCy vectors are {len(doc[3].vector)} dimensions long')\n# Get the vector for 'fun':\nprint(f'First 20 values of vector for \"{doc[3]}\"\\n', doc[3].vector[:20])","023049c4":"# Mean vector for the entire sentence\nprint(f'First 20 values of vector for phrase \"{phrase}\"\\n', doc.vector[:20])","7ef7b09a":"doc[0].similarity(doc[3])","e47bc742":"phrase = \"NLP is so bad!\"\ndoc2 = nlp(phrase)\ndoc2[0].similarity(doc2[3])","048d46e4":"doc[3].similarity(doc2[3])","a1c89f53":"doc.similarity(doc2)","4d8b8f95":"sentence_obama = 'Obama speaks to the media in Illinois'\nsentence_president = 'The President greets the press in Chicago'\nobama = nlp(sentence_obama)\npresident = nlp(sentence_president)\nobama.similarity(president)","d5910846":"sentence_obama2 = 'Obama speaks in Illinois'\nobama2 = nlp(sentence_obama2)\nobama2.similarity(president)","42565db8":"sentence_nlp = 'NLP is so fun!'\nnlp_fun = nlp(sentence_nlp)\nobama2.similarity(nlp_fun)","b5e2ceae":"president.similarity(nlp_fun)","250790f3":"import pandas as pd\nimport numpy as np\nimport spacy\nimport en_core_web_lg\nnlp = en_core_web_lg.load()","83c7046d":"path = '..\/input\/bag-of-words-meets-bags-of-popcorn-\/'\ntrain = pd.read_csv(f'{path}labeledTrainData.tsv', header = 0, delimiter = '\\t', quoting = 3)\ntest = pd.read_csv(f'{path}testData.tsv', header = 0, delimiter = '\\t', quoting = 3)\nprint(f'train dim:{train.shape}, test dim:{test.shape}')","f82fd69c":"train.head()","050d8a50":"features = []\nfor index, row in train.iterrows():\n   doc = nlp(row[\"review\"])\n   features.append(doc.vector)\n    \nfeatures_test = []\nfor index, row in test.iterrows():\n   doc = nlp(row[\"review\"])\n   features_test.append(doc.vector)","708b4652":"labels = train[\"sentiment\"]\nfeatures = train_vectors\n\nlabels_test = test[\"sentiment\"]\nfeatures_test = test_vectors","bc21d3b7":"from sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, classification_report\n\nclf = LinearSVC() # Whatever model name FOR CLASSIFICATION\nclf.fit(features, labels)\npreds_test = clf.predict(features_test)\n\nprint(classification_report(labels_test,\n                           preds_test,\n                           target_names=train[\"sentiment\"].unique()))","48475329":"from string import punctuation as sp\nimport re\nfrom spacy.lang.en import English\nparser = English()\nimport en_core_web_lg\nnlp = en_core_web_lg.load()\nfrom gensim.models import word2vec","04a6efeb":"print(nlp.Defaults.stop_words)\nprint(len(nlp.Defaults.stop_words))","2221ebb6":"nlp.Defaults.stop_words.add(\"my_new_stopword\")\nprint(len(nlp.Defaults.stop_words))","304bc8af":"nlp.Defaults.stop_words.remove(\"my_new_stopword\")\nprint(len(nlp.Defaults.stop_words))","8668f056":"STOPLIST = nlp.Defaults.stop_words\nSYMBOLS = \" \".join(sp).split(\" \") + [\"-\", \"...\", \"\u201d\", \"\u201d\"]\nfrom bs4 import BeautifulSoup\n\ndef lemmatizeText(document):\n    '''\n    Removes html tags\n    Replaces newlines, carriage returns and multiple spaces with a single space\n    Uncases text\n    Parses text into lemmas excluding stopwords, symbols and pronouns\n    '''\n    soup = BeautifulSoup(document)\n    text = soup.get_text(\" \")\n    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\s\\s+\", \" \")\n    text = text.lower()\n    tokens = nlp(text)\n    lemmas = []\n    for tok in tokens:\n        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n    tokens = lemmas\n    tokens = [tok for tok in tokens if tok not in STOPLIST]\n    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n    return tokens","cd676179":"lemmatizeText(train[\"review\"][0])","03cb5482":"import time\n\ncorpus = []\nstart = time.time()\nfor index, row in train.iterrows():\n   lemmatized_rev = lemmatizeText(row['review'])\n   corpus.append(lemmatized_rev)\n   if((index % 500)==0):\n        end = time.time()\n        print('{} rows processed in {} seconds'.format(index,end-start))\n        start = time.time() ","98d29ff9":"word_count = 0\nfor doc in corpus:\n   word_count += len(doc)\nprint(f'The corpus has {len(corpus)} documents and {word_count} words')","a8ff236c":"\nfrom gensim.models import Phrases\nfrom gensim.models.phrases import Phraser\nimport time\n\nphrases = Phrases(sentences=corpus,min_count=25,threshold=50)\nbigram = Phraser(phrases)\nstart = time.time() \nfor index,sentence in enumerate(corpus):\n    corpus[index] = bigram[sentence]\n    if((index % 5000)==0):\n        end = time.time()\n        print('{} rows processed in {} seconds'.format(index,end-start))\n        start = time.time() \n   ","3a5545b3":"start =  time.time()\nmodel = word2vec.Word2Vec(corpus, workers = 4, size = 100, min_count = 40, window = 10, sample = 0.0001)\nend = time.time()\nprint(end-start)","a9b4d98b":"model.init_sims(replace = True)\nmodel.save(fname_or_handle = \"w2v_imdb_100d\")","efcc68c9":"# class MySentences(object):\n#     def __init__(self, dirname):\n#         self.dirname = dirname\n \n#     def __iter__(self):\n#         for fname in os.listdir(self.dirname):\n#             for line in open(os.path.join(self.dirname, fname)):\n#                 yield line.split()\n \n# #sentences = MySentences('\/some\/directory') # a memory-friendly iterator","99f72265":"import numpy as np\nfrom sklearn.manifold import TSNE\nimport bokeh\nfrom gensim.models import Word2Vec\nimport matplotlib.pyplot as plt\n\nimdb_model =  gensim.models.Word2Vec.load(fname_or_handle='w2v_imdb_100d')","74c6138b":"def display_closestwords_tsnescatterplot(model, word):\n    arr = np.empty((0,100), dtype='f')\n    word_labels = [word]\n    # get close words\n    close_words = model.similar_by_word(word)\n    # add the vector for each of the closest words to the array\n    arr = np.append(arr, np.array([model[word]]), axis=0)\n    for wrd_score in close_words:\n        wrd_vector = model[wrd_score[0]]\n        word_labels.append(wrd_score[0])\n        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n        \n    # find tsne coords for 2 dimensions\n    tsne = TSNE(n_components=2, random_state=0)\n    np.set_printoptions(suppress=True)\n    Y = tsne.fit_transform(arr)\n\n    x_coords = Y[:, 0]\n    y_coords = Y[:, 1]\n    # display scatter plot\n    plt.scatter(x_coords, y_coords)\n\n    for label, x, y in zip(word_labels, x_coords, y_coords):\n        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n    plt.show()","f24b050c":"display_closestwords_tsnescatterplot(model, 'chef')\n","3a836078":"This blogpost describes a strategy for [correcting spelling using word embeddings](https:\/\/blog.usejournal.com\/a-simple-spell-checker-built-from-word-vectors-9f28452b6f26)","88d93dd3":"#### Analogies\nThe classical example: **man::king as woman::?**","5bd86c04":"The ubiqutous example of bias:\n**man::programmer as woman::?**","7e833e31":"Comparing 'good' to 'bad'","deb18784":"### Loading and comparing pretrained word embeddings\nWe load a few different models to compare how they evaluate similar queries","8da0494a":"Before we start transforming and processing text, we want to look at what are standard features in the libraries.","64013348":"# Intermediate Natural Language Processing (NLP)\n## Real World Applications of Word Embeddings","e2476f69":"Comparing 'NLP' to 'bad'","5e224647":"Comparing 'NLP is good' to 'NLP is bad'","a072832b":"**man::doctor as woman::?**","f867e2b1":"Corpuses have different emphases","f0c3e774":"### Notebook Organization:\n- Loading and comparing pretrained word embeddings\n- Applying word embeddings to a problem\n- Training your own embeddings\n","2dbca017":"Comparing 'NLP' to 'Good'","340f6c7c":"### Applying word embeddings to a ML pipeline\nDataset: [IMDB Data set for NLP analysis](https:\/\/www.kaggle.com\/rajathmc\/bag-of-words-meets-bags-of-popcorn-#labeledTrainData.tsv)","94d9de9e":"And now the reverse:\n**woman::doctor as man::?**","7e658084":"#### Spelling","90296b4d":"References:\n- Training word2vec embeddings: https:\/\/rare-technologies.com\/word2vec-tutorial\/\n- Sentiment analysis using word2vec: https:\/\/www.kaggle.com\/kyen89\/2-sentiment-analysis-word2vec\n- Data streaming using generators: https:\/\/rare-technologies.com\/data-streaming-in-python-generators-iterators-iterables\/\n- Using GloVE + Keras: https:\/\/www.kaggle.com\/jhoward\/improved-lstm-baseline-glove-dropout","f5802b9f":"### Training your own embeddings","3ecd8635":"### Dimensionality: a curse or not?\nNow we compare two GloVE models trained on the same (Twitter) data, one which is represented by vectors of 100 dimensions and another with 25 m=dimensions. "}}