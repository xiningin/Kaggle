{"cell_type":{"1e4c7b7c":"code","d0bf8955":"code","5661a8c0":"code","a3e55c42":"code","b033eacf":"code","ce93c6fc":"code","6ee0ba55":"code","a06bd872":"code","667e7149":"code","2996ed68":"code","46775132":"code","7ea28f48":"code","966ffca2":"code","091e7c96":"code","7748edd9":"code","52a4f667":"code","d9b96bea":"code","5f38b3a6":"code","89c90c92":"code","f17ec4d5":"code","bd94d14a":"code","bf87e601":"code","4eebb111":"code","d54dd369":"code","491b01b3":"code","6fbd5cef":"code","a66b1c33":"code","39863c0e":"code","1e68a175":"markdown","73c4275d":"markdown","9ce9a8ae":"markdown","e0110f07":"markdown","a06a8a53":"markdown","613b16fe":"markdown","5d90dc11":"markdown","9a23938a":"markdown"},"source":{"1e4c7b7c":"#Importing the essential libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","d0bf8955":"#Loading the dataset\n\ndata =pd.read_csv('..\/input\/wine-quality-selection\/winequality-white.csv')","5661a8c0":"#Exploring data \ndata.head()","a3e55c42":"data.shape","b033eacf":"data.info()","ce93c6fc":"data.describe()","6ee0ba55":"data['quality'].unique()","a06bd872":"#Checking for correlation \nfig, ax = plt.subplots(figsize=(8,8)) \nsns.heatmap(data.corr(), cmap=\"YlGnBu\", annot=True, ax=ax)","667e7149":"#Checking for outliers\nsns.set()\nplt.figure(figsize=(20,10))\nsns.boxplot(data=data,palette=\"Set3\")\nplt.show()","2996ed68":"lower_limit = data[\"free sulfur dioxide\"].mean() - 3*data[\"free sulfur dioxide\"].std()\nupper_limit = data[\"free sulfur dioxide\"].mean() + 3*data[\"free sulfur dioxide\"].std()\n\ndata = data[(data[\"free sulfur dioxide\"] > lower_limit) & (data[\"free sulfur dioxide\"] < upper_limit)]","46775132":"lower_limit = data['total sulfur dioxide'].mean() - 3*data['total sulfur dioxide'].std()\nupper_limit = data['total sulfur dioxide'].mean() + 3*data['total sulfur dioxide'].std()\ndata = data[(data['total sulfur dioxide'] > lower_limit) & (data['total sulfur dioxide'] < upper_limit)]","7ea28f48":"lower_limit = data['residual sugar'].mean() - 3*data['residual sugar'].std()\nupper_limit = data['residual sugar'].mean() + 3*data['residual sugar'].std()\ndata = data[(data['residual sugar'] > lower_limit) & (data['residual sugar'] < upper_limit)]\n","966ffca2":"data.shape","091e7c96":"#Updated box plot\nsns.set()\nplt.figure(figsize=(20,10))\nsns.boxplot(data=data,palette=\"Set3\")\nplt.show()","7748edd9":"#Dropping the dependent variable from the data\ndf=data.iloc[:,:-1]\ny=data.iloc[:,-1]","52a4f667":"#K-Means Clustering\n\nfrom sklearn.cluster import KMeans\n\nclusters = []\n\nfor i in range(1, 11):\n    km = KMeans(n_clusters=i).fit(df)\n    clusters.append(km.inertia_)\n    \nfig, ax = plt.subplots(figsize=(12, 8))\nsns.lineplot(x=list(range(1, 11)), y=clusters, ax=ax)\nax.set_title('Searching for Elbow')\nax.set_xlabel('Clusters')\nax.set_ylabel('Inertia')\n\nplt.show()","d9b96bea":"#Standardizing the data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndf = scaler.fit_transform(df)","5f38b3a6":"#Using dimensionality reduction\nfrom sklearn.decomposition import PCA\npca = PCA(2)\ndf = pca.fit_transform(df)","89c90c92":"#Applying kmeans to PCA reduced data\n#Applying kmeans to the dataset \/ Creating the kmeans classifier\nfrom sklearn import metrics\nkmeans = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\ny_kmeans = kmeans.fit_predict(df)\n\n#print metrics for reduced data\nprint('With PCA')\nprint('Homogeneity: {}'.format(metrics.homogeneity_score(y, kmeans.labels_)))\nprint('Completeness: {}'.format(metrics.completeness_score(y,kmeans.labels_)))\nprint('V-measure: {}'.format(metrics.v_measure_score(y,kmeans.labels_)) )\n","f17ec4d5":"#Getting unique labels\n \nu_labels = np.unique(kmeans.labels_)\n \n#plotting the results:\n \nfor i in u_labels:\n    plt.scatter(df[kmeans.labels_ == i , 0] , df[kmeans.labels_ == i , 1] , label = i)\nplt.legend()\nplt.show()","bd94d14a":"data['cluster']=y_kmeans","bf87e601":"data.head()","4eebb111":"plt.figure(figsize=(12, 8))\nsns.scatterplot(data['density'], data['alcohol'], hue=data['cluster'], \n                palette=sns.color_palette('hls', 3))\nplt.title('KMeans with 3 Clusters')\nplt.show()","d54dd369":"#Better visualization\nfig = plt.figure(figsize=(20,8))\nax = fig.add_subplot(121)\nsns.swarmplot(x='cluster', y='alcohol', data=data, ax=ax)\n","491b01b3":"from sklearn.cluster import MeanShift, estimate_bandwidth\nfig = plt.figure(figsize=(20,15))\nX= data\n##### KMeans #####\nax = fig.add_subplot(221)\n\nkm5 = KMeans(n_clusters=2).fit(df)\nX['Labels'] = km5.labels_\nsns.scatterplot(X['density'], X['alcohol'], hue=X['Labels'], style=X['Labels'],\n                palette=sns.color_palette('hls', 2), s=60, ax=ax)\nax.set_title('KMeans with 2 Clusters')\n\nfrom sklearn.cluster import AgglomerativeClustering \n##### Agglomerative Clustering #####\nax = fig.add_subplot(222)\n\nagglom = AgglomerativeClustering(n_clusters=2, linkage='average').fit(df)\nX['Labels'] = agglom.labels_\nsns.scatterplot(X['density'], X['alcohol'], hue=X['Labels'], style=X['Labels'],\n                palette=sns.color_palette('hls', 2), s=60, ax=ax)\nax.set_title('Agglomerative with 2 Clusters')\n\nfrom sklearn.cluster import MeanShift, estimate_bandwidth\n##### MEAN SHIFT #####\nax = fig.add_subplot(223)\n\nbandwidth = estimate_bandwidth(df, quantile=0.1)\nms = MeanShift(bandwidth).fit(df)\nX['Labels'] = ms.labels_\nsns.scatterplot(X['density'], X['alcohol'], hue=X['Labels'], style=X['Labels'], s=60,\n                palette=sns.color_palette('hls', np.unique(ms.labels_).shape[0]), ax=ax)\nax.set_title('MeanShift')\n\nplt.tight_layout()\nplt.show()","6fbd5cef":"#Bining the wine quality to be good or bad\n\nbins = (2,5.5,10) #classifying quality level below 6 as bad and above as good\nlabels = [0,1] #0 for bad, 1 for good\ndata['quality'] = pd.cut(data['quality'],bins=bins,labels=labels)","a66b1c33":"data.head()","39863c0e":"from sklearn.metrics import accuracy_score,confusion_matrix\nkmeans = KMeans(n_clusters=2).fit(df)\nkmeans_predict = kmeans.predict(df)\n\nkm_cm = confusion_matrix(kmeans_predict,data['quality'])\nax = sns.heatmap(km_cm,annot=True)\nax.set(xlabel='predict', ylabel='true')\nkm_as = accuracy_score(kmeans_predict,data['quality'])\nprint(\"KMeans clustering accuracy score: \",km_as)","1e68a175":"Project Objective: Given a wine dataset, apply unsupervised learning algorithms to it to determine whether the wine is good or not","73c4275d":"There are no negative values and the wines are qualified into 6 different types on the basis of quality.","9ce9a8ae":"The last algorithm automatically estimates the number of clusters to be 5 but the prominent clusters are 2. ","e0110f07":"Correlation exists between parameters like 'density' and 'residual sugar' but not this much to eliminate one of the features","a06a8a53":"Comparing different clustering algorithms to decide the number of classes: With 3 clusters, there was lot of overlapping, so tried for 2 clusters\n","613b16fe":"Some outliers can be seen for free and total sulfur dioxide and residual sugar. Removing that ","5d90dc11":"Possible number of clusters= 2 or 3","9a23938a":"Dataset has no missing value and 12 columns. The last column 'quality' is the labeled result that I will be using to determine the accuracy of my clustering algorithms"}}