{"cell_type":{"b066981a":"code","651d0c66":"code","9ad30531":"code","e51c311b":"code","68ea2b93":"code","724a7c78":"code","435564da":"code","a4cfcb4e":"code","531de873":"code","f54cc255":"markdown","c0a9c3c2":"markdown","35f0e931":"markdown","4699fe3d":"markdown","c86b5c01":"markdown","5f62a1ba":"markdown","850c8675":"markdown","52f135bb":"markdown"},"source":{"b066981a":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport pylab as py\n\npd.options.mode.chained_assignment = None \ndf = pd.read_csv('..\/input\/titanic\/train.csv' )\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv' )\n\nprint(df.isnull().sum())\n\ndf['Age'] = df['Age'].fillna(df['Age'].median())## setting_null_vals to -100\ndf = df.drop(['Cabin'], axis=1)\ndf = df.dropna(axis=0, subset=['Embarked'])\n\nfor step,i in enumerate(df['Name']):\n    df['Name'].iloc[step] = str(len(i))\n\nfor step,i in enumerate(df['Ticket']):\n    df['Ticket'].iloc[step] = str(len(i))\n\n'''df['Cabin'] = df['Cabin'].astype('string')\nfor step,i in enumerate(df['Cabin']):\n    if type(df['Cabin'].iloc[step]) == str:\n        df['Cabin'].iloc[step] = str(len(i))\n    else:\n        df['Cabin'].iloc[step] = '0' '''\ndic = {\n    'Q':'1',\n    'C':'2',\n    'S':'3'\n}\n\ndf['Embarked']=(df['Embarked']).astype('string')\nfor step,i in enumerate(df['Embarked']):\n    if type(df['Embarked'].iloc[step]) == str and df['Embarked'].iloc[step] != '-100':\n        df['Embarked'].iloc[step] = dic.get(i)\ndic = {\n    'male':'0',\n    'female':'1'\n}\n\n\ndf['Sex']=(df['Sex']).astype('string')\nfor step,i in enumerate(df['Sex']):\n    if type(df['Sex'].iloc[step]) == str:\n        df['Sex'].iloc[step] = dic.get(i)\n\n       \ndf = df.astype('float64')\ntrain = df\n\n## Frequency Distrib. w.r.t all features -->>\n\nplot = sns.displot(data=df, x=\"Age\", col=\"Survived\", kde=True, height = 2 )\nplot.fig.suptitle(\"Freq_distribution\",\n                  fontsize=20, fontdict={\"weight\": \"bold\"}, x = 1.5)\nsns.displot(data=df, x='Embarked', col='Survived', kde=True, height=2)\nsns.displot(data=df, x=\"Name\", col=\"Survived\", kde=True, height = 2) \nsns.displot(data=df, x=\"Ticket\", col=\"Survived\", kde=True, height = 2) \nsns.displot(data=df, x=\"Parch\", col=\"Survived\", kde=True, height = 2) \nsns.displot(data=df, x=\"Sex\", col=\"Survived\", kde=True, height = 2)\nsns.displot(data=df, x=\"Fare\", col=\"Survived\", kde=True, height = 2)\n\npy.show()\n","651d0c66":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.preprocessing import StandardScaler, Normalizer\n\n\ndf_new = train\n\nprint(df_new.shape)\ndf_new = df_new.dropna()\n\n\ndf_label = df_new['Survived']\ndf_train = df_new.drop(['PassengerId'], axis=1)\ndf_corr = df_train.corr() \nsns.heatmap(data = df_corr, square=True, annot=True)\npy.show()\n\nprint(df_train.corr()['Survived'].sort_values())\n\ndf_train = df_train.drop(['Survived'], axis=1)\nprint(df_train.shape)\n\npy.figure(figsize = (10,8))\n\nx_train = df_train.to_numpy()\ny_train = df_label.to_numpy()\n\nfs = SelectKBest(score_func=f_classif, k=8)\nx_selected = fs.fit_transform(x_train,y_train)\n\n## Scale the Data to compare on equal gounds\nx_train = x_selected\nx_train = StandardScaler().fit_transform(x_train)\nprint('x_train_shape: ',x_train.shape)\n\ncols = df_train.columns\ncols = (cols).to_numpy()\nsel_cols = cols[fs.get_support()]\nprint('Selected_Columns: ', sel_cols)","9ad30531":"df_train = pd.DataFrame(x_train, columns=sel_cols)\ndf_train.info()\ndf_e = pd.get_dummies(df_train['Embarked'], drop_first=True)\ndf_p = pd.get_dummies(df_train['Pclass'], drop_first=True)\n\ndf_train_final = pd.concat([df_train.drop(['Embarked', 'Pclass'], axis=1), df_e, df_p], axis=1)\n\nx_train = df_train_final.to_numpy()\nprint('x_train: ', x_train.shape)","e51c311b":"df = df_test #----For test_set_creation----- \n\nprint(df.isnull().sum())\n\ndf['Age'] = df['Age'].fillna(df['Age'].median())\ndf = df.drop(['Cabin'], axis=1)\ndf = df.dropna(axis=0, subset=['Embarked'])\ndf = df.dropna(axis=0, subset=['Fare'])\n\n\nfor step,i in enumerate(df['Name']):\n    df['Name'].iloc[step] = str(len(i))\n\nfor step,i in enumerate(df['Ticket']):\n    df['Ticket'].iloc[step] = str(len(i))\n\n'''df['Cabin'] = df['Cabin'].astype('string')\nfor step,i in enumerate(df['Cabin']):\n    if type(df['Cabin'].iloc[step]) == str:\n        df['Cabin'].iloc[step] = str(len(i))\n    else:\n        df['Cabin'].iloc[step] = '0' '''\ndic = {\n    'Q':'1',\n    'C':'2',\n    'S':'3'\n}\n\ndf['Embarked']=(df['Embarked']).astype('string')\nfor step,i in enumerate(df['Embarked']):\n    if type(df['Embarked'].iloc[step]) == str and df['Embarked'].iloc[step] != '-100':\n        df['Embarked'].iloc[step] = dic.get(i)\ndic = {\n    'male':'0',\n    'female':'1'\n}\n\n\ndf['Sex']=(df['Sex']).astype('string')\nfor step,i in enumerate(df['Sex']):\n    if type(df['Sex'].iloc[step]) == str:\n        df['Sex'].iloc[step] = dic.get(i)\ntest = df.astype('float64')\n\n##-----------For Test_set_creation------------------\ntest = df[sel_cols]\nsub_test = test.to_numpy()\nprint('Test_set_shape: ', sub_test.shape)\n##--------------------------------------------------\n\ndf = pd.DataFrame(sub_test, columns=sel_cols)\ndf.info()\ndf_e = pd.get_dummies(df['Embarked'], drop_first=True)\ndf_p = pd.get_dummies(df['Pclass'], drop_first=True)\n\ndf_final = pd.concat([df.drop(['Embarked', 'Pclass'], axis=1), df_e, df_p], axis=1)\n\nsub_test = df_final.to_numpy()\nprint('sub_test: ', sub_test.shape)\n\nsub_test = StandardScaler().fit_transform(sub_test)","68ea2b93":"from scipy.stats import uniform, randint\nimport xgboost as xgb\nfrom sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\nfrom sklearn.model_selection import cross_val_score, RandomizedSearchCV, train_test_split\nfrom sklearn.decomposition import PCA\n\nprint(x_train.shape, y_train.shape)\n\nx_t, x_val, y_t, y_val = train_test_split(x_train,y_train)\nx_val, x_test, y_val, y_test = train_test_split(x_val, y_val)\nprint('--')\nprint(x_t.shape, y_t.shape)\nprint(x_val.shape, y_val.shape)\nprint(x_test.shape, y_test.shape)\nprint('--')\n\n\nparams = {\n    \"colsample_bytree\": uniform(0.7, 0.3),\n    \"gamma\": uniform(0, 0.5),\n    \"learning_rate\": uniform(0.003, 0.3), # default 0.1 \n    \"max_depth\": randint(2, 6), # default 3\n    \"n_estimators\": randint(100, 500), # default 100\n    \"subsample\": uniform(0.6, 0.4)\n}\n\nxgb_model = xgb.XGBClassifier(eval_metric=['auc', 'error'], random_state = 1, use_label_encoder=False)\n\nsearch = RandomizedSearchCV(xgb_model, param_distributions=params, random_state =1, n_iter=200, cv=3,\n                            verbose=0, n_jobs=1, return_train_score=True) #cv : cross_val Split strat.\nsearch.fit(x_t, y_t)\nbest_param = search.best_params_\nprint('-')\nprint(search.best_score_)\nprint(best_param)\nprint('-')\n\nxgb_model = xgb.XGBClassifier(eval_metric=['auc', 'error'], random_state = 1, use_label_encoder=False, **best_param)\n\nxgb_model.fit(x_t, y_t, eval_set = [(x_val, y_val)],early_stopping_rounds = 20, verbose = 1)\nprint('Best_iteration: {}, Best_score: {}, Best_ntree_limit: {}'.format(xgb_model.best_iteration, xgb_model.best_score, \n                                                                         xgb_model.best_ntree_limit))\n\nyhat = xgb_model.predict(x_test)\nprint('Test_acc: ', accuracy_score(y_test, yhat))\nprint(yhat, y_test)\n\nax = xgb.plot_importance(xgb_model, max_num_features= x_train.shape[1], importance_type='gain', show_values=True) \n\nfig = ax.figure\nfig.set_size_inches(10, 3)","724a7c78":"from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, precision_recall_curve\nprint('--- Axis1 = ref_val, Axis0 = pred_val ---')\n\nyhat = xgb_model.predict_proba(x_val)\nypred = xgb_model.predict(x_val)\nprint('Confusion_Matrix: ')\nprint(confusion_matrix(y_val, ypred, labels = [1,0]).T)\nprint('Precsion_Score: ', precision_score(y_val, ypred, labels = [1,0]))\nprint('Recall_Score: ', recall_score(y_val, ypred, labels = [1,0]))\n\nprint('y_hat_shape: ',yhat.shape)\nyhat = yhat[:,1]\n\nns_probs = [0 for _ in range(len(y_val))]\nns_auc = roc_auc_score(y_val, ns_probs, labels = [1,0])\nlr_auc = roc_auc_score(y_val, yhat, labels = [1,0])\nprint()\nprint('random_classifier: ROC AUC=%.3f' % (ns_auc))\nprint('XGBclassifier: ROC AUC=%.3f' % (lr_auc))\n\nns_fpr, ns_tpr, _ = roc_curve(y_val, ns_probs)\nlr_fpr, lr_tpr, _ = roc_curve(y_val, yhat)\n\npy.plot(ns_fpr, ns_tpr, linestyle='--', label='random_classifier')\npy.plot(lr_fpr, lr_tpr, marker='.', label='XGBclassifier')\npy.xlabel('False Positive Rate')\npy.ylabel('True Positive Rate')\npy.legend()\npy.title('ROC_CURVE')\npy.show()\n\nlr_precision, lr_recall, _ = precision_recall_curve(y_val, yhat)\nprint('auc-score: ',auc(lr_recall, lr_precision))\nprint('f1-score: ', f1_score(y_val, ypred, labels = [1,0]))\nno_skill = len(y_val[y_val==1]) \/ len(y_val)\npy.plot([0, 1], [no_skill, no_skill], linestyle='--', label='random_classifier')\npy.plot(lr_recall, lr_precision, marker='.', label='XGBclassifier')\npy.xlabel('Recall')\npy.ylabel('Precision')\npy.legend()\npy.title('Precision_Recall_Curve')\npy.show()\n","435564da":"import pylab as py\nfrom sklearn.model_selection import KFold, RepeatedStratifiedKFold\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score, cross_validate\nfrom sklearn import tree\n\ndf_train = train\nlabel = df_train['Survived']\n\nprint(x_train.shape, y_train.shape)\n\nt_model = xgb_model\n\nplot = sns.displot(data=label, kde=False, height=5)\nplot.fig.suptitle(\"Data_distribution_Normal\",fontsize=20, fontdict={\"weight\": \"bold\"}, x = 0.6)\npy.show()\n\nlabel = label.to_numpy()\nlabel_0 = label[label==0]\nlabel_1 = label[label==1]\nprint('Dead: ',label_0.shape[0], ' ,Survived: ',label_1.shape[0])\nprint('Distrib_Ratio: ',label_0.shape[0]\/label_1.shape[0]) \n\nover = RandomOverSampler(sampling_strategy = 0.9)\ns_over = SMOTE(sampling_strategy = 0.95, k_neighbors = 5)\ns_under = RandomUnderSampler(sampling_strategy = 0.96)\n\nsteps2 = [('o', over), ('model', t_model)]\npipln2 = Pipeline(steps = steps2)\n\nsteps1 = [('model', t_model)]\npipln1 = Pipeline(steps = steps1)\n\npipln3 = Pipeline(steps = [('o', s_over),('u', s_under), ('model',t_model)])\n\ncv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 5, random_state =1)\n\nscore2 = cross_val_score(pipln2, x_train, y_train, cv=cv, scoring='f1_micro', n_jobs=1)\nscore2 = np.mean(score2)\nscore1 = cross_val_score(pipln1, x_train, y_train, cv=cv, scoring='f1_micro', n_jobs=1)\nscore1 = np.mean(score1)\nprint('--'*50)\nprint('f1_score: ', score1)\nprint('--'*50)\n\nx_over, y_over = over.fit_resample(x_train, y_train)\ndf_over = pd.DataFrame(y_over, columns = {'Survived'} )\ndf_over = df_over['Survived']\nplot1 = sns.displot(df_over, kde=False, height=5)\nplot1.fig.suptitle(\"Data_distribution_Rndm_overSampling\",fontsize=20, fontdict={\"weight\": \"bold\"}, x = 0.7)\npy.show()\n\nprint('y_sur: ', y_over[y_over==1].shape,'y_dead: ', y_over[y_over==0].shape)\nprint('--'*50)\nprint('Random_over_f1_score: ', score2)\nprint('--'*50)\n\nscore3 = cross_val_score(pipln3, x_train, y_train, cv=cv, scoring='f1_micro', n_jobs=1)\nscore3 = np.mean(score3)\n\nx_over, y_over = s_over.fit_resample(x_train, y_train)\ndf_over = pd.DataFrame(y_over, columns = {'Survived'} )\ndf_over = df_over['Survived']\nplot1 = sns.displot(df_over, kde=False, height=5)\nplot1.fig.suptitle(\"Data_distribution_SMOTE\",fontsize=20, fontdict={\"weight\": \"bold\"}, x = 0.6)\npy.show()\n\nprint('y_sur: ', y_over[y_over==1].shape,'y_dead: ', y_over[y_over==0].shape)\nprint('--'*50)\nprint('Smote_f1_score: ', score3)\nprint('--'*50)\n","a4cfcb4e":"import pylab as py\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import LocallyLinearEmbedding, Isomap, TSNE\nimport matplotlib.pyplot as plt\n\npca = PCA(n_components = 0.3)\nx_pca = pca.fit_transform(x_train)\nprint('x_pca_shape: ', x_pca.shape, ' ,y_train_shape: ',y_train.shape)\n\npy.title('PCA')\npy.scatter(x_pca[:,0],x_pca[:,1], c = y_train, marker = '.')\npy.show()\n\nembedding = LocallyLinearEmbedding(n_components=3)\nx_lle= embedding.fit_transform(x_train)\nprint('x_lle_shape: ',x_lle.shape)\n\nfig = plt.figure()\naxs1 = fig.add_subplot(1,1,1, projection='3d')\naxs1.set(title='LLE')\naxs1.scatter(x_lle[:, 0], x_lle[:, 1], x_lle[:,2], c=y_train)\npy.show()\n\nembedding = Isomap(n_components=2)\nx_iso= embedding.fit_transform(x_train)\nprint('x_iso_shape: ',x_iso.shape)\npy.title('Isomap')\npy.scatter(x_iso[:,0],x_iso[:,1], c = y_train, marker = '.')\npy.show()\n\nembedding1 = TSNE(n_components=2)\nembedding2 = TSNE(n_components=3)\nx_tsne1= embedding1.fit_transform(x_train)\nx_tsne2= embedding2.fit_transform(x_train)\nprint('x_tsne1_shape: ',x_tsne1.shape)\nprint('x_tsne2_shape: ',x_tsne2.shape)\n\nfig = plt.figure(figsize=(10,6))\naxs1 = fig.add_subplot(1,2,1)\naxs1.set(title='t-SNE \/ 2d')\naxs1.scatter(x_tsne1[:, 0], x_tsne1[:, 1], c=y_train)\naxs2 = fig.add_subplot(1,2,2, projection='3d')\naxs2.set(title='t-SNE \/ 3d')\naxs2.scatter(x_tsne2[:, 0], x_tsne2[:, 1],x_tsne2[:, 2], c=y_train)\npy.show()","531de873":"print(sub_test.shape)\nxgb_pred = xgb_model.predict(s_test)\nprint(test)\nsubmission = pd.concat([df_test.PassengerId, pd.DataFrame(xgb_pred)], axis = 'columns')\nsubmission.columns = [\"PassengerId\", \"Survived\"]\nsubmission.to_csv('submission.csv', header = True, index = False) #--- for saving\n(submission)","f54cc255":"## Training Mode","c0a9c3c2":"#### Test_set","35f0e931":"### Some_Visualizations","4699fe3d":"## Loading \/ Preprocessing of Data","c86b5c01":"#### training_set ","5f62a1ba":"## Feature Selection ","850c8675":"### More Intuition","52f135bb":"## Inference Mode"}}