{"cell_type":{"7f5477e0":"code","fc233365":"code","03c877ac":"code","e3b14b37":"code","7a6f3f40":"code","7f7cfd98":"code","b8d12ab2":"code","1e366a6a":"code","f122deec":"code","fc72a9b0":"code","41f21394":"code","10895127":"code","b67248a8":"code","e9d30c4e":"code","cc56650f":"code","ba7a4d59":"code","75048c64":"code","1b21bd3f":"code","ebdb0bb5":"markdown","aa85c4f5":"markdown","42055269":"markdown","781e0d9e":"markdown"},"source":{"7f5477e0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\n\nimport gensim\n\nprint(os.listdir(\"..\/input\/quora-insincere-questions-classification\/embeddings\/GoogleNews-vectors-negative300\/\"))","fc233365":"import numpy as np\nimport pandas as pd","03c877ac":"link = \"..\/input\/quora-insincere-questions-classification\/embeddings\/GoogleNews-vectors-negative300\/GoogleNews-vectors-negative300.bin\"\n\nembeddings = gensim.models.KeyedVectors.load_word2vec_format(link, binary = True)","e3b14b37":"## Reading files\n\nyelp = pd.read_csv(\"..\/input\/yelp-sentiment-dataset\/yelp.csv\")","7a6f3f40":"yelp.head()","7f7cfd98":"yelp = yelp.drop(\"Unnamed: 0\", axis=1)\nyelp.columns   ## Dropping the not-utilisable column","b8d12ab2":"yelp.head()","1e366a6a":"## Using Stopwords (i.e removed)##\n\nimport nltk\n\ndocs_vectors = pd.DataFrame()  ## empty dataframe\nstopwords = nltk.corpus.stopwords.words('english')   ## !! added later\n\n## in below... all lowercase shall help in covering all the words, instead of adding \"\"A-Z\"\" in RegEx which may not provide suitable outputs\nfor doc in yelp['review'].str.lower().str.replace('[^a-z ]', ''):\n    temp = pd.DataFrame()   ## initially empty, and empty on every iteration\n    for word in doc.split(' '):  ## !!\n        if word not in stopwords: \n            try:\n                word_vec = embeddings[word]  ## if present, the following code applies\n                temp = temp.append(pd.Series(word_vec), ignore_index = True)  ## .Series to make it easier to append \"without\" index labels\n            except:\n                pass\n    doc_vector = temp.mean()\n    docs_vectors = docs_vectors.append(doc_vector, ignore_index = True) ## added to the empty data frame\n\n# docs_vectors.shape ## ==> (1000 x 300) order","f122deec":"docs_vectors.head() ## a sparse matrix","fc72a9b0":"pd.isnull(docs_vectors).sum().sum() # No null values present","41f21394":"## adding a column in docs_vector of \"sentiment\"  + dropping the null values\n\ndocs_vectors['sentiment'] = yelp['sentiment']\ndocs_vectors = docs_vectors.dropna()","10895127":"from sklearn.model_selection import train_test_split \n\n## here vectorization (vectorizer) again shall not come, since we are calculated weights \nfrom sklearn.ensemble import AdaBoostClassifier \n\ntrain_x, test_x, train_y, test_y = train_test_split(docs_vectors.drop('sentiment', axis = 1),\n                                                   docs_vectors['sentiment'],\n                                                   test_size = 0.2,\n                                                   random_state = 1)\n\ntrain_x.shape, test_x.shape, train_y.shape, test_y.shape  ## Test and Train partitions","b67248a8":"model = AdaBoostClassifier(n_estimators = 900, random_state = 1)\nmodel.fit(train_x, train_y)\n\ntest_pred = model.predict(test_x)\n\nfrom sklearn.metrics import accuracy_score\naccuracy_score(test_y, test_pred)   \n\n## == 77.5% accuracy score using AdaBoost algorithm (with Stopwords removed)","e9d30c4e":"### Sentiment Analyzer to check out Sentiments\n\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\nsentiment = SentimentIntensityAnalyzer()","cc56650f":"reviews = yelp['review'].str.lower().str.replace('[^a-z ]', '')\nreviews   \n\n## the Yelp reviews are put into lowercase and then using RegEx, words are split seperated.\n## this format allows for better analysis of sentiment of words","ba7a4d59":"yelp['sentiment'].value_counts()   ","75048c64":"## Using a user-defined function to find out the sentiment out of Yelp reviews\n\ndef get_sentiment(text):\n    sentiment = SentimentIntensityAnalyzer() #### calling Intensity Analyzer\n    compound = sentiment.polarity_scores(text)['compound']  ### calling the 'compound' score for the \"text\" entered\n    if compound > 0:\n        return 1  ## positive\n    else:\n        return 0 ## negative\n    #else:\n        #return \"Neutral\"     \n    return compound\n\nyelp['sentiment_vader'] = yelp['review'].apply(get_sentiment) ### in the columns of \"imdb\"\nyelp['sentiment_vader'] ","1b21bd3f":"from sklearn.metrics import accuracy_score\n\naccuracy_score(yelp['sentiment'], yelp['sentiment_vader']) ## == 80.9% of accuracy score using VADER\n\n## ==> improved accuracy using VADER","ebdb0bb5":"### Adaptive Boost algorithm \n    - ****(to check accuracy of predictions on Yelp reviews)","aa85c4f5":"#### NOTE:\nIn \"compound score\" is taken from the following calculations are made - \n\ncompound score = [score \/ sqrt{(score^2)+alpha}]","42055269":"## **VADER package :\nValence Aware Dictionary and sEntiment Reasoner\n\nNOTE: '''presence of punctuations, capitals make impact on the individual \/ overall score... so DO NOT clean data or change anything in the text to avoid distorting the score\n\nAlso, works well for shorter documents.\n\nSTOPWORDS shall be HEEDED!\n\nSingle letter words are IGNORED! '''","781e0d9e":"#### Calculating accuracy score using VADER"}}