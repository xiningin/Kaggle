{"cell_type":{"af396e44":"code","c86dc3ed":"code","213c1f33":"code","1a37de6b":"code","3b80f791":"code","c81adde0":"code","4d47c525":"code","e39f2dd2":"code","aa797dd8":"code","a2639a1c":"code","e70a6ef2":"code","22de027b":"code","186bf377":"code","80a4ca68":"code","6704f5e4":"code","6ce59fc1":"code","4ba3d6e2":"code","d26d76d1":"code","80eb006f":"code","5f1d235b":"code","a55caf84":"code","55e562c2":"code","924f8e50":"code","09b7a1f1":"code","458c6cbd":"code","1129eee2":"code","d53a3351":"code","5630832f":"code","49c5842a":"code","30a78d01":"code","5daa0977":"code","25f03aa4":"code","98f29b56":"code","1ff7980e":"code","5a478f80":"code","93b2265e":"code","3ab062d4":"code","52e808cd":"code","a8445152":"markdown","9b2004d7":"markdown","b9320c87":"markdown","361e8e05":"markdown","ddb90671":"markdown","b59c2cba":"markdown","cf52cdeb":"markdown","d129c32c":"markdown","81b892c3":"markdown","67bf8902":"markdown","1f62c9ad":"markdown","d687a152":"markdown","fdc573a5":"markdown","b8c74ec0":"markdown","6e1c970e":"markdown","00cd1ccf":"markdown","8fd5dcd6":"markdown","17cda86d":"markdown","6a56cae5":"markdown","a7342329":"markdown","024ff1d7":"markdown","1275cf1f":"markdown","f58a638b":"markdown","57e37a2c":"markdown","db6aecc2":"markdown","d7d8c839":"markdown","6fbd4aef":"markdown","03d78350":"markdown","87223777":"markdown","10845132":"markdown","c7a1112b":"markdown","1aba63b4":"markdown","a0e73a7c":"markdown","b8a61cf5":"markdown","614ed3a8":"markdown","2af50fb4":"markdown","a3ea2921":"markdown","1a5542ee":"markdown","d624a680":"markdown","395001ef":"markdown","f8ae88b7":"markdown","752c41c9":"markdown","59c9f2bb":"markdown","37b3af42":"markdown"},"source":{"af396e44":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom IPython.display import display\nfrom sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn.metrics import confusion_matrix,accuracy_score,log_loss\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom keras.models import Sequential\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, LSTM, Dropout","c86dc3ed":"def load_activity_map():\n    map = {}\n    map[0] = 'transient'\n    map[1] = 'lying'\n    map[2] = 'sitting'\n    map[3] = 'standing'\n    map[4] = 'walking'\n    map[5] = 'running'\n    map[6] = 'cycling'\n    map[7] = 'Nordic_walking'\n    map[9] = 'watching_TV'\n    map[10] = 'computer_work'\n    map[11] = 'car driving'\n    map[12] = 'ascending_stairs'\n    map[13] = 'descending_stairs'\n    map[16] = 'vacuum_cleaning'\n    map[17] = 'ironing'\n    map[18] = 'folding_laundry'\n    map[19] = 'house_cleaning'\n    map[20] = 'playing_soccer'\n    map[24] = 'rope_jumping'\n    return map","213c1f33":"def generate_three_IMU(name):\n    x = name +'_x'\n    y = name +'_y'\n    z = name +'_z'\n    return [x,y,z]\n\ndef generate_four_IMU(name):\n    x = name +'_x'\n    y = name +'_y'\n    z = name +'_z'\n    w = name +'_w'\n    return [x,y,z,w]\n\ndef generate_cols_IMU(name):\n    # temp\n    temp = name+'_temperature'\n    output = [temp]\n    # acceleration 16\n    acceleration16 = name+'_3D_acceleration_16'\n    acceleration16 = generate_three_IMU(acceleration16)\n    output.extend(acceleration16)\n    # acceleration 6\n    acceleration6 = name+'_3D_acceleration_6'\n    acceleration6 = generate_three_IMU(acceleration6)\n    output.extend(acceleration6)\n    # gyroscope\n    gyroscope = name+'_3D_gyroscope'\n    gyroscope = generate_three_IMU(gyroscope)\n    output.extend(gyroscope)\n    # magnometer\n    magnometer = name+'_3D_magnetometer'\n    magnometer = generate_three_IMU(magnometer)\n    output.extend(magnometer)\n    # oreintation\n    oreintation = name+'_4D_orientation'\n    oreintation = generate_four_IMU(oreintation)\n    output.extend(oreintation)\n    return output\n\ndef load_IMU():\n    output = ['time_stamp','activity_id', 'heart_rate']\n    hand = 'hand'\n    hand = generate_cols_IMU(hand)\n    output.extend(hand)\n    chest = 'chest'\n    chest = generate_cols_IMU(chest)\n    output.extend(chest)\n    ankle = 'ankle'\n    ankle = generate_cols_IMU(ankle)\n    output.extend(ankle)\n    return output\n    \ndef load_subjects(root='..\/input\/ass2-time-series\/PAMAP2_Dataset\/Protocol\/subject'):\n    output = pd.DataFrame()\n    cols = load_IMU()\n    \n    for i in range(101,110):\n        path = root + str(i) +'.dat'\n        subject = pd.read_table(path, header=None, sep='\\s+')\n        subject.columns = cols \n        subject['id'] = i\n        output = output.append(subject, ignore_index=True)\n    output.reset_index(drop=True, inplace=True)\n    return output\n\ndata = load_subjects()","1a37de6b":"data","3b80f791":"def fix_data(data):\n    data = data.drop(data[data['activity_id']==0].index)\n    data = data.interpolate()\n    # fill all the NaN values in a coulmn with the mean values of the column\n    for colName in data.columns:\n        data[colName] = data[colName].fillna(data[colName].mean())\n    activity_mean = data.groupby(['activity_id']).mean().reset_index()\n    return data\n\ndata = fix_data(data)","c81adde0":"data.describe()","4d47c525":"print('Size of the data: ', data.size)\nprint('Shape of the data: ', data.shape)\nprint('Number of columns in the data: ', len(data.columns))\nresult_id = data.groupby(['id']).mean().reset_index()\nprint('Number of uniqe ids in the data: ', len(result_id))\nresult_act = data.groupby(['activity_id']).mean().reset_index()\nprint('Numbe of uniqe activitys in the data: ',len(result_act))","e39f2dd2":"def pd_fast_plot(pd,column_a,column_b,title, figsize=(10,6)):\n    plt.rcParams.update({'font.size': 16})\n    size = range(len(pd))\n    f, ax = plt.subplots(figsize=figsize) \n    plt.bar(size, pd[column_a], color=plt.cm.Paired(size))\n    a = ax.set_xticklabels(pd[column_b])\n    b = ax.legend(fontsize = 20)\n    c = ax.set_xticks(np.arange(len(pd)))\n    d = ax.set_title(title)\n    plt.show()\n    ","aa797dd8":"sampels = data.groupby(['id']).count().reset_index()\nsampels_to_subject = pd.DataFrame()\nsampels_to_subject['id'] = sampels['id']\nsampels_to_subject['sampels'] = sampels['time_stamp']\nsampels_to_subject = sampels_to_subject.sort_values(by=['sampels'])\npd_fast_plot(sampels_to_subject,'sampels','id','Number Of Samepls By Users')","a2639a1c":"map_ac = load_activity_map()\nsampels = data.groupby(['activity_id']).count().reset_index()\nsampels_to_subject = pd.DataFrame()\nsampels_to_subject['activity'] = [map_ac[x] for x in sampels['activity_id']]\nsampels_to_subject['sampels'] = sampels['time_stamp']\nsampels_to_subject = sampels_to_subject.sort_values(by=['sampels'])\npd_fast_plot(sampels_to_subject,'sampels','activity','Number Of Samepls By Activity',figsize=(40,7))","e70a6ef2":"sampels_heart_rate = pd.DataFrame()\nsampels_heart_rate['id'] = result_id['id']\nsampels_heart_rate['heart_rate'] = result_id['heart_rate']\nsampels_heart_rate = sampels_heart_rate.sort_values(by=['heart_rate'])\npd_fast_plot(sampels_heart_rate,'heart_rate','id','Avg heart Rate by Subject')","22de027b":"map_ac = load_activity_map()\nsampels_heart_rate = pd.DataFrame()\nsampels_heart_rate['activity'] = [map_ac[x] for x in result_act['activity_id']]\nsampels_heart_rate['heart_rate'] = result_act['heart_rate']\nsampels_heart_rate = sampels_heart_rate.sort_values(by=['heart_rate'])\npd_fast_plot(sampels_heart_rate,'heart_rate','activity','Avg heart Rate by Activity',figsize=(40,10))","186bf377":"samepls_tempreture = pd.DataFrame()\nsamepls_tempreture['hand'] = result_id['hand_temperature']\nsamepls_tempreture['chest'] = result_id['chest_temperature']\nsamepls_tempreture['ankle'] = result_id['ankle_temperature']\n\nax = samepls_tempreture.plot(kind='line', figsize=(20,6), title='Avg Tempatures by Subjects')\na = ax.set_xticklabels(result_id['id'])\nb = ax.legend(fontsize = 20)\nc = ax.set_xticks(np.arange(len(samepls_tempreture)))","80a4ca68":"map_ac = load_activity_map()\nsamepls_tempreture = pd.DataFrame()\nsamepls_tempreture['activity'] = [map_ac[x] for x in result_act['activity_id']]\nsamepls_tempreture['hand'] = result_act['hand_temperature']\nsamepls_tempreture['chest'] = result_act['chest_temperature']\nsamepls_tempreture['ankle'] = result_act['ankle_temperature']\n\nax = samepls_tempreture.plot(kind='line', figsize=(40,6), title='Avg Tempatures by Activity')\na = ax.set_xticklabels(samepls_tempreture['activity'])\nb = ax.legend(fontsize = 20)\nc = ax.set_xticks(np.arange(len(samepls_tempreture)))","6704f5e4":"samepls = pd.DataFrame()\nsamepls['hand_x'] = result_id['hand_3D_acceleration_16_x']\nsamepls['hand_y'] = result_id['hand_3D_acceleration_16_y']\nsamepls['hand_z'] = result_id['hand_3D_acceleration_16_z']\nax = samepls.plot(kind='line', figsize=(20,6), title='Avg Hand Acceleration Value by Subjects')\na = ax.set_xticklabels(result_id['id'])\nb = ax.legend(fontsize = 20)\nc = ax.set_xticks(np.arange(len(samepls)))","6ce59fc1":"from sklearn.preprocessing import MinMaxScaler\n\ndef split_train_test(data):\n    # create the test data\n    subject107 = data[data['id'] == 107]\n    subject108 = data[data['id'] == 108]\n    test = subject107.append(subject108)\n\n    # create the train data\n    train = data[data['id'] != 107]\n    train = data[data['id'] != 108]\n\n    # drop the columns id and time\n    test = test.drop([\"id\"], axis=1)\n    train = train.drop([\"id\"], axis=1)\n\n    # split train and test to X and y\n    X_train = train.drop(['activity_id','time_stamp'], axis=1).values\n    X_test = test.drop(['activity_id','time_stamp'], axis=1).values\n    \n    # make data scale to min max beetwin 0 to 1\n    min_max_scaler = MinMaxScaler()\n    min_max_scaler.fit(X_train)\n    min_max_scaler.fit(X_test)\n    X_train = min_max_scaler.transform(X_train)\n    X_test = min_max_scaler.transform(X_test)\n    \n    y_train = train['activity_id'].values\n    y_test = test['activity_id'].values\n    return X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = split_train_test(data)\nprint('Train shape X :',X_train.shape,' y ', y_train.shape)\nprint('Test shape X :',X_test.shape,' y ', y_test.shape)","4ba3d6e2":"from sklearn.preprocessing import MinMaxScaler\n\ndef base_line_preprocess(data):\n    train = data.groupby('activity_id')\n    X_base = train.mean().reset_index().drop(['activity_id','id','time_stamp'],axis=1).values\n    min_max_scaler = MinMaxScaler()\n    min_max_scaler.fit(X_base)\n    X_base = min_max_scaler.transform(X_base)\n    y_base = np.array(train['activity_id'].unique().explode().values).astype('float64')\n    return X_base, y_base\n\nX_base, y_base = base_line_preprocess(data)\nprint('X base shape: ', X_base.shape)\nprint('y base shape: ', y_base.shape)\n","d26d76d1":"reg = LogisticRegression()\nreg.fit(X_base, y_base)\npreds = reg.predict(X_train)\nprint('Logistic regression accuracy on train: ', accuracy_score(y_train,preds)*100)\npreds = reg.predict(X_test)\nprint('Logistic regression accuracy on test: ', accuracy_score(y_test,preds)*100)","80eb006f":"from sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor()\ndt.fit(X_train, y_train)","5f1d235b":"# predict on the train and the test\npreds = dt.predict(X_train)\nprint('Decision Tree Regressor accuracy on train: ', accuracy_score(y_train,preds)*100)\npreds = dt.predict(X_test)\nprint('Decision Tree Regressor accuracy on test: ', accuracy_score(y_test,preds)*100)\npreds = 0","a55caf84":"def create_lstm_data(X, y, step_back = 5, step_forword = 1):\n    out_X = []\n    out_y = []\n    size = len(X)\n    for i, features in enumerate(X):\n        if i >= step_back and i < size - step_forword:\n            tmp_X = []\n            tmp_y = []\n            for j in range(i-step_back,i):\n                tmp_X.extend([X[j]])\n            out_X.append(tmp_X)\n            for j in range(i,i+step_forword):\n                tmp_y.extend([y[j]])\n            out_y.append(tmp_y)\n    return np.array(out_X), np.array(out_y)\n\nX_lstm_train, y_lstm_train = create_lstm_data(X_train, y_train)\nX_lstm_test, y_lstm_test = create_lstm_data(X_test, y_test)","55e562c2":"from sklearn.preprocessing import OneHotEncoder\nhot = OneHotEncoder(handle_unknown='ignore', sparse=False)\nhot.fit(y_lstm_train)\nhot.fit(y_lstm_test)\n\ny_lstm_train = hot.transform(y_lstm_train)\ny_lstm_test = hot.transform(y_lstm_test)","924f8e50":"print('Train shape X lstm :',X_lstm_train.shape,' y ', y_lstm_train.shape)\nprint('Test shape X lstm :',X_lstm_test.shape,' y ', y_lstm_test.shape)\n","09b7a1f1":"lstm_model = Sequential()\nlstm_model.add(LSTM(6,input_shape=(X_lstm_train.shape[1],X_lstm_train.shape[2])))\nlstm_model.add(Dense(16 ,activation='relu'))\nlstm_model.add(Dense(y_lstm_train.shape[1], activation='softmax'))\n\nlstm_model.summary()\nlstm_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])","458c6cbd":"from keras.callbacks import EarlyStopping\nearly_stopping_monitor = EarlyStopping(patience=3)\nhistory = lstm_model.fit(X_lstm_train, y_lstm_train, validation_split = 0.2 , epochs = 10, callbacks=[early_stopping_monitor])","1129eee2":"# taken from https:\/\/keras.io\/visualization\/\ndef quick_plot_history(history):\n    # Plot training & validation accuracy values\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.show()\n\n    # Plot training & validation loss values\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.show()\n\nquick_plot_history(history)","d53a3351":"y = y_test[5:-1]\npreds = lstm_model.predict(X_lstm_test)\npreds_cat = np.argmax(preds,axis=1)\n# building a map of result to activity\nresult = np.unique(preds_cat).tolist() \nexpected = np.unique(y).tolist() \ncombined = list(zip(result,expected))\nconf_map = dict(combined)\n# transfoms the prediction to an activity\nresults = [conf_map[x] for x in preds_cat]\nprint('model accuracy on test :',accuracy_score(y,results)*100)","5630832f":"def quick_plot_con_matrix(y, results,labels):\n    # now print confusion metrix\n    con = confusion_matrix(y,results)\n    a = plt.figure(figsize=(25,15), dpi=50)\n    a = sns.heatmap(con, cmap='YlGnBu', annot=True, fmt = 'd', xticklabels=labels, yticklabels=labels)\n    a = plt.rcParams.update({'font.size': 20})\n    a = plt.title('Confusion Matrix')\n    a = plt.xlabel('Predictions')\n    a = plt.ylabel('Accpected')\n\nactivity_map = load_activity_map()\nlabels = [activity_map[conf_map[x]] for x in result]\nquick_plot_con_matrix(y,results, labels)","49c5842a":"def create_lstm_data(X, y, step_back = 5, step_forword = 1):\n    out_X = []\n    out_y = []\n    size = len(X)\n    for i, features in enumerate(X):\n        if i >= step_back and i < size - step_forword:\n            tmp_X = []\n            tmp_y = []\n            for j in range(i-step_back,i):\n                tmp_X.extend([X[j]])\n            out_X.append(tmp_X)\n            for j in range(i,i+step_forword):\n                tmp_y.extend([y[j]])\n            out_y.append(tmp_y)\n    return np.array(out_X), np.array(out_y)","30a78d01":"# Freeze the layers \nfor i,layer in enumerate(lstm_model.layers):\n    if i < 1:\n        print(layer)\n        layer.trainable = False","5daa0977":"from keras.callbacks import EarlyStopping\nearly_stopping_monitor = EarlyStopping(patience=3)\nhistory = lstm_model.fit(X_lstm_train, y_lstm_train, validation_split = 0.2 , epochs = 10, callbacks=[early_stopping_monitor])\nquick_plot_history(history)","25f03aa4":"y = y_test[5:-1]\npreds = lstm_model.predict(X_lstm_test)\npreds_cat = np.argmax(preds,axis=1)\n# building a map of result to activity\nresult = np.unique(preds_cat).tolist() \nexpected = np.unique(y).tolist() \ncombined = list(zip(result,expected))\nconf_map = dict(combined)\n# transfoms the prediction to an activity\nresults = [conf_map[x] for x in preds_cat]\nprint('model accuracy on test :',accuracy_score(y,results)*100)","98f29b56":"new_data = data.copy().reset_index()\n# new_data = new_data.drop('index',axis=1)\n# new_data = new_data.reset_index()\nnew_cols = None \nfor subject in range(101,110):\n    prev_act_1 = new_data[new_data['id'] == subject]\n    start = prev_act_1.head(2).index[1]\n    end = prev_act_1.tail(1).index[0]\n    prev_act_1 = prev_act_1.loc[start:end+1]\n    new_cols_1 = pd.DataFrame()\n    new_cols_1['prev_aid'] = prev_act_1['activity_id']\n    new_cols_1['prev_hr'] = prev_act_1['heart_rate']\n    new_cols_1['index'] = prev_act_1['index'] + 1\n    if new_cols is None:\n        new_cols = new_cols_1\n    else:\n        new_cols = new_cols.append(new_cols_1)\nnew_cols = new_data.merge(new_cols, on='index', how='left')\nnew_cols = new_cols.dropna()","1ff7980e":"from sklearn.preprocessing import OneHotEncoder\n\nX_train, X_test, y_train, y_test = split_train_test(new_cols)\nprint('Train shape X :',X_train.shape,' y ', y_train.shape)\nprint('Test shape X :',X_test.shape,' y ', y_test.shape)\n\nX_lstm_train, y_lstm_train = create_lstm_data(X_train, y_train)\nX_lstm_test, y_lstm_test = create_lstm_data(X_test, y_test)\nhot = OneHotEncoder(handle_unknown='ignore', sparse=False)\nhot.fit(y_lstm_train)\nhot.fit(y_lstm_test)\n\ny_lstm_train = hot.transform(y_lstm_train)\ny_lstm_test = hot.transform(y_lstm_test)\nprint('Train shape X lstm :',X_lstm_train.shape,' y ', y_lstm_train.shape)\nprint('Test shape X lstm :',X_lstm_test.shape,' y ', y_lstm_test.shape)\n","5a478f80":"from keras.layers import Dropout\nlstm_model = Sequential()\nlstm_model.add(LSTM(16,input_shape=(X_lstm_train.shape[1],X_lstm_train.shape[2])))\nlstm_model.add(Dense(64 ,activation='relu'))\nlstm_model.add(Dense(64 ,activation='relu'))\nlstm_model.add(Dropout(0.1))\nlstm_model.add(Dense(64 ,activation='relu'))\nlstm_model.add(Dense(64 ,activation='relu'))\nlstm_model.add(Dense(y_lstm_train.shape[1], activation='softmax'))\n\nlstm_model.summary()\nlstm_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])","93b2265e":"from keras.callbacks import EarlyStopping\nearly_stopping_monitor = EarlyStopping(patience=3)\nhistory = lstm_model.fit(X_lstm_train, y_lstm_train, validation_split = 0.2 , epochs = 10, callbacks=[early_stopping_monitor])","3ab062d4":"# taken from https:\/\/keras.io\/visualization\/\ndef quick_plot_history(history):\n    # Plot training & validation accuracy values\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.show()\n\n    # Plot training & validation loss values\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.show()\n\nquick_plot_history(history)","52e808cd":"y = y_test[5:-1]\npreds = lstm_model.predict(X_lstm_test)\npreds_cat = np.argmax(preds,axis=1)\n# building a map of result to activity\nresult = np.unique(preds_cat).tolist() \nexpected = np.unique(y).tolist() \ncombined = list(zip(result,expected))\nconf_map = dict(combined)\n# transfoms the prediction to an activity\nresults = [conf_map[x] for x in preds_cat]\nprint('model accuracy on test :',accuracy_score(y,results)*100)","a8445152":"Activity analysis for all the subjects ","9b2004d7":"### LSTM model:","b9320c87":"We will freaze all the layers except the last 2 layers","361e8e05":"we can see that suject 109 as the lowest tempetures in all the parts of the humen body that were measured, And the lowest activity temptures is rope_jumping","ddb90671":"* As we can see we have NaN in our data, so we need to change their value. We will take every NaN value and eplace it with the mean value of the column.\n* Also, we can note from the map that acitvity_id = 0 is not an valid activity","b59c2cba":"We can see the data isn't balenced:\n* Subject 109 as less sampels then all others subjects.\n* rope_jumping activity as less sampels then other activities ","cf52cdeb":"# Element Analysis ","d129c32c":"We can see that our model is overfitting, as in the graphs the validationn set loss.","81b892c3":"# Time Series Models - PAMAP2 DataSet\nWe will walkthroug first time series models, and exploring the PAMAP2 DataSet.  Link To PAMAP2 dataset - https:\/\/archive.ics.uci.edu\/ml\/datasets\/PAMAP2+Physical+Activity+Monitoring  ","67bf8902":"## Imports For This Note Book","1f62c9ad":"## Summary Table","d687a152":"### Pre-processing the data - we will use base line of grouping all the data by activities","fdc573a5":"Training and Fitting Logistic Regression ","b8c74ec0":"This is a lower result then the ML Desicion Tree Regressor ","6e1c970e":"we can see that almost all the subjects hand sensor are acting the same however there is somthing strang in subject 108 y value of his hand sensor?","00cd1ccf":"# 1) b. Balance Analysis\ncheck for each subject if there equal number of sampels","8fd5dcd6":"As we can see we are tryin to predict from a past 5 times activities the next time activity","17cda86d":"## 1) c. In time serice we are going to use self-supervised. \n### Exampels of Self-Supervised Tasks: \n   * Predicting the future from the past: We can create new features based by old features. In our case this is the sliding window of the lookback in the prev features to predict the upcoming value.\n   * Predicting the past from the present: Same as the above however now the prediction is on the past and not the future.\n   * Adjusting the model to predict simple target - In our case making the target simple as we can so it will be more easly to predict it, in our case prediction of the next activity by a given values.","6a56cae5":"## Example for Good prediction - most of the standing activity where correct, 25858.\n## Example for Bad prediction -  walking activity most recognized as ascending_staris, 58796.","a7342329":"Result on the training Set:","024ff1d7":"we can see we didn't improving for the prev model, and we got to overfitting.","1275cf1f":"Ways we improve Our Model -\n\n1. Adding new features:\n    * the prev activity\n    * the prev activity heart_rate\n2. Adding More Layers","f58a638b":"![image.png](attachment:image.png)","57e37a2c":"### Pre-processing the data using a sliding window","db6aecc2":"# Loading Always Comes First\nIn this section we will load the data from the data set. \n* load_activity_map - get the map of humen acctivities describe in the data set\n* generate_three_IMU - genetrate 3 columns of x, y, z data\n* generate_four_IMU - genetrate 4 columns of x, y, z, w data\n* generate_cols_IMU - for a given name generate 5 columns of the sensors as describe in the data set (accelmoter 16, 6, gyroscope, magnometer and orentaion\n* load_IMU - load all 52 columns of a subject in the data set\n* load_subjects - load all subjects from a given root","d7d8c839":"Subjects details","6fbd4aef":"## Sensor Analysis","03d78350":"## 2) g. Improve Our Model","87223777":"Training agein the model, now only the last 2 layers are improving.","10845132":"![image.png](attachment:image.png)","c7a1112b":"## Tempature Analysis","1aba63b4":"## 2) a. Selecting a validation strategy - train,test split\nwe can see that subject 107, 108 are having the most measurments in all activities, we will take them as our test train.\nAs Part of our spliting we will use min_max_scaler tahat takes all column in the data set and rescaling them between 0 to 1, for getting better results more quickly.","a0e73a7c":"\n## 2) d. Building NN Model Using LSTM","b8a61cf5":"## 2) f. How our model is doing?\nWe can see that our model is predict well, but not so good. \nAs part of his good parts is the number of featrues, 52 features of the data.\nAlso, the lookback of lstm improved the accrucy evne more, we have lookback value = 5.\nIn Order to improve the preformence we can implement:\n1. add more layers to the model.\n2. add more features to the data, more preprocessing\n3. increase the lookback value and looknext value","614ed3a8":"# 1) a. Exploratory DataSet from Documentation:\n\n![image.png](attachment:image.png)\n","2af50fb4":"The results of the model are great for a bench mark, however we will try Desicision Tree as our Ml model","a3ea2921":"we can see subject 109 as the higest heart rate, and activity lying as the smallest heart rate","1a5542ee":"As we can see our problem is classification we are trying to predict for a given time and values of the sensors the activity","d624a680":"Note in section 1) c. we suggested to learn from the past to predict the future. We already implemented it as our sliding window as our new features set.\nThe function create_lstm_data take num of steps back to learn and num of steps forward to predict.\n","395001ef":"### About The Data Set: \n> The PAMAP2 Physical Activity Monitoring dataset contains data of 18 different physical activities (such as walking, cycling, playing soccer, etc.), performed by 9 subjects wearing 3 inertial measurement units and a heart rate monitor. The dataset can be used for activity recognition and intensity estimation, while developing and applying algorithms of data processing, segmentation, feature extraction and classification","f8ae88b7":"## 2) b. Creating Naive Solution for the problem using Logistic Regression","752c41c9":"## 2) c. Creating a Bench Mark using DecisionTreeRegressor","59c9f2bb":"## 2) e. Fine Tuning our 2 last Dense Layers in the Our Model","37b3af42":"## Heart Rate Anlysis"}}