{"cell_type":{"bde40895":"code","866c45d1":"code","f47e0c03":"code","f4ffa230":"code","bf081694":"code","acb0a60d":"code","1cae4e87":"code","b518847f":"code","fcf181c7":"code","cde32c4a":"code","1addaf60":"code","dba3108a":"markdown","3283db8d":"markdown","95f2f53f":"markdown","5159a70e":"markdown","4a92e9ce":"markdown","7b76aa4c":"markdown","bece8261":"markdown","91b18e9a":"markdown","f96cdaf8":"markdown","ab7e69cb":"markdown","b49ace03":"markdown","b2283504":"markdown","14dfbbd4":"markdown"},"source":{"bde40895":"#####################################\n# Libraries\n#####################################\n# Common libs\nimport pandas as pd\nimport numpy as np\nimport sys\nimport os\nimport random\nfrom pathlib import Path\n\n# Image processing\nimport imageio\nimport cv2\nimport skimage.transform\n#from skimage.transform import rescale, resize, downscale_local_mean\n\n# Charts\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ML, statistics\nimport scipy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score\n\n# Tensorflow\n#from sklearn.preprocessing import OneHotEncoder\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\n\n#############################################\n# Settings\n#############################################\nimport os\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.\n\n# Settings\nplt.style.use('fivethirtyeight')\n#plt.style.use('seaborn')\n\n# toy=True - development mode, small samples, limited training, fast run\n# toy=False - full data, slow learning and run\ntoy=False\n","866c45d1":"all_df = pd.read_csv(\"..\/input\/train_labels.csv\")\nif toy:\n    all_df = all_df.sample(50000)\n\nall_df.head()","f47e0c03":"all_df.label.value_counts().plot(kind='bar')\nplt.title('Labels counts')\nplt.xlabel('Label')\nplt.ylabel('Count')\nplt.show()","f4ffa230":"class ImageViewer:\n    def read_img(self,id, folder='train'):\n        \"\"\"\n        Read image by it's id\n        \"\"\"\n        file='..\/input\/' + folder + '\/' + str(id) + '.tif'\n        im=cv2.imread(file)\n        return im\n\n    def draw_sample_images(self):\n        \"\"\"\n        Draw cancer and healthy images for EDA\n        \"\"\"\n        ncols=4\n        f, ax = plt.subplots(nrows=2,ncols=ncols, \n                             figsize=(4*ncols,5*2))\n        i=-1\n        captions=['Pathology', 'Good']\n        # Draw one row for cancer, one row for healthy images\n        for label in [0,1]:\n            i=i+1\n            samples = all_df[all_df['label']==label]['id'].sample(ncols).values\n            for j in range(0,ncols):\n                file_id=samples[j]\n                im=self.read_img(file_id)\n                ax[i, j].imshow(im)\n                ax[i, j].set_title(captions[i], fontsize=16)  \n        plt.tight_layout()\n        plt.show()\n    \nImageViewer().draw_sample_images()","bf081694":"class DataPreparation:\n    \"\"\"\n    Train\/test\n    \"\"\"\n    def train_test_split(self, all_df):\n        \"\"\"\n        Balanced split to train, test and val\n        \"\"\"\n        # Split to train and test before balancing\n        train_df, test_df = train_test_split(all_df, random_state=24)\n#         # Split train to train and validation datasets\n#         train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=24)\n        # Number of samples in each category\n        ncat_bal = train_df['label'].value_counts().max()\n        #ncat_bal = int(len(train_df)\/train_df['label'].cat.categories.size)\n        train_df = train_df.groupby('label', as_index=False).apply(lambda g:  g.sample(ncat_bal, replace=True, random_state=24)).reset_index(drop=True)\n        return train_df, test_df\n    \n    def plot_balanced(self, train_df, all_df):\n        \"\"\"\n        Plot samples per category before and after balancing\n        \"\"\"\n        f, axs = plt.subplots(1,2,figsize=(12,4))\n        # Before balancing\n        all_df.label.value_counts().plot(kind='bar', ax=axs[0])\n        axs[0].set_title('All labels')\n        axs[0].set_xlabel('Label')\n        axs[0].set_ylabel('Count')\n        # After balancing\n        train_df.label.value_counts().plot(kind='bar', ax=axs[1])\n        axs[1].set_title('Train labels after balancing')\n        axs[1].set_xlabel('Label')\n        axs[1].set_ylabel('Count')\n        plt.tight_layout()\n        plt.show()\n\n\n# Train\/test\/validation split with balanced labels in train\ndata_prep = DataPreparation()\ntrain_df, test_df = data_prep.train_test_split(all_df)\n\n# Plot before and after balancing\ndata_prep.plot_balanced(train_df, all_df)\n","acb0a60d":"class Generators:\n    \"\"\"\n    Train, validation and test generators\n    \"\"\"\n    def __init__(self, train_df, test_df):\n        self.batch_size=32\n        self.img_size=(64,64)\n        \n        # Base train\/validation generator\n        _datagen = ImageDataGenerator(\n            rescale=1.\/255.,\n            validation_split=0.25,\n            featurewise_center=False,\n            featurewise_std_normalization=False,\n            rotation_range=90,\n            width_shift_range=0.2,\n            height_shift_range=0.2,\n            horizontal_flip=True,\n            vertical_flip=True\n            )\n        # Train generator\n        self.train_generator = _datagen.flow_from_dataframe(\n            dataframe=train_df,\n            directory=\"..\/input\/train\/\",\n            x_col=\"id\",\n            y_col=\"label\",\n            has_ext=False,\n            subset=\"training\",\n            batch_size=self.batch_size,\n            seed=42,\n            shuffle=True,\n            class_mode=\"categorical\",\n            target_size=self.img_size)\n        print('Train generator created')\n        # Validation generator\n        self.val_generator = _datagen.flow_from_dataframe(\n            dataframe=train_df,\n            directory=\"..\/input\/train\/\",\n            x_col=\"id\",\n            y_col=\"label\",\n            has_ext=False,\n            subset=\"validation\",\n            batch_size=self.batch_size,\n            seed=42,\n            shuffle=True,\n            class_mode=\"categorical\",\n            target_size=self.img_size)    \n        print('Validation generator created')\n        # Test generator\n        _test_datagen=ImageDataGenerator(rescale=1.\/255.)\n        self.test_generator = _test_datagen.flow_from_dataframe(\n            dataframe=test_df,\n            directory=\"..\/input\/train\/\",\n            x_col=\"id\",\n            y_col='label',\n            has_ext=False,\n            class_mode=\"categorical\",\n            batch_size=self.batch_size,\n            seed=42,\n            shuffle=False,\n            target_size=self.img_size)     \n        print('Test generator created')\n\n        \n# Create generators        \ngenerators = Generators(train_df, test_df)\nprint(\"Generators created\")","1cae4e87":"class ModelTrainer:\n    \"\"\"\n    Create and fit the model\n    \"\"\"\n    \n    def __init__(self, generators):\n        self.generators = generators\n        self.img_width = generators.img_size[0]\n        self.img_height = generators.img_size[1]\n        \n    def create_model_small(self):\n        \"\"\"\n        Build CNN model using img_width, img_height from fields.\n        \"\"\"\n        model=Sequential()\n        model.add(Conv2D(16, kernel_size=3, input_shape=(self.img_width, self.img_height,3), activation='relu', padding='same'))\n        model.add(MaxPooling2D(2))\n        model.add(Conv2D(32, kernel_size=3, activation='relu', padding='same'))\n        model.add(Dropout(0.1))\n        model.add(Flatten())\n        model.add(Dense(64, activation = \"relu\"))        \n        # 1 y label\n        model.add(Dense(2, activation='softmax'))\n        model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n        return model\n    \n    def create_model(self,\n                    kernel_size = (3,3),\n                    pool_size= (2,2),\n                    first_filters = 32,\n                    second_filters = 64,\n                    third_filters = 128,\n                    first_dense=256,\n                    second_dense=128,\n                    dropout_conv = 0.3,\n                    dropout_dense = 0.3):\n\n        model = Sequential()\n        # First conv filters\n        model.add(Conv2D(first_filters, kernel_size, activation = 'relu', padding=\"same\",\n                         input_shape = (self.img_width, self.img_height,3)))\n        model.add(Conv2D(first_filters, kernel_size, padding=\"same\", activation = 'relu'))\n        model.add(Conv2D(first_filters, kernel_size, padding=\"same\", activation = 'relu'))\n        model.add(MaxPooling2D(pool_size = pool_size)) \n        model.add(Dropout(dropout_conv))\n\n        # Second conv filter\n        model.add(Conv2D(second_filters, kernel_size, padding=\"same\", activation ='relu'))\n        model.add(Conv2D(second_filters, kernel_size, padding=\"same\", activation ='relu'))\n        model.add(Conv2D(second_filters, kernel_size, padding=\"same\", activation ='relu'))\n        model.add(MaxPooling2D(pool_size = pool_size))\n        model.add(Dropout(dropout_conv))\n\n        # Third conv filter\n        model.add(Conv2D(third_filters, kernel_size, padding=\"same\", activation ='relu'))\n        model.add(Conv2D(third_filters, kernel_size, padding=\"same\", activation ='relu'))\n        model.add(Conv2D(third_filters, kernel_size, padding=\"same\", activation ='relu'))\n        model.add(MaxPooling2D(pool_size = pool_size))\n        model.add(Dropout(dropout_conv))\n\n        model.add(Flatten())\n        \n        # First dense\n        model.add(Dense(first_dense, activation = \"relu\"))\n        model.add(Dropout(dropout_dense))\n        # Second dense\n        model.add(Dense(second_dense, activation = \"relu\"))\n        model.add(Dropout(dropout_dense))\n        \n        # Out layer\n        model.add(Dense(2, activation = \"softmax\"))\n\n        model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n        return model\n        \n    \n    def train(self, model, toy):\n        \"\"\"\n        Train the model\n        \"\"\"\n        if toy:\n            epochs=3\n            steps_per_epoch=20\n            validation_steps=2\n        else:\n            epochs=100\n            steps_per_epoch=100\n            #steps_per_epoch=30\n            #steps_per_epoch=10\n            validation_steps=5\n            \n        # We'll stop training if no improvement after some epochs\n        earlystopper = EarlyStopping(monitor='val_acc', patience=10, verbose=1)\n        reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, \n                                   verbose=1, mode='max', min_lr=0.00001)\n        # Save the best model during the traning\n        checkpointer = ModelCheckpoint('best_model1.h5'\n                                        ,monitor='val_acc'\n                                        ,verbose=1\n                                        ,save_best_only=True\n                                        ,save_weights_only=True)\n        # Train\n        training = model.fit_generator(generator=self.generators.train_generator\n                                ,epochs=epochs\n                                ,steps_per_epoch=steps_per_epoch\n                                ,validation_data=self.generators.val_generator\n                                ,validation_steps=validation_steps\n                                ,callbacks=[earlystopper, checkpointer, reduce_lr])\n        # Get the best saved weights\n        model.load_weights('best_model1.h5')\n        return training\n    \n# Create and train the model\ntrainer = ModelTrainer(generators)\n\n# model = trainer.create_model(kernel_size = (3,3),\n#                     pool_size= (2,2),\n#                     first_filters = 64,\n#                     second_filters = 128,\n#                     third_filters = 256,\n#                     first_dense=256,\n#                     second_dense=128,\n#                     dropout_conv = 0.4,\n#                     dropout_dense = 0.3)\n\nmodel = trainer.create_model(kernel_size = (3,3),\n                    pool_size= (2,2),\n                    first_filters = 128,\n                    second_filters = 256,\n                    third_filters = 512,\n                    first_dense=256,\n                    second_dense=128,\n                    dropout_conv = 0.3,\n                    dropout_dense = 0.2)\n\nmodel.summary()","b518847f":"training=trainer.train(model, toy)\nprint(\"Trained\")","fcf181c7":"class Evaluator:\n    \"\"\"\n    Evaluaion :predict on test data (not submission data from test folder)\n    and print reports, plot results etc.\n    \"\"\"\n     \n    def __init__(self, model, training, generator, y_true):\n        self.training = training\n        self.generator = generator\n        # predict the data\n        steps=5\n        self.y_pred_raw = model.predict_generator(self.generator, steps=steps)\n        self.y_pred = np.argmax(self.y_pred_raw, axis=1)\n        self.y_true=y_true[:len(self.y_pred)]        \n    \n    \"\"\"\n    Accuracy, evaluation\n    \"\"\"\n    def plot_history(self):\n        \"\"\"\n        Plot training history\n        \"\"\"\n        ## Trained model analysis and evaluation\n        f, ax = plt.subplots(1,2, figsize=(12,3))\n        ax[0].plot(self.training.history['loss'], label=\"Loss\")\n        ax[0].plot(self.training.history['val_loss'], label=\"Validation loss\")\n        ax[0].set_title('Loss')\n        ax[0].set_xlabel('Epoch')\n        ax[0].set_ylabel('Loss')\n        ax[0].legend()\n\n        # Accuracy\n        ax[1].plot(self.training.history['acc'], label=\"Accuracy\")\n        ax[1].plot(self.training.history['val_acc'], label=\"Validation accuracy\")\n        ax[1].set_title('Accuracy')\n        ax[1].set_xlabel('Epoch')\n        ax[1].set_ylabel('Accuracy')\n        ax[1].legend()\n        plt.tight_layout()\n        plt.show()\n    \n    def plot_roc(self):\n        #y_pred_keras = model.predict_generator(test_gen, steps=len(df_val), verbose=1)\n        # Calculate roc\n        fpr_keras, tpr_keras, thresholds_keras = roc_curve(self.y_true, self.y_pred)\n        auc_keras = auc(fpr_keras, tpr_keras)\n        \n        plt.figure(1)\n        plt.plot([0, 1], [0, 1], 'k--')\n        plt.plot(fpr_keras, tpr_keras, label='area = {:.3f}'.format(auc_keras))\n        plt.xlabel('False positive rate')\n        plt.ylabel('True positive rate')\n        plt.title('ROC curve')\n        plt.legend(loc='best')\n        plt.show()\n        \n    def print_report(self):\n        \"\"\"\n        Predict and evaluate using ground truth from labels\n        Test generator did not shuffle \n        and we can use true labels for comparison\n        \"\"\"\n        #Print classification report\n        print(metrics.classification_report(self.y_true, self.y_pred))\n\n# Create evaluator instance\nevaluator = Evaluator(model, training, generators.test_generator, test_df.label.values)\n\n# Draw accuracy and loss charts\nevaluator.plot_history()\n\n# ROC curve\nevaluator.plot_roc()\n\n# Classification report\nevaluator.print_report()\n","cde32c4a":"class Submitter:\n    \"\"\"\n    Predict and submit\n    \"\"\"\n    def __init__(self, model, img_size):\n        self.model = model\n        batch_size=1000\n        print(\"Initializing submitter\")\n        #Submission generator\n        # flow_from_directory for input\/test didn't work for me, so quick fix is to use flow_from_dataframe with list of files\n        # Load list of files from test folder into dataframe\n        test_files_df=pd.DataFrame()\n        test_files_df['file']=os.listdir('..\/input\/test\/')\n        print(\"Loaded test files list\")\n        # Create generator in it\n        self.generator=ImageDataGenerator(rescale=1.\/255.).flow_from_dataframe(\n                    dataframe=test_files_df,\n                    directory=\"..\/input\/test\/\",\n                    x_col=\"file\",\n                    y_col=None,\n                    has_ext=True,\n                    class_mode=None,\n                    batch_size=batch_size,\n                    seed=42,\n                    shuffle=False,\n                    target_size=img_size)    \n        print('Submission generator created')        \n\n    def predict_for_submit(self):\n        \"\"\"\n        Predict submission test data and form dataframe to submit\n        \"\"\"\n        print(\"Forming submission dataframe...\")\n        # Predict\n        y_pred = self.model.predict_generator(self.generator)\n        y_pred = np.argmax(y_pred, axis=1)\n        # Create submission df\n        submission_df = pd.DataFrame({\n            'id':self.generator.filenames,\n            'label':y_pred })\n        # Filename is id, remove extension .tif\n        submission_df['id'] = submission_df['id'].apply(lambda x: x.split('.')[0])\n        print(f\"Submission dataframe created. Rows:{len(submission_df.values)}\")\n        \n        # Write to csv\n        submission_df.to_csv('submission.csv', index=False)\n        print(\"Submission completed: written submission.csv\")\n        return submission_df\n\nif not toy:\n    # Get dataframe for submission\n    submitter = Submitter(model, generators.img_size)\n    submission_df = submitter.predict_for_submit()     \n    submission_df.head()\nelse:\n    submission_df = pd.DataFrame()\n    print(\"Do not submit in toy mode\")","1addaf60":"df_sizes = pd.DataFrame({'Data type':['Train', 'Test', 'Submission'], \n                      'Row count':[len(train_df.values), len(test_df.values), len(submission_df.values)]})\n\nsns.barplot(x='Data type', y='Row count', data=df_sizes)\nplt.title('Rows in data')\nplt.show()","dba3108a":"## View sample images","3283db8d":"## Check labels balancing\nCheck whether labels are balanced.","95f2f53f":"## Finally, look at amounts of the data","5159a70e":"## Create image generator\nKeras **ImageDataGenerator** can work with dataframe of file names. Our train, validation and test dataframes contain file name in **id** column and **ImageDataGenerator** can understand id.","4a92e9ce":"# 1. Introduction\nJust one more kernel with CNN in this competition :) \n\nTo work with large data, use **ImageDataGenerator.flow_from_dataframe**  as input for model. \n\nFirst, import necessary libraries:","7b76aa4c":"# 4. Create and train the model\nPut all creation and training code in one class. Will experiment with model architecture later.\n\n","bece8261":"# 2. Quick EDA","91b18e9a":"# 3. Data preparation\nHere we are going to balance dataset and prepare image generator","f96cdaf8":"Input** input\/train** folder and** train_labels.csv** contains images and labels for train, validation and test. \"Test\" in terms of model evaluation after training, it is **not** submission data from** input\/test** folder.","ab7e69cb":"## Train and test split","b49ace03":"Labels are unbalanced in train dataset, will figure it out in preprocessing stage.","b2283504":"# 5. Submission\nUse **ImageDataGenerator** to reduce memory usage.  My initial idea was to use **generator.flow_from_directory** for **input\/test** folder but it didn't work for me. Quick fix is to use **generator.flow_from_dataframe** on dataframe with list of filenames.","14dfbbd4":"# 5. Evaluate trained model\nAlso put all evaluation code into one class for better code modularity."}}