{"cell_type":{"4bb1f1c6":"code","cc44fbd1":"code","e87076a4":"code","bbb3d10d":"code","e710426f":"code","df079910":"code","8872e9c5":"code","6331c4f5":"code","9f3c249b":"code","2631fba6":"code","50f3c4b5":"code","6b57426b":"code","ebb0902b":"code","015d11d0":"code","f2d5de03":"code","a49019cf":"code","2b79f192":"code","d5841caf":"code","83cb5607":"code","3ff7a0ff":"code","1cf0da92":"code","0c90b0dd":"code","bb6abfe5":"code","94dbd1bc":"code","317c9ed1":"code","a40bed3b":"code","2a948fd5":"code","58312a16":"code","b2f400c4":"code","e2917544":"code","df8696d2":"code","4671fc62":"code","4ba58866":"code","5837c390":"code","de4c4606":"code","b810952f":"code","a7385d3e":"code","239b2c39":"code","4374ed1e":"code","f58b2e9a":"code","7aa2ba9a":"code","d3a3724c":"code","aa502a42":"code","b397428d":"code","4cc4008b":"code","76e6d81c":"code","8dceefd8":"code","91f86b64":"code","b12cb72f":"code","e23dd625":"code","16809894":"code","6ec40d5a":"code","037586fa":"code","02bcae43":"code","aa737c60":"code","6ca7b379":"code","ab05347e":"code","ec596d0f":"code","3c48c276":"code","52736efd":"code","aea76056":"code","1f567ee8":"code","a0712758":"code","9b277a88":"code","864aaedd":"code","c80bb448":"code","9a00b82f":"code","64d62ae0":"code","9641379f":"code","f4d016fc":"code","12611ad3":"code","f6941a58":"code","d36d7394":"code","5b6a4cb7":"code","679a472a":"code","da8b48e6":"code","ef3b85ec":"code","fe1ac064":"code","50e1baab":"code","31867baa":"code","5088f941":"code","18f8d918":"code","f9cb7e85":"code","abf279aa":"code","74a6931b":"code","5aee9e7e":"code","2069cf6b":"code","c2ef6589":"code","44bbe0b6":"code","9c26ac20":"code","c75776bb":"markdown","4eab5240":"markdown","a4936849":"markdown","6e28942e":"markdown","03aea1c2":"markdown","7cd99d46":"markdown","25322f17":"markdown","c12a3fb2":"markdown","0f5e9a56":"markdown","e218e883":"markdown","b7bf7594":"markdown","26295674":"markdown","89b45baf":"markdown","26805954":"markdown","dc3908a0":"markdown","8565f111":"markdown","2aea5695":"markdown","2a9ada26":"markdown","27f340ac":"markdown","a242fc4b":"markdown","03d1c7cb":"markdown","dd19744b":"markdown","d6f54a59":"markdown","5658e54b":"markdown"},"source":{"4bb1f1c6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","cc44fbd1":"import matplotlib.pyplot as plt\nimport seaborn as sns","e87076a4":"df=pd.read_csv(\"..\/input\/diabetes.csv\")","bbb3d10d":"df.shape","e710426f":"df.dtypes","df079910":"df.isnull().sum()","8872e9c5":"df.head()","6331c4f5":"df['Pregnancies'].value_counts().plot.bar()","9f3c249b":"plt.figure(1)\n\nplt.subplot(121,title='Glucose Distribution')\n\nsns.distplot(df['Glucose'])\n\nprint(\"Skewness: %f\" % df['Glucose'].skew())\n\nplt.subplot(122, title='Glucose - Box Plot ')\ndf['Glucose'].plot.box(figsize=(16,5))\n\nplt.show()","2631fba6":"plt.figure(1)\n\nplt.subplot(121,title='BloodPressure Distribution')\n\nsns.distplot(df['BloodPressure'])\n\nprint(\"Skewness: %f\" % df['BloodPressure'].skew())\n\nplt.subplot(122, title='BloodPressure - Box Plot ')\ndf['BloodPressure'].plot.box(figsize=(16,5))\n\nplt.show()","50f3c4b5":"plt.figure(1)\n\nplt.subplot(121,title='SkinThickness Distribution')\n\nsns.distplot(df['SkinThickness'])\n\nprint(\"Skewness: %f\" % df['SkinThickness'].skew())\n\nplt.subplot(122, title='SkinThickness - Box Plot ')\ndf['SkinThickness'].plot.box(figsize=(16,5))\n\nplt.show()","6b57426b":"plt.figure(1)\n\nplt.subplot(121,title='Insulin Distribution')\n\nsns.distplot(df['Insulin'])\n\nprint(\"Skewness: %f\" % df['Insulin'].skew())\n\nplt.subplot(122, title='Insulin - Box Plot ')\ndf['Insulin'].plot.box(figsize=(16,5))\n\nplt.show()","ebb0902b":"plt.figure(1)\n\nplt.subplot(121,title='BMI Distribution')\n\nsns.distplot(df['BMI'])\n\nprint(\"Skewness: %f\" % df['BMI'].skew())\n\nplt.subplot(122, title='BMI - Box Plot ')\ndf['BMI'].plot.box(figsize=(16,5))\n\nplt.show()","015d11d0":"plt.figure(1)\n\nplt.subplot(121,title='DiabetesPedigreeFunction Distribution')\n\nsns.distplot(df['DiabetesPedigreeFunction'])\n\nprint(\"Skewness: %f\" % df['DiabetesPedigreeFunction'].skew())\n\nplt.subplot(122, title='DiabetesPedigreeFunction - Box Plot ')\ndf['DiabetesPedigreeFunction'].plot.box(figsize=(16,5))\n\nplt.show()","f2d5de03":"df['Age'].value_counts().plot.pie()","a49019cf":"def plot_bar(df,stack=False,displayVal=True):\n    ax = df.plot(kind='bar',figsize=(16,5),stacked=stack) \n    if displayVal:\n        for p in ax.patches:\n            h=round(p.get_height(),2)\n            x=round(p.get_x(),2)\n            ax.annotate(str(h), (x, h))","2b79f192":"#creating bins for the field\nbins=[15,30,45,60,75,90]\ngroup=['0-15','15-30','30-45', '60-75','>75']\ndf['age_bin']=pd.cut(df['Age'],bins,labels=group)\n#print(income_df_95)","d5841caf":"#Checking the how age of the person is impacting the diabetes status.\nx=pd.crosstab(df['age_bin'],df['Outcome'])\nx=x.div(x.sum(1).astype(float), axis=0)\nplot_bar(x,stack=False)","83cb5607":"#Checking the how Pregnancies of the person is impacting the diabetes status.\nx=pd.crosstab(df['Pregnancies'],df['Outcome'])\nx=x.div(x.sum(1).astype(float), axis=0)\nplot_bar(x,stack=False)","3ff7a0ff":"corr=df.corr()\nfig, ax = plt.subplots(figsize=(15,15)) \nsns.heatmap(corr,annot=True,linewidths=.5, ax=ax,fmt='.2f')","1cf0da92":"df.head(10)","0c90b0dd":"#Lets remove the column age-bin\ndf.drop('age_bin',axis=1,inplace=True)","bb6abfe5":"df.head(10)","94dbd1bc":"df.describe()","317c9ed1":"from sklearn.model_selection import train_test_split","a40bed3b":"# Putting feature variable to X\nX=df.drop('Outcome',axis=1)\nX.head(10)","2a948fd5":"# Putting response variable to y\ny=df['Outcome']\ny.head(5)","58312a16":"X_train,X_test,y_train,y_test=train_test_split(X,y,train_size=0.70,test_size=0.20,random_state=100)","b2f400c4":"X_train.columns","e2917544":"from sklearn.preprocessing import StandardScaler ","df8696d2":"scaler=StandardScaler()\nX_train[X_train.columns]=scaler.fit_transform(X_train[X_train.columns])","4671fc62":"X_train.head()","4ba58866":"import statsmodels.api as sm","5837c390":"# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","de4c4606":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()","b810952f":"from sklearn.feature_selection import RFE\nrfe = RFE(logreg, 6)             # running RFE with 13 variables as output\nrfe = rfe.fit(X_train, y_train)","a7385d3e":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","239b2c39":"col = X_train.columns[rfe.support_]","4374ed1e":"X_train.columns[~rfe.support_]","f58b2e9a":"X_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","7aa2ba9a":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","d3a3724c":"y_train_pred_final = pd.DataFrame({'Outcome':y_train.values, 'Outcome_Prob':y_train_pred})\ny_train_pred_final['Row_id'] = y_train.index\ny_train_pred_final.head()","aa502a42":"##### Creating new column 'predicted' with 1 if Outcome_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.Outcome_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","b397428d":"from sklearn import metrics","4cc4008b":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Outcome, y_train_pred_final.predicted )\nprint(confusion)","76e6d81c":"### Predicted     Diabetes(-)    Diabetes(+)\n### Actual\n### Diabetes(-)       307        44\n### Diabetes(+)        75        111","8dceefd8":"TN=307  #True -Ve\nFN=44   #False -ve\nFP=75   #False +ve\nTP=111  #True +ve\n\naccuracy=((TP+TN)\/(TP+TN+FP+FN))\nrecall=((TP)\/(TP+FP))\nprecision = ((TP)\/(FN+TP))\nprint(accuracy)\nprint(recall)\nprint(precision)","91f86b64":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Outcome, y_train_pred_final.predicted)","b12cb72f":"from sklearn.metrics import precision_score, recall_score","e23dd625":"precision_score(y_train_pred_final.Outcome, y_train_pred_final.predicted)","16809894":"recall_score(y_train_pred_final.Outcome, y_train_pred_final.predicted)","6ec40d5a":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","037586fa":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","02bcae43":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","aa737c60":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Outcome, y_train_pred_final.Outcome_Prob, drop_intermediate = False )","6ca7b379":"draw_roc(y_train_pred_final.Outcome, y_train_pred_final.Outcome_Prob)","ab05347e":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Outcome_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","ec596d0f":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Outcome, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","3c48c276":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","52736efd":"y_train_pred_final['final_predicted'] = y_train_pred_final.Outcome_Prob.map( lambda x: 1 if x > 0.3 else 0)\n\ny_train_pred_final.head()","aea76056":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Outcome, y_train_pred_final.final_predicted)","1f567ee8":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Outcome, y_train_pred_final.final_predicted )\nconfusion2","a0712758":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","9b277a88":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","864aaedd":"# Let us calculate specificity\nTN \/ float(TN+FP)","c80bb448":"from sklearn.metrics import precision_recall_curve","9a00b82f":"p, r, thresholds = precision_recall_curve(y_train_pred_final.Outcome, y_train_pred_final.Outcome_Prob)","64d62ae0":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","9641379f":"X_test[X_test.columns] = scaler.transform(X_test[X_test.columns])","f4d016fc":"X_test = X_test[col]\nX_test.head()","12611ad3":"X_test_sm = sm.add_constant(X_test)","f6941a58":"y_test_pred = res.predict(X_test_sm)","d36d7394":"y_test_pred[:10]","5b6a4cb7":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)","679a472a":"# Let's see the head\ny_pred_1.head()","da8b48e6":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","ef3b85ec":"# Putting CustID to index\ny_test_df['row_id'] = y_test_df.index","fe1ac064":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","50e1baab":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","31867baa":"y_pred_final.head()","5088f941":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Outcome_Prob'})","18f8d918":"# Rearranging the columns\ny_pred_final = y_pred_final.reindex_axis(['row_id','Outcome','Outcome_Prob'], axis=1)","f9cb7e85":"# Let's see the head of y_pred_final\ny_pred_final.head()","abf279aa":"y_pred_final['final_predicted'] = y_pred_final.Outcome_Prob.map(lambda x: 1 if x > 0.40 else 0)","74a6931b":"y_pred_final.head()","5aee9e7e":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.Outcome, y_pred_final.final_predicted)","2069cf6b":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Outcome, y_train_pred_final.final_predicted )\nconfusion2","c2ef6589":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","44bbe0b6":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","9c26ac20":"# Let us calculate specificity\nTN \/ float(TN+FP)","c75776bb":"We can clearly see that plot is  more flat on the right side and skewness of the plot is also not between -0.5 and +0.5. So distribution of this field is possitively skewed. Data for this field is not distributed normally","4eab5240":"##### Creating a dataframe with the actual churn flag and the predicted probabilities","a4936849":"****Data Preprocessing****","6e28942e":"It is clear that all the values in each featrue on a different scale and lets perform scalling","03aea1c2":"We can clearly see that plot is fairly distributed and skeness of the plot is also -0.5 and +0.5.","7cd99d46":"We can clearly see that plot is  more flat on the left side and skewness of the plot is also not between -0.5 and +0.5. So distribution of this field is negatively skewed. Data for this field is not distributed normally","25322f17":"We can clearly see that:\n* People within age group 15-30 and 30-45 have more prone to diabetes","c12a3fb2":"We can clearly see that plot is fairly distributed and skeness of the plot is also -0.5 and +0.5.","0f5e9a56":"1. It is clear that P value for Skinthickness and insulin is greater than 0.05 therefore these two featrues is not significant for prediction of our target variable","e218e883":"So we have outcome variable as 0 and 1. 1 means patient have diabetes and 0 means patient don't have diabetes","b7bf7594":"We can clearly see that plot is fairly distributed and skeness of the plot is also -0.5 and +0.5.","26295674":"#### From the curve above, 0.3 is the optimum point to take it as a cutoff probability.","89b45baf":"## Making predictions on the test set","26805954":"## Logistic Model Building","dc3908a0":"In the dataset have data who have pregnancy for 1,0,2 4,5 months[top 5]","8565f111":"There are 789 observations and 9 features","2aea5695":"We can clearly see that \n* People with pregnancies month as 7,8,9,11 have more prone to diabetes","2a9ada26":"**Bi-variate Analysis**","27f340ac":" **Feature Selection Using RFE**","a242fc4b":"So there are no feature which have null values","03d1c7cb":"1. Outcome(diabetes status) is 47% dependent on the value of glucose\n2. Outocme(diabetes status) is 29% depentdent on the BMI and 24% on age\n3. Outcome(diabetes status) is just 7% depedent on the blood pressure and skin thickness","dd19744b":"*Exploratory Data Analysis:*\n**Univariate Analysis**","d6f54a59":"Above plot clearly says that dataset have more data for young people as compared to old people","5658e54b":"We can clearly see that plot is fairly distributed and skeness of the plot is also -0.5 and +0.5."}}