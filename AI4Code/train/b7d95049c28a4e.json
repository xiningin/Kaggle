{"cell_type":{"82a15191":"code","49fadb82":"code","e82a8c85":"code","434adf94":"code","a7dfd56b":"code","3b87dc90":"code","14e36b12":"code","c9dc3aeb":"code","d8322a89":"code","b8aa4e32":"code","d49be4a7":"code","9211c718":"code","d8845f05":"code","cb81154d":"code","26aa8cb7":"code","945b6950":"code","484bdc5b":"code","3015b8e3":"code","6d11572b":"code","36e61873":"code","46cdb686":"code","e7b3a403":"code","45186c1d":"code","ad2a4534":"code","a497aa7e":"code","2626b42d":"code","1d5efd9f":"code","4a5cf606":"code","1e2965c9":"code","fe0c2316":"code","e490ec34":"code","e7134294":"code","3336b163":"code","a4cbcea6":"code","d8b072a6":"code","a29a2770":"code","3bfcd856":"code","7d79c601":"code","cab962bf":"code","2775a392":"code","8d3140dd":"code","93b4ed0e":"code","4ccff02e":"code","e9612750":"code","eb1c004b":"code","3c82c735":"code","ef0dc9fd":"code","ebf8d05b":"code","85515e57":"code","e7366f3c":"code","a821e755":"code","a2090568":"code","943f92d8":"code","4b9587ce":"code","bb3d48ea":"code","acb32abe":"code","fffd75b2":"code","a4ec94e5":"code","1f673006":"code","a5366979":"code","1a887c1b":"code","eca7b7cc":"code","7ac85332":"code","87ee2ffe":"code","d6cbdcd2":"code","e9eb07c9":"code","55851fb0":"code","aad269d8":"code","e953e70e":"code","8abd844f":"code","10d1c99b":"code","577a5eb1":"markdown","5bea938c":"markdown","25cb7f6b":"markdown","51820dc7":"markdown","acac820e":"markdown","f46fb960":"markdown","ade0ec9d":"markdown","3775466d":"markdown","db0f66fb":"markdown","a9df0d2b":"markdown","03c99c53":"markdown","640fde36":"markdown","0bffdc16":"markdown","b978a5a3":"markdown","27910c76":"markdown","01213863":"markdown","daaffddf":"markdown","4a4a29ee":"markdown","5cab5d0e":"markdown","f462ae04":"markdown","c2940ecc":"markdown","aecbcd43":"markdown","15a9bc5f":"markdown","83794a05":"markdown","acb1cb26":"markdown","8828277f":"markdown","cc3a9c13":"markdown","abec3662":"markdown","b7a03220":"markdown"},"source":{"82a15191":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor, BaggingRegressor\nfrom nltk.stem.snowball import SnowballStemmer\n\nstemmer = SnowballStemmer('english')\n\n# training_data = pd.read_csv(\"input\/train.csv.zip\", encoding=\"ISO-8859-1\")\n# testing_data = pd.read_csv(\"input\/test.csv.zip\", encoding=\"ISO-8859-1\")\n# attribute_data = pd.read_csv('input\/attributes.csv.zip')\n# descriptions = pd.read_csv('input\/product_descriptions.csv.zip')\n\n\ntraining_data = pd.read_csv(\"..\/input\/train.csv\", encoding=\"ISO-8859-1\")\ntesting_data = pd.read_csv(\"..\/input\/test.csv\", encoding=\"ISO-8859-1\")\nattribute_data = pd.read_csv('..\/input\/attributes.csv')\ndescriptions = pd.read_csv('..\/input\/product_descriptions.csv')\n","49fadb82":"print(\"training data shape is:\",training_data.shape)\nprint(\"testing data shape is:\",testing_data.shape)\nprint(\"attribute data shape is:\",attribute_data.shape)\nprint(\"description data shape is:\",descriptions.shape)","e82a8c85":"print(\"training data has empty values:\",training_data.isnull().values.any())\nprint(\"testing data has empty values:\",testing_data.isnull().values.any())\nprint(\"attribute data has empty values:\",attribute_data.isnull().values.any())\nprint(\"description data has empty values:\",descriptions.isnull().values.any())","434adf94":"training_data.head(10)","a7dfd56b":"print(\"there are in total {} products \".format(len(training_data.product_title.unique())))\nprint(\"there are in total {} search query \".format(len(training_data.search_term.unique())))\nprint(\"there are in total {} product_uid\".format(len(training_data.product_uid.unique())))\n\n\n","3b87dc90":"testing_data.head(10)","14e36b12":"print(\"there are in total {} products \".format(len(testing_data.product_title.unique())))\nprint(\"there are in total {} search query \".format(len(testing_data.search_term.unique())))\nprint(\"there are in total {} product_uid\".format(len(testing_data.product_uid.unique())))\n\n\n\n","c9dc3aeb":"attribute_data.head(10)","d8322a89":"print(\"there are in total {} product_uid \".format(len(attribute_data.product_uid.unique())))\nprint(\"there are in total {} names \".format(len(attribute_data.name.unique())))\nprint(\"there are in total {} values\".format(len(attribute_data.value.unique())))\n\n\n\n\n","b8aa4e32":"descriptions.head(10)","d49be4a7":"print(\"there are in total {} product_uid \".format(len(descriptions.product_uid.unique())))\nprint(\"there are in total {} product_descriptions \".format(len(descriptions.product_description.unique())))\n\n\n\n\n\n","9211c718":"(descriptions.product_description.str.count('\\d+') + 1).hist(bins=30)\n(descriptions.product_description.str.count('\\W')+1).hist(bins=30)\n\n\n","d8845f05":"(training_data.product_title.str.count(\"\\\\d+\") + 1).hist(bins=30)#plot number of digits in title\n(training_data.product_title.str.count(\"\\\\w+\") + 1).hist(bins=30)#plot number of digits in title\n\n\n\n","cb81154d":"(training_data.search_term.str.count(\"\\\\w+\") + 1).hist(bins=30) #plot number of words in search therms\n(training_data.search_term.str.count(\"\\\\d+\") + 1).hist(bins=30) #plot number of digits in search terms\n\n\n\n\n\n","26aa8cb7":"(training_data.relevance ).hist(bins=30)","945b6950":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm  \n\ntraining_data.relevance.plot(kind='hist', normed=True)\n\nmu, std = norm.fit(training_data.relevance)\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mu, std)\nplt.plot(x, p, 'k', linewidth=2)\ntitle = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\nplt.title(title)\n\nplt.show()","484bdc5b":"print('total data has html tags in',descriptions.product_description.str.count('<br$').values.sum())","3015b8e3":"descriptions[descriptions.product_description.str.contains(\"<br\")].values.tolist()[:3]","6d11572b":"descriptions.product_description.str.contains(\"Click here to review our return policy for additional information regarding returns\").values.sum()","36e61873":"training_data[training_data.search_term.str.contains(\"^\\\\d+ . \\\\d+$\")].head(10)","46cdb686":"training_data[training_data.product_uid==100030]","e7b3a403":"## let's create first the cleaning functions\nfrom bs4 import BeautifulSoup\nimport lxml\nimport re\nimport nltk\nfrom nltk.corpus import stopwords # Import the stop word list\nfrom nltk.metrics import edit_distance\nfrom string import punctuation\nfrom collections import Counter\n\n\ndef remove_html_tag(text):\n    soup = BeautifulSoup(text, 'lxml')\n    text = soup.get_text().replace('Click here to review our return policy for additional information regarding returns', '')\n    return text\n\ndef str_stemmer(doc):\n    # split into tokens by white space\n    tokens = doc.split()\n    # remove punctuation from each token\n    table = str.maketrans('', '', punctuation)\n    tokens = [w.translate(table) for w in tokens]\n    # remove remaining tokens that are not alphabetic\n    tokens = [word for word in tokens if word.isalpha()]\n    # filter out stop words\n    stop_words = set(stopwords.words('english'))\n    tokens = [w for w in tokens if not w in stop_words]\n    # filter out short tokens\n    tokens = [word for word in tokens if len(word) > 1]\n    return ' '.join(tokens)\n\ndef str_stemmer_title(s):\n#     return \" \".join([stemmer.stem(word) for word in s.lower().split()])\n    return \" \".join(map(stemmer.stem, s.lower().split()))\n\ndef str_common_word(str1, str2):\n    whole_set = set(str1.split())\n#     return sum(int(str2.find(word)>=0) for word in whole_set)\n    return sum(int(str2.find(word)>=0) for word in whole_set)\n\n\ndef get_shared_words(row_data):\n    return np.sum([str_common_word(*row_data[:-1]), str_common_word(*row_data[1:])])\n\n","45186c1d":"############### cleaning html tags ##################\nhas_tag_in = descriptions.product_description.str.contains('<br')\ndescriptions.loc[has_tag_in, 'product_description'] = descriptions.loc[has_tag_in, 'product_description'].map(lambda x:remove_html_tag(x))\n###############","ad2a4534":"import requests\nimport re\nimport time\nfrom random import randint\n\nSTART_SPELL_CHECK=\"<span class=\\\"spell\\\">Showing results for<\/span>\"\nEND_SPELL_CHECK=\"<br><span class=\\\"spell_orig\\\">Search instead for\"\nHTML_Codes = ((\"'\", '&#39;'),('\"', '&quot;'),('>', '&gt;'),('<', '&lt;'),('&', '&amp;'))\n\ndef spell_check(s):\n    q = '+'.join(s.split())\n    time.sleep(  randint(0,1) ) #relax and don't let google be angry\n    r = requests.get(\"https:\/\/www.google.co.uk\/search?q=\"+q)\n    content = r.text\n    start=content.find(START_SPELL_CHECK) \n    if ( start > -1 ):\n        start = start + len(START_SPELL_CHECK)\n        end=content.find(END_SPELL_CHECK)\n        search= content[start:end]\n        search = re.sub(r'<[^>]+>', '', search)\n        for code in HTML_Codes:\n            search = search.replace(code[1], code[0])\n        search = search[1:]\n    else:\n        search = s\n    return search ","a497aa7e":"training_data = pd.merge(training_data, descriptions, \n                         on=\"product_uid\", how=\"left\")","2626b42d":"training_data.head(3)","1d5efd9f":"print(\"It has blank\/empty fields \",training_data.isnull().values.sum())\n","4a5cf606":"print(\"has blank\/empty values\",training_data.isnull().values.any())","1e2965c9":"from nltk.corpus import brown, stopwords\nfrom nltk.cluster.util import cosine_distance\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom collections import Counter\n\n\ndef sentence_similarity(columns,stopwords=None):\n    sent1, sent2 = columns[0], columns[1]\n    sent1 = sent1.split(' ')\n    sent2 = sent2.split(' ')\n    if stopwords is None:\n        stopwords = []\n \n    sent1 = [w.lower() for w in sent1]\n    sent2 = [w.lower() for w in sent2]\n \n    all_words = list(set(sent1 + sent2))\n \n    vector1 = [0] * len(all_words)\n    vector2 = [0] * len(all_words)\n \n    # build the vector for the first sentence\n    for w in sent1:\n        if w in stopwords:\n            continue\n        vector1[all_words.index(w)] += 1\n \n    # build the vector for the second sentence\n    for w in sent2:\n        if w in stopwords:\n            continue\n        vector2[all_words.index(w)] += 1\n \n    return 1 - cosine_distance(vector1, vector2)\n\ndef get_jaccard_sim(columns): \n    str1, str2 = columns[0], columns[1]\n    a = set(str1.split()) \n    b = set(str2.split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\n\ndef calc_edit_dist(row):\n    return edit_distance(*row)\n\n    ","fe0c2316":"################begin testing\n## let's create first the cleaning functions\nfrom bs4 import BeautifulSoup\nimport lxml\nimport re\nimport nltk\nfrom nltk.corpus import stopwords # Import the stop word list\nfrom nltk.metrics import edit_distance\nfrom string import punctuation\nfrom collections import Counter\n\n\ndef remove_html_tag(text):\n    soup = BeautifulSoup(text, 'lxml')\n    text = soup.get_text().replace('Click here to review our return policy for additional information regarding returns', '')\n    return text\n\ndef str_stemmer(doc):\n    # split into tokens by white space\n    tokens = doc.split()\n    # remove punctuation from each token\n    table = str.maketrans('', '', punctuation)\n    tokens = [w.translate(table) for w in tokens]\n    # remove remaining tokens that are not alphabetic\n    tokens = [word for word in tokens if word.isalpha()]\n    # filter out stop words\n    stop_words = set(stopwords.words('english'))\n    tokens = [w for w in tokens if not w in stop_words]\n    # filter out short tokens\n    tokens = [word for word in tokens if len(word) > 1]\n    return ' '.join(tokens)\n\n\ndef str_stemmer_tokens(tokens):\n    # split into tokens by white space\n#     tokens = doc.split()\n    # remove punctuation from each token\n    table = str.maketrans('', '', punctuation)\n    tokens = [w.translate(table) for w in tokens]\n    # remove remaining tokens that are not alphabetic\n    tokens = [word for word in tokens if word.isalpha()]\n    # filter out stop words\n    stop_words = set(stopwords.words('english'))\n    tokens = [w for w in tokens if not w in stop_words]\n    # filter out short tokens\n    tokens = [word for word in tokens if len(word) > 1]\n    return ' '.join(tokens)\n\ndef str_stemmer_title(s):\n    return \" \".join(map(stemmer.stem, s))\n\ndef str_common_word(str1, str2):\n    whole_set = set(str1.split())\n#     return sum(int(str2.find(word)>=0) for word in whole_set)\n    return sum(int(str2.find(word)>=0) for word in whole_set)\n\n\n# def str_common_word(str1, str2):\n#     return sum(int(str2.find(word)>=0) for word in str1.split())\n\n\ndef str_common_word2(str1, str2):\n    part_of_first = set(str1)\n    return sum(1 for word in str2 if word in part_of_first)\n#     return sum(int(str2.find(word)>=0) for word in str1.split())\n\ndef get_shared_words_mut(row_data):\n    return np.sum([str_common_word2(*row_data[:-1]), str_common_word2(*row_data[1:])])\n\n\ndef get_shared_words_imut(row_data):\n    return np.sum([str_common_word(*row_data[:-1]), str_common_word2(*row_data[1:])])\n    \nfrom nltk.corpus import brown, stopwords\nfrom nltk.cluster.util import cosine_distance\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom collections import Counter\n\n\ndef sentence_similarity(columns,stopwords=None):\n    sent1, sent2 = columns[0], columns[1]\n    if stopwords is None:\n        stopwords = []\n \n    sent1 = [w.lower() for w in sent1]\n    sent2 = [w.lower() for w in sent2]\n \n    all_words = list(set(sent1 + sent2))\n \n    vector1 = [0] * len(all_words)\n    vector2 = [0] * len(all_words)\n \n    # build the vector for the first sentence\n    for w in sent1:\n        if w in stopwords:\n            continue\n        vector1[all_words.index(w)] += 1\n \n    # build the vector for the second sentence\n    for w in sent2:\n        if w in stopwords:\n            continue\n        vector2[all_words.index(w)] += 1\n \n    return 1 - cosine_distance(vector1, vector2)\n\ndef get_jaccard_sim(columns): \n    str1, str2 = columns[0], columns[1]\n    a = set(str1) \n    b = set(str2)\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n","e490ec34":"############## apply stemming #####################\n#  also .apply(, raw=True) might be a good options\n# https:\/\/github.com\/s-heisler\/pycon2017-optimizing-pandas to see why it is done on this way\n############## apply stemming #####################\n\n\n\ntraining_data['search_term_tokens'] = training_data.search_term.str.lower().str.split()\ntraining_data['product_title_tokens'] = training_data.product_title.str.lower().str.split()\ntraining_data['product_description_tokens'] = training_data.product_description.str.lower().str.split()\n\ntraining_data['search_term'] = [str_stemmer_title(_) for _ in training_data.search_term_tokens.values.tolist()]\ntraining_data['product_title'] = [str_stemmer_tokens(_) for _ in training_data.product_title_tokens.values.tolist()]\ntraining_data['product_description'] = [str_stemmer_tokens(_) for _ in training_data.product_description_tokens.values.tolist()]\n\n\ntraining_data['shared_words_mut'] = [get_shared_words_mut(columns)\n                         for columns in \n                         training_data[['search_term_tokens', 'product_title_tokens', 'product_description_tokens']].values.tolist()\n                        ]\n\ntraining_data['shared_words'] = list(map(get_shared_words_imut, training_data[['search_term','product_description', 'product_title']].values))\n\n\ntraining_data['j_dis_sqt'] = [get_jaccard_sim(rows) for rows in training_data[[\"search_term_tokens\",\"product_title_tokens\"]].values]\ntraining_data['j_dis_sqd'] = [get_jaccard_sim(rows) for rows in training_data[[\"search_term_tokens\",\"product_description_tokens\"]].values]\n\ntraining_data['search_query_length'] = training_data.search_term.str.len()\ntraining_data['number_of_words_in_descr'] = training_data.product_description.str.count(\"\\\\w+\")\n\n\ntraining_data['cos_dis_sqt'] = [ sentence_similarity(rows) for rows in training_data[[\"search_term\",\"product_title\"]].values]\ntraining_data['cos_dis_sqd'] = [sentence_similarity(rows) for rows in training_data[[\"search_term\",\"product_description\"]].values]\n\n\n","e7134294":"# this two lines takeing too long time to execute\n# training_data[\"edistance_sprot\"] = [edit_distance(word1, word2) for word1, word2 in\n#                                     training_data[[\"search_term\",\"product_title\"]].values.tolist()]\n\n\n# training_data[\"edistance_sd\"] = [edit_distance(word1, word2) for word1, word2 in\n#                                     training_data[[\"search_term\",\"product_description\"]].values.tolist()]","3336b163":"# training_data.corr()\ntraining_data.head(3)","a4cbcea6":"testing_data = pd.merge(testing_data, descriptions, \n                         on=\"product_uid\", how=\"left\")\nprint(\"has blank\/empty values\",testing_data.isnull().values.any())","d8b072a6":"############## apply stemming for test data #####################\n# testing_data['search_term'] = list(map(str_stemmer_title, testing_data['search_term'].values))\n# testing_data['product_title'] = list(map(str_stemmer, testing_data['product_title'].values))\n# testing_data['product_description'] = list(map(str_stemmer, testing_data['product_description'].values))\ntesting_data['search_term_tokens'] = testing_data.search_term.str.lower().str.split()\ntesting_data['product_title_tokens'] = testing_data.product_title.str.lower().str.split()\ntesting_data['product_description_tokens'] = testing_data.product_description.str.lower().str.split()\n\ntesting_data['search_term'] = [str_stemmer_title(_) for _ in testing_data.search_term_tokens.values.tolist()]\ntesting_data['product_title'] = [str_stemmer_tokens(_) for _ in testing_data.product_title_tokens.values.tolist()]\ntesting_data['product_description'] = [str_stemmer_tokens(_) for _ in testing_data.product_description_tokens.values.tolist()]\n\n############## end stemming #####################","a29a2770":"############## building custome feature for test data, let's build a few of them before compare which one is the best ###########\n# testing_data['shared_words'] = list(map(get_shared_words, testing_data[['search_term','product_description', 'product_title']].values))\n# testing_data[\"edistance_sprot\"] = list(map(calc_edit_dist, testing_data[[\"search_term\",\"product_title\"]].values))\n# testing_data[\"edistance_sd\"] = list(map(calc_edit_dist, testing_data[[\"search_term\",\"product_description\"]].values))\n\n\n# testing_data['cos_dis_sqt'] = list(map(sentence_similarity ,testing_data[[\"search_term\",\"product_title\"]].values))\n# testing_data['cos_dis_sqd'] = list(map(sentence_similarity, testing_data[[\"search_term\",\"product_description\"]].values))\n\n\n\n# testing_data['j_dis_sqt'] = list(map(get_jaccard_sim, testing_data[[\"search_term\",\"product_title\"]].values))\n# testing_data['j_dis_sqd'] = list(map(get_jaccard_sim, testing_data[[\"search_term\",\"product_description\"]].values))\n\n# testing_data['j_dis_sqt'] = list(map(get_jaccard_sim, testing_data[[\"search_term\",\"product_title\"]].values))\n# testing_data['j_dis_sqd'] = list(map(get_jaccard_sim, testing_data[[\"search_term\",\"product_description\"]].values))\n\n# testing_data['search_query_length'] = testing_data.search_term.str.len()\n# testing_data['number_of_words_in_descr'] = testing_data.product_description.str.count(\"\\\\w+\")\n\ntesting_data['shared_words_mut'] = [get_shared_words_mut(columns)\n                         for columns in \n                         testing_data[['search_term_tokens', 'product_title_tokens', 'product_description_tokens']].values.tolist()\n                        ]\n\ntesting_data['shared_words'] = list(map(get_shared_words_imut, testing_data[['search_term','product_description', 'product_title']].values))\n\n\ntesting_data['j_dis_sqt'] = [get_jaccard_sim(rows) for rows in testing_data[[\"search_term_tokens\",\"product_title_tokens\"]].values]\ntesting_data['j_dis_sqd'] = [get_jaccard_sim(rows) for rows in testing_data[[\"search_term_tokens\",\"product_description_tokens\"]].values]\n\ntesting_data['search_query_length'] = testing_data.search_term.str.len()\ntesting_data['number_of_words_in_descr'] = testing_data.product_description.str.count(\"\\\\w+\")\n\n\ntesting_data['cos_dis_sqt'] = [ sentence_similarity(rows) for rows in testing_data[[\"search_term\",\"product_title\"]].values]\ntesting_data['cos_dis_sqd'] = [sentence_similarity(rows) for rows in testing_data[[\"search_term\",\"product_description\"]].values]\n\n\n","3bfcd856":"#this two lines taking too long to execute\n\n# testing_data[\"edistance_sprot\"] = [edit_distance(word1, word2) for word1, word2 in\n#                                     testing_data[[\"search_term\",\"product_title\"]].values.tolist()]\n\n\n# testing_data[\"edistance_sd\"] = [edit_distance(word1, word2) for word1, word2 in\n#                                     testing_data[[\"search_term\",\"product_description\"]].values.tolist()]\n","7d79c601":"testing_data.corr()","cab962bf":"training_data.describe()","2775a392":"testing_data.describe()","8d3140dd":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(12, 12))\ntemp = training_data.drop(['product_uid','id'],axis=1)\nsns.heatmap(temp.corr(), annot=True)\nplt.show()","93b4ed0e":"import seaborn as sns\nplt.figure(figsize=(12, 12))\ntemp = testing_data.drop(['product_uid','id'],axis=1)\nsns.heatmap(temp.corr(), annot=True)\nplt.show()","4ccff02e":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm  \n\ntraining_data.cos_dis_sqd.plot(kind='hist', normed=True)\n\nmu, std = norm.fit(training_data.cos_dis_sqd)\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mu, std)\nplt.plot(x, p, 'k', linewidth=2)\ntitle = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\nplt.title(title)\n\nplt.show()","e9612750":"from statsmodels.graphics.gofplots import qqplot\nfrom scipy.stats import shapiro\n\n\nfrom matplotlib import pyplot\nqqplot(training_data.cos_dis_sqd, line='s')\npyplot.show()\n\nstat, p = shapiro(training_data.cos_dis_sqd)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))","eb1c004b":"from scipy.stats import normaltest\n\nstat, p = normaltest(training_data.cos_dis_sqd)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n    print('Sample looks Gaussian (fail to reject H0)')\nelse:\n    print('Sample does not look Gaussian (reject H0)')","3c82c735":"from scipy.stats import anderson\n\nresult = anderson(training_data.cos_dis_sqd)\nprint('Statistic: %.3f' % result.statistic)\np = 0\nfor i in range(len(result.critical_values)):\n    sl, cv = result.significance_level[i], result.critical_values[i]\n    if result.statistic < result.critical_values[i]:\n        print('%.3f: %.3f, data looks normal (fail to reject H0)' % (sl, cv))\n    else:\n        print('%.3f: %.3f, data does not look normal (reject H0)' % (sl, cv))","ef0dc9fd":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm  \n\ntraining_data.cos_dis_sqt.plot(kind='hist', normed=True)\n\nmu, std = norm.fit(training_data.cos_dis_sqt)\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mu, std)\nplt.plot(x, p, 'k', linewidth=2)\ntitle = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\nplt.title(title)\n\nplt.show()","ebf8d05b":"from matplotlib import pyplot\nqqplot(training_data.cos_dis_sqt, line='s')\npyplot.show()\n\nstat, p = shapiro(training_data.cos_dis_sqt)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))","85515e57":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm  \n\ntraining_data.shared_words.plot(kind='hist', normed=True)\n\nmu, std = norm.fit(training_data.shared_words)\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mu, std)\nplt.plot(x, p, 'k', linewidth=2)\ntitle = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\nplt.title(title)\n\nplt.show()","e7366f3c":"from statsmodels.graphics.gofplots import qqplot\nfrom scipy.stats import shapiro\n\n\nfrom matplotlib import pyplot\nqqplot(training_data.shared_words, line='s')\npyplot.show()\n\nstat, p = shapiro(training_data.shared_words)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))","a821e755":"# %matplotlib inline\n# import matplotlib.pyplot as plt\n# from scipy.stats import norm  \n\n# training_data.edistance_sprot.plot(kind='hist', normed=True)\n\n# mu, std = norm.fit(training_data.edistance_sprot)\n\n# xmin, xmax = plt.xlim()\n# x = np.linspace(xmin, xmax, 100)\n# p = norm.pdf(x, mu, std)\n# plt.plot(x, p, 'k', linewidth=2)\n# title = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\n# plt.title(title)\n\n# plt.show()","a2090568":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm  \n\ntraining_data.search_query_length.plot(kind='hist', normed=True)\n\nmu, std = norm.fit(training_data.search_query_length)\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mu, std)\nplt.plot(x, p, 'k', linewidth=2)\ntitle = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\nplt.title(title)\n\nplt.show()","943f92d8":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm  \n\ntesting_data.cos_dis_sqd.plot(kind='hist', normed=True)\n\nmu, std = norm.fit(testing_data.cos_dis_sqd)\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mu, std)\nplt.plot(x, p, 'k', linewidth=2)\ntitle = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\nplt.title(title)\n\nplt.show()","4b9587ce":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm  \n\ntesting_data.cos_dis_sqt.plot(kind='hist', normed=True)\n\nmu, std = norm.fit(testing_data.cos_dis_sqt)\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mu, std)\nplt.plot(x, p, 'k', linewidth=2)\ntitle = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\nplt.title(title)\n\nplt.show()","bb3d48ea":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm  \n\ntesting_data.shared_words.plot(kind='hist', normed=True)\n\nmu, std = norm.fit(testing_data.shared_words)\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mu, std)\nplt.plot(x, p, 'k', linewidth=2)\ntitle = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\nplt.title(title)\n\nplt.show()","acb32abe":"# %matplotlib inline\n# import matplotlib.pyplot as plt\n# from scipy.stats import norm  \n\n# testing_data.edistance_sprot.plot(kind='hist', normed=True)\n\n# mu, std = norm.fit(testing_data.edistance_sprot)\n\n# xmin, xmax = plt.xlim()\n# x = np.linspace(xmin, xmax, 100)\n# p = norm.pdf(x, mu, std)\n# plt.plot(x, p, 'k', linewidth=2)\n# title = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\n# plt.title(title)\n\n# plt.show()","fffd75b2":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm  \n\ntesting_data.search_query_length.plot(kind='hist', normed=True)\n\nmu, std = norm.fit(testing_data.search_query_length)\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mu, std)\nplt.plot(x, p, 'k', linewidth=2)\ntitle = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\nplt.title(title)\n\nplt.show()","a4ec94e5":"training_data.shape\n\nnp.max([np.log(74067) \/ np.log(x) for x in training_data.search_query_length])\n","1f673006":"sns.pairplot(training_data)","a5366979":"# sns.pairplot(testing_dataing_data)\ntraining_data.corr()","1a887c1b":"df_training = training_data.drop(['product_title','search_term','product_description', 'product_title_tokens', 'product_description_tokens','product_title_tokens','search_term_tokens'],axis=1)\n\ny_train = df_training['relevance'].values\nX_train = df_training.drop(['id','relevance'],axis=1).values","eca7b7cc":"df_training.head(3)","7ac85332":"# X_test = testing_data.drop(['id','product_title','search_term','product_description'],axis=1).values\nX_test = testing_data.drop(['id','product_title','search_term','product_description', 'product_title_tokens', 'product_description_tokens','product_title_tokens','search_term_tokens'],axis=1).values\n\nid_test = testing_data['id']\n","87ee2ffe":"from sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor(n_estimators = 1, n_jobs = -1, random_state = 17, verbose = 1)\nrfr.fit(X_train, y_train)\n\ny_pred = rfr.predict(X_test)\n\npd.DataFrame({\"id\": id_test, \"relevance\": y_pred}).to_csv('submission.csv',index=False)\n\n","d6cbdcd2":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression(n_jobs = -1)\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\n\n# pd.DataFrame({\"id\": id_test, \"relevance\": y_pred}).to_csv('submission.csv',index=False)","e9eb07c9":"import sklearn\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nparam_grid = {\n                'loss' : ['ls'],\n                'n_estimators' : [3], \n                'max_depth' : [9],\n                'max_features' : ['auto'] \n             }\n\ngbr = GradientBoostingRegressor()\n\nmodel_gbr = sklearn.model_selection.GridSearchCV(estimator = gbr, n_jobs = -1, param_grid = param_grid)\nmodel_gbr.fit(X_train, y_train)\n\ny_pred = model_gbr.predict(X_test)\n\n# pd.DataFrame({\"id\": id_test, \"relevance\": y_pred}).to_csv('submission.csv',index=False)","55851fb0":"from sklearn.ensemble import BaggingRegressor\nrf = RandomForestRegressor(max_depth = 20, max_features =  'sqrt', n_estimators = 3)\nclf = BaggingRegressor(rf, n_estimators=3, max_samples=0.1, random_state=25)\n\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\n\n# pd.DataFrame({\"id\": id_test, \"relevance\": y_pred}).to_csv('submission.csv',index=False)","aad269d8":"cat","e953e70e":"# define models which will be chained togher in a bigger model, which aims to predict the relevancy score\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor, BaggingRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedKFold\n\n#define standard scaler\nscaler = StandardScaler()\nscaler.fit(X_train, y_train)\nscaled_train_data = scaler.transform(X_train)\nscaled_test_data = scaler.transform(X_test)\n\n\nrf = RandomForestRegressor(n_estimators=4, max_depth=6, random_state=0)\nclf = BaggingRegressor(rf, n_estimators=4, max_samples=0.1, random_state=25)\n\n\npipeline = Pipeline(steps = [('scaling', scaler), ('baggingregressor', clf)])\n#end pipeline \npipeline.fit(X_train, y_train)\ny_pred = pipeline.predict(X_test)\n\n# pd.DataFrame({\"id\": id_test, \"relevance\": y_pred}).to_csv('submission.csv',index=False)\n","8abd844f":"from sklearn.linear_model import BayesianRidge\n\ngnb = BayesianRidge()\nparam_grid = {}\nmodel_nb = sklearn.model_selection.GridSearchCV(estimator = gnb, param_grid = param_grid, n_jobs = -1)\nmodel_nb.fit(X_train, y_train)\n\ny_pred = model_nb.predict(X_test)\n# pd.DataFrame({\"id\": id_test, \"relevance\": y_pred}).to_csv('submission.csv',index=False)","10d1c99b":"from xgboost import XGBRegressor\n\nxgb = XGBRegressor()\nparam_grid = {'max_depth':[5, 6], \n              'n_estimators': [130, 150, 170], \n              'learning_rate' : [0.1]}\nmodel_xgb = sklearn.model_selection.GridSearchCV(estimator = xgb, param_grid = param_grid, n_jobs = -1)\nmodel_xgb.fit(X_train, y_train)\n\ny_pred = model_xgb.predict(X_test)\n# pd.DataFrame({\"id\": id_test, \"relevance\": y_pred}).to_csv('submission.csv',index=False)\n","577a5eb1":"## 3.5 Chain model withing pipeline\n","5bea938c":"## Data Cleaning and Shape Examining \n","25cb7f6b":"let's examing if the same behaviour can be spotted on __testing__ dataset","51820dc7":"let's check wheather this is follows Gaussian distribution or not. Indeed it doesn't follow Gaussian distribution, that follows from Shapiro test","acac820e":"\n\n|Regressor|Train|Kaggle\n|:----------------------|----------|----------|\n|CatBoostRegressor|-|-|\n|XGBRegressor|-|-|\n|GradientBoostingRegressor|-|-|\n|PolynomialFeatures on GradientBoostingRegressor|-|-|\n|PolynomialFeatures on XGBRegressor|-|-|\n|PolynomialFeatures on LinearRegression|-|-|\n|PolynomialFeatures on BaggingRegressor on RandomForestRegressor|-|-|\n|BaggingRegressor on RandomForestRegressor|0.53480|0.53427|\n|Chaining toghether using Pipeline|0.53063|0.53100|\n|BayesianRidge|-|-|\n|LinearRegression|-|-|\n|PolynomialFeatures on BayesianRidge|-|-|\n|RandomForestRegressor|0.59063|0.58869|\n","f46fb960":"# 3. Let's start machine learning\nfirst of all let's create training and test data sets\n","ade0ec9d":"## 3.1 RandomForestRegressor","3775466d":"from above the following conclusion follows. \n   - At first there exists fields which has html tags in for __description__ dataset. (maybe and error made by the scrapper) along with _Click here to review our return policy_\n   - There is no missing\/empty values in any of these datasets \n   - in dataset __description__ field product_description contains more digits than word characters\n   - some query in dataset __training__ are too straight, it's hard to guess exactly what user meant in terms of broad  sense\n   - some of the search query in dataset __training__ has too specific meaning like 8 4616809045 9\t\n   - number of diggits appearence in the product title tends to be greater number of characters for dataset __training__ (and the same is true for search query field)\n   - the relevancy score is between 1 and 3. Because the density of product whose relevancy score is between 2 and 3 is higher we can conclude that most of search query has been classifield between 2 and 3\n   - The histogram of relevancy score doesn't follow standard distribution pattern\n   \n   \n   \nIn order to continue the analysis we will need the whole datasets\n   - description datasets might be joined together to training by the product_uid (the same holds for attribute datasets) then clean the html parts\n   \n   ","db0f66fb":"## Home Depot Product Search Relevance\n\n","a9df0d2b":"### Data description","03c99c53":"Shoppers rely on Home Depot\u2019s product authority to find and buy the latest products and to get timely solutions to their home improvement needs. From installing a new ceiling fan to remodeling an entire kitchen, with the click of a mouse or tap of the screen, customers expect the correct results to their queries \u2013 quickly. Speed, accuracy and delivering a frictionless customer experience are essential.\n\nIn this competition, Home Depot is asking Kagglers to help them improve their customers' shopping experience by developing a model that can accurately predict the relevance of search results.\n\nSearch relevancy is an implicit measure Home Depot uses to gauge how quickly they can get customers to the right products. Currently, human raters evaluate the impact of potential changes to their search algorithms, which is a slow and subjective process. By removing or minimizing human input in search relevance evaluation, Home Depot hopes to increase the number of iterations their team can perform on the current search algorithms.","640fde36":"## Feature Engineering","0bffdc16":"### File descriptions\n\n- train.csv - the training set, contains products, searches, and relevance scores\n- test.csv - the test set, contains products and searches. You must predict the relevance for these pairs.\n- product_descriptions.csv - contains a text description of each product. You may join this table to the training or test set via the product_uid.\n- attributes.csv -  provides extended information about a subset of the products (typically representing detailed technical specifications). Not every product will have attributes.\n- sample_submission.csv - a file showing the correct submission format\n- relevance_instructions.docx - the instructions provided to human raters\n\n### Data fields\n\n- id - a unique Id field which represents a (search_term, product_uid) pair\n- product_uid - an id for the products\n- product_title - the product title\n- product_description - the text description of the product (may contain HTML content)\n- search_term - the search query\n- relevance - the average of the relevance ratings for a given id\n- name - an attribute name\n- value - the attribute's value","b978a5a3":"## 3.2 LinearRegression\n","27910c76":"## 3.6 Naive Bayes","01213863":"## 3.7 XGBoost","daaffddf":"Examing the search query in the datasets __training__, there some misspelings for field _search_term_ contains a lot of misspelling (more than 3000). This might be fixed by using Google API ","4a4a29ee":"__test dataset__\nwe have to have to apply symmetric transformation for both data set, except relevance score field since it is target field. Except we are not allow to take any actions which might lead to overfitting the data","5cab5d0e":"## Data cleaning","f462ae04":"## 3.3 GradientBoostingRegressor","c2940ecc":"let's try to find out if wheather it follows normal distribution or not, by doing a few others test","aecbcd43":"From the below histogram we can conclude that the sum of shared words between search_query product_title, and product description follows the standard distribution.\n","15a9bc5f":"This data set contains a number of products and real customer search terms from Home Depot's website. The challenge is to predict a relevance score for the provided combinations of search terms and products. To create the ground truth labels, Home Depot has crowdsourced the search\/product pairs to multiple human raters.\n\nThe relevance is a number between 1 (not relevant) to 3 (highly relevant). For example, a search for \"AA battery\" would be considered highly relevant to a pack of size AA batteries (relevance = 3), mildly relevant to a cordless drill battery (relevance = 2), and not relevant to a snow shovel (relevance = 1).\n\nEach pair was evaluated by at least three human raters. The provided relevance scores are the average value of the ratings. There are three additional things to know about the ratings:\n\nThe specific instructions given to the raters is provided in relevance_instructions.docx.\nRaters did not have access to the attributes.\nRaters had access to product images, while the competition does not include images.\nYour task is to predict the relevance for each pair listed in the test set. Note that the test set contains both seen and unseen search terms.\n\n","83794a05":"Let's try to examing the data and try to spot if there are anything suspicious about it","acb1cb26":"Indeed correcting the misspelings words might help, due to ability of reproducing the result at Kaggle, we won't do spell correction","8828277f":"### Plan\nWe are going to do the following:\n0. Join dataset __training__ with __description__  by  _product uid_ (already done)\n\n2. Create num columns based on text columns\n    - count number of words from search query which appears both in product_title and product_description\n    - compute edit distnace from search query which appears both in product_title and product_title\n    - compute the cosine similarity between search query, product_title and product_description\n    - count number of words in the product description\n    - create new columns for each pair\n    \n3. Remove all text columns\n\nAs a result we will have vectors of numbers that suites well for the machine learning.","cc3a9c13":"We are going to apply the following models:\n1. RandomForestRegressor\n2. LinearRegression\n4. GradientBoostingRegressor \n5. BaggingRegressor\n6. Chain model withing pipeline\n7. XGBoost\n8. CatBoost\n9. Naive Baies\n10. PolynomialFeatures for all previous algorithms\n\n\n### Plan\nWe are going to do the following:\n0. Define pipeline\n1. drop non numeric columns because these information has been already transformed to numberic\n2. Apply the model which has been mentioned above within pipeline mode and outside pipeline\n3. Train models and compare their result on __test__ dataset\n4. write a summary about it\n\n\n\n","abec3662":"## 3.4 BaggingRegressor based on  RandomForestRegressor","b7a03220":"# Results"}}