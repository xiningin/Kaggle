{"cell_type":{"7e370d38":"code","367fce0d":"code","e34ab779":"code","b516882a":"code","4b845a0a":"code","5d961bc5":"code","ee49c5d3":"code","186018f4":"code","e15926fb":"code","f459c62b":"code","8a5b556a":"code","cb3647e7":"code","20c87606":"code","2337fe0f":"code","dfbfa5ff":"code","934e37a6":"code","6fe80786":"code","a31dc922":"code","f5985295":"code","07fe27d8":"code","9b491b66":"code","33474541":"code","600eb85e":"code","d944e432":"code","be221942":"code","62859594":"code","7eff7625":"code","606c59ca":"code","cde23ef7":"code","cc110c55":"code","7fd1505d":"code","564f5788":"code","9b2a3492":"code","b826168a":"code","b316a8b6":"code","65d74725":"code","fbd4a11d":"code","bfe79de7":"code","baa6bcdd":"code","e1c2e123":"code","d7e91d60":"code","6bf9d7d6":"code","3b868631":"code","b9ad30c4":"code","2db59c25":"code","deb6d491":"markdown","9403559c":"markdown","2c5fbf32":"markdown","952362ad":"markdown","3a3ba4a5":"markdown","78c94477":"markdown","7134eb74":"markdown","397cbc78":"markdown","c3b3eb1d":"markdown","67919a54":"markdown","28e1402c":"markdown","c0d7fa58":"markdown","4c519cfe":"markdown","5a19acee":"markdown","535c2cce":"markdown","2dc8dccd":"markdown","33823a9b":"markdown","d2874330":"markdown","bf67422c":"markdown"},"source":{"7e370d38":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.model_selection import StratifiedKFold","367fce0d":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntrain.head()","e34ab779":"train.shape","b516882a":"test = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ntest.head()","4b845a0a":"train.location.value_counts()","5d961bc5":"train.isnull().sum()","ee49c5d3":"test.isnull().sum()","186018f4":"train[\"keyword\"].replace(np.NAN, \"\", inplace=True)\ntrain[\"location\"].replace(np.NAN, \"\", inplace=True)\ntest[\"keyword\"].replace(np.NAN, \"\", inplace=True)\ntest[\"location\"].replace(np.NAN, \"\", inplace=True)","e15926fb":"train.isnull().sum()","f459c62b":"test.isnull().sum()","8a5b556a":"len(test[\"keyword\"])","cb3647e7":"contents = []\nfor data in [train, test]:\n    for i in range(data.shape[0]):\n        item = data.iloc[i]\n        sentence = item[\"keyword\"] + \" \" + item[\"text\"] + \" \" + item[\"location\"]\n        contents.append(sentence.lower())","20c87606":"tokenizer = tf.keras.preprocessing.text.Tokenizer()\n\ntokenizer.fit_on_texts(contents)","2337fe0f":"tokens = tokenizer.texts_to_sequences(contents)","dfbfa5ff":"word_counter = dict()\nfor token in tokens:\n    for item in token:\n        key = tokenizer.index_word[item]\n        if key in word_counter:\n            word_counter[key] += 1\n        else:\n            word_counter[key] = 1","934e37a6":"word_freq = pd.DataFrame({\"word\": word_counter.keys(), \"count\": word_counter.values()})\nword_freq.sort_values(ascending=False, by=\"count\", inplace=True)\nword_freq.head(10)","6fe80786":"word_freq[\"count\"].plot(kind=\"hist\")","a31dc922":"word_freq[word_freq[\"count\"] < 100].plot(kind=\"hist\")","f5985295":"word_freq[word_freq[\"count\"] <= 10].plot(kind=\"hist\")","07fe27d8":"word_freq[word_freq[\"count\"] <= 3].plot(kind=\"hist\")","9b491b66":"lower_thresold = 1\nword_appear_less = list(word_freq[word_freq[\"count\"] <= lower_thresold][\"word\"])","33474541":"len(word_appear_less)","600eb85e":"list(word_freq[\"word\"][:100])","d944e432":"stop_words = ['co',\n 't',\n 'http',\n 'the',\n 'a',\n 'in',\n 'to',\n 'of',\n 'and',\n 'i',\n 'is',\n 'on',\n 'for',\n 'you',\n 'my',\n 'it',\n 'with',\n 'that',\n 'by',\n 'at',\n 'this',\n 'new',\n 'from',\n 'https',\n 'are',\n 'be',\n 'was',\n 'have',\n 'like',\n 'as',\n 'up',\n 'just',\n 'your',\n 'not',\n 'but',\n 'me',\n 'so',\n 'no',\n 'all',\n 'will',\n 'after',\n 'an',\n 'we',\n \"i'm\",\n 'if',\n 'when',\n 'has',\n 'via',\n 'get',\n 'or',\n '2',\n 'more',\n 'about',\n 'now',\n 'he',\n 'how',\n 'they',\n 'one',\n 'people',\n 'what',\n \"it's\",\n 'who',\n 'news',\n 'over',\n 'been',\n 'do',\n 'ca',\n 'into',\n 'can',\n 'there',\n 'video',\n 'u',\n '3',\n 'would',\n 'world',\n 'her',\n 'us',\n 's',\n 'his',\n 'than',\n \"'\",\n '1',\n 'still',\n 'some'\n]","be221942":"exclude_set = set(word_appear_less + stop_words)","62859594":"new_sentences = []\nfor token in tokens:\n    new_token = []\n    for item in token:\n        word = tokenizer.index_word[item]\n        if not word in exclude_set:\n            new_token.append(word)\n    new_sentences.append(\" \".join(new_token))","7eff7625":"new_sentences[:10]","606c59ca":"new_tokenizer = tf.keras.preprocessing.text.Tokenizer()\nnew_tokenizer.fit_on_texts(new_sentences)","cde23ef7":"new_tokens = new_tokenizer.texts_to_sequences(new_sentences)","cc110c55":"new_tokens_lengths = [len(token) for token in  new_tokens]","7fd1505d":"lengths = pd.DataFrame({\"length\":new_tokens_lengths})","564f5788":"lengths.describe()","9b2a3492":"padding_tokens = tf.keras.preprocessing.sequence.pad_sequences(new_tokens, maxlen=30, padding='post', truncating='post')","b826168a":"x_train = padding_tokens[:len(train)]\ny_train = train[\"target\"]\nx_test = padding_tokens[len(train):]","b316a8b6":"train[\"target\"].value_counts()","65d74725":"x_train.shape","fbd4a11d":"x_train[2]","bfe79de7":"class BinaryCrossEntropy(tf.keras.losses.Loss):\n\n    def __init__(self, postive_rate = 0.5):\n        super().__init__()\n        self.negative_weights = postive_rate\n        self.positive_weights = 1 - postive_rate\n        \n    def call(self, y_true, y_pred):\n        print(y_true, y_pred)\n        y_true = tf.cast(y_true, y_pred.dtype)\n        pos = self.positive_weights * y_true * tf.math.log(y_pred + tf.keras.backend.epsilon())\n        neg = self.negative_weights * (1.0 - y_true) * tf.math.log(1.0 - y_pred + tf.keras.backend.epsilon())\n        return -(pos + neg)","baa6bcdd":"def tranformer_block(inputs, embed_dim, num_heads, ff_dim, dropout_rate=0.1, training=True):\n    attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)\n    attention_output = layers.Dropout(dropout_rate)(attention_output, training=training)\n    out1 = layers.LayerNormalization(epsilon=1e-6)(inputs + attention_output)\n    ffn_output = layers.Dense(ff_dim, activation=\"relu\")(out1)\n    ffn_output = layers.Dense(embed_dim)(ffn_output)\n    ffn_output = layers.Dropout(dropout_rate)(ffn_output, training=training)\n    output = layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n    return output","e1c2e123":"def embedding_block(inputs, maxlen, vocab_size, embed_dim):\n    positions = tf.range(start=0, limit=maxlen, delta=1)\n    positions = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)(positions)\n    x = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)(inputs)\n    return  x + positions","d7e91d60":"def get_model(maxlen, vocab_size, embed_dim, num_heads, ff_dim):\n    inputs = layers.Input(shape=(maxlen, ))\n    x = embedding_block(inputs, maxlen, vocab_size, embed_dim)\n    x = tranformer_block(x, embed_dim, num_heads, ff_dim, dropout_rate=0.1)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(20, activation=\"relu\")(x)\n    x = layers.Dropout(0.3)(x)\n    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n    return keras.Model(inputs=inputs, outputs=outputs)","6bf9d7d6":"vocab_size = len(new_tokenizer.index_word) + 1\nmax_content_length = 30\nprint(vocab_size, max_content_length)","3b868631":"model = get_model(max_content_length, vocab_size, 128, 2, 32)\nkeras.utils.plot_model(model, show_shapes=True)","b9ad30c4":"index = 1\nmodels = []\ntf.keras.backend.clear_session()\nfor train_indices, val_indices in StratifiedKFold(5, shuffle=True, random_state=42).split(x_train, y_train):\n    print(\"Fold %d\" %(index))\n    train_features, train_targets = x_train[train_indices], y_train[train_indices]\n    validation_features, validation_targets = x_train[val_indices], y_train[val_indices]\n    model_checkpoint_path = \"model%d.h5\"%(index)\n    model = get_model(max_content_length, vocab_size, 128, 4, 64)\n    loss = BinaryCrossEntropy(train_targets.mean())\n    adam = tf.keras.optimizers.Adam(1e-4)\n    model.compile(loss=loss, optimizer=adam, metrics=[\"accuracy\"])\n    early_stop = tf.keras.callbacks.EarlyStopping(patience=5)\n    recuce_Lr = tf.keras.callbacks.ReduceLROnPlateau(patience=2)\n    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(model_checkpoint_path, monitor=\"val_accuracy\", save_best_only=True, save_weights_only=True)\n    history = model.fit(train_features, train_targets, validation_data=(validation_features, validation_targets), epochs=100, callbacks=[early_stop, model_checkpoint])\n    pd.DataFrame(history.history).plot(kind=\"line\")\n    plt.title(\"Performance of Fold %d\"%(index))\n    plt.show()\n    model.load_weights(model_checkpoint_path)\n    y_val_pred = np.array(model.predict(validation_features) > 0.5, dtype=\"int\").reshape(-1)\n    cm = confusion_matrix(validation_targets, y_val_pred)\n    sns.heatmap(cm)\n    plt.show()\n    print(\"Classification Report: \\n\")\n    print(classification_report(validation_targets, y_val_pred))\n    acc_score = accuracy_score(validation_targets, y_val_pred)\n    print(\"Accuracy Score: %.2f\"%(acc_score))\n    models.append(model)\n    index += 1","2db59c25":"y_test = np.mean([model.predict(x_test).reshape(-1) for model in models], axis=0)\ny_test = np.array(y_test > 0.5, dtype=int)\nsubmission = pd.DataFrame({\"id\": test[\"id\"], \"target\": y_test})\nsubmission.to_csv(\"submission.csv\", index=False)","deb6d491":"### Embeding Layer\nTwo separate Embedding Layers, one for tokens, another for positions.","9403559c":"### Transformer Block","2c5fbf32":"### Tokenize texts","952362ad":"<a id=\"6.\"><\/a>\n## 6. Submission","3a3ba4a5":"## Table of Contents\n- [1. Overview](#1.)\n- [2. Import Packages and Datasets](#2.)\n- [3. Data Wrangling](#3.)\n- [4. Exploratory Data Analysis & Data Preprocessing](#4.)\n- [5. Model Development](#5.)\n- [6. Submission](#6.)","78c94477":"Let's see first 100 words. Choose stop words based on that. But remove some words related to disasters.","7134eb74":"As we can see, there are 25000 words just appear once and 3000 words appear twice. It would be hard for us to find patterns in them without prior knowledge. Machine can't learn from words just appear once or twice.","397cbc78":"### Lengths","c3b3eb1d":"<a id=\"4.\"><\/a>\n## 4. Exploratory Data Analysis & Data Preprocessing\n- Tokenize Texts\n- Show Staticstic info of texts","67919a54":"<a id=\"1.\"><\/a>\n## 1. Overview\nIn this notebook I will build a Disaster Tweets Classification Model using Transformer.","28e1402c":"<a id=\"3.\"><\/a>\n## 3. Data Wrangling\nLet's see null values for each column.","c0d7fa58":"### Remove words that appear too often","4c519cfe":"<a id=\"5.\"><\/a>\n## Model Development","5a19acee":"### Text Classification Model","535c2cce":"### BinaryCrossEntropy with weights \nUse this version of BinaryCrossEntropy to solve class imbalance problem.","2dc8dccd":"# Disaster Tweets Classification: Transformer","33823a9b":"<a id=\"2.\"><\/a>\n## 2. Import Packages and Datasets ","d2874330":"Create a new tokenizer to preprocess these texts again.","bf67422c":"### Remove words that seldom appears"}}