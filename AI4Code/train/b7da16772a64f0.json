{"cell_type":{"e66122e3":"code","b1b154e5":"code","e7759c50":"code","d802d3c2":"code","1b5059ee":"code","30d6d698":"code","8902b18e":"code","6b940df2":"code","a0073df6":"code","02d65e03":"code","ffebfabc":"code","8c52340d":"code","d194ee2b":"code","e306e905":"code","245fd3aa":"code","b4d6d3c6":"code","c86133a1":"code","bfe24a79":"code","481a82a8":"code","48a4be8b":"code","5bac0e29":"code","b7ed9be1":"code","65a6277b":"code","3113eeb4":"code","b948a97e":"markdown","13db289e":"markdown","2e9fdfc6":"markdown","77324d7d":"markdown","5b7923e2":"markdown","378b45b8":"markdown","9f669a55":"markdown","6e22b747":"markdown","1b2987e5":"markdown","c9510b83":"markdown","3ffc599c":"markdown","8d48a142":"markdown","22f3e081":"markdown","f752e89f":"markdown","5bc3b4e3":"markdown","eaaeca1e":"markdown","4f1fff4b":"markdown","7ddefc5f":"markdown","ccf48e13":"markdown","551fdf74":"markdown","2ef28a6f":"markdown","9e387370":"markdown","aa23ec19":"markdown","d8d884b9":"markdown","ae62fa0b":"markdown"},"source":{"e66122e3":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\nsns.set(style=\"white\")\n\nimport plotly\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n\nfrom plotly.offline import plot, iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\ndf = pd.read_csv('..\/input\/california-housing-prices\/housing.csv')","b1b154e5":"df.head() #  showing first 5 rows","e7759c50":"df.describe() #  inspecting numerical features","d802d3c2":"df.info() #  showing information about datatypes","1b5059ee":"df.isnull().sum() #  null values for all features","30d6d698":"plt.figure(figsize=(16,6))\nfig = sns.scatterplot(df.longitude, df.latitude, hue=df['median_house_value'],\n                      legend=False, palette='winter');","8902b18e":"from sklearn.neighbors import KNeighborsRegressor\n\n\ndef input_knn(data_frame=df):\n    \n    \"\"\"arguments: data_frame:pandas data frame - df default\n         returns: data_frame with filled Nan values\"\"\"\n    \n    numeric_features = df.select_dtypes(include=[np.number]) #  only features with numeric values\n    caterogical_features = df.select_dtypes(exclude=[np.number])  #  only features without numeric values\n    nan_columns = numeric_features.columns[numeric_features.isna().any()].to_list() #  features with empty values (NaNs)\n    no_nan_columns = numeric_features.columns.difference(nan_columns).values  #  features withouy empty values\n    \n    \n    for column in nan_columns:\n        imp_test = numeric_features[numeric_features[column].isna()] #  columns with null values \n        imp_train = numeric_features.dropna()\n        model = KNeighborsRegressor(n_neighbors=5)\n        knr = model.fit(imp_train[no_nan_columns], imp_train[column]) #  train model takes values from columns without null values\n        numeric_features.loc[df[column].isna(), column] = knr.predict(imp_test[no_nan_columns]) #  KNR predicts replaces null values\n        \n    return pd.concat([numeric_features, caterogical_features], axis=1)","6b940df2":"df_copy = df #  even if we have new df inputed with function above it's good practice to save old data frame\ndf=input_knn('total_bedrooms') #  imputing data\ndf.describe()","a0073df6":"def plot_histogram(column_name, data_frame=df):\n    \n    \"\"\"arguments: column_name:str - name of the column to be ploted\n       returns: histogram object\n    \"\"\"\n    \n    #  in case when column is not in data frame\n    if column_name not in data_frame.columns:\n        raise ValueError(f'Chose correct column from data frame colums: {data_frame.columns}')\n        \n    fig = px.histogram(data_frame=data_frame.sort_values(by=column_name), x=column_name,\n                       color_discrete_sequence=['blue'])\n    \n    \n    fig.update_layout(font=dict(family='Lato', size=16), \n                      title=dict(text=f'<b>histogram - {column_name}<b>',\n                                font=dict(size=24),\n                                x=.5),\n                     plot_bgcolor='lightblue',\n                     paper_bgcolor='lightblue',\n                     xaxis=dict(showgrid=False),\n                     yaxis=dict(showgrid=False))\n    \n    fig.show()","02d65e03":"numeric_columns = df.select_dtypes(np.number)\nfor column in numeric_columns:\n    plot_histogram(column)","ffebfabc":"df.sort_values(by='population', ascending=False)","8c52340d":"df = df.drop(labels = [15360, 9880])\nplot_histogram('population')\ndf.sort_values(by='population', ascending=False)","d194ee2b":"df.corr().style.background_gradient()","e306e905":"def correlation_heatmap(data_frame=df):\n    \"\"\"arguments: data_frame:pandas DataFrame\n       returns: correlation heatmap\"\"\"\n    \n    #  setting the context\n    sns.set(context='paper', font='moonspace')\n    \n    #  making correlation object and saving it into variable\n    correlation = df.corr()\n    \n    #  creating heatmap figure object (paper) and ax object (the plot)\n    fig, ax = plt.subplots(figsize=(12, 8))\n    \n    #  generating color palettes\n    cmap = sns.diverging_palette(220, 10, center='light', as_cmap=True)\n    \n    #  draw the heatmap\n    heatmap = sns.heatmap(correlation, vmax=1,vmin=-1,center=0, square=False, annot=True, cmap=cmap,\n                         lw=2, cbar=False)\n    \n    return heatmap","245fd3aa":"correlation_heatmap();","b4d6d3c6":"from sklearn.model_selection import train_test_split\n\nX = df.drop(['median_house_value', 'ocean_proximity'], axis=1)\ny = df['median_house_value']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.33,\n                                                   random_state=42)\n#  splitting the data into train and test","c86133a1":"from sklearn.linear_model import LinearRegression\n\nreg = LinearRegression() #  importing LinearRegression\n\nreg.fit(X_train, y_train) #  fitting the train data frame and train feature to the LinearRegression","bfe24a79":"predictions=reg.predict(X_test)","481a82a8":"print(f'actual: {y_test.mean()}')\nprint(f'predictions: {predictions.mean()}')","48a4be8b":"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nprint('MAE: ' + str(mean_absolute_error(y_test, predictions)))\nprint('MSE: ' + str(mean_squared_error(y_test, predictions)))\nprint('Score: '+ str(r2_score(y_test, predictions)))","5bac0e29":"from sklearn.ensemble import RandomForestRegressor\nreg = RandomForestRegressor() #  importing RandomForestRegressor","b7ed9be1":"reg.fit(X_train, y_train) #  fitting train data and train feature","65a6277b":"actual = y_test\npredictions = reg.predict(X_test)\nprint(f'Actual mean: {np.mean(actual)}')\nprint(f'Predicted mean: {np.mean(predictions)}')","3113eeb4":"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nprint('MAE: ' + str(mean_absolute_error(y_test, predictions)))\nprint('MSE: ' + str(mean_squared_error(y_test, predictions)))\nprint('Score: '+ str(r2_score(y_test, predictions)))","b948a97e":"*  there's only 207 null values in total_bedrooms\n*  unsupervised learning approach - KNN (K-nst Nearest Neighbours) is gonna be useful to inpute data to dataframe","13db289e":"### 1.2 DATA INSPECTING AND CLEANING","2e9fdfc6":"## 4.Training Model","77324d7d":"I'm going to inspect columns using **univeriate analysis** (analysis of only 1 variable). Histogram is good approach in this case. It allow us to see:\n\n* data distribution\n* otuliners\n* odd patterns\n* scale of axis","5b7923e2":"NULL VALUES","378b45b8":"### 4.1 Linear Regression","9f669a55":"*  target variable median_house_value is quite strong correlated to median_income variable\n* there's no more strong correlations - we could consider dropping some features which are very weak correlated but in this case they will stay","6e22b747":"## 3. EDA","1b2987e5":"Basic informations about dataset:\nThe data contains information from the 1990 California census. So although it may not help you with predicting current housing prices like the Zillow Zestimate dataset, it does provide an accessible introductory dataset for teaching people about the basics of machine learning.\n\n\nSize:\n* number of features (columns): 10\n* number of rows: 20640","c9510b83":"### 3.1 HISTOGRAMS","3ffc599c":"### 1.1 IMPORTING MODULES","8d48a142":"## 1. DATA PREPROCESSING","22f3e081":"## Housing distribution on the map","f752e89f":"Houses prices increases as well as ocean distance decreases - it's not big suprise. Houses with view on the ocean are mostly more expensive.","5bc3b4e3":"Less noticible:\n* total rooms column cointans outliners. There's big part of houses distributed between 0 - 5k but there's some houses which have even 30k and more\n* total bedrooms column contains otuliners. Mots of houses distributed in between 0 and 5k.\n* pupulation and household contains outliners as well. Population is distributet mostly between 0 and 5k and household 0 and 1k","eaaeca1e":"## 2. Data Imputation","4f1fff4b":"At first glance:\n* there's outliners at housing median age and median house value. It may be due to the way in which the data was sampled (median)\n* housing median age has outliner at 52k, median house value at 500k","7ddefc5f":"Dropping outliners from population column","ccf48e13":"### 4.2 Random Forest","551fdf74":"### Odd patterns and oustliners","2ef28a6f":"## 5.Conclusions","9e387370":"![lee-iacocca-s-california-house-for-sale.jpg](attachment:86ccb797-3071-4aeb-8a98-761cca2e258a.jpg)","aa23ec19":"### Correlation","d8d884b9":"Looks like RandomForestRegressor is the winner with score: 81","ae62fa0b":"Actual mean and predicted mean"}}