{"cell_type":{"3e46c728":"code","afb6c543":"code","235017ba":"code","c0ad50c6":"code","3a432555":"code","01e25bc7":"code","9238898c":"code","89f43aca":"code","373dd1f3":"code","d03ef1fb":"code","9d611943":"code","57968950":"code","cac3dc92":"code","ff4506c6":"code","e7f020da":"code","e0438900":"code","afb8af35":"code","35103223":"code","062012ba":"code","0199ffe4":"code","017ea9fc":"code","2f173787":"code","1c2f48bf":"code","18b27a8c":"code","7e948663":"code","8f836d8a":"code","b0268223":"code","d875fec4":"code","061164b7":"code","3c9e223b":"code","1eb2b099":"code","70382775":"code","1d2e7c91":"code","bddbdca4":"code","ad8bda29":"code","42e0edc7":"code","a941eb46":"code","e682dc78":"code","77329507":"code","45caa014":"code","2d775295":"code","77a3482d":"markdown","2521a7c1":"markdown","c1e0af49":"markdown","19c7c22b":"markdown","26e83b5a":"markdown","7b1b5880":"markdown","3eec9f3d":"markdown","35a7c962":"markdown","c26de71b":"markdown","c3d584c8":"markdown","1721c970":"markdown","53744ae8":"markdown","8d2ef60b":"markdown","4ba1203a":"markdown","c971670f":"markdown","6167cc9f":"markdown","4a2ea2c3":"markdown","fe62c99c":"markdown","21591739":"markdown","02cde0bb":"markdown","da2a2075":"markdown","7d5b17ac":"markdown","b1f0abd3":"markdown","c6b5a061":"markdown","47b3a888":"markdown","e688fe2f":"markdown","af7ea6ba":"markdown","ce4c33a2":"markdown","058bf410":"markdown","a49e806f":"markdown","bcc43870":"markdown","7f457871":"markdown","4b80e5e1":"markdown","8e4cd2aa":"markdown","0ba37ecf":"markdown","873c1605":"markdown","65d6ee2e":"markdown","7b6e3af9":"markdown","cd36d27e":"markdown","b2fb7190":"markdown","47283ea5":"markdown","6cd5b872":"markdown","a6d0d987":"markdown","60f92825":"markdown","35c8977b":"markdown","cdbafc6d":"markdown"},"source":{"3e46c728":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\nimport warnings\nimport os\nfrom statsmodels.graphics.gofplots import qqplot\nfrom IPython.core.interactiveshell import InteractiveShell\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.neighbors import KNeighborsRegressor\n\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\nwarnings.simplefilter(action=\"ignore\", category=Warning)\nInteractiveShell.ast_node_interactivity = \"all\"\n\n%reload_ext autoreload\n%autoreload 2\n\ndef set_seed(seed=42):\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\nset_seed()\n\nsns.set_style('whitegrid')\nsns.set_context('paper', font_scale=1.5)\nplt.style.use('fivethirtyeight')\npd.set_option(\"display.width\", 100)\npd.set_option(\"display.max_columns\", 25)\npd.set_option(\"display.max_rows\", 50)\n\nprint(\"setup-complete!\")","afb6c543":"raw = r\"..\/input\/sp-500-companies\/constituents-financials.csv\"\n\nsp_data = pd.read_csv(raw)","235017ba":"sp_data.head().style.background_gradient(cmap=\"Blues\") # check the first 5 rows","c0ad50c6":"sp_data.tail().style.background_gradient(cmap=\"Blues\") # check the last 5 rows","3a432555":"sp_data.info(verbose=True) # getting the information","01e25bc7":"categorical = sp_data.select_dtypes([\"category\", \"object\"]).columns\nfor cat_col in categorical:\n    print(f\"{cat_col} : {sp_data[cat_col].nunique()} uniqueness variable(s)\")","9238898c":"# Get the number of missing data points per column\nmissing_value_count = sp_data.isnull().sum()\n\n# Look at the missing points in the first 5 columns\nmissing_value_count.to_frame().T","89f43aca":"# Forming categorical columns\ncol_name_without_numeric_data = list(sp_data.select_dtypes(exclude=[np.number]).columns)\nprint(f\"columns without numeric data: {', '.join(col_name_without_numeric_data)}\")","373dd1f3":"# Let's see the duplicates in the columns\nfor col in col_name_without_numeric_data:\n    if sp_data[col].duplicated().any() == True:\n        print(f\"column {col} contains duplicates.\")\n    else:\n        print(f\"column {col} does not contain duplicates.\")","d03ef1fb":"# Check if spelling errors have been hit\nfor col in col_name_without_numeric_data:\n    print(f\"before {col}: {len(set(sp_data[col]))} after {col}: {len(set(sp_data[col].str.title().str.strip()))}\")","9d611943":"# Since 505\/5 is 101 let's have a look the first 101 companies\nsp_data.Name.sort_values().unique()[:101]","57968950":"# Let's have a look the first 101:202 companies\nsp_data.Name.sort_values().unique()[101:202]","cac3dc92":"# Let's have a look the first 202:303 companies\nsp_data.Name.sort_values().unique()[202:303]","ff4506c6":"# Let's have a look the first 303:404 companies\nsp_data.Name.sort_values().unique()[303:404]","e7f020da":"# Let's have a look the first 404:505 companies\nsp_data.Name.sort_values().unique()[404:505]","e0438900":"# let's have a look the name of our columns\nsp_data.columns","afb8af35":"# replacing and cleaning our name columns\nsp_data.columns = (sp_data.columns.str.replace(\"\/\", \" \")\n                                  .str.replace(\" \", \"_\")\n                                  .str.lower())\nsp_data.head(3).style.background_gradient(cmap=\"Blues\") # let's have a look","35103223":"# changing data type\nsp_data[\"market_cap\"] = sp_data[\"market_cap\"].astype(\"int64\")\nsp_data[\"ebitda\"] = sp_data[\"ebitda\"].astype(\"int64\")","062012ba":"sp_data.head(3).style.background_gradient(cmap=\"Blues\") # now let's have a look the first 3 rows","0199ffe4":"sp_data.describe(exclude=[np.number]).style.background_gradient(cmap=\"Blues\") # statistical measure categorical data","017ea9fc":"sp_data.describe(include=[np.number], percentiles=[.25, .5, .75]).astype(\"int64\").style.background_gradient(cmap=\"Blues\") # statistical measure numerical data","2f173787":"# Let's see the correlation from sp_data\n(sp_data.corr()[\"price\"] # getting the correlation and labels\n        .sort_values(ascending=False) # sort the values\n        .to_frame() # to frame\n        .T) # transpose it","1c2f48bf":"sns.set_style(\"whitegrid\") # set style\nplt.figure(figsize=(24, 18)) # set the figure\nsns.heatmap(sp_data.corr(), annot=True, fmt=\".1g\", cmap=\"bone_r\") # construct a heatmap\nplt.title(\"S&P 500 Companies Data Correlation\", weight=\"bold\", fontsize=35, pad=30) # title\nplt.xticks(weight=\"bold\", fontsize=15, rotation=30) # x-ticks\nplt.yticks(weight=\"bold\", fontsize=15, rotation=\"horizontal\"); # y-ticks","18b27a8c":"# Coefficient of Price\ncov_price = ((sp_data.price.std() \/ sp_data.price.mean()) * 100)\nprint(f\"coefficient of variation price: {cov_price}\")","7e948663":"fig, (ax1, ax2) = plt.subplots(\n    nrows=1, ncols=2, \n    figsize=(24, 10)\n)\n\n# Distplot axes 1\nsns.distplot(\n    sp_data[\"price\"], \n    bins=[0, 250, 500, 750, 1000, 1500, 2000], \n    ax=ax1\n)\nax1.set_title(\n    \"Price Distribution\", \n    weight=\"bold\", fontsize=25, \n    pad=30\n)\n\n# QQplot axes 2\nqqplot(\n    sp_data[\"price\"], \n    ax=ax2, \n    line=\"s\"\n)\nax2.set_title(\n    \"Quantile Quantile Plot\", \n    weight=\"bold\", fontsize=25, \n    pad=30\n);","8f836d8a":"fig, (ax1, ax2) = plt.subplots(\n    nrows=1, ncols=2, \n    figsize=(24, 10)\n)\n\n# Distplot axes 1\nsns.distplot(\n    sp_data[\"market_cap\"], \n    bins=100, \n    color=\"black\",\n    ax=ax1\n)\nax1.set_title(\n    \"Market Cap Distribution\", \n    weight=\"bold\", fontsize=25, \n    pad=30\n)\n\n# QQplot axes 2\nqqplot(\n    sp_data[\"market_cap\"], \n    ax=ax2, \n    line=\"s\"\n)\nax2.set_title(\n    \"Quantile Quantile Plot\", \n    weight=\"bold\", fontsize=25, \n    pad=30\n);","b0268223":"# checking skewness value\n# if value lies between -0.5 to 0.5  then it is normal otherwise skewed\nskew_value = sp_data.skew().sort_values(ascending=False).to_frame()\nskew_value.T","d875fec4":"# Getting the Mutual Information about the data\nX_mutual = sp_data.dropna().drop(columns=[\"name\", \"sector\"]).astype(\"int64\").reset_index().copy()\ny_mutual = X_mutual.pop(\"price\")\n\ndel X_mutual[\"index\"]\n\n# All discrete features should now have integer dtypes\ndiscrete_features = X_mutual.dtypes == int","061164b7":"# Make a function\ndef make_mi_score(X_mutual, y_mutual, discrete_features):\n    mi_scores = mutual_info_regression(X_mutual, y_mutual, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X_mutual.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_score(X_mutual, y_mutual, discrete_features)\nmi_scores.to_frame().T # show features with their mi-scores","3c9e223b":"def plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.figure(dpi=150, figsize=(13, 5))\n    plt.barh(width, scores, color=\"blue\")\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\", \n              weight=\"bold\", \n              fontsize=15, pad=25)\n\nplot_mi_scores(mi_scores)","1eb2b099":"def feature_engineering(data):\n    data[\"52_week_add\"] = data[\"52_week_low\"] + data[\"52_week_high\"]\n    data[\"52_week_sub\"] = data[\"52_week_low\"] - data[\"52_week_high\"]\n    data[\"52_week_div\"] = data[\"52_week_low\"] \/ data[\"52_week_high\"]\n    data[\"52_week_mul\"] = data[\"52_week_low\"] * data[\"52_week_high\"]\n    return data\n\nsp_data_feat = feature_engineering(sp_data).copy()","70382775":"sns.set_style(\"whitegrid\") # set style\nplt.figure(figsize=(24, 18)) # set the figure\nsns.heatmap(sp_data_feat.corr(), annot=True, fmt=\".1g\", cmap=\"bone_r\") # construct a heatmap\nplt.title(\"S&P 500 Companies Data Correlation\", weight=\"bold\", fontsize=35, pad=30) # title\nplt.xticks(weight=\"bold\", fontsize=15, rotation=30) # x-ticks\nplt.yticks(weight=\"bold\", fontsize=15, rotation=\"horizontal\"); # y-ticks","1d2e7c91":"# Let's see the correlation from sp_data\n(sp_data_feat.corr()[\"price\"] # getting the correlation and labels\n        .sort_values(ascending=False) # sort the values\n        .to_frame() # to frame\n        .T) # transpose it","bddbdca4":"# set seed\nset_seed()\n\n# Creating Features and Label variable\nX = sp_data.drop(columns=[\"name\", \"price\"])\ny = sp_data[\"price\"]\n\n# Splitting\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=.2, random_state=42\n)\n\n# Check the shape\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","ad8bda29":"# set seed\nset_seed()\n\n# Num Cols\nnumerical_pipe = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"mean\")),\n    (\"scaler\", MinMaxScaler())\n])\n\n# Cat Cols\ncategorical_pipe = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"encoder\", OneHotEncoder())\n])\n\n# Preprocessor\npreprocessor = ColumnTransformer([\n    (\"numeric\", numerical_pipe, list(X_train.drop(columns=[\"sector\"]).columns)),\n    (\"categoric\", categorical_pipe, [\"sector\"])\n])\n\n# Pipeline\npipeline = Pipeline([\n    (\"preprocessor\", preprocessor),\n    (\"model\", KNeighborsRegressor(n_neighbors=1, n_jobs=-1))\n])\n\n# Parameter\nparameter = {\n    \"model__n_neighbors\": np.arange(1, 10, 2),\n    \"model__p\": [1, 2],\n    \"model__weights\": [\"uniform\", \"distance\"]\n}","42e0edc7":"# set seed\nset_seed()\n\n# models\nmodel = GridSearchCV(pipeline, param_grid=parameter, cv=3, n_jobs=-1, verbose=1)\nmodel.fit(X_train, y_train)\n\nprint(model.score(X_train, y_train), model.best_score_, model.score(X_test, y_test))","a941eb46":"from sklearn.ensemble import RandomForestRegressor","e682dc78":"# set seed\nset_seed()\n\n# Num Cols\nnumerical_pipe = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"mean\")),\n    (\"scaler\", MinMaxScaler())\n])\n\n# Cat Cols\ncategorical_pipe = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"encoder\", OneHotEncoder())\n])\n\n# Preprocessor\npreprocessor = ColumnTransformer([\n    (\"numeric\", numerical_pipe, list(X_train.drop(columns=[\"sector\"]).columns)),\n    (\"categoric\", categorical_pipe, [\"sector\"])\n])\n\n# Pipeline\npipeline = Pipeline([\n    (\"preprocessor\", preprocessor),\n    (\"model\", RandomForestRegressor(n_jobs=-1, random_state=42))\n])\n\n# Parameter\nparameter = {\n    \"model__n_estimators\": [100, 200, 300, 400, 500],\n    \"model__max_depth\": [2, 3, 5, 7, 9],\n    \"model__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n    \"model__min_samples_leaf\": [2, 3, 5, 7, 9]\n}","77329507":"# set seed\nset_seed()\n\n# models\nmodel = GridSearchCV(pipeline, param_grid=parameter, cv=3, n_jobs=-1, verbose=0)\nmodel.fit(X_train, y_train)","45caa014":"print(f\"model train score: {model.score(X_train, y_train)},\\n\"\n      f\"model best score: {model.best_score_},\\n\"\n      f\"model test score: {model.score(X_test, y_test)},\\n\"\n      f\"model best estimator: {model.best_estimator_}\")","2d775295":"# Save model using pickle\nimport pickle\n\nfilename = \"sp_companies_best_model.sav\"\npickle.dump(model, open(filename, \"wb\"))","77a3482d":"> Well, that's a high number, okay, let's move on to the next steps","2521a7c1":"# **Check Data**\n\nLet's have a quick check for getting some information and maybe insight from it.","c1e0af49":"> Look like our data is clean right now, let's move on to the analysis.","19c7c22b":"---","26e83b5a":"## **Feature Engineering**","7b1b5880":"---","3eec9f3d":"> Hmm, we can't see market cap and ebitda clearly, let's change that type to Integer64.","35a7c962":"![s&p 500.jpg](attachment:25575e1b-62af-4528-9e25-d8ba21249632.jpg)\n\n[Image Source](https:\/\/www.google.com\/search?q=s%26p+500+logo&tbm=isch&hl=en&sa=X&ved=2ahUKEwjkopjVl7n1AhWO_TgGHYHDCN8QrNwCKAB6BQgBELsC&biw=1349&bih=663#imgrc=peMU6-ORzLafLM&imgdii=Ij_ODu0VK2orTM)","c26de71b":"---","c3d584c8":"> Great, look like our data does not have any duplicates value except Sector","1721c970":"# **Data Analysis**\n\nData Analysis is the process of systematically applying statistical and\/or logical techniques to describe and illustrate, condense and recap, and evaluate data. Indeed, researchers generally analyze for patterns in observations through the entire data collection phase *(Savenye, Robinson, $2004$)*.\nanalyze and investigate data sets and summarize their main characteristics, often employing data visualization methods.\n\nOr, the easier, you can think that, in Data Analysis we (Data Scientist or Data Analyst) what ever you want to call that, in this section, we're both looking for the correlation and also the relationships between every data (features and labels) or the variables using and applying the statistical and visualization methods for looking some patterns.","53744ae8":"**regards,**<br>\n**Azmi**","8d2ef60b":"# **About this file**\n\n### **List of companies in the S&P 500 (Standard and Poor\u2019s 500).**\n\nThe S&P $500$ is a free-float, capitalization-weighted index of the top $500$ publicly listed stocks in the US (top $500$ by market cap). The dataset includes a list of all the stocks contained there in and associated key financials such as price, market capitalization, earnings, price\/earnings ratio, price to book etc.\n\n### **Data**\n\nInformation on S&P $500$ index used to be available on the official webpage on the Standard and Poor\u2019s website but until they publish it back, Wikipedia is the best up-to-date and open data source.\n\n* Index listing - see extracted from Wikipedia\u2019s SP$500$ list of companies\n* Constituent financials - see (source via Yahoo Finance)\n\n### **Notes**\n\nMarket Capitalization and EBIDTA are in Billions.\nNote: For aggregate information on the S&P (dividends, earnings, etc), see Standard and Poor\u2019s $500$ Dataset.\n\n### **General Financial Notes**\n\nPublicly listed US companies are obliged various reports on a regular basis with the SEC. Of these 2 types are of especial interest to investors and others interested in their finances and business. These are:\n\n* $10$-K = Annual Report\n* $10$-Q = Quarterly report\n\n### **Content**\n\n#### **Independent Variables:**\n\n* `Name` **:** Name of the companies\n* `Sector` **:** A sector is an areas of the economy in which businesses share the same or related business activity, product, or service. Sectors represent a large grouping of companies with similar business activities, such as the extraction of natural resources and agriculture.\n* `Price\/Earning` **:** (Price to Earnings Ratio of the constituents of the S&P 500) Price to earnings ratio, based on trailing twelve month \"as reported\" earnings. Current PE is estimated from latest reported earnings and current market price.\n* `Dividend Yield` **:** The dividend yield-displayed as a percentage-is the amount of money a company pays shareholders for owning a share of its stock divided by its current stock price. Mature companies are the most likely to pay dividends.\n* `Earnings\/Share` **:** (EPS) is calculated as a company's profit divided by the outstanding shares of its common stock. The resulting number serves as an indicator of a company's profitability. The higher a company's ESP, the more profitable it is considered to be.\n* `52 Week High\/Low` **:** Is the highest and lowest price at which a security has traded during the time period that equates to one year and is viewed as a technical indicator. \n* `Market Cap` **:** (or market capitalization) refers to the total value of all a company's shares stock. Market cap measures what a company is worth on the open market, as well as the market's perception of its future prospects, because it reflects what investors willing to pay for its stock.\n* `EBITDA` **:** (or earnings before interest, taxes, depreciation, and amortization, is a measure of a company's overall financial performance and is uses an alternative to net income in some circumstances.\n* `Price\/Sales` **:** (Price to Sales Ratio) P\/S or Price to Revenue. Current price to sales ratio is estimated based on current market price.\n* `Price\/Book` **:** Current price to book ratio is estimated based on current market price and S&P 500 book value (the latest report by S&P).\n\n#### **Dependent Variable:**\n\n* `Price` **:** Represents how much the stock trades at-or the price agreed upon a buyers than sellers, the stock's price will climb.","4ba1203a":"> Check if the columns contains words in different case and extra spaces.","c971670f":"> Hmm, not bad, look like our model is overfitting, let's try Random Forest Regressor!","6167cc9f":"# **Read-in Dataset**","4a2ea2c3":"# **Splitting Data**","fe62c99c":"<p style=\"font-name: monospace; line-height: 2; font-size: 30px; font-weight: bold; letter-spacing: 2px; text-align: center;\">S&P 500 Companies - Knightbearr<\/p>","21591739":"> Let's clean the names","02cde0bb":"> Look at that! Amazing! okay, let's save our model!","da2a2075":"---","7d5b17ac":"> The companies name does not have any duplicated value, great! let's move on to the next steps.","b1f0abd3":"> Okay, let's fill that with the mean value later in the preprocessing steps. Now let's check wether the data has duplicate data or not.","c6b5a061":"# **Introduction**\n\n<p style=\"font-name: monospace; line-height: 2; font-size: 20px; font-weight: bold; letter-spacing: 2px; text-align: center;\">\u0628\u0633\u0645 \u0627\u0644\u0644\u0647 \u0627\u0644\u0631\u062d\u0645\u0646 \u0627\u0644\u0631\u062d\u064a\u0645<\/p>\n\n<p style=\"font-name: monospace; line-height: 2; font-size: 20px; font-weight: bold; letter-spacing: 2px; text-align: center;\">\u0627\u0644\u0633\u0644\u0627\u0645 \u0639\u0644\u064a\u0643\u0645 \u0648\u0631\u062d\u0645\u0629 \u0627\u0644\u0644\u0647 \u0648\u0628\u0631\u0643\u0627\u062a\u0647<\/p>\n\nHello, fellow Kagglers! and again, My name is Azmi, this is my first public notebook Analysis Regression that I made, please give me an upvote if you like it! and please, leave me your feedback or give me your insight about this data, Hope you like it! In this notebook we're going to Analyst S&P $500$ Companies data by doing so we're going to get some insights.\n\nI'm planning to go through feature (question) by feature and take a closer look at those features to inspect their relationships with previous features. So it's gonna take a while to finish this notebook with all features involved. I'm planning to update it regularly whenever I have free time.\n\n*Sorry if my English is bad :') I hope you like it!*\n\nmore about myself: https:\/\/linktr.ee\/mazmimuis\n","47b3a888":"**Coefficient of Variation**\n\nThe coefficient of variation is a measure of variance that can be used to compare a data distribution that has different units.\n\n* **The higher the Coefficient of Variation** = the wider the data you have compared to the average data (more difficult to predict)\n* **The Lower Coefficient of Variation** = The narrower the data you have compared to the Average data (Easier to predict)","e688fe2f":"---","af7ea6ba":"### **Getting Mutual Information**","ce4c33a2":"> Okay, let's see the correlation of our data and visualize it.","058bf410":"##### ***Note:*** Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example few times and compare the average outcome.","a49e806f":"## **Univariate Analysis**","bcc43870":"> wow, you see that? almost all of our Independent Variables have a great correlation with the Dependent Variable, okay let's see the categorical variable, btw, the cmap is kinda cute XD.","7f457871":"> The data contains $505$ rows and $12$ columns, okay, Name of Companies and the Sector of course. okay, let's get information about this data. Did you see that? Data contain null values, okay, thank you for the information, let's handle that later, for now, let's move on to the next step.","4b80e5e1":"# **Training**","8e4cd2aa":"<p style=\"font-name: monospace; line-height: 2; font-size: 20px; font-weight: bold; letter-spacing: 2px; text-align: center;\">\u062a\u062d\u064a\u0629 \u0637\u064a\u0628\u0629\n<\/p>","0ba37ecf":"### **How many missing data points do we have?**\n\nLet's see how many missing data we have in each column....","873c1605":"---","65d6ee2e":"# **Data Cleaning**\n\nData Cleaning is the process of fixing or removing incorrect, corupted, incorrectly formatted, duplicate, or incomplete data within a dataset. When combining multiple data sources, there are many opportunities for data to be duplicated or mislabeled.\n\nBut why we clean the data? Data cleansing ensures you only have the most recent files and important documents, so when you need to, you can find them with ease. It also helps ensure that you do not have significant amounts of personal information on your computer, which can be a security risk.","7b6e3af9":"*Let's get started...*\n\nWait...\n\n**Note:** *before you read this analysis, it would be nice to prepare snacks and coffee to accompany you*\n\nDone? Okay, let's start!\n\n---\n\n# **Import Necessary Libraries**","cd36d27e":"> There are several `market_cap` that exceed $\\$21,400,952,517$ but if we look at the distribution plot that we have created above, we can see that many are below that number, and also `market_cap` that exceed these lifts end up being outliers, but don't worry. , we can handle this by using `MinMaxScaler` later in the processing section.","b2fb7190":"> Not bad...","47283ea5":"> Here we can see, there are many companies whose stock prices are below $\\$250$ and only a few are above, also, we can see that there are some outliers due to stock prices that are in the range of $\\$1000 - \\$1806$.","6cd5b872":"> Great, but that's not enough, let's make barplot to make comparison easier...","a6d0d987":"> I see, what if we apply basic math operation to the 52 week low and 52 week high? let's see...","60f92825":"### **Uniqueness Categorical Variables**\nLet's have a look at categorical variables. How many unique values of these variables.","35c8977b":"> Let's see the statistical measurement of our data.","cdbafc6d":"# **Pipeline**\n\nWhy I'm using Pipeline? Pipeline utility is to help automate machine learning workflows. Pipelines work by allowing for a linear sequence of data transforms to be chained together culminating in a modeling process that can be evaluated. it can be wrapped and very useful and of course keep your code clean!"}}