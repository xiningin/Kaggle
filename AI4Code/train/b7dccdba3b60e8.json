{"cell_type":{"5180b3da":"code","07b61d00":"code","4a4b1714":"code","04c5d909":"code","eb411772":"code","84efd1f9":"code","7e3f3e41":"code","595bfb60":"code","0aac434c":"code","91806e6c":"code","de37257f":"code","4d6c3867":"code","9975cf54":"code","32d79e75":"code","846524ce":"code","303d3d0a":"code","2044225d":"code","a4249794":"code","edbaf5bc":"code","4804c42f":"code","73f521b6":"code","6a45a28d":"code","9c020829":"code","b29d6d1f":"code","822388dd":"code","b3699a95":"code","2c3a927a":"code","d1389df2":"code","0dce79bb":"code","09c6d940":"code","fa68fbb8":"code","70828364":"code","6d7080e9":"code","462171fc":"code","92068eff":"code","20acb0a0":"code","5e0bcf76":"code","a2285aed":"code","afea8e4a":"code","e99027f3":"code","7cf93710":"code","237c855e":"code","0836a85b":"code","1647b793":"code","5f271a20":"code","ba1f8469":"code","1f9b23ad":"code","f54574db":"code","aa21dc99":"code","06644822":"code","09effa1f":"code","462712e5":"code","6c16f83a":"code","99404288":"code","6d318996":"code","2119b2de":"code","46ae8c0a":"code","0d9b85c4":"code","b4f28e21":"code","e2b39196":"code","a41ffef1":"code","8718aadc":"code","5cda53ab":"code","50573420":"code","2d9cc925":"code","49a8e454":"code","d6daf2f1":"code","62b55a61":"code","1107e21b":"code","0312b4ae":"code","7af2e4fb":"code","2c3ff0ed":"code","14b8b52d":"code","d24abd80":"markdown","3b0debbf":"markdown","1c5f982c":"markdown","9a30a8df":"markdown","1ea076de":"markdown","296a7da1":"markdown","0212e814":"markdown","4b835c7a":"markdown","864f4141":"markdown","2c5fb800":"markdown","a80f2053":"markdown","5dfccc41":"markdown","55b3b690":"markdown","aab89a59":"markdown","96c4f8ec":"markdown","ba2d6bd9":"markdown","0f2ebfa0":"markdown","be572522":"markdown","0e643445":"markdown","7db5362a":"markdown","940949f7":"markdown","3157d013":"markdown","c264b2b9":"markdown","afec1680":"markdown","0e89c32e":"markdown","21629772":"markdown","6d5f30b8":"markdown","618ebdac":"markdown","f4c8d753":"markdown","f75cfb04":"markdown","58c7089e":"markdown","1a7def15":"markdown","b5649151":"markdown","cc18fc90":"markdown","6cd206a0":"markdown","e75d8eea":"markdown","2bf9d139":"markdown","23b7c16b":"markdown","568a52ed":"markdown","8c81232d":"markdown","9d3dd561":"markdown","9e55d3b4":"markdown","58d263fc":"markdown","89a76bf9":"markdown","9a991bab":"markdown","461acaac":"markdown","28cd8346":"markdown","a1daa6b8":"markdown"},"source":{"5180b3da":"# Import libraries necessary for this project\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n%matplotlib inline\n\n","07b61d00":"# Load the provided data set for Credit Card dataset\ndata = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\n\n# Lets see the top 5 data set ,if its been loaded correctyly and or not\ndata.head(5)","4a4b1714":"data.describe()","04c5d909":"data.shape","eb411772":"data_count_class=data\nclass_count = pd.value_counts(data['Class'], sort = True).sort_index()\nsns.countplot(x=\"Class\", data=data_count_class)\nplt.title(\"Class Count\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")","84efd1f9":"fraud_data = data[data_count_class.Class == 1]\nnormal_data = data[data_count_class.Class == 0]","7e3f3e41":"fraud_data.shape","595bfb60":"normal_data.shape","0aac434c":"sns.distplot(normal_data.Time, color='b')\nplt.title(\"Time feature distribution of Non-Fraud Transaction\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Frequency\")","91806e6c":"sns.distplot(fraud_data.Time, color='g')\nplt.title(\"Time feature distribution over all the Fraud Transaction\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Frequency\")","de37257f":"sns.pairplot(fraud_data)","4d6c3867":"sns.distplot(normal_data.Amount, color='g')\nplt.title(\"Distribution of Amount for Reguler Transaction\")\nplt.xlabel(\"Amount\")\nplt.ylabel(\"Frequency\")","9975cf54":"sns.distplot(fraud_data.Amount, color='r')\nplt.title(\"Amount feature distribution of Fraud Data\")\nplt.xlabel(\"Amount\")\nplt.ylabel(\"Dist Frequency\")","32d79e75":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Amount of transaction')\nbins = 50\n\nax1.hist(fraud_data.Amount, bins = bins)\nax1.set_title('Fraud Data')\n\nax2.hist(normal_data.Amount, bins = bins)\nax2.set_title('Non Fraud Data')\n\nplt.xlabel('Amount')\nplt.ylabel('Number of Transactions')\nplt.xlim((0, 21000))\nplt.yscale('log')\nplt.show()","846524ce":"f, (ax, ax_1) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Time for transaction vs Amount')\n\nax.scatter(fraud_data.Time, fraud_data.Amount)\nax.set_title('Fraud Data')\nax.grid(color='r', linestyle='-', linewidth=0.1)\n\n\nax_1.scatter(normal_data.Time, normal_data.Amount)\nax_1.set_title('Non Fraud Data')\nax_1.grid(color='r', linestyle='-', linewidth=0.1)\nplt.xlabel('Time in Seconds')\nplt.ylabel('Amount')\nplt.show()","303d3d0a":"from sklearn.preprocessing import StandardScaler\nmodel_data = data.drop(['Time'], axis=1)\nmodel_data['Amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\nmodel_data.head()","2044225d":"model_train = model_data.drop(\"Class\", 1).values\nmodel_train.shape","a4249794":"model_test = model_data[\"Class\"].values\nmodel_test.shape","edbaf5bc":"from imblearn.over_sampling import SMOTE","4804c42f":"sampling_train=model_train\nsampling_test=model_test\nsampler = SMOTE(random_state = 0, n_jobs = -1)\nmodel_train_lr , model_test_lr = sampler.fit_sample(sampling_train, sampling_test)","73f521b6":"model_train_lr.shape","6a45a28d":"model_test_lr.shape","9c020829":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(model_train_lr, model_test_lr, test_size = 0.25, random_state = 0)\n\n","b29d6d1f":"X_train.shape","822388dd":"X_test.shape","b3699a95":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(max_iter=300, random_state=0, n_jobs = -1)\nlr.fit(X_train, Y_train)","2c3a927a":"lr_prediction = lr.predict(X_test)\nprint(lr_prediction)","d1389df2":"lr_prediction.shape","0dce79bb":"#import sns and matplotlib for the graph & data visualization \nfrom matplotlib import pyplot\nimport seaborn as sns\n\n# import all the accuracy parameters : accuracy_score, recall_score, confusion_matrix, roc_auc_score\nfrom sklearn.metrics import accuracy_score,confusion_matrix,roc_auc_score,recall_score\n\n\n\nsc_lr_accuracy = accuracy_score(Y_test, lr_prediction)\nsc_lr_recall = recall_score(Y_test, lr_prediction)\nsc_lr_cm = confusion_matrix(Y_test, lr_prediction)\nsc_lr_auc = roc_auc_score(Y_test, lr_prediction)\n\nprint(\"Model has a Score_Accuracy: {:.3%}\".format(sc_lr_accuracy))\nprint(\"Model has a Score_Recall: {:.3%}\".format(sc_lr_recall))\nprint(\"Model has a Score ROC AUC: {:.3%}\".format(sc_lr_auc))\n","09c6d940":"sc_lr_cm = pd.DataFrame(sc_lr_cm, ['True Regular','True Fraud'],['Prediction Regular','Prediction Fraud'])\npyplot.figure(figsize = (8,4))\nsns.set(font_scale=1.4)\nsns.heatmap(sc_lr_cm, annot=True,annot_kws={\"size\": 16},fmt='g')","fa68fbb8":"print(sc_lr_cm)","70828364":"#importing sampling SMOTETomek\nfrom imblearn.combine import SMOTETomek\n\n","6d7080e9":"# Sample the data againg with SMOTETomek\nsampling_train=model_train\nsampling_test=model_test\nSMOTEtomek_sampler = SMOTETomek(random_state = 0, n_jobs = -1)\nmodel_input_rf , model_output_rf = SMOTEtomek_sampler.fit_sample(sampling_train, sampling_test)","462171fc":"model_input_rf.shape","92068eff":"from sklearn.model_selection import StratifiedShuffleSplit\n\nshuffle_splits = StratifiedShuffleSplit(n_splits=20, test_size=0.25, random_state=0)","20acb0a0":"for train, test in shuffle_splits.split(model_input_rf, model_output_rf):\n    X_train, X_test = model_input_rf[train], model_input_rf[test]\n    Y_train, Y_test = model_output_rf[train], model_output_rf[test]","5e0bcf76":"X_train.shape","a2285aed":"Y_train.shape","afea8e4a":"from sklearn.ensemble import RandomForestClassifier\n\n","e99027f3":"RandomForest_model = RandomForestClassifier(n_estimators= 200, criterion = 'entropy', random_state = 0, n_jobs = -1)\nRandomForest_model.fit(X_train, Y_train)","7cf93710":"RandomForest_predict = RandomForest_model.predict(X_test)","237c855e":"print(RandomForest_predict)","0836a85b":"sc_rf_accuracy = accuracy_score(Y_test, RandomForest_predict)\nsc_rf_recall = recall_score(Y_test, RandomForest_predict)\nsc_rf_cm = confusion_matrix(Y_test, RandomForest_predict)\nsc_rf_auc = roc_auc_score(Y_test, RandomForest_predict)\n\nprint(\"Model has a Score Accuracy: {:.3%}\".format(sc_rf_accuracy))\nprint(\"Model has a Score Recall: {:.3%}\".format(sc_rf_recall))\nprint(\"Model has a Score ROC AUC: {:.3%}\".format(sc_rf_auc))","1647b793":"sc_rf_cm = pd.DataFrame(sc_rf_cm, ['True Regular','True Fraud'],['Prediction Regular','Prediction Fraud'])\npyplot.figure(figsize = (8,4))\nsns.set(font_scale=1.4)\nsns.heatmap(sc_rf_cm, annot=True,annot_kws={\"size\": 16},fmt='g')","5f271a20":"from imblearn.under_sampling import TomekLinks\n","ba1f8469":"TomekLinks_sampler = TomekLinks()\nmodel_input_km , model_output_km = SMOTEtomek_sampler.fit_sample(model_train, model_test)","1f9b23ad":"from sklearn.cluster import KMeans\ninertia = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, max_iter = 200, random_state = 0, n_jobs = -1)\n    kmeans.fit(model_input_km)\n    inertia.append(kmeans.inertia_)\nplt.plot(range(1, 11), inertia)\nplt.title('Elbow score vs No. of clusters')\nplt.xlabel('No. of clusters')\nplt.ylabel('Score')\nplt.show()","f54574db":"kmeans = KMeans(n_clusters = 2, max_iter = 200, random_state = 0, n_jobs = -1).fit(model_input_km)","aa21dc99":"k_centers = kmeans.cluster_centers_","06644822":"from scipy import spatial\nkmean_distance = pd.DataFrame(spatial.distance.cdist(model_input_km, k_centers, 'euclidean'))\nkmean_distance['distance_mean'] = kmean_distance.apply(np.mean, axis=1)\nkmean_distance.head()","09effa1f":"cut_off = np.percentile(kmean_distance['distance_mean'], 95)","462712e5":"model_predict_km = np.where(kmean_distance['distance_mean'] >= cut_off, 1, 0)","6c16f83a":"sc_km_accuracy = accuracy_score(model_output_km, model_predict_km)\nsc_km_recall = recall_score(model_output_km, model_predict_km)\nsc_km_cm = confusion_matrix(model_output_km, model_predict_km)\nsc_km_auc = roc_auc_score(model_output_km, model_predict_km)\n\nprint(\"Model has a score Accuracy: {:.3%}\".format(sc_km_accuracy))\nprint(\"Model has a score Recall: {:.3%}\".format(sc_km_recall))\nprint(\"Model has a score ROC AUC: {:.3%}\".format(sc_km_auc))","99404288":"sc_km_cm = pd.DataFrame(sc_km_cm, ['True Regular','True Fraud'],['Prediction Regular','Prediction Fraud'])\npyplot.figure(figsize = (8,4))\nsns.set(font_scale=1.4)\nsns.heatmap(sc_km_cm, annot=True,annot_kws={\"size\": 20},fmt='g')","6d318996":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score","2119b2de":"from sklearn.model_selection import train_test_split\n\nX_train_knn, X_test_knn, Y_train_knn, Y_test_knn = train_test_split(model_train_lr[0:100000], model_test_lr[0:100000], test_size = 0.35, random_state = 0)","46ae8c0a":"# Lets take few no between 0 to 6 for K values\nno = list(range(0,6))\n\nKNeighbors = list(filter(lambda x: x%2!=0, no))\n\nCV_Sc = []\n\nfor k in KNeighbors:\n    KNN = KNeighborsClassifier(n_neighbors = k, algorithm = 'kd_tree')\n    recall_scores = cross_val_score(KNN, X_train_knn, Y_train_knn, cv = 5, scoring='recall')\n    CV_Sc.append(recall_scores.mean())","0d9b85c4":"import matplotlib.pyplot as plt\nplt.figure(figsize = (14, 12))\nplt.plot(KNeighbors, CV_Sc)\nplt.title(\"Neighbors Vs Recall Score for best KNeighbor Value\", fontsize=25)\nplt.xlabel(\"Number of Neighbors\", fontsize=25)\nplt.ylabel(\"Recall Score\", fontsize=25)\nplt.grid(linestyle='-', linewidth=0.5)","b4f28e21":"best_k = KNeighbors[CV_Sc.index(max(CV_Sc))]\n#we choose k on high CV_Score\nprint(\"Best value of K= \"+str(best_k)+\" \")\n\n","e2b39196":"from sklearn.metrics import recall_score\n\nKNN_best_model = KNeighborsClassifier(n_neighbors = best_k, algorithm = 'kd_tree')\nKNN_best_model.fit(X_train_knn, Y_train_knn)\nKNN_prediction = KNN_best_model.predict(X_test_knn)\n\nrecallTest = recall_score(Y_test_knn, KNN_prediction)\n\nprint(\"Recall Score of the knn classifier for best k values of \"+str(best_k)+\" is: \"+str(recallTest))\n\ncm = confusion_matrix(Y_test_knn, KNN_prediction)\n\nprint(cm)\ntn, fp, fn, tp = cm.ravel()\n(tn, fp, fn, tp)","a41ffef1":"import numpy as np\nimport pandas as pd\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n%matplotlib inline","8718aadc":"X_train_xgb,X_test_xgb,y_train_xgb,y_test_xgb =train_test_split(model_train_lr, model_test_lr, stratify=model_test_lr, random_state=42)","5cda53ab":"#Lets take diffetrnt no of Tree ranging 2 to 30 in interval of 5\ntree_range = range(2, 100, 10)\n\nscore1=[]\nscore2=[]\nfor tree in tree_range:\n    xgb=XGBClassifier(n_estimators=tree)\n    xgb.fit(X_train_xgb,y_train_xgb)\n    score1.append(xgb.score(X_train_xgb,y_train_xgb))\n    score2.append(xgb.score(X_test_xgb,y_test_xgb))\n    \n%matplotlib inline\nplt.plot(tree_range,score1,label= 'Accuracy : Training set')\nplt.plot(tree_range,score2,label= 'Accuracy : Testing set')\nplt.xlabel('Value of number of trees in XGboost')\nplt.ylabel('Accuracy Score')\nplt.legend()","50573420":"xgb=XGBClassifier(n_estimators=40)\nxgb.fit(X_train_xgb,y_train_xgb)\nprint('Accuracy of XGB n=6 on the testing dataset is :{:.3f}'.format(xgb.score(X_test_xgb,y_test_xgb)))","2d9cc925":"xgb_predict = xgb.predict(X_test_xgb)","49a8e454":"# import all the accuracy parameters : accuracy_score, recall_score, confusion_matrix, roc_auc_score\nfrom sklearn.metrics import accuracy_score,confusion_matrix,roc_auc_score,recall_score\n\n\n\nsc_xgb_accuracy = accuracy_score(y_test_xgb, xgb_predict)\nsc_xgb_recall = recall_score(y_test_xgb, xgb_predict)\nsc_xgb_cm = confusion_matrix(y_test_xgb, xgb_predict)\nsc_xgb_auc = roc_auc_score(y_test_xgb, xgb_predict)\n\nprint(\"Model has a Score Accuracy: {:.3%}\".format(sc_xgb_accuracy))\nprint(\"Model has a Score Recall: {:.3%}\".format(sc_xgb_recall))\nprint(\"Model has a Score ROC AUC: {:.3%}\".format(sc_xgb_auc))","d6daf2f1":"Prediction_Accuracy={\n    'Logistic Regression': sc_lr_accuracy,\n    'Random Forest': sc_rf_accuracy,\n    'K-Means': sc_km_accuracy,\n    'XGBoost': sc_xgb_accuracy\n}\n\nPrediction_Recall={\n    'Logistic Regression': sc_lr_recall,\n    'Random Forest': sc_rf_recall,\n    'K-Means': sc_km_recall,\n    'KNN':recallTest,\n    'XGBoost': sc_xgb_recall\n}\n\nPrediction_AUC={\n    'Logistic Regression': sc_lr_auc,\n    'Random Forest': sc_rf_auc,\n    'K-Means': sc_km_auc,\n    'XGBoost': sc_xgb_auc\n}","62b55a61":"#set colors\ncolors = ['b', 'g', 'r', 'c', 'm', 'y', 'g']\n\n\npyplot.title('Accuracy of different models')\npyplot.barh(range(len(Prediction_Accuracy)), list(Prediction_Accuracy.values()), align='center',color=colors)\npyplot.yticks(range(len(Prediction_Accuracy)), list(Prediction_Accuracy.keys()))\npyplot.xlabel('Accuracy_Score')","1107e21b":"pyplot.title('Recall Score of different models')\npyplot.barh(range(len(Prediction_Recall)), list(Prediction_Recall.values()), align='center',color=colors)\npyplot.yticks(range(len(Prediction_Recall)), list(Prediction_Recall.keys()))\npyplot.xlabel('Recall Score of different models')","0312b4ae":"pyplot.title('AUC Score of different models')\npyplot.barh(range(len(Prediction_AUC)), list(Prediction_AUC.values()), align='center',color=colors)\npyplot.yticks(range(len(Prediction_AUC)), list(Prediction_AUC.keys()))\npyplot.xlabel('AUC Score of different models')","7af2e4fb":"RandomForest_predict_check = RandomForest_model.predict(model_train_lr)","2c3ff0ed":"sc_rf_accuracy_ck = accuracy_score(model_test_lr, RandomForest_predict_check)\nsc_rf_recall_ck = recall_score(model_test_lr, RandomForest_predict_check)\nsc_rf_cm_ck = confusion_matrix(model_test_lr, RandomForest_predict_check)\nsc_rf_auc_ck = roc_auc_score(model_test_lr, RandomForest_predict_check)\n\nprint(\"Model has a Score Accuracy: {:.3%}\".format(sc_rf_accuracy_ck))\nprint(\"Model has a Score Recall: {:.3%}\".format(sc_rf_recall_ck))\nprint(\"Model has a Score ROC AUC: {:.3%}\".format(sc_rf_auc_ck))","14b8b52d":"print(sc_rf_cm_ck)","d24abd80":"# We have implemented Logistic Regression, Random Forest, K-Means and KNN moleds , Now lets compare all perfomance matrix and choose the best model","3b0debbf":"## Here we have the Unbalanced Data, so we need to apply Sampling ","1c5f982c":"# We have the Kmean Predict output data ...Lets check diffrent Scores","9a30a8df":"# Now we will try the Shuffle Splitting, which will shuffle all sampled data and split","1ea076de":"## Lets build the model with n_estimators=40  (best No of tree to build XGB)","296a7da1":"## Cheking the dimention of the Data Shape","0212e814":"# Now we have a proper set of Train and Test data split ,let build the model  \n# Building Model: Logistic Regression","4b835c7a":"## Plotting data for Data Visualization","864f4141":"# What we implemented so far ","2c5fb800":"We can see that no of Normal\/Regular Transaction has very high count as most reguler transaction with a regurel pattern of amount.\nFraud Transactions occures not reguler and count also very less.","a80f2053":"\u27a2Taking the dataset, exploratory data analysis would be performed here to check how these variables contribute to the outcome, though this might be limited and much values cant\u2019 be drawn since the variables aren\u2019t actual and the original names of the features\u2019 representation is hidden.\n\u27a2 Outlier detection and removal is up next, this phase seeks to find all points\/observations that are irregular or abnormal relative to the dataset.\n\u27a2 Initially, the data need to check for the missing values, we would do this by either replacing them with mean \/ median of the variables or a variable with high percentage of missing values is removed.\n\u27a2 The relation of different features is observed in visualizations.\n\u27a2 The data distribution of the features is also checked.\n\u27a2 The data is standardized in order to achieve zero mean and equal variance.\n\u27a2 We used the Supervised Learning Method and Random Forest classifier.\n\u27a2 Then the model performance metrics are calculated.\n\u27a2 The model is checked & tuned accordingly in order to achieve high performance, Non-Overfitting, low bias features.\n\u27a2 We will used different technique to find the important features.\n\u27a2 Use them to fit the model and maintain low\/reduced the complexity and increase the performance.\n\u27a2 We will next use the Unsupervised Learning Methods: K-means algorithm is fitted to the data by removing the Label feature.\n\u27a2 In the K Means, the distance between the cluster centroid and the instances are measured\n\u27a2 If the instances which are > 90-95% percentile are considered as outliers.\n\u27a2 These outliers need to compare with actual results to check the accuracy of the model.\n\u27a2 At the conclusion I will compare the performance of both the supervised and unsupervised learning methods and conclude the best fitted model for credit card fraud detection.","5dfccc41":"## For heavily imbalanced data lets consider \"Recall\" as a suitable error metric ","55b3b690":"## Now we see from the confusion metrix the True Prediction and False Prediction rate are very high","aab89a59":"# Lets test one of the final Model : Random Forest  with some random data","96c4f8ec":"## Lets cosider our sampling data set we prepared at the beginning and do Splitting Datasets into Train & Test for XGBoosting","ba2d6bd9":"## Lets do train & test split of the data into different Class , 0 or 1","0f2ebfa0":"It clearly shows the distribution of Time over Non-Fraud Transactions.","be572522":"We need to Scale the Amount as it is in different Scale","0e643445":"# Lets find the optimul Cluster-Number for the Kmean ","7db5362a":"# Scaling of Data Set","940949f7":"# Lets try the KnighbourClassifier also (KNN ) on our data set","3157d013":"# Now we see at the two-cluster number the Kmean has a elbow curve , so we will use two cluster Number","c264b2b9":"# Lets build the Unsupervised K-Means Clustering Model with cluster No=2","afec1680":"it is the distribution of Amount over reguler Transactions,we see one peak at the beginning but it becomes flat. The distribution range is 0 to 5000 in max cases.","0e89c32e":"# Lets calculate the 95 percentile of dist mean and take as cut off classifier ","21629772":"# We concluded XGBoost and Random Forest Classifier are the best fit model","6d5f30b8":"# We need a suitable error metrics for this problem.","618ebdac":"## Finding the best No of Tree to build XGBoosting","f4c8d753":"## Now we have the sampling balanced data approx 568K+ , Need to Split the data into training and testing part Now","f75cfb04":"## Now we have 568k+ sample data which need to split in train and test","58c7089e":"We have a train and test split ready now","1a7def15":"We are tring the sampling technique SMOTE here, lets see how it will be effective sampling technique .","b5649151":"## Importing Libraries and Data","cc18fc90":"### Lets Apply KNN on the dataset and we will find out the best k value using 5-Folds CV.","6cd206a0":"# Importing packages for XGBoost, Lets try the fast and most famous Gradient Boosting Model.","e75d8eea":"# We have the data for Second Model Random Forest ready , Lets build and check the model accuracy parameters\n\n\n# Random Forest \n\n# We will import it from sklearn.ensemble","2bf9d139":"# Capstone Project : ML predicton Model\n## Project: Credit Card Fraud Detection\n### Debottam Majilla & Kuldeepak Jhanji","23b7c16b":"### This graph shows the distribution of Amount for all Fraud Transactions,there is a huge peak ,but it becomes flat after 900. ","568a52ed":"## Final Model Selection : Conclusion ","8c81232d":"## Lets try another Sampling and see if the model accuracy improve or not","9d3dd561":"1. This is a class imbalance problem\n2. The non-fraud \/normal transactions are more than 2500000+ but the fraud transactions are very less approx 400+. ","9e55d3b4":"It shows the distribution of Time over all the Fraud Transactions.","58d263fc":"# Lets check the Accuracy Parameters and different scores","89a76bf9":"## Here best number of trees in XGB is 40 , we see the Training & Test Accuracy become flatten after 40 no of Trees consideration","9a991bab":"# Our proposed final model performed a test on random data set and it has Accuracy 99% Recal 100% and ROC AUC 99% , which is very high performer and accurate model to predict .","461acaac":"## We have the first LogisticRegression Model ready , Lets check the model accuracy parameters","28cd8346":"1.1) For a class imbalance dataset,Supervised Learning Methods clearly dominates ,compare to the Unsupervised Learning Methods \n\nWe compared the Accuracy ,Recall ,AUC score of Logistic ,Random Forest,Kmean and KNN \n\n1.2) **XGBoost and Random Forest Classifier have the higher accuracy than Logistic Regresssion followed by K-Means Clustering.** \n\n\n1.3) **XGBoost and Random Forest Classifier have higher recall score > than  Logistic Regresssion followed by K-Means Clustering.** \n\n\n\n1.4) **XGBoost and Random Forest Classifier have higher auc score than > Logistic Regresssion followed by K-Means Clustering.** \n\n\n\n1.5) After all validation therefore,we would like to conclude that Supervised Learning Methods :::\n     such as **XGBoost and Random Forest Classifier** gives a better prediction over Unsupervised Methods. \n     \n     \n     Random Forest :\n1. Model has a Score Accuracy: 99.987%\n2. Model has a Score Recall: 100.000%\n3. Model has a Score ROC AUC: 99.987%\n\n       XGBoost:\n1. Model has a Score Accuracy: 99.838%\n2. Model has a Score Recall: 99.934%\n3. Model has a Score ROC AUC: 99.838%","a1daa6b8":"# Lets try another Sampling for the Unbalanced Data and build a Kmean model"}}