{"cell_type":{"d4e672ae":"code","89c44b6b":"code","20719394":"code","f2b5da6a":"code","b4003273":"code","0ed5a41a":"code","efcdfc9c":"code","9a209f92":"code","c7189c56":"code","90cb549d":"code","9d6a4e52":"code","9533bfdb":"code","d12034e7":"code","3b9cde53":"code","1f24280e":"code","aeca71f7":"code","7ebc206d":"markdown","22ab4a31":"markdown","6cd6bc10":"markdown","0c9b422a":"markdown","314aa66e":"markdown","b1723def":"markdown","a70defde":"markdown","4993314d":"markdown","44da0193":"markdown","4bb39a63":"markdown","16dda7c8":"markdown","ad20b46a":"markdown","9a098c09":"markdown"},"source":{"d4e672ae":"from IPython.display import display, set_matplotlib_formats\nfrom collections import Counter\nfrom itertools import chain\nfrom keras.layers import Dense,Embedding,LSTM,Dropout\nfrom keras.models import Sequential\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom nltk.corpus import stopwords\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport gensim\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport re\nimport seaborn as sns\nimport warnings\nset_matplotlib_formats('svg')\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","89c44b6b":"df = pd.read_csv('\/kaggle\/input\/fake-news-detection\/data.csv')\ndf.head()","20719394":"df['website'] = df.URLs.apply(lambda x: x.split('\/')[2])\ndf.pivot_table(index = 'website', columns = 'Label', values = 'URLs', aggfunc='count').fillna(0).astype(int)","f2b5da6a":"sns.set_style(\"white\")\nsns.set_palette('Set2')\nfig, ax=plt.subplots(figsize=(3, 2))\nax = sns.countplot(df.Label)\nfig.show()","b4003273":"df['text'] = df['Headline'] + \" \" + df['Body']\ndf = df.drop(columns = ['Headline', 'Body'])\ndf = df.loc[~df['text'].isna()] # ","0ed5a41a":"stop_words = set(stopwords.words('english'))\nto_remove = ['\u2022', '!', '\"', '#', '\u201d', '\u201c', '$', '%', '&', \"'\", '\u2013', '(', ')', '*', '+', ',', '-', '.', '\/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '\u2026']\nstop_words.update(to_remove)\nprint('Number of stopwords:', len(stop_words))\n\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r'http\\S+', '', text)\n    text = re.sub('\\[[^]]*\\]', '', text)\n    text = (\" \").join([word for word in text.split() if not word in stop_words])\n    text = \"\".join([char for char in text if not char in to_remove])\n    return text\n\ndf['text'] = df['text'].apply(clean_text)","efcdfc9c":"def print_frequency(words, name):\n    counter = Counter(chain.from_iterable(words))\n    df_word_distribution = pd.DataFrame(counter.values(), index = counter.keys(), columns = ['Frequency'])\n    df_word_distribution.index = df_word_distribution.index.set_names(name)\n    df_word_distribution = df_word_distribution.sort_values(by = 'Frequency', ascending = False, axis=0)[:15]\n    df_word_distribution = df_word_distribution.pivot_table(columns = name)\n    df_word_distribution = df_word_distribution.sort_values(by = 'Frequency', ascending = False, axis=1)\n    display(df_word_distribution)\n    \nwords_fake = [s.split() for s in df.loc[df.Label == 0]['text']]\nwords_true = [s.split() for s in df.loc[df.Label == 1]['text']]\nprint_frequency(words_fake, 'Fake')\nprint_frequency(words_true, 'True')","9a209f92":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(6,3))\ntext_len=df[df.Label==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(text_len, bins = 50)\nax1.set_title('True texts')\ntext_len=df[df.Label==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(text_len, bins = 50)\nax2.set_title('Fake texts')\nfig.suptitle('Number of words in texts', y=0)\nfig.show()","c7189c56":"text_train, text_test, y_train, y_test = train_test_split(df['text'], df['Label'], test_size = 0.2, random_state = 42) ","90cb549d":"size_embedding = 200 #Dimensionality of the feature vectors\nwindows = 2 #Maximum distance between the current and predicted word within a sentence\nmin_count = 1 #Ignores words with total frequency lower than this\nmaxlen = 1000 #Length decided for the text (adjusted by padding and truncating)\n\ntext_train_splited = [article.split() for article in text_train]\nw2v_model = gensim.models.Word2Vec(sentences = text_train_splited, \n                                   size = size_embedding, \n                                   window = windows, \n                                   min_count = min_count)\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(text_train_splited)\ntext_train_tok = tokenizer.texts_to_sequences(text_train_splited)\nword_index = tokenizer.word_index\nprint('Sive of vocabulary: ', len(word_index))\n\ntext_train_tok_pad = pad_sequences(text_train_tok, maxlen=maxlen)","9d6a4e52":"def w2v_to_keras_weights(model, vocab):\n    vocab_size = len(vocab) + 1\n    weight_matrix = np.zeros((vocab_size, size_embedding))\n    for word, i in vocab.items():\n        weight_matrix[i] = model[word]\n    return weight_matrix\n\nembedding_vectors = w2v_to_keras_weights(w2v_model, word_index)","9533bfdb":"def set_model(embedding_vectors):\n    model = Sequential()\n    model.add(Embedding(embedding_vectors.shape[0], \n                        output_dim=embedding_vectors.shape[1],\n                        weights=[embedding_vectors], \n                        input_length=maxlen, \n                        trainable=False))\n    model.add(LSTM(units=32))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n    return model","d12034e7":"model = set_model(embedding_vectors = embedding_vectors)\nmodel.summary()\n\nhistory = model.fit(text_train_tok_pad, y_train, validation_split=0.2, epochs=30, batch_size = 32, verbose = 1)","3b9cde53":"def plot_loss_epochs(history):\n    epochs = np.arange(1,len(history.history['acc']) + 1,1)\n    train_acc = history.history['acc']\n    train_loss = history.history['loss']\n    val_acc = history.history['val_acc']\n    val_loss = history.history['val_loss']\n\n    fig , ax = plt.subplots(1,2, figsize=(7,3))\n    ax[0].plot(epochs , train_acc , '.-' , label = 'Train Accuracy')\n    ax[0].plot(epochs , val_acc , '.-' , label = 'Validation Accuracy')\n    ax[0].set_title('Train & Validation Accuracy')\n    ax[0].legend()\n    ax[0].set_xlabel(\"Epochs\")\n    ax[0].set_ylabel(\"Accuracy\")\n\n    ax[1].plot(epochs , train_loss , '.-' , label = 'Train Loss')\n    ax[1].plot(epochs , val_loss , '.-' , label = 'Validation Loss')\n    ax[1].set_title('Train & Validation Loss')\n    ax[1].legend()\n    ax[1].set_xlabel(\"Epochs\")\n    ax[1].set_ylabel(\"Loss\")\n    fig.tight_layout()\n    fig.show()\n    \nplot_loss_epochs(history)","1f24280e":"model.fit(text_train_tok_pad, y_train, epochs=15, batch_size = 16, verbose = 0)","aeca71f7":"text_train_splited = [article.split() for article in text_test]\ntext_test_tok = tokenizer.texts_to_sequences(text_train_splited)\ntext_test_tok_pad = pad_sequences(text_test_tok, maxlen=maxlen)\npred = (model.predict(text_test_tok_pad) > 0.5).astype(\"int32\")\n\nprint(classification_report(y_test, pred, target_names = ['Fake','Not Fake'])) \ncm = pd.DataFrame(confusion_matrix(y_test,pred))\n\nfig , ax = plt.subplots(figsize = (2,2))\nax = sns.heatmap(cm, annot = True, xticklabels = ['Fake','True'] , yticklabels = ['Fake','True'], cbar = False, fmt='')\nax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"Actual\"); fig.show()","7ebc206d":"We first fit our Word2Vec model on the training articles.   \nWe also tokenize the words of the articles, *i.e.* we associate a number to each word (the more frequent is the word, the lower is the number). The tokenized articles are then all set at the same length, thanks to padding and truncating.","22ab4a31":"21\/3988 = 0.5% of rows contain neither headline nor body. We remove these rows.\n\n**The next step is to clean the data:**\n1. Text is converted to lowercase.\n2. URLs are removed: due to internal website references, the model risks learning that an article containing a link to beforeitsnews.com is fake news. But we don't want the model to depend on the website.\n3. Braquets are removed.\n4. Stopwords are removed.\n5. Some characters are removed: The stopwords filtering only filtered the words. However, some punctuation marks is stuck to the words, such as \"Thus,\": the comma is not a separate word and therefore was not deleted.","6cd6bc10":"One of the simplest methods of text classification techniques is to associate each word with a frequency for the two (or more) classes. We can first look at the difference in the distribution of the most frequent words in the two classes:","0c9b422a":"# Training the model\nWe will use a simple LSTM. We do not want the model to modify the weights determined by the Word2Vec model, so we set the trainable parameter to False.","314aa66e":"# Data analysis\nHere we will answer three questions:\n- Is fake news class 0 or 1?\n- How was the dataset created?  \n- Is the dataset balanced?\n\nOne can think that the labels were created directly from the websites containing the articles. To check this, we extract websites from URLs and look at the count of each class.","b1723def":"The dataset is balanced, we can directly use it.\n\n# Data Preprocessing\nBecause we have both the title and the body, by concatenating the two, we will use both in our model. One can imagine that a model using only titles may also be feasible, but would probably be less precise.","a70defde":"# Introduction\nThis dataset consists of 3,988 articles, some of which are fake news. We want to be able to predict if an article is a fake news.\n**1.** First of all, without knowledge about data, we need to understand which class is associated with fake news.\n**2.** Secondly, we will clean the data.  \n**3.** Thirdly, we will apply word embedding.\n**4.** Finally, we will use a LSTM to classify the articles.  ","4993314d":"# Scores on test set\n\nFirst, we will re-train the model on 100% of the training set - We previously kept 20% for the validation.","44da0193":"We can see that fake news texts tend to be shorter. A very large majority of texts are less than 1000 words long, so we can define a length of 1000 words for all texts, by adding a padding to texts of less than 1000 words and truncating those of more than 1000 words.\n\nGiven that the embedding requires a training, we keep aside a test set to test our model once finalized.","4bb39a63":"We have to generate a matrix containing the weights of the embedding layer of the model. *w2v_to_keras_weights* returns the weight matrix with a row per word, and each row is the vector associated with the word.\n\nThe matrix shape is *(nb of words in the vocab +1, Dimensionality of the feature vectors)*.\nThe first row only contains the vector [0 ... 0], and is here for the never seen words (words not in the training set).","16dda7c8":"In implementations using this type of fake news filtering, it can be more problematic to have false positives, that is, real news described as false, rather than false negatives, and that can be filtered for no reason. Using a **neural network with uncertainty** can provide a way to adjust the false positive rate, instead of just taking the sigmoid output as is the case here, without any information about the confidence of the output.\n\nThe biggest problem with this challenge is the lack of data: train set is constituted of 3190 observations (minus 20% for the validation), and test set is constituted of 798 observations.\nThe h5 file is 136 MB but seems empty. We can think of downloading more data thanks to a **web scraping algorithm** applied on the above websites.","ad20b46a":"The distribution of words differs between the two classes. Building a model that only uses word frequency is possible, but we lose the meaning of the text, which also contains useful information. Therefore, we will instead move to a Word2Vec + LSTM solution, capable of grasping this meaning. We will start by implementing the Word2Vec part.\n# Word2Vec\n\nWord2Vec is a very popular word embedding technique which uses neural network, and that was developed in 2013 by Google.\nThere are two way of using the word embedding:\n1. Using word embedding in an unsupervised way: This means that, based on the sentences we have, the model tries to capture the context of a word in a document, that is, the relation to other words, semantic and syntactic similarities, etc...\n2. Using word embedding as a first layer in the neural network\/LSTM. This means that the weights are learned through backpropagation of the classification error. Therefore, weights are learned by backpropagating the classification error. The model does not necessarily try to capture the context of a word in a document, it adjusts the weights only in such a way as to reduce loss. \n\n\n**We will use here the first method.** It is faster because the embedding only needs to be done once, and not on each iteration of the LSTM training. An advantage of the first method is the possibility of using pre-trained models, that is, models which have already associated the words with vectors. We haven't explored this solution here.","9a098c09":"1. No website publishes both fake news and real news. The labels were most likely created from the websites the articles originated from.\n\n2. Looking at the websites, we can easily assume that \n    - **Fake news are labelled 0** (e.g. beforeitsnews.com).\n    - **True news are labelled 1** (e.g. bbc.com).  \n\nWe should therefore not use URLs to make predictions, which seems logical. The last question is, how balanced is the dataset?"}}