{"cell_type":{"436a8e03":"code","7a7860e9":"code","753d6741":"code","6db094bb":"code","b80e0608":"code","e6e1c7a1":"code","78a886a2":"code","32b96cf8":"code","b292e4e3":"code","1f48979f":"code","dded5e31":"code","80f28c0d":"code","99a4bdcb":"code","d7c4e467":"code","e988aa95":"code","ebd5b3df":"code","646e0e07":"code","868ade14":"code","9cff4378":"code","6093990f":"code","0850d6f8":"code","13805553":"code","2abc0820":"code","0aab840e":"code","cbc94f82":"code","3d8e1de7":"code","dd940959":"code","4b654422":"code","e897c72b":"code","ced5f58f":"code","dd587c9a":"code","df66bea7":"code","9a3e7d00":"code","bcba7f13":"code","a991e262":"code","55b210f1":"code","b80e2df7":"code","0d097e63":"code","d74d4b05":"code","e390ce13":"code","8c464202":"code","a01fdb8c":"code","64f9060a":"code","3c9ce275":"code","5d612c49":"code","e0718ec2":"code","5437ace0":"code","79e046c2":"code","e60930d8":"code","e4b8397b":"code","ddc1fb81":"code","c17e1344":"code","f57167e5":"code","6a6add78":"markdown","deb760df":"markdown","197beda3":"markdown"},"source":{"436a8e03":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport nltk\nfrom collections import Counter\n\nfrom plotly import graph_objs as go\nfrom sklearn import preprocessing \nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom keras.preprocessing import text, sequence\n\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM,Dropout, Bidirectional, Conv2D\nfrom keras.callbacks import ReduceLROnPlateau\nimport tensorflow as tf\nimport transformers\nfrom tokenizers import BertWordPieceTokenizer\nfrom keras.layers import LSTM,Dense,Bidirectional,Input\nfrom keras.models import Model\nimport torch\nimport transformers\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, plot_confusion_matrix, plot_precision_recall_curve\nimport warnings\nwarnings.simplefilter(\"ignore\")","7a7860e9":"data = pd.read_csv(\"..\/input\/mldm-case-study-report\/data.csv\")\ndata.head()","753d6741":"df = pd.DataFrame()","6db094bb":"df['text'] = data['content_words'].apply(lambda x:' '.join(eval(x)))\ndf['class'] = data['Level']","b80e0608":"df['class'].value_counts()","e6e1c7a1":"import nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\n\n# If not previously performed:\nnltk.download('stopwords')\n\nstemming = PorterStemmer()\nstops = set(stopwords.words(\"english\"))\n\ndef apply_cleaning_function_to_list(X):\n    cleaned_X = []\n    for element in X:\n        cleaned_X.append(clean_text(element))\n    return cleaned_X\n\n\ndef clean_text(raw_text):\n    text = raw_text.lower()\n    tokens = nltk.word_tokenize(text)\n    token_words = [w for w in tokens if w.isalpha()]\n    \n    stemmed_words = [stemming.stem(w) for w in token_words]\n    \n    meaningful_words = [w for w in stemmed_words if not w in stops]\n    \n    joined_words = ( \" \".join(meaningful_words))\n    \n    return joined_words","78a886a2":"def create_bag_of_words(X):\n    from sklearn.feature_extraction.text import CountVectorizer\n    \n    print ('Creating bag of words...')\n    \n    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n                                 tokenizer = None,    \\\n                                 preprocessor = None, \\\n                                 stop_words = None,   \\\n                                 ngram_range = [2,2], \\\n                                 max_features = 10000\n                                ) \n\n    train_data_features = vectorizer.fit_transform(X)\n    train_data_features = train_data_features.toarray()\n    \n    from sklearn.feature_extraction.text import TfidfTransformer\n    tfidf = TfidfTransformer()\n    tfidf_features = tfidf.fit_transform(train_data_features).toarray()\n\n    vocab = vectorizer.get_feature_names()\n   \n    return vectorizer, vocab, train_data_features, tfidf_features, tfidf","32b96cf8":"def sampling_k_elements(group, k=3):\n    if len(group) < k:\n        return group\n    return group.sample(k)\n\ndf_balanced = df.groupby('class').apply(sampling_k_elements,500).reset_index(drop=True)","b292e4e3":"df_balanced['class'].value_counts()","1f48979f":"text_to_clean= list(df_balanced['text'])\ncleaned_text = apply_cleaning_function_to_list(text_to_clean)\ndf_balanced['text']=cleaned_text\ndf_balanced.head()","dded5e31":"from sklearn.model_selection import train_test_split\nX = df_balanced['text']\ny = df_balanced['class']\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25)","80f28c0d":"vectorizer, vocab, train_data_features, tfidf_features, tfidf = create_bag_of_words(X_train)","99a4bdcb":"bag_dictionary = pd.DataFrame()\nbag_dictionary['ngram'] = vocab\nbag_dictionary['count'] = train_data_features[0]\nbag_dictionary['tfidf_features'] = tfidf_features[0]\n\nbag_dictionary.sort_values(by=['count'], ascending=False, inplace=True)\n# Show top 10\nprint(bag_dictionary.head(10))","d7c4e467":"from sklearn.linear_model import LogisticRegression\n\ndef train_logistic_regression(features, label):\n    print (\"Training the logistic regression model...\")\n    from sklearn.linear_model import LogisticRegression\n    ml_model = LogisticRegression(C = 100,random_state = 0)\n    ml_model.fit(features, label)\n    print ('Finished')\n    return ml_model","e988aa95":"ml_model = train_logistic_regression(tfidf_features, y_train)","ebd5b3df":"test_data_features = vectorizer.transform(X_test)\ntest_data_features = test_data_features.toarray()","646e0e07":"test_data_tfidf_features = tfidf.fit_transform(test_data_features)\ntest_data_tfidf_features = test_data_tfidf_features.toarray()","868ade14":"predicted_y = ml_model.predict(test_data_tfidf_features)\npred_prob = ml_model.predict_proba(test_data_tfidf_features)\ncorrectly_identified_y = predicted_y == y_test\naccuracy = np.mean(correctly_identified_y) * 100\nprint ('Accuracy = ',accuracy,\"%\")","9cff4378":"from sklearn.metrics import roc_curve\nfpr = {}\ntpr = {}\nthresh ={}\n\nn_class = 3\n\nfor i in range(n_class):    \n    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, pred_prob[:,i], pos_label=i)\n\nplt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='HQ vs Rest')\nplt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='LQ(Close) vs Rest')\nplt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='LQ(Open) vs Rest')\nplt.plot([0,1], linestyle='-', lw=2, color='r', label='random', alpha=0.8)\nplt.title('Multiclass ROC curve - Logistic Regression')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive rate')\nplt.legend(loc='best')\nplt.savefig('Multiclass ROC',dpi=300);    ","6093990f":"nb_classifier = MultinomialNB()\nnb_classifier.fit(tfidf_features,y_train)","0850d6f8":"predicted_y = nb_classifier.predict(test_data_tfidf_features)\npred_prob = nb_classifier.predict_proba(test_data_tfidf_features)\ncorrectly_identified_y = predicted_y == y_test\naccuracy = np.mean(correctly_identified_y) * 100\nprint ('Accuracy = ',accuracy,\"%\")","13805553":"from sklearn.metrics import roc_curve\nfpr = {}\ntpr = {}\nthresh ={}\n\nn_class = 3\n\nfor i in range(n_class):    \n    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, pred_prob[:,i], pos_label=i)\n\nplt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='HQ vs Rest')\nplt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='LQ(Close) vs Rest')\nplt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='LQ(Open) vs Rest')\nplt.plot([0,1], linestyle='-', lw=2, color='r', label='random', alpha=0.8)\nplt.title('Multiclass ROC curve - Naive Bayes')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive rate')\nplt.legend(loc='best')\nplt.savefig('Multiclass ROC',dpi=300);    ","2abc0820":"dt_classifier = DecisionTreeClassifier()\ndt_classifier.fit(tfidf_features,y_train)","0aab840e":"predicted_y = dt_classifier.predict(test_data_tfidf_features)\npred_prob = dt_classifier.predict_proba(test_data_tfidf_features)\ncorrectly_identified_y = predicted_y == y_test\naccuracy = np.mean(correctly_identified_y) * 100\nprint ('Accuracy = ',accuracy,\"%\")","cbc94f82":"from sklearn.metrics import roc_curve\nfpr = {}\ntpr = {}\nthresh ={}\n\nn_class = 3\n\nfor i in range(n_class):    \n    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, pred_prob[:,i], pos_label=i)\n\nplt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='HQ vs Rest')\nplt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='LQ(Close) vs Rest')\nplt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='LQ(Open) vs Rest')\nplt.plot([0,1], linestyle='-', lw=2, color='r', label='random', alpha=0.8)\nplt.title('Multiclass ROC curve - Decision Tree')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive rate')\nplt.legend(loc='best')\nplt.savefig('Multiclass ROC',dpi=300);    ","3d8e1de7":"df = pd.read_csv(\"..\/input\/60k-stack-overflow-questions-with-quality-rate\/train.csv\")\ndf['text'] = df['Title'] + \" \" + df['Body']\ncols_to_drop = ['Id', 'Tags', 'CreationDate', 'Title', 'Body']\ndf.drop(cols_to_drop, axis=1, inplace=True)\ndf = df.rename(columns={\"Y\": \"class\"})\n\nstop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation)\n\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r'[^(a-zA-Z)\\s]','', text)\n    return text\ndf['text'] = df['text'].apply(clean_text)\n\nlabel_encoder = preprocessing.LabelEncoder() \n  \n# Encode labels in column 'class'. \ndf['class']= label_encoder.fit_transform(df['class']) \ndf['class'].unique() \n\nx_train,x_test,y_train,y_test = train_test_split(df['text'], df['class'], test_size = 0.2, random_state = 42, stratify = df['class'])\nmax_features = 10000\nmaxlen = 300","dd940959":"tokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(x_train)\ntokenized_train = tokenizer.texts_to_sequences(x_train)\nx_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)","4b654422":"tokenized_test = tokenizer.texts_to_sequences(x_test)\nX_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)","e897c72b":"EMBEDDING_FILE = '..\/input\/glove-twitter\/glove.twitter.27B.200d.txt'","ced5f58f":"def get_coefs(word, *arr): \n    return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))","dd587c9a":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\n#change below line if computing normal stats is too slow\nembedding_matrix = embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","df66bea7":"batch_size = 256\nepochs = 2\nembed_size = 200","9a3e7d00":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.4, min_lr=0.0000001)","bcba7f13":"maxlen","a991e262":"model = Sequential()\nmodel.add(Embedding(max_features, output_dim=embed_size, weights=[embedding_matrix], input_length=maxlen, trainable=True))\nmodel.add(Bidirectional(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.4 , dropout = 0.4)))\nmodel.add(Bidirectional(LSTM(units=128 , recurrent_dropout = 0.2 , dropout = 0.2)))\nmodel.add(Dense(3, activation='softmax'))\nmodel.compile(optimizer=keras.optimizers.Adam(lr = 0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","55b210f1":"model.summary()","b80e2df7":"history = model.fit(x_train, y_train, batch_size = batch_size , validation_data = (X_test,y_test) , epochs = epochs , callbacks = [learning_rate_reduction])","0d097e63":"print(\"Accuracy of the model on Training Data is - \" , model.evaluate(x_train,y_train)[1]*100 , \"%\")\nprint(\"Accuracy of the model on Testing Data is - \" , model.evaluate(X_test,y_test)[1]*100 , \"%\")","d74d4b05":"epochs = [i for i in range(2)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\nax[0].set_title('Training & Testing Accuracy')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_loss , 'go-' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'ro-' , label = 'Testing Loss')\nax[1].set_title('Training & Testing Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Loss\")\nplt.show()","e390ce13":"pred = model.predict_classes(X_test)","8c464202":"print(classification_report(y_test, pred, target_names = ['HQ', 'LQ(Close)', 'LQ(Open)']))","a01fdb8c":"cm = confusion_matrix(y_test,pred)\ncm = pd.DataFrame(cm , index = ['HQ', 'LQ(Close)', 'LQ(Open)'] , columns = ['HQ', 'LQ(Close)', 'LQ(Open)'])\n\nplt.figure(figsize = (10,10))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['HQ', 'LQ(Close)', 'LQ(Open)'] , yticklabels = ['HQ', 'LQ(Close)', 'LQ(Open)'])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")","64f9060a":"pred_prob = model.predict_proba(X_test)","3c9ce275":"pred_prob","5d612c49":"from sklearn.metrics import roc_curve\nfpr = {}\ntpr = {}\nthresh ={}\n\nn_class = 3\n\nfor i in range(n_class):    \n    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, pred_prob[:,i], pos_label=i)\n\nplt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='HQ vs Rest')\nplt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='LQ(Close) vs Rest')\nplt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='LQ(Open) vs Rest')\nplt.plot([0,1], linestyle='-', lw=2, color='r', label='random', alpha=0.8)\nplt.title('Multiclass ROC curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive rate')\nplt.legend(loc='best')\nplt.savefig('Multiclass ROC',dpi=300);    ","e0718ec2":"def predict(text):\n    text = clean_text(text);\n    x_test = sequence.pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=maxlen)\n    score = model.predict([x_test])[0]\n\n    return score  ","5437ace0":"cols= ['HQ','LQ(CLOSE)','LQ(OPEN)']\ncols[predict(\"I am developing a media player using vlc-qt , Actually I want a button which will play do the fast backward operation. I don't have the problem with the fast forward operation but not able to implement the fast backward operation, Is there any function there in vlc-qt which will play the video backwards. Here are the buttons code which I am using for fast forward and fast backward operation\").argmax()]","79e046c2":"cols[predict(df['text'][7]).argmax()]","e60930d8":"cols[predict(df['text'][0]).argmax()]","e4b8397b":"cols[predict(df['text'][2]).argmax()]","ddc1fb81":"df['text'][7]","c17e1344":"df['text'][0]","f57167e5":"df['text'][2]","6a6add78":"**3. DECISION TREE (TFIDF MODEL)**","deb760df":"**1. LOGISTIC REGRESSION (TFIDF MODEL)**","197beda3":"**2. NAIVE BAYES (TFIDF MODEL)**"}}