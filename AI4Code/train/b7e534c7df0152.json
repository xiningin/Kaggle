{"cell_type":{"55ee20d3":"code","d03baf53":"code","de5d3dae":"code","2b01fa14":"code","621322a4":"code","f57b15cd":"code","16bd81f6":"code","dcc260d2":"code","ed573a5f":"code","9521591c":"code","976deff2":"code","b2263421":"code","8ca1794d":"code","4e1c1b26":"code","b42fcd63":"code","501ad7df":"code","54ac294d":"code","0a03d01e":"code","b8f4c7d0":"code","9610c3f3":"code","a531d5d2":"code","14f4c53f":"code","ee34aa07":"code","3d256f4e":"code","fc405a98":"code","82b706dd":"code","46133458":"code","d016227e":"code","7911701f":"code","5814637e":"code","097e3c01":"code","dcc56706":"code","df548a75":"code","82088759":"code","2ef0f62a":"code","d581fd8e":"code","78021fbc":"code","064ea431":"code","1eb2b868":"code","08c08666":"code","925abca2":"code","bb91cda2":"code","1717d60f":"code","fb0438b1":"code","8f666866":"code","d45b70a7":"code","de98666d":"code","3de584c4":"code","e100b383":"code","e1242545":"code","42a99dda":"code","c3f9fd1b":"code","ceb5c8fd":"code","e68e9d4c":"code","0e918f10":"code","db266105":"code","2071bf96":"code","db0dfd45":"code","2539b9ef":"code","b58e0d35":"code","40363ee5":"code","6cbecbde":"code","0671a803":"code","701df309":"code","a6b11359":"code","c6f473d0":"code","52d53b77":"code","940a7832":"code","0ea41ec3":"code","16f66121":"code","21966336":"code","7025a09b":"code","17b0426a":"code","4d52ecfd":"code","a40fdfe2":"code","70a945cd":"code","1f55923d":"code","6c17a410":"code","3899c5a3":"code","155037cd":"code","54382c26":"code","c0aa4485":"code","d1a2fc13":"code","3e555caa":"code","2a9ad063":"code","5c39ef02":"code","2b0a8f25":"code","ea3acc19":"code","2f3a52bd":"code","0f71fb8b":"code","02edb3af":"code","3d5bde87":"code","c0ada571":"code","a31b0849":"code","95708a58":"code","c1f12d07":"code","89192ef9":"code","a6ec4017":"code","0e88ecef":"code","5de28d45":"code","58a72f41":"code","29f9055a":"code","bb6a846d":"code","873f902b":"markdown","04a6741f":"markdown","d68a6d30":"markdown","a07de677":"markdown","ce511cae":"markdown","bb8f1184":"markdown","e93c052b":"markdown","a70423f0":"markdown","8d098374":"markdown","ac68361e":"markdown","5fe6cda9":"markdown","ba434eeb":"markdown","32eb4f2e":"markdown","56b3c829":"markdown","ab15fda1":"markdown","4b9d5bc4":"markdown","e3913840":"markdown","05824595":"markdown","8626d609":"markdown","3b21081a":"markdown","8794e4fd":"markdown","8f065a3e":"markdown","10746f32":"markdown","30f22236":"markdown","33ed3127":"markdown","fa3091ac":"markdown","16500c39":"markdown","a62935d3":"markdown","63716bea":"markdown","169384d1":"markdown","85776216":"markdown","07567de2":"markdown","b7a88920":"markdown","f0f37393":"markdown","a1a98445":"markdown","29648847":"markdown","ac7be275":"markdown","99dedbb5":"markdown","01439dd4":"markdown","c84697fa":"markdown","6a12128c":"markdown","b96dc99a":"markdown","dcfe80c2":"markdown","2645389e":"markdown"},"source":{"55ee20d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns #seaborn for data visualisation\nimport matplotlib as mpl #matplotlib for data visualisation\nimport matplotlib.pyplot as plt\n#import missingno as msno\nimport scipy.stats as st\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d03baf53":"train_data = pd.read_csv(\"..\/input\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/test.csv\")","de5d3dae":"train_data['PoolQC'].value_counts()","2b01fa14":"print(\"*\" * 50)\nprint(\"Train shape is : \", train_data.shape)\nprint(\"Test shape is :\", test_data.shape)\nprint(\"*\" * 50)","621322a4":"train_data.describe(include='all')","f57b15cd":"test_data.describe(include='all')","16bd81f6":"def dataHasNaN(df):\n    HasNaN = False\n    if df.count().min()!=df.shape[0]:\n        HasNaN = True\n    return HasNaN    \n\ndef is_categorical(array_like):\n    return array_like.dtype.name == 'object'","dcc260d2":"#Target variable distribution\nsns.distplot(train_data['SalePrice'])","ed573a5f":"print(\"Train data['SalePrice']\")\nprint(\"*\" * 50)\nprint(\"Skew     : \", train_data['SalePrice'].skew())\nprint(\"Kurtosis : \", train_data['SalePrice'].kurtosis())\nprint(\"*\" * 50)","9521591c":"train_data.columns","976deff2":"#GrLivArea: Above grade (ground) living area square feet\ntrain_data.plot.scatter(x='GrLivArea', y='SalePrice')","b2263421":"#TotalBsmtSF: Total square feet of basement area\ntrain_data.plot.scatter(x='TotalBsmtSF', y='SalePrice')","8ca1794d":"#OverallQual: Rates the overall material and finish of the house\nf, ax = plt.subplots(figsize=(8, 6))\nax = sns.boxplot(x='OverallQual', y='SalePrice', data=train_data)","4e1c1b26":"train_data.plot.scatter(x='YearBuilt', y='SalePrice')","b42fcd63":"corr = abs(train_data.corr())\nf, ax = plt.subplots(figsize=(10,8))\nsns.heatmap(corr, vmax=0.8, square=True)","501ad7df":"cols = corr['SalePrice'].nlargest(10).index.tolist()\ncorr = abs(train_data[cols].corr())\nsns.heatmap(corr, annot=True)","54ac294d":"collinear_indices = ['TotRmsAbvGrd', '1stFlrSF', 'GarageCars']\ncols = [index for index in cols if index not in collinear_indices]\nsns.pairplot(train_data[cols])","0a03d01e":"plt.figure(3); plt.title('Log Normal')\nsns.distplot(train_data['SalePrice'], kde=False, fit=st.lognorm)","b8f4c7d0":"res = st.probplot(train_data['SalePrice'], plot=plt)","9610c3f3":"#Normalising the target variable by taking its log.\n#train_data['SalePrice']\ntarget = np.log(train_data['SalePrice'])","a531d5d2":"#We can see in the plot that the target variable distribution now approaches a normal distribution.\nsns.distplot(target, fit=st.norm)\nfig = plt.figure()\nres = st.probplot(target, plot=plt)","14f4c53f":"print(\"Log SalePrice skew and kurtosis :\")\nprint(\"*\"*50)\nprint(\"Skew     : \", target.skew())\nprint(\"Kurtosis : \", target.kurtosis())\nprint(\"*\"*50)","ee34aa07":"sns.distplot(train_data['GrLivArea'], fit=st.norm)\nfig = plt.figure()\nres = st.probplot(train_data['GrLivArea'], plot=plt)","3d256f4e":"sns.distplot(train_data['TotalBsmtSF'], fit=st.norm)\nfig = plt.figure()\nres = st.probplot(train_data['TotalBsmtSF'], plot=plt)","fc405a98":"#Whether data has NaN\ndataHasNaN(train_data), dataHasNaN(test_data)","82b706dd":"#Keeping the ids for final output\n\ntrain_data_id = train_data['Id']\ntest_data_id = test_data['Id']\n\ntrain_data.drop(['Id'], axis=1, inplace=True)\ntest_data.drop(['Id'], axis=1, inplace=True)","46133458":"#Concatenate the frames for data preprocessing\n\nframes = [train_data.drop(['SalePrice'], axis =1), test_data]\ndf = pd.concat(frames)\ndf.reset_index(drop=True, inplace=True)","d016227e":"df['MSSubClass'] = df['MSSubClass'].astype(dtype='object')","7911701f":"#We would want to avoid multicollinearity. Hence, if two variables strongly correlate to each other \n#we would want to remove one of them.\n\ndf.drop(collinear_indices, axis=1, inplace=True)","5814637e":"dataHasNaN(df)","097e3c01":"na_df = (df.isnull().sum()\/len(df)) * 100\nna_df = na_df[na_df!=0].sort_values(ascending=False)\nna_df = pd.DataFrame({'Na ratio': na_df})\nna_df.head()","dcc56706":"na_df.index","df548a75":"df['PoolQC'] = df['PoolQC'].fillna(\"None\")\ndf['MiscFeature'] = df['MiscFeature'].fillna(\"None\")\ndf['Alley'] = df['Alley'].fillna(\"None\")\ndf['Fence'] = df['Fence'].fillna(\"None\")\ndf['FireplaceQu'] = df['FireplaceQu'].fillna(\"None\")","82088759":"df['LotFrontage'] = df.groupby('Neighborhood')['LotFrontage'].transform(\n                        lambda LotFrontage_Grp : LotFrontage_Grp.fillna(\n                            LotFrontage_Grp.median()\n                        )\n                    )\n#df.groupby('Neighborhood')['LotFrontage'].get_group('Blmngtn').median()","2ef0f62a":"for attr in ['GarageCond', 'GarageQual', 'GarageFinish', 'GarageType']:\n    df[attr] = df[attr].fillna('None')","d581fd8e":"df['GarageYrBlt'] = df['GarageYrBlt'].fillna(0)","78021fbc":"for attr in ['BsmtExposure', 'BsmtCond', 'BsmtQual', 'BsmtFinType2', 'BsmtFinType1']:\n    df[attr] = df[attr].fillna(\"None\")","064ea431":"df['MasVnrType'] = df['MasVnrType'].fillna('None')","1eb2b868":"df['MasVnrArea'] = df['MasVnrArea'].fillna(0)","08c08666":"df['MSZoning'] = df['MSZoning'].fillna(df['MSZoning'].mode()[0])","925abca2":"df['BsmtHalfBath'] = df['BsmtHalfBath'].fillna(0)\ndf['BsmtFullBath'] = df['BsmtFullBath'].fillna(0)","bb91cda2":"df['BsmtFullBath'] = df['BsmtHalfBath'] + 0.5 * df['BsmtFullBath']\ndf.drop('BsmtHalfBath', axis=1, inplace=True)","1717d60f":"df['Functional'] = df['Functional'].fillna('Typ')","fb0438b1":"df['Exterior1st'] = df['Exterior1st'].fillna(df['Exterior1st'].mode()[0])\ndf['Exterior2nd'] = df['Exterior2nd'].fillna(df['Exterior2nd'].mode()[0])","8f666866":"df['SaleType'] = df['SaleType'].fillna(df['SaleType'].mode()[0])","d45b70a7":"df['BsmtFinSF1'] = df['BsmtFinSF1'].fillna(0)\ndf['BsmtFinSF2'] = df['BsmtFinSF2'].fillna(0)\ndf['BsmtUnfSF'] = df['BsmtUnfSF'].fillna(0)","de98666d":"df['Electrical'] = df['Electrical'].fillna(df['Electrical'].mode()[0])","3de584c4":"df['Utilities'].value_counts()","e100b383":"df.drop('Utilities', axis=1, inplace=True)","e1242545":"df['KitchenQual'] = df['KitchenQual'].fillna(df['KitchenQual'].mode()[0])","42a99dda":"df['GarageArea'] = df['GarageArea'].fillna(0)","c3f9fd1b":"df['TotalBsmtSF'] = df['TotalBsmtSF'].fillna(0)","ceb5c8fd":"dataHasNaN(df)","e68e9d4c":"df['GarageQual'].value_counts()","0e918f10":"cat_col = [ elem for elem in df.columns.tolist() if is_categorical(df[elem])==True ]\nnum_col = [ elem for elem in df.columns.tolist() if elem not in cat_col ]","db266105":"len(cat_col)+len(num_col)==len(df.columns)","2071bf96":"skew_feats_series = abs(df[num_col].skew()).sort_values(ascending=False)\nskewness = pd.DataFrame({'skew' : skew_feats_series}).dropna()\n\nskewness.head(10)","db0dfd45":"dataHasNaN(skewness)","2539b9ef":"skewness = skewness[skewness['skew'] > 0.75]","b58e0d35":"skewness","40363ee5":"len(skewness)","6cbecbde":"skewness.index","0671a803":"from scipy.stats import boxcox_normplot\nlm,ppcc = boxcox_normplot(df['GrLivArea'], la=-5, lb=5)","701df309":"plt.plot(lm, ppcc)","a6b11359":"from scipy.stats import boxcox\nskewed_features = skewness.index\ntot_lm = 0\nfor feat in skewed_features:\n    op, lm = boxcox(df[feat])\n    tot_lm += lm","c6f473d0":"#df[skewness.index] = np.log1p(df[skewness.index])\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    df[feat] = boxcox1p(df[feat], lam)","52d53b77":"df.head(10)","940a7832":"df.describe(include='all')","0ea41ec3":"scale_1 = {'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}\ndf['BsmtQual'] = df['BsmtQual'].map(scale_1)\ndf['BsmtCond'] = df['BsmtCond'].map(scale_1)\ndf['BsmtExposure'] = df['BsmtExposure'].map({'None':0, 'No':1, 'Mn':2, 'Av':3, 'Gd':4})\ndf['HeatingQC'] = df['HeatingQC'].map(scale_1)\ndf['KitchenQual'] = df['KitchenQual'].map(scale_1)\ndf['FireplaceQu'] = df['FireplaceQu'].map(scale_1)\ndf['GarageQual'] = df['GarageQual'].map(scale_1)\ndf['GarageCond'] = df['GarageCond'].map(scale_1)\ndf['PoolQC'] = df['PoolQC'].map(scale_1)\n\nscale_2 = {'None':0, 'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6}\ndf['BsmtFinType1'] = df['BsmtFinType1'].map(scale_2)\ndf['BsmtFinType2'] = df['BsmtFinType2'].map(scale_2)\n\n'''\ndf['LotShape'] = df['LotShape'].map({'Reg':1, 'IR1':2, 'IR2':3, 'IR3':4})\ndf['Utilities'] = df['Utilities'].map({'AllPub':1, 'NoSeWa':2, 'No value':3})\ndf['LotConfig'] = df['LotConfig'].map({'Inside':1, 'Corner':2, 'CulDSac':3, 'FR2':4, 'FR3':5})\ndf['BldgType'] = df['BldgType'].map({'1Fam':1, '2fmCon':2, 'Duplex':3, 'TwnhsE':4, 'Twnhs':5})\ndf['HouseStyle'] = df['HouseStyle'].map({'1Story':1, '1.5Unf':2, '1.5Fin':3, '2Story':4, '2.5Unf':5, '2.5Fin':6})\ndf['CentralAir'] = df['CentralAir'].map({'Y':0, 'N':1})\ndf['Functional'] = df['Functional'].map({'No value':0, 'Typ':1, 'Min1':2, 'Min2':3, 'Mod':4, 'Maj1':5, 'Maj2':6, 'Sev':7, 'Sal':8})\ndf['GarageType'] = df['GarageType'].map({'No value':0, 'Detchd':1, 'CarPort':2, 'BuiltIn':3, 'Basment':4, 'Attchd':5, '2Types':6})\ndf['GarageFinish'] = df['GarageFinish'].map({'No value':0, 'Unf':1, 'RFn':2, 'Fin':3})\ndf['PavedDrive'] = df['PavedDr].map({'N':0, 'P':1, 'Y':2})\ndf['Fence'] = df['Fence'].map({'No value':0, 'MnWw':1, 'GdWo':2, 'MnPrv':3, 'GdPrv':4})\n'''","16f66121":"for col in df.columns:\n    if df[col].count().min()!=df.shape[0]:\n        print(col)","21966336":"dataHasNaN(df)","7025a09b":"df = pd.get_dummies(df, drop_first=True)","17b0426a":"X_train = df.iloc[:train_data.shape[0]]\ny_train = pd.DataFrame(target, columns=['SalePrice'])\nX_test = df.iloc[train_data.shape[0]:]","4d52ecfd":"plt.scatter(train_data['GrLivArea'], train_data['SalePrice'])","a40fdfe2":"plt.scatter(X_train['GrLivArea'], y_train['SalePrice'])","70a945cd":"plt.scatter(train_data['LotArea'], train_data['SalePrice'])","1f55923d":"plt.scatter(X_train['LotArea'], y_train['SalePrice'])","6c17a410":"plt.scatter(train_data['TotalBsmtSF'], train_data['SalePrice'])","3899c5a3":"plt.scatter(X_train['TotalBsmtSF'], y_train['SalePrice'])","155037cd":"from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train.loc[:,num_col] = sc_X.fit_transform(X_train.loc[:, num_col])\nX_test.loc[:, num_col] = sc_X.transform(X_test.loc[:, num_col])","54382c26":"from sklearn.linear_model import Lasso, Ridge, ElasticNet\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler","c0aa4485":"from sklearn.model_selection import KFold, cross_val_score\n\ndef rmse_cv(model):\n    kf = KFold(n_splits=10,shuffle=True, random_state=333 )\n    rmse= np.sqrt(-cross_val_score(model, X_train, y_train['SalePrice'].ravel(), scoring=\"neg_mean_squared_error\", cv=kf))\n    return(rmse)","d1a2fc13":"L1_model = Lasso(alpha = 0.0005, random_state=1)\nL1_pipe = make_pipeline(RobustScaler(), L1_model)\nL1_pipe.fit(X_train, y_train)","3e555caa":"L1_pipe.score(X_train, y_train)","2a9ad063":"rmse_cv(L1_pipe).mean(), rmse_cv(L1_pipe).std()","5c39ef02":"coef_L1 = pd.Series(L1_model.coef_, index = df.columns)\nprint(\"Total coefficient removed : \", sum(coef_L1==0))\n\nimp_coef_L1 = pd.concat([coef_L1.sort_values().head(10), \n                  coef_L1.sort_values().tail(10)])\n\n#plt.figure(figsize=(20,10))\nimp_coef_L1.plot(kind='barh')","2b0a8f25":"y_pred_L1 = np.exp(L1_pipe.predict(X_test)).ravel()","ea3acc19":"L2_model = Ridge(alpha = 0.0005, random_state=1)\nL2_pipe = make_pipeline(RobustScaler(), L2_model )\nL2_pipe.fit(X_train, y_train)","2f3a52bd":"L2_pipe.score(X_train, y_train)","0f71fb8b":"rmse_cv(L2_pipe).mean(), rmse_cv(L2_pipe).std()","02edb3af":"L2_model.coef_ = L2_model.coef_.reshape(L2_model.coef_.shape[1])","3d5bde87":"coef_L2 = pd.Series(L2_model.coef_, index = df.columns)\nprint(\"Total coefficient removed : \", sum(coef_L2==0))\n\nimp_coef_L2 = pd.concat([coef_L2.sort_values().head(10), \n                  coef_L2.sort_values().tail(10)])\n\n#plt.figure(figsize=(20,10))\nimp_coef_L2.plot(kind='barh')","c0ada571":"y_pred_L2 = np.exp(L2_pipe.predict(X_test)).ravel()","a31b0849":"ENet_model = ElasticNet(alpha = 0.0005, l1_ratio=0.1, random_state = 102)\nENet_pipe = make_pipeline(RobustScaler(), ENet_model )\nENet_pipe.fit(X_train, y_train)","95708a58":"ENet_pipe.score(X_train, y_train)","c1f12d07":"rmse_cv(ENet_pipe).mean(), rmse_cv(ENet_pipe).std()","89192ef9":"coef_ENet = pd.Series(ENet_model.coef_, index = df.columns)\nprint(\"Total coefficient removed : \", sum(coef_ENet==0))\n\nimp_coef_ENet = pd.concat([coef_ENet.sort_values().head(10), \n                  coef_ENet.sort_values().tail(10)])\n\nimp_coef_ENet.plot(kind='barh')","a6ec4017":"y_pred_ENet = np.exp(ENet_pipe.predict(X_test)).ravel()","0e88ecef":"y_pred_L1","5de28d45":"y_pred_L2","58a72f41":"y_pred_ENet","29f9055a":"y_pred = y_pred_ENet\ny_pred = pd.DataFrame({'Id': test_data_id, 'SalePrice': y_pred})","bb6a846d":"y_pred.to_csv(\"submission.csv\", index=False)","873f902b":"## Missing Values\nWe have to take care of missing values in both train and test data.","04a6741f":"## Elastic net model","d68a6d30":"Missing values usually have no basements.","a07de677":"Ref : https:\/\/www.statisticshowto.datasciencecentral.com\/box-cox-transformation\/ \n\n### Boxcox transform : \nA Box Cox transformation is a way to transform non-normal dependent variables into a normal shape. Normality is an important assumption for many statistical techniques; if your data isn\u2019t normal, applying a Box-Cox means that you are able to run a broader number of tests.","ce511cae":"As given in the dataset description, NaN values indicate absence of the feature i.e. No Garage","bb8f1184":"#### Log Normalisation\nConsider the distribution given below :\n<img src=\"https:\/\/blog.minitab.com\/hubfs\/Imported_Blog_Media\/transformed_distribution.jpg?t=1538456769261\" width=\"400px\">\nNow let's look at the log graph :\n<img src=\"https:\/\/blog.minitab.com\/hubfs\/Imported_Blog_Media\/log_function.jpg?t=1538456769261\" width=\"400px\">\n\nIf a logarithmic transformation is applied to this distribution, the differences between smaller values will be expanded (because the slope of the logarithmic function is steeper when values are small) whereas the differences between larger values will be reduced (because of the very moderate slope of the log distribution for larger values). If you inflate differences on the left tail and reduce differences on the right side tail, the result will be a symmetrical normal distribution, and a variance that is now constant (whatever the mean).","e93c052b":"Given in the dataset; for missing values fill \"Typ\"","a70423f0":"Filling KitchenQual with most frequent value since it's a categorical value.","8d098374":"## Data description","ac68361e":"Merging BsmtFullBath and BsmtHalfBath into one.","5fe6cda9":"### 4. Feature distribution","ba434eeb":"Fill the missing value with the most frequent value, as it is a categorical value","32eb4f2e":"## Scaling","56b3c829":"Fill value with the most frequent value, since MSZoning is a categorical value.","ab15fda1":"# A Study in Regression \n\nReferences :\n1.  https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n2.  https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models \n\n\n#### Please upvote this  notebook, if you find it helpful at all.","4b9d5bc4":"## Dummy variable conversion","e3913840":"## Bias Variance TradeOff\nRef : https:\/\/machinelearningmastery.com\/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning\/,  https:\/\/towardsdatascience.com\/understanding-the-bias-variance-tradeoff-165e6942b229, https:\/\/datanice.github.io\/machine-learning-101-what-is-regularization-interactive.html \n\nThe prediction error for any machine learning algorithm can be broken down into three parts:\n* Bias Error\n* Variance Error\n* Irreducible Error\n\nThe irreducible error cannot be reduced regardless of what algorithm is used. It is the error introduced from the chosen framing of the problem and may be caused by factors like unknown variables that influence the mapping of the input variables to the output variable.\n\n### Bias Error\nBias is the difference between the expected or average prediction of our model and the correct value which we are trying to predict. It is the assumptions made by the model to make target easier to learn. Model with high bias pays very little attention to the training data and oversimplifies the model. It always leads to high error on training and test data. (see diagram below)\n\nGenerally, parametric algorithms have a high bias making them fast to learn and easier to understand but generally less flexible. In turn, they have lower predictive performance on complex problems. \n\nExamples of low-bias machine learning algorithms include: Decision Trees, k-Nearest Neighbors and Support Vector Machines\n\n### Variance Error\nVariance is the amount that the estimate of the target function will change if different training data was used. Ideally, it should not change too much from one training dataset to the next, meaning that the algorithm is good at picking out the hidden underlying mapping between the inputs and the output variables. \n\nMachine learning algorithms that have a high variance are strongly influenced by the specifics of the training data and do not generalize on the data which it hasn\u2019t seen before. As a result, such models perform very well on training data but has high error rates on test data. (see diagram below)\n\nExamples of low-variance machine learning algorithms include: Linear Regression, Linear Discriminant Analysis and Logistic Regression.\n\n### TradeOff\nYou can see a general trend in the examples above:\n* Parametric or linear machine learning algorithms often have a high bias but a low variance.\n* Non-parametric or non-linear machine learning algorithms often have a low bias but a high variance.\n\nThe parameterization of machine learning algorithms is often a battle to balance out bias and variance. Below are two examples of configuring the bias-variance trade-off for specific algorithms:\n\n* The k-nearest neighbors algorithm has low bias and high variance, but the trade-off can be changed by increasing the value of k which increases the number of neighbors that contribute t the prediction and in turn increases the bias of the model.\n\n* The support vector machine algorithm has low bias and high variance, but the trade-off can be changed by increasing the C parameter that influences the number of violations of the margin allowed in the training data which increases the bias but decreases the variance.\n\nIf model complexity is too small, we risk underfitting. On the other hand, if the complexity is too large we risk overfitting, We have to find a balance. \n<img src=\"https:\/\/djsaunde.files.wordpress.com\/2017\/07\/bias-variance-tradeoff.png?w=1100\"\/>\n\nThere is no escaping the relationship between bias and variance in machine learning.\n* Increasing the bias will decrease the variance.\n* Increasing the variance will decrease the bias.\n\nThe goal of any supervised machine learning algorithm is to achieve low bias and low variance . In turn the algorithm should achieve good prediction performance.\n\nConsider the bulleye's diagram below :\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/1*k_D4-U7c3Tf8hJRpaOZoBQ.png\" width=\"400px\"\/>\n\nIn the above diagram, center of the target is a model that perfectly predicts correct values. As we move away from the bulls-eye our predictions become get worse and worse. We can repeat our process of model building to get separate hits on the target.\n\nIn supervised learning, underfitting happens when a model unable to capture the underlying pattern of the data. These models usually have high bias and low variance. It happens when we have very less amount of data to build an accurate model or when we try to build a linear model with a nonlinear data. Also, these kind of models are very simple to capture the complex patterns in data like Linear and logistic regression.\n\nIn supervised learning, overfitting happens when our model captures the noise along with the underlying pattern in data. It happens when we train our model a lot over noisy dataset. These models have low bias and high variance. These models are very complex like Decision trees which are prone to overfitting.\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1000\/1*9hPX9pAO3jqLrzt0IE3JzA.png\" width=\"800px\" >\nThere is a trade-off at play between these two concerns and the algorithms you choose and the way you choose to configure them are finding different balances in this trade-off for your problem\n\nIn reality, we cannot calculate the real bias and variance error terms because we do not know the actual underlying target function. Nevertheless, as a framework, bias and variance provide the tools to understand the behavior of machine learning algorithms in the pursuit of predictive performance.\n\n## What is regualrisation ?\nRef : https:\/\/towardsdatascience.com\/regularization-in-machine-learning-76441ddcf99a, https:\/\/codeburst.io\/what-is-regularization-in-machine-learning-aed5a1c36590, http:\/\/www.chioka.in\/differences-between-l1-and-l2-as-loss-function-and-regularization\/ <br>\nWhen the model fits the training data but does not perform well with the test data, we have an overfitting problem. \nRegularisation is a technique used to avoid overfitting. The basic idea behind regularisation is to decrease model complexity in case of overfitting, as we can see in the error vs complexity graph shown above. \n\nIn order to find the best model, the common method in machine learning is to define a loss or cost function that describes how well the model fits the data. The goal is to find the model that minimzes this loss function. This loss function is residual sum of squares (SS\/RSS) shown below ( y is the true value y^ is the predicted value) :\n<img src=\"https:\/\/lh3.googleusercontent.com\/dPeNAkHLy3cFXVziXQFcTJvEp15tQEddgkUjt1aOHkl8WraOD9ejUOhGT4jJQR5gOxSrD6fa1e78KoIyUdUafauI_wp2Y5DIQjo67XanxxEVEoWyLYcUCWJr_qmOuwg_6Q\"  width=\"300px\">\n\nRegularisation imposes a penalty over the loss function so that model doesn't get too complex. \n\n### L1 model : \nL1 model is also called Lasso regression.  Loss function for Lasso regression is :\n<img src=\"https:\/\/www.statisticshowto.datasciencecentral.com\/wp-content\/uploads\/2015\/09\/lasso-regression-300x80.png\">\n\nL1 model adds the magnitude of the coefficients as  penalty.  If lambda = 0 , we get back to  OLS, however if it is too large then the coefficients will be zero, hence underfitting. \n\nThe key difference is that Lasso produces sparse solutions, thereby working as a feature selector. Hence the name, Least Absolute Shrinkage and Selection Operator i.e. LASSO.  It is very useful when we have a lot of features and we want to do some feature selection.\n\n### L2 model : \nL2 model is called Ridge regression.  Loss function for Ridge regression is :\n<img src=\"http:\/\/www.thefactmachine.com\/wp-content\/uploads\/2015\/04\/14-Ridge-EQ1.gif\" >\n\nL2 model adds the sqaure of the coefficients as penalty. If  If lambda = 0 , we get back to  OLS, however if it is too large then the coefficients will be zero, hence underfitting. \n\n### L1 vs L2 :\nRobustness : It is generally defined as the resistance to outliers in the data. Since, L2 takes the square, it is much more sensitive to outliers as compared to L2 that takes the absolute value.\n\nSparsity : It refers to that only very few entries in a matrix (or vector) is non-zero. L1-norm has the property of producing many coefficients with zero values or very small values with few large coefficients.\n\nComputational efficiency : L1-norm does not have an analytical solution, but L2-norm does. This allows the L2-norm solutions to be calculated computationally efficiently. However, L1-norm solutions does have the sparsity properties which allows it to be used along with sparse algorithms, which makes the calculation more computationally efficient.\n\n### Sparsity\nIn a high-dimensional sparse vector, it would be nice to encourage weights to drop to exactly 0 where possible. A weight of exactly 0 essentially removes the corresponding feature from the model. Zeroing out features will save RAM and may reduce noise in the model.\n\nIt would be nice to encourage the weights for the meaningless dimensions to drop to exactly 0, which would allow us to avoid paying for the storage cost of these model coefficients at inference time. \n\nWould L2 regularization accomplish this task? Unfortunately not. L2 regularization encourages weights to be small, but doesn't force them to exactly 0.0.\n\nHowever, there is a regularization term called L1 regularization that  encourages many of the uninformative coefficients in our model to be exactly 0, and thus reap RAM savings at inference time.\n.\nL2 and L1 penalize weights differently:\n\n   * L2 penalizes weight2\n   * L1 penalizes |weight|\n\nConsequently, L2 and L1 have different derivatives:\n\n   * The derivative of L2 is 2 * weight\n   * The derivative of L1 is k (a constant, whose value is independent of weight)\n\nWe can think of the derivative of L2 as a force that removes x% of the weight every time. As we know, even if we remove x percent of a number billions of times, the diminished number will still never quite reach zero.  At any rate, L2 does not normally drive weights to zero.\n\nWe can think of the derivative of L1 as a force that subtracts some constant from the weight every time. However, thanks to absolute values, L1 has a discontinuity at 0, which causes subtraction results that cross 0 to become zeroed out. For example, if subtraction would have forced a weight from +0.1 to -0.2, L1 will set the weight to exactly 0. Eureka, L1 zeroed out the weight.\n\nL1 regularization\u2014penalizing the absolute value of all the weights\u2014turns out to be quite efficient for wide models.\n","05824595":"### Taking care of ordinal values","8626d609":"## Data Visualisation : \n### 1. Bivariate Analysis","3b21081a":"## Import the libraries","8794e4fd":"## L1 model : LASSO","8f065a3e":"## Homoscedasiticity Check :\nAfter normalisation, the columns have been transformed to be homoscadastic to fit our normality assumption.","10746f32":"LotFrontage is linear feet of street connected to property.  LotFrontage in the neighbourhood are likely to be similar, hence taking its median.","30f22236":"### 3. Target variable distribution","33ed3127":"The conical distribution previously seen is now gone. Just by solving normality in some variables, we have achieved homoscadasticity in our dataset.","fa3091ac":"## Regression ???\nRef : https:\/\/www.investopedia.com\/terms\/r\/regression.asp \n\nRegression is a statistical measure that attempts to determine the strength of the relationship between one dependent variable (usually denoted by Y) and a series of other changing variables (known as independent variables). \n\nThe two basic types of regression are linear regression and multiple linear regression, although there are non-linear regression methods for more complicated data and analysis. \n\nLinear regression uses one independent variable to explain or predict the outcome of the dependent variable Y, while multiple regression uses two or more independent variables to predict the outcome.\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/1*_TqRJ9SmwFzRigJhMiN2uw.png\" width=\"400px\">\n\n## Assumptions of Linear Regression\nRef : http:\/\/www.statisticssolutions.com\/assumptions-of-linear-regression\/\n\n1. Linear relationship : The relationship between the independent and dependent variables to be linear.  It is also important to check for outliers since linear regression is sensitive to outlier effects.  The linearity assumption can best be tested with scatter plots.\n\n2. Multivariate normality : The linear regression analysis requires all variables to be multivariate normal. This assumption can best be checked with a histogram or a Q-Q-Plot. When the data is not normally distributed a non-linear transformation (e.g., log-transformation) might fix this issue\n\n3. Multicollinearity : Linear regression assumes that there is little or no multicollinearity in the data.  Multicollinearity occurs when the independent variables are too highly correlated with each other.\n\n4. Auto-correlation : Linear regression analysis requires that there is little or no autocorrelation in the data.  Autocorrelation occurs when the residuals are not independent from each other.  In other words when the value of y(x+1) is not independent from the value of y(x).\n\n5. Homoscedasticity : Homoscedastcity meaning the residuals are equal across the regression line. The scatter plot is good way to check whether the data are homoscedastic. ","16500c39":"Since, absent value indicate there is probably no garage. Thus,  filling empty GarageArea values with the zeroes.","a62935d3":"## Load the dataset","63716bea":"NaN values likely indicate absence of basement half bathrooms. ","169384d1":"Consider only highly skewed values.","85776216":"Filling empty TotalBsmtSF with the zeroes, since there is probably no basement.","07567de2":"Cross Validation strategy : So, we are doing a 10-fold cross validation, splitting the data and shuffling it with KFold. ","b7a88920":"Assuming there's no masonry veneer, if the value is absent.","f0f37393":"### Imputing missing values\n\n<br>As given in the dataset, absent values in PoolQC, MiscFeature, Fence, Alley or Fireplace indicate the absence of the feature itself. Hence it would mean that there is no Pool, MiscFeature in the house.","a1a98445":"Since, most values are same. We are dropping this column.","29648847":"## R-Squared: \nR-Squared is a method to determine the goodness of fit of our model. It is the proportion of the variance in the dependent variable that is predictable from the independent variable(s). In other words, it tells how much variance in target is explained by the features.\n\nIf the mean or average line is denoted by :\n<img src=\"https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/44a8b57e2a4335f02faa2bd5003d94979af4f408\">\n\nwhere sum of square of residuals is the difference between true values and predicted ones:\n\n<img src=\"https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/2669c9340581d55b274d3b8ea67a7deb2225510b\">\n\nand total sum of squares is the difference between the true values and the average line:\n\n<img src=\"https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/aec2d91094ee54fbf0f7912d329706ff016ec1bd\">\n\nR-Squared kinda tells us how well our line fits with the model as compared to the average line. If the residual sum of squares is close to zero, R-Sq leads to 1; indicating a very good fit. \n\n<img src=\"https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/0ab5cc13b206a34cc713e153b192f93b685fa875\">\n","ac7be275":"If the value is absent, there is no masonry veneer, hence the area is zero.","99dedbb5":"#### We can clearly see that the distribution is positively skewed. We would have to normalise it before using it in the model.","01439dd4":"Filling GarageYrBuilt as 0 since, the garage is not there.","c84697fa":"As given in the dataset, NaN values indicate absence of the feature i.e No Basement.","6a12128c":"## Skewed features\nNow, taking care of the skewed features","b96dc99a":"### 2. Heatmap","dcfe80c2":"Filling Electrical with the most frequent value, since it's categorical.","2645389e":"## L2 model : Ridge"}}