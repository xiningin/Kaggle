{"cell_type":{"42f516fb":"code","3e917147":"code","853f2e8a":"code","c9e59f3b":"code","902be2f7":"code","49f2360c":"code","ada39e10":"code","fd50bcab":"code","930bdd8b":"code","afba7422":"code","89962e17":"code","bc34f66a":"code","22d87880":"code","d7cbaf80":"code","1da69fa6":"code","f4b572a5":"code","d9fa2379":"code","46443740":"code","dd589d75":"code","3f1e0d7f":"code","8b6c41d9":"code","ea4a452d":"code","b867109c":"code","e852688b":"code","4a340456":"code","5cabc833":"code","c1d5a2aa":"code","d1a32cc7":"code","9b534e65":"code","3e8f662c":"code","852dfdb9":"code","a2f580fc":"code","f1c992d8":"code","17842daf":"code","186757db":"code","da038d7d":"code","c80c648c":"code","b59588e0":"code","b77eb6c1":"code","5f61df4f":"code","842d01cd":"code","55c24765":"code","098f9e7c":"code","15493dfe":"code","20558eca":"code","1244acf2":"code","fcc9778e":"code","1b4c864d":"code","2b713d5d":"markdown","1b2c1c29":"markdown","7448a0c8":"markdown","6669f1fd":"markdown","ce82c62b":"markdown"},"source":{"42f516fb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3e917147":"tweets = pd.read_csv('\/kaggle\/input\/trainings\/narendramodi_tweets.csv')\ntweets.shape","853f2e8a":"tweets.head()","c9e59f3b":"# frequency analysis are used for categorical variable (boxplot,treemaps)\n# numerical columns- 5 point summary( distribution)  ( boxplot,histogram)\n# date - trend analysis(line,calender)\n# location columns - distribution by region(geographical analysis)(map)\n# text data -bag of word analysis( word cloud)","902be2f7":"docs = tweets['text']\ndocs.head()","49f2360c":"from nltk import word_tokenize\nwords = []\nfor i in tweets['text'].str.lower().str.replace('[^a-z\\s#@]',''):\n    words.extend(word_tokenize(i))","ada39e10":"len(words)","fd50bcab":"docs = tweets['text'].str.lower().str.replace('[^a-z\\s#@]','')  # remove everything other than alphabet,spaces,#,@  # converting the to lower case\ndoc_token = docs.str.split(' ')","930bdd8b":"token_all = []\nfor tokens in doc_token:\n    token_all.extend(tokens)\n\nprint('No of token in entire corpus: ' ,len(token_all))","afba7422":"token_freq =  pd.Series(token_all).value_counts().drop([''])\ntoken_freq  # to,the,of are stop word or common words","89962e17":"import nltk # natural language tool kit\nnltk.download('stopwords')  # it will download the stop word and need to done once","bc34f66a":"common_stopwords = nltk.corpus.stopwords.words('english')\ncommon_stopwords  # just list of common word","22d87880":"custom_stopwords = ['amp','rt']\nall_stopwords = np.hstack([common_stopwords,custom_stopwords])\nlen(all_stopwords)","d7cbaf80":"df_tokens = pd.DataFrame(token_freq).reset_index().rename(columns={'index':'token',0:'frequency'})","1da69fa6":"df_tokens = df_tokens[~df_tokens['token'].isin(custom_stopwords)]\ndf_tokens\n","f4b572a5":"df_tokens.set_index('token')['frequency'].head(25).plot.bar()","d9fa2379":"doc1 = 'i love india'\ndoc2 = 'i love cricket'\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt","46443740":"docs_string = ' '.join([doc1,doc2])\nwc =  WordCloud(background_color='white').generate(docs_string)\nplt.imshow(wc) # word cloud can only take single doc we need to use join all the doc","dd589d75":"docs_strings = ' '.join(docs)\nlen(docs_strings)\nwc2 =  WordCloud(background_color='white').generate(docs_strings)\nplt.figure(figsize=[14,8])\nplt.imshow(wc2)\nplt.axis('off')","3f1e0d7f":"hashtags = df_tokens[df_tokens['token'].str.startswith('#')]\nplt.figure(figsize=[14,8])\nhashtags.set_index('token')['frequency'].head(25).plot.bar()                   ","8b6c41d9":"tweets['created_at'] = pd.to_datetime(tweets['created_at'],format='%Y-%m-%d %H:%M:%S')\ntweets['year_month'] = tweets['created_at'].dt.strftime('%Y-%m')\ntweets['created_at'].describe()","ea4a452d":"hashtag_col = 'digital'\ntweets[hashtag_col] = tweets['text'].str.lower().str.contains(hashtag_col)\nhashtag_month_count = tweets.groupby(['year_month'])[hashtag_col].sum()\nhashtag_month_count","b867109c":"hashtag_month_count.plot.line()","e852688b":"for year_month in tweets['year_month'].unique():\n    sub_data = tweets[tweets['year_month'] == year_month]\n    docs = sub_data['text'].str.lower().str.replace('[^a-z\\s#@]', '')\n    docs_strings = ' '.join(docs)\n    len(docs_strings)\n    wc = WordCloud(background_color='white', stopwords=all_stopwords).generate(docs_strings)\n    plt.figure(figsize=(20,5))\n    plt.imshow(wc)\n    plt.title(year_month)\n    plt.axis('off');  ","4a340456":"imbd = pd.read_csv('\/kaggle\/input\/trainings\/IMDB Dataset.csv').sample(1000)\nprint(imbd.shape)\nimbd.head()","5cabc833":"docs = imbd['review'].str.lower().str.replace('[^a-z\\s#@]','') ","c1d5a2aa":"from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.model_selection import train_test_split","d1a32cc7":"train_doc,test_doc = train_test_split(pd.Series(docs),test_size=0.2,random_state=1)","9b534e65":"stopwords = nltk.corpus.stopwords.words('english')\nstopwords.remove('not')  # custom stopwords with no not removed\nvectorizer = CountVectorizer(stop_words=stopwords,min_df=10).fit(train_doc)  # this will get the unique tokens from the reviews and stop_words will remove the stopword and 5 as min doc it should appear\nvocab = vectorizer.get_feature_names()\nlen(vocab)  # this ungram token is found","3e8f662c":"train_dtm = vectorizer.transform(train_doc)\ntest_dtm = vectorizer.transform(test_doc)","852dfdb9":"train_dtm # sparse matrix is a matrix which most of the value is zeros.","a2f580fc":"df_train_dtm = pd.DataFrame(train_dtm.toarray(),index=train_doc.index,columns=vocab)\ndf_test_dtm = pd.DataFrame(test_dtm.toarray(),index=test_doc.index,columns=vocab)","f1c992d8":"from nltk.stem import PorterStemmer","17842daf":"stemmer = PorterStemmer()\nfor word in ['looking','looks','looked']:\n    \n    print(stemmer.stem(word))","186757db":"stemmer.stem('president')","da038d7d":"stemmer.stem('organization') # cannot take document at a time","c80c648c":"from gensim.parsing.preprocessing import PorterStemmer,remove_stopwords","b59588e0":"stemmer = PorterStemmer()\nstemmer.stem_documents(['this movie is good','this movie is really pathetic','awesome movie'])","b77eb6c1":"remove_stopwords('this movie is really pathetic') # can take one docu at a time","5f61df4f":"docs = imbd['review'].str.lower().str.replace('[^a-z\\s#@]','') \ndocs = docs.apply(remove_stopwords)\ndocs = stemmer.stem_documents(docs)","842d01cd":"train_doc,test_doc = train_test_split(pd.Series(docs),test_size=0.2,random_state=1)","55c24765":"vectorizer = CountVectorizer(min_df=10).fit(train_doc) \nvocab = vectorizer.get_feature_names()\nlen(vocab)","098f9e7c":"train_dtm = vectorizer.transform(train_doc)\ntest_dtm = vectorizer.transform(test_doc)","15493dfe":"df_train_dtm = pd.DataFrame(train_dtm.toarray(),index=train_doc.index,columns=vocab)\ndf_test_dtm = pd.DataFrame(test_dtm.toarray(),index=test_doc.index,columns=vocab)","20558eca":"df_train_dtm.sum().sort_values(ascending=False).head(25)","1244acf2":"vectorizer = TfidfVectorizer(min_df=5).fit(train_doc) \nvocab = vectorizer.get_feature_names()\ntrain_dtm_tfidf = vectorizer.transform(train_doc)\ntest_dtm_tfidf = vectorizer.transform(test_doc)\ndf_train_dtm_tfidf = pd.DataFrame(train_dtm_tfidf.toarray(),index=train_doc.index,columns=vocab)\ndf_train_dtm_tfidf.head()","fcc9778e":"vectorizer = CountVectorizer(min_df=5,ngram_range=(3,3)).fit(train_doc) \nvocab = vectorizer.get_feature_names()\nvocab[:10]  ","1b4c864d":"vectorizer = CountVectorizer(min_df=5,ngram_range=(1,3)).fit(train_doc) \nvocab = vectorizer.get_feature_names()\nvocab[:100]  # if give 1,3 will get bigram and terigram","2b713d5d":"## Bag of words analysis","1b2c1c29":"## Wordclouds","7448a0c8":"## Hashtag Analysis","6669f1fd":"## Stemming","ce82c62b":"## Vectorization - Example 2 IMBD"}}