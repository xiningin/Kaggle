{"cell_type":{"2ac1f128":"code","33fc1510":"code","a2dc4c74":"code","e7e89cbb":"code","488f0073":"code","cfaa7d2b":"code","fa9daa74":"code","f5621b16":"code","158dd168":"code","fea2b353":"code","3a8fef14":"code","d697f327":"code","a4547880":"code","8eeaa5db":"code","1a9c3ef4":"code","6ab15eea":"code","d3a0f66d":"code","5b9c3808":"code","2453b6d9":"code","557fcfdc":"markdown","e7ee98e3":"markdown","a2e9ea3a":"markdown","8293bdec":"markdown","8ff4323e":"markdown","ca54926f":"markdown"},"source":{"2ac1f128":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\nimport re\nimport string\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import regularizers\n\nsns.set_style('darkgrid')","33fc1510":"df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf2 = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","a2dc4c74":"df.head(5)","e7e89cbb":"df.info()","488f0073":"# Countplot of target column\nplt.figure(figsize=(12,6))\nsns.countplot(x = 'target', data = df, palette=\"Set2\")","cfaa7d2b":"# Check the keywords and their appearances, taking into account the target\nfig = plt.figure(figsize=(8, 72), dpi=100)\n\nsns.countplot(y=df['keyword'], hue=df['target'], palette=\"Set2\")","fa9daa74":"# Add new column 'text_len': Text length\ndf['text_len'] = df['text'].map(len)\ndf.head(3)","f5621b16":"# Histogram of text length for target = 0\nplt.figure(figsize=(12,6))\n\ng = sns.histplot(data = df[df['target'] == 0], \n             x = 'text_len', kde = True, palette='Set2').set_title(\"Text length histogram with marked mean for target 0\")\n\ng = plt.axvline(df[df['target'] == 0].text_len.mean(), \n                color='r', linestyle = '--')","158dd168":"# Histogram of text length for target 1\nplt.figure(figsize=(12,6))\n\ng = sns.histplot(data = df[df['target'] == 1], \n             x = 'text_len', kde = True, color='#fc8d62').set_title(\"Text length histogram with marked mean for target 1\")\n\ng = plt.axvline(df[df['target'] == 1].text_len.mean(), \n                color='r', linestyle = '--')","fea2b353":"# List of stopwords\nstopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]","3a8fef14":"# Define functions which will help clean the text\n\n####################################################################################\ndef remove_url(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\n####################################################################################\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n####################################################################################\ndef remove_html(text):\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    return re.sub(html, '', text)\n\n####################################################################################\n# Main func\ndef clean_sentences(sentences):\n    '''\n    Make text lowercase, remove text in square brackets, \n    remove punctuation, remove words containing numbers,\n    remove url, html, and emoji.\n    '''\n    cln_sentences = []\n    for text in sentences:\n      text = text.lower()\n      text = re.sub('\\[.*?\\]', '', text)\n      text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n      text = re.sub('\\w*\\d\\w*', '', text)\n      ###\n      text = remove_url(text)\n      text = remove_html(text)\n      text = remove_emoji(text)\n      ###\n      cln_sentences.append(text)\n    return cln_sentences","d697f327":"### Prepare data\n\n# Train \ntrain_sentences = []\ntrain_labels = []\n\n# append all text and label (target) to list\nfor ind, row in df.iterrows():\n  train_labels.append(row['target'])\n\n  sentence = row['text']\n  # remove stopwords in sentences\n  for word in stopwords:\n      token = \" \" + word + \" \"\n      sentence = sentence.replace(token, \" \")\n      sentence = sentence.replace(\"  \", \" \")\n  train_sentences.append(sentence)\n\n# Test\ntest_sentences = []\n\n# append all text and label (target) to list\nfor ind, row in df2.iterrows():\n  sentence = row['text']\n  # remove stopwords in sentences\n  for word in stopwords:\n      token = \" \" + word + \" \"\n      sentence = sentence.replace(token, \" \")\n      sentence = sentence.replace(\"  \", \" \")\n  test_sentences.append(sentence)\n\n\n## Use clean_sentences func\ntrain_sentences = clean_sentences(train_sentences)\ntest_sentences = clean_sentences(test_sentences)","a4547880":"# Set parameters\nvocab_size = 20000    # Max len of unique words\nembedding_dim = 100   # Embedding dimension value\nmax_length = 110      # Max length of sentence\npadding_type = 'post' # pad_sequences arg\ntrunc_type = 'post'   # trunc post\noov_tok = '<OOV>'     # Unknow words = <OOV>","8eeaa5db":"# Tokenizing and padding\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train_sentences)\nword_index = tokenizer.word_index\n\ntrain_sequences = tokenizer.texts_to_sequences(train_sentences)\ntrain_padded = pad_sequences(train_sequences, padding=padding_type, truncating=trunc_type, maxlen=max_length)\n\nvalidation_sequences = tokenizer.texts_to_sequences(test_sentences)\nvalidation_padded = pad_sequences(validation_sequences, padding=padding_type, truncating=trunc_type, maxlen=max_length)","1a9c3ef4":"# Prepare labels for model\ntraining_labels_final = np.array(train_labels)\n\n# Check shapes\nprint(training_labels_final.shape)","6ab15eea":"# GloVe\n# Note this is the 100 dimension version of GloVe from Stanford\nembeddings_index = {};\nwith open('..\/input\/glove6b100dtxt\/glove.6B.100d.txt') as f:\n    for line in f:\n        values = line.split();\n        word = values[0];\n        coefs = np.asarray(values[1:], dtype='float32');\n        embeddings_index[word] = coefs;\n\nembeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word);\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector;","d3a0f66d":"# Create a Model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n    tf.keras.layers.Conv1D(128, 2, activation='relu'),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(40, activation = 'relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(20, activation = 'relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(10, activation = 'relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(5, activation = 'relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\n\nnum_epochs = 10\n\nhistory = model.fit(train_padded, training_labels_final, epochs=num_epochs, verbose=2)\n\nprint(\"Training Complete\")","5b9c3808":"# Make predictions on test data\npred = model.predict_classes(validation_padded)","2453b6d9":"# export predictions to csv file. \nresults = pd.DataFrame(data = pred, columns = ['target'])\nresults = results.set_index(df2.id)\nresults.to_csv(\".\/results.csv\", index = True, sep=',')","557fcfdc":"# Contents\n\n[1. Imports](#1)\n<br><\/br>\n[2. EDA](#2)\n<br><\/br>\n[3. Data PreProcessing](#3)\n<br><\/br>\n[4. Creating and Trening a Model](#4)\n<br><\/br>\n[5. Make and Export Predictions](#5)","e7ee98e3":"<span id = \"5\"><\/span>\n# Make and Export Predictions","a2e9ea3a":"<span id = \"3\"><\/span>\n# Data PreProcessing","8293bdec":"<span id = \"2\"><\/span>\n# EDA","8ff4323e":"<span id = \"1\"><\/span>\n# Imports","ca54926f":"<span id = \"4\"><\/span>\n# Creating and Trening a Model"}}