{"cell_type":{"419c8180":"code","a5ee6cf1":"code","54a5e2fe":"code","5f4ce552":"code","785afbb8":"code","227e355a":"code","abf00685":"code","1a417801":"code","7d7c67e9":"code","08f56f17":"code","134fa31c":"code","4c4f2665":"code","8ddcaafb":"code","b76902b5":"code","dbe95621":"code","1fd7e512":"code","9e987886":"code","6c8e6dbe":"code","123116f6":"code","a7f58b06":"code","95f92de8":"code","5aa83727":"code","4eb4ad2c":"markdown","400f73d1":"markdown","a44109b1":"markdown","c868b7ff":"markdown","c235dc98":"markdown","e6a279c0":"markdown","11c75f9b":"markdown","2728667c":"markdown","4cf1f80f":"markdown","a57a34ba":"markdown","45969179":"markdown","22d9e9ff":"markdown","25853e6e":"markdown","bdbffcf7":"markdown","89249e99":"markdown","73cce53d":"markdown"},"source":{"419c8180":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)","a5ee6cf1":"%%time\n# credit to https:\/\/www.kaggle.com\/julian3833\/1-quick-start-read-csv-and-flatten-json-fields\/notebook\ndef load_df(csv_path='..\/input\/ga-customer-revenue-prediction\/train.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    json = __import__('json')\n    json_normalize = pd.io.json.json_normalize\n    df = pd.read_csv(\n        csv_path,\n        converters={ column: json.loads for column in JSON_COLUMNS },\n        dtype={ 'fullVisitorId': 'str', 'visitId': 'str' },\n        nrows=nrows\n    )\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f'{column}.{subcolumn}' for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    print(f'Loaded {csv_path}. Shape: {df.shape}')\n    return df\n\n# df_train = load_df()\n# df_test = load_df('..\/input\/ga-customer-revenue-prediction\/test.csv')","54a5e2fe":"# %%time\n# df_train.to_csv('train-flattened.csv', index=False)\n# df_test.to_csv('test-flattened.csv', index=False)","5f4ce552":"%%time\n# these have been saved earlier\ndf_train = pd.read_csv('..\/input\/ga-store-customer-revenue\/train-flattened.csv', dtype={ 'fullVisitorId': str, 'visitId': str, 'trafficSource.campaignCode': str },)\ndf_test = pd.read_csv('..\/input\/ga-store-customer-revenue\/test-flattened.csv', dtype={ 'fullVisitorId': str, 'visitId': str },)","785afbb8":"df_train['totals.transactionRevenue'] = df_train['totals.transactionRevenue'].astype(float)\ndf_train['totals.transactionRevenue'].notnull().sum()\nnp.sum(df_train.groupby('fullVisitorId')['totals.transactionRevenue'].sum() > 0)\ndf_train['totals.transactionRevenue'].sum() \/ 1000000\n(df_train['totals.transactionRevenue'][df_train['totals.transactionRevenue'] > 0] \/ 1000000).agg(['min', 'max', 'mean', 'median'])","227e355a":"columns = set(df_train.columns).intersection((set(df_test.columns))) # columns present in both train and test\nprint('Predictors that can be removed:', end=' ')\nfor i in df_train.columns[df_train.nunique(dropna=False) == 1].tolist() + df_test.columns[df_test.nunique(dropna=False) == 1].tolist():\n    if (i in columns):\n        print(i, end=', ')\n        columns.remove(i)","abf00685":"def count(var, p=True):\n    tmp = pd.concat([\n        df_train[var].value_counts(dropna=False),\n        df_train[df_train['totals.transactionRevenue'] > 0][var].value_counts(dropna=False),\n        df_test[var].value_counts(dropna=False),\n    ], keys=['train', 'train with revenue', 'test'], axis=1, sort=False)\n    tmp['train %'] = np.round(tmp['train'] \/ tmp['train'].sum() * 100, 1)\n    tmp['train with revenue %'] = np.round(tmp['train with revenue'] \/ tmp['train with revenue'].sum() * 100, 1)\n    tmp['test %'] = np.round(tmp['test'] \/ tmp['test'].sum() * 100, 1)\n    if p: print(tmp.shape)\n    return(tmp)","1a417801":"count('channelGrouping')","7d7c67e9":"count('device.browser')\ncount('device.deviceCategory')\ncount('device.operatingSystem')","08f56f17":"count('geoNetwork.continent')\ncount('geoNetwork.subContinent')\ncount('geoNetwork.country')\ncount('geoNetwork.region')\ncount('geoNetwork.city')\ncount('geoNetwork.metro')\ncount('geoNetwork.networkDomain')","134fa31c":"count('trafficSource.adContent')\ncount('trafficSource.adwordsClickInfo.adNetworkType')\ncount('trafficSource.adwordsClickInfo.gclId')\ncount('trafficSource.adwordsClickInfo.isVideoAd')\ncount('trafficSource.adwordsClickInfo.page')\ncount('trafficSource.adwordsClickInfo.slot')\ncount('trafficSource.campaign')\ncount('trafficSource.isTrueDirect')\ncount('trafficSource.keyword')\ncount('trafficSource.medium')\ncount('trafficSource.referralPath')\ncount('trafficSource.source')","4c4f2665":"count('totals.bounces')\ncount('totals.hits')\ncount('totals.pageviews')\ncount('totals.newVisits')","8ddcaafb":"for i in ['sessionId', 'visitId', 'fullVisitorId']:\n    print(i, 'Nunique:', df_train[i].nunique(), '. Null:', df_train[i].isnull().sum())\n    print('Common between train and test set:', len(set(df_train[i]).intersection(set(df_test[i]))))\n\n# fullVisitorId present in both train and test set\ntmp = list(set(df_train['fullVisitorId']).intersection(set(df_test['fullVisitorId'])))\n# the revenue of these fullVisitorId\ntmp = df_train[df_train['fullVisitorId'].isin(tmp)].groupby('fullVisitorId')['totals.transactionRevenue'].sum()\nprint('Number of fullVisitorId who spent money:', len(tmp[tmp > 0]), '. Total amount spent:', tmp.sum() \/ 1e6)\n\n# fullVisitorId only present in train\ntmp = list(set(df_train['fullVisitorId']).difference(set(df_test['fullVisitorId'])))\n# the revenue of these fullVisitorId\ntmp = df_train[df_train['fullVisitorId'].isin(tmp)].groupby('fullVisitorId')['totals.transactionRevenue'].sum()\nprint('Number of fullVisitorId only in the train set:', len(tmp), '. Number who spent money:', len(tmp[tmp > 0]), '. Total amount spent:', tmp.sum() \/ 1e6)","b76902b5":"tmp = df_train.groupby('fullVisitorId')['totals.transactionRevenue'].agg(['size', 'count', 'sum']).groupby('size').agg(['size', 'sum'])[[['count', 'size'], ['count', 'sum'], ['sum', 'sum']]]\ntmp.columns = ['size', 'count', 'revenue']\ntmp['revenue'] = tmp['revenue'] \/ 1e6\ntmp['size %'] = np.round(tmp['size'] \/ tmp['size'].sum() * 100, 2)\ntmp['count \/ size %'] = np.round(tmp['count'] \/ tmp['size'] * 100, 2)\ntmp['revenue %'] = np.round(tmp['revenue'] \/ tmp['revenue'].sum() * 100, 2)\ntmp","dbe95621":"plt.plot(tmp.index, tmp['count \/ size %']);\nplt.xlabel('Frequency of fullVisitorId')\nplt.ylabel('Percentage of number of transaction with revenue\\nover number of fullVisitorId')\nplt.xlim(0, 50)\nplt.ylim(0, 500)","1fd7e512":"tmp_date = pd.to_datetime(df_train['date'], format='%Y%m%d')\ntmp_visitStartTime = pd.to_datetime(df_train['visitStartTime'], unit='s')\n(tmp_date.dt.date - tmp_visitStartTime.dt.date).value_counts() # this shows most date and visitStartTime are the same\n\n# new columns\ndf_train['visitStartTime.month'] = tmp_visitStartTime.dt.month\ndf_train['visitStartTime.week'] = tmp_visitStartTime.dt.week\ndf_train['visitStartTime.day'] = tmp_visitStartTime.dt.day\ndf_train['visitStartTime.weekday'] = tmp_visitStartTime.dt.weekday\ndf_train['visitStartTime.hour'] = tmp_visitStartTime.dt.hour\n\n# do the same for test_df\ntmp_date = pd.to_datetime(df_test['date'], format='%Y%m%d')\ntmp_visitStartTime = pd.to_datetime(df_test['visitStartTime'], unit='s')\n(tmp_date.dt.date - tmp_visitStartTime.dt.date).value_counts() # this shows most date and visitStartTime are the same\n\ndf_test['visitStartTime.month'] = tmp_visitStartTime.dt.month\ndf_test['visitStartTime.week'] = tmp_visitStartTime.dt.week\ndf_test['visitStartTime.day'] = tmp_visitStartTime.dt.day\ndf_test['visitStartTime.weekday'] = tmp_visitStartTime.dt.weekday\ndf_test['visitStartTime.hour'] = tmp_visitStartTime.dt.hour","9e987886":"def make_button(var, title = '', max_row = 10, df = df_train, target_var = 'totals.transactionRevenue'):\n    # create data for the 'updatemenus' used by plotly\n    # agg data from var ~ target_vaar into size, count, mean, median\n    # return dict()\n    tmp = df[[var, target_var]].fillna(value={ var: -1 }).groupby(var)[target_var].agg(['size', 'sum', 'mean', 'median']) # use fillna for var, as groupby(var) doesn't work with na\n    tmp = tmp.sort_values('size', ascending=False)[:max_row][::-1] # by defaul, take only the top 10 rows ordered by size\n    tmp = {\n        'x': [tmp['size'].values, tmp['sum'].values , tmp['mean'].values, tmp['median'].values],\n        'y': [[str(i) for i in tmp.index.tolist()]] * 4, # str(i) to convert all to string, because some of the indexes are True, False\n    }\n    title = title or var\n    return dict(args=[tmp, { 'title': title }], label=title, method='update') \n\n# plotting\n## data\ntmp = make_button('device.deviceCategory', 'Device Category')\nx = tmp['args'][0]['x']\ny = tmp['args'][0]['y'][0]\n\n## trace\ntraces = [None] * 4\ntraces[0] = (go.Bar(x=x[0], y=y, orientation='h'))\ntraces[1] = (go.Bar(x=x[1], y=y, orientation='h'))\ntraces[2] = (go.Bar(x=x[3], y=y, orientation='h', name='Median')) # median goes first, the resulting bar graph will place median at the bottom\ntraces[3] = (go.Bar(x=x[2], y=y, orientation='h', name='Mean'))\n\n## fig, subplot\nfig = __import__('plotly').tools.make_subplots(1, 3, subplot_titles=['Number of record', 'Total revenue', 'Mean & Median'])\nfor i in range(3): fig.append_trace(traces[i], 1, i + 1)\nfig.append_trace(traces[-1], 1, 3)\n\n## fig, layout\nfig.layout.title = tmp['args'][1]['title']\nfig.layout.showlegend = False\nfig.layout.updatemenus = list([\n    dict(\n        buttons=[make_button(i) for i in [\n            'device.deviceCategory', 'device.operatingSystem', 'device.browser', 'device.isMobile',\n            'geoNetwork.continent', 'geoNetwork.subContinent', 'geoNetwork.country', 'geoNetwork.region', 'geoNetwork.metro', 'geoNetwork.city', 'geoNetwork.networkDomain',\n            'trafficSource.medium', 'trafficSource.campaign', 'trafficSource.isTrueDirect', 'trafficSource.adContent', 'trafficSource.adwordsClickInfo.adNetworkType',\n        ]] + [make_button(i, max_row=31) for i in [\n            'totals.bounces', 'totals.newVisits', 'totals.hits', 'totals.pageviews',\n            'visitStartTime.month', 'visitStartTime.week', 'visitStartTime.day', 'visitStartTime.weekday', 'visitStartTime.hour'\n        ]],\n        direction = 'down',\n        showactive = True,\n        x = 0,\n        xanchor = 'left',\n        y = 1.25,\n        yanchor = 'top' \n    ),\n])\n\n## now plot\npy.iplot(fig)\n\n# clean up for memory\n%xdel make_button\n%xdel tmp\n%xdel traces\n%xdel fig","6c8e6dbe":"# Convert target `totals.transactionRevenue` with log1p\ndf_train['totals.transactionRevenue'].fillna(0, inplace=True)\ny = np.log1p(df_train['totals.transactionRevenue'])\nplt.hist(y[y > 0], bins=50);","123116f6":"# fillna\nprint('Number of records which are null in the train and test set')\npd.concat([df_train[list(columns)].isnull().sum(0), df_test[list(columns)].isnull().sum(0)], keys=['train', 'test'], axis=1).sort_index()\n\n# fillna with 0\nfor i in ['totals.bounces', 'totals.newVisits', 'totals.pageviews', 'trafficSource.adwordsClickInfo.page']:\n    df_train[i].fillna(0, inplace=True)\n    df_test[i].fillna(0, inplace=True)\n\n# fillna with the word \"NONE\"\nfor i in ['trafficSource.adContent', 'trafficSource.adwordsClickInfo.adNetworkType', 'trafficSource.adwordsClickInfo.gclId', 'trafficSource.adwordsClickInfo.slot', 'trafficSource.keyword', 'trafficSource.referralPath']:\n    df_train[i].fillna('NONE', inplace=True)\n    df_test[i].fillna('NONE', inplace=True)\n\n# change True, False to 1, 0\ndf_train['trafficSource.adwordsClickInfo.isVideoAd'].replace({ np.nan: 1, False: 0 }, inplace=True); df_test['trafficSource.adwordsClickInfo.isVideoAd'].replace({ np.nan: 1, False: 0 }, inplace=True)\ndf_train['trafficSource.isTrueDirect'].replace({ np.nan: 0, True: 1 }, inplace=True); df_test['trafficSource.isTrueDirect'].replace({ np.nan: 0, True: 1 }, inplace=True)\ndf_train['device.isMobile'].replace({ True: 1, False: 0 }, inplace=True); df_test['device.isMobile'].replace({ True: 1, False: 0 }, inplace=True)\n\nprint('Number of records which are null in the train and test set after fillna')\npd.concat([df_train[list(columns)].isnull().sum(0), df_test[list(columns)].isnull().sum(0)], keys=['train', 'test'], axis=1).sum()","a7f58b06":"%%time\nfrom scipy import sparse\nfrom sklearn.preprocessing import OneHotEncoder\n\ndef encode_oh(var, threshold = 0.0005, ohe = OneHotEncoder(handle_unknown='ignore')):\n    levels = df_train[var].value_counts()\n    levels = levels[levels \/ len(df_train) > 0.0005].index.values.reshape(-1, 1) # need to use the threshold, otherwise the dimensions explode and eat up all the memory\n    ohe.fit(levels)\n    train = ohe.transform(df_train[var].values.reshape(-1, 1))\n    test = ohe.transform(df_test[var].values.reshape(-1, 1))\n    feature_names = [var + '__' + i[3:] for i in ohe.get_feature_names()]\n    return train, test, feature_names\n\n# LabelBinarizer\n# def encode_lb(var, threshold = 0.0001, array = None):\n#     lb = LabelEncoder()\n#     # var, the var to be encoded    \n#     # thresold, default at 0.0001 or 0.01%. Keep columns where at least 0.01% of rows are not 0\n#     # array, prodive an array of categories to be fitted\n#     lb.fit(array or df_train[var])\n#     train = lb.transform(df_train[var])\n#     test = lb.transform(df_test[var])\n#     columns = [var + '__' + str(i) for i in lb.classes_]\n#     columns_to_keep = train.sum(0) \/ train.sum() > thresold\n#     train = train[:, columns_to_keep]\n#     test = test[:, columns_to_keep]\n#     columns = np.array(columns)[columns_to_keep]    \n#     return train, test, columns\n    \n# predictors\n\n# categorical variables + numeric variables\ntmp = [encode_oh(i) for i in [\n    'channelGrouping',\n    'device.browser', 'device.deviceCategory', 'device.operatingSystem', 'geoNetwork.city', 'geoNetwork.continent', 'geoNetwork.country', 'geoNetwork.metro', 'geoNetwork.networkDomain', 'geoNetwork.region', 'geoNetwork.subContinent',\n    'trafficSource.adContent', 'trafficSource.adwordsClickInfo.adNetworkType', 'trafficSource.adwordsClickInfo.gclId', 'trafficSource.adwordsClickInfo.slot', 'trafficSource.campaign', 'trafficSource.keyword', 'trafficSource.medium', 'trafficSource.referralPath', 'trafficSource.source',\n]] + [(sparse.csr_matrix(df_train[i].values.reshape(-1, 1)), sparse.csr_matrix(df_test[i].values.reshape(-1, 1)), [i]) for i in [\n    'device.isMobile', 'totals.bounces', 'totals.hits', 'totals.newVisits', 'totals.pageviews', 'trafficSource.adwordsClickInfo.isVideoAd', 'trafficSource.adwordsClickInfo.page', 'trafficSource.isTrueDirect', 'visitNumber', 'visitStartTime.month', 'visitStartTime.week', 'visitStartTime.day', 'visitStartTime.weekday', 'visitStartTime.hour'\n]]\n\nX = sparse.hstack([i[0] for i in tmp]).tocsr()\nX_test = sparse.hstack([i[1] for i in tmp])\ncolumns = np.concatenate([i[2] for i in tmp])\n\n# # free up memory\n%xdel sparse\n%xdel encode_oh\n%xdel tmp","95f92de8":"%%time\nfrom sklearn.preprocessing import LabelEncoder\ntmp_cats = [\n    'channelGrouping',\n    'device.browser', 'device.deviceCategory', 'device.operatingSystem', 'geoNetwork.city', 'geoNetwork.continent', 'geoNetwork.country', 'geoNetwork.metro', 'geoNetwork.networkDomain', 'geoNetwork.region', 'geoNetwork.subContinent',\n    'trafficSource.adContent', 'trafficSource.adwordsClickInfo.adNetworkType', 'trafficSource.adwordsClickInfo.gclId', 'trafficSource.adwordsClickInfo.slot', 'trafficSource.campaign', 'trafficSource.keyword', 'trafficSource.medium', 'trafficSource.referralPath', 'trafficSource.source',\n    'sessionId', 'visitId', 'fullVisitorId', # fullVisitorId could be useful\n]\ntmp_nums = ['device.isMobile', 'totals.bounces', 'totals.hits', 'totals.newVisits', 'totals.pageviews', 'trafficSource.adwordsClickInfo.isVideoAd', 'trafficSource.adwordsClickInfo.page', 'trafficSource.isTrueDirect', 'visitNumber', 'visitStartTime', 'visitStartTime.month', 'visitStartTime.week', 'visitStartTime.day', 'visitStartTime.weekday', 'visitStartTime.hour']\n\n# Xl: X using labelEncoder\nXl = df_train[tmp_cats + tmp_nums].copy()\nXl_test = df_test[tmp_cats + tmp_nums].copy()\n\nl = LabelEncoder()\nprint('Processing:', end=' ')\nfor i in tmp_cats:\n    print(i, end=', ')\n    # because labelencoder cannot encode unseen labels\n    Xl[i] = l.fit_transform(Xl[i]) + 1 # all existing labels will be +1\n    in_index = Xl_test[i].isin(l.classes_) # select rows in test_df where labels have been seen \n    Xl_test.loc[in_index, i] = l.transform(Xl_test[i][in_index]) + 1\n    Xl_test.loc[~in_index, i] = 0 # all unseen label will be coded 0\n#     l.fit(np.concatenate([Xl[i].values.astype(str), Xl_test[i].values.astype(str)]))\n#     Xl[i] = l.transform(Xl[i].values.astype(str))\n#     Xl_test[i] = l.transform(Xl_test[i].values.astype(str))\n\n# free up memory\n%xdel l\n%xdel tmp_cats\n%xdel tmp_nums","5aa83727":"%%time\nimport pickle\n\nfor filename, obj in [\n    ('df_train', df_train), ('df_test', df_test),\n    ('X', X), ('X_test', X_test),\n    ('Xl', Xl), ('Xl_test', Xl_test),\n]:\n    with open(filename + '.pickle', 'wb') as handle: pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)","4eb4ad2c":"# Google Analytics Customer Revenue Prediction","400f73d1":"### geoNetwork\n#### geoNetwork.continent, geoNetwork.subContinent\n   - 5 continents + unset\n   - 23 subContinents\n   - America constituted to nearly half of the traffic and contributed to  more than 95% of the total revenues\n   - Europe and Asia had  also nearly half of the traffic, but their combined revenues were 2% of the total revenues\n\n#### geoNetwork.country, geoNetwork.region, geoNetwork.city, geoNetwork.metro\n- 228 cities, 483 regions, 956 cities, 123 metros\n- US has 40% traffic, but ~95% revenue\n- 483 Region: highest traffic came from \"not available\" (56%), followed by California (12%). But the \"not available\" only had ~40% revenue, while California had ~29% revenue. Interestingly, New York also had high revenue (13%) relative to its traffic (2.9%).\n- 956 citites. The city identified as \"not available\" has the highest traffic (56%) with ~ 39% revenue. The next in line is Mountain View with 4.5% traffic, but ~11% revenue.\n- Similary, metro of \"not available\" and \"not set\" have the highest combined traffic (~78%), but only ~42% revenue. Whereas San Francisco-Oakland-San Jose and New York have ~14% traffic but 39% revennue.\n\n#### geoNetwork.networkDomain\n- 41982 network domains\n- 40%  are \"not set\" and \"unknown\", but they contributed to ~61% of the total revenue","a44109b1":"### trafficSource\n- 77 trafficSource.adContent. Most of them are NaN\n- 4 trafficSource.adwordsClickInfo.adNetworkType. Again most are NaN\n- 59009 trafficSource.adwordsClickInfo.gclId: google click identifier. Very high cardiality, again mostly NaN\n- 2 trafficSource.adwordsClickInfo.isVideoAd. Mostly NaN \n- 12 trafficSource.adwordsClickInfo.page. Mostly NaN\n- 4 trafficSource.adwordsClickInfo.slot. Most NaN\n- 35 trafficSource.campaign. Most of the revenue came from \"not set\". Most of the leels are from the test set.\n- 2 trafficSource.isTrueDirect. NaN had 70% traffic but also did 39% revenue.\n- 5393 trafficSource.keyword: More than half are NaN, the others are mostly \"not provided\"\n- 7 trafficSource.medium. Most revenues came from \"referral\" which makes senses: customers are mote likely to purchase based on referral. However, \"affiliate\" has a rather low revenue given its traffic.\n- 3179 trafficSource.referralPath. Most revenues (53%) came from NaN, 45% from \"\/\"\n- 500 trafficSource.source. No NaN. Google took up 44% trafic and made 34% revenues. Youtube did badly, took up 23.5% traffic but generated 0.1% revenue. The best is mall.googleplex.com. It has only 7.3% traffic but generated 44.3% revenues. \"Direct\" aslo generated a heavy slice of revenue (17.7%) with only 15.8% traffic.\n","c868b7ff":"### Date & visitStartTime\n- convert to datetime using `pd.to_datetime`\n- Most date and visitStartTime are similar, some conversion from epoch to datetime in visitStartTime resulted in a diference of -1 days, likely due to not using the correct timezone. But this doesn't really matter.\n- Create new columns: month, week, day, weekday, hour based on visitStartTime.","c235dc98":"Define a function to load the CSV files","e6a279c0":"### totals\n- 2 totals.bounces. Half are 1, half are NaN. It makes senses that when bounce is 0, there is no revenue.\n- totals.hits range from 1 to 500. There is no revenue with just 1 hit. \n- totals.pageviews range from 1 to 500 with NaN. Again, no revenue with just 1 or NaN.\n- totals.newVisits: 1 (true) or NaN (not a new visit). There are more new visits (78%) compared to not-new visits (22%). It makes senses that new visits have lower revenue (39%) compared to non-new visits (61%)","11c75f9b":"## Plots","2728667c":"## Predictors\n1. There are 54 predictos in the train set and 52 in the test set.\n2. Predictors not present in the test set can be removed\n3. Predictors where values are the same for all records can also be removed","4cf1f80f":"Among the 714167 fullVisitorId\n- 620675 (~87%) only appeared once. 3120 (0.5%) had revenue. Total revenue is \\$237,084 (~15.4% of the total revenue).\n- The remaining 93492 fullVisitorId (~13%) appeared more than once and they made up the remaining ~85% of the total revenue.\n- 58711 (~8.2%) appearched twice. 2356 (4.0%) had revenue. Total spent is \\$216,988 (~14.1% of the total revenue).\n- 1 fullVisitorId appeared 255 times, but spent no money.\n- 1 fullVisitorId appeared 86 times, and spent money in 33 occassions, with a combined spent of \\$2,553.\n- 1 fullVisitorId appeared 278 times, and spent money in 16 occasssions, with a combined spent of \\$77,133. This is about 5% of the total revenue.\n\nIt appears the repeated fullVisitorId were more likely to purchase, up to about ~20 repeats","a57a34ba":"## Feature engineering\n","45969179":"- **Device category**: desktop has a higher total revenue compared to mobile & tablet proporrtional to their traffic.\n- **Operating systerm**: Macintosh has a higher revenues even though it has lower traffic compared to Windoes. Chrome OS also has a high revenue relative to its traffic.\n- **Browser**: Chrome has the higest traffic and revenue. Safari has the 2nd highest traffic, but its revenue is comparatively lower based on its traffic.\n- **Is mobile**: Mobile traffic is just a litte bit over 1\/3rd of non-mobile, but its revenue is only 1\/10th of non-mobile.\n- **Continent & subContinent**: Americas has proportinally higher revenues compared to Asia & Europe. Revenues came mostly from Northern America even though other continents have high traffic.\n- **Country**: again most revenues came from US.\n- **Region**: revenues came mostly from \"not vailable in demo dataset\", followed by California. Interestingly, New York, Texas, Washington & Illinois all have high revenues relative to their traffic.\n- **City**: again revenues came mostly from \"not avilable in demo dataset\". New York, Mountain View, San Francisco, Los Angeles have high revenues relative to their traffic.\n- **Metro**: similar patterns to City.\n- **Network domain**: revneues come predominantly from \"(not set)\". The unknown.unknow has a relatively high traffic, but low revenues.\n- **Traffic source**: mall.googleplex.com generated most revenues, given that it has lower traffic compared to the other. Youtube did poorly given it had high traffic.\n- **Medium**: referral generated a higher revenue than the other. It's understandable. Affiliate generated a much less revenue based on its traffic.\n- **Keyword, campain**: \"(not provided)\", \"(not set)\" have the highest revenue. Nothing much can be said, many of them are \"code\".\n- **Ad content**: Google Merchandise Collection has the highest revenue. Google online store doesn't seem to fare well given its traffic.\n- **gclId**: Google Click Identifier, nothing much can be said.\n- **totals.hit** and **totals.pageviews**: Most only had 1 hit or 1 pageview. Revenues came in more after about ~10 hits or pageviews\n- **Time**:\n  - Nov had the highest traffic, but Dec had the highest purchases (Xmas?)\n  - Week 5, 6, 8, 9, 10, 11, 15, 16, 18, 19, 21 to 25: no traffic, no revenue, strange!\n  - The 4th day of a month had the lowest revenue, mmm\n  - Friday and Saturday had low revenue, people going out?\n  - From 5 AM to 12 PM, revenues were lower. However, time zones may be different.","22d9e9ff":"### device\n\n#### browser\n- There are 129 of them in both train and test\n- The most popular is Chrome (68.7%), followed by Safari, then Firefox.\n- Some of them are present only in the train or test set and the counts are very low.\n\n#### device.deviceCategory\n- Desktops take up ~ 3 quarters of the train set, but only about 1\/3rd in the test set\n- Desktops contributed more than 90% of the total revenue\n\n#### device.operatingSystem\n- 24 levels in total\n- 6 of them: Windows, macintosh, Android, iOS, Linux and Chrome constituted to more than 99% of the market","25853e6e":"## Saving data\nSave these data to be used later","bdbffcf7":"## Target variable\n`totals.transactionRevenue`\n1. There are 903653 records in the train set, 804684 in the test set\n2. Only 11515 records resulted in revenue, the remaining 892138 records had zero revenue.\n3. Of these, only 9996 visitors (out of 714167 unique visitors) had non-zero revenue. This translates to: only 1.4% of visitors bought something.\n4. From 2016-08 to 2017-08, GStore in this dataset had \\$1,540,071  in revenue, or \\$4,219 per day. With a min spend of \\$0.01, max of \\$23,129, mean of \\$133, and median of \\$49","89249e99":"### channelGrouping\n- There are 8 unique levels\n- The majority is \"Organic Search\"","73cce53d":"### Various Id\n From 903653 records, there are:\n - 902755 sessionId\n - 886303 visitId\n - 714167  fullVisitorId\n \nNone of them have null value. Some of the ids are present in more than 1 record.\n\nSome of the Ids are present in both train & test set.\n- 5 sessionId are in both\n- 5 visitId are in both\n- 7679 fullVisitorId are in both\n- Of these 7679,  about 7% (N = 536) spent money. The combined amount is \\$245,847\n- In comparison, 706448 are only in the train set. Of them, about 1.34% (N = 9460) spent money. The combined amount is \\$1,294,224.\n- So maybe fullVisitorId is a good predictor?"}}