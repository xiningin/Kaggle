{"cell_type":{"1a5a19ee":"code","242f7861":"code","06fcddb5":"code","acc5f1a2":"code","edc8623f":"code","434c2f63":"code","3f3d24df":"code","41c759bd":"code","bf84749b":"code","eb226c0e":"code","95439596":"code","70226e2d":"code","ef3055cc":"code","67e6fe02":"code","7528cc59":"code","f946400a":"code","066133e4":"code","84cf20a5":"code","8d4b9e4d":"code","1fec322f":"code","dcff5422":"code","9a565ae5":"code","565e6fee":"code","e85abdbb":"code","6992d13e":"code","47a86aa0":"code","882b20d9":"code","341dfa39":"code","37b52e95":"code","5cd62c89":"code","6e5bd757":"code","7319b175":"code","c92df798":"code","47922acd":"code","ffb94c52":"code","4d8f6ca9":"code","d2e4769a":"code","013430d9":"code","909ee736":"code","c9ae5c57":"code","d7be1f22":"code","eb5e06c8":"code","c8473de9":"code","4b850347":"code","d75681cf":"code","08d8bdd4":"code","c4da284c":"code","02d1be46":"code","1fdb525b":"code","b2f5c99f":"code","0a5728ed":"code","a3f91cba":"code","27c2d33e":"code","032f4c61":"code","d2c363b8":"code","1a34d660":"code","b022de9d":"code","8f85c63e":"code","b9948666":"markdown","bc506bbc":"markdown","61359b97":"markdown","de4ee4cf":"markdown","ad5bf4a0":"markdown","28453b8c":"markdown","5a475ee3":"markdown","e45967c6":"markdown","d4a78e7f":"markdown","e830e3f2":"markdown","8592be07":"markdown","c2859dd2":"markdown","502c98ab":"markdown","94910d29":"markdown","228a3835":"markdown"},"source":{"1a5a19ee":"# numpy and pandas for data manipulation\nimport numpy as np\nimport pandas as pd \nfrom sklearn.preprocessing import LabelEncoder\nimport os \nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr","242f7861":"train_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","06fcddb5":"train_df.head()","acc5f1a2":"test_df['SalePrice'] = np.nan\ntest_df.head()","edc8623f":"print(train_df.shape, test_df.shape)","434c2f63":"train_df = train_df.drop(['Id'], axis = 1)\ntest_df = test_df.drop(['Id'], axis = 1)\ntarget = train_df['SalePrice']\nall_data = pd.concat([train_df, test_df], ignore_index = True)\nall_data.head()","3f3d24df":"target.hist()","41c759bd":"print(target.skew())\ntarget_log = np.log1p(np.array(target))","bf84749b":"pd.DataFrame(target_log).hist()","eb226c0e":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","95439596":"missing_values = missing_values_table(all_data)\nmissing_values","70226e2d":"all_data.dtypes.value_counts()","ef3055cc":"all_data.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","67e6fe02":"all_data.info()","7528cc59":"all_data['PoolQC'] = all_data['PoolQC'].fillna('NA')\nprint(all_data['PoolQC'].isnull().sum())\nall_data.head()","f946400a":"all_data.loc[(all_data['PoolQC'] == 'NA') & (all_data['PoolArea'] != 0)]","066133e4":"all_data.at[2420, 'PoolQC'] = 'Fa'\nall_data.at[2599, 'PoolQC'] = 'Fa'\nall_data.at[2503, 'PoolQC'] = 'Fa'","84cf20a5":"maps = {\n    'Ex': 5,\n    'Gd': 4,\n    'TA': 3,\n    'Fa': 2,\n    'NA': 1\n}\nall_data['PoolQC'] = all_data['PoolQC'].replace(maps)\nall_data.head()","8d4b9e4d":"all_data['MiscFeature'] = all_data['MiscFeature'].fillna('NA')\nall_data['Alley'] = all_data['Alley'].fillna('NA')\nall_data['Fence'] = all_data['Fence'].fillna('NA')\nall_data['FireplaceQu'] = all_data['FireplaceQu'].fillna('NA')\n\nmaps = {\n    'NA':1,\n    'Po':2,\n    'Fa':3,\n    'TA':4,\n    'Gd':5,\n    'Ex':6\n}\nall_data['FireplaceQu'] = all_data['FireplaceQu'].replace(maps)\n\nplt.scatter(all_data['LotFrontage'], all_data['SalePrice'])","1fec322f":"all_data[all_data['LotFrontage'].isnull() == True].SalePrice.median()\nall_data['LotFrontage'] = all_data['LotFrontage'].fillna(all_data['LotFrontage'].median())\nall_data['GarageCond'] = all_data['GarageCond'].fillna('NA').replace(maps)\nall_data['GarageQual'] = all_data['GarageQual'].fillna('NA').replace(maps)\n\nmapp = {\n    'Fin': 4,\n    'RFn': 3,\n    'Unf': 2,\n    'NA': 1\n}\nall_data['GarageFinish'] = all_data['GarageFinish'].fillna('NA').replace(mapp)\nall_data['GarageType'] = all_data['GarageType'].fillna('NA')\nall_data['GarageYrBlt'] = all_data['GarageYrBlt'].fillna(-1)\nall_data['GarageArea'] = all_data['GarageArea'].fillna(0.0)\nall_data['GarageCars'] = all_data['GarageCars'].fillna(0.0)\n\nall_data['BsmtHalfBath'] = all_data['BsmtHalfBath'].fillna(0.0)\nall_data['BsmtFullBath'] = all_data['BsmtFullBath'].fillna(0.0)\n\nmapp = {\n    'Gd':5,\n    'Av':4,\n    'Mn':3,\n    'No':2,\n    'NA':1\n}\nall_data['BsmtExposure'] = all_data['BsmtExposure'].fillna('NA').replace(mapp)\nall_data['BsmtCond'] = all_data['BsmtCond'].fillna('NA').replace(maps)\nall_data['BsmtQual'] = all_data['BsmtQual'].fillna('NA').replace(maps)\n\nmapp = {\n    'GLQ':7,\n    'ALQ':6,\n    'BLQ':5,\n    'Rec':4,\n    'LwQ':3,\n    'Unf':2,\n    'NA':1\n}\nall_data['BsmtFinType1'] = all_data['BsmtFinType1'].fillna('NA').replace(mapp)\nall_data['BsmtFinType2'] = all_data['BsmtFinType2'].fillna('NA').replace(mapp)\n\nall_data[['MasVnrType','MasVnrArea']][all_data['MasVnrType'].isnull() == True]\nall_data['MasVnrType'] = all_data['MasVnrType'].fillna('None')\nall_data['MasVnrArea'] = all_data['MasVnrArea'].fillna(0.0)\nall_data.at[2610, 'MasVnrType'] = 'Stone'\n\nall_data['BsmtFinSF1'] = all_data['BsmtFinSF1'].fillna(0.0)\nall_data['BsmtFinSF2'] = all_data['BsmtFinSF2'].fillna(0.0)\nall_data['BsmtUnfSF'] = all_data['BsmtUnfSF'].fillna(0.0)\nall_data['TotalBsmtSF'] = all_data['TotalBsmtSF'].fillna(0.0)\n\nall_data['SaleType'] = all_data.SaleType.fillna('Oth')\nall_data['KitchenQual'] = all_data.KitchenQual.fillna('Oth')\n\nfor col in all_data.columns:\n    if((all_data[col].isnull().sum() > 0) & (col != 'SalePrice')):\n        all_data[col] = all_data[col].fillna('NA')\n        \nall_data.head()","dcff5422":"import seaborn as sns\nsns.boxplot(x=all_data['MSSubClass'])","9a565ae5":"sns.boxplot(x=all_data['LotFrontage'])","565e6fee":"sns.boxplot(x=all_data['LotArea'])","e85abdbb":"sns.boxplot(x=all_data['YrSold'])","6992d13e":"linearFeat = []\ncor = all_data.corr()['SalePrice'].sort_values(ascending=False)[all_data.corr()['SalePrice'] > 0.5]\ncor.head(20)","47a86aa0":"all_Data = all_data.copy()","882b20d9":"all_data = all_data.drop('SalePrice', axis = 1)","341dfa39":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\nfor i in numeric_feats:\n    all_data[i] = scaler.fit_transform(np.array(all_data[i]).reshape(-1, 1))\nall_data.head()","37b52e95":"# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in all_data:\n    if all_data[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(all_data[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(all_data[col])\n            # Transform both training and testing data\n            all_data[col] = le.transform(all_data[col])\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","5cd62c89":"all_data = pd.get_dummies(all_data)\nall_data.head()","6e5bd757":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(random_state=1, max_depth=10)\ntrain = all_data[:train_df.shape[0]]\ntest = all_data[train_df.shape[0]:]\n# target_log = train['target']\nx_train, x_valid, y_train, y_valid = train_test_split(train, target_log, test_size=0.3, random_state = 1)\n\nmodel.fit(x_train,y_train)","7319b175":"dict(reversed(sorted(zip(model.feature_importances_, x_train.columns.values))))","c92df798":"from sklearn.metrics import mean_squared_error\nimport math \ny_pred_val = model.predict(x_valid)\nprint(math.sqrt(mean_squared_error(y_pred_val, y_valid)))","47922acd":"all_data = all_Data.copy()\nall_data.head()","ffb94c52":"all_data['totalBathroom'] = 0.5*all_data['HalfBath'] + all_data['FullBath'] + 0.5*all_data['BsmtHalfBath'] + all_data['FullBath']\nall_data.head()","4d8f6ca9":"all_data['age'] = all_data['YrSold'] - all_data['YearRemodAdd']\nismodeled = []\ndeltaYr = all_data['YearRemodAdd'] - all_data['YearBuilt']","d2e4769a":"ismodeled = []\nfor i in deltaYr:\n    if i == 0:\n        ismodeled.append(1)\n    else:\n        ismodeled.append(0)\nall_data['IsModeled'] = ismodeled\nall_data.head()","013430d9":"deltaYr = all_data['YrSold'] - all_data['YearBuilt']\nisnew = []\nfor i in deltaYr:\n    if i == 0:\n        isnew.append(1)\n    else:\n        isnew.append(0)\nall_data['IsNew'] = isnew\nall_data.head()","909ee736":"all_data['totalSquareFeet'] = all_data['GrLivArea'] + all_data['TotalBsmtSF']\nall_data.head()","c9ae5c57":"#log transform skewed numeric features:\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\nall_data[skewed_feats] = np.log1p(all_data[skewed_feats])","d7be1f22":"cor = all_data.corr()['SalePrice'].sort_values(ascending=False)[all_data.corr()['SalePrice'] > 0.5]\ncor.head(20)","eb5e06c8":"all_data = all_data.drop('SalePrice', axis = 1)","c8473de9":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\nfor i in numeric_feats:\n    all_data[i] = scaler.fit_transform(np.array(all_data[i]).reshape(-1, 1))\nall_data.head()","4b850347":"# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in all_data:\n    if all_data[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(all_data[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(all_data[col])\n            # Transform both training and testing data\n            all_data[col] = le.transform(all_data[col])\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","d75681cf":"all_data = pd.get_dummies(all_data)","08d8bdd4":"from sklearn.decomposition import KernelPCA \nfrom sklearn.decomposition import PCA\nkpca = KernelPCA(n_components = 80) \nall_data_pca = kpca.fit_transform(all_data)\nall_data_pca = pd.DataFrame(all_data_pca)\nall_data_pca.head()","c4da284c":"all_data_pca['Id'] = all_data_pca.index\nall_data_pca.head()","02d1be46":"train = all_data_pca[:train_df.shape[0]]\ntest = all_data_pca[train_df.shape[0]:]","1fdb525b":"target = pd.DataFrame({'SalePrice': target_log})\ntarget.shape","b2f5c99f":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nimport math  \nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nscore = []\npredict_val = pd.DataFrame(test['Id'])\n\nskf = KFold(n_splits = 5, shuffle=True, random_state=123)\nskf.get_n_splits(train, target_log)\noof_rfr_df = pd.DataFrame()\npredictions = pd.DataFrame(test['Id'])\nx_test = test.drop(['Id'], axis = 1)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(train, target)):\n    x_train, y_train = train.iloc[trn_idx], target.iloc[trn_idx]['SalePrice']\n    x_valid, y_valid = train.iloc[val_idx], target.iloc[val_idx]['SalePrice']\n    index = x_valid['Id']\n    yp = 0\n    x_train = x_train.drop(['Id'], axis = 1)\n    x_valid = x_valid.drop(['Id'], axis = 1)\n    rfr = RandomForestRegressor(random_state=1, max_depth=10)\n    rfr.fit(x_train,y_train)\n    score.append(math.sqrt(mean_squared_error(rfr.predict(x_valid), y_valid)))\n    yp += rfr.predict(x_test)\n    fold_pred = pd.DataFrame({'ID': index,\n                              'label':rfr.predict(x_valid)})\n    oof_rfr_df = pd.concat([oof_rfr_df, fold_pred], axis=0)\n    predictions['fold{}'.format(fold+1)] = yp","0a5728ed":"score = pd.DataFrame(score)\nprint(score[0].mean())\nprint(score[0].std())\noof_rfr_df = oof_rfr_df.sort_values('ID')\noof_rfr_df.head(10)","a3f91cba":"rfr_predict = pd.DataFrame()\nrfr_predict['predict'] = (predictions['fold1']+predictions['fold2']+predictions['fold3']+predictions['fold4']+predictions['fold5'])\/5\nrfr_predict.head()","27c2d33e":"import shap\n\nimportances = rfr.feature_importances_\nindices = np.argsort(importances)\n\nfeatures = train.columns\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","032f4c61":"explainer = shap.TreeExplainer(rfr)\nshap_values = explainer.shap_values(x_valid)","d2c363b8":"shap.summary_plot(shap_values, x_valid, plot_type=\"bar\")","1a34d660":"shap.summary_plot(shap_values, x_valid)","b022de9d":"predict = np.exp(rfr_predict['predict']-1)\nmysubmit = pd.DataFrame({'Id': test['Id']+1,\n                         'SalePrice': predict})\nmysubmit.head()","8f85c63e":"mysubmit.to_csv('mysubmit.csv', index=False)","b9948666":"## Handling Missing Value","bc506bbc":"## Common Sense Features","61359b97":"## Handling Missing Values","de4ee4cf":"The outliers lie not only on the training data but also on the testing data, which mean the outliers must have some useful information, that's why i chose not to eliminate them. Beside the reason above, the number of training data is equal to the number of testing data, if we eliminate the data, we will easily get overfit with the models.","ad5bf4a0":"## Log transform","28453b8c":"## Dealing with Categorical Values","5a475ee3":"# Clean Data","e45967c6":"# Feature Engineer","d4a78e7f":"## Scaling","e830e3f2":"# EDA","8592be07":"# Load Data","c2859dd2":"# Training with out feature engineering","502c98ab":"As the number of higher cor-relation features and the corelation between the saleprice and the features are higher, we can say our new features are good","94910d29":"# Modeling ","228a3835":"## PCA"}}