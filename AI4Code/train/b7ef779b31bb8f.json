{"cell_type":{"c5b4a82d":"code","ef3caa9a":"code","6e03838e":"code","4951b14a":"code","ca633847":"code","5dce8db6":"code","2615bdbe":"code","3278ed2c":"code","2979cf6f":"code","90816a1f":"code","748e132b":"code","f3ef24a7":"code","7e72c3a5":"code","49777811":"code","aea8b54f":"code","6f3a4bf3":"code","6c007ccf":"code","0f6189a9":"code","328a3ad0":"code","7bd8701f":"code","06dae578":"code","420fcb6d":"code","398ff994":"code","16c44a93":"code","b38b149b":"code","96aa0244":"code","7e9a3b1a":"code","638f3756":"code","9f74687a":"code","65c53a8a":"code","8414dd91":"code","950e2815":"code","be618dc2":"code","f23065ac":"markdown","0d8ca442":"markdown","3a2cd18a":"markdown","b728aced":"markdown","4c43daa8":"markdown","ab0328f5":"markdown","7711bf24":"markdown","7b56b4a5":"markdown","ddaea4bc":"markdown","29c6e6ef":"markdown","c110fffb":"markdown","bf6f4e13":"markdown"},"source":{"c5b4a82d":"import os\nprint(os.listdir(\"..\/input\"))","ef3caa9a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport collections\n%matplotlib inline","6e03838e":"from scipy import stats\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import binarize\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report , confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import metrics","4951b14a":"from sklearn import svm\nfrom sklearn import tree\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, BaggingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV","ca633847":"HR = pd.read_csv(\"..\/input\/HR.csv\")\nHR.shape","5dce8db6":"HR.head()","2615bdbe":"HR.tail()","3278ed2c":"#Check null or missing values in dataset. Our data is pretty clean, no missing values.\nHR.isnull().sum()","2979cf6f":"HR.describe()","90816a1f":"HR1 = HR[HR['left']==1]\nHR1.describe()","748e132b":"HR['dept'].unique()","f3ef24a7":"#Let us combine \u201ctechnical\u201d, \u201csupport\u201d and \u201cIT\u201d these three together and call them \u201ctechnical\u201d.\nHR['dept']=np.where(HR['dept'] =='support', 'technical', HR['dept'])\nHR['dept']=np.where(HR['dept'] =='IT', 'technical', HR['dept'])","7e72c3a5":"#After the change, this is how the department categories look:\n\nprint(HR['dept'].unique())","49777811":"sns.countplot(HR['left'])\nplt.savefig('department_bar_chart')","aea8b54f":"sns.countplot(HR['left'], hue=HR['dept'])\nplt.savefig('dept_bar_chart')","6f3a4bf3":"#Let's dig in a bit deeper and look at other features in our data. To do so we check for co-relation between the features.\nsns.set(font_scale=1)\nplt.figure(figsize=(12,10))\nsns.heatmap(HR.corr(),annot=True)\nplt.xticks(rotation=90)\nplt.show()","6c007ccf":"# delete emp_id which is not useful feature\nHR = HR.drop(['emp_id'], axis=1)","0f6189a9":"X = HR.drop(['left'],axis=1)\ny = HR['left']","328a3ad0":"X.keys()","7bd8701f":"X['satisfaction_level'].value_counts()\n# here you can find a string '~'in the last line which was causing the problem\n# you can do two things 1. you can eleminate that row. or convert that row into numbers.","06dae578":"# convert categorical dataset into numerical using LabelEncoder\nle = LabelEncoder()\nX['salary']= le.fit_transform(X['salary'])\nX['dept']= le.fit_transform(X['dept'])\nX['satisfaction_level'] = le.fit_transform(X['satisfaction_level']) #  I have converted that complete row into numbers","420fcb6d":"# Spilit Training and Testing Data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10, stratify=y)\n# Creating a seperate scaled set to be used in some models to improve our results\nXscaled_train = pd.DataFrame(preprocessing.scale(X_train.values))\nXscaled_test  =  pd.DataFrame(preprocessing.scale(X_test.values))","398ff994":"# Let's check the shape of training and testing dataset\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","16c44a93":"# logr = LogisticRegression()\n# logr.fit(X_train,y_train)","b38b149b":"logr = LogisticRegression(random_state=10)\nlogr.fit(Xscaled_train,y_train)\ny_pred = logr.predict_proba(Xscaled_test)","96aa0244":"from sklearn.metrics import roc_auc_score\nprint (roc_auc_score(y_test,y_pred[:,1]),\"Inital Score\") # here first you will have to import roc_auc_score.","7e9a3b1a":"tr = tree.DecisionTreeClassifier(class_weight='balanced',random_state=12)\ntr.fit(X_train,y_train)\npredtree = tr.predict(X_test)\nprint (roc_auc_score(y_test,predtree))","638f3756":"def imp_features_plot(data, x, y, palette,title):\n    sns.set(style=\"whitegrid\")\n    ft = sns.PairGrid(data, y_vars=y, x_vars=x, size=5, aspect=1)\n    ft.map(sns.stripplot,orient='h',palette=palette, edgecolor=\"black\",size=15)\n    for ax, title in zip(ft.axes.flat, titles):\n    # Set a different title for each axes\n        ax.set(title=title)\n    # Make the grid horizontal instead of vertical\n        ax.xaxis.grid(False)\n        ax.yaxis.grid(True)\n    plt.show()","9f74687a":"fo = {'Features':HR.drop('left',axis=1).columns.tolist(),'Importance':tr.feature_importances_}\nImportance = pd.DataFrame(fo,index=None).sort_values('Importance',ascending=False)\ntitles = [\"Importance of the various Features in predicting the outcome\"]\nimp_features_plot(Importance,'Importance','Features','Greens_r',titles)","65c53a8a":"rf = RandomForestClassifier() #Random Forest\nrf.fit(X_train,y_train)\nrfpred=rf.predict(X_test)\nrfp = dict(n_estimators=np.arange(5,25,10))\nprint (roc_auc_score(y_test,rfpred),'Initial Score')","8414dd91":"logreg= LogisticRegression(C=0.17,solver='sag')\nlogreg.fit(Xscaled_train,y_train)\nlogregprob = logreg.predict(Xscaled_test)\nfpr, tpr, thresholds = metrics.roc_curve(y_test, logregprob)\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC')\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.show()","950e2815":"svc = svm.SVC(probability=True,random_state=12)\nsvc.fit(Xscaled_train,y_train)\nscaledp = svc.predict_proba(Xscaled_test)\nprint (roc_auc_score(y_test,scaledp[:,1]),'Initial Score')","be618dc2":"gbr = GradientBoostingClassifier()\ngbr.fit(Xscaled_train,y_train)\ngbrp = gbr.predict_proba(Xscaled_test)\nprint (roc_auc_score(y_test,gbrp[:,1]),'Initial Score')","f23065ac":"The Decision Tree Classifier allows us to see which features were most important in predicting the outcomes, let's see how different features ranked in terms of their importance.","0d8ca442":"### 2. Decision Tree Classifier\nNote: Optimising this algorithm was very computationally expensive, hence I decided to do away with it.","3a2cd18a":"### 4. Support Vector Machine (SVM)\nWe fit a SVM and optimise the regularisation constant 'C'. We use the scaled X_train and X_test as the model is much more efficient under the same.","b728aced":"#### Objectives:\n1.\tEXPLAINING THE REASONS EMPLOYEE ARE PRONE TO LEAVE AFTER ANALYSING THIS DATA SET.\n2.\tEXPLAIN WHAT TYPE OF EMPLOYEE ARE PRONE TO LEAVE THE COMPANY.\n3.\tPREDICT THE FUTURE EMPLOYEE WHO WOULD TEND TO LEAVE THE COMPANY.","4c43daa8":"### Description:\n\nThe data is for company X which is trying to control attrition. There are two sets of data: \u201cExisting employees\u201d and \u201cEmployees who have left\u201d. Following attributes are available for every employee.\n* Satisfaction Level\n* Last evaluation\n* Number of projects\n* Average monthly hours\n* Time spent at the company\n* Whether they have had a work accident\n* Whether they have had a promotion in the last 5 years\n* Departments (column sales)\n* Salary\n* Whether the employee has left\n","ab0328f5":"## Training, Testing and Optimising","7711bf24":"# Human Resourece Attrition Analysis\n\n![](http:\/\/www.mammablog.org\/wp-content\/uploads\/2018\/06\/11.png)","7b56b4a5":"### 3. Random Forest Classifier","ddaea4bc":"### Gradient Boosting\nThe Gradient Boosting Classifier is arguably one of the best while dealing with Classification, we look to make the most of it by optimising its parameters iteratively.","29c6e6ef":"I hope this kernal is useful to you to learn exploratory data analysis and classification problem.\n\nIf find this notebook help you to learn, Please Upvote.\n\nThank You!!","c110fffb":"### 1. Logistic Regression","bf6f4e13":"## Preprocessing\nWe start by preparing our data for our models"}}