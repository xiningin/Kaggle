{"cell_type":{"6702a2cd":"code","55c668e9":"code","48797d0d":"code","b47bd559":"code","b1247f6c":"code","fa38e22a":"code","675b8474":"code","220f0dc2":"code","09383ba3":"code","88494537":"code","bae955ea":"code","13fb6215":"code","7b2a5cbf":"code","3a4c9a6e":"code","5a72897f":"code","236973c9":"code","de8c9f59":"code","08813edc":"code","ce4c905b":"code","1ef50acd":"code","a1bc1b28":"code","4245dfda":"markdown","1cc634c7":"markdown","3a2d79ba":"markdown","3be5cc25":"markdown","a814b119":"markdown","b38e948a":"markdown","1422afdf":"markdown","dc858bd1":"markdown","d3807636":"markdown"},"source":{"6702a2cd":"!nvidia-smi","55c668e9":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\nimport keras\nimport keras.layers as L\nimport math, random\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.preprocessing import image\nfrom random import shuffle\nfrom sklearn.model_selection import train_test_split\nimport plotly.express as px\nimport seaborn as sns\nimport os, gc, time\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n    except RuntimeError as e:\n        # \ud504\ub85c\uadf8\ub7a8 \uc2dc\uc791\uc2dc\uc5d0 \uba54\ubaa8\ub9ac \uc99d\uac00\uac00 \uc124\uc815\ub418\uc5b4\uc57c\ub9cc \ud569\ub2c8\ub2e4\n        print(e)\n!pip install -q -U tensorflow-addons\nimport tensorflow_addons as tfa\n\nfrom tensorflow.keras.mixed_precision import experimental as mixed_precision#!\npolicy = mixed_precision.Policy('mixed_float16')#!\nmixed_precision.set_policy(policy)#!\n\nrandom.seed(0)\nnp.random.seed(0)\ntf.random.set_seed(0)","48797d0d":"def id_to_path(idxs,train=True):\n    for idx in idxs:\n        path = ''\n        if train:\n            folder = 'train\/'\n        else:\n            folder = 'test\/'\n        path+=folder+idx[0]+'\/'+idx+'.npy'\n        if os.path.exists(path): yield path\ndef as_numpy_dataset(idxs, is_train):\n  loading = [np.load(path, mmap_mode = 'r') for path in id_to_path(idxs, is_train)]\n  print(loading[0].dtype, loading[0].shape )\n  loading = [I[::2] for I in loading]\n  tmp_result = np.asarray(loading)\n  print(tmp_result.dtype)\n  if tmp_result.dtype == np.float16: return tmp_result\n  return tmp_result.astype(np.float16)","b47bd559":"validation_rate = 0.118 # do not change after build dataset \ntry:\n  sample_submission = pd.read_csv('sample_submission.csv')\n  test_idx = sample_submission['id'].values\n  tests_DT = np.memmap('test_mmap', dtype = 'float16', mode = 'r', shape = (len(test_idx), 3, 273, 256) )\nexcept:\n  !sudo mv kaggle.json  ..\/root\/.kaggle\/kaggle.json \n  !pip install --upgrade --force-reinstall --no-deps kaggle\n  !kaggle competitions download -c seti-breakthrough-listen\n  \n  !unzip seti-breakthrough-listen.zip  -x train\/*\n  sample_submission = pd.read_csv('sample_submission.csv')\n  test_idx = sample_submission['id'].values\n  tmp_DT = as_numpy_dataset(test_idx, is_train  = False)\n  tests_DT = np.memmap('test_mmap', dtype = tmp_DT.dtype, mode = 'w+', shape = tmp_DT.shape)\n  tests_DT[:] = tmp_DT[:]\n  tests_DT.flush()\n  del tmp_DT\n  !sudo rm -r test\n  \ntry:\n  train_labels =pd.read_csv('train_labels.csv')\n  train_idx =  train_labels['id'].values\n  y = train_labels['target'].values\n  x_train,x_valid, y_train,y_valid = train_test_split(train_idx,y,test_size=validation_rate,random_state=42)\n  x_train, y_train = [np.asarray(I) for I in zip(*sorted(list(zip(x_train, y_train))))]\n  x_valid, y_valid = [np.asarray(I) for I in zip(*sorted(list(zip(x_valid, y_valid))))]\n\n  valid_DT = np.memmap('valid_mmap', dtype = 'float16', mode = 'r', shape = (y_valid.shape[0], 3, 273, 256))\n  valid_DT = valid_DT, y_valid\n\n  train_DT = np.memmap('train_mmap', dtype = 'float16', mode = 'r', shape = (y_train.shape[0], 3, 273, 256))\n  #train_DT = np.array(train_DT), y_train\n  train_DT = train_DT, y_train\nexcept:\n  #train_idx\uac00 \uc815\ub82c\ub418\uc5b4\uc788\uae30 \ub54c\ubb38\uc5d0 \ubd84\ub9ac\ucc98\ub9ac\ud6c4 \ubcd1\ud569\n  !sudo rm -r train\n  train_labels = pd.read_csv('train_labels.csv')\n  train_idx =  train_labels['id'].values\n  y = train_labels['target'].values\n  x_train,x_valid, y_train,y_valid = train_test_split(train_idx,y,test_size=validation_rate,random_state=42)\n  x_train, y_train = [np.asarray(I) for I in zip(*sorted(list(zip(x_train, y_train))))]\n  x_valid, y_valid = [np.asarray(I) for I in zip(*sorted(list(zip(x_valid, y_valid))))]\n  \n\n  !unzip seti-breakthrough-listen.zip train\/[0-9]\/*\n  tmp_train_DT = as_numpy_dataset(x_train, is_train  = True)\n\n  gc.collect();time.sleep(5)\n  valid_DT = as_numpy_dataset(x_valid, is_train  = True)\n  !sudo rm -r train \n\n\n\n  !unzip seti-breakthrough-listen.zip train\/[^0-9]\/*\n  !sudo rm seti-breakthrough-listen.zip\n\n  tmp_DT = np.concatenate([valid_DT, as_numpy_dataset(x_valid, is_train  = True)], axis = 0); del valid_DT\n  valid_DT = np.memmap('valid_mmap', dtype = tmp_DT.dtype, mode = 'w+', shape = tmp_DT.shape)\n  valid_DT[:] = tmp_DT[:]\n  valid_DT.flush()\n  valid_DT = valid_DT, y_valid\n  del tmp_DT\n  gc.collect();time.sleep(5)\n\n  ##\ubbf8\ub9ac \ucc98\ub9ac\ub41c\uac83 \uc800\uc7a5\n  train_DT1 = np.memmap('tmp_train_dt1', dtype = tmp_train_DT.dtype, mode = 'w+', shape = tmp_train_DT.shape)\n  train_DT1[:] = tmp_train_DT[:]\n  train_DT1.flush()\n  del tmp_train_DT\n  gc.collect();time.sleep(5)\n\n  ##\uc0c8\ub85c \uc0dd\uc131\n  tmp_train_DT = as_numpy_dataset(x_train, is_train  = True)\n  train_DT2 = np.memmap('tmp_train_dt2', dtype = tmp_train_DT.dtype, mode = 'w+', shape = tmp_train_DT.shape)\n  train_DT2[:] = tmp_train_DT[:]\n  train_DT2.flush()\n  del tmp_train_DT\n  gc.collect();time.sleep(5)\n\n  train_DT = np.concatenate([train_DT1, train_DT2], axis = 0)\n  !sudo rm -r train \n\n  train_DT_mmap = np.memmap('train_mmap', dtype = train_DT.dtype, mode = 'w+', shape = train_DT.shape)\n  train_DT_mmap[:] = train_DT[:]\n  train_DT_mmap.flush()\n  del train_DT, train_DT1, train_DT2\n  !sudo rm -r tmp_train_dt1\n  !sudo rm -r tmp_train_dt2\n  train_DT = train_DT_mmap, y_train","b1247f6c":"BATCH_SIZE = 16\ntrain_dataset = tf.data.Dataset.from_tensor_slices(train_DT).shuffle(512).batch(BATCH_SIZE, True).prefetch(tf.data.experimental.AUTOTUNE)\nvalid_dataset = tf.data.Dataset.from_tensor_slices(valid_DT).batch(BATCH_SIZE).prefetch(33)\ntest_dataset  = tf.data.Dataset.from_tensor_slices(tests_DT).batch(BATCH_SIZE).prefetch(32)","fa38e22a":"ACT = (lambda X: X * tf.keras.activations.tanh(tf.keras.activations.softplus(X)))\nACTL = tf.keras.layers.Lambda(ACT)\nACT_cosnorm = lambda x: tf.keras.activations.elu(x, alpha=0.05)\n\n@tf.function\ndef make_norm(Val, axis):\n    mean = tf.reduce_mean(tf.stop_gradient(Val), axis, True)\n    return (Val - mean) \/ (1e-6 + tf.norm((Val - mean),2, axis, True) )\n\nclass coscnn(tf.keras.layers.Layer):\n    def __init__(self, d, kk, ss, padding = 'VALID'):\n        super(coscnn, self).__init__()\n        self.args = [[1,*kk,1], [1,*ss,1], [1,1,1,1], padding]\n        self.num_outputs = d\n    def build(self, input_shape):\n        last_shape = int(input_shape[-1]) * self.args[0][-3] * self.args[0][-2]\n        self.kernel = self.add_weight(\n            shape=(last_shape, self.num_outputs), initializer=\"he_normal\", trainable=True\n        )\n        self.bias = self.add_weight(shape=(self.num_outputs,), initializer=\"zeros\", trainable=True)\n        if len(input_shape) == 4:\n          self.extracter = lambda inputs : tf.image.extract_patches(inputs, *self.args)\n        if len(input_shape) == 5:\n          self.extracter = TimeDistributed(Lambda(lambda inputs : tf.image.extract_patches(inputs, *self.args)))\n    def call(self, inp):\n        return make_norm(self.extracter(inp), -1) @ make_norm(self.kernel, 0) + self.bias","675b8474":"from tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nwide = .781\nX = IN = Input((3, 273, 256))\nX = tf.expand_dims(X, -1)\nX = coscnn(128, (13, 8), (13, 8), 'VALID')(X)#21x32\nprint(X.shape)\nX = ACT_cosnorm(X)\n\nDrops = Dropout(0.25)(tf.reduce_mean(tf.ones_like(X), -1, True))#pixel wise dropout\nX = X * Drops\n\nX = tf.concat(tf.unstack(X, axis = 1), axis = -1)\n\nX = Conv2D(int(512 * wide), 3, 1, padding = 'same', use_bias = False, kernel_initializer = 'he_normal')(X)\nX = ACT(BatchNormalization()(X))\nX = Conv2D(int(256 * wide), 3, 1, padding = 'same', use_bias = False, kernel_initializer = 'he_normal')(X)\nX = ACT(BatchNormalization()(X))\nX = Conv2D(int(512 * wide), 3, 1, padding = 'same', use_bias = False, kernel_initializer = 'he_normal')(X)\nX = ACT(BatchNormalization()(X))\nX = AveragePooling2D(padding = 'same')(X)#11x16\n\n\nX = Conv2D(int(1024 * wide), 3, 1, padding = 'same', use_bias = False, kernel_initializer = 'he_normal')(X)\nX = ACT(BatchNormalization()(X))\nX = Conv2D(int(512 * wide), 3, 1, padding = 'same', use_bias = False, kernel_initializer = 'he_normal')(X)\nX = ACT(BatchNormalization()(X))\nX = Conv2D(int(1024 * wide), 3, 1, padding = 'same', use_bias = False, kernel_initializer = 'he_normal')(X)\nX = ACT(BatchNormalization()(X))\nX = AveragePooling2D(padding = 'same')(X)#6x8\nX = Flatten()(X)\n\nX = Dense(1, dtype='float32', use_bias = False)(X)\nX = Activation('sigmoid', dtype='float32')(X)\nmodel = Model(IN, X)\nmodel.summary()\n\nEPOCHS = 20\nclass FCA(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, total_step, initial_learning_rate):\n        self.total_step = total_step\n        self.initial_learning_rate = initial_learning_rate\n\n    def __call__(self, step):\n        return tf.where(\n            tf.less(self.total_step * 72 \/ 100, step)\n            ,self.initial_learning_rate * (1-tf.cos(((step - self.total_step) \/ (self.total_step * 28 \/ 100) * 3.1415926535)))\/2\n            ,self.initial_learning_rate\n        )\n\ndef loss(y_true, y_pred):\n    return tf.keras.losses.binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0.1)\ntotal_run = y_train.shape[0] \/\/ BATCH_SIZE * EPOCHS\n\nopt = tfa.optimizers.RectifiedAdam(FCA(total_run, 0.001), weight_decay=0.0005, clipnorm = 0.01, total_steps = total_run, warmup_proportion=0.02)\nopt = tfa.optimizers.Lookahead(opt, sync_period=6, slow_step_size=0.5)\nopt = mixed_precision.LossScaleOptimizer(opt, loss_scale='dynamic')\nmodel.compile(optimizer=opt, loss=loss, metrics=[keras.metrics.AUC()])","220f0dc2":"def auc_plot(auc,val_auc):\n    plt.plot(auc)\n    plt.plot(val_auc)\n    plt.xlabel('epochs')\n    plt.ylabel('auc')\n    plt.title('auc vs epochs')\n    plt.legend(['auc','val_auc'])\n    plt.show()\ndef loss_plot(loss,val_loss):\n    plt.plot(loss)\n    plt.plot(val_loss)\n    plt.xlabel('epochs')\n    plt.ylabel('loss')\n    plt.title('loss vs epochs')\n    plt.legend(['loss','val_loss'])\n    plt.show()","09383ba3":"history = model.fit(train_dataset,epochs=EPOCHS,validation_data=valid_dataset)","88494537":"filters = model.layers[2].kernel\nf_min, f_max = tf.reduce_min(filters), tf.reduce_max(filters)\nprint(f_min, f_max)\nfilters = (filters - f_min) \/ (f_max - f_min)\nn_filters, ix = 64, 1\nfig, axss = plt.subplots(8, 8, figsize=(15,15))\nfor i, ax in zip(range(n_filters), [J for I in axss for J in I]):\n    # get the filter\n    f = filters[:, i]\n    f = tf.reshape(f, (13, 8))\n    ax.set_xticks([])\n    ax.set_yticks([])\n    # plot filter channel in grayscale\n    ax.imshow(f, cmap='gray')\n# show the figure\nplt.show()","bae955ea":"auc_plot(history.history['auc'],history.history['val_auc'])\nloss_plot(history.history['loss'],history.history['val_loss'])","13fb6215":"import seaborn as sns\n@tf.function\ndef pred(img):return model(img, training = False)","7b2a5cbf":"result = [[],[]]\nfor img, labs in train_dataset.take(33):\n  for I, lab in zip(pred(img), labs):  \n    result[lab].append(I)\nax = sns.distplot(result[0], bins = 10, label = 0, color = sns.color_palette(\"RdBu\", 9)[1])\nax = sns.distplot(result[1], bins = 10, label = 1, color = sns.color_palette(\"RdBu\", 9)[-2])","3a4c9a6e":"result = [[],[]]\nfor img, labs in valid_dataset.take(33):\n  for I, lab in zip(pred(img), labs):  \n    result[lab].append(I)\nax = sns.distplot(result[0], bins = 10, label = 0, color = sns.color_palette(\"RdBu\", 9)[1])\nax = sns.distplot(result[1], bins = 10, label = 1, color = sns.color_palette(\"RdBu\", 9)[-2])","5a72897f":"preds = model.predict(test_dataset)","236973c9":"preds = preds.reshape(-1)","de8c9f59":"submission = pd.DataFrame({'id':sample_submission['id'],'target':preds})","08813edc":"submission","ce4c905b":"submission.to_csv('submission.csv',index=False)","1ef50acd":"#preds = model.predict(test_dataset)\n#preds = preds.reshape(-1)\nplt.figure(figsize=(10,5))\nplt.hist(submission.target,bins=100, range = [0,1]);","a1bc1b28":"!kaggle competitions submit -c seti-breakthrough-listen -f submission.csv -m \"from colab - training result\"","4245dfda":"# cosine normalizing filter result","1cc634c7":"\n# preprocessing","3a2d79ba":"# References","3be5cc25":"# model define","a814b119":"## tf.data.Dataset setting","b38e948a":"# output distribution","1422afdf":"# submit","dc858bd1":"### https:\/\/www.kaggle.com\/assign\n### https:\/\/www.kaggle.com\/assign\/seti-with-colab-pro-gpu-tensorflow","d3807636":"## 1. you must setting with high-memory mode (25GB)\n## 2. if you use GPU, you must use colab PRO\n## 3. before unzip, do not store over 5.1GB data\n## 4. upload your API kaggle.json  on  default folder (content\/) \n> #### maybe, you have to launch once more(?)\n## after preprocessing, you can freely restart you session.\n## actually numpy memmap reading is \"very\" slow(250s reading one cycle of train dataset).\n> #### So most of time with small model, P100 and V100 have small difference.\n> #### but, use V100 if you can\n\n### download ~ 15min\n### unzip & processing ~ 1.1 hour\n### output shape = (None, 3, 273, 256)\n> ### if you want to use as (None, 273, 256, 3), \n> > ### than use tf.keras.layers.Permute((2,3,1))\n## if you restart, do not change validation_rate"}}