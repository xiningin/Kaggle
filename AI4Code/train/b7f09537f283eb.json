{"cell_type":{"e3dbd632":"code","795c6729":"code","9b32ee6e":"code","30cc3a12":"code","0dc3d601":"code","c7eca39a":"code","2d236182":"code","843f0bc7":"code","b05a0a92":"code","5b180528":"code","e909daf1":"code","515cae7d":"code","4605d951":"code","cdce5729":"code","ef609761":"code","03bf1980":"code","313826ac":"code","1c1ee76a":"code","129c97b3":"code","c729de0d":"code","5b06f13e":"code","9ae75337":"code","7233f40c":"code","72f275c7":"code","8e4453d1":"code","efc87945":"code","8ab90ba1":"code","648a0c0e":"code","a3a0890c":"code","8afdeada":"code","2c413635":"code","c898a859":"code","590d02ef":"code","35cf7aa0":"code","fc51a468":"code","ce2403f4":"code","a5c62172":"code","14e2e095":"code","c05108d0":"code","73837bb1":"code","8b939ced":"code","62d83ed9":"code","4d0c83ec":"code","8ed4ff15":"code","d3a0916e":"code","09f30ea6":"code","3960149e":"code","48adb754":"code","b2964530":"code","7f8eaff8":"code","e3064f19":"code","6bd2fefe":"markdown","92cc8ef9":"markdown","05974b15":"markdown","9b709644":"markdown","066ce626":"markdown","9d084ca6":"markdown","7e21aa3d":"markdown","cf388968":"markdown","5310750a":"markdown","0fe5a231":"markdown"},"source":{"e3dbd632":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","795c6729":"def read_file(filepath):\n    \n    with open(filepath) as file:\n        str_text = file.read()\n    \n    return str_text","9b32ee6e":"read_file(\"\/kaggle\/input\/mobidick\/moby_dick_four_chapters.txt\")","30cc3a12":"import spacy","0dc3d601":"nlp = spacy.load('en')\n","c7eca39a":"def remove_punc(text):\n    return [token.text.lower() for token in nlp(text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-\/:;<=>?@[\\\\]^_`{|}~\\t\\n ']","2d236182":"data = read_file(\"\/kaggle\/input\/mobidick\/moby_dick_four_chapters.txt\")","843f0bc7":"tokens = remove_punc(data)\ntokens","b05a0a92":"len(tokens)","5b180528":"# for the 25 words we will predict next 26th word\n\ntrain_len = 25+1\n\ntext_seq=[]\n\nfor i in range(train_len,len(tokens)):\n    seq= tokens[i-train_len:i]\n    text_seq.append(seq)","e909daf1":"print(text_seq[0])","515cae7d":"print(text_seq[1])","4605d951":"' '.join(text_seq[0])","cdce5729":"' '.join(text_seq[1])","ef609761":"' '.join(text_seq[2])","03bf1980":"#tokenize the sequences so that the keras can undersand\nfrom keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(text_seq)\nsequences = tokenizer.texts_to_sequences(text_seq)\nprint(sequences[0])","313826ac":"# to find out the ID of the word, we can use index_word from tokenizer\n\nfor i in sequences[0]:\n    print(f\"{i} : {tokenizer.index_word[i]}\")\n","1c1ee76a":"#this gives the overall word count of each words\ntokenizer.word_counts","129c97b3":"sequences = np.array(sequences)","c729de0d":"sequences","5b06f13e":"X = sequences[:,:-1]","9ae75337":"X.shape","7233f40c":"y = sequences[:,-1]\ny","72f275c7":"from keras.utils import to_categorical","8e4453d1":"vocabulary_size = len(tokenizer.word_counts)","efc87945":"y = to_categorical(y, num_classes=vocabulary_size+1)\n","8ab90ba1":"vocabulary_size","648a0c0e":"y.shape","a3a0890c":"seq_len= X.shape[1]\nseq_len","8afdeada":"from keras.models import Sequential\nfrom keras.layers import Dense,LSTM, Embedding","2c413635":"#Embedding: The first argument vocabulary_size is the number of distinct words in the training set.\n#The second argument seq_len indicates the size of the embedding vectors. \n#The input_length argumet, of course, determines the size of each input sequence.\n\ndef lstm_model(vocabulary_size,seq_len):\n    model= Sequential()\n    model.add(Embedding(vocabulary_size,seq_len,input_length=seq_len))\n    model.add(LSTM(50,return_sequences=True))  #50 is because of sequence length is 25 and we are using twice of it\n    model.add(LSTM(50))\n    model.add(Dense(50,activation='relu'))\n    \n    model.add(Dense(vocabulary_size,activation='softmax'))\n    \n    model.compile(loss='categorical_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n    \n    model.summary()\n    \n    return model","c898a859":"model=lstm_model(vocabulary_size+1, seq_len)","590d02ef":"model.fit(X, y, batch_size=128, epochs=300,verbose=1)","35cf7aa0":"model.fit(X, y, batch_size=128, epochs=100,verbose=1)","fc51a468":"model.fit(X, y, batch_size=128, epochs=200,verbose=1)","ce2403f4":"from pickle import dump,load\n\nmodel.save('lstmModel_90pc.h5')\ndump(tokenizer, open('token1', 'wb'))","a5c62172":"from random import randint\nfrom pickle import load\nfrom keras.models import load_model\nfrom keras.preprocessing.sequence import pad_sequences","14e2e095":"loaded_model = load_model('lstmModel_90pc.h5')","c05108d0":"loaded_model.summary()","73837bb1":"tokenizer = load(open ('token1','rb'))","8b939ced":"tokenizer.word_counts","62d83ed9":"encoded_text = tokenizer.texts_to_sequences([\"hello there\"])\n\nencoded_text","4d0c83ec":"def generate_new_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n    \n    output_text=[]\n    \n    input_text = seed_text\n    \n    \n    for i in range(num_gen_words):\n        \n        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n        \n        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n        \n        pred_word_ind = model.predict_classes(pad_encoded, verbose=0)[0]\n        \n        pred_word = tokenizer.index_word[pred_word_ind] \n        \n        input_text += ' ' + pred_word\n        \n        output_text.append(pred_word)\n        \n    return ' '.join(output_text)\n        \n    \n    ","8ed4ff15":"text_seq[0]","d3a0916e":"import random\nrandom.seed()\nrandom_pick = random.randint(0,len(text_seq))","09f30ea6":"print(random_pick)","3960149e":"text_seq[6677]","48adb754":"random_seed_text = text_seq[random_pick]\nrandom_seed_text","b2964530":"seed_text = ' '.join(random_seed_text)\n\nseed_text","7f8eaff8":"len(seed_text)","e3064f19":"generate_new_text(loaded_model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=50)","6bd2fefe":"# Generating New Text","92cc8ef9":"each of these rows represent a single line in text. and we are shifting one word over, 14 which is in the index 1 in line1 becomes index 0 in line 2.\n\nThe features are  the first 25 elements and the label is the last element of each line.","05974b15":"# LSTM Model","9b709644":"now we have to convert our sequences to numpy array","066ce626":"In this notebook, we are going to predict the text based on the learnings from moby dick book.","9d084ca6":"removing unwanted symbols which adds no value ","7e21aa3d":"From the above two sequences we can see that each sequence is one token over the other.","cf388968":"# Tokenize","5310750a":"so what we have done here is we have replaced the sequences of text to numbers and each of these numbers is an ID for a particular word.\n\n'call me ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on'\n\n[956, 14, 263, 51, 261, 408, 87, 219, 129, 111, 954, 260, 50, 43, 38, 315, 7, 23, 546, 3, 150, 259, 6, 2712, 14, 24]\n","0fe5a231":"these are the unique id's for each of the word and if the word is repeated in some other sentence, it will have the same id."}}