{"cell_type":{"964a08b4":"code","a02c613e":"code","953248e3":"code","f7f6b957":"code","8aff8eef":"code","a401d960":"code","ea2d6de6":"code","5fee6282":"code","389e6d04":"code","12758799":"code","dd7d4a61":"code","a60c0532":"code","f2b11af3":"code","d556fd12":"code","5e75ea31":"code","b7c2de1e":"code","88232a8d":"code","a071f7d5":"code","42adf9ff":"code","1e591618":"code","4fc03988":"code","05625b9b":"markdown","98064997":"markdown","6e05d176":"markdown","ab9767a0":"markdown","bc276980":"markdown","4f12c7bd":"markdown","9e02b597":"markdown","839bd0ee":"markdown","9e1927bd":"markdown","3ca335c4":"markdown","dcb84de8":"markdown","16f65572":"markdown","3a06290c":"markdown","9c80bdcb":"markdown","0cfd849f":"markdown","553f4d6e":"markdown","e715d705":"markdown","5b7ccdc9":"markdown","8fea5394":"markdown","dd7954e9":"markdown","f9a860f0":"markdown","5ef53f39":"markdown","38a795d3":"markdown"},"source":{"964a08b4":"from PIL import ImageTk, Image  \nimage = Image.open(\"..\/input\/glassss\/Untitledw21.png\")\nimage","a02c613e":"import pandas as pd # Importing pandas for data manipulation and analysis\nimport numpy as np # Importing python linear algebra library to do work with arrays\nfrom sklearn.model_selection import train_test_split # to split the data into train and test sets\nimport seaborn as sns # to import a python library to create alluring and communicative plots and graphs\nimport matplotlib.pyplot as plt # to import the ploting library of python language\nfrom sklearn.preprocessing import StandardScaler #Importing the Standard Sclaer from sklearn.preprocessing\nfrom sklearn.model_selection import GridSearchCV # Import grid search to choose the best parameters of model\nimport warnings # to import warnings\nwarnings.filterwarnings('ignore') # to import warnings as 'ignore'\nfrom sklearn.metrics import accuracy_score # Import accuracy score function of python that helps to evaluate models","953248e3":"df=pd.read_csv(\"..\/input\/glass\/glass.csv\") # to import csv file as data frame\nprint(df) # to see the dataframe","f7f6b957":"#Checking For Null values in our datasets and then removing the same.\npd.DataFrame(df.isna().sum()) #This will give the snapshot if me have any null values in our dataset.\n","8aff8eef":"df.info()","a401d960":"# In order to find the row that has occured more than once in a dataset\nduplicate_rows = df[df.duplicated()]\nprint(duplicate_rows)","ea2d6de6":"df.drop_duplicates() # drop duplicate values","5fee6282":"# visualizing numeric variables using seaborn\nsns.set(font_scale=1.5)\nsns.set_style(style='darkgrid')\nf, axes = plt.subplots(3,3,figsize=(25,25))\nsns.distplot( df[\"RI\"] ,hist_kws=dict(edgecolor=\"k\", linewidth=2, color=\"#0000ff\"),color=\"black\", ax=axes[0, 0])\nsns.distplot( df[\"Na\"] ,hist_kws=dict(edgecolor=\"k\", linewidth=2, color=\"#00cc00\"),color=\"black\", ax=axes[0, 1])\nsns.distplot( df[\"Mg\"] ,hist_kws=dict(edgecolor=\"k\", linewidth=2, color=\"#e68a00\"),color=\"black\", ax=axes[0, 2])\nsns.distplot( df[\"Al\"] , hist_kws=dict(edgecolor=\"k\", linewidth=2,color=\"#992600\"),color=\"black\", ax=axes[1, 0])\nsns.distplot( df[\"Si\"] ,hist_kws=dict(edgecolor=\"k\", linewidth=2, color=\"#e600ac\"),color=\"black\", ax=axes[1, 1])\nsns.distplot( df[\"K\"] ,hist_kws=dict(edgecolor=\"k\", linewidth=2, color=\"skyblue\"),color=\"black\", ax=axes[1, 2])\nsns.distplot( df[\"Ca\"] ,hist_kws=dict(edgecolor=\"k\", linewidth=2,color='orange'), color=\"black\", ax=axes[2, 0])\ndf['Ba'].plot.hist(color=['olive'],edgecolor=\"k\", linewidth=2,ax=axes[2, 1],title='Ba')\nsns.distplot( df[\"Fe\"] ,hist_kws=dict(edgecolor=\"k\", linewidth=2, color=\"red\"),color=\"black\", ax=axes[2, 2])","389e6d04":"# visualizing categorical features\ndf['Type'].value_counts().plot.bar(color=['olive','skyblue','red','orange','pink','blue'],title='Type of glass',edgecolor=\"k\", linewidth=2)","12758799":"# create correlation matrix\ncorrMatrix = round(df.corr(),1)\nplt.figure(figsize=(16,11))\n# to plot the matrix as heat map\nsns.heatmap(corrMatrix,annot=True,cmap='seismic',linewidths=2,linecolor='black')\nplt.title(\"Heatmap Correlation of Heart Failure Prediction\", fontsize = 23)\nplt.show()","dd7d4a61":"X = df.drop('Type', axis=1).values #Feature datasets for the purpose of calculation.\ny = df['Type'].values #Target data sets for the purpose of calculations.\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.85, random_state=42,shuffle=True)#Splitting the data into train and test sets.\npd.DataFrame(X_train) # to have a look at trained dataset","a60c0532":"#creating an object of Scaler\nscaler = StandardScaler()\n\n#Fitting the training features\nscaler.fit(X_train)\n\n#transforming the train features\nX_train_scaled = scaler.transform(X_train)\n\n#transforming the test features\nX_test_scaled = scaler.transform(X_test)","f2b11af3":"# create two separate train and test lists\ntrain_accuracies=[]\ntest_accuracies=[]","d556fd12":"# Part 1 -: SELECT THE BEST HYPERPARMETERS WITH HELP OF GRID SEARCH\nfrom sklearn.svm import SVC \n# defining parameter range \nHyper_parameters = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf']}  \nGridSearch_svc = GridSearchCV(estimator = SVC(),\n                               param_grid = Hyper_parameters,\n                               cv = 15,\n                               n_jobs = -1)\nGridSearch_svc.fit(X_train_scaled, y_train)\n\nprint(\"Best hyperparameters for model:\"+str(GridSearch_svc.best_params_))\nprint(\"Best estimator for model:\"+str(GridSearch_svc.best_estimator_))","5e75ea31":"#Part 2 -: Build a model with the help of best parameters\nsvc = SVC(C=100, gamma=0.1,kernel='rbf',probability=True)\nsvc.fit(X_train_scaled, y_train)\n# to predict the target values\npred_svc_test = svc.predict(X_test_scaled)\npred_svc_train = svc.predict(X_train_scaled)\ntrain_accuracy_svc=accuracy_score(y_train,pred_svc_train)*100\ntest_accuracy_svc=accuracy_score(y_test,pred_svc_test)*100\ntrain_accuracies.append(train_accuracy_svc)\ntest_accuracies.append(test_accuracy_svc)\n# to find the accuracy of the model on training and testing data\nprint(\"Accuracy on  Train data : {}\".format(accuracy_score(y_train,pred_svc_train)*100) )\nprint(\"Accuracy on  TEST data : {}\".format(accuracy_score(y_test,pred_svc_test)*100) )","b7c2de1e":"# Part 1 -: Hypertuning of parameters\nfrom sklearn.ensemble import RandomForestClassifier\n#The structure that Scikit-learn needs to run Grid search\nparam_grid={'max_depth':[3,4,5],\n           'max_leaf_nodes':[10,15,20],\n            'min_samples_leaf':[10,15,20,25]}\nfrom sklearn.model_selection import GridSearchCV\n#applying GridSearch on a Decisiontree classifier with a 3 different parameters:\ngrid_search = GridSearchCV(RandomForestClassifier(n_estimators=11,random_state=573),param_grid,cv=10,return_train_score=True)\ngrid_search.fit(X_train_scaled,y_train)\nprint(\"Best parameters:\"+str(grid_search.best_params_))\nprint(\"Best estimator:\"+str(grid_search.best_estimator_))","88232a8d":"#Part 2 -: Build a model with the help of best parameters\nrf = RandomForestClassifier(max_depth=5, max_leaf_nodes=10, min_samples_leaf=10,\n                       n_estimators=11, random_state=573)\nrf.fit(X_train_scaled, y_train)\n# to predict the target values\npred_rf_test = rf.predict(X_test_scaled)\npred_rf_train = rf.predict(X_train_scaled)\ntrain_accuracy_rf=accuracy_score(y_train,pred_rf_train)*100\ntest_accuracy_rf=accuracy_score(y_test,pred_rf_test)*100\ntrain_accuracies.append(train_accuracy_rf)\ntest_accuracies.append(test_accuracy_rf)\n# to find the accuracy of the model on training and testing data\nprint(\"Accuracy on  Train data : {}\".format(accuracy_score(y_train,pred_rf_train)*100) )\nprint(\"Accuracy on  TEST data : {}\".format(accuracy_score(y_test,pred_rf_test)*100) )","a071f7d5":"# Part 1 -: Hypertuning of parameters\n#XGBoost\nfrom xgboost import XGBClassifier\nxg=XGBClassifier(random_state=573)\n\n#List Hyperparameters that we want to tune.\n\nparameter_grid_xg={'learning_rate':[0.05, 0.10, 0.15, 0.20],'max_depth':[3,4,5],'gamma':[ 0.0, 0.1, 0.2 , 0.3]}\ngridsearch_xg = GridSearchCV(xg, parameter_grid_xg,cv=15)\ngridsearch_xg.fit(X_train_scaled, y_train);\n\n#Get best hyperparameters\ngridsearch_xg.best_params_","42adf9ff":"#Part 2 -: Build a model with the help of best parameters\nxg =XGBClassifier(gamma=0.2,learning_rate=0.05,max_depth=4,random_state=573)\nxg.fit(X_train_scaled, y_train)\n# to predict the target values\npred_xg_test = xg.predict(X_test_scaled)\npred_xg_train = xg.predict(X_train_scaled)\ntrain_accuracy_xg=accuracy_score(y_train,pred_xg_train)*100\ntest_accuracy_xg=accuracy_score(y_test,pred_xg_test)*100\ntrain_accuracies.append(train_accuracy_xg)\ntest_accuracies.append(test_accuracy_xg)\n# to find the accuracy of the model on training and testing data\nprint(\"Accuracy on  Train data : {}\".format(accuracy_score(y_train,pred_xg_train)*100) )\nprint(\"Accuracy on  TEST data : {}\".format(accuracy_score(y_test,pred_xg_test)*100) )","1e591618":"# create a list of labels of build models\nlabel = ['SVC','Random Forest','XG Boost']\nprint(label)\n\n#checking the train and test accuracies for all the parameter values\ntrain_accuracy = [round(num, 2) for num in train_accuracies]\nprint(\"Train Accuracies \"+str(train_accuracy))\n\ntest_accuracy = [round(num, 2) for num in test_accuracies]\nprint(\"\\nTest Accuracies \"+str(test_accuracy))","4fc03988":"#Accuracy dataframe\nAcc_df = pd.DataFrame({'Model':label,'Train Accuracy(%)': train_accuracy,'Test Accuracy(%)': test_accuracy})\n# Plot the heat map for the dataframe\nAcc_df.style.background_gradient(cmap='Blues')","05625b9b":"# **Part 5 -: Models**#","98064997":"# ***Glass Classification by Machine learning***","6e05d176":"# **Part 3 -: Exploratory Data Analysis**# ","ab9767a0":"# **Part 2 -: Data Preprocessing**# ","bc276980":"> ### Analysis -:According to the above output, 2nd type of glass has occured most frequently in the dataset.","4f12c7bd":"# **Part 4 -: Splitting and Scaling of Data**# ","9e02b597":"> ### Analysis -: Data contains no missing values, which is a good thing !","839bd0ee":"# **Part 1 -: Importing Libraries and Data**# ","9e1927bd":"> ### Analysis -: As we can see frome the above output, that the data is pretty much consistence as data type of all values of a particular feature is similar!\n> ### For example, all the values in \"Type\" column have int64 as data type.","3ca335c4":" ## **i)Support Vector Classification (SVC) Model**","dcb84de8":"> ### Analysis -: It is important to remove duplicate rows in order to avoid bais in dataset. As our dataset contains some duplicate rows, therefore, I have removed those rows to abstain any partiality in data","16f65572":"# **Part 6 -: Compare the models**#","3a06290c":"##  *i)Checking for missing data*","9c80bdcb":" ## **ii)Univariant Analysis of Categorical Features**","0cfd849f":" ## **iii)Checking duplicate values**","553f4d6e":" ## **ii)Random Forest Classifier Model**","e715d705":" ## **iii)XG Boost Classifier Model**","5b7ccdc9":"> ### Analysis -:i) RI -: The graph shows multimodal and right skewed distribution. It depicts that the value of refractive index for most of samples of different elements is in between 1.515 to 1.520.\n> ### ii)Na,Al and Fe has Unimodal distribution with the highest values lying in the range of 12 - 14, 1 - 2, -0.1 to 0.1 respectively.\n> ### iii)However,Ca,K,Si,Mg have bimodal distribution.Whereas, distribution of Ba is right skewed.","8fea5394":"> ### Analysis -: As per above output, I choose XG Boost as the best model as it's Train and Test accuracies are more than other models.","dd7954e9":" ## **ii)Checking Inconsistency in the data values**","f9a860f0":" ## **i)Univariant Analysis of Numerical Features**","5ef53f39":"> ### Analysis -:According to the correlation matrix, we can find out some interesting facts :\n> ### i) Calcium and refractive index are positively correlated to each other,which means on increasing the value of one, other will increase somewhat linearly.\n> ### ii) However, Type and Mg are negatively correlated to each other which demonstrate that both are nearly inversely proportional to each other.","38a795d3":" ## **iii)Multivariant analysis of all the features**"}}