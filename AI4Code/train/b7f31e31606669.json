{"cell_type":{"00593ba3":"code","d7b86b00":"code","e1817e15":"code","cfce0279":"code","3fed5d87":"code","7c12cc15":"code","e73c89e9":"code","fa72f140":"code","83c2fb80":"code","b7299157":"code","c9af0c66":"code","ae913b40":"code","fa27c512":"code","a759249d":"code","345594a1":"code","56eb39ca":"code","5f6670c9":"code","03838f76":"code","07986bb3":"code","af651813":"code","3cd61bb4":"code","d6a99852":"code","e565ab9e":"code","dde40613":"code","af7ab817":"code","8bf54259":"code","b1668dec":"code","9b28588e":"code","805c82c1":"code","ef5ed2a0":"code","aec1623e":"code","3b37e7d1":"code","71c368a1":"code","91d9eeca":"code","f1025139":"code","b87f68f7":"code","5e246a5d":"code","1ea0e437":"code","db0c7088":"code","dc613d21":"code","7a3f024e":"code","968e7a0a":"code","8e3cf352":"code","05ca2fe1":"code","bc5c891a":"code","32e818ae":"code","dd5566ff":"code","53c22090":"code","074f4800":"code","2835b967":"code","7c9e6a45":"code","ff356562":"code","89ddc069":"code","842dc356":"code","02961214":"code","7b26324e":"code","c880c1a7":"code","4669f2d4":"code","f66851eb":"code","59b90c8f":"code","ea269330":"code","a6a99994":"code","44e30b94":"code","9773eb33":"code","710ee10f":"code","bc204415":"code","b4cc5f5b":"code","051715f2":"code","eb9c7627":"code","9ec1a827":"code","693252e9":"code","8f3f1e4e":"code","522edadd":"code","1d7fb49e":"code","c1f747d7":"code","a3fc5fd0":"code","b8c4b514":"code","88a291ad":"code","b0e49ffb":"code","7ea7f072":"code","e4ad5678":"code","99cbd194":"code","edbbf023":"markdown","4608c407":"markdown","20fd88bc":"markdown","bbb09069":"markdown","312b0346":"markdown","380bd099":"markdown","4200b6df":"markdown","a6114c15":"markdown","2f61e43e":"markdown","ca5220a0":"markdown","a0c5897d":"markdown","388a4577":"markdown","4ede7945":"markdown","170b171e":"markdown","a75fa49d":"markdown","475dc6e7":"markdown","8f1e3704":"markdown","619ad9c5":"markdown","7b6fd204":"markdown","12aa52de":"markdown","26e3906f":"markdown"},"source":{"00593ba3":"# Importing Libraries\nimport os\nimport warnings\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler # Scaling the data set\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import RFE\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, roc_curve, auc, roc_auc_score, f1_score, plot_confusion_matrix, precision_score, recall_score\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report, roc_auc_score, roc_curve\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom xgboost import XGBClassifier\nimport itertools\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")\nsns.set(style=\"whitegrid\")","d7b86b00":"# Get the Datase directory path from Kaggle\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e1817e15":"# Display all the features\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_colwidth', -1) # make sure data and columns are displayed correctly without purge\npd.options.display.float_format = '{:20,.2f}'.format # display float value with correct precision ","cfce0279":"# Read Data using pandas\ncred_df=pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\ncred_df.head(2)","3fed5d87":"# Check Shape\ncred_df.shape","7c12cc15":"# Check columns\ncred_df.info(verbose=True)","e73c89e9":"dups=cred_df.duplicated()\nprint('Number of duplicate rows = %d' % (dups.sum()))\ncred_df[dups].head(10)","fa72f140":"# Create new DF after removing duplicates\ncred=cred_df[~dups]\nprint(\"Old shape :\",cred_df.shape)\nprint(\"New shape :\",cred.shape)","83c2fb80":"# Method for missing value pecentage\ndef check_missing_precentage(col_list, input_df):\n    null_cols = 100*input_df.loc[:,col_list].isnull().sum()\/len(input_df)\n    df = pd.DataFrame(null_cols).reset_index()\n    df.rename(columns={\"index\": \"Features\", 0:'Null_Percentage'}, inplace=True)\n    return df","b7299157":"# Check missing percentage values for all the columns\ncheck_missing_precentage(list(cred.columns), cred)","c9af0c66":"# Create sparate DF for  FRAUD and Non-Fraud Cases\ncred_nf=cred[cred['Class']==0] # Fraud\ncred_f=cred[cred['Class']==1] # Non-Fraud","ae913b40":"# Check the data distribution for both cases\nprint('Non-fraudulent cases is {} % of total'.format(100*len(cred_nf)\/len(cred['Class'])))\nprint('Fraudulent cases is {} % of total'.format(100*len(cred_f)\/len(cred['Class'])))\nprint('='*50)\nprint(cred['Class'].value_counts())\nprint('='*50)\ncred['Class'].value_counts().plot(kind='bar')","fa27c512":"# Data decription for fraud cases\ncred_f['Amount'].describe()","a759249d":"# Data decription for non-fraud cases\ncred_nf['Amount'].describe()","345594a1":"sns.boxplot(x = 'Class', y = 'Time', data = cred)","56eb39ca":"sns.barplot(x='Class', y='Amount', data=cred)","5f6670c9":"# Compute the correlation matrix\ncorr = cred.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(15, 12))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(200, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","03838f76":"# Keep the copy of df\nmodeling_df = cred.copy()","07986bb3":"# Variable separation\ndef variable_separation(df):\n    Y = df['Class']\n    X = df.drop(['Class'], axis=1)                  # We can't take mobile number for modeling\n    return X,Y","af651813":"# Feature Scaling\ndef scaling(df):\n    scaler = StandardScaler()\n    X_cols = list(df.columns) \n    X_scaled = scaler.fit_transform(df)\n    X_scaled = pd.DataFrame(X_scaled, columns=X_cols)\n    \n    return X_scaled","3cd61bb4":"# Split the Data using PCA\ndef split_data(X, y, num_of_features, training_size, is_pca):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=training_size, random_state=42)\n    print('X_train shape: ', X_train.shape)\n    print('X_test shape: ', X_test.shape)\n    \n    #Applying PCA\n    if is_pca:\n        pca = IncrementalPCA(n_components=num_of_features)    \n        X_train = pca.fit_transform(X_train)\n        X_test = pca.transform(X_test)\n        print('X_train_pca shape: ', X_train.shape)\n        print('X_test_pca shape: ', X_test.shape)\n\n    return X_train, X_test, y_train, y_test","d6a99852":"# Method to plot scree plot\ndef scree_plot(X_param):\n    pca = PCA(random_state=101)\n    pca.fit(X_param)\n    var_cumu = np.cumsum(pca.explained_variance_ratio_)\n    fig = plt.figure(figsize=[12,8])\n    plt.vlines(x=27, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\n    plt.hlines(y=0.98, xmax=60, xmin=0, colors=\"g\", linestyles=\"--\")\n    plt.plot(var_cumu)\n    plt.ylabel(\"Cumulative variance explained\")\n    plt.xlabel(\"No. of features\")\n    plt.show()\n    return pca","e565ab9e":"# GLobal Df for model summary\nmodels_summary = pd.DataFrame()","dde40613":"# Method to get Summary of models\ndef summary_of_models(df) :\n    global models_summary \n    models_summary = models_summary.append(df,ignore_index=True)","af7ab817":"# Method to plot confusion matrix\ndef draw_confusion_matrix(cm):\n    classes=[0,1]\n    cmap=plt.cm.BuPu\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title('Confusion matrix')\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","8bf54259":"# Method to show different evaluation metrics    \ndef show_model_metrics(y_test,y_pred,model_name):\n    cp = confusion_matrix(y_test,y_pred)\n    plt.figure()\n    draw_confusion_matrix(cp)\n    plt.show()\n    \n    accuracy = round(accuracy_score(y_test,y_pred),2)\n    recall = round(recall_score(y_test,y_pred),2)\n    precision = round(precision_score(y_test,y_pred),2)\n    auc = round(roc_auc_score(y_test,y_pred),2)\n    f1 = round(f1_score(y_test,y_pred),2)\n    \n    data = [[model_name,accuracy,recall,precision,auc,f1]] \n    df = pd.DataFrame(data, columns = ['Model', 'Accuracy','Precision','Recall','AUC','F1'])\n    summary_of_models(df)\n    return df ","b1668dec":"# Model evaluation method\ndef evaluate_model(dt_classifier):\n    print(\"Train Accuracy :\", accuracy_score(y_train, dt_classifier.predict(X_train)))\n    print(\"Train Confusion Matrix:\")\n    print(confusion_matrix(y_train, dt_classifier.predict(X_train)))\n    print(\"-\"*50)\n    print(\"Test Accuracy :\", accuracy_score(y_test, dt_classifier.predict(X_test)))\n    print(\"Test Confusion Matrix:\")\n    print(confusion_matrix(y_test, dt_classifier.predict(X_test)))","9b28588e":"# Plot ROC AUC Curve\ndef roc_auc_curve(X_ts, y_ts, y_pred_probability, classifier_name):\n    y_pred_prob = y_pred_probability[:,1]\n    fpr, tpr, thresholds = roc_curve(y_ts, y_pred_prob)\n    plt.plot([0,1],[0,1], 'k--')\n    plt.plot(fpr, tpr, label=f'{classifier_name}')\n    plt.xlabel('Flase +ve rate')\n    plt.ylabel('True +ve rate')\n    plt.title(f'{classifier_name} - ROC Curve')\n    plt.show()\n    \n    return print(f'AUC score (ROC): {roc_auc_score(y_ts, y_pred_prob)}\\n')","805c82c1":"# Method to plot Heat Map\ndef plot_heat_map(X_var, fig_size):\n    plt.figure(figsize=fig_size)\n    g=sns.heatmap(X_var.corr(method='pearson', min_periods=1), \n                  annot=True,\n                  cbar_kws={'fraction' : 0.01},\n                  cmap='OrRd',\n                  linewidth=1)\n    g.set_xticklabels(g.get_xticklabels(), rotation=45, horizontalalignment='right')\n    g.set_yticklabels(g.get_yticklabels(), rotation=45, horizontalalignment='right')\n    plt.show()","ef5ed2a0":"# Define X and y\nX, y = variable_separation(modeling_df)\nX.shape","aec1623e":"X.head(3)","3b37e7d1":"y.head(3)","71c368a1":"# Fraud percentage\nround(100*y.sum()\/len(y),2)","91d9eeca":"# Scaling X\nX_scaled = scaling(X)\nX_scaled.shape","f1025139":"X_scaled.head(3)","b87f68f7":"# Split into test and train set\nX_train, X_test, y_train, y_test = split_data(X_scaled, y, np.NaN, 0.7, False)  # Not using PCA : that's why np.Nan and False  ","5e246a5d":"# Modeling\nlog_reg = LogisticRegression(class_weight='balanced')\nreg_model = log_reg.fit(X_train, y_train)","1ea0e437":"# Applying RFE\nrfe = RFE(log_reg, 15)\nrfe = rfe.fit(X_train, y_train)","db0c7088":"# Print all the columns as RFE ranking\nlist(zip(X_train.columns, rfe.support_, rfe.ranking_))","dc613d21":"top_cols = list(X_train.columns[rfe.support_])\nprint('Top 15 features :',top_cols)","7a3f024e":"# New X_train and X_test\nX_train = X_train[top_cols]\nX_test = X_test[top_cols]","968e7a0a":"# Modeling with TOP features\nclassifier = LogisticRegression(random_state=0, penalty= 'l2')\nclassifier.fit(X_train, y_train)","8e3cf352":"# Perdiction\nprediction = classifier.predict(X_test)\npred_probs_test = classifier.predict_proba(X_test)","05ca2fe1":"# Checking R2 score\n\"{:2.2}\".format(metrics.roc_auc_score(y_test, pred_probs_test[:,1]))","bc5c891a":"# Show model metrics\nshow_model_metrics(y_test, prediction, \"LR(Default)\")","32e818ae":"# visualizing ROC Curve\nroc_auc_curve(X_test, y_test, pred_probs_test, 'Logistic regression')","dd5566ff":"print(f'Train score : {classifier.score(X_train, y_train)}')\nprint(f'Test score : {classifier.score(X_test, y_test)}')","53c22090":"# create a cross-validation scheme\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 42)\n\n# step-2: specify range of hyperparameters to tune\nhyper_params = [{'n_features_to_select': list(range(1, 15))}]\n\n\n# perform grid search\n# 1. specify model\nlr_model = LogisticRegression(class_weight='balanced')\nlr_model.fit(X_train, y_train)\nrfe = RFE(lr_model)             \n\n# 2.call GridSearchCV()\nmodel_cv = GridSearchCV(estimator = rfe, \n                        param_grid = hyper_params, \n                        scoring= 'roc_auc', \n                        cv = folds, \n                        verbose = 1,\n                        return_train_score=True)                    ","074f4800":"%%time\nmodel_cv.fit(X_train, y_train)","2835b967":"# cv results\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","7c9e6a45":"# plotting cv results\nplt.figure(figsize=(16,6))\n\nplt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_test_score\"])\nplt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_train_score\"])\nplt.xlabel('number of features')\nplt.ylabel('roc-auc-score')\nplt.title(\"Optimal Number of Features\")\nplt.legend(['test score', 'train score'], loc='upper left')","ff356562":"# final model\nn_features_optimal = 8\n\nfinal_lr_model = LogisticRegression()\nfinal_lr_model.fit(X_train, y_train)\n\nrfe = RFE(final_lr_model, n_features_to_select=n_features_optimal)\nrfe = rfe.fit(X_train, y_train)\n\n# predict prices of X_test\ny_pred = final_lr_model.predict(X_test)\npred_probs_test = final_lr_model.predict_proba(X_test)\n\n# Show model metrics\nshow_model_metrics(y_test, y_pred, \"LR (Tuned)\")","89ddc069":"roc_auc_curve(X_test, y_test, pred_probs_test, 'Logistic regression')","842dc356":"# train-test split\nX_train, X_test, y_train, y_test = split_data(X_scaled, y, np.NaN, 0.7, False)","02961214":"# Fitting Decision Tree Classifier Model\ndt = DecisionTreeClassifier(max_depth=3, class_weight='balanced')\ndt.fit(X_train, y_train)","7b26324e":"# Storing train and test predictions\ny_train_pred = dt.predict(X_train)\ny_test_pred = dt.predict(X_test)","c880c1a7":"# printing the confusion matrix\nshow_model_metrics(y_test, y_test_pred, \"DT(Default)\")","4669f2d4":"# Defining the parameter grid for tuning\nparams = {\n    'max_depth': [2, 3, 5, 10, 20],\n    'min_samples_leaf': [5, 10, 20, 50, 100],\n    'criterion': [\"gini\", \"entropy\"]\n}","f66851eb":"# Instantiate the randomized search model\nrandom_search = RandomizedSearchCV(estimator=dt, \n                           param_distributions=params, \n                           cv=4, n_jobs=-1, verbose=1, scoring = \"recall\")","59b90c8f":"# Fitting the model\nrandom_search.fit(X_train, y_train)","ea269330":"score_df = pd.DataFrame(random_search.cv_results_)\nscore_df.head()","a6a99994":"# checking top 5 sets of params based on mean test scores\nscore_df.nlargest(5,\"mean_test_score\")","44e30b94":"# checking best estimator for decision tree\ndt_best = random_search.best_estimator_\ndt_best","9773eb33":"# checking the metrics\nprint(classification_report(y_test, dt_best.predict(X_test)))","710ee10f":"# storing predictions\nprediction = random_search.predict(X_test)","bc204415":"# printing the confusion matrix\nshow_model_metrics(y_test, prediction, \"DT(Tuned)\")","b4cc5f5b":"from xgboost import XGBClassifier","051715f2":"# train-test split\nX_train, X_test, y_train, y_test = split_data(X_scaled, y, np.NaN, 0.7, False)","eb9c7627":"# fit model on training data\nxgb_model = XGBClassifier(class_weight='balanced')\nxgb_model.fit(X_train, y_train)","9ec1a827":"# Storing train and test predictions\ny_train_pred = xgb_model.predict(X_train)\ny_test_pred = xgb_model.predict(X_test)","693252e9":"# printing the confusion matrix\nshow_model_metrics(y_test, y_test_pred, \"XGBoost(Default)\")","8f3f1e4e":"# 1st-Run for best hyperparameters\nparameters = {'learning_rate': [0.1, 0.2, 0.3, 0.4, 0.5],\n              'max_depth': [2, 4, 6, 8, 10],\n              'min_child_weight': [3, 7, 11, 19, 25],\n              'n_estimators': [50, 100, 150, 200, 300, 500]}","522edadd":"scorer = metrics.make_scorer(metrics.roc_auc_score,\n                             greater_is_better=True,\n                             needs_proba=True,\n                             needs_threshold=False)","1d7fb49e":"clf_xgb = RandomizedSearchCV(estimator=xgb_model,\n                                       param_distributions=parameters,\n                                       n_jobs=-1,\n                                       cv=3,\n                                       scoring=scorer,\n                                       refit=True)","c1f747d7":"%%time\nclf_xgb.fit(X_train, y_train)","a3fc5fd0":"print(clf_xgb.best_params_)\nprint(clf_xgb.best_score_)\nprint(clf_xgb.best_estimator_)","b8c4b514":"final_model = XGBClassifier(class_weight='balanced',\n                                learning_rate=0.2,\n                                max_depth=4,\n                                min_child_weight=11,\n                                n_estimators=100)\n\nfinal_model.fit(X_train, y_train)","88a291ad":"# Storing train and test predictions\ny_train_pred = final_model.predict(X_train)\ny_test_pred = final_model.predict(X_test)","b0e49ffb":"# printing the confusion matrix\nshow_model_metrics(y_test, y_test_pred, \"XGBoost(Tuned)\")","7ea7f072":"pred_probs_test = final_model.predict_proba(X_test)","e4ad5678":"roc_auc_curve(X_test, y_test, pred_probs_test, 'XGBoost(Tuned)')","99cbd194":"models_summary","edbbf023":"- There is **not any missing value** present in our dataset","4608c407":"- This is typically a **class imbalance** problem and will be **taken care during modeling**.\n- Here, **0: Non-fraudulent cases** and **1: Fraudulent cases**\n- We have **0.17%** Fraud Cases only.","20fd88bc":"#### Missing Values","bbb09069":"- If we see carefully then we found that, if we select 8 feature than we can get score around 99%","312b0346":"#### Check duplicate records","380bd099":"### XGBoost","4200b6df":"### Decision Tree","a6114c15":"### Define Re-Usable Methods","2f61e43e":"#### Check class balance\/imbalance","ca5220a0":"- We can clearly see, there are **1081 duplicate records** present in our data set. Let's get rid of these duplicates.","a0c5897d":"## Modeling","388a4577":"### Understand the Data","4ede7945":"#### Hyperparameter tuning with Radomized Search CV method","170b171e":"Lets' define some re-usable methods","a75fa49d":"### Logistic Regression","475dc6e7":"### Data cleaning and preparation","8f1e3704":"### EDA","619ad9c5":"- As we can clearly see, Average amount is greater in Fraudulent cases.","7b6fd204":"Let's perform GCV","12aa52de":"- As we can see after 2 feature there is not much improvement in the score. To make the model more explainable we can take 8 Features.","26e3906f":"#### Hyperparameter Tuning"}}