{"cell_type":{"419198c6":"code","b045a858":"code","55c70411":"code","50b87bef":"code","4b606caf":"code","84d2dc04":"code","4e254dd7":"code","06c6081b":"code","fa5ab816":"code","0c5b7e7a":"code","3f1275e4":"code","84d3bd17":"code","7e547935":"code","fcb2afc9":"code","3b9fcdff":"code","ef00097d":"code","56a96d49":"code","0bfd3085":"code","83bd53bb":"code","e300ac55":"code","8aa79635":"code","7772574a":"code","1704b38a":"code","da59bf09":"code","d759237e":"code","2801bef1":"code","2c8d52f2":"code","1ad3f1ca":"code","e4413c74":"code","8f15abc6":"markdown","e997803c":"markdown","018b8a09":"markdown","6d6e1820":"markdown","dc6c03c9":"markdown","dffa0c79":"markdown","b18adeaa":"markdown","4b8cd3c5":"markdown","4521d5ee":"markdown","af122c83":"markdown","9c680632":"markdown","524a927b":"markdown","d0ce7c4f":"markdown","2dd5d70f":"markdown","0baf8e1d":"markdown","dc1e34fc":"markdown","9fcc6400":"markdown","208c9c43":"markdown","e6bb9a1c":"markdown"},"source":{"419198c6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotting\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom rnn_utils import mae, mse, rmse, mape, evaluate # helper evaluation functions\nfrom keras import Sequential\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU","b045a858":"FILE_PATH = \"\/kaggle\/input\/electric-power-consumption-data-set\/household_power_consumption.txt\"\ndf = pd.read_csv(FILE_PATH, sep=\";\", parse_dates={'ds':['Date', 'Time']}, na_values=['nan', '?'], infer_datetime_format=True,low_memory=False)\ndf.head()","55c70411":"print(f\"Missing values: {df.isnull().sum().any()}\")\n# imputation with the columns means\nfor j in range(0,8):        \n  df.iloc[:,j]=df.iloc[:,j].fillna(df.iloc[:,j].mean())\n# checking for missing values\nprint(f\"Missing values: {df.isnull().sum().any()}\")","50b87bef":"df_resample = df.resample('D', on='ds').sum() \ndf_resample.rename(columns={\"Global_active_power\":\"y\"}, inplace=True)\ndf_resample = df_resample[['y']]\ndf_resample.head()","4b606caf":"def create_lags(df, days=7):\n    # create lagged data for features\n    for i in range(days):\n        df[\"Lag_{lag}\".format(lag=i+1)] = df['y'].shift(i+1)\n    return df\n\ndef create_features(X, time_steps=1, n_features=7):\n    # create 3d dataset for input\n    cols, names = list(), list()\n    for i in range(1, time_steps+1):\n        cols.append(X.shift(-time_steps))\n        names += [name + \"_\" + str(i) for name in X.columns]\n        agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    # drop rows with NaN values\n    agg.dropna(inplace=True)\n    agg = agg.values.reshape(agg.shape[0], time_steps, n_features)\n    return agg\n\ndef create_dataset(df, yhat):\n    # yhat needs to be scaled\n    preds = pd.DataFrame(yhat.flatten())\n    temp = pd.concat([df.iloc[:,0], preds])\n    temp.columns = ['y']\n    date_idx = pd.date_range(start='2006-12-23', periods=temp.shape[0])\n    temp.set_index(date_idx, inplace=True)\n    return temp","84d2dc04":"chosen = df_resample.copy()\nchosen = create_lags(chosen)\nchosen.dropna(inplace=True)\n\n# Fit scaler on training data only to prevent data leakage\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler_x = scaler.fit(chosen.iloc[:1096,1:])\nscaler_y = scaler.fit(chosen.iloc[:1096,0].values.reshape(-1,1))\n\nx_scaled = scaler_x.transform(chosen.iloc[:,1:])\ny_scaled = scaler_y.transform(chosen.loc[:,['y']])\n\nscaled = np.hstack((x_scaled, y_scaled))\nscaled = pd.DataFrame(scaled, index=chosen.index, columns=chosen.columns)\nprint(scaled.shape)\nscaled.head()","4e254dd7":"train = scaled[:1096]\nval = scaled[1096:1256]\ntest = scaled[1256:]\nx_train = train.drop([\"y\"],axis=1)\ny_train = train[\"y\"]\nx_val = val.drop([\"y\"],axis=1)\ny_val = val[\"y\"]\nx_test = test.drop([\"y\"],axis=1)\ny_test = test[\"y\"]","06c6081b":"x_train_np = create_features(x_train, 7, 7)\nx_val_np = create_features(x_val, 7, 7)\nx_test_np = create_features(x_test, 7, 7)\n#print(x_train_np.shape, x_val_np.shape, x_test_np.shape)\ny_test = y_test[:x_test_np.shape[0]]\ny_train = y_train[:x_train_np.shape[0]]\ny_val = y_val[:x_val_np.shape[0]]\n#print(y_train.shape, y_val.shape, y_test.shape)","fa5ab816":"def fit_model(m, units, x_train_np, x_val_np, verbose=False):\n    model = Sequential()\n    model.add(m (units = units, return_sequences = True, input_shape = [x_train_np.shape[1], x_train_np.shape[2]]))\n    #model.add(Dropout(0.2))\n    model.add(m (units = units))\n    #model.add(Dropout(0.2))\n    model.add(Dense(units = 1))\n    # Compile Model\n    model.compile(loss='mse', optimizer='adam')\n    # Fit Model\n    history = model.fit(x_train_np, y_train, epochs=50, batch_size=70, \n                        validation_data=(x_val_np, y_val), verbose=False, shuffle=False)\n    return model","0c5b7e7a":"RNN_model = fit_model(SimpleRNN, 64, x_train_np, x_val_np)\nLSTM_model = fit_model(LSTM, 64, x_train_np, x_val_np)\nGRU_model = fit_model(GRU, 64, x_train_np, x_val_np)","3f1275e4":"RNN_preds = RNN_model.predict(x_test_np)\nLSTM_preds = LSTM_model.predict(x_test_np)\nGRU_preds = GRU_model.predict(x_test_np)","84d3bd17":"resultsDict = {}","7e547935":"rnn_preds = scaler_y.inverse_transform(RNN_preds)\ny_test_actual = scaler_y.inverse_transform(pd.DataFrame(y_test))\nresultsDict['RNN'] = evaluate(y_test_actual, rnn_preds)\nevaluate(y_test_actual, rnn_preds)","fcb2afc9":"plt.figure(figsize=(18,8))\nplt.plot(rnn_preds, \"r-\", label=\"Predicted\")\nplt.plot(y_test_actual, label=\"Actual\")\nplt.title('RNN')\nplt.legend()\nplt.grid(True)\nplt.savefig('1 - RNN.jpg', dpi=200)\nplt.show()","3b9fcdff":"lstm_preds = scaler_y.inverse_transform(LSTM_preds)\ny_test_actual = scaler_y.inverse_transform(pd.DataFrame(y_test))\nresultsDict['LSTM'] = evaluate(y_test_actual, lstm_preds)\nevaluate(y_test_actual, lstm_preds)","ef00097d":"plt.figure(figsize=(18,8))\nplt.plot(lstm_preds, \"r-\", label=\"Predicted\")\nplt.plot(y_test_actual, label=\"Actual\")\nplt.title('LSTM')\nplt.legend()\nplt.grid(True)\nplt.savefig('2 - LSTM.jpg', dpi=200)\nplt.show()","56a96d49":"gru_preds = scaler_y.inverse_transform(GRU_preds)\ny_test_actual = scaler_y.inverse_transform(pd.DataFrame(y_test))\nresultsDict['GRU'] = evaluate(y_test_actual, gru_preds)\nevaluate(y_test_actual, gru_preds)","0bfd3085":"plt.figure(figsize=(18,8))\nplt.plot(gru_preds, \"r-\", label=\"Predicted\")\nplt.plot(y_test_actual, label=\"Actual\")\nplt.title('GRU')\nplt.legend()\nplt.grid(True)\nplt.savefig('3 - GRU.jpg', dpi=200)\nplt.show()","83bd53bb":"chosen = df_resample.copy()\nchosen = create_lags(chosen)\nchosen.dropna(inplace=True)\n\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler_x = scaler.fit(chosen.iloc[:1096,1:])\nscaler_y = scaler.fit(chosen.iloc[:1096,0].values.reshape(-1,1))\n\nx_scaled = scaler_x.transform(chosen.iloc[:,1:])\ny_scaled = scaler_y.transform(chosen.loc[:,['y']])\n\nscaled = np.hstack((x_scaled, y_scaled))\nscaled = pd.DataFrame(scaled, index=chosen.index, columns=chosen.columns)\n\ntrain = scaled[:1078]\nval = scaled[1078:1256]\ntest = scaled[1256:]\n\nx_train = train.drop([\"y\"],axis=1)\ny_train = train[\"y\"]\nx_val = val.drop([\"y\"],axis=1)\ny_val = val[\"y\"]\nx_test = test.drop([\"y\"],axis=1)\ny_test = test[\"y\"]","e300ac55":"## Helper Function\ni = 0\ndef train_test_split(df, i=0):\n    chosen = create_lags(df)\n    chosen.dropna(inplace=True)\n    x_scaled = scaler_x.transform(chosen.iloc[:,1:])\n    y_scaled = scaler_y.transform(chosen.loc[:,['y']])\n\n    scaled = np.hstack((x_scaled, y_scaled))\n    scaled = pd.DataFrame(scaled, index=chosen.index, columns=chosen.columns)\n\n    train = scaled[:1078+i]\n    val = scaled[1078+i:1256+i]\n    test = scaled[1256+i:]\n    \n    x_train = train.drop([\"y\"],axis=1)\n    y_train = train[\"y\"]\n    x_val = val.drop([\"y\"],axis=1)\n    y_val = val[\"y\"]\n    x_test = test.drop([\"y\"],axis=1)\n    y_test = test[\"y\"]\n\n    n_features = len(x_train.columns)\n    return x_train, x_val, x_test, y_train, y_val, y_test\n\nx_train, x_val, x_test, y_train, y_val, y_test = train_test_split(df_resample, i)\nprint(x_test.shape)","8aa79635":"TIME_STEPS, N_FEATURES = 7, 7\nrnn, lstm, gru = list(), list(), list()\n\nfor i in range(0, len(x_test), 30):\n    temp = df_resample.copy()\n    x_train, x_val, x_test, y_train, y_val, y_test = train_test_split(temp, i)\n    \n    x_train_np = create_features(x_train, TIME_STEPS, N_FEATURES)\n    x_val_np = create_features(x_val, TIME_STEPS, N_FEATURES)\n    x_test_np = create_features(x_test, TIME_STEPS, N_FEATURES)\n    #print(x_train_np.shape, x_val_np.shape, x_test_np.shape)\n    y_test = y_test[:x_test_np.shape[0]]\n    y_train = y_train[:x_train_np.shape[0]]\n    y_val = y_val[:x_val_np.shape[0]]\n    #print(y_train.shape, y_val.shape, y_test.shape)\n    \n    if y_test.shape[0] != 0:\n        RNN_model = fit_model(SimpleRNN, 64, x_train_np, x_val_np)\n        LSTM_model = fit_model(LSTM, 64, x_train_np, x_val_np)\n        GRU_model = fit_model(GRU, 64, x_train_np, x_val_np)\n\n        RNN_preds = RNN_model.predict(x_test_np)\n        yhat_actual = scaler_y.inverse_transform(RNN_preds)\n        rnn.extend(yhat_actual.flatten()[:30])\n        LSTM_preds = LSTM_model.predict(x_test_np)\n        yhat_actual = scaler_y.inverse_transform(LSTM_preds)\n        lstm.extend(yhat_actual.flatten()[:30])\n        GRU_preds = GRU_model.predict(x_test_np)\n        yhat_actual = scaler_y.inverse_transform(GRU_preds)\n        gru.extend(yhat_actual.flatten()[:30])","7772574a":"resultsDict['RNN Rolling'] = evaluate(y_test_actual[7:], rnn)\nevaluate(y_test_actual[7:], rnn)","1704b38a":"plt.figure(figsize=(18,8))\nplt.plot(rnn, \"r-\", label=\"Predicted\")\nplt.plot(y_test_actual[7:], label=\"Actual\")\nplt.legend()\nplt.title('Rolling RNN')\nplt.grid(True)\nplt.savefig('4 - RNN (Rolling).jpg', dpi=200)\nplt.show()","da59bf09":"resultsDict['LSTM Rolling'] = evaluate(y_test_actual[7:], lstm)\nevaluate(y_test_actual[7:], lstm)","d759237e":"plt.figure(figsize=(18,8))\nplt.plot(lstm, \"r-\", label=\"Predicted\")\nplt.plot(y_test_actual[7:], label=\"Actual\")\nplt.legend()\nplt.title('Rolling LSTM')\nplt.grid(True)\nplt.savefig('5 - LSTM (Rolling).jpg', dpi=200)\nplt.show()","2801bef1":"resultsDict['GRU Rolling'] = evaluate(y_test_actual[7:], gru)\nevaluate(y_test_actual[7:], gru)","2c8d52f2":"plt.figure(figsize=(18,8))\nplt.plot(gru, \"r-\", label=\"Predicted\")\nplt.plot(y_test_actual[7:], label=\"Actual\")\nplt.legend()\nplt.title('Rolling GRU')\nplt.grid(True)\nplt.savefig('6 - GRU (Rolling).jpg', dpi=200)\nplt.show()","1ad3f1ca":"resultsDict","e4413c74":"fig,a =  plt.subplots(3,2, figsize=(25,9))\n\na[0][0].plot(rnn_preds, \"r-\", label=\"Predicted\")\na[0][0].plot(y_test_actual, label=\"Actual\")\na[0][0].legend()\na[0][0].grid(True)\na[0][0].set_title('RNN')\na[0][1].plot(rnn, \"r-\", label=\"Predicted\")\na[0][1].plot(y_test_actual[7:], label=\"Actual\")\na[0][1].legend()\na[0][1].grid(True)\na[0][1].set_title('Rolling RNN')\na[1][0].plot(lstm_preds, \"r-\", label=\"Predicted\")\na[1][0].plot(y_test_actual, label=\"Actual\")\na[1][0].legend()\na[1][0].grid(True)\na[1][0].set_title('LSTM')\na[1][1].plot(lstm, \"r-\", label=\"Predicted\")\na[1][1].plot(y_test_actual[7:], label=\"Actual\")\na[1][1].legend()\na[1][1].grid(True)\na[1][1].set_title('Rolling LSTM')\na[2][0].plot(gru_preds, \"r-\", label=\"Predicted\")\na[2][0].plot(y_test_actual, label=\"Actual\")\na[2][0].legend()\na[2][0].grid(True)\na[2][0].set_title('GRU')\na[2][1].plot(gru, \"r-\", label=\"Predicted\")\na[2][1].plot(y_test_actual[7:], label=\"Actual\")\na[2][1].legend()\na[2][1].grid(True)\na[2][1].set_title('Rolling GRU')\nplt.savefig('Summary.jpg', dpi=200)\nplt.show()","8f15abc6":"# 5. Results","e997803c":"## 3.1 RNN","018b8a09":"Here we have some helper functions that help to create some simple lagged features to add into our model. You incorporate more complicated time-series features in your own work.\n\nRecurrent Neural Networks can take in additional features as a 3-D array for input, where the three dimensions of this input are `sample`, `time_steps` and `features`:\n\n1. Samples - One sequence is one sample. A batch is comprised of one or more samples.\n2. Time Steps - One time step is one point of observation in the sample.\n3. Features - One feature is one observation at a time step.\n\nThis means that the input layer expects a 3D array of data when fitting the model and when making predictions, even if specific dimensions of the array contain a single value, e.g. one sample or one feature.","6d6e1820":"## 3.3 GRU","dc6c03c9":"### Train-val-test split","dffa0c79":"## Attribute Information\n\n1. ds: Date in format dd\/mm\/yyyy\n2. time: time in format hh:mm:ss\n3. globalactivepower: household global minute-averaged active power (in kilowatt)\n4. globalreactivepower: household global minute-averaged reactive power (in kilowatt)\n5. voltage: minute-averaged voltage (in volt)\n6. global_intensity: household global minute-averaged current intensity (in ampere)\n7. submetering1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).\n8. submetering2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.\n9. submetering3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner.","b18adeaa":"To simplify the problem, we just wish to forecast the future household electricity consumption, we will do some pre-processing to frame this problem properly:\n- Downsampling from minute-average activate power to daily active power for households\n- Imputation missing values with column mean\n- MinMax Normalization to preserve variable distributions for our Recurrent Neural Networks\n\nThe dataset is now daily France household electricity data from `2006-12-23` until `2010-11-26`","4b8cd3c5":"# 1. Loading the Data","4521d5ee":"# 3. Forecasting with Recurrent Neural Networks\nHere's a helper function to help us train our RNNs, LSTMs, GRUs, where we then forecast with them to get the normalized predictions","af122c83":"Because we are using the first 7 inputs as sequence to create features, then dropping the NA values, we have to first do:\n```\ny_test_actual[7:]\n```\nto enforce the lengths of the test values and the predictions to be of equal length","9c680632":"The helper evaluation functions are available [here](https:\/\/www.kaggle.com\/mingboi\/rnn-utils)","524a927b":"## 3.2 LSTM","d0ce7c4f":"Recreate the dataset, and write some helper functions for preprocessing, forecasting","2dd5d70f":"# 2. Pre-processing","0baf8e1d":"# 4. Rolling Forecast with RNN, LSTM, GRU\nInstead of forecasting out a very long sequence out `2010-06-01` to `2010-11-23`, 175 days between them is a medium-length sequence. Anecdotally, I can handle up to 300-length sequences with LSTM and GRU, but should experiment with a rolling forecasting schemes to see if it handles the potential vanishing gradient problem. \n\nBy rolling, we mean that we train on initial train set, predict next month. Expand training window to include the predictions from next month, then repeat the following cycle until we have our desired 175 prediction window:\n1. Predict one month ahead\n2. Create features based on predictions\n3. Expand training window to include the predictions\n\nThis means that the maximum output sequence is 30-length long and can be readily handled without vanishing gradient problems.","dc1e34fc":"Use a simple for loop here of train, predict, re-train, predict our forecast","9fcc6400":"# Time-series forecasting: Rolling multi-step forecasts with Recurrent Neural Networks\nRecurrent Neural Networks (RNN, LSTM, GRU) are capable of learning long-term dependencies from the input sequence, support additional exogeneous features as input. Looking at the results below, they are really good at time-series forecasting out of the box with minimal feature engineering! However, RNNs are also rather tricky to work with since they take in a 3D input, which may be hard for beginners to work with. Hopefully this notebook serves as sufficient code for you to adapt in your own work, but any queries I will try my best to answer here, or via [GitHub](https:\/\/github.com\/mingboiz)\n\n\n![img](https:\/\/github.com\/mingboi95\/forecasting\/blob\/main\/img\/Summary.jpg?raw=true)\n\n\nFor the sake of simplicity, a very simple train-test split is used, with data down-sampled to daily frequency for ease of interpretation. \nThis notebook serves as a guide for forecasting time-series with Deep Learning methods, and it implements the following time-series forecasting with:\n- Multiple features (multivariate time-series forecasting)\n- Multi-step and multiple features forecasting (multivariate, multi-step time-series forecasting)","208c9c43":"We preprocess by normalizing all variables first, taking care to avoid data leakage by using our MinMaxScaler on training data only","e6bb9a1c":"We a simple train-test split for illustration purposes, where we predict for values from `2010-06-01` onwards for the test set.\n\nTrain - `2006-12-23` - `2009-12-22`  \nVal - `2009-12-23` - `2010-05-31`  \nTest - `2010-06-01` - `2010-11-26`"}}