{"cell_type":{"a436a54a":"code","223a3352":"code","ba7c0547":"code","86b56375":"code","c94ca6fd":"code","a50811e0":"code","65692b2f":"code","e0bd2a4f":"code","cceea46c":"code","5c248ac7":"code","d231c9f8":"code","4484a641":"code","e74489fb":"code","5f490181":"code","a22031aa":"code","aeae9a16":"code","672f461f":"markdown","0746680c":"markdown","9cfd95e2":"markdown","345d4606":"markdown","54f18fdd":"markdown","8face6b0":"markdown","e5cc2633":"markdown","5e2b2f09":"markdown","c41a1d42":"markdown","31c2c85b":"markdown","3cca9003":"markdown","b147e277":"markdown"},"source":{"a436a54a":"! conda install -y -c rapidsai-nightly -c nvidia -c conda-forge \\\n    -c defaults rapids=0.13 python=3.6","223a3352":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport cudf\nimport cuml\nimport holoviews as hv\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nhv.extension('bokeh')","ba7c0547":"data = cudf.read_csv('\/kaggle\/input\/big-five-personality-test\/IPIP-FFM-data-8Nov2018\/data-final.csv', sep='\\t')","86b56375":"data.head()","c94ca6fd":"data.shape","a50811e0":"X = data.drop(columns=['dateload', 'screenw',\n                       'screenh', 'introelapse', 'testelapse', 'endelapse', 'IPC', 'country',\n                       'lat_appx_lots_of_err', 'long_appx_lots_of_err'])\nX = (X - X.mean()) \/ X.std()\nX = X.fillna(0)\nX.shape","65692b2f":"%%time\npca = cuml.PCA(n_components = 2)\nZ_pca = pca.fit_transform(X)\ncolumns = [f'Component {i} ({round(e * 100)}%)' for i, e in enumerate(pca.explained_variance_ratio_)]\nZ_pca.columns = columns","e0bd2a4f":"filter_top_ten = data.country.isin(data.country.value_counts().nlargest(10).index)","cceea46c":"hv.Scatter(Z_pca.assign(country = data.country).loc[filter_top_ten, :].to_pandas().sample(1000),\n           kdims=columns[0], vdims=[columns[1], 'country']).opts(title='CUML PCA', color='country', cmap='Category20', legend_position='right', width=1000, height=400)","5c248ac7":"%%time\nscikit_pca = PCA(n_components = 2)\nZ_scikit_pca = pd.DataFrame(scikit_pca.fit_transform(X.to_pandas()))\ncolumns = [f'Component {i} ({round(e * 100)}%)' for i, e in enumerate(scikit_pca.explained_variance_ratio_)]\nZ_scikit_pca.columns = columns","d231c9f8":"hv.Scatter(Z_scikit_pca.assign(country = data.country.to_pandas()).loc[filter_top_ten.to_pandas(), :].sample(1000),\n           kdims=columns[0], vdims=[columns[1], 'country']).opts(title='Scikit-learn PCA', color='country', cmap='Category20', legend_position='right', width=1000, height=400)","4484a641":"N = 10000","e74489fb":"%%time\ncuml_tsne = cuml.TSNE(n_components = 2)\nZ_cuml_tsne = cuml_tsne.fit_transform(X.iloc[:N,:])\nZ_cuml_tsne.columns = ['Component 1', 'Component 2']","5f490181":"hv.Scatter(Z_cuml_tsne.assign(country = data.country.iloc[:N]).loc[filter_top_ten.iloc[:N], :].to_pandas().sample(1000),\n           kdims=['Component 1'], vdims=['Component 2', 'country']).opts(title='CUML TSNE',color='country', cmap='Category20', legend_position='right', width=1000, height=400)","a22031aa":"%%time\nscikit_tsne = TSNE(n_components = 2)\nZ_scikit_tsne = pd.DataFrame(scikit_tsne.fit_transform(X.to_pandas().iloc[:N,:]), columns = ['Component 1', 'Component 2'])","aeae9a16":"hv.Scatter(Z_scikit_tsne\n           .assign(country = data.country.iloc[:N].to_pandas())\n           .loc[filter_top_ten.iloc[:N].to_pandas(), :]\n           .sample(1000),\n           kdims=['Component 1'], vdims=['Component 2', 'country']).opts(title='Scikit-learn TSNE', color='country', cmap='Category20', legend_position='right', width=1000, height=400)","672f461f":"## PCA","0746680c":"# Conclusion","9cfd95e2":"Using CUML","345d4606":"## TSNE","54f18fdd":"Using CUML","8face6b0":"Using Scikit-learn","e5cc2633":"# Methods","5e2b2f09":"I am really excited to see how this project develops and the kind of workflow it unlocks. Using it on Kaggle, it took forever to install and I had to experiment a bit to make sure I had the correct versions installed. I am sure this will change and through time become a more seamless experience. I have been really impressed by the Demo's given my Matthew Rocklin, who leads the DASK proejct, on his integration of CUML into DASK and the oppotunity this unlocks for a familiar multi-GPU, distributed computation. ","c41a1d42":"In this notebook, we are going to look at dimensionality reduction and manifold learning using PCA and TSNE.  As you can see from my import the structure of the CUML library is very sismilar to scikit-learn and will hopefully, with time, offer more and more features like those of Scikit.  CUML can be used with Scikit-learn pipelines, and while they do lack a StandardScaler, we could easily write one in cupy and use it for preprocessing. ","31c2c85b":"The aim of this notebook is not to share with you some big idea or new experiment, but rather just to share my excitement and anticipation for the Nvidia Rapids Ecosystem.  The RAPIDS Ecosystem features a suite of software libraries, designed to look and feel like Pandas, Numpy, Scikit-learn or NetworkX, for end-to-end GPU accelerated computation and data analysis. For some tasks this can greatly accelerate your speed of computation and reponsiveness without having to relearn a new framework or rewrite your entire codebase. ","3cca9003":"# Data\nThe data I will be working with today is the Big 5 Kaggle Dataset which presents the results of personality tests presented by a sample of individuals. This is quite a large dataset with over a million observations and 110 features. Some of these features represent components of the test while, other show when, where and how the test was administered. ","b147e277":"Using Scikit-learn"}}