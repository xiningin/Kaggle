{"cell_type":{"a07ae95d":"code","15c35bda":"code","9c771eec":"code","354b862f":"code","cc2b9503":"code","fbebba8f":"code","52c3b98c":"code","29063d8b":"code","7aa81642":"code","37465699":"code","fc802624":"code","cc1448b7":"code","71ec1e39":"code","eeda65a4":"code","b5992742":"code","9b311f06":"code","0f15efbc":"code","850ed751":"code","269a862f":"code","b04f66c3":"code","5aea32fe":"code","ccd306b5":"code","7b560fbc":"code","9a51867b":"code","2a332d20":"code","ef91433f":"code","683d2883":"code","e752f837":"code","49e03f9c":"code","3009a8e6":"code","5887b25f":"code","152db354":"code","bf8f0b11":"code","44ffcd91":"code","98968872":"code","071849f6":"code","a3f4934b":"code","d9699d98":"code","32b7bff6":"code","c0f31d67":"code","a76f64cc":"code","7cbe3a97":"code","ede88423":"code","f25e6bac":"code","614c73f7":"code","18a3f652":"code","37eb6509":"code","245bda36":"code","626c20a5":"code","7520445d":"code","cd658a45":"code","65e78dd3":"code","86b049ed":"code","b57918af":"code","d4c533ab":"code","265092e2":"code","2387ea25":"code","fb09d70b":"code","36e9dd1f":"code","ee04819d":"code","cf8d42a8":"code","00b3bc58":"code","8752e5d8":"code","a8a72b01":"code","0e4275b7":"code","6eda2f48":"code","48d7ab15":"code","f26c87a3":"code","009e5c4d":"code","f3988949":"code","ef45ff3e":"code","2e59fab2":"code","1b385375":"code","e920d5c1":"code","705f76ac":"code","5b7893ad":"code","6cb06391":"code","a0585c21":"code","ff427f90":"code","f8204cae":"code","668410e8":"code","03e8ac9c":"code","15688f24":"code","08f90f75":"code","db14ef6f":"code","a0f6684c":"code","20363463":"code","244aa8b4":"code","af0791c5":"code","a1ea3d20":"code","694a7728":"code","3b26cd53":"code","a6032c9a":"code","deeaf5f5":"code","bc420965":"code","5c5f982f":"code","e10b492e":"code","fd3e5b45":"code","953ad7e4":"code","946da0ce":"code","2769a374":"code","412c500f":"code","704c692a":"code","818db53b":"code","ce2277ed":"code","7d466e4a":"code","00ff3b9a":"code","bd57af23":"code","aa822870":"code","c9b01c36":"code","0beed65f":"code","f1874620":"code","3530ff65":"code","8b403880":"code","1cb55f1b":"code","a225d57f":"code","438303d8":"code","74d00ef0":"code","8a784fb0":"code","4ed68470":"code","1852761e":"code","5090950d":"code","a3fb61dd":"code","110df2da":"code","221d3b7b":"code","45f0ad57":"code","ef702ae8":"code","5730ce0d":"code","27f8f404":"code","d6e74200":"code","fc327981":"code","a9b5de73":"code","5c0e0b20":"code","ac83cf02":"code","5f676538":"code","a74668be":"code","85c23433":"code","8e33f7d6":"code","43d90af4":"code","e8ddbc6e":"code","18c0c8ea":"code","df6bc20f":"code","82d17ac9":"code","f06f6f6a":"code","99696709":"code","fd3f1ee2":"code","94a59834":"code","8edf23d8":"code","412cfa46":"code","aff705c9":"code","ef56bf8e":"markdown","77d6c0c4":"markdown","af847d50":"markdown","730ae739":"markdown","63ee169b":"markdown","58d1ea00":"markdown","ca9f3e96":"markdown","eeabac6c":"markdown","4f931262":"markdown","36ceb250":"markdown","240204ce":"markdown","1d68c9b9":"markdown","55bf49b3":"markdown","ced67ade":"markdown","ede4723d":"markdown","d8684545":"markdown","1ea830f1":"markdown","2248211c":"markdown","e6cd5936":"markdown","0a99110d":"markdown","e43abcad":"markdown","d1a57597":"markdown","47da46b3":"markdown","a663b8ba":"markdown","4c288443":"markdown","1394bf25":"markdown","33bb6f1b":"markdown","dff066ba":"markdown","571830d2":"markdown","263d3484":"markdown","7cd655cf":"markdown","37ba7527":"markdown","b59cb3c5":"markdown","250ce808":"markdown","0ca4cdf9":"markdown","83187e08":"markdown","0451cf5d":"markdown","42af0358":"markdown","b38ae9ea":"markdown","c72cbd06":"markdown","008f03b2":"markdown","9326e8ac":"markdown","e6ac7033":"markdown","2d2de3e1":"markdown","9641783d":"markdown","016835fe":"markdown","10854093":"markdown","4ee9a182":"markdown","6465d313":"markdown","b10dc352":"markdown","a106642e":"markdown","a0c6859b":"markdown","ff0c863c":"markdown","6ef6f708":"markdown","b730b0d9":"markdown","fa4af055":"markdown","aa162c12":"markdown","34876387":"markdown","3a1cf53f":"markdown","97a29e9e":"markdown","89d8bd76":"markdown","d9d15b03":"markdown","6e37e996":"markdown","038f5664":"markdown","76212a88":"markdown","99ba9ad9":"markdown","4eef2359":"markdown","cb19e4af":"markdown"},"source":{"a07ae95d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","15c35bda":"!pip install lazypredict","9c771eec":"!pip install -U pandas","354b862f":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom IPython.display import display, clear_output","cc2b9503":"%config Completer.use_jedi = False","fbebba8f":"sns.set()","52c3b98c":"pd.set_option('display.max_columns', 30)","29063d8b":"def load_preprocess_df(drop_missing=False):\n#     df = pd.read_csv('stroke_det_cat.csv')\n#     df.drop(columns='bmi_range', inplace=True)\n    \n    df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\n    df.drop(columns='id', inplace=True)\n    \n    if drop_missing:\n        df = df.dropna().reset_index(drop=True)\n    \n    cats = list(df.select_dtypes(include=['object', 'category']).columns)\n    nums = list(df.select_dtypes(exclude=['object', 'category']).columns)\n    \n    features_to_conv = ['hypertension', 'heart_disease', 'stroke']\n    cats.extend(features_to_conv)\n    for feature in features_to_conv:\n        if feature in nums:\n            nums.remove(feature)\n    print(f'Categorical variables:  {cats}')\n    print(f'Numerical variables:  {nums}')\n    \n    df = df.astype({i: 'object' for i in cats})\n    df = df.astype({i: 'int64' for i in features_to_conv})\n    \n    df = pd.concat([df[cats], df[nums]], axis=1)\n    return df","7aa81642":"df = load_preprocess_df(drop_missing=False)\ndf.head()","37465699":"df.dtypes","fc802624":"cats = list(df.select_dtypes(exclude=['float64']).columns)\nnums = list(df.select_dtypes(include=['float64']).columns)\nprint(f'Categorical variables:  {cats}')\nprint(f'Numerical variables:  {nums}')","cc1448b7":"# Check for the number of unique values in each column\npd.DataFrame([df.nunique(), df.dtypes], index=['nunique', 'dtype'])","71ec1e39":"for col in cats:\n    print(df[col].value_counts())\n    print()","eeda65a4":"df = df.drop(df[df['gender'] == 'Other'].index).reset_index(drop=True)\ndf.shape","b5992742":"df.gender.value_counts()","9b311f06":"# Nothing useful for predicting stroke, consider joining with `children` category\ndf.loc[df['work_type'] == 'Never_worked']","0f15efbc":"df.work_type.replace('children', 'Never_worked', inplace=True)","850ed751":"df.work_type.value_counts()","269a862f":"df.shape","b04f66c3":"df_copy = df.copy()","5aea32fe":"missing_df = df_copy[df_copy.isna().any(axis=1)]\nmissing_df.head()","ccd306b5":"missing_df['stroke'].value_counts()","7b560fbc":"df.stroke.value_counts()","9a51867b":"len(missing_df[missing_df.stroke == 1]) \/ len(df[df.stroke == 1])","2a332d20":"# A really fantastic and intelligent way to deal with blanks, \n# from Thoman Konstantin in: \n# https:\/\/www.kaggle.com\/thomaskonstantin\/analyzing-and-modeling-stroke-data\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeRegressor\n\nDT_bmi_pipe = Pipeline(steps=[\n                              ('scale',StandardScaler()),\n                              ('lr',DecisionTreeRegressor(random_state=42))\n                             ])\nX = df_copy[['age','gender','bmi']].copy()\nX.gender = X.gender.replace({'Male':0,'Female':1,'Other':-1}).astype(np.uint8)\n# print(X.gender.value_counts())\n\nMissing = X[X.bmi.isna()]\nX = X[~X.bmi.isna()]\nY = X.pop('bmi')\nDT_bmi_pipe.fit(X,Y)\npredicted_bmi = pd.Series(DT_bmi_pipe.predict(Missing[['age','gender']]),\n                          index=Missing.index)\ndf_copy.loc[Missing.index,'bmi'] = predicted_bmi","ef91433f":"df_copy.loc[missing_df.index].head()","683d2883":"fig = plt.figure(figsize=(16, 6), facecolor='white')\nsns.set_style('white')\ngs=fig.add_gridspec(1,2)\n\nax = [None, None]\n\nax[0]=fig.add_subplot(gs[0,0])\nax[1]=fig.add_subplot(gs[0,1])\n\nsns.kdeplot(data=df, x='bmi', ax=ax[0], color='coral', zorder=2)\nsns.kdeplot(data=df_copy, x='bmi', ax=ax[0], color='xkcd:sky blue', zorder=2)\nax[0].set_title('BMI KDE plot - Orig vs Imputed', fontsize=20,fontweight='bold', fontfamily='monospace')\n\nsns.histplot(data=df, x='bmi', ax=ax[1], element='step', color='coral', alpha=0.1)\nsns.histplot(data=df_copy, x='bmi', ax=ax[1], element='step', color='xkcd:sky blue', alpha=0.1)\nax[1].set_title('BMI Histogram - Orig vs Imputed', fontsize=20,fontweight='bold', fontfamily='monospace')\n\nfor ax_ in ax:\n    ax_.legend(['original', 'imputed'])\n    ax_.set_ylabel(None)\n    ax_.grid(which='both', axis='y', zorder=0, color='black', linestyle=':', dashes=(2,7), alpha=0.3)\n    for direction in ['top','right','left']:\n        ax_.spines[direction].set_visible(False)\n\n# sns.despine(left=True)\nplt.tight_layout()\nplt.show()","e752f837":"df_copy = df.copy()","49e03f9c":"from sklearn.model_selection import train_test_split\n\ndef train_validation_test_split(\n    X, y, train_size=0.8, val_size=None, test_size=None,\n    stratify=None, random_state=42, shuffle=True\n):\n    if not val_size:\n        val_size = (1 - train_size) \/ 2.\n        test_size = val_size\n        \n    assert int(train_size + val_size + test_size + 1e-7) == 1\n    \n    stratify_1 = y if stratify else None\n    \n    X_train_val, X_test, y_train_val, y_test = train_test_split(\n        X, y, test_size=test_size, stratify=stratify_1, \n        random_state=random_state, shuffle=shuffle)\n    \n    stratify_2 = y_train_val if stratify else None\n    \n    X_train, X_val, y_train, y_val = train_test_split(\n        X_train_val, y_train_val, test_size=val_size\/(train_size+val_size), \n        random_state=random_state, stratify=stratify_2, shuffle=shuffle)\n    \n    return X_train, X_val, X_test, y_train, y_val, y_test","3009a8e6":"# After spending some time trying to make this function work,\n#  I realized that train_test_split already has a `stratify` parameter that serves the same purpose...\n# So this function is actually not needed.\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\ndef stratified_split(\n    X, y, train_size=0.8, val_size=None, test_size=None, \n    random_state=42, shuffle=True\n):\n    if not val_size:\n        # print(\"[INFO] Validation size and test size are inferred from train_size!\")\n        val_size = (1 - train_size) \/ 2.\n        test_size = val_size\n    elif not test_size:\n        test_size = train_size - val_size\n        \n    assert int(train_size + val_size + test_size + 1e-7) == 1\n    \n    split = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n    for train_val_index, test_index in split.split(X, y):\n        X_train_val = X.loc[train_val_index]\n        X_test = X.loc[test_index]\n        y_train_val = y.loc[train_val_index]\n        y_test = y.loc[test_index]\n    \n    # Must reset the index for the DataFrame to locate the proper indices from the splits\n    X_train_val.reset_index(drop=True, inplace=True)\n    y_train_val.reset_index(drop=True, inplace=True)\n    \n    split = StratifiedShuffleSplit(n_splits=1, \n                                   test_size=val_size\/(train_size+val_size), \n                                   random_state=42)\n    for train_index, val_index in split.split(X_train_val, y_train_val):\n        X_train = X_train_val.loc[train_index]\n        X_val = X_train_val.loc[val_index]\n        y_train = y_train_val.loc[train_index]\n        y_val = y_train_val.loc[val_index]\n    \n    return X_train, X_val, X_test, y_train, y_val, y_test","5887b25f":"from functools import partial\n\nstratified_func = partial(train_validation_test_split, stratify=True)\nsplit_df = pd.DataFrame(columns=['not_stratified', 'stratified'])\n\nfor i, split_func in enumerate((train_validation_test_split, stratified_func)):\n    column_name = 'not_stratified' if i == 0 else 'stratified'\n    \n    *_, y_train, y_val, y_test = split_func(X=df_copy.drop(columns='stroke'),\n                                                                y=df_copy.stroke,\n                                                                train_size=0.70)\n    split_df[column_name] = pd.concat([df_copy.stroke.value_counts(), y_train.value_counts(), \n                          y_val.value_counts(), y_test.value_counts()], axis=0)\n\nsplit_df = split_df.reset_index(drop=True)\\\n           .rename(index={0: \"stroke_full\", 1: \"no_stroke_full\", \n                          2: \"stroke_train\", 3: \"no_stroke_train\",\n                          4: \"stroke_val\", 5: \"no_stroke_val\", \n                          6: \"stroke_test\", 7: \"no_stroke_test\"})\n\nbackground_color = \"#E3EDF0\"\n\nfig = plt.figure(figsize=(8, 6))\nax = sns.heatmap(split_df, annot=True, cmap=\"Paired\", fmt=\"\", linewidths=2, cbar=False, annot_kws={\"size\":14, \"fontfamily\":\"monospace\"})\nfig.patch.set_facecolor(background_color)\nax.set_facecolor(background_color) \nplt.title('Dataset Splits | Stratification', fontsize=15, fontweight='bold', fontfamily=\"monospace\")\nplt.xticks(fontsize=13); plt.yticks(fontsize=13)\nplt.show()","152db354":"X_train, X_val, X_test, y_train, y_val, y_test = train_validation_test_split(X=df_copy.drop(columns='stroke'),\n                                                                             y=df_copy.stroke, \n                                                                             stratify=True,\n                                                                             train_size=0.70)","bf8f0b11":"plt.figure(figsize=(12,6))\nsns.set_style('white')\n\nsns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(X_train[nums]), palette=\"cubehelix\")\n\nsns.despine(left=True, bottom=True)\nplt.grid(which='both', axis='y', zorder=0, color='black', linestyle=':', dashes=(2,7), alpha=0.3)\nplt.title('Original Distribution of Numerical Features', fontsize=20, fontweight='bold', fontfamily='monospace')\nplt.xlabel(None)\nplt.ylabel(None)\nplt.show()","44ffcd91":"from sklearn.preprocessing import StandardScaler\nstd_sc = StandardScaler()\nX_train_std = X_train.copy()\nX_train_std[nums] = std_sc.fit_transform(X_train_std[nums])","98968872":"plt.figure(figsize=(12,6))\nsns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(X_train_std[nums]), palette=\"cubehelix\")\n\nsns.despine(left=True, bottom=True)\nplt.grid(which='both', axis='y', zorder=0, color='black', linestyle=':', dashes=(2,7), alpha=0.3)\nplt.title('Standard Scaler', fontsize=20, fontweight='bold', fontfamily='monospace')\nplt.xlabel(None)\nplt.ylabel(None)\nplt.show()","071849f6":"from sklearn.preprocessing import RobustScaler\nrs = RobustScaler()\nX_train_rs = X_train.copy()\nX_train_rs[nums] = rs.fit_transform(X_train_rs[nums])\n# X_val_rs[nums] = rs.transform(X_val_rs[nums])\n# X_test_rs[nums] = rs.transform(X_test_rs[nums])","a3f4934b":"plt.figure(figsize=(12,6))\nsns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(X_train_rs[nums]), palette=\"cubehelix\")\n\nsns.despine(left=True, bottom=True)\nplt.grid(which='both', axis='y', zorder=0, color='black', linestyle=':', dashes=(2,7), alpha=0.3)\nplt.title('Robust Scaler', fontsize=20, fontweight='bold', fontfamily='monospace')\nplt.xlabel(None)\nplt.ylabel(None)\nplt.show()","d9699d98":"from sklearn.preprocessing import QuantileTransformer\nqt = QuantileTransformer(n_quantiles=10, random_state=42)\nX_train_qt = X_train.copy()\nX_train_qt[nums] = qt.fit_transform(X_train_qt[nums])","32b7bff6":"plt.figure(figsize=(12,6))\nsns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(X_train_qt[nums]), palette='cubehelix')\n\nsns.despine(left=True, bottom=True)\nplt.grid(which='both', axis='y', zorder=0, color='black', linestyle=':', dashes=(2,7), alpha=0.3)\nplt.title('Quantile Transformer', fontsize=20, fontweight='bold', fontfamily='monospace')\nplt.xlabel(None)\nplt.ylabel(None)\nplt.show()","c0f31d67":"from sklearn.preprocessing import PowerTransformer\npt = PowerTransformer()\nX_train_pt = X_train.copy()\nX_train_pt[nums] = pt.fit_transform(X_train_pt[nums])","a76f64cc":"plt.figure(figsize=(12,6))\nsns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(X_train_pt[nums]), palette='cubehelix')\n\nsns.despine(left=True, bottom=True)\nplt.grid(which='both', axis='y', zorder=0, color='black', linestyle=':', dashes=(2,7), alpha=0.3)\nplt.title('Power Transformer', fontsize=20, fontweight='bold', fontfamily='monospace')\nplt.xlabel(None)\nplt.ylabel(None)\nplt.show()","7cbe3a97":"df_nums_log = np.log(df_copy[nums])\ndf_nums_log.head()","ede88423":"plt.figure(figsize=(12,6))\nsns.set_style('white')\nsns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(df_nums_log), palette='cubehelix')\n\nsns.despine(left=True, bottom=True)\nplt.grid(which='both', axis='y', zorder=0, color='black', linestyle=':', dashes=(2,7), alpha=0.3)\nplt.title('Log Transformation', fontsize=20, fontweight='bold', fontfamily='monospace')\nplt.xlabel(None)\nplt.ylabel(None)\nplt.show()","f25e6bac":"from sklearn.metrics import make_scorer, classification_report, confusion_matrix\n\ntarget_names = ['No Stroke', 'Stroke']  # [0, 1]\n\ndef eval_model_on_train_val(model, transformed=False, \n                            return_pred=False, show_results=True):\n    # global X_train, y_train, X_val, y_val, X_train_tf, X_val_tf\n    \n    if transformed:\n        # model requires transformed dataset\n        X_train_ = X_train_tf.copy()\n        X_val_ = X_val_tf.copy()\n    else:\n        X_train_ = X_train.copy()\n        X_val_ = X_val.copy()\n    \n    # display(pd.DataFrame(X_train_).head())\n    y_pred = model.predict(X_train_)\n    if show_results:\n        print('[INFO] Evaluating on training set ...')\n        print(confusion_matrix(y_train, y_pred))\n        print(classification_report(y_train, y_pred, target_names=target_names))\n    \n    y_pred = model.predict(X_val_)\n    if show_results:\n        print('\\n[INFO] Evaluating on validation set ...')\n        print(confusion_matrix(y_val, y_pred))\n        print(classification_report(y_val, y_pred, target_names=target_names))\n    \n    if return_pred:\n        return y_pred","614c73f7":"df_copy = df.copy()","18a3f652":"X_train, X_val, X_test, y_train, y_val, y_test = train_validation_test_split(X=df_copy.drop(columns='stroke'),\n                                                                             y=df_copy.stroke, \n                                                                             stratify=True,\n                                                                             train_size=0.70)","37eb6509":"cats = list(df.select_dtypes(exclude=['float64']).columns)\nnums = list(df.select_dtypes(include=['float64']).columns)\nprint(f'Categorical variables:  {cats}')\nprint(f'Numerical variables:  {nums}')","245bda36":"categorical_cols = cats.copy()\nif 'stroke' in categorical_cols:\n    categorical_cols.remove('stroke')\nprint(categorical_cols)","626c20a5":"from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler, LabelEncoder, OrdinalEncoder\nfrom sklearn.impute import KNNImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.tree import ExtraTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.decomposition import PCA\nimport xgboost as xgb\n\ncat_pipeline = Pipeline([\n    # (\"one_hot_encoder\", OneHotEncoder(drop='first')),\n    (\"ordinal_encoder\", OrdinalEncoder()),\n    # ('std_scaler', StandardScaler(with_mean=True)),\n    ('rob_scaler', RobustScaler(with_centering=False)),\n])\n\nbmi_pipeline = Pipeline([\n    # ('std_scaler_bmi', StandardScaler(with_mean=True)),\n    ('rob_scaler_bmi', RobustScaler(with_centering=False)),\n    # add_indicator is very useful to add extra columns denoting missing values\n    ('KNN_imputer', KNNImputer(n_neighbors=3, add_indicator=True))\n])\n\npipe_1 = ColumnTransformer([\n    ('cat', cat_pipeline, categorical_cols),\n    # ('std_scaler_nums', StandardScaler(with_mean=True), ['avg_glucose_level', 'age']),\n    ('rob_scaler_nums', RobustScaler(with_centering=False), ['avg_glucose_level', 'age']),\n    # ('rob_scaler_glucose', RobustScaler(with_centering=False), ['avg_glucose_level']),\n    # ('std_scaler_age', StandardScaler(with_mean=False), ['age']),\n    ('bmi_pipeline', bmi_pipeline, ['bmi'])\n], remainder=\"passthrough\")\n\ntrain_pipe_1 = Pipeline([\n    ('feature_transform', pipe_1),\n    \n    ('select_kbest', SelectKBest(k=10)),\n    # ('pca_2', PCA(n_components=2)),\n    \n    ('rfc', RandomForestClassifier(random_state=42))\n])","7520445d":"# prepare transformed datasets for certain use cases\nX_train_tf = pipe_1.fit_transform(X_train)\nX_val_tf = pipe_1.transform(X_val)","cd658a45":"from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler, OrdinalEncoder\nfrom sklearn.impute import KNNImputer\nfrom sklearn.pipeline import Pipeline\n\ncat_pipeline = Pipeline([\n    (\"one_hot_encoder\", OneHotEncoder(drop='first')),\n    ('cat_scaler', RobustScaler(with_centering=False)),\n])\n\ncat_no_scale_pipeline = Pipeline([\n    (\"one_hot_encoder\", OneHotEncoder(drop='first')),\n])\n\npipe_2 = ColumnTransformer([\n    ('num_std_scaler', RobustScaler(with_centering=False), nums),\n    ('cat', cat_pipeline, categorical_cols),\n], remainder=\"passthrough\")\n\npipe_3 = ColumnTransformer([\n    ('num_std_scaler', RobustScaler(with_centering=False), nums),\n    ('cat', cat_no_scale_pipeline, categorical_cols),\n], remainder=\"passthrough\")\n\ntrain_pipe_2 = Pipeline([\n    ('pipe_2', pipe_2),\n    ('KNN_imputer', KNNImputer(n_neighbors=3, add_indicator=True)),\n    ('xgb', xgb.XGBClassifier(random_state=42)),\n])\n\ntrain_pipe_3 = Pipeline([\n    ('pipe_3', pipe_3),\n    ('KNN_imputer', KNNImputer(n_neighbors=3, add_indicator=True)),\n    ('xgb', xgb.XGBClassifier(random_state=42)),\n])\n\ntrain_pipe_4 = Pipeline([\n    ('pipe_1', pipe_1),\n    ('log_reg', LogisticRegression(random_state=42)),\n])","65e78dd3":"train_pipe_1.fit(X_train, y_train)","86b049ed":"eval_model_on_train_val(train_pipe_1)","b57918af":"X_train_tf = pipe_1.fit_transform(X_train)\nX_val_tf = pipe_1.transform(X_val)","d4c533ab":"from lazypredict.Supervised import LazyClassifier\nfrom sklearn.metrics import f1_score\nfrom functools import partial\n\ncustom_metric = partial(f1_score, average='binary')\ncustom_metric.__name__ = 'f1_binary'\nclf1 = LazyClassifier(verbose=0, ignore_warnings=True, random_state=42, custom_metric=custom_metric)\nclf2 = LazyClassifier(verbose=0, ignore_warnings=True, random_state=42, custom_metric=custom_metric)\n\nmodels_train, predictions_train = clf1.fit(X_train_tf, X_train_tf, y_train, y_train)\nmodels_val, predictions_val = clf2.fit(X_train_tf, X_val_tf, y_train, y_val)\n\nmodels_train","265092e2":"models_dict = clf2.provide_models(X_train_tf, X_val_tf, y_train, y_val)","2387ea25":"models_dict.keys()","fb09d70b":"eval_model_on_train_val(models_dict['AdaBoostClassifier'], transformed=True)","36e9dd1f":"models_train.columns","ee04819d":"metric = \"f1_binary\"\nmodels_train.sort_values(by=metric, ascending=False, inplace=True)\n\nfig = plt.figure(figsize=(6, 11))\nsns.set_theme(style=\"whitegrid\")\n\nax = sns.barplot(y=models_train.index, x=metric, data=models_train)\n\n\nfor p in ax.patches:\n    value = f'{(p.get_width() * 100):.2f}%'\n    x = p.get_x() + p.get_width() + 0.01\n    y = p.get_y() + p.get_height()\/2 + 0.15\n    ax.annotate(value, (x, y))\n\nplt.title('Training | F1 Score for Stroke Prediction', \n          fontdict=dict(fontsize=16,\n                        fontweight='bold', \n                        fontfamily='monospace'))\nplt.ylabel(None)\nplt.xlabel('F1 Score', fontsize=13, fontweight='bold')\nsns.despine()\nplt.grid(axis='x')\nplt.yticks(fontsize=13)\n# plt.savefig('training_result.png', bbox_inches='tight')\nplt.show()","cf8d42a8":"metric = \"f1_binary\"\nmodels_val.sort_values(by=metric, ascending=False, inplace=True)\n\nplt.figure(figsize=(6, 10))\nsns.set_theme(style=\"whitegrid\")\nax = sns.barplot(y=models_val.index, x=metric, data=models_val)\nplt.title('Validation | F1 Score for Stroke Prediction', \n          fontdict=dict(fontsize=15,\n                        fontweight='bold', \n                        fontfamily='monospace'\n                       ))\nplt.ylabel(None)\nplt.xlabel('F1 Score', fontsize=13, fontweight='bold')\nsns.despine()\n\nfor p in ax.patches:\n    value = f'{(p.get_width() * 100):.2f}%'\n    x = p.get_x() + p.get_width() + 0.002\n    y = p.get_y() + p.get_height()\/2 + 0.2\n    ax.annotate(value, (x, y))\n\nplt.yticks(fontsize=13)\n# plt.savefig('validation_result.png', bbox_inches='tight')\nplt.show()","00b3bc58":"background_color = \"#f7fdff\"\nfig = plt.figure(figsize=(15, 6), facecolor=background_color)\nsns.set_style('whitegrid',  {\"axes.facecolor\": background_color})\nsns.lineplot(x=models_train.index, y=metric, data=models_train, color='xkcd:sky blue')\nsns.lineplot(x=models_val.index, y=metric, data=models_val, color='coral')\nsns.despine()\nplt.xticks(rotation=90)\n\nplt.title('F1 Scores for Stroke Prediction', \n          fontdict=dict(fontsize=15,\n                        fontweight='bold', \n                        fontfamily='monospace'\n                       ))\nplt.xlabel(None)\nplt.ylabel('F1 score - Stroke')\nplt.legend(['Training', 'Validation'])\nplt.show()","8752e5d8":"avg_model = models_train + models_val \/ 2\n\nmetric = \"f1_binary\"\navg_model.sort_values(by=metric, ascending=False, inplace=True)\n\nplt.figure(figsize=(8, 10))\nsns.set_theme(style=\"whitegrid\")\nax = sns.barplot(y=avg_model.index, x=metric, data=avg_model)\nplt.title('Training & Validation | Average F1 Scores\\nStroke Prediction', \n          fontdict=dict(fontsize=15,\n                        fontweight='bold', \n                        fontfamily='monospace'\n                       ))\nplt.ylabel(None)\nplt.xlabel('Average F1 Score', fontsize=13, fontweight='bold')\nsns.despine()\n\nfor p in ax.patches:\n        value = f'{(p.get_width() * 100):.2f}%'\n        x = p.get_x() + p.get_width() + 0.002\n        y = p.get_y() + p.get_height()\/2 + 0.2\n        ax.annotate(value, (x, y))\n","a8a72b01":"df_copy = df.copy()","0e4275b7":"one_hot_cats = pd.get_dummies(df_copy.loc[:, 'gender':'smoking_status'], drop_first=True)","6eda2f48":"df_copy = df_copy.loc[:, 'hypertension': 'stroke']\ndf_copy = pd.concat([one_hot_cats, df_copy, df[nums]], axis=1)\ndf_copy.head()","48d7ab15":"df_copy = df.copy()","f26c87a3":"from sklearn.preprocessing import OrdinalEncoder\nord_enc = OrdinalEncoder()\ndf_copy[categorical_cols] = ord_enc.fit_transform(df_copy[categorical_cols])","009e5c4d":"df_copy.head()","f3988949":"df_copy = df.copy()","ef45ff3e":"X_train, X_val, X_test, y_train, y_val, y_test = train_validation_test_split(X=df_copy.drop(columns='stroke'), \n                                                                             y=df_copy.stroke, \n                                                                             stratify=True,\n                                                                             train_size=0.70)","2e59fab2":"X_train.head()","1b385375":"from sklearn.preprocessing import OrdinalEncoder\nord_enc = OrdinalEncoder()\nX_train[categorical_cols] = ord_enc.fit_transform(X_train[categorical_cols])\nX_val[categorical_cols] = ord_enc.transform(X_val[categorical_cols])\nX_test[categorical_cols] = ord_enc.transform(X_test[categorical_cols])","e920d5c1":"from sklearn.tree import ExtraTreeClassifier\n\netc_pipeline = Pipeline([('scaler', RobustScaler(with_centering=True)),\n                         ('imputer', KNNImputer(n_neighbors=6, add_indicator=True)),\n                         ('select_kbest', SelectKBest(k=10)),\n                         ('etc_clf', ExtraTreeClassifier(random_state=42))])","705f76ac":"etc_pipeline.fit(X_train, y_train)","5b7893ad":"etc_pred = eval_model_on_train_val(etc_pipeline, return_pred=True)","6cb06391":"etc_cm = confusion_matrix(y_val, etc_pred)","a0585c21":"import matplotlib as mpl\n\nbackground_color = \"#E3EDF0\"\n\n\ncolors = [\"lightblue\", \"xkcd:sky blue\", \"xkcd:sky blue\", \"xkcd:sky blue\"]\ncolormap = mpl.colors.LinearSegmentedColormap.from_list(\"\",  colors)\n\nfig = plt.figure(figsize=(6, 3))\nax = sns.heatmap(etc_cm, cmap=colormap, annot=True, fmt=\"d\", linewidths=5, cbar=False,\n            yticklabels=['Actual Non-Stroke','Actual Stroke'],\n            xticklabels=['Predicted Non-Stroke','Predicted Stroke'],\n            annot_kws={\"fontsize\": 13, \"fontfamily\": 'monospace'})\n\nplt.title('ExtraTreeClassifier | Confusion Matrix', size=15, fontfamily='serif')\nplt.yticks(size=13, fontfamily='serif')\nplt.xticks(size=13, fontfamily='serif')\nfig.patch.set_facecolor(background_color)\n# ax.set_facecolor(background_color)\nplt.show()","ff427f90":"# etc_pipeline.get_params()","f8204cae":"# from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\n# param_grid = {\n#     'select_kbest__k': [6, 7, 8, 9, 10],\n#     'imputer__n_neighbors': [2, 4, 6, 8],\n#     'etc_clf__max_depth': [35, 40, 45],\n#     'etc_clf__max_features': [2, 4, 6],\n#     'etc_clf__min_samples_leaf': [2, 4, 6],\n#     'etc_clf__min_samples_split': [2, 4, 6]\n# }\n\n# grid = GridSearchCV(estimator=etc_pipeline, \n#                      param_grid=param_grid, cv=3,\n#                      scoring='f1', refit=True,\n#                      verbose=3, n_jobs=4)\n\n# grid.fit(X_train, y_train)\n\n# print(f'\\nBest params -> {grid.best_params_}')\n# print(f'Best score -> {grid.best_score_}')\n# print(f'Validation score -> {grid.score(X_val, y_val)}')","668410e8":"# Best params -> {'etc_clf__max_depth': 35, 'etc_clf__max_features': 6, 'etc_clf__min_samples_leaf': 2, 'etc_clf__min_samples_split': 6, 'imputer__n_neighbors': 8, 'select_kbest__k': 9}\n# Best score -> 0.1898148148148148\n# Validation score -> 0.20338983050847456","03e8ac9c":"# etc_pred = eval_model_on_train_val(grid, return_pred=True)","15688f24":"# [INFO] Evaluating on training set ...\n# [[3369   31]\n#  [ 103   72]]\n#               precision    recall  f1-score   support\n\n#    No Stroke       0.97      0.99      0.98      3400\n#       Stroke       0.70      0.41      0.52       175\n\n#     accuracy                           0.96      3575\n#    macro avg       0.83      0.70      0.75      3575\n# weighted avg       0.96      0.96      0.96      3575\n\n\n# [INFO] Evaluating on validation set ...\n# [[714  16]\n#  [ 31   6]]\n#               precision    recall  f1-score   support\n\n#    No Stroke       0.96      0.98      0.97       730\n#       Stroke       0.27      0.16      0.20        37\n\n#     accuracy                           0.94       767\n#    macro avg       0.62      0.57      0.59       767\n# weighted avg       0.93      0.94      0.93       767","08f90f75":"# y_pred = etc_pipeline.predict(X_test)\n# print(confusion_matrix(y_test, y_pred))\n# print(classification_report(y_test, y_pred, target_names=target_names))","db14ef6f":"# [INFO] Evaluating on training set ...\n# [[3369   31]\n#  [ 103   72]]\n#               precision    recall  f1-score   support\n\n#    No Stroke       0.97      0.99      0.98      3400\n#       Stroke       0.70      0.41      0.52       175\n\n#     accuracy                           0.96      3575\n#    macro avg       0.83      0.70      0.75      3575\n# weighted avg       0.96      0.96      0.96      3575\n\n\n# [INFO] Evaluating on validation set ...\n# [[714  16]\n#  [ 31   6]]\n#               precision    recall  f1-score   support\n\n#    No Stroke       0.96      0.98      0.97       730\n#       Stroke       0.27      0.16      0.20        37\n\n#     accuracy                           0.94       767\n#    macro avg       0.62      0.57      0.59       767\n# weighted avg       0.93      0.94      0.93       767","a0f6684c":"scaler = RobustScaler()\nX_train_ = scaler.fit_transform(X_train.copy())\nX_val_ = scaler.transform(X_val.copy())\nX_test_ = scaler.transform(X_test.copy())\n\n\nimputer = KNNImputer(n_neighbors=6, add_indicator=True)\nX_train_ = imputer.fit_transform(X_train_)\nX_val_ = imputer.transform(X_val_)\nX_test_ = imputer.transform(X_test_)\n\nX_train_[:, :-1] = scaler.inverse_transform(X_train_[:, :-1])\nX_val_[:, :-1] = scaler.inverse_transform(X_val_[:, :-1])\nX_test_[:, :-1] = scaler.inverse_transform(X_test_[:, :-1])","20363463":"# X_train_ = X_train.copy()\n# X_val_ = X_val.copy()\n# X_train_['bmi_NaN'] = X_train_tf[:, -1]\n# X_val_['bmi_NaN'] = X_val_tf[:, -1]","244aa8b4":"X_train_ = pd.DataFrame(X_train_, columns=list(X_train.columns) + ['bmi_NaN'])\nX_val_ = pd.DataFrame(X_val_, columns=list(X_train.columns) + ['bmi_NaN'])\nX_val_ = pd.DataFrame(X_val_, columns=list(X_train.columns) + ['bmi_NaN'])\nX_train_.head()","af0791c5":"# Have to train a new classifier because SHAP does not support scikit-learn's pipeline\n# Using RandomForest here because it produced much better SHAP plot than ExtraTreeClassifier for some reason...\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train_, y_train)","a1ea3d20":"y_pred = clf.predict(X_val_)\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred, target_names=target_names))","694a7728":"import shap\n\nexplainer = shap.TreeExplainer(clf)\nshap_values = explainer.shap_values(X_val_)","3b26cd53":"sns.set_style('white')","a6032c9a":"shap.summary_plot(shap_values[1], X_val_, alpha=0.5)","deeaf5f5":"shap.dependence_plot('age', shap_values[1], X_val_, interaction_index=\"age\", \n                     alpha=0.5, show=False)\nplt.title(\"Age dependence plot\", \n          fontfamily='monospace', fontweight='bold', fontsize=16)","bc420965":"shap.dependence_plot('avg_glucose_level', shap_values[1], X_val_, \n                     interaction_index=\"avg_glucose_level\", alpha=0.5, show=False)\nplt.title(\"avg_glucose_level dependence plot\", \n          fontfamily='monospace', fontweight='bold', fontsize=16)\nplt.show()","5c5f982f":"shap.dependence_plot('bmi', shap_values[1], X_val_, \n                     interaction_index=\"bmi\", alpha=0.5, show=False)\nplt.title(\"BMI dependence plot\", \n          fontfamily='monospace', fontweight='bold', fontsize=16)\nplt.show()","e10b492e":"# To check the trained etc parameters\nfor k, v in etc_pipeline.get_params().items():\n    if str(k).startswith('etc_clf__'):\n        print(k, v)","fd3e5b45":"etc_pipeline.get_params().keys()","953ad7e4":"# from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\n# param_grid = {\n#     \"imputer__n_neighbors\": ([3, 4, 5, 6]),\n#     \"select_kbest__k\": ([6, 8, 10]),\n#     'etc_clf__min_child_weight': ([1, 5, 10]),\n#     'etc_clf__gamma': ([0, 0.5, 1, 1.5, 2, 5]),\n#     'etc_clf__subsample': ([0.6, 0.8, 1.0]),\n#     'etc_clf__colsample_bytree': ([0.6, 0.8, 1.0, 1.2]),\n#     'etc_clf__max_depth': ([3, 4, 5, 6, 7, 8])\n# }\n\n# rs_cv = RandomizedSearchCV(estimator=etc_pipeline, n_iter=100, \n#                            param_distributions=param_grid, cv=3,\n#                            scoring='f1_macro', refit=True,\n#                            verbose=3, n_jobs=4, random_state=42)\n\n# rs_cv.fit(X_train, y_train)\n\n# print(f'\\nBest params -> {rs_cv.best_params_}')\n# print(f'Best score -> {rs_cv.best_score_}')\n# print(f'Validation score -> {rs_cv.score(X_val, y_val)}')","946da0ce":"# rs_cv_df = pd.DataFrame(rs_cv.cv_results_)\n# rs_cv_df.sort_values('rank_test_score', inplace=True)\n# rs_cv_df.head()","2769a374":"# eval_model_on_train_val(rs_cv)","412c500f":"# from sklearn.model_selection import GridSearchCV\n\n# param_grid = {\n#     \"imputer__n_neighbors\": range(1, 5),\n#     \"select_kbest__k\": range(3, 9),\n#     \"etc_clf__max_depth\": [4],\n#     \"etc_clf__learning_rate\": [0.2, 0.3, 0.4],\n#     \"etc_clf__gamma\": [1, 2, 3],\n#     \"etc_clf__reg_lambda\": [9, 10, 11, 12],\n#     \"etc_clf__scale_pos_weight\": [5],\n#     \"etc_clf__subsample\": [0.8],\n#     \"etc_clf__colsample_bytree\": [0.5],\n# }\n\n# grid = GridSearchCV(estimator=etc_pipeline, \n#                     param_grid=param_grid, cv=3,\n#                     scoring='f1_macro', refit=True,\n#                     verbose=1, n_jobs=-1)\n\n# grid.fit(X_train, y_train)\n\n# print(f'\\nBest params -> {grid.best_params_}')\n# print(f'Best score -> {grid.best_score_}')\n# print(f'Validation score -> {grid.score(X_val, y_val)}')","704c692a":"# grid_df = pd.DataFrame(grid.cv_results_)\n# grid_df.sort_values('rank_test_score', inplace=True)\n# grid_df.head()","818db53b":"# eval_model_on_train_val(grid)","ce2277ed":"def tukey_outliers(x):\n    q1 = np.percentile(x, 25)\n    q3 = np.percentile(x, 75)\n    \n    iqr = q3 - q1\n    \n    lower_boundary = q1 - (iqr * 1.5)\n    upper_boundary = q3 + (iqr * 1.5)\n    \n    outliers = x[(x < lower_boundary) | (x > upper_boundary)]\n    return outliers, lower_boundary, upper_boundary\n\noutliers_bmi, lower_boundary_bmi, upper_boundary_bmi = tukey_outliers(X_train[\"bmi\"])\noutliers_glucose, lower_boundary_glucose, upper_boundary_glucose = tukey_outliers(X_train[\"avg_glucose_level\"])\nlen(outliers_bmi), len(outliers_glucose)","7d466e4a":"X_train, X_val, X_test, y_train, y_val, y_test = train_validation_test_split(X=df_copy.drop(columns='stroke'), y=df_copy.stroke, train_size=0.70)","00ff3b9a":"X_train_iqr = X_train.copy()\nX_train_iqr['bmi'].clip(lower_boundary_bmi, upper_boundary_bmi, inplace=True)\nX_train_iqr['avg_glucose_level'].clip(lower_boundary_glucose, upper_boundary_glucose, inplace=True)","bd57af23":"X_train_iqr.head()","aa822870":"X_train_rs = X_train.copy()\nX_train_rs.loc[:, 'age': 'bmi'] = rs.fit_transform(X_train_rs.loc[:, 'age': 'bmi'])\n\nplt.figure(figsize=(12,6))\nsns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(X_train_rs[nums]), palette=\"cubehelix\")\n\nsns.despine(left=True, bottom=True)\nplt.grid(which='both', axis='y', zorder=0, color='black', linestyle=':', dashes=(2,7), alpha=0.3)\nplt.title('Robust Scaler | Before Outlier Removal', fontsize=20,fontweight='bold', fontfamily='monospace')\nplt.xlabel(None)\nplt.ylabel(None)\nplt.show()","c9b01c36":"X_train_iqr_rs = X_train_iqr.copy()\nX_train_iqr_rs.loc[:, 'age': 'bmi'] = rs.fit_transform(X_train_iqr_rs.loc[:, 'age': 'bmi'])\n\nplt.figure(figsize=(12,6))\nsns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(X_train_iqr_rs[nums]), palette=\"cubehelix\")\n\nsns.despine(left=True, bottom=True)\nplt.grid(which='both', axis='y', zorder=0, color='black', linestyle=':', dashes=(2,7), alpha=0.3)\nplt.title('Robust Scaler | After Outlier Removal', fontsize=20,fontweight='bold', fontfamily='monospace')\nplt.xlabel(None)\nplt.ylabel(None)\nplt.show()","0beed65f":"from sklearn.preprocessing import OrdinalEncoder\nord_encoder = OrdinalEncoder()\nX_val_iqr = X_val.copy()\nX_train_iqr.loc[:, 'gender': 'smoking_status'] = ord_encoder.fit_transform(X_train_iqr.loc[:, 'gender': 'smoking_status'])\nX_val_iqr.loc[:, 'gender': 'smoking_status'] = ord_encoder.transform(X_val_iqr.loc[:, 'gender': 'smoking_status'])","f1874620":"etc_pipeline.fit(X_train_iqr, y_train)","3530ff65":"y_pred = etc_pipeline.predict(X_train_iqr)\nprint('[INFO] Evaluating on training set ...')\nprint(confusion_matrix(y_train, y_pred))\nprint(classification_report(y_train, y_pred, target_names=target_names))\n\ny_pred = etc_pipeline.predict(X_val_iqr)\nprint('\\n[INFO] Evaluating on validation set ...')\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred, target_names=target_names))","8b403880":"from sklearn.ensemble import IsolationForest\n\nX_train_tf = pipe_1.fit_transform(X_train)\nX_val_tf = pipe_1.transform(X_val)\n\n# identify outliers in the training dataset\niso = IsolationForest(contamination=0.1, random_state=42)\n\nyhat = iso.fit_predict(X_train_tf)\n# select all rows that are not outliers\nmask = (yhat != -1)\nX_train_iso, y_train_iso = X_train_tf[mask, :], y_train[mask]\n# summarize the shape of the updated training dataset\nprint(X_train_iso.shape, y_train_iso.shape)","1cb55f1b":"rfc = RandomForestClassifier(random_state=42)\nrfc.fit(X_train_iso, y_train_iso)","a225d57f":"xgb_clf = xgb.XGBClassifier(use_label_encoder=False, random_state=42)\nxgb_clf.fit(X_train_iso, y_train_iso)","438303d8":"eval_model_on_train_val(xgb_clf, transformed=True)","74d00ef0":"from sklearn.base import BaseEstimator, TransformerMixin\n\nBMI_idx = 13\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bmi_range=True): # no *args or **kargs\n        self.add_bmi_range = add_bmi_range\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X):\n        df = pd.DataFrame(X)\n        # display(df.head())\n        \n        if self.add_bmi_range:\n        \n            def create_bmi_range(bmi):\n                # create bmi_range\n                if bmi < 18.5:\n                    return 0  # underweight\n                elif bmi < 25.0:\n                    return 1  # normal\n                elif bmi < 30.0:\n                    return 2  # overweight\n                elif bmi < 40.0:\n                    return 3  # obesity\n                else:\n                    return 4  # extreme obesity\n            \n            # 'bmi_range' will become the last column -> 14\n            df['bmi_range'] = df.loc[:, BMI_idx].apply(create_bmi_range)\n            \n            # drop the bmi column\n            df = df.drop(columns=BMI_idx)\n            # display(df.head())\n            \n            return df.to_numpy()\n        else:\n            return X","8a784fb0":"categorical_cols = ['gender', 'ever_married', \n                    'work_type', 'Residence_type', \n                    'smoking_status', 'hypertension', \n                    'heart_disease']","4ed68470":"from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler, OrdinalEncoder\nfrom sklearn.impute import KNNImputer\nfrom sklearn.pipeline import Pipeline\n\ncat_pipeline = Pipeline([\n    (\"one_hot_encoder\", OneHotEncoder(drop='first')),\n    # (\"label_encoder\", OrdinalEncoder()),\n    ('std_scaler', StandardScaler(with_mean=False)),\n])\n\nbmi_pipeline = Pipeline([\n    ('std_scaler_bmi', StandardScaler(with_mean=False)),\n    # ('robust_scaler_bmi', RobustScaler(with_centering=False)),\n    # add_indicator is very useful to add extra columns denoting missing values\n    ('KNN_imputer', KNNImputer(n_neighbors=10, add_indicator=True))\n])\n\ntransformer_all = ColumnTransformer([\n    ('std_scaler_glucose', StandardScaler(with_mean=False), ['avg_glucose_level']),\n    ('std_scaler_age', StandardScaler(with_mean=False), ['age']),\n    ('cat', cat_pipeline, categorical_cols),\n    ('bmi_pipe', bmi_pipeline, ['bmi'])  # index = 13\n], remainder=\"passthrough\")\n\npipe_1 = Pipeline([\n    ('feature_transform', transformer_all),\n    ('bmi_range_adder', CombinedAttributesAdder()),\n    \n    # ('select_kbest_8', SelectKBest(k=8)),\n    # ('pca_2', PCA(n_components=2)),\n    \n    # ('rfc', RandomForestClassifier()),\n    # ('xgb', xgb.XGBClassifier(use_label_encoder=False)),\n])","1852761e":"# X_train_tf = pd.DataFrame(pipe_1.fit_transform(X_train))\nX_train_tf = (pipe_1.fit_transform(X_train))\nX_val_tf = (pipe_1.transform(X_val))","5090950d":"# identify outliers in the training dataset\niso = IsolationForest(contamination=0.1, random_state=42)\n\nyhat = iso.fit_predict(X_train_tf)\n# select all rows that are not outliers\nmask = (yhat != -1)\nX_train_iso, y_train_iso = X_train_tf[mask, :], y_train[mask]\n# summarize the shape of the updated training dataset\nprint(X_train_iso.shape, y_train_iso.shape)","a3fb61dd":"rfc = RandomForestClassifier(random_state=42)\nrfc.fit(X_train_iso, y_train_iso)","110df2da":"xgb_clf = xgb.XGBClassifier(use_label_encoder=False, random_state=42)\nxgb_clf.fit(X_train_iso, y_train_iso)","221d3b7b":"eval_model_on_train_val(xgb_clf, transformed=True)","45f0ad57":"# from sklearn.ensemble import GradientBoostingClassifier\n# from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n# from sklearn.metrics import f1_score, make_scorer\n\n# X_train_tf = pipe_1.fit_transform(X_train)\n# X_val_tf = pipe_1.transform(X_val)\n\n# gbc = GradientBoostingClassifier(random_state=42)\n\n# gbc_param_grid = {    \n#     'n_estimators': [i for i in range(50,201,50)],\n#     'max_depth': range(4, 9),\n#     'min_samples_split': [i for i in range(8, 17, 2)],\n#     'max_leaf_nodes': [i for i in range(8, 15, 2)],\n# }\n\n# grid = GridSearchCV(estimator=gbc, param_grid=gbc_param_grid, cv=3,\n#                     scoring={'f1': make_scorer(f1_score)}, refit='f1',\n#                     verbose=1, n_jobs=-1)\n\n# grid.fit(X_train_tf, y_train)\n\n# print('\\nBest params -> {}'.format(grid.best_params_))\n# print('Best score -> {}'.format(grid.best_score_))\n# print('Validation score -> {}'.format(grid.score(X_val_tf, y_val)))","ef702ae8":"# X_train_tf = pipe_1.fit_transform(X_train)\n# X_val_tf = pipe_1.transform(X_val)\n\n# xgb_clf = xgb.XGBClassifier(use_label_encoder=False, random_state=42)\n\n# param_grid = {\n#     \"max_depth\": [3, 4, 5, 7],\n#     \"learning_rate\": [0.1, 0.01, 0.05],\n#     \"gamma\": [0, 0.25, 1],\n#     \"reg_lambda\": [0, 1, 10],\n#     \"scale_pos_weight\": [1, 3, 5],\n#     \"subsample\": [0.8],\n#     \"colsample_bytree\": [0.5],\n# }\n\n# grid = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, cv=3,\n#                     scoring={'f1': make_scorer(f1_score)}, refit='f1',\n#                     verbose=1, n_jobs=-1)\n\n# grid.fit(X_train_tf, y_train)\n\n# print('\\nBest params -> {}'.format(grid.best_params_))\n# print('Best score -> {}'.format(grid.best_score_))\n# print('Validation score -> {}'.format(grid.score(X_val_tf, y_val)))","5730ce0d":"# eval_model_on_train_val(grid, transformed=True)","27f8f404":"# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.model_selection import RandomizedSearchCV, RepeatedStratifiedKFold\n# from sklearn.metrics import f1_score, make_scorer\n\n# X_train_tf = pipe_1.fit_transform(X_train)\n# X_val_tf = pipe_1.transform(X_val)\n\n# # Number of trees in random forest\n# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# # Number of features to consider at every split\n# max_features = ['auto', 'sqrt']\n# # Maximum number of levels in tree\n# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n# max_depth.append(None)\n# # Minimum number of samples required to split a node\n# min_samples_split = [2, 5, 10]\n# # Minimum number of samples required at each leaf node\n# min_samples_leaf = [1, 2, 4]\n# # Method of selecting samples for training each tree\n# bootstrap = [True, False]\n# # Create the random grid\n# random_grid = {'n_estimators': n_estimators,\n#                'max_features': max_features,\n#                'max_depth': max_depth,\n#                'min_samples_split': min_samples_split,\n#                'min_samples_leaf': min_samples_leaf,\n#                'bootstrap': bootstrap}\n\n# # cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n# cv = 3\n\n# # Use the random grid to search for best hyperparameters\n# # First create the base model to tune\n# rf = RandomForestClassifier()\n# # Random search of parameters, using 3 fold cross validation, \n# # search across 100 different combinations, and use all available cores\n# rf_random = RandomizedSearchCV(estimator=rf,\n#                                param_distributions=random_grid,\n#                                n_iter=100,\n#                                cv=cv,\n#                                scoring={'f1': make_scorer(f1_score)}, \n#                                refit='f1',\n#                                verbose=1,\n#                                random_state=42,\n#                                n_jobs=-1)\n# # Fit the random search model\n# rf_random.fit(X_train_tf, y_train)","d6e74200":"# print('\\nBest params -> {}'.format(rf_random.best_params_))\n# print('Best score -> {}'.format(rf_random.best_score_))\n# print('Validation score -> {}'.format(rf_random.score(X_val_tf, y_val)))","fc327981":"# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n# from sklearn.metrics import f1_score, make_scorer\n\n# X_train_tf = pipe_1.fit_transform(X_train)\n# X_val_tf = pipe_1.transform(X_val)\n\n# rfc = RandomForestClassifier(random_state=42)\n\n# # param_grid = {    \n# #     'n_estimators': [i for i in range(50,251,50)],\n# #     'max_depth': range(4, 11),\n# #     'min_samples_split': [i for i in range(8, 17, 2)],\n# #     'max_leaf_nodes': [i for i in range(8,17,2)],\n# # }\n\n# param_grid = {\n#     'bootstrap': [False],\n#     'max_depth': range(35, 46),\n#     'max_features': [1, 2, 3],\n#     'min_samples_leaf': [1, 2, 3, 4],\n#     'min_samples_split': [1, 2, 3, 4],\n#     'n_estimators': [1300, 1350, 1400, 1450, 1500]\n# }\n\n# # cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n# cv = 3\n\n# grid = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=cv,\n#                     scoring={'f1': make_scorer(f1_score)}, refit='f1',\n#                     verbose=1, n_jobs=-1)\n\n# grid.fit(X_train_tf, y_train)\n\n# print('\\nBest params -> {}'.format(grid.best_params_))\n# print('Best score -> {}'.format(grid.best_score_))\n# print('Validation score -> {}'.format(grid.score(X_val_tf, y_val)))","a9b5de73":"X_train_tf = pipe_1.fit_transform(X_train)\nX_val_tf = pipe_1.transform(X_val)","5c0e0b20":"# random forest with random undersampling for imbalanced classification\nfrom sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\nfrom imblearn.ensemble import BalancedRandomForestClassifier\n\nmodel = BalancedRandomForestClassifier(n_estimators=10, random_state=42)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\nscores = cross_val_score(model, X_train_tf, y_train, scoring='f1', cv=cv, n_jobs=-1)\nprint('Mean F1 score: %.3f' % np.mean(scores))","ac83cf02":"from imblearn.ensemble import EasyEnsembleClassifier\n\nmodel = EasyEnsembleClassifier(n_estimators=10, random_state=42)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\nscores = cross_val_score(model, X_train_tf, y_train, scoring='f1', cv=cv, n_jobs=-1)\nprint('Mean F1 score: %.3f' % np.mean(scores))","5f676538":"from imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import TomekLinks\n\nresample = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'), random_state=42)","a74668be":"X_res, y_res = resample.fit_resample(X_train_tf, y_train)","85c23433":"X_res_df, y_res_series = pd.DataFrame(X_res), pd.Series(y_res)","8e33f7d6":"X_res_df.head()","43d90af4":"X_res_df.isna().sum()","e8ddbc6e":"y_res_series.value_counts(), y_train.value_counts(), y_val.value_counts(), y_test.value_counts()","18c0c8ea":"from sklearn.model_selection import RepeatedStratifiedKFold\nmodel = xgb.XGBClassifier(use_label_encoder=False, random_state=42)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\nscores = cross_val_score(model, X_res, y_res, scoring='f1', cv=cv, n_jobs=-1)\nprint('Mean F1 score: %.3f' % np.mean(scores))","df6bc20f":"model = xgb.XGBClassifier(random_state=42)\nmodel.fit(X_res, y_res)","82d17ac9":"eval_model_on_train_val(model, transformed=True)","f06f6f6a":"# !pip install pycaret","99696709":"# # compare machine learning algorithms on the sonar classification dataset\n# from pycaret.classification import setup\n# from pycaret.classification import compare_models, tune_model, create_model\n# from sklearn.ensemble import ExtraTreesClassifier","fd3f1ee2":"# # setup the dataset\n# grid = setup(data=df, target='stroke', silent=True, n_jobs=None, imputation_type='simple')","94a59834":"# # evaluate models and compare models\n# best = compare_models(errors='raise')\n# # report the best model\n# print(best)","8edf23d8":"# model = create_model('qda')","412cfa46":"# # tune model hyperparameters\n# tuned_model = tune_model(best)\n# # report the best model\n# print(tuned_model)","aff705c9":"# model.score()","ef56bf8e":"# Visualizations for preprocessed numerical features","77d6c0c4":"- Stratified split generated very consistent splits by taking into account the percentage of samples for each class of the `stroke` labels.\n- Although it might not improve the model performance, this format is preferable as it is more representative of the proportion of real data.","af847d50":"# Setup","730ae739":"### One Hot Encoding","63ee169b":"## Robust Scaler","58d1ea00":"**Some extra pipelines for testing if necessary**","ca9f3e96":"- The validation score of `f1` is still not much improvement, `f1_macro` always hovers around 60%.","eeabac6c":"- Removing outliers did not improve the validation F1 score. F1_macro is still around 0.60, <br>\nthis is as expected because tree models are generally robust to outliers.","4f931262":"## Try adding BMI_range","36ceb250":"- Power Transformer generated more Gaussian-like distributions as expected, as implied by the almost identical lengths of the two tails of the each of the box plots.\n- Some outliers are still found in `bmi`, which also indicates that the original distribution of `bmi` was very right skewed.\n- This is still not ideal because their scales are still not perfectly same with one another.","240204ce":"# Preprocessing","1d68c9b9":"- Tried other methods and found not much improvement.","55bf49b3":"## Standard Scaler","ced67ade":"## Trying IsolationForest for outlier removal","ede4723d":"- Fine-tuning did not help much with improving the F1 score on `Stroke`, i.e. 0.20 after fine-tuning, before was 0.27.\n- Fine-tuning also made the training scores much worse than before.","d8684545":"# APPENDIX","1ea830f1":"- **NOTE**: The results seem to be not better than using KNN imputer. Therefore, not using this later.","2248211c":"### Train & Fine-tuning","e6cd5936":"- Quantile transformer totally removed all the outliers and changed the original distribution of the features, <br>\nwhich could result in loss of original information and correlation with other features, particularly the target feature","0a99110d":"# NOTE","e43abcad":"- Once again, removing outliers this way did not improve the validation F1 score. F1_macro is still lower than 0.60.","d1a57597":"This is the 3rd part of the series where I explore and model the stroke prediction dataset. <br>\nHere are the notebooks in the series in the correct order:\n\n1. **[Exploratory Data Analysis (EDA)](https:\/\/www.kaggle.com\/ansonnn\/stroke-prediction-eda)** <br> - Exploring the dataset to derive insights about distributions and relationships between features.<br><br>\n2. **[Statistical Analysis](https:\/\/www.kaggle.com\/ansonnn\/stroke-prediction-statistical-analysis)** <br> - Analyzing the normality of the features and their correlations. <br><br>\n3. **Feature Engineering and Modelling** - current notebook <br> - Preprocessing the features and building a model for evaluation","47da46b3":"## Removal of outliers via IQR","a663b8ba":"## Splitting Dataset","4c288443":"**First trial of training**","1394bf25":"- Both dependence plots of `avg_glucose_level` and `bmi` also show their respective thresholds where stroke is more prevalent at higher values.\n- Threshold is at around 150 for `avg_glucose_level`; while the threshold for `bmi` can be seen more clearly at closer to 30.\n- **NOTE**: The numerical features here are not scaled with any sort of algorithm yet, after sorting, the SHAP values would likely be different.\n- This is to allow us to see the impact of the actual values themselves in the dependence plots.","33bb6f1b":"- Out of all the missing BMI values, the percentage of data that accounts for stroke sufferers is a whopping **16%**. Therefore, this has to be dealt with appropriately.","dff066ba":"SHAP (SHapley Additive exPlanations) is a very good way to explain and interpret a model as it is much more intuitive compared to traditional means like feature importance.","571830d2":"### Imputing missing BMI values using a regressor","263d3484":"## Log Transformation","7cd655cf":"- Standard scaler is significantly affected by outliers.","37ba7527":"**NOTE**\n\n- The most important metrics to look out for are: precision, recall and especially F1 scores (with its variations, e.g. F1 macro-averaged).\n- F1 score will be the primary metric to monitor here due to the highly imbalanced `stroke` class (i.e. our `target` variable).\n- After conducting some quick research, it is decided that `ROC AUC` score is not chosen here because it is generally not good for imbalanced datasets, <br>\nF1 score would work better for evaluating the model's performance on imbalanced datasets.","b59cb3c5":"## SMOTE Tomek Resampling","250ce808":"- `avg_glucose_level` and `bmi` are right-skewed and have a lot of outliers.","0ca4cdf9":"- Validation score of f1 is not much better than before tuning.","83187e08":"## Missing BMI values","0451cf5d":"- Typical usage of `KNNImputer`:\n- Scale -> Impute -> Inverse scale\n- The reverse scaling is also performed in order to preserve the original values for interpreting later.","42af0358":"- Robust scaler scales them to about the same scale while still retaining their distributions, much better than Standard Scaler.\n- Robust scaler is commonly used to combat outliers in the distributions, by scaling the features in a way (using quartiles) that will not be influenced by outliers. ","b38ae9ea":"## Evaluation function","c72cbd06":" - In this case, `Ordinal Encoding` performs better than `One Hot Encoding` after testing on both.\n - Now move on to perform some hyperparameter tuning.","008f03b2":"## Random Forest with Resampling","9326e8ac":"# PyCaret Library","e6ac7033":"- It seems that `ExtraTreeClassifier` works the best when averaging both training and validation sets. <br>\n- `DecisionTreeClassifer` and `RandomForestClassifier` also performed quite well in comparison. <br>\nThese are some of the most popular models used. Therefore they will be used as the primary models for further exploring.","2d2de3e1":"- On the test set that has never been seen before, it performed very poorly with F1-score of below 0.10.\n- This is likely due to the imbalance of the dataset itself, or due to insufficient features to train a good predictive model.","9641783d":"## Testing with a shorter pipeline on pre-encoded features","016835fe":"- Using Ordinal Encoding as it produced the best results after testing","10854093":"## Original distribution","4ee9a182":"I have spent quite a long time trying to model this as this is my first time trying out so many things while building a model... <br>\nSo please have a look and thanks for checking out! <br>\n\nThere are numerous methods I tried to improve the model performance but did not help much, but I decided to include them under the APPENDIX section.\n\nSome of the notable problems in the dataset are:\n1. Imbalanced `stroke` classes - tried using SMOTE + Tomek to overcome this but seems like did not help much (can refer under APPENDIX section)\n2. Many outliers in the `avg_glucose_level` and `bmi` features - Tried removing using IsolationForest or IQR (APPENDIX section)\n3. Many missing values in the `bmi` features - Tried median imputation, DecisionTreeRegressor imputation, and KNearestNeighbor Imputation and settled with KNNImputer.","6465d313":"- Training score has dropped significantly after tuning for some reason I am not sure.\n- Validation score of macro-averaged is still not much better than before tuning.","b10dc352":"## Try tuning other classifiers","a106642e":"# Training","a0c6859b":"- From the dependence plot, we can see that `age` has a trend of increasing with SHAP values too.\n- This graph also shows that higher `age` -> more `stroke` predictions.","ff0c863c":"## Power Transformer","6ef6f708":"## Training Pipelines","b730b0d9":"- SHAP value is just like feature importance, but this graph can provide more insights by showing how much each feature can affect the target variable (model output) in both negative and positive predictions.\n- In this graph, the colors also represent the values of the features, from low to high value -> from blue to red\n- In this case, all the numerical features have significant impact on the model output, particularly the `age` feature, <br>\nwhich has the highest impact for predicting `stroke`.\n- `bmi_NaN` (just an indicator for missing BMI values) also shows very high impact on predicting `stroke`.","fa4af055":"## Choosing a model by using `lazypredict` library","aa162c12":"# CONCLUSION\n- More data needs to be collected especially for stroke sufferers in order to build a more robust model to predict whether a person is suffered from stroke or not.\n- The selection of data to be collected also needs to be more balanced and contains more features if possible.","34876387":"Best params -> {'imputer__n_neighbors': 1, 'select_kbest__k': 3, 'xgb_clf__colsample_bytree': 0.5, 'xgb_clf__gamma': 1, 'xgb_clf__learning_rate': 0.4, 'xgb_clf__max_depth': 4, 'xgb_clf__reg_lambda': 9, 'xgb_clf__scale_pos_weight': 5, 'xgb_clf__subsample': 0.8} <br>\nBest score -> 0.5945212854754867 <br>\nValidation score -> 0.5837303262377134","3a1cf53f":"Best params -> {'xgb_clf__subsample': 0.6, 'xgb_clf__min_child_weight': 1, 'xgb_clf__max_depth': 3, 'xgb_clf__gamma': 0, 'xgb_clf__colsample_bytree': 1.0, 'select_kbest__k': 10, 'imputer__n_neighbors': 5} <br>\nBest score -> 0.5599306011204763 <br>\nValidation score -> 0.5379518072289157","97a29e9e":"- F1 score for stroke is still around 10-15%","89d8bd76":"### Ordinal Encoding","d9d15b03":"- Ordinal encoding works the same with label encoding.","6e37e996":"### Final Evaluation on Test Set","038f5664":"## Hyperparameter Tuning on XGBoost","76212a88":"## Interpreting the model with SHAP","99ba9ad9":"- Log transformation resulted in very different scales, which is not good for training.","4eef2359":"#### ExtraTreeClassifier","cb19e4af":"## Quantile Transformer"}}