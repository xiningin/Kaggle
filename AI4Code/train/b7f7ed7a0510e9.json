{"cell_type":{"aac0a793":"code","89a2f8b7":"code","d97c796d":"code","4fad195b":"code","bbe86a2d":"code","8f39ab9e":"code","cd8255f1":"code","eceb109e":"code","2cbc1e6a":"code","b899edf7":"code","5a42356f":"code","4b046e9d":"code","0c7f361d":"code","464d29f8":"code","800bb408":"code","b3716008":"code","52dd4575":"code","e58c1dcd":"code","3d1263f6":"code","03116590":"code","ef32f1ed":"code","ec535de4":"code","a68f6cc9":"code","4917fe93":"code","980815b4":"code","300e4967":"code","c9d2598e":"code","06913fa4":"code","a77edfa4":"code","ccca41d7":"code","aec579ea":"code","fa866e03":"code","b1220869":"markdown","b16b5552":"markdown","1ce68d77":"markdown","5be16609":"markdown","5b79c54f":"markdown","4d702caf":"markdown"},"source":{"aac0a793":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","89a2f8b7":"from zipfile import ZipFile\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom subprocess import check_output\nfrom scipy import stats\nfrom scipy.stats import skew, norm, probplot, boxcox\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler","d97c796d":"for dirname, _, filenames in os.walk('\/kaggle\/input\/instacart-market-basket-analysis\/'):\n    for filename in filenames:        \n        archive = ZipFile(os.path.join(dirname, filename), mode='r')\n        archive.extractall(path=\"\/kaggle\/working\")\n        archive.close()\n\nprint(check_output([\"ls\", \"..\/working\"]).decode(\"utf8\"))","4fad195b":"order = pd.read_csv('.\/orders.csv')\norder_products_prior = pd.read_csv('.\/order_products__prior.csv')\norder_products_train = pd.read_csv('.\/order_products__train.csv')","bbe86a2d":"deparment = pd.read_csv('.\/departments.csv')\nproduct = pd.read_csv('.\/products.csv')\naisle = pd.read_csv('.\/aisles.csv')","8f39ab9e":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        col_type2 = df[col].dtype.name\n        \n        if ((col_type != object) and (col_type2 != 'category')):\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","cd8255f1":"# order = reduce_mem_usage(order)\n# order_products_prior = reduce_mem_usage(order_products_prior)\n# order_products_train = reduce_mem_usage(order_products_train)\n# deparment = reduce_mem_usage(deparment)\n# product = reduce_mem_usage(product)\n# aisle = reduce_mem_usage(aisle)","eceb109e":"order.describe().T","2cbc1e6a":"print('Data size of the order Dataset is {}'.format(order.shape))\nprint('Data size of the order_products_prior Dataset is {}'.format(order_products_prior.shape))\nprint('Data size of the order_products_train Dataset is {}'.format(order_products_train.shape))\nprint('Data size of the products Dataset is {}'.format(product.shape))\nprint('Data size of the departments Dataset is {}'.format(deparment.shape))\nprint('Data size of the aisles Dataset is {}'.format(aisle.shape))","b899edf7":"order[order.order_id == 2539329]","5a42356f":"order_products_prior[(order_products_prior.order_id == 3343014)]","4b046e9d":"order[order.order_id == 1187899]","0c7f361d":"order_products_train[(order_products_train.order_id == 1187899)]","464d29f8":"# deal with missing value of the column day_since_prior_order\norder.days_since_prior_order = order.days_since_prior_order.fillna(0)\n\n## get information about the number line, number line by reordered of each order on eval_set = train\ntrain_0 = order_products_train[order_products_train.reordered == 0].groupby(['order_id','reordered'])['product_id'].count().reset_index()\ntrain_1 = order_products_train[(order_products_train.reordered == 1)].groupby(['order_id','reordered'])['product_id'].count().reset_index()\ntrain_0.rename(columns={'product_id':'reordered_0'}, inplace=True)\ntrain_0 = train_0.drop('reordered', axis = 1)\ntrain_1.rename(columns={'product_id':'reordered_1'}, inplace=True)\ntrain_1 = train_1.drop('reordered', axis = 1)\ntrain_reordered = pd.merge(train_1, train_0, how = 'outer', on = 'order_id')\ntrain_reordered.fillna({'reordered_1':0, 'reordered_0':0}, inplace=True)\ndel train_0\ndel train_1\n\n\n## get information about the number line, number line by reordered of each order on eval_set = prior\nprior_0 = order_products_prior[order_products_prior.reordered == 0].groupby(['order_id','reordered'])['product_id'].count().reset_index()\nprior_1 = order_products_prior[(order_products_prior.reordered == 1)].groupby(['order_id','reordered'])['product_id'].count().reset_index()\nprior_0.rename(columns={'product_id':'reordered_0'}, inplace=True)\nprior_0 = prior_0.drop('reordered', axis = 1)\nprior_1.rename(columns={'product_id':'reordered_1'}, inplace=True)\nprior_1 = prior_1.drop('reordered', axis = 1)\nprior_reordered = pd.merge(prior_1, prior_0, how = 'outer', on = 'order_id')\nprior_reordered.fillna({'reordered_1':0, 'reordered_0':0}, inplace=True)\ndel prior_0\ndel prior_1\n\n## concat two datafarm: train and prior\n\ndf_reordered = pd.concat([prior_reordered,train_reordered])\ndf_reordered = df_reordered.sort_values(by = 'order_id', ascending= True).reset_index()\ndf_reordered = df_reordered.drop('index', axis = 1)\n\n## deal with missing value of the column reordered_0\ndf_reordered.fillna({'reordered_1':0, 'reordered_0':0}, inplace=True)\n\n## get information user_id from the Order Dataset\n\ndf_reordered = df_reordered.merge(order[['user_id','order_id','order_number','days_since_prior_order']], how = 'left', on = 'order_id')\n\ndf_reordered['total_line'] = df_reordered.reordered_0 + df_reordered.reordered_1\n\n\n## create a datafarm about order detail\n\ncus_orderdetail_df = df_reordered.groupby(['user_id']).agg({\n    'reordered_0': 'sum',\n    'reordered_1': 'sum',\n    'total_line' :'sum',\n    'order_number': 'count',\n    'days_since_prior_order': 'mean'\n}).reset_index()\n\ncus_orderdetail_df.rename(columns={'days_since_prior_order':'recency','order_number': 'fequency'}, inplace=True)","800bb408":"cus_orderdetail_df.describe().T","b3716008":"def QQ_plot(data, measure):\n    fig = plt.figure(figsize=(20,7))\n\n    #Get the fitted parameters used by the function\n    (mu, sigma) = norm.fit(data[measure])\n    \n    sns.set(style='darkgrid', font_scale=1.0)\n\n    #Kernel Density plot\n    fig1 = fig.add_subplot(121)\n    sns.distplot(data[measure], fit=norm)\n    fig1.set_title(measure + ' Distribution ( mu = {:.2f} and sigma = {:.2f} )'.format(mu, sigma), loc='center')\n    fig1.set_xlabel(measure)\n    fig1.set_ylabel('Frequency')\n\n    #QQ plot\n    fig2 = fig.add_subplot(122)\n    res = probplot(data[measure], plot=fig2)\n    fig2.set_title(measure + ' Probability Plot (skewness: {:.6f} and kurtosis: {:.6f} )'.format(data[measure].skew(), data[measure].kurt()), loc='center')\n\n    plt.tight_layout()\n    plt.show()","52dd4575":"for i in cus_orderdetail_df.iloc[1,1:].index:\n    QQ_plot(cus_orderdetail_df, i)","e58c1dcd":"# cus_order_df = order.groupby(['user_id']).agg({\n#     'order_number': 'count',\n#     'days_since_prior_order': ['min','mean','median', 'max']\n# })\n\n# cus_order_df.columns = [ ' '.join(str(i) for i in col) for col in cus_order_df.columns]\n# cus_order_df = cus_order_df.reset_index()","3d1263f6":"error = 0.0001\ncus_orderdetail_df['fequency_log'] = np.log(cus_orderdetail_df['fequency'])\ncus_orderdetail_df['reordered_0_log'] = np.log(cus_orderdetail_df['reordered_0'])\ncus_orderdetail_df['total_line_log'] = np.log(cus_orderdetail_df['total_line'])\ncus_orderdetail_df['recency_log'] = np.log(cus_orderdetail_df['recency']+ error)\ncus_orderdetail_df['reordered_1_log'] = np.log(cus_orderdetail_df['reordered_1']+ error)\nfeature_vector = ['fequency_log','recency_log','reordered_0_log','reordered_1_log','total_line_log']\nX_subset = cus_orderdetail_df[feature_vector]\nscaler = StandardScaler()\nX_subset[feature_vector] = scaler.fit_transform(X_subset[feature_vector])","03116590":"cl = 10\ncorte = 0.1\n\nanterior = 100000000000000\ncost = [] \nK_best = cl\n\nfor k in range (1, cl+1):\n    # Create a kmeans model on our data, using k clusters.  random_state helps ensure that the algorithm returns the same results each time.\n    model = KMeans(\n        n_clusters=k, \n        init='k-means++', #'random',\n        n_init=10,\n        max_iter=300,\n        tol=1e-04,\n        random_state=101)\n\n    model = model.fit(X_subset)\n\n    # These are our fitted labels for clusters -- the first cluster has label 0, and the second has label 1.\n    labels = model.labels_\n \n    # Sum of distances of samples to their closest cluster center\n    interia = model.inertia_\n    if (K_best == cl) and (((anterior - interia)\/anterior) < corte): K_best = k - 1\n    cost.append(interia)\n    anterior = interia\n\nplt.figure(figsize=(8, 6))\nplt.scatter(range (1, cl+1), cost, c='red')\nplt.show()\n\n# Create a kmeans model with the best K.\nprint('The best K sugesst: ',K_best)\nmodel = KMeans(n_clusters=K_best, init='k-means++', n_init=10,max_iter=300, tol=1e-04, random_state=101)\n\n# Note I'm scaling the data to normalize it! Important for good results.\nmodel = model.fit(X_subset)\n\n# These are our fitted labels for clusters -- the first cluster has label 0, and the second has label 1.\nlabels = model.labels_\n\n# And we'll visualize it:\nfig = plt.figure(figsize=(20,5))\nax = fig.add_subplot(121)\nplt.scatter(x = X_subset.iloc[:,1], y = X_subset.iloc[:,0], c=model.labels_.astype(float))\nax.set_xlabel(feature_vector[1])\nax.set_ylabel(feature_vector[0])\n\nplt.show()","ef32f1ed":"from sklearn.metrics import silhouette_samples, silhouette_score\n\n# A list holds the silhouette coefficients for each k\nsilhouette_coefficients = []\n\n# Notice you start at 2 clusters for silhouette coefficient\nfor k in range(2, 11):\n    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=10,max_iter=300, tol=1e-04, random_state=101)\n    kmeans.fit(X_subset)\n    score = silhouette_score(X_subset, kmeans.labels_)\n    silhouette_coefficients.append(score)","ec535de4":"plt.figure(figsize = (15,6))\nplt.plot(range(2, 11), silhouette_coefficients)\nplt.xticks(range(2, 11))\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"Silhouette Coefficient\")\nplt.show()","a68f6cc9":"cluster_centers = dict()\n\ncluster_num = [2, 5, 8]\nfor n_clusters in cluster_num:\n\n    clusterer = KMeans(n_clusters=n_clusters, init='k-means++', n_init=10,max_iter=300, tol=1e-04, random_state=101)\n    cluster_labels = clusterer.fit_predict(X_subset)\n    silhouette_avg = silhouette_score(X = X_subset, labels = cluster_labels)\n    cluster_centers.update({n_clusters :{'cluster_center':clusterer.cluster_centers_,\n                                         'silhouette_score':silhouette_avg,\n                                         'labels':cluster_labels}\n                           })\n\n    sample_silhouette_values = silhouette_samples(X = X_subset, labels = cluster_labels)","4917fe93":"## get the data of cluster center\ncent_transformed = scaler.inverse_transform(cluster_centers[2]['cluster_center'])\ncluster_2 = pd.DataFrame(np.exp(cent_transformed),columns=features)\ncent_transformed = scaler.inverse_transform(cluster_centers[5]['cluster_center'])\ncluster_5 = pd.DataFrame(np.exp(cent_transformed),columns=features)\ncent_transformed = scaler.inverse_transform(cluster_centers[8]['cluster_center'])\ncluster_8 = pd.DataFrame(np.exp(cent_transformed),columns=features)\n\n## add the column Number Cluster\n\ncluster_2['Number_Cluster'] = 'Cluster_2'\ncluster_5['Number_Cluster'] = 'Cluster_5'\ncluster_8['Number_Cluster'] = 'Cluster_8'\n\n## reset index\ncluster_2 = cluster_2.reset_index()\ncluster_5 = cluster_5.reset_index()\ncluster_8 = cluster_8.reset_index()\n\n## concat the datafarms\n\ncluster_center = pd.concat([cluster_2, cluster_5, cluster_8])\ncluster_center.rename(columns={'index':'Cluster'}, inplace=True)","980815b4":"cluster_center.to_csv(\"cluster_center.csv\", index = False, header = True)","300e4967":"features = ['fequency','recency','reordered_0','reordered_1','total_line']\nfor i in cluster_num:\n    print(\"for {} clusters the silhouette score is {:1.2f}\".format(i, cluster_centers[i]['silhouette_score']))\n    print(\"Centers of each cluster:\")\n    cent_transformed = scaler.inverse_transform(cluster_centers[i]['cluster_center'])\n    print(pd.DataFrame(np.exp(cent_transformed),columns=features))\n    print('-'*50)","c9d2598e":"cus_orderdetail_df['clusters_2'] = cluster_centers[2]['labels'] \ncus_orderdetail_df['clusters_5'] = cluster_centers[5]['labels']\ncus_orderdetail_df['clusters_8'] = cluster_centers[8]['labels']\ndisplay(cus_orderdetail_df.head())","06913fa4":"cus_orderdetail_df","a77edfa4":"\"\"\"\nThe list style using for matplotlib pyplot\n\n['Solarize_Light2', '_classic_test_patch', 'bmh', 'classic', 'dark_background', 'fast', \n'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn', 'seaborn-bright', 'seaborn-colorblind', \n'seaborn-dark', 'seaborn-dark-palette', 'seaborn-darkgrid', 'seaborn-deep', 'seaborn-muted', \n'seaborn-notebook', 'seaborn-paper', 'seaborn-pastel', 'seaborn-poster', 'seaborn-talk', 'seaborn-ticks', \n'seaborn-white', 'seaborn-whitegrid', 'tableau-colorblind10']\n\n\n\n\"\"\"\n\nplt.style.use(['fivethirtyeight', 'bmh'])\nfig = plt.figure(figsize=(20,7))\nf1 = fig.add_subplot(131)\nmarket = cus_orderdetail_df.clusters_2.value_counts()\ng = plt.pie(market, labels=market.index, autopct='%1.1f%%', shadow=True, startangle=90)\nplt.title('2 Clusters')\nf1 = fig.add_subplot(132)\nmarket = cus_orderdetail_df.clusters_5.value_counts()\ng = plt.pie(market, labels=market.index, autopct='%1.1f%%', shadow=True, startangle=90)\nplt.title('5 Clusters')\nf1 = fig.add_subplot(133)\nmarket = cus_orderdetail_df.clusters_8.value_counts()\ng = plt.pie(market, labels=market.index, autopct='%1.1f%%', shadow=True, startangle=90)\nplt.title('8 Clusters')\nplt.show()","ccca41d7":"import plotly as py\nimport plotly.graph_objs as go\npy.offline.init_notebook_mode()\nimport matplotlib.mlab as mlab\nimport matplotlib.cm as cm","aec579ea":"x_data = ['Cluster 0', 'Cluster 1','Cluster 2','Cluster 3','Cluster 4', 'Cluster 5', 'Cluster 6','Cluster 7']\ncolors = ['rgba(93, 164, 214, 0.5)', 'rgba(255, 144, 14, 0.5)', 'rgba(44, 160, 101, 0.5)', 'rgba(255, 65, 54, 0.5)', \n          'rgba(22, 80, 57, 0.5)', 'rgba(127, 65, 14, 0.5)', 'rgba(207, 114, 255, 0.5)', 'rgba(127, 96, 0, 0.5)']\n\nfeatures = ['fequency','recency','reordered_0','reordered_1','total_line']\ncutoff_quantile = 95\n\n\ndef box_visual(n_clusters):\n    cl = 'clusters_' + str(n_clusters)\n    for fild in range(0, len(features)):\n        field_to_plot = features[fild]        \n        y_data = list()\n        ymax = 0\n        for i in np.arange(0,n_clusters):\n            y0 = cus_orderdetail_df[cus_orderdetail_df[cl]==i][field_to_plot].values\n            y0 = y0[y0<np.percentile(y0, cutoff_quantile)]\n            if ymax < max(y0): ymax = max(y0)\n            y_data.insert(i, y0)\n\n        traces = []\n\n        for xd, yd, cls in zip(x_data[:n_clusters], y_data, colors[:n_clusters]):\n                traces.append(go.Box(y=yd, name=xd, boxpoints=False, jitter=0.5, whiskerwidth=0.2, fillcolor=cls,\n                    marker=dict( size=1, ),\n                    line=dict(width=1),\n                ))\n\n        layout = go.Layout(\n            title='Difference in {} with {} Clusters and {:1.2f} Score'.\\\n            format(field_to_plot, n_clusters, cluster_centers[n_clusters]['silhouette_score']),\n            yaxis=dict( autorange=True, showgrid=True, zeroline=True,\n                dtick = int(ymax\/10),\n                gridcolor='black', gridwidth=0.1, zerolinecolor='rgb(255, 255, 255)', zerolinewidth=2, ),\n            margin=dict(l=40, r=30, b=50, t=50, ),\n            paper_bgcolor='white',\n            plot_bgcolor='white',\n            showlegend=False\n        )\n\n        fig = go.Figure(data=traces, layout=layout)\n        fig.show()","fa866e03":"box_visual(2)","b1220869":"## 1.2 Related Data","b16b5552":"# 1. Read and Understand Data","1ce68d77":"## 1.1 Order Data","5be16609":"## 1.3 Reduce datasize","5b79c54f":"## 2. Data Processing","4d702caf":"#### Overview v\u1ec1 dataset:\n\n1.Dataset order ch\u1ee9a th\u00f4ng tin v\u1ec1 \u0111\u01a1n h\u00e0ng nh\u01b0: \n\n     - order_id: ID c\u1ee7a \u0111\u01a1n h\u00e0ng.\n     - user_id: ID c\u1ee7a kh\u00e1ch h\u00e0ng.\n     - eval_set: ki\u1ec3u dataset c\u1ee7a d\u00f2ng d\u1eef li\u1ec7u. \u1ede \u0111\u00e2y c\u00f3 th\u1ec3 l\u00e0: train, prior, test.\n     - order_number: s\u1ed1 th\u1ee9c t\u1ef1 \u0111\u01a1n h\u00e0ng c\u1ee7a kh\u00e1ch h\u00e0ng.\n     - order_dow: ng\u00e0y \u0111\u1eb7t h\u00e0ng trong tu\u1ea7n.\n     - order_hour_of_day: th\u1eddi gian gi\u1edd \u0111\u1eb7t h\u00e0ng trong ng\u00e0y.\n     - day_since_prior_order: kho\u1ea3ng c\u00e1ch th\u1eddi gian so v\u1edbi l\u1ea7n \u0111\u1eb7t h\u00e0ng tr\u01b0\u1edbc.\n\n2. Dataset order_products_prior v\u00e0 order_products_train: s\u1ebd c\u00f9ng m\u1ed9t ki\u1ec3u th\u00f4ng tin ch\u1ec9 kh\u00e1c l\u00e0 d\u00e0nh cho t\u1eadp data train hay prior:\n\n    - order_id: ID c\u1ee7a \u0111\u01a1n h\u00e0ng.\n    - product_id: ID c\u1ee7a s\u1ea3n ph\u1ea9m.\n    - add_to_cart_order: th\u1ee9 t\u1ef1 th\u00eam v\u00e0o gi\u1ecf h\u00e0ng c\u1ee7a \u0111\u01a1n h\u00e0ng.\n    - reordered: s\u1ea3n ph\u1ea9m trong \u0111\u01a1n h\u00e0ng \u0111\u01b0\u1ee3c \u0111\u1eb7t l\u1ea1i."}}