{"cell_type":{"289b9f3c":"code","7f14727f":"code","49235415":"code","6d3b9362":"code","765f399a":"code","0adf24ec":"code","22d1f406":"code","9db11944":"code","5a146124":"code","c762b4bc":"code","910b93d0":"code","e86bfc84":"code","311ef095":"code","cfa73283":"code","4598a85f":"code","0c38e3cd":"code","4f7e6a75":"code","f3dc6496":"code","87db00a3":"code","70376363":"markdown","90942c50":"markdown","088bfeb2":"markdown","6822dca6":"markdown","e3f72d59":"markdown","2c9648e6":"markdown","078fd22f":"markdown","00aa3032":"markdown","aaac9980":"markdown","317aa4fc":"markdown","dd686c8f":"markdown","b1a4f311":"markdown","7719fc2d":"markdown","7ffd1ba4":"markdown","7cd27f95":"markdown","c8144638":"markdown","de509926":"markdown","3ca25c66":"markdown","2f5fa0f6":"markdown","817c2849":"markdown"},"source":{"289b9f3c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7f14727f":"import graphviz, IPython\nimport matplotlib.lines as lines\nfrom matplotlib.ticker import FuncFormatter\nfrom sklearn.tree import export_graphviz\n\ndef draw_tree(tree, df):\n    s = export_graphviz(tree, out_file=None, feature_names=df.columns, filled=True)\n    return graphviz.Source(s)","49235415":"from sklearn import metrics as metrics\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import plot_confusion_matrix\n\ndef metricas(y_train,y_pred_train,y_test,y_pred_test):\n    valores=y.value_counts().index.to_list()\n    \n    # Matriz de confusion: Train\n    cm_train=metrics.confusion_matrix(y_train,y_pred_train,labels=valores)\n    df_cm=pd.DataFrame(cm_train,index=valores,columns=valores)\n    plt.figure(figsize=(8,5))\n    sns.heatmap(df_cm,annot=True,cmap=\"YlGnBu\")\n    plt.title('Matriz de Confusi\u00f3n: Train')\n    plt.xlabel('Predicci\u00f3n')\n    plt.ylabel('Valores Reales')\n    plt.show()\n    \n    # Matriz de confusion: Test\n    cm_test=metrics.confusion_matrix(y_test,y_pred_test,labels=valores)\n    df_cm=pd.DataFrame(cm_test,index=valores,columns=valores)\n    plt.figure(figsize=(8,5))\n    sns.heatmap(df_cm,annot=True,cmap=\"YlGnBu\")\n    plt.title('Matriz de Confusi\u00f3n: Test')\n    plt.xlabel('Predicci\u00f3n')\n    plt.ylabel('Valores Reales')\n    plt.show()\n    \n    accuracy_train=metrics.accuracy_score(y_train,y_pred_train)\n    accuracy_test=metrics.accuracy_score(y_test,y_pred_test)\n    precision_train=metrics.precision_score(y_train,y_pred_train,average='micro')\n    precision_test=metrics.precision_score(y_test,y_pred_test,average='micro')\n    recall_train=metrics.recall_score(y_train,y_pred_train,average='micro')\n    recall_test=metrics.recall_score(y_test,y_pred_test,average='micro')\n    f_score=f1_score(y_test,y_pred_test,average='micro')\n    \n    train = (accuracy_train*100, precision_train*100, recall_train*100)\n    test = (accuracy_test*100, precision_test*100, recall_test*100)\n\n    ind = np.arange(3)  # the x locations for the groups\n    ind_n = np.arange(4)  # the x locations for the groups\n    width = 0.3       # the width of the bars\n    \n    fig = plt.figure(figsize = (8,5))\n    ax = fig.add_subplot(111)\n    \n    rects1 = ax.bar(ind, train, width, color='r')\n    rects2 = ax.bar(ind+width, test, width, color='g')\n    rects3 = ax.bar(3, f_score*100, width, color='b')\n    \n    ax.set_ylabel('Scores')\n    ax.set_xticks(ind_n + width\/2)\n    ax.set_xticklabels( ('Accuracy', 'Precisi\u00f3n', 'Recall', 'F1 Score') )\n    ax.legend( (rects1[0], rects2[0]), ('Train', 'Test') )\n    \n    def autolabel(rects):\n        for rect in rects:\n            h = rect.get_height()\n            ax.text(rect.get_x()+rect.get_width()\/2., 1.00*h, '%.3f'%round(h,3),\n                    ha='center', va='bottom')\n\n    autolabel(rects1)\n    autolabel(rects2)\n    autolabel(rects3)\n    plt.title('Puntajes')\n    plt.ylim(0,120)\n    plt.show()\n    \n    return ","6d3b9362":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly as px","765f399a":"df = pd.read_csv('\/kaggle\/input\/mushroom-classification\/mushrooms.csv')\ndf.head()","0adf24ec":"print(\"Tenemos \",len(df.columns.to_list()),\" caracter\u00edsticas\")\nprint()\nprint(df.columns.to_list())","22d1f406":"df.dtypes","9db11944":"#print(df.columns.to_list())\nfor i in df.drop('class',axis=1).columns.to_list():\n    sns.countplot(x=i, data=df, hue='class')\n    plt.show()","5a146124":"df.isnull().sum()","c762b4bc":"df['veil-type'].unique()","910b93d0":"df_clean = df.drop('veil-type',axis=1).copy()","e86bfc84":"from scipy.stats import chi2_contingency\n\nfor i in df_clean.columns.to_list():\n    crossTab = pd.crosstab(index=df_clean['class'], columns=df[i])\n    crossTab\n    \n    print(i)\n\n    stat, p, dof, expected = chi2_contingency([crossTab.iloc[0].values,crossTab.iloc[1].values])# select significance value\n    alpha = 0.05# Determine whether to reject or keep your null hypothesis\n    print('significance=%.3f, p=%.3f' % (alpha, p))\n    if p <= alpha:\n        print('Las variables est\u00e1n asociadas(Se rechaza H0)')\n    else:\n        print('Las variables no est\u00e1n asociadas(No se rechaza H0)')","311ef095":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\ndf_final_label = pd.DataFrame()\n\nfor i in df_clean.columns.to_list():\n    le.fit(df_clean[i])\n    df_final_label[i] = le.transform(df_clean[i])\n\ndf_final_label.head()","cfa73283":"X = df_final_label.drop('class', axis=1)\ny = df_final_label['class']","4598a85f":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)","0c38e3cd":"X_train.head()","4f7e6a75":"from sklearn.tree import DecisionTreeClassifier\narbolDecision = DecisionTreeClassifier()\narbolDecision.fit(X_train, y_train)\ny_pred_train = arbolDecision.predict(X_train)\ny_pred_test = arbolDecision.predict(X_test)\nmetricas(y_train,y_pred_train,y_test,y_pred_test)","f3dc6496":"draw_tree(arbolDecision, X_train)","87db00a3":"from catboost import CatBoostClassifier\ncatBoost = CatBoostClassifier()\ncatBoost.fit(X_train, y_train)\ny_pred_train = catBoost.predict(X_train)\ny_pred_test = catBoost.predict(X_test)\nmetricas(y_train,y_pred_train,y_test,y_pred_test)","70376363":"# 3. Ingenier\u00eda de Datos","90942c50":"Todas nuestras variables son categ\u00f3ricas, esto es favorable para un Random Forest y Catboosting.","088bfeb2":"## 3.2. Conversi\u00f3n de Caracter\u00edsticas a num\u00e9ricas","6822dca6":"## 4.1. Partici\u00f3n Muestral","e3f72d59":"### \u00c1rbol de Decisi\u00f3n","2c9648e6":"# 4. Entrenamiento y Validaci\u00f3n","078fd22f":"Tenemos dos potentes algoritmos, pero \u00bfCu\u00e1l debemos elegir? Si bien los dos nos brindan potentes puntajes, con el CatBoost al ser un algoritmo complejo, perdemos interpretabilidad, pero con el \u00e1rbol de decisi\u00f3n obtenemos interpretabilidad, osea podemos saber como se llego al resultado de predecir un hongo venenoso de un hongo comestible.","00aa3032":"# 1. An\u00e1lisis Exploratorio de Datos","aaac9980":"## 2.1. Datos Nulos","317aa4fc":"## 3.1. Selecci\u00f3n de Caracter\u00edsticas","dd686c8f":"Vemos que la variables solo contiene un valor, por lo que ya que todas las filas est\u00e1n categorizadas de esta forma, no aporta ninguna informaci\u00f3n","b1a4f311":"# Conclusiones","7719fc2d":"## 2.2. Datos irrelevantes","7ffd1ba4":"# 2. Limpieza de Datos","7cd27f95":"### Filter Method -> Chi-Squared","c8144638":"### veil-type","de509926":"# Cargando Datos","3ca25c66":"# Pre-Funciones","2f5fa0f6":"El DataFrame no contiene datos nulos.","817c2849":"## 4.2. Algoritmos de Machine Learning"}}