{"cell_type":{"87f5eeef":"code","4635a840":"code","a3a5e0f5":"code","eb92056d":"code","5e98bec3":"code","f50725a9":"code","04a54058":"code","31775d4e":"code","f89af2c7":"code","e65413c8":"code","7235d459":"code","5e3dd0c5":"code","129a0063":"markdown","f77c74b1":"markdown","77ec862c":"markdown","ea29138e":"markdown","daa8672c":"markdown","6163d7f7":"markdown","a8c3d6ae":"markdown","64b10f4b":"markdown"},"source":{"87f5eeef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4635a840":"from keras.models import Model\nfrom keras.layers import Input, Dense\nfrom keras.datasets import fashion_mnist\nimport matplotlib.pyplot as plt","a3a5e0f5":"(x_train, _), (x_test, _) = fashion_mnist.load_data()","eb92056d":"x_train = x_train.astype(\"float32\") \/ 255.0\nx_test = x_test.astype(\"float32\") \/ 255.0","5e98bec3":"x_train = x_train.reshape(len(x_train), x_train.shape[1:][0]*x_train.shape[1:][1]) # len, 28*28\nx_test = x_test.reshape(len(x_test), x_test.shape[1:][0]*x_test.shape[1:][1])","f50725a9":"input_layer = Input(shape=(784,))\n\n#encode architecture\nencode_layer1 = Dense(64, activation='relu')(input_layer)\nencode_layer2 = Dense(32, activation='relu')(encode_layer1)\nencode_layer3 = Dense(16, activation='relu')(encode_layer2)\n\nlatent_view   = Dense(10, activation='sigmoid')(encode_layer3)\n\n#decode architecture\ndecode_layer1 = Dense(16, activation='relu')(latent_view)\ndecode_layer2 = Dense(32, activation='relu')(decode_layer1)\ndecode_layer3 = Dense(64, activation='relu')(decode_layer2)\n\noutput_layer  = Dense(784, activation = 'sigmoid')(decode_layer3)","04a54058":"autoencoder = Model(input_layer, output_layer)\nautoencoder.summary()","31775d4e":"from keras.callbacks import EarlyStopping\n\nautoencoder.compile(optimizer='adam', loss='mse')\nearly_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\nhist = autoencoder.fit(x_train, x_train, epochs=20, batch_size=256, validation_data=(x_train, x_train), callbacks=[early_stopping])","f89af2c7":"print(hist.history.keys())","e65413c8":"plt.plot(hist.history[\"loss\"], label = \"Train Loss\")\nplt.plot(hist.history[\"val_loss\"], label = \"Val Loss\")\nplt.legend()\nplt.show()","7235d459":"preds = autoencoder.predict(x_train)","5e3dd0c5":"from PIL import Image \nf, ax = plt.subplots(1,10)\nf.set_size_inches(80, 40)\nfor i in range(10):\n    ax[i].imshow(x_train[i].reshape(28, 28))\nplt.show()\nf, ax = plt.subplots(1,10)\nf.set_size_inches(80, 40)\nfor i in range(10):\n    ax[i].imshow(preds[i].reshape(28, 28))\nplt.show()","129a0063":"# Load the dataset","f77c74b1":"# Reshape","77ec862c":"# Create Autoencoder architecture","ea29138e":"# Fitting our model","daa8672c":"# Plotting the original and predicted image","6163d7f7":"# Normalization","a8c3d6ae":"# Plotting Losses","64b10f4b":"# Import Library"}}