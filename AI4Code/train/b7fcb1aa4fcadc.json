{"cell_type":{"69d7594d":"code","3fd7f387":"code","65efdbba":"code","24d3791d":"code","e58eb549":"code","d7cda028":"code","f91241de":"code","cd7641cb":"code","fbbefb55":"code","bd54ddbd":"code","68b8b1ed":"code","00b4f5b1":"code","fd5c810d":"code","27bb207e":"code","454ebc18":"code","0539ea53":"code","8f8b618a":"code","256c0771":"code","9df37474":"code","3b924098":"code","608f63d0":"code","4371674e":"code","50fef4f5":"code","780dccf5":"code","832c166f":"code","395ba142":"code","8bd11375":"code","4ad83952":"code","2f3d5a43":"code","713c0415":"code","54cbd7bb":"code","d2d56b7a":"code","e583d124":"code","ebf579b8":"code","947a7f11":"code","a5e6944d":"code","72e41282":"code","d0093be2":"code","191baa67":"markdown","9a903f42":"markdown","094b17a6":"markdown","8e6ee4ca":"markdown","14c5510f":"markdown","4e915038":"markdown","89f3f18d":"markdown","28b086b4":"markdown","a0042fc2":"markdown","7333c691":"markdown","c7ca53e3":"markdown","4950e0f5":"markdown","97b57291":"markdown","98a44d38":"markdown","28627cd1":"markdown","578f69d1":"markdown","06cd1301":"markdown","3c94b91d":"markdown","e6d45a41":"markdown","15129aac":"markdown","2cc57788":"markdown","714c2794":"markdown","d8c4451d":"markdown","4052014e":"markdown","4704b7a3":"markdown","c724076a":"markdown","cfd29926":"markdown","09c4d5b8":"markdown","0c341f82":"markdown","75ebae84":"markdown","2ff48227":"markdown","3df3a14e":"markdown","76462345":"markdown","5b747e04":"markdown","05716f16":"markdown","9c463716":"markdown","cddb589d":"markdown","a2903e12":"markdown"},"source":{"69d7594d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","3fd7f387":"\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\npatients=pd.read_csv('\/kaggle\/input\/indian-liver-patient-records\/indian_liver_patient.csv')","65efdbba":"patients.head()","24d3791d":"patients.shape","e58eb549":"patients['Gender']=patients['Gender'].apply(lambda x:1 if x=='Male' else 0)","d7cda028":"patients.head()","f91241de":"patients['Gender'].value_counts().plot.bar(color='peachpuff')","cd7641cb":"patients['Dataset'].value_counts().plot.bar(color='blue')","fbbefb55":"patients.isnull().sum()","bd54ddbd":"patients['Albumin_and_Globulin_Ratio'].mean()","68b8b1ed":"patients=patients.fillna(0.94)","00b4f5b1":"patients.isnull().sum()","fd5c810d":"sns.set_style('darkgrid')\nplt.figure(figsize=(25,10))\npatients['Age'].value_counts().plot.bar(color='darkviolet')","27bb207e":"plt.rcParams['figure.figsize']=(10,10)\nsns.pairplot(patients,hue='Gender')","454ebc18":"sns.pairplot(patients)","0539ea53":"f, ax = plt.subplots(figsize=(8, 6))\nsns.scatterplot(x=\"Albumin\", y=\"Albumin_and_Globulin_Ratio\",color='mediumspringgreen',data=patients);\nplt.show()","8f8b618a":"plt.figure(figsize=(8,6))\npatients.groupby('Gender').sum()[\"Total_Protiens\"].plot.bar(color='coral')","256c0771":"plt.figure(figsize=(8,6))\npatients.groupby('Gender').sum()['Albumin'].plot.bar(color='midnightblue')","9df37474":"plt.figure(figsize=(8,6))\npatients.groupby('Gender').sum()['Total_Bilirubin'].plot.bar(color='fuchsia')","3b924098":"corr=patients.corr()","608f63d0":"plt.figure(figsize=(20,10)) \nsns.heatmap(corr,cmap=\"Greens\",annot=True)","4371674e":"from sklearn.model_selection import train_test_split","50fef4f5":"patients.columns","780dccf5":"X=patients[['Age', 'Gender', 'Total_Bilirubin', 'Direct_Bilirubin',\n       'Alkaline_Phosphotase', 'Alamine_Aminotransferase',\n       'Aspartate_Aminotransferase', 'Total_Protiens', 'Albumin',\n       'Albumin_and_Globulin_Ratio']]\ny=patients['Dataset']","832c166f":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=123)","395ba142":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train,y_train)","8bd11375":"from sklearn.model_selection import KFold, cross_val_score\nkfold = KFold(n_splits=5,random_state=42)\nlogmodel = LogisticRegression(C=1, penalty='l1')\nresults = cross_val_score(logmodel, X_train,y_train,cv = kfold)\nprint(results)\nprint(\"Accuracy:\",results.mean()*100)","4ad83952":"from sklearn.model_selection import GridSearchCV, cross_val_score, learning_curve, train_test_split\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_curve, precision_recall_curve, accuracy_score\nrandom_state=42\nimport itertools","2f3d5a43":"log_clf = LogisticRegression(random_state = 42)\nparam_grid = {\n            'penalty' : ['l2','l1'],  \n            'C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n            }\n\nCV_log_clf = GridSearchCV(estimator = log_clf, param_grid = param_grid , scoring = 'accuracy', verbose = 1, n_jobs = -1)\nCV_log_clf.fit(X_train, y_train)\n\nbest_parameters = CV_log_clf.best_params_\nprint('The best parameters for using this model is', best_parameters)","713c0415":"# Confusion matrix \ndef plot_confusion_matrix(cm, classes,\n                          normalize = False,\n                          title = 'Confusion matrix\"',\n                          cmap = plt.cm.Blues) :\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 0)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment = 'center',\n                 color = 'white' if cm[i, j] > thresh else 'black')\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n# Show metrics \ndef show_metrics():\n    tp = cm[1,1]\n    fn = cm[1,0]\n    fp = cm[0,1]\n    tn = cm[0,0]\n    print('Accuracy  =     {:.3f}'.format((tp+tn)\/(tp+tn+fp+fn)))\n    print('Precision =     {:.3f}'.format(tp\/(tp+fp)))\n    print('Recall    =     {:.3f}'.format(tp\/(tp+fn)))\n    print('F1_score  =     {:.3f}'.format(2*(((tp\/(tp+fp))*(tp\/(tp+fn)))\/\n                                                 ((tp\/(tp+fp))+(tp\/(tp+fn))))))","54cbd7bb":"# Precision-recall curve\ndef plot_precision_recall():\n    plt.step(recall, precision, color = 'b', alpha = 0.2,\n             where = 'post')\n    plt.fill_between(recall, precision, step ='post', alpha = 0.2,\n                 color = 'b')\n\n    plt.plot(recall, precision, linewidth=2)\n    plt.xlim([0.0,1])\n    plt.ylim([0.0,1.05])\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision Recall Curve')\n    plt.show();","d2d56b7a":"# ROC curve\ndef plot_roc():\n    plt.plot(fpr, tpr, label = 'ROC curve', linewidth = 2)\n    plt.plot([0,1],[0,1], 'k--', linewidth = 2)\n   # plt.xlim([0.0,0.001])\n   # plt.ylim([0.0,1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.show();","e583d124":"# Learning curve\ndef plot_learning_curve(estimator, title, X, y, ylim = None, cv = None,\n                        n_jobs = 1, train_sizes = np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Plots a learning curve. http:\/\/scikit-learn.org\/stable\/modules\/learning_curve.html\n    \"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel('Training examples')\n    plt.ylabel('Score')\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv = cv, n_jobs = n_jobs, train_sizes = train_sizes)\n    train_scores_mean = np.mean(train_scores, axis = 1)\n    train_scores_std = np.std(train_scores, axis = 1)\n    test_scores_mean = np.mean(test_scores, axis = 1)\n    test_scores_std = np.std(test_scores, axis = 1)\n    plt.grid()\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha = 0.1, color = \"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color = \"r\",\n             label = \"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color = \"g\",\n             label = \"Cross-validation score\")\n    plt.legend(loc = \"best\")\n    return plt","ebf579b8":"# Cross val metric\ndef cross_val_metrics(model) :\n    scores = ['accuracy', 'precision', 'recall']\n    for sc in scores:\n        scores = cross_val_score(model, X, y, cv = 5, scoring = sc)\n        print('[%s] : %0.5f (+\/- %0.5f)'%(sc, scores.mean(), scores.std()))","947a7f11":"from sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import classification_report \nactual = y_test \npredicted = y_pred \nresults = confusion_matrix(actual, predicted) \nprint(\"Confusion Matrix :\")\nprint(results) \nprint('Accuracy Score :',accuracy_score(actual, predicted)) \nprint('Report : ')\nprint(classification_report(actual, predicted))","a5e6944d":"\nfrom sklearn.metrics import roc_curve\ndef plot_roc_cur(fper, tper):  \n    plt.plot(fper, tper, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.savefig('roc.png', dpi=500, bbox_inches='tight')\n    plt.show()","72e41282":"from sklearn.feature_selection import RFE","d0093be2":"log_clf = LogisticRegression(C = best_parameters['C'], \n                                 penalty = best_parameters['penalty'], \n                                 random_state = random_state)\n\nselector = RFE(log_clf)\nselector = selector.fit(X_train, y_train)\n\ny_pred = selector.predict(X_test)\ny_score = selector.predict_proba(X_test)[:,1]\n\n\n# Confusion maxtrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title='Logistic Confusion matrix')\nplt.show()\n\nshow_metrics()\n\n","191baa67":"**Now let us define our X and y.**","9a903f42":"**Thus we can conclude that our model performed at an accuracy of 71.5%.**","094b17a6":"**In this project, we are going to use the Indian Liver Patient Records dataset from kaggle.**","8e6ee4ca":"**We are going to check with the total proteins,albumin etc whether it is asscoiated with disease or not.**","14c5510f":"**Let us fill these null values by imputing the mean of that column.**","4e915038":"**Now, Let us import the cross validation score and Kfold and split them into 5.**","89f3f18d":"**We split the training and testing  in a certain ratio as 70 for training and 30 for testing.**","28b086b4":"**We can see that there are 4 null values in the Albumin and Globulin Ratio column.**","a0042fc2":"**Albumin Level is higher in the case in the case of male compared to female.**","7333c691":"**So there are 583 rows and 11 columns in our dataset.**","c7ca53e3":"**So protein intake is higher in the case of Male and comparitively less in females.**","4950e0f5":"**We are going to predict whether a patient has liver disease or not based on certain features.**","97b57291":"**Let us check the number of male and female using a countplot.**","98a44d38":"**Reading the Dataset:**","28627cd1":"**Let us view the pairplot of patients based on Gender.**","578f69d1":"**We can clearly see that males has more bilirubin content compared to females.**","06cd1301":"# LIVER DISEASE PREDICTION","3c94b91d":"**Now inorder to build our model we use Logistic Regression**","e6d45a41":"**Let us check the correlation between the features using a heatmap:**","15129aac":"**Let us compare the albumin and albumin and globulin ratio by a scatterplot.**","2cc57788":"**Let us check the age group of the patients.**","714c2794":"**So Let us start building our model.**","d8c4451d":"**So we have removed all the null values and we are ready to go !**","4052014e":"**Inorder to build a successful model we have to train and test the model.**","4704b7a3":"**Yes! Now we have filled the null values with the mean of that column.** ","c724076a":"**From the above graph, we can see that Number of males are more than the Number of females.**","cfd29926":"**Finally Let us compare them based on the Bilirubin content.**","09c4d5b8":"**Finally, we are calculating the accuracy of our model.**","0c341f82":"**Let us check the countplot of our Dataset column:**","75ebae84":"**For the purpose of prediction, we need to import more libraries. As we move on, we will import them.**","2ff48227":"**Let us check for the null values:**","3df3a14e":"**Let us compare the Gender based on the Protein Intake.**","76462345":"**Here X is our features and y is our target.**","5b747e04":"**Let us make the Gender column into numerical format:**","05716f16":"**Importing the Necessary Libraries:**","9c463716":"**Another point to be noted here is that higher the Bilirubin content, higher the case is prone to Liver disease.**","cddb589d":"**Here there is a column named Dataset which has two values. Here one of the value symbolises that the patient has \ndisease and the other value symbolises that the patient has no disease.**","a2903e12":"**Let us compare male and female based on Albumin Level.**"}}