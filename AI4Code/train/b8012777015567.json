{"cell_type":{"86aa0861":"code","e65c31f6":"code","1a08d394":"code","4cbe83c0":"code","535fd869":"code","92efc8fd":"code","b0331b89":"code","08cea9ed":"code","f7e6cf93":"code","2a83c1de":"code","66ebf00e":"code","d9659644":"code","7a6ff59a":"code","4f770d15":"code","1b550b45":"markdown","8b5ad4bd":"markdown","2a053f4b":"markdown","457d0498":"markdown","a715b96f":"markdown","063393c5":"markdown","b74d08aa":"markdown","cd5e89cd":"markdown","ef44057c":"markdown","41567f65":"markdown","5e2e061f":"markdown","520670f0":"markdown","206a11b4":"markdown"},"source":{"86aa0861":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndirectory = os.path.join(\"\/kaggle\/input\",\"tabular-playground-series-feb-2021\/\")\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the currentsession\n##print('install optuna')\n##!pip install optuna==2.5.0\n\nimport random\n\n## ================================================================================\n## additional imports\n## ================================================================================\nimport seaborn as sns; sns.set_theme()\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n##import lightgbm as lgb\n\n## use this for integrated hperparameter search\nimport optuna.integration.lightgbm as lgb\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import mutual_info_regression\n","e65c31f6":"print(directory)\ndf = pd.read_csv(os.path.join(directory,'train.csv'));\ndf.head()","1a08d394":"print('per column: ',df.isnull().sum());\nprint('total: ', df.isnull().sum().sum())\nprint('no additional processing needed')\ntarget = 'target';","4cbe83c0":"continuous_features = [i for i in df.columns if 'cont' in i]\ncat_features = [i for i in df.columns if 'cat'in i]\n\ncontinuous = df.filter(items=continuous_features)\n## a lot of useful info are simple functions in pandas, like corr\ncorr_mat= continuous.corr();\n\nmask = np.triu(np.ones_like(corr_mat, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(13,10));\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True);\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_mat, mask=mask, cmap=cmap, vmax=.7, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True);","535fd869":"continuous.hist(figsize=(15, 10), bins = 100, edgecolor='black', linewidth=0.2);\nplt.show();","92efc8fd":"f1 = continuous['cont1'];\nplt.figure(figsize = (10,10))\nplt.plot(f1,'.', markersize = 0.3);\n\n## cluster these values into bins\n#kmeans wants a 2d array\ninit = np.array([0.8,0.7, 0.61, 0.55, 0.48, 0.42,0.36, 0.29,0.27,0.1]).reshape(-1,1)\n## k-means experiment\nkm = KMeans(n_clusters=10, init = init).fit(f1.values.reshape(-1,1));\nprint(km);\nfor i in km.cluster_centers_:\n    plt.axhline(i[0], color = 'green');\nplt.yticks(np.linspace(0,1,11));\nplt.show();\n","b0331b89":"cm = df[[target]+continuous_features].corr();\ncm['target'][continuous_features].plot.bar(edgecolor='black', linewidth=0.2);","08cea9ed":"# Utility functions from Tutorial\ndef make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, n_neighbors = 20,discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    color = np.array([\"C0\"] * scores.shape[0])\n    # Color red for probes\n    idx = [i for i, col in enumerate(scores.index)\n           if col.startswith(\"PROBE\")]\n    color[idx] = \"C3\"\n    # Create plot\n    plt.barh(width, scores, color=color)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n    \n## WE have to downsample the data substantially to do this\nn_samples = 100000;\nsample_inds = random.sample(range(0, len(df)), n_samples)\nmi_scores = make_mi_scores(df[cat_features+continuous_features].iloc[sample_inds], df[target].iloc[sample_inds])\nplt.figure(figsize = (10,6))\nplot_mi_scores(mi_scores)","f7e6cf93":"categoricals = df[cat_features]\n\n## describe is much more useful for getting quick stats on categoricals than continuous\ncategoricals.describe()\n\n\n","2a83c1de":"#print(X.dtypes)\nif('cat4' in cat_features):\n    cat_features.remove('cat4')\nif('cat7' in cat_features):\n    cat_features.remove('cat7')\nX = df[continuous_features+cat_features]\ny = df[target]\nX[cat_features] = X[cat_features].astype(\"category\")\n## convert all cat features to type categorical (from object)\nprint(X.dtypes)","66ebf00e":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)","d9659644":"feature_names = continuous_features+cat_features\nlgtrain = lgb.Dataset(X_train, label=y_train, feature_name=feature_names, categorical_feature=cat_features)\n\nlgval = lgb.Dataset(X_val, label=y_val, feature_name=feature_names, categorical_feature=cat_features)\n# reg = lgb.LGBMRegressor(boosting_type='gbdt', num_leaves=31);\n# reg.fit(train_data);\n# print('train: ',reg.score(X_train, y_train))\n# print('test: ',reg.score(X_val,y_val))\nparams = {\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"num_leaves\": 10,\n    \"learning_rate\": 0.1,\n    \"bagging_fraction\": 0.7,\n    \"feature_fraction\": 0.7,\n    \"bagging_frequency\": 5,\n    \"verbosity\": -1\n}\nbest_params, tuning_history = dict(), list()\nevals_result = {}\nreg = lgb.train(params, lgtrain, 1000, valid_sets=[lgval], early_stopping_rounds=100, verbose_eval=20,\n                  evals_result=evals_result)\n\n## get rmse\nprint('final val rmse: ',(np.mean((reg.predict(X_val)-y_val)**2))**0.5)","7a6ff59a":"# print('best: ',best_params)\n# print();\n# print(tuning_history)\n#print(evals_result)","4f770d15":"test = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/test.csv')\nX_test = test[feature_names]\nX_test[cat_features] = X_test[cat_features].astype('category')\n# print(X_test.dtypes)\npreds = reg.predict(X_test)\nsubmission = pd.DataFrame({'id':test.id,'target':preds})\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()\n","1b550b45":"## Summary","8b5ad4bd":"## Distributions of continuous features\nQuestions to think about\n1. Outliers? Consider removal.\n2. Skewness? Consider transformations like sqrt or log.\n3. Multimodal?\n\nMost of the distributions are multimodal","2a053f4b":"## Load data","457d0498":"## Feature Interactions\nFeature interactions typically occur in most modelling problems. We cannot decompose our model into a sum of individual feature effects since the effect of one feature will depend on the concurrent value of another one. It might be useful to see if any of the features interact with respect to the target","a715b96f":"## Comparison with Mutual Information\nmutual info regression does not scale well with large data. We need to subsample and also reduce the variance in the MI estimate (so increase nearest neighbors arguments)\n\nMutual information has the one advantage in that it encodes relational info that can be nonlinear unlike correlation. Fascinatingly, the mutual info metrics on the features generally appear weak.\n\nAdditionally, we see that four categorical features have the least mutual information: cat4, cat7, cat6, cat0. We will see in the next section, that these labels are heavily imbalanced, which likely explains the issue. We might consider removing these from the model.","063393c5":"## Basic models","b74d08aa":"## Feature to Target Correlations\nnice, feature to target correlations are all very weak","cd5e89cd":"## Analyze Continuous Features\nBecause the features are anonymized, there isn't much qualitative information we can gather, but we can analyze the \"suitability\" of the features are for modeling.\n\n1. Look at distributions of features\n2. Look at collinearity: while collinearity is easily known to be undesirable for linear regression, they can also be undesirable for more complex models, such as trees, where it can confound feature importance analyses","ef44057c":"## Final Processing for modeling\nIf we want to add in categorical features with no processing, we have to convert the type from string to categoricals","41567f65":"## Categorical Features\nHow do we want to encode categorical features? If we actually use a tree-based model, we can leave the categorical features as is since a tree just operates on splits\n\nThings to check:\n1. any categorical features with highly unbalanced labels (i.e. 100% one category means there's no information to help in the model).\n2. Cardinality of categorical features (will inform what kind of encoding)\n\n### Types of categoricals\nDue to anonymization, we can't really figure out nominal or ordinal\n1. Binary: one versus the other\n2. Nominal: multi-group but no ordering of categories\n3. Ordinal: multi-group but ordering of categories\n\nLast time, we fit a model only on the continuous features for an RMSE of 0.86. Let's see what the categorical features add for us.","5e2e061f":"## Check first for missing\/bad entries","520670f0":"## Quickly look at the best hyperparams","206a11b4":"## Feature cont1 and cont8\nIt does not appear to be 100% continuous, what are the gaps? More importantly, does this change how we treat the feature? i.e. we might consider binning this variable and make it categorical.\n\nAgglomerative clustering: Unfavorable memory requirements $O(n^3)$"}}