{"cell_type":{"0c28a872":"code","6ccd4302":"code","c9ea9a86":"code","9850743e":"code","5192064a":"code","ed8614e6":"code","9495d1a1":"code","8fd6ad7f":"code","d4ed17cd":"code","30f233b6":"code","36bb6e66":"code","c6ead746":"code","6551bb83":"code","28cb718b":"code","28cd1d12":"code","815f322d":"code","0df05ed1":"code","3ba84861":"code","fe53c837":"code","dfff8d11":"code","c2574eda":"code","86f5be42":"code","5f4650bb":"code","10e8444a":"code","04be4da6":"code","c4b1bbf3":"code","6e2c0176":"code","ab7f0c39":"code","b9d653b3":"code","cc8ad322":"code","5c4bfa2b":"code","710ea601":"code","45052541":"code","7254acd9":"code","c9f0d9c5":"code","22dbe491":"code","56d25dfd":"code","9514f9fa":"code","af18efee":"code","09351818":"code","67ef1492":"code","8fe662d9":"code","47e570a3":"code","4e7f2183":"code","b0e6cc76":"code","62257ec6":"code","e0b16278":"code","a523cb50":"code","74b6879b":"code","65276234":"code","0027b1ce":"code","09afa481":"code","4c972362":"code","8293e452":"code","40510f84":"code","bb54dff5":"code","08ec20f1":"code","8af03846":"code","4bb7fe5f":"code","52ddd7e1":"code","533bdc95":"code","47e3c406":"code","2502a6e7":"code","f9bc9255":"code","f2febb1b":"code","f53252a6":"code","650e14ab":"code","c3e364eb":"code","a334bf0d":"code","aec5902c":"code","047634fa":"code","616d3961":"code","0a80abe5":"code","24767ea6":"code","c37a89ab":"code","f18f6c16":"code","06233c81":"code","963d818e":"code","4553138a":"code","4777c0d2":"code","3adb0b90":"code","86bd0d9f":"code","c0b37555":"code","fa03e4c1":"code","8a236650":"code","e6fdde20":"code","50980254":"code","94a7f371":"code","70415531":"code","fd747bd4":"code","72f3fccf":"code","be81dd14":"code","9d62bb7d":"code","b1348dbc":"code","af2800c1":"code","e95518ec":"code","a6415f86":"code","1491880d":"code","746a9919":"code","c3ead9b1":"code","4b2487f7":"code","24aa9d5b":"code","a9e40ef7":"code","438cd3d4":"code","d18ec8ab":"code","ea05aae2":"code","4402e675":"code","af466777":"code","1089de8e":"markdown","17e35860":"markdown","09f79afe":"markdown","00274184":"markdown","29166ccb":"markdown","24e9e894":"markdown","418db3c5":"markdown","9288d1c5":"markdown","4a52e007":"markdown","ae3bdade":"markdown","fc42d859":"markdown","54b6008d":"markdown","d9341f00":"markdown","cb4abdd5":"markdown","4b2b203f":"markdown","fde0717e":"markdown","69dbb17c":"markdown","ab9bf8b2":"markdown","9cac1ca8":"markdown","8969b472":"markdown","3db2481f":"markdown","a89fb34c":"markdown","ee20e231":"markdown","7c275bcd":"markdown","a58b9b48":"markdown","994e64e1":"markdown","b46d4279":"markdown","c10e190c":"markdown","ba307fc2":"markdown","e15fbb9c":"markdown","2b163867":"markdown","255dea6c":"markdown","ca6b5834":"markdown","97acae4d":"markdown","c7f5cd72":"markdown","8d1d5694":"markdown","468e9e70":"markdown","784f1162":"markdown","3f0f9e1d":"markdown","93bb0570":"markdown","8a15821c":"markdown","79476f08":"markdown","16f0afda":"markdown","0f24bc8c":"markdown","0a4161b8":"markdown","c2af241e":"markdown","3acf3cee":"markdown","f62f3839":"markdown","9e9bcf9f":"markdown","3bf06381":"markdown","a8a4bafe":"markdown","159180db":"markdown","fb0db536":"markdown","a429519b":"markdown","8aebe3cb":"markdown","72edc8bf":"markdown","cfd536c0":"markdown","08c60ae2":"markdown","db22208b":"markdown","6a47c1b5":"markdown","a978c20b":"markdown","0de772d2":"markdown","3e3d9180":"markdown","bf6093a2":"markdown","e9487f43":"markdown","2fbc3fb4":"markdown","5f79f39c":"markdown","16f42ae9":"markdown","cca5a94a":"markdown","679e2eef":"markdown","ef80d281":"markdown","41291baa":"markdown","10ab8244":"markdown","b6806e51":"markdown","6925bcd7":"markdown","9eaa8eb1":"markdown","4176f05f":"markdown","23032214":"markdown"},"source":{"0c28a872":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","6ccd4302":"df=pd.read_csv('..\/input\/hr-analytics\/HR_comma_sep.csv')\ndf.head()","c9ea9a86":"def explain(attribute):\n    features= {'satisfaction_level': \"It is employee satisfaction point, which ranges from 0-1.\",\n    'last_evaluation': 'It is evaluated performance by the employer, which also ranges from 0-1.',\n    'number_projects': 'How many of projects assigned to an employee?',\n    'average_monthly_hours': 'How many hours in averega an employee worked in a month?',\n    'time_spent_company': 'time_spent_company means employee experience. The number of years spent by an employee in the company.',\n    'work_accident': 'Whether an employee has had a work accident or not.',\n    'promotion_last_5years': 'Whether an employee has had a promotion in the last 5 years or not.',\n    'Department': \"Employee's working department\/division.\",\n    'Salary': \"Salary level of the employee such as low, medium and high.\",\n    'left': \"Whether the employee has left the company or not.\"}\n    return features[attribute]","9850743e":"df.duplicated().value_counts()","5192064a":"# df=df.drop_duplicates()","ed8614e6":"def summary(df, pred=None):\n    obs = df.shape[0]\n    Types = df.dtypes\n    Counts = df.apply(lambda x: x.count())\n    Min = df.min()\n    Max = df.max()\n    Uniques = df.apply(lambda x: x.unique().shape[0])\n    Nulls = df.apply(lambda x: x.isnull().sum())\n    print('Data shape:', df.shape)\n\n    if pred is None:\n        cols = ['Types', 'Counts', 'Uniques', 'Nulls', 'Min', 'Max']\n        str = pd.concat([Types, Counts, Uniques, Nulls, Min, Max], axis = 1, sort=True)\n\n    str.columns = cols\n    print('___________________________\\nData Types:')\n    print(str.Types.value_counts())\n    print('___________________________')\n    return str\n\n# display(summary(df).sort_values(by='Nulls', ascending=False))","9495d1a1":"summary(df)","8fd6ad7f":"df=df.rename({'Department ':'Department'},axis=1)","d4ed17cd":"print('Unique Values of Each Features:\\n')\nfor i in df:\n    print(f'{i}:\\n{sorted(df[i].unique())}\\n')","30f233b6":"def perc_col(df,col):\n    for i in sorted(df[col].unique(),reverse=True):\n        print('%s: %%%.2f' % (i, 100*df[col].value_counts()[i]\/len(df)))\n\nperc_col(df,'left')","36bb6e66":"sns.pairplot(df.drop(['Department','salary'],axis=1),hue='left');","c6ead746":"plt.figure(figsize=(10,7))\nsns.heatmap(df.corr(),annot=True, cmap=\"coolwarm\");","6551bb83":"df.corr()[\"left\"].sort_values().plot.barh();","28cb718b":"# Churn ratio of employee by Department\npd.DataFrame(df.groupby('salary')['left'].mean()).sort_values('left',ascending=False).plot(kind='bar')\npd.DataFrame(df.groupby('salary')['left'].mean()).sort_values('left',ascending=False).round(3).T*100","28cd1d12":"# Churn ratio of employee by Department\npd.DataFrame(df.groupby('Department')['left'].mean()).sort_values('left',ascending=False).plot(kind='bar',figsize=(12,4))\npd.DataFrame(df.groupby('Department')['left'].mean()).sort_values('left',ascending=False).round(3).T*100","815f322d":"plt.figure(figsize=(15,6))\nresult = df.groupby([\"Department\"])['left'].aggregate(np.mean).reset_index().sort_values('left',ascending=False)\nsns.barplot(x='Department', y=\"left\", data=df, hue='salary',order=result['Department'])\nplt.xticks(rotation=90)\nplt.show()","0df05ed1":"g = sns.FacetGrid(df,col='salary',height=4)\nax = g.map(sns.barplot, \"Department\", \"left\", palette = \"Blues_d\", order=result['Department'])\nax.set_xticklabels(rotation = 90);","3ba84861":"plt.figure(figsize=(6,6))\n\nexplode = [0,0.1]\nplt.pie(df['left'].value_counts(),explode=explode,autopct='%1.1f%%',shadow=True,startangle=60)\nplt.legend(labels=['Not Churn','Churn'])\nplt.title('Employee Distribution by Churn')\nplt.axis('off')\nplt.show()","fe53c837":"plt.figure(figsize=(8,5))\nsns.countplot(x='number_project', data=df, hue='left');","dfff8d11":"plt.figure(figsize=(16,5))\nplt.subplot(121)\nsns.countplot(x='time_spend_company', data=df, hue='left')\nplt.subplot(122)\nsns.kdeplot(df['time_spend_company'], hue=df['left'], shade=True)\nplt.show()","c2574eda":"pd.DataFrame(df.groupby('Department').satisfaction_level.mean()).sort_values('satisfaction_level',ascending=False).plot(kind='bar',figsize=(13,4))\npd.DataFrame(df.groupby('Department').satisfaction_level.mean()).sort_values('satisfaction_level',ascending=False).T.round(3)*100","86f5be42":"plt.figure(figsize=(16,8))\nplt.subplot(221)\nsns.kdeplot(df['satisfaction_level'], hue=df['left'], shade=True)\nplt.subplot(222)\nsns.kdeplot(df['satisfaction_level'], hue=df['salary'], shade=True)\nplt.subplot(223)\nsns.kdeplot(df['satisfaction_level'], hue=df['Work_accident'], shade=True)\nplt.subplot(224)\nsns.kdeplot(df['satisfaction_level'], hue=df['number_project'], shade=True)\nplt.show()","5f4650bb":"plt.figure(figsize=(15,4))\nplt.subplot(121)\nsns.kdeplot(df['last_evaluation'], hue=df['left'], shade=True)\n\nplt.subplot(122)\nsns.kdeplot(df['last_evaluation'], hue=df['number_project'], shade=True)\nplt.show()","10e8444a":"plt.figure(figsize=(8,6))\nsns.scatterplot(df['average_montly_hours'], df['satisfaction_level'],hue=df['left']);","04be4da6":"g=sns.FacetGrid(df,col='time_spend_company', hue='left',height=6)\ng.map(plt.scatter, 'average_montly_hours', 'satisfaction_level', alpha=0.7)\ng.add_legend();","c4b1bbf3":"g=sns.FacetGrid(df,col='number_project', hue='left',height=6)\ng.map(plt.scatter, 'average_montly_hours', 'satisfaction_level', alpha=0.7)\ng.add_legend();","6e2c0176":"plt.figure(figsize=(15,6))\n\nplt.subplot(1,3,1)\nsns.boxplot(df['salary'], df['average_montly_hours'])\n\nplt.subplot(1,3,2)\nsns.boxplot(df['salary'], df['satisfaction_level'])\n\nplt.subplot(1,3,3)\nsns.boxplot(df['salary'], df['last_evaluation'])\nplt.show()","ab7f0c39":"plt.figure(figsize=(15,6))\n\nplt.subplot(1,3,1)\nsns.boxplot(df['Department'], df['average_montly_hours'])\nplt.xticks(rotation=90)\n\nplt.subplot(1,3,2)\nsns.boxplot(df['Department'], df['satisfaction_level'])\nplt.xticks(rotation=90)\n\nplt.subplot(1,3,3)\nsns.boxplot(df['Department'], df['last_evaluation'])\nplt.xticks(rotation=90)\nplt.show()","b9d653b3":"df[['Department','salary']].sample(5)","cc8ad322":"from sklearn.preprocessing import LabelEncoder\ndf['Department']=LabelEncoder().fit_transform(df[['Department']])\ndf['salary']=LabelEncoder().fit_transform(df[['salary']])","5c4bfa2b":"# Scaling gerektiren ML modelleri icin df_scaled olusturalim\nfrom sklearn.preprocessing import StandardScaler\ndf_scaled = pd.DataFrame(StandardScaler().fit_transform(df),\n                         columns=df.columns)\ndf_scaled.head()","710ea601":"df_kmeans=df_scaled.drop('left',axis=1)","45052541":"!pip install pyclustertend","7254acd9":"from sklearn.cluster import KMeans\nfrom pyclustertend import hopkins\nfrom sklearn.metrics import silhouette_score\n\nhopkins(df_kmeans,df_kmeans.shape[0])","c9f0d9c5":"ssd = []\nK = range(2,10)\nfor k in K:\n    kmeans = KMeans(n_clusters = k).fit((df_kmeans))\n    ssd.append(kmeans.inertia_)","22dbe491":"plt.figure(figsize=(12,4))\nplt.plot(K, ssd, \"bo-\")\nplt.xlabel(\"Different k values\")\nplt.ylabel(\"inertia-error\")\nplt.title(\"Elbow Method\")\nplt.grid()\nplt.show()","56d25dfd":"from yellowbrick.cluster import KElbowVisualizer\nkmeans = KMeans()\nvisu = KElbowVisualizer(kmeans, k = (2,10))\nvisu.fit(df_kmeans)\nvisu.show();","9514f9fa":"ssd =[]\n\nK = range(2,10)\n\nfor k in K:\n    model = KMeans(n_clusters=k)\n    model.fit(df_kmeans)\n    ssd.append(model.inertia_)\n    print(f'Silhouette Score for {k} clusters: {silhouette_score(df_kmeans, model.labels_)}')","af18efee":"from sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE, ADASYN","09351818":"X=df.drop('left',axis=1)\ny=df.left\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=42) # stratify=y","67ef1492":"y_train.value_counts()","8fe662d9":"# sm = SMOTE(random_state=42)\n# X_smote, y_smote = sm.fit_sample(X_train, y_train)\n# y_smote.value_counts()","47e570a3":"# ad = ADASYN(random_state=42)\n# X_adasyn, y_adasyn = ad.fit_sample(X_train, y_train)\n# y_adasyn.value_counts()","4e7f2183":"# SMOTE\n# X_train, y_train = X_smote, y_smote\n\n# ADASYN\n# X_train, y_train = X_adasyn, y_adasyn","b0e6cc76":"!pip install lazypredict","62257ec6":"# import lazypredict\n# from lazypredict.Supervised import LazyClassifier\n\n# clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n# models, predictions = clf.fit (X_train, X_test, y_train, y_test)\n# models","e0b16278":"from sklearn.ensemble import GradientBoostingClassifier\nfrom yellowbrick.classifier import ClassificationReport\nfrom yellowbrick.datasets import load_occupancy\nfrom sklearn.metrics import accuracy_score,f1_score, recall_score, classification_report,confusion_matrix,precision_score,roc_auc_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit, GridSearchCV, RandomizedSearchCV","a523cb50":"gbc = GradientBoostingClassifier().fit(X_train, y_train)\ny_pred = gbc.predict(X_test)","74b6879b":"print('Confusion Matrix:',*confusion_matrix(y_test,y_pred), sep=\"\\n\")\nprint(classification_report(y_test, y_pred))","65276234":"gbc_accuracy = accuracy_score(y_test, y_pred)\ngbc_f1_score = f1_score(y_test, y_pred, average='weighted')\ngbc_recall = recall_score(y_test, y_pred, average='weighted')\nprint('gbc_accuracy:',gbc_accuracy,\n      '\\ngbc_f1_score:',gbc_f1_score,\n      '\\ngbc_recall:',gbc_recall)","0027b1ce":"gbc = GradientBoostingClassifier()","09afa481":"gbc_params = {\"n_estimators\": [50,100,300],\n             \"subsample\":[0.1,0.5,1],\n             \"max_depth\":[3,7,9],\n             \"learning_rate\":[0.1,0.01,0.3]}","4c972362":"# gbc_grid= GridSearchCV(gbc, gbc_params, cv = 5, \n#                             n_jobs = -1, verbose = 2).fit(X_train, y_train)","8293e452":"# gbc_grid= RandomizedSearchCV(gbc, gbc_params, cv = 5,\n#                              n_iter=10,\n#                             n_jobs = -1, verbose = 2,scoring='f1').fit(X_train, y_train)","40510f84":"# gbc_grid.best_params_","bb54dff5":"# gbc_tuned = GradientBoostingClassifier(learning_rate= 0.1, \n#                         max_depth= 9, \n#                         n_estimators= 300, \n#                         subsample= 0.5).fit(X_train, y_train)\n\n# y_pred = gbc_tuned.predict(X_test)","08ec20f1":"gbc_tuned = GradientBoostingClassifier(learning_rate= 0.5, \n                        max_depth= 9, \n                        n_estimators= 300, \n                        subsample= 1).fit(X_train, y_train)\n\ny_pred = gbc_tuned.predict(X_test)","8af03846":"print('Confusion Matrix:',*confusion_matrix(y_test,y_pred), sep=\"\\n\")\nprint(classification_report(y_test, y_pred))","4bb7fe5f":"gbc_accuracy = accuracy_score(y_test, y_pred)\ngbc_f1_score = f1_score(y_test, y_pred, average='weighted')\ngbc_recall = recall_score(y_test, y_pred, average='weighted')\nprint('gbc_accuracy:',gbc_accuracy,\n      '\\ngbc_f1_score:',gbc_f1_score,\n      '\\ngbc_recall:',gbc_recall)","52ddd7e1":"gbc_f1_true=float(classification_report(y_test, y_pred).split()[12])\ngbc_f1_true","533bdc95":"from sklearn.ensemble import RandomForestClassifier\nrf_model = RandomForestClassifier(n_estimators=100)\nrf_model.fit(X_train, y_train)\n\ny_pred = rf_model.predict(X_test)","47e3c406":"print('Confusion Matrix:',*confusion_matrix(y_test,y_pred), sep=\"\\n\")\nprint(classification_report(y_test, y_pred))","2502a6e7":"rfc_accuracy = accuracy_score(y_test, y_pred)\nrfc_f1_score = f1_score(y_test, y_pred, average='weighted')\nrfc_recall = recall_score(y_test, y_pred, average='weighted')\nprint('rfc_accuracy:',rfc_accuracy,\n      '\\nrfc_f1_score:',rfc_f1_score,\n      '\\nrfc_recall:',rfc_recall)","f9bc9255":"rf_f1_true=float(classification_report(y_test, y_pred).split()[12])\nrf_f1_true","f2febb1b":"# rfc_params = {\"n_estimators\":[100,300,500],\n#               \"max_depth\":[7,10,15],\n#               \"max_features\": [8,10,15],\n#               \"min_samples_split\": [4,6,8]}","f53252a6":"# rfc_grid = GridSearchCV(rf_model, rfc_params, cv = 5, n_jobs = -1, verbose = 2).fit(X_train, y_train)","650e14ab":"# rfc_grid= RandomizedSearchCV(rf_model, rfc_params, cv = 5,\n#                              n_iter=10,\n#                             n_jobs = -1, verbose = 2,scoring='f1').fit(X_train, y_train)","c3e364eb":"# rfc_grid.best_params_","a334bf0d":"# rfc_tuned = RandomForestClassifier(max_depth = 20,             \n#                                   max_features = 7, \n#                                   min_samples_split = 2, \n#                                   n_estimators = 130).fit(X_train, y_train)","aec5902c":"# y_pred = rfc_tuned.predict(X_test)\n# print('Confusion Matrix:',*confusion_matrix(y_test,y_pred), sep=\"\\n\")\n# print(classification_report(y_test, y_pred))","047634fa":"from xgboost import XGBClassifier\nxgb= XGBClassifier()\nxgb.fit(X_train , y_train)\ny_pred = xgb.predict(X_test)","616d3961":"print('Confusion Matrix:',*confusion_matrix(y_test,y_pred), sep=\"\\n\")\nprint(classification_report(y_test, y_pred))","0a80abe5":"xgb_accuracy = accuracy_score(y_test, y_pred)\nxgb_f1_score = f1_score(y_test, y_pred, average='weighted')\nxgb_recall = recall_score(y_test, y_pred, average='weighted')\nprint('xgb_accuracy:',xgb_accuracy,\n      '\\nxgb_f1_score:',xgb_f1_score,\n      '\\nxgb_recall:',xgb_recall)","24767ea6":"xgb = XGBClassifier()","c37a89ab":"xgb_params = {\"n_estimators\": [50,500,1000],\n             \"subsample\":[0.1,0.5,1],\n             \"max_depth\":[3,7,9],\n             \"learning_rate\":[0.1,0.01,0.3]}","f18f6c16":"# xgb_grid= GridSearchCV(xgb, xgb_params, cv = 5, \n#                             n_jobs = -1, verbose = 2).fit(X_train, y_train)","06233c81":"# xgb_grid= RandomizedSearchCV(xgb, xgb_params, cv = 5,\n#                              n_iter=10,\n#                             n_jobs = -1, verbose = 2,scoring='f1').fit(X_train, y_train)","963d818e":"# xgb_grid.best_params_","4553138a":"xgb_tuned = XGBClassifier(learning_rate= 0.1, \n                                max_depth= 9, \n                                n_estimators= 500, \n                                subsample= 1).fit(X_train, y_train)\n\ny_pred = xgb_tuned.predict(X_test)","4777c0d2":"xgb_tuned = XGBClassifier(learning_rate= 0.1, \n                                max_depth= 11, \n                                n_estimators= 600, \n                                subsample= 1).fit(X_train, y_train)\n\ny_pred = xgb_tuned.predict(X_test)","3adb0b90":"print('Confusion Matrix:',*confusion_matrix(y_test,y_pred), sep=\"\\n\")\nprint(classification_report(y_test, y_pred))","86bd0d9f":"xgb_accuracy = accuracy_score(y_test, y_pred)\nxgb_f1_score = f1_score(y_test, y_pred, average='weighted')\nxgb_recall = recall_score(y_test, y_pred, average='weighted')\nprint('xgb_accuracy:',xgb_accuracy,\n      '\\nxgb_f1_score:',xgb_f1_score,\n      '\\nxgb_recall:',xgb_recall)","c0b37555":"xgb_f1_true=float(classification_report(y_test, y_pred).split()[12])\nxgb_f1_true","fa03e4c1":"X=df_scaled.drop('left',axis=1)\ny=df.left\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=42) # stratify=y","8a236650":"from sklearn.neighbors import KNeighborsClassifier","e6fdde20":"knn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train,y_train)\ny_pred = knn.predict(X_test)","50980254":"neighbors = range(1,10)\ntrain_accuracy =np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\nfor i,k in enumerate(neighbors):\n    #Setup a knn classifier with k neighbors\n    knn = KNeighborsClassifier(n_neighbors = k)\n    \n    #Fit the model\n    knn.fit(X_train, y_train)\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train)\n    \n    #Compute accuracy on the test set\n    test_accuracy[i] = knn.score(X_test, y_test)","94a7f371":"plt.figure(figsize=(8,5))\nplt.title('k-NN assesment of number of neighbors')\nplt.plot(neighbors, test_accuracy, label='Accuracy of Test Data')\nplt.plot(neighbors, train_accuracy, label='Accuracy of Training Data')\nplt.legend()\nplt.xlabel('Number of neighbors')\nplt.ylabel('Accuracy')\nplt.show()","70415531":"error_rate = []\n# Her bir error rate icin olusan k degeri bu listeye atilacak\n# Will take some time\nfor i in range(1,10):\n    \n    model = KNeighborsClassifier(n_neighbors=i) # k= i\n    model.fit(X_train,y_train)\n    y_pred_i = model.predict(X_test)\n    error_rate.append(np.mean(y_pred_i != y_test)) ","fd747bd4":"plt.figure(figsize=(10,6))\nplt.plot(range(1,10),\n         error_rate,\n         color='blue', \n         linestyle='dashed', \n         marker='o',\n         markerfacecolor='red', \n         markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate');","72f3fccf":"# knn = KNeighborsClassifier()\n# knn_params = {\"n_neighbors\": range(1,10,2)}\n\n# knn_cv_model = GridSearchCV(knn, knn_params, cv=10).fit(X_train, y_train)","be81dd14":"# knn_cv_model.best_params_","9d62bb7d":"knn_tuned= KNeighborsClassifier(n_neighbors = 1).fit(X_train, y_train)\ny_pred = knn_tuned.predict(X_test)","b1348dbc":"print('Confusion Matrix:',*confusion_matrix(y_test,y_pred), sep=\"\\n\")\nprint(classification_report(y_test, y_pred))","af2800c1":"knn_accuracy = accuracy_score(y_test, y_pred)\nknn_f1_score = f1_score(y_test, y_pred, average='weighted')\nknn_recall = recall_score(y_test, y_pred, average='weighted')\nprint('knn_accuracy:',knn_accuracy,\n      '\\nknn_f1_score:',knn_f1_score,\n      '\\nknn_recall:',knn_recall)","e95518ec":"knn_f1_true=float(classification_report(y_test, y_pred).split()[12])\nknn_f1_true","a6415f86":"compare = pd.DataFrame({\"Model\": [\"Random Forest\", \"XGBoost\",\"Gradient Boosting\",\"K-Nearest Neighbor\"],\n                        \"Accuracy\": [rfc_accuracy, xgb_accuracy, gbc_accuracy,knn_accuracy],\n                        \"F1 Score\": [rfc_f1_score, xgb_f1_score, gbc_f1_score, knn_f1_score],\n                        \"Recall\": [rfc_recall, xgb_recall, gbc_recall,knn_recall],\n                        \"F1 Score (True)\": [rf_f1_true, xgb_f1_true, gbc_f1_true, knn_f1_true]})\n\ndef labels(ax):\n    for p in ax.patches:\n        width = p.get_width()    # get bar length\n        ax.text(width,       # set the text at 1 unit right of the bar\n                p.get_y() + p.get_height() \/ 2, # get Y coordinate + X coordinate \/ 2\n                '{:1.2f}'.format(width), # set variable to display, 2 decimals\n                ha = 'left',   # horizontal alignment\n                va = 'center')  # vertical alignment\n    \nplt.subplot(411)\ncompare = compare.sort_values(by=\"Accuracy\", ascending=False)\nax=sns.barplot(x=\"Accuracy\", y=\"Model\", data=compare, palette=\"Blues_d\")\nlabels(ax)\nplt.show()\n\nplt.subplot(412)\ncompare = compare.sort_values(by=\"Recall\", ascending=False)\nax=sns.barplot(x=\"Recall\", y=\"Model\", data=compare, palette=\"Blues_d\")\nlabels(ax)\nplt.xlabel('Recall (Weighted)')\nplt.show()\n\nplt.subplot(413)\ncompare = compare.sort_values(by=\"F1 Score\", ascending=False)\nax=sns.barplot(x=\"F1 Score\", y=\"Model\", data=compare, palette=\"Blues_d\")\nlabels(ax)\nplt.xlabel('F1 Score (Weighted)')\nplt.show()\n\nplt.subplot(414)\ncompare = compare.sort_values(by=\"F1 Score\", ascending=False)\nax=sns.barplot(x=\"F1 Score (True)\", y=\"Model\", data=compare, palette=\"Blues_d\")\nlabels(ax)\nplt.show()","1491880d":"feature_imp = pd.Series(xgb_tuned.feature_importances_,\n                        index=X.columns).sort_values(ascending=False)\n\nsns.barplot(x=feature_imp, y=feature_imp.index)\nplt.title(\"Feature Importance\")\nplt.show()\n\nfeature_imp[:10]","746a9919":"feature_imp = pd.Series(rf_model.feature_importances_,\n                        index=X.columns).sort_values(ascending=False)\n\nsns.barplot(x=feature_imp, y=feature_imp.index)\nplt.title(\"Feature Importance\")\nplt.show()\n\nfeature_imp[:10]","c3ead9b1":"import pickle\npickle.dump(xgb_tuned,open(\"XGBoost.pkl\",\"wb\"))\npickle.dump(rf_model,open(\"RandomForest.pkl\",\"wb\"))","4b2487f7":"xgb_model = pickle.load(open(\"XGBoost.pkl\",\"rb\"))\nrf_model = pickle.load(open(\"RandomForest.pkl\",\"rb\"))","24aa9d5b":"department_encode={'sales':7, \n                     'technical':9, \n                     'support':8, \n                     'IT':0, \n                     'product_mng':6, \n                     'marketing':5,\n                     'RandD':1, \n                     'accounting':2, \n                     'hr':3, \n                     'management':4}\n\nsalary_encode = {'low':1,\n                 'medium':2,\n                 'high':3}","a9e40ef7":"# my_dict = {'satisfaction_level':0.38,\n#            'last_evaluation':0.53, \n#            'number_project':2,\n#            'average_montly_hours':157,\n#            'time_spend_company':3,\n#            'Work_accident':0,\n# #            'left':1\n#            'promotion_last_5years':0,\n#            'Department':'sales', \n#            'salary':'low',\n#             }","438cd3d4":"my_dict = {'satisfaction_level':0.15,\n           'last_evaluation':0.44, \n           'number_project':3,\n           'average_montly_hours':199,\n           'time_spend_company':2,\n           'Work_accident':0,\n#            'left':0\n           'promotion_last_5years':0,\n           'Department':'management', \n           'salary':'low',\n            }","d18ec8ab":"X = pd.DataFrame.from_dict([my_dict])\nX[['salary','Department']]=pd.concat([X.salary.map(salary_encode),X.Department.map(department_encode)],axis=1)","ea05aae2":"prediction_XGB = xgb_model.predict(X)\nprediction_XGB","4402e675":"print(\"The Churn : \",'Yes' if prediction_XGB[0] else 'No')","af466777":"prob_XGB = xgb_model.predict_proba(X)\nprint(f'The Probability of the Customer Churn is %{round(prob_XGB[0][1]*100,1)}')","1089de8e":"# ``3.Random Forest Classifier``","17e35860":"#### Tunning","09f79afe":"### Data Insights\n\nIn the given dataset, you have two types of employee one who stayed and another who left the company. So, you can divide data into two groups and compare their characteristics. Here, you can find the average of both the groups using groupby() and mean() function.","00274184":"* Churt oranini etkileyen en buyuk motivasyonun maas segmenti oldugu goruluyor. \n* Bu motivasyonun departmana gore etkisi degisebiliyor. HR, Accounting ve RanD taki churn rate dususu daha smoote iken, management icin daha keskin bir dusus soz konusu.","29166ccb":"# #Determines\nIn this project you have HR data of a company. A study is requested from you to predict which employee will churn by using this data.\n\nThe HR dataset has 14,999 samples. In the given dataset, you have two types of employee one who stayed and another who left the company.\n\nYou can describe 10 attributes in detail as:\n- ***satisfaction_level:*** It is employee satisfaction point, which ranges from 0-1.\n- ***last_evaluation:*** It is evaluated performance by the employer, which also ranges from 0-1.\n- ***number_projects:*** How many of projects assigned to an employee?\n- ***average_monthly_hours:*** How many hours in averega an employee worked in a month?\n- **time_spent_company:** time_spent_company means employee experience. The number of years spent by an employee in the company.\n- ***work_accident:*** Whether an employee has had a work accident or not.\n- ***promotion_last_5years:*** Whether an employee has had a promotion in the last 5 years or not.\n- ***Department:*** Employee's working department\/division.\n- ***Salary:*** Salary level of the employee such as low, medium and high.\n- ***left:*** Whether the employee has left the company or not.\n\nFirst of all, to observe the structure of the data, outliers, missing values and features that affect the target variable, you must use exploratory data analysis and data visualization techniques. \n\nThen, you must perform data pre-processing operations such as ***Scaling*** and ***Label Encoding*** to increase the accuracy score of Gradient Descent Based or Distance-Based algorithms. you are asked to perform ***Cluster Analysis*** based on the information you obtain during exploratory data analysis and data visualization processes. \n\nThe purpose of clustering analysis is to cluster data with similar characteristics. You are asked to use the ***K-means*** algorithm to make cluster analysis. However, you must provide the K-means algorithm with information about the number of clusters it will make predictions. Also, the data you apply to the K-means algorithm must be scaled. In order to find the optimal number of clusters, you are asked to use the ***Elbow method***. Briefly, try to predict the set to which individuals are related by using K-means and evaluate the estimation results.\n\nOnce the data is ready to be applied to the model, you must ***split the data into train and test***. Then build a model to predict whether employees will churn or not. Train your models with your train set, test the success of your model with your test set. \n\nTry to make your predictions by using the algorithms ***Gradient Boosting Classifier***, ***K Neighbors Classifier***, ***Random Forest Classifier***. You can use the related modules of the ***scikit-learn*** library. You can use scikit-learn ***Confusion Metrics*** module for accuracy calculation. You can use the ***Yellowbrick*** module for model selection and visualization.\n\nIn the final step, you will deploy your model using Streamlit tool.\n\n","24e9e894":"* Employee nin calistigi proje sayisinda optimum deger 3. Bu sayinin artmasi veya azalmasi churn oranini artiriyor.","418db3c5":"# `1.Gradient Boosting Classifier`","9288d1c5":"> number_project=2 >> 1.cluster\n\n> number_project=4,5 >> 2.cluster\n\n> number_project=6,7 >> 3.cluster da kumelenmis\n\n> ML Modeling yapmadan data nin 3 clustera bolunecegini gormus olduk.","4a52e007":"![image-3.png](attachment:image-3.png)\n> 3 cluster zone da belirleyici olan, \"time_spend_company\" feature i oldugu goruluyor","ae3bdade":"> RF Default hiper parametrelerle daha basarili cikti. Default model ile training yapacagiz.","fc42d859":"* Salary ve Department in churn rate uzerinde en etkili iki feature oldugunu, descriptin analysis yaparken tespit etmistik. \n* Genel anlamda Clustering algoritmalarinda categoric features kullanilmasi tercih edilmez.\n* Cunku modeller, clustering islemini ortalamalar ve distance temelinde yapar.","54b6008d":"- Confusion Matrix : You can use scikit-learn metrics module for accuracy calculation. A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model. This gives us a holistic view of how well our classification model is performing and what kinds of errors it is making.\n\n    [Confusion Matrix](https:\/\/www.analyticsvidhya.com\/blog\/2020\/04\/confusion-matrix-machine-learning\/)","d9341f00":"### Importing Modules","cb4abdd5":"#### **Find Optimum K Value with Elbow Method**","4b2b203f":"#### SMOTE","fde0717e":"#### Evaluating Model Performance","69dbb17c":"#### ADASYN","ab9bf8b2":"> Churn rate te salary nin cok etkili oldugu goruluyor.","9cac1ca8":"Welcome to \"***Employee Churn Analysis Project***\". This is the second project of Capstone Project Series, which you will be able to build your own classification models for a variety of business settings. \n\nAlso you will learn what is Employee Churn?, How it is different from customer churn, Exploratory data analysis and visualization of employee churn dataset using ***matplotlib*** and ***seaborn***, model building and evaluation using python ***scikit-learn*** package. \n\nYou will be able to implement classification techniques in Python. Using Scikit-Learn allowing you to successfully make predictions with the Random Forest, Gradient Boosting Descent, KNN algorithms.\n\nAt the end of the project, you will have the opportunity to deploy your model using *Streamlit*.\n\nBefore diving into the project, please take a look at the determines and project structure.\n\n- NOTE: This tutorial assumes that you already know the basics of coding in Python and are familiar with model deployement as well as the theory behind K-Means, Gradient Boosting Descent, KNN, Random Forest, and Confusion Matrices.\n\n","8969b472":"### Handling Outliers","3db2481f":"#### Tunning","a89fb34c":"#### Tunning","ee20e231":"**last_evaluation**","7c275bcd":"* Lazy predict default degerlerle egitilmis modelin evaluation score larini verir. \n* Gradient Boosting Classifier, XGBoost, Random Forest ve KNN ile model egitecegiz. KNN de scale edilimis datanin kullanilmasi gerekmektedir. ","a58b9b48":"> Maas segmentinin churn uzerinde etkisi goruluyor, daha kucuk ayrintilari yakalamak icin, salary segmentine gore gruplayip gorsellestirelim","994e64e1":"#### The Elbow Method\n\n- \"Elbow Method\" can be used to find the optimum number of clusters in cluster analysis. The elbow method is used to determine the optimal number of clusters in k-means clustering. The elbow method plots the value of the cost function produced by different values of k. If k increases, average distortion will decrease, each cluster will have fewer constituent instances, and the instances will be closer to their respective centroids. However, the improvements in average distortion will decline as k increases. The value of k at which improvement in distortion declines the most is called the elbow, at which we should stop dividing the data into further clusters.\n\n    [The Elbow Method](https:\/\/en.wikipedia.org\/wiki\/Elbow_method_(clustering)\n\n    [The Elbow Method2](https:\/\/medium.com\/@mudgalvivek2911\/machine-learning-clustering-elbow-method-4e8c2b404a5d)\n\n    [KMeans](https:\/\/towardsdatascience.com\/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1)\n\nLet's find out the groups of employees who left. You can observe that the most important factor for any employee to stay or leave is satisfaction and performance in the company. So let's bunch them in the group of people using cluster analysis.","b46d4279":"* KNN de scale edilmis data kullanilir.","c10e190c":"### Loading Dataset\n\nLet's first load the required HR dataset using pandas's \"read_csv\" function.","ba307fc2":"## 1. Exploratory Data Analysis\n\nExploratory Data Analysis is an initial process of analysis, in which you can summarize characteristics of data such as pattern, trends, outliers, and hypothesis testing using descriptive statistics and visualization.","e15fbb9c":"### Employees Left\n\nLet's check how many employees were left?\nHere, you can plot a bar graph using Matplotlib. The bar graph is suitable for showing discrete variable counts.","2b163867":"#### Model Building","255dea6c":"### Feature Importance for Random Forest","ca6b5834":"#### Model Building","97acae4d":"- Yellowbrick: Yellowbrick is a suite of visualization and diagnostic tools that will enable quicker model selection. It\u2019s a Python package that combines scikit-learn and matplotlib. Some of the more popular visualization tools include model selection, feature visualization, classification and regression visualization\n\n    [Yellowbrick](https:\/\/www.analyticsvidhya.com\/blog\/2018\/05\/yellowbrick-a-set-of-visualization-tools-to-accelerate-your-model-selection-process\/)","c7f5cd72":"#### Tunning","8d1d5694":"### Number of Projects\n\nSimilarly, you can also plot a bar graph to count the number of employees deployed on how many projects?","468e9e70":"## 6. Model Deployement\n\nYou cooked the food in the kitchen and moved on to the serving stage. The question is how do you showcase your work to others? Model Deployement helps you showcase your work to the world and make better decisions with it. But, deploying a model can get a little tricky at times. Before deploying the model, many things such as data storage, preprocessing, model building and monitoring need to be studied. Streamlit is a popular open source framework used by data scientists for model distribution.\n\nDeployment of machine learning models, means making your models available to your other business systems. By deploying models, other systems can send data to them and get their predictions, which are in turn populated back into the company systems. Through machine learning model deployment, can begin to take full advantage of the model you built.\n\nData science is concerned with how to build machine learning models, which algorithm is more predictive, how to design features, and what variables to use to make the models more accurate. However, how these models are actually used is often neglected. And yet this is the most important step in the machine learning pipline. Only when a model is fully integrated with the business systems, real values \u200b\u200bcan be extract from its predictions.\n\nAfter doing the following operations in this notebook, jump to new .py file and create your web app with Streamlit.","784f1162":"**average_montly_hours**","3f0f9e1d":"* Unbalanced data set","93bb0570":"#### Scaling\n\nSome machine learning algorithms are sensitive to feature scaling while others are virtually invariant to it. Machine learning algorithms like linear regression, logistic regression, neural network, etc. that use gradient descent as an optimization technique require data to be scaled. Also distance algorithms like KNN, K-means, and SVM are most affected by the range of features. This is because behind the scenes they are using distances between data points to determine their similarity.\n\nScaling Types:\n- Normalization: Normalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling.\n\n- Standardization: Standardization is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation.\n\n    ","8a15821c":"> Hem elbow ile hem Grid Search ile optimum k=1 bulduk. Modeli k=1 ile egitiyoruz.","79476f08":"### Result\n* F1 score is used in the case where we have skewed classes i.e one type of class examples more than the other type class examples.\n* For Churn Analysis, `F1 score of True Class` is the most important parameter.\n* As we can say, XGBoost and Random Forest Classifier with SMOTE algorithm are the best models when we looked at the `F1 score of True Class`","16f0afda":"Here, Dataset is broken into two parts in ratio of 70:30. It means 70% data will used for model training and 30% for model testing.","0f24bc8c":"# ``4.XGBoost Classifer``","0a4161b8":"* Work_accident ile left arasinda hem dusuk hemde negatif bir korelasyon olmasi beklenmeyen bir durum.","c2af241e":"#### Label Encoding\n\nLots of machine learning algorithms require numerical input data, so you need to represent categorical columns in a numerical column. In order to encode this data, you could map each value to a number. e.g. Salary column's value can be represented as low:0, medium:1, and high:2. This process is known as label encoding, and sklearn conveniently will do this for you using LabelEncoder.\n\n","3acf3cee":"* hr: 'Human Resources'\n* RandD: 'Research and Development'","f62f3839":"### Split Data as Train and Test Set","9e9bcf9f":"### Save and Export Variables as .pkl","3bf06381":"#### Model Building","a8a4bafe":"#### Evaluating Model Performance","159180db":"> Minimum Error rate ve maximum accuracy saglayan k=1 dir.","fb0db536":"> Ustteki grafigin maas segmenti kirilimlarina bakalim","a429519b":"### Time Spent in Company\n\nSimilarly, you can also plot a bar graph to count the number of employees have based on how much experience?\n","8aebe3cb":"> satisfaction_level departmanlar arasinda dramatik farkliliklar gostermiyor.","72edc8bf":"#### Evaluating Model Performance","cfd536c0":"# WELCOME!","08c60ae2":"## 3. Data Pre-Processing","db22208b":"* KMeans clustera ayirma islemini dogrusal yaptigi icin bu data setini clustera basarili bolemiyor. yellowbrick k degeri vermedi ve silhoutte score lari dusuk cikti. \n* Bu data setinde dairesel clustering yapabilen DBSCAN kullanilirsa basarili olabilir.","6a47c1b5":"## 5. Model Building","a978c20b":"### Feature Importance for XGBoost","0de772d2":"> 3 farkli cluster zone goruluyor. Bunlarin alt kirilimlarina bakalim.","3e3d9180":"* Oversampling yapmak Evaluation score larini iyilestirmiyor. Aksine score larin 1 puan dusmesine sebep oluyor.","bf6093a2":"### `Compare Models Accuracies & F1 Scores & Recall`","e9487f43":"### Save and Export the Model as .pkl","2fbc3fb4":"## 2. Data Visualization\n\nYou can search for answers to the following questions using data visualization methods. Based on these responses, you can develop comments about the factors that cause churn.\n- How does the promotion status affect employee churn?\n- How does years of experience affect employee churn?\n- How does workload affect employee churn?\n- How does the salary level affect employee churn?","5f79f39c":"### Lazy Predict","16f42ae9":"# ``2.KNeighborsClassifer``","cca5a94a":"> Isverenin puanladigi employees(last_evaluation) icinden ya cok yuksek ya da dusuk puanlama alanlar, churn olmus. \n\n> Isveren fazla proje uzerinde calisanlara beklendigi uzere yuksek puan vermis.","679e2eef":".## 4. Cluster Analysis\n\n- Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning.\n\n    [Cluster Analysis](https:\/\/en.wikipedia.org\/wiki\/Cluster_analysis)\n\n    [Cluster Analysis2](https:\/\/realpython.com\/k-means-clustering-python\/)","ef80d281":"> Churm Employees in satisfaction_level i beklendigi uzere daha dusuk.\n\n> Number of Project e gore satisfaction_level degiskenlik gosteriyor, Bunun alt kirilimlarina bakarak verinin kilcallarinda gezinelim!","41291baa":"## **EXTRA**\n* Target feature i atip, Kmeans 2 farkli cluster olusturuyor mu, deneyelim.","10ab8244":"* Model drop_dublicate yapildigi zaman model basarisi dustugu gozlendi. Bu nedenle, dublicate observation drop edilmeden devam etme tercih edildi.","b6806e51":"# #Tasks\n\n#### 1. Exploratory Data Analysis\n- Importing Modules\n- Loading Dataset\n- Data Insigts\n\n#### 2. Data Visualization\n- Employees Left\n- Determine Number of Projects\n- Determine Time Spent in Company\n- Subplots of Features\n\n#### 3. Data Pre-Processing\n- Scaling\n- Label Encoding\n\n#### 4. Cluster Analysis\n- Find the optimal number of clusters (k) using the elbow method for for K-means.\n- Determine the clusters by using K-Means then Evaluate predicted results.\n\n#### 5. Model Building\n- Split Data as Train and Test set\n- Built Gradient Boosting Classifier, Evaluate Model Performance and Predict Test Data\n- Built K Neighbors Classifier and Evaluate Model Performance and Predict Test Data\n- Built Random Forest Classifier and Evaluate Model Performance and Predict Test Data\n\n#### 6. Model Deployement\n\n- Save and Export the Model as .pkl\n- Save and Export Variables as .pkl ","6925bcd7":"### Subplots of Features\n\nYou can use the methods of the matplotlib.","9eaa8eb1":"**satisfaction_level**","4176f05f":"> There is no outlier in dataset.","23032214":"![image.png](attachment:cdef70b8-3806-4bb0-90ba-fd256210da80.png)"}}