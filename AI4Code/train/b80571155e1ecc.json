{"cell_type":{"c02bdbee":"code","6f457d9d":"code","75b2d3d9":"code","18fdd843":"code","2263f0e3":"code","83db4844":"code","bfaccb1f":"code","3dcb1996":"code","9db232a0":"code","1bd9d09f":"code","bc3a10fd":"code","5f636f92":"code","ea3584e4":"code","31ebfdc4":"code","6e10ba6e":"code","4c3629a3":"code","5fd6326f":"code","9b05fd64":"code","2dbe37d1":"code","43c93c08":"code","2dd82034":"code","2ae937dc":"code","62e38156":"code","acfcc1ae":"code","6547e319":"code","fd0cf0d8":"code","15adc996":"code","c3da4605":"code","06098409":"code","913a9cdd":"code","93bd2ac5":"code","e668daf6":"markdown","e6990176":"markdown","21218ca3":"markdown","e3afce12":"markdown","5af48fd5":"markdown","fb7b8964":"markdown","60aaf25c":"markdown","a3a1a640":"markdown","46fff516":"markdown","61fd61f9":"markdown","5d13dd55":"markdown","6d74d4da":"markdown","bd9c8d46":"markdown","5c231a31":"markdown","e6579972":"markdown","1181685c":"markdown","d64c9a73":"markdown","bf0e3545":"markdown","ff319bcc":"markdown","b376516d":"markdown"},"source":{"c02bdbee":"import numpy as np\nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6f457d9d":"df = pd.read_csv('\/kaggle\/input\/breast-cancer-prediction-dataset\/Breast_cancer_data.csv')","75b2d3d9":"df.head()","18fdd843":"sns.set(rc={'figure.figsize':(10,7)})\nlabels = list(df.diagnosis.unique())\nsizes = list(df.diagnosis.value_counts())\n\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.2f%%', startangle=65)\nplt.show()","2263f0e3":"sns.heatmap(df.corr(), annot=True)","83db4844":"sns.pairplot(df, hue='diagnosis')","bfaccb1f":"fig, axes = plt.subplots(3, 2, figsize=(20,20))\n\nax = sns.boxplot(x=\"mean_area\", y=\"diagnosis\", data=df, orient='h', \n    ax=axes[0, 0])\nax = sns.boxplot(x=\"mean_perimeter\", y=\"diagnosis\", data=df, orient='h', \n    ax=axes[0, 1])\nax = sns.boxplot(x=\"mean_radius\", y=\"diagnosis\", data=df, orient='h', \n    ax=axes[1, 0])\nax = sns.boxplot(x=\"mean_texture\", y=\"diagnosis\", data=df, orient='h', \n    ax=axes[1, 1])\nax = sns.boxplot(x=\"mean_smoothness\", y=\"diagnosis\", data=df, orient='h', \n    ax=axes[2, 0])","3dcb1996":"f, axs = plt.subplots(2, 2, figsize=(12, 10))\nsns.scatterplot(data=df, x='mean_area', y='diagnosis', hue='diagnosis', ax=axs[0][0])\nsns.scatterplot(data=df, x='mean_perimeter', y='diagnosis', hue='diagnosis', ax=axs[0][1])\nsns.scatterplot(data=df, x='mean_radius', y='diagnosis', hue='diagnosis', ax=axs[1][0])\nf.tight_layout()","9db232a0":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\nX = df.drop('diagnosis', axis=1)\ny = df.pop('diagnosis')\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n\nscaler=MinMaxScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)","1bd9d09f":"from sklearn.metrics import accuracy_score, confusion_matrix, average_precision_score, classification_report\n\ndef results(pred, X_test, y_test, model):\n    \n    result = confusion_matrix(y_test, pred)\n\n    sns.heatmap(result, annot=True)\n    plt.title('Confusion Matrix')\n    plt.xlabel('Y predict')\n    plt.ylabel('Y test')\n    \n    score = model.score(X_test, y_test)\n    average_precision = average_precision_score(y_test, pred)\n    report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n    \n    print(f'Model score ..................................: {score}')\n    print(print(f'Average precision-recall score................: {average_precision}'))\n    print('\\n')\n    print(report)\n    print('\\n')","bc3a10fd":"from sklearn.linear_model import LogisticRegression\n\nmodel_log_reg = LogisticRegression(C=6).fit(X_train, y_train)\npred_log_reg = model_log_reg.predict(X_test)","5f636f92":"results(pred_log_reg, X_test, y_test, model_log_reg)","ea3584e4":"from sklearn.neighbors import KNeighborsClassifier\n\nmodel_k_nei = KNeighborsClassifier().fit(X_train, y_train)\npred_k_nei = model_k_nei.predict(X_test)","31ebfdc4":"results(pred_k_nei, X_test, y_test, model_k_nei)","6e10ba6e":"from sklearn.ensemble import RandomForestClassifier\n\nmodel_ran_for = RandomForestClassifier().fit(X_train, y_train)\npred_ran_for = model_ran_for.predict(X_test)","4c3629a3":"results(pred_ran_for, X_test, y_test, model_ran_for)","5fd6326f":"from xgboost import XGBClassifier\n\nmodel_XGB = XGBClassifier(n_estimators=1000, learning_rate=0.05, use_label_encoder=False).fit(X_train, y_train)\npred_XGB = model_XGB.predict(X_test)","9b05fd64":"results(pred_XGB, X_test, y_test, model_XGB)","2dbe37d1":"import lightgbm as lgb\n\nmodel_GBM = lgb.LGBMClassifier().fit(X_train, y_train)\npred_GBM = model_GBM.predict(X_test)","43c93c08":"results(pred_GBM, X_test, y_test, model_GBM)","2dd82034":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\npipeline = Pipeline([\n    ('min_max_scaler', MinMaxScaler()),\n    ('std_scaler', StandardScaler())\n])\n\nX_train1 = pipeline.fit_transform(X_train)\nX_test1 = pipeline.transform(X_test)","2ae937dc":"from sklearn.svm import LinearSVC\n\nmodel_SVC = LinearSVC().fit(X_train1, y_train)\npred_SVC = model_SVC.predict(X_test1)","62e38156":"results(pred_SVC, X_test1, y_test, model_SVC)","acfcc1ae":"from sklearn.svm import SVC\n\nmodel_poly_SVC = SVC(kernel='poly', degree=2, gamma='auto', coef0=1, C=5).fit(X_train1, y_train)\npred_poly_SVC = model_poly_SVC.predict(X_test1)","6547e319":"results(pred_poly_SVC, X_test1, y_test, model_poly_SVC)","fd0cf0d8":"from catboost import CatBoostClassifier\n\nmodel_catboost = CatBoostClassifier(loss_function='Logloss', eval_metric='Accuracy').fit(X_train, y_train, verbose=False)\npred_catboost = model_catboost.predict(X_test)","15adc996":"results(pred_catboost, X_test, y_test, model_catboost)","c3da4605":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks\n\nmodel = keras.Sequential([\n    layers.Dense(30, activation='relu'),\n    layers.Dropout(0.3),\n    layers.Dense(15, activation='relu'),\n    layers.Dropout(0.3),\n    layers.Dense(7, activation='relu'),\n    layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy'],\n)\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=5,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_test, y_test),\n    epochs=600,\n    verbose=0,\n    callbacks=[early_stopping]\n)","06098409":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title=\"Accuracy\")\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n      .format(history_df['val_loss'].min(), \n              history_df['val_binary_accuracy'].max()))","913a9cdd":"predictions=model.predict_classes(X_test)","93bd2ac5":"result = confusion_matrix(y_test, predictions)\n\nsns.heatmap(result, annot=True)\nplt.title('Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\n  \nscore = model.evaluate(X_test, y_test)\naverage_precision = average_precision_score(y_test, predictions)\nreport = pd.DataFrame(classification_report(y_test, predictions, output_dict=True))\n    \nprint(f'Model score ..................................: {score}')\nprint(print(f'Average precision-recall score................: {average_precision}'))\nprint('\\n')\nprint(report)\nprint('\\n')\n","e668daf6":"### Logistic regression","e6990176":"We will make a series of boxplots using the variable 'diagnosis' in the y axis.","21218ca3":"Let's create a train and test set.","e3afce12":"## CatBoost","5af48fd5":"## Predictions","fb7b8964":"We will first use a svm with linear kernel","60aaf25c":"### XGB Regressor ","a3a1a640":"### LightGBM","46fff516":"# Neural Network","61fd61f9":"Now we see the proportion of positives and negatives in the 'diagnosis' viariable ","5d13dd55":"Before using the svm, we have to prepare the data.","6d74d4da":"### From the boxplots and scatterplots, it's possible to notice that when the diagnosis is positive, the nodule has, in general, a smaller mean radius, mean perimeter, and mean area.","bd9c8d46":"###  Polynomial Kernel SVM","5c231a31":"The next step is to take a look at the correlation between the variables ","e6579972":"### Binary classification using K Nearest Neighbours","1181685c":"### SVM","d64c9a73":"### Random forests ","bf0e3545":"### The next step is to create a function that will plot the results for us.","ff319bcc":"### There are strongly correlated variables in this dataset","b376516d":"Not as good as the Logistic Regression algorithm"}}