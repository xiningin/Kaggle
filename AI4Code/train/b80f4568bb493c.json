{"cell_type":{"322d5270":"code","1bd379c6":"code","596cd261":"code","0e5af6b4":"code","7472b0af":"code","45db52e2":"code","f61f031d":"code","afcd1891":"code","fa989c13":"code","bce5dc1e":"code","93f9359b":"code","c8155392":"code","47630023":"code","9b305fed":"code","3dcdd3fd":"code","6b8dbf14":"code","a388a279":"code","b0df16ba":"code","2d2d7458":"code","cddead13":"code","1ec757a9":"code","1d4f8731":"code","b89f2dad":"code","d46e4034":"code","d703002a":"code","bdca0b78":"code","f4469692":"code","2b94daf1":"code","32b1d522":"code","47d7baaa":"code","db2ff466":"code","056aa522":"code","c3e8db5c":"code","85c3986d":"code","e40ee4c1":"code","dfe70424":"markdown","930f6c41":"markdown","219d287e":"markdown","65fec4e3":"markdown","9afecf1d":"markdown","8560872a":"markdown","af654ef6":"markdown","9b5608a3":"markdown","8aff05a9":"markdown","60a61016":"markdown","041189c3":"markdown"},"source":{"322d5270":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1bd379c6":"from PIL import Image","596cd261":"#Code by Ritwek Khosla https:\/\/www.kaggle.com\/vanvalkenberg\/hubble-telescope-images\/comments\n\ndef display_Image(path, save):\n    img1 = Image.open(path)\n    display(img1)\n    if save == True:\n        img1.save('jaffe.jpg')","0e5af6b4":"display_Image('\/kaggle\/input\/jaffe-img\/MK.HA3.118.tiff', 0)","7472b0af":"display_Image('\/kaggle\/input\/jaffe-img\/MK.NE3.115.tiff',True)","45db52e2":"display_Image('\/kaggle\/input\/jaffe-img\/KR.SA1.77.tiff', 0)","f61f031d":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","afcd1891":"from fastai.vision.all import *\nfrom fastai.imports import *\nfrom fastai.vision.data import *\nfrom fastai import *\nimport numpy as np\nimport fastai\nimport matplotlib.pyplot as plt","fa989c13":"path = Path(\"\/kaggle\/input\/jaffe-img\")\npath.ls()","bce5dc1e":"fnames = get_image_files(path\/\"images\")","93f9359b":"dblock = DataBlock()","c8155392":"dblock = DataBlock(get_items = get_image_files)","47630023":"#dls = dblock.dataloaders(path)\n#dls.show_batch()","9b305fed":"import cv2 as cv\nimport matplotlib.pyplot as plt","3dcdd3fd":"IMG_PATH = \"..\/input\/jaffe-img\/TM.DI3.195.tiff\"\n\nimgArray = cv.imread(IMG_PATH)","6b8dbf14":"plt.imshow(imgArray)\n\nplt.show()","a388a279":"convertedArray = cv.cvtColor(imgArray, cv.COLOR_BGR2RGB)\n\nplt.subplots(figsize=(15,10))\nplt.imshow(convertedArray);plt.show()","b0df16ba":"fig, ((ax1,ax2), (ax3,ax4)) =plt.subplots(2,2,figsize=(14,10))\n\nax1.imshow(convertedArray[:,:,0], cmap=\"Reds_r\"); ax1.set_title(\"R\", size=20) \nax2.imshow(convertedArray[:,:,1], cmap=\"Greens_r\"); ax2.set_title(\"G\", size=20)\nax3.imshow(convertedArray[:,:,2], cmap=\"Blues_r\"); ax3.set_title(\"B\", size=20)\n\nax4.axis(\"off\"); plt.tight_layout(); plt.show()","2d2d7458":"import cv2\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom skimage.filters import threshold_otsu\nimport numpy as np\nfrom glob import glob\nimport scipy.misc\nfrom matplotlib.patches import Circle,Ellipse\nfrom matplotlib.patches import Rectangle\nimport os\nfrom PIL import Image\nimport keras\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport gzip\n%matplotlib inline\nfrom keras.layers import Input,Conv2D,MaxPooling2D,UpSampling2D\nfrom keras.models import Model\nfrom tensorflow.keras.optimizers import RMSprop\n#from keras.optimizers import RMSprop\n#from keras.layers.normalization import BatchNormalization\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import (\n    BatchNormalization, SeparableConv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n)\nfrom tensorflow.keras import backend as K","cddead13":"#Code by Nagesh Singh https:\/\/www.kaggle.com\/nageshsingh\/recreating-fingerprints\n\nimport imageio\ndata = glob('..\/input\/jaffe-img\/*')\nimages = []\nfrom matplotlib.pyplot import imread\n\ndef readImages(data):\n    for i in range(len(data)):\n        img = cv2.imread(data[i])\n        img = cv2.resize(img,(224,224))\n        images.append(img)\n        \n    return images\n\nimages = readImages(data)","1ec757a9":"#Code by Nagesh Singh https:\/\/www.kaggle.com\/nageshsingh\/recreating-fingerprints\n\nimages_arr = np.asarray(images)\nimages_arr = images_arr.astype('float32')\nimages_arr.shape","1d4f8731":"print(\"Dataset (images) shape: {shape}\".format(shape=images_arr.shape))","b89f2dad":"#Code by Nagesh Singh https:\/\/www.kaggle.com\/nageshsingh\/recreating-fingerprints\n\n# Display random images in training data\nprint(\"Display random images in training data\")\nfor i in range(20,25):\n    plt.figure(figsize = (12,12))\n    plt.subplot(5, 5, i+1)\n    plt.imshow(cv2.cvtColor(images_arr[i], cv2.COLOR_BGR2RGB))\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\"\"\"\nx = \"..\/input\/jaffe-img\/\"\ndef plotImages(title,directory):\n    print(title)\n    plt.figure(figsize = (12,12))\n    for i in range(25):\n        plt.subplot(5, 5, i+1)\n        img = cv2.imread( directory+ \"\/\" + x[i])\n        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)); plt.axis('off')\n        plt.tight_layout()\n    plt.show()\n        \nplotImages(\"Images of Japanese Female Facial Expression\",\"..\/input\/jaffe-img\/\") \"\"\" ","d46e4034":"images_arr = images_arr.reshape(-1, 224,224, 1)\nimages_arr.shape","d703002a":"images_arr.dtype","bdca0b78":"np.max(images_arr)\nimages_arr = images_arr \/ np.max(images_arr)","f4469692":"np.max(images_arr), np.min(images_arr)","2b94daf1":"from sklearn.model_selection import train_test_split\ntrain_X,valid_X,train_ground,valid_ground = train_test_split(images_arr,images_arr,test_size=0.2,random_state=13)","32b1d522":"#Code by Nagesh Singh https:\/\/www.kaggle.com\/nageshsingh\/recreating-fingerprints\n\nbatch_size = 128\nepochs = 300\nx, y = 224, 224\ninput_img = Input(shape = (x, y, 1))","47d7baaa":"#Code by Nagesh Singh https:\/\/www.kaggle.com\/nageshsingh\/recreating-fingerprints\n\ndef autoencoder(input_img):\n    #encoder\n    #input = 28 x 28 x 1 (wide and thin)\n    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) #28 x 28 x 32\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1) #14 x 14 x 32\n    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1) #14 x 14 x 64\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2) #7 x 7 x 64\n    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2) #7 x 7 x 128 (small and thick)\n\n    #decoder\n    conv4 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3) #7 x 7 x 128\n    up1 = UpSampling2D((2,2))(conv4) # 14 x 14 x 128\n    conv5 = Conv2D(64, (3, 3), activation='relu', padding='same')(up1) # 14 x 14 x 64\n    up2 = UpSampling2D((2,2))(conv5) # 28 x 28 x 64\n    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up2) # 28 x 28 x 1\n    return decoded\n\nautoencoder = Model(input_img, autoencoder(input_img))","db2ff466":"#Code by Nagesh Singh https:\/\/www.kaggle.com\/nageshsingh\/recreating-fingerprints\n\nautoencoder.compile(loss='mean_squared_error', optimizer = RMSprop())","056aa522":"autoencoder.summary()","c3e8db5c":"#Code by Nagesh Singh https:\/\/www.kaggle.com\/nageshsingh\/recreating-fingerprints\n\n#Training\nautoencoder_train = autoencoder.fit(train_X, train_ground, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_ground))","85c3986d":"#Code by Nagesh Singh https:\/\/www.kaggle.com\/nageshsingh\/recreating-fingerprints\n\nloss = autoencoder_train.history['loss']\nval_loss = autoencoder_train.history['val_loss']\nepochs = range(300)\nplt.figure()\nplt.figure(figsize=(15,10))\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","e40ee4c1":"#Code by Nagesh Singh https:\/\/www.kaggle.com\/nageshsingh\/recreating-fingerprints\n\n#Prediction\npred = autoencoder.predict(valid_X)#Reconstruction of Test Images\nplt.figure(figsize=(20, 4))\nprint(\"Test Images\")\nfor i in range(5):\n    plt.subplot(1, 5, i+1)\n    plt.imshow(valid_ground[i, ..., 0], cmap='gray')\nplt.show()    \nplt.figure(figsize=(20, 4))\nprint(\"Reconstruction of Test Images\")\nfor i in range(5):\n    plt.subplot(1, 5, i+1)\n    plt.imshow(pred[i, ..., 0], cmap='gray')  \nplt.show()","dfe70424":"#What kind of images are those above? Fading away?","930f6c41":"#Error: Could not do one pass in your dataloader, there is something wrong in it","219d287e":"#Prediction","65fec4e3":"#Acknowledgment:\n\nNagesh Singh https:\/\/www.kaggle.com\/nageshsingh\/recreating-fingerprints\n\nRitwek Khosla https:\/\/www.kaggle.com\/vanvalkenberg\/hubble-telescope-images\/comments","9afecf1d":"<center style=\"font-family:verdana;\"><h1 style=\"font-size:200%; padding: 10px; background: #DC143C;\"><b style=\"color:white;\">Debunking a Fallacious Account of the JAFFE Dataset<\/b><\/h1><\/center>\n\n\"Excavating AI\" Re-excavated: Debunking a Fallacious Account of the JAFFE Dataset\n\nAuthor: Michael J. Lyons - arXiv:2107.13998 - The Origin of JAFFE:\n\n\"Twenty-five years ago, my colleagues Miyuki Kamachi and Jiro Gyoba and I designed and photographed JAFFE, a set of facial expression images intended for use in a study of face perception.\"\n\n\"In 2019, without seeking permission or informing us, Kate Crawford and Trevor Paglen exhibited JAFFE in two widely publicized art shows. In addition, they published a nonfactual account of the images in the essay \"Excavating AI: The Politics of Images in Machine Learning Training Sets.\"\n\n\"The present article recounts the creation of the JAFFE dataset and unravels each of Crawford and Paglen's fallacious statements. I also discuss JAFFE more broadly in connection with research on facial expression, affective computing, and human-computer interaction.\"\n\nhttps:\/\/arxiv.org\/abs\/2107.13998","8560872a":"#Above Errors: cannot import name 'RMSprop' and BatchNormalization from 'keras.optimizers' (\/opt\/conda\/lib\/python3.7\/site-packages\/keras\/optimizers.py)","af654ef6":"#Plot the loss plot between training and validation data to visualize the model performance.","9b5608a3":"#OK, 300 Epochs I'll suffer. That's too much time. If it goes to the limbo I'll change to 3 Epochs! ","8aff05a9":"#Since I couldn't make it with show_batch (data or dls_showbatch), let's go to plan B. ","60a61016":"<center style=\"font-family:verdana;\"><h1 style=\"font-size:200%; padding: 10px; background: #DC143C;\"><b style=\"color:white;\">Coding Facial Expressions with Gabor Wavelets<\/b><\/h1><\/center>\n\nCoding Facial Expressions with Gabor Wavelets (IVC Special Issue)\n\nAuthors: Michael J. Lyons, Miyuki G. Kamachi, Jiro Gyoba.\n\nJAFFE (Japanese Female Facial Expression) images and how they were photographed:\n\n\"The authors presented a method for extracting information about facial expressions from digital images. The method codes facial expression images using a multi-orientation, multi-resolution set of Gabor filters that are topographically ordered and approximately aligned with the face.\"\n\n\"A similarity space derived from this code is compared with one derived from semantic ratings of the images by human observers. Interestingly the low-dimensional structure of the image-derived similarity space shares organizational features with the circumplex model of affect, suggesting a bridge between categorical and dimensional representations of facial expression.\"\n\n\"Their results also indicate that it would be possible to construct a facial expression classifier based on a topographically-linked multi-orientation, multi-resolution Gabor coding of the facial images at the input stage. The significant degree of psychological plausibility exhibited by the proposed code may also be useful in the design of human-computer interfaces.\"\n\nhttps:\/\/www.researchgate.net\/publication\/344244548_Coding_Facial_Expressions_with_Gabor_Wavelets_IVC_Special_Issue","041189c3":"<center style=\"font-family:verdana;\"><h1 style=\"font-size:200%; padding: 10px; background: #DC143C;\"><b style=\"color:white;\">Japanese Female Facial Expression (JAFFE)<\/b><\/h1><\/center>\n\nThe Japanese Female Facial Expression (JAFFE) Dataset\n\n\nAuthors: Lyons, Michael; Kamachi, Miyuki;  Gyoba, Jiro\n\nSpecifications:\n\n10 Japanese female expressers\n\n7 Posed Facial Expressions (6 basic facial expressions + 1 neutral)\n\nSeveral images of each expression for each expresser \n\n213 images total\n\nEach image has averaged semantic ratings on 6 facial expressions by 60 Japanese viewers\n\nResolution 256x256 pixels\n\n8-bit grayscale;; Tiff format, no compression\n\nThe image dataset was planned and assembled by Michael Lyons, Miyuki Kamachi, and Jiro Gyoba, at Kyushu University, Japan.\n\n\"The JAFFE images may be used for non-commercial scientific research under certain terms of use, which must be accepted to access the data.\"\n\n\"The following article describes the JAFFE images and how they were photographed:\nMichael J. Lyons, Miyuki Kamachi, Jiro Gyoba.\"\n\nhttps:\/\/zenodo.org\/record\/3451524#.YcSqWmDMK3A"}}