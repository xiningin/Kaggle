{"cell_type":{"b4aa1f5f":"code","5ee84397":"code","3a52d0b7":"code","67575cb3":"code","5abcb568":"code","048cdac0":"code","d46fabed":"code","3ef5253d":"code","e113c4b5":"code","9909b5c6":"code","f815ffc0":"code","9314c47c":"code","e48205d0":"code","cfca7d70":"code","6c563657":"code","56809a7d":"code","544d157c":"code","d45e130f":"code","4f621737":"code","eac1d8f9":"code","2bbf8fa7":"code","07b59d22":"code","2ec70207":"code","89c85af5":"code","575d65bd":"code","edec6de2":"code","b296214d":"code","8f34453d":"code","05f950dc":"code","7c20cf43":"code","ead78c62":"code","b7f9bdf7":"code","f46d023e":"code","52710834":"code","7ec86809":"code","11c5f201":"markdown","6b01dc4a":"markdown","8b8aa030":"markdown","922cf828":"markdown","074736a4":"markdown","5fbfc22e":"markdown","af8fdc16":"markdown","8ab0cb50":"markdown","bbd8d99a":"markdown","8b43943f":"markdown","1389af6c":"markdown","5f705f39":"markdown","9367b313":"markdown","2d7bf952":"markdown","3b3903c6":"markdown","1992ef8c":"markdown","4e2d7ad0":"markdown","61c1a51c":"markdown","104a0171":"markdown","9edc1108":"markdown","06b9c2e6":"markdown","4bf9f3f6":"markdown","880d990c":"markdown","70b851fd":"markdown","4123d82c":"markdown","c00acc49":"markdown","719f21b8":"markdown","2033caf4":"markdown","30a8e2df":"markdown","8e65f544":"markdown"},"source":{"b4aa1f5f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport gc; gc.enable()\n\npd.set_option('display.max_columns', 100)","5ee84397":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')\nfinal_test = pd.read_csv('..\/input\/test.csv')\n\nprint(len(df_train))\nprint(df_train.shape)\nprint(df_train.columns)\ndf_train.head()","3a52d0b7":"print(len(df_test))\nprint(df_test.shape)\nprint(df_test.columns)\ndf_test.head()","67575cb3":"print('Missing Values in Train Data')\nprint(df_train.isnull().sum())\nprint('')\nprint('Missing Values in Test Data')\nprint(df_test.isnull().sum())","5abcb568":"# Filling the missing values in Age with the medians of Sex and Pclass groups\ndf_train['Age'] = df_train.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\ndf_test['Age'] = df_test.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))","048cdac0":"# Filling the missing values in Embarked with S\ndf_train['Embarked'] = df_train['Embarked'].fillna('S')\ndf_test['Embarked'] = df_test['Embarked'].fillna('S')","d46fabed":"test_med_fare = df_test.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\n\n# Filling the missing value in Fare with the median Fare of a 3rd class alone passenger\ndf_test['Fare'] = df_test['Fare'].fillna(test_med_fare)","3ef5253d":"# Creating Deck column from the first letter of the Cabin column (M stands for Missing)\ndf_train['Deck'] = df_train['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\ndf_test['Deck'] = df_test['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')","e113c4b5":"print('Deck Column for Train Set')\nprint(df_train['Deck'].value_counts())\nprint('')\nprint('Deck Column for Test Set')\nprint(df_test['Deck'].value_counts())","9909b5c6":"# Passenger in the T deck is changed to A\ntrain_idx = df_train[df_train['Deck'] == 'T'].index\ndf_train.loc[train_idx, 'Deck'] = 'A'","f815ffc0":"df_train['Deck'] = df_train['Deck'].replace(['A', 'B', 'C'], 'AB')\ndf_train['Deck'] = df_train['Deck'].replace(['D', 'E'], 'DE')\ndf_train['Deck'] = df_train['Deck'].replace(['F', 'G'], 'FG')\n\ndf_test['Deck'] = df_test['Deck'].replace(['A', 'B', 'C'], 'AB')\ndf_test['Deck'] = df_test['Deck'].replace(['D', 'E'], 'DE')\ndf_test['Deck'] = df_test['Deck'].replace(['F', 'G'], 'FG')","9314c47c":"df_train.drop(columns = ['Cabin'], inplace=True, axis=1)\ndf_test.drop(columns = ['Cabin'], inplace=True, axis=1)","e48205d0":"print('Missing values in Train data')\nprint(df_train.isnull().sum())\nprint('')\nprint('Missing values in Test data')\ndf_train.isnull().sum()","cfca7d70":"df_train['Sex'] = df_train.Sex.map({'female': 0, 'male': 1})\ndf_test['Sex'] = df_test.Sex.map({'female': 0, 'male': 1})","6c563657":"fare_labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\nage_labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\ndf_train['Fare'] = pd.qcut(df_train['Fare'], 13, labels=fare_labels)\ndf_test['Fare'] = pd.qcut(df_test['Fare'], 13, labels=fare_labels)\n\ndf_train['Age'] = pd.qcut(df_train['Age'], 10, labels=age_labels)\ndf_test['Age'] = pd.qcut(df_test['Age'], 10, labels=age_labels)","56809a7d":"df_train['Family_Size'] = df_train['SibSp'] + df_train['Parch'] + 1\ndf_test['Family_Size'] = df_test['SibSp'] + df_test['Parch'] + 1","544d157c":"df_train['Group_Size'] = df_train.groupby('Ticket')['Ticket'].transform('count')\ndf_test['Group_Size'] = df_test.groupby('Ticket')['Ticket'].transform('count')\n\ndf_train.drop(columns = ['Ticket'], inplace=True)\ndf_test.drop(columns = ['Ticket'], inplace=True)","d45e130f":"df_train['Title'] = df_train['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]","4f621737":"# Apply the .split() method to isolate the words in each 'Name' entry: first on the comma, then on the period\ndf_train['Title'] = df_train['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\ndf_train['Married'] = 0\ndf_train['Married'].loc[df_train['Title'] == 'Mrs'] = 1\n\ndf_test['Title'] = df_test['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\ndf_test['Married'] = 0\ndf_test['Married'].loc[df_test['Title'] == 'Mrs'] = 1","eac1d8f9":"df_train['Title'] = df_train['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss\/Mrs\/Ms')\ndf_train['Title'] = df_train['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr\/Military\/Noble\/Clergy')\n\ndf_test['Title'] = df_test['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss\/Mrs\/Ms')\ndf_test['Title'] = df_test['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr\/Military\/Noble\/Clergy')","2bbf8fa7":"print(df_train['Title'].value_counts())\ndf_test['Title'].value_counts()","07b59d22":"df_train.drop(columns = ['Name'], inplace=True)\ndf_test.drop(columns = ['Name'], inplace=True)","2ec70207":"df_test['Parch'] = df_test['Parch'].replace(9, 6)\n\ndf_train['Group_Size'] = df_train['Group_Size'].replace([6, 7], 5)","89c85af5":"df_train = pd.get_dummies(data=df_train, columns=['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Deck', 'Family_Size', 'Group_Size', 'Title'])\ndf_test = pd.get_dummies(data=df_test, columns=['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Deck', 'Family_Size', 'Group_Size', 'Title'])","575d65bd":"df_train.head()","edec6de2":"y = df_train['Survived']\n\ndf_train = df_train.drop(columns = ['Survived', 'PassengerId'], inplace=False)\ndf_test = df_test.drop(columns = ['PassengerId'], inplace=False)","b296214d":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom mlxtend.classifier import StackingCVClassifier","8f34453d":"X_train, X_test, y_train, y_test = train_test_split(df_train, y, test_size = 0.30, random_state=42)","05f950dc":"import pandas as pd\nimport numpy as np\nnp.random.seed(0)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\nimport xgboost as xgb\n\n%matplotlib inline","7c20cf43":"import xgboost as xgb\nimport lightgbm as lgb\n\nclf1 = KNeighborsClassifier()\nclf2 = LogisticRegressionCV(cv=7)\nclf3 = RandomForestClassifier(max_depth=3, n_estimators=150, random_state=42)\n\nxgb = xgb.XGBClassifier(silent=False,\n                        scale_pos_weight=1,\n                        learning_rate=0.02,  \n                        colsample_bytree = 0.4,\n                        subsample = 0.8,\n                        objective='binary:logistic', \n                        n_estimators=1000, \n                        reg_alpha = 0.3,\n                        max_depth=3, \n                        gamma=1)\n\n# [Starting from v0.16.0, StackingCVRegressor supports\n# `random_state` to get deterministic result.]\nsclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3],\n                            use_probas=False,\n                            meta_classifier=xgb,\n                            random_state=42)\n\nprint('7-fold cross validation:\\n')\n\nfor clf, label in zip([clf1, clf2, clf3, sclf], \n                      ['KNN', \n                       'LogisticRegression',\n                       'Random Forest',\n                       'StackingClassifier']):\n\n    scores = model_selection.cross_val_score(clf, X_train, y_train, \n                                              cv=7, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" \n          % (scores.mean(), scores.std(), label))","ead78c62":"print('7-fold cross validation:\\n')\n\nfor clf, label in zip([clf1, clf2, clf3, sclf], \n                      ['KNN',\n                       'LogisticRegression', \n                       'Random Forest',\n                       'StackingClassifier']):\n\n    scores = model_selection.cross_val_score(clf, X_test, y_test, \n                                              cv=7, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" \n          % (scores.mean(), scores.std(), label))","b7f9bdf7":"sclf.fit(X_train, y_train)\ntraining_preds = sclf.predict(X_train)\nval_preds = sclf.predict(X_test)\ntraining_accuracy = accuracy_score(y_train, training_preds)\nval_accuracy = accuracy_score(y_test, val_preds)\n\nprint(\"Training Accuracy: {:.4}%\".format(training_accuracy * 100))\nprint(\"Validation accuracy: {:.4}%\".format(val_accuracy * 100))","f46d023e":"# Predicting on the titanic test set with my stacked model:\ny_pred = sclf.predict_proba(df_test)","52710834":"# Grabbing the predictive probabilities that I'll need for my submission and converting them to integers:\ny_pred = y_pred[ : , 1]\n\ny_pred = pd.Series(y_pred)\ny_pred = (y_pred.round()).astype(int)","7ec86809":"submission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\nsubmission_df['PassengerId'] = final_test['PassengerId']\nsubmission_df['Survived'] = y_pred\n\nsubmission_df.to_csv('submissions.csv', header=True, index=False)","11c5f201":"Another suggestion from the tutorial that makes sense is the creation of a 'Ticket Frequency' column (what I've decided to call 'Group Size') from the 'Ticket' column. This feature will help capture the approximate size of the group each respective passenger was traveling with. With a measure of helpful information extracted from the 'Ticket' column; it can now be dropped.","6b01dc4a":"Stating the obvious, only the train data includes the target variable, 'Survived'.","8b8aa030":"Using the StackingCV Classifier, I use K-Nearest-Neighbors (KNN) (since we're dealing with a small dataset), Logistic Regression, Random Forest, and ExtraTrees for 'lower-level' models and XGB Classifier for my 'meta-level' model.","922cf828":"Regarding the two passengers with missing data in the 'Embarked' column of the train data set, others have researched the matter and concluded that these passengers left from Southhampton; hence, their values should be 'S'.","074736a4":"Tackling the 'Age' column first, I borrowed some code from a far more experienced Kaggler [https:\/\/www.kaggle.com\/gunesevitan\/advanced-feature-engineering-tutorial-with-titanic] and addressed the matter in both the train and test sets. The strategy of filling in these missing 'Age' values entails taking the median value of other passengers based on their 'Sex' and 'Pclass'.","5fbfc22e":"Importing the necessary libraries for modeling...","af8fdc16":"There is a single value in the 'Fare' column of the test data that needs to be addressed. Once again, this idea and the code are indebted to the experienced Kaggler mentioned above.","8ab0cb50":"### Feature Engineering","bbd8d99a":"A final feature engineering suggestion from the advanced tutorial was to squeeze out useful information embedded in the 'Name' column using the .split() method. First, the presence of the title 'Mrs.' (or lack thereof) can be used to create a 'Married' column. Though imperfect, especially given the lack of an analogous title for men that indicates their marital status, at least some information is captured with this effort. Second, the isolated remaining titles can be grouped together in regards to estimations of social position.","8b43943f":"Readying the data for modeling, I separate the target variable from the predictors and go ahead and drop 'PassengerId' from both the train and test sets.","1389af6c":"First, of course, import the necessary packages. I also like to preempitively change my display so that columns are never truncated (such an issue doesn't really apply to a dataset this small; but I think it's a good habit).","5f705f39":"With the 'Deck' column complete and created from the now obsolete 'Cabin' column, the latter can be removed from the test and train data sets.","9367b313":"Due to the proximity of the T deck to the A deck, the single passenger in the T deck can be changed to A.","2d7bf952":"### Data Cleaning","3b3903c6":"Next, I read in the train and test data sets, and briefly examine the train.","1992ef8c":"Wanting to play around with a small dataset that focused on classification, I decided to tackle the Titanic dataset. This notebook focuses on data cleaning--particularly addressing missing values, feature engineering (to a lesser degree), and model stacking.","4e2d7ad0":"The train and test sets are now ready to 'dummify'; dummy variables will help create a lot of machine readable columns. Some of the columns that I turn into dummy variables are ordinal rather than categorical; that is, they don't have to be turned into dummy variables, but given that I plan on using a few tree-based models, I've decided to dummy-up as many of my features as possible.","61c1a51c":"The general measure of proximity also justifies the combining of certain decks in order to simplify the data.","104a0171":"I split the train set data into train and test to get a sense of how my models are performing.","9edc1108":"All missing data should have been dealt with; it's worth double-checking to make sure.","06b9c2e6":"Thanks so much for perusing my notebook! I hope it was helpful.","4bf9f3f6":"Preemptively addressing a mismatch in columns between the train and test set, I used .value_counts() to discover discrepancies in both the 'Parch' values and the 'Group_Size' values. I replace the unique categories with the highest alternative in each column.","880d990c":"Checking each data set for NaNs or missing values, I find the following:","70b851fd":"The tutorial also suggests creating a 'Family Size' column by adding 1 (for the current passenger) to the 'SibSp' & 'Parch' columns. This makes good intuitive sense.","4123d82c":"The 'Advanced Feature Engineering Tutorial with Titanic' suggests that a decent amount of information gain is attained through the binning of both the 'Fare' column and the 'Age' column. The number of bins for 'Fare' and 'Age' were most likely arrived at through trial and error.","c00acc49":"### Model Stacking","719f21b8":"With the 'Age', 'Embarked' & 'Fare' missing values dealt with, the missing 'Cabin' values need to be addressed. Following the lead of the aforementioned Kaggler, 'Cabin' is simplified and translated into a 'Deck' column.","2033caf4":"I do the same with the test data.","30a8e2df":"## Data Cleaning, Feature Engineering & Model Stacking","8e65f544":"Aiming to transform the data set into a machine readable format, it's worth changing the 'Sex' column to 0s & 1s. "}}