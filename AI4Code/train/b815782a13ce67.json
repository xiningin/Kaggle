{"cell_type":{"9cd1e3b8":"code","04e12aee":"code","b9ff4534":"code","862a6e27":"code","87707a9e":"code","b1bb54b0":"code","db49b9a4":"code","2df7d871":"code","55356b30":"code","39ddbe0b":"code","08f4c48b":"code","a7ab299c":"code","cd2854bb":"code","d89105a0":"code","95ba463d":"code","56396c39":"code","95ceda13":"code","d8c58670":"code","fa71886c":"code","ef5b6151":"code","94d02940":"code","83645070":"code","25dcce5f":"code","48fb75e5":"code","c62dc201":"code","e6325d81":"code","a5cdc651":"code","43b40ac5":"code","81989f57":"code","30908aa8":"code","5fad8bc2":"code","2e20500f":"code","24adc4ec":"code","5478ef0b":"code","dcc7f5af":"code","4bbf7152":"code","fa440409":"code","80498a60":"code","70285fde":"code","4310c7a7":"code","4bc66912":"code","6c644581":"code","973c9dc8":"code","041324f9":"code","3863fc81":"code","8cfcf1c5":"code","3ecd2388":"code","b032a239":"code","4dfbe7cc":"code","63b22a68":"code","4e2ba659":"code","b87371d7":"code","7389622c":"code","15ab6b7f":"code","a5c8fe73":"code","4589a683":"code","68902129":"code","7b55df84":"markdown","10b6884d":"markdown","f5230190":"markdown","b85ef59c":"markdown","57782cc3":"markdown","e666750a":"markdown","846dae33":"markdown","8674c0e0":"markdown","a391d94c":"markdown","545225e8":"markdown","354716bd":"markdown","a133ed44":"markdown","355d7e44":"markdown","c4e45eee":"markdown","7cf04046":"markdown","eff0d2f7":"markdown","0a111f2b":"markdown","08c75128":"markdown","2c3068f9":"markdown","b72e8733":"markdown","f4ab4c7c":"markdown","0a89b959":"markdown","e56416bd":"markdown","89c49a0c":"markdown","a88d5e24":"markdown","9aa20671":"markdown","5d6030ce":"markdown","dccc7a0c":"markdown","d50222c3":"markdown","c8f64799":"markdown","4866bc47":"markdown","45f2b503":"markdown","f6cd8cbc":"markdown"},"source":{"9cd1e3b8":"import keras\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport seaborn as sb\nimport xgboost \nimport math\nimport time\nimport collections\nimport ipywidgets as widgets\nimport dill as pickle\nimport pickle as pk\n\n\nfrom pylab import rcParams\nfrom sklearn import ensemble\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\nfrom sklearn import svm\nfrom tqdm import tqdm\n\n\nfrom keras.layers import Dense, Dropout, LSTM\nfrom keras.models import Sequential,load_model\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import LinearRegression\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import confusion_matrix, recall_score, precision_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVR\n#from sklearn.metrics import mean_absolute_error\n#from sklearn.metrics import r2_score\n#from sklearn.metrics import mean_squared_error\nfrom scipy.stats import kurtosis, skew, norm, boxcox\nfrom ipywidgets import interact\nfrom IPython.display import Image\n\n\n# define path to save model\nmodel_path = 'binary_model.h5'\n\n# Setting seed for reproducibility\nnp.random.seed(6523423)  \nPYTHONHASHSEED = 0\n\n%autosave 60","04e12aee":"index_columns_names =  [\"id\",\"runtime\"]\nop_settings_columns = [\"setting\"+str(i) for i in range(1,4)]\ntag_columns =[\"tag\"+str(i) for i in range(1,22)]\ncolumn_names = index_columns_names + op_settings_columns + tag_columns\nprint(column_names)","b9ff4534":"train_df = pd.read_csv('..\/input\/dataset-pred-maint\/PM_train.txt', sep=\" \", header=None)\ntrain_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\ntrain_df.columns = [\"id\", \"runtime\", \"setting1\", \"setting2\",\"setting3\", \"tag1\", \"tag2\", \"tag3\", \n                    \"tag4\", \"tag5\", \"tag6\", \"tag7\", \"tag8\", \"tag9\", \"tag10\", \"tag11\", \"tag12\",\n                    \"tag13\", \"tag14\", \"tag15\", \"tag16\", \"tag17\", \"tag18\", \"tag19\", \"tag20\", \"tag21\"]\n\ntrain_df = train_df.sort_values(['id','runtime'])\ntrain_df","862a6e27":"train_df.describe()","87707a9e":"print(train_df.isna().sum())","b1bb54b0":"test_df = pd.read_csv('..\/input\/dataset-pred-maint\/PM_test.txt', sep=\" \", header=None)\ntest_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\ntest_df.columns = [\"id\", \"runtime\", \"setting1\", \"setting2\",\"setting3\", \"tag1\", \"tag2\", \"tag3\", \n                    \"tag4\", \"tag5\", \"tag6\", \"tag7\", \"tag8\", \"tag9\", \"tag10\", \"tag11\", \"tag12\",\n                    \"tag13\", \"tag14\", \"tag15\", \"tag16\", \"tag17\", \"tag18\", \"tag19\", \"tag20\", \"tag21\"]\n\ntest_df = test_df.sort_values(['id','runtime'])\ntest_df","db49b9a4":"test_df.describe()","2df7d871":"print(test_df.isna().sum())","55356b30":"train_df.info()","39ddbe0b":"# RUL - Remaining Useful Life\ndef remaining_useful_life(data, factor = 0):\n    df = data.copy()\n    fd_RUL = df.groupby('id')['runtime'].max().reset_index()\n    fd_RUL = pd.DataFrame(fd_RUL)\n    fd_RUL.columns = ['id','max']\n    df = df.merge(fd_RUL, on=['id'], how='left')\n    df['RUL'] = df['max'] - df['runtime']\n    df.drop(columns=['max'],inplace = True)\n    \n    return df[df['runtime'] > factor]\n\ndf = remaining_useful_life(train_df)\ndf_test = remaining_useful_life(test_df)","08f4c48b":"def plot_hist(variable):\n    print(\"min {} : {} \".format(variable, min(df[variable])))\n    print(\"max {} : {}\".format(variable, max(df[variable])))\n    \n    plt.figure(figsize=(9,3))\n    plt.hist(df[variable], color=\"skyblue\", ec=\"skyblue\", edgecolor='blue')\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with hist \".format(variable))\n    plt.show()","a7ab299c":"numericVar = [\"id\", \"runtime\", \"setting1\", \"setting2\",\"setting3\", \"tag1\", \"tag2\", \"tag3\", \n                    \"tag4\", \"tag5\", \"tag6\", \"tag7\", \"tag8\", \"tag9\", \"tag10\", \"tag11\", \"tag12\",\n                    \"tag13\", \"tag14\", \"tag15\", \"tag16\", \"tag17\", \"tag18\", \"tag19\", \"tag20\", \"tag21\"]\nfor n in numericVar:\n    plot_hist(n)","cd2854bb":"#delete columns with constant values that do not carry information about the state of the unit\ndf.drop(columns=['setting3', 'tag1', 'tag5', 'tag10', 'tag16', 'tag18', 'tag19'],inplace=True)\ndf_test.drop(columns=['setting3', 'tag1', 'tag5', 'tag10', 'tag16', 'tag18', 'tag19'],inplace=True)","d89105a0":"op_settings_columns = [\"setting1\",\"setting2\"]\ntag_columns = [\"tag2\", \"tag3\", \"tag4\", \"tag6\", \"tag7\", \"tag8\", \"tag9\", \"tag11\", \"tag12\",\n                    \"tag13\", \"tag14\", \"tag15\", \"tag17\", \"tag20\", \"tag21\"]\nprint(index_columns_names + op_settings_columns + tag_columns)","95ba463d":"def bar_plot(variable):\n    \n    # get feature\n    var = df[variable]\n    #count number of categorical variable (value\/sample)\n    varValue = var.value_counts()\n\n    #visualize\n    plt.figure(figsize=(9,3))\n    plt.bar(varValue.index, varValue,color = \"lightgreen\", edgecolor = \"black\", linewidth = 2)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}: \\n {}\".format(variable,varValue))","56396c39":"corr_matrix = df.corr()\nsb.heatmap(corr_matrix,annot=True,cmap='YlGnBu',linewidths=0.2)\nfig=plt.gcf()\nfig.set_size_inches(20,20)\nplt.show()","95ceda13":"df.head()","d8c58670":"# use seaborn to visualize featuresto target (RUL)\nexplore = sb.PairGrid(data=df.query('id < 15') ,\n                 x_vars=\"RUL\",\n                 y_vars=tag_columns + op_settings_columns,\n                 hue=\"id\", size=3, aspect=2.5)\nexplore = explore.map(plt.scatter, alpha=0.5)\nexplore = explore.set(xlim=(400,0))\nexplore = explore.add_legend()","fa71886c":"# operational setting 3 is stable, let's visualize op setting 1 and 2 against some of the most active sensors\ng = sb.pairplot(data=df.query('id < 15'),\n                 x_vars=[\"setting1\",\"setting2\"],\n                 y_vars=[\"tag2\", \"tag3\", \"tag4\", \"tag6\", \"tag7\", \"tag8\", \"tag9\", \"tag11\", \"tag12\", \"tag13\", \"tag14\", \"tag15\", \"tag17\", \"tag20\", \"tag21\"],\n                 hue=\"id\", aspect=1)","ef5b6151":"df.drop(columns=['setting1', 'setting2', 'tag6', 'tag14','tag9'],inplace=True)\ndf_test.drop(columns=['setting1', 'setting2', 'tag6', 'tag14', 'tag9'],inplace=True)","94d02940":"df2 = train_df.copy()\n\ndef plot_sensor(id, sensor_number):\n    plt.subplots(figsize=(14, 6))\n    \n    plt.plot(df2.loc[id, 'runtime'],\n            df2.loc[id, f'tag{sensor_number}'], 'ro-', label=f'Sensor = {sensor_number}, AssetID = {id}')\n    plt.legend(loc='best', fontsize=14)\n    plt.grid()\n    \n\nslider1 = widgets.IntSlider(min=min(train_df['id']), max=max(train_df['id']), step=1, value=1, description=r'$Number\\;of\\;machine$',\n                                                  style={'description_width': 'initial'}, \n                                                  layout=dict(width='30%'))\n\nslider2 = widgets.IntSlider(min=1, max=21, step=1, value=1, description=r'$Number\\;of\\;sensor$',\n                                                  style={'description_width': 'initial'}, \n                                                  layout=dict(width='30%'))\n\n\n\nui = widgets.HBox([slider1, slider2])\n\nout = widgets.interactive_output(plot_sensor, {'id': slider1, 'sensor_number': slider2})\n\ndisplay(ui, out)","83645070":"list(df.columns)","25dcce5f":"import collections\n\ndef detect_outliers(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3st quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier Step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indeces\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        # store indeces \n        outlier_indices.extend(outlier_list_col)\n        \n    outlier_indices = collections.Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1) \n    \n    return multiple_outliers","48fb75e5":"df.loc[detect_outliers(df,['id','runtime', 'tag2', 'tag3', 'tag4', 'tag7', 'tag8', \n                           'tag11', 'tag12', 'tag13', 'tag15', 'tag17', 'tag20', 'tag21', 'RUL'])]","c62dc201":"df_train = df.drop(detect_outliers(df,['id','runtime', 'tag2', 'tag3', 'tag4', 'tag7', 'tag8', \n                           'tag11', 'tag12', 'tag13', 'tag15', 'tag17', 'tag20', 'tag21', 'RUL']),axis = 0).reset_index(drop=True)","e6325d81":"def plot_hist(variable):\n    print(\"min {} : {} \".format(variable, min(df_train[variable])))\n    print(\"max {} : {}\".format(variable, max(df_train[variable])))\n    \n    plt.figure(figsize=(9,3))\n    plt.hist(df_train[variable], color=\"skyblue\", ec=\"skyblue\", edgecolor='blue')\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with hist \".format(variable))\n    plt.show()\n\nnumericVar = ['id','runtime', 'tag2', 'tag3', 'tag4', 'tag7', 'tag8', \n                           'tag11', 'tag12', 'tag13', 'tag15', 'tag17', 'tag20', 'tag21', 'RUL']\nfor n in numericVar:\n    plot_hist(n)","a5cdc651":"skewed_feats = df_train.apply(lambda x: skew(x.dropna())).sort_values(ascending = False)\nskewness = pd.DataFrame(skewed_feats, columns = [\"skewed\"])\nskewness","43b40ac5":"# RUL\nsb.distplot(df_train[\"RUL\"], fit = norm)\nplt.show()\n(mu, sigma) = norm.fit(df_train[\"RUL\"])\nprint(\"mu {} : {}, sigma {} : {}\".format(\"RUL\", mu, \"RUL\", sigma))\nprint()","81989f57":"# runtime\nsb.distplot(df_train[\"runtime\"], fit = norm)\nplt.show()\n(mu, sigma) = norm.fit(df_train[\"runtime\"])\nprint(\"mu {} : {}, sigma {} : {}\".format(\"runtime\", mu, \"runtime\", sigma))\nprint()","30908aa8":"# tag2\nsb.distplot(df_train[\"tag2\"], fit = norm)\nplt.show()\n(mu, sigma) = norm.fit(df_train[\"tag2\"])\nprint(\"mu {} : {}, sigma {} : {}\".format(\"tag2\", mu, \"tag2\", sigma))\nprint()","5fad8bc2":"# tag9\n#sb.distplot(df_train[\"tag9\"], fit = norm)\n#plt.show()\n#(mu, sigma) = norm.fit(df_train[\"tag9\"])\n#print(\"mu {} : {}, sigma {} : {}\".format(\"tag9\", mu, \"tag9\", sigma))\n#print()","2e20500f":"#df_train[\"tag9\"], lam = boxcox(df_train[\"tag9\"])","24adc4ec":"#df_train['tag9'].describe()","5478ef0b":"df_train.head()","dcc7f5af":"df_test.head()","4bbf7152":"# now it's time to clear out target leakage\nprint(df_train.shape)\nleakage_to_drop = ['id', 'runtime']  \ntrain_no_leakage = df_train.drop(leakage_to_drop, axis = 1)\nprint(train_no_leakage.shape)\n# set up features and target variable \ny_train = train_no_leakage['RUL']\nX_train = train_no_leakage.drop(['RUL'], axis = 1)","fa440409":"# now it's time to clear out target leakage\nprint(df_test.shape)\nleakage_to_drop = ['id', 'runtime']  \ntest_no_leakage = df_test.drop(leakage_to_drop, axis = 1)\nprint(test_no_leakage.shape)\n# set up features and target variable \ny_test = test_no_leakage['RUL']\nX_test = test_no_leakage.drop(['RUL'], axis = 1)","80498a60":"X_train.head()","70285fde":"y_train.head()","4310c7a7":"X_test.head()\nX_test.info()","4bc66912":"y_test.head()","6c644581":"#id_train = pd.DataFrame(df_train[\"id\"])\n#id_test = pd.DataFrame(df_test[\"id\"])\n\n#train = df_train\n#test = df_test\n\n#train.drop(columns=['id'], inplace=True)\n#test.drop(columns=['id'], inplace=True)","973c9dc8":"# I like to use a simple random forest to determine some of the most important\/meaningful features. Can be used as feature selection\n# create an exhuastive random forest (200 trees up to 15 levels deep)\nrf = ensemble.RandomForestRegressor()\nsingle_rf = ensemble.RandomForestRegressor(n_estimators = 200, max_depth = 15)\nsingle_rf.fit(X_train, y_train)\ny_pred = single_rf.predict(X_test)\nprint(\"complete\")","041324f9":"# graph feature importance\nimportances = single_rf.feature_importances_\nindices = np.argsort(importances)[::-1]\nfeature_names = X_train.columns    \nf, ax = plt.subplots(figsize=(11, 9))\nplt.title(\"Feature ranking\", fontsize = 20)\nplt.bar(range(X_train.shape[1]), importances[indices], color=\"b\", align=\"center\")\nplt.xticks(range(X_train.shape[1]), indices) #feature_names, rotation='vertical')\nplt.xlim([-1, X_train.shape[1]])\nplt.ylabel(\"importance\", fontsize = 18)\nplt.xlabel(\"index of the feature\", fontsize = 18)\nplt.show()\n\n# list feature importance\nimportant_features = pd.Series(data=single_rf.feature_importances_,index=np.array(X_train.columns))\nimportant_features.sort_values(ascending=False,inplace=True)\nprint(important_features.head(12))","3863fc81":"# random forest regression\n# create holdout\n\n# choose the model\nrf = ensemble.RandomForestRegressor()\n\n# set up 5-fold cross-validation\ncv = model_selection.KFold(5)\n\n# pipeline standardization and model\npipeline = Pipeline(steps=[('standardize', preprocessing.StandardScaler())\n                           , ('model', rf) ])\n# tune the model\nmy_min_samples_leaf = [2, 10, 25, 50, 100]\nmy_max_depth = [7, 8, 9, 10, 11, 12]\n\n# run the model using gridsearch, select the model with best search\noptimized_rf = GridSearchCV(estimator=pipeline\n                            , cv=cv\n                            , param_grid =dict(model__min_samples_leaf = my_min_samples_leaf, model__max_depth = my_max_depth)\n                            , scoring = 'neg_mean_squared_error'\n                            , verbose = 1\n                            , n_jobs = -1\n                           )\noptimized_rf.fit(X_train, y_train)\n\n# show the best model estimators\nprint(optimized_rf.best_estimator_)\n\n# evaluate metrics on holdout\ny_rf_pred = optimized_rf.predict(X_test)\nprint(\"Random Forest Mean Squared Error: \", mean_squared_error(y_test, y_rf_pred))\nprint(\"Random Forest Mean Absolute Error: \", mean_absolute_error(y_test, y_rf_pred))\nprint(\"Random Forest r-squared: \", r2_score(y_test, y_rf_pred))","8cfcf1c5":"# Elastic Net GLM\n# create holdout\n\n# choose the model\n\nglm_net = ElasticNet()\n\n# set up 5-fold cross-validation\ncv = model_selection.KFold(5)\n\n# pipeline standardization and model\npipeline = Pipeline(steps=[('standardize', preprocessing.StandardScaler())\n                           , ('model', glm_net) ])\n# tune the model\nmy_alpha = np.linspace(.01, 1, num=5)\nmy_l1_ratio = np.linspace(.01, 1, num=3)\n\n# run the model using gridsearch, select the model with best search\noptimized_glm_net = GridSearchCV(estimator=pipeline\n                            , cv=cv\n                            , param_grid =dict(model__l1_ratio = my_l1_ratio, model__alpha = my_alpha)\n                            , scoring = 'neg_mean_squared_error'\n                            , verbose = 1\n                            , n_jobs = -1\n                           )\noptimized_glm_net.fit(X_train, y_train)\n\n# show the best model estimators\nprint(optimized_glm_net.best_estimator_)\n\n# evaluate metrics on holdout\ny_netglm_pred = optimized_glm_net.predict(X_test)\nprint(\"GLM Elastic Net Mean Squared Error: \", mean_squared_error(y_test, y_netglm_pred))\nprint(\"GLM Elastic Net Mean Absolute Error: \", mean_absolute_error(y_test, y_netglm_pred))\nprint(\"GLM Elastic Net r-squared: \", r2_score(y_test, y_netglm_pred))","3ecd2388":"# Gradient Boosting\n# create holdout\n\n# choose the model\ngb = ensemble.GradientBoostingRegressor()\n\n# set up 5-fold cross-validation\ncv = model_selection.KFold(5)\n\n# pipeline standardization and model\npipeline = Pipeline(steps=[('standardize', preprocessing.StandardScaler())\n                           , ('model', gb) ])\n# tune the model\nmy_alpha = [.5, .75, .9]\nmy_n_estimators= [500]\nmy_learning_rate = [0.005, .01]\nmy_max_depth = [4, 5, 6]\n\n# run the model using gridsearch, select the model with best search\noptimized_gb = GridSearchCV(estimator=pipeline\n                            , cv=cv\n                            , param_grid =dict(model__max_depth = my_max_depth, model__n_estimators = my_n_estimators,\n                                              model__learning_rate = my_learning_rate, model__alpha = my_alpha)\n                            , scoring = 'neg_mean_squared_error'\n                            , verbose = 1\n                            , n_jobs = -1\n                           )\noptimized_gb.fit(X_train, y_train)\n\n# show the best model estimators\nprint(optimized_gb.best_estimator_)\n\n# evaluate metrics on holdout\ny_gb_pred = optimized_gb.predict(X_test)\nprint(\"Gradient Boosting Mean Squared Error: \", mean_squared_error(y_test, y_gb_pred))\nprint(\"Gradient Boosting Mean Absolute Error: \", mean_absolute_error(y_test, y_gb_pred))\nprint(\"Gradient Boosting r-squared: \", r2_score(y_test, y_gb_pred))","b032a239":"# Support Vector Machines (SVM)\n# create holdout\n\n# choose the model\nsvm = svm.SVR()\n\n# set up 5-fold cross-validation\ncv = model_selection.KFold(5)\n\n# pipeline standardization and model\npipeline = Pipeline(steps=[('standardize', preprocessing.StandardScaler())\n                           , ('model', svm) ])\n# tune the model\nmy_C = [1]\nmy_epsilon = [.05, .1, .15]\n\n# run the model using gridsearch, select the model with best search\noptimized_svm = GridSearchCV(estimator=pipeline\n                            , cv=cv\n                            , param_grid =dict(model__C = my_C, model__epsilon = my_epsilon)\n                            , scoring = 'neg_mean_squared_error'\n                            , verbose = 1\n                            , n_jobs = -1\n                           )\noptimized_svm.fit(X_train, y_train)\n\n# show the best model estimators\nprint(optimized_svm.best_estimator_)\n\n# evaluate metrics on holdout\ny_svm_pred = optimized_svm.predict(X_test)\nprint(\"SVM Mean Squared Error: \", mean_squared_error(y_test, y_svm_pred))\nprint(\"SVM Mean Absolute Error: \", mean_absolute_error(y_test, y_svm_pred))\nprint(\"SVM r-squared: \", r2_score(y_test, y_svm_pred))","4dfbe7cc":"# plot actual vs predicted Remaining Useful Life for the best model (SVM)\nfig, ax = plt.subplots()\nax.scatter(y_test, y_svm_pred, edgecolors=(0, 0, 0))\nax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\nax.set_xlabel('Actual RUL')\nax.set_ylabel('Predicted RUL')\nax.set_title('Remaining Useful Life Actual vs. Predicted')\nplt.show()","63b22a68":"# now let's look at turning this into a classification sol -> can we confidently identify when an asset within its last 20 cycles?\n# generate label columns for training data\ncycles = 20\ntrain_no_leakage['Target_20_Cycles'] = np.where(train_no_leakage['RUL'] <= cycles, 1, 0 )\ntrain_no_leakage.tail(5)","4e2ba659":"target_to_drop = ['RUL']\ntrain_final = train_no_leakage.drop(target_to_drop, axis = 1)\ny_train_final = train_final['Target_20_Cycles']\nX_train_final = train_final.drop(['Target_20_Cycles'], axis = 1)\ny_train_final.tail()","b87371d7":"test_no_leakage['Target_20_Cycles'] = np.where(test_no_leakage['RUL'] <= cycles, 1, 0 )\ntest_no_leakage.tail(5)","7389622c":"target_to_drop = ['RUL']\ntest_final = test_no_leakage.drop(target_to_drop, axis = 1)\ny_test_final = test_final['Target_20_Cycles']\nX_test_final = test_final.drop(['Target_20_Cycles'], axis = 1)\ny_test_final.tail()","15ab6b7f":"sb.scatterplot(x=\"tag11\", y=\"tag4\", hue=\"Target_20_Cycles\", data=train_no_leakage)\nplt.title('Scatter plot tag11 and tag4 - train dataset')","a5c8fe73":"sb.scatterplot(x=\"tag11\", y=\"tag4\", hue=\"Target_20_Cycles\", data=test_no_leakage)\nplt.title('Scatter plot tag11 and tag4 - test dataset')","4589a683":"# random forest regression\n# create holdout\n\n# choose the model\nrf = ensemble.RandomForestClassifier()\n\n# set up 5-fold cross-validation\ncv = model_selection.KFold(5)\n\n# pipeline standardization and model\npipeline = Pipeline(steps=[('standardize', preprocessing.StandardScaler())\n                           , ('model', rf) ])\n# tune the model\nmy_min_samples_leaf = [2, 25, 50]\nmy_max_depth = [8, 9, 10, 12]\n\n# run the model using gridsearch, select the model with best search\n\noptimized_rf = GridSearchCV(estimator=pipeline\n                            , cv=cv\n                            , param_grid =dict(model__min_samples_leaf = my_min_samples_leaf, model__max_depth = my_max_depth)\n                            , scoring = 'roc_auc'\n                            , verbose = 1\n                            , n_jobs = -1\n                           )\noptimized_rf.fit(X_train_final, y_train_final)\n\n# show the best model estimators\ny_pred_proba = optimized_rf.predict_proba(X_test_final)[:, 1]\ny_rf_pred = optimized_rf.predict(X_test_final)\nprint(optimized_rf.best_estimator_)","68902129":"print(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test_final, y_rf_pred))\nprint(\"Random Forest Accuracy: \"+\"{:.1%}\".format(accuracy_score(y_test_final, y_rf_pred)));\nprint(\"Random Forest Precision: \"+\"{:.1%}\".format(precision_score(y_test_final, y_rf_pred)));\nprint(\"Random Forest Recall: \"+\"{:.1%}\".format(recall_score(y_test_final, y_rf_pred)));\nprint(\"Classification Report:\")\nprint(classification_report(y_test_final, y_rf_pred))\n\nfpr, tpr, threshold = metrics.roc_curve(y_test_final, y_rf_pred)\nroc_auc = metrics.auc(fpr, tpr)\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","7b55df84":"<a id = \"7\"><\/a>\n<h2 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\"> \ud83d\udd75\ufe0f\u200d Outlier Detection <\/h2>","10b6884d":"<a id = \"6\"><\/a>\n<h2 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\"> \ud83d\udd0d Exploratory Data Analysis (EDA) <\/h2>","f5230190":"<ul>\n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> yes, we have come to the end. As you can see, our biggest success with random forest is 53% <\/strong> <\/p> <\/li>\n<\/ul>","b85ef59c":"<a id = \"4\"><\/a>\n<h3 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\"> \ud83d\udd22 Numerical Variable <\/h3>","57782cc3":"<ul>\n    <h3> <p style = \"color:black;font-weight:bold\"> Train set<\/p>  <\/h3>\n<\/ul>","e666750a":"Creating a KPI: RUL - Remaining Useful Life.","846dae33":"<a id = \"11\"><\/a>\n<h3 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\"> 1\ufe0f\u20e3 Random Forest Model<\/h3>","8674c0e0":"<ul>\n    <li style = \"color:red\"> <p style = \"color:black;font-weight:bold\"> after looking at the previous plots and the correlation matrix we've also decided to delete a few more variables, this time all the variables that returned correlation with RUL < |0.40|. Which is: 'setting1', 'setting2', 'tag6', 'tag9', 'tag14'.<\/p>  <\/li>\n<\/ul>","a391d94c":"<ol>    \n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> id : <\/strong> Asset code. The code represents a complete run of the asset until its failure. After its failure it is replace by another asset with id + 1 code.<\/p> <\/li>  \n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> runtime : <\/strong> A measure of time that resets after failure <\/p> <\/li>\n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> setting1 : <\/strong> Set point 1 <\/p> <\/li>  \n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> setting2 : <\/strong> Set point 2 <\/p> <\/li>\n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> setting3 : <\/strong> Set point 3 <\/p> <\/li>  \n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> tag1  : <\/strong> Sensor 1. <\/p> <\/li>\n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> tag2 : <\/strong> Sensor 2. <\/p> <\/li>  \n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> tag3 : <\/strong> Sensor 3.<\/p> <\/li>\n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> tag4 : <\/strong> Sensor 4.<\/p> <\/li>\n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> tag5 : <\/strong> Sensor 5.<\/p> <\/li>\n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> tag6 : <\/strong> Sensor 6.<\/p> <\/li>\n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> tag7 : <\/strong> Sensor 7<\/p> <\/li>\n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> tag8 : <\/strong> Sensor 8.<\/p> <\/li>\n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> tag9 : <\/strong> Sensor 9.<\/p> <\/li>\n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> tag10 : <\/strong> Sensor 10.<\/p> <\/li>\n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> tag11 : <\/strong> Sensor 11.<\/p> <\/li>\n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> tag12 : <\/strong> Sensor 12.<\/p> <\/li>\n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> tag13 : <\/strong> Sensor 13.<\/p> <\/li>\n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> tag14 : <\/strong> Sensor 14.<\/p> <\/li>\n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> tag15 : <\/strong> Sensor 15.<\/p> <\/li>\n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> tag16 : <\/strong> Sensor 16.<\/p> <\/li>\n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> tag17 : <\/strong> Sensor 17.<\/p> <\/li>\n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> tag18 : <\/strong> Sensor 18.<\/p> <\/li>\n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> tag19 : <\/strong> Sensor 19.<\/p> <\/li>\n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> tag20 : <\/strong> Sensor 20.<\/p> <\/li>\n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> tag21 : <\/strong> Sensor 21.<\/p> <\/li>\n<\/ol>","545225e8":"<a id = \"12\"><\/a>\n<h3 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\"> 2\ufe0f\u20e3 Elastic NET GLM Model<\/h3>","354716bd":"<ul>\n    our <li style = \"color:red\"> <p style = \"color:black;font-weight:bold\"> there is'nt categorical variables, but we will create 1 categorical variable which will represent the (classification target) indicating where the machine still has n=20 more life cycles or not. That will be exposed in the Modeling section.<\/p>  <\/li>\n<\/ul>","a133ed44":"<ul>\n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong>now let's look at turning this into a classification solution, can we confidently identify when an asset within its last 20 cycles? Let's start by generating label target columns for training and test data <\/strong> <\/p> <\/li>\n<\/ul>","355d7e44":"<ul>    \n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> int64 : <\/strong> id, runtime, tag17 and tag18.<\/p> <\/li>\n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> float64 : <\/strong> We see that all our remaining columns are float.<\/p> <\/li> \n<\/ul>","c4e45eee":"<a id = \"2\"><\/a>\n<h2 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\"> \ud83d\udcdd Variable Description <\/h2>","7cf04046":"<ul>\n    <li style = \"color:red\"> <p style = \"color:black;font-weight:bold\"> we've decided to delete a few columns with constat values that not carry information about the state of te unit, the following columns are going to be deleted: 'setting3', 'tag1', 'tag5', 'tag10', 'tag16', 'tag18', 'tag19'.<\/p>  <\/li>\n<\/ul>","eff0d2f7":"<h3 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\"> \ud83d\udcca Train - Test Split <\/h3>","0a111f2b":"<a id = \"10\"><\/a>\n<h2 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\"> \ud83d\udcc1 Modeling <\/h2>","08c75128":"<ul>\n    <li style = \"color:red\"> <p style = \"color:black;font-weight:bold\"> we checked the columns of the data. No null data. <\/p>  <\/li>\n<\/ul>","2c3068f9":"<ul>\n    <h3> <p style = \"color:black;font-weight:bold\"> Test set<\/p>  <\/h3>\n<\/ul>","b72e8733":"![Engine-Failure.jpg](https:\/\/www.gouldspumps.com\/ittgp\/medialibrary\/goulds\/website\/Products\/ICO-Open-Impeller-i-Frame-Pump\/ICO.png?ext=.png)\n\n<center><h1 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\" > PREDICTIVE MAINTENENCE<\/h1><\/center>\n<center><h3 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\" > BY: Ewerson Carneiro Pimenta <\/h3><\/center>\n\n<h2 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\" >\ud83d\udcdcIntroduction<\/h2>\n<p style = \"color:black;font-weight:500;text-indent:20px;font-size:16px\">Predictive maintenance techniques are designed to help determine the condition of in-service equipment in order to estimate when maintenance should be performed. This approach promises cost savings over routine or time-based preventive maintenance, because tasks are performed only when warranted. Thus, it is regarded as condition-based maintenance carried out as suggested by estimations of the degradation state of an item.<\/p>\n<p style = \"color:black;font-weight:500;text-indent:20px;font-size:16px\">In this notebook we're going to develope a routine to explore, describe, analyze and predict time of failure for pulp and paper machine dataset. In the context of the dataset, there's 2 tables available: `PM_train.txt` and `PM_test.txt` both have information about 100 unique asset id for the machines, including runtime (cycle) and other columns such as settings and tags.<\/p>\n\n<p style = \"color:black;font-weight:500;text-indent:20px;font-size:16px\">For the purpose of the test, our goal is to provide information of when a machine will fail in the next 20 unity times. The probability must be given twogether with the decision of failure, providing this information for each every single machine id.<\/p>\n    \n\n<h2 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\">\ud83d\udccb Content :<\/h2>\n\n<ul>\n    <li style = \"color:darkgray;font-size:15px\"> <a href = \"#1\" style = \"color:black;font-weight:bold\"> Load and Check Data <\/a> <\/li>\n    <li style = \"color:darkgray;font-size:15px\"> <a href = \"#2\" style = \"color:black;font-weight:bold\"> Variable Description <\/a> <\/li>   \n    <li style = \"color:darkgray;font-size:15px\"> <a href = \"#3\" style = \"color:black;font-weight:bold\"> Univariate Variable Analysis <\/a> <ul> <li style = \"color:lightgray\"><a href = \"#4\" style = \"color:black;font-weight:500\"> Numerical Variable  <\/a><\/li> <li style = \"color:lightgray\"><a href = \"#5\" style = \"color:black;font-weight:500\"> Categorical Variable <\/a><\/li> <\/ul>            \n    <li style = \"color:darkgray;font-size:15px\"> <a href = \"#6\" style = \"color:black;font-weight:bold\">  Exploratory Data Analysis (EDA)  <\/a> <\/li>\n            <li style = \"color:darkgray;font-size:15px\"> <a href = \"#7\" style = \"color:black;font-weight:bold\"> Outlier Detection <\/a> <\/li>\n        <li style = \"color:darkgray;font-size:15px\"> <a href = \"#8\" style = \"color:black;font-weight:bold\"> Feature Engineering <\/a> <ul> <li style = \"color:lightgray\"><a href = \"#9\" style = \"color:black;font-weight:500\"> What is skewness ?  <\/a> <\/ul>\n            <li style = \"color:darkgray;font-size:15px\"> <a href = \"#10\" style = \"color:black;font-weight:bold\"> Modeling   <\/a> <ul> <li style = \"color:lightgray\"><a href = \"#11\" style = \"background:white;color:#8B0000;border:0;border-radius:3px;font-family:Impact;font-size:14px\">1\ufe0f\u20e3 RandomForest Model <\/a><\/li> <li style = \"color:lightgray\"><a href = \"#12\" style = \"background:white;color:#8B0000;border:0;border-radius:3px;font-family:Impact;font-size:14px\"> 2\ufe0f\u20e3 Elastic NET GLM Model <\/a><\/li> <li style = \"color:lightgray\"><a href = \"#13\" style = \"background:white;color:#8B0000;border:0;border-radius:3px;font-family:Impact;font-size:14px\">3\ufe0f\u20e3 GBoost Model <\/a><\/li> <li style = \"color:lightgray\"><a href = \"#14\" style = \"background:white;color:#8B0000;border:0;border-radius:3px;font-family:Impact;font-size:14px\"> 4\ufe0f\u20e3 SVM Model <\/a><\/li> <\/ul>\n    <li style = \"color:darkgray;font-size:15px\"> <a href = \"#17\" style = \"color:black;font-weight:bold\"> Model Result <\/a> <\/ul> \n\n","f4ab4c7c":"<a id = \"13\"><\/a>\n<h3 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\"> 3\ufe0f\u20e3 Gradient Boosting (GB) model <\/h3>","0a89b959":"<a id = \"14\"><\/a>\n<h3 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\"> 4\ufe0f\u20e3 Support Vector Machine Model<\/h3>","e56416bd":"<a id = \"3\"><\/a>\n<h2 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\"> \u270d\ufe0f Univariate Variable Analysis <\/h2>\n\n<ul>    \n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> We see that our data consists of float and int columns. And there is no categorical variables in the dataset.  <\/strong>  <\/p>\n        <ul>\n            <li style = \"color:gray\"> <p style = \"color:black\"> Numerical Variable: all <\/p> <\/li>\n            <li style = \"color:gray\"> <p style = \"color:black\"> Categorical Variable: zero <\/p> <\/li>\n        <\/ul>\n    <\/li> \n<\/ul>","89c49a0c":"<a id = \"1\"><\/a>\n<h2 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\"> \u2714\ufe0f Load and Check Data <\/h2>","a88d5e24":"<a id = \"8\"><\/a>\n<h2 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\"> \ud83d\udcbb Feature Engineering <\/h2>","9aa20671":"REFER\u00caNCIAS:\n\n1. https:\/\/www.kaggle.com\/billstuart\/predictive-maintenance-ml-iiot\n2. https:\/\/www.kaggle.com\/nafisur\/predictive-maintenance-using-lstm-on-sensor-data\n3. https:\/\/www.kaggle.com\/ewersonpimenta\/heart-failure-modeling-skewness-97-5-acc\/edit\n4. https:\/\/www.kaggle.com\/darkside92\/nasa-turbofan-engine-rul-predictive-maintenance\n5. https:\/\/www.kaggle.com\/hanwsf8\/lstm-lgb-catb-for-predictive-maintenance-upper\n6. https:\/\/www.kaggle.com\/questions-and-answers\/40351#226297\n7. https:\/\/www.kaggle.com\/vinayak123tyagi\/damage-propagation-modeling-for-aircraft-engine\n8. https:\/\/www.kaggle.com\/kasevgen\/health-index-and-remaining-time\n9. https:\/\/www.pythonprogramming.in\/bar-chart-with-different-color-of-bars.html\n10. https:\/\/stackoverflow.com\/questions\/35719142\/zeppelin-scala-dataframe-to-python\n11. https:\/\/www.python-graph-gallery.com\/92-control-color-in-seaborn-heatmaps\n12. https:\/\/www.coder.work\/article\/347631\n13. https:\/\/www.javaer101.com\/pt\/article\/20624338.html\n14. https:\/\/ftp.automationdirect.com\/pub\/Product%20Reliability%20and%20MTBF.pdf\n15. https:\/\/medium.com\/swlh\/converting-a-probability-to-fail-into-a-time-to-failure-metric-9a0ddf6122c4\n16. https:\/\/www.kaggle.com\/gpreda\/lanl-earthquake-eda-and-prediction\n17. https:\/\/www.ntnu.edu\/documents\/624876\/1277590549\/chapt2-1.pdf\/4acd3094-e4b9-4bf2-bb20-ec9378d67261\n18. https:\/\/www.kaggle.com\/pimentaeu\/predictive-maintenance-case\/edit\/run\/60450340\n19. https:\/\/www.kaggle.com\/pimentaeu\/rul-prediction-by-pca-other-algo-s\/edit\n20. https:\/\/www.kaggle.com\/billstuart\/predictive-maintenance-ml-iiot\n21. https:\/\/www.kaggle.com\/kucherevskiy\/rul-prediction\n22. https:\/\/www.kaggle.com\/pimentaeu\/rul-prediction-by-pca-other-algo-s\/edit\n\n\n\n\n","5d6030ce":"<a id = \"9\"><\/a>\n<h3 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\"> \u2753 What is skewness ? <\/h3>\n\n![skewness.png](attachment:0433ca4a-3794-4936-b1de-e0ffedd4b2ab.png)\n\n<p style = \"color:black;font-weight:500\" > <strong> Skewness : <\/strong> Skewness refers to a distortion or asymmetry that deviates from the symmetrical bell curve, or normal distribution, in a set of data. In simple terms, it is how much a variable deviates from the normal distribution.  <\/p>\n\n<p style = \"color:black;font-weight:700\" >There are two types.<\/p>\n\n<ul> \n    <li style = \"color:darkred;font-weight:500\" >Right Skewed or Positive Skewed <\/li>\n    <li style = \"color:darkred;font-weight:500\" >Left Skewed or Negative Skewed<\/li>\n<\/ul>\n\n\n<p style = \"color:black;font-weight:500\" >Right Skewed or Positive Skewed --> The distribution has a rightward tail with respect to the normal distribution.<\/p>\n\n<p style = \"color:black;font-weight:500\" >Left Skewed or Negative Skewed --> The distribution has a tail to the left relative to the normal distribution.<\/p>\n\n<h3 style = \"color:gray\" >\nWhy is it important?<\/h3>\n<p style = \"color:black;font-weight:500\" >The model has difficulty in estimating the correct value at other points while focusing on the dense point while predicting on data that does not show a normal distribution.<\/p>\n\n<p style = \"color:black;font-weight:500\" >What do we do, we will look at our skewness values. If it is greater than 1, there is positive skewness, if it is less than -1, there is negative skewness.<\/p>\n\n<h5>Let's start.<\/h5>","dccc7a0c":"<p  style = \"color:black;font-weight:500\" > We will carry out our trainings using the models you see below. Finally, we will compare their achievements. <\/p>\n<ul>\n     <li style = \"color:darkred;font-weight:bold\" >RandomForest Model<\/li>\n     <li style = \"color:darkred;font-weight:bold\" >Elastic NET GLM Model<\/li>\n     <li style = \"color:darkred;font-weight:bold\" >GBoost Model<\/li>\n     <li style = \"color:darkred;font-weight:bold\" >SVM Model<\/li>\n<\/ul>","d50222c3":"<a id = \"5\"><\/a>\n<h3 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\"> \ud83d\udcb9 Categorical Variable <\/h3>","c8f64799":"<h2 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\" > \ud83d\udcd6 Import Library: <\/h2>","4866bc47":"<ul>\n    <li style = \"color:red\"> <p style = \"color:black;font-weight:bold\"> we checked the columns of the data. No null data. <\/p>  <\/li>\n<\/ul>","45f2b503":"<ul>\n<li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> We could fix this disorder by using boxcox transformation, but there's no variable skewed > |1| <\/strong> <\/p> <\/li>\n<\/ul>","f6cd8cbc":"<ul>\n    <li style = \"color:darkred;font-weight:bold\" > <p style = \"color:black;font-weight:400\" > <strong> is looks like tag11 and tag4 variables are good to predict when a engine is more likely to fail.<\/strong> <\/p> <\/li>\n<\/ul>"}}