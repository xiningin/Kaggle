{"cell_type":{"0c851f92":"code","936b6aec":"code","6cd5a269":"code","c3080ed0":"code","f0a67765":"code","396711df":"code","b5fb63d2":"code","44a6f1b8":"code","5e7322d8":"code","247be953":"code","16e4bc94":"code","44a9d393":"code","d199c87f":"code","e64915c4":"code","f3a3df51":"code","76b1f991":"code","de7c0ff2":"code","c947a4f6":"code","7a21e610":"code","f7c2ed6d":"code","01b3832a":"code","c67bf2c8":"code","c172a36d":"code","cda4dab8":"code","285b011f":"code","577c3e12":"code","7362c475":"code","e8afbf4c":"code","4157a429":"code","57bb741d":"code","39d14e7b":"code","1048c378":"code","387a0883":"markdown","cb846573":"markdown","1d59c569":"markdown","9a6c1c10":"markdown","21ec2237":"markdown","e80dc104":"markdown","2b376ef4":"markdown","0fd7d106":"markdown","a1745942":"markdown","eee51af1":"markdown","bf413dcb":"markdown","7335fc9f":"markdown","2264ddf6":"markdown","6e540de0":"markdown","f681b534":"markdown","20e16b5a":"markdown","cf5831a2":"markdown","d41aded7":"markdown","662caa1d":"markdown","58cfd1b4":"markdown","84fca6ad":"markdown","959d7ad6":"markdown","490ca20c":"markdown","5d661626":"markdown","1f260de7":"markdown","950a414e":"markdown"},"source":{"0c851f92":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","936b6aec":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras.utils.np_utils import to_categorical\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Conv2D,MaxPool2D,Flatten,Activation,Dense,Dropout,BatchNormalization\nfrom keras.models import Sequential\nfrom sklearn.metrics import confusion_matrix\nfrom keras.optimizers import Adam\n","6cd5a269":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","c3080ed0":"train.head()","f0a67765":"train.info(),train.shape","396711df":"sns.countplot(train['label'])\nplt.xlabel('Digits')\nplt.ylabel('Frequency')\nplt.show()","b5fb63d2":"test.head()","44a6f1b8":"test.shape","5e7322d8":"y_train = train['label'] #dependent_feature","247be953":"X_train = train.drop(['label'],axis=1)","16e4bc94":"X_train.head()","44a9d393":"# Normalize the data\nX_train = X_train \/ 255.0\ntest = test \/ 255.0\n","d199c87f":"# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\nX_train = X_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)","e64915c4":"y_train","f3a3df51":"# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\nY_train = to_categorical(y_train, num_classes = 10)\n","76b1f991":"Y_train","de7c0ff2":"X_train,X_val,Y_train,Y_val= train_test_split(X_train,Y_train)\n","c947a4f6":"g = plt.imshow(X_train[7][:,:,0])","7a21e610":"def build_model():\n    model = Sequential()\n\n    model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\n    model.add(BatchNormalization())\n    model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\n    model.add(MaxPool2D(pool_size=(2,2)))\n    model.add(Dropout(0.25))\n\n\n    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\n    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n    model.add(Dropout(0.25))\n    \n    model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\n    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n    model.add(Dropout(0.25))\n    \n    model.add(Flatten())\n    model.add(Dense(256, activation = \"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(10, activation = \"softmax\"))\n\n    return model","f7c2ed6d":"model= build_model()","01b3832a":"model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])","c67bf2c8":"model.summary()","c172a36d":"from keras.utils.vis_utils import plot_model\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n","cda4dab8":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)# randomly flip images\ndatagen.fit(X_train)","285b011f":"batch_size=64","577c3e12":"hist = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=64),\n                           steps_per_epoch=len(X_train)\/\/batch_size,\n                           epochs=20, #Increase this when not on Kaggle kernel\n                           verbose=1,  #1 for ETA, 0 for silent\n                           validation_data=(X_val[:400,:], Y_val[:400,:]))","7362c475":"final_loss, final_acc = model.evaluate(X_val, Y_val, verbose=0)\nprint(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss, final_acc))\n","e8afbf4c":"plt.plot(hist.history['loss'], color='b')\nplt.plot(hist.history['val_loss'], color='r')\nplt.show()\nplt.plot(hist.history['accuracy'], color='b')\nplt.plot(hist.history['val_accuracy'], color='r')\nplt.show()\n","4157a429":"y_hat_val = model.predict(X_val)","57bb741d":"y_pred = np.argmax(y_hat_val, axis=1)\ny_true = np.argmax(Y_val, axis=1)\ncm = confusion_matrix(y_true, y_pred)\nprint(cm)","39d14e7b":"y_hat = model.predict(test, batch_size=64)","1048c378":"y_pred = np.argmax(y_hat,axis=1)","387a0883":"5. Dropout Layer:- \nDropout is a technique used to improve over-fit on neural networks, Basically during training half of neurons on a particular layer will be deactivated. This improve generalization because force your layer to learn with different neurons the same \"concept\". During the prediction phase the dropout is deactivated. \n\nThen we'll repeat the same layer three times \n*All the layers other than output layer will have ReLu activation In a neural network, the activation function is responsible for transforming the summed weighted input from the node into the activation of the node or output for that input. The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero*. \n\n**Output layer**\n\n6. Flatten layer:- \nIn this lyaer we are literally going to flatten our pooled feature map into a column like in the image below.\n![image.png](attachment:image.png)\n\n7. Dense layer:- \nThe dense layer is a fully connected layer, meaning all the neurons in a layer are connected to those in the next layer.A densely connected layer provides learning features from all the combinations of the features of the previous layer\n\n**For the fully connected layer the activation function is Softmax: which is used for multiclass classifiaction**","cb846573":"**Before starting I would request you to please upvote this notebook because It has taken a large amount of time and efforts ,and an upvote will motivate me to make more such content, and give back to the community.**","1d59c569":"# Evaluating Model:-\nThis is the plot of model training and we can clearly see the decreasing loss and increasing accuracy","9a6c1c10":"# Understanding the problem:\nMNIST (\"Modified National Institute of Standards and Technology\") is the de facto \u201chello world\u201d dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\n\n***In this competition, our goal is to correctly identify digits from a dataset of tens of thousands of handwritten images.***","21ec2237":"# Content of the Notebook\n1. Data importing,preparation and visulisation\n2. Data Augmentation\n3. Buiding Model(in this I will explain CNN)\n4. Model fitting\n5. Model Evaluation\n6. Result Generation","e80dc104":"2. Batch Normalisation:-\nTo increase the stability of a neural network, batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation.\nbatch normalization allows each layer of a network to learn by itself a little bit more independently of other layers.\n3. Another Convutional layer:-\nIt perform the same action as the previous layer\n4. Max Pooling layer:-\nSometimes when the images are too large, we would need to reduce the number of trainable parameters. It is then desired to periodically introduce pooling layers between subsequent convolution layers. Pooling is done for the sole purpose of reducing the spatial size of the image.\n![image.png](attachment:image.png)\n\n","2b376ef4":"# Fitting the model:-\nWe are using model.fit_generator that takes the augmented data, I've defined epochs(number of times we will go through the model) as 20 (the larger the number of epochs the better the accuracy).\nFor the sake of notebook purpose I've taken it as 20, if you\"ll fork it change it to 50\/60","0fd7d106":"Confusion Matrix for better understanding of True positive and Negative","a1745942":"***If you liked this notebook and it helped you in learnig something please upvote,\nIt has taken a large amount of time, and an upvote will motivate me to make more such content, and give back to the community. \nThanks for reading.\nFeedbacks and Suggestions are welcomed.****","eee51af1":"# Compiling the model\nmodel.compile is used to compile the model the loss is Categoriacal crossentopy since we are doing multiclass classification, one can use Binary crossentropy for binary classification,\nThe opitimizer is Adam,they basically optimize loss and make trainning better and fast to read more\nhttps:\/\/algorithmia.com\/blog\/introduction-to-optimizers,\nhttps:\/\/towardsdatascience.com\/how-to-train-neural-network-faster-with-optimizers-d297730b3713\nplease go through above links","bf413dcb":"# Getting Started with CNN\n\n***Hi I am creating this notebook as a note to self as well as for making CNNs easy***\nWhen  I first came across with the idea of Deep learning and Convutional Nueral Network, it overwhelmed as well as excited me, so here I'll be sharing my learning and also will help you understand cnn better.\n\nIn this notebook I'll deal with Minst Dataset,\nI'll try my best to explain Each and everything thouroughly, also **I managed to achieve 99+% accuracy, so stay tuned**\n **If you are new to CNN or deep learning please look into this notebook it will learning easy**","7335fc9f":"#   How I built my model?\n\nHere I've created a function build_model,\n**Defining Cnn's Architecture**\nMost simply, we can compare an architecture with a building. It consists of walls, windows, doors, et cetera \u2013 and together these form the building. Explaining what a neural network architecture is benefits from this analogy. Put simply, it is a collection of components that is put in a particular order. The components themselves may be repeated and also may form blocks of components. Together, these components form a neural network: in this case, a CNN to be precise.\n\nSo the first step was to decide the model type as Seuential,**A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor**.\nSo basically,In a sequential layer there is one input and one output, and then the output is fed into another layer(can be seen, in the picture later)\n\nThe next step is to define the layers of single Network or Architechture,\n1. Convulutional layer:-   \nWhat a Convutional layer does, it basically perform a element-wise operation with filters(used for eedge detetction) as shown here:\n![image.png](attachment:image.png)\n","2264ddf6":"**Plotting labels and checking their frequency**","6e540de0":"# What is preprocessing and Why should i scale my data?\n Data preprocessing is simply a data mining technique which is used to transform the raw data in a useful and efficient format.\n \n Deep learning neural network models learn a mapping from input variables to an output variable.As such, ***the scale and distribution of the data drawn from the domain may be different for each variable.Input variables may have different units (e.g. feet, kilometers, and hours) that, in turn, may mean the variables have different scales.Differences in the scales across input variables may increase the difficulty of the problem being modeled.***\nA target variable with a large spread of values, in turn, may result in large error gradient values causing weight values to change dramatically, making the learning process unstable.\n** *Scaling input and output variables is a critical step in using neural network models***.\n\n","f681b534":"**Importing necessary libraries**","20e16b5a":"***Plotting an example of Image data from the training dataset***","cf5831a2":"# plotting model\nEverything that I've explained earlier can be understood in a better way with help of this","d41aded7":"***Splitting Train dataset into training and validation dataset***","662caa1d":"Converting y_train to categorical variable, you ask why?\nWe have to one hot encode integer data before passing it to our model\n","58cfd1b4":"**Here labels are the digits which we have to recognize**","84fca6ad":"**Dividing the train data set into dependent(labels) and independent(pixels) features**","959d7ad6":"**This is the notebook I've created  for  learning purpose, I've missed out following thing (for keeping it short and not giving too much information in one notebook)**\n1. Hyperparameter tunning :- \nCan done with the help of GridSearchCV, it helps you select the best parameters for your model,\n2. Callbacks:-\nCallback is a technique that prevents overfitting, Earlystopping and model checkpoint are few examples of callbacks\n","490ca20c":"Importing Data","5d661626":"# Data Augmentation:\nData augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data. Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks.\n![image.png](attachment:image.png)","1f260de7":"# Convutional Neural Network\n1. ***What is Convutional Neural Network?***\nA Convolutional Neural Network (ConvNet\/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects\/objects in the image and be able to differentiate one from the other. The pre-processing required in a ConvNet is much lower as compared to other classification algorithms.\n\n***The architecture of a ConvNet is analogous to that of the con\nconectivity pattern of Neurons in the Human Brain and was inspired by the organization of the Visual Cortex***\n\n2. ***How Convutional Neural Networl work?***\n\nhttps:\/\/e2eml.school\/how_convolutional_neural_networks_work.html\nhttps:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/architecture-of-convolutional-neural-networks-simplified-demystified\/\n\nPlease go trough the above links to get better understanding at the working of the CNN,\n\n![image.png](attachment:image.png)","950a414e":"# Creating Prediction"}}