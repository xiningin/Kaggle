{"cell_type":{"b20ca443":"code","d5febaa3":"code","f6979705":"code","2ea96762":"code","b4958453":"code","cb0da827":"code","958e824b":"code","7b03c91a":"code","04216523":"code","f920afd4":"code","f532fb9f":"code","c9e35287":"code","6ac5414d":"markdown","7a59b269":"markdown","269ad55d":"markdown","e279c13f":"markdown","a7bf59e1":"markdown","e0fca28a":"markdown","33587c69":"markdown","17e5faa5":"markdown","7e38fa1e":"markdown","52248e40":"markdown","ca4de78b":"markdown","b2143c64":"markdown","6088cb52":"markdown","e5ccf489":"markdown"},"source":{"b20ca443":"# Libraries \nimport pandas as pd\nimport numpy as np \nimport tensorflow as tf \nimport matplotlib.pyplot as plt \nfrom sklearn.preprocessing import OneHotEncoder,StandardScaler\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nimport seaborn as sns\n%matplotlib inline \n\n\n# ----------> Deep Learning from keras\nimport keras\nfrom tensorflow.keras import Sequential , Model\nfrom tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization,Concatenate\nfrom tensorflow.keras.optimizers import Adam, SGD, RMSprop\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping","d5febaa3":"# Functions\nclass PrintDots(Callback):\n    def on_epoch_end(self,epoch,logs):\n        if epoch % 100 == 0 :print('')\n        print('.', end = '')\n\n# Deep Learning Model\ndef  build_model (input_shape, layer_1, layer_2):\n    model = Sequential ()\n    model.add( Dense ( layer_1[0] , kernel_initializer = layer_1[1] , input_shape = (13,), activation =layer_1[2]))\n    if layer_2 != None:\n        #model.add(BatchNormalization(axis = 1))\n        model.add( Dense ( layer_2[0] , kernel_initializer = layer_2[1], activation =layer_2[2]))\n        model.add(Dense (1, activation = 'sigmoid'))\n    else:\n        model.add(Dense (1, activation = 'sigmoid'))\n\n    return model\n\n\n# Creating bins for Credit Score where 1 is the lowest and 4 the highest one\ndef splitting_credit_score (x):\n    if x <= 629 :\n        return 1\n    elif all([x>=630, x<=689]):\n        return 2\n    elif all([x>=690, x<=719]):\n        return 3\n    else :\n        return 4 \n\n\ndef Plotting (models):\n    fig, ax = plt.subplots(2,2, figsize =(25,10))\n    model_plotting = [models,\n                    ['One hidden layer + Adam Optimizer','One hidden layer + SGD and momentum  Optimizer',\n                    'Two hidden layer + SGD and momentum  Optimizer','Two hidden layer + RMSprop'],\n                    [ax[0,0],ax[0,1],ax[1,1],ax[1,0]]]\n\n    for num, i in enumerate(model_plotting[2]):\n        i.plot(model_plotting[0][num].history.history['loss'],'r' , marker = '.', label = 'Traning_Loss')\n        i.plot(model_plotting[0][num].history.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n        i.plot(model_plotting[0][num].history.history['accuracy'],'g' , marker = '.', label = 'Traning_Accuracy')\n        i.plot(model_plotting[0][num].history.history[\"val_accuracy\"],'y', marker='.', label=\"Validation Accuracy\")\n        i.legend()\n        i.set_ylim(0.1,1)\n        i.axhline(y=0.87, color='g', linestyle='-')\n        i.axhline(y=0.335, color='g', linestyle='-')\n        i.set_title(\"Model \"+str(num)+\" - \"+model_plotting[1][num])","f6979705":"# Loading Data\npath = '..\/input\/deep-learning-az-ann\/Churn_Modelling.csv'\ndf = pd.read_csv(path)\ndf.sample()","2ea96762":"columns_drop = ['RowNumber','CustomerId','Surname'] # Drop\ndf.drop(columns_drop,axis = 1, inplace= True )\nnum_  = df.select_dtypes(['int64','float64']).columns\n\n\n# - Dealing with Numerical Values - not normally distributed\nskw = df[num_].skew().sort_values(ascending=True)\ncolumns_skw = skw.loc[skw>0.75]\nfor i in columns_skw.index:\n    if i not in ['Exited']:\n        df[i]=np.log1p(df[i]).astype('float')\n\ndf['CreditScore'] = df['CreditScore'].apply(splitting_credit_score)\n\n# Deealing with Categorical Values \ndf_enc = df[['Geography','Gender']].copy()\nOHC =OneHotEncoder()\n\nfor col in df_enc.columns:\n    data = OHC.fit_transform(df_enc[[col]]).astype('int')\n    df_enc = df_enc.drop(col, axis = 1)\n    cats = OHC.categories_\n    new_cols = [\"_\".join([col,cat]) for cat in cats[0]]\n    new_df = pd.DataFrame(data.toarray(), columns = new_cols)\n    df_enc = pd.concat([df_enc,new_df],axis = 1)\n\n\ndf = pd.concat([df,df_enc], axis = 1)\ndf.drop(['Geography','Gender'], axis = 1 , inplace= True)\ndf.head(3)","b4958453":"from IPython.display import display\nfrom PIL import Image\npath=('..\/input\/deep-images\/1_Vyz8w8ZGiZCl17birZqIyw.png')\ndisplay(Image.open(path))\n","cb0da827":"# Training \noptimizer_1 = Adam(0.01)\noptimizer_2 = SGD(0.01, momentum = 0.5)\noptimizer_3= RMSprop(0.01, momentum = 0.5)\n\nlayer_1 = [int(8),'glorot_uniform','relu']\nlayer_2 = [int(5),'glorot_uniform','relu']\n\n\n\nlayers = [[layer_1,None],[layer_1,None], [layer_1 , layer_2],[layer_1 , layer_2]] #Defining Criteria for Hidden Layers\nopts = [optimizer_1,50],[optimizer_2,50],[optimizer_2,50],[optimizer_3,100]  # Defining Optimizer and Epoch size\nESP = EarlyStopping(monitor = 'loss', patience = 5)","958e824b":"# Splitting Data\nX = df.drop('Exited', axis = 1)\ny = df[['Exited']]\n\n# Preocessing_data for Deep Learning\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42,stratify=y)\n\n# Normalizing\nnormalizing = StandardScaler()\nX_train = normalizing.fit_transform(X_train)\nX_test = normalizing.fit_transform(X_test)\n\n# Training\nInput_Shape = X.shape[1] # 13  Columns \nmodels = []\npredictions = []\nfor lay, opt in zip(layers, opts):\n    model = build_model(Input_Shape,lay[0], lay[1])\n    model.compile(optimizer = opt[0] , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n    model.fit(X_train,y_train , batch_size = 32 , \n              epochs = opt[1] ,validation_data = (X_test,y_test), shuffle=True , \n              callbacks = [PrintDots(),ESP], verbose = 0)\n    models.append(model)  # Saving models \n\n#---------> plotting\nPlotting(models)\ny_pred = models[1].predict(X_test)\nscore, acc = model.evaluate(X_test, y_test,batch_size=10)\nprint('Test score:', score)\nprint('Test accuracy:', acc*100)\nprint('Classification report: \\n', classification_report(y_test, np.round(y_pred)))","7b03c91a":"from IPython.display import display\nfrom PIL import Image\npath=('..\/input\/deep-images\/GD_MOMENTuM.png')\ndisplay(Image.open(path))","04216523":"from sklearn.neighbors import LocalOutlierFactor\n\ndf_1 = df.copy() #DF_Scenario_1\nLOF = LocalOutlierFactor(n_neighbors = 25, novelty = False)\ndf_1['Outliers'] = LOF.fit_predict(df_1)\noutliers = df_1[df_1['Outliers'] ==-1].index\nprint(f\"number of outliers {len(outliers)}\")\ndf_1.drop('Outliers', axis = 1 , inplace = True)\ndf_1.drop(outliers , inplace=True)\n\n\n# Splitting Data\nX = df_1.drop('Exited', axis = 1)\ny = df_1[['Exited']]\n\n# Preocessing_data for Deep Learning\nX_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X, y, test_size=0.33, random_state=42,stratify=y)\n\n# Normalizing\nnormalizing = StandardScaler()\nX_train_2 = normalizing.fit_transform(X_train_2)\nX_test_2 = normalizing.fit_transform(X_test_2)\n\nmodels = []\nfor lay, opt in zip(layers, opts):\n    model = build_model(Input_Shape,lay[0], lay[1])\n    model.compile(optimizer = opt[0] , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n    model.fit(X_train_2,y_train_2 , batch_size = 32 , epochs = opt[1] ,\n            validation_data = (X_test_2,y_test_2), shuffle=True , callbacks = [PrintDots(),ESP], verbose = 0)\n    models.append(model)  # Saving models \n\n\n#---------> plotting\nPlotting(models)\ny_pred = models[1].predict(X_test_2)\nscore, acc = model.evaluate(X_test_2, y_test_2,batch_size=10)\nprint('Test score:', score)\nprint('Test accuracy:', acc*100)\nprint('Classification report: \\n', classification_report(y_test_2, np.round(y_pred)))","f920afd4":"df_2 = df.copy() #DF_Scenario_2\nX = df_2.drop('Exited', axis = 1)\ny = df_2[['Exited']]\n\nsmote = SMOTE (sampling_strategy='minority')\nX_smote, y_smote = smote.fit_resample(X,y.values.ravel())\n\nX_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X_smote, y_smote, test_size=0.33, random_state=42, stratify=y_smote)\n\n# Normalizing\nnormalizing = StandardScaler()\nX_train_3 = normalizing.fit_transform(X_train_3)\nX_test_3 = normalizing.fit_transform(X_test_3)\n\nmodels = []\nfor lay, opt in zip(layers, opts):\n  model = build_model(13,lay[0], lay[1])\n  model.compile(optimizer = opt[0] , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n  model.fit(X_train_3,y_train_3 , batch_size = 32 , epochs = opt[1] ,\n            validation_data = (X_test_3,y_test_3), shuffle=True , callbacks = [PrintDots(),ESP], verbose = 0)\n  models.append(model)  # Saving models \n\n\n#---------> plotting\nPlotting(models)\ny_pred = models[3].predict(X_test_3)\nscore, acc = model.evaluate(X_test_3, y_test_3,batch_size=10)\nprint('Test score:', score)\nprint('Test accuracy:', acc*100)\nprint('Classification report: \\n', classification_report(y_test_3, np.round(y_pred)))","f532fb9f":"from IPython.display import display\nfrom PIL import Image\npath=('..\/input\/deep-images\/no-sequential.png')\ndisplay(Image.open(path))","c9e35287":"# -----> Splitting Data According to its nature\ny = df[['Exited']]\nX  = df.drop('Exited', axis = 1)\ncolumns_ =X.columns\n\n# -----> types of data \ncategorical = X.select_dtypes('int64').columns\nnumerical = X.select_dtypes(['float64']).columns\n\n# -----> Splitting data\nshape_a = [X[categorical].shape[1],'Deep-Categorical']\nshape_b = [X[numerical].shape[1],'Deep-Numerical']\n\n# ----->  Standardize \nnormalize = StandardScaler()\nX = normalize.fit_transform(X)\ndata = pd.DataFrame(X, columns = columns_)\n\n# -----> Final Splitting \nX_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.33, random_state=42,stratify=y)\n\nX_train_A, X_train_B = data.loc[X_train.index,categorical],data.loc[X_train.index,numerical]\nX_test_A, X_test_B = data.loc[X_test.index,categorical],data.loc[X_test.index,numerical]\ny_train_, y_test_ = y.loc[y_train.index], y.loc[y_test.index]\n\n# -----> Non Sequential Model\n# Training \noptimizer_1 = Adam(0.01)\noptimizer_2 = SGD(0.01, momentum = 0.3)\noptimizer_3= RMSprop(0.01, momentum = 0.5)\nESP = EarlyStopping(monitor = 'loss', patience = 10)\n\ninput_A = Input(shape = (shape_a[0],) , name = shape_a[1])\ninput_B = Input(shape = (shape_b[0],) , name = shape_b[1])\nhidden_1 = Dense(6, activation = 'relu')(input_A)\nhidden_2 = Dense(2, activation = 'relu')(input_B)\nbatch_1 = BatchNormalization(axis = 1)(hidden_1)\nbatch_2 = BatchNormalization(axis = 1)(hidden_2)\nconcat = Concatenate()([batch_1,batch_2])\nmain = Dense(2, activation = 'relu')(concat)\noutput = Dense(1, name = 'output', activation = 'sigmoid')(main)\nmodel_ns = keras.models.Model(inputs =[input_A,input_B], outputs = [output])\n\nmodel_ns.compile(optimizer= optimizer_2 ,  loss = 'binary_crossentropy',metrics = ['accuracy'] )\nmodel_ns.fit((X_train_A,X_train_B),y_train_, epochs = 500,\n            validation_data = ((X_test_A, X_test_B) ,y_test_), shuffle=True ,callbacks = [ESP,PrintDots()], verbose = 0)\n\n\nplt.plot(model_ns.history.history['loss'],'r' , marker = '.', label = 'Traning_Loss')\nplt.plot(model_ns.history.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\nplt.plot(model_ns.history.history['accuracy'],'g' , marker = '.', label = 'Traning_Accuracy')\nplt.plot(model_ns.history.history[\"val_accuracy\"],'y', marker='.', label=\"Validation Accuracy\")\nplt.legend()\nplt.axhline(y=0.87, color='g', linestyle='-')\nplt.axhline(y=0.335, color='g', linestyle='-')\n\n\ny_pred = model_ns.predict((X_test_A, X_test_B))\nscore, acc = model_ns.evaluate((X_test_A, X_test_B) ,y_test_,batch_size=10)\nprint('Test score:', score)\nprint('Test accuracy:', acc*100)\nprint('Classification report: \\n', classification_report(y_test_, np.round(y_pred)))","6ac5414d":"## About this:\nIn here you will find some Deep learning models and several scenarios as well as SMOTE (oversampling for imbalanced data set) and Optimizer with Hyperparameter , at the end you will also find a ***No sequential NN Model** \n","7a59b269":"### Second Escenario - Finding Outliers and Keeping those with \"0 Estimated Salary \"\n1. I do this because FNN are robust to Outliers and of course Customer that has 0 balance 'no debt' might leave the bank \n2. Removing all those 0 will afect the model negatively (because i did it and showed worse results)\n3. Identified Outliers ","269ad55d":"##### Observations\n1. This model seem to have the same performance than model 1 in scenario #1 \n2. This model was only for practicing\/ Learning purpose \n3. Based on the graph showed above the model stil have room for improvement maybe if i apply SMOTE tecnhiques and balance the data ","e279c13f":"### First Escenario - Accuracy 86 , f1-score : 95, 56 to 0, 1 respectively \n1. Sequentaial and Fully connected NN\n2. Evaluating each of Optimizer\n3. Notice that the data is inbalanced and bais to Exited = 0\n4. Normalizing Data","a7bf59e1":"##### Observations\n1. In here we can see hug improvement but easy this is beause we have a balanced data \n2. If the original data would have been like this one , then the model would have been good enough to predict 0,1 closing banking account \/ no closing \n3. Notice that eventhough the accuracy now is 88, what is really important in this escenario is the f1 score for both classes","e0fca28a":"### Third Escenario -  SMOT for Oversampling + Deep Learning   --1> Accuracy 88\n1. I used SMOTE for oversampling sub target with less samples\n2. Used Standar Scaler to Normalize data \n3. Evaluate all optimzer and choose the best one\n\n***SMOTE create synthetic data samples that might have similars characteristic to the orignal data***","33587c69":"### Here Few of the Fuctions:\n1. Plotting Training and epochs - 4 subplots a\n2. Credit Score Transformation - According to Fedex USA\n3. Callbacks showing  \" ..... \" ","17e5faa5":"### General Pre-processing and Feature Engineering Techniques\n1. Dropping Inncesaries Columns\n2. Nomalizing Numerical Values\n3. Transforming Categorical Variables","7e38fa1e":"##### Observations\n1. after removing outliers the F1 Score dropped by -1 point , accuracy didnt improve\n2. Removing those outliers 30 aprox impact negatively the model \n3. in the next scenario I will use SMOTE to oversample the sub target with less samples","52248e40":"### General settings for Deep Learning \n1. Setting Optimizer Configurations\n2. Weigth Initialization\n3. Defining Number of layers \n4. Defining EarlyStopping and Criterias\n\n\n#### This is a represetantion of NN used in 1st, 2nd and 3rd escenario","ca4de78b":"### Loading Data set and extracting some samples","b2143c64":"##### Observations\n1. the model seems to overfits to the training data \n2. eventhough Accuracy is 86 , the trade off between f1-score for \"0\",\"1\" needs more improvement\n3. Model 1  - One hidden layer + SGD with Momentum has better performance","6088cb52":"### Fuctions","e5ccf489":"### Fourth Escenario - Non Sequential Feedforward NN \n1. Create Two Sequential NN (one for numerical value, the other for categorical values).\n2. Concat both NN in a sub NN that could learn the best combinations.\n3. Applied Batch Normalization to avoid variance in the input (check some videos in youtube).\n4. One perceptron using sigmoid fuction to predict the ouput.\n\n#### Visual Representation of the Network that I built"}}