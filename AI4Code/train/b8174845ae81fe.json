{"cell_type":{"8531ec56":"code","a2fd8de8":"code","c753e3ca":"code","86984df6":"code","d016f936":"code","c29b815f":"code","c826dcb1":"code","2375e907":"code","e08e1002":"code","ff28b22d":"code","bde6f7a2":"code","f255d220":"code","473c8622":"code","11d6edc6":"code","b2cf8454":"code","20a2a8c9":"code","fae1f22e":"code","e2abb666":"code","c0f5e03b":"code","63fbfe29":"code","13bf017a":"code","dd728fc0":"code","fa07cece":"code","59b41a5f":"code","6200a442":"code","fa984ed4":"code","bbf301c2":"code","b646a631":"code","60d9412b":"code","83673084":"code","e29d22b3":"code","cd6619b6":"code","800d3077":"code","8fd924bd":"code","b9ff5c0a":"code","f76ac453":"code","0a645c13":"code","3333ec46":"code","bf2bd739":"code","a1758ec1":"code","65730336":"code","ede03528":"code","bdd2d323":"code","482a8a29":"code","94aa229e":"code","776b5f75":"code","34c3998d":"code","58f8a2bd":"code","0c38205f":"code","d86d3b34":"code","c50418c5":"code","14f78539":"code","804b04b2":"code","23f59361":"code","1d96705a":"code","974978cd":"code","05a428d7":"code","95864b02":"code","a37c98d2":"code","953e8deb":"code","13e837aa":"code","1ef9baf3":"code","29d84bc1":"code","e1aa5104":"code","00b291bc":"markdown","529301a8":"markdown","7a6ad1a2":"markdown","c1665f64":"markdown","13147e3f":"markdown","3c416836":"markdown","14efd11c":"markdown","d4ddf30b":"markdown","ae158088":"markdown","6d8bc6ef":"markdown","57e4c7d9":"markdown","85a021e6":"markdown","a80cf5bd":"markdown","d250a7eb":"markdown","ba5182d3":"markdown","ac1c69e9":"markdown","5629ef50":"markdown","dbd1b66c":"markdown","730b8f4a":"markdown","75860901":"markdown","1d81c0a1":"markdown","2f092b7b":"markdown","54803b28":"markdown","d734f30d":"markdown","4ea73297":"markdown","6fdf8ee4":"markdown","8723a4a9":"markdown","71e55918":"markdown","23beea9c":"markdown","243461fb":"markdown","b22a7702":"markdown","d0318acc":"markdown","5dea9614":"markdown","643ed5bf":"markdown","4a16cd59":"markdown","f1f6c50c":"markdown","1483af00":"markdown","34d409e3":"markdown","3be4cea3":"markdown","fc316ec4":"markdown","c0f2c297":"markdown","03dfb2fb":"markdown"},"source":{"8531ec56":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport numpy as np\nimport os\n\n# Set plot style\nplt.style.use('ggplot')","a2fd8de8":"df_test = pd.read_csv(r'..\/input\/titanic\/test.csv')\ndf_test.head(5)","c753e3ca":"df_train = pd.read_csv(r'..\/input\/titanic\/train.csv')\ndf_train.head(5)","86984df6":"sns.pairplot(data=df_train, hue='Survived', diag_kws={'bw': 0.2})","d016f936":"# Age and Survival Outcome\nsns.distplot(df_train['Age'][df_train['Survived'] == 1], label='1')\nsns.distplot(df_train['Age'][df_train['Survived'] == 0], label='0').set_title('Distribution of Age and Survival')\nplt.legend(title='Survived')","c29b815f":"# Gender and Survival Outcome\nsns.countplot(data=df_train, x='Survived', hue='Sex').set_title('Surival Count by Gender')\nnum_male_alive = len(df_train[(df_train['Sex'] == 'male') & (df_train['Survived'] == 1)])\nnum_male_dead = len(df_train[(df_train['Sex'] == 'male') & (df_train['Survived'] == 0)])\nnum_female_alive = len(df_train[(df_train['Sex'] == 'female') & (df_train['Survived'] == 1)])\nnum_female_dead = len(df_train[(df_train['Sex'] == 'female') & (df_train['Survived'] == 0)])\n\nmale_survival_rate = ((num_male_alive) \/ (num_male_alive + num_male_dead)) * 100\nfemale_survival_rate = ((num_female_alive) \/ (num_female_alive + num_female_dead)) * 100\n\nprint('Male Survival Rate (%):', round(male_survival_rate, 2))\nprint('Female Survival Rate (%):', round(female_survival_rate, 2))","c826dcb1":"# Passenger class and survival outcome\nsns.countplot(data=df_train, x='Survived', hue='Pclass').set_title('Survival Count by Passenger Class')","2375e907":"print('Pclass 3:', len(df_train[df_train['Pclass'] == 3]))\nprint('Pclass 2:', len(df_train[df_train['Pclass'] == 2]))\nprint('Pclass 1:', len(df_train[df_train['Pclass'] == 1]))","e08e1002":"# Siblings\/Spouses and survival outcome\nsns.countplot(data=df_train,\n              x='Survived', hue='SibSp').set_title('Siblings\/Spouses Onboard Compared to Survival Outcome')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., title='SibSp')","ff28b22d":"# Parents\/Children and survival outcome\nsns.countplot(data=df_train,\n              x='Survived',\n              hue='Parch').set_title('Num Children\/Parents Onboard Compared to Survival Outcome')","bde6f7a2":"# Check survival outcome compared to embarkation location\nsns.countplot(data=df_train, x='Survived', hue='Embarked').set_title('Embark Location vs. Survival')\nprint('Total Embarked S:', len(df_train[df_train['Embarked'] == 'S']))\nprint('Total Embarked C:', len(df_train[df_train['Embarked'] == 'C']))\nprint('Total Embarked Q:', len(df_train[df_train['Embarked'] == 'Q']))","f255d220":"from collections import Counter\ntest_titles = [x[1].split('.')[0].lstrip() for x in df_test['Name'].str.split(',')]\ntrain_titles = [x[1].split('.')[0].lstrip() for x in df_train['Name'].str.split(',')]\n\ncombined_titles = Counter(test_titles) + Counter(train_titles)\ncombined_titles","473c8622":"# import this library to count instances\nfrom collections import Counter\n\n# View training data distribution\ndf_title = pd.DataFrame({'Title': list(Counter(train_titles).keys()), 'Count':list(Counter(train_titles).values())})\nsns.barplot(data=df_title, x='Title', y='Count')\nplt.title('Count of Passenger Titles')\nplt.xticks(rotation=45)","11d6edc6":"# Test dataset\nsns.heatmap(df_test.isnull(), yticklabels=False, cbar=False)\ndf_test.isna().sum()","b2cf8454":"# Train dataset\nsns.heatmap(df_train.isnull(), yticklabels=False, cbar=False)\ndf_train.isna().sum()","20a2a8c9":"sns.heatmap(df_train.corr())","fae1f22e":"df_test['Alone'] = (df_test['SibSp'] + df_test['Parch']).map(bool)\ndf_test['Alone'] = df_test['Alone'].map({True:1, False:0})\ndf_test.drop(['SibSp', 'Parch'], axis=1, inplace=True)\n\ndf_train['Alone'] = (df_train['SibSp'] + df_train['Parch']).map(bool)\ndf_train['Alone'] = df_train['Alone'].map({True:1, False:0})\ndf_train.drop(['SibSp', 'Parch'], axis=1, inplace=True)\ndf_train.head(3)","e2abb666":"from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(sparse=False)\n\ndf_train['Sex'] = enc.fit_transform(df_train[['Sex']])\n\ndf_test['Sex'] = enc.fit_transform(df_test[['Sex']])\ndf_test.head(2)","c0f5e03b":"df_embarked_encoded = pd.get_dummies(df_train['Embarked'], prefix='emb', drop_first=True)\ndf_train = pd.concat([df_train, df_embarked_encoded.astype(np.int64)], axis=1)\ndf_train.drop('Embarked', axis=1, inplace=True)\n\ndf_embarked_encoded = pd.get_dummies(df_test['Embarked'], prefix='emb', drop_first=True)\ndf_test = pd.concat([df_test, df_embarked_encoded.astype(np.int64)], axis=1)\ndf_test.drop('Embarked', axis=1, inplace=True)","63fbfe29":"# df_test.dropna(subset=['Embarked'], inplace=True)\n# df_test['Embarked'] = enc.fit_transform(df_test[['Embarked']])\n\n# df_train.dropna(subset=['Embarked'], inplace=True)\n# df_train['Embarked'] = enc.fit_transform(df_train[['Embarked']])","13bf017a":"# Generate new Title column\ntitles = [x[1].split('.')[0].strip() for x in df_train['Name'].str.split(',')]\n# titles = [x.strip() for x in titles]\ndf_train['Title'] = titles\n\ntitles = [x[1].split('.')[0].strip() for x in df_test['Name'].str.split(',')]\n# titles = [x.strip() for x in titles]\ndf_test['Title'] = titles","dd728fc0":"# Perform on training data\ntitles = [x[1].split('.')[0] for x in df_train['Name'].str.split(',')]\ntitles = [x.strip() for x in titles]\ndf_train['Title'] = titles\ndf_train = pd.concat([df_train, pd.get_dummies(df_train['Title'], drop_first=True).astype(np.int64)], axis=1)\ndf_train.drop('Title', axis=1, inplace=True)\n\n# Perform on testing data\ntitles = [x[1].split('.')[0] for x in df_test['Name'].str.split(',')]\ntitles = [x.strip() for x in titles]\ndf_test['Title'] = titles\ndf_test = pd.concat([df_test, pd.get_dummies(df_test['Title'], drop_first=True).astype(np.int64)], axis=1)\ndf_test.drop('Title', axis=1, inplace=True)\ndf_test.head()\n","fa07cece":"# If there are missing columns add them and then fill them with zeros\nfor title in combined_titles.keys():\n    if title not in df_train.columns:\n        print(title)\n        df_train = pd.concat([df_train, pd.DataFrame(np.zeros((len(df_train), 1)), columns=[title])], axis=1)\n        \nfor title in combined_titles.keys():\n    if title not in df_test.columns:\n        df_test = pd.concat([df_test, pd.DataFrame(np.zeros((len(df_test), 1)), columns=[title])], axis=1)","59b41a5f":"# Quick EDA now that we have encoded data\ndef get_title_survival_ratio(df, column):\n    survived = len(df[(df[column] == 1) & (df['Survived'] == 1)])\n    total = len(df[df[column] == 1])\n    try:\n        return  survived \/ total\n    except ZeroDivisionError:\n        return 0\n\ntitle_ratios = {}\nfor title in Counter(train_titles):\n    title_ratios[title] = get_title_survival_ratio(df_train, title)\n\nsns.barplot(data=pd.DataFrame({'Title': list(title_ratios.keys()), 'Survival Rate': list(title_ratios.values())}),\n            x='Title', y='Survival Rate')\nplt.xticks(rotation=45)\nplt.title('Title Survival Ratio')","6200a442":"drop_columns = ['Cabin', 'Ticket', 'Name']\ndf_test.drop(columns=drop_columns, inplace=True)\ndf_train.drop(columns=drop_columns, inplace=True)","fa984ed4":"from sklearn.preprocessing import scale\n\ndf_train.loc[:, 'Fare'] = scale(df_train['Fare'])\ndf_train.loc[:, 'Age'] = scale(df_train['Age'])\n\ndf_test.loc[:, 'Fare'] = scale(df_test['Fare'])\ndf_test.loc[:, 'Age'] = scale(df_test['Age'])\ndf_test.head(3)","bbf301c2":"df_train.groupby('Pclass').mean()","b646a631":"def guess_fare(columns):\n    \n    fare = columns[1]\n    pclass = columns[0]\n    \n    if pd.isnull(fare):\n        if pclass == 1:\n            return 84\n        elif pclass == 2:\n            return 20\n        else:\n            return 13\n    else:\n        return fare\n\ndf_train.loc[:, 'Fare'] = df_train[['Pclass', 'Fare']].apply(guess_fare, axis=1)\ndf_test.loc[:, 'Fare'] = df_test[['Pclass', 'Fare']].apply(guess_fare, axis=1)","60d9412b":"df_train.groupby('Pclass').mean()","83673084":"def guess_age(columns):\n    \n    age = columns[1]\n    pclass = columns[0]\n    \n    if pd.isnull(age):\n        if pclass == 1:\n            return 38\n        elif pclass == 2:\n            return 29\n        else:\n            return 25\n    else:\n        return age\n\ndf_train['Age'] = df_train[['Pclass', 'Age']].apply(guess_age, axis=1)\ndf_test['Age'] = df_test[['Pclass', 'Age']].apply(guess_age, axis=1)","e29d22b3":"df_pclass_encoded = pd.get_dummies(df_train['Pclass'], prefix='Pclass', drop_first=True)\ndf_train = pd.concat([df_train, df_pclass_encoded.astype(np.int64)], axis=1)\ndf_train.drop('Pclass', axis=1, inplace=True)\n\ndf_pclass_encoded = pd.get_dummies(df_test['Pclass'], prefix='Pclass', drop_first=True)\ndf_test = pd.concat([df_test, df_pclass_encoded.astype(np.int64)], axis=1)\ndf_test.drop('Pclass', axis=1, inplace=True)","cd6619b6":"print(len(df_train))\ndf_train.isna().sum()","800d3077":"print(len(df_test))\ndf_test.isna().sum()","8fd924bd":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import roc_curve, roc_auc_score\nimport numpy as np","b9ff5c0a":"# Create X y\nX = df_train.drop(['Survived'], axis=1)\ny = df_train['Survived']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=99, stratify=y)\nprint(X_train.shape)\nprint(X_test.shape)\n\n# print the shapes of the new y objects\nprint(y_train.shape)\nprint(y_test.shape)","f76ac453":"from sklearn import linear_model\nalphas = np.logspace(-4, -1, 10)\nscores = np.empty_like(alphas)\nfor i,a in enumerate(alphas):\n    lasso = linear_model.Lasso()\n    lasso.set_params(alpha=a)\n    lasso.fit(X_train, y_train)\n    scores[i] = lasso.score(X_test, y_test)\n    # print(a, lasso.coef_)\n    \nlassocv = linear_model.LassoCV()\nlassocv.fit(X, y)\nlassocv_score = lassocv.score(X, y)\nlassocv_alpha = lassocv.alpha_\n# print('CV', lassocv.coef_)\nprint('Alpha:', lassocv_alpha)\n\nplt.plot(alphas, scores, '-ko')\nplt.axhline(lassocv_score, color='b', ls='--')\nplt.axvline(lassocv_alpha, color='b', ls='--')\nplt.xlabel(r'$\\alpha$')\nplt.ylabel('Score')\nplt.xscale('log')\nsns.despine(offset=15)","0a645c13":"# Import Lasso\nfrom sklearn.linear_model import Lasso\n\n# Instantiate a lasso regressor: lasso\nlasso = Lasso(alpha=lassocv_alpha)\n\n# Fit the regressor to the data\nlasso_coef = lasso.fit(X, y).coef_\n\n# Compute and print the coefficients\ndf_plot = pd.DataFrame({'Feature': X.columns, 'Coefficient': lasso_coef})\nsns.lineplot(data=df_plot, x='Feature', y='Coefficient')\nplt.xticks(rotation=90)","3333ec46":"# Drop features according to Laso output to reduce complexity\ndrop_columns = list(df_plot[df_plot['Coefficient'] < 0]['Feature'])\nprint(drop_columns)\nX.drop(columns=drop_columns, inplace=True)\ny.drop(columns=drop_columns, inplace=True)","bf2bd739":"# Set up params for grid search\nc_space = np.logspace(-5, 8, 15)\nparam_grid = {'C': c_space}\n\n# Instantiate the model\nlogreg = LogisticRegression(max_iter=2000)\n\n# Perform GridSearch\nlogreg_cv = GridSearchCV(logreg, param_grid, cv=10)\nlogreg_cv.fit(X_train, y_train)\nlogreg = logreg_cv.best_estimator_\nprint('Best Params:', logreg_cv.best_params_)\nprint('Best Score:', logreg_cv.best_score_)","a1758ec1":"logreg.fit(X_train, y_train)","65730336":"cross_val_result = cross_val_score(logreg, X, y, cv=10)  # scoring = roc_auc?\nprint('10-fold CV Mean:', cross_val_result.mean())\nprint('10-fold CV STD:', cross_val_result.std())","ede03528":"# Look at model performance using ROC curve\ny_pred_prob = logreg.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\nauc_score = roc_auc_score(y_test, y_pred_prob) \nprint('AUC Score:', auc_score)\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.title('Logistic Regression ROC Curve')","bdd2d323":"# Delete the first row which is \"survived\"\ndf_corr = pd.DataFrame(X_train.columns.delete(0))\n# Rename columns\ndf_corr.columns = ['Feature']\n# Insert coefficient\ndf_corr['Coefficient'] = pd.Series(logreg.coef_[0])\ndf_corr.sort_values(by='Coefficient', ascending=False)","482a8a29":"# Check prediction with test set\ny_pred = logreg.predict(X_test)\nacc_logreg = metrics.accuracy_score(y_test, y_pred)\nprint('Accuracy Logistic Regression:', acc_logreg)\nprint(classification_report(y_test, y_pred))","94aa229e":"# Perform grid search to find optimum parameters\nknn = KNeighborsClassifier()\nparam_grid = {'n_neighbors': list(np.arange(1,30))}  \ngrid = GridSearchCV(knn, param_grid, cv=10)\ngrid.fit(X_train, y_train)\nknn = grid.best_estimator_\nprint('Best KNN Params:', grid.best_params_)\nprint('Best Score:', grid.best_score_)","776b5f75":"cross_val_result = cross_val_score(knn, X, y, cv=10)\nprint('10-fold CV Mean:', cross_val_result.mean())\nprint('10-fold CV STD:', cross_val_result.std())","34c3998d":"# Look at model performance using ROC curve\ny_pred_prob = knn.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\nauc_score = roc_auc_score(y_test, y_pred_prob) \nprint('AUC Score:', auc_score)\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr, label='KNN')\nplt.title('K Nearest Neighbors ROC Curve')","58f8a2bd":"# Check prediction with test set\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nacc_knn = metrics.accuracy_score(y_test, y_pred)\nprint('Accuracy K Nearest Neighbors:', acc_knn)\nprint(classification_report(y_test, y_pred))","0c38205f":"# Perform grid search to find optimum parameters\nparam_grid = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf'],\n              'probability': [False, True]}  \ngrid = GridSearchCV(SVC(), param_grid, refit = True)\ngrid.fit(X_train, y_train)\nprint('Best SVC Params:', grid.best_params_)\n\n# Instantiate\nsvc = SVC(C=grid.best_params_['C'],\n          gamma=grid.best_params_['gamma'],\n          kernel=grid.best_params_['kernel'],\n          probability=grid.best_params_['probability'])","d86d3b34":"cross_val_result = cross_val_score(svc, X, y, cv=10)\nprint('10-fold CV Mean:', cross_val_result.mean())\nprint('10-fold CV STD:', cross_val_result.std())","c50418c5":"# Check prediction with test set\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\nacc_svc = metrics.accuracy_score(y_test, y_pred)\nprint('Accuracy Survey Vector Machines:', acc_svc)\nprint(classification_report(y_test, y_pred))","14f78539":"# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {'max_depth': [3, 4, 5, 6, 7, 8, 9, 10, None],\n              'max_features': ['auto', 'sqrt', 'log2'],\n              'min_samples_leaf': [1, 3, 5, 6, 7, 8, 9],\n              'criterion': ['gini', 'entropy']}\n\n# Instantiate a Decision Tree classifier: tree\ntree = DecisionTreeClassifier()\n\n# Instantiate the RandomizedSearchCV object: tree_cv\ntree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n\n# Fit it to the data\ntree_cv.fit(X, y)\n\n# Print the tuned parameters and score\nprint('Best Params:', tree_cv.best_params_)\nprint('Best Score:',tree_cv.best_score_)","804b04b2":"tree = DecisionTreeClassifier(**tree_cv.best_params_)\ntree.fit(X_train, y_train)","23f59361":"cross_val_result = cross_val_score(tree, X, y, cv=10)\nprint('10-fold CV Mean:', cross_val_result.mean())\nprint('10-fold CV STD:', cross_val_result.std())","1d96705a":"# Look at model performance using ROC curve\ny_pred_prob = tree.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\nauc_score = roc_auc_score(y_test, y_pred_prob) \nprint('AUC Score:', auc_score)\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr, label='Decision Tree')\nplt.title('Decision Tree ROC Curve')","974978cd":"# Check prediction with test set\ntree.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\nacc_tree = metrics.accuracy_score(y_test, y_pred)\nprint('Accuracy Decision Tree Classifier:', acc_svc)\nprint(classification_report(y_test, y_pred))","05a428d7":"# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {'n_estimators': [400, 500, 1000, 1500, 2000],\n              'max_depth': list(np.arange(1,20)) + [None],\n              'max_features': ['auto', 'sqrt', 'log2'],\n              'min_samples_leaf': [3, 4, 5, 6, 7, 8, 9],\n              'criterion': ['gini', 'entropy']}\n\n# Instantiate a Decision Tree classifier: tree\nforest = RandomForestClassifier()\n\n# Instantiate the RandomizedSearchCV object: tree_cv\nforest_cv = RandomizedSearchCV(forest, param_dist, n_jobs=-1, cv=5)\n\n# Fit it to the data\nforest_cv.fit(X, y)\n\n# Print the tuned parameters and score\nprint('Best Params:', forest_cv.best_params_)\nprint('Best Score:',forest_cv.best_score_)","95864b02":"forest = forest_cv.best_estimator_\nforest.fit(X_train, y_train)","a37c98d2":"cross_val_result = cross_val_score(forest, X, y, cv=10)\nprint('10-fold CV Mean:', cross_val_result.mean())\nprint('10-fold CV STD:', cross_val_result.std())","953e8deb":"from sklearn.metrics import roc_curve, roc_auc_score\n\n# Look at model performance using ROC curve\ny_pred_prob = forest.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\nauc_score = roc_auc_score(y_test, y_pred_prob) \nprint('AUC Score:', auc_score)\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr, label='Random Forest')\nplt.title('Random Forest ROC Curve')","13e837aa":"# Check prediction with test set\nforest.fit(X_train, y_train)\ny_pred = forest.predict(X_test)\nacc_forest = metrics.accuracy_score(y_test, y_pred)\nprint('Accuracy Random Forest Classifier:', acc_forest)\nprint(classification_report(y_test, y_pred))","1ef9baf3":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest',\n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_forest, acc_tree]})\nmodels.sort_values(by='Score', ascending=False)","29d84bc1":"y_pred = forest.predict(df_test)\nsubmission = pd.DataFrame({\n        \"PassengerId\": df_test[\"PassengerId\"],\n        \"Survived\": y_pred\n    })\nsubmission\nsubmission.to_csv('kaggle_titanic_pred.csv', index=False)","e1aa5104":"# ! kaggle competitions submit -c titanic -f kaggle_titanic_pred.csv -m \"Version 1. RF n_estimators 1000\"","00b291bc":"# 1 - Exploratory Data Analysis\nLet's take a quick look of the plots. My hypothesis is that survival is correlated with passenger class, gender, and age.\n\n* The lower the class, the more expensive the ticket was. Did rich people have priority?\n* In times of crisis people often say \"*Women and children first!*\" Did that apply in the Titanic disaster?\n\nHowever, we will also look at the other columns and compare them to the survival rate to make sure we capture all available information. First, `sns.pairplot` will be used so we can have a general overview of the data and the `hue` will be set to the `Survived` column.","529301a8":"We can see the effect of the large variance because the accuracy of the Survey Vector Machine model is 0.72 but the mean of the cross validation is only 0.55!","7a6ad1a2":"### Random Forest","c1665f64":"It can be seen that `Pclass=3` has the largest number of passengers. This can explain the high death rate for that passenger class. If the lifeboats were equally distributed (more or less) `Pclass=3` would see the highest losses.","13147e3f":"We will scale the numerical data. This will help with classifier performance that are based on distance or clustering on a graph.","3c416836":"### Decision Tree","14efd11c":"# Fill in empty fare data","d4ddf30b":"# Compare Models\n\nWe can see that Random Forest has the best score with a score of `0.803922`. Compared to my initial runs this is an improvement and it is likely due to the feature engineering with the passenger names and titles.","ae158088":"Using lasso on our data we see that Sex has the highest coeffiient which means high relevance. This is similar with our results from the EDA section using `sns.heatmap()`.","6d8bc6ef":"There is a lot of empty data in the `Age` and `Cabin` columns. We will check if the null data can be filled in later. It should be noted that there is a single piece of null data in the `Fare` column of the test data which does not exist in the training data.\n\nLet's check the correlation using `sns.heatmap()`","57e4c7d9":"# 2 - Feature Engineering and Data Preprocessing\n\nThis section includes creating new data from existing data and preprocessing the data so that it can perform better during model training otherwise known as encoding. During encoding we should check if all unique pieces of data from `df_train` also exist in `df_test` because new columns will be generated based on the data being encoded. For example, it is observed that there is no person with `Title='the Countess'` in the test data but a person with that title exists in the training data. The dataframes should be made uniform so that the model can be trained correctly.","85a021e6":"# Building Models\nIn this section we will train different models using the Titanic dataset and tune them using GridSearchCV.","a80cf5bd":"0 means the passenger did not survive. 1 means the passenger survived. We can see that there is almost an equal distribution survival but when it comes to dying `Pclass=3` jumps ahead with the highest death rate.\n\nBut does this represent a good picture of how the passengers are distributed? How many passengers were in each class?","d250a7eb":"We can see that there is a higher rate of survival if you are a female. The survival rate for females was 74.2% while for males it was only 18.89%.","ba5182d3":"Then we perform a cross fold validation to ensure the train\/test data does not vary wildly.\n\nThe results show an average accuracy of 0.78 with a standard deviation of 0.02. This is a very low variance which means that test results will not vary wildly and are reliable results.","ac1c69e9":"# Encode Pclass Data","5629ef50":"Distribution of age and survival reveal a bump in survival rate among the very young.","dbd1b66c":"### Checking for feature selection","730b8f4a":"It seems that everyone is assigned a title. Let's explore that feature. We need to explore the training and testing data to ensure that we don't miss any titles.","75860901":"We can generate data based on the average fare per passenger class.","1d81c0a1":"We can assume titles of `Mr`, `Mrs`, and `Ms` are common titles. However, titles like `Master` or `Dr` can mean you have a higher standing in society. Does that factor help them survive? This will be explored more in the feature engineering section.","2f092b7b":"Checking through the `Cabin` column. It is too hard to impute data based on other columns and there is too much null data to observe any trend. `Ticket`, `Name`, and `PassengerId` seem to be unique identifiers and are not useful in this analysis.","54803b28":"### Embarked Encoding","d734f30d":"### Survey Vector Machines","4ea73297":"# Submission","6fdf8ee4":"Now we will check some of the specific columns and see how they compare to survival outcome.","8723a4a9":"When it comes to analyzing `SibSp` and `Parch` the only notable difference is that if you have no siblings\/spouses\/children\/parents onboard you have a higher rate of dying. For people with at least 1 of those onboard it makes little difference whether you survive or not.","71e55918":"### Logistic Regression","23beea9c":"Lastly, let's check the null data in both the test and train datasets.","243461fb":"# Check CSV files","b22a7702":"We are mainly looking at the correlation with the `Survived` column. We can see that Sex has one of the highest correlations with survival and `Pclass` has the lowest.","d0318acc":"### Title Feature Engineering and Encoding\nAll passengers have been assigned a title. Let us encode that.","5dea9614":"In the EDA, it was observed that there were some trends related to age. It would be important to fill in some of the data based on information gathered from other columns.\n\nHere are some assumptions based on age:\n* The younger you are the less money you have\n* The older you are the more of a tendency you have to be alone\n\nIn this case, a good way to fill data may be to use age averages of Pclass and substitute them for null data.","643ed5bf":"Embarked location also shows little relevance to survival outcome.","4a16cd59":"Check for null data before proceeding to model building.","f1f6c50c":"Both the SibSp and Parch can be simplified into whether or not the person was alone.","1483af00":"Check train\/test split variance using cross fold validation. The variance is large with the testing result. We see that the standard deviation for a 10-fold cross validation is almost 12%.","34d409e3":"It should be noted that the distribution of titles is very large. Some titles have only 1 person while some titles have more than 100. Moving on, we see that if you are a male with a common title of `Mr` you have a very high chance of death. Meanwhile, titles such as `Lady`, `Mile`, `Mme`, `Ms`, `Sir`, and `the Countess` all survived.","3be4cea3":"We perform a gridsearch to find the best parameter for `C`","fc316ec4":"# Fill in Empty Age Data","c0f2c297":"# Titanic Learning from Disaster\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n\nAccording to the Kaggle dataset page these are the columns and their meanings:\n\n* `Survival`: Whether or not the passenger survived. 0 = No, 1 = Yes\n* `Pclass`: Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd\n* `Sex`: Sex of passenger\n* `Age`: Age of passenger in years\n* `SibSp`: Num of siblings\/spouses onboard\n* `Parch`: Num of parents\/children onboard\n* `Ticket`: Ticket number\n* `Fare`: Passenger fare price\n* `Cabin`: Cabin number\n* `Embarked`: Port of embarkation C = Cherbourg, Q = Queenstown, S = Southampton\n","03dfb2fb":"### K Nearest Neighbors"}}