{"cell_type":{"8c57bf7f":"code","b842a445":"code","9c0a8f6c":"code","009802dc":"code","a4fac5ba":"code","08f9a4bb":"code","5ea66b65":"code","82cfa96c":"code","b31facc9":"code","ee811bdb":"code","f87f741a":"code","e372977f":"code","6b569ebf":"code","4caa134c":"code","d46f230b":"code","b5b7f5b9":"code","80a74c34":"code","82842259":"code","a40cf5f8":"code","6b3b9a88":"code","2110410d":"code","7d9692f2":"code","2a755117":"code","28ae8c52":"code","1239bead":"code","4a447abb":"code","4930ec89":"code","0bf59eed":"code","0c32a9e2":"code","66e24488":"markdown","f919a66f":"markdown","3d9680b5":"markdown","5eb44cb9":"markdown","1ae60309":"markdown","7c88217a":"markdown","51f62e2c":"markdown","eb7c12ad":"markdown","95b89b7e":"markdown"},"source":{"8c57bf7f":"SUBMISSION = True\nUSE_RESCUER_CV = not SUBMISSION\nLB = not SUBMISSION\nSEED = 209321206\nVERBOSE = 1","b842a445":"import pandas as pd\nSTART_TIME = pd.datetime.now()\n\nimport gc\nimport glob\nimport os\nimport re\nimport sys\nimport ujson as json\n\nfrom copy import deepcopy\nfrom functools import partial\n\nimport cv2\nimport keras.backend as K\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom catboost import CatBoostRegressor\n\nfrom IPython.core.display import display\nfrom joblib import Parallel,delayed\n\nfrom keras.models import Model\nfrom keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Input, Dense\nfrom keras.callbacks import EarlyStopping\n\nfrom lightgbm import LGBMRegressor,LGBMClassifier,plot_importance\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer,SnowballStemmer\n\nfrom xgboost import XGBRegressor,XGBClassifier # Not used\n\nfrom scipy.stats import mode\nfrom scipy import sparse\n\nfrom sklearn.base import BaseEstimator,RegressorMixin,TransformerMixin\nfrom sklearn.decomposition import PCA,TruncatedSVD\nfrom sklearn.ensemble import BaggingRegressor,ExtraTreesRegressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LinearRegression,Lasso,Ridge,ElasticNet,BayesianRidge,LassoCV,RidgeCV\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.model_selection import StratifiedKFold,train_test_split,GroupKFold\nfrom sklearn.pipeline import FeatureUnion,make_union,make_pipeline,_fit_transform_one,_transform_one\nfrom sklearn.preprocessing import MinMaxScaler,LabelEncoder\nfrom sklearn.utils.multiclass import unique_labels\n\nfrom tqdm import tqdm_notebook","9c0a8f6c":"import logging\nfrom logging import StreamHandler\n\nclass KaggleHandler(StreamHandler):\n    \n    def __init__(self):\n        StreamHandler.__init__(self)\n        \n    def emit(self, record):\n        msg_fmt = re.sub(r'[^0-9A-z\\.\\,\\-\\:\\=]', ' ', str(self.format(record)))\n        secs_passed = (pd.datetime.now() - START_TIME).seconds\n        msg = '[%ss] %s' % (secs_passed, msg_fmt)\n        os.system(f'echo {msg}')\n        sys.stdout.write(msg + '\\n')\n        \nlogger = logging.getLogger(__name__)\nlogger.handlers = []\nlogger.setLevel(logging.DEBUG)\nfmt = logging.Formatter('[%(asctime)s]: %(message)s')\nkh = KaggleHandler()\nkh.setFormatter(fmt)\nlogger.addHandler(kh)","009802dc":"logger.info('Seed set to %d' % SEED)\nnp.random.seed(SEED)","a4fac5ba":"logger.info(os.listdir(\"..\/input\"))","08f9a4bb":"logger.info(f'SUBMISSION={SUBMISSION}, USE_RESCUER_CV={USE_RESCUER_CV}, LB={LB}, VERBOSE={VERBOSE}')","5ea66b65":"def qwk(predicted_ratings, actual_ratings, nb_ratings=5):\n    \"\"\"Calculate quadratic weighted kappa.\"\"\"\n    assert len(actual_ratings) == len(predicted_ratings) #\"Both the vectors should be of same length\n    actual_ratings = actual_ratings.astype(np.int64)\n    predicted_ratings = predicted_ratings.astype(np.int64)\n\n    # 1. Get the confusion matrix \n    conf_mtx = confusion_matrix(actual_ratings, predicted_ratings).astype(np.float64)\n\n    # 2. Create a weight matrix\n    weight_mtx = np.zeros((nb_ratings, nb_ratings))\n    for i in range(nb_ratings):\n        for j in range(nb_ratings):\n            weight_mtx[i][j] = (i-j)**2 \/ ((nb_ratings-1)**2)\n\n    # 3.Get the histograms for both the raters\n    actual_ratings_hist = np.bincount(actual_ratings,minlength=nb_ratings)\n    predicted_ratings_hist = np.bincount(predicted_ratings, minlength=nb_ratings)\n\n    # 4. Perform an outer product of the histograms\n    out_prod = np.outer(actual_ratings_hist, predicted_ratings_hist).astype(np.float64)\n\n    # 5. Normalize both- the confusion matrix and the outer product matrix\n    conf_mtx \/= conf_mtx.sum()\n    out_prod \/= out_prod.sum()\n\n    # Calculate the weighted kappa\n    numerator = (conf_mtx * weight_mtx).sum()\n    denominator = (out_prod * weight_mtx).sum()\n    score = (1 -(numerator\/denominator))\n    return score","82cfa96c":"def get_rescuer_ids(df):\n    \"\"\"Return RescuerID indices and count.\"\"\"\n    rescuer_ids = (df.reset_index(drop=True).reset_index().groupby(['RescuerID']).agg({\n        'index': {'unique': lambda x: x.unique().tolist(), 'count': lambda x: x.nunique()},\n        'AdoptionSpeed': {'mean': lambda x: x.mean()}\n    }))\n    rescuer_ids.columns = rescuer_ids.columns.droplevel(0)\n    rescuer_ids['bucket'] = (\n        #pd.cut(rescuer_ids['mean'], bins=[-1,1,1.5,2,2.5,3,5]).cat.codes.astype(str) +\n        '1' +\n        pd.cut(rescuer_ids['count'], bins=[-1,2,4,10000]).cat.codes.astype(str)\n    )\n    rescuer_ids['count'] = rescuer_ids['bucket']\n    return rescuer_ids","b31facc9":"def find_nearest(array, value):\n    \"\"\"Find the nearest value from given array.\"\"\"\n    array = np.asarray(array)\n    idx = (np.abs(array - value)).argmin()\n    return idx\n\ndef labelize_match_target(y_pred, y_train):\n    \"\"\"Match target distributions.\"\"\"\n    pdf_train = np.bincount(y_train)\n    pdf_train = pdf_train \/ pdf_train.sum()\n    cdf_train = np.cumsum(pdf_train)\n    \n    counts,bins = np.histogram(y_pred, bins=4000000)\n    cdf_pred = np.cumsum(counts \/ counts.sum())\n    #print(y_train,y_pred)\n    cutoffs = []\n    for cutpoint in cdf_train:\n        cutoff_idx = find_nearest(cdf_pred, cutpoint)\n        cutoff = bins[cutoff_idx]\n        cutoffs.append(cutoff)\n    cutoffs[-1] = 4\n    if VERBOSE:\n        logger.info('Target match cutoffs: %s' % cutoffs)\n    y_pred_rounded = np.zeros(y_pred.shape[0], dtype=int) + 4\n    min_cutoff = 0.\n    for label,cutoff in enumerate(cutoffs):\n        y_pred_rounded[(y_pred >= min_cutoff) & (y_pred <= cutoff)] = label\n        min_cutoff = cutoff\n    return y_pred_rounded","ee811bdb":"def plot_confusion_matrix(y_true, y_pred,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    classes = np.sort(y_true.unique())[::-1]\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    plt.show()\n    \n    acc_str = ''\n    accs = (cm.diagonal() \/ cm.sum(axis=1)) * 100\n    for i in range(len(accs)):\n        logger.info('Class %s accuracy: %.1f %%' % (i,accs[i]))\n    return accs","f87f741a":"def validate(X, y, rescuer_ids, n_cv=8, n_folds=5):\n    \"\"\"Validation framework.\"\"\"\n    df_preds = []\n    cv_scores = []\n    cv_accs = []\n    \n    global CV,FOLD\n        \n    for j in range(n_cv):\n        CV = j\n        skf = StratifiedKFold(n_folds, shuffle=True, random_state=SEED+j)\n        if USE_RESCUER_CV:\n            rescuer_idx = rescuer_ids['unique'].values\n            cv_gen = skf.split(rescuer_idx, rescuer_ids['count'].values)\n        else:\n            cv_gen = skf.split(X, y)\n        scores = []\n        accs = []\n        for i,(train_idx,test_idx) in enumerate(cv_gen):\n            FOLD = i\n            if USE_RESCUER_CV:\n                train_idx = np.array([idx for l in rescuer_idx[train_idx] for idx in l])\n                test_idx = np.array([idx for l in rescuer_idx[test_idx] for idx in l])\n\n            # Fit and predict\n            X_train,y_train = X.iloc[train_idx],y.iloc[train_idx]\n            X_test,y_test = X.iloc[test_idx],y.iloc[test_idx]\n                        \n            # All data\n            y_pred = run_all(X_train, y_train, X_test)         \n            score = qwk(y_pred, y_test)\n            scores.append(score)\n            \n            acc = [-1, -1, -1, -1, -1]\n            if VERBOSE:\n                print(classification_report(y_test, y_pred))\n                #print(confusion_matrix(y_test, y_pred))\n                acc = plot_confusion_matrix(y_test, y_pred)\n            accs.append(acc)\n            \n            logger.info('CV %d score: %.3f (running mean %.3f and std %.3f) - Running acc. %s' % \n                        (i+1, score, np.mean(scores), np.std(scores), np.around(np.array(accs).mean(axis=0), 1)))\n            \n            # Save results\n            df_pred = pd.DataFrame()\n            df_pred['Prediction'] = y_pred\n            df_pred['Run'] = j\n            df_pred['Fold'] = i\n            df_pred['PetID'] = train['PetID'].values[test_idx]\n            df_preds.append(df_pred)\n            \n        cv_scores.append(scores)\n        cv_accs.append(accs)\n        cv_means = np.mean(cv_scores, axis=1)\n        logger.info('> Running CV mean after %d runs: %.3f [%.3f - %.3f] (std %.3f) - Individual [%.3f - %.3f] - Accs: %s' %\n                    (j+1,\n                     np.mean(cv_means), np.min(cv_means), np.max(cv_means), np.std(cv_means), \n                     np.min(cv_scores), np.max(cv_scores), np.around(np.array(cv_accs).mean(axis=1).mean(axis=0), 1)))\n        \n    pd.concat(df_preds, axis=0).to_csv('df_preds.csv', index=False)\n    all_cv_mean = np.mean(cv_means)\n    return all_cv_mean","e372977f":"class PandasTransform(TransformerMixin, BaseEstimator):\n    \n    def __init__(self, fn):\n        self.fn = fn\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None, copy=None):\n        return self.fn(X)\n\n    \nclass PandasFeatureUnion(FeatureUnion):\n    \n    def fit_transform(self, X, y=None, **fit_params):\n        self._validate_transformers()\n        result = Parallel(n_jobs=self.n_jobs)(\n            delayed(_fit_transform_one)(\n                transformer=trans,\n                X=X.copy(),\n                y=y,\n                weight=weight,\n                **fit_params)\n            for name, trans, weight in self._iter())\n\n        if not result:\n            # All transformers are None\n            return np.zeros((X.shape[0], 0))\n        Xs, transformers = zip(*result)\n        self._update_transformer_list(transformers)\n        if any(sparse.issparse(f) for f in Xs):\n            Xs = sparse.hstack(Xs).tocsr()\n        else:\n            Xs = self.merge_dataframes_by_column(Xs)\n        return Xs\n\n    def merge_dataframes_by_column(self, Xs):\n        return pd.concat(Xs, axis=\"columns\", copy=False)\n\n    def transform(self, X):\n        Xs = Parallel(n_jobs=self.n_jobs)(\n            delayed(_transform_one)(\n                transformer=trans,\n                X=X.copy(),\n                y=None,\n                weight=weight)\n            for name, trans, weight in self._iter())\n        if not Xs:\n            # All transformers are None\n            return np.zeros((X.shape[0], 0))\n        if any(sparse.issparse(f) for f in Xs):\n            Xs = sparse.hstack(Xs).tocsr()\n        else:\n            Xs = self.merge_dataframes_by_column(Xs)\n        return Xs\n    \n    \nclass PandasPipeline(TransformerMixin, BaseEstimator):\n    \n    def __init__(self, steps=None):\n        self.steps = steps\n        \n        if self.steps is None:\n            self.steps = []\n        \n    def fit_transform(self, X, y=None, **fit_kwargs):\n        X = X.copy()\n        for name,trs in self.steps:\n            X = trs.fit_transform(X)\n        return X\n    \n    def transform(self, X):\n        X = X.copy()\n        for name,trs in self.steps:\n            X = trs.transform(X)\n        return X\n    \n    \nclass FilterCommonValues(TransformerMixin, BaseEstimator):\n    \n    def __init__(self, train, test, cols, lim=5, fill_val=-1):\n        self.train = train\n        self.test = test\n        self.cols = cols\n        self.lim = lim\n        self.fill_val = fill_val\n            \n    def transform(self, X):\n        for col in self.cols:\n            if col in X:\n                X.loc[~X[col].isin(self.keep_vals_[col]), col] = self.fill_val\n        return X\n    \n    def fit_transform(self, X, y=None, **fit_kwargs):\n        self.keep_vals_ = {}\n        for col in self.cols:\n            lim_vals = (self.train[col].value_counts() < self.lim).replace(False, np.nan).dropna().index.tolist()\n            common_vals = list(set(self.test[col].unique()).intersection(self.train[col].unique()))\n            self.keep_vals_[col] = list(set(lim_vals + common_vals))\n        return self.transform(X)\n    \n    \nclass SelectColumns(TransformerMixin, BaseEstimator):\n    \n    def __init__(self, items=None, like=None, regex=None):\n        self.items = items\n        self.like = like\n        self.regex = regex\n        \n    def fit_transform(self, X, y=None, **fit_params):\n        return self.transform(X)\n    \n    def transform(self, X):\n        X = X.filter(items=self.items, like=self.like,\n                        regex=self.regex, axis=1)\n        if X.shape[1] == 0:\n            logger.info('Cannot find anything with search %s, %s, %s' % (\n                  self.items,self.like,self.regex))\n        if VERBOSE > 1:\n            msg = X.columns.values if X.shape[1] < 50 else np.random.choice(X.columns.values.flatten(), size=50)\n            logger.info(f'Selected {X.shape[1]} columns (max 50 shown): {msg}')\n        return X\n    \n    \nclass DropColumns(TransformerMixin, BaseEstimator):\n    \n    def __init__(self, items=None, like=None, regex=None):\n        self.items = items\n        self.like = like\n        self.regex = regex\n        \n    def fit_transform(self, X, y=None, **fit_params):\n        return self.transform(X)\n    \n    def transform(self, X):\n        drop_cols = X.iloc[0:0].filter(items=self.items, like=self.like,\n                        regex=self.regex, axis=1).columns.tolist()\n        X = X.drop(drop_cols, axis=1)\n        if VERBOSE > 1:\n            msg = X.columns.values if X.shape[1] < 100 else X.shape[1]\n            logger.info(f'Dropped to {X.shape[1]} columns: {msg}')\n        return X\n    \n    \nclass ConvertType(TransformerMixin, BaseEstimator):\n    \n    def __init__(self, cols, dtype):\n        self.cols = cols\n        self.dtype = dtype\n        \n    def fit_transform(self, X, y=None, **fit_params):\n        return self.transform(X)\n    \n    def transform(self, X):\n        X[self.cols] = X[self.cols].astype(self.dtype)\n        return X\n    \n    \nclass SelectType(TransformerMixin, BaseEstimator):\n    \n    def __init__(self, include=None, exclude=None):\n        self.include = include\n        self.exclude = exclude\n        \n    def fit_transform(self, X, y=None, **fit_kwargs):\n        return self.transform(X)\n    \n    def transform(self, X):\n        return X.select_dtypes(include=self.include, exclude=self.exclude)","6b569ebf":"class StemmedTfidfVectorizer(TfidfVectorizer):\n    \n    def __init__(self, stemmer, *args, **kwargs):\n        super(StemmedTfidfVectorizer, self).__init__(*args, **kwargs)\n        self.stemmer = stemmer\n        \n    def build_analyzer(self):\n        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n        return lambda doc: (self.stemmer.stem(word) for word in analyzer(doc.replace('\\n', ' ')))\n\n    \ndef grouped_feature(X, gr_col, agg_cols, agg_funcs):\n    gr = X.groupby(gr_col)[agg_cols].agg(agg_funcs)\n    gr.columns = [l1+'_'+'_'.join(gr_col)+'_'+l2 for l1,l2 in zip(gr.columns.get_level_values(0),\n                                             gr.columns.get_level_values(1))]\n    gr = gr.reset_index()\n    X = X.merge(gr, how='left', on=gr_col)\n    return X[gr.columns.drop(gr_col)]\n\n\nclass StratifiedBagger(BaseEstimator,RegressorMixin):\n    \"\"\"Out-of-fold bagging with stratification.\"\"\"\n    \n    def __init__(self, clf=None, n_bags=10, random_state=None, labelize=False, early_stopping_rounds=None):\n        self.clf = clf\n        self.n_bags = n_bags\n        self.random_state = random_state if random_state is not None else SEED\n        self.labelize = labelize\n        self.early_stopping_rounds = early_stopping_rounds\n        \n    def fit(self, X, y=None, **fit_params):\n        skf = StratifiedKFold(self.n_bags, shuffle=True, random_state=self.random_state)\n        folds = skf.split(X, y)\n        self.clf_ = []\n        self.y_ = y\n        for i,(train_idx,valid_idx) in enumerate(folds):\n            if isinstance(X, pd.DataFrame):\n                (X_train,y_train,X_valid,y_valid) = (X.iloc[train_idx],y.iloc[train_idx],\n                                                     X.iloc[valid_idx],y.iloc[valid_idx])\n            else:\n                (X_train,y_train,X_valid,y_valid) = (X[train_idx],y.iloc[train_idx],\n                                                     X[valid_idx],y.iloc[valid_idx])\n            clf = deepcopy(self.clf)\n            self.clf_.append(clf)\n            if self.early_stopping_rounds is None:\n                self.clf_[i].fit(X_train, y_train, **fit_params)\n            else:\n                self.clf_[i].fit(X_train, y_train, eval_set=[(X_valid,y_valid)], \n                                 early_stopping_rounds=self.early_stopping_rounds, \n                                 verbose=False, **fit_params)\n                \n            if VERBOSE:\n                if isinstance(self.clf_[i], LGBMRegressor):\n                    fig,ax = plt.subplots(1, 1, figsize=(12,12))\n                    plot_importance(self.clf_[i], ax=ax)\n                    plt.show()\n                    \n                if isinstance(self.clf_[i], XGBRegressor):\n                    fig,ax = plt.subplots(1, 1, figsize=(12,12))\n                    xgb_plot_importance(self.clf_[i], ax=ax)\n                    plt.show()\n                    \n                if isinstance(self.clf_[i], CatBoostRegressor):\n                    # # Feature Importance\n                    fea_imp = pd.DataFrame({'imp': self.clf_[i].feature_importances_, \n                                            'col': X_train.columns})\n                    fea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False])\n                    fea_imp.plot(kind='barh', x='col', y='imp', figsize=(12,12))\n                    plt.show()\n        return self\n    \n    def predict(self, X, y=None):\n        y_preds = np.zeros((X.shape[0],self.n_bags))\n        for i in range(self.n_bags):\n            y_pred = minmax_scale(self.clf_[i].predict(X), (0,4))\n            if self.labelize:\n                y_pred = labelize_match_target(y_pred, self.y_)\n            y_preds[:,i] = y_pred\n        \n        if self.labelize:\n            y_preds = mode(y_preds.T)[0].T.flatten().astype(np.int32)\n        else:\n            y_preds = y_preds.mean(-1)\n        return y_preds\n    \n    \nclass OOFTransformer(TransformerMixin, BaseEstimator):\n    \"\"\"Make regression out-of-fold predictions.\"\"\"\n    \n    def __init__(self, clf, n_folds=5, agg_func='mean', name=None):\n        self.clf = clf\n        self.n_folds = n_folds\n        self.agg_func = agg_func\n        self.name = name\n        \n        if self.name is None:\n            self.name = str(self.clf.__class__).split('.')[-1][1:-2]\n        \n    def fit_transform(self, X, y, **fit_kwargs):\n        self.name_ = 'oof_' + self.name\n        self.clf_ = []\n        skf = StratifiedKFold(self.n_folds, shuffle=True, random_state=SEED+1234)\n        folds = skf.split(X, y)\n        X_train = np.zeros((X.shape[0],))\n        \n        if not SUBMISSION and not USE_LB:\n            path = f'{self.name_}_{CV}_{FOLD}_X_train.csv'\n            if os.path.exists(path) and os.path.exists(path.replace('X_train','X_test')):\n                if VERBOSE:\n                    logger.info(f'Read existing for {self.name_} (fit)')\n                return pd.read_csv(path)\n        \n        for i,(fold_idx,oof_idx) in enumerate(folds):\n            self.clf_.append(deepcopy(self.clf))\n            if VERBOSE:\n                logger.info(f'Generating {self.name_} oof {i+1}\/{self.n_folds}')\n            if isinstance(X, pd.DataFrame):\n                (X_fold,y_fold,X_oof,y_oof) = (X.iloc[fold_idx],y.iloc[fold_idx],\n                                               X.iloc[oof_idx],y.iloc[oof_idx])\n            else:\n                (X_fold,y_fold,X_oof,y_oof) = (X[fold_idx],y.iloc[fold_idx],\n                                               X[oof_idx],y.iloc[oof_idx])\n            self.clf_[i].fit(X_fold, y_fold)\n            X_train[oof_idx] = self.clf_[i].predict(X_oof)\n            \n            if VERBOSE:\n                if isinstance(self.clf_[i], LGBMRegressor):\n                    fig,ax = plt.subplots(1, 1, figsize=(18,18))\n                    plot_importance(self.clf_[i], ax=ax)\n                    plt.show()\n                    \n                if isinstance(self.clf_[i], XGBRegressor):\n                    fig,ax = plt.subplots(1, 1, figsize=(18,18))\n                    xgb_plot_importance(self.clf_[i], ax=ax)\n                    plt.show()\n                    \n                if isinstance(self.clf_[i], CatBoostRegressor):\n                    # # Feature Importance\n                    fea_imp = pd.DataFrame({'imp': self.clf_[i].feature_importances_, \n                                            'col': X_train.columns})\n                    fea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False]).iloc[-30:]\n                    fea_imp.plot(kind='barh', x='col', y='imp', figsize=(18,18))\n                    plt.show()\n                    \n        X_train = pd.DataFrame(X_train, columns=[self.name_])\n        \n        if not SUBMISSION and not USE_LB:\n            X_train.to_csv(path, index=False)\n        \n        return X_train\n    \n    def transform(self, X):\n        if not SUBMISSION and not USE_LB:\n            path = f'{self.name_}_{CV}_{FOLD}_X_test.csv'\n            if os.path.exists(path):\n                if VERBOSE:\n                    print(f'Read existing for {self.name_} (transform)')\n                return pd.read_csv(path)\n        \n        X_test = np.zeros((X.shape[0],self.n_folds))\n        for i in range(self.n_folds):\n            X_test[:,i] = self.clf_[i].predict(X)\n        if self.agg_func == 'mean':\n            X_test = X_test.mean(-1)\n        elif self.agg_func == 'median':\n            X_test = np.median(X_test, axis=-1)\n        else:\n            raise ValueError(f'Unknown aggregation {self.agg_func}')\n        X_test = pd.DataFrame(X_test, columns=[self.name_])\n        \n        if not SUBMISSION and not USE_LB:\n            X_test.to_csv(path, index=False)\n        \n        return X_test\n\n    \nclass PredictTransformer(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, clf, labelize=True, name=None):\n        self.clf = clf\n        self.labelize = labelize\n        self.name = name\n        \n        if self.name is None:\n            self.name = str(self.clf.__class__).split('.')[-1][1:-2]\n        \n    def fit(self, X, y, **fit_kwargs):\n        self.name_ = 'pred_' + self.name\n        \n        self.clf.fit(X, y, **fit_kwargs)\n        if VERBOSE:\n            print(f'Fitting {self.name_}...')\n            if isinstance(self.clf, LGBMRegressor):\n                fig,ax = plt.subplots(1, 1, figsize=(18,18))\n                plot_importance(self.clf, ax=ax)\n                plt.show()\n\n            if isinstance(self.clf, XGBRegressor):\n                fig,ax = plt.subplots(1, 1, figsize=(18,18))\n                xgb_plot_importance(self.clf, ax=ax)\n                plt.show()\n\n            if isinstance(self.clf, CatBoostRegressor):\n                # # Feature Importance\n                fea_imp = pd.DataFrame({'imp': self.clf.feature_importances_, \n                                        'col': X_train.columns})\n                fea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False]).iloc[-30:]\n                fea_imp.plot(kind='barh', x='col', y='imp', figsize=(18,18))\n                plt.show()\n        y_pred = self.clf.predict(X)\n        \n        if self.labelize:\n            self.y_ = y\n        return self\n    \n    def transform(self, X):\n        y_pred = self.clf.predict(X)\n        if self.labelize:\n            y_pred = labelize_match_target(y_pred, self.y_)\n        return pd.DataFrame(y_pred, columns=[self.name_])","4caa134c":"logger.info('Raw data...')","d46f230b":"breed = pd.read_csv('..\/input\/petfinder-adoption-prediction\/breed_labels.csv')\ncolor = pd.read_csv('..\/input\/petfinder-adoption-prediction\/color_labels.csv')\nstate = pd.read_csv('..\/input\/petfinder-adoption-prediction\/state_labels.csv')","b5b7f5b9":"train = pd.read_csv('..\/input\/petfinder-adoption-prediction\/train\/train.csv')\ntest = pd.read_csv('..\/input\/petfinder-adoption-prediction\/test\/test.csv')\nlogger.info('Train shape: %s' % str(train.shape))\nlogger.info('Test shape: %s' % str(test.shape))","80a74c34":"# State information\next_state = pd.read_excel('..\/input\/malaysian-state-data\/external_state_data.xlsx', \n                          decimal=',')[['State','Population2','2016 GDP']]\ntrain = train.merge(ext_state, how='left', on='State')\ntest = test.merge(ext_state, how='left', on='State')","82842259":"train['Description'] = train['Description'].fillna('').astype(str)\ntest['Description'] = test['Description'].fillna('').astype(str)","a40cf5f8":"logger.info('Metadata and sentiment features...')","6b3b9a88":"# Add vertex_y and label_score for profile picture (-1.jpg)\ndef create_metadata(pet_id, train=True):\n    all_res = {}\n    for pic in range(1,2):\n        meta_path = '..\/input\/petfinder-adoption-prediction\/%s_metadata\/%s-%s.json' % ('train' if train else 'test', pet_id,pic)\n        pic_path = '..\/input\/petfinder-adoption-prediction\/%s_images\/%s-%s.jpg' % ('train' if train else 'test', pet_id,pic)\n        res = {\n            'PetID': pet_id, \n            f'vertex_y{pic}': 0, \n            f'label_score{pic}': 0,\n        }\n        try:\n            if os.path.exists(meta_path):\n                with open(meta_path, 'r') as f:\n                    d = json.load(f)\n                res[f'vertex_y{pic}'] = d['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n                if 'labelAnnotations' in d: \n                    res[f'label_score{pic}'] = d['labelAnnotations'][0]['score'] + 1\n            all_res.update(res)\n        except:\n            logger.warning('Failed loading JSON files!')\n            pass\n    return all_res","2110410d":"logger.info('Creating metadata features...')\ntrain_metadata_feats = Parallel(n_jobs=4)(delayed(create_metadata)(pet_id)\n                                for pet_id in tqdm_notebook(train['PetID'].unique()))\ntrain_metadata_feats = pd.DataFrame(train_metadata_feats)\n\ncreate_metadata_test = partial(create_metadata, train=False)\ntest_metadata_feats = Parallel(n_jobs=4)(delayed(create_metadata_test)(pet_id)\n                                         for pet_id in tqdm_notebook(test['PetID'].unique()))\ntest_metadata_feats = pd.DataFrame(test_metadata_feats)\n        \nlogger.info('Merging metadata features...')\ntrain = train.merge(train_metadata_feats, how='left', on='PetID')\ntest = test.merge(test_metadata_feats, how='left', on='PetID')\n\ndel train_metadata_feats,test_metadata_feats\ngc.collect()\nlogger.info('Metadata features done!')","7d9692f2":"logger.info('Image features...')","2a755117":"def resize_to_square(im, image_size):\n    old_size = im.shape[:2] # old_size is in (height, width) format\n    ratio = float(image_size)\/max(old_size)\n    new_size = tuple([int(x*ratio) for x in old_size])\n    # new_size should be in (width, height) format\n    im = cv2.resize(im, (new_size[1], new_size[0]))\n    delta_w = image_size - new_size[1]\n    delta_h = image_size - new_size[0]\n    top, bottom = delta_h\/\/2, delta_h-(delta_h\/\/2)\n    left, right = delta_w\/\/2, delta_w-(delta_w\/\/2)\n    color = [0, 0, 0]\n    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n    return new_im\n\ndef normal(img, target_size):\n    img = resize_to_square(img, target_size).astype(np.float32)\n    return img\n\ndef load_image(path, target_size, backbone_preprocess_func, preprocess_func):\n    img = cv2.imread(path)\n    img = preprocess_func(img, target_size)\n    img = backbone_preprocess_func(img)\n    return img[None,:,:,:]\n\ndef image_generator(paths, target_size, backbone_preprocess_func, \n                    preprocess_func, batch_size):\n    n_batches = int(np.ceil(len(paths) \/ batch_size))\n    yield n_batches\n    for i in range(n_batches):\n        batch_paths = paths[i*batch_size:(i+1)*batch_size]\n        yield np.concatenate([load_image(path, target_size, backbone_preprocess_func, preprocess_func)\n                              for path in batch_paths], axis=0)\n    \ndef get_feature_extractor(backbone, target_size, pooling):\n    inp = Input((target_size,target_size,3))\n    \n    if backbone == 'xception':\n        from keras.applications.xception import Xception\n        backbone = Xception(weights=None, include_top=False, input_tensor=inp)\n        backbone.load_weights('..\/input\/model-weights\/xception_weights_tf_dim_ordering_tf_kernels_notop.h5')\n    elif backbone == 'nasnetlarge':\n        from keras.applications.nasnet import NASNetLarge\n        backbone = NASNetLarge(weights=None, include_top=False, input_tensor=inp)\n        backbone.load_weights('..\/input\/model-weights\/nasnet_large_no_top.h5')\n    else:\n        raise ValueError('Unknown backbone %s!' % backbone)        \n    x = backbone.output\n    if pooling == 'max':\n        x = GlobalMaxPooling2D()(x)\n    else:\n        x = GlobalAveragePooling2D()(x)\n    m = Model(inputs=[inp], outputs=[x])\n    return m\n\ndef create_features(preprocess_func, backbone, pooling, batch_size=128):\n    train_img_paths = glob.glob('..\/input\/petfinder-adoption-prediction\/train_images\/*-1.jpg')\n    test_img_paths = glob.glob('..\/input\/petfinder-adoption-prediction\/test_images\/*-1.jpg')\n    img_paths = train_img_paths + test_img_paths\n    \n    logger.info('Loading backbone %s...' % backbone)\n    if backbone == 'xception':\n        from keras.applications.xception import preprocess_input\n        target_size = 299\n    elif backbone == 'nasnetlarge':\n        from keras.applications.nasnet import preprocess_input\n        target_size = 331\n    else:\n        raise ValueError('Unknown backbone %s!' % backbone)\n    backbone_preprocess_func = preprocess_input\n    feature_extractor = get_feature_extractor(backbone, target_size, pooling)\n    \n    logger.info('Creating features...')\n    img_gen = image_generator(img_paths, target_size, backbone_preprocess_func,\n                              preprocess_func, batch_size)\n    n_batches = next(img_gen)\n    pet_ids = [e.split('\/')[-1].split('-')[0] for e in img_paths]\n    features = feature_extractor.predict_generator(\n        img_gen, max_queue_size=10, steps=n_batches, verbose=1).astype(np.float32)\n    logger.info('Features shape: %s' % str(features.shape))\n        \n    logger.info('Saving features...')\n    n_train = len(train_img_paths)\n    preprocess_func_name = preprocess_func.__name__\n    feature_name = f'img_{preprocess_func_name}_{backbone}_{pooling}'\n    colnames = [f'{feature_name}_{i+1}' for i in range(features.shape[1])]\n    train_features = (pd.DataFrame(features[:n_train], index=pet_ids[:n_train], columns=colnames)\n                      .reset_index().rename(columns={'index': 'PetID'}))\n    test_features = (pd.DataFrame(features[n_train:], index=pet_ids[n_train:], columns=colnames)\n                     .reset_index().rename(columns={'index': 'PetID'}))\n    \n    logger.info('Feature extraction done')\n    return train_features,test_features","28ae8c52":"logger.info('Creating image features...')\ntrain_img_feats,test_img_feats = create_features(normal, 'xception', 'avg')\n\ntmp_train,tmp_test = create_features(normal, 'nasnetlarge', 'avg')\ntrain_img_feats = train_img_feats.merge(tmp_train, how='left', on='PetID')\ntest_img_feats = test_img_feats.merge(tmp_test, how='left', on='PetID')\n        \nlogger.info('Merging image features...')\ntrain = train.merge(train_img_feats, how='left', on='PetID')\ntest = test.merge(test_img_feats, how='left', on='PetID')\n\ndel train_img_feats,test_img_feats\ngc.collect()\nlogger.info('Image features done!')","1239bead":"def row_features(X):\n    \"\"\"Row level features.\"\"\"\n    \n    # Adoption or adopted words found in text\n    X['adopted'] = 1\n    X.loc[X['Description'].str.lower().str.contains('adopted'),'adopted'] = 0\n    X.loc[X['Description'].str.lower().str.contains('adoption'),'adopted'] = 2\n        \n    # RescuerID has more than one pet\n    rescuer_counts = (\n        X[['RescuerID','PetID']]\n        .groupby('RescuerID').agg({'PetID': lambda x: (x.count() > 1).astype(np.int8)})\n        .rename(columns={'PetID': 'MoreThanOneRescuerPet'})\n        .reset_index()\n    )\n    X = X.merge(rescuer_counts, how='left', on='RescuerID')\n    return X[['adopted','MoreThanOneRescuerPet']]\n\n\nclass ExtractLeaky(TransformerMixin, BaseEstimator):\n    \"\"\"Leaky features that will not be calculated before validation splits.\"\"\"\n    \n    def __init__(self, train, test):\n        self.train = train\n        self.test = test\n        \n    def _ratio(self, gr1, gr2, agg_col='PetID', agg_func='count'):\n        req_cols = list(set(gr1 + gr2 + [agg_col]))\n        df = pd.concat([self.train[req_cols], self.test[req_cols]], axis=0)\n        df1 = df.groupby(gr1)[agg_col].agg(agg_func).to_frame('gr1').reset_index()\n        df2 = df.groupby(gr2)[agg_col].agg(agg_func).to_frame('gr2').reset_index()\n        df = df.merge(df1, how='left', on=gr1).merge(df2, how='left', on=gr2)\n\n        colname = '%s_%s_%s_%s' % (agg_col,agg_func,'-'.join(gr1),'-'.join(gr2))\n        df[colname] = (df['gr1'] \/ df['gr2']).replace([-np.inf,np.inf],np.nan)\n        df = df.drop(['gr1','gr2'], axis=1)\n\n        self.train_feats_ = pd.concat([\n            self.train_feats_, df[colname].iloc[:self.n_train_]], axis=1)\n        self.test_feats_ = pd.concat([\n            self.test_feats_, df[colname].iloc[self.n_train_:].reset_index(drop=True)], axis=1)\n    \n    def fit_transform(self, X, y=None, **fit_kwargs):\n        self.n_train_ = self.train.shape[0]\n        self.train_feats_ = pd.DataFrame()\n        self.test_feats_ = pd.DataFrame()\n        \n        # Ratio features, (leaky!)\n        self._ratio(['Breed1'], ['State'], 'PetID', 'count')\n        self._ratio(['Breed1','RescuerID'], ['PetID'], 'PetID', 'count')\n        self._ratio(['PetID'], ['Breed1'], 'PhotoAmt', 'mean')\n        self._ratio(['RescuerID'], ['PetID'], 'Breed1', 'nunique')\n        self.train_feats_['Breed1_nunique_RescuerID_PetID'] = (\n            self.train_feats_['Breed1_nunique_RescuerID_PetID'].clip(1, self.test_feats_['Breed1_nunique_RescuerID_PetID'].max()))\n        self.test_feats_['Breed1_nunique_RescuerID_PetID'] = (\n            self.test_feats_['Breed1_nunique_RescuerID_PetID'].clip(1, self.test_feats_['Breed1_nunique_RescuerID_PetID'].max()))\n        return self.train_feats_\n    \n    def transform(self, X):\n        return self.test_feats_","4a447abb":"target = 'AdoptionSpeed'\ncommon_cols = test.columns.intersection(train.columns).values.tolist()\ntrain = train[common_cols + [target]]\ntest = test[common_cols]","4930ec89":"rescuer_ids = get_rescuer_ids(train)","0bf59eed":"# Take \"leaderboard\" out, if needed\nif LB:\n    train_idx,test_idx = train_test_split(rescuer_ids['unique'], \n        stratify=rescuer_ids['count'], test_size=0.2, random_state=209321206+73)\n    train_idx = [idx for sidx in train_idx for idx in sidx]\n    test_idx = [idx for sidx in test_idx for idx in sidx]\n    X_train,y_train,X_test,y_test = (train.iloc[train_idx].drop('AdoptionSpeed', axis=1),\n                                     train.iloc[train_idx][target],\n                                     train.iloc[test_idx].drop('AdoptionSpeed', axis=1),\n                                     train.iloc[test_idx][target])\n    rescuer_ids = get_rescuer_ids(train.iloc[train_idx]) # Update for these indices\nelse:\n    X_train,y_train,X_test = train.drop('AdoptionSpeed', axis=1),train[target],test","0c32a9e2":"def run_level0(X_train, y_train, X_test):\n    text_feats = PandasFeatureUnion([\n        ('Original', PandasTransform(lambda x: x)),\n        ('Raw description + TFIDF + SVD10', PandasPipeline([\n            ('select', SelectColumns(['Description'])),\n            ('preprocess', PandasTransform(lambda x: x.values.flatten().tolist())),\n            ('tfidf', TfidfVectorizer(min_df=2, max_features=None, \n                strip_accents='unicode', analyzer='word', token_pattern=r'(?u)\\b\\w+\\b', \n                 ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1)),\n            ('svd', TruncatedSVD(n_components=10, random_state=1337)),\n            ('df', PandasTransform(lambda x: \n                pd.DataFrame(x, columns=[f'lvl0_text_raw_desc_tfidf_svd10_{i}' for i in range(10)])))\n        ])),\n        ('Stemmed description + TFIDF + SVD5', PandasPipeline([\n            ('select', SelectColumns(['Description'])),\n            ('preprocess', PandasTransform(lambda x: x.fillna('').values.flatten().tolist())),\n            ('tfidf', StemmedTfidfVectorizer(SnowballStemmer('english', ignore_stopwords=False),\n                min_df=2, max_features=None, strip_accents='unicode', analyzer='word', \n                token_pattern=r'(?u)\\b\\w+\\b', ngram_range=(1, 3), use_idf=1, smooth_idf=1, \n                sublinear_tf=1)),\n            ('svd', TruncatedSVD(n_components=5, random_state=1337)),\n            ('df', PandasTransform(lambda x: \n                pd.DataFrame(x, columns=[f'lvl0_text_stem_desc_tfidf_svd5_{i}' for i in range(5)])))\n        ]))\n    ], n_jobs=1)\n    X_train = text_feats.fit_transform(X_train, y_train)\n    X_test = text_feats.transform(X_test)\n    \n    img_feats = PandasFeatureUnion([\n        ('Original', PandasTransform(lambda x: x)),\n        ('NasNet + PCA5', PandasPipeline([\n            ('select cols', SelectColumns(regex=r'(nasnetlarge_)')),\n            ('impute', SimpleImputer(fill_value=0.)),\n            ('var_thres', VarianceThreshold(0.)),\n            ('pca', PCA(5, random_state=234)),\n            ('df', PandasTransform(lambda x: \n                pd.DataFrame(x, columns=[f'lvl0_img_nasnet_pca5_{i}' for i in range(5)])))\n        ])),\n        ('NasNet + PCA400 + LR', OOFTransformer(make_pipeline(\n            SelectColumns(regex=r'(nasnetlarge_)'),\n            SimpleImputer(fill_value=0.),\n            VarianceThreshold(0.),\n            PCA(400, random_state=234),\n            LinearRegression(\n            )\n        ), name='lvl0_img_nasnet_pca400_lr')),\n        ('Xception + SVD150 + ET', OOFTransformer(make_pipeline(\n            SelectColumns(regex=r'(xception_)'),\n            SimpleImputer(fill_value=0.),\n            VarianceThreshold(0.),\n            TruncatedSVD(150, random_state=113120),\n            ExtraTreesRegressor(\n                n_estimators=200,\n                random_state=10233\n            )\n        ), name='lvl0_img_xception_svd150_pca400_lr')),\n    ], n_jobs=1)\n    X_train = img_feats.fit_transform(X_train, y_train)\n    X_test = img_feats.transform(X_test)\n    \n    if SUBMISSION or VERBOSE:\n        display(X_train.filter(like='oof_lvl0').corr())\n        display(X_test.filter(like='oof_lvl0').corr())\n    if SUBMISSION:\n        X_test.filter(like='oof_lvl0').to_csv('X_test_lvl0.csv', index=False)\n    return X_train,X_test\n\ndef run_level1(X_train, y_train, X_test):\n    normal_feats = [\n        'Type', 'Age', 'Breed1', 'Breed2', 'Gender',\n        'Color1', 'Color2', 'Color3', 'MaturitySize', \n        'FurLength', 'Vaccinated', 'Dewormed', 'Sterilized',\n        'Health', 'Quantity', 'Fee', 'State', 'VideoAmt', 'PhotoAmt',\n        'Population2', '2016 GDP', 'adopted', 'MoreThanOneRescuerPet',\n    ]\n    lvl1 = PandasFeatureUnion([\n        ('Original', PandasTransform(lambda x: x)),\n        ('LGB + Non-image', OOFTransformer(make_pipeline(\n            SelectColumns(normal_feats),\n            LGBMRegressor(\n                n_estimators=100,\n                num_leaves=16,\n                learning_rate=0.1,\n                max_depth=5,\n                colsample_bytree=0.5,\n                min_split_gain=0.02,\n                min_child_samples=100,\n                min_child_weight=0.02,\n                reg_lambda=0.01,\n                random_state=SEED+374,\n                objective='rmse'\n                #verbose=1\n            )\n        ), name='lvl1_lgb_non_img')),\n        ('LGB + Image and text', OOFTransformer(make_pipeline(\n            SelectColumns(regex=r'(lvl0_text|lvl0_img|PhotoAmt|label_score|vertex_y)'),\n            LGBMRegressor(\n                n_estimators=100,\n                num_leaves=16,\n                learning_rate=0.1,\n                max_depth=5,\n                colsample_bytree=0.5,\n                min_split_gain=0.02,\n                min_child_samples=100,\n                min_child_weight=0.02,\n                reg_lambda=0.01,\n                random_state=SEED+274,\n                objective='rmse'\n                #verbose=1\n            )\n        ), name='lvl1_lgb_img_text')),\n        ('LGB + All features bagging', OOFTransformer(make_pipeline(\n            SelectColumns(regex=r'(lvl0|label_score|vertex_y|%s)' % '|'.join(normal_feats)),\n            BaggingRegressor(\n                LGBMRegressor(\n                    n_estimators=100,\n                    num_leaves=16,\n                    learning_rate=0.1,\n                    max_depth=5,\n                    colsample_bytree=0.5,\n                    min_split_gain=0.02,\n                    min_child_samples=100,\n                    min_child_weight=0.02,\n                    reg_lambda=0.01,\n                    random_state=SEED+743,\n                    objective='rmse'\n                    #verbose=1\n                ),\n                n_estimators=20,\n                max_samples=0.8,\n                max_features=0.4,\n                bootstrap=True,\n                random_state=9843\n            )\n        ), name='lvl1_lgb_bagging'))\n    ], n_jobs=1)\n    X_train = lvl1.fit_transform(X_train, y_train)\n    X_test = lvl1.transform(X_test)\n    \n    lvl1_rescuer_oof_mean = PandasFeatureUnion([\n        ('original', PandasTransform(lambda x: x)),\n        ('rescuermean', PandasTransform(lambda x: \n            grouped_feature(x, ['RescuerID'], ['oof_lvl1_lgb_bagging'], ['mean'])))\n    ])\n    X_train = lvl1_rescuer_oof_mean.fit_transform(X_train)\n    X_test = lvl1_rescuer_oof_mean.transform(X_test)\n    \n    if SUBMISSION or VERBOSE:\n        display(X_train.filter(regex='(oof_lvl1_|pred_)').corr())\n        display(X_test.filter(regex='(oof_lvl1_|pred_)').corr())\n    if SUBMISSION:\n        X_test.filter(regex='(oof_lvl1_|pred_)').to_csv('X_test_lvl1.csv', index=False)\n    return X_train,X_test\n\ndef run_level2(X_train, y_train, X_test):\n    normal_feats = [\n        'Type', 'Age', 'Breed1', 'Breed2', 'Gender',\n        'Color1', 'Color2', 'Color3', 'MaturitySize', \n        'FurLength', 'Vaccinated', 'Dewormed', 'Sterilized',\n        'Health', 'Quantity', 'Fee', 'State', 'VideoAmt', 'PhotoAmt',\n        'Population2', '2016 GDP', 'adopted', 'MoreThanOneRescuerPet',\n    ]\n    lvl2 = PandasFeatureUnion([\n        ('Best LGB', PredictTransformer(make_pipeline(\n            SelectColumns(regex=r'(lvl0|lvl1|label_score|vertex_y|%s)' % '|'.join(normal_feats)),\n            LGBMRegressor(\n                n_estimators=100,\n                num_leaves=16,\n                learning_rate=0.1,\n                max_depth=5,\n                colsample_bytree=0.5,\n                min_split_gain=0.02,\n                min_child_samples=100,\n                min_child_weight=0.02,\n                reg_lambda=0.01,\n                random_state=SEED+74,\n                objective='rmse'\n                #verbose=1\n            )\n        ), name='lvl2_lgb')),\n        ('Best CatBoost', PredictTransformer(make_pipeline(\n            SelectColumns(regex=r'(lvl0|lvl1|label_score|vertex_y|%s)' % '|'.join(normal_feats)),\n            CatBoostRegressor(\n                n_estimators=250,\n                learning_rate=0.1,\n                max_depth=6,\n                colsample_bylevel=0.5,\n                l2_leaf_reg=1.,\n                random_state=SEED+1932,\n                verbose=-1\n            )\n        ), name='lvl2_catboost')),\n        ('Best Ridge', PredictTransformer(make_pipeline(\n            SelectColumns(regex=r'(oof_lvl1_lgb)'),\n            DropColumns(regex=r'(RescuerID)'),\n            BaggingRegressor(\n                BayesianRidge(\n                    normalize=False,\n                    fit_intercept=True\n                ),\n                n_estimators=100,\n                max_samples=0.8,\n                max_features=1.0,\n                bootstrap=True,\n                random_state=93843\n            )\n        ), name='lvl2_ridge'))\n    ], n_jobs=1)\n    X_train = lvl2.fit_transform(X_train, y_train)\n    X_test = lvl2.transform(X_test)\n    \n    if SUBMISSION or VERBOSE:\n        display(X_train.filter(regex='(oof_lvl2_|pred_)').corr())\n        display(X_test.filter(regex='(oof_lvl2_|pred_)').corr())\n    if SUBMISSION:\n        X_test.filter(regex='(oof_lvl2_|pred_)').to_csv('X_test_lvl2.csv', index=False)\n    return X_train,X_test\n\ndef run_level3(X_train, y_train, X_test):\n    y_pred = mode(X_test.values.T)[0].T.flatten().astype(np.int32)\n    return y_pred\n\ndef run_all(X_train, y_train, X_test):\n    X_train = X_train.reset_index(drop=True).copy()\n    X_test = X_test.reset_index(drop=True).copy()\n    \n    # Common preprocessing\n    if VERBOSE or SUBMISSION:\n        logger.info('>> Fitting basic pipeline...')\n    pl = PandasPipeline([\n        ('extract_features', PandasFeatureUnion([\n            ('Original features', PandasTransform(lambda x: x)),\n            ('Row features', PandasTransform(row_features)),\n            ('Leaky features', ExtractLeaky(X_train, X_test))\n        ])),\n        ('Common values', FilterCommonValues(X_train, X_test, ['Breed1','Breed2','State'])),\n        ('Categoricals', ConvertType([\n            'Type','Breed1','Breed2','Gender','Color1','Color2','Color3','MaturitySize',\n            'FurLength','Vaccinated','Dewormed','Sterilized','Health','State','adopted'], \n            'category'))\n    ])\n    X_train = pl.fit_transform(X_train)\n    X_test = pl.transform(X_test)\n\n    use_feats = (X_train.iloc[0:0].select_dtypes(include=['category','number'])\n                 .columns.tolist() + ['RescuerID','Description'])\n    X_train,X_test = X_train[use_feats],X_test[use_feats]\n    \n    if VERBOSE or SUBMISSION:\n        logger.info('>> Level 0...')\n    X_train,X_test = run_level0(X_train, y_train, X_test)\n    \n    if VERBOSE:\n        logger.info('>> Level 1...')\n    X_train,X_test = run_level1(X_train, y_train, X_test)\n    \n    if VERBOSE:\n        logger.info('>> Level 2...')\n    X_train,X_test = run_level2(X_train, y_train, X_test)\n    \n    if VERBOSE:\n        logger.info('>> Level 3...')\n    y_pred = run_level3(X_train, y_train, X_test)\n    if VERBOSE or SUBMISSION:\n        display(y_pred)\n    return y_pred\n    \nUSE_LB = False\n    \nif not SUBMISSION:\n    validate(X_train, y_train, rescuer_ids)\n    \nif LB:\n    USE_LB = True\n    y_pred = run_all(X_train, y_train, X_test)\n    msg = 'LB score: %.3f' % qwk(y_pred, y_test)\n    logger.info(msg)\n    \nif SUBMISSION and not LB:\n    y_pred = pd.DataFrame(run_all(X_train, y_train, X_test),\n                          columns=['AdoptionSpeed'], \n                          index=test['PetID']).reset_index()\n    y_pred.to_csv('submission.csv', index=False)\n    \n    fig,ax = plt.subplots(1,2,figsize=(16,4))\n    y_pred['AdoptionSpeed'].hist(ax=ax[0])\n    train['AdoptionSpeed'].hist(ax=ax[1])\n    ax[0].set_title('Prediction distribution')\n    ax[1].set_title('Train set distribution')\nelse:\n    logger.info('Will not make submission without full dataset, turn LB off!')","66e24488":"# Features - Image","f919a66f":"# Features - Custom","3d9680b5":"# Features - Finishing","5eb44cb9":"# Models","1ae60309":"# Imports","7c88217a":"# Features - Metadata and sentiment","51f62e2c":"---","eb7c12ad":"# Features - Raw data","95b89b7e":"# Helper functions"}}