{"cell_type":{"191b87f5":"code","c5e9dfa1":"code","7d98c9ab":"code","d68fa9ed":"code","633774e1":"code","ee49c9d6":"code","b2359956":"code","d395c273":"code","f6024b79":"code","5164e37d":"code","0d5d11be":"code","e858a1c0":"code","b8ca8556":"code","c5972d34":"code","ae244b76":"code","7ea7539d":"code","7ef0fd38":"code","3fbcffdd":"code","6cd83d21":"code","b941bcdf":"code","1bd5bf42":"code","99aee3a2":"code","45419a5b":"code","476cb88f":"code","883b4167":"code","ab96920b":"code","de539367":"code","83f963ff":"code","76bea45f":"code","122501ae":"code","74f7ba5b":"code","a9454fd7":"code","26a96f97":"code","011dc70b":"code","6ad84825":"code","a3ce4e15":"code","09984a93":"markdown","f8222ae6":"markdown","9fc76f59":"markdown","4e9bd26f":"markdown","c9850fd9":"markdown","62c0bcb3":"markdown","264699d4":"markdown","fac9667c":"markdown","f4e8b3f9":"markdown","a481bb34":"markdown","ba6caeb0":"markdown","9c8e3445":"markdown","c3fb4877":"markdown","431b1d98":"markdown","9b5a9bbd":"markdown","81d4f31e":"markdown","2dfb8de1":"markdown","836ed20c":"markdown"},"source":{"191b87f5":"import numpy as np                                 #importing the standard library for scientific calculation\nimport seaborn as sns                              #importing interative plotting library\nimport matplotlib.pyplot as plt                    #importing a plotting library\nimport pandas as pd                                #importing this library to read and store files\nfrom sklearn.model_selection import train_test_split  #this is used to split the data into training and test set\nfrom sklearn.metrics import classification_report, confusion_matrix   #These are some of the testing metrics we would be using in the course of our program","c5e9dfa1":"def sigmoid(Z):                                   #defining the sigmoid function that we would use in the later parts of the code\n    \"\"\"\n    Implements the sigmoid activation in numpy\n    \n    Arguments:\n    Z -- numpy array of any shape\n    \n    Returns:\n    A -- output of sigmoid(z), same shape as Z\n    cache -- returns Z as well, useful during backpropagation\n    \"\"\"\n    \n    A = 1\/(1+np.exp(-Z))                          #This is the definition of sigmoid function\n    cache = Z                                     #Storing some values in cache which could be used in back propagation later\n    \n    return A, cache                               #Returning the cache and the activation A\n\ndef relu(Z):                                      #Defining relu function that is used in various layers\n    \"\"\"\n    Implement the RELU function.\n\n    Arguments:\n    Z -- Output of the linear layer, of any shape\n\n    Returns:\n    A -- Post-activation parameter, of the same shape as Z\n    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n    \"\"\"\n    \n    A = np.maximum(0,Z)                         #Definition of relu\n    \n    assert(A.shape == Z.shape)                  #Ensuring that the shape stays consistent\n    \n    cache = Z                                   #storing the value Z in cache which would later be used in deep neural networks\n    return A, cache                             #returning the value of cache and activation\n\n\ndef relu_backward(dA, cache):                   #This is used for taking the derivatives of relu while using back propagation\n    \"\"\"\n    Implement the backward propagation for a single RELU unit.\n\n    Arguments:\n    dA -- post-activation gradient, of any shape\n    cache -- 'Z' where we store for computing backward propagation efficiently\n\n    Returns:\n    dZ -- Gradient of the cost with respect to Z\n    \"\"\"\n    \n    Z = cache                                  #Storing the values in cache\n    dZ = np.array(dA, copy=True) # just converting dz to a correct object. \n    \n    # When z <= 0, you should set dz to 0 as well. \n    dZ[Z <= 0] = 0\n    \n    assert (dZ.shape == Z.shape)\n    \n    return dZ\n\ndef sigmoid_backward(dA, cache):      #This is used for sigmoid backward function and in backpropagation\n    \"\"\"\n    Implement the backward propagation for a single SIGMOID unit.\n\n    Arguments:\n    dA -- post-activation gradient, of any shape\n    cache -- 'Z' where we store for computing backward propagation efficiently\n\n    Returns:\n    dZ -- Gradient of the cost with respect to Z\n    \"\"\"\n    \n    Z = cache \n    \n    s = 1\/(1+np.exp(-Z))\n    dZ = dA * s * (1-s)                 #This is the derivative which would later be returned\n    \n    assert (dZ.shape == Z.shape)        #Ensuring that the shape stays consistent\n    \n    return dZ\n\n","7d98c9ab":"df = pd.read_csv('..\/input\/Admission_Predict_Ver1.1.csv')","d68fa9ed":"df.head()","633774e1":"df.columns","ee49c9d6":"df['University Rating'].value_counts()","b2359956":"df['SOP'].value_counts()","d395c273":"sns.color_palette(\"Paired\")\nsns.distplot(df['SOP'], color = 'brown')","f6024b79":"df['LOR '].value_counts()","5164e37d":"sns.distplot(df['LOR '], color = 'darkorange')","0d5d11be":"df.tail()","e858a1c0":"sns.barplot(x = 'Research', y = 'GRE Score', data = df)","b8ca8556":"sns.jointplot(x = 'TOEFL Score', y = 'GRE Score', color = 'darkblue', data = df)","c5972d34":"sns.jointplot(x = 'TOEFL Score', y = 'GRE Score', kind = 'kde', data = df, color = 'pink')","ae244b76":"sns.jointplot(x = 'CGPA', y = 'GRE Score', kind = 'hex', data = df)","7ea7539d":"sns.jointplot(x = 'TOEFL Score', y = 'GRE Score', kind = 'reg', data = df, color = 'g')","7ef0fd38":"sns.jointplot(x = 'SOP', y = 'LOR ', kind = 'kde', color = 'g', data = df)","3fbcffdd":"sns.jointplot(x = 'SOP', y = 'LOR ', kind = 'hex', color = 'r', data = df)","6cd83d21":"df.columns","b941bcdf":"X = df.drop(['Chance of Admit ', 'Serial No.'], axis = 1) #here we would drop the columns that are not necessary for the input such as the serial number and chance of admit\ny = df['Chance of Admit ']                                #we would just require the ouput to be Chance of Admit\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 101)  #we are dividing the input along with the output into training and testing set\nX_train = X_train.T                                  #We take the transpose of the training set for simplicity\nX_test = X_test.T                                    #We take the transpose of the testing set for simplicity\ny_train = y_train[:, np.newaxis]                     #We want to convert it into a n-dimensional vector\ny_test = y_test[:, np.newaxis]                       #We convert it to n-dimensional vector\ny_train = y_train.T                                  #We take the transpose for simplicity\ny_test = y_test.T                                    #We take the transpose for simplicity\ny = y[:, np.newaxis]\nX = X.T                                              #We take the transpose for simplicity\ny = y.T                                              #We take the transpose for simplicity\nprint('the shape of the input is {}'.format(X.shape))    #here we would print the shape of the input\nprint('the shape of the output is {}'.format(y.shape))   #printing the shape of the output\nprint('the shape of the input training set is {}'.format(X_train.shape))  #printing the shape of input training set\nprint('the shape of the output training set is {}'.format(y_train.shape)) #printing the shape of output training set\nprint('the shape of the input training set is {}'.format(X_test.shape))   #printing the shape of input testing set\nprint('the shape of the output training set is {}'.format(y_test.shape))  #printing the shape of output testing set","1bd5bf42":"def initialize_parameters(n_x, n_h, n_y):     #this is a function used to initialize the weights and biases\n    w1 = np.random.randn(n_h, n_x) * 0.01     #we use xavier initialization in this process \n    b1 = np.zeros((n_h, 1))                   #we create an array of zeroes\n    w2 = np.random.randn(n_y, n_h) * 0.01     #we use xavier initialization in this process\n    b2 = np.zeros((n_y, 1))                   #we create an array of zeroes\n    parameters = {\"w1\": w1, \"b1\": b1, \"w2\": w2, \"b2\": b2}  #we load these parameters into a dictionary so that they can be used later\n    return parameters                         #we return the parameters to the function ","99aee3a2":"def initialize_parameters_deep(layer_dims):   #This function is used to create weights and biases for all the L layers in the neural network\n    L = len(layer_dims)                       #We take the dimensions of the input in the function\n    parameters = {}                           #We create an empty dictionary so that it could be accessed later in the function\n    for l in range(1, L):                     #We create a for loop to iterate through all the layers\n        parameters['w' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01  #We use xavier initialization for all the L-layers\n        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))                              #We create zeroes for the L-layer network\n    return parameters                         #We return the parameters","45419a5b":"def linear_forward(a, w, b):  #We propagate forward through the network in this function\n    z = np.dot(w, a) + b      #We take the dot product between the weight and the activation and add a bias to it\n    cache = (a, w, b)         #We store all these values in cache so that they could be accessed later\n    return z, cache           #We return these parameters to the function","476cb88f":"def linear_activation_forward(a_prev, w, b, activation): #We create this function to calculate both the linear part and also the activation parts in the network\n    if activation == \"sigmoid\":                          #If the activation is sigmiod, we process the information using sigmoid\n        z, linear_cache = linear_forward(a_prev, w, b)   #We save the ouput in z and linear_cache\n        a, activation_cache = sigmoid(z)                 #We then apply the sigmoid to z to produce activation a\n    elif activation == \"relu\":                           #If the activation is relu, we process the information using relu\n        z, linear_cache = linear_forward(a_prev, w, b)   #We save the ouput in z and linear_cache\n        a, activation_cache = relu(z)                    #We then apply the relu to z to produce activation a\n    cache = (linear_cache, activation_cache)             #We save both the linear cache and activation cache in cache that could later be used\n    return a, cache                                      #We return both the activation and the cache","883b4167":"def L_model_forward(X, parameters):                     #We create a model for the forward propagation of the network. \n    caches = []                                         #We define a new list to store the values \n    a = X                                               #The given value of X is taken as a\n    L = len(parameters) \/\/ 2                          \n    for l in range(1, L):\n        a_prev = a\n        a, cache = linear_activation_forward(a_prev, parameters['w' + str(l)], parameters['b' + str(l)], \"relu\")\n        caches.append(cache)\n    al, cache = linear_activation_forward(a, parameters['w' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n    caches.append(cache)\n    return al, caches","ab96920b":"def compute_cost(a, y):\n    m = y.shape[1]\n    cost = -(1 \/ m) * np.sum(y * np.log(a) + (1 - y) * np.log(1 - a))\n    cost = np.squeeze(cost)\n    return cost","de539367":"def linear_backward(dz, cache):\n    a_prev, w, b = cache\n    m = a_prev.shape[1]\n    dw = (1 \/ m) * np.dot(dz, a_prev.T)\n    db = (1 \/ m) * np.sum(dz, axis = 1, keepdims = True)\n    da_prev = np.dot(w.T, dz)\n    return da_prev, dw, db","83f963ff":"def linear_activation_backward(da, cache, activation):\n    linear_cache, activation_cache = cache\n    if activation == \"relu\":\n        dz = relu_backward(da, activation_cache)\n        da_prev, dw, db = linear_backward(dz, linear_cache)\n    elif activation == \"sigmoid\":\n        dz = sigmoid_backward(da, activation_cache)\n        da_prev, dw, db = linear_backward(dz, linear_cache)\n    return da_prev, dw, db\n        ","76bea45f":"def L_model_backward(al, y, caches):\n    grads = {}\n    L = len(caches)\n    m = al.shape[1]\n    y = y.reshape(al.shape)\n    dal = - (np.divide(y, al) - np.divide(1 - y, 1 - al))\n    current_cache = caches[L - 1]\n    grads[\"da\" + str(L-1)], grads[\"dw\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dal, current_cache, activation = \"sigmoid\")\n    for l in reversed(range(L - 1)):\n        current_cache = caches[l]\n        da_prev_temp, dw_temp, db_temp = linear_activation_backward(grads[\"da\" + str(l + 1)], current_cache, activation = \"relu\")\n        grads[\"da\" + str(l)] = da_prev_temp\n        grads[\"dw\" + str(l + 1)] = dw_temp\n        grads[\"db\" + str(l + 1)] = db_temp\n    return grads\n    \n    \n    ","122501ae":"def update_parameters(parameters, grads, learning_rate):\n    L = len(parameters) \/\/ 2\n    for l in range(L):\n        parameters[\"w\" + str(l + 1)] = parameters[\"w\" + str(l + 1)] - learning_rate * grads[\"dw\" + str(l + 1)]\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n    return parameters     ","74f7ba5b":"def two_layer_model(X, y, layers_dims, learning_rate = 0.001, num_iterations = 10000, print_cost = False):\n    grads = {}\n    costs = []\n    m = X.shape[1]\n    (n_x, n_h, n_y) = layers_dims\n    parameters = initialize_parameters(n_x, n_h, n_y)\n    w1 = parameters[\"w1\"]\n    b1 = parameters[\"b1\"]\n    w2 = parameters[\"w2\"]\n    b2 = parameters[\"b2\"]\n    for i in range(0, num_iterations):\n        a1, cache1 = linear_activation_forward(X, w1, b1, activation = \"relu\")\n        a2, cache2 = linear_activation_forward(a1, w2, b2, activation = \"sigmoid\")\n        cost = compute_cost(a2, y)\n        da2 = - (np.divide(y, a2) - np.divide(1 - y, 1 - a2))\n        da1, dw2, db2 = linear_activation_backward(da2, cache2, activation = \"sigmoid\")\n        da0, dw1, db1 = linear_activation_backward(da1, cache1, activation = \"relu\")\n        grads[\"dw1\"] = dw1\n        grads[\"db1\"] = db1\n        grads[\"dw2\"] = dw2\n        grads[\"db2\"] = db2\n        parameters = update_parameters(parameters, grads, learning_rate)\n        w1 = parameters[\"w1\"]\n        b1 = parameters[\"b1\"]\n        w2 = parameters[\"w2\"]\n        b2 = parameters[\"b2\"]\n        if print_cost and i % 1000 == 0:\n            print(\"cost after iteration {}:: {}\".format(i, np.squeeze(cost)))\n        if print_cost and i % 1000 == 0:\n            costs.append(cost)\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per tens)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    return parameters    ","a9454fd7":"layers_dims = [X.shape[0], 50, 25, 1]","26a96f97":"def L_layer_model(X, y, layers_dims, learning_rate = 0.03, num_iterations = 3000, print_cost = False):\n    costs = []\n    parameters = initialize_parameters_deep(layers_dims)\n    for i in range(0, num_iterations):\n        al, caches = L_model_forward(X, parameters)\n        cost = compute_cost(al, y)\n        grads = L_model_backward(al, y, caches)\n        parameters = update_parameters(parameters, grads, learning_rate)\n        if print_cost and i % 1000 == 0:\n            print(\"Cost after iteration {}: {}\".format(i, cost))\n        if print_cost and i % 1000 == 0:\n            costs.append(cost)\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per tens)')\n    plt.title(\"Learning rate = \" + str(learning_rate))\n    plt.show()\n    return parameters","011dc70b":"parameters = L_layer_model(X_train, y_train, layers_dims,learning_rate = 0.05, num_iterations = 60000, print_cost = True)","6ad84825":"def predict(parameters, X):\n    a2, cache = L_model_forward(X, parameters)\n    predictions = a2\n    return predictions","a3ce4e15":"predictions_test = predict(parameters, X_test)\npredictions_train = predict(parameters, X_train)\nprint('The accuracy of the training model: {}%'.format((1 - np.sum(np.abs(predictions_train - y_train))\/predictions_train.shape[1]) * 100))\nprint('The accuracy of the testing model: {}%'.format((1 - np.sum(np.abs(predictions_test - y_test))\/predictions_test.shape[1]) * 100))","09984a93":"This just gives a representation of the last five rows of the dataset.","f8222ae6":"We would be counting the number of students with various university ratings i.e. the ratings in which the students have pursued their studies. By looking at the result below, we can conclude that very few students have actually pursued their education in top - tier universities. A whole lot of students have their education in tier - 3 universities. ","9fc76f59":"Having a look at various columns that are necessary and understanding the dataset better.","4e9bd26f":"Finally, I've created a neural network model that would likely predict the chances of admission of a person based on some of the metrics like the** GRE Score, TOEFL Score, CGPA, University Rating and so on**. The model would predict it with **93 percent (approx) accuracy**. If you want to know your chances of admission, just use the \"predict\" function and input some of the metrics that are necessary for the algorithm to predict your chances of admission. Based on that, you could get a true and almost accurate estimate of whether you would be admitted in the university or not!!!","c9850fd9":"We would also plot a barplot to understand the relationship between the data. We take on the x-axis 'Research' column while on the y-axis 'GRE Score' to see the spread. All of these are drawn from seaborn (an interactive library to give plots).\n\nWe see from the plot that those who did research during their undergraduation were actually able to score above 300 on the GRE as compared to those who didn't do research. This would slighty give us an idea that those who do research would generally be able to score well on the GRE (in most of the cases but not all). ","62c0bcb3":"We would also use joint plot from seaborn to look at the visualization. It clearly shows that as the 'GRE Score' increases, there is also an increase in the 'TOEFL Score'. There might be some outliers like a person scoring a 290 on the GRE but getting a 105 (approx) on the TOEFL. But those exceptions are rare. The output is pretty much a straight like mostly with some outliers in between. ","264699d4":"Here, we would be looking at the first 5 rows to get an idea about the data set and understand it much better.","fac9667c":"We would also count the number of students with various LOR (Letter of recommendation) scores. We can see from the data below that there are a whole lot of students having a score of 3.0 in their LORs respectively. Apart from this, there are quite a lot of people who have 5.0 in their LORs which is brilliant. Based on this, we can conclude that most of the students have 5.0 in their LORs and 4.5 in their SOPs. That means these students must be brilliant. ","f4e8b3f9":"Now, I would be importing the data and store it in df. This consists of the** Graduate Admissions File**.","a481bb34":"We see a green line from the start to the finish. This is a regression plot; this plot gives an approximate relationship between the parameters on the x-axis and the y-axis.As we can see, there is a linear relationship between the independant and dependant variable. ","ba6caeb0":"Here, we would basically define **relu** and** sigmoid** activation functions. In addition to this, we are also defining the **backward relu** and **sigmoid** functions that are used to compute the derivatives with respect to the cost while using back propagation. ","9c8e3445":"Let us also count the number of students with gradings on their SOP. From the result, we can see that most of the students got a score of about 4.0 in their SOP. Apart from this, 6 students got an SOP score of 1.0 which is quite rare in general. The average case would be for the students who got around 4.5. Wow!! That means most of the students are perfect in writing the SOPs.","c3fb4877":"We would also plot  the distribution plot for these graphs. We would see how the information is spread about the mean and identify the outliers in the data. \nWe can easily observe from the data that there is onle a single student who has an LOR score of 1.0 who is an outlier. In addition to this, looking at the far end of the spectrum, we find that there are quite a lot of students who have an LOR score of 5.0 ","431b1d98":"We will also be plotting a distribution plot. It would give us a rough estimate as to how the data is spread. We use seaborn in this example to get an intutive sense of how the data is spread.\nAs can be seen from the graph, most of the students have a score range between 3.5 and 4. In addition, the students who have a score of 4.5 in their SOP are also relatively high. ","9b5a9bbd":" We could see from the above graph that there is a decrease in training set error as with the increase in the number of iterations. We could easily spot that after a few iterations (say 5000), there is a significant reduction in the cost or the error between the predicted value and the true value. We can conclude that as the number of iterations increase, there is a change for the reduction in the cost or the error associated with the problem. However, if the number of iterations increase to a point where there is almost zero error, the model could run into the problem of overfitting. Therefore, care must be taken to ensure that the model doesn't overfit. This is a problem where the model does very well on the training set while it does poorly on the test set. ","81d4f31e":"The below plot is a hex plot that shows relationship between 'CGPA' on the x-axis and 'GRE Score' on the y-axis. There is clearly a linear trend when we look at the plot. As the CGPA increases, the higher would be the GRE Score (most of the cases). Similar to a kde plot, the darker the region, more is the density of the data points given in the data set. Most of the students present in the dataset scored somewhere around 320 on the GRE and have a CGPA of 8.8. Additionally, there are a very few students who scored between 290 and 300 and then, got a 7.5 CGPA.","2dfb8de1":"Here, I've plotted a kde plot which shows the density of the dataset. The darker the region, the more dense is the data present at those points. What it means is most of the data that is given is concentrated at that region. For example, when we look at the first dark circle (sort of), we can say that most of the students scored between 310 - 315 on the GRE and 105 - 110 on the TOEFL. The similar result is true for the other dark region. If the region is light, it means that there is not much data available for those points. ","836ed20c":"**Welcome**\n\nHello, guys!! You've come to the right page. This page gives you  an intuition about how a neural network actually functions and the code that is necessary to run it. Also, I feel that people are directly jumping into some of the deep learning frameworks and libraries such as** tensorflow**, **keras** and **caffe** without gaining an intuition and the steps in which they operate to solve a particular machine learning problem. The main purpose of this page creation is to provide the reader with right knowledge and the steps that a neural network algorithm takes to solve a particular problem.\n\nIn general, people use some of the deep learning libraries such as **tensorflow**, **keras** and **caffe** to name a few. What they don't realize is how these models work in the backend.** I've written the code from scratch** so that even a person who is new to machine learning and deep learning could get a sense as to how the algorithm functions and the flow of logic and the code.\n\nIn the code below, there are various functions included and their definitions given so that the reader can understand the block of code. As these functions are defined, they would later be called in the main function that executes the code overall. The former functions are called \"helper functions\" while the function that is used to execute these helper functions is called \"master function\" or \"main function\". \n\nWe would go in a sequence of steps. We would first initialize the weights, apply forward propagation and load some of the constants in cache, apply back propagation and finally, update the parameters. Later, we would run the code in the main function and then design a prediction function that would take certain input parameters and give us an output as to whether a person would be admitted in a university or not.\n\nBy the way, this model that I've designed from scratch doesn't use some of the deep learning frameworks like **tensorfow** and** keras**. It was able to predict whether a person would be admitted to a university or not with **93% test accuracy**. And there were very few issues with either the overfitting or the underfitting. \n\nLet's get started in the machine learing project using **deep neural networks from scratch** without deep learning libraries such as tensorflow, keras and theano!!!!!!"}}