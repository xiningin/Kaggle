{"cell_type":{"ec62735e":"code","7eb336dc":"code","09da4655":"code","b04b8df5":"code","4ad73e42":"code","8d8a94fc":"code","23621d84":"code","28cb5efd":"code","f408b600":"code","10e20d4b":"code","8d00b9e1":"code","4a6d32e8":"code","6117ceb9":"code","799dd5c2":"code","abb1f892":"code","e0b6c3e3":"code","db209d62":"code","13c74ee4":"code","88d48f74":"code","e6f494d5":"markdown","a8e3050a":"markdown","9ff27308":"markdown","9ce771c5":"markdown","fbf87c3e":"markdown","b3a40f74":"markdown","0a2ebc02":"markdown","0355e744":"markdown","dc50c24e":"markdown","9bc4af81":"markdown","b41abe00":"markdown","1625d544":"markdown","8ec58a6f":"markdown","c4878c13":"markdown","241193ae":"markdown","852ced7b":"markdown","d0e8008f":"markdown","dd4249e9":"markdown","9c550c03":"markdown","1a9b683a":"markdown"},"source":{"ec62735e":"#import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import metrics\n\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","7eb336dc":"df = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/tanlitung\/Datasets\/master\/kc_house_data.csv\")\ndf.head()","09da4655":"df.isnull().sum()","b04b8df5":"df.describe()","4ad73e42":"df.shape","8d8a94fc":"#dropping obvious attributes that won't make any difference to our prediction\ndf.drop('id', axis = 1, inplace = True)\ndf.drop('date', axis = 1, inplace = True)","23621d84":"from sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler()\ncol_name = df.drop('price', axis = 1).columns[:]\nx = df.loc[:, col_name]\ny = df['price']\n\n# Normalizing x\nx = pd.DataFrame(data = min_max_scaler.fit_transform(x), columns = col_name)\n\n# Examine the normalized data\nprint(df.head())\nx.head()","28cb5efd":"fig, axs = plt.subplots(ncols = 3, nrows = 3, figsize = (10, 10))\nsns.regplot(y = df['price'], x = x['bathrooms'], ax = axs[0, 0])\nsns.regplot(y = df['price'], x = x['bedrooms'], ax = axs[0, 1])\nsns.regplot(y = df['price'], x = x['sqft_living'], ax = axs[0, 2])\nsns.regplot(y = df['price'], x = x['sqft_above'], ax = axs[1, 0])\nsns.regplot(y = df['price'], x = x['floors'], ax = axs[1, 1])\nsns.regplot(y = df['price'], x = x['yr_built'], ax = axs[1, 2])\nsns.regplot(y = df['price'], x = x['waterfront'], ax = axs[2, 0])\nsns.regplot(y = df['price'], x = x['grade'], ax = axs[2, 1])\nsns.regplot(y = df['price'], x = x['sqft_living15'], ax = axs[2, 2])\nplt.tight_layout()","f408b600":"plt.figure(figsize = (10, 10))\nsns.heatmap(df.corr(), annot = True)\nplt.show()","10e20d4b":"#scatterplot\nsns.set()\ncols = ['price', 'sqft_living', 'grade', 'bathrooms', 'bedrooms']\nsns.pairplot(df[cols], size = 2.5)\nplt.show();","8d00b9e1":"#histogram and normal probability plot\nsns.distplot(df['price'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df['price'], plot=plt)","4a6d32e8":"#applying log transformation\ndf['price'] = np.log(df['price'])\n#transformed histogram and normal probability plot\nsns.distplot(df['price'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df['price'], plot=plt)","6117ceb9":"#histogram and normal probability plot\nsns.distplot(df['sqft_living'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df['sqft_living'], plot=plt)","799dd5c2":"#data transformation\ndf['sqft_living'] = np.log(df['sqft_living'])\n#transformed histogram and normal probability plot\nsns.distplot(df['sqft_living'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df['sqft_living'], plot=plt)","abb1f892":"#Splitting the data\nfeatures = df.drop('price', axis = 1)\ntarget = df['price']\ntrain_features, test_features, train_target, test_target = train_test_split(features, target, test_size = 0.2, random_state = 5)\nprint(\"Train features shape : \", train_features.shape)\nprint(\"Train target shape   : \", train_target.shape)\nprint(\"Test features shape  : \", test_features.shape)\nprint(\"Test target shape    : \", test_target.shape)","e0b6c3e3":"#building model\nmodel = LinearRegression(normalize = True)\nmodel.fit(train_features, train_target)","db209d62":"print(\"Model intercept  : \", model.intercept_, \"\\n\")\nprint(\"Model coefficient: \", model.coef_, \"\\n\")\n\nfor i in range(len(features.columns)):\n    print(features.columns[i], \": \", model.coef_[i])","13c74ee4":"#model evaluation\ntrain_target_pred = model.predict(train_features)\nrmse = (np.sqrt(mean_squared_error(train_target, train_target_pred)))\nr2 = r2_score(train_target, train_target_pred)\n\n# Examine the first 10 predicted output from the model\noutput = pd.DataFrame(train_target[0:10])\noutput['Predicted'] = train_target_pred[0:10]\noutput['Difference'] = output['Predicted'] - output['price']\nprint(output, \"\\n\")\n\nprint(\"Model training performance:\")\nprint(\"---------------------------\")\nprint('RMSE is {}'.format(rmse))\nprint('R2 score is {}'.format(r2))\nprint(\"\\n\")","88d48f74":"accuracy = model.score(train_features, train_target)\nprint(accuracy)","e6f494d5":"Let's normalize the data now. Wait, but why?\n\nTo scale the data between values 0 to 1. It will help us get accurate results and visualization and also speeds up the calculation!","a8e3050a":"It shows that 'price' is not normal. It shows high peakedness, positive skewness and even does not follow the diagonal line. We can perform log transformations for this","9ff27308":"We can now have a look at the intercept and coefficients for our model","9ce771c5":"Fine, let's move ahead and check out normality of each of these features","fbf87c3e":"Almost perfect, right?! Let's do the same for other features","b3a40f74":"We will use the LinearRegression() model in this case and we fit the train_features and train_target to the model.\n\nNOTE: There is a parameter normalized = True, this enables the data to be normalized when fed into the model!\n","0a2ebc02":"We can see a little skewness here as well","0355e744":"R2 score with low RMSE value is ideal","dc50c24e":"whoa! this is a considerably a large dataset, we will have to check for any outliers or unimportant features. We will do that along the way, hang tight!","9bc4af81":"Let's build the model now, shall we?!","b41abe00":"That's it! We are done here!\n\nThere are many other techniques that are out there which can be more useful to evaluate the model! This is just something you can start off with.\n\nThank you for going through this notebook. I hope you found this useful and if you did, please upvote it! <3\n","1625d544":"This is a correlation matrix. \nEach square shows the correlation between the variables on each axis. Correlation ranges from -1 to +1. Values closer to zero means there is no linear trend between the two variables.\n\nThe close to 1 the correlation is the more positively correlated they are; that is as one increases so does the other and the closer to 1 the stronger this relationship is. A correlation closer to -1 is similar, but instead of both increasing one variable will decrease as the other increases.\n\nFor the rest the larger the number and darker the colour the higher the correlation between the two variables. The plot is also symmetrical about the diagonal since the same two variables are being paired together in those squares\n\nSo, from the above matrix, we can say that attributes 'sqft_living', 'grade', 'sqft_above' are having highest positive correlation with the target variable\n\n'sqft_living' and 'sqft_above' are strongly correlated variables. However, both those variables are almost similar. 'sqft_lviing' determines square fottage of the house and 'sqft_above' is the square footage of the entire house apart from basement. Similarly with 'sqft_living' and 'sqft_living15' ","8ec58a6f":"# Multiple Linear Regression\n\nAs opposed to Simple Linear Regression, multiple linear regression considers multiple predictors(X) and one target variable(y)\n\nNote that, in both, simple and multiple linear regression, the dependent variable i.e. target variable is always continous!","c4878c13":"great!!","241193ae":"As you can see, all our features are now in the range of 0-1\n\nLet's try to find relationship of every feature with the target variable","852ced7b":"As you can see, there is a positive linear relationship between price and sqft_living. But let's not stop here","d0e8008f":"Description of all the attributes\n\n*     id: ID of the house\n*     date: Data of house sold\n*     bedrooms: Number of bedrooms\n*     bathroooms: Number of bathrooms\n*     sqft_living: Square footage of house\n*     sqft_lot: Square footage of lot\n*     floors: Number of floors\/ Level\n*     waterfront: 1 = Waterfront view; 0 = No waterfront view\n*     view: 1 = House been viewed; 0 = House has not been viewed\n*     condition: 1 indicates worn out property and 5 excellent\n*     grade: Overall grade given to the housing unit, based on King County grading system. 1 poor ,13 excellent\n*     sqft_above: Square footage of house apart from basement\n*     sqft_below: Square footage of the basement\n*     yr_built: Year of house built\n*     yr_renovated: Year of house renovated\n*     zipcode: Zip code\n*     lat: Latitude coordination\n*     long: Longitude coordination\n*     sqft_living15: Square footage of house in 2015 (implies-- some renovations)\n*     sqft_lot15: Square footage of lot in 2015 (implies-- some renovations)\n*     price: Price of house sold","dd4249e9":"Let's predict and evaluate!","9c550c03":"Now, let's check for null values!","1a9b683a":"So now, let's see the relationship between the target variable and important features"}}