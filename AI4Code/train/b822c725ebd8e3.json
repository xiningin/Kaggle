{"cell_type":{"eb274a4b":"code","400bcf0e":"code","bd1e1ae1":"code","c866736f":"code","ba5386e8":"code","4e800494":"code","cd08e106":"code","56e7e217":"code","6859213a":"code","120e8810":"code","1178d0fe":"code","c583ddb1":"code","ed105c24":"code","2d648416":"code","a73d574f":"code","eb4f352f":"code","a5031ada":"code","9b847ca2":"markdown","ef1ee18d":"markdown","7598d7eb":"markdown","d34c69f5":"markdown","d2cb5733":"markdown","7f60c96b":"markdown","26b12ffe":"markdown","1feff90b":"markdown","f57b92eb":"markdown"},"source":{"eb274a4b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","400bcf0e":"import pandas as pd\nimport numpy as np\ntrain = pd.read_csv(\"\/kaggle\/input\/web-traffic-time-series-forecasting\/train_1.csv\").fillna(0) #handling missing values\ntrain = train.replace(np.nan,0) #handling missing values","bd1e1ae1":"from matplotlib import rcParams\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport matplotlib\nrcParams['figure.figsize'] = 18,8\ny = train.loc[1][1:]\nplt.plot(y) #plot a random time series to get the idea of what our data looks like\nplt.xlabel('Date-Time', fontsize=10)\nplt.ylabel('Traffic', fontsize=10)\nplt.title('Web Traffic- Original data')\nplt.show()","c866736f":"#first order and second order differencing to enforce stationarity\nfirst_order = y.diff()\nsecond_order = first_order.diff()\nplt.plot(y)\nplt.xlabel('Date-Time', fontsize=10)\nplt.ylabel('Traffic', fontsize=10)\nplt.title('Web Traffic- Original data')\nplt.show()\nplt.plot(first_order)\nplt.xlabel('Date-Time', fontsize=10)\nplt.ylabel('Traffic', fontsize=10)\nplt.title('Web Traffic- First Order difference')\nplt.show()\nplt.plot(second_order)\nplt.xlabel('Date-Time', fontsize=10)\nplt.ylabel('Traffic', fontsize=10)\nplt.title('Web Traffic- Second Order difference')\nplt.show()","ba5386e8":"import statsmodels.api as sm\nseries = y\ncycle, trend = sm.tsa.filters.hpfilter(series, 50) #time series decomposition\nfig, ax = plt.subplots(3,1)\nax[0].plot(series)\nax[0].set_title('Actual')\nax[1].plot(trend)\nax[1].set_title('Trend')\nax[2].plot(cycle)\nax[2].set_title('Cycle')\nplt.show()","4e800494":"ind = pd.to_datetime(y.index)\narr = []\nfor i in range(len(y)):\n    arr.append(y[i])\narr = pd.DataFrame(arr)\narr.index = ind\ndecomposition = sm.tsa.seasonal_decompose(arr) \ndecomposition.plot();","cd08e106":"#acf and pacf\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf\nplot_acf(arr);\nplot_pacf(arr);\nacf_values= sm.tsa.stattools.acf(y)\npacf_values= sm.tsa.stattools.pacf(y)","56e7e217":"#Distributing data based on language\nlang_dict = dict()\nfor i in range(len(train)):\n    lang = train['Page'][i][train['Page'][i].find(\".wikipedia\")-2:train['Page'][i].find(\".wikipedia\")]\n    temp=train.loc[i]\n    if(lang not in lang_dict.keys()):\n        lang_dict[lang] = [temp]\n    else:\n        lang_dict[lang].append(temp)","6859213a":"import numpy as np\nfrom matplotlib import pyplot as plt\ni=0\ndata_lang =[]\nfor lang in lang_dict.keys():\n        data_lang.append([lang])\n        for j in range(len(lang_dict[lang])):\n            data_lang[i].append(sum(lang_dict[lang][j][1:]))\n        i=i+1\nstats_lang = []\nfor lang in data_lang:\n    stats_lang.append([lang[0],sum(lang[1:]),len(lang[1:]),np.mean(lang[1:]),np.std(lang[1:])])\nimport pandas as pd\nstats_lang = pd.DataFrame(stats_lang[:8])\n\nindex = np.arange(len(stats_lang))\nplt.bar(index,stats_lang[:][3])\nplt.xlabel('Language', fontsize=10)\nplt.ylabel('Mean of web hits', fontsize=10)\nplt.xticks(index, stats_lang[:][0], fontsize=10, rotation=30)\nplt.title('Web Traffic mean based on language')\nplt.show()","120e8810":"#Distibuting Data based on access type and type\ntype_dict = {\"all-agent\":list(),\"spider\":list()}\naccess_dict = {\"access_dict\":list(),\"desktop\":list(),\"mobile-web\":list()}\nfor i in range(len(train)):\n    if(\"all-access\" in train[\"Page\"][i][train['Page'][i].find(\".wikipedia\"):]):\n        access_dict[\"access_dict\"].append(train.loc[i])\n    if(\"desktop\" in train[\"Page\"][i][train['Page'][i].find(\".wikipedia\"):]):\n        access_dict[\"desktop\"].append(train.loc[i])\n    if(\"mobile-web\" in train[\"Page\"][i][train['Page'][i].find(\".wikipedia\"):]):\n        access_dict[\"mobile-web\"].append(train.loc[i]) \n    if(\"all-agent\" in train[\"Page\"][i][train['Page'][i].find(\".wikipedia\"):]):\n        type_dict[\"all-agent\"].append(train.loc[i])\n    if(\"spider\" in train[\"Page\"][i][train['Page'][i].find(\".wikipedia\"):]):\n        type_dict[\"spider\"].append(train.loc[i])","1178d0fe":"i=0\ndata_type =[]\nfor type_x in type_dict.keys():\n        data_type.append([type_x])\n        for j in range(len(type_dict[type_x])):\n            data_type[i].append(sum(type_dict[type_x][j][1:]))\n        i=i+1\nstats_type = []\nfor type_x in data_type:\n    stats_type.append([type_x[0],sum(type_x[1:]),len(type_x[1:]),np.mean(type_x[1:]),np.std(type_x[1:])])\nimport pandas as pd\nstats_type = pd.DataFrame(stats_type[:8])\n\nindex = np.arange(len(stats_type))\nplt.bar(index,stats_type[:][3])\nplt.xlabel('Type', fontsize=10)\nplt.ylabel('Mean of web hits', fontsize=10)\nplt.xticks(index, stats_type[:][0], fontsize=10, rotation=30)\nplt.title('Web Traffic mean based on Type')\nplt.show()","c583ddb1":"i=0\ndata_access =[]\nfor access_x in access_dict.keys():\n        data_access.append([access_x])\n        for j in range(len(access_dict[access_x])):\n            data_access[i].append(sum(access_dict[access_x][j][1:]))\n        i=i+1\nstats_access = []\nfor access_x in data_access:\n    stats_access.append([access_x[0],sum(access_x[1:]),len(access_x[1:]),np.mean(access_x[1:]),np.std(access_x[1:])])\nimport pandas as pd\nstats_access = pd.DataFrame(stats_access[:8])\n\nindex = np.arange(len(stats_access))\nplt.bar(index,stats_access[:][3])\nplt.xlabel('access', fontsize=10)\nplt.ylabel('Mean of web hits', fontsize=10)\nplt.xticks(index, stats_access[:][0], fontsize=10, rotation=30)\nplt.title('Web Traffic mean based on access')\nplt.show()","ed105c24":"import pandas as pd\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport itertools\nimport statsmodels.api as sm\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor , RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom matplotlib import rcParams\ntrain = train.sample(10)","2d648416":"def split_sequence(sequence, n_steps):\n    X, Y = list(), list()\n    for i in range(len(sequence)):\n        end_ix = i + n_steps\n        if end_ix > len(sequence)-1:\n            break\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n        X.append(seq_x)\n        Y.append(seq_y)\n    return np.array(X),np.array(Y)","a73d574f":"def LSTM_MODEL(n,n_steps):\n    n_features = 1\n    model = Sequential()\n    model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features),return_sequences = True))\n    for layer in range(n):\n        model.add(LSTM(50, activation='relu',return_sequences = True))\n    model.add(LSTM(50, activation='relu'))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mse')\n    return model","eb4f352f":"rcParams['figure.figsize'] = 18,8\nlist_models = []\nregr_1 = DecisionTreeRegressor(max_depth=4)\nadaboostSVC = AdaBoostRegressor(n_estimators = 500, random_state = 42, learning_rate=0.01, base_estimator=regr_1)\nest = GradientBoostingRegressor(n_estimators=500, learning_rate=0.1, max_depth=4, random_state=0, loss='ls')\nregressor = RandomForestRegressor(max_depth=4 , random_state=0, n_estimators=500)\np,d,q = 1,0,2\nfor time_series in train.index:  \n    \n    #extracting the time series\n    error_list = []\n    list_of_model_pred = []\n    print(\"For Time series:\", time_series)\n    print(train.loc[time_series][0])\n    y = train.loc[time_series][1:]\n    ind = pd.to_datetime(y.index)\n    arr = []\n    for i in range(len(y)):\n        arr.append(y[i])\n    arr = pd.DataFrame(arr)\n    arr.index = ind\n    #split the dataset into training and testing data\n    test_X,test_Y = split_sequence(arr[0][-365:],30)\n    train_X,train_Y = split_sequence(arr[0][0:-365],30)\n    list_of_model_pred.append(y[-335:])\n    #Adaboost Model\n    model = adaboostSVC.fit(train_X, train_Y)\n    pred_Y = model.predict(test_X)\n    rmse = sqrt(mean_squared_error(test_Y,pred_Y))\n    error_list.append(rmse)\n    print(\"Adaboost Done with error: \",rmse)\n    list_of_model_pred.append(pred_Y)\n\n    #Gradient Boosting\n    est.fit(train_X, train_Y)\n    pred_Y = est.predict(test_X)\n    rmse = sqrt(mean_squared_error(test_Y, pred_Y))\n    error_list.append(rmse)\n    print(\"Gradient Boost Done with error: \",rmse)\n    list_of_model_pred.append(pred_Y)\n    \n    #Random Forrest\n    regressor.fit(train_X,train_Y)\n    pred_Y = regressor.predict(test_X)\n    rmse = sqrt(mean_squared_error(test_Y,pred_Y))\n    error_list.append(rmse)\n    print(\"Random Forest Regressor Done with error: \",rmse)\n    list_of_model_pred.append(pred_Y)\n    \n    # RNN and LSTM Model\n    n_features = 1\n    model = LSTM_MODEL(4,30)\n    train_X1 = train_X.reshape((train_X.shape[0], train_X.shape[1], n_features))\n    model.fit(train_X1, train_Y, epochs=200, verbose=0)\n    test_X1 = test_X.reshape((test_X.shape[0], test_X.shape[1], n_features))\n    pred_Y = model.predict(test_X1, verbose=0)\n    rmse = sqrt(mean_squared_error(test_Y,pred_Y))\n    error_list.append(rmse)\n    print(\"RNN and LSTM Done with error: \",rmse)\n    list_of_model_pred.append(pred_Y)\n    \n    #Sarimax Model\n    y = train.loc[time_series][1:]\n    mod = sm.tsa.statespace.SARIMAX(arr[:-365],\n                                order=(p, d, q),\n                                seasonal_order=(1,1,2, 12),\n                                enforce_stationarity=True,\n                                enforce_invertibility=False)\n    results = mod.fit()\n    pred = results.get_prediction(start=pd.to_datetime('2016-01-01'),end=pd.to_datetime('2016-12-31') )\n    rmse = sqrt(mean_squared_error(arr['2016-01-01':'2017-01-02'], pred.predicted_mean))\n    print(\"SARIMAX Done with error: \",rmse)\n    list_of_model_pred.append(pred_Y)\n    error_list.append(rmse)\n    \n    #Plot predicted vs Original for all Models\n    label_list = [\"Original Time Series\",\"Adaboost\",\"Gradient Boost\",\"Random Forest\",\"RNN and LSTM\",\"SARIMAX\"]\n    plt.style.use('seaborn-darkgrid')\n    palette = plt.get_cmap('Dark2')\n    for i in range(len(list_of_model_pred)):\n        plt.subplot(3,2, i+1)\n        if(i!=0):\n            plt.plot(list_of_model_pred[0], marker='', color='grey', linewidth=0.6, alpha=0.3)\n            plt.xlabel('Date', fontsize=10)\n            plt.ylabel('Number of web hits', fontsize=10)\n        plt.title(label_list[i], loc='left', fontsize=12, fontweight=0, color=palette(i))\n        plt.plot(list_of_model_pred[i], marker='', color=palette(i), linewidth=2.4, alpha=0.9, label=label_list[i])\n        plt.xlabel('Date', fontsize=10)\n        plt.ylabel('Number of web hits', fontsize=10)\n    plt.suptitle(train.loc[time_series][0], fontsize=13, fontweight=0, color='black', style='italic', y=1.02)\n    list_models.append(error_list)\n    plt.show()","a5031ada":"rcParams['figure.figsize'] = 18,8\nlist_models = pd.DataFrame(list_models)\nindex = [\"time series: \"+str(i) for i in train.index]\nlist_models.index = index\nlist_models.columns = [i for i in label_list[1:]]\nlist_models.head(10)\nlist_models.plot.line()\nplt.title(\"ERROR\")\nplt.show()\nlist_models.boxplot()\nplt.title(\"ERROR\")\nplt.show()","9b847ca2":"**ACF** is an (complete) auto-correlation function which gives us values of auto-correlation of any series with its lagged values. We plot these values along with the confidence band and tada! We have an ACF plot. In simple terms, it describes how well the present value of the series is related with its past values. A time series can have components like trend, seasonality, cyclic and residual. ACF considers all these components while finding correlations hence it\u2019s a \u2018complete auto-correlation plot\u2019.\n\n**PACF** is a partial auto-correlation function. Basically instead of finding correlations of present with lags like ACF, it finds correlation of the residuals (which remains after removing the effects which are already explained by the earlier lag(s)) with the next lag value hence \u2018partial\u2019 and not \u2018complete\u2019 as we remove already found variations before we find the next correlation. So if there is any hidden information in the residual which can be modeled by the next lag, we might get a good correlation and we will keep that next lag as a feature while modeling. Remember while modeling we don\u2019t want to keep too many features which are correlated as that can create multicollinearity issues. Hence we need to retain only the relevant features.","ef1ee18d":"Time series data can exhibit a variety of patterns, and it is often helpful to split a time series into several components, each representing an underlying pattern category.\n\nThere are three types of time series patterns: *trend, seasonality and cycles*. When we decompose a time series into components, we usually combine the trend and cycle into a single trend-cycle component (sometimes called the trend for simplicity). Thus we think of a time series as comprising three components: a trend-cycle component, a seasonal component, and a remainder component (containing anything else in the time series).","7598d7eb":"**Handling missing values:**\nWe noticed that there were a lot of missing values but the placement of those missing -NA- values were at the beginning of the data for individual time series. This meant that the web page was added to the domain after the given date and thus it was the best to replace them with 0.\n","d34c69f5":"**Proposed Idea:**\nMultiple learners can be used to predict time-series data. This is called Ensemble Learning. Ensemble learning is the process by which multiple models, such as classifiers or experts, are strategically generated and combined to solve a particular computational intelligence problem. Ensemble learning is primarily used to improve the performance of a model, or reduce the likelihood of an unfortunate selection of a poor one. This can be used to provide better prediction for time-series data. \n**Models Implemented:**\n1. SARIMAX: We made the code for the SARIMAX model and fine-tuned these parameters and found that p=1, d=0, q=2, P=1, D=1, Q=2 and m=12 gave the least value of RMSE for a general model for our entire training set.\n2. RNN and LSTM based Models: Here in our case, we made RNN and LSTM models to predict Time Series and as a first step we made a single layered(vanilla RNN) with 7 day look back which gave good results but did not work well as general model. Then we fine-tuned the parameters, i.e. number of layers and look back time and found that 4 layered with a 30 day look back time gave the best results as a general model.\n3. Adaboost Regressor: For our training data, we got best results with Decision Tree Regressor with max depth 4 as base estimator, learning rate as 0.01, n estimators as 5000 and random state as 42.\n4. Gradient Boost Regressor: For our training data, we got best results with n estimators as 500, learning rate as 0.1, max depth=4 and loss='ls'.\n5. Random Forrest Regressor:Here are the Tune-able Parameters which gave the best results for our data, max depth as 4 and n estimators as 500.\n\nAll these models are compared based on the RMSE value for the test data that is predicting data for the next whole year.","d2cb5733":"**Split the Data**\nHere the data is split based on Look Back n steps for Prediction of next value for regression based models.","7f60c96b":"**Evaluation**\n\nError Comparison of different Models Based on RMSE value over the next year data.","26b12ffe":"What are the types of the website?\n1. Based On LANGUAGE\n2. Based On ACCESS \n3. Based On TYPE","1feff90b":"**Loading the required modules**","f57b92eb":"**LSTM Model Tempelate**"}}