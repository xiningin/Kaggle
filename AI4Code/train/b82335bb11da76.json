{"cell_type":{"f9a8021d":"code","b9d96150":"code","6a490693":"code","2c5927da":"code","7452c481":"code","dde06447":"code","1cd41137":"code","47a06155":"code","c4f144e2":"code","b075245f":"code","cbfab163":"code","f7c16251":"code","40382212":"code","649d79fe":"code","0fcd8806":"code","1e3de71f":"code","35b61289":"code","16b12226":"code","b1eda342":"code","69b71cc9":"code","386f813c":"code","1dbc5e9e":"code","428dec31":"code","b2d5702f":"code","974ec8fc":"code","6ecdce44":"code","ec7225be":"code","c1086a12":"code","9f9d35aa":"code","dfe585cf":"code","3d1a7880":"code","bc91376b":"code","ff0f7711":"code","ba01e5eb":"markdown","4ba75108":"markdown","433a7d26":"markdown","6c7c25ca":"markdown","1c23e86d":"markdown","4f4f5a8a":"markdown","01c7728b":"markdown","5e773109":"markdown","6e3f9fb7":"markdown","fdab74ac":"markdown","ad676e51":"markdown","18238692":"markdown","17233b45":"markdown","5b31b5c2":"markdown","dad49411":"markdown","d703b4db":"markdown","a3b8ba49":"markdown","80704c05":"markdown","2e830d45":"markdown","ed873b7c":"markdown","71750e8b":"markdown","2259cfac":"markdown"},"source":{"f9a8021d":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import train_test_split","b9d96150":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","6a490693":"df = pd.read_csv(\"\/kaggle\/input\/appliances-energy-prediction\/KAG_energydata_complete.csv\")","2c5927da":"df.info()","7452c481":"df.describe()","dde06447":"df.head()","1cd41137":"sns.distplot(df['Appliances'])","47a06155":"df = df.drop(labels='date', axis=1)","c4f144e2":"train, test = train_test_split(df, test_size=0.3, shuffle=True, random_state=42)\ntest, val = train_test_split(df, test_size=0.5, shuffle=True, random_state=42)","b075245f":"class CustomDataset(Dataset):\n    def __init__(self, df):\n        self.X = torch.tensor(scale(df.drop(labels='Appliances', axis=1)).astype(np.float32)).to(device)\n        self.y = torch.tensor(df['Appliances'].values.astype(np.float32)).to(device)\n        \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, index):\n        return {'X': self.X[index], 'y': self.y[index]}","cbfab163":"trainD = CustomDataset(train.reset_index(drop=True))\ntestD = CustomDataset(test.reset_index(drop=True))\nvalD = CustomDataset(val.reset_index(drop=True))","f7c16251":"trainDL = DataLoader(trainD, batch_size=32, shuffle=True, num_workers=2)\ntestDL = DataLoader(testD, batch_size=32, num_workers=2)\nvalDL = DataLoader(valD, batch_size=32, num_workers=2)","40382212":"class FeedForwardNet(nn.Module):\n    def __init__(self):\n        super(FeedForwardNet, self).__init__()\n        self.input_layer = nn.Linear(27, 80)\n        self.hidden1 = nn.Linear(80, 40)\n        nn.init.xavier_uniform_(self.hidden1.weight)\n        self.sigmoid = nn.Sigmoid()\n        self.dropout = nn.Dropout()\n        self.batchnorm1 = nn.BatchNorm1d(40)\n        self.hidden2 = nn.Linear(40, 12)\n        self.output = nn.Linear(12, 1)\n        \n    def forward(self, x):\n        x = self.input_layer(x)\n        x = self.batchnorm1(self.sigmoid(self.hidden1(x)))\n        x = self.dropout(self.sigmoid(self.hidden2(x)))\n        x = self.output(x)\n        return x","649d79fe":"model = FeedForwardNet()\nmodel = model.to(device)","0fcd8806":"loss_function = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters())","1e3de71f":"def trainer(epochs, trainDL, valDL, model, loss_function, optimizer):\n    for epoch in range(epochs):\n        for i, data in enumerate(trainDL):\n            model.train()\n            output = model(data['X'])\n            t_loss = loss_function(output, data['y'].view(-1, 1))\n            optimizer.zero_grad()\n            t_loss.backward()\n            optimizer.step()\n            \n            v_loss = 0\n            with torch.no_grad():\n                model.eval()\n                for j, data in enumerate(valDL):\n                    loss = loss_function(model(data['X']), data['y'].view(-1, 1))\n                    v_loss += loss.item()\n            print(f\"Epoch: {epoch+1}, Batch: {i+1}, Training Loss: {str(round(t_loss.item(), 2))}, Validation Loss: {str(round(v_loss\/j, 2))}\")","35b61289":"trainer(5, trainDL, valDL, model, loss_function, optimizer)","16b12226":"def tester(testDL, model, loss_function):\n    model.eval()\n    total_loss = 0\n    for i, data in enumerate(testDL):\n        loss = loss_function(model(data['X']), data['y'].view(-1, 1))\n        total_loss += loss.item()\n    print(f\"Total Loss: {total_loss\/i}\")","b1eda342":"df = pd.read_csv(\"\/kaggle\/input\/diabetes\/diabetes.csv\")","69b71cc9":"df.info()","386f813c":"df.describe()","1dbc5e9e":"df.head()","428dec31":"train, test = train_test_split(df, test_size=0.2, shuffle=True, random_state=42)\ntest, val = train_test_split(test, test_size=0.5, shuffle=True, random_state=42)","b2d5702f":"class CustomDataset(Dataset):\n    def __init__(self, df):\n        self.X = torch.tensor(scale(df.drop(labels='Outcome', axis=1)).astype(np.float32)).to(device)\n        self.y = torch.tensor(df['Outcome'].values, dtype=torch.long).to(device)\n        \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, index):\n        return {'X': self.X[index], 'y': self.y[index]}","974ec8fc":"trainD = CustomDataset(train.reset_index(drop=True))\ntestD = CustomDataset(test.reset_index(drop=True))\nvalD = CustomDataset(val.reset_index(drop=True))","6ecdce44":"trainDL = DataLoader(trainD, batch_size=16, shuffle=True)\nvalDL = DataLoader(valD, batch_size=16)\ntestDL = DataLoader(testD, batch_size=16)","ec7225be":"class FFN(nn.Module):\n    def __init__(self):\n        super(FFN, self).__init__()\n        self.input = nn.Linear(8, 20)\n        self.hidden1 = nn.Linear(20, 6)\n        nn.init.xavier_uniform_(self.hidden1.weight)\n        self.sigmoid = nn.Sigmoid()\n        self.batchnorm = nn.BatchNorm1d(6)\n        self.output = nn.Linear(6, 2)\n        \n    def forward(self, x):\n        x = self.input(x)\n        x = self.batchnorm(self.sigmoid(self.hidden1(x)))\n        x = self.output(x)\n        return x","c1086a12":"model = FFN()\nmodel = model.to(device)","9f9d35aa":"loss_function = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)","dfe585cf":"def trainer(epochs, trainDL, valDL, model, optimizer, loss_function):\n    for epoch in range(epochs):\n        for i, data in enumerate(trainDL):\n            model.train()\n            output = model(data['X'])\n            t_loss = loss_function(output, data['y'])\n            optimizer.zero_grad()\n            t_loss.backward()\n            optimizer.step()\n            \n            with torch.no_grad():\n                v_loss = 0\n                model.eval()\n                for j, data in enumerate(valDL):\n                    loss = loss_function(model(data['X']), data['y'])\n                    v_loss += loss.item()\n            print(f\"Epoch: {epoch+1}, Batch: {i+1}, Training Loss: {str(round(t_loss.item(), 2))}, Validation Loss: {str(round(v_loss\/j, 2))}\")","3d1a7880":"trainer(10, trainDL, valDL, model, optimizer, loss_function)","bc91376b":"def tester(model, testDL):\n    model.eval()\n    total = 0\n    correct = 0\n    for i, data in enumerate(testDL):\n        output = model(data['X'])\n        values, indices = torch.max(output.data, 1)\n        total += data['y'].size(0)\n        correct += (indices == data['y']).sum().item()\n    print(f\"Accuracy: {(correct\/total)*100}\")","ff0f7711":"tester(model, testDL)","ba01e5eb":"### Loss Function & Optimizer","4ba75108":"### Feed Forward Network","433a7d26":"### Imports","6c7c25ca":"### Test","1c23e86d":"### Test","4f4f5a8a":"### Loading into DataFrame","01c7728b":"### DataLoaders","5e773109":"### Train","6e3f9fb7":"### Loss function & Optimizer","fdab74ac":"### Little EDA","ad676e51":"### Dataloaders","18238692":"### Train","17233b45":"### Preparing Dataset","5b31b5c2":"### Train, Test, Val split","dad49411":"### Feed Forward Network","d703b4db":"### Little EDA","a3b8ba49":"### Loading into dataframe","80704c05":"## Classification","2e830d45":"### Preparing Dataset","ed873b7c":"### To Improve Accuracy\n\n0. Initial Accuracy = 9482.92\n1. Standardize dataset (mean 0, standard deviation = 1) = 8597.80\n2. Activation function for Non-Linearity (tanh, sigmoid, RELU) = slow convergence\n3. Dropout = slower processing speed, slower convergence\n4. Batch Normalization = Faster convergence. Order = neuron layer -> activation -> batchnorm\n5. Weight Initialization\n6. Adjust Hyperparameters (like learning rate, epochs, optimizer parameters)\n\nNOTE: Don't put Dropout and Batch Normalization together\n\nSlower convergence doesn't mean it's bad. Maybe it's more regularized, so less overfitting","71750e8b":"## Regression","2259cfac":"### Train, Test, Val split"}}