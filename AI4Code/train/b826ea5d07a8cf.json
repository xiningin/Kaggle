{"cell_type":{"f4e3f9aa":"code","b78842d3":"code","917e148b":"code","fc7cda0b":"code","b6823d6f":"code","a4d301a0":"code","3c5f7114":"code","53546270":"code","da0ff0cc":"code","dec3eac8":"code","c3b0df88":"code","aec90187":"code","80014bd4":"code","ac49fbe3":"code","01440179":"code","320dd53a":"code","2238bdd0":"code","4ac5eb79":"code","91a3ef6e":"code","7f74a135":"code","fe3744c0":"code","ddc982ad":"code","9a781404":"code","5a8b7186":"code","e48a0187":"code","cc459e44":"code","791162a7":"markdown","b3ec1a5a":"markdown","161cbaf7":"markdown","4f74d9d3":"markdown","87ad6d83":"markdown","ed569edd":"markdown","d09f6fff":"markdown","4833d806":"markdown","bf180744":"markdown","dda5c91f":"markdown","f52da341":"markdown","c04ec4b5":"markdown","c76a825b":"markdown"},"source":{"f4e3f9aa":"import random\nrandom.seed(123)\n\nimport pandas as pd\nimport numpy as np\nimport datatable as dt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# importing feature selection and processing packages\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.feature_selection import SelectKBest,mutual_info_classif,SelectPercentile,VarianceThreshold\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS, ExhaustiveFeatureSelector as EFS\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,PowerTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\n\n# importing modelling packages\n\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\n\n# Optimisation Packages\n\nimport optuna\nfrom optuna import trial\nfrom optuna.samplers import TPESampler\nimport pprint\nimport joblib\nfrom skopt import BayesSearchCV\nfrom skopt.callbacks import DeadlineStopper, VerboseCallback, DeltaXStopper\nfrom skopt.space import Real, Categorical, Integer\nfrom time import time","b78842d3":"# using datatable for faster loading\n\ntrain = dt.fread(r'..\/input\/tabular-playground-series-sep-2021\/train.csv').to_pandas()\ntest = dt.fread(r'..\/input\/tabular-playground-series-sep-2021\/test.csv').to_pandas()\nsub = dt.fread(r'..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv').to_pandas()","917e148b":"train_data = train.copy()\ntest_data = test.copy()\n\ntrain_data = train_data.drop('id',axis=1)\ntest_data = test_data.drop('id',axis=1)\n\nfeatures = train_data.columns[:-1]","fc7cda0b":"# adding the magic features - using missing values and trends of the data\n\ntrain_data['n_missing'] = train_data[features].isna().sum(axis=1)\ntest_data['n_missing'] = test_data[features].isna().sum(axis=1)","b6823d6f":"# splitting data and imputing missing values\n\nX = train_data.drop('claim',axis=1)\ny = train_data['claim'] # the target variable\n\nX = X.apply(lambda x:x.fillna(np.mean(x)))\ntest_data = test_data.apply(lambda x:x.fillna(np.mean(x)))","a4d301a0":"# using minmax scaler to scale - we will use boosting models and need to converge to optimum value quickly\n\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\ntest_for_model = scaler.transform(test_data)","3c5f7114":"def train_model_optuna(trial, X_train, X_valid, y_train, y_valid):\n\n    preds = 0\n           \n     #A set of hyperparameters to optimize by optuna\n    lgbm_params = {\n                     \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n                     \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n                     \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n                      \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [20000]),        \n                      \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=1),\n                     \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=1), \n                     \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n                     \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.2, 0.95, step=0.1),\n                     \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n                     \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.2, 0.95, step=0.1),\n                      'device':'gpu'\n                     }\n\n    model = LGBMClassifier(**lgbm_params)\n    model.fit(X_train, y_train,eval_set=[(X_valid, y_valid)],eval_metric=\"auc\",\n               early_stopping_rounds=100,verbose=False)\n    \n    print(f\"Number of boosting rounds: {model.best_iteration_}\")\n    oof = model.predict_proba(X_valid)[:,1]\n    \n    return roc_auc_score(y_valid, oof)","53546270":"# setting up study on 0.67 training size\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(lambda trial: train_model_optuna(trial, X_train, X_valid,y_train, y_valid),\n                n_trials = 10)\n\n# Showing optimization results\n\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial parameters:', study.best_trial.params)\nprint('Best score:', study.best_value)","da0ff0cc":"def train_model_optuna_2(trial, X_train, X_valid, y_train, y_valid):\n    \n    params = {'iterations':trial.suggest_int(\"iterations\", 1000, 20000),\n              'od_wait':trial.suggest_int('od_wait', 500, 2000),\n              'task_type':\"GPU\",\n              'learning_rate' : trial.suggest_uniform('learning_rate', 0.02 , 1),\n              'reg_lambda': trial.suggest_uniform('reg_lambda',1e-5,100),\n              'subsample': trial.suggest_uniform('subsample',0.9,1.0),\n              'random_strength': trial.suggest_uniform('random_strength',10,50),\n              'depth': trial.suggest_int('depth',1,15),\n              'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n              'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n              'bootstrap_type':'Poisson',\n               }\n    \n    model = CatBoostClassifier(**params)\n    model.fit(X_train, y_train,eval_set=[(X_valid,y_valid)], early_stopping_rounds=150, verbose=False)\n    \n    oof = model.predict_proba(X_valid)[:,1]\n    \n    return roc_auc_score(y_valid, oof)","dec3eac8":"# setting up study on 0.67 training size\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(lambda trial: train_model_optuna_2(trial,X_train, X_valid,y_train, y_valid),\n                n_trials = 10)\n\n# Showing optimization results\n\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial parameters:', study.best_trial.params)\nprint('Best score:', study.best_value)","c3b0df88":"# LightGBM tuned parameters - I used 2000 instead of 20000 to get faster training\n# I am seeing what gets me good results for now\n\nlgbm_params = {'num_leaves': 2680, 'max_depth': 3, 'learning_rate': 0.1909226205418589,\n               'n_estimators': 20000, 'lambda_l1': 65, 'lambda_l2': 76,\n               'min_gain_to_split': 6.480697089399107, 'bagging_fraction': 0.7, \n               'bagging_freq': 1, 'feature_fraction': 0.6000000000000001}\n\n# CatBoost tuned parameters\n\ncat_params = {'iterations': 13969, 'od_wait': 1958, 'learning_rate': 0.04291773425770468,\n              'reg_lambda': 15.189348850727315, 'subsample': 0.9136087381151102, \n              'random_strength': 28.743778165335534, 'depth': 5, 'min_data_in_leaf': 25, \n              'leaf_estimation_iterations': 9,'bootstrap_type':'Poisson'}","aec90187":"folds = StratifiedKFold(n_splits = 10, random_state = 228, shuffle = True)\n\npredictions_lgb = np.zeros(len(test_for_model))\npredictions_cb = np.zeros(len(test_for_model))\n\nlgb_oof = np.zeros(X.shape[0])\ncat_oof = np.zeros(X.shape[0])\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X,y)):\n    print(f\"Fold: {fold+1}\")\n    X_train, X_val = X[trn_idx], X[val_idx]\n    y_train, y_val = y[trn_idx], y[val_idx]\n\n    model_lgb =  LGBMClassifier(device='gpu',**lgbm_params)\n    model_cb =  CatBoostClassifier(task_type='GPU',**cat_params,verbose=0)\n    \n    model_lgb.fit(X_train, y_train)\n    pred_lgb = model_lgb.predict_proba(X_val)[:,1]\n    lgb_oof[val_idx] = pred_lgb\n    print('ROC of LGB: ',roc_auc_score(y_val,pred_lgb))\n    \n    model_cb.fit(X_train, y_train)\n    pred_cb = model_cb.predict_proba(X_val)[:,1]\n    cat_oof[val_idx] = pred_cb\n    print('ROC of CB: ',roc_auc_score(y_val,pred_cb))\n    \n    print(\"-\"*50)\n    \n    predictions_lgb += model_lgb.predict_proba(test_for_model)[:,1] \/ folds.n_splits\n    predictions_cb += model_cb.predict_proba(test_for_model)[:,1] \/ folds.n_splits","80014bd4":"# calculating appropriate weights for ensemble\n\nimport scipy\ndef class_optimizer(X, a0, a1):\n    oof = X[0]*a0 + (1-X[0])*a1\n    return (1-roc_auc_score(y, oof))\n\nres = scipy.optimize.minimize(\n    fun=class_optimizer,\n    x0=[0.5],\n    args=tuple([lgb_oof, cat_oof]),\n    method='BFGS',\n    options={'maxiter': 1000})\n\nprint(res)\nprint(f\"coef0 {res.x[0]}, coef1 {1-res.x[0]}\")","ac49fbe3":"# Making ensemble using calculated weights\n\nensemble_oof = res.x[0] * lgb_oof + (1-res.x[0]) * cat_oof\nensemble_pred = res.x[0] * predictions_lgb  + (1-res.x[0]) * predictions_cb\n\n# tuned lgbm gave 0.81658 score and tuned catboost gave a score 0.81782\n\nprint(roc_auc_score(y, ensemble_oof))","01440179":"sub['claim'] = ensemble_pred\nsub.to_csv('submission_cb_lgb_tuned_ensemble.csv',index = False) # gave a score of 0.81804","320dd53a":"def Stacking(model, model_name, x_train, y_train, x_test, fold):\n\n    stk = StratifiedKFold(n_splits = fold, random_state = 42, shuffle = True)\n    \n    # Declaration Pred Datasets\n    train_fold_pred = np.zeros((x_train.shape[0], 1))\n    test_pred = np.zeros((x_test.shape[0], fold))\n    \n    for counter, (train_index, valid_index) in enumerate(stk.split(x_train, y_train)):\n        x_train, y_train = X[train_index], y[train_index]\n        x_valid, y_valid = X[valid_index], y[valid_index]\n        \n        print('------------ Fold', counter+1, 'Start! ------------')\n        if model_name == 'cat':\n            model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)])\n        elif model_name == 'lgbm':\n            model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], eval_metric = 'auc')            \n        print('------------ Fold', counter+1, 'Done! ------------')\n        \n        train_fold_pred[valid_index, :] = model.predict_proba(x_valid)[:, 1].reshape(-1, 1)\n        test_pred[:, counter] = model.predict_proba(x_test)[:, 1]\n    \n    test_pred_mean = np.mean(test_pred, axis = 1).reshape(-1, 1)\n\n    print('Done!')\n    \n    return train_fold_pred, test_pred_mean","2238bdd0":"model_lgb =  LGBMClassifier(device = 'gpu',**lgbm_params,verbose=0)\nmodel_cb =  CatBoostClassifier(task_type='GPU',**cat_params,verbose=0)\n\ncat_train, cat_test = Stacking(model_cb, 'cat', X, y, test_for_model, 10)\nlgbm_train, lgbm_test = Stacking(model_lgb, 'lgbm', X, y, test_for_model, 10)","4ac5eb79":"# creating stack datasets for our meta-classifier - using created feature\n\nnew_features = train_data.columns[train_data.columns!='claim']\nimp_features= ['n_missing']\nX_new = pd.DataFrame(X,columns=new_features)\ntest_for_model_new = pd.DataFrame(test_for_model,columns=new_features)\ntrain_new = X_new[imp_features]\ntest_new = test_for_model_new[imp_features]\n\nlgbm_train_1 = pd.DataFrame(lgb_oof,columns=['LGBM_train'])\ncat_train_1 = pd.DataFrame(cat_oof,columns=['CAT_train'])\nlgbm_test_1 = pd.DataFrame(predictions_lgb,columns=['LGBM_train'])\ncat_test_1 = pd.DataFrame(predictions_cb,columns=['CAT_train'])\n\nstack_x_train = pd.concat((train_new,lgbm_train_1, cat_train_1), axis = 1)\nstack_x_test = pd.concat((test_new,lgbm_test_1, cat_test_1), axis = 1)","91a3ef6e":"stk = StratifiedKFold(n_splits = 5, random_state = 42)\n\ntest_pred = 0\nfold = 1\ntotal_auc = 0\n\nfor train_index, valid_index in stk.split(stack_x_train, y):\n    x_train, y_train = stack_x_train.iloc[train_index], y[train_index]\n    x_valid, y_valid = stack_x_train.iloc[valid_index], y[valid_index]\n    \n    #lr = LogisticRegression(n_jobs = -1, random_state = 42, C = 1000, max_iter = 1000)\n    lr = RidgeClassifier()\n    lr.fit(x_train, y_train)\n    \n    valid_pred = lr.predict_proba(x_valid)[:, 1]\n    test_pred += lr.predict_proba(stack_x_test)[:, 1]\n    auc = roc_auc_score(y_valid, valid_pred)\n    total_auc += auc \/ 10\n    print('Fold', fold, 'AUC :', auc)\n    fold += 1\n    \nprint('Total AUC score :', total_auc)","7f74a135":"sub['claim'] = test_pred\/10\nsub.to_csv('submission_lgb_cb_tuned_log_FE_stacking.csv', index = 0) # gave a score of 0.81805","fe3744c0":"# Blending is a subtype of stacking - we will use only one fold, instead of 10 folds, here.\n\nX_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.33, random_state=2021,stratify=y)","ddc982ad":"model_lgb =  LGBMClassifier(device = 'gpu',verbose=0)\nmodel_cb =  CatBoostClassifier(task_type='GPU',verbose=0)\nmodel_xgb =  XGBClassifier(tree_method='gpu_hist',verbose=0)\n\npredictions_lgb = np.zeros(len(test_for_model))\npredictions_cb = np.zeros(len(test_for_model))\npredictions_xgb = np.zeros(len(test_for_model))\n\nlgb_oof = np.zeros(len(X_val))\ncat_oof = np.zeros(len(X_val))\nxgb_oof = np.zeros(len(X_val))","9a781404":"# first model - LightGBM\n\nmodel_lgb.fit(X_train,y_train)\nlgb_oof = model_lgb.predict_proba(X_val)[:,1]\npredictions_lgb = model_lgb.predict_proba(test_for_model)[:,1]\n\n# second model - Catboost\n\nmodel_cb.fit(X_train,y_train)\ncat_oof = model_cb.predict_proba(X_val)[:,1]\npredictions_cb = model_cb.predict_proba(test_for_model)[:,1]\n\n# third model - XGBoost\n\nmodel_xgb.fit(X_train,y_train)\nxgb_oof = model_xgb.predict_proba(X_val)[:,1]\npredictions_xgb = model_xgb.predict_proba(test_for_model)[:,1]","5a8b7186":"# creating datasets for our meta-classifier - using all features\n\nblend_x_val = pd.concat([pd.DataFrame(X_val),pd.DataFrame(lgb_oof,columns=['lgbm']),\n                         pd.DataFrame(cat_oof,columns=['cat']),\n                         pd.DataFrame(xgb_oof,columns=['xgb'])], axis = 1)\nblend_x_test = pd.concat([pd.DataFrame(test_for_model),pd.DataFrame(predictions_lgb,columns=['lgbm']),\n                          pd.DataFrame(predictions_cb,columns=['cat'])\n                          ,pd.DataFrame(predictions_xgb,columns=['xgb'])], axis = 1)","e48a0187":"model = CatBoostClassifier(task_type='GPU',verbose=0)\n\nmodel.fit(blend_x_val,y_val)\npredictions = model.predict_proba(blend_x_test)[:,1]","cc459e44":"sub['claim'] = predictions\nsub.to_csv('submission_lgb_cb_cb_FE_blending.csv', index = 0) # Only for illustration","791162a7":"I have used an untuned Logistic Regression model, based on what I learnt from the following notebook-\nhttps:\/\/www.kaggle.com\/junhyeok99\/stacking-ensemble-for-beginner\n\nPlease do upvote it. It helped me implement and learn stacking, being the beginner that I am, for the very first time!","b3ec1a5a":"<div style=\"background-color:rgba(215, 79, 21, 0.5);\">\n    <h1><center>Importing Libraries<\/center><\/h1>\n<\/div>","161cbaf7":"References for parameter ranges -\n1. https:\/\/www.kaggle.com\/mlanhenke\/tps-09-optuna-study-catboostclassifier\n2. https:\/\/www.kaggle.com\/ranjeetshrivastav\/catboost-lightgbm\n\nPlease do upvote their work. I have used a combination from both these notebooks.","4f74d9d3":"<div style=\"background-color:rgba(215, 79, 21, 0.5);\">\n    <h1><center>Ensembling<\/center><\/h1>\n<\/div>","87ad6d83":"<div style=\"background-color:rgba(215, 79, 21, 0.5);\">\n    <h1><center>Blending<\/center><\/h1>\n<\/div>","ed569edd":"<div style=\"background-color:rgba(215, 79, 21, 0.5);\">\n    <h1><center>Please upvote if you liked my notebook! Thanks :)<\/center><\/h1>\n<\/div>","d09f6fff":"<div style=\"background-color:rgba(215, 79, 21, 0.5);\">\n    <h1><center>Data Processing and Feature Engineering<\/center><\/h1>\n<\/div>","4833d806":"<div style=\"background-color:rgba(215, 79, 21, 0.5);\">\n    <h1><center>Inputting Data<\/center><\/h1>\n<\/div>","bf180744":"<div style=\"background-color:rgba(215, 79, 21, 0.5);\">\n    <h1><center>Optuna on LightGBM<\/center><\/h1>\n<\/div>","dda5c91f":"<div style=\"background-color:rgba(215, 79, 21, 0.5);\">\n    <h1><center>Stacking<\/center><\/h1>\n<\/div>","f52da341":"<div style=\"background-color:rgba(215, 79, 21, 0.5);\">\n    <h1><center>Tuned Hyperparameters<\/center><\/h1>\n<\/div>","c04ec4b5":"<div style=\"background-color:rgba(215, 79, 21, 0.5);\">\n    <h1><center>Optuna on CatBoost<\/center><\/h1>\n<\/div>","c76a825b":"The inspiration for choosing the hyper-parameter ranges is - \nhttps:\/\/www.kaggle.com\/bextuychiev\/lgbm-optuna-hyperparameter-tuning-w-understanding. \n\nPlease do upvote his work and check out his other illuminating notebooks. They are great if you have just started out on Kaggle.\nThis is the first time I set the lgbm ranges after understanding the parameters. Thanks a lot Bex!"}}