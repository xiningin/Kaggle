{"cell_type":{"0f4a4d24":"code","3e4b21f7":"code","8787dc44":"code","2ece4734":"code","edceec3f":"code","2f1d655c":"code","fb2630e8":"code","0998eb7d":"code","810f87c4":"code","306dccfc":"code","8f187fef":"code","46d6af42":"code","d8726c52":"code","9a531057":"code","200ce70a":"code","9174c1a9":"code","9840ec4f":"code","5eaf4f4f":"code","21c8ae89":"code","da6dfddf":"code","60e396a3":"code","4cc9b130":"code","d39ffcea":"code","60309df7":"code","e0c4f98c":"code","f6b00ace":"code","3abb78d8":"code","fbaed3c2":"code","c7d7e88a":"code","4955ba9c":"code","308b03eb":"markdown","59773184":"markdown","b0bffb7b":"markdown","f607fb69":"markdown","4e6863da":"markdown","6c91c5fb":"markdown","8d987a1b":"markdown","81e6afce":"markdown","7ae2d04c":"markdown","95a817d3":"markdown","7806e97c":"markdown","dc3805db":"markdown","5616762d":"markdown","8e35ec5d":"markdown","07ff540d":"markdown","586415fd":"markdown","32d98afa":"markdown","e633684d":"markdown","27bba509":"markdown"},"source":{"0f4a4d24":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3e4b21f7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import accuracy_score,confusion_matrix\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsns.set()\n%matplotlib inline","8787dc44":"data = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')  #import dataset\n\ndata.head()  #top 5 rows","2ece4734":"print (data.shape) # no. of rows and coloumns\nprint (\"**-------------------------------------**\\n\")\nprint (data.info())  # information of all the coloumns\nprint (\"**-------------------------------------**\\n\")\nprint (data.describe()) # statistics of the data\nprint (\"**-------------------------------------**\")","edceec3f":"# Check null values\n\ndata.isnull().sum()","2f1d655c":"# Correlation Matrix\n\nsns.heatmap(data.corr(),cbar=False,cmap='PuOr',annot=True)","fb2630e8":"data.describe()","0998eb7d":"# Coloumns contain zero(0) values\n\ncol=['Glucose' ,'BloodPressure' ,'SkinThickness', 'Insulin' ,'BMI']\n\nfor i in col:\n  data[i].replace(0,data[i].mean(),inplace=True)","810f87c4":"print(\"A histogram is a classic visualization tool that represents the distribution of one or more variables by counting the number of observations that fall within disrete bins.\")\n\np=data.hist(figsize = (20,20)) # ploting histogram","306dccfc":"sns.scatterplot(x='Age',y='Insulin',data=data) ","8f187fef":"sns.boxplot(x='Outcome',y='Pregnancies',data=data)","46d6af42":"data.var()","d8726c52":"# Data Standardization\n\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX =  pd.DataFrame(sc_X.fit_transform(data.drop([\"Outcome\"],axis = 1),),\n        columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age'])\n\ny=data.Outcome\n\nprint(X)\nprint(y)","9a531057":"#splitting the dataset\n\nfrom sklearn.model_selection import train_test_split        \nX_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size=.30,random_state=3)","200ce70a":"from sklearn.linear_model import LogisticRegression\nreg = LogisticRegression(C=1,penalty='l2')\nreg.fit(X_train,Y_train)\n\nlog_acc=accuracy_score(Y_test,reg.predict(X_test))\n\n\nprint(\"Train Set Accuracy:\"+str(accuracy_score(Y_train,reg.predict(X_train))*100))\nprint(\"Test Set Accuracy:\"+str(accuracy_score(Y_test,reg.predict(X_test))*100))\n\n#Train Set Accuracy:78.77094972067039\n#Test Set Accuracy:74.45887445887446","9174c1a9":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=9)                #knn classifier\nknn.fit(X_train,Y_train)\n\nknn_acc = accuracy_score(Y_test,knn.predict(X_test))\n\n\nprint(\"Train Set Accuracy:\"+str(accuracy_score(Y_train,knn.predict(X_train))*100))\nprint(\"Test Set Accuracy:\"+str(accuracy_score(Y_test,knn.predict(X_test))*100))\n\n#Train Set Accuracy:82.12290502793296\n#Test Set Accuracy:71.42857142857143","9840ec4f":"from sklearn.svm import SVC\n\nsvm = SVC()\nsvm.fit(X_train,Y_train)    \n\nsvm_acc= accuracy_score(Y_test,svm.predict(X_test))\n\n\nprint(\"Train Set Accuracy:\"+str(accuracy_score(Y_train,svm.predict(X_train))*100))\nprint(\"Test Set Accuracy:\"+str(accuracy_score(Y_test,svm.predict(X_test))*100))\n\n#Train Set Accuracy:85.1024208566108\n#Test Set Accuracy:74.02597402597402","5eaf4f4f":"from sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier(criterion='entropy',max_depth=5)\ndtc.fit(X_train, Y_train)\n\n\ndtc_acc= accuracy_score(Y_test,dtc.predict(X_test))\n\nprint(\"Train Set Accuracy:\"+str(accuracy_score(Y_train,dtc.predict(X_train))*100))\nprint(\"Test Set Accuracy:\"+str(accuracy_score(Y_test,dtc.predict(X_test))*100))\n\n#Train Set Accuracy:83.42644320297951\n#Test Set Accuracy:72.2943722943723","21c8ae89":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbc = GradientBoostingClassifier()\ngbc.fit(X_train, Y_train)\n\n\ngbc_acc=accuracy_score(Y_test,gbc.predict(X_test))\n\nprint(\"Train Set Accuracy:\"+str(accuracy_score(Y_train,gbc.predict(X_train))*100))\nprint(\"Test Set Accuracy:\"+str(accuracy_score(Y_test,gbc.predict(X_test))*100))\n\n#Train Set Accuracy:94.22718808193669\n#Test Set Accuracy:74.02597402597402","da6dfddf":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(booster = 'gbtree', learning_rate = 0.1, max_depth=6,n_estimators = 10)\nxgb.fit(X_train,Y_train)\n\nxgb_acc= accuracy_score(Y_test,xgb.predict(X_test))\n\nprint(\"Train Set Accuracy:\"+str(accuracy_score(Y_train,xgb.predict(X_train))*100))\nprint(\"Test Set Accuracy:\"+str(accuracy_score(Y_test,xgb.predict(X_test))*100))","60e396a3":"from sklearn.model_selection import train_test_split                #splitting the dataset\n                                                                 \ntrain,val_train,test,val_test = train_test_split(X,y,test_size=.50,random_state=3)\nx_train,x_test,y_train,y_test = train_test_split(train,test,test_size=.20,random_state=3)","4cc9b130":"#first model\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(x_train, y_train)","d39ffcea":"# second model\nsvm = SVC()\nsvm.fit(x_train, y_train)","60309df7":"pred_1=knn.predict(val_train)\npred_2=svm.predict(val_train)\n\n# addition of 2 predictions\nresult = np.column_stack((pred_1,pred_2))","e0c4f98c":"pred_test1=knn.predict(x_test)\npred_test2=svm.predict(x_test)\n\n\npredict_test=np.column_stack((pred_test1,pred_test2))","f6b00ace":"# stacking classifier\n#RandomForestClasifier:- In this prediction of other 2 classification is taken as x value\nfrom sklearn.ensemble import RandomForestClassifier\nrand_clf = RandomForestClassifier()\nrand_clf.fit(result,val_test)         ","3abb78d8":"rand_clf.score(result,val_test)","fbaed3c2":"rand_acc=accuracy_score(y_test ,rand_clf.predict(predict_test))\nrand_acc","c7d7e88a":"models = pd.DataFrame({\n    'Model': ['Logistic','KNN', 'SVC',  'Decision Tree Classifier',\n             'Gradient Boosting Classifier',  'XgBoost','Stacking'],\n    'Score': [ log_acc,knn_acc, svm_acc, dtc_acc, gbc_acc, xgb_acc,rand_acc,]\n})\n\nmodels.sort_values(by = 'Score', ascending = False)","4955ba9c":"colors = [\"yellow\", \"green\", \"orange\", \"magenta\",\"blue\",\"black\"]\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,8))\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Algorithms\")\nsns.barplot(x=models['Model'],y=models['Score'], palette=colors )\nplt.show()","308b03eb":"#### Minimum value of the above columns cannnot be zero (0),value of zero does not make sense and thus indicates missing value.","59773184":"> #### **Data Description :-**\n> \n> ##### * Pregnancies: Number of times pregnant\n> ##### * Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n> ##### * BloodPressure: Diastolic blood pressure (mm Hg)\n> ##### * SkinThickness: Triceps skin fold thickness (mm)\n> ##### * Insulin: 2-Hour serum insulin (mu U\/ml)\n> ##### * BMI: Body mass index (weight in kg\/(height in m)^2)\n> ##### * DiabetesPedigreeFunction: Diabetes pedigree functionr\n> ##### * Age: Age (years)\n> ##### * Cabin : Cabin Number\n> ##### * Outcome: Class variable (0 or 1)","b0bffb7b":"### 5. GradientBoostingClassifier","f607fb69":"### 1. Logistic Regression","4e6863da":"# ***Data Collection and Analysis***","6c91c5fb":"#### ***Diabetes is a major metabolic disorder which can affect entire body system adversely. Undiagnosed diabetes can increase the risk of cardiac stroke, diabetic nephropathy and other disorders. All over the world millions of people are affected by this disease. Early detection of diabetes is very important to maintain a healthy life. This disease is a reason of global concern as the cases of diabetes are rising rapidly. Machine learning (ML) is a computational method for automatic learning from experience and improves the performance to make more accurate predictions. In the current research we have utilized machine learning technique in Pima Indian diabetes dataset to develop trends and detect patterns with risk factors using Python data manipulation tool snd classify whether the patients is diabetic and non-diabetic.***","8d987a1b":"# Thank You!\n# Let me know ways to improve my journey, it's just the begining!","81e6afce":"# **Seperating Features and Labels**","7ae2d04c":"### **Diabetes is a disease that occurs when your blood glucose, also called blood sugar, is too high. Blood glucose is your main source of energy and comes from the food you eat. Insulin, a hormone made by the pancreas, helps glucose from food get into your cells to be used for energy.**","95a817d3":"### 4. DecisionTreeClassifier","7806e97c":"# Importing Dependencies","dc3805db":"# **Training the Models**","5616762d":"# ***Diabetes Prediction(EDA & Models)***","8e35ec5d":"### 3. SVC","07ff540d":"![](http:\/\/www.pregnancylab.net\/wp-content\/uploads\/2020\/02\/6a0148c706506d970c022ad3a2d315200b-pi.jpg)","586415fd":"# ***Exploratory Data Analysis***","32d98afa":"### 6. XGBClassifier","e633684d":"### 7.Stacking\n\n##### Stacking is a way of ensembling classification or regression models it consists of two-layer estimators. The first layer consists of all the baseline models that are used to predict the outputs on the test datasets. The second layer consists of Meta-Classifier or Regressor which takes all the predictions of baseline models as an input and generate new predictions.","27bba509":"### 2. KNearestNeighbors"}}