{"cell_type":{"94896550":"code","f201f68c":"code","92624872":"code","b8e69d24":"code","4508d27d":"code","84dc8a67":"code","67718e25":"code","d3c8645e":"code","948a3a47":"code","41950024":"code","cf60efe4":"code","e656e7de":"code","d674899d":"code","8a6ef07d":"code","711c754b":"code","a34c3efb":"code","eb74f1f5":"code","c457c9fb":"code","8195efa8":"code","ded341d3":"code","90b8697b":"code","c833ecd3":"code","0bd3f517":"code","627088cb":"code","331e5c90":"code","f95ea9cc":"code","816dd4ff":"code","c157c694":"code","1f691a57":"code","849fdf59":"code","25dacea0":"code","9c217141":"code","8fe2d65c":"code","607dccd8":"code","0473d774":"code","2e58d19b":"code","43082e84":"code","191dd28c":"code","8b156f9e":"code","f448c786":"code","2b64a4c6":"markdown","33fc7ca1":"markdown","3b07562c":"markdown","db710612":"markdown","969de793":"markdown","38893eb7":"markdown","325a99a3":"markdown","fe75247e":"markdown","096cad70":"markdown","fe2daf96":"markdown","43b5319b":"markdown","b38ea71f":"markdown","bfd5ef5a":"markdown","cc49cf03":"markdown","89586b13":"markdown","c0ce92ad":"markdown","4bf1af2b":"markdown","c1f0848b":"markdown","f2e919e4":"markdown","88bddd9e":"markdown","d5b1bef8":"markdown","51044785":"markdown","3aece1dd":"markdown","13f29dbb":"markdown","ebdd6580":"markdown","58da96a7":"markdown","186e5e9a":"markdown","057d255c":"markdown","fe22de9a":"markdown","36a34ab3":"markdown","7da8b823":"markdown","ae88bb77":"markdown","5f399d44":"markdown","a50dfbec":"markdown","ead60b61":"markdown","9c28d1e5":"markdown","719c838d":"markdown","84a0eba7":"markdown","a7bd1b7f":"markdown","f3e2e0ce":"markdown","215e321c":"markdown","b04988ae":"markdown","16aba860":"markdown","abe646f2":"markdown","42285c94":"markdown","cb3499b0":"markdown","fa13fd53":"markdown","c389dcdf":"markdown","8a0e4f6b":"markdown","9f72a29f":"markdown","6f05c82b":"markdown","ab900429":"markdown","04cace61":"markdown","5c06b20f":"markdown","20b5f96e":"markdown","e0d413a3":"markdown","84983543":"markdown","815a8716":"markdown"},"source":{"94896550":"# First up, I'll import every library that will be used in this project is imported at the start.\n\n# Data handling and processing\nimport pandas as pd\nimport numpy as np\n\n# Data visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Statistics\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom scipy.stats import randint as sp_randint\nfrom time import time\n\n# NLP\nimport nltk\nnltk.download('wordnet')\nimport re\nfrom textblob import TextBlob\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report","f201f68c":"#\u00a0Reading in data\ndata = pd.read_csv('..\/input\/Womens Clothing E-Commerce Reviews.csv')\ndata = data[['Clothing ID', 'Review Text', 'Recommended IND']]\ndata.columns = ['EmployeeID', 'Review Text', 'Recommend']","92624872":"# Inspecting the variables\ndata.info()","b8e69d24":"# Replacing blank variables with 'unknown' ready for processing\ndata['Review Text'].fillna('unknown', inplace=True)","4508d27d":"# Importing SKLearn's list of stopwords and then appending with my own words \nstop = text.ENGLISH_STOP_WORDS\n\n# Basic text cleaning function\ndef remove_noise(text):\n    \n    # Make lowercase\n    text = text.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n    \n    #\u00a0Remove whitespaces\n    text = text.apply(lambda x: \" \".join(x.strip() for x in x.split()))\n    \n    # Remove special characters\n    text = text.apply(lambda x: \"\".join([\" \" if ord(i) < 32 or ord(i) > 126 else i for i in x]))\n    \n    # Remove punctuation\n    text = text.str.replace('[^\\w\\s]', '')\n    \n    # Remove numbers\n    text = text.str.replace('\\d+', '')\n    \n    # Remove Stopwords\n    text = text.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n    \n    # Convert to string\n    text = text.astype(str)\n        \n    return text","84dc8a67":"# Applying noise removal function to data\ndata['Filtered Review Text'] = remove_noise(data['Review Text'])\ndata.head()","67718e25":"#\u00a0Defining a sentiment analyser function\ndef sentiment_analyser(text):\n    return text.apply(lambda Text: pd.Series(TextBlob(Text).sentiment.polarity))\n\n# Applying function to reviews\ndata['Polarity'] = sentiment_analyser(data['Filtered Review Text'])\ndata.head(10)","d3c8645e":"# Instantiate the Word tokenizer & Word lemmatizer\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\n# Define a word lemmatizer function\ndef lemmatize_text(text):\n    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n\n# Apply the word lemmatizer function to data\ndata['Filtered Review Text'] = data['Filtered Review Text'].apply(lemmatize_text)\ndata.head()","948a3a47":"# Getting a count of words from the documents\n# Ngram_range is set to 1,2 - meaning either single or two word combination will be extracted\ncvec = CountVectorizer(min_df=.005, max_df=.9, ngram_range=(1,2), tokenizer=lambda doc: doc, lowercase=False)\ncvec.fit(data['Filtered Review Text'])","41950024":"# Getting the total n-gram count\nlen(cvec.vocabulary_)","cf60efe4":"# Creating the bag-of-words representation\ncvec_counts = cvec.transform(data['Filtered Review Text'])\nprint('sparse matrix shape:', cvec_counts.shape)\nprint('nonzero count:', cvec_counts.nnz)\nprint('sparsity: %.2f%%' % (100.0 * cvec_counts.nnz \/ (cvec_counts.shape[0] * cvec_counts.shape[1])))","e656e7de":"# Instantiating the TfidfTransformer\ntransformer = TfidfTransformer()\n\n# Fitting and transforming n-grams\ntransformed_weights = transformer.fit_transform(cvec_counts)\ntransformed_weights","d674899d":"# Getting a list of all n-grams\ntransformed_weights = transformed_weights.toarray()\nvocab = cvec.get_feature_names()\n\n# Putting weighted n-grams into a DataFrame and computing some summary statistics\nmodel = pd.DataFrame(transformed_weights, columns=vocab)\nmodel['Keyword'] = model.idxmax(axis=1)\nmodel['Max'] = model.max(axis=1)\nmodel['Sum'] = model.drop('Max', axis=1).sum(axis=1)\nmodel.head(10)","8a6ef07d":"# Merging td-idf weight matrix with original DataFrame\nmodel = pd.merge(data, model, left_index=True, right_index=True)","711c754b":"# Printing the first 10 reviews left\nmodel.head(10)","a34c3efb":"# Getting a view of the top 20 occurring words\nocc = np.asarray(cvec_counts.sum(axis=0)).ravel().tolist()\ncounts_df = pd.DataFrame({'Term': cvec.get_feature_names(), 'Occurrences': occ})\ncounts_df.sort_values(by='Occurrences', ascending=False).head(25)","eb74f1f5":"# Getting a view of the top 20 weights\nweights = np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()\nweights_df = pd.DataFrame({'Term': cvec.get_feature_names(), 'Weight': weights})\nweights_df.sort_values(by='Weight', ascending=False).head(25)","c457c9fb":"# Plotting overall recommendations and getting value counts\nfig = plt.figure(figsize = (10,5))\nsns.countplot(x='Recommend', data = model)\n\nprint(data['Recommend'].value_counts())","8195efa8":"# Visualising polarity between recommending and non-recommending customers, then getting value counts\ng = sns.FacetGrid(model, col=\"Recommend\", col_order=[1, 0])\ng = g.map(plt.hist, \"Polarity\", bins=20, color=\"g\")\n\nrecommend = model.groupby(['Recommend'])\nrecommend['Polarity'].mean()","ded341d3":"# Get a list of columns for deletion\nmodel.columns","90b8697b":"# Drop all columns not part of the text matrix\nml_model = model.drop(['EmployeeID', 'Review Text', 'Filtered Review Text', 'Polarity', 'Keyword', 'Max', 'Sum'], axis=1)\n\n# Create X & y variables for Machine Learning\nX = ml_model.drop('Recommend', axis=1)\ny = ml_model['Recommend']\n\n# Create a train-test split of these variables\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)","c833ecd3":"# Defining a function to fit and predict ML algorithms\ndef model(mod, model_name, x_train, y_train, x_test, y_test):\n    mod.fit(x_train, y_train)\n    print(model_name)\n    acc = cross_val_score(mod, X_train, y_train, scoring = \"accuracy\", cv = 5)\n    predictions = cross_val_predict(mod, X_train, y_train, cv = 5)\n    print(\"Accuracy:\", round(acc.mean(),3))\n    cm = confusion_matrix(predictions, y_train)\n    print(\"Confusion Matrix:  \\n\", cm)\n    print(\"                    Classification Report \\n\",classification_report(predictions, y_train))","0bd3f517":"# 1. Gaussian Naive Bayes\ngnb = GaussianNB()\nmodel(gnb, \"Gaussian Naive Bayes\", X_train, y_train, X_test, y_test)","627088cb":"# 2. Random Forest Classifier\nran = RandomForestClassifier(n_estimators=50)\nmodel(ran, \"Random Forest Classifier\", X_train, y_train, X_test, y_test)","331e5c90":"# 3. Logistic Regression\nlog = LogisticRegression()\nmodel(log, \"Logistic Regression\", X_train, y_train, X_test, y_test)","f95ea9cc":"# 4. Linear SVC\nsvc = LinearSVC()\nmodel(svc, \"Linear SVC\", X_train, y_train, X_test, y_test)","816dd4ff":"# Import the hopeful solution to our problems\nfrom imblearn.over_sampling import SMOTE\nsmote=SMOTE()","c157c694":"# Setting up new variables for ML\nX_sm, y_sm = smote.fit_sample(X,y)\n\nX_train_sm, X_test_sm, y_train_sm, y_test_sm = train_test_split(X_sm, y_sm, test_size=0.3, random_state=100)","1f691a57":"# Defining a new function with revised inputs for the new SMOTE variables\ndef model_sm(mod, model_name, x_train_sm, y_train_sm, x_test_sm, y_test_sm):\n    mod.fit(x_train_sm, y_train_sm)\n    print(model_name)\n    acc = cross_val_score(mod, X_train_sm, y_train_sm, scoring = \"accuracy\", cv = 5)\n    predictions = cross_val_predict(mod, X_train_sm, y_train_sm, cv = 5)\n    print(\"Accuracy:\", round(acc.mean(),3))\n    cm = confusion_matrix(predictions, y_train_sm)\n    print(\"Confusion Matrix:  \\n\", cm)\n    print(\"                    Classification Report \\n\",classification_report(predictions, y_train_sm))","849fdf59":"# 1. Gaussian Naive Bayes\ngnb = GaussianNB()\nmodel_sm(gnb, \"Gaussian Naive Bayes\", X_train_sm, y_train_sm, X_test_sm, y_test_sm)","25dacea0":"# 2. Random Forest Classifier\nran = RandomForestClassifier(n_estimators=50)\nmodel_sm(ran, \"Random Forest Classifier\", X_train_sm, y_train_sm, X_test_sm, y_test_sm)","9c217141":"# 3. Logistic Regression\nlog = LogisticRegression()\nmodel_sm(log, \"Logistic Regression\", X_train_sm, y_train_sm, X_test_sm, y_test_sm)","8fe2d65c":"# 4. Linear SVC\nsvc = LinearSVC()\nmodel_sm(svc, \"Linear SVC\", X_train_sm, y_train_sm, X_test_sm, y_test_sm)","607dccd8":"# Creating a plot for feature importance\ndef importance_plotting(data,x,y,palette,title):\n    sns.set(style=\"whitegrid\")\n    ft = sns.PairGrid(data,y_vars=y,x_vars=x,size=5,aspect=1)\n    ft.map(sns.stripplot,orient='h',palette=palette, edgecolor=\"black\",size=15)\n    for ax, title in zip(ft.axes.flat, titles):\n        \n    # Set a different title for each axes\n        ax.set(title=title)\n        \n    # Make the grid horizontal instead of vertical\n        ax.xaxis.grid(False)\n        ax.yaxis.grid(True)\n\n    plt.show()","0473d774":"# Compile arrays of columns (words) and feature importances\nfi = {'Words':ml_model.drop('Recommend',axis=1).columns.tolist(),'Importance':ran.feature_importances_}\n\n# Bung these into a dataframe, rank highest to lowest then slice top 20\nImportance = pd.DataFrame(fi,index=None).sort_values('Importance',ascending=False).head(25)\n\n# Plot the graph!\ntitles = [\"Top 25 most important words in predicting product recommendation\"]\nimportance_plotting(Importance,'Importance','Words','Greens_r',titles)","2e58d19b":"# Getting prediction probabilities\ny_scores = ran.predict_proba(X_train_sm)\ny_scores = y_scores[:,1]\n\nprecision, recall, threshold = precision_recall_curve(y_train_sm, y_scores)","43082e84":"# Defining a new function to plot the precision-recall curve\ndef plot_precision_and_recall(precision, recall, threshold):\n    plt.plot(threshold, precision[:-1], \"r-\", label=\"precision\", linewidth=5)\n    plt.plot(threshold, recall[:-1], \"b\", label=\"recall\", linewidth=5)\n    plt.xlabel(\"Threshold\", fontsize=19)\n    plt.legend(loc=\"upper right\", fontsize=19)\n    plt.ylim([0, 1])\n\nplt.figure(figsize=(14, 7))\nplot_precision_and_recall(precision, recall, threshold)\nplt.show()","191dd28c":"# Compute the true positive and false positive rate\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_train_sm, y_scores)","8b156f9e":"# Plotting the true positive and false positive rate\ndef plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'r', linewidth=4)\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate (FPR)', fontsize=16)\n    plt.ylabel('True Positive Rate (TPR)', fontsize=16)\n\nplt.figure(figsize=(14, 7))\nplot_roc_curve(false_positive_rate, true_positive_rate)\nplt.show()","f448c786":"# Computing the ROC-AUC score\nr_a_score = roc_auc_score(y_train_sm, y_scores)\nprint(\"ROC-AUC-Score:\", r_a_score)","2b64a4c6":"## 4. Machine Learning","33fc7ca1":"### Algorithms round 1","3b07562c":"A binary variable 'Recommended' will be the focus of upcoming Machine Learning prediction. Let's see how the categories stack up:","db710612":"I am happy with that number as a starting point, less than 1000 was my initial aim. If I wanted to be more or less restrictive on n-gram selection, I could adjust the 'min_df' and 'max_df' parameters within my CountVectorizer, which controls for the minimum and maximum amount of documents each word should feature in.\n\nWe can now tackle the next step, which is to turn this document into a <b>'bag of words' representation<\/b>. This creates a separate column for each term that contains the count within each document. After that, we\u2019ll take a look at the <b>sparsity<\/b> of this representation which lets us know how many <b>nonzero values<\/b> there are in the dataset. The more sparse the data is the more challenging it will be to model, but that\u2019s a discussion for another day:","969de793":"### Set-up","38893eb7":"### ROC-AUC score","325a99a3":"## 1. Text Preprocessing","fe75247e":"Great, we have our weighted words! Just a few more steps required (below); I'm going to extract all of the feature names (which are the n-grams) and put these into a DataFrame along with the corresponding weights per review. Then I am going to add in at the end some summary statistics to understand per review:\n\n- The highest weighted word\n- The weight of this word\n- The total weighting per review.","096cad70":"### Assessing feature importance","fe2daf96":"Before moving onto lexicon normalisation, I want to gain a sense of the sentiment per review. I don't intend to use this for any machine learning purposes, more-so out of interest to understand whether reviews lean towards positivity or negativity. I'll come back to this again later.","43b5319b":"# Machine Learning with text data","b38ea71f":"### Precision-Recall Curve","bfd5ef5a":"### Merging datasets","cc49cf03":"## 6. Conclusion","89586b13":"It looks as though there is a definite positive lean throughout these most popular words, such as 'love', 'perfect' and 'flattering'. Now, let's see if the 20 highest weighted words throws up a similar list:","c0ce92ad":"Job done! In this kernel we have:\n\n- Started with an unstructured table of 23,000 clothing text reviews and corresponding Recommend vs Not Recommend classifications.\n- Taken the text reviews, cleaned them and built them into a matrix\n- Briefly explored the data before applying initial Machine Learning Algorithms\n- Balanced the unvenly weighted target variables before re-running the same Algorithms with improved findings\n- Identified the best performing model (Random Forest Classifier), explored it's most important features and computed further Precision\/Recall metrics (including ROC-AUC).\n- Achieved a final accuracy score above 90% on the model's training dataset, and an ROC-AUC score above 99%.","4bf1af2b":"Upon first inspection, it looks like there are some missing reviews fill in.","c1f0848b":"For each person the Random Forest algorithm classifies, it computes a probability based on a function and it classifies the review as 'Recommended' (when the score is bigger than the <b>threshold<\/b>) or as 'Not Recommended' (when the score is smaller than the <b>threshold<\/b>). This information can be displayed visually as a 'Presicion-Recall curve', which has usefulness in allowing us to tailor n algorithm to more exact precision and recall requirements. The below code will generate this for us:","f2e919e4":"### Lexicon Normalisation","88bddd9e":"Perhaps I will return to some or all of these points at a later date :). For now however, <b>thank you<\/b> for reading this kernel! Please do feel free to share with me your thoughts, feedback and any suggestions for improvement - I am always willing to learn new or more efficient techniques! Cheers. ","d5b1bef8":"# 3. Data Exploration","51044785":"While a lot of the statistics that I created earlier on such as Polarity and Keyword are interesting, I want to focus model predictions purely on the weighted text matrix. So, anything that doesn't feature as part of this will now be removed from the DataFrame.","3aece1dd":"### ROC-AUC Curve","13f29dbb":"Another type of textual noise is about the multiple representations exhibited by single word.\n\nFor example \u2013 \u201cplay\u201d, \u201cplayer\u201d, \u201cplayed\u201d, \u201cplays\u201d and \u201cplaying\u201d are the different variations of the word \u2013 \u201cplay\u201d. Though they mean different things, contextually they all are similar. This step converts all the disparities of a word into their normalized form (also known as lemma). Normalization is a pivotal step for feature engineering with text as it converts the high dimensional features (N different features) to the low dimensional space (1 feature), which is an ideal ask for any ML model.\n\nThere are two methods of lexicon normalisation; Stemming or Lemmatization. I will opt for Lemmatization, as this will return the root form of each word (rather than just stripping suffixes, which is stemming).","ebdd6580":"In this, my first Machine Learning project working with text data and applying Natural Language Processing (NLP) techniques, I will aim to predict with words alone whether or not a customer would recommend a purchased item of clothing. Other interesting questions within this kernel include: Which emotive words are most popular? Which aspect of the product is most important - the fit, colour, texture, price? And morever, what would such information mean for a retailer looking to best align their strategy to market demand? I'm not sure about you, but that's enough to spike my interest! There are 23,000 reviews and a binary target variable for overall product recommendation - let's crack on.","58da96a7":"The red line represents a purely random classifier (e.g a coin flip), so the aim is for our classifier (represented by the blue ROC curve) to be as far away from it as possible. This space under the curve is known as the AUC, and therefore a larger AUC space is indicative of a better model. For this Random Forest Classifier model, the graph therefore indicates very strong model performance (hoo-rah)!","186e5e9a":"Backing up the graph, a very strong ROC-AUC Score of over 99% has been achieved by this model.","057d255c":"The optimum Precision\/Recall threshold here looks within the region of 0.7, thereafter Recall plummets eventually down to near zero.","fe22de9a":"We're in business; a drastic improvement on Precision & Recall across all algorithms, even complemented with improved overall acccuracy on a couple as well. It's great to see the Naive Bayes Classifier now up at 85%, as this algorithm is typically well suited to crunching text data. However now exceeding the 90% bracket is the Random Forest Classifier, also boasting great Precision & Recall values too.","36a34ab3":"### Quick peak at the target variable","7da8b823":"Now let's read in the data, all 23,000 clothing reviews. I'm only interested in three columns within this project, which have been seelcted below.","ae88bb77":"In terms of how this kernel could be extended or improved, we could:\n\n- Take a more rigorous approach to mining the text data, such as categorising products, controlling for spelling errors \/ using more advanced modelling techniques such as topic modelling\n- Make better use of the Polarity metric within Machine Learning\n- Apply a more extensive list of algorithms (including Deep Learning) to the text data\n- Apply GridSearchCV or RandomisedSearchCV to optimise the final model\n- Create an ensemble of models for better prediction","5f399d44":"Now that we have a td-idf weight matrix, this can be fed directly into a predictive model. Before we do this, let's explore the current data a little more:","a50dfbec":"To help with Machine Learning I will define a function that will return the most prized statistics in one go. After initiating a model, this function will return an mean accuracy score following 5 folds of cross validation - this is to ensure that we are getting a smoothed out representation of both the training and test sets. Next this function will provide us with the Confusion Matrix; how many correct vs incorrect classifications have actually taken place within the given model? Last up, this function will churn out for us a Classification Report which details other important metrics such as Precision, Recall, the F1 score (which is just the harmonic mean of the former two), and support (which is the classification count). \n\nCombined, these metrics will provide rich insight into individual model performance and will guide better selection towards the best performing model, and how best to optimise it.","ead60b61":"## 5. Best Model: Random Forest Classifier","9c28d1e5":"Comparing the above two graphs, it appears as though Polarity can recognise that reviews left by customers who recommended their product gave more positive reviews, compared to those who did not recommend. This bodes well for using our text matrix to predict recommendations.","719c838d":"### Checking out the most salient words","84a0eba7":"Lastly, we will compute the size of this AUC space, known as the ROC-AUC score.","a7bd1b7f":"You will recall that I pulled a Polarity statistic during Step 1; let's see how this shapes up per target outcome:","f3e2e0ce":"Text is the most unstructured form of all the available data, therefore various types of noise are present in it. This means that the data is not readily analysable without any pre-processing. The entire process of cleaning and standardization of text, making it noise-free and ready for analysis is known as text preprocessing. This usually comprises two key steps:\n<br>\n1. Noise Removal\n2. Lexicon Normalisation","215e321c":"Any piece of text which is not relevant to the context of the data and the end-output can be specified as the noise.\n\nFor example \u2013 language stopwords (commonly used words of a language \u2013 is, am, the, of, in etc), URLs or links, punctuations and industry specific words. This step deals with removal of all types of noisy entities present in the text.\n\nFollowing is a python function to strip out noise throughout the reviews:","b04988ae":"### Algorithms round 2","16aba860":"To analyse a preprocessed data, it needs to be converted into features. Depending upon the usage, text features can be constructed using a variety of techniques \u2013 in this kernel I will be converting the data into statistical features.\n\nThe specific model in question is known as <b>'Term Frequency \u2013 Inverse Document Frequency' (TF \u2013 IDF)<\/b>\n\nTF-IDF is a weighted model commonly used for information retrieval problems. It aims to convert the text documents into vector models on the basis of occurrence of words in the documents without taking considering the exact ordering. For Example \u2013 let say there is a dataset of N text documents, In any document \u201cD\u201d, TF and IDF will be defined as \u2013\n\n- <b>Term Frequency (TF)<\/b> \u2013 TF for a term \u201ct\u201d is defined as the count of a term \u201ct\u201d in a document \u201cD\u201d\n- <b>Inverse Document Frequency (IDF)<\/b> \u2013 IDF for a term is defined as logarithm of ratio of total documents available in the corpus and number of documents containing the term T.\n- <b>TF . IDF<\/b> \u2013 TF IDF formula gives the relative importance of a term in a corpus (list of documents), given by the following formula below. Following is the code using python\u2019s scikit learn package to convert a text into tf idf vectors:","abe646f2":"### Noise Removal","42285c94":"Four models down, and we're up to 88% Accuracy with the Linear SVC. However, there seems to be an issue with Class 0 (would not recommend) across all trained models; looking at the LinearSVC both Precision & Recall are low, contributing towards a mediocre 0.60 F1 score. \n\nMy sense is this is due to class imbalance. As depicted earlier, there are around five times fewer 0 classifications compared to 1 classifications, which can sometimes be problematic within Machine Learning, whereby modelling generally tends to work better when there is an almost equal numbers of samples from each class in the target variable. With this dataset, we're way off that. \n\nTo overcome this problem, we can either DownSample the majority Class or UpSample the minority class. In this notebook I will use an Oversampling technique from the handy SMOTE library. Let's apply this technique and then re-run all four models to re-assess precision & recall performance.","cb3499b0":"Function defined - now let's get training! I'm only going to focus on four algorithms in this kernel, starting off with:","fa13fd53":"I've enjoyed working with this dataset and have learned heaps around using text data to predict an outcome - given this has been my first exposure to working with text, the buzz in being able to do just this has been great! It has also been an enjoyable experience using this data to predict genuinely meaningful and useful insights, such as knowing what consumers are looking for in a clothing product, and what matters most to them.","c389dcdf":"Given that the Random Forest now heads the pack in terms of accuracy, let's proceed with some further exploration and perhaps a spot of optimisation too. I'll begin with visualising the top predictive features from the Random Forest Classifier:","8a0e4f6b":"Taking the win by a clear margin as the strongest predictor of product recommendation is the word 'love'. This shouldn't come as too much of a surprise given that 'love' is a highly emotive word that conveys a larger feeling of positivity, as opposed to 'like' or 'nice', for example. Not just positive words but also negative ones such as 'disappointed' and 'unfortunately' have been useful in the prediction. \n\nOf particular interest to a retailer <i>might<\/i> be words such as:\n- Soft \/ Comfortable: Revealing the importance of texture, and how the clothes actually feel\n- Fit \/ Size: Both similar in nature and high ranking, the product needs to fit well.\n- Cheap: This word features in the top list, but quite far down the list! How important is price when reviewing a product at this retailer?","9f72a29f":"With the weighted text matrix already created, there is no pressing need for any further preprocessing\/engineering on these features. Therefore I will dive straight into Machine Learning.","6f05c82b":"Scikit-learn provides two methods to get to our end result (a TD-IDF weight matrix). One is a two-part process of using the CountVectorizer class to count how many times each term shows up in each document, followed by the TfidfTransformer class generating the weight matrix. The other does both steps in a single TfidfVectorizer class. In this Kernel I will proceed with method one; below is step one:","ab900429":"Now that we have term counts for each document, the TfidfTransformer can be applied to calculate the weights for each term in each document:","04cace61":"# 2. Getting a text matrix","5c06b20f":"Let's see which words are most frequent throughout the matrix:","20b5f96e":"There are nearly four times as many recommendations than there are non-recommendations. We'll need to keep a note of this as this could throw up a few issues when training algorithms - i'll touch on this in a little while.","e0d413a3":"> #### Can it be predicted whether or not a customer would recommend a purchased item of clothing based on solely their written review?","84983543":"Largely similar, albeit a slight difference in ranking. The reviews are looking good for this retailer currently! Let's find out a little more about this by inspecting the chosen target variable.","815a8716":"AUC is a metric for binary classification.\n\nAccuracy deals with ones and zeros, meaning you either got the class label right or you didn\u2019t. But many classifiers are able to quantify their uncertainty about the answer by outputting a probability value. To compute accuracy from probabilities you need a threshold to decide when zero turns into one. The most natural threshold is of course 0.5.\n\nLet\u2019s suppose you have a quirky classifier. It is able to get all the answers right, but it outputs 0.7 for negative examples and 0.9 for positive examples. Clearly, a threshold of 0.5 won\u2019t get you far, but 0.8 would be just perfect.\n\nThat\u2019s the whole point of using AUC - it considers all possible thresholds. Various thresholds result in different true positive\/false positive rates. As you decrease the threshold, you get more true positives, but also more false positives. The relation between them can be plotted:"}}