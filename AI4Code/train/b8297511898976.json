{"cell_type":{"7e06abfc":"code","b48e3042":"code","7da3c966":"code","5629f05b":"code","b974ee0b":"code","e2f36341":"code","cd4ca817":"code","34bf677f":"code","7c3e0d42":"code","af8cdf64":"code","80f88005":"code","56721b4c":"code","9adc85e1":"code","77185c82":"code","d279c401":"code","cd436f0f":"code","8dbf20ec":"code","c7c5f2d4":"code","96ae9e85":"code","95448bd3":"code","9c5f2403":"code","1b503e31":"code","69d2cb21":"code","86b2813b":"markdown","16bc0645":"markdown","987f28a4":"markdown","4db21488":"markdown","1db8b21b":"markdown","49ae0004":"markdown","bcfacd50":"markdown","02fd0105":"markdown","830f5c49":"markdown","88472ee8":"markdown"},"source":{"7e06abfc":"!pip install mediapipe opencv-python","b48e3042":"import mediapipe as mp # Import mediapipe\nimport cv2 # Import opencv\nimport csv\nimport os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport pickle","7da3c966":"mp_drawing = mp.solutions.drawing_utils # For Drawing the cordinates\nmp_hands = mp.solutions.hands # Solution specific for hand coordinate","5629f05b":"cap = cv2.VideoCapture(0)\nwith mp_hands.Hands(min_detection_confidence=0.5,min_tracking_confidence=0.5) as hands:\n    while cap.isOpened():\n        success, image = cap.read()\n\n        # Flip the image horizontally for a later selfie-view display, and convert\n        # the BGR image to RGB.\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        # To improve performance, optionally mark the image as not writeable to\n        # pass by reference.\n        image.flags.writeable = False\n        results = hands.process(image)\n\n        # Draw the hand annotations on the image.\n        image.flags.writeable = True\n        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n\n\n\n#         image_height, image_width, _ = image.shape\n\n        if results.multi_hand_landmarks:\n            for hand_landmarks in results.multi_hand_landmarks:\n                mp_drawing.draw_landmarks(\n                    image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n        cv2.imshow('MediaPipe Hands', image)\n        if cv2.waitKey(10) & 0xFF == ord('q'):\n            break\ncap.release()\n# cv2.destroyAllWindows()","b974ee0b":"import csv\nimport os\nimport numpy as np","e2f36341":"num_coords = len(hand_landmarks.landmark)\nnum_coords","cd4ca817":"landmarks = ['class']\nfor val in range(1, num_coords+1):\n    landmarks += ['x{}'.format(val), 'y{}'.format(val), 'z{}'.format(val)]","34bf677f":"landmarks","7c3e0d42":"with open('coords.csv', mode='w', newline='') as f:\n    csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n    csv_writer.writerow(landmarks)","af8cdf64":"class_name = \"Three\"","80f88005":"cap = cv2.VideoCapture(0)\nwith mp_hands.Hands(min_detection_confidence=0.5,min_tracking_confidence=0.5) as hands:\n    while cap.isOpened():\n        success, image = cap.read()\n        \n\n        # Flip the image horizontally for a later selfie-view display, and convert\n        # the BGR image to RGB.\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        # To improve performance, optionally mark the image as not writeable to\n        # pass by reference.\n        image.flags.writeable = False\n        results = hands.process(image)\n\n        # Draw the hand annotations on the image.\n        image.flags.writeable = True\n        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n\n\n\n        image_height, image_width, _ = image.shape\n\n        if results.multi_hand_landmarks:\n            for hand_landmarks in results.multi_hand_landmarks:\n                mp_drawing.draw_landmarks(\n                    image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n        \n        try:\n            hand = hand_landmarks.landmark\n            hand_row = list(np.array([[landmark.x, landmark.y, landmark.z] for landmark in hand]).flatten())\n            \n            row = hand_row\n            \n            # Append class name \n            row.insert(0, class_name)\n            \n            # Export to CSV\n            with open('coords.csv', mode='a', newline='') as f:\n                csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n                csv_writer.writerow(row) \n            \n        except:\n            pass\n            \n   \n        cv2.imshow('MediaPipe Hands 4', image)\n        if cv2.waitKey(10) & 0xFF == ord('q'):\n            break\ncap.release()\ncv2.destroyAllWindows()","56721b4c":"import pandas as pd\nfrom sklearn.model_selection import train_test_split","9adc85e1":"df = pd.read_csv('coords.csv')\n\nX = df.drop('class', axis=1) # features\ny = df['class'] # target value\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)","77185c82":"\nfrom sklearn.pipeline import make_pipeline \nfrom sklearn.preprocessing import StandardScaler \n\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier","d279c401":"\npipelines = {\n    'lr':make_pipeline(StandardScaler(), LogisticRegression()),\n    'rc':make_pipeline(StandardScaler(), RidgeClassifier()),\n    'rf':make_pipeline(StandardScaler(), RandomForestClassifier()),\n    'gb':make_pipeline(StandardScaler(), GradientBoostingClassifier()),\n}","cd436f0f":"fit_models = {}\nfor algo, pipeline in pipelines.items():\n    model = pipeline.fit(X_train, y_train)\n    fit_models[algo] = model","8dbf20ec":"fit_models['rc'].predict(X_test)","c7c5f2d4":"from sklearn.metrics import accuracy_score # Accuracy metrics \nimport pickle","96ae9e85":"\nfor algo, model in fit_models.items():\n    yhat = model.predict(X_test)\n    print(algo, accuracy_score(y_test, yhat))","95448bd3":"fit_models['rf'].predict(X_test)","9c5f2403":"\nwith open('Numbers.pkl', 'wb') as f:\n    pickle.dump(fit_models['rf'], f)","1b503e31":"with open('Numbers.pkl', 'rb') as f:\n    model = pickle.load(f)","69d2cb21":"cap = cv2.VideoCapture(0)\nwith mp_hands.Hands(min_detection_confidence=0.5,min_tracking_confidence=0.5) as hands:\n    while cap.isOpened():\n        success, image = cap.read()\n       \n        # Flip the image horizontally for a later selfie-view display, and convert\n        # the BGR image to RGB.\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        # To improve performance, optionally mark the image as not writeable to\n        # pass by reference.\n        image.flags.writeable = False\n        results = hands.process(image)\n\n        # Draw the hand annotations on the image.\n        image.flags.writeable = True\n        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n\n\n\n#         image_height, image_width, _ = image.shape\n\n        if results.multi_hand_landmarks:\n            for hand_landmarks in results.multi_hand_landmarks:\n                mp_drawing.draw_landmarks(\n                    image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n        try:\n            hand = hand_landmarks.landmark\n            hand_row = list(np.array([[landmark.x, landmark.y, landmark.z] for landmark in hand]).flatten())\n\n            row = hand_row\n#             print(row)\n\n\n            X = pd.DataFrame([row])\n            hand_language_class = model.predict(X)[0]\n            hand_language_prob = model.predict_proba(X)[0]\n            print(hand_language_class, hand_language_prob)\n\n\n\n            coords = tuple(np.multiply(\n                            np.array(\n                                (hand_landmarks.landmark[mp_hands.HandLandmark.WRIST].x, \n                                 hand_landmarks.landmark[mp_hands.HandLandmark.WRIST].y))\n                        , [640,480]).astype(int))\n\n            cv2.rectangle(image, \n                          (coords[0], coords[1]+5), \n                          (coords[0]+len(hand_language_class)*20, coords[1]-30), \n                          (245, 117, 16), -1)\n            cv2.putText(image, hand_language_class, coords, \n                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n\n            # Get status box\n            cv2.rectangle(image, (0,0), (250, 60), (245, 117, 16), -1)\n\n            # Display Class\n            cv2.putText(image, 'CLASS'\n                        , (95,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n            cv2.putText(image, hand_language_class.split(' ')[0]\n                        , (90,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n\n            # Display Probability\n            cv2.putText(image, 'PROB'\n                        , (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n            cv2.putText(image, str(round(hand_language_prob[np.argmax(hand_language_prob)],2))\n                        , (10,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n\n        except:\n            pass\n            \n   \n        cv2.imshow('MediaPipe Hands 2', image)\n        if cv2.waitKey(10) & 0xFF == ord('q'):\n            break\ncap.release()\ncv2.destroyAllWindows()","86b2813b":"## What is MediaPipe ?\n![image](https:\/\/google.github.io\/mediapipe\/images\/mediapipe_small.png)\n\nMediaPipe offers ready-to-use yet customizable Python solutions as a prebuilt Python package.\n\nRefer - [here](https:\/\/google.github.io\/mediapipe\/)","16bc0645":"## Step 2: Define Utils","987f28a4":"## Step 1: Install Libraries and Import Dependencies","4db21488":"## Step 4: Capture Landmarks & Export to CSV","1db8b21b":"## Step 6: Train Custom Model Using Scikit Learn","49ae0004":"## Step 9: Make Detections with Model","bcfacd50":"## Step 3: Make some Detection","02fd0105":"## Step 7: Train Machine Learning Classification Model","830f5c49":"## Step 8: Evaluate and Serialize Model","88472ee8":"## Step 5: Read in Collected Data and Process"}}