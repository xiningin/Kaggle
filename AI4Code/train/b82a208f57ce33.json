{"cell_type":{"a1b34e16":"code","9e1eeb33":"code","ccbf67a9":"code","abc0510d":"code","5ff5c8e6":"code","7536ede8":"code","55371215":"code","0c515848":"code","4c38b2d0":"code","3e7d393b":"code","9ca7fc95":"code","e62d4d44":"code","94ac7849":"code","9de21ce9":"code","c6b87dcb":"markdown","0badf8f7":"markdown","8b7b0f58":"markdown","73433f91":"markdown","1b904e61":"markdown","012d1049":"markdown","3942ebce":"markdown","7034256e":"markdown","237e1bb5":"markdown","756f312b":"markdown","7fbdf267":"markdown","66a7b23d":"markdown","6bf154a1":"markdown","da13e31a":"markdown","436d7783":"markdown","7fea0b9c":"markdown","5ca93be6":"markdown","44874840":"markdown","3a17af0b":"markdown","26c99fbc":"markdown","7033eba7":"markdown","f085c782":"markdown","a2edf886":"markdown","be9db978":"markdown"},"source":{"a1b34e16":"# For data creation and other tasks\nimport numpy as np\nimport random\nimport scipy\nfrom scipy.stats import norm\nimport pandas as pd\n\n## Mertics to evaluate the models \nfrom sklearn.metrics import accuracy_score # for Logistic Regression\n\n# For plotting the graphs.. \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns \n\n# For spliting the data into 80:20 ratio \nfrom sklearn.model_selection import train_test_split","9e1eeb33":"def generate_data(n,m,theta):\n    X = []\n    # m = 3  #change to user defined \n    # n = 100   #change to user defined \n    theta = int(n*(theta))  #change to user defined \n    for i in range(0,n):\n        X_i = scipy.stats.norm.rvs(0,1,m)\n        X.append(X_i)\n    \n    beta = scipy.stats.norm.rvs(0,1,m)\n    # for simplicity I am not adding '1' to either beta or X rather directly adding it to the 'odds' that will\n    ## be used as y1 which will be passed through cost fn with theta which will define whether it will be \n    ### '1' or '0' (bernolli distribution)\n    odds =  (np.exp(1+ np.matmul(X,beta)) \/ ( 1 + np.exp(1+ np.matmul(X,beta)) )) \n    y1 = []\n    for i in odds:\n        if(i >= 0.5):\n            y1.append(1)\n        else:\n            y1.append(0)\n    df1 = pd.DataFrame(X)\n    df2 = pd.DataFrame(y1)\n    df1['Y'] = df2[0]\n    #df1.head()\n    #df1.tail()\n\n    ## Adding noise using theta \n    change = df1.sample(theta).index\n    \n    for i in change:\n        if(df1.loc[i,'Y'] == 0):\n            df1.loc[i,'Y'] = 1 \n        else:\n            df1.loc[i,'Y'] = 0 \n    return df1","ccbf67a9":"def weightInitialization(n_features):\n    w = np.zeros((1,n_features))\n    b = 0\n    return w,b","abc0510d":"def sigmoid_activation(result):\n    final_result = 1\/(1+np.exp(-result))\n    return final_result","5ff5c8e6":"\ndef model_optimize(w, b, X, Y):\n    m = X.shape[0]\n    \n    #Prediction\n    final_result = sigmoid_activation(np.dot(w,X.T)+b)\n    Y_T = Y.T\n    cost = (-1\/m)*(np.sum((Y_T*np.log(final_result)) + ((1-Y_T)*(np.log(1-final_result)))))\n    #\n    \n    #Gradient calculation\n    dw = (1\/m)*(np.dot(X.T, (final_result-Y.T).T))\n    db = (1\/m)*(np.sum(final_result-Y.T))\n    \n    grads = {\"dw\": dw, \"db\": db}\n    \n    return grads, cost","7536ede8":"def model_predict(w, b, X, Y, learning_rate, no_iterations):\n    costs = []\n    for i in range(no_iterations):\n        #\n        grads, cost = model_optimize(w,b,X,Y)\n        #\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        #weight update\n        w = w - (learning_rate * (dw.T))\n        b = b - (learning_rate * db)\n        #\n        \n        if (i % 100 == 0):\n            costs.append(cost)\n            #print(\"Cost after %i iteration is %f\" %(i, cost))\n    \n    #final parameters\n    coeff = {\"w\": w, \"b\": b}\n    gradient = {\"dw\": dw, \"db\": db}\n    \n    return coeff, gradient, costs","55371215":"def predict(final_pred, m):\n    y_pred = np.zeros((1,m))\n    for i in range(final_pred.shape[1]):\n        if final_pred[0][i] > 0.5:\n            y_pred[0][i] = 1\n    return y_pred","0c515848":"\ndef Log_Res(n,m,theta,learning_rate,no_iterations):\n    df1 = generate_data(n,m,theta)\n        \n    X1 = df1.iloc[:,0:m].values\n    y1 = df1.iloc[:,m].values\n        \n    X_train,X_test,Y_train,Y_test = train_test_split(X1,y1,test_size = 0.2)\n    n_features = X_train.shape[1]\n    print('Number of Features', n_features)\n    w, b = weightInitialization(n_features)\n    #Gradient Descent\n    coeff, gradient, costs = model_predict(w, b, X_train, Y_train,learning_rate=0.0001,no_iterations=45000)\n    #Final prediction\n    w = coeff[\"w\"]\n    b = coeff[\"b\"]\n    print('Optimized weights - Beta', w)\n    print('Optimized intercept',b)\n    #\n    final_train_pred = sigmoid_activation(np.dot(w,X_train.T)+b)\n    final_test_pred = sigmoid_activation(np.dot(w,X_test.T)+b)\n    #\n    m_tr =  X_train.shape[0]\n    m_ts =  X_test.shape[0]\n       #\n    y_tr_pred = predict(final_train_pred, m_tr)\n    print('Training Accuracy',accuracy_score(y_tr_pred.T, Y_train))\n    #\n    y_ts_pred = predict(final_test_pred, m_ts)\n    print('Test Accuracy',accuracy_score(y_ts_pred.T, Y_test))\n    \n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per hundreds)')\n    plt.title('Cost reduction over time')\n    plt.show()","4c38b2d0":"## when there is no noise \nLog_Res(200,3,0.01,learning_rate=0.0001,no_iterations=5000)","3e7d393b":"## when there is some noise (10% noise)\nLog_Res(200,3,0.1,learning_rate=0.0001,no_iterations=5000)","9ca7fc95":"## when there is a lot of noise (30% noise)\nLog_Res(200,3,0.3,learning_rate=0.0001,no_iterations=5000)","e62d4d44":"## when n is small n = 50\nLog_Res(50,3,0.0,learning_rate=0.0001,no_iterations=5000)","94ac7849":"## when n is large n = 500\nLog_Res(500,3,0.0,learning_rate=0.0001,no_iterations=5000)","9de21ce9":"## when n is very large n = 1000\nLog_Res(10000,3,0.0,learning_rate=0.0001,no_iterations=5000)","c6b87dcb":"![1_MFMIEUC_dobhJrRjGK7PBg.jpeg](attachment:1_MFMIEUC_dobhJrRjGK7PBg.jpeg)","0badf8f7":"### Function -- weightInitialization(n_features):\n#### This is used to initialize weights and bias as '0'.","8b7b0f58":"### Function --- generate_data Takes 3 inputs \n### n - no. or rows.<br> m - no. of columns (no. of parameters X1,X2,....Xn) and<br>theta -- that is the noise in the data(Y ='0' or '1' when it is expected to be vise-versa.\n### Note: Here we are creating the data for the first time so we randomly select weight\/beta for generating y1 and if y1 > 0.5 after passing from sigmoidal function then they are classified as Y = '1' else '0'.\n### Don't worry if you don't understand it completely, it will be by the end of this notebook, Stay connected....","73433f91":"![logistic-regression-sample.png](attachment:logistic-regression-sample.png)","1b904e61":"## 2. Main Logistic Regression Functions","012d1049":"### If \u2018Z\u2019 goes to infinity, Y(predicted) will become 1 and if \u2018Z\u2019 goes to negative infinity, Y(predicted) will become 0.\n### Where ->  Z = WX + B","3942ebce":"## 2nd -- Effects of changing value of n","7034256e":"## 3. Let's Start with Analysis..","237e1bb5":"## Do UpVote and comment if it was helpfull. Thank You\ud83d\ude4f <br> Do Read Part 2: Of this notebook too --- https:\/\/www.kaggle.com\/achintyatripathi\/part-2-l1-l2-regularisation-in-log-regression?scriptVersionId=41343845","756f312b":"![1_5AYaGPV-gjYUf37d2IhgTQ.jpeg](attachment:1_5AYaGPV-gjYUf37d2IhgTQ.jpeg)","7fbdf267":"# Logistic Regression from Data generation to modelling from Scratch","66a7b23d":"## Main Function- Log_Res(n,m,theta,learning_rate,no_iterations):-  \n\n### Here we have called all the functions and show the 'Cost' vs 'Epochs' graphs for further evaluation","6bf154a1":"<font size = 3px>Logistic Regression was used in the biological sciences in early twentieth century. It was then used in many social science applications. Logistic Regression is used when the dependent variable(target) is categorical.<br><br>\nFor example,<br>\n1. To predict whether an email is spam (1) or (0) <br><br>\n2. Whether the tumor is malignant (1) or not (0) <br>\n\nConsider a scenario where we need to classify whether an email is spam or not. If we use linear regression for this problem, there is a need for setting up a threshold based on which classification can be done. Say if the actual class is malignant, predicted continuous value 0.4 and the threshold value is 0.5, the data point will be classified as not malignant which can lead to serious consequence in real time. <br><br>\nFrom this example, it can be inferred that linear regression is not suitable for classification problem. Linear regression is unbounded, and this brings logistic regression into picture. Their value strictly ranges from 0 to 1.","da13e31a":"### So let's start with it...","436d7783":"## 1. Data Generation","7fea0b9c":"### The above results are self explanatory that as we keep on increasing the noise in the data we experience relative drop in the accuracy of prediction significantly. Thus we can assume that the shape of data and values of Y in training have a deep connection with prediction. ","5ca93be6":"### For any analysis we require data. Conventionally we accure data from a .csv file but here I want everyone to understand what and why input data should be indepedentent and identically data and what effects it makes on the classification.","44874840":"## Cost function -- \n\n### Here I have got a set of Images for explaining this part ..","3a17af0b":"## Again UpVote and Comment if you liked the Tutorial\ud83d\ude4f\ud83d\ude4f . Stay connected as a 2nd part of this notebook will also come where I will talk about L1 L2 Regularisation.\n\n## Edit: Part 2: L1 L2 Regularisation in Log. Regression -- https:\/\/www.kaggle.com\/achintyatripathi\/part-2-l1-l2-regularisation-in-log-regression?scriptVersionId=41343845 ","26c99fbc":"### When the size of the dataset was increased the computation incereased However we can see that the data should not be very small nor very large as we don't want to underfit or overfit the model. <br> Having a very large dataset sometimes overfit the model as the model only needs to learn those parameters that can easily help us to find the theta(probablity of classification) using which we can evaluate the input we get and very small dataset may not help us to find one.","7033eba7":"## Function -- sigmoid_activation(result):","f085c782":"## 1st -- Effects of noise in the data","a2edf886":"## Final Conclusions:-- \n## For binary classification Logistic Regression works great. Even when the dataset increases it still does a good job in classification. If noise is large then the model's accuracy does decreases.","be9db978":"![1_RqXFpiNGwdiKBWyLJc_E7g.png](attachment:1_RqXFpiNGwdiKBWyLJc_E7g.png)"}}