{"cell_type":{"9196bdc9":"code","43abf9e9":"code","9462a6c9":"code","c661f856":"code","e6597472":"code","9bf7c3a3":"code","b6da5e7d":"code","17c85c5b":"code","0f9402ca":"code","37bf59b2":"code","5126fafb":"code","fd60eb9b":"code","2949f82e":"code","3c5ec687":"code","36a9b8cb":"code","a0c18bfc":"code","5624ff54":"code","bb5b60ff":"code","b46e6710":"code","06782680":"code","7247afd5":"code","5fb353b4":"code","86bf2a6e":"code","7cc39174":"code","8041948e":"code","82681e74":"code","35175540":"code","d27acf25":"code","68d2f697":"code","45acf129":"code","e1d0a90d":"code","a5220eb1":"code","3054b793":"code","96465b4f":"code","c2ca0354":"code","5a251894":"code","4161b3b4":"code","594bb954":"code","6f8c4043":"code","09726fab":"code","2589cf7a":"code","7053ae37":"code","144cf5be":"code","c539544b":"code","97574f9c":"code","4f828a45":"code","a3b35b26":"code","949ca41e":"code","e10c0fed":"code","c0639530":"code","f92930bd":"code","dcca6439":"code","5e72e6b4":"code","216636cb":"code","2f20c958":"code","91856eaa":"code","686f0e2f":"code","f306ce80":"code","5c121312":"code","14dfd490":"code","ca86a302":"code","e048da93":"code","a223f20a":"code","08a890a8":"code","2634e6a5":"code","5793eaab":"code","1401d618":"code","650a36b4":"code","d3dfe993":"code","2d473b17":"code","aed11ad2":"code","22317679":"code","f9e4fe36":"code","590dbd97":"code","c9a8d253":"code","743c2a86":"code","30e920ac":"code","999f967a":"code","29f47f41":"code","52ba263b":"code","5e934253":"code","1b14802c":"code","69b4a13a":"markdown","1eba4c83":"markdown","c8b53ec9":"markdown","f4891a06":"markdown","c0ba9a31":"markdown","3ac6169c":"markdown","036203ae":"markdown","5940d7f0":"markdown","4d1d0d90":"markdown","d6b70381":"markdown","e4f41833":"markdown","43cfea17":"markdown","e7ef55b1":"markdown","91e87745":"markdown","ea61bcbd":"markdown","bcca96ac":"markdown","56835449":"markdown","3cf1f354":"markdown","b6507cfd":"markdown","1e01d233":"markdown","384b35cb":"markdown","44e1c7cc":"markdown","c5b15c69":"markdown","a6b2126f":"markdown","61197674":"markdown","23ad9dfd":"markdown","e7d2ef4d":"markdown","dac88f97":"markdown","d51f9dad":"markdown","0c3e9f94":"markdown","2f0ddedb":"markdown","730acdb9":"markdown","afd669b9":"markdown","2ce92f34":"markdown","7054ebdd":"markdown","deeec201":"markdown","5d58d48a":"markdown","f743e9ca":"markdown","3ecf85ab":"markdown","5d4d9857":"markdown","be49e4ac":"markdown","84fce3fb":"markdown","e9b645ed":"markdown","8874681c":"markdown","8b2dec96":"markdown","056357af":"markdown","0a0264f2":"markdown","17691a63":"markdown","3eb50c04":"markdown","0d1104d9":"markdown","6ea65219":"markdown","7a658421":"markdown","3667fc41":"markdown","e011d4fa":"markdown","554ee43d":"markdown","86bbaed3":"markdown","e18c7944":"markdown","42ae0658":"markdown","d1eeab85":"markdown","86abe57c":"markdown","1567263a":"markdown","bb92dd0f":"markdown","63a0f2ee":"markdown","b32306ba":"markdown","3a79daf6":"markdown","f3820bae":"markdown","6d79033c":"markdown","4f2a1f35":"markdown","a8f121e4":"markdown"},"source":{"9196bdc9":"import pandas as pd\ntitanic = pd.read_csv('..\/input\/titanic\/train.csv')\ntitanic.info()","43abf9e9":"titanic.head()","9462a6c9":"titanic.isna().sum()","c661f856":"titanic.duplicated().sum()","e6597472":"titanic.info()","9bf7c3a3":"titanic.groupby(['Sex','Survived']).agg(count = ('Survived','count'))","b6da5e7d":"def my_dummy_model(sex): \n    if sex == 'female': \n        return 1\n    elif sex == 'male':\n        return 0\n\ntitanic['preds'] = [my_dummy_model(sex) for sex in titanic['Sex']]\ntitanic.head()","17c85c5b":"error_check = titanic.filter(['Survived','preds']).assign(check = lambda x: x['Survived'] == x['preds'])\nerror_check.sample(10)","0f9402ca":"# total number of correct answers\nerror_check['check'].sum()","37bf59b2":"# accuracy of our model is: \nerror_check['check'].sum() \/ error_check.shape[0]","5126fafb":"titanic.drop(columns=['preds'], inplace=True)","fd60eb9b":"from sklearn.model_selection import train_test_split\n\nX = titanic.drop(columns=['Survived'])\ny = titanic['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.75, random_state=8)\n\nX_train = pd.DataFrame(X_train, columns=X.columns)\nX_test = pd.DataFrame(X_test, columns=X.columns)","2949f82e":"(\n    X_train\n    .assign(Survived = y_train)\n    .groupby(['Sex','Survived'])\n    .agg(count = ('Survived','count'))\n    )","3c5ec687":"# Let's predict if someone will have survived or not with my dummy mdoel\nX_train['preds'] = [my_dummy_model(sex) for sex in X_train['Sex']]\nX_test['preds'] = [my_dummy_model(sex) for sex in X_test['Sex']]","36a9b8cb":"# results train data\n(\n    pd.DataFrame({\n        'Survived':y_train,\n        'preds':[my_dummy_model(sex) for sex in X_train['Sex']]\n        })\n    .assign(check = lambda x: x['Survived'] == x['preds'])['check']\n    .sum()\n) \/ len(y_train)","a0c18bfc":"# results train data\nacc_2nd = (\n    pd.DataFrame({\n        'Survived':y_test,\n        'preds':[my_dummy_model(sex) for sex in X_test['Sex']]\n        })\n    .assign(check = lambda x: x['Survived'] == x['preds'])['check']\n    .sum()\n) \/ len(y_test)\nacc_2nd","5624ff54":"X = titanic.filter(['Sex'])\ny = titanic.filter(['Survived'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.75, random_state=8)\n\nX_train = pd.DataFrame(X_train, columns=X.columns)\nX_test = pd.DataFrame(X_test, columns=X.columns)","bb5b60ff":"pd.get_dummies(X_train).head()","b46e6710":"from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(drop='first', sparse=False) # handle_unknown='ignore'\nenc.fit(X_train)","06782680":"# printing the category names\n# as we know we have specified `drop='first'`, we are going only to keep\n# the category male, which is in the 2nd positions of the array\nenc.get_feature_names()[0]","7247afd5":"X_train_enc = enc.transform(X_train)\nX_train_enc = pd.DataFrame(X_train_enc, columns=list(enc.get_feature_names()))\nX_train_enc.head()","5fb353b4":"from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier()\ntree.fit(X_train_enc, y_train)","86bf2a6e":"import matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree\n\nplt.figure(figsize=(15, 7))\nplot_tree(\n    tree,\n    filled=True, \n    rounded=True, \n    class_names=['Not Survived','Survived'], \n    feature_names=X_train_enc.columns\n    );","7cc39174":"# check the accuracy\nfrom sklearn.metrics import accuracy_score\npreds = tree.predict(X_train_enc)\naccuracy_score(preds, y_train)","8041948e":"# 1st we need to transform this metrics\nX_test_enc = pd.DataFrame(enc.transform(X_test), columns=list(enc.get_feature_names()))\n# time to predict \npreds = tree.predict(X_test_enc)\nacc_3rd = accuracy_score(preds, y_test)\nacc_3rd","82681e74":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(tree, X_test_enc, y_test, display_labels=['Not Survied', 'Survived']);","35175540":"titanic.sample(5)","d27acf25":"X = titanic.drop(columns=['Name','Ticket','Cabin','PassengerId','Survived'])\ny = titanic['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\n\nX_train = pd.DataFrame(X_train, columns=X.columns)\nX_test = pd.DataFrame(X_test, columns=X.columns)\n\nX_train.head()","68d2f697":"X_train.isna().sum()","45acf129":"# define numeric columns\nnum_col = ['Age','Fare','SibSp','Parch']\n\nfrom sklearn.impute import SimpleImputer\n# define missing values imputer strategy\nimp_num = SimpleImputer(strategy=\"median\")\n\nX_train[num_col] = imp_num.fit_transform(X_train[num_col])\nX_test[num_col] = imp_num.transform(X_test[num_col])\nX_train.isna().sum()","e1d0a90d":"cat_col = ['Pclass','Sex','Embarked']\n\nimp_cat = SimpleImputer(strategy='most_frequent')\n\nX_train[cat_col] = imp_cat.fit_transform(X_train[cat_col])\nX_test[cat_col] = imp_cat.transform(X_test[cat_col])\nX_train.isna().sum()","a5220eb1":"enc = OneHotEncoder(\n    drop='first',\n    sparse=False\n    )\nX_train_enc = enc.fit_transform(X_train[cat_col])\nX_test_enc = enc.transform(X_test[cat_col])\n\n# modify X_train\nX_train_enc = pd.DataFrame(X_train_enc, columns=list(enc.get_feature_names()), index=X_train.index)\n\nX_train = pd.concat([X_train, X_train_enc], axis=1, ignore_index=False).drop(columns=cat_col)\nX_train.head()","3054b793":"X_test_enc = pd.DataFrame(X_test_enc, columns=list(enc.get_feature_names()), index=X_test.index)\nX_test_enc\nX_test = pd.concat([X_test, X_test_enc], axis=1, ignore_index=False).drop(columns=cat_col)\nX_test.head()","96465b4f":"tree = DecisionTreeClassifier()\ntree.fit(X_train, y_train)","c2ca0354":"plt.figure(figsize=(100, 40))\nplot_tree(\n    tree,\n    filled=True, \n    rounded=True, \n    class_names=['Not Survived', 'Survived'], \n    feature_names=X_train.columns\n    );","5a251894":"# accuracy in train set\npreds = tree.predict(X_train)\naccuracy_score(preds, y_train)","4161b3b4":"# accuracy in test set\npreds = tree.predict(X_test)\nacc_4th = accuracy_score(preds, y_test)\nacc_4th","594bb954":"# confusion matrix\nplot_confusion_matrix(tree, X_test, y_test, display_labels=['Not Survied', 'Survived']);","6f8c4043":"tree = DecisionTreeClassifier(max_depth=3)\ntree.fit(X_train, y_train)\nplt.figure(figsize=(100, 40))\nplot_tree(\n    tree,\n    filled=True, \n    rounded=True, \n    class_names=['Not Survived', 'Survived'], \n    feature_names=X_train.columns\n    );","09726fab":"# accuracy in train set\npreds = tree.predict(X_train)\naccuracy_score(preds, y_train)","2589cf7a":"# accuracy in test set\npreds = tree.predict(X_test)\nacc_4th = accuracy_score(preds, y_test)\nacc_4th","7053ae37":"X = titanic.drop(columns=['Name','Ticket','Cabin','PassengerId','Survived'])\ny = titanic['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.75, random_state=8)\n\nX_train = pd.DataFrame(X_train, columns=X.columns)\nX_test = pd.DataFrame(X_test, columns=X.columns)\n\nX_train.head()","144cf5be":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\nnumeric_pipeline = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='median')),\n    # new scaling addition\n    ('scale', MinMaxScaler())\n])\n\ncategorical_pipeline = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('one-hot', OneHotEncoder(drop='first', sparse=False))\n])","c539544b":"num_col = ['Age','Fare','SibSp','Parch']\n\n# example how ot apply a pipline\npd.DataFrame(numeric_pipeline.fit_transform(X_train[num_col]), columns=num_col) # select_dtypes() another option","97574f9c":"cat_col = ['Pclass','Sex','Embarked']\nnum_col = ['Age','Fare','SibSp','Parch']\n\nfrom sklearn.compose import ColumnTransformer\n\nfull_processor = ColumnTransformer(transformers=[\n    ('number', numeric_pipeline, num_col), \n    ('category', categorical_pipeline, cat_col)\n])\n\npd.DataFrame(full_processor.fit_transform(X_train))","4f828a45":"# we can even add a model into our scikit learn pipeline\ntree_pipeline = Pipeline(steps=[\n    ('preprocess', full_processor), \n    ('model', DecisionTreeClassifier())\n])\ntree_pipeline.fit(X_train, y_train)","a3b35b26":"accuracy_score(tree_pipeline.predict(X_train), y_train)","949ca41e":"preds = tree_pipeline.predict(X_test)\nacc_5th = accuracy_score(preds, y_test)\nacc_5th","e10c0fed":"plot_confusion_matrix(tree_pipeline, X_test, y_test, display_labels=['Not Survied', 'Survived']);","c0639530":"# numerical and categorical pipeline\nnumeric_pipeline = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='median')),\n    ('scale', MinMaxScaler())\n])\ncategorical_pipeline = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('one-hot', OneHotEncoder(drop='first', sparse=False))\n])\n# automating the column selection\ncat_col = ['Pclass','Sex','Embarked']\nnum_col = ['Age','Fare','SibSp','Parch']\nfull_processor = ColumnTransformer(transformers=[\n    ('number', numeric_pipeline, num_col), \n    ('category', categorical_pipeline, cat_col)\n])","f92930bd":"# adding the model in our pipeline\ntree_pipeline = Pipeline(steps=[\n    ('preprocess', full_processor), \n    ('model', DecisionTreeClassifier(max_depth=4))\n])\n# fit our model to the train\ntree_pipeline.fit(X_train, y_train)\naccuracy_score(tree_pipeline.predict(X_train), y_train)","dcca6439":"# get column names\ncol_names = num_col.copy()\ncol_names.extend(\n    tree_pipeline.named_steps['preprocess'].transformers_[1][1].named_steps['one-hot'].get_feature_names()\n)\n\nplt.figure(figsize=(50, 10))\nplot_tree(\n    tree_pipeline['model'],\n    filled=True, \n    rounded=True, \n    feature_names=col_names, \n    class_names=['Not Survived', 'Survived']\n    );","5e72e6b4":"preds = tree_pipeline.predict(X_test)\nacc_6th = accuracy_score(preds, y_test)\nacc_6th","216636cb":"import seaborn as sns\n# our we improving our model?\nplt.subplots(figsize=(10, 5))\n# ax = plt.axes()\nresults = {\n    'dummy_model':acc_2nd, \n    'first_dt':acc_3rd, \n    'dt_more_features':acc_4th, \n    'min_max_scaling':acc_5th, \n    'manual_model_tunning':acc_6th\n}\n\nresults_df = pd.DataFrame({\n    'iteration':results.keys(),\n    'result':results.values()\n})\nsns.lineplot(data=results_df, x='iteration', y='result', color='grey')\nsns.scatterplot(data=results_df, x='iteration', y='result')\nplt.title('Accuracy evolution through iterations')","2f20c958":"# numerical and categorical pipeline\nnumeric_pipeline = Pipeline(steps=[\n    ('num_impute', SimpleImputer(strategy='median')),\n    ('scale', MinMaxScaler())\n])\ncategorical_pipeline = Pipeline(steps=[\n    ('cat_impute', SimpleImputer(strategy='most_frequent')),\n    ('one-hot', OneHotEncoder(drop='first', sparse=False))\n])\n# automating the column selection\ncat_col = ['Pclass','Sex','Embarked']\nnum_col = ['Age','Fare','SibSp','Parch']\nfull_processor = ColumnTransformer(transformers=[\n    ('number', numeric_pipeline, num_col), \n    ('category', categorical_pipeline, cat_col)\n])\n\n# adding the model in our pipeline\ntree_pipeline = Pipeline(steps=[\n    ('preprocess', full_processor), \n    ('model', DecisionTreeClassifier())\n])\n# automating the search of the different parameters for our model\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {\n    'model__max_depth': range(1, 8),\n    'model__min_samples_leaf': range(1, 10),\n    'model__min_samples_split': range(2, 10),\n    'model__criterion':['gini','entropy']\n    }\nsearch = GridSearchCV(\n    tree_pipeline, \n    param_grid, \n    cv=15,\n    scoring='accuracy', \n    verbose=1, \n    refit=True, \n    n_jobs=-1\n    )\n# fit our model to the train\n_ = search.fit(X_train, y_train)","91856eaa":"search.best_params_","686f0e2f":"accuracy_score(search.predict(X_train), y_train)","f306ce80":"acc_7th = accuracy_score(search.predict(X_test), y_test)\nacc_7th","5c121312":"plt.subplots(figsize=(15, 5))\n\nresults['auto model tunning'] = acc_7th\n\nresults_df = pd.DataFrame({\n    'iteration':results.keys(),\n    'result':results.values()\n})\nsns.lineplot(data=results_df, x='iteration', y='result', color='grey')\nsns.scatterplot(data=results_df, x='iteration', y='result')\nplt.title('Accuracy evolution through iterations')","14dfd490":"X = titanic.drop(columns=['Ticket','Cabin','PassengerId','Survived']) # 'Name' we are going to extract some information from it\ny = titanic['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\n\nX_train = pd.DataFrame(X_train, columns=X.columns)\nX_test = pd.DataFrame(X_test, columns=X.columns)\n\nX_train.sample(10)","ca86a302":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\nvect_name = vectorizer.fit_transform(X_train['Name'])\n(\npd.DataFrame(\n    vect_name.toarray(), \n    columns=vectorizer.get_feature_names()\n    )\n    .sum()\n    .sort_values(ascending=False)\n    .reset_index()\n    .head(15)\n)","e048da93":"# numerical and categorical pipeline\nnumeric_pipeline = Pipeline(steps=[\n    ('num_impute', SimpleImputer(strategy='median')),\n    ('scale', MinMaxScaler())\n])\ncategorical_pipeline = Pipeline(steps=[\n    ('cat_impute', SimpleImputer(strategy='most_frequent')),\n    ('one-hot', OneHotEncoder(drop='first', sparse=False))\n])\n# extract title from the name\nstatus_egnineering = Pipeline(steps=[\n    # will keep it at 7 to reduce the computational time\n    ('status', CountVectorizer(max_features=7))\n])\n# automating the column selection\ncat_col = ['Pclass','Sex','Embarked']\nnum_col = ['Age','Fare','SibSp','Parch']\nfull_processor = ColumnTransformer(transformers=[\n    ('number', numeric_pipeline, num_col), \n    ('category', categorical_pipeline, cat_col),\n    ('engineering', status_egnineering, 'Name')\n])\n\n# adding the model in our pipeline\ntree_pipeline = Pipeline(steps=[\n    ('preprocess', full_processor), \n    ('model', DecisionTreeClassifier())\n])\n# automating the search of the different parameters for our model\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {\n    'model__max_depth': range(1, 8),\n    'model__min_samples_leaf': range(1, 10),\n    'model__min_samples_split': range(2, 10),\n    'model__criterion':['gini','entropy']\n    }\nsearch = GridSearchCV(\n    tree_pipeline, \n    param_grid,\n    cv=15, \n    scoring='accuracy',\n    verbose=1,\n    refit=True, \n    n_jobs=-1\n    )\n# fit our model to the train\n_ = search.fit(X_train, y_train)\naccuracy_score(search.predict(X_train), y_train)","a223f20a":"acc_8th = accuracy_score(search.predict(X_test), y_test)\nacc_8th","08a890a8":"results","2634e6a5":"def plot_iterations_evolution(new_iteration_name, new_acc):\n    global results\n    results[new_iteration_name] = new_acc\n\n    results_df = pd.DataFrame({\n        'iteration':results.keys(),\n        'result':results.values()\n    })\n    plt.subplots(figsize=(20, 5))\n    sns.lineplot(data=results_df, x='iteration', y='result', color='grey')\n    sns.scatterplot(data=results_df, x='iteration', y='result')\n    plt.title('Accuracy evolution through iterations')\n    plt.show()\n\nplot_iterations_evolution('name engineering', acc_8th)","5793eaab":"# code does not work on kaggle sklearn version\n# cat_enc_col = (search\n#                 .best_estimator_\n#                 .named_steps['preprocess']\n#                 .transformers_[1][1]\n#                 .named_steps['one-hot']\n#                 .get_feature_names_out()\n#                 .tolist())\n# name_new_col = (search\n#                 .best_estimator_\n#                 .named_steps['preprocess']\n#                 .transformers_[2][1]\n#                 .get_feature_names_out()\n#                 .tolist())\n\n# pd.DataFrame({\n#     'cols': num_col + cat_enc_col + name_new_col,\n#     'importance':search.best_estimator_.named_steps['model'].feature_importances_\n# }).sort_values('importance', ascending=False)\n","1401d618":"# numerical and categorical pipeline\nnumeric_pipeline = Pipeline(steps=[\n    ('num_impute', SimpleImputer(strategy='median')),\n    ('scale', MinMaxScaler())\n])\ncategorical_pipeline = Pipeline(steps=[\n    ('cat_impute', SimpleImputer(strategy='most_frequent')),\n    ('one-hot', OneHotEncoder(drop='first', sparse=False))\n])\n# extract title from the name\nstatus_egnineering = Pipeline(steps=[\n    # we will let our decision tree to select which are the important features\n    ('status', CountVectorizer(max_features=40))\n])\n# automating the column selection\ncat_col = ['Pclass','Sex','Embarked']\nnum_col = ['Age','Fare','SibSp','Parch']\nfull_processor = ColumnTransformer(transformers=[\n    ('number', numeric_pipeline, num_col), \n    ('category', categorical_pipeline, cat_col),\n    ('engineering', status_egnineering, 'Name')\n])\n# defining the parameters of our model based on previous cv\n# (later on we can run again a gridsearch, but we want to speed up this process)\nparam_grid = {\n    'criterion': 'entropy',\n    'max_depth': 6,\n    'min_samples_leaf': 5,\n    'min_samples_split': 2\n    }\n\nfrom sklearn.feature_selection import SelectFromModel\n# adding the model in our pipeline\ntree_pipeline = Pipeline(steps=[\n    ('preprocess', full_processor), \n    # using a normal decision tree to decide which features are important\n    ('feature_selection', SelectFromModel(DecisionTreeClassifier())),\n    ('model', DecisionTreeClassifier(**param_grid))\n])\n# automating the search of the different parameters for our model\n# fit our model to the train\n_ = tree_pipeline.fit(X_train, y_train)\naccuracy_score(tree_pipeline.predict(X_train), y_train)","650a36b4":"# \n# # get the feature names from our model\n# cat_enc_col = (\n#     tree_pipeline\n#     .named_steps['preprocess']\n#     .transformers_[1][1]\n#     .named_steps['one-hot']\n#     .get_feature_names_out()\n#     .tolist()\n#     )\n# name_new_col = (\n#     tree_pipeline\n#     .named_steps['preprocess']\n#     .transformers_[2][1]\n#     .get_feature_names_out()\n#     .tolist()\n#     )\n# # find out the names generated on the preprocessing\n# preprocess_col = num_col + cat_enc_col + name_new_col\n# # sklearn returns only the positions of these columns\n# selected_cols_pos = tree_pipeline.named_steps['feature_selection'].get_feature_names_out()\n# # find the name of the columns based on the position selcted with sklearn trasnformer `SelectFromModel`\n# feature_names = [preprocess_col[int(col_name.replace('x',''))] for col_name in selected_cols_pos]\n\n# plt.figure(figsize=(50, 10))\n# plot_tree(\n#     tree_pipeline['model'],\n#     filled=True, \n#     rounded=True, \n#     feature_names=feature_names, \n#     class_names=['Not Survived', 'Survived']\n#     );","d3dfe993":"# looking at the feature importance\n# pd.DataFrame({\n#     'cols': feature_names,\n#     'importance':tree_pipeline.named_steps['model'].feature_importances_\n# }).sort_values('importance', ascending=False)","2d473b17":"acc_9th = accuracy_score(tree_pipeline.predict(X_test), y_test)\nacc_9th","aed11ad2":"plot_iterations_evolution('feature selection', acc_9th)","22317679":"from sklearn.metrics import confusion_matrix\n\ndef model_check(y_true, y_preds, model):     \n    print('Accuracy: ', accuracy_score(y_true, y_preds))\n    tn, fp, fn, tp = confusion_matrix(y_true, y_preds).ravel()\n    plot_confusion_matrix(model, X_test, y_test, display_labels=['Not Survied', 'Survived']);\n    return tn, fp, fn, tp\ntn, fp, fn, tp = model_check(y_test, tree_pipeline.predict(X_test), tree_pipeline)","f9e4fe36":"47 \/ (24 + 47)","590dbd97":"from sklearn.metrics import precision_score, recall_score, cohen_kappa_score, f1_score\nprint(f\"\"\"\n    Accuracy: {accuracy_score(tree_pipeline.predict(X_test), y_test)}\n    Recall\/Sensitivity: {recall_score(tree_pipeline.predict(X_test), y_test)}\n    Precision (True Positive Rate): {precision_score(tree_pipeline.predict(X_test), y_test)}\n    Specifity (False Positive Rate): {tn \/ (tn+fp)}\n    F Score: {f1_score(tree_pipeline.predict(X_test), y_test)}\n    Kappa: {cohen_kappa_score(tree_pipeline.predict(X_test), y_test)}\n\"\"\")","c9a8d253":"X_train_preds = X_train.copy()\n\nX_train_errors = (\n    X_train_preds\n    .assign(\n        preds = tree_pipeline.predict(X_train),\n        true = y_train\n        )\n    .query('true == 1')\n)\nX_train_errors.head()","743c2a86":"# tree_error = DecisionTreeClassifier()\n# _ = tree_pipeline.fit(X_train_errors.drop(columns=['preds','true']), X_train_errors['preds'])\n# # get the feature names from our model\n# cat_enc_col = (\n#     tree_pipeline\n#     .named_steps['preprocess']\n#     .transformers_[1][1]\n#     .named_steps['one-hot']\n#     .get_feature_names_out()\n#     .tolist()\n#     )\n# name_new_col = (\n#     tree_pipeline\n#     .named_steps['preprocess']\n#     .transformers_[2][1]\n#     .get_feature_names_out()\n#     .tolist()\n#     )\n# # find out the names generated on the preprocessing\n# preprocess_col = num_col + cat_enc_col + name_new_col\n# # sklearn returns only the positions of these columns\n# selected_cols_pos = tree_pipeline.named_steps['feature_selection'].get_feature_names_out()\n# # find the name of the columns based on the position selcted with sklearn trasnformer `SelectFromModel`\n# feature_names = [preprocess_col[int(col_name.replace('x',''))] for col_name in selected_cols_pos]\n\n# plt.figure(figsize=(50, 10))\n# plot_tree(\n#     tree_pipeline['model'],\n#     filled=True, \n#     rounded=True, \n#     feature_names=feature_names, \n#     class_names=['Not Survived', 'Survived']\n#     );\n","30e920ac":"# from sklearn.feature_selection import SelectFromModel\nfrom sklearn.neighbors import KNeighborsClassifier\n# adding the model in our pipeline\nknn_pipeline = Pipeline(steps=[\n    ('preprocess', full_processor), \n    # using a normal decision tree to decide which features are important\n    ('feature_selection', SelectFromModel(DecisionTreeClassifier())),\n    ('model', KNeighborsClassifier())\n])\n\n# automating the search of the different parameters for our model\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {\n    'model__n_neighbors': range(1, 10),\n    'model__weights': ['uniform', 'distance'],\n    'model__p': [1,2]\n    }\nsearch = GridSearchCV(\n    knn_pipeline, \n    param_grid,\n    cv=15, \n    scoring='accuracy',\n    verbose=1,\n    refit=True, \n    n_jobs=-1\n    )\n# fit our model to the train\n_ = search.fit(X_train, y_train)\naccuracy_score(search.predict(X_train), y_train)","999f967a":"acc_10th = accuracy_score(search.predict(X_test), y_test)\nacc_10th","29f47f41":"plot_iterations_evolution('Knn', acc_10th)","52ba263b":"# from sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\n# adding the model in our pipeline\nknn_pipeline = Pipeline(steps=[\n    ('preprocess', full_processor), \n    # using a normal decision tree to decide which features are important\n    ('feature_selection', SelectFromModel(DecisionTreeClassifier())),\n    ('model', RandomForestClassifier())\n])\n\n# automating the search of the different parameters for our model\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {\n    'model__max_depth': range(1, 8),\n    'model__min_samples_leaf': range(1, 10),\n    'model__min_samples_split': range(2, 10),\n    'model__criterion':['gini','entropy']\n    }\nsearch = GridSearchCV(\n    knn_pipeline, \n    param_grid,\n    cv=5, \n    scoring='accuracy',\n    verbose=1,\n    refit=True, \n    n_jobs=-1\n    )\n# fit our model to the train\n_ = search.fit(X_train, y_train)\naccuracy_score(search.predict(X_train), y_train)","5e934253":"acc_11th = accuracy_score(search.predict(X_test), y_test)\nacc_11th","1b14802c":"plot_iterations_evolution('RF', acc_11th)","69b4a13a":"## Train our model","1eba4c83":"## Basic exploration","c8b53ec9":"## Test our model","f4891a06":"## EXTRA: understanding feature importance in our model","c0ba9a31":"## Test our model","3ac6169c":"Our goal is to use the power of decision trees to train our model with more variables. Now that we know how to introduce categorical variables into it, let's add more information in our model:","036203ae":"### Clean code","5940d7f0":"## Testing our model","4d1d0d90":"Let's take a look at the two columns we want to compare:","d6b70381":"# 5th iteration: prunning the tree","e4f41833":"Our main problem is on the people that has survived, but in reality we are predicting them as not survived. Let's do a further exploration on the survival group: ","43cfea17":"# 8th iteration: Auto model tunning + Cross Validation","e7ef55b1":"#### Confusion matrix\n\nAnother way to visualise our errors is to use a confusion matrix:","91e87745":"We are following a wrong approach at creating this model, as we will have to split our data into a training and a testing. ","ea61bcbd":"## Training model","bcca96ac":"### Undestanding our model","56835449":"### With `pandas`","3cf1f354":"## Tunning our model","b6507cfd":"## Test sklearn pipline","1e01d233":"## Updateing our sklearn pipeline with `SelectFromModel` for feature selection","384b35cb":"Same results as we got on the 2nd iteration: 0.7313432835820896","44e1c7cc":"## Preprocess (nice and clean code)","c5b15c69":"## Test our model","a6b2126f":"### Duplicated rows","61197674":"## Create our dummy model (\"Train\")","23ad9dfd":"### Metrics on test","e7d2ef4d":"## Train and test split","dac88f97":"#### Embarked missing values\n\n\nCategorical variables: \n\n* Pclass : \tTicket class (1 = 1st, 2 = 2nd, 3 = 3rd)\n* Sex\n* Embarked : Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)","d51f9dad":"## Test our model","0c3e9f94":"## Pre process your data","2f0ddedb":"### Visualise our model","730acdb9":"## Test our model","afd669b9":"## Short exploration","2ce92f34":"# 12th iteration: Random Forest","7054ebdd":"## Pre processing: adding passenger status","deeec201":"# 2nd iteration: creating train and test","5d58d48a":"### Missing values","f743e9ca":"We already know that all this observations should be predicted as Sruvived, the question is to understand which are the factors that leads our decision tree to labale them as Not Survived. Here we can see the main factors: \n\n* When you are a male and you paid a high fare\n* When you are a male and your paid a really low fare but your are old","3ecf85ab":"### Check the metrics on train","5d4d9857":"## Test our model","be49e4ac":"## Train our model with a max depth of 3","84fce3fb":"# 9th iteration: get status from name","e9b645ed":"This is the same accuracy we got on the 2nd iteration, where we have created a dummy model with our own function (accuracy 2nd iteration was 0.7965653896961691)","8874681c":"## Evaluate my model","8b2dec96":"### Missing values","056357af":"### With `sklearn`","0a0264f2":"# 3rd iteration: our first scikit-learning model","17691a63":"#### Age missing values\n\nDefine numerical variables: \n* Age : Age in years\n* SibSp : # of siblings \/ spouses aboard the Titanic\n* Parch : # of parents \/ children aboard the Titanic\n* Fare : Passenger fare","3eb50c04":"## Preprocess","0d1104d9":"Based on this quick analyisis, we can apply the same model as before. ","6ea65219":"### Plot our tree","7a658421":"Time to create our first model, a decision tree. We are going only to use the column *Sex* as a predictor:","3667fc41":"Finding the accuracy for our train and test set:","e011d4fa":"## Split the dataset","554ee43d":"# 11th iteration: KNN","86bbaed3":"How to transform a categorical variable into a numeric variable?","e18c7944":"# 10th iteration: feature selection + error analysis","42ae0658":"### Understanding Count Vectorizer","d1eeab85":"### Encoding variables","86abe57c":"# 1st iteration: our dummy model","1567263a":"## Data quality check","bb92dd0f":"#### Understanding where the errors are commoing from","63a0f2ee":"## Preprocess: build our first sklean pipeline","b32306ba":"## Feature selection \n\nLet's decide which variables we think are important to predict if someone has survived or not to this disaster:","3a79daf6":"## Error analyisis","f3820bae":"Observations: \n\n* Total Not Survived: `65 + 18 = 83`. \n* Total Survived: `18 + 33 = 51`. \n* Not Survived correctly classified: `65` (`65 \/ 83 = 0.78` -> 78% over the total not survived people). \n* Survived correclty classified: `33` (`33 \/ 51 = 0.65` -> 65% over the total survived people).","6d79033c":"# 4th iteration: more features + missing values preprocessing","4f2a1f35":"# 7th iteration: Manual model tunning","a8f121e4":"# 6th iteration: scikit-learn pipeline"}}