{"cell_type":{"4ab19b68":"code","dee9d06b":"code","58bd61aa":"code","37bde505":"code","ed3a22af":"code","92863b71":"code","d723f071":"code","6559b119":"code","cbe15c60":"code","e11d6fcf":"code","79c36915":"code","fcb7f746":"code","1f176b54":"code","be5f2313":"code","be65d465":"code","010c8b15":"code","27b2f85b":"code","e48c286e":"code","82924a03":"code","e710f3bb":"code","9282a3f9":"code","10363421":"code","5362cd95":"code","22bdb4c2":"code","97f0591e":"code","0bea29c9":"code","24cef37e":"code","e12d4ad4":"code","6297fefa":"code","f1791c3b":"code","8784c025":"code","d13479a2":"code","4c360d10":"code","4d570e62":"code","40152b83":"code","7e6d27b3":"code","a856a32f":"code","92c444c8":"code","a988e929":"code","dee9c7b1":"code","0997756d":"code","a7eafb59":"code","0a4a76ce":"code","6e526686":"code","ed3f812d":"code","46e8d0ed":"code","31eb2503":"code","0d210f67":"code","e073d5ea":"code","b7d32866":"code","b4d32c76":"code","3b1dfc24":"code","d23e7110":"code","bda8d757":"code","29c51342":"code","7492b893":"code","4e189d0f":"code","ec7e9ff6":"code","520f8ab1":"code","751ac490":"code","c2634f08":"code","dfa4e773":"code","0a665a66":"code","39c3a115":"markdown","9cec7723":"markdown","d8ac3f6f":"markdown","005b133d":"markdown","97edb762":"markdown","e03e72bd":"markdown","ee946125":"markdown","3b2f99d3":"markdown","50e64d2f":"markdown","6cbfa3df":"markdown","683412e2":"markdown","0885d23f":"markdown","c115f4b5":"markdown","2aea90c8":"markdown","a2ac0fa2":"markdown","44b90327":"markdown","6e1947d6":"markdown","0e474ac6":"markdown","be87a775":"markdown","e579855c":"markdown","0ae9daa9":"markdown","1bf6b758":"markdown","611209e5":"markdown","5cfd5cb4":"markdown","1d653de3":"markdown","b5f3ec2b":"markdown","e92dc417":"markdown","d6058e97":"markdown","6ab5ec84":"markdown","f45a9035":"markdown","31491d89":"markdown","3ddf9889":"markdown","92c20c0e":"markdown","b2280b0d":"markdown","d41d747c":"markdown","791b5187":"markdown","0d707c5a":"markdown","7da94868":"markdown","8edd3d21":"markdown","3923761f":"markdown","68c7ecc4":"markdown","8949445f":"markdown","0934db25":"markdown","966acaba":"markdown","7ed080e4":"markdown","4180aa12":"markdown","f99b3bbd":"markdown","e268f906":"markdown","2ef94cb6":"markdown","742fb5fc":"markdown","6fcbf809":"markdown","28ec60bf":"markdown","eb3dd7a1":"markdown","cd5bf254":"markdown","fde1058e":"markdown","41b4037c":"markdown","c3937852":"markdown","089d6c14":"markdown","a22badf9":"markdown","66acc5a7":"markdown","466d3367":"markdown","c7977456":"markdown","25f3228d":"markdown","08d8300d":"markdown","b75439eb":"markdown","390e997b":"markdown","1327bb76":"markdown","0471aa41":"markdown","bf50e8be":"markdown","6cafd837":"markdown","d4c9d50a":"markdown","8fed2a12":"markdown","e4bde828":"markdown","ed1ddade":"markdown","d6a99345":"markdown","23c82f63":"markdown","8090d350":"markdown","5c077d52":"markdown","21876269":"markdown","2a21d261":"markdown","dfc79bfe":"markdown"},"source":{"4ab19b68":"import numpy as np\nimport scipy as sp\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Pandas options\npd.set_option('display.max_colwidth', 1000, 'display.max_rows', None, 'display.max_columns', None)\n\n# Plotting options\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set(style='whitegrid')","dee9d06b":"transactions = pd.read_csv('..\/input\/creditcard.csv')","58bd61aa":"transactions.shape","37bde505":"transactions.info()","ed3a22af":"transactions.isnull().any().any()","92863b71":"transactions.sample(5)","d723f071":"transactions['Class'].value_counts()","6559b119":"transactions['Class'].value_counts(normalize=True)","cbe15c60":"X = transactions.drop(labels='Class', axis=1) # Features\ny = transactions.loc[:,'Class']               # Response\ndel transactions                              # Delete the original data","e11d6fcf":"from sklearn.model_selection import train_test_split","79c36915":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\ndel X, y","fcb7f746":"X_train.shape","1f176b54":"X_test.shape","be5f2313":"# Prevent view warnings\nX_train.is_copy = False\nX_test.is_copy = False","be65d465":"X_train['Time'].describe()","010c8b15":"X_train.loc[:,'Time'] = X_train.Time \/ 3600\nX_test.loc[:,'Time'] = X_test.Time \/ 3600","27b2f85b":"X_train['Time'].max() \/ 24","e48c286e":"plt.figure(figsize=(12,4), dpi=80)\nsns.distplot(X_train['Time'], bins=48, kde=False)\nplt.xlim([0,48])\nplt.xticks(np.arange(0,54,6))\nplt.xlabel('Time After First Transaction (hr)')\nplt.ylabel('Count')\nplt.title('Transaction Times')","82924a03":"X_train['Amount'].describe()","e710f3bb":"plt.figure(figsize=(12,4), dpi=80)\nsns.distplot(X_train['Amount'], bins=300, kde=False)\nplt.ylabel('Count')\nplt.title('Transaction Amounts')","9282a3f9":"plt.figure(figsize=(12,4), dpi=80)\nsns.boxplot(X_train['Amount'])\nplt.title('Transaction Amounts')","10363421":"X_train['Amount'].skew()","5362cd95":"X_train.loc[:,'Amount'] = X_train['Amount'] + 1e-9 # Shift all amounts by 1e-9","22bdb4c2":"X_train.loc[:,'Amount'], maxlog, (min_ci, max_ci) = sp.stats.boxcox(X_train['Amount'], alpha=0.01)","97f0591e":"maxlog","0bea29c9":"(min_ci, max_ci)","24cef37e":"plt.figure(figsize=(12,4), dpi=80)\nsns.distplot(X_train['Amount'], kde=False)\nplt.xlabel('Transformed Amount')\nplt.ylabel('Count')\nplt.title('Transaction Amounts (Box-Cox Transformed)')","e12d4ad4":"X_train['Amount'].describe()","6297fefa":"X_train['Amount'].skew()","f1791c3b":"X_test.loc[:,'Amount'] = X_test['Amount'] + 1e-9 # Shift all amounts by 1e-9","8784c025":"X_test.loc[:,'Amount'] = sp.stats.boxcox(X_test['Amount'], lmbda=maxlog)","d13479a2":"sns.jointplot(X_train['Time'].apply(lambda x: x % 24), X_train['Amount'], kind='hex', stat_func=None, size=12, xlim=(0,24), ylim=(-7.5,14)).set_axis_labels('Time of Day (hr)','Transformed Amount')","4c360d10":"pca_vars = ['V%i' % k for k in range(1,29)]","4d570e62":"X_train[pca_vars].describe()","40152b83":"plt.figure(figsize=(12,4), dpi=80)\nsns.barplot(x=pca_vars, y=X_train[pca_vars].mean(), color='darkblue')\nplt.xlabel('Column')\nplt.ylabel('Mean')\nplt.title('V1-V28 Means')","7e6d27b3":"plt.figure(figsize=(12,4), dpi=80)\nsns.barplot(x=pca_vars, y=X_train[pca_vars].std(), color='darkred')\nplt.xlabel('Column')\nplt.ylabel('Standard Deviation')\nplt.title('V1-V28 Standard Deviations')","a856a32f":"plt.figure(figsize=(12,4), dpi=80)\nsns.barplot(x=pca_vars, y=X_train[pca_vars].skew(), color='darkgreen')\nplt.xlabel('Column')\nplt.ylabel('Skewness')\nplt.title('V1-V28 Skewnesses')","92c444c8":"plt.figure(figsize=(12,4), dpi=80)\nsns.distplot(X_train['V8'], bins=300, kde=False)\nplt.ylabel('Count')\nplt.title('V8')","a988e929":"plt.figure(figsize=(12,4), dpi=80)\nsns.boxplot(X_train['V8'])\nplt.title('V8')","dee9c7b1":"plt.figure(figsize=(12,4), dpi=80)\nplt.yscale('log')\nsns.barplot(x=pca_vars, y=X_train[pca_vars].kurtosis(), color='darkorange')\nplt.xlabel('Column')\nplt.ylabel('Kurtosis')\nplt.title('V1-V28 Kurtoses')","0997756d":"plt.figure(figsize=(12,4), dpi=80)\nsns.barplot(x=pca_vars, y=X_train[pca_vars].median(), color='darkblue')\nplt.xlabel('Column')\nplt.ylabel('Median')\nplt.title('V1-V28 Medians')","a7eafb59":"plt.figure(figsize=(12,4), dpi=80)\nsns.barplot(x=pca_vars, y=X_train[pca_vars].quantile(0.75) - X_train[pca_vars].quantile(0.25), color='darkred')\nplt.xlabel('Column')\nplt.ylabel('IQR')\nplt.title('V1-V28 IQRs')","0a4a76ce":"from sklearn.feature_selection import mutual_info_classif","6e526686":"mutual_infos = pd.Series(data=mutual_info_classif(X_train, y_train, discrete_features=False, random_state=1), index=X_train.columns)","ed3f812d":"mutual_infos.sort_values(ascending=False)","46e8d0ed":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import SGDClassifier","31eb2503":"pipeline_sgd = Pipeline([\n    ('scaler', StandardScaler(copy=False)),\n    ('model', SGDClassifier(max_iter=1000, tol=1e-3, random_state=1, warm_start=True))\n])","0d210f67":"param_grid_sgd = [{\n    'model__loss': ['log'],\n    'model__penalty': ['l1', 'l2'],\n    'model__alpha': np.logspace(start=-3, stop=3, num=20)\n}, {\n    'model__loss': ['hinge'],\n    'model__alpha': np.logspace(start=-3, stop=3, num=20),\n    'model__class_weight': [None, 'balanced']\n}]","e073d5ea":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer, matthews_corrcoef","b7d32866":"MCC_scorer = make_scorer(matthews_corrcoef)\ngrid_sgd = GridSearchCV(estimator=pipeline_sgd, param_grid=param_grid_sgd, scoring=MCC_scorer, n_jobs=-1, pre_dispatch='2*n_jobs', cv=5, verbose=1, return_train_score=False)","b4d32c76":"import warnings\nwith warnings.catch_warnings(): # Suppress warnings from the matthews_corrcoef function\n    warnings.simplefilter(\"ignore\")\n    grid_sgd.fit(X_train, y_train)","3b1dfc24":"grid_sgd.best_score_","d23e7110":"grid_sgd.best_params_","bda8d757":"from sklearn.ensemble import RandomForestClassifier","29c51342":"pipeline_rf = Pipeline([\n    ('model', RandomForestClassifier(n_jobs=-1, random_state=1))\n])","7492b893":"param_grid_rf = {'model__n_estimators': [75]}","4e189d0f":"grid_rf = GridSearchCV(estimator=pipeline_rf, param_grid=param_grid_rf, scoring=MCC_scorer, n_jobs=-1, pre_dispatch='2*n_jobs', cv=5, verbose=1, return_train_score=False)","ec7e9ff6":"grid_rf.fit(X_train, y_train)","520f8ab1":"grid_rf.best_score_","751ac490":"grid_rf.best_params_","c2634f08":"from sklearn.metrics import confusion_matrix, classification_report, matthews_corrcoef, cohen_kappa_score, accuracy_score, average_precision_score, roc_auc_score","dfa4e773":"def classification_eval(estimator, X_test, y_test):\n    \"\"\"\n    Print several metrics of classification performance of an estimator, given features X_test and true labels y_test.\n    \n    Input: estimator or GridSearchCV instance, X_test, y_test\n    Returns: text printout of metrics\n    \"\"\"\n    y_pred = estimator.predict(X_test)\n    \n    # Number of decimal places based on number of samples\n    dec = np.int64(np.ceil(np.log10(len(y_test))))\n    \n    print('CONFUSION MATRIX')\n    print(confusion_matrix(y_test, y_pred), '\\n')\n    \n    print('CLASSIFICATION REPORT')\n    print(classification_report(y_test, y_pred, digits=dec))\n    \n    print('SCALAR METRICS')\n    format_str = '%%13s = %%.%if' % dec\n    print(format_str % ('MCC', matthews_corrcoef(y_test, y_pred)))\n    if y_test.nunique() <= 2: # Additional metrics for binary classification\n        try:\n            y_score = estimator.predict_proba(X_test)[:,1]\n        except:\n            y_score = estimator.decision_function(X_test)\n        print(format_str % ('AUPRC', average_precision_score(y_test, y_score)))\n        print(format_str % ('AUROC', roc_auc_score(y_test, y_score)))\n    print(format_str % (\"Cohen's kappa\", cohen_kappa_score(y_test, y_pred)))\n    print(format_str % ('Accuracy', accuracy_score(y_test, y_pred)))","0a665a66":"classification_eval(grid_rf, X_test, y_test)","39c3a115":"Before we begin preprocessing, we split off a test data set. First split the data into features and response variable:","9cec7723":"Let's compare the descriptive stats of the PCA variables `V1-V28`.","d8ac3f6f":"That's a strong right skew. Let's use a power transform to bring the transaction amounts closer to a normal distribution. We'll use the [Box-Cox transform in SciPy](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.boxcox.html), but some of the amounts are zero (min = 0 above), so we need to shift the amounts first to make them positive. We'll shift by a very small amount, just $10^{-9}$.","005b133d":"The grid search, implemented by [`GridSearchCV`](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html), uses [`StratifiedKFold`](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold) with 5 folds for the train\/validation splits. We'll use [`matthews_corrcoef`](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.matthews_corrcoef.html) (the [Matthews correlation coefficient](https:\/\/en.wikipedia.org\/wiki\/Matthews_correlation_coefficient), MCC) as our scoring metric.","97edb762":"So the transactions indeed occur over a two-day period. Next let's plot a histogram of transaction times, with one bin per hour:","e03e72bd":"In this project we analyze a dataset of credit card transactions made over a two-day period in September 2013 by European cardholders. The dataset contains 284,807 transactions, of which 492 (0.17%) are fraudulent.\n\nEach transaction has 30 features, all of which are numerical. The features `V1, V2, ..., V28` are the result of a PCA transformation. To protect confidentiality, background information on these features is not available. The `Time` feature contains the time elapsed since the first transaction, and the `Amount` feature contains the transaction amount. The response variable, `Class`, is 1 in the case of fraud, and 0 otherwise.\n\nOur goal in this project is to construct models to predict whether a credit card transaction is fraudulent. We'll attempt a supervised learning approach. We'll also create visualizations to help us understand the structure of the data and unearth any interesting patterns.","ee946125":"## 4.2 Amount\n<a id='4.2'><\/a>","3b2f99d3":"How balanced are the classes, i.e. how common are fraudulent transactions?","50e64d2f":"[Mutual information](https:\/\/en.wikipedia.org\/wiki\/Mutual_information) is a non-parametric method to estimate the mutual dependence between two variables. Mutual information of 0 indicates no dependence, and higher values indicate higher dependence. According to the [sklearn User Guide](http:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html#univariate-feature-selection), \"mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation.\" We have 227,845 training samples, so mutual information should work well. Because the target variable is discrete, we use [`mutual_info_classif`](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif) (as opposed to [`mutual_info_regression`](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression) for a continuous target).","6cbfa3df":"# 2. Getting Started\n<a id='2'><\/a>","683412e2":"The IQRs of `V1-V28` are on a similar scale as the standard deviations.","0885d23f":"The maximum likelihood estimate of $\\lambda$ in the Box-Cox transform:","c115f4b5":"## 1.1 References\n<a id='1.1'><\/a>","2aea90c8":"Import basic libraries:","a2ac0fa2":"* [Kaggle Dataset](https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud)\n* [Kaggle Notebook](https:\/\/www.kaggle.com\/pileatedperch\/linear-svc-random-forest-mcc-0-77-0-88) - A version of this notebook hosted on Kaggle\n* [GitHub Repository](https:\/\/github.com\/jgcorliss\/credit-card-fraud) - The GitHub repository for this project","44b90327":"Plot the newly transformed amounts:","6e1947d6":"Perform the grid search:","0e474ac6":"Full table of descriptive stats:","be87a775":"The histogram is hard to read due to some outliers we can't see. A boxplot will show the outliers:","e579855c":"The random forest takes much longer to train on this fairly large dataset, so we don't actually do a hyperparameter grid search, only specifiying the number of estimators. We'll leave the grid search implemented in case we decide to try different hyperparameter values in the future.","0ae9daa9":"The calculated mutual informations of each variable with `Class`, in descending order:","1bf6b758":"The transaction amounts appear to be similarly distributed throughout the daytime hours. However, in the earliest hours of the day, around 5-7 AM, amounts around 2.5 are the most common (recall this is a Box-Cox transformed value). Perhaps everyone's buying their morning coffee?","611209e5":"# 4. Exploratory Data Analysis\n<a id='4'><\/a>","5cfd5cb4":"The boxplot is also hard to read due to the large number of outliers, which indicates high kurtosis in `V8`. This motivates us to plot the kurtoses of the PCA variables. The kurtosis method employed in pandas is Fisher\u2019s definition, for which the standard normal distribution has kurtosis 0.","1d653de3":"According to the cross-validated MCC scores, the random forest is the best-performing model, so now let's evaluate its performance on the test set.","b5f3ec2b":"We conduct EDA only on the training set, and leave the test set unknown.","e92dc417":"We'll conduct a grid search over several hyperparameter choices. The search uses 5-fold cross-validation with stratified folds. The type of linear classifier is chosen with the `loss` hyperparameter. For a linear SVC we set `loss = 'hinge'`, and for logistic regression we set `loss = 'log'`.\n\nSet the hyperparameter grids to search over, one grid for the linear SVC and one for logistic regression:","d6058e97":"# 7. Test Set Evaluation of the Best Model\n<a id='7'><\/a>","6ab5ec84":"According to the MCC, the random forest performed better on the test set than on the training set. This is probably due to the refit model being trained on the entire training data set, and not on the smaller CV folds.","f45a9035":"Mean cross-validated MCC score of the best estimator found:","31491d89":"We can see there are no outliers on the left and many outliers on the right. So the amounts certainly seem right-skewed. We can calculate the skewness to be sure:","3ddf9889":"The five most correlated variables with `Class` are, in decreasing order, V17, V14, V10, V12, and V11.","92c20c0e":"This is a pretty good MCC score---random guessing has a score of 0, and a perfect predictor has a score of 1. Now check the best hyperparameters found in the grid search:","b2280b0d":"What's the skewness of the transformed amounts? (Zero skewness would be ideal.)","d41d747c":"It looks like there are two lulls in credit card transactions during nighttime on each day.","791b5187":"Check basic metadata.","0d707c5a":"Summary statistics:","7da94868":"# 1. Introduction\n<a id='1'><\/a>","8edd3d21":"We were able to accurately identify fraudulent credit card transactions using a random forest model. We found that the five variables most correlated with fraud are, in decreasing order, V17, V14, V10, V12, and V11. Only a few preprocessing steps were necessary before constructing predictive models:\n* Split the data using a random, stratified train\/test split with a test size of 20%\n* Box-Cox power transform of the transaction amounts to remove skewness in the data\n* Mean and variance standardization of all features as part of a machine learning pipeline\n\nWe used the [Matthews correlation coefficient (MCC)](https:\/\/en.wikipedia.org\/wiki\/Matthews_correlation_coefficient) to compare the performance of different models. In cross validation, the best linear model (logistic regression, linear SVC) achieved a cross-validated MCC score of 0.807, and a random forest achieved a cross-validated MCC score of 0.856. We therefore chose the random forest as the better model, which obtained an MCC of 0.869 on the test set.\n\nTo improve a chosen model, we searched over a grid of hyperparameters and compared performance with cross-validation. It may be possible to improve the random forest model by further tweaking the hyperparameters, given additional time and\/or computational power.","3923761f":"So the linear SVC performed better than logistic regression, and with a high level of regularization ($\\alpha\\approx 483$).","68c7ecc4":"We do not need to rescale the data for tree-based models, so our pipeline will simply consist of the random forest model. We'll leave the pipeline implementation in place in case we want to add preprocessing steps in the future.","8949445f":"A few of the PCA variables are significantly skewed. Let's plot a histogram of one of the particularly skewed variables, `V8`, to see the distribution in detail.","0934db25":"Evaluate the random forest on the test set:","966acaba":"## Contents","7ed080e4":"The PCA variables have roughly unit variance, but as low as ~0.3 and as high as ~1.9. Plot the skewnesses next:","4180aa12":"Much better. The distribution appears to be bimodal, suggesting a divide between \"small\" and \"large\" purchases. Now let's check the descriptive stats of the transformed amounts:","f99b3bbd":"* [1. Introduction](#1)\n    * [1.1 References](#1.1)\n* [2. Getting Started](#2)\n* [3. Train\/Test Split](#3)\n* [4. Exploratory Data Analysis](#4)\n    * [4.1 Time](#4.1)\n    * [4.2 Amount](#4.2)\n    * [4.3 Time vs. Amount](#4.3)\n    * [4.4 V1-V28](#4.4)\n* [5. Mutual Information between Fraud and the Predictors](#5)\n* [6. Modeling](#6)\n    * [6.1 Logistic Regression and Support Vector Classifier](#6.1)\n    * [6.2 Random Forest](#6.2)\n* [7. Test Set Evaluation of the Best Model](#7)\n* [8. Conclusion](#8)","e268f906":"Perform the grid search:","2ef94cb6":"Let's convert the time from seconds to hours to ease the interpretation.","742fb5fc":"Are there any variables with missing data?","6fcbf809":"The class [`SGDClassifier`](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDClassifier.html) implements multiple linear classifiers with SGD training, which makes learning much faster on large datasets. We'll implement the model as a machine learning pipeline that includes [`StandardScaler`](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html) for data standardization (rescaling each variable to zero mean and unit variance).","28ec60bf":"# 8. Conclusion\n<a id='8'><\/a>","eb3dd7a1":"All of `V1-V28` have approximately zero mean. Now plot the standard deviations:","cd5bf254":"The random forest performed much better than the linear SVC---and without any hyperparameter tweaking!","fde1058e":"Now we're ready to build machine learning models to predict whether a transaction is fraudulent. We'll train the following models:\n* Logistic regression\n* Support vector classifier\n* Random forest","41b4037c":"Read in the data into a pandas dataframe.","c3937852":"The 99% confidence interval for $\\lambda$:","089d6c14":"Perform the Box-Cox transform:","a22badf9":"The histogram doesn't show us outliers. Let's try a boxplot:","66acc5a7":"Comparing the different quantiles, it looks like the amounts are very right-skewed. To verify this, plot a histogram of the transaction amounts:","466d3367":"What is the time of the last transaction, in days?","c7977456":"No! Let's view five randomly chosen transactions.","25f3228d":"## 4.3 Time vs. Amount\n<a id='4.3'><\/a>","08d8300d":"# 3. Train\/Test Split\n<a id='3'><\/a>","b75439eb":"We've learned that many of the PCA variables are heavy-tailed. The large numbers of outliers in `V1-V28` motivates us to consider robust descriptive statistics. Let's plot the medians:","390e997b":"## 4.4 V1-V28\n<a id='4.4'><\/a>","1327bb76":"## 4.1 Time\n<a id='4.1'><\/a>","0471aa41":"We'll use a test size of 20%. We also stratify the split on the response variable, which is very important to do because there are so few fraudulent transactions.","bf50e8be":"## 6.2 Random Forest\n<a id='6.2'><\/a>","6cafd837":"Only 0.17% (492 out of 284,807) transactions are fraudulent.","d4c9d50a":"Next we'll try a random forest model, implemented in [`RandomForestClassifier`](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html).","8fed2a12":"Is there a relationship between the transaction amounts and the time of day? Let's explore this question with a joint histogram using hexagonal bins. For this plot, we convert each transaction time to the hour of the day on which it occurred.","e4bde828":"The medians are also roughly zero. Next let's look at the interquartile ranges (IQR)*:\n\n*Pandas does not have a built-in IQR method, but we can use the [`quantile`](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.quantile.html) method to calculate the IQR.","ed1ddade":"It's tedious to interpret this table, so let's make some visualizations. We'll start by plotting the means:","d6a99345":"So our power transform removed most of the skewness in the `Amount` variable. Now we need to compute the Box-Cox transform on the test data amounts as well, using the $\\lambda$ value estimated on the training data.","23c82f63":"# 6. Modeling\n<a id='6'><\/a>","8090d350":"# 5. Mutual Information between Fraud and the Predictors\n<a id='5'><\/a>","5c077d52":"Note the log scale on the y-axis in the plot below:","21876269":"## 6.1 Logistic Regression and Support Vector Classifier\n<a id='6.1'><\/a>","2a21d261":"# Credit Card Fraud Detection\n\n***By Joe Corliss***\n\n**July 12, 2018**","dfc79bfe":"Few descriptive statistics for the `Time` variable:"}}