{"cell_type":{"29dfa904":"code","633241ad":"code","0ba6f9ff":"code","5d017e66":"code","336602d5":"code","ac1d7430":"code","c4bde12b":"code","f4dd9167":"code","c7f16975":"code","795ed8c4":"code","1e5d8d4c":"code","19692e2a":"code","07990115":"code","2871ccf5":"code","850550fe":"code","319c8bad":"code","bf645ee6":"code","b4c52d04":"code","d6cbbf7c":"code","4248390b":"code","bbf253e5":"code","81082b1c":"code","c067a20e":"markdown","258f00fb":"markdown","fdc66a0c":"markdown","5ebeebba":"markdown","564f1e01":"markdown","205beaca":"markdown","1184f600":"markdown","029c80db":"markdown","a3e39410":"markdown","02a10a37":"markdown","c5144015":"markdown","5bd6defd":"markdown","3c0c38af":"markdown","82889c5d":"markdown","c655bfa7":"markdown","77d7c3a1":"markdown","9f75a0f5":"markdown","1c7859de":"markdown","cc3d86b3":"markdown","0f1a8b78":"markdown","c77d737f":"markdown","1dd11cfa":"markdown","e9012484":"markdown","0aed874a":"markdown","0e53c9fb":"markdown","4417babc":"markdown","c3d32b14":"markdown","f46a666f":"markdown","da46afe1":"markdown","8d1dadf0":"markdown","5496d88a":"markdown","4e9caeac":"markdown","9f8b454e":"markdown","add288bf":"markdown","3c56c03d":"markdown","5f32f0a6":"markdown","9f5d367c":"markdown","c8e8f12e":"markdown","6b5dd8b6":"markdown","f4fda8dc":"markdown","ec6c43fc":"markdown","d2c06cf6":"markdown","4472287f":"markdown","ad73fea6":"markdown","a9a36203":"markdown"},"source":{"29dfa904":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\n\ndf_train = pd.read_csv('..\/input\/porto-seguro-safe-driver-prediction\/train.csv')\ndf_test = pd.read_csv('..\/input\/porto-seguro-safe-driver-prediction\/test.csv')\nprint('train data size', df_train.shape)\nprint('test data size', df_test.shape)\ndisplay(df_train.head())\ndisplay(df_test.head())","633241ad":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style('white')\nsns.set_palette(\"Paired\")\nsns.set(font_scale=1.5)\nplt.figure()\nsns.countplot(df_train['target'],palette='nipy_spectral')\nplt.show()","0ba6f9ff":"\nprint('Nan values =', df_train.isnull().sum().sum())\ndf_missing_train = pd.DataFrame({'column':df_train.columns, 'missing(%)':((df_train==-1).sum()\/df_train.shape[0])*100})\ndf_missing_test = pd.DataFrame({'column':df_test.columns, 'missing(%)':((df_test==-1).sum()\/df_test.shape[0])*100})\n\ndf_missing_train_nl = df_missing_train.nlargest(7, 'missing(%)')\ndf_missing_test_nl = df_missing_test.nlargest(7, 'missing(%)')\nsns.set_palette(sns.color_palette('nipy_spectral'))\nplt.figure(figsize=(16,6))\nsns.barplot(data= df_missing_train_nl, x='column', y='missing(%)',palette='nipy_spectral')\nplt.title('Missing values (%) in training set')\nplt.show()\nplt.figure(figsize=(16,6))\nsns.barplot(data= df_missing_test_nl, x='column', y='missing(%)',palette='nipy_spectral')\nplt.title('Missing values (%) in test set')\nplt.show()","5d017e66":"plt.figure(figsize=(18,10))\nsns.heatmap(df_train==-1, cmap='gray', cbar=False)\nplt.title('Missing values in training set')\nplt.show()","336602d5":"\nfeatures = df_train.columns.tolist()\ncat_features = [c for c in features if 'cat' in c]\nbin_features = [b for b in features if 'bin' in b]\ncat_features_df = pd.DataFrame({'Categorical features': cat_features})\nbin_features_df = pd.DataFrame({'Binary features': bin_features})\n\nn_row = len(cat_features)\nn_col = 2   \nn_sub = 1   \nfig = plt.figure(figsize=(20,50))\nplt.subplots_adjust(bottom=-0.2,top=1.2)\nfor i in range(len(cat_features)):\n    plt.subplot(n_row, n_col, n_sub)\n    sns.countplot(x= df_train[cat_features[i]],palette='nipy_spectral')\n    n_sub+=1\nplt.show()\n\nn_row = len(bin_features)\nn_col = 2   \nn_sub = 1      \nfig = plt.figure(figsize=(20,50))\nplt.subplots_adjust(bottom=-0.2,top=1.2)\nfor i in range(len(bin_features)):\n    plt.subplot(n_row, n_col, n_sub)\n    sns.countplot(x= df_train[bin_features[i]],palette='nipy_spectral')\n    n_sub+=1   \nplt.show()","ac1d7430":"int_features = df_train.select_dtypes(include=['int64']).columns.tolist()\n\nordinal_features = [o for o in int_features if ('cat' not in o and 'bin' not in o and 'id' not in o and 'target' not in o )]\nord_features_df = pd.DataFrame({'Ordinal features': ordinal_features})\n\nn_row = len(ordinal_features)\nn_col = 2   \nn_sub = 1      \nfig = plt.figure(figsize=(20,50))\nplt.subplots_adjust(bottom=-0.2,top=1.2)\nfor i in range(len(ordinal_features)):\n    plt.subplot(n_row, n_col, n_sub)\n    sns.countplot(x= df_train[ordinal_features[i]],palette='nipy_spectral')\n    n_sub+=1   \nplt.show()","c4bde12b":"cont_features = df_train.select_dtypes(include=['float64']).columns.tolist()\ncont_features_df = pd.DataFrame({'Numerical Continuous features': cont_features})\ncont_features.remove('ps_calc_01')\ncont_features.remove('ps_calc_02')\ncont_features.remove('ps_calc_03')\n\nn_row = len(cont_features)\nn_col = 2   \nn_sub = 1      \nfig = plt.figure(figsize=(20,30))\nplt.subplots_adjust(bottom=-0.2,top=1.2)\nfor i in range(len(cont_features)):\n    plt.subplot(n_row, n_col, n_sub)\n    sns.distplot(df_train[cont_features[i]], kde=False,color='Red')\n    n_sub+=1   \nplt.show()","f4dd9167":"a = df_train[cont_features]\nplt.figure(figsize=(10,6))\nsns.heatmap(a.corr(), annot=True,cmap='nipy_spectral')\nplt.title('Pearson Correlation of continuous features')\nplt.show()","c7f16975":"X_train = df_train.drop(['id', 'target'], axis=1)\ny_train = df_train['target']","795ed8c4":"from sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, recall_score, accuracy_score, f1_score, roc_auc_score\nfrom time import time\n\ndef metrics(true, preds):\n    accuracy = accuracy_score(true, preds)\n    recall = recall_score(true, preds)\n    f1score = f1_score(true, preds)\n    cf = confusion_matrix(true, preds)\n    print('accuracy: {}, recall: {}, f1-score: {}'.format(accuracy, recall, f1score))\n    print('Confusion matrix', cf)\n\ndef gini(true, preds):\n    res = 2* roc_auc_score(true, preds) - 1\n    return res\n\nX_tr, X_te, y_tr, y_te = train_test_split(X_train, y_train, random_state=42, test_size=0.2)\n\ndef test_clfs(clfs):\n    for clf in clfs:\n        print('------------------------------------------')\n        start = time()\n        clf = clf(random_state=42)\n        clf.fit(X_tr, y_tr)\n        y1_pred = clf.predict(X_te)\n        y1_pred_prob = clf.predict_proba(X_te)\n        print(str(clf), '\\nresults:')\n        metrics(y_te, y1_pred)\n        print('gini score', gini(y_te, y1_pred_prob[:, 1]))\n        end = time()\n        print('Processing time', end-start,'s')\n\nclassifiers = [LGBMClassifier, XGBClassifier, RandomForestClassifier]\ntest_clfs(classifiers)","1e5d8d4c":"from imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\n\nsm_methods = [SMOTE,RandomUnderSampler,RandomOverSampler]\nclf = LGBMClassifier(random_state=42)\nfor sm in sm_methods:\n    print('------------------------------------------')\n    sm = sm(random_state=42)\n    start = time()\n    X_train_resampled, y_train_resampled = sm.fit_sample(X_tr, y_tr)\n    clf.fit(X_train_resampled, y_train_resampled)\n    y_pred = clf.predict(X_te)\n    y_pred_prob = clf.predict_proba(X_te)\n    print(str(sm), 'results:')\n    print('training resampled shape', X_train_resampled.shape)\n    print('value counts in each class', y_train_resampled.value_counts())\n    metrics(y_te, y_pred)\n    gini_score = gini(y_te, y_pred_prob[:, 1])\n    print('Normalized gini score', gini_score)\n    end = time()\n    print('Processing time', end-start,'s')","19692e2a":"from sklearn.model_selection import StratifiedKFold, cross_val_predict\n\ncv = StratifiedKFold(n_splits=5)\nscores_gini = []\n\nfor train_index, validation_index in cv.split(X_train, y_train):\n    train, val = X_train.iloc[train_index], X_train.iloc[validation_index]\n    target_train, target_val = y_train.iloc[train_index], y_train.iloc[validation_index]\n    clf = LGBMClassifier(random_state=42)\n    clf.fit(train, target_train)\n    y_val_pred = clf.predict(val)\n    y_val_pred_prob = clf.predict_proba(val)\n    gini_score = gini(target_val, y_val_pred_prob[:, 1])\n    print('gini score:', gini_score)\n    scores_gini.append(gini_score)\n    \nprint('mean gini_score: {}'.format(np.mean(scores_gini)))\n","07990115":"df_train = pd.read_csv('..\/input\/porto-seguro-safe-driver-prediction\/train.csv')\nX_train = df_train.drop(['id', 'target'], axis=1)\ny_train = df_train['target']\nfeatures = df_train.columns.tolist()\ncat_features = [c for c in features if 'cat' in c]\nX_train = pd.get_dummies(X_train, columns=cat_features, prefix_sep='_', drop_first=True)\n","2871ccf5":"# subsample = [0.5, 0.7, 0.9]\n# num_leaves = [10, 12, 15, 20, 25, 30]\n# learning_rate = [0.1, 0.15, 0.2]\n# n_estimators = [50, 100, 150, 200]\n# min_child_weight = [0.01, 0.1, 1, 10, 50, 100, 150, 200]\n# min_child_samples = [5, 10, 15, 20, 25]\n\n# params = dict(num_leaves=num_leaves, subsample=subsample, learning_rate=learning_rate,\n#               n_estimators=n_estimators, min_child_samples=min_child_samples, min_child_weight=min_child_weight)\n# clf = GridSearchCV(estimator=LGBMClassifier(random_state=42, n_jobs=-1), param_grid=params, scoring='roc_auc', cv=cv, n_jobs=-1,\n#                    verbose=2)\n# clf.fit(X_train, y_train)\n\n# print('best params', clf.best_params_)\n# print('best score', clf.best_score_)\n\n# means = clf.cv_results_['mean_test_score']\n# stds = clf.cv_results_['std_test_score']\n# for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n#     print(\"%0.3f (+\/-%0.03f) for %r\"\n#           % (mean, std * 2, params))\nbest_params = {'learning_rate': 0.1, 'min_child_samples': 5, 'min_child_weight': 100, 'n_estimators': 100, 'num_leaves': 20, 'subsample': 0.5}\nprint('best parameters', best_params)","850550fe":"scores_gini = []\nfor train_index, validation_index in cv.split(X_train, y_train):\n    train, val = X_train.iloc[train_index], X_train.iloc[validation_index]\n    target_train, target_val = y_train.iloc[train_index], y_train.iloc[validation_index]\n    clf = LGBMClassifier(random_state=42, n_jobs=-1)\n    clf.set_params(**best_params)\n    clf.fit(train, target_train)\n    y_val_pred = clf.predict(val)\n    y_val_pred_prob = clf.predict_proba(val)\n    gini_score = gini(target_val, y_val_pred_prob[:, 1])\n    print('gini score:', gini_score)\n    scores_gini.append(gini_score)\n\nprint('mean gini_score: {}'.format(np.mean(scores_gini)))","319c8bad":"pd.set_option('display.max_columns', None)\ny_preds = y_val_pred_prob[:,1]\nresults = pd.DataFrame({'target_true':target_val , 'target_pred': y_preds})\nb = results.nlargest(10, columns='target_pred')\nb2 = results.nlargest(1000, columns='target_pred')\nc = results.nsmallest(1000, columns='target_pred')\nc_counts_1 = (c['target_true']==1).sum()\nb2_counts_1 =  (b2['target_true']==1).sum()\nprint('Numeber of 1s in 1000 smallest probabilities:', c_counts_1)\nprint('Number of 1s in 1000 largest probabilities:', b2_counts_1 )\ndisplay(b)\ndisplay(val.loc[b.index])","bf645ee6":"from sklearn.feature_selection import SelectFromModel\n\nfeatures = train.columns\nimportances = clf.feature_importances_\nindices = np.argsort(clf.feature_importances_)[::-1]\nimp_features= pd.DataFrame({'feature':features[indices], 'importance':importances[indices]})\ndisplay(imp_features)\nimp_features_50smallest = imp_features.nsmallest(50, 'importance')\nfeatures_to_drop = imp_features_50smallest['feature'].tolist()\nimp_features_10large = imp_features.nlargest(10, 'importance')\nplt.figure(figsize=(15,8))\nsns.barplot(data=imp_features_10large, x='feature', y='importance',palette='nipy_spectral')\nplt.show()","b4c52d04":"print(imp_features_10large)\ndisplay(val.loc[b.index, imp_features_10large.feature])","d6cbbf7c":"features = X_train.columns.tolist()\ncat_car_05 = [c for c in features if 'car_05_cat' in c]\ncalc_features = [c for c in features if 'calc' in c]\nX_train.drop(calc_features, axis=1, inplace=True)\nX_train.drop(cat_car_05, axis=1, inplace=True)\nX_train['ps_reg_03'].replace(-1, X_train['ps_reg_03'].median(), inplace=True)\nX_train['ps_car_14'].replace(-1, X_train['ps_car_14'].median(), inplace=True)\n\nscores_gini = []\nfor train_index, validation_index in cv.split(X_train, y_train):\n    train, val = X_train.iloc[train_index], X_train.iloc[validation_index]\n    target_train, target_val = y_train.iloc[train_index], y_train.iloc[validation_index]\n    clf = LGBMClassifier(random_state=42, n_jobs=-1)\n    clf.set_params(**best_params)\n    clf.fit(train, target_train)\n    y_val_pred = clf.predict(val)\n    y_val_pred_prob = clf.predict_proba(val)\n    gini_score = gini(target_val, y_val_pred_prob[:, 1])\n    print('gini score:', gini_score)\n    scores_gini.append(gini_score)\n    \nprint('mean gini_score: {}'.format(np.mean(scores_gini)))","4248390b":"y_preds = y_val_pred_prob[:,1]\nresults = pd.DataFrame({'target_true':target_val , 'target_pred': y_preds})\nb = results.nlargest(10, columns='target_pred')\nb2 = results.nlargest(1000, columns='target_pred')\nc = results.nsmallest(1000, columns='target_pred')\nc_counts_1 = (c['target_true']==1).sum()\nb2_counts_1 =  (b2['target_true']==1).sum()\nprint('Numeber of 1s in 1000 smallest probabilities:', c_counts_1)\nprint('Number of 1s in 1000 largest probabilities:', b2_counts_1 )\n\n","bbf253e5":"# Same process to df_test before submission\ndf_test = pd.read_csv('..\/input\/porto-seguro-safe-driver-prediction\/test.csv')\nX_test = df_test.drop('id', axis=1)\nX_test = pd.get_dummies(X_test, columns=cat_features, prefix_sep='_', drop_first=True)\nX_test.drop(calc_features, axis=1, inplace=True)\nX_test.drop(cat_car_05, axis=1, inplace=True)\nX_test['ps_reg_03'].replace(-1, X_test['ps_reg_03'].median(), inplace=True)\nX_test['ps_car_14'].replace(-1, X_test['ps_car_14'].median(), inplace=True)\nassert(X_train.shape[1]==X_test.shape[1])\nclf.fit(X_train, y_train)","81082b1c":"# Submission \npreds =  clf.predict_proba(X_test)[:,1]\ndf_subm = pd.read_csv('..\/input\/porto-seguro-safe-driver-prediction\/sample_submission.csv')\ndf_subm.loc[:,'target'] = preds\ndisplay(df_subm)\ndf_subm.to_csv('submission.csv', index=False)\n","c067a20e":"<a id=\"243\"><\/a>\n\n### Numerical continuous features\nWe will visualize the continous features with distplots:","258f00fb":"<a id=\"32\"><\/a>\n\n## Dealing with imbalance dataset\nWe will train our model with the transformed data. The transformations are random undersampling, random oversampling and SMOTE over sampling $\\href{https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/api.html}{[2]}$.","fdc66a0c":"We will replace missing values of ps_reg_03 and ps_car_14 with their median value. We also drop cat_car_05 due its many missing values. Indeed, it increases the score. ","5ebeebba":"<a id=\"241\"><\/a>\n\n### Categorical features","564f1e01":"Well, we increased our mean normalized gini score on cross-validation sets to 0.281. Let's get back to our data and see how our model performed on some examples.","205beaca":"'ps_reg_3' seems to be right skewed. We can see many -1 values in this feature which we will replace later.<br>\nNext figure is a pearson correlation plot which is a measure of linear correlation between two variables. ","1184f600":"What happened? Well, resampling helps our classifier to improve the hard classification. As we can see, accuracy is more realistic now (as we have same number of class labels) and from confusion matrix, recall and f1 score, we can see many positive points are correctly classified. However, gini score, did not improve. The reason could be that resampling helps our algorithm to make the classifications better, however, this doesn't matter for gini score. The only thing matters is that we can predict higher probabilities for an example whose true label is 1 than 0. By resampling, we are imposing some changes to our real original distribution which can result in loss of gini score. Since gini score is the metric for this competition, we will not perform resampling on the original dataset.\n","029c80db":"<a id=\"6\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:red; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>What I Learned<\/center><\/h2>\n","a3e39410":"<a id=\"242\"><\/a>\n\n### Ordinal features \nFeatures which are not categorical or binary are either numerical or ordinal. Ordinal feature can be something like the model year of the car. For example, when comparing 2011 and 2016 models of the same make, the order matters. ","02a10a37":"Before hyperparameter tuning, let's train our simple model with cross-validation sets. We will use StratifiedKFold to make sure we have same ratio of classes in each cross-val set. ","c5144015":"From the first table above, we can see that only 2 out of 10 highest predicted probabilities had true labels of 1. In the perfect world, we expect to see that any true label of 1 have high probabilities. In the second table, we can see the feature values of that 10 examples. Example 2 and Example 7 had true labels of 1, but why the others got high probability, even though their true label was 0?<br> In total, we can also see that from 1000 largest and smallest probabilities, there are 130, and 13 true 1s respectively. 13 out of 1000 for low probabilities seems good, but 130 out 1000 for the largest ones, does not seem good enough. <br>\nNext we are going to see the feature importances from the model's perspective.","5bd6defd":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:red; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Quick navigation<\/center><\/h2>\n\n    \n    \n* [Problem Description](#1)\n* [Explanatory Data Analysis (EDA)](#2)\n    - [Train and test data](#21)\n    - [Target labels](#22)\n    - [Missing values](#23)\n    - [Exploring features](#24)\n        - [Categorical features](#241)\n        - [Ordinal features](#242)\n        - [Numerical continuous features](#243)\n* [Training](#3)\n    - [Training three simple models](#31)\n    - [Dealing with imbalance dataset](#32)\n        - [Undersampling and Oversampling](#321)\n    - [Train and validate (Stratified KFold)](#33)\n    - [Hyperparameter tuning](#34)\n* [Evaluation and Summary](#4)\n* [Challenges](#5)\n* [What I Learned](#6)\n* [References](#7)\n","3c0c38af":"<a id=\"34\"><\/a>\n\n## Hyperparameter tuning\n\nWe are going to search among these parameters for our lightgbm classifiers: \n\nsubsample = [0.5, 0.7, 0.9] <br>\nnum_leaves = [10, 12, 15, 20, 25, 30]<br>\nlearning_rate = [0.1, 0.15, 0.2]<br>\nn_estimators = [50, 100, 150, 200]<br>\nmin_child_weight = [0.01, 0.1, 1, 10, 50, 100, 150, 200]<br>\nmin_child_samples = [5, 10, 15, 20, 25]<br>\n\nBefore search, we encode our categorial features into one hot format. ","82889c5d":"Ok, so far, we got mean gini score of 0.273 on our cross-validation sets. Let's tune some hyper parameters of our model. ","c655bfa7":"From the above figure we can see: <br>\n\n'ps_car_1' and 'ps_car_2' and 'ps_car_3' have high linear correlation with eachother.<br>\n\n'ps_car_12' and 'ps_car_'13' have the highest linear correlation.<br>\n\n'ps_car_13', and 'ps_car_15' also have high correlation.<br>\n\nIt seems that ps_calc continous features do not have any linear correlation with other continous features. ","77d7c3a1":"We can see that 'ps_car_03_cat' and 'ps_car_05_cat' have many missing values (69% and 45% respectively). For now, we are going to keep all the features.","9f75a0f5":"<a id=\"31\"><\/a>\n\n## Training three simple models\nFirst, we will train three models of random forest, xgboost and lightgbm to see how is their performance. \nAt this point, there is no change to the features (we just simply drop id column). \n","1c7859de":"Here are categorical and binary features. The following figures shows the frequency of each label in categorical and binary features.","cc3d86b3":"Now, let's see how the top 10 data differ in these features with each other.\n","0f1a8b78":"<a id=\"321\"><\/a>\n\n### Undersampling and Oversampling","c77d737f":"<a id=\"1\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:red; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Problem Description<\/center><\/h2>","1dd11cfa":"Before doing any data science project, it is important to ask ourselves what is the problem we are going to solve? Can we predict if someone is going to file an insurance claim next year? It seems this problem has too much randomness in its nature and it will be hard to predict the future based on the past data. This is why the best score of the leaderboard is not far better than a random guess. However, we cannot ignore the predictive power of some features. \n\nPorto Seguro, is one of Brazil\u2019s largest auto and homeowner insurance companies. The problem we are going to tackle is to train a model that can predict the probability that a driver will initiate an auto insurance claim in the next year. An accurate model  will allow the company to further tailor their prices and gain more profit and it can be benefical for safe drivers as well.\n\nIn this project, I will use Lightgbm boosting algorithm which is a gradient boosting framework that uses tree based learning algorithms. As we will see later, this algorithm is really light!\n<a href=\"https:\/\/lightgbm.readthedocs.io\/en\/latest\/\"> [1]<\/a>:","e9012484":"From the above figures we can see:<br>\n\n1) 'ps_ind_14' has almost only 0s.<br>\n\n2) Many of the ordinal features have more than 7 categories with some over 25. We have know idea what they really are.","0aed874a":"<a id=\"4\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:red; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Evaluation and Summary<\/center><\/h2>","0e53c9fb":"Now we train and test again with new features and tuned hyperparameters: ","4417babc":"<a id=\"23\"><\/a>\n\n## Missing values","c3d32b14":"<a id=\"5\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:red; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Challenges<\/center><\/h2>\n\nThere were many challenges in this problem which were mainly from the data while some from the algorithms. <br>\n\n1. The imbalance of the dataset was challenging. Learning how to do resampling, and why we should or should not do the resampling was a challenge. <br>\n\n2. Dealing with cross-validation and the problem of data leakage after resampling was challenging. <br>\n\n3. The limitation of the classifiers in terms of speed was challenging. <br>\n\n4. Feature engineering was very hard, because we do not know the real names of features and we can not have intuition on them. ","f46a666f":"<h1><center>Safe driver prediction: A comprehensive data analysis, visualization, and modeling<\/center><\/h1>\n<center><img src=\"https:\/\/images.unsplash.com\/photo-1449965408869-eaa3f722e40d?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1050&q=80\"><\/center>\n","da46afe1":"<a id=\"2\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:red; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Explanatory Data Analysis (EDA)<\/center><\/h2>\n","8d1dadf0":"Without parameter search, We can see that LGBMClassifier and XGBoost have better gini scores than randomforest. However, Lightgbm is mush faster and thus it can be helpful in extensive hyperparameter searching. Therefore, we will continue with parameter tuning only for Lightgbm.\n\nIt should be also noted that, as we can see from the confusion matrix, all the positive cases (1s) are missclassified, however we still have a very good gini score while recall is 0. The reason is for hard-classification the probability threshold of the classifier is 0.5. However, in this competition, we do not care about the hard predicted labels, and the only thing matters is the relative probability of the points in comparsion with each other. For example, we expect a higher probability for someone who is going to claim than someone who is not. Therefore, gini score is used in this competition and we will fine tune our parameters to maximize this score. ","5496d88a":"1. I learned how to deal with an imbalanced dataset and how important this concept is when some labels are rare. I got familiar with resampling strategies \n\n2. I learned how important it is to avoid data leakage from training set into validation sets. \n\n3. I learned about gini score metric as well the concept of ROC AUC score and when it matters in comparsion with other scores such as recall, f1-score.\n\n4. I learned how to investigate features after training to see if we can find any meaningful trend.\n\n5. I learned how to use different cross-validation techniques. \n\nI learned these from the references below. \n<font size=\"4\"> \n    <b>Please leave a comment and let me know what you learned from this kernel and upvote if it was useful :) <\/b><\/font>","4e9caeac":"We can see that the training data size is $595212\\times 59$ and test data size is $892816\\times58$. It means our testing data size is bigger. As we can see from the above tables, we have 59 columns in training and 58 in test, where the difference is the target variable. The feature names are encoded so we cannot know what they mean, however, we know the features that belong to similar groupings are tagged (e.g., ind, reg, car, calc). Also, we know that cat shows categorical variable and bin means binary. The target values are binary labels which 1 means that claim was filed.  ","9f8b454e":"<a id=\"3\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:red; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center> Training<\/center><\/h2>","add288bf":"<a id=\"7\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:red; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>References<\/center><\/h2>\n<a href=\"https:\/\/lightgbm.readthedocs.io\/en\/latest\/\"> [1] LightGBM\u2019s documentation<\/a> <br>\n<a href=\"https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/api.html\">[2] Imbalanced-learn documentation<\/a> <br>\n<a href=\"https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets\">[3] Credit Fraud || Dealing with Imbalanced Datasets<\/a> <br>\n<a href=\"https:\/\/medium.com\/lumiata\/cross-validation-for-imbalanced-datasets-9d203ba47e8#:~:text=I%20split%20the%20data%20first,into%20train%20and%20validation%20sets\">[4] Cross-Validation for Imbalanced Datasets<\/a>\n","3c56c03d":"We can see that although there is 0 Nan values in the dataset, there are alot of -1 values which are our missing values. Some of the features have many missing values. For example, 69% of ps_car_03_cat features are missing. Please note that the numbers are in percentage. Also, we can see that we have the same amount of missing values in both training and test set with the same distribution. To better see the frequency of missing values, we are using heatmap to visualize all the features. ","5f32f0a6":"<a id=\"24\"><\/a>\n\n## Exploring features \n\nWe will first devide the features into different types of categorical, binary, ordinal and continous numerical and then visualize all of them. We will see different patterns which are not really meaningful because we do not know the real names of the labels. We will get back to the EDA part after training machine learning, to see if we can understand differnt patterns. ","9f5d367c":"My approach to this project is explained through different steps. As a summary, we first train three different simple models to choose the best performance in terms of gini score and speed. Then, we will see how to deal with the imbalanced dataset. Afterwards, we will go through choosing cross-validation sets, performing hyperparameter tuning, feature engineering and choosing our final model. ","c8e8f12e":"From the above figures we can see the following:\n\n1) 'ps_car_11' has too many categories (104).<br>\n\n2) 'ps_ind_04_cat', 'ps_car_02_cat, 'ps_car_03_cat','ps_car_05_cat', and 'ps_car_07_cat', are binary, but with missing values of -1, they turned into three categories.<br>\n\n3) 'ps_ind_05_cat and 'ps_car_04_cat' have the most frequnency in their 0 category. <br>\n\n4) 'ps_car_08_cat has exactly two categories, but it is not named binary. The reason can be that this variable can take more than two categories but there are no examples in this dataset. <br>\n\n5) 'ps_car_10_cat' is almost always 1. <br>\n\n6) 'ps_ind_10_bin' and 'ps_ind_11_bin', 'ps_ind_12_bin'  and 'ps_ind_13_bin' are almost always 0. <br>","6b5dd8b6":"<a id=\"21\"><\/a>\n\n## Train and test data","f4fda8dc":"<a id=\"22\"><\/a>\n\n## Target labels","ec6c43fc":"Here is a visualization of the target labels. We can see that the labels are binary which means whether someone claimed (1) or not (0). The interesting point is that the target variables are not distributed equally, i.e., the dataset is not balanced. ","d2c06cf6":"Now we have 11 true 1s in the least 1000 probabilities, and 132 in the first 1000 high probabilities which means our algorithm improved.\n\nAlso the gini score is increased to 0.284. ","4472287f":"Here is the summary based on what we have done:\n\nIn this project, we first tested three different classifiers and realized that Light gradient booster (lighgbm) is the best in terms of accuracy and speed. Then, we explained how to deal with the imbalance of the data. Our solution did not include any kind of resampling. Then we expalined our cross-validation method and why we use Stratified KFold. Then, we performed hyperparameter tuning to take the most out of our algorithm. At last, we evaluated our algorithm to see what features are more important and how we can improve the relative predicted probability of true 1s with some changes in the features. The main limitations of this data science problem are the inherent randomness of the problem and not knowing exactly what the features are.","ad73fea6":"First, we are going to see the train and test data size and some of their examples. ","a9a36203":"<a id=\"33\"><\/a>\n\n## Train and validate (Stratified KFold)"}}