{"cell_type":{"469cba27":"code","d3d0f9d7":"code","2dcf62c0":"code","6321b36d":"code","248ba88d":"code","d6296e86":"code","de343de3":"code","c857df78":"code","728d9a36":"code","9d155d78":"code","7a7d74ab":"code","c8af9516":"code","78a75fa2":"code","46c65230":"code","5c2789cd":"code","66939b70":"code","c39a3b02":"code","67653f15":"code","a9b409fb":"code","a9111456":"code","a17edcc8":"code","b2f381ec":"code","7ca2ea17":"code","c9935431":"code","4d7c2d9a":"code","4307f8a0":"code","87437c34":"code","132c23cb":"code","1005a3d1":"code","5cfdaebd":"code","b94b59eb":"code","5ca31de8":"code","485063d1":"code","951a4514":"code","e6f8c36e":"code","ec1fe698":"code","e327d101":"code","9f836307":"code","a680b4d1":"code","b83304ae":"code","540fd9e7":"code","8e5bac61":"markdown","96825d62":"markdown","3deb89a3":"markdown","23bc2455":"markdown","bc670379":"markdown","c25600f3":"markdown","228ef674":"markdown","722a7039":"markdown","fe6517eb":"markdown","bb9a403e":"markdown","f2fbb844":"markdown","2851f94a":"markdown","8895210a":"markdown","c90d3ed0":"markdown","6d9b1a75":"markdown","3c5e599f":"markdown","d34931fa":"markdown","c5c2c1dc":"markdown","48eb3d83":"markdown","ef9c1666":"markdown"},"source":{"469cba27":"import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","d3d0f9d7":"#importing the data\ndata_train, data_test,data_ = pd.read_csv('..\/input\/titanic\/train.csv'),pd.read_csv('..\/input\/titanic\/test.csv'),pd.read_csv('..\/input\/titanic\/gender_submission.csv')","2dcf62c0":"data_train.head()","6321b36d":"data_test.head()","248ba88d":"data_.head()","d6296e86":"data_train.info()","de343de3":"data_test.info()","c857df78":"data_.info()","728d9a36":"data_test.insert(11, 'Survived', data_['Survived'],True)\ndata_test.head()","9d155d78":"def count_missing_value(data):\n    length = len(data)\n    null_count = data.isnull().sum()\n    nan_count = ((data=='nan') | (data=='NaN') | (data=='Nan')).sum()\n    empty_count = ((data=='') | (data==' ')).sum()\n    null_percent = (null_count\/ length)* 100\n    nan_percent = (nan_count\/ length)*100\n    empty_percent = (empty_count\/ length)*100\n    show = pd.DataFrame({'null_count' : null_count,\n                         'null_percent' : null_percent,\n                         'nan_count' : null_count,\n                         'nan_percent' : nan_percent,\n                         'empty_count' : empty_count,\n                         'empty_percent' : empty_percent\n                        })\n    return show","7a7d74ab":"print('Missing value of TRAINING DATA : \\n')\ncount_missing_value(data_train)","c8af9516":"print('Missing value of TEST DATA : \\n')\ncount_missing_value(data_test)","78a75fa2":"data_train= data_train.drop(labels = ['Cabin'], axis= 1)\ndata_test= data_test.drop(labels = ['Cabin'], axis= 1)","46c65230":"age_train_median= data_train['Age'].median()\nage_test_median= data_test['Age'].median()\n\ndata_train['Age'].fillna(age_train_median, inplace= True)\ndata_test['Age'].fillna(age_test_median, inplace= True)","5c2789cd":"location_test= data_test.loc[data_test['Fare'].isnull()]\nlocation_train= data_train.loc[data_train['Embarked'].isnull()]\n\nprint(location_test)\nprint(location_train)","66939b70":"data_train= data_train.drop(labels=[61, 829], axis=0)\ndata_test= data_test.drop(labels=[152], axis=0)","c39a3b02":"count_missing_value(data_train)","67653f15":"count_missing_value(data_test)","a9b409fb":"from sklearn.preprocessing import LabelEncoder\nle= LabelEncoder()\nobj_col = ['Sex', 'Embarked', 'Ticket','Name']\nfor i in obj_col:\n    data_test[i] = le.fit_transform(data_test[i])\n    data_train[i] = le.fit_transform(data_train[i])","a9111456":"data_test.head()","a17edcc8":"data_train.head()","b2f381ec":"fig , (ax1, ax2)= plt.subplots(2,1, figsize=(9,10))\nsns.heatmap(data_train.corr().abs(), annot= True, ax= ax1)\nsns.heatmap(data_test.corr().abs(), annot= True, ax= ax2)","7ca2ea17":"data_train= data_train.drop(labels = ['Name','PassengerId','SibSp'], axis= 1)\ndata_test_= data_test.drop(labels = ['Name','PassengerId','SibSp'], axis= 1)","c9935431":"women = pd.concat([data_test[data_test_['Sex']==0] , data_train[data_train['Sex']==0]])\nmen = pd.concat([data_test[data_test_['Sex']==1] , data_train[data_train['Sex']==1]])","4d7c2d9a":"fig, ax = plt.subplots(2,1 ,figsize=(8,9))\n\nax[0].bar('Women',len(women[women['Survived']==1]), label='Women Survived')\nax[0].bar('Men',len(men[men['Survived']==1]), label='Men Survived')\nax[0].set(title='SURVIVAL RATE STATISTICS', ylabel='SURVIVAL COUNT')\nax[0].legend()\n\nax[1].bar('Women',len(women[women['Survived']==0]), label='Women Died')\nax[1].bar('Men',len(men[men['Survived']==0]), label='Men Died')\nax[1].set(title='DEATH RATE STATISTICS', ylabel='DEATH COUNT')\nax[1].legend()\n\nplt.tight_layout()\nplt.show()","4307f8a0":"fig, ax = plt.subplots(2,1, figsize=(8,9))\n\nsns.distplot(women[women['Survived']==1].Age, kde=False, bins=20, ax=ax[0], label='Women survival AGE')\nsns.distplot(men[men['Survived']==1].Age, kde=False, bins=20, ax=ax[0], label='Men survival AGE')\nax[0].set(title='DEATH RATE STATISTICS', ylabel='DEATH COUNT', ylim=(0,120))\nax[0].legend()\n\nsns.distplot(women[women['Survived']==0].Age, kde=False, bins=20, ax=ax[1], label='Women death AGE')\nsns.distplot(men[men['Survived']==0].Age, kde=False, bins=20, ax=ax[1], label='Men death AGE')\nax[1].set(title='DEATH RATE STATISTICS', ylabel='DEATH COUNT', ylim=(0,270))\nax[1].legend()\n\nplt.show()","87437c34":"x_train= data_train.drop(labels= ['Survived'], axis= 1)\ny_train= data_train['Survived']\nx_test= data_test_.drop(labels= ['Survived'], axis= 1)\ny_test= data_test_['Survived']","132c23cb":"from sklearn.model_selection import GridSearchCV,KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nk_fold= KFold(n_splits= 3, random_state= 42)","1005a3d1":"algos1= {\n        'Logistic Regression ' : {'model': LogisticRegression(),\n                                 'para' : {'penalty': ['l1', 'l2'], 'C':[1,2,7,10], 'solver' :['liblinear', 'sag', 'saga']}\n                                 },\n        'KNN'                  : {'model' : KNeighborsClassifier(),\n                                'para': {'n_neighbors': np.arange(1,38,3), 'weights': ['distance', 'uniform']}\n                                 }\n        \n        }","5cfdaebd":"algos2= {\n        'Random Forest Classifier' : {'model' : RandomForestClassifier(),\n                                      'para' :{'criterion': ['gini', 'entropy'],'n_estimators':[10,25,50,100,200,250,400,500],\n                                               'max_depth' : [1,2,4,7],'max_features': ['auto', 'sqrt', 'log2'] }\n                                     },\n        'Random Forest Regressor' : {'model' : RandomForestRegressor(),\n                                     'para' :{'criterion' : ['mse', 'mae'],'max_depth' : [1,2,5,7],\n                                              'n_estimators':[10,25,50,100,200,250,400,500],\n                                              'max_features': ['auto', 'sqrt', 'log2']}\n                                    }\n        }","b94b59eb":"algos3= {\n        'Decision Tree Classifier' : {'model' : DecisionTreeClassifier(),\n                                      'para' : {'splitter': ['best', 'random'],'criterion': ['gini', 'entropy'],\n                                               'min_samples_split':[2,5,7]}\n                                     },\n        \n        'Decision Tree Regressor' : {'model' : DecisionTreeRegressor(),\n                                    'para' : {'criterion': ['mse', 'mae', 'friedman_mse'],\n                                             'splitter': ['best', 'random'],'min_samples_split':[2,5,10,7]}\n                                    }\n        }","5ca31de8":"score1 = []\nfor algo, param in algos1.items():\n    grid1= GridSearchCV(param['model'], param['para'], cv= k_fold)\n    grid1.fit(x_train, y_train) \n    y_pred= grid1.predict(x_test)\n    \n    score1.append(pd.Series({\n                'Estimator' : algo,\n                'best parameter' : grid1.best_params_,\n                'best score' : grid1.best_score_\n                }))","485063d1":"score2 = []\nfor algo, param in algos2.items():\n    grid2= GridSearchCV(param['model'], param['para'], cv= k_fold)\n    grid2.fit(x_train, y_train) #to  measure the time taken to fit the data.\n    y_pred= grid2.predict(x_test)\n    \n    score2.append(pd.Series({\n                'Estimator' : algo,\n                'best parameter' : grid2.best_params_,\n                'best score' : grid2.best_score_\n                }))","951a4514":"score3 = []\nfor algo, param in algos3.items():\n    grid3= GridSearchCV(param['model'], param['para'], cv= k_fold)\n    grid3.fit(x_train, y_train) #to  measure the time taken to fit the data.\n    y_pred= grid3.predict(x_test)\n    \n    score3.append(pd.Series({\n                'Estimator' : algo,\n                'best parameter' : grid3.best_params_,\n                'best score' : grid3.best_score_\n                }))","e6f8c36e":"# to show the complete row\npd.options.display.max_colwidth=100","ec1fe698":"# making the dataframe\n\nbest_result1 = pd.concat(score1, axis=1).T.set_index('Estimator')    \nbest_result2 = pd.concat(score2, axis=1).T.set_index('Estimator')\nbest_result3 = pd.concat(score3, axis=1).T.set_index('Estimator')","e327d101":"best_result1","9f836307":"best_result2","a680b4d1":"best_result3","b83304ae":"rf= RandomForestClassifier(criterion= 'entropy', max_depth= 7, max_features= 'sqrt', n_estimators= 10)\nrf.fit(x_train,y_train)\ny_pred_= rf.predict(x_test)\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_pred_,y_test)","540fd9e7":"submission = pd.DataFrame({\n        \"PassengerId\": data_test[\"PassengerId\"],\n        \"Survived\": y_pred_\n    })\nsubmission.to_csv('titanic.csv', index=False)","8e5bac61":"## Observing the correlation among variables","96825d62":"### MERGING 'survived' column to TEST DATA","3deb89a3":"From the above bar graphs, it is clear that the suvival rate of women is much higher than that of men.","23bc2455":"## Finding missing value","bc670379":"# DATA CLEANING","c25600f3":"## Removing less correlated columns","228ef674":"### Filling the empty rows","722a7039":"### Re-checking for missing value","fe6517eb":"As we can see, 'Logistic Regression' , 'Decision Tree Classifier' & 'Random Forest Classifier' gives the top 3 results. But, \n\n'Random Forest Classifier' comes at the top with the best result among all the estimators!","bb9a403e":"As we can see, 'Cabin' column has almost 3\/4th empty data which might affect our result, so, we'll rempve it.\n\nThe 'Age' has only 1\/5th empty data. We can replace the empty data with either mean, median or mode.\n\nAnd since, 'Fare' and 'Embarked' has only few empty value, so, we can remove it too.","f2fbb844":"## DECISION TREE REGRESSOR\/ CLASSIFIER","2851f94a":"## RANDOM FOREST REGRESSOR\/ CLASSIFIER","8895210a":"# EXPLORING THE DATA","c90d3ed0":"# -------------------------------------      ## END ##    ---------------------------------------------","6d9b1a75":"# Encoding the categorical values","3c5e599f":"# DATA VISUALISATION","d34931fa":"# PREDICTING THE RESULT\n## REGRESSOR vs CLASSIFIER","c5c2c1dc":"## LOGISTIC REGRESSOR & KNN","48eb3d83":"### Finding the location of empty row","ef9c1666":"Majority of the men dying are between the age of 16-47 years and, for women it is 17-32 years(though the count is very low as compared to men).\n\nMajority of the women surviving are between the age of 16-47 years and, for men it is 24-32 years."}}