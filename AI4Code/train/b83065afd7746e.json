{"cell_type":{"43aaab41":"code","ebdbeee8":"code","09c3d5bf":"code","b4684d46":"code","5f662228":"code","c0cccbff":"code","fa310671":"code","b076d5fd":"code","3a0efd2d":"code","1357bdea":"code","ef8cd8f3":"code","d411f277":"code","6db2fbff":"code","b6b919c8":"code","559657ad":"code","638c4f2e":"code","d1af9d45":"code","4c8b2a89":"code","756cfcc2":"code","a4c59dae":"code","2eaf0a16":"code","28c5a9d7":"code","2f1d2e8a":"code","305bf5c1":"code","7f1915a9":"code","50de6263":"code","cca81584":"code","4e1e353c":"code","a678504a":"code","758690f3":"code","3cb98948":"code","ea9b20b8":"code","3eef2f64":"markdown","af8aacf0":"markdown","df7fae1b":"markdown","862bca6b":"markdown","0cdb5d1f":"markdown","3ae9b439":"markdown","199810f1":"markdown","2873d73a":"markdown","f128f4e5":"markdown"},"source":{"43aaab41":"import os\nimport random\nimport re\nimport string\nfrom collections import Counter\n\nimport unidecode\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem import RSLPStemmer\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom torchtext.vocab import Vocab\nfrom torchtext.data.utils import get_tokenizer\n\nfrom pprint import pprint\nfrom tqdm.notebook import tqdm","ebdbeee8":"def seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything()\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","09c3d5bf":"# Load train data\nusecols = ['id', 'review_title', 'review_text']\nraw_train_data = pd.read_csv('..\/input\/i2a2-nlp-2021-sentiment-analysis\/train.csv', index_col='id', usecols=usecols + ['rating'])","b4684d46":"# Concatenate text features\nraw_train_data['review'] = raw_train_data['review_title'] + ' ' + raw_train_data['review_text']\n\nwith pd.option_context('display.max_colwidth', None):\n    display(raw_train_data[['review', 'rating']].head())","5f662228":"# Compile regular expressions\nremove_url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\nremove_email = re.compile(r'\\S+@\\S+')\nremove_duplicate_word = re.compile(r'\\b(\\w+?)\\1+')\nremove_duplicate_char = re.compile(r'([^rs])(?=\\1+)|(r)(?=r{2,})|(s)(?=s{2,})')\nremove_number = re.compile(r'\\d+')\nremove_extra_space = re.compile(r'\\s+')\n\n# Load punctuation\npunctuation = [*string.punctuation]\npunctuation.extend(['\u00ba', '\u00aa'])\n\ndef preprocess_text(text: str) -> str:\n\n    # Convert to lowercase\n    text = text.lower() \n    \n    # Apply regular expressions\n    text = remove_url.sub('', text)    # remove urls\n    text = remove_email.sub('', text)  # remove emails\n    text = remove_duplicate_word.sub(r'\\1', text) # remove duplicate words\n    text = remove_duplicate_char.sub('', text)    # remove duplicate chars; except \"rr\" and \"ss\" digraphs\n    text = remove_number.sub(' ', text)  # remove numbers\n\n    ## Expand abbreviatons\n    text = re.sub(r'\\b(n|\u00f1)([a\u00e3\u00e2]o)?\\b', ' n\u00e3o ', text)\n    text = re.sub(r'\\bt[a\u00e1]\\b', ' est\u00e1 ', text)\n    text = re.sub(r'\\b(p(r[oa\u00e1])?)\\b', ' para ', text)\n    text = re.sub(r'\\bq\\b', ' que ', text)\n    text = re.sub(r'\\bpq[s]?\\b', ' porque ', text)\n    text = re.sub(r'\\btb[m|n]?\\b', ' tamb\u00e9m ', text)\n    text = re.sub(r'\\vc[s]?\\b', ' voc\u00ea ', text)\n    text = re.sub(r'\\bmt[ao]?s?\\b', ' muito ', text)\n    text = re.sub(r'\\b(p(r[oa\u00e1]s?)?)\\b', ' para ', text)\n    text = re.sub(r'\\bhj\\b', ' hoje ', text)\n    text = re.sub(r'\\bobs\\b', ' observa\u00e7\u00e3o ', text)\n    text = re.sub(r'\\beh\\b', ' \u00e9 ', text)\n\n    # Proprocess long words\n    text = re.sub(r'(\u00f3timo)[v]?\\1+', r' \\1 ', text)\n    text = re.sub(r'\\b(ok)\\1+', r' \\1 ', text)\n    text = re.sub(r'\\b(natural)', r' \\1 ', text)\n    text = re.sub(r'(((bla)(ba)?)+\\2)', r' \\2 ', text)\n    text = re.sub(r'(altura|largura)x', r' \\1 ', text)\n    text = re.sub(r'(compra|embalagem)x', r' \\1 ', text) \n    text = re.sub(r'(ruim|regular|bom|[o\u00f3]timo|excelente)', r' \\1 ', text)\n\n\n    # Convert accented characters to their ASCII values\n    text = unidecode.unidecode(text)\n    \n    # Remove trailing spaces\n    text = remove_extra_space.sub(' ', text)\n\n    # Instatiate a TweetTokenizer object\n    tokenizer = TweetTokenizer(preserve_case=False, # lowercasing\n                               reduce_len=True, \n                               strip_handles=True)\n                               \n    # Tokenize the text    \n    tokens = tokenizer.tokenize(text)\n\n    # remove punctuation\n    tokens = [token for token in tokens if token not in punctuation]\n    \n    # remove non alphabetic and words longest than twenty chars\n    tokens = [token for token in tokens if token.isalpha() and len(token) <= 20]\n    \n    # Portuguese stemmer\n    st = RSLPStemmer()\n    tokens = [st.stem(token) for token in tokens]\n\n    return ' '.join(tokens)","c0cccbff":"%%time\nraw_train_data['review_clean'] = raw_train_data['review'].apply(preprocess_text)","fa310671":"with pd.option_context('display.max_colwidth', None):\n    display(raw_train_data.head())","b076d5fd":"# Split data into 80% for training and 20% for validation\ntrain_data, valid_data = train_test_split(raw_train_data[['review_clean', 'rating']],\n                                          test_size=0.2, \n                                          stratify=raw_train_data['rating'], \n                                          random_state=0)\n\n# Reset indices\ntrain_data.reset_index(drop=True, inplace=True)\nvalid_data.reset_index(drop=True, inplace=True)\n\n# Sanity chech\ntrain_data.shape, valid_data.shape","3a0efd2d":"class AmericanasDataset(Dataset):\n\n    def __init__(self, data):\n        self.text = data.iloc[:, 0]\n        self.label = data.iloc[:, 1]\n\n    def __len__(self):\n        return self.text.shape[0]\n\n    def __getitem__(self, index):\n        return self.text.iloc[index], self.label.iloc[index]","1357bdea":"# Create training and validation dataset\ntrain_data = AmericanasDataset(train_data)\nvalid_data = AmericanasDataset(valid_data)","ef8cd8f3":"# Sample\npprint(train_data.text[0], compact=True) \nprint(train_data.label[0])","d411f277":"# Create the vocabulary\ncounter = Counter()\ntokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n\nfor text, _ in tqdm(train_data, leave=None):\n    counter.update(tokenizer.tokenize(text))\n\nvocab = Vocab(counter, min_freq=1)","6db2fbff":"print(f'The vocabulary has {len(vocab):,} unique words\\n')\n\n# Show the 50 most common words\npprint(vocab.freqs.most_common(50), compact=True)","b6b919c8":"text_encoder = lambda text: [vocab[token] for token in tokenizer.tokenize(text)]\nlabel_encoder = lambda label: int(label) - 1","559657ad":"sentence = train_data.text[0]\nprint('Original sentence:', '--' * 10, sentence, sep='\\n')\nprint('\\nEncoded sentence:', '--' * 10, text_encoder(sentence), sep='\\n')","638c4f2e":"def collate_fn(batch):\n\n    list_text, list_label = [], []\n\n    for text, label in batch:\n\n        processed_text = torch.tensor(text_encoder(text), dtype=torch.long)\n        list_text.append(processed_text)\n\n        list_label.append(label_encoder(label))\n\n        text_tensor = pad_sequence(list_text, batch_first=True, \n                                   padding_value=vocab.stoi['<pad>'])\n        \n        label_tensor = torch.tensor(list_label, dtype=torch.long)\n\n    return text_tensor, label_tensor","d1af9d45":"train_loader = DataLoader(train_data, batch_size=64, shuffle=True, collate_fn=collate_fn, pin_memory=True)\nvalid_loader = DataLoader(valid_data, batch_size=64, shuffle=False, collate_fn=collate_fn, pin_memory=True)","4c8b2a89":"# Sample \nbatch = next(iter(train_loader))\nbatch[0].size(), batch[1].size()","756cfcc2":"class AmericanasClassifier(nn.Module):\n\n    def __init__(self, vocab_size, embedding_dim, hidden_size, n_layers, \n                 bidirectional, output_dim, dropout):        \n        super().__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim,\n                            hidden_size,\n                            n_layers,\n                            batch_first=True, \n                            bidirectional=bidirectional,\n                            dropout= 0 if n_layers < 2 else dropout)\n\n        self.dropout = nn.Dropout(dropout)\n\n        self.fc = nn.Linear(hidden_size * 2 if bidirectional else hidden,\n                            output_dim)\n\n    def forward(self, text):\n\n        embedded = self.dropout(self.embedding(text))\n\n        _, (hidden, cell) = self.lstm(embedded)\n\n        if self.lstm.bidirectional:\n            hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n        else:\n            hidden = hidden[-1, :, :]\n\n        return self.fc(self.dropout(hidden))","a4c59dae":"model_params = {\n    'vocab_size': len(vocab),\n    'embedding_dim': 100,\n    'hidden_size': 64,\n    'n_layers': 2, \n    'bidirectional': True,\n    'output_dim': raw_train_data['rating'].nunique(),\n    'dropout': 0.2,\n}\n\nmodel = AmericanasClassifier(**model_params)\nprint(model)","2eaf0a16":"def train(model, loader, optimizer, criterion, clip=1.0) -> tuple:\n    \n    epoch_loss, epoch_acc = 0, 0\n    \n    model.train()\n    \n    for text, label in loader:\n\n        text = text.to(device)\n        label = label.to(device)\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward step\n        output = model(text)\n        loss = criterion(output, label)\n        acc = accuracy(output, label)\n        \n        # Backward step\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        \n        optimizer.step()\n        \n        # Compute metrics\n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n    return epoch_loss \/ len(loader), epoch_acc \/ len(loader)","28c5a9d7":"@torch.no_grad()\ndef evaluate(model, loader, criterion) -> tuple:\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.eval()\n    \n    for text, label in loader:\n\n        text = text.to(device)\n        label = label.to(device)\n        \n        output = model(text)\n        loss = criterion(output, label)\n        acc = accuracy(output, label)\n\n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n    return epoch_loss \/ len(loader), epoch_acc \/ len(loader)","2f1d2e8a":"def accuracy(output: torch.tensor, label: torch.tensor) -> float:\n\n    _, pred = torch.max(output, dim=1)\n    correct = pred == label\n    acc = sum(correct) \/ len(correct)\n    \n    return acc","305bf5c1":"import time\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time \/ 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","7f1915a9":"model = model.to(device)\n\ncriterion = nn.CrossEntropyLoss().to(device)\n\nlearning_rate = 1e-2\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                 mode='min',\n                                                 patience=3,\n                                                 verbose=True)","50de6263":"EPOCHS = 30\n\nbest_valid_loss = np.inf\n\nhistory = {\n    'train_loss': [],\n    'train_accuracy': [],\n    'valid_loss': [],\n    'valid_accuracy': [],\n}\n\nfor epoch in range(EPOCHS):\n\n    start_time = time.time()\n    \n    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n    valid_loss, valid_acc = evaluate(model, valid_loader, criterion)\n    \n    end_time = time.time()\n\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.2%}')\n    print(f'\\tValid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.2%}\\n')\n    \n    # Save the best model\n    if valid_loss < best_valid_loss:\n        print(f'The validation loss decreaded: {best_valid_loss:.4f} --> {valid_loss:.4f}. Save the model...\\n')\n        best_valid_loss = valid_loss\n        # torch.save(model.state_dict(), 'americanas_best_model.pth')\n \n    # Updata the learning rate\n    scheduler.step(valid_loss)\n\n    history['train_loss'].append(train_loss)\n    history['train_accuracy'].append(train_acc)\n    history['valid_loss'].append(valid_loss)\n    history['valid_accuracy'].append(valid_acc)","cca81584":"hist = pd.DataFrame(history)\nhist.tail()","4e1e353c":"fig, ax = plt.subplots(1, 2, figsize=(12, 4))\nax = ax.flat\n\nfor ii, metric in enumerate(['loss', 'accuracy']):\n    sns.lineplot(x=hist.index, y=f'train_{metric}', data=hist, label=f'Training {metric}', ax=ax[ii])\n    sns.lineplot(x=hist.index, y=f'valid_{metric}', data=hist, label=f'Validation {metric}', ax=ax[ii])\n    ax[ii].legend(frameon=False)\n\nsns.despine()\nplt.tight_layout()","a678504a":"@torch.no_grad()\ndef predictions(model, loader):\n\n    model.eval()\n\n    all_preds = torch.tensor([]).to(device)\n\n    for text, label in loader:\n\n        text = text.to(device)\n        label = label.to(device)\n\n        output = model(text)\n        _, preds = torch.max(output.data, dim=1)\n\n        all_preds = torch.cat([all_preds, preds], dim=0)\n\n    # assert all_preds.size()[0] == len(test_data), 'Deve ser utilizada a base de teste para as previs\u00f5es.'\n\n    return all_preds.cpu()","758690f3":"preds = predictions(model, valid_loader).to(torch.long)\ny_test = np.array([label_encoder(label) for label in valid_data.label])","3cb98948":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n\ncm = confusion_matrix(y_test, preds) \ncm_norm = confusion_matrix(y_test, preds, normalize='true') \n\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\n\nConfusionMatrixDisplay(cm, display_labels=[1, 2, 3, 4, 5]).plot(cmap='Blues',\n                                                                ax=ax[0], \n                                                                values_format='d')\nax[0].set_title('Confusion Matrix', fontsize=14)\n\nConfusionMatrixDisplay(cm_norm, display_labels=[1, 2, 3, 4, 5]).plot(cmap='Blues',\n                                                                     values_format='.2%',\n                                                                     ax=ax[1])\n\nax[1].set_title('Normalized Confusion Matrix', fontsize=14)\n\nplt.plot(cmap='Blues', colorbar=True)\nplt.tight_layout();","ea9b20b8":"print(classification_report(y_test, preds, target_names=['1', '2', '3', '4', '5']))","3eef2f64":"## Preparing data loaders","af8aacf0":"## Preparing text data to feed the model","df7fae1b":"## Loading training and testing data","862bca6b":"## Training and evaluating the model","0cdb5d1f":"## Plotting results","3ae9b439":"## Classification report","199810f1":"## Loading useful libraries and modules","2873d73a":"## Confusion Matrix","f128f4e5":"## Preprocessing raw text data"}}