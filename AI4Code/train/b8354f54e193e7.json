{"cell_type":{"a1b5b28e":"code","d9899c65":"code","acad6c57":"code","f563cc0b":"code","1c3be74e":"code","2f269988":"code","9a0a8d3d":"code","99c12677":"code","1506b00d":"code","fff7cefd":"code","bca5cb92":"code","863a5519":"code","cbd312a3":"code","43ac245b":"code","b4ec0c3c":"code","036cee73":"code","b162f153":"code","3a433e65":"code","de4bbf18":"code","3a56e2b3":"code","116f25b5":"code","8681d91c":"code","14ba1e9b":"code","7e2ff734":"code","69f12d1e":"code","a7683e4c":"code","14003ac7":"code","2cdeae1c":"code","96ef93b8":"code","03c6b635":"code","9e38d04d":"code","25ceb867":"code","75b10b98":"code","444e99d7":"code","856947fc":"code","dc321201":"code","d9ea4fa7":"code","407173ad":"code","622f799c":"code","17a35f7a":"code","75fff2c3":"code","0ecbed96":"code","ee54ddee":"code","9ecc6724":"code","9ed9ad0a":"code","bf72f766":"code","63a69241":"code","5eac774b":"code","9f02ef6b":"code","e091b77e":"code","44e5a079":"code","edacc293":"code","69d8bacc":"markdown","bdfd5abe":"markdown","8c17a428":"markdown","1a55a0c3":"markdown","3fa57662":"markdown","b4998914":"markdown","85370cb4":"markdown","0e8a4548":"markdown","915fe6cb":"markdown","c079f358":"markdown","02e8d305":"markdown","f6305d2b":"markdown","34db6e9a":"markdown","b942821d":"markdown","4cbb5600":"markdown","d2b7a8ac":"markdown","f2d35495":"markdown","2f9deaf7":"markdown","fc45929b":"markdown","593c822a":"markdown","815cee54":"markdown","2f2a02b6":"markdown","c57350ba":"markdown","2347083c":"markdown","51e91aa3":"markdown","af8877f0":"markdown","7929c122":"markdown","82aeab86":"markdown","d5d5f1f7":"markdown","42bcaf04":"markdown","f2240577":"markdown","42237b7c":"markdown","0dd7d04c":"markdown","3e507437":"markdown","fa8af0ea":"markdown","48522fdc":"markdown","15cde416":"markdown","7bae11aa":"markdown","338b76cb":"markdown","fa07153a":"markdown","2423cf3a":"markdown","8b00849d":"markdown","78f7231c":"markdown","05c48f12":"markdown","950cb283":"markdown","46218693":"markdown","8076100d":"markdown","956a99bd":"markdown","d380f984":"markdown","d5adb6ce":"markdown","191f8b25":"markdown","826b1377":"markdown","df27e0e9":"markdown"},"source":{"a1b5b28e":"SEED                    = 12345   # global random seed for better reproducibility\nGLM_SELECTION_THRESHOLD = 0.001   # threshold above which a GLM coefficient is considered \"selected\"","d9899c65":"from rmltk import explain, evaluate, model                        # simple module for training, explaining, and eval\nimport h2o                                                        # import h2o python bindings to h2o java server\nfrom h2o.estimators.glm import H2OGeneralizedLinearEstimator\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\nfrom h2o.grid.grid_search import H2OGridSearch\nimport numpy as np                                                # array, vector, matrix calculations\nimport operator                                                   # for sorting dictionaries\nimport pandas as pd                                               # DataFrame handling\nimport time                                                       # for timers\nfrom sklearn.preprocessing import OneHotEncoder                   # for one-hot encoding\nimport xgboost as xgb                                             #xgboost for modeling\nfrom math import exp, expm1                                       #make exponential\nimport matplotlib.pyplot as plt      # plotting\npd.options.display.max_columns = 999 # enable display of all columns in notebook\n\n# enables display of plots in notebook\n%matplotlib inline\n\nnp.random.seed(SEED)                     # set random seed for better reproducibility\n\nh2o.init(max_mem_size='24G', nthreads=4) # start h2o with plenty of memory and threads\nh2o.remove_all()                         # clears h2o memory\nh2o.no_progress()                        # turn off h2o progress indicators    ","acad6c57":"df_train = pd.read_csv('https:\/\/gwu-workshop-kai.s3.amazonaws.com\/train.csv')\ntest = pd.read_csv('https:\/\/gwu-workshop-kai.s3.amazonaws.com\/test.csv')","f563cc0b":"# Categorical boolean mask\nmask = df_train.dtypes==object\n\n# filter categorical columns using mask and turn it into a list\ncats = df_train.columns[mask].tolist()\n\n#filter numeric\/float columns\nnums = df_train.columns[~mask].tolist()\n\n#print determine data types\nprint(\"Categorical =\" , cats)\nprint()\nprint(\"Numeric = \", nums )","1c3be74e":"split_ratio = 0.9 # 90%\/10% train\/test split\n\n# execute split\nsplit = np.random.rand(len(df_train)) < split_ratio\ntrain = df_train[split]\nvalid = df_train[~split]\n\n# summarize split\nprint('Train data rows = %d, columns = %d' % (train.shape[0], train.shape[1]))\nprint('Validation data rows = %d, columns = %d' % (valid.shape[0], valid.shape[1]))\n\n#summarize test set\nprint('Test data rows = %d, columns = %d' % (test.shape[0], test.shape[1]))","2f269988":"#describe categorical data\ndf_train[cats].describe()","9a0a8d3d":"#drop column\ndroplist = [\"Utilities\"]\ntrain = train.drop(columns = droplist)\nvalid = valid.drop(columns = droplist)\ntest = test.drop(columns = droplist)","99c12677":"missingsum = df_train.isnull().sum()\nmisslist = missingsum[missingsum>0].sort_values()\nmisslist.to_frame(name=\"Missing Value Counts\")","1506b00d":"#set 15% of total rows as threshold\ndrop_threshold = int(len(df_train) * 0.15)\n\n#set mask\nmask = misslist < drop_threshold\n\n#filter the variables\nmiss = list(misslist.index[mask])\ndrop = [name for name in misslist.index if name not in miss]\n\n#remove columns stated above\ntrain = train.drop(columns = drop)\nvalid = valid.drop(columns = drop)\ntest = test.drop(columns = drop)\n\n#print the missing values, and the dropped values\nprint(\"Missing values are:\",miss)\nprint()\nprint(\"Drop the following values:\",drop)\n","fff7cefd":"#filter the categorical missing variables\nmiss_cats = [name for name in miss if name in cats]\n\n#filter the numeric missing variables\nmiss_nums = [name for name in nums if name in miss]\n\nprint(\"Categorical missing variables are:\", miss_cats)\nprint()\nprint(\"Categorical numeric variables are:\", miss_nums)","bca5cb92":"#describe the data\nprint(train[miss_cats].describe())\nprint()\nprint(train[miss_nums].describe())","863a5519":"#set the variable list\nvar = [\"Electrical\",\"MasVnrType\",\"MasVnrArea\"]\n\n#use loop to fill the missing value with the most frequent records\nfor xs in var:\n    train[xs] = train[xs].fillna(train[xs].value_counts().index[0])\n    valid[xs] = valid[xs].fillna(valid[xs].value_counts().index[0])\n    test[xs] = test[xs].fillna(test[xs].value_counts().index[0])\n\n#check the result\nfor xs in var:\n    print(\"training\", xs, \"missing\",train[xs].isnull().sum(),\n         \", valid\", xs, \"missing\",valid[xs].isnull().sum(),\n         \", test\", xs, \"missing\",test[xs].isnull().sum(),)","cbd312a3":"# since NA means there is no basement, we change \"NA\" to \"None\"\n\n#set variable name list\nvar = [\"BsmtCond\",\"BsmtQual\",\"BsmtFinType1\",\"BsmtFinType2\",\"BsmtExposure\"]\n\n#use loop to change data\nfor xs in var:\n    train[xs] = train[xs].fillna(value = \"None\")\n    valid[xs] = valid[xs].fillna(value = \"None\")\n    test[xs] = test[xs].fillna(value = \"None\")\n    \n#double check result\nfor xs in var:\n    print(\"training\", xs, \"missing\",train[xs].isnull().sum(),\n         \", valid\", xs, \"missing\",valid[xs].isnull().sum(),\n         \", test\", xs, \"missing\",test[xs].isnull().sum(),)","43ac245b":"#set variable name list\nvar = [\"GarageCond\",\"GarageQual\",\"GarageFinish\",\"GarageType\"]\n\n#fill missing variables in var with None\nfor xs in var:\n    train[xs] = train[xs].fillna(value = \"None\")\n    valid[xs] = valid[xs].fillna(value = \"None\")\n    test[xs] = test[xs].fillna(value = \"None\")\n    \n#filling \"GarageYrBlt\" with \"0\"\ntrain['GarageYrBlt'] = train['GarageYrBlt'].fillna(value = 0)\nvalid['GarageYrBlt'] = valid['GarageYrBlt'].fillna(value = 0)\ntest['GarageYrBlt'] = test['GarageYrBlt'].fillna(value = 0)\n\n#double check result\nvar.append(\"GarageYrBlt\")\nfor xs in var:\n    print(\"training\", xs, \"missing\",train[xs].isnull().sum(),\n         \", valid\", xs, \"missing\",valid[xs].isnull().sum(),\n         \", test\", xs, \"missing\",test[xs].isnull().sum(),)","b4ec0c3c":"var = list(train.columns)\nvar.remove(\"Id\")\nvar.remove(\"SalePrice\")","036cee73":"#check pearson correlation\ny_name = \"SalePrice\"\ncorr = pd.DataFrame(train[var + [y_name]].corr()[y_name]).iloc[:-1]\ncorr.columns = ['Pearson Correlation Coefficient']\ncorr.sort_values(by ='Pearson Correlation Coefficient',ascending = False ).head(10)","b162f153":"#scatter plot of Overall\nplt.scatter(y = train[\"SalePrice\"],x= train[\"OverallQual\"])\n\n#find the index of outlier\nprint(train[[\"Id\",\"OverallQual\"]].loc[train[\"OverallQual\"]<3])\n\n#store the outlier\nout_OQ = list(train.loc[train[\"OverallQual\"]<3].index)","3a433e65":"#scatter plot of GrLivArea\nplt.scatter(y = train[\"SalePrice\"],x= train[\"GrLivArea\"])\n\n#find the index of outlier\nprint(train[[\"Id\",\"GrLivArea\"]].loc[train[\"GrLivArea\"]>4000])\n\n#store the outlier\nout_GA = list(train[\"Id\"].loc[train[\"GrLivArea\"]>4000].index)","de4bbf18":"#scatter plot of GarageCars\nplt.scatter(y = train[\"SalePrice\"],x= train[\"GarageCars\"])\n\n#find the index of outlier\nprint(train[[\"Id\",\"SalePrice\"]].loc[train[\"SalePrice\"]>700000])\n\n#store the outlier\nout_GC = list(train[\"Id\"].loc[train[\"SalePrice\"]>700000].index)","3a56e2b3":"#scatter plot of GarageArea\nplt.scatter(y = train[\"SalePrice\"],x= train[\"GarageArea\"])\n\n#find the index of outlier\nprint(train[[\"Id\",\"SalePrice\"]].loc[train[\"GarageArea\"]>1200])\n\n#store the outlier      \nout_GA2 = list(train[\"Id\"].loc[train[\"GarageArea\"]>1200].index)","116f25b5":"#scatter plot\nplt.scatter(y = train[\"SalePrice\"],x= train[\"TotalBsmtSF\"])\n\n#show the outlier Id\nprint(train[[\"Id\",\"SalePrice\"]].loc[train[\"TotalBsmtSF\"]>3000])\n\n#save the id\nout_TB = list(train[\"Id\"].loc[train[\"TotalBsmtSF\"]>3000].index)","8681d91c":"#scatter plot\nplt.scatter(y = train[\"SalePrice\"],x= train[\"1stFlrSF\"])\n\n#show the outlier Id\nprint(train[[\"Id\",\"SalePrice\"]].loc[train[\"1stFlrSF\"]>4000])\n\n#save the id\nout_1SF = list(train[\"Id\"].loc[train[\"1stFlrSF\"]>4000].index)","14ba1e9b":"plt.scatter(y = train[\"SalePrice\"],x= train[\"YearBuilt\"])\n\nout = train[(train[\"YearBuilt\"]<1900) & (train[\"SalePrice\"]>400000)]\nprint(out[[\"Id\",\"SalePrice\"]])\n\nout_YB = list(out.index)","7e2ff734":"#outliers\nout = []\nnames = [out_OQ, out_GA,out_GA2,out_GC,out_TB,out_1SF,out_YB]\nfor xs in names:\n    for n in xs:\n        out.append(n)\n\n#print the outlier id\nprint(out)\n\n#remove all outliers\ntrain = train.drop(out,axis = 0)\n\n#double check the shape\ntrain.shape","69f12d1e":"plt.boxplot(train[\"SalePrice\"])","a7683e4c":"#check log transformation on the \"SalePrice\" -looks good\ntrain[\"SalePrice\"].apply(np.log).hist()\n#valid[\"SalePrice\"].apply(np.log).hist()\n\n#apply log transformation\ntrain[\"SalePrice\"] = np.log(train[\"SalePrice\"])\nvalid[\"SalePrice\"] = np.log(valid[\"SalePrice\"])\nprint(train[\"SalePrice\"].head())","14003ac7":"#check the boxplot\nplt.boxplot(train[\"SalePrice\"])","2cdeae1c":"#remove the outlier\nout = train[train[\"SalePrice\"]<10.75].index\ntrain.drop(out,axis=0,inplace = True)","96ef93b8":"#deep copy the data frame\ntrain_glm = train.copy()\nvalid_glm = valid.copy()\ntest_glm = test.copy()\n\n# Categorical boolean mask\nmask = train_glm.dtypes==object\n\n# filter categorical columns using mask and turn it into a list\ncats = train_glm.columns[mask].tolist()","03c6b635":"#one-hot encode training frame\ntrain_glm = pd.get_dummies(train_glm)\ntrain_SP = train_glm[\"SalePrice\"]\n\n#one-hot encode test frame\nvalid_glm = pd.get_dummies(valid_glm)\nvalid_SP = valid_glm[\"SalePrice\"]\n\n\n#keep only the same new columns in the encoded new frames\ntrain_diff_cols = list(set(train_glm.columns) - set(valid_glm.columns))\nvalid_diff_cols = list(set(valid_glm.columns) - set(train_glm.columns))\ntrain_glm.drop(train_diff_cols, axis=1, inplace=True)\nvalid_glm.drop(valid_diff_cols, axis=1, inplace=True)\n\n\n#check that columns are actually the same in both frames\nprint(train_glm.shape)\nprint(valid_glm.shape)\nprint(all(train_glm.columns == valid_glm.columns))\n\n#one-hot encode test frame\ntest_glm = pd.get_dummies(test_glm)\n\n#keep only the same new columns in the encoded new frames\ntrain_diff_cols = list(set(train_glm.columns)-set(test_glm.columns))\ntrain_glm.drop(train_diff_cols, axis=1, inplace=True)\nvalid_glm.drop(train_diff_cols, axis=1, inplace=True)\n\n# check that columns are actually the same in encoded train and valid frames\nprint(train_glm.shape)\nprint(valid_glm.shape)\nprint(all(train_glm.columns == valid_glm.columns))\n\n#remove columns in encoded test not in encoded train and vals\ntrain_diff_cols = list(set(test_glm.columns)-set(train_glm.columns))\ntest_glm.drop(train_diff_cols, axis = 1, inplace = True)\n\n#check that columns are actually the same in all encoded frames\nprint(train_glm.shape)\nprint(valid_glm.shape)\nprint(test_glm.shape)\nprint(all(train_glm.columns == valid_glm.columns)\n      and all(valid_glm.columns == test_glm.columns))\n\n#add the SalePrice column to the train and valid\ntrain_glm[\"SalePrice\"] = train_SP\nvalid_glm[\"SalePrice\"] = valid_SP\n\n#check the shape\nprint(train_glm.shape)\nprint(valid_glm.shape)","9e38d04d":"#assign input and response variables\ny_name = \"SalePrice\"\nx_names = [name for name in train_glm if name not in [y_name, \"Id\"]]\n\nprint('y_name =', y_name)\nprint()\nprint('x_names =', x_names)","25ceb867":"#set the hyper parameter\nhyper_param = {'alpha': [0.01, 0.25, 0.5, 0.75, 0.99] }\n\n#create model and train it\ngrid = H2OGridSearch(H2OGeneralizedLinearEstimator(family=\"gaussian\",link = \"log\",lambda_search=True,seed=SEED),\n        hyper_params=hyper_param)\n\ngrid.train(x= x_names, y=y_name,training_frame=h2o.H2OFrame(train_glm), validation_frame=h2o.H2OFrame(valid_glm))","75b10b98":"best_glm = grid.get_grid()[0]\nbest_glm","444e99d7":"#the thereshold is set to be 0.001\nselected_feature = []\nprint('Best penalized GLM coefficients:')\nfor c_name, c_val in sorted(best_glm.coef().items(), key=operator.itemgetter(1)):\n    if abs(c_val) > GLM_SELECTION_THRESHOLD:\n        print('%s %s' % (str(c_name + ':').ljust(25), c_val))\n        if c_name != \"Intercept\":\n            selected_feature.append(c_name)","856947fc":"best_glm.varimp_plot()","dc321201":"# collect regularization paths from dict in DataFrame\nreg_path_dict = best_glm.getGLMRegularizationPath(best_glm)\n\nreg_path_frame = pd.DataFrame(columns=reg_path_dict['coefficients'][0].keys())\nfor i in range(0, len(reg_path_dict['coefficients'])): \n    reg_path_frame = reg_path_frame.append(reg_path_dict['coefficients'][i], \n                                           ignore_index=True)\n\n# plot regularization paths\nfig, ax_ = plt.subplots(figsize=(8, 6))\n_ = reg_path_frame[selected_feature].plot(kind='line', ax=ax_, title='Penalized GLM Regularization Paths',\n                                      colormap='gnuplot')\n_ = ax_.set_xlabel('Iteration')\n_ = ax_.set_ylabel('Coefficient Value')\n_ = ax_.axhline(c='k', lw=1, xmin=0.045, xmax=0.955)\n_ = plt.legend(bbox_to_anchor=(1.05, 0),\n               loc=3, \n               borderaxespad=0.)","d9ea4fa7":"fes = [\"GrLivArea\",\"OverallQual\",\"YearBuilt\",\"TotalBsmtSF\",\"OverallCond\"]\nbest_glm.partial_plot(h2o.H2OFrame(train_glm),cols = fes)","407173ad":"#use valid set to make prediction\nyhat = best_glm.predict(h2o.H2OFrame(valid_glm))\n\n#merge\nglm_yhat_valid = pd.concat([valid_glm.reset_index(drop=True),\n                           yhat.as_data_frame()],\n                           axis = 1)\n\n#rename\nglm_yhat_valid = glm_yhat_valid.rename(columns = {\"predict\":\"p_SalePrice\"})\n\n#find percentile\nglm_percentile_dict = explain.get_percentile_dict(\"p_SalePrice\",glm_yhat_valid,\"Id\")\n\n#display\nglm_percentile_dict","622f799c":"#show the SalePrice and predicted result\nglm_yhat_valid[[\"SalePrice\",\"p_SalePrice\"]].head(10)","17a35f7a":"#'get current axis'\nax = plt.gca()\n\nglm_yhat_valid.plot(x = \"Id\",y = \"SalePrice\",color = \"red\",ax=ax)\nglm_yhat_valid.plot(x = \"Id\",y = \"p_SalePrice\", ax = ax)\n\n#plt.show()","75fff2c3":"#assign model roles\nx=train_glm[x_names]\ny=train_glm['SalePrice']\n\n#set the model\nmodel_xgb = xgb.XGBRegressor(n_estimators=1150, \n                             max_depth=3, \n                             learning_rate=0.06,\n                             colsample_bytree=0.2) \n\nmodel_xgb.fit(x, y)","0ecbed96":"model_xgb","ee54ddee":"#make prediction\np_SalePrice = best_glm.predict(h2o.H2OFrame(test_glm))\np_SalePrice = p_SalePrice.as_data_frame()\n\n#format a new data frame\nId = test_glm[\"Id\"]\nglm = pd.DataFrame(data = Id)\nglm[\"SalePrice\"] = p_SalePrice\n\n#make exponential\nglm[\"SalePrice\"] = np.exp(glm[\"SalePrice\"])\n    \n#export csv file\nglm.to_csv(r\"..\/glm_submission.csv\",index = False, header = True)","9ecc6724":"#display the head\nglm.head()","9ed9ad0a":"#make prediction\ntest_glm.drop(columns = \"Id\",inplace = True)\npred_xgb=model_xgb.predict(test_glm)\npred_xgb = np.exp(pred_xgb)\n\n#create dataframe\nxgb = pd.DataFrame(data = test[\"Id\"])\nxgb[\"SalePrice\"] = pred_xgb\nxgb.to_csv(r\"..\/xgb_submission.csv\",index = False, header = True)","bf72f766":"xgb.head()","63a69241":"#calculate the average prediction result\navg_result = (xgb[\"SalePrice\"] +glm[\"SalePrice\"])\/2\navg = pd.DataFrame(data = test[\"Id\"])\navg[\"SalePrice\"] = avg_result\n\navg.to_csv(r\"..\/avg_submission.csv\",index = False, header = True)","5eac774b":"avg.head(10)","9f02ef6b":"#gbm\npros_gbm1 = H2OGradientBoostingEstimator(nfolds=5,\n                                        seed=SEED,\n                                        keep_cross_validation_predictions = True)\npros_gbm1.train(x=selected_feature, y=y_name, training_frame=h2o.H2OFrame(train_glm),validation_frame=h2o.H2OFrame(valid_glm))\n\n\n#get score history\nsh1 = pros_gbm1.score_history()\n\n#set variables\nx_var = \"number_of_trees\"\ny1 = \"training_rmse\"\ny2 = \"validation_rmse\"\n\n##'get current axis'\nax = plt.gca()\n\nsh1.plot(x = x_var,y = y1,color = \"red\",ax=ax)\nsh1.plot(x = x_var,y = y2, ax = ax)","e091b77e":"yhat_gbm1 = pros_gbm1.predict(h2o.H2OFrame(valid_glm))\n#merge\nglm_yhat_valid = pd.concat([valid_glm.reset_index(drop=True),\n                           yhat_gbm1.as_data_frame()],\n                           axis = 1)\n\n#rename\nglm_yhat_valid = glm_yhat_valid.rename(columns = {\"predict\":\"p_SalePrice\"})\n\n#find percentile\nglm_percentile_dict = explain.get_percentile_dict(\"p_SalePrice\",glm_yhat_valid,\"Id\")\n\n#display\nglm_percentile_dict\n\nglm_yhat_valid[[\"SalePrice\",\"p_SalePrice\"]].head(10)\n\n#'get current axis'\nax = plt.gca()\n\nglm_yhat_valid.plot(x = \"Id\",y = \"SalePrice\",color = \"red\",ax=ax)\nglm_yhat_valid.plot(x = \"Id\",y = \"p_SalePrice\", ax = ax)\n\n#plt.show()","44e5a079":"pros_gbm2 = H2OGradientBoostingEstimator(nfolds=5,\n                                        seed=SEED,\n                                        keep_cross_validation_predictions = True)\npros_gbm2.train(x=x_names, y=y_name, training_frame=h2o.H2OFrame(train_glm),validation_frame=h2o.H2OFrame(valid_glm))\n\n#get score history\nsh2 = pros_gbm2.score_history()\n\n##'get current axis'\nax = plt.gca()\n\nsh2.plot(x = x_var,y = y1,color = \"red\",ax=ax)\nsh2.plot(x = x_var,y = y2, ax = ax)\n\nyhat_gbm2 = pros_gbm2.predict(h2o.H2OFrame(valid_glm))\n#merge\nglm_yhat_valid2 = pd.concat([valid_glm.reset_index(drop=True),\n                           yhat_gbm2.as_data_frame()],\n                           axis = 1)\n","edacc293":"#rename\nglm_yhat_valid2 = glm_yhat_valid.rename(columns = {\"predict\":\"p_SalePrice\"})\n\n#find percentile\nglm_percentile_dict2 = explain.get_percentile_dict(\"p_SalePrice\",glm_yhat_valid,\"Id\")\n\n#display\nglm_percentile_dict2\n\n#'get current axis'\nax = plt.gca()\n\nglm_yhat_valid2.plot(x = \"Id\",y = \"SalePrice\",color = \"red\",ax=ax)\nglm_yhat_valid2.plot(x = \"Id\",y = \"p_SalePrice\", ax = ax)\n\n#plt.show()\n","69d8bacc":"#### assign model roles","bdfd5abe":"##### boxplot the SalePrice","8c17a428":"average the result","1a55a0c3":"#### determine the type of remaining missing data","3fa57662":"## 1. Explore, and Prepare UCI Credit Card Default Data","b4998914":"#### show the final output","85370cb4":"#### log transform of Y variable -- SalePrice","0e8a4548":"#### 2. directly use gbm, with dummy variables","915fe6cb":"### create dummy variables","c079f358":"#### TotalBsmtSF","02e8d305":"* **`GarageCond`**: Evaluates the Garage condition\n* **`GarageQual`**: Garage quality\n* **`GarageFinish`**: Interior finish of the garage\n* **`GarageType`**: Garage location\n* **`GarageYrBlt`**: Year garage was built\n\nFor \"GarageCond\",\"GarageQual\",\"GarageFinish\",\"GarageType\", NA means no garages.","f6305d2b":"### impute the data","34db6e9a":"### Summarize and handle missing value of training data","b942821d":"#### GarageCars","4cbb5600":"### remove Outlier","d2b7a8ac":"## Alternative methods: Construct GBM\n\nThess methods are not selected to predict the test SalePrice.","f2d35495":"#### determine data types","2f9deaf7":"## Split into to train and validation (before doing data prep!!!)","fc45929b":"Use valid set to predict the result","593c822a":"#### 1stFlrSF","815cee54":"#### glm prediction result for test set","2f2a02b6":"### 2.1 GLM feature selection","c57350ba":"### describe all the categorical values in the original train model -- df_train","2347083c":"show the head of glm prediction result","51e91aa3":"### Average result","af8877f0":"#### check pearson correlation","7929c122":"#### create h2o glm model","82aeab86":"#### compare the SalePrice and predicted result","d5d5f1f7":"#### GrLivArea","42bcaf04":"#### one-hot encode vategorical variables","f2240577":"#### remove columns with 15% or more missing values","42237b7c":"#### display the result","0dd7d04c":"#### Global hyperpameters","3e507437":"#### display the glm","fa8af0ea":"### find some percentile of yhat in the validadtion data","48522fdc":"#### YearBuilt","15cde416":"* **`BsmtCond`**: Evaluates the general condition of the basement\n* **`BsmtQual`**: Evaluates the height of the basement\n* **`BsmtFinType1`**: Rating of basement finished area\n* **`BsmtFinType2`**: Rating of basement finished area (if multiple types)\n* **`BsmtExposure`**: Refers to walkout or garden level walls\n\nFor the above variables, NA means no basement. Filling missing variables to None.","7bae11aa":"show the head of xgb prediction result","338b76cb":"#### OverallQual","fa07153a":"use valid set to get predict Sale_Price","2423cf3a":"Since **`Utilities`** have only 2 unique values but 1459 out of 1460 values are the same, drop the column.","8b00849d":"#### describe the missing data","78f7231c":"#### python imports and inits","05c48f12":"# House Prices: Advanced Regression Techniques\n\n## House Prices: Advanced Regression Techniques\n\n#### Team Name: MLGW4\n#### Score: 0.11878\n#### Rank: top 14%","950cb283":"#### plot the comparison","46218693":"#### Import data and reformat","8076100d":"#### xgboost prediction result for test set","956a99bd":"#### 1. gbm with selected feature","d380f984":"* **`Electrical`**: Electrical: Electrical system\n* **`MasVnrType`**: Masonry veneer type\n* **`MasVnrArea`**: Masonry veneer type\n\nFor the above variables, filling the missing values with the most frequent records","d5adb6ce":"#### GarageArea","191f8b25":"## 2.2 XGboost","826b1377":"## 2. Train Models","df27e0e9":"#### plot the partial dependence"}}