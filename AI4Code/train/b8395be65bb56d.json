{"cell_type":{"4d0a6257":"code","61298acc":"code","639bf87d":"code","920ce491":"code","991e968b":"markdown","d68d6218":"markdown","86d0f93e":"markdown","aaad047c":"markdown","cea5499f":"markdown","0d38206a":"markdown","25ac3278":"markdown","fb2397d6":"markdown","53a85976":"markdown","d74116ac":"markdown"},"source":{"4d0a6257":"from IPython.display import YouTubeVideo      \nYouTubeVideo('KFnuOeD8m5I')","61298acc":"YouTubeVideo('Kc8qbgwPzU0')","639bf87d":"YouTubeVideo('s838HC_5pkM')","920ce491":"from IPython.display import IFrame\n\nurl = 'https:\/\/developers-dot-devsite-v2-prod.appspot.com\/machine-learning\/crash-course\/fitter\/graph_f95207894da1506ac43649903412c2e708392bab2f084d53bf6e6aae44010a36.frame'\nIFrame(url, width=800, height=800)\n","991e968b":"# Life is Gradient Descent - Part 2","d68d6218":"## Iterations","86d0f93e":"\n![image.png](attachment:image.png)","aaad047c":"Hello,\n\nIn this session we are dicussing Gradient Descent from the base of concept till Mathematical approach.\n\nSection is divided into three parts....\n\nYou can go also through the video explanation of this notebook below with technical rythm:)\n\nPart 1: https:\/\/youtu.be\/KFnuOeD8m5I  - Concept  \n\nNotebook: https:\/\/www.kaggle.com\/farsanas\/gradient-descent-part-1\n\nPart 2: https:\/\/youtu.be\/Kc8qbgwPzU0  - Life is Gradient Descent\n\nPart 3: https:\/\/youtu.be\/s838HC_5pkM  - Mathematical approach towards Gradient Descent\n\nNotebook: https:\/\/www.kaggle.com\/farsanas\/gradient-descent-mathematical-approach-part3\n","cea5499f":"![LR.PNG](attachment:LR.PNG)\n\n\n  The value by which weights are updated is determined by Learning Rate","0d38206a":"### Goldilocks learning rate\n \n     Goldilocks value is related to how flat the loss function is. If you know the value of loss function is small then you can safely try a larger learning rate","25ac3278":"## How fast should I?????","fb2397d6":"## Importance of Learning Rate\n\n    Points to consider:\n    1. Learning rate is too small - learning will take too long \n    \n    2. Learning rate is too large - bounce","53a85976":"![hill.PNG](attachment:hill.PNG)","d74116ac":"After lot of iterations we obtain weights and bias which has minimum loss\n\n![itr.PNG](attachment:itr.PNG)"}}