{"cell_type":{"e642a18d":"code","0036c82d":"code","e5e6cfed":"code","77ba61e8":"code","bc509bd1":"code","0487b342":"code","0f8b7143":"code","7ca0f8e3":"code","a2558ef5":"code","14991154":"code","d69f2f34":"code","caa288d7":"code","8f932692":"code","59bbe48d":"code","0a9123e8":"code","1925a72d":"code","ac8a05ea":"code","c452fc56":"code","616bccf9":"code","1cfe5d91":"code","6ed5ae6e":"code","2cc6a9ef":"code","f639d434":"code","32da955e":"code","3618c187":"code","2daf273e":"code","f1f9c249":"code","6f4dbd0b":"code","8b23a9a7":"code","145ddf4f":"code","e9469d6c":"code","ebf816e5":"code","2a324926":"code","89493b77":"code","21a7ba80":"code","93967213":"markdown","2170051e":"markdown","53037c32":"markdown","30091f0d":"markdown","5e85a867":"markdown","c55f0cb7":"markdown","c7fe04d9":"markdown","c98a3a37":"markdown","af4c3dfe":"markdown","8fae5de6":"markdown","9d2147dc":"markdown","86c2b8d1":"markdown","0e3c2974":"markdown","9114add1":"markdown","7eb6b390":"markdown","cc5e0a3c":"markdown","7bb00c82":"markdown","b213cb64":"markdown","17954321":"markdown","3259b6d9":"markdown","5debf87b":"markdown","1947e703":"markdown","9fbe0726":"markdown","a1658f9e":"markdown","46611fc1":"markdown","af806765":"markdown","7d1d55b4":"markdown","710c8681":"markdown","7d7c6b18":"markdown","d86b0bd0":"markdown","ad0caafe":"markdown","9032971f":"markdown","76c9ce27":"markdown"},"source":{"e642a18d":"!mkdir Dataset","0036c82d":"# copy VITON dataset\n!cp -r \/kaggle\/input\/viton-dataset\/ACGPN_TestData\/test_color Dataset\n!cp -r \/kaggle\/input\/viton-dataset\/ACGPN_TestData\/test_edge Dataset\n!cp -r \/kaggle\/input\/viton-dataset\/ACGPN_TestData\/test_mask Dataset\n!cp -r \/kaggle\/input\/viton-dataset\/ACGPN_TestData\/test_colormask Dataset","e5e6cfed":"# copy TestData\n!cp -r \/kaggle\/input\/tt-ryon\/test_img Dataset\n!cp -r \/kaggle\/input\/tryon-testdata\/Dataset\/test_pose Dataset\n!cp -r \/kaggle\/input\/tryon-testdata\/Dataset\/test_label Dataset\n!rm Dataset\/test_pose\/*\n!rm Dataset\/test_label\/*","77ba61e8":"!git clone https:\/\/github.com\/rkuo2000\/semantic-segmentation-pytorch # predict.py add output a _gray.png\n%cd semantic-segmentation-pytorch","bc509bd1":"from torchvision import transforms\n\nfrom segmentation.data_loader.segmentation_dataset import SegmentationDataset\nfrom segmentation.data_loader.transform import Rescale, ToTensor\nfrom segmentation.trainer import Trainer\nfrom segmentation.predict import *\nfrom segmentation.models import all_models\nfrom util.logger import Logger","0487b342":"model_name = \"pspnet_mobilenet_v2\"\ndevice = 'cuda'\nbatch_size = 4\nn_classes = 34 \ncheck_point_stride = 1 # store checkpoints every 1 epoch   \nimage_axis_minimum_size = 200\n\nnum_epochs = 0    # 1 for 1st training\n                  # n for retraining\n                  # 0 for detect-only\npretrained = False# True  for num_epochs=1 without logger.load_model below\n                  # False for num_epochs=n with    logger.load_model below\n                  # False for detect-only  with    logger.load_model below\nfixed_feature = False\n\nlogger = Logger(model_name=model_name, data_name='example')","0f8b7143":"### Model\nmodel = all_models.model_from_name[model_name](n_classes, batch_size, \n                                               pretrained=pretrained, \n                                               fixed_feature=fixed_feature)\nmodel.to(device)","7ca0f8e3":"# copy checkpoint\n!mkdir -p runs\/models\/pspnet_mobilenet_v2\/example\n!cp \/kaggle\/input\/pspnet-viton-checkpoints\/epoch_1 runs\/models\/pspnet_mobilenet_v2\/example\n\n# Load checkpoint\nlogger.load_model(model,'epoch_1')","a2558ef5":"test_img_file   = '..\/Dataset\/test_img\/000000_0.jpg'\ntest_label_file = '..\/Dataset\/test_label\/000000_0.png'","14991154":"### Writing gray label\npredict(model, test_img_file, test_label_file )","d69f2f34":"from IPython.display import Image","caa288d7":"# show test image\nImage(test_img_file)","8f932692":"# show test color label\nImage(test_label_file)","59bbe48d":"# show test gray label\nImage(test_label_file.replace('.png','_gray.png'))","0a9123e8":"# copy gray label to overwrite color label as test_label .png\n!mv ..\/Dataset\/test_label\/000000_0_gray.png ..\/Dataset\/test_label\/000000_0.png","1925a72d":"%cd ..","ac8a05ea":"!pip install tfjs-graph-converter","c452fc56":"!git clone https:\/\/github.com\/rwightman\/posenet-pytorch  # fix bugs\n%cd posenet-pytorch","616bccf9":"file = '..\/Dataset\/test_img\/000000_0.jpg'","1cfe5d91":"import torch\nfrom posenet.constants import *\nfrom posenet.decode_multi import decode_multiple_poses\nfrom posenet.models.model_factory import load_model\nfrom posenet.utils import *\n\nnet = load_model(101)\nnet = net.cuda()\noutput_stride = net.output_stride\nscale_factor = 1.0\n\ninput_image, draw_image, output_scale = posenet.read_imgfile(file, scale_factor=scale_factor, output_stride=output_stride)","6ed5ae6e":"import torch\nwith torch.no_grad():\n    input_image = torch.Tensor(input_image).cuda()\n\n    heatmaps_result, offsets_result, displacement_fwd_result, displacement_bwd_result = net(input_image)\n\n    pose_scores, keypoint_scores, keypoint_coords = posenet.decode_multiple_poses(\n        heatmaps_result.squeeze(0),\n        offsets_result.squeeze(0),\n        displacement_fwd_result.squeeze(0),\n        displacement_bwd_result.squeeze(0),\n        output_stride=output_stride,\n        max_pose_detections=10,\n        min_pose_score=0.25)","2cc6a9ef":"import matplotlib.pyplot as plt\n# read image\nimage = plt.imread(file)\n\nposes = []\n# find face keypoints & detect face mask\nfor pi in range(len(pose_scores)):\n    if pose_scores[pi] != 0.:\n        print('Pose #%d, score = %f' % (pi, pose_scores[pi]))       \n        keypoints = keypoint_coords.astype(np.int32) # convert float to integer\n        print(keypoints[pi])\n        poses.append(keypoints[pi])\nprint(len(poses))","f639d434":"img = plt.imread(file)\ni=0\npose = poses[0]\nplt.imshow(img)    \nfor y,x in pose:\n    plt.plot(x, y, 'w.') # 'w.': color='white', marker='.'\n    plt.text(x, y, str(i), color='r', fontsize=10)\n    i+=1   \nplt.show()","32da955e":"# map rccpose-to-openpose mapping\nindices = [0, (5,6), 6, 8, 10, 5, 7, 9, 12, 14, 16, 11, 13, 15, 2, 1, 4, 3]","3618c187":"# convert rcnnpose to openpose\ni=0\nopenpose = []\nfor ix in indices:\n    if ix==(5,6):\n        openpose.append([int((pose[5][1]+pose[6][1])\/2), int((pose[5][0]+pose[6][0])\/2), 1])   \n    else:\n        openpose.append([int(pose[ix][1]),int(pose[ix][0]),1])        \n    i+=1\n    \nprint(openpose)","2daf273e":"plt.imshow(img)\ni=0\nfor x,y,z in openpose:\n    plt.plot(x, y, 'w.') # 'w.': color='white', marker='.'\n    plt.text(x, y, str(i), color='r', fontsize=10)\n    i+=1\nplt.show()","f1f9c249":"import json\ncoords = []\nfor x,y,z in openpose:\n    coords.append(float(x))\n    coords.append(float(y))\n    coords.append(float(z))\n\ndata = {\"version\": 1.0}\npose_dic = {}\npose_dic['pose_keypoints'] = coords\ntmp = []\ntmp.append(pose_dic)\ndata[\"people\"]=tmp\nprint(data)\n\npose_name = '..\/Dataset\/test_pose\/000000_0_keypoints.json'\nwith open(pose_name,'w') as f:\n     json.dump(data, f)  ","6f4dbd0b":"%cd ..","8b23a9a7":"# read pose\nimport numpy as np\nimport json\n#pose_name = '\/kaggle\/input\/tryon-testdata\/Dataset\/test_pose\/000000_0_keypoints.json'\npose_name = 'Dataset\/test_pose\/000000_0_keypoints.json'\nwith open(pose_name, 'r') as f:\n     pose_label = json.load(f)\n     pose_data = pose_label['people'][0]['pose_keypoints']\n     pose_data = np.array(pose_data)\n     pose_data = pose_data.reshape((-1,3))\nprint(pose_data)\nprint(len(pose_data))","145ddf4f":"# show pose keypoints on the test image\nimg = plt.imread('\/kaggle\/input\/tt-ryon\/test_img\/000000_0.jpg')\nplt.imshow(img)\ni=0\nfor x,y,z in pose_data: \n    plt.plot(x, y, 'w.') # 'w.': color='white', marker='.'\n    plt.text(x, y, str(i), color='r', fontsize=10)\n    i+=1\nplt.show()\nprint(i)","e9469d6c":"!git clone https:\/\/github.com\/rkuo2000\/DeepFashion_Try_On\n%cd DeepFashion_Try_On","ebf816e5":"%cd ACGPN_inference","2a324926":"# copy the pre-trained model (checkpoint)\n!cp -rf \/kaggle\/input\/acgpn-checkpoints\/label2city checkpoints","89493b77":"!python test.py --dataroot ..\/..\/Dataset --color_name 000118_1.jpg","21a7ba80":"from IPython.display import Image\nImage('.\/sample\/000000_0.jpg')","93967213":"## Dataset: VITON dataset \n    This dataset contains 16,253 image pairs, further splitting into a training set of 14,221 paris and a testing set of 2,032 pairs.","2170051e":"## Repro [Github](https:\/\/github.com\/IanTaehoonYoo\/semantic-segmentation-pytorch) (semantic-segmentation-pytorch)","53037c32":"## Show image & its label (segmentation)","30091f0d":"# Pose Detection: [PoseNet](https:\/\/arxiv.org\/abs\/1505.07427)","5e85a867":"### OpenPose COCO format\n![](https:\/\/github.com\/CMU-Perceptual-Computing-Lab\/openpose\/blob\/master\/.github\/media\/keypoints_pose_18.png?raw=true)","c55f0cb7":"## Repro [Github](https:\/\/github.com\/switchablenorms\/DeepFashion_Try_On) (DeepFashion Try On)","c7fe04d9":"#### License: The use of this software is RESTRICTED to non-commercial research and educational purposes.","c98a3a37":"![](https:\/\/external-content.duckduckgo.com\/iu\/?u=https%3A%2F%2Fhszhao.github.io%2Fprojects%2Fpspnet%2Ffigures%2Fpspnet.png&f=1&nofb=1)","af4c3dfe":"# PSPNet + PoseNet + ACGPN","8fae5de6":"## Detect label (semantic segmentation)","9d2147dc":"![](https:\/\/github.com\/switchablenorms\/DeepFashion_Try_On\/raw\/master\/images\/tryon.png)","86c2b8d1":"## Copy Dataset","0e3c2974":"### save keypoints.json","9114add1":"## Verify _keypoints.json","7eb6b390":"## Repro [Github](https:\/\/github.com\/rwightman\/posenet-pytorch) (PoseNet-PyTorch)","cc5e0a3c":"### Display TryOn result","7bb00c82":"### find keypoint coordinates in poses","b213cb64":"### draw keypoints ","17954321":"## Load checkpoint ","3259b6d9":"## Convert Keypoints (from PoseNet to OpenPose)","5debf87b":"### PoseNet output\n![](https:\/\/debuggercafe.com\/wp-content\/uploads\/2020\/10\/keypoint_exmp.jpg)","1947e703":"#### output : sample\/000000_0.jpg","9fbe0726":"#### copy TestData (image, pose, label)","a1658f9e":"## Test TryOn model ","46611fc1":"![TryOn.jpg](attachment:TryOn.jpg)\n\n#### real_image (original) -----> pose_map -----> cloth_mask -----> color (dress) -----> fake_image (generated)","af806765":"### read .json","7d1d55b4":"# Try On: [Towards Photo-Realistic Virtual Try-On by Adaptively Generating, CVPR'20](https:\/\/arxiv.org\/abs\/2003.05863)","710c8681":"#### copy VITON dataset (color, edge, mask, colormask)","7d7c6b18":"![](https:\/\/github.com\/tensorflow\/tfjs-models\/raw\/master\/posenet\/demos\/camera.gif)","d86b0bd0":"### Download pre-trained model (checkpoint)","ad0caafe":"# Semantic Segmentation: [PSPNet](https:\/\/arxiv.org\/abs\/1505.07427)-MobileNet-v2\n","9032971f":"## Detect Pose keypoints","76c9ce27":"### show PoseNet keypoints"}}