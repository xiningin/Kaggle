{"cell_type":{"68fd9582":"code","86cb069e":"code","f45e9a22":"code","7c3b6792":"code","06fba41e":"code","22f5b149":"code","121e9d0b":"code","5a8e887e":"code","99dbd05b":"code","9cd68ee9":"code","6594dab7":"code","00c3f229":"code","b168f9ea":"code","2f5270ce":"code","29def93d":"code","4fc28643":"code","08121bfe":"markdown","6093ba00":"markdown","f38ca9d4":"markdown","f5176c86":"markdown","6a7217ee":"markdown","02c60532":"markdown","6bfb6563":"markdown","f78b0b2d":"markdown","c86ad67d":"markdown"},"source":{"68fd9582":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom fastai.tabular.all import *\nimport optuna\nfrom optuna.integration import FastAIV2PruningCallback\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","86cb069e":"path = Path('\/kaggle\/input\/tabular-playground-series-aug-2021')\nPath.BASE_PATH = path\n\ndf = pd.read_csv(path\/'train.csv', low_memory=False)\ndf.head()","f45e9a22":"dep_var = 'loss'\ncont, cat = cont_cat_split(df, dep_var=dep_var)\ncont.remove('id')\nlen(cont), len(cat)","7c3b6792":"def rmse(y_hat, y):\n    \"\"\"\n    Root Mean Squared Error\n    Note: the competition does not divide by N so we don't here\n    \"\"\"\n    return torch.sqrt(torch.mean((y_hat - y)**2))","06fba41e":"def create_dls(df, bs = 2048, valid_pct = 0.25):\n    to = TabularPandas(df, \n                  procs=[Normalize],\n                  y_names=dep_var,\n                  cont_names=cont,\n                  cat_names=cat,\n                  splits=RandomSplitter(valid_pct)(range_of(df)),\n                  reduce_memory=False)\n    return to.dataloaders(bs=bs)","22f5b149":"early_stopping_cbs = [\n    EarlyStoppingCallback(monitor='valid_loss', patience=2, min_delta=0.01),\n]\n\ndef train_nn(dls, layers, ps, wd, y_range_eps, cbs):\n    learn = tabular_learner(dls, layers=layers,\n                            #procs=[Normalize],\n                            y_range=(dls.train.y.min() - y_range_eps, dls.train.y.max() + y_range_eps),\n                            config={'ps': ps, 'act_cls': nn.LeakyReLU(inplace=True)}, \n                            wd=wd, \n                            loss_func=rmse)\n    suggested_lr = learn.lr_find(show_plot=False)\n    learn.fit_one_cycle(2, 1e-3, cbs=early_stopping_cbs + cbs)\n    learn.fit_one_cycle(55, 1e-3 \/ 100, cbs=early_stopping_cbs + cbs)\n    return learn","121e9d0b":"dls = create_dls(df, bs=8192, valid_pct=0.5)\n\ndef objective(trial: optuna.Trial, y_range_eps: float):\n    num_layers = trial.suggest_int('num_layers', 1, 3)\n    wd = trial.suggest_float('wd', 0.0, 1.0)\n    layers = [\n        trial.suggest_categorical('layer_0', [512, 256, 128]),\n        trial.suggest_categorical('layer_1', [512, 256, 128]),\n        trial.suggest_categorical('layer_2', [512, 256, 128]),\n    ]\n    ps = [\n        trial.suggest_discrete_uniform('dropout_0', 0.0, 0.95, 0.05),\n        trial.suggest_discrete_uniform('dropout_1', 0.0, 0.95, 0.05),\n        trial.suggest_discrete_uniform('dropout_2', 0.0, 0.95, 0.05),\n    ]\n    learn = train_nn(dls, layers[:num_layers], ps[:num_layers], wd, \n                     y_range_eps=y_range_eps,\n                     cbs=[FastAIV2PruningCallback(trial, monitor='valid_loss',)])\n    return learn.validate()[0]","5a8e887e":"# Comment out to save time\n\n# timeout = 3600 * 2\n# study = optuna.create_study(direction='minimize',\n#                            pruner=optuna.pruners.MedianPruner(n_warmup_steps=4, interval_steps=2))\n# study.optimize(lambda trial: objective(trial, y_range_eps=0.00),\n#                n_trials=35, timeout=timeout)","99dbd05b":"optuna.visualization.plot_optimization_history(study)","9cd68ee9":"optuna.visualization.plot_param_importances(study)","6594dab7":"print('Number of finished trials:', len(study.trials))\nprint('Best trial parameters:', study.best_trial.params)\nprint('Best score:', study.best_value)","00c3f229":"learn = train_nn(dls, layers=[512, 256],\n                 ps=[0.6, 0.7], wd=0.373, y_range_eps=0.0, cbs=[])","b168f9ea":"test_df = pd.read_csv(path\/'test.csv')\ntest_dl = learn.dls.test_dl(test_df)\ntest_dl.show_batch()","2f5270ce":"preds, _ = learn.get_preds(dl=test_dl)\npreds = preds.numpy().flatten()","29def93d":"submission_df = pd.DataFrame({'id': test_df.id, 'loss': preds})\nsubmission_df.head()","4fc28643":"submission_df.to_csv('submission.csv', index=False)","08121bfe":"# Get data and split into continuous and categorical variables\n\nNote that for this dataset we'll treat all the features as **continuous**","6093ba00":"# Helper functions\n\nWe'll use RMSE (Root Mean Squared Error) for the loss function.\nLet's also make helper functions to create `DataLoaders` and to train our Neural Net (NN)","f38ca9d4":"## Please upvote if you found this useful","f5176c86":"# Optuna Hyperparameter Tuning\nI just learned about Optuna and it's been a game changer. Now I don't have to painstakingly fiddle with hyperparameters. Instead I just specify a range of values and Optuna takes care of the rest. \n\nIt's honestly so convenient it almost feels like a cheat code. ","6a7217ee":"# Create Submission","02c60532":"# Please upvote if you found this useful ","6bfb6563":"Notice how in our `objective` function, we always decrease the layer sizes. This is because it doesn't really make sense to increase layer_sizes through the network. Increasing layer sizes means we're asking the model to generate more numbers from less numbers. Instead what makes sense is to have the layer sizes __decrease__ because then we're asking the model to take a large number of features and turn them into a smaller, more feature-rich representation. ","f78b0b2d":"# Train Model using Optuna Hyperparameters","c86ad67d":"# Optuna Results\n\nFor this dataset I have not been able to get below ~7.90 on the validation set using a NN; and not below 7.94 on the final evaluation set used by kaggle.\n\nIn a way this is expected, as XGBRegressor and CatBoostRegressor will often perform better on tabular datasets. Exactly why NN's cannot achieve similar performance is something I'm still looking into. With XGB- and CatBoost- Regressors, I've been able to achieve 7.87 but still cannot hit the SOTA 7.85. If you have any suggestions, tips, or comments, please share them below! "}}