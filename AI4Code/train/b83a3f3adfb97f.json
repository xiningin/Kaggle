{"cell_type":{"793f1b6e":"code","ba7f010a":"code","71ecdd84":"code","bb177209":"code","c0acbecd":"code","c3dc74b4":"code","a005a240":"code","71b84101":"code","8771ca96":"code","9e416a23":"code","d2e5466d":"code","6b9e907d":"code","6dca5603":"code","8ca69172":"code","59c7c8a9":"code","1f83ec83":"code","835ea8e5":"code","6cc2b8b6":"code","67522c34":"code","aeee3479":"code","4806b320":"code","73627495":"code","210c6079":"code","82e2e77e":"code","80d61df5":"code","42f62dd6":"code","b8483226":"code","5793a605":"code","7213c572":"code","45c00df8":"code","2ea6725b":"code","79429345":"code","babd0ea9":"code","7ad68e55":"code","9682cc75":"code","8beacd42":"code","1b433fd4":"code","2b5a8cac":"code","7826c24a":"code","d3e228a6":"code","1b08dd01":"code","a524ccec":"markdown","904fffd1":"markdown","97d5a88a":"markdown","019b1e76":"markdown","d65c1aaa":"markdown","91e2142f":"markdown","4ecc6c5d":"markdown","1cc692d9":"markdown","f8bdfc9b":"markdown","cc253f83":"markdown","25f6db9e":"markdown","a8f7ff94":"markdown","f4e5cc2c":"markdown","1843e46d":"markdown","9f80707a":"markdown","f281d360":"markdown","df3ef21e":"markdown"},"source":{"793f1b6e":"# data analysis and wrangling\nimport numpy as np \nimport pandas as pd \nimport random as rnd\nimport math\nimport statsmodels.api as sm\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\n\n#Visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# machine learning\nimport sklearn\nimport tensorflow as tf\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score","ba7f010a":"train_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv') \ntest_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ncombine = [train_df, test_df]","71ecdd84":"display(train_df.head())\ndisplay(train_df.tail())","bb177209":"train_df.info()\nprint('_'*40)\n# test_df.info()","c0acbecd":"train_df.describe()","c3dc74b4":"train_df.describe(include=['O'])","a005a240":"correlation_df = train_df.copy()\ndisplay(correlation_df.head(10))","71b84101":"correlation_df.isna().sum().sort_values().tail(20)","8771ca96":"num_attribs = ['LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold', 'SalePrice'] \ncat_attribs = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']\n\nlen(num_attribs) + len(cat_attribs)","9e416a23":"correlation_df[cat_attribs] = correlation_df[cat_attribs].astype(str)","d2e5466d":"encoder = OrdinalEncoder()\ncorrelation_df[cat_attribs] = encoder.fit_transform(correlation_df[cat_attribs])","6b9e907d":"imputer = SimpleImputer(strategy=\"median\")\ncorrelation_df[num_attribs] = imputer.fit_transform(correlation_df[num_attribs])","6dca5603":"display(correlation_df.head(10))\ncorrelation_df[num_attribs].isna().sum()","8ca69172":"corr = correlation_df.corr()\nsns.heatmap(corr)","59c7c8a9":"columns = np.full((corr.shape[0],), True, dtype=bool)\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if corr.iloc[i,j] >= 0.7:\n            if columns[j]:\n                columns[j] = False\nselected_columns = correlation_df.columns[columns]\ncorrelation_df_table = correlation_df[selected_columns]","1f83ec83":"correlation_df","835ea8e5":"import statsmodels.api as sm\nregressor_OLS = sm.OLS(endog = correlation_df['SalePrice'], exog = correlation_df.loc[:, correlation_df.columns != 'SalePrice'].values).fit()\nregressor_OLS.summary()","6cc2b8b6":"def find_max_index(list_of_falues):\n    local_maximum = -100\n    local_index = 0\n    for i in range(len(list_of_falues)):\n        if list_of_falues[i] > local_maximum:\n            local_maximum = list_of_falues[i]\n            index = i\n    return index\n\nmax_p_value = 1\n\n\nwhile max_p_value > 0.05:\n    regressor_OLS = sm.OLS(endog = correlation_df['SalePrice'], exog = correlation_df.loc[:, correlation_df.columns != 'SalePrice'].values).fit()\n    max_p_value = regressor_OLS.pvalues[find_max_index(regressor_OLS.pvalues)]\n    if max_p_value > 0.05:\n        correlation_df.drop(correlation_df.columns[find_max_index(regressor_OLS.pvalues)], axis=1, inplace = True)\n\nregressor_OLS.summary()","67522c34":"interesting_columns = correlation_df.columns","aeee3479":"test_df.info()","4806b320":"working_df_train = train_df.copy()[interesting_columns[:-1]]\nworking_df_test = test_df.copy()[interesting_columns[:-1]]\n\ny_train = train_df.copy()[interesting_columns[-1]]\ny_train","73627495":"working_df_train.info()\nnum_attrib = ['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'MasVnrArea', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'BsmtFullBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'GarageCars', 'WoodDeckSF', 'ScreenPorch', 'PoolArea', 'YrSold']\ncat_attrib = ['Street', 'Neighborhood', 'Condition2', 'BldgType', 'HouseStyle', 'RoofMatl', 'Exterior1st', 'MasVnrType', 'ExterQual', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'KitchenQual', 'Functional', 'FireplaceQu', 'PoolQC', 'SaleCondition']","210c6079":"print(working_df_test['Exterior1st'].mode())\nprint(working_df_test['KitchenQual'].mode())\nprint(working_df_test['Functional'].mode())","82e2e77e":"mode_exterior1st = working_df_test['Exterior1st'].mode()\nworking_df_test['Exterior1st'] = working_df_test['Exterior1st'].fillna('VinylSd')\nworking_df_test['KitchenQual'] = working_df_test['KitchenQual'].fillna('TA')\nworking_df_test['Functional'] = working_df_test['Functional'].fillna('Typ')\n\nworking_df_test.info()","80d61df5":"num_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('std_scaler', StandardScaler()),\n    ])\n\ncat_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"constant\", fill_value = 'empty')),\n        ('encoder', OneHotEncoder(sparse=False)),\n    ])\n\nfull_pipeline = ColumnTransformer([\n        (\"cat\", cat_pipeline, cat_attrib),\n        (\"num\", num_pipeline, num_attrib),\n    ])\n\nX_train = full_pipeline.fit_transform(working_df_train)\nX_test = full_pipeline.transform(working_df_test)","42f62dd6":"print(pd.DataFrame(X_train))","b8483226":"print(pd.DataFrame(X_test))","5793a605":"rf = RandomForestRegressor(n_estimators = 200, max_depth = 20, random_state = 0)\nrf.fit(X_train, y_train)\n\nfrom sklearn.metrics import r2_score, mean_squared_log_error\nprint(r2_score(y_train, rf.predict(X_train)))\nprint(mean_squared_log_error(y_train, rf.predict(X_train)))\n\n#from sklearn.model_selection import GridSearchCV\n#parameters_rf = [{'n_estimators': [200, 1000], \n#                  'max_depth': [14, 20]}]\n#grid_search_rf = GridSearchCV(estimator = rf,\n#                           param_grid = parameters_rf,\n#                           scoring = 'neg_mean_squared_log_error',\n#                           cv = 5,\n#                           verbose = 1)\n#grid_search_rf.fit(X_train, y_train)\n#best_log_error_rf = grid_search_rf.best_score_\n#best_parameters_rf = grid_search_rf.best_params_\n#print(\"Best error: {:.2f} \".format(best_log_error_rf))\n#print(\"Best Parameters:\", best_parameters_rf)","7213c572":"from xgboost import XGBRegressor\nboost = XGBRegressor(n_estimators = 550, max_depth = 3, eta = 0.1, random_state = 0)\nboost.fit(X_train, y_train)\n\nfrom sklearn.metrics import r2_score, mean_squared_log_error\nprint(r2_score(y_train, boost.predict(X_train)))\nprint(mean_squared_log_error(y_train, boost.predict(X_train)))\n\nfrom sklearn.model_selection import GridSearchCV\nparameters_boost = [{'n_estimators': [400, 500, 600],\n                     'max_depth' : [2, 3, 4]}]\ngrid_search_boost = GridSearchCV(estimator = boost,\n                           param_grid = parameters_boost,\n                           scoring = 'neg_mean_squared_log_error',\n                           cv = 10,\n                           verbose = 1)\n# grid_search_boost.fit(X_train, y_train)\n#best_log_error_boost = grid_search_boost.best_score_\n#best_parameters_boost = grid_search_boost.best_params_\n#print(\"Best error: {:.4f} \".format(best_log_error_boost))\n#print(\"Best Parameters:\", best_parameters_boost)\n\n#print(\"Grid scores on development set:\")\n#print()\n#means = grid_search_boost.cv_results_['mean_test_score']\n#stds = grid_search_boost.cv_results_['std_test_score']\n#for mean, std, params in zip(means, stds, grid_search_boost.cv_results_['params']):\n#    print(\"%0.5f (+\/-%0.05f) for %r\"\n#            % (mean, std * 2, params))","45c00df8":"from sklearn.neighbors import KNeighborsRegressor\nknn = KNeighborsRegressor(n_neighbors = 6, metric = 'minkowski', p = 2)\nknn.fit(X_train, y_train)\n\nfrom sklearn.metrics import r2_score, mean_squared_log_error\nprint(r2_score(y_train, knn.predict(X_train)))\nprint(mean_squared_log_error(y_train, knn.predict(X_train)))\n\nfrom sklearn.model_selection import GridSearchCV\nparameters_knn = [{'n_neighbors': [3, 4, 5, 6, 7, 8, 9, 2, 10, 15]}]\ngrid_search_knn = GridSearchCV(estimator = knn,\n                           param_grid = parameters_knn,\n                           scoring = 'neg_mean_squared_log_error',\n                           cv = 10,\n                           verbose = 1)\n#grid_search_knn.fit(X_train, y_train)\n#best_log_error_knn = grid_search_knn.best_score_\n#best_parameters_knn = grid_search_knn.best_params_\n#print(\"Best error: {:.4f} \".format(best_log_error_knn))\n#print(\"Best Parameters:\", best_parameters_knn)","2ea6725b":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\nfrom sklearn.metrics import r2_score, mean_squared_log_error\nprint(r2_score(y_train, regressor.predict(X_train)))\nprint(mean_squared_log_error(y_train, regressor.predict(X_train)))","79429345":"# y_pred_rf = rf.predict(X_test)","babd0ea9":"# y_pred_boost = boost.predict(X_test)","7ad68e55":"#submission_house_price_rf = pd.DataFrame({'Id' : test_df['Id'],\n#                                         'SalePrice' : y_pred_rf})\n#submission_house_price_rf.to_csv('\/kaggle\/working\/submission_house_price_rf_2.csv', index=False)","9682cc75":"#submission_house_price_boost = pd.DataFrame({'Id' : test_df['Id'],\n#                                         'SalePrice' : y_pred_boost})\n#submission_house_price_boost.to_csv('\/kaggle\/working\/submission_house_price_boost_no_scaling_depth4_n500.csv', index=False)","8beacd42":"number_neurons = 175\nnumber_of_layers_relu = 3\nann = tf.keras.models.Sequential()\nann.add(tf.keras.layers.Dense(units=number_neurons, activation = 'tanh')) \nfor i in range(number_of_layers_relu):\n    ann.add(tf.keras.layers.Dense(units=number_neurons, activation = 'relu')) \nann.add(tf.keras.layers.Dense(units=1, activation = 'linear')) ","1b433fd4":"from tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nes = EarlyStopping(monitor='val_loss', mode='min', patience = 50, verbose=1)\nmc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True) \n\nopt = keras.optimizers.Adam(learning_rate=0.005)\nann.compile(optimizer = opt, loss = 'mean_squared_logarithmic_error', metrics = [tf.keras.metrics.MeanSquaredLogarithmicError()])\nhistory = ann.fit(X_train, y_train, validation_split = 0.10, batch_size = 32, epochs = 3000, callbacks=[es, mc])","2b5a8cac":"history_dict = history.history\nloss_values = history_dict['loss']\nlog_loss_values = history_dict['mean_squared_logarithmic_error']\nval_loss_values = history_dict['val_loss']\nval_log_loss_values = history_dict['val_mean_squared_logarithmic_error']\nepochs = range(1, len(loss_values) + 1)\n\nplt.plot(epochs, loss_values, color = 'blue', label='Training loss')\nplt.scatter(epochs, val_loss_values, color = 'red', s = 5, label='Validation loss')\nplt.title('Training & Validation Loss', fontsize=16)\nplt.xlabel('Epochs', fontsize=16)\nplt.ylabel('Loss', fontsize=16)\nplt.legend()\n#plt.ylim(0, 200)\nplt.show()\n\nplt.plot(epochs, log_loss_values, color = 'blue', label='Training loss')\nplt.scatter(epochs, val_log_loss_values, color = 'red', s = 5, label='Validation loss')\nplt.title('Training & Validation Log Loss', fontsize=16)\nplt.xlabel('Epochs', fontsize=16)\nplt.ylabel('Log Loss', fontsize=16)\nplt.legend()\nplt.ylim(0, 0.1)\nplt.show()","7826c24a":"history_dict = history.history\nloss_values = history_dict['loss']\nlog_loss_values = history_dict['mean_squared_logarithmic_error']\nval_loss_values = history_dict['val_loss']\nval_log_loss_values = history_dict['val_mean_squared_logarithmic_error']\nepochs = range(1, len(loss_values) + 1)\n\nplt.plot(epochs, loss_values, color = 'blue', label='Training loss')\nplt.scatter(epochs, val_loss_values, color = 'red', s = 5, label='Validation loss')\nplt.title('Training & Validation Loss', fontsize=16)\nplt.xlabel('Epochs', fontsize=16)\nplt.ylabel('Loss', fontsize=16)\nplt.legend()\n#plt.ylim(0, 200)\nplt.show()\n\nplt.plot(epochs, log_loss_values, color = 'blue', label='Training loss')\nplt.scatter(epochs, val_log_loss_values, color = 'red', s = 5, label='Validation loss')\nplt.title('Training & Validation Log Loss', fontsize=16)\nplt.xlabel('Epochs', fontsize=16)\nplt.ylabel('Log Loss', fontsize=16)\nplt.legend()\nplt.ylim(0, 0.1)\nplt.show()","d3e228a6":"y_pred_ann = np.squeeze(ann.predict(X_test), axis = 1)","1b08dd01":"submission_house_price_ann = pd.DataFrame({'Id' : test_df['Id'],\n                                         'SalePrice' : y_pred_ann})\nsubmission_house_price_ann.to_csv('\/kaggle\/working\/submission_house_price_ann.csv', index=False)","a524ccec":"**In the majority of cases empty cells mean lack of this object. We will label this cells as 0 class**","904fffd1":"### **<font color = 'green'>3.1 Initializing the ANN<\/font>**","97d5a88a":"**So, we built a simple linear regression model and obtained information about the parameters.\nNow we are dooing backward elimination. The loop is created, and in the loop body the simple linear regression model is built. Then, the parameter with the highest p-value is detected and eliminated and again and again until we reach the situation when the highset p-value is lower than 0.05 (statistical significance). In the end we will get truncated table, containing much less number of columns**","019b1e76":"## 4.1 Random forest","d65c1aaa":"# 2. Finding correlations and necessary features","91e2142f":"### **<font color = 'green'>3.2 Training the ANN<\/font>**","4ecc6c5d":"### **<font color = 'green'>3.3 Plot the loss vs epochs<\/font>**","1cc692d9":"boost_blending = XGBRegressor(n_estimators = 75, max_depth = 3, eta = 0.1, random_state = 0)\nboost_blending.fit(X_train_predicted_from_previous, y_2nd_level)\n\nfrom sklearn.metrics import r2_score, mean_squared_log_error\nprint(r2_score(y_2nd_level, boost_blending.predict(X_train_predicted_from_previous)))\nprint(mean_squared_log_error(y_2nd_level, boost_blending.predict(X_train_predicted_from_previous)))\n\nfrom sklearn.model_selection import GridSearchCV\nparameters_blend = [{'n_estimators': [50, 100, 75, 125, 150, 200, 500],\n                     'max_depth' : [2, 3, 4],\n                     'eta' : [0.1, 0.05, 0.2]}]\ngrid_search_blend = GridSearchCV(estimator = boost_blending,\n                           param_grid = parameters_blend,\n                           scoring = 'neg_mean_squared_log_error',\n                           cv = 5,\n                           verbose = 1)\ngrid_search_blend.fit(X_train_predicted_from_previous, y_2nd_level)\nbest_log_error_blend = grid_search_blend.best_score_\nbest_parameters_blend = grid_search_blend.best_params_\nprint(\"Best error: {:.4f} \".format(best_log_error_blend))\nprint(\"Best Parameters:\", best_parameters_blend)","f8bdfc9b":"submission_house_price_blend = pd.DataFrame({'Id' : test_df['Id'],\n                                         'SalePrice' : y_pred_blend})\nsubmission_house_price_blend.to_csv('\/kaggle\/working\/submission_house_price_blending.csv', index=False)","cc253f83":"# 6. ANN","25f6db9e":"# 5. Blending","a8f7ff94":"# 3. Extracting values and data preprocessing","f4e5cc2c":"There are a lot of features. It's complicated to study them separetely, therefore we will try to find correlation between them, and will exclude those who hav high correlation.","1843e46d":"# Building the model","9f80707a":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost import XGBRegressor\n\nX_first_lvl, X_2nd_level, y_first_lvl, y_2nd_level = train_test_split(X_train, y_train, test_size = 0.2, random_state = 0)\n\nrf_1 = RandomForestRegressor(n_estimators = 200, max_depth = 20, random_state = 0)\nboost_1 = XGBRegressor(n_estimators = 550, max_depth = 3, eta = 0.1, random_state = 0)\nknn_1 = KNeighborsRegressor(n_neighbors = 6, metric = 'minkowski', p = 2)\nregressor_1 = LinearRegression()\n\nrf_1.fit(X_first_lvl, y_first_lvl)\nboost_1.fit(X_first_lvl, y_first_lvl)\nknn_1.fit(X_first_lvl, y_first_lvl)\nregressor_1.fit(X_first_lvl, y_first_lvl)\n\ny_pred_rf_1 = rf_1.predict(X_2nd_level)\ny_pred_boost_1 = boost_1.predict(X_2nd_level)\ny_pred_knn_1 = knn_1.predict(X_2nd_level)\ny_pred_regressor_1 = regressor_1.predict(X_2nd_level)\n\nX_train_predicted_from_previous = pd.DataFrame({'predicted_rf' : y_pred_rf_1,\n                                              'predicted_boost' : y_pred_boost_1,\n                                              'predicted_linear' : y_pred_regressor_1\n    \n})\n","f281d360":"y_pred_rf_1_test = rf_1.predict(X_test)\ny_pred_boost_1_test = boost_1.predict(X_test)\ny_pred_knn_1_test = knn_1.predict(X_test)\ny_pred_regressor_1_test = regressor_1.predict(X_test)\n\nX_test_predicted_from_previous = pd.DataFrame({'predicted_rf' : y_pred_rf_1_test,\n                                              'predicted_boost' : y_pred_boost_1_test,\n                                              'predicted_linear' : y_pred_regressor_1_test\n    \n})\n\ny_pred_blend = boost_blending.predict(X_test_predicted_from_previous)","df3ef21e":"# 1. Looking at data"}}