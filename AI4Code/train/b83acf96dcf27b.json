{"cell_type":{"b2bedc43":"code","6b59f8b8":"code","942d7406":"code","5d6e37ba":"code","b89b3000":"code","25383873":"code","cca62e39":"code","7e800ad9":"code","1398b786":"code","e89ce999":"code","730f867b":"code","bd26825e":"code","d52535a0":"code","2036ec35":"code","beef4ea5":"code","641b955c":"code","0acd25e7":"code","97a52b3f":"code","1766db71":"code","66556e45":"code","47f60e0b":"code","8dea6302":"code","6fa68157":"code","35acac14":"code","f0728723":"code","c757821b":"code","d024cd94":"code","2aea9293":"code","e6012cda":"code","c1c28571":"code","fbe776f6":"code","e9a965b0":"code","93519834":"code","6a762445":"code","6f9298a6":"code","de59ebc4":"code","2341ddd8":"code","83790abd":"code","d69e9eef":"code","4bfc17cf":"code","712aff02":"code","e3078971":"code","dd0bb2bb":"code","fa5b1b65":"code","a9355ef5":"code","db6f6964":"code","244f99e0":"code","a4357b8e":"code","4f91ec1b":"code","8b6a7b4e":"code","0fceac4a":"code","6815abe5":"code","85319ccd":"markdown","06fe363f":"markdown","9068f085":"markdown","bfd01218":"markdown","73d6b8ce":"markdown","ddec0615":"markdown","df2bd380":"markdown","4ae56dd3":"markdown","2cca7c67":"markdown","406c74a5":"markdown","36c793b9":"markdown","892f1aee":"markdown","039e1007":"markdown","0d21c19d":"markdown","d610ab8a":"markdown","c76930ff":"markdown","59dd28dd":"markdown","8619ccda":"markdown","8942923e":"markdown"},"source":{"b2bedc43":"#importing librariaes\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","6b59f8b8":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport warnings\nwarnings.filterwarnings(\"ignore\")","942d7406":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/test.csv')\n\nprint(train.shape)\nprint(test.shape)","5d6e37ba":"train.dtypes","b89b3000":"train1=train.drop(['Id'],1)\ntest1=test.drop(['Id'],1)","25383873":"import gc\ndel train,test\ngc.collect()","cca62e39":"from collections import Counter\nprint(sorted(Counter(train1['Cover_Type']).items()))\nsns.countplot(train1['Cover_Type'],palette='Greens')","7e800ad9":"train1= train1[train1.Cover_Type != 5]","1398b786":"fig, axes = plt.subplots(4, 3, figsize=(20,15))\nsns.histplot(ax=axes[0,0],data=train1, x=\"Slope\",kde=True,hue=\"Cover_Type\",palette=\"YlGn\")\nsns.histplot(ax=axes[0,1],data=train1, x=\"Elevation\",kde=True,hue=\"Cover_Type\",palette=\"YlGn\")\nsns.histplot(ax=axes[0,2],data=train1, x=\"Aspect\",kde=True,hue=\"Cover_Type\",palette=\"YlGn\")\nsns.histplot(ax=axes[1,0],data=train1, x=\"Horizontal_Distance_To_Hydrology\",kde=True,hue=\"Cover_Type\",palette=\"YlGn\")\nsns.histplot(ax=axes[1,1],data=train1, x=\"Vertical_Distance_To_Hydrology\",kde=True,hue=\"Cover_Type\",palette=\"YlGn\")\nsns.histplot(ax=axes[1,2],data=train1, x=\"Horizontal_Distance_To_Roadways\",kde=True,hue=\"Cover_Type\",palette=\"YlGn\")\nsns.histplot(ax=axes[2,0],data=train1, x=\"Hillshade_9am\",kde=True,hue=\"Cover_Type\",palette=\"YlGn\")\nsns.histplot(ax=axes[2,1],data=train1, x=\"Hillshade_Noon\",kde=True,hue=\"Cover_Type\",palette=\"YlGn\")\nsns.histplot(ax=axes[2,2],data=train1, x=\"Hillshade_3pm\",kde=True,hue=\"Cover_Type\",palette=\"YlGn\")\nsns.histplot(ax=axes[3,0],data=train1, x=\"Horizontal_Distance_To_Fire_Points\",kde=True,hue=\"Cover_Type\",palette=\"YlGn\")","e89ce999":"fig, axes = plt.subplots(15, 3, figsize=(15, 60))\nsns.countplot(ax=axes[0,0], data=train1, x='Wilderness_Area1',palette='YlGn')\nsns.countplot(ax=axes[0,1], data=train1, x='Wilderness_Area2',palette='YlGn')\nsns.countplot(ax=axes[0,2], data=train1, x='Wilderness_Area3',palette='YlGn')\nsns.countplot(ax=axes[1,0], data=train1, x='Wilderness_Area4',palette='YlGn')\nsns.countplot(ax=axes[1,1], data=train1, x='Soil_Type1',palette='YlGn')\nsns.countplot(ax=axes[1,2], data=train1, x='Soil_Type2',palette='YlGn')\nsns.countplot(ax=axes[2,0], data=train1, x='Soil_Type3',palette='YlGn')\nsns.countplot(ax=axes[2,1], data=train1, x='Soil_Type4',palette='YlGn')\nsns.countplot(ax=axes[2,2], data=train1, x='Soil_Type5',palette='YlGn')\nsns.countplot(ax=axes[3,0], data=train1, x='Soil_Type6',palette='YlGn')\nsns.countplot(ax=axes[3,1], data=train1, x='Soil_Type7',palette='YlGn')\nsns.countplot(ax=axes[3,2], data=train1, x='Soil_Type8',palette='YlGn')\nsns.countplot(ax=axes[4,0], data=train1, x='Soil_Type9',palette='YlGn')\nsns.countplot(ax=axes[4,1], data=train1, x='Soil_Type10',palette='YlGn')\nsns.countplot(ax=axes[4,2], data=train1, x='Soil_Type11',palette='YlGn')\nsns.countplot(ax=axes[5,0], data=train1, x='Soil_Type12',palette='YlGn')\nsns.countplot(ax=axes[5,1], data=train1, x='Soil_Type13',palette='YlGn')\nsns.countplot(ax=axes[5,2], data=train1, x='Soil_Type14',palette='YlGn')\nsns.countplot(ax=axes[6,0], data=train1, x='Soil_Type15',palette='YlGn')\nsns.countplot(ax=axes[6,1], data=train1, x='Soil_Type16',palette='YlGn')\nsns.countplot(ax=axes[6,2], data=train1, x='Soil_Type17',palette='YlGn')\nsns.countplot(ax=axes[7,0], data=train1, x='Soil_Type18',palette='YlGn')\nsns.countplot(ax=axes[7,1], data=train1, x='Soil_Type19',palette='YlGn')\nsns.countplot(ax=axes[7,2], data=train1, x='Soil_Type20',palette='YlGn')\nsns.countplot(ax=axes[8,0], data=train1, x='Soil_Type21',palette='YlGn')\nsns.countplot(ax=axes[8,1], data=train1, x='Soil_Type22',palette='YlGn')\nsns.countplot(ax=axes[8,2], data=train1, x='Soil_Type23',palette='YlGn')\nsns.countplot(ax=axes[9,0], data=train1, x='Soil_Type24',palette='YlGn')\nsns.countplot(ax=axes[9,1], data=train1, x='Soil_Type25',palette='YlGn')\nsns.countplot(ax=axes[9,2], data=train1, x='Soil_Type26',palette='YlGn')\nsns.countplot(ax=axes[10,0], data=train1, x='Soil_Type27',palette='YlGn')\nsns.countplot(ax=axes[10,1], data=train1, x='Soil_Type28',palette='YlGn')\nsns.countplot(ax=axes[10,2], data=train1, x='Soil_Type29',palette='YlGn')\nsns.countplot(ax=axes[11,0], data=train1, x='Soil_Type30',palette='YlGn')\nsns.countplot(ax=axes[11,1], data=train1, x='Soil_Type31',palette='YlGn')\nsns.countplot(ax=axes[11,2], data=train1, x='Soil_Type32',palette='YlGn')\nsns.countplot(ax=axes[12,0], data=train1, x='Soil_Type33',palette='YlGn')\nsns.countplot(ax=axes[12,1], data=train1, x='Soil_Type34',palette='YlGn')\nsns.countplot(ax=axes[12,2], data=train1, x='Soil_Type35',palette='YlGn')\nsns.countplot(ax=axes[13,0], data=train1, x='Soil_Type36',palette='YlGn')\nsns.countplot(ax=axes[13,1], data=train1, x='Soil_Type37',palette='YlGn')\nsns.countplot(ax=axes[13,2], data=train1, x='Soil_Type38',palette='YlGn')\nsns.countplot(ax=axes[14,0], data=train1, x='Soil_Type39',palette='YlGn')\nsns.countplot(ax=axes[14,1], data=train1, x='Soil_Type40',palette='YlGn')","730f867b":"train1.isnull().sum()","bd26825e":"train_x=train1.drop(['Soil_Type7','Soil_Type15'],1)\ntest_x=test1.drop(['Soil_Type7','Soil_Type15'],1)\ndel test1,train1\ngc.collect()\n","d52535a0":"train_x.describe()","2036ec35":"train_x[\"Aspect\"][train_x[\"Aspect\"] < 0] += 360\ntrain_x[\"Aspect\"][train_x[\"Aspect\"] > 359] -= 360\n\ntest_x[\"Aspect\"][test_x[\"Aspect\"] < 0] += 360\ntest_x[\"Aspect\"][test_x[\"Aspect\"] > 359] -= 360","beef4ea5":"train_x.loc[train_x[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\ntest_x.loc[test_x[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n\ntrain_x.loc[train_x[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\ntest_x.loc[test_x[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n\ntrain_x.loc[train_x[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\ntest_x.loc[test_x[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n\ntrain_x.loc[train_x[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\ntest_x.loc[test_x[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n\ntrain_x.loc[train_x[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\ntest_x.loc[test_x[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n\ntrain_x.loc[train_x[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\ntest_x.loc[test_x[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255","641b955c":"'''cols= ['Elevation','Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology','Aspect',\n       'Hillshade_Noon','Hillshade_3pm','Hillshade_9am','Slope','Wilderness_Area4','Wilderness_Area3','Soil_Type25','Wilderness_Area1','Soil_Type13','Soil_Type23','Soil_Type12','Soil_Type38']'''","0acd25e7":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n \n    return df","97a52b3f":"train_x = reduce_mem_usage(train_x)\ntest_x = reduce_mem_usage(test_x)","1766db71":"'''train_x=train_x.filter(items=cols)\ntest_x=test_x.filter(items=cols)'''","66556e45":"print(train_x.shape)\nprint(test_x.shape)","47f60e0b":"def outlier_function(df, col_name):\n    ''' this function detects first and third quartile and interquartile range for a given column of a dataframe\n    then calculates upper and lower limits to determine outliers conservatively\n    returns the number of lower and uper limit and number of outliers respectively\n    '''\n    first_quartile = np.percentile(np.array(df[col_name].tolist()), 25)\n    third_quartile = np.percentile(np.array(df[col_name].tolist()), 75)\n    IQR = third_quartile - first_quartile\n                      \n    upper_limit = third_quartile+(3*IQR)\n    lower_limit = first_quartile-(3*IQR)\n    outlier_count = 0\n                      \n    for value in df[col_name].tolist():\n        if (value < lower_limit) | (value > upper_limit):\n            outlier_count +=1\n    return lower_limit, upper_limit, outlier_count","8dea6302":"n_features=['Elevation','Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology','Aspect',\n       'Hillshade_Noon','Hillshade_3pm','Hillshade_9am','Slope']\n# loop through all columns to see if there are any outliers\nfor column in n_features:\n    if outlier_function(train_x, column)[2] > 0:\n        print(\"There are {} outliers in {}\".format(outlier_function(train_x, column)[2], column))","6fa68157":"y=train_x['Cover_Type']\ntrain_x=train_x.drop(['Cover_Type'],1)","35acac14":"from sklearn import preprocessing\n\nle = preprocessing.LabelEncoder()\nle.fit(y)\ny=le.transform(y)","f0728723":"'''from sklearn.preprocessing import MinMaxScaler\n\nx= pd.concat([train_x,test_x],axis=0)\ntrans = MinMaxScaler()\ntrans.fit(x)\nX= trans.transform(train_x)\ntest_x = trans.transform(test_x)\ndata= pd.DataFrame(X, columns=train_x.columns)'''","c757821b":"'''from sklearn.preprocessing import RobustScaler\n\nscaler = RobustScaler()\ntrain_x[n_features] = scaler.fit_transform(train_x[n_features])\ntest_x[n_features] = scaler.fit_transform(test_x[n_features])\ndata= pd.DataFrame(train_x, columns=train_x.columns)'''","d024cd94":"'''fig, axes = plt.subplots(4, 3, figsize=(20,15))\nsns.histplot(ax=axes[0,0],data=data, x=\"Slope\",kde=True,color=\"#71C074\")\nsns.histplot(ax=axes[0,1],data=data, x=\"Elevation\",kde=True,color=\"#71C074\")\nsns.histplot(ax=axes[0,2],data=data, x=\"Aspect\",kde=True,color=\"#71C074\")\nsns.histplot(ax=axes[1,0],data=data, x=\"Horizontal_Distance_To_Hydrology\",kde=True,color=\"#71C074\")\nsns.histplot(ax=axes[1,1],data=data, x=\"Vertical_Distance_To_Hydrology\",kde=True,color=\"#71C074\")\nsns.histplot(ax=axes[1,2],data=data, x=\"Horizontal_Distance_To_Roadways\",kde=True,color=\"#71C074\")\nsns.histplot(ax=axes[2,0],data=data, x=\"Hillshade_9am\",kde=True,color=\"#71C074\")\nsns.histplot(ax=axes[2,1],data=data, x=\"Hillshade_Noon\",kde=True,color=\"#71C074\")\nsns.histplot(ax=axes[2,2],data=data, x=\"Hillshade_3pm\",kde=True,color=\"#71C074\")\nsns.histplot(ax=axes[3,0],data=data, x=\"Horizontal_Distance_To_Fire_Points\",kde=True,color=\"#71C074\")'''","2aea9293":"'''del data\ngc.collect()'''","e6012cda":"train_x.isna().sum()","c1c28571":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(train_x, y, test_size=0.4, random_state=1234)\nx_test1, x_val, y_test1, y_val = train_test_split(x_test, y_test, test_size=0.2, random_state=1234)\n\nprint(\"X train shape: \", x_train.shape)\nprint(\"X validation shape: \", x_val.shape)\nprint(\"X test shape: \", x_test.shape)\nprint(\"Y train shape: \", y_train.shape)\nprint(\"Y validation shape: \", y_val.shape)\nprint(\"Y test shape: \", y_test.shape)\ndel y\ngc.collect()","fbe776f6":"!pip install pytorch-tabnet","e9a965b0":"'''y_train=y_train.values\ny_test=y_test.values\ny_test1=y_test1.values\ny_val=y_val.values'''\n\nx_train=x_train.values\nx_test=x_test.values\nx_test1=x_test1.values\nx_val=x_val.values\n","93519834":"test_x=test_x.values","6a762445":"import pytorch_tabnet\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nimport torch\nfrom sklearn.metrics import roc_auc_score, accuracy_score","6f9298a6":"'''clf1 = TabNetClassifier() \nclf1.fit(\n  x_train, y_train,\n  eval_set=[(x_val, y_val)],\n  eval_name=['valid'],\n  eval_metric=['accuracy'],\n  max_epochs=50 , patience=15\n)'''\n","de59ebc4":"# define the model\nclf2 = TabNetClassifier(optimizer_fn=torch.optim.Adam,\n                       optimizer_params=dict(lr=2e-2),\n                       scheduler_params={\"step_size\":4, # how to use learning rate scheduler\n                                         \"gamma\":0.9},\n                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n                       mask_type='entmax' # \"sparsemax\"\n                      )\n\n# fit the model \nclf2.fit(\n    x_train,y_train,\n    eval_set=[(x_train, y_train), (x_val, y_val)],\n    eval_name=['train', 'valid'],\n    eval_metric=['accuracy'],\n    max_epochs=160 , patience=25\n)            ","2341ddd8":"# determine best accuracy for test set\npreds = clf2.predict(x_test1)\ntest_acc = accuracy_score(preds, y_test1)\n\n# determine best accuracy for validation set\npreds_valid = clf2.predict(x_val)\nvalid_acc = accuracy_score(preds_valid, y_val)\n\nprint(\"Accuracy score on validation set:\", valid_acc)\nprint(\"Accuracy score on test set: \",test_acc)\n","83790abd":"# plot losses\nplt.plot(clf2.history['loss'])","d69e9eef":"# plot accuracy\nplt.plot(clf2.history['valid_accuracy'])","4bfc17cf":"# find and plot feature importance\ny_pred = clf2.predict(x_test1)\nclf2.feature_importances_\nfeat_importances = pd.Series(clf2.feature_importances_, index=train_x.columns)\nfeat_importances.nlargest(20).plot(kind='barh')","712aff02":"from sklearn.metrics import classification_report\npreds = clf2.predict(x_test1)\nprint(classification_report(preds,y_test1))","e3078971":"#test_x=test_x.values\npreds = clf2.predict(test_x)\npreds","dd0bb2bb":"newpred=le.inverse_transform(preds)","fa5b1b65":"newpred","a9355ef5":"from pytorch_tabnet.pretraining import TabNetPretrainer\n# TabNetPretrainer\nunsupervised_model = TabNetPretrainer(\n    optimizer_fn=torch.optim.Adam,\n    optimizer_params=dict(lr=2e-2),\n    mask_type='entmax' # \"sparsemax\"\n)\n\nunsupervised_model.fit(\n    X_train=x_train,\n    eval_set=[x_val],\n    pretraining_ratio=0.8,\n    max_epochs=20 , patience=4,\n)\n\nclf = TabNetClassifier(\n    optimizer_fn=torch.optim.Adam,\n    optimizer_params=dict(lr=3e-2),\n    scheduler_params={\"step_size\":10, # how to use learning rate scheduler\n                      \"gamma\":0.9},\n    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n    mask_type='sparsemax' # This will be overwritten if using pretrain model\n)\n\nclf.fit(\n    X_train=x_train, y_train=y_train,\n    eval_set=[(x_train, y_train), (x_val, y_val)],\n    eval_name=['train', 'valid'],\n    eval_metric=['accuracy'],\n    max_epochs=20 , patience=5,\n    from_unsupervised=unsupervised_model\n)\n\n","db6f6964":"predst = clf.predict(x_test1)\ntest_acc = accuracy_score(predst, y_test1)\n\n\npreds_valid = clf.predict(x_val)\nvalid_acc = accuracy_score(preds_valid, y_val)\n\nprint(\"Accuracy score on validation set:\", valid_acc)\nprint(\"Accuracy score on test set: \",test_acc)","244f99e0":"feat_importances = pd.Series(clf.feature_importances_, index=train_x.columns)\nfeat_importances.nlargest(20).plot(kind='barh')","a4357b8e":"# plot losses\nplt.plot(clf.history['loss'])","4f91ec1b":"# plot accuracy\nplt.plot(clf.history['valid_accuracy'])","8b6a7b4e":"preds1 = clf.predict(test_x)\npreds1","0fceac4a":"df_submission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/sample_submission.csv')\ndf_submission['Cover_Type']=newpred\ndf_submission.to_csv('submission.csv',index=False)","6815abe5":"df_submission.head()","85319ccd":"#### <u>Architecture<\/u>\n\n![1__DKm0iz5fATChDHjlWqNVQ.png](attachment:cf0c046f-7561-4746-9019-7438d52c824b.png)","06fe363f":"#### <u>Overview<\/u>\n\n*  <b> TabNet inputs raw tabular data without any preprocessing and is trained using gradient descent-based optimisation.<\/b>\n* <b> TabNet uses sequential attention to choose features at each decision step, enabling interpretability and   better learning as the learning capacity is used for the most useful features.<\/b>\n*  <b>Feature selection is instance-wise, e.g. it can be different for each row of the training dataset.<\/b>\n*  <b>TabNet employs a single deep learning architecture for feature selection and reasoning, this is known as soft         feature selection.<\/b>\n* <b> Above design choices allows TabNet to enable two kinds of interpretability: local interpretability that   visualises the importance of features and how they are combined for a single row, and global interpretability which      quantifies the contribution of each feature to the trained model across the dataset.<\/b>","9068f085":"### TabNet ..","bfd01218":"#### only one observation for type 5. hence I will remove it.","73d6b8ce":"#### TabNet with default parameters.","ddec0615":"#### <u> scaling data <\/u>","df2bd380":"## <p style=\"background-color:#86FDB4;color:black;font-size:20px;text-align:center;border-radius:10px 10px;border: 3px solid green;\"> \ud83c\udf0d Submission <\/p>","4ae56dd3":"<p style=\"background-color:#65FB9F;color:black;font-size:25px;text-align:center;border-radius:10px 10px;font-weight:bold;border: 3px solid green;\">Tabular Playground Series - Dec 2021 \ud83c\udf04\ud83e\udded<\/p>\n\n<center><img src=\"https:\/\/github.com\/Isharaneranjana\/kaggle_gif\/blob\/main\/Forest%20cover%20type%20prediction%20(1).gif?raw=True\"><\/center>","2cca7c67":"#### TabNet can also be pretrained as an unsupervised model. Pretraining involves deliberately masking certain cells and learning relationships between these cells and adjacent columns by predicting the masked values. The weights learned can then be saved and used for a supervised task.","406c74a5":"<center><img src=\"https:\/\/media.giphy.com\/media\/4FQMuOKR6zQRO\/giphy.gif\"><\/center>","36c793b9":"#### <u> Categorical features<\/u>","892f1aee":"#### <u> Numerical features<\/u>","039e1007":"#### <u>Useful Sources<\/u>\n\n*  <b>Original paper : https:\/\/arxiv.org\/abs\/1908.07442 <\/b>\n*  <b> TabNet \u2014 Deep Neural Network for Structured, Tabular Data : https:\/\/towardsdatascience.com\/tabnet-deep-neural-network-for-structured-tabular-data-39eb4b27a9e4<\/b>\n*  <b> TabNet: The End of Gradient Boosting?: https:\/\/towardsdatascience.com\/tabnet-e1b979907694<\/b>\n","0d21c19d":"<center><font size=\"4\" color=\"black\"><b>Highly appreciate your questions or feedback related to this notebook. THANK YOU <span style='font-size:22px;'>&#128522;<\/span><\/b> <\/font><\/center>","d610ab8a":"#### There are no missing values in all the variables. ","c76930ff":"## <p style=\"background-color:#86FDB4;color:black;font-size:20px;text-align:center;border-radius:10px 10px;border: 3px solid green;\">\ud83d\udd0fDescription of the dataset <\/p>\n\n<font size=\"4\">This dataset contains 56 variables and 4000000  observations about different cover types.<\/font>\n<font size=\"4\">Here's a brief version of the data description file.<\/font>\n    \n* Elevation - Elevation in meters\n* Aspect - Aspect in degrees azimuth\n* Slope - Slope in degrees\n* Horizontal_Distance_To_Hydrology - Horz Dist to nearest surface water features\n* Vertical_Distance_To_Hydrology - Vert Dist to nearest surface water features\n* Horizontal_Distance_To_Roadways - Horz Dist to nearest roadway\n* Hillshade_9am (0 to 255 index) - Hillshade index at 9am, summer solstice\n* Hillshade_Noon (0 to 255 index) - Hillshade index at noon, summer solstice\n* Hillshade_3pm (0 to 255 index) - Hillshade index at 3pm, summer solstice\n* Horizontal_Distance_To_Fire_Points - Horz Dist to nearest wildfire ignition points\n* Wilderness_Area (4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation\n* Soil_Type (40 binary columns, 0 = absence or 1 = presence) - Soil Type designation\n* Cover_Type (7 types, integers 1 to 7) - Forest Cover Type designation\n    ","59dd28dd":"## <p style=\"background-color:#86FDB4;color:black;font-size:20px;text-align:center;border-radius:10px 10px;border: 3px solid green;\"> \u26fa semi supervised TabNet <\/p>","8619ccda":"<center><img src=\"https:\/\/media.giphy.com\/media\/j1Xyt3DHfJcmk\/giphy.gif\" style=\"width:400px;height:280px;\"><\/center>","8942923e":"##  <p style=\"background-color:#86FDB4;color:black;font-size:20px;text-align:center;border-radius:10px 10px;\"> \ud83c\udf04  Introduction <\/p>\n\n<font size=\"4\">The data is synthetically generated by a GAN that was trained on a the data from the Forest Cover Type Prediction. This dataset is much larger and may or may not have the same relationship to the target as the original data.The original study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. Each observation is a 30m x 30m patch. You are asked to predict an integer classification for the forest cover type. The seven types are:<\/font>\n\n* 1 - Spruce\/Fir\n* 2 - Lodgepole Pine\n* 3 - Ponderosa Pine\n* 4 - Cottonwood\/Willow\n* 5 - Aspen\n* 6 - Douglas-fir\n* 7 - Krummholz\n\n"}}