{"cell_type":{"b9f0a22c":"code","23bc5c7c":"code","3975f9ae":"code","c526cdec":"code","3904cb57":"code","5e32df34":"code","9ee08ce1":"code","afbd9323":"code","446c09ce":"code","4695abd4":"code","da228bcc":"code","9b0c1ec3":"code","092b6a25":"code","771eb17d":"code","d193f435":"code","8e283eaf":"code","7d56179c":"code","b2f0bbfb":"code","b14789a2":"code","c8e8bb42":"code","90a9e242":"markdown","30bc85a5":"markdown","b48657bd":"markdown","4379272f":"markdown","e3d644cd":"markdown","b63c38ac":"markdown","82f23279":"markdown","0df8e8f2":"markdown","9da5aed2":"markdown","72209730":"markdown","d0d569ec":"markdown","7676562a":"markdown","fb4225fa":"markdown","7421daac":"markdown","138e9cec":"markdown","0eae9cf9":"markdown","a4c20bda":"markdown","2b65c447":"markdown","32390030":"markdown","31435a24":"markdown","c0e6e7d7":"markdown","5d017d32":"markdown"},"source":{"b9f0a22c":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm","23bc5c7c":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3975f9ae":"raw_data = pd.read_csv(r'\/kaggle\/input\/bank-data\/Bank-data.csv')\nraw_data.head()","c526cdec":"data=raw_data.copy()\ndata= data.drop(['Unnamed: 0'],axis=1)\ndata['y'] = data['y'].map({'yes':1, 'no':0})\ndata.head()","3904cb57":"data.describe()","5e32df34":"data.columns","9ee08ce1":"y = data['y']\nx1 = data['duration']","afbd9323":"x = sm.add_constant(x1)\nreg_log = sm.Logit(y,x)\nresults_log = reg_log.fit()","446c09ce":"results_log.summary()","4695abd4":"# the odds of duration are the exponential of the log odds from the summary table\nnp.exp(0.0051)","da228bcc":"# To avoid writing them out every time, we save the names of the estimators of our model in a list. \nestimators=['interest_rate','march','credit','previous','duration']\n\nX1 = data[estimators]\ny = data['y']","9b0c1ec3":"X = sm.add_constant(X1)\nreg_logit = sm.Logit(y,X)\nresults_logit = reg_logit.fit()\n","092b6a25":"results_logit.summary()","771eb17d":"def confusion_matrix(data,actual_values,model):\n    pred_values = model.predict(data)\n    bins=np.array([0,0.5,1])\n    cm = np.histogram2d(actual_values, pred_values, bins=bins)[0]\n    accuracy = (cm[0,0]+cm[1,1])\/cm.sum()\n    return cm, accuracy","d193f435":"confusion_matrix(X,y,results_logit)","8e283eaf":"raw_data2 = pd.read_csv(r'\/kaggle\/input\/bank-data-testing\/Bank-data-testing.csv')\ndata_test = raw_data2.copy()\ndata_test = data_test.drop(['Unnamed: 0'], axis = 1)","7d56179c":"data_test['y'] = data_test['y'].map({'yes':1, 'no':0})\ndata_test.head()","b2f0bbfb":"y_test = data_test['y']\nX1_test = data_test[estimators]\nX_test = sm.add_constant(X1_test)","b14789a2":"confusion_matrix(X_test, y_test, results_logit)","c8e8bb42":"confusion_matrix(X,y, results_logit)","90a9e242":"Create the confusion matrix of the model and estimate its accuracy.","30bc85a5":"# Understanding Logistic Regression Tables","b48657bd":"- The dependent variable is 'duration'. \n- The model used is a Logit regression (logistic in common lingo), while the method - Maximum Likelihood Estimation (MLE). It has clearly converged after classifyin 518 observations.\n\n- The Pseudo R-squared is 0.21 which is within the 'acceptable region'.\n\n- The duration variable is significant and its coefficient is -1.7001.\n\n- The constant is also significant and equals: 0.0051\t","4379272f":"### Interpretation","e3d644cd":"# Find the odds of duration","b63c38ac":"Load the test data from the 'Bank_data_testing.csv' file provided. (Remember to convert the outcome variable \u2018y\u2019 into Boolean).","82f23279":"Determine the test confusion matrix and the test accuracy and compare them with the train confusion matrix and the train accuracy.","0df8e8f2":"Looking at the test acccuracy we see a number which is a tiny but lower: 86.04%, compared to 86.29% for train accuracy.","9da5aed2":"Run the regression.","72209730":"We can be omitting many causal factors in our simple logistic model, so we instead switch to a multivariate logistic regression model. Add the \u2018interest_rate\u2019, \u2018march\u2019, \u2018credit\u2019 and \u2018previous\u2019 estimators to our model and run the regression again.","d0d569ec":"# Confusion Matrix","7676562a":"# Test the model","fb4225fa":"Use 'duration' as the independent variable.","7421daac":"# Declare the dependent and the independent variables\n","138e9cec":"### Simple Logistic Regression","0eae9cf9":"- The odds of duration are pretty close to 1. This tells us that although duration is a significant predictor, a change in 1 day would barely affect the regression.\n\n- Note that we could have inferred that from the coefficient itself.\n\n- Finally, note that the data is not standardized (scaled) and duration is a feature of a relatively big order of magnitude.","a4c20bda":"Using the same code as in the previous exercise, try to interpret the summary table.\n\n### More information about the dataset: \nNote that <i> interest rate<\/i> indicates the 3-month interest rate between banks and <i> duration <\/i> indicates the time since the last contact was made with a given consumer. The <i> previous <\/i> variable shows whether the last marketing campaign was successful with this customer. The <i>March<\/i> and <i> May <\/i> are Boolean variables that account for when the call was made to the specific customer and <i> credit <\/i> shows if the customer has enough credit to avoid defaulting.\n\n<i> Notes: \n    <li> the first column of the dataset is an index one; <\/li>\n    <li> you don't need the graph for this exercise; <\/li>\n    <li> the dataset used is much bigger <\/li>\n<\/i>","2b65c447":"## Load the data","32390030":"## Import the relevant libraries","31435a24":"# Expand the model","c0e6e7d7":"Load the \u2018Bank_data.csv\u2019 dataset.","5d017d32":"### Declare the dependent and independent variables"}}