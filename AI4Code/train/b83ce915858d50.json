{"cell_type":{"54aa0207":"code","a4e4027c":"code","90060bf8":"code","2cddadac":"code","0b98e677":"code","04ea8866":"code","a494e792":"code","b29bfd25":"code","0d52418a":"code","bde3315a":"code","2036a23b":"code","38c46758":"code","51ed5268":"code","1d5f04b8":"code","77962a76":"code","464e8c0e":"code","04b7a88b":"code","25d93e4f":"code","84afa9c6":"code","43f89509":"code","92f344e6":"code","68d8caed":"code","5539a0f0":"code","40f559d0":"code","364fc961":"code","3bb92c65":"code","ebf98291":"code","74f3ec58":"code","7ed224e7":"code","2ee47173":"code","a8e98c53":"code","b54066d0":"code","9c479096":"code","25b7ede1":"markdown","bb84b4c6":"markdown","3d3557ab":"markdown","89aa56b8":"markdown","ff19f0e8":"markdown","05763ba3":"markdown","d1068b52":"markdown","700d61df":"markdown","32fc4cb2":"markdown","1d64ed49":"markdown","28184abe":"markdown","5461ed7e":"markdown","fae3bd35":"markdown","9004f676":"markdown","d1ff2761":"markdown","bd4a7a21":"markdown","fc19da9b":"markdown","09617831":"markdown","00e278c0":"markdown","456efe29":"markdown","c2ae183f":"markdown","38facc88":"markdown","2feeecd3":"markdown","c74aee1e":"markdown","64c89319":"markdown","caf02d8c":"markdown","9f2ac1e4":"markdown","c32fd28c":"markdown","7fee3c0d":"markdown","58254b22":"markdown"},"source":{"54aa0207":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a4e4027c":"!pip install tfa-nightly","90060bf8":"!pip install -q pyyaml h5py","2cddadac":"import tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport random \nimport tensorflow_addons as tfa","0b98e677":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(\"Running on Tpu\" , tpu.master())\nexcept ValueError as e:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS:\" , strategy.num_replicas_in_sync)\n        ","04ea8866":"random.seed(1)","a494e792":"GCS_DS_PATH = KaggleDatasets().get_gcs_path()","b29bfd25":"AUTO = tf.data.experimental.AUTOTUNE\nignore_order = tf.data.Options()\nignore_order.experimental_deterministic = False\nLABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), \n        \"class\": tf.io.FixedLenFeature([], tf.int64)}\n\nUNLABELED_TFREC_FORMAT = {\n    \"image\": tf.io.FixedLenFeature([], tf.string), \n    \"id\": tf.io.FixedLenFeature([], tf.string), \n    \n}\nIMAGE_SIZE = [512,512]\nEPOCHS = 20\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync ","0d52418a":"training_data = tf.io.gfile.glob(GCS_DS_PATH + \"\/tfrecords-jpeg-512x512\/train\/*.tfrec\")\nvalidation_data = tf.io.gfile.glob(GCS_DS_PATH + \"\/tfrecords-jpeg-512x512\/val\/*.tfrec\")\ntesting_data = tf.io.gfile.glob(GCS_DS_PATH + \"\/tfrecords-jpeg-512x512\/test\/*.tfrec\")\nNUM_CLASSES = 104\nNUM_TRAINING_IMAGES = 12753\nNUM_TEST_IMAGES = 7382\nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES \/\/ BATCH_SIZE","bde3315a":"def flip(x: tf.Tensor) -> tf.Tensor:\n    check = random.randint(0,9)\n    \n    if check < 5:\n        x = tf.image.random_flip_left_right(x)\n        return x\n    \n    x = tf.image.random_flip_up_down(x)\n\n    return x\n\n\n\ndef color(x: tf.Tensor) -> tf.Tensor:\n    x = tf.image.random_hue(x, 0.08)\n    x = tf.image.random_saturation(x, 0.6, 1.6)\n    x = tf.image.random_brightness(x, 0.05)\n    x = tf.image.random_contrast(x, 0.7, 1.3)\n    return x \n\n\n\ndef rotate(x: tf.Tensor) -> tf.Tensor:\n    return tf.image.rot90(x, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))\n\n\n\ndef shear(x: tf.Tensor) -> tf.Tensor:\n    check = random.randint(0 , 5)\n    \n    if check > 3:\n        x = tfa.image.shear_x(x , 0.2 , 0)\n        return x\n    \n    x = tfa.image.shear_y(x , 0.2 , 0)\n    \n    return x\n\ndef random_all(x:tf.Tensor) -> tf.Tensor:\n    x = tf.image.random_flip_left_right(x)\n    x = tf.image.random_flip_up_down(x)\n    x = tf.image.random_hue(x, 0.08)\n    x = tf.image.random_saturation(x, 0.6, 1.6)\n    x = tf.image.random_brightness(x, 0.05)\n    x = tf.image.random_contrast(x, 0.7, 1.3)\n    return tf.image.rot90(x, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))","2036a23b":"def augment_data(image , label):\n    \n    check = random.randint(0,9)\n    \n    if check == 1:\n        image = flip(image)\n        \n    elif check == 2:\n        image = color(image)\n    \n    elif check == 3:\n        image = rotate(image)\n    \n    elif check == 4:\n        image = shear(image)\n    \n    elif check == 5:\n        image = random_all(image)\n    \n    return image , label","38c46758":"def decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0  \n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image","51ed5268":"def read_labeled_tfrecord(record):\n    record = tf.io.parse_single_example(record , LABELED_TFREC_FORMAT)\n    image = decode_image(record['image'])\n    label = tf.cast(record['class'] , tf.int32)\n    return image , label\n    ","1d5f04b8":"def read_unlabeled_tfrecord(record):\n    record = tf.io.parse_single_example(record , UNLABELED_TFREC_FORMAT)\n    image = decode_image(record['image'])\n    id_num = record['id'] \n    return image , id_num","77962a76":"def load_dataset(filenames , labeled=True , ordered = False):\n    if not ordered:\n        ignore_order.experimental_deterministic = False\n    \n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord)\n    return dataset","464e8c0e":"def load_augmented_dataset(filenames , labeled=True , ordered = False):\n    if not ordered:\n        ignore_order.experimental_deterministic = False\n    \n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord)\n    dataset = dataset.map(augment_data)\n    return dataset","04b7a88b":"def get_training_dataset():\n    dataset = load_dataset(training_data , labeled = True , ordered = False)\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    return dataset","25d93e4f":"def get_augmented_dataset():\n    dataset = load_augmented_dataset(training_data , labeled = True , ordered = False)\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    return dataset","84afa9c6":"def get_validation_dataset():\n    dataset = load_dataset(validation_data , labeled = True , ordered = False)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    return dataset ","43f89509":"def get_test_dataset(ordered = False):\n    dataset = load_dataset(testing_data , labeled = False , ordered = ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    return dataset ","92f344e6":"training_dataset = get_training_dataset()\nvalidation_dataset = get_validation_dataset()","68d8caed":"!pip install efficientnet","5539a0f0":"import efficientnet.tfkeras as efn\n","40f559d0":"callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss' , patience = 4)\n\nlearning_rate_start =  0.00001\n\nlearning_rate_max = 0.00005 * strategy.num_replicas_in_sync\n\nlearning_rate_min = 0.0001\n\nlearning_rate_boost_epochs = 3\n\nlearning_rate_sustain_epochs = 0 \n\nlearning_rate_decay = 0.9\n\ndef learning_rate_schedule(epoch):\n    if epoch < learning_rate_boost_epochs:\n        \n        lr = (learning_rate_max - learning_rate_start) \/ learning_rate_boost_epochs * epoch + learning_rate_start\n        \n    elif epoch < learning_rate_boost_epochs + learning_rate_sustain_epochs:\n        \n        lr = learning_rate_max\n        \n    else:\n        \n        lr = (learning_rate_max - learning_rate_min) * learning_rate_decay **(epoch - learning_rate_boost_epochs - learning_rate_sustain_epochs) + learning_rate_min\n        \n    return lr\n\n\nlearning_rate_callback = tf.keras.callbacks.LearningRateScheduler(learning_rate_schedule , verbose = True)\n","364fc961":"def create_model():\n    with strategy.scope():\n        input_layer = tf.keras.layers.Input(shape = (*IMAGE_SIZE,3))\n\n        pretrained_model = efn.EfficientNetB7(include_top = False , weights = 'noisy-student' , input_shape = (*IMAGE_SIZE,3) , input_tensor = input_layer , pooling='avg')   \n\n        for layer in pretrained_model.layers:\n            layer.trainable = True\n\n        X = tf.keras.layers.Dropout(0.2)(pretrained_model.layers[-1].output)\n\n        X = tf.keras.layers.Dense(NUM_CLASSES , activation = 'softmax' , dtype= 'float32')(X)\n\n\n        model = tf.keras.Model(inputs = input_layer , outputs = X)\n\n        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001,\n        beta_1=0.9,\n        beta_2=0.999,\n        epsilon=1e-07,\n        amsgrad=False)\n\n        loss = tf.keras.losses.SparseCategoricalCrossentropy()\n\n        model.compile(loss = loss , optimizer = optimizer , metrics=['sparse_categorical_accuracy'])\n        \n        return model","3bb92c65":"model = create_model()\nmodel.summary()","ebf98291":"model.fit(training_dataset , epochs = EPOCHS , validation_data = validation_dataset , steps_per_epoch = STEPS_PER_EPOCH , callbacks = [learning_rate_callback] )\n","74f3ec58":"augmented_dataset = get_augmented_dataset()","7ed224e7":"model.fit(augmented_dataset , epochs = EPOCHS , validation_data = validation_dataset , steps_per_epoch = STEPS_PER_EPOCH , callbacks = [learning_rate_callback] )","2ee47173":"test_dataset = get_test_dataset(ordered=True)","a8e98c53":"print(\"Predicting\")\ntest_images_ds = test_dataset.map(lambda image , idnum : image)\nprob = model.predict(test_images_ds)\npred = np.argmax(prob , axis = -1)\nprint(pred)","b54066d0":"print(\"Generating Csv\")\ntest_ids_ds = test_dataset.map(lambda image , idnum : idnum).unbatch()\ntest_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U')\nnp.savetxt('submission.csv' , np.rec.fromarrays([test_ids , pred]), fmt=['%s' , '%d'] , delimiter=',',header='id,label' , comments='')","9c479096":"!head submission.csv","25b7ede1":"# **Loading Testing Data**","bb84b4c6":"# **Converting Tensor To jpeg Images So It Can Be Processed And Augmented**","3d3557ab":"# Importing EfficientNet","89aa56b8":"# **Importing path for kaggle dataset**","ff19f0e8":"# Mapping the available dataset to labeled and unlabeled ","05763ba3":"# Convert training dataset into batches","d1068b52":"# **Reading Unlabeled tfrec**","700d61df":"Note : Importing directly from the kaggle input dir didn't worked for me on TPU's . Was stuck for sometime ","32fc4cb2":"# **Initialize the strategy for TPU if available**","1d64ed49":"# Getting dataset to be predicted","28184abe":"# **Importing The Binary or TFREC**","5461ed7e":"# **Generating CSV For Submission**","fae3bd35":"# **Training With Augmented Data**","9004f676":"# **Training Without Augmented Data As WarmUp**","d1ff2761":"# **Reading Labeled tfrec**","bd4a7a21":"# **Mapping data to be augmented**","fc19da9b":"# **Data Augmentation**","09617831":"Note : Here it randomly augments the data. Augmenting here for some reason didnt converse loss into global minimum. So commented it out","00e278c0":"**Callbacks Used**\n* Early Stopping so the model doesnt overfits \n* Learning Rate Scheduling as constant learning rate didn't give much accuracy ","456efe29":"# **Getting Augmented Data**","c2ae183f":"# **Some Data Augmentation (flip , rotate , color shifting)**","38facc88":"> ****Note :  I experimented with different models like VGG-16 , VGG-19 , ResNet50 , NASNetLarge , Xception  but the best result came from efficientnetB7**","2feeecd3":"# **Since TensorFlow doesn't provides this classification model yet. We need to install it from third party or PIP**","c74aee1e":"# **Converting dev\/validation dataset into batches**","64c89319":"# **Defining Required Variables And Data Format**","caf02d8c":"# **Installing Tensorflow Addons For Data Augmentation**","9f2ac1e4":"# **Data Augment**","c32fd28c":"# **IMPORT REQUIRED LIBRARIES**","7fee3c0d":"# **Defining Model and Callbacks **","58254b22":"# **Predictions**"}}