{"cell_type":{"1099101a":"code","b1f77417":"code","0de86cf6":"code","3ef6a3b7":"code","8ad55bf5":"code","093b9ff7":"code","d7993910":"code","072b1e99":"code","8054dc44":"code","7cdc31f9":"code","1924d1d0":"code","2a4752c1":"code","1b6e57f0":"code","1cdb3530":"code","6554064c":"code","80491693":"code","a682c6fe":"code","ed252772":"code","36d83b6f":"code","401ec73b":"code","49f6ea72":"code","cc874384":"code","eecf7af7":"code","ea05eef9":"code","ac6b2af0":"markdown","b1df1589":"markdown","8f65d1cd":"markdown","1d292282":"markdown","17e77f2b":"markdown","ff2850e7":"markdown"},"source":{"1099101a":"import gc\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tqdm import tqdm\n\nkaggle_kernel = True","b1f77417":"!pip install faiss-cpu\n!pip install adabelief-tf","0de86cf6":"import faiss\nfrom adabelief_tf import AdaBeliefOptimizer","3ef6a3b7":"glove = pd.read_pickle(\"..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl\")","8ad55bf5":"keys = []\nX = []\nfor k, v in tqdm(glove.items()):\n    if len(v) != 300:\n        print(k)\n        continue\n    keys.append(k)\n    X.append(v)\nX = np.array(X, dtype=np.float32)\nX = tf.nn.l2_normalize(X, axis=1).numpy()\n\ndel glove\ngc.collect()","093b9ff7":"from sklearn.model_selection import train_test_split\nX_train, X_test = train_test_split(X, train_size=2000000, random_state=2021)\nprint(X_train.shape)\nprint(X_test.shape)","d7993910":"if kaggle_kernel:\n    X_test = X_test[:20000]\ndel X\ngc.collect()","072b1e99":"import faiss\n\nindex = faiss.IndexFlatIP(300)\nindex.add(X_train)\n\nanswers = []\nbatch_size = 1024\nfor i in tqdm(range(-(-len(X_test)\/\/batch_size))):\n    _, I = index.search(X_test[batch_size*i:batch_size*(i+1)], 10)\n    answers.append(I)\nanswers = np.vstack(answers)","8054dc44":"hash_size = 256\nglove_dim = 300","7cdc31f9":"class LSH():\n    def __init__(self, centroids, d=8):\n        self.centroids = centroids\n        self.d = d\n        \n        self.out_dtype = f\"uint{d}\"\n        self.n_clusters = centroids.shape[0]\n        self.n_dim = centroids.shape[1]\n        \n        self.bitnum = tf.bitwise.left_shift(tf.ones(self.d, dtype=self.out_dtype), tf.cast(tf.range(self.d), self.out_dtype))\n        self.bitnum = tf.reshape(self.bitnum, (1, self.d))\n        self.zeros = tf.zeros((1, self.d), dtype=self.out_dtype)\n        \n        self.split_size = []\n        s = self.n_clusters * 1\n        for i in range(-(-self.n_clusters\/\/self.d)):\n            self.split_size.append(min(s, self.d))\n            s = max(s-self.d, 0)\n    \n\n    def transform(self, X):\n        cos = tf.reshape(tf.matmul(tf.expand_dims(self.centroids, axis=0), tf.expand_dims(X, axis=1), transpose_b=True), (len(X), -1))\n\n        res = []\n        for c, s in zip(tf.split(cos, self.split_size, axis=1), self.split_size):\n            c = tf.concat([c, tf.zeros((len(X), self.d-s))], axis=1)\n            res.append(tf.reduce_sum(tf.where(c > .0, self.bitnum, self.zeros), axis=1))\n        return tf.stack(res, axis=1)","1924d1d0":"lsh = LSH(tf.cast(tf.nn.l2_normalize(np.random.randn(hash_size, glove_dim), axis=1), \"float32\"))\nX_train_hashed = []\nbatch_size = 2**14\nfor i in tqdm(range(-(-len(X_train)\/\/batch_size))):\n    X_train_hashed.append(lsh.transform(X_train[batch_size*i:batch_size*(i+1)]).numpy())\nX_train_hashed = np.vstack(X_train_hashed)\nX_test_hashed = lsh.transform(X_test).numpy()","2a4752c1":"# usually IndexBinaryFlat is x8 faster on my PC but it doesn't works on kaggle kernel\n# it takes 3min for 0.2M saple on my PC(Intel Core-i5 8400)\nbindex = faiss.IndexBinaryFlat(hash_size)\nbindex.add(X_train_hashed)\npred = []\nbatch_size = 1024\nfor i in tqdm(range(-(-len(X_test)\/\/batch_size))):\n    X_t = X_test_hashed[batch_size*i:batch_size*(i+1)]\n    D, I = bindex.search(X_t, k=1000)\n    pred.append(I)\npred = np.vstack(pred)","1b6e57f0":"#recall @ 10\nnp.mean(np.any(answers[:, :1] == pred[:, :10], axis=1))","1cdb3530":"#recall @ 100\nnp.mean(np.any(answers[:, :1] == pred[:, :100], axis=1))","6554064c":"#recall @ 1000\nnp.mean(np.any(answers[:, :1] == pred[:, :1000], axis=1))","80491693":"# anisotoropic loss: https:\/\/arxiv.org\/abs\/1908.10396\n# k-means Hashing: https:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.306.7826&rep=rep1&type=pdf","a682c6fe":"class AnisotoropicIPQuantizer(tf.keras.Model):\n    def __init__(self, d=2, b=5, h=5., l=0.1, beta=0.9):\n        super(AnisotoropicIPQuantizer, self).__init__()\n        self.d = d\n        self.b = b\n        self.n_cluster = 2**b\n        self.h = h\n        self.beta = beta\n        \n        self.lambda_ = tf.Variable(tf.constant(l, dtype=\"float32\"), trainable=False)\n        \n        self.w = tf.Variable(tf.ones((self.n_cluster, self.n_cluster), dtype=\"float32\"), trainable=False)\n        \n        \n        self.query_ids = tf.Variable(tf.range(self.n_cluster, dtype=\"int32\"), trainable=False)\n        self.key_ids = tf.Variable(tf.range(self.n_cluster, dtype=\"int32\"), trainable=False)\n        \n        self.query_centroids = tf.Variable(tf.random.normal((self.n_cluster, self.d), dtype=\"float32\"), trainable=True)\n        self.key_centroids = tf.Variable(tf.random.normal((self.n_cluster, self.d), dtype=\"float32\"), trainable=True)\n    \n    def reset_centroid(self, mean, std):\n        self.query_centroids.assign(tf.random.normal((self.n_cluster, self.d), mean=mean, stddev=std, dtype=\"float32\"))\n        self.key_centroids.assign(tf.random.normal((self.n_cluster, self.d), mean=mean, stddev=std, dtype=\"float32\"))\n    \n    def init_centroid(self, X):\n        for s in range(2):\n            new_centroids = []\n            choosed = tf.zeros(len(X), dtype=\"bool\")\n\n            ix = tf.cast(tf.random.uniform([1], maxval=len(X))\/\/1, \"int32\")[0]\n            choosed = tf.where(tf.range(len(X)) == ix, True, choosed)\n            new_centroids.append(X[ix])\n\n            X_p = tf.reshape(X, (len(X), 1, self.d))\n\n            for i in range(self.n_cluster-1):\n                C = tf.expand_dims(X[ix], axis=0)\n                R_p = tf.matmul((X_p - C), X_p, transpose_b=True) \/ tf.norm(X_p, axis=2, keepdims=True) * X_p\n                R_o = (X_p - C) - R_p\n                dist = tf.math.sqrt(self.h * tf.pow(tf.norm(R_p, axis=2), 2) + tf.pow(tf.norm(R_o, axis=2), 2))\n                dist = tf.maximum(tf.where(tf.expand_dims(choosed, axis=0), 0, tf.reshape(dist, (1, -1))), tf.keras.backend.epsilon())\n                ix = tf.cast(tf.random.categorical(tf.math.log(dist\/tf.reduce_sum(dist, keepdims=True)), 1)[0][0], \"int32\")\n                choosed = tf.where(tf.range(len(X)) == ix, True, choosed)\n                new_centroids.append(X[ix])\n            \n            if s:\n                self.query_centroids.assign(tf.stack(new_centroids, axis=0))\n            else:\n                self.key_centroids.assign(tf.stack(new_centroids, axis=0))\n        \n    @tf.function(experimental_relax_shapes=True)\n    def partial_fit(self, X):\n        X_p = tf.expand_dims(X, axis=1)\n        C_q = tf.expand_dims(self.query_centroids, axis=0)\n        R_q_p = tf.matmul((X_p - C_q), X_p, transpose_b=True) \/ tf.norm(X_p, axis=2, keepdims=True) * X_p\n        R_q_o = (X_p - C_q) - R_q_p\n\n        C_k = tf.expand_dims(self.key_centroids, axis=0)\n        R_k_p = tf.matmul((X_p - C_k), X_p, transpose_b=True) \/ tf.norm(X_p, axis=2, keepdims=True) * X_p\n        R_k_o = (X_p - C_k) - R_k_p\n\n\n        query_loss = self.h * tf.pow(tf.norm(R_q_p, axis=2), 2) + tf.pow(tf.norm(R_q_o, axis=2), 2)\n        cqix = tf.stop_gradient(tf.math.argmin(query_loss, axis=1))\n\n        key_loss = self.h * tf.pow(tf.norm(R_k_p, axis=2), 2) + tf.pow(tf.norm(R_k_o, axis=2), 2)\n        ckix = tf.stop_gradient(tf.math.argmin(key_loss, axis=1))\n\n        cqmem = tf.expand_dims(tf.math.unsorted_segment_sum(tf.ones(len(X_p)), cqix, self.n_cluster), axis=1)\n        ckmem = tf.expand_dims(tf.math.unsorted_segment_sum(tf.ones(len(X_p)), ckix, self.n_cluster), axis=0)\n        E_quan = (tf.reduce_mean(tf.gather_nd(query_loss, tf.stack([tf.range(len(X_p), dtype=\"int64\"), cqix], axis=1)))\n                  + tf.reduce_mean(tf.gather_nd(key_loss, tf.stack([tf.range(len(X_p), dtype=\"int64\"), ckix], axis=1))))\n\n        w = cqmem * ckmem\n        w = w \/ tf.reduce_mean(w)\n        self.w.assign(self.w * self.beta + (1. - self.beta) * w)\n        ip = tf.matmul(self.query_centroids, self.key_centroids, transpose_b=True)\n        hd = tf.cast(tf.raw_ops.PopulationCount(x=tf.bitwise.bitwise_xor(tf.reshape(self.query_ids, (-1, 1)), tf.reshape(self.key_ids, (1, -1)))), \"float32\")\n\n        x_hat = tf.reduce_mean(ip * self.w)\n        y_hat = tf.reduce_mean(hd * self.w)\n        \n        alpha = tf.reduce_sum((hd - y_hat)*(ip - x_hat)) \/ tf.reduce_sum((ip - x_hat)*(ip - x_hat))\n        alpha = tf.where(alpha > 0, -alpha, alpha)\n        beta = y_hat - alpha*x_hat\n\n        E_aff = tf.reduce_mean(tf.pow(hd - ip*alpha - beta, 2.) * self.w)\n\n        E = E_quan + E_aff * self.lambda_\n        return E, E_quan, E_aff, w, ip\n    \n    def query_transform(self, X):\n        X_p = tf.expand_dims(X, axis=1)\n        C = tf.expand_dims(self.query_centroids, axis=0)\n        R_p = tf.matmul((X_p - C), X_p, transpose_b=True) \/ tf.norm(X_p, axis=2, keepdims=True) * X_p\n        R_o = (X_p - C) - R_p\n\n        all_loss = self.h * tf.pow(tf.norm(R_p, axis=2), 2) + tf.pow(tf.norm(R_o, axis=2), 2)\n        cix = tf.gather(self.query_ids, tf.cast(tf.math.argmin(all_loss, axis=1), \"int32\"))\n        cmem = tf.expand_dims(tf.math.unsorted_segment_sum(tf.ones(len(X_p)), cix, self.n_cluster), axis=1)\n        \n        res = []\n        for i in tf.range(self.b):\n            res.append(tf.cast(tf.bitwise.bitwise_and(tf.bitwise.right_shift(cix, i), 1), \"uint8\"))\n        return tf.stack(res, axis=1), cmem\n    \n    def key_transform(self, X):\n        X_p = tf.expand_dims(X, axis=1)\n        C = tf.expand_dims(self.key_centroids, axis=0)\n        R_p = tf.matmul((X_p - C), X_p, transpose_b=True) \/ tf.norm(X_p, axis=2, keepdims=True) * X_p\n        R_o = (X_p - C) - R_p\n\n        all_loss = self.h * tf.pow(tf.norm(R_p, axis=2), 2) + tf.pow(tf.norm(R_o, axis=2), 2)\n        cix = tf.gather(self.key_ids, tf.cast(tf.math.argmin(all_loss, axis=1), \"int32\"))\n        cmem = tf.expand_dims(tf.math.unsorted_segment_sum(tf.ones(len(X_p)), cix, self.n_cluster), axis=1)\n        \n        res = []\n        for i in tf.range(self.b):\n            res.append(tf.cast(tf.bitwise.bitwise_and(tf.bitwise.right_shift(cix, i), 1), \"uint8\"))\n        return tf.stack(res, axis=1), cmem","ed252772":"class KMeansHashEncoder():\n    def __init__(self, n_dim=300, d=6, b=5, h=5., l=1.):\n        self.quantizers = [AnisotoropicIPQuantizer(d=d, b=b, h=h, l=l) for _ in range(n_dim\/\/d)]\n\n        self.n_dim = n_dim\n        self.d = d\n        self.b = b\n        self.h = h\n        self.l = l\n        \n    def train(self, X, optimizer_class=tf.keras.optimizers.SGD, optimizer_params={\"lr\": 0.1}, batch_size=2**16, train_epochs=30, lambdas=None):\n        for i, quantizer in tqdm(enumerate(self.quantizers), total=self.n_dim\/\/self.d):\n            X_p = X[:, self.d*i:self.d*(i+1)]\n            optimizer = optimizer_class(**optimizer_params)\n            \n            @tf.function\n            def train_step(X, quantizer, optimizer):\n                with tf.GradientTape() as tape:\n                    loss, _, _, _, _ = quantizer.partial_fit(X)\n                gradients = tape.gradient(loss, quantizer.trainable_variables)\n                gradients = [tf.clip_by_norm(g, 10) for g in gradients]\n                if tf.reduce_all([tf.reduce_all(tf.math.is_finite(grad)) for grad in gradients]):\n                    optimizer.apply_gradients(zip(gradients, quantizer.trainable_variables))\n                else:\n                    tf.print(\"contain nan or error\")    \n                return loss\n            quantizer.init_centroid(X_p)\n            steps = -(-len(X_p)\/\/batch_size)\n            if lambdas is None:\n                lambdas = np.ones(train_epochs * steps) * self.l\n            elif type(lambdas) == int:\n                lambdas = np.ones(train_epochs * steps) * lambdas\n            else:\n                assert len(lambdas) == train_epochs * steps\n            \n            for epoch in range(train_epochs):\n                ixs = np.arange(len(X_p))\n                np.random.shuffle(ixs)\n                for i in range(steps):\n                    quantizer.lambda_.assign(lambdas[epoch*steps + i])\n                    X_batch = X_p[ixs[batch_size*i:batch_size*(i+1)]]\n                    loss = train_step(X_batch, quantizer, optimizer) \n\n    def query_transform(self, X):\n        res = []\n        for i in range(self.n_dim \/\/ self.d):\n            res.append(self.quantizers[i].query_transform(X[:, self.d*i:self.d*(i+1)])[0])\n        res = tf.concat(res, axis=1).numpy()\n        X_hashed = []\n        tdim = self.n_dim\/\/self.d*self.b\n        size = -(-tdim\/\/128) * 128 \/\/ 8\n        for i in range(size):\n            r = res[:, 8*i:8*(i+1)]\n            X_hashed.append(np.sum(r * np.power(2, np.arange(r.shape[1])).reshape(1, -1), axis=1, keepdims=True).astype(np.uint8))\n        X_hashed = np.concatenate(X_hashed, axis=1)\n        return X_hashed\n    \n    def key_transform(self, X):\n        res = []\n        for i in range(self.n_dim \/\/ self.d):\n            res.append(self.quantizers[i].key_transform(X[:, self.d*i:self.d*(i+1)])[0])\n        res = tf.concat(res, axis=1).numpy()\n        X_hashed = []\n        tdim = self.n_dim\/\/self.d*self.b\n        size = -(-tdim\/\/128) * 128 \/\/ 8\n        for i in range(size):\n            r = res[:, 8*i:8*(i+1)]\n            X_hashed.append(np.sum(r * np.power(2, np.arange(r.shape[1])).reshape(1, -1), axis=1, keepdims=True).astype(np.uint8))\n        X_hashed = np.concatenate(X_hashed, axis=1)\n        return X_hashed","36d83b6f":"from adabelief_tf import AdaBeliefOptimizer\nbatch_size = 2**16\ntrain_epochs = 30\nsteps = -(-len(X_train)\/\/batch_size)\nlambdas = np.exp(np.linspace(np.log(1.), np.log(.01), train_epochs * steps))\n\nencoder = KMeansHashEncoder(n_dim=glove_dim, d=6, b=5, h=5., l=1.)\nencoder.train(X_train, AdaBeliefOptimizer, {\"lr\":1e-1, \"print_change_log\":False}, batch_size=batch_size, train_epochs=train_epochs, lambdas=lambdas)","401ec73b":"batch_size = 2**14\nX_train_hashed = []\nfor i in tqdm(range(-(-len(X_train)\/\/batch_size))):\n    X_train_hashed.append(encoder.key_transform(X_train[batch_size*i:batch_size*(i+1)]))\nX_train_hashed = np.concatenate(X_train_hashed, axis=0)\nX_test_hashed = encoder.query_transform(X_test)","49f6ea72":"bindex = faiss.IndexBinaryFlat(hash_size)\nbindex.add(X_train_hashed)\npred = []\nbatch_size = 1024\nfor i in tqdm(range(-(-len(X_test)\/\/batch_size))):\n    X_t = X_test_hashed[batch_size*i:batch_size*(i+1)]\n    D, I = bindex.search(X_t, k=1000)\n    pred.append(I)\npred = np.vstack(pred)","cc874384":"#recall @ 10\nnp.mean(np.any(answers[:, :1] == pred[:, :10], axis=1))","eecf7af7":"#recall @ 100\nnp.mean(np.any(answers[:, :1] == pred[:, :100], axis=1))","ea05eef9":"#recall @ 1000\nnp.mean(np.any(answers[:, :1] == pred[:, :1000], axis=1))","ac6b2af0":"## Naive LSH","b1df1589":"## make dataset","8f65d1cd":"# preparation","1d292282":"## Anisotoropic K-means Hashing","17e77f2b":"# Model","ff2850e7":"## create answer"}}