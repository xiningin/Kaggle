{"cell_type":{"5d0ca2cb":"code","548353b1":"code","18863f23":"code","86fdd7d7":"code","38c39db2":"code","ab3842ab":"code","27e0b56e":"code","7c10658c":"code","a6a18314":"code","cf7fbd19":"code","502cd26f":"code","0965e829":"code","af238b64":"code","060e0b86":"code","b9689d6c":"code","5bc23646":"code","883f0fe2":"code","44ce0ec4":"code","16c7efde":"code","8bb02d69":"code","d5ead0ea":"code","163b4257":"markdown","76f274ae":"markdown","dd65b37b":"markdown","d2a31ec8":"markdown","98b9c534":"markdown","8a815185":"markdown","a564f570":"markdown","0e8fdbd9":"markdown","bce26196":"markdown","6acb9de2":"markdown"},"source":{"5d0ca2cb":"# Library\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport librosa\nimport librosa.display\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport tensorflow.keras.models as models\nimport tensorflow.keras.layers as layers\nimport IPython\nimport sklearn\nimport seaborn as sns\nfrom sklearn.utils import shuffle\n\n%load_ext tensorboard","548353b1":"#This notebook use ESC-50 datasets on Kaggle Kernel, follow the step on README for further instructions.\nCSV_FILE_PATH = \"..\/input\/environmental-sound-classification-50\/esc50.csv\"  # path of csv file\nDATA_PATH = \"..\/input\/environmental-sound-classification-50\/audio\/audio\/16000\/\" # path to folder containing audio files\n# Reading the CSV File\ndf = pd.read_csv(CSV_FILE_PATH)\ndf.head()","18863f23":"# We are going to use only 10 classes for early development\ndf_10 = df[df['esc10']==True]\ndf_10 = df_10.drop(['esc10','src_file','take'], axis=1)\n\n# We need to convert the classes into numbers so the model could process it\nclasses = df_10['category'].unique()\nclass_dict = {i:x for x,i in enumerate(classes)}\nprint(class_dict)\ndf_10['target'] = df_10['category'].map(class_dict)\n\n# Creating sample of each classes\nsample_df = df_10.drop_duplicates(subset=['target'])\nsample_df = sample_df.drop(sample_df.index[4:10])\nprint(sample_df.head())","86fdd7d7":"# Class Conf will save the settings we are going to use in this notebook\nclass conf:\n    sr = 16000\n    duration = 3\n    hop_length = 340*duration\n    fmin = 20\n    fmax = sr \/\/ 2\n    n_mels = 128\n    n_fft = n_mels * 20\n    samples = sr * duration\n    epochs = 30\n\ndef read_audio(conf, pathname, trim_long_data):\n    y, sr = librosa.load(pathname, sr=conf.sr)\n    # trim silence\n    if 0 < len(y): # workaround: 0 length causes error\n        y, _ = librosa.effects.trim(y) # trim, top_db=default(60)\n    # make it unified length to conf.samples\n    if len(y) > conf.samples: # long enough\n        if trim_long_data:\n            y = y[0:0+conf.samples]\n    else: # pad blank\n        padding = conf.samples - len(y)    # add padding at both ends\n        offset = padding \/\/ 2\n        y = np.pad(y, (offset, conf.samples - len(y) - offset), 'constant')\n    return y\n\ndef audio_to_melspectrogram(conf, audio):\n    spectrogram = librosa.feature.melspectrogram(audio, \n                                                 sr=conf.sr,\n                                                 n_mels=conf.n_mels,\n                                                 hop_length=conf.hop_length,\n                                                 n_fft=conf.n_fft,\n                                                 fmin=conf.fmin,\n                                                 fmax=conf.fmax)\n    spectrogram = librosa.power_to_db(spectrogram)\n    return spectrogram\n\ndef show_melspectrogram(conf, mels, title='Log-frequency power spectrogram'):\n    librosa.display.specshow(mels, x_axis='time', y_axis='mel', \n                             sr=conf.sr, hop_length=conf.hop_length,\n                            fmin=conf.fmin, fmax=conf.fmax)\n    plt.colorbar(format='%+2.0f dB')\n    plt.title(title)\n    plt.show()","38c39db2":"# Visualization of Soundwave\nfig, ax = plt.subplots(4, figsize = (8, 6))\nfig.suptitle('Sound Waves', fontsize=16)\ncolor = ['#A300F9', '#4300FF', '#009DFF', '#00FFB0']\ni=0\nfor row in sample_df.itertuples(): \n    signal , rate = librosa.load(DATA_PATH+ row[1], sr=conf.sr)\n    librosa.display.waveplot(y = signal, sr = rate, color = color[i], ax=ax[i])\n    ax[i].set_ylabel(classes[row[3]], fontsize=13)\n    i +=1","ab3842ab":"# Visualization of Mel Spectogram\nfig, ax = plt.subplots(4, figsize = (8, 6))\nfig.suptitle('Mel Spectogram', fontsize=16)\ni=0\nfor row in sample_df.itertuples(): \n    signal , rate = librosa.load(DATA_PATH+ row[1], sr=conf.sr)\n    mel_spec = audio_to_melspectrogram(conf, signal)\n    librosa.display.specshow(mel_spec, sr = conf.sr, hop_length = conf.hop_length, x_axis = 'time', \n                         fmin=conf.fmin, fmax=conf.fmax, y_axis = 'mel', ax=ax[i])\n    ax[i].set_ylabel(row[3], fontsize=13)\n    i +=1","27e0b56e":"# Visualization of MFCC Plot\nfig, ax = plt.subplots(4, figsize = (8, 5))\nfig.suptitle('MFCC', fontsize=16)\ni=0\nfor row in sample_df.itertuples(): \n    signal , rate = librosa.load(DATA_PATH+ row[1], sr=16000)\n    mfcc = librosa.feature.mfcc(signal , rate , n_mfcc=13, dct_type=3)\n    ax[i].imshow(mfcc, cmap='rainbow',interpolation='nearest')\n    ax[i].set_ylabel(row[3], fontsize=13)\n    i +=1\n\nprint(np.array(mfcc).shape)","7c10658c":"INPUTSHAPE = (128, 32, 1)\ndef create_model():\n    created_model =  models.Sequential([\n        layers.Conv2D(64 , (3,3),activation = 'relu',padding='same', input_shape = INPUTSHAPE),\n        layers.BatchNormalization(),\n        layers.Conv2D(64, (3,3), activation='relu',padding='same'),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D((2,2), strides=(2,2)),\n        layers.Dropout(0.2),\n\n        layers.Conv2D(128, (3,3), activation='relu',padding='same'),                      \n        layers.BatchNormalization(),\n        layers.Conv2D(128, (3,3), activation='relu',padding='same'),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D((2,2), strides=(2,2)),\n        layers.Dropout(0.2),\n\n        layers.Conv2D(256, (3,3), activation='relu',padding='same'),\n        layers.BatchNormalization(),\n        layers.Conv2D(256, (3,3), activation='relu',padding='same'),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D((2,2), strides=(2,2)),    \n        layers.Dropout(0.2),\n\n        layers.GlobalAveragePooling2D(),\n\n        layers.Dense(256 , activation = 'relu'),\n        layers.Dense(256 , activation = 'relu'),\n        layers.Dense(len(classes) , activation = 'softmax')\n    ])\n\n    created_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics=['acc'])\n    return created_model","a6a18314":"# Our model summary\nmodel = create_model()\nprint(model.summary())","cf7fbd19":"%mkdir \"cpkt\"\n%mkdir \"logs\"\nLOGDIR = \"logs\"\nCPKT = \"cpkt\/\"\n\n#this callback is used to prevent overfitting.\ncallback_1 = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto',\n    baseline=None, restore_best_weights=False\n)\n\n#this checkpoint saves the best weights of model at every epoch\ncallback_2 = tf.keras.callbacks.ModelCheckpoint(\n    CPKT, monitor='val_loss', verbose=0, save_best_only=True,\n    save_weights_only=True, mode='auto', save_freq='epoch', options=None\n)\n\n#this is for tensorboard\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOGDIR)","502cd26f":"# This function will create our training and validation dataset.\n# The fold number will be used as our validation data, and the rest as the training data.\ndef preprocess(fold):\n    x_train , y_train = [] , []\n    x_val , y_val = [] , []\n    \n    train_df = df_10[df_10.fold != fold]\n    val_df = df_10[df_10.fold == fold]\n    \n    for data in train_df.itertuples():\n        sig , sr = librosa.load(DATA_PATH+data[1], sr=16000)\n        #Creating three random 2 second clip from each audio file, to create more samples\n        for i in range(4):\n            sig_ = sig[i : int((i+2)+sr)]\n            mel_spec = audio_to_melspectrogram(conf, sig_)\n            x_train.append(mel_spec)\n            y_train.append(data[3])\n\n    for data in val_df.itertuples():\n        sig , sr = librosa.load(DATA_PATH+data[1], sr=16000)\n        #Creating three random 2 second clip from each audio file, to create more samples\n        for i in range(4):\n            sig_ = sig[i : int((i+2)+sr)]\n            mel_spec = audio_to_melspectrogram(conf, sig_)\n            x_val.append(mel_spec)\n            y_val.append(data[3])\n            \n    # convert list to numpy array\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n    \n    x_val = np.array(x_val)\n    y_val = np.array(y_val)\n\n    #one-hot encoding the target\n    y_train = tf.keras.utils.to_categorical(y_train , num_classes=len(classes))\n    y_val = tf.keras.utils.to_categorical(y_val , num_classes=len(classes))\n\n    # our tensorflow model takes input as (no_of_sample , height , width , channel).\n    # here X has dimension (no_of_sample , height , width).\n    # So, the below code will reshape it to (no_of_sample , height , width , 1).\n    x_train, y_train = shuffle(x_train, y_train)\n    x_val, y_val = shuffle(x_val, y_val)\n    \n    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n    x_val = x_val.reshape(x_val.shape[0], x_val.shape[1], x_val.shape[2], 1)\n    \n    return (x_train, y_train, x_val, y_val)","0965e829":"num_folds = len(df_10.fold.unique())\n\n#save the model history in a list after fitting so that we can plot later\nmodel_history = []\nmetrics = []\n\n\n# The training section will use k-fold cross validation, just as suggested by the dataset creator.\n# Cross Validation training will make a more robust model and prevent bias on training.\nfor fold in range(1, num_folds+1):\n    print('\\n\\nTraining fold', fold)\n    print('*' * 20)\n    \n    x_train, y_train, x_val, y_val = preprocess(fold)\n    model = create_model()\n    history = model.fit(x_train,y_train ,\n            validation_data=(x_val,y_val),\n            epochs=conf.epochs,\n            callbacks = [callback_1], verbose=2)\n    eval_score = model.evaluate(x_val, y_val)\n    print(\"Val Score: \",eval_score )\n    model_history.append(history)\n    metrics.append(eval_score)","af238b64":"# Each fold will have this amount of data\nprint(\"Training on \", len(x_train),\" samples\")\nprint(\"Evaluating on \", len(x_val),\" samples\")\n\nsum_acc = 0\nsum_loss = 0\n\n# Evaluation Score\nprint(\"Results of training\")\nfor fold in range(num_folds):\n    print(\"Evaluation Score on Fold \", fold+1, \" :\", metrics[fold])\n    sum_acc += metrics[fold-1][1]\n    sum_loss += metrics[fold-1][0]\n\nprint(\"Average Eval Acc : \", sum_acc\/num_folds)\nprint(\"Average Loss Acc : \", sum_loss\/num_folds)","060e0b86":"color = ['black', 'red', 'green', 'blue', 'purple']\nplt.figure(figsize=(15,5))\nplt.title('Accuracies vs Epochs')\nfor fold in range(num_folds):\n    label_name_train = 'Train Accuracy Fold ' + str(fold)\n    label_name_val = 'Val Accuracy Fold ' + str(fold)\n    plt.plot(model_history[fold].history['acc'], label=label_name_train, color=color[fold])\n    #Uncomment to see Val Accuracy Plot\n#     plt.plot(model_history[fold].history['val_acc'], label=label_name_val, color=color[fold], linestyle = \"dashdot\")\n\nplt.legend()\nplt.show()","b9689d6c":"new_model = models.load_model('..\/input\/chainsaw-testing\/')","5bc23646":"# Creating a confusion matrix to see the error occured\ny_pred = new_model.predict(x_val)\nconfusion_matrix = sklearn.metrics.confusion_matrix(np.argmax(y_val, axis=1), np.argmax(y_pred, axis=1))\n\nax = plt.subplot()\nsns.heatmap(confusion_matrix, annot=True, cmap=\"YlGnBu\", ax=ax);\n\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(classes, rotation='vertical'); ax.yaxis.set_ticklabels(classes, rotation='horizontal');","883f0fe2":"# The audio we are using is this one\nimport IPython.display as ipd\nsig , sr = librosa.load('..\/input\/chainsaw-testing\/chainsaw-01.wav', sr=conf.sr)\nipd.display(ipd.Audio(sig, rate=sr))\nlibrosa.display.waveplot(y = sig, sr = sr)","44ce0ec4":"def split_audio(audio_data, w, h, threshold_level, tolerence=10):\n    split_map = []\n    start = 0\n    data = np.abs(audio_data)\n    threshold = threshold_level*np.mean(data[:25000])\n    inside_sound = False\n    near = 0\n    for i in range(0,len(data)-w, h):\n        win_mean = np.mean(data[i:i+w])\n        if(win_mean>threshold and not(inside_sound)):\n            inside_sound = True\n            start = i\n        if(win_mean<=threshold and inside_sound and near>tolerence):\n            inside_sound = False\n            near = 0\n            split_map.append([start, i])\n        if(inside_sound and win_mean<=threshold):\n            near += 1\n    return split_map","16c7efde":"# To identify the sounds in the audio, we are going to cut the soundwave into several parts\n# The clip will be clipped to it's highlight (noisiest) with certain interval\n\nsound_clips = split_audio(sig, 10000, 2500, 15, 10)\nduration = len(sig)\ni = 1\n\nfor intvl in sound_clips:\n    clip, index = librosa.effects.trim(sig[intvl[0]:intvl[1]],       \n                                       top_db=20, frame_length=512, hop_length=64)\n    mel_spec = audio_to_melspectrogram(conf, clip)\n    testing = np.array(mel_spec)\n    testing = testing.reshape(1, testing.shape[0], testing.shape[1], 1)\n    pred = new_model.predict(testing)\n    \n    blank = np.zeros(intvl[0]-0)\n    blank2 = np.zeros(duration-intvl[1])\n    temp = np.append(blank,clip)\n    temp = np.append(temp,blank2)\n    librosa.display.waveplot(y = temp, sr = sr, )\n    \n    print(\"Clip Number :\", i)\n    print(\"Interval from : \", intvl[0]\/16000, \" to \",intvl[1]\/16000, \"seconds\")\n    i += 1\n    if(pred.max() > 0.8):\n        print(\"Results : \", classes[np.argmax(pred)], \"\\n\")\n    else:\n        print(\"Results : Unknown\")\n        print(\"Confidence Level : \", pred)\n        print(\"Highest Confidence Level : \", classes[np.argmax(pred)], \" of \", np.max(pred)*100, \"%\\n\")\n        ipd.display(ipd.Audio(clip, rate=sr))","8bb02d69":"# Showing the Mel Spectogram that is passed to the model\n\nfig, ax = plt.subplots(5, figsize = (15, 10))\nfig.suptitle('Mel Spectogram', fontsize=16)\ni=0\nfor intvl in sound_clips:\n    clip, index = librosa.effects.trim(sig[intvl[0]:intvl[1]],       \n                                       top_db=20, frame_length=512, hop_length=64)\n    mel_spec = audio_to_melspectrogram(conf, clip)\n    librosa.display.specshow(mel_spec, sr = conf.sr, hop_length = conf.hop_length, x_axis = 'time', \n                         fmin=conf.fmin, fmax=conf.fmax, y_axis = 'mel', ax=ax[i])\n    i +=1","d5ead0ea":"# Uncomment code below to save the model to folder \"\/kaggle\/working\/export\"\n# The exported model will be on SavedModel Tensorflow format, which is the default for Tensorflow 2.% model\n\n# model.save(\"export\")","163b4257":"# <center> Visualization <\/center>","76f274ae":"# <center> Evaluation <\/center>","dd65b37b":"# <center> Modifying VGG16 Model <\/center>","d2a31ec8":"# <center> Bonus Section <\/center>\n#### <center> Unhide and uncomment the code section below to load my trained model <\/center> ","98b9c534":"# <center> Acquire Training and Testing Data <\/center>","8a815185":"# <center> Training<\/center>","a564f570":"#### More about Cross Validation : [Link](https:\/\/www.kaggle.com\/dansbecker\/cross-validation)","0e8fdbd9":"# <center> Saving Model <\/center>","bce26196":"#### There are some audio files I inserted to my datasets, let's see how the model identify sounds in the audio.","6acb9de2":"# Jagawana - Forest Logging Detection\n\n### \n<div class=\"alert alert-block alert-success\"> \ud83d\udccc This notebook is created for a capstone project, we are creating a Forest Logging Detection System to identify Chainsaws and Gunshots from forest ambiance sounds.<\/div>\n\n### Workflow Problem Definition\nForests are huge and the terrain is hard to pass through, on the other side, forest ranger usually comprises of only several people. Often, rangers are patrolling the forest area for 1\u20132 weeks in a month, which means there are many opportunities for illegal loggers to get in and out without any patrol. This gap hole could be prevented by incorporating technology for the ranger and forests.\n\nJagawana is a Wide Sensor Network System deployed in the forests to prevent Ilegal Logging. By using sensors to pick up voices in the forests, we could monitor what happened in the forest in real-time. We deployed a Machine Learning Model to process the sounds taken by the sensor, then the model will identify the sounds into various categories, such as chainsaws, trucks, gunshot, and burning sounds.\n   \n### Workflow Goals\nOur Machine Learning Model main goals is to **Classifying Forests Ambience Sounds**. We may want to classify or categorize sounds taken by our sensors. Our priority is to identify chainsaw sounds and alert users from Android App. Though categorizing other sounds is as important to map out fauna habitats, and for further research data.\n\n### Workflow Stages :\nThis notebook workflow goes through seven stages.\n1. Acquire training and testing data.\n2. Wrangle, prepare, cleanse the data.\n3. Analyze, identify patterns, and explore the data.\n4. Model, predict and solve the problem.\n5. Visualize, report, and present the problem solving steps and final solution.\n6. Exporting Models\n\n### Resources and References\n* We use ESC-50 datasets for early development, using VGG-16 Models as our baseline and slightly adjust it for audio classification.\n* Papers papers papers\n\n<div class=\"alert alert-block alert-warning\"> \ud83d\udccc This project is still on development, feel free to comment or contact me through link in profile.<\/div>"}}