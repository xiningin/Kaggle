{"cell_type":{"7346a661":"code","be99032f":"code","b0baecfe":"code","3d9df67e":"code","7db5d89e":"code","46b7c374":"code","bd6631fb":"code","7f6b1fef":"code","4cc41fd8":"code","371ee126":"code","7a544cae":"code","0e68744c":"code","ef59f8b0":"code","29428254":"code","80692d4e":"code","086d674e":"code","2a8eaa52":"code","b095ea02":"code","2935e0c4":"code","d00df026":"code","34864914":"code","6ec5302d":"code","a9453e25":"code","a8b8374e":"code","73c4021b":"code","9fe95a2c":"code","cd8d82fb":"code","2b281e90":"code","914e7171":"code","c0146a82":"code","fa172c24":"code","db26a07b":"code","e6968c30":"code","698aff40":"code","4b2f027b":"code","3b77f318":"code","163dc6dc":"code","d01df43f":"code","8b61b6bf":"code","4972951c":"code","d73ae747":"code","90f22187":"code","119c7096":"code","79f3de32":"code","9b83128d":"code","684267ac":"code","2223df94":"code","c0dbfc0d":"code","9c9f4e75":"code","f87d09b7":"code","dabca18a":"code","dce14276":"code","b6dbacf3":"code","896dd970":"code","72b5b856":"code","d58df199":"code","17527533":"code","eafde852":"code","9dcb7f2d":"code","fb4c2b19":"code","503c52b9":"code","3a767681":"code","de995fbd":"code","025505f0":"code","80655df8":"code","6e02d49b":"code","09f3cdc8":"code","4e272a87":"code","5a834898":"code","8b7390dc":"code","b35a6482":"code","300f3576":"code","23d757d5":"markdown","6f60f0ce":"markdown","ebe9c83e":"markdown","1e195e31":"markdown","c9169c5d":"markdown","4caf5b67":"markdown","4cb89c72":"markdown","32c8880b":"markdown","206dbfe3":"markdown","479bb154":"markdown","4654f4c9":"markdown","dc6f251a":"markdown","35b2627c":"markdown","0193f8a8":"markdown","00d901e6":"markdown","a392630e":"markdown","ab48e24d":"markdown","eab95394":"markdown","0705f221":"markdown","7431720c":"markdown","6beeab80":"markdown","f1b97c71":"markdown","f2bcbfbc":"markdown","c52633b5":"markdown","d25e9b1e":"markdown","d61cfc9f":"markdown","7d02262b":"markdown","a467f870":"markdown","f01aae7e":"markdown","7f08ab26":"markdown","a9320f4d":"markdown","669bec58":"markdown","0d595cab":"markdown","dfbae859":"markdown","0b27e4a8":"markdown","7fdcee6e":"markdown","8c93c5c4":"markdown","f8ca0e30":"markdown"},"source":{"7346a661":"import pandas as pd\nimport numpy as np\nimport datetime as dt\nimport os\nimport os.path\nfrom pathlib import Path\nimport glob\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, GlobalAveragePooling2D, SpatialDropout2D\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.xception import Xception\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.mobilenet import MobileNet\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.metrics import confusion_matrix, classification_report, recall_score, precision_score, f1_score, roc_auc_score, roc_curve\nfrom tensorflow.keras.preprocessing import image\nfrom PIL import Image","be99032f":"# Selecting folder paths in training_set\ndir_ = Path('..\/input\/dogs-cats-images\/dataset\/training_set')\ntrain_filepaths = list(dir_.glob(r'**\/*.jpg'))\n\n# Mapping labels...\nlabels_training = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], train_filepaths))\n\n# Training paths & labels\ntrain_filepaths = pd.Series(train_filepaths, name = 'File').astype(str)\ntrain_labels = pd.Series(labels_training, name='Label')\n\n# Concatenating...\ntrainset_df = pd.concat([train_filepaths, train_labels], axis=1)","b0baecfe":"# Selecting folder paths in test_set\ndir_ = Path('..\/input\/dogs-cats-images\/dataset\/test_set')\ntest_filepaths = list(dir_.glob(r'**\/*.jpg'))\n\n# Mapping labels...\nlabels_test = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], test_filepaths))\n\n# Test paths & labels\ntest_filepaths = pd.Series(test_filepaths, name = 'File').astype(str)\ntest_labels = pd.Series(labels_test, name='Label')\n\n# Concatenating...\ntestset_df = pd.concat([test_filepaths, test_labels], axis=1)","3d9df67e":"# Viewing data in both datasets\nprint('Training Dataset:')\n\nprint(f'Number of images in the training dataset: {trainset_df.shape[0]}')\n\nprint(f'Number of images with cats: {trainset_df[\"Label\"].value_counts()[0]}')\nprint(f'Number of images with dogs: {trainset_df[\"Label\"].value_counts()[1]}\\n')\n      \nprint('Test Dataset:')\n      \nprint(f'Number of images in the test dataset: {testset_df.shape[0]}')\n\nprint(f'Number of images with cats: {testset_df[\"Label\"].value_counts()[0]}')\nprint(f'Number of images with dogs: {testset_df[\"Label\"].value_counts()[1]}')","7db5d89e":"vc = trainset_df['Label'].value_counts()\nplt.figure(figsize = (9, 5))\nsns.barplot(x = vc.index, y = vc, palette = \"Set2\")\nplt.title(\"Number of images for each category in the Training Dataset\", fontsize = 11)\nplt.show()","46b7c374":"vc = testset_df['Label'].value_counts()\nplt.figure(figsize = (9, 5))\nsns.barplot(x = vc.index, y = vc, palette = \"Set2\")\nplt.title(\"Number of images from each category in the Test Dataset\", fontsize = 11)\nplt.show()","bd6631fb":"trainset_df = trainset_df.sample(frac = 1, random_state = 56).reset_index(drop = True)\ntestset_df = testset_df.sample(frac = 1, random_state = 56).reset_index(drop = True)\n\ndisplay(trainset_df.head())\n\ntestset_df.head()","7f6b1fef":"# converting the Label to a numeric format for testing later...\nLE = LabelEncoder()\n\ny_test = LE.fit_transform(testset_df[\"Label\"])","4cc41fd8":"plt.style.use(\"dark_background\")","371ee126":"figure = plt.figure(figsize=(10, 10))\nx = plt.imread(trainset_df[\"File\"][3])\nplt.imshow(x)\nplt.xlabel(x.shape)\nplt.title(trainset_df[\"Label\"][3])","7a544cae":"figure = plt.figure(figsize=(10,10))\nx = plt.imread(trainset_df[\"File\"][48])\nplt.imshow(x)\nplt.xlabel(x.shape)\nplt.title(trainset_df[\"Label\"][48])","0e68744c":"fig, axes = plt.subplots(nrows = 5,\n                        ncols = 5,\n                        figsize = (10,10),\n                        subplot_kw = {\"xticks\":[],\"yticks\":[]})\n\nfor i,ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(trainset_df[\"File\"][i]))\n    ax.set_title(trainset_df[\"Label\"][i])\nplt.tight_layout()\nplt.show()","ef59f8b0":"figure = plt.figure(figsize=(10,10))\nx = plt.imread(testset_df[\"File\"][25])\nplt.imshow(x)\nplt.xlabel(x.shape)\nplt.title(testset_df[\"Label\"][25])","29428254":"figure = plt.figure(figsize=(10,10))\nx = plt.imread(testset_df[\"File\"][7])\nplt.imshow(x)\nplt.xlabel(x.shape)\nplt.title(testset_df[\"Label\"][7])","80692d4e":"fig, axes = plt.subplots(nrows = 5,\n                        ncols = 5,\n                        figsize = (10,10),\n                        subplot_kw = {\"xticks\":[],\"yticks\":[]})\n\nfor i,ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(testset_df[\"File\"][i]))\n    ax.set_title(testset_df[\"Label\"][i])\nplt.tight_layout()\nplt.show()","086d674e":"train_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                    shear_range = 0.2,\n                                    zoom_range = 0.1,\n                                    rotation_range = 20,\n                                    width_shift_range = 0.1,\n                                    height_shift_range = 0.1,\n                                    horizontal_flip = True,\n                                    vertical_flip = True,\n                                    validation_split = 0.1)\n\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)","2a8eaa52":"print(\"Preparing the training dataset ...\")\ntraining_set = train_datagen.flow_from_dataframe(\n    dataframe = trainset_df,\n    x_col = \"File\",\n    y_col = \"Label\",\n    target_size = (128, 128),\n    color_mode = \"rgb\",\n    class_mode = \"binary\",\n    batch_size = 32,\n    shuffle = True,\n    seed = 2,\n    subset = \"training\")\n\nprint(\"Preparing the validation dataset ...\")\nvalidation_set = train_datagen.flow_from_dataframe(\n    dataframe = trainset_df,\n    x_col = \"File\",\n    y_col = \"Label\",\n    target_size = (128, 128),\n    color_mode =\"rgb\",\n    class_mode = \"binary\",\n    batch_size = 32,\n    shuffle = True,\n    seed = 2,\n    subset = \"validation\")\n\nprint(\"Preparing the test dataset ...\")\ntest_set = test_datagen.flow_from_dataframe(\n    dataframe = testset_df,\n    x_col = \"File\",\n    y_col = \"Label\",\n    target_size = (128, 128),\n    color_mode =\"rgb\",\n    class_mode = \"binary\",\n    shuffle = False,\n    batch_size = 32)\n\nprint('Data generators are ready!')","b095ea02":"print(\"Training: \")\nprint(training_set.class_indices)\nprint(training_set.image_shape)\nprint(\"---\" * 8)\nprint(\"Validation: \")\nprint(validation_set.class_indices)\nprint(validation_set.image_shape)\nprint(\"---\" * 8)\nprint(\"Test: \")\nprint(test_set.class_indices)\nprint(test_set.image_shape)","2935e0c4":"# Callbacks\ncb = [EarlyStopping(monitor = 'loss', mode = 'min', patience = 25, restore_best_weights = True)]","d00df026":"CNN = Sequential()\n\nCNN.add(Conv2D(32, (3, 3), input_shape = (128, 128, 3), activation = 'relu'))\nCNN.add(BatchNormalization())","34864914":"CNN.add(MaxPooling2D(pool_size = (2, 2)))","6ec5302d":"CNN.add(Conv2D(32, (3, 3), activation = 'relu'))\nCNN.add(MaxPooling2D(pool_size = (2, 2)))","a9453e25":"CNN.add(Conv2D(64, (3, 3), activation = 'relu'))\nCNN.add(SpatialDropout2D(0.2))\nCNN.add(MaxPooling2D(pool_size = (2, 2)))","a8b8374e":"CNN.add(Flatten())","73c4021b":"# Input layer\nCNN.add(Dense(units = 128, activation = 'relu'))\nCNN.add(Dropout(0.2))\n# Output layer (binary classification)\nCNN.add(Dense(units = 1, activation = 'sigmoid'))\n\nprint(CNN.summary())","9fe95a2c":"plot_model(CNN, to_file='CNN_model.png', show_layer_names = True , show_shapes = True)","cd8d82fb":"# Compile\nCNN.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Start of counting time...\nstart = dt.datetime.now()\n\n# Train\nCNN_model = CNN.fit(training_set, epochs = 50, validation_data = validation_set, callbacks = cb)\n\n# End of counting time...\nend = dt.datetime.now()\ntime_CNN = end - start\nprint ('\\nTraining and validation time is: ', time_CNN)","2b281e90":"acc = CNN_model.history['accuracy']\nval_acc = CNN_model.history['val_accuracy']\nloss = CNN_model.history['loss']\nval_loss = CNN_model.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\n\nplt.legend()\n\nplt.show()","914e7171":"score_CNN = CNN.evaluate(test_set)\nprint(\"Test Loss:\", score_CNN[0])\nprint(\"Test Accuracy:\", score_CNN[1])","c0146a82":"y_pred_CNN = CNN.predict(test_set)\ny_pred_CNN = np.round(y_pred_CNN)\n\nrecall_CNN = recall_score(y_test, y_pred_CNN)\nprecision_CNN = precision_score(y_test, y_pred_CNN)\nf1_CNN = f1_score(y_test, y_pred_CNN)\nroc_CNN = roc_auc_score(y_test, y_pred_CNN)","fa172c24":"print(classification_report(y_test, y_pred_CNN))","db26a07b":"plt.figure(figsize = (6, 4))\n\nsns.heatmap(confusion_matrix(y_test, y_pred_CNN),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","e6968c30":"# Save the model\nmodelFileName = 'cats-dogs-classifier.h5'\nCNN.save(modelFileName)\nprint('model saved as', modelFileName)","698aff40":"CNN_base_inc = InceptionV3(input_shape = (128, 128, 3), include_top = False, weights = 'imagenet')","4b2f027b":"for layer in CNN_base_inc.layers:\n    layer.trainable = False","3b77f318":"x = layers.Flatten()(CNN_base_inc.output)","163dc6dc":"x = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(0.1)(x)\nx = layers.Dense(1, activation='sigmoid')(x)\n\nCNN_inc = Model(CNN_base_inc.input, x)","d01df43f":"# Compilation\nCNN_inc.compile(optimizer = RMSprop(lr = 0.0001), loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Start of counting time\nstart = dt.datetime.now()\n\n# Training and validation\nCNN_inc_history = CNN_inc.fit(training_set, epochs = 50, validation_data = validation_set, callbacks = cb)\n\n# End of Time Counting\nend = dt.datetime.now()\ntime_CNN_inc = end - start\nprint ('\\nTraining and validation time is: ', time_CNN_inc)","8b61b6bf":"acc = CNN_inc_history.history['accuracy']\nval_acc = CNN_inc_history.history['val_accuracy']\nloss = CNN_inc_history.history['loss']\nval_loss = CNN_inc_history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\n\nplt.legend()\n\nplt.show()","4972951c":"score_inc = CNN_inc.evaluate(test_set)\nprint(\"Test Loss:\", score_inc[0])\nprint(\"Test Accuracy:\", score_inc[1])","d73ae747":"y_pred_inc = CNN_inc.predict(test_set)\ny_pred_inc = np.round(y_pred_inc)\n\nrecall_inc = recall_score(y_test, y_pred_inc)\nprecision_inc = precision_score(y_test, y_pred_inc)\nf1_inc = f1_score(y_test, y_pred_inc)\nroc_inc = roc_auc_score(y_test, y_pred_inc)","90f22187":"print(classification_report(y_test, y_pred_inc))","119c7096":"plt.figure(figsize = (6, 4))\n\nsns.heatmap(confusion_matrix(y_test, y_pred_inc),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","79f3de32":"# Save the model\nmodelFileName = 'fire_classifier_model-inc.h5'\nCNN_inc.save(modelFileName)\nprint('model saved as', modelFileName)","9b83128d":"CNN_base_xcep = Xception(input_shape = (128, 128, 3), include_top = False, weights = 'imagenet')\nCNN_base_xcep.trainable = False","684267ac":"CNN_xcep = Sequential()\nCNN_xcep.add(CNN_base_xcep)\nCNN_xcep.add(GlobalAveragePooling2D())\nCNN_xcep.add(Dense(128))\nCNN_xcep.add(Dropout(0.1))\nCNN_xcep.add(Dense(1, activation = 'sigmoid'))\n\nCNN_xcep.summary()","2223df94":"plot_model(CNN_xcep, show_layer_names = True , show_shapes = True)","c0dbfc0d":"# Compilation\nCNN_xcep.compile(optimizer='adam', loss = 'binary_crossentropy',metrics=['accuracy'])\n\n# Start of counting time\nstart = dt.datetime.now()\n\n# Training and validation\nCNN_xcep_history = CNN_xcep.fit(training_set, epochs = 50, validation_data = validation_set, callbacks = cb)\n\n# End of Time Counting\nend = dt.datetime.now()\ntime_CNN_xcep = end - start\nprint ('\\nTraining and validation time: ', time_CNN_xcep)","9c9f4e75":"acc = CNN_xcep_history.history['accuracy']\nval_acc = CNN_xcep_history.history['val_accuracy']\nloss = CNN_xcep_history.history['loss']\nval_loss = CNN_xcep_history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\n\nplt.legend()\n\nplt.show()","f87d09b7":"score_xcep = CNN_xcep.evaluate(test_set)\nprint(\"Test Loss:\", score_xcep[0])\nprint(\"Test Accuracy:\", score_xcep[1])","dabca18a":"y_pred_xcep = CNN_xcep.predict(test_set)\ny_pred_xcep = np.round(y_pred_xcep)\n\nrecall_xcep = recall_score(y_test, y_pred_xcep)\nprecision_xcep = precision_score(y_test, y_pred_xcep)\nf1_xcep = f1_score(y_test, y_pred_xcep)\nroc_xcep = roc_auc_score(y_test, y_pred_xcep)","dce14276":"print(classification_report(y_test, y_pred_xcep))","b6dbacf3":"plt.figure(figsize = (6, 4))\n\nsns.heatmap(confusion_matrix(y_test, y_pred_xcep),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","896dd970":"modelFileName = 'fire_classifier_model-xcep.h5'\nCNN_xcep.save(modelFileName)\nprint('model saved as', modelFileName)","72b5b856":"CNN_base_mobilenet = MobileNet(input_shape = (128, 128, 3), include_top = False, weights = 'imagenet')","d58df199":"for layer in CNN_base_mobilenet.layers:\n    layer.trainable = False","17527533":"CNN_mobilenet = Sequential()\nCNN_mobilenet.add(BatchNormalization(input_shape = (128, 128, 3)))\nCNN_mobilenet.add(CNN_base_mobilenet)\nCNN_mobilenet.add(BatchNormalization())\nCNN_mobilenet.add(GlobalAveragePooling2D())\nCNN_mobilenet.add(Dropout(0.5))\nCNN_mobilenet.add(Dense(1, activation = 'sigmoid'))\n\nCNN_mobilenet.summary()","eafde852":"plot_model(CNN_mobilenet, show_layer_names = True , show_shapes = True)","9dcb7f2d":"# Compilation\nCNN_mobilenet.compile(optimizer='adam',loss = 'binary_crossentropy', metrics=['accuracy'])\n\n# Start of counting time\nstart = dt.datetime.now()\n\n# Training and validation\nCNN_mobilenet_history = CNN_mobilenet.fit(training_set, epochs = 50, validation_data = validation_set, callbacks = cb)\n\n# End of Time Counting\nend = dt.datetime.now()\ntime_CNN_mobilenet = end - start\nprint ('\\nTraining and validation time: ', time_CNN_mobilenet)","fb4c2b19":"acc = CNN_mobilenet_history.history['accuracy']\nval_acc = CNN_mobilenet_history.history['val_accuracy']\nloss = CNN_mobilenet_history.history['loss']\nval_loss = CNN_mobilenet_history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\n\nplt.legend()\n\nplt.show()","503c52b9":"score_mn = CNN_mobilenet.evaluate(test_set)\nprint(\"Test Loss:\", score_mn[0])\nprint(\"Test Accuracy:\", score_mn[1])","3a767681":"y_pred_mn = CNN_mobilenet.predict(test_set)\ny_pred_mn = np.round(y_pred_mn)\n\nrecall_mn = recall_score(y_test, y_pred_mn)\nprecision_mn = precision_score(y_test, y_pred_mn)\nf1_mn = f1_score(y_test, y_pred_mn)\nroc_mn = roc_auc_score(y_test, y_pred_mn)","de995fbd":"print(classification_report(y_test, y_pred_mn))","025505f0":"plt.figure(figsize = (6, 4))\n\nsns.heatmap(confusion_matrix(y_test, y_pred_mn),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","80655df8":"# Save the model\nmodelFileName = 'fire_classifier_model-mobilenet.h5'\nCNN_mobilenet.save(modelFileName)\nprint('model saved as', modelFileName)","6e02d49b":"models= [('ConvNet', time_CNN, np.mean(CNN_model.history['accuracy']), np.mean(CNN_model.history['val_accuracy'])),\n         ('Inception', time_CNN_inc, np.mean(CNN_inc_history.history['accuracy']), np.mean(CNN_inc_history.history['val_accuracy'])),\n         ('Xception', time_CNN_xcep, np.mean(CNN_xcep_history.history['accuracy']), np.mean(CNN_xcep_history.history['val_accuracy'])),\n         ('MobileNet', time_CNN_mobilenet, np.mean(CNN_mobilenet_history.history['accuracy']), np.mean(CNN_mobilenet_history.history['val_accuracy']))]\n\ndf_all_models = pd.DataFrame(models, columns = ['Model', 'Time', 'Training accuracy (%)', 'Validation Accuracy (%)'])\n\ndf_all_models","09f3cdc8":"models = [('ConvNet', score_CNN[1], recall_CNN, precision_CNN, f1_CNN, roc_CNN),\n          ('Inception', score_inc[1], recall_inc, precision_inc, f1_inc, roc_inc),\n          ('Xception', score_xcep[1], recall_xcep, precision_xcep, f1_xcep, roc_xcep),\n          ('MobileNet', score_mn[1], recall_mn, precision_mn, f1_mn, roc_mn)]\n\ndf_all_models_testset = pd.DataFrame(models, columns = ['Model', 'Test accuracy (%)', 'Recall (%)', 'Precision (%)', 'F1 (%)', 'AUC'])\n\ndf_all_models_testset","4e272a87":"plt.subplots(figsize=(12, 10))\nsns.barplot(y = df_all_models_testset['Test accuracy (%)'], x = df_all_models_testset['Model'], palette = 'icefire')\nplt.xlabel(\"Models\")\nplt.title('Accuracy')\nplt.show()","5a834898":"r_probs = [0 for _ in range(len(y_test))]\nr_auc = roc_auc_score(y_test, r_probs)\nr_fpr, r_tpr, _ = roc_curve(y_test, r_probs)\n\nfpr_cnn, tpr_cnn, _ = roc_curve(y_test, y_pred_CNN)\nfpr_inc, tpr_inc, _ = roc_curve(y_test, y_pred_inc)\nfpr_xcep, tpr_xcep, _ = roc_curve(y_test, y_pred_xcep)\nfpr_mn, tpr_mn, _ = roc_curve(y_test, y_pred_mn)","8b7390dc":"sns.set_style('darkgrid')\n\nplt.plot(r_fpr, r_tpr, linestyle='--', label='Random prediction (AUROC = %0.3f)' % r_auc)\n\nplt.plot(fpr_cnn, tpr_cnn, marker='.', label='ConvNet (AUROC = %0.3f)' % roc_CNN)\nplt.plot(fpr_inc, tpr_inc, marker='.', label='Inception (AUROC = %0.3f)' % roc_inc)\nplt.plot(fpr_xcep, tpr_xcep, marker='.', label='Xception (AUROC = %0.3f)' % roc_xcep)\nplt.plot(fpr_mn, tpr_mn, marker='.', label='MobileNet (AUROC = %0.3f)' % roc_mn)\n\nplt.title('ROC Plot')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend() \nplt.show()","b35a6482":"test_set.class_indices","300f3576":"plt.style.use(\"dark_background\")\n\n\nfig, axes = plt.subplots(nrows = 4,\n                         ncols = 4,\n                         figsize = (20, 20),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(testset_df[\"File\"].iloc[i]))\n    ax.set_title(f\"True: {testset_df.Label.iloc[i]}\\n Predicted:\\nConvNet: {y_pred_CNN[i]}\\nInception: {y_pred_inc[i]}\\nXception: {y_pred_xcep[i]}\\nMobileNet: {y_pred_mn[i]}\")\nplt.tight_layout()\nplt.show()","23d757d5":"## 12. Construction of the fourth model (MobileNet)\nThe MobileNet model proposed by Howard et al. (2017), is a CNN architecture that were created to perform computer vision tasks on mobile devices and embedded systems. They are based on in-depth separable convolution operations, which lessens the burden of operations in the first layers.\n\n<p><img src = \"https:\/\/nitheshsinghsanjay.github.io\/images\/mobtiny_fig.PNG\" alt><\/p>","6f60f0ce":"## Detection of Cats and Dogs using Convolutional Neural Networks\n\n<p><img src = \"https:\/\/i.ibb.co\/9HTfBTL\/cd.png\" alt><\/p>\n\n#### Dataset information:\n\n- The data was collected to train a model to distinguish between images containing dogs (images of dogs) and images of cats (images of cats), so the whole problem is binary classification.\n\n\nThe data is divided into 2 folders:\n- The folder `` training_set`` contains 8000 images (4000 images of dogs and 4000 of cats) for training the model.\n- The folder `` test_set`` contains 2000 images (1000 images of dogs and 1000 of cats) for testing the model.\n\nThe dataset can be found on the `` Kaggle`` platform at the link below:\n\n- https:\/\/www.kaggle.com\/chetankv\/dogs-cats-images","ebe9c83e":"###### Step 5 - Model training history\n\nWe can see how accuracy improves over time, eventually leveling off. Correspondingly, the loss decreases over time. Plots like these can help diagnose overfitting. If we had seen an upward curve in the loss of validation over time (a U shape in the graph), we would suspect that the model was starting to memorize the test set and would not generalize well to new data.","1e195e31":"###### Step 3 - Hidden Layers","c9169c5d":"###### Step 5 - Dense Neural Networks\n\nParameters of the `` RNA``:\n\n     Dense - All neurons connected\n     units - Number of neurons that are part of the hidden layer\n     activation - Activation function that will be inserted\n     Dropout - is used to decrease the chance of overfitting (20% of the input neurons are zeroed)\n\nParameters of the ``EarlyStopping``:\n\n     monitor - Metric to be monitored\n     patience - Number of seasons without improvement in the model, after the training is interrupted\n     restore_best_weights - Restores the best weights if training is interrupted","4caf5b67":"###### Step 2 - Flattening\n    Transforming the matrix to a vector to enter the Artificial Neural Network layer","4cb89c72":"###### Step 4 - Flattening\n    \n     Transforming the matrix to a vector to enter the Artificial Neural Network layer","32c8880b":"###### Step 6 - Model compilation and training\n\nNow that we have specified the model architecture, we will compile the model for training. For this, we need to specify the loss function (what we are trying to minimize), the optimizer (how we want to do to minimize the loss) and the metric (how we will judge the model's performance). Next, we will call .fit to start training the process.\n\n``Compile`` parameters:\n\n     optimizer - descent of the gradient and descent of the stochastic gradient\n     loss - Loss function (binary_crossentropy as there is only one exit)\n     metrics - Evaluation metrics (obs - more than one can be placed)\n\n``Fit`` parameters:\n\n     train_data - training database\n     epochs - number of seasons\n     validation_data - test database\n     callbacks - Using EarlyStopping\n     validation_steps - number of images to validation","206dbfe3":"###### Step 2 - Max Pooling\nReduced image size by focusing on the most important features\n\n     Matrix definition with a total of 4 pixels (2, 2)","479bb154":"## 4. Generating batches of images\nIn this part we will generate batches of images increasing the training data, for the test database we will just normalize the data using [ImageDataGenerator](https:\/\/keras.io\/api\/preprocessing\/image\/#imagedatagenerator-class)\n\nParameters of ``ImageDataGenerator``:\n\n    rescale - Transform image size (normalization of data)\n    shear_range - Random geometric transformations\n    zoom_range - Images that will be zoomed\n    rotation_range - Degree of image rotation\n    width_shift_range - Image Width Change Range\n    height_shift_range - Image height change range\n    horizontal_flip - Rotate images horizontally\n    vertical_flip - Rotate images vertically\n    validation_split - Images that have been reserved for validation (0-1)","4654f4c9":"###### Step 2 - Dense Neural Networks\n\n    Dense - All connected neurons\n    units - Number of neurons that are part of the hidden layer\n    activation - Activation function that will be inserted\n    Dropout - is used to decrease the chance of overfitting (40% of input neurons are zeroed)","dc6f251a":"## 1. Imports from libraries","35b2627c":"###### Step 6 - Viewing results and generating forecasts","0193f8a8":"Use of callbacks to monitor models and see if metrics will improve, otherwise training is stopped.\n\n``EarlyStopping`` parameters:\n\n    monitor - Metrics that will be monitored\n    patience - Number of times without improvement in the model, after these times the training is stopped\n    restore_best_weights - Restores best weights if training is interrupted","00d901e6":"## 13. Viewing the results of all models","a392630e":"###### Step 5 - Model training history\n\nWe can see how accuracy improves over time, eventually leveling off. Correspondingly, the loss decreases over time. Plots like these can help diagnose overfitting. If we had seen an upward curve in the loss of validation over time (a U shape in the graph), we would suspect that the model was starting to memorize the test set and would not generalize well to new data.","ab48e24d":"###### Step 4 - Model compilation and training\n\nNow that we have specified the model architecture, we will compile the model for training. For this, we need to specify the loss function (what we are trying to minimize), the optimizer (how we want to do to minimize the loss) and the metric (how we will judge the model's performance). Next, we will call .fit to start training the process.\n\n``Compile`` parameters:\n\n     optimizer - descent of the gradient and descent of the stochastic gradient\n     loss - Loss function (binary_crossentropy as there is only one exit)\n     metrics - Evaluation metrics (obs - more than one can be placed)\n\n``Fit`` parameters:\n\n     train_data - training database\n     epochs - number of seasons\n     validation_data - test database\n     callbacks - Using EarlyStopping\n     validation_steps - number of images to validation","eab95394":"###### Step 4 - Model compilation and training\n\nNow that we have specified the model architecture, we will compile the model for training. For this, we need to specify the loss function (what we are trying to minimize), the optimizer (how we want to do to minimize the loss) and the metric (how we will judge the model's performance). Next, we will call .fit to start training the process.\n\n``Compile`` parameters:\n\n     optimizer - descent of the gradient and descent of the stochastic gradient\n     loss - Loss function (binary_crossentropy as there is only one exit)\n     metrics - Evaluation metrics (obs - more than one can be placed)\n\n``Fit`` parameters:\n\n     train_data - training database\n     epochs - number of seasons\n     validation_data - test database\n     callbacks - Using EarlyStopping\n     validation_steps - number of images to validation","0705f221":"###### Observing the images from the training dataset","7431720c":"###### Step 8 - Viewing results and generating forecasts","6beeab80":"## 2. Organizing Training and Testing Dataframes","f1b97c71":"###### Step 2 - Dense Neural Networks\n\n    Dense - All connected neurons\n    units - Number of neurons that are part of the hidden layer\n    activation - Activation function that will be inserted\n    Dropout - is used to decrease the chance of overfitting (40% of input neurons are zeroed)","f2bcbfbc":"## 5. Directory of training, validation and test images\n\nHere we make the division of the image bases for training, validation and testing of the model, for that we use the [flow_from_dataframe](https:\/\/keras.io\/api\/preprocessing\/image\/#flowfromdataframe-method)\n\nParameters of ``flow_from_directory``:\n\n    dataframe - Dataframe containing the images directory\n    x_col - Column name containing the images directory\n    y_col - Name of the column containing what we want to predict\n    target_size - size of the images (remembering that it must be the same size as the input layer)\n    color_mode - RGB color standard\n    class_mode - binary class mode (cat\/dog)\n    batch_size - batch size (32)\n    shuffle - Shuffle the data\n    seed - optional random seed for the shuffle\n    subset - Subset of data being training and validation (only used if using validation_split in ImageDataGenerator)","c52633b5":"###### Observing the images from the test dataset","d25e9b1e":"###### Step 8 - Viewing results and generating forecasts","d61cfc9f":"###### Step 1 - Convolution\nFeature Detector and Feature Map\n\n    Number of filters (32)\n    Dimensions of the feature detector (3, 3)\n    Definition of height \/ width and RGB channels (128, 128, 3)\n    Activation function to remove negative values from the image - 'relu'\n    Processing acceleration - BatchNormalization","7d02262b":"###### Step 1 - Base model creation\n    input_shape - Setting the height\/width and RGB channels (128, 128, 3)\n    include_top - Fully connected layer will not be included on top\n    weights - Pre-training using imagenet","a467f870":"###### Step 1 - Base model creation\n\n    input_shape - Setting the height\/width and RGB channels (128, 128, 3)\n    include_top - Fully connected layer will not be included on top\n    weights - Pre-training using imagenet","f01aae7e":"## 3. Observing the images","7f08ab26":"###### Step 3 - Dense Neural Networks\n\n    Dense - All connected neurons\n    units - Number of neurons that are part of the hidden layer\n    activation - Activation function that will be inserted\n    Dropout - is used to decrease the chance of overfitting (40% of input neurons are zeroed)","a9320f4d":"###### Step 5 - Model training history\n\nWe can see how accuracy improves over time, eventually leveling off. Correspondingly, the loss decreases over time. Plots like these can help diagnose overfitting. If we had seen an upward curve in the loss of validation over time (a U shape in the graph), we would suspect that the model was starting to memorize the test set and would not generalize well to new data.","669bec58":"## 6. Construction of the first model (ConvNet)\n\nCNNs are a specific type of artificial neural network that is very effective for image classification because they are able to take into account the spatial coherence of the image, that is, that pixels close to each other are often related.\n\nThe construction of a CNN begins with specifying the model type. In our case, we will use a ``Sequential`` model.\n\n<p><img src = \"https:\/\/i.ibb.co\/0jWhFsW\/ConvNet.png\" alt><\/p>","0d595cab":"###### Step 7 - Model training history\n\nWe can see how accuracy improves over time, eventually leveling off. Correspondingly, the loss decreases over time. Plots like these can help diagnose overfitting. If we had seen an upward curve in the loss of validation over time (a U shape in the graph), we would suspect that the model was starting to memorize the test set and would not generalize well to new data.","dfbae859":"###### Step 8 - Viewing results and generating forecasts","0b27e4a8":"###### Step 4 - Model compilation and training\n\nNow that we have specified the model architecture, we will compile the model for training. For this, we need to specify the loss function (what we are trying to minimize), the optimizer (how we want to do to minimize the loss) and the metric (how we will judge the model's performance). Next, we will call .fit to start training the process.\n\n``Compile`` parameters:\n\n     optimizer - descent of the gradient and descent of the stochastic gradient\n     loss - Loss function (binary_crossentropy as there is only one exit)\n     metrics - Evaluation metrics (obs - more than one can be placed)\n\n``Fit`` parameters:\n\n     train_data - training database\n     epochs - number of seasons\n     validation_data - test database\n     callbacks - Using EarlyStopping\n     validation_steps - number of images to validation","7fdcee6e":"## 9. Construction of the second model (Inception)\nThe [InceptionV3](https:\/\/keras.io\/api\/applications\/inceptionv3\/) model proposed by Szegedy et al. (2015), is a CNN architecture that seeks to solve several large-scale image recognition problems and can also be used in transfer learning problems. Its differential is the presence of convolutional characteristics extractor modules. These modules have the functionality to learn with fewer parameters that contain a greater range of information.\n\n<p><img src = \"https:\/\/cloud.google.com\/tpu\/docs\/images\/inceptionv3onc--oview.png?hl=pt-br\" alt><\/p>","8c93c5c4":"## 10. Construction of the third model (Xception)\nThe [Xception](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications\/Xception) model proposed by Chollet et al.(2016), is a CNN architecture similar to the Inception described above and, has the difference that the initiation modules were replaced by separable convolutions in depth. Xception has the same amount of parameters as InceptionV3 with a total of 36 convolutional layers. Thus, having a more efficient use of parameters.\n\n<p><img src = \"https:\/\/miro.medium.com\/max\/1688\/1*J8dborzVBRBupJfvR7YhuA.png\" alt><\/p>","f8ca0e30":"###### Step 1 - Base model creation\n\n    input_shape - Setting the height\/width and RGB channels (128, 128, 3)\n    include_top - Fully connected layer will not be included on top\n    weights - Pre-training using imagenet"}}