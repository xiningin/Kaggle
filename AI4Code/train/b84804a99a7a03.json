{"cell_type":{"c85f0c42":"code","94b7e9ee":"code","52bbfc95":"code","43d02c05":"code","997211fa":"code","fe9cbae0":"code","dec6b5e2":"code","50336946":"code","59ea2f43":"code","98670a8a":"code","4591e0de":"code","e5620e46":"code","912e3ca5":"code","b248d42e":"code","b4a2557d":"code","0427c3f2":"code","a3d72576":"code","fe6ead12":"code","f5f398b6":"code","96ccbcdb":"code","74bc8eea":"code","7a16fc95":"code","be4c2b9c":"code","16965644":"code","c1fc75d4":"code","94eb1e05":"code","9e0071fe":"code","d282ed97":"code","dacb4b8c":"code","b5be94f5":"code","2d2ef73b":"code","fe2f80af":"code","def40897":"code","b8380a26":"code","9bf08015":"code","b910370d":"code","c8ae58b1":"code","c7431b9f":"code","f46d62a1":"code","4765d406":"code","2fa37155":"code","b5f4a0db":"code","0bc07cc5":"code","7c73ecef":"code","88539f42":"code","d3bf2cb1":"code","df924911":"code","9f20f6a3":"code","fa0df6de":"code","69881d77":"code","c8b2f0ec":"code","83352ac2":"code","00db82bd":"code","d0a5e062":"code","a1c518ed":"code","5a8d12e8":"code","9d20f3cc":"markdown","f8a79125":"markdown","100fde7b":"markdown","385575b5":"markdown","5e01bf0c":"markdown","0c394136":"markdown","e888c208":"markdown","977622d3":"markdown","616136ac":"markdown","fed6bc8f":"markdown","32bd871b":"markdown","99662d95":"markdown","2c3c2b5a":"markdown","a9e5b196":"markdown","76e2555f":"markdown","3c5b6a85":"markdown","4ed27500":"markdown","145b4b37":"markdown"},"source":{"c85f0c42":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nfrom scipy.spatial.distance import cdist\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.decomposition import PCA, IncrementalPCA\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport string\n\n# Little utility to clean NaNs, inifinity from dataframe\ndef clean_dataset(df):\n    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n    df.dropna(inplace=True)\n    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n    return df[indices_to_keep].astype(np.float64)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) \n# that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n","94b7e9ee":"def logic(index):\n    if index % 1000 == 0:\n       return False\n    return True","52bbfc95":"# Load Dataset\n#df = pd.read_csv(\"\/kaggle\/input\/steam-reviews-2021\/steam_reviews.csv\",nrows=50000,skiprows= lambda x: logic(x))\ndf = pd.read_csv(\"\/kaggle\/input\/steam-reviews-2021\/steam_reviews.csv\", nrows = 8000, low_memory=False, skiprows= lambda x: logic(x))\ndf.head()\nprint(\"Len: \" + str(len(df)))","43d02c05":"# Output all columns name\nlist(df.columns)","997211fa":"all_columns_with_numbers = ['app_id','review_id','timestamp_created','timestamp_updated','votes_helpful','votes_funny','weighted_vote_score',\n          'comment_count','author.num_games_owned','author.steamid','author.num_reviews','author.playtime_forever',\n          'author.playtime_last_two_weeks','author.playtime_at_review']\ncolumns = ['app_id','timestamp_created','timestamp_updated','votes_helpful','votes_funny','weighted_vote_score',\n          'comment_count','author.num_games_owned','author.num_reviews','author.playtime_forever',\n          'author.playtime_last_two_weeks','author.playtime_at_review']\ncolumns_2 = ['timestamp_created','timestamp_updated','votes_helpful','votes_funny','weighted_vote_score',\n          'comment_count','author.num_games_owned','author.num_reviews','author.playtime_forever',\n          'author.playtime_last_two_weeks','author.playtime_at_review']\ncolumns_3 = ['votes_helpful','votes_funny','weighted_vote_score',\n          'comment_count','author.num_games_owned','author.num_reviews','author.playtime_forever',\n          'author.playtime_last_two_weeks','author.playtime_at_review']\ndf1 =  df.loc[:, all_columns_with_numbers]\ndf1.head()","fe9cbae0":"# Cleanup\ndf1 = clean_dataset(df1)\ndf1.head()","dec6b5e2":"def elbow(df): # Elbow method for best k\n    distortions = []\n    inertias = []\n    mapping1 = {}\n    mapping2 = {}\n\n    K = range(1, 10)\n\n    for k in K:\n        # Building and fitting the model\n        kmeanModel = KMeans(n_clusters=k).fit(df)\n        kmeanModel.fit(df)\n\n        distortions.append(sum(np.min(cdist(df, kmeanModel.cluster_centers_,\n                                            'euclidean'), axis=1)) \/ df.shape[0])\n        inertias.append(kmeanModel.inertia_)\n\n        mapping1[k] = sum(np.min(cdist(df, kmeanModel.cluster_centers_,\n                                       'euclidean'), axis=1)) \/ df.shape[0]\n        mapping2[k] = kmeanModel.inertia_\n\n        print(\"Iter \"+str(k)+\" of \"+str(max(K))+\" done...\")\n    \n    plt.plot(K, distortions, 'bx-')\n    plt.xlabel('Values of K')\n    plt.ylabel('Distortion')\n    plt.title('The Elbow Method using Distortion')\n    plt.show()\n    \n    plt.plot(K, inertias, 'bx-')\n    plt.xlabel('Values of K')\n    plt.ylabel('Inertia')\n    plt.title('The Elbow Method using Inertia')\n    plt.show()\n","50336946":"%%time\nelbow(df1)","59ea2f43":"# Is 3 the optimal? [Choose optimal]\noptimal = 3\nkmeans = KMeans(n_clusters=optimal).fit(df1)","98670a8a":"# Optimal k-means\nlabels = pd.DataFrame(kmeans.labels_) #This is where the label output of the KMeans we just ran lives. Make it a dataframe so we can concatenate back to the original data\nlabeled_df = pd.concat((df1,labels),axis=1)\nlabeled_df = labeled_df.rename({0:'labels'},axis=1)\nlabeled_df.head()","4591e0de":"# Visual\nsns.lmplot(x='weighted_vote_score',y='comment_count',data=labeled_df,hue='labels',fit_reg=False)","e5620e46":"# Visual according to PCA\nsns.lmplot(x='timestamp_updated',y='timestamp_created',data=labeled_df,hue='labels',fit_reg=False)","912e3ca5":"### PCA ###\n\n# We have to exclude some columns avoiding strange behaviours, but the final result won't change\ndf1_ = df1[columns_2]\n\ndf_normalized=(df1_ - df1_.mean()) \/ df1_.std() # mean normalization\n#df_normalized=(df1-df1.min())\/(df1.max()-df1.min()) # min-max normalization\n\n\n\npca = PCA(n_components=2)\n\npca.fit(df_normalized)\n\n# Reformat and view results\nloadings = pd.DataFrame(pca.components_.T,\n    columns=['PC%s' % _ for _ in range(0,2)],\n    index=df1_.columns)\nprint(loadings)","b248d42e":"plt.plot(pca.explained_variance_ratio_)\nplt.ylabel('Explained Variance')\nplt.xlabel('Components')\nplt.show()","b4a2557d":"## First method PCA\n\nreduced = pca.transform(df_normalized)\nreduced_df = pd.DataFrame(data = reduced\n             , columns = ['principal component 1', 'principal component 2'])\n\nlabeled__df = pd.concat((reduced_df,labels),axis=1)\nlabeled__df = labeled__df.rename({0:'labels'},axis=1)\nlabeled__df['color'] = labeled__df.apply (lambda row:  'r' if row['labels'] == 1 else ('g' if row['labels'] == 0 else 'b') , axis=1)\nplt.xlabel('Principal Component - 1')\nplt.ylabel('Principal Component - 2')\nplt.title(\"Principal Component Analysis of Steam Dataset\")\n        \nplt.scatter(labeled__df['principal component 1']\n               , labeled__df['principal component 2'], c = labeled__df['color'])\nplt.show()","0427c3f2":"#Another scaling\nscaler = MinMaxScaler()\ndf2 = scaler.fit_transform(df1[all_columns_with_numbers])","a3d72576":"pca_ = PCA(n_components = 2)\npca_.fit(df2)\nreduced = pca_.transform(df2)\nprint(reduced)","fe6ead12":"reduced_df = pd.DataFrame(data = reduced\n             , columns = ['principal component 1', 'principal component 2'])\nreduced_df.head()","f5f398b6":"print('Explained variation per principal component: {}'.format(pca_.explained_variance_ratio_))","96ccbcdb":"labeled__df = pd.concat((reduced_df,labels),axis=1)\nlabeled__df = labeled__df.rename({0:'labels'},axis=1)\nlabeled__df.tail()","74bc8eea":"labeled__df['color'] = labeled__df.apply (lambda row:  'r' if row['labels'] == 1 else ('g' if row['labels'] == 0 else 'b') , axis=1)","7a16fc95":"plt.xlabel('Principal Component - 1')\nplt.ylabel('Principal Component - 2')\nplt.title(\"Principal Component Analysis of Steam Dataset\")\n        \nplt.scatter(labeled__df['principal component 1']\n               , labeled__df['principal component 2'], c = labeled__df['color'])\nplt.show()","be4c2b9c":"# New imports \nimport re\nimport nltk.corpus\nfrom unidecode                        import unidecode\nfrom nltk.tokenize                    import word_tokenize\nfrom nltk                             import SnowballStemmer\nfrom sklearn.feature_extraction.text  import TfidfVectorizer\nfrom sklearn.preprocessing            import normalize","16965644":"columns = ['app_name','language','review']","c1fc75d4":"# Take this data for tfidf\ndf_target = df.loc[:, columns]\ndf_target.head()","94eb1e05":"# We no longer need text columns \ndf = df.drop(columns=columns)\ndf.head()","9e0071fe":"scaler = MinMaxScaler()\ndf[list(df.columns)] = scaler.fit_transform(df[list(df.columns)])","d282ed97":"df_target.head(60)","dacb4b8c":"# removes a list of words (ie. stopwords) from a tokenized list.\ndef removeWords(listOfTokens, listOfWords):\n    return [token for token in listOfTokens if token not in listOfWords]\n\n# applies stemming to a list of tokenized words\ndef applyStemming(listOfTokens, stemmer):\n    return [stemmer.stem(token) for token in listOfTokens]\n\n# removes any words composed of less than 2 or more than 21 letters\ndef twoLetters(listOfTokens):\n    twoLetterWord = []\n    for token in listOfTokens:\n        if len(token) <= 2 or len(token) >= 21:\n            twoLetterWord.append(token)\n    return twoLetterWord","b5be94f5":"# Should we inlcude title in our analysis?\ninclude_title = False","2d2ef73b":"# some heavy preprocess\ni = 0 \ndef process_row(row):\n    global i \n    global include_title\n    print(\"Tfidf Progress: \"+str(\"{:.5f}\".format(i\/(len(df_target)-1) *100)) + \"%\",end=\"\\r\")\n    i += 1\n    \n    language = row['language']\n    reuslt = None\n    if(include_title):\n        result = (str(row['review']) + \" \" + str(row['app_name'])).replace(u'\\ufffd', '8')   # Replaces the ASCII '\ufffd' symbol with '8'\n    else:\n        result = (str(row['review'])).replace(u'\\ufffd', '8')   # Replaces the ASCII '\ufffd' symbol with '8'\n    result = result.replace(',', '')          # Removes commas\n    result = result.rstrip('\\n')              # Removes line breaks\n    result = result.casefold()                # Makes all letters lowercase\n\n    result = re.sub('\\W_',' ', result)        # removes specials characters and leaves only words\n    result = re.sub(\"\\S*\\d\\S*\",\" \", result)   # removes numbers and words concatenated with numbers IE h4ck3r. Removes road names such as BR-381.\n    result = re.sub(\"\\S*@\\S*\\s?\",\" \", result) # removes emails and mentions (words with @)\n    result = re.sub(r'http\\S+', '', result)   # removes URLs with http\n    result = re.sub(r'www\\S+', '', result)    # removes URLs with www\n    \n    result = re.sub(r\"\\s+(.)\\1+\\b\", \"\",result)               # remove spammed chars\n    result = ''.join([i for i in result if not i.isdigit()]) # remove numbers\n    result = re.sub(r'[^\\w\\s]','',result)                    # remove punctuation\n   \n\n    listOfTokens = word_tokenize(result)\n    twoLetterWord = twoLetters(listOfTokens)\n\n\n    try: \n        # Not all languages are supported \n        # We remove stopwords and stem only with supported languages\n        stopwords = nltk.corpus.stopwords.words(language)\n        stopwords.append(\"game\")\n        \n        param_stemmer = SnowballStemmer(language)\n        listOfTokens = applyStemming(listOfTokens, param_stemmer)\n        listOfTokens = removeWords(listOfTokens, stopwords)\n        \n        \n\n    finally:\n        # Output the tokens\n        listOfTokens = list(filter(lambda item: len(item) < 25,listOfTokens))   # other spam filter\n        listOfTokens = removeWords(listOfTokens, [\"game\",\"gam\",\"igr\",\"juego\",\"you\",\"jogao\"]) # some game translations (en,en,ru,sp,ch,pt)\n        \n        result   = \" \".join(listOfTokens)\n        result = unidecode(result)\n        result = re.sub(r'[^\\x00-\\x7F]+','', result)    # remove non ascii\n\n        return result","fe2f80af":"english_only = False","def40897":"if english_only:\n    len_ = len(df_target)\n    df_target = df_target[df_target.language == 'english']\n    df_target.head()\n    print(\"Length before filtering \" + str(len_) + \" Length after filtering \"+ str(len(df_target)))","b8380a26":"%%time\ndf_target['tokens'] = df_target.apply(lambda row: process_row(row),axis=1)","9bf08015":"df_target.head(60)","b910370d":"%%time\n# tf-idf\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(df_target['tokens'])\ntf_idf = pd.DataFrame(data = X.toarray(), columns=vectorizer.get_feature_names())\n\ntf_idf.head()","c8ae58b1":"%%time\nbool_list = ['recommended','steam_purchase','received_for_free','written_during_early_access']\ndf[bool_list] = df[bool_list].astype(int)","c7431b9f":"df = pd.concat([df, tf_idf], axis=1)","f46d62a1":"df = clean_dataset(df)","4765d406":"df.head(60)","2fa37155":"%%time\ndf_ = df.copy()\n\npca__ = IncrementalPCA(n_components = 1024, batch_size=4096)\nreduced = pca__.fit_transform(df)","b5f4a0db":"columns__ = ['pca_comp_%i' % i\n   for i in range(reduced.shape[1])\n]\n\ndf = pd.DataFrame(data = reduced\n             , columns = columns__)\ndf.head()","0bc07cc5":"%%time\nelbow(df)","7c73ecef":"# Is 4 the new optimal? [Choose optimal]\noptimal = 4\nkmeans = KMeans(n_clusters=optimal).fit(tf_idf)","88539f42":"def get_top_features_cluster(tf_idf_array, prediction, n_feats):\n    labels = np.unique(prediction)\n    dfs = []\n    for label in labels:\n        id_temp = np.where(prediction==label) # indices for each cluster\n        x_means = np.mean(tf_idf_array[id_temp], axis = 0) # returns average score across cluster\n        sorted_means = np.argsort(x_means)[::-1][:n_feats] # indices with top 20 scores\n        features = vectorizer.get_feature_names()\n        best_features = [(features[i], x_means[i]) for i in sorted_means]\n        df = pd.DataFrame(best_features, columns = ['features', 'score'])\n        dfs.append(df)\n    return dfs\n\ndef plotWords(dfs, n_feats):\n    plt.figure(figsize=(8, 4))\n    for i in range(0, len(dfs)):\n        plt.title((\"Most Common Words in Cluster {}\".format(i)), fontsize=10, fontweight='bold')\n        sns.barplot(x = 'score' , y = 'features', orient = 'h' , data = dfs[i][:n_feats])\n        plt.show()","d3bf2cb1":"final_df_array = tf_idf.to_numpy()\nprediction = kmeans.predict(tf_idf)\nn_feats = 20\ndfs = get_top_features_cluster(final_df_array, prediction, n_feats)\nplotWords(dfs, 13)","df924911":"from wordcloud   import WordCloud","9f20f6a3":"# Transforms a centroids dataframe into a dictionary to be used on a WordCloud.\ndef centroidsDict(centroids, index):\n    a = centroids.T[index].sort_values(ascending = False).reset_index().values\n    centroid_dict = dict()\n\n    for i in range(0, len(a)):\n        centroid_dict.update( {a[i,0] : a[i,1]} )\n\n    return centroid_dict\n\ndef generateWordClouds(centroids):\n    wordcloud = WordCloud(max_font_size=100, background_color = 'white')\n    for i in range(0, len(centroids)):\n        centroid_dict = centroidsDict(centroids, i)        \n        wordcloud.generate_from_frequencies(centroid_dict)\n\n        plt.figure()\n        plt.title('Cluster {}'.format(i))\n        plt.imshow(wordcloud)\n        plt.axis(\"off\")\n        plt.show()","fa0df6de":"centroids = pd.DataFrame(kmeans.cluster_centers_)\ncentroids.columns = tf_idf.columns\ngenerateWordClouds(centroids)","69881d77":"# Optimal k-means\n\noptimal = 4\nkmeans = KMeans(n_clusters=optimal).fit(df)\n\nlabels = pd.DataFrame(kmeans.labels_) #This is where the label output of the KMeans we just ran lives. Make it a dataframe so we can concatenate back to the original data\nlabeled_df = pd.concat((df,labels),axis=1)\nlabeled_df = labeled_df.rename({0:'labels'},axis=1)\nlabeled_df.head()","c8b2f0ec":"pca_ = PCA(n_components = 2)\npca_.fit(df_)\nreduced = pca_.transform(df_)\nprint(reduced)","83352ac2":"reduced_df = pd.DataFrame(data = reduced\n             , columns = ['principal component 1', 'principal component 2'])\nreduced_df.head()","00db82bd":"print('Explained variation per principal component: {}'.format(pca_.explained_variance_ratio_))","d0a5e062":"labeled__df = pd.concat((reduced_df,labels),axis=1)\nlabeled__df = labeled__df.rename({0:'labels'},axis=1)\nlabeled__df.tail()","a1c518ed":"labeled__df['color'] = labeled__df.apply (lambda row:  'r' if row['labels'] == 1 else ('orange' if row['labels'] == 0 \n                                                                                       else ('blue' if row['labels'] == 2 \n                                                                                             else('green' if row['labels'] == 3 \n                                                                                                  else ('cyan' if row['labels'] == 4 else 'black')))) , axis=1)","5a8d12e8":"plt.xlabel('Principal Component - 1')\nplt.ylabel('Principal Component - 2')\nplt.title(\"Principal Component Analysis of Steam Dataset\")\n        \nplt.scatter(labeled__df['principal component 1']\n               , labeled__df['principal component 2'], c = labeled__df['color'])\nplt.show()","9d20f3cc":"# Final PCA\nComputed only on two axis in order to plot data","f8a79125":"# TF-IDF\nBuild TF-IDF index in order to make text eligiable for k-means","100fde7b":"# Import Dataset\nLoad dataset, only some rows taken evry 1000 occurrences in order to look only at the same game reviews","385575b5":"# Display data by choosing only two axes (at random)\nBy just changing the \"perspective\" without any other changes","5e01bf0c":"# Final K means\nLast k means computation","0c394136":"# Make sense to clustering\nPlot most common words from clusters","e888c208":"## PCA with sklearn min max\nSame as earlier, but with sklearn minmax scaler, just in case there is some difference (and per column scale)","977622d3":"# Heavy Review and App Name preprocess\n* normalizing\n* stemming\n* tokenizing\n\nOption to keep english only records, in order to have some coherence between data, but it is just an empirical hypotesis","616136ac":"# Final K means","fed6bc8f":"## Final clustering","32bd871b":"# One hot encoder\nUse one hot encoder from pd to encode booleans","99662d95":"# Preprocessing\nNow it is time to really go deep into our datset","2c3c2b5a":"# PCA to reduce size\nAfter tfifd the size needs to be reduced, we use PCA one again","a9e5b196":"## PCA with pandas utils (default mean normalization)\nTo display data, we coute PCA with sklearn library and panda utils. \nIt is possible to choose between mean normalization and min-max normalization","76e2555f":"# K means without any processing\nK means is computed by ruling out all non-numeric columns including booleans and strings.\n\nIt is possibile to choose between:\n* all columns with numeric values\n* meaningful numeric values (ids excluded)","3c5b6a85":"# Word Cloud\nTry to visualize some info about clusters","4ed27500":"# Combine Data\nMerge one hot and tfidf with some cleanup","145b4b37":"# Normalize\nMin-max normalization with sklearn libs"}}