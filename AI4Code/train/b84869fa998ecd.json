{"cell_type":{"c1c2da4f":"code","5ae34c39":"code","90bfb176":"code","da07b743":"code","3d05fe19":"code","16c9af63":"code","ec8643af":"code","79b4a222":"code","762fdd39":"code","04ee49d8":"code","fc99e217":"code","1321a9f4":"code","14155047":"code","e1bc9533":"code","b5c651f3":"code","3522a452":"code","2baa86ad":"code","85208a2b":"code","7e70f4b9":"code","33d58b87":"code","795738b7":"code","e20dc5b8":"code","a1a82327":"code","0effb482":"code","528556b2":"markdown","faf925fe":"markdown","249c9460":"markdown","9091830c":"markdown","2082af98":"markdown","6dc1a8a7":"markdown","92a7e526":"markdown","da08fd38":"markdown","405a3b21":"markdown","c6bee49b":"markdown","4a3473a8":"markdown","b6e1073c":"markdown","3b6d16b2":"markdown","e55303fa":"markdown","3b9fce5d":"markdown","114dbc5a":"markdown","daa50fbd":"markdown","655d94e9":"markdown","639696e2":"markdown","a1614f91":"markdown","4ce07eca":"markdown","3f50f80f":"markdown","ec180b1e":"markdown"},"source":{"c1c2da4f":"import string\nimport re\nfrom numpy import array, argmax, random, take\nimport pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, RepeatVector\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import load_model\nfrom keras import optimizers\nimport matplotlib.pyplot as plt\n%matplotlib inline\npd.set_option('display.max_colwidth', 200)","5ae34c39":"# function to read raw text file\ndef read_text(filename):\n        # open the file\n        file = open(filename, mode='rt', encoding='utf-8')\n        \n        # read all text\n        text = file.read()\n        file.close()\n        return text","90bfb176":"# split a text into sentences\ndef to_lines(text):\n    sents = text.strip().split('\\n')\n    sents = [i.split('\\t') for i in sents]\n    return sents","da07b743":"data = read_text(\"..\/input\/bilingual-sentence-pairs\/deu.txt\")\ndeu_eng = to_lines(data)\ndeu_eng = array(deu_eng)","3d05fe19":"deu_eng = deu_eng[:50000,:]","16c9af63":"# Remove punctuation\ndeu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]]\ndeu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]]\n\ndeu_eng","ec8643af":"# convert text to lowercase\nfor i in range(len(deu_eng)):\n    deu_eng[i,0] = deu_eng[i,0].lower()\n    deu_eng[i,1] = deu_eng[i,1].lower()","79b4a222":"# empty lists\neng_l = []\ndeu_l = []\n\n# populate the lists with sentence lengths\nfor i in deu_eng[:,0]:\n      eng_l.append(len(i.split()))\n\nfor i in deu_eng[:,1]:\n      deu_l.append(len(i.split()))\n\nlength_df = pd.DataFrame({'eng':eng_l, 'deu':deu_l})\n\nlength_df.hist(bins = 30)\nplt.show()","762fdd39":"# function to build a tokenizer\ndef tokenization(lines):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer","04ee49d8":"# prepare english tokenizer\neng_tokenizer = tokenization(deu_eng[:, 0])\neng_vocab_size = len(eng_tokenizer.word_index) + 1\n\neng_length = 8\nprint('English Vocabulary Size: %d' % eng_vocab_size)","fc99e217":"# prepare Deutch tokenizer\ndeu_tokenizer = tokenization(deu_eng[:, 1])\ndeu_vocab_size = len(deu_tokenizer.word_index) + 1\n\ndeu_length = 8\nprint('Deutch Vocabulary Size: %d' % deu_vocab_size)","1321a9f4":"# encode and pad sequences\ndef encode_sequences(tokenizer, length, lines):\n    seq = tokenizer.texts_to_sequences(lines)\n    # pad sequences with 0 values\n    seq = pad_sequences(seq, maxlen=length, padding='post')\n    return seq","14155047":"from sklearn.model_selection import train_test_split\n\n# split data into train and test set\ntrain, test = train_test_split(deu_eng, test_size=0.2, random_state = 12)","e1bc9533":"# prepare training data\ntrainX = encode_sequences(deu_tokenizer, deu_length, train[:, 1])\ntrainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n\n# prepare validation data\ntestX = encode_sequences(deu_tokenizer, deu_length, test[:, 1])\ntestY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])","b5c651f3":"# build NMT model\ndef define_model(in_vocab,out_vocab, in_timesteps,out_timesteps,units):\n    model = Sequential()\n    model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n    model.add(LSTM(units))\n    model.add(RepeatVector(out_timesteps))\n    model.add(LSTM(units, return_sequences=True))\n    model.add(Dense(out_vocab, activation='softmax'))\n    return model","3522a452":"# model compilation\nmodel = define_model(deu_vocab_size, eng_vocab_size, deu_length, eng_length, 512)","2baa86ad":"rms = optimizers.RMSprop(lr=0.001)\nmodel.compile(optimizer=rms, loss='sparse_categorical_crossentropy')","85208a2b":"filename = 'model.h1.24_jan_19'\ncheckpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n\n# train model\nhistory = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n                    epochs=30, batch_size=512, validation_split = 0.2,callbacks=[checkpoint], \n                    verbose=1)","7e70f4b9":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['train','validation'])\nplt.show()","33d58b87":"model = load_model('model.h1.24_jan_19')\npreds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))","795738b7":"def get_word(n, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == n:\n            return word\n    return None","e20dc5b8":"preds_text = []\nfor i in preds:\n    temp = []\n    for j in range(len(i)):\n        t = get_word(i[j], eng_tokenizer)\n        if j > 0:\n            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n                temp.append('')\n            else:\n                temp.append(t)\n        else:\n            if(t == None):\n                temp.append('')\n            else:\n                temp.append(t) \n\n    preds_text.append(' '.join(temp))","a1a82327":"pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})","0effb482":"# print 15 rows randomly\npred_df.head(15)","528556b2":"<p style=\"font-size:150%;\">CODE REFERENCE: https:\/\/www.analyticsvidhya.com\/blog\/2019\/01\/neural-machine-translation-keras\/<\/p>\n\n<p style=\"font-size:170%;\">First, let\u2019s breakdown the architecture of an RNN at a high level. Referring to the diagram above, there are a few parts of the model we to be aware of:<\/p>\n\n<ul>\n    <li style=\"font-size:150%;\">Inputs. Input sequences are fed into the model with one word for every time step. Each word is encoded as a unique integer so that it maps to the German dataset vocabulary.<\/li>\n    <li style=\"font-size:150%;\">Embedding Layers. Embeddings are used to convert each word to a vector. The size of the vector depends on the complexity of the vocabulary.<\/li>\n    <li style=\"font-size:150%;\">LSTM Layer (Encoder). This is where the context from word vectors in previous time steps is applied to the current word vector.<\/li>\n    <li style=\"font-size:150%;\">Dense Layers (Decoder). These are typical fully connected layers used to decode the encoded input into the correct translation sequence.<\/li>\n    <li style=\"font-size:150%;\">he outputs are returned as a sequence of integers or one-hot encoded vectors which can then be mapped to the English dataset vocabulary.<\/li>\n<\/ul>\n    \n    \n<center><h1 style=\"font-size:150%;\">Model Architecture<\/h1><\/center>\n<center><img src=\"https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2019\/01\/architecture.png\"><\/center>","faf925fe":"<h1> Define the Model<\/h1>","249c9460":"<h1> Prediction on unseen data<\/h1>","9091830c":"<center><h1 style=\"font-size:200%; background-color:skyblue; color:black; padding:15px; font-family:Garamond;\"><b>Intro to Notebook<\/b><\/h1><\/center>","2082af98":"<ul>\n    <li style=\"font-size:150%;\">When we feed our sequences of word IDs into the model, each sequence needs to be the same length. To achieve this, padding is added to any sequence that is shorter than the max length (i.e. shorter than the longest sentence).<\/li>\n<\/ul>\n\n<center><img src=\"https:\/\/miro.medium.com\/max\/1728\/0*6jZTOE0P7_i7N8pn.png\"><\/center>","6dc1a8a7":"<center><h1 style=\"font-size:300%; background-color:skyblue; color:grey; padding:60px; font-family:Garamond;\"><b>MACHINE TRANSLATION Using Seq2Seq Modelling<\/b><\/h1><\/center>","92a7e526":"<h1>Import the Required Libraries<\/h1>","da08fd38":"<h1> Load the Data <\/h1>","405a3b21":"<center><h1 style=\"font-size:200%; background-color:skyblue; color:black; padding:15px; font-family:Garamond;\"><b>Model Building<\/b><\/h1><\/center>","c6bee49b":"<h1> Text Cleaning \/ Preprocessing<\/h1>","4a3473a8":"<ul>\n    <li style=\"font-size:150%;\">We are using the RMSprop optimizer in this model as it\u2019s usually a good choice when working with recurrent neural networks.<\/li>\n    <li style=\"font-size:150%;\">Here I have used \u2018sparse_categorical_crossentropy\u2018 as the loss function. This is because the function allows us to use the target sequence as is, instead of the one-hot encoded format. One-hot encoding the target sequences using such a huge vocabulary might consume our system\u2019s entire memory.<\/li>\n               \n<\/ul>","b6e1073c":"<ul>\n    <li style=\"font-size:150%;\">ModelCheckpoint() function to save the model with the lowest validation loss. I personally prefer this method over early stopping.<\/li>\n<\/ul>","3b6d16b2":"<ul>\n    <li style=\"font-size:150%;\">It\u2019s time to encode the sentences. We will encode German sentences as the input sequences and English sentences as the target sequences. This has to be done for both the train and test datasets.<\/li>","e55303fa":"<center><h1 style=\"font-size:200%; background-color:white; color:lightwhite; padding:15px; font-family:Garamond;\">\u201cIf you talk to a man in a language he understands, that goes to his head. If you talk to him in his own language, that goes to his heart.\u201d<br> \u2013 <b>Nelson Mandela<\/b><\/h1><\/center>","3b9fce5d":"<ul>\n    <li style=\"font-size:150%;\">A Seq2Seq model requires that we convert both the input and the output sentences into integer sequences of fixed length.<\/li>\n    <li style=\"font-size:150%;\">Now, vectorize our text data by using Keras\u2019s Tokenizer() class. It will turn our sentences into sequences of integers. We can then pad those sequences with zeros to make all the sequences of the same length.<\/li>\n    <li style=\"font-size:150%;\">Prepare tokenizers for both the German and English sentences<\/li>\n<\/ul>","114dbc5a":"<ul>\n    <li style=\"font-size:150%;\">We tokenized the data \u2014 i.e., converted the text to numerical values. This allows the neural network to perform operations on the input data.<\/li>\n    <li style=\"font-size:150%;\">When we run the tokenizer, it creates a word index, which is then used to convert each sentence to a vector.<\/li>","daa50fbd":"<h1> Fit the Model<\/h1>","655d94e9":"<img src='https:\/\/miro.medium.com\/max\/875\/1*WWXJ0w6YByfPA9KKmDx2Ug.jpeg' style=\"width:1200px;height:600px;\">","639696e2":"<ul>\n    <li style=\"font-size:150%;\">The actual data contains over 150,000 sentence-pairs. However, we will use only the first 50,000 sentence pairs to reduce the training time of the model. You can change this number as per your system\u2019s computation power.<\/li>\n<\/ul>","a1614f91":"<center><h1 style=\"font-size:200%; background-color:skyblue; color:black; padding:15px; font-family:Garamond;\"><b>Let's start the Implementation<\/b><\/h1><\/center>","4ce07eca":"<center><h1> CONCLUSION<\/h1><\/center>\n<br>\n\n<ul>\n    <li style=\"font-size:150%;\">Our Seq2Seq model does a decent job. But there are several instances where it misses out on understanding the key words.<\/li>\n    <li style=\"font-size:150%;\">These are the challenges you will face on a regular basis in NLP. But these aren\u2019t immovable obstacles. We can mitigate such challenges by using more training data and building a better (or more complex) model.<\/li>\n<\/ul>","3f50f80f":"<ul>\n    <li style=\"font-size:150%;\">We will get rid of the punctuation marks and then convert all the text to lower case.<\/li>\n<\/ul>","ec180b1e":"<p style=\"font-size:150%;\">The objective is to Explain How Seq 2 Seq and LSTMs are used for Machine Translations using an example dataset of converting a German sentence to its English counterpart.<\/p>\n\n<h1> What is Seq2Seq Modelling ?<\/h1>\n\n<ul>\n    <li style=\"font-size:150%;\">Sequence-to-sequence learning (Seq2Seq) is about training models to convert sequences from one domain (e.g. sentences in English) to sequences in another domain (e.g. the same sentences translated to German). Our aim is to translate given sentences from German language to English.<\/li>\n    <li style=\"font-size:150%;\">Sequence-to-Sequence (seq2seq) models are used for a variety of NLP tasks, such as text summarization, speech recognition, DNA sequence modeling, among others.<\/li>\n    <li style=\"font-size:150%;\">Here, both the input and output are sentences. In other words, these sentences are a sequence of words going in and out of a model. This is the basic idea of Sequence-to-Sequence modeling. The figure below tries to explain this method.<\/li>\n<\/ul>\n\n<center><img src=\"https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2019\/01\/enc_dec_simple.png\"><\/center>\n\n<h2>  Here's how it works:<\/h2>\n\n<ul>\n    <li style=\"font-size:150%;\">Feed the embedding vectors for source sequences (German), to the encoder network, one word at a time.<\/li>\n    <li style=\"font-size:150%;\">Encode the input sentences into fixed dimension state vectors. At this step, we get the hidden and cell states from the encoder LSTM, and feed it to the decoder LSTM.<\/li>\n    <li style=\"font-size:150%;\">These states are regarded as initial states by decoder. Additionally, it also has the embedding vectors for target words (English).<\/li>\n    <li style=\"font-size:150%;\">Decode and output the translated sentence, one word at a time. In this step, the output of the decoder is sent to a softmax layer over the entire target vocabulary.<\/li>\n<\/ul>\n\n<h1> What is LSTM ?<\/h1>\n\n<ul>\n    <li style=\"font-size:150%;\">Long Short-Term Memory (LSTM) networks are a modified version of recurrent neural networks, which makes it easier to remember past data in memory. The vanishing gradient problem of RNN is resolved here. LSTM is well-suited to classify, process and predict time series given time lags of unknown duration. It trains the model by using back-propagation. In an LSTM network, three gates are present:<\/li>\n<\/ul>\n\n<center><img src=\"https:\/\/miro.medium.com\/max\/700\/1*MwU5yk8f9d6IcLybvGgNxA.jpeg\"><\/center>\n\n<ul>\n    <li style=\"font-size:150%;\"><b>Input gate \u2014<\/b> discover which value from input should be used to modify the memory. Sigmoid function decides which values to let through 0,1. and tanh function gives weightage to the values which are passed deciding their level of importance ranging from-1 to 1.<\/li>\n    <center><img src=\"https:\/\/miro.medium.com\/max\/500\/1*k1lxwjsxxn8O4BEiVlQNdg.png\"><\/center>\n    <li style=\"font-size:150%;\"><b>Forget gate \u2014<\/b> discover what details to be discarded from the block. It is decided by the sigmoid function. it looks at the previous state(ht-1) and the content input(Xt) and outputs a number between 0(omit this)and 1(keep this)for each number in the cell state Ct\u22121.<\/li>\n    <center><img src=\"https:\/\/miro.medium.com\/max\/500\/1*bQnecA5sy_eepNkL8I-95A.png\"><\/center>\n    <li style=\"font-size:150%;\"><b>Output gate \u2014<\/b> the input and the memory of the block is used to decide the output. Sigmoid function decides which values to let through 0,1. and tanh function gives weightage to the values which are passed deciding their level of importance ranging from-1 to 1 and multiplied with output of Sigmoid.<\/li>\n    <center><img src=\"https:\/\/miro.medium.com\/max\/700\/1*s8532P11PgGi2sZqikZ2kA.png\"><\/center>\n<\/ul>"}}