{"cell_type":{"7e0eea69":"code","f6bbca6c":"code","c405ae94":"code","b1036c0f":"code","a8fdaa50":"code","874efe05":"code","5c4d1e2d":"code","7c6d3141":"code","39d51adb":"code","d64bcab6":"code","3c85e0ae":"code","b64f6594":"code","4e9c2770":"code","6db1c223":"code","007c2fdd":"code","50cf0c49":"code","036f403b":"code","2ec76fde":"code","ee9c5b8f":"code","397e813d":"code","1d5c26cf":"code","6480e3a5":"code","fce3e6f6":"code","d85218c9":"code","d49c0ae5":"code","5003e6af":"code","b78bda39":"code","091f8a1a":"code","faab703d":"code","0743b415":"code","19dee715":"code","17346cf4":"code","7d36dc67":"code","0b3c29a2":"code","9f832310":"code","a62c0c10":"code","5c6f5bad":"code","1bb6aba9":"code","54fd4b29":"code","b6efed98":"code","d99e2527":"code","24df16f5":"code","82ac8774":"code","84162df2":"code","b020f5ec":"code","6491b018":"code","03c1199a":"code","92d70d18":"code","c4c424c4":"code","10905410":"markdown","03f871fd":"markdown","1f030b14":"markdown","89c50456":"markdown","1be024cb":"markdown","3297788d":"markdown","9aa8ccbd":"markdown","94947373":"markdown","c17e6d5a":"markdown","7641d921":"markdown","d5ba6ddf":"markdown","8581815b":"markdown","a0a53232":"markdown","34162d58":"markdown","3ca58d4e":"markdown"},"source":{"7e0eea69":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f6bbca6c":"df = pd.read_csv('\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ndf.head()","c405ae94":"df.isnull().sum()","b1036c0f":"df.info()","a8fdaa50":"df.describe()","874efe05":"import seaborn as sns\nimport matplotlib.pyplot as plt","5c4d1e2d":"sns.countplot(df['DEATH_EVENT'])","7c6d3141":"sns.countplot(df['DEATH_EVENT'], hue=df['diabetes'])","39d51adb":"plt.figure(figsize=(12,12))\nsns.heatmap(df.corr())","d64bcab6":"plt.hist(df['age'], bins=20)\nplt.show()","3c85e0ae":"sns.pairplot(df)","b64f6594":"df.columns","4e9c2770":"plt.hist(df['creatinine_phosphokinase'], bins=20)\nplt.show()","6db1c223":"sns.scatterplot(x=df['age'], y=df['serum_creatinine'], hue=df['DEATH_EVENT'])","007c2fdd":"sns.scatterplot(x=df['time'], y=df['serum_creatinine'], hue=df['DEATH_EVENT'])","50cf0c49":"sns.scatterplot(x=df['age'], y=df['platelets'], hue=df['DEATH_EVENT'])","036f403b":"sns.scatterplot(x=df['age'], y=df['serum_sodium'], hue=df['DEATH_EVENT'])","2ec76fde":"sns.countplot(df['diabetes'], hue=df['DEATH_EVENT'])","ee9c5b8f":"sns.countplot(df['high_blood_pressure'], hue=df['DEATH_EVENT'])","397e813d":"sns.countplot(df['sex'], hue=df['DEATH_EVENT'])","1d5c26cf":"sns.countplot(df['smoking'], hue=df['DEATH_EVENT'])","6480e3a5":"sns.countplot(df['anaemia'], hue=df['DEATH_EVENT'])","fce3e6f6":"X = df.drop('DEATH_EVENT', axis=1)\ny = df['DEATH_EVENT']","d85218c9":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, precision_recall_curve, cohen_kappa_score, f1_score","d49c0ae5":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 0, stratify=y)","5003e6af":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","b78bda39":"# example of grid searching key hyperparametres for logistic regression\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l2', 'l1']\nc_values = [100, 10, 1.0, 0.1, 0.01]\n# define grid search\ngrid = dict(solver=solvers,penalty=penalty,C=c_values)\n\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=3, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train, y_train)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","091f8a1a":"best_model1 = LogisticRegression(C=1.0, solver='liblinear', penalty = 'l1')\nbest_model1.fit(X_train, y_train)\ny_pred = best_model1.predict(X_test)","faab703d":"from sklearn.metrics import confusion_matrix,accuracy_score,roc_curve\n\nprint(confusion_matrix(y_test, y_pred))\nprint('Accuracy of our model is: ', accuracy_score(y_test, y_pred)*100)","0743b415":"# get importance\nimportance = best_model1.coef_[0]\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.bar([x for x in X.columns], importance)\nplt.xticks(rotation=90)\nplt.show()","19dee715":"from sklearn.ensemble import RandomForestClassifier","17346cf4":"rf1 = RandomForestClassifier()\nn_estimators = [100,200, 300, 400]\nmax_depth = [6,5,7]\nmin_samples_split = [8,9,7,6]\n\nhyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n              min_samples_split = min_samples_split)\n\ngridF = GridSearchCV(rf1, hyperF, cv = 5, verbose = 1, \n                      n_jobs = -1)\nbestF = gridF.fit(X_train, y_train)","7d36dc67":"# summarize results\nprint(\"Best: %f using %s\" % (bestF.best_score_, bestF.best_params_))","0b3c29a2":"rf_best = RandomForestClassifier(max_depth= 5, min_samples_split= 8, n_estimators= 300)","9f832310":"rf_best.fit(X_train, y_train)\ny_pred = rf_best.predict(X_test)\n\nprint(confusion_matrix(y_test, y_pred))\nprint('Accuracy of our model is: ', accuracy_score(y_test, y_pred))","a62c0c10":"features = pd.DataFrame()\nfeatures['Feature'] = X.columns\nfeatures['Importance'] = rf_best.feature_importances_\nfeatures.sort_values(by=['Importance'], ascending=False, inplace=True)\nfeatures.set_index('Feature', inplace=True)\n\nfeatures.plot(kind='bar', figsize=(20, 10))","5c6f5bad":"from sklearn.svm import SVC \n# defining parameter range \nparam_grid = {'C': [ 1, 10, 100], \n            'gamma': [ 0.1, 0.01, 0.001], \n            'kernel': ['rbf', 'poly']} \n\ngrid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3, cv=5, scoring='accuracy') \n\n# fitting the model for grid search \ngrid.fit(X_train, y_train)","1bb6aba9":"# print best parameter after tuning \nprint(grid.best_params_) \n\n# print how our model looks after hyper-parameter tuning \nprint(grid.best_estimator_)","54fd4b29":"y_pred_svm = grid.predict(X_test)\n\nprint(confusion_matrix(y_test, y_pred_svm))\nprint('Accuracy of our model is: ', accuracy_score(y_test, y_pred_svm))","b6efed98":"print(classification_report(y_test, y_pred_svm))","d99e2527":"from sklearn.ensemble import AdaBoostClassifier","24df16f5":"clf = AdaBoostClassifier(n_estimators=200, random_state=0)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)","82ac8774":"print(confusion_matrix(y_test, y_pred))\nprint('Accuracy of our model is: ', accuracy_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\nprint(f1_score(y_test, y_pred))","84162df2":"features = pd.DataFrame()\nfeatures['Feature'] = X.columns\nfeatures['Importance'] = clf.feature_importances_\nfeatures.sort_values(by=['Importance'], ascending=False, inplace=True)\nfeatures.set_index('Feature', inplace=True)\n\nfeatures.plot(kind='bar', figsize=(20, 10))","b020f5ec":"from sklearn.neighbors import KNeighborsClassifier","6491b018":"error_rate = []\n\nfor i in range(1, 40):\n  knn = KNeighborsClassifier(n_neighbors=i)\n  knn.fit(X_train, y_train)\n  pred_i = knn.predict(X_test)\n  error_rate.append(np.mean(pred_i != y_test))","03c1199a":"plt.figure(figsize=(12,12))\n\nplt.plot(range(1,40), error_rate, color='b', linestyle='dashed', marker='o', markerfacecolor='red')\nplt.title('Error rate vs K')","92d70d18":"knn_11 = KNeighborsClassifier(n_neighbors=11)\nknn_11.fit(X_train, y_train)\npred = knn_11.predict(X_test)\n\nprint(confusion_matrix(y_test, pred))\nprint(accuracy_score(y_test, pred))\nprint(classification_report(y_test, pred))\nprint(f1_score(y_test, pred))","c4c424c4":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\n#ROC for Ada Boost\nada_roc_auc = roc_auc_score(y_test, clf.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n\n#ROC for Random Forrest\nrf_roc_auc = roc_auc_score(y_test, rf_best.predict(X_test))\nrf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test, rf_best.predict_proba(X_test)[:,1])\n\n#ROC Curve for Random Forest & Ada Boost\nplt.figure()\nplt.plot(fpr, tpr, label='Ada Boost (area = %0.2f)' % ada_roc_auc)\nplt.plot(rf_fpr, rf_tpr, label='Random Forest (area = %0.2f)' % rf_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('ROC')\nplt.show()","10905410":"We can see from the above ROC plot that Random forests perform better than our ADA boost model","03f871fd":"Observations:\n1. The model gave zero weight to 'High B.P' and 'Smoking'\n2. The most important features according to our model are - 'time', 'age', 'ejection_fraction', 'sodium_creatinine'","1f030b14":"## Ada Boost Classifier","89c50456":"## 1. Logistic Regression Classifier","1be024cb":"Getting Feature Weights","3297788d":"So we have the following models which performed the best:\n1. Random Forests\n2. SVC\n3. Ada Boost","9aa8ccbd":"## Random Forest Classifier","94947373":"We see that KNN does not give a high accuracy as compared to other models described above","c17e6d5a":"Here in the case of Random Forest Classifier also we see that:\n1. The important features remain the same \n2. Features like 'Smoking', 'diabetes' are given less importance","7641d921":"## Support Vector Classifier","d5ba6ddf":"## In this notebook we shall explore 5 different algorithms and see what each one has to offer","8581815b":"## Model Building","a0a53232":"### We saw that our data is present in different scales and also there are not many outliers present in the dataset, so we can go forward with the simple Standard Scaler to try with.","34162d58":"## K Nearest Classifiers","3ca58d4e":"We see that the graph of error vs K levels around K values of 11-14 and again after K = 33, but values of K of 30+ often have high bias but less variance, so we should use K = 11"}}