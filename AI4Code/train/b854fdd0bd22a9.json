{"cell_type":{"176080e7":"code","582b1994":"code","4acaf9d6":"code","d251de38":"code","a2a35f49":"code","e6aca332":"code","de912e8b":"code","6929feea":"code","57e2ba24":"code","c70e9b76":"code","42510820":"code","b21ba09e":"code","2b001267":"code","f5e0ab89":"code","df3e4a55":"code","9524dcd6":"code","8bce4661":"code","9bd2929c":"code","1952f0eb":"code","4207b8d5":"code","b69c8567":"code","aa05a519":"code","41bf6b3a":"code","07f190ba":"code","efebf268":"code","32ec3600":"code","eddf052e":"code","119873e7":"code","6ed0a696":"code","898e8f7b":"code","e40b3258":"code","a8467252":"code","7d55188e":"code","6e32e554":"code","2f6bbea4":"code","d7b5304d":"code","490f749f":"code","0aaa1386":"code","bb2c6693":"code","57e8591c":"code","0f20a927":"code","c86e94b8":"code","d363b94c":"code","761c6dba":"code","322077cd":"code","2609a0dc":"code","b0e5e9ca":"code","91b68df6":"code","5d5230a1":"code","ecf9ef05":"code","d5eea69a":"code","0c08dc26":"code","dfa83970":"markdown"},"source":{"176080e7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","582b1994":"# raries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n# print(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\nimport itertools\nimport os\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder\nfrom sklearn.metrics import confusion_matrix\n\nfrom tensorflow import keras\nlayers = keras.layers\nmodels = keras.models\n\n\n# This code was tested with TensorFlow v1.8\nprint(\"You have TensorFlow version\", tf.__version__)","4acaf9d6":"import nltk\n\nimport warnings\nwarnings.filterwarnings('ignore')","d251de38":"from nltk.corpus import reuters\ntrain_documents, train_categories = zip(*[(reuters.raw(i), reuters.categories(i)) for i in reuters.fileids() if i.startswith('training\/')])\ntest_documents, test_categories = zip(*[(reuters.raw(i), reuters.categories(i)) for i in reuters.fileids() if i.startswith('test\/')])","a2a35f49":"documents = train_documents + test_documents\ncategories = train_categories + test_categories\ndata = {'documents':documents , 'categories':categories}","e6aca332":"print(type(categories))\nprint(type(documents))\n# print(type(data))\n# print(type(data[]))\nprint(len(documents))","de912e8b":"print(len(documents))\nprint(len(categories))\ndata  = pd.DataFrame(data)","6929feea":"data.head(50)","57e2ba24":"cat = np.unique(data['categories'])\nprint(len(cat))","c70e9b76":"data['categories'].value_counts()","42510820":"# print(len(train_documents))\n# print(len(test_documents))\nfrom sklearn.preprocessing import LabelEncoder\ngle = LabelEncoder()\ngenre_labels = gle.fit_transform(data['categories'])\ngenre_mappings = {index: label for index, label in \n                  enumerate(gle.classes_)}\ngenre_mappings","b21ba09e":"tokenize = keras.preprocessing.text.Tokenizer(num_words=1000, \n                                              char_level=False)","2b001267":"# tokenize.fit_on_texts(documents)","f5e0ab89":"import nltk \nimport string \nimport re \n\ndef text_lowercase(text): \n    return text.lower()\n\n# Remove numbers \ndef remove_numbers(text): \n    result = re.sub(r'\\d+', '', text) \n    return result \n\n# remove punctuation \ndef remove_punctuation(text): \n    translator = str.maketrans('', '', string.punctuation) \n    return text.translate(translator) \n\n# remove whitespace from text \ndef remove_whitespace(text): \n    return  \" \".join(text.split()) \n\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \n  \n# remove stopwords function \ndef remove_stopwords(text): \n    stop_words = set(stopwords.words(\"english\")) \n    word_tokens = word_tokenize(text) \n    filtered_text = [word for word in word_tokens if word not in stop_words] \n    return filtered_text \n  \nfrom nltk.stem.porter import PorterStemmer \nfrom nltk.tokenize import word_tokenize \nstemmer = PorterStemmer() \n  \n# stem words in the list of tokenised words \ndef stem_words(text): \n    word_tokens = word_tokenize(text) \n    stems = [stemmer.stem(word) for word in word_tokens] \n    return stems \n\n\n\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize \nlemmatizer = WordNetLemmatizer() \n# lemmatize string \ndef lemmatize_word(text): \n    word_tokens = word_tokenize(text) \n    # provide context i.e. part-of-speech \n    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens] \n    return lemmas \n\n\ndef preprocessing(text):\n    result = text_lowercase(text)\n#     result = remove_numbers(result)\n#     result = remove_punctuation(result)\n#     result = remove_whitespace(result)\n    result = remove_stopwords(result)\n    \n#     result = stem_words(result)\n#     result = lemmatize_word(result)\n    \n    \n    return result\n","df3e4a55":"#using data_list we can preprocess the sentences here\nimport time\nsentences = [sentence for sentence in documents]\nprint(len(sentences))\nstart_time  = time.time()\npreprocessed_sentences = [preprocessing(sentence) for sentence in sentences]\nend_time = time.time()\nprint(\"total time taken in preprocessing {}\".format(end_time-start_time))","9524dcd6":"print(len(preprocessed_sentences))\nprint(len(sentences[4]))\nprint(len(preprocessed_sentences[4]))\nprint(sentences[4])\nprint(preprocessed_sentences[4])","8bce4661":"# dictionary for words according to frequency\nfrom collections import defaultdict\nword_freq = defaultdict(int)\n\nfor sent in preprocessed_sentences:\n    for i in sent:\n        word_freq[i] += 1\nprint(len(word_freq))\n\nword_freq['subject']\nmost_freq_words = sorted(word_freq, key=word_freq.get, reverse=True)[:2000]","9bd2929c":"vocabulary = [(key,value) for key,value in word_freq.items() if(value>=5)]","1952f0eb":"\nprint(len(vocabulary))\nvocabulary[10]\n\ndef Convert(tup, di): \n    for a, b in tup: \n        di.setdefault(a, []).append(b) \n    return di \n\nvocabulary_dict = {}\nvocabulary_dict = Convert(vocabulary,vocabulary_dict)\nprint(len(vocabulary_dict))","4207b8d5":"EMBEDDING_FILE = '\/root\/input\/GoogleNews-vectors-negative300.bin.gz'\n!wget -P \/root\/input\/ -c \"https:\/\/s3.amazonaws.com\/dl4j-distribution\/GoogleNews-vectors-negative300.bin.gz\"\n","b69c8567":"from gensim.models import KeyedVectors\nword2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\nx = word2vec.word_vec(\"test\")","aa05a519":"import multiprocessing\n\nfrom gensim.models import Word2Vec\n\nimport logging  # Setting up the loggings to monitor gensim\nlogging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n\ncores = multiprocessing.cpu_count()\n\nprint(cores)","41bf6b3a":"from gensim.test.utils import get_tmpfile\nfrom gensim.models.callbacks import CallbackAny2Vec\n\n\nclass EpochSaver(CallbackAny2Vec):\n#      '''Callback to save model after each epoch.'''\n\n    def __init__(self, path_prefix):\n        self.path_prefix = path_prefix\n        self.epoch = 0\n\n    def on_epoch_end(self, model):\n        output_path = get_tmpfile('{}_epoch{}.model'.format(self.path_prefix, self.epoch))\n        model.save(output_path)\n        self.epoch += 1","07f190ba":"class EpochLogger(CallbackAny2Vec):\n    def __init__(self):\n        self.epoch = 0\n   \n    def on_epoch_begin(self, model):\n         print(\"Epoch #{} start\".format(self.epoch))\n\n    def on_epoch_end(self, model):\n        print(\"Epoch #{} end\".format(self.epoch))\n        self.epoch += 1\n\nepoch_logger = EpochLogger()\n# w2v_model = Word2Vec(common_texts, iter=5, size=10, min_count=0, seed=42, callbacks=[epoch_logger])","efebf268":"from gensim.models import Word2Vec\nimport time\nw2v_model = Word2Vec(min_count=5,\n                     window=2,\n                     size=300,\n                     sample=6e-5, \n                     alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20,\n                     workers=4)\n\nstart = time.time()\nw2v_model.build_vocab(preprocessed_sentences, progress_per=10000\/2)\ntotal_examples = w2v_model.corpus_count\nprint(total_examples)\n\nw2v_model.build_vocab([list(word2vec.vocab.keys())], update=True)\nw2v_model.intersect_word2vec_format(EMBEDDING_FILE, binary =True, encoding='utf8')\nw2v_model.train(preprocessed_sentences,total_examples=total_examples,epochs=30,callbacks=[epoch_logger])\n\nprint(\"total time taken {}\".format(time.time()-start))\n\n\nw2v_model.save('fine_tuned_30epochs_bbc_model')\n","32ec3600":"import os\nos.getcwd()","eddf052e":"from gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nw2v_model = Word2Vec.load(\"fine_tuned_30epochs_bbc_model\")\nvector  = w2v_model.wv['news']\nprint(vector.shape)\nw2v_model = w2v_model.wv","119873e7":"import time\nstart_time  = time.time()\npreprocessed_sentences = [preprocessing(sentence) for sentence in sentences]\nend_time = time.time()\nprint(\"total time taken in preprocessing {}\".format(end_time-start_time))","6ed0a696":"start = time.time()\ndata_matrix = []\nfor i in range(len(preprocessed_sentences)):\n    word_dict ={}\n    for index,j in enumerate(preprocessed_sentences[i]):\n            if(j not in word_dict):\n                word_dict[j]=1\n            else:\n                word_dict[j]+=1\n    arr = []\n    for word in w2v_model.vocab:\n        if(word in word_dict):\n            arr.append(word_dict[word])\n        else:\n            arr.append(0)\n        \n    \n    data_matrix.append(arr)\nend = time.time()\nprint('total time taken in building matrix {}'.format(end-start))\n","898e8f7b":"data_matrix = np.array(data_matrix)\nprint(data_matrix.shape)\nprint(data_matrix.dtype)\ndata_matrix = data_matrix.astype('int32')\nnp.save('data_matrix',data_matrix)","e40b3258":"data_matrix = np.load('data_matrix.npy')","a8467252":"len(most_freq_words)\nmost_freq_words[101]","7d55188e":"H_matrix = []\nstart = time.time()\nfor i in w2v_model.vocab:\n    arr = []\n    for word in most_freq_words:\n        similar = w2v_model.similarity(word, i)\n        if(similar>0):\n            arr.append(similar)\n        else:\n            arr.append(0)\n    H_matrix.append(arr)\n\n\n\n\n\n\n\nprint('time taken in generation {}'.format(time.time()-start))\nstart = time.time()\nH_matrix = np.array(H_matrix)\n\n\nprint('time taken in saving H_matrix {}'.format(time.time() - start))","6e32e554":"type(w2v_model.vocab)","2f6bbea4":"print(H_matrix.dtype)\nH_matrix = H_matrix.astype('float16')\nnp.save(\"H_matrix\",H_matrix)\nH_matrix = np.load('H_matrix.npy')","d7b5304d":"\nprint('data_matrix shape : {} and dtype of data : {}'.format(data_matrix.shape, data_matrix.dtype))\nprint('H_matrix shape : {} and dtype of H_matrix : {}'.format(H_matrix.shape, H_matrix.dtype))\n","490f749f":"import time\nstart = time.time()\nmatrix_z = data_matrix.dot(H_matrix)\nprint('time taken in saving H_matrix {}'.format (time.time() - start))","0aaa1386":"np.save('matrix_z_reuters_without_negSimilarity',matrix_z)\n\nprint('finally I get this matrix! huuu,,,')","bb2c6693":"import numpy as np","57e8591c":"matrix_z = np.load('matrix_z_reuters_without_negSimilarity.npy')\nmatrix_z = np.around(matrix_z,decimals=3)","0f20a927":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_squared_error,confusion_matrix, precision_score, recall_score, auc,roc_curve, classification_report\nfrom sklearn import ensemble, linear_model, neighbors, svm, tree, neural_network\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import svm,model_selection, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","c86e94b8":"from sklearn.preprocessing import MultiLabelBinarizer\n\nmlb = MultiLabelBinarizer()\ntrain_labels = mlb.fit_transform(categories)\n# test_labels = mlb.transform(test_categories)","d363b94c":"train_labels","761c6dba":"y = train_labels\nX = matrix_z","322077cd":"X_train, X_test, y_train, y_test = train_test_split(X , y , test_size=0.2, random_state=38)","2609a0dc":"# from sklearn.model_selection import GridSearchCV \n# from sklearn.svm import SVC\n# import time\n# start = time.time()\n# # defining parameter range \n# param_grid = {'C': [0.001,0.01, 0.1],  \n# #               'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n#               'kernel': ['linear']}  \n  \n# grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3,n_jobs=4) \n  \n# # fitting the model for grid search \n# grid.fit(X_train, y_train) \n\n# print('time taken in training SMV model is {}'.format(time.time()-start))\n# # print best parameter after tuning \n# print(grid.best_params_) \n  \n# # print how our model looks after hyper-parameter tuning \n# print(grid.best_estimator_) \n","b0e5e9ca":"%%time\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\n\nclassifier = OneVsRestClassifier(LinearSVC(),n_jobs=-1)\nclassifier.fit(X_train, y_train)","91b68df6":"%%time\nfrom sklearn.model_selection import KFold, cross_val_score\n\nkf = KFold(n_splits=10, random_state = 42, shuffle = True)\nscores = cross_val_score(classifier, X_train, y_train, cv = kf)","5d5230a1":"print('Cross-validation scores:', scores)\nprint('Cross-validation accuracy: {:.4f} (+\/- {:.4f})'.format(scores.mean(), scores.std() * 2))","ecf9ef05":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\npredictions = classifier.predict(X_test)\n\naccuracy = accuracy_score(y_test, predictions)\n\nmacro_precision = precision_score(y_test, predictions, average='macro')\nmacro_recall = recall_score(y_test, predictions, average='macro')\nmacro_f1 = f1_score(y_test, predictions, average='macro')\n\nmicro_precision = precision_score(y_test, predictions, average='micro')\nmicro_recall = recall_score(y_test, predictions, average='micro')\nmicro_f1 = f1_score(y_test, predictions, average='micro')\n\ncm = confusion_matrix(y_test.argmax(axis = 1), predictions.argmax(axis = 1))","d5eea69a":"print(\"Accuracy: {:.4f}\\nPrecision:\\n- Macro: {:.4f}\\n- Micro: {:.4f}\\nRecall:\\n- Macro: {:.4f}\\n- Micro: {:.4f}\\nF1-measure:\\n- Macro: {:.4f}\\n- Micro: {:.4f}\".format(accuracy, macro_precision, micro_precision, macro_recall, micro_recall, macro_f1, micro_f1))","0c08dc26":"import matplotlib.pyplot as plt\nimport seaborn as sb\nimport pandas as pd\n\ncm_plt = pd.DataFrame(cm[:73])\n\nplt.figure(figsize = (25, 25))\nax = plt.axes()\n\nsb.heatmap(cm_plt, annot=True)\n\nax.xaxis.set_ticks_position('top')\n\nplt.show()","dfa83970":"%%time\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\n\nclassifier = OneVsRestClassifier(LinearSVC())\nclassifier.fit(vectorised_train_documents, train_labels)"}}