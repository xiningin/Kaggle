{"cell_type":{"d6c6bb78":"code","362352db":"code","7dd518a4":"code","b3550071":"code","71885179":"code","2e9c934b":"code","8c0deda4":"code","a2851bbc":"code","7b5b05fd":"code","d572a81d":"code","b6707fcf":"code","2e48bc34":"code","a1af361c":"code","c67692a0":"code","cb13ce51":"code","52ce10a8":"code","c6a4bca3":"code","19d2d4e8":"code","2aada566":"code","23a6d894":"code","fb2025ef":"code","9f96f8ea":"code","91d9629b":"code","6246f164":"code","2ed6644d":"code","2fa1d09c":"code","4be6cad7":"code","83a0907c":"code","0e14de7a":"code","3db761a6":"code","28883f81":"code","af49befe":"code","a2c38ef9":"code","f78ea58e":"code","a374458b":"code","b6939abb":"code","0d7aa1ef":"code","fcf8ae66":"code","ccff8c5e":"code","b32dee37":"code","c202e143":"code","19c3065d":"code","cd6a5c2b":"code","51ace9af":"code","ed7cbe05":"code","5ece53ae":"code","663d9fc9":"code","2d3e90dd":"code","7ccad99a":"code","532f1afe":"code","70fc82fc":"code","1947815d":"code","45403c92":"code","7004f840":"code","d29ee89a":"code","afd901ef":"code","27fc6d5e":"code","ae5924f6":"code","5b99bc5f":"code","400a26a3":"code","af102e3e":"code","8ee8f30f":"code","f89fd168":"code","2c656c95":"code","497e3c22":"code","d220ad48":"code","a97ad220":"code","b076b60d":"code","ce7f30f6":"code","8b0dd29d":"code","5aa6d6a0":"code","2e85f5cb":"code","02d21ee1":"code","360afe90":"code","0bc4693d":"code","a4af6d71":"code","50b831ca":"code","b5280915":"code","61b9adf9":"code","a19474a3":"code","e42179b1":"code","70918614":"code","23b69838":"code","7a89054f":"code","f728a72d":"code","c28113fa":"code","25f5b403":"code","70d4b749":"code","4f0d4d49":"code","bd0eadf5":"code","3d9f7465":"code","3ddb5b72":"code","93ba1dd9":"code","0a848e1c":"code","c06d087e":"code","ac8e4ee4":"code","3d218415":"code","8f1a79ea":"code","57839ab0":"code","d387809a":"code","8187c3cf":"code","6f171eea":"code","16bdbe2f":"code","2f73135e":"code","69455072":"code","1b70b944":"code","d72935b1":"code","741fd949":"code","77483f0e":"code","baca4289":"code","6b1e0adc":"code","d64cdfa7":"code","62e2c341":"code","8afc85ce":"code","1f54cbbc":"code","74b59c4e":"code","6c8630e7":"code","8323a57a":"markdown","fe8df32d":"markdown","96fbeb3b":"markdown","46865d3e":"markdown","f492f21c":"markdown","0fd22888":"markdown","04112fed":"markdown","f17e07e2":"markdown","1c5d3166":"markdown","a997f99b":"markdown","42375dcf":"markdown","b64dd1e9":"markdown","4fced93a":"markdown","6f74ddad":"markdown","c8f77707":"markdown","061920d3":"markdown","e8129371":"markdown","354a37d1":"markdown","664457dd":"markdown","2278bb30":"markdown","4843292f":"markdown","059a861f":"markdown","7641c3c6":"markdown","5b7947c1":"markdown","a50819a6":"markdown","2a03be78":"markdown","cc9fa75f":"markdown","418d8294":"markdown","1311f2f9":"markdown","f183b8b0":"markdown","b1d6ad2c":"markdown","5213086e":"markdown","9106041f":"markdown","c7c9d0b0":"markdown","2a38b979":"markdown","0bb63e10":"markdown","c36428e6":"markdown","4ac2e19a":"markdown","9512125f":"markdown","83176155":"markdown","6b7b1224":"markdown","6c8e9a9b":"markdown","97191091":"markdown","2e5d09b4":"markdown","dae39d74":"markdown","26e8afcd":"markdown","b3c5b6be":"markdown","fbd05912":"markdown","1d85a5ca":"markdown","70f60e68":"markdown","392c1f94":"markdown","54324e0c":"markdown","927b78ad":"markdown","d804efc7":"markdown","3518a707":"markdown","85b6000e":"markdown","2c44d533":"markdown","18fd7571":"markdown","cb3286ea":"markdown","36e7037c":"markdown","f4e4a20e":"markdown","94d33208":"markdown","2cee8bb8":"markdown","578db03c":"markdown","1a3ec839":"markdown","434fa180":"markdown","271dd41a":"markdown","b85a0ed2":"markdown","cd1c8c59":"markdown","ff0fe35b":"markdown","27b97997":"markdown","6c72b803":"markdown","5739d01e":"markdown","fa5241f1":"markdown","da9ca49c":"markdown","d7021d8a":"markdown","359ad85c":"markdown","fcfe9ee7":"markdown","9d50ff19":"markdown","460e9970":"markdown","748f0dfc":"markdown","955c0d6a":"markdown","81f12a8b":"markdown","506ebe45":"markdown","e4531967":"markdown","ce4e4b3c":"markdown","11b0c0ce":"markdown","5e44a333":"markdown","e3d5b08f":"markdown","057f1a4d":"markdown","ac6e68aa":"markdown","fe0ad2fa":"markdown","be70f9e4":"markdown"},"source":{"d6c6bb78":"# Importing modules needed for processing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\npd.options.mode.chained_assignment = None","362352db":"train = \"..\/input\/edureka\/Edureka_Consumer_Complaints_train.csv\"\ntest = \"..\/input\/edureka\/Edureka_Consumer_Complaints_test.csv\"","7dd518a4":"# train = str(input(\"Enter the address and the name of the train file:-\\n\"))\n# test = str(input(\"Enter the address of the name of the test file:-\\n\"))","b3550071":"# Reading dataset from the system\n\ntrain_data = pd.read_csv(train, parse_dates=['Date received', 'Date sent to company'])\ntest_data = pd.read_csv(test, parse_dates=['Date received', 'Date sent to company'])\n\nmain_train_data = train_data # Saving the main train data in different variable just in case we need in future\nmain_test_data = test_data # Saving the main test data in different variable just in case we need in future","71885179":"train_data.head(5) # Looking the raw train data","2e9c934b":"test_data.tail(5) # Looking the test data","8c0deda4":"test_data.shape","a2851bbc":"# Looking for total missing values in each columns the train data\ntrain_miss = train_data.isnull().sum()\ntrain_miss.to_frame().transpose()","7b5b05fd":"# Looking for total missing values in the test data\ntest_miss = test_data.isnull().sum()\ntest_miss.to_frame().transpose()","d572a81d":"'''\nConverting the strings in \"Consumer complaint narrative\" to lower case. There are various reasons this is done like the user\nis frustrated so he\/she might have written the complaints in block letters or maybe it is his\/hers preferred writting style. \nBut, in most cases they serves no purpose while operating.\n'''\n\ntrain_data['Consumer complaint narrative'] = train_data['Consumer complaint narrative'].str.lower()\ntest_data['Consumer complaint narrative'] = test_data['Consumer complaint narrative'].str.lower()\n\n# Seperating categorical values\n\ntrain_cat_data = train_data.select_dtypes(include='object')\ntest_cat_data = test_data.select_dtypes(include='object')","b6707fcf":"# Here we can see how many unique values are there in the categorical dataset\n# This code will perform operation for the training dataset\n\ntrain_cat_data.nunique().to_frame().transpose()","2e48bc34":"# Here we can see how many unique values are there in the categorical dataset\n# This code will perform operation for the testing dataset\n\ntest_cat_data.nunique().to_frame().transpose()","a1af361c":"# Python code to display the most frequent categorical values in training data\n\ntrain_cat_data.mode()","c67692a0":"# Python code to display the most frequent categorical values in test data\n\ntest_cat_data.mode()","cb13ce51":"train_cat_data['Issue'].value_counts().head(10)","52ce10a8":"index = train_cat_data['Issue'].value_counts().index\nvalues = train_cat_data['Issue'].value_counts().values\nplt.figure(figsize=(15, 8))\nax = plt.axes()\nax.tick_params(width='1')\nax.set_facecolor(\"#7cb7e22f\")\nplt.bar(index, values, label='Issues', color='#12a4d9')\nplt.title('Issues raised by customers\\n', fontsize=15, fontstyle='italic')\nplt.xlabel('\\nIssues', fontsize=13)\nplt.ylabel('Frequency\\n', fontsize=13)\nplt.xticks(rotation=90)\nplt.legend()\nplt.show()","c6a4bca3":"index = train_cat_data['Issue'].value_counts().index[0:10]\nvalues = train_cat_data['Issue'].value_counts().values[0:10]\nplt.figure(figsize=(10, 10))\nax = plt.axes()\nax.set_facecolor('#f5f0e1')\nplt.barh(index, values, label='Issues', color='#ffc13b')\nplt.yticks(fontstyle='italic')\nplt.ylabel('Issues\\n', fontsize=13)\nplt.xticks(rotation=90, fontsize=12)\nplt.xlabel('\\nFrequency', fontsize=13)\nplt.title('Number of individual issues\\n', fontsize=15, fontstyle='italic')\nplt.legend()\nplt.show()","19d2d4e8":"train_cat_data['Product'].value_counts().to_frame().transpose()","2aada566":"product_data = train_cat_data[train_cat_data['Product'] == 'Mortgage']","23a6d894":"issues = product_data['Issue'].value_counts().index\nfreq = product_data['Issue'].value_counts().values\nissue_counts = pd.DataFrame({'Issues': issues, 'Frequency': freq})\nissue_counts","fb2025ef":"ax = plt.axes()\nax.set_facecolor('#e1dd72')\nplt.bar(train_cat_data.groupby(['Product']).size().index, \n        train_cat_data.groupby(['Product']).size().values, label='Products', color='#1b6535')\nplt.xlabel('\\nProduct', fontsize=12)\nplt.xticks(rotation=90, fontsize=12)\nplt.ylabel('Frequency\\n', fontsize=12)\nplt.title('Bar graph of products with highest complaints\\n', fontsize=15, fontstyle='italic')\nplt.legend()\nplt.show()","9f96f8ea":"company = train_cat_data['Company'].value_counts().index\nnum_of_complaints = train_cat_data['Company'].value_counts().values\ncompany_df = pd.DataFrame({'Company': company, 'Number of complaints': num_of_complaints})\ncompany_df.head(10)","91d9629b":"train_cat_data['Submitted via'].value_counts().to_frame()\n# Here we can see there are 6 methods by which the complaints have been submitted which can be easily visualized in a Pie-chart","6246f164":"slices = train_cat_data['Submitted via'].value_counts().values\nmedium = train_cat_data['Submitted via'].value_counts().index\ncolours = ['#daf2dc', '#ffcce7', '#81b7d2', '#4d5198', '#e75874', '#5c3c92']\nexplode = (0.06, 0, 0.4, 0, 0.09, 0.5)\nplt.pie(slices, labels=medium, explode=explode, colors=colours, shadow=True, autopct='%.1f%%', radius=3)\nplt.legend()\nplt.show()","2ed6644d":"state = train_cat_data.groupby(['State']).size().index\ncount = train_cat_data.groupby(['State']).size().values\nplt.figure(figsize=(20, 40))\nax = plt.axes()\nax.set_facecolor('#e5e5dc')\nplt.barh(state, count, color='#26495c')\nplt.title('Figure to show geographical distributions\\n', fontsize=15)\nplt.xlabel('\\nCounts', fontsize=15)\nplt.ylabel('States\\n', fontsize=15)\nplt.show()","2fa1d09c":"pd.set_option('display.max_columns', None)\nstate_count = pd.DataFrame(dict(States=state, Counts=count)).transpose()\nstate_count","4be6cad7":"# Let's make a new dataset for this operation\ntime_data = pd.DataFrame({})\ntime_data['Date'] = train_data['Date received']\ntime_data['Month Name'] = train_data['Date received'].dt.month_name()\ntime_data['Month Number'] = train_data['Date received'].dt.month\ntime_data['Day Name'] = train_data['Date received'].dt.day_name()\ntime_data['Day Number'] = train_data['Date received'].dt.day\ntime_data['Year'] = train_data['Date received'].dt.year\n#time_data['Month\/Year'] = time_data['Month Number'].map(str) + '-' + time_data['Year'].map(str)\n#time_data['Month\/Year'] = pd.to_datetime(time_data['Month\/Year']).dt.to_period('M')\ntime_data","83a0907c":"month_counts = time_data.groupby(['Month Number']).size()\nmonths = month_counts.index\ncounts = month_counts.values\nplt.figure(figsize=(12, 6))\nax = plt.axes()\nax.set_facecolor('#aed6dc')\nplt.bar(months, counts, label='Complaints', color='#6883bc')\nplt.grid(color='#141414')\nplt.title('Checking rise and fall of complaints with respect to months\\n', fontsize=15)\nplt.xticks(months, fontsize=12)\nplt.yticks(fontsize=12)\nplt.xlabel('\\nMonths', fontsize=15)\nplt.ylabel('Complaints\\n', fontsize=15)\nplt.legend(fontsize=12)\nplt.show()","0e14de7a":"format_date = time_data.groupby(['Date']).size().to_frame('Counts')\nformat_date\n#month_years = pd.Series(format_months.index)\n#complaint_counts = pd.Series(format_months.values)\n#temp_month_df = pd.DataFrame(dict(Month_group = month_years, Complaints = complaint_counts))","3db761a6":"fm = format_date['Counts'].resample('W').mean()","28883f81":"plt.figure(figsize=(20,10))\nax = plt.axes()\nax.set_facecolor('#32c8f312')\nfm.plot(color='#1868ae')\nplt.grid()\nplt.grid(which='minor', color='#d72631', alpha=0.2)\nplt.grid(which='major', color='#3a6b35', alpha=0.8)\nplt.xlabel('\\nTime line ->', fontsize='15')\nplt.ylabel('Rate of complaints\\n', fontsize='15')\nplt.show()","af49befe":"diff = month_counts.diff()\ndiff # Finding the difference between number of complaints with respect to month","a2c38ef9":"day_counts = time_data.groupby(['Day Name']).size()\n# Making a temporary dataframe\nday_df = pd.DataFrame()\nday_df['Day'] = day_counts.index\nday_df['Counts'] = day_counts.values\nday_name = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nday_df['Day'] = pd.Categorical(day_df['Day'], categories=day_name, ordered=True)\nday_df = day_df.sort_values('Day')\nday_df","f78ea58e":"plt.figure(figsize=(10, 4))\nplt.barh(day_df['Day'], day_df['Counts'], label='Complaints', color='#316879')\nplt.title('Complaints with respect to day of the week\\n', fontsize=15, fontstyle='italic')\nplt.xlabel('\\nDay', fontsize=13)\nplt.xticks(rotation=90, fontsize=12)\nplt.ylabel('Complaints\\n', fontsize=15)\nplt.yticks(fontsize=12, fontstyle='italic')\nplt.legend(fontsize=12)\nplt.show()","a374458b":"response_counts = train_cat_data['Company response to consumer'].value_counts()\nresponse = response_counts.index\ncounts = response_counts.values\nplt.figure(figsize=(15, 5))\nax = plt.axes()\nplt.barh(response, counts, color='#d2601a') # Color name is Orange\nplt.grid(color='#1d3c45')# Color name is Pine Green\nax.set_facecolor('#fff1e1') # Color name is Light Peach\nplt.xlabel('\\nCounts', fontsize=12)\nplt.ylabel('Responses\\n', fontsize=12)\nplt.yticks(rotation=20, fontsize='13')\nplt.title('Count of responses to complaints\\n', fontsize=17)\nplt.show()","b6939abb":"# Making a temporary dataset\nresponse = pd.DataFrame()\nresponse['Timely Response'] = train_cat_data['Timely response?']\nresponse['Dispute'] = train_cat_data['Consumer disputed?']\nresponse['States'] = train_cat_data['State']\nresponse","0d7aa1ef":"# Check how many timely responses were observed\nt_res = response['Timely Response'].value_counts()\nt_res","fcf8ae66":"# Check how many time customer disputed\nc_dis = response['Dispute'].value_counts()\nc_dis","ccff8c5e":"sns.countplot(x='Timely Response', data=response, palette='hls')\nplt.show()","b32dee37":"sns.countplot(x='Dispute', data=response, palette='hls')\nplt.show()","c202e143":"# Checking how many time customer did not dispute after timely response\n\nnd_check1 = len(response[(response['Timely Response'] == 'Yes') & (response['Dispute'] == 'No')])\nnd_check1","19c3065d":"# Checking how many times customer disputed after timely response\n\nnd_check2 = len(response[(response['Timely Response'] == 'Yes') & (response['Dispute'] == 'Yes')])\nnd_check2","cd6a5c2b":"# Checking how many times customer disputed after no timely response\n\nnd_check3 = len(response[(response['Timely Response'] == 'No') & (response['Dispute'] == 'Yes')])\nnd_check3","51ace9af":"# Checking how many times customer did not dispute after no timely response\n\nnd_check4 = len(response[(response['Timely Response'] == 'No') & (response['Dispute'] == 'No')])\nnd_check4","ed7cbe05":"(nd_check1 \/ (nd_check1 + nd_check2)) * 100","5ece53ae":"(nd_check4 \/ (nd_check3 + nd_check4)) * 100","663d9fc9":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import train_test_split as tts","2d3e90dd":"# Building a dataset by taking only the important columns we need for the operation.\n\nraw_text_df = train_cat_data[['Product', 'Consumer complaint narrative']]\nraw_text_df.head(5)","7ccad99a":"# Removing missing values\ntext_df = raw_text_df.dropna(axis=0).reset_index()\ntext_df.head(5)","532f1afe":"len(text_df)","70fc82fc":"# Create a new column 'category_id' with encoded categories \ntext_df['Product_id'] = text_df['Product'].factorize()[0]\nproduct_id_df = text_df[['Product', 'Product_id']].drop_duplicates()\nproduct_id_df","1947815d":"len(product_id_df)","45403c92":"text_df.head(20)","7004f840":"X_data = text_df['Consumer complaint narrative']\ny_data = text_df['Product']\nX_train, X_test, y_train, y_test = tts(X_data, y_data, test_size=0.25, random_state=5)","d29ee89a":"tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, ngram_range=(1, 2), stop_words='english')\nfitted_vectorizer = tfidf.fit(X_train)\ntfidf_vectorizer_vectors = fitted_vectorizer.transform(X_train)","afd901ef":"model = LinearSVC().fit(tfidf_vectorizer_vectors, y_train)","27fc6d5e":"model.score(tfidf_vectorizer_vectors, y_train)","ae5924f6":"new_complaint = \"\"\"The bank obtained the property through a foreclosure. To the best of my knowledge any efforts to date to sell the property have been by an online auction process. This process has not allowed for interior inspection prior to the auction bid and could be one of the reasons why the minimum amount required by the Bank has not resulted in a sale. The home has remained vacant since XXXX, XXXX. The Bank is not maintaining the property. As President of the XXXX XXXX in which this property is located and as a home owner who lives XXXX doors from this house, I 'm particularly concerned about the vacant status and lack of maintenance of this home. A letter was sent to JP Morgan Chase XXXX\/XXXX\/XXXX requesting they work with the XXXX XXXX to retain a local Real Estate Agent who could market the property for them. A reply was requested by XXXX\/XXXX\/XXXX. To date ( XXXX\/XXXX\/XXXX ) we have received no response. \n\"\"\"\nprint(model.predict(fitted_vectorizer.transform([new_complaint])))","5b99bc5f":"new_complaint = \"\"\"Impersonated an attorney or official\"\"\"\nprint(model.predict(fitted_vectorizer.transform([new_complaint])))","400a26a3":"new_complaint = \"\"\"Not given enough info to verify debt\"\"\"\nprint(model.predict(fitted_vectorizer.transform([new_complaint])))","af102e3e":"new_complaint = \"PayPal is randomly sending PayPal account holders debit cards without permission ( unless permission is buried in fine print ). Recipients of the card do n't know what it is, why we are receiving it, if it was fraudulently requested, etc. -- then we have to spend 20 minutes holding for PayPal customer service only to find out that PayPal randomly sends the cards to account holders whether or not they request it. It is absolutely ridiculous and PayPal should know better.\"\nprint(model.predict(fitted_vectorizer.transform([new_complaint])))","8ee8f30f":"# Let us have a quick look at the train data\nmain_train_data.head(2)","f89fd168":"main_train_data.isnull().sum().to_frame().transpose()","2c656c95":"processed_data = main_train_data","497e3c22":"fill_columns = ['Sub-product', 'Sub-issue', 'Company public response', 'Consumer consent provided?', 'State', 'ZIP code']\nprocessed_data[fill_columns] = processed_data[fill_columns].fillna('Not Available', axis=1)","d220ad48":"processed_data.head(5)","a97ad220":"processed_data = processed_data.drop(['Tags', 'Consumer complaint narrative', 'Complaint ID'], axis=1)","b076b60d":"processed_data.head(2)","ce7f30f6":"# processed_data['Day received'] = processed_data['Date received'].dt.day\nprocessed_data['Day received name'] = processed_data['Date received'].dt.day_name()\nprocessed_data['Week received'] = processed_data['Date received'].dt.week\nprocessed_data['Year received'] = processed_data['Date received'].dt.year\n# processed_data['Day sent to company'] = processed_data['Date sent to company'].dt.day\nprocessed_data['Day name sent to company'] = processed_data['Date sent to company'].dt.day_name()\nprocessed_data['Week sent to company'] = processed_data['Date sent to company'].dt.week\nprocessed_data['Year sent to company'] = processed_data['Date sent to company'].dt.year\nprocessed_data['Time interval'] = (processed_data['Date sent to company'] - processed_data['Date received']).dt.days","8b0dd29d":"# Dropping Date Columns\nprocessed_data2 = processed_data.drop(['Date received', 'Date sent to company'], axis=1)","5aa6d6a0":"processed_data2.dtypes.to_frame().transpose()","2e85f5cb":"# we will use LabelEncoder as lb as stated above\nobject_cols = processed_data2.select_dtypes(include='object').columns.tolist()\nobject_cols # Filtering out the columns which have categorical data","02d21ee1":"objects = ['Product', 'Sub-product', 'Issue', 'Sub-issue', 'Company public response', 'Company', 'State',\n           'ZIP code', 'Consumer consent provided?', 'Submitted via', 'Company response to consumer',\n           'Timely response?', 'Day received name', 'Day name sent to company']\ndispute = 'Consumer disputed?'","360afe90":"objects","0bc4693d":"# We will drop the Consumer complaint narrative column\n# First we will drop the unwanted columns\nprocessed_test_data = test_data.drop(['Tags', 'Consumer complaint narrative', 'Complaint ID'], axis=1)\n# Filling missing values with string \"Not Available\"\nfill_columns = ['Sub-product', 'Sub-issue', 'Company public response', 'Consumer consent provided?', 'State', 'ZIP code']\nprocessed_test_data[fill_columns] = processed_data[fill_columns].fillna('Not Available', axis=1)","a4af6d71":"# processed_test_data['Day received'] = processed_test_data['Date received'].dt.day\nprocessed_test_data['Day received name'] = processed_test_data['Date received'].dt.day_name()\nprocessed_test_data['Week received'] = processed_test_data['Date received'].dt.week\nprocessed_test_data['Year received'] = processed_test_data['Date received'].dt.year\n# processed_test_data['Day sent to company'] = processed_test_data['Date sent to company'].dt.day\nprocessed_test_data['Day name sent to company'] = processed_test_data['Date sent to company'].dt.day_name()\nprocessed_test_data['Week sent to company'] = processed_test_data['Date sent to company'].dt.week\nprocessed_test_data['Year sent to company'] = processed_test_data['Date sent to company'].dt.year\nprocessed_test_data['Time interval'] = (processed_test_data['Date sent to company'] - processed_test_data['Date received']).dt.days","50b831ca":"# Dropping Date Columns\nprocessed_test_data2 = processed_test_data.drop(['Date received', 'Date sent to company'], axis=1)","b5280915":"from sklearn.preprocessing import LabelEncoder\ncolumn_label = LabelEncoder()\nfor column in objects:\n    processed_data2[column] = column_label.fit_transform(processed_data2[column])\n    processed_test_data2[column] = column_label.fit_transform(processed_test_data2[column])\nprocessed_data2[dispute] = column_label.fit_transform(processed_data2[dispute])","61b9adf9":"processed_data2","a19474a3":"final_data = processed_data2","e42179b1":"y = final_data['Consumer disputed?']\nX = final_data.drop('Consumer disputed?', axis=1)","70918614":"y.value_counts()","23b69838":"# Using Smote process\nfrom imblearn.over_sampling import SMOTE","7a89054f":"sm = SMOTE()","f728a72d":"X_new, y_new = sm.fit_sample(X, y)","c28113fa":"print(\"Original X shape:- \", X.shape, \"\\nNew X shape:- \", X_new.shape)","25f5b403":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn.metrics import accuracy_score, r2_score, roc_auc_score, f1_score, confusion_matrix, precision_score, recall_score, classification_report","70d4b749":"kf = KFold(n_splits=5, shuffle=True)","4f0d4d49":"predictors = X_new.columns.tolist()\npredictors","bd0eadf5":"# Comparing single model using different features via. KFold\n# First we will check for the whole dataset before splitting\n\nX1 = X_new[['Product', 'Issue', 'Company', 'State', 'Submitted via', 'Timely response?', \n                  'Timely response?', 'Time interval']].values\nX2 = X_new[['Product', 'Sub-product', 'Issue', 'Sub-issue', 'Company', 'State', 'Submitted via',\n                  'Company response to consumer', 'Timely response?', 'Time interval']].values\nX3 = X_new[predictors].values\ny = y_new.values","3d9f7465":"'''# Let us do cross validation for the decision tree\n\ndef score_model(X, y, kf):\n    accuracy_scores = []\n    precision_scores = []\n    recall_scores = []\n    f1_scores = []\n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        dt = DecisionTreeClassifier()\n        dt.fit(X_train, y_train)\n        y_pred = dt.predict(X_test)\n        accuracy_scores.append(accuracy_score(y_test, y_pred))\n        precision_scores.append(precision_score(y_test, y_pred))\n        recall_scores.append(recall_score(y_test, y_pred))\n        f1_scores.append(f1_score(y_test, y_pred))\n    print(\"Accuracy: \", np.mean(accuracy_scores))\n    print(\"Precision: \", np.mean(precision_scores))\n    print(\"Recall: \", np.mean(recall_scores))\n    print(\"F1 Score: \", np.mean(f1_scores), \"\\n\")\n\nprint(\"Decision tree for split X1: \")\nscore_model(X1, y, kf)\nprint(\"Decision tree for split X2: \")\nscore_model(X2, y, kf)\nprint(\"Decision tree for split X3: \")\nscore_model(X3, y, kf)\n'''","3ddb5b72":"'''\n# Let us do cross validation for the random forest\n\ndef score_model(X, y, kf):\n    accuracy_scores = []\n    precision_scores = []\n    recall_scores = []\n    f1_scores = []\n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        rfc = RandomForestClassifier()\n        rfc.fit(X_train, y_train)\n        y_pred = rfc.predict(X_test)\n        accuracy_scores.append(accuracy_score(y_test, y_pred))\n        precision_scores.append(precision_score(y_test, y_pred))\n        recall_scores.append(recall_score(y_test, y_pred))\n        f1_scores.append(f1_score(y_test, y_pred))\n    print(\"Accuracy: \", np.mean(accuracy_scores))\n    print(\"Precision: \", np.mean(precision_scores))\n    print(\"Recall: \", np.mean(recall_scores))\n    print(\"F1 Score: \", np.mean(f1_scores), \"\\n\")\n\nprint(\"Random forest for split X1: \")\nscore_model(X1, y, kf)\nprint(\"Random forest for split X2: \")\nscore_model(X2, y, kf)\nprint(\"Random forest for split X3: \")\nscore_model(X3, y, kf)\n'''","93ba1dd9":"model_X_train, model_X_test, model_y_train, model_y_test = tts(X_new, y_new, test_size=0.25, random_state=2)","0a848e1c":"rfc_model = RandomForestClassifier(n_estimators=100, random_state=10)\nrfc_model.fit(model_X_train, model_y_train)","c06d087e":"rfc_model.score(model_X_train, model_y_train)","ac8e4ee4":"y_pred = rfc_model.predict(model_X_test)","3d218415":"print(\"Accuracy Score:- \", accuracy_score(model_y_test, y_pred))\nprint(\"R2 Score- \", r2_score(model_y_test, y_pred))\nprint(\"Precision:- \", precision_score(model_y_test, y_pred))\nprint(\"Recall:- \", recall_score(model_y_test, y_pred))\nprint(\"F1 score:- \", f1_score(model_y_test, y_pred))\nprint(\"\\nConfusion Matrix:- \\n\", confusion_matrix(model_y_test, y_pred))","8f1a79ea":"ft_imp = pd.Series(rfc_model.feature_importances_, index=[predictors]).sort_values(ascending=False)","57839ab0":"ft_imp","d387809a":"final_X_data = X_new.drop(['Timely response?'], axis=1)\nfinal_y_data = y_new","8187c3cf":"final_X_data.columns.tolist()","6f171eea":"from sklearn.model_selection import GridSearchCV # Importing grid search","16bdbe2f":"'''\nestimators = [10, 30, 100, 150, 200]\nparam_grid = dict(n_estimators=estimators)\nparam_grid # Selecting parameters\n'''","2f73135e":"# rfc_temp_model = RandomForestClassifier(random_state=10)\n# grid = GridSearchCV(rfc_temp_model, param_grid, cv=5)","69455072":"'''\ngrid_X_data = final_X_data.sample(10000, replace=True, random_state=2)\ngrid_y_data = final_y_data.sample(10000, replace=True, random_state=2)\n'''","1b70b944":"# grid.fit(grid_X_data, grid_y_data)","d72935b1":"'''\nprint(\"Best Parameter: \", grid.best_params_)\nscores = grid.cv_results_['mean_test_score']\nprint(scores)\n'''","741fd949":"'''\n# Elbow Graph\nplt.figure(figsize=(10, 4))\nplt.plot(estimators, scores)\nplt.xlabel(\"\\nn_estimators\")\nplt.ylabel(\"accuracy\\n\")\nplt.xlim(100, 300)\nplt.grid()\nplt.grid(which='minor', color='#d72631', alpha=0.2)\nplt.title(\"Elbow graph for Grid Search CV\")\nplt.show()\n'''","77483f0e":"final_X_train, final_X_test, final_y_train, final_y_test = tts(final_X_data, final_y_data, test_size=0.2, random_state=2)","baca4289":"final_model = RandomForestClassifier(n_estimators=170, random_state=10)\nfinal_model.fit(final_X_train, final_y_train)","6b1e0adc":"final_model.score(final_X_data, final_y_data)","d64cdfa7":"y_pred = final_model.predict(final_X_test)","62e2c341":"print(\"Accuracy:- \", accuracy_score(final_y_test, y_pred))\nprint(\"Precision:- \", precision_score(final_y_test, y_pred))\nprint(\"F1 score:- \",  f1_score(final_y_test, y_pred))\nprint(\"Recall:- \", recall_score(final_y_test, y_pred))\nprint(\"Confusion matrix:-\\n\", confusion_matrix(final_y_test, y_pred))","8afc85ce":"final_test_data = processed_test_data2.drop('Timely response?', axis=1)\nfinal_test_data","1f54cbbc":"final_y_pred = final_model.predict(final_test_data)","74b59c4e":"final_y_pred.shape","6c8630e7":"pd.Series(final_y_pred).to_csv('Final predictions.csv', index=False, header=None)","8323a57a":"### **Conclusion:-**\nSo, we finally completed programming a model to predict if a customer will dispute or not. Due to time constraint and hardware limitations I could not prepare a satisfactory model as I intended. We had to skip some important steps in LabelEncoding and cross validations. But, we completed this model at approximate 80% accuracy atleast on the train data and let's hope this model worked well on the test data.\n\nThank you Edureka! for assigning this project. (\u273f\u25e0\u203f\u25e0)","fe8df32d":"=============================***| 4. If there some products which receive a higher number of complaints |***===========================","96fbeb3b":"==========================***|9. Filtering the most common responses |***========================","46865d3e":"===================================***|6. How are the complaints submitted |***=================================","f492f21c":"#### We will try to use SMOTE. Let's see what we come up with.","0fd22888":"*Remove the triple quotes from the cells below to run the KFold Cross Validation on Decision tree and Random Forest Classifier*","04112fed":"Performing operations for Feature engineering and building a useful model hopefully. \u30fd(^\u25c7^*)\/","f17e07e2":"We will use Random Forest Classifier since the dataset is huge and also we oversampled it. Under this circumtance Logistic regression may give low accuracy and decision tree will not be able to take a proper decision unless properly tuned. But, let us look at it.","1c5d3166":"From this information we can see that \"Mortage\" has maximum number of complaints which is quite common in developed cities and countries.","a997f99b":"## Preparing Final Model:-","42375dcf":"#### *Using Grid Search now:-*","b64dd1e9":"From the figure above we most complaints are received on Wednesday compared to Tuesday. This is the total complaints from all the datapoints for various years and months on this specific days. (\u25d5\u203f\u25d5\u273f)","4fced93a":"### Let us prepare the data for modeling\n\nWe will drop unwanted columns if needed, drop missing rows or columns, adjust the date-time formats, etc.","6f74ddad":"Here we can see since that maximum complaints come in March.","c8f77707":"   ==========================\n   ***|2. Number of unique values in categorical columns\n   |***==========================","061920d3":"==========================***|8. Checking if the complaints rise in any specific month or day of the week|***========================","e8129371":"Performing operations to classify the products based on terms in \"Customer complaint narrative\" \u30fd(^\u25c7^*)\/","354a37d1":"Preparing Data","664457dd":"Answering questions respectively as mentioned:-","2278bb30":"This time we tried to write a sub issue from the test data and it predicted correctly.","4843292f":"### Model selection and evaluation","059a861f":"Checking the data types in the processed data before we start encoding.","7641c3c6":"   ============================***|\n    1. Analysing missing values in the columns\n   |***============================","5b7947c1":"**We will use Random State parameter from now on so that we do not come up with different values everytime we run our model.**","a50819a6":"Hence, the most frequent issue is \"Loan modification, collection, foreclosure\" (\u25d5\u203f\u25d5\u273f)","2a03be78":"### **Preparing the test dataset to predict if the customer will dispute.**","cc9fa75f":"Let us refine the dataset into useful informations.\n\n.............................................................................","418d8294":"# Part 1 - Data Exploration","1311f2f9":"#### *We will run a Random Forest Classifier to get a quick view of more details regarding how the model will work on the dataset and future datasets.*","f183b8b0":"=================***| 10. Checking if responding in timely manner alter the number of consumers thet disputed |***==============","b1d6ad2c":"##### CAUTION:- Due to hardware limitations in my system some processes took a long time to complete.","5213086e":"Hence, all companies do not receive the same number of complaints. Maximum number of complaints are received by \"Bank of America\". (\u25d5\u203f\u25d5\u273f)","9106041f":"The model predicted the product from the customer complaint narrative correctly.","c7c9d0b0":"The graph can be made a little clearer by reducing the number of issues we want to see. For our case let it be 10.\nSo, we want to see the frequency of top 10 issues.","2a38b979":"# Part 3 - Classification Models and Feature Engineering","0bb63e10":"Now, we will see if all companies receive same number of complaints. We can find this by using value counts like before or by using Matplotlib also.","c36428e6":"To check how many complaints each products have we can use value count to sort the number of times a products name is appeared in a dataset. This way we can find out if there is any product that has more number of complaints compared to others.","4ac2e19a":"Preparing test Data","9512125f":"Modify the address above and run the whole notebook. (\u3065\uff61\u25d5\u203f\u203f\u25d5\uff61)\u3065","83176155":"*Since, the number of rows was originally about 3lakhs and after over sampling it increased to a larger number, we should be using an estimator of around 200 or 300. But, we will stick around 170*","6b7b1224":"Hence, we can conclude that this model performs accurately. A little error may occur sometimes which would be a rare instance. (\u25d5\u203f\u25d5\u273f)","6c8e9a9b":"We can have a visual representation of the issues for a clearer picture. We will use Matplotlib \"Bar Graph\" to visualize the different issues.","97191091":"Our data is ready to cook. \uff61\u25d5\u203f\u25d5\uff61","2e5d09b4":"### Feature Engineering:","dae39d74":"A few columns have missing values","26e8afcd":"***Based on the data above there are 12 different types of products in this dataset.***","b3c5b6be":"From the above figure we can see that most of the complaints come in the month of March. However, this figure does not show the rise and fall of the number of complaints for every month for every years. This figure just gives a view of the significant rise and fall of the number of complaints among the 12 months. We can see that there is huge difference of complaints between month of February and March. Let's see how many more complaints are received in the month of March than February. (\u25d5\u203f\u25d5\u273f)","fbd05912":"#### Splitting Train and Test Data","1d85a5ca":"This program will perform the required task needed to solve the problem. \u30fd(^\u25c7^*)\/","70f60e68":"### Let's use Grid Search cross validation and Elbow graph to find the best parameters.","392c1f94":"Now, we will perform feature engineering. \u30fd(\u00b4\u25bd\uff40)\u30ce","54324e0c":"Based on the figure above we can see that most of the complaints were closed with an explanation. (\u25d5\u203f\u25d5\u273f)","927b78ad":"These scores are not that satisfactory so, I guess we can go a little further with feature importance and Grid Search CV.\n\nWe will also use Elbow graph to find at which paramter the accuracy will level out. That way we may reduce some complexities in the model.\n\n(\uff89\u25d5\u30ee\u25d5)\uff89*:\uff65\uff9f\u2727","d804efc7":"Now, we will find how which medium is used by the customers to file the complaints. We will do this by using a pie-chart because, there are only a few methods by which a customer can file a complaint and this will be easy to visualize by using a pie-chart.","3518a707":"If we need further exploration to find which are the issues raised by the \"Mortage\" product we can perform the following set of codes.","85b6000e":"Based on this data we can see that \"Timely response?\" column is the least significant column in the entire dataset as we have observed before. So, we will drop that column. Dropping that column may reduce our accuracy by a little factor but, we will try to compensate for that by changing some parameters in the final model.","2c44d533":" *With a little bit of more tuning this accuracy could have been increased a little further. But, due to hardware limitations I am stopping the designing of this model here.* \u30fd(\u3003\uff3e\u25bd\uff3e\u3003)\uff89","18fd7571":"So, these are the issues raised for the product \"Mortage\".","cb3286ea":"These are my default addresses of the files. These addresses can be different for different users.\n\nTrain data:-\nD:\\Data Science Projects\\Edureka Course\\Projects\\Mid Project 1\\Edureka! Data\\Edureka_Consumer_Complaints_train.csv\n\nTest data:-\nD:\\Data Science Projects\\Edureka Course\\Projects\\Mid Project 1\\Edureka! Data\\Edureka_Consumer_Complaints_test.csv\n\nEdit this data to match your system and copy-paste the address in the below cell.","36e7037c":"From the graph above we can by how much margin the complaints against \"Mortage\" have been filed. (\u25d5\u203f\u25d5\u273f)\nThat's a lot.","f4e4a20e":"=============================***|5. If all the companies receive the same number of complaints |***============================","94d33208":"This time we tried to predict the product from the customer narrative from the test data and it predicted correctly.","2cee8bb8":"Hence, we obtained the final predictions and saved our data in a csv file named \"Final predictions.csv\"","578db03c":"(\u3003^\u2207^)\uff89","1a3ec839":"Based on the data above we can clearly see that the most common issue with the customers is - \"Loan modification, collection, foreclosure\".\nWe can proceed further and look what other issues are common among the customers by applying the following line of code.","434fa180":"*For now we will go with Random Forest Classifier with split 3 i.e will take all the features that we built. That looks good for now compared to the decision tree.*","271dd41a":"We can also compare by how much margin the complaints against mortgage have been filed by using simple bar graph as follows.","b85a0ed2":"# Part 2 - Text Based Modeling","cd1c8c59":"We can have a look at more detailed count of the states in a mini dataset as follows:-","ff0fe35b":"I ran the test on 10000 samples, 20000 samples and lastly on 50000 samples. Upon using Grid Search on 50000 samples we get that the best parameter for number of decision trees or n_estimators should be about 260. But, in order to reduce the computational expense we will stick to the most common answer I obsereved with 10000 and 20000 sample. So, we will go with n_estimators = 170.","27b97997":"|***| Here we can see the top 10 issues raised by the customers.\nThere are 60,185 customers who raised the same issue i.e \"Loan modification,collection,foreclosure\"\nSimilarly, we can find the top raised issues in the test dataset. |***|","6c72b803":"#### CAUTION: This cross validation will take some time.","5739d01e":"### **Running classification again**","fa5241f1":"Value of new y will be equal to shape of new X train and it will be a series.","da9ca49c":"And, we will drop Consumer complaint narrative, Tags and Complaint ID column.","d7021d8a":"===================================***|7. Geographical distributions of the company |***=====================================","359ad85c":"The test data does not have the \"Consumer disputed?\" column. ","fcfe9ee7":"# Mid Program project\n# :::::::::::::::::::::::::::::\n\nFor POST GRADUATE PROGRAM IN DATA-SCIENCE using Python under Edureka! (\u273f\u25e0\u203f\u25e0)","9d50ff19":"   =====================================\n   ***|3. Top issues raised by customers\n   |***=================================","460e9970":"Let's see the rise and fall in complaints with respect to day of the week. For this I will use a line plot.","748f0dfc":"***Now, we will find which day of the week most the complains are received.***","955c0d6a":"From the above information we can say that maximum complaints were submitted via. web followed by referral, phone and so on. (\u25d5\u203f\u25d5\u273f)","81f12a8b":"**Based on the missing value of the test data, we cannot drop any rows with missing values from the test data. But, we can remove columns which we don't need. And the columns in the train data should be equal to the test data. We may need drop a few columns from the train data if needed.**","506ebe45":"### Let's go for feature importance.","e4531967":"We see a signifiant number of complaints by mid year 2015 (\u25d5\u203f\u25d5\u273f)","ce4e4b3c":"This time also we tried to enter a sub issue from the test data and it predicted correctly.","11b0c0ce":"We have missing values in both the test data and the train dataset. (\u25d5\u203f\u25d5\u273f)","5e44a333":"Based on the above figure we can see that most of the complaints were submitted from state \"California\" followed by Florida, Texas, New York and so on.","e3d5b08f":"Based on the values above we can assume that giving a timely response or not does not have a significant effect on the changes in the number of customer dispute. (\u25d5\u203f\u25d5\u273f)","057f1a4d":"Looks like the accuracy is pretty good.","ac6e68aa":"Remove the triple quotes to run the Grid Search. In order to save some computation time we will skip this step since we already ran Grid Search several times while building this model.","fe0ad2fa":"**We will perform the grid search on a smaller dataset to save computation time and try to estimate if that parameter will be good for the whole dataset** (\uff61\u25d5\u203f\u25d5\uff61)","be70f9e4":"### Splitting the data"}}