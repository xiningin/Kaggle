{"cell_type":{"15f42d99":"code","c40bc4c0":"code","b62e107a":"code","25fb3afe":"code","d8ba14ea":"code","d17fe13a":"code","1cef1157":"code","9c3195e0":"code","2b6ef2f4":"code","8490f04f":"code","4459b734":"code","a7fd95de":"code","c4ea9c44":"code","255100a7":"code","48541ee9":"code","b3a1b54a":"markdown","840beba8":"markdown","9bb7424f":"markdown"},"source":{"15f42d99":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, BatchNormalization, Dropout, LSTM, Activation\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom tensorflow.keras.utils import to_categorical\n\nfrom cv2 import imread, resize","c40bc4c0":"data = pd.read_csv('..\/input\/water-quality\/waterQuality1.csv')\n\ndata = data.drop('ammonia', axis = 1)\ndata = data.drop('flouride', axis = 1)\ndata = data.drop('selenium', axis = 1)\ndata = data.drop('uranium', axis = 1)\n\ndata = data[data['is_safe'] != '#NUM!']\ndata['is_safe'] = data['is_safe'].astype(int)\n\ncolumns = dict(zip(list(range(data.shape[1])), data.columns.values.tolist()))\nis_safe = data['is_safe']\n\ndata = pd.DataFrame(StandardScaler().fit_transform(data)).rename(columns = columns)\ndata['is_safe'] = is_safe\ndata = data.dropna()\ndata.head(6)","b62e107a":"X = data.drop('is_safe', axis = 1)\nY = data['is_safe']\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = 13)","25fb3afe":"def build_():\n    model = Sequential()\n    \n    model.add(Dense(256, input_dim = 16))\n    model.add(Activation('relu'))\n    model.add(Dense(128))\n    model.add(Activation('relu'))\n    model.add(Dense(128))\n    model.add(Activation('relu'))\n    model.add(Dense(64))\n    model.add(Activation('relu'))\n    model.add(Dense(16))\n    model.add(Activation('relu'))\n    model.add(Dense(2, activation = 'sigmoid'))\n    \n    model.compile(\n        loss = 'categorical_crossentropy',\n        optimizer = 'sgd',\n        metrics = [keras.metrics.Recall(name = 'recall')]\n    )\n    return model\n\n\nclass MLP():\n    def __init__(self, epochs, batch_size):\n        self.clf = KerasClassifier(build_, epochs = epochs, batch_size = batch_size)\n        \n    def fit_(self, x, y):\n        self.clf.fit(x, y, verbose = 0)\n    \n    def predict_(self, x, y):\n        self.p = self.clf.predict(x)\n        return np.sum(y == self.p) \/ len(y)\n        \n        \nclf = MLP(epochs = 250, batch_size = 32)\nclf.fit_(X_train, Y_train)\n\nclf.predict_(X_test, Y_test)","d8ba14ea":"path = '..\/input\/flowers-recognition\/flowers'\nclasses = np.array(os.listdir(path))\nfor i, j in enumerate(classes):\n    print(i, j)","d17fe13a":"X = []\nY = []\n\nfor i, j in enumerate(classes):\n    folder = path + '\/' + str(j)\n    for image in os.listdir(folder):\n        img = imread((os.path.join(folder, image)), 1)[...,::-1] \/ 255.0\n        img = resize(img, (100, 100))\n        X.append(img)\n        Y.append(i)","1cef1157":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 13)\n\nX_train = np.array(X_train)\nY_train = to_categorical(np.array(Y_train))\nX_test = np.array(X_test)","9c3195e0":"def build_():\n    model = Sequential()\n    \n    model.add(Conv2D(32, (3, 3), input_shape = (100, 100, 3)))\n    model.add(Activation('relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size = (2, 2)))\n\n    model.add(Conv2D(32, (3, 3)))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size = (2, 2)))\n\n    model.add(Conv2D(64, (3, 3)))\n    model.add(Activation('relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size = (2, 2)))\n    \n    model.add(Flatten())\n    model.add(Dense(64))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(5, activation = 'softmax'))\n\n    model.compile(\n            loss = 'categorical_crossentropy',\n            optimizer = 'rmsprop',\n            metrics = [keras.metrics.Recall(name = 'recall'), \n                       keras.metrics.Accuracy(name = 'accuracy')]\n        )\n\n    return model\n\n\nclass CNN():\n    def __init__(self, epochs, batch_size):\n        self.clf = KerasClassifier(build_, epochs = epochs, batch_size = batch_size)\n        \n    def fit_(self, x, y):\n        self.clf.fit(x, y, verbose = 0)\n    \n    def predict_(self, x, y):\n        self.p = self.clf.predict(x)\n        return np.sum(y == self.p) \/ len(y)\n        \n        \nclf = CNN(epochs = 25, batch_size = 16)\nclf.fit_(X_train, Y_train)\n\nclf.predict_(X_test, Y_test)","2b6ef2f4":"df = pd.read_csv('..\/input\/time-series-forecasting-with-yahoo-stock-price\/yahoo_stock.csv')\ndf = df.drop(['Volume', 'Adj Close'], axis = 1)\ndf.Date = pd.to_datetime(df['Date'])\ndf.set_index('Date', inplace = True)\n\ndf","8490f04f":"df.isnull().sum()","4459b734":"df[['High','Low','Open','Close']].plot(figsize = (16, 6))","a7fd95de":"sc = MinMaxScaler(feature_range=(0,1))\n\ndef load_data(datasetname, column, seq_len):\n    # A support function to help prepare datasets for an RNN\/LSTM\/GRU\n    data = datasetname.loc[:,column]\n\n    sequence_length = seq_len + 1\n    result = []\n    for index in range(len(data) - sequence_length):\n        result.append(data[index: index + sequence_length])\n    \n    result = np.array(result)\n\n    #Last 10% is used for validation test, first 90% for training\n    row = round(0.9 * result.shape[0])\n    train = result[:int(row), :]\n    np.random.shuffle(train)\n    x_train = train[:, :-1]\n    y_train = train[:, -1]\n    x_test = result[int(row):, :-1]\n    y_test = result[int(row):, -1]\n\n    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))  \n\n    return [x_train, y_train, x_test, y_test]","c4ea9c44":"X_train, Y_train, X_test, Y_test = load_data(df, 'Close', 50)\n\ndf['Close'][:int(len(df['Close']) * .9)].plot(figsize=(16,4),legend=True)\ndf['Close'][int(len(df['Close']) * .9):].plot(figsize=(16,4),legend=True)\nplt.legend(['Training set (First 90%)','Test set (Last 10%)'])\nplt.show()","255100a7":"model=Sequential()\n\nmodel.add(LSTM(25, activation = 'relu', input_shape = (X_train.shape[1],1)))\nmodel.add(Dense(10))\nmodel.add(Dense(1))\n\nmodel.compile(\n    loss = 'mean_squared_error',\n    optimizer = 'adam'\n)\n\n\nmodel.fit(X_train, Y_train, epochs=10, batch_size=32)","48541ee9":"prediction = model.predict(X_test)\n\n\nplt.plot(Y_test, label='Test Data') \nplt.plot(prediction, label='Prediction') \nplt.legend()\nplt.show() ","b3a1b54a":"3. \u0420\u0435\u043a\u0443\u0440\u0440\u0435\u043d\u0442\u043d\u044b\u0435 \u0441\u0435\u0442\u0438\n\n\u0420\u0435\u0448\u0438\u0442\u0435 \u043b\u044e\u0431\u0443\u044e \u0437\u0430\u0434\u0430\u0447\u0443 \u043f\u043e \u0432\u0430\u0448\u0435\u043c\u0443 \u0432\u044b\u0431\u043e\u0440\u0443, \u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u043c \u0443\u0441\u043b\u043e\u0432\u0438\u0435\u043c \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u0440\u0435\u043a\u0443\u0440\u0440\u0435\u043d\u0442\u043d\u043e\u0439 \u0441\u0435\u0442\u0438 (\u044d\u0442\u043e \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0433\u043e \u0440\u044f\u0434\u0430 \u0438\u043b\u0438 \u0437\u0430\u0434\u0430\u0447\u0430 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432)\n","840beba8":"1. \u041f\u043e\u043b\u043d\u043e\u0441\u0432\u044f\u0437\u043d\u044b\u0435 \u0441\u0435\u0442\u0438 (\u043c\u043d\u043e\u0433\u043e\u0441\u043b\u043e\u0439\u043d\u044b\u0439 \u043f\u0435\u0440\u0441\u0435\u043f\u0442\u0440\u043e\u043d)\n\n\u0420\u0435\u0448\u0438\u0442\u0435 \u0437\u0430\u0434\u0430\u0447\u0443 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0434\u0430\u043d\u043d\u044b\u0445, \u0441 \u043a\u043e\u0442\u043e\u0440\u044b\u043c\u0438 \u0432\u044b \u0440\u0430\u0431\u043e\u0442\u0430\u043b\u0438 \u0432 \u043b\u0430\u0431\u043e\u0440\u0430\u0442\u043e\u0440\u043d\u043e\u0439 \u2116 2, \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0441\u0440\u0430\u0432\u043d\u0438\u0442\u0435 \u0441 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u043c\u0438 \u0440\u0430\u043d\u0435\u0435","9bb7424f":"2. \u0421\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u044b\u0435 \u0441\u0435\u0442\u0438 \n\n\u0420\u0435\u0448\u0438\u0442\u0435 \u0437\u0430\u0434\u0430\u0447\u0443 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 (\u0435\u0441\u043b\u0438 \u0432 \u0432\u044b\u0431\u0440\u0430\u043d\u043d\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u043c\u043d\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u043e\u0432, \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043e\u0441\u0442\u0430\u0432\u0438\u0442\u044c, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, 5)\n"}}