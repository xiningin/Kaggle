{"cell_type":{"2bb213a8":"code","b4ed8180":"code","32c3806b":"code","ce4e3f7a":"code","d3da8503":"code","794175cc":"code","0d7d85d4":"code","678b7f7d":"code","115d8df7":"code","776013b4":"code","fae79846":"code","ac78d9ec":"code","6c60ab68":"code","73345bda":"code","e07d6e02":"code","cb9db197":"code","d7a28d8f":"code","c80f75ea":"code","5d1971b8":"code","86a1e3a2":"code","f8f4367e":"code","6301c60d":"code","55f952bf":"code","a2a5a06d":"code","8a7cf1f9":"code","4dbb135f":"code","320e6e81":"code","56ae751e":"code","35e64876":"code","1994f3f9":"code","85242982":"code","ac299e2a":"code","d4d8610b":"code","7c92d863":"code","ccc1c843":"code","de6470d1":"code","2047038d":"code","23cb33d2":"code","b45c2604":"code","beb9e6a7":"code","a32b975c":"code","6048851a":"code","feb6d0a6":"code","b477bbfe":"code","50f29706":"code","6fdc5b2b":"code","13038aaf":"code","832e3630":"code","aa8a4d1b":"code","ba9863a8":"code","45956177":"markdown","b7d18f82":"markdown","593e7e2c":"markdown","0e503718":"markdown","452789ea":"markdown","4487ad32":"markdown","3fff95fb":"markdown","874f7f84":"markdown","d636532a":"markdown"},"source":{"2bb213a8":"import warnings\nwarnings.filterwarnings('ignore')","b4ed8180":"!mkdir ~\/.kaggle","32c3806b":"! cp \"..\/input\/kaggle1\/kaggle.json\" ~\/.kaggle\/","ce4e3f7a":"!kaggle competitions download -c heberhackathon","d3da8503":"import zipfile\n\ndata = zipfile.ZipFile('.\/heberhackathon.zip')\ndata.extractall()","794175cc":"!dir","0d7d85d4":"import pandas as pd \nimport numpy as np\ndata = pd.read_csv('.\/sample_submission.csv')\ndata.head()","678b7f7d":"train_model = pd.read_pickle('.\/images_array_train.pkl')","115d8df7":"train_data = np.array(train_model)\ntrain_data.shape","776013b4":"test_model = pd.read_pickle('.\/images_array_test.pkl')","fae79846":"test_data = np.array(test_model)\ntest_data.shape","ac78d9ec":"target_model = pd.read_pickle('.\/target_train.pkl')","6c60ab68":"target_data = np.array(target_model)\ntarget_data.shape","73345bda":"train_data.dtype","e07d6e02":"target_data.dtype","cb9db197":"x_train = train_data.astype('float32')\nx_test = test_data.astype('float32')\nx_train \/= 255\nx_test \/= 255","d7a28d8f":"x_train.dtype","c80f75ea":"y_train = target_data","5d1971b8":"from tensorflow.keras.utils import to_categorical\n\nnum_classes = 10\ny_train = to_categorical(y_train, num_classes)","86a1e3a2":"print(y_train.shape)\nprint(y_train.dtype)\nprint(y_train)","f8f4367e":"print(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)","6301c60d":"import random\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(60,60))\nfor i in range(200):\n    plt.subplot(20,20,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    rand_no = random.randint(0,len(x_train))     \n    plt.imshow(x_train[rand_no], cmap='gray')\n    plt.xlabel([y_train[rand_no]])","55f952bf":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.optimizers import RMSprop,SGD,Adam,Nadam\nfrom tensorflow.keras.utils import to_categorical\nimport matplotlib.pyplot as plt\n%matplotlib inline","a2a5a06d":"import warnings\nwarnings.filterwarnings('ignore')","8a7cf1f9":"model1 = Sequential()\nmodel1.add(Conv2D(filters=160, kernel_size=(3,3), activation='relu', padding='same', input_shape=(28,28,3)))\nmodel1.add(MaxPooling2D())\nmodel1.add(Conv2D(filters=192, kernel_size=(3,3), activation='relu', padding='same'))\nmodel1.add(MaxPooling2D())\nmodel1.add(Conv2D(filters=224, kernel_size=(3,3), activation='relu', padding='same'))\nmodel1.add(MaxPooling2D())\nmodel1.add(Flatten())\nmodel1.add(Dense(134, activation='tanh'))\nmodel1.add(Dense(10, activation='softmax'))\n\nmodel1.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(0.001), metrics=['accuracy'])\nmodel1.summary()","4dbb135f":"history = model1.fit(x_train,y_train,batch_size=1,epochs=20,shuffle=True)","320e6e81":"score = model1.predict(x_test,verbose=1)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","56ae751e":"sc = model1.evaluate(x_train, y_train, verbose=1)\nprint('Test loss:', sc[0])\nprint('Test accuracy:',sc[1])","35e64876":"erm =[]\nfor i in range(0,len(score)):\n    erm.append((np.argmax((score[i]>0.5)*1)))\ny_prediction = np.array(erm)\ny_prediction","1994f3f9":"a = []\nfor i in range(3000):\n    a.append(i)","85242982":"submission = pd.DataFrame()\nsubmission['id'] = a\nsubmission['class'] = y_prediction\nsubmission.to_csv('submissionshuffle3.csv', index=False)\nprint(submission.head(8))\nprint(data)","ac299e2a":"submission.nunique()","d4d8610b":"submission['class'].plot(kind='hist')","7c92d863":"import datetime\nimport tensorflow as tf\nimg_rows, img_cols = 28, 28","ccc1c843":"def train_model(model,train,test,num_classes):\n    \n    model.compile(optimizer=tf.keras.optimizers.Adam(0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n\n \n    model.fit(x_train,y_train,batch_size=10,epochs=10,verbose=2)\n  \n    print('--------------------------------------------------------------------------------')\n    print('\\n')\n  \n  \n    score=model.evaluate(x_train,y_train,verbose=0)\n  \n    print('test loss ',score[0])\n    print('test accuracy ',score[1])","de6470d1":"train_model(model1,[x_train,y_train],[x_test,y_train],10)","2047038d":"erm =[]\nfor i in range(0,len(score)):\n    erm.append((np.argmax((score[i]>0.5)*1)))\ny_prediction3 = np.array(erm)\ny_prediction3","23cb33d2":"a = []\nfor i in range(3000):\n    a.append(i)","b45c2604":"submission = pd.DataFrame()\nsubmission['id'] = a\nsubmission['class'] = y_prediction\nsubmission.to_csv('submissionTLnew.csv', index=False)\nprint(submission.head(8))\nprint(data)","beb9e6a7":"submission.nunique()","a32b975c":"submission['class'].plot(kind='hist')","6048851a":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential","feb6d0a6":"data_augmentation = keras.Sequential(\n  [\n    layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\", \n                                                 input_shape=(28, \n                                                              28,\n                                                              3)),\n    layers.experimental.preprocessing.RandomRotation(0.1),\n    layers.experimental.preprocessing.RandomZoom(0.1),\n  ]\n)","b477bbfe":"model2 = Sequential([\n    data_augmentation,\n    layers.Conv2D(filters=228, kernel_size=(3,3), activation='relu', padding='same'),\n    layers.MaxPooling2D(),\n    layers.Conv2D(filters=192, kernel_size=(3,3), activation='relu', padding='same'),\n    layers.MaxPooling2D(),\n    layers.Conv2D(filters=160, kernel_size=(3,3), activation='relu', padding='same'),\n    layers.MaxPooling2D(),\n    layers.Flatten(),\n    layers.Dense(134, activation='tanh'),\n    layers.Dense(10, activation='softmax')\n])\n\nmodel2.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(0.001), metrics=['accuracy'])\n#model2.summary()\nprint('||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||')\nhistory2 = model2.fit(x_train,y_train,batch_size=10,epochs=20,shuffle=True)","50f29706":"model2 = train_model(model2,[x_train,y_train],[x_test,y_train],10)","6fdc5b2b":"erm =[]\nfor i in range(0,len(score)):\n    erm.append((np.argmax((score[i]>0.5)*1)))\ny_predictionn = np.array(erm)\ny_predictionn","13038aaf":"a = []\nfor i in range(3000):\n    a.append(i)","832e3630":"submission = pd.DataFrame()\nsubmission['id'] = a\nsubmission['class'] = y_predictionn\nsubmission.to_csv('submissionaug1.csv', index=False)\nprint(submission.head(8))\nprint(data)","aa8a4d1b":"submission.nunique()","ba9863a8":"submission['class'].plot(kind='hist')","45956177":"## Understand what kind of images","b7d18f82":"## Converting greyscale and datatype","593e7e2c":"# Train model","0e503718":"# Test model","452789ea":"# Target Data","4487ad32":"# Using Data augmentation and transfer learning","3fff95fb":"### Maheshvaran S\n### 205229119","874f7f84":"# Using Transfer Learning","d636532a":"# Creating Model"}}