{"cell_type":{"ca1d78cd":"code","0010553e":"code","429e4513":"code","2d7e8d86":"code","8bd74468":"code","80f76734":"code","e2d100d8":"code","d22e7ad0":"code","994f09b6":"code","6c33893d":"code","592da934":"code","02835729":"code","c04d2907":"code","3dab5391":"code","6b927281":"code","6061585f":"code","c00afc8e":"code","e5e65d4d":"code","a3a75c88":"code","2c083a9c":"code","8e48d582":"code","2cd72a89":"code","f7f4a4f0":"code","69f811f8":"code","67081c17":"code","43c86278":"code","2924b703":"code","fd0a48b9":"code","9ff965f1":"code","1a870404":"code","4cb3778c":"code","f9d7297b":"code","4236f5f2":"code","879eb557":"code","51da29b4":"markdown","0f44f610":"markdown","361a405e":"markdown","7e7ac44e":"markdown","ef89f66a":"markdown","333f7d9a":"markdown","ab145879":"markdown","80402ef0":"markdown","7b54abbf":"markdown"},"source":{"ca1d78cd":"tez_path = '..\/input\/tez-modified-tqdm\/'\neffnet_path = '..\/input\/pytorch-efficientnet'\nimport sys\nsys.path.append(tez_path)\nsys.path.append(effnet_path)\nsys.path.append('..\/input\/multistartifiedkfold')","0010553e":"import albumentations\nimport pandas as pd\nimport plotly.express as px\nimport seaborn as sns\nimport tez\nimport torch\nfrom tqdm.notebook import tqdm\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torchaudio\nimport librosa\nimport random\nimport tez\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport audioread\nimport cv2\nfrom sklearn.metrics import label_ranking_average_precision_score\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nfrom efficientnet_pytorch import EfficientNet\nimport soundfile\nfrom pathlib import Path\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","429e4513":"path = Path('..\/input\/rfcx-species-audio-detection')","2d7e8d86":"df = pd.read_csv(path\/'train_tp.csv')","8bd74468":"df['recording_id'] = df['recording_id']","80f76734":"files = df.recording_id.tolist()","e2d100d8":"print(df.shape)\ndf.head()","d22e7ad0":"fnames = df.recording_id.unique().tolist()\ndf_gr = df.groupby(['recording_id'])","994f09b6":"bird_dict = {}\nfor fn in tqdm(fnames):\n    lbls = np.zeros(24)\n    temp = df_gr.get_group(fn)\n    sps = temp.species_id.unique()\n    for ss in sps:\n        lbls[ss] = 1\n    bird_dict[fn] = lbls","6c33893d":"bird_df = pd.DataFrame.from_dict(bird_dict,orient='index').reset_index()\nbird_df.columns = ['recording_id'] + ['species_id_'+str(x) for x in range(24)]","592da934":"bird_df.head()","02835729":"df_agg = df.groupby(['recording_id']).agg({'t_min':lambda x :min(x),'t_max':lambda x :max(x)}).reset_index()","c04d2907":"df_agg['duration'] = df_agg['t_max'] - df_agg['t_min']\ndf_agg['duration'] = df_agg['duration'].apply(lambda x: x+abs(x-10) if x<=3 else x)","3dab5391":"trn_df = bird_df.merge(df_agg,on='recording_id',how='left')","6b927281":"trn_df['recording_id'] = '..\/input\/rfcx-species-audio-detection\/train\/' +trn_df['recording_id'] + '.flac'","6061585f":"trn_df.sample(n=10)","c00afc8e":"tar_cols = ['species_id_'+str(x) for x in range(24)]","e5e65d4d":"import librosa\nimport cv2,os\n#from https:\/\/www.kaggle.com\/daisukelab\/creating-fat2019-preprocessed-data\ndef mono_to_color(X, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6):\n    # Stack X as [X,X,X]\n#     X = np.stack([X, X, X], axis=-1)\n\n    # Standardize\n    mean = mean or X.mean()\n    X = X - mean\n    std = std or X.std()\n    Xstd = X \/ (std + eps)\n    _min, _max = Xstd.min(), Xstd.max()\n    norm_max = norm_max or _max\n    norm_min = norm_min or _min\n    if (_max - _min) > eps:\n        # Normalize to [0, 255]\n        V = Xstd\n        V[V < norm_min] = norm_min\n        V[V > norm_max] = norm_max\n        V = 255 * (V - norm_min) \/ (norm_max - norm_min)\n        V = V.astype(np.uint8)\n    else:\n        # Just zero\n        V = np.zeros_like(Xstd, dtype=np.uint8)\n    return V\n\ndef build_spectrogram(path,offset,duration=15):\n    y, sr = librosa.load(path,offset=np.floor(offset),duration=np.ceil(duration))\n    total_secs = y.shape[0] \/ sr\n    M = librosa.feature.melspectrogram(y=y, sr=sr)\n    M = librosa.power_to_db(M)\n    #print(M.shape)\n    M = mono_to_color(M)\n    if M.shape[1]<600:\n        new_img = np.zeros((128,600))\n        new_img[:M.shape[0],:M.shape[1]] = M\n        M = new_img\n    else:\n        M = M\n    return M[:,:600]","a3a75c88":"n = random.randint(0,400)\nimg = build_spectrogram(trn_df.iloc[n]['recording_id'],offset=int(trn_df.iloc[n]['t_min']),duration=15);img.shape","2c083a9c":"plt.figure(1,figsize=(10,6))\nplt.imshow(img,cmap='inferno');","8e48d582":"trn_df = trn_df.sample(frac=1.,random_state = 2020)\ntrn_df['kfold'] = -1\ny = trn_df[tar_cols].values\nkf = MultilabelStratifiedKFold(n_splits=5,random_state = 2020,shuffle = True)\nfor fold ,(trn_,val_ )in enumerate(kf.split(X=trn_df,y=y)):\n    trn_df.loc[val_,'kfold'] = fold","2cd72a89":"trn_df.to_csv('rain_forest_train_kfold.csv',index=False)","f7f4a4f0":"px.violin(data_frame=trn_df,x='duration',title='duration',box=True)","69f811f8":"IMAGE_SIZE = 128\ntrain_aug = albumentations.Compose(\n    [\n        #albumentations.Resize(128, 600,p=1.0),\n        albumentations.Normalize(\n            mean=[0.485],\n            std=[0.229],\n            max_pixel_value=255.0,\n            p=1.0,\n        ),\n    ],\n    p=1.0,\n)\n\nvalid_aug = albumentations.Compose(\n    [\n        #albumentations.Resize(128, 600, p=1.0),\n        albumentations.Normalize(\n            mean=[0.485],\n            std=[0.229],\n            max_pixel_value=255.0,\n            p=1.0,\n        ),\n    ],\n    p=1.0,\n)","67081c17":"class AudioDataset:\n    def __init__(self,audio_paths,targets,offset,duration,augmentations=None,channel_first=False,grayscale=True):\n        self.audio_paths = audio_paths\n        self.targets = targets\n        self.offset = offset\n        self.duration = duration\n        self.augmentations = augmentations\n        \n    def __len__(self):\n        return len(self.audio_paths)\n    \n    def __getitem__(self,item):\n        targets = self.targets[item]\n        image = build_spectrogram(self.audio_paths[item],self.offset[item],10)\n        image = np.array(image)\n        #print(image.shape)\n        if self.augmentations is not None:\n            augmented = self.augmentations(image=image)\n            image = augmented[\"image\"]\n        image = np.nan_to_num(image)\n        image_tensor = torch.tensor(image)\n        image_tensor = image_tensor.unsqueeze(0)\n        return {\"image\": image_tensor,\n                \"targets\": torch.tensor(targets,dtype=torch.float)}","43c86278":"FOLD = 2","2924b703":"df_train,df_valid = trn_df[trn_df.kfold!=FOLD],trn_df[trn_df.kfold==FOLD]\ndf_train = df_train.reset_index(drop=True)\ndf_valid = df_valid.reset_index(drop=True)\ntrain_targets = df_train[tar_cols].values\nvalid_targets = df_valid[tar_cols].values","fd0a48b9":"train_dataset = AudioDataset(df_train.recording_id,\n                             train_targets,\n                             offset=df_train['t_min'].values,\n                             duration=df_train['duration'].values,\n                             augmentations=train_aug)\nvalid_dataset = AudioDataset(df_valid.recording_id,\n                             valid_targets,\n                             offset=df_valid['t_min'].values,\n                             duration=df_valid.duration.values,\n                             augmentations=valid_aug)","9ff965f1":"plt.figure(1,figsize=(10,6))\nplt.imshow(valid_dataset[5]['image'].numpy()[0,:,:],cmap='inferno');","1a870404":"wp3 = '..\/input\/efficientnet-pytorch\/efficientnet-b3-c8376fa2.pth'","4cb3778c":"class SpeciesModel(tez.Model):\n    def __init__(self):\n        super().__init__()\n\n        self.effnet = EfficientNet.from_pretrained(\"efficientnet-b3\",weights_path=wp3)\n\n        self.effnet._conv_stem.in_channels = 1\n        weight = self.effnet._conv_stem.weight.mean(1, keepdim=True)\n        self.effnet._conv_stem.weight = torch.nn.Parameter(weight)\n\n        self.dropout = nn.Dropout(0.1)\n        self.out = nn.Linear(1536, 24)\n        self.step_scheduler_after = \"epoch\"\n        self.step_scheduler_metric = \"valid_label_rank_avg_prec_sc\"\n        \n\n    def monitor_metrics(self, outputs, targets):\n        outputs = outputs.cpu().detach().numpy()\n        targets = targets.cpu().detach().numpy()\n        \n        return {\"label_rank_avg_prec_sc\": label_ranking_average_precision_score(targets,outputs)}\n\n    def fetch_optimizer(self):\n        opt = torch.optim.Adam(self.parameters(), lr=1e-3)\n        return opt\n\n    def fetch_scheduler(self):\n        rlr = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            self.optimizer,\n            verbose=True,\n            factor=0.7,\n            mode=\"max\",\n            patience=2,\n            threshold=0.01,\n        )\n        return rlr\n\n    def forward(self, image, targets=None):\n        batch_size, _, _, _ = image.shape\n\n        x = self.effnet.extract_features(image)\n        x = F.adaptive_avg_pool2d(x, 1).reshape(batch_size, -1)\n        outputs = self.out(self.dropout(x))\n        if targets is not None:\n            loss = nn.BCEWithLogitsLoss()(\n                outputs, targets.type_as(outputs)\n            )\n            metrics = self.monitor_metrics(outputs, targets)\n            return outputs, loss, metrics\n        return outputs, None, {}","f9d7297b":"model = SpeciesModel()","4236f5f2":"from tez.callbacks import EarlyStopping\nes = EarlyStopping(\n    monitor=\"valid_label_rank_avg_prec_sc\", model_path=\"model.bin\", patience=5, mode=\"max\"\n    )","879eb557":"model.fit(\n        train_dataset,\n        valid_dataset=valid_dataset,\n        train_bs=64,\n        valid_bs=16,\n        device=\"cuda\",\n        epochs=10,\n        callbacks=[es],\n        fp16=True)","51da29b4":"## Species Audio Detection Model","0f44f610":"## Returns spectogram","361a405e":"## Converting Labels to OneHotEncoded targets","7e7ac44e":"## MultiStratifiedKfold","ef89f66a":"## Processing csv files","333f7d9a":"## RainForest Species Audio Detection Pytorch Starter","ab145879":"## Audio Dataset","80402ef0":"### Let's check a valid_dataset sample:","7b54abbf":"**Special Thanks to [Abhishek Thakur](https:\/\/www.kaggle.com\/abhishek) for tez Library which makes traing faster and Still pretty close to raw pytorch code**"}}