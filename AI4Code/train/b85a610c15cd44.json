{"cell_type":{"21c774fe":"code","ba0d4715":"code","3d98e106":"code","d1e3730c":"code","4d25aff7":"code","6f2c279c":"code","cb6b0b6e":"code","a420c137":"code","5a6ec72c":"code","089fe671":"code","c1254b67":"code","9877354f":"code","a831cc2a":"code","cba7375b":"code","f81b4bea":"code","cefc5b3d":"code","ca73ec59":"code","79eea67b":"code","a930fbc9":"code","e130e6f5":"code","c68f21a0":"code","3c3b4e12":"code","3d495f8b":"code","fdad2ea8":"code","8f60182a":"code","a4c8a29d":"code","ecfd7576":"code","48acb984":"code","59289b3a":"code","057b6aaf":"code","9b598058":"code","fbf4f824":"markdown","5949a52e":"markdown","8c5e35f7":"markdown","3a35768f":"markdown","2c69e526":"markdown","13402694":"markdown","e3f908eb":"markdown","30106f69":"markdown","2f92e0be":"markdown","88d38a71":"markdown","e819379b":"markdown","0a7054a6":"markdown","d64a89a6":"markdown"},"source":{"21c774fe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport tensorflow as tf\n\nSEED = 2018\n\nnp.random.seed(SEED)\ntf.set_random_seed(SEED)\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n#Librerias\nimport os\nimport time\nimport numpy as np \nimport pandas as pd \nfrom tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate, Lambda\nfrom keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D, BatchNormalization\nfrom keras.optimizers import Adam, Nadam\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.callbacks import Callback\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.engine.topology import Layer\nimport gc, re\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, auc, precision_recall_curve","ba0d4715":"import time\nfrom tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate, Lambda\nfrom keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D, BatchNormalization\nfrom keras.optimizers import Adam, Nadam\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.callbacks import Callback\n\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\nfrom keras.engine.topology import Layer\n\n\nimport gc, re\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, auc, precision_recall_curve","3d98e106":"#Limpiamos el texto, tanto los caracteres especiales como las contracciones y se reemplaza en el texto\n\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3', \n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', \n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', \n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', \n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\ndef clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        if punct in x:\n            x = x.replace(punct, f' {punct} ')\n    return x\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",              \n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"ain't\" :  \"will not\",\n\"didn't\": \"did not\"}\n\nmispellings, mispellings_re = _get_mispell(mispell_dict)\n\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n    return mispellings_re.sub(replace, text)","d1e3730c":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")","4d25aff7":"print(\"Train shape : \",train_df.shape)\nprint(\"Test shape : \",test_df.shape)","6f2c279c":"#Limpiar texto\ntrain_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: clean_text(x))\ntest_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: clean_text(x))\n    \n#Limpio numero\ntrain_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\ntest_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\n    \n#Limpio contracciones\ntrain_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\ntest_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\n    \ntrain_X = train_df[\"question_text\"].fillna(\"_##_\").values\nsplits = list(StratifiedKFold(n_splits=10,random_state=2018).split(train_X,train_df['target'].values))","cb6b0b6e":"## Se divide para entrenar\ntrain_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2018)\n\n# Algunas config de los valores\nembed_size = 300 # tamano max de cad avector\nmax_features = 50000 # numero de palabras unicas\nmaxlen = 100 # num max de palabras que tiene la pregunta\n\n# Se complentan los valores faltantes\ntrain_X = train_df[\"question_text\"].fillna(\"_na_\").values\nval_X = val_df[\"question_text\"].fillna(\"_na_\").values\ntest_X = test_df[\"question_text\"].fillna(\"_na_\").values\n\n# Tokenizan las oraciones\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n# Rellenan las oraciones\ntrain_X = pad_sequences(train_X, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\n# Consigue los valores objetivos\ntrain_y = train_df['target'].values\nval_y = val_df['target'].values","a420c137":"inp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size)(inp)\nx = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(model.summary())","5a6ec72c":"# Entrenando el modelo\nmodel.fit(train_X, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))","089fe671":"pred_noemb_val_y = model.predict([val_X], batch_size=1024, verbose=1)\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_noemb_val_y>thresh).astype(int))))","c1254b67":"pred_noemb_test_y = model.predict([test_X], batch_size=1024, verbose=1)","9877354f":"del model, inp, x\nimport gc; gc.collect()\ntime.sleep(10)","a831cc2a":"!ls ..\/input\/embeddings\/","cba7375b":"EMBEDDING_FILE = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n        \ninp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nx = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","f81b4bea":"model.fit(train_X, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))","cefc5b3d":"pred_glove_val_y = model.predict([val_X], batch_size=1024, verbose=1)\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_glove_val_y>thresh).astype(int))))","ca73ec59":"pred_glove_test_y = model.predict([test_X], batch_size=1024, verbose=1)","79eea67b":"del word_index, embeddings_index, all_embs, embedding_matrix, model, inp, x\nimport gc; gc.collect()\ntime.sleep(10)","a930fbc9":"EMBEDDING_FILE = '..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n        \ninp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nx = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","e130e6f5":"model.fit(train_X, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))","c68f21a0":"pred_fasttext_val_y = model.predict([val_X], batch_size=1024, verbose=1)\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_fasttext_val_y>thresh).astype(int))))","3c3b4e12":"pred_fasttext_test_y = model.predict([test_X], batch_size=1024, verbose=1)","3d495f8b":"del word_index, embeddings_index, all_embs, embedding_matrix, model, inp, x\nimport gc; gc.collect()\ntime.sleep(10)","fdad2ea8":"EMBEDDING_FILE = '..\/input\/embeddings\/paragram_300_sl999\/paragram_300_sl999.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n        \ninp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nx = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","8f60182a":"model.fit(train_X, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))","a4c8a29d":"pred_paragram_val_y = model.predict([val_X], batch_size=1024, verbose=1)\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_paragram_val_y>thresh).astype(int))))","ecfd7576":"pred_paragram_test_y = model.predict([test_X], batch_size=1024, verbose=1)","48acb984":"del word_index, embeddings_index, all_embs, embedding_matrix, model, inp, x\nimport gc; gc.collect()\ntime.sleep(10)","59289b3a":"pred_val_y = 0.33*pred_glove_val_y + 0.33*pred_fasttext_val_y + 0.34*pred_paragram_val_y \nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_val_y>thresh).astype(int))))","057b6aaf":"pred_test_y = 0.33*pred_glove_test_y + 0.33*pred_fasttext_test_y + 0.34*pred_paragram_test_y\npred_test_y = (pred_test_y>0.35).astype(int)\nout_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\nout_df['prediction'] = pred_test_y\nout_df.to_csv(\"submission.csv\", index=False)","9b598058":"# pred_test_y = np.sum([outputs[i][1] * reg.coef_[i] for i in range(len(outputs))], axis = 0)\ncoefs = [0.35,0.25,0.2,0.2]\n# pred_test_y = np.mean([outputs[i][1] for i in range(len(outputs))], axis = 0)\npred_test_y = np.sum([outputs[i][1]*coefs[i] for i in range(len(coefs))], axis = 0)\n\npred_test_y = (pred_test_y > results['threshold']).astype(int)\ntest_df = pd.read_csv(\"..\/input\/test.csv\", usecols=[\"qid\"])\nout_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\nout_df['prediction'] = pred_test_y\nout_df.to_csv(\"submission.csv\", index=False)","fbf4f824":"Se entrene al modelo utilizando una muestra de entrenamiento y se supervisa la m\u00e9trica en la muestra v\u00e1lida","5949a52e":"Ahora vamos a usar las incrustaciones de texto r\u00e1pido entrenadas en el cuerpo de Wiki News en vez de las incrustaciones de Glove y se reconstruye el modelo.","8c5e35f7":"Los resultados parecen ser mejores que el modelo sin incrustaciones pre-entrenadas.","3a35768f":" * Se divide la data de entrenamiento en train yval sample. Como la validacion cruzada consume mucho tiempo se dividen los valores\n * Los valores incompletos se llenan con el texto with '_na_'\n * Se tokeniza la columna texto y se convierte en vectores secuenciales\n * Se rellena la secuencia segun sea necesario: si el n\u00famero de palabras en el texto es mayor que 'max_len', se trunca a 'max_len' o si el n\u00famero de palabras en el texto es menor que 'max_len' se agregua ceros para los valores restantes.","2c69e526":"Ahora que se ha terminado con todos los pasos de preprocesamiento necesarios, se procede a entrenar el modelo GRU bidireccional.","13402694":"Aqui usamos las incrustaciones de paragramas y se construye el modelo para hacer predicciones","e3f908eb":"* Las incrustaciones en general pre-entrenadas parecen dar mejores resultados en comparaci\u00f3n con el modelo sin pre-entrenamiento.\n* El rendimiento de las diferentes incrustaciones pre-entrenadas es casi similar.\n \nLos resultados de los modelos con diferentes incrustaciones pre-entrenadas son similares, hay una buena posibilidad de que se pueda capturar diferentes tipos de informaci\u00f3n de los datos. Entonces se hace una combinaci\u00f3n de estos tres modelos promediando sus predicciones.","30106f69":"Se obtiene algunas un modelo de GRU de linea de base sin inscrustaciones pre-entrenadas y se usan las inscrustaciones proporcionadas para reconstruir el modelo nuevamente","2f92e0be":"Ahora que nuestro modelo de construcci\u00f3n est\u00e1 listo, se limpia algo de memoria antes de continuar con el siguiente paso.","88d38a71":"Obtenemos las predicciones del conjunto de pruebas y se guardan","e819379b":"Se obtiene algunas un modelo de GRU de linea de base sin inscrustaciones pre-entrenadas y se usan las inscrustaciones proporcionadas para reconstruir el modelo nuevamente","0a7054a6":"El resultado parece ser mejor que los modelos individuales pre-entrenados, por lo que creamos un archivo de env\u00edo utilizando esta combinaci\u00f3n de modelos.","d64a89a6":"Ahora se obtiene las predicciones de la muestra de validaci\u00f3n y tambi\u00e9n se obtiene el mejor umbral para la puntuaci\u00f3n de F"}}