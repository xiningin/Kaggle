{"cell_type":{"f5bd6e14":"code","00d87c40":"code","365d4630":"code","8449e25f":"code","81442380":"code","f167134d":"code","e6fb243c":"code","50736b9d":"code","6edde4ce":"code","3675d8d1":"code","1045d4de":"code","f28c282a":"code","240ce2ba":"code","e12b7fc6":"code","7d687cfb":"code","756c7ad6":"markdown","bb8fec7a":"markdown","424ae7c0":"markdown","98096076":"markdown","ab68252d":"markdown","e6fb8cc8":"markdown"},"source":{"f5bd6e14":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"seaborn-whitegrid\")\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","00d87c40":"import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv(\"\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")\nplt.figure(figsize = [9,9])\nplt.scatter(df.pelvic_incidence, df.sacral_slope)\nplt.xlabel(\"pelvic incidence\")\nplt.ylabel(\"sacral slope\")\nplt.show()","365d4630":"from sklearn.linear_model import LinearRegression\nlinear_reg = LinearRegression()\nx = df.pelvic_incidence.values.reshape(-1,1)\ny = df.sacral_slope.values.reshape(-1,1)\nlinear_reg.fit(x,y)\ny_head = linear_reg.predict(x)","8449e25f":"#visualize\nplt.figure(figsize = [9,9])\nplt.scatter(x,y)\nplt.xlabel(\"pelvic incidence\")\nplt.ylabel(\"sacral slope\")\nplt.plot(x,y_head,color=\"red\")\nplt.show()","81442380":"#prediction\nb0 = linear_reg.predict([[0]])\nb1 = linear_reg.coef_\nprint(\"b0 is:\",b0)\nprint(\"b1 is:\",b1)\n#Basic question \n#if I determine pelvic incidence = 67, what's my sacral slope\nprint(\"Predict:\", b1*67+b0)","f167134d":"#It's time to r2 score\nfrom sklearn.metrics import r2_score\nprint(\"R2:\",r2_score(y,y_head))","e6fb243c":"from sklearn.linear_model import LinearRegression\nx= df.iloc[:,[1,2,3]].values\ny= df.pelvic_incidence.values.reshape(-1,1)\n\nmultiple_reg = LinearRegression()\nmultiple_reg.fit(x,y)\nprint(\"bo:\", multiple_reg.intercept_) #or print(\"b0: \",multiple_reg.predict(0))\nprint(\"b1,b2,b3:\",multiple_reg.coef_)\n\n ","50736b9d":"#If I want to see these values in a properly form\nb1 = 1.00000000e+00\nb2 = 1.81432369e-11\nb3 = 1.00000000e+00\nprint((\"%.5f\" %b1,\"%.5f\" %b2,\"%.5f\" %b3))\n#Change the .(value)f, and see the format if you view\n","6edde4ce":"y_head = multiple_reg.predict(x)\n#R2 Score\nfrom sklearn.metrics import r2_score\nprint(\"R2 : \", r2_score(y,y_head))","3675d8d1":"#Different way to show R2 Score\nprint(\"R2 : \", multiple_reg.score(x,y))","1045d4de":"from sklearn.preprocessing import PolynomialFeatures\npolynomial_regression = PolynomialFeatures(degree= 3)\nx_polynomial = polynomial_regression.fit_transform(x)","f28c282a":"from sklearn.linear_model import LinearRegression\nplt.figure(figsize = [18,18])\npoly_reg = LinearRegression()\npoly_reg.fit(x_polynomial,y)\ny_head = poly_reg.predict(x_polynomial)\nplt.plot(x,y_head, color= \"blue\")\nplt.show()","240ce2ba":"#R2 Score\nfrom sklearn.metrics import r2_score\nprint(\"R^2 : \",r2_score(y,y_head))","e12b7fc6":"from sklearn.tree import DecisionTreeRegressor\nimport numpy as np\nx=df.pelvic_incidence.values.reshape(-1,1)\ny=df.sacral_slope.values.reshape(-1,1)\ntree=DecisionTreeRegressor()\ntree.fit(x,y)\n\nx_=np.arange(min(x),max(x),0.01).reshape(-1,1)\ny_head=tree.predict(x_)\nplt.plot(x_,y_head,color=\"red\")\nplt.show()","7d687cfb":"x=df.pelvic_incidence.values.reshape(-1,1)\ny=df.sacral_slope.values.reshape(-1,1)\nplt.figure(figsize=[18,18])\nplt.scatter(x,y)\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrandom_reg= RandomForestRegressor(n_estimators=100,random_state=42)\n\nrandom_reg.fit(x,y)\n\nx_=np.arange(min(x),max(x),0.01).reshape(-1,1)\ny_head=random_reg.predict(x_)\nplt.plot(x_,y_head,color=\"red\")\nplt.show()","756c7ad6":"<font color =\"red\">   \n## Content:  \n    \n### * 1. [Linear Regression:](#1)\n### * 2. [Multiple Linear Regression:](#2)\n### * 3. [Polynomial Linear Regression:](#3)\n### * 4. [Decision Tree Regression:](#4)\n### * 5. [Random Forest Regression:](#5)\n    \n    ","bb8fec7a":"<a id = \"2\"><\/a><br>\n## Multiple Linear Regression\n\nMultiple linear regression is used to estimate the relationship between two or more independent variables and one dependent variable.\nWe will use to 3 features in this dataset.\n\ny=b0+b1x1+b2x2+...+bn*xn\n","424ae7c0":"<a id = \"1\"><\/a><br>\n## Linear Regression\nLinear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable.\n\n* y=b0+b1*x\n* b1=constant(bias)(the value of y when x = 0)\n* b0=coefficient\n\n An explanatory variable (x) is required for linear regression so I used pelvic_incidence column for this regression. And target is sacral slope column.","98096076":"<a id = \"3\"><\/a><br>\n## Polynomial Linear Regression\nPolynomial Regression is a form of Linear regression known as a special case of Multiple linear regression which estimates the relationship as an nth degree polynomial. Polynomial Regression is sensitive to outliers so the presence of one or two outliers can also badly affect the performance.\n\ny=b0+b1x+b2x^2+...+bn*x^n\n","ab68252d":"<a id = \"4\"><\/a><br>\n## Decision Tree Regression\nThe decision trees is used to fit a sine curve with addition noisy observation. As a result, it learns local linear regressions approximating the sine curve.\n\nWe can see that if the maximum depth of the tree (controlled by the max_depth parameter) is set too high, the decision trees learn too fine details of the training data and learn from the noise, i.e. they overfit.","e6fb8cc8":"<a id = \"5\"><\/a><br>\n## Random Forest Regression\n\nRandom forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time."}}