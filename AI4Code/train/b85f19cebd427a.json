{"cell_type":{"b4f57380":"code","c8b85dcf":"code","fe332a72":"code","6ce61f10":"code","b51c4326":"code","1b8c52b1":"code","dd716c95":"code","916b80ff":"code","7abc6e15":"code","047ab1d4":"code","557fd298":"code","18c601ab":"code","83258ae0":"code","78cf9ed4":"code","f886c151":"code","b86c102c":"code","d69905f8":"code","c8ce0bf0":"code","9714dea5":"code","a8324bea":"code","40de6e00":"code","5a9da584":"code","fd91724e":"markdown","72e41814":"markdown","908e3f9f":"markdown","6fcc0c23":"markdown","62f4a411":"markdown","f3717784":"markdown","745e3873":"markdown","2b0e70a7":"markdown","e948fefc":"markdown","729f65a5":"markdown","95ea0c60":"markdown","311bf00d":"markdown","90efc873":"markdown","90689925":"markdown","4c3caae6":"markdown","c949a226":"markdown","c818ce47":"markdown","16ab3b94":"markdown","b8d7939a":"markdown","0daaeb1e":"markdown","9f4db68d":"markdown","f97d6d57":"markdown"},"source":{"b4f57380":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport sqlite3\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n!pip install bioinfokit\nfrom bioinfokit.analys import stat, get_data\n\npd.set_option('display.max_rows', 1500)\npd.set_option('display.max_columns', 1000)\npd.set_option('display.width', 1000)\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c8b85dcf":"# Create a SQL connection to our SQLite database\ncon = sqlite3.connect(\"\/kaggle\/input\/california-traffic-collision-data-from-switrs\/switrs.sqlite\")\n\ncur = con.cursor()\n\nparties_query = \" SELECT * FROM parties WHERE case_id IN \\\n(SELECT case_id FROM collisions WHERE motorcycle_collision == 1)\"\n\nvictims_query = \" SELECT * FROM victims WHERE case_id IN \\\n(SELECT case_id FROM collisions WHERE motorcycle_collision == 1)\"\n\n# Read the data\ncollisions1 = pd.read_sql_query(\"SELECT * FROM collisions WHERE motorcycle_collision == 1\", con)\nparties1 = pd.read_sql_query(parties_query, con)\nvictims1 = pd.read_sql_query(victims_query, con)\n\n# Save the data as csv files\ncollisions1.to_csv('collisions.csv',index=False)\nparties1.to_csv('parties.csv',index=False)\nvictims1.to_csv('victims.csv',index=False)\n\ncon.close()","fe332a72":"# We will change dtype when we need it, multiple types are memory inefficient\ncollisions = pd.read_csv('collisions.csv',dtype=str)\nparties = pd.read_csv('parties.csv',dtype=str)\nvictims = pd.read_csv('victims.csv',dtype=str)","6ce61f10":"# How many collisions are we dealing with\nprint(\"The number of unique case_ids is {n}\".format(n=len(parties['case_id'].unique())))","b51c4326":"# How large is our data\nprint(\"There are {x} records from collisions.\".format(x=collisions.shape[0]))\nprint(\"There are {y} records from parties.\".format(y=parties.shape[0]))\nprint(\"There are {z} records from victims.\".format(z=victims.shape[0]))","1b8c52b1":"all_vehicles = parties.groupby(['statewide_vehicle_type']).agg({'case_id':'count'})","dd716c95":"motorcycle = ['motorcycle or scooter', 'moped']\nmotorcycles = parties[parties.statewide_vehicle_type.isin(motorcycle)]\n\n# What are the most popular motorbikes in collisions\ncount1 = motorcycles.groupby(['vehicle_make']).agg({'case_id':'count'})\ncount_sorted1 = count1.sort_values(by='case_id',ascending=False)\ncount_sorted1.head(20)","916b80ff":"# Will roughly cover motorcycles with freq > 100\nmakes = {\n    'HONDA':['HOND'],\n    'HARLEY-DAVIDSON':['HARL','HD','HARLE','HARLEY DAVIDSON','HARLEY D','HARLEY-D','HARLEY'],\n    'YAMAHA':['YAMA','YAMAH'],\n    'KAWASAKI':['KAWA','KAWK','KAWI','KAWAS'],\n    'SUZUKI':['SUZU','SUZI'],\n    'DUCATI':['DUCA','DUCAT','DUCATI'],\n    'TRIUMPH':['TRIU','TRUM','TRIUM'],\n    'INDIAN':['INDIAN (MOTORCYCLE)','INDI'],\n    'SPCN':['SPCNS']\n}\n\nmotorcycle_makes = motorcycles.vehicle_make.to_numpy()\n\nfor i, moto in enumerate(motorcycle_makes):\n    for make in makes:\n        if moto in makes[make]:\n            motorcycle_makes[i] = make\n            break\n\n# Update the dataset with cleaned data\nmotorcycles.vehicle_make = motorcycle_makes","7abc6e15":"# What are the most popular motorbikes in collisions (cleaned version)\ncount2 = motorcycles.groupby(['vehicle_make']).agg({'case_id':'count'})\ncount_sorted2 = count2.sort_values(by='case_id',ascending=False)\ncount_sorted2.head(20)","047ab1d4":"# String ---> time\ncollisions['collision_time'] = pd.to_timedelta(collisions['collision_time'])\n\n# Group into 1 hour slots (e.g. 0 days 15:00 <=> [15:00-16:00])\ntimes = collisions.groupby(\n    pd.Grouper(key='collision_time',freq='60Min',label='right')).agg(\n    {'case_id':'count'}).reset_index()\n\nsorted_times = times.sort_values(by='case_id',ascending=False)\nsorted_times.head(10)","557fd298":"# Change datatypes as necessary\ncollisions.case_id = collisions.case_id.astype(np.float)\ncollisions.collision_date = pd.to_datetime(collisions.collision_date,format='%Y-%m-%d')\n\n# Assess what days are weekends and weekdays; {1 = weekday, 0 = weekend}\nday_type = np.array([1 if x.weekday() < 5 else 0 for x in collisions['collision_date']])\n\n# Add to the data\ncollisions['day_type'] = day_type\n\n# Group by weekend\/weekday & times\nt = pd.Grouper(key='collision_time',freq='60Min') #time grouper\ngrouped_df = collisions.groupby([t, 'day_type']).agg({'case_id':'count'}).reset_index()\n\n# How many weekdays & weekends?\nno_weekdays = sum(1 for _ in collisions.loc[collisions['day_type'] == 1]['day_type'])\nno_weekends = sum(1 for _ in collisions.loc[collisions['day_type'] == 0]['day_type'])\n\n# Calculate number of accidents on weekdays & weekends as a fraction of the total number of weekdays & weekends\n# respectively\nfracs = np.array(\n    [x[2]\/no_weekdays if x[1] == 1\n     else x[2]\/no_weekends\n     for i,x in grouped_df.iterrows()\n     ]\n)\n\n# Add to our data\ngrouped_df['frac'] = fracs\n\n# Sort the data\nsorted_grouped = grouped_df.sort_values(by='frac',ascending=False)\nsorted_grouped.head(10)","18c601ab":"# pwgc = P(weekend|crash)\npwgc = no_weekends\/(no_weekdays + no_weekends)\n# pw = P(weekend)\npw = 2\/7\n\n# pcgw = P(crash|weekend)\npcgw = pwgc\/pw\n# pcgwd = P(crash|weekday)\npcgwd = (1-pwgc)\/(1-pw)\n\nprint(\"P(crash|weekend) = {x}.P(crash)\".format(x=pcgw))\nprint(\"P(crash|weekday) = {x}.P(crash)\".format(x=pcgwd))","83258ae0":"# Change datatype\nparties.case_id = parties.case_id.astype(np.float)\n\n# Get the N most popular motorcycle makes\nN = 20\ncount = motorcycles.groupby(['vehicle_make']).agg({'case_id':'count'}).reset_index()\nN_most_popular = count.sort_values(by='case_id',ascending=False).head(N)\n\n# Get the data on the N most popular motorcycle makes\ntop_N_motorcycles = N_most_popular.vehicle_make\ntop_N_data = parties[parties.vehicle_make.isin(top_N_motorcycles)]\n\nprint(\"The top {n} motorcycles account for {x} collisions\".format(\n    n=N,x=len(top_N_data)))\n\n# Joing motorcycles and collisions on case_id\nmerged_df = top_N_data.set_index('case_id').join(collisions.set_index('case_id')).reset_index()\n\n# Group by vehicle_make & day_type (weekend\/weekday)\ngb_vd = merged_df.groupby(['day_type', 'vehicle_make']).agg(\n    {'case_id':'count'}\n).reset_index()\n\n# Pivot table:\npivot_table = gb_vd.pivot_table(index='vehicle_make',columns='day_type',values='case_id')\n\n# Chi-square Test for independence\nres = stat()\nres.chisq(df=pivot_table)\n\nprint(res.summary)","78cf9ed4":"# Change specific columns to numeric\nnumeric_cols = ['killed_victims','injured_victims','motorcyclist_injured_count',\n                'motorcyclist_killed_count','party_count']\n\ncollisions[numeric_cols] = collisions[numeric_cols].astype(np.float)","f886c151":"# Weather input\nw = ['weather_1','weather_2']\ncollisions[w] = collisions[w].replace(np.nan, '-',regex=True)\n\n# Create list of tuples of weather:\nweather = list(zip(collisions['weather_1'],collisions['weather_2']))\ncollisions['weather'] = weather\n\ngb = 'weather'\ncollisions[gb] = collisions[gb].astype(str)\n\n# Group by weather and sort by most popular\nconditions_grouped = collisions.groupby([gb]).agg(\n    {'case_id':'count'}\n).reset_index()\n\nconditions_grouped.sort_values(by='case_id',ascending=False)","b86c102c":"def plot_condition(data, conditions, variable):\n    # Replace nan by '-'\n    nans = conditions\n    data[nans] = data[nans].replace(np.nan, '-',regex=True)\n\n    n = len(conditions)\n\n    fig, axs = plt.subplots(ncols=len(conditions), sharey=True)\n    \n    fig.set_figheight(8)\n    fig.set_figwidth(8)\n    \n    # Plot each condition seperately\n    for i, cond in enumerate(conditions):\n        grouped = data.groupby(cond)\n        agg_grouped = grouped.agg({'killed_victims':'mean',\n                                   'injured_victims':'mean',\n                                   'motorcyclist_injured_count':'mean',\n                                   'motorcyclist_killed_count':'mean',\n                                   'party_count':'mean',\n                                   'case_id':'count'}).reset_index()\n\n        # Use Log-scale to 'compress' scale\n        x = np.log(agg_grouped['case_id'])\n        y = np.log(agg_grouped[variable])\n\n        color_labels = agg_grouped[cond].unique()\n\n        # List of colors in the color palettes\n        rgb_values = sns.color_palette(\"Set2\", len(color_labels))\n\n        # Map weather to the colors\n        color_map = dict(zip(color_labels, rgb_values))\n        \n        # Plot the points w\/labels\n        if n != 1:\n            axs[i].scatter(x, y,c=agg_grouped[cond].map(color_map))\n\n            for j, txt in enumerate(agg_grouped[cond]):\n                axs[i].annotate(txt, (x[j], y[j]))\n\n            axs[i].set_ylabel('(Log-scale) {X}'.format(X=variable.replace('_',' ')))\n\n            axs[i].set_xlabel(\"(Log-scale) Count\")\n        else:\n            axs.scatter(x, y, c=agg_grouped[cond].map(color_map))\n\n            for j, txt in enumerate(agg_grouped[cond]):\n                axs.annotate(txt, (x[j], y[j]))\n\n            axs.set_ylabel('(Log-scale) {X}'.format(X=variable.replace('_', ' ')))\n\n            axs.set_xlabel(\"(Log-scale) Count\")\n\n    plt.suptitle(' & '.join(c for c in conditions))\n    plt.show()\n    return None","d69905f8":"plot_condition(collisions, ['weather_1','weather_2'], 'killed_victims')\n# RuntimeWarning: divide by zero encountered in log is our friend as it removes values with no occurances","c8ce0bf0":"nans = ['weather_1','weather_2']\ncollisions[nans] = collisions[nans].replace('-', np.nan,regex=True)\n\n# Get months\nmonths = np.array([m.month for m in collisions['collision_date']])\ncollisions['month'] = months\n\n# Group collisions by month and weather\ngb_month_weather1 = collisions.groupby(['month','weather_1'])\nagg_month_weather1 = gb_month_weather1.agg({'case_id':'count'}).reset_index()\n\ngb_month_weather2 = collisions.groupby(['month','weather_2'])\nagg_month_weather2 = gb_month_weather2.agg({'case_id':'count'}).reset_index()\n\n# Create pivot tables of the above grouped dataframe\np_table1 = agg_month_weather1.pivot_table(index='month',\n                                          columns='weather_1',\n                                          values='case_id')\n\np_table2 = agg_month_weather2.pivot_table(index='month',\n                                          columns='weather_2',\n                                          values='case_id')\n\n# Replace NaN's with 0\np_table1 = p_table1.replace(np.nan,0)\np_table2 = p_table2.replace(np.nan,0)\n\n# Combine the results from both pivot tables, i.e. combine weather_1 & weather_2 results\np_table1[p_table2.columns] = p_table2[p_table2.columns]+p_table1[p_table2.columns]\n\n# Sum(counts of each weather type) across each month\np_table1['sum'] = p_table1.sum(axis=1)\n\n#P(condition|month) = p_condition_given_month\np_condition_given_month = p_table1.iloc[:,0:7].div(p_table1[\"sum\"], axis=0)\n# All unique weathers\nweathers = p_condition_given_month.columns\n\n# p(condition_i) = sum_j(P(condition_i|month_j).P(month_j)) = 1\/12 * sum_j(P(condition_i|month_j))\np_condition = 1\/12 * p_condition_given_month.sum(axis=0)\n\n# Get P(crash n condition_i) for each i:\ngb_weather1 = collisions.groupby(['weather_1']).agg({'case_id':'count'}).reset_index()\ngb_weather2 = collisions.groupby(['weather_2']).agg({'case_id':'count'}).reset_index()\n\n# Combine weather_1 & weather_2 totals\nfor i,b in gb_weather2.iterrows():\n    gb_weather1.at[i+1, 'case_id'] += b[1]\n    \n# Total\ntotal = gb_weather1['case_id'].sum()\n\n# P(condition|crash) = \np_condition_given_crash = gb_weather1['case_id'] \/ total\n\n# P(crash|condition_i) = P(condition_i|crash) * P(crash) \/ P(condition_i)\np_crash_given_cond = np.array([x \/ p_condition[i] for i,x in enumerate(p_condition_given_crash)])\n\n# P(crash|condition_i) as DF\np_cr_g_co = pd.DataFrame(list(zip(weathers,\n                                  p_crash_given_cond,\n                                  p_condition)))\np_cr_g_co.columns = ['Condition', 'P(Crash|Condition)\/P(Crash)','P(Condition)']\n\np_cr_g_co = p_cr_g_co.sort_values(by='P(Crash|Condition)\/P(Crash)',ascending=False)\n\np_cr_g_co","9714dea5":"collisions['killed_victims'] = collisions['killed_victims'].astype(np.float)\ncollisions['killed_victims'] = collisions['killed_victims'].replace(np.nan,0)\n\nplt.hist(collisions['killed_victims'],bins=5,density=True)\nplt.suptitle(\"Distribution of Fatalities\")\nplt.show()","a8324bea":"# Total sample size\nn = len(collisions)\n\n# Denominator of alpha_MLE\nsum_ln_x = sum(np.log(x+1) for x in collisions['killed_victims'])\n\n# Shape parameter\nalpha = round(n\/sum_ln_x,2)\n\n# m = minimum possible value\na, m = alpha, 1.  # shape and mode\n\n# Simulate 1000 results from this distributin\ns = (np.random.pareto(a, 1000) + 1) * m\n\n# Plot the estimated distribution\nfig, ax = plt.subplots()\n\ncount, bins, _ = ax.hist(s, 100, density=True)\nfit = a*m**a \/ bins**(a+1)\n\nax.plot(bins, max(count)*fit\/max(fit), linewidth=2, color='r')\nax.set_xlabel('Number of deaths + 1')\nax.set_ylabel('P((Num Deaths + 1) | Crash)')\n\nplt.suptitle('Estimation of P(Num Deaths | Crash) using a Pareto Distribution \\n \\\n             with shape paramter (alpha) = {a} & minimum number = {m}'.format(\n    a=alpha,m=m))\n\nplt.show()","40de6e00":"# Remove snow\np_condition_given_month = p_condition_given_month.drop('snowing',axis=1)\np_condition = 1\/12 * p_condition_given_month.sum(axis=0)\n\n# All weathers \nweathers = p_condition_given_month.columns\n\ncollisions_with_deaths = collisions[collisions.killed_victims > 0]\n\ngb_weather1 = collisions_with_deaths.groupby(['weather_1']).agg({'case_id':'count'}).reset_index()\ngb_weather2 = collisions_with_deaths.groupby(['weather_2']).agg({'case_id':'count'}).reset_index()\n\n# Combine weather_1 & weather_2 totals\nfor i,b in gb_weather2.iterrows():\n    gb_weather1.at[i+1, 'case_id'] += b[1]\n\n# Total\ntotal = gb_weather1['case_id'].sum()\n\n# P(condition|crash&death) =\np_condition_given_crash = gb_weather1['case_id'] \/ total\n\n# P(crash&death|condition_i) = P(condition_i|crash&death) * P(crash&death) \/ P(condition_i)\np_crash_given_cond = np.array([x \/ p_condition[i] for i,x in enumerate(p_condition_given_crash)])\n\n# P(crash&death|condition_i) as DF\np_cr_g_co = pd.DataFrame(list(zip(weathers,\n                                  p_crash_given_cond,\n                                  p_condition)))\np_cr_g_co.columns = ['Condition',\n                     'P((Crash & Fatality)|Condition)\/P(Crash & Fatality)',\n                     'P(Condition)']\n\np_cr_g_co = p_cr_g_co.sort_values(by='P((Crash & Fatality)|Condition)\/P(Crash & Fatality)',\n                                  ascending=False)\n\np_cr_g_co","5a9da584":"plot_condition(collisions, ['road_condition_1','road_condition_2'], 'killed_victims')","fd91724e":"Without taking into account weather conditions, we can see that the probability of having fatalities in a collision is **close to zero**.\n\nOur estimated distribution calculates probabilities $>1$ however this is ok.\n\nNext, we take into account the **weather conditions**.","72e41814":"The p-value for the Pearson Chi-Square Test for Independence is highly significant. Hence we conclude that vehicle make and day type (i.e. weekday or weekend) are **dependent**.","908e3f9f":"The weather is **nice & clear 88%** of the time in California when crashes occur.\n\nYou are **7% more likely** to crash your motorbike when it is clear than when it is cloudy.\n\nBut what is $P((\\mbox{crash} \\cap \\mbox{fatal}) | \\mbox{condition}_i)$?\n\n### $P((\\mbox{crash} \\cap \\mbox{fatal}) | \\mbox{condition}_i)$:\n\ni.e. **Are certain weather conditions more fatal than others**?\n\nThe method is the same as above, so we just need to calculate:\n\n* $P(\\mbox{condition}_i | (\\mbox{crash} \\cap \\mbox{fatal}))$, and\n* $P((\\mbox{crash} \\cap \\mbox{fatal})) = P((\\mbox{fatal} | \\mbox{crash}))\\cdot P(\\mbox{crash}) $\n\nand plug these into:\n\n$$P((\\mbox{crash} \\cap \\mbox{fatal}) | \\mbox{condition}_i) = \\frac{P(\\mbox{condition}_i | (\\mbox{crash} \\cap \\mbox{fatal}))}{P(\\mbox{condition}_i)}\\cdot P((\\mbox{fatal} | \\mbox{crash}))\\cdot P(\\mbox{crash})$$\n\n#### Step 1: Model $P((\\mbox{fatal} | \\mbox{crash}))$\n\nWe must assess the distribution of fatalities using a histogram & best fit line:","6fcc0c23":"Obstruction, construction & reduced width (& normal) have the **highest average fatalities.**","62f4a411":"## Road conditions\n\nLet's assess what road conditions appear the most in collisions","f3717784":"There can be multiple parties in a collision.","745e3873":"# Conclusions:\n\n* You are **more likely to crash** your motorbike on the **weekend**,\n* Whether you crash on the weekend or during the week will be **dependent on what motorbike** you are driving,\n* **Wind** and fog are the **most deadly** weather condition for motorbike crashes.","2b0e70a7":"Weekdays seem more popular, let's normalise a bit...","e948fefc":"##\u00a0Popularity of Crashes under Weather Conditions","729f65a5":"It looks like fog and wind (& other) have the highest average fatality rate.\n\nWe can combine weather_1 & weather_2. However this implies the (innacurate) heavy assumption that weather_1 & weather_2 are **independent**.\n\nLet's see if we can model this.\n\n### P(Crash|Condition)\n\nLet's model the $P(\\mbox{crash}|\\mbox{condition}_i)$.\n\n$$P(\\mbox{crash}|\\mbox{condition}_i) = \\frac{P(\\mbox{crash} \\cap \\mbox{condition}_i)}{P(\\mbox{condition}_i)}$$\n\n$$P(\\mbox{crash} \\cap \\mbox{condition}_i) = P(\\mbox{condition}_i |  \\mbox{crash})\\cdot P(\\mbox{crash})$$\n\n$$\\implies P(\\mbox{crash}|\\mbox{condition}_i) = \\frac{P(\\mbox{condition}_i |  \\mbox{crash})}{P(\\mbox{condition}_i)}\\cdot P(\\mbox{crash})$$\n\nWe can model $P(\\mbox{condition}_i)$ as a marginal distribution using month:\n\n$$P(\\mbox{condition}_i) = \\sum_{j=January}^{December}P(\\mbox{condition}_i \\cap \\mbox{month}_j) = \\sum_{j=January}^{December} P(\\mbox{condition}_i | \\mbox{month}_j) \\cdot P(\\mbox{month}_j)$$\n\nwhere as one might expect $P(\\mbox{month}_j) = \\frac{1}{12}$.\n\nAltogether we have that,\n\n$$P(\\mbox{crash}|\\mbox{condition}_i) = \\frac{P(\\mbox{condition}_i |  \\mbox{crash})}{\\sum_{j=January}^{December} P(\\mbox{condition}_i | \\mbox{month}_j) \\cdot P(\\mbox{month}_j)}\\cdot P(\\mbox{crash})$$","95ea0c60":"## Weather\n\nMost popular weather conditions","311bf00d":"# Vehicle Make & Weekend crash independent?\n\nPerform Pearson $\\chi$-Square Test for independence on vehicle_make & day_type. Our statistic is,\n\n$$\\chi_{q} = \\sum \\frac{(\\mbox{observed}-\\mbox{expected})^2}{\\mbox{expected}}$$\n\nwhere we sum over all rows & columns and $q = (\\mbox{#columns} - 1) \\cdot (\\mbox{#rows} - 1)$ is our degrees of freedom.\n\nOur null hypothesis is: vehicle_make & day_type are independent.","90efc873":"# Most popular crash times","90689925":"### Fatalities in a motorcycle collision are 68% more likely to occur in windy conditions than in clear conditions!","4c3caae6":"It looks to me like we can use a [Pareto Distribution](https:\/\/en.wikipedia.org\/wiki\/Pareto_distribution#Estimation_of_parameters) to model this. We will model the probability of a observing a number deaths given that there has been a motorcycle crash. \n\nLet,\n\n$$P(\\mbox{#fatalities} + 1 = x | \\mbox{crash})  = f_X(x) = \\frac{\\alpha x_m^{\\alpha}}{x^{\\alpha+1}}$$ \n\nwhere:\n\n* $\\alpha$ is the shape parameter, which is known as the tail index, and\n* $x_m$ is the (necessarily positive) minimum possible value of $X$\n\nWe have modelled $P(\\mbox{#fatalities} + 1 = x | \\mbox{crash})$ because the Pareto Distribution requires the minimum possible value of $X$ to be positive, and so in this case $X$ has minimum number of $1$, instead of $0$ as it would be if we modelled $P(\\mbox{#fatalities} = x | \\mbox{crash})$.\n\nTo calculate $\\alpha$ we use the **maximum likelihood estimator** which results from differentiating the likelihood function of $f_X(x)$ with respect to $\\alpha$, setting this equal to zero and rearranging for $\\alpha$. This yields:\n\n$$\\alpha_{MLE} = \\frac{n}{\\sum_{i=1}^n{\\ln{x_i}}}$$\n\nwhere:\n\n* $n$ is the the total size of the sample $(x_1,...,x_n)$","c949a226":"# More likely to crash on weekend?\n\nUsing Baye's Rule repeatidly we have that\n\n$$P(\\mbox{crash}|\\mbox{weekend}) = \\frac{P(\\mbox{weekend},\\mbox{crash})}{P(\\mbox{weekend})}$$\n\n\n$$P(\\mbox{weekend},\\mbox{crash}) = P(\\mbox{weekend}|\\mbox{crash}) \\cdot P(\\mbox{crash})$$\n\n\n$$\\implies P(\\mbox{crash}|\\mbox{weekend}) = \\frac{P(\\mbox{weekend}|\\mbox{crash})}{P(\\mbox{weekend})}\\cdot P(\\mbox{crash})$$\n\n\nSimilarly, we can calculate,\n\n\n$$P(\\mbox{crash}|\\mbox{weekday}) = \\frac{P(\\mbox{weekday}|\\mbox{crash})}{P(\\mbox{weekday})}\\cdot P(\\mbox{crash})$$","c818ce47":"# Analysis of Motorcycle Crashes\n\nWelcome to this notebook where we will use Bayesian Statistics, statistical tests & visualisation techniques to answer the questions:\n\n* Are you more likely to crash on the **weekend**?\n* Does your **type of motorcycle** impact whether you are more likely to crash on the weekend?\n* How do **weather conditions** impact your likeliness to crash?\n* Are some weather conditions more fatal than others?\n* Do **road conditions** play into the severity of the crash as well?","16ab3b94":"# Conditions:\n\nNext, we analyse the conditions under which crashes take place and if you are more likely to crash your motorbike under certain conditions.","b8d7939a":"Now more weekends appear... which leads to the question:","0daaeb1e":"Since the normalising constant P(crash) is constant, we conclude that **there is a higher probability of crashing your motorbike at the weekend than on a weekday**.\n\nWe can make this more complicated (& accurate) by replacing weekday & weekend by (driving on a weekday) & (driving on a weekend) respectively then estimating  $P($driving on a weekday$)$ & $P($driving on a weekend$)$.","9f4db68d":"# Data Cleaning\n\ne.g.\n* HOND ---> HONDA, \n* HD ---> HARLEY-DAVIDSON","f97d6d57":"Clear is by far the most popular, not a surprise for California!\n\n## Function to plot conditions"}}