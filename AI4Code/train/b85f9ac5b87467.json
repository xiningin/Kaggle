{"cell_type":{"d0a65620":"code","c483d5fe":"code","885eaf3a":"code","37ca1669":"code","912e4bb3":"code","95227d07":"code","1a257682":"code","310f565e":"code","919fdbd4":"code","7d3e3977":"code","172865c7":"code","11a9dd99":"code","182a5a76":"code","b9522a34":"code","e8961157":"code","a3129fb2":"code","a51e2137":"code","7d63e488":"code","b2272252":"code","76201f38":"code","2f88d519":"code","09fe1263":"code","9e434cb3":"code","218dc68e":"code","13348b3d":"code","80622dda":"code","1c35078c":"code","88166e21":"code","68a32f9c":"code","263c75f0":"code","1d8c7c23":"code","61a3e2d9":"code","55f56c73":"code","1d09bd01":"code","3cd1f436":"code","178ae2b1":"code","2bd7deb2":"code","e4d91175":"code","f0d189c9":"code","90f9f3da":"code","b89fd88b":"code","33d1caf3":"code","c409a11c":"code","c2cdee94":"code","6f3667e4":"code","63fa9d3c":"code","55752a8a":"code","6ea372d4":"code","27032517":"code","b992278b":"code","71e54ccd":"code","60cb48b9":"code","60dfd5ef":"code","e29ccb89":"code","db8e783e":"code","04bc90d5":"code","dcafce11":"code","1f8e31fa":"code","d4e36682":"code","db9c85d0":"code","d9bfa3e0":"code","c543b4a0":"code","5610a007":"code","888f90dd":"code","93723e9f":"code","f205106b":"code","f66d7f99":"code","8d53ad6a":"code","659010ad":"code","2ddd9dcd":"code","1cb5c993":"code","fbcd88b4":"code","c214dfd6":"code","4efa2184":"code","87c11896":"code","442ac341":"code","6375791f":"code","1eea6c96":"code","e76f0078":"code","f442f25e":"code","2f498c34":"code","0c125e39":"code","d81b5e62":"code","6851b6cc":"code","3f64d9b2":"code","9f6b7364":"code","ceebb891":"code","7534584f":"code","e3b81d1d":"code","ee9a7bdf":"code","2613ddb2":"code","ac539704":"code","401f605a":"code","ee5d08cb":"code","5df4b05c":"code","86f0edf6":"code","b0083039":"code","491cff2e":"code","3ec19a04":"code","34c83621":"code","955fb64c":"code","fd0e8cd0":"code","d305136e":"code","59550b14":"code","8dc2143a":"code","8cb15ae9":"code","a45cc014":"code","2971c1f6":"code","f2de6d70":"code","57d74435":"code","1b26ebbb":"code","25dc4cbb":"code","dae67869":"code","a51de447":"code","c172db11":"code","d218b8a1":"code","3caa4ae5":"code","3f030b17":"code","cce5094f":"code","5ccbeba6":"code","82570d4b":"code","366d34a7":"code","e205fc59":"code","08b5a038":"code","3025b74d":"code","a6521473":"code","bec61bcf":"code","90fd6391":"markdown","5161ef8f":"markdown","7ae020dd":"markdown","fa454b47":"markdown","d8af02d3":"markdown","f62084e6":"markdown","ed0ecd39":"markdown","1bbce691":"markdown","ac8ea49c":"markdown","0dea356c":"markdown","19d75f32":"markdown","d9115145":"markdown","15c005ce":"markdown","fbae3ca0":"markdown","d86fc821":"markdown","2be98f20":"markdown","b0d0bb7c":"markdown","afbdacd2":"markdown","00e7fcb2":"markdown","f7cc4dca":"markdown","99519ae6":"markdown","f3ebc3f0":"markdown","a2c80986":"markdown","062429c0":"markdown","d0138199":"markdown","a1fd811d":"markdown","bb4ff335":"markdown","d520a2ad":"markdown","2e81b1f4":"markdown","22f4c2fa":"markdown","89837c00":"markdown","c4608873":"markdown","1bda7d99":"markdown","de635ba0":"markdown","e6b1b240":"markdown","df431b58":"markdown","1d5c86fa":"markdown","c4ee7529":"markdown","ffdd01e4":"markdown","970b09d4":"markdown","53f9ff9a":"markdown","ebfb2116":"markdown","61b51c7f":"markdown","9064b94a":"markdown","e08b2ca4":"markdown","4d300669":"markdown","cc22096c":"markdown","ffec97aa":"markdown","4854d0a0":"markdown","a0329de0":"markdown","ef850186":"markdown","ef90a98d":"markdown","e18db4b4":"markdown","b2568e9f":"markdown","8c610571":"markdown","d463a9d2":"markdown","3eea6763":"markdown","71b8aa4f":"markdown","6abaad15":"markdown","860949b3":"markdown","91597c1d":"markdown","440be181":"markdown","384b0077":"markdown","1736a5ad":"markdown","fd65f008":"markdown","2eba9574":"markdown","e89572f3":"markdown","564d0e87":"markdown","1f4344f9":"markdown"},"source":{"d0a65620":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n\n\n# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","c483d5fe":"#Read data from csv file\ndf_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_test=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","885eaf3a":"print('The shape of our training set: ',df_train.shape[0], 'Passengers', 'and', df_train.shape[1] -1 , 'features' ,df_train.shape[1] , 'columns' )\nprint('The shape of our testing set: ',df_test.shape[0], 'Passengers', 'and', df_test.shape[1], 'features')\nprint('The testing set has 1 column  less than the training set, which is Survived , the target to predict  ')","37ca1669":"print(df_train.columns.values)","912e4bb3":"# preview the data from head\ndf_train.head(3)","95227d07":"# preview the data from tail\ndf_train.tail(3)","1a257682":"# split data train into Numeric and Categorocal\nnumeric = df_train.select_dtypes(exclude='object')\ncategorical = df_train.select_dtypes(include='object')","310f565e":"print(\"\\nNumber of categorical features : \",(len(categorical.axes[1])))\nprint(\"\\n\", categorical.axes[1])\ncategorical.head()","919fdbd4":"df_train.describe(include=['O'])","7d3e3977":"print(\"\\nNumber of numeric features : \",(len(numeric.axes[1])))\nprint(\"\\n\", numeric.axes[1])","172865c7":"df_train.describe()","11a9dd99":"df_train.info()\nprint('_'*50)\ndf_test.info()","182a5a76":"# Isolate the numeric features and check his relevance\n\nnum_corr = numeric.corr()\ntable = num_corr['Survived'].sort_values(ascending=False).to_frame()\ncm = sns.light_palette(\"green\", as_cmap=True)\ntb = table.style.background_gradient(cmap=cm)\ntb","b9522a34":"#Survived correlation matrix\nk = 5 #number of variables for heatmap\ncols = df_train.corr().nlargest(k, 'Survived')['Survived'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","e8961157":"#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='blue',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(df_train)","a3129fb2":"df_train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n#print('_'*50)","a51e2137":"df_train[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n\n","7d63e488":"df_train[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","b2272252":"df_train[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","76201f38":"\ndf_train[[\"Embarked\", \"Survived\"]].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","2f88d519":"fig1, ax1 = plt.subplots()\nax1.pie(df_train['Survived'].groupby(df_train['Survived']).count(), \n        labels = ['Not Survived', 'Survived'], autopct = '%1.1f%%')\nax1.axis('equal')\n\nplt.show()","09fe1263":"g = sns.FacetGrid(df_train, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","9e434cb3":"g = sns.FacetGrid(df_train, col='Survived')\ng.map(plt.hist, 'Fare', bins=30)","218dc68e":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid = sns.FacetGrid(df_train, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","13348b3d":"# grid = sns.FacetGrid(train_df, col='Embarked')\ngrid = sns.FacetGrid(df_train, row='Embarked', size=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","80622dda":"\n\n# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})\ngrid = sns.FacetGrid(df_train, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","1c35078c":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived', palette={0: 'k', 1: 'w'})\ngrid = sns.FacetGrid(df_train, row='Pclass', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","88166e21":"# Countplot \n\nsns.catplot(x =\"Sex\", hue =\"Survived\", kind =\"count\", data = df_train) ","68a32f9c":"# Countplot \n\nsns.catplot(x =\"Embarked\", hue =\"Survived\", kind =\"count\", data = df_train) ","263c75f0":"# Countplot \n\nsns.catplot(x =\"Pclass\", hue =\"Survived\", kind =\"count\", data = df_train) ","1d8c7c23":"# Countplot \n\nsns.catplot(x =\"SibSp\", hue =\"Survived\", kind =\"count\", data = df_train) ","61a3e2d9":"# Countplot \n\nsns.catplot(x =\"Parch\", hue =\"Survived\", kind =\"count\", data = df_train) \n\n","55f56c73":"plt.figure(figsize=(10,7))\nsns.boxplot(x='Pclass',y='Age',data=df_train)\n","1d09bd01":"plt.figure(figsize=(10,6))\nsns.boxplot(x='Pclass',y='Fare',data=df_train)\n","3cd1f436":"plt.figure(figsize=(10,7))\nsns.boxplot(x='Survived',y='Fare',data=df_train)\n","178ae2b1":"#Embarked\nplt.figure(figsize=(10,7))\nsns.boxplot(x='Embarked',y='Fare',data=df_train)\n","2bd7deb2":"group = df_train.groupby(['Pclass', 'Survived']) \npclass_survived = group.size().unstack() \n  \nsns.heatmap(pclass_survived, annot = True, fmt =\"d\") ","e4d91175":"group = df_train.groupby(['Embarked', 'Survived']) \npclass_survived = group.size().unstack() \n  \nsns.heatmap(pclass_survived, annot = True, fmt =\"d\") ","f0d189c9":"group = df_train.groupby(['Sex', 'Survived']) \npclass_survived = group.size().unstack() \n  \nsns.heatmap(pclass_survived, annot = True, fmt =\"d\")","90f9f3da":"#missing data in Traing examples\ntotal = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\nmissing_data.head(12)\n","b89fd88b":"print(df_train.shape)\nprint(df_test.shape)","33d1caf3":"na = df_train.shape[0] #na is the number of rows of the original training set\nnb = df_test.shape[0]  #nb is the number of rows of the original test set\ny_target = df_train['Survived'].to_frame()\n#Combine train and test sets\nc1 = pd.concat((df_train, df_test), sort=False).reset_index(drop=True)\n#Drop the target \"Survived\" and Id columns\nc1.drop(['Survived'], axis=1, inplace=True)\nc1.drop(['PassengerId'], axis=1, inplace=True)\nprint(\"Total size for train and test sets is :\",c1.shape)","c409a11c":"##msv1 method to visualize missing values per columns\ndef msv1(data, thresh=20, color='black', edgecolor='black', width=15, height=3): \n    \"\"\"\n    SOURCE: https:\/\/www.kaggle.com\/amiiiney\/price-prediction-regularization-stacking\n    \"\"\"\n    \n    plt.figure(figsize=(width,height))\n    percentage=(data.isnull().mean())*100\n    percentage.sort_values(ascending=False).plot.bar(color=color, edgecolor=edgecolor)\n    plt.axhline(y=thresh, color='r', linestyle='-')\n    plt.title('Missing values percentage per column', fontsize=20, weight='bold' )\n    plt.text(len(data.isnull().sum()\/len(data))\/1.7, thresh+12.5, 'Columns with more than %s%s missing values' %(thresh, '%'), fontsize=12, color='crimson',\n         ha='left' ,va='top')\n    plt.text(len(data.isnull().sum()\/len(data))\/1.7, thresh - 5, 'Columns with less than %s%s missing values' %(thresh, '%'), fontsize=12, color='green',\n         ha='left' ,va='top')\n    plt.xlabel('Columns', size=15, weight='bold')\n    plt.ylabel('Missing values percentage')\n    plt.yticks(weight ='bold')\n    \n    return plt.show()","c2cdee94":"#missing data in Traing examples and test set \ntotal = c1.isnull().sum().sort_values(ascending=False)\npercent = (c1.isnull().sum()\/c1.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\nmissing_data.head(12)","6f3667e4":"#call method msv1 to visualization missing value per columns \nmsv1(c1, 77, color=('silver', 'gold', 'lightgreen', 'skyblue', 'lightpink'))","63fa9d3c":"# drop columns (features ) with > 79% missing vales\nc=c1.dropna(thresh=len(c1)*0.15, axis=1)\nprint('We dropped ',c1.shape[1]-c.shape[1], ' features in the combined set')","55752a8a":"print('The shape of the combined dataset after dropping features with more than 21% M.V.', c.shape)","6ea372d4":"allna = (c.isnull().sum() \/ len(c))*100\nallna = allna.drop(allna[allna == 0].index).sort_values()\n\n\n##msv2 method to visualize missing values per columns less than threshold \ndef msv2(data, width=12, height=8, color=('silver', 'gold','lightgreen','skyblue','lightpink'), edgecolor='black'):\n    \"\"\"\n    SOURCE: https:\/\/www.kaggle.com\/amiiiney\/price-prediction-regularization-stacking\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(width, height))\n\n    allna = (data.isnull().sum() \/ len(data))*100\n    tightout= 0.008*max(allna)\n    allna = allna.drop(allna[allna == 0].index).sort_values().reset_index()\n    mn= ax.barh(allna.iloc[:,0], allna.iloc[:,1], color=color, edgecolor=edgecolor)\n    ax.set_title('Missing values percentage per column', fontsize=15, weight='bold' )\n    ax.set_xlabel('Percentage', weight='bold', size=15)\n    ax.set_ylabel('Features with missing values', weight='bold')\n    plt.yticks(weight='bold')\n    plt.xticks(weight='bold')\n    for i in ax.patches:\n        ax.text(i.get_width()+ tightout, i.get_y()+0.1, str(round((i.get_width()), 2))+'%',\n            fontsize=10, fontweight='bold', color='grey')\n    return plt.show()","27032517":"#call method msv2 to visualization missing value per columns less than threshold\nmsv2(c)","b992278b":"NA=c[allna.index.to_list()]","71e54ccd":"NAcat=NA.select_dtypes(include='object')\nNAnum=NA.select_dtypes(exclude='object')\nprint('We have :',NAcat.shape[1],'categorical features with missing values')\nprint('We have :',NAnum.shape[1],'numerical features with missing values')","60cb48b9":"NAnum.head()","60dfd5ef":"NANUM= NAnum.isnull().sum().to_frame().sort_values(by=[0]).T\ncm = sns.light_palette(\"lime\", as_cmap=True)\n\nNANUM = NANUM.style.background_gradient(cmap=cm)\nNANUM","e29ccb89":"#complete missing age with median\n\nc['Age']=c.Age.fillna(c.Age.median())\n\n#complete missing Fare (ticket price) with median\n\nc['Fare']=c.Age.fillna(c.Fare.median())\n","db8e783e":"bb=c[allna.index.to_list()]\nnan=bb.select_dtypes(exclude='object')\nN= nan.isnull().sum().to_frame().sort_values(by=[0]).T\ncm = sns.light_palette(\"lime\", as_cmap=True)\n\nN= N.style.background_gradient(cmap=cm)\nN","04bc90d5":"NAcat.head()","dcafce11":"NAcat1= NAcat.isnull().sum().to_frame().sort_values(by=[0]).T\ncm = sns.light_palette(\"lime\", as_cmap=True)\n\nNAcat1 = NAcat1.style.background_gradient(cmap=cm)\nNAcat1","1f8e31fa":"#complete embarked with mode\n    \nc['Embarked'].fillna(c['Embarked'].mode()[0], inplace = True)","d4e36682":"# Replace the Cabin number by the type of cabin 'X' if not\nc[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in c['Cabin'] ])","db9c85d0":"FillNA=c[allna.index.to_list()]\n\n\n\nFillNAcat=FillNA.select_dtypes(include='object')\n\nFC= FillNAcat.isnull().sum().to_frame().sort_values(by=[0]).T\ncm = sns.light_palette(\"lime\", as_cmap=True)\n\nFC= FC.style.background_gradient(cmap=cm)\nFC","d9bfa3e0":"FillNAnum=FillNA.select_dtypes(exclude='object')\n\nFM= FillNAnum.isnull().sum().to_frame().sort_values(by=[0]).T\ncm = sns.light_palette(\"lime\", as_cmap=True)\n\nFM= FM.style.background_gradient(cmap=cm)\nFM","c543b4a0":"c.isnull().sum().sort_values(ascending=False).head(10)","5610a007":"c.shape","888f90dd":"c.head(1)","93723e9f":"c['Title'] = c['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n","f205106b":"\nc_t=pd.concat([c, y_target], axis=1)","f66d7f99":"pd.crosstab(c_t['Title'], c_t['Sex'])","8d53ad6a":"#we will just 'None' in categorical features\n#Categorical missing values\nNAcols=c.columns\nfor col in NAcols:\n    if c[col].dtype == \"object\":\n        c[col] = c[col].fillna(\"None\")\nc['Title'] = c['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n\nc_t=pd.concat([c, y_target], axis=1)\nc_t[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","659010ad":"#for dataset in c:\n\n\n\n\nc['Title'] = c['Title'].replace('Mlle', 'Miss')\nc['Title'] = c['Title'].replace('Ms', 'Miss')\nc['Title'] = c['Title'].replace('Mme', 'Mrs')\n\n\nc_t=pd.concat([c, y_target], axis=1)\nc_t[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n\n\n","2ddd9dcd":"#for dataset in c:\nc['FamilySize'] = c['SibSp'] + c['Parch'] + 1\n\n\n\nc_t=pd.concat([c, y_target], axis=1)\nc_t[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n\n#c_t[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean()","1cb5c993":"c['IsAlone'] = 0\nc.loc[c['FamilySize'] == 1, 'IsAlone'] = 1\n\n\nc_t=pd.concat([c, y_target], axis=1)\nprint (c_t[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean())","fbcd88b4":"c.loc[ c['Fare'] <= 7.91, 'Fare'] = 0\nc.loc[(c['Fare'] > 7.91) & (c['Fare'] <= 14.454), 'Fare'] = 1\nc.loc[(c['Fare'] > 14.454) & (c['Fare'] <= 31), 'Fare']   = 2\nc.loc[ c['Fare'] > 31, 'Fare'] = 3","c214dfd6":"#c['CategoricalFare'] = pd.qcut(c['Fare'], 4)\nc_t=pd.concat([c, y_target], axis=1)\nprint (c_t[['Fare', 'Survived']].groupby(['Fare'], as_index=False).mean())","4efa2184":"  \nc.loc[c['Age']<16,'Age'] = 0\nc.loc[(c['Age']>16) & (c['Age']<=32),'Age'] =1\nc.loc[(c['Age']>32) & (c['Age']<=48),'Age'] =2\nc.loc[(c['Age']>48) & (c['Age']<=64),'Age'] =3\nc.loc[c['Age']>64,'Age'] =4\n    \n#c['CategoricalAge'] = pd.qcut(c['Age'], 4)\nc_t=pd.concat([c, y_target], axis=1)\nprint (c_t[['Age', 'Survived']].groupby(['Age'], as_index=False).mean())    \n    \n    \n    \n","87c11896":"c_t.head(3)","442ac341":"c.loc[c['Ticket']=='LINE']","6375791f":"c['Ticket'] = c['Ticket'].replace('LINE','LINE 0')","1eea6c96":"# remove dots and slashes\nc['Ticket'] = c['Ticket'].apply(lambda x: x.replace('.','').replace('\/','').lower())\ndef get_prefix(ticket):\n    lead = ticket.split(' ')[0][0]\n    if lead.isalpha():\n        return ticket.split(' ')[0]\n    else:\n        return 'NoPrefix'\n    \nc['Prefix'] = c['Ticket'].apply(lambda x: get_prefix(x))","e76f0078":"c['TNumeric'] = c['Ticket'].apply(lambda x: int(x.split(' ')[-1])\/\/1)\nc['TNlen'] = c['TNumeric'].apply(lambda x : len(str(x)))\nc['LeadingDigit'] = c['TNumeric'].apply(lambda x : int(str(x)[0]))\nc['TGroup'] = c['Ticket'].apply(lambda x: str(int(x.split(' ')[-1])\/\/10))\n\nc.head()","f442f25e":"#c['CategoricalAge'] = pd.qcut(c['Age'], 4)\nc_t=pd.concat([c, y_target], axis=1)\nprint (c_t[['LeadingDigit', 'Survived']].groupby(['LeadingDigit'], as_index=False).mean())    \n    ","2f498c34":"#c['CategoricalAge'] = pd.qcut(c['Age'], 4)\nc_t=pd.concat([c, y_target], axis=1)\nprint (c_t[['Cabin', 'Survived']].groupby(['Cabin'], as_index=False).mean())    \n    ","0c125e39":"c['Cabin'] = c['Cabin'].replace('A', 'Y')\nc['Cabin'] = c['Cabin'].replace('B', 'Y')\nc['Cabin'] = c['Cabin'].replace('C', 'Y')\nc['Cabin'] = c['Cabin'].replace('D', 'Y')\nc['Cabin'] = c['Cabin'].replace('E', 'Y')\nc['Cabin'] = c['Cabin'].replace('F', 'Y')\nc['Cabin'] = c['Cabin'].replace('G', 'Y')\nc['Cabin'] = c['Cabin'].replace('T', 'Y')\nc['Cabin'] = c['Cabin'].replace('X', 'X')","d81b5e62":"#c['CategoricalAge'] = pd.qcut(c['Age'], 4)\nc_t=pd.concat([c, y_target], axis=1)\nprint (c_t[['Cabin', 'Survived']].groupby(['Cabin'], as_index=False).mean())   ","6851b6cc":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nc['Title'] = c['Title'].map(title_mapping)\nc['Title'] = c['Title'].fillna(0)\n\n\n\nc['Sex'] = c['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\nc.head()\n\nc['Embarked'] = c['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\nc.head(10)","3f64d9b2":"\n#c['Sex_Class_Embark'] = 0 \n    \nc.loc[(c['Sex'] == 1) & ((c['Pclass'] == 1) | (c['Pclass'] == 2) ) & ((c['Embarked'] == 0)  | (c['Embarked'] == 1)  | (c['Embarked'] == 2)),'Sex_Class_Embark'] = 0\n    \n    \nc.loc[(c['Sex'] == 1) & (c['Pclass'] == 3) & ((c['Embarked'] == 1)  | (c['Embarked'] == 2)),'Sex_Class_Embark'] = 1\n    \n\nc.loc[(c['Sex'] == 0) & (c['Pclass'] == 1) & ((c['Embarked'] == 0)  | (c['Embarked'] == 1)),'Sex_Class_Embark'] = 2\nc.loc[(c['Sex'] == 1) & (c['Pclass'] == 3) & (c['Embarked'] == 0),'Sex_Class_Embark'] = 2\n    \n    \nc.loc[(c['Sex'] == 0) & ((c['Pclass'] == 2)  | (c['Pclass'] == 3) ) & ((c['Embarked'] == 0)  | (c['Embarked'] == 1)  | (c['Embarked'] == 2)),'Sex_Class_Embark'] = 3\n    \n    \nc.loc[(c['Sex'] == 0) & ((c['Pclass'] == 1)  |(c['Pclass'] == 2) ) & (c['Embarked'] == 2),'Sex_Class_Embark'] = 4\n    \n    ","9f6b7364":"#c['CategoricalFare'] = pd.qcut(c['Fare'], 4)\nc_t=pd.concat([c, y_target], axis=1)\nprint (c_t[['Sex_Class_Embark', 'Survived']].groupby(['Sex_Class_Embark'], as_index=False).mean())","ceebb891":"\n\nc.head(2)","7534584f":"print (c_t[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False))\nprint('_'*60)\n\nprint (c_t[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False))\nprint('_'*60)\n\n\nprint (c_t[['Sex_Class_Embark', 'Survived']].groupby(['Sex_Class_Embark'], as_index=False).mean().sort_values(by='Survived', ascending=False))\nprint('_'*60)\n#print (c_t[['Name', 'Survived']].groupby(['Name'], as_index=False).mean().sort_values(by='Survived', ascending=False))\n#print('_'*60)\n\nprint (c_t[['Title', 'Survived']].groupby(['Title'], as_index=False).mean().sort_values(by='Survived', ascending=False))\nprint('_'*60)\n\nprint (c_t[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False))\nprint('_'*60)\n\n\n#print (c_t[['Ticket', 'Survived']].groupby(['Ticket'], as_index=False).mean().sort_values(by='Survived', ascending=False))\n#print('_'*60)\n\n\nprint (c_t[['Age', 'Survived']].groupby(['Age'], as_index=False).mean().sort_values(by='Survived', ascending=False))\nprint('_'*60)\n\n\n\nprint (c_t[['Fare', 'Survived']].groupby(['Fare'], as_index=False).mean().sort_values(by='Survived', ascending=False))\nprint('_'*60)\n\nprint (c_t[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False))\nprint('_'*60)\n\n\nprint (c_t[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False))\nprint('_'*60)\n\n\nprint (c_t[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False))\nprint('_'*60)\n\n\nprint (c_t[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean().sort_values(by='Survived', ascending=False))\nprint('_'*60)\n\nprint (c_t[['Cabin', 'Survived']].groupby(['Cabin'], as_index=False).mean().sort_values(by='Survived', ascending=False))\nprint('_'*60)\n\n\n","e3b81d1d":"#c = c.drop([ 'Parch' ,'SibSp','FamilySize',], axis = 1)\n#c = c.drop(['Name','Ticket' ,'Parch', 'SibSp' ,'FamilySize' ,'Sex_Class_Embark' ], axis = 1)\n#c = c.drop(['Name','Ticket' ,'Parch', 'SibSp' ,'FamilySize' ,'Sex_Class_Embark'], axis = 1)\n#c = c.drop(['Name','Ticket','TNumeric','FamilySize' ,'Parch', 'SibSp' ], axis = 1)\nc = c.drop(['Name','Ticket','TNumeric','FamilySize'], axis = 1)","ee9a7bdf":"c.isnull().sum().sort_values(ascending=False).head(20)","2613ddb2":"c.describe()","ac539704":"c.describe(include=['O'])","401f605a":"\nc.head(2) ","ee5d08cb":"#c = c.drop(['Name','Ticket','TNumeric','FamilySize' ,'Parch' , 'SibSp','Sex_Class_Embark'], axis = 1)\n#c['Sex_Class_Embark'] = c['Sex_Class_Embark'].astype(str)\n#c = c.drop(['Name','Ticket','TNumeric','FamilySize'], axis = 1)\nc['LeadingDigit'] = c['LeadingDigit'].astype(str)\nc['TNlen'] = c['TNlen'].astype(str)\nc['IsAlone'] = c['IsAlone'].astype(str)\n#c['FamilySize'] = c['FamilySize'].astype(str)\nc['Title'] = c['Title'].astype(str)\nc['Embarked'] = c['Embarked'].astype(str)\nc['Fare'] = c['Fare'].astype(str)\nc['Parch'] = c['Parch'].astype(str)\nc['SibSp'] = c['SibSp'].astype(str)\n\nc['Age'] = c['Age'].astype(str)\nc['Sex'] = c['Sex'].astype(str)\nc['Pclass'] = c['Pclass'].astype(str)\nc['Cabin'] = c['Cabin'].astype(str)\nc['Sex_Class_Embark'] = c['Sex_Class_Embark'].astype(str) \n\n ","5df4b05c":"c.describe(include=['O'])","86f0edf6":"c.shape","b0083039":"c.describe()","491cff2e":"cb=pd.get_dummies(c)\nprint(\"the shape of the original dataset\",c.shape)\nprint(\"the shape of the encoded dataset\",cb.shape)\nprint(\"We have \",cb.shape[1]- c.shape[1], 'new encoded features')","3ec19a04":"cb.head(2)","34c83621":"cb1=pd.get_dummies(c,drop_first=True)\nprint(\"the shape of the original dataset\",c.shape)\nprint(\"the shape of the encoded dataset\",cb1.shape)\nprint(\"We have \",cb1.shape[1]- c.shape[1], 'new encoded features')","955fb64c":"cb1.head(2)","fd0e8cd0":"# dummy with  dont drop frist\nTrain = cb[:na]  #na is the number of rows of the original training set\n                 \nTest = cb[na:]  #testset  after clean missing values and feature engineering and encoder  we do NOT apply outliers on it\n\n# dummy with  drop frist\ntrain_1 = cb1[:na] \ntest_1 =  cb1[na:]","d305136e":"print(train_1.shape)\nprint(y_target.shape)\nprint(test_1.shape)\n","59550b14":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import LabelEncoder\n\n# NAIBE BAYES\nfrom sklearn.naive_bayes import GaussianNB\n#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n#RANDOM FOREST\nfrom sklearn.ensemble import RandomForestClassifier\n#LOGISTIC REGRESSION\nfrom sklearn.linear_model import LogisticRegression\n#SVM\nfrom sklearn.svm import SVC\n#DECISON TREE\nfrom sklearn.tree import DecisionTreeClassifier\n#XGBOOST\nfrom xgboost import XGBClassifier\n#AdaBoosting Classifier\nfrom sklearn.ensemble import AdaBoostClassifier\n#GradientBoosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n#HistGradientBoostingClassifier\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier, StackingClassifier\n\nfrom sklearn.model_selection import cross_val_score,StratifiedKFold,GridSearchCV\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler ,Normalizer , MinMaxScaler, RobustScaler \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")","8dc2143a":"X_train,X_test,y_train,y_test = train_test_split(train_1,y_target,test_size=0.30 ,  shuffle=True, random_state=42)","8cb15ae9":"sk_fold = StratifiedKFold(10,shuffle=True, random_state=42)\n#sc =StandardScaler()\n\nsc =StandardScaler()\n#sc =Normalizer()\n#sc = MinMaxScaler()\n\n\nX_train= sc.fit_transform(X_train)\n\nX_train_1= sc.transform(train_1.values)\n\nX_test= sc.transform(X_test)\n\nX_submit= sc.transform(test_1.values)\n\n\ng_nb = GaussianNB()\nknn = KNeighborsClassifier()\nran_for  = RandomForestClassifier()\nlog_reg = LogisticRegression()\nsvc = SVC()\ntree= DecisionTreeClassifier()\nxgb = XGBClassifier()\n\nada_boost = AdaBoostClassifier()\ngrad_boost = GradientBoostingClassifier(n_estimators=100)\nhist_grad_boost = HistGradientBoostingClassifier()\n\n\n\n\nclf = [(\"Naive Bayes\",g_nb,{}),\\\n       (\"K Nearest\",knn,{\"n_neighbors\":[3,5,8],\"leaf_size\":[25,30,35]}),\\\n       (\"Random Forest\",ran_for,{\"n_estimators\":[100],\"random_state\":[42],\"min_samples_leaf\":[5,10,20,40,50],\"bootstrap\":[False]}),\\\n       (\"Logistic Regression\",log_reg,{\"penalty\":['l2'],\"C\":[100, 10, 1.0, 0.1, 0.01] , \"solver\":['saga']}),\\\n       (\"Support Vector\",svc,{\"kernel\": [\"linear\",\"rbf\"],\"gamma\":['auto'],\"C\":[0.1, 1, 10, 100, 1000]}),\\\n       (\"Decision Tree\", tree, {}),\\\n       (\"XGBoost\",xgb,{\"n_estimators\":[200],\"max_depth\":[3,4,5],\"learning_rate\":[.01,.1,.2],\"subsample\":[.8],\"colsample_bytree\":[1],\"gamma\":[0,1,5],\"lambda\":[.01,.1,1]}),\\\n       \n       (\"Adapative Boost\",ada_boost,{\"n_estimators\":[100],\"learning_rate\":[.6,.8,1]}),\\\n       (\"Gradient Boost\",grad_boost,{}),\\\n     \n       (\"Histogram GB\",hist_grad_boost,{\"loss\":[\"binary_crossentropy\"],\"min_samples_leaf\":[5,10,20,40,50],\"l2_regularization\":[0,.1,1]})]\n\n\nstack_list=[]\ntrain_scores = pd.DataFrame(columns=[\"Name\",\"Train Score\",\"Test Score\"])\n\ni=0\nfor name,clf1,param_grid in clf:\n    clf = GridSearchCV(clf1,param_grid=param_grid,scoring=\"accuracy\",cv=sk_fold,return_train_score=True)\n    clf.fit(X_train,y_train) #.reshape(-1,1)\n    y_pred = clf.best_estimator_.predict(X_test)\n    cm = confusion_matrix(y_test,y_pred)\n    \n    #train_scores.loc[i]= [name,cross_val_score(clf,X_train,y_train,cv=sk_fold,scoring=\"accuracy\").mean(),(cm[0,0]+cm[1,1,])\/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])]\n    train_scores.loc[i]= [name,clf.best_score_,(cm[0,0]+cm[1,1,])\/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])]\n    stack_list.append(clf.best_estimator_)\n    i=i+1\n    \nest = [(\"g_nb\",stack_list[0]),\\\n       (\"knn\",stack_list[1]),\\\n       (\"ran_for\",stack_list[2]),\\\n       (\"log_reg\",stack_list[3]),\\\n       (\"svc\",stack_list[4]),\\\n       (\"dec_tree\",stack_list[5]),\\\n       (\"XGBoost\",stack_list[6]),\\\n       (\"ada_boost\",stack_list[7]),\\\n       (\"grad_boost\",stack_list[8]),\\\n       (\"hist_grad_boost\",stack_list[9])]\n\n\n\nsc = StackingClassifier(estimators=est,final_estimator = None,cv=sk_fold,passthrough=False)\nsc.fit(X_train,y_train)\ny_pred = sc.predict(X_test)\ncm1 = confusion_matrix(y_test,y_pred)\ny_pred_train = sc.predict(X_train)\ncm2 = confusion_matrix(y_train,y_pred_train)\ntrain_scores.append(pd.Series([\"Stacking\",(cm2[0,0]+cm2[1,1,])\/(cm2[0,0]+cm2[0,1]+cm2[1,0]+cm2[1,1]),(cm1[0,0]+cm1[1,1,])\/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])],index=train_scores.columns),ignore_index=True)\n","a45cc014":"#Import Libraries\nfrom sklearn.ensemble import VotingClassifier\n#----------------------------------------------------\n#Applying VotingClassifier Model \n'''\nensemble.VotingClassifier(estimators, voting=\u2019hard\u2019, weights=None,n_jobs=None, flatten_transform=None)\n'''\n#loading Voting Classifier\nVotingClassifierModel = VotingClassifier(estimators=[(\"grad_boost\",stack_list[8]),(\"svc\",stack_list[4]) , (\"ada_boost\",stack_list[7])], voting='hard')\nVotingClassifierModel.fit(X_train, y_train)\n\n\n#Calculating Details\nprint('VotingClassifierModel Train Score is : ' , VotingClassifierModel.score(X_train, y_train))\nprint('VotingClassifierModel Test Score is : ' , VotingClassifierModel.score(X_test, y_test))\nprint('----------------------------------------------------')","2971c1f6":"y=df_train['Survived'].to_frame()\nprint(X_train_1.shape)\nprint(y.shape)\nprint(X_submit.shape)\n\n","f2de6d70":"#Calculating Prediction\n\ny_submit = VotingClassifierModel.predict(X_submit)\nsubmit = pd.DataFrame({\n        \"PassengerId\": df_test.PassengerId,\n        \"Survived\": y_submit\n    })","57d74435":"\nsubmit.PassengerId = submit.PassengerId.astype(int)\nsubmit.Survived = submit.Survived.astype(int)\nsubmit.to_csv(\"titanic_submit.csv\", index=False)","1b26ebbb":"print('Predicted Value for VotingClassifierModel is : ' , y_submit[:10])\n \n\n","25dc4cbb":"'''\n# dummy with  dont drop frist\nTrain = cb[:na]  #na is the number of rows of the original training set\n                 \nTest = cb[na:]  #testset  after clean missing values and feature engineering and encoder  we do NOT apply outliers on it\n\n# dummy with  drop frist\ntrain_1 = cb1[:na] \ntest_1 =  cb1[na:]\n\nprint(train_1.shape)\nprint(y_target.shape)\nprint(test_1.shape)\n'''","dae67869":"'''\nfrom sklearn.decomposition import PCA\n\npca = PCA(0.95, whiten=True)\npca_train_data = pca.fit_transform(train_1)\nprint(pca_train_data.shape,'\\n')\n\nexplained_variance = pca.explained_variance_ratio_ \nprint(explained_variance)\n'''","a51de447":"'''\ntrain_1=pd.DataFrame(pca_train_data)\ntest_1=pd.DataFrame(pca.transform(test_1))\n\nprint(train_1.shape)\nprint(y_target.shape)\nprint(test_1.shape)\n'''","c172db11":"'''\nX_train,X_test,y_train,y_test = train_test_split(train_1,y_target,test_size=0.30, random_state=42)\n'''","d218b8a1":"'''\nsk_fold = StratifiedKFold(10,shuffle=True, random_state=42)\n#sc =StandardScaler()\n\nsc =StandardScaler()\n#sc =Normalizer()\n#sc = MinMaxScaler()\n\n\nX_train= sc.fit_transform(X_train)\n\nX_train_1= sc.transform(train_1.values)\n\nX_test= sc.transform(X_test)\n\nX_submit= sc.transform(test_1.values)\n\n\ng_nb = GaussianNB()\nknn = KNeighborsClassifier()\nran_for  = RandomForestClassifier()\nlog_reg = LogisticRegression()\nsvc = SVC()\ntree= DecisionTreeClassifier()\nxgb = XGBClassifier()\n\nada_boost = AdaBoostClassifier()\ngrad_boost = GradientBoostingClassifier(n_estimators=100)\nhist_grad_boost = HistGradientBoostingClassifier()\n\n\n\n\nclf = [(\"Naive Bayes\",g_nb,{}),\\\n       (\"K Nearest\",knn,{\"n_neighbors\":[3,5,8],\"leaf_size\":[25,30,35]}),\\\n       (\"Random Forest\",ran_for,{\"n_estimators\":[100],\"random_state\":[42],\"min_samples_leaf\":[5,10,20,40,50],\"bootstrap\":[False]}),\\\n       (\"Logistic Regression\",log_reg,{\"penalty\":['l2'],\"C\":[100, 10, 1.0, 0.1, 0.01] , \"solver\":['saga']}),\\\n       (\"Support Vector\",svc,{\"kernel\": [\"linear\",\"rbf\"],\"gamma\":['auto'],\"C\":[0.1, 1, 10, 100, 1000]}),\\\n       (\"Decision Tree\", tree, {}),\\\n       (\"XGBoost\",xgb,{\"n_estimators\":[200],\"max_depth\":[3,4,5],\"learning_rate\":[.01,.1,.2],\"subsample\":[.8],\"colsample_bytree\":[1],\"gamma\":[0,1,5],\"lambda\":[.01,.1,1]}),\\\n       \n       (\"Adapative Boost\",ada_boost,{\"n_estimators\":[100],\"learning_rate\":[.6,.8,1]}),\\\n       (\"Gradient Boost\",grad_boost,{}),\\\n     \n       (\"Histogram GB\",hist_grad_boost,{\"loss\":[\"binary_crossentropy\"],\"min_samples_leaf\":[5,10,20,40,50],\"l2_regularization\":[0,.1,1]})]\n\n\nstack_list=[]\ntrain_scores = pd.DataFrame(columns=[\"Name\",\"Train Score\",\"Test Score\"])\n\ni=0\nfor name,clf1,param_grid in clf:\n    clf = GridSearchCV(clf1,param_grid=param_grid,scoring=\"accuracy\",cv=sk_fold,return_train_score=True)\n    clf.fit(X_train,y_train) #.reshape(-1,1)\n    y_pred = clf.best_estimator_.predict(X_test)\n    cm = confusion_matrix(y_test,y_pred)\n    \n    #train_scores.loc[i]= [name,cross_val_score(clf,X_train,y_train,cv=sk_fold,scoring=\"accuracy\").mean(),(cm[0,0]+cm[1,1,])\/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])]\n    train_scores.loc[i]= [name,clf.best_score_,(cm[0,0]+cm[1,1,])\/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])]\n    stack_list.append(clf.best_estimator_)\n    i=i+1\n    \nest = [(\"g_nb\",stack_list[0]),\\\n       (\"knn\",stack_list[1]),\\\n       (\"ran_for\",stack_list[2]),\\\n       (\"log_reg\",stack_list[3]),\\\n       (\"svc\",stack_list[4]),\\\n       (\"dec_tree\",stack_list[5]),\\\n       (\"XGBoost\",stack_list[6]),\\\n       (\"ada_boost\",stack_list[7]),\\\n       (\"grad_boost\",stack_list[8]),\\\n       (\"hist_grad_boost\",stack_list[9])]\n\n\n\n\n'''","3caa4ae5":"'''\nsc = StackingClassifier(estimators=est,final_estimator = None,cv=sk_fold,passthrough=False)\nsc.fit(X_train,y_train)\ny_pred = sc.predict(X_test)\ncm1 = confusion_matrix(y_test,y_pred)\ny_pred_train = sc.predict(X_train)\ncm2 = confusion_matrix(y_train,y_pred_train)\ntrain_scores.append(pd.Series([\"Stacking\",(cm2[0,0]+cm2[1,1,])\/(cm2[0,0]+cm2[0,1]+cm2[1,0]+cm2[1,1]),(cm1[0,0]+cm1[1,1,])\/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])],index=train_scores.columns),ignore_index=True)\n\n'''","3f030b17":"'''\n# dummy with  dont drop frist\ntrain = cb[:na]  #na is the number of rows of the original training set\n                 \ntest = cb[na:]  #testset  after clean missing values and feature engineering and encoder  we do NOT apply outliers on it\n\n# dummy with  drop frist\ntrain_1 = cb1[:na] \ntest_1 =  cb1[na:]\n\nprint(train_1.shape)\nprint(y_target.shape)\nprint(test_1.shape)\nprint(\"****************************************************\")\nprint(train.shape)\nprint(y_target.shape)\nprint(test.shape)\n\n'''","cce5094f":"'''\n# Importing libraries\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.svm import SVC\n\nX = np.r_[train,test]\nprint('X shape :',X.shape)\nprint('\\n')\n\n# USING THE GAUSSIAN MIXTURE MODEL \n\n#The Bayesian information criterion (BIC) can be used to select the number of components in a Gaussian Mixture in an efficient way. \n#In theory, it recovers the true number of components only in the asymptotic regime\nlowest_bic = np.infty\nbic = []\nn_components_range = range(1, 7)\n\n#The GaussianMixture comes with different options to constrain the covariance of the difference classes estimated: \n# spherical, diagonal, tied or full covariance.\ncv_types = ['spherical', 'tied', 'diag', 'full']\nfor cv_type in cv_types:\n    for n_components in n_components_range:\n        gmm = GaussianMixture(n_components=n_components,covariance_type=cv_type)\n        gmm.fit(X)\n        bic.append(gmm.aic(X))\n        if bic[-1] < lowest_bic:\n            lowest_bic = bic[-1]\n            best_gmm = gmm\n            \nbest_gmm.fit(X)\ngmm_train = best_gmm.predict_proba(train)\ngmm_test = best_gmm.predict_proba(test)\n'''","5ccbeba6":"'''\nX_train,X_test,y_train,y_test = train_test_split(gmm_train,y_target,test_size=0.30, random_state=101)\nX_train.shape,X_test.shape,y_train.shape,y_test.shape\n'''","82570d4b":"'''\nsk_fold = StratifiedKFold(10,shuffle=True, random_state=42)\n\n\nX_train= X_train\n\n\nX_train_1= pd.DataFrame(gmm_train).values\n\nX_test= X_test\n\nX_submit =  pd.DataFrame(gmm_test).values\n\ng_nb = GaussianNB()\nknn = KNeighborsClassifier()\nran_for  = RandomForestClassifier()\nlog_reg = LogisticRegression()\nsvc = SVC()\ntree= DecisionTreeClassifier()\nxgb = XGBClassifier()\n\nada_boost = AdaBoostClassifier()\ngrad_boost = GradientBoostingClassifier(n_estimators=100)\nhist_grad_boost = HistGradientBoostingClassifier()\n\n\n\n\nclf = [(\"Naive Bayes\",g_nb,{}),\\\n       (\"K Nearest\",knn,{\"n_neighbors\":[3,5,6,7,8,9,10],\"leaf_size\":[25,30,35]}),\\\n       (\"Random Forest\",ran_for,{\"n_estimators\":[10, 50, 100, 200,400],\"max_depth\":[3, 10, 20, 40],\"random_state\":[99],\"min_samples_leaf\":[5,10,20,40,50],\"bootstrap\":[False]}),\\\n       (\"Logistic Regression\",log_reg,{\"penalty\":['l2'],\"C\":[100, 10, 1.0, 0.1, 0.01] , \"solver\":['saga']}),\\\n       (\"Support Vector\",svc,{\"kernel\": [\"linear\",\"rbf\"],\"gamma\":[0.05,0.0001,0.01,0.001],\"C\":[0.1, 1, 10, 100, 1000]},),\\\n      \n       (\"Decision Tree\", tree, {}),\\\n       (\"XGBoost\",xgb,{\"n_estimators\":[200],\"max_depth\":[3,4,5],\"learning_rate\":[.01,.1,.2],\"subsample\":[.8],\"colsample_bytree\":[1],\"gamma\":[0,1,5],\"lambda\":[.01,.1,1]}),\\\n       \n       (\"Adapative Boost\",ada_boost,{\"n_estimators\":[100],\"learning_rate\":[.6,.8,1]}),\\\n       (\"Gradient Boost\",grad_boost,{}),\\\n     \n       (\"Histogram GB\",hist_grad_boost,{\"loss\":[\"binary_crossentropy\"],\"min_samples_leaf\":[5,10,20,40,50],\"l2_regularization\":[0,.1,1]})]\n\n\nstack_list=[]\ntrain_scores = pd.DataFrame(columns=[\"Name\",\"Train Score\",\"Test Score\"])\n\ni=0\nfor name,clf1,param_grid in clf:\n    clf = GridSearchCV(clf1,param_grid=param_grid,scoring=\"accuracy\",cv=sk_fold,return_train_score=True)\n    clf.fit(X_train,y_train) #.reshape(-1,1)\n    y_pred = clf.best_estimator_.predict(X_test)\n    cm = confusion_matrix(y_test,y_pred)\n    \n    #train_scores.loc[i]= [name,cross_val_score(clf,X_train,y_train,cv=sk_fold,scoring=\"accuracy\").mean(),(cm[0,0]+cm[1,1,])\/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])]\n    train_scores.loc[i]= [name,clf.best_score_,(cm[0,0]+cm[1,1,])\/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])]\n    stack_list.append(clf.best_estimator_)\n    i=i+1\n    \nest = [(\"g_nb\",stack_list[0]),\\\n       (\"knn\",stack_list[1]),\\\n       (\"ran_for\",stack_list[2]),\\\n       (\"log_reg\",stack_list[3]),\\\n       (\"svc\",stack_list[4]),\\\n       (\"dec_tree\",stack_list[5]),\\\n       (\"XGBoost\",stack_list[6]),\\\n       (\"ada_boost\",stack_list[7]),\\\n       (\"grad_boost\",stack_list[8]),\\\n       (\"hist_grad_boost\",stack_list[9])]\n\n\n\n\n'''\n","366d34a7":"'''\nsc = StackingClassifier(estimators=est,final_estimator = None,cv=sk_fold,passthrough=False)\nsc.fit(X_train,y_train)\ny_pred = sc.predict(X_test)\ncm1 = confusion_matrix(y_test,y_pred)\ny_pred_train = sc.predict(X_train)\ncm2 = confusion_matrix(y_train,y_pred_train)\ntrain_scores.append(pd.Series([\"Stacking\",(cm2[0,0]+cm2[1,1,])\/(cm2[0,0]+cm2[0,1]+cm2[1,0]+cm2[1,1]),(cm1[0,0]+cm1[1,1,])\/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])],index=train_scores.columns),ignore_index=True)\n\n'''","e205fc59":"'''\ny=df_train['Survived'].to_frame()\n'''","08b5a038":"'''\nprint(X_train_1.shape)\nprint(y.shape)\nprint(X_submit.shape)\n'''\n\n","3025b74d":"\n'''\nstack_list[7].fit(X_train_1,y)\ny_submit = stack_list[7].predict(X_submit)\nsubmit = pd.DataFrame({\n        \"PassengerId\": df_test.PassengerId,\n        \"Survived\": y_submit\n    })\n    \n'''","a6521473":"    \n'''\nsubmit.PassengerId = submit.PassengerId.astype(int)\nsubmit.Survived = submit.Survived.astype(int)\nsubmit.to_csv(\"titanic_submit.csv\", index=False)\n    \n'''","bec61bcf":"    \n'''\nsubmit.PassengerId = submit.PassengerId.astype(int)\nsubmit.Survived = submit.Survived.astype(int)\nsubmit.to_csv(\"titanic_submit.csv\", index=False)\n    \n'''","90fd6391":"### 1.5-Ticket\n","5161ef8f":"### Quantitative data:\n*  discrete, continuous\n* are measures of values or counts and are expressed as numbers.\n\n* Quantitative data are data about numeric variables (e.g. how many; how much; or how often).\n\n### Qualitative data:\n*  ordinal , nominal\n* are measures of 'types' and may be represented by a name, symbol, or a number code.\n\n* Qualitative data are data about categorical variables (e.g. what type).","7ae020dd":"### 4.2 One hot encoding","fa454b47":"#### Next we extract the numeric portion of the ticket. We create extra features such as:\n\n * 1-The numerical component (TNumeric)\n * 2-Number of digits (TNlen)\n * 3-Leading digit (LeadingDigit)\n * 4-Group similar tickets by discarding the last digits (TGroup)\n \nTGroup is a feature that I thought it would help to capture what has been discussed in here. The idea is similar ticket numbers (not just identical numbers) is tied to groups of people. This is important because we know that identifying groups is helpful and looking at just family ties is not sufficient for the best model.","d8af02d3":"We fit and predict on the best SVC model that we derived based on the scores.","f62084e6":"## Outline\nThe following sections are included in this notebook:\n\n### A. [Load and Parse Data](#section-one)\n\n### B. [Exploratory Data Analysis (EDA)](#section-two)\n   1. [Missing Data](#section-two-a)    \n   2. [Distribution of the Target Variable](#section-two-b)    \n   3. [Distribution of the Numeric Feature Variable](#section-two-c)\n   4. [Outliers](#section-two-d)\n   5. [Categorical Feature Cardinality](#section-two-e)\n    \n### C. [Preprocessing](#section-three)\n   1. [Initial Preprocessing](#section-three-a)\n   2. [Building a Preprocessing Pipeline](#section-three-b)\n       * [Define custom transformers](#section-three-b1)\n       * [Define helper functions](#section-three-b2)\n       * [Define Training Data](#section-three-b3)\n       * [Define datatypes and encoding](#section-three-b4)\n       * [Build the preprocessing pipeline](#section-three-b5)\n      \n        \n### D. [Fit and Evaluate the Model](#section-four)\n   1. [Cross-Validation](#section-four-a)\n       \n       * Naive Bayes classifier\n       * KNN or k-Nearest Neighbors  \n       * Random Forest \n       * Logistic Regression             \n       * Support Vector Machines  \n       * Decision Tree\n       * XGBOOST Classifier \n       * AdaBoosting Classifier\n       * GradientBoostingClassifier \n       * HistGradientBoostingClassifier\n       * Principal Component Analysis (PCA) \n       * Gaussian Mixture \n       * Grid Search\n      \n   \n   2. [Model Stacking](#section-four-c)\n    \n### E. [Predict Test Dataset and Submit](#section-five)","ed0ecd39":"## **Applying Gaussian Mixture and Grid Search to improve the accuracy**\n\nWe select the above three algorithms **(KNN, Random Forest and SVM)** which  gave maximum accuracy for further analysis","1bbce691":"* But before going any further, we start by cleaning the data from missing values. I set the threshold to 21% (red line), all columns with more than 21% missing values will be dropped.\n\n* First thing to do is get rid of the features with more than 21 % missing values (figure above). For example the Cabine's missing values ,. But replacing those (more than 21%) missing values with \"no Cabine\" will leave us with a feature with low variance, and low variance features are uniformative for machine learning models. So we drop the features with more than 21 % missing values.\n\n* Features with >21 % missing values , we will drop","ac8ea49c":"#### Extract new feature after mapping","0dea356c":"## Model, predict and solve\n\nNow we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n\n- Grid Search\n- Naive Bayes classifier\n- KNN or k-Nearest Neighbors\n- Random Forrest\n- Logistic Regression\n- Support Vector Machines\n- Decision Tree\n- XGBOOST Classifier\n- AdaBoosting Classifier\n- GradientBoostingClassifier\n- HistGradientBoostingClassifier\n- Principal Component Analysis (PCA)\n- Gaussian Mixture","19d75f32":"Introducing another concept now i.e. **K-Fold Cross-validation**, sometimes called rotation estimation, is a model validation technique for assessing how the results of a statistical analysis will generalize to an independent data set.\n\nCross-Validation can be used to evaluate performance of a model by handling the variance problem of the result set.\n\nIn this approach, the data used for training and testing are non-overlapping. To implement, first separate your data set into two subsets. One subset you use for training and other for testing. Now, do the exercise again by swapping the data sets. Report the average test result. This is call 2-fold cross validation. \n\nSimilarly if you divide your entire data set in to five sub sets and perform the exercise ten times and report the average test result then that would be 10-fold cross validation (which is what we'll be doing now).","d9115145":"**What is the distribution of numerical feature values across the samples?**\n\nThis helps us determine, among other early insights, how representative is the training dataset of the actual problem domain.\n\n- Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).\n- Survived is a categorical feature with 0 or 1 values.\n- Around 38% samples survived representative of the actual survival rate at 32%.\n- Most passengers (> 75%) did not travel with parents or children.\n- Nearly 30% of the passengers had siblings and\/or spouse aboard.\n- Fares varied significantly with few passengers (<1%) paying as high as $512.\n- Few elderly passengers (<1%) within age range 65-80.","15c005ce":"### We isolate the missing values from the rest of the dataset to have a good idea of how to treat them","fbae3ca0":"<a id=\"section-two-a\"><\/a>\n### 1. Missing Data\n\nWe have collected several assumptions and decisions regarding our datasets and solution requirements. So far we did not have to change a single feature or value to arrive at these. Let us now execute our decisions and assumptions for correcting, creating, and completing goals.\n\n* Correcting by dropping features\n* Correcting by fill features\n","d86fc821":"### Target Distribution","2be98f20":"<a id=\"section-two\"><\/a>\n# B. Exploratory Data Analysis (EDA)\nThe purpose of EDA is to get familiar with our data, but not so familiar that we begin making assumptions about the model fit! In Kaggle competitions it can be tempting to overfit the training data in hopes of a lower test score, but this often doesn't bode well for real world applications. Typically, it's best to let the data speak for itself and allow the model the flexibility to find correlations between the target and features. Afterall, the models in todays age are very robust. \n\n### Do Preprocessing Later!\nThis is really more of a personal opinion. I find it hard to keep track of data processing done in cells throughout an EDA section. Typically, I prefer to do all the preprocessing in a single code block or even better in a pipeline. This way I know the preprocessing is being applied the same way to the train, validation, and test datasets. I use EDA as a way to identify the preprocessing steps that need to take place and potential feature engineering opportunities. \n\nRemember, it's best to do preprocessing in a pipeline!!!\n\nIn this section I will explore the following common issues:\n1. Missing Data\n2. Distribution of the Target Variable\n3. Distribution of the Feature Variables\n4. Outliers\n5. Categorical Feature Cardinality","b0d0bb7c":"* so we have titles. let's categorize it and check the title impact on survival rate.","afbdacd2":"\nWe fit and predict on the best XGB model that we derived based on the scores.","00e7fcb2":"# check shape of training and test set","f7cc4dca":"### Correlating categorical features\n\nNow we can correlate categorical features with our solution goal.\n\n**Observations.**\n\n- Female passengers had much better survival rate than males. Confirms classifying (#1).\n- Exception in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily direct correlation between Embarked and Survived.\n- Males had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports. Completing (#2).\n- Ports of embarkation have varying survival rates for Pclass=3 and among male passengers. Correlating (#1).\n\n**Decisions.**\n\n- Add Sex feature to model training.\n- Complete and add Embarked feature to model training.","99519ae6":"### 1.3-Fare categorize it into 4 ranges.","f3ebc3f0":"### 4.1 Numerical features:\nWe start with numerical features that are actually categorical, for example \"Pclass\", the values are from 1 to 12, each number is assigned to a month November is number 11 while March is number 3. 11 is just the order of the months and not a given value, so we convert the \"Month Sold\" feature to categorical","a2c80986":"## Analyze by describing data\nPandas also helps describe the datasets answering following questions early in our project.\n\n\n### Which features are available in the dataset?\n\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [link](https:\/\/www.kaggle.com\/c\/titanic\/data)","062429c0":"###  1.2-SibSp and Parch (Family)","d0138199":"##### Categorical ","a1fd811d":"### Missing values percentage per column with less than 21 %","bb4ff335":"### 1.4-Age\n* Age is a continous feature, there is a problem with Continous Variables in Machine Learning Models.\n* We need to convert these continous values into categorical values\n* The maximum age of a passenger was 80. So lets divide the range from 0-80 into 5 bins. So 80\/5=16. So bins of size 16","d520a2ad":"\n\n* 1.The Survived variable is our outcome or dependent variable. It is a binary nominal datatype of 1 for survived and 0 for did   not   survive. All other variables are potential predictor or independent variables. It's important to note, more predictor variables   do not make a better model, but the right variables.\n\n* 2.The PassengerID and Ticket variables are assumed to be random unique identifiers, that have no impact on the outcome variable.     Thus, they will be excluded from analysis.\n\n* 3.The Pclass variable is an ordinal datatype for the ticket class, a proxy for socio-economic status (SES), representing 1 = upper   class, 2 = middle class, and 3 = lower class.\n\n* 4.The Name variable is a nominal datatype. It could be used in feature engineering to derive the gender from title, family size     from surname, and SES from titles like doctor or master. Since these variables already exist, we'll make use of it to see if       title, like master, makes a difference.\n\n* 5.The Sex and Embarked variables are a nominal datatype. They will be converted to dummy variables for mathematical calculations.\n\n* 6.The Age and Fare variable are continuous quantitative datatypes.\n\n* 7.The SibSp represents number of related siblings\/spouse aboard and Parch represents number of related parents\/children aboard.\n  Both are discrete quantitative datatypes. \n  This can be used for feature engineering to create a family size and is alone variable.\n\n* 8.The Cabin variable is a nominal datatype that can be used in feature engineering for approximate position on ship when the         incident occurred and SES from deck levels. However, since there are many null values, it does not add value and thus is           excluded from analysis.","2e81b1f4":"### What are the data types for various features?\n\n* Helping us during converting goal.\n\n","22f4c2fa":"### Now what do we do in combine data that contains less than 21% missing values","89837c00":"#### Categorical Feature","c4608873":"Exporting the data to submit.","1bda7d99":"* With the number of siblings\/spouse and the number of children\/parents we can create new feature called Family Size.","de635ba0":"We further split the training set in to a train and test set to validate our model.","e6b1b240":"# 1.6-Cabin","df431b58":"# 4- Encoding categorical features:","1d5c86fa":"Coming to the modeling part. We first scale the data using standard scaler.\nWe use grid search with stratified kfold validation for 9 algorithms.\nWe get the scores from the cross validation for all these models and run a prediction on the test data from our train_test_split.\nFor stacking we get the accuracy based on fitting the train and test set.","c4ee7529":"<a id=\"section-one\"><\/a>\n# A. Load and Parse Data\n\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames.\nWe also combine these datasets to run certain operations on both datasets together.","ffdd01e4":"#### Which features are categorical?\n\n\n * These values classify the samples into sets of similar samples.\n * Within categorical features are the values nominal, ordinal, ratio, or interval based? \n * Among other things this helps us select the appropriate plots for visualization.","970b09d4":"#### Creating new feature extracting from existing\n\nWe want to analyze if Name feature can be engineered to extract titles and test correlation between titles and survival, before dropping Name and PassengerId features.\n\nIn the following code we extract Title feature using regular expressions. The RegEx pattern `(\\w+\\.)` matches the first word which ends with a dot character within Name feature. The `expand=False` flag returns a DataFrame.\n\n**Observations.**\n\nWhen we plot Title, Age, and Survived, we note the following observations.\n\n- Most titles band Age groups accurately. For example: Master title has Age mean of 5 years.\n- Survival among Title Age bands varies slightly.\n- Certain titles mostly survived (Mme, Lady, Sir) or did not (Don, Rev, Jonkheer).\n\n**Decision.**\n\n- We decide to retain the new Title feature for model training.","53f9ff9a":"## Descriptive statistics","ebfb2116":"* We can replace many titles with a more common name or classify them as `Rare`.","61b51c7f":"Exporting the data to submit.","9064b94a":"### Mapping Feature ","e08b2ca4":"The **predict_proba** method will take in new data points and predict the responsibilities for each Gaussian. In other words, the probability that this data point came from each distribution.\n\n\n\n**Now Applying Grid Search Algorithm:** \n\nTo identify the best algorithm and best parameters","4d300669":"### Author: Abdelwahed Ashraf \n\n### Linkedin: [Link](https:\/\/www.linkedin.com\/in\/abdelwahed-ashraf-090523169\/)\n\n### Kaggle: [Link](https:\/\/www.kaggle.com\/abdelwahed43)\n\n*If this Kernel helped you in any way,I would be very much appreciated to your <font color='red'>UPVOTES<\/font>*\n\n# Notebook Objective\nThe objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\n\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\n\n## Overview\nThe Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n\n### Why using pipelines to do preprocessing is important?\nPipelines are great because they package your data processing and model fitting into a single package. This makes it easy to manage complex models and do feature engineering. Pipelines are even better becuase they eliminate aspects of data leakage. \n\nData leakage is a major problem when developing a machine learning model. Typically, it causes a model to have seemingly high performance in the training and even validation stages. However, once the model is deployed in production (i.e. predicting unseen test data) it is likely to perform much worse than anticipated. A common cause of data leakage is Train-Test split contamination. This can occur, for example, when a preprocessing step is fit to both train and validation datasets (i.e. fitting a SimpleImpute before calling train-test split). With pipelines this can easily be avoided as perprocessing is applied independtly on the train and validation data. \n","cc22096c":"### * Numerical features:","ffec97aa":"### Correlating numerical and ordinal features\n\nWe can combine multiple features for identifying correlations using a single plot. This can be done with numerical and categorical features which have numeric values.\n\n**Observations.**\n\n- Pclass=3 had most passengers, however most did not survive. Confirms our classifying assumption #2.\n- Infant passengers in Pclass=2 and Pclass=3 mostly survived. Further qualifies our classifying assumption #2.\n- Most passengers in Pclass=1 survived. Confirms our classifying assumption #3.\n- Pclass varies in terms of Age distribution of passengers.\n\n**Decisions.**\n\n- Consider Pclass for model training.","4854d0a0":"#### Which features are numerical?\n\n* Which features are numerical? These values change from sample to sample.\n* Within numerical features are the values discrete, continuous, or timeseries based?\n* Among other things this helps us select the appropriate plots for visualization.","a0329de0":"**What is the distribution of categorical features?**\n\n- Names are unique across the dataset (count=unique=891)\n- Sex variable as two possible values with 65% male (top=male, freq=577\/count=891).\n- Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin.\n- Embarked takes three possible values. S port used by most passengers (top=S)\n- Ticket feature has high ratio (22%) of duplicate values (unique=681).","ef850186":"After observing the above graph, we can say that women were more likely to survived than men as they have high rate of survival than man. Hence, in determining whether a passenger will survive or not, gender(male or female) plays an important role.","ef90a98d":"* it seems has a good effect on our prediction but let's go further and categorize people to check whether they are alone in this ship or not\n* We can create another feature called IsAlone.","e18db4b4":"### Correlating categorical and numerical features\n\nWe may also want to correlate categorical features (with non-numeric values) and numeric features. We can consider correlating Embarked (Categorical non-numeric), Sex (Categorical non-numeric), Fare (Numeric continuous), with Survived (Categorical numeric).\n\n**Observations.**\n\n- Higher fare paying passengers had better survival. Confirms our assumption for creating (#4) fare ranges.\n- Port of embarkation correlates with survival rates. Confirms correlating (#1) and completing (#2).\n\n**Decisions.**\n\n- Consider banding Fare feature.","b2568e9f":"#### Dropping UnNeeded Features\n\nName--> We don't need name feature as it cannot be converted into any categorical value, we extract Title  \n\nAge--> We have the Age_band feature, so no need of this, we extract Age  Range\n\nTicket--> It is any random string that cannot be categorised.\n\nFare--> We have the Fare_cat feature, so unneeded , we extract Fare Range\n\nTNumeric -->We don't need name feature as it cannot be converted into any categorical value.\n\n\n\n\n","8c610571":"### 1.1-Name","d463a9d2":"### * Categorical features:","3eea6763":"We split the dataframe to get our original passenger data from the training set and the test set.","71b8aa4f":"before the cleaning data we combine training and test data in order to remain keep the same structure\n\n\nClean and Edit Dataframes We must combine train and test datasets. Because This processes are must be carried out together","6abaad15":"#### Categorical: \n       Survived     nominal datatype\n       Sex          nominal datatype\n       Embarked     nominal datatype\n       Name         nominal datatype\n       Cabin        nominal datatype\n       Ticket       nominal datatype\n      \n       Pclass       ordinal datatype\n\n#### Numerical : \n      Age         continuous quantitative datatypes.\n      Fare        continuous quantitative datatypes.\n      \n      \n      PassengerId discrete quantitative datatypes\n      SibSp       discrete quantitative datatypes\n      Parch.      discrete quantitative datatypes","860949b3":"## Analyze by visualizing data\n\nNow we can continue confirming some of our assumptions using visualizations for analyzing the data.\n\n### Correlating numerical features\n\nLet us start by understanding correlations between numerical features and our solution goal (Survived).\n\nA histogram chart is useful for analyzing continous numerical variables like Age where banding or ranges will help identify useful patterns. The histogram can indicate distribution of samples using automatically defined bins or equally ranged bands. This helps us answer questions relating to specific bands (Did infants have better survival rate?)\n\nNote that x-axis in historgram visualizations represents the count of samples or passengers.\n\n**Observations.**\n\n- Infants (Age <=4) had high survival rate.\n- Oldest passengers (Age = 80) survived.\n- Large number of 15-25 year olds did not survive.\n- Most passengers are in 15-35 age range.\n\n**Decisions.**\n\nThis simple analysis confirms our assumptions as decisions for subsequent workflow stages.\n\n- We should consider Age (our assumption classifying #2) in our model training.\n- Complete the Age feature for null values (completing #1).\n- We should band age groups (creating #3).","91597c1d":"## **Principal Component Analysis**\n\nPCA helps us to identify patterns in data based on the correlation between features. Used to reduce number of variables in your data by extracting important one from a large pool. Thus, it reduces the dimension of your data with the aim of retaining as much information as possible.\n\nHere we will use a straightforward PCA, asking it to preserve 85% of the variance in the projected data.","440be181":"### 1-Feature engineering\nFeature engineering is very important to improve the model's performance","384b0077":"## Analyze by pivoting features\n\nTo confirm some of our observations and assumptions, we can quickly analyze our feature correlations by pivoting features against each other. We can only do so at this stage for features which do not have any empty values. It also makes sense doing so only for features which are categorical (Sex), ordinal (Pclass) or discrete (SibSp, Parch) type.\n\n- **Pclass** We observe significant correlation (>0.5) among Pclass=1 and Survived (classifying #3). We decide to include this feature in our model.\n- **Sex** We confirm the observation during problem definition that Sex=female had very high survival rate at 74% (classifying #1).\n- **SibSp and Parch** These features have zero correlation for certain values. It may be best to derive a feature or a set of features from these individual features (creating #1).","1736a5ad":"### Traing Models with Feature Scaling\n\n### **Feature Scaling**\n\nTwo approaches are shown below:\n1. The **StandardScaler** assumes your data is normally distributed within each feature and will scale them such that the distribution is now centred around 0, with a standard deviation of 1.\n\n2. The **normalizer** scales each value by dividing each value by its magnitude in n-dimensional space for n number of features.","fd65f008":"### We split them to:\n\n* Categorical features\n* Numerical features","2eba9574":"### split training data into numeric and categorical data","e89572f3":"### Assumtions based on data analysis\n\nWe arrive at following assumptions based on data analysis done so far. We may validate these assumptions further before taking appropriate actions.\n\n**Correlating.**\n\nWe want to know how well does each feature correlate with Survival. We want to do this early in our project and match these quick correlations with modelled correlations later in the project.\n\n**Completing.**\n\n1. We may want to complete Age feature as it is definitely correlated to survival.\n2. We may want to complete the Embarked feature as it may also correlate with survival or another important feature.\n\n**Correcting.**\n\n1. Ticket feature may be dropped from our analysis as it contains high ratio of duplicates (22%) and there may not be a correlation between Ticket and survival.\n2. Cabin feature may be dropped as it is highly incomplete or contains many null values both in training and test dataset.\n3. PassengerId may be dropped from training dataset as it does not contribute to survival.\n4. Name feature is relatively non-standard, may not contribute directly to survival, so maybe dropped.\n\n**Creating.**\n\n1. We may want to create a new feature called Family based on Parch and SibSp to get total count of family members on board.\n2. We may want to engineer the Name feature to extract Title as a new feature.\n3. We may want to create new feature for Age bands. This turns a continous numerical feature into an ordinal categorical feature.\n4. We may also want to create a Fare range feature if it helps our analysis.\n\n**Classifying.**\n\nWe may also add to our assumptions based on the problem description noted earlier.\n\n1. Women (Sex=female) were more likely to have survived.\n2. Children (Age<?) were more likely to have survived. \n3. The upper-class passengers (Pclass=1) were more likely to have survived.","564d0e87":"We further split the training set in to a train and test set to validate our model.","1f4344f9":"Before compelete cleaning the data, we zoom at the features with missing values, those missing values won't be treated equally. Some features have barely 1 or 2 missing values, we will use thesome methods to fill them."}}