{"cell_type":{"182a7af0":"code","66016495":"code","483591a2":"code","b8d5f174":"code","6a8caeda":"code","a2eed899":"code","0409d45d":"code","86628afc":"code","1e0f4ec7":"code","c7a5143a":"code","395c45ab":"code","750fbaee":"code","c3d2af9b":"code","47b82cb7":"markdown","08f87c42":"markdown","19fcd698":"markdown","f5c1a769":"markdown","02e112f0":"markdown","39b55f08":"markdown","5d966a98":"markdown","ec2eb5a8":"markdown"},"source":{"182a7af0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","66016495":"data = pd.read_csv(\"..\/input\/creating-hugedataset\/hDS.csv\") \ndata","483591a2":"# Split Data\nfrom sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(data, train_size=0.70, test_size=0.30, random_state=0)\n\nX_train = train.copy()\nX_test = test.copy()\nX_test.drop('SalePrice', axis='columns', inplace=True)\nX_train.drop('SalePrice', axis='columns', inplace=True)\ny = test[\"SalePrice\"]\ny_train = train[\"SalePrice\"]\n\n# For cross_validation:\nx_data = data.copy()\n# x_data - all data just without 'SalePrice' column\nx_data.drop('SalePrice', axis ='columns', inplace = True)\ny_data = data[\"SalePrice\"]","b8d5f174":"import time\nstart_time = time.time()\n# Creating and fitting a model \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\n\nmodel = RandomForestRegressor(n_estimators = 200, max_depth = 5, random_state = 1)\nmodel.fit(X_train, y_train)\nval_predictions = model.predict(X_test)\n\nval_mae = mean_absolute_error(val_predictions, y)\nval_r2 = r2_score(y_pred = val_predictions,y_true = y)\nprint(\"--- %s run time in seconds ---\" % (time.time() - start_time))","6a8caeda":"print('_____________________________','\\nMAE score:', val_mae,\n      '\\n_____________________________','\\nR^2 score:',val_r2,\n      '\\n_____________________________')","a2eed899":"start_time = time.time()\nfrom sklearn.feature_selection import VarianceThreshold\nfor i in range(15):\n    n = 0.08 + i*0.001\n    selector = VarianceThreshold(threshold = round(n, 3))\n    x_data_reduced_by_VT = selector.fit_transform(x_data)\n    print('Initial number of columns: 5142', '\\nNumber of columns for dispersion threshold =', round(n, 3),'is:',pd.DataFrame(x_data_reduced_by_VT).shape[1])\n# Save names of columns! only in the last iteration with threshold = 1.5\nselector = VarianceThreshold(threshold = 1.5)\nx_data_reduced_by_VT = selector.fit_transform(x_data)\nx_data_reduced_by_VT = x_data[x_data.columns[selector.get_support(indices=True)]]\n#x_data_reduced_by_VT = selector.fit_transform(x_data)\nprint('Initial number of columns: 5142', '\\nNumber of columns for dispersion threshold = 1.5','is:',pd.DataFrame(x_data_reduced_by_VT).shape[1])\nprint(\"--- %s run time in seconds ---\" % (time.time() - start_time))","0409d45d":"data_2 = pd.DataFrame(x_data_reduced_by_VT)\ndata_2 = data_2.join(data['SalePrice'])\ndata_2","86628afc":"start_time = time.time()\n# Work with a new dataframe\ntrain_1, test_1 = train_test_split(data_2, train_size=0.70, test_size=0.30, random_state=0)\n\nX_train_1 = train_1.copy()\nX_test_1 = test_1.copy()\nX_test_1.drop('SalePrice', axis='columns', inplace=True)\nX_train_1.drop('SalePrice', axis='columns', inplace=True)\ny_1 = test_1[\"SalePrice\"]\ny_train_1 = train_1[\"SalePrice\"]\n\nmodel = RandomForestRegressor(n_estimators = 200, max_depth = 5, random_state = 1)\nmodel.fit(X_train_1, y_train_1)\nval_predictions = model.predict(X_test_1)\n\nval_mae = mean_absolute_error(val_predictions, y_1)\nval_r2 = r2_score(y_pred = val_predictions,y_true = y_1)\n\nprint('_____________________________','\\nMAE score:', val_mae,\n      '\\n_____________________________','\\nR^2 score:',val_r2,\n      '\\n_____________________________')\n\n\nprint(\"--- %s run time in seconds ---\" % (time.time() - start_time))","1e0f4ec7":"start_time = time.time()\nfrom sklearn.feature_selection import mutual_info_regression\ndef make_mi_scores(x, y):\n    mi_scores = mutual_info_regression(x, y, discrete_features = 'auto')\n    mi_scores = pd.Series(mi_scores, name = 'MI Scores', index = x.columns)\n    mi_scores = mi_scores.sort_values(ascending = False)\n    return mi_scores\nmi_scores = make_mi_scores(x_data, y_data)\nprint(mi_scores[:50])\nprint(\"--- %s run time in seconds ---\" % (time.time() - start_time))","c7a5143a":"start_time = time.time()\ncorr_matrix = data.corr()\nprint(\"--- %s run time in seconds ---\" % (time.time() - start_time))\ncorr_matrix","395c45ab":"corr_matrix","750fbaee":"s = corr_matrix.unstack()\nso = s.sort_values(kind = \"quicksort\")","c3d2af9b":"start_time = time.time()\nfor i in range(len(so)):\n    if (so[i] != 1) and (so[i] > 0.8):\n        print(so[[i]])\nprint(\"--- %s run time in seconds ---\" % (time.time() - start_time))","47b82cb7":"## 1. Removing features with low variance (*VarianceThreshold*)","08f87c42":"## Conclusion:\n1. As we would expect, the noise values have a low MI.","19fcd698":"### Correlation: Pearson's standard correlation coefficient","f5c1a769":"## 2. Univariate feature selection\nQuite a lot of different methods are referred to univariate feature selection. According to [Feature selection](https:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html#univariate-feature-selection), there are the following options:\n   1. For regression: *f_regression*, *mutual_info_regression*\n   2. For classification: *chi2*, *f_classif*, *mutual_info_classif*\n\nAlso here, I would separately highlight the check for the correlation of features with each other using the Pearson coefficient. I will give an example of consideration *mutual_info_regression*\n\n### Mutual_info_regression (MI)","02e112f0":"## Conclusion:\nThus, we see that *noise* features are not highly correlated, and this is true, because they were created as a pseudo-random dataset.\nThis means that checking for correlation will not always help you get rid of *noise*. It should be understood that Pearson's correlation shows only a linear relationship between features.","39b55f08":"# Outcomes:\n1. It was shown that a small **dispersion threshold** eliminates practically all the introduced *noise*. In this case, this is good, but is it always so? Rather no, in fact, the *noise* was normalized data from 0 to 1, they certainly have a low threshold of variance, and it is incorrect to compare them along with the values measured in thousands. By and large, when using this method on heterogeneous data, we run the risk of removing the normalized values, regardless of their hypothetical quality for the model. Thus, for reliable analysis, the data must be of the same scale. In general, many studies have shown that **features with low variance do not have predictive power**. \n<br>*Data processing time by this method: t \u2248 3 s.*\n2. It has been shown that the *noise* values have a **low MI**. Thus, the indicator of **mutual information** can indicate a high degree of influence of a particular feature on the target. However, this feature does not take into account that features with a low *MI* individually can show a high *MI* when they are combined according to a certain rule (Feature Engeneering).\n<br>*Data processing time by this method: t \u2248 43 s.*\n3. It has been shown that *noise* features **are not highly correlated**, and this is true, because they were created as a pseudo-random dataset. This means that checking for correlation will not always help you get rid of *noise*. It should be understood that **Pearson's correlation shows only a linear relationship between features**. \n<br>*Data processing time by this method: t \u2248 264 + 102 \u2248 366 s.*\n4. Time to train the model on a simple tree on data 5142 rows \u00d7 5142 columns: t \u2248 256 s. MAE score: 22297; R^2 score: 0.78\n<br>Time to train the model on a simple tree on data  with **dispersion threshold** 1460 rows \u00d7 31 columns: t \u2248 1 s. MAE score: 20261; R^2 score: 0.82","5d966a98":"## Conclusion:\n1. It was shown that a small dispersion threshold eliminates practically all the introduced *noise*. In this case, this is good, but is it always so?\nRather no, in fact, the *noise* was normalized data from 0 to 1, they certainly have a low threshold of variance, and it is incorrect to compare them along with the values measured in thousands. By and large, when using this method on heterogeneous data, we run the risk of removing the normalized values, regardless of their hypothetical quality for the model.\n2. Thus, for reliable analysis, the data must be of the same scale. In general, many studies have shown that features with low variance do not have predictive power: [low-variance-features-useless](https:\/\/www.kaggle.com\/fchmiel\/low-variance-features-useless)","ec2eb5a8":"## What is it? hugeDS \nThe presented dataframe is a collection of cleaned and reduced data from the training set of the [house price predictive task](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course\/data?select=train.csv) and 5 000 random features normalized from 0 to 1 acting as *noise*. This dataframe is presented for the practice of various techniques for cleaning large data sets from \"unnecessary\" features, that is, *noise*. It is shown that the random forest model has a certain, albeit weak, predictive power (due to the presence of *noise* in the form of a large number of arbitrary features)."}}