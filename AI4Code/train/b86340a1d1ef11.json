{"cell_type":{"5f895dfd":"code","136a3ca3":"code","8475f5b0":"code","684a8638":"code","6947e767":"code","6bc92b2f":"code","30558d13":"code","78ed769e":"markdown","7b307273":"markdown","c9eb3693":"markdown","10f946ff":"markdown","7f6ade71":"markdown","c35fbff3":"markdown"},"source":{"5f895dfd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","136a3ca3":"from google.cloud import bigquery\n\nclient = bigquery.Client()\n\ndataset_reference = client.dataset(\"github_repos\", project=\"bigquery-public-data\")\n\ndataset = client.get_dataset(dataset_reference)","8475f5b0":"tables = list(client.list_tables(dataset))\n\nfor table in tables:\n    print(table.table_id)","684a8638":"license_reference = dataset_reference.table(\"licenses\")\n\nlicenses = client.get_table(license_reference)\n\nclient.list_rows(licenses, max_results = 5).to_dataframe()","6947e767":"files_reference = dataset_reference.table(\"sample_files\")\n\nfiles = client.get_table(files_reference)\n\nclient.list_rows(files, max_results=5).to_dataframe()","6bc92b2f":"# Query to determine the number of files per license, sorted by number of files\nquery = \"\"\"\n        SELECT l.license, COUNT(*) as num_of_files\n        FROM `bigquery-public-data.github_repos.sample_files` AS sf\n        INNER JOIN `bigquery-public-data.github_repos.licenses` AS l\n            ON sf.repo_name = l.repo_name\n        GROUP BY l.license\n        ORDER BY num_of_files DESC\n        \"\"\"\n\n# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 10 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\nfile_count_by_license = query_job.to_dataframe()","30558d13":"print(file_count_by_license)","78ed769e":"So now we will the combine the two tables using an **inner join**, so that we can sort by the license column in the *license table* while using the values from the *licenses table*.","7b307273":"Since we are going to be working with multiple tables, our references need to be more specific. In this case I will be naming the *licenses* table as shown above as well as *files* table as shown below!","c9eb3693":"## SQL Practice 6\n\nJust some code to learn using SQL integrated within the *Kaggle environment*. \n\nIn this practice notebook we will be covering the basics of *joining tables* and specifically the **inner join**. \n\nAs always, first we need to set up our environment so that we are able to run python code that will create a client that connects to the *Google BigQuery* servers that will let us work with the data sets and tables. We will be working with a public dataset that has information regarding `github repos`!","10f946ff":"Next, let's work on joining two tables together! But first we will have to create references for the tables for Kaggle to fetch them from BigQuery.","7f6ade71":"Next we can print the results of the table in python, which will streamline the data from both tables using our query from above!","c35fbff3":"I also want to get a list of all the tables in the dataset, so that I can figure out which ones that I would like to query and *join* together."}}