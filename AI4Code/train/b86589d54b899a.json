{"cell_type":{"2caefe21":"code","a992c403":"code","07537e3c":"code","8aba8449":"code","7b3c9e73":"code","9ee2a334":"code","043102ac":"code","65838245":"code","19ff7335":"code","db89a72c":"code","a074eeeb":"code","c2b06322":"code","183a4a0d":"markdown","e8076989":"markdown","4010243a":"markdown","5e85cd88":"markdown","36ab7bdc":"markdown","25565154":"markdown","f9b3f9fe":"markdown","5567b7c2":"markdown","3cea14f7":"markdown","9cd77ca3":"markdown","2eb85811":"markdown","55f69a2c":"markdown"},"source":{"2caefe21":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.cluster import KMeans\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a992c403":"data = pd.read_csv('\/kaggle\/input\/3.12. Example.csv')\ndata.head()","07537e3c":"plt.scatter(data['Satisfaction'], data['Loyalty'])\nplt.xlabel('Satisfaction')\nplt.ylabel('Loyalty')\nplt.show()","8aba8449":"x = data.copy()","7b3c9e73":"kmeans = KMeans(2)\nkmeans.fit(x)","9ee2a334":"clusters = x.copy()\nclusters['cluster_pred'] = kmeans.fit_predict(x)","043102ac":"plt.scatter(clusters['Satisfaction'], clusters['Loyalty'], c=clusters['cluster_pred'], cmap='rainbow')\nplt.xlabel('Satisfaction')\nplt.ylabel('Loyalty')\nplt.show()","65838245":"from sklearn import preprocessing\nx_scaled = preprocessing.scale(x)\nx_scaled","19ff7335":"wcss = []\nfor i in range(1, 10):\n    kmeans = KMeans(i)\n    kmeans.fit(x_scaled)\n    wcss.append(kmeans.inertia_)\n    \nwcss","db89a72c":"number_cluster = range(1, 10)\nplt.plot(number_cluster, wcss)\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')\nplt.show()","a074eeeb":"# Just change the parameter passed to the KMeans(2, 3, 4, 5)\nkmeans_new = KMeans(4)\nkmeans_new.fit(x_scaled)\nclusters_new =  x.copy()\nclusters_new['cluster_pred'] = kmeans_new.fit_predict(x_scaled)\nclusters_new.head()","c2b06322":"plt.scatter(clusters_new['Satisfaction'], clusters_new['Loyalty'], c=clusters_new['cluster_pred'], cmap='rainbow')\nplt.xlabel('Satisfaction')\nplt.ylabel('Loyalty')\nplt.show()","183a4a0d":"### Clustering results","e8076989":"As from above, we can take 2, 3, 4, and 5 as k values. It is not clear which one is the best, so we are gonna explore them below","4010243a":"### Load the data","5e85cd88":"### Plot the data","36ab7bdc":"About the data:\n\nSatisfaction is self-reported (on a scale of 1 to 10, 10 is the highest satisfcation).\nSatisfaction is discrete variable\n\nLoyalty is measured based on the number of purhases per year and some other factors. It is continuous data type. Range is from -2.5 to +2.5, as the data is already standardized","25565154":"### Standardize the variable","f9b3f9fe":"### Elbow Method","5567b7c2":"From above plot, we can see that at the satisfaction value of 6, almost there is a vertical separation line, this happened because kmeans has just considered satisfaction as independent variable, and left loyalty, as the values are not normalized, so the next step is to standardize the satisfaction (as loyalty is already standardized)","3cea14f7":"### Import the relevant libraries","9cd77ca3":"### Explore clustering solutions and select the number of clusters","2eb85811":"### Select the features","55f69a2c":"### Clustering"}}