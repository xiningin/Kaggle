{"cell_type":{"a79bd5c7":"code","a6c690be":"code","4fe6b1a8":"code","1c83c022":"code","d9c930ad":"code","f7c4e4b7":"code","5bc246b1":"code","58a7de20":"code","b2300ee0":"code","5a952534":"code","95b14389":"code","405a6455":"code","11f59f8d":"code","e20b76f6":"code","373a129d":"code","693825e4":"code","6dbd9797":"code","2a412df7":"code","879f661e":"code","44a835ee":"code","ce9126c7":"code","637f8a98":"code","e0b18d85":"code","13a4854e":"code","5f59e677":"code","aadccabd":"code","4831e982":"code","0ba29d32":"code","8aa1f2ee":"code","8051ce2d":"code","59a30cc3":"code","7460c2b3":"code","439f6299":"code","eedd49b3":"code","f2e87c0f":"code","d8454122":"code","401b5dce":"code","f6beebbd":"code","f648a21d":"code","31918d27":"markdown","e2a5d727":"markdown","f88e9fc1":"markdown","175bf82c":"markdown","f96a5442":"markdown","7e011641":"markdown","03ae27bc":"markdown","764f5fb1":"markdown","59414574":"markdown","fdcf3dab":"markdown","3a012e85":"markdown","e2b7f27f":"markdown","c0c4ffeb":"markdown","a1c40009":"markdown","d629f9df":"markdown","aa07f836":"markdown","e58a6e1f":"markdown","d0a4fcee":"markdown","94f81ad8":"markdown","5bf763bf":"markdown","2a5c805d":"markdown","bd79476b":"markdown","426e3438":"markdown","f7f32266":"markdown","62b66796":"markdown","c4d057f9":"markdown","47919862":"markdown","1fad0f85":"markdown","341a21ca":"markdown","df816db5":"markdown","89cd9283":"markdown","b22893e4":"markdown","3db2a858":"markdown","51a309d6":"markdown","adc0f40a":"markdown","833b51b0":"markdown","0dc04864":"markdown","791fe973":"markdown","f919b794":"markdown"},"source":{"a79bd5c7":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","a6c690be":"# Reference: https:\/\/www.kaggle.com\/fedesoriano\/stroke-prediction-dataset\ndf = pd.read_csv(\"..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")","4fe6b1a8":"df.info()","1c83c022":"df.head()","d9c930ad":"fig = plt.figure(figsize=(14,5),facecolor='#f7f6f6',)\naxes = plt.subplot2grid((1,1),(0,0))\n\nplt.pie(x = [df.stroke.value_counts()[0], df.stroke.value_counts()[1]],\n        pctdistance=0.70,labels = ['No stroke','Stroke'], autopct='%1.1f%%',\n        colors = ['#869ef3','#ff5252'], labeldistance= 1.1, radius = 0.9,)\n\ncentre_circle = plt.Circle((0,0),0.8,fc='white') \n\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n# plt.tight_layout()\nplt.text(2.5,-0.2,'The Dataset is highly imbalanced!\\n\\nWe have only 249 values for the minority \\nclass and 4861 values for the majority class.', fontsize=12)\nplt.show()\n","f7c4e4b7":"# checking for null values\nfig = plt.figure(figsize=(12,5),facecolor='#f6f5f5',)\naxes = plt.subplot2grid((1,1),(0,0))\nsns.heatmap(df.isnull(), cmap = ['#869ef3','#ff5252'])\nplt.text(18,2684,'Only \"bmi\" is containing null values. It has a \\ntotal of 201 null values.', fontsize = 12)\nplt.show()","5bc246b1":"# checking the correlation\nfig = plt.figure(figsize=(12,6),facecolor='#f6f5f5',)\naxes = plt.subplot2grid((1,1),(0,0))\n\nsns.heatmap(df.corr(), cmap = 'coolwarm',annot = True)\nplt.text(12,4,'No correlation can be seen.', fontsize = 12)\nplt.show()\n# We don't have any major correlation among the features as shown below.","58a7de20":"df.describe()","b2300ee0":"# checking skewness\nnumerical_cols = df.columns[df.dtypes == 'float64']\ndf[numerical_cols].skew()","5a952534":"fig = plt.figure(figsize=(14,5), facecolor = '#f6f5f5')\n\naxes = plt.subplot2grid((1,3),(0,0))\nsns.boxplot(y = df.age, color = '#869ef3')\nplt.yticks([])\nplt.ylabel('')\nplt.title('Age')\n\naxes =plt.subplot2grid((1,3), (0,1))\nsns.boxplot(y = df.avg_glucose_level, color = '#869ef3')\nplt.yticks([])\nplt.ylabel('')\nplt.title('Average Glucose Level')\n\naxes =plt.subplot2grid((1,3), (0,2))\nsns.boxplot(y = df.bmi, color = '#869ef3')\nplt.yticks([])\nplt.ylabel('')\nplt.title('Bmi')\n\nplt.tight_layout()\nplt.show()","95b14389":"female_percent = round(df.gender.value_counts(normalize = True)*100,2)[0]\nmale_percent = round(df.gender.value_counts(normalize = True)*100,2)[1]\n\nprivate_percent = round(df.work_type.value_counts(normalize = True)*100,2)[0]\nself_percent = round(df.work_type.value_counts(normalize = True)*100,2)[1]\ngovt_percent = round(df.work_type.value_counts(normalize = True)*100,2)[2]\nchild_percent = round(df.work_type.value_counts(normalize = True)*100,2)[3]\nnever_percent = round(df.work_type.value_counts(normalize = True)*100,2)[4]\n\nfsmoke_percent = round(df.smoking_status.value_counts(normalize = True)*100,2)[2]\nnsmoke_percent = round(df.smoking_status.value_counts(normalize = True)*100,2)[0]\nsmoke_percent = round(df.smoking_status.value_counts(normalize = True)*100,2)[3]\nunknown_percent = round(df.smoking_status.value_counts(normalize = True)*100,2)[1]\n\n","405a6455":"categorical_cols = df.columns[(df.dtypes  == 'object') | (df.dtypes ==  'int64') ].drop(['id', 'stroke'])\ncategorical_cols","11f59f8d":"fig = plt.figure(figsize=(18,12), facecolor = '#f6f5f5')\npalette = ['#869ef3','#ff5252']\n\naxes = plt.subplot2grid((3,3),(0,0))\nsns.countplot(data = df, x = 'gender', hue = 'stroke', palette = ['#869ef3','#ff5252'])\nplt.ylabel('')\n\naxes = plt.subplot2grid((3,3),(0,1))\nsns.countplot(data = df, x = 'hypertension', hue = 'stroke', palette = palette)\nplt.ylabel('')\n\naxes = plt.subplot2grid((3,3),(0,2))\nsns.countplot(data = df, x = 'heart_disease', hue = 'stroke', palette = palette)\nplt.ylabel('')\n\naxes = plt.subplot2grid((3,3),(1,0))\nsns.countplot(data = df, x = 'ever_married', hue = 'stroke', palette = palette)\nplt.ylabel('')\n\naxes = plt.subplot2grid((3,3),(1,1))\nsns.countplot(data = df, x = 'work_type', hue = 'stroke', palette = palette)\nplt.ylabel('')\n\naxes = plt.subplot2grid((3,3),(1,2))\nsns.countplot(data = df, x = 'Residence_type', hue = 'stroke', palette = palette)\nplt.ylabel('')\n\naxes = plt.subplot2grid((3,3),(2,0))\nsns.countplot(data = df, x = 'smoking_status', hue = 'stroke', palette = palette)\nplt.ylabel('')\n\nplt.tight_layout()\nplt.show()","e20b76f6":"hypertension_prob = round(df[df[\"hypertension\"] == 1][\"stroke\"].mean()*100 - df[df[\"hypertension\"]== 0][\"stroke\"].mean()*100, 2)\nheart_disease_prob = round(df[df[\"heart_disease\"] == 1][\"stroke\"].mean()*100 - df[df[\"heart_disease\"]== 0][\"stroke\"].mean()*100, 2)\nsmoking_prob = round(df[(df[\"smoking_status\"] == \"formerly smoked\") | (df[\"smoking_status\"] == \"smokes\")][\"stroke\"].mean()*100 - df[df[\"smoking_status\"]== \"never smoked\"][\"stroke\"].mean()*100, 2)\nprivate_prob = round(df[df[\"work_type\"] == \"Private\"][\"stroke\"].mean()*100,0)\nself_employ_prob = round(df[df[\"work_type\"] == \"Self-employed\"][\"stroke\"].mean()*100,2)\nchild_prob = round(df[df[\"work_type\"] == \"children\"][\"stroke\"].mean()*100,2)\nmarried_prob = round(df[df[\"ever_married\"] == \"Yes\"][\"stroke\"].mean()*100 - df[df[\"ever_married\"]== \"No\"][\"stroke\"].mean()*100, 2)","373a129d":"sns.pairplot(df.drop('id', axis =1), hue = 'stroke', palette = palette )\nplt.tight_layout()\nplt.show()","693825e4":"# Defining x and y\nX = df.drop(['id', 'stroke'], axis =1)\ny = df.stroke\n\n# Importing Libraries\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.pipeline import make_pipeline\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n\n# Preprocessing of numerical data\nnumerical_transformer = Pipeline(steps = [\n    ('imputer', KNNImputer(n_neighbors = 8)),\n    ('scaler', StandardScaler())\n])\n\n# Preprocessing of categorical data\ncategorical_transformer = OneHotEncoder(handle_unknown='ignore')\n\n# Bundel preprocessing for numerical and categorical \npreprocessor = ColumnTransformer(transformers = [\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols),\n\n])\n\n# Importing Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n\n# creating instance of models\nlr = LogisticRegression()\nknc = KNeighborsClassifier()\ndtc = DecisionTreeClassifier()\nrfc = RandomForestClassifier()\nsvc = SVC()\nxgb = XGBClassifier()\nabc = AdaBoostClassifier()\n\n\n# models = [lr, knc, dtc, rfc, svc, xgb, abc, cbc]\nmodels = [lr, knc, dtc, rfc, svc, xgb, abc]\nmodels_name = ['Logistic Regression', \"K Neighbor Classifier\", \"Decision Tree Classifier\", 'Random Forest Classifier','Support Vector Classifier', 'XGBoost Classifier','ADABoost Classifier']","6dbd9797":"# metric\nfrom sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n\n# Without power transformer\ni = 0\navg_score = 0\nmodel_wo_score = {}\nfor classifiers in models:\n    my_pipeline = Pipeline( steps = [\n        ('preprocessor', preprocessor),\n        ('model', classifiers)\n    ])\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n    scores = cross_val_score(my_pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n    avg_score = avg_score + np.mean(scores)\n    model_wo_score[models_name[i]] = round(np.mean(scores), 3)\n    i += 1\n\nimbalace_wo_power_transform = avg_score\/7\n\na = pd.DataFrame(data = model_wo_score, index = ['roc_auc']).transpose().sort_values(by = 'roc_auc', ascending = False)\na = a.reset_index().rename(columns = {'index': 'models'})","2a412df7":"fig = plt.figure(figsize=(12,4), facecolor = '#f6f5f5')\naxes = plt.subplot2grid((1,1),(0,0))\n\nsns.barplot(data = a, y = 'models', x = 'roc_auc', palette = 'coolwarm' )\naxes.bar_label(axes.containers[0],size = 10)\nplt.tight_layout()\n\nplt.show()","879f661e":"# With power transformer\nimport warnings\nwarnings.filterwarnings('ignore')\n\ni = 0\navg_score = 0\nmodel_wo_score = {}\nfor classifiers in models:\n    my_pipeline = Pipeline( steps = [\n        ('preprocessor', preprocessor),\n        ('power', PowerTransformer()),\n        ('model', classifiers)\n    ])\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n    scores = cross_val_score(my_pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n    avg_score = avg_score + np.mean(scores)\n    model_wo_score[models_name[i]] = round(np.mean(scores), 3)\n    i += 1\n    \nimbalace_w_power_transform = avg_score\/7\n\na = pd.DataFrame(data = model_wo_score, index = ['roc_auc']).transpose().sort_values(by = 'roc_auc', ascending = False)\na = a.reset_index().rename(columns = {'index': 'models'})","44a835ee":"\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfig = plt.figure(figsize=(12,4), facecolor = '#f6f5f5')\naxes = plt.subplot2grid((1,1),(0,0))\n\nsns.barplot(data = a, y = 'models', x = 'roc_auc', palette = 'coolwarm' )\naxes.bar_label(axes.containers[0],size = 10)\nplt.tight_layout()\n\nplt.show()","ce9126c7":"X.head()","637f8a98":"y.value_counts()","e0b18d85":"import warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\n\nunder = RandomUnderSampler(sampling_strategy = 'majority')\navg_score = 0\nmodel_wo_score = {}\ni = 0\nfor classifiers in models:\n    \n    my_pipeline = make_pipeline(preprocessor, under, classifiers)\n\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n    scores = cross_val_score(my_pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n    avg_score = avg_score + np.mean(scores)\n    model_wo_score[models_name[i]] = round(np.mean(scores), 3)\n    i += 1\n\n\nundersampling_majority = avg_score\/7\n","13a4854e":"import warnings\nwarnings.filterwarnings('ignore')\n\na = pd.DataFrame(data = model_wo_score, index = ['roc_auc']).transpose().sort_values(by = 'roc_auc', ascending = False)\na = a.reset_index().rename(columns = {'index': 'models'})\n\nfig = plt.figure(figsize=(12,4), facecolor = '#f6f5f5')\naxes = plt.subplot2grid((1,1),(0,0))\n\nsns.barplot(data = a, y = 'models', x = 'roc_auc', palette = 'coolwarm' )\naxes.bar_label(axes.containers[0],size = 10)\nplt.tight_layout()\n\nplt.show()","5f59e677":"import warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom imblearn.over_sampling import RandomOverSampler\n\nover = RandomOverSampler(sampling_strategy = 'minority')\navg_score = 0\nmodel_wo_score = {}\ni = 0\nfor classifiers in models:\n    \n    my_pipeline = make_pipeline(preprocessor, over, classifiers)\n\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n    scores = cross_val_score(my_pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n    avg_score = avg_score + np.mean(scores)\n    model_wo_score[models_name[i]] = round(np.mean(scores), 3)\n    i += 1\n\noversampling_minority = avg_score\/7\n","aadccabd":"import warnings\nwarnings.filterwarnings('ignore')\n\na = pd.DataFrame(data = model_wo_score, index = ['roc_auc']).transpose().sort_values(by = 'roc_auc', ascending = False)\na = a.reset_index().rename(columns = {'index': 'models'})\n\nfig = plt.figure(figsize=(12,4), facecolor = '#f6f5f5')\naxes = plt.subplot2grid((1,1),(0,0))\n\nsns.barplot(data = a, y = 'models', x = 'roc_auc', palette = 'coolwarm' )\naxes.bar_label(axes.containers[0],size = 10)\nplt.tight_layout()\n\nplt.show()","4831e982":"import warnings\nwarnings.filterwarnings('ignore')\n\nunder = RandomUnderSampler(sampling_strategy = 0.5)\nover = RandomOverSampler(sampling_strategy = 0.1)\n\navg_score = 0\nmodel_wo_score = {}\ni = 0\nfor classifiers in models:\n    \n    my_pipeline = make_pipeline(preprocessor, over, under, classifiers)\n\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n    scores = cross_val_score(my_pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n    avg_score = avg_score + np.mean(scores)\n    model_wo_score[models_name[i]] = round(np.mean(scores), 3)\n    i += 1\n\n\nover_under = avg_score\/7\n","0ba29d32":"import warnings\nwarnings.filterwarnings('ignore')\na = pd.DataFrame(data = model_wo_score, index = ['roc_auc']).transpose().sort_values(by = 'roc_auc', ascending = False)\na = a.reset_index().rename(columns = {'index': 'models'})\n\nfig = plt.figure(figsize=(12,4), facecolor = '#f6f5f5')\naxes = plt.subplot2grid((1,1),(0,0))\n\nsns.barplot(data = a, y = 'models', x = 'roc_auc', palette = 'coolwarm' )\naxes.bar_label(axes.containers[0],size = 10)\nplt.tight_layout()\n\nplt.show()","8aa1f2ee":"import warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom imblearn.over_sampling import SMOTE\n\nover = SMOTE(k_neighbors = 5)\nmodel_wo_score = {}\navg_score = 0\ni = 0\nfor classifiers in models:\n    \n    my_pipeline = make_pipeline(preprocessor, over,  classifiers)\n\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n    scores = cross_val_score(my_pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n    avg_score = avg_score + np.mean(scores)\n    model_wo_score[models_name[i]] = round(np.mean(scores), 3)\n    i += 1\n\n\nsmote = avg_score\/7\n","8051ce2d":"import warnings\nwarnings.filterwarnings('ignore')\n\na = pd.DataFrame(data = model_wo_score, index = ['roc_auc']).transpose().sort_values(by = 'roc_auc', ascending = False)\na = a.reset_index().rename(columns = {'index': 'models'})\n\nfig = plt.figure(figsize=(12,4), facecolor = '#f6f5f5')\naxes = plt.subplot2grid((1,1),(0,0))\n\nsns.barplot(data = a, y = 'models', x = 'roc_auc', palette = 'coolwarm' )\naxes.bar_label(axes.containers[0],size = 10)\nplt.tight_layout()\n\nplt.show()","59a30cc3":"import warnings\nwarnings.filterwarnings('ignore')\n\nover = SMOTE(sampling_strategy=0.1)\nunder = RandomUnderSampler(sampling_strategy=0.5)\ni = 0\navg_score = 0\nmodel_wo_score ={}\nfor classifiers in models:\n    \n    my_pipeline = make_pipeline(preprocessor, over, under, classifiers)\n\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n    scores = cross_val_score(my_pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n    avg_score = avg_score + np.mean(scores)\n    model_wo_score[models_name[i]] = round(np.mean(scores), 3)\n    i += 1\n\n\nsmote_under = avg_score\/7","7460c2b3":"import warnings\nwarnings.filterwarnings('ignore')\n\na = pd.DataFrame(data = model_wo_score, index = ['roc_auc']).transpose().sort_values(by = 'roc_auc', ascending = False)\na = a.reset_index().rename(columns = {'index': 'models'})\n\nfig = plt.figure(figsize=(12,4), facecolor = '#f6f5f5')\naxes = plt.subplot2grid((1,1),(0,0))\n\nsns.barplot(data = a, y = 'models', x = 'roc_auc', palette = 'coolwarm' )\naxes.bar_label(axes.containers[0],size = 10)\nplt.tight_layout()\n\nplt.show()","439f6299":"x_ =[imbalace_wo_power_transform, imbalace_w_power_transform, undersampling_majority, oversampling_minority,over_under, smote, smote_under]\n\ny_ = ['imbalace_wo_power_transform','imbalace_w_power_transform','undersampling_majority', 'oversampling_minority','over_under', 'smote','smote_under']\n\nfig = plt.figure(figsize=(16,4), facecolor = '#f6f5f5')\naxes = plt.subplot2grid((1,1),(0,0))\n\nplt.plot(y_,x_,  color='#869ef3', marker='o')\nplt.xticks(rotation = 90)\nplt.show()\n","eedd49b3":"import warnings\nwarnings.filterwarnings('ignore')\n\n\nlr = LogisticRegression()\nknc = KNeighborsClassifier()\ndtc = DecisionTreeClassifier()\nrfc = RandomForestClassifier()\nsvc = SVC()\nxgb = XGBClassifier()\nabc = AdaBoostClassifier()\n\nmodels = [lr, knc, dtc, rfc, svc, xgb, abc]\nunder = RandomUnderSampler(sampling_strategy = 0.5)\nmodel_score = {}\n\ni = 0\nfor classifiers in models:\n    \n    my_pipeline = make_pipeline(preprocessor, under, classifiers)\n\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n    scores1 = cross_val_score(my_pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n    scores2 = cross_val_score(my_pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    scores3 = cross_val_score(my_pipeline, X, y, scoring='recall', cv=cv, n_jobs=-1)\n    scores4 = cross_val_score(my_pipeline, X, y, scoring='f1', cv=cv, n_jobs=-1)\n    scores5 = cross_val_score(my_pipeline, X, y, scoring='precision', cv=cv, n_jobs=-1)\n    \n    model_score[models_name[i]] = [np.mean(scores1), np.mean(scores2),  np.mean(scores3),np.mean(scores4), np.mean(scores5)]\n    i += 1","f2e87c0f":"score_df = pd.DataFrame(model_score).transpose()\nscore_df.rename(columns = {0 : 'Roc_Auc', 1: 'Accuracy', 2:'Recall', 3: 'F1', 4: 'Precision'}, inplace = True)\nscore_df.sort_values(by = 'Roc_Auc', ascending = False, inplace = True)","d8454122":"fig = plt.figure(figsize=(16,4), facecolor = '#f6f5f5')\naxes = plt.subplot2grid((1,1),(0,0))\n\nsns.heatmap(data = score_df, cmap = 'coolwarm', annot = True,)\nplt.show()","401b5dce":"# hypertuning\nfrom sklearn.model_selection import GridSearchCV\n\nmy_pipeline = make_pipeline(preprocessor, under, LogisticRegression())\n\nparam_grid = {\n    'logisticregression__C': np.logspace(-4,4,20),\n    'logisticregression__solver' : ['lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga'],\n    'randomundersampler__sampling_strategy' : [0.50,0.60,0.70,0.80]\n\n}\n\nclf = GridSearchCV(my_pipeline, param_grid = param_grid, cv= 5, scoring = 'roc_auc',n_jobs =-1, verbose = 10)\n\nclf.fit(X, y)\nprint(clf.best_params_)    \nprint(clf.best_score_) \n","f6beebbd":"model_score_lr = {}\nunder = RandomUnderSampler(sampling_strategy = clf.best_params_['randomundersampler__sampling_strategy'])\navg_score = 0\nmy_pipeline = make_pipeline(preprocessor, under, LogisticRegression(C =clf.best_params_['logisticregression__C'], solver = clf.best_params_['logisticregression__solver']))\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n\nscores2 = cross_val_score(my_pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\nscores3 = cross_val_score(my_pipeline, X, y, scoring='f1', cv=cv, n_jobs=-1)\nscores4 = cross_val_score(my_pipeline, X, y, scoring='precision', cv=cv, n_jobs=-1)\nscores5 = cross_val_score(my_pipeline, X, y, scoring='recall', cv=cv, n_jobs=-1)\nmodel_score_lr['LR'] = [clf.best_score_, np.mean(scores2),  np.mean(scores3),np.mean(scores4), np.mean(scores5)]\n","f648a21d":"score_lr_df = pd.DataFrame(model_score_lr).transpose()\nscore_lr_df.rename(columns = {0 : 'Roc_Auc', 1: 'Accuracy', 2:'F1', 3: 'Precision', 4: 'Recall'}, inplace = True)\nscore_lr_df","31918d27":"There isn't much difference when we are using power transformer and when we aren't using it.","e2a5d727":"<span style = 'color:#ff5252; font-weight: bold'>1. Balance or Imbalance Data<\/span>","f88e9fc1":"Considering all the parameters Logistic regression is performing the best. So, lets hypertune it.","175bf82c":"<h2 style = 'text-align:center ;color: #ff5252;font-weight: bold;background-color:#ececec;border-radius: 15px 50px; padding: 12px;'>Hyperparameter Optimization<\/h2>","f96a5442":"<span style = 'color:#ff5252; font-weight: bold'>3. Correlation<\/span>","7e011641":"<span style = 'color:#ff5252; font-weight: bold'>4. Skewness<\/span>","03ae27bc":"<span style = 'color:#ff5252; font-weight: bold'>Important insights:<\/span>\n<ul style=\"list-style-type:circle\">\n<li>The dataset contains <span style = 'color: #0036fa'>58.59% females<\/span> and <span style = 'color: #0036fa'>41.39% males.<\/span><\/li>\n\n<li>People living in urban and rural areas are almost same.<\/li>\n\n<li>We can see a great disparity among the people who are having hypertension and heart diseases compared to those who are not having these problems.<\/li>\n\n<li>Almost <span style = 'color: #0036fa'>50% of the participants have never married<\/span>.<\/li>\n\n<li><span style = 'color: #0036fa'>Most of the participants are private workers<\/span>, with 57.24% share, followed by self employers having 16.03% participants, then we have those who have government job, followed by children and those who never worked, with 13.44%, 12.86% and 0.43% share respectively.<\/li>\n\n<li>Its good to see that there are more non smokers. <span style = 'color: #0036fa'>37.03% people never smoked in their life<\/span>, 17.32% used to smoke, 15.44% smokes and 30.22% are unknown.<\/li>\n\n<\/ul>","764f5fb1":"<span style = 'color:#ff5252; font-weight: bold'>5. SMOTE with undersampling<\/span>","59414574":"<h2 style = 'text-align:center ;color: #ff5252;font-weight: bold;background-color:#ececec;border-radius: 15px 50px; padding: 12px;'>Conclusion<\/h2>","fdcf3dab":"<h2 style = 'text-align:center ;color: #ff5252;font-weight: bold;background-color:#ececec;border-radius: 15px 50px; padding: 12px;'>Imbalance Data<\/h2>","3a012e85":"Skewness is quite evident here, 'avg_glucose_level' and 'bmi' are having strong right skew. As, we will be performing application of various models on the dataset to compare the best model out of the lot, we would require to fix this skewness. For this purpose we would simply use power transform.","e2b7f27f":"<span style = 'color:#ff5252; font-weight: bold'>1. Without power transformer<\/span>\n<br\/>\n\nThe first model will be without power transformer, to see the impact of skewness.<span style = 'color: #0036fa'> I will be using roc_auc scoring metric for this imbalance data.<\/span>","c0c4ffeb":"<span style = 'color:#ff5252; font-weight: bold'>Performance of all the models<\/span>","a1c40009":"<span style = 'color: #0036fa'>From above its pretty clear that undersampling is working best for this particular dataset. Moreover, Logistic Regression is having the best performance hence we will use that as our final model.<\/span>","d629f9df":"<h2 style = 'text-align:center ;color: #ff5252;font-weight: bold;background-color:#ececec;border-radius: 15px 50px; padding: 12px;'>Importing Libraries<\/h2>","aa07f836":"<span style = 'color:#ff5252; font-weight: bold'>2. With power transformer<\/span>","e58a6e1f":"<span style = 'color:#ff5252; font-weight: bold'>Basic Pipeline:<\/span>\n<ul style=\"list-style-type:circle\">\n    <li>Preprocessing of data i.e, both numerical and categorical data processing.<\/li>\n    <li>Numerical data will involve imputation of missing value and standardization of the scale.<\/li>\n    <li>Categorical data will involve One hot encoding.<\/li>\n    <li>I will try to do implmentation with both balance and unbalance data.<\/li>\n    <li>For the balance data we will further use following techniques: undersampling, oversampling, and SMOTE.<\/li>\n    <li>The last stage would be to plug the processed data to the ML model<\/li>\n<\/ul>","d0a4fcee":"<h2 style = 'text-align:center ;color: #ff5252;font-weight: bold;background-color:#ececec;border-radius: 15px 50px; padding: 12px;'>Our Data<\/h2>","94f81ad8":"According to the World Health Organization (WHO) <span style = \"color : #0036fa\">stroke is the 2nd leading cause of death globally<\/span>, responsible for approximately 11% of total deaths. This dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relavant information about the patient.\n\n<span style = 'color:#ff5252; font-weight: bold'>Attribute Information<\/span>\n\n1. id: unique identifier\n\n2. gender: \"Male\", \"Female\" or \"Other\"\n\n3. age: age of the patient\n\n4. hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n\n5. heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n\n6. ever_married: \"No\" or \"Yes\"\n\n7. work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n\n8. Residence_type: \"Rural\" or \"Urban\"\n\n9. avg_glucose_level: average glucose level in blood\n\n10. bmi: body mass index\n\n11. smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n\n12. stroke: 1 if the patient had a stroke or 0 if not\n\n*Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient","5bf763bf":"<span style = 'color:#ff5252; font-weight: bold'>2. Null Values<\/span>","2a5c805d":"<span style = 'color:#ff5252; font-weight: bold'>4. SMOTE<\/span>","bd79476b":"<h1 style=\"text-align:center ;background-color:#ececec;border-radius: 15px 50px; padding: 12px;font-weight : bold;font-family: 'Lucida Console', 'Courier New', monospace;color: #ff5252\">STROKE PREDICTION<\/h1>\n<center><img src=\"https:\/\/www.maxpixel.net\/static\/photo\/1x\/Medical-Healthcare-Stroke-Brain-Inflammation-3168269.png\" alt=\"stroke\" width=\"500\" height=\"600\" ><\/center>\n","426e3438":"<ul style=\"list-style-type:circle\">\n    <li>From the plot above it is pretty clear that there isn't much difference among gender and residence type with their affinity towards getting a stroke.<\/li>\n    <li>People who have <span style = 'color: #0036fa'>hypertension have 9.29% more probability<\/span> of getting a stroke.<\/li>\n    <li>People who have <span style = 'color: #0036fa'>heart_disease have 12.85% more probability<\/span> of getting a stroke.<\/li>\n    <li>People who smoke or have smoked, have 1.93% more probability of getting a stroke.<\/li>\n    <li>People who work in private sector and who do government jobs have similar around 5.0% probability of getting stroke.<\/li> \n    <li>People who are <span style = 'color: #0036fa'>self employed have 7.94% probability of getting a stroke.<\/span><\/li> \n    <li>People who work in private sector have 0.29% probability of getting stroke.<\/li> \n    <li>Children and those who have <span style = 'color: #0036fa'>never worked, have almost nil chances of getting a stroke.<\/span><\/li>\n    <li>People who are <span style = 'color: #0036fa'>married have 4.91% higher probability of getting stroke<\/span> than those who are unmarried.<\/li>\n<\/ul>","f7f32266":"<h2 style = 'text-align:center ;color: #ff5252;font-weight: bold;background-color:#ececec;border-radius: 15px 50px; padding: 12px;'>Balance Data<\/h2>\n<br\/>\nTo make data balance we are going to use three techniques, 1) Undersampling, 2) Oversampling, 3) SMOTE. Also, we aren't going to use any power transformer because there wasn't much improvement.","62b66796":"Since bmi is having 201 null values which only accounts for 3.9% we can safely impute those values by using simple imputer method.","c4d057f9":"![flowchart.jpg](attachment:ca94e24c-31f3-4dea-9252-940bf4dab4f1.jpg)","47919862":"Now, we are going to check the data for the following things:\n<ul style=\"list-style-type:circle\">\n    <li>Whether the data is balance ?<\/li>\n    <li>Whether there are null values present in the data ?<\/li>\n    <li>Whether the data is skewed ?<\/li>\n    <li>Whether the data's features are correlated ?<\/li>\n<\/ul>","1fad0f85":"<span style = 'color:#ff5252; font-weight: bold'>3. Combination of under and over sampling<span>","341a21ca":"It is quite clear from the above plot that the data is imbalanced, we would need to fix this to have a better performing model.\n *  <span style = 'color:#0036fa'>The target class having no stroke, accounts for 95.1% of the whole data which is 4861 out of 5110 values.<\/span>\n *  On the other hand target class having stroke, accounts for only 249 out of 5110 values which is only 4.9%.\n * I would try 3 methods for fixing the data imbalance problem, which are <span style = 'color:#0036fa'>Undersampling, Oversampling and SMOTE.<\/span>","df816db5":"<h2 style = 'text-align:center ;color: #ff5252;font-weight: bold;background-color:#ececec;border-radius: 15px 50px; padding: 12px;'>Multivariate Analysis<\/h2>","89cd9283":"<h2 style = 'text-align:center ;color: #ff5252;font-weight: bold;background-color:#ececec;border-radius: 15px 50px; padding: 12px;'>Univariate Analysis<\/h2>\n","b22893e4":"Checking the performance of all the models on various metrics to get a better picture of their performance","3db2a858":"We implemented various models, but in the end Logistic Regression is found to be the best for this particular dataset. Our model after hypertuning achieved ROC_AUC score of 0.84, Accuracy of 0.76, F1 of 0.23, Pecision of 0.14 and Recall of 0.75. Further feature engingeering and implementation of other models can be done to find a better performing model.","51a309d6":"From above it is obvious that the <span style = 'color: #0036fa'>'age' has slight left skew<\/span>, whereas, <span style = 'color: #0036fa'>'avg_glucose_level'<\/span> and  <span style = 'color: #0036fa'>'bmi' have strong right skew.<\/span>","adc0f40a":"Creating pipeline and importing models and libraries.","833b51b0":"<span style = 'color:#ff5252; font-weight: bold'>2. Oversampling<\/span>","0dc04864":"<h2 style = 'text-align:center ;color: #ff5252;font-weight: bold;background-color:#ececec;border-radius: 15px 50px; padding: 12px;'>Modelling<\/h2>","791fe973":"<span>\nBasic inferences from the first look at the data:\n<br\/>\n    \n<ul style=\"list-style-type:circle\">\n    <li>The shape of the datset is (5110, 12), i.e, it contains <span style = 'color:#0036fa'>12 features.<\/span><\/li>\n    <li>The Dataset contains both categorical and numerical features.<\/li>\n    <li style = 'color:#0036fa'>Only 'bmi' feature is having some null values.<\/li>\n    <li>Our Target columns is the stroke column and it is represented as a binary class.<\/li>\n    <li>1 for categorical class represent +ve and 0 for being -ve.<\/li>\n    <li style = 'color:#0036fa'>We have a binary classification problem here and we are going to do prediction on the target class 'stroke'.<\/li>\n<\/ul>\n","f919b794":"<span style = 'color:#ff5252; font-weight: bold'>1. Undersampling<\/span>"}}