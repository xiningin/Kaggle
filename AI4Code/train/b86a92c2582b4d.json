{"cell_type":{"3edd9f4e":"code","438baaed":"code","5b230c49":"code","4ab5661c":"code","4e9fd113":"code","32c903ab":"code","4ca1c137":"code","619c85b8":"code","9cb72580":"code","fca375a7":"code","946d061a":"code","2b851584":"code","32cfc006":"code","72e921af":"code","74524071":"code","0fd6ccaf":"code","414aff1d":"code","18ab3754":"code","e9f4da69":"code","fdafe641":"code","b4aa0464":"code","cd5787c0":"code","1a1254e4":"code","a703d25a":"code","9159fcc7":"code","c8230f3e":"code","2a4af4a1":"code","78a50c72":"markdown","098097dd":"markdown","976f5c64":"markdown","880755ff":"markdown","98bad077":"markdown","62a6c49b":"markdown","e7d8dceb":"markdown","a513bf4b":"markdown","928bbdd2":"markdown","b100d981":"markdown","ecdaeff9":"markdown","d17071e0":"markdown","937e8268":"markdown","862de1c5":"markdown","627e93c5":"markdown","302e7313":"markdown","e38ca56b":"markdown","94730c8b":"markdown","faa1ca23":"markdown","70d61d6b":"markdown","9adc2550":"markdown","9e4fd8ef":"markdown","e3d93a53":"markdown","0998d954":"markdown","4fa777fe":"markdown","883f316d":"markdown","bbed3ffa":"markdown","74b1884b":"markdown","fda73882":"markdown","64642951":"markdown"},"source":{"3edd9f4e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","438baaed":"df = pd.read_csv(\"\/kaggle\/input\/pokemon\/Pokemon.csv\")","5b230c49":"df.head()","4ab5661c":"df.info()","4e9fd113":"numerical = df.iloc[:,4:11]\nnumerical","32c903ab":"numerical = df.iloc[:,4:11]\nfig, axes = plt.subplots(2,4, figsize=(20,10))\na = 0\nb = 0\nfor col in numerical.columns:\n    sns.distplot(numerical[col], ax = axes[a][b], kde=False )\n    b+=1\n    if b == 4:\n        b = 0\n        a += 1","4ca1c137":"sns.countplot(df[\"Legendary\"])","619c85b8":"categorical = df[[\"Type 1\", \"Type 2\", \"Generation\", \"Legendary\"]]\n\n\nfig, axes = plt.subplots(3, figsize=(10,20))\n# a = 0\nb = 0\nfor col in categorical.columns:\n    if col == \"Legendary\":\n        break\n    sns.countplot(y = categorical[col], hue= \"Legendary\", data = df, ax = axes[b])\n#     axes[a][b].set_xticklabels(axes[a][b].get_xticklabels(), rotation=90)\n    b+=1\n# ax = sns.countplot(df[\"Type 1\"])\n# ax.set_xticklabels(ax.get_xticklabels(), rotation=45)","9cb72580":"plt.figure(figsize=(8,5))\nsns.scatterplot(x=\"Total\", y=\"Attack\", hue=\"Legendary\", data=df)","fca375a7":"plt.figure(figsize=(8,5))\nsns.swarmplot(x=\"Generation\", y=\"Total\", hue=\"Legendary\", data=df)","946d061a":"from sklearn.preprocessing import LabelEncoder  #For convert categorical feature into number\nle = LabelEncoder()\ndf[\"Type 1\"] = le.fit_transform(df[\"Type 1\"])\ndf[\"Type 2\"] = df[\"Type 2\"].fillna(\"Null\") #Remove NaN values\ndf[\"Type 2\"] = le.fit_transform(df[\"Type 2\"])\ndf.head()","2b851584":"plt.figure(figsize=(16,9))\nsns.heatmap(df.corr(), annot = True)","32cfc006":"from sklearn.model_selection import train_test_split\nX = df.drop(['Name', 'Legendary'], axis = 1)\ny = df['Legendary']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0) #Split dataset into train and test set","72e921af":"result = dict()","74524071":"from sklearn.linear_model import LogisticRegression\nlogR = LogisticRegression()\nlogR.fit(X_train, y_train)\ny_pred = logR.predict(X_test)\nres = logR.score(X_test, y_test)*100\nprint(res)\n\nresult[\"logR(NP)\"] = res","0fd6ccaf":"from sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn import metrics\n\n\ncol_trans = make_column_transformer(\n            (OneHotEncoder(),['Type 1', 'Type 2', 'Generation']), #One Hot Encoder, re labelling type columns, because i heard that one hot encoder works better\n            (StandardScaler(),['Total','HP','Attack','Defense','Sp. Atk','Sp. Def','Speed']), #Scaling numeric data, so the training proccess will be better and faster\n            remainder = 'passthrough') #Not doing anything for the rest of columns","414aff1d":"pipe = make_pipeline(col_trans,logR)\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\n\nres = pipe.score(X_test, y_test)*100\nprint(res)\nresult[\"logR(P)\"] = res","18ab3754":"#SVM\nfrom sklearn.svm import SVC\n\nsvm_model = SVC()\npipe = make_pipeline(col_trans, svm_model)\npipe.fit(X_train, y_train)\nres = pipe.score(X_test, y_test)*100\n\nprint(res)\nresult[\"SVM\"] = res\n# y_pred = pipe.predict(X_test)\n# print('Accuracy score on Test data: {}'.format(metrics.accuracy_score(y_test,y_pred)*100))","e9f4da69":"from keras import backend as K\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))\n\ngamma = 2.0\nepsilon = K.epsilon()\n","fdafe641":"\nmodel = Sequential()\n\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(256))\nmodel.add(Activation('relu'))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(optimizer = Adam(1e-4),\n             loss = \"binary_crossentropy\",\n             metrics = [\"accuracy\", f1_m]\n)","b4aa0464":"from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nearly_stop = EarlyStopping(patience=20, verbose=1, monitor='val_accuracy', mode='max')\n","cd5787c0":"history = model.fit(\n        X_train,\n        y_train,\n        epochs = 100,\n        callbacks=[early_stop],\n        validation_split=0.2,\n        verbose = 0\n    \n)","1a1254e4":"res = model.evaluate(X_train, y_train)[1]*100\nresult[\"NN\"] = res","a703d25a":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=0)\npipe = make_pipeline(col_trans, rf)\npipe.fit(X_train, y_train)\nres = pipe.score(X_test, y_test)*100\n\nprint(res)\nresult[\"RF\"] = res\n","9159fcc7":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier(learning_rate=0.06, n_estimators = 100,validation_fraction = 0.2, n_iter_no_change=100)\npipe = make_pipeline(col_trans, gb)\npipe.fit(X_train, y_train)\nres = pipe.score(X_test, y_test)*100\n\nprint(res)\nresult[\"GB\"] = res","c8230f3e":"list(result.values())","2a4af4a1":"fig, ax = plt.subplots(figsize=(10,10))\nbar_plot = plt.bar(result.keys(), result.values())\n\ndef autolabel(rects):\n    x = 0\n    for idx,rect in enumerate(bar_plot):\n        height = rect.get_height()\n        ax.text(rect.get_x() + rect.get_width()\/2., 0.5*height,\n                round(list(result.values())[x], 2),\n                ha='center', va='bottom', rotation=0, size=20, color=\"white\")\n        x+=1\n\nautolabel(bar_plot)\nax.set_title(\"Prediction Result\", size=20)","78a50c72":"Let's just use simple Neural Network, these layers work best so far","098097dd":"# SVM\nNow, how about SVM?","976f5c64":"A little preprocessing, so we can see the correlation between features","880755ff":"Just a simple logistic regression","98bad077":"These two work best so far, I will try another algorithm in the future","62a6c49b":"# Random Forest and Gradient Boosting","e7d8dceb":"Now, let's see the distribution of categorical features","a513bf4b":"**90.625%**, not better than logistic regression","928bbdd2":"Let's try another logistic regression, but with preprocessed data","b100d981":"# Logistic Regression","ecdaeff9":"I'm using transformer and pipeline, they're classes provided by scikit-learn for auto preprocessing and modelling so we don't have to write longer codes :D","d17071e0":"# Preprocessing and Prediction","937e8268":"# Neural Network","862de1c5":"Can't tell much, all of the categorical features seems to be independent","627e93c5":"As what I said before, legendary pokemons tend to have higher stats","302e7313":"**88.75%**, not bad at all\n\nBut of course it can be better, let's do another preprocessing!\n","e38ca56b":"Splitting dataset ","94730c8b":"# Read and Inspect the data","faa1ca23":"F1-Score is a value for calculating Precision and Recall at the same time\n\nIn this case, it's just a bonus","70d61d6b":"Now, let's inspect the plot some features to see how is the legendary pokemon's stats are dispersed","9adc2550":"Now let's see the correlations","9e4fd8ef":"All of the features have big correlations, so I'm going to use them all","e3d93a53":"**Hi There!** I built some models to predict Legendary Pokemon using various algorithm, \n\nPlease give me some advice to improve my notebook since I'm still a beginner :)","0998d954":"As you can see, the accuracy increased 5% because of the preprocessing, **wow!**","4fa777fe":"All of the distributions are right-skewed\n\nIt's probably due to the number of legendary pokemon is small compared to non-legendary pokemon, and legendary pokemon tends to have higher stats","883f316d":"There's no null value here, let's take the quantitative columns (numerical) and then plot their distributions","bbed3ffa":"# Conclusion\n\n**That's it!** Gradient Boosting work best for predicting legendary pokemon (so far)\n\nPlease kindly give me feedback and advice, so I can improve my notebook, thanks!! :)","74b1884b":"**92-94%,** not bad...","fda73882":"**Let's predict without further preprocessing**","64642951":"# Result"}}