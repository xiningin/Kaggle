{"cell_type":{"a370812d":"code","3b494fbb":"code","5cf6d797":"code","d750fc27":"code","19eb5dab":"code","b37f39e3":"code","474ab19d":"code","bd46c8bc":"code","38cb5670":"code","5099d226":"code","da5b4be8":"code","38f6630a":"code","8a63766f":"code","80ba53bd":"code","f81cf6fc":"code","2087171d":"code","d2d0cf32":"code","c21e134d":"code","c04410bc":"code","c731e1a0":"code","9a6ac86c":"markdown","97c3b14a":"markdown","338bb914":"markdown","7afe0d0e":"markdown","5f8959fe":"markdown","c135c12b":"markdown"},"source":{"a370812d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm.auto import tqdm\nfrom glob import glob\nimport time, gc\nimport cv2\nimport albumentations as A\nfrom tensorflow import keras\nimport matplotlib.image as mpimg\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Model\nfrom keras.models import clone_model\nfrom keras.layers import Dense,Conv2D,Flatten,MaxPool2D,Dropout,BatchNormalization, Input\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.callbacks import ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport PIL.Image as Image, PIL.ImageDraw as ImageDraw, PIL.ImageFont as ImageFont\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3b494fbb":"train_df_ = pd.read_csv('\/kaggle\/input\/bengaliai-cv19\/train.csv')\ntest_df_ = pd.read_csv('\/kaggle\/input\/bengaliai-cv19\/test.csv')\nclass_map_df = pd.read_csv('\/kaggle\/input\/bengaliai-cv19\/class_map.csv')\nsample_sub_df = pd.read_csv('\/kaggle\/input\/bengaliai-cv19\/sample_submission.csv')","5cf6d797":"train_df_ = train_df_.drop(['grapheme'], axis=1, inplace=False)","d750fc27":"train_df_[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']] = train_df_[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']].astype('uint8')","19eb5dab":"HEIGHT = 137\nWIDTH = 236\nSIZE = 64\nCROP_SIZE = 64\nIMG_SIZE=64\nN_CHANNELS=1","b37f39e3":"def resize(df, size=64, need_progress_bar=True):\n    resized = {}\n    resize_size=64\n    angle=0\n    if need_progress_bar:\n        for i in tqdm(range(df.shape[0])):\n            #image = cv2.resize(df.loc[df.index[i]].values.reshape(137,236),(size,size),None,fx=0.5,fy=0.5,interpolation=cv2.INTER_AREA)\n            image=df.loc[df.index[i]].values.reshape(137,236)\n            #Centering\n            image_center = tuple(np.array(image.shape[1::-1]) \/ 2)\n            matrix = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n            image = cv2.warpAffine(image, matrix, image.shape[1::-1], flags=cv2.INTER_LINEAR,\n                            borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n            #Scaling\n            matrix = cv2.getRotationMatrix2D(image_center, 0, 1.0)\n            image = cv2.warpAffine(image, matrix, image.shape[1::-1], flags=cv2.INTER_LINEAR,\n                            borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n            #Removing Blur\n            #aug = A.GaussianBlur(p=1.0)\n            #image = aug(image=image)['image']\n            #Noise Removing\n            #augNoise=A.MultiplicativeNoise(p=1.0)\n            #image = augNoise(image=image)['image']\n            #Removing Distortion\n            #augDist=A.ElasticTransform(sigma=50, alpha=1, alpha_affine=10, p=1.0)\n            #image = augDist(image=image)['image']\n            #Brightness\n            augBright=A.RandomBrightnessContrast(p=1.0)\n            image = augBright(image=image)['image']\n            _, thresh = cv2.threshold(image, 30, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n            contours, _ = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:]\n\n            idx = 0 \n            ls_xmin = []\n            ls_ymin = []\n            ls_xmax = []\n            ls_ymax = []\n            for cnt in contours:\n                idx += 1\n                x,y,w,h = cv2.boundingRect(cnt)\n                ls_xmin.append(x)\n                ls_ymin.append(y)\n                ls_xmax.append(x + w)\n                ls_ymax.append(y + h)\n            xmin = min(ls_xmin)\n            ymin = min(ls_ymin)\n            xmax = max(ls_xmax)\n            ymax = max(ls_ymax)\n\n            roi = image[ymin:ymax,xmin:xmax]\n            resized_roi = cv2.resize(roi, (resize_size, resize_size),interpolation=cv2.INTER_AREA)\n            #image=affine_image(image)\n            #image= crop_resize(image)\n            #image = cv2.resize(image,(size,size),interpolation=cv2.INTER_AREA)\n            #image=resize_image(image,(64,64))\n            #image = cv2.resize(image,(size,size),interpolation=cv2.INTER_AREA)\n            #gaussian_3 = cv2.GaussianBlur(image, (5,5), cv2.BORDER_DEFAULT) #unblur\n            #image = cv2.addWeighted(image, 1.5, gaussian_3, -0.5, 0, image)\n            #kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]]) #filter\n            #image = cv2.filter2D(image, -1, kernel)\n            #ret,image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n            resized[df.index[i]] = resized_roi.reshape(-1)\n    else:\n        for i in range(df.shape[0]):\n            #image = cv2.resize(df.loc[df.index[i]].values.reshape(137,236),(size,size),None,fx=0.5,fy=0.5,interpolation=cv2.INTER_AREA)\n            image=df.loc[df.index[i]].values.reshape(137,236)\n            image_center = tuple(np.array(image.shape[1::-1]) \/ 2)\n            matrix = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n            image = cv2.warpAffine(image, matrix, image.shape[1::-1], flags=cv2.INTER_LINEAR,\n                            borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n            matrix = cv2.getRotationMatrix2D(image_center, 0, 1.0)\n            image = cv2.warpAffine(image, matrix, image.shape[1::-1], flags=cv2.INTER_LINEAR,\n                            borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n            #Removing Blur\n            #aug = A.GaussianBlur(p=1.0)\n            #image = aug(image=image)['image']\n            #Noise Removing\n            #augNoise=A.MultiplicativeNoise(p=1.0)\n            #image = augNoise(image=image)['image']\n            #Removing Distortion\n            #augDist=A.ElasticTransform(sigma=50, alpha=1, alpha_affine=10, p=1.0)\n            #image = augDist(image=image)['image']\n            #Brightness\n            augBright=A.RandomBrightnessContrast(p=1.0)\n            image = augBright(image=image)['image']\n            _, thresh = cv2.threshold(image, 30, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n            contours, _ = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:]\n\n            idx = 0 \n            ls_xmin = []\n            ls_ymin = []\n            ls_xmax = []\n            ls_ymax = []\n            for cnt in contours:\n                idx += 1\n                x,y,w,h = cv2.boundingRect(cnt)\n                ls_xmin.append(x)\n                ls_ymin.append(y)\n                ls_xmax.append(x + w)\n                ls_ymax.append(y + h)\n            xmin = min(ls_xmin)\n            ymin = min(ls_ymin)\n            xmax = max(ls_xmax)\n            ymax = max(ls_ymax)\n\n            roi = image[ymin:ymax,xmin:xmax]\n            resized_roi = cv2.resize(roi, (resize_size, resize_size),interpolation=cv2.INTER_AREA)\n            #image=affine_image(image)\n            #image= crop_resize(image)\n            #image = cv2.resize(image,(size,size),interpolation=cv2.INTER_AREA)\n            #image=resize_image(image,(64,64))\n            #image = cv2.resize(image,(size,size),interpolation=cv2.INTER_AREA)\n            #gaussian_3 = cv2.GaussianBlur(image, (5,5), cv2.BORDER_DEFAULT) #unblur\n            #image = cv2.addWeighted(image, 1.5, gaussian_3, -0.5, 0, image)\n            #kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]]) #filter\n            #image = cv2.filter2D(image, -1, kernel)\n            #ret,image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n            resized[df.index[i]] = resized_roi.reshape(-1)\n    resized = pd.DataFrame(resized).T\n    return resized","474ab19d":"def get_dummies(df):\n    cols = []\n    for col in df:\n        cols.append(pd.get_dummies(df[col].astype(str)))\n    return pd.concat(cols, axis=1)","bd46c8bc":"inputs = Input(shape = (IMG_SIZE, IMG_SIZE, 1))\n\nmodel = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1))(inputs)\nmodel = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = MaxPool2D(pool_size=(2, 2))(model)\nmodel = Conv2D(filters=32, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\nmodel = Dropout(rate=0.3)(model)\n\nmodel = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = MaxPool2D(pool_size=(2, 2))(model)\nmodel = Conv2D(filters=64, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = Dropout(rate=0.3)(model)\n\nmodel = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = MaxPool2D(pool_size=(2, 2))(model)\nmodel = Conv2D(filters=128, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = Dropout(rate=0.3)(model)\n\nmodel = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = MaxPool2D(pool_size=(2, 2))(model)\nmodel = Conv2D(filters=256, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = Dropout(rate=0.3)(model)\n\nmodel = Flatten()(model)\nmodel = Dense(1024, activation = \"relu\")(model)\nmodel = Dropout(rate=0.3)(model)\ndense = Dense(512, activation = \"relu\")(model)\n\nhead_root = Dense(168, activation = 'softmax')(dense)\nhead_vowel = Dense(11, activation = 'softmax')(dense)\nhead_consonant = Dense(7, activation = 'softmax')(dense)\n\nmodel = Model(inputs=inputs, outputs=[head_root, head_vowel, head_consonant])","38cb5670":"model.summary()","5099d226":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","da5b4be8":"# Set a learning rate annealer. Learning rate will be half after 3 epochs if accuracy is not increased\nlearning_rate_reduction_root = ReduceLROnPlateau(monitor='dense_3_accuracy', \n                                            patience=3, \n                                            verbose=1,\n                                            factor=0.5, \n                                            min_lr=0.00001)\nlearning_rate_reduction_vowel = ReduceLROnPlateau(monitor='dense_4_accuracy', \n                                            patience=3, \n                                            verbose=1,\n                                            factor=0.5, \n                                            min_lr=0.00001)\nlearning_rate_reduction_consonant = ReduceLROnPlateau(monitor='dense_5_accuracy', \n                                            patience=3, \n                                            verbose=1,\n                                            factor=0.5, \n                                            min_lr=0.00001)","38f6630a":"batch_size = 256\nepochs = 25","8a63766f":"# helper for mixup\ndef get_rand_bbox(width, height, l):\n    r_x = np.random.randint(width)\n    r_y = np.random.randint(height)\n    r_l = np.sqrt(1 - l)\n    r_w = np.int(width * r_l)\n    r_h = np.int(height * r_l)\n    return r_x, r_y, r_l, r_w, r_h\n\n# custom image data generator\nclass MultiOutputDataGenerator(keras.preprocessing.image.ImageDataGenerator):\n    # custom image generator\n    def __init__(self, featurewise_center = False, samplewise_center = False, \n                 featurewise_std_normalization = False, samplewise_std_normalization = False, \n                 zca_whitening = False, zca_epsilon = 1e-06, rotation_range = 0.0, width_shift_range = 0.0, \n                 height_shift_range = 0.0, brightness_range = None, shear_range = 0.0, zoom_range = 0.0, \n                 channel_shift_range = 0.0, fill_mode = 'nearest', cval = 0.0, horizontal_flip = False, \n                 vertical_flip = False, rescale = None, preprocessing_function = None, data_format = None, validation_split = 0.0, \n                 mix_up_alpha = 0.0, cutmix_alpha = 0.0): # additional class argument\n    \n        # parent's constructor\n        super().__init__(featurewise_center, samplewise_center, featurewise_std_normalization, samplewise_std_normalization, \n                         zca_whitening, zca_epsilon, rotation_range, width_shift_range, height_shift_range, brightness_range, \n                         shear_range, zoom_range, channel_shift_range, fill_mode, cval, horizontal_flip, vertical_flip, rescale, \n                         preprocessing_function, data_format, validation_split)\n\n        # Mix-up\n        assert mix_up_alpha >= 0.0\n        self.mix_up_alpha = mix_up_alpha\n        \n        # Cutmix\n        assert cutmix_alpha >= 0.0\n        self.cutmix_alpha = cutmix_alpha\n\n    def mix_up(self, X1, y1, X2, y2, ordered_outputs, target_lengths):\n        assert X1.shape[0] == y1.shape[0] == X2.shape[0] == y2.shape[0]\n        batch_size = X1.shape[0]\n        l = np.random.beta(self.mix_up_alpha, self.mix_up_alpha, batch_size)\n        X_l = l.reshape(batch_size, 1, 1, 1)\n        y_l = l.reshape(batch_size, 1)\n        X = X1 * X_l + X2 * (1-X_l)\n        target_dict = {}\n        i = 0\n        for output in ordered_outputs:\n            target_length = target_lengths[output]\n            target_dict[output] = y1[:, i: i + target_length] * y_l + y2[:, i: i + target_length] * (1 - y_l)\n            i += target_length\n        y = None\n        for output, target in target_dict.items():\n            if y is None:\n                y = target\n            else:\n                y = np.concatenate((y, target), axis=1)\n        return X, y\n    \n    def cutmix(self, X1, y1, X2, y2, ordered_outputs, target_lengths):\n        assert X1.shape[0] == y1.shape[0] == X2.shape[0] == y2.shape[0]\n        lam = np.random.beta(self.cutmix_alpha, self.cutmix_alpha)\n        width = X1.shape[1]\n        height = X1.shape[0]\n        r_x, r_y, r_l, r_w, r_h = get_rand_bbox(width, height, lam)\n        bx1 = np.clip(r_x - r_w \/\/ 2, 0, width)\n        by1 = np.clip(r_y - r_h \/\/ 2, 0, height)\n        bx2 = np.clip(r_x + r_w \/\/ 2, 0, width)\n        by2 = np.clip(r_y + r_h \/\/ 2, 0, height)\n        X1[:, bx1:bx2, by1:by2, :] = X2[:, bx1:bx2, by1:by2, :]\n        X = X1\n        target_dict = {}\n        i = 0\n        for output in ordered_outputs:\n            target_length = target_lengths[output]\n            target_dict[output] = y1[:, i: i + target_length] * lam + y2[:, i: i + target_length] * (1 - lam)\n            i += target_length\n        y = None\n        for output, target in target_dict.items():\n            if y is None:\n                y = target\n            else:\n                y = np.concatenate((y, target), axis=1)\n        return X, y\n    \n    def flow(self,\n             x,\n             y=None,\n             batch_size=32,\n             shuffle=True,\n             sample_weight=None,\n             seed=None,\n             save_to_dir=None,\n             save_prefix='',\n             save_format='png',\n             subset=None):\n        \n        # for multi-outputs\n        targets = None\n        target_lengths = {}\n        ordered_outputs = []\n        for output, target in y.items():\n            if targets is None:\n                targets = target\n            else:\n                targets = np.concatenate((targets, target), axis=1)\n            target_lengths[output] = target.shape[1]\n            ordered_outputs.append(output)\n        \n        # parent flow\n        batches = super().flow(x, targets, batch_size, shuffle, sample_weight, seed, save_to_dir, save_prefix, save_format, subset)\n        \n        # custom processing\n        while True:\n            batch_x, batch_y = next(batches)\n            \n            # mixup or cutmix\n            if (self.mix_up_alpha > 0) & (self.cutmix_alpha > 0):\n                while True:\n                    batch_x_2, batch_y_2 = next(batches)\n                    m1, m2 = batch_x.shape[0], batch_x_2.shape[0]\n                    if m1 < m2:\n                        batch_x_2 = batch_x_2[:m1]\n                        batch_y_2 = batch_y_2[:m1]\n                        break\n                    elif m1 == m2:\n                        break\n                if np.random.rand() < 0.5:\n                    batch_x, batch_y = self.mix_up(batch_x, batch_y, batch_x_2, batch_y_2, ordered_outputs, target_lengths)\n                else:\n                    batch_x, batch_y = self.cutmix(batch_x, batch_y, batch_x_2, batch_y_2, ordered_outputs, target_lengths)\n            \n                target_dict = {}\n                i = 0\n                for output in ordered_outputs:\n                    target_length = target_lengths[output]\n                    target_dict[output] = batch_y[:, i: i + target_length]\n                    i += target_length\n                    \n                yield batch_x, target_dict","80ba53bd":"i = 0 # example\ntrain_df = pd.merge(pd.read_parquet(f'\/kaggle\/input\/bengaliai-cv19\/train_image_data_{i}.parquet'), train_df_, on='image_id').drop(['image_id'], axis=1)\n\n# Visualize few samples of current training dataset\n#fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16, 8))\n#count=0\n#for row in ax:\n    #for col in row:\n        #col.imshow(resize(train_df.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic'], axis=1).iloc[[count]], need_progress_bar=False).values.reshape(-1).reshape(IMG_SIZE, IMG_SIZE).astype(np.float64))\n        #count += 1\n#plt.show()\n\nX_train = train_df.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic'], axis=1)\nX_train = resize(X_train)\/255\n\n# CNN takes images in shape `(batch_size, h, w, channels)`, so reshape the images\nX_train = X_train.values.reshape(-1, IMG_SIZE, IMG_SIZE, N_CHANNELS)\n\nY_train_root = pd.get_dummies(train_df['grapheme_root']).values\nY_train_vowel = pd.get_dummies(train_df['vowel_diacritic']).values\nY_train_consonant = pd.get_dummies(train_df['consonant_diacritic']).values\n\nprint(f'Training images: {X_train.shape}')\nprint(f'Training labels root: {Y_train_root.shape}')\nprint(f'Training labels vowel: {Y_train_vowel.shape}')\nprint(f'Training labels consonants: {Y_train_consonant.shape}')\n\n# Divide the data into training and validation set (test size was set to 0.08!)\nx_train, x_test, y_train_root, y_test_root, y_train_vowel, y_test_vowel, y_train_consonant, y_test_consonant = train_test_split(X_train, Y_train_root, Y_train_vowel, Y_train_consonant, test_size=0.10, random_state=666)\ndel train_df\ndel X_train\ndel Y_train_root, Y_train_vowel, Y_train_consonant\n\n# Data augmentation for creating more training data\ndatagen = MultiOutputDataGenerator(\n    featurewise_center=False,  # set input mean to 0 over the dataset\n    samplewise_center=False,  # set each sample mean to 0\n    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n    samplewise_std_normalization=False,  # divide each input by its std\n    zca_whitening=False,  # apply ZCA whitening\n    rotation_range=16,  # randomly rotate images in the range (degrees, 0 to 180, was 8)\n    zoom_range = 0.15, # Randomly zoom image \n    width_shift_range=0.15,  # randomly shift images horizontally (fraction of total width)\n    height_shift_range=0.15,  # randomly shift images vertically (fraction of total height)\n    horizontal_flip=False,  # randomly flip images\n    vertical_flip=False,\n    mix_up_alpha = 0.2, \n    cutmix_alpha = 0.2)  \n\n\n# This will just calculate parameters required to augment the given data.\ndatagen.fit(x_train)\n\n# show images\ndef show_imgs(imgs, row, col):\n    if len(imgs) != (row * col):\n        raise ValueError(\"Invalid imgs len:{} col:{} row:{}\".format(len(imgs), row, col))\n\n    fig = plt.figure(figsize=(12, 12))\n    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n    plt.title(\"Mixup and Cutmix Augmentation\")\n    for i, img in enumerate(imgs):\n        plot_num = i+1\n        ax = fig.add_subplot(row, col, plot_num, xticks=[], yticks=[])\n        ax.imshow(img)\n    \n    plt.show()\n\n# let's have a look if our custom ImageDataGenerator works fine\nmax_img_num = 12\nimgs = []\nfor d in datagen.flow(x_train, {'dense_3': y_train_root, 'dense_4': y_train_vowel, 'dense_5': y_train_consonant}, batch_size=64):\n    # note that target_size = (height, width)\n    imgs.append(np.squeeze(d[0][0, :, :, 0].reshape(-1).reshape(1, IMG_SIZE, IMG_SIZE).astype(np.float64), axis=0))\n    if (len(imgs) % max_img_num) == 0:\n        break\nshow_imgs(imgs, row=3, col=4)\ndel x_train\ndel x_test\ndel y_train_root\ndel y_test_root\ndel y_train_vowel\ndel y_test_vowel\ndel y_train_consonant\ndel y_test_consonant\ngc.collect()\n","f81cf6fc":"i = 0 # example\nhistories = []\nfor i in range(4):\n    train_df = pd.merge(pd.read_parquet(f'\/kaggle\/input\/bengaliai-cv19\/train_image_data_{i}.parquet'), train_df_, on='image_id').drop(['image_id'], axis=1)\n\n    # Visualize few samples of current training dataset\n    fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16, 8))\n    count=0\n    for row in ax:\n        for col in row:\n            col.imshow(resize(train_df.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic'], axis=1).iloc[[count]], need_progress_bar=False).values.reshape(-1).reshape(IMG_SIZE, IMG_SIZE).astype(np.float64))\n            count += 1\n    plt.show()\n\n    X_train = train_df.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic'], axis=1)\n    X_train = resize(X_train)\/255\n\n    # CNN takes images in shape `(batch_size, h, w, channels)`, so reshape the images\n    X_train = X_train.values.reshape(-1, IMG_SIZE, IMG_SIZE, N_CHANNELS)\n\n    Y_train_root = pd.get_dummies(train_df['grapheme_root']).values\n    Y_train_vowel = pd.get_dummies(train_df['vowel_diacritic']).values\n    Y_train_consonant = pd.get_dummies(train_df['consonant_diacritic']).values\n\n    print(f'Training images: {X_train.shape}')\n    print(f'Training labels root: {Y_train_root.shape}')\n    print(f'Training labels vowel: {Y_train_vowel.shape}')\n    print(f'Training labels consonants: {Y_train_consonant.shape}')\n\n    # Divide the data into training and validation set (test size was set to 0.08!)\n    x_train, x_test, y_train_root, y_test_root, y_train_vowel, y_test_vowel, y_train_consonant, y_test_consonant = train_test_split(X_train, Y_train_root, Y_train_vowel, Y_train_consonant, test_size=0.10, random_state=666)\n    del train_df\n    del X_train\n    del Y_train_root, Y_train_vowel, Y_train_consonant\n\n    # Data augmentation for creating more training data\n    datagen = MultiOutputDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=16,  # randomly rotate images in the range (degrees, 0 to 180, was 8)\n        zoom_range = 0.15, # Randomly zoom image \n        width_shift_range=0.15,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.15,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False,\n        mix_up_alpha = 0.2, \n        cutmix_alpha = 0.2)  \n\n\n    # This will just calculate parameters required to augment the given data.\n    datagen.fit(x_train)\n     # Fit the model\n    history = model.fit_generator(datagen.flow(x_train, {'dense_3': y_train_root, 'dense_4': y_train_vowel, 'dense_5': y_train_consonant}, batch_size=batch_size),\n                              epochs = epochs, validation_data = (x_test, [y_test_root, y_test_vowel, y_test_consonant]), \n                              steps_per_epoch=x_train.shape[0] \/\/ batch_size, \n                              callbacks=[learning_rate_reduction_root, learning_rate_reduction_vowel, learning_rate_reduction_consonant])\n\n    histories.append(history)\n    \n    # Delete to reduce memory usage\n    del x_train\n    del x_test\n    del y_train_root\n    del y_test_root\n    del y_train_vowel\n    del y_test_vowel\n    del y_train_consonant\n    del y_test_consonant\n    gc.collect()","2087171d":"%matplotlib inline\ndef plot_loss(his, epoch, title):\n    plt.style.use('ggplot')\n    plt.figure()\n    plt.plot(np.arange(0, epoch), his.history['loss'], label='train_loss')\n    plt.plot(np.arange(0, epoch), his.history['dense_3_loss'], label='train_root_loss')\n    plt.plot(np.arange(0, epoch), his.history['dense_4_loss'], label='train_vowel_loss')\n    plt.plot(np.arange(0, epoch), his.history['dense_5_loss'], label='train_consonant_loss')\n    \n    plt.plot(np.arange(0, epoch), his.history['val_dense_3_loss'], label='val_train_root_loss')\n    plt.plot(np.arange(0, epoch), his.history['val_dense_4_loss'], label='val_train_vowel_loss')\n    plt.plot(np.arange(0, epoch), his.history['val_dense_5_loss'], label='val_train_consonant_loss')\n    \n    plt.title(title)\n    plt.xlabel('Epoch #')\n    plt.ylabel('Loss')\n    plt.legend(loc='upper right')\n    plt.show()\n\ndef plot_acc(his, epoch, title):\n    plt.style.use('ggplot')\n    plt.figure()\n    plt.plot(np.arange(0, epoch), his.history['dense_3_accuracy'], label='train_root_acc')\n    plt.plot(np.arange(0, epoch), his.history['dense_4_accuracy'], label='train_vowel_accuracy')\n    plt.plot(np.arange(0, epoch), his.history['dense_5_accuracy'], label='train_consonant_accuracy')\n    \n    plt.plot(np.arange(0, epoch), his.history['val_dense_3_accuracy'], label='val_root_acc')\n    plt.plot(np.arange(0, epoch), his.history['val_dense_4_accuracy'], label='val_vowel_accuracy')\n    plt.plot(np.arange(0, epoch), his.history['val_dense_5_accuracy'], label='val_consonant_accuracy')\n    plt.title(title)\n    plt.xlabel('Epoch #')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='upper right')\n    plt.show()","d2d0cf32":"for dataset in range(4):\n    plot_loss(histories[dataset], epochs, f'Training Dataset: {dataset}')\n    plot_acc(histories[dataset], epochs, f'Training Dataset: {dataset}')","c21e134d":"del histories\ngc.collect()","c04410bc":"preds_dict = {\n    'grapheme_root': [],\n    'vowel_diacritic': [],\n    'consonant_diacritic': []\n}","c731e1a0":"components = ['consonant_diacritic', 'grapheme_root', 'vowel_diacritic']\ntarget=[] # model predictions placeholder\nrow_id=[] # row_id place holder\nfor i in range(4):\n    df_test_img = pd.read_parquet('\/kaggle\/input\/bengaliai-cv19\/test_image_data_{}.parquet'.format(i)) \n    df_test_img.set_index('image_id', inplace=True)\n\n    X_test = resize(df_test_img, need_progress_bar=False)\/255\n    X_test = X_test.values.reshape(-1, IMG_SIZE, IMG_SIZE, N_CHANNELS)\n    \n    preds = model.predict(X_test)\n\n    for i, p in enumerate(preds_dict):\n        preds_dict[p] = np.argmax(preds[i], axis=1)\n\n    for k,id in enumerate(df_test_img.index.values):  \n        for i,comp in enumerate(components):\n            id_sample=id+'_'+comp\n            row_id.append(id_sample)\n            target.append(preds_dict[comp][k])\n    del df_test_img\n    del X_test\n    gc.collect()\n\ndf_sample = pd.DataFrame(\n    {\n        'row_id': row_id,\n        'target':target\n    },\n    columns = ['row_id','target'] \n)\ndf_sample.to_csv('submission.csv',index=False)\ndf_sample.head()","9a6ac86c":"# Visualization of some examples with mixup and cutmix augumentation","97c3b14a":"## Training with Mixup+Cutmix Augmentation","338bb914":"# Custom Image Generator with Mixup & Cutmix\nMixup+Cutmix Credit goes to [this notebook](https:\/\/www.kaggle.com\/code1110\/mixup-cutmix-in-keras)","7afe0d0e":"In this notebook i have added Mixup and Cutmix Augmentation with deep cnn model. I have decreased the epochs becuase it will take too much time with augmentation. Mixup+Cutmix Credit goes to [this notebook](https:\/\/www.kaggle.com\/code1110\/mixup-cutmix-in-keras)","5f8959fe":"# Image Processing","c135c12b":"# Deep CNN Model"}}