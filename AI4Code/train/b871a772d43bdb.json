{"cell_type":{"d0512a6c":"code","89e5c2d6":"code","d0eb4094":"code","3a728843":"code","139d4d3a":"code","58567912":"code","0c8ce96c":"markdown"},"source":{"d0512a6c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","89e5c2d6":"df=pd.read_csv(\"\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv\")","d0eb4094":"df.head(26)","3a728843":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import LogisticRegression\n\n\ndf=df.drop(['ssc_b','hsc_b'],axis=1)\n\ny=df['status'].copy()\n#Dropping the output label from main dataframe and storing it in y\ndf=df.drop(['status','salary'],axis=1)\n#drop salary since we are only predicted if the candidate is placed or not. So if we include salary in this situation, it would\n#lead to target data leakage\nl=LabelEncoder()\n#encode work column as 0 or 1\ndf['workex']=l.fit_transform(df['workex'])\ns=(df.dtypes=='object')\np=list(s[s].index)\nprint(p)\n\n\n\n\n\n\n\n\n\n\n\n","139d4d3a":"#split the data in to training and testing sets\nxtrain,xtest,ytrain,ytest=train_test_split(df,y,test_size=0.2,random_state=3)\n#One hot encoding all the object columns whose names are stored in p\none=OneHotEncoder(handle_unknown='ignore',sparse=False)\npd1=pd.DataFrame(one.fit_transform(xtrain[p]))\npd2=pd.DataFrame(one.fit_transform(xtest[p]))\npd1.index=xtrain.index\npd2.index=xtest.index\npd1.columns=one.get_feature_names(p)\npd2.columns=one.get_feature_names(p)\nxtrain=xtrain.drop(p,axis=1)\nxtest=xtest.drop(p,axis=1)\ndf1=pd.concat([xtrain,pd1],axis=1)\ndf2=pd.concat([xtest,pd2],axis=1)\nprint(df1)\n","58567912":"#since we only need n-1 columns out of n columns generated by onehotencoding, drop them too\ndf1=df1.drop(['gender_F','hsc_s_Arts','degree_t_Comm&Mgmt','specialisation_Mkt&Fin','sl_no'],axis=1)\ndf2=df2.drop(['gender_F','hsc_s_Arts','degree_t_Comm&Mgmt','specialisation_Mkt&Fin','sl_no'],axis=1)\n\nytrain=l.fit_transform(ytrain)\nytest=l.fit_transform(ytest)\nmodel=DecisionTreeClassifier(random_state=1)\nmodel1=LogisticRegression(random_state=1)\n\nm=model.fit(df1,ytrain)\nm1=model1.fit(df1,ytrain)\n\nprint(\"Model accuracy with test samples for Decision trees:\")\nprint(m.score(df2,ytest))\nyx=m.predict(df2)\nprint(\"Mean absolute error of model with testing data for decision trees:\")\nprint(mean_absolute_error(yx,ytest))\n\nprint(\"Model accuracy with test samples for Logistic Regression:\")\nprint(m1.score(df2,ytest))\nyx1=m1.predict(df2)\nprint(\"Mean absolute error of model with testing data for Logistic Regression:\")\nprint(mean_absolute_error(yx1,ytest))","0c8ce96c":"A simple decision tree classifier and the logistic regression classifier is used to predict if a candidate with some specific features like marks,education, experience will be recruited or not.\nThere is no enough data. So the model perfectly fits the model with zero error. But since lack of data, the model gives out an error which is also approximately equal to zero and the accuracy of the logistic regression model stands at 90 percent.\n* Still the accuracy can be improved by trying out different models but the data should also be sufficient enough.\nOne thing to note is that since the dataset is small, using simple models is always better with most of the default configurations\n"}}