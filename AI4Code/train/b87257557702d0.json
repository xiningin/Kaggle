{"cell_type":{"0e560a84":"code","1fd0b84f":"code","ed935749":"code","7fd0c7c0":"markdown"},"source":{"0e560a84":"# La funci\u00f3n loadDataSet () crea algunas muestras experimentales. \n# La primera variable devuelta por esta funci\u00f3n es la colecci\u00f3n de documentos despu\u00e9s \n# de la segmentaci\u00f3n de entrada, y la segunda variable es una categor\u00eda personalizada\n\ndef loadDataSet():\n    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n    classVec = [0, 1, 0, 1, 0, 1]\n    return postingList, classVec\n\n# Crea una lista de diccionarios \u00fanica\ndef createVocabList(dataSet):\n    vocabSet = set([])               # Crea un conjunto vac\u00edo\n    for document in dataSet:\n        vocabSet = vocabSet | set(document)   #Crea la uni\u00f3n de dos conjuntos\n    return list(vocabSet)\n\n# Vector de documento de salida\ndef setOfWords2Vec(vocabList, inputSet):\n    returnVec = [0]*len(vocabList)       #Crear un vector con los 0 elementos\n    # Palabras de conjunto de datos cruzados\n    for word in inputSet:\n        #Palabras existentes en bolsa de palabras\n        if word in vocabList:\n            #index se usa para encontrar el primer sub\u00edndice coincidente\n            returnVec[vocabList.index(word)] = 1\n        else:\n            print (\"the word: %s is not in my Vocabulary!\" % word)\n    return returnVec\n\ndataSet,classVec = loadDataSet()\n\n# Resultado de la prueba: puede ver que la oraci\u00f3n se transforma en un vector digital.\n\nvocabset = createVocabList(dataSet)\nprint(vocabset)\nprint(\"\\n\")\n\nreturnVec = setOfWords2Vec(vocabset,dataSet[0])\nprint(returnVec)\n\n","1fd0b84f":"from numpy import *\n\ndef loadDataSet():\n    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n    classVec = [0, 1, 0, 1, 0, 1]\n    return postingList, classVec\n\n# Crea una lista de diccionarios \u00fanica\ndef createVocabList(dataSet):\n    vocabSet = set([])               # Crea un conjunto vac\u00edo\n    for document in dataSet:\n        vocabSet = vocabSet | set(document)   #Crea la uni\u00f3n de dos conjuntos\n    return list(vocabSet)\n\n# Vector de documento de salida\ndef setOfWords2Vec(vocabList, inputSet):\n    returnVec = [0]*len(vocabList)       #Crear un vector con los 0 elementos\n    # Palabras de conjunto de datos cruzados\n    for word in inputSet:\n        #Palabras existentes en bolsa de palabras\n        if word in vocabList:\n            #index se usa para encontrar el primer sub\u00edndice coincidente\n            returnVec[vocabList.index(word)] = 1\n        else:\n            print (\"the word: %s is not in my Vocabulary!\" % word)\n    return returnVec\n\n# Algoritmo de entrenamiento, buscando probabilidad\n\"\"\"\n El pseudoc\u00f3digo de esta funci\u00f3n es el siguiente:\n Cuente el n\u00famero de documentos en cada categor\u00eda\n Para cada documento de formaci\u00f3n:\n\t Para cada categor\u00eda:\n\t\t Si la entrada aparece en el documento, aumente el recuento de la entrada.\n\t\t Incrementar el valor de recuento de todas las entradas.\n\t Para cada categor\u00eda:\n\t\t Para cada entrada:\n\t\t Divida el n\u00famero de entradas por el n\u00famero total de entradas para obtener la probabilidad condicional\n\t Devuelve la probabilidad condicional de cada categor\u00eda\n\"\"\"\ndef trainNB0(trainMatrix,trainCategory):\n    numTrainDocs = len(trainMatrix)\n    numWords = len(trainMatrix[0])\n    pAbusive = sum(trainCategory)\/float(numTrainDocs)\n    # Probabilidad de inicializaci\u00f3n\n    p0Num = zeros(numWords); p1Num = zeros(numWords)\n    p0Denom = 0.0; p1Denom = 0.0\n    # Recorre el documento, agrega vectores\n    for i in range(numTrainDocs):\n        if trainCategory[i] == 1:           # Documentos de insulto, adici\u00f3n de vectores\n            p1Num += trainMatrix[i]\n            p1Denom += sum(trainMatrix[i])\n        else:                               # Adici\u00f3n de vector de documento no insultante\n            p0Num += trainMatrix[i]\n            p0Denom += sum(trainMatrix[i])\n\n    p1Vect = p1Num\/p1Denom         # La probabilidad de que cada palabra aparezca en la categor\u00eda de insulto\n    p0Vect = p0Num\/p0Denom         # La probabilidad de que cada palabra aparezca en la categor\u00eda no insultante\n    return p0Vect,p1Vect,pAbusive\n\ndataSet, classVec = loadDataSet()\nvocabset = createVocabList(dataSet)\ntrainMat = []\n\nfor postinDoc in dataSet:\n    trainMat.append(setOfWords2Vec(vocabset,postinDoc))\np0Vect, p1Vect, pAbusive = trainNB0(trainMat,classVec)\n\n# Vectores de probabilidad de dos categor\u00edas y la probabilidad de ser un documento insultante:\n    \nprint(p0Vect)   # Probabilidad vectorial de palabras no insultantes\nprint(p1Vect)   # Insulto palabra vector probabilidad\nprint(pAbusive)  #Probabilidad de documentos ofensivos","ed935749":"from numpy import *\n\ndef loadDataSet():\n    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n    classVec = [0, 1, 0, 1, 0, 1]\n    return postingList, classVec\n\n# Crea una lista de diccionarios \u00fanica\ndef createVocabList(dataSet):\n    vocabSet = set([])               # Crea un conjunto vac\u00edo\n    for document in dataSet:\n        vocabSet = vocabSet | set(document)   #Crea la uni\u00f3n de dos conjuntos\n    return list(vocabSet)\n\n# Vector de documento de salida\ndef setOfWords2Vec(vocabList, inputSet):\n    returnVec = [0]*len(vocabList)       #Crear un vector con los 0 elementos\n    # Palabras de conjunto de datos cruzados\n    for word in inputSet:\n        #Palabras existentes en bolsa de palabras\n        if word in vocabList:\n            #index se usa para encontrar el primer sub\u00edndice coincidente\n            returnVec[vocabList.index(word)] = 1\n        else:\n            print (\"the word: %s is not in my Vocabulary!\" % word)\n    return returnVec\n\n\ndef trainNB0(trainMatrix,trainCategory):\n    numTrainDocs = len(trainMatrix)\n    numWords = len(trainMatrix[0])\n    pAbusive = sum(trainCategory)\/float(numTrainDocs)\n    p0Num = zeros(numWords); p1Num = zeros(numWords)\n    p0Denom = 0.0; p1Denom = 0.0\n    # Recorre el documento, agrega vectores\n    for i in range(numTrainDocs):\n        if trainCategory[i] == 1:           # Documentos de insulto, adici\u00f3n de vectores\n            p1Num += trainMatrix[i]\n            p1Denom += sum(trainMatrix[i])\n        else:                               # Adici\u00f3n de vector de documento no insultante\n            p0Num += trainMatrix[i]\n            p0Denom += sum(trainMatrix[i])\n\n    p1Vect = p1Num\/p1Denom         # La probabilidad de que cada palabra aparezca en la categor\u00eda de insulto\n    p0Vect = p0Num\/p0Denom         # La probabilidad de que cada palabra aparezca en la categor\u00eda no insultante\n    return p0Vect,p1Vect,pAbusive\n\n# Probar algoritmo\ndef classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n    #Calcule la probabilidad de abuso\n    p1 = sum(vec2Classify * p1Vec) + log(pClass1)\n    #Calcular probabilidad no abusiva\n    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n    #Ver qu\u00e9 probabilidad es alta\n    if p1 > p0:\n        return 1\n    else:\n        return 0\n\ndef testingNB():\n    # test_list=[]; test_class=[0,1]\n    # Cargar conjunto de datos\n    listOPosts,listClasses = loadDataSet()\n    #Create vocabulario\n    myVocabList = createVocabList(listOPosts)\n    trainMat=[]\n    for postinDoc in listOPosts:\n        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))\n    testEntry = ['love', 'my', 'dalmation']\n    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n    print (testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb))\n    testEntry = ['stupid', 'garbage']\n    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n    print (testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb))\n\ntestingNB()","7fd0c7c0":"# Clasificador Naive Bayes\n\nEn teor\u00eda de la probabilidad y miner\u00eda de datos, un clasificador Naive Bayes es un clasificador probabil\u00edstico fundamentado en el teorema de Bayes y algunas hip\u00f3tesis simplificadoras adicionales. Es a causa de estas simplificaciones, que se suelen resumir en la hip\u00f3tesis de independencia entre las variables predictoras, que recibe el apelativo de naive, es decir, **ingenuo**.\n\n**Ejercicio de clasificaci\u00f3n de documentos**\n\nhttp:\/\/pdln.blogspot.com\/2013\/04\/clasificacion-de-texto-naive-bayes-paso.html\n\nSupongamos que tengo 4 documentos en mi corpus de entrenamiento, y dos clases: \"C\" y \"U\" que indican si un documento habla sobre Chile o sobre Uruguay respectivamente. Tengo ademas un quinto documento con el cual voy a probar mi clasificador:\n\n* Corpus         Doc   Palabras                    Clase\n* Entrenamiento   1   Chileno Santiago Chileno    C\n*                 2   Chileno Chileno Valparaiso  C\n*                 3   Chileno Allende             C\n*                 4   Montevideo Uruguay Chileno  U\n* Prueba          5   Chileno Chileno Chileno Montevideo Uruguay  ?\n\nLas formulas de Na\u00efve Bayes son:\n^             \nP(c) =  Nc \/ N\n\ndonde N es el n\u00famero de documentos\n\n^          \nP(w | c) = cantidad (w | c) +1  \/ cantidad(c) + |V|\n\ny luego calculamos la clase del doc 5 viendo cual maximiza su probabilidad:\n\ncmap =  (argmax P(cj) \u220f P(xi | cj)) \/ cj \u2208 C        \n\nBueno, ahora a calcular!\n\nP(C) = 3\/4              Tengo 3 documentos de clase C, de un total de 4\nP(U) = 1\/4              Tengo 1 solo documento de clase U, de un total de 4\n\nV = 6, es el largo del vocabulario, palabras sin repetir\n\nCalculamos las probabilidades condicionales que nos interesan para clasificar al documento 5\n\nPara eso, por cada palabra del documento a clasificar calculamos\n\nP(Chileno|C)=(5+1)\/(8+6)=3\/7 #Esta 5 veces en doc clase C y 8 palabras tiene la clase C y 6 son las palabras del corpus sin repetici\u00f3n\nP(Montevideo| C) = (0+1) \/ (8+6) = 1\/14\nP(Uruguay|C) = (0+1) \/ (8+6) = 1\/14\n\nP(Chileno| U) =  (1+1) \/ (3+6) = 2\/9\nP(Montevideo| U) =(1+1) \/ (3+6) = 2\/9\nP(Uruguay|U) =(1+1) \/ (3+6) = 2\/9\n\nAhora calculemos cual de ambas probabilidades es m\u00e1s grande:\n\nP(C|doc5) \u221d 3\/4 * 3\/7 * 3\/7 * 3\/7 * 1\/14 * 1\/14 \u2248 0.0003\n\nP(U|doc5) \u221d 1\/4 * 2\/9 * 2\/9 * 2\/9 * 2\/9 * 2\/9  \u2248 0.0001\n\nEntonces, seg\u00fan este modelo el documento 5 habla sobre Chile, es decir, pertenece a la clase C\n\nNOTA: usamos el signo \u221d para indicar que la probabilidad calculada es proporcional y no igual ya que eliminamos el denominador de la ecuaci\u00f3n, total solo nos interesa saber cual es mayor. El denominador tendr\u00eda que ser P(d).\n\n\n"}}