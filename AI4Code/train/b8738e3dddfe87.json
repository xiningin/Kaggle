{"cell_type":{"10397e1a":"code","c07d4ab5":"code","e7b24ef1":"code","0af1ed46":"code","faa2eac7":"code","5f92e0f8":"code","ac121474":"markdown"},"source":{"10397e1a":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport sys\nimport time\nfrom transformers import AutoTokenizer, TFAutoModelForQuestionAnswering\nimport textwrap\nimport re\nimport attr\nimport abc\nimport string\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom IPython.display import HTML\nfrom os import listdir\nfrom os.path import isfile, join\n\nimport warnings  \nwarnings.filterwarnings('ignore')\nMAX_ARTICLES = 1000\nbase_dir = '\/kaggle\/input'\ndata_dir = base_dir + '\/covid-19-articles'\ndata_path = data_dir + '\/covid19.csv'\nmodel_path = base_dir + '\/biobert-qa\/biobert_squad2_cased'\ndf=pd.read_csv(data_path)\nclass ResearchQA(object):\n    def __init__(self, data_path, model_path):\n        print('Loading data from', data_path)\n        self.df = pd.read_csv(data_path)\n        print('Initializing model from', model_path)\n        self.model = TFAutoModelForQuestionAnswering.from_pretrained(model_path, from_pt=True)\n        tf.saved_model.save(self.model, '\/kaggle\/output')\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        self.retrievers = {}\n        self.build_retrievers()\n        self.main_question_dict = dict()\n        \n    \n    def build_retrievers(self):\n        df = self.df\n        abstracts = df[df.abstract.notna()].abstract\n        self.retrievers['abstract'] = TFIDFRetrieval(abstracts)\n        body_text = df[df.body_text.notna()].body_text\n        self.retrievers['body_text'] = TFIDFRetrieval(body_text)\n\n    def retrieve_candidates(self, section_path, question, top_n):\n        candidates = self.retrievers[section_path[0]].retrieve(question, top_n)\n        return self.df.loc[candidates.index]\n          \n        \n    def get_answers(self, question, section='abstract', keyword=None, max_articles=1000, batch_size=4):\n        df = self.df\n        answers = []\n        section_path = section.split('\/')\n\n        if keyword:\n            candidates = df[df[section_path[0]].str.contains(keyword, na=False, case=False)]\n        else:\n            candidates = self.retrieve_candidates(section_path, question, top_n=max_articles) #get top N candidate articles based on similarity score\n        if max_articles:\n            candidates = candidates.head(max_articles)\n\n        text_list = []\n        indices = []\n        for idx, row in candidates.iterrows():\n            if section_path[0] == 'body_text':\n                text = self.get_body_section(row.body_text, section_path[1])\n            else:\n                text = row[section]\n            if text and isinstance(text, str):\n                text_list.append(text)\n                indices.append(idx)\n\n        num_batches = len(text_list) \/\/ batch_size\n        all_answers = []\n        for i in range(num_batches):\n            batch = text_list[i * batch_size:(i+1) * batch_size]\n            answers = self.get_answers_from_text_list(question, batch)\n            all_answers.extend(answers)\n\n        last_batch = text_list[batch_size * num_batches:]\n        if last_batch:\n            all_answers.extend(self.get_answers_from_text_list(question, last_batch))\n\n        columns = ['doi', 'authors', 'journal', 'publish_time', 'title', 'cohort_size']\n        processed_answers = []\n        for i, a in enumerate(all_answers):\n            if a:\n                row = candidates.loc[indices[i]]\n                new_row = [a.text, a.start_score, a.end_score, a.input_text]\n                new_row.extend(row[columns].values)\n                processed_answers.append(new_row)\n        answer_df = pd.DataFrame(processed_answers, columns=(['answer', 'start_score',\n                                                 'end_score', 'context'] + columns))\n        return answer_df.sort_values(['start_score', 'end_score'], ascending=False)\n\n    def get_body_section(self, body_text, section_name):\n      sections = body_text.split('<SECTION>\\n')\n      for section in sections:\n        lines = section.split('\\n')\n        if len(lines) > 1:\n          if section_name.lower() in lines[0].lower():\n            return section\n\n    def get_answers_from_text_list(self, question, text_list, max_tokens=512):\n      tokenizer = self.tokenizer\n      model = self.model\n      inputs = tokenizer.batch_encode_plus(\n          [(question, text) for text in text_list], add_special_tokens=True, return_tensors='tf',\n          max_length=max_tokens, truncation_strategy='only_second', pad_to_max_length=True)\n      input_ids = inputs['input_ids'].numpy()\n      answer_start_scores, answer_end_scores = model(inputs)\n      answer_start = tf.argmax(\n          answer_start_scores, axis=1\n      ).numpy()  # Get the most likely beginning of each answer with the argmax of the score\n      answer_end = (\n          tf.argmax(answer_end_scores, axis=1) + 1\n      ).numpy()  # Get the most likely end of each answer with the argmax of the score\n\n      answers = []\n      for i, text in enumerate(text_list):\n        input_text = tokenizer.decode(input_ids[i, :], clean_up_tokenization_spaces=True)\n        input_text = input_text.split('[SEP] ', 2)[1]\n        answer = tokenizer.decode(\n            input_ids[i, answer_start[i]:answer_end[i]], clean_up_tokenization_spaces=True)\n        score_start = answer_start_scores.numpy()[i][answer_start[i]]\n        score_end = answer_end_scores.numpy()[i][answer_end[i]-1]\n        if answer and not '[CLS]' in answer:\n          answers.append(Answer(answer, score_start, score_end, input_text))\n        else:\n          answers.append(None)\n      return answers\n    \n\nclass Retrieval(abc.ABC):\n  \"\"\"Base class for retrieval methods.\"\"\"\n\n  def __init__(self, docs, keys=None):\n    \"\"\"\n    Args:\n      docs: a pd.Series of strings. The text to retrieve.\n      keys: a pd.Series. Keys (e.g. ID, title) associated with each document.\n    \"\"\"\n    self._docs = docs.copy()\n    if keys is not None:\n      self._docs.index = keys\n    self._model = None\n    self._doc_vecs = None\n\n  def _top_documents(self, q_vec, top_n=10):\n    similarity = cosine_similarity(self._doc_vecs, q_vec)\n    rankings = np.argsort(np.squeeze(similarity))[::-1]\n    ranked_indices = self._docs.index[rankings]\n    return self._docs[ranked_indices][:top_n]\n\n  @abc.abstractmethod\n  def retrieve(self, query, top_n=10):\n    pass\n\nclass TFIDFRetrieval(Retrieval):\n  \"\"\"Retrieve documents based on cosine similarity of TF-IDF vectors with query.\"\"\"\n\n  def __init__(self, docs, keys=None):\n    \"\"\"\n    Args:\n      docs: a list or pd.Series of strings. The text to retrieve.\n      keys: a list or pd.Series. Keys (e.g. ID, title) associated with each document.\n    \"\"\"\n    super(TFIDFRetrieval, self).__init__(docs, keys)\n    self._model = TfidfVectorizer()\n    self._doc_vecs = self._model.fit_transform(docs)\n\n  def retrieve(self, query, top_n=10):\n    q_vec = self._model.transform([query])\n    return self._top_documents(q_vec, top_n)\n\n@attr.s\nclass Answer(object):\n    text = attr.ib()\n    start_score = attr.ib()\n    end_score = attr.ib()\n    input_text = attr.ib()\n    \nstyle = '''\n<style>\n.hilight {\n  background-color:#cceeff;\n}\na {\n  color: #000 !important;\n  text-decoration: underline;\n}\n.question {\n  font-size: 20px;\n  font-style: italic;\n  margin: 10px 0;\n}\n.info {\n  padding: 10px 0;\n}\ntable.dataframe {\n  max-height: 450px;\n  text-align: left;\n}\n.meta {\n  margin-top: 10px;\n}\n.journal {\n  color: green;\n}\n.footer {\n  position: absolute;\n  bottom: 20px;\n  left: 20px;\n}\n<\/style>\n'''\n\ndef format_context(row):\n  text = row.context\n  answer = row.answer\n  highlight_start = text.find(answer)\n\n  def find_context_start(text):\n    idx = len(text) - 1\n    while idx >= 2:\n      if text[idx].isupper() and re.match(r'\\W ', text[idx - 2:idx]):\n        return idx\n      idx -= 1\n    return 0 \n  context_start = find_context_start(text[:highlight_start])\n  highlight_end = highlight_start + len(answer)\n  context_html = (text[context_start:highlight_start] + '<span class=hilight>' + \n                  text[highlight_start:highlight_end] + '<\/span>' + \n                  text[highlight_end:highlight_end + 1 + text[highlight_end:].find('. ')])\n  context_html += f'<br><br>score: {row.start_score:.2f}'\n  return context_html\n\n\ndef format_author(authors):\n  if not authors or not isinstance(authors, str):\n    return 'Unknown Authors'\n  name = authors.split(';')[0]\n  name = name.split(',')[0]\n  return name + ' et al'\n\ndef format_info(row):\n  meta = []\n  authors = format_author(row.authors) \n  if authors:\n    meta.append(authors)\n  meta.append(row.publish_time)\n  meta = ', '.join(meta)\n \n  html = f'''\\\n  <a class=\"title\" target=_blank href=\"http:\/\/doi.org\/{row.doi}\">{row.title}<\/a>\\\n  <div class=\"meta\">{meta}<\/div>\\\n  '''\n\n  journal = row.journal\n  if journal and isinstance(journal, str):\n    html += f'<div class=\"journal\">{journal}<\/div>'\n\n  return html\n\ndef render_results(main_question, answers):\n  id = main_question[:20].replace(' ', '_')\n  html = f'<h1 id=\"{id}\" style=\"font-size:20px;\">{main_question}<\/h1>'\n  for q, a in answers.items():\n    # TODO: skipping empty answers. Maybe we should show\n    # top retrieved docs.\n    if a.empty:\n      continue\n    # clean up question\n    if '?' in q:\n        q = q.split('?')[0] + '?'\n    html += f'<div class=question>{q}<\/div>' + format_answers(a)\n  display(HTML(style + html))\n\ndef format_answers(a):\n    a = a.sort_values('start_score', ascending=False)\n    a.drop_duplicates('doi', inplace=True)\n    out = []\n    for i, row in a.iterrows():\n        if row.start_score < 0:\n            continue\n        info = format_info(row)\n        context = format_context(row)\n\n        cohort = ''\n        if not np.isnan(row.cohort_size):\n            cohort = int(row.cohort_size)\n        temp=df[df['doi']==row.doi]\n        text = temp['body_text']\n        summ=summarizer(str(text), max_length=1000,   min_length=30)\n        out.append([context, info,summ])\n    out = pd.DataFrame(out, columns=['answer', 'article','summ'])\n    return out.to_html(escape=False, index=False)\n\ndef render_answers(a):\n    display(HTML(style + format_answers(a)))","c07d4ab5":"from transformers import pipeline\nsummarizer = pipeline('summarization')","e7b24ef1":"model1 = TFAutoModelForQuestionAnswering.from_pretrained(model_path, from_pt=True)\ntf.saved_model.save(model1, '\/kaggle\/working\/')","0af1ed46":"qa = ResearchQA(data_path, model_path)","faa2eac7":"answers = qa.get_answers('What drugs are effective?',max_articles=5)\nrender_answers(answers)","5f92e0f8":"answers = qa.get_answers('What kind of cytokines play a major role in host response?',max_articles=5)\nrender_answers(answers)","ac121474":"# Covid-19 Data mining tool\n\n\n**Overview**\n\nWe present how to answer the growing questions in the COVID-19 to enable researchers around the world to benefit from this tool to easily get relevant answers to their questions along with the relevant papers from the CORD-19 Open Research Dataset of Covid19 literature given by Allen Institute for AI. \n\nOur goal is to produce a smart literature review where we use question-answering to find the relevant answers in the document set as well as the evidence for each answer. We've used the COVID-19 Open Research Dataset Challenge,a pretrained BioBERT model fine-tuned on SQuAD 2.0 as QnA model.We believe a good literature review should include automated machine learning and domain technology. Our final approach is expected to give auto summarisation of the relevant papers and to enable speech recognition for more ease.\n\n**Methodology**\n\nDataset used: CORD-19 (Kaggle Competition data)\n* Filter a set of CORD-19 documents in the list associated with COVID-19 (keyword) and save this meta information in covid19.csv.\n* Create a retrieval system using the TF-IDF similarity measure to identify top N documents from the corpus based on similarity score with the question being asked.\n* At last, using the QnA model BioBERT model fine tuned using SQUAD 2.0 dataset to get relevant answer within each document and highlight the answer in each candidate document.\n\n**Q&A Model Background**\n\nBERT (Devlin et al. 2018) is a verbal model of learning that has been learned from a multimodal imaging model of the guirectional Transformer (Vaswani et al. 2017). After naming Corpora for a common natural language, BERT can be easily configured with many lower NLP functions, achieving maximum performance using very small amounts of data. Recent work has shown significant improvements in various functions using similar BERT or Transformer models, and BERT variations have been modified in certain domains by pretending to be for a specific company - e.g. BioBERT (Lee et al. 2019) promoted BERT pretending to be a professional book company from PubMed.\nThe general function of the NLP standard is to answer the answers to the questions: given the question and the related role, the model reads the output quoted in the paragraph that best answers the question. The standard measurement data for this function is SQuAD. Notably, in the latest model (SQuAD 2.0, Rajpurkar et al. 2018), approximately \u2153 of questions in the data cannot be intentionally inaccessible, so that top models should learn to answer when given insufficient evidence. (We found that fine-tuning this data improved the quality of our responses compared to SQuAD 1.1.)\n\n**Implementation**\n\nGiven the biomedical content of the CORD-19 corpus, and the sparse, uncertain nature of question answering with this dataset (i.e. most documents do not contain good answers to most questions), our Q&A system is thus powered by a pretrained BioBERT model fine-tuned for extractive question answering using SQuAD 2.0.\n\n* Firstly, extract meta information of all articles related to covid 19 search keywords(Covid19, SARS COV2 etc.) from the CORD-19 open research dataset in a csv file.\n* Based on TF-IDF similarity score of question with the document's title, abstract and text. we select top N candidate articles based on the similarity score.\n* We then used the fine tuned(BioBERT's Transfer learning on SQUAD2.0) model to get the answer for that question from these top N relevant articles.\n\n\nWe use the Huggingface Transformers library for fine tuning with our Q&A model. As shown in this [link](https:\/\/github.com\/huggingface\/transformers\/tree\/master\/examples\/question-answering), how to fine tune BERT model on SQUAD dataset. But we've used BiOBERT model instead of BERT model as BioBERT is found to be better than The model checkpoint is included with the submission and can be loaded below. To reproduce this checkpoint, use the run_squad.py script included in the Huggingface Transformers examples with the following command (takes ~8 hours on a GTX 1080):\n```\npython run_squad.py \\\n  --model_type bert \\\n  --model_name_or_path monologg\/biobert_v1.1_pubmed \\\n  --do_train \\\n  --do_eval \\\n  --train_file SQUAD_DIR\/train-v2.0.json \\\n  --predict_file SQUAD_DIR\/dev-v2.0.json \\\n  --per_gpu_train_batch_size 8 \\\n  --learning_rate 3e-5 \\\n  --num_train_epochs 4 \\\n  --max_seq_length 384 \\\n  --doc_stride 128 \\\n  --output_dir \/tmp\/biobert_squad2\/ \\\n  --version_2_with_negative\n ```\nTo improve accuracy and relevance of the results, we pre-filter the corpus using a list of keywords related to COVID-19, as shown in this kernel. At query time, the top candidate documents are automatically retrieved based on TF-IDF cosine similarity with the query, with the option to manually filter using only keywords if desired. By default, we extract answers from the abstract, discussion, and conclusion sections. Answers are ranked based on the model's start score of the answer span. \n\n\n**Discussion**\n\nA literature review, while taking time, is probably the most important step in understanding a particular research topic or question. With this epidemic, the number of books produced was enormous, and dumping this information is almost impossible.\nBy using NLP methods, we are able to automatically filter them with a lot of noise and precisely place relevant information. Using a streamlined process, our Q&A model can easily be updated to include new publications as they are published, giving researchers easy access to the most innovative results. In addition, the questions we present here are made with the needs of clinical users in mind, and are broad enough to recur in future disease attacks.\nIn the future, we would like to improve our program by adding summary skills to integrate results across topics and extract additional context for each article, including credibility or evidence-based evidence. These are open issues that challenge the NLP. The courses presented should be highly reviewed by experts, and consensus should be agreed upon between health care workers and the public.\n\n**Acknowledgements**\n**\nThis notebook is inspired and reproduced from the Google Health medical records research team (https:\/\/github.com\/Google-Health\/records-research) and by other Kagglers.\n\n\n**Future Work**\n\n1. Integrating this search tool with Speech recognition APIs to ease the speed with which researchers can interact with this feature.\n1. A Whatsapp chatbot can be designed to provide it's access to everyone right from their Mobile phones.\n\n\n"}}