{"cell_type":{"87cede66":"code","ae9054f7":"code","de9cfe45":"code","4258dfce":"code","c0df858e":"code","3d4ac2d6":"code","364c542b":"code","061f53b7":"code","988928a0":"code","cd475f6e":"code","0bc3dc89":"code","49fc18c4":"code","e0b43a3c":"code","4ba50a33":"code","bbba76b3":"code","3678dd5c":"code","f61279a5":"code","c994f65f":"code","3a59364b":"code","8669b4c7":"code","65e60e93":"code","595faca2":"code","405208f8":"code","3c8461da":"code","e82ec1ab":"code","1cdc5635":"code","f0fe137c":"code","578ee659":"code","d5f6d91f":"code","8a9ce641":"code","c5fd1958":"code","5fb90a8d":"code","83047f3f":"code","e50381a2":"code","1b54bf11":"code","90a5362f":"code","59d26536":"code","292f8355":"code","e09c38b8":"code","09bf48c8":"code","8cf8368d":"code","a7df8141":"markdown","9246c85f":"markdown","9a6d788a":"markdown","b845916d":"markdown","6c886dac":"markdown","e3422947":"markdown"},"source":{"87cede66":"import os\nimport cv2\nimport re\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import preprocessing\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","ae9054f7":"os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'","de9cfe45":"input_image_path='\/kaggle\/input\/flickr-image-dataset\/flickr30k_images\/flickr30k_images'\ninput_df=pd.read_csv('\/kaggle\/input\/flickr-image-dataset\/flickr30k_images\/results.csv',on_bad_lines='skip')\nimage_size=(128,128)\nlatent_dim=256\nepochs=20\ninput_df.head()","4258dfce":"def split_dataframe(df,samples=55000):\n    image_comment_table={}\n    print('Making the image comment table')\n    num_samples=min(samples,df.shape[0])\n    for col in df.columns:\n        for sample_index in tqdm(range(num_samples)):\n            text=df[col].iloc[sample_index].split('| ')\n            image=text[0]\n            comment=text[-1]\n            if not image in image_comment_table:\n                image_comment_table[image]=comment\n    print('Making the list of images')\n    image_names=[image for image,_ in tqdm(image_comment_table.items())]\n    print('Making the list of comments')\n    comments=[comment for _,comment in tqdm(image_comment_table.items())]\n    return image_names,comments","c0df858e":"image_names,comments=split_dataframe(df=input_df)\ndel input_df","3d4ac2d6":"print(image_names[:5])","364c542b":"def make_image_list(image_path,image_names,image_size=(128,128)):\n    images=[]\n    for image_name in tqdm(image_names):\n        image_filepath=os.path.join(image_path,image_name)\n        image=cv2.imread(image_filepath)\n        image=cv2.resize(image,image_size)\n        images.append(image\/255.)\n    return images","061f53b7":"images=make_image_list(\n    image_path=input_image_path,image_names=image_names,\n    image_size=image_size\n)\ndel image_names","988928a0":"def plot_images(images,row,col):\n    fig=plt.figure(figsize=(16,16))\n    for i in range(len(images)):\n        fig.add_subplot(row,col,i+1)\n        plt.axis('off')\n        plt.imshow(images[i])\n    plt.tight_layout()\n    plt.show()","cd475f6e":"plot_images(images[:4],row=2,col=2)","0bc3dc89":"print(comments[:5])","49fc18c4":"def generate_text_with_start_and_end_tokens(text_list):\n    char_set=set()\n    input_sequence_length=0\n    text_data=[]\n    for text in tqdm(text_list):\n        text=text.lower()\n        text=re.sub('[^a-z0-9\\s]','',text)\n        text='\\t'+text+'\\n'\n        text_data.append(text)\n        for char in text:\n            if not char in char_set:\n                char_set.add(char)\n        input_sequence_length=max(input_sequence_length,len(text))\n    char_set=sorted(list(char_set))\n    return text_data,char_set,input_sequence_length","e0b43a3c":"text_data,char_set,input_sequence_length=generate_text_with_start_and_end_tokens(comments)\ndel comments","4ba50a33":"print(char_set)","bbba76b3":"chars_to_tokens={char:num for num,char in enumerate(char_set)}\nprint(chars_to_tokens)","3678dd5c":"tokens_to_chars={num:char for char,num in chars_to_tokens.items()}\nprint(tokens_to_chars)","f61279a5":"print(input_sequence_length)","c994f65f":"vocab_size=len(char_set)\nnum_comments=len(text_data)\nprint(f'Number of comments: {num_comments}')\nprint(f'Number of characters: {vocab_size}')","3a59364b":"def one_hot_encode_text(text_list):\n    one_input_hot_text=np.zeros(shape=(num_comments,input_sequence_length,vocab_size))\n    one_output_hot_text=np.zeros(shape=(num_comments,input_sequence_length,vocab_size))\n    for text_index,text in enumerate(text_list):\n        for char_index,char in enumerate(text):\n            one_input_hot_text[text_index,char_index,chars_to_tokens[char]]=1\n        one_input_hot_text[text_index,char_index+1:,chars_to_tokens[' ']]=1\n        for char_index,char in enumerate(text):\n            if char_index>1:\n                one_output_hot_text[text_index,char_index-1,chars_to_tokens[char]]=1\n        one_output_hot_text[text_index,char_index:,chars_to_tokens[' ']]=1\n    return one_input_hot_text,one_output_hot_text","8669b4c7":"one_input_hot_text,one_output_hot_text=one_hot_encode_text(text_data)\ndel text_data","65e60e93":"def decode_one_hot_text(text):\n    ans=''\n    for position in text[1:-1]:\n        char_index=np.argmax(position,axis=-1)\n        ans+=tokens_to_chars[char_index]\n    return ans","595faca2":"for encoded_text in one_input_hot_text[:5]:\n    print(decode_one_hot_text(encoded_text))","405208f8":"def permute_indices(length,split_pct=70):\n    idxs=np.random.permutation(length)\n    train_idxs=idxs[:length*split_pct\/\/100]\n    valid_idxs=idxs[length*split_pct\/\/100:]\n    return train_idxs,valid_idxs","3c8461da":"def delete_idxs(data,idxs):\n    print('Deleting.......')\n    for idx in tqdm(idxs):\n        del data[idx]","e82ec1ab":"def make_dataset(images,input_comments,target_comments,idxs):\n    image_data=[images[idx] for idx in tqdm(idxs)]\n    delete_idxs(image,idxs)\n    input_comments_data=[input_comments[idx] for idx in tqdm(idxs)]\n    delete_idxs(input_comments_data,idxs)\n    target_comments_data=[target_comments[idx] for idx in tqdm(idxs)]\n    delete_idxs(target_comments_data,idxs)\n    image_data=tf.convert_to_tensor(image_data)\n    input_comments_data=tf.convert_to_tensor(input_comments_data)\n    target_comments_data=tf.convert_to_tensor(target_comments_data)\n    return image_data,input_comments_data,target_comments_data","1cdc5635":"train_idxs,valid_idxs=permute_indices(num_comments,split_pct=80)","f0fe137c":"train_image,train_input_comments,train_target_comments=make_dataset(\n    images=images,input_comments=one_input_hot_text,\n    target_comments=one_output_hot_text,idxs=train_idxs\n)","578ee659":"valid_image,valid_input_comments,valid_target_comments=make_dataset(\n    images=images,input_comments=one_input_hot_text,\n    target_comments=one_output_hot_text,idxs=valid_idxs\n)","d5f6d91f":"del images,one_input_hot_text,one_output_hot_text","8a9ce641":"class ConvStack(layers.Layer):\n    def __init__(self,filters,n_conv=4,kernel_size=3,strides=1,**kwargs):\n        super().__init__(**kwargs)\n        self.model=[]\n        for _ in range(n_conv):\n            self.model.append(layers.Conv2D(\n                filters=filters,kernel_size=kernel_size,\n                strides=strides,padding='same',activation='relu'\n            ))\n            self.model.append(layers.BatchNormalization())\n    def call(self,inputs):\n        output=inputs\n        for hidden_layer in self.model:\n            output=hidden_layer(output)\n        return tf.keras.activations.relu(output)","c5fd1958":"class SE_box(layers.Layer):\n    def __init__(self,filters,ratio=4,**kwargs):\n        super().__init__(**kwargs)\n        self.model=[]\n        self.model.append(layers.GlobalAvgPool2D())\n        self.model.append(layers.Dense(units=filters\/\/ratio,activation='relu'))\n        self.model.append(layers.Dense(units=filters,activation='sigmoid'))\n    def call(self,inputs):\n        output=inputs\n        for hidden_layer in self.model:\n            output=hidden_layer(output)\n        output=layers.Multiply()([inputs,output])\n        return tf.keras.activations.relu(output)","5fb90a8d":"class SE_resnet(layers.Layer):\n    def __init__(self,filters,n_conv=4,kernel_size=3,strides=1,ratio=4,**kwargs):\n        super().__init__(**kwargs)\n        self.model=[]\n        self.model.append(ConvStack(\n            filters=filters,n_conv=n_conv,\n            kernel_size=kernel_size,strides=strides\n        ))\n        self.model.append(SE_box(\n            filters=filters,ratio=ratio\n        ))\n    def call(self,inputs):\n        output=inputs\n        for hidden_layer in self.model:\n            output=hidden_layer(output)\n        output=layers.Concatenate()([output,inputs])\n        return tf.keras.activations.relu(output)","83047f3f":"#decoder model\ndecoder_model_inputs=layers.Input(shape=(None,vocab_size))\ndecoder_lstm1=layers.LSTM(latent_dim,return_state=True,return_sequences=True)\ndecoder_output,hidden_state,cell_state=decoder_lstm1(decoder_model_inputs)\ninit_state=[hidden_state,cell_state]\ndecoder_lstm2=layers.LSTM(latent_dim,return_state=True,return_sequences=True)\ndecoder_output,hidden_state,cell_state=decoder_lstm2(decoder_output,initial_state=init_state)\n\n#CNN\nencoder_model_inputs=layers.Input(shape=(*image_size,3))\nfilters,n_conv,ratio=8,4,4\nencoder_model_hidden_layer=SE_resnet(\n    filters=filters,n_conv=n_conv,ratio=ratio\n)(encoder_model_inputs)\nencoder_model_hidden_layer=layers.AvgPool2D(pool_size=2,strides=2)(encoder_model_hidden_layer)\nfor _ in range(int(np.log2(image_size[0]))-1):\n    filters*=2\n    encoder_model_hidden_layer=SE_resnet(\n        filters=filters,n_conv=n_conv,ratio=ratio\n    )(encoder_model_hidden_layer)\n    encoder_model_hidden_layer=layers.AvgPool2D(\n        pool_size=2,strides=2\n    )(encoder_model_hidden_layer)\nencoder_output_layer=layers.Dense(units=latent_dim,activation='relu')\nencoder_model_output=encoder_output_layer(encoder_model_hidden_layer)\nencoder_model_output=tf.squeeze(encoder_model_output,[1])\n\n#Combining both their outputs\nfinal_inputs=layers.Add()([decoder_output,cell_state,encoder_model_output])\noutput_layer=layers.Dense(units=vocab_size,activation='relu')\ndecoder_output_=output_layer(final_inputs)\ndecoder_output_=tf.nn.softmax(decoder_output_)\n\n#Putting the model together\nmodel=tf.keras.models.Model(inputs=(encoder_model_inputs,decoder_model_inputs),outputs=decoder_output_)","e50381a2":"model.summary()","1b54bf11":"model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])","90a5362f":"history=model.fit(\n    x=(train_image,train_input_comments),\n    y=train_target_comments,\n    validation_data=((valid_image,valid_input_comments),valid_target_comments),\n    epochs=epochs,\n    batch_size=20,\n)","59d26536":"encoder_model=tf.keras.models.Model(inputs=encoder_model_inputs,outputs=encoder_model_output)","292f8355":"def generate_words(image):\n    caption=''\n    decoder_input=np.zeros(shape=(1,1,vocab_size))\n    decoder_input[:,:,chars_to_tokens['\\t']]=1\n    encoder_output=encoder_model.predict(image)\n    init_state=None\n    while True:\n        if init_state is not None:\n            infer_lstm_output1,hidden_state1,cell_state1=decoder_lstm1(decoder_input,initial_state=init_state)\n        else:\n            infer_lstm_output1,hidden_state1,cell_state1=decoder_lstm1(decoder_input)\n        infer_lstm_output2,hidden_state2,cell_state2=decoder_lstm2(\n            infer_lstm_output1,initial_state=[hidden_state1,cell_state1])\n        final_infer_inputs=layers.Add()([infer_lstm_output2,cell_state2,encoder_output])\n        infer_decoder_output=output_layer(final_infer_inputs)\n        infer_decoder_output=tf.nn.softmax(infer_decoder_output)\n        caption_token=int(np.argmax(infer_decoder_output,axis=-1))\n        caption_char=tokens_to_chars[caption_token]\n        if len(caption)==input_sequence_length or caption_char=='\\n':\n            break\n        caption+=caption_char\n        decoder_input=np.zeros(shape=(1,1,vocab_size))\n        decoder_input[:,:,caption_token]=1\n        init_state=[hidden_state2,cell_state2]\n    return caption","e09c38b8":"def decode_text(text):\n    ans=''\n    for position in text[1:-1]:\n        char_index=np.argmax(position,axis=-1)\n        ans+=tokens_to_chars[char_index]\n    return ans","09bf48c8":"def plot_image(image):\n    plt.figure(figsize=(8,8))\n    plt.imshow(image)\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()","8cf8368d":"for valid_input_image,target_comment in zip(train_image[:1],train_target_comments[:1]):\n    for _ in range(100):\n        print('-',end='')\n    print()\n    plot_image(valid_input_image)\n    valid_input_image=tf.reshape(valid_input_image,shape=(1,*valid_input_image.shape))\n    caption=generate_words(valid_input_image)\n    print(f'The caption generated by the model is: {caption}')\n    print(f'The actual caption of is: {decode_text(target_comment)}')","a7df8141":"# Generating text\n(Improvement is required here and I would really appreciate some tips, the problem is that my Character level Image Captioning system is able to give a good performance on the validation set while calling the fit method on the model but fails to generate text that is even comprehensible. I personally feel that the source of the problem is the function which I have used to generate the text or the fact that length of the captions in the data is way too long for an LSTM to perform well(in which case I have no option but to truncate the captions), but I am not able to troubleshoot it.)","9246c85f":"# Preprocessing image data","9a6d788a":"# Training the model","b845916d":"## Defining the models","6c886dac":"# Making the datasets","e3422947":"# Preprocessing text data"}}