{"cell_type":{"876c0a0f":"code","d53ac818":"code","184d1943":"code","57212214":"code","ef294235":"code","b4db6def":"code","287ff207":"code","f522f504":"code","571ad1de":"code","b7206ece":"code","ab067d13":"code","cb2800ec":"code","7e5048a8":"code","6540fe55":"code","e6df09b2":"code","5d842a1c":"code","f6c307c3":"code","827d395e":"code","b45cd867":"code","c311cdad":"code","bb0763fb":"code","08c1cd50":"code","0ecd2b3e":"code","bac9a37c":"code","4be6794e":"code","f4c03026":"code","b9bd8581":"code","21e38e16":"code","00eb41d4":"code","d92c0426":"code","166cf6f2":"code","0410fb8c":"code","24a1448f":"code","0c6cd3b3":"code","89af1b34":"markdown","47ec1070":"markdown","8367e4f7":"markdown","0ccdda7d":"markdown","e264dd8c":"markdown","222464db":"markdown","8ce35f47":"markdown","3800de5f":"markdown","a8b5fae5":"markdown","16d25286":"markdown","8996ac92":"markdown","54d93fbe":"markdown","a535097d":"markdown","6b8c094c":"markdown","9bbb1f38":"markdown","56ebd44d":"markdown","22d443ee":"markdown","0f3951ed":"markdown","1c1a06e5":"markdown","4534f6ca":"markdown","57c3a88a":"markdown","1af45217":"markdown","b3ea695c":"markdown","08b925ea":"markdown","96dc3e05":"markdown","e753c975":"markdown","e2f459f2":"markdown","2b8b15b0":"markdown","6dd3198e":"markdown","1e58b5ae":"markdown","a38083c6":"markdown","4106b234":"markdown","48df37a3":"markdown","f6270f60":"markdown","6415c524":"markdown","299e94fc":"markdown","64b79f9d":"markdown","81f3888c":"markdown"},"source":{"876c0a0f":"import os\nimport datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, explained_variance_score, accuracy_score, roc_auc_score","d53ac818":"# Input data files are available in the read-only \"..\/input\/\" directory\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n# Read train.csv as Pandas Dataframe\ndf_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n\n# Read test.csv as Pandas Dataframe\ndf_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\n# Create series of PassengerId, used for recreating submission format\nseries_passenger_id = df_test['PassengerId']\n\ndisplay(df_train.head())","184d1943":"df_train.drop(labels='PassengerId', axis=1, inplace=True)\ndf_test.drop(labels='PassengerId', axis=1, inplace=True)","57212214":"# Print features for which values are NaN\nfor col in df_train.columns:\n      n_missing = df_train[col].isnull().sum()\n      if n_missing > 0:\n            print(f'df_train[{col}] contains #{n_missing} missing values!')","ef294235":"df_train.dropna(subset=['Embarked'], inplace=True)  # Embarked contains only 2 rows with missing values\ndf_test.dropna(subset=['Embarked'], inplace=True)","b4db6def":"surv = sum(df_train['Survived'])\nsurv_women = df_train.loc[df_train.Sex == 'female'][\"Survived\"]\nsurv_men = df_train.loc[df_train.Sex == 'male'][\"Survived\"]\n\n# Target distribution (y = survival)\nprint(f'{surv} from the {len(df_train)} training observations survived the Titanic, indicating a survival rate of {surv \/ len(df_train):.2%}')","287ff207":"# Feature-Target Distribution\nprint(df_train.dtypes)","f522f504":"# Continuous Features\ncol_cont = df_train.select_dtypes(include='float64')\n\nfor col in col_cont:\n    q = pd.qcut(df_train[col], 10)\n    print(df_train.pivot_table('Survived', index=q), end='\\n\\n')","571ad1de":"def pivot_cat(df, cols):\n    \"\"\"Print Pivot for categorical features in relation to 'Survived'.\"\"\"\n    for col in cols:\n        if df[col].nunique() < 32:  # only plot categorical features with less than 32 unique values\n            pivot = pd.crosstab(df['Survived'], df[col], values='Ticket', aggfunc='count', normalize='columns')\n            print(f'{pivot}', end='\\n\\n')\n        else:\n            print(f'No pivot created as #{df[col].nunique()} unique values retrieved for Categorical Feature: {col}', end='\\n\\n')\n\n# Categorical Features\ncol_cat = df_train.select_dtypes(exclude='float64')\n\nprint(f'{surv} from the {len(df_train)} training observations survived the Titanic, from which {sum(surv_women)}'\n      f' are female ({sum(surv_women) \/ surv:.2%}), and {sum(surv_men)} are male ({sum(surv_men) \/ surv:.2%})')\nprint(\n    f'From the total number of woman {sum(surv_women) \/ len(surv_women):.2%} survived, and from the total number of men'\n    f' {sum(surv_men) \/ len(surv_men):.2%} survived the Titanic', end='\\n\\n')\n\npivot_cat(df_train, col_cat)  # print the survival rate per categorical feature category","b7206ece":"def plot_corr(df, cols=None):\n    \"\"\"Plot Heatmap for numerical features.\"\"\"\n    sns.set_style(\"whitegrid\")\n    fig, ax = plt.subplots(figsize=(11.7, 8.27))\n    sns.heatmap(ax=ax, data=df.corr().round(2), annot=True, cmap='RdYlGn', vmin=-1, vmax=1, center=0, linewidths=.5)\n    ax.set_title(f'Heatmap of numerical features:')\n\nplot_corr(df_train)","ab067d13":"def plot_hist(df, cols):\n    \"\"\"Plot Histogram for continuous features.\"\"\"\n    sns.set_style(\"whitegrid\")\n    for col in cols:\n        fig, ax = plt.subplots(figsize=(10, 10))\n        sns.histplot(x=col, data=df, kde=True, hue=\"Survived\", element=\"step\", bins=20,\n                    palette={0: \"darkred\", 1: \"lightgreen\"}, ax=ax)\n        ax.set_title(f'Histogram continuous feature: {col}')\n        ax.set_xlabel(col)\n        ax.set_ylabel('Frequency')\n\nplot_hist(df_train, col_cont)","cb2800ec":"def plot_count(df, cols):\n    \"\"\"Plot Countplot for categorical features.\"\"\"\n    sns.set_style(\"whitegrid\")\n    for col in cols:\n        if df[col].nunique() < 32:  # only plot categorical features with less than 32 unique values\n            fig, ax = plt.subplots(figsize=(10, 10))\n            sns.countplot(x=col, hue='Survived', data=df, palette={0: \"darkred\", 1: \"lightgreen\"}, ax=ax)\n            ax.set_title(f'Countplot categorical feature: {col}')\n            ax.set_xlabel(col)\n            ax.set_ylabel('Frequency')\n\n        else:\n            print(f'No plot saved as #{df[col].nunique()} unique values were retrieved for Categorical Feature: {col}')\n\nplot_count(df_train, col_cat)","7e5048a8":"def imp_age(df):\n    \"\"\"Missing values in Age are filled with median age per Sex\/Pclass group due to high correlation\"\"\"\n    df_pivot = df.groupby(['Sex', 'Pclass']).median()['Age']\n    df['Age'] = df.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\n    for index, value in df_pivot.iteritems():\n        print(f'Imputed Sex\/Pclass: {index} - Age: {value}')\n    return df\n\n\ndef imp_fare(df):\n    \"\"\"Missing values in Fare are filled with mean fare per Pclass group due to high correlation\"\"\"\n    df_pivot = df.groupby(['Pclass']).mean()['Fare']\n    df['Fare'] = df.groupby(['Pclass'])['Fare'].apply(lambda x: x.fillna(x.mean()))\n    for index, value in df_pivot.iteritems():\n        print(f'Imputed Pclass: {index} - Fare: {value}')\n    return df\n\n\ndf_train = imp_age(df_train)  # Impute median age based on Sex\/Pclass groups\ndf_test = imp_age(df_test)\n\ndf_train = imp_fare(df_train)  # Impute mean fare based on Pclass groups\ndf_test = imp_fare(df_test)","6540fe55":"# Creating new features from arithmetics\ndf_train['Family_Size'] = 1 + df_train['SibSp'] + df_train['Parch']\ndf_test['Family_Size'] = 1 + df_test['SibSp'] + df_test['Parch']\n\ndf_train['Cabin_n'] = df_train.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))  # Count Cabins, 0 is NaN\ndf_test['Cabin_n'] = df_test.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\n\ndf_train['Cabin_section'] = df_train.Cabin.apply(lambda x: str(x)[0])  # Retrieve section, first Cabin character\ndf_test['Cabin_section'] = df_test.Cabin.apply(lambda x: str(x)[0])\n\ndf_train['Name_title'] = df_train.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())  # Retrieve title from Name\ndf_test['Name_title'] = df_test.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n\ndf_train['freq_Ticket'] = df_train.groupby('Ticket')['Ticket'].transform('count')  # Indicates n people travelling with the same ticket\ndf_test['freq_Ticket'] = df_test.groupby('Ticket')['Ticket'].transform('count')","e6df09b2":"# Creating new features from Binning Continuous features\ndf_train['bin_Fare'] = pd.qcut(df_train['Fare'], 15)\ndf_test['bin_Fare'] = pd.qcut(df_test['Fare'], 15)\n\ndf_train['bin_Age'] = pd.qcut(df_train['Age'], 10)\ndf_test['bin_Age'] = pd.qcut(df_test['Age'], 10)","5d842a1c":"df_train['bin_Sex'] = df_train.Sex.map({'male': 0, 'female': 1})  # Map binary 'Sex'\ndf_test['bin_Sex'] = df_test.Sex.map({'male': 0, 'female': 1})","f6c307c3":"encoding = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Large', 6: 'Large', 7: 'Large', 8: 'Large',\n            9: 'Large', 10: 'Large', 11: 'Large', 12: 'Large'}  # Based on Countplot families of 2-4 persons are 'Small'\n\ndf_train[f'ord_Family_Size'] = df_train['Family_Size'].map(encoding)\ndf_test[f'ord_Family_Size'] = df_test['Family_Size'].map(encoding)","827d395e":"def enc_label(df, cols):\n    \"\"\"\"Returns the dataframe with label encoding of provided columns\"\"\"\n    enc = LabelEncoder()\n    for col in cols:\n        df[f'lab_{col}'] = enc.fit_transform(df[col])\n    return df\n\ncol_label = ['Embarked', 'bin_Fare', 'bin_Age', 'Cabin_section', 'Name_title', 'ord_Family_Size']\n\ndf_train = enc_label(df_train, col_label)\ndf_test = enc_label(df_test, col_label)","b45cd867":"def enc_1hot(df, col):\n    \"\"\"Returns the dataframe with 1 hot encoding of provided columns\"\"\"\n    df = pd.concat([df, pd.get_dummies(df[col], prefix= col)], axis=1)\n    return df\n\ndf_train = enc_1hot(df_train, 'Embarked')  # one-hot encode categorical feature 'Embarked'\ndf_test = enc_1hot(df_test, 'Embarked')\n\ndf_train = enc_1hot(df_train, 'Cabin_section')  # one-hot encode categorical feature 'Cabin_section'\ndf_test = enc_1hot(df_test, 'Cabin_section')","c311cdad":"df_train['norm_Fare'] = np.log(df_train['Fare'] + 1)","bb0763fb":"    plot_count(df_train, ['Family_Size'])  # Plot Creating new features from arithmetics\n    plot_count(df_train, ['Cabin_n'])  # Plot Creating new features from arithmetics\n    plot_count(df_train, ['Cabin_section'])  # Plot Creating new features from arithmetics\n    plot_count(df_train, ['Name_title'])  # Plot Creating new features from arithmetics\n    plot_count(df_train, ['bin_Fare'])  # Plot Creating new features from Binning Continuous features\n    plot_count(df_train, ['bin_Age'])  # Plot Creating new features from Binning Continuous features\n    plot_hist(df_train, ['norm_Fare'])  # Plot Normalizing","08c1cd50":"TUNE = False\nSEED = 1\nFOLDS = 5\n\ny = df_train['Survived']\n\nfeatures = df_test.select_dtypes(include=['float64', 'int64', 'int32', 'uint8']).columns  # Only select numerical features, excludes all encoded features of type 'object'\n\ndf_train.drop(['Survived'], axis=1, inplace=True)\nX = df_train[features]","0ecd2b3e":"def mi_score(X, y):\n    \"\"\"\"Mutual information (MI) between two random variables is a non-negative value, which measures the dependency\n        between the variables. It is equal to zero if and only if two random variables are independent, and higher\n        values mean higher dependency.\"\"\"\n    X = X.select_dtypes(include=['float64', 'int64', 'int32', 'uint8'])  # All features should now have numerical dtypes\n    mi_scores = mutual_info_regression(X, y, random_state=1)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    mi_scores = mi_scores.rename('mi_scores').reset_index()\n    return mi_scores\n\n\ndef plot_mi_scores(x, y):\n    \"\"\"\"Mutual information (MI) between two random variables is a non-negative value, which measures the dependency\n        between the variables. It is equal to zero if and only if two random variables are independent, and higher\n        values mean higher dependency.\"\"\"\n    sns.set_style(\"whitegrid\")\n    fig, ax = plt.subplots(figsize=(10, 10))\n    sns.barplot(ax=ax, x=x.values, y=y.values, orient='h', palette='crest')\n    # ax.yticks(width, ticks)\n    ax.set_title(\"Mutual Information Scores\")\n\n\nmi_scores = mi_score(X, y)\nplot_mi_scores(mi_scores['mi_scores'], mi_scores['index'])\n\n# Set threshold of MI score > 0.02 for a feature to be included\nfeature_scope = mi_scores[mi_scores['mi_scores']>0.02]['index']","bac9a37c":"display(mi_scores[mi_scores['mi_scores']>0.02])\n\nX_pred = df_test[feature_scope]\n\n# Setup train\/test data\nX_train, X_test, y_train, y_test = train_test_split(X[feature_scope], y, stratify=y, random_state=SEED)","4be6794e":"def model_cv_score(model, x, y, cv=FOLDS):\n    \"\"\"Model mean Cross Validation score\"\"\"\n    model_name = type(model).__name__\n    cv_score = cross_val_score(model, x, y, cv=cv)\n    print(f'{model_name} - #{cv} fold Cross Validation: Mean - {cv_score.mean():.2%} | Min - {np.min(cv_score):.2%} | Max - {np.max(cv_score):.2%}')\n\n\nbase_tree = DecisionTreeClassifier(random_state=SEED)\nbase_forest = RandomForestClassifier(random_state=SEED)\nbase_gbc = GradientBoostingClassifier(random_state=SEED)\nbase_svc = SVC(random_state=SEED)\nbase_knn = KNeighborsClassifier()\n\nbase_models = [base_tree, base_forest, base_gbc, base_svc, base_knn]\n\nfor model in base_models:\n    model_cv_score(model, X_train, y_train)","f4c03026":"def tune_tree(model, x, y, cv=FOLDS):\n    \"\"\"Tune parameter grid for sklearn.tree.DecisionTreeClassifier\"\"\"\n    param_grid = {'criterion': ['gini', 'entropy'],\n                  'max_depth': [15, 20, 25],\n                  'max_features': ['auto', 'sqrt', 0.5],\n                  'min_samples_leaf': [2, 3],\n                  'min_samples_split': [2, 3, 4]}\n    grid_search = GridSearchCV(model, param_grid=param_grid, cv=cv, verbose=True)\n    tuned_param_grid = grid_search.fit(x, y).best_params_\n    print(f'Tuned Parameters: {tuned_param_grid}')\n    return tuned_param_grid\n\nif TUNE:  # tunes parameters if True (set at Model chapter start)\n    param_grid = tune_tree(base_tree, X_train, y_train)\nelse:  # trains model based on previously tuned parameters to save computing time\n    param_grid = {'criterion': 'gini',\n                  'max_depth': 15,\n                  'max_features': 0.5,\n                  'min_samples_leaf': 3,\n                  'min_samples_split': 3}\n    print(f'Default Parameters: {param_grid}')\n\ntuned_tree = DecisionTreeClassifier(random_state=SEED).set_params(**param_grid)\nmodel_cv_score(tuned_tree, X_train, y_train)","b9bd8581":"def tune_forest(model, x, y, cv=FOLDS):\n    \"\"\"Tune parameter grid for sklearn.ensemble.GradientBoostingClassifier\"\"\"\n    param_grid = {'n_estimators': [250, 400, 500],\n                  'criterion': ['gini', 'entropy'],\n                  'bootstrap': [True],\n                  'max_depth': [15, 20, 25],\n                  'max_features': ['auto', 'sqrt', 10],\n                  'min_samples_leaf': [2, 3],\n                  'min_samples_split': [2, 3]}\n    grid_search = GridSearchCV(model, param_grid=param_grid, cv=cv, verbose=True)\n    tuned_param_grid = grid_search.fit(x, y).best_params_\n    print(f'Tuned Parameters: {tuned_param_grid}')\n    return tuned_param_grid\n\n\nif TUNE:  # tunes parameters if True (set at Model chapter start)\n    param_grid = tune_gbc(base_knn, X_train, y_train)\nelse:  # trains model based on previously tuned parameters to save computing time\n    param_grid = {'n_estimators': 250,\n                  'criterion': 'gini',\n                  'bootstrap': True,\n                  'max_depth': 15,\n                  'max_features': 10,\n                  'min_samples_leaf': 3,\n                  'min_samples_split': 3}\n    print(f'Default Parameters: {param_grid}')\n    \ntuned_forest = RandomForestClassifier(random_state=SEED).set_params(**param_grid)\n\nmodel_cv_score(tuned_forest, X_train, y_train)","21e38e16":"def tune_knn(model, x, y, cv=FOLDS):\n    \"\"\"Tune parameter grid for sklearn.ensemble.GradientBoostingClassifier\"\"\"\n    param_grid = {'n_neighbors' : [3,5,7,9],\n                  'weights' : ['uniform', 'distance'],\n                  'algorithm' : ['auto', 'ball_tree','kd_tree'],\n                  'p' : [1,2]}\n    grid_search = GridSearchCV(model, param_grid=param_grid, cv=cv, verbose=True)\n    tuned_param_grid = grid_search.fit(x, y).best_params_\n    print(f'Tuned Parameters: {tuned_param_grid}')\n    return tuned_param_grid\n\n\nif TUNE:  # tunes parameters if True (set at Model chapter start)\n    param_grid = tune_gbc(base_knn, X_train, y_train)\nelse:  # trains model based on previously tuned parameters to save computing time\n    param_grid = {'n_neighbors' : 7,\n                  'weights' : 'uniform',\n                  'algorithm' : 'auto',\n                  'p' : 2}\n    print(f'Default Parameters: {param_grid}')\n    \ntuned_knn = KNeighborsClassifier().set_params(**param_grid)\n\nmodel_cv_score(tuned_knn, X_train, y_train)","00eb41d4":"def tune_gbc(model, x, y, cv=FOLDS):\n    \"\"\"Tune parameter grid for sklearn.ensemble.GradientBoostingClassifier\"\"\"\n    param_grid = {'n_estimators': [100, 250],\n                  'max_depth': [15],\n                  'max_features': [10],\n                  'min_samples_leaf': [3],\n                  'min_samples_split': [3]}\n    grid_search = GridSearchCV(model, param_grid=param_grid, cv=cv, verbose=True)\n    tuned_param_grid = grid_search.fit(x, y).best_params_\n    print(f'Tuned Parameters: {tuned_param_grid}')\n    return tuned_param_grid\n\n\nif TUNE:  # tunes parameters if True (set at Model chapter start)\n    param_grid = tune_gbc(base_gbc, X_train, y_train)\nelse:  # trains model based on previously tuned parameters to save computing time\n    param_grid = {'n_estimators': 250,\n                  'max_depth': 15,\n                  'max_features': 'auto',\n                  'min_samples_leaf': 3,\n                  'min_samples_split': 3}\n    print(f'Default Parameters: {param_grid}')\n    \ntuned_gbc = GradientBoostingClassifier(random_state=SEED).set_params(**param_grid)\n\nmodel_cv_score(tuned_gbc, X_train, y_train)","d92c0426":"def tune_svc(model, x, y, cv=FOLDS):\n    \"\"\"Tune parameter grid for sklearn.svm.SupportVectorClassifier\"\"\"\n    param_grid = {'kernel': ['rbf'],\n                  'gamma': [.1, .5, 1, 2, 5, 10],\n                  'C': [.1, 1, 10, 100, 1000]}\n    grid_search = GridSearchCV(model, param_grid=param_grid, cv=cv, verbose=True)\n    tuned_param_grid = grid_search.fit(x, y).best_params_\n    print(f'Tuned Parameters: {tuned_param_grid}')\n    return tuned_param_grid\n\n\n# Tune SupportVectorClassifier()\nif TUNE:  # tunes parameters if True (set at Model chapter start)\n    param_grid = tune_gbc(base_gbc, X_train, y_train)\nelse:  # trains model based on previously tuned parameters to save computing time\n    param_grid = {'C': 1,\n                  'gamma': 0.1,\n                  'kernel': 'rbf'}\n    print(f'Default Parameters: {param_grid}')\n\ntuned_svc = SVC(probability=True, random_state=SEED).set_params(**param_grid)\n\nmodel_cv_score(tuned_svc, X_train, y_train)","166cf6f2":"tuned_models = [('Tree', tuned_tree), ('Forest', tuned_forest), ('GBC', tuned_gbc), ('KNN', tuned_knn), ('SVC', tuned_svc)]\n\nvoting_clf_hard = VotingClassifier(estimators=tuned_models, voting='hard')\nmodel_cv_score(voting_clf_hard, X_train, y_train)\n\nvoting_clf_soft = VotingClassifier(estimators=tuned_models, voting='soft')\nmodel_cv_score(voting_clf_soft, X_train, y_train)","0410fb8c":"ensemble_models = [voting_clf_hard, voting_clf_soft]\n\n# Fit the model\nfor model in ensemble_models:\n    model_name = type(model).__name__\n    \n    # Fit model\n    model.fit(X_train, y_train)\n\n    # Predict labels\n    y_predict = model.predict(X_test)\n\n    # Evaluate predict\n    print(f'Model {model_name} predict accuracy: {accuracy_score(y_test, y_predict):.2%}')  # Evaluate predict","24a1448f":"# Make predictions via selected model\nmodel = voting_clf_soft\n\ny_pred = model.predict(X_pred)","0c6cd3b3":"#convert output to dataframe \nfinal_data = {'PassengerId': series_passenger_id, 'Survived': y_pred}\nsubmission = pd.DataFrame(data=final_data)\n\n#prepare submission files \nsubmission.to_csv('submission.csv', index =False)\n\ndisplay(submission.head())","89af1b34":"# Feature engineering","47ec1070":"#### Missing values","8367e4f7":"#### Binary encoding","0ccdda7d":"##### Conclusions:\n- Feature correlation indicates relationships which may be used for creating new features\n- Feature correlation does not indicate alarming multicollinearity\n- Spikes in a distribution (f.i. 'Age') may be captured via a decision tree model\n- Ordinal relations (f.i. 'Pclass) may be captured via a linear model\n- Skewed features (f.i. Fare) may be normalized for capturing the relation towards the target","e264dd8c":"#### Source Data","222464db":"# Submission output","8ce35f47":"#### Model tuning","3800de5f":"#### Impute missing values","a8b5fae5":"#### New features from binning","16d25286":"##### Conclusions:\n- The Distribution of Age indicates that children (<16) have a higher survival rate\n- The Distribution of Fare indicates that a higher Fare (10.5+) indicates a higher survival rate","8996ac92":"#### Model specification","54d93fbe":"#### New features from arithmetics","a535097d":"#### Normalizing","6b8c094c":"#### Target distribution","9bbb1f38":"##### Conclusions:\n- The minority of persons in the training set survived the Titanic","56ebd44d":"#### Feature-Target distribution","22d443ee":"#### Categorical features","0f3951ed":"#### Fit-Predict tuned models","1c1a06e5":"##### Conclusions:\n- For 'Cabin' the majority of data points are missing, hence imputing data would probably not add value\n- For 'Age' and 'Fare' missing datapoints may be imputed\n- For 'Embarked' only 2 values are missing which will be dropped","4534f6ca":"#### Ensemble models","57c3a88a":"#### 1-Hot encoding","1af45217":"#### Continuous features","b3ea695c":"#### Label encoding","08b925ea":"# Data preprocessing","96dc3e05":"# Setup","e753c975":"#### Model data preprocessing","e2f459f2":"# Exploratory Data Analysis","2b8b15b0":"#### Predict","6dd3198e":"#### Feature engineering plots","1e58b5ae":"##### Conclusions:\n- 'PassengerId' is a mere row identifier which is dropped from the analysis","a38083c6":"# Model\n\nThe steps to building and using a model are:\n\nPrepare: Preprocessing of the source data for training and testing your model.  \nSpecify: Define the type of model that will be used, and the parameters of the model.  \nFit: Capture patterns from provided data. This is the heart of modeling.  \nPredict: Predict the values for the prediction target (y).  \nEvaluate: Determine how accurate the model's predictions are.  ","4106b234":"##### Conclusions:\n- Female passengers have a higher survival rate than male\n- Passengers embarked from Cherbourg (C) have the highest survival rate\n- Passengers travelling with Parents or Children have a higher survival rate\n- Passengers travelling class 1 or 2 have a higher survival rate than passengers travelling class 3\n- Passengers travelling with a Sibling or Spouse have a higher survival rate","48df37a3":"Hi all, I created this notebook to for creating an insightful analysis without overcomplicating. For each step in analysing the dataset I have listed 'conclusions' to indicate my take-away's from the analysis results. If you have any suggestions for further improving this notebook please let me know via the comments!\n\n**By means of this notebook analysis you will be able to answer the following questions:**\n- What chance did passengers have for surviving the Titanic?\n- Is there some truth in the saying 'mothers and children first'?\n- Did paying more for your ticket mean a passenger would have a higher chance of surviving?\n- What was the effect of residing in a certain part of the ship?\n- Would one have a higher chance of surviving when travelling with family?\n\nTraining: https:\/\/www.kaggle.com\/c\/titanic  \nInspiration: https:\/\/www.kaggle.com\/kenjee","f6270f60":"#### Frequency encoding","6415c524":"#### Feature selection","299e94fc":"#### Correlation","64b79f9d":"#### Packages","81f3888c":"# Table of Contents\n\n## Setup\n* [Packages](#Packages)\n* [Source Data](#Source-Data)\n\n## Data Preprocessing\n* [Missing values](#Missing-values)\n* [Target distribution](#Target-distribution)\n* [Feature-Target distribution](#Feature-Target-distribution)\n\n## Exploratory Data Analysis\n* [Correlation](#Correlation)\n* [Continuous Features](#Continuous-features)\n* [Categorical Features](#Categorical-features)\n\n## Feature Engineering\n* [Impute missing values](#Impute-missing-values)\n* [New features from arithmetics](#New-features-from-arithmetics)\n* [New features from binning](#New-features-from-binning)\n* [Binary encoding](#Binary-encoding)\n* [Frequency encoding](#Frequency-encoding)\n* [Label encoding](#Label-encoding)\n* [1-Hot encoding](#1-Hot-encoding)\n* [Normalizing](#Normalizing)\n* [Feature engineering plots](#Feature-engineering-plots)\n\n## Model\n* [Model data preprocessing](#Model-data-preprocessing)\n* [Model specification](#Model-specification)\n* [Fit-Predict base models](#Fit-Predict-base-models)\n* [Evaluate base models](#Evaluate-base-models)\n* [Model tuning](#Model-tuning)\n* [Model predict](#Model-predict)\n\n## Submission output\n"}}