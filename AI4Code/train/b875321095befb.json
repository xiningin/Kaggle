{"cell_type":{"acc95a48":"code","730acc73":"code","2dce5c9a":"code","62cce5d7":"code","259827e3":"code","5cabf6ee":"code","a33cfa49":"code","d882ec03":"code","57a28259":"code","a99455eb":"code","f6f6b0d0":"code","ffefd2d4":"code","a4bad518":"code","3aa41db4":"code","4a508f47":"code","75ed646b":"code","06926b3a":"code","711ba538":"code","dc29d9e0":"code","14441f3f":"code","d73c4133":"code","8a3e33c6":"code","e80df2c3":"code","3073d75f":"code","272cbc90":"code","5e7b3fcd":"code","4f62a630":"code","135f1bae":"code","f10aabfc":"code","5255bfe5":"code","6a8984e8":"code","6a1b469f":"code","e4466fd8":"code","e2df0a86":"code","fcbd47fe":"code","b4edc2ba":"code","0627b669":"code","dd0bfd6d":"code","cc80bd3b":"code","2f0cdd3a":"code","7e9c89a7":"code","3545fd0e":"code","fb683c07":"code","e6f9d217":"code","6ffe6db3":"code","e40fc2bb":"code","0a4e1533":"code","21eab7cf":"code","55dcacbc":"code","c820a9fe":"code","4e9b8e52":"code","35544bed":"code","1c47a994":"code","c657233e":"code","4afc3f57":"code","887b2649":"code","25499fa7":"code","f91dafa5":"code","104f493d":"code","9ec23db7":"markdown","f9c4ac98":"markdown","9e3fbb2c":"markdown","0ff7a836":"markdown","4bd6d31f":"markdown","729ee90f":"markdown","385cb61b":"markdown","da27f5a3":"markdown","7e0be4ff":"markdown","0f4709f4":"markdown","179f7fd4":"markdown","f51faae9":"markdown","a0834287":"markdown","d60af664":"markdown","7895a858":"markdown","1772b8fb":"markdown","49813f47":"markdown","7108477e":"markdown","811c86e7":"markdown","e0652416":"markdown","345ae135":"markdown","b8fd50fd":"markdown","98d27baa":"markdown","1052ea44":"markdown","f12668d0":"markdown","4de74f92":"markdown","eadac259":"markdown","34c61106":"markdown","cc2c20f2":"markdown","eed042bb":"markdown","17871582":"markdown","72bc2f74":"markdown","592e9e58":"markdown","c25a42e2":"markdown","cfcc8f85":"markdown","3caff20c":"markdown","e8f3d7ea":"markdown","f787e4b7":"markdown","10376718":"markdown","0bf58209":"markdown","e469d2b0":"markdown","f8a76b70":"markdown","c544a98e":"markdown","92ae0533":"markdown","f93c6bc9":"markdown","69f5cb91":"markdown"},"source":{"acc95a48":"import numpy as np \nimport pandas as pd \nimport os\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt \n\nimport transformers\nimport random\n\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nscaler = torch.cuda.amp.GradScaler() # GPU\u3067\u306e\u9ad8\u901f\u5316\u3002\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # cpu\u304cgpu\u304b\u3092\u81ea\u52d5\u5224\u65ad\ndevice","730acc73":"SEED = 508\n\ndef random_seed(SEED):\n    \n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n\nrandom_seed(SEED)","2dce5c9a":"df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ndf","62cce5d7":"df = df.iloc[:100,:]\ndf","259827e3":"class EvalDataSet(Dataset):\n    \n    def __init__(self,df):\n        \n        self.df = df\n        \n    def __len__(self):\n        \n        return len(self.df)\n    \n    def __getitem__(self,idx):\n        \n        \n        return { \"id\":self.df.index[idx]  }","5cabf6ee":"test_dataset = EvalDataSet(df)","a33cfa49":"test_dataset[0]","d882ec03":"df","57a28259":"test_batch = 4","a99455eb":"test_dataloader1 = DataLoader(test_dataset,batch_size=test_batch,shuffle = True,num_workers=4,pin_memory=True)","f6f6b0d0":"print(len(test_dataloader1))\nfor a in test_dataloader1:\n    print(a)\n    break","ffefd2d4":"print(len(test_dataloader1))\nfor a in test_dataloader1:\n    print(a)\n    break","a4bad518":"for a in test_dataloader1:\n    result1 = a\n    print(result1)\n    break","3aa41db4":"test_dataloader2 = DataLoader(test_dataset,batch_size=test_batch,shuffle = True,num_workers=4,pin_memory=True)","4a508f47":"for a in test_dataloader2:\n    result2 = a\n    print(result2)\n    break","75ed646b":"result1[\"id\"] == result2[\"id\"]","06926b3a":"random_seed(SEED)","711ba538":"test_dataloader1 = DataLoader(test_dataset,batch_size=test_batch,shuffle = True,num_workers=4,pin_memory=True)","dc29d9e0":"for a in test_dataloader1:\n    result1 = a\n    print(a)\n    break","14441f3f":"random_seed(SEED)","d73c4133":"test_dataloader2 = DataLoader(test_dataset,batch_size=test_batch,shuffle = True,num_workers=4,pin_memory=True)","8a3e33c6":"for a in test_dataloader2:\n    result2 = a\n    print(a)\n    break","e80df2c3":"result1[\"id\"] == result2[\"id\"]","3073d75f":"random_seed(SEED)","272cbc90":"test_dataloader1 = DataLoader(test_dataset,batch_size=test_batch,shuffle = True,num_workers=4,pin_memory=True)","5e7b3fcd":"for a in test_dataloader1:\n    result1 = a\n    print(a)\n    break","4f62a630":"random_seed(SEED)","135f1bae":"valid_dataloader1 = DataLoader(test_dataset,batch_size=test_batch,shuffle = False,num_workers=4,pin_memory=True)","f10aabfc":"for a in valid_dataloader1:\n    print(a)\n    break","5255bfe5":"for a in test_dataloader1:\n    result2 = a\n    print(a)\n    break","6a8984e8":"result1[\"id\"] == result2[\"id\"]","6a1b469f":"import numpy as np \nimport pandas as pd \nimport os\n       \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold","e4466fd8":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt \n\nimport transformers\nimport random\n\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nscaler = torch.cuda.amp.GradScaler() \n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # cpu\u304cgpu\u304b\u3092\u81ea\u52d5\u5224\u65ad\ndevice","e2df0a86":"tokenizer = transformers.BertTokenizer.from_pretrained(\"..\/input\/bert-base-uncased\")\n","fcbd47fe":"max_sens=314","b4edc2ba":"class BERTDataSet(Dataset):\n    \n    def __init__(self,sentences,targets):\n        \n        self.sentences = sentences\n        self.targets = targets\n        \n    def __len__(self):\n        \n        return len(self.sentences)\n    \n    def __getitem__(self,idx):\n        \n        sentence = self.sentences[idx]\n        \n        bert_sens = tokenizer.encode_plus(\n                                sentence,\n                                add_special_tokens = True, \n                                max_length = max_sens, # \u4e0a\u3067314\u306b\u8a2d\u5b9a\u3057\u3066\u3044\u307e\u3059\n                                pad_to_max_length = True, \n                                return_attention_mask = True,\n        truncation=True)\n\n        ids = torch.tensor(bert_sens['input_ids'], dtype=torch.long)\n        mask = torch.tensor(bert_sens['attention_mask'], dtype=torch.long)\n        token_type_ids = torch.tensor(bert_sens['token_type_ids'], dtype=torch.long)\n     \n            \n        target = torch.tensor(self.targets[idx],dtype=torch.float)\n        \n        return {\n                'ids': ids,\n                'mask': mask,\n                'token_type_ids': token_type_ids,\n                'targets': target\n            }","0627b669":"from transformers import AdamW\nLR=2e-5\n\nmodel = transformers.BertForSequenceClassification.from_pretrained(\"..\/input\/bert-base-uncased\",num_labels=1)\noptimizer = AdamW(model.parameters(), LR,betas=(0.9, 0.999), weight_decay=1e-2) ","dd0bfd6d":"from transformers import get_linear_schedule_with_warmup\n\n\nepochs = 20\n\ntrain_batch = 16\n\n\ntrain_steps = int(len(df)\/train_batch*epochs)\nprint(train_steps)\n\nnum_steps = int(train_steps*0.1)\n\nscheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)","cc80bd3b":"from tqdm import tqdm","2f0cdd3a":"def training(\n    train_dataloader,\n    model,\n    optimizer,scheduler\n):\n    \n    model.train()\n    torch.backends.cudnn.benchmark = True\n\n    allpreds = []\n    alltargets = []\n\n    for a in tqdm(train_dataloader):\n\n        losses = []\n\n        optimizer.zero_grad()\n\n        with torch.cuda.amp.autocast():\n\n            ids = a[\"ids\"].to(device,non_blocking=True)\n            mask = a[\"mask\"].to(device,non_blocking=True)\n            tokentype = a[\"token_type_ids\"].to(device,non_blocking=True)\n\n            output = model(ids,mask)\n            output = output[\"logits\"].squeeze(-1)\n\n            target = a[\"targets\"].to(device,non_blocking=True)\n\n            loss = loss_fn(output,target)\n\n\n            # For scoring\n            losses.append(loss.item())\n            allpreds.append(output.detach().cpu().numpy())\n            alltargets.append(target.detach().squeeze(-1).cpu().numpy())\n\n        scaler.scale(loss).backward() # backwards of loss\n        scaler.step(optimizer) # Update optimizer\n        scaler.update() # scaler update\n\n        scheduler.step() # Update learning rate schedule\n        \n        del loss\n\n        # Combine dataloader minutes\n\n    allpreds = np.concatenate(allpreds)\n    alltargets = np.concatenate(alltargets)\n\n    # I don't use loss, but I collect it\n\n    losses = np.mean(losses)\n\n    # Score with rmse\n    train_rme_loss = np.sqrt(mean_squared_error(alltargets,allpreds))\n\n    return losses,train_rme_loss","7e9c89a7":"def loss_fn(output,target):\n    return torch.sqrt(nn.MSELoss()(output,target))","3545fd0e":"random_seed(SEED)\ntrain_dataset = BERTDataSet(df[\"excerpt\"],df[\"target\"])\n\ntrain_dataloader = DataLoader(train_dataset,batch_size=16,shuffle = True,num_workers=4,pin_memory=True)\n\nmodel = transformers.BertForSequenceClassification.from_pretrained(\"..\/input\/bert-base-uncased\",num_labels=1)\n\nmodel.to(device)\nLR=2e-5\noptimizer = AdamW(model.parameters(), LR,betas=(0.9, 0.999), weight_decay=1e-2) # AdamW optimizer\n\ntrain_steps = int(len(df)\/train_batch*epochs)\n\nnum_steps = int(train_steps*0.1)\n\nscheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)\n\nscaler = torch.cuda.amp.GradScaler() \n","fb683c07":"losses,train_rmse_loss = training(train_dataloader,model,optimizer,scheduler)","e6f9d217":"losses","6ffe6db3":"train_rmse_loss","e40fc2bb":"random_seed(SEED)\ntrain_dataset = BERTDataSet(df[\"excerpt\"],df[\"target\"])\n\ntrain_dataloader = DataLoader(train_dataset,batch_size=16,shuffle = True,num_workers=4,pin_memory=True)\n\nmodel = transformers.BertForSequenceClassification.from_pretrained(\"..\/input\/bert-base-uncased\",num_labels=1)\n\nmodel.to(device)\nLR=2e-5\noptimizer = AdamW(model.parameters(), LR,betas=(0.9, 0.999), weight_decay=1e-2) # AdamW optimizer\n\ntrain_steps = int(len(df)\/train_batch*epochs)\n\nnum_steps = int(train_steps*0.1)\n\nscheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)\n\nscaler = torch.cuda.amp.GradScaler() \n","0a4e1533":"losses2,train_rmse_loss2 = training(train_dataloader,model,optimizer,scheduler)","21eab7cf":"losses2","55dcacbc":"train_rmse_loss2","c820a9fe":"train_rmse_loss == train_rmse_loss2","4e9b8e52":"def initialization1():\n    random_seed(SEED)\n    train_dataset = BERTDataSet(df[\"excerpt\"],df[\"target\"])\n\n    train_dataloader = DataLoader(train_dataset,batch_size=16,shuffle = True,num_workers=4,pin_memory=True)\n\n    model = transformers.BertForSequenceClassification.from_pretrained(\"..\/input\/bert-base-uncased\",num_labels=1)\n\n    model.to(device)\n    LR=2e-5\n    optimizer = AdamW(model.parameters(), LR,betas=(0.9, 0.999), weight_decay=1e-2) # AdamW optimizer\n\n    train_steps = int(len(df)\/train_batch*epochs)\n\n    num_steps = int(train_steps*0.1)\n\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)\n\n    scaler = torch.cuda.amp.GradScaler() \n    \n    \n","35544bed":"initialization1()\nlosses,train_rmse_loss = training(train_dataloader,model,optimizer,scheduler)\nprint(losses,train_rmse_loss)","1c47a994":"initialization1()\nlosses2,train_rmse_loss2 = training(train_dataloader,model,optimizer,scheduler)\nprint(losses2,train_rmse_loss2)","c657233e":"train_rmse_loss == train_rmse_loss2","4afc3f57":"def initialization2():\n    random_seed(SEED)\n    train_dataset = BERTDataSet(df[\"excerpt\"],df[\"target\"])\n\n    train_dataloader = DataLoader(train_dataset,batch_size=16,shuffle = True,num_workers=4,pin_memory=True)\n\n    model = transformers.BertForSequenceClassification.from_pretrained(\"..\/input\/bert-base-uncased\",num_labels=1)\n\n    model.to(device)\n    LR=2e-5\n    optimizer = AdamW(model.parameters(), LR,betas=(0.9, 0.999), weight_decay=1e-2) # AdamW optimizer\n\n    train_steps = int(len(df)\/train_batch*epochs)\n\n    num_steps = int(train_steps*0.1)\n\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)\n\n    scaler = torch.cuda.amp.GradScaler() \n    \n    return train_dataloader,model,optimizer,scheduler,scaler\n","887b2649":"train_dataloader,model,optimizer,scheduler,scaler = initialization2()\nlosses,train_rmse_loss = training(train_dataloader,model,optimizer,scheduler)\nprint(losses,train_rmse_loss)","25499fa7":"train_dataloader,model,optimizer,scheduler,scaler = initialization2()\nlosses2,train_rmse_loss2 = training(train_dataloader,model,optimizer,scheduler)\nprint(losses2,train_rmse_loss2)","f91dafa5":"train_rmse_loss == train_rmse_loss2","104f493d":"def initialization3():\n    global train_dataloader,model,optimizer,scheduler,scaler\n    random_seed(SEED)\n    train_dataset = BERTDataSet(df[\"excerpt\"],df[\"target\"])\n    train_dataloader = DataLoader(train_dataset,batch_size=16,shuffle = True,num_workers=4,pin_memory=True)\n    model = transformers.BertForSequenceClassification.from_pretrained(\"..\/input\/bert-base-uncased\",num_labels=1)\n    model.to(device)\n    LR=2e-5\n    optimizer = AdamW(model.parameters(), LR,betas=(0.9, 0.999), weight_decay=1e-2) # AdamW optimizer\n    train_steps = int(len(df)\/train_batch*epochs)\n    num_steps = int(train_steps*0.1)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)\n    scaler = torch.cuda.amp.GradScaler()\ninitialization3()\nlosses,train_rmse_loss3 = training(train_dataloader,model,optimizer,scheduler)\nprint(train_rmse_loss == train_rmse_loss3)","9ec23db7":"## When the initialization was made into a function, it was not reproduced.\n## \u521d\u671f\u5316\u3092\u95a2\u6570\u5316\u3057\u305f\u5834\u5408\u3001\u518d\u73fe\u3057\u306a\u304b\u3063\u305f\u3002","f9c4ac98":"# Summary\n\n## In order to make the data with proper reproducibility ...\n\n## 1. You need to fix the random seeds again before accessing the dataloader.\n\n## 2. If you want to make a function for initialization, you need to return the data properly.\n\n#### In the my shared notebook, I adopt this principle. I return the score from 0.546 to 0.528.","9e3fbb2c":"## **1.5 Summary so far**\n### In the my shared notebook, I used to access the dataloader several times for explanation, so it seems that the score was getting worse when I changed the code a little. I found it important to fix the random seeds just before.\n\n\u79c1\u306e\u30b7\u30a7\u30a2\u3057\u305fnotebook\u3067\u306f\u3001\u89e3\u8aac\u7528\u306b\u4f55\u5ea6\u304bdataloader\u306b\u30a2\u30af\u30bb\u30b9\u3057\u3066\u3044\u305f\u305f\u3081\u3001\u30b3\u30fc\u30c9\u3092\u5c11\u3057\u5909\u3048\u308b\u3068\u30b9\u30b3\u30a2\u304c\u60aa\u304f\u306a\u3063\u3066\u3057\u307e\u3063\u305f\u3088\u3046\u3067\u3059\u3002\u30e9\u30f3\u30c0\u30e0\u30b7\u30fc\u30c9\u3092\u76f4\u524d\u306b\u304d\u3061\u3093\u3068\u56fa\u5b9a\u3059\u308b\u3053\u3068\u304c\u5927\u4e8b\u3060\u3068\u308f\u304b\u308a\u307e\u3057\u305f\u3002","0ff7a836":"\n------------------------------\u4ee5\u4e0b\u3001\u65e5\u672c\u8a9e\u3067\u3059-----------------------------------\n#### \u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3067\u306f\u3001\u4ee5\u4e0b\u306e\u79c1\u306e\u30b7\u30a7\u30a2\u3057\u305fnotebook\u3067\u3001\u306a\u305c\u30b9\u30b3\u30a2\u304c0.528(ver7)\u304b\u30890.546(ver9)\u306b\u60aa\u5316\u3057\u3001\u305d\u308c\u3092\u6539\u5584\u3057\u30660.528(ver10)\u306b\u623b\u305b\u305f\u304b\u306e\u7406\u7531\u3092\u30b7\u30a7\u30a2\u3057\u307e\u3059\u3002\n\u203b https:\/\/www.kaggle.com\/chumajin\/pytorch-bert-beginner-s-room-version\u3000\n\n#### \u4e00\u8a00\u3067\u8a00\u3048\u3070\u3001\u518d\u73fe\u6027\u306e\u554f\u984c\u3067\u3001\u7c21\u5358\u306a\u5b9f\u9a13\u304b\u3089\u7406\u89e3\u3057\u305f\u5b89\u5b9a\u3057\u3066\u30c7\u30fc\u30bf\u3092\u51fa\u3059\u30b3\u30fc\u30c9\u306e\u66f8\u304d\u65b9\u3092\u793a\u3057\u307e\u3059\u3002\n\n#### \u4f55\u304b\u3057\u3089\u304a\u5f79\u306b\u7acb\u3066\u305f\u3089\u5e78\u3044\u3067\u3059\u3002**upvote\/\u30d5\u30a9\u30ed\u30fc\u3057\u3066\u9802\u3051\u305f\u3089\u5b09\u3057\u3044\u3067\u3059\uff01**\n#### \u4ed6\u306enotebook\u3082upvote\u3057\u3066\u9802\u3051\u305f\u65b9\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\uff01","4bd6d31f":"#### Create a dataset. \n\u3000\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u4f5c\u6210","729ee90f":"## **I was able to reproduce it properly by using the function to initialize.**\n\n\n**\u521d\u671f\u5316\u3059\u308b\u95a2\u6570\u3092\u7528\u3044\u3066\u3001\u304d\u3061\u3093\u3068\u518d\u73fe\u3055\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u305f\u3002**","385cb61b":"Comparing the score\n\n\n\u30b9\u30b3\u30a2\u6bd4\u8f03","da27f5a3":"## Do it again to see the reproducibility\n\u518d\u73fe\u6027\u3092\u898b\u308b\u305f\u3081\u306b\u3082\u3046\u4e00\u5ea6\u884c\u3046","7e0be4ff":"# **0. Preparation**\n\u4e0b\u6e96\u5099","0f4709f4":"# \u307e\u3068\u3081\n\n## \u304d\u3061\u3093\u3068\u518d\u73fe\u6027\u306e\u3042\u308b\u30c7\u30fc\u30bf\u3092\u51fa\u3059\u305f\u3081\u306b\u306f\u30fb\u30fb\u30fb\n\n## 1. dataloader\u3092\u8aad\u3080\u3053\u3080\u524d\u306b\u30e9\u30f3\u30c0\u30e0\u30b7\u30fc\u30c9\u3092\u518d\u5ea6fix\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002\n\n## 2. \u521d\u671f\u5316\u3092\u95a2\u6570\u306b\u3059\u308b\u5834\u5408\u306f\u3001\u304d\u3061\u3093\u3068return\u3067\u30c7\u30fc\u30bf\u3092\u8fd4\u3057\u3066\u3042\u3052\u308b\u3053\u3068\u304c\u5fc5\u8981\u3002\n\n#### \u30b7\u30a7\u30a2\u3057\u305fnotebook\u3067\u306f\u3001\u3053\u306e\u539f\u7406\u3092\u53d6\u308a\u5165\u308c\u3066\u30010.546\u306b\u30b9\u30b3\u30a2\u304c\u60aa\u304f\u306a\u3063\u305f\u306e\u3092\u3082\u3068\u306e0.528\u306b\u623b\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3057\u305f\u3002","179f7fd4":"------------------------------------------------------------\n## Below is a successful example. Return each data.\n\n## \u4ee5\u4e0b\u3001\u3046\u307e\u304f\u3044\u3063\u305f\u4f8b\u3002\u30ea\u30bf\u30fc\u30f3\u3067\u30c7\u30fc\u30bf\u3092\u304d\u3061\u3093\u3068\u8fd4\u3059\u3002","f51faae9":"## **Do it again to see the reproducibility**\n\u518d\u73fe\u6027\u3092\u898b\u308b\u305f\u3081\u306b\u3082\u3046\u4e00\u5ea6\u884c\u3046","a0834287":"#### Create a new DataLoader in the experiment. \n\n\u5b9f\u9a13\u3067\u3001\u65b0\u305f\u306b\u30c7\u30fc\u30bf\u30ed\u30fc\u30c0\u30fc\u4f5c\u6210 (test_dataloader2)","d60af664":"--------------------------------------------------------------------------------------\n### Failure example\n### \u5931\u6557\u4f8b","7895a858":"Example\n\n\u4f8b","1772b8fb":"---------------------------------------------------------","49813f47":"# **1. A simple experiment to understand the order in which data is loaded from the Dataloader**\n   Dataloader\u306e\u4e2d\u304b\u3089\u3069\u306e\u3088\u3046\u306a\u9806\u756a\u3067\u30c7\u30fc\u30bf\u304c\u8aad\u307f\u3060\u3055\u308c\u308b\u304b\u3092\u7406\u89e3\u3059\u308b\u7c21\u5358\u306a\u5b9f\u9a13","7108477e":"#### It was confirmed that the order was out of order when it was read repeatedly.\n#### \u7e70\u308a\u8fd4\u3057\u8aad\u307f\u8fbc\u3080\u3068\u9806\u756a\u304c\u305a\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3002","811c86e7":"#### Fix random seed and access DataLoader with shuffle = False and read again\nrandom seed\u3092\u56fa\u5b9a\u3057\u3066shuffle=False\u306eDataLoader\u306b\u30a2\u30af\u30bb\u30b9\u3057\u3066\u304b\u3089\u518d\u5ea6\u8aad\u3080","e0652416":"# **About this notebook**\n\n#### I will share the reasons why my score got worse from 0.528(ver6) to 0.546(ver15) and how I could improve it back to 0.528(ver19) in my shared notebook.\n\u203b\u3000My shared notebook is Pytorch BERT Beginner's Room : https:\/\/www.kaggle.com\/chumajin\/pytorch-bert-beginner-s-room. \n#### It is because of reproducibility, I will show you how to get stable results from which I found in my simple experiment.","345ae135":"## Do it again to see the reproducibility\n\u518d\u73fe\u6027\u3092\u898b\u308b\u305f\u3081\u306b\u3082\u3046\u4e00\u5ea6\u884c\u3046","b8fd50fd":"#### I think this code is useful for accurate K-folding and comparing models. \n## Thank you for reading. If you find it useful, I would be grateful if you could **upvote**.\n## Using this technique, I compared BERT and RoBERTa with various random seed.\n## Please refer it! https:\/\/www.kaggle.com\/chumajin\/bert-v-s-roberta-english","98d27baa":"### It is important from here. Initialize and train. The scheduler and scaler are updated, so they must be run again if you are using them.\n\u3053\u3053\u304b\u3089\u5927\u4e8b\u3067\u3059\u3002\u521d\u671f\u5316\u3057\u3066\u8a13\u7df4\u3059\u308b\u3002\u203b\u3000scheduler\u3084scaler\u306f\u3001update\u3059\u308b\u306e\u3067\u3001\u4f7f\u7528\u3059\u308b\u65b9\u306f\u3082\u3046\u4e00\u5ea6\u5165\u308c\u306a\u3044\u3068\u518d\u73fe\u6027\u3067\u307e\u305b\u3093","1052ea44":"#### I just copied it to the point where I trained by machine learning, so it is OK just to run it for a while. If you want to understand the contents, see reference.\n#### \u6a5f\u68b0\u5b66\u7fd2\u3067\u8a13\u7df4\u3059\u308b\u3068\u3053\u308d\u307e\u3067\u30b3\u30d4\u30fc\u3057\u305f\u3060\u3051\u306a\u3093\u3067\u3001\u3057\u3070\u3089\u304f\u3001\u8d70\u3089\u305b\u308b\u3060\u3051\u3067OK\u3067\u3059\u3002\u4e2d\u8eab\u3092\u7406\u89e3\u3057\u305f\u3051\u308c\u3070\u3001ref\u3092\u898b\u3066\u304f\u3060\u3055\u3044\u3002","f12668d0":"## v11 \u30b3\u30e1\u30f3\u30c8\u3044\u305f\u3060\u3044\u305f\u3082\u306e\u3092\u8ffd\u52a0\u3057\u307e\u3059(global\u306b\u98db\u3070\u3059)","4de74f92":"## **It is confirmed that the random seed has to be fixed again to initialize the Dataloader just before loading..**\nDataloader\u3092\u521d\u671f\u5316\u3059\u308b\u306e\u306b\u306f\u3001\u8aad\u307f\u8fbc\u3080\u76f4\u524d\u306b\u30e9\u30f3\u30c0\u30e0\u30b7\u30fc\u30c9\u3092\u3082\u3046\u4e00\u5ea6\u56fa\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3057\u305f\u3002","eadac259":"### **I'm looking forward to helping you, I would be glad if you could upvote!**\n### Thank you to those who have upvoted before!","34c61106":"## However, I have noticed that the score may not be reproduced even if this is fixed. So let's experiment with it next.\n\u3057\u304b\u3057\u306a\u304c\u3089\u3001\u3053\u308c\u3092\u76f4\u3057\u3066\u3082\u30b9\u30b3\u30a2\u304c\u518d\u73fe\u3057\u306a\u3044\u5834\u5408\u304c\u3042\u308b\u3053\u3068\u306b\u304d\u3065\u304d\u307e\u3057\u305f\u3002\u305d\u306e\u305f\u3081\u3001\u6b21\u306b\u305d\u308c\u3092\u5b9f\u9a13\u3057\u307e\u3059\u3002","cc2c20f2":"\u304d\u3061\u3093\u3068True\u304c\u8fd4\u3063\u3066\u304d\u307e\u3059\u3002\u203b\u3000\u52c9\u5f37\u306b\u306a\u308a\u307e\u3057\u305f\u3002\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059!","eed042bb":"## Comparing the score.\n\u30b9\u30b3\u30a2\u6bd4\u8f03","17871582":"#### I confirmed False.It indicates that the loading order is different.\n#### I expected True because I had thought that the order would be reset if I made a new data loader.\n \n \n False\u3092\u78ba\u8a8d\u3002\u8aad\u307f\u3060\u3055\u308c\u308b\u9806\u756a\u304c\u7570\u306a\u3063\u3066\u3044\u308b\u3068\u3044\u3046\u610f\u5473\u3067\u3059\u3002\n \u65b0\u3057\u304fDataLoader\u3092\u4f5c\u3063\u305f\u3089\u3001\u9806\u756a\u304c\u30ea\u30bb\u30c3\u30c8\u3055\u308c\u308b\u3068\u601d\u3063\u3066\u3044\u305f\u306e\u3067\u3001True\u3092\u671f\u5f85\u3057\u305f\u304c\u3001False\u3092\u78ba\u8a8d\u3002","72bc2f74":"#### Create a class that outputs the index to see the loading order\n\u3000\u8aad\u307f\u3060\u3055\u308c\u308b\u9806\u756a\u3092\u898b\u308b\u305f\u3081\u306b\u3001\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u51fa\u529b\u3059\u308b\u30af\u30e9\u30b9\u3092\u4f5c\u6210","592e9e58":"#### Even if you access the DataLoader with Shuffle = False, the order seems to change when you load the DataLoader with Shuffle = True.\nShuffle=False\u306eDataLoader\u306b\u30a2\u30af\u30bb\u30b9\u3057\u3066\u3082\u305d\u306e\u3042\u3068Shuffle=True\u306eDataLoader\u3092\u8aad\u3080\u3068\u9806\u756a\u304c\u5909\u308f\u308b\u3088\u3046\u3060\u3002","c25a42e2":"------------------------------------------------------------------","cfcc8f85":"## **1.4 What happens if you access shuffle = False?**  \nShuffle = False\u306b\u30a2\u30af\u30bb\u30b9\u3057\u305f\u5834\u5408\u3069\u3046\u306a\u308b\u304b ?","3caff20c":"## How to get back in order??\n\n\u3069\u3046\u3059\u308c\u3070\u3001\u9806\u756a\u304c\u623b\u308b\u304b ?","e8f3d7ea":"#### Since it is a simple experiment, the first 100 are extracted. \n\u3000\u7c21\u5358\u306a\u5b9f\u9a13\u306a\u306e\u3067\u3001\u6700\u521d\u306e100\u500b\u3092\u62bd\u51fa\u3002","f787e4b7":"#### Random seed fixation. \n\u3000\u30e9\u30f3\u30c0\u30e0\u30b7\u30fc\u30c9\u306e\u56fa\u5b9a","10376718":"## **1.1 Understanding the order when loading DataLoader repeatedly**  \n\u7e70\u308a\u8fd4\u3057DataLoader\u3092\u8aad\u3093\u3060\u3068\u304d\u306e\u3001\u9806\u756a","0bf58209":"# **2. Experiments on reproducibility when doing machine learning from my shared notebook.**\n\u79c1\u306enotebook\u203b\u304b\u3089\u6a5f\u68b0\u5b66\u7fd2\u3092\u3059\u308b\u3068\u304d\u306e\u518d\u73fe\u6027\u306e\u5b9f\u9a13\n\n\n\u203b English : https:\/\/www.kaggle.com\/chumajin\/pytorch-bert-beginner-s-room\n\n\n\u203b Japanese : https:\/\/www.kaggle.com\/chumajin\/pytorch-bert-beginner-s-room-version","e469d2b0":"## **1.2 Understanding the order when creating a new dataloader.**    \n  \u65b0\u3057\u304fdataloader\u3092\u4f5c\u3063\u305f\u3068\u304d\u306e\u9806\u756a\u306e\u7406\u89e3","f8a76b70":"--------------------------------------------------------------------------------","c544a98e":"#### With this method, the score was reproduced properly. The problem is if you make a function for initialization.\n\n#### \u3053\u306e\u3084\u308a\u65b9\u306a\u3089\u304d\u3061\u3093\u3068\u30b9\u30b3\u30a2\u304c\u518d\u73fe\u3057\u305f\u3002\u554f\u984c\u306f\u3001\u521d\u671f\u5316\u306e\u3068\u3053\u308d\u3092\u95a2\u6570\u5316\u3057\u305f\u5834\u5408","92ae0533":"#### \u3053\u306e\u30b3\u30fc\u30c9\u306f\u6b63\u78ba\u306bK-fold\u56de\u3057\u305f\u308a\u3001\u30e2\u30c7\u30eb\u9593\u3092\u6bd4\u8f03\u3057\u305f\u308a\u3059\u308b\u3068\u304d\u306b\u5f79\u7acb\u3064\u3068\u601d\u3044\u307e\u3059\u3002\n### \u8aad\u3093\u3067\u3044\u305f\u3060\u3044\u3066\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\u4f55\u304b\u304a\u5f79\u306b\u7acb\u3066\u3070\u3001**upvote**\u3057\u3066\u9802\u3051\u308b\u3068\u5b09\u3057\u3044\u3067\u3059\u3002\n### \u3053\u306e\u30c6\u30af\u30cb\u30c3\u30af\u3092\u4f7f\u3063\u3066\u3001\u30e9\u30f3\u30c0\u30e0\u30b7\u30fc\u30c9\u3092\u632f\u3063\u3066\u3001BERT\u3068RoBERTa\u306e\u6bd4\u8f03\u3092\u884c\u3044\u307e\u3057\u305f\u3002\n### \u826f\u304b\u3063\u305f\u3089\u3001\u3053\u3061\u3089\u3082\u3054\u53c2\u7167\u304f\u3060\u3055\u3044\u3002https:\/\/www.kaggle.com\/chumajin\/bert-v-s-roberta-english","f93c6bc9":"## **1.3 Random seeds were fixed again to solve it.**\n\u5bfe\u7b56\u3068\u3057\u3066\u30e9\u30f3\u30c0\u30e0\u30b7\u30fc\u30c9\u3092\u3082\u3046\u4e00\u5ea6\u56fa\u5b9a\u3057\u305f","69f5cb91":"## Do it again to see the reproducibility\n\u518d\u73fe\u6027\u3092\u898b\u308b\u305f\u3081\u306b\u3082\u3046\u4e00\u5ea6\u884c\u3046"}}