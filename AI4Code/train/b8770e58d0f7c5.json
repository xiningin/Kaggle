{"cell_type":{"1e6b9046":"code","2fe89cdd":"code","1acd4264":"code","2e22ff35":"code","0c046e8c":"code","71699208":"code","af73e135":"code","3395854a":"code","6cc06c9f":"code","608d5518":"code","fc154420":"code","93d7afaa":"code","53ee649b":"code","9d5b38ef":"code","b2baabfd":"code","67ee8bf0":"code","c87bdefb":"code","5d9b86b1":"code","5d9b3237":"code","5c7c3868":"code","c1850f21":"code","dc6f38cd":"code","43122a3b":"code","3d13be47":"code","75784857":"code","faa298b8":"code","8850872c":"code","edc316cd":"code","9484cbef":"code","1d425580":"code","110d5146":"code","d481a52a":"code","3d263414":"code","92bc3d62":"code","88a0a702":"code","b0298eda":"code","529f8feb":"code","b9e612ad":"code","370617a4":"code","d00fa478":"code","95cb01ab":"markdown","a22066d4":"markdown","c0cdf570":"markdown","1447b965":"markdown","27316663":"markdown","64a3355b":"markdown","5e65df76":"markdown","91986592":"markdown","658f1e73":"markdown","ab21f978":"markdown","caa36cf9":"markdown","bf349801":"markdown","c042b484":"markdown","3e8d741a":"markdown"},"source":{"1e6b9046":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2fe89cdd":"from matplotlib import pyplot as plt\nimport seaborn as sns\nimport re\nfrom nltk import word_tokenize","1acd4264":"df = pd.read_csv(\"\/kaggle\/input\/vehicle-dataset-from-cardekho\/Car details v3.csv\")","2e22ff35":"df.head(5)","0c046e8c":"df = df[[c for c in df if c not in ['year']] \n       + ['year']]","71699208":"pd.set_option('float_format', '{:f}'.format)\ndf.describe()","af73e135":"print(df['seats'].isnull().value_counts())\nprint(df['engine'].isnull().value_counts())\nprint(df['mileage'].isnull().value_counts())\nprint(df['max_power'].isnull().value_counts())","3395854a":"#Create a function to convert \nfig=plt.figure()\ndef fix_null(column):\n    df[column] = df[column].replace(np.nan, '00.00', regex=True)\n    for h,i in enumerate(df[column]):\n        try:\n            df.loc[h, column] = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", i)\n        except:\n            df.loc[h, column] = 00.00\n    df[column] = df[column].astype('float')\n    sns.kdeplot(data=df, x=column)\n    plt.title('Density of Column Prior to Adjustment')\n    plt.show()\n    av_calc = df[df[column] > 0][column].median()\n    df[column] = df[column].replace(0,av_calc, regex = True)\n\n","6cc06c9f":"for column in ['mileage', 'engine', 'max_power']:\n    fix_null(column)","608d5518":"df['seats'] = df['seats'].replace(np.nan, 0, regex=True)\nav_seats = round(df[df['seats'] > 0]['seats'].median())\ndf['seats'] = df['seats'].replace(0,av_seats, regex = True)","fc154420":"import json\n\nwith open('\/kaggle\/input\/car-makes\/Car Manufacturers.json') as f:\n  data = json.load(f)\n\nprint(data)","93d7afaa":"df['make'] = ''\n\nfor num, car in enumerate(df['name']):\n    tokens = word_tokenize(car)\n    for token in tokens:\n        if token in data:\n            df.loc[num,'make'] = token\n        elif token == 'Maruti':\n            df.loc[num,'make'] = 'Suzuki'","53ee649b":"df.columns","9d5b38ef":"from matplotlib.ticker import ScalarFormatter, FormatStrFormatter\nfig, ax = plt.subplots(figsize=(15,10))\ny_pos = np.arange(len(df['make'].unique()))\nax.barh(df['make'].unique(), df.groupby(['make']).selling_price.mean(), align='center')\nax.set_yticks(y_pos)\nax.set_yticklabels(sorted(list(df.make.unique())))\nstart, end = ax.get_xlim()\n\nax.xaxis.set_major_formatter(FormatStrFormatter('%.0f'))\nax.xaxis.set_ticks(np.arange(start, end, 500000))\n\nax.invert_yaxis()  # labels read top-to-bottom\nax.set_ylabel('Make')\nax.set_xlabel('Price')\nplt.xticks(rotation=70)\nax.set_title('Mean Price by Make')\n\nplt.show()","b2baabfd":"df.selling_price.describe()","67ee8bf0":"fig, ax = plt.subplots(figsize=(12,8))\nax.boxplot(df['selling_price'], vert=False)\nplt.title('Price Breakdown')\nplt.xlabel('Price')\nax.tick_params(labelleft=False)   \nax.xaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n\nax.xaxis.set_ticks(np.arange(0, max(df['selling_price']), 500000))\nplt.xticks(rotation=70)\nplt.show()","c87bdefb":"df_list = []\n\nfor make in df['make'].unique():\n    df_list.append(df[df['make'] == make].selling_price)\n    \nfig, ax = plt.subplots(figsize=(12,15))\nax.boxplot(df_list)\n\nax.set_xticklabels(df['make'].unique())\nax.yaxis.set_ticks(np.arange(0, max(df['selling_price']), 500000))\nplt.setp( ax.xaxis.get_majorticklabels(), rotation=-45, ha=\"left\" )\nax.yaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n\n\nplt.show()","5d9b86b1":"adj_df = pd.DataFrame()\n\nfor make in df['make'].unique():\n    df_make = df[df['make'] == make]\n    low = np.percentile(df_make.selling_price, 1)\n    high = np.percentile(df_make.selling_price, 99)\n    print(make, low, high, df_make.selling_price.count())\n    if df_make.selling_price.count() > 10:\n        adj_df = adj_df.append(df_make[(df_make['selling_price'] > low) & (df_make['selling_price'] < high)], ignore_index = True)\n    else:\n        adj_df = adj_df.append(df_make, ignore_index = True)","5d9b3237":"adj_df","5c7c3868":"cat_col = ['fuel', 'seller_type', 'transmission', 'owner']\ncounter = 0\nfig, axs = plt.subplots(1, 4, figsize=(15, 8), sharey=True)\n\nfor cat in cat_col:\n    names = list(adj_df[cat].value_counts().keys())\n    values = list(adj_df[cat].value_counts().values)\n    axs[counter].bar(names, values)\n    for i, v in enumerate(values):\n        axs[counter].text(i, v, str(v), fontweight='bold', ha='center', rotation=70)\n    axs[counter].set_title(cat)\n    axs[counter].tick_params(axis='x', labelrotation=45)\n    counter += 1","c1850f21":"plt.figure(figsize = (10,5))\nsns.pairplot(adj_df, hue=\"seller_type\", vars = ['selling_price', 'km_driven', 'mileage','engine', 'max_power', 'year'], diag_kind=\"hist\")\nplt.title(\"Fuel Type of Car Per Year\")\nplt.show()","dc6f38cd":"fig = plt.figure(figsize = (10,8))\nsns.lmplot(data=adj_df, x=\"max_power\", y=\"selling_price\", hue=\"transmission\", height=5)\nplt.title(\"Max Power to Price\")\nplt.show()","43122a3b":"fig = plt.figure(figsize = (10,8))\nsns.lmplot(data=adj_df, x=\"engine\", y=\"selling_price\", hue=\"owner\", height=5)\nplt.show()","3d13be47":"from sklearn import preprocessing\n\n\nfor col in list(adj_df.columns[2:]):\n    if (adj_df[col].dtype == 'int64') or (adj_df[col].dtype == 'float64'):\n        x = np.array(adj_df[col]) #returns a numpy array\n        x = np.reshape(x,(-1,1))\n        min_max_scaler = preprocessing.MinMaxScaler()\n        x_scaled = min_max_scaler.fit_transform(x)\n        adj_df[col] = x_scaled","75784857":"#drop name and torque from the dataset so it can be added into the models. \nnorm_df = adj_df.drop(['torque', 'name'], axis=1)","faa298b8":"for col in norm_df.columns[1:]:\n    if (norm_df[col].dtype == 'object'):\n        norm_df = pd.get_dummies(norm_df, columns=[col], prefix = [col])","8850872c":"pd.set_option(\"display.max_columns\", 101)\nnorm_df","edc316cd":"#Use the train_test_split to split the data into a training and testing dataset.\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(norm_df.iloc[:,1:len(norm_df.columns)], norm_df.iloc[:, 0:1], test_size=.2, random_state=10)","9484cbef":"#I decided to use the linear, Decision Tree and KNeighbors regression models. \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor","1d425580":"linreg = LinearRegression()\nlinreg.fit(x_train, y_train)\npre_linear = linreg.predict(x_test)","110d5146":"r_sq = linreg.score(x_train, y_train)\nprint('Coefficient of determination:', r_sq)","d481a52a":"#Ran a for loop to determine the best tradeoff between number of branches and accuracy of the results\nscore_list = []\n\nfor i in range(2,20):\n    decreg = DecisionTreeRegressor(max_depth = i)\n    decreg.fit(x_train, y_train)\n    pre_tree = decreg.predict(x_test)\n    r_sq = decreg.score(x_test, y_test)\n    score_list.append(r_sq)","3d263414":"fig = plt.figure()\nplt.plot(list(range(2,20)), score_list)\nplt.title(\"Best Depth For The Tree\")\nplt.xticks(list(range(2,20)))\nplt.ylabel(\"R-Squared Score\")\nplt.xlabel(\"Depth of Tree\")\nplt.grid()\nplt.show()","92bc3d62":"decreg = DecisionTreeRegressor(max_depth = 11)\ndecreg.fit(x_train, y_train)\npre_tree = decreg.predict(x_test)\nr_sq = decreg.score(x_test, y_test)\nprint('coefficient of determination:', r_sq)","88a0a702":"score_list = []\n\nfor i in range(2,20):\n    KNreg = KNeighborsRegressor(n_neighbors = i)\n    KNreg.fit(x_train, y_train)\n    pre_KN = KNreg.predict(x_test)\n    r_sq = KNreg.score(x_test, y_test)\n    score_list.append(r_sq)","b0298eda":"fig = plt.figure()\nplt.plot(list(range(2,20)), score_list)\nplt.title(\"Best Number of Neighbors\")\nplt.xticks(list(range(2,20)))\nplt.ylabel(\"R-Squared Score\")\nplt.xlabel(\"Number of Neighbors\")\nplt.grid()\nplt.show()","529f8feb":"KNreg = KNeighborsRegressor(n_neighbors = 2)\nKNreg.fit(x_train, y_train)\npre_KN = KNreg.predict(x_test)\nr_sq = KNreg.score(x_test, y_test)\nprint('coefficient of determination:', r_sq)","b9e612ad":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 100)\ndec_scores = cross_val_score(decreg, x_train, y_train, scoring='r2', cv=folds)\nKN_scores = cross_val_score(KNreg, x_train, y_train, scoring='r2', cv=folds)\ndec_sc = dec_scores.mean()\nKN_sc = KN_scores.mean()  \nprint(\"New Decision Tree Coefficient of Determination: \", dec_sc)\nprint(\"New KNeighbors Coefficient of Determination: \", KN_sc)","370617a4":"from sklearn.feature_selection import RFE\nrfe = RFE(estimator = DecisionTreeRegressor(max_depth = 11), n_features_to_select=15)\nx_rfe = rfe.fit_transform(x_train,y_train)\n\ndecreg.fit(x_rfe,y_train)              \ntemp = pd.Series(rfe.support_,index = x_train.columns)\nselected_features_rfe = temp[temp==True].index\nprint(selected_features_rfe)","d00fa478":"adj_r_sq = decreg.score(x_rfe, y_train)\nprint('Reduced Features Coefficient of Determination: ', adj_r_sq)","95cb01ab":"### Visualization ###\nThe first area that I wanted to look at was how the data was broken down by categorical value. To do this I ran a some bar graphs to help review this. In most of the cases, there were usually two major variables in our categorical features that had the majority of cars.\n\nfuel: Diesel and Petrol\n\nseller_type: Individual and Dealer\n\ntransmission: Manual and Automatic\n\nowner: First and Second","a22066d4":"We see that both provide a relatively R-squared value, with K Nearest Neighbors having the highest value of the two. Next I wanted to see if there were some features that were less useful to the Decision Tree model (I wasn't able to find something similar for KNeighbors). When we look below we see that after taking only the top 15 features, the Decision Tree R-squared actually increases which is likely due to the loss of some of the negatively correlated features. It was interesting that outside of the numerical values (km_driven, mileage, engine, etc.), the next most important features were mostly the makes of the cars. ","c0cdf570":"After looking at the pair plot, I ran a couple of scatterplots with the line of best fit. The first was a comparison between the selling price and max power, with the transmission categorical feature added for colouring. The second comparison depicted the selling price to the engine, with the owner categorical feature as an added variable. Out of the two, it seems like the max_power and selling price for automatic vehicles have a pretty positive relationship.","1447b965":"In some of the columns, there were some missing values. To adjust the missing values in these columns, I chose to update the missing values with the median values of the column. There are some outliers that will have an overly large effect on the mean of the columns.","27316663":"Below we see a box plot showing the breakdown of prices over the overall datasets. I could drop some of the major outliers based on this graph, but there is a chance that a car make may be worth much more than the majority of the dataset which would lead me to completely cut out a certain make. In order to adjust for this, I will look at box plot by each of the makes to determine the outliers in the dataset.","64a3355b":"Next, I ran a pair plot to quickly see if there were any noticably linear relationships in our data. To add additional information, I wanted to look at seller_type to see if this would help in finding any interesting relationships. At first glance it seems like there could be something between the max_power\/selling_price and engine\/selling_price","5e65df76":"## Conclusion ##\nAfter going through all of our models, it seems like the information provided for each of these cars were a strong indicator of the price. It seems like our reduced features model would provide us with the strongest R-squared; however, if our dataset continued to expand we may start to see that other features would become more important in our predictions. I would recommend using either KNeighbors or Decision Tree (Pre feature selection) to determine the price. ","91986592":"The next area of preprocessing that I wanted to look at was the the make of each car. In order to do this, I found a listing of car makes and uploaded the list into the notebook. By doing this, it should allow the model to pull in a more accurate prediction of the price. In order to do this, I tokenized the car names to find whether any of the words were included in my listing of makes.","658f1e73":"We see below that there are a few major outliers, but the majority are not massively affecting the mean of the overall dataset. In order to clean up the larger outliers, I felt that dropping the amounts that are lower than 1 percent and higher than 99 percent.","ab21f978":"For the Decision Tree and K Nearest Neighbour, I wanted to run a for loop to determine the best tradeoff between depth\/number of neighbors and speed of the model. To determine this number, I ran a couple of line plots and used the elbow method to determine the best value for each.","caa36cf9":"### Additional Preprocessing for Models ###\nWhen running models it is always smart to normalize the data to ensure that a certain feature is not given a higher significance purely because it is a higher value than the other features. In addition, to include the categorical variables in the models, I needed to add dummies to provide them with binary values. ","bf349801":"## Introduction ##\nIt is very difficult to determine the best price when you're purchasing a new car. There are multiple features that you have to look at to determine its actual worth. Throughout this notebook, we will try to determine the best models to predict a cars price.","c042b484":"In the above models, we see the following R squared values:\n\nLinear Regression: 85.67%\n\nDecision Tree: 92.92%\n\nKNearest Neighbors: 94.48%\n\nIt seems they are all relatively accurate; however, I chose to move forward with additional testing for the Decision Tree and KNearest Neighbors because of their tendency to overfit. To ensure that they were not overfit, I ran a KFold random sample to see how they performed.","3e8d741a":"Work done on the selling price of the car was the last part of the preprocessing that we needed to look at. In the below chart, we see the means prices of the cars that were sold. There is quite a wide variety among the selling price means among the car makes. Many of the means prices make sense at first glance (ex. Lexus, BMW, Audi have some of the higher resale values), but I would like to see if there are any significant outliers in these prices."}}