{"cell_type":{"6925ec0e":"code","d5435fbb":"code","6280dc92":"code","bbbeb824":"code","73b48782":"code","59440af8":"code","25ac2ba7":"code","3faa3bb9":"code","24b89d65":"code","4dbd224c":"code","29e8f040":"code","27cb09f3":"code","0722921f":"code","c7cb2c2a":"code","f6e58e74":"code","38aa91ff":"code","ac8260b3":"code","504de7e2":"code","602ada90":"code","2d7bf89a":"code","fbc0a857":"code","fa92c711":"code","c7d784b7":"code","5fc7a8fd":"code","93d6a61d":"code","0e33cf4b":"code","a9d841a3":"code","0b201b3c":"code","45cf814a":"code","95254881":"code","e40576c0":"code","1faba80c":"code","fbfcc677":"code","c178bf3e":"code","2ef4f683":"code","4aa11366":"code","5b9db651":"code","85db700f":"code","46ca74d4":"code","ecc60e3b":"code","8d4c68bf":"code","fce9be7c":"code","e811083d":"code","9aee215c":"code","2f19dde8":"code","88239565":"code","9f84c026":"code","6b1b9f8c":"code","be0db230":"code","aeaa878e":"code","bae3f518":"code","f86d4a89":"code","399e2597":"code","88187be1":"code","cd33adf9":"code","4bbf9fae":"code","1cc6605b":"code","ace0312a":"code","4f020abc":"code","7f2d06ac":"code","2398c998":"code","124d3cfc":"code","7eea6880":"code","ebd5779a":"code","5449028e":"code","f4e55812":"code","fd654ea3":"code","88010bff":"code","fc3061bb":"code","86c0a957":"code","c9a73710":"code","6d2bb2ef":"code","2de990e5":"code","a196eb15":"markdown","cb572422":"markdown","b8fd0ed1":"markdown","06347d1b":"markdown","7a728de9":"markdown","e2753aaa":"markdown","6ab9e250":"markdown","d5e73971":"markdown","92d5a15b":"markdown","9054901d":"markdown","1a0fa1a1":"markdown","acffffa1":"markdown","ed1f7e30":"markdown","b1011cd6":"markdown","99360635":"markdown","c8519f98":"markdown","060e38b0":"markdown","556045bd":"markdown","07b9f4d8":"markdown","3025cb04":"markdown","aa85c9b3":"markdown","608a9895":"markdown","23cc0d23":"markdown","973658fd":"markdown","e048da83":"markdown","3ddf67be":"markdown","b1eeafc4":"markdown","4120faf9":"markdown","04caf1ac":"markdown","214b1cf3":"markdown","599928b8":"markdown","ae7e0c8c":"markdown","81880b59":"markdown","2b308be8":"markdown","0a6b0440":"markdown","2b8b9d11":"markdown","7cf8eb85":"markdown","3e87ad43":"markdown","1a0c55b9":"markdown","5d00c88b":"markdown","3ca39b23":"markdown","5406abbd":"markdown","23032a34":"markdown"},"source":{"6925ec0e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nimport os\nprint(os.listdir())\n\nimport warnings\nwarnings.filterwarnings('ignore')","d5435fbb":"dataset = pd.read_csv(\"..\/input\/heart.csv\")","6280dc92":"type(dataset)","bbbeb824":"dataset.shape","73b48782":"dataset.head(5)","59440af8":"dataset.sample(5)","25ac2ba7":"dataset.describe()","3faa3bb9":"dataset.info()","24b89d65":"###Luckily, we have no missing values","4dbd224c":"info = [\"age\",\"1: male, 0: female\",\"chest pain type, 1: typical angina, 2: atypical angina, 3: non-anginal pain, 4: asymptomatic\",\"resting blood pressure\",\" serum cholestoral in mg\/dl\",\"fasting blood sugar > 120 mg\/dl\",\"resting electrocardiographic results (values 0,1,2)\",\" maximum heart rate achieved\",\"exercise induced angina\",\"oldpeak = ST depression induced by exercise relative to rest\",\"the slope of the peak exercise ST segment\",\"number of major vessels (0-3) colored by flourosopy\",\"thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\"]\n\n\n\nfor i in range(len(info)):\n    print(dataset.columns[i]+\":\\t\\t\\t\"+info[i])","29e8f040":"dataset[\"target\"].describe()","27cb09f3":"dataset[\"target\"].unique()","0722921f":"print(dataset.corr()[\"target\"].abs().sort_values(ascending=False))","c7cb2c2a":"#This shows that most columns are moderately correlated with target, but 'fbs' is very weakly correlated.","f6e58e74":"y = dataset[\"target\"]\n\nsns.countplot(y)\n\n\ntarget_temp = dataset.target.value_counts()\n\nprint(target_temp)","38aa91ff":"print(\"Percentage of patience without heart problems: \"+str(round(target_temp[0]*100\/303,2)))\nprint(\"Percentage of patience with heart problems: \"+str(round(target_temp[1]*100\/303,2)))\n\n#Alternatively,\n# print(\"Percentage of patience with heart problems: \"+str(y.where(y==1).count()*100\/303))\n# print(\"Percentage of patience with heart problems: \"+str(y.where(y==0).count()*100\/303))\n\n# #Or,\n# countNoDisease = len(df[df.target == 0])\n# countHaveDisease = len(df[df.target == 1])","ac8260b3":"dataset[\"sex\"].unique()","504de7e2":"sns.barplot(dataset[\"sex\"],y)","602ada90":"dataset[\"cp\"].unique()","2d7bf89a":"sns.barplot(dataset[\"cp\"],y)","fbc0a857":"dataset[\"fbs\"].describe()","fa92c711":"dataset[\"fbs\"].unique()","c7d784b7":"sns.barplot(dataset[\"fbs\"],y)","5fc7a8fd":"dataset[\"restecg\"].unique()","93d6a61d":"sns.barplot(dataset[\"restecg\"],y)","0e33cf4b":"dataset[\"exang\"].unique()","a9d841a3":"sns.barplot(dataset[\"exang\"],y)","0b201b3c":"dataset[\"slope\"].unique()","45cf814a":"sns.barplot(dataset[\"slope\"],y)","95254881":"#number of major vessels (0-3) colored by flourosopy","e40576c0":"dataset[\"ca\"].unique()","1faba80c":"sns.countplot(dataset[\"ca\"])","fbfcc677":"sns.barplot(dataset[\"ca\"],y)","c178bf3e":"### Analysing the 'thal' feature","2ef4f683":"dataset[\"thal\"].unique()","4aa11366":"sns.barplot(dataset[\"thal\"],y)","5b9db651":"sns.distplot(dataset[\"thal\"])","85db700f":"from sklearn.model_selection import train_test_split\n\npredictors = dataset.drop(\"target\",axis=1)\ntarget = dataset[\"target\"]\n\nX_train,X_test,Y_train,Y_test = train_test_split(predictors,target,test_size=0.20,random_state=0)","46ca74d4":"X_train.shape","ecc60e3b":"X_test.shape","8d4c68bf":"Y_train.shape","fce9be7c":"Y_test.shape","e811083d":"from sklearn.metrics import accuracy_score","9aee215c":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\n\nlr.fit(X_train,Y_train)\n\nY_pred_lr = lr.predict(X_test)","2f19dde8":"Y_pred_lr.shape","88239565":"score_lr = round(accuracy_score(Y_pred_lr,Y_test)*100,2)\n\nprint(\"The accuracy score achieved using Logistic Regression is: \"+str(score_lr)+\" %\")","9f84c026":"from sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\n\nnb.fit(X_train,Y_train)\n\nY_pred_nb = nb.predict(X_test)","6b1b9f8c":"Y_pred_nb.shape","be0db230":"score_nb = round(accuracy_score(Y_pred_nb,Y_test)*100,2)\n\nprint(\"The accuracy score achieved using Naive Bayes is: \"+str(score_nb)+\" %\")","aeaa878e":"from sklearn import svm\n\nsv = svm.SVC(kernel='linear')\n\nsv.fit(X_train, Y_train)\n\nY_pred_svm = sv.predict(X_test)","bae3f518":"Y_pred_svm.shape","f86d4a89":"score_svm = round(accuracy_score(Y_pred_svm,Y_test)*100,2)\n\nprint(\"The accuracy score achieved using Linear SVM is: \"+str(score_svm)+\" %\")","399e2597":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=7)\nknn.fit(X_train,Y_train)\nY_pred_knn=knn.predict(X_test)","88187be1":"Y_pred_knn.shape","cd33adf9":"score_knn = round(accuracy_score(Y_pred_knn,Y_test)*100,2)\n\nprint(\"The accuracy score achieved using KNN is: \"+str(score_knn)+\" %\")","4bbf9fae":"from sklearn.tree import DecisionTreeClassifier\n\nmax_accuracy = 0\n\n\nfor x in range(200):\n    dt = DecisionTreeClassifier(random_state=x)\n    dt.fit(X_train,Y_train)\n    Y_pred_dt = dt.predict(X_test)\n    current_accuracy = round(accuracy_score(Y_pred_dt,Y_test)*100,2)\n    if(current_accuracy>max_accuracy):\n        max_accuracy = current_accuracy\n        best_x = x\n        \n#print(max_accuracy)\n#print(best_x)\n\n\ndt = DecisionTreeClassifier(random_state=best_x)\ndt.fit(X_train,Y_train)\nY_pred_dt = dt.predict(X_test)","1cc6605b":"print(Y_pred_dt.shape)","ace0312a":"score_dt = round(accuracy_score(Y_pred_dt,Y_test)*100,2)\n\nprint(\"The accuracy score achieved using Decision Tree is: \"+str(score_dt)+\" %\")","4f020abc":"from sklearn.ensemble import RandomForestClassifier\n\nmax_accuracy = 0\n\n\nfor x in range(2000):\n    rf = RandomForestClassifier(random_state=x)\n    rf.fit(X_train,Y_train)\n    Y_pred_rf = rf.predict(X_test)\n    current_accuracy = round(accuracy_score(Y_pred_rf,Y_test)*100,2)\n    if(current_accuracy>max_accuracy):\n        max_accuracy = current_accuracy\n        best_x = x\n        \n#print(max_accuracy)\n#print(best_x)\n\nrf = RandomForestClassifier(random_state=best_x)\nrf.fit(X_train,Y_train)\nY_pred_rf = rf.predict(X_test)","7f2d06ac":"Y_pred_rf.shape","2398c998":"score_rf = round(accuracy_score(Y_pred_rf,Y_test)*100,2)\n\nprint(\"The accuracy score achieved using Decision Tree is: \"+str(score_rf)+\" %\")","124d3cfc":"import xgboost as xgb\n\nxgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\nxgb_model.fit(X_train, Y_train)\n\nY_pred_xgb = xgb_model.predict(X_test)","7eea6880":"Y_pred_xgb.shape","ebd5779a":"score_xgb = round(accuracy_score(Y_pred_xgb,Y_test)*100,2)\n\nprint(\"The accuracy score achieved using XGBoost is: \"+str(score_xgb)+\" %\")","5449028e":"from keras.models import Sequential\nfrom keras.layers import Dense","f4e55812":"# https:\/\/stats.stackexchange.com\/a\/136542 helped a lot in avoiding overfitting\n\nmodel = Sequential()\nmodel.add(Dense(11,activation='relu',input_dim=13))\nmodel.add(Dense(1,activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","fd654ea3":"model.fit(X_train,Y_train,epochs=300)","88010bff":"Y_pred_nn = model.predict(X_test)","fc3061bb":"Y_pred_nn.shape","86c0a957":"rounded = [round(x[0]) for x in Y_pred_nn]\n\nY_pred_nn = rounded","c9a73710":"score_nn = round(accuracy_score(Y_pred_nn,Y_test)*100,2)\n\nprint(\"The accuracy score achieved using Neural Network is: \"+str(score_nn)+\" %\")\n\n#Note: Accuracy of 85% can be achieved on the test set, by setting epochs=2000, and number of nodes = 11. ","6d2bb2ef":"scores = [score_lr,score_nb,score_svm,score_knn,score_dt,score_rf,score_xgb,score_nn]\nalgorithms = [\"Logistic Regression\",\"Naive Bayes\",\"Support Vector Machine\",\"K-Nearest Neighbors\",\"Decision Tree\",\"Random Forest\",\"XGBoost\",\"Neural Network\"]    \n\nfor i in range(len(algorithms)):\n    print(\"The accuracy score achieved using \"+algorithms[i]+\" is: \"+str(scores[i])+\" %\")","2de990e5":"sns.set(rc={'figure.figsize':(15,8)})\nplt.xlabel(\"Algorithms\")\nplt.ylabel(\"Accuracy score\")\n\nsns.barplot(algorithms,scores)","a196eb15":"### Naive Bayes","cb572422":"## I. Importing essential libraries","b8fd0ed1":"### Decision Tree","06347d1b":"##### ca=4 has astonishingly large number of heart patients","7a728de9":"### Checking correlation between columns","e2753aaa":"#### Printing out a few columns","6ab9e250":"### Analysing the restecg feature","d5e73971":"#### Description","92d5a15b":"#### Let's understand our columns better:","9054901d":"### XGBoost","1a0fa1a1":"## Exploratory Data Analysis (EDA)","acffffa1":"### K Nearest Neighbors","ed1f7e30":"##### We realize that people with restecg '1' and '0' are much more likely to have a heart disease than with restecg '2'","b1011cd6":"### I'm a beginner to Kaggle, and am learning to practice Machine Learning code. I hope this Kernel can help out a few others, like myself, who are looking for a beginner-friendly kernel for getting better at Machine Learning,\n### I'd love some feedback. If there's anything I've done wrong, or anything new or better that I can add to my code, I'd really appreciate valuable feedback on it. \n\n### Cheers!","99360635":"## V. Model Fitting","c8519f98":"### Random Forest","060e38b0":"### Logistic Regression","556045bd":"### Analysing the 'Chest Pain Type' feature","07b9f4d8":"### Neural Network","3025cb04":"### First, analysing the target variable:","aa85c9b3":"### We observe that, we can achieve the best accuracy of 95.08% using Random Forest <br> <br>","608a9895":"### We'll analyse 'sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca' and 'thal' features","23cc0d23":"### SVM","973658fd":"### Analysing the 'Sex' feature","e048da83":"## VI. Output final score","3ddf67be":"##### We observe, that Slope '2' causes heart pain much more than Slope '0' and '1'","b1eeafc4":"#### Clearly, this is a classification problem, with the target variable having values '0' and '1'","4120faf9":"##### We notice, that chest pain of '0', i.e. the ones with typical angina are much less likely to have heart problems","04caf1ac":"##### We notice, that females are more likely to have heart problems than males","214b1cf3":"##### As expected, the CP feature has values from 0 to 3","599928b8":"## IV. Train Test split","ae7e0c8c":"### Analysing the FBS feature","81880b59":"### Analysing the 'exang' feature","2b308be8":"##### People with exang=1 i.e. Exercise induced angina are much less likely to have heart problems","0a6b0440":"## II. Importing and understanding our dataset ","2b8b9d11":"#### Verifying it as a 'dataframe' object in pandas","7cf8eb85":"##### We notice, that as expected, the 'sex' feature has 2 unique features","3e87ad43":"## <font size=5> <strong> Predicting presence of Heart Disease using Machine Learning  <\/strong> <\/font>","1a0c55b9":"#### Analysing the 'target' variable","5d00c88b":"### Analysing the 'ca' feature","3ca39b23":"##### Nothing extraordinary here","5406abbd":"### Analysing the Slope feature","23032a34":"#### Shape of dataset"}}