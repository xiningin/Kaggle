{"cell_type":{"cdd827e3":"code","8032edef":"code","61284d7c":"code","03c45509":"code","88d4e6e8":"code","12087aed":"code","ad2241fb":"code","0a055a3b":"code","53c075df":"code","56317ba8":"code","3430df5a":"code","0312fed5":"code","99934e6e":"code","97e44254":"code","6322e502":"code","f20de46c":"markdown","e39f7a1e":"markdown"},"source":{"cdd827e3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8032edef":"!pip install lightautoml","61284d7c":"from lightautoml.automl.presets.text_presets import TabularNLPAutoML\nfrom lightautoml.tasks import Task\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nimport torch","03c45509":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","88d4e6e8":"from nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\nstop_words = stopwords.words('english')\nstemmer = SnowballStemmer(\"english\")","12087aed":"def make_lower(document):\n    return document.lower()\n\ndef stem_sentences(sentence):\n    tokens = sentence.split()\n    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n    return ' '.join(stemmed_tokens)\n\ntrain_df['text'] = train_df['text'].apply(make_lower)\ntrain_df['text'] = train_df['text'].replace(r'[!@#,$%^?&*\\'\\]\\[()_+-={}<>;:\"\\.]+', ' ', regex=True)\ntrain_df['text'] = train_df['text'].replace(r'[\\s]+', ' ', regex = True)\ntrain_df['text'] = train_df['text'].replace(r'http\\S+','', regex = True) \n\n\ntest_df['text'] = test_df['text'].apply(make_lower)\ntest_df['text'] = test_df['text'].replace(r'[!@#,$%^?&*\\'\\]\\[()_+-={}<>;:\"\\.]+', ' ', regex=True)\ntest_df['text'] = test_df['text'].replace(r'[\\s]+', ' ', regex = True)\ntest_df['text'] = test_df['text'].replace(r'http\\S+','', regex = True) ","ad2241fb":"train_df['text'] = train_df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\ntrain_df['text'] = train_df['text'].apply(stem_sentences)\n\ntest_df['text'] = test_df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\ntest_df['text'] = test_df['text'].apply(stem_sentences)","0a055a3b":"N_THREADS = 4 \nRANDOM_STATE = 42 \nTARGET_NAME = 'target' \n\nnp.random.seed(RANDOM_STATE)\ntorch.set_num_threads(N_THREADS)","53c075df":"task = Task('binary')","56317ba8":"roles = {'target': TARGET_NAME, 'text': ['text']}","3430df5a":"automl = TabularNLPAutoML(task = task,\n                        gpu_ids=0,\n                       timeout = 3600,\n                       cpu_limit = N_THREADS,\n                       general_params = {'nested_cv': False, 'use_algos': [['linear_l2', 'lgb', 'lgb_tuned']]},\n                       reader_params = {'cv': 3, 'random_state': RANDOM_STATE},\n                       tuning_params = {'max_tuning_iter': 20, 'max_tuning_time': 30},\n#                        general_params = {'nested_cv': False, 'use_algos': [['linear_l2']]},\n#                        linear_pipeline_params = {'text_features': \"tfidf\"},\n#                        tfidf_params = {'svd': True, 'tfidf_params': {'ngram_range': (1, 1)} },\n#                        text_params = {'lang': 'en', 'bert_model': 'cardiffnlp\/twitter-roberta-base-sentiment'},\n#                        autonlp_params = {'model_name': 'random_lstm_bert',\n#                                          'transformer_params': {'dataset_params': {\n#                                                                                   'max_length': 150\n#                                                               }\n#                                         },\n#                                         }\n                          \n                    )\n\noof_pred = automl.fit_predict(train_df, roles = roles, verbose=3)\nprint('oof_pred:\\n{}\\nShape = {}'.format(oof_pred, oof_pred.shape))","0312fed5":"sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsample_submission","99934e6e":"test_pred = automl.predict(test_df)","97e44254":"sample_submission['target'] = test_pred.data[:,0]\nsample_submission['target'] = round(sample_submission['target'])\nsample_submission['target'] = sample_submission['target'].astype('int')\nsample_submission.head(20)","6322e502":"sample_submission.to_csv(\"submission.csv\", index=False)","f20de46c":"# Preprocessing of Data","e39f7a1e":"# LightAutoML"}}