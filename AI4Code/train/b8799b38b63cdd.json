{"cell_type":{"f40daef9":"code","e9dee1ff":"code","4ecedcab":"code","7eed0a21":"code","c3da285e":"code","8a71aa4b":"code","f3864108":"code","b1ca5e45":"code","196f24d5":"code","5d6b2f42":"code","26d18fb5":"code","b1e45a08":"code","b728fac0":"code","1aec2534":"code","f8f9355e":"code","5bb484a4":"code","0181f01d":"code","448e5cc5":"code","63933f9c":"code","dba886c4":"code","b9309cdb":"code","4fd04174":"code","91078933":"code","db399ef3":"code","614555f4":"code","a3b67ea1":"markdown","1d75ff4d":"markdown","b6614494":"markdown","1ba8a213":"markdown","1576b9b8":"markdown","250b02bc":"markdown","0d06c353":"markdown","36bc6638":"markdown","80deeea9":"markdown"},"source":{"f40daef9":"import numpy as np\nimport pandas as pd\nimport io","e9dee1ff":"x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1,1))\n# reshapes to 2D, 1 col and unlimited rows.\n\ny = np.array([5, 20, 14, 32, 22, 38])\n\nprint(x)\nprint(y)","4ecedcab":"from sklearn.linear_model import LinearRegression\n\n# Create instance of class\nmodel = LinearRegression()","7eed0a21":"# calculate b0 and b1, intercept and coefficients\nmodel = model.fit(x,y)","c3da285e":"# Getting R sqare value\n# Coefficient of determination\nmodel.score(x, y)","8a71aa4b":"print('Intercept', model.intercept_) # single value","f3864108":"print('Slope, b1, coefficient', model.coef_) # array of coeffs","b1ca5e45":"# Predict Y\ny_ = model.predict(x)\nprint(y_)","196f24d5":"# same as we use f(x)\ny_ = model.intercept_ + model.coef_*x\nprint(y_)","5d6b2f42":"# Visualizing\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(x,y, color='red')\nplt.plot(x, model.predict(x), color='blue')\nplt.title('X vs Y Training Set')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()","26d18fb5":"# DataSet for other examples\n\ndata = '''\nPosition,Level,Salary\nBusiness Analyst,1,45000\nJunior Consultant,2,50000\nSenior Consultant,3,60000\nManager,4,80000\nCountry Manager,5,110000\nRegion Manager,6,150000\nPartner,7,200000\nSenior Partner,8,300000\nC-level,9,500000\nCEO,10,1000000\n'''\ndf = pd.read_csv(io.StringIO(data), sep=',')\ndf","b1e45a08":"X = df.iloc[:, 1:2].values # all rows column Level\ny = df.iloc[:, 2].values # all rows column Salary, independent to be predicted","b728fac0":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly_reg = PolynomialFeatures(degree = 4)\nX_poly = poly_reg.fit_transform(X) # adds square of column in x\nX_poly","1aec2534":"poly_reg.fit(X_poly, y)\nmodel = LinearRegression()\nmodel.fit(X_poly, y)","f8f9355e":"# Viz results\n\nplt.scatter(X, y, color = \"red\")\nplt.plot(X, model.predict(X_poly), color = \"blue\")\nplt.title('X vs Y Desicion Tree')\nplt.xlabel('Position Level')\nplt.ylabel('Salary')\nplt.show()","5bb484a4":"from sklearn.tree import DecisionTreeRegressor\n\nmodel = DecisionTreeRegressor(random_state = 0)\nmodel.fit(X, y)","0181f01d":"y_pred = model.predict(np.array(6.5).reshape(-1,1))\ny_pred","448e5cc5":"X_grid = np.arange(min(X), max(X), 0.1)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.scatter(X, y, color = \"red\")\nplt.plot(X_grid, model.predict(X_grid), color = \"blue\")\nplt.title('X vs Y Desicion Tree')\nplt.xlabel('Position Level')\nplt.ylabel('Salary')\nplt.show()\n\n# Here we can see the step prediction \n# that is average Y for a particular node and division\n","63933f9c":"import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport seaborn as sns","dba886c4":"# add column to statmodel\nx_ = sm.add_constant(x)\nprint(x_) # this makes two columns, 1st for intercept and 2nd for coef\ny = np.array([5, 20, 14, 32, 22, 38])","b9309cdb":"model = sm.OLS(y, x) # ordinary least square\nresults = model.fit() # this gives variables results\nprint(results.summary())","4fd04174":"# Plot outputs\nplt.scatter(x, y,  color='black')\nplt.plot(x, y_, color='blue', linewidth=3)\nplt.show()","91078933":"df = pd.DataFrame([x.reshape(1,-1)[0],y]).transpose()\ndf.columns = ['x','y']\ndf\n","db399ef3":"\nsns.jointplot(x='x',y='y',data=df, kind='reg')","614555f4":"ep = y-y_.flatten()\nprint('epsilons',ep)\nprint('mean',round(np.mean(ep)))\nprint('sum',round(np.sum(ep)))\nprint('standard deviation',round(np.std(ep),4))\nsns.distplot(ep) # not a norm dist","a3b67ea1":"### Handling non-linearity\n\nWhen we begin with data analysis, 95% time should be spent on understanding data. How variables are related? Are they linear or have a curve? Linear Regression will try to find a best fit line but actually the relation might be curved. So, rather than new non-linear technique, we can **transform the data**.\n\nTo find non-linearity make **scatter plots** between Y and all Xs, or pair plot. Plot the Y - dependent variable against each of the Xs - explainatory variables and visually see for non linearity if any. Take transformations, like log(X3) or log(Y) and see if R^2 is improved.\n\nCommon transformations:\n- log\n- exp\n- sq, sr root, cubes\n- inverse\n\nThese transformations need to be non-linear, it can't be 6x +5.\n\n$$ln(a) = log_e a = b$$\n\n$$a = exp(b) = e^b$$\n\n$$ln(a x b) = ln (a) + ln (b)$$\n\n$$ln(a\/b) = ln(a) -ln(b)$$\n\n$$ln (1) =0$$\n\n$$ln (-1) = not defined$$\n\n$$ln (0) = not defined$$\n\n\nif our data has 0 or -ve values the we **cannot** take log transformations.\n\n### Log-Log Model (or Elasticity Model)\n\n$ ln(y) = \\beta_0 + \\beta_1ln(x_1) + \\beta_2ln(x_2) + ... +  $\n\n**Interpretation**: 1% inc in x1 increases y by beta1% and so on...\n\ne.g. $ ln(sales) = \\beta_0 + \\beta_1ln(price) + \\beta_2ln(AdExp) + \\beta_2ln(Promotions) $\n\nsay, b2 = 0.04, p-val=0.00001\n\nso, 1% inc in AdExp increases Sales by 0.04%. That's how log-log model should be interpreted.\n\nEconomists love this kind of interpretation, called **elasticity**.\n\n### Semi-log model (growth rate model)\n\n$ ln(y) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... +  $\n\n**Interpretation**: 1 unit inc in $x_1$ increases y by $(\\beta_1\\times100) \\% $ , and so on. Semi-log model can also be used to find YoY growth as 1 unit inc in year gives us B1\\*100% increase in Y. If it is mixed, log and non-log then the interpretation changes. If both x and y are log then log log, if ln(y) and x, then semi-log.","1d75ff4d":"## Random Forest Regression\n\nWe can have regression RF similar to classification RF. RF is a version of Ensamble Learning, other verion is gradient boosting. \n\n**Ensamble Learning** is when we take algo(s) multiple times and make something more powerful than original. They are more stable to data change.\n\n1. We take K data points and make a decision tree\n2. we make too many such trees \n3. For new data point we predict from each of these trees. So if we have 500 trees, we get 500 y_pred for one new data point, we average it out to get y value.\n\nIt improves accuracy. Non-linear regression.\n\nWe get more stairs compared to DT. RF breaks in to more steps.","b6614494":"## SVR - Support Vector Regressor\n\nThis used support vector machine for fitting the curve.\n\n```python\nfrom sklearn.svm import SVR\nmodel = SVR(kernel = 'rbf')\nmodel.fit(X, y)\n```\n\n","1ba8a213":"# Use StatModels library","1576b9b8":"# Regressions\n\nWe predict y for given x values, it is a continuous prediction.\n\n## Linear Regression Theory\n\n**Linear Regression** is relation in **observations** ($x_i$ : set of variables) and **outcome** ($y$ : variable of interest) which tells us how x variables explain y. It is a line which has shortest distance to all possible outcomes from observation.\n\n### Covariance and Correlation\n\n**Covariance** is variation of data from the mean. The value depends on unit and is not standardized. \n\nTo calculate, find variation of each point from mean and then the product of variations. Then sum these variations and divide by n-1: \n\n$$Cov(Y,X) = \\frac{1}{n-1}\\sum_{i=1}^{n} (y_i - \\bar{y})(x_i - \\bar{x})$$\n\nHere, +\/- tells us relation but value does not tell the strenght as it varies with the unit of measure. We standardise values to find strength, in correlation.\n\n**Correlation** is how variables are linearly related to each other. It is standardized and does not change with the unit of X. The sign tells us +\/- relation and the value tells us the strength. $cor(Y,X) = 0$ does *not* mean that they are not related, it only means that they are *not linearly* related as they may have non-linear relation, like, $ Y = 50 - X^2 $.\n\nTo calculate divide the diff from mean by standard deviation:\n\n$$ Cor(Y,X) = \\frac{Cov(Y,X)}{S_xS_y} = \\frac{1}{n-1}\\sum_{i=1}^{n}\\left (\\frac{y_i - \\bar{y}}{S_y}  \\right )\\left (\\frac{x_i - \\bar{x}}{S_x}  \\right ) = \\frac{\\sum(y_i - \\bar{y})(x_i - \\bar{x})}{\\sqrt{\\sum(y_i - \\bar{y})^2\\sum(x_i - \\bar{x})^2}}  $$\n\n### Use of Linear Regression\n\n- **Relation**: To understand relation between X and Y, e.g. is sales related to advertisement, location, color etc? If so how, +\/-, less\/more.\n\n- **Prediction**: To make predictions about Y variable, once we train the model (equation) from test data we can then predict Y from new X varables.\n\n### Steps in Linear Regression\n\nWe often do, modelling, estimation, inference, prediction and evaluation. And them may be repeat the cycle with corrections.\n\n#### 1. Modelling (define): Develop a regression model\n\nThe initial step in doing linear regression is to model the data. In this, we **identify** the variables of interest that we think are of importance in expaining y. This is usually based on past experienece. The variables are **not exhaustive**. Simple linear regression is formulated by following equation.\n\n$$ Y = \\beta_0 + \\beta_1 X + \\varepsilon  $$\n\n$\\beta_k$ are called **regression coefficients**. We estimate these using the sample data.\n\n$\\epsilon$ is the **random error**. This occurs because we might be missing some important x variable or x and y variable might not be linearly related. This is difference of actual point from the fitted point. This is also measure of **goodness of fit** of the regression model.\n\n5 methods of building models:\n1. All-in - put all that you have to use, based on prior knowledge\/criteria\n2. Backward Eliminaiton - take all variables, fit, remove var with highest p-value, higher than significant level 0.05. Again repeat, fit remove, until highest p-value is less than significant elimination level.\n3. Forward Elimination - opporsite of 2.\n4. Bidirectional Elimination - mix fo 2 and 3.\n5. Score Comparision - make all possible combination of models. $2^n - 1$ models and find best based on criteria, accuracy, R sq, etc.\n\nModelling should take 95% of time, thinking about variables, x variables. The remaining steps below, estimation, inference and prediction, should take 5% of the time.\n\nFor example, \nprice\/advertising cost\/promotion costs\n- e.g. $UnitsSold = \\beta_0 + \\beta_1 Price + \\beta_2 AdExp + \\beta_3 PromExp + \\epsilon$\n\n\n#### 2. Estimation (fit): Using software to estimate the model\n\nPrior data is required for each X, it could be collected over time. Use excel\/r\/python to fit the model. Linear regression gives **estimates** of regression coefficients, or **predicted weights**, denoted by $b_0, b_1, ..., b_r$. We use **least squares method**, min sum of vertical distances, basically the software draws many lines and calculates sum of squares, then returns line for which the sum of squares is least. \n\n$$ \\hat\\beta_1 = \\frac{Cov(Y,X)}{Var(X)} = Cor(Y,X)\\frac{S_y}{S_x} $$\n\nHere, we have an **assumption** that Y and X are *linearly related* hence the estimated coefficient we got needs to be checked against this assumption before drawing any conclusion..\n\nIt gives us **estimated regression function**\n\n$$ y = f(X) = b_0 + b_1x_1 + ... + b_kx_k $$\n\nNow for each i'th observations, or i'th row, we can put in the function to get **estimated** or **predicted response**, $f(X_i)$. This should be as close to actual reponse $y_i$. The difference, $y_i - f(X_i)$ is called **residuals**. Now our aim is always to predict the beta estimates such that it minimizes the residuals, or deviation from actual y.\n\nTo get best predicted weights we minimize the **sum of squared residuals (SSR)**. for all i observations.\n\n$$ SSR = \\sum_i(y_i - f(X_i))^2 $$\n\nFor one x it is called simple linear regression, for more than one x, it is called **multiple linear regression**. In multiple linear regression with **two** x, it represents a **regression plane in a three-dimensional space**. The goal of regression is to determine the values of the weights $b_0, b_1 and b_k$ such that this plane is as close as possible to the actual responses and yield the minimal SSR. \n\nWe also get **regression output**.Also here, we should get expected sign of estimators, else there might be some **problem**, like, multi-colinearity.\n\n\n#### 3. Inference: Interpreting the estimated regression model\n\nInference in unerstanding the estimators and other stats from linear regression summary output. $\\beta_1$ is, when the $X_1$ variable **increases by one unit** then the Y variable **increases by $\\beta_1$ units**, all other variables in the model being kept at constant. So on for other betas.\n\nFor eg, $SalesUnits = -25096.83 - 5055.27Price + 648.61AdExp + 1802.61PromExp$. Here, $\\beta_0$ is fixed cost of production, without making any unit.\n\n$\\beta_0$ is the value of Y when all X are 0. **All $x_i$ are 0**  needs to be relevant. It can be relevant when we talk about fixed cost of production. So units produced can be 0 but still b0 would be fixed cost. But in our sales model, we cannot infer that when price of product is 0, then b0 is number of untis sold. We need to make **managerally relevant interpretation** of $beta_0$. To **improve $\\beta_0$ estimation** we need to adjust data. We can change data by, say, x* = x - mean of x. This will keep all coefficients same but will change $b_0$. In this case all x* will be 0 only if x is equal to mean of x. Hence, we are saying that, when our ad expediture is equal to average of ad expenditure in past two years then units sold is $b_0$. Rather than saying that when ad expenditre is zero in the original case. When we cannot make x equal to 0, for eg, $Income = b_0 + b_1Age$, here $b_0$ is **only to fit the beta** to the model but we **cannot make inference** that it is the minimum income, no!\n\n**Regression line through Origin** means that model has **no intercept** or $b_0$. The residuals do not necessarily add up to zero. SST != SSE + SSR and $R^2$ identities are also not true.\n\n\n#### 4. Prediction: Making predictions about Y variable\n\nWe can make predictions of Y value based on Xs, put value of Xs and beta estimators to get predicted Y value. For eg, create 3rd scenarios, how much to set price, how much to spend on ad and promotions, based on these scenarios we get Y that can help us decide on how to increase sales\/profit. The farther the X is from the X-bar mean, the larger is the Std Err. So we should take precaution if predicting value farther from our observations.\n\n#### 5. Evaluate: Determine how accurate the model's predictions are.\n\n**Quality of Fit** or goodness of fit tells us how good our model predicts. **Larger t, or smaller p**, means **strong** linear relationship b\/w x and y, use it to keep\/remove observations in model. The scatter plot of Y and Ycap can also tell us the strength. The closer the stronger, this is same as Cor(Y,Ycap).\n\nNeeds update.. We can also measure by computing,\n\n- SST, total sum of squared deviations (from y avg) = $(y_i - \\bar{y})^2$\n- SSR, sum of squares due to regression\n- SSE, smu of squared errors (residuals) = $(y_i - \\hat{y_i})^2$\n- They are related: SST = SSR + SSE\n\n**R Squared: Goodness of Fit**\n\n$$ R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST} = Cor(y,x)^2 = Cor(y,\\hat{y})^2 = R^2 $$\n\n0 < R^2 < 1, SSE<SST. Higher R^2 means strong linear relationship.\n\nAs SSE gets low, SSE\/SST is low, hence Rsq increases and better is fit. Greater is better.\n\n**Adjusted R Squared**\n\nNow in multiple linear regression, if we add a variable, then the Rqs will never decrease, because, the SST remains the same as mean is same, but the SSE will decrease, hence, R will increase or remain same. Rsq is always increasing hence biased on adding new variables.\n\n$$ Adj R^2 = 1 - (1 - R^2)\\frac{n-1}{n-p-1} $$\n\np - number of regressors\n\nn - sample size\n\nThis balances, Rsq increases and (n-p-1) increases hence penalises for adding variables. But if variable is worth it will inc Adj R sqrd.\n\nSo, if you add var and Adj Rsq is dec then question yourself and adjust the variables.\n\nRepeat steps, modelling, estimation, inference & prediction, again to improve the model.\n\n\n**Standardizing**\n\nWe might need to standardize the variables when we want to compare them for their effect on expaining Y. Standardizing makes them unit free, hence we can compare them. they can be standardized by doing $(x-\\mu)\/\\sigma$ for all X and Y. Then we can run the regression model and interpret the coefficients. Here, **one std dev change in X changes Y by b1 std dev**.\n\n**Missing Values**\n\nWe need to handle missing missing values in the data. THere are many techniques to do this. **Delete** if you have lots of data.\n\n\n### Assumptions\n\nAssumptions of a Linear Regression are:\n1. Linearity\n2. Homoscedasticity\n3. Multivariate normality\n4. Independence of errors\n5. Lack of multicollinearity\n\nThere are a few assumptions that we have considered when we have estimated the coefficients. One of them is that all $\\sum \\hat{y} - y$ or $\\epsilon$ is normally distributed around 0. The more we deviate from this assumption, the bad would our model fit.\n\n$$\\epsilon \\sim N(0,\\sigma)$$\n\nSince we have this assumption and we have estimated betas, we can do a hypothesis test around this.\n\nNow, CLT says that sample mean has Normal distribution with mean is population mean, and std is pop sd \/ root n, $ \\bar{x} \\sim N(\\mu,\\sigma\/\\sqrt{n}) $ . We actually never know the population mean. We estimate it from sample. Similarly we never know actual betas, we only estimate the bees from sample training data. Hence, all bees follow a norml distn with mean beta and std is std of beta.\n\n$$b_k \\sim N(\\beta_k, std_{b_k})$$\n\nSkipping lot of derivations, leads us to below equation that we can use for hypothesis testing.\n\n$$\\frac{b_0 - \\beta_0}{S_{b_0}} \\sim t_{n-k-1}$$\n\nHere, $S_{b_0}$ is std err of $b_0$, n is observations, k is number of x variables. \n\n\n### Hypothesis Tests\n\nHypothesis test is done to handle the uncertainities of the estimates from sample. $b_0$ is estimate of beta, hence it is not a true value and we need to  have a confidence interval around it. \n\nFor eg, Toy example, for ad_expense impact on units sold, the coefficient estimate is 600 but sales people say it should be around 300, then we have to check. The estimate is from sample, hence estimate and is uncertain, it is a belief, and it can be accepted or rejected. Hence, Hypothesis test. We need to **formulate a hypothesis test** for this. If SME says 1000 dollar increase in ad increses units sold by 300 units. Then, \n\nHo: beta2 = 300, this is claim made, hence mu\nHa: beta2 != 300\n\nMethod 1: $x = b_2 = 648.6 ; \\mu = \\beta_2 = 300 ; \\sigma = S_{b_2} = 209$ from regression output. Now it get back to std value and t-distribution.\nOur estimated coeff, b2, follows a t-dist, its std value = (x-mu)\/sigma, this is **t-statistic**. It is 1.67, or 1.67 std to the right of mu. Now on the graph, check if this lies in **rejection region** for a certain confidence level. Here, conf level is 95%, so alpha = 0.05, and it is two tail test, so we will use $\\alpha\/2=0.025$, deg of freedom is n-k-1, residuals. Use T.INV to get std value from probablity and deg of freedom, using `T.INV(0.025,20)` it gives cutoff value as -2.71. So if t-statistic is beyond +-2.71 then we are in rejection region, but as t-statistic does not lie in either region hence we fail to reject the NULL hypothesis. Thus, our data does not has enough evidence to not reject the NULL hypothesis. Hence, the claim may be true. Out data has much noise.\n\nMethod 2: **p-value** is probability that the sample mean would be >= sample mean, given NULL hypothesis (mean is somevalue) is true. Here, p-value is twice the value cutoff by t-statistic in two tail test. Use the t-statistic found above to calculate the p-value. if p-value is lower than alpha then we can reject the NULL hypothesis, p-value = 2X(1-T.DIST(1.67,20,TRUE)) = 0.11, since .11 > 0.05 alpha. Hence, we fail to reject the NULL hypothesis. Remember: **When p is low the NULL has to go**.\n\nMethod 3: Use **confidence interval** in regression output. If the claim in in CI, then we fail to reject the claim.\n\n#### Joint Hypothesis Test\n\nWhen we have to test if the the two independent variables have the same on effect on the dependent variable then we do a joint hypothesis, eg, if horsepower and weight have same effect on mileage. We have two betas in the hypo test. We need to **restrict the model** and calculate the f-statistic. We can restrict the model by combining the variables (sum). Note the Sum of Squares residual and degree of freedom from the regression output.\n\n$$ F Statistic = \\frac{(SS_R - SS_{UR})\/(df_R - df_{UR})}{SS_{UR}\/df_{UR}} $$\n\nThe F-statistic has F-distribution with $[(df_R - df_{UR}),df_{UR}]$. Calculate p-value from this F-stat then decide on the hypothesis.\n\nalso, check overlap of conf intervals,\n- if they overlap, then do a hypothesis test\n- if not then, reject,\n- if withing, the accept.\n\neg, if harvest output is our Y, then does 1cm increase in Rainfall has similar effect as 1 degree increase in temprature?\n\n\n### Regression Output\nEvery statistical software nowadays produces regression summary or output. This can answer many questions related to variables.\n\n#### R-Square\nIt tells how much percentage of Y is explained by the observations. Rsq may be low, then also model may have value as coefficients tell us the marginal impact. Rsq is imp but we should also consider the algebric signs of coefficients, their magniture should be plausible, they should be precisely estimated, and also check if you have missed any imp right-hand-side variable.\n\n#### P-Values\nThe p-values in regression output are the values for a hypothesis test in which we say that the true value of the **coefficient is 0**, i.e., weather one unit change in x has 0 impact on y. By looking at them, we can tell which observation is **significant in explaining** my Y variable. Usually, if p-value if **higher than 0.05** then the variable becomes **insignificant** in explaining Y.\n\n#### Standard Error\nThis is error of residuals. It follows a Normal(0,std dev), the estimation of this *std dev* is std err.\n\n#### Coefficients\nThey are unit dependent, their magnitude cannot be used for comparision. They tell us the change in Y variable for one unit change in them. If you need to compare then stanrdize them.\n\n#### Confidence Intervals\nThey tell us CI at a certain confidence percentage. They are CI for estimates of betas.\n\n\n#### F-Statistic\nThis is test of **joint-hypothesis**, that, Ho: B1 = B2 = B3 =0. Ha: Ho not true, or hypo that all var jointy have no impact on explaining Y variable. \n\n### Anomalies of Regression\nThere are many problems, like \n\n#### Multi-Colinearity\nWe might get high Rsq which means model is good fit, but also high p-value which makes observations insignificant or unexpected signs of coefficients, this might be **problem of multicollinearity**. It occurs when there is a nearly linear relationship among a set of explanatory variables, i.e., they are highly correlated, usually more than 90%. To solve it do a **pair-wise correlation** and just drop one of the correlated variable.\n\nIt might not be a problem when we only have to predict but not understand individual observation effect.\n\n\n","250b02bc":"## Polynomial Linear Regression\n\nWe can have a curve as best fit line between x and y. For this, we need parabolic line which can come by polynomial equation. It depends on case, may work better. \n\n$$ y = b_0 + b_1x_1 + b_2x_1^2 + ... + b_nx_1^n $$\n\nSill we find b0123.. which is why we still call it linear. Non-linear would be one that has combination of b0123.. as coefficient, like $b_1\/(b_2-b_3)$\n","0d06c353":"### Categorical Variables\n\n\nCategorical variables are columns on which we **cannot** do **mathematical operations** like, Gender, values are **simply labels**, male or female. Color of car, (red, black, blue, silver). Also, numerical variable like, **Quarter of year** (1,2,3,4), is categorical as we cannot do operations, like 4th quarter is not 2 times 2nd quarter. They are simply labels.\n\nThey can be helpful in reg analysis, for example, does color affect sale of cars? Now software cannot take colors as inputs to mathematical equations. So we use specific technique to incorporate catagorical var in regression model, we use **Dummy Variables**. It can take only 0 or 1.They are one less than the number of categories in the caregorical vaariables. Eg, we need 1 dummy var for gender. 3 for quarters, 3 for color. The category assigned to **0 becomes referece category**. Any inference is always in reference to each oher, eg, male earn more than female, regionA is far than regionB.\n\n**For Example:** We have deliveries to Region A, Region B and Region C. In this case, we need 2 dummy variables, or columns, regA, regB, or any combination of these. We take RegB and RegC as `RegB=IF(B2=\"B\",1,0)` where B2 col is region. Same for C. Now RegionA will always be **0**, it becomes **reference var**, interpretation of B and C will be w.r.t region A, it is our choice and can be changed. The model formed is:\n\n$$Minutes = \\beta_0 + \\beta_1RegionB + \\beta_2RegionC + \\beta_3Parcels$$","36bc6638":"## Decision Tree Regression\n\nUses decision tree algorithm to predict y. for eg, It divides the x1x2 plane into various splits, like x1<20 then this, then x2<160 then this and so on. Then based on all these splits we get regions on a plane or tree with leaves and nodes. It knows a y value which is average of all y_s in that region.\n\nThese splits happen based on information entropy.\n\nThe predicted y are not a line or curve but are constant values for an interval of x. So it is a step graph.","80deeea9":"## Using python library for Linear Regression.\n\nWe will be using Numpy to store data in array. We will use Scikit learn library to model data and apply linear regression on the data. MatplotLib and Seaborn is used for visualization. Statsmodel is used to get detailed summary of regression."}}