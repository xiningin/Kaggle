{"cell_type":{"07627f69":"code","74edb97b":"code","871bd3ec":"code","80b9bb89":"code","18105a4f":"code","10a495f6":"code","44bd9266":"code","fc988e2d":"code","c190b77f":"markdown","088dd169":"markdown","520e0c4e":"markdown","92ce5888":"markdown","cdb8f562":"markdown","8b321fa6":"markdown","d8fd6196":"markdown","6026fb3a":"markdown","682a3e8b":"markdown","f6f80f14":"markdown","9164f3ba":"markdown","38af24e2":"markdown"},"source":{"07627f69":"import tensorflow.keras\nfrom keras.models import Sequential\nfrom keras.models import load_model\nfrom keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\nimport numpy as np\nimport random\nfrom collections import deque","74edb97b":"# Action space include 3 actions: Buy, Sell, and Sit\n#Setting up the experience replay memory to deque with 1000 elements inside it\n#Empty list with inventory is created that contains the stocks that were already bought\n#Setting up gamma to 0.95, that helps to maximize the current reward over the long-term\n#Epsilon parameter determines whether to use a random action or to use the model for the action. \n#In the beginning random actions are encouraged, hence epsilon is set up to 1.0 when the model is not trained.\n#And over time the epsilon is reduced to 0.01 in order to decrease the random actions and use the trained model\n#We're then set the speed of decreasing epsililon in the epsilon_decay parameter\n\n#Defining our neural network:\n#Define the neural network function called _model and it just takes the keyword self\n#Define the model with Sequential()\n#Define states i.e. the previous n days and stock prices of the days\n#Defining 3 hidden layers in this network\n#Changing the activation function to relu because mean-squared error is used for the loss\n\n\nclass Agent:\n    \n    \n    def __init__(self, state_size, is_eval=False, model_name=\"\"):\n        self.state_size = state_size # normalized previous days\n        self.action_size = 3 # sit, buy, sell\n        self.memory = deque(maxlen=1000)\n        self.inventory = []\n        self.model_name = model_name\n        self.is_eval = is_eval\n        self.gamma = 0.95\n        self.epsilon = 1.0\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n        self.model = load_model(model_name) if is_eval else self._model()\n        \n        \n    def _model(self):\n        model = Sequential()\n        model.add(Dense(units=64, input_dim=self.state_size, activation=\"relu\"))\n        model.add(Dense(units=32, activation=\"relu\"))\n        model.add(Dense(units=8, activation=\"relu\"))\n        model.add(Dense(self.action_size, activation=\"linear\"))\n        model.compile(loss=\"mse\", optimizer=Adam(lr=0.001))\n        return model\n    def act(self, state):\n        if not self.is_eval and random.random()<= self.epsilon:\n            return random.randrange(self.action_size)\n        options = self.model.predict(state)\n        return np.argmax(options[0])\n    def expReplay(self, batch_size):\n        mini_batch = []\n        l = len(self.memory)\n        for i in range(l - batch_size + 1, l):\n            mini_batch.append(self.memory[i])\n        for state, action, reward, next_state, done in mini_batch:\n            target = reward\n            if not done:\n                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n            target_f = self.model.predict(state)\n            target_f[0][action] = target\n            self.model.fit(state, target_f, epochs=1, verbose=0)\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay","871bd3ec":"import math\n\n# prints formatted price\ndef formatPrice(n):\n\treturn (\"-$\" if n < 0 else \"$\") + \"{0:.2f}\".format(abs(n))\n\n# returns the vector containing stock data from a fixed file\ndef getStockDataVec(key):\n\tvec = []\n\tlines = open(\"\" + key + \".csv\", \"r\").read().splitlines()\n\n\tfor line in lines[1:]:\n\t\tvec.append(float(line.split(\",\")[4]))\n\n\treturn vec\n\n# returns the sigmoid\ndef sigmoid(x):\n\treturn 1 \/ (1 + math.exp(-x))\n\n# returns an an n-day state representation ending at time t\ndef getState(data, t, n):\n\td = t - n + 1\n\tblock = data[d:t + 1] if d >= 0 else -d * [data[0]] + data[0:t + 1] # pad with t0\n\tres = []\n\tfor i in range(n - 1):\n\t\tres.append(sigmoid(block[i + 1] - block[i]))\n\n\treturn np.array([res])\n","80b9bb89":"import sys\n\nif len(sys.argv) != 4:\n\tprint (\"Usage: python train.py [stock] [window] [episodes]\")\n\texit()\n\n\n# stock_name = input(\"Enter stock_name, window_size, Episode_count\")\n#Fill the given information when prompted: \n#Enter stock_name = GSPC_Training_Dataset\n#window_size = 10\n#Episode_count = 100 or it can be 10 or 20 or 30 and so on.\n\n# window_size = input()\n# episode_count = input()\n# stock_name = str(stock_name)\n# window_size = int(window_size)\n# episode_count = int(episode_count)\n\nwindow_size = 10\nepisode_count = 1\nstock_name = \"..\/input\/gspc-training-dataset\/GSPC_Evaluation_Dataset\"\n\nagent = Agent(window_size)\ndata = getStockDataVec(stock_name)\nl = len(data) - 1\nbatch_size = 32\n\nfor e in range(episode_count + 1):\n\tprint (\"Episode \" + str(e) + \"\/\" + str(episode_count))\n\tstate = getState(data, 0, window_size + 1)\n\n\ttotal_profit = 0\n\tagent.inventory = []\n\n\tfor t in range(l):\n\t\taction = agent.act(state)\n\n\t\t# sit\n\t\tnext_state = getState(data, t + 1, window_size + 1)\n\t\treward = 0\n\n\t\tif action == 1: # buy\n\t\t\tagent.inventory.append(data[t])\n\t\t\tprint (\"Buy: \" + formatPrice(data[t]))\n\n\t\telif action == 2 and len(agent.inventory) > 0: # sell\n\t\t\tbought_price = agent.inventory.pop(0)\n\t\t\treward = max(data[t] - bought_price, 0)\n\t\t\ttotal_profit += data[t] - bought_price\n\t\t\tprint (\"Sell: \" + formatPrice(data[t]) + \" | Profit: \" + formatPrice(data[t] - bought_price))\n\n\t\tdone = True if t == l - 1 else False\n\t\tagent.memory.append((state, action, reward, next_state, done))\n\t\tstate = next_state\n\n\t\tif done:\n\t\t\tprint (\"--------------------------------\")\n\t\t\tprint (\"Total Profit: \" + formatPrice(total_profit))\n\t\t\t\n\n\t\tif len(agent.memory) > batch_size:\n\t\t\tagent.expReplay(batch_size)\n\n\t#if e % 10 == 0:\n\t\tagent.model.save(\"model_ep\" + str(e))","18105a4f":"!pwd","10a495f6":"!kaggle kernels output shilpaar\/problem-statement-deep-q-learning","44bd9266":"!mkdir ..\/input\/saved-model\/variables","fc988e2d":"import sys\nfrom keras.models import load_model\n\n\nif len(sys.argv) != 3:\n\tprint (\"Usage: python evaluate.py [stock] [model]\")\n\texit()\n\n\n# stock_name = input(\"Enter Stock_name, Model_name\")\n# model_name = input()\n\n# stock_name = \"..\/input\/gspc-training-dataset\/GSPC_Evaluation_Dataset\"\n\n#Note: \n#Fill the given information when prompted: \n# Enter \nstock_name = \"..\/input\/gspc-training-dataset\/GSPC_Evaluation_Dataset\"\nmodel_name = \"model_ep1\"\n\nmodel = load_model(\"\" + model_name)\nwindow_size = model.layers[0].input.shape.as_list()[1]\n\nagent = Agent(window_size, True, model_name)\ndata = getStockDataVec(stock_name)\nl = len(data) - 1\nbatch_size = 32\n\nstate = getState(data, 0, window_size + 1)\ntotal_profit = 0\nagent.inventory = []\n\nfor t in range(l):\n\taction = agent.act(state)\n\n\t# sit\n\tnext_state = getState(data, t + 1, window_size + 1)\n\treward = 0\n\n\tif action == 1: # buy\n\t\tagent.inventory.append(data[t])\n\t\tprint (\"Buy: \" + formatPrice(data[t]))\n\n\telif action == 2 and len(agent.inventory) > 0: # sell\n\t\tbought_price = agent.inventory.pop(0)\n\t\treward = max(data[t] - bought_price, 0)\n\t\ttotal_profit += data[t] - bought_price\n\t\tprint (\"Sell: \" + formatPrice(data[t]) + \" | Profit: \" + formatPrice(data[t] - bought_price))\n\n\tdone = True if t == l - 1 else False\n\tagent.memory.append((state, action, reward, next_state, done))\n\tstate = next_state\n\n\tif done:\n\t\tprint (\"--------------------------------\")\n\t\tprint (stock_name + \" Total Profit: \" + formatPrice(total_profit))","c190b77f":"### **Train and build the model**","088dd169":"#**Stock Trading Using Deep Q-Learning**\n","520e0c4e":"### **Evaluate the model and agent**","92ce5888":"## **Solution**","cdb8f562":"### **Import the libraries** ","8b321fa6":"## **Problem Statement**","d8fd6196":"### **Create a DQN agent**","6026fb3a":"**Note: Run the training section for considerable episodes so that while evaluating the model it can generate significant profit.** \n","682a3e8b":"**Use the instruction below to prepare an agent**\n","f6f80f14":"<!-- import sys\nfrom keras.models import load_model\n\n\nif len(sys.argv) != 3:\n\tprint (\"Usage: python evaluate.py [stock] [model]\")\n\texit()\n\n\nstock_name = input(\"Enter Stock_name, Model_name\")\nmodel_name = input()\n#Note: \n#Fill the given information when prompted: \n#Enter stock_name = GSPC_Evaluation_Dataset\n#Model_name = respective model name\n\nmodel = load_model(\"\" + model_name)\nwindow_size = model.layers[0].input.shape.as_list()[1]\n\nagent = Agent(window_size, True, model_name)\ndata = getStockDataVec(stock_name)\nl = len(data) - 1\nbatch_size = 32\n\nstate = getState(data, 0, window_size + 1)\ntotal_profit = 0\nagent.inventory = []\n\nfor t in range(l):\n\taction = agent.act(state)\n\n\t# sit\n\tnext_state = getState(data, t + 1, window_size + 1)\n\treward = 0\n\n\tif action == 1: # buy\n\t\tagent.inventory.append(data[t])\n\t\tprint (\"Buy: \" + formatPrice(data[t]))\n\n\telif action == 2 and len(agent.inventory) > 0: # sell\n\t\tbought_price = agent.inventory.pop(0)\n\t\treward = max(data[t] - bought_price, 0)\n\t\ttotal_profit += data[t] - bought_price\n\t\tprint (\"Sell: \" + formatPrice(data[t]) + \" | Profit: \" + formatPrice(data[t] - bought_price))\n\n\tdone = True if t == l - 1 else False\n\tagent.memory.append((state, action, reward, next_state, done))\n\tstate = next_state\n\n\tif done:\n\t\tprint (\"--------------------------------\")\n\t\tprint (stock_name + \" Total Profit: \" + formatPrice(total_profit))\n -->","9164f3ba":"Prepare an agent by implementing Deep Q-Learning that can perform unsupervised trading in stock trade. The aim of this project is to train an agent that uses Q-learning and neural networks to predict the profit or loss by building a model and implementing it on a dataset that is available for evaluation.\n\n\nThe stock trading index environment provides the agent with a set of actions:<br>\n* Buy<br>\n* Sell<br>\n* Sit\n\nThis project has following sections:\n* Import libraries \n* Create a DQN agent\n* Preprocess the data\n* Train and build the model\n* Evaluate the model and agent\n<br><br>\n\n**Steps to perform**<br>\n\nIn the section **create a DQN agent**, create a class called agent where:\n* Action size is defined as 3\n* Experience replay memory to deque is 1000\n* Empty list for stocks that has already been bought\n* The agent must possess the following hyperparameters:<br>\n  * gamma= 0.95<br>\n  * epsilon = 1.0<br>\n  * epsilon_final = 0.01<br>\n  * epsilon_decay = 0.995<br>\n\n\n    Note: It is advised to compare the results using different values in hyperparameters.\n\n* Neural network has 3 hidden layers\n* Action and experience replay are defined\n\n\n","38af24e2":"### **Preprocess the stock market data**"}}