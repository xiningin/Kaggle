{"cell_type":{"7840bd0f":"code","b82b5e79":"code","59a7b77d":"code","6ec0c17d":"code","17cdb07c":"code","87a79eaa":"code","405c668f":"code","4e12d91e":"code","47a31d71":"markdown","1c53a1e0":"markdown","70a1b4aa":"markdown","dc8a42b0":"markdown","486f1e06":"markdown","1a5e090e":"markdown","ef18556a":"markdown","dfa4379a":"markdown","a731eacf":"markdown"},"source":{"7840bd0f":"import os\nimport numpy as np\nimport cv2\nimport pandas as pd\nimport tensorflow as tf\nimport gc\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom skimage.feature import canny\nfrom skimage import exposure\nimport matplotlib.pyplot as plt","b82b5e79":"def print_progress(it, mIoU,loss):\n    # Calculate the accuracy on the training-set.\n    now = time.strftime(\"%c\")\n    print(\"Iteration \" + str(it) + \" --- mIoU: \" + str(mIoU) + \" --- Loss: \" + str(loss) + \" --- \" + now);\n\n\ndef RLenc(img, order='F', format=True):\n    \"\"\"\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    \"\"\"\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  ## list of run lengths\n    r = 0  ## the current run length\n    pos = 1  ## count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n        r = 0\n\n    if format:\n        z = ''\n\n        for rr in runs:\n            z += '{} {} '.format(rr[0], rr[1])\n        return z[:-1]\n    else:\n        return runs\n\n\ndef create_submission(pred,test_fns,score, threshold):\n    pred_dict = {fn[:-4]: RLenc(np.round(pred[i, :, :, 0] > threshold)) for i, fn in enumerate(test_fns)};\n    sub = pd.DataFrame.from_dict(pred_dict, orient='index')\n    sub.index.names = ['id']\n    sub.columns = ['rle_mask']\n    sub.to_csv('submission_'+str(score)+'.csv')\n\n\ndef transform_images(x_train,y_train,depths):\n    x = [i for i in x_train];\n    y = [i for i in y_train];\n    d = [i for i in depths];\n\n    for i in range(len(x_train)):\n        (h, w) = x_train[i].shape[:2];\n        center = (w \/ 2, h \/ 2);\n        # flip h\n        x.append(np.array(cv2.flip(x_train[i],0)))\n        y.append(np.array(cv2.flip(y_train[i],0)))\n        d.append(depths[i]);\n        # flip v\n        x.append(np.array(cv2.flip(x_train[i], 1)))\n        y.append(np.array(cv2.flip(y_train[i], 1)))\n        d.append(depths[i]);\n        # flip h & v\n        x.append(np.array(cv2.flip(x_train[i], -1)))\n        y.append(np.array(cv2.flip(y_train[i], -1)))\n        d.append(depths[i]);\n        '''\n        # rotate 90\n        M = cv2.getRotationMatrix2D(center,90,1.0)\n        x.append(np.array(cv2.warpAffine(x_train[i], M,(h,w))))\n        y.append(np.array(cv2.warpAffine(y_train[i], M,(h,w))))\n        d.append(depths[i]);\n        # rotate 180\n        M = cv2.getRotationMatrix2D(center, 180, 1.0)\n        x.append(np.array(cv2.warpAffine(x_train[i], M, (h, w))))\n        y.append(np.array(cv2.warpAffine(y_train[i], M, (h, w))))\n        d.append(depths[i]);\n        # rotate 270\n        M = cv2.getRotationMatrix2D(center, 270, 1.0)\n        x.append(np.array(cv2.warpAffine(x_train[i], M, (h, w))))\n        y.append(np.array(cv2.warpAffine(y_train[i], M, (h, w))))\n        d.append(depths[i]);\n        '''\n    return x, y, d;\n\n\ndef conv_layer(input, filters, kernel_size, strides, k_init, k_reg, activation=tf.nn.relu, dropout=0.,\n               p_type=None,p_size=(2,2),p_stride=(2,2)):\n\n    l = tf.layers.conv2d(inputs=input, filters=filters, kernel_size=kernel_size, strides=strides, activation=activation,\n                         kernel_initializer=k_init, kernel_regularizer=k_reg,padding='same');\n\n    if p_type == None:\n        d = tf.layers.dropout(inputs=l, rate=dropout);\n    elif p_type == 'avg':\n        a = tf.layers.average_pooling2d(inputs=l, pool_size=p_size, strides=p_stride);\n        d = tf.layers.dropout(inputs=a, rate=dropout);\n    elif p_type == 'max':\n        m = tf.layers.max_pooling2d(inputs=l, pool_size=p_size, strides=p_stride);\n        d = tf.layers.dropout(inputs=m, rate=dropout);\n\n    print(\"Layer created with shape: \" + str(d.shape))\n\n    return d;\n\n\ndef convt_layer(input, filters, kernel_size, strides, k_init, k_reg, activation=tf.nn.relu, dropout=0.,\n               p_type=None,p_size=(2,2),p_stride=(2,2)):\n\n    l = tf.layers.conv2d_transpose(inputs=input, filters=filters, kernel_size=kernel_size, strides=strides,\n                                   activation=activation,kernel_initializer=k_init, kernel_regularizer=k_reg, padding='same');\n\n    if p_type == None:\n        d = tf.layers.dropout(inputs=l, rate=dropout);\n    elif p_type == 'avg':\n        a = tf.layers.average_pooling2d(inputs=l, pool_size=p_size, strides=p_stride);\n        d = tf.layers.dropout(inputs=a, rate=dropout);\n    elif p_type == 'max':\n        m = tf.layers.max_pooling2d(inputs=l, pool_size=p_size, strides=p_stride);\n        d = tf.layers.dropout(inputs=m, rate=dropout);\n\n    print(\"Layer created with shape: \" + str(d.shape))\n\n    return d;","59a7b77d":"class Data():\n    def __init__(self):\n        self.width = 128;\n        self.channels = 1;\n        self.x_train = None;\n        self.y_train = None;\n        self.depths = None;\n        self.x_val = None;\n        self.y_val = None;\n        self.x_test = None;\n        self.next_batch = 0;\n        self.end_epoch = False;\n        self.test_fns = None;\n        self.feats = None;\n\n    def load_train(self):\n        print(\"Loading train data...\");\n        TRAIN_IMAGE_DIR = '..\/input\/train\/images\/'\n        TRAIN_MASK_DIR = '..\/input\/train\/masks\/'\n\n        train_fns = os.listdir(TRAIN_IMAGE_DIR)\n\n        depths = pd.read_csv('..\/input\/depths.csv');\n        max_depth = np.max(depths['z'].values);\n        print('Max depth: ' + str(max_depth));\n        depths['z'] = np.array(depths['z'].values)\/max_depth;\n\n        x_train = [np.array(cv2.resize(cv2.imread(TRAIN_IMAGE_DIR + p, cv2.IMREAD_GRAYSCALE),(128,128)),dtype=np.uint8) for p in train_fns]\n        x_train = [exposure.equalize_adapthist(x) for x in x_train];\n        x_train = np.array(x_train \/ 255\n        #x_train = np.expand_dims(x_train, axis=3)\n\n        y_train = [np.array(cv2.resize(cv2.imread(TRAIN_MASK_DIR + p, cv2.IMREAD_GRAYSCALE),(128,128)),dtype=np.uint8) for p in train_fns]\n        y_train = np.array(y_train) \/ 255\n        #y_train = np.expand_dims(y_train, axis=3)\n\n        self.width = x_train.shape[1];\n\n        depths = [np.full((self.width,self.width),depths[depths['id'] == p[:-4]]['z'].values) for p in train_fns];\n        #depths = np.expand_dims(depths, axis=3)\n\n        #self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(self.x_train, self.y_train,\n        #                                                                      random_state=23, test_size=0.2)\n\n        x_train, y_train, depths = transform_images(x_train,y_train,depths);\n\n        feats = [canny(x) for x in x_train];\n\n        self.x_train = np.array(x_train);\n        del x_train\n        gc.collect()\n        self.y_train = np.array(y_train);\n        del y_train\n        gc.collect()\n        self.depths = np.array(depths);\n        del depths\n        gc.collect()\n        self.feats = np.array(feats);\n        del feats\n        gc.collect()\n\n        self.x_train = np.expand_dims(self.x_train, axis=3)\n        self.y_train = np.expand_dims(self.y_train, axis=3)\n        self.depths = np.expand_dims(self.depths, axis=3)\n        self.feats = np.expand_dims(self.feats, axis=3)\n        self.channels = self.x_train.shape[3];\n\n        self.shuffleTrainData();\n\n        print(\"Finish loading!\");\n        print(\"x_train shape:\");\n        print(self.x_train.shape);\n        print(\"y_train shape:\");\n        print(self.y_train.shape);\n        print(\"depths shape:\");\n        print(self.depths.shape);\n        print(\"feats shape:\");\n        print(self.feats.shape);\n\n    def load_test(self):\n        del self.x_train, self.y_train\n        gc.collect()\n        self.x_train = None;\n        self.y_train = None;\n        gc.collect()\n\n        print(\"Loading test data...\");\n        TEST_IMAGE_DIR = '..\/input\/test\/images\/'\n\n        self.test_fns = os.listdir(TEST_IMAGE_DIR)\n\n        depths = pd.read_csv('..\/input\/depths.csv');\n        max_depth = np.max(depths['z'].values);\n        print('Max depth: ' + str(max_depth));\n        depths['z'] = np.array(depths['z'].values) \/ max_depth;\n\n        self.depths = depths;\n\n        self.next_batch = 0;\n        self.end_epoch = False;\n\n        print(\"Finish loading!\");\n\n    def getNextTrainBatch(self,batch_size):\n        init = int(self.next_batch * batch_size);\n        end = int(init + batch_size);\n\n        self.next_batch += 1;\n\n        if end > len(self.x_train):\n            end = int(len(self.x_train));\n            init = int(end - batch_size);\n            self.end_epoch = True;\n            self.next_batch = 0;\n\n        x = self.x_train[init:end];\n        y = self.y_train[init:end];\n        z = self.depths[init:end];\n        f = self.feats[init:end];\n\n        return x, y, z, f;\n\n    def getNextTestBatch(self,batch_size):\n        TEST_IMAGE_DIR = '..\/input\/test\/images\/';\n        init = int(self.next_batch * batch_size);\n        end = int(init + batch_size);\n\n        self.next_batch += 1;\n\n        if end >= len(self.test_fns):\n            end = int(len(self.test_fns));\n            self.end_epoch = True;\n            self.next_batch = 0;\n\n        test_fns = self.test_fns[init:end];\n        #print(\"Length test_fns: \" + str(len(test_fns)));\n        x_test = [\n            np.array(cv2.resize(cv2.imread(TEST_IMAGE_DIR + p, cv2.IMREAD_GRAYSCALE), (128, 128)), dtype=np.uint8) for p\n            in test_fns]\n\n        x_test = [exposure.equalize_adapthist(x) for x in x_test];\n\n        x_test = np.array(x_test) \/ 255\n\n        feats = [canny(x) for x in x_test];\n\n        x_test = np.expand_dims(x_test, axis=3)\n        #print(\"Length x_test: \" + str(len(x_test)));\n        width = x_test.shape[1];\n\n        depths = [np.full((width, width), self.depths[self.depths['id'] == p[:-4]]['z'].values) for p in test_fns];\n        depths = np.expand_dims(depths, axis=3)\n\n        feats = np.expand_dims(feats, axis=3)\n\n        return x_test, depths, feats, np.arange(init,end,1);\n\n    def shuffleTrainData(self):\n        rng_state = np.random.get_state();\n        np.random.shuffle(self.x_train);\n        np.random.set_state(rng_state);\n        np.random.shuffle(self.y_train);\n        np.random.set_state(rng_state);\n        np.random.shuffle(self.depths);\n        np.random.set_state(rng_state);\n        np.random.shuffle(self.feats);\n        self.next_batch = 0;\n        return;","6ec0c17d":"batch_size = 100;\ndropout_rate = .0;\nlearning_rate = .01;\nlr_decay = .9;\nreg = .0;\nmax_epochs = 1;\nsalt_threshold = .5;\n\ndata = Data();\ndata.load_train();","17cdb07c":"tf.reset_default_graph();\ntf.logging.set_verbosity(tf.logging.ERROR)\n# inputs\nwith tf.variable_scope('input'):\n    x = tf.placeholder(tf.float32, shape=[None, data.width, data.width, data.channels], name='x');\n    y_true = tf.placeholder(tf.float32, shape=[None, data.width, data.width, data.channels], name='y_true');\n    z = tf.placeholder(tf.float32, shape=[None, data.width, data.width, data.channels], name='z');\n    feats = tf.placeholder(tf.float32, shape=[None, data.width, data.width, 1], name='feats');\n    lr = tf.placeholder(tf.float32, name='learning_rate');\n    dropout = tf.placeholder(tf.float32, name='dropout');\n\nx_ = tf.concat([x,z,feats],axis=3);\n\nprint(\"Total input shape: \" + str(x_.shape));\n\ntf_batchsize = tf.shape(x)[0];\nxavier = tf.contrib.layers.xavier_initializer();\nreg_regr = tf.contrib.layers.l2_regularizer(scale=reg)\n\n# conv layers\nwith tf.variable_scope('encoder'):\n    e1 = conv_layer(input=x_, filters=8, kernel_size=(3,3), strides=(1,1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n    e2 = conv_layer(input=e1, filters=8, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout,p_type='avg');\n\n    e3 = conv_layer(input=e2, filters=16, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n    e4 = conv_layer(input=e3, filters=16, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout, p_type='avg');\n\n    e5 = conv_layer(input=e4, filters=32, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n    e6 = conv_layer(input=e5, filters=32, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout, p_type='avg');\n\n    e7 = conv_layer(input=e6, filters=64, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n    e8 = conv_layer(input=e7, filters=64, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout, p_type='avg');\n\n    e9 = conv_layer(input=e8, filters=128, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n    e10 = conv_layer(input=e9, filters=128, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n\nwith tf.variable_scope('decoder'):\n    d1 = convt_layer(input=e10, filters=64, kernel_size=(2,2), strides=(2,2), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n\n    c1 = tf.concat([d1,e7],axis=3);\n    d2 = conv_layer(input=c1, filters=64, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n    d3 = conv_layer(input=d2, filters=64, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                         dropout=dropout);\n\n    d4 = convt_layer(input=d3, filters=32, kernel_size=(2,2), strides=(2,2), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n\n    c2 = tf.concat([d4,e5],axis=3);\n    d5 = conv_layer(input=c2, filters=32, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n    d6 = conv_layer(input=d5, filters=32, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n\n    d7 = convt_layer(input=d6, filters=16, kernel_size=(2,2), strides=(2,2), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n\n    c3 = tf.concat([d7, e3], axis=3);\n    d8 = conv_layer(input=c3, filters=16, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n    d9 = conv_layer(input=d8, filters=16, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n\n    d10 = convt_layer(input=d9, filters=8, kernel_size=(2, 2), strides=(2, 2), k_init=xavier, k_reg=reg_regr,\n                         dropout=dropout);\n\n    c4 = tf.concat([d10, e1], axis=3);\n    d11 = conv_layer(input=c4, filters=8, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n    d12 = conv_layer(input=d11, filters=8, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n\nwith tf.variable_scope('output'):\n    y_pred = convt_layer(input=d12, filters=1, kernel_size=(1,1), strides=(1,1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout,activation=tf.nn.sigmoid);\n\nwith tf.variable_scope('loss'):\n    y_true_flat = tf.reshape(y_true,[tf_batchsize,-1]);\n    y_pred_flat = tf.reshape(y_pred, [tf_batchsize, -1]);\n    loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=y_true_flat,\n                                               logits=y_pred_flat);\n        #loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true,logits=y_pred);\n\nwith tf.variable_scope('metrics'):\n    IoU, IoU_op = tf.metrics.mean_iou(labels=tf.cast(y_true,tf.int32),\n                                          predictions=tf.to_int32(y_pred > salt_threshold),num_classes=2,name='mIoU');\n\noptimizer = tf.train.AdamOptimizer(learning_rate=lr, name='op').minimize(loss);","87a79eaa":"    session = tf.Session()\n    session.run(tf.global_variables_initializer())\n    session.run(tf.local_variables_initializer())","405c668f":"print(\"Starting training...\");\n\nepoch = 1;\nlr = learning_rate;\nstep = 0;\nmIoU = [];\nmIoU_over_steps = [];\nxent = [];\nloss_over_steps = [];\n\nfeed_dict_val = {'input\/x:0': data.x_val, 'input\/y_true:0': data.y_val, 'input\/dropout:0': 0};\n\nwhile epoch <= max_epochs:\n\n    x_batch, y_batch, z_batch, f_batch = data.getNextTrainBatch(batch_size);\n\n    feed_dict_train = {'input\/x:0': x_batch, 'input\/y_true:0': y_batch, 'input\/z:0': z_batch,\n                           'input\/feats:0': f_batch,'input\/learning_rate:0': lr,\n                           'input\/dropout:0': dropout_rate};\n\n    _ = session.run(['op'], feed_dict=feed_dict_train);\n    _ = session.run([IoU_op], feed_dict=feed_dict_train);\n    mIoU,xent = session.run([IoU,loss], feed_dict=feed_dict_train);\n    mIoU_over_steps.append(mIoU);\n    loss_over_steps.append(xent);\n\n    if step % 10 == 0:\n            #_ = session.run([IoU_op], feed_dict=feed_dict_val);\n            #mIoU, xent = session.run([IoU, loss], feed_dict=feed_dict_val);\n        print_progress(step, np.mean(mIoU_over_steps), np.mean(loss_over_steps));\n        mIoU_over_steps = [];\n        loss_over_steps = [];\n        plt.imshow(y_batch[0]);\n    if data.end_epoch:\n        print('End of epoch ' + str(epoch));\n        epoch += 1;\n        lr *= lr_decay;\n        data.end_epoch = False;\n        data.shuffleTrainData();\n\n    step += 1;\n\ndel data\ngc.collect()","4e12d91e":"data = Data();\ndata.load_test();\nsub = np.zeros((len(data.test_fns), data.width, data.width, data.channels));\n\nwhile not data.end_epoch:\n    x, z, f, idx = data.getNextTestBatch(batch_size);\n    feed_dict_test = {'input\/x:0': x, 'input\/z:0': z, 'input\/feats:0': f, 'input\/dropout:0': 0};\n    sub[idx] = session.run([y_pred],feed_dict_test);\n\nsession.close();\n\nsubs_resized = [];\n\nfor i in range(len(sub)):\n    subs_resized.append(np.array(cv2.resize(sub[i],dsize=(101,101)),dtype=np.int8));\n\nsubs_resized = np.array(subs_resized);\nsubs_resized = np.expand_dims(subs_resized, axis=3)\nprint(subs_resized.shape)\n\ncreate_submission(subs_resized,data.test_fns,mIoU,salt_threshold)","47a31d71":"## Initializing variables","1c53a1e0":"## Imports","70a1b4aa":"## Parameters and data loading","dc8a42b0":"## Building the computational graph","486f1e06":"## Usefull functions","1a5e090e":"## Data class","ef18556a":"## Starting training","dfa4379a":"## Introduction\nThis is an attempt to create a benchmark using Tensorflow and the Unet. **The 0.38 score is because the kernel is not training at all (actually, it predicts all 0 at the output).** Any help to solve this problem would be really appreciated.","a731eacf":"## Predicting"}}