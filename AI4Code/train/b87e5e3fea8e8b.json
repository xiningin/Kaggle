{"cell_type":{"a8708da6":"code","63bb361a":"code","7a2b4b66":"code","7391b4e2":"code","2389d119":"code","9784ac26":"code","6742e87e":"code","f8f89d3a":"code","2ca7fe87":"code","07d124af":"code","da075136":"code","893fa5ab":"code","4cc61904":"code","8ce8ac0f":"code","bd5c8f02":"code","f4269dac":"code","883b68e8":"code","aa9094fd":"code","8072db86":"code","7558f6b0":"code","019fdf57":"code","1c279e6c":"code","0610b19a":"code","c1ed2643":"code","a85f4139":"code","266a6a3d":"code","8db62b3f":"code","9becf76c":"code","35083664":"code","f79a46a3":"code","410095f6":"code","af40582f":"code","24662839":"code","68530bc8":"code","68b0622f":"code","9d71018d":"code","c57256d7":"code","39956dd7":"code","e2176774":"code","aa8a1af8":"code","9ea05488":"markdown","4271137a":"markdown","01d6f365":"markdown","e4f34a66":"markdown","6828caf5":"markdown","9f6f8618":"markdown","caf091c4":"markdown","d5000773":"markdown"},"source":{"a8708da6":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.svm import SVR","63bb361a":"train = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')\nsub = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')","7a2b4b66":"train.head()","7391b4e2":"test.head()","2389d119":"print(\"Training data shape:\",train.shape)\nprint(\"Test data shape:\",test.shape)","9784ac26":"#checking for null values\nprint(train.isna().sum().sum())\nprint(test.isna().sum().sum())","6742e87e":"train.drop('id',axis=1,inplace=True)\ntest.drop('id',axis=1,inplace=True)","f8f89d3a":"cols = train.columns.tolist()","2ca7fe87":"plt.figure(figsize=(24, 6*(104\/4)))\nfor i in range(len(train.columns.tolist())):\n    plt.subplot(26, 4, i+1)\n    if i <= 99:\n        plt.hist(train[f'f{i}'])\n        plt.xlabel(f'f{i}')\n    else:\n        plt.hist(train['loss'])\n        plt.xlabel('Loss')\nplt.show()","07d124af":"corr = train.corr()\nplt.figure(figsize=(20,20))\nsns.heatmap(corr)\nplt.show()","da075136":"print(\"Training data shape after droping ID colunmn:\",train.shape)\nprint(\"Test data shape after droppig ID column:\",test.shape)","893fa5ab":"cols = test.columns","4cc61904":"X = train[cols]\ny = train['loss']\ntest = test","8ce8ac0f":"X.head()","bd5c8f02":"test.head()","f4269dac":"y.head()","883b68e8":"#scaling the data \nss = StandardScaler()\nX_scaled = ss.fit_transform(X)\ntest_scaled = ss.fit_transform(test)","aa9094fd":"train_oof = np.zeros((train.shape[0],))\ntest_preds = np.zeros((test.shape[0],))","8072db86":"train_oof.shape","7558f6b0":"test_preds.shape","019fdf57":"n_splits = 5\nkf = KFold(n_splits=n_splits, random_state=137, shuffle=True)\n\nfor j, (train_index, val_index) in enumerate(kf.split(train)):\n    print(\"Fitting fold\", j+1)\n    train_features = X_scaled[train_index]\n    train_target = y[train_index]\n\n    \n    val_features = X_scaled[val_index]\n    val_target = y[val_index]\n\n    \n    model = LinearRegression()\n    model.fit(train_features, train_target)\n    val_pred = model.predict(val_features)\n    train_oof[val_index] = val_pred.flatten()\n    test_preds += model.predict(test_scaled).flatten()\ntest_preds = test_preds\/n_splits","1c279e6c":"print(\"RMSE for Linear Regression model\",np.sqrt(mse(y,train_oof)))","0610b19a":"train_oof_ridge = np.zeros((train.shape[0],))\ntest_preds_ridge = np.zeros((test.shape[0],))","c1ed2643":"kf = KFold(n_splits=n_splits, random_state=137, shuffle=True)\n\nfor j, (train_index, val_index) in enumerate(kf.split(train)):\n    print(\"Fitting fold\", j+1)\n    train_features = X_scaled[train_index]\n    train_target = y[train_index]\n\n    \n    val_features = X_scaled[val_index]\n    val_target = y[val_index]\n\n    \n    model = Ridge(alpha = 0.25)\n    model.fit(train_features, train_target)\n    val_pred = model.predict(val_features)\n    train_oof_ridge[val_index] = val_pred.flatten()\n    test_preds_ridge += model.predict(test_scaled).flatten()\ntest_preds_ridge = test_preds_ridge \/ n_splits","a85f4139":"print(\"RMSE for Ridge Regression model\",np.sqrt(mse(y,train_oof_ridge)))","266a6a3d":"train_oof_lasso = np.zeros((train.shape[0],))\ntest_preds_lasso = np.zeros((test.shape[0],))","8db62b3f":"kf = KFold(n_splits=n_splits, random_state=137, shuffle=True)\n\nfor j, (train_index, val_index) in enumerate(kf.split(train)):\n    print(\"Fitting fold\", j+1)\n    train_features = X_scaled[train_index]\n    train_target = y[train_index]\n\n    \n    val_features = X_scaled[val_index]\n    val_target = y[val_index]\n\n    \n    model = Lasso(alpha = 0.25)\n    model.fit(train_features, train_target)\n    val_pred = model.predict(val_features)\n    train_oof_lasso[val_index] = val_pred.flatten()\n    test_preds_lasso += model.predict(test_scaled).flatten()\ntest_preds_lasso = test_preds_lasso \/ n_splits","9becf76c":"print(\"RMSE for lasso Regression model\",np.sqrt(mse(y,train_oof_lasso)))","35083664":"import tensorflow as tf\nfrom tensorflow import keras","f79a46a3":"model = tf.keras.Sequential([\n    tf.keras.layers.Dense(1024,input_dim=X_scaled.shape[1],kernel_initializer='normal',activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(512,activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(256,activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(128,activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(64,activation='relu'),\n    tf.keras.layers.Dense(1,activation='relu')\n])\nmodel.summary()","410095f6":"adam = tf.keras.optimizers.Adam()\nmodel.compile(loss='mean_squared_error',optimizer=adam)","af40582f":"model.fit(X_scaled,y,validation_split=0.25,epochs=5,verbose=1,shuffle=True)","24662839":"train_pred = model.predict(X_scaled)","68530bc8":"train_pred.shape","68b0622f":"print(\"RMSE for Neural Network Model\",np.sqrt(mse(y,train_pred)))","9d71018d":"y_pred = model.predict(test_scaled)","c57256d7":"sub","39956dd7":"sub['loss'] = y_pred","e2176774":"sub","aa8a1af8":"sub.to_csv('submission.csv',index=False)","9ea05488":"A lot more can be done using neural network.\n* Increasing the no. of layers.\n* Increasing no. of neurons in the layers.\n* Using dropout to handle overfitting.\n\nTill now NN works better the the basic regression model such as Linear Regression, Ridge or Lasso.\n\nLet me know in the comment if you have some idea to imporve the model.","4271137a":"Relationship between the features and loss is very low.","01d6f365":"Linear Regresssion Model","e4f34a66":"Neural Network Model","6828caf5":"Lasso Regersssion","9f6f8618":"Ridge Regression Model","caf091c4":"#### Model Training ","d5000773":"As we can see from the above plots lots of features are left and right skewed.\nAnd looking at the plots we can say none of the feature is a categorial feature all are continous."}}