{"cell_type":{"bd7a6ba0":"code","e5ff9e25":"code","fe207b04":"code","3eca1041":"code","508359cf":"code","de889f67":"code","b360b944":"code","464f84a1":"code","bbf07de8":"code","d36dd25d":"code","3118ab11":"code","eead0963":"code","f392152f":"code","fa235351":"code","996b1704":"code","37e6c382":"code","4f669b00":"code","50b641c8":"code","9a1d7a61":"code","650cf8f0":"code","c8a4b094":"code","819bd3d1":"code","5ff42029":"code","09ac6025":"code","152e73f2":"code","7758a41e":"code","c91251f6":"code","8644e574":"code","2b9a40ec":"code","d66b102f":"code","e71468db":"code","dcc186dc":"code","9d9a4e8e":"code","4c996b64":"code","4d7ece7d":"markdown","653a4074":"markdown","85ee4812":"markdown","82601af2":"markdown","85832267":"markdown","3774c7b9":"markdown","5066a2b9":"markdown","616c54d3":"markdown","0b303d2b":"markdown","d702d4d8":"markdown","d387a311":"markdown"},"source":{"bd7a6ba0":"import numpy as np\nimport pandas as pd\nimport os\nimport time\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score","e5ff9e25":"os.listdir(\"..\/input\")","fe207b04":"train_df = pd.read_csv(\"..\/input\/movie-reviews-classification\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/movie-reviews-classification\/test.csv\")","3eca1041":"train_df.head()","508359cf":"test_df.head()","de889f67":"print(f\"Datase shape: train: {train_df.shape}; test: {test_df.shape}\")","b360b944":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","464f84a1":"missing_data(train_df)","bbf07de8":"missing_data(test_df)","d36dd25d":"def plot_features_distribution(features, title, df, isLog=False):\n    plt.figure(figsize=(12,6))\n    plt.title(title)\n    for feature in features:\n        if(isLog):\n            sns.distplot(np.log1p(df[feature]),kde=True,hist=False, bins=120, label=feature)\n        else:\n            sns.distplot(df[feature],kde=True,hist=False, bins=120, label=feature)\n    plt.xlabel('')\n    plt.legend()\n    plt.show()\n\ndef plot_count(feature, title, df, size=1):\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:30], palette='Set3')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    if(size > 2):\n        plt.xticks(rotation=90, size=8)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n    plt.show() \n\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(feature,df):\n    data = df.loc[~df[feature].isnull(), feature].values\n    count = (~df[feature].isnull()).sum()\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=200,\n        max_font_size=40, \n        scale=5,\n        random_state=1\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(10,10))\n    plt.axis('off')\n    fig.suptitle(\"Prevalent words in {} ({} rows)\".format(feature,count), fontsize=20)\n    fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()\n\ndef show_confusion_matrix(valid_y, predicted, size=1, trim_labels=False):\n    mat = confusion_matrix(valid_y, predicted)\n    plt.figure(figsize=(4*size, 4*size))\n    sns.set()\n    target_labels = np.unique(valid_y)\n    if(trim_labels):\n        target_labels = [x[0:70] for x in target_labels]\n    sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n                xticklabels=target_labels,\n                yticklabels=target_labels\n               )\n    plt.xlabel('true label')\n    plt.ylabel('predicted label')\n    plt.show()","3118ab11":"show_wordcloud('primary_title', train_df)","eead0963":"show_wordcloud('primary_title', test_df)","f392152f":"show_wordcloud('original_title', train_df)","fa235351":"show_wordcloud('original_title', test_df)","996b1704":"show_wordcloud('text', train_df)","37e6c382":"show_wordcloud('text', test_df)","4f669b00":"plot_features_distribution(['start_year', 'end_year'], 'Running years distribution (train)', train_df)","50b641c8":"plot_features_distribution(['start_year', 'end_year'], 'Running years distribution (test)', test_df)","9a1d7a61":"plot_features_distribution(['runtime_minutes'], 'Runtime minutes distribution (train)', test_df)","650cf8f0":"plot_features_distribution(['runtime_minutes'], 'Runtime minutes distribution (test)', test_df)","c8a4b094":"print(f\"Movies in train: {train_df.title_id.nunique()} and test: {test_df.title_id.nunique()}\")\nl1 = set(train_df.title_id.unique())\nl2 = set(test_df.title_id.unique())\ncard_int = len(l1.intersection(l2))\nprint(f\"Common movies in train & test: {card_int}\")","819bd3d1":"plot_count('is_adult', 'Adult movie (train)', train_df, size=1)","5ff42029":"plot_count('is_adult', 'Adult movie (test)', test_df, size=1)","09ac6025":"def count_vect_feature(feature, df, max_features=5000):\n    start_time = time.time()\n    cv = CountVectorizer(max_features=max_features,\n                             ngram_range=(1, 3),\n                             stop_words='english')\n    X_feature = cv.fit_transform(df[feature])\n    print('Count Vectorizer `{}` completed in {} sec.'.format(feature, round(time.time() - start_time,2)))\n    return X_feature\n","152e73f2":"data = pd.concat([train_df, test_df])","7758a41e":"data.shape","c91251f6":"X_feature = count_vect_feature('text', data, max_features=5000)","8644e574":"X = X_feature[0:train_df.shape[0]]\nX_test = X_feature[train_df.shape[0]:]\ny = train_df.polarity.values\nprint(f\"X: {X.shape} test X: {X_test.shape} y: {y.shape}\")\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size = 0.3, random_state = 42) \nprint(f\"train X: {train_X.shape}, valid X: {valid_X.shape}, train y: {train_y.shape}, valid y: {valid_y.shape}\")","2b9a40ec":"clf = MultinomialNB(fit_prior=True)","d66b102f":"clf.fit(train_X, train_y)","e71468db":"predicted_valid = clf.predict(valid_X)\nshow_confusion_matrix(valid_y, predicted_valid, size=1)\nprint(classification_report(valid_y, predicted_valid))","dcc186dc":"print(f\"ROC-AUC: {roc_auc_score(predicted_valid, valid_y)}\")","9d9a4e8e":"predict_test = clf.predict(X_test)","4c996b64":"submission = pd.read_csv('..\/input\/movie-reviews-classification\/sampleSubmission.csv')\nsubmission['polarity'] = predict_test\nsubmission.to_csv('submission.csv', index=False)","4d7ece7d":"Only one movie is common between `train` and `test`.","653a4074":"# Read the data","85ee4812":"# Baseline model\n\nWe will create a very simple model using only review text data, based on Count vectorizer, without text data pre-processing.","82601af2":"## Missing data","85832267":"# Load packages","3774c7b9":"Missing data are ~5% of `end_year` in both train and test as well as ~1% `runtime_minutes`.","5066a2b9":"Let's calculate the validation score using the metric for this competition, ROC-AUC:","616c54d3":"## Check the data","0b303d2b":"# Data exploration","d702d4d8":"# Submission\n\nLet's predict the polarity for the test set.","d387a311":"# Further improvements\n\n* Pre-process the text data: clean contraction, clean special characters, eliminate stop words, use lematization or stemming;\n* Use different text vectorizing options;\n* Use other models;\n* Add additional features (categorical or numerical);\n* Perform model tuning\n"}}