{"cell_type":{"ae7cabe4":"code","ea716534":"code","895ce765":"code","c7a5372a":"code","ae6575c9":"code","9ae49449":"code","9c154a80":"code","d84ab8b7":"code","1d23e1b5":"code","3052cc29":"code","84f23c70":"code","30fc8ca8":"code","8d69b20c":"code","383f4ed3":"code","f9e71df9":"code","8a4c2d93":"code","6dd9259e":"code","a30149e0":"code","c2e401b9":"code","510e38a6":"markdown","b12e3b62":"markdown","6c89e29a":"markdown","76614eee":"markdown","3f02aa42":"markdown","ceed7766":"markdown","84034b86":"markdown","ca16f1e9":"markdown","d2457a37":"markdown","34fc3223":"markdown","4a8e1534":"markdown","3d6225fa":"markdown","fcddb618":"markdown","038acc36":"markdown","5dee0c75":"markdown","7fc85dfe":"markdown"},"source":{"ae7cabe4":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndados = pd.read_csv('..\/input\/beer-consumption-sao-paulo\/Consumo_cerveja.csv',thousands='.', decimal=',')\ndados = dados.dropna()\ndados = dados.reset_index(drop=True)\ndados['Data'] = pd.to_datetime(dados['Data']).dt.strftime('%d\/%m\/%Y')\nprint(dados.shape)\ndados","ea716534":"dados.describe().round(2)","895ce765":"dados.corr().round(4)","c7a5372a":"def plotar_grafico(dado,title,ylabel,xlabel):\n    fig, ax = plt.subplots(figsize=(20, 6)) \n    ax.set_title(title, fontsize=20)\n    ax.set_ylabel(ylabel, fontsize=16)\n    ax.set_xlabel(xlabel, fontsize=16)\n    ax = dado.plot(fontsize=14)\n    \n    \nplotar_grafico(dados['Consumo de cerveja (litros)'], 'Consumo de Cerveja', 'Litros', 'Dias')\nplotar_grafico(dados['Temperatura Media (C)'], 'Temperatura', 'Celsius', 'Dias')\nplotar_grafico(dados['Precipitacao (mm)'], 'Chuva', 'mililitros', 'Dias')","ae6575c9":"plotar_grafico(dados['Consumo de cerveja (litros)'][:36], 'Consumo de Cerveja', 'Litros', 'Dias')\nplotar_grafico(dados['Temperatura Media (C)'][:36], 'Temperatura', 'Celsius', 'Dias')\nplotar_grafico(dados['Precipitacao (mm)'][:36], 'Chuva', 'mililitros', 'Dias')","9ae49449":"import seaborn as sns\n\nax = sns.boxplot(y='Final de Semana', x='Consumo de cerveja (litros)', data=dados, orient='h', width=0.5)\nax.figure.set_size_inches(12, 6)\nax.set_title('Consumo de Cerveja', fontsize=20)\nax.set_xlabel('Litros', fontsize=16)\nax.set_ylabel('Final de Semana', fontsize=16)\nax","9c154a80":"ax = sns.distplot(dados['Consumo de cerveja (litros)'])\nax.figure.set_size_inches(12, 6)\nax.set_title('Distrubui\u00e7\u00e3o de Frequ\u00eancias', fontsize=20)\nax.set_ylabel('Consumo de Cerveja(Litros)', fontsize=16)\nax","d84ab8b7":"ax = sns.pairplot(dados, kind='reg', y_vars='Consumo de cerveja (litros)', x_vars=['Temperatura Media (C)', 'Precipitacao (mm)', 'Final de Semana'])\nax","1d23e1b5":"from sklearn.model_selection import train_test_split\nX= dados[['Temperatura Media (C)', 'Precipitacao (mm)', 'Final de Semana']]\nY= dados['Consumo de cerveja (litros)']\n","3052cc29":"x_treino, x_teste, y_treino, y_teste = train_test_split(X, Y, test_size=0.33, random_state=150)","84f23c70":"ax = sns.jointplot(x='Temperatura Media (C)', y='Consumo de cerveja (litros)', data=dados, kind='reg')\nax.fig.suptitle('Dispers\u00e3o - Consumo X Temperatura', fontsize=18, y=1.05)","30fc8ca8":"from sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nmodelo = LinearRegression()","8d69b20c":"modelo.fit(x_treino, y_treino)\ny_previsto = modelo.predict(x_teste)\nprint('R\u00b2 = {}'.format(metrics.r2_score(y_teste, y_previsto).round(2)))","383f4ed3":"temp_med=35\nchuva=3\nfds=0\nentrada=[[temp_med, chuva, fds]]\nprint('{0:.2f} litros'.format(modelo.predict(entrada)[0]))","f9e71df9":"%%time\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.svm import SVR\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\nfrom sklearn.linear_model import (\n    BayesianRidge,\n    SGDRegressor,\n)\n\n# split the dataset beteween train and test\ndef split(x, y, plot=False):\n    # seed\n    # train_test_split\n    train_x, test_x, train_y, test_y = train_test_split(\n        x, y, test_size=0.1, random_state=42367,\n    )\n    if plot:\n        print(\n            \"sizes: train (x,y) and test (x,y)\",\n            train_x.shape,\n            train_y.shape,\n            test_x.shape,\n            test_y.shape,\n        )\n    return train_x, test_x, train_y, test_y\n\n\n# Just train and valid the model\ndef run_reg_linear(train_x, test_x, train_y, test_y, model, plot=False):\n    model.fit(train_x, train_y)\n    test_pred = model.predict(test_x)\n\n    mse = mean_squared_error(test_y, test_pred)\n    mae = mean_absolute_error(test_y, test_pred)\n    r2 = r2_score(test_y, test_pred)\n\n    if plot:\n        print(\"*\" * 40)\n        print(\"r2 score\", r2)\n        print(\"mse\", mse)\n        print(\"mae\", mae)\n        print(\"*\" * 40)\n\n    return r2, mse\n\n\n# Train with all models then return a table with scores\ndef train_test_show(train_x, test_x, train_y, test_y):\n    valores = []\n    models = [\n        (\"BayesianRidge\", BayesianRidge(n_iter=3000,compute_score=True)),\n        (\"MLPRegressor\", MLPRegressor(learning_rate='adaptive',max_iter=10000)),\n        (\"RandomForestRegressor\", RandomForestRegressor(n_jobs=-1)),\n    ]\n    for model in models:\n        print(model[0])\n        valores.append(\n            (model[0], *run_reg_linear(train_x, test_x, train_y, test_y, model[1]))\n        )\n    valores = pd.DataFrame(valores, columns=[\"Model\", \"R2\", \"MSE\"])\n    return valores.style.background_gradient(cmap=\"Reds\", low=0, high=1)\n\n\ntrain_test_show(x_treino, x_teste, y_treino, y_teste)","8a4c2d93":"from sklearn.model_selection import RandomizedSearchCV\nimport numpy as np\n\ntol = [round(x\/1000000.,5) for x in np.linspace(start=0, stop=100000, num=2000)]\nalpha_1 = [round(x\/100000.,4) for x in np.linspace(start=0, stop=100000, num=2000)]\nalpha_2 = [round(x\/100000.,4) for x in np.linspace(start=0, stop=100000, num=2000)]\nlambda_1 = [round(x\/100000.,4) for x in np.linspace(start=0, stop=100000, num=2000)]\nlambda_2 = [round(x\/100000.,4) for x in np.linspace(start=0, stop=100000, num=2000)]\nfit_intercept = [True,False]\nnormalize = [True,False]\nn_iter = [10000]\n    \nrandom_grid = {\n    \"tol\": tol,\n    \"alpha_1\": alpha_1,\n    \"alpha_2\": alpha_2,\n    \"lambda_1\": lambda_1,\n    \"lambda_2\": lambda_2,   \n    \"fit_intercept\": fit_intercept,  \n    \"normalize\": normalize,\n    'n_iter':n_iter\n\n\n}\n\nbr = BayesianRidge()\nbr_random = RandomizedSearchCV(\n    estimator=br,\n    param_distributions=random_grid,\n    n_iter=2000,\n    cv=5,\n    verbose=2,\n    random_state=42,\n    n_jobs=-1,\n)\n\nbr_random.fit(\n   x_treino, y_treino\n)","6dd9259e":"p = br_random.best_params_\nprint(p)\nscore= run_reg_linear(x_treino, x_teste, y_treino, y_teste, BayesianRidge(\n    tol=p['tol'],\n    alpha_1= p['alpha_1'],\n    alpha_2= p['alpha_2'],\n    lambda_1= p['lambda_1'],\n    lambda_2= p['lambda_2'],   \n    fit_intercept= p['fit_intercept'],  \n    normalize= p['normalize'],\n    n_iter=p['n_iter']\n), True)","a30149e0":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start=1, stop=1000, num=100)]\n# Number of features to consider at every split\nmax_features = [\"auto\", \"sqrt\"]\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(1, 1100, num=110)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 20,25,30]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4,6,8,10]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {\n    \"n_estimators\": n_estimators,\n    \"max_features\": max_features,\n    \"max_depth\": max_depth,\n    \"min_samples_split\": min_samples_split,\n    \"min_samples_leaf\": min_samples_leaf,\n    \"bootstrap\": bootstrap,\n}\n\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor(n_jobs=-1)\n# Random search of parameters, using 3 fold cross validation,\n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(\n    estimator=rf,\n    param_distributions=random_grid,\n    n_iter=500,\n    cv=4,\n    verbose=2,\n    random_state=42,\n    n_jobs=-1,\n)\n# Fit the random search model\nrf_random.fit(\n   x_treino, y_treino\n)","c2e401b9":"p = rf_random.best_params_\nprint(p)\nscore= run_reg_linear(x_treino, x_teste, y_treino, y_teste, RandomForestRegressor(\n    n_estimators=p['n_estimators'],\n    max_features= p['max_features'],\n    max_depth= p['max_depth'],\n    min_samples_split= p['min_samples_split'],\n    min_samples_leaf= p['min_samples_leaf'],   \n    bootstrap= p['bootstrap'],  \n), True)\n\n","510e38a6":"#### Nitidamente o boxplot nos mostra que nos finais de semanas o consumo \u00e9 mais elevado","b12e3b62":"#### Exemlpo de predict","6c89e29a":"#### Dividindo o conjunto de dados entre treino e valida\u00e7\u00e3o","76614eee":"## BayesianRidge","3f02aa42":"## Random Forest","ceed7766":"# Consumo de cerveja: semana x final de semana","84034b86":"Retas de Regress\u00e3o Liner conforme indicado pela Matriz de Correspond\u00eancia.","ca16f1e9":"# Descri\u00e7\u00e3o geral dos dados","d2457a37":"# Dispers\u00e3o dos dados","34fc3223":"### 36 primeiros dias","4a8e1534":"# Importando e formatando os dados","3d6225fa":"# Modelo de Regress\u00e3o Linear","fcddb618":"### Iniciando a regress\u00e3o linear","038acc36":"# Visualizacao","5dee0c75":"# Matriz de Correla\u00e7\u00e3o","7fc85dfe":"### Todo o dataset"}}