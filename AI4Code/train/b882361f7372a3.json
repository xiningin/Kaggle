{"cell_type":{"62718801":"code","d8642b79":"code","dd8168fa":"code","00db81ff":"code","ed93094f":"code","0ee8bd20":"code","5a428677":"code","b9646f2b":"code","24e6e4fe":"code","88ac00b9":"code","aeb3cf72":"code","6d925223":"code","3ae4297f":"code","eeed7eb2":"code","b7255d66":"code","5d8b439e":"code","d5e78247":"code","afab74ee":"code","243d23b4":"code","343335d5":"code","d05a9987":"code","0f86c3d0":"code","a66edd82":"code","911db307":"code","07443c04":"code","560f4b02":"code","3d7e6a08":"code","0d7ed295":"code","ba9e1b6e":"code","5d59ad66":"code","0adb6215":"code","a6fe387c":"code","7c0a161a":"code","4cce0de3":"markdown","2bd6c9c2":"markdown","05af9982":"markdown","955fdde7":"markdown","42383239":"markdown","21b1fe96":"markdown","3d2c0933":"markdown","0f052105":"markdown","faeefd87":"markdown","02782e98":"markdown","b17125a8":"markdown","af527d44":"markdown","7d689fd5":"markdown","73878798":"markdown"},"source":{"62718801":"from IPython.display import HTML\nHTML('''\n<script>\n  function code_toggle() {\n    if (code_shown){\n      $('div.input').hide('500');\n      $('#toggleButton').val('Show Code')\n    } else {\n      $('div.input').show('500');\n      $('#toggleButton').val('Hide Code')\n    }\n    code_shown = !code_shown\n  }\n\n  $( document ).ready(function(){\n    code_shown=true;\n  });\n<\/script>\n<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Hide Code\"><\/form>''')","d8642b79":"import os\nprint(os.listdir(\"..\/input\"))","dd8168fa":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom keras.utils import np_utils","00db81ff":"import warnings\nwarnings.filterwarnings('ignore')\n\ndf = pd.read_csv(\"..\/input\/train.csv\")\n\ndf.head()","ed93094f":"print(df.shape)\nprint(df.columns)","0ee8bd20":"percent_missing = df.isnull().sum() * 100 \/ len(df)\nmissing_values = pd.DataFrame({'column_name': df.columns,\n                               'percent_missing': percent_missing})\nmissing_values","5a428677":"X = df.drop(['Id','MSZoning','Street','LotShape','LotConfig','LandSlope','Neighborhood','Condition1','Condition2','BldgType',\n         'HouseStyle','RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','ExterQual','ExterCond','Foundation',\n         'BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n         'Alley','LotArea','LandContour','Utilities','PoolQC','Fence','MiscFeature','SaleType',\n         'SaleCondition','PavedDrive','GarageCond','GarageQual','SalePrice']\n        , axis=1)","b9646f2b":"X = X.fillna(0)","24e6e4fe":"# BsmtUnfSF\t...\tGarageArea\n# X[['TotalBsmtSF', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical',\n#        '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath',\n#        'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr',\n#        'KitchenQual', 'TotRmsAbvGrd', 'Functional', 'Fireplaces',\n#        'FireplaceQu', 'GarageType', 'GarageYrBlt', 'GarageFinish',\n#        'GarageCars']]\nX1 = X.drop(['Heating','HeatingQC','CentralAir','Electrical','KitchenQual','Functional','FireplaceQu','GarageType',\n             'GarageFinish']\n               , axis=1)","88ac00b9":"import warnings\nwarnings.filterwarnings('ignore')\n\ndf2 = pd.read_csv(\"..\/input\/test.csv\")\n\ndf2.head()","aeb3cf72":"X2 = df2.drop(['Id','MSZoning','Street','LotShape','LotConfig','LandSlope','Neighborhood','Condition1','Condition2','BldgType',\n         'HouseStyle','RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','ExterQual','ExterCond','Foundation',\n         'BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n         'Alley','LotArea','LandContour','Utilities','PoolQC','Fence','MiscFeature','SaleType',\n         'SaleCondition','PavedDrive','GarageCond','GarageQual']\n        , axis=1)","6d925223":"X2 = X2.drop(['Heating','HeatingQC','CentralAir','Electrical','KitchenQual','Functional','FireplaceQu','GarageType',\n             'GarageFinish']\n               , axis=1)","3ae4297f":"X2 = X2.fillna(0)","eeed7eb2":"percent_missing = df2.isnull().sum() * 100 \/ len(df2)\nmissing_values = pd.DataFrame({'column_name': df2.columns,\n                               'percent_missing': percent_missing})\nmissing_values","b7255d66":"df2 = df2.fillna(0)","5d8b439e":"import seaborn as sns\nimport matplotlib.pyplot as plt","d5e78247":"import seaborn as sns\n\ncorr=df.corr()\nsns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values)","afab74ee":"# Thanks to: https:\/\/scikit-learn.org\/stable\/auto_examples\/classification\/plot_classifier_comparison.html\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.decomposition import PCA\n\nh = .02  # step size in the mesh\n\nnames = [\"Nearest Neighbors\"]\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"linear\", C=0.025),\n    SVC(gamma=2, C=1),\n    GaussianProcessClassifier(1.0 * RBF(1.0)),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    MLPClassifier(alpha=1, max_iter=1000),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    QuadraticDiscriminantAnalysis()]\n\nX = X1.values\npca = PCA(n_components=2,svd_solver='full')\nX = pca.fit_transform(X)\ny = df['SalePrice']\n\nrng = np.random.RandomState(2)\n\ndatasets = [df]\n\nfigure = plt.figure(figsize=(27, 9))\ni = 1\n# iterate over datasets\nfor ds_cnt, ds in enumerate(datasets):\n    X = StandardScaler().fit_transform(X)\n    X_train, X_test, y_train, y_test = \\\n        train_test_split(X, y, test_size=.3, random_state=42)\n\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # just plot the dataset first\n    cm = plt.cm.RdBu\n    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n    if ds_cnt == 0:\n        ax.set_title(\"Input data\")\n    # Plot the training points\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n               edgecolors='k')\n    # Plot the testing points\n    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n               edgecolors='k')\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n    i += 1\n\n    # iterate over classifiers\n    for name, clf in zip(names, classifiers):\n        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n\n        # Plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, x_max]x[y_min, y_max].\n        if hasattr(clf, \"decision_function\"):\n            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n        else:\n            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n        # Put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n\n        # Plot the training points\n        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n                   edgecolors='k')\n        # Plot the testing points\n        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n                   edgecolors='k', alpha=0.6)\n\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n        ax.set_xticks(())\n        ax.set_yticks(())\n        if ds_cnt == 0:\n            ax.set_title(name)\n        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n                size=15, horizontalalignment='right')\n        i += 1\n\nplt.tight_layout()\nplt.show()","243d23b4":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nX_Train = X1.values\nY_Train = df['SalePrice']\n\nX_Train = StandardScaler().fit_transform(X_Train)\n\nX_Test = X2.values\nX_Test = StandardScaler().fit_transform(X_Test)","343335d5":"# Preprocessing :\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom itertools import product\n\n# Classifiers :\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA","d05a9987":"trainedmodel = LogisticRegression().fit(X_Train,Y_Train)\npredictions =trainedmodel.predict(X_Test)\ntrainedmodel.score(X_Train, Y_Train)","0f86c3d0":"trainedforest = RandomForestClassifier(n_estimators=700).fit(X_Train,Y_Train)\npredictionforest = trainedforest.predict(X_Test)\ntrainedforest.score(X_Train, Y_Train)","a66edd82":"submission = pd.DataFrame({\n        \"Id\": df2[\"Id\"],\n        \"SalePrice\": predictionforest\n    })\nsubmission.to_csv('sample_submission.csv', index=False)","911db307":"trainedsvm = svm.LinearSVC().fit(X_Train, Y_Train)\npredictionsvm = trainedsvm.predict(X_Test)\ntrainedsvm.score(X_Train, Y_Train)","07443c04":"trainedtree = tree.DecisionTreeClassifier().fit(X_Train, Y_Train)\npredictionstree = trainedtree.predict(X_Test)\ntrainedtree.score(X_Train, Y_Train)","560f4b02":"trainedlda = LinearDiscriminantAnalysis().fit(X_Train, Y_Train)\npredictionlda = trainedlda.predict(X_Test)\ntrainedlda.score(X_Train, Y_Train)","3d7e6a08":"trainednb = GaussianNB().fit(X_Train, Y_Train)\npredictionnb = trainednb.predict(X_Test)\ntrainednb.score(X_Train, Y_Train)","0d7ed295":"from xgboost import XGBClassifier\nfrom xgboost import plot_tree\nimport matplotlib.pyplot as plt\nmodel = XGBClassifier()\n\n# Train\nmodel.fit(X_Train, Y_Train)\n\nplot_tree(model)\nplt.figure(figsize = (50,55))\nplt.show()","ba9e1b6e":"pca = PCA(n_components=2,svd_solver='full')\n\nX_Train = X1.values\nY_Train = df['SalePrice']\n\nX_Train = StandardScaler().fit_transform(X_Train)\nX_Train1 = pca.fit_transform(X_Train)\n\nX_Test = X2.values\nX_Test = StandardScaler().fit_transform(X_Test)\nX_Test1 = pca.fit_transform(X_Train)\n\ntrainednb = GaussianNB().fit(X_Train1, Y_Train)\ntrainedsvm = svm.LinearSVC().fit(X_Train1, Y_Train)\ntrainedforest = RandomForestClassifier(n_estimators=700).fit(X_Train1,Y_Train)\ntrainedmodel = LogisticRegression().fit(X_Train1,Y_Train)\n\nprint('Naive Bayes')\npredictionnb = trainednb.predict(X_Test1)\nprint(trainednb.score(X_Train1, Y_Train))\n\nprint('SVM')\npredictionsvm = trainedsvm.predict(X_Test1)\nprint(trainedsvm.score(X_Train1, Y_Train))\n\nprint('Random Forest')\npredictionforest = trainedforest.predict(X_Test1)\nprint(trainedforest.score(X_Train1, Y_Train))\n\nprint('Logistic Regression')\npredictions =trainedmodel.predict(X_Test1)\nprint(trainedmodel.score(X_Train1, Y_Train))","5d59ad66":"# Thanks to: https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_voting_decision_regions.html\n\nx_min, x_max = X_Train1[:, 0].min() - 1, X_Train1[:, 0].max() + 1\ny_min, y_max = X_Train1[:, 1].min() - 1, X_Train1[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                     np.arange(y_min, y_max, 0.1))\n\nf, axarr = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(10, 8))\n\nfor idx, clf, tt in zip(product([0, 1], [0, 1]),\n                        [trainednb, trainedsvm, trainedforest, trainedmodel],\n                        ['Naive Bayes Classifier', 'SVM',\n                         'Random Forest', 'Logistic Regression']):\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n\n    axarr[idx[0], idx[1]].contourf(xx, yy, Z,cmap=plt.cm.coolwarm, alpha=0.4)\n    axarr[idx[0], idx[1]].scatter(X_Train1[:, 0], X_Train1[:, 1], c=Y_Train,\n                                  s=20, edgecolor='k')\n    axarr[idx[0], idx[1]].set_title(tt)\n\nplt.show()","0adb6215":"from keras.utils.np_utils import to_categorical\nY_Train = to_categorical(Y_Train)","a6fe387c":"from keras.models import Sequential\nfrom keras.utils import np_utils\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense, Dropout, BatchNormalization, Activation\n\ninput_dim = X_Train.shape[1]\nnb_classes = Y_Train.shape[1]\n\nmodel = Sequential()\nmodel.add(Dense(512, input_dim=input_dim))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(256))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(nb_classes))\nmodel.add(BatchNormalization())\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n\nprint(\"Training...\")\nmodel.fit(X_Train, Y_Train, nb_epoch=3, batch_size=16, validation_split=0.1, verbose=80)\n\npreds = model.predict_classes(X_Test, verbose=0)","7c0a161a":"scores = model.evaluate(X_Train, Y_Train)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","4cce0de3":"**Feature Engineering**","2bd6c9c2":"Logistic Regression","05af9982":"Random Forest","955fdde7":"Linear Discriminant Anaylsis","42383239":"**Machine Learning**","21b1fe96":"**Preprocessing**","3d2c0933":"Naive Bayes","0f052105":"Principal Component Analysis","faeefd87":"XGBoost","02782e98":"**House Prices**","b17125a8":"Decision Tree","af527d44":"**Data Visualization**","7d689fd5":"****Deep Learning****","73878798":"Support Vector Machines"}}