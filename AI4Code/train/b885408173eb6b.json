{"cell_type":{"fe9d06be":"code","ff041de3":"code","ba44e503":"code","0abb407d":"code","827280b3":"code","d796768a":"code","3d1437b9":"code","37d777db":"code","899f4985":"code","15900e90":"code","ff720aa1":"code","c7dba09b":"code","ec5e9672":"code","6a549177":"code","c9352ff0":"code","af7ed6e6":"code","05f296d4":"code","87fc406e":"code","9a6893c9":"code","c00afe7c":"code","a0c25253":"code","c12e4525":"code","e405bc53":"code","45b348fb":"code","4f5a3415":"code","096afecb":"code","52d0bb31":"code","5fc1c5f1":"code","69e9b7e9":"code","0e7435c4":"code","18b9081e":"code","6d2c3fd1":"markdown","4ece7fed":"markdown","4fa0ec69":"markdown","2b3fa997":"markdown","a7ea0aa2":"markdown","c690632b":"markdown","af5846ef":"markdown","2aba7889":"markdown","86e5aca9":"markdown","22466012":"markdown","88721f33":"markdown","26a93685":"markdown"},"source":{"fe9d06be":"# Importing essential libraries\nimport numpy as np\nimport pandas as pd","ff041de3":"# Loading the dataset\ndf = pd.read_csv('..\/input\/heart-disease-dataset\/data.csv')","ba44e503":"# Returns number of rows and columns of the dataset\ndf.shape","0abb407d":"# Returns an object with all of the column headers\ndf.columns","827280b3":"# Returns different datatypes for each columns (float, int, string, bool, etc.)\ndf.dtypes","d796768a":"# Returns the first x number of rows when head(x). Without a number it returns 5\ndf.head()","3d1437b9":"# Returns the last x number of rows when tail(x). Without a number it returns 5\ndf.tail()","37d777db":"# Returns true for a column having null values, else false\ndf.isnull().any()","899f4985":"# Returns basic information on all columns\ndf.info()","15900e90":"# Returns basic statistics on numeric columns\ndf.describe().T","ff720aa1":"# Importing essential libraries\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","c7dba09b":"# Plotting histogram for the entire dataset\nfig = plt.figure(figsize = (15,15))\nax = fig.gca()\ng = df.hist(ax=ax)","ec5e9672":"# Visualization to check if the dataset is balanced or not\ng = sns.countplot(x='target', data=df)\nplt.xlabel('Target')\nplt.ylabel('Count')","6a549177":"# Selecting correlated features using Heatmap\n\n# Get correlation of all the features of the dataset\ncorr_matrix = df.corr()\ntop_corr_features = corr_matrix.index\n\n# Plotting the heatmap\nplt.figure(figsize=(20,20))\nsns.heatmap(data=df[top_corr_features].corr(), annot=True, cmap='RdYlGn')","c9352ff0":"dataset = pd.get_dummies(df, columns=['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'])","af7ed6e6":"dataset.columns","05f296d4":"from sklearn.preprocessing import StandardScaler\nstandScaler = StandardScaler()\ncolumns_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\ndataset[columns_to_scale] = standScaler.fit_transform(dataset[columns_to_scale])","87fc406e":"dataset.head()","9a6893c9":"# Splitting the dataset into dependent and independent features\nX = dataset.drop('target', axis=1)\ny = dataset['target']","c00afe7c":"# Importing essential libraries\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score","a0c25253":"# Finding the best accuracy for knn algorithm using cross_val_score \nknn_scores = []\nfor i in range(1, 21):\n  knn_classifier = KNeighborsClassifier(n_neighbors=i)\n  cvs_scores = cross_val_score(knn_classifier, X, y, cv=10)\n  knn_scores.append(round(cvs_scores.mean(),3))","c12e4525":"# Plotting the results of knn_scores\nplt.figure(figsize=(20,15))\nplt.plot([k for k in range(1, 21)], knn_scores, color = 'red')\nfor i in range(1,21):\n    plt.text(i, knn_scores[i-1], (i, knn_scores[i-1]))\nplt.xticks([i for i in range(1, 21)])\nplt.xlabel('Number of Neighbors (K)')\nplt.ylabel('Scores')\nplt.title('K Neighbors Classifier scores for different K values')","e405bc53":"# Training the knn classifier model with k value as 12\nknn_classifier = KNeighborsClassifier(n_neighbors=12)\ncvs_scores = cross_val_score(knn_classifier, X, y, cv=10)\nprint(\"KNeighbours Classifier Accuracy with K=12 is: {}%\".format(round(cvs_scores.mean(), 4)*100))","45b348fb":"# Importing essential libraries\nfrom sklearn.tree import DecisionTreeClassifier","4f5a3415":"# Finding the best accuracy for decision tree algorithm using cross_val_score \ndecision_scores = []\nfor i in range(1, 11):\n  decision_classifier = DecisionTreeClassifier(max_depth=i)\n  cvs_scores = cross_val_score(decision_classifier, X, y, cv=10)\n  decision_scores.append(round(cvs_scores.mean(),3))","096afecb":"# Plotting the results of decision_scores\nplt.figure(figsize=(20,15))\nplt.plot([i for i in range(1, 11)], decision_scores, color = 'red')\nfor i in range(1,11):\n    plt.text(i, decision_scores[i-1], (i, decision_scores[i-1]))\nplt.xticks([i for i in range(1, 11)])\nplt.xlabel('Depth of Decision Tree (N)')\nplt.ylabel('Scores')\nplt.title('Decision Tree Classifier scores for different depth values')","52d0bb31":"# Training the decision tree classifier model with max_depth value as 3\ndecision_classifier = DecisionTreeClassifier(max_depth=3)\ncvs_scores = cross_val_score(decision_classifier, X, y, cv=10)\nprint(\"Decision Tree Classifier Accuracy with max_depth=3 is: {}%\".format(round(cvs_scores.mean(), 4)*100))","5fc1c5f1":"# Importing essential libraries\nfrom sklearn.ensemble import RandomForestClassifier","69e9b7e9":"# Finding the best accuracy for random forest algorithm using cross_val_score \nforest_scores = []\nfor i in range(10, 101, 10):\n  forest_classifier = RandomForestClassifier(n_estimators=i)\n  cvs_scores = cross_val_score(forest_classifier, X, y, cv=5)\n  forest_scores.append(round(cvs_scores.mean(),3))","0e7435c4":"# Plotting the results of forest_scores\nplt.figure(figsize=(20,15))\nplt.plot([n for n in range(10, 101, 10)], forest_scores, color = 'red')\nfor i in range(1,11):\n    plt.text(i*10, forest_scores[i-1], (i*10, forest_scores[i-1]))\nplt.xticks([i for i in range(10, 101, 10)])\nplt.xlabel('Number of Estimators (N)')\nplt.ylabel('Scores')\nplt.title('Random Forest Classifier scores for different N values')","18b9081e":"# Training the random forest classifier model with n value as 90\nforest_classifier = RandomForestClassifier(n_estimators=90)\ncvs_scores = cross_val_score(forest_classifier, X, y, cv=5)\nprint(\"Random Forest Classifier Accuracy with n_estimators=90 is: {}%\".format(round(cvs_scores.mean(), 4)*100))","6d2c3fd1":"# EDA","4ece7fed":"## Random Forest Classifier","4fa0ec69":"## Handling categorical features\n\nAfter exploring the dataset, I observed that converting the categorical variables into dummy variables using 'get_dummies()'. Though we don't have any strings in our dataset it is necessary to convert ('sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal') these features.\n\n*Example: Consider the 'sex' column, it is a binary feature which has 0's and 1's as its values. Keeping it as it is would lead the algorithm to think 0 is lower value and 1 is a higher value, which should not be the case since the gender cannot be ordinal feature.*","2b3fa997":"## KNeighbors Classifier Model","a7ea0aa2":"# Data Visualization","c690632b":"# **Data Preprocessing**","af5846ef":"### Feature Selection","2aba7889":"# **Model Building**\n\nI will be experimenting with 3 algorithms:\n1. KNeighbors Classifier\n2. Decision Tree Classifier\n3. Random Forest Classifier","86e5aca9":"## Feature Scaling","22466012":"# Heart Disease Classification- Part2\n![3d_rendered_illustration_-_heart_attack-Sebastian_Kaulitzki_d8743c68eb3c4250817a2a23a847caeb-620x480.jpg](https:\/\/d2jx2rerrg6sh3.cloudfront.net\/image-handler\/picture\/2016\/5\/3d_rendered_illustration_-_heart_attack-Sebastian_Kaulitzki_d8743c68eb3c4250817a2a23a847caeb-620x480.jpg)\n#### Part1: https:\/\/www.kaggle.com\/lykin22\/heart-disease-classification","88721f33":"## Decision Tree Classifier","26a93685":"# **Feature Engineering**"}}