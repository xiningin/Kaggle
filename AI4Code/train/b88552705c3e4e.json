{"cell_type":{"65144053":"code","f2ac8aee":"code","0fc46498":"code","570407bf":"code","271d5719":"code","9bd0f6ae":"code","6a90b3b2":"code","95150c85":"code","6974a38a":"code","d3e3b2af":"code","043fa8c2":"code","13bb95a7":"code","283e70f6":"code","3ed36107":"code","7eea9030":"code","28418a50":"code","e9cd7f1a":"code","25f4857d":"code","3e8c5a44":"code","a8f1b863":"code","7bba6835":"code","c089d287":"code","32481464":"markdown","474b5609":"markdown","2cd3cd9a":"markdown","d4f45b68":"markdown","1da3f2c7":"markdown","1e45ef1f":"markdown","7959554a":"markdown","4f6cd21a":"markdown","f373ecaa":"markdown","34e2ff3c":"markdown","174d4d23":"markdown","b98402f9":"markdown","fde994c4":"markdown","2fc65534":"markdown"},"source":{"65144053":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T\nfrom sklearn.neighbors import NearestNeighbors\n\nimport torch\nfrom torch import nn\nfrom torch import optim\n\nimport cv2\nimport os\nfrom PIL import Image","f2ac8aee":"# base bath for kaggle\npath = '\/kaggle\/input\/images-alike\/'","0fc46498":"# load the validate dataset\ndf = pd.read_csv(path + 'validate.csv', index_col=0)\ndf.head()","570407bf":"def draw_pair_similars(df, idx):\n    dim = (300, 300)\n    img_a = cv2.imread(path + 'images\/' + str(df.iloc[idx]['image_a']) + '.jpg')\n    img_a = cv2.resize(img_a, dim, interpolation = cv2.INTER_AREA)\n    \n    img_b = cv2.imread(path + 'images\/' + str(df.iloc[idx]['image_b']) + '.jpg')\n    img_b = cv2.resize(img_b, dim, interpolation = cv2.INTER_AREA)\n    \n    imgs = [img_a, img_b]\n    \n    f, ax = plt.subplots(1, 2, figsize=(12, 12))\n    for ix, img in enumerate(imgs):\n        ax[ix].imshow(img)\n        ax[ix].axis('off')\n        \n    plt.subplots_adjust(wspace=0, hspace=0)\n    plt.show()","271d5719":"draw_pair_similars(df, 84)","9bd0f6ae":"class AlikeDataset(Dataset):\n\n    def __init__(self, path, transform=None):\n        self.path = path + 'images\/'\n        self.files = self.absolute_file_paths(self.path)\n        self.transform = transform\n        \n    def absolute_file_paths(self, directory):\n        path = os.path.abspath(directory)\n        return [entry.path for entry in os.scandir(path) if entry.is_file()]\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n\n        img_path = self.files[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n\n        if self.transform is not None:\n            tensor_image = self.transform(image)\n            return tensor_image, tensor_image\n        \n        return image","6a90b3b2":"# create transformers\ntransforms = T.Compose([T.Resize((352, 128)),\n                        T.ToTensor()])\n\n# create dataset\nalike_ds = AlikeDataset(path, transforms)","95150c85":"plt.figure(figsize = (10,10))\nplt.imshow(alike_ds[0][0].permute(1, 2, 0))","6974a38a":"train_size = int(len(alike_ds) * 0.75)\nval_size = len(alike_ds) - train_size\n\n# split data to train and test\ntrain_dataset, val_dataset = torch.utils.data.random_split(alike_ds, [train_size, val_size]) \n\n# create the train dataloader\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n \n# create the validation dataloader\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n\n# create the full dataloader\nfull_loader = torch.utils.data.DataLoader(alike_ds, batch_size=32)","d3e3b2af":"class ConvEncoder(nn.Module):\n    \"\"\"\n    A simple Convolutional Encoder Model\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(3, 16, (3, 3), padding=(1, 1))\n        self.relu1 = nn.ReLU(inplace=True)\n        self.maxpool1 = nn.MaxPool2d((2, 2))\n\n        self.conv2 = nn.Conv2d(16, 32, (3, 3), padding=(1, 1))\n        self.relu2 = nn.ReLU(inplace=True)\n        self.maxpool2 = nn.MaxPool2d((2, 2))\n\n        self.conv3 = nn.Conv2d(32, 64, (3, 3), padding=(1, 1))\n        self.relu3 = nn.ReLU(inplace=True)\n        self.maxpool3 = nn.MaxPool2d((2, 2))\n\n        self.conv4 = nn.Conv2d(64, 128, (3, 3), padding=(1, 1))\n        self.relu4 = nn.ReLU(inplace=True)\n        self.maxpool4 = nn.MaxPool2d((2, 2))\n\n        self.conv5 = nn.Conv2d(128, 256, (3, 3), padding=(1, 1))\n        self.relu5 = nn.ReLU(inplace=True)\n        self.maxpool5 = nn.MaxPool2d((2, 2))\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu1(x)\n        x = self.maxpool1(x)\n\n        x = self.conv2(x)\n        x = self.relu2(x)\n        x = self.maxpool2(x)\n\n        x = self.conv3(x)\n        x = self.relu3(x)\n        x = self.maxpool3(x)\n\n        x = self.conv4(x)\n        x = self.relu4(x)\n        x = self.maxpool4(x)\n\n        x = self.conv5(x)\n        x = self.relu5(x)\n        x = self.maxpool5(x)\n        \n        return x","043fa8c2":"class ConvDecoder(nn.Module):\n    \"\"\"\n    A simple Convolutional Decoder Model\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.deconv1 = nn.ConvTranspose2d(256, 128, (2, 2), stride=(2, 2))\n        self.relu1 = nn.ReLU(inplace=True)\n\n        self.deconv2 = nn.ConvTranspose2d(128, 64, (2, 2), stride=(2, 2))\n        self.relu2 = nn.ReLU(inplace=True)\n\n        self.deconv3 = nn.ConvTranspose2d(64, 32, (2, 2), stride=(2, 2))\n        self.relu3 = nn.ReLU(inplace=True)\n\n        self.deconv4 = nn.ConvTranspose2d(32, 16, (2, 2), stride=(2, 2))\n        self.relu4 = nn.ReLU(inplace=True)\n\n        self.deconv5 = nn.ConvTranspose2d(16, 3, (2, 2), stride=(2, 2))\n        self.relu5 = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.deconv1(x)\n        x = self.relu1(x)\n\n        x = self.deconv2(x)\n        x = self.relu2(x)\n\n        x = self.deconv3(x)\n        x = self.relu3(x)\n\n        x = self.deconv4(x)\n        x = self.relu4(x)\n\n        x = self.deconv5(x)\n        x = self.relu5(x)\n        return x","13bb95a7":"device = \"cuda\"\nloss_fn = nn.MSELoss()\n\nencoder = ConvEncoder().to(device)\ndecoder = ConvDecoder().to(device)\n\nautoencoder_params = list(encoder.parameters()) + list(decoder.parameters())\noptimizer = optim.Adam(autoencoder_params, lr=1e-3)","283e70f6":"def train_step(encoder, decoder, train_loader, loss_fn, optimizer, device):\n    \"\"\"\n    Performs a single training step\n    \"\"\"\n    encoder.train()\n    decoder.train()\n\n    for batch_idx, (train_img, target_img) in enumerate(train_loader):\n        train_img = train_img.to(device)\n        target_img = target_img.to(device)\n        \n        optimizer.zero_grad()\n\n        enc_output = encoder(train_img)\n        dec_output = decoder(enc_output)\n        \n        loss = loss_fn(dec_output, target_img)\n\n        loss.backward()\n        optimizer.step()\n\n    return loss.item()","3ed36107":"def val_step(encoder, decoder, val_loader, loss_fn, device):\n    \"\"\"\n    Performs a single validation step\n    \"\"\"\n\n    encoder.eval()\n    decoder.eval()\n    \n    with torch.no_grad():\n        for batch_idx, (train_img, target_img) in enumerate(val_loader):\n\n            train_img = train_img.to(device)\n            target_img = target_img.to(device)\n\n            enc_output = encoder(train_img)\n            dec_output = decoder(enc_output)\n\n            loss = loss_fn(dec_output, target_img)\n\n    return loss.item()","7eea9030":"train_losses, val_losses = [], []\n\nfor epoch in range(10):\n    \n        train_loss = train_step(encoder, decoder, train_loader, loss_fn, optimizer, device=device)\n        train_losses.append(train_loss)\n        \n        val_loss = val_step(encoder, decoder, val_loader, loss_fn, device=device)\n        val_losses.append(val_loss)\n        \n        if(epoch % 10 == 0):\n            print(\"Epoch:{0:3d}, Train_Loss:{1:1.3f}, Valid_Loss:{2:1.3f}\"\n                  .format(epoch, train_loss, val_loss))","28418a50":"plt.figure(figsize=(8,8))\nplt.plot(train_losses, '-o', label='Train Losses')\nplt.plot(val_losses, 'g-o', label='Valid Losses')\nplt.legend()","e9cd7f1a":"def create_embedding(encoder, full_loader, embedding_dim, device):\n    \"\"\"\n    Creates embedding using encoder from dataloader.\n    Returns: Embedding of size (num_images_in_loader + 1, c, h, w)\n    \"\"\"\n\n    encoder.eval()\n\n    embedding = torch.randn(embedding_dim)\n    \n    with torch.no_grad():\n        for batch_idx, (train_img, target_img) in enumerate(full_loader):\n            train_img = train_img.to(device)\n            enc_output = encoder(train_img).cpu()\n            embedding = torch.cat((embedding, enc_output), 0)\n    \n    return embedding","25f4857d":"embedding_shape = (1, 256, 11, 4)\nembeddings = create_embedding(encoder, full_loader, embedding_shape, device)","3e8c5a44":"def get_similar_image_idx(image_tensor, num_images, embeddings, device):\n    \"\"\"\n    Given an image and number of similar images to search.\n    Returns the num_images closest neares images.\n    \"\"\"\n    image_tensor = image_tensor.unsqueeze(0)\n    image_tensor = image_tensor.type(torch.cuda.FloatTensor)\n    \n    with torch.no_grad():\n        image_embedding = encoder(image_tensor).cpu().detach().numpy()\n        \n    flattened_embedding = image_embedding.reshape((image_embedding.shape[0], -1))\n\n    knn = NearestNeighbors(n_neighbors=num_images, metric=\"cosine\")\n    knn.fit(embeddings.reshape(embeddings.shape[0], -1))\n\n    _, indices = knn.kneighbors(flattened_embedding)\n    indices_list = indices.tolist()\n    return indices_list[0][0]","a8f1b863":"img_idx = 0\n\nsimilar_idx = get_similar_image_idx(alike_ds[img_idx][0], 1, embeddings, device)","7bba6835":"def draw_similar_idx(df, img_idx, similar_idx):\n    imgs = [df[img_idx][0].permute(1, 2, 0), df[similar_idx][0].permute(1, 2, 0)]\n    f, ax = plt.subplots(1, 2, figsize=(12, 12))\n    for ix, img in enumerate(imgs):\n        ax[ix].imshow(img)\n        ax[ix].axis('off')","c089d287":"draw_similar_idx(alike_ds, img_idx, similar_idx)","32481464":"## Decoder","474b5609":"# Images Loaders","2cd3cd9a":"# Embeddings","d4f45b68":"# k-nearest neighbors","1da3f2c7":"# Goal\n\nCreate a model that given an image as input, retrieves the image of the most similar image.","1e45ef1f":"# Dataset Model","7959554a":"# Plot Losses","4f6cd21a":"<h1 align=center style='color:blue; border:1px dotted blue;'>Image Similarity Search in PyTorch<\/h1>","f373ecaa":"# Model","34e2ff3c":"# Training","174d4d23":"# Load Data","b98402f9":"1. First we train the models for autoeconding:\n\n[Input] -> [Encoder] -> [Decoder] -> [MSE] -> [Optimize]\n\n2. We combine all the encoders in order to create the embeddings:\n\n[Encoders] -> [Embeddings]\n\n3. Train a k-nn model with the embeddings, use an image to find the nearest neighbors embeddings and retrieve the image\n\n[Image] -> [k-nn] -> [Similar Image]","fde994c4":"# Architecture","2fc65534":"## Encoder"}}