{"cell_type":{"dcd0556d":"code","b10a1f9f":"code","85bfd0d1":"code","88d216cd":"code","498450f4":"code","55397f37":"code","ade8379a":"code","d89e9045":"code","41d18ca6":"code","a379663a":"code","bc6acda4":"code","7d4ab4d7":"code","7af006ea":"code","80c63940":"code","03d0c338":"code","7f068e61":"code","e24f71f7":"code","f2425de4":"code","76cd7365":"code","402c621f":"code","4ca3bc88":"code","09a99f8c":"code","3fa8312c":"code","9ca5491e":"code","6f41471c":"code","71dc1f9c":"code","d1c1b746":"code","2e07f807":"code","d9fd9ac5":"code","f52b1c63":"code","c47c14e0":"code","f15b7be2":"code","710164ec":"code","3d74425c":"code","e50e4ccf":"code","a24a6590":"code","017f2076":"markdown","59d4c135":"markdown","dc8f732d":"markdown","03c4be7b":"markdown","60d178b3":"markdown","88f37218":"markdown","31fdbe72":"markdown","6c9f11ef":"markdown","4685ee81":"markdown","72d87969":"markdown","b148a2a1":"markdown","99474d6e":"markdown","60381186":"markdown","00f61b66":"markdown","fa6f89cf":"markdown","361f7e5c":"markdown","13c7f8ba":"markdown","97808ed6":"markdown","2f3476ae":"markdown","fcf2349f":"markdown","e169ec70":"markdown"},"source":{"dcd0556d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Graphics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.offline as offline\npy.init_notebook_mode(connected=True)\ninit_notebook_mode(connected=True)\noffline.init_notebook_mode()\n%matplotlib inline\n\n# Sklearn and TensorFlow\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.utils.testing import all_estimators\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nimport tensorflow as tf\n\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=RuntimeWarning)\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b10a1f9f":"df_application_train = pd.read_csv(\"\/kaggle\/input\/home-credit-default-risk\/application_train.csv\")\nprint(\"Shape:\", df_application_train.shape)\ndf_application_train.head()","85bfd0d1":"df_temp = df_application_train[\"TARGET\"].value_counts()\n\ntrace = go.Pie(\n    labels = df_temp.index,\n    values = df_temp.values,\n)\n\ndata = [trace]\n\nlayout = go.Layout(\n    title = \"Loan Repayed or not\",\n    xaxis=dict(\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    yaxis=dict(\n        titlefont=dict(\n            size=16,\n            color='rgb(107, 107, 107)'\n        ),\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n)\n)\n\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(template=\"seaborn\")\npy.iplot(fig)","88d216cd":"df_temp = df_application_train[\"NAME_CONTRACT_TYPE\"].value_counts()\n\nfig = {\n  \"data\": [\n    {\n      \"values\": df_temp.values,\n      \"labels\": df_temp.index,\n      \"domain\": {\"x\": [0, .48]},\n      \"hole\": .7,\n      \"type\": \"pie\"\n    },\n    \n    ],\n  \"layout\": {\n        \"title\":\"Contract type\",\n    }\n}\n\nfig = go.Figure(fig)\nfig.update_layout(template=\"seaborn\")\niplot(fig)","498450f4":"df_temp = df_application_train[\"NAME_FAMILY_STATUS\"].value_counts()\n\nfig = {\n  \"data\": [\n    {\n      \"y\": df_temp.values,\n      \"x\": df_temp.index,\n      \"type\": \"bar\"\n    },\n    \n    ],\n  \"layout\": {\n        \"title\":\"Family Status\",\n    }\n}\n\nfig = go.Figure(fig)\nfig.update_layout(template=\"seaborn\")\niplot(fig)","55397f37":"df_temp = df_application_train[\"OCCUPATION_TYPE\"].value_counts()\n\nfig = {\n  \"data\": [\n    {\n        \"x\": df_temp.index,\n        \"y\": df_temp.values,\n        \"type\": \"bar\"\n    },\n    \n    ],\n  \"layout\": {\n        \"title\":\"Occupation of applicant\\'s\"\n    }\n}\n\nfig = go.Figure(fig)\nfig.update_layout(template=\"seaborn\")\niplot(fig)","ade8379a":"df_temp = pd.DataFrame(df_application_train.isnull().sum().sort_values(ascending=False)\/len(df_application_train), columns=[\"MISSING_VALUES\"])\ndf_temp = df_temp[df_temp[\"MISSING_VALUES\"] > 0]\n\nfig = {\n  \"data\": [\n    {\n        \"x\": df_temp.index,\n        \"y\": df_temp.MISSING_VALUES.values,\n        \"type\": \"bar\"\n    },\n    \n    ],\n  \"layout\": {\n        \"title\":\"Columns with missing values (%)\"\n    }\n}\n\nfig = go.Figure(fig)\nfig.update_layout(template=\"seaborn\")\niplot(fig)","d89e9045":"# Missing values\n## For low number of missing values we'll apply the mean for float64 objects and \"Other\" for string columns.\n\nlow_missing_values_col = [\"DAYS_LAST_PHONE_CHANGE\", \"CNT_FAM_MEMBERS\", \"AMT_ANNUITY\", \"AMT_GOODS_PRICE\", \"EXT_SOURCE_2\",\n                          \"DEF_30_CNT_SOCIAL_CIRCLE\", \"DEF_60_CNT_SOCIAL_CIRCLE\", \"OBS_60_CNT_SOCIAL_CIRCLE\",\n                          \"OBS_30_CNT_SOCIAL_CIRCLE\", \"NAME_TYPE_SUITE\"]\n\nfor col in low_missing_values_col:\n    if df_application_train[col].dtype == \"object\":\n        df_application_train[col].fillna(\"Other\", inplace=True)\n    elif df_application_train[col].dtype == \"float64\":\n        df_application_train[col].fillna(df_application_train[col].mean(), inplace=True)\n    else:\n        print(\"{} has a different dtype.\".format(col))\n    print(\"{}:\".format(col), df_application_train[col].dtype)","41d18ca6":"# Medium quantity\n## The strategy for medium quantity of missing values will be remove rows with np.nan.\n\nmedium_missing_values_col = [\"AMT_REQ_CREDIT_BUREAU_HOUR\", \"AMT_REQ_CREDIT_BUREAU_MON\", \"AMT_REQ_CREDIT_BUREAU_WEEK\",\n                             \"AMT_REQ_CREDIT_BUREAU_DAY\", \"AMT_REQ_CREDIT_BUREAU_YEAR\", \"AMT_REQ_CREDIT_BUREAU_QRT\",\n                            \"EXT_SOURCE_3\"]\n\nprint(\"df_application_train shape before remove missing rows:\", df_application_train.shape)\n    \nfor col in medium_missing_values_col:\n    print(col, df_application_train[col].dtype)\n    if col == \"EXT_SOURCE_3\":\n        print(\"Mean:\", df_application_train[col].mean(), \"\\n\")\n    else:\n        print(df_application_train[col].unique(), \"\\n\")\n\ndf_application_train.dropna(subset=medium_missing_values_col, inplace=True)\nprint(\"df_application_train shape after remove missing rows:\", df_application_train.shape)","a379663a":"# For high quantity of missing values, we'll drop these columns\ndf_application_train.dropna(axis=\"columns\", how=\"any\", inplace=True)\ndf_application_train.shape","bc6acda4":"# Check missing values\nmissing_values = df_application_train.isnull().sum().sum()\nprint(\"Quantiy of missing values:\", missing_values)","7d4ab4d7":"# Categorical features\ndf_application_train_dtypes = pd.DataFrame(df_application_train.dtypes, columns=[\"dtypes\"])\ncat_features = df_application_train_dtypes[(df_application_train_dtypes[\"dtypes\"] == \"object\") | (df_application_train_dtypes[\"dtypes\"] == \"category\")].index.tolist()\nprint(\"Categorical features ({}):\".format(len(cat_features)), cat_features)","7af006ea":"from sklearn.preprocessing import LabelEncoder\n\n# instantiate labelencoder object\nle = LabelEncoder()\n\n# apply le on categorical feature columns\ndf_application_train[cat_features] = df_application_train[cat_features].apply(lambda col: le.fit_transform(col))\ndf_application_train.loc[:][cat_features].head(10)","80c63940":"#Using Pearson Correlation\nplt.figure(figsize=(12,10))\ncor = df_application_train.corr()\nsns.heatmap(cor, cmap=plt.cm.Reds)\nplt.show()","03d0c338":"#Correlation with TARGET variable\ncor_target = abs(cor[\"TARGET\"])\n\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.05].sort_values(ascending=False)\nrelevant_features[1:]","7f068e61":"# Pairplot only for relevant_features\nsns.pairplot(df_application_train, hue=\"TARGET\", vars=relevant_features.index.tolist())\nplt.show()","e24f71f7":"# Split in X and Y and apply MinMaxScaler (0, 1)\n\nX = df_application_train.drop(columns=[\"SK_ID_CURR\", \"TARGET\"])\ny = df_application_train[\"TARGET\"].values\n\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X.values)\n\nprint(\"X shape:\", X.shape)","f2425de4":"def do_pca(n_components, data):\n    '''\n    Transforms data using PCA to create n_components, and provides back the results of the\n    transformation.\n\n    INPUT: n_components - int - the number of principal components to create\n           data - the data you would like to transform\n\n    OUTPUT: pca - the pca object created after fitting the data\n            X_pca - the transformed X matrix with new number of components\n    '''\n    X = StandardScaler().fit_transform(data)\n    pca = PCA(n_components)\n    X_pca = pca.fit_transform(X)\n    return pca, X_pca\n\ndef scree_plot(pca):\n    '''\n    Creates a scree plot associated with the principal components \n    \n    INPUT: pca - the result of instantian of PCA in scikit learn\n            \n    OUTPUT:\n            None\n    '''\n    num_components=len(pca.explained_variance_ratio_)\n    ind = np.arange(num_components)\n    vals = pca.explained_variance_ratio_\n \n    plt.figure(figsize=(10, 6))\n    ax = plt.subplot(111)\n    cumvals = np.cumsum(vals)\n    ax.bar(ind, vals)\n    ax.plot(ind, cumvals)\n    #for i in range(num_components):\n    #    ax.annotate(r\"%s%%\" % ((str(vals[i]*100)[:4])), (ind[i]+0.2, vals[i]), va=\"bottom\", ha=\"center\", fontsize=12)\n \n    ax.xaxis.set_tick_params(width=0)\n    ax.yaxis.set_tick_params(width=2, length=12)\n \n    ax.set_xlabel(\"Principal Component\")\n    ax.set_ylabel(\"Variance Explained (%)\")\n    plt.title('Explained Variance Per Principal Component')\n    \npca, X_pca = do_pca(X.shape[1], X)\nX_pca = None #just cleaning memory\nscree_plot(pca)","76cd7365":"n_components = 40\npca, X_pca = do_pca(n_components, X)\n\nprint(\"Explained variance for {} components: {:.2f}%\".format(n_components, sum(pca.explained_variance_ratio_)*100))","402c621f":"print(\"PCA: Top 10 Explained variance\", \"\\n\")\nfor i in range(len(pca.explained_variance_ratio_[:10])):\n    print(\"component {}: {:.2f}%\".format(i+1, pca.explained_variance_ratio_[i]*100))","4ca3bc88":"# Split the dataset in train and test.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nprint(\"train size:\", len(X_train))\nprint(\"test size:\", len(X_test))","09a99f8c":"class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n\nclass_weights = {\n    0: class_weights[0],\n    1: class_weights[1]\n}\n\nclass_weights","3fa8312c":"print(\"Sklearn's classifier algorithms with class_weight attribute:\", \"\\n\")\nestimators = all_estimators(type_filter='classifier')\nfor name, class_ in estimators:\n    try:\n        if hasattr(class_(), 'class_weight'): \n            print(name)\n    except:\n        pass","9ca5491e":"%%time\nclf_log = LogisticRegression(solver=\"liblinear\", class_weight=class_weights)\nclf_log.fit(X_train, y_train)\n\ny_pred = clf_log.predict(X_test)\n\nprint(classification_report(y_test, y_pred, digits=4))","6f41471c":"%%time\nclf_rf = RandomForestClassifier(n_estimators=50, class_weight=class_weights, n_jobs=-1, min_samples_leaf=200, max_depth=30)\nclf_rf.fit(X_train, y_train)\n\ny_pred = clf_rf.predict(X_test)\n\nprint(classification_report(y_test, y_pred, digits=4))","71dc1f9c":"%%time\n# GridSearch for RandomForestClassifier\nmodel = RandomForestClassifier(class_weight=class_weights)\n\nparameters = {\n    \"n_estimators\": [10, 30, 50, 100],\n    \"criterion\": [\"gini\", \"entropy\"],\n    \"max_depth\": [10, 20, 30],\n    \"min_samples_leaf\": [50, 100, 200]    \n}\n\nclf_rf = GridSearchCV(model, parameters, cv=5, n_jobs=-1, verbose=1)\nclf_rf.fit(X_train, y_train)\n\ny_pred = clf_rf.predict(X_test)\nprint(clf_rf.best_estimator_)\nprint(\"\\n\")\nprint(classification_report(y_test, y_pred, digits=4))","d1c1b746":"%%time\nfrom lightgbm import LGBMClassifier\nclf_lgbmc = LGBMClassifier(objective='binary', metric='auc', class_weight=class_weights, n_estimators=1000, max_depth=20)\n\nclf_lgbmc.fit(X_train, y_train,\n              eval_set = [(X_test, y_test)],\n              early_stopping_rounds=50,\n              verbose=0)\n\ny_pred = clf_lgbmc.predict(X_test, num_iteration=clf_lgbmc.best_iteration_)\n\nprint(classification_report(y_test, y_pred, digits=4))","2e07f807":"%%time\n# GridSearch for LGBMClassifier\nmodel = LGBMClassifier(class_weight=class_weights, objective='binary', metric='auc')\n    \nparameters = {\n    \"n_estimators\": [100, 300, 500, 1000],\n    \"boosting_type\": [\"gbdt\", \"dart\", \"goss\"],\n    \"max_depth\": [10, 20, 30] \n}\n\nclf_lgbmc = GridSearchCV(model, parameters, cv=5, n_jobs=-1, verbose=1)\nclf_lgbmc.fit(X_train, y_train)\n\ny_pred = clf_lgbmc.predict(X_test)\n\nprint(classification_report(y_test, y_pred, digits=4))","d9fd9ac5":"%%time\nimport keras.backend as K\n\nX_train_k = X_train\ny_train_k = np.array(y_train)\n\nX_test_k = X_test\ny_test_k = np.array(y_test)\n\ndef f1_keras(y_true, y_pred):\n    y_pred = K.round(y_pred)\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    precision = tp \/ (tp + fp + K.epsilon())\n    recall = tp \/ (tp + fn + K.epsilon())\n\n    f1 = 2*precision*recall \/ (precision+recall+K.epsilon())\n    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)\n\n# TensorFlow\/Keras\nclf_keras = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(512, input_dim=X_train_k.shape[1], activation=\"relu\"),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(256, activation=\"relu\"),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(128, activation=\"relu\"),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(64, activation=\"relu\"),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n    \nclf_keras.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\", f1_keras])\n\nclf_keras.fit(X_train_k, y_train_k, epochs=50, class_weight=class_weights, use_multiprocessing=True, batch_size=128)\n\nmodel_evals = clf_keras.evaluate(X_test_k, y_test_k)\n\nprint(\"\\n\")\nprint(\"Evaluate Model\")\nprint(\"Loss: {}\".format(model_evals[0]))\nprint(\"Accuracy: {}\".format(model_evals[1]))\nprint(\"F1-Score: {}\".format(model_evals[2]))","f52b1c63":"clf_keras.summary()","c47c14e0":"def shame_function(y_pred):\n    \"\"\"\n    This function update y_pred for 1 or 0.\n    \n    I think that have a better solution for that, but I don't find anything at this time.\n    \"\"\"\n    result = []\n    for n in y_pred:\n        if n[0] >= 0.5:\n            result.append(1)\n        else:\n            result.append(0)\n    return result\n\ny_pred = shame_function(clf_keras.predict(X_test))\n\nprint(\"y_test\")\nprint(\"% of target == 1: {:.2f}%\".format((len(y_test.tolist())\/sum(y_test.tolist()))))\nprint(y_test.tolist()[:10])\nprint(\"\\n\")\nprint(\"y_pred\")\nprint(\"% of target == 1: {:.2f}%\".format((len(y_pred)\/sum(y_pred))))\nprint(y_pred[:10])","f15b7be2":"def preprocessing(df, n_components=None, return_pca=False):\n    # Missing values\n    low_missing_values_col = [\"DAYS_LAST_PHONE_CHANGE\", \"CNT_FAM_MEMBERS\", \"AMT_ANNUITY\", \"AMT_GOODS_PRICE\", \"EXT_SOURCE_2\",\n                              \"DEF_30_CNT_SOCIAL_CIRCLE\", \"DEF_60_CNT_SOCIAL_CIRCLE\", \"OBS_60_CNT_SOCIAL_CIRCLE\",\n                              \"OBS_30_CNT_SOCIAL_CIRCLE\", \"NAME_TYPE_SUITE\"]\n\n    for col in low_missing_values_col:\n        if df[col].dtype == \"object\":\n            df[col].fillna(\"Other\", inplace=True)\n        elif df[col].dtype == \"float64\":\n            df[col].fillna(df[col].mean(), inplace=True)\n\n\n    medium_missing_values_col = [\"AMT_REQ_CREDIT_BUREAU_HOUR\", \"AMT_REQ_CREDIT_BUREAU_MON\", \"AMT_REQ_CREDIT_BUREAU_WEEK\",\n                                 \"AMT_REQ_CREDIT_BUREAU_DAY\", \"AMT_REQ_CREDIT_BUREAU_YEAR\", \"AMT_REQ_CREDIT_BUREAU_QRT\",\n                                \"EXT_SOURCE_3\"]\n\n    for col in medium_missing_values_col:\n        if df[col].dtype == \"object\":\n            df[col].fillna(\"Other\", inplace=True)\n        elif df[col].dtype == \"float64\":\n            df[col].fillna(df[col].mean(), inplace=True)\n            \n    df.dropna(axis=\"columns\", how=\"any\", inplace=True)\n    \n    # Split in X and Y\n    X = df.dropna(axis=\"columns\", how=\"any\")\n    X.drop(columns=[\"SK_ID_CURR\", \"TARGET\"], inplace=True, errors=\"ignore\")\n\n    try:\n        y = df[\"TARGET\"].values\n    except:\n        y = None\n        print(\"TARGET column not found.\")\n    \n    X_dtypes = pd.DataFrame(X.dtypes, columns=[\"dtypes\"])\n    cat_features = X_dtypes[(X_dtypes[\"dtypes\"] == \"object\") | (X_dtypes[\"dtypes\"] == \"category\")].index.tolist()\n    \n    # instantiate labelencoder object\n    le = LabelEncoder()\n    # apply le on categorical feature columns\n    X[cat_features] = X[cat_features].apply(lambda col: le.fit_transform(col))\n    X.loc[:][cat_features].head(10)\n\n    scaler = MinMaxScaler()\n    X = scaler.fit_transform(X.values)\n\n    if n_components == None:\n        n_components = X.shape[1]\n        \n    pca, X_pca = do_pca(n_components, X)\n    \n    if y != None and return_pca != False:\n        return X_pca, y\n    elif y != None and return_pca == True:\n        return X_pca, y, pca\n    elif y == None and return_pca == True:\n        return X_pca, pca\n    else:\n        return X_pca\n\ndef submission_file(model, keras_model=False, n_components=70, filename=\"submission.csv\"):\n    \"\"\"\n    Args\n    \n    model: Model to predict\n    keras_model: True\/False if we are using Keras Model or not.\n    n_components: To preprocessing PCA.\n    \n    Return\n    Link to submission file download.\n    \"\"\"\n    from IPython.display import FileLink\n    \n    df_application_test = pd.read_csv(\"\/kaggle\/input\/home-credit-default-risk\/application_test.csv\")\n    submission = preprocessing(df_application_test, n_components=n_components)\n\n    if keras_model:\n        predicts = shame_function(model.predict(submission))\n    else:\n        predicts = model.predict(submission)\n\n    df_sample_submission = pd.read_csv(\"\/kaggle\/input\/home-credit-default-risk\/sample_submission.csv\")\n    df_sample_submission[\"TARGET\"] = predicts\n\n    df_sample_submission.to_csv(filename, index=False)\n    print(\"{} salved.\".format(filename))\n    return FileLink(r'{}'.format(filename))","710164ec":"# LogisticRegression\nsubmission_file(clf_log, n_components=70,\n                filename=\"submission_LogisticRegression.csv\")","3d74425c":"# RandomForest\nsubmission_file(clf_rf, n_components=70,\n                filename=\"submission_RandomForest.csv\")","e50e4ccf":"# LGBMClassifier\nsubmission_file(clf_lgbmc, n_components=70,\n                filename=\"submission_LGBMClassifier.csv\")","a24a6590":"# TensorFlow\/Keras\nsubmission_file(clf_keras, keras_model=True, n_components=70,\n                filename=\"submission_TensorFlow-Keras.csv\")","017f2076":"## \/\/ pandas profiling\n\nI like to run [pandas_profiling](https:\/\/pandas-profiling.github.io\/pandas-profiling\/docs\/) in the dataset. It's provide a lot of information about a dataset and we can do a quick analysis with it.\n\nBecause of the dataset size it could not be run on Kaggle Notebook.\n\nI ran this on [AWS SageMaker](https:\/\/aws.amazon.com\/sagemaker\/) (ml.m5.24xlarge, vCPU: 96, Mem (GiB): 384GB). \u00af\\\\_(\u30c4)_\/\u00af\n\n[dataset.overview.html](https:\/\/dougtrajano.github.io\/udacity_capstone_project\/dataset_overview.html)","59d4c135":"> What sklearn algorithms has **class_weight** as attribute?","dc8f732d":"[RandomForestClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html)","03c4be7b":"---\n# \/\/ prepare dataset\n\n\nIn this section I'll check the best features and prepare the dataset to be used as **X** and **Y** in the modeling.\n\nWe will convert categorical features to numeric features using [LabelEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html).\n\nWe will remove some features that aren't important or are inappropriate for the model as well. As example, it's inappropriate to use the gender for predict if a person can pay or not the loan.","60d178b3":"---\n\n# \/\/ PCA\n\nNow we have 70 features in the X. Can we apply a dimensionality reduction technique?\n\nYes! PCA (Principal Component Analysis) can discovery it for us what importance each feature has in the dataset.","88f37218":"## \/\/ missing values\n\nThe strategy to handle with missing values is divided by low, medium and high quantity of missing values. For each of these we will apply a different technique to solve it.","31fdbe72":"> We can see that we have an imbalanced classes. We will need to work with before fit the model.","6c9f11ef":"---\n\n# \/\/ metric's definition\n\nAs metric to evaluate models I'll use [F1-Score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html). See below a little description about this metric.\n\n> The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n>\n> `F1 = 2 * (precision * recall) \/ (precision + recall)`\n\nAnother metric that we will use is the Kaggle's Score. We have a csv file called `application_test.csv` that can be used to predict and submit to Kaggle. Kaggle will provide a score for us.","4685ee81":"---\n\n# \/\/ modeling\n\nI tried build model with commum machine learning algorithms (LogisticRegression, RandomForestClassifier, etc.) and neural networks with TensorFlow\/Keras.\n\nSee below the best results for this dataset.","72d87969":"---\n# \/\/ load datasets\n\nWe'll working only with `df_application_train.csv` in this study.","b148a2a1":"> Handling unbalanced classes","99474d6e":"> Top occupations and it's percentage.\n> \n> - Laborers: 17,95%\n> - Sales staff: 10,44%\n> - Core staff: 8,97%\n> - Managers: 6,95%\n> - Drivers: 6,05%","60381186":"[LGBMClassifier](https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.LGBMClassifier.html)","00f61b66":"# Home Credit Default Risk\n\nThis is part of my capstone project on [Data Scientist nanodegree](https:\/\/www.udacity.com\/course\/data-scientist-nanodegree--nd025) at [Udacity](www.udacity.com)! \\o\/\n\nAuthor: Douglas Trajano\n> [LinkedIn](https:\/\/www.linkedin.com\/in\/douglas-trajano\/) | [GitHub](https:\/\/github.com\/DougTrajano)\n\n---\n\n# \/\/ objective\n\n> Can you predict how capable each applicant is of repaying a loan?\n\nWell, it's a binary classification between `0` and `1`. Customer can repay or not his loan?\n\n---\n\n# \/\/ dataset\n\nThe dataset has 7 files. All these files can be downloaded [here](https:\/\/www.kaggle.com\/c\/home-credit-default-risk\/data)\n\n![](https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/home-credit\/home_credit.png)\n\n### application_{train|test}.csv\n> Size (train): 308k x 122\n>\n> Size (test): 48.7k x 121\n\n- This is the main table, broken into two files for Train (with TARGET) and Test (without TARGET).\n- Static data for all applications. One row represents one loan in our data sample.\n\n\n### bureau.csv\n> Size: 1.72m x 17\n\n- All client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in our sample).\n- For every loan in our sample, there are as many rows as number of credits the client had in Credit Bureau before the application date.\n\n### bureau_balance.csv\n> Size: 27.3m x 3\n\n- Monthly balances of previous credits in Credit Bureau.\n- This table has one row for each month of history of every previous credit reported to Credit Bureau \u2013 i.e the table has (#loans in sample * # of relative previous credits * # of months where we have some history observable for the previous credits) rows.\n\n### POS_CASH_balance.csv\n> Size: 10.0m x 8\n\n- Monthly balance snapshots of previous POS (point of sales) and cash loans that the applicant had with Home Credit.\n- This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample \u2013 i.e. the table has (#loans in sample * # of relative previous credits * # of months in which we have some history observable for the previous credits) rows.\n\n### credit_card_balance.csv\n> Size: 3.84m x 23\n\n- Monthly balance snapshots of previous credit cards that the applicant has with Home Credit.\n- This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample \u2013 i.e. the table has (#loans in sample * # of relative previous credit cards * # of months where we have some history observable for the previous credit card) rows.\n\n### previous_application.csv\n> Size: 1.67m x 37\n\n- All previous applications for Home Credit loans of clients who have loans in our sample.\n- There is one row for each previous application related to loans in our data sample.\n\n### installments_payments.csv\n> Size: 13.6m x 8\n\n- Repayment history for the previously disbursed credits in Home Credit related to the loans in our sample.\n- There is a) one row for every payment that was made plus b) one row each for missed payment.\n- One row is equivalent to one payment of one installment OR one installment corresponding to one payment of one previous Home Credit credit related to loans in our sample.\n\n### HomeCredit_columns_description.csv\n> Size: 219 x 5\n\n- This file contains descriptions for the columns in the various data files.","fa6f89cf":"---\n\n# \/\/ exploratory analysis\n\nIn this part of the notebook we will explore the dataset.\n\nWe will work with `application_train.csv` because this dataset has 122 characteristics for the loans, we have 307511 loans (one loan for row).\n\nWe also has a TARGET that we will use to fit the model.","361f7e5c":"[TensorFlow](https:\/\/www.tensorflow.org)\/[Keras](http:\/\/keras.io)","13c7f8ba":"> 63,88% applicants are married.","97808ed6":"---\n\n# \/\/ submission\n\nIn this section I'll create a submission.csv file to send to the Leaderboard.","2f3476ae":"---\n# \/\/ results\n\nYou can see the complete result in the [medium post](https:\/\/medium.com\/@dougtrajano\/ia-applied-in-credit-risk-home-credit-b70412ef8f02).","fcf2349f":"[LogisticRegression](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html)","e169ec70":"We have a lot of features with missing values. We need to handle with it before create our model."}}