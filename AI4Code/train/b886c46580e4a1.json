{"cell_type":{"e9372575":"code","d8d18765":"code","4d1dc7ec":"code","4cc61aa0":"code","c72d1f01":"code","d4993b29":"code","fe5aa445":"code","1db6b59e":"code","49f3cfd0":"code","8b2d96c1":"code","2b955d82":"code","078afeef":"code","47312625":"code","4b7ca2d4":"code","a1a2a944":"code","f19bb4ed":"code","be596a56":"code","0ab3a934":"code","428918f0":"code","216f270b":"code","352d9e97":"code","1f3d47e9":"code","1a868223":"code","c3ec735d":"code","7827c16a":"code","3f85a0cd":"code","7ea102db":"code","c1481968":"code","b3df6b7e":"code","5b8e93f7":"code","8277de91":"code","17fd8b64":"code","d49fd8ab":"code","16344438":"code","819ac90b":"code","870180a3":"code","c47318d8":"code","6c9c7757":"code","5791afce":"code","67725461":"code","342d4818":"code","1de601a3":"code","e4dc874c":"code","2c96b5f0":"code","b76289a7":"code","ed170f76":"code","e8684913":"code","32baa093":"code","62223677":"code","32526138":"code","921344d5":"code","a7f2e50b":"code","bea10c10":"code","bc84f20e":"code","54fd66ec":"code","e1df27d9":"code","1ca4e6cb":"code","bb1c3744":"code","22b58427":"code","23fabc73":"code","6f8b1934":"code","7da37e12":"code","6656fc43":"code","1050c1dd":"code","40d37af5":"code","1593df31":"code","51e6ba43":"code","40743f86":"code","77929b99":"code","30b2d026":"code","ff7ad0b5":"code","a63bb6f2":"code","2b4038ea":"code","a41b5384":"code","3b019ce8":"code","c4325989":"code","ac7aab8e":"code","73d9fe30":"code","16a6cd2e":"code","f68e85cd":"code","38e00a5e":"code","1f2a9a26":"code","f9b93943":"code","b197ba5a":"code","da5580a9":"markdown","aea349e8":"markdown","d654e2e1":"markdown","200a0497":"markdown","a8173612":"markdown","17830472":"markdown","a1c77edc":"markdown","eb66deb4":"markdown","99c0a592":"markdown","744e0efd":"markdown","1af64e0d":"markdown","88565d2d":"markdown","cf4f1d64":"markdown","2a920667":"markdown","5214b3d7":"markdown","76f728e4":"markdown","951b120b":"markdown","99a6654b":"markdown","59bffdaa":"markdown","dc2b9ac6":"markdown","11ff7c36":"markdown","6a555fb5":"markdown","34315d0a":"markdown","859baaea":"markdown","56ae2421":"markdown","e38f2629":"markdown","0228bbde":"markdown","a67c4710":"markdown","53f86432":"markdown","7213801d":"markdown","b3c5e399":"markdown","1ac52aa7":"markdown","9242106b":"markdown","e2b4fd01":"markdown","e251aec4":"markdown","0ec16d00":"markdown","00f46bef":"markdown","879bc2e3":"markdown","18373946":"markdown","6f0622f2":"markdown","aa49b57e":"markdown","0a60d942":"markdown","b9650379":"markdown","df5a638e":"markdown","18865367":"markdown","abc45a2a":"markdown","f342d105":"markdown","54f88c64":"markdown","31f20db8":"markdown","039da890":"markdown","a37c37af":"markdown","cb9fd8ca":"markdown","6ed6bbf0":"markdown","8d96c58e":"markdown","18216d6c":"markdown","8ddb1fb0":"markdown","94dc9942":"markdown","136a33c4":"markdown","2324bdea":"markdown","6adb178b":"markdown","f8c175cf":"markdown","70d1c85e":"markdown","cb335438":"markdown","9b037e27":"markdown","085a5d77":"markdown","10d7b1da":"markdown","0a951ab5":"markdown","74d528ce":"markdown","bb6043e8":"markdown","c4152f6b":"markdown","23e40efd":"markdown","987b5067":"markdown","5cb8fa48":"markdown","118bb878":"markdown","eda5a1ec":"markdown","a24583c6":"markdown","2c91c896":"markdown","dea656cb":"markdown","afa4042d":"markdown","ae312cda":"markdown","78ed28aa":"markdown","31fe58d5":"markdown","53565682":"markdown","ec8c5c91":"markdown","b495b635":"markdown","f389a4d9":"markdown","883d2456":"markdown","9c6dd42d":"markdown","8ffa3729":"markdown","0b05d0fa":"markdown","29851a1f":"markdown","8a0f37fc":"markdown"},"source":{"e9372575":"import numpy as np\nimport seaborn as sns\nimport pandas as pd\nfrom pandas import DataFrame\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression","d8d18765":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)","4d1dc7ec":"all_data.shape","4cc61aa0":"all_data","c72d1f01":"missing_values = (all_data.isnull().sum() \/ len(all_data)) * 100\nmissing_values = missing_values.drop(missing_values[missing_values == 0].index).sort_values(ascending=False)\nmissing_values.head(30)","d4993b29":"is_nan = all_data['LotFrontage'] == 0\n","fe5aa445":"plt.scatter(x = all_data['LotFrontage'], y = all_data['SalePrice'])\nplt.ylabel('SalePrice', fontsize=15)\nplt.xlabel('LotFrontage', fontsize=15)\nplt.show()","1db6b59e":"all_data[all_data[\"LotFrontage\"] > 250]","49f3cfd0":"plt.scatter(x = all_data['MasVnrArea'], y = all_data['SalePrice'])\nplt.ylabel('SalePrice', fontsize=15)\nplt.xlabel('MasVnrArea', fontsize=15)\nplt.show()","8b2d96c1":"plt.scatter(x = all_data['GarageYrBlt'], y = all_data['SalePrice'])\nplt.ylabel('SalePrice', fontsize=15)\nplt.xlabel('GarageYrBlt', fontsize=15)\nplt.show()","2b955d82":"correlation = train.corr().abs()\ncorrelation[correlation > 0.5][\"SalePrice\"]","078afeef":"plt.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=15)\nplt.xlabel('GrLivArea', fontsize=15)\nplt.show()","47312625":"plt.scatter(x = train['GarageCars'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=15)\nplt.xlabel('GarageCars', fontsize=15)\nplt.show()","4b7ca2d4":"import seaborn as sns\n\ndef create_frequency_chart(data, x_axis, category, is_categorical = False, ticks = 10):\n    new_data = data.copy()\n    if not is_categorical:\n        buket_size = max(new_data[x_axis].values) \/ ticks\n        bins = [i * buket_size for i in range(ticks)]\n        new_data[\"buckets\"] = pd.cut(new_data[x_axis], bins)\n\n    #Try to fill the code when the x_axis is not continuous\n\n    sns.catplot(x=\"buckets\", kind=\"count\", data=new_data, hue=category, aspect=5)","a1a2a944":"create_frequency_chart(train, \"SalePrice\", \"GarageCars\")","f19bb4ed":"plt.scatter(x = train['OverallQual'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=15)\nplt.xlabel('OverallQual', fontsize=15)\nplt.show()","be596a56":"create_frequency_chart(train, \"SalePrice\", \"OverallQual\")","0ab3a934":"plt.scatter(x = train['TotalBsmtSF'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=15)\nplt.xlabel('TotalBsmtSF', fontsize=15)\nplt.show()","428918f0":"data_sample = train[(train['TotalBsmtSF'] < 2000) & (train['TotalBsmtSF'] > 0)]\n","216f270b":"plt.scatter(x = data_sample['TotalBsmtSF'], y = data_sample['SalePrice'])\nplt.ylabel('SalePrice', fontsize=15)\nplt.xlabel('TotalBsmtSF', fontsize=15)\nplt.show()","352d9e97":"plt.scatter(x = train['1stFlrSF'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=15)\nplt.xlabel('1stFlrSF', fontsize=15)\nplt.show()","1f3d47e9":"plt.scatter(x = train['FullBath'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=15)\nplt.xlabel('FullBath', fontsize=15)\nplt.show()","1a868223":"#A .corr() method will be added to the data and passed as the first argument.\n#An argument called annot=True helps display the correlation coefficient(deleted for better visualization)\n#A good trick used to reduce the number displayed and improve readability: fmt =\u2019.3g'or fmt = \u2018.1g' \n#The next 3 arguments are for rescaling the colorbar.\n#vmin is the minimum value of the bar; vmax is the maximum value of the bar; and center\n#Changing shape of matrixes into squares using square = True\n\nplt.subplots(figsize=(15, 15))\n\nsns.heatmap(train.corr(), fmt = '.2g', vmax = .6, square = True);\n\n","c3ec735d":"train[\"GarageYrBlt\"]","7827c16a":"plt.subplots(figsize=(15, 15))\nk = 16 #number of variables for heatmap\ncorrematrix = train.corr()\ncols = corrematrix.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale= 1)\nsns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 8}, yticklabels=cols.values, xticklabels=cols.values)\n","3f85a0cd":"all_data.shape","7ea102db":"all_data = all_data.drop(all_data[(all_data['GrLivArea'] > 4000) & (all_data['SalePrice'] < 250000)].index)","c1481968":"all_data = all_data.drop(all_data[(all_data['LotFrontage'] > 250) & (all_data['SalePrice'] < 300000)].index)","b3df6b7e":"all_data.shape","5b8e93f7":"all_data = all_data.drop(all_data[(all_data['MasVnrArea'] > 1500) ].index)","8277de91":"all_data = all_data.drop([\"Id\"], axis=1)","17fd8b64":"all_data.shape","d49fd8ab":"cols = [\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\"]\n\nfor col in cols:\n    all_data[col] = all_data[col].fillna(\"None\")","16344438":"for value in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[value] = all_data[value].fillna('None')","819ac90b":"for value in ('GarageArea', 'GarageCars'):\n    all_data[value] = all_data[value].fillna(0)","870180a3":"for value in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[value] = all_data[value].fillna(0)","c47318d8":"for value in ('BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[value] = all_data[value].fillna(0)","6c9c7757":"all_data[\"LotFrontage\"] = all_data[\"LotFrontage\"].fillna(0)\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n","5791afce":"all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","67725461":"plt.scatter(x = all_data['TotalSF'], y = all_data['SalePrice'])\nplt.ylabel('SalePrice', fontsize=15)\nplt.xlabel('TotalSF', fontsize=15)\nplt.show()","342d4818":"cols = [\"MSSubClass\",\"OverallCond\", \"YrSold\", \"MoSold\"]\n\nfor col in cols:\n    all_data[col] = all_data[col].apply(str)","1de601a3":"from sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\nall_data[\"MSZoning\"] = labelencoder.fit_transform(all_data[\"MSZoning\"].astype(str))\n","e4dc874c":"cols = ['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold', 'GarageYrBlt']\nfor col in cols:\n    all_data[col] = LabelEncoder().fit_transform(list(all_data[col].values))","2c96b5f0":"all_data = pd.get_dummies(all_data)","b76289a7":"train = all_data[~all_data[\"SalePrice\"].isna()]\ntest = all_data[all_data[\"SalePrice\"].isna()]","ed170f76":"test[test.isna().any(axis=1)]","e8684913":"x_train = train.drop([\"SalePrice\"], axis=1)\ny_train = train[\"SalePrice\"].values\nx_test = test.drop([\"SalePrice\"], axis=1)","32baa093":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.20, random_state=42)","62223677":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge","32526138":"ridgeReg = Ridge(alpha=5,  fit_intercept=True, normalize='deprecated', copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)","921344d5":"ridgeReg = ridgeReg.fit(X_train, y_train)\npred_val = ridgeReg.predict(X_val)\nfrom sklearn.metrics import max_error\nprint(f\"Maximum error in Ridge Regression is: {max_error(y_val, pred_val)}\")","a7f2e50b":"from sklearn.metrics import r2_score\n\nr2_score(y_val, pred_val)","bea10c10":"#Feature importance\n\n#features = pd.DataFrame({\"feature\": ridgeReg.feature_names_in_, \"importance\": ridgeReg.coef_})\n#features.sort_values(\"importance\", ascending=False).head()\n#print(f\"{len(features[features['importance'] != 0])} are used in the model\")","bc84f20e":"from sklearn.linear_model import Lasso","54fd66ec":"LassoReg = Lasso(alpha=20, fit_intercept=True, normalize='deprecated', precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')","e1df27d9":"LassoReg = LassoReg.fit(X_train, y_train)","1ca4e6cb":"predict_val = LassoReg.predict(X_val)\nfrom sklearn.metrics import max_error\nprint(f\"Maximum error in Lasso Regression is {max_error(y_val, predict_val)}\")\n","bb1c3744":"from sklearn.metrics import r2_score\n\nr2_score(y_val, predict_val)","22b58427":"#Feature importance\n\n#features = pd.DataFrame({\"feature\": LassoReg.feature_names_in_, \"importance\": LassoReg.coef_})\n#features.sort_values(\"importance\", ascending=False).head()\n#print(f\"{len(features[features['importance'] != 0])} are used in the model\")","23fabc73":"from sklearn.tree import DecisionTreeRegressor\n\n\nclf = DecisionTreeRegressor(splitter='best', max_depth=400, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, ccp_alpha=0.0)","6f8b1934":"clf = clf.fit(X_train, y_train)\npred_val = clf.predict(X_val)","7da37e12":"#Feature importance\n\nfeatures = pd.DataFrame({\"feature\":train.drop([\"SalePrice\"], axis=1).columns, \"importance\": clf.feature_importances_})\nprint(f\"{len(features[features['importance'] != 0])} are used in the model\")","6656fc43":"features.sort_values(\"importance\", ascending=False)","1050c1dd":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nprint(f\"Mean Squared Error in Decision Tree Regressor is {mean_squared_error(y_val, pred_val)}\")\nprint(f\"Mean Absolute Error in Decision Tree Regressor is {mean_absolute_error(y_val, pred_val)}\")","40d37af5":"print(f\"Maximum error in Decison Tree Regression is {max_error(y_val, pred_val)}\")","1593df31":"from sklearn.metrics import r2_score\n\nr2_score(y_val, pred_val)","51e6ba43":"#!pip install xgboost==1.5.0","40743f86":"import pandas as pd\nimport xgboost as xgb\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error","77929b99":"xgb_model = xgb.XGBRegressor(\n    n_estimators=100,\n    reg_lambda=1,\n    gamma=0,\n    max_depth=3\n)","30b2d026":"xgb_model.fit(X_train, y_train)","ff7ad0b5":"#Feature Importance\n\nfeatures = pd.DataFrame({\"feature\":train.drop([\"SalePrice\"], axis=1).columns, \"importance\": xgb_model.feature_importances_})\nprint(f\"{len(features[features['importance'] != 0])} are used in the model\")\n","a63bb6f2":"features.sort_values(\"importance\", ascending=False)","2b4038ea":"predict_val = xgb_model.predict(X_val)","a41b5384":"print(mean_squared_error(y_val, predict_val))\nprint(mean_absolute_error(y_val, predict_val))","3b019ce8":"predict_val = xgb_model.predict(X_val)\nprint(f\"Maximum error in XGBoost model is {max_error(y_val, predict_val)}\")\nprint(f\"The r2_score is {r2_score(y_val, predict_val)}\")","c4325989":"xgb_model = xgb.XGBRegressor(n_estimators=500)\nxgb_model.fit(X_train, y_train, early_stopping_rounds=5, \n             eval_set=[(X_val, y_val)], verbose=False)","ac7aab8e":"predict_val = xgb_model.predict(X_val)\nprint(f\"Maximum error in XGBoost model is {max_error(y_val, predict_val)}\")\nprint(f\"The r2_score is {r2_score(y_val, predict_val)}\")","73d9fe30":"\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm, skew #to help us run some statistics\n\nx_train = train.drop([\"SalePrice\"], axis=1)\ny_train = train[\"SalePrice\"].values\nx_test = test.drop([\"SalePrice\"], axis=1)\n\n#Distribution\nsns.distplot(y_train , fit = norm);","16a6cd2e":"#Let us also check out the QQ-plot\nfig = plt.figure()\nres = stats.probplot(y_train, plot=plt)\nplt.show()","f68e85cd":" \ny_train_alt = np.log1p(y_train)\n\n#Check the new distribution \nsns.distplot(y_train_alt , fit=norm);","38e00a5e":"#Let us also check out the QQ-plot\nfig = plt.figure()\nres = stats.probplot(y_train_alt, plot=plt)\nplt.show()","1f2a9a26":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train_alt, y_val_alt = train_test_split(x_train, y_train_alt, test_size=0.20, random_state=42)","f9b93943":"xgb_model = xgb_model.fit(X_train, y_train_alt)\npred_val = xgb_model.predict(X_val)\nprint(f\"R2_score in Decision Tree Regression is {r2_score(y_val_alt, pred_val)}\")","b197ba5a":"pred_val = np.expm1(pred_val)\nprint(f\"Maximum error in Decison Tree Regression is {max_error(y_val, pred_val)}\")","da5580a9":"# House Price Prediction project\n\nLet's import all the usefull library for this project:","aea349e8":"Some categorical variables are regarded as numerical features. We will transform them into categorical data.","d654e2e1":"Let's plot another feature with potential outliers; MasVnrArea.\n\nMasVnrArea stands for Masonry veneer area in square feet. \n\nMasonry Veneer is a nonstructural facing of brick, stone, concrete masonry or other masonry material securely attached to a wall or backing. ","200a0497":"Now we will check if there are more missing values in any of the features.","a8173612":"The SalePrice correlation matrix brings a clearer picture of what we discussed regarding correlations above.\n\nA new observation is made from the matrix:\n\n'GarageCars' and 'GarageArea' are  strongly correlated variables. They also have approximately similar correlations with other features throughout the matrix. This could be because  the number of cars that fit into the garage is a consequence of the garage area. They are interrelated. \n","17830472":"Our next plot will be a scatterplot of SalePrice against the TotalBsmtSF Feature \n\n\nTotalBsmtSF: Total square feet of basement area","a1c77edc":"Classification is a type of supervised machine learning problem where the target (response) variable is categorical.\n\nGiven the training data, which contains the known label, the classifier approximates a mapping function (f) from the input variables (X) to output variables (Y).","eb66deb4":"Let's expand the graph a bit more so we can see a clearer pattern from the plot. We will use data_sample","99c0a592":"Let's twist the n_estimators parameter using early_stopping_rounds and see if it improves our model.\n\n    The argument early_stopping_rounds offers a way to automatically find the ideal value. Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for n_estimators. ","744e0efd":"PoolQC : data description says NA means \"No Pool\". \n\nWhich make sense because of the huge ratio of missing value (+99%) and also because not every house has a pool. \n\nThis case is similar for MiscFeature, Alley, Fence, and FirePlaceQu\n\nWe will replace all NA values with None so they are not considered missing values.","1af64e0d":"BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 \n\nThese are categorical basement-related features, NaN means that there is no basement so we will replace them with None.","88565d2d":"We will create a correlation matrix to represent the correlation between features.\n\nFor data scientists, checking correlations is an important part of the exploratory data analysis process. This is one of the methods used to decide which features affect the target variable the most, and in turn, get used in predicting this target variable.","cf4f1d64":"# Data Engineering","2a920667":"# Model Design\n\n\nResearch and summarize 4 models:\n- Summary of the model\n- Advantages and drawbacks\n- Train and predict the validation data\n- Look at the feature importance, create evaluation matrices\n- Select the final model and explain why you picked it\n\nFor performance metrics, refere to https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.metrics\n\n- Tree based model (Look at random forest as well): https:\/\/scikit-learn.org\/stable\/auto_examples\/tree\/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py\n\n\n- Linear Regression (with regularization, Lasso and Ridge): https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html?highlight=linear%20regression#sklearn.linear_model.LinearRegression\n\n- Xgboost (Extension of Tree based model): https:\/\/towardsdatascience.com\/xgboost-python-example-42777d01001e?gi=37505fc61178\n\n- Multilayer Perceptron networkv (optional): https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor","5214b3d7":"GarageArea and GarageCars are numerical variables. Missing values for these features may indicate that the house has no Garage.\n\nTherefore we will replace missing values with 0. To indicate no(0) GarageArea and 0 number of garage cars","76f728e4":"Let's update the missing values from 'LotFrontage' and 'MasVnrArea'. \n\nSince they are numerical features, we will replace missing values with zeros(0)","951b120b":"# Data Analysis\n\nLet's list all the features that contains outliers or missing values:\n\n- Missing values: Alley, MasVnrType,BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, Electrical, FirePlaceQu, GarageType, GarageFinish, GarageQual, GarageCond\n- Potential outliers:  LotFrontage, MasVnrArea, GarageYrBlt\n\n \n","99a6654b":"Starting with the GrLivArea Feature","59bffdaa":"# Data Cleaning","dc2b9ac6":"Parameters used in Ridge Regression\n\nalpha(float,  default=1.0)\n\nRegularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. \n\nfit_intercept (bool, default=True) \n\nUsed to fit the intercept for this model. If set to false, no intercept will be used in calculations (i.e. X and y are expected to be centered).\n\nnormalize (bool, default=False)\n\nThis parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use StandardScaler before calling fit on an estimator with normalize=False.\n\nmax_iter (int, default=None)\n\nRefers to the Maximum number of iterations for conjugate gradient solver. For \u2018sparse_cg\u2019 and \u2018lsqr\u2019 solvers, the default value is determined by scipy.sparse.linalg. \n\ntol (float, default=1e-3)\n\nPrecision of the solution.\n\n","11ff7c36":"XGBoost is a short term for Extreme Gradient Boost. \n\nThe values in each leaf are the residuals. That is, the difference between the prediction and the actual value of the independent variable, and not the sample of the dependent variable.\n\nGain is the improvement in accuracy brought about by the split. The split that is done by linear scanning.\n\nAfter calculating the gain, the optimal threshold is the threshold with maximum gain.\n\nNote: When the gain is negative, it implies that the split does not yield better results than would otherwise have been the case had we left the tree as it was.\n\nAdvantages and Disadvantages.\n\n    - Great for supervised learning\n\n    - Difficult to interpret the results.","6a555fb5":"We will specifically check for the correlation between saleprice and other features.","34315d0a":"### Lasso","859baaea":"We will create a new feature called TotalSF by adding the features: bsmtSF + 1stFlrSF + 2ndFlrSF  and then check how it relates to SalePrice\n\nTotalSF will represent the total square footage of the house.","56ae2421":"Let's check the relative importance attributed to each feature, in determining the price of the house.","e38f2629":"From the scatter plot, it is hard to determine if there is any trend or how the FullBath Feature affects the sale Price.","0228bbde":"MsSubClass - The Building Class\n\nBuilding class usually denotes the general condition of a commercial property and indicates quality of location and amenities.\n\nSo this is a category.","a67c4710":"### Inputing Missing Values","53f86432":"### Linear Regression(with regularization, Lasso and Ridge)","7213801d":"We will Plot all the Features with Potential Outliers. \n\nStarting with LotFrontage.\n\nLotFrontage stands for  Linear feet of street connected to property","b3c5e399":"### Correlation Matrix","1ac52aa7":"Let's fit our model","9242106b":"Now we will intialize an instance of XGBRegressor Class. \n\nWe will also choose the number of estimators, maximum depth, as well as the values of lambda and gamma","e2b4fd01":"We will also replace GarageType, GarageFinish, GarageQual and GarageCond missing data with None.","e251aec4":"The train-test split procedure is used to estimate the performance of machine learning algorithms when they are used to make predictions on data not used to train the model.\n\nIt is appropriate when you have a very large dataset, a costly model to train, or require a good estimate of model performance quickly.\n\nIt is good to choose a split percentage that meets the project\u2019s objectives with considerations of:\nComputational cost in training and evaluating the model as well as the Training and Test set representativeness.\n\nAnother important consideration is that rows are assigned to the train and test sets randomly. This is doe by setting the \u201crandom_state\u201d to an integer value.","0ec16d00":"This model however uses way less features compared to Lasso and Ridge. It is important to tweek the attributes and see how it can be improved","00f46bef":"Let's load our dataset in memory: ","879bc2e3":"XGBoost has a few parameters that can dramatically affect the model's accuracy and training speed\n\nn_estimators: specifies how many times to go through the modeling cycle.\n\n    Too low a value causes underfitting, which is inaccurate predictions on both training data and new data. Too large a value causes overfitting, which is accurate predictions on training data, but inaccurate predictions on new data (which is what we care about). Typical values range from 100-1000.","18373946":"### SalePrice Correlation Matrix","6f0622f2":"We will label encode other categorical features together by creating a list and using a for loop.","aa49b57e":"A score of 0.86... indicates there is some good perfomance in the model as it is closer to 1.0, the best score.","0a60d942":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https:\/\/deepnote.com?utm_source=created-in-deepnote-cell&projectId=dce2ee20-40d8-40b8-af4f-e6174e68a806' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image\/svg+xml;base64,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' > <\/img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote<\/span><\/a>","b9650379":"Now we will look at the 1stFlrSF Feature. \n\nWill be interesting to see how the square footage of the first floor may affect sale price of a house.","df5a638e":"The QQ Plot is more linear, less curvy. ","18865367":"Regression trees are type of decision trees since the set of splitting rules used to segment the\npredictor space can be summarized in a tree. \n\nIn a regression tree, each leaf represents a numerical value.\nValue that brings the least residual becomes the root of the tree\n\nThese involve stratifying or segmenting the predictor space into a number of simple regions.\n\nDecision trees are typically drawn upside down, in the sense that the leaves are at the bottom of the tree.\n\nThe points along the tree where the predictor space is split are referred to as internal nodes.\n\nRandom Forest method grows multiple trees which are combined to yield a single consesus prediction.\n\n\nAdvantages and Disadvantages\n\n    - They are simple and useful for interpretation.\n\n    - Trees can easily handle qualitative predictors without the need to create dummy variables.\n\n    - Combining a large number of trees can often result in dramatic improvements in prediction accuracy, at the expense of some loss interpretation.\n\n    - In terms of prediction accuracy, they are not competitive with the best learning approaches.\n\nBagging, is a general-purpose procedure for reducing the variance of a statistical learning method.\n\nRandom forests provide an improvement over bagged trees by way of a small tweak that decorrelates the trees. This reduces the variance when we average the trees.\n\nSimilar to bagging, in random forests we build a number of decision trees in bootstrapped training samples.\n","abc45a2a":"Now we will remove the outliers from the features plotted above","f342d105":"Let's first analyse the GrLivArea feature:\n\nGrLivArea - Ground Living area in the house\n\nPloting GrLivArea with SalePrice to look at the data distribution.","54f88c64":"### Tree Based Model","31f20db8":"Maximum error here is about twice as small compared to the max error in Decision Tree Regressor\n\nThe r2_score is also the highest compared to all other models.","039da890":"### Potential Outliers","a37c37af":"We will analyse all features that have higher correlatins to the SalePrice.","cb9fd8ca":"### OneHotEncoder","6ed6bbf0":"Let's look at the data","8d96c58e":"LotFrontage\n\nWe will drop LotFrontage x- values greater than 250 since they are outliers","18216d6c":"MasVnrArea\n\nThe 0 x-values were very congested, we will remove them to see a better visualization of the plot\n\nWe will also remove the 1600 x-value since it is an outlier","8ddb1fb0":"From the scatter plot, SalePrice seems to increase as the GrLivArea increases, A Linear relationship: With an exception of a few values with GrLivArea greater than 4000 that show a different behavior(outliers). \n\nWhile cleaning the data, we will remove the outliers to minimize errors in the model.","94dc9942":"### Missing Values","136a33c4":"### XGBoost","2324bdea":"### Ridge","6adb178b":"The Q-Q plot, or quantile-quantile plot, is a graphical tool to help us assess if a set of data plausibly came from some theoretical distribution such as a Normal or exponential.\n\nNotice that the points in our plot form a curve instead of a straight line. Normal Q-Q plots that look like this usually mean the sample data are skewed.","f8c175cf":"We will check the feature importnce and see how many featires are used in the model","70d1c85e":"So in our case, increasing the n_estimators led to an increase in maximum error and a decrease in r2_score.","cb335438":"r2_score\n\n-It is the Coefficient of determination , used to evaluate the performance of a model.\n\nBest score is 1.0 and least could be 0.","9b037e27":"From the observation above in xgboost model, we see that working on skewness did not improve the performance of the model. \n\nThe model seems to be resistant to the skewness of the data","085a5d77":"### Loading Data Set","10d7b1da":"Skewness measures the shift of the distribution from the normal bell curve.\n\nPositive skew value denotes right shift whereas negative skew value denotes left shift.","0a951ab5":"Thereafter we evaluate the model and make predictions","74d528ce":"### Skewness of Data","bb6043e8":"Label Encoding - changes categorical data or text to be represented numerically.\n\nSo we will transform categorical data into numerical data before applying the models. Starting with MSZoning","c4152f6b":"From the plot, most of the values are compacted between the 0 and 150 values of LotFrontage measurement. \n\nIt shows a general trend of SalePrice gradually increasing as the LotFrontage increases with some outliers as the Lot Frontage increases above 150.","23e40efd":"Changing overall condition into categorical data\n\nTransforming YrSold and MoSold as well because the year or month is a mere category, it does not signify anything else numerically. Cannot be measured.","987b5067":"We can use the mean squared error, maximum error and r2_score to evaluate the model performance. The mean squared error is the average of the differences between the predictions and the actual values squared.","5cb8fa48":"What one hot encoding does is, it takes a column which has categorical data, which has been label encoded, and then splits the column into multiple columns. The numbers are replaced by 1s and 0s, depending on which column has what value.","118bb878":"Then we build and fit a model just like we did with Lasso and Ridge","eda5a1ec":"### Higher Correlations","a24583c6":"1stFlrSF - First Floor Square Footage\n\nFrom the scatterplot, there are a few outliers which do not follow the general trend: Sale Price increases as the 1stFlrSF increases; A positive Linear Relationship.","2c91c896":"### Variable Transformation","dea656cb":"Decision Tree Parameters;\n\n-Splitter(default = 'best') ; Is the strategy used to choose a split at each node.'best' chooses the best split and 'random'chooses the best random split.\n\n-Max_depth(default = 'None') ; Refers to the maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split.\n\n-min_samples_split(default = 2) ; the minimum number of samples required to split an internal node.\n\n-min-samples_leaf(default = 1) ; The minimum number of samples required to be at a leaf node.\n\n-min_weight_fraction_leaf(dedault = 0.0) ; The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n\n-max_features(default = 'None') ; refers to the maximum number of features to be considered when looking for the BEST Split.\n\n-random_state(default = 'None') ; controls the randomness of an estimator. \n\n-min_impurity_decrease(default = 0.0) ; this parameter makes a node to be split if the split brings about a decrease of the impurity.\n\nccp_alpha(default = 0.0) ; Complexity parameter used for Minimal Cost-Complexity Pruning(an algorithm used to prune a tree to avoid over-fitting). The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. ","afa4042d":"### Train and Test Split","ae312cda":"BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath:\n\nMissing values in these numerical basement features are more likely because the house has no basement.Therefore we will replace them with 0.","78ed28aa":"Our next plot will be a scatterplot of SalePrice against the GarageCars \n\nThey have a correlation of approx. 0.64","31fe58d5":"From the list generated above, features that have higher correlations(greater than 0.5) are:-\n\nOverallQual, YearBuilt, YearRemodAdd, TotalBsmntSF, 1stFloorSF, GrLivArea, FullBath, TotRmsAbvGrd, GarageCars, and GarageArea","53565682":"### Label Encoding","ec8c5c91":"Summary of the Model\n\n    - Uses Ordinary Least Squares\n\n    - Linear model with coefficients that minimize the residual some of squares between observed and predicted targets.\n\nRidge and Lasso regression are some of the simple techniques to reduce model complexity and prevent over-fitting which may result from simple linear regression.\n\nRidge Regression; - In ridge regression, the cost function is altered by adding a penalty equivalent to square of the magnitude of the coefficients.\n\nRidge Regression shrinks the coefficients and it helps to reduce the model complexity and multi-collinearity. Ridge estimates sparse coefficients with l2 regularization\n\nLeast Absolute Shrinkage And Selection Operator regularization (LASSO)\n\nIn Lasso Regression, the main difference is that instead of  taking the square of the coefficients, magnitudes are taken into account. Lasso estimates sparse coefficients with l1 regularization\n\nThe Lasso is a shrinkage and selection method for linear regression. An L1 penalty minimizes the size of all coefficients and allows any coefficient to go to the value of zero, effectively removing input features from the model.\n\nLasso Regression helps in feature selection and reducing overfitting.\n\nThe computation of the lasso solutions is a quadratic programming problem, and can be tackled by standard numerical analysis algorithms.","b495b635":"We will evaluate our model perfomance by calculating the errors and r2_score of the model","f389a4d9":"We will go through each feature and try to get a better understanding as to why they have missing values.\n\nPoolQc - Pool Quality\n\nPoolQc may have a lot of missing values because it is hard to determine the quality of the pool. Also, because it is common for houses to not have pools.\n\nAlley - Type of Alley access\n\nA lot of data is missing for the alley feature because there may not be alleys accessible to the house.\n\nMiscFeature - Miscellaneous feature not covered in other categories\n\nMost houses may not have features that aren't already cvered in other categories.\n\nFence - Fence Quality\n\nA lot of houses do not have fences surrounding them. A good example is row houses. So the missing values of fences could be because there were no fences in the houses to begin with.\n\nFirePlaceQu - Fire Place Quality\n\nMissing values here are labeled as nan ,  which was used to indicate houses with no fireplace. In this case, they were not missing values instead they represented 0 values.\n\nLot Frontage - Linear feet of street connected to property\n\n\nGarageType - Garage Location\n\nThis feature is easy to describe and categorize since it depends on the garage location whether it is builtin, attached to the house, basement garage or detached from the house. So, If there are any missing values then it is because there is no garage in the house.\n\nGarageYrBlt - Year garage was built\n\nMissing values would be because the data for when the garage was built could not be found\/determined.\n\nGarageFinish - Interior finish of the garage\n\nGarage Interior finish was categorized into finished, roughly finished, unfinished and no garage. Data values with NA\/NaN values indicate that the house does not have a garage.\n\nGarageQual - Garage Quality ,  GarageCond. - Garage Condition.\n\nSimilar to GarageFinish, the missing values in both GarageQual and GarageCond may indicate the absence of a garage in a house.\n\nBsmt Exposure - Refers to walkout or garden level wall\n\nSome houses have no exposure in their basements and others have no basements. Missing values could be due to either of the two.\n\nBsmtFinType2 - Rating of basement finished area (if multiple types)\n\n\n\n\n","883d2456":"From the xgb model, The Overall Quality and The Total Square Footage of the house are the factors which highly determine its price. This makes sense because when people are trying to get a house they keenly consider quality and house size to be determining factors.","9c6dd42d":"Another potential outlier is the feature GarageYrBlt. We will plot this as well to see the relationship between SalePrice and Year when the garage was built.","8ffa3729":"From the Plot, SalePrice seems to increase as the TotalSF increases. One value at TotalSF = 7000(Approx.) seems to be a bit higher and far from the rest. I assume it could be because some of the square footage is a great vegetation\/garden area or has interesting features within the house.\n\nFor instance most houses with huge square footage would have themed basements as a pool table area or some sort of underground bar.","0b05d0fa":"To transform the target Variable 'SalePrice, we will use log transformation.\n\nLog transformation in our case will be done using the  numpy fuction log1p which  applies log(1+x) to all elements of the column","29851a1f":"The nature of the plot does not give us a lot of information that can be analyzed.\n\nA general trend of Sale Price increasing as the number of GarageCars increases is seen as we expected. With a few outliers from the position 4 of GarageCars. The outliers indicate that there may be other factors affecting the sale price.\n\nLet us create a box plot instead to get a better understanding of the data from GarageCars and SalePrice.","8a0f37fc":"From the boxplot above, there a few observations seen.\n\nThe plot shows its common for houses within the price range [75500, 151000] to have two garage cars. More than 400 were counted.\n\nAlso, there seems to be a fair amount of houses with 3 garage cars when the saleprice is >200,000.\n\nAs the price range increases, the number of houses with garage cars decreases. So does this mean expensive houses do not have built in garages or maybe data has recorded less houses within the more expensive price range? \n"}}