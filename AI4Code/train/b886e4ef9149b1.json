{"cell_type":{"1cad4b46":"code","40483091":"code","2833b27b":"code","b97fdc00":"code","d2821d58":"code","149d05db":"code","54dc8fe2":"code","b874edb9":"code","a72bed5c":"code","71921d8a":"code","48f50263":"code","177d527e":"code","aaa4bebe":"code","4189fa73":"code","863407bb":"code","59b14bde":"code","29465d2f":"code","f535ae27":"code","d5dcbf87":"code","fb061ebd":"markdown","f02dbcd9":"markdown","a5ef60df":"markdown","56feba3d":"markdown","a89ac285":"markdown","bbf6cc2d":"markdown","95a81b6d":"markdown","95b0c979":"markdown","4b54458c":"markdown","d9281822":"markdown","af3b06e2":"markdown","c4db91bb":"markdown","f4f567c4":"markdown","4b602846":"markdown","90c7431a":"markdown","3ebc047c":"markdown","3673e2c6":"markdown","3c6d2188":"markdown","b306da72":"markdown","a2d237c4":"markdown","60953ea2":"markdown","a9a5489c":"markdown","37e25431":"markdown","5f4ff1c6":"markdown","d944c65a":"markdown","0680ff38":"markdown","1f423dd5":"markdown","6b8093ad":"markdown","c4f15d5c":"markdown","273d4070":"markdown","ee2f2576":"markdown","af2c4b98":"markdown"},"source":{"1cad4b46":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ntrain = pd.read_csv('..\/input\/human-activity-recognition-with-smartphones\/train.csv')\ntrain.head()","40483091":"train.shape","2833b27b":"test = pd.read_csv(\"\/kaggle\/input\/human-activity-recognition-with-smartphones\/test.csv\")","b97fdc00":"test.shape","d2821d58":"print('No of duplicates in train: {}'.format(sum(train.duplicated())))\nprint('No of duplicates in test : {}'.format(sum(test.duplicated())))","149d05db":"print('We have {} NaN\/Null values in train'.format(train.isnull().values.sum()))\nprint('We have {} NaN\/Null values in test'.format(test.isnull().values.sum()))","54dc8fe2":"# import libraries for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('whitegrid')\nplt.rcParams['font.family'] = 'Dejavu Sans'","b874edb9":"plt.figure(figsize=(16,8))\nplt.title('Data provided by each user', fontsize=20)\nsns.countplot(x='subject',hue='Activity', data = train)\nplt.show()\n","a72bed5c":"plt.title('No of Datapoints per Activity', fontsize=15)\nsns.countplot(train.Activity)\nplt.xticks(rotation=90)\nplt.show()","71921d8a":"columns = train.columns\n\n# Removing '()' from column names\ncolumns = columns.str.replace('[()]','')\ncolumns = columns.str.replace('[-]', '')\ncolumns = columns.str.replace('[,]','')\n\ntrain.columns = columns\ntest.columns = columns\n\ntest.columns","48f50263":"train.to_csv('f_train.csv', index=False)\ntest.to_csv('f_test.csv', index=False)","177d527e":"sns.set_palette(\"Set1\", desat=0.80)\nfacetgrid = sns.FacetGrid(train, hue='Activity', size=6,aspect=2)\nfacetgrid.map(sns.distplot,'tBodyAccMagmean', hist=False)\\\n    .add_legend()\nplt.annotate(\"Stationary Activities\", xy=(-0.956,17), xytext=(-0.9, 23), size=20,\\\n            va='center', ha='left',\\\n            arrowprops=dict(arrowstyle=\"simple\",connectionstyle=\"arc3,rad=0.1\"))\n\nplt.annotate(\"Moving Activities\", xy=(0,3), xytext=(0.2, 9), size=20,\\\n            va='center', ha='left',\\\n            arrowprops=dict(arrowstyle=\"simple\",connectionstyle=\"arc3,rad=0.1\"))\nplt.show()","aaa4bebe":"# for plotting purposes taking datapoints of each activity to a different dataframe\ndf1 = train[train['Activity']==1]\ndf2 = train[train['Activity']==2]\ndf3 = train[train['Activity']==3]\ndf4 = train[train['Activity']==4]\ndf5 = train[train['Activity']==5]\ndf6 = train[train['Activity']==6]\n\n\n\nplt.figure(figsize=(12,8))\nplt.subplot(1,2,1)\nplt.title(\"Static Activities(closer view)\")\nsns.distplot(train[train[\"Activity\"]==\"SITTING\"]['tBodyAccMagmean'],hist = False, label = 'Sitting')\nsns.distplot(train[train[\"Activity\"]==\"STANDING\"]['tBodyAccMagmean'],hist = False,label = 'Standing')\nsns.distplot(train[train[\"Activity\"]==\"LAYING\"]['tBodyAccMagmean'],hist = False, label = 'Laying')\nplt.axis([-1.02, -0.5, 0, 35])\nplt.subplot(1,2,2)\nplt.title(\"Dynamic Activities(closer view)\")\nsns.distplot(train[train[\"Activity\"]==\"WALKING\"]['tBodyAccMagmean'],hist = False, label = 'Sitting')\nsns.distplot(train[train[\"Activity\"]==\"WALKING_DOWNSTAIRS\"]['tBodyAccMagmean'],hist = False,label = 'Standing')\nsns.distplot(train[train[\"Activity\"]==\"WALKING_UPSTAIRS\"]['tBodyAccMagmean'],hist = False, label = 'Laying')","4189fa73":"plt.figure(figsize=(7,7))\nsns.boxplot(x='Activity', y='tBodyAccMagmean',data=train, showfliers=False, saturation=1)\nplt.ylabel('Acceleration Magnitude mean')\nplt.axhline(y=-0.7, xmin=0.1, xmax=0.9,dashes=(5,5), c='g')\nplt.axhline(y=-0.05, xmin=0.4, dashes=(5,5), c='m')\nplt.xticks(rotation=90)\nplt.show()","863407bb":"sns.boxplot(x='Activity', y='angleXgravityMean', data=train)\nplt.axhline(y=0.08, xmin=0.1, xmax=0.9,c='m',dashes=(5,3))\nplt.title('Angle between X-axis and Gravity_mean', fontsize=15)\nplt.xticks(rotation = 40)\nplt.show()","59b14bde":"sns.boxplot(x='Activity', y='angleYgravityMean', data = train, showfliers=False)\nplt.title('Angle between Y-axis and Gravity_mean', fontsize=15)\nplt.xticks(rotation = 40)\nplt.axhline(y=-0.22, xmin=0.1, xmax=0.8, dashes=(5,3), c='m')\nplt.show()","29465d2f":"import numpy as np\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f535ae27":"# performs t-sne with different perplexity values and their repective plots..\n\ndef perform_tsne(X_data, y_data, perplexities, n_iter=1000, img_name_prefix='t-sne'):\n        \n    for index,perplexity in enumerate(perplexities):\n        # perform t-sne\n        print('\\nperforming tsne with perplexity {} and with {} iterations at max'.format(perplexity, n_iter))\n        X_reduced = TSNE(verbose=2, perplexity=perplexity).fit_transform(X_data)\n        print('Done..')\n        \n        # prepare the data for seaborn         \n        print('Creating plot for this t-sne visualization..')\n        df = pd.DataFrame({'x':X_reduced[:,0], 'y':X_reduced[:,1] ,'label':y_data})\n        \n        # draw the plot in appropriate place in the grid\n        sns.lmplot(data=df, x='x', y='y', hue='label', fit_reg=False, size=8,\\\n                   palette=\"Set1\",markers=['^','v','s','o', '1','2'])\n        plt.title(\"perplexity : {} and max_iter : {}\".format(perplexity, n_iter))\n        img_name = img_name_prefix + '_perp_{}_iter_{}.png'.format(perplexity, n_iter)\n        print('saving this plot as image in present working directory...')\n        plt.savefig(img_name)\n        plt.show()\n        print('Done')\n","d5dcbf87":"X_pre_tsne = train.drop(['subject', 'Activity'], axis=1)\ny_pre_tsne = train['Activity']\nperform_tsne(X_data = X_pre_tsne,y_data=y_pre_tsne, perplexities =[5,10,20])","fb061ebd":"### 3. Magnitude of an acceleration can saperate it well","f02dbcd9":"\"___Without domain knowledge EDA has no meaning, without EDA a problem has no soul.___\"","a5ef60df":"### Observation\nOur data is well balanced (almost)","56feba3d":"\n* Accelerometer and Gyroscope readings are taken from 30 volunteers(referred as subjects) while performing the following 6 Activities.\n\n    1. Walking     \n    2. WalkingUpstairs \n    3. WalkingDownstairs \n    4. Standing \n    5. Sitting \n    6. Lying.\n\n\n* Readings are divided into a window of 2.56 seconds with 50% overlapping. \n\n* Accelerometer readings are divided into gravity acceleration and body acceleration readings,\n  which has x,y and z components each.\n\n* Gyroscope readings are the measure of angular velocities which has x,y and z components.\n\n* Jerk signals are calculated for BodyAcceleration readings.\n\n* Fourier Transforms are made on the above time readings to obtain frequency readings.\n\n* Now, on all the base signal readings., mean, max, mad, sma, arcoefficient, engerybands,entropy etc., are calculated for each window.\n\n* We get a feature vector of 561 features and these features are given in the dataset.\n\n* Each window of readings is a datapoint of 561 features.\n\n## Problem Framework\n\n* 30 subjects(volunteers) data is randomly split to 70%(21) test and 30%(7) train data.\n* Each datapoint corresponds one of the 6 Activities.\n","a89ac285":"# Apply t-sne on the data ","bbf6cc2d":"\n\n+ __Static and Dynamic Activities__\n\n    - In static activities (sit, stand, lie down) motion information will not be very useful.\n\t- In the dynamic activities (Walking, WalkingUpstairs,WalkingDownstairs) motion info will be significant.\n\n\n","95a81b6d":"we will remove the commas and brackets to out features so that we can apply directly","95b0c979":"# Quick overview of the dataset :\n","4b54458c":"# Exploratory Data Analysis","d9281822":"__ Observations__:\n- If tAccMean is < -0.8 then the Activities are either Standing or Sitting or Laying.\n- If tAccMean is > -0.6 then the Activities are either Walking or WalkingDownstairs or WalkingUpstairs.\n- If tAccMean > 0.0 then the Activity is WalkingDownstairs.\n- We can classify 75% the Acitivity labels with some errors.","af3b06e2":"save our data to csv file for future prediction ,we will use these files when we will do predictions in the next notebook","c4db91bb":"As we can see there are no duplicates in the train and test dataset","f4f567c4":"# HumanActivityRecognition\n\n<br>\n\n\nThis project is to build a model that predicts the human activities such as Walking, Walking_Upstairs, Walking_Downstairs, Sitting, Standing or Laying.\n\nThis dataset is collected from 30 persons(referred as subjects in this dataset), performing different activities with a smartphone to their waists. The data is recorded with the help of sensors (accelerometer and Gyroscope) in that smartphone. This experiment was video recorded to label the data manually.\n\n## How data was recorded\n\nBy using the sensors(Gyroscope and accelerometer) in a smartphone, they have captured '3-axial linear acceleration'(_tAcc-XYZ_) from accelerometer and '3-axial angular velocity' (_tGyro-XYZ_) from Gyroscope with several variations. \n\n> prefix 't' in those metrics denotes time.\n\n> suffix 'XYZ' represents 3-axial signals in X , Y, and Z directions.\n\n### Feature names\n\n1. These sensor signals are preprocessed by applying noise filters and then sampled in fixed-width windows(sliding windows) of 2.56 seconds each with 50% overlap. ie., each window has 128 readings. \n\n2. From Each window, a feature vector was obtianed by calculating variables from the time and frequency domain.\n> In our dataset, each datapoint represents a window with different readings \n3. The accelertion signal was saperated into Body and Gravity acceleration signals(___tBodyAcc-XYZ___ and ___tGravityAcc-XYZ___) using some low pass filter with corner frequecy of 0.3Hz.\n\n4. After that, the body linear acceleration and angular velocity were derived in time to obtian _jerk signals_ (___tBodyAccJerk-XYZ___ and ___tBodyGyroJerk-XYZ___). \n\n5. The magnitude of these 3-dimensional signals were calculated using the Euclidian norm. This magnitudes are represented as features with names like _tBodyAccMag_, _tGravityAccMag_, _tBodyAccJerkMag_, _tBodyGyroMag_ and _tBodyGyroJerkMag_.\n\n6. Finally, We've got frequency domain signals from some of the available signals by applying a FFT (Fast Fourier Transform). These signals obtained were labeled with ___prefix 'f'___ just like original signals with ___prefix 't'___. These signals are labeled as ___fBodyAcc-XYZ___, ___fBodyGyroMag___ etc.,.\n\n7. These are the signals that we got so far.\n\t+ tBodyAcc-XYZ\n\t+ tGravityAcc-XYZ\n\t+ tBodyAccJerk-XYZ\n\t+ tBodyGyro-XYZ\n\t+ tBodyGyroJerk-XYZ\n\t+ tBodyAccMag\n\t+ tGravityAccMag\n\t+ tBodyAccJerkMag\n\t+ tBodyGyroMag\n\t+ tBodyGyroJerkMag\n\t+ fBodyAcc-XYZ\n\t+ fBodyAccJerk-XYZ\n\t+ fBodyGyro-XYZ\n\t+ fBodyAccMag\n\t+ fBodyAccJerkMag\n\t+ fBodyGyroMag\n\t+ fBodyGyroJerkMag\n\n8. We can esitmate some set of variables from the above signals. ie., We will estimate the following properties on each and every signal that we recoreded so far.\n\n\t+ ___mean()___: Mean value\n\t+ ___std()___: Standard deviation\n\t+ ___mad()___: Median absolute deviation \n\t+ ___max()___: Largest value in array\n\t+ ___min()___: Smallest value in array\n\t+ ___sma()___: Signal magnitude area\n\t+ ___energy()___: Energy measure. Sum of the squares divided by the number of values. \n\t+ ___iqr()___: Interquartile range \n\t+ ___entropy()___: Signal entropy\n\t+ ___arCoeff()___: Autorregresion coefficients with Burg order equal to 4\n\t+ ___correlation()___: correlation coefficient between two signals\n\t+ ___maxInds()___: index of the frequency component with largest magnitude\n\t+ ___meanFreq()___: Weighted average of the frequency components to obtain a mean frequency\n\t+ ___skewness()___: skewness of the frequency domain signal \n\t+ ___kurtosis()___: kurtosis of the frequency domain signal \n\t+ ___bandsEnergy()___: Energy of a frequency interval within the 64 bins of the FFT of each window.\n\t+ ___angle()___: Angle between to vectors.\n\n9. We can obtain some other vectors by taking the average of signals in a single window sample. These are used on the angle() variable'\n`\n\t+ gravityMean\n\t+ tBodyAccMean\n\t+ tBodyAccJerkMean\n\t+ tBodyGyroMean\n\t+ tBodyGyroJerkMean\n\n\n###  Y_Labels(Encoded)\n+ In the dataset, Y_labels are represented as numbers from 1 to 6 as their identifiers.\n\n\t- WALKING as __1__\n\t- WALKING_UPSTAIRS as __2__\n\t- WALKING_DOWNSTAIRS as __3__\n\t- SITTING as __4__\n\t- STANDING as __5__\n\t- LAYING as __6__\n    \n## Train and test data were saperated\n - The readings from ___70%___ of the volunteers were taken as ___trianing data___ and remaining ___30%___ subjects recordings were taken for ___test data___\n \n## Data\n\n* All the data is present in 'UCI_HAR_dataset\/' folder in present working directory.\n     - Feature names are present in 'UCI_HAR_dataset\/features.txt'\n     - ___Train Data___\n         - 'UCI_HAR_dataset\/train\/X_train.txt'\n         - 'UCI_HAR_dataset\/train\/subject_train.txt'\n         - 'UCI_HAR_dataset\/train\/y_train.txt'\n     - ___Test Data___\n         - 'UCI_HAR_dataset\/test\/X_test.txt'\n         - 'UCI_HAR_dataset\/test\/subject_test.txt'\n         - 'UCI_HAR_dataset\/test\/y_test.txt'\n         \n\n## Data Size :\n> 27 MB\n","4b602846":"### 2. Stationary and Moving activities are completely different","90c7431a":"## Problem Statement\n\n + Given a new datapoint we have to predict the Activity","3ebc047c":"As we can see We have got almost same number of reading from all the subjects means there are not sygnificant difference in reading then we should not worry about it","3673e2c6":"# Data Cleaning","3c6d2188":"Here we will see these datapoints in 2 dimensions and try to observe the behaviour of the datapoints","b306da72":"\nAs we can clearly see the difference between stationary activities and Moving Activities\n\nas per above pdf distribution we can look closer by dividing these pdfs ","a2d237c4":"__ Observations__:\n* If angleX,gravityMean > 0 then Activity is Laying.\n* We can classify all datapoints belonging to Laying activity with just a single if else statement.","60953ea2":"## 5. Save this dataframe in a csv files","a9a5489c":"### ***Conclusion***\n\n-->As we can see all the features except standing and sitting can be seperated very easily \n\n--> Model will probably be confused between standing and sitting","37e25431":"## 4. Changing feature names ","5f4ff1c6":"## Predictions in the Next Notebook.....","d944c65a":"### 1. Featuring Engineering from Domain Knowledge \n","0680ff38":"## 1. Check for Duplicates","1f423dd5":"## 2. Checking for NaN\/null values","6b8093ad":"## Obtain the  train data ","c4f15d5c":"### 4. Position of GravityAccelerationComponants also matters ","273d4070":"## 3. Check for data imbalance","ee2f2576":"## Obtain the  test data ","af2c4b98":"In this this dataset we should not be worried about null values because there are no null values   "}}