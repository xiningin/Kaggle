{"cell_type":{"9c02fd7c":"code","267e3314":"code","ecfa8aff":"code","4ecf110a":"code","09d2a016":"code","40f20e93":"code","cfc8972b":"code","3c5351a3":"code","60b217dd":"code","91cd5fb7":"code","49864b8c":"code","fabcdc94":"code","7e27a9ae":"code","6a6e8a2e":"code","3a6f60fb":"code","b517139e":"code","bebd8068":"code","67c73406":"markdown","771f8926":"markdown","063e42c7":"markdown","2b665811":"markdown","f1a7aa26":"markdown","9a118168":"markdown","68714cd9":"markdown","e68806f6":"markdown","6517e9b1":"markdown","26b9a9a5":"markdown","7ed30fb1":"markdown"},"source":{"9c02fd7c":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf1=pd.read_csv('..\/input\/Dataset-2019.csv')","267e3314":"df1.head()","ecfa8aff":"# feature names as a list\ncol = df1.columns       # .columns gives columns names in data \nprint(col)","4ecf110a":"# y includes our labels and x includes our features\ny = df1.is_auto_renew                 # 0 or 1 \nlist = ['is_auto_renew']\nx = df1.drop(list,axis = 1 )\nx.head()","09d2a016":"ax = sns.countplot(y,label=\"Count\")       # 1 = 118894, 0 =118894\nprint(y.value_counts())","40f20e93":"x.describe()\n","cfc8972b":"drop_list1 = ['msno']\nx_1 = x.drop(drop_list1,axis = 1 )        # do not modify x, we will use it later \nx_1.head()","3c5351a3":"#correlation map\nf,ax = plt.subplots(figsize=(14, 14))\nsns.heatmap(x_1.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","60b217dd":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n\n# split data train 70 % and test 30 %\nx_train, x_test, y_train, y_test = train_test_split(x_1, y, test_size=0.3, random_state=42)\n\n#random forest classifier with n_estimators=10 (default)\nclf_rf = RandomForestClassifier(random_state=43)      \nclr_rf = clf_rf.fit(x_train,y_train)\n\nac = accuracy_score(y_test,clf_rf.predict(x_test))\nprint('Accuracy is: ',ac)\ncm = confusion_matrix(y_test,clf_rf.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")","91cd5fb7":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n# find best scored 5 features\nselect_feature = SelectKBest(chi2, k=5).fit(x_train, y_train)","49864b8c":"print('Score list:', select_feature.scores_)\nprint('Feature list:', x_train.columns)","fabcdc94":"x_train_2 = select_feature.transform(x_train)\nx_test_2 = select_feature.transform(x_test)\n#random forest classifier with n_estimators=10 (default)\nclf_rf_2 = RandomForestClassifier()      \nclr_rf_2 = clf_rf_2.fit(x_train_2,y_train)\nac_2 = accuracy_score(y_test,clf_rf_2.predict(x_test_2))\nprint('Accuracy is: ',ac_2)\ncm_2 = confusion_matrix(y_test,clf_rf_2.predict(x_test_2))\nsns.heatmap(cm_2,annot=True,fmt=\"d\")","7e27a9ae":"from sklearn.feature_selection import RFE\n# Create the RFE object and rank each pixel\nclf_rf_3 = RandomForestClassifier()      \nrfe = RFE(estimator=clf_rf_3, n_features_to_select=5, step=1)\nrfe = rfe.fit(x_train, y_train)","6a6e8a2e":"print('Chosen best 5 feature by rfe:',x_train.columns[rfe.support_])\n#not best","3a6f60fb":"from sklearn.feature_selection import RFECV\n\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nclf_rf_4 = RandomForestClassifier() \nrfecv = RFECV(estimator=clf_rf_4, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation\nrfecv = rfecv.fit(x_train, y_train)\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', x_train.columns[rfecv.support_])","b517139e":"# Plot number of features VS. cross-validation scores\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score of number of selected features\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","bebd8068":"import numpy as np\nclf_rf_5 = RandomForestClassifier()      \nclr_rf_5 = clf_rf_5.fit(x_train,y_train)\nimportances = clr_rf_5.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in clf_rf.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(x_train.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\n\nplt.figure(1, figsize=(14, 13))\nplt.title(\"Feature importances\")\nplt.bar(range(x_train.shape[1]), importances[indices],\n       color=\"g\", yerr=std[indices], align=\"center\")\nplt.xticks(range(x_train.shape[1]), x_train.columns[indices],rotation=90)\nplt.xlim([-1, x_train.shape[1]])\nplt.show()","67c73406":"Finally, we find best 6 features that are[ 'msnoid', 'payment_method_id', 'payment_plan_days', 'plan_list_price',\n       'transaction_date', 'membership_expire_date] for best classification. Lets look at best accuracy with plot","771f8926":"5) Tree based feature selection and random forest classification\u00b6\n","063e42c7":"Like previous method, we will use 5 features. However, which 5 features will we use ? We will choose them with RFE method.\n\n","2b665811":"As you can seen in plot above, after 5 best features importance of features decrease. Therefore we can focus these 5 features. As I sad before, I give importance to understand features and find best of them.","f1a7aa26":"Accuracy is almost 91% and as it can be seen in confusion matrix, we make few wrong prediction. What we did up to now is that we choose features according to correlation matrix and according to selectkBest method. Although we use 5 features in selectkBest method accuracies look similar. Now lets see other feature selection methods to find better results.","9a118168":"Accuracy is almost 96% and as it can be seen in confusion matrix, we make few wrong prediction. Now lets see other feature selection methods to find better results.","68714cd9":"1) Feature selection with correlation and random forest classification\u00b6\n","e68806f6":"3) Recursive feature elimination (RFE) with random forest\u00b6\n","6517e9b1":"**Feature Selection and Random Forest Classification\u00b6\n**","26b9a9a5":"**2) Univariate feature selection and random forest classification\u00b6\n**","7ed30fb1":"4) Recursive feature elimination with cross validation and random forest classification\u00b6\n"}}