{"cell_type":{"7ddc6d3a":"code","85822d7d":"code","03b8d7c4":"code","b188b463":"code","f53b6803":"code","98bd3b95":"code","ce223f19":"code","853ed7b9":"code","be2f71c9":"code","2d2c7cfa":"code","95734555":"code","8ed1aa7f":"code","291f811f":"code","912f1bf4":"code","574a44d7":"code","01877911":"code","e89d72d9":"code","9ffb302a":"code","2b02ec89":"code","af1b97e4":"code","26def24b":"code","c61fa774":"code","c7217f46":"code","b69cc6f6":"code","f647eb9d":"code","20090636":"code","4e2f5597":"code","8105b6a6":"code","fde6cc6b":"markdown","58e2386b":"markdown","cefbb930":"markdown","14909631":"markdown","4d27f821":"markdown","42cdc868":"markdown"},"source":{"7ddc6d3a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","85822d7d":"#loading libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nimport os\nimport glob as gb\nimport cv2\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tqdm import tqdm","03b8d7c4":"path = '..\/input\/flowers-recognition\/flowers\/flowers\/'","b188b463":"# print out classes \nfor folder in os.listdir(path):\n    print(folder)\n","f53b6803":"# this function tacke path and return list of (x , y) , x -> shape , y -> The number of times that shape appears \n# this function load all images\ndef shapeCount (Gpath):\n    size = []\n    pathname = str(Gpath+'*\/*.jpg')\n    imgsDir = gb.glob(pathname= pathname)\n    for imgDir in tqdm(imgsDir):\n        img = plt.imread(imgDir)\n        size.append(img.shape)\n    print(\"number of Imges = \",len(size))\n    return pd.Series(size).value_counts()","98bd3b95":"# print list of (x , y) , x -> shape , y -> The number of times that shape appears \nprint (shapeCount(path))","ce223f19":"# I_SIZE is ths size of the image after resize it\n# calsses is list of different type of flowers\n\nI_SIZE = 200\nclasses = ['daisy','sunflower','tulip','rose','dandelion']\n","853ed7b9":"# this function take all data then shuffle this data\n# reArrange this data to X (numpy array of images) , y (numpy array of labels)\ndef reArangeData(data):\n    # shuffle\n    import random\n    random.shuffle(data)\n    \n    # get X,Y\n    X = []\n    y = []\n    for img,lable in data:\n        X.append(img)\n        y.append(lable)\n        \n    # convert it to npArray\n    return np.array(X),np.array(y)\n\n# this function take the images path => load all this images \n# => return X (numpy array of images) , y (numpy array of labels)\ndef loadImages (path):\n    Dlist = []\n    for folder in os.listdir(path):\n        pathname = str(path +folder+'\/*.jpg')\n        files = gb.glob(pathname= pathname)\n        for file in tqdm(files):\n            image = cv2.imread(file ,cv2.IMREAD_COLOR )\n            #image = cv2.cvtColor(image , cv2.COLOR_BGR2HSV)\n            image_array = cv2.resize(image , (I_SIZE ,I_SIZE))\n            Dlist.append( [image_array , classes.index(folder)] )\n    print(len(Dlist))\n    return reArangeData(Dlist)\n\n","be2f71c9":"# load all data\nX , y = loadImages(path)","2d2c7cfa":"# normalize all data\nX=X\/255","95734555":"# figure out samples of this data\nplt.figure(figsize=(10,10))\nfor i in range(16):\n    plt.subplot(4,4,i+1)\n    plt.imshow(X[i])\n    plt.axis('off')\n    plt.title(classes[y[i]])","8ed1aa7f":"# Divide the data into two groups, the training group at 90 percent and the test group at 10 percent\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)","291f811f":"print('X_train shape is : ',X_train.shape)\nprint('y_train shape is : ',y_train.shape)\nprint(\"---------------------------------------------\")\nprint('X_test shape id : ',X_test.shape)\nprint('y_test shape id : ',y_test.shape)","912f1bf4":"# X_train = keras.utils.normalize(X_train,axis=1)\n# X_test = keras.utils.normalize(X_test,axis=1)\n#X_train = X_train\/255\n#X_test = X_test\/255","574a44d7":"from keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ndatagen.fit(X_train)","01877911":"relu = tf.nn.relu\nsoftmax = tf.nn.softmax\ninput_shape = (I_SIZE , I_SIZE,3)\n\nmodel = keras.models.Sequential([\n    keras.layers.Conv2D(128,kernel_size=(3,3) ,activation=relu , input_shape = input_shape ),\n    keras.layers.MaxPool2D(4,4),\n    keras.layers.Dropout(rate=0.3),\n    keras.layers.Conv2D(512,kernel_size=(3,3),activation=relu),\n    \n    keras.layers.MaxPool2D(4,4),\n    keras.layers.Conv2D(128,kernel_size=(3,3),activation=relu),\n    keras.layers.Dropout(rate=0.3),\n    keras.layers.Conv2D(64,kernel_size=(3,3),activation=relu),\n   \n    keras.layers.Flatten(),\n    keras.layers.Dense(512,activation=relu),\n    keras.layers.Dense(128,activation=relu),\n    keras.layers.Dropout(rate=0.5),\n    keras.layers.Dense(64,activation=relu),\n    keras.layers.Dense(5,activation=softmax)\n])","e89d72d9":"# Setup the way the model learns\n# loss function sparse_categorical_crossentropy\n# Use Adam optimization algorithm is an extension to stochastic gradient descent with learning rate 0.001\n\nloss = keras.losses.sparse_categorical_crossentropy\nadam = keras.optimizers.Adam(lr=0.001)\nmodel.compile(optimizer=adam,\n             loss = loss ,\n             metrics=['accuracy'])","9ffb302a":"# print model summary and find the toal number of parameters is 2,938,053 \n#(which mean it take time to learn the model)\n\nprint(model.summary())","2b02ec89":"# train the model and set 70 epochs to to avoid overfitting (we try different number and find it the pest)\n# and set the batch size as 32\n\nepochs = 70\nbatch_size = 32\ntrain_img_gen = datagen.flow(X_train, y_train, batch_size=batch_size)\ntest_image_gen = datagen.flow(X_test, y_test, batch_size=batch_size)\nThisModel = model.fit_generator(train_img_gen,\n                                verbose=1,\n                                steps_per_epoch=len(X_train) \/ 32,\n                                validation_data=test_image_gen,\n                                epochs=epochs\n                               )","af1b97e4":"#evaluation test result\nErr,Acc = model.evaluate(X_test,y_test)\nprint(\"Err : \" , Err)\nprint(\"Acc : \", Acc)","26def24b":"# plot graph for train accuracy evaluation with each epoch\nplt.plot(ThisModel.history['accuracy'])","c61fa774":"# plot graph for train loss value evaluation with each epoch\nplt.plot(ThisModel.history['loss'])","c7217f46":"# plot graph for validation accuracy evaluation with each epoch\nplt.plot(ThisModel.history['val_accuracy'])","b69cc6f6":"# plot graph for validation loss value evaluation with each epoch\nplt.plot(ThisModel.history['val_loss'])","f647eb9d":"# predict X_test labels\ny_predict = model.predict_classes(X_test)\nprint(y_predict[1] , y_test[1])","20090636":"# clasification report \nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_predict))","4e2f5597":"# figure out samples of this data with the true value and predicted ones\nplt.figure(figsize=(10,10))\nst = 10\nfor i in range(st,st+16):\n    plt.subplot(4,4,i+1-st)\n    plt.imshow(X_test[i])\n    plt.title(\"True : {} , Predict : {}\".format(y_test[i] , y_predict[i]))\n    plt.axis('off')\n    #plt.title(classes[y[i]])","8105b6a6":"model.save('model.h5')","fde6cc6b":"In order to avoid overfitting problem, we need to expand artificially our dataset. We can make your existing dataset even larger. The idea is to alter the training data with small transformations to reproduce the variations occuring when someone is take photo.\n\nFor example, the flower is not centered \nThe scale is not the same \nThe image is rotated...\n\nApproaches that alter the training data in ways that change the array representation while keeping the label the same are known as data augmentation techniques. Some popular augmentations people use are grayscales, horizontal flips, vertical flips, random crops, color jitters, translations, rotations, and much more. \n\nBy applying just a couple of these transformations to our training data, we can easily double or triple the number of training examples and create a robust model.\n\nThe improvement is important : \n   - Without data augmentation i obtained an accuracy of 62%\n   - With data augmentation i achieved 78% of accuracy","58e2386b":"# create the model","cefbb930":"# Data augmentation ","14909631":"# Evaluate the model","4d27f821":"### Load Data","42cdc868":"# flowers-recognition\n## Classification with Deep neural network using CNN"}}