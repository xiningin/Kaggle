{"cell_type":{"2710467b":"code","2cd9e4ad":"code","57259d18":"code","38cc4c0e":"code","2193ddd7":"code","89b98586":"code","1575a5cd":"code","828c8f1f":"code","9f497beb":"code","70193c25":"code","2e12dc49":"code","963af7fa":"code","f7daf9e9":"code","e1be9994":"code","be709e6d":"code","5ccb510e":"code","e8092916":"code","df33ec16":"code","9cd6246d":"code","2cc42a0a":"code","927cea0e":"code","599a69ff":"code","bced6d06":"code","acb11548":"code","6acf5373":"code","0b295665":"code","1fbc526a":"code","b61bd40e":"code","b850e521":"code","adae19f2":"code","21d305a4":"code","8dad021f":"code","749ffdd1":"code","507b1a30":"code","4c9fe61d":"code","58a856aa":"code","e5da7078":"code","ccb0f1c5":"code","fb2e56f7":"code","d50930cd":"code","4a95080d":"code","b97a72f0":"code","e1605c8d":"code","34a2c224":"code","73eb03e8":"code","cb5f69a1":"code","3b919bdd":"code","d3872a9e":"code","87341dd7":"code","c6aafce6":"code","cb221f50":"code","cac15eef":"code","4a10b024":"code","a95614fc":"code","b90fbe26":"code","7ffc6823":"code","76cadddc":"code","74d6d5e4":"code","b8e2722c":"code","4b613cd9":"markdown","dc30d282":"markdown","c3e12b30":"markdown","1d16885b":"markdown","0d0775bb":"markdown","bbbbb253":"markdown","fea5c751":"markdown","6c628881":"markdown","d11ca324":"markdown","9bca815b":"markdown","fee374b8":"markdown","5f1ab911":"markdown","9fcc372e":"markdown"},"source":{"2710467b":"\nimport pandas as pd\nimport numpy as np\n\n\nimport seaborn as sns\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\nimport os\nfrom tqdm.notebook import tqdm\nimport gc\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns; sns.set()\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import ParameterGrid\n\n#import shap\n#shap.initjs()","2cd9e4ad":"TARGET_COL = \"diabetes_mellitus\"\ndf = pd.read_csv(\"..\/input\/widsdatathon2021\/TrainingWiDS2021.csv\")\nprint(df.shape)\ntest = pd.read_csv(\"..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv\")\nprint(test.shape)\n","57259d18":"df['label']='train'\ntest['label']='test'\nframes = [df,test]\njoin_df = pd.concat(frames, keys=['x', 'y'])\nassert len(join_df) == len(df) + len(test)","38cc4c0e":"#join_df.describe().T","2193ddd7":"lst = join_df.isna().sum()\/len(join_df)\np = pd.DataFrame(lst)\np.reset_index(inplace=True)\np.columns = ['a','b']\nlow_count = p[p['b']>0.8]\ntodelete=low_count['a'].values","89b98586":"todelete","1575a5cd":"join_df.drop(todelete,axis=1,inplace=True)","828c8f1f":"join_df.shape","9f497beb":"join_df.head()","70193c25":"#helps with reducing memory usage\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","2e12dc49":"join_df.drop(['Unnamed: 0','encounter_id'],inplace=True,axis=1)","963af7fa":"## Print the categorical columns\nprint([c for c in df.columns if (1<df[c].nunique()) & (df[c].dtype != np.number)& (df[c].dtype != int) ])","f7daf9e9":"## Print the categorical columns\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\nnewdf = join_df.select_dtypes(include=numerics)\nnumeric_cols = newdf.columns","e1be9994":"numeric_cols ","be709e6d":"#join_df.describe()","5ccb510e":"# Need to do column by column due to memory constraints\ncategorical_cols =  ['elective_surgery','hospital_id','icu_id',\n 'ethnicity', 'gender', 'hospital_admit_source', 'icu_admit_source', 'icu_stay_type', 'icu_type','aids','cirrhosis','hepatic_failure','immunosuppression',\n 'leukemia','lymphoma','solid_tumor_with_metastasis','elective_surgery','apache_post_operative','arf_apache','fio2_apache','gcs_unable_apache','gcs_eyes_apache',\n 'gcs_motor_apache','gcs_verbal_apache','intubated_apache','ventilated_apache','solid_tumor_with_metastasis']\nfor i, v in tqdm(enumerate(categorical_cols)):\n    join_df[v] = join_df[v].fillna(join_df[v].value_counts().index[0])\n","e8092916":"for i, v in tqdm(enumerate([numeric_cols])):\n    join_df[v] =join_df.groupby(['ethnicity','gender'], sort=False)[v].apply(lambda x: x.fillna(x.mean()))","df33ec16":"join_df[categorical_cols].isna().sum()","9cd6246d":"from sklearn.preprocessing import OrdinalEncoder\n\n# In loop to minimize memory use\nfor i, v in tqdm(enumerate(categorical_cols)):\n    join_df[v] = OrdinalEncoder(dtype=\"int\").fit_transform(join_df[[v]])\n    \n\ngc.collect()","2cc42a0a":"train = join_df[join_df['label']==\"train\"]\npredict = join_df[join_df['label']=='test']","927cea0e":"train.reset_index(inplace=True)\ntrain.drop(['level_0','level_1','label'],inplace=True,axis =1 )","599a69ff":"train.shape","bced6d06":"predict.reset_index(inplace=True)\npredict.drop(['level_0','level_1','diabetes_mellitus','label'],inplace=True,axis=1)","acb11548":"predict.shape","6acf5373":"features = train.columns","0b295665":"num_feature = [col for col in features if col not in categorical_cols]","1fbc526a":"num_feature = [col for col in features if col not in categorical_cols and train[col].dtype != 'object']\ndrop_columns=[]\ncorr = train[num_feature].corr()\n# Drop highly correlated features \ncolumns = np.full((corr.shape[0],), True, dtype=bool)\n\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if corr.iloc[i,j] >=0.999 :\n            if columns[j]:\n                columns[j] = False\n                print('FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(train[num_feature].columns[i] , train[num_feature].columns[j], corr.iloc[i,j]))\n        elif corr.iloc[i,j] <= -0.995:\n            if columns[j]:\n                columns[j] = False\n","b61bd40e":"drop_columns = train[num_feature].columns[columns == False].values\nprint('drop_columns',len(drop_columns),drop_columns)","b850e521":"train.drop(drop_columns,inplace=True,axis =1 )","adae19f2":"predict.drop(drop_columns,inplace=True,axis =1 )","21d305a4":"train[TARGET_COL].value_counts()\/len(train)","8dad021f":"print(train.shape,predict.shape)","749ffdd1":"# Separate majority and minority classes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\ndf_majority = train[train['diabetes_mellitus']==0]\ndf_minority = train[train['diabetes_mellitus']==1]\n\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples=83798,    # to match majority class\n                                 random_state= 303) # reproducible results\n \n# Combine majority class with upsampled minority class\ndf_upsampled = pd.concat([df_majority, df_minority_upsampled])\n \n# Display new class counts\ndf_upsampled.diabetes_mellitus.value_counts()","507b1a30":"train = df_upsampled","4c9fe61d":"X_train, X_test, y_train, y_test = train_test_split(\n     train[[c for c in train if TARGET_COL != c]], train[TARGET_COL], test_size=0.20, random_state=42)\nprint(X_train.shape,X_test.shape)","58a856aa":"X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\nprint(X_train.shape,X_valid.shape)","e5da7078":"X_train.head()","ccb0f1c5":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report","fb2e56f7":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_jobs=-1, random_state=0)\nrf.fit(X_train, y_train)","d50930cd":"features = X_train.columns","4a95080d":"\nprint(f\"accuracy score is {accuracy_score(y_test, rf.predict(X_test))}\")\nprint(metrics.classification_report(y_test, rf.predict(X_test), labels=[0, 1]))","b97a72f0":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# sorted(zip(clf.feature_importances_, X.columns), reverse=True)\nfeature_imp = pd.DataFrame(sorted(zip(rf.feature_importances_,X_train.columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(10, 20))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('RF Features')\nplt.tight_layout()\nplt.show()\n","e1605c8d":"# pred = rf.predict_proba(predict)[:,1]\n# test[TARGET_COL] = pred\n# test[[\"encounter_id\",\"diabetes_mellitus\"]].to_csv(\".\/submission_baseline.csv\",index=False)","34a2c224":"from numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics","73eb03e8":"model = XGBClassifier(n_jobs=-1)\nmodel.fit(X_train, y_train)","cb5f69a1":"print(f\"accuracy score is {accuracy_score(y_test, model.predict(X_test))}\")\nprint(metrics.classification_report(y_test, model.predict(X_test), labels=[0, 1]))","3b919bdd":"model = LGBMClassifier()\nmodel.fit(X_train, y_train)","d3872a9e":"print(f\"accuracy score is {accuracy_score(y_valid, model.predict(X_valid))}\")\nprint(metrics.classification_report(y_valid, model.predict(X_valid), labels=[0, 1]))","87341dd7":"pred = model.predict_proba(predict)[:,1]\ntest[TARGET_COL] = pred\ntest[[\"encounter_id\",\"diabetes_mellitus\"]].to_csv(\".\/submission_baseline_2.csv\",index=False)","c6aafce6":"from lightgbm import LGBMClassifier","cb221f50":"model = LGBMClassifier(\n                              random_state=33,\n                              early_stopping_rounds = 250,\n                              n_estimators=1000,\n                              boosting_type='gbdt', num_leaves=151, max_depth=- 1, learning_rate=0.02, subsample_for_bin=200, \n                              min_split_gain=0.5, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, \n                              colsample_bytree=.75, reg_alpha=1.3, reg_lambda=0.1,  n_jobs=- 1,\n                              silent=True, importance_type='split')\n\nmodel.fit(\n    X_train,\n    y_train,\n    eval_set=[(X_valid, y_valid)],\n    eval_metric='auc',  \n    verbose=False,\n)\n#{'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 20, 'n_estimators': 1000}","cac15eef":"print(f\"accuracy score is {accuracy_score(y_test, model.predict(X_test))}\") #0.02- 0.8423\nprint(metrics.classification_report(y_test, model.predict(X_test), labels=[0, 1]))","4a10b024":"# pred = model.predict_proba(predict)[:,1]\n# test[TARGET_COL] = pred\n# test[[\"encounter_id\",\"diabetes_mellitus\"]].to_csv(\".\/submission_tuned_lgbm.csv\",index=False)","a95614fc":"model= LGBMClassifier(\n                              random_state=33,\n                              early_stopping_rounds = 250,\n                              n_estimators=10000,min_data_per_group=5, # reduce overfitting when using categorical_features\n                              boosting_type='gbdt', num_leaves=151, max_depth=- 1, learning_rate=0.02, subsample_for_bin=200000, \n                              min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, \n                              colsample_bytree=.75, reg_alpha=1.3, reg_lambda=0.1,  n_jobs=- 1,cat_smooth=1.0, \n                              silent=True, importance_type='split')\n\nmodel.fit(\n    X_train,\n    y_train,\n    eval_set=[(X_valid, y_valid)],\n    eval_metric = 'auc',\n    verbose=False\n)","b90fbe26":"print(f\"accuracy score is {accuracy_score(y_test, model.predict(X_test))}\") #0.02- 0.8423\nprint(metrics.classification_report(y_test, model.predict(X_test), labels=[0, 1]))","7ffc6823":"pred = model.predict_proba(predict)[:,1]\ntest[TARGET_COL] = pred\ntest[[\"encounter_id\",\"diabetes_mellitus\"]].to_csv(\"submission_tuned_lgbm_2.csv\",index=False)","76cadddc":"def algorithm_pipeline(X_train_data, X_test_data, y_train_data, y_test_data, X_valid,y_valid,\n                       model, param_grid, cv=10, scoring_fit='neg_mean_squared_error',\n                       do_probabilities = False):\n    gs = GridSearchCV(\n        estimator=model,\n        param_grid=param_grid, \n        cv=cv, \n        n_jobs=-1, \n        scoring=scoring_fit,\n        verbose=2\n    )\n    fitted_model = gs.fit(X_train_data, y_train_data, eval_set=[(X_valid, y_valid)], eval_metric='l1')\n    \n    if do_probabilities:\n      pred = fitted_model.predict_proba(X_test_data)\n    else:\n      pred = fitted_model.predict(X_test_data)\n    \n    return fitted_model, pred","74d6d5e4":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV","b8e2722c":"\"\"\"\nmodel = lightgbm.LGBMClassifier()\nparam_grid = {\n    'learning_rate' : [0.03,0.04,0.05,0.06],\n    'n_estimators': [1000],\n     'colsample_bytree': [0.7],\n     'max_depth': [20],\n     'num_leaves': [50, 100, 200],\n    # 'reg_alpha': [1.1, 1.2, 1.3],\n    # 'reg_lambda': [1.1, 1.2, 1.3],\n    # 'min_split_gain': [0.3, 0.4],\n    # 'subsample': [0.7, 0.8, 0.9],\n    # 'subsample_freq': [20]\n}\n\nmodel, pred = algorithm_pipeline(X_train, X_test, y_train, y_test,X_valid,y_valid, model, \n                                 param_grid, cv=5, scoring_fit='accuracy')\n\nprint(model.best_score_)\nprint(model.best_params_)\n\"\"\"","4b613cd9":"### Seperating train and predict(test)","dc30d282":"### Fill categorical columns","c3e12b30":"### Grid Search Approach\nThis will take some time and educated guess to set the parameters\n","1d16885b":"### Joining train and test to ensure encodings done correctly","0d0775bb":"### Import the Libraries","bbbbb253":"### Load the Data","fea5c751":"### Create split","6c628881":"\n### Remove correlated columns\nCredit: https:\/\/www.kaggle.com\/ankitmalik\/simple-neural-net-0-84706","d11ca324":"### Model with Hyper Parameter Tuning","9bca815b":"### We can use the following code to delete certain colums that are empty more than 80%","fee374b8":"# Prediction using lightgbm model with some hyperparameter tuning\n\nThe following code has gotten me to 0.85ish score. \n\nSome of the feature engineering I applied is:\n\n* Remove cols with higher than 80% nans\n* Fill na categorical variables witht the most common values\n* Fill na for numeric values with mean values after [groupby(ethnicity, gender)]\n* Remove highly correlated columns\n* Convert categorical variables to encodings\n\nThe model is a lightgbm model with some hyperparameter tuning, it helped with a little improvement\n\nNext thing to try: Embeddings with neural net\n\nP.S. Happy to learn what others are doing and open to joining teams\n\n","5f1ab911":"### Baseline Model","9fcc372e":"### Convert categorical\/binary variables to OrdinalEncoders"}}