{"cell_type":{"d96bdcff":"code","d169532f":"code","6ce26ce1":"code","7937a6a6":"code","ac3edf73":"code","8e171ff6":"code","702f8f39":"code","54a2c28b":"code","f2202efa":"code","504d5a20":"code","901c9554":"code","cb20125b":"code","c572b9b3":"code","c87c425a":"code","0b134f22":"code","5578c844":"code","3f1e0b00":"code","0ef473c6":"code","92fc61df":"code","fb725fcd":"markdown","79dd3040":"markdown","b799c5f1":"markdown","d7fea61f":"markdown","5b433a70":"markdown","0f7e06cd":"markdown","fb137ff2":"markdown","3039e59b":"markdown","bd51d9a3":"markdown","afe52a6b":"markdown","b83cb215":"markdown","a797d357":"markdown","6494aeeb":"markdown","9a848ce4":"markdown","69f2d3c8":"markdown","fdfa28af":"markdown","f7e36f22":"markdown"},"source":{"d96bdcff":"import numpy as np\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport glob\nimport shutil\nimport sys\nsys.path.append('..\/input\/tensorflow-great-barrier-reef')\nimport torch\nfrom PIL import Image\nimport ast","d169532f":"ROOT_DIR  = '\/kaggle\/input\/tensorflow-great-barrier-reef\/'\nweights_path = '..\/input\/yolov5-1920-4\/'   # ..\/input\/yolov5-1920-4\/best.pt\nCKPT_PATH = weights_path + '\/best.pt'\nIMG_SIZE  = 10000#int(2000*3)  # \nCONF      = 0.275     # 1920*3 + conf-0.3\nIOU       = 0.2\nAUGMENT   = True     # TTA will run for an hour, will gain improvement\nTRACKING  = False\nFDA_aug   = False","6ce26ce1":"# if TRACKING:\n# Dependencies\n%cd \/kaggle\/input\/norfair031py3\/\n!pip install commonmark-0.9.1-py2.py3-none-any.whl -f .\/ --no-index\n!pip install rich-9.13.0-py3-none-any.whl\n\n!mkdir \/kaggle\/working\/tmp\n!cp -r \/kaggle\/input\/norfair031py3\/filterpy-1.4.5\/filterpy-1.4.5\/ \/kaggle\/working\/tmp\/\n%cd \/kaggle\/working\/tmp\/filterpy-1.4.5\/\n!pip install .\n!rm -rf \/kaggle\/working\/tmp\n\n%cd \/kaggle\/input\/norfair031py3\/\n!pip install norfair-0.3.1-py3-none-any.whl -f .\/ --no-index\n%cd \/kaggle\/working\/\n\nfrom norfair import Detection, Tracker\n\ndef to_norfair(detects, frame_id):\n    result = []\n    for x_min, y_min, x_max, y_max, score in detects:\n        xc, yc = (x_min + x_max) \/ 2, (y_min + y_max) \/ 2\n        w, h = x_max - x_min, y_max - y_min\n        result.append(Detection(points=np.array([xc, yc]), scores=np.array([score]), data=np.array([w, h, frame_id])))\n\n    return result\n\n# Euclidean distance function to match detections on this frame with tracked_objects from previous frames\ndef euclidean_distance(detection, tracked_object):\n    return np.linalg.norm(detection.points - tracked_object.estimate)\n\ntracker = Tracker(\n    distance_function=euclidean_distance, \n    distance_threshold=30,\n    hit_inertia_min=3,\n    hit_inertia_max=6,\n    initialization_delay=1,\n)\nframe_id = 0\n","7937a6a6":"def get_path(row):\n    row['image_path'] = f'{ROOT_DIR}\/train_images\/video_{row.video_id}\/{row.video_frame}.jpg'\n    return row","ac3edf73":"# # Train Data\n# df = pd.read_csv(f'{ROOT_DIR}\/train.csv')\n# df = df.progress_apply(get_path, axis=1)\n# df['annotations'] = df['annotations'].progress_apply(lambda x: ast.literal_eval(x))\n# display(df.head(2))","8e171ff6":"# df['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\n# data = (df.num_bbox>0).value_counts()\/len(df)*100\n# print(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")","702f8f39":"# check https:\/\/github.com\/awsaf49\/bbox for source code of following utility functions\n# from bbox.utils import coco2yolo, coco2voc, voc2yolo\n# from bbox.utils import draw_bboxes, load_image\n# from bbox.utils import clip_bbox, str2annot, annot2str\n\n# def get_bbox(annots):\n#     bboxes = [list(annot.values()) for annot in annots]\n#     return bboxes\n\n# def get_imgsize(row):\n#     row['width'], row['height'] = imagesize.get(row['image_path'])\n#     return row\n\n# np.random.seed(32)\n# colors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n#           for idx in range(1)]","54a2c28b":"def voc2yolo(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    voc  => [x1, y1, x2, y1]\n    yolo => [xmid, ymid, w, h] (normalized)\n    \"\"\"\n    \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]\/ image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]\/ image_height\n    \n    w = bboxes[..., 2] - bboxes[..., 0]\n    h = bboxes[..., 3] - bboxes[..., 1]\n    \n    bboxes[..., 0] = bboxes[..., 0] + w\/2\n    bboxes[..., 1] = bboxes[..., 1] + h\/2\n    bboxes[..., 2] = w\n    bboxes[..., 3] = h\n    \n    return bboxes\n\ndef yolo2voc(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    voc  => [x1, y1, x2, y1]\n    \n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n    \n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]\/2\n    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n    \n    return bboxes\n\ndef coco2yolo(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    coco => [xmin, ymin, w, h]\n    yolo => [xmid, ymid, w, h] (normalized)\n    \"\"\"\n    \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # normolizinig\n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]\/ image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]\/ image_height\n    \n    # converstion (xmin, ymin) => (xmid, ymid)\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\/2\n    \n    return bboxes\n\ndef yolo2coco(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    coco => [xmin, ymin, w, h]\n    \n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # denormalizing\n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]* image_height\n    \n    # converstion (xmid, ymid) => (xmin, ymin) \n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]\/2\n    \n    return bboxes\n\ndef voc2coco(bboxes, image_height=720, image_width=1280):\n    bboxes  = voc2yolo(bboxes, image_height, image_width)\n    bboxes  = yolo2coco(bboxes, image_height, image_width)\n    return bboxes\n\n\ndef load_image(image_path):\n    return cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n\n\ndef plot_one_box(x, img, color=None, label=None, line_thickness=None):\n    # Plots one bounding box on image img\n    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) \/ 2) + 1  # line\/font thickness\n    color = color or [random.randint(0, 255) for _ in range(3)]\n    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n    if label:\n        tf = max(tl - 1, 1)  # font thickness\n        t_size = cv2.getTextSize(label, 0, fontScale=tl \/ 3, thickness=tf)[0]\n        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl \/ 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n\ndef draw_bboxes(img, bboxes, classes, class_ids, colors = None, show_classes = None, bbox_format = 'yolo', class_name = False, line_thickness = 2):  \n     \n    image = img.copy()\n    show_classes = classes if show_classes is None else show_classes\n    colors = (0, 255 ,0) if colors is None else colors\n    \n    if bbox_format == 'yolo':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:\n            \n                x1 = round(float(bbox[0])*image.shape[1])\n                y1 = round(float(bbox[1])*image.shape[0])\n                w  = round(float(bbox[2])*image.shape[1]\/2) #w\/2 \n                h  = round(float(bbox[3])*image.shape[0]\/2)\n\n                voc_bbox = (x1-w, y1-h, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(get_label(cls)),\n                             line_thickness = line_thickness)\n            \n    elif bbox_format == 'coco':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:            \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                w  = int(round(bbox[2]))\n                h  = int(round(bbox[3]))\n\n                voc_bbox = (x1, y1, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n\n    elif bbox_format == 'voc_pascal':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes: \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                x2 = int(round(bbox[2]))\n                y2 = int(round(bbox[3]))\n                voc_bbox = (x1, y1, x2, y2)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n    else:\n        raise ValueError('wrong bbox format')\n\n    return image\n\ndef get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\ndef get_imgsize(row):\n    row['width'], row['height'] = imagesize.get(row['image_path'])\n    return row\n\nnp.random.seed(32)\ncolors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n          for idx in range(1)]","f2202efa":"!mkdir -p \/root\/.config\/Ultralytics\n!cp \/kaggle\/input\/yolov5-font\/Arial.ttf \/root\/.config\/Ultralytics\/","504d5a20":"def load_model(ckpt_path, conf=0.25, iou=0.50):\n    model = torch.hub.load('..\/input\/d\/awsaf49\/yolov5-lib-ds',\n                           'custom',\n                           path=ckpt_path,\n                           source='local',\n                           force_reload=True)  # local repo\n    model.conf = conf  # NMS confidence threshold\n    model.iou  = iou   # NMS IoU threshold\n    model.classes = None   # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs\n    model.multi_label = False  # NMS multiple labels per box\n    model.max_det = 1000  # maximum number of detections per image\n    return model","901c9554":"def predict(model, img, size=768, augment=False):\n    height, width = img.shape[:2]\n    results = model(img, size=size, augment=augment)  # custom inference size\n    preds   = results.pandas().xyxy[0]\n    bboxes  = preds[['xmin','ymin','xmax','ymax']].values\n    if len(bboxes):\n        bboxes  = voc2coco(bboxes,height,width).astype(int)\n        confs   = preds.confidence.values\n        return bboxes, confs\n    else:\n        return [],[]\n    \ndef format_prediction(bboxes, confs):\n    annot = ''\n    if len(bboxes)>0:\n        for idx in range(len(bboxes)):\n            xmin, ymin, w, h = bboxes[idx]\n            conf             = confs[idx]\n            annot += f'{conf} {xmin} {ymin} {w} {h}'\n            annot +=' '\n        annot = annot.strip(' ')\n    return annot\n\ndef show_img(img, bboxes, bbox_format='yolo'):\n    names  = ['starfish']*len(bboxes)\n    labels = [0]*len(bboxes)\n    img    = draw_bboxes(img = img,\n                           bboxes = bboxes, \n                           classes = names,\n                           class_ids = labels,\n                           class_name = True, \n                           colors = colors, \n                           bbox_format = bbox_format,\n                           line_thickness = 2)\n    return Image.fromarray(img).resize((800, 400))","cb20125b":"def tracking_function(tracker, frame_id, bboxes, scores):\n    \n    detects = []\n    predictions = []\n    \n    if len(scores)>0:\n        for i in range(len(bboxes)):\n            box = bboxes[i]\n            score = scores[i]\n            x_min = int(box[0])\n            y_min = int(box[1])\n            bbox_width = int(box[2])\n            bbox_height = int(box[3])\n            detects.append([x_min, y_min, x_min+bbox_width, y_min+bbox_height, score])\n            predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n#             print(predictions[:-1])\n    # Update tracks using detects from current frame\n    tracked_objects = tracker.update(detections=to_norfair(detects, frame_id))\n    for tobj in tracked_objects:\n        bbox_width, bbox_height, last_detected_frame_id = tobj.last_detection.data\n        if last_detected_frame_id == frame_id:  # Skip objects that were detected on current frame\n            continue\n        # Add objects that have no detections on current frame to predictions\n        xc, yc = tobj.estimate[0]\n        x_min, y_min = int(round(xc - bbox_width \/ 2)), int(round(yc - bbox_height \/ 2))\n        score = tobj.last_detection.scores[0]\n\n        predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n        \n    return predictions","c572b9b3":"def extract_bbox(anno):\n    for idx, a in enumerate(anno):\n        try:\n            anno[idx] = [a['x'], a['y'], a['width'], a['height']]\n        except:\n            break\n    return anno","c87c425a":"!cp ..\/input\/yolov5-font\/Arial.ttf \/root\/.config\/Ultralytics\/","0b134f22":"# print(\"Use TRACKING...\")    \n# tracker = Tracker(\n# distance_function=euclidean_distance, \n# distance_threshold=30,\n# hit_inertia_min=3,\n# hit_inertia_max=6,\n# initialization_delay=1,\n# )\n\n# model = load_model(CKPT_PATH, conf=CONF, iou=IOU)\n# image_paths = df[df.num_bbox>1].sample(100)\n# frame_id = 0\n# for idx, path in enumerate(image_paths.image_path.tolist()):\n#     img = cv2.imread(path)[...,::-1]\n#     if FDA_aug:\n#         img = FDA_trans(image=img)['image']\n#     bboxes, confis = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n#     predict_box = tracking_function(tracker, frame_id, bboxes, confis)\n\n#     if len(predict_box)>0:\n#         box = [list(map(int,box.split(' ')[1:])) for box in predict_box]\n#     else:\n#         box = []\n#     display(show_img(img, box, bbox_format='coco'))\n#     display(show_img(img, bboxes, bbox_format='coco'))  # Predict\n#     display(show_img(img, extract_bbox(image_paths.iloc[idx].annotations), bbox_format='coco'))\n#     if idx>3:\n#         break\n#     frame_id += 1\n    \n# # print(\"Not Use TRACKING...\")    \n# # model = load_model(CKPT_PATH, conf=CONF, iou=IOU)\n\n# # # d = df[df.num_bbox>1].sample(100)\n\n# # for idx, path in enumerate(image_paths.image_path.tolist()):\n# #     img = cv2.imread(path)[...,::-1]\n# #     bboxes, confis = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n# # #     print(bboxes, extract_bbox(d.iloc[idx].annotations), confis)\n\n    \n# #     display(show_img(img, extract_bbox(image_paths.iloc[idx].annotations), bbox_format='coco'))\n\n# #     if idx>3:\n# #         break","5578c844":"import greatbarrierreef\nenv = greatbarrierreef.make_env()# initialize the environment\niter_test = env.iter_test()      # an iterator which loops over the test set and sample submission","3f1e0b00":"!ls \/kaggle\/working","0ef473c6":"if TRACKING:\n    tracker = Tracker(\n    distance_function=euclidean_distance, \n    distance_threshold=30,\n    hit_inertia_min=3,\n    hit_inertia_max=6,\n    initialization_delay=1,\n)\n\n    model = load_model(CKPT_PATH, conf=CONF, iou=IOU)\n\n    frame_id =0\n    for idx, (img, pred_df) in enumerate(tqdm(iter_test)):\n        if FDA_aug:\n            img = FDA_trans(image=img)['image']\n        bboxes, confs  = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n\n        predictions = tracking_function(tracker, frame_id, bboxes, confs)\n\n        prediction_str = ' '.join(predictions)\n        pred_df['annotations'] = prediction_str\n        env.predict(pred_df)\n        if frame_id < 3:\n            if len(predictions)>0:\n                box = [list(map(int,box.split(' ')[1:])) for box in predictions]\n            else:\n                box = []\n            display(show_img(img, box, bbox_format='coco'))\n    #     print('Prediction:', pred_df)\n        frame_id += 1\n\nelse:\n    model = load_model(CKPT_PATH, conf=CONF, iou=IOU)\n    for idx, (img, pred_df) in enumerate(tqdm(iter_test)):\n        bboxes, confs  = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n        annot          = format_prediction(bboxes, confs)\n        pred_df['annotations'] = annot\n        env.predict(pred_df)\n        if idx<3:\n            display(show_img(img, bboxes, bbox_format='coco'))\n    ","92fc61df":"sub_df = pd.read_csv('submission.csv')\nsub_df.head()","fb725fcd":"# [Tensorflow - Help Protect the Great Barrier Reef](https:\/\/www.kaggle.com\/c\/tensorflow-great-barrier-reef)\n> Detect crown-of-thorns starfish in underwater image data\n\n<img src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/31703\/logos\/header.png?t=2021-10-29-00-30-04\">","79dd3040":"# \ud83d\udd28 Helper","b799c5f1":"## Please Upvote if you find this Helpful","d7fea61f":"## Please Upvote if you find this Helpful","5b433a70":"# \ud83d\udcd6 Meta Data\n* `train_images\/` - Folder containing training set photos of the form `video_{video_id}\/{video_frame}.jpg`.\n\n* `[train\/test].csv` - Metadata for the images. As with other test files, most of the test metadata data is only available to your notebook upon submission. Just the first few rows available for download.\n\n* `video_id` - ID number of the video the image was part of. The video ids are not meaningfully ordered.\n* `video_frame` - The frame number of the image within the video. Expect to see occasional gaps in the frame number from when the diver surfaced.\n* `sequence` - ID of a gap-free subset of a given video. The sequence ids are not meaningfully ordered.\n* `sequence_frame` - The frame number within a given sequence.\n* `image_id` - ID code for the image, in the format `{video_id}-{video_frame}`\n* `annotations` - The bounding boxes of any starfish detections in a string format that can be evaluated directly with Python. Does not use the same format as the predictions you will submit. Not available in test.csv. A bounding box is described by the pixel coordinate `(x_min, y_min)` of its lower left corner within the image together with its `width` and `height` in pixels --> (COCO format).","0f7e06cd":"# \ud83d\udcda Import Libraries","fb137ff2":"# \ud83d\udccc Key-Points\n* One have to submit prediction using the provided **python time-series API**, which makes this competition different from previous Object Detection Competitions.\n* Each prediction row needs to include all bounding boxes for the image. Submission is format seems also **COCO** which means `[x_min, y_min, width, height]`\n* Copmetition metric `F2` tolerates some false positives(FP) in order to ensure very few starfish are missed. Which means tackling **false negatives(FN)** is more important than false positives(FP). \n$$F2 = 5 \\cdot \\frac{precision \\cdot recall}{4\\cdot precision + recall}$$","3039e59b":"## Init `Env`","bd51d9a3":"## Run Inference on **Train**","afe52a6b":"# \ud83d\udd2d Inference","b83cb215":"## Run Inference on **Test**","a797d357":"## Helper","6494aeeb":"## Number of BBoxes","9a848ce4":"# Tracker","69f2d3c8":"# \ud83d\udc40 Check Submission","fdfa28af":"<img src=\"https:\/\/www.pngall.com\/wp-content\/uploads\/2018\/04\/Under-Construction-PNG-File.png\">","f7e36f22":"<div class=\"alert alert-success\" role=\"alert\">\nThis work consists of two parts:     \n    <ul>\n        <li> PART 1 - UTILIZE YOLOv5 MODEL!!<\/li>\n        <li> PART 2 - LARGER RESOLUTION INFERENCE!!<\/a><\/li>\n    <\/ul>\n    \n<\/div>\n\n<div class=\"alert alert-warning\">\n<strong>Feel free to use it and enjoy!\n    I really appreciate if you upvote this notebook. Thank you! <\/strong>\n<\/div>\n"}}