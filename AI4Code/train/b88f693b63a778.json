{"cell_type":{"f1afa78b":"code","abb89cf4":"code","126eefd8":"code","7ed87145":"code","3777fe3b":"code","1f8abb34":"code","eb6f41c1":"code","7bf5aa85":"code","b4857f08":"code","a5c19282":"code","51027462":"code","bd394fb6":"code","df68dd94":"code","8fc9ff3e":"code","91e57343":"code","dccf7e52":"code","fbed9468":"code","88faca8d":"code","d2dbad0f":"code","3120516c":"code","1eb9aa31":"code","bf2482c9":"code","6e014b1f":"code","06ad349e":"code","1c1795f0":"code","722c7111":"code","7f17a461":"code","baff3d48":"code","00b47089":"code","95f2182e":"code","6b0ca5ba":"code","50c061bb":"code","2ed2c9ea":"code","8474f7ae":"code","6852d0d0":"code","8bfcb984":"code","81750468":"code","1775995d":"code","c101d7ed":"code","f0b1b0b4":"code","08f5237f":"code","6559faa8":"code","531fab75":"code","fa76cde1":"code","2ba9692a":"code","6809946f":"code","2411b996":"code","2c2d501b":"code","91460d99":"code","d9bb199a":"code","7a6535e7":"code","589adacb":"code","74a72d6f":"code","ba28af9e":"code","d387bebe":"code","c08ccdf4":"code","ddad9f6c":"code","87880f78":"code","ed86d5bf":"code","7ee4a6a7":"code","37d1a2b9":"code","4b3582a5":"code","c26cf943":"code","87e733f1":"code","d1aafe90":"code","42d6cc63":"code","c4965415":"code","7c3be300":"code","be057e40":"code","c77c0a38":"code","610e9e39":"code","e3b9075e":"code","74ca497b":"markdown","623b3a43":"markdown","40613805":"markdown","dd610d39":"markdown","c0641a6e":"markdown","64998572":"markdown","21be0fc8":"markdown","19e160f2":"markdown","60e0f79c":"markdown","be9d8b0c":"markdown","9f15eea1":"markdown","16ab9796":"markdown","2a46d2d6":"markdown","0fcebfd3":"markdown","4a49ce18":"markdown","71c9ec45":"markdown","d5dad654":"markdown","a5b3c641":"markdown","7ebb3c94":"markdown","082159ef":"markdown","331b2ef8":"markdown","23cd30e1":"markdown","24edd35b":"markdown","53b902db":"markdown","7eb2f6f5":"markdown","25420f4a":"markdown","b219c016":"markdown","4ed16109":"markdown","4c9d607a":"markdown","2ba50dd1":"markdown","b7732643":"markdown","6e5f8630":"markdown","65e5ca73":"markdown","af17904a":"markdown","8f77ef75":"markdown"},"source":{"f1afa78b":"import warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\nimport os\nimport re\nimport json\nimport time\nimport glob\nfrom collections import defaultdict\nfrom textblob import TextBlob\nfrom functools import partial\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nimport nltk\nimport urllib.request\nfrom PIL import Image\nimport spacy\nnlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\nnlp.max_length = 4000000\nfrom nltk.probability import FreqDist\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom tqdm.autonotebook import tqdm\nimport string\n\n%matplotlib inline\n\nos.listdir('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/')","abb89cf4":"train_df = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\nsample_sub = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')\ntrain_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\ntest_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/test'","126eefd8":"train_df.head()","7ed87145":"def read_append_return(filename, train_files_path=train_files_path, output='text'):\n    \"\"\"\n    Function to read json file and then return the text data from them and append to the dataframe\n    \"\"\"\n    json_path = os.path.join(train_files_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","3777fe3b":"%%time\ntqdm.pandas()   \ntrain_df['text'] = train_df['Id'].progress_apply(read_append_return)","1f8abb34":"train_df.head()","eb6f41c1":"%%time\ntqdm.pandas()\nsample_sub['text'] = sample_sub['Id'].progress_apply(partial(read_append_return, train_files_path=test_files_path))","7bf5aa85":"sample_sub.head()","b4857f08":"sample_sub['text']","a5c19282":"def text_cleaning(text):\n    '''\n    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n    text - Sentence that needs to be cleaned\n    '''\n    text = ''.join([k for k in text if k not in string.punctuation])\n    text = re.sub('r[^\\w\\s]', ' ', str(text).lower()).strip()\n    lem = nltk.stem.wordnet.WordNetLemmatizer()\n    text = lem.lemmatize(text)\n#     text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n#     text = re.sub(\"\/'+\/g\", ' ', text)\n    \n    return text","51027462":"%%time\ntqdm.pandas()\ntrain_df['text'] = train_df['text'].progress_apply(text_cleaning)","bd394fb6":"train_df.head()","df68dd94":"text = train_df['text']\nprint(text)","8fc9ff3e":"words =list(train_df['cleaned_label'].values)\nstopwords=['ourselves', 'hers','the','of','and','in', 'between', 'yourself', 'but', 'again','of', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\nsplit_words=[]\nfor word in words:\n    lo_w=[]\n    list_of_words=str(word).split()\n    for w in list_of_words:\n        if w not in stopwords:\n            lo_w.append(w)\n    split_words.append(lo_w)\nallwords = []\nfor wordlist in split_words:\n    allwords += wordlist","91e57343":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()","dccf7e52":"temp_1 = [x.lower() for x in train_df['dataset_label'].unique()]\ntemp_2 = [x.lower() for x in train_df['dataset_title'].unique()]\ntemp_3 = [x.lower() for x in train_df['cleaned_label'].unique()]\n\nexisting_labels = set(temp_1 + temp_2 + temp_3)\nid_list = []\nlables_list = []\nfor index, row in tqdm(sample_sub.iterrows()):\n    sample_text = row['text']\n    row_id = row['Id']\n    temp_df = train_df[train_df['text'] == text_cleaning(sample_text)]\n    cleaned_labels = temp_df['cleaned_label'].to_list()\n    for known_label in existing_labels:\n        if known_label in sample_text.lower():\n            cleaned_labels.append(clean_text(known_label))\n    cleaned_labels = [clean_text(x) for x in cleaned_labels]\n    cleaned_labels = set(cleaned_labels)\n    lables_list.append('|'.join(cleaned_labels))\n    id_list.append(row_id)","fbed9468":"submission1 = pd.DataFrame()\nsubmission1['Id'] = id_list\nsubmission1['PredictionString'] = lables_list","88faca8d":"submission1.head()","d2dbad0f":"from spacy import displacy\nfrom collections import Counter\nimport en_core_web_lg\nnlp = en_core_web_lg.load()","3120516c":"sample_sub2 = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')","1eb9aa31":"%%time\ntqdm.pandas()\nsample_sub2['text'] = sample_sub2['Id'].progress_apply(partial(read_append_return, train_files_path=test_files_path))","bf2482c9":"sample_sub2['text'] = sample_sub2['text'].str.replace('[^\\w\\s]','')\nsample_sub2.head()","6e014b1f":"#del train_df\n# del sample_sub","06ad349e":"sample_sub2.head()","1c1795f0":"import gc\ngc.collect()","722c7111":"datalist = pd.read_csv('..\/input\/para-data\/datalist.csv')","7f17a461":"datalist.head()","baff3d48":"datasetLabels= datalist['Data']\nprint(datasetLabels)","00b47089":"a = [(x,i) for x, y in zip(sample_sub2['Id'],sample_sub2['text']) for i in datasetLabels if i in y]\nprint (a)","95f2182e":"sample_sub2 = pd.DataFrame(a, columns=['Id', 'PredictionString'])\nsample_sub2 = sample_sub2.groupby('Id').agg({'PredictionString': '|'.join}).reset_index()\nsample_sub2.head()","6b0ca5ba":"sample_sub2['PredictionString'] = [x.lower() for x in sample_sub2['PredictionString'].unique()]","50c061bb":"#my_list = [\"cohort study [lbc1936; n\",\"lothian birth cohort study [lbc1936\",\"Information Resource Incorporated (IRI\",\"on@3\", \"two#\", \"thre%e\"]\nremovetable = str.maketrans('', '', ',@#%([')\nsample_sub2['PredictionString'] = [s.translate(removetable) for s in sample_sub2['PredictionString']]\nprint(sample_sub2['PredictionString'])","2ed2c9ea":"# stop_words = [\"the \"\n#              ]\n\n# pat = '|'.join(r\"\\b{}\\b\".format(x) for x in stop_words)\n# sample_sub2['PredictionString'] = sample_sub2['PredictionString'].str.replace(pat, '')\n\n\n# #top_words = set(stop_words)\n# #f = lambda x: ' '.join(w for w in x.split() if not w in stop_words)\n# #newdata['Verbatim2'] = newdata['Verbatim'].apply(f)\n\n# print (sample_sub2['PredictionString'])","8474f7ae":"submission2 = pd.DataFrame()\nsubmission2['Id'] = sample_sub2['Id']\nsubmission2['PredictionString'] = sample_sub2['PredictionString']\nsubmission2.head()","6852d0d0":"def read_json_pub(filename, train_files_path=train_files_path, output='text'):\n    json_path = os.path.join(train_files_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","8bfcb984":"def text_cleaning2(text):\n    '''\n    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n    text - Sentence that needs to be cleaned\n    '''\n    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n    text = re.sub(' +', ' ', text)\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    return text","81750468":"def clean_text2(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()","1775995d":"# random sample for testing\ntrain_sample=train_df.sample(n = 10)\n\ntrain_sample","c101d7ed":"govt_df = pd.read_csv('..\/input\/govtdataset\/data_set_800.csv')","f0b1b0b4":"govt_df.head()","08f5237f":"start_time = time.time()\n\n\n#### remove >.5 jaccard matches from predicitons\ndef jaccard_similarity(s1, s2):\n    l1 = s1.split(\" \")\n    l2 = s2.split(\" \")    \n    intersection = len(list(set(l1).intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    return float(intersection) \/ union\n\n#############################\n#path=train_data_path\npath=test_files_path\n\n#for training use train_sample\n\n#for submission use sample_sub\n\n#############\n\ncolumn_names = [\"Id\", \"PredictionString\"]\n\nsubmission3 = pd.DataFrame(columns = column_names)\n\nto_append=[]\nfor index, row in sample_sub.iterrows():\n    to_append=[row['Id'],'']\n    large_string = str(read_append_return(row['Id'],path))\n    clean_string=text_cleaning2(large_string)\n    for index, row2 in govt_df.iterrows():\n        query_string = str(row2['title'])\n        if query_string in clean_string:\n            if to_append[1]!='' and clean_text2(query_string) not in to_append[1]:\n                to_append[1]=to_append[1]+'|'+clean_text(query_string)\n            if to_append[1]=='':\n                to_append[1]=clean_text2(query_string)\n     \n    ###### remove similar jaccard\n    #got_label=to_append[1].split('|')\n    #filtered=[]\n    #filtered_labels = ''\n    #for label in sorted(got_label, key=len):\n        #label = clean_text(label)\n        #if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 1.0 for got_label in filtered):\n            #filtered.append(label)\n            #if filtered_labels!='':\n                #filtered_labels=filtered_labels+'|'+label\n            #if filtered_labels=='':\n                #filtered_labels=label\n    #to_append[1] = filtered_labels         \n    #print ('################')\n    #print (to_append)\n    #print (large_string)\n    #print ('################')\n    ###### remove similar jaccard\n    govt_df_length = len(submission3)\n    submission3.loc[govt_df_length] = to_append\n# submission3.to_csv('submission3.csv', index = False)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\nsubmission3","6559faa8":"# submission3 = pd.DataFrame()\n# submission3['Id'] = govt_data['Id']\n# submission3['PredictionString'] = govt_data['PredictionString']\n# submission3.head()","531fab75":"##submission = pd.DataFrame(data= {'Id' : sample_sub['Id'], 'PredictionString' : ['','','','']})","fa76cde1":"##submission.head()","2ba9692a":"merged = pd.concat([submission1, submission2,submission3], sort=True).drop_duplicates()\nsubmission = merged.dropna().groupby(['Id'], as_index=False).agg({'PredictionString' : '|'.join}).reset_index(drop=True)\nprint(submission)","6809946f":"(pd.concat(submission.melt(id_vars='Id').dropna() for df in [submission1,submission2])\n    .groupby(['Id','variable'])['value'].apply(lambda x: '|'.join(x.unique()))\n    .unstack()\n)","2411b996":"def remove_dup(strng):\n    '''\n     Input a string and split them \n    '''\n    return '|'.join(list(dict.fromkeys(strng.split('|'))))","2c2d501b":"# my_dict = {'Tags':[\"Museum, Art Museum, Shopping, Museum\",'Drink, Drink','Shop','Visit'],'Country':['USA','USA','USA', 'USA']}\n# df = pd.DataFrame(my_dict)\nsubmission['PredictionString'] = submission['PredictionString'].apply(lambda x: remove_dup(x))\nsubmission","91460d99":"submission['PredictionString'] = submission['PredictionString'].apply(lambda x: '|'.join(sorted(x.split('|'))))\nprint(submission)","d9bb199a":"submission['PredictionString'][1]","7a6535e7":"submission.head()","589adacb":"submission.to_csv('submission.csv', index=False)","74a72d6f":"#test = pd.concat([train_df,train_df],axis=0,ignore_index=True)","ba28af9e":"submission1['PredictionString'][0]","d387bebe":"submission2['PredictionString'][0]","c08ccdf4":"submission3['PredictionString'][0]","ddad9f6c":"submission['PredictionString'][0]","87880f78":"from sklearn.cluster import KMeans \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics import pairwise_distances\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\n\nfrom scipy.stats import multivariate_normal as mvn\nimport nltk\nimport os\nimport random\n\nplt.style.use('fivethirtyeight')\n","ed86d5bf":"from sklearn.feature_extraction.text import TfidfVectorizer\ndata = submission['PredictionString']\n\n\ntf_idf_vectorizor = TfidfVectorizer(stop_words = 'english',#tokenizer = tokenize_and_stem,\n                             max_features = 20000)\ntf_idf = tf_idf_vectorizor.fit_transform(data)\ntf_idf_norm = normalize(tf_idf)\ntf_idf_array = tf_idf_norm.toarray()","7ee4a6a7":"class Kmeans:\n    \"\"\" K Means Clustering\n    \n    Parameters\n    -----------\n        k: int , number of clusters\n        \n        seed: int, will be randomly set if None\n        \n        max_iter: int, number of iterations to run algorithm, default: 200\n        \n    Attributes\n    -----------\n       centroids: array, k, number_features\n       \n       cluster_labels: label for each data point\n       \n    \"\"\"\n    \n    def __init__(self, k, seed = None, max_iter = 200):\n        self.k = k\n        self.seed = seed\n        if self.seed is not None:\n            np.random.seed(self.seed)\n        self.max_iter = max_iter\n        \n            \n    \n    def initialise_centroids(self, data):\n        \"\"\"Randomly Initialise Centroids\n        \n        Parameters\n        ----------\n        data: array or matrix, number_rows, number_features\n        \n        Returns\n        --------\n        centroids: array of k centroids chosen as random data points \n        \"\"\"\n        \n        initial_centroids = np.random.permutation(data.shape[0])[:self.k]\n        self.centroids = data[initial_centroids]\n\n        return self.centroids\n    \n    \n    def assign_clusters(self, data):\n        \"\"\"Compute distance of data from clusters and assign data point\n           to closest cluster.\n        \n        Parameters\n        ----------\n        data: array or matrix, number_rows, number_features\n        \n        Returns\n        --------\n        cluster_labels: index which minmises the distance of data to each\n        cluster\n            \n        \"\"\"\n        \n        if data.ndim == 1:\n            data = data.reshape(-1, 1)\n        \n        dist_to_centroid =  pairwise_distances(data, self.centroids, metric = 'euclidean')\n        self.cluster_labels = np.argmin(dist_to_centroid, axis = 1)\n        \n        return  self.cluster_labels\n    \n    \n    def update_centroids(self, data):\n        \"\"\"Computes average of all data points in cluster and\n           assigns new centroids as average of data points\n        \n        Parameters\n        -----------\n        data: array or matrix, number_rows, number_features\n        \n        Returns\n        -----------\n        centroids: array, k, number_features\n        \"\"\"\n        \n        self.centroids = np.array([data[self.cluster_labels == i].mean(axis = 0) for i in range(self.k)])\n        \n        return self.centroids\n    \n    \n    def convergence_calculation(self):\n        \"\"\"\n        Calculates \n        \n        \"\"\"\n        pass\n    \n    def predict(self, data):\n        \"\"\"Predict which cluster data point belongs to\n        \n        Parameters\n        ----------\n        data: array or matrix, number_rows, number_features\n        \n        Returns\n        --------\n        cluster_labels: index which minmises the distance of data to each\n        cluster\n        \"\"\"\n        \n        return self.assign_clusters(data)\n    \n    def fit_kmeans(self, data):\n        \"\"\"\n        This function contains the main loop to fit the algorithm\n        Implements initialise centroids and update_centroids\n        according to max_iter\n        -----------------------\n        \n        Returns\n        -------\n        instance of kmeans class\n            \n        \"\"\"\n        self.centroids = self.initialise_centroids(data)\n        \n        # Main kmeans loop\n        for iter in range(self.max_iter):\n\n            self.cluster_labels = self.assign_clusters(data)\n            self.centroids = self.update_centroids(data)          \n            if iter % 100 == 0:\n                print(\"Running Model Iteration %d \" %iter)\n        print(\"Model finished running\")\n        return self    ","37d1a2b9":"from sklearn.datasets import make_blobs\n# create blobs\ndata = make_blobs(n_samples=200, n_features=2, centers=3, cluster_std=1.6, random_state=50)\n# create np array for data points\npoints = data[0]\n# create scatter plot\nplt.scatter(data[0][:,0], data[0][:,1], c=data[1], cmap='viridis')\nplt.xlim(-15,15)\nplt.ylim(-15,15)\n\nX = data[0]\nX[2]","4b3582a5":"temp_k  = Kmeans(4, 1, 600)\ntemp_fitted  = temp_k.fit_kmeans(X)\nnew_data = np.array([[1.066, -8.66],\n                    [1.87876, -6.516],\n                    [-1.59728965,  8.45369045],\n                    [1.87876, -6.516]])\ntemp_fitted.predict(new_data)","c26cf943":"sklearn_pca = PCA(n_components = 2)\nY_sklearn = sklearn_pca.fit_transform(tf_idf_array)\ntest_e = Kmeans(3, 1, 600)\n%time fitted = test_e.fit_kmeans(Y_sklearn)\npredicted_values = test_e.predict(Y_sklearn)\n\nplt.scatter(Y_sklearn[:, 0], Y_sklearn[:, 1], c=predicted_values, s=50, cmap='viridis')\n\ncenters = fitted.centroids\nplt.scatter(centers[:, 0], centers[:, 1],c='black', s=300, alpha=0.6);","87e733f1":"from sklearn.cluster import KMeans\nn_clusters = 3\nsklearn_pca = PCA(n_components = 2)\nY_sklearn = sklearn_pca.fit_transform(tf_idf_array)\nkmeans = KMeans(n_clusters= n_clusters, max_iter=600, algorithm = 'auto')\n%time fitted = kmeans.fit(Y_sklearn)\nprediction = kmeans.predict(Y_sklearn)\n\nplt.scatter(Y_sklearn[:, 0], Y_sklearn[:, 1],c=prediction ,s=50, cmap='viridis')\n\ncenters2 = fitted.cluster_centers_\nplt.scatter(centers2[:, 0], centers2[:, 1],c='black', s=300, alpha=0.6);","d1aafe90":"number_clusters = range(1, 5)\n\nkmeans = [KMeans(n_clusters=i, max_iter = 600) for i in number_clusters]\nkmeans\n\nscore = [kmeans[i].fit(Y_sklearn).score(Y_sklearn) for i in range(len(kmeans))]\nscore = [i*-1 for i in score]\n\nplt.plot(number_clusters, score)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Score')\nplt.title('Elbow Method')\nplt.show()","42d6cc63":"def get_top_features_cluster(tf_idf_array, prediction, n_feats):\n    labels = np.unique(prediction)\n    dfs = []\n    for label in labels:\n        id_temp = np.where(prediction==label) # indices for each cluster\n        x_means = np.mean(tf_idf_array[id_temp], axis = 0) # returns average score across cluster\n        sorted_means = np.argsort(x_means)[::-1][:n_feats] # indices with top 20 scores\n        features = tf_idf_vectorizor.get_feature_names()\n        best_features = [(features[i], x_means[i]) for i in sorted_means]\n        df = pd.DataFrame(best_features, columns = ['features', 'score'])\n        dfs.append(df)\n    return dfs\ndfs = get_top_features_cluster(tf_idf_array, prediction, 20)","c4965415":"import seaborn as sns\nplt.figure(figsize=(8,6))\nsns.set(font_scale = 2)\nsns.barplot(x = 'score' , y = 'features', orient = 'h' , data = dfs[0][:15])","7c3be300":"plt.figure(figsize=(8,6))\nsns.set(font_scale = 2)\nsns.barplot(x = 'score' , y = 'features', orient = 'h' , data = dfs[1][:15])","be057e40":"plt.figure(figsize=(8,6))\nsns.set(font_scale = 2)\nsns.barplot(x = 'score' , y = 'features', orient = 'h' , data = dfs[2][:15])","c77c0a38":"for i, df in enumerate(dfs):\n    df.to_csv('df_'+str(i)+'.csv')","610e9e39":"def plot_features(dfs):\n    fig = plt.figure(figsize=(14,12))\n    x = np.arange(len(dfs[0]))\n    for i, df in enumerate(dfs):\n        ax = fig.add_subplot(1, len(dfs), i+1)\n        ax.set_title(\"Cluster: \"+ str(i), fontsize = 14)\n        ax.spines[\"top\"].set_visible(False)\n        ax.spines[\"right\"].set_visible(False)\n        ax.set_frame_on(False)\n        ax.get_xaxis().tick_bottom()\n        ax.get_yaxis().tick_left()\n        ax.ticklabel_format(axis='x', style='sci', scilimits=(-2,2))\n        ax.barh(x, df.score, align='center', color='#40826d')\n        yticks = ax.set_yticklabels(df.features)\n    plt.show();\nplot_features(dfs)","e3b9075e":"\n\nplt.figure(figsize=(100,50))\nplt.scatter(submission['PredictionString'],submission['Id'])\nplt.xlabel('Weight')\nplt.ylabel('Height')\nplt.title('Data Distribution')\nplt.show()","74ca497b":"# Optimal Clusters","623b3a43":"<a id='3'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">3. Data Vizualization<\/p>","40613805":"<p style=\"color:blue;font-size:20px;\">\ud83d\udcddAlso, we have the text of for the sample_submission file<\/p>","dd610d39":"# govt dataset","c0641a6e":"We are provided with 4 main pieces of data:\n\n* `train.csv:` The CSV file containing all the metadata of the publications, such as their title and the dataset they utilize.\n* `train:` The directory containing the actual publications that are referenced in train.csvin JSON format.\n* `test:` The directory containing the actual publications that will be used for testing purposes (thus, with no ground truth CSV file available).\n* `sample_submission.csv:` The CSV file containing all the publications IDs in the test set, for which we'll have to populate the prediction column.","64998572":"<p style=\"color:blue;font-size:25px;\">\ud83d\udcddMerge all text from json into train.csv with colmn name text<\/p>","21be0fc8":"# Govt Datasets","19e160f2":"<p style=\"color:blue;font-size:20px;\">\ud83d\udcddGreat! we don't have any null values.<\/p>","60e0f79c":"# PredictionString after Merging dataframes","be9d8b0c":"# <p style=\"font-family:newtimeroman; text-align:center; font-size:40px;\">A Novel Data Extraction Framework Using Natural Language Processing (DEFNLP) Techniques<\/p>\n<p style=\"font-family:newtimeroman; text-align:center; font-size:30px;\">Discover how data is used for the public good<\/p>","9f15eea1":"<p style=\"color:green;font-size:20px;\">Hope you like this kernel.\n    <\/br>Please don't forget to upvote and leave your valuable comment.   \n       <\/br> Thank you!<\/p> <p style=\"font-size:100px\">&#128540;<span style='font-size:100px;'>&#128521;<\/span><span style='font-size:100px;'>&#128518;<\/span><span style='font-size:100px;'>&#128516;<\/span><span style='font-size:100px;'>&#128513;<\/span><span style='font-size:100px;'>&#128514;<\/span><\/p>","16ab9796":"*Command to do a regex substitution*","2a46d2d6":"## In this notebook basically we have to predict text for some strings by using nlp techniques\n*** \n>The objective of the competition is to identify the mention of datasets within scientific publications.\n\nThis competition challenges data scientists to show how publicly funded data are used to serve science and society. Evidence through data is critical if government is to address the many threats facing society, including; pandemics, climate change, Alzheimer\u2019s disease, child hunger, increasing food production, maintaining biodiversity, and addressing many other challenges. Yet much of the information about data necessary to inform evidence and science is locked inside publications.\n\nCan natural language processing find the hidden-in-plain-sight data citations? Can machine learning find the link between the words used in research articles and the data referenced in the article?\n\nNow is the time for data scientists to help restore trust in data and evidence. In the United States, federal agencies are now mandated to show how their data are being used. The new Foundations of Evidence-based Policymaking Act requires agencies to modernize their data management. New Presidential Executive Orders are pushing government agencies to make evidence-based decisions based on the best available data and science. And the government is working to respond in an open and transparent way.\n\nThis competition will build just such an open and transparent approach. The results will show how public data are being used in science and help the government make wiser, more transparent public investments. It will help move researchers and governments from using ad-hoc methods to automated ways of finding out what datasets are being used to solve problems, what measures are being generated, and which researchers are the experts. Previous competitions have shown that it is possible to develop algorithms to automate the search and discovery of references to data. Now, we want the Kaggle community to develop the best approaches to identify critical datasets used in scientific publications.\n\nIn this competition, you'll use natural language processing (NLP) to automate the discovery of how scientific data are referenced in publications. Utilizing the full text of scientific publications from numerous research areas gathered from CHORUS publisher members and other sources, you'll identify data sets that the publications' authors used in their work.\n\nIt achieves this goal by working with the agencies to create value for the taxpayer from the careful use of data by building new technologies to enable secure access to and sharing of confidential microdata and by training agency staff to acquire modern data skills.\n\nIf successful, you'll help support evidence in government data. Automated NLP approaches will enable government agencies and researchers to quickly find the information they need. The approach will be used to develop data usage scorecards to better enable agencies to show how their data are used and bring down a critical barrier to the access and use of public data.","0fcebfd3":"# Extracting top features","4a49ce18":"<a id='2'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">2. Data Exploration<\/p>","71c9ec45":"# Reading csv files and train & test file paths","d5dad654":"<p style=\"color:blue;font-size:20px;\">\ud83d\udcddWe have our data cleaned!<\/p>","a5b3c641":"<a id='1'><\/a>\n# <p style=\"background-color:blue; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 10px 25px;color:white;\">Table of Content<\/p>\n* [1. Importing Modules & Libraries](#1)\n* [2. Data Exploration](#2)\n* [3. Data Vizualization](#3)\n* [4. Data Cleaning](#4)\n* [5. Baseline Model & Submission](#5)","7ebb3c94":"<p style=\"color:blue;font-size:20px;\">\ud83d\udcddRemove stop words from <b>cleaned_label<\/b> of train.csv<\/p>","082159ef":"<a id='1'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">1. Importing Modules & Libraries<\/p>","331b2ef8":"# remove Duplicates from dataframe","23cd30e1":"<a id='0'><\/a>\n# <p style=\"background-color:purple; font-family:newtimeroman; font-size:100%; text-align:center; border-radius: 15px 50px; color:white;\">Please dont forget to upvote the notebook <\/p>","24edd35b":"# Fine Tunning Through Huggingface Models","53b902db":"<a id='1'><\/a>\n## <p style=\"text-align:center;\">Data Description<\/p>\ntrain.csv - labels and metadata for the training set train\/test directory - the full text of the training\/test set's publications in JSON format, broken into sections with section titles\n\n* `id` - publication id - note that there are multiple rows for some training documents, indicating multiple mentioned datasets.\n* `pub_title` - title of the publication (a small number of publications have the same title).\n* `dataset_title` - the title of the dataset that is mentioned within the publication.\n* `dataset_label` - a portion of the text that indicates the dataset.\n* `cleaned_label` - the dataset_label, as passed through the clean_text function from the Evaluation page.\n\nsample_submission.csv - a sample submission file in the correct format.\n* `Id` - publication id.\n* `PredictionString` - To be filled with equivalent of cleaned_label of train data.","7eb2f6f5":"<p style=\"color:blue;font-size:20px;\">\ud83d\udcddtqdm is used to show any code running with a progress bar<\/p>","25420f4a":"<a id='4'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">4. Data Cleaning<\/p>","b219c016":"#  Baseline Modelling ","4ed16109":"<p style=\"color:blue;font-size:20px;\">\ud83d\udcddWe have our text appended in our train dataframe<\/p>","4c9d607a":"# **Sorting in a alphabetical Order**","2ba50dd1":"<a id='5'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">5. Baseline model and Submission<\/p>","b7732643":"### Observations\n\n- 1) There are duplicate id's meaning that there are some pulications that are using mutiple datasets. That's why that id is repeating.\n- 2) Same is the case with pub_title. A single publication is using mutiple datasets.\n- 3) There is NO one to one mapping of id and pub_title. Meaning that there are cases when two different publications (from two different authors) have same title. Well, interesting!!!\n- 4) There 45 dataset titles but 130 dataet labels. Meaning that there are some datasets that has multiple labels. We'll look into how these two are related.","6e5f8630":"<p style=\"color: blue;font-size:15px;\">\ud83d\udcdd We have duplicate ids but we are not going to remove them from data because one research paper may consist of more than one datasets. Anyhow duplicate ids with respective datasets are shown below<\/p>","65e5ca73":"# PredictionString Analysis","af17904a":"# SK learn Implementation","8f77ef75":"<p style=\"color:blue;font-size:20px;\">\ud83d\udcddMerging DataTables into one Submission File<\/p>"}}