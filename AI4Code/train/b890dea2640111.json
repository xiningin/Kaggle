{"cell_type":{"5c371c41":"code","960a4654":"code","bb326a87":"code","d25e5fd6":"code","556272be":"code","b2cc4bfe":"code","7e9bdf18":"code","db6c8799":"code","d1d20197":"code","b35a245c":"code","31a7821d":"code","5eeb99d9":"code","f15ad03f":"code","72399860":"code","140420ce":"code","20e4a4f9":"code","f1088eb9":"code","031a0355":"code","ca3c3db8":"code","35f2462b":"code","27fd9fd1":"code","15e7dc6a":"code","aa634f33":"code","d0f24dbf":"code","729ef0d0":"code","86dbc794":"code","988525c1":"code","b7554344":"code","4ed75a68":"code","7faf2cea":"code","ac161b8f":"code","9547e2c1":"markdown","92ae42b9":"markdown","8a75937d":"markdown","aecd1ed7":"markdown","7df0b719":"markdown","44b9f421":"markdown"},"source":{"5c371c41":"#import libraries\nimport numpy as np \nimport seaborn as sns\nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings #filter warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\nprint(\"Setup Complete\")","960a4654":"#loading train data\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\nprint(train.shape)\ntrain.head()","bb326a87":"#loading test data\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\nprint(test.shape)\ntest.head()","d25e5fd6":"# put labels into y_train variable\ny_train = train[\"label\"]\n# Drop 'label' column\nX_train = train.drop(labels = [\"label\"],axis = 1) ","556272be":"#selecting a given row\nrow = train.iloc[3]\nrow","b2cc4bfe":"image_1d = row.iloc[1:].values #taking only pixel columns\nprint(image_1d.shape) #image","7e9bdf18":"image_2d = image_1d.reshape((28, 28)) #reshape the 1d array into it's original 2D format","db6c8799":"print(\"label:\", row.label)\n\nplt.imshow(image_2d, cmap=\"gray\") # Display an image","d1d20197":"#normalize the data to grayscale to reduce the effect of illumination's differences\nX_train = X_train \/ 255.0\ntest = test \/ 255.0\nprint(\"x_train shape: \",X_train.shape)\nprint(\"test shape: \",test.shape)","b35a245c":"#reshape data to add an extra dimension in the end(28x28x1) to 3D matrix\nX_train = X_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)\nprint(\"x_train shape: \",X_train.shape)\nprint(\"test shape: \",test.shape)","31a7821d":"from keras.utils.np_utils import to_categorical \ny_train = to_categorical(y_train, num_classes = 10)","5eeb99d9":"# Split the train and the validation set for the fitting\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.3, random_state=1)\nprint(\"x_train shape\",X_train.shape)\nprint(\"x_test shape\",X_valid.shape)\nprint(\"y_train shape\",y_train.shape)\nprint(\"y_test shape\",y_valid.shape)","f15ad03f":"# Some examples\nplt.imshow(X_train[2][:,:,0],cmap='gray')\nplt.show()","72399860":"#importing all necessary modules required to train the model\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nimport keras\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.layers.advanced_activations import LeakyReLU","140420ce":"model = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),activation='linear',padding='same',input_shape=(28,28,1)))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(MaxPooling2D((2, 2),padding='same'))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\nmodel.add(LeakyReLU(alpha=0.1))                  \nmodel.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\nmodel.add(Dropout(0.4))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='linear'))\nmodel.add(LeakyReLU(alpha=0.1))           \nmodel.add(Dropout(0.3))\nmodel.add(Dense(10, activation='softmax'))","20e4a4f9":"#compiling the model\nmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])","f1088eb9":"model.summary()","031a0355":"#train the model\nmodel_train = model.fit(X_train, y_train, batch_size=64,epochs=20,verbose=1,validation_data=(X_valid, y_valid))","ca3c3db8":"test_eval = model.evaluate(X_valid, y_valid, verbose=0)","35f2462b":"print('Test loss:', test_eval[0])\nprint('Test accuracy:', test_eval[1])","27fd9fd1":"accuracy = model_train.history['accuracy']\nval_accuracy = model_train.history['val_accuracy']\nloss = model_train.history['loss']\nval_loss = model_train.history['val_loss']\nepochs = range(len(accuracy))\nplt.plot(epochs, accuracy, 'bo', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","15e7dc6a":"#predict labels\npredicted_classes = model.predict(X_valid)","aa634f33":"# Predict the values from the validation dataset\nY_pred = model.predict(X_valid)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_valid,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Reds\",linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","d0f24dbf":"#getting the shape of our test data\ntest.shape","729ef0d0":"test.min(), test.max()","86dbc794":"y_sub = model.predict(test)\ny_sub","988525c1":"y_sub = np.argmax(np.round(y_sub), axis=1)","b7554344":"y_sub","4ed75a68":"sub = pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\")\nsub[\"Label\"] = y_sub\nsub.head()","7faf2cea":"sub","ac161b8f":"sub.to_csv(\"submission.csv\", index=False)","9547e2c1":"We will use a batch_size of 64, 10 classes and 20 epochs.","92ae42b9":"# Model submission","8a75937d":"# Convolutional Neural Network\n\nThe images are of size 28 x 28. We already converted the image matrix to an array, rescaled it between 0 and 1 and reshaped it so that it's of size 28 x 28 x 1. We will feed this as an input to the network.\n\nWe will use three convolutional layers:\n\n    The first layer will have 32-3 x 3 filters,\n    The second layer will have 64-3 x 3 filters and\n    The third layer will have 128-3 x 3 filters.\n\nIn addition, there are three max-pooling layers each of size 2 x 2.\n","aecd1ed7":"Since the predictions we get are floating point values, it will not be feasible to compare the predicted labels with test labels. We will round off the output which will convert the float values into an integer. We will also use np.argmax() to select the index number which has a higher value in a row.","7df0b719":"The graphs above show a little diffeence between Training accuracy and Validation accuracy, and Training loss and Validation loss. We can safely say that is is little to no evidence of overfitting or underfitting. We will move on to predicting the labels.","44b9f421":"# Converting to one hot encoding\n\nThe one hot encoding will be a row vector, and for each image, it will have a dimension of 1x10. The vector consists of all zeros except for the class that it represents.\n"}}