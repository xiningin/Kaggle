{"cell_type":{"4474b721":"code","3e085fd9":"code","df1774e0":"code","00079be1":"code","d13e8ecd":"code","41cbc919":"code","71070720":"code","14481f71":"code","5273c044":"code","779966a5":"code","a604a97a":"code","ecc07224":"code","cb4b1b03":"code","1f460968":"code","f6495a99":"code","f4196d1b":"code","3c48e133":"code","76e57ed2":"markdown","776b99c1":"markdown"},"source":{"4474b721":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import LancasterStemmer\nfrom nltk.stem import SnowballStemmer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport string\nimport re\nimport warnings as wr\n\n\nwr.filterwarnings(\"ignore\")\n","3e085fd9":"data=pd.read_csv(r'..\/input\/sms-spam-collection-dataset\/spam.csv',encoding=\"ISO-8859-1\")\nx=data.copy()","df1774e0":"x.head()","00079be1":"missingValues=x.isna().sum()\/x.shape[0]\nprint(f'Missing Values Ratio : \\n{missingValues}')","d13e8ecd":"sns.countplot(x[\"v1\"])","41cbc919":"mapping={'ham':0,'spam':1}\nx['v1']=x['v1'].replace(mapping)","71070720":"x=x.fillna(' ')\nx['concat']=x['v2']+x['Unnamed: 2']+x['Unnamed: 3']+x['Unnamed: 4']","14481f71":"x.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4','v2'],axis=1,inplace=True)\nx.rename(columns={'v1':'classifier','concat':'texts'},inplace=True)","5273c044":"x['texts'] = x['texts'].str.lower()\nx['#tokens'],x['#char'],x['#sentences'],x['#stopwords'],x['#tokensAfter'],x['#numbersinTxt'],x['#uniqueWords'],x['Mails&phones'],x['#len_steem']=0,0,0,0,0,0,0,0,0\nx['pureText']=x['texts']","779966a5":"def lemmaOrStem(tokens,stemm=False,stemmer='',lemma=False,partOfSpeech=''):\n\n    if (stemm):\n        if(stemmer=='Porter'):\n            porter = PorterStemmer()\n            porterStemming = [porter.stem(j)  for j in tokens]\n            return porterStemming\n\n        #---------landstremmer------#    \n        elif(stemmer=='lancaster'):    \n        \n            lancaster=LancasterStemmer()\n            lancasterStemming = [lancaster.stem(j)  for j in tokens]\n            return lancasterStemming\n            \n    #---------------SnowballStemmer--------#\n\n        elif(stemmer=='snow') :\n                sm = SnowballStemmer(\"english\")\n                snowballStemming = [sm.stem(j)  for j in tokens]\n                return snowballStemming\n   \n    if(lemma):        \n    #----------Lemmatization---------#\n        lemm = nltk.WordNetLemmatizer()\n        #ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n        lemm_1 = [lemm.lemmatize(j,pos=partOfSpeech) for j in tokens]\n        return lemm_1\n   \n","a604a97a":"def mail_phone(text):\n    mail=re.findall(r\"\\S{0,}\\@[a-z]{0,}\\.[a-z]{1,4}\",text)\n    phone=re.findall(r\"[\\+]?[0-9]{2}\\s*[0-9]{6,9}\",text)\n    return mail,phone","ecc07224":"\nstop = set(stopwords.words('english'))\n\nfor i in range(0,x.shape[0]):\n    \n    x['#char'][i]=len(x['texts'][i])\n    \n    tokens = word_tokenize(x['texts'][i])\n    x['#tokens'][i]= len(tokens)\n    \n    sentences=nltk.sent_tokenize(x['texts'][i])\n    if(len(sentences)==1 or len(sentences)==0):\n        x['#sentences'][i] =0\n    else : x['#sentences'][i] =1   \n    \n    \n    tokensOfpureTextWithoutstop=[token for token in x['texts'][i].split() if token not in stop]\n    x['pureText'][i] = \" \".join(tokensOfpureTextWithoutstop)\n    \n    textWithoutPunctuations= re.findall(r'\\w+',x['pureText'][i])\n    x['pureText'][i] = \" \".join(textWithoutPunctuations)\n    \n    counNum=re.findall(r'\\d+',x['pureText'][i])\n    x['#numbersinTxt'][i]=len(counNum)\n    \n    x['#uniqueWords'][i]=len(set(textWithoutPunctuations))\n\n    x['#tokensAfter'][i]= len(textWithoutPunctuations)\n    \n    x['#stopwords'][i]=(x['#tokens'][i])-(len(tokensOfpureTextWithoutstop))\n    \n    mail,phone=mail_phone(x['pureText'][i])\n    x['Mails&phones'][i]=len(mail)+len(phone)\n    \n    lemmaText=lemmaOrStem(x['texts'][i].split(),stemm=False,stemmer='',lemma=True,partOfSpeech='n')\n    x['#len_steem'][i]=len(lemmaText)","cb4b1b03":"x.head()","1f460968":"x.drop(['texts'],axis=1,inplace=True)","f6495a99":"corr = x.iloc[:,0:-1].corr()\nsns.heatmap(corr,annot=corr)","f4196d1b":"x.drop(['#stopwords'],axis=1,inplace=True)","3c48e133":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\n\n\nlr=LogisticRegression(max_iter=10000)\n\nlr.fit(x.iloc[:,1:-1],x.iloc[:,0])\nprint(lr.score(x.iloc[:,1:-1],x.iloc[:,0]))\n\nnb=MultinomialNB()\nnb.fit(x.iloc[:,1:-1],x.iloc[:,0])\nprint(nb.score(x.iloc[:,1:-1],x.iloc[:,0]))\n\nsvm=SVC()\nsvm.fit(x.iloc[:,1:-1],x.iloc[:,0])\nprint(svm.score(x.iloc[:,1:-1],x.iloc[:,0]))\n","76e57ed2":"Data isn't balanced","776b99c1":"**Works on Feature we extract**"}}