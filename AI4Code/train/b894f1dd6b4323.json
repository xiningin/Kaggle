{"cell_type":{"80201e0f":"code","135fa1b9":"code","34832eeb":"code","757bcb7f":"code","538d4148":"code","9a47869c":"code","9cbdadb3":"code","f57b1977":"code","fc1e1339":"code","584613fd":"code","43abd935":"code","e394938b":"code","f0bcb547":"code","04edad4e":"code","07751d1c":"code","7bd9e5e0":"code","c99691e9":"code","8d5e49e4":"code","fa4e2523":"code","0d1c63ad":"code","d939410c":"code","3e861f3a":"code","5544f5a4":"code","8ddd4316":"code","cb4a311b":"markdown","e730e313":"markdown","023f7efb":"markdown","f1f3379b":"markdown","1127055d":"markdown","fca8b617":"markdown","f121cba1":"markdown","b4cb1050":"markdown","dfcaed33":"markdown"},"source":{"80201e0f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\npd.set_option('max_colwidth', None)","135fa1b9":"path = '\/kaggle\/input\/hackathon'\nfiles = [f'{path}\/task_1-google_search_english_original_metadata.csv',\n         f'{path}\/task_1-google_search_translated_to_english_metadata.csv']","34832eeb":"dfs = []\nfor file in files:\n    df = pd.read_csv(file, encoding = \"ISO-8859-1\")\n    dfs.append(df)\ndf = pd.concat(dfs, ignore_index=True)","757bcb7f":"df.drop(['Is Processed', 'Comments', 'language', 'query'], axis=1, inplace=True)\ndf.drop(df[df['is_downloaded']==False].index, inplace=True)\ndf['char_number'] = pd.to_numeric(df['char_number'], errors='coerce')\ndf.drop(df[df['char_number']==0].index, inplace=True)\ndf.drop_duplicates('url', keep=False, inplace=True)\ndf.drop(df[df['url'].str.contains('researchgate.net')].index, inplace=True)\ndf.drop(df[df['is_pdf']].index, inplace=True)\nassert all(df[df['alpha_2_code'].isna()]['country']=='Namibia')\ndf['alpha_2_code'].fillna('NA', inplace=True)","538d4148":"from urllib.parse import urlparse\ndf['url_domain'] = df['url'].apply(lambda x: urlparse(x).netloc)\ndf_ncbi = df[df['url_domain']=='www.ncbi.nlm.nih.gov']","9a47869c":"! pip install pandarallel","9cbdadb3":"import bs4 as bs\nimport urllib.request\nfrom pandarallel import pandarallel\npandarallel.initialize(progress_bar=True)\n\ndef get_url_title(url):\n    try:\n        req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla\/5.0'})\n        source = urllib.request.urlopen(req).read()\n        soup = bs.BeautifulSoup(source,'lxml')\n        if not soup.title:\n            print('No title')\n            print(url)\n            return \"\"\n        return soup.title.text\n    except urllib.error.HTTPError as e:\n        print(e)\n        print(url)\n        return \"\"","f57b1977":"df_ncbi['url_title'] = df_ncbi['url'].parallel_apply(get_url_title)","fc1e1339":"pd.options.mode.chained_assignment = None","584613fd":"df_ncbi['title_has_country'] = df_ncbi.apply(lambda row: row['country'] in row['url_title'], axis=1)\ndf_ncbi.drop(df_ncbi[df_ncbi['title_has_country'] == False].index, inplace=True)","43abd935":"df_ncbi.drop_duplicates('url_title', inplace=True)","e394938b":"f\"Working with {df_ncbi.shape[0]} sources\"","f0bcb547":"import spacy\nnlp = spacy.load('en_core_web_sm')\n\ndef get_snippets(text):\n    '''\n        Returns sentences in the text which contain more than 5 tokens and at least one verb.\n    '''\n    return [sent.text.strip() for sent in nlp(text).sents \n                 if len(sent.text.strip()) < 350 and len(sent.text.strip().split()) > 5 and any([token.pos_ == 'VERB' for token in sent])]","04edad4e":"keywords = {\n    'first_year': [\"first\", \"bcg\", \"policy\", \"initiated\", \"bacille\", \"calmette\", \"guerin\", \n                   \"introduced\", \"distributed\", \"vaccination\", \"included\", \"immunisation\", \n                   \"started\", \"since\", \"schedule\", \"recommended\", \"vaccine\", \"programme\", \"program\",\n                  \"start\", \"universal\"],\n    'last_year': [\"abolished\", \"suspended\", \"until\", \"continued\", \"withdrawn\"],\n    'is_mandatory': [\"voluntary\", \"mandatory\", \"compulsory\", \"few\"],\n    'timing': [\"birth\", \"aged\", \"children\", \"age\", \"child\", \"given\", \"target\", \"weeks\", \"obstetric\", \"newborns\"],\n    'strain': [\"strain\", \"substrain\", \"strains\", \"mycobacterium\", \"pasteur\", \n               \"1173p2\", \"ssi\", \"danish\", \"1331\"],\n    'has_revaccinations': [\"revaccination\", \"revaccinations\"],\n    'revaccination_timing': [\"revaccination\", \"revaccinations\"],\n    'location': [\"intradermal\", \"injection\", \"arm\", \"left\", \"right\", \"deltoid\", \"muscle\"],\n    'manufacturer': [\"producer\", \"manufacturer\", \"sanofi\", \"pasteur\", \"ltd\", \n                    'unk', 'serum', 'mikrogen', 'rivm', 'merieux', 'institute', 'statens', 'laboratory', 'biomed', 'ncipd',\n       'ssi',  'biofarma', 'bulbio', 'intervax', 'intervac', 'pfizer', 'aventis'],\n    'supplier': ['ltd', 'serum', 'institute','mikrogen',\n       'rivm', 'merieux', 'sanofi', 'pasteur', 'statens','unk', 'laboratory', 'paho',\n       'medoka', 'biomed', 'wholesaler', 'unicef', 'bulbio', 'intervax', 'aventis', 'ncipd'],\n    'groups': ['high-risk', 'risk', 'travel', 'high incidence', 'high risk', 'TB incidence']\n}","07751d1c":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ndef get_score(corpus, vocab):\n    vec = TfidfVectorizer(vocabulary=vocab).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.mean(axis=1)\n    return sum_words","7bd9e5e0":"def read_text(row):\n    code = row['alpha_2_code']\n    filename=row['filename'].replace('.txt', '')\n    filename = f'\/kaggle\/input\/hackathon\/task_1-google_search_txt_files_v2\/{code}\/{filename}.txt'\n    \n    with open(filename, 'r') as file:\n        data = file.read()\n    return data","c99691e9":"import tqdm\n\ndfs = []\nfor _, row in tqdm.tqdm(df_ncbi.iterrows()):\n    data = read_text(row)\n    \n    snippets = get_snippets(data)\n    \n    result = pd.DataFrame()\n    result['sentence'] = snippets\n    result['len'] = result['sentence'].apply(len)\n    result['country'] = row['country']\n    result['url'] = row['url']\n    \n    for question, words in keywords.items():\n        result[question] = get_score(snippets, words)\n    \n    result = result.replace(0, np.nan)\n    result = result.dropna(how='all', axis=0, subset=keywords.keys())\n    \n    dfs.append(result)","8d5e49e4":"result = pd.concat(dfs, ignore_index=True)","fa4e2523":"result.drop_duplicates('sentence', inplace=True)","0d1c63ad":"f\"Working with {result.shape[0]} snippets\"","d939410c":"for k in keywords.keys():\n    print(k)\n    display(result.sort_values(k, ascending=False)[[k, 'sentence','country','len']].head(3))","3e861f3a":"path = '\/kaggle\/input\/bcg-manually-reviewed-cleaned'\nfile = f'{path}\/manually_reviewed_cleaned.csv'\ndf_man = pd.read_csv(file, encoding = \"ISO-8859-1\")","5544f5a4":"question_names = ['first_year','last_year','is_mandatory','timing','strain','has_revaccinations','revaccination_timing','location','manufacturer', 'supplier', 'groups']\ndf_man.columns = ['alpha_2_code', 'country', 'url', 'filename', 'is_pdf','Comments',\n              'Snippet'] + question_names + ['snippet_len', 'text_len']","8ddd4316":"for k in keywords.keys():\n    print(k)\n    df_ncbi_ans = result.sort_values(k, ascending=False)[[k, 'sentence','country', 'url']].head(30)\n    df_man_ans = df_man[df_man[k].notna()][[k, 'country', 'url']]\n    df_man_ans['sentence'] = df_man_ans[k]\n    df_man_ans[k] = 1\n    \n    df_ans = pd.concat([df_man_ans, df_ncbi_ans], ignore_index=True)\n    df_ans['label'] = 1\n    \n    df_ans.to_csv(f'\/kaggle\/working\/{k}_labeled.csv', index=False)","cb4a311b":"The following keywords were extracted by hand by inspecting the texts from the manually reviewed dataset.","e730e313":"# Motivation\nIn [Task 1: Supervised dataset from manually reviewed](http:\/\/https:\/\/www.kaggle.com\/didizlatkova\/task-1-supervised-dataset-from-manually-reviewed) we created a labeled dataset for the first question (Which is the First Year of the BCG Policy) using the manually reviewed dataset provided by the organizers.\n\nUnfortunately, the manually reviewed dataset does not contain enough examples for correct answers for the 11 questions we are interested in. Even worse, for the revaccination timing question there are no answers provided at all. This means that if we want to have 11 datasets to train models on, we need to create them ourselves.\nWe do that by using a very simple keyword tf-idf model that does not need any training.\n\nThe end goal is to use the extracted answers as positive examples in a labeled dataset for supervised learning.","023f7efb":"The answers are saved in 11 separate files - one per question.","f1f3379b":"## Split into snippets","1127055d":"## Key words model","fca8b617":"## Filter sources","f121cba1":"We combine the positive examples we already have from the manually reviewed dataset with the top 30 answers for each question ranked by our simple keyword model.","b4cb1050":"## Combine with manually reviewed","dfcaed33":"The final notebook which extends the BCG atlas dataset is here: [Task 1: 11 supervised models for the final dataset](https:\/\/www.kaggle.com\/didizlatkova\/task-1-11-supervised-models-for-the-final-dataset)"}}