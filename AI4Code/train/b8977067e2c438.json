{"cell_type":{"93ae91db":"code","604d9733":"code","34ee0d4b":"code","4284a544":"code","e4a36660":"code","d30fdb61":"code","53365017":"code","8aa9e89d":"code","200b66b4":"code","e17d2c91":"code","b74770ba":"code","81c57ac3":"code","4fa88976":"code","d037cefd":"code","b66fb4c0":"code","6cfd4bd0":"code","58756d30":"code","9b0d74ac":"code","b4941347":"code","532b7f0a":"code","cc0515bc":"code","dac59bf4":"code","4b3becb8":"code","14206cfa":"code","1bdc420f":"code","e6f984c3":"code","61a0479f":"code","7d67d3a8":"code","decc3e1c":"code","cc4eccee":"code","442b060e":"code","2fcb64fd":"code","c400d02f":"code","407fbb12":"code","5a298d52":"code","51c4adea":"code","3efa2b7c":"code","0f6b7123":"code","87fced01":"code","a69c73b0":"code","7fb28294":"code","1cd6d78d":"code","30a59865":"code","4130f7af":"code","e0f82360":"code","b2aa8d90":"code","37de3f89":"code","2f47bd7f":"code","00dbc4a8":"code","7ace05d3":"code","076023ec":"code","314526bf":"code","9edc825c":"code","3c61b366":"code","c54dfde5":"code","65a6253f":"code","cc6638c6":"code","4a3948d0":"code","691a3dae":"code","46f4af9f":"code","635c58a3":"code","09d25136":"code","c7b570cc":"code","8b580055":"code","3b0bca5d":"code","259faf18":"markdown","d0b80979":"markdown","b7c735c7":"markdown","42070c3c":"markdown","8bdbb92b":"markdown","0f50f84a":"markdown","be636de3":"markdown","2a94f4b3":"markdown","18306bff":"markdown","e282f11d":"markdown","d6a23acd":"markdown","7279a68d":"markdown","63332c64":"markdown","f3a810be":"markdown","05ac86e8":"markdown","ade31dd8":"markdown","d244190d":"markdown","11826321":"markdown","b02a719a":"markdown","cbcfcd41":"markdown","018cfb7e":"markdown","eff7530b":"markdown","aeff5b57":"markdown","0f8a8df3":"markdown","1e5e7adb":"markdown","3384fbbf":"markdown"},"source":{"93ae91db":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.graph_objects as go\nfrom mlxtend.plotting import plot_confusion_matrix\n\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nimport tensorflow as tf\n\nimport optuna\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\n\ndef seed_everything(seed=2020):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\nseed_everything(0)\n\nsns.set_style(\"whitegrid\")\npalette_ro = [\"#ee2f35\", \"#fa7211\", \"#fbd600\", \"#75c731\", \"#1fb86e\", \"#0488cf\", \"#7b44ab\"]\n\nROOT = \"..\/input\/heart-failure-clinical-data\"","604d9733":"df = pd.read_csv(ROOT + \"\/heart_failure_clinical_records_dataset.csv\")\n\nprint(\"Data shape: \", df.shape)\ndf.head()","34ee0d4b":"df.isnull().sum()","4284a544":"fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(16, 12))\n\nsns.countplot(x=\"anaemia\", ax=ax1, data=df,\n              palette=palette_ro[3::-3], alpha=0.9)\nsns.countplot(x=\"diabetes\", ax=ax2, data=df,\n              palette=palette_ro[3::-3], alpha=0.9)\nsns.countplot(x=\"high_blood_pressure\", ax=ax3, data=df,\n              palette=palette_ro[3::-3], alpha=0.9)\nsns.countplot(x=\"sex\", ax=ax4, data=df,\n              palette=palette_ro[2::3], alpha=0.9)\nsns.countplot(x=\"smoking\", ax=ax5, data=df,\n              palette=palette_ro[3::-3], alpha=0.9)\nsns.countplot(x=\"DEATH_EVENT\", ax=ax6, data=df,\n              palette=palette_ro[1::5], alpha=0.9)\nfig.suptitle(\"Distribution of the binary features and DEATH_EVENT\", fontsize=18);","e4a36660":"bin_features = [\"anaemia\", \"diabetes\", \"high_blood_pressure\", \"sex\", \"smoking\"]\n\ndf_d = pd.DataFrame(columns=[0, 1, \"value\"])\nfor col in bin_features:\n    for u in df[col].unique():\n        df_d.loc[col+\"_\"+str(u)] = 0\n        for i in df[\"DEATH_EVENT\"].unique():\n            if u == 0:\n                df_d[\"value\"][col+\"_\"+str(u)] = \"0 (False)\"\n            else:\n                df_d[\"value\"][col+\"_\"+str(u)] = \"1 (True)\"\n            df_d[i][col+\"_\"+str(u)] = df[df[col]==u][\"DEATH_EVENT\"].value_counts(normalize=True)[i] * 100\n\ndf_d = df_d.reindex(index=[\"anaemia_0\", \"anaemia_1\", \"diabetes_0\", \"diabetes_1\", \"high_blood_pressure_0\", \"high_blood_pressure_1\",\n                           \"sex_0\", \"sex_1\", \"smoking_0\", \"smoking_1\"])\ndf_d.at[\"sex_0\", \"value\"] = \"0 (Female)\"\ndf_d.at[\"sex_1\", \"value\"] = \"1 (Male)\"\n\nfig = go.Figure(data=[\n    go.Bar(y=[[\"anaemia\", \"anaemia\",\"diabetes\",\"diabetes\",\"high_blood_pressure\",\"high_blood_pressure\",\"sex\",\"sex\",\"smoking\",\"smoking\"], list(df_d[\"value\"])],\n           x=df_d[0], name=\"DEATH_EVENT = 0<br>(survived)\", orientation='h', marker=dict(color=palette_ro[1])),\n    go.Bar(y=[[\"anaemia\", \"anaemia\",\"diabetes\",\"diabetes\",\"high_blood_pressure\",\"high_blood_pressure\",\"sex\",\"sex\",\"smoking\",\"smoking\"], list(df_d[\"value\"])],\n           x=df_d[1], name=\"DEATH_EVENT = 1<br>(dead)\", orientation='h', marker=dict(color=palette_ro[6]))\n])\nfig.update_layout(barmode=\"stack\",\n                  title=\"Percentage of DEATH_EVENT per binary features\")\nfig.update_yaxes(autorange=\"reversed\")\nfig.show(config={\"displayModeBar\": False})","d30fdb61":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nrange_bin_width = np.arange(df[\"age\"].min(), df[\"age\"].max()+5, 5)\n\nsns.distplot(df[\"age\"], ax=ax1, bins=range_bin_width, color=palette_ro[5])\nsns.distplot(df[df[\"DEATH_EVENT\"]==0].age, label=\"DEATH_EVENT=0\", ax=ax2, bins=range_bin_width, color=palette_ro[1])\nsns.distplot(df[df[\"DEATH_EVENT\"]==1].age, label=\"DEATH_EVENT=1\", ax=ax2, bins=range_bin_width, color=palette_ro[6])\nax1.set_title(\"age distribution\", fontsize=16);\nax2.set_title(\"Relationship between age and DEATH_EVENT\", fontsize=16)\n\nax1.axvline(x=df[\"age\"].median(), color=palette_ro[5], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==0].age.median(), color=palette_ro[1], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==1].age.median(), color=palette_ro[6], linestyle=\"--\", alpha=0.5)\n\nax1.annotate(\"Min: {:.0f}\".format(df[\"age\"].min()), xy=(df[\"age\"].min(), 0.010), \n             xytext=(df[\"age\"].min()-7, 0.015),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.2\"))\nax1.annotate(\"Max: {:.0f}\".format(df[\"age\"].max()), xy=(df[\"age\"].max(), 0.005), \n             xytext=(df[\"age\"].max(), 0.008),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.2\"))\nax1.annotate(\"Med: {:.0f}\".format(df[\"age\"].median()), xy=(df[\"age\"].median(), 0.032), \n             xytext=(df[\"age\"].median()-8, 0.035),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.25\"))\n\nax2.annotate(\"Survived\\nMed: {:.0f}\".format(df[df[\"DEATH_EVENT\"]==0].age.median()), xy=(df[df[\"DEATH_EVENT\"]==0].age.median(), 0.033), \n             xytext=(df[df[\"DEATH_EVENT\"]==0].age.median()-18, 0.035),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.25\"))\nax2.annotate(\"Dead\\nMed: {:.0f}\".format(df[df[\"DEATH_EVENT\"]==1].age.median()), xy=(df[df[\"DEATH_EVENT\"]==1].age.median(), 0.026), \n             xytext=(df[df[\"DEATH_EVENT\"]==1].age.median()+7, 0.029),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.25\"))\nax2.legend();","53365017":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nsns.distplot(df[\"creatinine_phosphokinase\"], ax=ax1, color=palette_ro[5])\nsns.distplot(df[df[\"DEATH_EVENT\"]==0].creatinine_phosphokinase, label=\"DEATH_EVENT=0\", ax=ax2, hist=None, color=palette_ro[1])\nsns.distplot(df[df[\"DEATH_EVENT\"]==1].creatinine_phosphokinase, label=\"DEATH_EVENT=1\", ax=ax2, hist=None, color=palette_ro[6])\nax1.set_title(\"creatinine_phosphokinase distribution\", fontsize=16);\nax2.set_title(\"Relationship between creatinine_phosphokinase and DEATH_EVENT\", fontsize=16)\n\nax1.axvline(x=df[\"creatinine_phosphokinase\"].median(), color=palette_ro[5], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==0].creatinine_phosphokinase.median(), color=palette_ro[1], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==1].creatinine_phosphokinase.median(), color=palette_ro[6], linestyle=\"--\", alpha=0.5)\n\nax1.annotate(\"Min: {:,}\".format(df[\"creatinine_phosphokinase\"].min()), xy=(df[\"creatinine_phosphokinase\"].min(), 0.00085), \n             xytext=(df[\"creatinine_phosphokinase\"].min()-700, 0.0010),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.2\"))\nax1.annotate(\"Max: {:,}\".format(df[\"creatinine_phosphokinase\"].max()), xy=(df[\"creatinine_phosphokinase\"].max(), 0.00005), \n             xytext=(df[\"creatinine_phosphokinase\"].max()-500, 0.0002),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.2\"))\nax1.annotate(\"Med: {:.0f}\".format(df[\"creatinine_phosphokinase\"].median()), xy=(df[\"creatinine_phosphokinase\"].median(), 0.0014), \n             xytext=(df[\"creatinine_phosphokinase\"].median()+500, 0.0015),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.25\"))\n\nax2.annotate(\"Survived\\nMed: {:.0f}\".format(df[df[\"DEATH_EVENT\"]==0].creatinine_phosphokinase.median()), xy=(df[df[\"DEATH_EVENT\"]==0].creatinine_phosphokinase.median(), 0.00145), \n             xytext=(df[df[\"DEATH_EVENT\"]==0].creatinine_phosphokinase.median()+600, 0.00145),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.25\"))\nax2.annotate(\"Dead\\nMed: {:.0f}\".format(df[df[\"DEATH_EVENT\"]==1].creatinine_phosphokinase.median()), xy=(df[df[\"DEATH_EVENT\"]==1].creatinine_phosphokinase.median(), 0.00135), \n             xytext=(df[df[\"DEATH_EVENT\"]==1].creatinine_phosphokinase.median()+700, 0.00125),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.25\"))\nax2.legend();","8aa9e89d":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nrange_bin_width = np.arange(df[\"ejection_fraction\"].min(), df[\"ejection_fraction\"].max()+1, 1)\n\nsns.distplot(df[\"ejection_fraction\"], ax=ax1, bins=range_bin_width, color=palette_ro[5])\nsns.distplot(df[df[\"DEATH_EVENT\"]==0].ejection_fraction, label=\"DEATH_EVENT=0\", ax=ax2, bins=range_bin_width, color=palette_ro[1])\nsns.distplot(df[df[\"DEATH_EVENT\"]==1].ejection_fraction, label=\"DEATH_EVENT=1\", ax=ax2, bins=range_bin_width, color=palette_ro[6])\nax1.set_title(\"ejection_fraction distribution\", fontsize=16);\nax2.set_title(\"Relationship between ejection_fraction and DEATH_EVENT\", fontsize=16)\n\nax1.axvline(x=df[\"ejection_fraction\"].median(), color=palette_ro[5], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==0].ejection_fraction.median(), color=palette_ro[1], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==1].ejection_fraction.median(), color=palette_ro[6], linestyle=\"--\", alpha=0.5)\n\nax1.annotate(\"Min: {:,}\".format(df[\"ejection_fraction\"].min()), xy=(df[\"ejection_fraction\"].min(), 0.005), \n             xytext=(df[\"ejection_fraction\"].min()-5, 0.022),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.2\"))\nax1.annotate(\"Max: {:,}\".format(df[\"ejection_fraction\"].max()), xy=(df[\"ejection_fraction\"].max(), 0.001), \n             xytext=(df[\"ejection_fraction\"].max(), 0.022),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.2\"))\nax1.annotate(\"Med: {:.0f}\".format(df[\"ejection_fraction\"].median()), xy=(df[\"ejection_fraction\"].median(), 0.041), \n             xytext=(df[\"ejection_fraction\"].median()+5, 0.074),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.25\"))\n\nax2.annotate(\"Survived\\nMed: {:.0f}\".format(df[df[\"DEATH_EVENT\"]==0].ejection_fraction.median()), xy=(df[df[\"DEATH_EVENT\"]==0].ejection_fraction.median(), 0.051), \n             xytext=(df[df[\"DEATH_EVENT\"]==0].ejection_fraction.median()+5, 0.091),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.25\"))\nax2.annotate(\"Dead\\nMed: {:.0f}\".format(df[df[\"DEATH_EVENT\"]==1].ejection_fraction.median()), xy=(df[df[\"DEATH_EVENT\"]==1].ejection_fraction.median(), 0.03), \n             xytext=(df[df[\"DEATH_EVENT\"]==1].ejection_fraction.median()-18, 0.04),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.25\"))\nax2.legend();","200b66b4":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nsns.distplot(df[\"platelets\"], ax=ax1, color=palette_ro[5])\nsns.distplot(df[df[\"DEATH_EVENT\"]==0].platelets, label=\"DEATH_EVENT=0\", ax=ax2, hist=None, color=palette_ro[1])\nsns.distplot(df[df[\"DEATH_EVENT\"]==1].platelets, label=\"DEATH_EVENT=1\", ax=ax2, hist=None, color=palette_ro[6])\nax1.set_title(\"platelets distribution\", fontsize=16);\nax2.set_title(\"Relationship between platelets and DEATH_EVENT\", fontsize=16)\n\nax1.axvline(x=df[\"platelets\"].median(), color=palette_ro[5], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==0].platelets.median(), color=palette_ro[1], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==1].platelets.median(), color=palette_ro[6], linestyle=\"--\", alpha=0.5)\n\nax1.annotate(\"Min: {:,.0f}\".format(df[\"platelets\"].min()), xy=(df[\"platelets\"].min(), 2e-7), \n             xytext=(df[\"platelets\"].min()-50000, 7e-7),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.2\"))\nax1.annotate(\"Max: {:,.0f}\".format(df[\"platelets\"].max()), xy=(df[\"platelets\"].max(), 1e-7), \n             xytext=(df[\"platelets\"].max()-30000, 7e-7),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.2\"))\nax1.annotate(\"Med: {:,.0f}\".format(df[\"platelets\"].median()), xy=(df[\"platelets\"].median(), 5.9e-6), \n             xytext=(df[\"platelets\"].median()+25000, 5.5e-6),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.25\"))\n\nax2.annotate(\"Survived\\nMed: {:,.0f}\".format(df[df[\"DEATH_EVENT\"]==0].platelets.median()), xy=(df[df[\"DEATH_EVENT\"]==0].platelets.median(), 6.2e-6), \n             xytext=(df[df[\"DEATH_EVENT\"]==0].platelets.median()+50000, 5.5e-6),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.25\"))\nax2.annotate(\"Dead\\nMed: {:,.0f}\".format(df[df[\"DEATH_EVENT\"]==1].platelets.median()), xy=(df[df[\"DEATH_EVENT\"]==1].platelets.median(), 4.5e-6), \n             xytext=(df[df[\"DEATH_EVENT\"]==1].platelets.median()-200000, 5.2e-6),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.25\"))\nax2.legend();","e17d2c91":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nrange_bin_width = np.arange(df[\"serum_creatinine\"].min(), df[\"serum_creatinine\"].max()+0.25, 0.25)\n\nsns.distplot(df[\"serum_creatinine\"], ax=ax1, bins=range_bin_width, color=palette_ro[5])\nsns.distplot(df[df[\"DEATH_EVENT\"]==0].serum_creatinine, label=\"DEATH_EVENT=0\", ax=ax2, bins=range_bin_width, color=palette_ro[1])\nsns.distplot(df[df[\"DEATH_EVENT\"]==1].serum_creatinine, label=\"DEATH_EVENT=1\", ax=ax2, bins=range_bin_width, color=palette_ro[6])\nax1.set_title(\"serum_creatinine distribution\", fontsize=16);\nax2.set_title(\"Relationship serum_creatinine age and DEATH_EVENT\", fontsize=16)\n\nax1.axvline(x=df[\"serum_creatinine\"].median(), color=palette_ro[5], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==0].serum_creatinine.median(), color=palette_ro[1], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==1].serum_creatinine.median(), color=palette_ro[6], linestyle=\"--\", alpha=0.5)\n\nax1.annotate(\"Min: {:.1f}\".format(df[\"serum_creatinine\"].min()), xy=(df[\"serum_creatinine\"].min(), 0.31), \n             xytext=(df[\"serum_creatinine\"].min()-0.7, 0.5),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.2\"))\nax1.annotate(\"Max: {:.1f}\".format(df[\"serum_creatinine\"].max()), xy=(df[\"serum_creatinine\"].max(), 0.05), \n             xytext=(df[\"serum_creatinine\"].max()-0.2, 0.25),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.2\"))\nax1.annotate(\"Med: {:.1f}\".format(df[\"serum_creatinine\"].median()), xy=(df[\"serum_creatinine\"].median(), 1.22), \n             xytext=(df[\"serum_creatinine\"].median()+0.5, 1.3),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.25\"))\n\nax2.annotate(\"Survived\\nMed: {:.1f}\".format(df[df[\"DEATH_EVENT\"]==0].serum_creatinine.median()), xy=(df[df[\"DEATH_EVENT\"]==0].serum_creatinine.median(), 1.47), \n             xytext=(df[df[\"DEATH_EVENT\"]==0].serum_creatinine.median()-1.3, 1.5),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.25\"))\nax2.annotate(\"Dead\\nMed: {:.1f}\".format(df[df[\"DEATH_EVENT\"]==1].serum_creatinine.median()), xy=(df[df[\"DEATH_EVENT\"]==1].serum_creatinine.median(), 0.62), \n             xytext=(df[df[\"DEATH_EVENT\"]==1].serum_creatinine.median()+0.4, 0.7),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.25\"))\nax2.legend();","b74770ba":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nrange_bin_width = np.arange(df[\"serum_sodium\"].min(), df[\"serum_sodium\"].max()+1, 1)\n\nsns.distplot(df[\"serum_sodium\"], ax=ax1, bins=range_bin_width, color=palette_ro[5])\nsns.distplot(df[df[\"DEATH_EVENT\"]==0].serum_sodium, label=\"DEATH_EVENT=0\", ax=ax2, bins=range_bin_width, color=palette_ro[1])\nsns.distplot(df[df[\"DEATH_EVENT\"]==1].serum_sodium, label=\"DEATH_EVENT=1\", ax=ax2, bins=range_bin_width, color=palette_ro[6])\nax1.set_title(\"serum_sodium distribution\", fontsize=16);\nax2.set_title(\"Relationship between serum_sodium and DEATH_EVENT\", fontsize=16)\n\nax1.axvline(x=df[\"serum_sodium\"].median(), color=palette_ro[5], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==0].serum_sodium.median(), color=palette_ro[1], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==1].serum_sodium.median(), color=palette_ro[6], linestyle=\"--\", alpha=0.5)\n\nax1.annotate(\"Min: {:.0f}\".format(df[\"serum_sodium\"].min()), xy=(df[\"serum_sodium\"].min(), 0.005), \n             xytext=(df[\"serum_sodium\"].min()-3, 0.015),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.2\"))\nax1.annotate(\"Max: {:.0f}\".format(df[\"serum_sodium\"].max()), xy=(df[\"serum_sodium\"].max(), 0.005), \n             xytext=(df[\"serum_sodium\"].max(), 0.015),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.2\"))\nax1.annotate(\"Med: {:.0f}\".format(df[\"serum_sodium\"].median()), xy=(df[\"serum_sodium\"].median(), 0.103), \n             xytext=(df[\"serum_sodium\"].median()-6, 0.115),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.25\"))\n\nax2.annotate(\"Survived\\nMed: {:.0f}\".format(df[df[\"DEATH_EVENT\"]==0].serum_sodium.median()), xy=(df[df[\"DEATH_EVENT\"]==0].serum_sodium.median(), 0.117), \n             xytext=(df[df[\"DEATH_EVENT\"]==0].serum_sodium.median()+5, 0.135),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.25\"))\nax2.annotate(\"Dead\\nMed: {:.0f}\".format(df[df[\"DEATH_EVENT\"]==1].serum_sodium.median()), xy=(df[df[\"DEATH_EVENT\"]==1].serum_sodium.median(), 0.09), \n             xytext=(df[df[\"DEATH_EVENT\"]==1].serum_sodium.median()-5.5, 0.11),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.25\"))\nax2.legend();","81c57ac3":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nrange_bin_width = np.arange(df[\"time\"].min(), df[\"time\"].max()+10, 10)\n\nsns.distplot(df[\"time\"], ax=ax1, bins=range_bin_width, color=palette_ro[5])\nsns.distplot(df[df[\"DEATH_EVENT\"]==0].time, label=\"DEATH_EVENT=0\", ax=ax2, bins=range_bin_width, color=palette_ro[1])\nsns.distplot(df[df[\"DEATH_EVENT\"]==1].time, label=\"DEATH_EVENT=1\", ax=ax2, bins=range_bin_width, color=palette_ro[6])\nax1.set_title(\"time distribution\", fontsize=16);\nax2.set_title(\"Relationship between time and DEATH_EVENT\", fontsize=16)\n\nax1.axvline(x=df[\"time\"].median(), color=palette_ro[5], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==0].time.median(), color=palette_ro[1], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==1].time.median(), color=palette_ro[6], linestyle=\"--\", alpha=0.5)\n\nax1.annotate(\"Min: {:.0f}\".format(df[\"time\"].min()), xy=(df[\"time\"].min(), 0.0021), \n             xytext=(df[\"time\"].min()-30, 0.0032),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.2\"))\nax1.annotate(\"Max: {:.0f}\".format(df[\"time\"].max()), xy=(df[\"time\"].max(), 0.001), \n             xytext=(df[\"time\"].max(), 0.0017),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.2\"))\nax1.annotate(\"Med: {:.0f}\".format(df[\"time\"].median()), xy=(df[\"time\"].median(), 0.0041), \n             xytext=(df[\"time\"].median()+8, 0.0052),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.25\"))\n\nax2.annotate(\"Survived\\nMed: {:.0f}\".format(df[df[\"DEATH_EVENT\"]==0].time.median()), xy=(df[df[\"DEATH_EVENT\"]==0].time.median(), 0.0035), \n             xytext=(df[df[\"DEATH_EVENT\"]==0].time.median()-40, 0.007),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.25\"))\nax2.annotate(\"Dead\\nMed: {:.0f}\".format(df[df[\"DEATH_EVENT\"]==1].time.median()), xy=(df[df[\"DEATH_EVENT\"]==1].time.median(), 0.0082), \n             xytext=(df[df[\"DEATH_EVENT\"]==1].time.median()+7, 0.0105),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.25\"))\nax2.legend();","4fa88976":"fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n\nsns.scatterplot(x=df[\"serum_creatinine\"], y=df[\"ejection_fraction\"], ax=ax,\n                palette=[palette_ro[1], palette_ro[6]], hue=df[\"DEATH_EVENT\"])\nax.plot([0.9, 5.3], [13, 80.0], color=\"gray\", ls=\"--\")\n\nfig.suptitle(\"Relationship between serum_creatinine and ejection_fraction against DEATH_EVENT\", fontsize=18);","d037cefd":"fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n\nsns.heatmap(df.corr(), ax=ax, vmax=1, vmin=-1, center=0,\n            annot=True, fmt=\".2f\",\n            cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            mask=np.triu(np.ones_like(df.corr(), dtype=np.bool)))\n\nfig.suptitle(\"Diagonal correlation matrix\", fontsize=18);","b66fb4c0":"num_features = [\"age\", \"creatinine_phosphokinase\", \"ejection_fraction\", \"platelets\", \"serum_creatinine\", \"serum_sodium\", \"time\"]\nnum_features_s = []\n\nfor i in range(len(num_features)):\n    num_features_s.append(num_features[i] + \"_s\")\n\nsc = StandardScaler()\ndf[num_features_s] = sc.fit_transform(df[num_features])\ndf.head()","6cfd4bd0":"X = df.copy()\ny = X[\"DEATH_EVENT\"]\nX = X.drop([\"DEATH_EVENT\"], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y, random_state=0)\nX_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)\nX_train.head()","58756d30":"features = [\"age\", \"anaemia\", \"creatinine_phosphokinase\", \"diabetes\", \"ejection_fraction\", \"high_blood_pressure\", \"platelets\",\n            \"serum_creatinine\", \"serum_sodium\", \"sex\", \"smoking\", \"time\"]\nNFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(features)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = lgb.LGBMClassifier(objective=\"binary\",\n                             metric=\"binary_logloss\")\n    clf.fit(X_tr, y_tr, eval_set = [(X_va, y_va)],\n            early_stopping_rounds=10,\n            verbose=-1)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test[features], num_iteration=clf.best_iteration_)\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\n\nprint(f\"\\nOut-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), features), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of default LightGBM\", fontsize=18);","9b0d74ac":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of default LightGBM\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1-score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","b4941347":"features = [\"ejection_fraction\", \"serum_creatinine\", \"time\"]\n\ndef objective(trial):\n    skf = StratifiedKFold(n_splits=NFOLD)\n    oof = np.zeros((len(X_train), ))\n\n    for fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n        print(f\"FOLD {fold_id+1}\")\n        X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n        y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n\n        clf = lgb.LGBMClassifier(objective=\"binary\",\n                                 metric=\"binary_logloss\",\n                                 colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.4, 1.0),\n                                 learning_rate = trial.suggest_uniform(\"learning_rate\", 1e-8, 1.0),\n                                 max_depth = trial.suggest_int(\"max_depth\", 2, 32),\n                                 min_child_samples = trial.suggest_int(\"min_child_samples\", 3, 500),\n                                 min_child_weight = trial.suggest_loguniform(\"min_child_weight\", 1e-4, 1e+1),\n                                 n_estimators = trial.suggest_int(\"n_estimators\", 20, 200),\n                                 num_leaves = trial.suggest_int(\"num_leaves\", 2, 512),\n                                 reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 10.0),\n                                 reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 10.0),\n                                 subsample = trial.suggest_uniform(\"subsample\", 0.4, 1.0),\n                                 subsample_freq = trial.suggest_int(\"subsample_freq\", 0, 7),\n                                )\n        clf.fit(X_tr, y_tr, eval_set = [(X_va, y_va)],\n                early_stopping_rounds=20,\n                verbose=-1)\n        oof[va_idx] = clf.predict(X_va)\n        \n    score = accuracy_score(y_train, oof)\n    return score\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=0))\nstudy.optimize(objective, n_trials=100)","532b7f0a":"study.best_params","cc0515bc":"optuna.importance.get_param_importances(study)","dac59bf4":"fig = optuna.visualization.plot_param_importances(study)\nfig.show(config={\"displayModeBar\": False})","4b3becb8":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(features)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = lgb.LGBMClassifier(objective=\"binary\",\n                             metric=\"binary_logloss\",\n                             **study.best_params)\n    clf.fit(X_tr, y_tr, eval_set = [(X_va, y_va)],\n            early_stopping_rounds=10,\n            verbose=-1)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test[features], num_iteration=clf.best_iteration_)\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\ny_pred_gbm = np.mean(y_preds, axis=1)\n\nprint(f\"\\nOut-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), features), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of optimized LightGBM\", fontsize=18);","14206cfa":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of optimized LightGBM\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","1bdc420f":"features = [\"age\", \"anaemia\", \"creatinine_phosphokinase\", \"diabetes\", \"ejection_fraction\", \"high_blood_pressure\", \"platelets\",\n            \"serum_creatinine\", \"serum_sodium\", \"sex\", \"smoking\", \"time\"]\nNFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(features)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = xgb.XGBClassifier()\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test[features])\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), features), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of default XGBoost\", fontsize=18);","e6f984c3":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of default XGBoost\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1-score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","61a0479f":"features = [\"ejection_fraction\", \"serum_creatinine\", \"time\"]\n\ndef objective(trial):\n    skf = StratifiedKFold(n_splits=NFOLD)\n    models = []\n    imp = np.zeros((NFOLD, len(features)))\n    oof = np.zeros((len(X_train), ))\n    y_preds = np.zeros((len(X_test), NFOLD))\n\n    for fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n        # print(f\"FOLD {fold_id+1}\")\n        X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n        y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n\n        clf = xgb.XGBClassifier(n_estimators = trial.suggest_int(\"n_estimators\", 20, 200),\n                                max_depth = trial.suggest_int(\"max_depth\", 2, 32),\n                                learning_rate = trial.suggest_uniform(\"learning_rate\", 1e-8, 1.0),\n                                min_child_weight = trial.suggest_loguniform(\"min_child_weight\", 1e-4, 1e+1),\n                                subsample = trial.suggest_uniform(\"subsample\", 0.4, 1.0),\n                                colsample_bytree = trial.suggest_uniform(\"colsample_bytree\", 0.4, 1.0),\n                                reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 10.0),\n                                reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 10.0),\n                                scale_pos_weight = trial.suggest_int(\"scale_pos_weight\", 1, 100)\n                                )\n        clf.fit(X_tr, y_tr)\n        oof[va_idx] = clf.predict(X_va)\n        \n    score = accuracy_score(y_train, oof)\n    return score\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=0))\nstudy.optimize(objective, n_trials=100)","7d67d3a8":"study.best_params","decc3e1c":"optuna.importance.get_param_importances(study)","cc4eccee":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(features)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = xgb.XGBClassifier(**study.best_params)\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test[features])\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\ny_pred_xgb = np.mean(y_preds, axis=1)\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), features), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of optimized XGBoost\", fontsize=18);","442b060e":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of optimized XGBoost\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","2fcb64fd":"features = [\"age\", \"anaemia\", \"creatinine_phosphokinase\", \"diabetes\", \"ejection_fraction\", \"high_blood_pressure\", \"platelets\",\n            \"serum_creatinine\", \"serum_sodium\", \"sex\", \"smoking\", \"time\"]\nNFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(features)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = CatBoostClassifier(loss_function=\"Logloss\")\n    clf.fit(X_tr, y_tr, eval_set = [(X_va, y_va)],\n            early_stopping_rounds=10,\n            verbose=False)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.get_feature_importance()\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test[features])\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), features), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of default CatBoost\", fontsize=18);","c400d02f":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of default CatBoost\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1-score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","407fbb12":"features = [\"ejection_fraction\", \"serum_creatinine\", \"time\"]\n\ndef objective(trial):\n    skf = StratifiedKFold(n_splits=NFOLD)\n    models = []\n    imp = np.zeros((NFOLD, len(features)))\n    oof = np.zeros((len(X_train), ))\n    y_preds = np.zeros((len(X_test), NFOLD))\n\n    for fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n        # print(f\"FOLD {fold_id+1}\")\n        X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n        y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n\n        clf = CatBoostClassifier(loss_function=\"Logloss\",\n                                 iterations = trial.suggest_int(\"iterations\", 1000, 6000),\n                                 learning_rate = trial.suggest_uniform(\"learning_rate\", 1e-4, 1e-1),\n                                 l2_leaf_reg = trial.suggest_loguniform(\"l2_leaf_reg\", 1e-8, 10.0),\n                                 # bagging_temperature = trial.suggest_loguniform(\"bagging_temperature\", 1e-8, 100.0),\n                                 subsample = trial.suggest_uniform(\"subsample\", 0.4, 1.0),\n                                 # random_strength = trial.suggest_loguniform(\"random_strength\", 1e-8, 100.0),\n                                 depth = trial.suggest_int(\"depth\", 2, 16),\n                                 min_data_in_leaf = trial.suggest_int(\"min_data_in_leaf\", 1, 200),\n                                 # od_type = trial.suggest_categorical(\"od_type\", [\"IncToDec\", \"Iter\"]),\n                                 # od_wait = trial.suggest_int(\"od_wait\", 10, 50)\n                                )\n        clf.fit(X_tr, y_tr, eval_set = [(X_va, y_va)],\n                early_stopping_rounds=10,\n                verbose=False)\n        oof[va_idx] = clf.predict(X_va)\n        \n    score = accuracy_score(y_train, oof)\n    return score\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=0))\nstudy.optimize(objective, n_trials=80)","5a298d52":"study.best_params","51c4adea":"optuna.importance.get_param_importances(study)","3efa2b7c":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(features)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = CatBoostClassifier(loss_function=\"Logloss\",\n                             **study.best_params)\n    clf.fit(X_tr, y_tr, eval_set = [(X_va, y_va)],\n            early_stopping_rounds=10,\n            verbose=False)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.get_feature_importance()\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test[features])\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\ny_pred_cat = np.mean(y_preds, axis=1)\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), features), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of optimized CatBoost\", fontsize=18);","0f6b7123":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of optimized CatBoost\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","87fced01":"features = [\"age\", \"anaemia\", \"creatinine_phosphokinase\", \"diabetes\", \"ejection_fraction\", \"high_blood_pressure\", \"platelets\",\n            \"serum_creatinine\", \"serum_sodium\", \"sex\", \"smoking\", \"time\"]\nNFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(features)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = RandomForestClassifier(random_state=0)\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test[features])\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), features), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of default Random forest\", fontsize=18);","a69c73b0":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of default Random forest\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","7fb28294":"features = [\"ejection_fraction\", \"serum_creatinine\", \"time\"]\n\ndef objective(trial):\n    skf = StratifiedKFold(n_splits=NFOLD)\n    models = []\n    imp = np.zeros((NFOLD, len(features)))\n    oof = np.zeros((len(X_train), ))\n    y_preds = np.zeros((len(X_test), NFOLD))\n\n    for fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n        # print(f\"FOLD {fold_id+1}\")\n        X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n        y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n\n        clf = RandomForestClassifier(random_state=0,\n                                     n_estimators = trial.suggest_int(\"n_estimators\", 20, 200),\n                                     max_depth = trial.suggest_int(\"max_depth\", 2, 32),\n                                     min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 16),\n                                     min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 16))\n        clf.fit(X_tr, y_tr)\n        oof[va_idx] = clf.predict(X_va)\n        \n    score = accuracy_score(y_train, oof)\n    return score\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=0))\nstudy.optimize(objective, n_trials=50)","1cd6d78d":"study.best_params","30a59865":"optuna.importance.get_param_importances(study)","4130f7af":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(features)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = RandomForestClassifier(random_state=0,\n                                 **study.best_params)\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test[features])\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\ny_pred_rf = np.mean(y_preds, axis=1)\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), features), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of optimized Random forest\", fontsize=18);","e0f82360":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of optimized Random forest\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","b2aa8d90":"features = [\"age\", \"anaemia\", \"creatinine_phosphokinase\", \"diabetes\", \"ejection_fraction\", \"high_blood_pressure\", \"platelets\",\n            \"serum_creatinine\", \"serum_sodium\", \"sex\", \"smoking\", \"time\"]\nNFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(features)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = ExtraTreesClassifier(random_state=0)\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test[features])\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), features), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of default Extremely randomized trees\", fontsize=18);","37de3f89":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of default Extremely randomized trees\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","2f47bd7f":"features = [\"age\", \"ejection_fraction\", \"serum_creatinine\", \"time\"]\n\ndef objective(trial):\n    skf = StratifiedKFold(n_splits=NFOLD)\n    models = []\n    imp = np.zeros((NFOLD, len(features)))\n    oof = np.zeros((len(X_train), ))\n    y_preds = np.zeros((len(X_test), NFOLD))\n\n    for fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n        # print(f\"FOLD {fold_id+1}\")\n        X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n        y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n\n        clf = RandomForestClassifier(random_state=0,\n                                     n_estimators = trial.suggest_int(\"n_estimators\", 20, 200),\n                                     max_depth = trial.suggest_int(\"max_depth\", 2, 32),\n                                     min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 16),\n                                     min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 16))\n        clf.fit(X_tr, y_tr)\n        oof[va_idx] = clf.predict(X_va)\n        \n    score = accuracy_score(y_train, oof)\n    return score\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=0))\nstudy.optimize(objective, n_trials=50)","00dbc4a8":"study.best_params","7ace05d3":"optuna.importance.get_param_importances(study)","076023ec":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(features)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = ExtraTreesClassifier(random_state=0,\n                               **study.best_params)\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test[features])\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\ny_pred_ert = np.mean(y_preds, axis=1)\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), features), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of optimized Extremely randomized trees\", fontsize=18);","314526bf":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of optimized Extremely randomized trees\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","9edc825c":"features = [\"age_s\", \"anaemia\", \"creatinine_phosphokinase_s\", \"diabetes\", \"ejection_fraction_s\", \"high_blood_pressure\", \"platelets_s\",\n            \"serum_creatinine_s\", \"serum_sodium_s\", \"sex\", \"smoking\", \"time_s\"]\nNFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = LogisticRegression(random_state=0)\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test[features])\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")","3c61b366":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of default linear model\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","c54dfde5":"features = [\"age_s\", \"creatinine_phosphokinase_s\", \"ejection_fraction_s\", \"serum_creatinine_s\", \"serum_sodium_s\", \"time_s\"]\nNFOLD = 10\n\ndef objective(trial):\n    skf = StratifiedKFold(n_splits=NFOLD)\n    models = []\n    imp = np.zeros((NFOLD, len(features)))\n    oof = np.zeros((len(X_train), ))\n    y_preds = np.zeros((len(X_test), NFOLD))\n\n    for fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n        # print(f\"FOLD {fold_id+1}\")\n        X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n        y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n\n        clf = LogisticRegression(random_state=0,\n                                 C = trial.suggest_uniform(\"C\", 0.1, 10.0),\n                                 intercept_scaling = trial.suggest_uniform(\"intercept_scaling\", 0.1, 2.0),\n                                 max_iter = trial.suggest_int(\"max_iter\", 100, 1000)\n                                 )\n        clf.fit(X_tr, y_tr)\n        oof[va_idx] = clf.predict(X_va)\n        \n    score = accuracy_score(y_train, oof)\n    return score\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=0))\nstudy.optimize(objective, n_trials=20)","65a6253f":"study.best_params","cc6638c6":"optuna.importance.get_param_importances(study)","4a3948d0":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = LogisticRegression(random_state=0,\n                             **study.best_params)\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test[features])\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\ny_pred_lm = np.mean(y_preds, axis=1)\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")","691a3dae":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of optimized linear model\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","46f4af9f":"features = [\"age_s\", \"anaemia\", \"creatinine_phosphokinase_s\", \"diabetes\", \"ejection_fraction_s\", \"high_blood_pressure\", \"platelets_s\",\n            \"serum_creatinine_s\", \"serum_sodium_s\", \"sex\", \"smoking\", \"time_s\"]\nNFOLD = 10\nseed_everything(0)\n\nBATCH_SIZE = 32\n\nskf = StratifiedKFold(n_splits=NFOLD)\noof = np.zeros((len(X_train), 1))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(128, activation=\"relu\", input_shape=(len(features), )),\n        tf.keras.layers.Dropout(0.1),\n        tf.keras.layers.Dense(128, activation=\"relu\"),\n        tf.keras.layers.Dense(1)\n    ])\n    \n    model.compile(loss=\"binary_crossentropy\",\n                  optimizer=tf.keras.optimizers.Adam(lr=0.001,\n                                                     decay=0.0),\n                  metrics=[\"accuracy\"])\n    \n    model.fit(X_tr, y_tr,\n              validation_data=(X_va, y_va),\n              epochs=100, batch_size=BATCH_SIZE,\n              verbose=0)\n    \n    oof[va_idx] = model.predict(X_va, batch_size=BATCH_SIZE, verbose=0)\n    y_preds += model.predict(X_test[features], batch_size=BATCH_SIZE, verbose=0) \/ NFOLD\n\noof = (np.mean(oof, axis=1) > 0.5).astype(int)\ny_pred = (np.mean(y_preds, axis=1) > 0.5).astype(int)\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")","635c58a3":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of deep learning\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","09d25136":"features = [\"ejection_fraction_s\", \"serum_creatinine_s\", \"serum_sodium_s\", \"time_s\"]\nNFOLD = 10\nseed_everything(0)\n\nBATCH_SIZE = 32\n\nskf = StratifiedKFold(n_splits=NFOLD)\noof = np.zeros((len(X_train), 1))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(64, activation=\"elu\", input_shape=(len(features), )),\n        tf.keras.layers.Dropout(0.1),\n        tf.keras.layers.Dense(64, activation=\"elu\"),\n        tf.keras.layers.Dropout(0.1),\n        tf.keras.layers.Dense(1)\n    ])\n    \n    model.compile(loss=\"binary_crossentropy\",\n                  optimizer=tf.keras.optimizers.Adam(lr=0.001,\n                                                     decay=0.0),\n                  metrics=[\"accuracy\"])\n    \n    model.fit(X_tr, y_tr,\n              validation_data=(X_va, y_va),\n              epochs=102, batch_size=BATCH_SIZE,\n              verbose=0)\n    \n    oof[va_idx] = model.predict(X_va, batch_size=BATCH_SIZE, verbose=0)\n    y_preds += model.predict(X_test[features], batch_size=BATCH_SIZE, verbose=0) \/ NFOLD\n\noof = (np.mean(oof, axis=1) > 0.5).astype(int)\ny_pred = (np.mean(y_preds, axis=1) > 0.5).astype(int)\ny_pred_dl = y_pred\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")","c7b570cc":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of optimized deep learning\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","8b580055":"y_pred_em = y_pred_gbm + y_pred_xgb*2 + y_pred_cat + y_pred_rf + y_pred_ert*2 + y_pred_lm + y_pred_dl\ny_pred_em = (y_pred_em > 3.0).astype(int)\n\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred_em)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred_em)}\")","3b0bca5d":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred_em), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of the ensembled model\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1-score={:0.4f}\".format(accuracy_score(y_test, y_pred_em), f1_score(y_test, y_pred_em)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","259faf18":"<a id=\"ensemble\"><\/a>\n# Simple ensemble \ud83e\udd1d","d0b80979":"<a id=\"overview\"><\/a>\n# Overview \ud83e\uddd0\n<img src=\"https:\/\/i.imgur.com\/wC22WgL.jpeg\" width=\"600\"><br>\nWe are going to predict mortality by heart failure based on the 12 features included in the data set. This can be used to help hospitals in assessing the severity of patients with cardiovascular diseases (CVDs).<br>\n<font color=\"RoyalBlue\">\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u542b\u307e\u308c\u308b12\u306e\u7279\u5fb4\u91cf\u306b\u57fa\u3065\u3044\u3066\u5fc3\u4e0d\u5168\u306b\u3088\u308b\u6b7b\u4ea1\u7387\u3092\u4e88\u6e2c\u3057\u3066\u3044\u304d\u307e\u3059\u3002\u3053\u306e\u4e88\u6e2c\u306f\u3001\u75c5\u9662\u3067\u5fc3\u8840\u7ba1\u75be\u60a3\uff08CVDs\uff09\u60a3\u8005\u306e\u91cd\u75c7\u5ea6\u3092\u8a55\u4fa1\u3059\u308b\u969b\u306b\u5f79\u7acb\u3066\u3089\u308c\u308b\u3067\u3057\u3087\u3046\u3002<\/font><br>\n\nI have also run a similar analysis in R ([Heart Failure\u2763\ufe0f EDA & Prediction with R (91.5%acc)](https:\/\/www.kaggle.com\/snowpea8\/heart-failure-eda-prediction-with-r-91-5-acc)), \nif you would like to take a look at it.<br>\n<font color=\"RoyalBlue\">\u540c\u69d8\u306e\u5206\u6790\u3092 R \u3067\u3082\u5b9f\u884c\u3057\u3066\u3044\u307e\u3059\u306e\u3067\u3001\u305d\u3061\u3089\u3082\u53c2\u8003\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002<\/font><br>\n\nIn this notebook, we will first discover and visualize the data to gain insights. Then we split the data into a training and a test set and use the training set to train various machine learning models. At the same time, we evaluate the performance of the models with cross-validation. Finally, we will ensemble each model to improve its accuracy.<br>\n<font color=\"RoyalBlue\">\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3067\u306f\u3001\u307e\u305a\u6d1e\u5bdf\u3092\u5f97\u308b\u305f\u3081\u306b\u30c7\u30fc\u30bf\u3092\u7814\u7a76\u3001\u53ef\u8996\u5316\u3057\u307e\u3059\u3002\u305d\u308c\u304b\u3089\u30c7\u30fc\u30bf\u3092\u8a13\u7df4\u7528\u3068\u30c6\u30b9\u30c8\u7528\u306b\u5206\u5272\u3057\u3001\u8a13\u7df4\u30bb\u30c3\u30c8\u3092\u4f7f\u3063\u3066\u69d8\u3005\u306a\u6a5f\u68b0\u5b66\u7fd2\u30e2\u30c7\u30eb\u3092\u8a13\u7df4\u3057\u307e\u3059\u3002\u540c\u6642\u306b\u3001\u4ea4\u5dee\u691c\u8a3c\u3067\u30e2\u30c7\u30eb\u306e\u6027\u80fd\u3092\u8a55\u4fa1\u3057\u307e\u3059\u3002\u6700\u5f8c\u306b\u305d\u308c\u305e\u308c\u306e\u30e2\u30c7\u30eb\u3092\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3057\u3001\u7cbe\u5ea6\u306e\u5411\u4e0a\u3092\u76ee\u6307\u3057\u3066\u3044\u304d\u307e\u3059\u3002<\/font>\n\n# Table of contents \ud83d\udcd6\n* [Overview \ud83e\uddd0](#overview)\n* [Setup \ud83d\udcbb](#setup)\n* [Load CSV data \ud83d\udcc3](#load)\n* [Explore CSV data \ud83d\udcca](#explore)\n    * [Distribution of the binary features](#binary)\n    * [Distribution of the numeric features](#numeric)\n* [Data preprocessing \ud83e\uddf9](#preprocessing)\n* [Train models and make predictions \ud83d\udcad](#models)\n    * [LightGBM \ud83c\udf33](#gbm)\n    * [XGBoost \ud83c\udf33](#xgb)\n    * [CatBoost \ud83c\udf33](#cat)\n    * [Random forest \ud83c\udf33](#rf)\n    * [Extremely randomized trees \ud83c\udf33](#ert)\n    * [Linear model \ud83d\udcc8](#lm)\n    * [Deep learning \ud83e\udde0](#dl)\n* [Simple ensemble \ud83e\udd1d](#ensemble)\n\n<a id=\"setup\"><\/a>\n# Setup \ud83d\udcbb\nAll seed values are fixed at zero.<br>\n<font color=\"RoyalBlue\">\u30b7\u30fc\u30c9\u5024\u306f\u5168\u30660\u3067\u56fa\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002<\/font><br>","b7c735c7":"<a id=\"rf\"><\/a>\n## Random forest \ud83c\udf33","42070c3c":"We were able to get better accuracy by using the ensemble model. Thanks so much for reading!<br>\n<font color=\"RoyalBlue\">\u8907\u6570\u306e\u30e2\u30c7\u30eb\u3092\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3059\u308b\u3053\u3068\u3067\u3088\u308a\u826f\u3044\u7cbe\u5ea6\u3092\u51fa\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3057\u305f\u3002\u3053\u3053\u307e\u3067\u8aad\u3093\u3067\u304f\u3060\u3055\u308a\u3069\u3046\u3082\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3057\u305f\uff01<\/font>","8bdbb92b":"Insights:\n\n* For diabetes, sex, and smoking, there was little difference in the distribution of the objective variable.\n<br>\u3000<font color=\"RoyalBlue\">diabetes, sex, smoking \u306b\u304a\u3044\u3066\u306f\u3001\u76ee\u7684\u5909\u6570\u306e\u5206\u5e03\u306b\u307b\u3068\u3093\u3069\u5dee\u306f\u898b\u3089\u308c\u306a\u3044\u3002<\/font>\n* For anaemia and high_blood_pressure, there are some differences in the distributions of the objective variables, but we do not know if we can say that the differences are significant.\n<br>\u3000<font color=\"RoyalBlue\">anaemia, high_blood_pressure \u306b\u304a\u3044\u3066\u306f\u3001\u76ee\u7684\u5909\u6570\u306e\u5206\u5e03\u306b\u591a\u5c11\u306e\u5dee\u304c\u3042\u308b\u304c\u3001\u6709\u610f\u306a\u5dee\u304c\u3042\u308b\u3068\u8a00\u3048\u308b\u304b\u3069\u3046\u304b\u306f\u5206\u304b\u3089\u306a\u3044\u3002<\/font>\n\n<a id=\"numeric\"><\/a>\n## Distribution of the numeric features","0f50f84a":"Insights:\n\n* The explanatory variables that can be said to be significantly correlated with the objective variable are, in order of increasing correlation, `time`, `serum_creatinine`, `ejection_fraction`, `age`, and `serum_creatinine`.\n<br>\u3000<font color=\"RoyalBlue\">\u76ee\u7684\u5909\u6570\u306b\u5bfe\u3057\u3066\u6709\u610f\u306b\u76f8\u95a2\u304c\u3042\u308b\u3068\u8a00\u3048\u308b\u8aac\u660e\u5909\u6570\u306f\u3001\u76f8\u95a2\u304c\u9ad8\u3044\u9806\u306b time, serum_creatinine, ejection_fraction, age, serum_creatinine \u306e\uff15\u3064\u3067\u3042\u308b\u3002<\/font>\n* The correlation between explanatory variables is not very high.\n<br>\u3000<font color=\"RoyalBlue\">\u8aac\u660e\u5909\u6570\u540c\u58eb\u306e\u76f8\u95a2\u306f\u305d\u308c\u307b\u3069\u9ad8\u304f\u306a\u3044\u3002<\/font>\n\n<a id=\"preprocessing\"><\/a>\n# Data preprocessing \ud83e\uddf9\nExcept for models based on decision trees, we need to do feature scaling. In this case, let's do standardization (converting the mean of each feature to 0 and the standard deviation to 1). There are no categorical variables in this case, so there is no need for one-hot encoding or anything else.<br>\n<font color=\"RoyalBlue\">\u6c7a\u5b9a\u6728\u3092\u30d9\u30fc\u30b9\u306b\u3057\u305f\u30e2\u30c7\u30eb\u4ee5\u5916\u3067\u306f\u3001\u7279\u5fb4\u91cf\u306e\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3092\u884c\u3046\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u4eca\u56de\u306f\u3001\u6a19\u6e96\u5316\uff08\u5404\u7279\u5fb4\u91cf\u306e\u5e73\u5747\u3092\uff10\u3001\u6a19\u6e96\u504f\u5dee\u3092\uff11\u306b\u5909\u63db\uff09\u3092\u884c\u3063\u3066\u304a\u304d\u307e\u3057\u3087\u3046\u3002\u4eca\u56de\u306f\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u304c\u7121\u3044\u306e\u3067\u3001\u30ef\u30f3\u30db\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306a\u3069\u306f\u5fc5\u8981\u3042\u308a\u307e\u305b\u3093\u3002<\/font>","be636de3":"<a id=\"cat\"><\/a>\n## CatBoost \ud83c\udf33","2a94f4b3":"<a id=\"load\"><\/a>\n# Load CSV data \ud83d\udcc3","18306bff":"<a id=\"dl\"><\/a>\n## Deep learning \ud83e\udde0\nBecause of the long learning time, I manually adjusted the hyperparameters instead of optuna.<br>\n<font color=\"RoyalBlue\">\u5b66\u7fd2\u306b\u6642\u9593\u304c\u304b\u304b\u3063\u305f\u305f\u3081\u3001optuna \u3067\u306f\u306a\u304f\u624b\u52d5\u3067\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8abf\u6574\u3057\u3066\u3044\u307e\u3059\u3002<\/font>","e282f11d":"<a id=\"models\"><\/a>\n# Train models and make predictions \ud83d\udcad\nNow, let's create some models and check the performance measures. We will also optimize the models using optuna.<br>\nThe performance measure for classifiers are as follows.<br>\n<font color=\"RoyalBlue\">\u3067\u306f\u3001\u3044\u304f\u3064\u304b\u306e\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3057\u3001\u6027\u80fd\u6307\u6a19\u3092\u78ba\u8a8d\u3057\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\u307e\u305f\u3001optuna \u3092\u4f7f\u3063\u3066\u30e2\u30c7\u30eb\u306e\u6700\u9069\u5316\u3082\u884c\u3044\u307e\u3059\u3002<br>\n\u5206\u985e\u5668\u306e\u6027\u80fd\u6307\u6a19\u306b\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u3082\u306e\u304c\u3042\u308a\u307e\u3059\u3002<\/font>\n\n> Referenced from Hands-On Machine Learning with Scikit-Learn and TensorFlow (Aurelien Geron, 2017).\n* accuracy - the ratio of correct predictions\n<br>\u3000<font color=\"RoyalBlue\">\u6b63\u89e3\u7387 - \u6b63\u3057\u3044\u4e88\u6e2c\u306e\u5272\u5408<\/font>\n* confusion matrix - counting the number of times instances of class A are classified as class B\n<br>\u3000<font color=\"RoyalBlue\">\u6df7\u540c\u884c\u5217 - \u30af\u30e9\u30b9\uff21\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u304c\u30af\u30e9\u30b9\uff22\u306b\u5206\u985e\u3055\u308c\u305f\u56de\u6570\u3092\u6570\u3048\u308b<\/font>\n* precision - the accuracy of the positive predictions\n<br>\u3000<font color=\"RoyalBlue\">\u9069\u5408\u7387 - \u967d\u6027\u306e\u4e88\u6e2c\u306e\u6b63\u89e3\u7387\uff08\u967d\u6027\u3067\u3042\u308b\u3068\u4e88\u6e2c\u3057\u305f\u3046\u3061\u3001\u5f53\u305f\u3063\u3066\u3044\u305f\u7387\uff09<\/font>\n* recall (sensitivity, true positive rate: TPR) - the ratio of positive instances that are correctly detected by the classifier\n<br>\u3000<font color=\"RoyalBlue\">\u518d\u73fe\u7387\uff08\u611f\u5ea6\u3001\u771f\u967d\u6027\u7387\uff09- \u5206\u985e\u5668\u304c\u6b63\u3057\u304f\u5206\u985e\u3057\u305f\u967d\u6027\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306e\u5272\u5408\uff08\u672c\u5f53\u306b\u967d\u6027\u3067\u3042\u308b\u30b1\u30fc\u30b9\u306e\u3046\u3061\u3001\u967d\u6027\u3060\u3068\u5224\u5b9a\u3067\u304d\u305f\u7387\uff09<\/font>\n* F1 score - the harmonic mean of precision and recall\n<br>\u3000<font color=\"RoyalBlue\">F1 \u30b9\u30b3\u30a2\uff08F \u5024\uff09 - \u9069\u5408\u7387\u3068\u518d\u73fe\u7387\u306e\u8abf\u548c\u5e73\u5747\uff08\u7b97\u8853\u5e73\u5747\u306b\u6bd4\u3079\u3001\u8abf\u548c\u5e73\u5747\u306f\u4f4e\u3044\u5024\u306b\u305d\u3046\u3067\u306a\u3044\u5024\u3088\u308a\u3082\u305a\u3063\u3068\u5927\u304d\u306a\u91cd\u307f\u3092\u7f6e\u304f\uff09<\/font>\n* AUC - the area under the ROC curve (plotting the true positive rate (another name for recall) against the false positive rate)\n<br>\u3000<font color=\"RoyalBlue\">AUC - ROC \u66f2\u7dda\uff08\u507d\u967d\u6027\u7387\u306b\u5bfe\u3059\u308b\u771f\u967d\u6027\u7387\uff08\u518d\u73fe\u7387\uff09\u3092\u30d7\u30ed\u30c3\u30c8\u3057\u305f\u66f2\u7dda\uff09\u306e\u4e0b\u306e\u9762\u7a4d<\/font><br>\n\nIn this notebook, we will look at their accuracy, F1 score, and confusion matrix.<br>\n<font color=\"RoyalBlue\">\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3067\u306f\u3001\u6b63\u89e3\u7387\u3001F1 \u30b9\u30b3\u30a2\u3001\u305d\u3057\u3066\u6df7\u540c\u884c\u5217\u3092\u898b\u3066\u3044\u304d\u307e\u3059\u3002<\/font>\n\n<a id=\"gbm\"><\/a>\n## LightGBM \ud83c\udf33","d6a23acd":"<a id=\"xgb\"><\/a>\n## XGBoost \ud83c\udf33","7279a68d":"Insights:\n\n* The distribution is roughly symmetrical and almost bell-shaped, with no value exceeding 148, but there are rare cases below 125.\n<br>\u3000<font color=\"RoyalBlue\">\u307b\u3068\u3093\u3069\u5de6\u53f3\u5bfe\u79f0\u306e\u91e3\u9418\u578b\u306b\u8fd1\u3044\u5206\u5e03\u3067\u3001148\u3092\u8d85\u3048\u308b\u5024\u306f\u7121\u3044\u304c\u3001125\u672a\u6e80\u306e\u30b1\u30fc\u30b9\u306f\u7a00\u306b\u5b58\u5728\u3059\u308b\u3002<\/font>\n* By objective variable, there is some difference in the median and in the distribution. The values of survivors are clustered around the median, while the values of deaths are lower and tend to be more dispersed.\n<br>\u3000<font color=\"RoyalBlue\">\u76ee\u7684\u5909\u6570\u5225\u306b\u898b\u308b\u3068\u3001\u4e2d\u592e\u5024\u306b\u3082\u5206\u5e03\u306b\u3082\u591a\u5c11\u306e\u5dee\u304c\u3042\u308b\u3002\u751f\u5b58\u8005\u306e\u5024\u306f\u4e2d\u592e\u5024\u4ed8\u8fd1\u306b\u96c6\u307e\u3063\u3066\u3044\u308b\u304c\u3001\u6b7b\u4ea1\u8005\u306e\u5024\u306f\u3088\u308a\u4f4e\u304f\u3001\u5206\u6563\u50be\u5411\u306b\u3042\u308b\u3002<\/font>","63332c64":"Insights:\n\n* The distribution is heavily skewed to one side, with rare cases having values more than four times the median.\n<br>\u3000<font color=\"RoyalBlue\">\u7247\u5074\u306b\u88fe\u306e\u91cd\u3044\u5206\u5e03\u3068\u306a\u3063\u3066\u304a\u308a\u3001\u7a00\u306b\u4e2d\u592e\u5024\u306e\uff14\u500d\u4ee5\u4e0a\u306e\u5024\u3092\u6301\u3064\u30b1\u30fc\u30b9\u304c\u3042\u308b\u3002<\/font>\n* By objective variable, there are considerable differences in the shape of the distribution. For survivors, the values are clustered around the median, but for the dead, there are often cases where the values exceed 1.5.\n<br>\u3000<font color=\"RoyalBlue\">\u76ee\u7684\u5909\u6570\u5225\u306b\u898b\u308b\u3068\u3001\u5206\u5e03\u306e\u5f62\u306b\u304b\u306a\u308a\u306e\u5dee\u304c\u3042\u308b\u3002\u751f\u5b58\u8005\u306f\u5024\u304c\u307b\u307c\u4e2d\u592e\u5024\u4ed8\u8fd1\u306b\u96c6\u307e\u3063\u3066\u3044\u308b\u304c\u3001\u6b7b\u4ea1\u8005\u306f1.5\u3092\u8d85\u3048\u308b\u3088\u3046\u306a\u30b1\u30fc\u30b9\u304c\u3057\u3070\u3057\u3070\u3042\u308b\u3002<\/font>","f3a810be":"Insights:\n\n* The distribution is roughly symmetrical and almost bell-shaped.\n<br>\u3000<font color=\"RoyalBlue\">\u5de6\u53f3\u5bfe\u79f0\u306e\u91e3\u9418\u72b6\u306b\u8fd1\u3044\u5206\u5e03\u3092\u3068\u3063\u3066\u3044\u308b\u3002<\/font>\n* By objective variable, there is little difference in the median. Survivors have slightly higher platelet counts, and the values are clustered around the median.\n<br>\u3000<font color=\"RoyalBlue\">\u76ee\u7684\u5909\u6570\u5225\u306b\u898b\u308b\u3068\u3001\u4e2d\u592e\u5024\u306b\u307b\u3068\u3093\u3069\u5dee\u306f\u7121\u3044\u3002\u751f\u5b58\u8005\u306e\u65b9\u304c\u82e5\u5e72\u8840\u5c0f\u677f\u6570\u304c\u591a\u304f\u3001\u5024\u304c\u4e2d\u592e\u5024\u4ed8\u8fd1\u306b\u96c6\u307e\u3063\u3066\u3044\u308b\u3002<\/font>","05ac86e8":"The figure below is based on a [scatterplot from the paper](https:\/\/bmcmedinformdecismak.biomedcentral.com\/articles\/10.1186\/s12911-020-1023-5\/figures\/3)\n.","ade31dd8":"<a id=\"ert\"><\/a>\n## Extremely randomized trees \ud83c\udf33","d244190d":"Insights:\n\n* The distribution is heavily skewed to one side, with the highest value more than 30 times the median.\n<br>\u3000<font color=\"RoyalBlue\">\u7247\u5074\u306b\u88fe\u306e\u91cd\u3044\u5206\u5e03\u3068\u306a\u3063\u3066\u304a\u308a\u3001\u6700\u9ad8\u3067\u4e2d\u592e\u5024\u306e30\u500d\u4ee5\u4e0a\u306e\u5024\u3092\u6301\u3064\u30b1\u30fc\u30b9\u304c\u3042\u308b\u3002<\/font>\n* By objective variable, there is little difference in the median, although there are some differences in the distribution.\n<br>\u3000<font color=\"RoyalBlue\">\u76ee\u7684\u5909\u6570\u5225\u306b\u898b\u308b\u3068\u3001\u5206\u5e03\u306b\u591a\u5c11\u306e\u9055\u3044\u306f\u3042\u308c\u3069\u3001\u4e2d\u592e\u5024\u306b\u306f\u307b\u3068\u3093\u3069\u5dee\u304c\u7121\u3044\u3002<\/font>","11826321":"Insights:\n\n* The distribution is discrete, not continuous, with the first peak near 38 and the second peak near 60.\n<br>\u3000<font color=\"RoyalBlue\">\u9023\u7d9a\u7684\u3067\u306f\u306a\u304f\u96e2\u6563\u7684\u306a\u5206\u5e03\u3092\u3068\u3063\u3066\u3044\u308b\u300238\u4ed8\u8fd1\u306b\u7b2c\u4e00\u306e\u5c71\u304c\u300160\u4ed8\u8fd1\u306b\u7b2c\u4e8c\u306e\u5c71\u304c\u3042\u308b\u3002<\/font>\n* By objective variable, there are considerable differences in the shape of the distribution and in the median. Survivors are mostly located near the first and second mountains. The values of the dead are mostly around 30 and decrease slowly from there.\n<br>\u3000<font color=\"RoyalBlue\">\u76ee\u7684\u5909\u6570\u5225\u306b\u898b\u308b\u3068\u3001\u5206\u5e03\u306e\u5f62\u306b\u3082\u4e2d\u592e\u5024\u306b\u3082\u304b\u306a\u308a\u306e\u5dee\u304c\u3042\u308b\u3002\u751f\u5b58\u8005\u306f\u7b2c\u4e00\u306e\u5c71\u3068\u7b2c\u4e8c\u306e\u5c71\u4ed8\u8fd1\u306b\u591a\u3044\u3002\u6b7b\u4ea1\u8005\u306e\u5024\u306f30\u4ed8\u8fd1\u304c\u591a\u304f\u3001\u305d\u3053\u304b\u3089\u7de9\u3084\u304b\u306b\u6e1b\u5c11\u3057\u3066\u3044\u304f\u3002<\/font>","b02a719a":"Insights:\n\n* The distribution of the follow-up period is spread out with no large peaks, and there are small peaks around 90 and 200.\n<br>\u3000<font color=\"RoyalBlue\">\u7d4c\u904e\u89b3\u5bdf\u671f\u9593\u306e\u5206\u5e03\u306b\u306f\u5927\u304d\u306a\u5c71\u306f\u7121\u304f\u3070\u3089\u3051\u3066\u3044\u3066\u300190\u4ed8\u8fd1\u3068200\u4ed8\u8fd1\u306b\u5c0f\u3055\u306a\u5c71\u304c\u3042\u308b\u3002<\/font>\n* By objective variable, there are clear differences in the medians and distributions. Survivors have a long follow-up period and two peaks in the distribution, while the dead tend to have a short follow-up period, with a gradual decrease from a large peak around 30 days.\n<br>\u3000<font color=\"RoyalBlue\">\u76ee\u7684\u5909\u6570\u5225\u306b\u898b\u308b\u3068\u3001\u4e2d\u592e\u5024\u3084\u5206\u5e03\u306b\u660e\u78ba\u306a\u5dee\u304c\u3042\u308b\u3002\u751f\u5b58\u8005\u306f\u7d4c\u904e\u89b3\u5bdf\u671f\u9593\u304c\u9577\u304f\u5206\u5e03\u306b\uff12\u3064\u306e\u5c71\u304c\u3042\u308b\u304c\u3001\u6b7b\u4ea1\u8005\u306f\u7d4c\u904e\u89b3\u5bdf\u671f\u9593\u304c\u77ed\u3044\u50be\u5411\u306b\u3042\u308a\u300130\u65e5\u4ed8\u8fd1\u306e\u5927\u304d\u306a\u5c71\u304b\u3089\u7de9\u3084\u304b\u306b\u6e1b\u5c11\u3057\u3066\u3044\u304f\u3002<\/font>","cbcfcd41":"Insights:\n\n* The distribution of the objective variable is not 1:1, but is biased.\n<br>\u3000<font color=\"RoyalBlue\">\u76ee\u7684\u5909\u6570\u306e\u5206\u5e03\u306f\uff11\uff1a\uff11\u3067\u306f\u306a\u304f\u3001\u504f\u308a\u304c\u3042\u308b\u3002<\/font>","018cfb7e":"> This plot shows a clear distinction between alive patients and dead patients, that we highlighted by manually inserting a black straight line.\n\n<font color=\"RoyalBlue\">\u3053\u306e\u56f3\u306f\u3001\u751f\u5b58\u3057\u305f\u60a3\u8005\u3068\u6b7b\u4ea1\u3057\u305f\u60a3\u8005\u306e\u660e\u78ba\u306a\u9055\u3044\u3092\u793a\u3057\u3066\u304a\u308a\u3001\u624b\u52d5\u3067\u9ed2\u3044\u76f4\u7dda\u3092\u633f\u5165\u3057\u3066\u5f37\u8abf\u3057\u3066\u3044\u307e\u3059\u3002<\/font>","eff7530b":"Dataset from: [Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone](https:\/\/bmcmedinformdecismak.biomedcentral.com\/articles\/10.1186\/s12911-020-1023-5)\n\n* `age` - Age\n* `anaemia` - Decrease of red blood cells or hemoglobin (boolean) (0:`False`, 1:`True`)\n<br>\u3000<font color=\"RoyalBlue\">\u8ca7\u8840 - \u8d64\u8840\u7403\u307e\u305f\u306f\u30d8\u30e2\u30b0\u30ed\u30d3\u30f3\u306e\u6e1b\u5c11\u304c\u8d77\u3053\u3063\u3066\u3044\u308b\u304b<\/font>\n* `creatinine_phosphokinase` - Level of the CPK enzyme in the blood (mcg\/L)\n<br>\u3000<font color=\"RoyalBlue\">\u30af\u30ec\u30a2\u30c1\u30f3\u30d5\u30a9\u30b9\u30d5\u30a9\u30ad\u30ca\u30fc\u30bc - \u8840\u4e2d CPK \u9175\u7d20\uff08\u7b4b\u8089\u7d30\u80de\u306e\u30a8\u30cd\u30eb\u30ae\u30fc\u4ee3\u8b1d\u306b\u91cd\u8981\u306a\u5f79\u5272\u3092\u679c\u305f\u3059\u9175\u7d20\uff09\u306e\u30ec\u30d9\u30eb\uff08\u03bcg\/L\uff09<\/font>\n* `diabetes` - If the patient has diabetes (boolean) (0:`False`, 1:`True`)\n<br>\u3000<font color=\"RoyalBlue\">\u7cd6\u5c3f\u75c5 - \u60a3\u8005\u304c\u7cd6\u5c3f\u75c5\u304b\u3069\u3046\u304b<\/font>\n* `ejection_fraction` - Percentage of blood leaving the heart at each contraction (percentage)\n<br>\u3000<font color=\"RoyalBlue\">\u99c6\u51fa\u7387 - \u5fc3\u62cd\u3054\u3068\u306b\u5fc3\u81d3\u304c\u9001\u308a\u51fa\u3059\u8840\u6db2\u91cf\uff08\u99c6\u51fa\u91cf\uff09\uff0f\u5fc3\u81d3\u304c\u62e1\u5f35\u3057\u305f\u3068\u304d\u306e\u5de6\u5fc3\u5ba4\u5bb9\u91cf\uff08\uff05\uff09\n<br>\u3000\uff08\u203b\u3000\u5143\u8ad6\u6587\u3067\u91cd\u8981\u8996\uff09<\/font>\n* `high_blood_pressure` - If the patient has hypertension (boolean) (0:`False`, 1:`True`)\n<br>\u3000<font color=\"RoyalBlue\">\u9ad8\u8840\u5727 - \u60a3\u8005\u304c\u9ad8\u8840\u5727\u304b\u3069\u3046\u304b<\/font>\n* `platelets` - Platelets in the blood (kiloplatelets\/mL)\n<br>\u3000<font color=\"RoyalBlue\">\u8840\u5c0f\u677f\u6570 - \u8840\u4e2d\u306e\u8840\u5c0f\u677f\u6570\uff08\u5343\uff0fmL\uff09<\/font>\n* `serum_creatinine` - Level of serum creatinine in the blood (mg\/dL)\n<br>\u3000<font color=\"RoyalBlue\">\u8840\u6e05\u30af\u30ec\u30a2\u30c1\u30cb\u30f3\u5024 - \u8840\u4e2d\u306e\u8840\u6e05\u30af\u30ec\u30a2\u30c1\u30cb\u30f3\uff08\u814e\u81d3\u306e\u7cf8\u7403\u4f53\u304b\u3089\u6392\u6cc4\u3055\u308c\u308b\uff09\u306e\u30ec\u30d9\u30eb\uff08mg\/dL\uff09\n<br>\u3000\uff08\u203b\u3000\u5143\u8ad6\u6587\u3067\u91cd\u8981\u8996\uff09<\/font>\n* `serum_sodium` - Level of serum sodium in the blood (mEq\/L)\n<br>\u3000<font color=\"RoyalBlue\">\u8840\u6e05\u30ca\u30c8\u30ea\u30a6\u30e0\u5024 - \u8840\u4e2d\u306e\u8840\u6e05\u30ca\u30c8\u30ea\u30a6\u30e0\u5024\u306e\u30ec\u30d9\u30eb\uff08mEq\/L\uff09<\/font>\n* `sex` - Woman or man (binary) (0: Woman, 1: Man)\n* `smoking` - If the patient smokes or not (boolean) (0:`False`, 1:`True`)\n* `time` - Follow-up period (days)\n<br>\u3000<font color=\"RoyalBlue\">\u6642\u9593 - \u60a3\u8005\u306e\u7d4c\u904e\u89b3\u5bdf\u6642\u9593\uff08\u65e5\uff09<\/font>\n* `DEATH_EVENT` - If the patient deceased during the follow-up period (boolean)\n<br>\u3000<font color=\"RoyalBlue\">\u6b7b\u4ea1 - \u7d4c\u904e\u89b3\u5bdf\u671f\u9593\u4e2d\u306b\u60a3\u8005\u304c\u6b7b\u4ea1\u3057\u305f\u304b\u3069\u3046\u304b<\/font>\n\n<a id=\"explore\"><\/a>\n# Explore CSV data \ud83d\udcca","aeff5b57":"<a id=\"binary\"><\/a>\n## Distribution of the binary features","0f8a8df3":"The `train_test_split()` function is used to split the train set and the test set. You can split the dataset while keeping the ratio of `y` by specifying `y` as the argument `stratify`.<br>\n<font color=\"RoyalBlue\">train_test_split() \u95a2\u6570\u3067\u8a13\u7df4\u7528\u30bb\u30c3\u30c8\u3068\u30c6\u30b9\u30c8\u7528\u30bb\u30c3\u30c8\u3092\u5206\u5272\u3057\u307e\u3059\u3002\u3053\u306e\u3068\u304d\u3001\u5f15\u6570 stratify \u306b y \u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067\u3001y \u306e\u5272\u5408\u3092\u4fdd\u3063\u305f\u307e\u307e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u5206\u5272\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002<\/font>","1e5e7adb":"<a id=\"lm\"><\/a>\n## Linear model \ud83d\udcc8","3384fbbf":"Insights:\n\n* The age of patients was highest around 60 years old, and the number of patients decreased in a bell-shaped pattern around that age.\n<br>\u3000<font color=\"RoyalBlue\">\u60a3\u8005\u306e\u5e74\u9f62\u306f60\u6b73\u4ed8\u8fd1\u304c\u6700\u3082\u591a\u304f\u3001\u305d\u3053\u3092\u4e2d\u5fc3\u306b\u91e3\u9418\u72b6\u306b\u6e1b\u5c11\u3057\u3066\u3044\u308b\u3002<\/font>\n* There is a difference in the distribution of each objective variable, with the younger the age, the more difficult it is to die; the probability density reverses after the age of just under 70.\n<br>\u3000<font color=\"RoyalBlue\">\u76ee\u7684\u5909\u6570\u5225\u306b\u898b\u308b\u3068\u5206\u5e03\u306b\u5dee\u304c\u3042\u308a\u3001\u5e74\u9f62\u304c\u82e5\u3044\u307b\u3069\u6b7b\u4ea1\u3057\u3065\u3089\u3044\u50be\u5411\u306b\u3042\u308b\u300270\u6b73\u5f31\u3092\u5883\u306b\u78ba\u7387\u5bc6\u5ea6\u304c\u9006\u8ee2\u3059\u308b\u3002<\/font>"}}