{"cell_type":{"d5dcc419":"code","168df617":"code","5f5083f8":"code","31f869eb":"code","ea7c22b9":"code","19e08e9d":"code","996ee0fe":"code","8da635a0":"code","ea378221":"code","5a49c0ee":"code","26e468ff":"markdown","4e5ab283":"markdown","b9da868e":"markdown","2b324534":"markdown"},"source":{"d5dcc419":"import tensorflow as tf\nimport tensorflow_datasets as tfds\n\nimport os\nimport numpy as np","168df617":"DIRECTORY_URL = 'https:\/\/storage.googleapis.com\/download.tensorflow.org\/data\/illiad\/'\nFILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n\nfor name in FILE_NAMES:\n    # get_file utility downloads dataset file from download link\n    # returns path to downloaded file\n    # name = name of downloaded file, cahe_dir = where temporary datasets dir will be downloaded\n    _ = tf.keras.utils.get_file(name, origin=DIRECTORY_URL+name, cache_dir = \".\")","5f5083f8":"# Inspect txt data\nwith open(\".\/datasets\/cowper.txt\", \"r\") as f:\n    for i in range(5):\n        print(f.readline())\n        \n# Seems like they are poet","31f869eb":"parent_dir = os.path.dirname(_)\nlabeled_data_sets = []\n\ndef labeler(example, index):\n  return example, tf.cast(index, tf.int64)  \n\nfor i,file_name in enumerate(FILE_NAMES):\n    # Seems like thsi guy creates object tf.data.Dataset\n    lines_dataset = tf.data.TextLineDataset(os.path.join(parent_dir, file_name)) # What does this guy do actually?\n    # Apply tf.data.Dataset.map, do not confuse them with the python map function\n    # this is tf method, however behaviours are similar\n    # Took elements(lines) as input and returned tuples (line,i(0,1 or 2)) 2 len tuples\n    labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n    labeled_data_sets.append(labeled_dataset)","ea7c22b9":"BUFFER_SIZE = 50000\nBATCH_SIZE = 64\nTAKE_SIZE = 5000","19e08e9d":"# concatenate textline data's\nall_labeled_data = labeled_data_sets[0]\nfor labeled_dataset in labeled_data_sets[1:]:\n    all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n\n\nall_labeled_data = all_labeled_data.shuffle(\n    BUFFER_SIZE, reshuffle_each_iteration=False)\n\n# inspect some examples form dataset\nfor ex in all_labeled_data.take(5):\n    print(ex)","996ee0fe":"##### Some insights about dataset.shuffle #####\n\ndataset = tf.data.Dataset.range(10)\nprint(list(dataset.as_numpy_iterator()))\n# buffer size (=2 in this case) indicates that how many elements' position will be changed\ndataset = dataset.shuffle(2, reshuffle_each_iteration=False)\nprint(list(dataset.as_numpy_iterator()))\n\n##### Some insights about dataset.shuffle #####","8da635a0":"# build keras tokenizer instead of tensorflow deprecated tokenizer\ntokenizer = tf.keras.preprocessing.text.Tokenizer(lower=True)\n\nvocabulary_set = set()\nfor text_tensor, _ in all_labeled_data:\n    some_tokens = tokenizer.(text_tensor.numpy())\n    print(some_tokens)\n    vocabulary_set.update(some_tokens)\n\nvocab_size = len(vocabulary_set)\nvocab_size","ea378221":"! ls .\/","5a49c0ee":"print(_)","26e468ff":"# csv_file = tf.keras.utils.get_file('heart.csv', 'https:\/\/storage.googleapis.com\/applied-dl\/heart.csv', cache_dir = \".\") #dosyay\u0131 indir vve bir path olu\u015ftur\ncsv_file","4e5ab283":"### Encode dataset as index numbers","b9da868e":"### Welcome simple tf.data experiments\n* [Click for the guide I followed here](https:\/\/www.tensorflow.org\/tutorials\/load_data\/text)\n* [Also for keras tokenizer](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/text\/Tokenizer)\n \n### Insights:\n* Don't use tf.data when keras preprocessing is enough\n\n(stopped this because all tensorflow.data.Dataset's will be deprecated for text)\n\n### Instead check this notebooks with keras:\n","2b324534":"https:\/\/machinelearningmastery.com\/prepare-text-data-deep-learning-keras\/"}}