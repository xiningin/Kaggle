{"cell_type":{"2f732c22":"code","c9c3a43f":"code","502d23fb":"code","1cec9b47":"code","c4efaab6":"code","8f92b226":"code","a7724c3c":"code","a72a3d65":"code","a171a848":"code","e7565c80":"code","ff137817":"code","c3da645b":"code","9afbf62c":"code","b1a7fb94":"code","461fad10":"code","093342a2":"code","ca2e35e6":"code","5b5e9f73":"code","6d7a9468":"code","8b6fb5d1":"code","cb24c6ab":"code","d9edd80f":"code","42579eb0":"code","e53a225f":"code","6bb3d62e":"code","3275b7d2":"code","5475ec35":"code","2d7feea0":"code","a8641fa8":"code","c0e8812a":"code","ca9968a1":"code","d16cfa61":"code","6efb5551":"code","586272fa":"code","ba3ef2a9":"code","6a82d77c":"code","9a1f539f":"code","5ad5fb42":"code","85fc03ac":"code","c714049d":"code","f265167d":"code","f831fd95":"code","20555603":"code","12e8d2cf":"code","4419324e":"code","06cb60cb":"code","1b989759":"code","d0a67307":"code","328f9706":"code","520b1313":"code","e253925b":"code","3344c85c":"code","fb37f105":"code","5544cc3f":"code","472d3cc1":"markdown","00885306":"markdown","92c274b1":"markdown","955e2a0a":"markdown","6e2932b1":"markdown","a32739f4":"markdown","682b115d":"markdown","29e547ee":"markdown","f53dfcbb":"markdown","af58af85":"markdown","b63ba4e5":"markdown","eabad788":"markdown","5625ea4f":"markdown","0f03a280":"markdown","1341af19":"markdown","a9f0e3ca":"markdown","aea052c0":"markdown","f0d4aa4e":"markdown","0ccbf2f2":"markdown","965fd3f6":"markdown","a12c2a8a":"markdown","413e9e37":"markdown","dfac5dcd":"markdown","b98ff7c1":"markdown","97aaa9a7":"markdown","f416bbee":"markdown","cf6f3195":"markdown","6dea54b1":"markdown","98d38201":"markdown","6d364594":"markdown","32137110":"markdown","0044ebf1":"markdown","3ea0e8e2":"markdown","f4eb25cd":"markdown","771806b3":"markdown","94f04106":"markdown","e794bb3c":"markdown","a27d8ce5":"markdown","fcebf74d":"markdown","29da2d20":"markdown","db539398":"markdown","ed44e3e0":"markdown"},"source":{"2f732c22":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nmpl.style.use('ggplot')","c9c3a43f":"df = pd.read_csv(\"..\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv\")","502d23fb":"df.head(3)","1cec9b47":"df.info()","c4efaab6":"df.drop(['id', 'name', 'last_review'], inplace=True, axis=1)\ndf['host_name'].replace(np.nan, 'anonymous', inplace=True)\ndf['reviews_per_month'].replace(np.nan, 0, inplace=True)","8f92b226":"df.describe()","a7724c3c":"df = df.loc[df['minimum_nights'] < 500]","a72a3d65":"df.info()","a171a848":"df_count = df._get_numeric_data()\ndf_count.hist(bins = 50, figsize=(20,15))\nplt.show()","e7565c80":"fig, ax = plt.subplots(figsize=(10,7))\n\ndf.host_name.value_counts().to_frame().reset_index().iloc[:20,:].plot(kind='bar', x='index', legend=False, ax=ax)\nplt.xlabel('Names')\nplt.ylabel('Number of listings')\n\nplt.show()","ff137817":"#Further brief data observation\ndf.neighbourhood_group.value_counts()","c3da645b":"fig, ax = plt.subplots(figsize=(10,7))\n# Group and calculate all listings per each host\ndf_top_host = df.groupby('host_id').agg('count').sort_values(by=['calculated_host_listings_count'], ascending=False).reset_index()\n# Plot only the top 30 of them\ndf_top_host.iloc[:30,[df_top_host.columns.get_loc('host_id'),df_top_host.columns.get_loc('calculated_host_listings_count')]].plot(kind='bar', x='host_id', y='calculated_host_listings_count', ax=ax, legend=False)\nplt.xlabel('Host ID')\nplt.ylabel('Number of listings per host')\n\nplt.show()","9afbf62c":"df.room_type.value_counts()","b1a7fb94":"# Import GeoPandas library to operate Geo data.\nimport geopandas as gpd","461fad10":"# Let's first read two main maps we're going to work with. \n# The map with boroughs boundaties and the map with neighbourhoods boundaties\n\nbd = gpd.read_file('..\/input\/airbnbny\/Borough_Boundaries.geojson') # This is for boroughs\n\nnhd = gpd.read_file('..\/input\/airbnbny\/neighborhoods.geojson') # This is for neighbourhoods\n","093342a2":"# Explore it:\n\nbd.head(3)","ca2e35e6":"nhd.head(3)","5b5e9f73":"# !conda install --channel conda-forge descartes\nnhd.plot()\nplt.show()","6d7a9468":"# Rename columns in the geoframes to correspond with the initial dataframe.\n\nnhd.rename(columns={'ntaname' : 'neighbourhood'}, inplace=True)\nbd.rename(columns = {'boro_name' : 'neighbourhood_group'}, inplace=True)","8b6fb5d1":"# This is for the boroughs\ndf_bd = df.merge(bd, on='neighbourhood_group')\ndf_bd.drop('boro_code', axis=1, inplace=True)\n\n# This is for the neighbourhoods\ndf_nhd = df.merge(nhd, on = 'neighbourhood')\ndf_nhd.drop(['boro_name', 'boro_code', 'county_fips', 'ntacode'], axis=1, inplace=True)","cb24c6ab":"# Take a look what we've got for the neighbourhoods:\ndf_bd.head(3)","d9edd80f":"# Take a look what we've got for the boroughs:\ndf_nhd.head(3)","42579eb0":"# We need to group by boroughs and count the amount of listings for the each group:\nbc = df.groupby('neighbourhood_group', as_index=False).agg('count')\n\n# Extract only those columns we're looking for:\nbc = bc.iloc[:,:2]\n\n# Merge boroughs geodata with the count calculated dataset on the 'neighbourhood_group' column:\ngeo_bc = bd.merge(bc, on=\"neighbourhood_group\")","e53a225f":"# This is for the \u043cisual aesthetics only:\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\nfig, ax = plt.subplots(figsize=(12,10))\nax.set_aspect('equal')\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n\n# Finally, plot it:\ngeo_bc.plot(column='host_id', cmap='Wistia', legend=True, ax=ax, cax=cax)\n\ndef getXY(pt, x_adj=0, y_adj=0):\n    return (pt.x+x_adj, pt.y+y_adj)\n\n# Add annotations:\nfor index, value in enumerate(geo_bc['host_id']):\n    label = format(int(value), ',')\n    ax.annotate(label, xy=getXY(geo_bc.geometry.iloc[index].centroid,0,-0.015), ha='center', color='black')\n    ax.annotate(geo_bc.iloc[index,geo_bc.columns.get_loc('neighbourhood_group')], xy=getXY(geo_bc.geometry.iloc[index].centroid), ha='center', color='black')\n\nax.set_title(label='Number of listings per borough', fontdict=None, loc='center')\n\nplt.plot()","6bb3d62e":"# bp here stands for borough price (mean)\nbp = df.groupby(['neighbourhood_group'], as_index=False).agg('mean')\n\n# Exctract what we need:\nbp = bp.iloc[:,[0,4]]\n\n# Merge boroughs geodata with the mean calculated dataset on the 'neighbourhood_group' column:\ngeo_bp = bd.merge(bp, on='neighbourhood_group')","3275b7d2":"fig, ax = plt.subplots(figsize=(12,10))\nax.set_aspect('equal')\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n\n# Finally, plot it:\ngeo_bp.plot(column='price', cmap='summer_r', legend=True, ax=ax, cax=cax)\n\ndef getXY(pt, x_adj=0, y_adj=0):\n    return (pt.x+x_adj, pt.y+y_adj)\n\n# Add annotations:\nfor index, value in enumerate(geo_bp['price']):\n    label = format(int(value), ',')\n    ax.annotate(label, xy=getXY(geo_bc.geometry.iloc[index].centroid,0,-0.015), ha='center', color='black')\n    ax.annotate(geo_bc.iloc[index,geo_bc.columns.get_loc('neighbourhood_group')], xy=getXY(geo_bc.geometry.iloc[index].centroid), ha='center', color='black')\n\nax.set_title(label='Mean one-night-stay cost , $', fontdict=None, loc='center')\n\nplt.plot()\n","5475ec35":"# Loading the crimes dataset\ncrimes2018 = pd.read_csv('..\/input\/airbnbny\/nyc_crimes_2018.csv', parse_dates=['CMPLNT_TO_DT'],index_col = None)\ncrimes2018.drop('Unnamed: 0', inplace=True, axis=1)","2d7feea0":"crimes2018.head()","a8641fa8":"crimes2018.OFNS_DESC.value_counts()","c0e8812a":"geo_crimes2018 = gpd.GeoDataFrame(crimes2018, geometry=gpd.points_from_xy(crimes2018.Longitude, crimes2018.Latitude))\ngeo_crimes2018 = geo_crimes2018.loc[:,['geometry']]\ngeo_crimes2018.dropna(axis=0, inplace=True)\ngeo_crimes2018.reset_index()\ngeo_crimes2018.head()","ca9968a1":"from scipy import ndimage\nimport matplotlib.pylab as pylab\n\ndef heatmap(d, bins=(100,100), smoothing=1.3, cmap='jet'):\n    def getx(pt):\n        return pt.coords[0][0]\n\n    def gety(pt):\n        return pt.coords[0][1]\n\n    x = list(d.geometry.apply(getx))\n    y = list(d.geometry.apply(gety))\n\n    heatmap, xedges, yedges = np.histogram2d(y, x, bins=bins)\n    extent = [yedges[0], yedges[-1], xedges[-1], xedges[0]]\n\n    logheatmap = np.log(heatmap)\n    logheatmap[np.isneginf(logheatmap)] = 0\n    logheatmap = ndimage.filters.gaussian_filter(logheatmap, smoothing, mode='nearest')\n    \n    plt.imshow(logheatmap, cmap=cmap, extent=extent, aspect='auto')\n    plt.colorbar()\n    plt.gca().invert_yaxis()\n    plt.show()\n    \nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\npylab.rcParams['figure.figsize'] = 12, 10\n\n# nhd.plot(ax=ax)\nheatmap(geo_crimes2018, bins=250, smoothing=0.9)\n","d16cfa61":"# Transform df to geopandas format:\ngeo_df = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude))\n\n# Inner join geopandas df to nhd (neighbourhood GeoJSON dataset):\ngeo_df_j = gpd.sjoin(nhd, geo_df, how='inner', op='intersects')\n\ngeo_df_j.head(3)","6efb5551":"# Group by neighbourhood_left (from nbh) and calculate the mean price for each neighbourhood:\nnb_join_price = geo_df_j.groupby('neighbourhood_left', as_index=False).agg('mean')\n\n# Extract what we need:\nnb_join_price = nb_join_price.iloc[:,[nb_join_price.columns.get_loc('neighbourhood_left'),nb_join_price.columns.get_loc('price')]]\n\nnb_join_price.head(2)","586272fa":"# Merge  with the joined geo df\ngeo_nb_join_price = geo_df_j.merge(nb_join_price, on='neighbourhood_left')","ba3ef2a9":"fig, ax = plt.subplots(figsize=(14,12))\nax.set_aspect('equal')\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n\n# Finally, plot it:\ngeo_nb_join_price.plot(column='price_y', figsize=(10,10), cmap='RdPu', legend=True, ax=ax, cax=cax)\n\ndef getXY(pt, x_adj=0, y_adj=0):\n    return (pt.x+x_adj, pt.y+y_adj)\n\n# Add annotations:\nfor index, value in enumerate(geo_nb_join_price['price_y']):\n    label = format(int(value), ',')\n    ax.annotate(label, xy=getXY(geo_nb_join_price.geometry.iloc[index].centroid,0,0), ha='center', color='black', fontsize=8)\n\nax.set_title(label='Mean one-night-stay cost , $', fontdict=None, loc='center')\n\nplt.plot()","6a82d77c":"nb_join_price[nb_join_price['price'] > 500]","9a1f539f":"geo_nb_join_price[geo_nb_join_price['neighbourhood_left'] == 'Rossville-Woodrow'].loc[:,['neighbourhood_left','price_x']]","5ad5fb42":"# Load a Theatres layer\ntheatres = gpd.read_file('..\/input\/airbnbny\/theaters.geojson')\n\n# Load a Museums layer\nmuseums = gpd.read_file('..\/input\/airbnbny\/museums.geojson')","85fc03ac":"from mpl_toolkits.axes_grid1 import make_axes_locatable\nfig, ax = plt.subplots(figsize=(12,10))\nax.set_aspect('equal')\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n\n# Plot a base layer\nbase = geo_nb_join_price.plot(column='price_y', ax=ax, figsize=(10,10), cmap='RdPu', legend=True, cax=cax)\n\n# Plot a Theatres layer\ntheatres.plot(ax=ax, marker='o', color='red', markersize=6)\n\n# Plot a Museums layer\nmuseums.plot(ax=ax, marker='o', color='green', markersize=5)\n\n# Set title\nax.set_title(label='NY Theatres and Museums Distribution', fontdict=None, loc='center')\n\nplt.show()","c714049d":"# Group by neighbourhood_left (from the joined geo dataframe) and calculate the mean availability for each neighbourhood:\nnb_avb = geo_df_j.groupby('neighbourhood_left', as_index=False).agg('mean')\n\n# Pick up the columns we need:\nnb_avb = nb_avb.loc[:,['neighbourhood_left', 'availability_365']]\n\n# Merge  with the joined geo df\ngeo_nb_avb = geo_df_j.merge(nb_avb, on='neighbourhood_left')\n\ngeo_nb_avb.head(3)","f265167d":"from mpl_toolkits.axes_grid1 import make_axes_locatable\nfig, ax = plt.subplots(figsize=(12,10))\nax.set_aspect('equal')\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n\n# Base plot\nbase = geo_nb_avb.plot(column='availability_365_y', ax=ax, figsize=(10,10), cmap='YlOrRd_r', legend=True, alpha=0.5, cax=cax)\n\n# Set title\nax.set_title(label='Avg. room availability per year', fontdict=None, loc='center')\n\nplt.show()","f831fd95":"import seaborn as sns\n\ncorr = df_count.loc[:,~df_count.columns.isin(['latitude','longitude'])].corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"dark\"):\n    ax = sns.heatmap(corr, mask=mask, vmax=.3, square=True)","20555603":"# Let's dummify categirical variables 'neighbourhood':\ndummy_var = pd.get_dummies(df['neighbourhood'])\n\n# And join them back to the df:\ndf_num_n = pd.concat([df_count, dummy_var], axis=1)\n\n# Then dummify categirical variables 'room_type':\nroom_dummies = pd.get_dummies(df['room_type'])\n\n# And join them back to the df:\ndf_num_n = pd.concat([df_num_n, room_dummies], axis=1)\n\n# Drop host_id\ndf_num_n.drop('host_id', inplace=True, axis=1)","12e8d2cf":"# Further data exploration\nfig, ax = plt.subplots(1, 3)\n\nplt.subplot(331)\nplt.scatter(x=df_num_n.loc[:,'minimum_nights'], y=df_num_n.loc[:,['price']])\nplt.xlabel('minimum_nights')\nplt.ylabel('price')\n\nplt.subplot(332)\nplt.scatter(x=df_num_n.loc[:,['number_of_reviews']], y=df_num_n.loc[:,['price']])\nplt.xlabel('number_of_reviews')\n\nplt.subplot(333)\nplt.scatter(x=df_num_n.loc[:,['reviews_per_month']], y=df_num_n.loc[:,['price']])\nplt.xlabel('reviews_per_month')\n\nplt.show()","4419324e":"df_num_n.head(2)","06cb60cb":"df_num_n[['price']].describe()","1b989759":"bins = [0, 69, 175, 10000]\ngroup_names = ['Low', 'Medium', 'High']\ndf_num_n['price_binned'] = pd.cut(df_num_n['price'], bins, labels = group_names, include_lowest = True)","d0a67307":"# Create a copy for backup\ndf_prbn = df_num_n.copy()\n\n# Drop 'price' column\ndf_prbn.pop('price')\n\n# Set y = 'price_binned' column values and exclude it from the df\ny = df_prbn.pop('price_binned')\n\ndf_prbn.head(1)","328f9706":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(df_prbn, y, test_size=0.2, random_state=1)","520b1313":"from sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics\nrfc = RandomForestClassifier(n_estimators = 100,\n                            n_jobs = -1,\n                            max_features = \"auto\",\n                            random_state = 888,\n                            min_samples_leaf=1)\nrfc.fit(x_train,y_train)\nyhat = rfc.predict(x_test)\nprint(metrics.accuracy_score(y_test, yhat))","e253925b":"# Let's fund the best number of min_samples_leaf:\nresults = []\nfor sam in range(1,11):\n    rfc = RandomForestClassifier(n_estimators = 100,\n                            n_jobs = -1,\n                            max_features = \"auto\",\n                            random_state = 888,\n                            min_samples_leaf=sam)\n    rfc.fit(x_train,y_train)\n    yhat = rfc.predict(x_test)\n    results.append(metrics.accuracy_score(y_test, yhat))\nplt.plot(results)\nplt.xticks(np.arange(len(results)), np.arange(1,11))\nplt.show()","3344c85c":"results = []\nfor tr in range(600,2000,100):\n    rfc = RandomForestClassifier(n_estimators = tr,\n                            n_jobs = -1,\n                            max_features = \"auto\",\n                            random_state = 888,\n                            min_samples_leaf=2)\n    rfc.fit(x_train,y_train)\n    yhat = rfc.predict(x_test)\n    results.append(metrics.accuracy_score(y_test, yhat))\nplt.plot(results)\nplt.xticks(np.arange(len(results)), np.arange(600,2000,100))\nplt.show()","fb37f105":"results = []\nmf_opt=[\"auto\", None, \"sqrt\", \"log2\", 0.9, 0.2]\n\nfor max_f in mf_opt:\n    rfc = RandomForestClassifier(n_estimators = 1000,\n                            n_jobs = -1,\n                            max_features = max_f,\n                            random_state = 888,\n                            min_samples_leaf=2)\n    rfc.fit(x_train,y_train)\n    yhat = rfc.predict(x_test)\n    results.append(metrics.accuracy_score(y_test, yhat))\nplt.plot(results)\nplt.xticks(np.arange(len(results)), [\"auto\", None, \"sqrt\", \"log2\", 0.9, 0.2])\nplt.show()","5544cc3f":"rfc = RandomForestClassifier(n_estimators = 1000,\n                            n_jobs = -1,\n                            max_features = 0.2,\n                            random_state = 888,\n                            min_samples_leaf=2)\nrfc.fit(x_train,y_train)\nyhat = rfc.predict(x_test)\nprint('The accuracy of the model I managed to build is %s percent' % round(100*metrics.accuracy_score(y_test, yhat),2))","472d3cc1":"To finalize with visualization, lets plot mean availability and see which neighborhoods are the most bookable.","00885306":"From the experimentation I know, that some neighborhood names in the initial dataset and from the geo dataset are different. So, we need slightly another approach to this task.","92c274b1":"Now I'm using a function for plotting the heatmap. This function I got on:<br>\nhttps:\/\/nbviewer.jupyter.org\/gist\/perrygeo\/c426355e40037c452434","955e2a0a":"Factually, the is the most expensive neighborhood is Rossville-Woodrow. However, this is due to few observations in this area (only 4) and two lots with an extremely high price.","6e2932b1":"While we're working with this map, I want to see the mean one-neight cost for the each borough.","a32739f4":"Wow, looks like Michael is the top name among host. Interestingly, my name is Mikhail (Michael) and I was the host for a couple of years too! Now, if you know any Michael in your friends, ask him about a good price on AirBnb :)","682b115d":"However, in order to find out how safe the expensive NY boroughs are, we're going to explore the NYPD Database with all criminal reports . Also, I expect to see less criminal activity in the districts with a higher mean one-night-stay costs.   ","29e547ee":"Well, the data on the plots seems to be logically distributed. To notice something interesting, we're going to take a look on some features more in detail. ","f53dfcbb":"Great! I would not say that this is what I didn't expect to see, but this is a great demonstration of the power of visualization.","af58af85":"Here we're converting the pandas dataset into a geopandas dataset:","b63ba4e5":"Amazing, top 30 hosts live very luxury lives I guess. To my conclusion, they are whether management companies or people with an impressive amount of property in own. All in all, do they pay taxes from that?","eabad788":"Or better this way:","5625ea4f":"The optimum for 'max_features' is 0.2. Changing of other parameters didn't bring any improvement.","0f03a280":"Cool, now let's merge'em on the corresponding columns and drop unnecessary features.","1341af19":"Let's go over the next feature I want to explore - calculated_host_listings_count. What I want to know here is do people make business with AirBnb professionally or it's rather a privat activity.","a9f0e3ca":"Not bad, but not excellent. Quite a usable model.","aea052c0":"Now It became clear for me why people, who book housing through AirBnb prefer to pay so much to stay in Manhattan - all cultural life is located there. However, I see here a very useful tip for travelers - to book in Bronx, which is twice cheaper and the equally safe as neighboring Manhattan. ","f0d4aa4e":"Interestingly, but the most expensive borough Manhattan seems to be more dangerous than Bronx! <br><br>\nI'll dig deeper then, to find the reasons why people are ready to pay so much to stay in Manhattan, but first, let's visualize neighborhoods rather than boroughs and see what else we can see.<br>","0ccbf2f2":"Collinearity is not detected.","965fd3f6":"Now my aim is to visualize the density of the listings across the NY's boroughs","a12c2a8a":"What I want to conclude here is that the closer listings are located to Manhattan, the busier host are.","413e9e37":"Checking the correlation among numeric features","dfac5dcd":"Now the dataset looks complete and that means it's time to build first plots","b98ff7c1":"We need to do some data manipulation for the further analysis, so let's perform it.","97aaa9a7":"<h3>Predictive Analysis<\/h3>","f416bbee":"Well, min_samples_leaf = 2 is the optimum for this parameter. The next parameter is 'n_estimators':","cf6f3195":"<h3>Model Tuning<\/h3>","6dea54b1":"Here we see the boundaries for the price binning:","98d38201":"Awesome! This works like tipycal dataframe, but Geo!","6d364594":"<h3>Thanks for going through the notebook!<\/h3>","32137110":"Now, it's time for more advanced visualization with GeoJSON maps","0044ebf1":"Now I want to use machine learning tecniques to build a model which could predict the price for the lot. However, since we don't have a lot of descriptive features related to any particulat listing, all models I built were poorly usable. The key here was to bin the price into categories LOW, MEDIUM and HIGH and try to classify each position according this groups.","3ea0e8e2":"I want to add some extra layers and check one idea:","f4eb25cd":"The final model training:","771806b3":"<h1>Intro<\/h1><br>\n\nThis project contains some explorations on questions we could raise when working with nyc_air_bnb dataset.\nHere I used descriptive analysis and predictive at the end.<br>\nEnjoy the reading.<br>\n\n\n<b>CONTENT<\/b>\n<li>Data Exploration<\/li>\n<li>Basic Analysis<\/li>\n<li>Advanced Visualization<\/li>\n<li>Predictive Analysis. <i>SPOILER: The accuracy I managed to get is quite usable<\/i><\/li>","94f04106":"<h3>Data Exploration<\/h3><br>\nLet's load take a brief look on the dataset we have:","e794bb3c":"For further analysis, I'll cut off the outliers like those where minimum_night set to too high value. Usually, hosts make so to make a pause in their rent activity or to deal with guests directly, avoiding Airbnb's fees,  after a brief conversation in the in-platform chat.","a27d8ce5":"To summarize the plot above, if you're used to living high off the hog and want to feel the hustle and bustle of the big city, then you'd better afford to settle in Manhattan. Otherwise, you'd better stay in Queens or Staten Island (the Bronx is considered a dangerous place for newcomers) and get to the downtown by car or by bus.","fcebf74d":"Previously, I filtered the NYPD database and left only complaints related to the most severe offenses:<br><br>\n<li>ASSAULT 3 & RELATED OFFENSES<\/li>\n<li>GRAND LARCENY<\/li>\n<li>FELONY ASSAULT<\/li>\n<li>DANGEROUS DRUGS<\/li>\n<li>SEX CRIMES<\/li>\n<li>RAPE<\/li>\nwhich happened in 2018 the whole DB in more than 6.5m rows.","29da2d20":"<h3>Model Developlemt<\/h3>","db539398":"Ok, the optimum for 'n_estimators' is 1000. Now, we're checking the last parameter 'max_features':","ed44e3e0":"I see here, that there are several columns with incomplete data:<br><br>\n<li>name<\/li>We don't actually need the topics of the listings until we decide to conduct NLP (but we won't). Decsn: Drop it.\n<li>host_name<\/li>In a matter of curiosity, we'll conduct one exploration of this feature. Decsn: Retain, Replace NaN's.\n<li>last_review<\/li>If we had a column with the listing open date, we'd probably find an application for this feature. Decsn: Drop it.\n<li>reviews_per_month<\/li>We'll use this column while conducting predictive analysis. Decsn: Retain, Replace NaN's.\n<li>id<\/li>Index. Decsn: Drop it."}}