{"cell_type":{"373aff2c":"code","6ffb5432":"code","41c6eae2":"code","0ee1fc61":"code","2c1c01cd":"code","dce3b6cb":"code","f1df1780":"code","60351a25":"code","d20bb879":"code","afb65852":"code","e67db33f":"code","9773eac9":"code","7c4cadc1":"code","13633955":"code","f62ad4e0":"code","0726d1c0":"code","0cf0bb83":"code","f8e19ae9":"code","a0de78cb":"code","a8b0ffad":"code","f13264d6":"code","fca562ee":"code","670525a6":"code","b311e6e0":"code","5316c31b":"code","d9f9bc6d":"code","4a23c36e":"code","6ac4fe20":"code","0b719c20":"code","486ff231":"code","c21dda87":"code","3a9fb020":"code","f57b743c":"code","0400116c":"code","3827a426":"code","c77dbe02":"code","062de4f2":"code","1f3b84ac":"code","727bdff1":"code","f1cbc0af":"code","3cb1ce8a":"code","7fddf392":"code","98ba6d94":"code","7a27dbdd":"code","888b531f":"code","fdfd1bd0":"code","644b76cb":"code","e704a5fb":"code","668eb667":"code","9e78a05e":"code","f7d75a1c":"code","5389be97":"markdown","cd9f34e1":"markdown","448bcf0e":"markdown","13511f98":"markdown","22d70976":"markdown","46fcca40":"markdown","dc0f4692":"markdown","92564ef3":"markdown","59cf9ca7":"markdown","8f950583":"markdown","ddf7cb21":"markdown","b4421d24":"markdown","167c955a":"markdown","aebcecb7":"markdown","e61de716":"markdown","e865133c":"markdown","bede2884":"markdown","28f26aa4":"markdown","ce2cc1bb":"markdown","bea3ff59":"markdown","20a0a4c9":"markdown","b6162bc1":"markdown","91cc318e":"markdown","0126cc61":"markdown","19a32a91":"markdown","0b9d0187":"markdown","2a434782":"markdown","b6cf057b":"markdown","7a023600":"markdown","dbe598d9":"markdown","9c7c4acd":"markdown","cd955293":"markdown","9167afc7":"markdown","8ec18586":"markdown"},"source":{"373aff2c":"%%capture\n# https:\/\/github.com\/parrt\/dtreeviz\/issues\/108\n# updated versions are needed for MLJarSupervised\n! pip3 install graphviz==0.15.0\nimport graphviz\nprint(graphviz.__version__)","6ffb5432":"import os\nimport time\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport warnings\nimport logging\nfrom warnings import simplefilter\n\nwarnings.filterwarnings('ignore')\nlogging.captureWarnings(True)\nsimplefilter(action='ignore', category=FutureWarning)","41c6eae2":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntrain_data.head()","0ee1fc61":"test_data = pd.read_csv('..\/input\/titanic\/test.csv')\ntest_data.head()","2c1c01cd":"submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsubmission.head()","dce3b6cb":"sns.heatmap(train_data.isnull(), cbar=False)","f1df1780":"sns.heatmap(test_data.isnull(), cbar=False)","60351a25":"sns.swarmplot(data=train_data, x='Sex', y='Age', hue=\"Survived\")","d20bb879":"sns.swarmplot(data=train_data, x='Sex', y='Fare', hue=\"Survived\")","afb65852":"sns.swarmplot(data=train_data, x='Sex', y='Pclass', hue=\"Survived\")","e67db33f":"sns.swarmplot(data=train_data, x='Sex', y='Parch', hue=\"Survived\")","9773eac9":"sns.swarmplot(data=train_data, x='Sex', y='SibSp', hue=\"Survived\")","7c4cadc1":"from sklearn.impute import SimpleImputer\n\ndef impute_nan_values(data, column):\n    imr = SimpleImputer(missing_values=np.nan, strategy='median')\n    print(f\"Number of {column} NaN values before impute: {data[column].isnull().sum().sum()}\")\n    imr = imr.fit(data[[column]])\n    data[column] = imr.transform(data[[column]]).ravel()\n    print(f\"Number of {column} NaN values after impute: {data[column].isnull().sum().sum()}\")\n\ndef remove_nan_values(data, column):\n    print(f\"Number of {column} NaN values before impute: {data[column].isnull().sum().sum()}\")\n    _data = data[data[column].notnull()]\n    print(f\"Number of {column} NaN values after impute: {_data[column].isnull().sum().sum()}\")\n    return _data","13633955":"for column in train_data.columns:\n    print(f\"{column}: {str(sum(train_data[column].isnull()))} missing values\")\n\nimpute_nan_values(train_data, 'Age')\ntrain_data = remove_nan_values(train_data, 'Embarked')","f62ad4e0":"for column in test_data.columns:\n    print(f\"{column}: {str(sum(test_data[column].isnull()))} missing values\")\n\nimpute_nan_values(test_data, 'Age')\nimpute_nan_values(test_data, 'Fare')","0726d1c0":"\"\"\"\nUsage of 'Z-score' (z = x \u2013 \u03bc \/ \u03c3) to find outliers\n\"\"\"\ndef outliers_z_score(data):\n    outliers=[]\n    threshold = 6\n\n    mean_y = np.mean(data)\n    stdev_y = np.std(data)\n\n    for i in data:\n        z_score = (i-mean_y) \/ stdev_y\n        if np.abs(z_score) > threshold:\n            outliers.append(i)\n    return outliers","0cf0bb83":"# Age feature\nage_outliers = outliers_z_score(train_data['Age'])\nprint(f\"Age outliers: {age_outliers}\")\nfor ao in age_outliers:     \n    train_data = train_data[train_data.Age != ao]\n\n\n# Fare feature\nfare_outliers = outliers_z_score(train_data['Fare'])\nprint(f\"Fare outliers: {fare_outliers}\")\nfor fo in fare_outliers:     \n    train_data = train_data[train_data.Fare != fo]\n    \n    \n# Parch feature\nfare_outliers = outliers_z_score(train_data['Parch'])\nprint(f\"Parch outliers: {fare_outliers}\")\nfor po in fare_outliers:     \n    train_data = train_data[train_data.Parch != po]\n\n# SibSp feature\nsibsp_outliers = outliers_z_score(train_data['SibSp'])\nprint(f\"SibSp outliers: {sibsp_outliers}\")\nfor so in sibsp_outliers:     \n    train_data = train_data[train_data.SibSp != so]","f8e19ae9":"sns.swarmplot(data=train_data, x='Sex', y='Fare', hue=\"Survived\")","a0de78cb":"sns.swarmplot(data=train_data, x='Sex', y='Age', hue=\"Survived\")","a8b0ffad":"sns.swarmplot(data=train_data, x='Sex', y='Parch', hue=\"Survived\")","f13264d6":"sns.swarmplot(data=train_data, x='Sex', y='SibSp', hue=\"Survived\")","fca562ee":"# not going to use these columns to train\/test on\ntrain_data.drop(['Name', 'PassengerId', 'Cabin', 'Ticket'], inplace=True, axis=1)\ntest_data.drop(['Name', 'PassengerId', 'Cabin', 'Ticket'], inplace=True, axis=1)","670525a6":"print(train_data.dtypes)","b311e6e0":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\n\ntrain_data[\"Embarked\"] = label_encoder.fit_transform(train_data[\"Embarked\"])\ntrain_data[\"Sex\"] = label_encoder.fit_transform(train_data[\"Sex\"])\n\ntest_data[\"Embarked\"] = label_encoder.fit_transform(test_data[\"Embarked\"])\ntest_data[\"Sex\"] = label_encoder.fit_transform(test_data[\"Sex\"])","5316c31b":"print(train_data.dtypes)","d9f9bc6d":"f,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(train_data.corr(), annot=True, linewidths=1, ax=ax)","4a23c36e":"target = train_data['Survived']\ntrain_data.drop(['Survived'], inplace=True, axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(train_data, target, test_size=0.25, random_state=42, shuffle=False)\nprint(f'Sizes: X_train={X_train.shape}, y_train={y_train.shape}, X_test={X_test.shape}, y_test={y_test.shape}')\n\n# will need this later on for AutoGluon\nX_train_with_target = X_train.copy()\nX_train_with_target['Survived'] = target\n\nprint(f'Sizes: X_train_with_targer={X_train_with_target.shape}')","6ac4fe20":"sns.heatmap(X_train.isnull(), cbar=False)","0b719c20":"X_train.head()","486ff231":"best_model = None\nbest_model_name = None\nbest_model_acc = 0.0\n\nmodels = []\n\ndef validate_model(model_name, model, accuracy):\n    global best_model, best_model_name, best_model_acc, models\n    \n    models.append([model_name, accuracy])\n\n    print()\n    print(f\"Current accuracy of model {model_name}: {accuracy}\")\n    print(f\"Previous best accuracy of model {best_model_name}: {best_model_acc}\")\n\n    if accuracy > best_model_acc:\n        print(f\"Improved previous accuracy!\")\n        best_model_acc = accuracy\n        best_model = model\n        best_model_name = model_name\n    else:\n        print(f\"Did not improve previous accuracy.\")","c21dda87":"%%time\nfrom xgboost import XGBClassifier\nwarnings.filterwarnings('ignore')\nlogging.captureWarnings(True)\nsimplefilter(action='ignore', category=FutureWarning)\n\nxgboost_model = XGBClassifier(tree_method='gpu_hist')\nxgboost_model.fit(X_train, y_train)\ny_preds_xgboost = xgboost_model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_preds_xgboost)\nvalidate_model('xgboost', xgboost_model, accuracy)","3a9fb020":"%%capture\n%%bash\n# https:\/\/github.com\/automl\/auto-sklearn\/issues\/101\napt-get remove swig\napt-get install swig3.0\nln -s \/usr\/bin\/swig3.0 \/usr\/bin\/swig\npip3 install pyrfr\n# https:\/\/stackoverflow.com\/questions\/55833509\/attributeerror-type-object-callable-has-no-attribute-abc-registry\npip3 uninstall -y typing","f57b743c":"%%capture\n%%bash\n# actual installation\n# curl https:\/\/raw.githubusercontent.com\/automl\/auto-sklearn\/master\/requirements.txt | xargs -n 1 -L 1 pip3 install\npip3 install auto-sklearn","0400116c":"%%time\nimport autosklearn.classification\nwarnings.filterwarnings('ignore')\nlogging.captureWarnings(True)\nsimplefilter(action='ignore', category=FutureWarning)\n\n# set time_left_for_this_task to prevent trail getting stuck (default 3600 seconds)\nauto_sklearn_model = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task=3600, n_jobs=-1)\n\nauto_sklearn_model.fit(X_train, y_train)\ny_preds_autosklearn = auto_sklearn_model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_preds_autosklearn)\nvalidate_model('autosklearn', auto_sklearn_model, accuracy)","3827a426":"%%capture\n%%bash\nrm -rf hyperopt-sklearn\ngit clone https:\/\/github.com\/hyperopt\/hyperopt-sklearn.git\n(cd hyperopt-sklearn && pip3 install -e .)\nmv hyperopt-sklearn\/hpsklearn \/opt\/conda\/lib\/python3.7\/site-packages\/hpsklearn","c77dbe02":"! export OMP_NUM_THREADS=1\nos.environ['OMP_NUM_THREADS'] = \"1\"","062de4f2":"%%time\nfrom hpsklearn import HyperoptEstimator, any_classifier, any_preprocessing\nwarnings.filterwarnings('ignore')\nlogging.captureWarnings(True)\nsimplefilter(action='ignore', category=FutureWarning)\n\n# setting seed to avoid trail getting stuck\nnp.random.seed(42)\n\n# set max_evals to prevent too long search (default 100)\n# set trail timeout to prevent trail getting stuck (default None)\nestim = HyperoptEstimator(\n    classifier=any_classifier('my_clf'),\n    preprocessing=any_preprocessing('my_pre'),\n    n_jobs=-1,\n    max_evals=40,\n    trial_timeout=400\n)\n\nestim.fit(X_train, y_train)\ny_preds_hyperopt = estim.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_preds_hyperopt)\nvalidate_model('hyperopt', estim, accuracy)\n\nprint(estim.best_model())","1f3b84ac":"%%capture\n%%bash\npip3 install tpot","727bdff1":"%%time\nfrom tpot import TPOTClassifier\nwarnings.filterwarnings('ignore')\nlogging.captureWarnings(True)\nsimplefilter(action='ignore', category=FutureWarning)\n\n# set generations and population_size to prevent too long search (default 100 both)\ntpot_classifier = TPOTClassifier(generations=50, population_size=50, verbosity=2, n_jobs=-1)\ntpot_classifier.fit(X_train, y_train)\ny_preds_tpot = tpot_classifier.predict(X_test)\n\ntpot_classifier.export('tpot_pipeline.py')\n\naccuracy = accuracy_score(y_test, y_preds_tpot)\nvalidate_model('tpot', tpot_classifier, accuracy)","f1cbc0af":"%%capture\n%%bash\npython3 -m pip install --upgrade \"mxnet<2.0.0\"\npip3 install autogluon autogluon.tabular\n# https:\/\/github.com\/awslabs\/autogluon\/issues\/810\npip3 install --upgrade pillow","3cb1ce8a":"%%time\nfrom autogluon.tabular import TabularPrediction as task\nwarnings.filterwarnings('ignore')\nlogging.captureWarnings(True)\nsimplefilter(action='ignore', category=FutureWarning)\n\n# autogluon needs target in the training_data\npredictor = task.fit(train_data=X_train_with_target, label='Survived')\ny_preds_autogluon = predictor.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_preds_autogluon)\nvalidate_model('autogluon', predictor, accuracy)\n\nprint(predictor.leaderboard())","7fddf392":"%%capture\n%%bash\npip3 install h2o","98ba6d94":"%%time\nimport h2o\nfrom h2o.sklearn import H2OAutoMLClassifier\nwarnings.filterwarnings('ignore')\nlogging.captureWarnings(True)\nsimplefilter(action='ignore', category=FutureWarning)\n\nh2o.init()\n\n# set max_runtime_secs to prevent too long search (default 3600)\naml = H2OAutoMLClassifier(max_runtime_secs=3600)\n\naml.fit(X_train, y_train.values)\ny_preds_h2o = aml.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_preds_h2o)\nvalidate_model('H2OautoML', aml, accuracy)","7a27dbdd":"%%capture\n%%bash\n# https:\/\/github.com\/tensorflow\/tensorflow\/issues\/42441\npip3 install autokeras emcee pyDOE","888b531f":"%%time\nimport autokeras as ak\nimport tensorflow as tf\ntf.get_logger().setLevel(logging.ERROR)\nwarnings.filterwarnings('ignore')\nlogging.captureWarnings(True)\nsimplefilter(action='ignore', category=FutureWarning)\n\n# set max_trials to prevent too long search (default 100)\nclf = ak.StructuredDataClassifier(overwrite=True, max_trials=100)\nclf.fit(x=X_train, y=y_train)\ny_preds_autokeras = clf.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_preds_autokeras)\nvalidate_model('autokeras', clf, accuracy)\n\nprint(clf.export_model().summary())","fdfd1bd0":"%%capture\n%%bash\npip3 install mljar-supervised\npip3 install matplotlib==3.1.3","644b76cb":"%%time\nfrom supervised.automl import AutoML\nwarnings.filterwarnings('ignore')\nlogging.captureWarnings(True)\nsimplefilter(action='ignore', category=FutureWarning)\n\n# https:\/\/github.com\/mljar\/mljar-supervised#available-modes-books\n# set total_time_limit to prevent too long search (default 3600 seconds)\n# features_selection causes issues with xgboost on gpu\nautoml = AutoML(\n    mode=\"Compete\",\n    stack_models=True,\n    train_ensemble=True,\n    total_time_limit=3600,\n    features_selection=False\n)\n\nautoml.fit(X_train, y_train)\ny_preds_mljar = automl.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_preds_mljar)\nvalidate_model('mljar-supervised', automl, accuracy)\n\nautoml.get_leaderboard()","e704a5fb":"models_df = pd.DataFrame(models, columns=['model_name', 'accuracy'])\nmodels_df.sort_values(by=['accuracy'], ascending=False, inplace=True)\nmodels_df = models_df.reset_index(drop=True)\nmodels_df","668eb667":"print(best_model_name)\nprint(best_model)\nprint(best_model_acc)","9e78a05e":"y_preds = best_model.predict(test_data)\nsubmission['Survived'] = y_preds.ravel().astype(int)\nsubmission.to_csv('submission.csv', index = False)","f7d75a1c":"! rm -rf *\/","5389be97":"Small recap about the fiels:\n* sibsp - Number of Siblings\/Spouses Aboard\n* parch - Number of Parents\/Children Aboard\n* class - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n\nWe can several things on this corr plot:\n- the higher the fare the lower the Pclass (that indicates higher class) => negative correlation of -0.61\n- the higher the survival rate the lower the sex (that indicates female 0 \/ male 1) => negative correlation of -0.55\n- the higher the Parch the higher the SibSp (that indicates large families) => positive correlation of 0.40","cd9f34e1":"<a class=\"anchor\" id=\"HyperOptSklearn\"><\/a>\n### HyperOptSklearn","448bcf0e":"We are going to train an xgboost as a baseline.","13511f98":"### Remove Outliers","22d70976":"Check one last time for null values.","46fcca40":"<a class=\"anchor\" id=\"H2OAutoML\"><\/a>\n### H2OAutoML","dc0f4692":"TODO: incorporate [AutoPyTorch](https:\/\/github.com\/automl\/Auto-PyTorch)","92564ef3":"Without the outliers it already looks much better!","59cf9ca7":"## <center style=\"background-color: #6dc8b5; width:30%;\">Contents<\/center>\n* [Import Libraries](#Import)\n* [Load Data](#Load)\n* [Visualize Data](#Visualize)\n* [Preprocess Data](#Preprocess)\n* [Train Models](#Train)\n    1. [XGBoost](#XGBoost)\n    2. [AutoSklearn](#AutoSklearn)\n    3. [HyperOptSklearn](#HyperOptSklearn)\n    4. [TPOT](#TPOT)\n    5. [AutoGluon](#AutoGluon)\n    6. [H2OAutoML](#H2OAutoML)\n    7. [AutoKeras](#AutoKeras)\n    8. [MLJarSupervised](#MLJarSupervised)\n* [Submission File](#Submission)\n* [Cleanup](#Cleanup)","8f950583":"Below we print out the best performing AutoML model. Let's use this model to generate predictions for our final submission.","ddf7cb21":"<a class=\"anchor\" id=\"Import\"><\/a>\n# Import Libraries","b4421d24":"<a class=\"anchor\" id=\"TPOT\"><\/a>\n### TPOT","167c955a":"<a class=\"anchor\" id=\"Load\"><\/a>\n# Load Data","aebcecb7":"<a class=\"anchor\" id=\"AutoGluon\"><\/a>\n### AutoGluon","e61de716":"I had a hard time getting everything installed in one environment, there were a lot of package versions clashing because each AutoML library needed specific versions. I install each AutoML library when I need to, although this might break other installations.","e865133c":"# AutoML Comparison On Titanic Dataset","bede2884":"Secondly we are going to remove outliers","28f26aa4":"Sidenote: it was really really really difficult to get all of these AutoML algorithms to work in one notebook, I've encountered a lot of dependency issues. If you ever use AutoML, pick one to run in your notebook.","ce2cc1bb":"<a class=\"anchor\" id=\"AutoSklearn\"><\/a>\n### AutoSklearn","bea3ff59":"<a class=\"anchor\" id=\"Preprocess\"><\/a>\n# Preprocess Data","20a0a4c9":"<a class=\"anchor\" id=\"AutoKeras\"><\/a>\n### AutoKeras","b6162bc1":"Check the outliers (if there are any) we will remove.","91cc318e":"### Impute\/Remove NaN Values","0126cc61":"### Drop redundant columns","19a32a91":"First we are going to impute\/remove the NaN values.","0b9d0187":"Check the NaN values which we will later solve.","2a434782":"<a class=\"anchor\" id=\"Submission\"><\/a>\n# Submission File","b6cf057b":"<a class=\"anchor\" id=\"Train\"><\/a>\n# Train Models","7a023600":"We are going to try the following AutoML libraries and train an XGBoost as a baseline.\n\n- [TPOT](https:\/\/github.com\/EpistasisLab\/tpot)\n- [AutoGluon](https:\/\/github.com\/awslabs\/autogluon)\n- [AutoSklearn](https:\/\/github.com\/automl\/auto-sklearn)\n- [H2OAutoML](https:\/\/github.com\/h2oai\/h2o-3)\n- [AutoKeras](https:\/\/github.com\/keras-team\/autokeras)\n- [MLJarSupervised](https:\/\/github.com\/mljar\/mljar-supervised)\n- [HyperOptSklearn](https:\/\/github.com\/hyperopt\/hyperopt-sklearn)","dbe598d9":"<a class=\"anchor\" id=\"XGBoost\"><\/a>\n### XGBoost","9c7c4acd":"### Categorical To Numerical Columns","cd955293":"<a class=\"anchor\" id=\"Visualize\"><\/a>\n# Visualize Data","9167afc7":"<a class=\"anchor\" id=\"MLJarSupervised\"><\/a>\n### MLJarSupervised","8ec18586":"<a class=\"anchor\" id=\"Cleanup\"><\/a>\n# Cleanup"}}