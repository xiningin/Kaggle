{"cell_type":{"ea1823c3":"code","b3fef56c":"code","03daf95a":"code","e2ae9dd1":"code","749f5352":"code","b89a997b":"code","125f57f1":"code","5d711429":"code","d6e115b1":"code","98e17d63":"code","2254550d":"code","a7924948":"code","4b6a2739":"code","860ab52c":"code","d0fdf22a":"code","11732440":"code","6343e993":"code","9c28ebaa":"code","667a2d91":"code","3b838e48":"code","75587cb4":"code","4efad919":"code","35e79331":"code","8125a4d0":"code","24a81176":"code","a09de8df":"code","419f4752":"code","7b23caf3":"code","5f0f390a":"markdown","99e66eb2":"markdown","30faf00e":"markdown","20ac4964":"markdown","d9950004":"markdown","1de50414":"markdown","59403aaf":"markdown","25504194":"markdown","6baf8eb9":"markdown","3b0e7798":"markdown","61d41374":"markdown","020d5ae0":"markdown","0ea4e8db":"markdown"},"source":{"ea1823c3":"import numpy as np\nimport pandas as pd\nimport os\n\nimport keras\nkeras.__version__","b3fef56c":"DATA_DIRECTORY = os.path.abspath(\"..\/input\/data\/data\")\n\nTEST_SPLIT = 0.2\nVALIDATION_SPLIT = 0.2\nRANDOM_SEED=42\n\nnp.random.seed(RANDOM_SEED)","03daf95a":"def categorized_from_directory(path):\n    \"\"\"Returns a Pandas dataframe with the `category` and `path` of each image.\"\"\"\n    rows = []\n    for category in os.listdir(path):\n        category_path = os.path.join(path, category)\n        for image in os.listdir(category_path):\n            image_path = os.path.join(category_path, image)\n            rows.append({'category': category, 'path': image_path})\n    return pd.DataFrame(rows)","e2ae9dd1":"all_classes = [f\"{i}x{j}\" for i in range(7) for j in range(0, i + 1)]","749f5352":"from sklearn.model_selection import train_test_split\n\nfull_data = categorized_from_directory(DATA_DIRECTORY)\n\n# Put aside a test set for final evaluation\ntrain_data, test_data = train_test_split(\n    full_data, \n    test_size=TEST_SPLIT, \n    stratify=full_data['category'])\n\n# Further decompose training data for training and validation\ntrain_data, validation_data = train_test_split(\n    train_data, \n    test_size=VALIDATION_SPLIT \/ (1 - TEST_SPLIT),\n    stratify=train_data['category'])","b89a997b":"num_categories = len(full_data['category'].unique())\n\nassert num_categories == len(all_classes)\n\nnum_categories","125f57f1":"train_data.head()","5d711429":"len(train_data), len(validation_data), len(test_data)","d6e115b1":"train_data.groupby('category').count()","98e17d63":"test_data.groupby('category').count()","2254550d":"BATCH_SIZE = 20\nIMAGE_SIZE = (100, 100)","a7924948":"from keras.preprocessing.image import ImageDataGenerator\n\ndef flow_from_datagenerator(datagen, data, batch_size=BATCH_SIZE, shuffle=True):\n    \"\"\"Returns a generator from an ImageDataGenerator and a dataframe.\"\"\"\n    return datagen.flow_from_dataframe(\n        dataframe=data, \n        x_col=\"path\", \n        y_col=\"category\", class_mode='categorical', \n        batch_size=batch_size, \n        target_size=IMAGE_SIZE,\n        shuffle=shuffle,\n        classes=all_classes)","4b6a2739":"train_datagen = ImageDataGenerator(\n    rescale=1.0 \/ 255, \n    rotation_range=360, \n    #width_shift_range=0.1, \n    #height_shift_range=0.1,\n    brightness_range=(-0.1, 0.1),\n    #shear_range=0.1,\n    zoom_range=0.2,\n    horizontal_flip=False,\n    vertical_flip=False)\n\ntrain_generator = flow_from_datagenerator(train_datagen, train_data)\n\ntrain_steps = train_generator.n \/\/ train_generator.batch_size","860ab52c":"validation_datagen = ImageDataGenerator(rescale=1.0 \/ 255)\nvalidation_generator = flow_from_datagenerator(validation_datagen, validation_data)\n\nvalidation_steps = validation_generator.n \/\/ validation_generator.batch_size","d0fdf22a":"test_datagen = ImageDataGenerator(rescale=1.0 \/ 255)\ntest_generator = flow_from_datagenerator(test_datagen, test_data)\n\ntest_steps = test_generator.n \/\/ test_generator.batch_size","11732440":"from keras import layers\nfrom keras import models\nfrom keras import optimizers\nfrom keras import callbacks","6343e993":"model = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu',\n                        input_shape=(100, 100, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dropout(0.3))\nmodel.add(layers.Dense(256, activation='relu'))\nmodel.add(layers.Dense(num_categories, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.RMSprop(lr=1e-4),\n              metrics=['acc'])","9c28ebaa":"model.summary()","667a2d91":"EPOCHS = 1200","3b838e48":"# Overwrite best model \u2014 we don't want to accidentally fill the storage space\ncheckpoint = callbacks.ModelCheckpoint(\n    \"best_model.h5\", \n    monitor='val_acc', \n    verbose=1, \n    save_best_only=True, \n    mode='max')\n\nhistory = model.fit_generator(\n    train_generator, \n    steps_per_epoch=train_steps, \n    epochs=EPOCHS, \n    validation_data=validation_generator, \n    validation_steps=validation_steps,\n    callbacks=[checkpoint]\n    )","75587cb4":"model.save('final_model.h5')","4efad919":"import matplotlib.pyplot as plt\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()","35e79331":"plt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()","8125a4d0":"pd.DataFrame({'metric': model.metrics_names, \n              'values': model.evaluate_generator(test_generator, steps=test_steps)})","24a81176":"from keras.models import load_model\nbest_model = load_model(\"best_model.h5\")\n\npd.DataFrame({'metric': best_model.metrics_names, \n              'values': best_model.evaluate_generator(test_generator, steps=test_steps)})","a09de8df":"full_datagen = ImageDataGenerator(rescale=1.0 \/ 255)\nfull_generator = flow_from_datagenerator(full_datagen, full_data)\n\nfull_steps = full_generator.n \/\/ full_generator.batch_size\n\npd.DataFrame({'metric': best_model.metrics_names, \n              'values': best_model.evaluate_generator(full_generator, steps=full_steps)})","419f4752":"NUM_SAMPLES = 10\nsample_data = test_data.sample(NUM_SAMPLES)\n\nsample_datagen = ImageDataGenerator(rescale=1.0 \/ 255)\nsample_generator = flow_from_datagenerator(sample_datagen, sample_data, \n                                           batch_size=1, shuffle=False)\n\nsample_steps = sample_generator.n \/\/ sample_generator.batch_size","7b23caf3":"from keras.preprocessing import image\n\nsample_predictions = np.argmax(best_model.predict_generator(sample_generator, \n                                                            steps=sample_steps), axis=1)\n\nfor i in range(NUM_SAMPLES):\n    img = image.load_img(sample_data['path'].iloc[i], target_size=IMAGE_SIZE)\n    plt.figure(i)\n    plt.imshow(image.array_to_img(img))\n    true_category = sample_data['category'].iloc[i]\n    predicted_category = all_classes[sample_predictions[i]]\n    plt.title(f\"Predicted category {predicted_category} (actual {true_category})\")","5f0f390a":"### Evaluation of the best model on the test (holdout) set","99e66eb2":"The dataset contains images of all kinds of orientations taken at different distances from the tiles.  There is also a great deal of variation to brightness, background and such.\n\nTo deal with that \u2014 and to avoid overfitting \u2014\u00a0perform a series of augmentations to the training data. (I have ignored flips since I belive the images are only rotated and some tiles are only rotationally invariant, eg., the '\ud83c\udc42' tile.)","30faf00e":"Let's take a look at some samples from the validation data and see what the model makes of them.","20ac4964":"### Set up data generators for Keras","d9950004":"After about 1000 epochs on my MBPr the best model had an accuracy of ~94% on the test set\nPutting some actual thought into the architecture and training can probably improve on this a great deal","1de50414":"Train and save the model as soon as it shows improvement (unfortunately there's currently no way of loading a previously trained model from an older version of the kernel yet).","59403aaf":"# CNN for Domino Tile Recognition\n\nI wanted to try out [PlaidML](https:\/\/github.com\/plaidml\/plaidml) on my MacBook Pro (late-2016, the least powerful version with the 2Gb AMD Radeon Pro 450) and found this fun dataset.\n\nNot much thought has been put into the model, but some tweaking might give acceptable results.  Training the model on my laptop I got an accuracy of about 94% on the test set after training for ~1000 epochs without tweaking anything.  (The speed is pretty decent \u2014 takes only about 30% longer to train on the MBPr compared to the kernel.)\n\nI have borrowed pretty heavily from Fran\u00e7ois Chollet's book _Deep Learning with Python_.","25504194":"A fairly random model \u2014\u00a0no particular thought has gone into the architecture:","6baf8eb9":"## Another look at the classification (visually)","3b0e7798":"## Model","61d41374":"## Splitting data for training, validation, and testing\n\nWe'll try to make sure there is roughly an equal number of images for each kind of domino tile.","020d5ae0":"## Evaluation of the model's performance on the test set\n\n### Evaluation of the final model on the test (holdout) set","0ea4e8db":"### Evaluation of the best model on the full set"}}