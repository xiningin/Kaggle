{"cell_type":{"2233988f":"code","afe32c0c":"code","ea4c1887":"code","b374d3e1":"code","f4cbbceb":"code","3b0c6daa":"code","b9d707b6":"code","46b498ce":"code","4ccf0343":"code","641ffe1a":"code","2061bb40":"code","e75be733":"code","c72590c5":"code","b86f865f":"code","a087e688":"code","e7c3808b":"code","7617b701":"code","92ced44b":"code","d0622cc9":"code","a04ee043":"markdown","ffe51828":"markdown","155bf86c":"markdown","9bb2f00f":"markdown","4f6b5b12":"markdown","66054a12":"markdown","9e9479fa":"markdown"},"source":{"2233988f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","afe32c0c":"# Author - Gowtham Ch\n# https:\/\/www.linkedin.com\/in\/gauthamchowta\/","ea4c1887":"import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OrdinalEncoder,LabelBinarizer","b374d3e1":"weather = ['Clear', 'Clear', 'Clear', 'Clear', 'Clear', 'Clear',\n            'Rainy', 'Rainy', 'Rainy', 'Rainy', 'Rainy', 'Rainy',\n            'Snowy', 'Snowy', 'Snowy', 'Snowy', 'Snowy', 'Snowy']\n\ntimeOfWeek = ['Workday', 'Workday', 'Workday',\n            'Weekend', 'Weekend', 'Weekend',\n            'Workday', 'Workday', 'Workday',\n            'Weekend', 'Weekend', 'Weekend',\n            'Workday', 'Workday', 'Workday',\n            'Weekend', 'Weekend', 'Weekend']\n\ntimeOfDay = ['Morning', 'Lunch', 'Evening',\n            'Morning', 'Lunch', 'Evening',\n            'Morning', 'Lunch', 'Evening',\n            'Morning', 'Lunch', 'Evening',\n            'Morning', 'Lunch', 'Evening',\n            'Morning', 'Lunch', 'Evening',\n            ]\ntrafficJam = ['Yes', 'No', 'Yes',\n            'No', 'No', 'No',\n            'Yes', 'Yes', 'Yes',\n            'No', 'No', 'No',\n            'Yes', 'Yes', 'Yes',\n            'Yes', 'No', 'Yes'\n            ]","f4cbbceb":"df = pd.DataFrame(zip(weather,timeOfWeek,timeOfDay,trafficJam),columns = ['weather','timeOfWeek','timeOfDay','trafficJam'])\ndf","3b0c6daa":"weather = df['weather'].values.reshape(-1,1)\ntimeOfWeek = df['timeOfWeek'].values.reshape(-1,1) \ntimeOfDay = df['timeOfDay'].values.reshape(-1,1)","b9d707b6":"weather.shape,timeOfWeek.shape","46b498ce":"def preprocess():\n    # Using ordinal encoder to convert the categories in the range from 0 to n-1\n    wea_enc = OrdinalEncoder()\n    weather_ = wea_enc.fit_transform(weather)\n\n    timeOfWeek_enc = OrdinalEncoder()\n    timeOfWeek_ = timeOfWeek_enc.fit_transform(timeOfWeek)\n\n    timeOfDay_enc = OrdinalEncoder()\n    timeOfDay_ = timeOfDay_enc.fit_transform(timeOfDay)\n    # Stacking all the features\n    X = np.column_stack((weather_,timeOfWeek_,timeOfDay_))\n    # Changing the type to int\n    X = X.astype(int)\n    # Doing one hot encoding on the target data\n    y = df['trafficJam']\n    lb = LabelBinarizer()\n    y_ = lb.fit_transform(y)\n    if y_.shape[1] == 1:\n        y_ = np.concatenate((1 - y_, y_), axis=1)\n    return X,y_,lb.classes_","4ccf0343":"X,y,classes = preprocess()\nX.shape, y.shape","641ffe1a":"def counts_based_onclass(X,y):\n    \n    # No of feature\n    n_features = X.shape[1]\n    # No of classes\n    n_classes = y.shape[1]\n    \n    count_matrix = []\n    # For each feature\n    for i in range(n_features):\n        count_feature = []\n        # Get that particuar feature from the dataset\n        X_feature = X[:,i]\n        # For each class\n        for j in range(n_classes):\n            # Get the datapoints that belong to the class - j\n            mask = y[:,j].astype(bool)\n            # Using masking filter out the datapoints that belong to this class- j in the given feature - i\n            # Using bincount -- count all the different categories present in the given feature\n            counts = np.bincount(X_feature[mask])\n            \n            count_feature.append(counts)\n            \n        count_matrix.append(np.array(count_feature))\n        # Finding the count of datapoints beloging to each class -- we will use it to calculate prior probabilities.\n        class_count = y.sum(axis=0)\n        \n    return count_matrix,n_features,n_classes,class_count\n            \n            \n    ","2061bb40":"count_matrix,n_features,n_classes,class_count = counts_based_onclass(X,y)","e75be733":"count_matrix","c72590c5":"def calculate_likelihood_probs(count_matrix,alpha,n_features):\n    log_probabilities = []\n    for i in range(n_features):\n        num = count_matrix[i] + alpha\n        den = num.sum(axis = 1).reshape(-1,1)\n        log_probability = np.log(num) - np.log(den)\n        log_probabilities.append(log_probability)\n    return log_probabilities\n        ","b86f865f":"def calculate_prior_probs(class_count):\n    \n    num = class_count\n    den = class_count.sum()\n    \n    return np.log(num)-np.log(den)\n    ","a087e688":"prior_probs = calculate_prior_probs(class_count)","e7c3808b":"log_probs = calculate_likelihood_probs(count_matrix,1,n_features)","7617b701":"log_probs","92ced44b":"def predict(query_point,log_probs,prior_probs):\n    \n    # Intializing an empty array\n    probs = np.zeros((1,n_classes))\n    # For each feature\n    for i in range(n_features):\n        # Get the category_id of the feature - i from the query_point\n        category = query_point[i]\n        # Fetch the corresponding log_probability table and add continue to add them for all the features\n        probs+=log_probs[i][:,category]\n    # Finally add posterior probability\n    probs+=prior_probs\n    # Finding the maximum of the probabilities and fetching the corresponding class\n    return classes[np.argmax(probs)]\n    ","d0622cc9":"from sklearn.naive_bayes import CategoricalNB\nclf = CategoricalNB()\nclf.fit(X, df['trafficJam'])\nprint('Sklearn feature log-probabilities\\n',clf.feature_log_prob_)\nprint('Manually implemented likelihood probabilities\\n',log_probs)\n\nprint('Sklearn feature prior-probabilities\\n',clf.class_log_prior_)\nprint('Manually implemented prior probabilities\\n',prior_probs)\n\nprint()\nprint('Sklearn predict',clf.predict(X[4:5]))\nprint('Manual predict',predict(X[4],log_probs,prior_probs))","a04ee043":"Preprocessing the data:  \nConverting the categorical data into a numerical form using ordinal encoding. The features are converted to ordinal integers.  \nThis results in a single column of integers (0 to n_categories \u2014 1) per feature.","ffe51828":"2. Calculate the counts\/presence of each feature based on class.\nBelow is an example of what we will try to achieve for each and every feature.\n![](https:\/\/miro.medium.com\/max\/1050\/1*d4QGBmtWM-nnF5saxGLNTA.jpeg)\n\nOur main aim in this part is to generate this count matrix for each and every feature.\n* For each feature \u2014 Extract that column from the data \u2014 X_feature\n* For a class \u2192 Identify where the class will be zero and where it will be one and convert it into a boolean.\n* Now mask your input feature and do simply counting.","155bf86c":"3. Calculating Likelihood probabilities:\n\n * For each feature, we are adding alpha(Laplace smoothing) if provided.\n * Doing row-wise sum, as discussed in the image above.\n * Finding log-probabilities \u2014 log(num\/den) \u2192 log(num)-log(den).","9bb2f00f":"**BINGO!!**","4f6b5b12":"5. Calculating Posterior probability:\nGet the corresponding log_probs from each feature.\nAdd these probabilities with the prior probability to get our final posterior probability.\n![](https:\/\/miro.medium.com\/max\/1050\/1*PrWFtad8j5S1SFaUc0w1og.jpeg)","66054a12":"Count_matrix will give an output this way, For each of the features you have 2D -array(The first row corresponding to No and the second row corresponding to Yes).","9e9479fa":"Steps:\n1. Preprocessing the data.\n2. Calculate the counts\/presence of each feature based on class.\n3. Calculate likelihood probability.\n4. Calculate prior probability.\n5. Calculate posterior probability for a given query point \u2192 Predict function"}}