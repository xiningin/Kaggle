{"cell_type":{"576bc5d9":"code","bb45370c":"code","1a22f050":"code","f9fd0454":"code","4056ef0d":"code","acfa6167":"code","87b6e8ec":"code","2985a813":"code","6043f985":"code","0d35edfa":"code","365b2bee":"code","a5cdb955":"code","67a88eb0":"code","e13e2513":"code","dfdd25b7":"code","b0b9d4b4":"code","0d6e6ddb":"code","286bf897":"code","265c14ef":"code","91bced43":"code","8239163c":"code","04852379":"code","01d56275":"code","bacd3e4e":"code","98038ff6":"code","52d01eee":"code","aa43464a":"code","833dd6cb":"code","ab0394da":"code","c426a998":"code","2d031921":"code","1f209fab":"code","deb034c2":"code","5cad30e6":"code","a7b0fdce":"code","f6913b0c":"code","444d0eeb":"code","a9abae05":"code","20d411f2":"code","8806be2b":"code","b5ff5525":"code","97dea359":"code","31c390e2":"code","8a865c78":"code","9dc14974":"code","ac285811":"code","11881083":"code","e1da6f9d":"markdown","5a81b2be":"markdown","abc963e3":"markdown","c9de1dee":"markdown","2a3d3337":"markdown","0e835ba6":"markdown","a72e6fac":"markdown","9bca4eba":"markdown","fc12bb19":"markdown","7477f1bc":"markdown","ab55447b":"markdown","9786a259":"markdown","7ecfb99e":"markdown","4325d936":"markdown","0d9e3b14":"markdown","36c92b69":"markdown","5359813a":"markdown"},"source":{"576bc5d9":"import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.datasets import load_files\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer","bb45370c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1a22f050":"zomato = pd.read_csv(\"\/kaggle\/input\/zomato-bangalore-restaurants\/zomato.csv\")\nzomato.head()","f9fd0454":"zomato.info()","4056ef0d":"# Percentage of Nul Values\npd.DataFrame(round(zomato.isnull().sum()\/zomato.shape[0] * 100,3), columns = [\"Nan\"])  ","acfa6167":"zomato=zomato.drop(['url','dish_liked','phone'],axis=1)","87b6e8ec":"zomato.columns","2985a813":"zomato= zomato.rename(columns={\"approx_cost(for two people)\" : \"Cost_of_two\",\n                                \"reviews_list\": \"reviews\",\n                                 \"menu_item\"  : \"menu\",\n                                 \"listed_in(type)\": \"type\",\n                                 \"listed_in(city)\": \"city\"})","6043f985":"zomato.head()","0d35edfa":"zomato['rate']=zomato['rate'].str.split('\/').str[0]","365b2bee":"sd= zomato.copy(deep=True)\nsd= zomato[zomato['rate']=='NEW']\nfd= zomato[zomato['rate'].isna()]\nsfd= pd.concat([sd,fd])\nsfd= sfd[sfd['reviews']!=\" \"] ## Making sure that the review is not empty","a5cdb955":"import nltk\nnltk.download(\"stopwords\")\n\nstopwords= nltk.corpus.stopwords.words('english')\nstopwords.remove('not')\nstemmer= nltk.stem.PorterStemmer()\nimport re\n\ndef clean_data(doc):\n    doc= doc.lower()\n    doc= re.sub(r\"\\W\",\" \",doc)\n    doc= re.sub(r\"\\d\",\" \",doc)\n    doc= re.sub(r\"\\s+\",\" \",doc)\n    doc=re.sub(\"[^a-z\\s]\",\"\",doc)\n    words= doc.split(\" \")\n    doc= re.sub(r\" [@#$%\\&\\*\\(\\)\\<\\>\\?\\'\\\":;\\]\\[-] \", \" \",doc)\n    word_imp= [ stemmer.stem(word) for word in words if word not in stopwords]\n    doc_cleaned=\" \".join(word_imp)\n    return doc_cleaned\nsid= SentimentIntensityAnalyzer()\nsfd[\"reviews\"]=sfd[\"reviews\"].apply(clean_data)","67a88eb0":"sfd['scores'] = sfd['reviews'].apply(lambda reviews: sid.polarity_scores(reviews))\n\nsfd['compound']  = sfd['scores'].apply(lambda score_dict: score_dict['compound'])\n\nsfd['neu']  = sfd['scores'].apply(lambda score_dict: score_dict['neu'])\n\nsfd['comp_score'] = sfd['compound'].apply(lambda c: 'pos' if c >=0.2 else 'neg')\n\nsfd.head()","e13e2513":"sfd[\"comp_score\"].value_counts()","dfdd25b7":"def comp(comp_score):\n    if comp_score =='pos':\n        return 4\n    else:\n        return 3\nsfd['new_rate']=sfd['comp_score'].apply(comp)","b0b9d4b4":"nsd= sfd.drop(['scores','compound','neu','rate','comp_score'],1)\nnsd=nsd.rename(columns={\"new_rate\":\"rate\"})\nnzomato= zomato.loc[(zomato['rate']!='NEW')]\nnzomato =nzomato.loc[zomato['rate'].notna()]\nframe=[nzomato,nsd]\nfinal=pd.concat(frame)","0d6e6ddb":"final.head()","286bf897":"final= final[final['rate']!='-']\nfinal['Cost_of_two']=final['Cost_of_two'].replace(\",\",\"\",regex=True).astype(float)","265c14ef":"final['Cost_of_two'].plot(kind='hist',figsize=(8,8))","91bced43":"### Adjustments on the data types\nfinal['rate']=final['rate'].astype(\"float\")\n# round(final['Cost_of_two'].mean()) ---- > 586\nfinal['Cost_of_two']=final['Cost_of_two'].fillna('586')\nfinal['rest_type']= final['rest_type'].fillna('Quick Bites')  # Filled with mode(maximum occurance)","8239163c":"menu= final[final['cuisines'].isna()]\n# There are around 10 restrants without the cuisines type we can fill after analysing the menu enterily","04852379":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","01d56275":"WC= menu[menu['menu'].map(lambda d: len(d)) > 5]\n# Start with one menu:\n# Create and generate a word cloud image:\nfor i in range(0,4):\n    text= WC.menu.iloc[i]\n    wordcloud = WordCloud().generate(text)\n\n# Display the generated image:\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.title(WC['name'].iloc[i])\n    plt.show()","bacd3e4e":"final.loc[(pd.isnull(final.cuisines)), 'cuisines'] = \"Briyani\"\nfinal.loc[(final.name==\"Lassi Spot\"),\"cuisines\"]= \"Desserts, Ice Cream \"","98038ff6":"final.isnull().sum()","52d01eee":"# Top 20 shops in the zomato list\nfinal['name'].value_counts()[:20].plot(kind='bar')","aa43464a":"# Bangalore a IT hub everone needs a quick bites (reallly sad)\nfinal['rest_type'].value_counts()[:20].plot(kind='bar')","833dd6cb":"plt.figure(figsize=(14,4))\nplt.grid()\nsns.distplot(final['Cost_of_two'])\n#final['Cost_of_two'].value_counts().sort_values(ascending=False).plot(kind='bar')\nplt.show()","ab0394da":"print(final[final['name'].str.lower().str.contains(\"briyani\")].shape)\nBriyani= final[final['name'].str.lower().str.contains(\"briyani\")]\nBriyani.shape","c426a998":"Briyani.groupby('name')['location'].count().plot(figsize=(8,8),kind='pie')","2d031921":"Briyani.groupby('location')['location'].count().plot(figsize=(8,8),kind='pie')","1f209fab":"Briyani.groupby('location')['Cost_of_two'].mean().plot(figsize=(12,8),kind='line')\n# Briyani is costly in IndiraNagar and lesser in JP Nagar,Peenya","deb034c2":"Briyani.groupby('online_order')['location'].count()","5cad30e6":"# Getting dummies is not going to work here as we have columns with address,name,menu\n# Using factorize with reference to one of the kaggle kernels\n\ndef Encode(zomato):\n    for column in zomato.columns[~zomato.columns.isin(['rate', 'Cost_of_two', 'votes'])]:\n        zomato[column] = zomato[column].factorize()[0]\n    return zomato\n\nzomato_en = Encode(final.copy())","a7b0fdce":"zomato_en","f6913b0c":"zomato_en['Cost_of_two']=zomato_en['Cost_of_two'].astype(int)","444d0eeb":"X= zomato_en.drop('Cost_of_two',axis=1)\ny= zomato_en['Cost_of_two']","a9abae05":"import scipy.stats as st\nimport statsmodels.api         as     sm","20d411f2":"#Adding constant column of ones, mandatory for sm.OLS model\nX_1 = sm.add_constant(X)\n#Fitting sm.OLS model\nmodel = sm.OLS(y,X_1).fit()\nmodel.pvalues\n#model.summary()","8806be2b":"#Backward Elimination\ncols = list(X.columns)\npmax = 1\nwhile (len(cols)>0):\n    p= []\n    X_1 = X[cols]\n    X_1 = sm.add_constant(X_1)\n    model = sm.OLS(y,X_1).fit()\n    p = pd.Series(model.pvalues.values[1:],index = cols)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features_BE = cols\nprint(selected_features_BE)","b5ff5525":"X= X[selected_features_BE]","97dea359":"from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor,AdaBoostRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score,accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport xgboost","31c390e2":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)","8a865c78":"RFR=RandomForestRegressor()\nRFR.fit(X_train,y_train)\n\nGB= GradientBoostingRegressor()\nGB.fit(X_train,y_train)\n\nAda= AdaBoostRegressor()\nAda.fit(X_train,y_train)\n\nXG= xgboost.XGBRegressor()\nXG.fit(X_train,y_train)","9dc14974":"RFR_train_pred=RFR.predict(X_train)\nGB_train_predict=GB.predict(X_train)\nAda_train_predict=Ada.predict(X_train)\nXG_train_predict=XG.predict(X_train)","ac285811":"RFR_test_pred=RFR.predict(X_test)\nGB_test_predict=GB.predict(X_test)\nAda_test_predict=Ada.predict(X_test)\nXG_test_predict=XG.predict(X_test)","11881083":"## r2_score\n\nr2_score_train=[r2_score(y_train,RFR_train_pred),r2_score(y_train,GB_train_predict),r2_score(y_train,Ada_train_predict),r2_score(y_train,XG_train_predict)]\nr2_score_test =[r2_score(y_test,RFR_test_pred),r2_score(y_test,GB_test_predict),r2_score(y_test,Ada_test_predict),r2_score(y_test,XG_test_predict)]\na=pd.DataFrame({'r2_score_train':r2_score_train,'r2_score_test':r2_score_test})\nplt.figure(figsize=[10,5]) \na.plot(kind='bar')\nplt.xticks(np.arange(4),['RFR','GB','ada','XG'])\nplt.show()","e1da6f9d":"### Changes with respect to the rate and the cost","5a81b2be":"Most of them are spending on zomato food within the price range of 200-1000","abc963e3":"### All the missing datas are imputed and ready for Prediction of rate","c9de1dee":"- Assumption NEW starts lets be good as they are trying new\n- Compound_Score with score greater than 0.2 will be given positive with a rating as 4 \n- Lesser than 0.2 is given negative with a rating as 3","2a3d3337":"Inference:\n\n- There is around 35 briyani centers and top 5 unique briyani shops spreaded across\n- Ambur star briyani is one with the more count compared to other briyani centers\n- While karnatka briyani point is unique to a particular location\n\n- BTM has a highest briyani foods(Briyani lovers it the place to chill)\n- Price range of briyani varies from 200-500\n- Most of them with online orders","0e835ba6":"### Beginners Code open for suggestions.. Thanks","a72e6fac":"## Overall RandomForest and XGboost are performing best in the prediction","9bca4eba":"### Filling the NEW shops and null values with the rating based on the reviews","fc12bb19":"### Fixing Cost_of_Two","7477f1bc":"### Dropping the columns \n\n- Ideas to fill the null values\n- Lets drop dish_liked as it contains more than 50% null values\n- Drop the phone number as we cannot manipulate them\n- approx_cost-- We can try to cluster it according to its neighbors price or using the mean values\n- Rate - We try to fill after analysing the reveiws_list","ab55447b":"### Briyani Shops\n","9786a259":"### Cuisines","7ecfb99e":"### Using backward (wrapper method) to select the feature","4325d936":"## Problem Statement\n\nEstimate\n1.  Type of restrants,dishes,cuisines,cost for dinning preffered\n2.  Location famous for online orders and book_table(what rest_type)\n3.  Places with common menus and unique dishes.\n4.  Use the voting,rate to estimate the best restrant in bangalores\n5.  Sentimental analysis (review list)\n6.  Estimating the price with the help of model prediction","0d9e3b14":"### Univariate Analysis ","36c92b69":"### Altering the column Names as required","5359813a":"### Data is clean for futher processing"}}