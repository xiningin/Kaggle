{"cell_type":{"883ada90":"code","1466bcb7":"code","51d8a434":"code","42aaed4d":"code","274849e4":"code","7791a3cb":"code","d70a309c":"code","2f9cbaf8":"code","86b9ade0":"code","78fa7a82":"code","8c26b852":"code","aff6e574":"code","4f562457":"code","3c925df7":"code","b1b31131":"code","19854531":"code","390ede70":"code","66ad1ed7":"code","2b2bcff6":"code","b7358da4":"code","9f3e9c43":"code","3971c0a2":"code","cd3492b8":"code","abd1c0e9":"code","33af2b60":"code","1421031d":"code","7a8c9a4f":"code","c450b556":"code","53eafd35":"code","e21a103f":"code","5a11427b":"code","20d583e8":"code","66d867aa":"code","e0aeecfc":"code","781446af":"code","4fc5c320":"code","42e0d49b":"code","fe2fe480":"code","68170166":"code","0128842d":"code","bddf8da5":"code","90c91cee":"code","0d8dd9d6":"code","9434daa9":"code","c50451d1":"code","9af2574b":"code","de8634ad":"code","b681310a":"code","badfa0e1":"code","f15e0eb8":"code","01db3bb9":"code","df8a898e":"code","1142c867":"code","a832fd38":"markdown","f79bcacc":"markdown","13c76a36":"markdown","8580b3fa":"markdown","1abb1bbd":"markdown","0a96d7cd":"markdown","6a11a17f":"markdown","ab545f71":"markdown","60f22333":"markdown","44337d1e":"markdown","f592e9fb":"markdown","50c8622e":"markdown","5bf44fb7":"markdown","fb336e7d":"markdown","cbf19e0d":"markdown","1d625aa9":"markdown","8822a483":"markdown","5779338d":"markdown","dac68153":"markdown","a1a61905":"markdown","a11db03a":"markdown","9f1a32cb":"markdown","8b5d6d61":"markdown","c9f0131f":"markdown","0fa3ec0b":"markdown","c4e8f21a":"markdown","39b8e774":"markdown","bc9a90b7":"markdown"},"source":{"883ada90":"# Essentials\nimport numpy as np\nimport pandas as pd\nimport datetime\nimport random\n\n# show all columns\npd.set_option('display.max_columns', None)\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows = 8000\n\n#nlp processing\nimport re, string\nimport spacy\nfrom spacy.matcher import PhraseMatcher\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows = 8000\n\n#pre-processiong\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\n# Metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import f1_score\n\n# Model selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom mlxtend.classifier import StackingCVClassifier\n\n\n","1466bcb7":"# load data and merge the 'app_desc' column intp the AppleStore dataframe\nwhole_data=pd.read_csv('..\/input\/app-store-apple-data-set-10k-apps\/AppleStore.csv',index_col=0)\ndesc_data=pd.read_csv('..\/input\/app-store-apple-data-set-10k-apps\/appleStore_description.csv')\ndesc_data.drop(['track_name','size_bytes'],axis=1,inplace=True)\nwhole_data=whole_data.merge(desc_data,how='inner',on='id',).reset_index(drop=True)\nid_keep=whole_data.pop('id')","51d8a434":"whole_data.info()","42aaed4d":"sns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.histplot(whole_data['user_rating'], color=\"b\",kde=True);\nax.xaxis.grid(False) # False: remove grid along y-axis\nax.set(ylabel=\"Count\")\nax.set(xlabel=\"Average Rating\")\nax.set(title=\"Rating distribution\")\nsns.despine(left=True) # Do not show the left axis when left=True\nplt.show()","274849e4":"# Skewness and kurtosis\nprint(\"Skewness: %f\" % whole_data['user_rating'].skew()) #normal distribution is expected to have 0 as skewness\nprint(\"Kurtosis: %f\" % whole_data['user_rating'].kurt()) #normal distribution is expected to have 3 as kurtosis","7791a3cb":"whole_data.head()","d70a309c":"paid_data=whole_data.loc[whole_data['price']>0,:].sort_values(by=['price'],ascending=False).reset_index(drop=True)\nfree_data=whole_data.loc[whole_data['price']==0,:].reset_index(drop=True)\nprint(\"There are {} paid apps, taking the propotion of {}%.\". format(paid_data.shape[0],round((paid_data.shape[0]\/whole_data.shape[0])*100,2)))\nprint(\"There are {} free apps, taking the propotion of {}%.\". format(free_data.shape[0],round((free_data.shape[0]\/whole_data.shape[0])*100,2)))","2f9cbaf8":"# Draw two histgrams to show differences of rating between paid and free apps.\nfig, axes = plt.subplots(1,2,figsize=(20, 8))\nfor data,ax in zip([paid_data,free_data],axes):\n    sns.histplot(data['user_rating'], color=\"b\",kde=True,ax=ax)\n    ax.set_xlabel(\"average user rating\")\naxes[0].set_title(\"average user rating for paid apps\")\naxes[1].set_title(\"average user rating for free apps\")","86b9ade0":"# Calculate the ratio of each section of ratings for paid and free apps (e.g. the proportion of rating 0-1 in paid and free apps)\ndef generate_rating_dum(dfs):\n    for data in dfs:\n        data['rating01']=data['user_rating'].apply(lambda x: 1 if x>=0 and x<=1 else 0)\n        data['rating12']=data['user_rating'].apply(lambda x: 1 if x>1 and x<=2 else 0)\n        data['rating23']=data['user_rating'].apply(lambda x: 1 if x>2 and x<=3 else 0)\n        data['rating34']=data['user_rating'].apply(lambda x: 1 if x>3 and x<=4 else 0)\n        data['rating45']=data['user_rating'].apply(lambda x: 1 if x>4 and x<=5 else 0)\n    \ngenerate_rating_dum(dfs=[paid_data,free_data])\nprint('#'*50)\nprint('The ratio of ratings 0-1 for paid apps is {}%'.format(round((paid_data.rating01.sum()\/paid_data.shape[0])*100,2)))\nprint('The ratio of ratings 1-2 for paid apps is {}%'.format(round((paid_data.rating12.sum()\/paid_data.shape[0])*100,2)))\nprint('The ratio of ratings 2-3 for paid apps is {}%'.format(round((paid_data.rating23.sum()\/paid_data.shape[0])*100,2)))\nprint('The ratio of ratings 3-4 for paid apps is {}%'.format(round((paid_data.rating34.sum()\/paid_data.shape[0])*100,2)))\nprint('The ratio of ratings 4-5 for paid apps is {}%'.format(round((paid_data.rating45.sum()\/paid_data.shape[0])*100,2)))\nprint('#'*50)\nprint('The ratio of ratings 0-1 for free apps is {}%'.format(round((free_data.rating01.sum()\/free_data.shape[0])*100,2)))\nprint('The ratio of ratings 1-2 for free apps is {}%'.format(round((free_data.rating12.sum()\/free_data.shape[0])*100,2)))\nprint('The ratio of ratings 2-3 for free apps is {}%'.format(round((free_data.rating23.sum()\/free_data.shape[0])*100,2)))\nprint('The ratio of ratings 3-4 for free apps is {}%'.format(round((free_data.rating34.sum()\/free_data.shape[0])*100,2)))\nprint('The ratio of ratings 4-5 for free apps is {}%'.format(round((free_data.rating45.sum()\/free_data.shape[0])*100,2)))","78fa7a82":"f, ax = plt.subplots(figsize=(20, 10))\n#Check the new distribution \nsns.histplot(paid_data['price'], color=\"g\",kde=True);\nax.xaxis.grid(False) # False: remove grid along y-axis\nax.set(ylabel=\"Count\")\nax.set(xlabel=\"Price\")\nax.set(title=\"Price distribution for paid apps\")\nsns.despine(left=True) # Do not show the left axis when left=True\nplt.show()","8c26b852":"# We can tell that there are some outliers for price.\nprice_counts=paid_data['price'].value_counts().sort_values(ascending=False)\nprint('price counts for paid apps:\\n{}'.format(price_counts))\n# We consider price higher that 30 as outliers\npaid_data_no_outlier=paid_data.loc[paid_data['price']<30]\nf, ax = plt.subplots(figsize=(10, 7))\n#Check the new distribution \nsns.histplot(paid_data_no_outlier['price'], color=\"g\",kde=True);\nax.xaxis.grid(False) # False: remove grid along y-axis\nax.set(ylabel=\"Count\")\nax.set(xlabel=\"Price\")\nax.set(title=\"Price distribution for paid apps without outliers\")\nsns.despine(left=True) # Do not show the left axis when left=True\nplt.show()","aff6e574":"genre_counts=whole_data['prime_genre'].value_counts().sort_values(ascending=False)\nprint('genre counts for paid apps:\\n{}\\n'.format(genre_counts))\nprint('\\nGenre that has the most apps is:{},and genre that has the least apps is:{}\\n'.format(genre_counts.idxmax(),genre_counts.idxmin()))\nmean_rating=whole_data.groupby(['prime_genre']).user_rating.mean().sort_values(ascending=False)\nprint(\"\\nmean ratings for each genre are:\\n{}\".format(mean_rating))\nprint('\\nGenre that has the highest average rating is:{},and genre that has the lowest average rating is:{}\\n'.format(mean_rating.idxmax(),mean_rating.idxmin()))\nfig,ax=plt.subplots(figsize=(20,6))\nax=sns.scatterplot(data=mean_rating)\nax.set(ylabel=\"Mean rating score\")\nax.set(xlabel=\"Genre of apps\")\nax.set(title=\"Mean rating scores for different genre\")\nplt.xticks(rotation=45)","4f562457":"print('There are {} genres and we reduce genres to six for better analysis'.format(len(list(genre_counts.index))))\ntop_five_genres=list(genre_counts.index)[:5]\ndef reduce_genre(genre):\n    if genre in top_five_genres:\n        return genre\n    else:\n        return 'Others'\nwhole_data['reduced_genre']=whole_data['prime_genre'].apply(lambda x:reduce_genre(x))\nreduced_genre=list(whole_data['reduced_genre'].value_counts(ascending=False).index)\nfig,axes=plt.subplots(2,3,figsize=(25,10))\nfor genre,ax in zip(reduced_genre,axes.ravel()):# since here we are tring to plot 2*3 subplots. we need to ravel axes.\n    data=whole_data.loc[whole_data['reduced_genre']==genre,'price']\n    ax.set_title(\"{}\".format(genre))\n    sns.histplot(data, color=\"b\",kde=True,ax=ax)","3c925df7":"# notice that except for education and others, the maximun prices for other genres are no more than 30.\n# Let take a close look at the education and others genre.\neducation_data=whole_data.loc[whole_data['reduced_genre']=='Education']\n# Show the price and rating of the app that has a price higher than 30\nhigh_priced_education_apps=education_data.loc[education_data.price>=30,['track_name','price','user_rating','prime_genre','reduced_genre']].sort_values(by='price')\nothers_data=whole_data.loc[whole_data['reduced_genre']=='Others']\nhigh_priced_other_apps=others_data.loc[others_data.price>=30,['track_name','price','user_rating','prime_genre','reduced_genre']].sort_values(by='price')\napps_over_30=pd.concat([high_priced_education_apps,high_priced_other_apps]).sort_values(by='price').reset_index(drop=True)\napps_over_30","b1b31131":"# A violin plot draws a combination of boxplot and kernel density estimate. \nwhole_data['ispaid']=whole_data['price']>0\nwhole_data['ispaid']=whole_data['ispaid'].apply(lambda x:'paid' if x==True else 'free')\nfig,ax=plt.subplots(figsize=(15,7))\nplt.style.use('fast')\nplt.title(\"Distribution of User ratings\",fontsize=16) # fontsize is used to set the font of letters\nax=sns.violinplot(data=whole_data,x='reduced_genre',y='user_rating',hue='ispaid',orient='v',kse=False,split=True,scale='count',linewidth=2,palette=['#fdd470','#45cea2'])\nplt.xlabel('')\nplt.ylabel('User rating')\nplt.legend()","19854531":"free = whole_data.loc[whole_data.price==0,'reduced_genre'].value_counts().reset_index()\npaid = whole_data.loc[whole_data.price>0,'reduced_genre'].value_counts().reset_index()\ntotal= whole_data.loc[:,'reduced_genre'].value_counts().reset_index()\nfree.columns=['genre','free_apps']\npaid.columns=['genre','paid_apps']\ntotal.columns=['genre','total_apps']\npaid_free=free.merge(paid,on='genre').merge(total,on='genre')\npaid_free['paid_percentage']=(paid_free['paid_apps']\/paid_free['total_apps'])*100\npaid_free['free_percentage']=(paid_free['free_apps']\/paid_free['total_apps'])*100\npaid_free","390ede70":"list_free= paid_free.free_percentage.tolist()\nlist_paid=paid_free.paid_percentage.tolist()\n\nplt.figure(figsize=(15,8))\nN=6\nind = np.arange(N)    # the x locations for the groups\nwidth =0.56   # the width of the bars: can also be len(x) sequence\n\np1 = plt.bar(ind, list_free, width, color='#45cea2')\np2 = plt.bar(ind, list_paid, width,bottom=list_free,color='#fdd470')\nplt.xticks(ind,paid_free.genre.tolist())\nplt.legend((p1[0], p2[0]), ('free', 'paid'),loc='upper right')\nplt.title(\"The proportion of free and paid apps for differnt genres\")\nplt.show()","66ad1ed7":"whole_data['size_mb']=whole_data['size_bytes']\/1048576 \nfig,axes=plt.subplots(1,2,figsize=(10,8))\nsns.scatterplot(x='size_mb',y='price',data=whole_data,ax=axes[0],color='b')\nsns.scatterplot(x='size_mb',y='user_rating',data=whole_data,ax=axes[1],color='g')\naxes[0].set_title('Does size influence the price of apps?')\naxes[0].set_xlabel('Size(Mb)')\naxes[1].set_title('Does size influence the rating of apps?')\naxes[1].set_xlabel('Size(Mb)')\naxes[1].set_ylabel('User rating')","2b2bcff6":"# How about for each genres-price and size,hue=reduced_genre\n#lmplot is used to fit a regression\nsns.set_style(\"whitegrid\")\n# here we need to define hue and col.\n# hue can be latter used to appoint color and col is used to define the subset of data to be drawn.\nax=sns.lmplot(data=whole_data,x='size_mb',y='price',aspect=2,col_wrap=2,hue='reduced_genre',col='reduced_genre',fit_reg=False,palette = sns.color_palette(\"pastel\", 6))","b7358da4":"# How about for each genres-rating and size,hue=reduced_genre\n#lmplot is used to fit a regression\nsns.set_style(\"whitegrid\")\n# here we need to define hue and col.\n# hue can be latter used to appoint color and col is used to define the subset of data to be drawn.\nax=sns.lmplot(data=whole_data,x='size_mb',y='user_rating',aspect=2,col_wrap=2,hue='reduced_genre',col='reduced_genre',fit_reg=False,palette = sns.color_palette(\"pastel\", 6))","9f3e9c43":"def random_color_generator(number_of_colors): #generate random colors\n    color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n                 for i in range(number_of_colors)]\n    return color\n\napp_number_each_genre=list(whole_data['reduced_genre'].value_counts(ascending=False))\nplt.figure(figsize=(10,10))\nmy_circle=plt.Circle( (0,0), 0.5, color='white')\nplt.pie(app_number_each_genre, labels=reduced_genre, colors = random_color_generator(6))\np=plt.gcf() # Get the current figure.\np.gca().add_artist(my_circle)\nplt.show()\n","3971c0a2":"paid_free","cd3492b8":"plt.figure(figsize=(10,10))\nf=pd.DataFrame(index=np.arange(0,11,2),data=paid_free.free_apps.values,columns=['num']) #seperate the index so that when sort index,num is listed accoring to genres.\np=pd.DataFrame(index=np.arange(1,12,2),data=paid_free.paid_apps.values,columns=['num'])\nfinal = pd.concat([f,p],names=['labels']).sort_index()\nfinal.num.tolist()\n\n# draw the outside circle\nfig, ax = plt.subplots()\nax.axis('equal')\nmypie, _ = ax.pie(paid_free.total_apps.tolist(), radius=2.5, labels=paid_free.genre.tolist(), colors=random_color_generator(6))\nplt.setp( mypie, width=1.2, edgecolor='white')\n\n# prepare for the inner circle\nh = ['Free', 'Paid']\nsubgroup_names= 6*h\nsub= ['#45cea2','#fdd470']\nsubcolors= 6*sub\nsubgroup_size=final.num.tolist()\n\n# draw the inner circle\nmypie2, _ = ax.pie(subgroup_size, radius=1.6, labels=subgroup_names, labeldistance=0.7, colors=subcolors)\nplt.setp( mypie2, width=0.8, edgecolor='white')\nplt.margins(0,0)\n\n# show it\nplt.show()","abd1c0e9":"numeric_dtypes = ['int64','float64']\nnumeric = []\nfor i in whole_data.columns: # The column labels of the DataFrame.\n    if whole_data[i].dtype in numeric_dtypes and i!='user_rating':\n        numeric.append(i)     \nprint('The number of numeric features is {}'.format(len(numeric)))\n","33af2b60":"fig, axs = plt.subplots(ncols=1, nrows=1, figsize=(12, 36))\nplt.subplots_adjust(right=2) #Adjust the subplot layout parameters. set the position of right edge of subplots\nplt.subplots_adjust(top=2)\nsns.color_palette(\"pastel\", 8) #Return a list of colors defining a palette.\n# visualising some more outliers in the data values\nfor i, feature in enumerate(list(whole_data[numeric]), 1):# enumerate staring from index 1 instead of 0\n    plt.subplot(len(list(numeric)), 3, i) # draw the ith subplot \n    sns.scatterplot(x=feature, y='user_rating', hue='user_rating',palette='Blues', data=whole_data)\n        \n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5) #size set up fontsize \n    plt.ylabel('user_rating', size=15, labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12) #Change the appearance of ticks, tick labels, and gridlines.\n        plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(loc='best', prop={'size': 10})\n        \nplt.show()","1421031d":"corr = whole_data.corr()\nplt.subplots(figsize=(15,12))\nsns.heatmap(corr,vmax=.8, cmap=\"Blues\", square=True,annot=True)","7a8c9a4f":"whole_data.head()","c450b556":"# Customed functions to clean input text\n#Remove punctuations, links, mentions and \\r\\n new line characters\n#Remove punctuations, links, mentions and \\r\\n new line characters\ndef strip_all_entities(text): \n    text = text.replace('\\r', '').replace('\\n', ' ').replace('\\n', ' ').lower() #remove \\n and \\r and lowercase\n    text = re.sub(r\"(?:\\@|https?\\:\/\/)\\S+\", \"\", text) #remove links and mentions(@)\n    text = re.sub(r'[^\\x00-\\x7f]',r'', text) #remove non utf8\/ascii characters such as '\\x9a\\x91\\x97\\x9a\\x97'\n    banned_list= string.punctuation + '\u00c3'+'\u00b1'+'\u00e3'+'\u00bc'+'\u00e2'+'\u00bb'+'\u00a7'+'\u2605'+'\u25a0'+'\u00bb'+'^'+'\u00bf'+'*'\n    table = str.maketrans('', '', banned_list)\n    text = text.translate(table)\n    return text\n\n#Filter special characters such as & and $ present in some words\ndef filter_chars(a):\n    sent = []\n    for word in a.split(' '):\n        if ('$' in word) | ('&' in word):\n            sent.append('')\n        else:\n            sent.append(word)\n    return ' '.join(sent)\n\ndef remove_mult_spaces(text): # remove multiple spaces\n    return re.sub(\"\\s\\s+\" , \" \", text)","53eafd35":"texts_new= []\nfor t in whole_data.app_desc:\n    texts_new.append(remove_mult_spaces(filter_chars(strip_all_entities(t))))\nwhole_data['text_clean']=texts_new","e21a103f":"# Then we check our clean text to see if we have removed to much information during data cleaning.\ntext_len = []\nfor text in whole_data.text_clean:\n    tweet_len = len(text.split())\n    text_len.append(tweet_len)\nwhole_data['text_len']=text_len\n\nplt.figure(figsize=(7,5))\nax = sns.countplot(x='text_len', data=whole_data.loc[whole_data['text_len']<5], palette='mako')\nplt.title('Cleaned app description with less than 10 words')\nplt.yticks([]) #remove yticks\nax.bar_label(ax.containers[0])\nplt.ylabel('count')\nplt.xlabel('')\nplt.show()","5a11427b":"# we keep app descriptions that still have 5 or more than 5 words after data cleaning.\n\nprint(\"rows for data before deleting:{}\".format(whole_data.shape[0]))\nwhole_data_clean= whole_data.loc[whole_data['text_len'] > 4,:]\nprint(\"rows for data after deleting:{}\".format(whole_data_clean.shape[0]))\n","20d583e8":"# There's no missing value.\nwhole_data_clean.isnull().sum()","66d867aa":"# whole_data_clean.app_desc[50:100]","e0aeecfc":"# according to our experience, app description that contain words such as: sales,save,free to attract customer's attention. \n# Besides,app description may include words such as: most, popular,worldwide,word,million,billion,best,top to brag about the product's quality.\n# Hence, we search whether app description contains these two types of words and generate two dummies.\n\nbargain=['sale','on sale','save','free','only','cheap','cheapest','bargain','lowest','low','limited']\nquality=['most','popular','rated','bestselling','rating','perfect','world','winner','best','over','powerful','richest','leading','first','easiest','unlimited','better','famous']\n\n# Load the SpaCy model\nnlp = spacy.blank('en') \n#First lets find whether there are words in each app description having words listed in 'bargain'\n# Create the PhraseMatcher object. The tokenizer is the first argument. Use attr = 'LOWER' to make consistent capitalization\nmatcher = PhraseMatcher(nlp.vocab, attr='LOWER') \n# Create a list of tokens for each item in the bargain\nbargain_tokens_list = [nlp(item) for item in bargain]\n# Add the item patterns to the matcher. \nmatcher.add(\"bargain\",bargain_tokens_list) \n\nbargain_dumm=[]\nfor idx, review in whole_data_clean.iterrows():\n    doc = nlp(review.text_clean) \n    # Using the matcher from the previous exercise\n    matches = matcher(doc) \n    # Create a set of the items found in the review text\uff0c\n    found_items =set([doc[match[1]:match[2]].text.lower() for match in matches]) \n    if len(found_items)!=0:\n        bargain_dumm.append(1)\n    else:\n        bargain_dumm.append(0)","781446af":"# Then lets find whether there are words in each app description having words listed in 'quality'\nmatcher2 = PhraseMatcher(nlp.vocab, attr='LOWER') \nquality_tokens_list = [nlp(item) for item in quality]\n# Add the item patterns to the matcher. \nmatcher2.add(\"quality\",quality_tokens_list) \nquality_dumm=[]\nfor idx, review in whole_data_clean.iterrows():\n    doc = nlp(review.text_clean) \n    # Using the matcher from the previous exercise\n    matches = matcher2(doc) \n    # Create a set of the items found in the review text\uff0c\n    found_items =set([doc[match[1]:match[2]].text.lower() for match in matches]) \n    if len(found_items)!=0:\n        quality_dumm.append(1)\n    else:\n        quality_dumm.append(0)","4fc5c320":"whole_data_clean['bargain_dumm']=bargain_dumm\nwhole_data_clean['quality_dumm']=quality_dumm\nwhole_data_clean['bar_qua_sum']=whole_data_clean['bargain_dumm']+whole_data_clean['quality_dumm']\nwhole_data_clean['cost_effective_dumm']=whole_data_clean['bar_qua_sum']==2\nwhole_data_clean.drop(['bar_qua_sum'],axis=1,inplace=True)\nprint('There are {} descriptions involve bargain information, while {} descriptions do not involve.'.format(whole_data_clean.bargain_dumm.value_counts().tolist()[0],whole_data_clean.bargain_dumm.value_counts().tolist()[1]))\nprint('There are {} descriptions involve quality information, while {} descriptions do not involve.'.format(whole_data_clean.quality_dumm.value_counts().tolist()[0],whole_data_clean.quality_dumm.value_counts().tolist()[1]))\nprint('There are {} descriptions indicate cost effective information, while {} descriptions do not involve.'.format(whole_data_clean.cost_effective_dumm.value_counts().tolist()[0],whole_data_clean.cost_effective_dumm.value_counts().tolist()[1]))","42e0d49b":"# Let's have a look at our data\nwhole_data_clean.head(5)","fe2fe480":"# Let's generate more new features for future prediction.\n# 1.count the percentage of current rating \nwhole_data_clean['cur_rating_per']=(whole_data_clean['rating_count_ver']\/(whole_data_clean['rating_count_tot']))*100\n# 2. dummy:whether current rating has improved\nwhole_data_clean['cur_better_rating']=whole_data_clean['user_rating_ver']>whole_data_clean['user_rating']\n# 3. how many main versions before\nwhole_data_clean['main_version_num']=whole_data_clean['ver'].str.split(pat='.',expand=True)[0]\n# delete some abnormal versions\nwhole_data_clean['main_version_num']=whole_data_clean['main_version_num'].apply(lambda x: 0 if x=='0' else (1 if x=='1' else (2 if x=='2' else (3 if x=='3' else (4 if x=='4' else (5 if x=='5' else (6 if x=='6' else (7 if x=='7' else (8 if x=='8' else (9 if x=='9' else (10 if x=='10' else (11 if x=='11'else (12 if x=='12' else (13 if x=='13' else (14 if x=='14' else (15 if x=='15' else(16 if x=='16' else(17 if x=='17' else (18 if x=='18' else (19 if x=='19' else 20))))))))))))))))))))\nwhole_data_clean.loc[whole_data_clean['main_version_num']<20]\n# 4. dummy: generate more accurate category classification for prime_genre (11 categories)\nwhole_data_clean['accu_reduced_genre']=whole_data_clean['prime_genre'].apply(lambda x: 0 if x=='Games' else (1 if x=='Entertainment' else (2 if x=='Education' else (3 if x=='Photo & Video' else (4 if x=='Utilities' else (5 if x=='Health & Fitness' else (6 if x=='Productivity' else (7 if x=='Social Networking' else (8 if x=='Music' else (9 if x=='Lifestyle' else 10))))))))))\n# 5. dummy: whether used screenshots for display or not\nwhole_data_clean['use_screenshots']=whole_data_clean['ipadSc_urls.num']>0\n# 6. dummy: whether the app only supports one language or not\nwhole_data_clean['only_one_language']=whole_data_clean['lang.num']==1\n# 7. dummy_target: if rating equals or is higher than 4,then 1,else 0\nwhole_data_clean['rating_dum']=whole_data_clean['user_rating']>=4\nwhole_data_clean.shape","68170166":"# generate df, which will be used to predict.\ndf=whole_data_clean.copy()\ndf.drop(['track_name','currency','ver','prime_genre','app_desc','text_clean','user_rating'],axis=1,inplace=True)\ndf=pd.get_dummies(df).reset_index(drop=True)\ndf.fillna(0,axis=1,inplace=True)\n","0128842d":"# shuffle our data\ndf=df.sample(frac=1).reset_index(drop=True)\nprint(\"We use 80% data for train({}*0.8={}), 20% data for test({}*0.2={})\".format(df.shape[0],round(df.shape[0]*0.8),df.shape[0],round(df.shape[0]*0.2)))\ndata=df[:5450]\ntest_data=df[5450:]\ny_data=data.pop('rating_dum')\nX_data=data.copy()\ny_test=test_data.pop('rating_dum')\nX_test=test_data.copy()","bddf8da5":"#train_valid split \nX_train,X_valid,y_train,y_valid=train_test_split(X_data,y_data,test_size=0.2, random_state=1, stratify=y_data)\nprint('X_train shape:', X_train.shape)\nprint('X_valid shape:', X_valid.shape)\nprint('X_test shape:', X_test.shape)","90c91cee":"# Setup cross validation folds\nskf=StratifiedKFold(n_splits=8,shuffle=True,random_state=1)","0d8dd9d6":"pipe_lr_1=make_pipeline(RobustScaler(),LogisticRegression())\nparam_grid_lr = {'logisticregression__C': [0.01, 0.1, 1, 10, 100],\n              'logisticregression__penalty':['l2'],\n              'logisticregression__solver':['newton-cg','lbfgs','liblinear','sag','saga'],\n              'logisticregression__n_jobs':[-1]}\ngrid_lr_1= GridSearchCV(pipe_lr_1, param_grid_lr, cv=skf,scoring='f1')\ngrid_lr_1.fit(X_train, y_train)\nprint(\"Best estimator:\\n{}\".format(grid_lr_1.best_estimator_))\nprint(\"Best gridsearch score: {:.3f}\".format(grid_lr_1.best_score_))\nprint(\"Score on validset:{:.3f}\".format(grid_lr_1.score(X_valid,y_valid)))","9434daa9":"# customized function to draw confusion matrix\ndef cal_evaluation(classifier, cm):\n    tn = cm[0][0]\n    fp = cm[0][1]\n    fn = cm[1][0]\n    tp = cm[1][1]\n    accuracy  = (tp + tn) \/ (tp + fp + fn + tn + 0.0)\n    precision = tp \/ (tp + fp + 0.0)\n    recall = tp \/ (tp + fn + 0.0)\n    print (classifier)\n    print (\"Accuracy is: %0.3f\" % accuracy)\n    print (\"precision is: %0.3f\" % precision)\n    print (\"recall is: %0.3f\" % recall)\n    \ndef draw_confusion_matrices(confusion_matricies):\n    class_names = ['Rated below 4','Rated equaling or over 4']\n    for cm in confusion_matrices:\n        classifier, cm = cm[0], cm[1]\n        cal_evaluation(classifier, cm)\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n        cax = ax.matshow(cm, interpolation='nearest',cmap=plt.get_cmap('Blues'))\n        plt.title('Confusion matrix for %s' % classifier)\n        fig.colorbar(cax)\n        ax.set_xticklabels([''] + class_names)\n        ax.set_yticklabels([''] + class_names)\n        plt.xlabel('Predicted')\n        plt.ylabel('True')\n        plt.show()\n    \n# seems that standardscaler and robustscaler do no make difference on the result.\n# let's take a look at the confusion matrix.\n%matplotlib inline\nconfusion_matrices = [\n    (\"Logistic Regression\", confusion_matrix(y_valid,grid_lr_1.predict(X_valid))),\n]\n\ndraw_confusion_matrices(confusion_matrices)","c50451d1":"lr_results=grid_lr_1.best_estimator_.named_steps['logisticregression'].coef_.reshape(32,1)\nfeatures=X_train.columns.values.reshape(32,1)\nfeatures=pd.DataFrame(features,columns=['feature_name'])\ncoef=pd.DataFrame(lr_results,columns=['coef'])\nfeatures_coef=features.join(coef)\nfeatures_coef_pos=features_coef.sort_values(by='coef',ascending=False).reset_index(drop=True)\nfeatures_coef_pos10=features_coef_pos[:10]\nfeatures_coef_neg=features_coef.sort_values(by='coef',ascending=True).reset_index(drop=True)\nfeatures_coef_neg10=features_coef_neg[:10]\nten_pos_neg_features=features_coef_pos10.join(features_coef_neg10,lsuffix='_pos',rsuffix='_neg')\nten_pos_neg_features# indicate the most contributing features","9af2574b":"pipe_rf=make_pipeline(RobustScaler(),RandomForestClassifier())\nparam_grid_rf = {'randomforestclassifier__n_estimators': [10,100,1000,10000],\n              'randomforestclassifier__max_features':['auto', 'sqrt', 'log2'],\n              'randomforestclassifier__bootstrap':[True],\n              'randomforestclassifier__random_state':[1],\n              'randomforestclassifier__n_jobs':[-1]}\ngrid_rf= GridSearchCV(pipe_rf, param_grid_rf, cv=skf,scoring='f1')\ngrid_rf.fit(X_train, y_train)\nprint(\"Best estimator:\\n{}\".format(grid_rf.best_estimator_))\nprint(\"Best gridsearch score: {:.3f}\".format(grid_rf.best_score_))\nprint(\"Score on validset:{:.3f}\".format(grid_rf.score(X_valid,y_valid)))","de8634ad":"# confusion matrix\nconfusion_matrices = [\n    (\"Random Forest\", confusion_matrix(y_valid,grid_rf.predict(X_valid))),\n]\n\ndraw_confusion_matrices(confusion_matrices)","b681310a":"def plot_feature_importances(model):\n    n_features = X_train.shape[1]\n    fig = plt.figure(figsize=(24, 12))\n    plt.barh(range(n_features), model.feature_importances_, align='center',color='g')\n    plt.yticks(np.arange(n_features), X_train.columns.values)\n    plt.xlabel(\"Feature importance\")\n    plt.ylabel(\"Feature\")\n    plt.title('Feature importance for random forest classifier',fontsize=16)\n\nplot_feature_importances(model=grid_rf.best_estimator_.named_steps['randomforestclassifier'])","badfa0e1":"xgb=XGBClassifier(objective='binary:logistic',eval_metric='logloss')\nparam_grid_xgb = {'min_child_weight': [0.1,1],\n              'subsample':[0.8,0.9,1.0],\n              'colsample_bytree':[0.8,0.9,1.0],\n              'max_depth':[6,9,12], \n              'learning_rate':[0.001,0.1],\n              'n_jobs':[-1],\n              'random_state':[1] }\ngrid_xgb= GridSearchCV(xgb, param_grid_xgb, cv=skf,scoring='f1')\ngrid_xgb.fit(X_train, y_train)\nprint(\"Best estimator:\\n{}\".format(grid_xgb.best_estimator_))\nprint(\"Best gridsearch score: {:.3f}\".format(grid_xgb.best_score_))\nprint(\"Score on validset:{:.3f}\".format(grid_xgb.score(X_valid,y_valid)))","f15e0eb8":"# confusion matrix\nconfusion_matrices = [\n    (\"XGBOOST\", confusion_matrix(y_valid,grid_xgb.predict(X_valid))),\n]\n\ndraw_confusion_matrices(confusion_matrices)\nplot_feature_importances(model=grid_xgb.best_estimator_)","01db3bb9":"# Stack up all the models above, optimized using xgboost\nstack_gen = StackingCVClassifier(classifiers=(grid_lr_1.best_estimator_, grid_rf.best_estimator_, grid_xgb.best_estimator_),\n                                meta_classifier=XGBClassifier(eval_metric='logloss'),use_features_in_secondary=True,cv=skf,use_probas=True,random_state=1)\nstack_gen_model = stack_gen.fit(np.array(X_train), np.array(y_train))\n\n# Get final precitions from the blended model\nblended_score = f1_score(y_valid, stack_gen_model.predict(np.array(X_valid)))\nprint(\"Blending prediction score on validate data {:.3f}\".format(blended_score))","df8a898e":"# confusion matrix\nconfusion_matrices = [\n    (\"Stacking\", confusion_matrix(y_valid,stack_gen_model.predict(X_valid))),\n]\n\ndraw_confusion_matrices(confusion_matrices)","1142c867":"test_score=f1_score(y_test, stack_gen_model.predict(np.array(X_test)))\nprint(\"Blending prediction score on test data {:.3f}\".format(test_score))","a832fd38":"##  For other features","f79bcacc":"## Random Forest Classifier","13c76a36":"It seems that size has nothing to do with price and user rating. Let's take a closer look when we seperate the data into categories.","8580b3fa":"## Base model:Logistic Regression\n","1abb1bbd":"## 2. How to visualize price distribution of paid apps ?","0a96d7cd":"## Insights\n1. Count of paid apps is exponentially decreases as the price increases.\n2. Very few apps have been priced above 30. So its important to keep price of your app below 30.","6a11a17f":"## Stacking up these three models","ab545f71":"## Insight:\n\n1. We can tell that the percentage of rating the app between 0-1 is much higher for free apps. \n\n2. The percentage of rating 3 and more is higher for paid apps that free apps.\n\n\nThese indicate that paid apps are overall better than free apps in quality. \n","60f22333":"# PART I. EDA\n\n### The Goal\n\n* Each row in the dataset describes the characteristics related to the rating of a mobile app.\n* Assume that the higher the average user rating value is, the closer the app's position is to the top of the chart.Our goal is to predict the average user rating value. ","44337d1e":"## Insight\n\n1. Education has the highest number of apps that are not free.\n2. Entertainment,others and games are high in the number of free apps.","f592e9fb":"## 0. user_rating: the average user rating value is the variable we are tring to predict","50c8622e":"## XGBOOST Classifier","5bf44fb7":"## 6. Does size influence price and rating?","fb336e7d":"As we can see, there are lots of cleaned app description with less than 5 words: this is due to the cleaning performed before. This means that some app description contained only mentions, hashtags and links, which have been removed. We will drop these empty tweets and also those with less than 5 words.","cbf19e0d":"## 4. How does the price distribution affected by category ?","1d625aa9":"## Insights\n\nNo matter for price or for user rating, the size of app does not have any influence on them.","8822a483":"## 1. How to visiualize rating distribution for paid and free apps?","5779338d":"## 8. Other visualizations of features","dac68153":"## 5. The proportion of free and paid apps for differnt genres","a1a61905":"## For description\n\nWe try to identify whether each app description contains certain information that indicates the product is cheap in price or the product is good in quality\/popular. We generate 3 dummy variables from the description information:\n\n1. bargain_dumm:whether description indicates the product price is low\/on sale.\n2. quality_dumm:whether description indicates the product is popular\/in good quality.\n3. cost_effective_dumm: whether description contains both bargain and quality information.","a11db03a":"## 3. Are there significant rating differences between categories?","9f1a32cb":"## Insights\n\n1. Genre does affect the rating. We can tell from above analysis that Music averagely speaking wins the highest mean rating score(3.811594), while the Finance genre gets the lowest mean rating (1.634615).\n\n2. Top five highest mean rating genres are: Music, Productivity, Games,Photo&Videos and Health & Fitness; Lowest five are: News, Navigation,Book,Catalogs and Finance.\n\n\nThese indicates that we may want to do apps within genres that receive high mean rating. ","8b5d6d61":"## Insights\n1. All genres have the most apps for the price of 0.99.\n2. Except for medical,music,reference,business,education,navigation and productivity, all other genres limit their apps' price with in 30.\n3. Education genre has the most number of expensive apps and the prices are generally at the top of all apps. \n4. The width of violinplot shows the probability of a value and we can tell for most of the genres, free apps are more likely get ratings of 0.","c9f0131f":"## 7. How are the apps distributed category wise ? can we split by paid category ?","0fa3ec0b":"## ","c4e8f21a":"# PART III. Predicting user rating","39b8e774":"# PART II. Feature engineering","bc9a90b7":"## Apply model on test data"}}