{"cell_type":{"bc499120":"code","10a0bddc":"code","dbdae8b7":"code","3fa65e0d":"code","a04a828b":"code","dcdeb5aa":"code","b3c3ba6b":"code","c8149831":"code","8792a7c5":"code","0a9efd29":"code","b0181a1e":"code","ffc5e157":"code","76250e15":"markdown","b39137a7":"markdown","92f21c01":"markdown","3098c08d":"markdown","30608330":"markdown","12b3d19d":"markdown"},"source":{"bc499120":"from ast import literal_eval\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\nTRAIN_PATH = '\/kaggle\/input\/tensorflow-great-barrier-reef\/train_images'\nN_SAMP = 6000\n\ndf = pd.read_csv('\/kaggle\/input\/tensorflow-great-barrier-reef\/train.csv')\nn_with_annotations = (df['annotations'] != '[]').sum()\n\ndf = pd.concat([\n    df[df['annotations'] != '[]'],\n    df[df['annotations'] == '[]'].sample(N_SAMP - n_with_annotations)\n]).sample(frac=1).reset_index(drop = True)\n\ndf['is_valid'] = df['video_id'] == 2\ndf['annotations'] = df['annotations'].apply(literal_eval)\ndf['path'] = df.apply(lambda row: f\"{TRAIN_PATH}\/video_{row['video_id']}\/{row['video_frame']}.jpg\", axis = 1)\n\ndf.tail()","10a0bddc":"def coco(df):\n    \n    annotion_id = 0\n    images = []\n    annotations = []\n\n    categories = [{'id': 0, 'name': 'cots'}]\n\n    for i, row in tqdm(df.iterrows(), total = len(df)):\n\n        images.append({\n            \"id\": i,\n            \"file_name\": f\"{row['image_id']}.jpg\",\n            \"height\": 720,\n            \"width\": 1280,\n        })\n        for bbox in row['annotations']:\n            annotations.append({\n                \"id\": annotion_id,\n                \"image_id\": i,\n                \"category_id\": 0,\n                \"bbox\": list(bbox.values()),\n                \"area\": bbox['width'] * bbox['height'],\n                \"segmentation\": [],\n                \"iscrowd\": 0\n            })\n            annotion_id += 1\n\n    json_file = {'categories':categories, 'images':images, 'annotations':annotations}\n    return json_file","dbdae8b7":"json_train = coco(df[~df['is_valid']])\njson_valid = coco(df[ df['is_valid']])","3fa65e0d":"import json\n\nwith open('\/kaggle\/working\/annotations_train.json', 'w', encoding='utf-8') as f:\n    json.dump(json_train, f, ensure_ascii=True, indent=4)\n    \nwith open('\/kaggle\/working\/annotations_valid.json', 'w', encoding='utf-8') as f:\n    json.dump(json_valid, f, ensure_ascii=True, indent=4)","a04a828b":"import os\nos.makedirs('\/kaggle\/working\/train2017', exist_ok=True)\nos.makedirs('\/kaggle\/working\/val2017', exist_ok=True)","dcdeb5aa":"import shutil\nfor i, row in tqdm(df.iterrows(), total = len(df)):\n    base_dir = 'val2017' if row['is_valid'] else 'train2017'\n    fname = f\"{row['image_id']}.jpg\"\n    shutil.copyfile(row['path'], f\"\/kaggle\/working\/{base_dir}\/{fname}\")","b3c3ba6b":"!pip install -Uqqq 'git+https:\/\/github.com\/cocodataset\/cocoapi.git#subdirectory=PythonAPI'","c8149831":"from pycocotools.coco import COCO\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom random import sample","8792a7c5":"data_dir = '\/kaggle\/working\/train2017'\nann_file = '\/kaggle\/working\/annotations_train.json'\ncoco = COCO(ann_file)\nimg_ids = coco.getImgIds()","0a9efd29":"n_row = 3\nn_col = 2\nimgs = coco.loadImgs(sample(img_ids, n_row * n_col))\n_, axs = plt.subplots(n_row, n_col, figsize=(12 * n_col, 8 * n_row))\naxs = axs.flatten()\nfor img, ax in zip(imgs, axs):\n    img_img = Image.open(f\"{data_dir}\/{img['file_name']}\")\n    anns = coco.loadAnns(coco.getAnnIds(imgIds=[img['id']]))\n    ax.imshow(img_img)\n    plt.sca(ax)\n    coco.showAnns(anns, draw_bbox=True)\n    plt.axis('off')\nplt.show()","b0181a1e":"shutil.make_archive('val2017', 'zip', 'val2017')\nshutil.make_archive('train2017', 'zip', 'train2017')","ffc5e157":"shutil.rmtree('val2017') \nshutil.rmtree('train2017') ","76250e15":"## Converting to COCO","b39137a7":"## Ziping the files so kaggle can assemble a dataset\n\nthe final dataset can be found [HERE](https:\/\/www.kaggle.com\/coldfir3\/great-barrier-reef-yolov5)","92f21c01":"## Loading the DataFrame and spliting into train\/test\n\nI have a whole discussion topic ([here](https:\/\/www.kaggle.com\/c\/tensorflow-great-barrier-reef\/discussion\/293723)) where I explain why I think spliting by video is the best approach, but feel free to change the train\/val split strategy","3098c08d":"## Saving into a json file","30608330":"## Sanity check","12b3d19d":"<h1><center>Simple YoloX Dataset Generator (COCO-JSON)<\/center><\/h1>     \n\n<center><img src = \"https:\/\/i.imgur.com\/iatgdo5.jpg\" width = \"635\" height = \"235\"\/><\/center>         \n\nThis dataset was built to be compatible with the train (train.py) script that can be found [HERE](https:\/\/github.com\/Megvii-BaseDetection\/YOLOX). I also have a training notebook that you can find [HERE](https:\/\/www.kaggle.com\/coldfir3\/yolox). The inference notebook is still WIP. The resulting kaggle Dataset cand be found [HERE](https:\/\/www.kaggle.com\/coldfir3\/cots-yolox-dataset)\n\nThe tree main tasks into converting this dataset to YoloX format are:\n1. Splitting into train and test\n1. Converting the bboxes to COCO format\n1. Saving the annotations to a .json file\n\nI took inspiration on [this](https:\/\/www.kaggle.com\/remekkinas\/yolox-training-pipeline-cots-dataset-lb-0-507) amazing notebook\n\n<h3 style='background:orange; color:black'><center>Consider upvoting this notebook if you found it helpful.<\/center><\/h3>"}}