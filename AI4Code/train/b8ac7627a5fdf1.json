{"cell_type":{"0db92de1":"code","bf581481":"code","b063215a":"code","e8d4b2e0":"code","c8a54b4b":"code","b1637734":"code","23b20f24":"code","e1c6d8a2":"code","3b93dc99":"code","7d945fe8":"code","136c7db0":"code","98e8a267":"code","f8e76560":"code","c1dd143b":"code","d22e0295":"code","e4ed4223":"code","d5046c48":"code","592e4456":"code","b59ab10c":"code","88a2b8a7":"code","276f4de4":"code","e079d7ba":"code","3bac5bb5":"code","4654d3cb":"code","28a70256":"code","961bad7b":"code","bb897e39":"code","0f414163":"markdown","e7af3f35":"markdown","96a839b2":"markdown","56432941":"markdown","7101f914":"markdown","b8b609e4":"markdown","e6d6249e":"markdown","edceee95":"markdown","3d15dc6a":"markdown","62267d41":"markdown","202b49c0":"markdown","e38ba63d":"markdown","efc69dea":"markdown","4a906fa1":"markdown","e9e47777":"markdown","23b53799":"markdown","a0c4dada":"markdown","4ed80092":"markdown"},"source":{"0db92de1":"import pandas as pd\nimport numpy as np\nimport time\n\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, StratifiedKFold\n# from sklearn.metrics import roc_auc_score\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom xgboost import XGBClassifier\n\nfrom pathlib import Path\n\nimport optuna\nfrom optuna.samplers import TPESampler\n\nimport holidays","bf581481":"pd.options.display.float_format = '{:,.2f}'.format\npd.set_option('display.max_columns', 15)\nplt.style.use('ggplot')","b063215a":"class Config:\n    debug = False\n    competition = \"TPS_202201\"\n    seed = 42\n    NFOLDS = 5\n    EPOCHS = 10","e8d4b2e0":"data_dir = Path('..\/input\/tabular-playground-series-jan-2022') # Change for every project","c8a54b4b":"%%time\ntrain_df = pd.read_csv(data_dir \/ \"train.csv\", parse_dates=['date']\n#                       nrows=100000\n                      )\n\ntest_df = pd.read_csv(data_dir \/ \"test.csv\", parse_dates=['date'])\nsample_submission = pd.read_csv(data_dir \/ \"sample_submission.csv\")\n\nprint(f\"train data: Rows={train_df.shape[0]}, Columns={train_df.shape[1]}\")\nprint(f\"test data : Rows={test_df.shape[0]}, Columns={test_df.shape[1]}\")","b1637734":"train_df.info()","23b20f24":"train_df.head()","e1c6d8a2":"# Country List:['Finland' 'Norway' 'Sweden']\nholiday_FI = holidays.CountryHoliday('FI', years=[2015, 2016, 2017, 2018, 2019])\nholiday_NO = holidays.CountryHoliday('NO', years=[2015, 2016, 2017, 2018, 2019])\nholiday_SE = holidays.CountryHoliday('SE', years=[2015, 2016, 2017, 2018, 2019])\n\nholiday_dict = holiday_FI.copy()\nholiday_dict.update(holiday_NO)\nholiday_dict.update(holiday_SE)\n\ntrain_df['date'] = pd.to_datetime(train_df['date']) # Convert the date to datetime.\ntrain_df['holiday_name'] = train_df['date'].map(holiday_dict)\ntrain_df['is_holiday'] = np.where(train_df['holiday_name'].notnull(), 1, 0)\ntrain_df['holiday_name'] = train_df['holiday_name'].fillna('Not Holiday')\n\ntest_df['date'] = pd.to_datetime(test_df['date']) # Convert the date to datetime.\ntest_df['holiday_name'] = test_df['date'].map(holiday_dict)\ntest_df['is_holiday'] = np.where(test_df['holiday_name'].notnull(), 1, 0)\ntest_df['holiday_name'] = test_df['holiday_name'].fillna('Not Holiday')","3b93dc99":"def create_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Create features base on the date variable, the idea is to extract as much \n    information from the date componets.\n    Args\n        df: Input data to create the features.\n    Returns\n        df: A DataFrame with the new time base features.\n    \"\"\"\n    \n    df['date'] = pd.to_datetime(df['date']) # Convert the date to datetime.\n    \n    # Start the creating future process.\n    df['year'] = df['date'].dt.year\n    df['quarter'] = df['date'].dt.quarter\n    df['month'] = df['date'].dt.month\n    df['day'] = df['date'].dt.day\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['dayofmonth'] = df['date'].dt.days_in_month\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['weekofyear'] = df['date'].dt.weekofyear\n    df['weekday'] = df['date'].dt.weekday\n    df['is_weekend'] = np.where((df['weekday'] == 5) | (df['weekday'] == 6), 1, 0)\n    \n    return df\n\ntrain_df = create_time_features(train_df)\ntest_df = create_time_features(test_df)\n","7d945fe8":"CATEGORICAL = ['country', 'store', 'product', 'holiday_name']","136c7db0":"def encode_categorical_features(df, categorical_colums = CATEGORICAL):\n    \"\"\"\n    Use the label encoder to encode categorical features...\n    Args\n        df\n        categ_colums\n    Returns\n        df\n    \"\"\"\n    le = LabelEncoder()\n    for col in categorical_colums:\n        df[col] = le.fit_transform(df[col])\n    return df\n","98e8a267":"def transform_target(df: pd.DataFrame, target: str) -> pd.DataFrame:\n    \"\"\"\n    Apply a log transformation to the target for better optimization \n    during training.\n    \"\"\"\n    df[target] = np.log(df[target])\n    return df\n\ntrain_df = transform_target(train_df, 'num_sold')","f8e76560":"x_data = train_df.drop(['row_id', 'date', 'num_sold'], axis=1)\ny = train_df.num_sold\n\nx_test = test_df.drop(['row_id', 'date'], axis=1)\n","c1dd143b":"X = encode_categorical_features(x_data)\nX_test = encode_categorical_features(x_test)","d22e0295":"X.sample(5)","e4ed4223":"avoid = ['row_id', 'date', 'num_sold']\nFEATURES = [feat for feat in X.columns if feat not in avoid]\n\n# Print a list of all the features created...\nprint(FEATURES)","d5046c48":"X[FEATURES].sample(5)","592e4456":"X.shape, X[FEATURES].shape","b59ab10c":"def SMAPE(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)","88a2b8a7":"# smape val: 10.24285605422652\n\nxgb_params = {\n                'n_estimators': 4492,\n                'learning_rate': 0.01,\n                'subsample': 1.0,\n                'colsample_bytree': 0.2,\n                'max_depth': 15,\n                'gamma': 1.0328829988080024,\n                'reg_alpha': 100,\n                'reg_lambda': 93 }\n\n# xgb_params['tree_method'] = 'gpu_hist'\n# xgb_params['predictor'] = 'gpu_predictor'","276f4de4":"# https:\/\/www.kaggle.com\/zhangcheche\/tps-2022-jan-xgboost-baseline\n# smape val: 4.585028665637415\n\nxgb_params = {\n    \"n_estimators\": 500, # 3082,\n    \"learning_rate\": 0.01,\n    \"subsample\": 0.7,\n    \"colsample_bytree\": 0.8,\n    \"max_depth\": 15,\n    \"gamma\": 4.475257278569414,\n    \"reg_alpha\": 95,\n    \"reg_lambda\": 100,\n}\n","e079d7ba":"final_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\n\nfeat_import = np.zeros(len(FEATURES))\n\nkf = KFold(n_splits=Config.NFOLDS, shuffle=True, random_state=Config.seed)\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X = X, y = y)):\n\n    print(10*\"=\", f\"Fold={fold}\", 10*\"=\")\n    start_time = time.time()\n    x_train = X.loc[train_idx, :]\n    x_valid = X.loc[valid_idx, :]\n    \n    y_train = y[train_idx]\n    y_valid = y[valid_idx]\n    model = XGBRegressor(**xgb_params)\n\n    model.fit(x_train, y_train,\n          early_stopping_rounds=200,\n          eval_set=[(x_valid, y_valid)],\n          eval_metric='rmse',\n          verbose=0)\n\n    \n    preds_valid = model.predict(x_valid)\n    final_valid_predictions.update(dict(zip(valid_idx, preds_valid)))\n    \n    smape = SMAPE(y_valid,  preds_valid)\n    scores.append(smape)\n    \n    test_preds = model.predict(X_test)\n    final_test_predictions.append(test_preds)\n    \n    feat_import += model.feature_importances_\n\n    run_time = time.time() - start_time\n    print(f\"Fold={fold}, SMAPE: {smape:.8f}, Run Time: {run_time:.2f}\")\n","3bac5bb5":"mean_score = np.mean(scores)\n\nprint(f\"Scores -> mean: {np.mean(scores):.8f}, std: {np.std(scores):.8f}\")","4654d3cb":"y_test = model.predict(x_test)","28a70256":"y_test","961bad7b":"pred = np.exp(y_test)","bb897e39":"sample_submission.num_sold = pred\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission","0f414163":"<div style=\"background-color:rgba(128, 0, 128, 0.6);border-radius:5px;display:fill\"><h1 style=\"text-align: center;padding: 12px 0px 12px 0px;\">Feature Engineering<\/h1>\n<\/div>","e7af3f35":"## Time Features","96a839b2":"# Extract Target and Drop Unused Columns","56432941":"<div style=\"background-color:rgba(128, 0, 128, 0.6);border-radius:5px;display:fill\"><h1 style=\"text-align: center;padding: 12px 0px 12px 0px;\">SMAPE Error Function<\/h1>\n<\/div>","7101f914":"<div style=\"background-color:rgba(128, 0, 128, 0.6);border-radius:5px;display:fill\"><h1 style=\"text-align: center;padding: 12px 0px 12px 0px;\">Inference on Test Data<\/h1>\n<\/div>","b8b609e4":"# Label Encode Categorical Features\n\nSource: https:\/\/www.kaggle.com\/zhangcheche\/tps-2022-jan-xgboost-baseline","e6d6249e":"<div style=\"background-color:rgba(128, 0, 128, 0.6);border-radius:5px;display:fill\"><h1 style=\"text-align: center;padding: 12px 0px 12px 0px;\">Configuration<\/h1>\n<\/div>\n","edceee95":"<div style=\"background-color:rgba(128, 0, 128, 0.6);border-radius:5px;display:fill\"><h1 style=\"text-align: center;padding: 12px 0px 12px 0px;\">XGBoost Model<\/h1>\n<\/div>\n\n# References\n\n- https:\/\/www.kaggle.com\/zhangcheche\/tps-2022-jan-xgboost-baseline\n- https:\/\/www.kaggle.com\/cv13j0\/tps-jan22-quick-eda-xgboost\n- [\ud83c\udfa2 Introduction to Exploratory Data Analysis\n](https:\/\/www.kaggle.com\/robikscube\/introduction-to-exploratory-data-analysis\/)\n- [The Ultimate Pandas Introduction [2022]](https:\/\/www.kaggle.com\/robikscube\/the-ultimate-pandas-introduction-2022)\n","3d15dc6a":"# Transform Target","62267d41":"<div style=\"background-color:rgba(128, 0, 128, 0.6);border-radius:5px;display:fill\"><h1 style=\"text-align: center;padding: 12px 0px 12px 0px;\">Load Train\/Test Data<\/h1>\n<\/div>","202b49c0":"<div style=\"background-color:rgba(128, 0, 128, 0.6);border-radius:5px;display:fill\"><h1 style=\"text-align: center;padding: 12px 0px 12px 0px;\">XGBoost Model<\/h1>\n<\/div>\n","e38ba63d":"<div style=\"background-color:rgba(128, 0, 128, 0.6);border-radius:5px;display:fill\"><h1 style=\"text-align: center;padding: 12px 0px 12px 0px;\">Train with Cross Validation<\/h1>\n<\/div>","efc69dea":"<div style=\"background-color:rgba(128, 0, 128, 0.6);border-radius:5px;display:fill\"><h1 style=\"text-align: center;padding: 12px 0px 12px 0px;\">Visualizations<\/h1>\n<\/div>","4a906fa1":"<div style=\"background-color:rgba(128, 0, 128, 0.6);border-radius:5px;display:fill\"><h1 style=\"text-align: center;padding: 12px 0px 12px 0px;\">Explore the Data<\/h1>\n<\/div>","e9e47777":"<div style=\"background-color:rgba(128, 0, 128, 0.6);border-radius:5px;display:fill\"><h1 style=\"text-align: center;padding: 12px 0px 12px 0px;\">Import Libraries<\/h1>\n<\/div>","23b53799":"<div style=\"background-color:rgba(128, 0, 128, 0.6);border-radius:5px;display:fill\"><h1 style=\"text-align: center;padding: 12px 0px 12px 0px;\">Scores<\/h1>\n<\/div>\n\nsmape val: 10.24285605422652\n","a0c4dada":"<div style=\"background-color:rgba(128, 0, 128, 0.6);border-radius:5px;display:fill\"><h1 style=\"text-align: center;padding: 12px 0px 12px 0px;\">Submission<\/h1>\n<\/div>\n","4ed80092":"## Holidays\n\nSource: https:\/\/www.kaggle.com\/zhangcheche\/tps-2022-jan-xgboost-baseline"}}