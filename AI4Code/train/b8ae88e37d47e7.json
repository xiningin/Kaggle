{"cell_type":{"7eb516d7":"code","ed9c28f9":"code","b7ed5432":"code","aff920dc":"code","21806748":"code","9a3a394f":"code","7f541f55":"code","5db6d3dc":"code","ac4979ac":"code","30408122":"code","c524abec":"code","5259ab41":"code","77a775ee":"code","efb3ab1a":"code","5c5747b2":"code","6aae3784":"code","aef13ea9":"code","239e6db1":"code","4759192a":"code","176687e8":"code","0d149759":"code","ee517090":"code","8187c499":"code","9f60a8cd":"code","3695fd0a":"code","5b698d0b":"code","8ab090e0":"code","01a03ce3":"markdown","a3541166":"markdown","1fe37b2e":"markdown","e59bf551":"markdown","59504f0a":"markdown","1b6f7ca7":"markdown","789e11f0":"markdown","55661819":"markdown","9ed60b89":"markdown","6623c3a2":"markdown","baa54acf":"markdown","51eafe7a":"markdown","90542542":"markdown","9cb5049f":"markdown","a85a2049":"markdown","d0ed71be":"markdown","1eca23be":"markdown","9e3e6892":"markdown","0081c73f":"markdown","ed9ed651":"markdown","c5e2836f":"markdown","05a757c0":"markdown","ad75d972":"markdown","ae8d09ce":"markdown","2e4fda3f":"markdown"},"source":{"7eb516d7":"import pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import mutual_info_classif\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","ed9c28f9":"df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/train.csv', index_col='Id').reset_index(drop=True)\ndf.head()","b7ed5432":"X = df.drop(['Cover_Type'], axis=1)\ny = df['Cover_Type']","aff920dc":"y.value_counts()","21806748":"class5_index = None\nfor index, val in enumerate(y):\n    if val == 5:\n        class5_index = index\n        \nX = X.drop([class5_index])\ny = y.drop([class5_index])","9a3a394f":"# y.unique()","7f541f55":"X_train, X_val, y_train, y_val = train_test_split(\n    X,\n    y,\n    test_size=0.2,\n    random_state=42,\n    stratify=y\n)\n# df_train, df_val = train_test_split(df, test_size=0.2, random_state=42)","5db6d3dc":"# df_train = df_train.reset_index(drop=True)\n# df_train.head()\nX_train","ac4979ac":"X_train.info()","30408122":"numerical = []\ncategorical = []\nfor col in X_train.columns:\n    if X_train[col].nunique() <= 2:\n        categorical.append(col)\n    else:\n        numerical.append(col)\n        \ncategorical","c524abec":"X_numerical = X_train[numerical]\n\nX_numerical.head()","5259ab41":"X_numerical.describe()","77a775ee":"X_numerical.isna().any()","efb3ab1a":"for c in numerical:\n    plt.hist(X_numerical[c], bins=100)\n    plt.xlabel(c)\n    plt.show()","5c5747b2":"pearson_corr = X_numerical.corr(method='pearson').abs()\n\nfig, ax = plt.subplots(figsize=(6, 6))\n\nplt.title(\"Correlation Plot\\nAbsolute value of Pearson's Correlation Coefficient\\n\\n\")\nsns.heatmap(pearson_corr,\n            cmap=sns.diverging_palette(230, 10, as_cmap=True),\n            square=True,\n            vmin=0,\n            vmax=1,\n            ax=ax)\nplt.show()","6aae3784":"spearman_corr = X_numerical.corr(method='spearman').abs()\n\nfig, ax = plt.subplots(figsize=(6, 6))\n\nplt.title('Correlation Plot\\nAbsolute value of Spearman Correlation Coefficient\\n\\n')\nsns.heatmap(spearman_corr,\n            cmap=sns.diverging_palette(230, 10, as_cmap=True),\n            square=True,\n            vmin=0,\n            vmax=1,\n            ax=ax)\nplt.show()","aef13ea9":"anova_f_values = f_classif(X_numerical, y_train)[0]\n\nlinear_corr = pd.Series(anova_f_values, index=X_numerical.columns)\nlinear_corr","239e6db1":"non_linear_corr = X_numerical.corrwith(y_train, method='kendall')\nnon_linear_corr","4759192a":"X_categorical = X_train[categorical]\n\nX_categorical.head()","176687e8":"X_categorical.isna().any()","0d149759":"pearson_corr = X_categorical.corr(method='pearson').abs()\n\nfig, ax = plt.subplots(figsize=(6, 6))\n\nplt.title(\"Correlation Plot\\nAbsolute value of Pearson's Correlation Coefficient\\n\\n\")\nsns.heatmap(pearson_corr,\n            cmap=sns.diverging_palette(230, 10, as_cmap=True),\n            square=True,\n            vmin=0,\n            vmax=1,\n            ax=ax)\nplt.show()","ee517090":"spearman_corr = X_categorical.corr(method='spearman').abs()\n\nfig, ax = plt.subplots(figsize=(6, 6))\n\nplt.title('Correlation Plot\\nAbsolute value of Spearman Correlation Coefficient\\n\\n')\nsns.heatmap(spearman_corr,\n            cmap=sns.diverging_palette(230, 10, as_cmap=True),\n            square=True,\n            vmin=0,\n            vmax=1,\n            ax=ax)\nplt.show()","8187c499":"X_categorical['Soil_Type7'].value_counts()","9f60a8cd":"X_categorical['Soil_Type15'].value_counts()","3695fd0a":"X_categorical = X_categorical.drop(['Soil_Type7', 'Soil_Type15'], axis=1)","5b698d0b":"chi_square = chi2(X_categorical, y_train)[0]\n\nchi_square = pd.Series(chi_square, index=X_categorical.columns)\nchi_square","8ab090e0":"mutual_info = mutual_info_classif(X_categorical, y_train, discrete_features=True, random_state=42)\n\nmutual_info = pd.Series(mutual_info, index=X_categorical.columns)\nmutual_info","01a03ce3":"### Feature Redundance\n\nNow, we will find redundant categorical features.\n\nWe will try to find linear correlation between features using Pearson's correlation coefficient and non-linear correlation using Spearman's correlation.\n\nFor both we will plot a correlation matrix to make the result readable.\n\nSource: https:\/\/machinelearningmastery.com\/how-to-use-correlation-to-understand-the-relationship-between-variables\/","a3541166":"The training data does not have any missing values but the testing data can. So, we will fill the missing values with the most frequent value in the feature.","1fe37b2e":"The numerical features of trainig data have no missing values.\n\nEven though the test data might have. So, we will in advance decide how to fill any missing values if found.\n\nThe methodology used for numerical features is:\n- Fill with mean if the feature has Gaussian distribution\n- Fill with meadian otherwise\n\nTo find if the feature is Gaussian or not we will plot histograms of each feature.","e59bf551":"The more the ANOVA F-value the more important the feature is in predicting the result.","59504f0a":"The closer the value to 1 the more important the feature is in predicting the result.\n\nIf and which features to remove we will decide by training some simple linear models after removing the features one by one based on their correlation values and evaluating their scores.","1b6f7ca7":"The more the Mutual Information value the more important the feature is in predicting the result.\n\nIf and which features to remove we will decide by training some simple linear models after removing the features one by one based on their correlation values and evaluating their scores.","789e11f0":"Therefore, features `Wilderness_Area{n}` and `Soil_Type{n}` are categorical features.","55661819":"`Wilderness_Area1` and `Wilderness_Area3` are somewhat correalated so we may decide to remove one of those.\n\nThe other important observation we can make is that the plot along features `Soil_Type7` and `Soil_Type15` is weird. To understand the reason for it we can check the values they have.","9ed60b89":"### Feature Selection\n\nNow, we will try to find feature relevance with the target.\n\nFor this we will use Chi-Squared test and Mutual Information.\n\nSource: https:\/\/machinelearningmastery.com\/feature-selection-with-real-and-categorical-data\/","6623c3a2":"## Categorical Features","baa54acf":"From the plots, we can see that only `Elevation` and `Hillshade_3pm` are Gaussian-like. So, we will fill missing values of those with mean.","51eafe7a":"Both of these features have only 1 value, so it is better to remove them.","90542542":"# Splitting into training and validation sets\n\nSplitting before EDA ensures that the validation dataset does not contribute to the decision making and is only used for validation.","9cb5049f":"# Exploratory Data Analysis (EDA)","a85a2049":"On first sight it looks like the dataset only has numerical features but some features (like `Wilderness_Area{n}`) look like they are actually categorical but encoded.\n\nWe can confirm this.","d0ed71be":"None of the features are correlated with each other.","1eca23be":"## Numerical Features","9e3e6892":"### Feature Redundance\n\nNext, we will look at correlation between features to find if there are any redundant features.\n\nWe will try to find linear correlation between features using Pearson's correlation coefficient and non-linear correlation using Spearman's correlation.\n\nFor both we will plot a correlation matrix to make the result readable.\n\nSource: https:\/\/machinelearningmastery.com\/how-to-use-correlation-to-understand-the-relationship-between-variables\/","0081c73f":"The more the Chi-squared value the more important the feature is in predicting the result.","ed9ed651":"### Missing Values","c5e2836f":"### Feature Selection\n\nNow, we will try to find feature relevance with the target.\n\nFor this we will use ANOVA F-value to find linear relationship and Kendall's $\\tau$ coefficient for non-linear relationship.\n\nSource: https:\/\/machinelearningmastery.com\/feature-selection-with-real-and-categorical-data\/","05a757c0":"In this notebook I will try to gain insights from data. This notebook will help in later deciding what model would be best for this problem.","ad75d972":"The classes are very imbalanced. Class 5 only has 1 sample, so in my opinion it would be better to just remove it.\n\nFor more discussion on this: https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/293064","ae8d09ce":"# Initialization","2e4fda3f":"### Missing Values"}}