{"cell_type":{"29a4ec95":"code","495b10f8":"code","188b1f79":"code","79093e19":"code","e2ee0a04":"code","bfd3bca4":"code","cfe613d7":"code","ef84752c":"code","b800513a":"code","d7e38c17":"code","486c4909":"code","bccec36e":"code","4a9ff82d":"code","feae116e":"code","b47fd286":"code","5b549f7f":"code","9c80cbb7":"code","222f9a9b":"code","36721bf1":"markdown","d51159e1":"markdown","8c61ab7c":"markdown","0e2c53c7":"markdown","ad03b9f6":"markdown","5ced9e1e":"markdown"},"source":{"29a4ec95":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf","495b10f8":"# Ploting images with landmarks\ndef plot_image_landmarks(img_array, df_landmarks, index):\n    plt.imshow(img_array[index, :, :, 0], cmap = 'gray')\n    plt.scatter(df_landmarks.iloc[index][0: -1: 2], df_landmarks.iloc[index][1: : 2], c = 'y')\n    plt.show()","188b1f79":"features = np.load('..\/input\/face-images-with-marked-landmark-points\/face_images.npz')\nfeatures = features.get(features.files[0]) # images\nfeatures = np.moveaxis(features, -1, 0)\nfeatures = features.reshape(features.shape[0], features.shape[1], features.shape[1], 1)","79093e19":"keypoints = pd.read_csv('..\/input\/face-images-with-marked-landmark-points\/facial_keypoints.csv')\nkeypoints.head()","e2ee0a04":"# Cleaing data\nkeypoints = keypoints.fillna(0)\nnum_missing_keypoints = keypoints.isnull().sum(axis = 1)\nnum_missing_keypoints","bfd3bca4":"new_features = features[keypoints.index.values, :, :, :] #Nums of rows,w, H, Channels\nnew_features = new_features \/ 255\nkeypoints.reset_index(inplace = True, drop = True)","cfe613d7":"plot_image_landmarks(new_features, keypoints, 3)","ef84752c":"x_train, x_test, y_train, y_test = train_test_split(new_features, keypoints, test_size=0.2)","b800513a":"from tqdm.keras import TqdmCallback\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, LeakyReLU, BatchNormalization\nfrom tensorflow.keras.initializers import glorot_uniform\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.optimizers import SGD, Adam","d7e38c17":"img_size = 96","486c4909":"model = Sequential()\n\nmodel.add(Input(shape=(img_size, img_size, 1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32, (3,3), padding=\"same\",kernel_initializer=glorot_uniform()))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, (3,3), padding=\"same\",kernel_initializer=glorot_uniform()))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128, (3,3), padding=\"same\",kernel_initializer=glorot_uniform()))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))  \n\nmodel.add(Flatten())\nmodel.add(Dense(256,kernel_initializer=glorot_uniform()))\nmodel.add(LeakyReLU(alpha=0.1))\n\nmodel.add(Dropout(0.5)) \n\nmodel.add(Dense(64,kernel_initializer=glorot_uniform()))\nmodel.add(LeakyReLU(alpha=0))\n\nmodel.add(Dense(30,kernel_initializer=glorot_uniform()))\n\nmodel.summary()\nmodel.compile(loss='mean_squared_error', optimizer=Adam(), metrics=['mean_squared_error'])","bccec36e":"BATCH_SIZE = 100\nEPOCHS = 150","4a9ff82d":"history = model.fit(\n    x_train, y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_data=(x_test, y_test),\n    shuffle=True,\n    verbose=1,\n)\n","feae116e":"plt.plot(history.history['mean_squared_error'], label='MSE (training data)')\nplt.plot(history.history['val_mean_squared_error'], label='MSE (validation data)')\nplt.title('MSE for Facial keypoints')\nplt.ylabel('MSE value')\nplt.xlabel('No. epoch')\nplt.legend(loc=\"upper left\")\nplt.show()","b47fd286":"y_pred = model.predict(x_test)\ny_pred","5b549f7f":"def plot_img_preds(images, truth, pred, index):\n    plt.imshow(images[index, :, :, 0], cmap = 'gray')\n    \n    t = np.array(truth)[index]\n    plt.scatter(t[0::2], t[1::2], c = 'y')\n    \n    p = pred[index, :]\n    plt.scatter(p[0::2], p[1::2], c = 'r')\n    \n    plt.show()","9c80cbb7":"plot_img_preds(x_test, y_test, y_pred, 3)","222f9a9b":"plot_img_preds(x_test, y_test, y_pred, 18)","36721bf1":"# **Training Model**","d51159e1":"# **Reading data and Preprocessing**","8c61ab7c":"**kernel_initializer** in Keras : Initializers define the way to set the initial random weights of Keras layers.\n\n**glorot_uniform()** : It draws samples from a uniform distribution within [-limit, limit] where limit is sqrt(6 \/ (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.\n\nWe are using **the Mean Squared Error** as we are performing a regression task. A small learning rate is always good if you have a good amount of data","0e2c53c7":"# **Introduction**\n\nFace Detection Systems have great uses in today\u2019s world which demands security, accessibility or joy! Today, we will be building a model that can plot 15 key points on a face.\n\nFace Landmark Detection models form various features we see in social media apps. The face filters you find on Instagram are a common use case. The algorithm aligns the mask on the image keeping the face landmarks as base points.\n\nIn this notebook, we'll develop a model which marks keypoints on a given image of a human face. We'll build a Convolutional Neural Network which takes an image and returns a array of keypoints.\n\nWe'll require a GPU Hardware accelerator for training the model. Change the runtime type to GPU by going to Tools > Change Runtime Type > Hardware Accelerator > GPU.\n\n![](https:\/\/miro.medium.com\/max\/2000\/1*qNNr1hrFoaeAWru7VI0SbQ.png)","ad03b9f6":"# **Model Evaluation**","5ced9e1e":"# **Our Model**"}}