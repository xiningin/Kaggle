{"cell_type":{"5f9a5b81":"code","93f7d933":"code","9762d98e":"code","3ec8732c":"code","bedeb853":"code","3f2b8659":"code","a0eff5c2":"code","1e851aef":"code","018ed41d":"code","c16c9514":"code","feb498ce":"code","1afac1d5":"code","0d7637aa":"code","272fb354":"code","7c7d71f0":"code","902bfe93":"code","fe154038":"code","245e8dbd":"code","c36a6e8e":"code","53639bf3":"code","ac205e68":"code","401552da":"code","a6cc8683":"code","d94f3ace":"code","8f83838b":"code","34d266ac":"code","aca7724a":"code","d0c4aca6":"code","9ace00cd":"code","503ec297":"code","59eee6a8":"code","bd391e62":"code","6aaa842b":"code","65484217":"code","df1d3930":"markdown","83588935":"markdown","b26f46b3":"markdown","4641fb61":"markdown","b03aa799":"markdown","ed09117b":"markdown","79336279":"markdown","d3492beb":"markdown","bea2eaa2":"markdown","cef29d1d":"markdown","6d165bd0":"markdown","ad6435a0":"markdown","e04ac678":"markdown","ab6d467a":"markdown"},"source":{"5f9a5b81":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","93f7d933":"# General\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom skimage.filters import threshold_otsu\nfrom skimage import io , transform\nimport numpy as np\nfrom glob import glob\nfrom scipy import misc\nfrom matplotlib.patches import Circle,Ellipse\nfrom matplotlib.patches import Rectangle\nimport os\nfrom PIL import Image\n\n\n# For Model Works\nimport keras\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport gzip\n%matplotlib inline\nfrom keras.layers import Input,Conv2D,MaxPooling2D,UpSampling2D\nfrom keras.models import Model\nfrom keras.optimizers import RMSprop\nfrom keras.layers.normalization import BatchNormalization","9762d98e":"train_data = glob('..\/input\/fingerprint-dataset-for-fvc2000-db4-b\/dataset_FVC2000_DB4_B\/dataset\/train_data\/*')\nlen(train_data)","3ec8732c":"# load images and store it in list after conversion\nimages = []\ndef read_images(data):\n    for i in range(len(data)):\n        img = io.imread(data[i])\n        img = transform.resize(img,(224,224))\n        images.append(img)\n    return images","bedeb853":"read_images(train_data)","3f2b8659":"# Converting To Array\nimages_arr = np.asarray(images)\nimages_arr = images_arr.astype('float32')\nimages_arr.shape","a0eff5c2":"# shape of data\nprint('Dataset Shape : {shape}'.format(shape = images_arr.shape))","1e851aef":"print('First 3 Images:')\n# Display the first image in training data\nfor i in range(3):\n    plt.figure(figsize=[3, 3])\n    train_img = np.reshape(images_arr[i], (224,224))\n    plt.imshow(train_img, cmap='gray')\n    plt.show()","018ed41d":"# converting image of size (224 , 224 ) to matrix of dimension (224 , 224 ,1)\nimages_arr = images_arr.reshape(-1 , 224 ,224 , 1)\nprint('Converted Images Shape :' , images_arr.shape )","c16c9514":"# Checking Data Type\nprint('Data Type Of Images : ' , images_arr.dtype)","feb498ce":"# Rescaling Images i.e dividing data\/max(data_max) i.e pixel\/ max_pixel value\n\nimages_max = np.max(images_arr)\n      \nimages_arr = images_arr \/ images_max\nprint(images_arr[1])","1afac1d5":"# splitting data using sklearn's train_test_split\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_X,valid_X,train_ground_truth,valid_ground_truth = train_test_split(images_arr ,\n                                                      images_arr ,\n                                                      test_size = 0.2,\n                                                      random_state = 15) # vaidation_data\n","0d7637aa":"len(train_X) , len(valid_X)","272fb354":"len(train_ground_truth) , len(valid_ground_truth)","7c7d71f0":"# Defining Hyper Parameters\nbatch_size = 128\nepochs = 200\ninChannel = 1\nx, y = 224, 224\ninput_img = Input(shape = (x, y, inChannel))","902bfe93":"def autoencoder(input_img):\n    #encoder\n    #input = 28 x 28 x 1 (wide and thin)\n    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) #28 x 28 x 32\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1) #14 x 14 x 32\n    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1) #14 x 14 x 64\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2) #7 x 7 x 64\n    encoder_last = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2) #7 x 7 x 128 (small and thick)\n\n    #decoder\n    conv4 = Conv2D(128, (3, 3), activation='relu', padding='same')(encoder_last) #7 x 7 x 128\n    up1 = UpSampling2D((2,2))(conv4) # 14 x 14 x 128\n    conv5 = Conv2D(64, (3, 3), activation='relu', padding='same')(up1) # 14 x 14 x 64\n    up2 = UpSampling2D((2,2))(conv5) # 28 x 28 x 64\n    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up2) # 28 x 28 x 1\n    return decoded","fe154038":"auto_encoder = Model(input_img, autoencoder(input_img))","245e8dbd":"auto_encoder.compile(loss='mean_squared_error', optimizer = RMSprop())\nauto_encoder.summary()","c36a6e8e":"encoder_train = auto_encoder.fit(train_X ,\n                            train_ground_truth , \n                            batch_size = batch_size , \n                            epochs = epochs ,\n                            verbose = 1,\n                            validation_data =(valid_X, valid_ground_truth))","53639bf3":"loss = encoder_train.history['loss']\nval_loss = encoder_train.history['val_loss']\nepochs = range(200)\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","ac205e68":"pred = auto_encoder.predict(valid_X)","401552da":"# Visualizing Predictions\n\nplt.figure(figsize=(20, 4))\nprint(\"Test Images\")\nfor i in range(5):\n    plt.subplot(1, 5, i+1)\n    plt.imshow(valid_ground_truth[i, ..., 0], cmap='gray')\nplt.show()    \nplt.figure(figsize=(20, 4))\nprint(\"Reconstruction of Test Images\")\nfor i in range(5):\n    plt.subplot(1, 5, i+1)\n    plt.imshow(pred[i, ..., 0], cmap='gray')  \nplt.show()","a6cc8683":"real= glob('..\/input\/fingerprint-dataset-for-fvc2000-db4-b\/dataset_FVC2000_DB4_B\/dataset\/real_data\/*')\nlen(real)","d94f3ace":"test_images = []\ndef read_images(data):\n    for i in range(len(data)):\n        img = io.imread(data[i])\n        img = transform.resize(img,(224,224))\n        test_images.append(img)\n    return test_images","8f83838b":"real_images = read_images(real)","34d266ac":"# checking len\nlen(real_images)","aca7724a":"# Converting to numpy array\nreal_data = np.asarray(real_images)\nreal_data = real_data.astype('float32')\nreal_data[1]","d0c4aca6":"# shape of dataset\nreal_data.shape","9ace00cd":"# rescale\nreal_data = real_data \/ np.max(real_data)\nreal_data[1]","503ec297":"# reshaping as input for model\nreal_data = real_data.reshape(-1, 224,224, 1)\nreal_data.shape","59eee6a8":"pred = auto_encoder.predict(real_data)","bd391e62":"plt.figure(figsize=(20, 4))\nprint(\"Test Real World Images\")\nfor i in range(5):\n    plt.subplot(1, 5, i+1)\n    plt.imshow(real_data[i, ..., 0], cmap='gray')\nplt.show()    \nplt.figure(figsize=(20, 4))\nprint(\"Reconstruction of Real World Images\")\nfor i in range(5):\n    plt.subplot(1, 5, i+1)\n    plt.imshow(pred[i, ..., 0], cmap='gray')  \nplt.show()","6aaa842b":"print('Thanks For Watching')","65484217":"autoencoder = auto_encoder.save_weights('autoencoder.h5')","df1d3930":"**As Can Be Seen , Our Model Preety Close To Predicting Actual Output Of Validation Set(Not Seen By Model) , Lets See How It Performs On real World Data(Test_data)**","83588935":"# Prediction On Real Data","b26f46b3":"# Spltting data For Training And Testing\n\n#### for this task, you don't need training and testing labels. That's why you will pass the training images twice. Your training images will both act as the input , as well as the ground truth similar to the labels you have in the classification task.\n\n>train_X , valid_X - training and testing data \n\n>train_ground_truth, valid_ground_truth  - truth labels for classification .\n\n\n","4641fb61":"# Plotting Loss And Val Loss","b03aa799":"# Optional Saving Model \n\n> autoencoder.load_weights('autoencoder.h5') # to load after saving\n\n","ed09117b":"## Loading Data","79336279":"# Predict On Validation Data","d3492beb":"# Predicting On Real World Data","bea2eaa2":"# Data Exploration","cef29d1d":"# Auto Encoder \n\n## Structure Of Auto Encoder\n\n### Encoder\n\n>The first layer will have 32 filters of size 3 x 3, followed by a downsampling (max-pooling) layer,\n\n>The second layer will have 64 filters of size 3 x 3, followed by another \ndownsampling layer,\n\n>The final layer of encoder will have 128 filters of size 3 x 3.\n\n\n### Decoder\n\n>The first layer will have 128 filters of size 3 x 3 followed by an upsampling layer,\n\n>The second layer will have 64 filters of size 3 x 3 followed by another upsampling layer,\n\n>The final layer of encoder will have one filter of size 3 x 3.\n\n\n### The max-pooling layer will downsample the input by two times each time you use it, while the upsampling layer will upsample the input by two times each time it is used.\n\n\n> Note: The number of filters, the filter size, the number of layers, number of epochs you train your model, are all hyperparameters and should be decided  by experimenting , feel free to change:)\n","6d165bd0":"# Loading Modules","ad6435a0":"# Data PreProcessing","e04ac678":"# Data Pre Processing","ab6d467a":"**As Can Be Seen Our Model Is Doing Great On Real World Images Too , Which Is Very Good As Now This Model Can Be Use To Create Very Close Looking FingerPrint , It Is Thus Usefull in Cybersecurity , And Forensics Too\nThanks For Viewing :)**\n# **@Purnendu**"}}