{"cell_type":{"bb18c117":"code","447aa369":"code","51925977":"code","ca1471f1":"code","da7f76aa":"code","dc625a60":"code","661820e2":"code","12ef3a7b":"code","41b8d6f3":"code","83997c6a":"code","d42c720d":"markdown","51fdf077":"markdown","6d505d96":"markdown","87a45375":"markdown","2bbef6d0":"markdown","0f15d770":"markdown","77e758ac":"markdown"},"source":{"bb18c117":"# Imported libraries for dataframes, arrays, and plots\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport seaborn as sns\nfrom urllib.request import urlretrieve\nfrom scipy import sparse\n\n# Imported libraries for preprocessing and modeling\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import normalize\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import NMF\nfrom sklearn.manifold import TSNE","447aa369":"# Import the Wikipedia's articles dataset\nurl_words = 'https:\/\/raw.githubusercontent.com\/jadoonengr\/DataCamp-Notes\/master\/datasets\/Wikipedia%20articles\/wikipedia-vocabulary-utf8.txt'\nwords = np.genfromtxt(url_words, dtype='str')\n\nurl_articles = 'https:\/\/raw.githubusercontent.com\/jadoonengr\/DataCamp-Notes\/master\/datasets\/Wikipedia%20articles\/wikipedia-vectors.csv'\nurlretrieve(url_articles, 'articles.csv')\narticles = pd.read_csv('articles.csv', sep=',')\narticles = articles.drop(articles.columns[0], axis=1)","51925977":"# manipulate and transforme the articles dataset\narticles = articles.transpose()\narticles.columns = list(words)\nart_csr = sparse.csr_matrix(articles)","ca1471f1":"# SVD dimentional reduction\ndef svd_reduction(data_csr, c, svd_index, iteration):\n    svd = TruncatedSVD(n_components=c, n_iter=iteration)\n    features = pd.DataFrame(svd.fit_transform(data_csr), index = svd_index)\n    compo_var = svd.explained_variance_\n    compo_var_ratio = svd.explained_variance_ratio_\n    print('the '+str(c)+' constructed components explain over: {}'.format(round(compo_var_ratio.sum()*100, 2))+'%')\n    return features\nsvd_features = svd_reduction(data_csr=art_csr, c=50, svd_index = list(articles.index), iteration=20)","da7f76aa":"# Elbow graph for optimal number of clusters\ndef elbow_graph(data, rang_nclusters):\n    inertias = []\n    for k in rang_nclusters:\n        model = KMeans(n_clusters=k)\n        model.fit(data)\n        inertias.append(model.inertia_)\n    plt.plot(rang_nclusters, inertias, 'o--')\n    plt.xlabel('Number of clusters used')\n    plt.ylabel('Amount of inertia')\n    plt.title('The Elbow graph for optimal number of clusters')\n    plt.show()\nelbow_graph(data=svd_features, rang_nclusters=range(5,20))","dc625a60":"# Fit a Kmeans clustering on svd_features\ndef mult_KMeans(svd_data, label_obs, rang_nclusters):\n    articles_clus = pd.DataFrame({'articles':label_obs})\n    for k in rang_nclusters:\n        scaler = StandardScaler()\n        kmeans=KMeans(n_clusters=k)\n        pipeline=make_pipeline(scaler, kmeans)\n        pred_clus = pipeline.fit_predict(svd_data)\n        articles_clus.insert(k-rang_nclusters[0]+1, 'clus_k='+str(k), pred_clus, True)\n        # Plot histogram for each clustering\n        plt.figure(figsize=(20,20))\n        sns.countplot(x='clus_k='+str(k), data=articles_clus)\n        plt.xlabel('clusters', fontsize=18)\n        plt.ylabel('number of articles in each cluster', fontsize=18)\n        plt.title('KMEANS Clustering with k='+str(k), fontsize=18)\n        plt.show()\n    articles_clus.index=list(articles_clus.iloc[:,0])\n    articles_clus = articles_clus.drop(articles_clus.columns[0], axis=1)\n    return articles_clus\narticles_clus = mult_KMeans(svd_data=svd_features, label_obs=list(articles.index), rang_nclusters=range(5,20))","661820e2":"# Fit a TSNE dimentional reduction to get component for better visualization\ntsne_model = TSNE(n_components=2, perplexity=10, learning_rate=100)\ntsne_features = tsne_model.fit_transform(svd_features)\ntsne_features = pd.DataFrame(tsne_features, index = list(articles.index), columns=['cp0', 'cp1'])","12ef3a7b":"# Create df dataframe that contain the components and the differents results of KMeans clustering\nfor col in articles_clus.columns:\n    tsne_features[col]=articles_clus.loc[:,col]\ndf = tsne_features\nxs = df.iloc[:,0]\nys = df.iloc[:,1]\n\n# Visualize the clusters in a scatter plot using TSNE components:\nfor clus in list(df.columns[2:]):\n    sns.lmplot(x='cp0', y='cp1', data=df, fit_reg=False, hue=clus, legend=False)\n    for x, y, comp in zip(xs, ys, list(df.index)):\n        plt.annotate(comp, (x,y), fontsize=7, alpha=0.8)\n    plt.legend(loc='upper right')\n    plt.show()","41b8d6f3":"# Function of the recommander System\ndef syst_recommendation(data, c, k, ind, article): # data must be a csr format and ind is a list of index of the pandas data frame\n    model = NMF(n_components=c)\n    model.fit(data)\n    nmf_features = model.transform(data)\n    features = pd.DataFrame(nmf_features, index = ind) # ind here is the list of article's name\n    norm_features = normalize(features)\n    df = pd.DataFrame(norm_features, index=ind)\n    current_article = df.loc[article]\n    similiraty = df.dot(current_article)\n    print(similiraty.nlargest(k))","83997c6a":"# Fit a recommandation example of article 'Social search'\nart_csr = sparse.csr_matrix(articles)    \nsyst_recommendation(data = art_csr, c=10, k=6, ind=list(articles.index), article = 'Social search')\nsyst_recommendation(data = art_csr, c=10, k=6, ind=list(articles.index), article = 'Cristiano Ronaldo')","d42c720d":"We build and elbow function that will help finding the optimal number of clusters, or at least a range of optimal number of clusters.","51fdf077":"The svd_reduction need the following paramters:\n* data_csr : A dataset of type csr, a sparse data can be transformed to a csr dataset\n* c : The number of components we need for our dimentional reduction\n* svd_index : The list of index of the new dataset build on the new components\n* iteration : Number of iteration for the dimentional reduction","6d505d96":"**This Project aims to build a system recommadation of articles. We have a document with 60 articles of differents fields, the purpose of the work is to search for articles that are similar to a specific one and that we can suggest to the reader.\nThe project is established in two parts:**\n1. The first part looks for a clustering of articles, based on the ferquency of 13125 words in each of the 60 articles.\n\nWe have used the SVD dimensional reduction for the articles dataset. While PCA is used for all possible kind of numerical dataset, SVD is a dimensional reduction used for sparse dataset. The components (50 components that extract 94% of information from the original dataset, this new dataset is named in the code bellow svd_features(60 article, 50 features)) we get are used as the new features for the 60 articles, and based on them we will try to fit multiple KMEANS clustering ( Multiple values of K).\nThe next step in that part, is apply TSNE dimensional reduction on the svd_features, since TSNE gives better visualizations of observations. Our purpose here is to visualize the cluster constructed previously.\n2. The second part aims to exploit the NMF dimentional reduction to build that recommendation system.\n\nAt this part of the project, we build a function 'syst_recommendation' that fit an NMF dimensional reduction for the sparse dataset, use the NMF's components of each article in order to get any kind of similarity.\n","87a45375":"The mult_KMeans function perfroms multiple Kmeans model at the same time and plot an histogram that count the number of articles in each cluster.","2bbef6d0":"The TSNE dimentional reduction aims to build a new data for the articles, It will be a DataFrame of 60 articles and 2 TSNE components. The TSNE technic helps in getting better visualization, so we will merge its results with the those of the KMeans clustering.","0f15d770":"2. The recommander System for similar wikipedia article","77e758ac":"We have the original dataset which is called **\"articles\"**, and we build on it a csr dataset called **\"art_csr\"**\n\n**1. Dimenstional reduction, clustering and visualization**"}}