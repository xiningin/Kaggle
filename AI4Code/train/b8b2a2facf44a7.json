{"cell_type":{"5fb9eff5":"code","a45ca4ca":"code","7f2cba20":"code","52aef280":"code","8c5c055b":"code","8c45a3fd":"code","025e3bd4":"code","d97fa8ae":"code","eceb4ec3":"code","7f773952":"code","5eaa5465":"code","9c87b062":"code","b076c550":"code","862e1e5d":"code","4a5fe196":"code","46e3e243":"code","765fc782":"code","f9721e93":"code","b01abf5a":"code","65d61bb6":"code","8fb6107b":"code","528ff874":"code","338ae3ac":"code","cc712701":"code","5950f36b":"code","8fb597ae":"code","f5c2c5f5":"code","7a2060d0":"code","e1906daa":"code","fbcccb9a":"code","8b681900":"code","ae7f2660":"code","b0aa0011":"code","3fd6b699":"code","905aaa3c":"code","24b74b41":"code","f88dd1f6":"code","3808007a":"code","0af647ed":"code","09b7a7f5":"markdown","e988979b":"markdown","2754e166":"markdown","810faac0":"markdown","36ca8d16":"markdown","c250978b":"markdown","825c3ce0":"markdown","8d0472e8":"markdown","75db388f":"markdown","591b1dd8":"markdown","8fcc6a89":"markdown","69f1cd80":"markdown","3860227d":"markdown","92298680":"markdown","8e4d943a":"markdown","6cf0eb81":"markdown","3c39fc7d":"markdown","15a64f6c":"markdown","45a11ef7":"markdown","faf3da73":"markdown","eac5b00a":"markdown","6f4900d3":"markdown","a058fc3d":"markdown","7417337a":"markdown","73bc9afd":"markdown","f30496ee":"markdown","e65ade55":"markdown","a76c902c":"markdown","9a47fcee":"markdown","0acb2398":"markdown","f69eb6d2":"markdown","59ce5d5e":"markdown","99340fa6":"markdown","6cfecd3e":"markdown","2a909293":"markdown","95a0c370":"markdown","60542e69":"markdown","f14e0da7":"markdown","5f94be38":"markdown","baa5ee23":"markdown","0b30f42a":"markdown","3aa0e726":"markdown","9db83907":"markdown","ead0a6ed":"markdown","7edba206":"markdown","288f6b16":"markdown","47ab908a":"markdown","2605b3bc":"markdown","b5f6b90b":"markdown","fdbe6a96":"markdown","f8c45702":"markdown","b317d83a":"markdown","e0135311":"markdown"},"source":{"5fb9eff5":"import gzip\nimport tensorflow as tf\nimport numpy as np\nfrom collections import namedtuple\nimport matplotlib.pyplot as plt","a45ca4ca":"def read32(bytestream):\n    return np.frombuffer(bytestream.read(4), \n                         dtype=np.dtype(np.uint32).newbyteorder('>'))[0]","7f2cba20":"def one_hot(labels_dense, num_classes):\n    \"\"\"Convert class labels from scalars to one-hot vectors.\n    Args:\n        labels_dense: A numpy array containing image labels\n        num_classes: Number of classes in the image\n    Returns:\n        labels_one_hot: A numpy array containing one-hot encoded data.\n    \"\"\"\n    \n    num_labels = labels_dense.shape[0]\n    index_offset = np.arange(num_labels) * num_classes\n    labels_one_hot = np.zeros((num_labels, num_classes))\n    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n    \n    return labels_one_hot","52aef280":"def extract_images(f):\n    \"\"\"Extract the images into a 4D uint8 numpy array [index, y, x, depth].\n    Args:\n        f: A file object that can be passed into a gzip reader.\n    Returns:\n        data: A 4D uint8 numpy array [index, y, x, depth].\n    Raises:\n        ValueError: If the bytestream does not start with 2051.\n    \"\"\"\n    \n    print('Extracting', f.name)\n    \n    with gzip.GzipFile(fileobj=f) as bytestream:\n        magic = read32(bytestream)\n        \n        if magic != 2051:\n            raise ValueError('Invalid magic number %d in MNIST image file: %s' % (magic, f.name))\n        \n        num_images = read32(bytestream)\n        rows = read32(bytestream)\n        cols = read32(bytestream)\n        buf = bytestream.read(rows * cols * num_images)\n        data = np.frombuffer(buf, dtype=np.uint8)\n        data = data.reshape(num_images, rows, cols, 1)\n        \n        return data","8c5c055b":"def extract_labels(f, one_hot_encode=False, num_classes=10):\n    \"\"\"Extract the labels into a 1D uint8 numpy array [index].\n    Args:\n        f: A file object that can be passed into a gzip reader.\n        one_hot_encode: Does one hot encoding for the result. Default False.\n        num_classes: Number of classes for the one hot encoding.\n    Returns:\n        labels: a 1D uint8 numpy array.\n    Raises:\n        ValueError: If the bystream doesn't start with 2049.\n    \"\"\"\n    \n    print('Extracting', f.name)\n    with gzip.GzipFile(fileobj=f) as bytestream:\n        magic = read32(bytestream)\n        \n        if magic != 2049:\n            raise ValueError('Invalid magic number %d in MNIST label file: %s' % (magic, f.name))\n            \n        num_items = read32(bytestream)\n        buf = bytestream.read(num_items)\n        labels = np.frombuffer(buf, dtype=np.uint8)\n        \n        if one_hot_encode:\n            return one_hot(labels, num_classes)\n        \n        return labels","8c45a3fd":"class Dataset(object):\n\n    def __init__(self, images, labels, one_hot=False, dtype=tf.float32, reshape=True, seed=None):\n        \"\"\"Construct a dataset given a image and its labels.   \n\n        Args:\n            images: Numpy image data.\n            labels: Image labels\n            one_hot: Ind\u0131cates one hot encoding for the data. Default is false\n            dtype: Data type of our image.It can be either `uint8` to leave \n                   the input as `[0, 255]`, or `float32` to rescale into `[0, 1]`.\n            reshape: Convert shape from [num examples, rows, columns, depth]\n                     to [num examples, rows*columns] (assuming depth == 1)\n            seed: Provides for convenient deterministic testing\n\n        Returns:\n\n        \"\"\"\n\n        seed1, seed2 = tf.random.get_seed(seed)\n\n        # If op level seed is not set, use whatever graph level seed is returned\n        np.random.seed(seed1 if seed is None else seed2)\n\n        if dtype not in (tf.uint8, tf.float32):\n            raise TypeError('Invalid image dtype %r, expected uint8 or float32' % dtype)\n\n        assert images.shape[0] == labels.shape[0], (\n          'images.shape: %s labels.shape: %s' % (images.shape, labels.shape))\n        \n        self.num_examples = images.shape[0]\n        \n        # Convert shape from [num examples, rows, columns, depth]\n        # to [num examples, rows*columns] (assuming depth == 1)\n        if reshape:\n            assert images.shape[3] == 1\n            images = images.reshape(images.shape[0],\n                                    images.shape[1] * images.shape[2])\n        if dtype == tf.float32:\n            # Convert from [0, 255] -> [0.0, 1.0].\n            images = images.astype(np.float32)\n            images = np.multiply(images, 1.0 \/ 255.0)\n            \n        self.features = images\n        self.targets = labels\n        self.epochs_completed = 0\n        self.index_in_epoch = 0\n        \n    def next_batch(self, batch_size, shuffle=True):\n        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n    \n        start = self.index_in_epoch\n        \n        # Shuffle for the first epoch\n        if self.epochs_completed == 0 and start == 0 and shuffle:\n            perm0 = np.arange(self.num_examples)\n            np.random.shuffle(perm0)\n            self.features = self.features[perm0]\n            self.targets = self.targets[perm0]\n\n        # Go to the next epoch\n        if start + batch_size > self.num_examples:\n            # Finished epoch\n            self.epochs_completed += 1\n            # Get the rest examples in this epoch\n            rest_num_examples = self.num_examples - start\n            images_rest_part = self.features[start:self.num_examples]\n            labels_rest_part = self.targets[start:self.num_examples]\n            \n            # Shuffle the data\n            if shuffle:\n                perm = np.arange(self.num_examples)\n                np.random.shuffle(perm)\n                self.features = self.features[perm]\n                self.targets = self.targets[perm]\n                \n            # Start next epoch\n            start = 0\n            self.index_in_epoch = batch_size - rest_num_examples\n            end = self.index_in_epoch\n            images_new_part = self.features[start:end]\n            labels_new_part = self.targets[start:end]\n            \n            return np.concatenate((images_rest_part, images_new_part), \n                                  axis=0), np.concatenate((labels_rest_part, \n                                                           labels_new_part), axis=0)\n        \n        else:\n            self.index_in_epoch += batch_size\n            end = self.index_in_epoch\n            \n            return self.features[start:end], self.targets[start:end]","025e3bd4":"def read_raw_mnist(img_names, one_hot=False, dtype=tf.float32, val_size=5000, \n                   reshape=True, seed=None):\n    \"\"\" Reads the original mnist data as in zipped format.\n    \n    Args:\n        img_names: A dictionary containing names of the training and test images.\n                   Dictionary keys should be 'train_images', 'train_labels', \n                   'test_images', 'test_labels'\n        one_hot: Indicates whether labels should be converted to one-hot encoding.Default False\n        dtype: Data type of images. Default to tf.float32\n        reshape: Convert shape from [num examples, rows, columns, depth]\n                 to [num examples, rows*columns] (assuming depth == 1). Default True\n        val_size: Validation data size apart from test data\n        \n    Returns:\n        Datasets as a namedtuple contains train, validation, test data.\n    \"\"\"\n    TRAIN_IMAGES = img_names['train_images']\n    TRAIN_LABELS = img_names['train_labels']\n    TEST_IMAGES = img_names['test_images']\n    TEST_LABELS = img_names['test_labels']\n\n    with tf.gfile.Open(TRAIN_IMAGES, 'rb') as f:\n        train_images = extract_images(f)\n\n    with tf.gfile.Open(TRAIN_LABELS, 'rb') as f:\n        train_labels = extract_labels(f, one_hot_encode=one_hot)\n\n    with tf.gfile.Open(TEST_IMAGES, 'rb') as f:\n        test_images = extract_images(f)\n\n    with tf.gfile.Open(TEST_LABELS, 'rb') as f:\n        test_labels = extract_labels(f, one_hot_encode=one_hot)\n\n\n    val_images = train_images[:val_size]\n    val_labels = train_labels[:val_size]\n    train_images = train_images[val_size:]\n    train_labels = train_labels[val_size:]\n\n    kwargs = dict(dtype=dtype, reshape=reshape, seed=seed)\n    \n    train = Dataset(train_images, train_labels, **kwargs)\n    val = Dataset(val_images, val_labels, **kwargs)\n    test = Dataset(test_images, test_labels, **kwargs)\n    \n    Datasets = namedtuple('Datasets', ['train', 'val', 'test'])\n    \n    return Datasets(train=train, val=val, test=test)","d97fa8ae":"TRAIN_IMAGES = '..\/input\/train-images-idx3-ubyte.gz'\nTRAIN_LABELS = '..\/input\/train-labels-idx1-ubyte.gz'\nTEST_IMAGES = '..\/input\/t10k-images-idx3-ubyte.gz'\nTEST_LABELS = '..\/input\/t10k-labels-idx1-ubyte.gz'\n\nimg_data = {'train_images': TRAIN_IMAGES, 'train_labels': TRAIN_LABELS, \n            'test_images':TEST_IMAGES, 'test_labels': TEST_LABELS }\n\nmnist = read_raw_mnist(img_data, one_hot=True)","eceb4ec3":"sess = tf.InteractiveSession()","7f773952":"num_classes = 10\nnum_pixels = 28*28\nbatches = 50\nepochs = 1000","5eaa5465":"X = tf.placeholder(tf.float32, shape=[None, num_pixels])\ny = tf.placeholder(tf.float32, shape=[None, num_classes])","9c87b062":"# Weight & Bias tensor\nw = tf.Variable(tf.zeros([num_pixels, num_classes],tf.float32))\nb = tf.Variable(tf.zeros([num_classes],tf.float32))","b076c550":"# run the op initialize_all_variables using an interactive session\nsess.run(tf.global_variables_initializer())","862e1e5d":"# mathematical operation to add weights and biases to the inputs\ntf.matmul(X, w) + b","4a5fe196":"Y = tf.nn.softmax(tf.matmul(X, w) + b)","46e3e243":"loss = tf.losses.softmax_cross_entropy(onehot_labels=y, logits=Y)","765fc782":"train = tf.train.GradientDescentOptimizer(0.5).minimize(loss)","f9721e93":"#Load 50 training examples for each training iteration   \nfor i in range(epochs):\n    batch = mnist.train.next_batch(batch_size=batches)\n    train.run(feed_dict={X: batch[0], y: batch[1]})","b01abf5a":"is_correct = tf.equal(tf.argmax(Y, 1), tf.argmax(y, 1))\naccuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\nacc = accuracy.eval(feed_dict={X: mnist.test.features, y: mnist.test.targets}) * 100\nprint(\"The final accuracy for the simple ANN model is: {} % \".format(acc) )\n\nsess.close()","65d61bb6":"mnist = read_raw_mnist(img_data, one_hot=True)","8fb6107b":"width = 28 # width of the image in pixels \nheight = 28 # height of the image in pixels\nchannels = 1\nkernel_size = 5\nepochs = 600\nlearning_rate = 1e-4","528ff874":"tf.reset_default_graph()","338ae3ac":"with tf.name_scope('Inputs'):\n    x = tf.placeholder(tf.float32, shape=[None, num_pixels], name='Features')\n    y = tf.placeholder(tf.float32, shape=[None, num_classes], name='Labels')\n    \nwith tf.name_scope('Reshape_Input'):    \n    X = tf.reshape(x, [-1, width, height, channels], name='Reshaped_Features')  ","cc712701":"with tf.name_scope('Conv1_Weights'):\n    w_conv1 = tf.Variable(tf.truncated_normal([kernel_size, kernel_size, channels, 32], stddev=0.1), \n                          name='weights')\n    b_conv1 = tf.Variable(tf.constant(0.1, shape=[32]), name='biases') # need 32 biases for 32 outputs","5950f36b":"with tf.name_scope('Convolution_1'):\n    conv1 = tf.nn.conv2d(X, w_conv1, strides=[1, 1, 1, 1], padding='SAME', name='Conv2D') + b_conv1","8fb597ae":"with tf.name_scope('Conv1_ReLU_Activation'):\n    relu_conv1 = tf.nn.relu(conv1, name='ReLU')","f5c2c5f5":"with tf.name_scope('Conv1_Max_Pooling'):\n    # Max Poll 2x2\n    max_conv1 = tf.nn.max_pool(relu_conv1, ksize=[1, 2, 2, 1] , \n                               strides=[1, 2, 2, 1], padding='SAME', name='Max_Pool') ","7a2060d0":"# Init variables for convolution layer\nwith tf.name_scope('Conv2_Weights'):\n    w_conv2 = tf.Variable(tf.truncated_normal([kernel_size, kernel_size, 32, 64], stddev=0.1), name='weights')\n    b_conv2 = tf.Variable(tf.constant(0.1, shape=[64]), name='biases') #need 64 biases for 64 outputs\n\n# Convolve image with weight tensor and add biases.\nwith tf.name_scope('Convolution_2'):\n    conv2 = tf.nn.conv2d(max_conv1, w_conv2, strides=[1, 1, 1, 1], padding='SAME', name='Conv2D') + b_conv2\n    \n# Apply the ReLU activation Function\nwith tf.name_scope('Conv2_ReLU_Activation'):\n    relu_conv2 = tf.nn.relu(conv2, name='ReLU')\n\n# Apply the max pooling 2x2    \nwith tf.name_scope('Conv2_Max_Pooling'):\n    max_conv2 = tf.nn.max_pool(relu_conv2, ksize=[1, 2, 2, 1], \n                               strides=[1, 2, 2, 1], padding='SAME', name='Max_Pool') ","e1906daa":"# Flattening Second Layer\nwith tf.name_scope('Flatten'):\n    layer2_mat = tf.reshape(max_conv2, shape=[-1, 7 * 7 * 64], name='flatten')\n\n# Weights and Biases between layer 2 and 3\n# Composition of the feature map from the last layer (7x7) multiplied \n# by the number of feature maps (64); 1024 outputs to Softmax layer\nwith tf.name_scope('Flatten_Weights'):\n    w_fc1 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1024], stddev=0.1), name='weights')\n    b_fc1 = tf.Variable(tf.constant(0.1, shape=[1024]), name='biases') # need 1024 biases for 1024 outputs\n\n# Matrix Multiplication (applying weights and biases)\nwith tf.name_scope('Flatten_Matrix_Mul'):\n    fcl = tf.matmul(layer2_mat, w_fc1, name='MatMul') + b_fc1\n\n# Apply the ReLU activation Function\nwith tf.name_scope('Flatten_ReLU_Activation'):\n    relu_fc1 = tf.nn.relu(fcl, name='ReLU')","fbcccb9a":"with tf.name_scope('DropOut'):\n    keep = tf.placeholder(tf.float32, name='Keep')\n    drop = tf.nn.dropout(relu_fc1, keep, name='Drop')","8b681900":"# Weights and Biases\n# In last layer, CNN takes the high-level filtered images and translate \n# them into votes using softmax. Input channels: 1024 (neurons from the 3rd Layer); 10 output features\nwith tf.name_scope('Softmax_Weights'):\n    w_fc2 = tf.Variable(tf.truncated_normal([1024, num_classes], stddev=0.1), name='weights') #1024 neurons\n    # 10 possibilities for digits [0,1,2,3,4,5,6,7,8,9]\n    b_fc2 = tf.Variable(tf.constant(0.1, shape=[num_classes]), name='biases') \n\n# Matrix Multiplication (applying weights and biases)\nwith tf.name_scope('Softmax_MatMul'):\n    fc = tf.matmul(drop, w_fc2, name='MatMul') + b_fc2\n\n# Apply the Softmax activation Function\n# softmax allows us to interpret the outputs of fcl4 as probabilities.\n# So,Y_CNN  is a tensor of probabilities\nwith tf.name_scope('Softmax'):\n    Y_CNN = tf.nn.softmax(fc, name='softmax')","ae7f2660":"with tf.name_scope('Loss'):\n    loss = tf.losses.softmax_cross_entropy(y, Y_CNN)\n\nwith tf.name_scope('Train'):\n    train = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n    \nwith tf.name_scope('Accuracy'):\n    is_true = tf.equal(tf.argmax(Y_CNN, 1), tf.argmax(y, 1), name='output')\n    acc = tf.reduce_mean(tf.cast(is_true, tf.float32), name='accuracy')","b0aa0011":"# evaluate in batches to avoid out-of-memory issues\nn_batches = mnist.test.features.shape[0] \/\/ 50  # Floor division\ncumulative_accuracy = 0.0\nsess = tf.Session()\n\nsess.run(tf.global_variables_initializer())\n\n# Graph data\nwriter = tf.summary.FileWriter('logs')\nwriter.add_graph(sess.graph)\n\nfor i in range(epochs):\n    batch = mnist.train.next_batch(50)\n    if i%100 == 0:\n        train_acc = sess.run(acc, feed_dict={x:batch[0], y: batch[1], keep: 1.0})\n        print(\"Step %d, training accuracy %g\" %(i, float(train_acc)))\n    sess.run(train, feed_dict={x: batch[0], y: batch[1], keep: 0.5})","3fd6b699":"# Evaluate the Model\nfor index in range(n_batches):\n    batch = mnist.test.next_batch(50)\n    cumulative_accuracy += sess.run(acc, feed_dict={x: batch[0], y: batch[1], keep: 1.0})\nprint(\"test accuracy {}\".format(cumulative_accuracy \/ n_batches))","905aaa3c":"def get_activations(layer, stimuli):\n    units = sess.run(layer, feed_dict={x:np.reshape(stimuli, [1,784], order='F'), keep:1.0})\n    plot_nn_filter(units)","24b74b41":"def plot_nn_filter(units):\n    import math\n    filters = units.shape[3]\n    plt.figure(1, figsize=(20,20))\n    n_columns = 6\n    n_rows = math.ceil(filters \/ n_columns) + 1\n    for i in range(filters):\n        plt.subplot(n_rows, n_columns, i+1)\n        plt.title('Filter ' + str(i))\n        plt.imshow(units[0,:,:,i], interpolation=\"nearest\", cmap=\"gray\")\n    plt.tight_layout()","f88dd1f6":"image = mnist.test.features[0]\nplt.imshow(np.reshape(image, [height,width]), interpolation=\"nearest\", cmap=\"gray\")","3808007a":"get_activations(conv1, image)","0af647ed":"get_activations(conv2, image)","09b7a7f5":"### Data Processing\n\nIn this part, we are going to see how we can read the mnist data from the original data source. Almost %80 of the AI work is usually processing the data as can be seen from this case also. We will read our data using numpy, convert labels to one-hot encoding, scale our data between 0-1, shuffle the data and retrieve batches repeatedly.  ","e988979b":"<h4>Dropout Layer, Optional phase for reducing overfitting<\/h4>\n\nIt is a phase where the network \"forget\" some features. At each training step in a mini-batch, some units get switched off randomly so that it will not interact with the network. That is, it weights cannot be updated, nor affect the learning of the other network nodes.  This can be very useful for very large neural networks to prevent overfitting.","2754e166":"<h4>Defining kernel weight and bias<\/h4>\n\nWe define a kernel here. The size of the filter\/kernel is 5x5;  Input channels is 1 (grayscale);  and we need 32 different feature maps (here, 32 feature maps means 32 different filters are applied on each image. So, the output of convolution layer would be 28x28x32). In this step, we create a filter \/ kernel tensor of shape <code>[filter_height, filter_width, in_channels, out_channels]<\/code>","810faac0":"! tensorboard --logdir='logs\/'","36ca8d16":"# CONVOLUTIONAL NEURAL NETWORK APPLICATION","c250978b":"### Softmax Regression\n\nSoftmax is an activation function that is normally used in classification problems. It generate the probabilities for the output. For example, our model will not be 100% sure that one digit is the number nine, instead, the answer will be a distribution of probabilities where, if the model is right, the nine number will have a larger probability than the other other digits.\n\nFor comparison, below is the one-hot vector for a nine digit label:","825c3ce0":"**EXTRACT LABELS**","8d0472e8":"<h4>Readout Layer (Softmax Layer)<\/h4>\n\nType: Softmax, Fully Connected Layer.","75db388f":"## Visualization\n\nLet's take a look at all the filters(weights and biases) to see what our model does.","591b1dd8":"<a id=\"ref1\"><\/a>\n<h2>What is Deep Learning?<\/h2>\n\n<b>Brief Theory:<\/b> Deep learning (also known as deep structured learning, hierarchical learning or deep machine learning) is a branch of machine learning based on a set of algorithms that attempt to model high-level abstractions in data by using multiple processing layers, with complex structures or otherwise, composed of multiple non-linear transformations.\n\n<img src=\"https:\/\/ibm.box.com\/shared\/static\/gcbbrh440604cj2nksu3f44be87b8ank.png\" alt=\"HTML5 Icon\" style=\"width: 600px; height: 450px;\">\n<div style=\"text-align: center\">It's time for deep learning. Our brain doesn't work with only one or three layers. Why it would be different with machines?. <\/div>","8fcc6a89":"<h4>Converting images of the data set to tensors<\/h4>\n\nThe input image is 28 pixels by 28 pixels, 1 channel (grayscale). In this case, the first dimension is the <b>batch number<\/b> of the image, and can be of any size (so we set it to -1). The second and third dimensions are width and height, and the last one is the image channels.","69f1cd80":"Third layer completed","3860227d":"<img src=\"https:\/\/www.katacoda.com\/basiafusinska\/courses\/tensorflow-getting-started\/tensorflow-mnist-beginner\/assets\/network.png\" alt=\"HTML5 Icon\" style=\"width:800px;height:350px;\"> \n<div style=\"text-align:center\">Our Model<\/div>","92298680":"<div class=\"alert alert-block alert-info\">\n<h2>Table of Contents<\/h2>\n<ol>\n    <li><a href=\"#ref1\">What is Deep Learning<\/a><\/li>\n    <li><a href=\"#ref2\">Simple test: Is TensorFlow working?<\/a><\/li>\n    <li><a href=\"#ref3\">1st part: classify MNIST using a simple model<\/a><\/li>\n    <li><a href=\"#ref4\">Evaluating the final result<\/a><\/li>\n    <li><a href=\"#ref5\">How to improve our model?<\/a><\/li>\n    <li><a href=\"#ref6\">2nd part: Deep Learning applied on MNIST<\/a><\/li>\n    <li><a href=\"#ref7\">Summary of the Deep Convolutional Neural Network<\/a><\/li>\n    <li><a href=\"#ref8\">Define functions and train the model<\/a><\/li>\n    <li><a href=\"#ref9\">Evaluate the model<\/a><\/li>\n<\/ol>    \n<\/div>","8e4d943a":"<h4>Apply the max pooling<\/h4>\n\n<b>max pooling<\/b> is a form of non-linear down-sampling. It partitions the input image into a set of rectangles and, and then find the maximum value for that region. \n\nLets use <b>tf.nn.max_pool<\/b> function to perform max pooling. \n<b>Kernel size:<\/b> 2x2 (if the window is a 2x2 matrix, it would result in one output pixel)  \n<b>Strides:<\/b> dictates the sliding behaviour of the kernel. In this case it will move 2 pixels everytime, thus not overlapping. The input is a matrix of size 28x28x32, and the output would be a matrix of size 14x14x32.\n\n<img src=\"https:\/\/ibm.box.com\/shared\/static\/kmaja90mn3aud9mro9cn8pbbg1h5pejy.png\" alt=\"HTML5 Icon\" style=\"width: 900px; height: 500px;\"> \n\n","6cf0eb81":"This is the part where you configure the optimizer for your Neural Network. There are several optimizers available, in our case we will use Gradient Descent because it is a well established optimizer.","3c39fc7d":"## Introduction\n\nIn this section, we will use the famous [MNIST Dataset](http:\/\/yann.lecun.com\/exdb\/mnist\/) to build two Neural Networks capable to perform handwritten digits classification. The first Network is a simple Multi-layer Perceptron (MLP) and the second one is a Convolutional Neural Network (CNN). In other words, when given an input our algorithm will say, with some associated error, what type of digit this input represents.","15a64f6c":"**A CONTAINER FOR OUR DATA INCLUDING BATCH FUNCTION**","45a11ef7":"<b>In Practice, defining the term \"Deep\":<\/b> in this context, deep means that we are studying a Neural Network which has several hidden layers (more than one), no matter what type (convolutional, pooling, normalization, fully-connected etc). The most interesting part is that some papers noticed that Deep Neural Networks with the right architectures\/hyper-parameters achieve better results than shallow Neural Networks with the same computational power (e.g. number of neurons or connections). \n\n<b>In Practice, defining \"Learning\":<\/b> In the context of supervised learning, digits recognition in our case, the learning part consists of a target\/feature which is to be predicted using a given set of observations with the already known final prediction (label). In our case, the target will be the digit (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) and the observations are the intensity and relative position of the pixels. After some training, it is possible to generate a \"function\" that map inputs (digit image) to desired outputs(type of digit). The only problem is how well this map operation occurs. While trying to generate this \"function\", the training process continues until the model achieves a desired level of accuracy on the training data.","faf3da73":"A machine does not have all this certainty, so we want to know what is the best guess, but we also want to understand how sure it was and what was the second better option. Below is an example of a hypothetical distribution for a nine digit:","eac5b00a":"epochs = 20000  \nwith tf.Session() as sess:  \n    sess.run(tf.global_variables_initializer())  \n    \n    for i in range(epochs):\n        batch = mnist.train.next_batch(50)\n        if i%100 == 0:\n            train_acc = acc.eval(feed_dict={x:batch[0], y: batch[1], keep: 1.0})\n            print(\"Step %d, training accuracy %g\" %(i, float(train_acc)))\n        train.run(feed_dict={x: batch[0], y: batch[1], keep: 0.5})","6f4900d3":"Before, we assigned the weights and biases but we did not initialize them with null values. For this reason, TensorFlow need to initialize the variables that you assign.  \nPlease notice that we're using this notation \"sess.run\" because we previously started an interactive session.","a058fc3d":"<h3>Convolutional Layer 2<\/h3>","7417337a":"0 -->0.01  \n1 -->0.02   \n2 -->0.03  \n3 -->0.02  \n4 -->0.12  \n5 -->0.01  \n6 -->0.03  \n7 -->0.06  \n8 -->0.1  \n9 -->0.6   ","73bc9afd":"<h3>Convolutional Layer 1<\/h3>","f30496ee":"<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n<font size = 3><strong><i>You can run this cell if you REALLY have time to wait, or you are running it using Cloud Power (<b>change the type of the cell to code<\/b>)<\/i><\/strong><\/font>","e65ade55":"In this tutorial, we first classify MNIST using a simple Multi-layer perceptron and then, in the second part, we use deeplearning to improve the accuracy of our results.","a76c902c":"**EXTRACT IMAGE DATA**","9a47fcee":"Now we are going to create the weights and biases, for this purpose they will be used as arrays filled with zeros. The values that we choose here can be critical, but we'll cover a better way on the second part, instead of this type of initialization.","0acb2398":"<a id=\"ref7\"><\/a>\n<h2>Summary of the Deep Convolutional Neural Network<\/h2>\n\nNow is time to remember the structure of  our network\n\n#### 0) Input - MNIST dataset\n#### 1) Convolutional and Max-Pooling\n#### 2) Convolutional and Max-Pooling\n#### 3) Fully Connected Layer\n#### 4) Processing - Dropout\n#### 5) Readout layer - Fully Connected\n#### 6) Outputs - Classified digits","f69eb6d2":"<a id=\"ref3\"><\/a>\n<h2>1st part: classify MNIST using a simple model.<\/h2>\n\nWe are going to create a simple Multi-layer perceptron, a simple type of Neural Network, to perform classification tasks on the MNIST digits dataset. If you are not familiar with the MNIST dataset, please consider to read more about it: <a href=\"http:\/\/yann.lecun.com\/exdb\/mnist\/\">click here<\/a> \n\n<h3>What is MNIST?<\/h3>\n\nAccording to LeCun's website, the MNIST is a: \"database of handwritten digits that has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image\".\n\n<h3>Import the MNIST dataset using TensorFlow built-in feature<\/h3>\n\nIt's very important to notice that MNIST is a high optimized data-set and it does not contain images. You will need to build your own code if you want to see the real digits. Another important side note is the effort that the authors invested on this data-set with normalization and centering operations.  ","59ce5d5e":"### Using InteractiveSession\n\nYou have two basic options when using TensorFlow to run your code:\n\n- [Build graphs and run session] Do all the set-up and THEN execute a session to evaluate tensors and run operations (ops) \n- [Interactive session] create your coding and run on the fly. \n\nFor this first part, we will use the interactive session that is more suitable for environments like Jupyter notebooks.","99340fa6":"The imported data can be divided as follow:\n\n- Training (mnist.train) >>  Use the given dataset with inputs and related outputs for training of NN. In our case, if you give an image that you know that represents a \"nine\", this set will tell the neural network that we expect a \"nine\" as the output.  \n        - 55,000 data points\n        - mnist.train.features for inputs\n        - mnist.train.targets for outputs\n  \n   \n- Validation (mnist.val) >> The same as training, but now the data is used to generate model properties (classification error, for example) and from this, tune parameters like the optimal number of hidden units or determine a stopping point for the back-propagation algorithm  \n        - 5,000 data points\n        - mnist.val.features for inputs\n        - mnist.val.targets for outputs\n  \n  \n- Test (mnist.test) >> the model does not have access to this informations prior to the testing phase. It is used to evaluate the performance and accuracy of the model against \"real life situations\". No further optimization beyond this point.  \n        - 10,000 data points\n        - mnist.test.features for inputs\n        - mnist.test.targets for outputs\n  ","6cfecd3e":"## A Look to Our Graph\n\n<a href=\"https:\/\/ibb.co\/XxwcmHd\"><img src=\"https:\/\/i.ibb.co\/3mJgtK8\/graph2.png\" alt=\"graph2\" border=\"0\"><\/a><br \/>\n","2a909293":"<img src=\"https:\/\/ibm.box.com\/shared\/static\/vn26neef1nnv2oxn5cb3uueowcawhkgb.png\" style=\"width: 800px; height: 400px;\" alt=\"HTML5 Icon\" >\n\n<h4>Convolve with weight tensor and add biases.<\/h4>\n\nTo create convolutional layer, we use <b>tf.nn.conv2d<\/b>. It computes a 2-D convolution given 4-D input and filter tensors.\n\nInputs:\n- tensor of shape [batch, in_height, in_width, in_channels]. x of shape [batch_size, 28 ,28, 1]\n- a filter \/ kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]. W is of size [5, 5, 1, 32]\n- stride which is  [1, 1, 1, 1]. The convolutional layer, slides the \"kernel window\" across the input tensor. As the input tensor has 4 dimensions:  [batch, height, width, channels], then the convolution operates on a 2D window on the height and width dimensions. __strides__ determines how much the window shifts by in each of the dimensions. As the first and last dimensions are related to batch and channels, we set the stride to 1. But for second and third dimension, we could set other values, e.g. [1, 2, 2, 1]\n    \n    \nProcess:\n- Change the filter to a 2-D matrix with shape [5\\*5\\*1,32]\n- Extracts image patches from the input tensor to form a *virtual* tensor of shape `[batch, 28, 28, 5*5*1]`.\n- For each batch, right-multiplies the filter matrix and the image vector.\n\nOutput:\n- A `Tensor` (a 2-D convolution) of size tf.Tensor 'add_7:0' shape=(?, 28, 28, 32)- Notice: the output of the first convolution layer is 32 [28x28] images. Here 32 is considered as volume\/depth of the output image.","95a0c370":"Our First layer is now completed. Let's start building the second second layer.","60542e69":"Train using minibatch Gradient Descent.\n\nIn practice, Batch Gradient Descent is not often used because is too computationally expensive. The good part about this method is that you have the true gradient, but with the expensive computing task of using the whole dataset in one time. Due to this problem, Neural Networks usually use minibatch to train.","f14e0da7":"First, we want to compare which labels were predicted correctly by using `tf.argmax` function. `tf.equal` returns the list of booleans so by casting the values to float and then calculating the average we finally get the accuracy of the model.","5f94be38":"Second layer completed. So, what is the output of the second layer, layer2?\n- it is 64 matrix of [7x7]","baa5ee23":"<h4>Weights and Biases of kernels<\/h4>\n\nWe apply the convolution again in this layer. Lets look at the second layer kernel:  \n- Filter\/kernel: 5x5 (25 pixels) \n- Input channels: 32 (from the 1st Conv layer, we had 32 feature maps) \n- 64 output feature maps  \n\n<b>Notice:<\/b> here, the input image is [14x14x32], the filter is [5x5x32], we use 64 filters of size [5x5x32], and the output of the convolutional layer would be 64 convolved image, [14x14x64].\n\n<b>Notice:<\/b> the convolution result of applying a filter of size [5x5x32] on image of size [14x14x32] is an image of size [14x14x1], that is, the convolution is functioning on volume.","0b30f42a":"**READ MNIST DATA**","3aa0e726":"## 2nd Part Deep Learning on MNIST\n\nIn the first part, we learned how to use a simple ANN to classify MNIST. Now we are going to expand our knowledge using a Deep Neural Network. \n\n\nArchitecture of our network is:\n    \n- (Input) -> [batch_size, 28, 28, 1]  >> Apply 32 filter of [5x5]\n- (Convolutional layer 1)  -> [batch_size, 28, 28, 32]\n- (ReLU 1)  -> [?, 28, 28, 32]\n- (Max pooling 1) -> [?, 14, 14, 32]\n- (Convolutional layer 2)  -> [?, 14, 14, 64] \n- (ReLU 2)  -> [?, 14, 14, 64] \n- (Max pooling 2)  -> [?, 7, 7, 64] \n- [fully connected layer 3] -> [1x1024]\n- [ReLU 3]  -> [1x1024]\n- [Drop out]  -> [1x1024]\n- [fully connected layer 4] -> [1x10]\n\n<img src=\"https:\/\/www.katacoda.com\/basiafusinska\/courses\/tensorflow-getting-started\/tensorflow-mnist-expert\/assets\/convolution.png\" alt=\"HTML5 Icon\" style=\"width:800px;height:350px;\"> \n<div style=\"text-align:center\">Our Model<\/div>\n\n\n\nThe next cells will explore this new architecture. In this part, we are going to build our graph and visualize the whole architecture of the system so that we understand it very well. ","9db83907":"Logistic function output is used for the classification between two target classes 0\/1. Softmax function is generalized type of logistic function. That is, Softmax can output a multiclass categorical probability distribution.","ead0a6ed":"<img src=\"https:\/\/ibm.box.com\/shared\/static\/iizf4ui4b2hh9wn86pplqxu27ykpqci9.png\" style=\"width: 800px; height: 400px;\" alt=\"HTML5 Icon\" >\n\n\n<h4>Apply the ReLU activation Function<\/h4>\n\nIn this step, we just go through all outputs convolution layer, <b>conv1<\/b>, and wherever a negative number occurs, we swap it out for a 0. It is called ReLU activation Function.<br> Let f(x) is a ReLU activation function $f(x) = max(0,x)$.","7edba206":"<h3>Fully Connected Layer<\/h3>\n\nYou need a fully connected layer to use the Softmax and create the probabilities in the end. Fully connected layers take the high-level filtered images from previous layer, that is all 64 matrices, and convert them to a flat array.\n\nSo, each matrix [7x7] will be converted to a matrix of [49x1], and then all of the 64 matrix will be connected, which make an array of size [3136x1]. We will connect it into another layer of size [1024x1]. So, the weight between these 2 layers will be [3136x1024]\n\n\n<img src=\"https:\/\/ibm.box.com\/shared\/static\/pr9mnirmlrzm2bitf1d4jj389hyvv7ey.png\" alt=\"HTML5 Icon\" style=\"width: 800px; height: 400px;\"> \n","288f6b16":"The <span style=\"background-color:#dcdcdc\"> one-hot = True<\/span> argument only means that, in contrast to Binary representation, the labels will be presented in a way that to represent a number N, the $N^{th}$ bit is 1 while the the other bits are 0. For example, five and zero in a binary code would be:\n\n<pre>\nNumber representation:    0\nBinary encoding:        [2^5]  [2^4]   [2^3]   [2^2]   [2^1]   [2^0]  \nArray\/vector:             0      0       0       0       0       0 \n\nNumber representation:    5\nBinary encoding:        [2^5]  [2^4]   [2^3]   [2^2]   [2^1]   [2^0]  \nArray\/vector:             0      0       0       1       0       1  \n<\/pre>\n\nUsing a different notation, the same digits using one-hot vector representation can be show as: \n\n<pre>\nNumber representation:    0\nOne-hot encoding:        [5]   [4]    [3]    [2]    [1]   [0]  \nArray\/vector:             0     0      0      0      0     1   \n\nNumber representation:    5\nOne-hot encoding:        [5]   [4]    [3]    [2]    [1]    [0]  \nArray\/vector:             1     0      0      0      0      0   \n<\/pre>","47ab908a":"0 --> 0  \n1 --> 0   \n2 --> 0  \n3 --> 0  \n4 --> 0  \n5 --> 0  \n6 --> 0  \n7 --> 0  \n8 --> 0  \n9 --> 1  ","2605b3bc":"It is a function that is used to minimize the difference between the right answers (labels) and estimated outputs by our Network.","b5f6b90b":"It is a best practice to create placeholders before variable assignments when using TensorFlow. Here we'll create placeholders for inputs (\"Xs\") and outputs (\"Ys\").   \n\n<b>Placeholder 'X':<\/b> represents the \"space\" allocated input or the images. \n<ul>\n    <li>Each input has 784 pixels distributed by a 28 width x 28 height matrix<\/li>   \n    <li>The 'shape' argument defines the tensor size by its dimensions.<\/li>     \n    <li>1st dimension = None. Indicates that the batch size, can be of any size.<\/li>     \n    <li>2nd dimension = 784. Indicates the number of pixels on a single flattened MNIST image.<\/li>    \n<\/ul>\n    \n<b>Placeholder 'Y':<\/b> represents the final output or the labels.\n<ul>\n    <li>10 possible classes (0, 1, 2, 3, 4, 5, 6, 7, 8, 9)<\/li>  \n    <li>The 'shape' argument defines the tensor size by its dimensions.<\/li>    \n    <li>1st dimension = None. Indicates that the batch size, can be of any size.<\/li>     \n    <li>2nd dimension = 10. Indicates the number of targets\/outcomes<\/li>   \n<\/ul>\n<b>dtype for both placeholders:<\/b> if you not sure, use tf.float32. The limitation here is that the later presented softmax function only accepts float32 or float64 dtypes. For more dtypes, check TensorFlow's documentation <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/DType\">here<\/a>\n","fdbe6a96":"## How to Improve Our Model\n\n<h4>Several options as follow:<\/h4>\n<ul>\n    <li>Regularization of Neural Networks using DropConnect<\/li>\n    <li>Multi-column Deep Neural Networks for Image Classification<\/li> \n    <li>APAC: Augmented Pattern Classification with Neural Networks<\/li>\n    <li>Simple Deep Neural Network with Dropout<\/li>\n<\/ul>\n<h4>In the next part we are going to explore the option:<\/h4>\n<ul>\n    <li>Simple Deep Neural Network with Dropout (more than 1 hidden layer)<\/li>\n<\/ul> ","f8c45702":"**ONE-HOT ENCODING**","b317d83a":"## Train the Neural Network\n\n<h4>Define the loss function<\/h4>\n\nWe need to compare our output, layer4 tensor, with ground truth for all mini_batch. we can use <b>cross entropy<\/b> to see how bad our CNN is working - to measure the error at a softmax layer.\n\nThe following code shows an toy sample of cross-entropy for a mini-batch of size 2 which its items have been classified.","e0135311":"The only difference for our next operation to the picture below is that we are using the mathematical convention for what is being executed in the illustration. The tf.matmul operation performs a matrix multiplication between x (inputs) and W (weights) and after the code add biases.\n\n\n<img src=\"https:\/\/ibm.box.com\/shared\/static\/88ksiymk1xkb10rgk0jwr3jw814jbfxo.png\" alt=\"HTML5 Icon\" style=\"width:400px;height:350px;\"> \n<div style=\"text-align:center\">Illustration showing how weights and biases are added to neurons\/nodes. <\/div>"}}