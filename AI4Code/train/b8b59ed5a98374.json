{"cell_type":{"b02bfc9c":"code","b122a7f3":"code","e68f9a83":"code","54e7f0fe":"code","7e976e1f":"code","6af3198b":"code","ae07caae":"code","9904a606":"code","dac18936":"code","2aa89116":"code","2d659df9":"code","b7932e77":"code","f16e23c5":"code","1ffae7a5":"markdown","7b557c9a":"markdown"},"source":{"b02bfc9c":"import os\nimport random\nimport sys\nimport numpy as np\nfrom numpy.core.fromnumeric import _trace_dispatcher\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom pathlib import Path\nimport cv2\nfrom sklearn.model_selection import train_test_split","b122a7f3":"# limit the GPU memory growth\ngpu = tf.config.list_physical_devices('GPU')\nprint(\"Num GPUs Available: \", len(gpu))\nif len(gpu) > 0:\n    tf.config.experimental.set_memory_growth(gpu[0], True)","e68f9a83":"height = 800\nwidth = 800\nchannel = 8\nbatch_size 8\nepochs = 30\nseed = 26\ninput_depth = 4","54e7f0fe":"inputdatapath = \"..\/input\/MYTFRECORDSET\"\noutputpath = \".\/\"","7e976e1f":"class ClassificationModel(tf.keras.Model):\n    def __init__(self,**kwargs):\n        super(ClassificationModel, self).__init__(**kwargs)\n        self.efficient_net = tf.keras.applications.resnet_v2.ResNet152V2(weights=\"imagenet\",include_top=False,pooling='avg',input_tensor=None)\n        self.Conv2D_filter = tf.keras.layers.Conv2D(3, kernel_size=3, padding='same',input_shape=(height,width, 4))\n        self.dence64 = tf.keras.layers.Dense(64, activation='relu')\n        self.dence1 = tf.keras.layers.Dense(1, activation='sigmoid')\n\n    def call(self, input_tensor, training=True):\n        x = self.Conv2D_filter(input_tensor)\n        x = self.efficient_net(x)\n        x = self.dence64(x)\n        return self.dence1(x) \n\n    def train_step(self, data):\n        x ,y = data\n        with tf.GradientTape() as tape:\n            predictions = self(x, training=True)\n            loss = self.compiled_loss(y,predictions,regularization_losses=self.losses)\n            print(y,predictions)\n        gradients = tape.gradient(loss, model.trainable_variables)\n        self.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        self.compiled_metrics.update_state(y, predictions)\n        return {m.name: m.result() for m in self.metrics}\n\n    def test_step(self, data):\n        # Unpack the data\n        x, y = data\n        # Compute predictions\n        y_pred = self(x, training=False)\n        # Updates the metrics tracking the loss\n        self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n        # Update the metrics.\n        self.compiled_metrics.update_state(y, y_pred)\n        # Return a dict mapping metric names to current value.\n        # Note that it will include the loss (tracked in self.metrics).\n        return {m.name: m.result() for m in self.metrics}","6af3198b":"def deserialize_example(serialized_string):\n    image_feature_description = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'MGMT_value': tf.io.FixedLenFeature([], tf.float32)\n    }\n    parsed_record = tf.io.parse_single_example(serialized_string, image_feature_description)\n    image = tf.io.decode_raw(parsed_record['image'], tf.float64)\n    image = tf.reshape(image,[height,width,4])\n    \n    label = parsed_record['MGMT_value']\n    return image, label","ae07caae":"def argument_image(img,label):\n    \n    img = tf.cast(img, tf.float32) \/ 255.0\n\n    splitted_img = tf.split(img, input_depth, axis=-1)\n\n    augment_img = []\n    for each_img in splitted_img:\n        img = tf.repeat(each_img, repeats=3, axis=-1)\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_saturation(img, 0.9, 1.3)\n        img = tf.image.random_contrast(img, 0.8, 1.2)\n        img = tf.image.random_brightness(img, 0.2)\n        img, _, _ = tf.split(img, 3, axis=-1)\n        img = tf.expand_dims(img, 0)\n        augment_img.append(img)\n        \n    img = tf.concat(augment_img, axis=-1)      \n    img = tf.reshape(img, [height, width, input_depth])\n    return img,label\n\ndef argument_image_val(img,label):   \n    img = tf.cast(img, tf.float32) \/ 255.0\n    return img,label","9904a606":"model = ClassificationModel()\nloss_func = tf.keras.losses.BinaryCrossentropy(from_logits=True)\nopt = tf.keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(\noptimizer=opt, \nloss=loss_func,\nmetrics=[tf.keras.metrics.AUC(), \n    tf.keras.metrics.BinaryCrossentropy(from_logits=True)]\n)","dac18936":"earlyStopping = tf.keras.callbacks.EarlyStopping(patience=3, min_delta=0.001, verbose=1,restore_best_weights=True)","2aa89116":"#Please rewirte path for your TFRecode Position\ntrainset = tf.data.TFRecordDataset(str(inputdatapath \/ \"brain_train.tfrec\"),compression_type=\"GZIP\").map(deserialize_example).map(argument_image).batch(batch_size)\nvalnset = tf.data.TFRecordDataset(str(inputdatapath \/ \"brain_val.tfrec\"),compression_type=\"GZIP\").map(deserialize_example).map(argument_image_val).batch(batch_size)","2d659df9":"history = model.fit(\ntrainset, \nepochs=25,\nverbose=1,\ncallbacks=[earlyStopping],\nvalidation_data=valnset)","b7932e77":"tf.keras.backend.clear_session()\n\ntrain_score = model.evaluate(trainset, batch_size=batch_size)\ny_train = model.predict(trainset)\nvalidation_score = model.evaluate(valnset, batch_size=batch_size)\ny_val = model.predict(valnset)\nprint('Train loss :', train_score[0])\nprint('Train accuracy :', train_score[1])\nprint('validation loss :', validation_score[0])\nprint('validation accuracy :', validation_score[1])","f16e23c5":"weightdatapath = Path(outputpath,'weight')\nmodel.save_weights(Path(weightdatapath,\"weight.ckpt\"))","1ffae7a5":"This is my first Release Code.\n\nI learn from [this code](https:\/\/www.kaggle.com\/josepc\/rsna-effnet) so much,Thank you.\n\nPlease Run with My this code[[Create_TFRecord_for_RSNA-Radiogenomic](https:\/\/www.kaggle.com\/hazigin\/create-tfrecord-for-rsna-radiogenomic)]","7b557c9a":"If you feel this code is usefull\n\nPlease up Vote!!\n\nThank You!!"}}