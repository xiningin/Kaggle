{"cell_type":{"bae5ad09":"code","c2370320":"code","87708d45":"code","680ef771":"code","84bf52a0":"code","77408553":"code","e3b4c520":"code","07417ae6":"code","6d2d8143":"code","b03d70e4":"code","dc5c9765":"code","58ab364f":"code","a62a16a7":"code","5ed17721":"code","b0626d9c":"code","21af84d6":"code","6d2324ab":"code","d3a7ce23":"code","605258ea":"code","bbb22dd6":"code","ed2b575c":"code","4c2beb21":"code","64a39aea":"code","adcf1967":"code","aac93121":"code","61a95099":"code","29301381":"code","26ff1c77":"code","3e570ee6":"code","5e821d26":"code","2a0ab816":"code","95b3710d":"code","37f3885a":"code","79a9a2cc":"code","83a05fb1":"code","94ddb1f3":"code","a5d69a1b":"code","93eb7b5a":"code","e95f91f8":"code","a3079228":"code","e60859c1":"code","283ad080":"code","d2d70257":"code","3282f60b":"code","4813e503":"code","6c83a9c6":"code","074994d2":"code","e510d09e":"code","6e57e9d6":"code","7e960ad7":"code","d687262e":"code","c3e3d363":"code","10274aee":"code","d6e10d3f":"code","7eddf737":"code","b7e72a0a":"code","4bff7e6f":"code","408eaa1b":"code","df6a8801":"code","b216bea7":"code","0dd3ea16":"code","763b356a":"code","a98e1f9f":"code","5b161d4c":"code","783f87f6":"markdown","e9e7213e":"markdown","7368a495":"markdown","a94f9832":"markdown","fa81c1ad":"markdown","5fa7387c":"markdown","fcd5d46d":"markdown"},"source":{"bae5ad09":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom  sklearn.linear_model import LinearRegression\nfrom xgboost import XGBClassifier\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler\n\nfrom scipy import stats\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error\n\nimport warnings\nwarnings.filterwarnings('ignore')","c2370320":"\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","87708d45":"train_df=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_df=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n","680ef771":"test_df.head()","84bf52a0":"\nprint(\"There are {} rows and {} columns in the train dataset\".format(train_df.shape[0],train_df.shape[1]))\nprint(\"There are {} rows and {} columns in the test dataset\".format(test_df.shape[0],test_df.shape[1]))\n","77408553":"\nprint(\"There are {} rows and {} columns in the train dataset\".format(train_df.shape[0],train_df.shape[1]))\nprint(\"There are {} rows and {} columns in the test dataset\".format(test_df.shape[0],test_df.shape[1]))\n","e3b4c520":"train_df.columns","07417ae6":"# print(\"train_df\")\n# print(train_df.info())\n# print(\"##\"*50)\n# print(test_df.info())","6d2d8143":"train_obj=train_df.select_dtypes(include=[\"object\"])\ntrain_num=train_df.select_dtypes(exclude=[\"O\"])\ntest_obj=test_df[train_obj.columns.values]\ntest_num=test_df.select_dtypes(exclude=[\"O\"])\n\ntrain_num.head().style.background_gradient(cmap=\"plasma\")","b03d70e4":"train_obj.columns.values","dc5c9765":"print(train_obj.shape)\nprint(test_obj.shape)","58ab364f":"plt.figure(figsize=(6,4))\nsns.distplot(train_num[\"SalePrice\"])\nplt.xlabel(\"value\")\nplt.ylabel(\" # of houses\")\nplt.title(\"SALE PRICE VARING\")\nplt.show()","a62a16a7":"## here we cam see that these columns are less important than others\n##hence drop these columns.\ntrain_num.corr().SalePrice[lambda x : abs(x)< 0.2]\n","5ed17721":"## drop these unwanted columns\nfor  x in train_num:\n    y=train_num.corr().SalePrice[x]\n    if abs(y)<0.2:\n        train_num=train_num.drop([\"{}\".format(x)],axis=1)\n        test_num=test_num.drop([\"{}\".format(x)],axis=1)\n        \nprint(train_num.columns)","b0626d9c":"## see what are the most corrlated ones \nmst_corr=train_num.corr().SalePrice[lambda x : abs(x)> 0.5].index\nmst_corr","21af84d6":"# ## see how the corrlation varing\n# plt.figure(figsize=(8,8))\n# sns.heatmap(np.corrcoef(train_num[mst_corr].values.T),annot=True,xticklabels=mst_corr,yticklabels=mst_corr)\n# plt.show()","6d2324ab":"# # see how plot works\n# more_corr=train_num.corr().SalePrice[lambda x : abs(x)> 0.6].index\n# sns.set()\n# sns.pairplot(train_num[more_corr.values])\n# plt.show()","d3a7ce23":"train_num.shape","605258ea":"train_num.isnull().sum()","bbb22dd6":"train_num.LotFrontage.hist()\n\n\ntrain_num.LotFrontage.mean()","ed2b575c":"##clearly we can see that most data are between 45-75\n##fill it with the random numbers\n#  train_num[\"LotFrontage\"]=train_num[\"LotFrontage\"].fillna((45,75))\nmean=train_num.LotFrontage.mean()\nstd=train_num.LotFrontage.std()\ntot=train_num.LotFrontage.isnull().sum()\n\nrand=np.random.randint(mean-std,mean+std,size=tot)\ntrain_num[\"LotFrontage\"][np.isnan(train_num.LotFrontage)]=rand\ntest_num[\"LotFrontage\"][np.isnan(test_num.LotFrontage)]=np.random.randint(mean-std,mean+std,size=test_num.LotFrontage.isnull().sum())\ntrain_num.isnull().sum()\n##after filling the data see the distributiuon\ntrain_num.LotFrontage.hist()","4c2beb21":"# fill other missing value consist columns\ntrain_num.GarageYrBlt.isnull().sum()\ntrain_num.GarageYrBlt.hist()\ntrain_num.GarageYrBlt.describe()\n","64a39aea":"train_num[\"GarageYrBlt\"]=train_num[\"GarageYrBlt\"].fillna(round(train_num[\"GarageYrBlt\"].mean()))\ntest_num[\"GarageYrBlt\"]=test_num[\"GarageYrBlt\"].fillna(round(test_num[\"GarageYrBlt\"].mean()))\ntrain_num[\"GarageYrBlt\"].isnull().sum()","adcf1967":"train_num.MasVnrArea.isnull().sum()\ntrain_num.MasVnrArea.hist()\ntrain_num.MasVnrArea.describe()\n\n","aac93121":"\n\ntrain_num[\"MasVnrArea\"]=train_num[\"MasVnrArea\"].fillna(train_num[\"MasVnrArea\"].mean())\ntest_num[\"MasVnrArea\"]=test_num[\"MasVnrArea\"].fillna(test_num[\"MasVnrArea\"].mean())\ntrain_num[\"MasVnrArea\"].isnull().sum()","61a95099":"##outliers check\nplt.scatter(train_num.SalePrice,train_num.OverallQual) ##since most corralted one with the Saleprice\nplt.axvline(700000)\n","29301381":"##remove outliers\n\ntrain_num=train_num.drop(train_num[train_num['SalePrice']>7000000 ].index, )\n","26ff1c77":"train_num.isnull().sum()\n##numeric part is done","3e570ee6":"##obj part\ndrop_ids=train_obj.isnull().sum()[lambda x : x !=0  ]\ndrop_L_id=drop_ids[lambda x :x <500].index.values\ndrop_ids=drop_ids[lambda x :x >500].index\n","5e821d26":"# drop most missing value contain columns\ntrain_obj=train_obj.drop(drop_ids.values,axis=1)\ntest_obj=test_obj.drop(drop_ids.values,axis=1)\n\n","2a0ab816":"## fill other columns with the most freqemt ones\nimputer=SimpleImputer(missing_values=np.NAN,strategy='most_frequent')","95b3710d":"imputer = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\nfor x in drop_L_id:\n    \n    train_obj[x] = imputer.fit_transform(train_obj[x].values.reshape((-1, 1)))\n    test_obj[x] = imputer.fit_transform(test_obj[x].values.reshape((-1, 1)))\n\ntrain_obj.isnull().sum()","37f3885a":"\ntrain_num[\"SalePrice\"]=np.log(1+train_num[\"SalePrice\"])\nstats.probplot(train_num.SalePrice,plot=plt)","79a9a2cc":"# do the same for most corralated columns:\ntrain_num[\"OverallQual\"]=np.log(1+train_num[\"OverallQual\"])\ntrain_num[\"GarageArea\"]=np.log(1+train_num[\"GarageArea\"])\ntrain_num[\"GrLivArea\"]=np.log(1+train_num[\"GrLivArea\"])\n\n##for test data\ntest_num[\"OverallQual\"]=np.log(1+test_num[\"OverallQual\"])\ntest_num[\"GarageArea\"]=np.log(1+test_num[\"GarageArea\"])\ntest_num[\"GrLivArea\"]=np.log(1+test_num[\"GrLivArea\"])\n\n","83a05fb1":"print(train_num.shape)\nprint(train_obj.shape)","94ddb1f3":"print(test_num.shape)\nprint(test_obj.shape)","a5d69a1b":"# for feature in test.drop(['LotFrontage'], axis=1):\n#     \n#     test_obj[feature] = imputer.fit_transform(test_obj[feature].values.reshape((-1, 1)))","93eb7b5a":"  ##Catorical into labels\ntrain_obj=pd.get_dummies(train_obj)\ntest_obj=pd.get_dummies(test_obj)\n#finally concat both things togher\n\ntrain_obj.isnull().sum()\ntrain=pd.concat([train_num,train_obj],axis=1)\n\ntest=pd.concat([test_num,test_obj],axis=1)","e95f91f8":"othmiss=test.isnull().sum()[lambda x : x>0 ].index\n\nfor x in othmiss:\n    test[x] = imputer.fit_transform(test[x].values.reshape((-1, 1)))\n    \n    \n\n","a3079228":"\nx = train.drop(['SalePrice'], axis=1)\n\ny= train['SalePrice']\n","e60859c1":"test.head()","283ad080":"print(train.shape)\nprint(test.shape)","d2d70257":"test_df.head()","3282f60b":"err={}\n##linear regression\nLr=LinearRegression()\nLr.fit(x,y)\n\nLr_predict=Lr.predict(x)\nLr.score(x,y)\n\n###find the error\nlr_err=mean_squared_log_error(Lr_predict,y)\nerr[\"lr_err\"]=lr_err\nlr_err\n","4813e503":"##ridge prediction\nfrom sklearn.linear_model import Ridge\nridge_model= Ridge(alpha=10)\nridge_model.fit(x, y)\ny_pred= ridge_model.predict(x)\n##error \nrid_err=mean_squared_log_error(y_pred,y)\nerr[\"rid_err\"]=rid_err\nrid_err","6c83a9c6":"##random Forest Regressor\nforest_reg = RandomForestRegressor()\nforest_reg.fit(x, y)\nRFR_pr = forest_reg.predict(x)\n# print(forest_reg.score(x, y))\nRFR_err=(mean_squared_log_error(RFR_pr, y))\nerr[\"RFR_err\"]=RFR_err\nRFR_err","074994d2":"import xgboost as xg\nxg_reg = xg.XGBRegressor(objective ='reg:linear',\n                   n_estimators = 300, \n                          seed = 123)\nxg_reg.fit(x, y)\nxg_pred = xg_reg.predict(x)\n# print(xg_reg.score(x, y))\nxg_err=(mean_squared_log_error(xg_pred, y))\nerr[\"xg_err\"]=xg_err\nxg_err","e510d09e":"###lassso regression\nfrom sklearn.linear_model import LassoCV\nlasso_cv_model= LassoCV(eps=0.01, n_alphas=100, cv=5)\nlasso_cv_model.fit(x,y)\nlass_pred=lasso_cv_model.predict(x)\nlass_err=mean_squared_log_error(lass_pred,y)\nerr[\"lass_err\"]=lass_err\nlass_err\n","6e57e9d6":"##elasticnet cv predictions\nfrom sklearn.linear_model import ElasticNetCV\nelastic_model= ElasticNetCV(l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1],cv=5, max_iter=100000)\nelastic_model.fit(x, y)\nelastic_pred=elastic_model.predict(x)\nela_err=mean_squared_log_error(elastic_pred,y)\nerr[\"ela_err\"]=ela_err\nela_err","7e960ad7":" err={k: v for k, v in sorted(err.items(), key=lambda item: item[1])}\nerr","d687262e":"rest = set(x.columns.values) - set(list(test.columns.values))\nfor feature in list(rest):\n    test[feature] = 0","c3e3d363":"# from sklearn.ensemble import StackingRegressor\n# from sklearn.pipeline import make_pipeline\n# from sklearn import linear_model\n# xgb=xg\n# import lightgbm as lgb\n# from sklearn.neighbors import KNeighborsRegressor\n# from sklearn.svm import SVR\n# from sklearn.preprocessing import RobustScaler","10274aee":"# base_learners = [\n#                  ('Lasso', linear_model.Lasso(tol = 1e-7, \n#                            alpha = 0.00028, max_iter = 3000)),\n    \n#                  ('El_Net', linear_model.ElasticNet(tol = 1e-6, \n#                             alpha = 0.00044, l1_ratio = 0.61, max_iter = 4000)),\n    \n#                  ('XGB', xgb.XGBRegressor(use_label_encoder = False, \n#                          eval_metric = 'rmse',                   \n#                          n_estimators = 5000,\n#                          reg_alpha = 0.1,\n#                          reg_lambda = 0.005,\n#                          learning_rate = 0.0125,\n#                          max_depth = 13,\n#                          min_child_weight = 4,\n#                          gamma = 0.04,\n#                          subsample = 0.7,\n#                          colsample_bytree = 0.6)),\n    \n#                  ('LGBM', lgb.LGBMRegressor(\n#                           n_estimators = 9000,\n#                           reg_lambda = 1.8,\n#                           reg_alpha = 0.01,\n#                           min_child_samples = 13,\n#                           subsample = 0.8,\n#                           subsample_freq = 11,\n#                           num_leaves = 101,\n#                           max_depth = 3,\n#                           max_bin = 160,\n#                           learning_rate = 0.005,\n#                           colsample_bytree = 0.1)),\n    \n#                  ('KNN', make_pipeline(RobustScaler(), \n#                          KNeighborsRegressor(\n#                          leaf_size = 25,\n#                          n_neighbors = 9,\n#                          p = 1,\n#                          weights = 'distance',\n#                          metric = 'minkowski',\n#                          algorithm = 'ball_tree'))),\n    \n#                  ('SVR', make_pipeline(RobustScaler(), \n#                          SVR(\n#                          kernel = 'rbf',\n#                          C =  10, \n#                          epsilon =  0.017,\n#                          gamma =  0.0007)))\n#                 ]","d6e10d3f":"# Final_stack = StackingRegressor(estimators = base_learners, \n#                                 final_estimator =xgb.XGBRFRegressor())","7eddf737":"import lightgbm as lgb\nlgb=lgb.LGBMRegressor(\n                          n_estimators = 9000,\n                          reg_lambda = 1.8,\n                          reg_alpha = 0.01,\n                          min_child_samples = 13,\n                          subsample = 0.8,\n                          subsample_freq = 11,\n                          num_leaves = 101,\n                          max_depth = 3,\n                          max_bin = 160,\n                          learning_rate = 0.005,\n                          colsample_bytree = 0.1)","b7e72a0a":"lgb.fit(x,y)\nlgb_pred=lgb.predict(x)\nlgb_err=mean_squared_log_error(lgb_pred,y)\nerr[\"lgb_err\"]=lgb_err\nlgb_err","4bff7e6f":"from sklearn.neighbors import KNeighborsRegressor \nknn_model=KNeighborsRegressor(\n                         leaf_size = 25,\n                         n_neighbors = 9,\n                         p = 1,\n                         weights = 'distance',\n                         metric = 'minkowski',\n                         algorithm = 'ball_tree')\n\n\nknn_model.fit(x.to_numpy(),y.values)\nknn_output=knn_model.predict(x.to_numpy())\nknn_predict=mean_squared_log_error(knn_output,y)\nerr[\"Knn_err\"]=knn_predict\n\n# err[\"knn+xg\"]\nknn_predict","408eaa1b":"# from catboost import CatBoostRegressor\n\n# # dataset = numpy.array([[1,4,5,6],[4,5,6,7],[30,40,50,60],[20,15,85,60]])\n# # train_labels = [1.2,3.4,9.5,24.5]\n# # model = CatBoostRegressor(learning_rate=1, depth=6,)\n# model.fit(x,y)\n","df6a8801":" err={k: v for k, v in sorted(err.items(), key=lambda item: item[1])}\nerr","b216bea7":"## missing values Checking part for Test dataset\n\n# import pandas as pd\n\n# def clean_dataset(df):\n#     assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n#     df.dropna(inplace=True)\n#     indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n#     return df[indices_to_keep].astype(np.float64)\n\n#  test_new=clean_dataset(test)\n# test_new.shape\n\nnp.any(np.isnan(test.to_numpy()))","0dd3ea16":"x.shape","763b356a":"##made same column length\nrest = set(x.columns.values) - set(list(test.columns.values))\nfor feature in list(rest):\n    test[feature] = 0","a98e1f9f":" test_predict=np.exp(knn_model.predict(test))-1\n","5b161d4c":"ids = test_df['Id']\n \nsubmission = pd.DataFrame({\n        \"Id\": ids,\n        \"SalePrice\": test_predict\n    })\nsubmission.to_csv('submission.csv', index=False)\n\nsubmission","783f87f6":"## Stacking the model","e9e7213e":"## Train the Model","7368a495":"## make data for test","a94f9832":"### First let's analyze numerical dataset","fa81c1ad":"## Scaling","5fa7387c":"# Submission","fcd5d46d":"# Model Trainings"}}