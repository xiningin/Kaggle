{"cell_type":{"a82135ac":"code","91c63e9a":"code","11db244e":"code","ac2b4e4a":"code","1812800a":"code","7893a519":"code","d1a003b3":"code","fb9c729d":"code","26c6f135":"code","932b45c8":"code","f976e81d":"code","fc67c099":"code","891b5d90":"code","5652cd08":"code","dbacb18a":"code","9e02e2e3":"code","226c4653":"code","8e3a8631":"code","209ae4ad":"code","b8ff0891":"code","5210bd80":"code","4d28595f":"code","544f256c":"code","bac348fa":"code","a86f44d4":"code","2ff0edc9":"code","da540670":"markdown","a52f617a":"markdown","39c1a3ce":"markdown","547d8bea":"markdown","c4ea7de7":"markdown","771d9ff9":"markdown","2f8fefe9":"markdown","3fcf7936":"markdown","fb982b78":"markdown","26fcdfa8":"markdown","e7b9b7e7":"markdown","f73153d9":"markdown","5e63f8b1":"markdown","36e3e4c9":"markdown","183058cc":"markdown","77dcd27d":"markdown","b2b3a357":"markdown","8aebee20":"markdown"},"source":{"a82135ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","91c63e9a":"#import modules\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","11db244e":"data=pd.read_csv(\"..\/input\/logistic-regression-heart-disease-prediction\/framingham_heart_disease.csv\")\ndata.head()\n\n\n","ac2b4e4a":"data.shape","1812800a":"data.describe().T","7893a519":"data.isnull().sum()","d1a003b3":"data.dropna(axis=0,inplace=True)\ndata.isnull().sum()","fb9c729d":"print(data.TenYearCHD.value_counts())\nsns.countplot(x='TenYearCHD',data=data,palette='hls')\nplt.show()","26c6f135":"sns.scatterplot(data=data,x=\"diaBP\",y=\"heartRate\",hue=\"TenYearCHD\",size=50,legend=False)","932b45c8":"healthy=len(data[data['TenYearCHD']==0])\ndiseased=len(data[data['TenYearCHD']==1])\nPercent_of_healthy = healthy\/(healthy+diseased)\nPercent_of_diseased= diseased\/(healthy+diseased)\nprint(\"Percentage of healthy people is : \",Percent_of_healthy*100)\nprint(\"Percentage of diseased people is : \",Percent_of_diseased*100)","f976e81d":"plt.figure(figsize=(20,20))\nsns.heatmap(data.corr(),annot=True,cmap='RdYlGn')","fc67c099":"data=data.drop('education',axis=1)\n\ndata=data.drop('cigsPerDay',axis=1)","891b5d90":"data.groupby('TenYearCHD').mean()","5652cd08":"data= data.drop(['diaBP','BMI','heartRate'],axis=1)","dbacb18a":"data.head()","9e02e2e3":"x = data.loc[:,data.columns != 'TenYearCHD']\ny = data.loc[:,data.columns == 'TenYearCHD']","226c4653":"from imblearn.under_sampling import RandomUnderSampler\nfrom collections import Counter","8e3a8631":"# instantiating the random undersampler\nrus = RandomUnderSampler() \n# resampling X, y\nX_rus, y_rus = rus.fit_resample(x, y)\n# new class distribution\nprint(Counter(y_rus))","209ae4ad":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nX_train, X_test, y_train, y_test = train_test_split(X_rus, y_rus, test_size=0.3, random_state=0)\nlogreg = LogisticRegression(class_weight=\"balanced\",max_iter=1000)\nlogreg.fit(X_train, y_train)","b8ff0891":"y_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))","5210bd80":"from sklearn.metrics import confusion_matrix\ncnf = confusion_matrix(y_test, y_pred)\n\n\nfig, ax = plt.subplots(figsize=(5, 5))\nax.imshow(cnf)\nax.grid(False)\nax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\nax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\nax.set_ylim(1.5, -0.5)\nfor i in range(2):\n    for j in range(2):\n        ax.text(j, i, cnf[i, j], ha='center', va='center', color='red')\nplt.show()","4d28595f":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","544f256c":"from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\nmae = mean_absolute_error(y_test,y_pred)\nmse= mean_squared_error(y_test,y_pred)\nr2 = r2_score(y_test,y_pred)","bac348fa":"print(mae)\n","a86f44d4":"print(mse)\n","2ff0edc9":"print(r2)","da540670":"**Implementing the model**","a52f617a":"R2 score","39c1a3ce":"*Checking NULL values in the dataset*","547d8bea":"As diaBP,BMI,heartRate have values in similar zones and almost equal to one another for 1 or 0 values for TenYearCHD hence they can be dropped.\n\nIn other words,these 3 have low variance for categoreies of TenYearCHD","c4ea7de7":"Mean Squared error","771d9ff9":"importing libraries","2f8fefe9":"**Data Exploration**","3fcf7936":"> Random UnderSampling\n\nThis method seeks to randomly select and remove samples from the majority class, consequently reducing the number of examples in the majority class in the transformed data.\n\n\u201cIn random under-sampling (potentially), vast quantities of data are discarded. This can be highly problematic, as the loss of such data can make the decision boundary between the minority and majority instances harder to learn, resulting in a loss in classification performance.\u201d\n\nUsing this approach is effective in situations where the minority class has a sufficient amount of examples despite the severe imbalance. On the other hand, it is always important to consider the prospects of valuable information being deleted as we randomly remove them from our data set since we have no way to detect or preserve the examples that are information rich in the majority class.","fb982b78":"Mean Absolute Error","26fcdfa8":"We will eliminate the Null Values by droping the rows having the NULL values","e7b9b7e7":" From the above plots , we observe that our dataset is unbalanced.\n\nWe will balance it using various methods during fitting the model.Balancing is important as logistic regression on imbalanced dataset will give biased results.","f73153d9":"Confusion Matrix","5e63f8b1":"**Model Evaluation**","36e3e4c9":"For balancing the dataset we will use Random Undersampling.\n\n\n","183058cc":"**Feature Selection**","77dcd27d":"From the above correlation matrix we can find a very high correlation between currentSmoker and cigsPerDay,hence we can drop any one of them.\n\nAlso,education does not play any important role in heart attack hence we can also drop that column.","b2b3a357":"**Dataset Balancing**","8aebee20":"*Plotting Data w.r.t. the target class*"}}