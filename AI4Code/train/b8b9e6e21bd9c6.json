{"cell_type":{"08b92462":"code","384eebfa":"code","41ffbfe7":"code","d2d712de":"code","71bf5cb5":"code","272b9c96":"code","67ae04bb":"code","e3ab9535":"code","4204bed6":"code","ed58b78c":"code","5849363e":"code","e414ed2f":"code","3e51308e":"code","ab7ecb83":"code","145c5552":"code","bc36393b":"code","91773ef8":"code","473d3fa7":"code","58ed7522":"code","1d9f624a":"code","f75b309f":"code","71850d1d":"code","769a1f16":"code","1cba0087":"code","5f258cc7":"code","3a90c7a9":"code","9bd433ab":"code","faee7448":"markdown","700fd983":"markdown","55eb505b":"markdown","6dcda656":"markdown","18504a0a":"markdown","bf7f8dd3":"markdown","d02951c6":"markdown","08a3077e":"markdown","518714c2":"markdown","168f3724":"markdown","143e6b67":"markdown","5d99de8f":"markdown","4d1a37f1":"markdown","aa05e51a":"markdown","552eb674":"markdown"},"source":{"08b92462":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)import \nimport seaborn as sns\nimport folium\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","384eebfa":"data = pd.read_csv('..\/input\/eceap-sites\/DCYF_ECEAP_Sites.csv')","41ffbfe7":"data.shape","d2d712de":"data.columns","71bf5cb5":"address_df = data.dropna(how='all', subset=['GeoCodedPhysicalAddress'])\naddress_df.shape","272b9c96":"data['GeoCodedPhysicalAddress'].head()","67ae04bb":"# Prepare lists to hold the data\nlong = []\nlat = []\nresult = []\n#traverse the column by row, this for loop will return a string\nfor element in address_df['GeoCodedPhysicalAddress']:\n    piece_list = element.split('\\n') ### The column seperated the address by next lines\n    last_element = piece_list[-1]  ### each geo coordinate is in the end of the record\n    if(last_element[0] == '('): # some elements dont have geo coordinates only physical addresses\n        last_element = last_element[1:] #Remove the '(' in the beggining of the coor\n        last_element = last_element[:-1] #Remove the ')' in the end of the coor\n        list_coor = last_element.split(',') # seperate the lat, long \n        list_coor[1] = list_coor[1][1:] #Removes the empty space in the front of the longitude coordinate\n        long.append(float(list_coor[0])) \n        lat.append(float(list_coor[1])) \n        result.append(element)  # we have to save which element we got for later, so we know which ones had geo coordinates \ndf_long = pd.DataFrame(long)\ndf_lat = pd.DataFrame(lat)","e3ab9535":"data = data[data['GeoCodedPhysicalAddress'].isin(result)]# we remove from the data what rows dident have coordinates\ndata['Latitude'] = df_lat\ndata['Longitude'] = df_long","4204bed6":"# import libraries for mapping\nimport geopandas as gpd\nfrom shapely.geometry import Point, Polygon\nimport matplotlib.pyplot as plt","ed58b78c":"gdf = gpd.GeoDataFrame(\n    data, geometry=gpd.points_from_xy(data.Latitude, data.Longitude))\ngdf.head()","5849363e":"fp = '..\/input\/washingtonboundaries\/WA_State_Boundary.shp'\nwa_map = gpd.read_file(fp)","e414ed2f":"wa_map.bounds","3e51308e":"gdf.set_crs(epsg = 3857, inplace = True)\ngdf.crs","ab7ecb83":"ax = wa_map.plot(figsize=(15,15), color='red')\ngdf.plot(ax=ax, figsize=(15,15), marker = '*', color='lightgreen')\nplt.show()","145c5552":"import requests\npage = requests.get(\"https:\/\/en.wikipedia.org\/wiki\/List_of_Washington_locations_by_per_capita_income\")\npage.status_code","bc36393b":"from bs4 import BeautifulSoup as bsoup\nsoup = bsoup(page.content, 'html.parser')\nprint(soup)","91773ef8":"table = soup.find(class_='wikitable sortable')\ntable_rows = table.find_all('tr')\ncounty = []\nper_capita = []\nmedian = []\npopulation = []\nfor tr in table_rows:\n    county.append(tr.contents[3].text.rstrip())\n    per_capita.append(tr.contents[5].text.rstrip())\n    median.append(tr.contents[7].text.rstrip())\n    population.append(tr.contents[11].text.rstrip())","473d3fa7":"d = {'County':county,'Per_Capita_Income':per_capita,'Median_Household_Income':median, 'Population': population} # turn the 3 lists into a dictionary for easy transference\ndf = pd.DataFrame(d)\nprint(df.shape)\ndf.head()","58ed7522":"df = df.iloc[1:]\ndf.reset_index(drop = True, inplace = True)\ndf.head()","1d9f624a":"joined_df = data.join(df.set_index('County'), on='County')\njoined_df.columns","f75b309f":"slots_df = joined_df[joined_df['County'] == 'Spokane']\npopulation = slots_df['Population'].iloc[1]\nfunded = sum(slots_df['Total Funded Slots'])\nprint(population)","71850d1d":"data_count_values = data['County'].value_counts()","769a1f16":"new_list = []\nfor item in df['Median_Household_Income']:\n    item = item.replace('$','')\n    item = item.replace(',','')\n    new_list.append(float(item))\ndf3 = pd.DataFrame({'county': df['County'], 'median': new_list})\nax3 = df3.plot.bar(figsize= (12,6), x='county', y='median')","1cba0087":"df['Population'] = df['Population'].str.replace(\",\",\"\",)\ndf['Population'] = df['Population'].astype(int)","5f258cc7":"df.drop([4,10],inplace = True)","3a90c7a9":"df.head()","9bd433ab":"ax4 = df.plot.bar(figsize= (12,6), x='County', y='Population')\nax5 = ax4.twinx()  # instantiate a second axes that shares the same x-axis\n\ncolor = 'tab:blue'\nax5.set_ylabel('Number of ECEAPs', color= 'black') \nax5.plot( data['County'].unique(),data_count_values , color='black')\nax5.tick_params(axis='y', labelcolor=color)","faee7448":"The process I use in pulling the websites data from the table is followed. You have to know a little html and there are websites that do not allow this so be aware of permission issues. I scroll htrough the html code until I find the information that associates with the data. Then I find the taglines. Beautiful Soup is a fantastic sraping tool that makes this process very easy once a little learning is complete. ","700fd983":"Notice that the latitude and longitude coordinates are attached to the end of the row. They are seperated by the next line '\\n' string. Thus if we parse on this we can prepare the data for the next task. ","55eb505b":"We will first inspect the data to understand the importance of the impact made and look at locations to tell if the environment the sites are in are of the nature for impact. I mean to determine the process used in establishing a place of value dependet upon the environment. This way the mission statement of ECEAP can be recognized.","6dcda656":"I personally have involved myself for many years now in advocating for Early Childhood Education in a program known as ECEAP. The purpose of this notebook is to better understand the impact that ECEAP is making. Early Childhood education and Assitance Program (ECEAP, pronounced 'E-cap') is a program designed around targeting low income and at risk families and helping with a comprehensives family approach of State Funded Preschool. Its impact has been proven to have a long term impact on the educational success and family development of the child and their family. I admire anyone who has been apart of the program.","18504a0a":"The data aquired was from https:\/\/data.wa.gov\/Education\/DCYF-ECEAP-Sites\/8ydb-ddzd\/data a Washingotn State sponsered database.","bf7f8dd3":"The import of the data for the address type from the website, merged the geo-coordinates and physical addresses of the sites. Thus a little data management is in order to establish columns that are useable. The latitude and longitude coordinates ar ewhat is being extracted. ","d02951c6":"## Importing information from Wikipedia for per capita income, relevant to county","08a3077e":"The dictionary gets created from the data pulled in. HTML requires headers of the columns inside the same tags, so lets remove them. ","518714c2":"### Odd import occurence","168f3724":"Next let's inspect the ratios available in population totals vs. county sizes against the total sites compared to county sites. ","143e6b67":"To aquire the data I use Washington States Data website to pull info about the location and connections of ECEAP so that we can explore its locations to recognize where the need is at. Later I webscrape to determine the median income of counties in Washington State. Then we perform some exploratory analysis on the data. ","5d99de8f":"# Understanding the impact of ECEAP through Beautiful Soup Web Scraping and Feature Engineering\n![](https:\/\/www.wsaheadstarteceap.com\/images\/img_library\/crp\/1-20-9-Parent%20Ambassadors%202013%20copy1.jpg)","4d1a37f1":"![](https:\/\/www.wsaheadstarteceap.com\/images\/design\/main\/logo.jpg)","aa05e51a":"Here we will be taking a look at the level of income and the amount of slots that are funded by the state.","552eb674":"Next we begin pulling in data from wikipedia for county income. We are going to compare the county income to the amount of ECEAP sites in each area. "}}