{"cell_type":{"50b9b5bf":"code","54224f65":"code","6e0b8559":"code","a5e9e92c":"code","77bc4fd7":"code","7fbf0e2e":"code","5a007ff7":"code","f70c9816":"code","2fc5d44a":"code","dd5010d9":"code","1d2f74b0":"code","27aa73b2":"code","6c2b5dc2":"code","5f901df9":"code","dbc13274":"code","2947145a":"code","9ac16512":"code","ea23516d":"code","c2425646":"code","a981702b":"code","c5988ff2":"code","afd5c28a":"code","e43bb60f":"code","dc6ce3b4":"code","f271e368":"code","1a50332a":"code","69b88825":"markdown","6a90d656":"markdown","5eb3da4f":"markdown","5064f0a2":"markdown","41ae423b":"markdown","72be73bb":"markdown","4928cbfe":"markdown","779918af":"markdown","d0c19d1b":"markdown","46b92583":"markdown","36704876":"markdown","011d6fcd":"markdown","1086f873":"markdown","6cb323e3":"markdown","79b27acf":"markdown","557fb9d0":"markdown","853bc8b5":"markdown","b48e117a":"markdown","73086ee9":"markdown","34c3632a":"markdown","0cca0f96":"markdown","26addb81":"markdown","935a07b1":"markdown","ab94c8c4":"markdown","6e4422ec":"markdown","74c7bc5d":"markdown","3bd4018d":"markdown","b3eb8b2c":"markdown","00969b08":"markdown","82fa4cd2":"markdown","ffbc09eb":"markdown","27f05671":"markdown","9e9ddb37":"markdown","a2bdb96b":"markdown","ccc6579b":"markdown","86718462":"markdown","fabae672":"markdown","5dfe5ad5":"markdown","c6855536":"markdown","a716d025":"markdown","a32548d9":"markdown","b4c3455c":"markdown","96e3412e":"markdown","c8a32d5c":"markdown","af754a97":"markdown"},"source":{"50b9b5bf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","54224f65":"from IPython.display import HTML\nHTML(\"\"\"\n<style>\nh1,h2,h3 {\n\tmargin: 1em 0 0.5em 0;\n\tfont-weight: 600;\n\tfont-family: 'Titillium Web', sans-serif;\n\tposition: relative;  \n\tfont-size: 36px;\n\tline-height: 40px;\n\tpadding: 15px 15px 15px 2.5%;\n\tcolor: #00018D;\n\tbox-shadow: \n\t\tinset 0 0 0 1px rgba(97,0,45, 1), \n\t\tinset 0 0 5px rgba(53,86,129, 1),\n\t\tinset -285px 0 35px #F2D8FF;\n\tborder-radius: 0 10px 0 15px;\n\tbackground: #FFD8B2\n    \n}\n<\/style>\n\"\"\")","6e0b8559":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, plot, iplot\nimport cv2\nimport random\nimport os\nimport glob\nfrom tqdm.notebook import tqdm\nimport albumentations as A\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, Flatten, MaxPooling2D, Dense, Dropout , BatchNormalization\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import VGG16\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom keras.callbacks import ReduceLROnPlateau","a5e9e92c":"train_data = glob.glob('..\/input\/chest-xray-pneumonia\/chest_xray\/train\/**\/*.jpeg')\ntest_data = glob.glob('..\/input\/chest-xray-pneumonia\/chest_xray\/test\/**\/*.jpeg')\nval_data = glob.glob('..\/input\/chest-xray-pneumonia\/chest_xray\/val\/**\/*.jpeg')\n\nprint(\"\u3030\"*20)\nprint(f\"Training Set has: {len(train_data)} images\")\nprint(f\"Testing Set has: {len(test_data)} images\")\nprint(f\"Validation Set has: {len(val_data)} images\")\nprint(\"\u3030\"*20)","77bc4fd7":"DIR = \"..\/input\/chest-xray-pneumonia\/chest_xray\/\"\nsets = [\"train\", \"test\", \"val\"]\nall_pneumonia = []\nall_normal = []\n\nfor cat in sets:\n    path = os.path.join(DIR, cat)\n    norm = glob.glob(os.path.join(path, \"NORMAL\/*.jpeg\"))\n    pneu = glob.glob(os.path.join(path, \"PNEUMONIA\/*.jpeg\"))\n    all_normal.extend(norm)\n    all_pneumonia.extend(pneu)\nprint(\"\u3030\"*20)\nprint(f\"Total Pneumonia Images: {len(all_pneumonia)}\")\nprint(f\"Total Normal Images: {len(all_normal)}\")\nprint(\"\u3030\"*20)","7fbf0e2e":"## Plotly chart for Class distribution\nlabels = [\"Normal\",'Pneumonia ']\nvalues = [len(all_normal), len(all_pneumonia)]\ncolors = ['green', 'pink']\nfig = go.Figure(data=[go.Pie(labels=labels,\n                             values=values,hole=.5)])\nfig.update_traces(hoverinfo='value', textinfo='label+percent', textfont_size=20,\n                  marker=dict(colors=colors, line=dict(color='#000000', width=3)))\nfig.update_layout(title=\"Image Category Distribution\",\n                  titlefont={'size': 30},      \n                  )\niplot(fig)","5a007ff7":"random.shuffle(all_normal)\nrandom.shuffle(all_pneumonia)\nimages = all_normal[:50] + all_pneumonia[:50]","f70c9816":"\nfig=plt.figure(figsize=(15, 10))\ncolumns = 4; rows = 5\nfor i in range(1, columns*rows +1):\n    img = cv2.imread(images[i])\n    img = cv2.resize(img, (128, 128))\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(img)\n    plt.axis(False)","2fc5d44a":"fig=plt.figure(figsize=(15, 10))\ncolumns = 4; rows = 2\nfor i in range(1, columns*rows +1):\n    img = cv2.imread(images[i])\n    img = cv2.resize(img, (512, 512))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n    img = cv2.addWeighted (img, 4, cv2.GaussianBlur(img, (0,0), 512\/10), -4, 128)\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(img)\n    plt.axis(False)","dd5010d9":"fig=plt.figure(figsize=(15, 10))\ncolumns = 4; rows = 2\nfor i in range(1, columns*rows +1):\n    img = cv2.imread(images[i])\n    img = cv2.resize(img, (512, 512))\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n    f = np.fft.fft2(img)\n    fshift = np.fft.fftshift(f)\n    magnitude_spectrum = 20*np.log(np.abs(fshift))\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(magnitude_spectrum)\n    plt.axis(False)","1d2f74b0":"fig=plt.figure(figsize=(15, 10))\ncolumns = 5; rows = 2\nfor i in range(1, columns*rows +1):\n    img = cv2.imread(images[i])\n    img = cv2.resize(img, (512, 512))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    kernel = np.ones((5, 5), np.uint8)\n    img_erosion = cv2.erode(img, kernel, iterations=3)\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(img_erosion)\n    plt.axis(False)","27aa73b2":"fig=plt.figure(figsize=(15, 10))\ncolumns = 5; rows = 2\nfor i in range(1, columns*rows +1):\n    img = cv2.imread(images[i])\n    img = cv2.resize(img, (512, 512))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    kernel = np.ones((5, 5), np.uint8)\n    img_erosion = cv2.dilate(img, kernel, iterations=3)\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(img_erosion)\n    plt.axis(False)\n","6c2b5dc2":"fig=plt.figure(figsize=(15, 10))\ncolumns = 5; rows = 2\nfor i in range(1, columns*rows +1):\n    img = cv2.imread(images[i])\n    img = cv2.resize(img, (512, 512))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    edges = cv2.Canny(img, 80, 100)\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(edges)\n    plt.axis(False)","5f901df9":"train_gen = ImageDataGenerator(\n    rescale=1\/255.,\n    horizontal_flip=True,\n    vertical_flip=False,\n    rotation_range=0.3,\n    zoom_range=0.4\n)\nval_gen = ImageDataGenerator(\n    rescale=1\/255.,\n)","dbc13274":"Train = train_gen.flow_from_directory(\n    \"..\/input\/chest-xray-pneumonia\/chest_xray\/train\",batch_size=16,\n    target_size=(224, 224),#class_mode=\"binary\" \n)\nval = train_gen.flow_from_directory(\n    \"..\/input\/chest-xray-pneumonia\/chest_xray\/test\",batch_size=8,\n    target_size=(224, 224),#class_mode=\"binary\" \n)","2947145a":"model = Sequential()\nmodel.add(Conv2D(32,(3,3),strides=(1, 1),activation='relu',padding='same', input_shape=(224, 224, 3)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Conv2D(64,(3,3),strides=(1, 1) ,padding='same',activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Conv2D(128,(3,3),strides=(1, 1),padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Conv2D(256,(3,3),strides=(1, 1),padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Flatten())\n\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\nmodel.summary()","9ac16512":"## Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","ea23516d":"early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5,restore_best_weights=True)\n\nhistory = model.fit_generator(Train,epochs=20,validation_data=val,steps_per_epoch=50,callbacks=[early_stopping_cb])","c2425646":"plt.figure(figsize=(8,6))\nplt.title('Accuracy scores')\nplt.plot(history.history['accuracy'],'go-')\nplt.plot(history.history['val_accuracy'],'ro-')\nplt.legend(['accuracy', 'val_accuracy'])\nplt.show()\nplt.figure(figsize=(8,6))\nplt.title('Loss value')\nplt.plot(history.history['loss'],'go-')\nplt.plot(history.history['val_loss'],'ro-')\nplt.legend(['loss', 'val_loss'])\nplt.show()","a981702b":"base_model=VGG16(include_top=False, weights=None,input_shape=(224,224,3), pooling='avg',classes=2)","c5988ff2":"base_model.load_weights(\"..\/input\/vgg16\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\")","afd5c28a":"base_model.summary()","e43bb60f":"model=Sequential()\nmodel.add(base_model)\nmodel.add(Flatten())\n\nmodel.add(Dense(128,activation='relu'))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dense(2,activation='softmax'))\n\n## Freezing the layers\nfor layer in base_model.layers:\n    layer.trainable=False\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","dc6ce3b4":"history = model.fit_generator(Train,epochs=20,validation_data=val,steps_per_epoch=50,callbacks=[early_stopping_cb])","f271e368":"plt.figure(figsize=(8,6))\nplt.title('Accuracy scores')\nplt.plot(history.history['accuracy'],'go-')\nplt.plot(history.history['val_accuracy'],'ro-')\nplt.legend(['accuracy', 'val_accuracy'])\nplt.show()\nplt.figure(figsize=(8,6))\nplt.title('Loss value')\nplt.plot(history.history['loss'],'go-')\nplt.plot(history.history['val_loss'],'ro-')\nplt.legend(['loss', 'val_loss'])\nplt.show()","1a50332a":"from IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#5700AC','#eb3446','Nosifer','Smokum',35,10\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h4 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h4>\"\"\"%string))\n    \n    \ndhtml('Do UPVOTE if you like my work ' )","69b88825":"# \ud83d\udd0d Basic Data Exploration:","6a90d656":"<p style=\"font-size:15px;color:#1C37A6;font-weight:600;\">Now, let\u2019s look into the pixel distributions. We\u2019ll use the Fourier method for this.<\/p>","5eb3da4f":"<p style=\"font-size:15px;color:#1C37A6;font-weight:600;\">Shuffling the images randomly.<\/p>","5064f0a2":"\n<p style=\"font-size:15px;color:#1C37A6;font-weight:600;\">Now it\u2019s time to train the model! Here we are going to use fit_generator() instead of fit() because we are going to take the train data from the train_gen object.\n<\/p>\n","41ae423b":"<p style=\"font-size:15px;color:#1C37A6;font-weight:600;\">Pneumonia is an infection that inflames the air sacs in one or both lungs. The air sacs may fill with fluid or pus (purulent material), causing cough with phlegm or pus, fever, chills, and difficulty breathing. A variety of organisms, including bacteria, viruses and fungi, can cause pneumonia.<br>\nPneumonia can range in seriousness from mild to life-threatening. It is most serious for infants and young children, people older than age 65, and people with health problems or weakened immune systems.<\/p>","72be73bb":"<p style=\"font-size:15px;color:#1C37A6;font-weight:600;\">Viewing the images in X-ray<\/p>","4928cbfe":"\n<p style=\"font-size:15px;color:#1C37A6;font-weight:600;\">Now, let\u2019s visualize the performance of our model:\n<\/p>\n","779918af":"![](https:\/\/miro.medium.com\/max\/1896\/1*qsbsCVyu376kqdnNcdxmmw.png)","d0c19d1b":"\n<p style=\"font-size:20px;color:#1C37A6;font-weight:600;\">VGG16: \n<\/p>\n","46b92583":"<p style=\"font-size:15px;color:#1C37A6;font-weight:600;\">First, we convert the images to greyscale and then apply Gaussian blur to them.<\/p>","36704876":"<p style=\"font-size:20px;color:#1C37A6;font-weight:600;\">Let's apply the concept of Transfer Learning.<\/p>","011d6fcd":"![](https:\/\/i.pinimg.com\/originals\/7c\/3b\/63\/7c3b63598dc8b65b93a9532d4228947b.gif)","1086f873":"<p style=\"font-size:15px;color:#1C37A6;font-weight:600;\">The dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (Pneumonia\/Normal). There are 5,863 X-Ray images (JPEG) and 2 categories (Pneumonia\/Normal).<br>\nChest X-ray images (anterior-posterior) were selected from retrospective cohorts of pediatric patients of one to five years old from Guangzhou Women and Children\u2019s Medical Center, Guangzhou. All chest X-ray imaging was performed as part of patients\u2019 routine clinical care.<br>\nFor the analysis of chest x-ray images, all chest radiographs were initially screened for quality control by removing all low quality or unreadable scans. The diagnoses for the images were then graded by two expert physicians before being cleared for training the AI system. In order to account for any grading errors, the evaluation set was also checked by a third expert.<\/p>","6cb323e3":"<p style=\"font-size:15px;color:#1C37A6;font-weight:600;\">Now it\u2019s time to actually build the neural network architecture. Let\u2019s start with the input layer (input1). So this layer basically takes all the image samples in our X data. Hence we need to ensure that the first layer accepts the exact same shape as the image size. <br>\nThen, this input1 layer is connected to several convolution-pooling layer pairs before eventually being flattened and connected to dense layers. Notice that all hidden layers in the model are using the ReLU activation function due to the fact that ReLU is faster to compute compared to sigmoid, and thus, the training time required is shorter.\n<\/p>\n","79b27acf":"<p style=\"font-size:15px;color:#1C37A6;font-weight:600;\">Image Erosion:<\/p>\n","557fb9d0":"<br>\n<h1 style = \"font-size:35px; font-family:cursive ; font-weight : bold; background-color:#FFBFBF;color : #C10000; text-align: center; border-radius: 10px 100px;\"> \ud83c\udfe5 Pneumonia Detection from scratch <\/h1>\n<br>\n","853bc8b5":"<p style=\"font-size:15px;color:#1C37A6;font-weight:600;\">First, we divide our data to create a training and validation set using the Keras Image DataGenerator.<\/p>\n","b48e117a":"<p style=\"font-size:15px;color:#1C37A6;font-weight:600;\">A Convolutional Neural Network (ConvNet\/CNN) is a Deep Learning algorithm that can take in an input image, assign importance (learnable weights and biases) to various aspects\/objects of the image, and be able to differentiate one from the other. The pre-processing required in a ConvNet is much lower as compared to other classification algorithms. While in primitive methods, filters are hand-engineered, with enough training, ConvNets can learn these filters\/characteristics.<\/p>","73086ee9":"<p style=\"font-size:20px;color:#00817E;font-weight:600;\">\u25b8 Stride:<\/p>\n<p style=\"font-size:15px;color:#1C37A6;font-weight:600;\">Stride is the number of pixels shifts over the input matrix. When the stride is 1, then we move the filters to 1 pixel at a time.<\/p>\n","34c3632a":"* https:\/\/keras.io\/\n* https:\/\/www.kaggle.com\/rajmehra03\/a-comprehensive-guide-to-transfer-learning\/data\n* https:\/\/towardsdatascience.com\/convolutional-neural-networks-explained-9cc5188c4939\n* https:\/\/www.youtube.com\/watch?v=iaSUYvmCekI\n* https:\/\/towardsdatascience.com\/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\n* https:\/\/www.kaggle.com\/ashagutlapalli\/computer-vision-101-with-opencv\n* https:\/\/towardsdatascience.com\/deep-learning-with-keras-cheat-sheet-2021-python-for-data-science-fba43636a9a1\n","0cca0f96":"# Introduction to Pneumonia Dataset:","26addb81":"# What is Pneumonia?","935a07b1":"<p style=\"font-size:15px;color:#1C37A6;font-weight:600;\">All these images might look like a bunch of green dots on a blue background, but that\u2019s not all. These images are basically magnitude spectrum which tells us where the majority of the growth is.<\/p>\n","ab94c8c4":"![](https:\/\/www.researchgate.net\/profile\/Max-Ferguson\/publication\/322512435\/figure\/fig3\/AS:697390994567179@1543282378794\/Fig-A1-The-standard-VGG-16-network-architecture-as-proposed-in-32-Note-that-only.png)","6e4422ec":"![](https:\/\/www.drugs.com\/health-guide\/images\/022dc126-fc5d-4e54-9a78-75f2c9ea4bb6.jpg)","74c7bc5d":"# Breaking down the CNN:","3bd4018d":"# \ud83d\ude9a Loading Data:","b3eb8b2c":"![](https:\/\/miro.medium.com\/max\/2126\/1*W2D564Gkad9lj3_6t9I2PA@2x.gif)","00969b08":"![](https:\/\/cezannec.github.io\/assets\/cnn_intro\/CNN_ex.png)","82fa4cd2":"<p style=\"font-size:25px;color:#188100;font-weight:Bold;\">\u25c7 Convolution Layer:<\/p>\n<p style=\"font-size:15px;color:#1C37A6;font-weight:600;\">The convolution layer is the core building block of the CNN. It carries the main portion of the network\u2019s computational load.<br>\nThis layer performs a dot product between two matrices, where one matrix is the set of learnable parameters otherwise known as a kernel, and the other matrix is the restricted portion of the receptive field. The kernel is spatially smaller than an image but is more in-depth. This means that, if the image is composed of three (RGB) channels, the kernel height and width will be spatially small, but the depth extends up to all three channels.<br>\nAfter multiplication, the result obtained is called a Feature map, as shown below.<\/p>\n","ffbc09eb":"![](https:\/\/miro.medium.com\/max\/1920\/1*Ww3AMxZeoiB84GVSRBr4Bw.png)","27f05671":"# \u2714 Importing Libraries:","9e9ddb37":"<p style=\"font-size:15px;color:#1C37A6;font-weight:600;\">Dilation of Images<\/p>\n","a2bdb96b":"<p style=\"font-size:20px;color:#00817E;font-weight:600;\">\u25b8 Padding:<\/p>\n<p style=\"font-size:15px;color:#1C37A6;font-weight:600;\">Sometimes, the filter does not fit perfectly fit the input image. So to avoid information loss, we have two options:<br>\n1) Pad the picture with zeros (zero-padding) so that it fits.<br>\n2) Drop the part of the image where the filter did not fit. This is called valid padding, which keeps only a valid part of the image<\/p>\n","ccc6579b":"<p style=\"font-size:20px;color:#1C37A6;font-weight:600;\">Let's apply what we have learned so far.<\/p>","86718462":"\n<p style=\"font-size:15px;color:#1C37A6;font-weight:600;\">Now, let\u2019s visualize the performance of our new model:\n<\/p>\n","fabae672":"# Concept of Transfer Learning:","5dfe5ad5":"<p style=\"font-size:15px;color:#1C37A6;font-weight:600;\">Now let\u2019s use OpenCV\u2019s Canny Edge Detection:<\/p>\n","c6855536":"# \ud83d\udc68\u200d\ud83d\udcbb Model Building:","a716d025":"# What is CNN?","a32548d9":"![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/1\/19\/2D_Convolution_Animation.gif)","b4c3455c":"<p style=\"font-size:25px;color:#188100;font-weight:Bold;\">\u25c7 Fully Connected Layer:<\/p>\n<p style=\"font-size:15px;color:#1C37A6;font-weight:600;\">The layer we call as FC layer, we flattened our matrix into the vector and feed it into a fully connected layer like a neural network.\n<\/p>\n","96e3412e":"![](https:\/\/developers.google.com\/machine-learning\/practica\/image-classification\/images\/maxpool_animation.gif)","c8a32d5c":"<p style=\"font-size:25px;color:#188100;font-weight:Bold;\">\u25c7 Pooling Layer:<\/p>\n<p style=\"font-size:15px;color:#1C37A6;font-weight:600;\">The Pooling layer replaces the output of the network at certain locations by deriving a summary statistic of the nearby outputs. This helps in reducing the spatial size of the representation, which decreases the required amount of computation and weights. The pooling operation is processed on every slice of the representation individually.<br>\nThere are several pooling functions such as the average of the rectangular neighborhood, L2 norm of the rectangular neighborhood, and a weighted average based on the distance from the central pixel. However, the most popular process is max pooling, which reports the maximum output from the neighborhood.<\/p>\n","af754a97":"# Some Resources:"}}