{"cell_type":{"7ea053a4":"code","44795077":"code","9b941bec":"code","3153a665":"code","a26eb106":"code","4eec8825":"code","892af1ee":"code","f66de0bc":"code","6062f766":"code","0c297db1":"code","f47a8abe":"code","49bc10ca":"code","9a0a8187":"code","d29b4b56":"code","85db35c3":"code","2416ecfe":"code","b0c6b46e":"code","e9872f88":"code","e31c747b":"code","88ff1a51":"code","1b0c06de":"code","aec85e53":"code","a8b0499c":"code","ce228bd3":"code","d97befef":"code","dd292612":"code","3355f1f4":"code","7c960faa":"code","5a1414d8":"code","e6833dea":"code","a16766da":"code","de8c991d":"code","f4c0911c":"code","f611f51e":"code","943cbe96":"code","98b0477a":"code","b3803aef":"code","88e0dc06":"code","b10058f7":"code","0fd7c75f":"code","15eecb37":"code","39fa34e7":"code","81d8be49":"code","4efd7ae3":"code","0f914438":"markdown","07bb30a6":"markdown","a485cdaf":"markdown","4c2a4ec1":"markdown","2bab3e93":"markdown","f357a69c":"markdown","cce9f803":"markdown","c12ef07b":"markdown","e29b48a9":"markdown","cb987a1f":"markdown"},"source":{"7ea053a4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","44795077":"df=pd.read_csv(\"..\/input\/feedback-prize-2021\/train.csv\")","9b941bec":"df.shape","3153a665":"df.info()","a26eb106":"df.head(2)","4eec8825":"# fetching all the labels\nlist(df['discourse_type'].unique())","892af1ee":"# checking count of all the labels\ndf['discourse_type'].value_counts()","f66de0bc":"def bar_plot(df, x, y, color, get_width, get_x, y_lim, rotation, title, xlabel, ylabel, annot_size, xticks_size, legend):\n    fig, ax = plt.subplots(figsize = (12,7), dpi = 80)\n\n#     cmap = cm.get_cmap(color, df.shape[0])   \n#     clrs = [matplotlib.colors.rgb2hex(cmap(i)) for i in range(cmap.N)]\n\n    ax=sns.barplot(x=df[x], y=df[y], palette=color)\n\n    total = sum(df[y])\n    for i in ax.patches:\n        ax.text(i.get_x()+get_x, i.get_height()+get_width, \\\n                str(round((i.get_height()\/total)*100, 2))+'%', fontsize=annot_size, weight='bold',\n                    color='black')\n\n    plt.title(title, size=20, weight='bold', color='#1b515e')\n    plt.xlabel(xlabel, fontsize=15, weight='bold', color='#1b515e')\n    plt.ylabel(ylabel, fontsize=15, weight='bold', color='#1b515e')\n    plt.xticks(rotation=rotation, fontsize=xticks_size)\n    plt.yticks(fontsize=12)\n    plt.ylim(y_lim)\n\n    for i in ['bottom', 'left']:\n        ax.spines[i].set_color('white')\n        ax.spines[i].set_linewidth(1.5)\n    \n    right_side = ax.spines[\"right\"]\n    right_side.set_visible(False)\n    top_side = ax.spines[\"top\"]\n    top_side.set_visible(False)\n\n    ax.set_axisbelow(True)\n    ax.grid(color='#b2d6c7', linewidth=1, axis='y', alpha=.3)\n#     MA = mpatches.Patch(color=clrs[0], label=legend)\n#     ax.legend(handles=[MA], prop={'size': 10.5}, loc='best', borderpad=1, \n#               labelcolor=clrs[0], edgecolor='white');\n    \n    plt.show()","6062f766":"q1_df = pd.DataFrame(df['discourse_type'].value_counts()).reset_index().rename(columns={'index':'discourse_type', 'discourse_type':'Users'})","0c297db1":"q1_df","f47a8abe":"bar_plot(q1_df, 'discourse_type', 'Users', 'rainbow', 100, -.05, (0, 60000), 0,\n        '\\n Different discourse_type\\n', '\\n discourse_type\\n', '\\n Users\\n',\n        10, 13, 'discourse_type with Maximum\\n no. of Users')","49bc10ca":"df.columns","9a0a8187":"df['len_word'] = df['discourse_text'].apply(lambda x: len(x.split()))","d29b4b56":"# Category wise Dataframes\ndf_lead = df.loc[df[\"discourse_type\"]=='Lead']\ndf_claim = df.loc[df[\"discourse_type\"]=='Claim']\ndf_evidence = df.loc[df[\"discourse_type\"]=='Evidence']\ndf_position = df.loc[df[\"discourse_type\"]=='Position']\ndf_concluding = df.loc[df[\"discourse_type\"]=='Concluding Statement']\ndf_rebuttal = df.loc[df[\"discourse_type\"]=='Rebuttal']\ndf_counterclaim = df.loc[df[\"discourse_type\"]=='Counterclaim']","85db35c3":"# Lead\nplt.figure(figsize=(5,5))\nprint(\"mean of word lengths: \",df_lead.len_word.mean())\nsns.distplot(df_lead['len_word'],  color=\"b\")\nplt.show()","2416ecfe":"# Claim\nplt.figure(figsize=(5,5))\nprint(\"mean of word lengths: \",df_claim.len_word.mean())\nsns.distplot(df_claim['len_word'],  color=\"b\")\nplt.show()","b0c6b46e":"# Evidence\nplt.figure(figsize=(5,5))\nprint(\"mean of word lengths: \",df_evidence.len_word.mean())\nsns.distplot(df_evidence['len_word'],  color=\"b\")\nplt.show()","e9872f88":"# Position\nplt.figure(figsize=(5,5))\nprint(\"mean of word lengths: \",df_position.len_word.mean())\nsns.distplot(df_position['len_word'],  color=\"b\")\nplt.show()","e31c747b":"# Concluding Statement\nplt.figure(figsize=(5,5))\nprint(\"mean of word lengths: \",df_concluding.len_word.mean())\nsns.distplot(df_concluding['len_word'],  color=\"b\")\nplt.show()","88ff1a51":"# Rebuttal\nplt.figure(figsize=(5,5))\nprint(\"mean of word lengths: \",df_rebuttal.len_word.mean())\nsns.distplot(df_rebuttal['len_word'],  color=\"b\")\nplt.show()","1b0c06de":"# Counterclaim\nprint(\"mean of word lengths: \",df_counterclaim.len_word.mean())\nplt.figure(figsize=(5,5))\nsns.distplot(df_counterclaim['len_word'],  color=\"b\")\nplt.show()","aec85e53":"def horizontal_bar_plot(df, x, y, color, get_width, get_y, x_lim, x_spine, plot_size, title, xlabel, ylabel, annot_size, legend, legend_loc):\n    fig, ax = plt.subplots(figsize = plot_size, dpi = 80)\n\n#     cmap = cm.get_cmap(color, df.shape[0])   \n#     clrs = [matplotlib.colors.rgb2hex(cmap(i)) for i in range(cmap.N)]\n\n    ax=sns.barplot(x=df[x], y=df[y], palette=color)\n\n    total = sum(df[x])\n    for p in ax.patches:\n        plt.text(p.get_width()+get_width, p.get_y()+get_y,\n                '{:.2f}%'.format(p.get_width()*100\/total),ha='center', va='center', fontsize=annot_size, color='black', weight='bold')\n\n    plt.title(title, size=20, weight='bold', color='#1b515e')\n    plt.xlabel(xlabel, fontsize=15, weight='bold', color='#1b515e')\n    plt.ylabel(ylabel, fontsize=15, weight='bold', color='#1b515e')\n    plt.xticks(fontsize=13)\n    plt.yticks(fontsize=12)\n    plt.xlim(x_lim)\n\n    for i in ['top', 'left', 'right']:\n        side = ax.spines[i]\n        side.set_visible(False)\n\n    ax.set_axisbelow(True)\n    ax.spines['bottom'].set_bounds(x_spine)\n    ax.grid(color='#b2d6c7', linewidth=1, axis='y', alpha=.3)\n\n#     MA = mpatches.Patch(color=clrs[0], label=legend)\n#     ax.legend(handles=[MA], prop={'size': 10.5}, loc=legend_loc, borderpad=1, \n#               labelcolor=[clrs[0]], edgecolor='white');","a8b0499c":"q5_df = pd.DataFrame(df['discourse_type_num'].value_counts()).reset_index().rename(columns={'index':'discourse_type_num', 'discourse_type_num':'Users'})[:15]\n\nhorizontal_bar_plot(q5_df, 'Users', 'discourse_type_num', 'winter', 10, 0.8, (0, 15000), \n                    (0, 15000), (10, 8), '\\n Different discourse_type_num Users\\n', \n                    '\\nUsers', 'discourse_type_num', 15, 'Role with maximum Records', 'best')","ce228bd3":"ax = df.groupby('discourse_type')[['discourse_start','discourse_end']].mean().sort_values('discourse_start').plot(kind='barh', figsize=(10, 5),)\nax.set_title('Average Discourse Label Start and End', fontsize=16)\nplt.show()","d97befef":"df['full_text'] = df['discourse_text'].groupby(df['id']).transform(lambda x: ' '.join(x)) # obviously we will have duplicates","dd292612":"df.full_text.iloc[0]","3355f1f4":"text_length = df['full_text'].drop_duplicates().apply(len)","7c960faa":"fig = plt.figure(figsize=(10,8))\nax1 = text_length.plot(kind='hist', color = \"#120f7a\", bins=100)\nax1.set_title('Essay Length Distribution')\nax1.set_xlabel(\"Essay Length\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","5a1414d8":"word_count = df['full_text'].drop_duplicates().apply(lambda x: len(str(x).split()))\nfig = plt.figure(figsize=(10,8))\n\nax1 = word_count.plot(kind='hist', color = \"#120f7a\", bins=100)\nax1.set_title('Word Count Distribution')\nax1.set_xlabel(\"Word Count\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","e6833dea":"from nltk.corpus import stopwords\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.feature_extraction.text import CountVectorizer","a16766da":"def get_top_n_words(corpus, n=None, remove_stop_words=False, n_words=1): # if n_words=1 -> unigrams, if n_words=2 -> bigrams..\n    if remove_stop_words:\n        vec = CountVectorizer(stop_words = 'english', ngram_range=(n_words, n_words)).fit(corpus)\n    else:\n        vec = CountVectorizer(ngram_range=(n_words, n_words)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","de8c991d":"common_words = get_top_n_words(df['full_text'].drop_duplicates(), 20, remove_stop_words=True, n_words=1)\nfor word, freq in common_words:\n    print(word, freq)","f4c0911c":"df_tmp = pd.DataFrame(common_words, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = df_tmp.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Unigram Distribution')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","f611f51e":"common_words = get_top_n_words(df['full_text'].drop_duplicates(), 20, remove_stop_words=True, n_words=2)\nfor word, freq in common_words:\n    print(word, freq)","943cbe96":"df_tmp = pd.DataFrame(common_words, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = df_tmp.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Bigram Distribution')\nax1.set_xlabel(\"Bigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","98b0477a":"common_words = get_top_n_words(df['full_text'].drop_duplicates(), 20, remove_stop_words=True, n_words=3)\nfor word, freq in common_words:\n    print(word, freq)","b3803aef":"df_tmp = pd.DataFrame(common_words, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = df_tmp.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Trigram Distribution')\nax1.set_xlabel(\"Trigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","88e0dc06":"text_Lead = df[df.discourse_type == 'Lead'].discourse_text.values\ntext_Position = df[df.discourse_type == 'Position'].discourse_text.values\ntext_Evidence = df[df.discourse_type == 'Evidence'].discourse_text.values\ntext_Claim = df[df.discourse_type == 'Claim'].discourse_text.values\ntext_Concluding_Statement = df[df.discourse_type == 'Concluding Statement'].discourse_text.values\ntext_Counterclaim = df[df.discourse_type == 'Counterclaim'].discourse_text.values\ntext_Rebuttal = df[df.discourse_type == 'Rebuttal'].discourse_text.values\n\ncommon_words_Lead = get_top_n_words(text_Lead, 20, remove_stop_words=True, n_words=1)\ncommon_words_Position = get_top_n_words(text_Position, 20, remove_stop_words=True, n_words=1)\ncommon_words_Evidence = get_top_n_words(text_Evidence, 20, remove_stop_words=True, n_words=1)\ncommon_words_Claim = get_top_n_words(text_Claim, 20, remove_stop_words=True, n_words=1)\ncommon_words_Concluding_Statement = get_top_n_words(text_Concluding_Statement, 20, remove_stop_words=True, n_words=1)\ncommon_words_Counterclaim = get_top_n_words(text_Counterclaim, 20, remove_stop_words=True, n_words=1)\ncommon_words_Rebuttal = get_top_n_words(text_Rebuttal, 20, remove_stop_words=True, n_words=1)","b10058f7":"df_tmp_Lead = pd.DataFrame(common_words_Lead, columns = ['text' , 'count'])\ndf_tmp_Position = pd.DataFrame(common_words_Position, columns = ['text' , 'count'])\ndf_tmp_Evidence = pd.DataFrame(common_words_Evidence, columns = ['text' , 'count'])\ndf_tmp_Claim = pd.DataFrame(common_words_Claim, columns = ['text' , 'count'])\ndf_tmp_Concluding_Statement = pd.DataFrame(common_words_Concluding_Statement, columns = ['text' , 'count'])\ndf_tmp_Counterclaim = pd.DataFrame(common_words_Counterclaim, columns = ['text' , 'count'])\ndf_tmp_Rebuttal = pd.DataFrame(common_words_Rebuttal, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_Lead.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Lead Unigram Distribution')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_Position.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n   kind='bar', color = \"#120f7a\")\nax2.set_title('Position Unigram Distribution')\nax2.set_xlabel(\"Unigrams\")\nax2.set_ylabel(\"Frequency\")\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_Evidence.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Evidence Unigram Distribution')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_Claim.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n   kind='bar', color = \"#120f7a\")\nax2.set_title('Claim Unigram Distribution')\nax2.set_xlabel(\"Unigrams\")\nax2.set_ylabel(\"Frequency\")\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_Concluding_Statement.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Concluding Statement Unigram Distribution')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_Counterclaim.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n   kind='bar', color = \"#120f7a\")\nax2.set_title('Counterclaim Unigram Distribution')\nax2.set_xlabel(\"Unigrams\")\nax2.set_ylabel(\"Frequency\")\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_Rebuttal.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Rebuttal Unigram Distribution')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","0fd7c75f":"text_Lead = df[df.discourse_type == 'Lead'].discourse_text.values\ntext_Position = df[df.discourse_type == 'Position'].discourse_text.values\ntext_Evidence = df[df.discourse_type == 'Evidence'].discourse_text.values\ntext_Claim = df[df.discourse_type == 'Claim'].discourse_text.values\ntext_Concluding_Statement = df[df.discourse_type == 'Concluding Statement'].discourse_text.values\ntext_Counterclaim = df[df.discourse_type == 'Counterclaim'].discourse_text.values\ntext_Rebuttal = df[df.discourse_type == 'Rebuttal'].discourse_text.values\n\ncommon_words_Lead = get_top_n_words(text_Lead, 20, remove_stop_words=True, n_words=2)\ncommon_words_Position = get_top_n_words(text_Position, 20, remove_stop_words=True, n_words=2)\ncommon_words_Evidence = get_top_n_words(text_Evidence, 20, remove_stop_words=True, n_words=2)\ncommon_words_Claim = get_top_n_words(text_Claim, 20, remove_stop_words=True, n_words=2)\ncommon_words_Concluding_Statement = get_top_n_words(text_Concluding_Statement, 20, remove_stop_words=True, n_words=2)\ncommon_words_Counterclaim = get_top_n_words(text_Counterclaim, 20, remove_stop_words=True, n_words=2)\ncommon_words_Rebuttal = get_top_n_words(text_Rebuttal, 20, remove_stop_words=True, n_words=2)","15eecb37":"df_tmp_Lead = pd.DataFrame(common_words_Lead, columns = ['text' , 'count'])\ndf_tmp_Position = pd.DataFrame(common_words_Position, columns = ['text' , 'count'])\ndf_tmp_Evidence = pd.DataFrame(common_words_Evidence, columns = ['text' , 'count'])\ndf_tmp_Claim = pd.DataFrame(common_words_Claim, columns = ['text' , 'count'])\ndf_tmp_Concluding_Statement = pd.DataFrame(common_words_Concluding_Statement, columns = ['text' , 'count'])\ndf_tmp_Counterclaim = pd.DataFrame(common_words_Counterclaim, columns = ['text' , 'count'])\ndf_tmp_Rebuttal = pd.DataFrame(common_words_Rebuttal, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_Lead.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Lead Bigram Distribution')\nax1.set_xlabel(\"Bigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_Position.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n   kind='bar', color = \"#120f7a\")\nax2.set_title('Position Bigram Distribution')\nax2.set_xlabel(\"Bigrams\")\nax2.set_ylabel(\"Frequency\")\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_Evidence.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Evidence Bigram Distribution')\nax1.set_xlabel(\"Bigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_Claim.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n   kind='bar', color = \"#120f7a\")\nax2.set_title('Claim Bigram Distribution')\nax2.set_xlabel(\"Bigrams\")\nax2.set_ylabel(\"Frequency\")\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_Concluding_Statement.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Concluding Statement Bigram Distribution')\nax1.set_xlabel(\"Bigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_Counterclaim.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n   kind='bar', color = \"#120f7a\")\nax2.set_title('Counterclaim Bigram Distribution')\nax2.set_xlabel(\"Bigrams\")\nax2.set_ylabel(\"Frequency\")\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_Rebuttal.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Rebuttal Bigram Distribution')\nax1.set_xlabel(\"Bigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","39fa34e7":"text_Lead = df[df.discourse_type == 'Lead'].discourse_text.values\ntext_Position = df[df.discourse_type == 'Position'].discourse_text.values\ntext_Evidence = df[df.discourse_type == 'Evidence'].discourse_text.values\ntext_Claim = df[df.discourse_type == 'Claim'].discourse_text.values\ntext_Concluding_Statement = df[df.discourse_type == 'Concluding Statement'].discourse_text.values\ntext_Counterclaim = df[df.discourse_type == 'Counterclaim'].discourse_text.values\ntext_Rebuttal = df[df.discourse_type == 'Rebuttal'].discourse_text.values\n\ncommon_words_Lead = get_top_n_words(text_Lead, 20, remove_stop_words=True, n_words=3)\ncommon_words_Position = get_top_n_words(text_Position, 20, remove_stop_words=True, n_words=3)\ncommon_words_Evidence = get_top_n_words(text_Evidence, 20, remove_stop_words=True, n_words=3)\ncommon_words_Claim = get_top_n_words(text_Claim, 20, remove_stop_words=True, n_words=3)\ncommon_words_Concluding_Statement = get_top_n_words(text_Concluding_Statement, 20, remove_stop_words=True, n_words=3)\ncommon_words_Counterclaim = get_top_n_words(text_Counterclaim, 20, remove_stop_words=True, n_words=3)\ncommon_words_Rebuttal = get_top_n_words(text_Rebuttal, 20, remove_stop_words=True, n_words=3)\n","81d8be49":"df_tmp_Lead = pd.DataFrame(common_words_Lead, columns = ['text' , 'count'])\ndf_tmp_Position = pd.DataFrame(common_words_Position, columns = ['text' , 'count'])\ndf_tmp_Evidence = pd.DataFrame(common_words_Evidence, columns = ['text' , 'count'])\ndf_tmp_Claim = pd.DataFrame(common_words_Claim, columns = ['text' , 'count'])\ndf_tmp_Concluding_Statement = pd.DataFrame(common_words_Concluding_Statement, columns = ['text' , 'count'])\ndf_tmp_Counterclaim = pd.DataFrame(common_words_Counterclaim, columns = ['text' , 'count'])\ndf_tmp_Rebuttal = pd.DataFrame(common_words_Rebuttal, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_Lead.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Lead Trigram Distribution')\nax1.set_xlabel(\"Trigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_Position.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n   kind='bar', color = \"#120f7a\")\nax2.set_title('Position Trigram Distribution')\nax2.set_xlabel(\"Trigrams\")\nax2.set_ylabel(\"Frequency\")\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_Evidence.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Evidence Trigram Distribution')\nax1.set_xlabel(\"Trigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_Claim.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n   kind='bar', color = \"#120f7a\")\nax2.set_title('Claim Trigram Distribution')\nax2.set_xlabel(\"Trigrams\")\nax2.set_ylabel(\"Frequency\")\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_Concluding_Statement.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Concluding Statement Trigram Distribution')\nax1.set_xlabel(\"Trigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_Counterclaim.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n   kind='bar', color = \"#120f7a\")\nax2.set_title('Counterclaim Trigram Distribution')\nax2.set_xlabel(\"Trigrams\")\nax2.set_ylabel(\"Frequency\")\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_Rebuttal.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Rebuttal Trigram Distribution')\nax1.set_xlabel(\"Trigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","4efd7ae3":"del df_tmp_Lead, df_tmp_Position, df_tmp_Evidence, df_tmp_Claim, df_tmp_Concluding_Statement, df_tmp_Counterclaim, df_tmp_Rebuttal, common_words_Lead, common_words_Position, common_words_Evidence, common_words_Claim, common_words_Concluding_Statement, common_words_Counterclaim,common_words_Rebuttal ","0f914438":"# Word Count","07bb30a6":"# trigram","a485cdaf":"# Trigram","4c2a4ec1":"# Bigram","2bab3e93":"# Text length","f357a69c":"# Distribution of top n-grams for each discourse type","cce9f803":"# Bigram","c12ef07b":"# unigram","e29b48a9":"# unigram","cb987a1f":"# Distribution of top n-grams for full-text essays"}}