{"cell_type":{"8a19d302":"code","cc956322":"code","67fa5cdf":"code","78aff62d":"code","0be55c67":"code","331ac605":"code","44311f78":"code","0a56fad4":"code","cd14e3fa":"code","eec9e7af":"code","78693830":"code","6bf13c32":"code","e282f598":"code","0f5f8393":"code","e2a39e91":"code","d443635b":"code","ec8b10e1":"code","c344986f":"code","321e6eb4":"code","d41e29e3":"code","efabaa46":"code","4a8fc7fc":"code","6bcfe67a":"markdown","81c55b5a":"markdown","28c0f304":"markdown","4c74e2a7":"markdown","a02c6be6":"markdown","057d9d48":"markdown","7881edcc":"markdown","c81d6c0e":"markdown","3aeba0ce":"markdown","25d6094f":"markdown","10a44156":"markdown","17891dab":"markdown","79ec4c82":"markdown","c0a0f51a":"markdown","0ca99a17":"markdown","ef9a58f0":"markdown","56d90919":"markdown","07160beb":"markdown","a0c2fc5b":"markdown","29b39635":"markdown","f0b2355a":"markdown"},"source":{"8a19d302":"# Import the pandas package, then use the \"read_csv\" function to get the labeled training data\nimport pandas as pd       \ntraining = pd.read_csv(\"..\/input\/word2vec-nlp-tutorial\/labeledTrainData.tsv.zip\", header=0, \\\n                       delimiter=\"\\t\", quoting=3)\nprint(training.shape)\n\n# Get target variable\ny_train = training['sentiment']\nx_train = training.drop([\"sentiment\"], axis=1)\n                      \ntraining.head()\n","cc956322":"import seaborn as sns\nsns.countplot(y_train)","67fa5cdf":"import matplotlib.pyplot as plt \nimport numpy as np\n# Get mean of positive and negative reviews\navg_pos_reviews = training[training.sentiment==1].review.apply(lambda x: len(x.split())).mean()\navg_neg_reviews = training[training.sentiment==0].review.apply(lambda x: len(x.split())).mean()\n\nplt.figure(figsize=(10, 3))\nplt.barh(['Positive', 'Negative'], [avg_pos_reviews, avg_neg_reviews], height=0.5)\nplt.xticks(np.arange(0, 300, 25))\nplt.xlabel('Average Number of words')\nplt.ylabel('Sentiment')\nplt.show()","78aff62d":"import nltk\nimport numpy as np\nimport matplotlib.pyplot as plt","0be55c67":"# Import list of stopwords from library NLTK\nfrom nltk.corpus import stopwords\n\nstopwords_list = set(stopwords.words(\"english\"))\nprint(f'List of stopwords:\\n{stopwords_list}\\n')\n\n# We remove negation words in list of stopwords\nno_stopwords = [\"not\",\"don't\",'aren','don','ain',\"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\",\n               'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\",\n               \"won't\", 'wouldn', \"wouldn't\"]\nfor no_stopword in no_stopwords:\n    stopwords_list.remove(no_stopword)\n    \n#print(stopwords_list)\nprint(f'Final list of stopwords:\\n{stopwords_list}')","331ac605":"# Import Lemmatizer from NLTK\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\n# function that receive a list of words and do lemmatization:\ndef lemma_stem_text(words_list):\n    # Lemmatizer\n    text = [lemmatizer.lemmatize(token.lower()) for token in words_list]# eighties->eight or messages->message or drugs->drug\n    text = [lemmatizer.lemmatize(token.lower(), \"v\") for token in text]# going-> go or started->start or watching->watch\n    return text\n\nword_example = \"feet\"\nprint(f'The word \"{word_example}\" is transformed to \"{lemma_stem_text([word_example])[0]}\"')","44311f78":"import re\nre_negation = re.compile(\"n't \")\n\n# function that receive a sequence of woords and return the same sequence transforming\n# abbreviated negations to the standard form.\ndef negation_abbreviated_to_standard(sent):\n    sent = re_negation.sub(\" not \", sent)\n    return sent\n\nword_example = \"I aren't \"\nprint(f'The sentence \"{word_example}\" is transformed to \"{negation_abbreviated_to_standard(word_example)}\"')","0a56fad4":"# Import function BeautifulSoup to clean text of HTML tags\nfrom bs4 import BeautifulSoup \n\ndef review_to_words(raw_review):\n    # 1. Remove HTML tags\n    review_text = BeautifulSoup(raw_review).get_text() \n    \n    # 2. Transform abbreviated negations to the standard form.\n    review_text = negation_abbreviated_to_standard(review_text)\n    \n    # 3. Remove non-letters and non-numbers   \n    letters_numbers_only = re.sub(\"[^a-zA-Z_0-9]\", \" \", review_text) \n    \n    # 4. Convert to lower case and split into individual words (tokenization)\n    words = np.char.lower(letters_numbers_only.split())                             \n    \n    # 5. Remove stop words\n    meaningful_words = [w for w in words if not w in stopwords_list]   \n    \n    # 6. Apply lemmatization function\n    lemma_words = lemma_stem_text(meaningful_words)\n    \n    # 7. Join the words back into one string separated by space, and return the result.\n    return( \" \".join(lemma_words))   ","cd14e3fa":"# Clean first review\nclean_review = review_to_words(x_train[\"review\"][0] )\n\n# Print original review, sentiment and cleaned review\nprint(f'Text of original review:\\n{x_train[\"review\"][0]}\\n')\nprint(f'Sentiment review: {y_train[0]}\\n')\nprint(f'Text of cleaned review:\\n{clean_review}') ","eec9e7af":"# We get the text of reviews in the training set\nreviews = x_train['review']\n\n# We initialize an empty list to add the clean reviews\ncleaned_train_reviews = []\n\n# We loop over each review and clean it  \nfor i in reviews:\n    cleaned_train_reviews.append(review_to_words(i))","78693830":"# Import tf-idf encoding from sklearn library\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Define some hiperparameters of encoded\nvectorizer = TfidfVectorizer(max_features=20000, ngram_range = (1,2))\n\n# Create the training set with the words encoded as features of the reviews\ntrain_data_features = vectorizer.fit_transform(cleaned_train_reviews)\n\nprint(train_data_features.shape)","6bf13c32":"# Import the logistic regression model from sklearn \nfrom sklearn.linear_model import LogisticRegression\n\n# Define the model\nmodel = LogisticRegression(random_state=0, solver='lbfgs',\n                            multi_class='multinomial')\n# Train model\nmodel.fit(train_data_features, y_train)\n","e282f598":"# Read the test data\ntest = pd.read_csv(\"..\/input\/word2vec-nlp-tutorial\/testData.tsv.zip\", header=0, delimiter=\"\\t\", \\\n                   quoting=3 )\nprint(test.shape)\n\n# Create an empty list and append the clean reviews one by one\nnum_reviews = len(test[\"review\"])\nclean_test_reviews = [] \n\n# Clean the text of all reviews in the training set\nprint(\"Cleaning and parsing the test set movie reviews...\\n\")\nfor i in range(0,num_reviews):\n    clean_review = review_to_words( test[\"review\"][i] )\n    clean_test_reviews.append( clean_review )\n\n# Create the test set with the words encoded as features of the reviews\ntest_data_features = vectorizer.transform(clean_test_reviews)\n\n\n# Use the logistic regression model to make sentiment label predictions\nresult = model.predict(test_data_features)\n\n# Copy the results to a pandas dataframe with an \"id\" column and a \"sentiment\" column\noutput = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )","0f5f8393":"# Use pandas to save the dataframe\noutput.to_csv(\"submission.csv\", index=False, quoting=3 )","e2a39e91":"# We split train dataset\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_data_features, y_train, test_size=0.2, random_state=42)","d443635b":"# We train two models: random forest and logistic regression\nfrom sklearn.ensemble import RandomForestClassifier\n# Initialize a Random Forest classifier with 500 trees\nforest = RandomForestClassifier(n_estimators = 500, max_depth = None, min_samples_split=2, min_samples_leaf =1,\n                                bootstrap = True, random_state=0)\n# Train the model\nforest = forest.fit(X_train, y_train)\n# Print score of model(using test dataset)\nprint(forest.score(X_test, y_test))","ec8b10e1":"# Initialize a logistic regression model \nlogistic = LogisticRegression(random_state=0, solver='lbfgs',\n                            multi_class='multinomial')\n# Train the model\nlogistic = logistic.fit(X_train, y_train)\n# Print score of model(using test dataset)\nprint(logistic.score(X_test, y_test))","c344986f":"y_pred_forest  = forest.predict(X_test)\ny_pred_logistic  = logistic.predict(X_test)","321e6eb4":"from sklearn.metrics import confusion_matrix\nconfusion_matrix_forest = confusion_matrix(y_test, y_pred_forest, labels=[1,0])\nconfusion_matrix_forest","d41e29e3":"import seaborn as sns\n# plot the confusion matrix\nax = plt.axes()\nsns.heatmap(confusion_matrix_forest, annot=True, fmt=\"d\")\nax.set_title('Confusion matrix Random Forest')","efabaa46":"confusion_matrix_logistic = confusion_matrix(y_test, y_pred_logistic, labels=[1,0])\nconfusion_matrix_logistic\n","4a8fc7fc":"# plot the confusion matrix\nax = plt.axes()\nsns.heatmap(confusion_matrix_logistic, annot=True, fmt=\"d\")\nax.set_title('Confusion matrix Logistic Regression')\n","6bcfe67a":"We check if the training dataset is balanced.","81c55b5a":"#### Negations","28c0f304":"Finally, we build a dataframe to submission.","4c74e2a7":"We see how is cleaned the first review in the training set.","a02c6be6":"## Model","057d9d48":"## Data exploration","7881edcc":"We create a function to change negation abbreviate forms to standard using a regular expression.","c81d6c0e":"## Vectorization","3aeba0ce":"Let's see the number of words averaged for positive and negative reviews.","25d6094f":"#### Stopwords","10a44156":"Create training and validation datasets. We train both logistic regression and random forest classifiers and evaluate them in the validation dataset. ","17891dab":"We create a function to clean the text of a review using the functions defined previously.","79ec4c82":"#### Lemmatize reviews","c0a0f51a":"## Data processing","0ca99a17":"## Load dataset","ef9a58f0":"We load the stopwords list and remove from the list those we do not want to delete from the text. We do not remove negation words because reviews with a lot of these can indicate that the review has a negative sentiment.","56d90919":"We predict the sentiment of the reviews in the test dataset.","07160beb":"## Appendix","a0c2fc5b":"## Predictions in Test dataset","29b39635":"In this notebook, we train a text classifier to detect the sentiment of movie reviews.","f0b2355a":"We clean the text of all reviews in the training set."}}