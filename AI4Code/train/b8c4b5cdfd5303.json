{"cell_type":{"2ab86e32":"code","e50db9b1":"code","3f6aa2d7":"code","e123ac19":"code","d273258a":"code","e31f0279":"code","78530c26":"code","e6e7a8ab":"code","26b0a650":"code","99774118":"code","e0e9f9ba":"code","d437e82b":"code","2270c2c3":"code","4992fc9e":"markdown","639344e4":"markdown","66eee1fe":"markdown","e3ea95b4":"markdown","9c5661b4":"markdown","cae51fd2":"markdown","e13fc72f":"markdown","7a626895":"markdown","eeeaf273":"markdown","570456c1":"markdown","0f8d19f6":"markdown"},"source":{"2ab86e32":"import numpy as np\n\n# This is our initial data; one entry per \"sample\"\n# (in this toy example, a \"sample\" is just a sentence, but\n# it could be an entire document).\nsamples = ['The cat sat on the mat.', 'The dog ate my homework.']\n\n# First, build an index of all tokens in the data.\ntoken_index = {}\nfor sample in samples:\n    # We simply tokenize the samples via the `split` method.\n    # in real life, we would also strip punctuation and special characters\n    # from the samples.\n    for word in sample.split():\n        if word not in token_index:\n            # Assign a unique index to each unique word\n            token_index[word] = len(token_index) + 1\n            # Note that we don't attribute index 0 to anything.\n\n# Next, we vectorize our samples.\n# We will only consider the first `max_length` words in each sample.\nmax_length = 10\n\n# This is where we store our results:\nresults = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\nfor i, sample in enumerate(samples):\n    for j, word in list(enumerate(sample.split()))[:max_length]:\n        index = token_index.get(word)\n        results[i, j, index] = 1.\n#token_index[\"cat\"]\n#results.shape\n#results[0,1]","e50db9b1":"from keras.preprocessing.text import Tokenizer\n\nsamples = ['The cat sat on the mat.', 'The dog ate my homework.']\n\n# We create a tokenizer, configured to only take\n# into account the top-1000 most common words\ntokenizer = Tokenizer(num_words=1000)\n# This builds the word index\ntokenizer.fit_on_texts(samples)\n\n# This turns strings into lists of integer indices.\nsequences = tokenizer.texts_to_sequences(samples)\n\n# You could also directly get the one-hot binary representations.\n# Note that other vectorization modes than one-hot encoding are supported!\none_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n\n# This is how you can recover the word index that was computed\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","3f6aa2d7":"from keras.layers import Embedding\n\n# The Embedding layer takes at least two arguments:\n# the number of possible tokens, here 1000 (1 + maximum word index),\n# and the dimensionality of the embeddings, here 64.\nembedding_layer = Embedding(1000, 64)","e123ac19":"\nfrom keras.datasets import imdb\nfrom keras import preprocessing\n\n# Number of words to consider as features\nmax_features = 10000\n# Cut texts after this number of words \n# (among top max_features most common words)\nmaxlen = 20\n\n# Load the data as lists of integers.\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n\n# This turns our lists of integers\n# into a 2D integer tensor of shape `(samples, maxlen)`\nx_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)","d273258a":"x_train.shape\ny_train.shape\nx_train[1]","e31f0279":"from keras.models import Sequential\nfrom keras.layers import Flatten, Dense\n\nmodel = Sequential()\n# We specify the maximum input length to our Embedding layer\n# so we can later flatten the embedded inputs\nmodel.add(Embedding(10000, 8, input_length=maxlen))\n# After the Embedding layer, \n# our activations have shape `(samples, maxlen, 8)`.\n\n# We flatten the 3D tensor of embeddings \n# into a 2D tensor of shape `(samples, maxlen * 8)`\nmodel.add(Flatten())\n\n# We add the classifier on top\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\nmodel.summary()\n\nhistory = model.fit(x_train, y_train,\n                    epochs=10,\n                    batch_size=32,\n                    validation_split=0.2)","78530c26":"from keras.models import Sequential\nfrom keras.layers import Embedding, SimpleRNN\n\nmodel = Sequential()\nmodel.add(Embedding(10000, 32))\nmodel.add(SimpleRNN(32))\nmodel.summary()","e6e7a8ab":"#We can do a little more deep learning here....\nmodel = Sequential()\nmodel.add(Embedding(10000, 32))\nmodel.add(SimpleRNN(32, return_sequences=True))\nmodel.add(SimpleRNN(32, return_sequences=True))\nmodel.add(SimpleRNN(32, return_sequences=True))\nmodel.add(SimpleRNN(32))  # This last layer only returns the last outputs.\nmodel.summary()","26b0a650":"from keras.datasets import imdb\nfrom keras.preprocessing import sequence\n\nmax_features = 10000  # number of words to consider as features\nmaxlen = 500  # cut texts after this number of words (among top max_features most common words)\nbatch_size = 32\n\nprint('Loading data...')\n(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\nprint(len(input_train), 'train sequences')\nprint(len(input_test), 'test sequences')\n\nprint('Pad sequences (samples x time)')\ninput_train = sequence.pad_sequences(input_train, maxlen=maxlen)\ninput_test = sequence.pad_sequences(input_test, maxlen=maxlen)\nprint('input_train shape:', input_train.shape)\nprint('input_test shape:', input_test.shape)","99774118":"#Let's train a simple recurrent network using an Embedding layer and a SimpleRNN layer:\nfrom keras.layers import Dense\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(SimpleRNN(32))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\nhistory = model.fit(input_train, y_train,\n                    epochs=5,\n                    batch_size=128,\n                    validation_split=0.2)","e0e9f9ba":"import matplotlib.pyplot as plt\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","d437e82b":"from keras.layers import LSTM\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(LSTM(32))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['acc'])\nhistory = model.fit(input_train, y_train,\n                    epochs=5,\n                    batch_size=128,\n                    validation_split=0.2)","2270c2c3":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","4992fc9e":"##Using kears for word-level one-hot encoding","639344e4":"##Our first simple RNN","66eee1fe":"Plots to learn how we did.....","e3ea95b4":"### *Optional*","9c5661b4":"## Word level one hot encoding","cae51fd2":"##Learning word embeddings","e13fc72f":"Now let's try to use such a model on the IMDB movie review classification problem. First, let's preprocess the data:","7a626895":" We only specify the output dimensionality of the LSTM layer, and leave every other argument (there are lots) to the Keras defaults. Keras has good defaults, and things will almost always \"just work\" without you having to spend time tuning parameters by hand.","eeeaf273":"##Let's do some LSTM's in Keras","570456c1":"#Examples from chapter 6\nhttps:\/\/github.com\/fchollet\/deep-learning-with-python-notebooks","0f8d19f6":"##Using pre-trained word embeddings"}}