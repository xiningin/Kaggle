{"cell_type":{"1323240b":"code","b9006b47":"code","fd857661":"code","893a74c6":"code","64ae0b8c":"code","014061b4":"code","6d906cbd":"markdown","3e36aa76":"markdown","8f953a7c":"markdown","3efeccd0":"markdown","8b8dd97d":"markdown","021e3573":"markdown","65f1445a":"markdown","110bdfe7":"markdown"},"source":{"1323240b":"UNLABELED_PATH = [\"\/kaggle\/input\/ecssd\/ECSSD\/Image\", \"\/kaggle\/input\/ecssd\/ECSSD\/Mask\"]\nLABELED_PATH = [\"\/kaggle\/input\/pascal-s\/Pascal-S\/Image\", \"\/kaggle\/input\/pascal-s\/Pascal-S\/Mask\"]","b9006b47":"!cd \/kaggle\/input\/pascal-s\/ && ls","fd857661":"import os\n\nimport torch.utils.data as data\nfrom PIL import Image\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nimport math\n\n\nclass JointResize(object):\n    def __init__(self, size):\n        if isinstance(size, int):\n            self.size = (size, size)\n        elif isinstance(size, tuple):\n            self.size = size\n        else:\n            raise RuntimeError(\"size\u53c2\u6570\u8bf7\u8bbe\u7f6e\u4e3aint\u6216\u8005tuple\")\n\n    def __call__(self, img, mask):\n        img = img.resize(self.size)\n        mask = mask.resize(self.size)\n        return img, mask\n\ndef make_dataset(root, prefix=('jpg', 'png')):\n    img_path = root[0]\n    gt_path = root[1]\n    img_list = [os.path.splitext(f)[0] for f in os.listdir(img_path) if f.endswith(prefix[0])]\n    return [(os.path.join(img_path, img_name + prefix[0]), os.path.join(gt_path, img_name + prefix[1])) for img_name in img_list]\n\n\n# \u4ec5\u9488\u5bf9\u8bad\u7ec3\u96c6\nclass ImageFolder(data.Dataset):\n    def __init__(self, root, mode, in_size, prefix, use_bigt=False, split_rate=(1, 3)):\n        \"\"\"split_rate = label:unlabel\"\"\"\n        assert isinstance(mode, str), 'isTrain\u53c2\u6570\u9519\u8bef\uff0c\u5e94\u8be5\u4e3abool\u7c7b\u578b'\n        self.root_labeled = root[0]\n        self.mode = mode\n        self.use_bigt = use_bigt\n        \n        self.imgs_labeled = make_dataset(self.root_labeled, prefix=prefix)\n        self.split_rate = split_rate\n        self.r_l_rate = split_rate[1] \/\/ split_rate[0]\n        len_labeled = len(self.imgs_labeled)\n\n        self.root_unlabeled = root[1]\n        self.imgs_unlabeled = make_dataset(self.root_unlabeled, prefix=prefix)\n        len_unlabeled = len(self.imgs_unlabeled)\n\n        len_unlabeled = self.r_l_rate * len_labeled\n        self.imgs_unlabeled = self.imgs_unlabeled * (self.r_l_rate + math.ceil(len_labeled \/ len_unlabeled))  # \u6269\u5c55\u65e0\u6807\u7b7e\u7684\u6570\u636e\u5217\u8868\n        self.imgs_unlabeled = self.imgs_unlabeled[0:len_unlabeled]\n\n        self.length = len_labeled + len_unlabeled\n        print(f\"\u4f7f\u7528\u6269\u5145\u6bd4\u4f8b\u4e3a\uff1a{len(self.imgs_labeled) \/ len(self.imgs_unlabeled)}\")\n\n        # \u4ec5\u662f\u4e3a\u4e86\u7b80\u5355\u800c\u4ec5\u4f7f\u7528\u4e00\u79cd\u53d8\u6362\n        self.train_joint_transform = JointResize(in_size)\n        self.train_img_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # \u5904\u7406\u7684\u662fTensor\n        ])\n        # ToTensor \u64cd\u4f5c\u4f1a\u5c06 PIL.Image \u6216\u5f62\u72b6\u4e3a H\u00d7W\u00d7D\uff0c\u6570\u503c\u8303\u56f4\u4e3a [0, 255] \u7684 np.ndarray \u8f6c\u6362\u4e3a\u5f62\u72b6\u4e3a D\u00d7H\u00d7W\uff0c\n        # \u6570\u503c\u8303\u56f4\u4e3a [0.0, 1.0] \u7684 torch.Tensor\u3002\n        self.train_gt_transform = transforms.ToTensor()\n\n    def __getitem__(self, index):\n        if index % (self.r_l_rate + 1) == 0:\n            labeled_index = index \/\/ (self.r_l_rate + 1)\n            img_path, gt_path = self.imgs_labeled[labeled_index]  # 0, 1 => 10550\n        else:\n            unlabeled_index = index \/\/ (self.r_l_rate + 1) + index % (self.r_l_rate + 1)\n            img_path, gt_path = self.imgs_unlabeled[unlabeled_index]  # 1, 2, 3\n\n        img = Image.open(img_path).convert('RGB')\n        img_name = (img_path.split(os.sep)[-1]).split('.')[0]\n\n        gt = Image.open(gt_path).convert('L')\n        img, gt = self.train_joint_transform(img, gt)\n        img = self.train_img_transform(img)\n        gt = self.train_gt_transform(gt)\n        if self.use_bigt:\n            gt = gt.ge(0.5).float()  # \u4e8c\u503c\u5316\n        return img, gt, img_name  # \u8f93\u51fa\u540d\u5b57\u65b9\u4fbf\u6bd4\u8f83\n\n    def __len__(self):\n        return self.length\n    \nprint(f\" ==>> \u4f7f\u7528\u7684\u8bad\u7ec3\u96c6 <<==\\n -->> LABELED_PATH\uff1a{LABELED_PATH}\\n -->> UNLABELED_PATH\uff1a{UNLABELED_PATH}\")\ntrain_set = ImageFolder((LABELED_PATH, UNLABELED_PATH), \"train\", 320, prefix=('.jpg', '.png'), use_bigt=True, split_rate=(12, 36))\n# \u7531\u4e8etrain_set\u5185\u90e8\u7684\u6bd4\u4f8b\u987a\u5e8f\u662f\u56fa\u5b9a\u7684\uff0c\u6240\u4ee5\u4e3a\u4e86\u4fdd\u6301\u6bd4\u4f8b\u5173\u7cfb\uff0c\u4e0d\u80fd\u518d\u4f7f\u7528`shuffle=True`\ntrain_loader = DataLoader(train_set, batch_size=48, num_workers=8, shuffle=False, drop_last=True, pin_memory=True)  \n\ndef split_data(data):\n    labeled_data = []\n    unlabeled_data = []\n    for i, item in enumerate(data):\n        if i % 4 == 0:\n            labeled_data.append(item)\n        else:\n            unlabeled_data.append(item)\n    return labeled_data, unlabeled_data\n\nfor train_idx, train_data in enumerate(train_loader):\n    train_inputs, train_gts, train_names = train_data\n    print(train_names)\n    \n    # \u6b63\u5e38\u8bad\u7ec3\u4e2d\u4e0b\u9762\u5e94\u8be5\u6709\uff0c\u8fd9\u91cc\u4e3a\u4e86\u65b9\u4fbf\u5c31\u5173\u6389\u4e86\n    # train_inputs = train_inputs.to(self.dev)\n    # train_gts = train_gts.to(self.dev)\n    train_labeled_inputs, train_unlabeled_inputs = split_data(train_inputs)\n    train_labeled_gts, _ = split_data(train_gts)\n    train_labeled_names, train_unlabeled_names = split_data(train_names)\n    print(\"labeled_names \", train_labeled_names)\n    print(\"unlabeled_names \", train_unlabeled_names)\n\n    # otr_total = self.net(train_inputs)\n    # labeled_otr, unlabeled_otr = otr_total.split((12, 36), dim=0)\n    # with torch.no_grad():\n    #     ema_unlabeled_otr = ema_model(train_unlabeled_inputs)\n    print(\" ==>> \u4e00\u4e2aBatch\u7ed3\u675f\u4e86 <<== \")\n    if train_idx == 2:\n        break\nprint(\" ==>> \u4e00\u4e2aEpoch\u7ed3\u675f\u4e86 <<== \")","893a74c6":"import os\n\nimport torch.utils.data as data\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nimport math\n\n\nclass JointResize(object):\n    def __init__(self, size):\n        if isinstance(size, int):\n            self.size = (size, size)\n        elif isinstance(size, tuple):\n            self.size = size\n        else:\n            raise RuntimeError(\"size\u53c2\u6570\u8bf7\u8bbe\u7f6e\u4e3aint\u6216\u8005tuple\")\n\n    def __call__(self, img, mask):\n        img = img.resize(self.size)\n        mask = mask.resize(self.size)\n        return img, mask\n\ndef make_dataset(root, prefix=('jpg', 'png')):\n    img_path = root[0]\n    gt_path = root[1]\n    img_list = [os.path.splitext(f)[0] for f in os.listdir(img_path) if f.endswith(prefix[0])]\n    return [(os.path.join(img_path, img_name + prefix[0]), os.path.join(gt_path, img_name + prefix[1])) for img_name in img_list]\n\n\n# \u4ec5\u9488\u5bf9\u8bad\u7ec3\u96c6\nclass ImageFolder(data.Dataset):\n    def __init__(self, root, mode, in_size, prefix, use_bigt=False, split_rate=(1, 3)):\n        \"\"\"split_rate = label:unlabel\"\"\"\n        assert isinstance(mode, str), 'isTrain\u53c2\u6570\u9519\u8bef\uff0c\u5e94\u8be5\u4e3abool\u7c7b\u578b'\n        self.mode = mode\n        self.use_bigt = use_bigt\n        self.split_rate = split_rate\n        self.r_l_rate = split_rate[1] \/\/ split_rate[0]\n\n        self.root_labeled = root[0]\n        self.imgs_labeled = make_dataset(self.root_labeled, prefix=prefix)\n\n        len_labeled = len(self.imgs_labeled)\n        self.length = len_labeled\n\n        self.root_unlabeled = root[1]\n        self.imgs_unlabeled = make_dataset(self.root_unlabeled, prefix=prefix)\n        \n        len_unlabeled = self.r_l_rate * len_labeled\n        \n        self.imgs_unlabeled = self.imgs_unlabeled * (self.r_l_rate + math.ceil(len_labeled \/ len_unlabeled))  # \u6269\u5c55\u65e0\u6807\u7b7e\u7684\u6570\u636e\u5217\u8868\n        self.imgs_unlabeled = self.imgs_unlabeled[0:len_unlabeled]\n\n        print(f\"\u4f7f\u7528\u6bd4\u4f8b\u4e3a\uff1a{len_labeled \/ len_unlabeled}\")\n\n        # \u4ec5\u662f\u4e3a\u4e86\u7b80\u5355\u800c\u4ec5\u4f7f\u7528\u4e00\u79cd\u53d8\u6362\n        self.train_joint_transform = JointResize(in_size)\n        self.train_img_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # \u5904\u7406\u7684\u662fTensor\n        ])\n        # ToTensor \u64cd\u4f5c\u4f1a\u5c06 PIL.Image \u6216\u5f62\u72b6\u4e3a H\u00d7W\u00d7D\uff0c\u6570\u503c\u8303\u56f4\u4e3a [0, 255] \u7684 np.ndarray \u8f6c\u6362\u4e3a\u5f62\u72b6\u4e3a D\u00d7H\u00d7W\uff0c\n        # \u6570\u503c\u8303\u56f4\u4e3a [0.0, 1.0] \u7684 torch.Tensor\u3002\n        self.train_gt_transform = transforms.ToTensor()\n\n    def __getitem__(self, index):\n        # \u8fd9\u91cc\u4e00\u6b21\u6027\u8bfb\u53d6\u6700\u7b80\u5316\u6bd4\u4f8b\u6570\u91cf\u7684\u6837\u672c\uff0c\u6240\u6709\u7684\u6837\u672c\u9700\u8981\u5355\u72ec\u5904\u7406\n        img_labeled_path, gt_labeled_path = self.imgs_labeled[index]  # 0, 1 => 850\n        img_labeled = Image.open(img_labeled_path).convert('RGB')\n        img_labeled_name = (img_labeled_path.split(os.sep)[-1]).split('.')[0]\n\n        gt_labeled = Image.open(gt_labeled_path).convert('L')\n        back_gt_labeled = gt_labeled  # \u7528\u4e8e\u65e0\u6807\u7b7e\u6570\u636e\u4f7f\u7528\u8054\u5408\u8c03\u6574\u51fd\u6570\u7684\u65f6\u5019\u4ee3\u66ff\u65e0\u6807\u7b7e\u6570\u636e\u771f\u503c\u8fdb\u884c\u5360\u4f4d\n        img_labeled, gt_labeled = self.train_joint_transform(img_labeled, gt_labeled)\n        img_labeled = self.train_img_transform(img_labeled)\n        gt_labeled = self.train_gt_transform(gt_labeled)\n        if self.use_bigt:\n            gt_labeled = gt_labeled.ge(0.5).float()  # \u4e8c\u503c\u5316\n        data_labeled = [img_labeled, gt_labeled, img_labeled_name]\n        \n        data_unlabeled = [[], []]\n        for idx_periter in range(self.r_l_rate):\n            # \u8fd9\u91cc\u4e0d\u518d\u4f7f\u7528\u771f\u503c\uff0c\u76f4\u63a5\u4f7f\u7528`_`\u63a5\u6536\n            img_unlabeled_path, _ = self.imgs_unlabeled[index\/\/self.r_l_rate+idx_periter]  # 0, 1, 2, 3 => 3*850\n            img_unlabeled = Image.open(img_unlabeled_path).convert('RGB')\n            img_unlabeled_name = (img_unlabeled_path.split(os.sep)[-1]).split('.')[0]\n\n            img_unlabeled, _ = self.train_joint_transform(img_unlabeled, back_gt_labeled)  # \u8fd9\u91cc\u4e3a\u4e86\u4f7f\u7528\u90a3\u4e2a\u8054\u5408\u8c03\u6574\u7684\u8f6c\u6362\u7c7b\uff0c\u4f7f\u7528\u4e0a\u9762\u7684target\u8fdb\u884c\u66ff\u4ee3\uff0c\u4f46\u662f\u8981\u6ce8\u610f\uff0c\u4e0d\u8981\u518d\u8fd4\u56de\u4e86\n            img_unlabeled = self.train_img_transform(img_unlabeled)\n                        \n            data_unlabeled[0].append(img_unlabeled)\n            data_unlabeled[1].append(img_unlabeled_name)\n\n        return data_labeled, data_unlabeled  # \u8f93\u51fa\u540d\u5b57\u65b9\u4fbf\u6bd4\u8f83\n\n    def __len__(self):\n        return self.length\n    \nprint(f\" ==>> \u4f7f\u7528\u7684\u8bad\u7ec3\u96c6 <<==\\n -->> LABELED_PATH\uff1a{LABELED_PATH}\\n -->> UNLABELED_PATH\uff1a{UNLABELED_PATH}\")\ntrain_set = ImageFolder((LABELED_PATH, UNLABELED_PATH), \"train\", 320, prefix=('.jpg', '.png'), use_bigt=True, split_rate=(12, 36))\n# \u7531\u4e8etrain_set\u5185\u90e8\u7684\u6bd4\u4f8b\u987a\u5e8f\u5df2\u7ecf\u88ab\u56fa\u5b9a\u5230\u6bcf\u4e00\u6b21iter\u4e2d\uff0c\u6240\u4ee5\u53ef\u4ee5\u4f7f\u7528`shuffle=True`\ntrain_loader = DataLoader(train_set, batch_size=12, num_workers=8, shuffle=True, drop_last=False, pin_memory=True)  \n\nfor train_idx, train_data in enumerate(train_loader):\n    data_labeled, data_unlabeled = train_data\n    \n    train_labeled_inputs, train_labeled_gts, train_labeled_names = data_labeled\n    print(train_labeled_inputs.size(), train_labeled_gts.size())\n    print(\"train_labeled_names \", train_labeled_names)\n    \n    train_unlabeled_inputs_list, train_unlabeled_names = data_unlabeled\n    train_unlabeled_inputs = torch.cat(train_unlabeled_inputs_list, dim=0)\n    print(train_unlabeled_inputs.size())\n    print(\"train_unlabeled_names \", train_unlabeled_names)\n    \n    train_labeled_inputs_batchsize = train_labeled_inputs.size(0)\n    train_unlabeled_inputs_batchsize = train_unlabeled_inputs.size(0)\n    \n    # \u6b63\u5e38\u8bad\u7ec3\u4e2d\u4e0b\u9762\u5e94\u8be5\u6709\uff0c\u8fd9\u91cc\u4e3a\u4e86\u65b9\u4fbf\u5c31\u5173\u6389\u4e86\uff0c\u8fd9\u91cc\u4e4b\u6240\u4ee5\u4e0d\u5148\u8fdb\u884ccat\u518d\u8fdb\u884cto(dev)\uff0c\u662f\u4e3a\u4e86\u4fbf\u4e8e\u540e\u9762ema_model\u8f93\u5165\u7684\u65f6\u5019\u4f7f\u7528\u4e00\u4e2a\u5df2\u7ecf\u5728gpu\u4e0a\u7684\u5f20\u91cf\uff0c\u514d\u53bb\u4e86\u518d\u6b21\u642c\u8fd0\u7684\u9ebb\u70e6\n    # train_labeled_inputs = train_labeled_inputs.to(dev)\n    # train_unlabeled_inputs = train_unlabeled_inputs.to(dev)\n    # train_gts = train_labeled_gts.to(self.dev)\n    train_inputs = torch.cat([train_labeled_inputs, train_unlabeled_inputs], dim=0)\n\n    # otr_total = net(train_inputs)\n    # labeled_otr, unlabeled_otr = otr_total.split((train_labeled_inputs_batchsize, train_unlabeled_inputs_batchsize), dim=0)\n    # with torch.no_grad():\n    #     ema_unlabeled_otr = ema_model(train_unlabeled_inputs)\n    print(\" ==>> \u4e00\u4e2aBatch\u7ed3\u675f\u4e86 <<== \")\n    if train_idx == 2:\n        break\nprint(\" ==>> \u4e00\u4e2aEpoch\u7ed3\u675f\u4e86 <<== \")","64ae0b8c":"sampler = [x for x in range(10)]\nprint(f\"\u539f\u59cbSampler\uff1a{sampler}\")\n\nfrom torch.utils.data.sampler import SequentialSampler\nprint(f\"\u987a\u5e8f\u91c7\u6837\uff1a{[x for x in SequentialSampler(sampler)]}\")\n\nfrom torch.utils.data.sampler import RandomSampler\nprint(f\"\u968f\u673a\u7f6e\u4e71\uff1a{[x for x in RandomSampler(data_source=sampler, replacement=True, num_samples=5)]}\")","014061b4":"import os\n\nimport torch.utils.data as data\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nimport math\n\n\nclass JointResize(object):\n    def __init__(self, size):\n        if isinstance(size, int):\n            self.size = (size, size)\n        elif isinstance(size, tuple):\n            self.size = size\n        else:\n            raise RuntimeError(\"size\u53c2\u6570\u8bf7\u8bbe\u7f6e\u4e3aint\u6216\u8005tuple\")\n\n    def __call__(self, img, mask):\n        img = img.resize(self.size)\n        mask = mask.resize(self.size)\n        return img, mask\n\ndef make_dataset(root, prefix=('jpg', 'png')):\n    img_path = root[0]\n    gt_path = root[1]\n    img_list = [os.path.splitext(f)[0] for f in os.listdir(img_path) if f.endswith(prefix[0])]\n    return [(os.path.join(img_path, img_name + prefix[0]), os.path.join(gt_path, img_name + prefix[1])) for img_name in img_list]\n\n\n# \u4ec5\u9488\u5bf9\u8bad\u7ec3\u96c6\nclass ImageFolder(data.Dataset):\n    def __init__(self, root, mode, in_size, prefix, use_bigt=False, split_rate=(1, 3)):\n        \"\"\"split_rate = label:unlabel\"\"\"\n        assert isinstance(mode, str), 'isTrain\u53c2\u6570\u9519\u8bef\uff0c\u5e94\u8be5\u4e3abool\u7c7b\u578b'\n        self.mode = mode\n        self.use_bigt = use_bigt\n        self.split_rate = split_rate\n        self.r_l_rate = split_rate[1] \/\/ split_rate[0]\n\n        self.root_labeled = root[0]\n        self.imgs_labeled = make_dataset(self.root_labeled, prefix=prefix)\n\n        len_labeled = len(self.imgs_labeled)\n        self.length = len_labeled\n\n        self.root_unlabeled = root[1]\n        self.imgs_unlabeled = make_dataset(self.root_unlabeled, prefix=prefix)\n        \n        len_unlabeled = self.r_l_rate * len_labeled\n        \n        self.imgs_unlabeled = self.imgs_unlabeled * (self.r_l_rate + math.ceil(len_labeled \/ len_unlabeled))  # \u6269\u5c55\u65e0\u6807\u7b7e\u7684\u6570\u636e\u5217\u8868\n        self.imgs_unlabeled = self.imgs_unlabeled[0:len_unlabeled]\n\n        print(f\"\u4f7f\u7528\u6bd4\u4f8b\u4e3a\uff1a{len_labeled \/ len_unlabeled}\")\n\n        # \u4ec5\u662f\u4e3a\u4e86\u7b80\u5355\u800c\u4ec5\u4f7f\u7528\u4e00\u79cd\u53d8\u6362\n        self.train_joint_transform = JointResize(in_size)\n        self.train_img_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # \u5904\u7406\u7684\u662fTensor\n        ])\n        # ToTensor \u64cd\u4f5c\u4f1a\u5c06 PIL.Image \u6216\u5f62\u72b6\u4e3a H\u00d7W\u00d7D\uff0c\u6570\u503c\u8303\u56f4\u4e3a [0, 255] \u7684 np.ndarray \u8f6c\u6362\u4e3a\u5f62\u72b6\u4e3a D\u00d7H\u00d7W\uff0c\n        # \u6570\u503c\u8303\u56f4\u4e3a [0.0, 1.0] \u7684 torch.Tensor\u3002\n        self.train_gt_transform = transforms.ToTensor()\n\n    def __getitem__(self, index):\n        # \u8fd9\u91cc\u4e00\u6b21\u6027\u8bfb\u53d6\u6700\u7b80\u5316\u6bd4\u4f8b\u6570\u91cf\u7684\u6837\u672c\uff0c\u6240\u6709\u7684\u6837\u672c\u9700\u8981\u5355\u72ec\u5904\u7406\n        img_labeled_path, gt_labeled_path = self.imgs_labeled[index]  # 0, 1 => 850\n        img_labeled = Image.open(img_labeled_path).convert('RGB')\n        img_labeled_name = (img_labeled_path.split(os.sep)[-1]).split('.')[0]\n\n        gt_labeled = Image.open(gt_labeled_path).convert('L')\n        back_gt_labeled = gt_labeled  # \u7528\u4e8e\u65e0\u6807\u7b7e\u6570\u636e\u4f7f\u7528\u8054\u5408\u8c03\u6574\u51fd\u6570\u7684\u65f6\u5019\u4ee3\u66ff\u65e0\u6807\u7b7e\u6570\u636e\u771f\u503c\u8fdb\u884c\u5360\u4f4d\n        img_labeled, gt_labeled = self.train_joint_transform(img_labeled, gt_labeled)\n        img_labeled = self.train_img_transform(img_labeled)\n        gt_labeled = self.train_gt_transform(gt_labeled)\n        if self.use_bigt:\n            gt_labeled = gt_labeled.ge(0.5).float()  # \u4e8c\u503c\u5316\n        data_labeled = [img_labeled, gt_labeled, img_labeled_name]\n        \n        data_unlabeled = [[], []]\n        for idx_periter in range(self.r_l_rate):\n            # \u8fd9\u91cc\u4e0d\u518d\u4f7f\u7528\u771f\u503c\uff0c\u76f4\u63a5\u4f7f\u7528`_`\u63a5\u6536\n            img_unlabeled_path, _ = self.imgs_unlabeled[index\/\/self.r_l_rate+idx_periter]  # 0, 1, 2, 3 => 3*850\n            img_unlabeled = Image.open(img_unlabeled_path).convert('RGB')\n            img_unlabeled_name = (img_unlabeled_path.split(os.sep)[-1]).split('.')[0]\n\n            img_unlabeled, _ = self.train_joint_transform(img_unlabeled, back_gt_labeled)  # \u8fd9\u91cc\u4e3a\u4e86\u4f7f\u7528\u90a3\u4e2a\u8054\u5408\u8c03\u6574\u7684\u8f6c\u6362\u7c7b\uff0c\u4f7f\u7528\u4e0a\u9762\u7684target\u8fdb\u884c\u66ff\u4ee3\uff0c\u4f46\u662f\u8981\u6ce8\u610f\uff0c\u4e0d\u8981\u518d\u8fd4\u56de\u4e86\n            img_unlabeled = self.train_img_transform(img_unlabeled)\n                        \n            data_unlabeled[0].append(img_unlabeled)\n            data_unlabeled[1].append(img_unlabeled_name)\n\n        return data_labeled, data_unlabeled  # \u8f93\u51fa\u540d\u5b57\u65b9\u4fbf\u6bd4\u8f83\n\n    def __len__(self):\n        return self.length\n    \n    \ndef my_collate(batch):\n    # \u9488\u5bf9\u9001\u8fdb\u6765\u7684\u4e00\u4e2abatch\u7684\u6570\u636e\u8fdb\u884c\u6574\u5408\uff0cbatch\u7684\u5404\u9879\u8868\u793a\u5404\u4e2a\u6837\u672c\n    # batch \u4ec5\u6709\u4e00\u9879 batch[0] \u5bf9\u5e94\u4e8e\u4e0b\u9762\u7684 train_data\n    # batch[0][0], batch[0][1] <==> data_labeled, data_unlabeled = train_data\n    # batch[0][0][0], batch[0][0][1], batch[0][0][2] <==> train_labeled_inputs, train_labeled_gts, train_labeled_names = data_labeled\n    # batch[0][1][0], batch[0][2][1] <==> train_unlabeled_inputs_list, train_unlabeled_names = data_unlabeled\n    \n    # \u6700\u76f4\u63a5\u7684\u65b9\u6cd5\uff1a\n    train_labeled_inputs, train_labeled_gts, train_labeled_names = [], [], []\n    train_unlabeled_inputs_list, train_unlabeled_names = [], []\n    for batch_iter in batch:\n        x, y = batch_iter\n        train_labeled_inputs.append(x[0])\n        train_labeled_gts.append(x[1])\n        train_labeled_names.append(x[2])\n        \n        train_unlabeled_inputs_list += y[0]\n        train_unlabeled_names += y[1]\n\n    train_labeled_inputs = torch.stack(train_labeled_inputs, 0)\n    train_unlabeled_inputs_list = torch.stack(train_unlabeled_inputs_list, 0)\n    train_labeled_gts = torch.stack(train_labeled_gts, 0)\n    print(train_unlabeled_inputs_list.size())\n    return ([train_labeled_inputs, train_unlabeled_inputs_list], [train_labeled_gts],\n            [train_labeled_names, train_unlabeled_names])\n\nprint(f\" ==>> \u4f7f\u7528\u7684\u8bad\u7ec3\u96c6 <<==\\n -->> LABELED_PATH\uff1a{LABELED_PATH}\\n -->> UNLABELED_PATH\uff1a{UNLABELED_PATH}\")\ntrain_set = ImageFolder((LABELED_PATH, UNLABELED_PATH), \"train\", 320, prefix=('.jpg', '.png'), use_bigt=True, split_rate=(12, 36))\n# a simple custom collate function, just to show the idea\ntrain_loader = DataLoader(train_set, batch_size=12, num_workers=4, collate_fn=my_collate, shuffle=True, drop_last=False, pin_memory=True)\nprint(\" ==>> data_loader\u6784\u5efa\u5b8c\u6bd5 <<==\")\n\nfor train_idx, train_data in enumerate(train_loader):\n\n    train_inputs, train_gts, train_names = train_data\n    \n    train_labeled_inputs, train_unlabeled_inputs = train_inputs\n    train_labeled_gts = train_gts[0]\n    train_labeled_names, train_unlabeled_names = train_names\n    print(train_labeled_inputs.size(), train_labeled_gts.size())\n    print(\"train_labeled_names \", train_labeled_names)\n    print(train_unlabeled_inputs.size())\n    print(\"train_unlabeled_names \", train_unlabeled_names)\n    \n    train_labeled_inputs_batchsize = train_labeled_inputs.size(0)\n    train_unlabeled_inputs_batchsize = train_unlabeled_inputs.size(0)\n    \n    # \u6b63\u5e38\u8bad\u7ec3\u4e2d\u4e0b\u9762\u5e94\u8be5\u6709\uff0c\u8fd9\u91cc\u4e3a\u4e86\u65b9\u4fbf\u5c31\u5173\u6389\u4e86\uff0c\u8fd9\u91cc\u4e4b\u6240\u4ee5\u4e0d\u5148\u8fdb\u884ccat\u518d\u8fdb\u884cto(dev)\uff0c\u662f\u4e3a\u4e86\u4fbf\u4e8e\u540e\u9762ema_model\u8f93\u5165\u7684\u65f6\u5019\u4f7f\u7528\u4e00\u4e2a\u5df2\u7ecf\u5728gpu\u4e0a\u7684\u5f20\u91cf\uff0c\u514d\u53bb\u4e86\u518d\u6b21\u642c\u8fd0\u7684\u9ebb\u70e6\n    # train_labeled_inputs = train_labeled_inputs.to(dev)\n    # train_unlabeled_inputs = train_unlabeled_inputs.to(dev)\n    # train_gts = train_labeled_gts.to(self.dev)\n    train_inputs = torch.cat([train_labeled_inputs, train_unlabeled_inputs], dim=0)\n\n    # otr_total = net(train_inputs)\n    # labeled_otr, unlabeled_otr = otr_total.split((train_labeled_inputs_batchsize, train_unlabeled_inputs_batchsize), dim=0)\n    # with torch.no_grad():\n    #     ema_unlabeled_otr = ema_model(train_unlabeled_inputs)\n    print(\" ==>> \u4e00\u4e2aBatch\u7ed3\u675f\u4e86 <<== \")\n    if train_idx == 0:\n        break\nprint(\" ==>> \u4e00\u4e2aEpoch\u7ed3\u675f\u4e86 <<== \")","6d906cbd":"## TODO\n- [?] \u8bfb\u53d6DUTS-TR\u548cMixFlickrDUS\u7528\u4e8e\u8bad\u7ec3\n- [?] \u6bcf\u4e2abatch\u90fd\u8981\u4fdd\u8bc1\u5305\u542b1\/4\u7684DUTS-TR\u7684\u6570\u636e\u96c6\u548c3\/4\u7684MixFlickrDUTS\n- [?] \u9488\u5bf9\u8bad\u7ec3\u96c6\u4f7f\u7528\u4e0d\u540c\u7684\u589e\u5f3a\u65b9\u5f0f\n\n\u4e0d\u8003\u8651\u6d4b\u8bd5\u96c6\uff0c\u56e0\u4e3a\u6d4b\u8bd5\u96c6\u5b8c\u5168\u53ef\u4ee5\u4f7f\u7528\u4e00\u4e2a\u72ec\u7acb\u7684ImageFolder\u7c7b\u6784\u9020\u3002","3e36aa76":"## \u65b9\u6cd5\u4e00\uff1a\u901a\u8fc7\u5bf9`__getitem__`\u7684\u7d22\u5f15\u8fdb\u884c\u8ba1\u7b97\uff0c\u6309\u7167\u6bd4\u4f8b\u5173\u7cfb\u9009\u62e9\u5bf9\u5e94\u6570\u636e\u96c6\u7684\u6570\u636e\n\n```python\nif index % (self.r_l_rate + 1) == 0:\n    label_index = index \/\/ (self.r_l_rate + 1)\n    img_path, gt_path = self.imgs_label[label_index]  # 0, 1 => 10550\nelse:\n    unlabel_index = index \/\/ (self.r_l_rate + 1) + index % (self.r_l_rate + 1)\n    img_path, gt_path = self.imgs_unlabel[unlabel_index]  # 1, 2, 3\n```","8f953a7c":"## \u65b9\u6cd5\u4e8c\uff1a\u76f4\u63a5\u5728`__getitem__`\u4e2d\u4e00\u6b21\u6027\u8bfb\u53d6\u6700\u7b80\u5316\u6bd4\u4f8b\u6570\u91cf\u7684\u6837\u672c\n\n\u4e0a\u9762\u7684\u7528\u6cd5\u867d\u7136\u7b80\u5355\uff0c\u76f4\u63a5\u5728\u4e00\u4e2aImageFolder\u4e2d\u5bf9\u6570\u636e\u8fdb\u884c\u7ec4\u5408\uff0c\u4f46\u662f\u8fd9\u6837\u4f1a\u5bfc\u81f4\u4e00\u4e2a\u95ee\u9898\uff0c\u8bad\u7ec3\u7684\u65f6\u5019\u65e0\u6cd5\u4f7f\u7528`shuffle=True`\u8bbe\u5b9a\uff0c\u5bf9\u4e8e\u8bad\u7ec3\u5e76\u4e0d\u5b8c\u7f8e\u3002\n\n\u9664\u4e86\u8fd9\u91cc\u7684\u8bbe\u7f6e\u65b9\u5f0f\uff0c\u8fd8\u6709\u4e00\u79cd\u503c\u5f97\u53c2\u8003\uff1a\u5728[PoolNet](https:\/\/github.com\/backseason\/PoolNet\/blob\/master\/dataset\/joint_dataset.py#L12-L50)\u7684\u8bbe\u7f6e\u4e2d\uff0c\u662f\u76f4\u63a5\u5bf9\u4e8e\u6bcf\u6b21\u8fed\u4ee3\u6309\u71671:1\u7684\u6bd4\u4f8b\u8f93\u5165\uff0c\u6240\u4ee5\u5176\u5728`__getitem__`\u4e2d\u76f4\u63a5\u540c\u65f6`imread`\u4e24\u4e2a\u6570\u636e\u96c6\u7684\u56fe\u50cf\u3002\u867d\u7136\u8fd9\u6837\u6bd4\u8f83\u7b80\u5355\uff0c\u4f46\u662f\u5374\u4e5f\u662f\u76f4\u63a5\u6709\u6548\u3002\n\n\u4e0b\u9762\u4eff\u5199\u4e00\u4efd\u3002","3efeccd0":"\u7531\u4e0a\u53ef\u89c1\uff0cSampler\u672c\u8d28\u5c31\u662f\u4e2a\u5177\u6709\u7279\u5b9a\u89c4\u5219\u7684\u53ef\u8fed\u4ee3\u5bf9\u8c61\uff0c\u4f46\u53ea\u80fd\u5355\u4f8b\u8fed\u4ee3\u3002\n\n\u5982 `[x for x in range(10)]`, range(10)\u5c31\u662f\u4e2a\u6700\u57fa\u672c\u7684Sampler\uff0c\u6bcf\u6b21\u5faa\u73af\u53ea\u80fd\u53d6\u51fa\u5176\u4e2d\u7684\u4e00\u4e2a\u503c.","8b8dd97d":"\u8fd9\u4e00\u90e8\u5206\u9700\u8981\u5206\u6790\u4e0bDataLoader\u7684\u51e0\u4e2a\u53c2\u6570\u3002\n\n> \u53c2\u8003\u8d44\u6599\uff1a\n> * https:\/\/blog.csdn.net\/u014380165\/article\/details\/79058479\n> * pytorch\u5b66\u4e60\u7b14\u8bb0\uff08\u5341\u56db\uff09\uff1a DataLoader\u6e90\u7801\u9605\u8bfb https:\/\/blog.csdn.net\/u012436149\/article\/details\/78545766\n> * Pytorch\u4e2d\u7684\u6570\u636e\u52a0\u8f7d\u827a\u672f http:\/\/studyai.com\/article\/11efc2bf\n\n```python\nclass DataLoader(object):\n    r\"\"\"\n    Data loader. Combines a dataset and a sampler, and provides\n    single- or multi-process iterators over the dataset.\n\n    Arguments:\n        dataset (Dataset): dataset from which to load the data. \n            \u81ea\u5b9a\u4e49\u7684Dataset\u7c7b\u7684\u5b50\u7c7b\uff0c\u5b9e\u73b0\u4e86\u57fa\u672c\u7684\u6570\u636e\u7684\u8bfb\u53d6\u6d41\u7a0b\uff0c\u4f8b\u5982\u83b7\u53d6\u5730\u5740\u5217\u8868\u3001\u6839\u636e\u7d22\u5f15\u6253\u5f00\u56fe\u7247\u3001\n            \u56fe\u7247\u9884\u5904\u7406\u7b49\u7b49\n        batch_size (int, optional): how many samples per batch to load \n            (default: ``1``). \n            \u5982\u5b57\u9762\u542b\u4e49\uff0c\u786e\u5b9a\u4e86batchsize\uff0c\u53ef\u77e5batch\u662f\u5bf9\u6570\u4e2a\u6837\u672c\u7684\u5305\u88c5\n        shuffle (bool, optional): set to ``True`` to have the data reshuffled \n            at every epoch (default: ``False``). \n            \u662f\u5426\u6bcf\u4e2a\u5468\u671f\u90fd\u6253\u4e71\u6570\u636e\u7684\u539f\u59cb\u987a\u5e8f\uff0c\u4e00\u822c\u662f\u8bad\u7ec3\u7684\u65f6\u5019\u4e3aTrue\uff0c\u6d4b\u8bd5\u4e3aFalse\n        sampler (Sampler, optional): defines the strategy to draw samples \n            from the dataset. If specified, ``shuffle`` must be False. \n            \u5b9a\u4e49\u4e86\u4ece\u6570\u636e\u4e2d\u91c7\u6837\u7684\u7b56\u7565\uff0c\u4e00\u6b21\u8fd4\u56de\u4e00\u4e2a\u6837\u672c\u7684\u7d22\u5f15\uff0c\u8fd9\u662fSampler\u7684\u5b50\u7c7b\uff0c\u6b64\u65f6\u5fc5\u987b\u5173\u95edshuffle\u64cd\u4f5c\uff0c\n            \u76f8\u5f53\u4e8e\u4f60\u5f97\u81ea\u5df1\u5b9e\u73b0\n        batch_sampler (Sampler, optional): like sampler, but returns a batch of\n            indices at a time. Mutually exclusive with :attr:`batch_size`,\n            :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`.\n            \u548csampler\u7c7b\u4f3c\uff0c\u4f46\u662f\u8fd9\u4e2a\u8ddf\u66f4\u8fdb\u4e00\u6b65\uff0c\u5b9a\u4e49\u4e86\u9488\u5bf9batch\u7ea7\u522b\u7684\u6570\u636e\u7684\u91c7\u6837\u7b56\u7565\uff0c\u4e0ebatch_size\/\n            shuffle\/sampler\/drop_last\u4e92\u65a5\uff0c\u4e00\u6b21\u53ef\u4ee5\u8fd4\u56de\u4e00\u4e2abatch\u7684\u7d22\u5f15\n        num_workers (int, optional): how many subprocesses to use for data\n            loading. 0 means that the data will be loaded in the main process.\n            (default: ``0``)\n            \u8bfb\u53d6\u6570\u636e\u4f7f\u7528\u7684\u5b50\u8fdb\u7a0b\u6570\u76ee\uff0c\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u53ef\u4ee5\u52a0\u5feb\u6570\u636e\u8bfb\u53d6\n        collate_fn (callable, optional): merges a list of samples to form a mini-batch.\n            \u662f\u4e00\u4e2a\u53ef\u8c03\u7528\u7684\u5bf9\u8c61\uff0c\u7528\u6765\u5408\u5e76\u6837\u672c\uff0c\u6784\u5efamini-batch\n        pin_memory (bool, optional): If ``True``, the data loader will copy tensors\n            into CUDA pinned memory before returning them.  If your data elements\n            are a custom type, or your ``collate_fn`` returns a batch that is a custom type\n            see the example below.\n            \u5982\u679c\u4e3aTrue\uff0c\u6570\u636e\u52a0\u8f7d\u5668\u5728\u8fd4\u56de\u524d\u5c06\u5f20\u91cf\u590d\u5236\u5230CUDA\u56fa\u5b9a\u5185\u5b58\u4e2d\n        drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n            if the dataset size is not divisible by the batch size. If ``False`` and\n            the size of dataset is not divisible by the batch size, then the last batch\n            will be smaller. (default: ``False``)\n            \u662f\u5426\u4e22\u5f03\u6bcf\u4e2a\u5468\u671f\u6700\u540e\u4e00\u4e2a\u4e0d\u5b8c\u6574\u7684batch\uff0c\u5982\u679c\u5b58\u5728\u7684\u8bdd\n        timeout (numeric, optional): if positive, the timeout value for collecting a batch\n            from workers. Should always be non-negative. (default: ``0``)\n            \u662f\u4e00\u4e2a\u975e\u8d1f\u503c\uff0c\u6765\u6307\u5b9a\u4eceworkers\u4e2d\u83b7\u53d6\u6570\u636e\u7684timeout\u53c2\u6570\uff0c\u8d85\u8fc7\u8fd9\u4e2a\u65f6\u95f4\u8fd8\u6ca1\u8bfb\u53d6\u5230\u6570\u636e\u7684\u8bdd\u5c31\u4f1a\u62a5\u9519\n        worker_init_fn (callable, optional): If not ``None``, this will be called on each\n            worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n            input, after seeding and before data loading. (default: ``None``) \n            \u5982\u679c\u4e0d\u662fNone\uff0c\u5c06\u5728\u6bcf\u4e2aworker\u5b50\u8fdb\u7a0b\u4e0a\u8c03\u7528\uff0c\u4f7f\u7528worker id\u4f5c\u4e3a\u8f93\u5165\uff0c\u5728seeding\u4e4b\u540e\u4ee5\u53ca\u6570\u636e\u52a0\u8f7d\u4e4b\u524d\n            \uff08\u8fd9\u4e2a\u4e0d\u592a\u61c2\uff0c\u76ee\u524d\u8fd8\u4e0d\u7406\u89e3\u7528\u6cd5\uff09\n\n    .. note:: When ``num_workers != 0``, the corresponding worker processes are created each time\n              iterator for the DataLoader is obtained (as in when you call\n              ``enumerate(dataloader,0)``).\n              At this point, the dataset, ``collate_fn`` and ``worker_init_fn`` are passed to each\n              worker, where they are used to access and initialize data based on the indices\n              queued up from the main process. This means that dataset access together with\n              its internal IO, transforms and collation runs in the worker, while any\n              shuffle randomization is done in the main process which guides loading by assigning\n              indices to load. Workers are shut down once the end of the iteration is reached.\n\n              Since workers rely on Python multiprocessing, worker launch behavior is different\n              on Windows compared to Unix. On Unix fork() is used as the default\n              muliprocessing start method, so child workers typically can access the dataset and\n              Python argument functions directly through the cloned address space. On Windows, another\n              interpreter is launched which runs your main script, followed by the internal\n              worker function that receives the dataset, collate_fn and other arguments\n              through Pickle serialization.\n\n              This separate serialization means that you should take two steps to ensure you\n              are compatible with Windows while using workers\n              (this also works equally well on Unix):\n\n              - Wrap most of you main script's code within ``if __name__ == '__main__':`` block,\n                to make sure it doesn't run again (most likely generating error) when each worker\n                process is launched. You can place your dataset and DataLoader instance creation\n                logic here, as it doesn't need to be re-executed in workers.\n              - Make sure that ``collate_fn``, ``worker_init_fn`` or any custom dataset code\n                is declared as a top level def, outside of that ``__main__`` check. This ensures\n                they are available in workers as well\n                (this is needed since functions are pickled as references only, not bytecode).\n\n              By default, each worker will have its PyTorch seed set to\n              ``base_seed + worker_id``, where ``base_seed`` is a long generated\n              by main process using its RNG. However, seeds for other libraies\n              may be duplicated upon initializing workers (w.g., NumPy), causing\n              each worker to return identical random numbers. (See\n              :ref:`dataloader-workers-random-seed` section in FAQ.) You may\n              use :func:`torch.initial_seed()` to access the PyTorch seed for\n              each worker in :attr:`worker_init_fn`, and use it to set other\n              seeds before data loading.\n\n    .. warning:: If ``spawn`` start method is used, :attr:`worker_init_fn` cannot be an\n                 unpicklable object, e.g., a lambda function.\n\n    The default memory pinning logic only recognizes Tensors and maps and iterables\n    containg Tensors.  By default, if the pinning logic sees a batch that is a custom type\n    (which will occur if you have a ``collate_fn`` that returns a custom batch type),\n    or if each element of your batch is a custom type, the pinning logic will not\n    recognize them, and it will return that batch (or those elements)\n    without pinning the memory.  To enable memory pinning for custom batch or data types,\n    define a ``pin_memory`` method on your custom type(s).\n    \u9ed8\u8ba4\u7684\u5185\u5b58\u56fa\u5b9a\u903b\u8f91\u4ec5\u8bc6\u522b\u5f20\u91cf\uff0c\u5305\u542b\u5f20\u91cf\u7684\u6620\u5c04\u548c\u8fed\u4ee3\u3002\n    \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5982\u679c\u56fa\u5b9a\u903b\u8f91\u770b\u5230\u4e00\u4e2a\u81ea\u5b9a\u4e49\u7c7b\u578b\u7684\u6279\u5904\u7406\uff08\u5982\u679c\u60a8\u6709\u4e00\u4e2a\u8fd4\u56de\u81ea\u5b9a\u4e49\u6279\u5904\u7406\u7c7b\u578b\u7684collate_fn\uff0c\u6216\u8005\n    \u5982\u679c\u6279\u5904\u7406\u7684\u6bcf\u4e2a\u5143\u7d20\u90fd\u662f\u81ea\u5b9a\u4e49\u7c7b\u578b\uff0c\u5219\u4f1a\u53d1\u751f\u8fd9\u79cd\u60c5\u51b5\uff09 \u903b\u8f91\u5c06\u65e0\u6cd5\u8bc6\u522b\u5b83\u4eec\uff0c\u5b83\u5c06\u8fd4\u56de\u8be5\u6279\u6b21\uff08\u6216\u90a3\u4e9b\u5143\u7d20\uff09\u5e76\u4e14\n    \u4e0d\u56fa\u5b9a\u5185\u5b58\u3002\u8981\u4e3a\u81ea\u5b9a\u4e49\u6279\u5904\u7406\u6216\u6570\u636e\u7c7b\u578b\u542f\u7528\u5185\u5b58\u56fa\u5b9a\uff0c\u8bf7\u5728\u81ea\u5b9a\u4e49\u7c7b\u578b\u4e0a\u5b9a\u4e49`pin_memory`\u65b9\u6cd5\u3002\n\n    Example::\n\n        class SimpleCustomBatch:\n            def __init__(self, data):\n                transposed_data = list(zip(*data))\n                self.inp = torch.stack(transposed_data[0], 0)\n                self.tgt = torch.stack(transposed_data[1], 0)\n\n            def pin_memory(self):\n                self.inp = self.inp.pin_memory()\n                self.tgt = self.tgt.pin_memory()\n                return self\n\n        def collate_wrapper(batch):\n            return SimpleCustomBatch(batch)\n\n        inps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\n        tgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\n        dataset = TensorDataset(inps, tgts)\n\n        loader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper,\n                            pin_memory=True)\n\n        for batch_ndx, sample in enumerate(loader):\n            print(sample.inp.is_pinned())\n            print(sample.tgt.is_pinned())\n\n    \"\"\"\n```\n\n\u8fd9\u91cc\u4e3b\u8981\u5173\u6ce8\u53c2\u6570\u4e2d\u7684`sampler`\u3001`batch_sampler`\u4ee5\u53ca`collate_fn`\u7684\u7528\u6cd5\u3002\n\n### `sampler`\u3001`batch_sampler`\n\n\u9996\u5148\u53ef\u4ee5\u770b\u9ed8\u8ba4\u8981\u6c42\u662f\u5982\u4f55\uff1a\n\n```python\n        # batch_sampler\u6307\u5b9a\u7684\u65f6\u5019\uff0c\u8981\u6c42batch_size=1\/shuule=False\/sampler=None\/drop_last=False\n        # \u4e5f\u5c31\u662fbatch_sampler\u9700\u8981\u5b8c\u6210\u8bfb\u53d6\u5e76\u5212\u5206batch\u3001\u7f6e\u4e71\u6570\u636e\u3001\u5904\u7406\u6700\u540e\u7684batch\u7b49\u9700\u6c42\n        if batch_sampler is not None:\n            if batch_size > 1 or shuffle or sampler is not None or drop_last:\n                raise ValueError('batch_sampler option is mutually exclusive '\n                                 'with batch_size, shuffle, sampler, and '\n                                 'drop_last')\n            self.batch_size = None\n            self.drop_last = None\n\n        # sampler\u6307\u5b9a\u7684\u65f6\u5019\uff0c\u8981\u6c42shuffle=False\uff0c\u4e5f\u5c31\u662fsampler\u9700\u8981\u5b8c\u6210\u6570\u636e\u7684\u83b7\u53d6\u6253\u4e71\u7684\u9700\u6c42\n        if sampler is not None and shuffle:\n            raise ValueError('sampler option is mutually exclusive with '\n                             'shuffle')\n\n        if self.num_workers < 0:\n            raise ValueError('num_workers option cannot be negative; '\n                             'use num_workers=0 to disable multiprocessing.')\n\n        # batch_sampler\u548csampler\u90fd\u6ca1\u6709\u6307\u5b9a\u7684\u65f6\u5019\uff0csampler\u6839\u636eshuffle\u6765\u786e\u5b9a\u9ed8\u8ba4\u7684\u8bbe\u7f6e\u4e3a\n        # RandomSampler\u548cSequentialSampler\uff0c\u53ef\u4ee5\u770b\u51fa\u6765\uff0c\u4e00\u4e2a\u662f\u968f\u673a\u62bd\u53d6\uff08\u6240\u8c13\u7f6e\u4e71\uff09\u4e00\u4e2a\u662f\n        # \u6309\u7167\u987a\u5e8f\u62bd\u53d6\uff0c\u800cbatch_sampler\u8bbe\u7f6e\u4e3aBatchSampler\uff0c\u6240\u4ee5\u8bf4\uff0c\u82e5\u60f3\u8981\u81ea\u5df1\u5b9e\u73b0batch_sampler\n        # \u6216\u8005sampler\uff0c\u53ea\u8981\u6a21\u4eff\u8fd9\u4e09\u4e2a\u7c7b\u5373\u53ef\n        if batch_sampler is None:\n            if sampler is None:\n                if shuffle:\n                    sampler = RandomSampler(dataset)\n                else:\n                    sampler = SequentialSampler(dataset)\n            batch_sampler = BatchSampler(sampler, batch_size, drop_last)\n\n        self.sampler = sampler\n        self.batch_sampler = batch_sampler\n        self.__initialized = True\n```\n\n> \u7b80\u8a00\u4e4b\uff0c\u91c7\u6837\u5668\u5b9a\u4e49\u4e86\u7d22\u5f15\uff08index\uff09\u7684\u4ea7\u751f\u89c4\u5219\uff0c\u6309\u6307\u5b9a\u89c4\u5219\u53bb\u4ea7\u751f\u7d22\u5f15\uff0c\u4ece\u800c\u63a7\u5236\u6570\u636e\u7684\u8bfb\u53d6\u673a\u5236(http:\/\/studyai.com\/article\/11efc2bf)\n\n\u67e5\u770b\u8fd9\u51e0\u4e2a\u7c7b\uff0c\u8fd9\u91cc\u7684\u4ee3\u7801\u6765\u81ea[V1.1.0](https:\/\/github.com\/pytorch\/pytorch\/blob\/v1.1.0\/torch\/utils\/data\/sampler.py)\uff1a\n\n```python\nimport torch\nfrom torch._six import int_classes as _int_classes\n\n\nclass Sampler(object):\n    r\"\"\"Base class for all Samplers.\n    Every Sampler subclass has to provide an __iter__ method, providing a way\n    to iterate over indices of dataset elements, and a __len__ method that\n    returns the length of the returned iterators.\n    \n    \u6bcf\u4e2aSampler\u7684\u5b50\u7c7b\uff08\u540e\u9762\u7684\u90a3\u4e9b\u91c7\u96c6\u6570\u636e\u7684\u7c7b\uff09\u90fd\u8981\u5305\u542b\u4e0b\u9762\u8fd9\u51e0\u4e2a\u65b9\u6cd5\n    \"\"\"\n\n    def __init__(self, data_source):\n        pass\n\n    def __iter__(self):\n        raise NotImplementedError\n\n    def __len__(self):\n        # \u5728V1.2.0\u4e2d\uff0c\u6ca1\u6709\u4e86\u8fd9\u4e2a\u9700\u6c42\uff1ahttps:\/\/github.com\/pytorch\/pytorch\/blob\/v1.2.0\/torch\/utils\/data\/sampler.py#L23-L48\n        raise NotImplementedError\n\n\nclass SequentialSampler(Sampler):\n    r\"\"\"Samples elements sequentially, always in the same order.\n    Arguments:\n        data_source (Dataset): dataset to sample from\n        \n    \u4fdd\u8bc1\u6bcf\u4e2a\u5468\u671f\u6309\u7167\u56fa\u5b9a\u7684\u987a\u5e8f\u8bfb\u53d6\uff0c\u6240\u4ee5\u8fd9\u91cc\u76f4\u63a5\u4f7f\u7528\u4e86range(len(self.data_source))\u4f5c\u4e3a\u987a\u5e8f\n    \"\"\"\n\n    def __init__(self, data_source):\n        self.data_source = data_source\n\n    def __iter__(self):\n        return iter(range(len(self.data_source)))\n\n    def __len__(self):\n        return len(self.data_source)\n\n\nclass RandomSampler(Sampler):\n    r\"\"\"Samples elements randomly. If without replacement, then sample from a shuffled dataset.\n    If with replacement, then user can specify ``num_samples`` to draw.\n    Arguments:\n        data_source (Dataset): dataset to sample from\n        replacement (bool): samples are drawn with replacement if ``True``, default=``False``\n        num_samples (int): number of samples to draw, default=`len(dataset)`. This argument\n            is supposed to be specified only when `replacement` is ``True``.\n            \n    \u8fd4\u56de\u968f\u673a\u6253\u4e71\u540e\u7684\u7d22\u5f15\u8fed\u4ee3\u5668\n    \"\"\"\n\n    def __init__(self, data_source, replacement=False, num_samples=None):\n        self.data_source = data_source\n        self.replacement = replacement\n        self._num_samples = num_samples\n\n        if not isinstance(self.replacement, bool):\n            raise ValueError(\"replacement should be a boolean value, but got \"\n                             \"replacement={}\".format(self.replacement))\n\n        if self._num_samples is not None and not replacement:\n            raise ValueError(\"With replacement=False, num_samples should not be specified, \"\n                             \"since a random permute will be performed.\")\n\n        if not isinstance(self.num_samples, int) or self.num_samples <= 0:\n            raise ValueError(\"num_samples should be a positive integer \"\n                             \"value, but got num_samples={}\".format(self.num_samples))\n\n    # \u5173\u4e8e\u8be5\u88c5\u9970\u5668\uff1ahttps:\/\/www.programiz.com\/python-programming\/property\n    # \u8fd9\u91cc\u4e3a\u79c1\u6709\u5c5e\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u63a5\u53e3\n    @property\n    def num_samples(self):\n        # dataset size might change at runtime\n        if self._num_samples is None:\n            return len(self.data_source)\n        return self._num_samples\n\n    def __iter__(self):\n        n = len(self.data_source)\n        if self.replacement:\n            # \u8fd9\u91cc\u53ef\u4ee5\u91cd\u65b0\u5236\u5b9a\u7d22\u5f15\u5217\u8868\u957f\u5ea6(=self.num_samples)\uff0c\u7d22\u5f15\u5217\u8868\u6700\u5927\u503c(=len(self.data_source)\u662f\u56fa\u5b9a\u7684\n            return iter(torch.randint(high=n, size=(self.num_samples,), dtype=torch.int64).tolist())\n        # torch.randperm(n) Returns a random permutation of integers from 0 to n - 1. \n        # https:\/\/pytorch.org\/docs\/1.1.0\/torch.html#torch.randperm\n        return iter(torch.randperm(n).tolist())\n\n    def __len__(self):\n        return self.num_samples\n\n\nclass SubsetRandomSampler(Sampler):\n    r\"\"\"Samples elements randomly from a given list of indices, without replacement.\n    Arguments:\n        indices (sequence): a sequence of indices\n        \n    \u8fd9\u91cc\u662f\u5bf9\u4e8e\u539f\u6709\u7d22\u5f15\u5e8f\u5217\u53d6\u51fa\u4e00\u4e2a\u5b50\u96c6\n    \"\"\"\n\n    def __init__(self, indices):\n        self.indices = indices\n\n    def __iter__(self):\n        return (self.indices[i] for i in torch.randperm(len(self.indices)))\n\n    def __len__(self):\n        return len(self.indices)\n\n\nclass WeightedRandomSampler(Sampler):\n    r\"\"\"Samples elements from [0,..,len(weights)-1] with given probabilities (weights).\n    Args:\n        weights (sequence)   : a sequence of weights, not necessary summing up to one\n        num_samples (int): number of samples to draw\n        replacement (bool): if ``True``, samples are drawn with replacement.\n            If not, they are drawn without replacement, which means that when a\n            sample index is drawn for a row, it cannot be drawn again for that row.\n            \u4e3aTrue\u7684\u65f6\u5019\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a\u6709\u653e\u56de\u62bd\u53d6\uff0cFalse\u53ef\u4ee5\u7406\u89e3\u4e3a\u65e0\u653e\u56de\u62bd\u53d6\n    Example:\n        >>> list(WeightedRandomSampler([0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True))\n        [0, 0, 0, 1, 0]\n        >>> list(WeightedRandomSampler([0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False))\n        [0, 1, 4, 3, 2]\n        \n    \u8fd9\u91cc\u6839\u636e\u5bf9\u5e94\u7684\u6982\u7387\u6765\u91c7\u6837\u6837\u672c\uff0c\u786e\u5b9a\u7d22\u5f15\u8fed\u4ee3\u5668\n    \"\"\"\n\n    def __init__(self, weights, num_samples, replacement=True):\n        if not isinstance(num_samples, _int_classes) or isinstance(num_samples, bool) or \\\n                num_samples <= 0:\n            raise ValueError(\"num_samples should be a positive integer \"\n                             \"value, but got num_samples={}\".format(num_samples))\n        if not isinstance(replacement, bool):\n            raise ValueError(\"replacement should be a boolean value, but got \"\n                             \"replacement={}\".format(replacement))\n        self.weights = torch.as_tensor(weights, dtype=torch.double)\n        self.num_samples = num_samples\n        self.replacement = replacement\n\n    def __iter__(self):\n        # torch.multinomial\u591a\u9879\u5f0f\u5206\u5e03\u6839\u636e\u6743\u91cd\u8fdb\u884c\u91c7\u6837\uff1ahttps:\/\/baike.baidu.com\/item\/%E5%A4%9A%E9%A1%B9%E5%88%86%E5%B8%83\n        # https:\/\/pytorch.org\/docs\/1.1.0\/torch.html#torch.multinomial\n        return iter(torch.multinomial(self.weights, self.num_samples, self.replacement).tolist())\n\n    def __len__(self):\n        return self.num_samples\n\n    \n# BatchSampler \u662f\u57fa\u4e8e Sampler \u6765\u6784\u9020\u7684\uff1a BatchSampler = Sampler + BatchSize\nclass BatchSampler(Sampler):\n    r\"\"\"Wraps another sampler to yield a mini-batch of indices.\n    Args:\n        sampler (Sampler): Base sampler.\n        batch_size (int): Size of mini-batch.\n        drop_last (bool): If ``True``, the sampler will drop the last batch if\n            its size would be less than ``batch_size``\n    Example:\n        >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n        >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n        \n    \n    \"\"\"\n\n    def __init__(self, sampler, batch_size, drop_last):\n        if not isinstance(sampler, Sampler):\n            raise ValueError(\"sampler should be an instance of \"\n                             \"torch.utils.data.Sampler, but got sampler={}\"\n                             .format(sampler))\n        if not isinstance(batch_size, _int_classes) or isinstance(batch_size, bool) or \\\n                batch_size <= 0:\n            raise ValueError(\"batch_size should be a positive integer value, \"\n                             \"but got batch_size={}\".format(batch_size))\n        if not isinstance(drop_last, bool):\n            raise ValueError(\"drop_last should be a boolean value, but got \"\n                             \"drop_last={}\".format(drop_last))\n        self.sampler = sampler\n        self.batch_size = batch_size\n        self.drop_last = drop_last\n\n    def __iter__(self):\n        # \u8fd9\u91cc\u4f7f\u7528yield\u751f\u6210\u6700\u7ec8\u7684\u8fed\u4ee3batch\n        batch = []\n        for idx in self.sampler:\n            batch.append(idx)\n            if len(batch) == self.batch_size:\n                yield batch\n                batch = []\n        # \u8fd9\u91cc\u5224\u65ad\u4e86\u6700\u540e\u4e00\u4e2a\u53ef\u80fd\u5b58\u5728\u7684\u4e0d\u5b8c\u6574\u7684batch\n        if len(batch) > 0 and not self.drop_last:\n            yield batch\n\n    def __len__(self):\n        if self.drop_last:\n            # \u820d\u5f03\u6700\u540e\u4e00\u4e2a\u4e0d\u5b8c\u6574\u7684batch\uff0c\u5411\u4e0b\u53d6\u6574\n            return len(self.sampler) \/\/ self.batch_size\n        else:\n            # \u82e5\u80fd\u6574\u9664\uff0c\u5219self.batch_size-1\u6574\u9664\u540e\u6ca1\u6709\u5f71\u54cd\uff0c\u56e0\u4e3a\u7ed3\u679c\u4e3a0\n            # \u82e5\u662f\u4e0d\u80fd\u6574\u9664\uff0c\u5219len(self.sampler)\u5fc5\u7136\u8981\u6bd4self.batch_size\u7684\u6574\u6570\u500d\u591a\u51fa[1, self.batch_size-1]\u7684\u8fd9\u4e2a\u95ed\u533a\u95f4\u8303\u56f4\u7684\u503c\uff0c\n            # \u6240\u4ee5\u518d\u52a0\u4e0a\u4e00\u4e2a\u8be5\u8303\u56f4\u6700\u5927\u7684\u503cself.batch_size-1\u5fc5\u5b9a\u4f1a\u4f4d\u4e8e[len(self.sampler), (len(self.sampler)\/\/self.batch_size+1)*self.batch_size]\u8be5\u533a\u95f4\u5185\uff0c\u7ed3\u679c\u6b63\u597d\u591a\u51fa\u6765\u4e00\u4e2a\u9700\u8981\u7684(+1)\n            return (len(self.sampler) + self.batch_size - 1) \/\/ self.batch_size\n```","021e3573":"## \u65b9\u6cd5\u4e09\uff1a\u6539\u9020DataLoader\n\n\u8fd9\u4e00\u70b9\u4e3b\u8981\u53d7\u5230\u4e86[mean-teacher](https:\/\/github.com\/CuriousAI\/mean-teacher\/blob\/master\/pytorch\/mean_teacher\/data.py#L105-L132)\u7684\u542f\u53d1\u3002\n\n```python\nclass TwoStreamBatchSampler(Sampler):\n    \"\"\"Iterate two sets of indices\n    An 'epoch' is one iteration through the primary indices.\n    During the epoch, the secondary indices are iterated through\n    as many times as needed.\n    \"\"\"\n    def __init__(self, primary_indices, secondary_indices, batch_size, secondary_batch_size):\n        self.primary_indices = primary_indices\n        self.secondary_indices = secondary_indices\n        self.secondary_batch_size = secondary_batch_size\n        self.primary_batch_size = batch_size - secondary_batch_size\n\n        assert len(self.primary_indices) >= self.primary_batch_size > 0\n        assert len(self.secondary_indices) >= self.secondary_batch_size > 0\n\n    def __iter__(self):\n        primary_iter = iterate_once(self.primary_indices)\n        secondary_iter = iterate_eternally(self.secondary_indices)\n        return (\n            primary_batch + secondary_batch\n            for (primary_batch, secondary_batch)\n            in  zip(grouper(primary_iter, self.primary_batch_size),\n                    grouper(secondary_iter, self.secondary_batch_size))\n        )\n\n    def __len__(self):\n        return len(self.primary_indices) \/\/ self.primary_batch_size\n```\n\n\u8c03\u7528\u7684\u65f6\u5019\uff1a\n\n```python\n    dataset = torchvision.datasets.ImageFolder(traindir, train_transformation)\n\n    if args.labels:\n        with open(args.labels) as f:\n            labels = dict(line.split(' ') for line in f.read().splitlines())\n        labeled_idxs, unlabeled_idxs = data.relabel_dataset(dataset, labels)\n\n    if args.exclude_unlabeled:\n        sampler = SubsetRandomSampler(labeled_idxs)\n        batch_sampler = BatchSampler(sampler, args.batch_size, drop_last=True)\n    elif args.labeled_batch_size:\n        batch_sampler = data.TwoStreamBatchSampler(\n            unlabeled_idxs, labeled_idxs, args.batch_size, args.labeled_batch_size)\n    else:\n        assert False, \"labeled batch size {}\".format(args.labeled_batch_size)\n\n    train_loader = torch.utils.data.DataLoader(dataset,\n                                               batch_sampler=batch_sampler,\n                                               num_workers=args.workers,\n                                               pin_memory=True)\n```","65f1445a":"### \u8865\u5145\n\n\u4e0a\u9762\u7684\u64cd\u4f5c\u4e2d\uff0c\u4e5f\u53ef\u4ee5\u8003\u8651\u5c06`img_unlabeled`\u548c`img_labeled`\u76f4\u63a5\u6309\u7167\u6bd4\u4f8b\u653e\u5230\u4e00\u8d77\uff0c\u800c\u771f\u503c\u90e8\u5206\u4ec5\u662f\u8fd4\u56de`gt_labeled`\uff0c\u540c\u65f6`img_unlabeled_name`\u548c`img_labeled_name`\u4e00\u8d77\u8fd4\u56de\uff0c\u4e0b\u9762\u662f\u4f8b\u5b50\uff1a            \n\n```python\n    def __getitem__(self, index):\n        # \u8fd9\u91cc\u4e00\u6b21\u6027\u8bfb\u53d6\u6700\u7b80\u5316\u6bd4\u4f8b\u6570\u91cf\u7684\u6837\u672c\uff0c\u6240\u6709\u7684\u6837\u672c\u9700\u8981\u5355\u72ec\u5904\u7406\n        total_img, labeled_gt, total_name = [], [], []\n        \n        img_labeled_path, gt_labeled_path = self.imgs_labeled[index]  # 0, 1 => 850\n        img_labeled = Image.open(img_labeled_path).convert('RGB')\n        img_labeled_name = (img_labeled_path.split(os.sep)[-1]).split('.')[0]\n\n        gt_labeled = Image.open(gt_labeled_path).convert('L')\n        back_gt_labeled = gt_labeled  # \u7528\u4e8e\u65e0\u6807\u7b7e\u6570\u636e\u4f7f\u7528\u8054\u5408\u8c03\u6574\u51fd\u6570\u7684\u65f6\u5019\u4ee3\u66ff\u65e0\u6807\u7b7e\u6570\u636e\u771f\u503c\u8fdb\u884c\u5360\u4f4d\n        img_labeled, gt_labeled = self.train_joint_transform(img_labeled, gt_labeled)\n        img_labeled = self.train_img_transform(img_labeled)\n        gt_labeled = self.train_gt_transform(gt_labeled)\n        if self.use_bigt:\n            gt_labeled = gt_labeled.ge(0.5).float()  # \u4e8c\u503c\u5316\n        total_img.append(img_labeled)\n        labeled_gt.append(gt_labeled)\n        total_name.append(img_labeled_name)\n        \n        for idx_periter in range(self.r_l_rate):\n            # \u8fd9\u91cc\u4e0d\u518d\u4f7f\u7528\u771f\u503c\uff0c\u76f4\u63a5\u4f7f\u7528`_`\u63a5\u6536\n            img_unlabeled_path, _ = self.imgs_unlabeled[index\/\/self.r_l_rate+idx_periter]  # 0, 1, 2, 3 => 3*850\n            img_unlabeled = Image.open(img_unlabeled_path).convert('RGB')\n            img_unlabeled_name = (img_unlabeled_path.split(os.sep)[-1]).split('.')[0]\n\n            img_unlabeled, _ = self.train_joint_transform(img_unlabeled, back_gt_labeled)  # \u8fd9\u91cc\u4e3a\u4e86\u4f7f\u7528\u90a3\u4e2a\u8054\u5408\u8c03\u6574\u7684\u8f6c\u6362\u7c7b\uff0c\u4f7f\u7528\u4e0a\u9762\u7684target\u8fdb\u884c\u66ff\u4ee3\uff0c\u4f46\u662f\u8981\u6ce8\u610f\uff0c\u4e0d\u8981\u518d\u8fd4\u56de\u4e86\n            img_unlabeled = self.train_img_transform(img_unlabeled)\n                        \n            total_img.append(img_unlabeled)\n            total_name.append(img_unlabeled_name)\n\n        return total_img, labeled_gt, total_name  # \u8f93\u51fa\u540d\u5b57\u65b9\u4fbf\u6bd4\u8f83\n```\n\n\u8fd9\u6837\u5728\u8fd4\u56de\u4e4b\u540e\u53ea\u9700\u8981\u5bf9\u6570\u636e\u8fdb\u884c\u5206\u5272\u540e\u5904\u7406\u5373\u53ef\uff0c\u4f46\u662f\u8fd9\u91cc\u7684**\u5206\u5272**\u9700\u8981\u6309\u7167\u95f4\u9694\u5206\u5272\uff0c\u5e76\u4e0d\u65b9\u4fbf\u3002","110bdfe7":"### `collate_fn`\n\n\u53c2\u8003\u8d44\u6599\uff1a\n\n- https:\/\/jdhao.github.io\/2017\/10\/23\/pytorch-load-data-and-make-batch\/#loading-variable-size-input-images\n- https:\/\/www.cnblogs.com\/king-lps\/p\/10990304.html\n\n\u67e5\u770b\u6e90\u4ee3\u7801(https:\/\/github.com\/pytorch\/pytorch\/blob\/v1.1.0\/torch\/utils\/data\/_utils\/collate.py#L31)\uff1a\n\n```python\ndef default_collate(batch):\n    r\"\"\"Puts each data field into a tensor with outer dimension batch size\n    \n    \u4e00\u822c\u662f\u8f93\u5165\u7684batch\u4e2d\u7b2c\u4e00\u4f4d\u4e3a\u56fe\u50cf\uff0c\u7b2c\u4e8c\u4f4d\u4e3a\u6807\u7b7e\uff0c\u6240\u4ee5\u8fd9\u91cc\u76f4\u63a5\u5224\u65ad\u7b2c\u4e00\u4f4d\u7684\u7c7b\u578b\u3002\u7b2c\u4e8c\u4f4d\u4e0a\u4e5f\u9700\u8981\u8003\u8651\u662f\u5426\u53ef\u4ee5\u88abstack\uff0c\n    \u5bf9\u4e8e\u5206\u5272\u4efb\u52a1\u800c\u8a00\uff0c\u771f\u503c\u4e5f\u662f\u56fe\u7247\uff0c\u6240\u4ee5\u4e5f\u5f97\u4fdd\u8bc1\u56fe\u7247\u6709\u7740\u76f8\u540c\u7684\u5927\u5c0f\n    \u5c06batch\u4e2d\u7684\u6570\u636e\u8fdb\u884c\u6574\u7406\uff0c\u5c06\u4e00\u7cfb\u5217\u56fe\u50cf\u548c\u76ee\u6807\u6253\u5305\u4e3a\u5f20\u91cf(\u5f20\u91cf\u7684\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u4e3a\u6279\u5927\u5c0f)\u3002\n    \n    The default `collate_fn` expects all the images in a batch to have the same size \n    because it uses `torch.stack()` to pack the images. If the images provided by \n    Dataset have variable size, you have to provide your custom `collate_fn`.\n    \"\"\"\n\n    elem_type = type(batch[0])\n    if isinstance(batch[0], torch.Tensor):\n        out = None\n        if _use_shared_memory:\n            # If we're in a background process, concatenate directly into a\n            # shared memory tensor to avoid an extra copy\n            numel = sum([x.numel() for x in batch])\n            storage = batch[0].storage()._new_shared(numel)\n            out = batch[0].new(storage)\n        return torch.stack(batch, 0, out=out)\n    \n    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n            and elem_type.__name__ != 'string_':\n        elem = batch[0]\n        if elem_type.__name__ == 'ndarray':\n            # array of string classes and object\n            if np_str_obj_array_pattern.search(elem.dtype.str) is not None:\n                raise TypeError(error_msg_fmt.format(elem.dtype))\n\n            return default_collate([torch.from_numpy(b) for b in batch])\n        if elem.shape == ():  # scalars\n            py_type = float if elem.dtype.name.startswith('float') else int\n            return numpy_type_map[elem.dtype.name](list(map(py_type, batch)))\n        \n    elif isinstance(batch[0], float):\n        return torch.tensor(batch, dtype=torch.float64)\n    elif isinstance(batch[0], int_classes):\n        return torch.tensor(batch)\n    elif isinstance(batch[0], string_classes):\n        return batch\n    \n    elif isinstance(batch[0], container_abcs.Mapping):\n        return {key: default_collate([d[key] for d in batch]) for key in batch[0]}\n    elif isinstance(batch[0], tuple) and hasattr(batch[0], '_fields'):  # namedtuple\n        return type(batch[0])(*(default_collate(samples) for samples in zip(*batch)))\n    elif isinstance(batch[0], container_abcs.Sequence):\n        transposed = zip(*batch)\n        return [default_collate(samples) for samples in transposed]\n\n    raise TypeError((error_msg_fmt.format(type(batch[0]))))\n```\n\n\u8fd9\u91cc\u4e2a\u6839\u636e\u8f93\u5165\u7684\u7c7b\u578b\u6765\u5b9e\u73b0\u5bf9\u4e8e\u4e0d\u540c\u7c7b\u522b\u7684\u6570\u636e\u7684\u8fd4\u56de\u4e0e\u5212\u5206\u3002\u53ef\u89c1\u6709\u51e0\u5904\u4f7f\u7528\u4e86\u9012\u5f52\u7684\u64cd\u4f5c\u91cd\u590d\u7528\u4e86\u8be5\u51fd\u6570\u3002"}}