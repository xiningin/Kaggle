{"cell_type":{"4b3a7252":"code","84eb342d":"code","f4158ab7":"code","e73e95af":"code","7d6c56c5":"code","6497d2a1":"code","c2296a9d":"code","9f8a6cf5":"code","99b24725":"code","668d2a22":"code","5845a395":"code","ee520a2e":"code","0de56b91":"code","e99d9811":"code","7087c964":"code","49b8086d":"code","26412fab":"code","c8997ecf":"code","ce276c14":"code","4403c951":"code","4de8c401":"code","2e0c09cd":"code","7457c67b":"code","bb0549fe":"code","2086f66a":"code","7086abc8":"code","95d04f73":"code","5cd75e9f":"code","4d07fd3c":"code","12c316ed":"code","1f2dba97":"code","f5e6369d":"code","c7569b21":"markdown","3fe4ec93":"markdown","79f68f64":"markdown","0c0a9715":"markdown","aa740473":"markdown","bcd4671e":"markdown","75ff1bec":"markdown","daa590c9":"markdown","7e83d462":"markdown","13a2e74f":"markdown","9c8c43ac":"markdown","99eb29f2":"markdown","336fba03":"markdown","46e42e7a":"markdown","dc8a46be":"markdown","f6ecb1df":"markdown","6d275a09":"markdown","644a7035":"markdown","4adabc74":"markdown","125bd991":"markdown","1f33bde6":"markdown","27922153":"markdown","a6986791":"markdown","09f75b0c":"markdown"},"source":{"4b3a7252":"#Basic Modules\nfrom datetime import datetime\nimport pandas as pd\nimport numpy as np\n\n#Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#PreProcessing\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import preprocessing\nimport category_encoders as ce\n\n#Model Building\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import Lasso, Ridge, ElasticNet\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n#Optional Modules\nfrom IPython.display import display\npd.options.display.max_columns = None\nimport warnings\nwarnings.filterwarnings(\"ignore\")","84eb342d":"Sample = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/train.csv\",index_col = 'Id')\nTest = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/test.csv\",index_col = 'Id')","f4158ab7":"Sample.shape","e73e95af":"Sample.head()","7d6c56c5":"Tgt_Col = 'SalePrice'\nNum_Col = Sample.select_dtypes(exclude='object').drop(Tgt_Col,axis=1).columns\nCat_Col = Sample.select_dtypes(include='object').columns\n\nprint(\"Numerical Columns : \" , len(Num_Col))\nprint(\"Categorical Columns : \" , len(Cat_Col))","6497d2a1":"sns.distplot(Sample[Tgt_Col])\nplt.ticklabel_format(style='plain', axis='y')\nplt.title(\"SalePrice's Distribution\")\nplt.show()\n\nprint('Skewness : ' , str(Sample[Tgt_Col].skew()))","c2296a9d":"sns.distplot(np.log(Sample[Tgt_Col]+1))\nplt.ticklabel_format(style='plain', axis='y')\nplt.title(\"SalePrice's Distribution\")\nplt.show()\n\nprint('Skewness : ' , str(np.log(Sample[Tgt_Col]+1).skew()))","9f8a6cf5":"Sample[Num_Col].describe().round(decimals=2)","99b24725":"fig = plt.figure(figsize=(12,18))\nfor idx,col in enumerate(Num_Col):\n    fig.add_subplot(9,4,idx+1)\n    sns.distplot(Sample[col].dropna(), kde_kws={'bw':0.1})\n    plt.xlabel(col)\nplt.tight_layout()\nplt.show()","668d2a22":"cor = Sample.corr()\n\nimport matplotlib.style as style\nstyle.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (30,20))\n\nmask = np.zeros_like(cor, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(cor, cmap=sns.diverging_palette(8, 150, n=10),mask = mask, annot = True,vmin=-1,vmax=1);\nplt.title(\"Heatmap of all the Features\", fontsize = 30);","5845a395":"fig = plt.figure(figsize=(12,18))\nfor idx,col in enumerate(Num_Col):\n    fig.add_subplot(9,4,idx+1)\n    if abs(cor.iloc[-1,idx])<0.1:\n        sns.scatterplot(Sample[col],Sample[Tgt_Col],color='red')\n    elif abs(cor.iloc[-1,idx])>=0.5:\n        sns.scatterplot(Sample[col],Sample[Tgt_Col],color='green')\n    else:\n        sns.scatterplot(Sample[col],Sample[Tgt_Col],color='blue')\n    plt.title(\"Corr to SalePrice : \" + (np.round(cor.iloc[-1,idx],decimals=2)).astype(str))\nplt.tight_layout()\nplt.show()","ee520a2e":"Sample[Num_Col].isna().sum().sort_values(ascending=False).head()","0de56b91":"Sample[Cat_Col].describe()","e99d9811":"for col in Sample[Cat_Col]:\n    if Sample[col].isnull().sum() > 0 :\n        print (col , \" : \", Sample[col].isnull().sum() , Sample[col].unique())","7087c964":"Sample_copy = Sample.copy()\n\nSample_copy['MasVnrArea'] = Sample['MasVnrArea'].fillna(0)\n\nCat_Cols_Fill_NA = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'MasVnrType',\n                      'BsmtFinType2', 'BsmtExposure', 'BsmtFinType1', 'BsmtQual', 'BsmtCond',\n                      'GarageQual', 'GarageFinish', 'GarageType','GarageCond']\n\nfor cat in Cat_Cols_Fill_NA:\n    Sample_copy[cat] = Sample_copy[cat].fillna(\"NA\")","49b8086d":"Sample_copy.isna().sum().sort_values(ascending = False).head()","26412fab":"Sample_copy = Sample_copy.drop(Sample_copy[Sample_copy['LotFrontage']>200].index)\nSample_copy = Sample_copy.drop(Sample_copy[Sample_copy['LotArea']>100000].index)\nSample_copy = Sample_copy.drop(Sample_copy[Sample_copy['MasVnrArea']>1200].index)\nSample_copy = Sample_copy.drop(Sample_copy[Sample_copy['BsmtFinSF1']>4000].index)\nSample_copy = Sample_copy.drop(Sample_copy[Sample_copy['TotalBsmtSF']>4000].index)\nSample_copy = Sample_copy.drop(Sample_copy[(Sample_copy['GrLivArea']>4000) & (Sample_copy[Tgt_Col]<300000)].index)\nSample_copy = Sample_copy.drop(Sample_copy[Sample_copy['BsmtFinSF2']>1300].index)\nSample_copy = Sample_copy.drop(Sample_copy[Sample_copy['1stFlrSF']>4000].index)\nSample_copy = Sample_copy.drop(Sample_copy[Sample_copy['EnclosedPorch']>500].index)\nSample_copy = Sample_copy.drop(Sample_copy[Sample_copy['MiscVal']>5000].index)\nSample_copy = Sample_copy.drop(Sample_copy[(Sample_copy['LowQualFinSF']>600) & (Sample_copy[Tgt_Col]>400000)].index)","c8997ecf":"Sample_copy[Tgt_Col] = np.log(Sample_copy[Tgt_Col]+1)\nSample_copy = Sample_copy.rename(columns={'SalePrice': 'SalePriceLog'})\nTgt_features = 'SalePriceLog'","ce276c14":"Sample_copy['MSSubClass'] = Sample_copy['MSSubClass'].astype(str)\nSample_copy['OverallQual'] = Sample_copy['OverallQual'].astype(str)\nSample_copy['OverallCond'] = Sample_copy['OverallCond'].astype(str)","4403c951":"Num_features = Sample_copy.select_dtypes(exclude='object').drop(Tgt_features,axis=1).columns\nCat_features = Sample_copy.select_dtypes(include='object').columns","4de8c401":"cor = Sample_copy.corr()\ncor_list = cor.abs().unstack()\ncor_list[cor_list>0.75].sort_values(ascending=False)[34:].drop_duplicates()","2e0c09cd":"Collinear = ['GarageArea','TotRmsAbvGrd','GarageYrBlt','1stFlrSF']","7457c67b":"Low_Corr = []\n\nfor idx,col in enumerate(Num_features):\n    if abs(cor.iloc[-1,idx])<=0.1:\n        Low_Corr.append(col)","bb0549fe":"features_drop = ['SalePriceLog'] + Low_Corr + Collinear\n\nX = Sample_copy.drop(features_drop, axis=1)\ny = Sample_copy[Tgt_features]","2086f66a":"numeric_features = X.select_dtypes(exclude='object').columns\ncategorical_features = X.select_dtypes(include='object').columns\n\nskewed_feats = X[numeric_features].apply(lambda x: x.skew())\nhigh_skew = skewed_feats[skewed_feats > 0.5]\nskew_features = high_skew.index\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)","7086abc8":"RobustScaler = preprocessing.RobustScaler\nPowerTransformer = preprocessing.PowerTransformer\n\nmodel_list = {'RF_Model' : RandomForestRegressor(random_state=5),\n              'XGB_Model' : XGBRegressor(objective ='reg:squarederror',n_estimators=1000, learning_rate=0.05),\n              'Lasso_Model' : Lasso(alpha=0.0005), \n              'Ridge_Model' : Ridge(alpha=0.002), \n              'Elastic_Net_Model' : ElasticNet(alpha=0.02, random_state=5, l1_ratio=0.7), \n              'GBR_Model' : GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=4, random_state=5)}\n\ndef inv_y(transformed_y):\n    return np.exp(transformed_y)","95d04f73":"skew_transformer = Pipeline(steps=[('imputer', SimpleImputer()),\n                                   ('scaler', PowerTransformer())])\n\nnumeric_transformer = Pipeline(steps=[('imputer', SimpleImputer())])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer1', SimpleImputer(strategy='constant', fill_value='NA')),\n    ('encoder', ce.one_hot.OneHotEncoder()),\n    ('imputer2', SimpleImputer())])\n    \npreprocessor = ColumnTransformer(\n    transformers=[('skw', skew_transformer, skew_features),\n                  ('num', numeric_transformer, numeric_features),\n                  ('cat', categorical_transformer, categorical_features)])","5cd75e9f":"model_score = pd.Series()\n    \nfor key in model_list.keys():\n    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                           ('scaler', RobustScaler()),\n                           ('model', model_list[key])])\n    \n    model = pipe.fit(train_X, train_y)\n    \n    pred_y = model.predict(val_X)\n    val_mae = mean_absolute_error(inv_y(pred_y), inv_y(val_y))\n    model_score[key] = val_mae\n    \ntop_2_model = model_score.nsmallest(n=2)\nprint(top_2_model)","4d07fd3c":"from sklearn.model_selection import cross_val_score\n\nimputed_X = preprocessor.fit_transform(X,y)\nn_folds = 10\n\nfor model in top_2_model.index:\n    scores = cross_val_score(model_list[model], imputed_X, y, scoring='neg_mean_squared_error', \n                             cv=n_folds)\n    mae_scores = np.sqrt(-scores)\n\n    print(model + ':')\n    print('Mean RMSE = ' + str(mae_scores.mean().round(decimals=3)))\n    print('Error std deviation = ' + str(mae_scores.std().round(decimals=3)) + '\\n')","12c316ed":"param_grid = [{'alpha': [0.001, 0.0005, 0.0001]}]\ntop_reg = Lasso()\n\ngrid_search = GridSearchCV(top_reg, param_grid, cv=5, \n                           scoring='neg_mean_squared_error')\n\ngrid_search.fit(imputed_X, y)\n\ngrid_search.best_params_","1f2dba97":"test_X = Test.copy()\n\ntest_X['MasVnrArea'] = test_X['MasVnrArea'].fillna(0)\n\ncat_cols_fill_na = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'MasVnrType',\n                      'BsmtFinType2', 'BsmtExposure', 'BsmtFinType1', 'BsmtQual', 'BsmtCond',\n                      'GarageQual', 'GarageFinish', 'GarageType','GarageCond']\n\nfor cat in cat_cols_fill_na:\n    test_X[cat] = test_X[cat].fillna(\"NA\")\n    \ntest_X['MSSubClass'] = test_X['MSSubClass'].astype(str)\ntest_X['OverallQual'] = test_X['OverallQual'].astype(str)\ntest_X['OverallCond'] = test_X['OverallCond'].astype(str)\n\nif 'SalePriceLog' in features_drop:\n    features_drop.remove('SalePriceLog')\n\ntest_X = test_X.drop(features_drop, axis=1)","f5e6369d":"final_model = Lasso(alpha=0.0005, random_state=5)\n\npipe = Pipeline(steps=[('preprocessor', preprocessor),\n                       ('scaler', RobustScaler()),\n                       ('model', final_model)])\n\nmodel = pipe.fit(X, y)\n\ntest_preds = model.predict(test_X)\n\noutput = pd.DataFrame({'Id': Test.index,\n                       'SalePrice': inv_y(test_preds)})\n\noutput.to_csv(str(datetime.now().strftime('%Y%m%d_%H%M%S')) + '.csv', index=False)","c7569b21":"# 1. Exploratory Data Analysis (EDA)","3fe4ec93":"# 3. Feature Selection and Engineering\nHighly correlated features needed to find out and exclude one of it. <br>\nNumerical features with low correlations with target variable should be excluded. <br>\nRedefine highly skewed, numeric and categorical features.","79f68f64":"### 1.4 Categorical Features","0c0a9715":"### 2.2 Transform Target and Redefine Features\nTarget variable is log transformed to reduce skewness. <br>\nThe remaining missing values will be imputed later. <br>\nRedefine numerical and categorical features.","aa740473":"# 4. Machine Learning Algorithm","bcd4671e":"# 2. Data Cleaning and Preprocessing","75ff1bec":"# Read input file","daa590c9":"I am someone who is not from data science background but have keen on it. Therefore, I started my self learning path by learning through any platform available across the internet since early 2020. This is the my first experience to run through a quite complete data science process from Analysing Data till Making Prediction. I believe that there are still a lot of improvement can be made for my model and is open to recieve any advises and comments. Do leave your advises or comments, really appreciate it.","7e83d462":"Information get:\n1. There are some columns which are actually categorical variables.\n2. From histogram, there are some features are skewed.\n2. From heatmap, there are few features are highly correlated to each others.\n3. From scatter plot, there are few certain outliers.\n4. From scatter plot, those low correlated features.\n5. There are some columns having missing values.","13a2e74f":"Information get:\n1. Target column is skewed which might affect the accuracy of predictive models.\n2. Log transformation on target column can effectively reduce the skewness.","9c8c43ac":"# 6. Reference\n\n[House Prices: 1st Approach to data science process](https:\/\/www.kaggle.com\/cheesu\/house-prices-1st-approach-to-data-science-process\/notebook) by [Chee Su Goh](https:\/\/www.kaggle.com\/cheesu)","99eb29f2":"### 1.2 Analyse Target Variable","336fba03":"Information obtain:\n1. There are 1460 observations in this sample data\n2. There are 80 attributes (79 features and 1 target).\n3. Among the 79 features, there are 36 are numeric and 43 are categorical","46e42e7a":"### 4.2 Evaluate Each Model and Cross Validate Top Model\nLoop through all the models to get the each model's predictive performance. <br>\nCross validate the top 2 models which give the best performance. <br>","dc8a46be":"### 4.1 Setup\nDefine the scaler to standardize the data. <br>\nDefine reverse log function to inverse transform the log(saleprice). <br>\nDefine all the models to try for predict the saleprice. <br>\nSetup preprocessing process using pipeline.","f6ecb1df":"# 5. Making Prediction","6d275a09":"### 5.1 Pre-processing Test Data\nRepeat the manual preprocessing defined before on test data.","644a7035":"### 2.1 Missing Values and Outliers\nFill NA to those categorical columns with missing values. <br>\nFor MasVnrArea, it should be 0 since the MasVnrType is NA. <br>\nThere is 3 columns left with missing values, will handle it with imputer later. <br>\nDrop those columns which are considered as outliers.","4adabc74":"### 1.1 Simple understanding about the data","125bd991":"### 1.3 Numerical Columns","1f33bde6":"Information get:\n1. There are columns having missing values.\n2. These missing value are actually means Not Available(NA) after looking at the data description.\n4. Only the missing value for Electrical feature does not means NA.","27922153":"### 5.2 Create Final Model and Predict\nDefine the model using the best hyperparameter and predict it.","a6986791":"# Import Modules","09f75b0c":"### 4.3 Choose Algorithm and Fine Tune\nDecide the best model by considering the MAE score and RMSE score. <br>\nSearch for best combination of hyperparameter for the model."}}