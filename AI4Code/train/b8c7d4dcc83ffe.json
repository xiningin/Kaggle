{"cell_type":{"e9954d97":"code","fbaf303c":"code","7020f862":"code","7c1fd90c":"code","38055a96":"code","4eda8f65":"code","dd05a65f":"code","0e5d5f31":"code","de490a7e":"code","3f9487e2":"code","66265beb":"code","70c513ef":"code","a0d06fc9":"code","f8117f87":"code","334dfad8":"code","cd03d1ee":"code","a9e67898":"code","fcb24ad6":"code","2ef22032":"code","fe8f2d54":"code","ebccfaf1":"code","b12d6013":"code","d9716af7":"code","03cdd526":"code","55222ba0":"code","8d98e70a":"code","cdffc519":"code","1bd6d940":"code","11be085b":"code","65f84c21":"code","307e3482":"code","0fc5cd3c":"code","eac92b2c":"code","0d91d088":"code","c7e18078":"code","feafd1f9":"code","a6e4f19f":"code","492e527a":"code","0cf735ea":"markdown","1e81a78d":"markdown","9fced700":"markdown","cc5102c5":"markdown","56591f89":"markdown","54e2d88f":"markdown","d4d586b1":"markdown","1ab78859":"markdown","1c32283d":"markdown","7594acff":"markdown","76c7ba55":"markdown","e8bc1720":"markdown","16669d8b":"markdown","775e03e9":"markdown","86fbb3a4":"markdown","6bbde39b":"markdown","282dbc30":"markdown","b3bd4353":"markdown","ba007abc":"markdown","ae70b201":"markdown","58cea8d0":"markdown","2a548e1f":"markdown"},"source":{"e9954d97":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fbaf303c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scikitplot as skplt\nimport seaborn as sns\n\nfrom distutils.version import LooseVersion as Version\nfrom sklearn import __version__ as sklearn_version\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.metrics import confusion_matrix\n\nif Version(sklearn_version)<'0.18':\n    from sklearn.cross_validation import train_test_split\nelse:\n    from sklearn.model_selection import train_test_split\n    \nif Version(sklearn_version)<'0.18':\n    from sklearn.grid_search import GridSearchCV\nelse:\n    from sklearn.model_selection import GridSearchCV\n\ndf=pd.read_csv('\/kaggle\/input\/wine-quality-from-chemical-properties\/winequality.csv')","7020f862":"df.head()","7c1fd90c":"df.info()","38055a96":"df['quality'].value_counts()","4eda8f65":"sns.set(style='whitegrid', context='notebook')\ncols1=['residual_sugar','chlorides','free_S_dioxide','total_S_dioxide','density','pH','sulphates','alcohol','quality']\nsns.pairplot(df[cols1])\nplt.show()","dd05a65f":"cols=['fixed_acidity','volatile_acidity','citric_acid','residual_sugar','chlorides','free_S_dioxide','total_S_dioxide','density','pH','sulphates']\ncm=np.corrcoef(df[cols].values.T)\nsns.set(font_scale=1.5)\nhm=sns.heatmap(cm,cbar=True,annot=True,square=True,fmt='.2f',annot_kws={'size':7},yticklabels=cols,xticklabels=cols)\nplt.show()","0e5d5f31":"sns.violinplot(data=df[cols],palette='Set3',inner='points')\nplt.xticks(rotation=90)\nplt.show()","de490a7e":"X=df[cols]\ny=df['quality']\n\nX_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.3, random_state=42)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)\nprint(y_train.shape)","3f9487e2":"clf1=LogisticRegression(penalty='l2',multi_class='auto', random_state=1,max_iter=250)\npipe1=Pipeline([('scl', StandardScaler()),\n               ('clf1', clf1)])\n    \nparameters1={'clf1__C': [0.001, 0.1, 1, 10, 100],\n            'clf1__solver': ['newton-cg', 'lbfgs']}\ngs1=GridSearchCV(estimator=pipe1, param_grid=parameters1, scoring='accuracy', cv=9, iid=True)\ngs1=gs1.fit(X_train, y_train)\ny_pred_clf1 = gs1.predict(X_test)\nskplt.metrics.plot_confusion_matrix(y_test, y_pred_clf1,normalize=True, figsize=(7,7))\nplt.show()\nprint('linear regresion accuracy score: %.3f' %gs1.best_score_)\nprint('Best parameters:')\nprint(gs1.best_params_)","66265beb":"clf2=DecisionTreeClassifier(criterion='entropy', random_state=0)\ngs2=GridSearchCV(estimator=clf2, param_grid=[\n        {'max_depth':[1,2,3,4,5,6,7,8,None]}], scoring='accuracy',cv=9,iid=True)\ngs2=gs2.fit(X_train, y_train)\ny_pred_clf2 = gs2.predict(X_test)\nskplt.metrics.plot_confusion_matrix(y_test, y_pred_clf2,normalize=True, figsize=(7,7))\nplt.show()\nprint('decision tree classifier accuracy: %.3f' %gs2.best_score_)\nprint('Best parameters:')\nprint(gs2.best_params_)","70c513ef":"clf3=KNeighborsClassifier(algorithm='auto')\npipe3=Pipeline([('scl', StandardScaler()),\n                ('clf3', clf3)])\nparameters3={'clf3__n_neighbors':[1,2,3,4,5,6,7,8,9,10],\n             'clf3__leaf_size':[1,5,10]}\ngs3=GridSearchCV(estimator=pipe3, param_grid=parameters3, scoring='accuracy', cv=9, iid=True)\ngs3=gs3.fit(X_train, y_train)\ny_pred_clf3 = gs3.predict(X_test)\nskplt.metrics.plot_confusion_matrix(y_test, y_pred_clf3,normalize=True, figsize=(7,7))\nplt.show()\nprint('KNeighbors classifier accuracy: %.3f'%gs3.best_score_)\nprint('Best parameters:')\nprint(gs3.best_params_)","a0d06fc9":"clf4=RandomForestClassifier(n_estimators=100, criterion='entropy')\nclf4.fit(X_train, y_train)\ny_pred_clf4 = clf4.predict(X_test)\nskplt.metrics.plot_confusion_matrix(y_test, y_pred_clf4,normalize=True, figsize=(7,7))\nplt.show()\n#print(clf4.feature_importances_)\nprint('Random forest classifier accuracy: %.3f' %clf4.score(X_test,y_test))\nprint('Features importances:')\nfor i, item in enumerate(clf4.feature_importances_): \n    print(cols[i]+': %.3f' %item)","f8117f87":"mv_clf=VotingClassifier(estimators=[('dt',gs2), ('kn', gs3), ('rf', clf4)], voting='hard', weights=[1,1,3])\nmv_clf.fit(X_train, y_train)\ny_pred_mv_clf = mv_clf.predict(X_test)\nskplt.metrics.plot_confusion_matrix(y_test, y_pred_mv_clf,normalize=True, figsize=(7,7))\nplt.show()\nprint('Voiting classifier accuracy: %.3f' %mv_clf.score(X_test,y_test))","334dfad8":"def logistic_regresion_classifier(X_train, y_train, X_test, y_test, cv):\n    clf=LogisticRegression(penalty='l2',multi_class='auto', random_state=1,max_iter=250)\n    pipe=Pipeline([('scl', StandardScaler()),\n               ('clf', clf)])\n    \n    parameters={'clf__C': [0.001, 0.1, 1, 10, 100],\n            'clf__solver': ['newton-cg', 'lbfgs']}\n    gs=GridSearchCV(estimator=pipe, param_grid=parameters, scoring='accuracy', cv=cv, iid=True)\n    gs=gs.fit(X_train, y_train)\n    y_pred_clf = gs.predict(X_test)\n    skplt.metrics.plot_confusion_matrix(y_test, y_pred_clf,normalize=True, figsize=(7,7))\n    plt.show()\n    print('linear regresion accuracy score: %.3f' %gs.best_score_)\n    ","cd03d1ee":"def decision_tree_classifier(X_train, y_train, X_test, y_test, cv):\n    clf=DecisionTreeClassifier(criterion='entropy', random_state=0)\n    gs=GridSearchCV(estimator=clf, param_grid=[\n        {'max_depth':[1,2,3,4,5,6,7,8,None]}], scoring='accuracy',cv=cv,iid=True)\n    gs=gs.fit(X_train, y_train)\n    y_pred_clf = gs.predict(X_test)\n    skplt.metrics.plot_confusion_matrix(y_test, y_pred_clf,normalize=True, figsize=(7,7))\n    plt.show()\n    print('decision tree classifier accuracy: %.3f' %gs.best_score_)\n    return clf","a9e67898":"def KNeighbors_classifier(X_train, y_train, X_test, y_test,cv):\n    clf=KNeighborsClassifier(algorithm='auto')\n    pipe=Pipeline([('scl', StandardScaler()),\n                ('clf', clf)])\n    parameters={'clf__n_neighbors':[1,2,3,4,5,6,7,8,9,10],\n             'clf__leaf_size':[1,5,10]}\n    gs=GridSearchCV(estimator=pipe, param_grid=parameters, scoring='accuracy', cv=cv, iid=True)\n    gs=gs.fit(X_train, y_train)\n    y_pred_clf = gs.predict(X_test)\n    skplt.metrics.plot_confusion_matrix(y_test, y_pred_clf,normalize=True, figsize=(7,7))\n    plt.show()\n    print('KNeighbors classifier accuracy: %.3f'%gs.best_score_)\n    return gs","fcb24ad6":"def Random_forest_classifier(X_train, y_train, X_test, y_test):\n    clf=RandomForestClassifier(n_estimators=300, criterion='entropy')\n    clf.fit(X_train, y_train)\n    print('Random forest classifier accuracy: %.3f' %clf.score(X_test,y_test))\n    y_pred_clf = clf.predict(X_test)\n    skplt.metrics.plot_confusion_matrix(y_test, y_pred_clf,normalize=True, figsize=(7,7))\n    plt.show()\n    return clf","2ef22032":"cols=['chlorides','total_S_dioxide','sulphates','alcohol']\nX1=df[cols]\ny1=df['quality']\n\nX_train1, X_test1, y_train1, y_test1=train_test_split(X1,y1,test_size=0.3, random_state=42)","fe8f2d54":"logistic_regresion_classifier(X_train1, y_train1, X_test1, y_test1, 9)","ebccfaf1":"gs21=decision_tree_classifier(X_train1, y_train1, X_test1, y_test1, 9)","b12d6013":"gs31=KNeighbors_classifier(X_train1, y_train1, X_test1, y_test1, 9)","d9716af7":"clf41=Random_forest_classifier(X_train1, y_train1, X_test1, y_test1)","03cdd526":"mv_clf1=VotingClassifier(estimators=[('dt1',gs21), ('kn1', gs31), ('rf1', clf41)], voting='hard')\nmv_clf1.fit(X_train1, y_train1)\ny_pred_mv_clf1 = mv_clf1.predict(X_test1)\nskplt.metrics.plot_confusion_matrix(y_test1, y_pred_mv_clf1,normalize=True, figsize=(7,7))\nplt.show()\nprint('voting classifier accuracy: %.3f' %mv_clf1.score(X_test1,y_test1))","55222ba0":"newdf=df.drop(df[df['total_S_dioxide']>200].index)","8d98e70a":"nX=newdf[cols]\nny=newdf['quality']\n\nnX_train, nX_test, ny_train, ny_test=train_test_split(nX,ny,test_size=0.3, random_state=42)","cdffc519":"logistic_regresion_classifier(nX_train, ny_train, nX_test, ny_test, 8)","1bd6d940":"ngs2=decision_tree_classifier(nX_train, ny_train, nX_test, ny_test, 8)","11be085b":"ngs3=KNeighbors_classifier(nX_train, ny_train, nX_test, ny_test, 8)","65f84c21":"nclf4=Random_forest_classifier(nX_train, ny_train, nX_test, ny_test)","307e3482":"nmv_clf=VotingClassifier(estimators=[('ndt',ngs2), ('nkn', ngs3), ('nrf', nclf4)], voting='hard', weights=[1,1,3])\nnmv_clf.fit(nX_train, ny_train)\nny_pred_mv_clf = nmv_clf.predict(nX_test)\nskplt.metrics.plot_confusion_matrix(ny_test, ny_pred_mv_clf,normalize=True, figsize=(7,7))\nplt.show()\nprint('Voiting classifier accuracy: %.3f' %nmv_clf.score(nX_test,ny_test))","0fc5cd3c":"dat=df.drop(df[df['quality']<5].index)\ndata=dat.drop(dat[dat['quality']>7].index)\ndata['quality'].value_counts()","eac92b2c":"newX=data[cols]\nnewy=data['quality']\n\nnewX_train, newX_test, newy_train, newy_test=train_test_split(newX,newy,test_size=0.3, random_state=42)","0d91d088":"logistic_regresion_classifier(newX_train, newy_train, newX_test, newy_test, 8)","c7e18078":"newgs2=decision_tree_classifier(newX_train, newy_train, newX_test, newy_test, 8)","feafd1f9":"newgs3=KNeighbors_classifier(newX_train, newy_train, newX_test, newy_test, 8)","a6e4f19f":"newclf4=Random_forest_classifier(newX_train, newy_train, newX_test, newy_test)","492e527a":"newmv_clf=VotingClassifier(estimators=[('newdt',newgs2), ('newkn', newgs3), ('newrf', newclf4)], voting='hard', weights=[1,1,3])\nnewmv_clf.fit(newX_train, newy_train)\nnewy_pred_mv_clf = newmv_clf.predict(newX_test)\nskplt.metrics.plot_confusion_matrix(newy_test, newy_pred_mv_clf,normalize=True, figsize=(7,7))\nplt.show()\nprint('Voiting classifier accuracy: %.3f' %newmv_clf.score(newX_test,newy_test))","0cf735ea":"[Section 1](#section1)\n\n[Section 2](#section2)\n\n[Section 3](#section3)\n\n[Section 4](#section4)\n\n[Section 5](#section5)","1e81a78d":"<a id='section2'>Section 2<\/a>","9fced700":"<a id='section4'>Section 4<\/a>","cc5102c5":"The same code for data with the biggest features importance\u2019s pointed from random forest classifier.","56591f89":"Majority voiting classifier","54e2d88f":"Here I use EDA methods on winequality datasets (Section 1). Then I train on it logistic regression classifier, random forest classifier, KNearest neighbors classifier, decision tree classifier and voting classifier (Section 2). Usually voting classifier improves prediction of week classifiers. Then I check if the data reduction will improve predictions. I do it in several ways. At first I use random forest classifiers attribute: feature_importances_ to choose the most important features (Section 3), then I reduce the outliers (Section 4) and at the end I treat data of wine quality with value of 3,4 and 8 as a outliers (Section 5).","d4d586b1":"Spliting data into train and test datasets.","1ab78859":"Violinplot is a nice tool to see the outliers. As one can see the total_S_dioxide feature may have two outliers (values over 200).","1c32283d":"Pairplot shows that there is no clear, linear dependency between the wine quality and the rest of features in dataset. \nMaybe some liner dependency you can see in wine quality and alcohol content ;)\nThe plot below shows nice correlation between features in datasets.","7594acff":"**Conclusions**\n\nThe samples of wine quality of 3, 4 and 8 have very poor predictions. However the best results gives Random forest classifier. When I treet samples of wine quality with values 3, 4 and 8 as a outliers the classifiers accuracy increases and the best result gives voting classiefier.","76c7ba55":"The winequality dataset consists of 12 features describing composition of wine, its physical\/chemical properties (i.e. pH, density) and its quality.","e8bc1720":"<a id='section3'>Section 3<\/a>","16669d8b":"Random forest classifier","775e03e9":"All data are numerical and there is no non-null values in data set. The datasets consists with little information of wine quality equal 3,4 and 8.","86fbb3a4":"Decision tree calssifier","6bbde39b":"<a id='section5'>Section 5<\/a>","282dbc30":"Because is going to be a lot writing the same code, I packed everything in functions ;)","b3bd4353":"Removing outliers","ba007abc":"Logistic regresion classifier","ae70b201":"KNeighbors calssifier","58cea8d0":"<a id='section1'>Section 1<\/a>","2a548e1f":"Making analysis only for data with wine qualiy equal 5,6,and 7. "}}