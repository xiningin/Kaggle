{"cell_type":{"32ba3e9c":"code","1b949850":"code","2758e1d4":"code","532610ef":"code","45220ef5":"code","c1bed507":"code","86a25ef7":"code","debbb428":"code","35212f7a":"code","2aff8f53":"code","a0d67063":"code","b363d549":"code","9c86c4e7":"code","24eee4dc":"code","2f50d74e":"code","8b8c0f7d":"code","2b8d88a9":"code","3051f4a4":"code","75b13e07":"code","495fe159":"code","3ee10215":"code","1ba0bd0c":"code","3b1d5502":"code","70285b46":"code","1543ebda":"code","4bbc5999":"code","282911ac":"code","1395d650":"code","aacacc25":"code","afe011e7":"code","cd48f191":"code","2565163c":"code","8ce4078d":"code","27313262":"code","8c4171dd":"code","f28ceb80":"code","8bc45b07":"code","8ea37372":"code","c42afbae":"code","dbaa6d64":"code","181532cf":"code","2e8ba99f":"code","6228b9c3":"code","f8eee2ba":"code","ff38e3e9":"code","b703093f":"code","ffcf881a":"code","e064ed8b":"code","c7a6d430":"code","61031d67":"code","d142bd14":"code","bc237033":"code","30faa662":"code","7fb5c7cd":"code","c5bcc445":"code","1275cc08":"code","d8fe2aa7":"code","8940cf9e":"code","17b5fbb4":"code","fdbce402":"code","b651886f":"code","ac1b16f8":"code","9c289334":"code","1faec666":"code","96a40509":"code","3a1aa0e1":"code","4e302e66":"code","cd9de479":"code","c08e145a":"code","053dc78a":"code","8d30ec8a":"code","4ae22bab":"code","589551f7":"code","c90211b0":"code","6bb14738":"code","2713ad4e":"code","7ca4c172":"code","6de652a2":"code","8f411cf5":"code","9f1156da":"code","1ed55eac":"code","17c6239c":"markdown","a30ceb14":"markdown","3ea7019a":"markdown","2d8fef97":"markdown","b3c372cf":"markdown","5a4639c0":"markdown","17c8d4a9":"markdown","ebf0c53b":"markdown","8d40c692":"markdown","74ead72c":"markdown","602cdbe7":"markdown","112e785a":"markdown","884f77b9":"markdown","a5abb777":"markdown","d30056b6":"markdown","8a82f309":"markdown","8ebfd310":"markdown","93f065b0":"markdown","f46c7e7f":"markdown","a6e839a6":"markdown","329e9408":"markdown"},"source":{"32ba3e9c":"from sklearn.datasets import load_boston\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split \n\nfrom sklearn.linear_model import LinearRegression\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n%matplotlib inline","1b949850":"boston_data = load_boston()","2758e1d4":"type(boston_data)","532610ef":"boston_data","45220ef5":"# dir(object name) : used to get lisk of attributes associated with the python object","c1bed507":"dir(boston_data)","86a25ef7":"print(boston_data.DESCR)","debbb428":"type(boston_data.DESCR)","35212f7a":"print(boston_data.data)\n\nprint(type(boston_data.data))\n\nprint(boston_data.data.shape)","2aff8f53":"boston_data.feature_names","a0d67063":"boston_data.target # values in thousands","b363d549":"# Creating dataframe\ndata = pd.DataFrame(data=boston_data.data, columns=boston_data.feature_names)","9c86c4e7":"# Adding target values to dataframe\ndata[\"PRICE\"] = boston_data.target","24eee4dc":"data","2f50d74e":"data.count() #Number of rows for each columns","8b8c0f7d":"pd.isnull(data)","2b8d88a9":"data.isnull().any()","3051f4a4":"data.info()","75b13e07":"plt.figure(figsize=(10,6))\nplt.hist(data['PRICE'],bins=50,ec = 'black')\nplt.xlabel(\"Price in Thousands\")\nplt.ylabel(\"Number of houses\")\nplt.show()","495fe159":"plt.figure(figsize=(10,6))\nsns.histplot(data['PRICE'],bins=50)\nplt.xlabel(\"Price in Thousands\")\nplt.ylabel(\"Number of houses\")\nplt.show()","3ee10215":"#distplot contains two things histogarm  and probability density fn (KDE)\n\nplt.figure(figsize=(10,6))\nsns.distplot(data['PRICE'],bins=50,hist=False)\nplt.xlabel(\"Price in Thousands\")\nplt.ylabel(\"Number of houses\")\nplt.show()","1ba0bd0c":"plt.figure(figsize=(10,6))\nsns.distplot(data['RM'])\nplt.xlabel(\"avg number of rooms\")\nplt.ylabel(\"Number of houses\")\nplt.show()","3b1d5502":"plt.figure(figsize=(10,6))\nplt.hist(data['RM'],ec = 'black',color='green')\nplt.xlabel(\"Average number of rooms\")\nplt.ylabel(\"Number of houses\")\nplt.show()","70285b46":"data['RM'].mean()","1543ebda":"data[\"RAD\"]","4bbc5999":"plt.figure(figsize=(10,6))\nplt.hist(data['RAD'],bins=24,ec = 'black')\nplt.xlabel(\"Accessibility from highway\")\nplt.ylabel(\"Number of houses\")\nplt.show()","282911ac":"data['RAD'].value_counts()","1395d650":"frequency = data['RAD'].value_counts()\nfrequency.index\n\nplt.figure(figsize=(10,6))\nplt.bar(frequency.index,height=frequency)\nplt.xlabel(\"Accessibility from highway\")\nplt.ylabel(\"Number of houses\")\nplt.show()","aacacc25":"# How many properties are near charles river\n\nnum_houses_near_river = data[data['CHAS']==1].shape[0]\nnum_houses_near_river","afe011e7":"data['CHAS'].value_counts()","cd48f191":"data.describe()","2565163c":"# Why correlation is imp : 1)Strength and 2)Direction","8ce4078d":"data['PRICE'].corr(data['RM'])","27313262":"data.corr()  #Pearson corr","8c4171dd":"## Having high corr could lead to multicolinearity i.e We could not get exact how much each feature contributes","f28ceb80":"mask = np.zeros_like(data.corr())\ntriangel_indices = np.triu_indices_from(mask)\nmask[triangel_indices] = True\nmask","8bc45b07":"plt.figure(figsize=(16,10))\nsns.heatmap(data.corr(),mask=mask, annot=True, annot_kws={\"size\":14})\nsns.set_style(\"white\")\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show","8ea37372":"nox_dis = round(data['NOX'].corr(data['DIS']),2)\n\nplt.figure(figsize=(12,6))\nplt.scatter(data['DIS'],data['NOX'] , alpha=0.6,s=80, color='indigo')\nplt.xlabel('Distance from Employment',fontsize=14)\nplt.ylabel('Nitric Oxide Pollution',fontsize=14)\nplt.title(f\"DIS VS NOX (Correlation : {nox_dis})\", fontsize=14)\nplt.show()","c42afbae":"#In seaborn scatter plot is jointplot\n\nsns.set()  #reset styling\nsns.set_context('talk')\nsns.set_style('darkgrid')\nplt.figure(figsize=(12,6))\nsns.jointplot(x=data['DIS'], y=data['NOX'], kind='hex', size=8)\nplt.show()","dbaa6d64":"sns.set()  #reset styling\nsns.set_context('talk')\nsns.set_style('darkgrid')\nplt.figure(figsize=(12,6))\nsns.jointplot(x=data['TAX'], y=data['RAD'], size=5)\nplt.show()","181532cf":"# See linear relation for above graph\n\nsns.lmplot(x='TAX', y='RAD', data=data, size=7)\nplt.show()","2e8ba99f":"# Price vs RM\n\nrm_price = round(data['RM'].corr(data['PRICE']),2)\n\nplt.figure(figsize=(12,6))\nplt.scatter(data['RM'],data['PRICE'] , alpha=0.6,s=80, color='indigo')\nplt.xlabel('Number of rooms',fontsize=14)\nplt.ylabel('Price of houses',fontsize=14)\nplt.title(f\"RM VS PRICE (Correlation : {rm_price})\", fontsize=14)\nplt.show()","6228b9c3":"sns.lmplot(x='RM', y='PRICE', data=data, size=7)\nplt.show()","f8eee2ba":"# Diagonal show the histogram and every thing else shows scatter plot","ff38e3e9":"'''%%time\n\nsns.pairplot(data, kind='reg', plot_kws={'line_kws':{'color':'red'}})\nplt.show()'''","b703093f":"price = data['PRICE']\nfeatures = data.drop('PRICE',axis=1)","ffcf881a":"X_train, X_test, y_train, y_test = train_test_split(features, price, test_size=0.2, random_state=10)","e064ed8b":"# % of training set\nlen(X_train)\/len(features)","c7a6d430":"# % of test set\nlen(X_test)\/len(features)","61031d67":"regr = LinearRegression()\nregr.fit(X_train,y_train)","d142bd14":"print(\"Intercepts : \",regr.intercept_)\npd.DataFrame(data=regr.coef_, index = X_train.columns, columns=['coef'])","bc237033":"# R-Square : score\n\nprint(\"Training data r-squared : \",regr.score(X_train,y_train))\nprint(\"Test data r-squared : \",regr.score(X_test,y_test))","30faa662":"# Skew of normal distribution is 0","7fb5c7cd":"data['PRICE'].skew()","c5bcc445":"# Using log to transform gives less difference between data points\n\ny_log = np.log(data['PRICE'])\ny_log.head()","1275cc08":"y_log.skew()","d8fe2aa7":"sns.distplot(y_log)\nplt.title(f'Log Price {y_log.skew()}')\nplt.show()","8940cf9e":"sns.lmplot(x='LSTAT', y='PRICE', data=data, size=7, scatter_kws = {'alpha':0.6}, line_kws = {'color':'darkred'})\nplt.show()","17b5fbb4":"transformed_data = features\ntransformed_data['log_price'] = y_log\n \nsns.lmplot(x='LSTAT', y='log_price', data=transformed_data, size=7, scatter_kws = {'alpha':0.6}, line_kws = {'color':'darkred'})\nplt.show()","fdbce402":"price = np.log(data['PRICE'])\nfeatures = data.drop('PRICE',axis=1)","b651886f":"X_train, X_test, y_train, y_test = train_test_split(features, price, test_size=0.2, random_state=10)","ac1b16f8":"regr = LinearRegression()\nregr.fit(X_train,y_train)","9c289334":"print(\"Intercepts : \",regr.intercept_)\npd.DataFrame(data=regr.coef_, index = X_train.columns, columns=['coef'])","1faec666":"# R-Square : score\n\nprint(\"Training data r-squared : \",regr.score(X_train,y_train))\nprint(\"Test data r-squared : \",regr.score(X_test,y_test))","96a40509":"# Charles river property premium (reverse the log coef of charles value above)\n\nnp.e**0.080331  #1084 $ more ","3a1aa0e1":"# Add intercept to X_train and apply OLS(ordinary least square) to get regression model\n\n# ** similar to add Xo in ML practice (0o + 01X1....) here Xo \nX_incl_constant = sm.add_constant(X_train)\n\nmodel = sm.OLS(y_train,X_incl_constant)\nresults = model.fit()\n\npd.DataFrame({'Coef':results.params,'p-values':round(results.pvalues,3)})","4e302e66":"# INDUS AND AGE features are not significant as it has p values > 0.05","cd9de479":"# Multicollinearity : why it is not good?\n'''1) Loss Reliability\n   2) High Variability in theta estimate\n   3) Strange Findings'''\n","c08e145a":"# variance_inflation_factor takes two arg exog : ndarray and exog_idx : index of which column we want to see vif \nvariance_inflation_factor(exog  = X_incl_constant.values, exog_idx = 1) # for CRIM column","053dc78a":"# to get vif for all columns loop through it\nvif_values = [variance_inflation_factor(exog  = X_incl_constant.values, exog_idx = i) \n              for i in range(X_incl_constant.shape[1])]\n\npd.DataFrame({\"Coef \":X_incl_constant.columns,\"vif_values\":np.round(vif_values,2)})","8d30ec8a":"# Original model with log_price and all features\n\nX_incl_constant = sm.add_constant(X_train)\n\nmodel = sm.OLS(y_train,X_incl_constant)\nresults = model.fit()\n\norg_coef = pd.DataFrame({'Coef':results.params,'p-values':round(results.pvalues,3)})\n\nprint(\"Original model with log_price and all features\")\nprint(\"\\nBIC is : \",results.bic)\nprint(\"R-Squared is : \",results.rsquared)","4ae22bab":"# Reduced model #1 Removed INDUS\n\nX_incl_constant = sm.add_constant(X_train)\nX_incl_constant = X_incl_constant.drop(['INDUS'],axis=1)\n\nmodel = sm.OLS(y_train,X_incl_constant)\nresults = model.fit()\n\ncoef_minus_indus = pd.DataFrame({'Coef':results.params,'p-values':round(results.pvalues,3)})\n\nprint(\"Reduced model #1 Removed INDUS\")\nprint(\"\\nBIC is : \",results.bic)\nprint(\"R-Squared is : \",results.rsquared)","589551f7":"# Reduced model #2 Removed INDUS,AGE\n\nX_incl_constant = sm.add_constant(X_train)\nX_incl_constant = X_incl_constant.drop(['INDUS','AGE'],axis=1)\n\nmodel = sm.OLS(y_train,X_incl_constant)\nresults = model.fit()\n\nreduced_coef = pd.DataFrame({'Coef':results.params,'p-values':round(results.pvalues,3)})\n\nprint(\"Reduced model #2 Removed INDUS,AGE\")\nprint(\"\\nBIC is : \",results.bic)\nprint(\"R-Squared is : \",results.rsquared)","c90211b0":"# Checking above three models side by side\nframes = [org_coef, coef_minus_indus, reduced_coef]\npd.concat(frames,axis=1)","6bb14738":"#Model No: 1\n# Modified model transformed using log price and dropped two features\n\nprice = np.log(data['PRICE'])\nfeatures = data.drop(['PRICE','INDUS','AGE'],axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(features, price, test_size=0.2, random_state=10)\n\nX_incl_constant = sm.add_constant(X_train)\n\nmodel = sm.OLS(y_train,X_incl_constant)\nresults = model.fit()\n\n#residuals = y_train - results.fittedvalues\n#residuals\n\n#results.resid\n\n# Graph of Actual vs Predicted\ncorr = round(y_train.corr(results.fittedvalues),2)\ncorr\n\n# for log prices\n\nplt.figure(figsize=(15,7))\nplt.scatter(x=y_train, y=results.fittedvalues, c='navy', alpha=0.6)\nplt.plot(y_train, y_train, color='red')\nplt.xlabel(\"Actual house price $ y _i$\", fontsize=14)\nplt.ylabel(\"Predicted house price $ \\hat y _i$\",  fontsize=14)\nplt.title(f\"Actul vs Predicted house prices (Corr = {corr})\",  fontsize=14)\nplt.show()\n\n# without log price\n\nplt.figure(figsize=(15,7))\nplt.scatter(x=np.e**y_train, y=np.e**results.fittedvalues, c='navy', alpha=0.6)\nplt.plot(np.e**y_train, np.e**y_train, color='red')\nplt.xlabel(\"Actual house price $ y _i$\", fontsize=14)\nplt.ylabel(\"Predicted house price $ \\hat y _i$\",  fontsize=14)\nplt.title(f\"Actul vs Predicted house prices (Corr = {corr})\",  fontsize=14)\nplt.show()\n\n\n# Residuals vs Predicted values\n\nplt.figure(figsize=(15,7))\nplt.scatter(x=results.fittedvalues, y=results.resid, c='navy', alpha=0.6)\n\nplt.xlabel(\"Predicted log prices\", fontsize=14)\nplt.ylabel(\"Residuls\",  fontsize=14)\nplt.title(\"Residuls vs Predicted Values\",  fontsize=14)\nplt.show()\n\n\n# Distribution of Residuals (log price) # checking for normality\n\nresid_mean = round(results.resid.mean(),3)\n\nresid_skew = round(results.resid.skew(),3)\nprint(f\"Residuals mean : {resid_mean}  \\nResiduals skew : {resid_skew}\")\n\nplt.figure(figsize=(12,9))\nsns.distplot(results.resid, color='navy')\nplt.title(f\"Log price residuals \\nResiduals mean : {resid_mean}  Residuals skew : {resid_skew}\", fontsize=14)\nplt.show()\n\n# Above graph shows that there are more values to both extreme comparing to normal distribution\n\n# Mean Squared Error & R squared\nreduced_log_mse = round(results.mse_resid,3)\nreduced_log_rsquared = round(results.rsquared,3)","2713ad4e":"#Model No: 2\n# Original model using normal prices and with all features\n\nprice = data['PRICE']\nfeatures = data.drop(['PRICE'],axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(features, price, test_size=0.2, random_state=10)\n\nX_incl_constant = sm.add_constant(X_train)\n\nmodel = sm.OLS(y_train,X_incl_constant)\nresults = model.fit()\n\n#residuals = y_train - results.fittedvalues\n#residuals\n\n#results.resid\n\n# Graph of Actual vs Predicted\ncorr = round(y_train.corr(results.fittedvalues),2)\ncorr\n\nplt.figure(figsize=(15,7))\nplt.scatter(x=y_train, y=results.fittedvalues, c='navy', alpha=0.6)\nplt.plot(y_train, y_train, color='red')\nplt.xlabel(\"Actual house price in 000s $ y _i$\", fontsize=14)\nplt.ylabel(\"Predicted house price in 000s $ \\hat y _i$\",  fontsize=14)\nplt.title(f\"Actul vs Predicted house prices (Corr = {corr})\",  fontsize=14)\nplt.show()\n\n\n# Residuals vs Predicted values\n\nplt.figure(figsize=(15,7))\nplt.scatter(x=results.fittedvalues, y=results.resid, c='navy', alpha=0.6)\n\nplt.xlabel(\"Predicted prices\", fontsize=14)\nplt.ylabel(\"Residuls\",  fontsize=14)\nplt.title(\"Residuls vs Predicted Values\",  fontsize=14)\nplt.show()\n\n\n# Distribution of Residuals (log price) # checking for normality\n\nresid_mean = round(results.resid.mean(),3)\n\nresid_skew = round(results.resid.skew(),3)\nprint(f\"Residuals mean : {resid_mean}  \\nResiduals skew : {resid_skew}\")\n\nplt.figure(figsize=(12,9))\nsns.distplot(results.resid, color='navy')\nplt.title(f\"Normal price residuals \\nResiduals mean : {resid_mean}  Residuals skew : {resid_skew}\", fontsize=14)\nplt.show()\n\n# Above graph shows that there are more values to both extreme comparing to normal distribution\n\n# Mean Squared Error & R squared\nfull_normal_mse = round(results.mse_resid,3)\nfull_normal_rsquared = round(results.rsquared,3)","7ca4c172":"#Model No: 3\n# Model Omitting key features using log price\n\nprice = np.log(data['PRICE'])\nfeatures = data.drop(['PRICE','INDUS','AGE','LSTAT','RM','NOX','CRIM'],axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(features, price, test_size=0.2, random_state=10)\n\nX_incl_constant = sm.add_constant(X_train)\n\nmodel = sm.OLS(y_train,X_incl_constant)\nresults = model.fit()\n\n#residuals = y_train - results.fittedvalues\n#residuals\n\n#results.resid\n\n# Graph of Actual vs Predicted\ncorr = round(y_train.corr(results.fittedvalues),2)\ncorr\n\n# for log prices\n\nplt.figure(figsize=(15,7))\nplt.scatter(x=y_train, y=results.fittedvalues, c='navy', alpha=0.6)\nplt.plot(y_train, y_train, color='red')\nplt.xlabel(\"Actual house price $ y _i$\", fontsize=14)\nplt.ylabel(\"Predicted house price $ \\hat y _i$\",  fontsize=14)\nplt.title(f\"Actul vs Predicted house prices with omitted key features (Corr = {corr})\",  fontsize=14)\nplt.show()\n\n# without log price\n\nplt.figure(figsize=(15,7))\nplt.scatter(x=np.e**y_train, y=np.e**results.fittedvalues, c='navy', alpha=0.6)\nplt.plot(np.e**y_train, np.e**y_train, color='red')\nplt.xlabel(\"Actual house price $ y _i$\", fontsize=14)\nplt.ylabel(\"Predicted house price $ \\hat y _i$\",  fontsize=14)\nplt.title(f\"Actul vs Predicted house prices (Corr = {corr})\",  fontsize=14)\nplt.show()\n\n\n# Residuals vs Predicted values\n\nplt.figure(figsize=(15,7))\nplt.scatter(x=results.fittedvalues, y=results.resid, c='navy', alpha=0.6)\n\nplt.xlabel(\"Predicted log prices\", fontsize=14)\nplt.ylabel(\"Residuls\",  fontsize=14)\nplt.title(\"Residuls vs Predicted Values\",  fontsize=14)\nplt.show()\n\n# Mean Squared Error & R squared\nomitted_var_mse = round(results.mse_resid,3)\nomitted_var_rsquared = round(results.rsquared,3)","6de652a2":"# We can see banding of datapoints from above graph which tells we are missing some important features \n#(because we have dropped key features)","8f411cf5":"# Comparring all mse and rsquarred\n\npd.DataFrame({\"Rsquared\": [reduced_log_rsquared,full_normal_rsquared,omitted_var_rsquared],\n              \"MSE\": [reduced_log_mse,full_normal_mse,omitted_var_mse],\n              \"RMSE\": np.sqrt([reduced_log_mse,full_normal_mse,omitted_var_mse])},\n             index=['Reduced Log model',\"Full normal model\",\"Omitted var model\"])\n\n# R-squared is relative measure with value always between 0 - 1 and does not contain any units\n# MSE is absolute measure and has units same as target value (for log models units are in log and normal model units are in 000s)\n","9f1156da":"# Best way to do prediction is to give the estimated value and the range\n# By the normal distribution the range is set to +2s.d and -2s.d in normal distribution i.e +2RMSE and -2RSME","1ed55eac":"# Eg Calculate for 30000$ for reduced log\n\nprint(\"For 30000$ \")\nprint(\"1 s.d in log price :\", np.sqrt(reduced_log_mse))\nprint(\"2 s.d in log price :\", 2 * np.sqrt(reduced_log_mse))\n\nupper_bound = np.log(30) + 2 * np.sqrt(reduced_log_mse)\nlower_bound = np.log(30) - 2 * np.sqrt(reduced_log_mse)\nprint(\"\\nThe upper bound in log price for 95% of interval is :\",upper_bound)\nprint(\"The lower bound in log price for 95% of interval is :\",lower_bound)\n\n\nprint(\"\\nThe upper bound in $ price for 95% of interval is :\",np.e**upper_bound*1000)\nprint(\"The lower bound in $ price for 95% of interval is :\",np.e**lower_bound*1000)","17c6239c":"## Rerun regression using log values","a30ceb14":"### If there is any patters for eg img below, then there might be something going worng in the model\n![Screenshot%20from%202021-02-03%2010-56-54.png](attachment:Screenshot%20from%202021-02-03%2010-56-54.png)","3ea7019a":"## Train and Test data","2d8fef97":"## This Notebook covers basics for Multivariate Regression using Boston House Price dataset.\n- EDA\n- Viz\n- Analysing Coefficients (p-value, VIF, BIC)\n- Analysing Residuals\n","b3c372cf":"## Descriptive statistics ,scatter and outliers","5a4639c0":"\n#### Drawbacks of Corr\n#### 1) Only works with continous data\n#### 2) Corr doesnt implies caisation\n#### 3) Works for linear relations\n#### 4) Outliers can disturb corr\n#### Best way is to find corr along with visuals \n","17c8d4a9":"## Multivariable Regression","ebf0c53b":"## Visualizing data - Histogram, Distribution and barcharts","8d40c692":"## Cleaning data : check for missing values","74ead72c":"### To embed link in jupyter nb use : [] () --> link sholud be placed inside () and [] is used to display the text over that link\n\n[For info google it](http:\/\/www.google.com\/)","602cdbe7":"## Data exploration with pandas","112e785a":"## Gather Data","884f77b9":"### Why we should look for patterns between residuals and predicted values\n### For a good model there should be no pattern between residuals and predicted values\n![Screenshot%20from%202021-02-03%2010-48-57.png](attachment:Screenshot%20from%202021-02-03%2010-48-57.png)","a5abb777":"##  Importing packages","d30056b6":"## Variance Inflation Factor (VIF) to check for multicollinearity\n#### For example suppose we interpret TAX feature for all other features\n### $ TAX = \\alpha _0 + \\alpha _1 RM +  + \\alpha _2 NOX +... + \\alpha _{12} LSTAT       $\n\n### $ VIF_{TAX} = \\frac {1}{( 1-R _{TAX} ^2 )}       $\n\n### Any feature with vif value > 10 is problematic and has multicollinearity","8a82f309":"## Data Transformation","8ebfd310":"## P-value and evaluating coefficient (p <0.05 -->significant , p>0.05 --> not significant)","93f065b0":"## Residuals and Residuals plot","f46c7e7f":"## Multivariable Regression","a6e839a6":"## Model Simplification and BIC \n### (Baysian Information Criterion : Compares multiple models and gives BIC value, Less value is better)","329e9408":"## Correlation \n## $ \\rho_{xy} = corr(x,y) $    -->  $ -1.0 \\leq \\rho_{xy} \\leq +1.0 $"}}