{"cell_type":{"84001eac":"code","aec1dec0":"code","467f6ea7":"code","95bb1d4f":"code","1aec0eba":"code","7f533dd9":"code","2134f61b":"code","13303520":"code","4f70142d":"code","0e4ac1fa":"markdown","b520f82c":"markdown","c305c542":"markdown","30a2cbc3":"markdown","b80d4a17":"markdown","73815769":"markdown"},"source":{"84001eac":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport matplotlib.pyplot as plt\nfrom torchvision.io import read_image\nfrom torch.utils.data import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import Resize","aec1dec0":"image_path = '..\/input\/plant-pathology-2021-fgvc8\/train_images'\n\ndf = pd.read_csv('..\/input\/plant-pathology-2021-fgvc8\/train.csv')\ntrain_df , val_df = train_test_split(df)","467f6ea7":"class CustomImageDataset_from_csv(Dataset):\n    def __init__(self, dataframe , img_dir ,  transform = None , label_transform = None):\n        self.img_labels = dataframe #pd.read_csv(annotations_file)\n        self.img_dir = img_dir\n        self.transform = transform\n        self.label_transform = label_transform\n        \n    def __len__(self):\n        return len(self.img_labels)\n    \n    def __getitem__(self , idx):\n        img_path = os.path.join(self.img_dir , self.img_labels.iloc[idx, 0])\n        print(img_path)\n        image = read_image(img_path)\n        label = self.img_labels.iloc[idx, 1]\n        if self.transform:\n            image = self.transform(image)\n        if self.label_transform:\n            label = self.target_transform(label)\n        \n        return(image, label , img_path)","95bb1d4f":"train_data = CustomImageDataset_from_csv(train_df , image_path , transform = Resize((224, 224)))\nval_data = CustomImageDataset_from_csv(val_df , image_path , transform = Resize((224, 224)))","1aec0eba":"batch_size = 4\ntrain_dataloader = DataLoader(train_data, batch_size = batch_size , shuffle = True)\nval_dataloader = DataLoader(val_data, batch_size = batch_size , shuffle = True)","7f533dd9":"sample_images , sample_labels , x = next(iter(train_dataloader))","2134f61b":"img = Image.open('..\/input\/plant-pathology-2021-fgvc8\/train_images\/9987460a83f7c3e6.jpg')\nimg = Image.open(str(x[0]))\n\nplt.imshow(img, cmap = 'magma')","13303520":"img = sample_images[0] #.squeeze()\nprint(img.shape)\nimg = img.reshape(img.shape[2],img.shape[1],img.shape[0])\nimg.shape","4f70142d":"plt.imshow(img)","0e4ac1fa":"# Work in progress. Soon would add more.","b520f82c":"# ImageLoader from folder\n\nhttps:\/\/www.kaggle.com\/crowww\/a-large-scale-fish-dataset\n\n\nhttps:\/\/www.kaggle.com\/gpiosenka\/100-bird-species?select=birds","c305c542":"# ImageLoader from TFRecord files\n\nhttps:\/\/www.kaggle.com\/nickuzmenkov\/nih-chest-xrays-tfrecords","30a2cbc3":"https:\/\/www.kaggle.com\/c\/plant-pathology-2021-fgvc8","b80d4a17":"# Image dataloader from a csv file\/dataframe\n\n","73815769":"# ImageLoader - where Image pixel data is stored in csv file \n\nExample dataset - https:\/\/www.kaggle.com\/c\/Kannada-MNIST"}}