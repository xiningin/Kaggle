{"cell_type":{"346dde2b":"code","a04e9302":"code","78b03aa2":"code","3a48c0b3":"code","c85bdbc6":"code","8dfd9344":"code","464e24a9":"code","3db0f376":"code","8e2498bf":"code","d8978f2c":"code","568bada0":"code","9077e691":"code","d47c98f5":"code","53f4719f":"code","2e0c0bcb":"code","b71142dc":"code","802a3491":"code","d6610da9":"code","cb4cf101":"code","2e6d03b3":"code","b2dae602":"code","c9a0f851":"code","c5437cdb":"code","fe6ad0df":"code","bf63d252":"code","3d95ce27":"code","dbd0415e":"code","d60c4178":"code","521a3859":"code","fef04cda":"code","dad0bc05":"code","35ac3ff6":"code","e8449fdc":"code","377ced37":"code","b5cdc0d6":"code","ea03eb87":"code","b055fec1":"code","c6621004":"code","143ddc44":"code","259ccc4d":"code","f595da9e":"code","9dc5f216":"code","7c143600":"code","ab67f604":"code","f4b4e31c":"code","819543d4":"code","adb82534":"code","c2215f76":"code","f71a834c":"code","bbafd46c":"code","9bb7846e":"code","98f92db7":"code","a0c0dcb6":"code","8fe1482c":"code","ccc60016":"code","9eff91c5":"code","9a9d0d77":"code","ad90f429":"code","a06e01cb":"code","ae11a2e3":"code","d6627137":"markdown","310980ce":"markdown","0a908584":"markdown","3cc536ab":"markdown","32c12afa":"markdown","2e3a5784":"markdown","534785fe":"markdown","a55e0323":"markdown","73b50a61":"markdown","78723c3c":"markdown","3d2b920a":"markdown","20f052a5":"markdown","40fad8be":"markdown","b7bdfe6e":"markdown","0ea44397":"markdown","991e6bbc":"markdown","8beba1f3":"markdown","0c1ae5f8":"markdown","52ec34dd":"markdown","5f587d21":"markdown","460d16bb":"markdown","29757fe4":"markdown","dafd1717":"markdown","3c494a5d":"markdown","6e8aec34":"markdown","a6c61882":"markdown","63864b9b":"markdown","82de66a2":"markdown","bb2e92f5":"markdown","b86b6f2f":"markdown","05c7cfef":"markdown","826babd8":"markdown","84b0beb4":"markdown","b6812d87":"markdown","43bf5875":"markdown","ea9b0dde":"markdown","96632f01":"markdown","c9b7ee0f":"markdown","6ec23007":"markdown","b4ef6bf9":"markdown","7e6babbf":"markdown","fa737511":"markdown","5ea5aad6":"markdown","ff2788ae":"markdown","5383211e":"markdown","0fa4be81":"markdown","8321d27b":"markdown","bc51f5e9":"markdown","d84bca72":"markdown","f8032654":"markdown","0af847ee":"markdown","8eecbdbe":"markdown","acf565ca":"markdown","40c4a0fd":"markdown","43f5622b":"markdown","324556e6":"markdown","16a12fca":"markdown","f71a20ca":"markdown","b23d83cb":"markdown"},"source":{"346dde2b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a04e9302":"import numpy as np \nimport pandas as pd\nimport pandas\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n%matplotlib inline\nimport seaborn as sns; sns.set()\n\nfrom sklearn import tree\nimport graphviz \nimport os\nimport preprocessing \n\nimport numpy as np \nimport pandas as pd \nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nfrom pandas_profiling import ProfileReport\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, f_classif\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.svm import LinearSVC\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nfrom sklearn.preprocessing import normalize\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier, XGBRFClassifier\nfrom xgboost import plot_tree, plot_importance\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","78b03aa2":"dataset = pandas.read_csv('\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv')\ndataset.sample(10)","3a48c0b3":"dataset.tail()","c85bdbc6":"dataset.drop(\"sl_no\", axis=1, inplace=True)","8dfd9344":"dataset.info()","464e24a9":"def bar_plot(variable):\n    # get feature\n    var = dataset[variable]\n    # count number of categorical variable(value\/sample)\n    varValue = var.value_counts()\n    \n    # visualize\n    plt.figure(figsize = (9,3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}:\\n{}\".format(variable,varValue))","3db0f376":"categorical = (dataset.dtypes == \"object\")\ncategorical_list = list(categorical[categorical].index)\n\nprint(\"Categorical variables:\")\nprint(categorical_list)","8e2498bf":"sns.set_style('darkgrid')\nfor c in categorical_list:\n    bar_plot(c)","d8978f2c":"numerical_float64 = (dataset.dtypes == \"float64\")\nnumerical_float64_list = list(numerical_float64[numerical_float64].index)\n\nprint(\"Numerical variables:\")\nprint(numerical_float64_list)","568bada0":"def plot_hist(variable):\n    plt.figure(figsize = (9,3))\n    plt.hist(dataset[variable], bins = 50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} Distribution with Histogram\".format(variable))\n    plt.show()","9077e691":"for n in numerical_float64_list:\n    plot_hist(n)","d47c98f5":"plt.figure(figsize=(25,15))\n\nplt.subplot(2,3,1)\nsns.histplot(dataset['ssc_p'], color = 'red', kde = True).set_title('ssc_p Interval and Counts')\n\nplt.subplot(2,3,2)\nsns.histplot(dataset['hsc_p'], color = 'green', kde = True).set_title('hsc_p Interval and Counts')\n\nplt.subplot(2,3,3)\nsns.histplot(dataset['degree_p'], kde = True, color = 'blue').set_title('degree_p Interval and Counts')\n\nplt.subplot(2,3,4)\nsns.histplot(dataset['etest_p'], kde = True, color = 'pink').set_title('etest_p Interval and Counts')\n\nplt.subplot(2,3,5)\nsns.histplot(dataset['mba_p'], kde = True, color = 'yellow').set_title('mba_p Interval and Counts')\n\nplt.subplot(2,3,6)\nsns.histplot(dataset['salary'], kde = True, color = 'black').set_title('salary Interval and Counts')","53f4719f":"plt.figure(figsize=(25,15))\n\nplt.subplot(2,3,1)\nsns.boxenplot(x=dataset['status'], y=dataset['ssc_p'],\n              color=\"r\", \n              scale=\"linear\", data=dataset)\n\nplt.subplot(2,3,2)\nsns.boxenplot(x=dataset['status'], y=dataset['hsc_p'],\n              color=\"g\", \n              scale=\"linear\", data=dataset)\n\nplt.subplot(2,3,3)\nsns.boxenplot(x=dataset['status'], y=dataset['degree_p'],\n              color=\"b\", \n              scale=\"linear\", data=dataset)\n\nplt.subplot(2,3,4)\nsns.boxenplot(x=dataset['status'], y=dataset['etest_p'],\n              color=\"pink\", \n              scale=\"linear\", data=dataset)\n\nplt.subplot(2,3,5)\nsns.boxenplot(x=dataset['status'], y=dataset['mba_p'],\n              color=\"yellow\", \n              scale=\"linear\", data=dataset)\n\nplt.subplot(2,3,6)\nsns.boxenplot(x=dataset['status'], y=dataset['salary'],\n              color=\"black\", \n              scale=\"linear\", data=dataset)","2e0c0bcb":"dataset['status'].unique()","b71142dc":"status_mapping = {'Not Placed': 0, 'Placed': 1}\ndataset['status'] = dataset['status'].map(status_mapping)","802a3491":"dataset[[\"gender\",\"status\"]].groupby([\"gender\"], as_index = False).count().sort_values(by=\"status\",ascending = False)","d6610da9":"df = dataset\n\nlabels = dataset['gender'].value_counts().index\npie1 = dataset['gender'].value_counts().values\n# figure\nfig = {\n  \"data\": [\n    {\n      \"values\": pie1,\n      \"labels\": labels,\n      \"domain\": {\"x\": [0, .5]},\n      \"name\": \"\",\n      \"hoverinfo\":\"label+percent+name+value\",\n      \"hole\": .2,\n      \"type\": \"pie\"\n    },],\n  \"layout\": {\n        \"title\":\"Distribution of Genders\",\n        \"annotations\": [\n            { \"font\": { \"size\": 25},\n              \"showarrow\": True,\n              \"text\": \"Genders\",\n                \"x\": 1,\n                \"y\": 1,\n            },\n        ]\n    }\n}\niplot(fig)","cb4cf101":"dataset[[\"ssc_b\",\"status\"]].groupby([\"ssc_b\"], as_index = False).count().sort_values(by=\"status\",ascending = False)","2e6d03b3":"labels = dataset['ssc_b'].value_counts().index\nsizes = dataset['ssc_b'].value_counts().values\n\nplt.figure(figsize = (8,8))\nplt.pie(sizes, labels=labels, autopct='%1.1f%%')\nplt.title(\"Distribution of Samples by 'ssc_b'\",color = 'black',fontsize = 15)","b2dae602":"dataset[[\"hsc_b\",\"status\"]].groupby([\"hsc_b\"], as_index = False).count().sort_values(by=\"status\",ascending = False)","c9a0f851":"labels = dataset['hsc_b'].value_counts().index\nsizes = dataset['hsc_b'].value_counts().values\n\nplt.figure(figsize = (8,8))\nplt.pie(sizes, labels=labels, autopct='%1.1f%%')\nplt.title(\"Distribution of Samples by 'hsc_b'\",color = 'black',fontsize = 15)","c5437cdb":"dataset[[\"degree_t\",\"status\"]].groupby([\"degree_t\"], as_index = False).count().sort_values(by=\"status\",ascending = False)","fe6ad0df":"labels = dataset['degree_t'].value_counts().index\nsizes = dataset['degree_t'].value_counts().values\n\nplt.figure(figsize = (8,8))\nplt.pie(sizes, labels=labels, autopct='%1.1f%%')\nplt.title(\"Distribution of Samples by 'degree_t'\",color = 'black',fontsize = 15)","bf63d252":"dataset[[\"workex\",\"status\"]].groupby([\"workex\"], as_index = False).count().sort_values(by=\"status\",ascending = False)","3d95ce27":"counts = dataset['workex'].value_counts()\n\nplt.figure(figsize=(10,7))\nsns.barplot(x=counts.index, y=counts.values, palette=\"Set3\")\n\nplt.ylabel('Count')\nplt.xlabel('workex', style = 'normal', size = 24)\n\nplt.xticks(rotation = 45, size = 12)\nplt.yticks(rotation = 45, size = 12)\n\nplt.title('Distribution of workex',color = 'black',fontsize=15)\nplt.show()","dbd0415e":"dataset[[\"specialisation\",\"status\"]].groupby([\"specialisation\"], as_index = False).count().sort_values(by=\"status\",ascending = False)","d60c4178":"labels = dataset['specialisation'].value_counts().index\nsizes = dataset['specialisation'].value_counts().values\n\nplt.figure(figsize = (8,8))\nplt.pie(sizes, labels=labels, autopct='%1.1f%%')\nplt.title(\"Distribution of Samples by 'specialisation'\",color = 'black',fontsize = 15)","521a3859":"import seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n\nplt.figure(figsize=(20,15))\n\nplt.subplot(2,3,1)\nsns.swarmplot(x = dataset[dataset['status'] == 1]['status'], y=\"salary\",hue=\"ssc_b\", data=dataset, palette=\"PRGn\")\n\nplt.subplot(2,3,2)\nsns.swarmplot(x = dataset[dataset['status'] == 1]['status'], y=\"salary\",hue=\"hsc_b\", data=dataset, palette=\"Wistia_r\")\n\nplt.subplot(2,3,3)\nsns.swarmplot(x = dataset[dataset['status'] == 1]['status'], y=\"salary\",hue=\"hsc_s\", data=dataset, palette=\"gist_ncar_r\")\n\nplt.subplot(2,3,4)\nsns.swarmplot(x = dataset[dataset['status'] == 1]['status'], y=\"salary\",hue=\"degree_t\", data=dataset, palette=\"gist_earth\")\n\nplt.subplot(2,3,5)\nsns.swarmplot(x = dataset[dataset['status'] == 1]['status'], y=\"salary\",hue=\"workex\", data=dataset, palette=\"rocket_r\")\n\nplt.subplot(2,3,6)\nsns.swarmplot(x = dataset[dataset['status'] == 1]['status'], y=\"salary\",hue=\"specialisation\", data=dataset, palette=\"twilight\")\n\nplt.show()","fef04cda":"sns.set_theme(style=\"darkgrid\")\n\n\nsns.boxenplot(x=dataset['status'], y=dataset['degree_p'],\n              color=\"b\", \n              scale=\"linear\", data=dataset)","dad0bc05":"sns.swarmplot(x=\"status\", y=\"degree_p\", data=dataset, palette=\"PRGn\")","35ac3ff6":"counts = dataset['specialisation'].value_counts()\n\n#dataset[dataset['status'] == 1]['status']\n\nplt.figure(figsize=(10,7))\nsns.barplot(x=counts.index, y=counts.values, palette=\"Set3\")\n\nplt.ylabel('Count')\nplt.xlabel('workex', style = 'normal', size = 24)\n\nplt.xticks(rotation = 45, size = 12)\nplt.yticks(rotation = 45, size = 12)\n\nplt.title('Distribution of specialisation',color = 'black',fontsize=15)\nplt.show()","e8449fdc":"import pandas_profiling as pp\npp.ProfileReport(dataset)","377ced37":"dataset.corr()","b5cdc0d6":"plt.figure(figsize=(12,8)) \nsns.heatmap(dataset.corr(), annot=True, cmap='Dark2_r', linewidths = 2)\nplt.show()","ea03eb87":"sns.pairplot(dataset, hue = 'status')","b055fec1":"#This code is retrieved from here: https:\/\/www.kaggle.com\/kanncaa1\/dataiteam-titanic-eda#Introduction\n\ndef detect_outliers(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indeces\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        # store indeces\n        outlier_indices.extend(outlier_list_col)\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers","c6621004":"dataset.loc[detect_outliers(dataset,['ssc_p', 'hsc_p', 'degree_p', 'etest_p', 'mba_p', 'salary'])]","143ddc44":"dataset.isnull().sum()","259ccc4d":"dataset[dataset['salary'].isnull()]","f595da9e":"dataset['salary'] = dataset['salary'].fillna(0)","9dc5f216":"dataset[dataset['salary'].isnull()]","7c143600":"cat_var = ['gender', 'ssc_b', 'hsc_b', 'hsc_s', 'degree_t', 'workex', 'specialisation']\n\nfor i in range (0, len(cat_var)):\n    print(f'Unique Values for {cat_var[i]}', dataset[f'{cat_var[i]}'].unique())","ab67f604":"gender_mapping = {'M': 0, 'F': 1}\ndataset['gender'] = dataset['gender'].map(gender_mapping)","f4b4e31c":"ssc_b_mapping = {'Others': 0, 'Central': 1}\ndataset['ssc_b'] = dataset['ssc_b'].map(ssc_b_mapping)","819543d4":"hsc_b_mapping = {'Others': 0, 'Central': 1}\ndataset['hsc_b'] = dataset['hsc_b'].map(hsc_b_mapping)","adb82534":"workex_mapping = {'No': 0, 'Yes': 1}\ndataset['workex'] = dataset['workex'].map(workex_mapping)","c2215f76":"specialisation_mapping = {'Mkt&HR': 0, 'Mkt&Fin': 1}\ndataset['specialisation'] = dataset['specialisation'].map(specialisation_mapping)","f71a834c":"onehotencoder = OneHotEncoder()","bbafd46c":"one_hot = ['hsc_s', 'degree_t']\n\nfor i in range(0, len(one_hot)):\n    dataset[f'{one_hot[i]}'] = pd.Categorical(dataset[f'{one_hot[i]}'])\n    dummies = pd.get_dummies(dataset[f'{one_hot[i]}'], prefix = f'{one_hot[i]}_encoded')\n    dataset.drop([f'{one_hot[i]}'], axis=1, inplace=True)\n    dataset = pd.concat([dataset, dummies], axis=1)","9bb7846e":"dataset","98f92db7":"dataset.drop(\"salary\", axis=1, inplace=True)","a0c0dcb6":"features = dataset.columns.drop('status')\n\nlabel = ['status']\n\nX = dataset[features]\ny = dataset[label]","8fe1482c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101) \nX_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n\nprint(f'Total # of sample in whole dataset: {len(X)}')\nprint(f'Total # of sample in train dataset: {len(X_train)}')\nprint(f'Total # of sample in validation dataset: {len(X_valid)}')\nprint(f'Total # of sample in test dataset: {len(X_test)}')","ccc60016":"sc=StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","9eff91c5":"models = {\n    'GaussianNB': GaussianNB(),\n    'BernoulliNB': BernoulliNB(),\n    'LogisticRegression': LogisticRegression(),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'SupportVectorMachine': SVC(),\n    'DecisionTreeClassifier': DecisionTreeClassifier(),\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier(),\n    'Stochastic Gradient Descent':  SGDClassifier(max_iter=5000, random_state=0),\n    'Neural Nets': MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5000, 10), random_state=1),\n}\n\nmodelNames = [\"GaussianNB\", 'BernoulliNB','LogisticRegression','RandomForestClassifier','SupportVectorMachine',\n             'DecisionTreeClassifier', 'KNeighborsClassifier','GradientBoostingClassifier',\n             'Stochastic Gradient Descent', 'Neural Nets']\n\ntrainScores = []\nvalidationScores = []\ntestScores = []\n\nfor m in models:\n  model = models[m]\n  model.fit(X_train, y_train)\n  score = model.score(X_valid, y_valid)\n  #print(f'{m} validation score => {score*100}')\n    \n  print(f'{m}') \n  train_score = model.score(X_train, y_train)\n  print(f'Train score of trained model: {train_score*100}')\n  trainScores.append(train_score*100)\n\n  validation_score = model.score(X_valid, y_valid)\n  print(f'Validation score of trained model: {validation_score*100}')\n  validationScores.append(validation_score*100)\n\n  test_score = model.score(X_test, y_test)\n  print(f'Test score of trained model: {test_score*100}')\n  testScores.append(test_score*100)\n  print(\" \")\n    \n  y_predictions = model.predict(X_test)\n  conf_matrix = confusion_matrix(y_predictions, y_test)\n\n  print(f'Confussion Matrix: \\n{conf_matrix}\\n')\n\n  predictions = model.predict(X_test)\n  cm = confusion_matrix(predictions, y_test)\n\n  tn = conf_matrix[0,0]\n  fp = conf_matrix[0,1]\n  tp = conf_matrix[1,1]\n  fn = conf_matrix[1,0]\n  accuracy  = (tp + tn) \/ (tp + fp + tn + fn)\n  precision = tp \/ (tp + fp)\n  recall    = tp \/ (tp + fn)\n  f1score  = 2 * precision * recall \/ (precision + recall)\n  specificity = tn \/ (tn + fp)\n  print(f'Accuracy : {accuracy}')\n  print(f'Precision: {precision}')\n  print(f'Recall   : {recall}')\n  print(f'F1 score : {f1score}')\n  print(f'Specificity : {specificity}')\n  print(\"\") \n  print(f'Classification Report: \\n{classification_report(predictions, y_test)}\\n')\n  print(\"\")\n   \n  for m in range (1):\n    current = modelNames[m]\n    modelNames.remove(modelNames[m])\n\n  preds = model.predict(X_test)\n  confusion_matr = confusion_matrix(y_test, preds) #normalize = 'true'\n  print(\"############################################################################\")\n  print(\"\")\n  print(\"\")\n  print(\"\")","9a9d0d77":"plt.figure(figsize=(20,10))\nsns.set_style('darkgrid')\nplt.title('Train - Validation - Test Scores of Models', fontweight='bold', size = 24)\n\nbarWidth = 0.25\n \nbars1 = trainScores\nbars2 = validationScores\nbars3 = testScores\n \nr1 = np.arange(len(bars1))\nr2 = [x + barWidth for x in r1]\nr3 = [x + barWidth for x in r2]\n \nplt.bar(r1, bars1, color='blue', width=barWidth, edgecolor='white', label='train', yerr=0.5,ecolor=\"black\",capsize=10)\nplt.bar(r2, bars2, color='#557f2d', width=barWidth, edgecolor='white', label='validation', yerr=0.5,ecolor=\"black\",capsize=10, alpha = .50)\nplt.bar(r3, bars3, color='red', width=barWidth, edgecolor='white', label='test', yerr=0.5,ecolor=\"black\",capsize=10, hatch = '-')\n \nmodelNames = [\"GaussianNB\", 'BernoulliNB','LogisticRegression','RandomForestClassifier','SupportVectorMachine',\n             'DecisionTreeClassifier', 'KNeighborsClassifier','GradientBoostingClassifier',\n             'Stochastic Gradient Descent', 'Neural Nets']\n    \nplt.xlabel('Algorithms', fontweight='bold', size = 24)\nplt.ylabel('Scores', fontweight='bold', size = 24)\nplt.xticks([r + barWidth for r in range(len(bars1))], modelNames, rotation = 75)\n \nplt.legend()\nplt.show()","ad90f429":"for i in range(10):\n    print(f'Accuracy of {modelNames[i]} -----> {testScores[i]}')","a06e01cb":"models = {\n    'RandomForestClassifier': RandomForestClassifier(),\n    'DecisionTreeClassifier': DecisionTreeClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier(),\n}\n\nfor m in models:\n  model = models[m]\n  model.fit(X_train, y_train)\n  \n  print(f'{m}') \n  best_features = SelectFromModel(model)\n  best_features.fit(X, y)\n\n  transformedX = best_features.transform(X)\n  print(f\"Old Shape: {X.shape} New shape: {transformedX.shape}\")\n  print(\"\\n\")\n\n  imp_feature = pd.DataFrame({'Feature': features, 'Importance': model.feature_importances_})\n  plt.figure(figsize=(10,4))\n  plt.title(\"Feature Importance Graphic\")\n  plt.xlabel(\"importance \")\n  plt.ylabel(\"features\")\n  plt.barh(imp_feature['Feature'],imp_feature['Importance'],color = 'rgbkymc')\n  plt.show()","ae11a2e3":"models = {\n    'BernoulliNB': BernoulliNB(),\n    'LogisticRegression': LogisticRegression(),\n    'Stochastic Gradient Descent':  SGDClassifier(max_iter=5000, random_state=0),\n}\n\nfor m in models:\n  model = models[m]\n  model.fit(X_train, y_train)\n  \n  print(f'{m}') \n  best_features = SelectFromModel(model)\n  best_features.fit(X, y)\n\n  transformedX = best_features.transform(X)\n  print(f\"Old Shape: {X.shape} New shape: {transformedX.shape}\")\n  print(\"\\n\")","d6627137":"<a id=\"15\"><\/a> \n# Pandas Profiling","310980ce":"Source for Feature Importance Code: https:\/\/www.kaggle.com\/umutalpaydn\/heart-disease-analysis-classification#Feature-Importance","0a908584":"<a id=\"13\"><\/a> \n## specialisation - status","3cc536ab":"<a id=\"25\"><\/a> \n# Evaluation of Models","32c12afa":"<a id=\"1\"><\/a> \n# Importing the Necessary Libraries","2e3a5784":"### If you have questions please ask them on the comment section.","534785fe":"<a id=\"14\"><\/a> \n## Triple review","a55e0323":"<a id=\"11\"><\/a> \n## degree_t - status","73b50a61":"<a id=\"17\"><\/a> \n# Anomaly Detection","78723c3c":"<a id=\"26\"><\/a> \n# Conclusion","3d2b920a":"<a id=\"9\"><\/a> \n## ssc_b - status","20f052a5":"<a id=\"18\"><\/a> \n# Missing Values","40fad8be":"One Hot Encoding is the binary representation of categorical variables. This process requires categorical values to be mapped to integer values first. Next, each integer value is represented as a binary vector with all values zero except the integer index marked with 1.\n\nOne Hot Encoding makes the representation of categorical data more expressive and easy. Many machine learning algorithms cannot work directly with categorical data, so categories must be converted to numbers. This operation is required for input and output variables that are categorical.\n\nIn this part, I converted categorical datas to the binary values. This operation increases the accuracy.\n","b7bdfe6e":"<a id=\"6\"><\/a> \n### Numerical Variables","0ea44397":"*** Categorical Variables:** ['gender', 'ssc_b', 'hsc_b', 'hsc_s', 'degree_t', 'workex', 'specialisation', 'status']\n\n*** Numerical Variables:** ['ssc_p', 'hsc_p', 'degree_p', 'etest_p', 'mba_p', 'salary']","991e6bbc":"<a id=\"24\"><\/a> \n# Scores of Models","8beba1f3":"**When we look at those who do not have 'Salary' data, we see that 'status' = 0. This means that those with 'status' = 0 do not receive a salary. Therefore, I will replace these people's salary column with 0.**","0c1ae5f8":"Standardization is a method in which the mean value is 0 and the standard deviation is 1, and the distribution approaches the normal. The formula is as follows, we subtract the average value from the value we have, then divide it by the variance value.","52ec34dd":"Hello. I conducted an EDA and ML study on Campus Recruitment Dataset in this notebook. I performed Data Analysis on the dataset using visualization tools. Next, I tried using the data to predict whether a candidate would be hired on campus. I've added my comments and inferences under the code snippets.","5f587d21":"Content:\n\n1. [Importing the Necessary Libraries](#1)\n1. [Read Datas & Explanation of Features & Information About Datasets](#2)\n   1. [Variable Descriptions](#3)\n   1. [Univariate Variable Analysis](#4)\n      1. [Categorical Variables](#5)\n      1. [Numerical Variables](#6)\n1. [Basic Data Analysis](#7)\n   1. [gender](#8)\n   1. [ssc_b](#9)\n   1. [hsc_b](#10)\n   1. [degree_t](#11)\n   1. [workex](#12)\n   1. [specialisation](#13)\n   1. [Triple Review](#14)\n1. [Questions](#27)\n   1. [Does percentage matters for one to get placed?](#28)\n   1. [Which degree specialization is much demanded by corporate?](#29)\n1. [Pandas Profiling](#15)\n1. [Correlation](#16)\n1. [Anomaly Detection](#17)\n1. [Missing Values](#18)\n   1. [salary](#19)\n1. [Encoding](#20)\n   1. [Label Encoding](#21)\n   1. [One-Hot Encoding](#22)\n1. [Train-Test Split](#23)\n1. [Scores of Models](#24)\n1. [Evaluation of Models](#25)\n   1. [Another Question: Which factor influenced a candidate in getting placed?](#40)\n1. [Conclusion](#26)      ","460d16bb":"<a id=\"21\"><\/a> \n## Label Encoding","29757fe4":"<a id=\"3\"><\/a> \n## Variable Descriptions","dafd1717":"**When we look at the accuracy scores, we are predicting the 'status' with 88% accuracy with GBC and SVM. Also, when we look at the another statistical values such as, precision, recall, f1 score and specifity, we can conclude that out predictions are accurate.**","3c494a5d":"<a id=\"12\"><\/a> \n## workex - status","6e8aec34":"<a id=\"19\"><\/a> \n## salary","a6c61882":"**When we ask this question to the three classification algorithms, we see that the 'ssc_p' value is the most important property for the candidate to be 'placed'.**","63864b9b":"<a id=\"20\"><\/a> \n# Encoding","82de66a2":"I will perform Label Encoding for 'gender', 'ssc_b', 'hsc_b', 'workex' and 'specialisation'. For the others I will make One-Hot Encoding.","bb2e92f5":"## I have been denied access to my account. That's why I'm sharing it again.","b86b6f2f":"### I will be glad if you can give feedback.","05c7cfef":"Pandas profiling is a useful library that generates interactive reports about the data. With using this library, we can see types of data, distribution of data and various statistical information. This tool has many features for data preparing. Pandas Profiling includes graphics about specific feature and correlation maps too. You can see more details about this tool in the following url: https:\/\/pandas-profiling.github.io\/pandas-profiling\/docs\/master\/rtd\/","826babd8":"Anomaly is one that differs \/ deviates significantly from other observations in the same sample. An anomaly detection pattern produces two different results. The first is a categorical tag for whether the observation is abnormal or not; the second is a score or trust value. Score carries more information than the label. Because it also tells us how abnormal the observation is. The tag just tells you if it's abnormal. While labeling is more common in supervised methods, the score is more common in unsupervised and semisupervised methods.","84b0beb4":"<a id=\"2\"><\/a> \n# Read Datas & Explanation of Features & Information About Datasets","b6812d87":"<a id=\"8\"><\/a> \n## Gender - Status","43bf5875":"<a id=\"22\"><\/a> \n## One-Hot Encoding","ea9b0dde":"There is no null values anymore.","96632f01":"***As you can see, there is no outliar data.***","c9b7ee0f":"<a id=\"28\"><\/a> \n## Does percentage matters for one to get placed?","6ec23007":"**Why am I dropping the 'salary' column?**\n\n* When you train ML algorithms without dropping the 'salary' column, you will find that the accuracy is 100% for most. This is because those who are 'placed' have a salary value and those who are 'not placed' have a 'salary' value of 0.\n* When a classification is made in this way, those with 'salary' = 0 are directly classified as 'not placed'. While correct, we want to determine whether the candidates are 'placed' or 'not placed' based on their given qualifications.\n* Therefore, I will classify it by dropping the 'salary' column.","b4ef6bf9":"**As you can see in this barplot, Mkt&Fin specialization is much demanded by corporate.**","7e6babbf":"# Exploratory Data Analysis and Machine Learning Classification on Campus Recruitment\n","fa737511":"<a id=\"29\"><\/a> \n## Which degree specialization is much demanded by corporate?","5ea5aad6":"In this notebook, I examined Campus Recruitment Dataset. Firstly, I made Exploratory Data Analysis, Visualization, then I applied Machine Learning algorithms to this dataset. \n\nIf you liked this notebook, you may want to see my other notebooks :)\n\n* If you have questions, please comment them. I will try to explain if you don't understand.\n* If you liked this notebook, please let me know :)\n\n* ***Thank you for your time.***","ff2788ae":"We have 67 null values in total. salary includes all. ","5383211e":"<a id=\"4\"><\/a> \n## Univariate Variable Analysis","0fa4be81":"<a id=\"5\"><\/a> \n### Categorical Variables","8321d27b":"***sl_no:** Serial Number\n\n***gender:** Gender- Male='M',Female='F'\n\n***ssc_p:** Secondary Education percentage- 10th Grade\n\n***ssc_b:** Board of Education- Central\/ Others\n\n***hsc_p:** Higher Secondary Education percentage- 12th Grade\n\n***hsc_b:** Board of Education- Central\/ Others\n\n***hsc_s:** Specialization in Higher Secondary Education\n\n***degree_p:** Degree Percentage\n\n***degree_t:** Under Graduation(Degree type)- Field of degree education\n\n***workex:** Work Experience\n\n***etest_p:** Employability test percentage ( conducted by college)\n\n***specialisation:** Post Graduation(MBA)- Specialization\n\n***mba_p:** MBA percentage\n\n***status:** Status of placement- Placed\/Not placed\n\n***salary:** Salary offered by corporate to candidates\n\nSource: https:\/\/www.kaggle.com\/benroshan\/factors-affecting-campus-placement","bc51f5e9":"<a id=\"23\"><\/a> \n# Train - Test Split","d84bca72":"***I dropped 'id' column because it can cause unwanted correlation.***","f8032654":"<a id=\"7\"><\/a> \n# Basic Data Analysis","0af847ee":"<a id=\"10\"><\/a> \n## hsc_b - status","8eecbdbe":"<a id=\"40\"><\/a> \n## Another Question: Which factor influenced a candidate in getting placed?","acf565ca":"<a id=\"27\"><\/a> \n# Questions","40c4a0fd":"<a id=\"16\"><\/a> \n# Correlation","43f5622b":"Label Encoding is an encoding technique for handling categorical variables. In this technique, each data is assigned a unique integer.","324556e6":"**When we look at the graphics that placed above, we can see that percentage is important to get placed. The percentage value of those who are 'placed' starts from 55-60, while the percentage value of those who are 'not placed' starts from around 50. It can be said that the higher the 'degree_p' value, the higher the probability of being 'placed'.**","16a12fca":"These are the ML algorithms that will apply to dataset. Results will contain train-validation-test scores, confusion matrix, statistical information and classification reports for each algorithm.","f71a20ca":"I will handle Categorical Values.","b23d83cb":"Now, we don't have categorical variables. Dataset is ready for Machine Leraning algorithms."}}