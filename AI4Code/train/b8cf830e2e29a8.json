{"cell_type":{"f025cb67":"code","a90a5d41":"code","e5806992":"code","98e2e2e2":"code","b703433d":"code","b08fd537":"code","eff40036":"code","1744e9b4":"code","2e24b84c":"code","ce94229b":"code","48b350e5":"code","4984ce77":"code","6610df8e":"code","0de6886e":"code","936a1632":"code","d035f689":"markdown","b96f359a":"markdown","ddbb044d":"markdown","570d367a":"markdown","346b93f1":"markdown","1aed22f3":"markdown"},"source":{"f025cb67":"import pip._internal as pip\npip.main(['install', '--upgrade', 'numpy==1.17.2'])\nimport numpy as np\nimport pandas as pd\n\nimport glob\nimport re\n\nimport warnings\n# warnings.simplefilter(action = 'ignore', category = FutureWarning)\nwarnings.filterwarnings('ignore')\n\nfrom itertools import combinations\nfrom scipy.stats import chisquare\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom xgboost import XGBClassifier\n\nfrom lwoku import get_accuracy\n\nimport seaborn as sns\n%matplotlib inline\nimport matplotlib.pyplot as plt","a90a5d41":"tactic_03_results = pd.read_csv('..\/input\/tactic-99-summary\/tactic_03_results.csv', index_col='Id', engine='python')\n\nmodel = []\nS_test = []\nS_train = []\nfor index, row in tactic_03_results.iterrows():\n    model += [row['Model']]\n    S_test += [pd.read_csv('..\/input\/tactic-03-hyperparameter-optimization\/submission_' + row['Model'] + '.csv', index_col=0, engine='python')]\n    S_train += [pd.read_csv('..\/input\/tactic-03-hyperparameter-optimization\/train_' + row['Model'] + '.csv', index_col=0, engine='python')]","e5806992":"S_test = pd.concat(S_test, axis=1)\nS_test.columns = model\nS_test","98e2e2e2":"S_train = pd.concat(S_train, axis=1)\nS_train.columns = model\nS_train","b703433d":"tactic_03_results[['Model', 'Score']]","b08fd537":"low_scored_models = tactic_03_results[['Model', 'Score']].query('Score < 0.7')['Model'].tolist()\nprint('Drop models: {}'.format(low_scored_models))\nS_test.drop(low_scored_models, axis='columns', inplace=True)\nS_train.drop(low_scored_models, axis='columns', inplace=True)","eff40036":"def acc(p1, p2):\n    return np.sum(p1 == p2, axis=0) \/ float(p1.shape[0])\n\ncorr = S_test.corr(method=acc)\nf, ax = plt.subplots(figsize=(len(model), len(model)))\nsns.heatmap(corr, cmap=\"Oranges\", annot=True, fmt='.3f');","1744e9b4":"mean_corr = corr.mean()\nmean_corr","2e24b84c":"X = pd.read_csv('..\/input\/learn-together\/train.csv', index_col='Id', engine='python')\ny = X['Cover_Type'].copy()","ce94229b":"from sklearn.model_selection import GridSearchCV\nparameters = {\n    'n_estimators': range(5, 15),\n    'max_depth': [3, 4, 5],\n    'learning_rate': [x\/100 for x in range(5, 13)]\n}\nmodel = XGBClassifier(n_estimators=12, max_depth=3, learning_rate=0.08, random_state=0, n_jobs=-1)\nclf = GridSearchCV(model, parameters, cv=5)\nclf.fit(S_train, y)","48b350e5":"from grid_search_utils import plot_grid_search, table_grid_search\nplot_grid_search(clf)","4984ce77":"clf.best_estimator_","6610df8e":"results = pd.DataFrame(columns = ['Model combination',\n                                  'Accuracy'])\n\nmodel = XGBClassifier(n_estimators=8, max_depth=4, learning_rate=0.09, random_state=0, n_jobs=-1)\n\nimport time\nt0 = time.time()\n\n# Loop from single model to n models:\nfor n in range(1, len(mean_corr) + 1):\n    # Get combinations for n models\n    n_model_combinations = combinations(mean_corr.index, n)\n    # Loop for all the combinations of n models\n    for i in list(n_model_combinations):\n        n_model_combination = list(i)\n        accuracy = get_accuracy(model, S_train[n_model_combination], y, cv=3)\n        t1 = time.time()\n#         print('Model combination {}: {} in {} seconds'.format(n_model_combination, accuracy, (t1 - t0)))\n        t0 = t1\n        results = results.append({\n            'Model combination': n_model_combination,\n            'Accuracy': accuracy\n        }, ignore_index = True)\n\nresults = results.sort_values('Accuracy', ascending=False).reset_index(drop=True)","0de6886e":"results = results.sort_values('Accuracy', ascending=False).reset_index(drop=True)\nresults.to_csv('results.csv', index=True, index_label='Id')\nresults[0:10]","936a1632":"for index, row in results[0:10].iterrows():\n    model.fit(S_train[row['Model combination']], y)\n    y_test_pred = pd.Series(model.predict(S_test[row['Model combination']]), index=S_test.index)\n    name = '_'.join(row['Model combination'])\n    y_test_pred.to_csv('submission_' + name + '.csv', header=['Cover_Type'], index=True, index_label='Id')","d035f689":"## Drop low scored models","b96f359a":"# Read predictions","ddbb044d":"# 2nd level model","570d367a":"# Correlations","346b93f1":"# Submit\nFor the 10 first model combinations of the results, the model is fitted, predicted and submitted.","1aed22f3":"# Introduction\nThe aim of this notebook is to obtain the stack of optimized classifiers which maximise the accuracy in the train set.\nThen use this stack to predict the dependent variable in the test set.\n\nIn the previous notebook [Tactic 03. Hyperparameter optimization](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization),\nthere are the predictions (for train and test) of the analised models.\n\nThe first thing to look is the correlations of the predictions.\n\n---\n\nCredits to the Experts (Please like their kernels)  \nPaulo Pinto: [GMEAN of low correlation](https:\/\/www.kaggle.com\/paulorzp\/gmean-of-low-correlation-lb-0-952x)  \n[Nanashi](https:\/\/www.kaggle.com\/jesucristo)"}}