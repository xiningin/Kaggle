{"cell_type":{"5294ecdd":"code","9888b891":"code","f1338a9f":"code","b4ea17c0":"code","f37ce554":"code","9d973473":"code","faebf131":"code","1650fd7b":"code","c91c7ef4":"code","4d958b7d":"code","6f61bdac":"code","aebd5b70":"code","207f0842":"code","3d108028":"code","9f6c5002":"code","c384d6cd":"code","09511888":"code","bf716801":"code","931f2cff":"code","0bde79c9":"code","2e560449":"code","3fff58c4":"code","1ddf4aba":"code","93c842e5":"code","6e871e45":"code","486615f2":"code","a33cf99f":"code","d0c5aca8":"code","2308961a":"code","6b541ab7":"code","3cec2e8d":"code","c416e339":"code","9ce62b4e":"code","aadd6b42":"code","b702f1e5":"code","0100fd25":"code","0b5a87d0":"code","798636fd":"code","a4d6f6d6":"code","d498498e":"code","00744dab":"code","66ea06d0":"code","e1c334bf":"code","74b84da8":"code","814fc12d":"code","5543facd":"code","ea521456":"code","b1334280":"code","2e3312b5":"code","acb5378c":"code","9823dc69":"code","020531ba":"code","eb6a33bd":"code","6526cabf":"code","81cfd388":"code","64e04a3e":"code","1a399f8f":"code","85bd2899":"code","32c6d56b":"code","d96840d5":"code","b7b2aba0":"code","adc79c85":"code","59d395b7":"code","821270ee":"code","3314a76e":"code","e09aa83c":"code","1ba7f827":"code","002179dc":"code","459b4a15":"code","c477fd65":"code","97e6eb2a":"code","8986a21c":"code","787a024d":"code","71924a52":"code","7f97f043":"code","413e593a":"code","beab79d3":"markdown","f6bde4ce":"markdown","81050508":"markdown","4ac8af0a":"markdown","f9b9ab1c":"markdown","27459654":"markdown","432a53d3":"markdown","242c0ec2":"markdown","63c61ee3":"markdown","b5e40c45":"markdown","fb0916f9":"markdown","7bbb2bca":"markdown","c3e48870":"markdown","222acd26":"markdown","a3879ed9":"markdown","d8c25cb6":"markdown","46a2d3d1":"markdown","09afa4f1":"markdown","85cba760":"markdown","1fd312a8":"markdown","7351ec76":"markdown","c38ad18b":"markdown","9627e508":"markdown","cd3d5b3c":"markdown","a24b3715":"markdown","fae4ec55":"markdown","7f61d543":"markdown","079a1062":"markdown","247be67b":"markdown","92e926c8":"markdown","ac06ccf0":"markdown","ac5798af":"markdown","99c714f7":"markdown","475f49f0":"markdown","45080057":"markdown","4f3e2ba2":"markdown","323bfa47":"markdown","1e58c09c":"markdown","dbc7a2db":"markdown","0d3d1f91":"markdown","c5276372":"markdown","f6d0c1cf":"markdown","38b4a73c":"markdown","1b78619a":"markdown","37ec947e":"markdown","ecdd30f4":"markdown","5e0a026f":"markdown","8aa4268e":"markdown","04d16a8d":"markdown","8eddd35a":"markdown","994dfd8b":"markdown","6d49c7c7":"markdown","f30b3584":"markdown","ed63a69e":"markdown","c2b6d991":"markdown","df901236":"markdown","107213a9":"markdown","8eb112e7":"markdown","22ca23dc":"markdown","3cbf1ddf":"markdown","cd7f4991":"markdown","5ffe7b66":"markdown","b27bef27":"markdown","ad898bfe":"markdown","5dc9b19e":"markdown","d9676cd5":"markdown","d336604c":"markdown","bb83182d":"markdown","5ad000f5":"markdown","ac4fb69e":"markdown","958d69d9":"markdown","4c78318c":"markdown","a4d77fd9":"markdown","222f4572":"markdown","0e8e7c45":"markdown","225cc834":"markdown","bb7039eb":"markdown","2197979a":"markdown","6a2514f6":"markdown","c166f3dd":"markdown","245c68ab":"markdown","bdceca69":"markdown","14f10df3":"markdown","395e4958":"markdown","9bf0b82b":"markdown","5d1734cb":"markdown","0d212f9c":"markdown","9656eb79":"markdown","4f7ef96b":"markdown","acd94d8c":"markdown"},"source":{"5294ecdd":"# Import the library\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom pandas.io.json import json_normalize \nimport ipywidgets as widgets\nfrom ipywidgets import widgets, interact\nfrom plotly.offline import iplot\nimport plotly.graph_objs as go\n\nfilePath = '..\/input\/ntt-data-global-ai-challenge-06-2020\/COVID-19_and_Price_dataset.csv'","9888b891":"#Import Data\nworldData = pd.read_csv(filePath)","f1338a9f":"print('Number of rows : ', worldData.shape[0])\nprint('Number of features : ', worldData.shape[1])","b4ea17c0":"worldData.head(5)","f37ce554":"#descriptive statistics summary\nworldData['Price'].describe()","9d973473":"# Prob distribution\nsns.distplot(worldData['Price']);","faebf131":"trace0 = go.Scatter(\n    x = worldData['Date'],\n    y = worldData['Price'],\n    mode = 'lines',\n    name = 'lines'\n)\n\ndata = [trace0]  # assign traces to data\nlayout = go.Layout(\n    title = 'Oil Price Trend during Covid',\n    xaxis = dict(title = 'Date'), # x-axis label\n    yaxis = dict(title = 'Oil Price'), # y-axis label\n    hovermode ='closest' \n)\nfig = go.Figure(data=data,layout=layout)\niplot(fig)","1650fd7b":"colums = ['World_total_cases','World_total_deaths','UnitedStates_total_cases','Russia_total_cases',\n          'UnitedKingdom_total_cases','Spain_total_cases','Germany_total_cases','France_total_cases',\n          'Singapore_total_cases','Italy_total_cases','Israel_total_cases','Brazil_total_cases',\n          'UnitedArabEmirates_total_cases','SaudiArabia_total_cases','Qatar_total_cases','Turkey_total_cases',\n          'Iran_total_cases','Oman_total_cases','Kuwait_total_cases','Egypt_total_cases','China_total_cases',\n          'India_total_cases','Price']\ncorrelation = pd.DataFrame(worldData[colums].corr(method ='pearson'))\n","c91c7ef4":"f, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(correlation, vmax=1, square=True);","4d958b7d":"#scatterplot\nsns.set()\ncolums = ['World_total_cases','UnitedStates_total_cases',\n          'Italy_total_cases','UnitedArabEmirates_total_cases','SaudiArabia_total_cases',\n          'Iran_total_cases',\n          'Price']\nsns.pairplot(worldData[colums], height = 2.5)\nplt.show();","6f61bdac":"# Custom functions \n#Base Variables->\ninputDetails = {'cb1': '', 'cb2': '', 'cb3': '', 'cb4': '', 'cb5': '','cb6': '', \n                'threshold_from' : '', 'threshold_to':'', 'cb7': ''}\nselectedColumns = []\nColumnNameArrcorr = np.array([])\nfilteredWorlCovidData = []\n\ndef seperator():\n    print('-------------------------------------------------------------------------------------')\n\ndef processData():\n    #All Column Name\n    colNameArr = np.array(list(worldData.columns))\n\n    segColNames = { \n        'totalCasesColumnNameArr': [] , \n        'newCasesColumnNameArr': [], \n        'totalDeathCasesColumnNameArr': [],\n        'newDeathCasesColumnNameArr': [],\n        'price':[]\n    }\n\n    totalCasesColumnNameArr = []\n    newCasesColumnNameArr = []\n    totalDeathCasesColumnNameArr = []\n    newDeathCasesColumnNameArr = []\n    price = []\n    world = []\n\n    # Segregating the data into different Arrays\n    for items in colNameArr:\n        if 'total_cases' in items:\n            totalCasesColumnNameArr.append(items)\n\n    for items in colNameArr:\n        if 'new_cases' in items:\n            newCasesColumnNameArr.append(items)\n\n    for items in colNameArr:\n        if 'total_deaths' in items:\n            totalDeathCasesColumnNameArr.append(items)\n\n    for items in colNameArr:\n        if 'new_deaths' in items:\n            newDeathCasesColumnNameArr.append(items)\n\n    for items in colNameArr:\n        if 'Price' in items:\n            price.append(items)\n\n    for items in colNameArr:\n        if 'World' in items:\n            world.append(items)        \n        \n    segColNames['totalCasesColumnNameArr'] = totalCasesColumnNameArr\n    segColNames['newCasesColumnNameArr'] = newCasesColumnNameArr\n    segColNames['totalDeathCasesColumnNameArr'] = totalDeathCasesColumnNameArr\n    segColNames['newDeathCasesColumnNameArr'] = newDeathCasesColumnNameArr\n    segColNames['price'] = price\n    segColNames['world'] = world\n\n    if inputDetails['cb1'] == True:\n        for item in segColNames['totalCasesColumnNameArr']:\n            selectedColumns.append(item)\n    if inputDetails['cb2'] == True:\n        for item in segColNames['newCasesColumnNameArr']:\n            selectedColumns.append(item)\n    \n    if inputDetails['cb3'] == True:\n        for item in segColNames['totalDeathCasesColumnNameArr']:\n            selectedColumns.append(item)\n    \n    if inputDetails['cb4'] == True:\n        for item in segColNames['newDeathCasesColumnNameArr']:\n            selectedColumns.append(item)\n    \n    if inputDetails['cb5'] == True:\n        for item in segColNames['price']:\n            selectedColumns.append(item)\n\n    # selectedColumns\n    for i in range(len(selectedColumns)):\n        if 'World' in item:\n            del selectedColumns[i]\n\n    if inputDetails['cb6'] == True:\n        for item in segColNames['world']:\n            selectedColumns.append(item)        \n\n\n    # Find the Relation Matrix\n    filteredWorlCovidData = pd.read_csv(filePath, usecols=selectedColumns )\n    label_encoder = LabelEncoder()\n    filteredWorlCovidData.iloc[:,0] = label_encoder.fit_transform(filteredWorlCovidData.iloc[:,0]).astype('float64')\n    corrWorlCovidData = filteredWorlCovidData.corr()\n    ColumnNameArrcorr = np.array(corrWorlCovidData)\n    rowNumber = ColumnNameArrcorr.shape[0]\n    colNumber = ColumnNameArrcorr.shape[1]\n    print(ColumnNameArrcorr)\n    seperator()\n    \n    if inputDetails['cb7'] == True :\n        # making a Relational List <= threshold\n        not_related = []\n        not_related_line = { 'COL1': '', 'COL2': '', 'CORRVALUE' : 0}\n    \n        for i in range(rowNumber):\n            for j in range(colNumber):\n                if ( float(inputDetails['threshold_from']) <= ColumnNameArrcorr[i, j] <= float(inputDetails['threshold_to'])):\n                    not_related_line['COL1'] = filteredWorlCovidData.columns[i]\n                    not_related_line['COL2'] = filteredWorlCovidData.columns[j]\n                    not_related_line['CORRVALUE'] = ColumnNameArrcorr[i, j]\n                    not_related.append(not_related_line)\n                    not_related_line = { 'COL1': '', 'COL2': '', 'CORRVALUE' : 0}\n            \n        finalCorr = pd.DataFrame(json_normalize(not_related))\n\n        #Export the Data to CSV\n        exportToExcel = finalCorr.sort_values(by=['CORRVALUE'])\n        exportToExcel.to_csv (r'.\\exportCorr.csv', index = False, header=True)\n        print('Data is exported to exportCorr.csv')\n        seperator()\n    if inputDetails['cb8'] == True :\n        # fig = sns.heatmap(corrWorlCovidData,center=0, cmap='BrBG', annot=True,linecolor='white', linewidths=1)\n        sfig = sns.heatmap(corrWorlCovidData, annot=True,cmap='coolwarm', linecolor='white', linewidths=1)\n        # fig\n        figure = sfig.get_figure()    \n        figure.savefig('exportCorr.png', dpi=1080) \n        print('HeatMap is exported to exportCorr.png')\n        seperator()\n    \n    \n","aebd5b70":"# Create text widget for output\ninput_threshold_from = widgets.Text( placeholder='Threshold from',value='-.9')\ninput_threshold_to = widgets.Text( placeholder='Threshold to',value='.9')\n\ncb1 = widgets.Checkbox(description=\"Total Case\")\ncb2 = widgets.Checkbox(despcription=\"New Case\")\ncb3 = widgets.Checkbox(description=\"Total Death Case\")\ncb4 = widgets.Checkbox(despcription=\"New Death Case\")\ncb5 = widgets.Checkbox(despcription=\"Price\",value=True)\ncb6 = widgets.Checkbox(despcription=\"World\",value=True)\ncb7 = widgets.Checkbox(despcription=\"Take CSV Backup\",value=True)\ncb8 = widgets.Checkbox(despcription=\"Take IMG Backup\")\n\npb = widgets.Button(\n    description='Submit Choices',\n    disabled=False,\n    value='1',\n    button_style='success',\n    tooltip='Submit Input'\n)\n\n\ndef on_button_clicked(b):\n    inputDetails['cb1'] = cb1.value\n    inputDetails['cb2'] = cb2.value\n    inputDetails['cb3'] = cb3.value\n    inputDetails['cb4'] = cb4.value\n    inputDetails['cb5'] = cb5.value\n    inputDetails['cb6'] = cb6.value\n    inputDetails['cb7'] = cb7.value\n    inputDetails['cb8'] = cb8.value\n    inputDetails['threshold_from'] = input_threshold_from.value\n    inputDetails['threshold_to'] = input_threshold_to.value\n    print('Selected Value: ', inputDetails)\n    processData()\n    \npb.on_click(on_button_clicked)\n\nui = widgets.VBox([widgets.HBox([cb1, cb2, cb5]), \n                   widgets.HBox([cb3, cb4, cb6]), \n                   widgets.HBox([input_threshold_from, input_threshold_to]),widgets.HBox([pb,cb7,cb8])])\n\n\ndisplay(ui)\n# for some reasons I need to reset descriptions\ncb1.description=\"Total Case\"\ncb2.description=\"New Case\"\ncb3.description=\"Total Death Case\"\ncb4.description=\"New Death Case\"\ncb5.description=\"Price\"\ncb6.description=\"World\"\ncb7.description=\"Take CSV Backup\"\ncb8.description=\"Take IMG Backup\"","207f0842":"data = worldData[['World_total_cases','Price']]","3d108028":"data['World_total_cases'].describe()","9f6c5002":"# Spliting data to training and test sets----------------->\ntrain = data.iloc[:92,:].values\ntrain = pd.DataFrame(train)\ntrain.columns = ['World_total_cases','Price']\n\ntest = data.iloc[92:125,:].values\ntest = pd.DataFrame(test)\ntest.columns = ['World_total_cases','Price']","c384d6cd":"##Min-Max Normalization\ndef norm_train(x):\n  return (x - train_stats['mean']) \/ (train_stats['max']-train_stats['min'])\ndef norm_test(x):\n  return (x - test_stats['mean']) \/(test_stats['max']-test_stats['min'])\n\ntrain_stats = train['World_total_cases'].describe()\ntrain_stats = train_stats.transpose()\n\ntest_stats = test['World_total_cases'].describe()\ntest_stats = test_stats.transpose()\n\ntrain['World_total_cases'] = norm_train(train['World_total_cases'])\ntest['World_total_cases'] = norm_train(test['World_total_cases'])\n","09511888":"train['World_total_cases'].describe()","bf716801":"test['World_total_cases'].describe()","931f2cff":"# import Linear Regression lib \nfrom sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nX = np.reshape(train['World_total_cases'].values ,(92,1))\ny = np.reshape(train['Price'].values ,(92,1))\n","0bde79c9":"\nlin_reg.fit(X, y)","2e560449":"\n# Visualizing the Linear Regression results\ndef viz_linear():\n    plt.scatter(X, y, color='red')\n    plt.plot(X, lin_reg.predict(X), color='blue')\n    plt.title('Oil Price Prediction on training data (Linear Regression)')\n    plt.xlabel('Total Number of Covid Cases')\n    plt.ylabel('Oil Prices')\n    plt.show()\n    return\nviz_linear()","3fff58c4":"X_test = np.reshape(test['World_total_cases'].values,(33,1))\ny_test = np.reshape(test['Price'].values,(33,1))\n#Let 's predict oil prices on test set for next 5 days\npred_linreg = lin_reg.predict(X_test)\npred_linreg = pd.DataFrame(pred_linreg.flatten())\npred_linreg.head(5)","1ddf4aba":"#Let's check what were the actual prices in test set for next 5 days\ntest['Price'].head()","93c842e5":"X_test = np.reshape(test['World_total_cases'].values,(33,1))\ny_test = np.reshape(test['Price'].values,(33,1))\n# Visualizing the Linear Regression results\ndef viz_linear():\n    plt.scatter(X_test, y_test, color='red')\n    plt.plot(X_test, lin_reg.predict(X_test), color='blue')\n    plt.title('Oil Price Prediction on test data (Linear Regression)')\n    plt.xlabel('Total Number of Covid Cases')\n    plt.ylabel('Oil Prices')\n    plt.show()\n    return\nviz_linear()","6e871e45":"# Fitting Polynomial Regression to the dataset: degree = 3\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree=2)\nX_poly = poly_reg.fit_transform(X)\npol_reg = LinearRegression()\npol_reg.fit(X_poly, y)\n\n# Visualizing the Polymonial Regression results\ndef viz_polymonial():\n    plt.scatter(X, y, color='red')\n    plt.plot(X, pol_reg.predict(poly_reg.fit_transform(X)), color='blue')\n    plt.title('Oil Price Prediction on train data (Polynomial Regression)')\n    plt.xlabel('Total Number of Covid Cases')\n    plt.ylabel('Oil Prices')\n    plt.show()\n    return\nviz_polymonial()","486615f2":"#Let 's predict oil prices on test set for next 5 days\npred_poly  =  pol_reg.predict(poly_reg.fit_transform(X_test))\npred_poly = pd.DataFrame(pred_poly.flatten())\npred_poly.head(5)","a33cf99f":"# Visualizing the Polymonial Regression results\ndef viz_polymonial():\n    plt.scatter(X_test, y_test, color='red')\n    plt.plot(X_test, pol_reg.predict(poly_reg.fit_transform(X_test)), color='blue')\n    plt.title('Oil Price Prediction on test data (Polynomial Regression)')\n    plt.xlabel('Total Number of Covid Cases')\n    plt.ylabel('Oil Prices')\n    plt.show()\n    return\nviz_polymonial()","d0c5aca8":"import xgboost as xgb","2308961a":"gbm = xgb.XGBRegressor(colsample_bytree = 1, learning_rate = 0.1,\nmax_depth = 10 , n_estimators = 50)","6b541ab7":"gbm.fit(X,y)","3cec2e8d":"# Visualizing the results\ndef viz_linear():\n    plt.scatter(X, y, color='red')\n    plt.plot(X, gbm.predict(X), color='blue')\n    plt.title('Oil Price Prediction on training data (Xgboost)')\n    plt.xlabel('Total Number of Covid Cases')\n    plt.ylabel('Oil Prices')\n    plt.show()\n    return\nviz_linear()","c416e339":"def viz_linear():\n    plt.scatter(X_test, y_test, color='red')\n    plt.plot(X_test, gbm.predict(X_test), color='blue')\n    plt.title('Oil Price Prediction on test data (Xgboost)')\n    plt.xlabel('Total Number of Covid Cases')\n    plt.ylabel('Oil Prices')\n    plt.show()\n    return\nviz_linear()","9ce62b4e":"import tensorflow as tf\nfrom tensorflow import keras\nmodel = tf.keras.Sequential([keras.layers.Dense(units=64,activation='tanh', input_shape=[1]),\n                             keras.layers.Dense(units=128,activation='tanh'),\n                                 keras.layers.Dense(units=1)])\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(0.001), loss='mean_squared_error')\nmodel.summary()","aadd6b42":"model.fit(X, y, epochs=500)","b702f1e5":"# Visualizing the results\ndef viz_linear():\n    plt.scatter(X, y, color='red')\n    plt.plot(X, model.predict(X), color='blue')\n    plt.title('Oil Price Prediction on training data (Tensorflow)')\n    plt.xlabel('Total Number of Covid Cases')\n    plt.ylabel('Oil Prices')\n    plt.show()\n    return\nviz_linear()","0100fd25":"def viz_linear():\n    plt.scatter(X_test, y_test, color='red')\n    plt.plot(X_test, model.predict(X_test), color='blue')\n    plt.title('Oil Price Prediction on test data (Xgboost)')\n    plt.xlabel('Total Number of Covid Cases')\n    plt.ylabel('Oil Prices')\n    plt.show()\n    return\nviz_linear()","0b5a87d0":"# Import Lib\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport seaborn as sns\nimport plotly.express as px\nimport ipywidgets as widgets\nfrom ipywidgets import widgets, interact\n\nfilePath = '..\/input\/ntt-data-global-ai-challenge-06-2020\/Crude_oil_trend.csv'","798636fd":"sns.set_style('whitegrid')\nraw_df = pd.read_csv(filePath)\nraw_df.head(5)","a4d6f6d6":"df = raw_df.groupby(['Date'])['Price'].sum().reset_index()\ndf['Price'].plot(kind='hist')\nprint(df['Price'].describe())","d498498e":"bin_labels10 = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\ndf['quantile_ex_1'] = pd.qcut(df['Price'],q=10,labels=bin_labels10)\n\nbin_labels20 = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n                '11', '12', '13', '14', '15', '16', '17', '18', '19', '20']\ndf['quantile_ex_2'] = pd.qcut(df['Price'],q=20,labels=bin_labels20)\n\ndf.head()\n# df['quantile_ex_1'].value_counts()\n# df['quantile_ex_2'].value_counts()","00744dab":"new_df = pd.DataFrame(df)\ndf_melt = new_df.melt(id_vars='Date', value_vars=['quantile_ex_1', 'quantile_ex_2'])\nfig = px.line(df_melt, x='Date' , y='value' , color='variable')\n\nfig.update_xaxes(\n    rangeslider_visible=True,\n    rangeselector=dict(\n        buttons=list([\n            dict(count=1, label=\"1 month\", step=\"month\", stepmode=\"backward\"),\n            dict(count=6, label=\"6 month\", step=\"month\", stepmode=\"backward\"),\n            dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n            dict(count=1, label=\"1 year\", step=\"year\", stepmode=\"backward\"),\n            dict(step=\"all\")\n        ])\n    )\n)\nfig.show()","66ea06d0":"# Count the number of quantile_ex_2 Changes\n\n# Count the Similar number of quantile_ex_2 Chnages in past records\n\n# Find the Projection on the Data on Arima\n\n# Try to find out the pattern","e1c334bf":"from pandas.io.json import json_normalize\n\nbinArr = np.array(df)\n\ndataQuantileRanges1 = []\ndataQuantileRanges2 = []\n\ndataQuantileRangesLine = {'BINNO': '', 'LOW': '', 'HIGH': '', 'COUNT': 0, 'DELTA' : 0}\n\nfor binno in range(1,21):\n    binVal = []\n    for item in binArr:\n        if item[3] == str(binno):\n            binVal.append(item[1])\n    dataQuantileRangesLine['BINNO'] = 'Bin no - ' + str(binno)\n    dataQuantileRangesLine['LOW'] = round(np.array(binVal).min(),2)\n    dataQuantileRangesLine['HIGH'] = round(np.array(binVal).max(),2)\n    dataQuantileRangesLine['COUNT'] = len(np.array(binVal))\n    dataQuantileRangesLine['DELTA'] = round(np.array(binVal).max(),2) - round(np.array(binVal).min(),2)\n    dataQuantileRanges2.append(dataQuantileRangesLine)\n    dataQuantileRangesLine = {'BINNO': '', 'LOW': '', 'HIGH': '', 'COUNT': 0, 'DELTA' : 0}\n\ndf_dataQuantileRanges2 = json_normalize(dataQuantileRanges2)\n\nfor binno in range(1,11):\n    binVal = []\n    for item in binArr:\n        if item[2] == str(binno):\n            binVal.append(item[1])\n    dataQuantileRangesLine['BINNO'] = 'Bin no - ' + str(binno)\n    dataQuantileRangesLine['LOW'] = round(np.array(binVal).min(),2)\n    dataQuantileRangesLine['HIGH'] = round(np.array(binVal).max(),2)\n    dataQuantileRangesLine['COUNT'] = len(np.array(binVal))\n    dataQuantileRangesLine['DELTA'] = round(np.array(binVal).max(),2) - round(np.array(binVal).min(),2)\n    dataQuantileRanges1.append(dataQuantileRangesLine)\n    dataQuantileRangesLine = {'BINNO': '', 'LOW': '', 'HIGH': '', 'COUNT': 0, 'DELTA' : 0}\n\ndf_dataQuantileRanges1 = json_normalize(dataQuantileRanges1)\n\nquantile_ex_1 = df['quantile_ex_1'].value_counts()\nquantile_ex_2 = df['quantile_ex_2'].value_counts()","74b84da8":"from IPython.display import clear_output\nradbtn = widgets.RadioButtons( \n            options=['Display quantile_ex_1', \n                     'Display quantile_ex_2', \n                     'dataQuantileRanges1', \n                     'dataQuantileRanges2'],\n    description='Your Choice',\n    disabled=False )\ndef on_button_clicked(b):\n    clear_output(wait=False)\n    display(radbtn,pb)\n    if radbtn.value == 'Display quantile_ex_1':\n        print(quantile_ex_1)\n    if radbtn.value == 'Display quantile_ex_2':\n        print(quantile_ex_2)\n    if radbtn.value == 'dataQuantileRanges1':\n        print(df_dataQuantileRanges1)\n    if radbtn.value == 'dataQuantileRanges2':\n        print(df_dataQuantileRanges2)  \n    \n    \npb = widgets.Button(\n    description='Submit Choices',\n    disabled=False,\n    value='1',\n    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n    tooltip='Submit Input'\n)\n\npb.on_click(on_button_clicked)\n\ndisplay(radbtn,pb)","814fc12d":"# Define the d and q parameters to take any value between 0 and 1\nq = range(0, 3)\nd = range(0, 2)\n# Define the p parameters to take any value between 0 and 3\np = range(0, 4)","5543facd":"import warnings\nimport itertools\n# Generate all different combinations of p, q and q triplets\npdq = list(itertools.product(p, d, q))","ea521456":"from statsmodels.tsa.arima_model import ARIMA\n# pdq\n\nnew_df1 = df[['Date','quantile_ex_2']].copy()\ntrain = new_df1\ntrain.set_index(['Date'], inplace=True)\ntrain['quantile_ex_2']=pd.to_numeric(train['quantile_ex_2'])\ntrain\n# train.set_index(['Date'], inplace=True)\n# train = \ntrain.plot()\nplt.ylabel('quantile_ex_2')\nplt.xlabel('Date')\nplt.show()","b1334280":"model = ARIMA(train,order= (2,2,0)) # 2,2,0\nresults = model.fit(disp=0)","2e3312b5":"forcastFromPast = results.forecast(28)","acb5378c":"forcastFromPast","9823dc69":"preds = forcastFromPast[0]\npreds.astype(int)","020531ba":"# last 300 days array from Data frame\n# Seperate line for frame\n\nresults.plot_predict()","eb6a33bd":"# Import the necessary libs\nimport matplotlib.pyplot as plt\nimport cv2\nfrom sklearn.cluster import KMeans\nimport ntpath\n\n# Mention the file path to the Image which is to be considered \nfilePath = '..\/input\/ntt-data-global-ai-challenge-06-2020\/NTL-dataset\/tif\/USA-20200101.tif'\n# filePath = '.\/ntt-data-global-ai-challenge-06-2020\/NTL-dataset\/tif\/Italy-S20200101-E20200110.tif'\nfileName = ntpath.basename(filePath)\nprint(fileName)","6526cabf":"# Now plot Map Display in Pyplot\n\nimage = cv2.imread(filePath)\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\nplt.figure()\nplt.axis(\"off\")\nplt.imshow(image)\n","81cfd388":"# Get the Data again into the variable \"img\"\n# This will give you the idea on how many cluster have to be choosen for KMEANS\n# Clearly 5 sparks you are getting from the data\n\nimg = cv2.imread(filePath)\ncolor = ('b','g','r')\n\nfor i,col in enumerate(color):\n    histr = cv2.calcHist([img],[i],None,[256],[0,256])\n    plt.plot(histr,color = col)\n    plt.xlim([0,256])\n    plt.show()","64e04a3e":"image = image.reshape((image.shape[0] * image.shape[1], 3))\n#Set the Cluster\nclt = KMeans(n_clusters = 4)\nclt.fit(image)","1a399f8f":"def centroid_histogram(clt):\n    numLabels = np.arange(0, len(np.unique(clt.labels_)) + 1) \n    (hist, _) = np.histogram(clt.labels_, bins = numLabels)\n    hist = hist.astype(\"float\")\n    hist \/= hist.sum()\n    return hist\n\ndef plot_colors(hist, centroids):\n    Legend = np.zeros((10, 90, 3), dtype = \"uint8\")\n    bar = np.zeros((50, 200, 3), dtype = \"uint8\")\n    startX = 0\n    hist_array = []\n    for (percent, color) in zip(hist, centroids):    \n        endX = startX + (percent * 200)\n        cv2.rectangle(bar, (int(startX), 0), (int(endX), 50),color.astype(\"uint8\").tolist(), -1)\n        startX = endX\n        hist_array_line = { \n            'percent1': '', \n            'color1': ''\n        }\n        hist_array_line['percent1'] = percent\n        hist_array_line['color1'] = color\n        hist_array.append(hist_array_line)\n    return bar , hist_array\n\n############################\n# printbar -> Plots the Bar Graph\n# printvalue -> Plots the Value\n# hist_array -> needed for Hist Value plot\n# bar -> Plot the bar diagram\n##############################\n\ndef plotAnalysis(printbar,printvalue,clt):\n\n    hist = centroid_histogram(clt)\n    bar, hist_array = plot_colors(hist, clt.cluster_centers_)\n    \n    blank_img = np.ones(shape=(200,800,3),dtype=np.int16)\n    blank_img = blank_img * 255\n    blank_img\n\n    lineNumber = 1\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    for line in hist_array:\n        dynamicYStart = 20 * lineNumber\n        dynamicYEnd = dynamicYStart + 20\n        runColor = line['color1'].astype(\"uint8\").tolist()\n        cv2.putText(blank_img,text=str(line['percent1']),org=(60,dynamicYStart*2 + 10 ),fontFace=font,fontScale=1,color=runColor,thickness=3,lineType=cv2.LINE_AA)\n        lineNumber = lineNumber + 1\n    \n    if printbar == 'X':\n        plt.figure()\n        plt.axis(\"off\")\n        plt.imshow(bar)\n    if printvalue == 'X':\n        plt.figure()\n        plt.axis(\"off\")\n        plt.imshow(blank_img)\n    return hist_array","85bd2899":"# Let's build a histogram of those clusters \n# Now Represent the number of pixels as color\n\nplotAnalysis('X', # If X then print the bar\n             'X', # If X then print the legend\n             clt)","32c6d56b":"import numpy as np \nimport matplotlib.pyplot as plt\nimport cv2\nfrom sklearn.cluster import KMeans\nimport ntpath\nimport os\n\ncountrylist = []\nfilePath = '..\/input\/ntt-data-global-ai-challenge-06-2020\/NTL-dataset\/tif\/'\nfilelist = os.listdir(filePath)\n\nimageDataArr = []\n\nfor file in filelist:\n    imageData = { 'country' : '', 'date':'', 'filename': '', 'fullfilepath':'', 'image' : [], 'clt': '' }\n    country = file.split(\"-\")\n    imageData\n    if country[0] not in countrylist:\n        countrylist.append(country[0])\n    imageData['country'] = country[0]\n    imageData['date'] = country[1].split(\".\")[0]\n    imageData['filename'] = file\n    imageData['fullfilepath'] = filePath + file\n    \n    image = cv2.imread(imageData['fullfilepath'])\n    imageData['image'] = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n#   You can do the below section to execute all the KMEANS at a same time  \n#     # Kmeans\n#     image = image.reshape((image.shape[0] * image.shape[1], 3))\n#     clt = KMeans(n_clusters = 4)\n#     imageData['clt'] = clt.fit(image)\n    \n    imageDataArr.append(imageData)\n        \n\n# All file details in -> imageDataArr\n# All Country Name -> countrylist","d96840d5":"countrylist","b7b2aba0":"import pandas as pd\nfrom pandas.io.json import json_normalize \n\ndef saveintofile():\n    histDataArr = []\n    \n    for line in trainedData:\n        imageData = line\n        imageData['hist'] = plotAnalysis('', '', line['clt'])\n        histDataArr.append(imageData)\n\n    def arrangeHist(hist):\n        finalReturn = []\n        for line in hist:\n            finalReturnLine = {}\n            finalReturnLine['percent'] = line['percent1']\n            finalReturnLine['color'] = line['color1'][0]\n            finalReturn.append(finalReturnLine)\n        pd_finalReturn = pd.DataFrame(json_normalize(finalReturn))\n        pd_finalReturn = pd_finalReturn.sort_values(by=['color'])\n        histDict = {}\n        histDict['PERCENTCAT1'] = np.array(pd_finalReturn)[0][0]\n        histDict['PERCENTCAT2'] = np.array(pd_finalReturn)[1][0]\n        histDict['PERCENTCAT3'] = np.array(pd_finalReturn)[2][0]\n        histDict['PERCENTCAT4'] = np.array(pd_finalReturn)[3][0]\n\n        histDict['COLORCAT1'] = np.array(pd_finalReturn)[0][1]\n        histDict['COLORCAT2'] = np.array(pd_finalReturn)[1][1]\n        histDict['COLORCAT3'] = np.array(pd_finalReturn)[2][1]\n        histDict['COLORCAT4'] = np.array(pd_finalReturn)[3][1]    \n    \n        return histDict\n\n\n\n    refinedData = []\n    for line in histDataArr:\n        refinedDataline = {}\n        refinedDataline['date'] = pd.to_datetime(str(line['date']),format='%Y%m%d')\n        refinedDataline['filename'] = line['filename']\n        refinedDataline['fullfilepath'] = line['fullfilepath']\n        hist = arrangeHist(line['hist'])\n        refinedDataline['PERCENTCAT1'] = hist['PERCENTCAT1']\n        refinedDataline['PERCENTCAT2'] = hist['PERCENTCAT2']\n        refinedDataline['PERCENTCAT3'] = hist['PERCENTCAT3']\n        refinedDataline['PERCENTCAT4'] = hist['PERCENTCAT4']\n    \n        refinedDataline['COLORCAT1'] = hist['COLORCAT1']\n        refinedDataline['COLORCAT2'] = hist['COLORCAT2']\n        refinedDataline['COLORCAT3'] = hist['COLORCAT3']\n        refinedDataline['COLORCAT4'] = hist['COLORCAT4']\n        refinedData.append(refinedDataline)\n\n    # Data Saved to excel\n    df = pd.DataFrame(refinedData)\n    transformedDataFilename = \".\/refinedData\" + dropdown.value + \".xlsx\"\n    df.to_excel(excel_writer = transformedDataFilename)\n","adc79c85":"import ipywidgets as widgets\nfrom ipywidgets import widgets, interact\n\ntrainedData = []\nselcountrydetails = []\n\npb = widgets.Button(\n    description='Submit Choices',\n    disabled=False,\n    value='1',\n    button_style='success',\n    tooltip='Submit Input'\n)\n\ndef trainData():\n    run = 0\n    len(selcountrydetails)\n    for line in selcountrydetails:\n        imageData = { 'country' : '', 'date':'', 'filename': '', 'fullfilepath':'', 'image' : [], 'clt': '' }\n        imageData = line\n        image = cv2.imread(line['fullfilepath'])    \n        ###################################\n        # Kmeans\n        image = image.reshape((image.shape[0] * image.shape[1], 3))\n        clt = KMeans(n_clusters = 4)\n        imageData['clt'] = clt.fit(image)     \n        ###################################\n        trainedData.append(imageData)\n        print(run)\n        run = run+1\n    print('--------Training Completed-------')\n    \n    #Convert the trained dataset to an Excel File\n    saveintofile()\n\ndef on_button_clicked(b):\n    print('Selected Country: ',dropdown.value)    \n    for line in imageDataArr:\n        if ( dropdown.value == line['country']):\n            selcountrydetails.append(line)\n    print('Image Dataset Length: ',len(selcountrydetails))    \n    print('--------Training Started-------')\n    trainData()\n\npb.on_click(on_button_clicked)\n\ndropdown = widgets.Dropdown(\n    options=countrylist,\n    description='Number:',\n    disabled=False\n)\n\nui = widgets.VBox([widgets.HBox([dropdown, pb])])\ndisplay(ui)","59d395b7":"import ipywidgets as widgets\nfrom ipywidgets import widgets, interact\nimport pandas as pd\nfrom pandas.io.json import json_normalize \n\nhistImageCountry = ''\ndef fetchFromExcel(b):\n    global histImageCountry\n    fileName = \".\/refinedData\" + dropdown.value + \".xlsx\"\n    histImageCountry = pd.read_excel(fileName)\n    histImageCountry.sort_values(\"date\",axis = 0, ascending = True, inplace = True)\n    \n    print('Data is loaded now to play with')\n\npb1 = widgets.Button(\n    description='Submit Choices',\n    disabled=False,\n    value='1',\n    button_style='success',\n    tooltip='Submit Input'\n)\n\npb1.on_click(fetchFromExcel)\n\ndropdown = widgets.Dropdown(\n    options=countrylist,\n    description='Number:',\n    disabled=False\n)\n\nui = widgets.VBox([widgets.HBox([dropdown, pb1])])\ndisplay(ui)","821270ee":"# Lets Plot directly\nimport plotly.express as px\ndf_melt = histImageCountry.melt(id_vars='date', value_vars=['PERCENTCAT1','PERCENTCAT2','PERCENTCAT3','PERCENTCAT4'])\nfig = px.line(df_melt, x='date' , y='value' , color='variable')\n\nfig.update_xaxes(\n    rangeslider_visible=True,\n    rangeselector=dict(\n        buttons=list([\n            dict(count=1, label=\"1 month\", step=\"month\", stepmode=\"backward\"),\n            dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n            dict(step=\"all\")\n        ])\n    )\n)\nfig.show()","3314a76e":"createRatio = []\nfor line in np.array(histImageCountry):\n    createRatioline = {}\n    createRatioline['date'] = line[1]\n    createRatioline['filename'] = line[2]\n    createRatioline['fullfilepath'] = line[3]\n    createRatioline['PERCENTCAT1'] = line[4] \/ np.array(histImageCountry)[0][4]\n    createRatioline['PERCENTCAT2'] = line[5] \/ np.array(histImageCountry)[0][5]\n    createRatioline['PERCENTCAT3'] = line[6] \/ np.array(histImageCountry)[0][6]\n    createRatioline['PERCENTCAT4'] = line[7] \/ np.array(histImageCountry)[0][7]\n    createRatioline['PRECENTVECTOR'] = np.sqrt( createRatioline['PERCENTCAT4']**2 + \n                                                createRatioline['PERCENTCAT3']**2 + \n                                                createRatioline['PERCENTCAT2']**2 + \n                                                createRatioline['PERCENTCAT1']**2)\n    createRatio.append(createRatioline)\n\ndf = pd.DataFrame(createRatio)\n\nprint('All the dataset in Activity Ratio:::-->> ')\n\nimport plotly.express as px\ndf_melt = df.melt(id_vars='date', value_vars=['PERCENTCAT1','PERCENTCAT2','PERCENTCAT3','PERCENTCAT4','PRECENTVECTOR'])\nfig = px.line(df_melt, x='date' , y='value' , color='variable')\n\nfig.update_xaxes(\n    rangeslider_visible=True,\n    rangeselector=dict(\n        buttons=list([\n            dict(count=1, label=\"1 month\", step=\"month\", stepmode=\"backward\"),\n            dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n            dict(step=\"all\")\n        ])\n    )\n)\nfig.show()\n\nprint('All the dataset in Activity Magnitude:::-->> ')\n\ndfNew = df[['date','PRECENTVECTOR']]\ndf_melt = dfNew.melt(id_vars='date', value_vars=['PRECENTVECTOR'])\nfig = px.line(df_melt, x='date' , y='value' , color='variable')\nfig.update_xaxes(\n    rangeslider_visible=True,\n    rangeselector=dict(\n        buttons=list([\n            dict(count=1, label=\"1 month\", step=\"month\", stepmode=\"backward\"),\n            dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n            dict(step=\"all\")\n        ])\n    )\n)\nfig.show()","e09aa83c":"Lets plot dataset of USA From : 6th to 16th of Jan.","1ba7f827":"files = []\nfig = plt.figure(figsize=(50, 50))\ni=0\nfor days in [\"%02d\" % x for x in range(6,16)]:\n    filePath = '..\/input\/ntt-data-global-ai-challenge-06-2020\/NTL-dataset\/tif\/USA-202001' + days + '.tif'\n    image = cv2.imread(filePath)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    i = i + 1\n    sub = fig.add_subplot(5,2,i)\n    sub.imshow(image)","002179dc":"# Our variable is dfNew\ndfNew.head()","459b4a15":"dfSeasonal = dfNew[['PRECENTVECTOR']]\ndates = pd.date_range(start='1\/1\/2020', periods=160, freq='D')\ndfseason = dfNew[['PRECENTVECTOR']]\ndfseason.set_index","c477fd65":"import statsmodels.api as sm\n# Use seasonal Decompose\ndecomposition = sm.tsa.seasonal_decompose(dfSeasonal,model='multiplicative',period=30)\ndecomposition.plot()\ndfpercent = decomposition.trend\ndfpercent = pd.DataFrame(dfpercent)\ndfpercent.plot()\ndfpercent","97e6eb2a":"# Fetch the price from the COVID Cases ->\ncovidDatafilePath = '..\/input\/ntt-data-global-ai-challenge-06-2020\/COVID-19_and_Price_dataset.csv'\nworldData = pd.read_csv(covidDatafilePath)\n\nworldDataPrice = worldData[ ['Date','Price' ] ]\nworldDataPrice[\"Date\"]= pd.to_datetime(worldDataPrice[\"Date\"])   \nworldDataPrice.head()\nworldDataPriceArr = np.array(worldDataPrice)\nlen(worldDataPriceArr)","8986a21c":"combinedData = []\ni = 0\nfor line in np.array(dfpercent):\n    try:\n        combineLine = {}\n        worldDataPriceArr[i]\n        combineLine['Date'] = worldDataPriceArr[i][0]\n        combineLine['Price'] = worldDataPriceArr[i][1]\n        combineLine['ACTIVITYMAGNITUDE'] = line[0]\n        combinedData.append(combineLine)\n        i = i + 1\n    except IndexError as ierr:\n        print(i)\n        break;\n\nlen(combinedData)","787a024d":"mergedData = pd.DataFrame(combinedData)\nmergedData.head()\nnewData = mergedData[['Date','Price','ACTIVITYMAGNITUDE']]\nnewData.head()","71924a52":"import matplotlib.pyplot as plt\nimport numpy as np\n \n# create data\nx = np.array(newData['Date'])\ny = np.array(newData['Price'])\nz = np.array(newData['ACTIVITYMAGNITUDE'])\n\n# use the scatter function\nplt.scatter(x, y, s=z, alpha=1)\nplt.show()","7f97f043":"plt.plot(np.array(newData['Price']),np.array(newData['ACTIVITYMAGNITUDE']))","413e593a":"newData.info()","beab79d3":"# Analysis on Map Data to extract information from it\n\nWhat we are going to here is understand the pattern of the MAP images and get the extracted data out of it \n\nIn brief we have catagorized the entire analysis into the below sections:\n\n1. Reading the Data\n2. Understand the possible cluster\n3. Traning the image data for KMEANS Algorithm\n4. Populate a histogram and legend for the Image\n5. Create a timeseries out of the each image \n6. Defination of Activity Magnitude\n7. Understand the distorted seasonal effects (fullmoon effects \/ Cloud visibility) in the Map images\n8. Refine the Map data by removing this distored data and extract\n9. \n\nAt Last we will conclude with a implementaion thoughts \n\n## Defination Activity Ratio: Ratio of the White Color with the dark Section.","f6bde4ce":"**Load Data: Oil Trend along with Covid cases**","81050508":"Lets start with first data set, given in COVID-19_and_Price_dataset.csv","4ac8af0a":"# Introduction","f9b9ab1c":"Let's split the data set into training and test dataset so that we can validate performance of our models.","27459654":"The above figure is directly ploted from the histogram values of all the images for the selected country. This might not make sense as the time series plot from this has no reference to check. So lets Change the dataset a bit and check what we can find out.","432a53d3":"2. Along with these two column, we have 832(834 - 2) column in between.These column represents region wise COVID situation.","242c0ec2":"**Multivariate study:** We'll try to understand how the dependent variable and independent variables are related to each other","63c61ee3":"So this dataset have just 125 rows\/records and quite high number of colum\/features(i.e 834). We will now have a look on features availabe....","b5e40c45":"1. First column represent to Date and last column represnt to oil price on that perticular date.\n","fb0916f9":"*Oil price is mostly co-related with total number of covid cases in these coutries except China and Brazil.So instead of taking all independent variables as input variable, we will choose only one.Lets consider only World_total_cases as input variable which we will use in model training*","7bbb2bca":"**Let's try some boosting and bagging techninque for prediction**","c3e48870":"## Follow these Steps\n\n## -> The below cell is to Train your dataset\n\n1. Execute the below section with each Country Name\n2. Each succesfull execution will create a excel file in your system","222acd26":"So, minimum value of oil price is 26.428933 and maximum value is 57.544133. But ,what? standard daviation is 12.193102 which is quite high. Lets check its distribution...","a3879ed9":"# Let' s come to third and last dataset. Most intresting and fun part!","d8c25cb6":"Do analyze the standard Histogram dataset we have taken two types of approach:\n\n1. Taken the reference of the 1st day and plotted the values - Activity Ratio\n2. Taken the sqroot of the summed value of (PERCENTCAT*)^2 - Activity Magnitude","46a2d3d1":"**Univariable study** : Here We'll just focus on the dependent variable (Oil Price) and try to know a little bit more about it","09afa4f1":"For any regression problem,input data should not have multicollinearity.","85cba760":"**Tensorflow Regression**","1fd312a8":"# Exploratory Data Analysis - EDA : First Data Set","7351ec76":"**Xgboost**","c38ad18b":"ok ,so oil price does not look like completely normally distributed. Prices data are mostly close either 30 or 50.","9627e508":"Let's apply normalization to our data set. We are using here min-max normalization","cd3d5b3c":"## Load the below cell first - This converts the trained dataset to an Excel file\n\n--> Preprocessing before Saving the trained model into excel ","a24b3715":"*Let's do some preprocessing!*","fae4ec55":"## Step for Mass Data Load","7f61d543":"Let's download data from Crude_oil_trend_From1986-01-02_To2020-06-08 and check what info it has...","079a1062":"**Note:** we can see model has overfitted to training data and prediction is worst on test data. We can control overfitting by doing hyperparmeter tuning that choosed in model(ex-learning_rate,n_estimators).","247be67b":"Prediction price are quite low as compare to actula.Lets plot predictions on whole test set.","92e926c8":"**Import Packages**","ac06ccf0":"*Only total number of cases in China is not much co-related with total number of cases with other countries :)*","ac5798af":"Let's start with very simple linear regression model to predict future oil prices on the basis of covid cases and check how it goes...","99c714f7":"## THE END-------------------------","475f49f0":"Here we will look into correlation between independent variables(covide cases) and dependent variable(Oil Price) as well as\ncorrelation within independent variables(covide cases)","45080057":"## Now how to create a feature out of it -->","4f3e2ba2":"**Polynomial regression seems worst solution than linear regression! So What next?**","323bfa47":"Predictions are not that great. Lets plot predictions on whole test set.","1e58c09c":"**Predictions on test data looks better than other models. We can even get better model by choosing different architecure(Number of hidden uints,number layers or types layers). This is just a sample model** ","dbc7a2db":"## Details to Choose the Cluster no as 4\n\nNow we have to discuss about the number clusters to be choosen.\nThe activity data will be displayed when the colors are closer to \"white\".\nBased upon this Analysis we can take into consider that rightmost colors in the above graph \n\n![image.png](attachment:image.png)\n","0d3d1f91":"-> These above values is done for USA-20200101.tif\". Similary we can take all the imgaes of USA and create a nice Time series graph out of it to understand the Activity ratios in throughout Lockdown period. \n\n-> More Over this Data can be extrapolated to merge as a new Feature with the \"World Covid Total Cases Excel\". ","c5276372":"*Oil price is mostly co-related with total number of covid cases in these countries except China and Brazil.So instead of taking all independent variables as input variable we will choose only one.Lets consider only World_total_cases as input variable which we will use in model training*","f6d0c1cf":"## 4. Populate a histogram and legend for the Image","38b4a73c":"## 1. Reading the Data","1b78619a":"**Polynomial Regression**","37ec947e":"## At Last we will concluding this Topic with an implementaion thoughts\n\nThis Dataset Contains error for each an every values which has a cloud cover or irregular images. ","ecdd30f4":"## Now lets understand: why this spark is comming?\n![image.png](attachment:image.png)","5e0a026f":"# Let's look into second data set...","8aa4268e":"**Now ARIMA**","04d16a8d":"4. For example for region Aruba we have 4 coulms: Aruba_total_cases,Aruba_new_cases,Aruba_total_deaths,Aruba_new_deaths We can also see,records for few dates are missing.ex- Data records for 2020-01-04 and 2020-01-05 are not there,reason behind these days were either weekends or Holidays","8eddd35a":"3. For each region there are 4 colums, which represnets : total_case, news_cases,total_deaths,new_deaths\n","994dfd8b":"As we know,we have 832 features in dataset.But many of them will be correlated to each other.This may results to multicollinearity in dataset.","6d49c7c7":"# EDA  : Second Data Set","f30b3584":"**Have you ever tried Tensorflow for Regression? Let's do it**","ed63a69e":"Oil price data is availabe here, from date 31-12-2019 to 29-06-2020(close to 7 Months). Let's look at trend of oil price during this period.","c2b6d991":"3 Degree Polynomial","df901236":"This notebook results collective work of [Irfan Ahmad](https:\/\/www.linkedin.com\/in\/irfan-ahmad-31103055\/) and [Sabarna Chatterjee](https:\/\/www.linkedin.com\/in\/sabarna-chatterjee-b942108b\/)\n","107213a9":"**As we can see here we have only 2 colums 'Date'and 'Price'. So to predict future price we may consider this as a time series problem**","8eb112e7":"**But here we will try time series with some variation.Instead to predicting actual price what if convert whole data set in number of bins and predict price falling into which bin?**","22ca23dc":"In this notebook we explore all given data sets one by one and see how these data can help to predict oil price using different models.","3cbf1ddf":" \n* Uni-variate and Bi-variate Analysis on COVID Data\n* Correlation Matrix and Extraction program - ( Image + Excel ) of COVID Data\n* Modelling - Lin Reg. | Polynomial Reg. | XG Boost ( Bagging + Boosting ) | TensorFlow\n* ARIMA with Binning model on Oil Price Trends\n* Image Histogram on MAP Data and Activity Index generation\n* Seasonality reduction of Moonlight Effects\n* Activity Index vs Oil Price Analysis ","cd7f4991":"*Let's say you want to compare correaltion within total number of cases of all countries as well as with price. Choose check box 'Total cases' and 'Price' and press sumbit.A CSV file will be generated with correation matrix.*","5ffe7b66":"Agenda of this challenge is to investigate how coronavirus pandemic has changed oil markets and predict future oil prices. To solve this problem, we have below data files in hand.","b27bef27":"**Check the dataset by clicking the below radio button to get the dataset,\ndataQuantileRanges1 ->10 Bin details dataQuantileRanges2 ->20 Bin details**","ad898bfe":"**Linear Regression**","5dc9b19e":"**Do you want to look into correlation between each and every coulmn? we have totla 832 column. So we can have 832 x 832 total comparisions,which is difficult to plot.**","d9676cd5":"**We have just looked correlations between covid cases of few countries and oil prices. But actualy we have 832 coulmns available in dataset.**","d336604c":"## Load the Excel file\n\n## Execution of the below cell will create a Dropdown from which you can select any Country to do further analysis","bb83182d":"Lets try to implement an ARIMA Model just to find out the trend of this bins predict the Bins. This arima model prediction data will give the response as the bin where the oil price could be","5ad000f5":"As we have decided to use only World_total_cases as independent variable because most of them are correlated to each other.So here we will be having only two variables one independent(World_total_cases) and one dependent variable(Price)","ac4fb69e":"The below methods works for ->\n\ncentroid_histogram -> This function taked the trained model input data and converts into a \"hist[]\" Array\n\nplot_colors -> This function converts the percentages in \"hist[]\" Array and then plots the histogram with its speciifc width\n\nplotAnalysis -> This function helps to create a legend for the histogram plot","958d69d9":"# Authors","4c78318c":"Create quantile","a4d77fd9":"We could see most of the independent variables are highly co-related to each other.","222f4572":"Let's check relationship between total number of covid cases among different countries those could have possiable impact on oil price. As well as,lets look dependencies between oil price and total number cases in these countries.","0e8e7c45":"**Let's try with polynomial regression as actual curve of price does not look like linear!**","225cc834":"Instead of using either of these two separate algorithms , we will use [Xgboost](https:\/\/en.wikipedia.org\/wiki\/XGBoost). This combines both bagging and boosting technique.","bb7039eb":"## 2. Understanding the possible cluster","2197979a":"# What you will get here?","6a2514f6":"Let's plot scatter plot between Oil price and few top correlated variables","c166f3dd":"## 3. Traning the image data for KMEANS Algorithm\n\nLets take the cluster as 4 and train the KMEANS","245c68ab":"**Correlation check -Multicollinearity**","bdceca69":"In this note book, we are not going to tune hyper parameters.","14f10df3":"Quantile Charts","395e4958":"*Clearly model is not predicting well as soon as it goes far*","9bf0b82b":"## What we can can understand from this?\n\nAs the image contains the 4 types of color cluster ( As choosen while the KMEANS). \nWe are going to seggregate these colors into different ranges.\n\ncatagory 1 - (Dark Black)\ncatagory 2 - (Semi Black)\ncatagory 3 - (Grey)\ncatagory 4 - (Near White)","5d1734cb":"# From the above pictures we can extract below conclusion:\n\n-In the above images, we can see sharp few white images on regular interval of times\n\n**This due to the seasonal effects of the extra light due to fullmoon**\n\n-> we need to refine this effects to get a smooth curve out of it:","0d212f9c":"We have created a report below where we can choose multiple coulms and extract their correlations value into a csv file.","9656eb79":"# Models","4f7ef96b":"Gradient Boosting Decision Tree is one of the popular boosting technique. Simlarly Random forest in known for bagging. Both techniques are part of Ensemble learning. To learn more about bagging and boosting ,please follow link : [Ensemble learning](https:\/\/en.wikipedia.org\/wiki\/Ensemble_learning) ","acd94d8c":"1. COVID-19_and_Price_dataset.csv\n2. Crude_oil_trend.csv\n3. NTL-dataset.zip "}}