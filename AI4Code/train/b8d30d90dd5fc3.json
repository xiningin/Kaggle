{"cell_type":{"514266fb":"code","d3e823cc":"code","4ff23738":"code","1189dfd0":"code","5ecb4a11":"code","75d7c964":"code","46e2fb3d":"code","75f4e10a":"code","0ea58c60":"code","1cb4a1ef":"code","b350e987":"code","257e07c6":"code","355f0e64":"code","dd9be4cc":"code","573cd1d7":"code","dd11592c":"code","a903639e":"code","19449981":"code","24a4daca":"code","59238ee0":"code","774a6c45":"code","b7c579d2":"code","3ceb9e5d":"code","c63f573f":"code","9a13c530":"code","df1ef13b":"code","01d94bde":"code","a4abfdf7":"code","bf98c109":"code","48c34f5a":"code","2d966661":"code","47007764":"code","1ea7738d":"code","221ee412":"code","05efa6d2":"code","843f87de":"code","87a0e6d8":"markdown","e0f93f17":"markdown","665ab756":"markdown","865344f4":"markdown","9a481f12":"markdown","bacd3612":"markdown","4b6e104f":"markdown","b4c30bd4":"markdown","6a7cc033":"markdown","550206d0":"markdown","5e6ae3a6":"markdown","774c0ea1":"markdown","91d4b80a":"markdown","4b34fb0e":"markdown","8e1903c8":"markdown","f4a4ac45":"markdown","8293e337":"markdown","5f55c684":"markdown","8e121ebc":"markdown","0e7c7ba3":"markdown","31b0f1ff":"markdown","404c8953":"markdown","a9fda654":"markdown","d5989183":"markdown","7f955652":"markdown","a4c9b54d":"markdown","3680e157":"markdown","658e078e":"markdown","5d98bbe9":"markdown","d02c1836":"markdown","7d7ccc3d":"markdown","bdf38507":"markdown","15541fc7":"markdown","14c42961":"markdown","e412eddb":"markdown","1e49d0b7":"markdown"},"source":{"514266fb":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","d3e823cc":"transactions = pd.read_csv('..\/input\/work-sample\/ANZ synthesised transaction dataset.csv')\ntransactions.head()","4ff23738":"transactions['txn_description'].value_counts()","1189dfd0":"#Examining salary transaction data\nsalary_transactions = transactions[transactions.txn_description == 'PAY\/SALARY'].copy()\nsalary_transactions.account.value_counts()","5ecb4a11":"#Calculating the annual salary by summing the total credit transactions from the quarter and multiplying by 4\nannual_salary = salary_transactions.groupby('account').amount.sum().reset_index()\nannual_salary['amount'] = annual_salary['amount'] * 4\nannual_salary.head()","75d7c964":"#columns for new dataframe, to begin with\ncolumns_to_copy = ['account','long_lat','gender','age']\n\n#Creating the dataframe, getting rid of duplicate entries (so each account has only one entry)\ndf = transactions[columns_to_copy].copy()\ndf = df.drop_duplicates()\ndf.account.value_counts()","46e2fb3d":"#Sorting by 'account' so we can easily add features later\ndf = df.sort_values('account').reset_index()\ndf.drop(['index'],axis=1,inplace=True)\ndf.head()","75f4e10a":"df['Salary'] = annual_salary['amount']\n\nprint('Salary column stats:')\nprint(df['Salary'].describe())\n\ndf.head()","0ea58c60":"# Extracting the long\/lat from the long_lat column\nlong = df.long_lat.apply(lambda x: float(x.split('-')[0].replace(' ','')))\nlat = df.long_lat.apply(lambda x: float(x.split('-')[1].replace(' ','')))\n\n#Converting into radians to use with numpys cos\/sin functions\nlong = (long * np.pi) \/ 180\nlat = (lat * np.pi) \/ 180\n\n#Removing the now useless long_lat column\ndf.drop('long_lat',axis=1,inplace=True)","1cb4a1ef":"#Converting long and lat into 3d cartesians\ndf['Xpos'] = np.cos(long) * np.cos(lat)\ndf['Ypos'] = np.sin(long) * np.cos(lat)\ndf['Zpos'] = np.sin(lat)\n\n#Re-ording columns so salary is at the end\ndf = df[df.columns[[0,1,2,4,5,6,3]]].copy()","b350e987":"df['gender'] = df['gender'].apply(lambda x: 1 if x =='F' else 0)","257e07c6":"df.head()","355f0e64":"#Gathering the data\nquarter_spend = transactions[transactions.movement == 'debit'].groupby('account').amount.sum().reset_index()\n\n#Adding to our dataframe for analysis\ndf['quarter_spend'] = quarter_spend['amount']\ndf = df[df.columns[[0,1,2,3,4,5,7,6]]].copy()\ndf","dd9be4cc":"#Observing the different possible values\nprint(list(transactions.txn_description.unique()))","573cd1d7":"#Excluding 'PAY\/SALARY' as this was used to calculate Salary (and is 'credit' not 'debit')\ntxn_features = [list(transactions.txn_description.unique())[i] for i in [0,1,2,3,5]]","dd11592c":"#Adding each feature to the dataframe\nfor feature in txn_features:\n    feature_spend = transactions[transactions.txn_description == feature].groupby('account').amount.sum().reset_index()\n    \n    df['{}_total'.format(feature)] = feature_spend['amount']\n    \ndf = df[df.columns[[0,1,2,3,4,5,6,8,9,10,11,12,7]]].copy()\ndf.head()","a903639e":"average_balance = transactions.groupby('account').balance.mean().reset_index()\n\ndf['average_balance'] = average_balance['balance']\ndf =df[df.columns[[0,1,2,3,4,5,6,7,8,9,10,11,13,12]]].copy()\ndf.head()","19449981":"df.drop('account',axis=1,inplace=True)","24a4daca":"total_na = df.isnull().sum().sort_values(ascending=False)\npercentage = (df.isnull().sum()*100\/df.shape[0]).sort_values(ascending=False)\n\nsns.set(font_scale=1.2)\nfig, ax = plt.subplots(figsize=(12, 8))\nsns.barplot(x=percentage.index[:5],y=percentage[:5])\n\n\nplt.xlabel('Features')\nplt.ylabel('Percent of missing values')\nplt.title('Percent missing data by feature')\nplt.show()\n\npercentage = percentage.map('{:,.2f}%'.format)\ncombined = pd.concat([total_na,percentage],axis=1,keys=['Total NA','Percentage'])\ncombined.head(5)","59238ee0":"#Removing columns\ndf.drop(['INTER BANK_total','PHONE BANK_total'],axis=1,inplace=True)","774a6c45":"#Replacing SALES-POS_total NA values with zeros\ndf['SALES-POS_total'] = df['SALES-POS_total'].fillna(0)\ndf.head()","b7c579d2":"df.hist(edgecolor='black', linewidth=1.3,figsize=(16,13))\nplt.show()","3ceb9e5d":"df['Salary'] = np.log1p(df['Salary'])","c63f573f":"#Plotting a heatmap to visualize correlation between variables (most importantly with salary)\nplt.figure(figsize=(16, 6))\nheatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True)\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);","9a13c530":"df = df.drop('POS_total',axis=1)","df1ef13b":"#importing the tool\nfrom sklearn.preprocessing import StandardScaler","01d94bde":"cols_to_scale = ['age', 'Xpos', 'Ypos', 'Zpos', 'quarter_spend', \n                 'SALES-POS_total', 'PAYMENT_total', 'average_balance']\n\nfor column in cols_to_scale:\n    scale = StandardScaler().fit(df[[column]])\n    df[column] = scale.transform(df[[column]])\n    \ndf","a4abfdf7":"#Splitting data into test and train\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nnp.random.seed(5)\n\ny_data = df['Salary']\nx_data = df.drop('Salary',axis=1)\nx_train,x_test,y_train,y_test = train_test_split(x_data,y_data,train_size=0.8)\n\nmodel_1 = LinearRegression()","bf98c109":"#Fitting the model\nmodel_1.fit(x_train,y_train)\n\n#Converting the predictions back to a typical salary figure\npredictions_1 = model_1.predict(x_test)\npredictions_2 = np.expm1(model_1.predict(x_test))","48c34f5a":"from sklearn.metrics import mean_absolute_error as mae\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer\n\nprint('Mean Absolute Error: {}'.format(mae((y_test),predictions_1)))\n\nprint('Mean Squared Log Error: {}'.format(metrics.mean_squared_log_error(y_test,predictions_1)))\n\ncv_score = cross_val_score(model_1,x_data,y_data,cv=10,scoring=make_scorer(mae))\nprint('Mean cross val score: {}'.format(cv_score.mean()))\n\nprint('\\nMean Absolute Error (un-transformed): {}'.format(mae(np.expm1(y_test),predictions_2)))\nprint('\\nCross val scores: {}'.format(cv_score))\n","2d966661":"import xgboost as xgb\n\nmodel_2 = xgb.XGBRegressor()","47007764":"model_2.fit(x_train,y_train)\n\n#Converting the predictions back to a typical salary figure\npredictions_1 = model_2.predict(x_test)\npredictions_2 = np.expm1(model_2.predict(x_test))","1ea7738d":"print('Mean Absolute Error: {}'.format(mae((y_test),predictions_1)))\n\nprint('Mean Squared Log Error: {}'.format(metrics.mean_squared_log_error(y_test,predictions_1)))\n\ncv_score = cross_val_score(model_2,x_data,y_data,cv=10,scoring=make_scorer(mae))\nprint('Mean cross val score: {}'.format(cv_score.mean()))\n\nprint('\\nMean Absolute Error (un-transformed): {}'.format(mae(np.expm1(y_test),predictions_2)))\nprint('\\nCross val scores: {}'.format(cv_score))\n","221ee412":"#xgb model\nxgbmodel = xgb.XGBRegressor()\nxgbmodel.fit(x_data,y_data)\nxgbpredictions = np.expm1(xgbmodel.predict(x_data))\n\n#scikit model\nscikitmodel = LinearRegression()\nscikitmodel.fit(x_data,y_data)\nscikitpredictions = np.expm1(scikitmodel.predict(x_data))\n\nfinal_predictions = 0.80*xgbpredictions + scikitpredictions * 0.2","05efa6d2":"results = pd.DataFrame([quarter_spend['account'],xgbpredictions,scikitpredictions,final_predictions,np.expm1(y_data)])\nresults = results.transpose()\nresults = results.rename(columns={'Unnamed 1': 'SciKit','Unnamed 0':'XGB','Unnamed 2':'Combined'})","843f87de":"print('R^2 score: {}'.format(metrics.r2_score(np.expm1(y_data),final_predictions)))\nprint('Mean Squared Log Error: {}'.format(metrics.mean_squared_log_error(np.expm1(y_data),final_predictions)))\n\n\nprint('Mean Absoloute Error: {}'.format(mae(np.expm1(y_data),final_predictions)))\nresults","87a0e6d8":"### Encoding gender","e0f93f17":"#### Next we will add the average balance of the account as a feature","665ab756":"## Feature Engineering\n\nWe will start with looking at the total spending in the 3 months\n\nWe could multiply the data by 4 for annual spending, but the features will be standardised anyway","865344f4":"# Visualisiations","9a481f12":"## Cleaning and visualising data\n\n- First we should drop the column 'account' as it wont be used it any further analysis","bacd3612":"### Changing long_lat into x-y-z co-ordinates\n","4b6e104f":"#### I will also add a column for each of the different transaction types, and the total of each of them","b4c30bd4":"### Heatmap (the most exciting visualisation)","6a7cc033":"- Putting Results into a table and viewing statistics","550206d0":"### Importing some libraries","5e6ae3a6":"Now we need to train a model on our features, to try and predict values of Salary. I will try a couple methods first, and pick the best one (opting not to create an ensemble this time, to keep costs down if the data was massive for example)","774c0ea1":"We can't really get a model to work with long_lat as it is. We should get (x,y,z) data, which the model can work with (reasonably well)","91d4b80a":"- Making a new model which uses all the training data","4b34fb0e":"# Modelling\n\n","8e1903c8":"### Decision Tree based model (XGBoost)","f4a4ac45":"## Standardizing the features\n\n- Having features on a similar scale can help gradient descent converge more quickly towards the minimum values.\n- Using standardization instead of normalization as a lot of the features tend to follow a Gaussian shaped distribution or have outliers","8293e337":"Here we can have a look a a couple of metrics for this model\n\nWe will compare the MAE, MSLE, and the Mean cross validation scores, using a 'mae' scorer, for the sk model and xgboost. Lower is better for all 3","5f55c684":"In this work sample, I will be calculating the annual salary of a customer using transactional data, and then training a model on other features to be able to predict this salary.\n\nI was tasked to create a model with a MAE of about 20,000, with a time frame of roughly 3 hours. I managed to achieve a MAE of about 13,000, which is over 30% lower than requested!\n\nThis would be useful for predicting customers annual salaries in the future, if we only have limited information on them. Using a model like this, we would be able to better target promotional offers, products, advertisements etc.","8e121ebc":"# Predictive Analytics ANZ Sample\n\nBy Charlie Gaynor\n\n CharlieJackGaynor@Gmail.com\n\nlinkedin.com\/in\/CharlieGaynor","0e7c7ba3":"# Description","31b0f1ff":"The salary is skewed to the right. This is a problem because most ML models don't do well with non-normally distributed data. We can apply a log(1+x) transform to fix the skew.","404c8953":"\n\n- First we must calculate the annual salary (estimate) using the pre-existing data\n\n- Then we'll observe the correlations with both existing and engineered features\n\n- We will use a variety of ML methods to create a model to predict the annual salary of a customer\n","a9fda654":"- Visualising the NULL values","d5989183":"We can tentatively look at the scores for our final model. Of course this is nearly pointless, because our model will be fit very closely to the training data.\n\nWe have too little data to be able to train a robust, non over-fitted model.","7f955652":"# Presenting Results","a4c9b54d":"### Sci-Kit Regression","3680e157":"I decide to remove the two features with a large amount of NA values, and replace the NA value in Sales-POS_total with 0","658e078e":"POS_total has essentially no correlation with Salary, and correlates strongly with quarter_spend, so we will drop it. I will tentatively keep the 'SALES-POS_total' feature.","5d98bbe9":"The XGB model is clearly over-fitting the train data, whilst SciKit is not (as much). <br>\n\nXGB out performed SciKit when splitting the data though, so we still heavily favour using XGBoost over SciKit's for future predictions","d02c1836":"Here we can see there are multiple payments for each account, which we need to consider","7d7ccc3d":"#### We will need to create a new dataframe to be used for the analysis, where each row contains the information for only one account\n\n#### We will initialise the dataframe with the features we will be using for predictive analytics, + 'account' (identifier)","bdf38507":"We see that the XGBoost model gives much better results than SKlearn. Hence we should give XGBoost much more weighting than Sci-kit when combining","15541fc7":"### Calculating annual salary and creating a new dataframe\n\nWe can see we want to add up all the transactions of type ('PAY\/SALARY'), then multiply by 4 (as the data\nonly covers one quarter), for each account.\n\nWe might be neglecting things like Christmas bonuses, but implementing a Christmas bonus would likely cause as many problems as it fixes (as some might get them at different times or no bonuses at all)\n","14c42961":"Let's look at the variety of the transactions","e412eddb":"# By Charlie Gaynor","1e49d0b7":"### Examining data"}}