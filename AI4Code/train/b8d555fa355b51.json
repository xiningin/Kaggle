{"cell_type":{"efad0aaf":"code","84ba2009":"code","4c5d132b":"code","b57952bc":"code","d82c12d0":"code","547c06eb":"code","2015561f":"code","16cf246c":"code","1e10ebc6":"code","dc3dde6c":"code","a61e7266":"code","6b982f28":"code","86abc1a7":"code","6807b101":"code","05e8d922":"code","f6adf517":"code","57310af2":"code","912bac01":"code","1734eae4":"code","e274337b":"code","32d67f7d":"code","91fafde8":"code","d0c8b63b":"code","84cf6ab8":"code","53c5371c":"code","4c3a33b3":"code","abccd504":"code","c1433521":"code","b4c4a35a":"code","3f8bdfc6":"code","866ef730":"code","625c5a74":"markdown","73330b09":"markdown","f74fb15d":"markdown","cba4017c":"markdown","aab9807e":"markdown","3e80dd0b":"markdown","dede2263":"markdown","b48e8af3":"markdown","b26060fb":"markdown","215cf845":"markdown","a320f6f1":"markdown","1c9de249":"markdown","a1072ee2":"markdown","b5889aaf":"markdown","8eb3cd65":"markdown","97f18e15":"markdown","00684c1e":"markdown"},"source":{"efad0aaf":"!pip install pyspark","84ba2009":"import os\nimport json\nimport pyspark\nfrom pyspark import SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport os\nimport nltk\nimport re\nimport spacy\nfrom spacy.lang.fr.stop_words import STOP_WORDS\nimport string\nfrom pyspark.sql.functions import lit\nfrom pyspark.sql.functions import monotonically_increasing_id \n","4c5d132b":"sparkSession = SparkSession.builder.appName(\"SentimentAnalysis\").getOrCreate()\n","b57952bc":"df = pd.read_csv('..\/input\/insurance-reviews-france\/Comments.csv')","d82c12d0":"df = df.drop(['Unnamed: 0'], axis=1)\n","547c06eb":"schema = StructType([\n    StructField(\"Name\", StringType(), True),\n    StructField(\"Comment\", StringType(), True),\n    StructField(\"Month\", IntegerType(), True), \n    StructField(\"year\", StringType(), True),\n])","2015561f":"df_sp = sparkSession.createDataFrame(df,schema =schema )\n","16cf246c":"df_sp.show()","1e10ebc6":"df_sp = df_sp.filter(df_sp.Comment != 'NaN')","dc3dde6c":"rdd_df = df_sp.rdd.zipWithIndex()\ndf_sp = rdd_df.toDF()\ndf_sp = df_sp.withColumn('Name', df_sp['_1'].getItem(\"Name\")).withColumn('Comment', df_sp['_1'].getItem(\"Comment\")).withColumn('Month', df_sp['_1'].getItem(\"Month\")).withColumn('Year', df_sp['_1'].getItem(\"Year\")).withColumn('Index', df_sp['_2'])\ndf_sp = df_sp.select('Index', 'Name','Comment','Month','Year')\n","a61e7266":"df_sp.show(5)","6b982f28":"comments_rdd = df_sp.select(\"Comment\").rdd.flatMap(lambda x: x)","86abc1a7":"comments_rdd_lower = comments_rdd.map(lambda x : x.lower())\n","6807b101":"comments_rdd_lower.collect()","05e8d922":"def sentence_tokenization(x):\n    return nltk.sent_tokenize(x)\n","f6adf517":"comments_rdd_tok = comments_rdd_lower.map(sentence_tokenization)\n","57310af2":"comments_rdd_tok.collect()","912bac01":"def word_TokenizeFunctSentence(x):\n    sentence_splitted = []\n    for line in x:\n        splitted = []\n        for word in re.sub(\"\\W\",\" \", line).split():\n            splitted.append(word)\n        sentence_splitted.append(splitted)\n    return sentence_splitted\ncomments_rdd_word_tok_sentence = comments_rdd_tok.map(word_TokenizeFunctSentence)\n","1734eae4":"comments_rdd_word_tok_sentence.collect()","e274337b":"stop_words=set(STOP_WORDS)\n\ndeselect_stop_words = ['n\\'', 'ne','pas','plus','personne','aucun','ni','aucune','rien']\nfor w in deselect_stop_words:\n    if w in stop_words:\n        stop_words.remove(w)\n    else:\n        continue","32d67f7d":"stop_words","91fafde8":"def removeStopWordsSentencesFunct(x):\n    sentence_stop=[]\n    for j in x:\n        fil=[]\n        for w in j:\n            if not ((w in stop_words) or (len(w) == 1)):\n                fil.append(w)\n        sentence_stop.append(' '.join(fil))\n    return sentence_stop\n\nstopwordRDDSen = comments_rdd_word_tok_sentence.map(removeStopWordsSentencesFunct)\n","d0c8b63b":"stopwordRDDSen.collect()","84cf6ab8":"def joinTokensFunct(x):\n    joinedTokens_list = []\n    x = \" \".join(x)\n    joinedTokens_list.append(re.sub(\"\\W\",\" \", x))\n    return joinedTokens_list\njoinedTokens = stopwordRDDSen.map(joinTokensFunct)","53c5371c":"joinedTokens.collect()","4c3a33b3":"my_words = [\"s\u00e9curit\u00e9\",\"prix\", \"sociale\" , \"remboursement\" , \"dentaire\", \"aide\" , \"pack\" , \"optique\" , \"soins\" ,\n\"enfant\",\"hospitalisation\" , \"handicap\" , \"document\" , \"retraite\" , \"carte\" , \"m\u00e9dicament\" , \"lunettes\" ,\n\"appareil\" , \"changement\" , \"accident\" , \"intervention\",\"garantie\",\"augmentation\",\"implant\", \"pharmacie\" ,\"attente\", \"formule\" ,\n\"maternit\u00e9\" , \"cotisation\", \"cpam\" , \"diab\u00e8te\", \"auditif\",\n\"commercial\", \"opticien\" , \"euros\" , \"retard\" , \"contrat\", \"prestation\", \"dossier\" , \"chirurgie\" , \"r\u00e9siliation\" ]","abccd504":"def TopicsSentences(x):\n    topics =[]        \n    topic =[]\n\n    for i in x:\n        for ext in my_words:\n            if (ext in i):\n                topic.append(ext)\n    return topic\ntopics = stopwordRDDSen.map(TopicsSentences)\n","c1433521":"topics.collect()","b4c4a35a":"comments_after_preproc = sparkSession.createDataFrame([w for w in joinedTokens.collect()], ['comments_after_preproc'])   \nrdd_df2 = comments_after_preproc.rdd.zipWithIndex()\ncomments_after_preproc = rdd_df2.toDF()\ncomments_after_preproc = comments_after_preproc.withColumn('comments_after_preproc', comments_after_preproc['_1'])\ncomments_after_preproc = comments_after_preproc.withColumn('Index', comments_after_preproc['_2'])\ncomments_after_preproc = comments_after_preproc.select('Index', 'comments_after_preproc')\n\n\nTopics = sparkSession.createDataFrame(topics,schema = \"array<string>\")    \ntopics_df = Topics.rdd.zipWithIndex()\nTopics = topics_df.toDF()\nTopics = Topics.withColumn('Topics', Topics['_1'])\nTopics = Topics.withColumn('Index', Topics['_2'])\nTopics = Topics.select('Index', 'Topics')\n","3f8bdfc6":"df_spark4 = df_sp.join(comments_after_preproc, on=['Index']).join(Topics, on=['Index'])\n","866ef730":"df_spark4.show(5)","625c5a74":"<font color='black'> Select the Comments feature <\/font>","73330b09":"<font color='black'> Drop Nan Values <\/font>","f74fb15d":"# <font color='red'> Data Preprocessing <\/font>","cba4017c":"\n<font color='black'> set of Spacy's default stop words and delete negation words <\/font>","aab9807e":"\n<font color='black'> Sentence tokenization <\/font>","3e80dd0b":"\n#  <font color='blue'> Text Preprocessing and Subject Extraction Using Pyspark <\/font>","dede2263":"Add the comments after preprocessing and the topics to our Dataframe.","b48e8af3":"\n<font color='black'> Join Tokens <\/font>","b26060fb":"<font color='black'> Load CSV to pandas Dataframe <\/font>","215cf845":"# I hope you find this kernel useful\n# Your <font color='red'> UPVOTES <\/font> would be highly appreciated","a320f6f1":"<font color='black'> Convert the data into lowercase. <\/font>","1c9de249":" \n <font color='black'> Add index column <\/font>","a1072ee2":"# <font color='red'> Subject Extraction <\/font>","b5889aaf":"\n<font color='black'> Convert pandas DataFrame To Pyspark DataFrame <\/font>","8eb3cd65":"\n<font color='black'> Word tokenization <\/font>","97f18e15":"\n<font color='black'> Drop  Unnamed: 0 column <\/font>","00684c1e":"# <font color='red'> Create Spark Session <\/font>"}}