{"cell_type":{"84ad357a":"code","e2cc0846":"code","9863bda4":"code","4abd541b":"code","d166881a":"code","25937a07":"code","f397e6a3":"code","4dd434d0":"code","4c9a6fe6":"code","f3b16b0d":"code","74226697":"code","f302f418":"code","859d8920":"code","f93b5175":"code","ca05cb7c":"code","dc9b3069":"markdown"},"source":{"84ad357a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\/\"))","e2cc0846":"from torchtext.data import Field, Dataset, Example\nimport pandas as pd\n\nclass DataFrameDataset(Dataset):\n    \"\"\"Class for using pandas DataFrames as a datasource\"\"\"\n    def __init__(self, examples, fields, filter_pred=None):\n        \"\"\"\n        Create a dataset from a pandas dataframe of examples and Fields\n        Arguments:\n         examples pd.DataFrame: DataFrame of examples\n         fields {str: Field}: The Fields to use in this tuple. The\n             string is a field name, and the Field is the associated field.\n         filter_pred (callable or None): use only exanples for which\n             filter_pred(example) is true, or use all examples if None.\n             Default is None\n        \"\"\"\n        self.examples = examples.apply(SeriesExample.fromSeries, args=(fields,), axis=1).tolist()\n        if filter_pred is not None:\n            self.examples = filter(filter_pred, self.examples)\n        self.fields = dict(fields)\n        # Unpack field tuples\n        for n, f in list(self.fields.items()):\n            if isinstance(n, tuple):\n                self.fields.update(zip(n, f))\n                del self.fields[n]\n\n\nclass SeriesExample(Example):\n    \"\"\"Class to convert a pandas Series to an Example\"\"\"\n\n    @classmethod\n    def fromSeries(cls, data, fields):\n        return cls.fromdict(data.to_dict(), fields)\n\n    @classmethod\n    def fromdict(cls, data, fields):\n        ex = cls()\n        for key, field in fields:\n            if key not in data:\n                raise ValueError(\"Specified key `{}` was not found in the input data\".format(key))\n            if field is not None:\n                setattr(ex, key, field.preprocess(data[key]))\n            else:\n                setattr(ex, key, data[key])\n        return ex","9863bda4":"# prepare data\n\nimport numpy as np\nimport pandas as pd\n\nfrom torchtext import data\nfrom torchtext import datasets\nfrom torchtext.vocab import Vectors, GloVe\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n\nfield_text = data.Field(lower=True, tokenize='spacy', include_lengths=True, batch_first=True, fix_length=300)\nfield_label = data.LabelField()\nfields = [(\"text\", field_text), (\"label\", field_label)]","4abd541b":"class CustomGloVe(Vectors):\n    def __init__(self, **kwargs):\n        name = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\n        super(CustomGloVe, self).__init__(name, **kwargs)","d166881a":"def data_generator(n_folds=1, valid_size=0.33, seed=999):\n    df_train = pd.read_csv(\"..\/input\/train.csv\", index_col='qid')\n    df_train_neg = df_train[df_train['target']==1]\n    df_train_pos = df_train[df_train['target']==0].sample(n=df_train_neg.shape[0] * 5)\n    df_train = pd.concat([df_train_neg,df_train_pos],axis=0)\n    print(\"down sampling shape is {} {}\".format(*df_train.shape))\n    \n    df_train.rename({'question_text':'text','target':'label'}, axis=1, inplace=True)\n    df_train[\"text\"] = df_train['text'].str.replace(\"\\n\", \" \")\n\n    sss = StratifiedShuffleSplit(n_splits=n_folds, test_size=valid_size, random_state=seed)\n    for i, (train_indices, valid_indices) in enumerate(sss.split(df_train, df_train.label)):\n        _train, _valid = df_train.iloc[train_indices], df_train.iloc[valid_indices]\n        train_ds, valid_ds = DataFrameDataset(_train, fields), DataFrameDataset(_valid, fields)\n        yield (train_ds, valid_ds)","25937a07":"def load_dataset():\n    train_ds, valid_ds = next(data_generator(n_folds=1))\n    field_text.build_vocab(train_ds, vectors=CustomGloVe())\n    field_label.build_vocab(train_ds)\n    word_embeddings = field_text.vocab.vectors\n\n    print (\"Length of Text Vocabulary: \" + str(len(field_text.vocab)))\n    print (\"Vector size of Text Vocabulary: \", field_text.vocab.vectors.size())\n    print (\"Label Length: \" + str(len(field_label.vocab)))\n\n    train_iter, valid_iter = data.BucketIterator.splits((train_ds, valid_ds), \n                                                        batch_size=32, \n                                                        sort_key=lambda x: len(x.text), \n                                                        repeat=False, \n                                                        shuffle=True)\n    vocab_size = len(field_text.vocab)\n    return field_text, vocab_size, word_embeddings, train_iter, valid_iter","f397e6a3":"# _*_ coding: utf-8 _*_\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\n\n\nclass RCNN(nn.Module):\n    def __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n        super(RCNN, self).__init__()\n        \"\"\"\n\t\tArguments\n\t\t---------\n\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n\t\toutput_size : 2 = (pos, neg)\n\t\thidden_sie : Size of the hidden_state of the LSTM\n\t\tvocab_size : Size of the vocabulary containing unique words\n\t\tembedding_length : Embedding dimension of GloVe word embeddings\n\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n\t\t\n\t\t\"\"\"\n\n        self.batch_size = batch_size\n        self.output_size = output_size\n        self.hidden_size = hidden_size\n        self.vocab_size = vocab_size\n        self.embedding_length = embedding_length\n\n        self.word_embeddings = nn.Embedding(\n            vocab_size, embedding_length)  # Initializing the look-up table.\n        self.word_embeddings.weight = nn.Parameter(\n            weights, requires_grad=False\n        )  # Assigning the look-up table to the pre-trained GloVe word embedding.\n        self.dropout = 0.8\n        self.lstm = nn.LSTM(\n            embedding_length,\n            hidden_size,\n            dropout=self.dropout,\n            bidirectional=True)\n        self.W2 = nn.Linear(2 * hidden_size + embedding_length, hidden_size)\n        self.label = nn.Linear(hidden_size, output_size)\n\n    def forward(self, input_sentence, batch_size=None):\n        \"\"\" \n\t\tParameters\n\t\t----------\n\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n\t\t\n\t\tReturns\n\t\t-------\n\t\tOutput of the linear layer containing logits for positive & negative class which receives its input as the final_hidden_state of the LSTM\n\t\tfinal_output.shape = (batch_size, output_size)\n\t\t\n\t\t\"\"\"\n        \"\"\"\n\t\t\n\t\tThe idea of the paper \"Recurrent Convolutional Neural Networks for Text Classification\" is that we pass the embedding vector\n\t\tof the text sequences through a bidirectional LSTM and then for each sequence, our final embedding vector is the concatenation of \n\t\tits own GloVe embedding and the left and right contextual embedding which in bidirectional LSTM is same as the corresponding hidden\n\t\tstate. This final embedding is passed through a linear layer which maps this long concatenated encoding vector back to the hidden_size\n\t\tvector. After this step, we use a max pooling layer across all sequences of texts. This converts any varying length text into a fixed\n\t\tdimension tensor of size (batch_size, hidden_size) and finally we map this to the output layer.\n\t\t\"\"\"\n        # embedded input of shape = (batch_size, num_sequences, embedding_length)\n        input = self.word_embeddings(input_sentence)\n        # input.size() = (num_sequences, batch_size, embedding_length)\n        input = input.permute(1, 0, 2)\n        if batch_size is None:\n            if torch.cuda.is_available():\n                # Initial hidden state of the LSTM\n                h_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).cuda())\n                # Initial cell state of the LSTM\n                c_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).cuda())\n            else:\n                h_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size))\n                c_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size))\n        else:\n            if torch.cuda.is_available():\n                h_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n                c_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n            else:\n                h_0 = Variable(torch.zeros(2, batch_size, self.hidden_size))\n                c_0 = Variable(torch.zeros(2, batch_size, self.hidden_size))\n\n        output, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n\n        final_encoding = torch.cat((output, input), 2).permute(1, 0, 2)\n        # y.size() = (batch_size, num_sequences, hidden_size)\n        y = self.W2(final_encoding)\n        # y.size() = (batch_size, hidden_size, num_sequences)\n        y = y.permute(0, 2, 1)\n        # y.size() = (batch_size, hidden_size, 1)\n        y = F.max_pool1d(y, y.size()[2])\n        y = y.squeeze(2)\n        logits = self.label(y)\n\n        return logits","4dd434d0":"import os\nimport time\nimport torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport numpy as np","4c9a6fe6":"TEXT, vocab_size, word_embeddings, train_iter, valid_iter = load_dataset()","f3b16b0d":"def clip_gradient(model, clip_value):\n    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n    for p in params:\n        p.grad.data.clamp_(-clip_value, clip_value)","74226697":"def train_model(model, train_iter, epoch):\n    total_epoch_loss = 0\n    total_epoch_acc = 0\n    \n    if torch.cuda.is_available():\n        model.cuda()\n        \n    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n    steps = 0\n    model.train()\n    for idx, batch in enumerate(train_iter):\n        text = batch.text[0]\n        target = batch.label\n        target = torch.autograd.Variable(target).long()\n        if torch.cuda.is_available():\n            text = text.cuda()\n            target = target.cuda()\n        if (text.size()[0] is not 32):# One of the batch returned by BucketIterator has length different than 32.\n            continue\n        optim.zero_grad()\n        prediction = model(text)\n        loss = loss_fn(prediction, target)\n        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n        acc = 100.0 * num_corrects\/len(batch)\n        loss.backward()\n        clip_gradient(model, 1e-1)\n        optim.step()\n        steps += 1\n        \n        if steps % 100 == 0:\n            print (f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n        \n        total_epoch_loss += loss.item()\n        total_epoch_acc += acc.item()\n        \n    return total_epoch_loss\/len(train_iter), total_epoch_acc\/len(train_iter)","f302f418":"def eval_model(model, val_iter):\n    total_epoch_loss = 0\n    total_epoch_acc = 0\n    model.eval()\n    with torch.no_grad():\n        for idx, batch in enumerate(val_iter):\n            text = batch.text[0]\n            if (text.size()[0] is not 32):\n                continue\n            target = batch.label\n            target = torch.autograd.Variable(target).long()\n            if torch.cuda.is_available():\n                text = text.cuda()\n                target = target.cuda()\n            prediction = model(text)\n            loss = loss_fn(prediction, target)\n            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n            acc = 100.0 * num_corrects\/len(batch)\n            total_epoch_loss += loss.item()\n            total_epoch_acc += acc.item()\n\n    return total_epoch_loss\/len(val_iter), total_epoch_acc\/len(val_iter)","859d8920":"earning_rate = 2e-5\nbatch_size = 32\noutput_size = 2\nhidden_size = 256\nembedding_length = 300","f93b5175":"model = RCNN(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)","ca05cb7c":"loss_fn = F.cross_entropy\n\nfor epoch in range(10):\n    train_loss, train_acc = train_model(model, train_iter, epoch)\n    val_loss, val_acc = eval_model(model, valid_iter)\n    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')","dc9b3069":"## load data"}}