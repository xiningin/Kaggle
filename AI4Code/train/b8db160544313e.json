{"cell_type":{"4c5b1465":"code","095fa5e4":"code","864bf41c":"code","c6793692":"code","01c04d9e":"code","5f84bd40":"code","dfc66261":"code","6bad17b0":"code","fd3222ea":"code","15298952":"code","6eb32ce6":"code","0fdf0c8d":"code","0ee0ad22":"code","80771495":"code","7918c6f0":"code","6cab0528":"code","704751e2":"code","18d79364":"code","8978443a":"code","769d5fce":"code","33ff87ed":"code","88fe5cf2":"code","07ade62e":"code","80678fc2":"code","e8f2ca5a":"code","dfef9890":"code","7f84f8ee":"code","03f11b63":"code","bdc96aef":"code","dd84ac03":"code","c73cab80":"code","fa13759a":"code","4366a381":"code","4dceb08d":"code","d6c95508":"code","f88918c5":"code","fe630b0c":"code","937d6e69":"code","94eb5703":"code","867cab46":"code","35463d11":"code","04111545":"code","45c80f9e":"code","657ed3ec":"code","f24aa500":"code","6e8eef45":"code","97c7231b":"code","0422d615":"code","8c7a2101":"code","e9781d9c":"code","2ada9d84":"code","af84c6d8":"code","dc59753f":"code","668d8608":"code","9201fa37":"code","176f04f7":"code","92c43ad7":"code","4d694623":"markdown","7d4a71af":"markdown","165c8f34":"markdown","b8c0ea51":"markdown","a89441ab":"markdown","b5a242bb":"markdown","66f9e805":"markdown","d06d0353":"markdown","272d044d":"markdown","804ac863":"markdown","f227162e":"markdown","6987223b":"markdown","1d909bd8":"markdown"},"source":{"4c5b1465":"import pandas as pd\nimport numpy as np","095fa5e4":"trained = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')               # importing datasets\ntrained.head()                                               ","864bf41c":"test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest.head()","c6793692":"test.shape","01c04d9e":"trained.drop(['PassengerId','Name','Ticket','Cabin'],axis=1,inplace=True)","5f84bd40":"test.drop(['PassengerId','Name','Ticket','Cabin'],axis=1,inplace=True)","dfc66261":"trained.head(10)","6bad17b0":"trained.info()                 #information about the dataset","fd3222ea":"trained.describe()           #describing the dataset","15298952":"trained.isnull().sum()       #checking the null values","6eb32ce6":"# filling the null values with its mean\ntrained['Age'].fillna(trained['Age'].mean(),inplace=True)\ntrained['Age']","0fdf0c8d":"test['Age'].fillna(test['Age'].mean(),inplace=True)\ntest['Age']","0ee0ad22":"trained.isnull().sum()","80771495":"trained['Embarked'].replace('nan',np.nan,inplace=True)\ntrained['Embarked'].fillna(trained['Embarked'].mode()[0],inplace=True)","7918c6f0":"trained.isnull().sum()","6cab0528":"test['Embarked'].replace('nan',np.nan,inplace=True)\ntest['Embarked'].fillna(test['Embarked'].mode()[0],inplace=True)","704751e2":"trained.head()","18d79364":"trained['Sex']=trained['Sex'].map({'male':0,'female':1})\n\ntrained['Sex']","8978443a":"test['Sex']=test['Sex'].map({'male':0,'female':1})\n\ntest['Sex']","769d5fce":"trained['Embarked']=trained['Embarked'].map({'S':0,'C':1,'Q':2})","33ff87ed":"test['Embarked']=test['Embarked'].map({'S':0,'C':1,'Q':2})","88fe5cf2":"trained.head()","07ade62e":"trained.isnull().any()","80678fc2":"test['Fare'].replace(('nan',np.nan),inplace=True)\ntest['Fare'].fillna(test['Fare'].mean(),inplace=True)","e8f2ca5a":"test.isnull().any()","dfef9890":"trained['Embarked'].unique()\n\n","7f84f8ee":"x=trained.drop(['Survived'],axis=1)\n\ny=trained['Survived']","03f11b63":"#train_test_splitting of the dataset\nfrom sklearn.model_selection import train_test_split \nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=0)","bdc96aef":"from sklearn.linear_model import LogisticRegression\n\n#log_reg = LogisticRegression()\n\n#\/opt\/conda\/lib\/python3.7\/site-packages\/sklearn\/linear_model\/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n#STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n#\n#Increase the number of iterations (max_iter) or scale the data as shown in:\n#    https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html\n#Please also refer to the documentation for alternative solver options:\n#    https:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#logistic-regression\n#  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n\nlog_reg = LogisticRegression(max_iter=1500)\nlog_reg.fit(x_train,y_train)                         ","dd84ac03":"#hyperparameter tuning of logistic regression\nfrom sklearn.model_selection import GridSearchCV\nparam = {\n         'penalty':['l1','l2'],\n         'C':[0.001, 0.01, 0.1, 1, 10, 20,100, 1000]\n}\nlr= LogisticRegression(penalty='l1')\ncv=GridSearchCV(log_reg,param,cv=5,n_jobs=-1)\ncv.fit(x_train,y_train)\ncv.predict(x_test)","c73cab80":" #best_parameters\nprint(\"Best CV params\", cv.best_params_)          ","fa13759a":" #best_score\nprint(\"Best CV score\", cv.best_score_)","4366a381":" #best_estimators\nbest_lr = cv.best_estimator_\nbest_lr","4dceb08d":"prob = log_reg.predict_proba(x_train)\nprint(\"Maximum predicted probability\",np.max(prob))","d6c95508":"from sklearn.svm import SVC\nsvm= SVC()\nsvm.fit(x_train,y_train)\ny_preds=svm.predict(x_test)\nsvm.score(x_test,y_test)","f88918c5":"from sklearn.linear_model import SGDClassifier\nlinear_classifier = SGDClassifier(random_state=0)\n\n\nparameters = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], \n             'loss':['hinge','log'], 'penalty':['l1','l2']}\nsearcher = GridSearchCV(linear_classifier, parameters, cv=10)\nsearcher.fit(x_train, y_train)\n\n\nprint(\"Test accuracy of best grid search hypers:\", searcher.score(x_test, y_test))","fe630b0c":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier()\n\nknn.fit(x_train, y_train)\n\npred = knn.predict(x_test)\n","937d6e69":"#accuracy,confusion matrix and classification report\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nprint(\"accuracy score for trained data\",accuracy_score(y_train,knn.predict(x_train)))\nprint(\"accuracy score is\",accuracy_score(y_test,pred))\n\nprint(\"Confusion matrix\",confusion_matrix(y_test,pred))\n\nprint(\"Report\",classification_report(y_test,pred))","94eb5703":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(max_depth=6, random_state=123)\n\ndt.fit(x_train,y_train)\n\ny_preds = dt.predict(x_test)","867cab46":"print(\"accuracy score for trained data\",accuracy_score(y_train,dt.predict(x_train)))\nprint(\"accuracy score is\",accuracy_score(y_test,y_preds))\n\nprint(\"Confusion matrix\",confusion_matrix(y_test,y_preds))\n\nprint(\"Report\",classification_report(y_test,y_preds))","35463d11":"#hyper parameter tuning\ngrid_param = {\n    'criterion' : ['gini', 'entropy'],\n    'max_depth' : [3, 5, 7, 10,13],\n    'splitter' : ['best', 'random'],\n    'min_samples_leaf' : [1, 2, 3, 5, 7],\n    'min_samples_split' : [1, 2, 3, 5, 7],\n    'max_features' : ['auto', 'sqrt', 'log2']\n}\n\ndecision = GridSearchCV(dt, grid_param, cv = 5, n_jobs = -1, verbose = 1)\ndecision.fit(x_train, y_train)\ndecision.best_score_","04111545":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier(max_depth=10,min_samples_split=9)\nrfc.fit(x_train,y_train)\n\npredict=rfc.predict(x_test)\n\nprint(accuracy_score(y_test,predict))\n\n","45c80f9e":"#accuracy score,classification report,confusion matrix\nprint(\"accuracy score for trained data\",accuracy_score(y_train,rfc.predict(x_train)))\nprint(\"accuracy score is\",accuracy_score(y_test,predict))\n\nprint(\"Confusion matrix\",confusion_matrix(y_test,predict))\n\nprint(\"Report\",classification_report(y_test,predict))","657ed3ec":"from sklearn.ensemble import AdaBoostClassifier\nadb = AdaBoostClassifier(base_estimator = dt)\n\nadb.fit(x_train,y_train)\n\npredicts=adb.predict(x_test)","f24aa500":"print(\"accuracy score for trained data\",accuracy_score(y_train,adb.predict(x_train)))\nprint(\"accuracy score is\",accuracy_score(y_test,predicts))\n\nprint(\"Confusion matrix\",confusion_matrix(y_test,predicts))\n\nprint(\"Report\",classification_report(y_test,predicts))","6e8eef45":"#hyper parameter tuning.\ngrid_param = {\n    'n_estimators' : [100, 120, 150, 180, 200],\n    'learning_rate' : [0.01, 0.1, 1, 10],\n    'algorithm' : ['SAMME', 'SAMME.R']\n}\n\nsearch_ada = GridSearchCV(adb, grid_param, cv = 10, n_jobs = -1, verbose = 1)\nsearch_ada.fit(x_train, y_train)","97c7231b":"search_ada.best_score_","0422d615":"adc = AdaBoostClassifier(base_estimator = dt, algorithm = 'SAMME', learning_rate = 0.01, n_estimators = 200)\nadc.fit(x_train, y_train)","8c7a2101":"accuracy_score(y_test,adc.predict(x_test))","e9781d9c":"from sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier()\ngb.fit(x_train, y_train)","2ada9d84":"accuracy_score(y_test,gb.predict(x_test))","af84c6d8":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(objective='binary:logistic',booster = 'gbtree', gamma=5,learning_rate = 0.1, max_depth = 5, n_estimators = 100,colsample_bytree=1)\nxgb.fit(x_train, y_train)","dc59753f":"accuracy_score(y_test,xgb.predict(x_test))","668d8608":"print(\"accuracy score for trained data\",accuracy_score(y_train,xgb.predict(x_train)))\nprint(\"accuracy score is\",accuracy_score(y_test,xgb.predict(x_test)))\n\nprint(\"Confusion matrix\",confusion_matrix(y_test,xgb.predict(x_test)))\n\nprint(\"Report\",classification_report(y_test,xgb.predict(x_test)))","9201fa37":"from sklearn.ensemble import VotingClassifier \nclassifiers = [('Logistic Regression', log_reg),('K Nearest Neighbours', knn), ('Classification Tree', gb)]\n# Instantiate a VotingClassifier vc\nvc = VotingClassifier(estimators=classifiers )     \n\n# Fit vc to the training set\nvc.fit(x_train,y_train)   \n\n# Evaluate the test set predictions\ny_pred = vc.predict(x_test)\n\n# Calculate accuracy score\naccuracy = accuracy_score(y_test,y_pred)\nprint('Voting Classifier: {:.3f}'.format(accuracy))\n","176f04f7":"finals_predi = vc.predict(test)\n\nfinals_predi","92c43ad7":"predictio = pd.DataFrame(finals_predi)\nsubs_d = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsubs_d['Survived'] = predictio\nsubs_d.to_csv('Submissionsd1.csv', index = False)","4d694623":"**SGDClassifier**","7d4a71af":"**XGBClassifier**","165c8f34":"Converting categorical data to numerical.","b8c0ea51":"**VotingClassifier**","a89441ab":"**Logistic Regression**","b5a242bb":"**AdaBoostClassifier**","66f9e805":"# Introduction\n\nThis notebook refers to\nhttps:\/\/www.kaggle.com\/anshigupta01\/titanic-prediction-top-17","d06d0353":"**RandomForestClassifier**","272d044d":"Dropping some columns which doesnot play a significant role in the dataset.","804ac863":"**KNN**","f227162e":"**GradientBoostingClassifier**","6987223b":"**Support Vector Classifier**","1d909bd8":"**DecisionTreeClassifier**"}}