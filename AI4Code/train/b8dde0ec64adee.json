{"cell_type":{"d20985f0":"code","34c3d013":"code","3ec33e1a":"code","6a6b3584":"code","b7eb70d9":"code","627b0203":"code","2bee102a":"code","10a7bb8c":"code","950c1c3b":"code","eaa39597":"code","a19f7e66":"code","016a204d":"code","3269654c":"code","c21831db":"code","d56291fc":"code","4201b9d6":"code","c09aea39":"code","5d68261e":"code","6806f40b":"code","8a56225b":"code","d8fcafd9":"code","04525752":"code","d4f0ee8d":"code","4864204e":"code","2d602266":"code","8bc3c6a3":"code","d4a98dee":"code","80b08ab0":"code","8eeca5e9":"code","a3fb4e79":"code","31579c3f":"code","5b7ed3d2":"code","8d612995":"code","9b8a61ba":"code","fda408a8":"code","064f0c3c":"code","f0357ee6":"code","b73628e0":"code","157b4f01":"code","3636c106":"code","b8db42f6":"code","d8afc379":"code","6a850b79":"code","0fb36df5":"code","3a925825":"code","a748478d":"code","59cc5aa9":"code","4d3db5ac":"code","95ec2884":"markdown","967f452b":"markdown","54ec2fe4":"markdown","8cd11a13":"markdown","fadb2aa2":"markdown","9d376fc4":"markdown","876a299a":"markdown","69ac39da":"markdown","94089b51":"markdown","176d6b30":"markdown"},"source":{"d20985f0":"# Directive pour afficher les graphiques dans Jupyter\n%matplotlib inline\n\n# Pandas : librairie de manipulation de donn\u00e9es\n# NumPy : librairie de calcul scientifique\n# MatPlotLib : librairie de visualisation et graphiques\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import model_selection\n\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import datasets","34c3d013":"import tensorflow as tf\n\nfrom tensorflow.keras.models import Sequential, load_model\n\nfrom tensorflow.keras.layers import InputLayer, Dense, Dropout, Flatten\n\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, MaxPool2D\n\nfrom tensorflow.keras.utils import to_categorical\n\nfrom tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator\n","3ec33e1a":"from tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom tensorflow.keras.layers.experimental.preprocessing import Rescaling, RandomFlip, RandomRotation, RandomZoom, RandomContrast, RandomTranslation","6a6b3584":"def plot_scores(train) :\n    accuracy = train.history['accuracy']\n    val_accuracy = train.history['val_accuracy']\n    epochs = range(len(accuracy))\n    plt.plot(epochs, accuracy, 'b', label='Score apprentissage')\n    plt.plot(epochs, val_accuracy, 'r', label='Score validation')\n    plt.title('Scores')\n    plt.legend()\n    plt.show()","b7eb70d9":"train_data_dir = \"..\/input\/marvel-heroes\/marvel\/train\" \nimage_size = (299, 299)\n\ndataset = image_dataset_from_directory(\n    train_data_dir,\n    image_size=image_size\n)\n","627b0203":"plt.figure(figsize=(15, 25))\nclass_names = dataset.class_names\nfor images, labels in dataset.take(1):\n    for i in range(32):\n        plt.subplot(7, 5, i + 1)\n        plt.imshow(np.array(images[i]).astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")","2bee102a":"train_data_dir = \"..\/input\/marvel-heroes\/marvel\/train\" \nimage_size = (299, 299)\n\ntrain_dataset = image_dataset_from_directory(\n    train_data_dir,\n    label_mode=\"categorical\",\n    image_size=image_size\n)\n\ntest_data_dir = \"..\/input\/marvel-heroes\/marvel\/valid\"\nimage_size = (299, 299)\n\nvalidation_dataset = image_dataset_from_directory(\n    test_data_dir,\n    label_mode=\"categorical\",\n    image_size=image_size\n)","10a7bb8c":"AUTOTUNE = tf.data.experimental.AUTOTUNE\n\ntrain_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\nvalidation_dataset = validation_dataset.cache().prefetch(buffer_size=AUTOTUNE)","950c1c3b":"# Mod\u00e8le CNN \nmodel = Sequential()                                      #mod\u00e8le en couche\nmodel.add(InputLayer(input_shape=(299, 299, 3)))          #obligatoire de donner la taille de l'ia$mage\nmodel.add(Rescaling(scale=1.\/255))                        \nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(20, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\n#model.add(Dense(2, activation='softmax', kernel_initializer=tf.keras.initializers.Constant(0.01)))\nmodel.add(Dense(8, activation='softmax'))\n\n# Compilation du mod\u00e8le\nmodel.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(1e-4), metrics=['accuracy'])   #compil    optimizer: ici on choisit son learning rate.si \u00e7a converge pas , on modifit ce param\u00e8tre","eaa39597":"history = model.fit(\n    train_dataset, \n    validation_data=validation_dataset, \n    epochs=50,\n    verbose=1)","a19f7e66":"plot_scores(history)","016a204d":"data_augmentation = Sequential([\n    RandomFlip(\"horizontal\"),\n    RandomRotation(0.1),\n    RandomZoom((-0.1,0.1)),\n    RandomContrast(0.05),  \n    RandomTranslation(0.1,0.1)\n])","3269654c":"# Mod\u00e8le CNN \nmodel = Sequential()                                      #mod\u00e8le en couche\nmodel.add(InputLayer(input_shape=(299, 299, 3)))          #obligatoire de donner la taille de l'ia$mage\nmodel.add(data_augmentation)                              #couche d'orientation \nmodel.add(Rescaling(scale=1.\/255))                        \nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(20, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\n#model.add(Dense(2, activation='softmax', kernel_initializer=tf.keras.initializers.Constant(0.01)))\nmodel.add(Dense(8, activation='softmax'))\n# Compilation du mod\u00e8le\nmodel.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(1e-4), metrics=['accuracy'])   #compil    optimizer: ici on choisit son learning rate.si \u00e7a converge pas , on modifit ce param\u00e8tre","c21831db":"history = model.fit(\n    train_dataset, \n    validation_data=validation_dataset, \n    epochs=100,\n    verbose=1)","d56291fc":"plot_scores(history)","4201b9d6":"model = Sequential()\nmodel.add(InputLayer(input_shape=(299, 299, 3)))\nmodel.add(data_augmentation)\nmodel.add(Rescaling(scale=1.\/255))\nmodel.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Flatten())\nmodel.add(Dense(8, activation='softmax', kernel_initializer=tf.keras.initializers.Constant(0.01)))\n\n# Compilation du mod\u00e8le\nmodel.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(1e-4), metrics=['accuracy'])","c09aea39":"history = model.fit(\n    train_dataset, \n    validation_data=validation_dataset, \n    epochs=50,\n    verbose=1)","5d68261e":"plot_scores(history)","6806f40b":"from tensorflow.keras.applications import Xception\nxception = Xception(weights='imagenet', include_top=False, input_shape=(299,299,3))\nxception.trainable = False","8a56225b":"model = Sequential()\nmodel.add(data_augmentation)\nmodel.add(Rescaling(scale=1.\/255))\nmodel.add(xception)\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(8, activation='softmax'))\n\n# Compilation du mod\u00e8le\nmodel.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(1e-4), metrics=['accuracy'])","d8fcafd9":"history = model.fit(\n    train_dataset, \n    validation_data=validation_dataset, \n    epochs=25,\n    verbose=1)","04525752":"plot_scores(history)","d4f0ee8d":"!pip install imageai","4864204e":"!wget \"https:\/\/github.com\/OlafenwaMoses\/ImageAI\/releases\/download\/essential-v4\/pretrained-yolov3.h5\"","2d602266":"!wget \"..\/input\/groupe\/marvel.jpg\"","8bc3c6a3":"from PIL import Image\nImage.open('..\/input\/groupe\/marvel.jpg')","d4a98dee":"from imageai.Detection import ObjectDetection","80b08ab0":"detector = ObjectDetection()\ndetector.setModelTypeAsYOLOv3()\ndetector.setModelPath('pretrained-yolov3.h5')\ndetector.loadModel()\ndetections = detector.detectObjectsFromImage(input_image='..\/input\/groupe\/marvel.jpg',\n                                             output_image_path='detected.png',\n                                             minimum_percentage_probability=30)\n\nfor eachObject in detections:\n    print(eachObject[\"name\"] , \" : \", eachObject[\"percentage_probability\"], \" : \", eachObject[\"box_points\"] )\n    print(\"--------------------------------\")","8eeca5e9":"detections","a3fb4e79":"Image.open('detected.png')","31579c3f":"!wget 'https:\/\/github.com\/OlafenwaMoses\/ImageAI\/releases\/download\/essentials-v5\/resnet50_coco_best_v2.1.0.h5'","5b7ed3d2":"detector = ObjectDetection()\ndetector.setModelTypeAsRetinaNet()\ndetector.setModelPath('resnet50_coco_best_v2.1.0.h5')\ndetector.loadModel()\ndetections = detector.detectObjectsFromImage(input_image='..\/input\/marvel-heroes\/marvel\/train\/captain america\/pic_012.jpg',\n                                             output_image_path='detected.png',\n                                             minimum_percentage_probability=50)\n\nfor eachObject in detections:\n    print(eachObject[\"name\"] , \" : \", eachObject[\"percentage_probability\"], \" : \", eachObject[\"box_points\"] )\n    print(\"--------------------------------\")","8d612995":"Image.open('detected.png')","9b8a61ba":"!pip install imageai","fda408a8":"!wget \"https:\/\/github.com\/OlafenwaMoses\/ImageAI\/releases\/download\/essential-v4\/pretrained-yolov3.h5\"","064f0c3c":"!wget 'https:\/\/github.com\/OlafenwaMoses\/ImageAI\/releases\/download\/essentials-v5\/resnet50_coco_best_v2.1.0.h5'","f0357ee6":"import numpy as np\nimport os\nimport shutil\nfrom pathlib import Path\nfrom PIL import Image","b73628e0":"root_annots_path = '..\/input\/marvel2\/captainwidow\/annotations\/'\nroot_images_path = '..\/input\/marvel2\/captainwidow\/my-project-name\/'\n\nannots_path = sorted([i for i in Path(root_annots_path).glob('*.xml')])\nimages_path = sorted([i for i in Path(root_images_path).glob('*.jpg')])\n\nn_imgs = len(images_path)\n\nclasses = np.array([\"black widow\",\"captain america\", \"blackwidow\"])","157b4f01":"with open(annots_path[120], 'r') as f:\n    print(f.read())","3636c106":"os.makedirs('imageai\/data\/train\/images', exist_ok=True)\nos.makedirs('imageai\/data\/train\/annotations', exist_ok=True)\n\nos.makedirs('imageai\/data\/validation\/images', exist_ok=True)\nos.makedirs('imageai\/data\/validation\/annotations', exist_ok=True)\n\nos.makedirs('imageai\/data\/test\/images', exist_ok=True)\nos.makedirs('imageai\/data\/test\/annotations', exist_ok=True)","b8db42f6":"n_imgs = 200\nn_split = n_imgs \/\/ 20\n\n\nfor i, (annot_path, img_path) in enumerate(zip(annots_path, images_path)):    \n    if i > n_imgs:\n        break\n    # train-val-test split\n    if i < n_split:\n        shutil.copy(img_path, 'imageai\/data\/test\/images\/' + img_path.parts[-1])\n        shutil.copy(annot_path, 'imageai\/data\/test\/annotations\/' + annot_path.parts[-1])\n    elif n_split <= i < n_split*5:\n        shutil.copy(img_path, 'imageai\/data\/validation\/images\/' + img_path.parts[-1])\n        shutil.copy(annot_path, 'imageai\/data\/validation\/annotations\/' + annot_path.parts[-1])\n    else:\n        shutil.copy(img_path, 'imageai\/data\/train\/images\/' + img_path.parts[-1])\n        shutil.copy(annot_path, 'imageai\/data\/train\/annotations\/' + annot_path.parts[-1])","d8afc379":"print(len(list(Path('imageai\/data\/train\/annotations\/').glob('*.xml'))))\nprint(len(list(Path('imageai\/data\/validation\/annotations\/').glob('*.xml'))))\nprint(len(list(Path('imageai\/data\/test\/annotations\/').glob('*.xml'))))","6a850b79":"from imageai.Detection.Custom import DetectionModelTrainer\n\ntrainer = DetectionModelTrainer()\ntrainer.setModelTypeAsYOLOv3()\ntrainer.setDataDirectory(data_directory=\".\/imageai\/data\/\")\ntrainer.setTrainConfig(object_names_array=[\"black widow\", \"captain america\", \"blackwidow\"],\n                       batch_size=8,\n                       num_experiments=10,\n                       train_from_pretrained_model=\"pretrained-yolov3.h5\")\n\ntrainer.trainModel()","0fb36df5":"from imageai.Detection.Custom import DetectionModelTrainer\n\ntrainer = DetectionModelTrainer()\ntrainer.setModelTypeAsYOLOv3()\ntrainer.setDataDirectory(data_directory=\".\/imageai\/data\/\")\nmetrics = trainer.evaluateModel(model_path=\"imageai\/data\/models\/\",\n                                json_path=\"imageai\/data\/json\/detection_config.json\",\n                                iou_threshold=0.2,\n                                object_threshold=0.3,\n                                nms_threshold=0.5)","3a925825":"!wget \"https:\/\/img.filmsactu.net\/datas\/films\/c\/a\/captain-america-3\/xl\/captain-america-3-5735a89c20363.jpg\"","a748478d":"from imageai.Detection.Custom import CustomObjectDetection\n\ndetector = CustomObjectDetection()\ndetector.setModelTypeAsYOLOv3()\ndetector.setModelPath(\"imageai\/data\/models\/detection_model-ex-006--loss-0032.539.h5\")\ndetector.setJsonPath(\"imageai\/data\/json\/detection_config.json\")\ndetector.loadModel()\ndetections = detector.detectObjectsFromImage(minimum_percentage_probability=50,\n                                             input_image=\"..\/input\/marvel2\/captainwidow\/my-project-name\/black (116).jpg\",\n                                             output_image_path=\"detected.jpg\")\nfor detection in detections:\n    print(detection[\"name\"], \" : \", detection[\"percentage_probability\"], \" : \", detection[\"box_points\"])\n    ","59cc5aa9":"Image.open('detected.jpg')","4d3db5ac":"detections","95ec2884":"Retina Net","967f452b":"![image.png](attachment:b592ea0e-9ebb-4509-90e5-24f5b6dedac7.png)","54ec2fe4":"![image.png](attachment:4e88387e-e06d-4f8b-aef0-4e9042ebbf20.png)","8cd11a13":"## RECONNAISSANCE D'OBJETS","fadb2aa2":"Avec epochs = 50\n![image.png](attachment:a605544f-9226-41ab-aa66-e9651f2f7e8b.png)\n\nAvec epochs = 100\n![image.png](attachment:80ff033b-3d84-42a5-9b0d-182be6603a08.png)","9d376fc4":"## FICHIERS XML","876a299a":"## Transfer learning avec Xception","69ac39da":"Fonction : ","94089b51":"![image.png](attachment:d4cad179-464d-48bc-997b-372cddff7509.png)","176d6b30":"**Essai avec un mod\u00e8le de type VGG16 :**\n"}}