{"cell_type":{"06546e0d":"code","97b479a0":"code","565943a8":"code","4f39cc39":"code","0f19e6d9":"code","9ec4de5f":"code","1bba197a":"code","894c67b2":"code","d0d11a48":"code","3ed9676a":"code","9b1c36c9":"code","d7f9a1dd":"code","80461b20":"code","26813e37":"code","91865ada":"code","84e8f4f8":"code","b658e909":"code","72af3bb7":"code","9730253a":"code","a6a00dd8":"code","9fea34a8":"code","8eafe3b2":"code","c754a5d5":"code","c3f43aac":"code","9ec1c6a1":"code","5e8e6426":"code","e2a31ef1":"code","32895a87":"code","f89d1b59":"code","ccdeca46":"code","b6b74330":"code","bfbc6ad5":"code","89abf2be":"code","40285042":"code","349d5a1a":"code","507091e9":"code","fbad2c79":"code","a68f6079":"code","cae31e45":"markdown","496bc2a1":"markdown","e7a7e985":"markdown","9c55c17b":"markdown","20e77853":"markdown","22aa17ec":"markdown","b729cddb":"markdown"},"source":{"06546e0d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nsns.set_theme()","97b479a0":"customer = pd.read_csv('\/kaggle\/input\/loan-prediction-based-on-customer-behavior\/Training Data.csv')","565943a8":"customer.head()","4f39cc39":"customer.info()","0f19e6d9":"customer.isnull().sum()","9ec4de5f":"customer.drop('Id',axis = 1, inplace = True)","1bba197a":"customer.head()","894c67b2":"customer.describe()","d0d11a48":"corr = customer.corr()\ncorr","3ed9676a":"plt.figure(figsize=(10,5))\nsns.heatmap(corr,annot = True,cmap = 'coolwarm')","9b1c36c9":"sns.distplot(customer['Age'],kde=False,color='darkred',bins = 30)","d7f9a1dd":"sns.countplot(x='Experience',data=customer)","80461b20":"customer['Risk_Flag'].value_counts()","26813e37":"sns.countplot(x='Risk_Flag', data=customer)","91865ada":"sns.countplot(x='CURRENT_HOUSE_YRS',hue ='Risk_Flag', data=customer)","84e8f4f8":"sns.countplot(x='Married\/Single',hue ='Risk_Flag', data=customer)","b658e909":"customer['Profession'].nunique()","72af3bb7":"customer['Profession'].value_counts()","9730253a":"sns.boxplot(x='Risk_Flag',y='Age',hue='Married\/Single',data=customer)","a6a00dd8":"customer['Income']\/ 1000","9fea34a8":"customer.head()","8eafe3b2":"Marriage = pd.get_dummies(customer['Married\/Single'],drop_first=True)\nHouse_Ownership = pd.get_dummies(customer['House_Ownership'],drop_first=True)\nCar_Ownership = pd.get_dummies(customer['Car_Ownership'],drop_first=True)","c754a5d5":"customer.drop(['Married\/Single','House_Ownership','Car_Ownership','Profession','CITY','STATE',],axis=1,inplace=True)","c3f43aac":"customer","9ec1c6a1":"customer = pd.concat([customer,Marriage,House_Ownership,Car_Ownership],axis=1)","5e8e6426":"customer","e2a31ef1":"X = customer.drop('Risk_Flag',axis=1)\ny = customer['Risk_Flag']","32895a87":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","f89d1b59":"from imblearn.over_sampling import SMOTE\n\noversample = SMOTE()\nX_train, y_train = oversample.fit_resample(X_train, y_train)","ccdeca46":"from sklearn.tree import DecisionTreeClassifier\n\ndct = DecisionTreeClassifier()\ndct.fit(X_train,y_train)\ndct_predict = dct.predict(X_test)","b6b74330":"print(accuracy_score (y_test, dct_predict))\nprint(roc_auc_score (y_test, dct_predict))","bfbc6ad5":"dct_cfm = confusion_matrix(y_test,dct_predict)\n\ngroup_counts = ['{0:0.0f}'.format(value) for value in\n                dct_cfm.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in\n                     dct_cfm.flatten()\/np.sum(dct_cfm)]\nlabels = [f'{v2}\\n{v3}' for  v2, v3 in\n          zip(group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(dct_cfm, annot=labels, fmt='', cmap='Blues')","89abf2be":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\nlr_predictions = lr.predict(X_test)","40285042":"print(accuracy_score (y_test, lr_predictions))\nprint(roc_auc_score (y_test, lr_predictions)) ","349d5a1a":"lr_cfm = confusion_matrix(y_test,lr_predictions)\n\ngroup_counts = ['{0:0.0f}'.format(value) for value in\n                lr_cfm.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in\n                     lr_cfm.flatten()\/np.sum(lr_cfm)]\nlabels = [f'{v2}\\n{v3}' for  v2, v3 in\n          zip(group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(lr_cfm, annot=labels, fmt='', cmap='mako')","507091e9":"from sklearn.ensemble import RandomForestClassifier\n\nrft = RandomForestClassifier()\nrft.fit(X_train,y_train)\nrft_predict = rft.predict(X_test)","fbad2c79":"print(accuracy_score (y_test, rft_predict))\nprint(roc_auc_score (y_test, rft_predict)) ","a68f6079":"rft_cfm = confusion_matrix(y_test,rft_predict)\n\ngroup_counts = ['{0:0.0f}'.format(value) for value in\n                rft_cfm.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in\n                     rft_cfm.flatten()\/np.sum(rft_cfm)]\nlabels = [f'{v2}\\n{v3}' for  v2, v3 in\n          zip(group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(rft_cfm, annot=labels, fmt='', cmap='rocket_r')","cae31e45":"* DecisionTreeClassifier give us 0.86 accuracy and 0.86 auc score \n* From the confusion matrix we can see that the model is tend to have 0 for the output it is because the data is imbalance","496bc2a1":"##  Conclusion\n\n* From the three model that we use RandomForestClassifier give us higher accuracy and auc score than the other two model that we use \n* There will be improvement soon to have bette auc score any feedback is welcome and appreciated","e7a7e985":"# Loan Prediction Project \n\nIn this project we will be working with a loan customer behaviour data set, indicating whether or not a loan customer will default on loan or not. We will try to create a model that will predict whether or not they will default on a loan based on customer behaviour.\n\nThis data set contains the following features:\n\n* 'income': Income of the user\n* 'Age': cutomer age in years\n* 'experience': Professional experience of the user in years\n* 'profession': Profession\n* 'married': Whether married or single\n* 'City': City of consumer\n* 'house_ownership': Owned or rented or neither\n* 'currentjobyears': Years of experience in the current job\n* 'currenthouseyears': Number of years in the current residence\n* 'risk_flag\t': Defaulted on a loan\n* 'state\t': State of residence\n\n","9c55c17b":"* From the correlation we can see that customer who have bigger income and and have lived in their house for a long time tend not to default on loans \n* overall the higher the number in the variable the less likely they are to default on loan","20e77853":"from the value counts and countplot we can see that the data is very unbalanced which can cause poor performance in prediction, to overcome this we will do SMOTE(Synthetic Minority Oversampling Technique","22aa17ec":"* RandomForestClassifier give us 0.90 accuracy and 0.86 auc score \n* From the confusion matrix we can see that the model is tend to have 0 for the output it is because the data is imbalance","b729cddb":"* LogisticRegression give us 0.88 accuracy and 0.5 auc score\n* From the confusion matrix we can see that the model always give negative or 0 for the output so even though it has high accuracy this model cannot be used "}}