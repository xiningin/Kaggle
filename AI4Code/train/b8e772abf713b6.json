{"cell_type":{"522cf585":"code","f3c7d4f9":"code","1f7ba2ad":"code","c9078b75":"code","67683740":"code","b8dd9f09":"code","19a42318":"code","ae21c3f3":"code","e716c7b2":"code","0c2b6f20":"code","516ece78":"code","91786d59":"code","4f2f09c8":"code","d3070767":"code","382689ff":"code","d65a4c03":"code","ed50a679":"code","36e9149f":"code","9e8a2c05":"code","b532d72c":"code","0238afb4":"code","fdee2337":"code","cc756785":"code","54425f29":"code","762b02df":"code","5f5419af":"code","311f10e6":"code","1d8a4390":"code","c190a3ae":"code","055f0aa7":"code","6006b4df":"code","da1c7694":"code","0ca8ef12":"code","ef11bbf2":"code","7ed45e5f":"code","6da521c1":"code","d8654981":"code","3d5cd9d1":"markdown","e07887cf":"markdown","9eca84fa":"markdown","3119357b":"markdown","0ed8c314":"markdown","5a09202a":"markdown","62a23f79":"markdown","62152b97":"markdown","60dfa92a":"markdown","e7939cd5":"markdown","e5fd101f":"markdown","4add6254":"markdown","4ee87f47":"markdown","555aa17b":"markdown","58c94e54":"markdown","214112dd":"markdown","bc3cb5d1":"markdown","4815edca":"markdown","09e13ce8":"markdown","25718702":"markdown","10ab87f0":"markdown","b72289f0":"markdown","847ece41":"markdown","a2f8dc9e":"markdown","08edfcba":"markdown","9da04feb":"markdown"},"source":{"522cf585":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os","f3c7d4f9":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1f7ba2ad":"churn_data = pd.read_csv('\/kaggle\/input\/churn-modelling\/Churn_Modelling.csv')","c9078b75":"# Checking the head of the dataframe\n\nchurn_data.head()","67683740":"# Checking info about data\nchurn_data.info()","b8dd9f09":"churn_data.drop(['RowNumber', 'CustomerId', 'Surname'], axis = 1,  inplace = True)","19a42318":"churn_data.head()","ae21c3f3":"# Checking how many cities are there in Geography column\nchurn_data['Geography'].unique()","e716c7b2":"X = churn_data.iloc[:,:-1]\ny = churn_data.iloc[:, -1]","0c2b6f20":"# For Geography column\nX[['Germany','Spain']] = pd.get_dummies(X['Geography'], drop_first = True)\nX.drop('Geography', axis = 1, inplace = True)","516ece78":"# For Gender column\nX['Male'] = pd.get_dummies(X['Gender'], drop_first = True)\nX.drop('Gender', axis = 1, inplace = True)","91786d59":"X.head()","4f2f09c8":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 51)","d3070767":"print(X_train.shape)\nprint(X_test.shape)","382689ff":"X_train = X_train.to_numpy()\nX_test = X_test.to_numpy()\ny_train = y_train.to_numpy()\ny_test = y_test.to_numpy()","d65a4c03":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","ed50a679":"# Check scaled data\n# print(X_train)","36e9149f":"import keras\nfrom keras.models import Sequential \nfrom keras.layers import Dense","9e8a2c05":"model = Sequential()","b532d72c":"# Adding the input layer and first hidden layer\nmodel.add(keras.Input(11,))\nmodel.add(Dense(6, kernel_initializer = 'uniform', activation = 'relu'))\n\n# Adding the second hidden layer\nmodel.add(Dense(6, kernel_initializer = 'uniform', activation = 'relu'))\n\n\n# Adding the third hidden layer\nmodel.add(Dense(1, kernel_initializer = 'uniform', activation = 'sigmoid'))","0238afb4":"model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","fdee2337":"model.fit(X_train, y_train, batch_size = 10, epochs = 50)","cc756785":"# Predicting the test set results\ny_pred = model.predict(X_test)\n\ny_pred = (y_pred > 0.5)","54425f29":"# Evaluating the model\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\nacc_score = accuracy_score(y_test, y_pred)\nprint(\"Accuracy : \",acc_score)","762b02df":"# Predicting on new data\nnew_prediction = model.predict(scaler.fit_transform(np.array([[0.0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])))\nnew_prediction = (new_prediction > 0.5)\n\nprint(new_prediction)","5f5419af":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score","311f10e6":"# Builing the classifier function\ndef build_classifier():\n    model = Sequential()\n    model.add(keras.Input(11,))\n    model.add(Dense(6, kernel_initializer = 'uniform', activation = 'relu'))\n    model.add(Dense(6, kernel_initializer = 'uniform', activation = 'relu'))\n    model.add(Dense(1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return model","1d8a4390":"# Passing values to KerasClassifier \nmodel = KerasClassifier(build_fn = build_classifier, batch_size = 10, epochs = 50)","c190a3ae":"# We are using 10 fold cross validation here\naccuracies = cross_val_score(estimator = model, X = X_train, y = y_train, cv = 5)","055f0aa7":"# Checking the accuracies\nprint(accuracies)","6006b4df":"# Checking the mean and standard deviation of the accuracies obtained\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Mean : \",mean)\nprint(\"Variance : \",variance)","da1c7694":"from sklearn.model_selection import GridSearchCV","0ca8ef12":"# Building the classifier function\ndef build_classifier(optimizer):\n    model = Sequential()\n    model.add(keras.Input(11,))\n    model.add(Dense(6, kernel_initializer = 'uniform', activation = 'relu'))\n    model.add(Dense(6, kernel_initializer = 'uniform', activation = 'relu'))\n    model.add(Dense(1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    model.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return model","ef11bbf2":"# Passing values to KerasClassifier\nmodel = KerasClassifier(build_fn = build_classifier, batch_size = 10, epochs = 50)","7ed45e5f":"# Using Grid Search CV to getting the best parameters\nparameters = {'batch_size': [25, 32],\n             'epochs': [100, 150],\n             'optimizer': ['adam', 'rmsprop']}\n\ngrid_search = GridSearchCV(estimator = model, param_grid = parameters,\n                          scoring = 'accuracy',\n                          cv = 5)\n\ngrid_search.fit(X_train, y_train)","6da521c1":"best_parameters = grid_search.best_params_ \nbest_accuracy = grid_search.best_score_","d8654981":"print(\"Best Parameters : \",best_parameters)\nprint(\"Best Accuracy : \",best_accuracy)","3d5cd9d1":"#### Tuning the model","e07887cf":"## Churn Modelling\n\n***Churn rate, in its broadest sense, is a measure of the number of individuals or items moving out of a collective group over a specific period. It is one of two primary factors that determine the steady-state level of customers a business will support. (Source -> Wikipedia)***\n\n![image.png](attachment:image.png)\n\nIn this notebook the data set contains details of a bank's customers and the target variable is a binary variable reflecting the fact whether the customer left the bank (closed his account) or he continues to be a customer.","9eca84fa":"***See, the accuracy of our model got increased. Although it's not a big jump but a 1% increase is quite good.***","3119357b":"### Adding the input layer and hidden layers","0ed8c314":"### Evaluating the ANN\n\nWe will be using cross_val_score along with KerasClassifier wrapper to evalute our model.","5a09202a":"***So, on this note I'm gonna say bye\ud83e\udd17***\n\n***If you liked the notebook. Please give an upvote \ud83d\ude03***","62a23f79":"### Making the predictions and evaluating the model","62152b97":"## Evaluating, Improving and Tuning the ANN","60dfa92a":"### Improving the ANN\n\nHere, we are using Grid Search CV to improve our model by tuning the hyperparameters.\n\nAnd we are also adding dropout layers to our model to reduce any kind of overfitting.","e7939cd5":"Because our KerasClassifier wrapper expects its first argument to be a function, we will be building a classifier function first.","e5fd101f":"As we can see we are getting a good accuracy and a very small variance therefore we can say that using k-fold cross validation we obtained a more generalized model as compared to the previous one.\n\nBut the question is --->  Can we beat this accuracy ??\n\n***Let's see***\ud83e\udd28","4add6254":"But the model we developed here is not generalized because we can't say our model is good by just training it on a single test set.\n\nCan we improve performance of our model. Let's see","4ee87f47":"#### In this notebook you will see\n\n* ***Data Preprocessing***\n\n  1. Importing the libraries\n  2. Loading the data.\n  3. Encoding categorical data\n  4. Splitting the data into training and test set\n    \n    \n* ***Building the ANN(Artificial Neural Network)***\n\n  1. Inititalizing the ANN\n  2. Adding the input layer and hidden layers\n  3. Compiling the model\n  4. Fitting the ANN to the training data\n  5. Making the predictions and evaluating the model\n  \n    \n* ***Evaluating, Improving and Tuning the ANN***\n\n  1. Evaluating the model(Using Keras Classifier and K-fold cross validation)\n  2. Improving the ANN(Tuning model hyperparameters using Grid Search CV)","555aa17b":"As, we can see clearly we have categorical data in Geography and Gender column.\n\nWe will convert these to one hot representation, so that we can feed data to our model.","58c94e54":"### Importing the libraries","214112dd":"### Compiling the ANN","bc3cb5d1":"Now, let's convert train and test data to numpy arrays because our keras model will require data in array form.","4815edca":"### Loading the data","09e13ce8":"### Splitting the data into training and test set","25718702":"## Data Preprocessing","10ab87f0":"As, we can see RowNumber, CustomerId and Surname are not useful for us here. We will drop these features.","b72289f0":"### Encoding categorical data","847ece41":"### Fitting the ANN to the training data","a2f8dc9e":"## Building the ANN model\n\nIn this step we will import keras libraries and build an Artificial Neural Network.","08edfcba":"### Feature scaling\n\nLet's do feature scaling to convert variables to similar scale. So, that all features can be treated equally important.","9da04feb":"### Initializing the ANN"}}