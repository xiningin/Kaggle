{"cell_type":{"ef834e62":"code","9db7fa28":"code","39ad541c":"code","b03a7a44":"code","b74a1d6b":"code","6514d65d":"code","0682dcad":"code","e7cbd427":"code","c3bac042":"code","7ce3ff33":"code","a6c8284a":"code","ebb8e623":"code","85287b05":"code","c5900238":"code","cc6aed32":"code","daa680c2":"code","f94488b6":"code","6936a236":"code","1c309169":"code","730f3d61":"code","58b9d007":"code","b5c9a15d":"code","4534bf4b":"code","cd9988cd":"code","2e2cb958":"code","bc1b444b":"code","acdaa7c6":"code","2a42f034":"code","1c98b238":"code","f20cfdfe":"code","a1d7b75f":"code","f14b8ece":"markdown","290a58d8":"markdown","60717aeb":"markdown","4def15ae":"markdown","0662528f":"markdown","f31bdee5":"markdown","630547ca":"markdown","7d1133cd":"markdown","fc20a472":"markdown","939f1419":"markdown","99f0eac3":"markdown","a9475e25":"markdown","0ef52ff7":"markdown","f6238400":"markdown","0c867dc7":"markdown","d27ad253":"markdown"},"source":{"ef834e62":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9db7fa28":"# for installing Tokenization\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","39ad541c":"import nltk  # for text manipulation \nimport string \nimport warnings \nimport numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt  \nimport re\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nimport tokenization\n\npd.set_option(\"display.max_colwidth\", 200) \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n\n%matplotlib inline","b03a7a44":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","b74a1d6b":"print(train.shape)\ntrain.head()","6514d65d":"print(test.shape)\ntest.head()","0682dcad":"# looking into the number of real and unreal comments\ntrain['target'].value_counts()","e7cbd427":"#Distribution of real and unreal comment\nsns.barplot(train['target'].value_counts().index,train['target'].value_counts())","c3bac042":"# distribution of length of the tweets, in terms of words, in both train and test data.\nlength_train = train['text'].str.len() \nlength_test = test['text'].str.len() \nplt.hist(length_train, bins=20, label=\"train_text\") \nplt.hist(length_test, bins=20, label=\"test_text\") \nplt.legend() \nplt.show()","7ce3ff33":"#Distribution of real and unreal characters in train data\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntrain_len=train[train['target']==1]['text'].str.len()\nax1.hist(train_len)\nax1.set_title('Disaster tweets')\ntrain_len=train[train['target']==0]['text'].str.len()\nax2.hist(train_len, color=\"orange\")\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","a6c8284a":"#function for removing pattern\ndef remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n    return input_txt\n\n# remove '#' handle\ntrain['tweet'] = np.vectorize(remove_pattern)(train['text'], \"#[\\w]*\")\ntest['tweet'] = np.vectorize(remove_pattern)(test['text'], \"#[\\w]*\") \ntrain.head()","ebb8e623":"#Delete everything except alphabet\ntrain['tweet'] = train['tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\ntest['tweet'] = test['tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\ntrain.head()","85287b05":"#Dropping words whose length is less than 3\ntrain['tweet'] = train['tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ntest['tweet'] = test['tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ntrain.head()","c5900238":"#convert all the words into lower case\ntrain['tweet'] = train['tweet'].str.lower()\ntest['tweet'] = test['tweet'].str.lower()","cc6aed32":"from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \nset(stopwords.words('english'))\n\n# set of stop words\nstops = set(stopwords.words('english')) \n\n# tokens of words  \ntrain['tokenized_sents'] = train.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\ntest['tokenized_sents'] = test.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\n\n#function to remove stop words\ndef remove_stops(row):\n    my_list = row['tokenized_sents']\n    meaningful_words = [w for w in my_list if not w in stops]\n    return (meaningful_words)\n\n#removing stop words\ntrain['clean_tweet'] = train.apply(remove_stops, axis=1)\ntest['clean_tweet'] = test.apply(remove_stops, axis=1)\ntrain.drop([\"tweet\",\"tokenized_sents\"], axis = 1, inplace = True)\ntest.drop([\"tweet\",\"tokenized_sents\"], axis = 1, inplace = True)","daa680c2":"train.head()","f94488b6":"#re-join the words after tokenization\ndef rejoin_words(row):\n    my_list = row['clean_tweet']\n    joined_words = ( \" \".join(my_list))\n    return joined_words\n\ntrain['clean_tweet'] = train.apply(rejoin_words, axis=1)\ntest['clean_tweet'] = test.apply(rejoin_words, axis=1)\ntrain.head()","6936a236":"#Visualization of all the words using word cloud\nfrom wordcloud import WordCloud \nall_word = ' '.join([text for text in train['clean_tweet']])\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_word) \nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off') \nplt.show()","1c309169":"#Visualization of all the words which signify real disaster\nnormal_words =' '.join([text for text in train['clean_tweet'][train['target'] == 1]]) \nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words) \nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","730f3d61":"#Visualization of all the words which signify unreal disaster\nnormal_words =' '.join([text for text in train['clean_tweet'][train['target'] == 0]]) \nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words) \nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","58b9d007":"#Dropping the unnecessary column\ntrain.drop([\"keyword\",\"location\",\"text\"], axis = 1, inplace = True)\ntest.drop([\"keyword\",\"location\",\"text\"], axis = 1, inplace = True)\ntrain.head()","b5c9a15d":"from transformers import BertTokenizer\n\ntokenizer_QA = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n","4534bf4b":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer_QA.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer_QA.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\n\ndef build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","cd9988cd":"#Bert Module\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","2e2cb958":"#tokenization\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n# tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","bc1b444b":"train_input = bert_encode(train.clean_tweet.values, tokenizer_QA, max_len=160)\ntest_input = bert_encode(test.clean_tweet.values, tokenizer_QA, max_len=160)\ntrain_labels = train.target.values","acdaa7c6":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","2a42f034":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=16\n)","1c98b238":"model.load_weights('model.h5')\ntest_pred = model.predict(test_input)","f20cfdfe":"submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsubmission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('new_submission.csv', index=False)","a1d7b75f":"submission","f14b8ece":"# Importing Library","290a58d8":"# Basic EDA","60717aeb":"# Data Inspection","4def15ae":"**Prediction on test data**","0662528f":"# Implementing Bert\n\n**Helper Function**","f31bdee5":"# Build Model","630547ca":"# Data Visualization","7d1133cd":"You can see each word is tokenized , seperated with comma.","fc20a472":"# If you like my kernal... Please upvote","939f1419":"Word Cloud is a data visualization technique used for representing text data in which the size of each word indicates its frequency or importance. \n\nWords which are bigger in size, more often they are mentioned than the smaller words.","99f0eac3":"# Load Data","a9475e25":"**Submission**","0ef52ff7":"**Reference:**\nImplemented BERT by taking help from below notebook\nhttps:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub","f6238400":"# Text Normalization","0c867dc7":"**Stop Words:** Stop words are generally the most common words in a language.\n\nWhy do we remove stop words?\n\nWords such as articles and some verbs are usually considered stop words because they don't help us to find the context or the true meaning of a sentence. These are words that can be removed without any negative consequences to the final model that you are training.","d27ad253":"# Data Cleaning"}}