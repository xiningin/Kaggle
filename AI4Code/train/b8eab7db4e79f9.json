{"cell_type":{"44446231":"code","a853be85":"code","a2e3e0ac":"code","3c16f726":"code","47102a7b":"code","c1f9b476":"code","273900d3":"code","c10f33a4":"code","1d8e2b4b":"code","37f25ec7":"code","4af1bd80":"code","66b98f7b":"code","92344165":"code","617cbbd6":"code","5571f2c9":"code","494aeddb":"code","d5d4fc5b":"code","0b21a4af":"code","64342538":"code","23ec078f":"code","166ee1c7":"code","43a9df91":"code","2b993261":"code","94830dc6":"code","61ac14c2":"code","31163bf0":"code","c3fdc050":"code","76c454fa":"code","6848e54b":"code","e3bbeed0":"code","b152813e":"code","940903a8":"code","7cd46a1a":"code","b63f2187":"code","4c25c409":"code","8486236a":"code","970d970c":"code","ef9c02e9":"code","554cad69":"code","176c3144":"code","8247ea69":"code","e93a4335":"code","0f0bbbea":"code","875df2ad":"code","abdfaf75":"code","4ac13d43":"code","26cd6316":"code","3c51ca59":"code","16cee2de":"code","5e93fb74":"code","041d37de":"code","8012699d":"code","acb9052d":"code","8c362d4f":"code","d6eb14f0":"code","c488f15c":"code","b1d1d9c3":"code","68cf0ae2":"code","46a1587d":"code","e825c5ad":"code","503d0917":"code","f6a5f162":"code","1f95ddda":"code","89b12ba0":"code","d35fe774":"code","185eede3":"code","eb488fc9":"code","86c6a2e3":"code","a61acfc1":"code","a7bdac3f":"code","ad94a95d":"code","3cba4477":"code","17c4cf10":"code","fb7dfcc9":"code","23837aad":"code","8ee25dd5":"code","ccd63256":"code","0bae8c5d":"code","d4e42df0":"markdown","d64dfaeb":"markdown","f84f5262":"markdown","9b37e76e":"markdown","9eed9cce":"markdown","7ef34226":"markdown","7ba2fc6c":"markdown","ab1ddf58":"markdown","19c225be":"markdown","5144e3ff":"markdown","8d9d9987":"markdown","f8faeefd":"markdown","312fb827":"markdown","cd9f6358":"markdown","2e7f25c7":"markdown","42bf00de":"markdown","53ca9113":"markdown","0cd8ba85":"markdown","f7bc60c4":"markdown","b538bf8a":"markdown","6973cc17":"markdown","1901fa7d":"markdown","5ba7be17":"markdown","cec159ea":"markdown","c5bad6b8":"markdown","cbbc4b2b":"markdown","7944149f":"markdown","1a72b1ae":"markdown","233e1e19":"markdown","c32be56e":"markdown","cb23e6dd":"markdown","57b66e74":"markdown","e48c191c":"markdown","2dc3fc0e":"markdown","c2994d35":"markdown","d809f302":"markdown","aa5c67d5":"markdown","3ddcfb83":"markdown","4ed600ae":"markdown","05bb6768":"markdown","ee03823a":"markdown","3b3f6d7f":"markdown","67068148":"markdown","192a7804":"markdown","b3d0bf17":"markdown"},"source":{"44446231":"# Import TensorFlow\nimport tensorflow as tf\nprint(tf.__version__) # find the version number (should be 2.x+)","a853be85":"# run pip install tf-nightly-gpu if you get this error: tensorflow\/stream_executor\/cuda\/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n# restart reknel, then run previous cell again.","a2e3e0ac":"# Create a scalar (rank 0 tensor)\nscalar = tf.constant(7)\nscalar","3c16f726":"# Check the number of dimensions of a tensor (ndim stands for number of dimensions)\nscalar.ndim","47102a7b":"#Create a vector\nvector = tf.constant([5,7])\nvector","c1f9b476":"# Check the number of dimensions of our vector tensor\nvector.ndim","273900d3":"# Create a matrix (more than 1 dimension)\nmatrix = tf.constant([[1, 5],\n                      [7, 1]])\nmatrix","c10f33a4":"matrix.ndim","1d8e2b4b":"# Create another matrix and define the datatype\nanother_matrix = tf.constant([[10., 7.],\n                              [3., 2.],\n                              [8., 9.]], dtype=tf.float16) # specify the datatype with 'dtype'\nanother_matrix","37f25ec7":"# Even though another_matrix contains more numbers, its dimensions stay the same\nanother_matrix.ndim","4af1bd80":"# How about a tensor? (more than 2 dimensions, although, all of the above items are also technically tensors)\ntensor = tf.constant([\n  [[0, 1, 2, 3, 4],\n   [5, 6, 7, 8, 9]],\n  [[10, 11, 12, 13, 14],\n   [15, 16, 17, 18, 19]],\n  [[20, 21, 22, 23, 24],\n   [25, 26, 27, 28, 29]],])\ntensor","66b98f7b":"tensor.ndim","92344165":"# Create the same tensor with tf.Variable() and tf.constant()\nchangeable_tensor = tf.Variable([10, 7])\nunchangeable_tensor = tf.constant([10, 7])\nchangeable_tensor, unchangeable_tensor","617cbbd6":"# Will error (requires the .assign() method)\nchangeable_tensor[0] = 7\nchangeable_tensor","5571f2c9":"# Won't error\nchangeable_tensor[0].assign(7)\nchangeable_tensor","494aeddb":"# Will error (can't change tf.constant())\nunchangeable_tensor[0].assign(7)\nunchangleable_tensor","d5d4fc5b":"# Create two random (but the same) tensors\nrandom_1 = tf.random.Generator.from_seed(42) # set the seed for reproducibility\nrandom_1 = random_1.normal(shape=(3, 2)) # create tensor from a normal distribution \nrandom_2 = tf.random.Generator.from_seed(42)\nrandom_2 = random_2.normal(shape=(3, 2))\n\n# Are they equal?\nrandom_1, random_2, random_1 == random_2\n","0b21a4af":"# Create two random (and different) tensors\nrandom_3 = tf.random.Generator.from_seed(42)\nrandom_3 = random_3.normal(shape=(3, 2))\nrandom_4 = tf.random.Generator.from_seed(11)\nrandom_4 = random_4.normal(shape=(3, 2))\n\n# Check the tensors and see if they are equal\nrandom_3, random_4, random_1 == random_3, random_3 == random_4","64342538":"# Shuffle a tensor (valuable for when you want to shuffle your data)\nnot_shuffled = tf.constant([[10, 7],\n                            [3, 4],\n                            [2, 5]])\n# Gets different results each time\ntf.random.shuffle(not_shuffled)","23ec078f":"# Shuffle in the same order every time using the seed parameter (won't acutally be the same)\ntf.random.shuffle(not_shuffled, seed=42)","166ee1c7":"# Shuffle in the same order every time\n\n# Set the global random seed\ntf.random.set_seed(42) \n\n# Set the operation random seed\ntf.random.shuffle(not_shuffled, seed=42)","43a9df91":"# Set the global random seed\ntf.random.set_seed(42) # if you comment this out you'll get different results\n\n# Set the operation random seed\ntf.random.shuffle(not_shuffled)","2b993261":"# Make a tensor of all ones\ntf.ones(shape=(3, 2))","94830dc6":"# Make a tensor of all zeros\ntf.zeros(shape=(3, 2))","61ac14c2":"import numpy as np\nnumpy_A = np.arange(1, 25, dtype=np.int32) # create a NumPy array between 1 and 25\nA = tf.constant(numpy_A,  \n                shape=[2, 4, 3]) # note: the shape total (2*4*3) has to match the number of elements in the array\nnumpy_A, A","31163bf0":"# Create a rank 4 tensor (4 dimensions)\nrank_4_tensor = tf.zeros([2, 3, 4, 5])\nrank_4_tensor","c3fdc050":"rank_4_tensor.shape, rank_4_tensor.ndim, tf.size(rank_4_tensor)","76c454fa":"# Get various attributes of tensor\nprint(\"Datatype of every element:\", rank_4_tensor.dtype)\nprint(\"Number of dimensions (rank):\", rank_4_tensor.ndim)\nprint(\"Shape of tensor:\", rank_4_tensor.shape)\nprint(\"Elements along axis 0 of tensor:\", rank_4_tensor.shape[0])\nprint(\"Elements along last axis of tensor:\", rank_4_tensor.shape[-1])\nprint(\"Total number of elements (2*3*4*5):\", tf.size(rank_4_tensor).numpy()) # .numpy() converts to NumPy array","6848e54b":"# Get the first 2 items of each dimension\nrank_4_tensor[:2, :2, :2, :2]","e3bbeed0":"# Get the dimension from each index except for the final one\nrank_4_tensor[:1, :1, :1, :]","b152813e":"# Create a rank 2 tensor (2 dimensions)\nrank_2_tensor = tf.constant([[10, 7],\n                             [3, 4]])\n\n# Get the last item of each row\nrank_2_tensor[:, -1]","940903a8":"# Add an extra dimension (to the end)\nrank_3_tensor = rank_2_tensor[..., tf.newaxis] # in Python \"...\" means \"all dimensions prior to\"\nrank_2_tensor, rank_3_tensor # shape (2, 2), shape (2, 2, 1)","7cd46a1a":"#You can achieve the same using tf.expand_dims().\ntf.expand_dims(rank_2_tensor, axis=-1) # \"-1\" means last axis","b63f2187":"# You can add values to a tensor using the addition operator\ntensor = tf.constant([[10, 7], [3, 4]])\ntensor + 10","4c25c409":"# Original tensor unchanged\ntensor","8486236a":"# Multiplication (known as element-wise multiplication)\ntensor * 10","970d970c":"# Subtraction\ntensor - 10","ef9c02e9":"# Use the tensorflow function equivalent of the '*' (multiply) operator\ntf.multiply(tensor, 10)","554cad69":"# The original tensor is still unchanged\ntensor","176c3144":"# Matrix multiplication in TensorFlow\nprint(tensor)\ntf.matmul(tensor, tensor)","8247ea69":"# Matrix multiplication with Python operator '@'\ntensor @ tensor","e93a4335":"# Create (3, 2) tensor\nX = tf.constant([[1, 2],\n                 [3, 4],\n                 [5, 6]])\n\n# Create another (3, 2) tensor\nY = tf.constant([[7, 8],\n                 [9, 10],\n                 [11, 12]])\nX, Y","0f0bbbea":"# Try to matrix multiply them (will error)\nX @ Y","875df2ad":"# Example of reshape (3, 2) -> (2, 3)\ntf.reshape(Y, shape=(2, 3))","abdfaf75":"# Try matrix multiplication with reshaped Y\nX @ tf.reshape(Y, shape=(2, 3))","4ac13d43":"# Example of transpose (3, 2) -> (2, 3)\ntf.transpose(X)","26cd6316":"# Try matrix multiplication \ntf.matmul(tf.transpose(X), Y)","3c51ca59":"# You can achieve the same result with parameters\ntf.matmul(a=X, b=Y, transpose_a=True, transpose_b=False)","16cee2de":"# Create a new tensor with default datatype (float32)\nB = tf.constant([1.7, 7.4])\n\n# Create a new tensor with default datatype (int32)\nC = tf.constant([1, 7])\nB, C","5e93fb74":"# Change from float32 to float16 (reduced precision)\nB = tf.cast(B, dtype=tf.float16)\nB","041d37de":"# Change from int32 to float32\nC = tf.cast(C, dtype=tf.float32)\nC","8012699d":"# Create a tensor with 50 random values between 0 and 100\nE = tf.constant(np.random.randint(low=0, high=100, size=50))\nE","acb9052d":"# Find the minimum\ntf.reduce_min(E)","8c362d4f":"# Find the maximum\ntf.reduce_max(E)","d6eb14f0":"# Find the mean\ntf.reduce_mean(E)","c488f15c":"# Find the sum\ntf.reduce_sum(E)","b1d1d9c3":"# Create a tensor with 50 values between 0 and 1\nF = tf.constant(np.random.random(50))\nF","68cf0ae2":"# Find the maximum element position of F\ntf.argmax(F)","46a1587d":"# Find the minimum element position of F\ntf.argmin(F)","e825c5ad":"# Find the maximum element position of F\nprint(f\"The maximum value of F is at position: {tf.argmax(F).numpy()}\") \nprint(f\"The maximum value of F is: {tf.reduce_max(F).numpy()}\") \nprint(f\"Using tf.argmax() to index F, the maximum value of F is: {F[tf.argmax(F)].numpy()}\")\nprint(f\"Are the two max values the same (they should be)? {F[tf.argmax(F)].numpy() == tf.reduce_max(F).numpy()}\")","503d0917":"# Create a rank 5 (5 dimensions) tensor of 50 numbers between 0 and 100\nG = tf.constant(np.random.randint(0, 100, 50), shape=(1, 1, 1, 1, 50))\nG.shape, G.ndim","f6a5f162":"# Squeeze tensor G (remove all 1 dimensions)\nG_squeezed = tf.squeeze(G)\nG_squeezed.shape, G_squeezed.ndim","1f95ddda":"# Create a list of indices\nsome_list = [0, 1, 2, 3]\n\n# One hot encode them\ntf.one_hot(some_list, depth=4)","89b12ba0":"# Specify custom values for on and off encoding\ntf.one_hot(some_list, depth=4, on_value=\"We're live!\", off_value=\"Offline\")","d35fe774":"# Create a new tensor\nH = tf.constant(np.arange(1, 10))\nH","185eede3":"# Square it\ntf.square(H)","eb488fc9":"# Find the squareroot (will error), needs to be non-integer\ntf.sqrt(H)","86c6a2e3":"# Change H to float32\nH = tf.cast(H, dtype=tf.float32)\nH","a61acfc1":"# Find the square root\ntf.sqrt(H)","a7bdac3f":"# Find the log (input also needs to be float)\ntf.math.log(H)","ad94a95d":"# Create a variable tensor\nI = tf.Variable(np.arange(0, 5))\nI","3cba4477":"# Assign the final value a new value of 50\nI.assign([0, 1, 2, 3, 50])","17c4cf10":"# Add 10 to every element in I\nI.assign_add([10, 10, 10, 10, 10])","fb7dfcc9":"# The change happens in place\nI","23837aad":"# Create a tensor from a NumPy array\nJ = tf.constant(np.array([3., 7., 10.]))\nJ","8ee25dd5":"# Convert tensor J to NumPy with np.array()\nnp.array(J), type(np.array(J))","ccd63256":"# Convert tensor J to NumPy with .numpy()\nJ.numpy(), type(J.numpy())","0bae8c5d":"# Create a tensor from NumPy and from an array\nnumpy_J = tf.constant(np.array([3., 7., 10.])) # will be float64 (due to NumPy)\ntensor_J = tf.constant([3., 7., 10.]) # will be float32 (due to being TensorFlow default)\nnumpy_J.dtype, tensor_J.dtype","d4e42df0":"# Squaring, log, square root\n\nMany other common mathematical operations you'd like to perform at some stage, probably exist.\n\nLet's take a look at:\n\n* tf.square() - get the square of every value in a tensor.\n* tf.sqrt() - get the squareroot of every value in a tensor (note: the elements need to be floats or this will error).\n* tf.math.log() - get the natural log of every value in a tensor (elements need to floats).","d64dfaeb":"# Tensors and NumPy\n\nWe've seen some examples of tensors interact with NumPy arrays, such as, using NumPy arrays to create tensors.\n\nTensors can also be converted to NumPy arrays using:\n\n* np.array() - pass a tensor to convert to an ndarray (NumPy's main datatype).\n* tensor.numpy() - call on a tensor to convert to an ndarray.\n\nDoing this is helpful as it makes tensors iterable as well as allows us to use any of NumPy's methods on them.","f84f5262":"# Changing the datatype of a tensor\n\nSometimes you'll want to alter the default datatype of your tensor.\n\nThis is common when you want to compute using less precision (e.g. 16-bit floating point numbers vs. 32-bit floating point numbers).\n\nComputing with less precision is useful on devices with less computing capacity such as mobile devices (because the less bits, the less space the computations require).\n\nYou can change the datatype of a tensor using tf.cast().","9b37e76e":"Now let's try to change one of the elements of the changable tensor.","9eed9cce":"You can also add dimensions to your tensor whilst keeping the same information present using tf.newaxis.","7ef34226":"# Getting information from tensors (shape, rank, size)\n\nThere will be times when you'll want to get different pieces of information from your tensors, in particuluar, you should know the following tensor vocabulary:\n\n**Shape:** The length (number of elements) of each of the dimensions of a tensor.\n\n**Rank:** The number of tensor dimensions. A scalar has rank 0, a vector has rank 1, a matrix is rank 2, a tensor has rank n.\n\n**Axis or Dimension:** The dimension is how many elements are in a particular axis.\n\n**Size:** The total number of items in the tensor.\n\nYou'll use these especially when you're trying to line up the shapes of your data to the shapes of your model.\n\nFor example, making sure the shape of your image tensors are the same shape as your models input layer.\n\nWe've already seen one of these before using the ndim attribute. Let's see the rest.","7ba2fc6c":"Why didn't the numbers come out the same?\n\nIt's due to rule #4 of the tf.random.set_seed() documentation.\n> \"4. If both the global and the operation seed are set: Both seeds are used in conjunction to determine the random sequence.\"","ab1ddf58":"# Matrix mutliplication# \nOne of the most common operations in machine learning algorithms is matrix multiplication.\n\nTensorFlow implements this matrix multiplication functionality in the tf.matmul() method.\n\n![](https:\/\/miro.medium.com\/max\/427\/1*96qrPHcvXBVM01I1lUKS8g.png)\n\n\n\n**For matrix multiplication, the number of columns in the first matrix must be equal to the number of rows in the second matrix. The result matrix has the number of rows of the first and the number of columns of the second matrix.**\n","19c225be":"Both of these examples work because our tensor variable is of shape (2, 2).\n\nWhat if we created some tensors which had mismatched shapes?","5144e3ff":"# Finding the min, max, mean, sum (aggregation)\n\nYou can quickly aggregate (perform a calculation on a whole tensor) tensors to find things like the minimum value, maximum value, mean and sum of all the elements.\n\nTo do so, aggregation methods typically have the syntax reduce()_[action], such as:\n\n* tf.reduce_min() - find the minimum value in a tensor.\n* tf.reduce_max() - find the maximum value in a tensor (helpful for when you want to find the highest prediction probability).\n* tf.reduce_mean() - find the mean of all elements in a tensor.\n* tf.reduce_sum() - find the sum of all elements in a tensor.\n\nNote: typically, each of these is under the math module, e.g. tf.math.reduce_min() but you can use the alias tf.reduce_min().\nLet's see them in action.","8d9d9987":"You can also index tensors just like Python lists.","f8faeefd":"We can do this with either:\n\n* tf.reshape() - allows us to reshape a tensor into a defined shape.\n\n![](https:\/\/numpy.org\/doc\/stable\/_images\/np_reshape.png)\n\n\n* tf.transpose() - switches the dimensions of a given tensor.\n\n![](https:\/\/ajcr.net\/images\/stride-guide\/2d_array_b_example_transpose.png)","312fb827":"By default tensors have dtype=float32, where as NumPy arrays have dtype=float64.\n\nThis is because neural networks (which are usually built with TensorFlow) can generally work very well with less precision (32-bit rather than 64-bit).","cd9f6358":"# Manipulating tf.Variable tensors\n\nTensors created with tf.Variable() can be changed in place using methods such as:\n\n* .assign() - assign a different value to a particular index of a variable tensor.\n* .add_assign() - add to an existing value and reassign it at a particular index of a variable tensor.","2e7f25c7":"Which one should you use? tf.constant() or tf.Variable()?\nIt will depend on what your problem requires. However, most of the time, TensorFlow will automatically choose for you (when loading data or modelling data).\n","42bf00de":"You can also use the equivalent TensorFlow function. Using the TensorFlow function (where possible) has the advantage of being sped up later down the line when running as part of a [TensorFlow graph](https:\/\/www.tensorflow.org\/tensorboard\/graphs).","53ca9113":"So which should you use?\n\nAgain, most of the time these operations (when they need to be run, such as during the training a neural network, will be implemented for you).\n\nBut generally, whenever performing a matrix multiplication and the shapes of two matrices don't line up, you will transpose (not reshape) one of them in order to line them up.","0cd8ba85":"To change an element of a tf.Variable() tensor requires the assign() method.","f7bc60c4":"# DEEP LEARNING FOR BEGINNERS FROM SCRATCH. LESSON 0 \ud83d\udcd9\ud83d\udcd9\ud83d\udcd9\n\n\n## Before we begin\n\n### If you find this notebook useful then please upvote\ud83d\udd25\n\n### You can also check my GitHub profile to find more interesting projects.\n\nIn this notebook, we're going to cover some of the most fundamental concepts of tensors using TensorFlow.\n\nContent:\n\n* Introduction to tensors (creating tensors)\n* Getting information from tensors (tensor attributes)\n* Manipulating tensors (tensor operations)\n* Tensors and NumPy\n\n\nWhat is TensorFlow?\n\nTensorFlow is an open-source end-to-end machine learning library for preprocessing data, modelling data and serving models (getting them into the hands of others).\n\nWhy use TensorFlow?\n\nRather than building machine learning and deep learning models from scratch, it's more likely you'll use a library such as TensorFlow. This is because it contains many of the most common machine learning functions you'll want to use.\n\n\n![](https:\/\/www.viatech.com\/wp-content\/uploads\/2018\/05\/Artificial_Intelligence_781x512.jpg)","b538bf8a":"# Manipulating tensors (tensor operations)\n\nFinding patterns in tensors (numberical representation of data) requires manipulating them.\n\nAgain, when building models in TensorFlow, much of this pattern discovery is done for you.\n\n**Basic operations**\n\nYou can perform many of the basic mathematical operations directly on tensors using Pyhton operators such as, +, -, *.","6973cc17":"Let's try tf.reshape() first.","1901fa7d":"Creating Tensors with tf.constant()\n\nYou usually won't create tensors yourself. This is because TensorFlow has modules built-in (such as tf.io and tf.data) which are able to read your data sources and automatically convert them to tensors and then later on, neural network models will process these for us.\n\nBut for now, because we're getting familar with tensors themselves and how to manipulate them, we'll see how we can create them ourselves.\n\nWe'll begin by using tf.constant().","5ba7be17":"# Introduction to tensors (creating tensors)","cec159ea":"Because, \"Operations that rely on a random seed actually derive it from two seeds: the global and operation-level seeds. This sets the global seed.\"","c5bad6b8":"# One-hot encoding\n\nIf you have a tensor of indicies and would like to one-hot encode it, you can use tf.one_hot().\n\nYou should also specify the depth parameter (the level which you want to one-hot encode to).","cbbc4b2b":"# Other ways to make tensors\n\nThough you might rarely use these (remember, many tensor operations are done behind the scenes for you), you can use tf.ones() to create a tensor of all ones and tf.zeros() to create a tensor of all zeros.","7944149f":"You can also find the standard deviation (tf.reduce_std()) and variance (tf.reduce_variance()) of elements in a tensor using similar methods.","1a72b1ae":"You can also turn NumPy arrays in into tensors.\n\nRemember, the main difference between tensors and NumPy arrays is that tensors can be run on GPUs.\n\n> Note: A matrix or tensor is typically represented by a capital letter (e.g. X or A) where as a vector is typically represented by a lowercase letter (e.g. y or b).","233e1e19":"![](https:\/\/www.tensorflow.org\/guide\/images\/tensor\/3-axis_numpy.png)  ![](https:\/\/www.tensorflow.org\/guide\/images\/tensor\/3-axis_front.png) ![](https:\/\/www.tensorflow.org\/guide\/images\/tensor\/3-axis_block.png)","c32be56e":"The random tensors we've made are actually pseudorandom numbers (they appear as random, but really aren't).\nIf we set a seed we'll get the same random numbers (if you've ever used NumPy, this is similar to np.random.seed(42)).\nSetting the seed says, \"hey, create some random numbers, but flavour them with X\" (X is the seed).\nWhat do you think will happen when we change the seed?","cb23e6dd":"By default, TensorFlow creates tensors with either an int32 or float32 datatype.\n\nThis is known as 32-bit precision (the higher the number, the more precise the number, the more space it takes up on your computer).","57b66e74":"It worked, let's try the same with a reshaped X, except this time we'll use tf.transpose() and tf.matmul().","e48c191c":"Since we used tf.constant(), the original tensor is unchanged (the addition gets done on a copy).","2dc3fc0e":"# Finding the positional maximum and minimum\n\nHow about finding the position a tensor where the maximum value occurs?\n\nThis is helpful when you want to line up your labels (say ['Green', 'Blue', 'Red']) with your prediction probabilities tensor (e.g. [0.98, 0.01, 0.01]).\n\nIn this case, the predicted label (the one with the highest prediction probability) would be 'Green'.\n\nYou can do the same for the minimum (if required) with the following:\n\n* tf.argmax() - find the position of the maximum element in a given tensor.\n* tf.argmin() - find the position of the minimum element in a given tensor.","c2994d35":"We can create random tensors by using the tf.random.Generator class.","d809f302":"A scalar is known as a rank 0 tensor. Because it has no dimensions (it's just a number).","aa5c67d5":"This is known as a rank 3 tensor (3-dimensions), however a tensor can have an arbitrary (unlimited) amount of dimensions.\n\nFor example, you might turn a series of images into tensors with shape (224, 224, 3, 32), where:\n224, 224 (the first 2 dimensions) are the height and width of the images in pixels.\n\n3 is the number of colour channels of the image (red, green blue).\n32 is the batch size (the number of images a neural network sees at any one time).\n\nAll of the above variables we've created are actually tensors. But you may also hear them referred to as their different names (the ones we gave them):\n\nscalar: a single number.\n\nvector: a number with direction (e.g. wind speed with direction).\n\nmatrix: a 2-dimensional array of numbers.\n\ntensor: an n-dimensional arrary of numbers (where n can be any number, a 0-dimension tensor is a scalar, a 1-dimension tensor is a vector).\n\nTo add to the confusion, the terms matrix and tensor are often used interchangably.\n\nGoing forward since we're using TensorFlow, everything we refer to and use will be tensors.","3ddcfb83":"# Creating random tensors\n\nRandom tensors are tensors of some abitrary size which contain random numbers.\n\nWhy would you want to create random tensors?\n\nThis is what neural networks use to intialize their weights (patterns) that they're trying to learn in the data.\n\nFor example, the process of a neural network learning often involves taking a random n-dimensional array of numbers and refining them until they represent some kind of pattern (a compressed way to represent the original data).\n","4ed600ae":"Now let's try to change a value in a tf.constant() tensor.","05bb6768":"# Squeezing a tensor (removing all single dimensions)\n\nIf you need to remove single-dimensions from a tensor (dimensions with size 1), you can use tf.squeeze().\n\ntf.squeeze() - remove all dimensions of 1 from a tensor.\n\n![](https:\/\/static.packt-cdn.com\/products\/9781788834131\/graphics\/Images\/B09475_01_20.jpg)","ee03823a":"# Creating Tensors with tf.Variable()\n\nYou can also (although you likely rarely will, because often, when working with data, tensors are created for you automatically) create tensors using tf.Variable().\n\nThe difference between tf.Variable() and tf.constant() is tensors created with tf.constant() are immutable (can't be changed, can only be used to create a new tensor), where as, tensors created with tf.Variable() are mutable (can be changed).","3b3f6d7f":"This kind of data manipulation is a reminder: you'll spend a lot of your time in machine learning and working with neural networks reshaping data (in the form of tensors) to prepare it to be used with various operations (such as feeding it to a model).","67068148":"You can also specify values for on_value and off_value instead of the default 0 and 1.","192a7804":"Trying to matrix multiply two tensors with the shape (3, 2) errors because the inner dimensions don't match.\n\nWe need to either:\n\n* Reshape X to (2, 3) so it's (2, 3) @ (3, 2).\n\n* Reshape Y to (3, 2) so it's (3, 2) @ (2, 3).","b3d0bf17":"What if you wanted to shuffle the order of a tensor?\n\nWait, why would you want to do that?\n\nLet's say you working with 15,000 images of cats and dogs and the first 10,000 images of were of cats and the next 5,000 were of dogs. This order could **effect how a neural network learns** (it may overfit by learning the order of the data), instead, it might be a good idea to move your data around.\n"}}