{"cell_type":{"619cd450":"code","ee24268f":"code","9ff85ad6":"code","8b092149":"code","13cedd9e":"code","601b0c14":"code","49532e61":"code","3134c89c":"code","75f5efdb":"code","0976b3ef":"code","841123c4":"code","70f86fb5":"code","6a8c7d26":"code","3c1fc529":"code","4b66cca3":"code","4d735dd4":"code","472ffe6c":"code","dae579dc":"code","9d5ec18a":"code","1f7a459a":"code","46e3a32f":"code","26fdd1f7":"code","0e3c77e9":"code","f67505ec":"code","c71e7e48":"code","54d46418":"code","89594596":"code","a6969f4a":"code","07ce5291":"code","8f9d6aac":"code","dfc098c5":"code","91f2288c":"code","5a072e1d":"code","5bd4c07f":"code","7708e268":"code","e9100615":"code","bef0309b":"code","e75ec14b":"code","d335deb3":"code","896b581b":"markdown","69cae1fb":"markdown","d1888df2":"markdown","24255042":"markdown","493bbf42":"markdown","c193178e":"markdown","897e0311":"markdown","272db8a2":"markdown","b4dffeca":"markdown","7f1ac841":"markdown","f81fe037":"markdown","fd778616":"markdown","0f464811":"markdown","460bc242":"markdown","6d4bfbfd":"markdown","e1cbc8da":"markdown","098f5045":"markdown","735f4689":"markdown","f26dc66e":"markdown","f87c7a4b":"markdown","d3a52217":"markdown"},"source":{"619cd450":"# Basic imports\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport string\nimport re\n\n# NLP imports(nltk)\nimport nltk\n#nltk.download('punkt')\nfrom nltk.tokenize import TreebankWordTokenizer\nfrom nltk.stem import WordNetLemmatizer\n#nltk.download('wordnet')\nfrom nltk.util import ngrams\nfrom nltk.corpus import stopwords\n\n# to automate the NLP extraction...\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.pipeline import make_pipeline\n\n# Cross_val_score is the new class for today...\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\n# different regression models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n# additional models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Neural Network!!\nfrom sklearn.neural_network import MLPClassifier\n\n# machine learning basics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n#####\n\nimport nltk\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n%matplotlib inline\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nimport string\nimport re\nimport scipy.stats as stats\n\n##data preprocessing\n#nltk.download('punkt')\nfrom nltk.tokenize import word_tokenize, TreebankWordTokenizer\n\nfrom nltk.corpus import stopwords\n\nfrom nltk import SnowballStemmer, PorterStemmer, LancasterStemmer\n\n#nltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\n\n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\nfrom textblob import TextBlob\n\n# to automate the NLP extraction and create bag of words\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#cross validation\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn import metrics\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\n\n# different regression models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n# additional models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Neural Network!!\nfrom sklearn.neural_network import MLPClassifier\n\n#select model\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nfrom collections import Counter\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.multiclass import OneVsRestClassifier","ee24268f":"train_data = pd.read_csv('..\/input\/train.csv')","9ff85ad6":"train_data.info()","8b092149":"train_data","13cedd9e":"train_data['president'] = train_data['president'].replace('deKlerk',0)\ntrain_data['president'] = train_data['president'].replace('Mandela',1)\ntrain_data['president'] = train_data['president'].replace('Mbeki',2)\ntrain_data['president'] = train_data['president'].replace('Motlanthe',3)\ntrain_data['president'] = train_data['president'].replace('Zuma',4)\ntrain_data['president'] = train_data['president'].replace('Ramaphosa',5)","601b0c14":"faxis = train_data.copy()\nfaxis.head()","49532e61":"#Bar graph that shows how many post for each personality type there is \n\ncount_person = faxis.groupby('president').count().sort_values('text', ascending=False)\n\nf, ax = plt.subplots(figsize=(10, 7))\ncount_person['text'].plot(kind='bar', edgecolor='black',\n                           linewidth=1.5, width=0.6, color = 'c')\nax.set_xlabel('President')\nax.set_ylabel('Number of posts')\nax.set_title('Number of posts per President');\nplt.show()","3134c89c":"train_donut = []\nfor i, row in faxis.iterrows():\n    for text in row['text'].split('.'):\n        train_donut.append([row['president'], text])\ntrain_donut = pd.DataFrame(train_donut, columns=['president', 'text'])\n\ntrain_donut","75f5efdb":"train_donut['text'] = train_donut['text'].str.lower()\ntrain_donut.head()","0976b3ef":"def remove_punctuation(text):\n    punc = ''.join([l for l in text if l not in string.punctuation])\n    return punc\n\ntrain_donut['text'] = train_donut['text'].apply(remove_punctuation)","841123c4":"train_donut['text'] = train_donut['text'].apply(lambda x: x.replace('\u201d',''))\ntrain_donut['text'] = train_donut['text'].apply(lambda x: x.replace('\u201c',''))\ntrain_donut['text'] = train_donut['text'].apply(lambda x: x.replace('\u2018',''))\ntrain_donut['text'] = train_donut['text'].apply(lambda x: x.replace('\u00ea','e'))\ntrain_donut['text'] = train_donut['text'].apply(lambda x: x.replace('\ufffd',''))","70f86fb5":"train_donut","6a8c7d26":"train_donut['text'].replace(' ', np.nan, inplace=True)\ntrain_donut['text'].replace('', np.nan, inplace=True)\ntrain_donut.dropna(subset=['text'], inplace=True)\ntrain_donut\n","3c1fc529":"tokeniser = TreebankWordTokenizer()\ntrain_donut['tokens'] = train_donut['text'].apply(tokeniser.tokenize)\ntrain_donut.head()","4b66cca3":"lemmatizer = WordNetLemmatizer()","4d735dd4":"def donut_lemma(words, lemmatizer):\n    lemma = [lemmatizer.lemmatize(word) for word in words]\n    return lemma","472ffe6c":"#apply above function to the mbti data\ntrain_donut['lemma'] = train_donut['text'].apply(donut_lemma, args=(lemmatizer,  ))\ntrain_donut.head(12)","dae579dc":"#All presidents\nfrom wordcloud import WordCloud\ncloud = WordCloud(width=1440, height=1080, max_words = 100).generate(' '.join(train_donut['text']))\nplt.figure(figsize=(20, 15))\nplt.imshow(cloud)\nplt.axis('off')","9d5ec18a":"#DeKlerk\n\ndeklerk = train_donut.loc[train_donut['president'] ==0]\ncloud = WordCloud(width=1440, height=1080,max_words=100).generate(' '.join(deklerk['text']))\nplt.figure(figsize=(20, 15))\nplt.imshow(cloud)\nplt.axis('off')","1f7a459a":"#Mandela\n\nmandela = train_donut.loc[train_donut['president'] ==1]\ncloud = WordCloud(width=1440, height=1080,max_words=100).generate(' '.join(mandela['text']))\nplt.figure(figsize=(20, 15))\nplt.imshow(cloud)\nplt.axis('off')","46e3a32f":"#Mbeki\n\nmbeki = train_donut.loc[train_donut['president'] ==2]\ncloud = WordCloud(width=1440, height=1080,max_words=100).generate(' '.join(mbeki['text']))\nplt.figure(figsize=(20, 15))\nplt.imshow(cloud)\nplt.axis('off')","26fdd1f7":"#Mothlanthe\n\nmothlanthe = train_donut.loc[train_donut['president'] ==3]\ncloud = WordCloud(width=1440, height=1080,max_words=100).generate(' '.join(mothlanthe['text']))\nplt.figure(figsize=(20, 15))\nplt.imshow(cloud)\nplt.axis('off')","0e3c77e9":"#Zuma\n\nzuma = train_donut.loc[train_donut['president'] ==4]\ncloud = WordCloud(width=1440, height=1080,max_words=100).generate(' '.join(zuma['text']))\nplt.figure(figsize=(20, 15))\nplt.imshow(cloud)\nplt.axis('off')","f67505ec":"#Ramaphose\n\nramaphose = train_donut.loc[train_donut['president'] ==5]\ncloud = WordCloud(width=1440, height=1080,max_words=100).generate(' '.join(ramaphose['text']))\nplt.figure(figsize=(20, 15))\nplt.imshow(cloud)\nplt.axis('off')","c71e7e48":"import nltk\nnltk.download('stopwords')\n\nstop = stopwords.words('english')\n\nvect = CountVectorizer(stop_words=stop,  \n                             min_df=2, \n                             max_df=0.5, \n                             ngram_range=(1, 2))\n\n\n\nvect.fit(train_donut['text'])","54d46418":"X = vect.fit_transform(train_donut['text'])\nX = X.toarray()\nY = train_donut['president']","89594596":"# Train-test split, using type variable as target and posts variable as predictor\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.001, random_state=42)\nprint('X_train', X_train.shape)\nprint('X_test', X_test.shape)\nprint('y_train', y_train.shape)\nprint('y_test', y_test.shape)","a6969f4a":"# Fit and score a LR\nlrc = LogisticRegression(random_state = 0)\nlrc.fit(X_train, y_train)\nprint(\"TRAINING SET\")\nprint(\"Accuracy: \", lrc.score(X_train, y_train))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_train, lrc.predict(X_train)))\nprint(\"Logistic Regression Classification Report:\")\nprint(classification_report(y_train, lrc.predict(X_train)))\nprint(\"\")\n\n\nprint(\"TEST SET\")\nprint(\"Accuracy: \", lrc.score(X_test, y_test))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, lrc.predict(X_test)))\nprint(\"Logistic Regression Classification Report:\")\nprint(classification_report(y_test, lrc.predict(X_test)))\n","07ce5291":"## read the test data\npres_speech = pd.read_csv('..\/input\/test.csv')","8f9d6aac":"#Make text Lower\npres_speech['text'] = pres_speech['text'].str.lower()\n\n\n#Remove punctuation\ndef remove_punctuation(text):\n    punc = ''.join([l for l in text if l not in string.punctuation])\n    return punc\n\npres_speech['text'] = pres_speech['text'].apply(remove_punctuation)\n\n\n#Remove special Characters\npres_speech['text'] = pres_speech['text'].apply(lambda x: x.replace('\u201d',''))\npres_speech['text'] = pres_speech['text'].apply(lambda x: x.replace('\u201c',''))\npres_speech['text'] = pres_speech['text'].apply(lambda x: x.replace('\u2018',''))\npres_speech['text'] = pres_speech['text'].apply(lambda x: x.replace('\u00ea','e'))\npres_speech['text'] = pres_speech['text'].apply(lambda x: x.replace('\ufffd',''))\n\n\n#Delete Blanks\npres_speech['text'].replace(' ', np.nan, inplace=True)\npres_speech['text'].replace('', np.nan, inplace=True)\npres_speech.dropna(subset=['text'], inplace=True)\n\npres_speech","dfc098c5":"X_Test = vect.transform(pres_speech['text'])\nX_Test = X_Test.toarray()\nY = pres_speech['sentence']","91f2288c":"Xt_Train = vect.transform(train_donut['text'])\nXt_Train = Xt_Train.toarray()\nY_train = train_donut['president']","5a072e1d":"Pred_Train = lrc.predict(Xt_Train)","5bd4c07f":"Train_Submit = pd.DataFrame(\n{\n    'President': Y_train,\n    'Pred_president': Pred_Train,\n})\n\n\nTrain_Submit.head(10)","7708e268":"Pred_Pres = lrc.predict(X_Test)","e9100615":"Test_Submit = pd.DataFrame(\n{\n    'sentence': Y,\n    'president': Pred_Pres,\n})\n","bef0309b":"Test_Submit = Test_Submit.reset_index(drop=True)","e75ec14b":"Test_Submit","d335deb3":"Test_Submit.to_csv('test_data_submission_donut_squad.csv',index=False)","896b581b":"### Make text Lowercase","69cae1fb":"### Remove punctuation and Digits","d1888df2":"## Predict test data labels from winning model","24255042":"Tokenising is to use a tokenizer to divied the data(text) into a sequence of tokens(words). This makes it easier to analyse.","493bbf42":"### Inspecting of Data","c193178e":"## Train Donut Start","897e0311":"## Predict test data labels from winning model","272db8a2":"## An Intersting Graph","b4dffeca":"## Import test data","7f1ac841":"### CountVectorizer","f81fe037":"## Classification","fd778616":"## Natural Language Processing","0f464811":"Lemmatizing is the process of grouping words of similar meaning together. So, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma.\n\nSometimes you will wind up with a very similar word, but other times you will wind up with a completely different word.","460bc242":"# Delete blank","6d4bfbfd":"This function converts a collection of text documents to a matrix of token counts.\n\nThere are different things the CountVectorizer does, such as removing stop words. These things are the parameters and belows is a list and describtion of the parameters used.\n\n\n|Parameter that are used | Decsription|\n|:--------------- | :-------------|\n|stop_words  |Removes all the English Stop words|\n|min_df  |Ignore terms that have a document frequency strictly lower than the given threshold. (This value is also called \"cut-off\" in the literature.) |\n|max_df  |Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words) |\n|ngram_range  | The lower and upper boundary of the range of n-values for different n-grams to be extracted|\n","e1cbc8da":"### Modeling: LR","098f5045":"### Lemmatization","735f4689":"### Importing of Data","f26dc66e":"# Modeling","f87c7a4b":"## Import libraries","d3a52217":"## Import, inspect and clean training data"}}