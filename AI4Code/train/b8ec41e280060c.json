{"cell_type":{"da6adf09":"code","d459613b":"code","e7e31344":"code","5195d520":"code","70b779e6":"code","0c5ecae3":"code","e045e22a":"code","8d67985b":"code","f79f9fc6":"code","d1e6156d":"code","78a4f7ac":"code","76688310":"code","d88c94d9":"code","6dc11c91":"code","21c462d5":"code","738ce8a5":"markdown","8f0450d9":"markdown","6d3b4251":"markdown","f763ecf0":"markdown","7a7c570c":"markdown","9c29fbf9":"markdown","22c89586":"markdown","bfd759c9":"markdown","cd304295":"markdown","ca45f4dd":"markdown","b3d5bea3":"markdown","389261d5":"markdown"},"source":{"da6adf09":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport os\n\napp_store_dataset = pd.read_csv('..\/input\/AppleStore.csv').set_index('id')\napp_store_description_dataset = pd.read_csv('..\/input\/appleStore_description.csv').set_index('id')\n","d459613b":"app_store_dataset.columns.values","e7e31344":"# Feature engineer:\ndf = app_store_dataset\ndf_description = app_store_description_dataset\n\n# Change 1,0 columns into boolean\ndf['vpp_lic'] = df['vpp_lic'].astype('bool') \n\ndf['cont_rating'].unique()\n#array(['4+', '12+', '17+', '9+'], dtype=object)\n\n# Remove the '+' from the rating and change dtype to int\ndf['cont_rating'] = (df['cont_rating']\n                    .apply   (lambda x: x.replace('+',''))\n                    .astype  (int))\n\ndf['currency'].unique()\n# array(['USD'], dtype=object) \n\n# Currency is the same params to all table - remove this column\ndf.drop('currency', axis = 1, inplace = True)\n\n# Crate 3 columns with Major version, Minor verion, Patch version\ndf['ver'].isnull().sum()\n# 0 - no null in the columns\n\n# Remove unNeeded string (v3.4 , version3.4 , V3.V4) \ndf['ver'] = df['ver'].apply(lambda x: re.findall(\"([\\d.]*\\d+)\", x)[0])\ndf['major_version']  = df['ver'].str.split('.').str[0]\ndf['minor_version']  = df['ver'].str.split('.').str[1]\ndf['patch_version']  = df['ver'].str.split('.').str[2]\n\n# Remove null (version without minor or patch)\ndf['major_version'].fillna(value = 0, inplace = True)\ndf['minor_version'].fillna(value = 0, inplace = True)\ndf['patch_version'].fillna(value = 0, inplace = True)\n\n# Change all type into int\ndf['major_version'] = df['major_version'].astype(int)\ndf['minor_version'] = df['minor_version'].astype(int)\ndf['patch_version'] = df['patch_version'].astype(int)\n# remove the version column (all data in the new culoms)\ndf.drop('ver',axis = 1,inplace = True)\n\n# add descrition to the main table\ndf = pd.concat([df, df_description['app_desc']], axis=1)\n\n# add new column with is app is free (price == 0)\ndf['free_app'] = df['price'] == 0 \n\n# add new column wich is app is paid\ndf['paid_app'] = df['free_app']== 0\n\ndf.head(10)","5195d520":"#most rating apps\ndf.sort_values('rating_count_tot', ascending = False)['track_name'].head()","70b779e6":"plt.figure(figsize=(9,5))\n#df=app.iloc[:,[3,5,6,7,8,9]]\nsns.heatmap(df.corr(),linewidths=.5,cmap=\"YlGnBu\")\nplt.show()","0c5ecae3":"# Find price Exceeding observations:\ndf.plot.scatter('rating_count_tot' , 'price')\nplt.show()\n\ndf_numerics = df.select_dtypes(exclude=['object'])\ncols = list(df_numerics.columns)\ndf_copy = df.copy()\n\ndf_copy['price_zscore'] =  (df_copy['price'] - df['price'].mean())\/df_copy['price'].std(ddof=0)\nprint(\"prices zscore\")\nprint(df_copy['price_zscore'].nlargest(5))\n\n# id\n# 551215116    51.137358\n# 308368164    42.564853\n# 849732663    16.847336\n# 320279293    12.561083\n# 491998279     9.989332\n# Name: price_zscore, dtype: float64\n\n# We have 2 row with extremely deviation after the standard deviation \n# remove the out liars \ndf = df.drop(df[df['price'] > 200].index)","e045e22a":"#How much of which genre in appStore?\ndf[\"prime_genre\"] = df['prime_genre'].astype('category')\ndf[\"prime_genre\"].value_counts(ascending = True).plot.barh(figsize=(10, 5), title = \"Genre Count\")\nplt.xlabel('Count')\nplt.show()\n\n# What is the mean price for each genre\ndf.groupby('prime_genre')['price'].mean().sort_values().plot.barh(figsize=(10, 5), title = \"Mean price Vs Genre\")\nplt.xlabel('Mean Price')\nplt.show()\n","8d67985b":"# Free app Vs Paid app\n\nfree_app_mean = df.groupby('free_app').mean()\n\n# rating count free vs paid\nfree_app_mean['rating_count_tot'].plot.bar(title='rating count free vs paid')\nplt.xlabel('Free app')\nplt.ylabel('rating countr')\nplt.show()\n# paid much more rating.\n\n# size bytes free vs paid\nfree_app_mean['size_bytes'].plot.bar(title='size bytes free vs paid')\nplt.xlabel('Free app')\nplt.ylabel('size bytes')\nplt.show()\n# free app as more bytes size (bigger apps)\n\n\n#compering for each genre the paid and unpaid apps\ndf.groupby('prime_genre')[['free_app','paid_app']] \\\n    .sum().plot(kind='bar', rot=100, figsize=(15, 5), grid =True)\nplt.show()\n","f79f9fc6":"#how many apps are rated with each rating?\ndf.groupby('user_rating').size().plot.bar()\nplt.show()","d1e6156d":"# the connection between the rating of the app and the size of app\nrating_v_size=app_store_dataset.groupby('size_bytes')['user_rating'].mean().hist(bins=10,color='purple')\nrating_v_size.set_title('connection between rating and size')\nrating_v_size.set_xlabel('user_rating')\nrating_v_size.set_ylabel('size_bytes')","78a4f7ac":"import nltk\nfrom nltk.corpus import stopwords\n\n# Word analyiest\n\nstop_words = stopwords.words('english')\nstop_words.extend(['the','for','and','of','a','by'])\npat = r'[{}[]^`\u00b4-_\u00b7@|\u00bf?\u00a1!\\\"?.!\/;:<>\u2019\u2022\u201c\u201d\u2013\u00bb%\u25a0]'.format('|'.join(stop_words))\n\ndef cleanString(series):\n    series = (pd.Series    (' '.join(series).lower().split())\n             .apply       (lambda x: nltk.word_tokenize(x))\n             .apply       (lambda x: ''.join(filter(str.isalpha, x)))\n             )\n    return (series[~series.isin(stop_words)]\n            .value_counts()\n            .iloc[1:]\n           )\n\nmostRepeatedWordsTitle = cleanString(df['track_name'])[:15]\nprint(mostRepeatedWordsTitle)\nmostRepeatedWordsTitle.plot.bar(title = \"Most repeated words Title\", rot=100,figsize=(10, 5))\nplt.show()\n\n\nmust_used_word = mostRepeatedWordsTitle[:10].index\nrating_vs_words = pd.Series()\nprice_vs_words = pd.Series()\n\nrating_vs_words['rating mean'] = df['rating_count_ver'].mean()\nprice_vs_words['price mean'] = df['price'].mean()\n\nfor word in must_used_word:\n    filter_by_word_df = df[df['track_name'].str.lower().str.contains(word)]\n    rating_vs_words[word] = filter_by_word_df['rating_count_ver'].mean()\n    price_vs_words[word] = filter_by_word_df['price'].mean()\n\nfig, axes = plt.subplots(nrows=2, ncols=1)\nfig.tight_layout()\n\nrating_vs_words.plot.bar(title = \"Words Vs Rating\",\n                         ax=axes[0], \n                         rot=100,figsize=(10, 10))\naxes[0].axhline(rating_vs_words['rating mean'].max(), color=\"gray\")\n\nprice_vs_words.plot.bar(title = \"Words Vs Price\",\n                        ax=axes[1],\n                        rot=100,figsize=(10, 10))\naxes[1].axhline(price_vs_words['price mean'].max(), color=\"gray\")\n\n\nplt.show()","76688310":"# Check correlation between common word and user rating , price , free app\n\ndf_copy = df.copy()\ncolums_names = []\nfor commonWord in mostRepeatedWordsTitle[:5].index:\n    colum = commonWord+\"_word\"\n    df_copy[colum] = df['track_name'].str.lower().str.contains(commonWord)\n    colums_names.append(colum)\n\ndf_copy.groupby('user_rating')[colums_names].sum().plot(kind='bar',\n                                                        rot=100,\n                                                        figsize=(20, 10),\n                                                        grid =True,\n                                                        title = \"User Vs key word\")\nplt.legend(prop={'size': 20})\nplt.show()\n\ndf_copy.groupby('price')[colums_names].sum().plot(kind='bar',\n                                                  rot=100,\n                                                  figsize=(20, 10),\n                                                  grid =True,\n                                                  title = \"Price Vs key word\")\nplt.legend(prop={'size': 20})\nplt.xlim(-1,10)\nplt.show()\n\ndf_copy.groupby('free_app')[colums_names].sum().plot(kind='bar',\n                                                        rot=100,\n                                                        figsize=(10, 5),\n                                                        grid =True,\n                                                        title = \"Free app Vs key word\")\nplt.legend(prop={'size': 10})\nplt.show()\n","d88c94d9":"# age investigation\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n# precentage of each age from the total\npercantage_age=df.groupby('cont_rating')['cont_rating'].apply (lambda x:x.count()\/len(df)).plot.pie(autopct='%.2f%%',counterclock=False, shadow=True)\nplt.title('percentage of app ages')\nplt.show()","6dc11c91":"# how mush support languages in any version (show only major)\nmajor_lang = df.groupby('major_version').agg([np.mean,np.size])['lang.num']\nmajor_lang = major_lang[major_lang['size'] >= 10]\nmajor_lang['mean'].plot.bar(color = 'blue' ,alpha=0.5, title = \"Major ver count Vs support languages\")\nmajor_lang['mean'].plot(color = 'red',figsize=(10, 5))\nplt.ylabel('Mean support languages')\nplt.show()\n\n# how mush thr rating total in any version (show only major)\nmajor_rating_count_tot = df.groupby('major_version').agg([np.mean,np.size])['rating_count_tot']\nmajor_rating_count_tot = major_rating_count_tot[major_rating_count_tot['size'] >= 10]\nmajor_rating_count_tot['mean'].plot.bar(color = 'blue' ,alpha=0.5, title = \"Major ver count Vs rating count\")\nmajor_rating_count_tot['mean'].plot(color = 'red',figsize=(10, 5))\nplt.ylabel('Mean rating count')\nplt.show()","21c462d5":"# screens \n\nfrom scipy import stats \n\ndf.groupby('ipadSc_urls.num').mean()['rating_count_tot'].plot.bar(title = \"Rating vs Snapshot screens\")\nplt.ylabel('Mean rating')\nplt.xlabel('Number of snapshot screens')\nplt.show()\n\nipadSc_rating_count = df.groupby('ipadSc_urls.num').count()['rating_count_tot']\nipadSc_rating_std = df.groupby('ipadSc_urls.num').std()['rating_count_tot']\nipadSc_rating_mean = df.groupby('ipadSc_urls.num').mean()['rating_count_tot']\n\nprint('mean rating count by screen count')\nprint(df.groupby('ipadSc_urls.num').mean()['rating_count_tot'])\nprint()\nprint('std rating count by screen count')\nprint(df.groupby('ipadSc_urls.num').std()['rating_count_tot'])\nprint()\nprint('count rating count by screen count')\nprint(df.groupby('ipadSc_urls.num').count()['rating_count_tot'])\nf_val, p_val = stats.f_oneway(ipadSc_rating_count , ipadSc_rating_mean,ipadSc_rating_std)  \nprint()\nprint (\"One-way ANOVA P =\", p_val)\nprint(\"If P < 5% (0.05) we can claim with high confidence that the means of the results of all three experiments are very significantly different\")","738ce8a5":"We can see word that indicative of:\n- price - hd, full, pro, hidden\n- free app - free\n- rating - free , app","8f0450d9":"## Free Vs paid","6d3b4251":"## App title ","f763ecf0":"## Cleaning the data:\n    \n   - Change cont_rating to int from string ('4+', '12+', '17+', '9+') remove \"+\"\n   -  Make vpp_lic as bool\n   -  Remove currency from df , only 1 unique ('USD')\n   -  Change ver in new 3 columns (v1.3.4 = 1, 3, 4)\n   -  Add Free app and Paid app columns ","7a7c570c":"## Columns ","9c29fbf9":"The conclusion of snapshot screens cannot be relied upon the 'Rating vs Snapshot screens' table","22c89586":"## App ages","bfd759c9":"## Major version","cd304295":"We that we have 2 exceptions data (250 and 300), and the Zscore is 51.1 and 42.5 (very big).\nSo we can remove the outlier data","ca45f4dd":"## Snapshot screens (in store)","b3d5bea3":"Game genre has the must app. (Huge ratio )","389261d5":"### App with most rating "}}