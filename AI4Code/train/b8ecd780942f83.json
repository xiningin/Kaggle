{"cell_type":{"39aa0582":"code","59f2f2be":"code","b0f05fbd":"code","256df88f":"code","c3656135":"code","13b7faa8":"code","38a214c6":"code","2156ae94":"code","694aa745":"code","fff5e6a3":"code","3ab2a1e8":"code","73b3f18f":"code","19873b50":"code","8a826080":"code","bf9b6c99":"code","9f4c8eb2":"code","8663c4f5":"code","c2eec3d3":"code","d3bc3a09":"code","fef4d1f8":"code","07e1792d":"code","19f83234":"code","ef67bfac":"code","85e40ca8":"code","9dd715a9":"code","7385d0c4":"code","7e9a516b":"code","589e0caf":"code","af2e2b04":"code","1f74b98b":"code","0c70b67d":"code","1a8b2b55":"code","0bc0db91":"code","308fc910":"code","112ce546":"code","606da3bd":"code","309053d0":"code","499cb06a":"code","1683a170":"code","aaebe723":"code","c7818340":"code","4a171157":"code","0cc5e55b":"code","f888ddd3":"code","a7b0bf30":"code","76194243":"code","69a31f80":"code","8b0cd461":"code","f02b86ce":"code","876054c5":"code","c1a7a532":"code","363c8f2e":"code","f7b756ce":"markdown","effe330b":"markdown","5c8a8cea":"markdown","4db9bf16":"markdown","bb2cb9e4":"markdown","14e2673b":"markdown","4fb820b7":"markdown","1def7900":"markdown","e770ac6b":"markdown","634d9499":"markdown","fa393e6c":"markdown","663f1208":"markdown","ea06dd1a":"markdown","d1e4dd88":"markdown","918a075a":"markdown","a065e6d9":"markdown","78365967":"markdown","51cfa7d7":"markdown","246abb1b":"markdown","a580372e":"markdown","92a3e830":"markdown","d6d92b3a":"markdown"},"source":{"39aa0582":"# System\nimport os\n\n# Time\nimport time\nimport datetime\n\n# Numerical\nimport numpy as np\nimport pandas as pd\n\n# Tools\nimport itertools\nfrom collections import Counter\n\n# NLP\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n# from pywsd.utils import lemmatize_sentence\n\n# Preprocessing\nfrom sklearn import preprocessing\nfrom sklearn.utils import class_weight as cw\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\n# Model Selection\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\n# Machine Learning Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier, ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Evaluation Metrics\nfrom sklearn import metrics \nfrom sklearn.metrics import f1_score, accuracy_score,confusion_matrix,classification_report\n\n# Deep Learing Preprocessing - Keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.utils import to_categorical\n\n# Deep Learning Model - Keras\nfrom keras.models import Model\nfrom keras.models import Sequential\n\n# Deep Learning Model - Keras - CNN\nfrom keras.layers import Conv1D, Conv2D, Convolution1D, MaxPooling1D, SeparableConv1D, SpatialDropout1D, \\\n    GlobalAvgPool1D, GlobalMaxPool1D, GlobalMaxPooling1D \nfrom keras.layers.pooling import _GlobalPooling1D\nfrom keras.layers import MaxPooling2D, GlobalMaxPooling2D, GlobalAveragePooling2D\n\n# Deep Learning Model - Keras - RNN\nfrom keras.layers import Embedding, LSTM, Bidirectional\n\n# Deep Learning Model - Keras - General\nfrom keras.layers import Input, Add, concatenate, Dense, Activation, BatchNormalization, Dropout, Flatten\nfrom keras.layers import LeakyReLU, PReLU, Lambda, Multiply\n\n\n\n# Deep Learning Parameters - Keras\nfrom keras.optimizers import RMSprop, Adam\n\n# Deep Learning Callbacs - Keras\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nprint(os.listdir(\"..\/input\"))","59f2f2be":"# print date and time for given type of representation\ndef date_time(x):\n    if x==1:\n        return 'Timestamp: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now())\n    if x==2:    \n        return 'Timestamp: {:%Y-%b-%d %H:%M:%S}'.format(datetime.datetime.now())\n    if x==3:  \n        return 'Date now: %s' % datetime.datetime.now()\n    if x==4:  \n        return 'Date today: %s' % datetime.date.today() ","b0f05fbd":"input_directory = r\"..\/input\/\"\noutput_directory = r\"..\/output\/\"\n\nif not os.path.exists(output_directory):\n    os.mkdir(output_directory)\n    \nfigure_directory = \"..\/output\/figures\"\nif not os.path.exists(figure_directory):\n    os.mkdir(figure_directory)\n    \n    \nfile_name_pred_batch = figure_directory+r\"\/result\"\nfile_name_pred_sample = figure_directory+r\"\/sample\"","256df88f":"df = pd.read_csv(input_directory + \"spam.csv\", encoding='latin-1')\ndf.head()","c3656135":"df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1, inplace=True)\ndf = df.rename(columns={\"v1\":\"label\", \"v2\":\"text\"})\ndf.head()","13b7faa8":"df_new = df.copy()\ndf_stat = df.copy()","38a214c6":"lmm = WordNetLemmatizer()\nporter_stemmer = PorterStemmer()\nsnowball_stemmer = SnowballStemmer('english')\n\nstop_words = set(stopwords.words('english'))","2156ae94":"# df_new['parsed'] = df_new['text'].apply(lambda x: x.lower())\n# df_new['parsed'] = df_new['parsed'].apply(lambda x: word_tokenize(x))\n\n# df_new['no_stop'] = df_new['parsed'].apply(lambda x: [word for word in str(x).split() if word not in stop_words])\n\n# df_new['stem'] = df_new['no_stop'].apply(lambda x: [snowball_stemmer.stem(word) for word in x])\n# df_new['stem'] =  df_new['stem'].apply(lambda x: \" \".join(x))\n\n# df_new['lemi'] =  df_new['no_stop'].apply(lambda x: \" \".join(x))\n# df_new['lemi'] =  df_new['lemi'].apply(lambda x: lmm.lemmatize(x))\n\n# df_new['parsed'] = df_new['parsed'].apply(lambda x: ' '.join(x))\n# df_new['no_stop'] = df_new['no_stop'].apply(lambda x: ' '.join(x))\n# df_new['stem'] = df_new['stem'].apply(lambda x: ' '.join(x))\n# df_new['lemi'] = df_new['lemi'].apply(lambda x: ' '.join(x))\n\n# df_new.head()","694aa745":"df_stat[\"text_clean\"] = df_stat[\"text\"].apply(lambda x: re.sub(\"[^a-zA-Z]\", \" \", x.lower()))\n\ndf_stat[\"length\"] = df_stat[\"text\"].apply(lambda x: len(x))\ndf_stat[\"token_count\"] = df_stat[\"text\"].apply(lambda x: len(x.split(\" \")))\ndf_stat[\"unique_token_count\"] = df_stat[\"text\"].apply(lambda x: len(set(x.lower().split(\" \"))))\ndf_stat[\"unique_token_count_percent\"] = df_stat[\"unique_token_count\"]\/df_stat[\"token_count\"]\n\ndf_stat[\"length_clean\"] = df_stat[\"text_clean\"].apply(lambda x: len(x))\ndf_stat[\"token_count_clean\"] = df_stat[\"text_clean\"].apply(lambda x: len(x.split(\" \")))\n\ndf_stat.head()","fff5e6a3":"sns.set_style(\"ticks\")\nfigsize=(20, 5)\n\nticksize = 18\ntitlesize = ticksize + 8\nlabelsize = ticksize + 5\n\nxlabel = \"Label\"\nylabel = \"Count\"\n\ntitle = \"Number of ham and spam messages\"\n\n\nparams = {'figure.figsize' : figsize,\n          'axes.labelsize' : labelsize,\n          'axes.titlesize' : titlesize,\n          'xtick.labelsize': ticksize,\n          'ytick.labelsize': ticksize}\n\nplt.rcParams.update(params)\n\ncol1 = \"label\"\ncol2 = \"label\"\nsns.countplot(x=df[col1])\nplt.title(title.title())\nplt.xlabel(xlabel)\nplt.ylabel(ylabel)\nplt.xticks(rotation=90)\nplt.plot()\n\ndf.label.value_counts()","3ab2a1e8":"s1 = df_stat[df_stat['label'] == 'ham']['text'].str.len()\ns2 = df_stat[df_stat['label'] == 'spam']['text'].str.len()\ns3 = df_stat[df_stat['label'] == 'ham']['text_clean'].str.len()\ns4 = df_stat[df_stat['label'] == 'spam']['text_clean'].str.len()\ns5 = df_stat[df_stat['label'] == 'ham']['text'].str.split().str.len()\ns6 = df_stat[df_stat['label'] == 'spam']['text'].str.split().str.len()\ns7 = df_stat[df_stat['label'] == 'ham']['text_clean'].str.split().str.len()\ns8 = df_stat[df_stat['label'] == 'spam']['text_clean'].str.split().str.len()\n\nsns.set()\nsns.set_style(\"ticks\")\n\nfigsize=(20, 15)\n\nticksize = 14\ntitlesize = ticksize + 8\nlabelsize = ticksize + 5\n\nxlabel = \"Length\"\nylabel = \"Count\"\n\ntitle1 = \"Length Distribution\"\ntitle2 = \"Length Distribution (Clean)\"\ntitle3 = \"Word Count Distribution\"\ntitle4 = \"Word Count Distribution (Clean)\"\n\n\n\nparams = {'figure.figsize' : figsize,\n          'axes.labelsize' : labelsize,\n          'axes.titlesize' : titlesize,\n          'xtick.labelsize': ticksize,\n          'ytick.labelsize': ticksize}\n\nplt.rcParams.update(params)\n# fig.subplots_adjust(hspace=0.5, wspace=0.5)\n\ncol1 = \"len\"\ncol2 = \"label\"\nplt.subplot(221)\nsns.distplot(s1, label='Ham')\nsns.distplot(s2, label='Spam')\nplt.title(title1.title())\nplt.xlabel(xlabel)\nplt.ylabel(ylabel)\nplt.legend()\n\nplt.subplot(222)\nsns.distplot(s3, label='Ham (Clean)')\nsns.distplot(s4, label='Spam (Clean)')\nplt.title(title2.title())\nplt.xlabel(xlabel)\nplt.ylabel(ylabel)\nplt.legend()\n\nplt.subplot(223)\nsns.distplot(s5, label='Ham Word')\nsns.distplot(s6, label='Spam Word')\nplt.title(title3.title())\nplt.xlabel(xlabel)\nplt.ylabel(ylabel)\nplt.legend()\n\nplt.subplot(224)\nsns.distplot(s7, label='Ham Word')\nsns.distplot(s8, label='Spam Word')\nplt.title(title4.title())\nplt.xlabel(xlabel)\nplt.ylabel(ylabel)\nplt.legend()\nplt.show()","73b3f18f":"X_train,X_test,y_train,y_test = train_test_split(df[\"text\"],df[\"label\"], test_size = 0.2, random_state = 10)","19873b50":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","8a826080":"vect = CountVectorizer()\nX_train_df = vect.fit_transform(X_train)\nX_test_df = vect.transform(X_test)","bf9b6c99":"print(vect.get_feature_names()[0:20])\nprint(vect.get_feature_names()[-20:])","9f4c8eb2":"models = {\n    \"SVC\": svm.SVC(kernel=\"linear\"),\n    \"MultinomialNB\": MultinomialNB(),\n    \"LogisticRegression\": LogisticRegression(),\n    \"KNeighborsClassifier\": KNeighborsClassifier(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n    \"RandomForestClassifier\": RandomForestClassifier(),\n    \"AdaBoostClassifier\": AdaBoostClassifier(),\n    \"BaggingClassifier\": BaggingClassifier(),\n    \"ExtraTreesClassifier\": ExtraTreesClassifier()\n}\nprediction = dict()\nscore_map = {}\n\nfor model_name in models:\n    model = models[model_name]\n    model.fit(X_train_df,y_train)\n    prediction[model_name] = model.predict(X_test_df)\n    score = accuracy_score(y_test, prediction[model_name])\n    score_map[model_name] = score\n#     print(\"{}{}{}\".format(model_name, \": \", score))","8663c4f5":"result = pd.DataFrame()\nresult[\"model\"] = score_map.keys()\nresult[\"score\"] = score_map.values()\nresult[\"score\"] = result[\"score\"].apply(lambda x: x*100)","c2eec3d3":"def plot_model_performace(result):\n    sns.set_style(\"ticks\")\n    figsize=(22, 6)\n\n    ticksize = 12\n    titlesize = ticksize + 8\n    labelsize = ticksize + 5\n\n    xlabel = \"Model\"\n    ylabel = \"Score\"\n\n    title = \"Model Performance\"\n\n    params = {'figure.figsize' : figsize,\n              'axes.labelsize' : labelsize,\n              'axes.titlesize' : titlesize,\n              'xtick.labelsize': ticksize,\n              'ytick.labelsize': ticksize}\n\n    plt.rcParams.update(params)\n\n    col1 = \"model\"\n    col2 = \"score\"\n    sns.barplot(x=col1, y=col2, data=result)\n    plt.title(title.title())\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.xticks(rotation=90)\n    plt.grid()\n    plt.plot()\n    plt.show()\n    print(result)","d3bc3a09":"plot_model_performace(result)","fef4d1f8":"param_grid = {\n    \"C\": np.concatenate(\n        [\n            np.arange(0.0001, 0.001, 0.0001),\n            np.arange(0.001, 0.01, 0.001),\n            np.arange(0.01, 0.1, 0.01),\n            np.arange(0.1, 1, 0.1),\n            np.arange(1, 10, 1),\n            np.arange(10, 100, 5)\n        ],\n        axis=None),\n    \n    \"kernel\": (\"linear\", \"rbf\", \"poly\", \"sigmoid\"),\n#     \"kernel\": (\"linear\", \"poly\"),\n#     \"degree\": list(np.arange(1,25, 1)),\n#     \"gamma\": np.concatenate(\n#         [\n#             np.arange(0.0001, 0.001, 0.0001),\n#             np.arange(0.001, 0.01, 0.001),\n#             np.arange(0.01, 0.1, 0.01),\n#             np.arange(0.1, 1, 0.1),\n#             np.arange(1, 10, 1),\n#             np.arange(10, 100, 5)\n#         ],\n#         axis=None)\n}\n# print(param_grid)\n# model = svm.SVC(class_weight=\"balanced\")\n# grid = GridSearchCV(model, param_grid, n_jobs=-1, verbose=1, cv=3)\n# grid.fit(X_train_df,y_train)\n# print(\"{}{}\".format(\"Best Estimator: \", grid.best_estimator_))\n# print(\"{}{}\".format(\"Best Params: \", grid.best_params_))\n# print(\"{}{}\".format(\"Best Scores: \", grid.best_score_))","07e1792d":"param_grid = {\n    \"alpha\": np.concatenate(\n        [\n            np.arange(0.0001, 0.001, 0.0001),\n            np.arange(0.001, 0.01, 0.001),\n            np.arange(0.01, 0.1, 0.01),\n            np.arange(0.1, 1, 0.1),\n            np.arange(1, 10, 1),\n            np.arange(10, 100, 5)\n        ]) \n}\n\nmodel = MultinomialNB()\ngrid_cv_model = GridSearchCV(model, param_grid, n_jobs=-1, verbose=3, cv=3)\ngrid_cv_model.fit(X_train_df, y_train)","19f83234":"print(\"{}{}\".format(\"Best Estimator: \", grid_cv_model.best_estimator_))\nprint(\"{}{}\".format(\"Best Params:    \", grid_cv_model.best_params_))\nprint(\"{}{}\".format(\"Best Scores:    \", grid_cv_model.best_score_))","ef67bfac":"print(classification_report(y_test, prediction['MultinomialNB'], target_names = [\"Ham\", \"Spam\"]))","85e40ca8":"def plot_confusion_matrix(y_test, y_pred, title=\"\"):\n    conf_mat = confusion_matrix(y_test, y_pred)\n    conf_mat_normalized = conf_mat.astype('float') \/ conf_mat.sum(axis=1)[:, np.newaxis]\n\n#     sns.set_style(\"ticks\")\n    figsize=(22, 5)\n\n    ticksize = 18\n    titlesize = ticksize + 8\n    labelsize = ticksize + 5\n\n    xlabel = \"Predicted label\"\n    ylabel = \"True label\"\n\n\n    params = {'figure.figsize' : figsize,\n              'axes.labelsize' : labelsize,\n              'axes.titlesize' : titlesize,\n              'xtick.labelsize': ticksize,\n              'ytick.labelsize': ticksize}\n\n    plt.rcParams.update(params)\n\n    plt.subplot(121)\n    sns.heatmap(conf_mat, annot=True)\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n\n    plt.subplot(122)\n    sns.heatmap(conf_mat_normalized, annot=True)\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.show()\n\n\n    print(\"Confusion Matrix:\\n\")\n    print(conf_mat)\n    print(\"\\n\\nConfusion Matrix Normalized:\\n\")\n    print(conf_mat_normalized)","9dd715a9":"plot_confusion_matrix(y_test, prediction['MultinomialNB'], title=\"MultinomialNB\")","7385d0c4":"X_test[y_test < prediction[\"MultinomialNB\"] ]","7e9a516b":"X_test[y_test > prediction[\"MultinomialNB\"] ]","589e0caf":"main_model_dir = output_directory + r\"models\/\"\nmain_log_dir = output_directory + r\"logs\/\"\n\ntry:\n    os.mkdir(main_model_dir)\nexcept:\n    print(\"Could not create main model directory\")\n    \ntry:\n    os.mkdir(main_log_dir)\nexcept:\n    print(\"Could not create main log directory\")\n\n\n\nmodel_dir = main_model_dir + time.strftime('%Y-%m-%d %H-%M-%S') + \"\/\"\nlog_dir = main_log_dir + time.strftime('%Y-%m-%d %H-%M-%S')\n\n\ntry:\n    os.mkdir(model_dir)\nexcept:\n    print(\"Could not create model directory\")\n    \ntry:\n    os.mkdir(log_dir)\nexcept:\n    print(\"Could not create log directory\")\n    \nmodel_file = model_dir + \"{epoch:02d}-val_acc-{val_acc:.2f}-val_loss-{val_loss:.2f}.hdf5\"","af2e2b04":"print(\"Settting Callbacks\")\n\ncheckpoint = ModelCheckpoint(\n    model_file, \n    monitor='val_acc', \n    save_best_only=True)\n\nearly_stopping = EarlyStopping(\n    monitor='val_loss',\n    patience=2,\n    verbose=1,\n    restore_best_weights=True)\n\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.6,\n    patience=1,\n    verbose=1)\n\n\ncallbacks = [checkpoint, reduce_lr, early_stopping]\n\n# callbacks = [early_stopping]\n\nprint(\"Set Callbacks at \", date_time(1))","1f74b98b":"X = df.text\nY = df.label\n\nlabel_encoder = LabelEncoder()\n\nY = label_encoder.fit_transform(Y)\n\nY = Y.reshape(-1, 1)","0c70b67d":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15)\n\nmax_words = len(set(\" \".join(X_train).split()))\nmax_len = X_train.apply(lambda x: len(x)).max()\n\n# max_words = 1000\n# max_len = 150","1a8b2b55":"tokenizer = Tokenizer(num_words=max_words)\n\ntokenizer.fit_on_texts(X_train)\n\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_train_seq = sequence.pad_sequences(X_train_seq, maxlen=max_len)","0bc0db91":"# Calculate Class Weights\ndef get_weight(y):\n    class_weight_current =  cw.compute_class_weight('balanced', np.unique(y), y)\n    return class_weight_current","308fc910":"class_weight = get_weight(Y_train.flatten())","112ce546":"def get_rnn_model():\n    model = Sequential()\n    \n    model.add(Embedding(max_words, 50, input_length=max_len))\n    model.add(LSTM(64))\n    \n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(256, activation='relu'))\n    \n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(1, activation='sigmoid'))\n    \n    model.summary()\n    \n    return model\n\n\ndef get_cnn_model():   \n    model = Sequential()\n    \n    model.add(Embedding(max_words, 50, input_length=max_len))\n    \n    model.add(Conv1D(64, 3, padding='valid', activation='relu', strides=1))\n    model.add(GlobalMaxPooling1D())\n    \n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(256, activation='relu'))\n    \n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(1, activation='sigmoid'))\n    \n    model.summary()\n    return model\n","606da3bd":"def plot_performance(history=None, figure_directory=None, ylim_pad=[0, 0]):\n    xlabel = 'Epoch'\n    legends = ['Training', 'Validation']\n\n    plt.figure(figsize=(20, 5))\n\n    y1 = history.history['acc']\n    y2 = history.history['val_acc']\n\n    min_y = min(min(y1), min(y2))-ylim_pad[0]\n    max_y = max(max(y1), max(y2))+ylim_pad[0]\n\n\n    plt.subplot(121)\n\n    plt.plot(y1)\n    plt.plot(y2)\n\n    plt.title('Model Accuracy\\n'+date_time(1), fontsize=17)\n    plt.xlabel(xlabel, fontsize=15)\n    plt.ylabel('Accuracy', fontsize=15)\n    plt.ylim(min_y, max_y)\n    plt.legend(legends, loc='upper left')\n    plt.grid()\n\n    y1 = history.history['loss']\n    y2 = history.history['val_loss']\n\n    min_y = min(min(y1), min(y2))-ylim_pad[1]\n    max_y = max(max(y1), max(y2))+ylim_pad[1]\n\n\n    plt.subplot(122)\n\n    plt.plot(y1)\n    plt.plot(y2)\n\n    plt.title('Model Loss\\n'+date_time(1), fontsize=17)\n    plt.xlabel(xlabel, fontsize=15)\n    plt.ylabel('Loss', fontsize=15)\n    plt.ylim(min_y, max_y)\n    plt.legend(legends, loc='upper left')\n    plt.grid()\n    if figure_directory:\n        plt.savefig(figure_directory+\"\/history\")\n\n    plt.show()\n","309053d0":"model1 = get_rnn_model()","499cb06a":"# loss = 'categorical_crossentropy'\nloss = 'binary_crossentropy'\nmetrics = ['accuracy']","1683a170":"print(\"Starting...\\n\")\n\nstart_time = time.time()\nprint(date_time(1))\n\nprint(\"\\n\\nCompliling Model ...\\n\")\nlearning_rate = 0.001\noptimizer = Adam(learning_rate)\n# optimizer = Adam()\n\nmodel1.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\nverbose = 1\nepochs = 100\nbatch_size = 128\nvalidation_split = 0.2\n\nprint(\"Trainning Model ...\\n\")\n\nhistory1 = model1.fit(\n    X_train_seq,\n    Y_train,\n    batch_size=batch_size,\n    epochs=epochs,\n    verbose=verbose,\n    callbacks=callbacks,\n    validation_split=validation_split,\n    class_weight =class_weight\n    )\n\nelapsed_time = time.time() - start_time\nelapsed_time = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n\nprint(\"\\nElapsed Time: \" + elapsed_time)\nprint(\"Completed Model Trainning\", date_time(1))","aaebe723":"plot_performance(history=history1)","c7818340":"model2 = get_cnn_model()","4a171157":"print(\"Starting...\\n\")\n\nstart_time = time.time()\nprint(date_time(1))\n\nprint(\"\\n\\nCompliling Model ...\\n\")\nlearning_rate = 0.001\noptimizer = Adam(learning_rate)\n# optimizer = Adam()\n\nmodel2.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\nverbose = 1\nepochs = 100\nbatch_size = 128\nvalidation_split = 0.2\n\nprint(\"Trainning Model ...\\n\")\n\nhistory2 = model2.fit(\n    X_train_seq,\n    Y_train,\n    batch_size=batch_size,\n    epochs=epochs,\n    verbose=verbose,\n    callbacks=callbacks,\n    validation_split=validation_split,\n    class_weight =class_weight\n    )\n\nelapsed_time = time.time() - start_time\nelapsed_time = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n\nprint(\"\\nElapsed Time: \" + elapsed_time)\nprint(\"Completed Model Trainning\", date_time(1))","0cc5e55b":"plot_performance(history=history2)","f888ddd3":"test_X_seq = tokenizer.texts_to_sequences(X_test)\ntest_X_seq = sequence.pad_sequences(test_X_seq, maxlen=max_len)\naccuracy1 = model1.evaluate(test_X_seq, Y_test)\naccuracy2 = model2.evaluate(test_X_seq, Y_test)","a7b0bf30":"print(\"Model Performance of RNN (Test Accuracy):\")\nprint('Accuracy: {:0.2f}%\\nLoss: {:0.3f}\\n'.format(accuracy1[1]*100, accuracy1[0]))\n\nprint(\"\\nModel Performance of RNN (Test Accuracy):\")\nprint('v: {:0.2f}%\\nLoss: {:0.3f}\\n'.format(accuracy2[1]*100, accuracy2[0]))","76194243":"ypreds1 = model1.predict_classes(test_X_seq, verbose=1)\nypreds2 = model2.predict_classes(test_X_seq, verbose=1)","69a31f80":"print(classification_report(Y_test, ypreds1, target_names = [\"Ham\", \"Spam\"]))","8b0cd461":"plot_confusion_matrix(Y_test, ypreds1, title=\"RNN\")","f02b86ce":"print(classification_report(Y_test, ypreds2, target_names = [\"Ham\", \"Spam\"]))","876054c5":"plot_confusion_matrix(Y_test, ypreds2, title=\"CNN\")","c1a7a532":"row1 = pd.DataFrame({'model': 'RNN', 'score': accuracy1[1]*100}, index=[-1])\nresult = pd.concat([row1, result.ix[:]]).reset_index(drop=True)\nrow2 = pd.DataFrame({'model': 'CNN', 'score': accuracy2[1]*100}, index=[-1])\nresult = pd.concat([row2, result.ix[:]]).reset_index(drop=True)","363c8f2e":"plot_model_performace(result)","f7b756ce":"## Output Configuration","effe330b":"### 10.5.1 Evaluation","5c8a8cea":"# Reference:\n1. [Text Preprocessing and Machine Learning Modeling](https:\/\/www.kaggle.com\/futurist\/text-preprocessing-and-machine-learning-modeling)\n2. [keras mlp cnn test for text classification](https:\/\/www.kaggle.com\/jacklinggu\/keras-mlp-cnn-test-for-text-classification)","4db9bf16":"### 10.3.1. RNN","bb2cb9e4":"# 3. Read Data","14e2673b":"## 10.3. Model Trainning","4fb820b7":"# 9. Evaluation Metrics","1def7900":"## 10.1. Preprocessing","e770ac6b":"#### 10.3.1.2  Visualization","634d9499":"## 10.2 Model","fa393e6c":"# 4 . Visualization","663f1208":"# 7. Model Trainning","ea06dd1a":"### 10.3.1. RNN","d1e4dd88":"# 5. Preprocessing","918a075a":"#### 10.5.1.2 Visualization","a065e6d9":"# 8. Hyper Parameter Search","78365967":"# 6. Feature Extraction","51cfa7d7":"# 10. Deep Learning","246abb1b":"## 10.5 Inference\/ Prediction","a580372e":"# 2. Functions","92a3e830":"# 1. Import","d6d92b3a":"#### 10.3.1.2 Visualization"}}