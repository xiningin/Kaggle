{"cell_type":{"841bbd58":"code","cb5e229e":"code","a28eb641":"code","16959671":"code","2aa9f47d":"code","a6574855":"code","345af186":"code","4b86baff":"code","99ee0d60":"code","f262d086":"code","9012c211":"code","569ea84f":"code","1e946c3e":"code","ed5466b1":"code","d4020b98":"code","49b9813e":"code","ff484d15":"code","47fdae13":"code","8e593025":"code","80150de1":"code","e582b8c8":"code","96c9ddb4":"code","ff02de15":"code","cab04cf9":"code","772435fb":"code","da0cb977":"code","323fef67":"code","22b4ab25":"code","73facd7e":"code","907325cd":"code","0a879c79":"code","64acef1a":"code","e7e7b01f":"code","2abde91f":"code","6ea16ae4":"code","f5a5407d":"code","e507d63d":"code","5e2c1d34":"code","e854abc7":"code","e79f66e7":"code","68d9a91c":"markdown","a192b98c":"markdown","6f89db71":"markdown","5d02fd6c":"markdown","8d7550b1":"markdown","595b843a":"markdown","1c61c325":"markdown","dbf482e1":"markdown","6f59a371":"markdown","8154e341":"markdown","596d87ee":"markdown","8ed0c900":"markdown","fcd412a1":"markdown","bcca2045":"markdown"},"source":{"841bbd58":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","cb5e229e":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","a28eb641":"print(\"Training samples: {}\".format(train.shape[0]))\nprint(\"Test samples: {}\".format(test.shape[0]))\ntrain.head(5)","16959671":"train.info()","2aa9f47d":"plt.figure(figsize= (10,7))\nsns.countplot(y= train['keyword'], order=train['keyword'].value_counts().iloc[:10].index)\nplt.title('Top 10 most keywords used on dataset')\nplt.show()","a6574855":"plt.figure(figsize= (10,7))\nsns.countplot(y= train['location'], order=train['location'].value_counts().iloc[:10].index)\nplt.title('Top 10 most frequent places where tweets were published')\nplt.show()","345af186":"train['text'].describe()","4b86baff":"train = train.drop(['keyword', 'location'], axis=1)\ntest = test.drop(['keyword', 'location'], axis=1)","99ee0d60":"def stopwords(text):\n    stopwords = set(nltk.corpus.stopwords.words('english'))\n    palavras = [i for i in text.split() if not i in stopwords]\n    return (\" \".join(palavras))","f262d086":"train['text'] = [stopwords(i) for i in train['text']]\ntrain['text'][:10]\n\n# applying to test\ntest['text'] = [stopwords(i) for i in test['text']]","9012c211":"def remove_carac(text):\n    text = text.str.lower() \n    text = text.str.replace(r\"\\#\",\"\") \n    text = text.str.replace(r\"http\\S+\",\"\")  \n    text = text.str.replace(r\"@\",\"\")\n    text = text.str.replace(r\"[^a-zA-Z#]\", \" \")\n    text = text.str.replace(\"\\s{2,}\", \"\")\n    return text","569ea84f":"train['clean_text'] = remove_carac(train['text'])\n\n# applying to test\ntest['clean_text'] = remove_carac(test['text'])\n\ntrain['clean_text'][:10]","1e946c3e":"from nltk.stem import WordNetLemmatizer\n\nlem = WordNetLemmatizer()\n\ndef lemmatization(texto):\n    palavras = []\n    for w in texto.split():\n        palavras.append(lem.lemmatize(w))\n    return (\" \".join(palavras))","ed5466b1":"train['clean_text'] = [lemmatization(f) for f in train['clean_text']]\ntrain['clean_text'][:10]\n\n# Applying to test\ntest['clean_text'] = [lemmatization(f) for f in test['clean_text']]","d4020b98":"from nltk.stem.snowball import SnowballStemmer\n\ndef stemming(texto):\n    stemmer = SnowballStemmer(language='english')\n    palavras = []\n    for w in texto.split():\n        palavras.append(stemmer.stem(w))\n    return (\" \".join(palavras))","49b9813e":"train['clean_text'] = [stemming(t) for t in train['clean_text']]\n\n# applying to test\ntest['clean_text'] = [stemming(t) for t in test['clean_text']]\n\ntrain['clean_text'][:10]","ff484d15":"# droping the 'text' column and organizing\ntrain = train.drop(['text'], axis=1)\ntrain = train.reindex(columns=['clean_text', 'target'])\n\ntest = test.drop(['text'], axis=1)","47fdae13":"# concat the train and test dataset first\ndf = pd.concat([train,test], ignore_index=True)\n\n# droping the words with less than 3 letters for plot\ndf['clean_text'] = df['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ndf.head()","8e593025":"# Using the WordCloud to see the frequency of the words\nfrom wordcloud import WordCloud\n\npalavras = ' '.join([text for text in df['clean_text']])\n\nwordcloud = WordCloud(width=800, height = 500, max_font_size=110).generate(palavras)\n\n# plotting\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\n\nfig1 = plt.gcf()\nfig1.savefig('words.png')\nplt.show()","80150de1":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport transformers\nimport tqdm","e582b8c8":"# changing the device to GPU\ndevice = torch.device('cuda:0')","96c9ddb4":"# using the roBERTa model\nroberta_weights = 'roberta-base'\nroberta_model = transformers.RobertaModel.from_pretrained(roberta_weights).to(device)\nroberta_token = transformers.RobertaTokenizer.from_pretrained(roberta_weights)","ff02de15":"# tokenizing the clean_text column on train dataset\ntokenized = [torch.tensor(roberta_token.encode(i)).unsqueeze(0).to(device) for i in train['clean_text']]","cab04cf9":"embeddings = []\n\nroberta_model.eval()  # the eval lock the model and stop training\nwith torch.no_grad(): # aplying non-training to torch\n    for x in tqdm.notebook.tqdm(tokenized):  # tqdm is a progress bar\n        embeddings.append(roberta_model(x)[1].cpu().numpy())  # changing the device to cpu for interation with sklearn","772435fb":"# transforming the embeddings list to a array of arrays\nembeddings_numpy = np.array(embeddings).squeeze()\n\nembeddings_numpy.shape","da0cb977":"# defining the features and target\nX = embeddings_numpy\ny = train['target']","323fef67":"from sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.5, random_state=42)","22b4ab25":"from sklearn.model_selection import RepeatedKFold\n\nkf = RepeatedKFold(n_splits= 2, n_repeats=10, random_state=42)\n\n\nfor train_lines, valid_lines in kf.split(X):\n    X_train, X_valid = X[train_lines], X[valid_lines]\n    y_train, y_valid = y[train_lines], y[valid_lines]","73facd7e":"from sklearn.metrics import classification_report\nfrom lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier(boosting_type= 'gbdt', objective='binary', learning_rate= 0.1, random_state=42, class_weight='balanced', num_leaves=32)\nlgbm.fit(X_train, y_train)\n\nprint(classification_report(y_valid, lgbm.predict(X_valid)))","907325cd":"from sklearn.metrics import confusion_matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None, normalize=True):\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n    plt.figure(figsize=(10, 7))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","0a879c79":"y_pred = lgbm.predict(X_valid)\n\nplot_confusion_matrix(confusion_matrix(y_valid, y_pred), ['Disaster', 'Not_disaster'])","64acef1a":"tokenized_test = [torch.tensor(roberta_token.encode(i)).unsqueeze(0).to(device) for i in test['clean_text']]","e7e7b01f":"test_embeddings = []\n\nroberta_model.eval()  # the eval lock the model and stop training\nwith torch.no_grad(): # aplying non-training to torch\n    for x in tqdm.notebook.tqdm(tokenized_test):  # tqdm is a progress bar\n        test_embeddings.append(roberta_model(x)[1].cpu().numpy())  # changing the device to cpu for interation with sklearn","2abde91f":"test_embeddings_numpy = np.array(test_embeddings).squeeze()\n\ntest_embeddings_numpy.shape","6ea16ae4":"# predicting the test\nfeatures = test_embeddings_numpy\n\npred = lgbm.predict(features)","f5a5407d":"sample_submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","e507d63d":"sample_submission['target'].shape","5e2c1d34":"sample_submission['target'] = pred\nsample_submission['target'].value_counts()","e854abc7":"sample_submission.to_csv('submission.csv', index=False)","e79f66e7":"!head -n10 submission.csv","68d9a91c":"## Split","a192b98c":"## Stemming","6f89db71":"## Cross Validation","5d02fd6c":"## Confusion Matrix","8d7550b1":"## Ploting the frequecy words","595b843a":"# Transforming to Series for submission","1c61c325":"## Test","dbf482e1":"## Exploratory data analysis","6f59a371":"## Removing caracters","8154e341":"## Stopwords","596d87ee":"## Lemmatization","8ed0c900":"## Drop the columns with no iteration ","fcd412a1":"## Tokenizer and Vectorization with roBERTa","bcca2045":"## Building the LGBM model"}}