{"cell_type":{"961332f8":"code","fc24661d":"code","18111460":"code","8cf001cb":"code","46423cb9":"code","7d8d5e9c":"code","a2bd8c65":"code","706a18dc":"code","3ff7cf30":"code","49af3101":"code","1ceea41c":"code","9f362487":"code","33c08d84":"code","1a10a3c5":"code","963b4a48":"code","c0633852":"code","a5546dd6":"code","afefbca2":"code","c5f3ae90":"code","b4a6a0a4":"code","a142d32c":"code","556ef751":"code","4b44796e":"code","79e70e40":"code","f326d4cd":"code","a36de585":"code","120e38d5":"code","0e5f4a25":"code","16203dc3":"code","e4faa203":"code","8525a74f":"code","2d5c9f00":"code","3da3c93f":"code","f937ceb4":"code","061012f9":"code","d29e09a6":"code","f7b96af6":"code","8a84977a":"code","79734e20":"code","0c531ba3":"code","22085778":"code","d49b0087":"code","34c540ce":"code","fa1dd701":"code","b9c97a7c":"code","531f4b09":"code","b81ae12c":"code","0325d005":"code","5f9f5ade":"code","da218494":"markdown","00a6f35b":"markdown","235e7580":"markdown","2b606b6f":"markdown","48d6b793":"markdown","8223660e":"markdown","33592154":"markdown","5d148bbf":"markdown","1e8f85da":"markdown","461f0d22":"markdown","fe8f9d8b":"markdown","a3bfd869":"markdown","ba3acda3":"markdown"},"source":{"961332f8":"# Importing  Libraries\nfrom numpy.random import seed\nseed(101)\n\nimport pandas as pd\nimport numpy as np\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\n\nimport os\nimport cv2\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport shutil\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntf.random.set_seed(101)","fc24661d":"# Setting Some Pre-Requisites\nIMAGE_SIZE=96\nIMAGE_CHANNELS=3\nSAMPLE_SIZE=80000         # We will be training 80,000 samples from each label","18111460":"# So, what are the files which are available?\n\nos.listdir('..\/input\/histopathologic-cancer-detection')","8cf001cb":"# So, how many images are there in each of the folder in the training dataset?\n\nprint(len(os.listdir('..\/input\/histopathologic-cancer-detection\/train')))\nprint(len(os.listdir('..\/input\/histopathologic-cancer-detection\/test')))","46423cb9":"# Creating a dataframe of all the training images\n\ndf_data = pd.read_csv('..\/input\/histopathologic-cancer-detection\/train_labels.csv')\n\n# removing this image because it caused a training error previously\ndf_data[df_data['id'] != 'dd6dfed324f9fcb6f93f46f32fc800f2ec196be2']\n\n# removing this image because it's black\ndf_data[df_data['id'] != '9369c7278ec8bcc6c880d99194de09fc2bd4efbe']\n\n\nprint(df_data.shape)","7d8d5e9c":"df_data['label'].value_counts()","a2bd8c65":"# source: https:\/\/www.kaggle.com\/gpreda\/honey-bee-subspecies-classification\n\ndef draw_category_images(col_name,figure_cols, df, IMAGE_PATH):\n    \n    \"\"\"\n    Give a column in a dataframe,\n    this function takes a sample of each class and displays that\n    sample on one row. The sample size is the same as figure_cols which\n    is the number of columns in the figure.\n    Because this function takes a random sample, each time the function is run it\n    displays different images.\n    \"\"\"\n    \n\n    categories = (df.groupby([col_name])[col_name].nunique()).index\n    f, ax = plt.subplots(nrows=len(categories),ncols=figure_cols, \n                         figsize=(4*figure_cols,4*len(categories))) # adjust size here\n    # draw a number of images for each location\n    for i, cat in enumerate(categories):\n        sample = df[df[col_name]==cat].sample(figure_cols) # figure_cols is also the sample size\n        for j in range(0,figure_cols):\n            file=IMAGE_PATH + sample.iloc[j]['id'] + '.tif'\n            im=cv2.imread(file)\n            ax[i, j].imshow(im, resample=True, cmap='gray')\n            ax[i, j].set_title(cat, fontsize=16)  \n    plt.tight_layout()\n    plt.show()","706a18dc":"IMAGE_PATH = '..\/input\/histopathologic-cancer-detection\/train\/' \n\ndraw_category_images('label',4, df_data, IMAGE_PATH)","3ff7cf30":"# Create the Train and Validation Sets\n\ndf_0=df_data[df_data['label']==0].sample(SAMPLE_SIZE,random_state=101)\ndf_1=df_data[df_data['label']==1].sample(SAMPLE_SIZE,random_state=101)\n\n# concat the dataframes\ndf_data = pd.concat([df_0, df_1], axis=0).reset_index(drop=True)\n# shuffle\ndf_data = shuffle(df_data)\n\ndf_data['label'].value_counts()","49af3101":"# Now, for the train-test split\n\n# stratify=y creates a balanced validation set.\ny = df_data['label']\n\ndf_train, df_val = train_test_split(df_data, test_size=0.10, random_state=101, stratify=y)\n\nprint(df_train.shape)\nprint(df_val.shape)","1ceea41c":"from pathlib import Path\n\ndir_path = Path('.\/base_dir')\n\ntry:\n    dir_path.rmdir()\nexcept OSError as e:\n    print(\"Error: %s : %s\" % (dir_path, e.strerror))\n","9f362487":"# Create a new directory so that we will be using the ImageDataGenerator\nbase_dir='base_dir'\nos.mkdir(base_dir)\n\n# now we create 2 folders inside 'base_dir':\n\n# train_dir\n    # a_no_tumor_tissue\n    # b_has_tumor_tissue\n\n# val_dir\n    # a_no_tumor_tissue\n    # b_has_tumor_tissue\n\n\n\n# create a path to 'base_dir' to which we will join the names of the new folders\n# train_dir\ntrain_dir = os.path.join(base_dir, 'train_dir')\n\nos.mkdir(train_dir)\n\n# val_dir\nval_dir = os.path.join(base_dir, 'val_dir')\n\nos.mkdir(val_dir)\n\n\n\n# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\n# Inside each folder we create seperate folders for each class\n\n# create new folders inside train_dir\nno_tumor_tissue = os.path.join(train_dir, 'a_no_tumor_tissue')\nos.mkdir(no_tumor_tissue)\nhas_tumor_tissue = os.path.join(train_dir, 'b_has_tumor_tissue')\nos.mkdir(has_tumor_tissue)\n\n\n# create new folders inside val_dir\nno_tumor_tissue = os.path.join(val_dir, 'a_no_tumor_tissue')\n#os.rmdir(no_tumor_tissue)\nos.mkdir(no_tumor_tissue)\nhas_tumor_tissue = os.path.join(val_dir, 'b_has_tumor_tissue')\n#os.rmdir(has_tumor_tissue)\nos.mkdir(has_tumor_tissue)","33c08d84":"# check that the folders have been created\nos.listdir('base_dir\/train_dir')","1a10a3c5":"# Set the id as the index in df_data\ndf_data.set_index('id', inplace=True)","963b4a48":"# Get a list of train and val images\ntrain_list = list(df_train['id'])\nval_list = list(df_val['id'])\n\n\ncountt=1\n# Transfer the train images\n\nfor image in train_list:\n    \n    # the id in the csv file does not have the .tif extension therefore we add it here\n    fname = image + '.tif'\n    # get the label for a certain image\n    target = df_data.loc[image,'label']\n    \n    print(countt)\n    # these must match the folder names\n    if target == 0:\n        label = 'a_no_tumor_tissue'\n    if target == 1:\n        label = 'b_has_tumor_tissue'\n    \n    # source path to image\n    src = os.path.join('..\/input\/histopathologic-cancer-detection\/train', fname)\n    # destination path to image\n    dst = os.path.join(train_dir, label, fname)\n    # copy the image from the source to the destination\n    shutil.copyfile(src, dst)\n    countt=countt+1\n\n\n# Transfer the val images\ncount=1\nfor image in val_list:\n    \n    # the id in the csv file does not have the .tif extension therefore we add it here\n    fname = image + '.tif'\n    # get the label for a certain image\n    target = df_data.loc[image,'label']\n    print(count)\n    \n    # these must match the folder names\n    if target == 0:\n        label = 'a_no_tumor_tissue'\n    if target == 1:\n        label = 'b_has_tumor_tissue'\n    \n\n    # source path to image\n    src = os.path.join('..\/input\/histopathologic-cancer-detection\/train', fname)\n    # destination path to image\n    dst = os.path.join(val_dir, label, fname)\n    # copy the image from the source to the destination\n    shutil.copyfile(src, dst)\n    count=count+1","c0633852":"# check how many train images we have in each folder\n\nprint(len(os.listdir('base_dir\/train_dir\/a_no_tumor_tissue')))\nprint(len(os.listdir('base_dir\/train_dir\/b_has_tumor_tissue')))","a5546dd6":"# check how many val images we have in each folder\n\nprint(len(os.listdir('base_dir\/val_dir\/a_no_tumor_tissue')))\nprint(len(os.listdir('base_dir\/val_dir\/b_has_tumor_tissue')))","afefbca2":"# Set up the generators\ntrain_path = 'base_dir\/train_dir'\nvalid_path = 'base_dir\/val_dir'\ntest_path = '..\/input\/histopathologic-cancer-detection\/test'\n\nnum_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = 10\nval_batch_size = 10\n\n\ntrain_steps = np.ceil(num_train_samples \/ train_batch_size)\nval_steps = np.ceil(num_val_samples \/ val_batch_size)","c5f3ae90":"datagen = ImageDataGenerator(rescale=1.0\/255)\n\ntrain_gen = datagen.flow_from_directory(train_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=train_batch_size,\n                                        class_mode='categorical')\n\nval_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=val_batch_size,\n                                        class_mode='categorical')\n\n# Note: shuffle=False causes the test dataset to not be shuffled\ntest_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=1,\n                                        class_mode='categorical',\n                                        shuffle=False)","b4a6a0a4":"kernel_size = (3,3)\npool_size= (2,2)\nfirst_filters = 32\nsecond_filters = 64\nthird_filters = 128\n\ndropout_conv = 0.3\ndropout_dense = 0.3\n\n\nmodel = Sequential()\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu', input_shape = (96, 96, 3)))\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size)) \nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(dropout_dense))\nmodel.add(Dense(2, activation = \"softmax\"))\n\nmodel.summary()","a142d32c":"model.compile(Adam(lr=0.0001), loss='binary_crossentropy', \n              metrics=['accuracy'])","556ef751":"# Get the labels that are associated with each index\nprint(val_gen.class_indices)","4b44796e":"filepath = \"model.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n                             save_best_only=True, mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, \n                                   verbose=1, mode='max', min_lr=0.00001)\n                              \n                              \ncallbacks_list = [checkpoint, reduce_lr]\n\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n                    validation_data=val_gen,\n                    validation_steps=val_steps,\n                    epochs=10, verbose=1,\n                   callbacks=callbacks_list)","79e70e40":"# get the metric names so we can use evaulate_generator\nmodel.metrics_names","f326d4cd":"# Here the best epoch will be used.\n\n\n\nval_loss, val_acc = \\\nmodel.evaluate_generator(test_gen, \n                        steps=len(df_val))\n\nprint('val_loss:', val_loss)\nprint('val_acc:', val_acc)","a36de585":"#Save the last model  \nmodel.save('model.h5')","120e38d5":"# display the loss and accuracy curves\n\nimport matplotlib.pyplot as plt\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()","0e5f4a25":"# make a prediction\npredictions = model.predict_generator(test_gen, steps=len(df_val), verbose=1)","16203dc3":"predictions.shape","e4faa203":"#Save the last model\n#model.save('..\/input\/model.h5')","8525a74f":"# This is how to check what index keras has internally assigned to each class. \ntest_gen.class_indices","2d5c9f00":"# Put the predictions into a dataframe.\n# The columns need to be ordered to match the output of the previous cell\n\ndf_preds = pd.DataFrame(predictions, columns=['no_tumor_tissue', 'has_tumor_tissue'])\n\ndf_preds.head()\n","3da3c93f":"# Get the true labels\ny_true = test_gen.classes\n\n# Get the predicted labels as probabilities\ny_pred = df_preds['has_tumor_tissue']","f937ceb4":"from sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_true, y_pred)","061012f9":"# Get the labels of the test images.\n\ntest_labels = test_gen.classes\ntest_labels.shape","d29e09a6":"# argmax returns the index of the max value in a row\ncm = confusion_matrix(test_labels, predictions.argmax(axis=1))\n# Print the label associated with each class\ntest_gen.class_indices","f7b96af6":"from sklearn.metrics import plot_confusion_matrix","8a84977a":"# Delete base_dir and it's sub folders to free up disk space.\n\nshutil.rmtree('base_dir')\n#[CREATE A TEST FOLDER DIRECTORY STRUCTURE]\n\n# We will be feeding test images from a folder into predict_generator().\n# Keras requires that the path should point to a folder containing images and not\n# to the images themselves. That is why we are creating a folder (test_images) \n# inside another folder (test_dir).\n\n# test_dir\n    # test_images\n\n# create test_dir\ntest_dir = 'test_dir'\nos.mkdir(test_dir)\n    \n# create test_images inside test_dir\ntest_images = os.path.join(test_dir, 'test_images')\nos.mkdir(test_images)\n# check that the directory we created exists\nos.listdir('test_dir')","79734e20":"# Transfer the test images into image_dir\n\ntest_list = os.listdir('..\/input\/histopathologic-cancer-detection\/test')\n\nfor image in test_list:\n    \n    fname = image\n    \n    # source path to image\n    src = os.path.join('..\/input\/histopathologic-cancer-detection\/test', fname)\n    # destination path to image\n    dst = os.path.join(test_images, fname)\n    # copy the image from the source to the destination\n    shutil.copyfile(src, dst)\n# check that the images are now in the test_images\n# Should now be 57458 images in the test_images folder\n\nlen(os.listdir('test_dir\/test_images'))","0c531ba3":"test_path ='test_dir'\n\n\n# Here we change the path to point to the test_images folder.\n\ntest_gen = datagen.flow_from_directory(test_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=1,\n                                        class_mode='categorical',\n                                        shuffle=False)","22085778":"num_test_images = 57458\n\n\n\npredictions = model.predict_generator(test_gen, steps=num_test_images, verbose=1)","d49b0087":"# Are the number of predictions correct?\n# Should be 57458.\n\nlen(predictions)","34c540ce":"# Put the predictions into a dataframe\n\ndf_preds = pd.DataFrame(predictions, columns=['no_tumor_tissue', 'has_tumor_tissue'])\n\ndf_preds.head()","fa1dd701":"# This outputs the file names in the sequence in which \n# the generator processed the test images.\ntest_filenames = test_gen.filenames\n\n# add the filenames to the dataframe\ndf_preds['file_names'] = test_filenames\n\ndf_preds.head()","b9c97a7c":"# Create an id column\n\n# A file name now has this format: \n# test_images\/00006537328c33e284c973d7b39d340809f7271b.tif\n\n# This function will extract the id:\n# 00006537328c33e284c973d7b39d340809f7271b\n\n\ndef extract_id(x):\n    \n    # split into a list\n    a = x.split('\/')\n    # split into a list\n    b = a[1].split('.')\n    extracted_id = b[0]\n    \n    return extracted_id\n\ndf_preds['id'] = df_preds['file_names'].apply(extract_id)\n\ndf_preds.head()","531f4b09":"# Get the predicted labels.\n# We were asked to predict a probability that the image has tumor tissue\ny_pred = df_preds['has_tumor_tissue']\n\n# get the id column\nimage_id = df_preds['id']","b81ae12c":"submission = pd.DataFrame({'id':image_id, \n                           'label':y_pred, \n                          }).set_index('id')\n\nsubmission.to_csv('patch_preds.csv', columns=['label']) \nsubmission.head()","0325d005":"# Delete the test_dir directory we created to prevent a Kaggle error.\n# Kaggle allows a max of 500 files to be saved.\n\nshutil.rmtree('test_dir')","5f9f5ade":"import seaborn as sns\nimport matplotlib.pyplot as plt     \n\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); ","da218494":"### So, now let us dive into the domain which involves Data Collection:\n\n\n* #### The data that is provided to us for classification are the histopathological images. These images are glass slide microscope images of lymph nodes that are stained with hematoxylin and eosin (H&E). \n* #### Hematoxylin and eosin (H&E) is the most widely used stain in histology and allows localization of nuclei and extracellular proteins. Hematoxylin, not a dye itself, produces the blue Hematin via an oxidation reaction with nuclear histones causing nuclei to show blue. \n* #### Typically nuclei are stained blue, whereas cytoplasm and extracellular parts in various shades of pink.\n\n* #### Lymph nodes are small glands that filter the fluid in the lymphatic system and they are the first place a breast cancer is likely to spread. \n\n* #### Histological assessment of lymph node metastases is part of determining the stage of breast cancer in TNM classification which is a globally recognized standard for classifying the extent of spread of cancer. \n\n### Links for Reference\n\n* <a href='https:\/\/www.cancer.net\/navigating-cancer-care\/cancer-basics\/what-metastasis'>What is Metastatis?<\/a>\n* <a href='https:\/\/en.wikipedia.org\/wiki\/H%26E_stain'>Hematoxylin and eosin (H&E) Staining<\/a>\n* <a href='https:\/\/www.medicalnewstoday.com\/articles\/320987#:~:text=Lymphocytes%20are%20white%20blood%20cells,immune%20cells%20that%20include%20lymphocytes.'>Lymphocytes<\/a>\n* <a href='https:\/\/www.healthdirect.gov.au\/lymph-nodes'>Lymph Nodes<\/a>\n* <a href='https:\/\/en.wikipedia.org\/wiki\/TNM_staging_system'>TNM Classification<\/a>","00a6f35b":"# 5. Submission","235e7580":"## I hope that you found my notebook useful. It took me 2 weeks to analyze the problem and then perform the coding. So, if you enjoyed the notebook, please leave an upvote. Thanks a lot for reading!!","2b606b6f":"# 1 \n\n# a). Problem Statment\n\n> ### Task - The problem is mainly a BINARY IMAGE CLASSIFICATION PROBLEM. The Problem focuses on identifying the presence of metastases from a 96 * 96 digital histopathology images\n\n> ### Metric Evaluation - Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target. \n\n<img src='https:\/\/i.stack.imgur.com\/kqxaJ.png' style=\"width:500px;height:300px;\">\n\n\n# b). Analysis of the problem Statment\n\n> ## What Exactly the problem statment conveys to us?\n> ### 1. The problem deals with the Binary Classification of the Image that has a shape of 96px * 96px. It involves identifying the metastases from the 96px * 96px digital histapathology images.\n\n> ### 2. One key challenge is that the metastases can be as small as single cells in a large area of tissue.\n","48d6b793":"# Confusion Matrix","8223660e":"### About the Domain: \nObviously, I do not know much about Biology,I made some notes about the following terminologies :\n* Histopathology\n* Lymphocytes\n* Lymph Nodes\n\n### So, let us see some of the biological terminologies involved \n\n### 1. Histopathology - Histopathology is the diagnosis and study of diseases of the tissues, and involves examining tissues and\/or cells under a microscope. Histopathologists are responsible for making tissue diagnoses and helping clinicians manage a patient's care.\n\n<img src = 'https:\/\/www.news-medical.net\/image.axd?picture=2018%2F12%2FBy_Vshivkova-1.jpg' style=\"width:500px;height:300px;\">\n\n\n#\n\n### 2. Lymphocytes - Lymphocytes are white blood cells that are also one of the body's main types of immune cells. They are made in the bone marrow and found in the blood and lymph tissue. The immune system is a complex network of cells known as immune cells that include lymphocytes.\n\n\n<img src = 'https:\/\/healthmattersio.files.wordpress.com\/2018\/05\/lymphocytes-healthmatters-io.png?w=1600&h=1200&crop=1' style=\"width:500px;height:300px;\">\n\n\n#\n### 3. Lymph Nodes- Lymph nodes are small lumps of tissue that contain white blood cells, which fight infection. They filter lymph fluid, which is composed of fluid and waste products from your body tissues. Lymph nodes also help activate your immune system if you have an infection.\n\n\n","33592154":"# 4. Validation and Analysis \n\n* ### Metrics\n* ### Prediction and Activation Visualizations\n* ### ROC and AUC","5d148bbf":"# 2.  Data Understanding\n\n* The dataset contains the histopathological Images, each image is 96px * 96px. \n\n* A positive label indicates that the center 32x32px region of a patch contains at least one pixel of tumor tissue. Tumor tissue in the outer region of the patch does not influence the label. This outer region is provided to enable fully-convolutional models that do not use zero-padding, to ensure consistent behavior when applied to a whole-slide image.\n\n* Kaggle says that :\n                    'The original PCam dataset contains duplicate images due to its probabilistic sampling,\n                    however, the version presented on Kaggle does not contain duplicates. We have otherwise \n                    maintained the same data and splits as the PCam benchmark.'\n                   \n* Also, one of the hing is that the problem states that the training Data contains **50\/50** Images of both the labels i.e. the training contains equal proportion of both the labels, however on analysis it was found to be nearly equal to **60\/40**, which we will consider while we design the model\n\n\n\n\n* ### **IS DATA RELEVANT TO THE PROBLEM ?**\n> This dataset is a combination of two independent datasets collected in Radboud University Medical Center (Nijmegen, the Netherlands), and the University Medical Center Utrecht (Utrecht, the Netherlands). The slides are produced by routine clinical practices and a trained pathologist would examine similar images for identifying metastases.\n\n* ### So, now let us move forward to design our model","1e8f85da":"### The Histopathological Images:","461f0d22":"# 3. Designing the Model  (Coding Part)","fe8f9d8b":"### The model that I have choosen for this problem has been taken from <a href = 'https:\/\/www.kaggle.com\/fmarazzi\/baseline-keras-cnn-roc-fast-10min-0-925-lb'>Baseline Keras CNN<\/a>","a3bfd869":"### Confusion Matrix","ba3acda3":"# What things are included in this Kernel ?\n\n* Problem Statment and the Analysis of the Problem Statment\n* Data Understanding\n* Designing the Model\n* Validation And Analysis\n    * Metrics\n    * Prediction and Activation Visualizations\n    * ROC AND AUC\n* Submission\n"}}