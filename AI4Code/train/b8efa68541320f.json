{"cell_type":{"88d590fb":"code","c0477734":"code","b2fda0fe":"code","6a5fbb5c":"code","1e342b1d":"code","6456cfaa":"code","9b4d2a3e":"code","f8770156":"code","eb479d49":"code","0e11167f":"code","154ac5e3":"code","bedbabe6":"code","0c52b274":"code","d8bc3d3b":"code","8587d408":"code","97fa6ec9":"code","4ec92c70":"code","3fd6697f":"code","9fff1b95":"markdown","5e08833c":"markdown","1f656133":"markdown","d2f2e729":"markdown","91a55014":"markdown","4edbe3fb":"markdown","4b80a56f":"markdown","c178e7d1":"markdown","3ad8cfec":"markdown","3a6ba695":"markdown","65b77d6d":"markdown","2105cc83":"markdown","6e5ef139":"markdown","5423de27":"markdown","cdfbf8f5":"markdown","d7156357":"markdown","26bbbba7":"markdown","d910dc4d":"markdown","a5fca421":"markdown","9ad27688":"markdown","fc06b03a":"markdown","e0fcc9ad":"markdown","475b6319":"markdown","69a4f9b9":"markdown","743ea230":"markdown","91a8ec2a":"markdown","18b9fe95":"markdown","9c9a839c":"markdown","d192672b":"markdown"},"source":{"88d590fb":"%%capture\n\n## Importing Packages\nimport math\nimport nltk\nimport random\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n## Basic File Paths\ndata_dir = \"..\/input\/tweets-blogs-news-swiftkey-dataset-4million\/final\/en_US\"\nfile_path = data_dir + \"\/en_US.twitter.txt\"\n\n## nltk settings\nnltk.data.path.append(data_dir)\nnltk.download('punkt')\n\n## Opening the File in read mode (\"r\")\nwith open(file_path, \"r\") as f:\n    data = f.read()","c0477734":"def preprocess_pipeline(data) -> 'list':\n\n    # Split by newline character\n    sentences = data.split('\\n')\n    \n    # Remove leading and trailing spaces\n    sentences = [s.strip() for s in sentences]\n    \n    # Drop Empty Sentences\n    sentences = [s for s in sentences if len(s) > 0]\n    \n    # Empty List to hold Tokenized Sentences\n    tokenized = []\n    \n    # Iterate through sentences\n    for sentence in sentences:\n        \n        # Convert to lowercase\n        sentence = sentence.lower()\n        \n        # Convert to a list of words\n        token = nltk.word_tokenize(sentence)\n        \n        # Append to list\n        tokenized.append(token)\n        \n    return tokenized\n\n\n## Pass our data to this function    \ntokenized_sentences = preprocess_pipeline(data)","b2fda0fe":"## Obtain Train and Test Split \ntrain, test = train_test_split(tokenized_sentences, test_size=0.2, random_state=42)\n\n## Obtain Train and Validation Split \ntrain, val = train_test_split(train, test_size=0.25, random_state=42)","6a5fbb5c":"def count_the_words(sentences) -> 'dict':\n    \n  # Creating a Dictionary of counts\n  word_counts = {}\n\n  # Iterating over sentences\n  for sentence in sentences:\n    \n    # Iterating over Tokens\n    for token in sentence:\n    \n      # Add count for new word\n      if token not in word_counts.keys():\n        word_counts[token] = 1\n        \n      # Increase count by one\n      else:\n        word_counts[token] += 1\n        \n  return word_counts","1e342b1d":"def handling_oov(tokenized_sentences, count_threshold) -> 'list':\n\n  # Empty list for closed vocabulary\n  closed_vocabulary = []\n\n  # Obtain frequency dictionary using previously defined function\n  words_count = count_the_words(tokenized_sentences)\n    \n  # Iterate over words and counts \n  for word, count in words_count.items():\n    \n    # Append if it's more(or equal) to the threshold \n    if count >= count_threshold :\n      closed_vocabulary.append(word)\n\n  return closed_vocabulary","6456cfaa":"def unk_tokenize(tokenized_sentences, vocabulary, unknown_token = \"<unk>\") -> 'list':\n\n  # Convert Vocabulary into a set\n  vocabulary = set(vocabulary)\n\n  # Create empty list for sentences\n  new_tokenized_sentences = []\n  \n  # Iterate over sentences\n  for sentence in tokenized_sentences:\n\n    # Iterate over sentence and add <unk> \n    # if the token is absent from the vocabulary\n    new_sentence = []\n    for token in sentence:\n      if token in vocabulary:\n        new_sentence.append(token)\n      else:\n        new_sentence.append(unknown_token)\n    \n    # Append sentece to the new list\n    new_tokenized_sentences.append(new_sentence)\n\n  return new_tokenized_sentences","9b4d2a3e":"def cleansing(train_data, test_data, count_threshold):\n    \n  # Get closed Vocabulary\n  vocabulary = handling_oov(train_data, count_threshold)\n    \n  # Updated Training Dataset\n  new_train_data = unk_tokenize(train_data, vocabulary)\n    \n  # Updated Test Dataset\n  new_test_data = unk_tokenize(test_data, vocabulary)\n\n  return new_train_data, new_test_data, vocabulary","f8770156":"min_freq = 6\nfinal_train, final_test, vocabulary = cleansing(train, test, min_freq)","eb479d49":"def count_n_grams(data, n, start_token = \"<s>\", end_token = \"<e>\") -> 'dict':\n\n  # Empty dict for n-grams\n  n_grams = {}\n \n  # Iterate over all sentences in the dataset\n  for sentence in data:\n        \n    # Append n start tokens and a single end token to the sentence\n    sentence = [start_token]*n + sentence + [end_token]\n    \n    # Convert the sentence into a tuple\n    sentence = tuple(sentence)\n\n    # Temp var to store length from start of n-gram to end\n    m = len(sentence) if n==1 else len(sentence)-1\n    \n    # Iterate over this length\n    for i in range(m):\n        \n      # Get the n-gram\n      n_gram = sentence[i:i+n]\n    \n      # Add the count of n-gram as value to our dictionary\n      # IF n-gram is already present\n      if n_gram in n_grams.keys():\n        n_grams[n_gram] += 1\n      # Add n-gram count\n      else:\n        n_grams[n_gram] = 1\n        \n  return n_grams","0e11167f":"def prob_for_single_word(word, previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary_size, k = 1.0) -> 'float':\n\n  # Convert the previous_n_gram into a tuple \n  previous_n_gram = tuple(previous_n_gram)\n    \n  # Calculating the count, if exists from our freq dictionary otherwise zero\n  previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n  \n  # The Denominator\n  denom = previous_n_gram_count + k * vocabulary_size\n\n  # previous n-gram plus the current word as a tuple\n  nplus1_gram = previous_n_gram + (word,)\n\n  # Calculating the nplus1 count, if exists from our freq dictionary otherwise zero \n  nplus1_gram_count = nplus1_gram_counts[nplus1_gram] if nplus1_gram in nplus1_gram_counts else 0\n\n  # Numerator\n  num = nplus1_gram_count + k\n\n  # Final Fraction\n  prob = num \/ denom\n  return prob","154ac5e3":"def probs(previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0) -> 'dict':\n\n  # Convert to Tuple\n  previous_n_gram = tuple(previous_n_gram)\n\n  # Add end and unknown tokens to the vocabulary\n  vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n\n  # Calculate the size of the vocabulary\n  vocabulary_size = len(vocabulary)\n\n  # Empty dict for probabilites\n  probabilities = {}\n\n  # Iterate over words \n  for word in vocabulary:\n    \n    # Calculate probability\n    probability = prob_for_single_word(word, previous_n_gram, \n                                           n_gram_counts, nplus1_gram_counts, \n                                           vocabulary_size, k=k)\n    # Create mapping: word -> probability\n    probabilities[word] = probability\n\n  return probabilities","bedbabe6":"def auto_complete(previous_tokens, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0, start_with=None):\n\n    \n    # length of previous words\n    n = len(list(n_gram_counts.keys())[0]) \n    \n    # most recent 'n' words\n    previous_n_gram = previous_tokens[-n:]\n    \n    # Calculate probabilty for all words\n    probabilities = probs(previous_n_gram,n_gram_counts, nplus1_gram_counts,vocabulary, k=k)\n\n    # Intialize the suggestion and max probability\n    suggestion = None\n    max_prob = 0\n\n    # Iterate over all words and probabilites, returning the max.\n    # We also add a check if the start_with parameter is provided\n    for word, prob in probabilities.items():\n        \n        if start_with != None: \n            \n            if not word.startswith(start_with):\n                continue \n\n        if prob > max_prob: \n\n            suggestion = word\n            max_prob = prob\n\n    return suggestion, max_prob","0c52b274":"def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n\n    # See how many models we have\n    count = len(n_gram_counts_list)\n    \n    # Empty list for suggestions\n    suggestions = []\n    \n    # IMP: Earlier \"-1\"\n    \n    # Loop over counts\n    for i in range(count-1):\n        \n        # get n and nplus1 counts\n        n_gram_counts = n_gram_counts_list[i]\n        nplus1_gram_counts = n_gram_counts_list[i+1]\n        \n        # get suggestions \n        suggestion = auto_complete(previous_tokens, n_gram_counts,\n                                    nplus1_gram_counts, vocabulary,\n                                    k=k, start_with=start_with)\n        # Append to list\n        suggestions.append(suggestion)\n        \n    return suggestions","d8bc3d3b":"n_gram_counts_list = []\nfor n in range(1, 6):\n    n_model_counts = count_n_grams(final_train, n)\n    n_gram_counts_list.append(n_model_counts)","8587d408":"previous_tokens = [\"i\", \"was\", \"about\"]\nsuggestion = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n\ndisplay(suggestion)","97fa6ec9":"print(\"unigram count:\" , len(n_gram_counts_list[0]))\nprint(\"bigram count:\", len(n_gram_counts_list[1]))\nprint(\"trigram count:\", len(n_gram_counts_list[2]))\nprint(\"quadgram count:\", len(n_gram_counts_list[3]))\nprint(\"quintgram count:\", len(n_gram_counts_list[4]))","4ec92c70":"# Storing to file\nwith open(\"en_counts.txt\", 'wb') as f:\n    pickle.dump(n_gram_counts_list, f)","3fd6697f":"# Storing to file\nwith open(\"vocab.txt\", 'wb') as f:\n    pickle.dump(vocabulary, f)","9fff1b95":"<a id=\"section-one\"><\/a>\n# \ud83d\udcda Theory\n\nLet's delve into the theory and try to gain a intuition about n-gram language models.\n\nN-Gram models are Statistical(Probabilistic) Language models that aim to assign probabilities to a given sequence of words. Any N-gram is just a sequence of \"n\" words. For example, \"Saurav\" is a unigram and \"Hi There\" is a bigram. \n\n\nThe task is to find out if we can compute $P(w | h)$ given a word $w$ and some history $h$. One could say that we can compute the probability of a given next word, using all the previous words in the sentence. For example using the last sentence, we could calculate: \n\n$$\\large\nP ( \\, word \\, | \\, One \\, could \\, say \\, that \\, we \\, can \\, compute \\, the \\, probability \\, of \\, a \\, given \\, next \\,)\n$$\n\n---\n\nOne such approach could be to use **relative frequency counts** to compute this probability, i.e. ,**Out of the times we saw the history $h$, how many times was it followed by the word $w$**\n\nOr \n\n$$\nP ( \\, word \\, | \\, One \\, could \\, say \\, that \\, we \\, can \\, compute \\, the \\, probability \\, of \\, a \\, given \\, next \\,) = \\frac{C(\\, One \\, could \\, say \\, that \\, we \\, can \\, compute \\, the \\, probability \\, of \\, a \\, given \\, next \\, word)}{C(\\, One \\, could \\, say \\, that \\, we \\, can \\, compute \\, the \\, probability \\, of \\, a \\, given \\, next \\,)}\n$$\n---\n\nIntuitively it seems infeasible to perform this over an entire corpus; especially it is of a significant a size. This is the motivation behind the N-gram model, instead of using the entire corpus, we approximate this probability using just `n` previous words.\n\nFor instance if $w_{1:n}$ represents the sequence of words $w_1w_2...w_n$, then using the chain rule of probability we can write,  \n\n\n$$\\large\nP(w_{1:n}) = P(w_1)P(w_2 | w_1)P(w_3 | w_{1:2})...P(w_n|w_{1:n-1})\n$$\n\n\n$$\\large\nP(w_{1:n}) = \\prod_{k=1}^{n}P(w_k | w_{1:k-1})\n$$\n\n<a id=\"bigram-model\"><\/a>\n## The Bigram Model \u2461\n\nA Bigram Model corresponds to a model which approximates the probability of a word given all the previous words $P(w_n|w_{1:n\u22121})$ by using only the conditional probability of the preceding word $P(w_n|w_{n\u22121})$. Thus we assume that $P(w_n|w_{1:n\u22121}) \u2248 P(w_n|w_{n\u22121})$. This approximation is known as the **Markov** approximation. Thus, for the Bigram model, the probability for an entire sequence can be approximated as:\n\n$$\\large\nP(w_{1:n}) \u2248 \\prod_{k=1}^{n}P(w_{k}|w_{k\u22121}) \n$$\n\n<a id=\"estimation\"><\/a>\n## Estimation \u2a70\n\nTo estimate such probabilities we use the **Maximum Likelihood Estimation (MLE)**. An MLE estimate for the parameters of an n-gram model can be obtained by getting counts from a corpus, and normalizing the counts so that they lie between 0 and 1.\n\nFor a Bigram model, the MLE Estimation can be given by:\n\n$$\\large\nP(w_n | w_{n-1}) \\frac{C(w_{n-1}w_n)}{\\sum_{w} C(w_{n-1}w)}\n$$\n\n---\nFor the general case of MLE n-gram parameter estimation:\n\n$$\\large\nP(w_n|w_{n\u2212N+1:n\u22121}) = \\frac{C(w_{n\u2212N+1:n\u22121}w_n)}{C(w_{n\u2212N+1:n\u22121})}\n$$","5e08833c":"<a id=\"misc\"><\/a>\n# \ud83e\uddd0 Miscellaneous ","1f656133":"Let's see how many n-grams we have in our corpus.","d2f2e729":"<a id=\"inference\"><\/a>\n# \ud83d\ude0a Inference","91a55014":"Here, we create a list of n-gram counts for a arbitrary range `(1,6)`","4edbe3fb":"<a id=\"auto-complete\"><\/a>\n# \ud83d\udcac The Auto-Complete System","4b80a56f":"Finally, we build our `auto_complete` fn. We simply loop over all the words in the vocabulary assuming that they can be the next word and then return the word with it's probability. ","c178e7d1":"Let's give it a sample input of \"i was about\" in a tokenized manner and get multiple suggestions using the above calculated n-gram counts with smoothing-factor, `k` = 1.0 ","3ad8cfec":"<a id=\"final\"><\/a>\n## \ud83e\uddfc Final Cleaning Pipeline","3a6ba695":"<a id=\"unk\"><\/a>\n## \ud83e\udd37\ud83c\udffb Adding UNK Tokens","65b77d6d":"<a id=\"pre-process\"><\/a>\n# \ud83e\uddfd Pre-Processing pipeline","2105cc83":"One of the most essential steps in dealing with Textual data is handling Out-of-vocabulary words. This helps the model to handle words which are not present in the training corpus. First step in this process is to create a `closed_vocabulary`. This function creates a closed vocabulary containing only those words according to the `count_threshold` parameter.","6e5ef139":"<a id=\"split\"><\/a>\n# \u2702\ufe0f Splitting into Train, Valid and Test","5423de27":"In this function we'll add `<unk>` tokens, to those words which are not in the `closed_vocabulary` which we just made.","cdfbf8f5":"As our dataset is quite big, we'll only use those words that appear `k` times in our dataset. In this function, we'll create a frequency dictionary for our vocabulary. ","d7156357":"We create a simple pipeline function which: \n\n* splits the datasets by the `\\n` character \n\n* remove leading and trailing spaces \n\n* drop empty sentences. \n\n* Tokenize sentences using `nltk.word_tokenize`","26bbbba7":"<a id=\"clean\"><\/a>\n# \ud83e\uddf9 Cleaning the Data","d910dc4d":"# Table of Content\n\n* [\ud83d\udcda Theory](#section-one)\n    * [The Bigram Model \u2461](#bigram-model)\n    * [Estimation \u2a70](#estimation)\n* [\ud83d\udcc2 Basic Setup](#basic-setup)\n* [\ud83e\uddfd Pre-Processing pipeline](#pre-process)\n* [\u2702\ufe0f Splitting into Train, Valid and Test](#split)\n* [\ud83e\uddf9 Cleaning the Data](#clean)\n    * [\ud83d\udcd4 Creating a Frequency Dictionary](#frequency)\n    * [\ud83d\udd12 Creating a Closed Vocabulary](#closed) \n    * [\ud83e\udd37\ud83c\udffb Adding UNK Tokens](#unk)\n    * [\ud83e\uddfc Final Cleaning Pipeline](#final)\n* [\ud83d\udcaa\ud83c\udffb Building The \"Model\"](#build)\n* [\ud83d\udcac The Auto-Complete System](#auto-complete)\n* [\ud83d\ude0a Inference](#inference)\n* [\ud83e\uddd0 Miscellaneous](#misc)","a5fca421":"In this section, we just export this list to a `.txt` file so that we can use this for inference rather than \"training\" each time.","9ad27688":"Now, we loop over all the words in the vocabulary and then compute their probabilites using our `prob_for_single_word()` fn.","fc06b03a":"This kernel aims to expand on what I've learned from the deeplearning.ai NLP Specialisation, one of the assignments for that course used **n-gram models** to build a auto-completion program. Most of this kernel is ported from that assignment. The dataset used for this kernel is very similar to the one used for the assignment.\n\nThe aim is to convert this into a web application and deploy it to Heroku using streamlit. The Streamlit App can be found [here](https:\/\/autocomplete-ngram.herokuapp.com\/) Takes a while to load \ud83d\ude05. Below is the screenshot of how the Application looks like. [Link to the Github repository](https:\/\/github.com\/SauravMaheshkar\/Auto-Completion-using-N-Gram-Models).\n\n![App Screenshot](https:\/\/raw.githubusercontent.com\/SauravMaheshkar\/Auto-Completion-using-N-Gram-Models\/master\/assets\/app.png)\n\n####  If you liked this project and would like to read the code and see some of my other work, don't forget to \u2b50 the [repository](https:\/\/github.com\/SauravMaheshkar\/Auto-Completion-using-N-Gram-Models) and follow [me](https:\/\/github.com\/SauravMaheshkar).","e0fcc9ad":"<a id=\"basic-setup\"><\/a>\n# \ud83d\udcc2 Basic Setup","475b6319":"<a id=\"closed\"><\/a>\n## \ud83d\udd12 Creating a Closed Vocabulary","69a4f9b9":"We can also loop over all the various n-gram models to get multiple suggestions. This function just extends from the previously defined function by **taking multiple n-gram counts** instead of one. This allows us to take unigram, bigram, .. counts into account as well.","743ea230":"<a id=\"frequency\"><\/a>\n## \ud83d\udcd4 Creating a Frequency Dictionary","91a8ec2a":"<a id=\"build\"><\/a>\n# \ud83d\udcaa\ud83c\udffb Building The \"Model\"","18b9fe95":"This function calculates the priority for the next word given the prior n-gram. This function also implements k-smoothing which helps account for unseen n-grams. Using the previously defined formula:\n\n\n$$\\large\nP(w_n|w_{n\u2212N+1:n\u22121}) = \\frac{C(w_{n\u2212N+1:n\u22121}w_n)}{C(w_{n\u2212N+1:n\u22121})}\n$$\n\n---\n\n### K-smoothing\n\nBut what if we come across a n-gram that wasn't in the training set. Then our denominator would would become zero and our definition of probability will become invalid. Thus, we use k-smoothing, which adds a positive constant $k$ to each numerator and $k \\times |V|$ in the denominator, where $|V|$ is the number of words in the vocabulary. This ensures any n-gram with zero count has the same probability of $\\frac{1}{|V|}$. Thus, our original estimation get's modified to:\n\n$$\\large\nP(w_n|w_{n\u2212N+1:n\u22121}) = \\frac{C(w_{n\u2212N+1:n\u22121}w_n) + k}{C(w_{n\u2212N+1:n\u22121} + k |V|)}\n$$","9c9a839c":"This is a helper function, which will come in handy during inference. This function returns a mapping from n-grams to their frequency in the dataset. ","d192672b":"In this **hidden** code cell, we'll import our packages. As we'll implement n-gram models from scratch we'll just use numpy and Additionally nltk  (just for Tokenization)."}}