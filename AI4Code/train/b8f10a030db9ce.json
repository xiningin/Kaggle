{"cell_type":{"f8b4e1de":"code","01e3ebd5":"code","99ac2d65":"code","179b7298":"code","b1c36881":"code","98a5117e":"code","d618f07a":"code","3faadb98":"code","228471a7":"code","ab6c9299":"code","f83b4228":"code","fd355776":"code","89544c78":"markdown","d9ed7940":"markdown","1bed0517":"markdown","b6970db6":"markdown","2d12d851":"markdown","2eeb10ec":"markdown","46bf5bda":"markdown","463f63ce":"markdown","0de133aa":"markdown"},"source":{"f8b4e1de":"# Global variables for testing changes to this notebook quickly\nNUM_FOLDS = 8  \nRANDOM_SEED = 0\nNUM_TREES = 2000\nEARLY_STOP = 50\nSUBMIT = True","01e3ebd5":"# Essentials\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nimport pyarrow\nimport pickle\nimport time\nimport matplotlib\nfrom matplotlib import pyplot as plt\n\n# Models and Evaluation\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBClassifier, plot_importance\n\n# Hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')","99ac2d65":"%%time\n# Load data\ntrain = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\n\n# Drop ID\ntrain.drop('id', axis = 'columns', inplace = True)\ntest.drop('id', axis = 'columns', inplace = True)\n\n# Downcast training data\nfor col, dtype in train.dtypes.iteritems():\n    if dtype.name.startswith('int'):\n        train[col] = pd.to_numeric(train[col], downcast ='integer')\n    elif dtype.name.startswith('float'):\n        train[col] = pd.to_numeric(train[col], downcast ='float')\n\n# Downcast test data\nfor col, dtype in test.dtypes.iteritems():\n    if dtype.name.startswith('int'):\n        test[col] = pd.to_numeric(test[col], downcast ='integer')\n    elif dtype.name.startswith('float'):\n        test[col] = pd.to_numeric(test[col], downcast ='float')\n\n# Feature columns\nfeatures = [x for x in train.columns if x not in ['id','claim']]","179b7298":"def score_xgboost():\n\n    # Vectors to store predictions\/scores\n    test_preds, oof_preds = np.zeros((test.shape[0],)), np.zeros((train.shape[0],))\n    fi_scores, scores = np.zeros(len(features)), np.zeros(NUM_FOLDS)\n\n    # Stratified k-fold cross-validation\n    skf = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state = RANDOM_SEED)\n    for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train[\"claim\"])):\n       \n        # Training and Validation Sets\n        X_train, X_valid = train[features].iloc[train_idx], train[features].iloc[valid_idx]\n        y_train, y_valid = train[\"claim\"].iloc[train_idx], train[\"claim\"].iloc[valid_idx]\n\n        start = time.time()\n\n        # Define model\n        model = XGBClassifier(\n            random_state = RANDOM_SEED,\n            n_estimators = NUM_TREES,\n            tree_method='gpu_hist',\n            gpu_id=0,\n            predictor=\"gpu_predictor\",\n        )\n\n        # Train model\n        model.fit(\n                X_train, y_train,\n                verbose = False,\n                eval_set = [(X_train, y_train), (X_valid, y_valid)],\n                eval_metric = [\"auc\",\"logloss\"],\n                early_stopping_rounds = EARLY_STOP\n        )\n\n        # Get predictions\n        valid_preds = model.predict_proba(X_valid)[:,1]\n        test_preds += model.predict_proba(test[features])[:, 1] \/ NUM_FOLDS\n        fi_scores += model.feature_importances_ \/ NUM_FOLDS\n        oof_preds[valid_idx] = valid_preds\n        scores[fold] = roc_auc_score(y_valid, valid_preds)\n        end = time.time()\n        print(f'Fold {fold} (AUC): {round(scores[fold], 6)} in {round(end-start, 3)}s')\n\n    print(\"\\nAverage AUC:\", round(scores.mean(), 6))\n    print(\"Worst AUC:\", round(scores.min(), 6))\n    return model, scores.mean(), oof_preds, test_preds, fi_scores","b1c36881":"def training_plot(xgb_model):\n    # Get model evaluation results\n    results = xgb_model.evals_result()\n    num_iter = len(results['validation_0']['auc'])\n    x_axis = range(0, num_iter)\n\n    # Plot training curve\n    fig, ax = plt.subplots(figsize = (9,6))\n    ax.plot(x_axis, results['validation_0']['auc'], label='Train')\n    ax.plot(x_axis, results['validation_1']['auc'], label='Valid')\n    plt.axvline(x=xgb_model.best_iteration, color='k', linestyle='--')\n    ax.legend()\n    plt.ylabel('AUC')\n    plt.xlabel('Iterations')\n    plt.title('XGBoost AUC')\n    plt.grid(True)\n    plt.show()","98a5117e":"model, score, oof_preds, test_preds, fi_scores = score_xgboost()\n\nsubmission['claim'] = test_preds\nsubmission.to_csv('baseline_submission.csv', index=False)","d618f07a":"training_plot(model)","3faadb98":"def create_row_stats(data):\n    data['nan_count'] = data[features].isnull().sum(axis=1)\n    data['nan_std'] = data[features].isnull().std(axis=1)\n    data['min'] = data[features].min(axis=1)\n    data['std'] = data[features].std(axis=1)\n    data['max'] = data[features].max(axis=1)\n    data['median'] = data[features].median(axis=1)\n    data['mean'] = data[features].mean(axis=1)\n    data['var'] = data[features].var(axis=1)\n    data['sum'] = data[features].sum(axis=1)\n    data['sem'] = data[features].sem(axis=1)\n    data['skew'] = data[features].skew(axis=1)\n    data['median_abs_dev'] = stats.median_abs_deviation(data[features], axis=1)\n    data['zscore'] = (np.abs(stats.zscore(data[features]))).sum(axis=1)\n    return data","228471a7":"# Create new features\ntrain = create_row_stats(train)\ntest = create_row_stats(test)\nfeatures = [x for x in train.columns if x not in ['id','claim']]\n\nmodel, score, oof_preds, test_preds, fi_scores = score_xgboost()\n\nfi_scores = pd.Series(\n    data = fi_scores, \n    index = features\n).sort_values(ascending = False)","ab6c9299":"# Most important features\nfi_scores.head(5)","f83b4228":"# Least important features\nfi_scores.tail(5)","fd355776":"# Remove low importance features\ntrain.drop(['nan_std','var','zscore','std'], axis = 'columns', inplace = True)\ntest.drop(['nan_std','var','zscore','std'], axis = 'columns', inplace = True)\nfeatures = [x for x in test.columns]\n\nmodel, score, oof_preds, test_preds, fi_scores = score_xgboost()\n\nsubmission['claim'] = test_preds\nsubmission.to_csv('best_features_submission.csv', index=False)","89544c78":"# XGBoost Baseline","d9ed7940":"# Simple XGBoost Models\n\nIn this notebook we get a baseline for the GPU-enabled XGBoost model. We mostly use default settings except for the following:\n\n* We set a higher value for `n_estimators`, a lower value for `learning_rate` and enable `early_stopping_rounds` to avoid overfitting on each fold\n\nWe use [this dataset](https:\/\/www.kaggle.com\/rsizem2\/tps0921foldsfeather), which is equivalent to the original data except we have a predefined cross-validation scheme and each feature has been downcast to it's lowest possible subtype.","1bed0517":"## Feature Importances\n\nWe look at the most important and least important features as determined by XGBoost","b6970db6":"### 2. Training Plot","2d12d851":"## Train Model","2eeb10ec":"# Final Submission\n\nWe use only the features which XGBoost found more important than any of the original features.\n\n## Remove Low Importance Features","46bf5bda":"## Load Data","463f63ce":"## Helper Functions\n\n### 1. Scoring Function","0de133aa":"# Feature Engineering"}}