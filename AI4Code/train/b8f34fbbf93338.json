{"cell_type":{"6ba4d7d3":"code","ff31f1f7":"code","8cdb8cf3":"code","ea6ad376":"code","e3c3af13":"code","6ec0d9be":"code","4863500a":"code","b11ba5e3":"code","2121f809":"code","1904510d":"code","ba15e148":"code","570ac540":"code","4a0f7f35":"code","59858330":"code","ac3c8f5f":"markdown"},"source":{"6ba4d7d3":"# import libs\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport gc\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy.matlib\n\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\n\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom joblib import Parallel, delayed\n\nimport shutil\nimport glob\n\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\nfrom sklearn.metrics import r2_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import KFold\n\n\nimport torch\nfrom torch.optim import Adam, SGD\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import r2_score\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\n\n\npath_submissions = '\/'\n\ntarget_name = 'target'\nscores_folds = {}","ff31f1f7":"def transform_csv2pickle(path, usecols, dtype):\n    train = pd.read_csv(\n        path,\n        usecols=usecols,\n        dtype=dtypes\n    )\n    train.to_pickle('train.pkl')\n\n\npath = '..\/input\/ubiquant-market-prediction\/train.csv'\n\nbasecols = ['row_id', 'time_id', 'investment_id', 'target']\nfeatures = [f'f_{i}' for i in range(300)]\n\ndtypes = {\n    'row_id': 'str',\n    'time_id': 'uint16',\n    'investment_id': 'uint16',\n    'target': 'float32',\n}\nfor col in features:\n    dtypes[col] = 'float32'","8cdb8cf3":"%%time\ntrain = pd.read_pickle('..\/input\/ump-train-picklefile\/train.pkl')","ea6ad376":"train = train[train.time_id>1100]\ntrain","e3c3af13":"# kfold based on the knn++ algorithm\n\nout_train = train.pivot(index='time_id', columns='investment_id', values='target')\n\nout_train = out_train.fillna(out_train.mean())\nout_train.head()","6ec0d9be":"from numpy.random import seed\nseed(42)\nimport tensorflow as tf\ntf.random.set_seed(42)\nfrom tensorflow import keras\nimport numpy as np\nfrom keras import backend as K\nfrom keras.models import load_model\n# data separation based on knn ++\nnfolds = 5 # number of folds\nindex = []\ntotDist = []\nvalues = []\n# generates a matriz with the values of \nmat = out_train.values\n\nscaler = MinMaxScaler(feature_range=(-1, 1))\nmat = scaler.fit_transform(mat)\n\nnind = int(mat.shape[0]\/nfolds) # number of individuals\n\n# adds index in the last column\nmat = np.c_[mat,np.arange(mat.shape[0])]\n\n\nlineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\n\nlineNumber = np.sort(lineNumber)[::-1]\n\nfor n in range(nfolds):\n    totDist.append(np.zeros(mat.shape[0]-nfolds))\n    \n# saves index\nfor n in range(nfolds):\n    \n    values.append([lineNumber[n]])    \n\n\ns=[]\nfor n in range(nfolds):\n    s.append(mat[lineNumber[n],:])\n    \n    mat = np.delete(mat, obj=lineNumber[n], axis=0)\n\nfor n in range(nind-1):    \n\n    luck = np.random.uniform(0,1,nfolds)\n    \n    for cycle in range(nfolds):\n         # saves the values of index           \n\n        s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n\n        sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n        totDist[cycle] += sumDist        \n                \n        # probabilities\n        f = totDist[cycle]\/np.sum(totDist[cycle]) # normalizing the totdist\n        j = 0\n        kn = 0\n        for val in f:\n            j += val        \n            if (j > luck[cycle]): # the column was selected\n                break\n            kn +=1\n        lineNumber[cycle] = kn\n        \n        # delete line of the value added    \n        for n_iter in range(nfolds):\n            \n            totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n            j= 0\n        \n        s[cycle] = mat[lineNumber[cycle],:]\n        values[cycle].append(int(mat[lineNumber[cycle],-1]))\n        mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n\n\nfor n_mod in range(nfolds):\n    values[n_mod] = out_train.index[values[n_mod]]","4863500a":"es = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=10, verbose=0,\n    mode='min',restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.1, patience=5, verbose=0,\n    mode='min')","b11ba5e3":"colNames = list(train)\n\ncolNames.remove('time_id')\ncolNames.remove('target')\ncolNames.remove('row_id')\ncolNames.remove('investment_id')","2121f809":"test = pd.read_csv('..\/input\/ubiquant-market-prediction\/example_test.csv')","1904510d":"#https:\/\/bignerdranch.com\/blog\/implementing-swish-activation-function-in-keras\/\nfrom keras.backend import sigmoid\ndef swish(x, beta = 1):\n    return (x * sigmoid(beta * x))\n\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Activation\nget_custom_objects().update({'swish': Activation(swish)})","ba15e148":"hidden_units = (128,64,32)\nembedding_size = 24\n\ncat_data = train['investment_id']\n\ndef base_model():\n    inv_id_input = keras.Input(shape=(1,), name='investment_id')\n    num_input = keras.Input(shape=(len(colNames),), name='num_data')\n\n\n    #embedding, flatenning and concatenating\n    inv_embedded = keras.layers.Embedding(max(cat_data)+1, embedding_size, \n                                           input_length=1, name='investment_embedding')(inv_id_input)\n    inv_flattened = keras.layers.Flatten()(inv_embedded)\n    out = keras.layers.Concatenate()([inv_flattened, num_input])\n\n\n    # Add one or more hidden layers\n    out = keras.layers.Dense(4096, activation='relu')(out)\n    out = keras.layers.Reshape((256, 16))(out)\n    out = keras.layers.Conv1D(filters=16, kernel_size=5, strides=1, activation='relu')(out)\n    out = keras.layers.MaxPooling1D(pool_size=2)(out)\n    out = keras.layers.Flatten()(out)\n\n    #out = keras.layers.Concatenate()([out, num_input])\n    out = keras.layers.Dense(16, activation='relu')(out)\n    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n    \n    \n    model = keras.Model(\n    inputs = [inv_id_input, num_input],\n    outputs = out,\n    )\n    \n    return model","570ac540":"model_name = 'NN'\npred_name = 'pred_{}'.format(model_name)\n\nn_folds = 5\nkf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=42)\nscores_folds[model_name] = []\ncounter = 1\n\nfeatures_to_consider = list(train)\n\nfeatures_to_consider.remove('time_id')\nfeatures_to_consider.remove('target')\nfeatures_to_consider.remove('row_id')\ntry:\n    features_to_consider.remove('pred_NN')\nexcept:\n    pass\n\n\ntrain[features_to_consider] = train[features_to_consider].fillna(train[features_to_consider].mean())\ntest[features_to_consider] = test[features_to_consider].fillna(test[features_to_consider].mean())\n\ntrain[pred_name] = 0\ntest['target'] = 0\n\n\nfor n_count in range(n_folds):\n    print('CV {}\/{}'.format(counter, n_folds))\n    \n    indexes = np.arange(nfolds).astype(int)    \n    indexes = np.delete(indexes,obj=n_count, axis=0) \n    \n    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n    \n    X_train = train.loc[train.time_id.isin(indexes), features_to_consider]\n    y_train = train.loc[train.time_id.isin(indexes), target_name]\n    X_test = train.loc[train.time_id.isin(values[n_count]), features_to_consider]\n    y_test = train.loc[train.time_id.isin(values[n_count]), target_name]\n    \n    \n    model = base_model()\n    model.summary()\n    model.compile(\n        keras.optimizers.Adam(learning_rate=0.0001),\n        loss='mse'\n    )\n    \n    try:\n        features_to_consider.remove('investment_id')\n    except:\n        pass\n    \n    num_data = X_train[features_to_consider]\n    \n    scaler = MinMaxScaler(feature_range=(-1, 1))         \n    num_data = scaler.fit_transform(num_data.values)    \n    \n    cat_data = X_train['investment_id']    \n    target =  y_train\n    \n    num_data_test = X_test[features_to_consider]\n    num_data_test = scaler.transform(num_data_test.values)\n    cat_data_test = X_test['investment_id']\n\n    model.fit([cat_data, num_data], \n              target,               \n              batch_size=1024,\n              epochs=100,\n              validation_data=([cat_data_test, num_data_test], y_test),\n              callbacks=[es, plateau],\n              validation_batch_size=len(y_test),\n              shuffle=True,\n             verbose = 1)\n\n    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n    print('score:',pd.DataFrame({\"y_train\":y_test, \"oof_train\": preds}).corr()[\"y_train\"][\"oof_train\"])\n    \n    tt =scaler.transform(test[features_to_consider].values)\n    test[target_name] += model.predict([test['investment_id'], tt]).reshape(1,-1)[0].clip(-1.2e2,1.2e2)\/100\n    model.save(f'nn_{n_count}.h5')\n    counter += 1\n    features_to_consider.append('investment_id')\n    del model\n    gc.collect()","4a0f7f35":"from keras.models import load_model\ntest = pd.read_csv('..\/input\/ubiquant-market-prediction\/example_test.csv')\nfeatures_to_consider = test.drop(['row_id','time_id','investment_id'],axis=1).columns\nnn0 = load_model('.\/nn_0.h5')\nnn1 = load_model('.\/nn_1.h5')\nnn2 = load_model('.\/nn_2.h5')\nnn3 = load_model('.\/nn_3.h5')\nnn4 = load_model('.\/nn_4.h5')\n","59858330":"# inference\nimport ubiquant\nenv = ubiquant.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\nfor (test_df, sample_prediction_df) in iter_test:\n    test_df = test_df.drop([\"row_id\"], axis=1)\n    tt =scaler.transform(test_df[features_to_consider].values)\n    pred_nn0 = nn0.predict([test_df['investment_id'],tt]).reshape(1,-1)[0].clip(-1.2e2,1.2e2)\/100\n    pred_nn1 = nn1.predict([test_df['investment_id'],tt]).reshape(1,-1)[0].clip(-1.2e2,1.2e2)\/100\n    pred_nn2 = nn2.predict([test_df['investment_id'],tt]).reshape(1,-1)[0].clip(-1.2e2,1.2e2)\/100\n    pred_nn3 = nn3.predict([test_df['investment_id'],tt]).reshape(1,-1)[0].clip(-1.2e2,1.2e2)\/100\n    pred_nn4 = nn4.predict([test_df['investment_id'],tt]).reshape(1,-1)[0].clip(-1.2e2,1.2e2)\/100\n\n    sample_prediction_df['target'] =  (pred_nn0 + pred_nn1 + pred_nn2 + pred_nn3 + pred_nn4) \/ 5  # make your predictions here\n    env.predict(sample_prediction_df)   # register your predictions","ac3c8f5f":"# NN"}}