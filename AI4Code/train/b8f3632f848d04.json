{"cell_type":{"5cb19bb6":"code","c4389f21":"code","81726687":"code","dec5291e":"code","2d6c9d0b":"code","1c598831":"code","7b9f0619":"code","2cf4d382":"code","f4f4dc18":"code","3b5c7377":"code","cacefdf3":"code","50712137":"code","28b96e2d":"code","8b4247ab":"code","c2b4d38f":"code","79efa45a":"code","5e0ae300":"markdown","c8effa4f":"markdown","d6075b53":"markdown","050120e8":"markdown","56abdb55":"markdown","c21a9515":"markdown","30c93e11":"markdown","c95a5248":"markdown","7bc85c18":"markdown"},"source":{"5cb19bb6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib as ml\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix,plot_confusion_matrix\n%matplotlib inline\nml.style.use('ggplot')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c4389f21":"data = pd.read_csv('\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv',encoding='latin-1')\nprint(data.shape)\ndata.head()","81726687":"data.info()","dec5291e":"cols = {'v1':'Label','v2':'Text'}\ndata = data.rename(columns=cols)\ndata","2d6c9d0b":"col3 = data['Unnamed: 2'].isnull().sum()\/data.shape[0]\ncol4 = data['Unnamed: 3'].isnull().sum()\/data.shape[0]\ncol5 = data['Unnamed: 4'].isnull().sum()\/data.shape[0]\n\nprint(\"Portion of NaN values in the 3rd column : \",(col3*100),\"%\")\nprint(\"Portion of NaN values in the 4th column : \",(col4*100),\"%\")\nprint(\"Portion of NaN values in the 5th column : \",(col5*100),\"%\")","1c598831":"data.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True)\ndata.head(10)","7b9f0619":"# Getting all words in all 5572 SMS's\ntexts = list(data.Text.values)\nwords = []\nfor i in texts:\n    words.extend(i.split(\" \"))\nlen(words)","2cf4d382":"# Replacing non-alphabetical instances\nfor i in range(len(words)):\n    if not words[i].isalpha():\n        words[i] = \"\"\n\n# Create a Counter dictionary to maintain ('word':count) tuples.\nword_dict = Counter(words)\nword_dict.most_common(1)     # Get the 1st most common word.","f4f4dc18":"del word_dict['']\n# Check\nword_dict.most_common(1)","3b5c7377":"# Getting the 3000 most common words in text messages (Why 3000 ? Hit and trial)\nw_new = word_dict.most_common(3000)\nw_new","cacefdf3":"features,columns = [],[]\n\n# Creating the columns(features)\nfor word in w_new:\n    columns.append(word[0])\n\n# Creating data\nfor s in texts:\n    d = []\n    for wrd in w_new:\n        d.append(s.count(wrd[0]))\n    features.append(d)\n\n# Create dataframe\ndf = pd.DataFrame(features,columns=columns)\ndf","50712137":"df['label'] = 0\ndf","28b96e2d":"labels = {'spam':1,'ham':0}             # Spam = 1, Not spam = 0\nfor i in range(df.shape[0]):\n    df.iloc[i,3000] = int(labels[data.iloc[i,0]])\ndf","8b4247ab":"# X = df.iloc[:,:3000].values\nX = np.array(features)\n# Y = df['label'].values\nY = np.array(df.label)\n\ntrain_x,test_x,train_y,test_y = train_test_split(X,Y,test_size=0.7,random_state=10)\nprint(\"Train X size = \",train_x.shape,\", Test X size = \",test_x.shape)\nprint(\"Train Y size = \",train_y.shape,\", Test Y size = \",test_y.shape)","c2b4d38f":"# Defining the Naive Bayes classifier\nmnb = MultinomialNB(alpha = 5, fit_prior = True)\n# Training the model\nmnb.fit(train_x,train_y)","79efa45a":"y_pred = mnb.predict(test_x)\nprint(confusion_matrix(test_y,y_pred,labels=[1,0],normalize=None))\ntn,fp,fn,tp = confusion_matrix(test_y,y_pred).ravel()\nprint(\"\\nTP = \",tp,\", FP = \",fp,\", FN = \",fn,\", TN = \",tn)\nprint(\"Confirmation : TP + FP + FN + TN = \",tp+fp+fn+tn)\nprint(\"Equal to test_y size(3901)\")\n# Accuracy\nprint(\"\\nAccuracy = \",(tp+tn)*100\/(tp+tn+fp+fn),\"%\")","5e0ae300":"##### Seems alright. Now we form our features(word counts for each message)\n\n## FEATURE ENGINEERING","c8effa4f":"## TRAINING THE MODEL","d6075b53":"##### This shouldn't be the case. The most common word can't be a ' '. So we remmove the occurrence of ' '(unwanted character) in the word_dict","050120e8":"## PREDICTIONS AND CONFUSION MATRIX\n##### Confusion matrix to get an idea of the accuracy of the model. X-axis = Predicted label. Y-axis = True label.","56abdb55":"## DROPPING 'TEXT' AND ASSIGNING LABELS MANUALLY\n\n##### 1. Since the label is of two types only ('spam' or 'ham'), we can manually encode them instead of using a LabelEncoder() object.\n##### 2. We do not need the text messages feature anymore, because we have already got the count of 3000 most popular words in each one of them. So it is not merged","c21a9515":"##### We see a huge magnitude of the data is NULL in the last 3 columns. So we drop the last 3 columns","30c93e11":"### RENAMING COLUMNS","c95a5248":"### FIND RATIO OF NULL VALUES IN LAST 3 COLUMNS TO DECIDE WHETHER TO KEEP OR DROP","7bc85c18":"## FILTERING OUT THE COMMON WORDS AND FORMING A FREQUENCY DATASET."}}