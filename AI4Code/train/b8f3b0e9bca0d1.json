{"cell_type":{"1d0a7157":"code","ee9eac7f":"code","33386531":"code","44c81fcd":"code","20f5093c":"code","a3058c14":"code","9018e1b0":"code","6142c973":"code","34ee318b":"code","742d82dc":"code","1c78420a":"code","01e0232f":"code","a741a210":"code","348f1965":"code","c7522440":"code","4f0ce7f1":"code","45c70e51":"markdown","a167eaca":"markdown","9540f6da":"markdown","48b490dc":"markdown","dc2df2a3":"markdown","22b81f8a":"markdown","b55f5604":"markdown","77417f98":"markdown","acb652d7":"markdown"},"source":{"1d0a7157":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\ndf = pd.read_csv ('..\/input\/unsw-nb15\/UNSW_NB15_training-set.csv')\n","ee9eac7f":"X = df.copy()\nY = X.label\n\nX\n\n\n","33386531":"X= X.drop([\"attack_cat\",\"id\",\"label\"],axis=1)\nX","44c81fcd":"string_fields = X.select_dtypes('object').columns.values\nX = pd.get_dummies(X, columns=string_fields)\n","20f5093c":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=1)\nprint(\"Train size = \"+str(len(X_train)) + \"Test_size = \"+str(len(X_test)))\n","a3058c14":"from xgboost import XGBClassifier, plot_importance, plot_tree\nfrom sklearn.metrics import roc_auc_score\n#average_precision_score\n\n\nxgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\nxgb_model.fit(X_train, y_train)\n","9018e1b0":"from sklearn.metrics import accuracy_score, precision_recall_curve, auc, confusion_matrix, classification_report, recall_score, roc_auc_score \ny_pred = xgb_model.predict (X_test)\npredictions = [round(value) for value in y_pred]\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred) \narea = auc(recall, precision)\nprint('------------ Results for XGBClassifier ---------------')\nprint('matrice de confusion:',confusion_matrix(y_test, y_pred))\n#print('cr:', classification_report(y_test,y_pred))\n#print('recall_score:', recall_score(y_test,y_pred))\nprint('roc_auc_score:',roc_auc_score(y_test,y_pred))\nprint(\"Area Under P-R Curve: \",area)","6142c973":"from xgboost import plot_importance\nfrom matplotlib import pyplot\n# plot feature importance\nplot_importance(xgb_model)\npyplot.show()","34ee318b":"for importance_type in ('weight', 'gain', 'cover'):\n    print('%s: ' % importance_type, xgb_model.get_booster().get_score(importance_type=importance_type))\n","742d82dc":"from xgboost import plot_tree\nimport matplotlib.pyplot as plt\nplot_tree(xgb_model)\nplt.show()\n\n","1c78420a":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nx = StandardScaler().fit_transform(X.copy())\npca = PCA(n_components=2)\npca_x = pca.fit_transform(x)\n\nprincipalDf = pd.DataFrame(data = pca_x, columns = ['principal component 1', 'principal component 2'])\nfinalDf = pd.concat([principalDf, Y], axis = 1)","01e0232f":"fig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1)\nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\ntargets = [0,1]\ncolors = ['r', 'b']\nfor target, color in zip(targets, colors):\n    indicesToKeep = finalDf.label == target\n    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1'],\n               finalDf.loc[indicesToKeep, 'principal component 2'],\n               c = color,\n               s = 50)\n    ax.legend(targets)\n    ax.grid()","a741a210":"print(\"Independant parameters = \"+str(pca.explained_variance_))","348f1965":"from sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(pca_x, Y)\nY_pca = reg.predict(pca_x)\nY_pca - Y","c7522440":"from sklearn.metrics import accuracy_score, precision_recall_curve, auc, confusion_matrix, classification_report, recall_score, roc_auc_score \ny_pred = xgb_model.predict (X_test)\npredictions = [round(value) for value in y_pred]\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred) \narea = auc(recall, precision)\nprint('------------ Results for XGBClassifier ---------------')\nprint('matrice de confusion:',confusion_matrix(y_test, y_pred))\nprint('cr:', classification_report(y_test,y_pred))\nprint('recall_score:', recall_score(y_test,y_pred))\nprint('roc_auc_score:',roc_auc_score(y_test,y_pred))\n","4f0ce7f1":"from matplotlib import pyplot\nresults = xgb_model.evals_result()\npyplot.plot(results['validation_0']['logloss'], label='train')\npyplot.plot(results['validation_1']['logloss'], label='test')\npyplot.legend()\npyplot.show()\n\n","45c70e51":"Statistics Analysis","a167eaca":"Train the model","9540f6da":"The plot shows learning curves for the train and test dataset where the x-axis is the number of iterations of the algorithm (or the number of trees added to the ensemble) and the y-axis is the logloss of the model. Each line shows the logloss per iteration for a given dataset.\n\nFrom the learning curves, we can see that the performance of the model on the training dataset (blue line) is better or has lower loss than the performance of the model on the test dataset (orange line), as we might generally expect","48b490dc":"Machine Learning Analysis","dc2df2a3":"AUPRC = 98.05%","22b81f8a":"Data Cleaning","b55f5604":"Evaluating and reporting","77417f98":"The model involved XGBoost performs very well in predicting attacks with a very low error rate below 2%. \nAs we could see on the graph related to the importance of the variables, it is the STTL variable and ct_dst_sport_ltm that have the most importance compared to our model, result confirmed by our binary tree, always with the same discriminating variables.","acb652d7":"We obtained a learning sample of size: 61749 and a test sample of size: 20583"}}