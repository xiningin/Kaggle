{"cell_type":{"86d061f6":"code","65d2cca9":"code","e66c5131":"code","f7583727":"code","3e6d404f":"code","821e8bda":"code","b75d3245":"code","b1118ee1":"code","39db7908":"code","72b731ee":"code","c766db31":"code","6cde08f5":"code","a435ac30":"code","c7613669":"code","2e0cf26f":"code","0082fca5":"code","4a39163f":"markdown"},"source":{"86d061f6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","65d2cca9":"data_train=pd.read_csv('..\/input\/train.csv')\nprint('training shape is:',data_train.shape)\ndata_test=pd.read_csv('..\/input\/test.csv')\nprint('test shape is:',data_test.shape)\ncols=list(data_train.columns)\nprint(cols)","e66c5131":"string_cols = []\nfor col in cols: \n    col_meaning = data_train[~data_train[col].isna()][col].tolist()\n    type_col = type(col_meaning[0]).__name__ \n    if type_col == 'str' or type_col == 'object':\n        string_cols.append(col)","f7583727":"'Alley' in string_cols","3e6d404f":"data_train['flag'] = 'train'\ndata_test['flag'] = 'test'\ndata = pd.concat([data_train,data_test])","821e8bda":"def add_dummies(data):\n    small_data = pd.DataFrame()\n    for col in string_cols:\n        curr_data = pd.get_dummies(data[col],prefix = col)\n        data = data.drop(col,axis = 1)\n        small_data = pd.concat([small_data,curr_data], axis = 1)\n        print(col,small_data.shape)\n    data = pd.concat([data,small_data],axis = 1)\n    return data\ndata = add_dummies(data)","b75d3245":"cols = list(data.columns)\nfor col in cols:\n    if type(data[col][0]).__name__ == 'str':\n        print(col)\ndata.dtypes","b1118ee1":"data_train = data[data['flag']=='train']\ndata_test = data[data['flag'] == 'test']\ndata_train = data_train.drop(['flag'],axis = 1)\ndata_test = data_test.drop(['flag'],axis = 1)\n\nprint(data_train.shape)\nprint(data_test.shape)\ncols_train = set(data_train.columns)\ncols_test = set(data_test.columns)\n#print(data_train.columns)\n#print(data_test.columns)\nprint(len(cols_train.intersection(cols_test)))","39db7908":"cols = list(data_train.columns)\n#for col in cols:\n#    print(col)\nnum_cols = ['TotRmsAbvGrd','3SsnPorch','BedroomAbvGr','GarageArea',\n            'GrLivArea','LotArea','GarageYrBlt','PoolArea']\nfor col in num_cols:\n    data_train[col] = data_train[col].apply(lambda x: np.log(x+1))\nfor col in num_cols:\n    print(data_train[col].describe())","72b731ee":"data_train = data_train[~(data_train['SalePrice'].isna())]\nX_train = data_train.drop('SalePrice',axis = 1)\nY_train = data_train['SalePrice']\nX_test = data_test","c766db31":"#linear regression\nfrom sklearn.ensemble import RandomForestRegressor as rfreg\nfrom sklearn.metrics import mean_squared_error as rmse_1\nfrom sklearn.impute import SimpleImputer\nmy_imputer=SimpleImputer()\n\nX_train=my_imputer.fit_transform(X_train)\nX_test=my_imputer.fit_transform(X_test)\n\nlinreg=rfreg(n_estimators = 1000, max_depth = 14,min_samples_split = 15, max_features = 145,\n             oob_score=True,n_jobs=-1)\nfitted_model = linreg.fit(X_train,Y_train)\npredictions=fitted_model.predict(X_test)\n","6cde08f5":"from sklearn.model_selection import RandomizedSearchCV\nrandomforest = rfreg(oob_score=True,n_jobs=-1)\ndistributions = dict(n_estimators = [100,200,500,1000], \n                     max_depth = [8,10,14,16,20],\n                     min_samples_split = [5,15,30], \n                     max_features = [17,50,80,145],\n                     )\nclf = RandomizedSearchCV(randomforest, distributions, random_state=0)\nsearch = clf.fit(X_train,Y_train)\nprint(search.best_params_)","a435ac30":"#fitted_params = {'n_estimators': 200, 'min_samples_split': 5, 'max_features': 50, 'max_depth': 14}\nfitted_model = rfreg(n_estimators = 1000, \n                     max_depth = 4,\n                     min_samples_split = 30, \n                     max_features = 17,\n                     oob_score=True,\n                     n_jobs=-1)\nfitted_model.fit(X_train,Y_train)\npredictions = fitted_model.predict(X_test)","c7613669":"\npred_train = fitted_model.predict(X_train)\nprint(rmse_1(pred_train,Y_train)**0.5)\nfrom sklearn.metrics import r2_score as rs\nprint(rs(Y_train,pred_train))\nprint(fitted_model.oob_score_)","2e0cf26f":"Y_train.describe()","0082fca5":"predictions_table=pd.DataFrame()\npredictions_table['Id'] = [1461+i for i in range(1459)]\npredictions_table['SalePrice'] = predictions\npredictions_table.to_csv('vanilla_log_submission.csv',index = False)","4a39163f":"Hence we have successfully turned all the text columns into dummy, binary variables. Now, let's use different models."}}