{"cell_type":{"3c493420":"code","87b53867":"code","1d9c3008":"code","e8d1d0c2":"code","2bbd8360":"code","1c6dd56a":"code","e2fa2b46":"code","176586f3":"code","72530875":"code","88b8a0c1":"code","9841df91":"code","8831cb8a":"code","31e8a436":"code","21a27678":"code","a0d7ab37":"code","e5073f47":"code","f2550a07":"code","69690d0c":"code","0bd55cb4":"code","2242325a":"code","15275f30":"code","6165e663":"code","6409e31b":"code","4bc9774c":"code","6721e177":"code","a566bfed":"code","af3e98b2":"code","d1a026bf":"code","44392329":"code","21e33cca":"code","0315bc0c":"code","a7b80fcb":"code","86950555":"code","47d3759d":"code","886900f6":"code","f0a2f626":"code","c94e5eb0":"code","178828af":"code","66c68eb5":"code","36db1be0":"code","e1af23eb":"code","16c03ec3":"code","56369303":"code","ebc7d162":"code","396df28d":"code","eb343650":"code","820ef1d7":"code","708f8f5f":"code","4863dc96":"code","404c9e34":"code","133cdb3e":"code","df7170e4":"code","a525a620":"code","203b5a60":"code","2e03a989":"markdown","ef29e280":"markdown","d7f73269":"markdown","c8c6a943":"markdown","fd06e070":"markdown","beb6567d":"markdown","300a15e7":"markdown","42bc570d":"markdown","a0f89dce":"markdown","d5c8585c":"markdown","e940eb77":"markdown","77ce468f":"markdown","c0d81904":"markdown","a36a2b02":"markdown","e154f089":"markdown","4e99e146":"markdown","7c98ecea":"markdown","68024b8b":"markdown","8dbd6c50":"markdown","2ef9d7e8":"markdown","fd0aefb1":"markdown","e617f72b":"markdown","f849c4a8":"markdown","c33f1662":"markdown","f921a177":"markdown","ee9f710b":"markdown","dbe37fa9":"markdown","94407f9b":"markdown","73d2c6c5":"markdown","b1d1d379":"markdown","d7873146":"markdown","344aeef5":"markdown","d1b9d38d":"markdown","765eb072":"markdown","a3f308cf":"markdown","d641a3cc":"markdown","460a8bee":"markdown","1793e585":"markdown","8b18ef47":"markdown","0dd0b1b4":"markdown","847970e5":"markdown","18d11547":"markdown","55ed0cb3":"markdown","ffdb5d3e":"markdown","ca6ce64b":"markdown","c317edab":"markdown","ad68134d":"markdown","3c4273e8":"markdown","dd323c61":"markdown","febeae2f":"markdown","6562337c":"markdown","fd83ddad":"markdown","e15381ba":"markdown","3e157043":"markdown","46c614e1":"markdown","1bf37411":"markdown","b98c353b":"markdown","4f458c7a":"markdown","16b5871d":"markdown","2e6bfef1":"markdown","f9be5512":"markdown","736c3e89":"markdown","5b645dce":"markdown","0a303c4b":"markdown","396c88cc":"markdown","7f1b3bf2":"markdown","941073f9":"markdown","ba9fd3fe":"markdown","2fdc7853":"markdown","d5ecec6a":"markdown"},"source":{"3c493420":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nimport scipy\nfrom cycler import cycler\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import LabelEncoder","87b53867":"from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\nfrom sklearn.model_selection import KFold, GridSearchCV, cross_val_score, learning_curve\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score, precision_score, recall_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier as rfc, GradientBoostingClassifier as gbc\nfrom sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nimport optuna\nfrom functools import partial","1d9c3008":"mpl.rcParams['figure.dpi'] = 120\nmpl.rcParams['axes.spines.top'] = False\nmpl.rcParams['axes.spines.right'] = False\n\nraw_light_palette = [\n    (0, 122, 255), # Blue\n    (255, 149, 0), # Orange\n    (52, 199, 89), # Green\n    (255, 59, 48), # Red\n    (175, 82, 222),# Purple\n    (255, 45, 85), # Pink\n    (88, 86, 214), # Indigo\n    (90, 200, 250),# Teal\n    (255, 204, 0)  # Yellow\n]\n\nlight_palette = np.array(raw_light_palette)\/255","e8d1d0c2":"def binary_binary(data, feature1, feature2, palette1=None, palette2=None, s=''):\n    feature1_values = data[feature1].unique()\n    feature2_values = data[feature2].unique()\n    \n    light_palette = np.array(raw_light_palette)\/255\n    \n    if not palette1:\n        palette1 = ['#dddddd', mpl.colors.to_hex(light_palette[2])]\n    if not palette2:\n        palette2 = [light_palette[0], light_palette[3]]\n    \n    fig = plt.figure(figsize=(15, 11))\n    gs = fig.add_gridspec(3, 4)\n\n    ax_count_plot1 = fig.add_subplot(gs[:2,:2])\n    sns.countplot(x=data[feature1], hue=data[feature2], palette=palette1, ax=ax_count_plot1)\n    ax_count_plot2 = fig.add_subplot(gs[:2,2:4])\n    sns.countplot(x=data[feature2], hue=data[feature1], palette=palette2, ax=ax_count_plot2)\n\n    ax_count_plot2.set_ylabel('')\n    ax_pie1 = fig.add_subplot(gs[2, 0])\n    ax_pie2 = fig.add_subplot(gs[2, 1])\n    ax_pie3 = fig.add_subplot(gs[2, 2])\n    ax_pie4 = fig.add_subplot(gs[2, 3])\n\n    ax_pie1.axis('off')\n    male = data[data[feature1]==feature1_values[0]][feature2].value_counts()[[feature2_values[0], feature2_values[1]]]\n    ax_pie1.pie(male, labels=male.index, autopct='%1.1f%%', explode=(0, 0.1), startangle=90,\n                   colors=palette1)\n\n    ax_pie2.axis('off')\n    female = data[data[feature1]==feature1_values[1]][feature2].value_counts()[[feature2_values[0], feature2_values[1]]]\n    ax_pie2.pie(female, labels=female.index, autopct='%1.1f%%',explode = (0, 0.1), startangle=90,\n                    colors=palette1)\n\n    ax_pie3.axis('off')\n    notsurvived = data[data[feature2]==feature2_values[0]][feature1].value_counts()[[feature1_values[0], feature1_values[1]]]\n    ax_pie3.pie(notsurvived, labels=notsurvived.index, autopct='%1.1f%%',startangle=90,\n                          colors=palette2, textprops={'color':\"w\"}, explode=(0, 0.1))\n\n    ax_pie4.axis('off')\n    survived = data[data[feature2]==feature2_values[1]][feature1].value_counts()[[feature1_values[0], feature1_values[1]]]\n    ax_pie4.pie(survived, labels=survived.index, autopct='%1.1f%%', explode=(0, 0.1),\n                       startangle=90, colors=palette2, textprops={'color': 'w'})\n\n    fig.suptitle(f'[{feature1} & {feature2}] Conditional Distribution', fontweight='bold', fontsize=20)\n    fig.text(x=0.5, y=.94,\n             s=s,\n            ha='center', va='top')\n\n    plt.show()\n    \n    \ndef categorical_binary(data, nominal, binary, s=''):\n    bar_data = data[[nominal, binary]].groupby(nominal)[binary].value_counts().sort_index().unstack().fillna(0)\n    bar_data['ratio'] = bar_data.iloc[:,1] \/ (bar_data.iloc[:,0] + bar_data.iloc[:,1]) * 100\n\n    high_category = bar_data['ratio'].sort_values(ascending=False).index[0]\n    high_index = list(bar_data['ratio'].index).index(high_category)\n    row_category = bar_data['ratio'][bar_data['ratio'] > 0].sort_values(ascending=False).index[-1]\n    row_index = list(bar_data['ratio'].index).index(row_category)\n\n    fig = plt.figure(figsize=(15, 10))\n    gs = fig.add_gridspec(3, 4)\n    ax = fig.add_subplot(gs[:-1,:])\n\n    bar_data['ratio'].sort_values()\n    color_map = ['#d4dddd' for _ in range(data[nominal].nunique())]\n    color_map[row_index] = light_palette[3]\n    color_map[high_index] = light_palette[2]\n\n    bars = ax.bar(bar_data.index, bar_data.ratio, color=color_map, width=.55, edgecolor='black', linewidth=.7)\n    ax.spines[['top', 'right', 'left']].set_visible(False)\n    ax.bar_label(bars, fmt='%.2f%%')\n\n    mean = data[binary].mean() * 100\n    ax.axhline(mean, color='red', linewidth=.7, linestyle='dashdot')\n   \n    ax.set_yticks(np.arange(0, 120, 20))\n    ax.grid(axis='y', linestyle='-', alpha=0.4)\n    ax.set_ylim(0, 100)\n    \n    ax_bottom = fig.add_subplot(gs[-1,:])\n    bottom_data = data[nominal].value_counts().sort_index()\n    bars = ax_bottom.bar(bottom_data.index, bottom_data, width=0.55, \n                         edgecolor='black', \n                         linewidth=0.7)\n    ax_bottom.spines[[\"top\",\"right\",\"left\"]].set_visible(False)\n    ax_bottom.bar_label(bars, fmt='%d', label_type='center', color='white')\n    ax_bottom.grid(axis='y', linestyle='-', alpha=0.4)\n    \n    fig.text(0.1, 1, f'{nominal} & {binary}', fontsize=15, fontweight='bold', fontfamily='serif', ha='left')\n    fig.text(0.1, 0.96, s=s, fontsize=12, fontweight='light', fontfamily='serif', ha='left')\n    \n    plt.show()\n    \n    \ndef binary_twocategorical(data, cat1, cat2, binary, ax=None):\n    g = sns.heatmap(data.groupby([cat1, cat2])[binary].aggregate('mean').unstack() * 100,\n               square=True, annot=True, fmt='.2f', linewidth=2,\n               cbar_kws={'orientation': 'horizontal'}, cmap=sns.diverging_palette(240, 10, as_cmap=True),\n               ax=ax)\n    g.set_title(f'{binary} ratio by {cat1} and {cat2}', weight='bold')\n    \n    \ndef drawcorr(data):\n    mask = np.zeros_like(data.corr(), dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n\n    sns.heatmap(data.corr(),\n                square=True, annot=True, fmt='.2f', center=0, linewidth=2,\n                cbar=False, cmap=sns.diverging_palette(240, 10, as_cmap=True), mask=mask)\n    plt.show()\n    \n    \ndef show_missing(data):\n    light_palette = np.array(raw_light_palette)\/255\n    f, ax = plt.subplots(1, 1, figsize=(7, 5))\n    missing_count_by_feature = data.isnull().sum()[data.isnull().sum() > 0].sort_values(ascending=False)\n    missing_percent_by_feature = missing_count_by_feature \/ data.shape[0] * 100\n    ax.bar(missing_percent_by_feature.index, 100, color='#dadada', width=.5)\n    bar=ax.bar(missing_percent_by_feature.index, missing_percent_by_feature.values, width=.5, color='red')\n    ax.bar_label(bar, fmt='%.01f %%')\n    ax.spines.left.set_visible(False)\n    ax.set_yticks([])\n    ax.set_title('Null Data Ratio', fontweight='bold')\n    plt.show()\n    return pd.DataFrame([missing_count_by_feature, missing_percent_by_feature], index=['count', 'percent']).T\n\n\ndef scatter_by_binary(data, feature1, feature2, binary):\n    fig = plt.figure(figsize=(17, 10))\n    gs = fig.add_gridspec(2,3)\n    ax = fig.add_subplot(gs[:,:2])\n    sns.scatterplot(x=feature1, y=feature2, hue=binary, size=10, data=data, ax=ax)\n    ax.set_title(f'{feature1} & {feature2}', loc='left', fontweight='bold')\n\n    for v in [0, 1]:\n        ax = fig.add_subplot(gs[v,2])\n        sns.histplot(x=f'{feature1}', y=feature2, data=data[data[binary]==v], ax=ax)\n        ax.set_title(f'{binary}={v}', loc='left', fontweight='bold')\n\n    plt.show()\n    \n    \ndef piechart(data, std_name, xsize=12, ysize=5, sub_name=''):\n    total_count = data.sum()\n    item_size = len(data)\n    if sub_name:\n            sub_name = 'of ' + sub_name\n    f, ax =plt.subplots(1, item_size, figsize=(xsize, ysize), facecolor='whitesmoke')\n    for i, (item, count)  in enumerate(data.items()):\n        colors = ['white'] * item_size\n        colors[i] = 'crimson'\n        ax[i].pie(data, colors=colors)\n        ax[i].set_title(item, fontweight='bold', size=15)\n        ax[i].set_xlabel(f'{count \/ total_count * 100:.2f}%',\n                        fontweight='bold', size=13)\n    f.suptitle(f'Percentage {sub_name} by {std_name}',\n              fontweight='bold', size=20,\n              color='crimson')","2bbd8360":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ntest_id = test.PassengerId","1c6dd56a":"train.head()","e2fa2b46":"test.head()","176586f3":"all_data = pd.concat((train, test), axis=0)","72530875":"all_data.info()","88b8a0c1":"# binary\nall_data.Survived.value_counts()","9841df91":"# nominal\nall_data.Pclass.value_counts()","8831cb8a":"# numerical\nall_data.Age.value_counts()","31e8a436":"all_data.describe(include='all')","21a27678":"features = []\nfeatures.append({'Name': 'PassengerID', 'Type': 'object \/ nominal', 'desc': \"To be removed because it's an identification value.\"})\nfeatures.append({'Name': 'Survived', 'Type': 'numeric \/ binary nominal', 'desc': 'Target variable'})\nfeatures.append({'Name': 'Pclass', 'Type': 'numeric \/ nominal', 'desc': ''})\nfeatures.append({'Name': 'Name', 'Type': 'object \/ nominal', 'desc': \"It's hard to use as it is. Derivative variables can be generated (using Mr, Miss, etc.)\"})\nfeatures.append({'Name': 'Sex', 'Type': 'object \/ binary nominal', 'desc': ''})\nfeatures.append({'Name': 'Age', 'Type': 'numeric \/ continuous', 'desc': 'Missing value exists (average, median, sigma replacement)'})\nfeatures.append({'Name': 'SibSp', 'Type': 'numeric \/ continuous', 'desc': 'Derivative variable can be generated (with Parch)'})\nfeatures.append({'Name': 'Parch', 'Type': 'numeric \/ continuous', 'desc': 'Derivative variable can be generated (with SibSp)'})\nfeatures.append({'Name': 'Ticket', 'Type': 'object \/ nominal', 'desc': \"It's hard to use as it is\"})\nfeatures.append({'Name': 'Fare', 'Type': 'numeric \/ continuous', 'desc': 'Missing value exists (average, median, sigma replacement)'})\nfeatures.append({'Name': 'Cabin', 'Type': 'object \/ nominal', 'desc': \"It's hard to use as it is\"})\nfeatures.append({'Name': 'Embarked', 'Type': 'object \/ nominal', 'desc': ''})\nfeature_table = pd.DataFrame(features)","a0d7ab37":"color_list=[['whitesmoke', 'white', 'white']]\nfig = plt.figure()\nax = fig.add_axes([0,0,1,0.01])\nax.text(0.2, 2.3,'Features Information',size=20)\nax.axis('off')\ntable=ax.table(cellText = feature_table.values, colLabels=feature_table.columns,\n                  colColours=['skyblue']*3, cellColours=color_list*12,\n              colWidths=[.3,.5, 1.5], rowLoc='center', colLoc='center', cellLoc='left')\ntable.auto_set_font_size(False) \ntable.set_fontsize(13)\ntable.scale(1.0, 2.3)\nplt.show()","e5073f47":"missing_table = show_missing(all_data)\nmissing_table","f2550a07":"misspct = missing_table.iloc[0, 1]\nnotmisspct = 100 - misspct\nmissing_palette = ['#dddddd', mpl.colors.to_hex(light_palette[2])]\n\nfig = plt.figure(figsize=(10, 5))\ng = fig.add_gridspec(2,2)\nax = fig.add_subplot(g[:, 1])\nax.pie([misspct, notmisspct], labels=['Miss', 'Not Miss'],\n      autopct='%.2f%%', explode=(0, 0.1), startangle=90, colors=missing_palette)\nfig.suptitle(\"Cabin' missing \/ not missing percent\", va='top', ha='center')\nfig.text(0.37, .84, 'The missing value ratio exceeds 15%.\\nBecause there are too many missing values, Cabin is subject to removal.',)\nplt.show()","69690d0c":"pd.pivot_table(train, values='Age', index=['Pclass'],\n              columns=['Survived'], aggfunc=[np.mean, np.std]).style.bar(subset=['mean']).background_gradient(subset=['std'])","0bd55cb4":"train_size = train.shape[0]\ncabins = all_data.Cabin\n\nall_data.drop(['Cabin'], axis=1, inplace=True)\n#all_data['Age'] = all_data['Age'].fillna(all_data['Age'].median())\nfor c in [1, 2, 3]:\n    for s in [0, 1]:\n        all_data[(all_data.Pclass == c) & (all_data.Survived == s)]['Age'] =\\\n        all_data[(all_data.Pclass == c) & (all_data.Survived == s)].Age.fillna(\n        all_data[(all_data.Pclass == c) & (all_data.Survived == s)].Age.mean())\nall_data['Age'] = all_data['Age'].fillna(all_data['Age'].median())\nall_data['Embarked'] = all_data['Embarked'].fillna(all_data['Embarked'].mode()[0])\nall_data['Fare'] = all_data['Fare'].fillna(all_data['Fare'].median())","2242325a":"all_data.drop(['Survived'], axis=1).isnull().sum()","15275f30":"all_data.Name.value_counts()","6165e663":"import re\n\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\nall_data['Title'] = all_data.Name.apply(get_title)","6409e31b":"all_data['Fare_bin'] = pd.qcut(all_data.Fare, 5)\ndef age_bin(num):\n    for i in range(1, 100):\n        if num < 10*i :  return f'{(i-1) * 10} ~ {i*10}'\nall_data['Age_bin'] = all_data['Age'].apply(age_bin)","4bc9774c":"fig, ax = plt.subplots(1, 2, figsize=(15, 7))\nsns.distplot(all_data[all_data['Survived']==0]['Age'], label='survived: 0', ax=ax[0], color='red')\nsns.distplot(all_data[all_data['Survived']==1]['Age'], label='survived: 1', ax=ax[0], color='blue')\nsns.distplot(LabelEncoder().fit_transform(all_data[all_data['Survived']==0]['Age_bin']), label='survived: 0', ax=ax[1], color='red')\nsns.distplot(LabelEncoder().fit_transform(all_data[all_data['Survived']==1]['Age_bin']), label='survived: 1', ax=ax[1], color='blue')\nax[1].set_xlabel('Age')\nax[1].set_ylabel('')\nplt.legend()\nfig.suptitle('Age \/ Age bin Distribution by Survived', weight='bold', size=20)\nplt.show()","6721e177":"fig, ax = plt.subplots(1, 2, figsize=(15, 7))\nsns.distplot(all_data[all_data['Survived']==0]['Fare'], label='survived: 0', ax=ax[0], color='red')\nsns.distplot(all_data[all_data['Survived']==1]['Fare'], label='survived: 1', ax=ax[0], color='blue')\nsns.distplot(LabelEncoder().fit_transform(all_data[all_data['Survived']==0]['Fare_bin']), label='survived: 0', ax=ax[1], color='red')\nsns.distplot(LabelEncoder().fit_transform(all_data[all_data['Survived']==1]['Fare_bin']), label='survived: 1', ax=ax[1], color='blue')\nax[1].set_xlabel('Fare')\nax[1].set_ylabel('')\nplt.legend()\nfig.suptitle('Fare \/ Fare bin Distribution by Survived', weight='bold', size=20)\n\nplt.show()","a566bfed":"all_data['FamilySize'] = all_data.SibSp + all_data.Parch","af3e98b2":"all_data['IsAlone'] = all_data.FamilySize.apply(lambda x: 0 if x > 0 else 1)\nall_data['HasCabin'] = cabins.apply(lambda x: 0 if type(x) == float else 1)","d1a026bf":"all_data.head()","44392329":"binary_binary(all_data, 'Sex', 'Survived')","21e33cca":"binary_binary(all_data, 'IsAlone', 'Survived')","0315bc0c":"binary_binary(all_data, 'HasCabin', 'Survived')","a7b80fcb":"categorical_binary(all_data, 'Age_bin', 'Survived')","86950555":"categorical_binary(all_data, 'FamilySize', 'Survived')","47d3759d":"categorical_binary(all_data, 'Title', 'Survived')","886900f6":"piechart(all_data.groupby('Pclass').Survived.count(),\n         std_name='Pclass', sub_name='Survived',\n         xsize=15)","f0a2f626":"plt.subplots(figsize=(10, 10))\ndrawcorr(all_data.iloc[:train_size])","c94e5eb0":"plt.subplots(figsize=(12, 9))\nbinary_twocategorical(all_data, 'Pclass', 'Age_bin', 'Survived')","178828af":"f, ax = plt.subplots(1, 2, figsize=(12, 5))\nbinary_twocategorical(all_data, 'Sex', 'Pclass', 'Survived', ax[0])\nbinary_twocategorical(all_data, 'Sex', 'Embarked', 'Survived', ax[1])\nplt.show()","66c68eb5":"scatter_by_binary(train, 'Age', 'Fare', 'Survived')","36db1be0":"drop_features = [\n    'PassengerId', 'Name', 'SibSp', 'Ticket'\n]\nall_data.drop(drop_features, axis=1, inplace=True)","e1af23eb":"nominals = [\n    'Sex', 'Embarked', 'Title'\n]\norders = [\n    'Age_bin', 'Fare_bin'\n]\nle = LabelEncoder()\nfor f in orders:\n    all_data[f] = le.fit_transform(all_data[f])","16c03ec3":"train = all_data.iloc[:train_size, :]\ntest = all_data.iloc[train_size:, :]\n\ng = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked',\n       u'FamilySize', u'Title', u'Age_bin', u'Fare_bin']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])\nplt.show()","56369303":"all_data = pd.get_dummies(all_data)","ebc7d162":"all_data","396df28d":"train = all_data.iloc[:train_size, :]\nX_train, y_train = train.drop(['Survived'], axis=1), train.Survived.values\n#X_train = X_train[X_train.columns[X_train.dtypes == 'uint8']].astype('int')\nX_test = all_data.iloc[train_size:, :].drop(['Survived'], axis=1)","eb343650":"model_logistic = LogisticRegression(solver='liblinear')\nmodel_rfc = rfc()\nmodel_etc = ExtraTreesClassifier()\nmodel_abc = AdaBoostClassifier(DecisionTreeClassifier())\nmodel_gbc = gbc()\nmodel_svc = SVC(kernel='linear', probability=True)\nmodel_xgb = XGBClassifier()\nmodel_lgbm = LGBMClassifier()\n\nmodels = {\n    'Logistic Regresison': model_logistic,\n    'Random Forest': model_rfc,\n    'ExtraTrees': model_etc,\n    'AdaBoost': model_abc, \n    'GradientBoost': model_gbc,\n    'SVC': model_svc,\n    'XGBoost': model_xgb,\n    'LightGBM': model_lgbm\n}\nvalidation_scores = {}","820ef1d7":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\nfor name, model in models.items():\n    print(f\"{name}'s KFold Start\")\n    score = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=kf, n_jobs=-1, verbose=0).mean()\n    print(f\"{name}'s cross validation score: {score:.6f}\\n\")\n    validation_scores[name] = score","708f8f5f":"sns.barplot(y=list(validation_scores.keys()), \n            x=list(validation_scores.values()),\n            orient='h', palette=\"Set3\")\nplt.show()","4863dc96":"rf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\netc_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\nabc_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\ngbc_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\nmodels_params = {\n    'Random Forest': [model_rfc, rf_param_grid],\n    'ExtraTrees': [model_etc, etc_param_grid],\n    'AdaBoost': [model_abc, abc_param_grid], \n    'GradientBoost': [model_gbc, gbc_param_grid],\n    'SVC': [model_svc, svc_param_grid],\n}\nfinal_models = {\n    \n}","404c9e34":"for name, [model, param] in models_params.items():\n    print(f'{name} grid search start')\n    search = GridSearchCV(model, param, cv=kf, n_jobs=-1, verbose=1,\n                         scoring='accuracy').fit(X_train, y_train)\n    final_models[name] = search.best_estimator_\n    print(f'best score: {search.best_score_}\\n')","133cdb3e":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","df7170e4":"for name, model in final_models.items():\n    plot_learning_curve(model, name, X_train, y_train, cv=kf)","a525a620":"model_voting = VotingClassifier(estimators=[('rfc', final_models['Random Forest']), ('ebc', final_models['ExtraTrees']),\n('svc', final_models['SVC']), ('abc',final_models['AdaBoost']),('gbc',final_models['GradientBoost'])], voting='soft', n_jobs=-1)","203b5a60":"# best score model.\ntest_Survived = pd.Series(final_models['Random Forest'].predict(X_test), name=\"Survived\").astype('int')\nresults = pd.concat([test_id, test_Survived],axis=1)\nresults.to_csv(\"submission.csv\",index=False)","2e03a989":"I referred to the following kernel.  \nhttps:\/\/www.kaggle.com\/subinium\/tps-apr-highlighting-the-data","ef29e280":"We looked at the mean and standard deviation of Age according to Pclass and Survived.<br>\nThe average of \"age\" varies depending on the group. I thought I could replace Age's missing values with the average of each group.","d7f73269":"Visualize the cross-validation results of each model.<br>\nThe performance of each algorithm was formed similarly. I decided to use Random Forest, ExtraTrees, AdaBoost, GradientBoost, and SVC.","c8c6a943":"Variables stored in letters cannot be used for analysis by themselves, so they must be changed to numerical variables.<br>\nThe basic conversion method is to apply LabelEncoding as a variable representing ranking\/order, and OnehotEncoding as other nominal variables.<br><br>\nThe following is the LabelEncoding process for ranking\/order variables.","fd06e070":"First, understand the meaning and type of each variable.<br><br>\n'info' shows the data type and missing value status of each variable.","beb6567d":"This time, let's visualize the relationship between several variables.The combination of <br> variables may provide clear insights in determining whether they are alive or not.","300a15e7":"<font color=\"#FBBF44\" size=4><b>Binary and dependent variables.<\/b><\/font>","42bc570d":"The survival rate of children with Pclass 2 is 100%. On the other hand, the survival rate of those in their 50s with a Pclass of 3 is zero.","a0f89dce":"Value_counts() help you identify the actual type of each variable.<br>\nFor example, Survived's data type is int, but it is actually a binary variable that requires encoding.<br><br>\nHere are some examples.","d5c8585c":"<a id=\"4.1\"><\/a>\n<font color=\"#08B4E4\" size=+1.5><b>4.1. Simple modeling and evaluation<\/b><\/font>","e940eb77":"This time, we visualized the survival rate according to the value of 'Pclass'.<br>\nCan I use this variable?","77ce468f":"Based on the results derived above, let's summarize the insights for each variable.","c0d81904":"<font color=\"#FBBF44\" size=4><b>Category variable and dependent variable<\/b><\/font>","a36a2b02":"Even with the same problem, there are various models with different approaches. Each model's performance is different, and the points that the model can recognize and solve are also different.<br>\nTherefore, in some cases, it is better to synthesize the opinions of multiple models and produce final results than to use a single model.<br>\nI used RandomForest as the final model, but I also used the voting ensemble technique (RandomForest is also a kind of ensemble technique).","e154f089":"<a id=\"4\"><\/a>\n<font color=\"#089371\" size=+2.5><b>4. Modeling<\/b><\/font>","4e99e146":"Now, our datasets have more meaning than before.","7c98ecea":"<a id=\"4.4\"><\/a>\n<font color=\"#08B4E4\" size=+1.5><b>4.4. Select final model and submit<\/b><\/font>","68024b8b":"The functions below help visualization.","8dbd6c50":"<font color=\"#FBBF44\" size=4><b>Add binary variables<\/b><\/font>","2ef9d7e8":"Name is a nominal variable, but the types are too diverse and the frequency is very small, making it unsuitable for analysis.<br>\nInstead, if you look closely at the values, you can see that keywords such as \"Miss,\" \"Mr,\" and \"Master\" are commonly included.<br>\nI think these keywords can explain personal information better than meaningless names.<br>\nThis approach is 'Parsing'.","fd0aefb1":"Describe helps you understand the basic statistics or frequency of each variable.","e617f72b":"<p style=\"background-color:#08B4E4;font-family:sans-serif;color:white;font-size:200%;text-align:center;border-radius:10px 10px;margin:0 auto;height:30px;line-height:30px;weight:bold\">TABLE OF CONTENTS<\/p> \n    \n* [0. Introduction](#0)   \n* [1. Import Modules](#1)\n* [2. Data Load](#2)\n* [3. EDA and Feature Engineering](#3)\n    * [3.1. Select Types](#3.1)\n    * [3.2. Cleansing - Missing values](#3.2)\n    * [3.3. Derivative features](#3.3)\n    * [3.4. Feature Enginnering - Select features](#3.4)\n    * [3.5. Feature Enginnering - Transform features](#3.5)\n* [4. Modeling](#4)\n    * [4.1 Simple modeling and evaluation](#4.1)\n    * [4.2 Advanced modeling with tunning](#4.2)\n    * [4.3 Ensembles](#4.3)\n    * [4.4 Select final model and submit](#4.4)\n","f849c4a8":"For analysis, the training data and test data are divided.","c33f1662":"Sometimes 'intervals' can help you understand dependent variables more than continuous values.<br>\nFor example, focusing on the difference between those in their 20s and 30s can be of great help in analysis rather than looking at the difference between those in their 20s and 21s. The same goes for Fare.<br><br>\nThis approach is called 'Bining'. This time, let's proceed with the binding for Age and Fare first and then visualize it.","f921a177":"In order to improve the performance of the model, the hyper parameter must be adjusted. I used GridSearchCV.<br>\nOptimization using GridSearch originally requires a lot of time, effort, know-how and experience.<br>\nSince this kernel focused on grasping the overall process, the range of each parameter was simple by referring to the other kernel.","ee9f710b":"Sometimes it is possible to set a criterion for a variable and divide it into two cases.<br>\nIt mainly represents whether you have something as a binary variable, which can highlight more meaning than the original value.<br><br>\nI added variables representing <br> ('people on board with family \/ alone'), <br> ('people with Cabin' \/ 'people without Cabin') <br>.","dbe37fa9":"The datasets we currently have are training data and test data.<br>\nSuppose that after learning with the training data, we conducted a model evaluation using the test data and found a bad result.<br>\nTuning and re-learning the model for better results will enable you to produce better results than before. However, during the re-learning process, the model indirectly acquires information on the test data.<br>\nThis will prevent us from fully evaluating the generalization performance of the model, and the model overfitting the currently given training data and test data.<br>\nTherefore, we should use a method called 'cross-validation'. After configuring a separate dataset for verification, evaluate the performance of the model.<br>\nKfold is the most basic and widely used cross-verification method. In particular, it is good to use when the amount of data is not as large as the current Titanic dataset.","94407f9b":"In this kernel, I will explain <b>basic and core processes<\/b> of binary classification for beginners.<br>\nSince I am also a beginner, I referred to various kernels and tried to explain them kindly based on my understanding.<br>\nIn particular, I wrote the EDA and preprocessing process in detail based on visualization.<br><br>\nClassification is one of the most important tasks of machine learning. Dependent variables expressed in 0 and 1 are given, and the goal is to learn the pattern of explanatory variables and guess the correct answer. In this problem, the presence or absence of passengers is a dependent variable.<br>\nThere are several variables in the dataset that can explain the dependent variable (name, age, gender, etc.). Among them, there are numerical types and category types. Humans can interpret them, but models only recognize numbers. Therefore, some variables require the task of converting them into numbers. To do that, we need to be aware of the types of variables.<br>\nOutliers or missing values may exist in the dataset. There are many reasons why they occur, but in the end, we need to manage them. You can also create new variables that can make the dataset more meaningful than existing variables, or identify and remove unnecessary variables.<br>\nWe call this process <b>preprocessing<\/b>, and for this, we need to understand the variables properly through <b>EDA<\/b>.<br><br>\nThe main models of machine learning for classification problems include parametric methods such as logistic regression analysis, tree-based ensemble techniques (random forest, XGBM, etc.), and Support Vector Machine.<br>\nI'll briefly introduce it in this kernel.","73d2c6c5":"In general, nominal variables can be replaced with the mode, and numerical continuous variables can be replaced with mean or median values.<br> Therefore, we decided to replace Embarked with the mode and Fare with the median value.<br><br>\nNow, let's remove the missing values!","b1d1d379":"Correlation analysis tells the linear relationship between variables.<br>\nAs expected, the correlation coefficient between SibSp and Parch is very high. Since this causes multicollinearity, you can consider removing two variables. We can remove variables more safely because we have already added FamilySize variables.","d7873146":"<a id=\"3.1\"><\/a>\n<font color=\"#08B4E4\" size=+1.5><b>3.1. Select Types<\/b><\/font>","344aeef5":"Regardless of Pclass, the survival rate of men is low. Women with Embarked Q or S have a relatively lower survival rate than women with C.","d1b9d38d":"If it has the same meaning and the same effect, the smaller the number of variables, the better.<br>\nWe previously grasped the meaning of SibSp and Parch through univariate search. They all mean family relationships, so they can be combined into one variable.<br><br>\nI think this approach can be called 'combination or reduction'.","765eb072":"Reading data description documents helps you understand the meaning and actual type of individual variables.\n\nSurvival: Survival 0 = No, 1 = Yes<br>\nPclass: Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd<br>\nSex: Sex<br>\nAge: Age in years<br>\nSibsp: # of siblings \/ spouses aboard the Titanic<br>\nParch: # of parents \/ children aboard the Titanic<br>\nTicket: Ticket number<br>\nFare: Passenger fare<br>\nCabin: Cabin number\t<br>\nEmbarked: Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton","a3f308cf":"<font color=\"#FBBF44\" size=4><b>Age and Fare<\/b><\/font>","d641a3cc":"Fare, too, makes a clearer difference when binding.","460a8bee":"Now, for basic performance comparison of each algorithm, we create the most basic model and proceed with cross validation.<br>\nKfold, which has five cross-validation sets, was used.","1793e585":"The younger the age and the higher the Fare, the higher the survival rate.<br><br>\nIt was confirmed that all the variables discussed above are necessary for analysis.<br>\nLet's remove some variables that are not very helpful for analysis.","8b18ef47":"Missing values adversely affect the analysis. Therefore, we must remove missing values.<br>\nLet's find the missing values.","0dd0b1b4":"<a id=\"1\"><\/a>\n<font color=\"#089371\" size=+2.5><b>1. Import Modules<\/b><\/font>","847970e5":"I looked into the relationship between Survived and Sex. The survival rate varies greatly depending on the sex.","18d11547":"<font color=\"#FBBF44\" size=4><b>Name<\/b><\/font>","55ed0cb3":"<a id=\"4.2\"><\/a>\n<font color=\"#08B4E4\" size=+1.5><b>4.2. Advanced modeling with tunning<\/b><\/font>","ffdb5d3e":"First, we looked at the relationship between the age group and survival rate. From the figure above, we can see that a particular group has a high and low survival rate.","ca6ce64b":"Let's look at the relationship between nominal variables and dependent variables with multiple categories.","c317edab":"<a id=\"2\"><\/a>\n<font color=\"#089371\" size=+2.5><b>2. Data Load<\/b><\/font>","ad68134d":"Create 'all_data' that combines training data and test data for EDA.","3c4273e8":"<a id=\"3.2\"><\/a>\n<font color=\"#08B4E4\" size=+1.5><b>3.2. Cleansing - Missing values<\/b><\/font>","dd323c61":"<a id=\"4.3\"><\/a>\n<font color=\"#08B4E4\" size=+1.5><b>4.3. Ensembles<\/b><\/font>","febeae2f":"<font color=\"#FBBF44\" size=4><b>SibSp and Parch<\/b><\/font>","6562337c":"Likewise, we visualized the survival rates according to the values of IsAlone and HasCabin that we just added.<br><br>\nWe can conclude that women have a higher survival rate than men, with families than alone, and with Cabin.<br>\nIf you have an observation that comes with a woman, a family, and has Cabin, you are more likely to have survived.","fd83ddad":"<p style='text-align:center;font-family: sans-serif;font-weight:bold;color:#616161;font-size:25px;margin: 30px;'>Personal<font color='#08B4E4' size='7'>ML<\/font>Notebook<\/p>\n<p style='text-align:center;font-family: sans-serif ;font-weight:bold;color:black;font-size:40px;margin: 30px;'>Titanic - The basic process of binary classification<\/p>\n<p style=\"text-align:center;font-family: sans-serif ;font-weight:bold;color:#616161;font-size:20px;margin: 30px;\">classification<\/p>","e15381ba":"<a id=\"0\"><\/a>\n<font color=\"#089371\" size=+2.5><b>0. Introduction<\/b><\/font>","3e157043":"I created some models for problem solving.","46c614e1":"Likewise, we applied visualization to FamilySize and Title. There are differences in survival rates depending on the category, so they are all excellent variables.","1bf37411":"<a id=\"3.6\"><\/a>\n<font color=\"#08B4E4\" size=+1.5><b>3.6. Feature Engineering - Transform features<\/b><\/font>","b98c353b":"![ship-3401500_640.jpg](attachment:d821fc6f-e8d8-4332-ba99-dfb69855fc44.jpg)","4f458c7a":"Derivative variables enrich the meaning of datasets.<br>\nCombining existing variables, deriving hidden meanings, or calculating can lead to new variables.<br><br>\nCreate variables that have better meaning than the original variables.","16b5871d":"Except for Survived, which exists only in training data,<br>\nCabin, Age, Embarked, and Fare are variables with missing values.<br><br>\nMore than half of Cabin is missing. It has so many missing values!<br>\nOn the other hand, Embarked and Fare have very few missing values. We previously confirmed that Embarked is a nominal variable and Fare is a numerical variable. The data type may determine how to process missing values.<br>","2e6bfef1":"After Onehot Encoding, check the final completed dataset.","f9be5512":"I'm thinking of removing Cabin altogether because it has too many missing values.","736c3e89":"The age group clearly explains the survival rate rather than the continuous Age.","5b645dce":"<a id=\"3.3\"><\/a>\n<font color=\"#08B4E4\" size=+1.5><b>3.3. Feature Engineering - Derivative features<\/b><\/font>","0a303c4b":"Let's visualize the scores for each model's training data and verification data.","396c88cc":"<a id=\"3.5\"><\/a>\n<font color=\"#08B4E4\" size=+1.5><b>3.5. Feature Engineering - Select features<\/b><\/font>","7f1b3bf2":"<font color=\"#FBBF44\" size=4><b>Mutivariate search<\/b><\/font>","941073f9":"Before Onehot Encoding, I drew a pair plot.","ba9fd3fe":"<a id=\"3\"><\/a>\n<font color=\"#089371\" size=+2.5><b>3. EDA and Feature Engineering<\/b><\/font>","2fdc7853":"Now, let's go through some exploration to determine which variables are important and which variables are not needed.<br>\nWhat is a good variable in this problem?<br><br>\nI think the variable that clearly distinguishes survival is a good variable.","d5ecec6a":"Now let's start GridSearching for each model. GridSearchCV provides cross-validation as the name suggests. I used five folds.<br>\nBest_estimator_' is the model with the best cross-validation performance, and it also provides hyperparameters."}}