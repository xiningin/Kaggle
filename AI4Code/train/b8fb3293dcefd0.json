{"cell_type":{"ebfbac1a":"code","bbbd3948":"code","c838f6d3":"code","7adf0fea":"code","c2f73ea9":"code","713d6401":"code","467bcd59":"code","ab2aebca":"code","fac31dea":"code","8c21c241":"code","6c818302":"code","faa7c3bc":"code","c7dc2127":"code","50d603ce":"code","8214e658":"code","7f9d9da3":"code","633132c0":"code","d62c3f5b":"code","572648f9":"code","4aefbe6e":"markdown","7b65c72d":"markdown","d8ae81d2":"markdown","6d698bde":"markdown","3ce28dfc":"markdown","ac0e0ccb":"markdown","c04da247":"markdown","d1b60916":"markdown","2960d3d1":"markdown","c8bbae0e":"markdown"},"source":{"ebfbac1a":"!pip install discover_feature_relationships\n!pip install hvplot","bbbd3948":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom discover_feature_relationships import discover\nimport sklearn\nimport holoviews as hv\nimport geoviews as gv\nimport geopandas as gpd\nimport hvplot.pandas\nfrom cartopy import crs\nimport pycountry\nfrom fuzzywuzzy import fuzz\n\nimport altair as alt\nimport matplotlib.pyplot as plt\n%matplotlib inline\nalt.renderers.enable('notebook')\nhv.extension('bokeh', 'matplotlib')\ngv.extension('bokeh', 'matplotlib')","c838f6d3":"from IPython.display import HTML\n\nimport altair as alt\nfrom  altair.vega import v3\nimport json\n\nvega_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega@' + v3.SCHEMA_VERSION\nvega_lib_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-lib'\nvega_lite_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-lite@' + alt.SCHEMA_VERSION\nvega_embed_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-embed@3'\nnoext = \"?noext\"\n\npaths = {\n    'vega': vega_url + noext,\n    'vega-lib': vega_lib_url + noext,\n    'vega-lite': vega_lite_url + noext,\n    'vega-embed': vega_embed_url + noext\n}\n\nworkaround = \"\"\"\nrequirejs.config({{\n    baseUrl: 'https:\/\/cdn.jsdelivr.net\/npm\/',\n    paths: {}\n}});\n\"\"\"\n\ndef add_autoincrement(render_func):\n    # Keep track of unique <div\/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n            \n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    chart_str = \"\"\"\n    <div id=\"{id}\"><\/div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"anything?\");\n    }});\n    console.log(\"really...anything?\");\n    <\/script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )\n\nHTML(\"\".join((\n    \"<script>\",\n    workaround.format(json.dumps(paths)),\n    \"<\/script>\",\n    \"This code block sets up embedded rendering in HTML output and<br\/>\",\n    \"provides the function `render(chart, id='vega-chart')` for use below.\"\n)))","7adf0fea":"df_2015 = pd.read_csv(\"..\/input\/2015.csv\")\ndf_2016 = pd.read_csv(\"..\/input\/2016.csv\")\ndf_2017 = pd.read_csv(\"..\/input\/2017.csv\")","c2f73ea9":"targets = ['Low', 'Low-Mid', 'Top-Mid', 'Top']\nh_cols = ['Country', 'GDP', 'Family', 'Life', 'Freedom', 'Generosity', 'Trust']\ndef prep_frame(df_year, year):\n    df = pd.DataFrame()\n    # Work around to load 2015, 2016, 2017 data into one common column\n    target_cols = []\n    for c in h_cols:\n        target_cols.extend([x for x in df_year.columns if c in x])\n    df[h_cols] = df_year[target_cols]\n    df['Happiness Score'] = df_year[[x for x in df_year.columns if 'Score' in x]]\n    # Calculate quartiles on the data.\n    df[\"target\"] = pd.qcut(df[df.columns[-1]], len(targets), labels=targets)\n    df[\"target_n\"] = pd.qcut(df[df.columns[-2]], len(targets), labels=range(len(targets)))\n    # Append year and assign to multi-index\n    df['Year'] = year\n    df = df.set_index(['Country', 'Year'])\n    return df\ndf = prep_frame(df_2015, 2015)\ndf = df.append(prep_frame(df_2016, 2016), sort=False)\ndf = df.append(prep_frame(df_2017, 2017), sort=False)\ndf.head()","713d6401":"spearman_cormatrix= df.corr(method='spearman')\nspearman_cormatrix","467bcd59":"fig, ax = plt.subplots(ncols=2,figsize=(24, 8))\nsns.heatmap(spearman_cormatrix, vmin=-1, vmax=1, ax=ax[0], center=0, cmap=\"viridis\", annot=True)\nsns.heatmap(spearman_cormatrix, vmin=-.25, vmax=1, ax=ax[1], center=0, cmap=\"Accent\", annot=True)","ab2aebca":"sns.pairplot(df.drop(['target_n'], axis=1), hue='target')\n#hvplot.scatter_matrix(df.drop(['target_n'], axis=1), c='target')\n\n#plt.show()","fac31dea":"classifier_overrides = set()\ndf_results = discover.discover(df.drop(['target', 'target_n'],axis=1).sample(frac=1), classifier_overrides)","8c21c241":"fig, ax = plt.subplots(ncols=2,figsize=(24, 8))\nsns.heatmap(df_results.pivot(index='target', columns='feature', values='score').fillna(1).loc[df.drop(['target', 'target_n'],axis=1).columns,df.drop(['target', 'target_n'],axis=1).columns],\n            annot=True, center=0, ax=ax[0], vmin=-1, vmax=1, cmap=\"viridis\")\nsns.heatmap(df_results.pivot(index='target', columns='feature', values='score').fillna(1).loc[df.drop(['target', 'target_n'],axis=1).columns,df.drop(['target', 'target_n'],axis=1).columns],\n            annot=True, center=0, ax=ax[1], vmin=-0.25, vmax=1, cmap=\"Accent\")\nplt.plot()","6c818302":"#from sklearn.decomposition import PCA\nfrom sklearn.decomposition import MiniBatchSparsePCA as PCA\npca = PCA(n_components=2,\n          batch_size=10,\n          normalize_components=True,\n          random_state=42)\nprincipalComponents = pca.fit_transform(df[h_cols[1:-2]])\n\nsource = df.copy()\nsource['component 1'] = principalComponents[:,0]\nsource['component 2'] = principalComponents[:,1]\nsource.head()","faa7c3bc":"base = alt.Chart(source.reset_index())\n\nxscale = alt.Scale(domain=(source['component 1'].min(), source['component 1'].max()))\nyscale = alt.Scale(domain=(source['component 2'].min(), source['component 2'].max()))\n\narea_args = {'opacity': .6, 'interpolate': 'step'}\n\npoints = base.mark_circle(size=60).encode(\n    alt.X('component 1', scale=xscale),\n    alt.Y('component 2', scale=yscale),\n    color='target',\n    tooltip=['Country', 'target', 'GDP', 'Family', 'Life']\n).properties(height=600,width=600).interactive()\n\n\ntop_hist = base.mark_area(**area_args).encode(\n    alt.X('component 1:Q',\n          # when using bins, the axis scale is set through\n          # the bin extent, so we do not specify the scale here\n          # (which would be ignored anyway)\n          bin=alt.Bin(maxbins=20, extent=xscale.domain),\n          stack=None,\n          title=''\n         ),\n    alt.Y('count()', stack=None, title=''),\n    alt.Color('target:N'),\n).properties(height=60,width=600)\n\nright_hist = base.mark_area(**area_args).encode(\n    alt.Y('component 2:Q',\n          bin=alt.Bin(maxbins=20, extent=yscale.domain),\n          stack=None,\n          title='',\n         ),\n    alt.X('count()', stack=None, title=''),\n    alt.Color('target:N'),\n).properties(width=60,height=600)\n\nrender(top_hist & (points | right_hist))\n\n","c7dc2127":"from sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler()\ntmp_df = df.iloc[df.index.get_level_values('Year') == 2017].reset_index()\ntmp_df.loc[:,[\"Happiness Score\"]+h_cols[1:]] = min_max_scaler.fit_transform(tmp_df[[\"Happiness Score\"]+h_cols[1:]])","50d603ce":"hvplot.parallel_coordinates(tmp_df, 'target', cols=[\"Happiness Score\"]+h_cols[1:], alpha=.3, tools=['hover', 'tap'], width=800, height=500)","8214e658":"tmp_df.sort_values(by='Generosity', ascending=False).head()","7f9d9da3":"rank_df = tmp_df[h_cols[:4]].rank(axis=0,numeric_only=True, method='dense', ascending=False)\nrank_df['Country'] = tmp_df['Country']\nrank_df['Influence'] = tmp_df[h_cols].rank(axis=0,numeric_only=True, method='dense').idxmax(axis=1)\nrank_df['True Influence'] = tmp_df[h_cols[:4]].rank(axis=0,numeric_only=True, method='dense').idxmax(axis=1)","633132c0":"# Country names are hard.\ncountries = {}\nfor country in pycountry.countries:\n    countries[country.alpha_3] = country.name\nworld_map = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nworld_map['Country'] = [countries.get(country, 'Unknown Code') for country in list(world_map['iso_a3'])]\n\nfor q in world_map['Country']:\n    if \"Unknown Code\" in q:\n        world_map.loc[world_map.Country == q, 'Country'] = world_map.loc[world_map.Country == q, 'name']\n    elif q in \"Ivory Coast\":\n        world_map.loc[world_map.Country == q, 'Country'] = \"C\u00f4te d'Ivoire\"\n    elif q in \"Viet Nam\":\n        world_map.loc[world_map.Country == q, 'Country'] = \"Vietnam\"\n    elif \"Korea\" in q:\n        world_map.loc[world_map.Country == q, 'Country'] = \"South Korea\"\n        \n\nfor x in rank_df['Country']:\n    if not x in list(world_map['Country']):\n        for q in world_map['Country']:\n            if (x[:5] in q) and (not x[:5] in \"South\"):\n                world_map.loc[world_map.Country == q, 'Country'] = x\n                break\n            elif fuzz.partial_ratio(x,q) > 75:\n                world_map.loc[world_map.Country == q, 'Country'] = x\n                break\n        else:\n            if not x in list(world_map['name']):\n                world_map.loc[world_map.Country == q, 'Country'] = x\n            \n\nwith pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n    print(world_map[['iso_a3', 'name', 'Country']])","d62c3f5b":"gv_frame = pd.merge(world_map, rank_df, on='Country')\n\nbackground = gv.Polygons(gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))).opts(color=\"#FFFFFF\")\nclusters = gv.Polygons(gv_frame, vdims=['True Influence', 'Influence', 'Country']).opts(tools=['hover', 'tap'], cmap='Accent', show_legend=True, legend_position='bottom_left')\n\n((background * clusters).opts(width=800, height=500, projection=crs.PlateCarree()))\n","572648f9":"background = gv.Polygons(gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))).opts(color=\"#FFFFFF\")\nclusters = gv.Polygons(gv_frame, vdims=['Influence', 'True Influence' ,'Country']).opts(tools=['hover', 'tap'], cmap='Dark2', show_legend=True, legend_position='bottom_left')\n\n((background * clusters).opts(width=800, height=500, projection=crs.PlateCarree()))","4aefbe6e":"# Beyond Simple Correlation\nIn the scatterplots, we see that `GDP`, `Family`, and `Life Expectancy` are quite linearly correlated with some noise. I find the auto-correlation of `Trust` most fascinating here, where everything is bad, but if trust is high, the distribution is all over the place. It seems to be just a negative indicator on a threshold.\n\nAt PyCon I found this interesting package by Ian Ozsvald that uses. It trains random forrests to predict features from each other, going a bit beyond simple correlation.","7b65c72d":"# Peak Happiness\nWe know that the highest influence comes from `GDP`, `Life Expectancy` and `Family`, let's see, which on actually is the strongest in each country and compare it to all apparently important values.","d8ae81d2":"# Conclusion\nIt seems like the common criticism for \"The World Happiness Report\" is quite valid. A high focus on GDP and strongly correlated features such as family and life expectancy.\n\nIt goes well with common wisdom that money makes you happy up to a certain threshold (about 70,000 in the US). Having a good social net is important and family tends to provide that. High life expectancy and health make you worry less about how you'll survive and more about upvotes on kaggle, so:\n### Go hug your mum, get a raise and upvote this kernel","6d698bde":"This gets interesting. Trust in government is a better predictor of the Happiness Score than Family. Possibly because of the funny 'thresholding effect' we discovered in the scatterplot?\n\nAdditionally, although family correlated quite well, it does not have strong predictive value. Maybe because all the distributions of the quartiles are quite close in the scatterplot?\n\n\n# Does it Separate?","3ce28dfc":"Let's construct a Dataframe that contains the actual features.\n\nParts starting with `Happiness`, `Whisker` and the `Dystopia.Residual` are basically targets, just different targets.\nDystopia Residual compares each countries scores to the theoretical unhappiest country in the world.\nSince the data from the years have a bit of a different naming convention, so I'll abstract these to a common name.","ac0e0ccb":"What's up with generosity though? That Low-Mid ranked country being a massive outlier is definitely worth investigating.","c04da247":"Myanmar and Indonesia being exceptionally generous here, but low GDP and low-ish life expectancy hinder its claim to fame.","d1b60916":"It looks like `GDP`, `Family`, and `Life Expectancy` are strongly correlated with the Happiness score. `Freedom` and correlates quite well with the Happiness score, however, Freedom correlates quite well with all data. `Government Trust` still has a mediocre correlation with the Happiness score.\n\nLet's look at a pairwise comparison of our variables. The color is based on quartiles of the `Happiness.Score` so `[0%-25%, 25%-50%, 50%-75%, 75%-100%]`. Clearly, the last row and column aren't particularly meaningful regarding the colors.","2960d3d1":"We calculated correlation matrixes of the `Spearman` kind. On the left you see a continuous colormap, on the right you see a binned map.","c8bbae0e":"# How does it connect?\nSometimes it's nice to just trace the relative ranking of a country throughout their features. Parallel coordinate plots seem to be relatively uncommon among plotting libraries, so this is the best we got."}}