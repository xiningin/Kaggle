{"cell_type":{"09eaa88f":"code","1f1729ff":"code","5d13da03":"code","d31df8d1":"code","7af86e58":"code","7f128a7e":"code","6492e66b":"code","c183a675":"code","bfb157bd":"code","7769df47":"code","31285008":"code","e54ec3a4":"code","79627b17":"code","7ee7eed7":"code","b0c51c98":"code","d20d49e6":"code","52ca833d":"code","6f1ac864":"code","0946ceff":"code","7fc69099":"code","7694e3da":"code","c0d870cd":"code","adefb714":"code","6243537e":"code","ae69a64b":"code","ec9d0ae2":"code","12e80bb4":"code","1b5145d6":"code","82b1297f":"code","07d67d81":"code","7fc1517b":"code","21dfe782":"code","07a48f4f":"code","3bf33350":"code","ec104d6e":"code","f1afe749":"code","289c07e7":"code","4150f319":"code","e11c24b6":"code","1fe563cf":"code","e54f62ef":"code","666bb581":"code","4414cf6a":"code","aaad551c":"code","380e0ea7":"code","7e41e9b2":"markdown","2891d130":"markdown","6e56c1fc":"markdown","4471951d":"markdown","1abb435c":"markdown","a480f563":"markdown","c13184a0":"markdown","08ad50d8":"markdown","4d12e0d9":"markdown","0568f6ce":"markdown","154c5775":"markdown","92c1f670":"markdown","00f1eb3a":"markdown","52080787":"markdown","8f1e5c3b":"markdown","defd8a20":"markdown","08a787f6":"markdown","b6c8c550":"markdown","c84cb167":"markdown","3f079da5":"markdown","3ddf2d14":"markdown","f4f512ce":"markdown","2abd7d8d":"markdown","90f72763":"markdown","cda7cfc4":"markdown","eee3c1a7":"markdown","dc00f8c5":"markdown","7b485549":"markdown","aeec8f2c":"markdown"},"source":{"09eaa88f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1f1729ff":"data = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')","5d13da03":"data.shape","d31df8d1":"# basic statics\ndata.describe()","7af86e58":"# randome 10 data points\ndata.sample(10)","7f128a7e":"column_names = list(data)\ncolumn_names","6492e66b":"# counting number of missing values for each column\n\nnum_missing = (data[column_names[1:6]] == 0).sum()\nnum_missing","c183a675":"# replacing value of 0 with  nan\ndata[column_names[1:6]] = data[column_names[1:6]].replace(0,np.nan)\n\n# count number of nan values in each column\ndata.isnull().sum()","bfb157bd":"# randome 10 datapoints\ndata.sample(10)","7769df47":"# drop rows with missing values\n\nprint('Shape before dropna',data.shape)\n\ndata.dropna(inplace=True)\n\nprint('Shape after dropna',data.shape)","31285008":"data.isnull().mean()","e54ec3a4":"data = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')","79627b17":"# replacing value of 0 with  nan\ndata[column_names[1:6]] = data[column_names[1:6]].replace(0,np.nan)","7ee7eed7":"# making new column with ? as input value\ndata = data.replace('nan',np.nan).fillna('?')\ndata.sample(10)","b0c51c98":"# exporting existing csv to new csv named as sample_data\ndata.to_csv('sample_data.csv',index=False)","d20d49e6":"# now reading prepared dataset which have ? not nan into it\ndata1 = pd.read_csv('.\/sample_data.csv')","52ca833d":"data1.sample(10)","6f1ac864":"# isnull() will not work here\ndata1.isnull().sum()","0946ceff":"# can read dataset puting in picture ?\ndata2 = pd.read_csv('.\/sample_data.csv',na_values= '?')","7fc69099":"data2.sample(10)","7694e3da":"data2.isnull().sum()","c0d870cd":"(data2.isnull().mean())*100","adefb714":"data2.head()","6243537e":"all_columns = list(data2)\nall_columns","ae69a64b":"all_columns[:-1]","ec9d0ae2":"# spliting data into dependent and in-dependent columns\ndependent_columns_data = data2[all_columns[:-1]]\nin_dependent_columns_data = data2[all_columns[-1]]","12e80bb4":"in_dependent_columns_data.head(5)\n","1b5145d6":"dependent_columns_data.head(5)","82b1297f":"# total missing values before imupute\nprint(f'Missing values before imputation:\\n{dependent_columns_data.isnull().sum()}')","07d67d81":"#### Replacing each missing value with mean value of its column ####\n\n# imputing missing values in data with mean value of its column\nfrom sklearn.impute import SimpleImputer\n\n# defining imputer instance with mean strategy\nimputer = SimpleImputer(strategy='mean') # mean strategy\n\n# fitting a SimpleImputer instance on data\nimputer.fit(dependent_columns_data)","7fc1517b":"# transforming data with SimpleImputer instance\ntransformed_dependent_columns_data = imputer.transform(dependent_columns_data)","21dfe782":"# total missing values after imupute\nprint('Missing values after impute: %d'% sum(np.isnan(transformed_dependent_columns_data).flatten()))","07a48f4f":"# defining a modeling Pipeline with a SimpleImputer transform\n\"\"\"\nmodel = RandomeForestClassifier()\nimputer = SimpleImputer(strategy='mean')\npipeline = Pipeline(steps=[('i',imputer),('m',model)])\n\"\"\"","3bf33350":"# evaluating a model on a dataset with statistical mean imputation and random forest\n\nfrom sklearn.impute import SimpleImputer # simple imputer \nfrom sklearn.model_selection import RepeatedStratifiedKFold # we will go fo 10-fold \nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier # Random Fores Classifier model\nfrom sklearn.model_selection import cross_val_score\n\n\n# spliting data into dependent and in-dependent columns\ndependent_columns_data = data2[all_columns[:-1]]\nin_dependent_columns_data = data2[all_columns[-1]]\n\n########\nmodel = RandomForestClassifier()\nimputer = SimpleImputer(strategy='mean')\npipeline = Pipeline(steps=[('i',imputer),('m',model)])\ncv = RepeatedStratifiedKFold(n_splits=10,n_repeats=3,random_state=108)\n\n# model evalution using accuracy matric\nscore = cross_val_score(pipeline,dependent_columns_data,in_dependent_columns_data,scoring='accuracy',cv=cv,n_jobs=-1,verbose=5)\n\nprint('MeanAccuracy: %.3f (%.3f)' % (np.mean(score),np.std(score)))","ec104d6e":"# comparing model performance with different statistical imputation strategies\n\nfrom sklearn.impute import SimpleImputer # simple imputer \nfrom sklearn.model_selection import RepeatedStratifiedKFold # we will go fo 10-fold \nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier # Random Fores Classifier model\nfrom sklearn.model_selection import cross_val_score\n\n\n# spliting data into dependent and in-dependent columns\ndependent_columns_data = data2[all_columns[:-1]]\nin_dependent_columns_data = data2[all_columns[-1]]\n\n#######\nresults = list()\nstrategies = ['mean','median','most_frequent','constant']\nfor s in strategies:\n    pipeline = Pipeline(steps=[('i',SimpleImputer(strategy=s)),('m',RandomForestClassifier())])\n    cv = RepeatedStratifiedKFold(n_splits=10,n_repeats=3,random_state=108)\n    score = cross_val_score(pipeline,dependent_columns_data,in_dependent_columns_data,scoring='accuracy',cv=cv,n_jobs=-1,verbose=5)\n    results.append(score)\n    print('=> %s: %.3f (%.3f)' %(s,np.mean(score),np.std(score)))","f1afe749":"# plotting above model performance using boxplot   \nimport matplotlib.pyplot as plt # adding for visualization\n\nplt.figure(figsize=(10,5))\nplt.boxplot(results,labels=strategies,showmeans=True)\n\nplt.title('Comparing model performance with different statistical imputation strategies using box and whisker plot')\nplt.xlabel('Strategie')\nplt.ylabel('Score')\nplt.show()","289c07e7":"model = RandomForestClassifier()\nimputer = SimpleImputer(strategy='constant')\npipeline = Pipeline(steps=[('i',imputer),('m',model)])\ncv = RepeatedStratifiedKFold(n_splits=10,n_repeats=3,random_state=108)\n\n# model evalution using accuracy matric\nscore = cross_val_score(pipeline,dependent_columns_data,in_dependent_columns_data,scoring='accuracy',cv=cv,n_jobs=-1,verbose=5)\n\nprint('MeanAccuracy: %.3f (%.3f)' % (np.mean(score),np.std(score)))","4150f319":"## KNNImputer to impute missing values\nfrom sklearn.impute import KNNImputer # imputer\n\n# defining a KNNImputer instance\nimputer = KNNImputer(n_neighbors=5,weights='uniform',metric='nan_euclidean')\n\n# fit\nimputer.fit(dependent_columns_data)\n\n# KNNImputer instance to transform\ntransformed_dependent_columns_data = imputer.transform(dependent_columns_data)\n\nprint('Nan After KnnImputation: %d' % sum(np.isnan(transformed_dependent_columns_data).flatten()))","e11c24b6":"# KNNImputer and Model Evaluation(evaluating model on dataset transformed with KNNImputer)\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import KNNImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n\nmodel = RandomForestClassifier()\nimputer = KNNImputer()\npipeline = Pipeline(steps=[('i',imputer),('m',model)])\ncv = RepeatedStratifiedKFold(n_splits=10,n_repeats=3,random_state=108)\n\nscore = cross_val_score(pipeline,dependent_columns_data,in_dependent_columns_data,scoring='accuracy',cv=cv,n_jobs=-1,verbose=2)\nprint('Mean Accuracy: %.3f (%.3f)' % (np.mean(score),np.std(score)))","1fe563cf":"## KNNImputer and Different Number of Neighbors i.e. k\n\nresult = list()\nstrategies = [str(i) for i in [3,5,7,9,15,18,20]] # k from 1 to 20\nfor s in strategies:\n    pipeline = Pipeline(steps=[('i',KNNImputer(n_neighbors=int(s))),('m',RandomForestClassifier())])\n    cv = RepeatedStratifiedKFold(n_splits=10,n_repeats=3,random_state=101)\n    score = cross_val_score(pipeline,dependent_columns_data,in_dependent_columns_data,scoring='accuracy',cv=cv,n_jobs=-1)\n    result.append(score)\n    print('=> When K:%s %.3f %.3f' %(s,np.mean(score),np.std(score)))","e54f62ef":"# ploting model performance using boxplot   \nplt.figure(figsize=(10,5))\nplt.boxplot(result,labels=strategies,showmeans=True)\n\nplt.title('Comparing Knnimputation performance with different k values using box and whisker plot')\nplt.xlabel('K value')\nplt.ylabel('Score')\nplt.show()","666bb581":"# Using IterativeImputer to impute missing values\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n\n# defining imputer instance\nimputer = IterativeImputer(n_nearest_features=None,imputation_order='ascending')\n\n# fitting on dataset\nimputer.fit(dependent_columns_data)\n\n# transforming dataset\ntransformed_dependent_columns_data = imputer.transform(dependent_columns_data)\n\nprint('Nan after IterativeImputer: %d' % sum(np.isnan(transformed_dependent_columns_data).flatten()))","4414cf6a":"# evaluating model on a dataset transformed with IterativeImput\nmodel = RandomForestClassifier()\nimputer = IterativeImputer()\npipeline = Pipeline(steps=[('i',imputer), ('m',model)])\n\ncv = RepeatedStratifiedKFold(n_splits=10,n_repeats=3,random_state=1)\nscore = cross_val_score(pipeline,dependent_columns_data,in_dependent_columns_data,scoring='accuracy',cv=cv,n_jobs=-1)\n\nprint('Mean Accuracy: %.3f (%.3f)'% (np.mean(score), np.std(score)))","aaad551c":"# comparing model performance with different data imputation order in IterativeImputer\n\nresult = list()\nstrategies = ['ascending','descending','roman','arabic','random']\nfor s in strategies:\n    pipeline = Pipeline(steps=[('i',IterativeImputer(imputation_order=s)),('m',RandomForestClassifier())])\n    cv = RepeatedStratifiedKFold(n_splits=10,n_repeats=3,random_state=101)\n    score = cross_val_score(pipeline,dependent_columns_data,in_dependent_columns_data,scoring='accuracy',cv=cv,n_jobs=-1)\n    result.append(score)\n    print('=>%s %.3f (%.3f)'% (s,np.mean(score),np.std(score)))","380e0ea7":"# ploting model performance using boxplot   \nplt.figure(figsize=(10,5))\nplt.boxplot(result,labels=strategies,showmeans=True)\n\nplt.title('Comparing imputation order performance with different imputations using box and whisker plot')\nplt.xlabel('Imputation order')\nplt.ylabel('Score')\nplt.show()","7e41e9b2":"### 2.Removing rows with Missing Values\n* using `dropna()` function that can be used to drop either columns or rows with missing data","2891d130":"### 4.Imputing Missing values using statistics method\nUsing `SimpleImputer class` which support statistics imputation<br>\n* scikit-learn machine learning library provides `SimpleImputer class` that supports statistical imputation\n\n**`SimpleImputer(strategy='mean')`**","6e56c1fc":"<center> <h4 style=\"background-color:orange\"><br>Bird Eye View(Data Cleaning)<br><\/h4><\/center>\n\n1. `Basic Cleaning`\n    * Redundant Samples\n    * Redundant Features\n2. `Outliers Cleaning`\n    * Extream Values\n3. `Missing Values Cleaning`\n    * Mark\n    * Impute\n","4471951d":"Minor fluctuation around mean performance (green triagles) can be seen","1abb435c":"\n<center> <h6 style=\"background-color:orange\"><br>Data Preparation Tasks<br><\/h6><\/center>\n\n1. `Data Cleaning` ==>  Identifying and correcting mistakes or errors in data\n2. `Feature Selection` ==>  Identifying those input variables that are most relevant to task \n3. `Data Transforms` ==>  Changing the scale or distribution of variables\n4. `Feature Engineering` ==>  Deriving new variables from available data\n5. `Dimensionality Reduction` ==>  Creating compact projections of data","a480f563":"<a id='1'><\/a><center> <h4 style=\"background-color:orange; color:white\" ><br>Loading data again for new case<br><\/h4>\n\nMissing values can be seen in these forms: \n* `Nothing at all () `\n* `Empty string (' ')`\n* `String NULL`\n* `String undefined `\n* `N\/A `\n* `NaN`\n* `Number 0`","c13184a0":"### 4.2 Comparing Different Statistical Imputational Strategies\nIts impossible to say which static imputation stratgey is best so using all one by one and finding best one\n1. mean\n2. most frequent (mode)\n3. constant(0) strategies <br>\n4. median\n\nEnd with taking mean accuracy of each approach","08ad50d8":"#### 6.2 IterativeImputer and Different Imputation Order\n* Imputation is performed in ascending order from feature with least missing values to feature with most\n* We can experiment with `different imputation order strategies` such as \n    1. `Descending` \n    2. `Right-to-left (Arabic)` \n    3. `Left-to-right (Roman)`\n    4. `Random`","4d12e0d9":"### 4.1.SimpleImputer(mean) and Model Evaluation\nIt is a good practice to `evaluate machine learning models on a dataset using k-fold cross validation` <br>\n**`Note:`**\n* To correctly apply statistical missing data imputation and `avoid data leakage`, \n    * it is required that statistics calculated for each column are `calculated on training dataset only` then \n    * applied to train and test sets for each fold in dataset <br>\n\n`Use of:`\n* `sklearn.pipeline import Pipeline` | model pipeline to avoide data leakage\n    * 1st Statical imputation\n    * 2nd model\n* `sklearn.impute import SimpleImputer` | with `mean` strategy\n* `sklearn.model_selection import RepeatedStratifiedKFold`\n\n\nEvaluating Mean-imputed dataset and Random forest modeling pipeline for this dataset with repeated 10-fold cross-validation","0568f6ce":"#### Use of index=False\n","154c5775":"<center> <h4 style=\"background-color:orange\"><br>Outlier Identification and Removal<br><\/h4><\/center>\n\nMost of the time dataset can contain `Extreme Values` which can be outside range of what is expected and we call thouse values `Outliers`  <br>\nIn general ML modeling can be improved by understanding and even removing these outlier values\n\n\n\nTo deal with such values(Outliar values) we use: <br>\n1. `Univariate Statistics` ==> to identify and remove outliers from a data\n    * `Standard Deviation`\n    * `Interquartile Range`\n2. `Outlier Detection Model` ==> to identify and remove rows from training dataset in order to increase predictive modeling performance\n    * Automatic Outlier Detection and removal using  `LOF` => local outlier factor\n","92c1f670":"`Best number of neighbors cannot be said for this` as KNNImputer takes 5 as default <br>\n\n* `K` : `key hyperparameter for the KNN algorithm`\n    * controls number of nearest neighbors that are used to contribute to a prediction\n\n`It is good practice to test a suite of different values for k`","00f1eb3a":"# Mark and Remove Missing Data or perform Imputation\n1. `Mark missing values` i.e. `Mark Invalid or Corrupt values as missing in dataset`\n2. `Remove rows with Missing Values`\n3. Find `Number and % of all missing values` for each Column\n4. Impute Missing values using Statistics Methos(`Statistical Imputation`) as `Data Preparation Method`\n    * `Column mean value`\n    * `Column median value`\n    * `Column mode value` (most frequent value)\n    * `Some Constant value`\n5. `Knn Imputation` (good then all of above)\n6. `Iterative Imputation` (defining a model to predict each missing feature)","52080787":"### 5. Knn Imputation\n**`KnnImpute` surpasses commonly used row average method (as well as filling missing values with zeros)** <br>\n\nConfiguration of KNN imputation often involves \n1. `selecting distance measure`\n    * will not include NaN values when calculating distance between members of training dataset\n    * set via `metric argument` `default : Euclidean`\n2. `number of contributing neighbors` for each prediction(k-hyperparameter of KNN algorithm)\n    * number of `neighbors set to 5 by default`\n    * can be configured by `n_neighbors` argument\n3. `weights` distance measure can be weighed proportional to distance between instances(rows), this is set to a `uniform weighting by default`","8f1e5c3b":"Using diabetes dataset <br>\nIt is a binary classification problem <br>\nNaive model can achieve an accuracy of about 65 percent on this dataset (A good score is about 77 percent) <br>\n`This dataset is known to have missing values`\n* there are missing observations for some columns that are marked as a zero value\n    * zero for body mass index or blood pressure is invalid","defd8a20":"### 6. Iterative Imputation\n* This sophisticated approach involves defining a model to predict each missing feature modeled as a function of all other features and to repeat this process of estimating feature values multiple times <br>\n* Repetition allows refined estimated values for other features to be used as input in subsequent iterations of predicting missing values this is iterative imputation<br>\n* Each feature is imputed sequentially, one after other, allowing prior imputed values to be used as part of a model in predicting subsequent features <br>\n\n`Imputing missing values with iterative models as a data preparation method` <br>\n\nThis approach may be generally referred to as: <br>\n1. `Fully Conditional Specification` (FCS) or \n2. `Multivariate Imputation by chained equations` (MICE)\n\n\nDifferent `Regression Algorithms` can be used to estimate missing values for each feature, in general \n* `Linear Methods` are often used for simplicity\n    * `Number of iterations of procedure` is often `kept small as 10`\n","08a787f6":"`Mode is also known as most Frequent`<br>\nAbove distribution of accuracy scores for constant strategy may be better than other strategies","b6c8c550":"\n<center> <h2 style=\"background-color:orange\"><br>Data Cleaning<br><\/h2><\/center>\n\nIdentifying and correcting mistakes or errors in data<br>\n\nOnce messy, noisy, corrupt observations are identified, they can be addressed\n* This might involve removing a row or a column \n* It might involve replacing observations with new values \n\nThere are general data cleaning operations that can be performed, such as:\n1. `Using statistics to define normal data and identify outliers`\n2. `Identifying columns that have Same Value i.e No Variance and removing them`\n3. `Identifying duplicate rows of data and removing them`\n4. `Marking empty values as missing`\n5. `Imputing missing values using statistics or a learned model`\n\n`NOTE`: <br>\nData cleaning is typically performed first prior to other data preparation operations\n\n\n<h2><center>Basic Data Cleaning<\/center><\/h2>\n\n1. `Identify columns that containa Single Value`\n2. `Delete columns that contain a Single Value`\n3. `Consider columns that have Very Few Values`\n4. `Remove columns that have a Low Variance`\n5. `Identify rows that contain duplicate data`\n6. `Delete rows that contain duplicate data`\n\n<h2><center>Outlier Identification and Removal<\/center><\/h2>\n\n1. `Standard deviation Method`\n2. `Interquartile range Method`\n3. `Automatic outlier Detection`\n\n<h2><center>Remove Missing Data<\/center><\/h2>\n\n1. `Mark missing values and remove rows with missing values`\n\n<h2><center>Statistical Imputation<\/center><\/h2>\n\n1. `Using simple-imputer`\n\n<h2><center>KNN Imputation<\/center><\/h2>\n\n1. `Using kNN-imputer`\n\n\n<h2><center>Iterative Imputation<\/center><\/h2>\n\n1. `Using Iterative-imputer`","c84cb167":"### 4.3 Creating final modeling pipeline with Constant imputation strategy and random forest algorithm\n* Will make a prediction for new data","3f079da5":"Above I am Applying DataImputation to each fold of cross-validation procedure <br>\n`Pipeline is evaluated using three repeats of 10-fold cross-validation`","3ddf2d14":"### 1.Marking missing values","f4f512ce":"---\nIt is good practice to `evaluate ml models` on a dataset using `k-fold cross-validation` <br>\n\nTo correctly apply nearest neighbor missing data imputation and `avoid data leakage`,\n1. it is required that models calculated for each column are calculated on training dataset only\n2. then applied to train and test sets for each fold in dataset <br>\nFor this `creating a modeling pipeline` is necessory using `Pipeline Class`\n1. `first step` => nearest neighbor imputation\n2. `second step` => model","2abd7d8d":"### Main Code Step's","90f72763":"Here we have to consider number-0 as Null value\n","cda7cfc4":"Some column have min value as 0, indicates an invalid or missing value <br>\ncol 1(Glucose) to 5(BMI) have an invalid zero minimum value","eee3c1a7":"If inplace set to `False` then pandas will return a copy of Dataframe with operations performed on it","dc00f8c5":"#### 6.1 IterativeImputer and Model Evaluation","7b485549":"### 3.Finding  % of all missing values for each Column","aeec8f2c":"### 3.Finding  % of all missing values for each Column"}}