{"cell_type":{"a44c48c5":"code","08bad023":"code","da07884b":"code","e1bea590":"code","60897a2e":"code","86148b20":"code","369a3b40":"code","aa159e6b":"code","905cf0d1":"code","e5e13937":"code","a22f6277":"code","c31f97a9":"code","19598d88":"code","278875c8":"code","408fb64b":"code","346a618f":"code","f9f7dfdc":"code","4f19077b":"code","6a5b698c":"code","3ddba07b":"code","534733ab":"code","fea5add4":"code","c3a5f8f0":"code","8c796f4f":"markdown","779ef4d2":"markdown","437ccbd3":"markdown","e9dd8b49":"markdown","1588a4f5":"markdown","bf959440":"markdown","499c422d":"markdown","c6e8a3ee":"markdown","4cddd713":"markdown"},"source":{"a44c48c5":"import tensorflow as tf\nprint(tf.__version__)","08bad023":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom  cv2 import imread\nfrom os import getcwd, listdir, path\nimport os # I dont know why, just bear with me\nfrom time import time\n\nfrom tensorflow.data import Dataset\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import Model, Sequential\nfrom tensorflow.keras.layers import Dense, Input, BatchNormalization, Flatten\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n","da07884b":"#m = listdir(\"data\\math\\math_oper\")\nm = os.listdir(\"..\/input\/math800a\")\nprint(m)","e1bea590":"def get_tf_image(filename,d):\n    im = imread(path.join(d,filename))\n    return tf.convert_to_tensor(im)","60897a2e":"def get_generator( directory, classes, rescale = 1\/255.,batch_size=20, seed=None, target_size=(64, 64, )):\n    img_gen = ImageDataGenerator( rescale =  rescale, validation_split=0.1)\n    \n    return img_gen.flow_from_directory(\n        directory, \n        batch_size=batch_size, \n        #color_mode=\"rgb\",# \"grayscale\", \"rgb\", \"rgba\"\n        target_size=target_size,#(64, 64, ), #pixels in each image\n        classes=classes,#names of subdirs to look into\n        class_mode=\"categorical\", #keep categorical, since many subdirs\n        \n        seed=seed #random number \n    )\n","86148b20":"classes=[str(i) for i in range(1,67,2)]\n\ntrain_dir = \"..\/input\/math800a\"\nvalid_dir = \"..\/input\/math800a\/valid\"\n\ntest_dir = \"..\/input\/math800a\/test\" \n\nclasses = ['c_sub - Copy','c_div - Copy','c_add_ - Copy','c_mul - Copy'] #Just folder names\n\ntrain_generator = get_generator(train_dir, classes, rescale = 1\/255., target_size=(272, 96, ))\nvalid_generator = get_generator(valid_dir, classes, rescale = 1\/255., target_size=(272, 96, ))\ntest_generator = get_generator(test_dir, classes, rescale = 1\/255.  , target_size=(272, 96, ))","369a3b40":"# Display a few images and labels from the training set\n\nbatch = next(train_generator)\nbatch_images = np.array(batch[0])\nbatch_labels = np.array(batch[1])\nlsun_classes = classes #[str(i) for i in range(1,67,2)]#['classroom', 'conference_room', 'church_outdoor']\n\nprint(batch_images[1].shape)\n","aa159e6b":"def get_model(input_shape, output_shape=(1,),name=None):\n    model = Sequential(name=name)\n    \n    model.add( Input(input_shape))\n    model.add(Flatten())\n    model.add( BatchNormalization(momentum=0.8))\n    model.add( Dense(100, activation='relu'))\n    model.add( BatchNormalization(momentum=0.8))\n    #model.add(  Dense(33, activation='sigmoid'))\n    model.add(  Dense(output_shape[0], activation='sigmoid'))\n    return model","905cf0d1":"def get_model2(input_shape, output_shape=(1,),name=None):\n    model = Sequential(name=name)\n    \n    model.add( Input(input_shape))\n    model.add(Flatten())\n\n    model.add( BatchNormalization(momentum=0.8))\n    model.add( Dense(260, activation='relu'))\n    \n    model.add( BatchNormalization(momentum=0.8))\n    model.add(  Dense(20, activation='relu'))\n    \n    model.add(  Dense(output_shape[0], activation='sigmoid'))\n    \n    return model","e5e13937":"def get_model3(input_shape, output_shape=(1,),name=None):\n    model = Sequential(name=name)\n    \n    model.add( Input(input_shape))\n    model.add(Flatten())\n    \n    model.add( BatchNormalization(momentum=0.8))\n    model.add( Dense(8000, activation='relu'))\n    \n    model.add( BatchNormalization(momentum=0.8))\n    model.add(  Dense(400, activation='relu'))\n    \n    model.add( BatchNormalization(momentum=0.8))\n    model.add(  Dense(20, activation='relu'))\n    \n    model.add(  Dense(output_shape[0], activation='sigmoid'))\n    \n    return model","a22f6277":"###Get the shape:\nm = next(train_generator)\n\nprint(m[0].shape)\nprint(m[1].shape)\n\ninput_shape_ = m[0][0].shape\noutput_shape_ = m[1][0].shape\n\nmodel_no_callback = get_model2(input_shape=input_shape_, output_shape=output_shape_)\nmodel_no_callback.summary()","c31f97a9":"# Compile the model\noptimizer_ = optimizers.Adam(learning_rate=1e-2)\n#model.compile(optimizer=optimizer_, loss='categorical_crossentropy', metrics=['accuracy'])\n#model.compile(optimizer=optimizer_, loss='kl_divergence', metrics=['kullback_leibler_divergence'])\n# etc..\n\n#090221:\nmodel_no_callback.compile(optimizer=optimizer_, loss='categorical_crossentropy', metrics=['accuracy'])\n\n\"\"\"\n'categorical_crossentropy' is taken since we have more than two classes (we cannot use \"binary_crossentropy\") and\n    the classes are ctaegorical (not numerical) and \n    the classes are not converte to one-hot as in [0,0,1,0]\n    \n\"\"\"","19598d88":"# Train the model\ndef train_model(model, epochs = 3, num_images = 800, batch_size = 20,\n                train_dir = \"..\/input\/math800a\",\n                valid_dir = \"..\/input\/math800a\/valid\",\n                test_dir  = \"..\/input\/math800a\/test\",\n                classes = ['c_sub - Copy','c_div - Copy','c_add_ - Copy','c_mul - Copy'],\n                callbacks=[]\n    ):\n\n    #steps per epoch:\n    spe = num_images \/\/ batch_size ##15, #300 images in batches of 20\n\n    ##reset generators:\n    train_generator = get_generator(train_dir, classes, batch_size=batch_size, rescale = 1\/255., target_size=(272, 96, ))\n    valid_generator = get_generator(valid_dir, classes, batch_size=batch_size, rescale = 1\/255., target_size=(272, 96, ))\n    test_generator  = get_generator(test_dir,  classes, batch_size=batch_size, rescale = 1\/255., target_size=(272, 96, ))\n\n    c = time()\n    history = model.fit(train_generator, \n                        #validation_data=valid_generator,\n                        #validation_split=0.1, ## \"Nice try!\"(C) -Tensorflow \n                        steps_per_epoch=spe,\n                        callbacks=callbacks,\n                        epochs=epochs)\n    d = time()-c\n    print(\"duration: {} s\".format(d))\n    return history","278875c8":"history = train_model(model_no_callback, epochs = 50,\n            #train_dir = \"..\/input\/math800a\",\n            #valid_dir = \"..\/input\/math800a\/valid\",\n            #test_dir  = \"..\/input\/math800a\/test\",\n            #classes = ['c_sub - Copy','c_div - Copy','c_add_ - Copy','c_mul - Copy'],\n            #callbacks=[],\n             num_images = 800, batch_size = 20\n    )","408fb64b":"model_no_callback.evaluate(test_generator)","346a618f":"model_no_callback.save(\"model_no_callback-50epx-valid_01_090221\")\nmodel_no_callback.save_weights(\"model_no_callback-50epx-valid_01_090221_weights\")","f9f7dfdc":"# summarize history for accuracy\n'''\nlegend = []\nfor k in history.history.keys():\n    plt.plot(history.history[k])#['kullback_leibler_divergence'])\n    plt.title('model performance')\n    plt.ylabel(\"\")#(k)#('measure')\n    plt.xlabel('epoch')\n    legend.append(k)\n#plt.legend(['train', 'test'], loc='upper left')\nplt.legend(legend, loc='upper left')\nplt.show()\n'''\n## Above is good for cramping all graphs in one square. Some people like it.\n\nplt.title('model performance')\nfor k in history.history.keys():\n    plt.plot(history.history[k])#['kullback_leibler_divergence'])\n    plt.ylabel(k)\n    plt.xlabel('epoch')\n    #plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    \nplt.plot(history.history['val_accuracy'])\nplt.title('model performance')\nplt.ylabel(\"val_acc\")\nplt.xlabel('epoch')\n#plt.legend(['train', 'test'], loc='upper left')\n#plt.legend(legend, loc='upper left')\nplt.show()","4f19077b":"from tensorflow.keras.callbacks import Callback","6a5b698c":"\nclass CustomCallback(Callback):\n    \n    def on_train_begin(self, logs=None):\n        # logs is the information saved as \"history\" after fitting\n        # to add more parameters use python dictionary notation:\n        logs['val_loss'] = []\n        logs['val_accuracy'] = []\n        keys = list(logs.keys())\n        print(\"Starting training; got log keys: {}\".format(keys))\n\n    def on_epoch_end(self, epoch, logs=None):\n        ## evaluate  on the  validation data to compute the va_loss and val_accuracy :\n        [curr_val_loss, curr_val_acc] = model.evaluate(valid_generator) \n        \n        keys = list(logs.keys())\n        print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n        \n        print(\"...loss on val data: {}\".format(curr_val_loss))\n        \n        ## Records for info for the current epoch:\n        logs['val_loss'] = curr_val_loss ## Should be logs['val_loss'].append(curr_val_loss), but whatever works..\n        logs['val_accuracy'] = curr_val_acc ## Same appalment!\n","3ddba07b":"model = get_model2(input_shape=input_shape_, output_shape=output_shape_)\n\nmodel.compile(optimizer=optimizer_, loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = train_model(model, epochs = 50,\n            #train_dir = \"..\/input\/math800a\",\n            #valid_dir = \"..\/input\/math800a\/valid\",\n            #test_dir  = \"..\/input\/math800a\/test\",\n            #classes = ['c_sub - Copy','c_div - Copy','c_add_ - Copy','c_mul - Copy'],\n            callbacks=[CustomCallback()],\n             num_images = 800, batch_size = 20\n    )","534733ab":"history.history.keys()","fea5add4":"plt.title('model performance')\nfor k in history.history.keys():\n    plt.plot(history.history[k])#['kullback_leibler_divergence'])\n    plt.ylabel(k)\n    plt.xlabel('epoch')\n    #plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    ","c3a5f8f0":"model.save(\"model2-50epx-valid_01_090221\")\nmodel.save_weights(\"model2-50epx-valid_01_090221_weights\")","8c796f4f":"## Plot the Accuracy","779ef4d2":"## Train a model with callback recording *validation accuracy*","437ccbd3":"## Instanciate and compile a model","e9dd8b49":"Above model does not track validation accuracy. Probably due to the fact that we feed the data using `ImageDataGenerator.flow_from_directory(...)`. \n\nHence the error: `KeyError: 'val_accuracy'`\n\nTo obtain the accuracy, we can use following callback.\n\nModifying the example from https:\/\/www.tensorflow.org\/guide\/keras\/custom_callback","1588a4f5":"## Define some models","bf959440":"## Create a data generator","499c422d":"## Train the model without callbacks","c6e8a3ee":"## Write the trainig procedure","4cddd713":"Similarly, metric could be computed at each batch. Left as an exercise :DD"}}