{"cell_type":{"b53a83a0":"code","46cd8ee5":"code","e0f3945b":"code","958cfaa9":"code","10add78a":"code","0465aaf3":"code","ee59b7fa":"code","cd32eaa8":"code","cecfab55":"code","6cbdc9fb":"code","01ac7b4d":"code","9e97160e":"code","c96dd895":"code","58f68924":"code","11625a18":"code","e57be7ff":"markdown","6f6223a6":"markdown","04ca802d":"markdown","9295d0fe":"markdown","6d53ee72":"markdown","9f3afe32":"markdown","524a6a02":"markdown","b718fd53":"markdown","ddeb0889":"markdown","1a7f735c":"markdown"},"source":{"b53a83a0":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import ParameterGrid\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBRegressor\n\nimport optuna\n        \ninput_path = Path('\/kaggle\/input\/tabular-playground-series-feb-2021\/')","46cd8ee5":"train = pd.read_csv(input_path \/ 'train.csv', index_col='id')\ntest = pd.read_csv(input_path \/ 'test.csv', index_col='id')\nsubmission = pd.read_csv(input_path \/ 'sample_submission.csv', index_col='id')\ntarget = train.pop('target')","e0f3945b":"# Concatenate train and test sets before encoding to guarantee that they will have the same columns\ntraintest = pd.concat([train, test])\ncat_features = [f'cat{i}' for i in range(10)]\ndummies = pd.get_dummies(traintest, columns=cat_features, drop_first=True)\n\n# Create new train and test sets with one-hot encodings\ntrain_ohe = dummies.iloc[:train.shape[0], :]\ntest_ohe = dummies.iloc[train.shape[0]:, :]","958cfaa9":"display(train_ohe.head())","10add78a":"X_train, X_valid, y_train, y_valid = train_test_split(train_ohe, target, test_size=0.1, random_state=0)","0465aaf3":"# Base model\nmodel = XGBRegressor(tree_method='gpu_hist')\nmodel.fit(X_train, y_train)","ee59b7fa":"y_pred = model.predict(X_valid)\nrmse = mean_squared_error(y_valid, y_pred, squared=False)\nprint('RMSE =', f'{rmse:0.5f}')","cd32eaa8":"# Grid search\n\n#grid = {\n#    'random_state': [0], \n#    'n_estimators': [100, 500, 1000, 2000, 5000, 10000],\n#    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n#    'learning_rate': [0.001, 0.01, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.60, 1.00],\n#    'reg_lambda': [0.0, 0.01, 0.05, 0.10, 1.0, 10.0],\n#    'reg_alpha': [0.0, 0.01, 0.05, 0.10, 1.0, 10.0],\n#    'gamma': [0.0, 0.01, 0.05, 0.10, 1.0, 10.0],\n#    'subsample': [0.8, 0.9, 1.0],\n#    'colsample_bytree': [0.1, 0.2, 0.3, 0.4, 0.5],\n#    'tree_method': ['gpu_hist'],\n#}","cecfab55":"#scores = []\n#for g in ParameterGrid(grid):\n#    model = XGBRegressor()\n#    model.set_params(**g)\n#    model.fit(X_train, y_train)\n#    y_pred = model.predict(X_valid)\n#    score = mean_squared_error(y_valid, y_pred, squared=False)\n#    scores.append(score)\n#    print('RMSE =', f'{score:0.5f}', 'Parameters:', g)\n#best_idx = np.argmin(scores)\n#print('Best score: ', scores[best_idx], ParameterGrid(grid)[best_idx])","6cbdc9fb":"def objective(trial):\n    params = {\n        'random_state': 0,\n        'n_estimators': trial.suggest_categorical('n_estimators', [10000]),\n        'max_depth': trial.suggest_int('max_depth', 3, 8),\n        'learning_rate': trial.suggest_float('learning_rate', 0.001, 1.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10),\n        'gamma': trial.suggest_float('gamma', 0.0, 10),\n        'subsample': trial.suggest_categorical('subsample', [0.8, 0.9, 1.0]),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.1, 0.2, 0.3, 0.4, 0.5]),\n        'tree_method':'gpu_hist'    # comment this line if GPU is off\n    }\n    model = XGBRegressor(**params) \n    model.fit(X_train, y_train, eval_set=[(X_valid,y_valid)], early_stopping_rounds=1000, verbose=0)\n    y_pred = model.predict(X_valid)\n    rmse = mean_squared_error(y_valid, y_pred, squared=False)\n    \n    return rmse","01ac7b4d":"%%time\nstudy = optuna.create_study(direction='minimize',sampler=optuna.samplers.TPESampler(seed=0))\nstudy.optimize(objective, n_trials=100)\nprint('Number of finished trials:', len(study.trials))\nprint('Best parameters:', study.best_trial.params)\nprint('Best RMSE:', study.best_trial.value)","9e97160e":"optuna.visualization.plot_optimization_history(study)","c96dd895":"params = study.best_params\nparams['random_state'] = 0\nparams['n_estimators'] = 10000\nparams['tree_method'] = 'gpu_hist'","58f68924":"n_folds = 10\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=0)\ny_pred = np.zeros(test_ohe.shape[0])\n\nfor fold, (train_index, valid_index) in enumerate(kf.split(train_ohe, target)):\n    print(\"Running Fold {}\".format(fold + 1))\n    X_train, X_valid = pd.DataFrame(train_ohe.iloc[train_index]), pd.DataFrame(train_ohe.iloc[valid_index])\n    y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n    model = XGBRegressor(**params)\n    model.fit(\n        X_train,\n        y_train,\n        eval_set=[(X_valid, y_valid)],\n        early_stopping_rounds=1000,\n        verbose=0,\n    )\n    y_pred += model.predict(test_ohe) \/ n_folds\n    \nprint(\"Done!\")","11625a18":"submission['target'] = y_pred\nsubmission.to_csv('xgboost_optuna.csv')","e57be7ff":"# Visualize optimization history","6f6223a6":"# Preliminary tests to estimate ranges for the parameters","04ca802d":"These are some of the values I tried for each parameter.","9295d0fe":"# Load libraries and data","6d53ee72":"# Encode categorical variables with one-hot encoding (ohe)\n\nFrom what I've read, XGBoost only works with categorical variables encoded in this manner.","9f3afe32":"### Please feel free to add comments and suggestions. Thanks! \ud83d\ude0a","524a6a02":"# XGBoost with parameter tunning using Optuna","b718fd53":"# Set objective function for Optuna with parameters and their ranges\n\nIn order to search for the best parameter values, I created the following function with the parameters I wanted to be tuned. To set the ranges, I did some preliminary tests varying them individually. Finally, I put everything together as shown below.","ddeb0889":"# Recover best parameters found and build final predictions","1a7f735c":"This notebook implements Optuna to tune XGBoost parameters. A considerable part of the code was borrowed from [this notebook](https:\/\/www.kaggle.com\/hamzaghanmi\/xgboost-hyperparameter-tuning-using-optuna) and I also took advice from [this blog post](https:\/\/towardsdatascience.com\/fine-tuning-xgboost-in-python-like-a-boss-b4543ed8b1e) to select which parameters should be tuned."}}