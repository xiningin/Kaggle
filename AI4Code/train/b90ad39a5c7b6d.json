{"cell_type":{"206fbd47":"code","50501590":"code","51f6413c":"code","4632cb39":"code","e9355d4d":"code","1844ba7e":"code","6f77e250":"code","8bba537b":"code","bcbdd179":"code","ccb5bf8d":"code","a7a92058":"code","ac057e01":"code","ebf62819":"code","19da3daf":"code","b7098acb":"code","177c8153":"code","d0ff054e":"code","0b3728b5":"code","2048ebed":"code","03bae637":"code","ff5758af":"code","cb78ea6c":"code","8be9474f":"code","f71d2ca5":"code","4eec1a6d":"code","8dd296ee":"code","1abd3e38":"code","b8869057":"code","bde3ae77":"code","11125598":"code","d64656e0":"markdown","13467925":"markdown","1d1c6d53":"markdown","378d05bc":"markdown","3c46b09b":"markdown","6fd21802":"markdown","1872ccbe":"markdown","97d6a5f2":"markdown","13378d38":"markdown","5e15f677":"markdown","848c1b19":"markdown","3909f503":"markdown","b38ea683":"markdown","178103d9":"markdown","2891d321":"markdown","22a67f57":"markdown","e5fc2d5a":"markdown","f0840197":"markdown","fa31a62c":"markdown","89828ac2":"markdown","79d784d1":"markdown","d6a32c55":"markdown","f731c51b":"markdown"},"source":{"206fbd47":"import datetime\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight') # useful for time series\n\nfrom scipy.stats import norm\nfrom colorama import Fore\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","50501590":"train = pd.read_csv(\"..\/input\/daily-climate-time-series-data\/DailyDelhiClimateTrain.csv\",\n                    index_col = \"date\", parse_dates = True)\n\ntest = pd.read_csv(\"..\/input\/daily-climate-time-series-data\/DailyDelhiClimateTest.csv\",\n                    index_col = \"date\", parse_dates = True)\n\ntrain.tail()","51f6413c":"print(f\"Train shape : {train.shape}\\nTest shape : {test.shape}\\n\")\ntrain.info()\ntest.info()","4632cb39":"axs = train.plot(subplots=True, layout=(2,2), figsize=(20,6))\nfor i, col in enumerate(train.columns) :\n    axs[i\/\/2][i%2].set_title(col)\n    axs[i\/\/2][i%2].get_legend().remove()\nplt.show()","e9355d4d":"def var_visualisation(df, var, color):\n    colors = {\"orange\":\"YlOrRd\", \"green\":\"YlGn\", \"blue\":\"PuBu\", \"purple\":\"BuPu\"}\n    var_col = df[var]\n    \n    fig, axs = plt.subplots(1,2, figsize=(15,4))\n    fig.suptitle(var, weight='bold', fontsize=16)\n\n    axs[0].set_title(f\"{var} Displot\")\n    sns.distplot(var_col, fit=norm, color=color, ax=axs[0])\n    axs[0].legend([f\"Skew : {var_col.skew():.2f}\\nKurt  : {var_col.kurt():.2f}\"])\n\n    axs[1].set_title(f\"Boxplot - {var}\")\n    sns.boxplot(x=var_col, palette=colors[color], ax=axs[1])\n                                                        \n    plt.tight_layout()\n    plt.show()\n    \ncolors = [\"blue\", \"purple\", \"orange\", \"green\"]\nfor i, col in enumerate(train.columns):\n    var_visualisation(train, col, colors[i])","1844ba7e":"from sklearn.neighbors import LocalOutlierFactor\n\nclf = LocalOutlierFactor()\ny_pred = clf.fit_predict(train)\nscores = clf.negative_outlier_factor_\nthreshold = -2.5\ntrain = train.drop(train.loc[scores < threshold].index)","6f77e250":"for i, col in enumerate(train.columns) :\n    fig, ax = plt.subplots(1,3, figsize=(18,3))\n    ax[0].set_title(col)\n    ax[1].set_title(\"Weekly \" + col)\n    ax[2].set_title(\"Monthly  \"+ col)\n    train[col].plot(ax=ax[0])\n    train[col].resample('7D').mean().plot(ax=ax[1])\n    train[col].resample('M').mean().plot(ax=ax[2])\n    plt.show()","8bba537b":"df = train.resample('7D').mean() #weekly resampling\naxs = df.plot(subplots=True, layout=(2,2), figsize=(20,6))\nfor i, col in enumerate(train.columns) :\n    axs[i\/\/2][i%2].set_title(col)\n    axs[i\/\/2][i%2].get_legend().remove()\nplt.show()","bcbdd179":"window = 52\nfig, axs = plt.subplots(2,2, figsize=(20,8))\nfor i, col in enumerate(train.columns) :\n    sns.lineplot(x=df.index, y=df[col], ax=axs[i\/\/2][i%2])\n    sns.lineplot(x=df.index, y=df[col].rolling(window=window, center=True).mean(), ax=axs[i\/\/2][i%2], label='rolling mean')\n    sns.lineplot(x=df.index, y=df[col].rolling(window=window, center=True).std(), ax=axs[i\/\/2][i%2], label='rolling std')\nplt.tight_layout()\nplt.show()","ccb5bf8d":"from statsmodels.tsa.stattools import adfuller\n\nfor col in df.columns :\n    res = adfuller(df[col].values)\n    print(col+'\\n')\n    print(f\"Test statistic : {res[0]}\\np-value : {res[1]}\\nNb of lags : {res[2]}\\n\"\n          f\"Nb obs : {res[3]}\\nCritical values : {res[4]}\\n\")","a7a92058":"def visualize_adfuller_results(series, title, ax):\n    result = adfuller(series)\n    significance_level = 0.05\n    adf_stat = result[0]\n    p_val = result[1]\n    crit_val_1 = result[4]['1%']\n    crit_val_5 = result[4]['5%']\n    crit_val_10 = result[4]['10%']\n\n    if (p_val < significance_level) & ((adf_stat < crit_val_1)):\n        linecolor = 'forestgreen' \n    elif (p_val < significance_level) & (adf_stat < crit_val_5):\n        linecolor = 'orange'\n    elif (p_val < significance_level) & (adf_stat < crit_val_10):\n        linecolor = 'red'\n    else:\n        linecolor = 'purple'\n    sns.lineplot(x=df.index, y=series, ax=ax, color=linecolor)\n    ax.set_title(f'ADF Statistic {adf_stat:0.3f}, p-value: {p_val:0.3f}\\nCritical Values 1%: {crit_val_1:0.3f}, 5%: {crit_val_5:0.3f}, 10%: {crit_val_10:0.3f}', fontsize=14)\n    ax.set_ylabel(ylabel=title, fontsize=14)\n    \nfig, axs = plt.subplots(2,2, figsize=(18, 8))\nfor i, col in enumerate(df.columns) :\n    visualize_adfuller_results(df[col].values, col, axs[i\/\/2][i%2])\nplt.tight_layout()\nplt.show()","ac057e01":"df[\"wind_speed_diff_1\"] = df[\"wind_speed\"].diff(periods = 1)\ndf[\"wind_speed_diff_1\"].iloc[0] = 0\ndf.head()","ebf62819":"fig, ax = plt.subplots(1,2, figsize=(18, 4))\nvisualize_adfuller_results(df[\"wind_speed\"].values, \"wind_speed\", ax[0])\nvisualize_adfuller_results(df[\"wind_speed_diff_1\"].values, \"wind_speed_diff_1\", ax[1])\nplt.show()","19da3daf":"df['year'] = df.index.year\ndf['month'] = df.index.month\ndf['day'] = df.index.day\ndf['day_of_year'] = df.index.dayofyear\ndf['week_of_year'] = df.index.weekofyear\ndf['quarter'] = df.index.quarter\ndf['season'] = df['month'] % 12 \/\/ 3 + 1\n\nmonth_in_year = 12\ndf['month_sin'] = np.sin(2*np.pi*df['month']\/month_in_year)\ndf['month_cos'] = np.cos(2*np.pi*df['month']\/month_in_year)","b7098acb":"from statsmodels.tsa.seasonal import seasonal_decompose\n\ndef plot_seasonal(res, axs):\n    res.observed.plot(ax=axs[0])\n    axs[0].set_ylabel('Observed')\n    res.trend.plot(ax=axs[1], color='orange')\n    axs[1].set_ylabel('Trend')\n    res.seasonal.plot(ax=axs[2], color='green')\n    axs[2].set_ylabel('Seasonal')\n    res.resid.plot(ax=axs[3], color='red')\n    axs[3].set_ylabel('Residual')\n\nfig, axs = plt.subplots(4,4, figsize=(20,10))\n\nfor i, col in enumerate(df.columns[:4]):\n    res = seasonal_decompose(df[col], period=52, extrapolate_trend='freq')\n    df[f\"{col}_trend\"] = res.trend\n    df[f\"{col}_seasonal\"] = res.seasonal\n    \n    plot_seasonal(res, axs[:,i%4])\n    axs[0][i%4].set_title(col)\n    \nplt.tight_layout()\nplt.show()","177c8153":"weeks_in_month = 4\n\nfor col in df.columns[:4]:\n    df[f'{col}_seasonal_shift_b_1m'] = df[f'{col}_seasonal'].shift(-1 * weeks_in_month)\n    df[f'{col}_seasonal_shift_1m'] = df[f'{col}_seasonal'].shift(1 * weeks_in_month)","d0ff054e":"fig, ax = plt.subplots(4,1, figsize=(15, 8))\nfig.suptitle('Seasonal Components of Features')\n\nfor i, col in enumerate(df.columns[:4]):\n    sns.lineplot(x=df.index, y=df[col+'_seasonal'], ax=ax[i])\n    ax[i].set_xlim([datetime.date(2014, 1, 1), datetime.date(2016, 1, 1)])\n    ax[i].set_ylabel(ylabel=col)\n    \nplt.tight_layout()\nplt.show()","0b3728b5":"fig, ax = plt.subplots(1,2, figsize=(16, 9))\n\ncorr = df.loc[:,df.columns[:4]].corr()\nax[0].set_title(\"Correlation matrix\")\nsns.heatmap(corr, annot=True, cmap=\"RdYlBu\", vmin=-1, ax=ax[0])\n\nshifted_cols = [\n    'meantemp_seasonal_shift_b_1m',\n    'humidity_seasonal',\n    'wind_speed_seasonal',         \n    'meanpressure_seasonal_shift_b_1m'\n]\ncorr = df[shifted_cols].corr()\nax[1].set_title(\"Correlation matrix shifted features\")\nsns.heatmap(corr, annot=True, cmap=\"RdYlBu\", vmin=-1, ax=ax[1])\nplt.tight_layout()\nplt.show()","2048ebed":"from pandas.plotting import autocorrelation_plot\n\nfig, axs = plt.subplots(2,2, figsize=(16,8))\nfor i, col in enumerate(df.columns[:4]) :\n    axs[i\/\/2][i%2].set_title(col)\n    autocorrelation_plot(df[col], ax=axs[i\/\/2][i%2])\nplt.tight_layout()\nplt.show()","03bae637":"from statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\nfor col in df.columns[:4] :\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 4))\n    fig.suptitle(col, fontweight='bold')\n    plot_acf(df[col], lags=50, ax=ax[0])\n    plot_pacf(df[col], lags=50, ax=ax[1])\n    plt.tight_layout()\n    plt.show()","ff5758af":"target = \"meantemp\"\n\ntrain_df = df.reset_index()[[\"date\", target]].copy()\ntrain_df.columns = ['ds', 'y'] # required for Prophet\n\ntest_df = test.reset_index()[[\"date\", target]].copy()\ntest_df.columns = ['ds', 'y']\n\nX_train, y_train = pd.DataFrame(train_df.iloc[:,0]), pd.DataFrame(train_df.iloc[:,1])\nX_test, y_test = pd.DataFrame(test_df.iloc[:,0]), pd.DataFrame(test_df.iloc[:,1])\n\nprint(f\"df shape      : {df.shape}\\n\"\n      f\"X_train shape : {X_train.shape}\\n\"\n      f\"X_test shape : {X_test.shape}\")","cb78ea6c":"from sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom fbprophet import Prophet\n\n# Train the model\nmodel = Prophet()\nmodel.fit(train_df)\n\n# X_valid = model.make_future_dataframe(periods=test_size, freq='w')\n\n# Predict on test set\ny_pred = model.predict(X_test)\n\n# Calculate metrics\nscore_mae = mean_absolute_error(y_test, y_pred['yhat'])\nscore_rmse = np.sqrt(mean_squared_error(y_test, y_pred['yhat']))\n\nprint(Fore.GREEN + 'RMSE: {}'.format(score_rmse)+'\\n')\n\n# Plot the forecast\nfig, ax = plt.subplots(figsize=(15,6))\nax.set_xlim([datetime.date(2016, 1, 1), datetime.date(2017, 4, 25)])\n\nmodel.plot(y_pred, ax=ax)\nsns.lineplot(x=X_test['ds'], y=y_test['y'], ax=ax, color='orange', label='Ground truth')\n\nax.set_title(f'Prediction \\n MAE: {score_mae:.2f}, RMSE: {score_rmse:.2f}', fontsize=14)\nax.set_xlabel(xlabel='Date')\nax.set_ylabel(ylabel=target)\n\nplt.show()","8be9474f":"from statsmodels.tsa.arima_model import ARIMA\n\n# Fit model\nmodel = ARIMA(y_train, order=(2,0,4))\nmodel_fit = model.fit()\n\n# Prediction with ARIMA\ny_pred, se, conf = model_fit.forecast(114)\n\n# Calcuate metrics\nscore_mae = mean_absolute_error(y_test, y_pred)\nscore_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n\nprint(Fore.GREEN + 'RMSE: {}'.format(score_rmse))\n\nfig, ax = plt.subplots(figsize=(15,6))\n\nmodel_fit.plot_predict(1, 323, ax=ax)\nsns.lineplot(x=np.arange(len(train_df), len(train_df)+ len(test_df)), y=y_test['y'], ax=ax, color='orange', label='Ground truth')\n\nax.set_title(f'Prediction \\n MAE: {score_mae:.2f}, RMSE: {score_rmse:.2f}', fontsize=14)\nax.set_xlabel(xlabel='Date')\nax.set_ylabel(ylabel=target)\n\nplt.show()","f71d2ca5":"!pip install pmdarima","4eec1a6d":"import pmdarima as pm\n\nmodel = pm.auto_arima(y_train, start_p=1, start_q=1,\n                      test='adf',       # use adftest to find optimal 'd'\n                      max_p=5, max_q=10, # maximum p and q\n                      m=1,              # frequency of series\n                      d=None,           # let model determine 'd'\n                      seasonal=False,   # No Seasonality\n                      start_P=0, \n                      D=0, \n                      trace=True,\n                      error_action='ignore',  \n                      suppress_warnings=True, \n                      stepwise=True)\n\nprint(model.summary())","8dd296ee":"model.plot_diagnostics(figsize=(16,8))\nplt.show()","1abd3e38":"from sklearn.preprocessing import MinMaxScaler\n\ntrain_data = train_df.filter(['y'])\ntest_data = test_df.filter(['y'])\n\nscaler = MinMaxScaler(feature_range=(-1, 0))\nscaled_train_data = scaler.fit_transform(train_data)\nscaled_test_data = scaler.fit_transform(test_data)","b8869057":"# Defines the rolling window\nlook_back = 1\n\ndef create_dataset(dataset, look_back=1):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    return np.array(dataX), np.array(dataY)\n\nX_train, y_train = create_dataset(scaled_train_data, look_back)\nX_test, y_test = create_dataset(scaled_test_data, look_back)\n\n# reshape input to be [samples, time steps, features]\nX_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\nX_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n\nprint(len(X_train), len(X_test))","bde3ae77":"from keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout\n\n#Build the LSTM model\nmodel = Sequential()\nmodel.add(LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(128, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(64, return_sequences=False))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(25))\nmodel.add(Dense(1))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mean_squared_error')\n\n#Train the model\nmodel.fit(X_train, y_train, batch_size=32, epochs=30, validation_data=(X_test, y_test))\n\nmodel.summary()","11125598":"# Lets predict with the model\ntrain_predict = model.predict(X_train)\ntest_predict = model.predict(X_test)\n\n# invert predictions\ntrain_predict = scaler.inverse_transform(train_predict)\ny_train = scaler.inverse_transform([y_train])\n\ntest_predict = scaler.inverse_transform(test_predict)\ny_test = scaler.inverse_transform([y_test])\n\n# Get the root mean squared error (RMSE) and MAE\nscore_rmse = np.sqrt(mean_squared_error(y_test[0], test_predict[:,0]))\nscore_mae = mean_absolute_error(y_test[0], test_predict[:,0])\nprint(Fore.GREEN + 'RMSE: {}'.format(score_rmse))\n\nx_train_ticks = train_df['ds']\ny_train = train_df['y']\nx_test_ticks = test_df.iloc[look_back+1:,0]\n\n# Plot the forecast\nfig, ax = plt.subplots(figsize=(15,6))\nax.set_xlim([datetime.date(2016, 1, 1), datetime.date(2017, 4, 25)])\n\nsns.lineplot(x=x_train_ticks, y=y_train, ax=ax, label='Train Set')\nsns.lineplot(x=x_test_ticks, y=test_predict[:,0], ax=ax, color='green', label='Prediction')\nsns.lineplot(x=x_test_ticks, y=y_test[0], ax=ax, color='orange', label='Ground truth')\n\nax.set_title(f'Prediction \\n MAE: {score_mae:.2f}, RMSE: {score_rmse:.2f}', fontsize=14)\nax.set_xlabel(xlabel='Date', fontsize=14)\nax.set_ylabel(ylabel=target, fontsize=14)\n\nplt.show()","d64656e0":"## **Data preprocessing**\n\n### Removing outliers","13467925":"### Lag","1d1c6d53":"There are no missing values.","378d05bc":"### Auto-ARIMA","3c46b09b":"## **Feature Engineering**\n\n### Date features","6fd21802":"## Variables visualization\n\n* **date :** Date of format YYYY-MM-DD\n* **meantemp :** Mean temperature averaged out from multiple 3 hour intervals in a day.\n* **humidity :** Humidity value for the day (units are grams of water vapor per cubic meter volume of air).\n* **wind_speed :** Wind speed measured in kmph.\n* **meanpressure :** Pressure reading of weather (measure in atm).","1872ccbe":"We can see some outliers in the *meanpressure* values.","97d6a5f2":"## Import libraries","13378d38":"# **Daily Climate**\n\n**Daily climate data in the city of Delhi from 2013 to 2017.**\n\n**To discover different concepts around time series, I was interested in different notebooks that helped me a lot, especially this one :**\n* https:\/\/www.kaggle.com\/andreshg\/timeseries-analysis-a-complete-guide\n\n**Don't hesitate to go and have a look at it first ! It contains more explanations.**","5e15f677":"### ARIMA\n\nThe Auto-Regressive Integrated Moving Average (ARIMA) model describes the autocorrelations in the data.<br\/> The model assumes that the time-series is stationary.","848c1b19":"As we can see, all of the columns appear to be stationary (0.05 significant level).","3909f503":"**Augmented Dickey-Fuller (ADF)**\n\n*Augmented Dickey-Fuller* is a statistical test to determine whether a time series is stationary.<\/br>\n\n* **Null Hypothesis - H0** : Time series is not stationary.\n* **Alternate Hypothesis - H1** : Time series is stationary.\n\nWe can reject **Null Hypothesis** if : \n* *p-value* <= significance level (default: 0.05)\n* ADF statistic < critical value","b38ea683":"### LSTM","178103d9":"### Prophet","2891d321":"### Autocorrelation analysis\n\n* Autocorrelation Function (ACF)\n* Partial Autocorrelation Function (PACF)","22a67f57":"## Import data","e5fc2d5a":"## **EDA**","f0840197":"**Diffencing**\n\nTechnique for transforming series into stationarity series.","fa31a62c":"We can see more correlation with the lagged features.","89828ac2":"### Stationarity\n\n* **Constant mean** and mean is **not time-dependent**\n* **Constant variance** and variance is **not time-dependent**\n* **Constant covariance** and covariance is **not time-dependent**","79d784d1":"### Resample \/ Smoothing data","d6a32c55":"### Time Series Decomposition\n\n* **Level :** The average value in the series.\n* **Trend :** The increasing or decreasing value in the series.\n* **Seasonality :** The repeating short-term cycle in the series.\n* **Noise :** The random variation in the series.","f731c51b":"## **Modelling**\n\n### Data preparation"}}