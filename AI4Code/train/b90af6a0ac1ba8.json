{"cell_type":{"e2f1ac26":"code","7a22f392":"code","d9f9d8a4":"code","c4a0e5a6":"code","94f8115d":"code","ffc6a20b":"code","fa6add84":"code","9dd9dbc8":"code","ae483f5d":"code","f3a48d93":"code","d45a0ce0":"code","271d6f20":"code","1333c9e6":"code","5ac0ba19":"code","3f50e506":"code","b3b83e10":"code","ae45e4b9":"code","ccee169e":"code","a456b7ba":"markdown","3b7d926b":"markdown","30fa5d9d":"markdown","5b250f8a":"markdown","4bf3c8fb":"markdown","e718f7c2":"markdown","900e2d10":"markdown","e5ed48e9":"markdown","2b002215":"markdown","c857cc82":"markdown"},"source":{"e2f1ac26":"import os\nimport time\nimport h5py\nimport numpy as np\nimport pandas as pd","7a22f392":"!wc -l ..\/input\/train.csv","d9f9d8a4":"n_lines_train = 629145481","c4a0e5a6":"chunk_size = 150000\nn_segments = (n_lines_train-1)\/\/chunk_size\nn_segments","94f8115d":"leftover = n_lines_train - n_segments*chunk_size\nleftover","ffc6a20b":"leftover\/n_lines_train","fa6add84":"input_dir = \"..\/input\"\noutput_dir = \"\"","9dd9dbc8":"#create the hdf5 file\nh5_file = h5py.File(os.path.join(output_dir, \"train.h5\"), \"w\")","ae483f5d":"#create datasets within the top level group in that file\nsound_dset = h5_file.create_dataset(\"sound\", shape=(n_segments, chunk_size), dtype=np.int16)\nttf_dset = h5_file.create_dataset(\"ttf\", shape=(n_segments,), dtype=np.float32)","f3a48d93":"#iterate over all 629 million lines and save them out in chunks of 150,000\n#this takes a while ...\nchunk_size = 150000\nlines_to_read = chunk_size*n_segments\n\nprinted_warning = False\nwith open(os.path.join(input_dir, \"train.csv\")) as f:\n    x_stack, y_stack = [], []\n    last_ttf = np.inf\n    hdr = f.readline()\n    for line_idx in range(lines_to_read):\n        cx, cy = f.readline().split(\",\")\n        cx = int(cx)\n        if np.abs(cx) > 32767:\n            if not printed_warning:\n                printed_warning = True\n                print(\"line {} is too big to be an int16\".format(line_idx+1))\n        cy = float(cy)\n        if cy < last_ttf:\n            last_ttf = cy\n            y_stack.append(cy)\n        x_stack.append(cx)\n        if line_idx % chunk_size == chunk_size-1:\n            sound_dset[line_idx\/\/chunk_size] = np.array(x_stack).astype(np.int16)\n            ttf_dset[line_idx\/\/chunk_size] = np.mean(y_stack)\n            x_stack, y_stack = [], []\n            last_ttf = np.inf","d45a0ce0":"h5_file.close()","271d6f20":"start_time = time.time()\nhf = h5py.File(os.path.join(output_dir, \"train.h5\"))\nsegs = np.array(hf[\"sound\"][:100])\nseg_ttf = np.array(hf[\"ttf\"][:100])\nhf.close()\nend_time = time.time()\nprint(\"{} seconds\".format(end_time-start_time))","1333c9e6":"start_time = time.time()\ndf = pd.read_csv(\n    os.path.join(input_dir, \"train.csv\"), \n    nrows=chunk_size*100,#limit to the first 100 segments \n    dtype={\"acoustic_data\":np.int16, \"time_to_failure\":np.float32}\n)\nend_time = time.time()\nprint(\"{} seconds\".format(end_time-start_time))","5ac0ba19":"test_files = os.listdir(os.path.join(input_dir, \"test\"))\n\nwith h5py.File(os.path.join(output_dir, \"test.h5\"), \"w\") as h5_file:\n    sound_dset = h5_file.create_dataset(\"sound\", (len(test_files), chunk_size))\n    seg_ids = []\n\n    for fidx, fname in enumerate(test_files):\n        cdata = pd.read_csv(os.path.join(input_dir, \"test\", fname), dtype=np.int16)[\"acoustic_data\"].values\n        sound_dset[fidx] = cdata\n        seg_ids.append(fname.split(\".\")[0])\n\n    h5_file[\"seg_id\"] = np.array(seg_ids).astype(np.string_)","3f50e506":"#loading back in the test data segment strings\nhf = h5py.File(os.path.join(output_dir, \"test.h5\"))","b3b83e10":"#without a .astype(str) the resulting array is of type bytes\nseg_ids = np.array(hf[\"seg_id\"])\nseg_ids","ae45e4b9":"#adding a astype call gets us nice unicode strings\n#that play well with python 3\nseg_ids = np.array(hf[\"seg_id\"]).astype(str)\nseg_ids","ccee169e":"hf.close()","a456b7ba":"The training ata for this competition is provided as a several hundred million line CSV with two columns. I find this a rather awkward data format to deal with. \n\nSimply attempting to load the csv via pandas read_csv takes a long time and isn't very memory efficient. When beginning to experiment with this data in kaggle kernels the pd.read_csv caused a dead kernel when the compute instance ran out of memory. Instead of suffering through a long running custom python loop to do all my feature engineering I have decided to reformat the training data to make the loading faster, more memory efficient and formatted in a way that more closely mimics the way the data is used to generate predictions on the testing segments. \n\nSince the \"time_to_failure\" column varies only very slowly it really needn't be provided for every single row separately, this is doubly true since we have been asked to predict only one such time per stretch of 150,000 acoustic data points in the test segements. By storing just one time_to_failure value per 150,000 training data rows we can save roughly a factor of 2 in memory usage right away, not to mention the extra convenience of having our training and testing data in a similar format. \n\nThe acoustic data has a maximum dynamic range which is small enough to allow us to represent it with arrays of type int16 which saves us a factor of 4 in terms of memory footprint versus storing the data as int64 (this by itself was not enough to. \n\nFinally we can store the data out in the hdf5 data format which will dramatically improve the time required to load the data from disk (roughly 1,000x speedup relative to a pd.read_csv call).\n\nYou can use this alternate format for the training data in your kernels by adding the output of this kernel as an additional data source (see https:\/\/www.kaggle.com\/product-feedback\/45472 )","3b7d926b":"Unfortunately the training data doesn't neatly divide into an integer number of the test segment size chunks. But when dealing with 600+ million time points a few tens of thousand more or less probably won't make much of a difference (it makes up just one ten thousandth part of the training data). So I will just ignore the last few training data time points. ","30fa5d9d":"While HDF5 is fantastic for storing numerical data it can be somewhat painful to use for storing text data. Unfortunately HDF5 likes storing string arrays as ascii and numpy likes string arrays to be either of object type, fixed length ascii (which works for hdf5 just fine) or unicode (which doesn't work with hdf5). \n\nSo when we write out the seg_id's we need to use np.string_ data format which will encode the strings as ascii. Then when we load the seg_id's back in we need to explicitly cast the numpy array back to the \"str\" type so that python will treat the resulting array as strings instead of as type \"bytes\". ","5b250f8a":"we create the hdf5 file and then will create named datasets within the file and then iterate over the csv and fill in the dataset rows one by one. We also could have first loaded the data into numpy arrays and then simply assigned the arrays directly out to the hdf5 file which is often a more convenient interface, but doing it this way allows us to deal with files which are much too large to fit directly into memory.","4bf3c8fb":"now compare this with the time to read in the data from the csv using pandas.","e718f7c2":"Happy kaggling.","900e2d10":"count the number of lines in the training file.","e5ed48e9":"Now that we have a nice hdf5 file lets do some comparisons versus the csv file. First off the h5 file on disk takes up only 1.2 Gb versus 8.9 Gb for the uncompressed csv. Most of this difference can be attributed to the fact that the time_to_failure column takes a lot of characters to express as a character string and we are storing just 1\/150,000th as many numbers and in a binary format. ","2b002215":"What about the time to read in the data from disk? Lets load the first 100 segments from our hdf5 file and compare that to using pandas.read_csv on the raw data.","c857cc82":"After putting the data into the hdf5 file the read time is fast, the memory usage is reduced and the training data is neatly formatted into 150,000 length segments and single time labels just like at testing time. \n\nTo maximize convenience we can also turn the test segment data into a hdf5 file with the same format. "}}