{"cell_type":{"2ef15052":"code","da122377":"code","9b1a006b":"code","9cebaacf":"code","a111a589":"code","f18a138b":"code","e8d9cf19":"code","00421ff6":"code","e5faf698":"code","adb40b44":"code","38c229af":"code","15950db3":"code","f911cbcc":"code","7dc1fb50":"code","577aa4a3":"code","55559817":"code","2f6fcef5":"code","5ef9276d":"code","d2aa07c3":"code","e5050c7b":"code","295fa546":"code","843be31c":"code","4a33b690":"code","29041f3d":"code","e34a90e8":"code","ba62caf8":"code","c280448a":"code","502c21c8":"code","54d8a8d5":"code","b77f8000":"code","52550ab7":"code","3270ab55":"code","6f89280b":"code","a55f9910":"code","17deae6a":"code","323fc13f":"code","8e17722f":"code","d3e6e219":"code","2b1eac71":"code","9fe44e16":"code","70a7d77e":"code","93d70401":"markdown","a32fc951":"markdown","51443f5d":"markdown","2553e8d3":"markdown","fdeab2d8":"markdown","78f8824e":"markdown","67e33807":"markdown","9885c7f0":"markdown","a64a8fd5":"markdown","eee3da17":"markdown","7f6e3ec2":"markdown","0928ccae":"markdown","46767ce6":"markdown","e02b7041":"markdown","d14268a6":"markdown","4a446005":"markdown","a5ddbf82":"markdown","86849f03":"markdown","6ebafe15":"markdown","7da579b3":"markdown","3b4e9904":"markdown","310daddf":"markdown"},"source":{"2ef15052":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","da122377":"data = pd.read_csv(\"\/kaggle\/input\/heart-failure-prediction\/heart.csv\")\ndata.head()","9b1a006b":"data.describe().T","9cebaacf":"data.info()","a111a589":"def bar_plot(variable, color):\n    var = data[variable]\n    varValue = var.value_counts()\n\n    # visualize\n    plt.figure(figsize=(9, 3))\n    plt.bar(varValue.index, varValue, color=color)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n\n    print(\"{}:\\n{}\".format(variable, varValue))\n\n\ncategory = [\"Sex\", \"ChestPainType\", \"RestingECG\", \"ExerciseAngina\", \"ST_Slope\", \"HeartDisease\", \"FastingBS\"]\ncolor_list = [\"red\", \"green\", \"blue\", \"orange\", \"yellow\", \"purple\", \"gray\"]\nfor i in range(7):\n    bar_plot(category[i], color_list[i])","f18a138b":"# Numerical Variable\ndef kde_plot(variable, color):\n    # visualize\n    plt.figure(figsize=(9, 6))\n    sns.kdeplot(data[variable], color=color, shade= True)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"Hist ile {} da\u011f\u0131l\u0131m\u0131\".format(variable))\n    plt.show()\n    \nnumeric = [\"Age\", \"RestingBP\", \"Cholesterol\", \"MaxHR\", \"Oldpeak\"]\ncolor_list2 = [\"red\", \"green\", \"blue\", \"orange\", \"yellow\"]\nfor i in range(5):\n    kde_plot(numeric[i], color_list2[i])","e8d9cf19":"# Sex - HeartDisease\nprint(data[[\"Sex\",\"HeartDisease\"]].groupby([\"Sex\"],as_index = False).mean().sort_values(by=\"HeartDisease\",ascending=False))\n# FastingBS - HeartDisease\nprint(data[[\"FastingBS\",\"HeartDisease\"]].groupby([\"FastingBS\"],as_index = False).mean().sort_values(by=\"HeartDisease\",ascending=False))\n# ChestPainType - HeartDisease\nprint(data[[\"ChestPainType\",\"HeartDisease\"]].groupby([\"ChestPainType\"],as_index = False).mean().sort_values(by=\"HeartDisease\",ascending=False))","00421ff6":"list_data = [\"Age\", \"RestingBP\", \"Cholesterol\", \"MaxHR\", \"Oldpeak\"]\nfor i in list_data:\n    sns.boxplot(x=data[i])\n    plt.show()","e5faf698":"# Ayk\u0131r\u0131 de\u011ferleri alt ve \u00fcst e\u015fik de\u011ferlere bask\u0131lama\ndef find_quantile(df, variable):\n    IQR = df[variable].quantile(0.75) - df[variable].quantile(0.25)\n\n    alt_sinir = df[variable].quantile(0.25) - (IQR * 1.5)\n    ust_sinir = df[variable].quantile(0.75) + (IQR * 1.5)\n    return alt_sinir, ust_sinir\n\n\nage_alt_sinir, age_ust_sinir = find_quantile(data, \"Age\")\nresting_alt_sinir, resting_ust_sinir = find_quantile(data, \"RestingBP\")\nchol_alt_sinir, chol_ust_sinir = find_quantile(data, \"Cholesterol\")\nmax_alt_sinir, max_ust_sinir = find_quantile(data, \"MaxHR\")\npeak_alt_sinir, peak_ust_sinir = find_quantile(data, \"Oldpeak\")\n\n\ndef count_outlier(df, variable, alt_sinir, ust_sinir):\n    x = df[df[variable] < alt_sinir][variable].size\n    y = df[df[variable] > ust_sinir][variable].size\n    print(variable, \"Outlier say\u0131s\u0131: \", x + y)\n\n\ncount_outlier(data, \"Age\", age_alt_sinir, age_ust_sinir)\ncount_outlier(data, \"RestingBP\", resting_alt_sinir, resting_ust_sinir)\ncount_outlier(data, \"Cholesterol\", chol_alt_sinir, chol_ust_sinir)\ncount_outlier(data, \"MaxHR\", max_alt_sinir, max_ust_sinir)\ncount_outlier(data, \"Oldpeak\", peak_alt_sinir, peak_ust_sinir)","adb40b44":"data.isnull().sum()","38c229af":"# correlation\nsns.heatmap(data[[\"Age\", \"RestingBP\", \"Cholesterol\", \"MaxHR\", \"Oldpeak\", \"HeartDisease\"]].corr(), annot=True)\nplt.show()","15950db3":"sns.factorplot(x=\"ChestPainType\", y= \"HeartDisease\", data = data, kind=\"bar\")\nplt.ylabel(\"HeartDisease Probability\")\nplt.show()","f911cbcc":"sns.pairplot(data, hue=\"HeartDisease\")\nplt.show()","7dc1fb50":"# Encoding\nencoder = LabelEncoder()\nlist_variable = [\"Sex\", \"ChestPainType\", \"RestingECG\", \"ExerciseAngina\", \"ST_Slope\"]\nfor i in list_variable:\n    data[i] = encoder.fit_transform(data[i])","577aa4a3":"print(data.head())","55559817":"from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras import callbacks\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.wrappers.scikit_learn import KerasClassifier","2f6fcef5":"X = data.drop(\"HeartDisease\", axis=1)\ny = data[\"HeartDisease\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.30,\n                                                    random_state=42)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","5ef9276d":"knn = KNeighborsClassifier()\nknn_model = knn.fit(X_train, y_train)\n\ny_pred = knn_model.predict(X_test)\nprint(\"Accuracy Score: \", accuracy_score(y_test, y_pred))","d2aa07c3":"# Stratified K-Fold cross validation\n\nskf = StratifiedKFold(n_splits=10)\n\n# Hyperparametre tuning with GridSearchCV\nknn_params = {\"n_neighbors\": np.arange(1, 50)}\nknn = KNeighborsClassifier()\nknn_cv_model = GridSearchCV(knn, knn_params, cv=skf, scoring=\"accuracy\")\nknn_cv_model.fit(X_train, y_train)\n\nprint(\"En iyi skor: {}\".format(knn_cv_model.best_score_))\nprint(\"En iyi K de\u011feri {}\".format(knn_cv_model.best_params_))","e5050c7b":"# Tuned Model\nknn = KNeighborsClassifier(n_neighbors=11)\nknn_tuned = knn.fit(X_train, y_train)","295fa546":"y_pred = knn_tuned.predict(X_test)\ny_pred_train = knn_tuned.predict(X_train)\nprint(\"Test Accuracy Score: \", accuracy_score(y_test, y_pred))\nprint(\"Train Accuracy Score: \", accuracy_score(y_train, y_pred_train))","843be31c":"# confusion matrix\nconf_mat = confusion_matrix(y_test, y_pred)\n# plot the confusion matrix\nf, ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(conf_mat, annot=True, linewidths=0.01, cmap=\"Greens\", linecolor=\"gray\", fmt='.1f', ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n","4a33b690":"print(classification_report(y_test, y_pred))","29041f3d":"score = cross_val_score(knn_tuned, X_train, y_train, cv=skf, scoring=\"accuracy\")\nprint(\"Train score degerleri: \", score)\nprint(\"Train  Mean Score:\", score.mean())\nprint(\"Train Std : \" + str(score.std()))\n\nscore = cross_val_score(knn_tuned, X_test, y_test, cv=skf, scoring=\"accuracy\")\nprint(\"Test score degerleri: \", score)\nprint(\"Test Mean Score:\", score.mean())\nprint(\"Test Std : \" + str(score.std()))","e34a90e8":"def build_model(layers):\n    model = Sequential()\n    for i, nodes in enumerate(layers):\n        if i == 0:\n            # Input layer - first hidden layer\n            model.add(Dense(16, input_dim=11, activation=\"relu\"))\n            model.add(Dropout(0.25))\n        else:\n            model.add(Dense(nodes))\n            model.add(Activation('relu'))\n            model.add(Dropout(0.25))\n    model.add(Dense(1, kernel_initializer=\"uniform\", activation=\"sigmoid\"))\n    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n    return model","ba62caf8":"earlystopping = callbacks.EarlyStopping(monitor='val_loss',\n                                        mode='min',\n                                        verbose=1,\n                                        patience=20)\nmodel = KerasClassifier(build_fn=build_model,\n                        callbacks=[earlystopping],\n                        validation_data=(X_test, y_test))","c280448a":"skf = StratifiedKFold(n_splits=10)\n\nlayers = [[16, 16], [32, 16, 32], [32, 16, 16]]\n\nparams = {\"layers\": layers,\n          \"batch_size\": [32, 64],\n          \"epochs\": [100, 200, 300]}\n\ngrid_cv = GridSearchCV(model, param_grid=params, cv=skf, scoring='accuracy')\ngrid_cv.fit(X_train, y_train,\n            validation_data=(X_test, y_test),\n            callbacks=[earlystopping])","502c21c8":"print(\"En iyi parametreler: {}\".format(grid_cv.best_params_))\nprint(\"En iyi Skor: {}\".format(grid_cv.best_score_))","54d8a8d5":"def create_model():\n    ann_model = Sequential()\n    # Input\n    ann_model.add(Dense(32, activation='relu', input_dim=X_train.shape[1]))\n    ann_model.add(Dropout(0.25))\n\n    ann_model.add(Dense(16, activation='relu', kernel_initializer=\"uniform\"))\n    ann_model.add(Dropout(0.25))\n\n    ann_model.add(Dense(16, activation='relu', kernel_initializer=\"uniform\"))\n    ann_model.add(Dropout(0.25))\n\n    # Dropout\n    ann_model.add(Dense(1, activation='sigmoid', kernel_initializer=\"uniform\"))\n    optimizer = Adam(learning_rate=0.001)\n    ann_model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n    return ann_model","b77f8000":"model = create_model()\n\nhistory = model.fit(X_train, y_train,\n                    batch_size=32, epochs=300,\n                    validation_data=(X_test, y_test),\n                    callbacks=[earlystopping])","52550ab7":"earlystopping = callbacks.EarlyStopping(monitor='val_loss',\n                                        mode='min',\n                                        verbose=1,\n                                        patience=20)\nkeras_clf = KerasClassifier(create_model,\n                            validation_data=(X_test, y_test),\n                            epochs=300, batch_size=32,\n                            callbacks=[earlystopping])","3270ab55":"# Stratified K-Fold cross validation\nskf = StratifiedKFold(n_splits=10)\n\naccuracies = cross_val_score(estimator=keras_clf,\n                             X=X_train, y=y_train, cv=skf\n                             , scoring=\"accuracy\")","6f89280b":"mean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy mean: \" + str(mean))\nprint(\"Accuracy variancce: \" + str(variance))\nprint(accuracies)","a55f9910":"# summarize history for acc\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='lower right')\nplt.show()\n\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","17deae6a":"# Threshold for 0.5;\ny_pred = model.predict(X_test)\nprediction_label = [np.argmax(i) for i in y_pred]\nbinary_prediction = []\n\nfor i in y_pred:\n    if i > 0.5:\n        binary_prediction.append(1)\n    else:\n        binary_prediction.append(0)\n\nprint(\"Model Score: \", accuracy_score(y_test, binary_prediction))","323fc13f":"from sklearn.metrics import r","8e17722f":"# confusion matrix\nconf_mat = confusion_matrix(y_test, binary_prediction)\n# plot the confusion matrix\nf, ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(conf_mat, annot=True, linewidths=0.01, cmap=\"Greens\", linecolor=\"gray\", fmt='.1f', ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","d3e6e219":"print(classification_report(y_test, binary_prediction))","2b1eac71":"# Threshold 0.4 i\u00e7in;\ny_pred = model.predict(X_test)\nprediction_label = [np.argmax(i) for i in y_pred]\nbinary_prediction = []\n\nfor i in y_pred:\n    if i > 0.4:\n        binary_prediction.append(1)\n    else:\n        binary_prediction.append(0)\nprint(\"Model Score: \", accuracy_score(y_test, binary_prediction))","9fe44e16":"# confusion matrix\nconf_mat = confusion_matrix(y_test, binary_prediction)\n# plot the confusion matrix\nf, ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(conf_mat, annot=True, linewidths=0.01, cmap=\"Greens\", linecolor=\"gray\", fmt='.1f', ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","70a7d77e":"print(classification_report(y_test, binary_prediction))","93d70401":"<a id = \"14\"><\/a><br>\n## Train - Test Split","a32fc951":"<a id = \"12\"><\/a><br>\n## ChestPainType -- HeartDisease","51443f5d":"<a id = \"3\"><\/a><br>\n# Univariate Variable Analysis\n* Categorical variable -> Sex, ChestPainType, RestingECG, ExerciseAngina, ST_Slope, HeartDisease, FastingBS\n* Numerical variable ->  Age, RestingBP, Cholesterol, MaxHR, Oldpeak","2553e8d3":"<a id = \"10\"><\/a><br>\n# Visualization","fdeab2d8":"<a id=\"1\"><\/a><br>\n## 1.Load and Check Data","78f8824e":"<a id = \"11\"><\/a><br>\n## Correlation Between Age -- RestingBP -- Cholesterol -- MaxHR -- Oldpeak","67e33807":"<a id = \"19\"><\/a><br>\n## Hyperparameter Tuning -- Grid Search -- Cross Validation for YSA","9885c7f0":"<a id = \"18\"><\/a><br>\n## Model with YSA","a64a8fd5":"<a id=\"8\"><\/a><br>\n# Missing Value\n* Find Missing Value","eee3da17":"<a id = \"16\"><\/a><br>\n## Hyperparameter Tuning -- Grid Search -- Cross Validation for KNN","7f6e3ec2":"<a id=\"7\"><\/a><br>\n# Outlier Detection","0928ccae":"* Eksik veri yok","46767ce6":"# Introduction\nKardiyovask\u00fcler hastal\u0131klar (KVH), her y\u0131l tahminen 17,9 milyon can alarak, d\u00fcnya \u00e7ap\u0131ndaki t\u00fcm \u00f6l\u00fcmlerin %31'ini olu\u015fturan, k\u00fcresel olarak 1 numaral\u0131 \u00f6l\u00fcm nedenidir.\n5CVD \u00f6l\u00fcmlerinden d\u00f6rd\u00fc kalp krizi ve fel\u00e7 nedeniyledir ve bu \u00f6l\u00fcmlerin \u00fc\u00e7te biri 70 ya\u015f\u0131n alt\u0131ndaki ki\u015filerde erken meydana gelir. Kalp yetmezli\u011fi, CVD'lerin neden\noldu\u011fu yayg\u0131n bir olayd\u0131r ve bu veri seti, olas\u0131 bir kalp hastal\u0131\u011f\u0131n\u0131 tahmin etmek i\u00e7in kullan\u0131labilecek 11 \u00f6zellik i\u00e7erir. Kardiyovask\u00fcler hastal\u0131\u011f\u0131 olan veya y\u00fcksek\nkardiyovask\u00fcler risk alt\u0131nda olan ki\u015filer (hipertansiyon, diyabet, hiperlipidemi veya \u00f6nceden belirlenmi\u015f hastal\u0131k gibi bir veya daha fazla risk fakt\u00f6r\u00fcn\u00fcn varl\u0131\u011f\u0131\nnedeniyle), bir makine \u00f6\u011frenimi modelinin \u00e7ok yard\u0131mc\u0131 olabilece\u011fi erken tespit ve y\u00f6netime ihtiya\u00e7 duyar.\n\n<font color = 'blue'>\nContent:\n  \n1.[Load and Check Data](#1)\n    \n2.[Variable Description](#2)\n    \n*     [Univariate Variable Analysis](#3)\n        * [Categorical Variable](#4)\n        * [Numerical Variable](#5)\n    \n3. [Basic Data Analysis](#6)\n    \n4. [Outlier Detection](#7)\n\n5.[Missing Value](#8)\n    \n   * [Find Missing value](#9)  \n    \n\n6. [Visualization](#10)\n      * [Correlation Between Age--RestingBP--Cholesterol--MaxHR--Oldpeak ](#11)\n      * [ChestPainType -- HeartDisease](#12)\n   \n7. [Modeling](#13)\n      * [Train - Test Split](#14)\n      * [Model with KNN](#15)\n      * [Hyperparameter Tuning -- Grid Search -- Cross Validation for KNN](#16)\n      * [Prediction and Submission](#17)\n      * [Model with YSA](#18)\n      * [Hyperparameter Tuning -- Grid Search -- Cross Validation for YSA](#19)\n      * [Prediction and Submission](#20)","e02b7041":"<a id = \"20\"><\/a><br>\n## Prediction and Submission","d14268a6":" <a id=\"6\"><\/a><br>\n# Basic Data Analysis\n* Sex - HeartDisease\n* FastingBS - HeartDisease\n* ChestPainType - HeartDisease","4a446005":"<a id = \"4\"><\/a><br>\n## Categorical Variable ","a5ddbf82":"<a id = \"17\"><\/a><br>\n## Prediction and Submission","86849f03":"<a id = \"15\"><\/a><br>\n## Model with KNN","6ebafe15":"<a id = \"5\"><\/a><br>\n## Numerical Variable","7da579b3":"* I will not apply any operation to outlier values.","3b4e9904":"<a id=\"1\"><\/a><br>\n## 2. Variable Description\n1. Age -> Hastan\u0131n ya\u015f\u0131\n1. Sex -> Hastan\u0131n cinsiyeti\n1. ChestPainType -> G\u00f6\u011f\u00fcs a\u011fr\u0131s\u0131 tipi [TA: Tipik Angina, ATA: Atipik Angina, NAP: Anjinal olmayan a\u011fr\u0131, ASY: Asemptomatik]\n1. RestingBP -> Dinlenme kan bas\u0131nc\u0131 [mm Hg]\n1. Cholesterol -> Serum kolestr\u00f6l\u00fc [mm\/dl]\n1. FastingBS -> A\u00e7l\u0131k kan \u015fekeri [1: if FastingBS > 120 mg\/dl, 0: otherwise]\n1. RestingECG -> Dinlenme elektrokardiyogram sonu\u00e7lar\u0131 [Normal: Normal, ST: ST-T dalga anormalli\u011fi olan \n   T dalgas\u0131 inversiyonlar\u0131 ve\/veya ST elevasyonu veya depresyonu > 0.05 mV), LVH: Estes kriterlerine g\u00f6re olas\u0131 veya kesin sol ventrik\u00fcl hipertrofisini g\u00f6steriyor]\n1. MaxHR -> Ula\u015f\u0131lan maksimum kal at\u0131\u015f h\u0131z\u0131 [60 ile 202 aras\u0131nda say\u0131sal de\u011fer]\n1. ExerciseAngina -> egzersize ba\u011fl\u0131 angina [Y: Evet, N: Hay\u0131r]\n1.Oldpeak -> oldpeak = ST [Depresyonda \u00f6l\u00e7\u00fclen say\u0131sal de\u011fer]\n1.ST_Slope -> zirve egzersiz ST segmentinin e\u011fimi [Up: upsloping(e\u011fimli), Flat: flat(d\u00fcz), Down: dowmsloping(a\u015fa\u011f\u0131 e\u011fimli)]\n1.HeartDisease -> output class [1: heart disease, 0: Normal]","310daddf":"<a id = \"13\"><\/a><br>\n## Modeling"}}