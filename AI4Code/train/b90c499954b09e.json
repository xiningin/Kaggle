{"cell_type":{"d1e76090":"code","7ca84424":"code","73285376":"code","a9ceb2e0":"code","7abb271f":"code","ab34a9d5":"code","7a1f3388":"code","a48dba2b":"code","16d52462":"code","d550d5d4":"code","a5a24c82":"code","072b20f7":"code","db8aae30":"code","e7772acd":"code","1eedd7c6":"code","645b9713":"code","b35f07a8":"code","d9f8aaf5":"code","48af8709":"code","ff4b3d61":"code","993db948":"code","5454683c":"code","703f12ab":"code","ebb03488":"code","f6c33a06":"code","64d3811d":"code","973061a5":"code","9a2730c3":"code","f21e1837":"code","c2199120":"code","4c2ecb7a":"markdown","e369d50c":"markdown","20448a52":"markdown","d87ce703":"markdown","1fe6ec3a":"markdown","251f5064":"markdown","25380303":"markdown","f1d592ca":"markdown"},"source":{"d1e76090":"import numpy as np\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport re\nimport nltk\nimport os\nimport time\nimport matplotlib.pyplot as plt\n\nfrom gensim.models import KeyedVectors, FastText\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow.keras import callbacks, models, layers\n\n# tokenization\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom scipy.stats import rankdata","7ca84424":"LSTM_SIZE =  128\n# EMB_SIZE = 4\n# MAX_WORDS = 25_000\n\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')","73285376":"EPOCHS = 100\nBATCH_SIZE = 16\nDEBUG = False","a9ceb2e0":"df = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\ndf['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) > 0 ).astype(int)\ndf = df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\ndf.sample(5)","7abb271f":"min_len = (df['y'] == 1).sum()\ndf_y0_undersample = df[df['y'] == 0].sample(n=min_len, random_state=201)\ndf = pd.concat([df[df['y'] == 1], df_y0_undersample])\nif(DEBUG):\n    df = df.sample(n=400, random_state=201)\ndf['y'].value_counts()","ab34a9d5":"stop_words = stopwords.words(\"english\")\nclass DataPipeline:\n    def __init__(self):\n        #self.tokenizer = Tokenizer(num_words=MAX_WORDS)\n        self.fmodel = FastText.load('..\/input\/jigsaw-regression-based-data\/FastText-jigsaw-100D\/Jigsaw-Fasttext-Word-Embeddings.bin')\n        self.emb_dims = 100\n        \n    def clean(self, comment):\n        clean_html = BeautifulSoup(comment).get_text()\n        clean_non_letters = re.sub(\"[^a-zA-Z]\", \" \", clean_html)\n        cleaned_lowercase = clean_non_letters.lower()\n        words = cleaned_lowercase.split()\n        cleaned_words = [w for w in words if w not in stop_words]\n        return \" \".join(cleaned_words)\n    \n#     def fit(self, clean_text):\n#         self.tokenizer.fit_on_texts(clean_text)\n#         self.total_words = len(self.tokenizer.word_index) + 1\n        \n    def fit_transform(self, text):\n        sequences = [[self.fmodel.wv[self.clean(txt)]] for txt in text]\n#         self.fit(clean_text)\n#         sequences = self.tokenizer.texts_to_sequences(clean_text)\n#         self.max_sequence_len = max([len(x) for x in sequences])\n#         padded_sequences = np.array(pad_sequences(sequences, maxlen=self.max_sequence_len, padding='pre'))\n        return np.array(sequences)\n    \n    def transform(self, text):\n        sequences = [[self.fmodel.wv[self.clean(txt)]] for txt in text]\n#         sequences = self.tokenizer.texts_to_sequences(clean_text)\n#         padded_sequences = np.array(pad_sequences(sequences, maxlen=self.max_sequence_len, padding='pre'))\n        return np.array(sequences)\n\ndata_pipeline = DataPipeline()","7a1f3388":"X_train, X_val, y_train, y_val = train_test_split(data_pipeline.fit_transform(df.text), df.y, test_size=0.2, random_state=0)\nprint(X_train.shape, y_train.shape, X_val.shape, y_val.shape)","a48dba2b":"def to_dataset(data, labels):\n    dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n    dataset = dataset.cache().shuffle(data.shape[0] + 1).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n    return dataset\nsimple_train_ds = to_dataset(X_train, y_train)\nsimple_val_ds = to_dataset(X_val, y_val)","16d52462":"class SimpleModel(tf.keras.Model):\n    def __init__(self, lstm_size):\n        super(SimpleModel, self).__init__(name='')\n        self.lstm1 = layers.LSTM(lstm_size, return_sequence=True)\n        self.lstm2 = layers.LSTM(lstm_size)\n        self.dense = layers.Dense(1, activation='sigmoid')\n        \n    def call(self, input_tensor, training=False):\n        x = self.lstm1(input_tensor)\n        x = self.lstm2(x)\n        return self.dense(x)","d550d5d4":"simple_optimizer = tf.keras.optimizers.Adam(1e-4)\nsimple_model = SimpleModel(LSTM_SIZE)\nsimple_model.compile(loss='binary_crossentropy', optimizer=simple_optimizer, metrics=['binary_accuracy'])\nsimple_model.build((None, 1, data_pipeline.emb_dims))\nsimple_model.summary()","a5a24c82":"simple_reducer = callbacks.ReduceLROnPlateau(monior='val_loss', factor=0.5, patience=2, mode='min', cooldown=1)\nsimple_stopper = callbacks.EarlyStopping(monitor='val_loss', patience=4, mode='min', restore_best_weights=True)\nsimple_hist = simple_model.fit(simple_train_ds,\n                               epochs=EPOCHS,\n                               verbose=1,\n                               callbacks=[simple_stopper, simple_reducer],\n                               validation_data=simple_val_ds)\nresults = simple_model.evaluate(simple_val_ds)\nprint(f\"results: {results}, type: {type(results)}\")","072b20f7":"fig, axs = plt.subplots(3, 1, figsize=(8,8), tight_layout=True)\n\naxs[0].plot(simple_hist.history['loss'])\naxs[0].plot(simple_hist.history['val_loss'])\naxs[0].set_title('binary_crossentropy Loss')\naxs[0].set_ylabel('Loss')\naxs[0].set_xlabel('Epoch')\naxs[0].legend(['train', 'val'], loc='upper right')\n\naxs[1].plot(simple_hist.history['binary_accuracy'])\naxs[1].plot(simple_hist.history['val_binary_accuracy'])\naxs[1].set_title('binary_accuracy Metric')\naxs[1].set_ylabel('Error')\naxs[1].set_xlabel('Epoch')\naxs[1].legend(['train', 'val'], loc='upper left')\n\naxs[2].plot(simple_hist.history['lr'])\naxs[2].set_title('Learining Rate')\naxs[2].set_ylabel('LR')\naxs[2].set_xlabel('Epoch')\n# plt.savefig(f'\/kaggle\/working\/{name}_graphs.png')\nplt.show()","db8aae30":"# df_val = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\n# if(DEBUG):\n#     df_val = df_val.sample(n=400)\n# X2_train, X2_val, y2_train, y2_val = train_test_split(data_pipeline.transform(df_val.less_toxic), data_pipeline.transform(df_val.more_toxic), test_size=0.2, random_state=0)\n# print(X2_train.shape, y2_train.shape, X2_val.shape, y2_val.shape)","e7772acd":"df_val = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\nless_toxic, more_toxic = data_pipeline.transform(df_val.less_toxic), data_pipeline.transform(df_val.more_toxic)","1eedd7c6":"# def to_dataset_complex(data1, data2):\n#     dataset = tf.data.Dataset.from_tensor_slices((data1, data2))\n#     dataset = dataset.cache().shuffle(data1.shape[0] + 1).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n#     return dataset\n# complex_train_ds = to_dataset_complex(X2_train, y2_train)\n# complex_val_ds = to_dataset_complex(X2_val, y2_val)","645b9713":"# def loss_function(lower, upper):\n#     return tf.nn.relu(tf.math.subtract(lower, upper))\n# #     over_under_err = tf.math.square(tf.nn.relu(tf.math.subtract(lower, upper)))\n# #     separation_err = tf.math.square(tf.math.subtract(tf.ones_like(upper), tf.nn.relu(tf.math.subtract(upper, lower))))\n# #     return tf.math.add(over_under_err, separation_err)\n\n# def complex_metric(more_severe, less_severe):\n#     acc = tf.math.greater(more_severe, less_severe)\n#     acc = tf.where(acc, 0.0, 1.0)\n#     return acc","b35f07a8":"# class ComplexModel(tf.keras.Model):\n#     def __init__(self, lstm_size):\n#         super(ComplexModel, self).__init__(name='')\n#         self.lstm1 = layers.LSTM(lstm_size)\n#         self.dense = layers.Dense(1, activation='sigmoid')\n        \n#     def train_step(self, data):\n# #         if(len(data) == 3):\n# #             X, y, sample_weights = data\n# #         else:\n# #             X, y = data\n# #             sample_weights = None\n#         with tf.GradientTape() as tape:\n#             less_severe, more_severe = self.call(data, training=True)\n#             loss = self.compiled_loss(less_severe, more_severe)\n#             grads = tape.gradient(loss, self.trainable_variables)\n#         self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n#         self.compiled_metrics.update_state(less_severe, more_severe)\n#         return {m.name: m.result() for m in self.metrics}\n    \n#     def test_step(self, data):\n#         less_severe, more_severe = self.call(data, training=False)\n#         self.compiled_loss(less_severe, more_severe, regularization_losses=self.losses)        \n#         self.compiled_metrics.update_state(less_severe, more_severe)\n#         return {m.name: m.result() for m in self.metrics}\n    \n#     def call(self, input_tensor, training=False):\n#         less_severe_tensor, more_severe_tensor = input_tensor\n#         less_severe = self.lstm1(less_severe_tensor)\n#         less_severe = self.dense(less_severe)\n#         more_severe = self.lstm1(more_severe_tensor)\n#         more_severe = self.dense(more_severe)\n#         return less_severe, more_severe","d9f8aaf5":"len(simple_model.weights)\nfor i in range(len(simple_model.weights)):\n    print(simple_model.weights[i].shape)","48af8709":"# complex_optimizer = tf.keras.optimizers.Adam(1e-4)\n# complex_model = ComplexModel(LSTM_SIZE)\n# complex_model.compile(loss=loss_function, optimizer=complex_optimizer, metrics=[complex_metric])\n# complex_model.fit(complex_val_ds, epochs=1, verbose=0);","ff4b3d61":"# len(complex_model.weights)\n# for i in range(len(complex_model.weights)):\n#     print(complex_model.weights[i].shape)\n\n# complex_model.set_weights(simple_model.get_weights())","993db948":"# complex_model.evaluate(complex_val_ds)","5454683c":"# complex_reducer = callbacks.ReduceLROnPlateau(monior='val_loss', factor=0.5, patience=2, mode='min', cooldown=1)\n# complex_stopper = callbacks.EarlyStopping(monitor='val_loss', patience=4, mode='min', restore_best_weights=True)\n# complex_hist = complex_model.fit(complex_train_ds,\n#                                  epochs=EPOCHS,\n#                                  verbose=1,\n#                                  callbacks=[complex_stopper, complex_reducer],\n#                                  validation_data=complex_val_ds)\n# results = complex_model.evaluate(complex_val_ds)\n# print(f\"results: {results}, type: {type(results)}\")","703f12ab":"# fig, axs = plt.subplots(3, 1, figsize=(8,8), tight_layout=True)\n\n# axs[0].plot(complex_hist.history['loss'])\n# axs[0].plot(complex_hist.history['val_loss'])\n# axs[0].set_title('Custom Loss')\n# axs[0].set_ylabel('Loss')\n# axs[0].set_xlabel('Epoch')\n# axs[0].legend(['train', 'val'], loc='upper right')\n\n# axs[1].plot(complex_hist.history['complex_metric'])\n# axs[1].plot(complex_hist.history['val_complex_metric'])\n# axs[1].set_title('complex_metric')\n# axs[1].set_ylabel('Error')\n# axs[1].set_xlabel('Epoch')\n# axs[1].legend(['train', 'val'], loc='upper left')\n\n# axs[2].plot(complex_hist.history['lr'])\n# axs[2].set_title('Learining Rate')\n# axs[2].set_ylabel('LR')\n# axs[2].set_xlabel('Epoch')\n# # plt.savefig(f'\/kaggle\/working\/{name}_graphs.png')\n# plt.show()","ebb03488":"val_simple_less, val_simple_more = simple_model(less_toxic).numpy(), simple_model(more_toxic).numpy()\n# val_complex_less, val_comples_more = complex_model([X2_val, y2_val])","f6c33a06":"(val_simple_less < val_simple_more).mean()","64d3811d":"# (val_complex_less.numpy() < val_comples_more.numpy()).mean()","973061a5":"# complex_model.evaluate(complex_val_ds)","9a2730c3":"df_sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\nX_test = data_pipeline.transform(df_sub.text)","f21e1837":"p3 = simple_model(X_test)\ndf_simple_sub = df_sub.copy()\ndf_simple_sub['score'] = p3\nprint(df_simple_sub['score'].count())\nprint(df_simple_sub['score'].nunique())\ndf_simple_sub['score']=rankdata(df_simple_sub['score'], method='ordinal') \ndf_simple_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","c2199120":"# p3 = complex_model(X_test)\n# df_complex_sub = df_sub.copy()\n# df_complex_sub['score'] = p3\n# print(df_complex_sub['score'].count())\n# print(df_complex_sub['score'].nunique())\n# df_complex_sub[['comment_id', 'score']].to_csv(\"complex_submission.csv\", index=False)","4c2ecb7a":"# Create train data\n\nThe competition was multioutput\n\nWe turn it into a binary toxic\/ no-toxic classification","e369d50c":"# Undersample\n\nThe dataset is very unbalanced. Here we undersample the majority class. Other strategies might work better.","20448a52":"# Simple Model","d87ce703":"# Complex data generation","1fe6ec3a":"# Imports","251f5064":"# transform the data","25380303":"# Complex Model","f1d592ca":"# Submission"}}