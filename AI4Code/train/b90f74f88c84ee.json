{"cell_type":{"fc22f081":"code","33cc1e59":"code","a65900be":"code","f19d7a94":"code","f8fbb8d0":"code","9ee2c01b":"code","8170f661":"code","a1ac4185":"code","c3cf22f1":"code","f99901f9":"code","0acdff6e":"code","b909ae48":"code","f408bc2d":"code","8ebce4af":"code","6e4fecde":"code","39ee2dd4":"code","7271ae08":"code","0f5deaf7":"code","c09783a7":"code","4ccba87f":"code","39249019":"code","98642b0c":"code","e1dd91c4":"code","0cb7307f":"code","bba34b19":"code","89ce3d14":"code","a7938c7b":"code","61e861dd":"code","5e78f7dc":"code","ffb2cb0b":"code","60ae0cb6":"code","e6ccb276":"code","09d2b194":"code","0e4a619a":"code","0268f131":"code","97c89dba":"code","672a1bd7":"code","2f01ee51":"code","2b4a5166":"code","0647a11a":"code","8818e2b2":"code","4435b29a":"code","2dd85b52":"code","a74be9b1":"code","df3dd58e":"code","99754bb0":"code","76928250":"code","e4720fe9":"code","9f76dd1e":"code","50427c34":"code","eb617b8b":"code","ff9066d3":"code","d6a3b700":"code","160b2952":"markdown","7b4f0db5":"markdown","c6acce9a":"markdown","8b45a878":"markdown","83a2bae8":"markdown","0875bb8e":"markdown","c49760aa":"markdown","b2643de5":"markdown","4399a23d":"markdown","58e7e17d":"markdown","ad5a9957":"markdown","cebb87af":"markdown","7b32bc6a":"markdown","da67d26e":"markdown","51271084":"markdown","867f5e1f":"markdown","56b6e0bb":"markdown","fe2ff4ef":"markdown","1fe40785":"markdown","c7462311":"markdown","c0836d41":"markdown","6fee249a":"markdown","f01f88af":"markdown","9ec09c36":"markdown","7b211553":"markdown","e6ab8168":"markdown","a142c858":"markdown","4168c43e":"markdown","1694c155":"markdown","1dfc14a3":"markdown","8649639d":"markdown","ec88a7a4":"markdown","38979f80":"markdown","b6c36cef":"markdown","e182f0f4":"markdown","c8163255":"markdown","c399f6ed":"markdown","b626d4ee":"markdown","4e20b6b5":"markdown","f8f396b4":"markdown","de3103e8":"markdown","e6e74d75":"markdown","20d95310":"markdown"},"source":{"fc22f081":"import pandas as pd \nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport seaborn as sns\nimport datetime as dt\nimport time\nimport math\nfrom tqdm.notebook import tqdm\nimport itertools\nimport random\nimport re\nimport gc\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom category_encoders import OrdinalEncoder, TargetEncoder\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import mean_squared_log_error, roc_auc_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegression, ElasticNet\nfrom sklearn.inspection import permutation_importance\nimport category_encoders as ce\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.metrics import AUC\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom lightgbm import LGBMClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans, MiniBatchKMeans\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import quantile_transform\n\n%matplotlib inline\npd.options.display.max_rows = 1000\npd.options.display.max_columns = 200\n\n#import os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n        \nDIR = '\/kaggle\/input\/homework-for-students4plus'","33cc1e59":"# \u30c6\u30fc\u30d6\u30eb\u306f\u30b0\u30ed\u30fc\u30d0\u30eb\u306b\u4fdd\u6301\u3057\u3066\u304a\u304d\u9069\u5b9c\u4fee\u6b63\ndf_train_ori, df_test_ori = None, None\ndf_train, df_test = None, None\ndf_holdout = None\n\ndf_train_linear, df_test_linear = None, None\n\n# \u30c6\u30ad\u30b9\u30c8\u306e\u7279\u5fb4\u5316\u306f\u6642\u9593\u304c\u304b\u304b\u308b\u306e\u3067\u30ad\u30e3\u30c3\u30b7\u30e5\u3092\u4fdd\u5b58\u3057\u3066\u304a\u304f\u3053\u3068\u306b\u3059\u308b\nsr_title_train, sr_title_test = None, None\nsr_emp_title_train, sr_emp_title_test = None, None\ntfidf_pca_title_train, tfidf_pca_title_test = None, None # \u7dda\u5f62\u30e2\u30c7\u30eb\u7528: title\u5217\u306etfidf->pca\u7279\u5fb4\ntfidf_pca_emp_title_train, tfidf_pca_emp_title_test = None, None # \u7dda\u5f62\u30e2\u30c7\u30eb\u7528: emp_title\u5217\u306etfidf->pca\u7279\u5fb4\ntfidf_pca_kmeans_cluster_train, tfidf_pca_kmeans_cluster_test = None, None # \u6c7a\u5b9a\u6728\u30e2\u30c7\u30eb\u7528: emp_title\u5217\u306etfidf->kmeans\u7279\u5fb4\ntfidf_pca_logreg_prediction_train, tfidf_pca_logreg_prediction_test = None, None # \u6c7a\u5b9a\u6728\u30e2\u30c7\u30eb\u7528: emp_title\u5217\u306etfidf->LogisticRegression\u306e\u4e88\u6e2c\n\n# \u30ec\u30b3\u30fc\u30c9\u5168\u4f53\u306e\u30af\u30e9\u30b9\u30bf\u60c5\u5831\u3082\u30b0\u30ed\u30fc\u30d0\u30eb\u306b\u4fdd\u5b58\u3057\u3066\u304a\u304f\nrecords_pca_tsne_kmeans_cluster_train, records_pca_tsne_kmeans_cluster_test = None, None\nrecoeds_kmeans_cluster_train, recoeds_kmeans_cluster_test = None, None","a65900be":"# \u30ab\u30c6\u30b4\u30ea\u30fb\u6570\u5024\u30fb\u30d5\u30e9\u30b0\u7279\u5fb4\u91cf\u306e\u4e00\u89a7\u3082\u30b0\u30ed\u30fc\u30d0\u30eb\u3067\u88dc\u5b8c\ncols_qualitative, cols_quantitative, cols_flg = [], [], []\ncols_unnecessary = []\n\n# \u4e88\u6e2c\u7d50\u679c\u3092\u683c\u7d0d\u3001\u5e73\u5747\u5024\u306b\u3057\u3066\u63d0\u51fa\ny_pred_final = None\ny_preds_list_for_ensemble = []\n\n# \u6700\u5f8c\u306b\u30b9\u30b3\u30a2\u3092\u307e\u3068\u3081\u3066\u8868\u793a\u3059\u308b\u305f\u3081\u306e\u4e00\u6642\u4fdd\u7ba1\u5834\u6240\u3068\u3057\u3066\u5229\u7528\nscore_cv_avr, score_holdout = None, None\ncvscore_list_for_ensemble, holdoutscore_list_for_ensemble = [], []\n\n# LightGBM\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u30b0\u30ed\u30fc\u30d0\u30eb\u3067\u4fdd\u6301\nlgbm_params = {'objective': 'binary', 'metric': 'auc', 'learning_rate': 0.05}","f19d7a94":"def get_cols_to_use():\n    global cols_qualitative, cols_quantitative, cols_flg\n    return cols_qualitative+cols_quantitative+cols_flg","f8fbb8d0":"# \u30e1\u30a4\u30f3\u30d5\u30a1\u30a4\u30eb\u306e\u8aad\u307f\u8fbc\u307f\n# cache=True\u306a\u3089\u3001\u6642\u9593\u77ed\u7e2e\u306e\u305f\u3081\u4e8c\u56de\u76ee\u4ee5\u964d\u306e\u8aad\u307f\u8fbc\u307f\u6642\u306b\u30b3\u30d4\u30fc\u3092\u8fd4\u3059\ndef load_merged_df(cache=True):\n    print('load_merged_df(cache={})'.format(cache))\n    global df_train, df_test, df_train_ori, df_test_ori\n    \n    if df_train_ori is not None: # \u30ad\u30e3\u30c3\u30b7\u30e5\u304c\u3042\u308c\u3070\u30ad\u30e3\u30c3\u30b7\u30e5\u3092\u8fd4\u3059\n        print(' - already done, loading cache.')\n        df_train = df_train_ori.copy()\n        df_test = df_test_ori.copy()\n        return \n    else:\n        df_train_ori = pd.read_csv('{}\/train.csv'.format(DIR))\n        df_test_ori = pd.read_csv('{}\/test.csv'.format(DIR))\n        merge_other_tables() # \u30b0\u30ed\u30fc\u30d0\u30eb\u306edf_train_ori, df_test_ori\u306b\u7d50\u5408\n        df_train = df_train_ori.copy()\n        df_test = df_test_ori.copy()\n        if cache is False:\n            df_train_ori = None\n            df_test_ori = None\n        return \n\n# \u305d\u306e\u4ed6\u306e\u30d5\u30a1\u30a4\u30eb\u306e\u8aad\u307f\u8fbc\u307f, \u30de\u30fc\u30b8 \u203b\u30b0\u30ed\u30fc\u30d0\u30eb\u306e_ori\u306b\u30c6\u30fc\u30d6\u30eb\u3092\u4fdd\u5b58\ndef merge_other_tables():\n    global df_train_ori, df_test_ori\n\n    # df + df_zip_code\n    df_zip_code = pd.read_csv('{}\/free-zipcode-database.csv'.format(DIR), dtype={'WorldRegion': str})\n    df_zip_code = df_zip_code[['State', 'LocationType', 'TotalWages']]\n    df_zip_code = df_zip_code[~df_zip_code.duplicated(subset='State')]\n    df_train_merged = pd.merge(df_train_ori, df_zip_code, left_on='addr_state', right_on='State', how='left').drop(columns='State')\n    df_test_merged = pd.merge(df_test_ori, df_zip_code, left_on='addr_state', right_on='State', how='left').drop(columns='State')\n    del df_zip_code\n\n    # df_state_gdp + df_state_latlong\n    df_state_gdp = pd.read_csv('{}\/US_GDP_by_State.csv'.format(DIR))\n    df_state_latlong = pd.read_csv('{}\/statelatlong.csv'.format(DIR))\n    df_state_latlong = df_state_latlong[['State', 'City']]\n    df_state_latlong.rename(columns={'State': 'StateAbbr'}, inplace=True)\n    df_state_gdp_df_state_latlong = pd.merge(df_state_gdp, df_state_latlong, left_on='State', right_on='City', how='left').drop(columns='City')\n    df_state_gdp_df_state_latlong = df_state_gdp_df_state_latlong[df_state_gdp_df_state_latlong['year'] == 2015] # \u9762\u5012\u304f\u3055\u305d\u3046\u306a\u306e\u30672015\u306e\u30c7\u30fc\u30bf\u3060\u3051\u306b\u3059\u308b\n    del df_state_gdp, df_state_latlong\n\n    # (df + df_zip_code) + (df_state_gdp + df_state_latlong)\n    df_train_merged = pd.merge(df_train_merged, df_state_gdp_df_state_latlong, left_on='addr_state', right_on='StateAbbr', how='left').drop(columns=['State', 'StateAbbr', 'year'])\n    df_test_merged = pd.merge(df_test_merged, df_state_gdp_df_state_latlong, left_on='addr_state', right_on='StateAbbr', how='left').drop(columns=['State', 'StateAbbr', 'year'])\n    del df_state_gdp_df_state_latlong\n\n    df_train_ori = df_train_merged.rename(columns={col: 'EXTER:'+col for col in ['LocationType', 'TotalWages', 'State & Local Spending', \n                                                                             'Gross State Product', 'Real State Growth %', 'Population (million)']})\n    df_test_ori = df_test_merged.rename(columns={col: 'EXTER:'+col for col in ['LocationType', 'TotalWages', 'State & Local Spending', \n                                                                           'Gross State Product', 'Real State Growth %', 'Population (million)']})\n    del df_train_merged, df_test_merged\n\n    return ","9ee2c01b":"# \u65e5\u4ed8\u578b\u306b\u5909\u63db\ndef object2date(convert_target_col):\n    print('object2date(\\'{}\\')'.format(convert_target_col))\n    global df_train, df_test\n    \n    def replace_date_col(x):\n        return x.replace('Jan', '01-01').replace('Feb', '01-02').replace('Mar', '01-03').replace('Apr', '01-04').replace('May', '01-05').replace('Jun', '01-06').replace('Jul', '01-07').replace('Aug', '01-08').replace('Sep', '01-09').replace('Oct', '01-10').replace('Nov', '01-11').replace('Dec', '01-12')\n\n    df_train[convert_target_col] = df_train[convert_target_col].fillna('01-01-1900')\n    df_test[convert_target_col] = df_test[convert_target_col].fillna('01-01-1900')\n    \n    df_train[convert_target_col] = df_train[convert_target_col].map(replace_date_col)\n    df_test[convert_target_col] = df_test[convert_target_col].map(replace_date_col)\n\n    df_train[convert_target_col] = pd.to_datetime(df_train[convert_target_col], format='%d-%m-%Y')\n    df_test[convert_target_col] = pd.to_datetime(df_test[convert_target_col], format='%d-%m-%Y')\n    \n    return ","8170f661":"# \u6700\u65b0\u306e\u30c7\u30fc\u30bf\u306e\u307f\u3092\u62bd\u51fa\ndef extract_recent_data(oldest_year):\n    print('extract_recent_data(oldest_year={})'.format(oldest_year))\n    global df_train, df_test\n    \n    df_train = df_train[df_train['issue_d'] >= dt.datetime(oldest_year, 1, 1)]\n    df_test = df_test[df_test['issue_d'] >= dt.datetime(oldest_year, 1, 1)]\n    \n    return ","a1ac4185":"# \u7c21\u5358\u306a\u9806\u5e8f\u5c3a\u5ea6\u3092int\u578b\u306b\u5909\u63db\ndef ordinal2int(convert_target_col):\n    print('ordinal2int(\\'{}\\')'.format(convert_target_col))\n    global df_train, df_test\n    \n    convert_mae = sorted(df_train[convert_target_col].unique())\n    convert_ato = list(range(len(convert_mae))[::-1])\n    df_train[convert_target_col] = df_train[convert_target_col].replace(convert_mae, convert_ato)\n    df_train[convert_target_col] = df_train[convert_target_col].astype(int)\n    df_test[convert_target_col] = df_test[convert_target_col].replace(convert_mae, convert_ato)\n    df_test[convert_target_col] = df_test[convert_target_col].astype(int)\n    \n    return","c3cf22f1":"# \u7279\u6b8a\u306a\u66f8\u304d\u65b9\u306e\u9806\u5e8f\u5c3a\u5ea6\u3092int\u578b\u306b\u5909\u63db\ndef emplength2int():\n    print('emplength2int()')\n    global df_train, df_test\n    \n    convert_target_col = 'emp_length'\n\n    df_train[convert_target_col] = df_train[convert_target_col].fillna('#')\n    df_test[convert_target_col] = df_test[convert_target_col].fillna('#')\n\n    convert_mae = ['#', '< 1 year', '1 year', '2 years', '3 years', '4 years', '5 years', '6 years', '7 years', '8 years', '9 years', '10+ years']\n    convert_ato = [-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    df_train[convert_target_col] = df_train[convert_target_col].replace(convert_mae, convert_ato)\n    df_train[convert_target_col] = df_train[convert_target_col].astype(int)\n    df_test[convert_target_col] = df_test[convert_target_col].replace(convert_mae, convert_ato)\n    df_test[convert_target_col] = df_test[convert_target_col].astype(int)\n    \n    return","f99901f9":"def object2str(convert_target_col):\n    print('object2str(\\'{}\\')'.format(convert_target_col))\n    global df_train, df_test\n    \n    df_train[convert_target_col] = df_train[convert_target_col].fillna('#')\n    df_train[convert_target_col] = df_train[convert_target_col].astype(str)\n    df_test[convert_target_col] = df_test[convert_target_col].fillna('#')\n    df_test[convert_target_col] = df_test[convert_target_col].astype(str)\n    \n    return","0acdff6e":"def preprocess_text(preprocess_target_col):\n    print('preprocess_text(\\'{}\\')'.format(preprocess_target_col))\n    global df_train, df_test, sr_title_train, sr_title_test, sr_emp_title_train, sr_emp_title_test\n    \n    def replace_text(x):\n        x = x.replace('\\t', '').replace('!', '').replace('?', '').replace(':)', '').replace('&', '')\\\n        .replace('\/', '').replace('-', '').replace('.', '').replace('\\\"', '').replace('\\'', '').replace(',', '')\\\n        .replace('$', '').replace('+', '').replace('(', '').replace(')', '').replace('%', '').replace('_', ' ')\n        x = re.sub('\\d+', '', x) # \u6570\u5b57\u524a\u9664\n        x = re.sub('  +', ' ', x) # \u7a7a\u767d\u306e\u9023\u7d9a\u306f\u7a7a\u767d1\u3064\u306b\n        x = re.sub('^ ', '', x) # 1\u6587\u5b57\u76ee\u306e\u7a7a\u767d\u524a\u9664\n        return x\n    \n    # \u3059\u3067\u306b\u30c6\u30ad\u30b9\u30c8\u51e6\u7406\u3092\u3057\u3066\u3044\u305f\u3089\u30ad\u30e3\u30c3\u30b7\u30e5\u306e\u30b3\u30d4\u30fc\u3092\u8fd4\u3059\n    if preprocess_target_col == 'title':\n        if sr_title_train is not None:\n            print(' - already done, loading cache.')\n            df_train[preprocess_target_col] = sr_title_train\n            df_test[preprocess_target_col] = sr_title_test\n            return\n        \n        else:\n            df_train[preprocess_target_col] = df_train[preprocess_target_col].str.lower()\n            df_test[preprocess_target_col] = df_test[preprocess_target_col].str.lower()\n            df_train[preprocess_target_col] = df_train[preprocess_target_col].map(replace_text)\n            df_test[preprocess_target_col] = df_test[preprocess_target_col].map(replace_text)\n            sr_title_train = df_train[preprocess_target_col].copy()\n            sr_title_test = df_test[preprocess_target_col].copy()\n            \n    else:\n        if sr_emp_title_train is not None:\n            print(' - already done, loading cache.')\n            df_train[preprocess_target_col] = sr_emp_title_train\n            df_test[preprocess_target_col] = sr_emp_title_test\n            return\n        \n        else:\n            df_train[preprocess_target_col] = df_train[preprocess_target_col].str.lower()\n            df_test[preprocess_target_col] = df_test[preprocess_target_col].str.lower()\n            df_train[preprocess_target_col] = df_train[preprocess_target_col].map(replace_text)\n            df_test[preprocess_target_col] = df_test[preprocess_target_col].map(replace_text)\n            sr_emp_title_train = df_train[preprocess_target_col].copy()\n            sr_emp_title_test = df_test[preprocess_target_col].copy()\n\n    return ","b909ae48":"def preprocess_words_of_text(preprocess_target_col):\n    print('preprocess_words_of_text(\\'{}\\')'.format(preprocess_target_col))\n    global df_train, df_test\n    \n    def replace_text(x):\n        x = x.replace('my ', '').replace('bye ', '').replace('the ', '') # stop words\n        x = x.replace('cards', 'card').replace('credit consolidation', 'credit card consolidation')\\\n        .replace('payoff', 'pay off').replace('cc', 'credit card').replace('improvements', 'improvement')\\\n        .replace('refi ', 'refinancing ').replace('refinance', 'refinancing').replace('consoildation', 'consolidation')\\\n        .replace('considation', 'consolidation').replace('consolodation', 'consolidation')\\\n        .replace('refy ', 'refinancing ').replace('bills', 'bill') # nayose\n        x = re.sub('refi$', 'refinancing', x) # refi\u3067\u7d42\u308f\u308b\n        x = re.sub('refy$', 'refinancing', x) # refi\u3067\u7d42\u308f\u308b\n        return x\n\n    df_train[preprocess_target_col] = df_train[preprocess_target_col].map(replace_text)\n    df_test[preprocess_target_col] = df_test[preprocess_target_col].map(replace_text)\n    \n    return ","f408bc2d":"def zipcode2str():\n    print('zipcode2str()')\n    global df_train, df_test\n    \n    convert_target_col = 'zip_code'\n\n    df_train[convert_target_col] = df_train[convert_target_col].str.replace('xx', '')\n    df_train[convert_target_col] = df_train[convert_target_col].astype(str)\n    df_test[convert_target_col] = df_test[convert_target_col].str.replace('xx', '')\n    df_test[convert_target_col] = df_test[convert_target_col].astype(str)\n    \n    return","8ebce4af":"def load_cols_by_dtype():\n    print('load_cols_by_dtype()')\n    global df_train, df_test, cols_qualitative, cols_quantitative, cols_flg, cols_unnecessary\n    \n    cols_necessary = [col for col in df_train.columns if col not in cols_unnecessary+[COL_ID, COL_TARGET]]\n    cols_qualitative = sorted([col for col, dtype in df_train[cols_necessary].dtypes.iteritems() if dtype == 'object'])\n    cols_quantitative = [col for col, dtype in df_train[cols_necessary].dtypes.iteritems() if dtype != 'object']\n    cols_flg = sorted([col for col in cols_quantitative if df_train[col].unique().tolist() == [0, 1]])\n    cols_quantitative = sorted(list(set(cols_quantitative) - set(cols_flg)))\n\n    print(' - unnecessary: {}'.format(cols_unnecessary))\n    print(' - category: {}'.format(cols_qualitative))\n    print(' - numeric: {}'.format(cols_quantitative))\n    print(' - flag: {}'.format(cols_flg))\n    \n    return ","6e4fecde":"def plot_histgrams(plot_target_cols):\n    print('plot_histgrams(plot_target_cols={})'.format(plot_target_cols))\n    global df_train, df_test\n    \n    def plot_hist(col):\n        plt.figure(figsize=[6, 4])\n        df_train[col].hist(density=True, alpha=0.5, bins=50)\n        df_test[col].hist(density=True, alpha=0.5, bins=50)\n        plt.xlabel(col)\n        plt.ylabel('density')\n        plt.show()\n        plt.close()\n\n    for col in plot_target_cols:\n        print(col, '\\n')\n        print('n_unique: ', len(df_train[col].unique()))\n        print('n_null: ', len(df_train[df_train[col].isnull()]))\n        print(df_train[col].describe())\n        print(df_test[col].describe())\n        plt.figure()\n        plot_hist(col)\n        \n    return ","39ee2dd4":"# \u30c6\u30ad\u30b9\u30c8\u5217\u306e\u6b20\u640d\u88dc\u5b8c(#\u3067\u65e2\u306b\u88dc\u5b8c\u6e08\u307f\u306a\u306e\u3067\u30d5\u30e9\u30b0\u51e6\u7406\u306e\u307f)\ndef fillna_text(fillna_target_col, add_flg=False):\n    print('fillna_text(\\'{}\\')'.format(fillna_target_col), '+flg' if add_flg else '')\n    global df_train, df_test, cols_quantitative\n    \n    isnull_flgs_train = (df_train[fillna_target_col] == '#')\n    isnull_flgs_test = (df_test[fillna_target_col] == '#')\n\n    # Adding isnull_flg\n    if add_flg:\n        col_isnull_flg = 'FLG:{}_isnull'.format(fillna_target_col)\n        df_train[col_isnull_flg] = 0\n        df_train.loc[isnull_flgs_train, col_isnull_flg] = 1\n        df_test[col_isnull_flg] = 0\n        df_test.loc[isnull_flgs_test, col_isnull_flg] = 1\n        cols_quantitative.append(col_isnull_flg)\n    \n    return ","7271ae08":"# \u6570\u5024\u5217\u306e\u6b20\u640d\u88dc\u5b8c\ndef fillna_numeric(fillna_target_col, fillna_by='median', val_min=None, val_max=None, add_flg=False):\n    print('fillna_numeric(\\'{}\\', fillna_by={})'.format(fillna_target_col, fillna_by), '+flg' if add_flg else '')\n    global df_train, df_test, cols_quantitative\n    \n    isnull_flgs_train = df_train[fillna_target_col].isnull()\n    isnull_flgs_test = df_test[fillna_target_col].isnull()\n    \n    # filling NA\n    if (type(fillna_by) is int) or (type(fillna_by) is float): # \u88dc\u5b8c\u5024\u6307\u5b9a\u306e\u5834\u5408\n        df_train.loc[isnull_flgs_train, fillna_target_col] = fillna_by\n        df_test.loc[isnull_flgs_test, fillna_target_col] = fillna_by\n        \n    if fillna_by == 'random': # random\u306e\u5834\u5408(\u5168\u3066\u306e\u5024\u304b\u3089\u30e9\u30f3\u30c0\u30e0\u62bd\u51fa\u3057\u3066\u88dc\u5b8c)\n        fill_by_train = random.choices(df_train.loc[~isnull_flgs_train, fillna_target_col].tolist(), k=len(df_train[isnull_flgs_train]))\n        fill_by_test = random.choices(df_test.loc[~isnull_flgs_test, fillna_target_col].tolist(), k=len(df_test[isnull_flgs_test]))\n        df_train.loc[isnull_flgs_train, fillna_target_col] = fill_by_train\n        df_test.loc[isnull_flgs_test, fillna_target_col] = fill_by_test\n        \n    if fillna_by == 'minmax': # minmax\u306e\u5834\u5408(\u6700\u5c0f\u5024\u3068\u6700\u5927\u5024\u3067\u7d5e\u3089\u308c\u308b\u5019\u88dc\u306e\u5024\u7fa4\u304b\u3089\u30e9\u30f3\u30c0\u30e0\u62bd\u51fa\u3057\u3066\u88dc\u5b8c)\n        val_btwn_minmax_flgs = df_train[fillna_target_col].between(val_min, val_max)\n        val_btwn_minmax = df_train.loc[val_btwn_minmax_flgs, fillna_target_col].tolist()\n        fill_by_train = random.choices(val_btwn_minmax, k=len(df_train[isnull_flgs_train]))\n        fill_by_test = random.choices(val_btwn_minmax, k=len(df_test[isnull_flgs_test]))\n        df_train.loc[isnull_flgs_train, fillna_target_col] = fill_by_train\n        df_test.loc[isnull_flgs_test, fillna_target_col] = fill_by_test\n        \n    if fillna_by == 'median': # \u4e2d\u592e\u5024\u88dc\u5b8c\u306e\u5834\u5408\n        median = df_train[fillna_target_col].median()\n        df_train.loc[isnull_flgs_train, fillna_target_col] = median\n        df_test.loc[isnull_flgs_test, fillna_target_col] = median\n        \n    # Adding isnull_flg\n    if add_flg:\n        col_isnull_flg = 'FLG:{}_isnull'.format(fillna_target_col)\n        df_train[col_isnull_flg] = 0\n        df_train.loc[isnull_flgs_train, col_isnull_flg] = 1\n        df_test[col_isnull_flg] = 0\n        df_test.loc[isnull_flgs_test, col_isnull_flg] = 1\n        cols_quantitative.append(col_isnull_flg)\n    \n    return ","0f5deaf7":"def log_transformation(transform_target_col, remove_outlier=False, add_flg=False):\n    print('log_transformation(\\'{}\\')'.format(transform_target_col), '+remove_outlier' if remove_outlier else '', '+flg' if add_flg else '')\n    global df_train, df_test, cols_flg\n    \n    df_train[transform_target_col] = df_train[transform_target_col].apply(np.log1p)\n    df_test[transform_target_col] = df_test[transform_target_col].apply(np.log1p)\n    \n    # Removing Outliers\n    if remove_outlier:\n        p01 = df_train[transform_target_col].quantile(0.01)\n        p99 = df_train[transform_target_col].quantile(0.99)\n        isoutlier_flgs_train = ((df_train[transform_target_col] < p01) | (df_train[transform_target_col] > p99))\n        isoutlier_flgs_test = ((df_test[transform_target_col] < p01) | (df_test[transform_target_col] > p99))\n        #print(' - number of outlier(train): {}'.format(len(df_train[isoutlier_flgs_train])))\n        #print(' - number of outlier(test) : {}'.format(len(df_test[isoutlier_flgs_test])))\n        df_train[transform_target_col] = df_train[transform_target_col].clip(p01, p99)\n        df_test[transform_target_col] = df_test[transform_target_col].clip(p01, p99)\n    \n        # Adding outlier_flg\n        if add_flg:\n            col_isoutlier_flg = 'FLG:{}_isoutlier'.format(transform_target_col)\n            df_train[col_isoutlier_flg] = 0\n            df_train.loc[isoutlier_flgs_train, col_isoutlier_flg] = 1\n            df_test[col_isoutlier_flg] = 0\n            df_test.loc[isoutlier_flgs_test, col_isoutlier_flg] = 1\n            update_cols(add_cols_flg=[col_isoutlier_flg])\n        \n    return ","c09783a7":"# \u4f8b\u3048\u3070interval=0.5\u306a\u3089\u3001[1.2, 2.5, 3.8]\u306f[1.0, 2.5, 3.5]\u306b\u5909\u63db\u3055\u308c\u308b\ndef binning_values(cut_target_col, interval):\n    print('cut_values(\\'{}\\', interval={})'.format(cut_target_col, interval))\n    global df_train, df_test\n    \n    if int(str(interval*1000).split('.')[1]) != 0: # \u5c0f\u6570\u70b9\u4ee5\u4e0b\u7b2c3\u4f4d\u307e\u3067\u306a\u3089interval\u306b\u6307\u5b9a\u53ef\u80fd(0.001\u306f\u30aa\u30c3\u30b1\u30fc\u30010.0005\u3068\u304b\u306f\u3060\u3081)\n        print(' > error: interval is too small')\n        return\n    \n    # \u4fc2\u6570\u3092\u7b97\u51fa\n    a = 1\n    for i in range(3):\n        if interval*a < 1:\n            a *= 10\n    \n    # \u7aef\u6570\u3092\u5207\u308a\u6368\u3066\u308b\n    def cut(x):\n        return (x*a-((x*a)%(interval*a)))\/a\n        \n    df_train[cut_target_col] = df_train[cut_target_col].map(cut)\n    df_test[cut_target_col] = df_test[cut_target_col].map(cut)\n    \n    return ","4ccba87f":"def rank_gauss_transformation(n_split=5):\n    print('rank_gauss_transformation(n_split={})'.format(n_split))\n    global df_train, df_test, cols_quantitative\n    \n    def split_list(l, n):\n        for idx in range(0, len(l), n):\n            yield l[idx:idx + n]\n    \n    # \u30e1\u30e2\u30ea\u3092\u98df\u3046\u306e\u306710\u500b\u305a\u3064\u306b\u5206\u3051\u3066\u5b9f\u65bd\n    for cols in list(split_list(cols_quantitative, n_split)):\n        df_all = pd.concat([df_train[cols], df_test[cols]], axis=0)\n        df_all[cols] = quantile_transform(df_all[cols], n_quantiles=100, random_state=0, output_distribution='normal')\n        df_train_transformed = df_all.iloc[:df_train.shape[0], :]\n        df_test_transformed = df_all.iloc[df_train.shape[0]:, :]\n        del df_all\n        df_train = pd.concat([df_train.drop(columns=cols), df_train_transformed], axis=1)\n        df_test = pd.concat([df_test.drop(columns=cols), df_test_transformed], axis=1)\n        del df_train_transformed, df_test_transformed\n\n    return","39249019":"# \u30c6\u30ad\u30b9\u30c8\u5217\u306e\u6b20\u640d\u88dc\u5b8c(#\u3067\u65e2\u306b\u88dc\u5b8c\u6e08\u307f\u306a\u306e\u3067\u30d5\u30e9\u30b0\u51e6\u7406\u306e\u307f)\ndef fillna_text(fillna_target_col, add_flg=False):\n    print('fillna_text(\\'{}\\')'.format(fillna_target_col), '+flg' if add_flg else '')\n    global df_train, df_test, cols_flg\n    \n    df_train[fillna_target_col] = df_train[fillna_target_col].fillna('#')\n    df_test[fillna_target_col] = df_test[fillna_target_col].fillna('#')\n    isnull_flgs_train = (df_train[fillna_target_col] == '#')\n    isnull_flgs_test = (df_test[fillna_target_col] == '#')\n\n    # Adding isnull_flg\n    if add_flg:\n        col_isnull_flg = 'FLG:{}_isnull'.format(fillna_target_col)\n        df_train[col_isnull_flg] = 0\n        df_train.loc[isnull_flgs_train, col_isnull_flg] = 1\n        df_test[col_isnull_flg] = 0\n        df_test.loc[isnull_flgs_test, col_isnull_flg] = 1\n        update_cols(add_cols_flg=[col_isnull_flg])\n    \n    return ","98642b0c":"# \u6570\u5024\u5217\u306e\u6b20\u640d\u88dc\u5b8c\ndef fillna_numeric(fillna_target_col, fillna_by='median', val_min=None, val_max=None, add_flg=False):\n    print('fillna_numeric(\\'{}\\', fillna_by={})'.format(fillna_target_col, fillna_by), '+flg' if add_flg else '')\n    global df_train, df_test, cols_flg\n    \n    isnull_flgs_train = df_train[fillna_target_col].isnull()\n    isnull_flgs_test = df_test[fillna_target_col].isnull()\n    \n    # filling NA\n    if (type(fillna_by) is int) or (type(fillna_by) is float): # \u88dc\u5b8c\u5024\u6307\u5b9a\u306e\u5834\u5408\n        df_train.loc[isnull_flgs_train, fillna_target_col] = fillna_by\n        df_test.loc[isnull_flgs_test, fillna_target_col] = fillna_by\n        \n    if fillna_by == 'random': # random\u306e\u5834\u5408(\u5168\u3066\u306e\u5024\u304b\u3089\u30e9\u30f3\u30c0\u30e0\u62bd\u51fa\u3057\u3066\u88dc\u5b8c)\n        fill_by_train = random.choices(df_train.loc[~isnull_flgs_train, fillna_target_col].tolist(), k=len(df_train[isnull_flgs_train]))\n        fill_by_test = random.choices(df_test.loc[~isnull_flgs_test, fillna_target_col].tolist(), k=len(df_test[isnull_flgs_test]))\n        df_train.loc[isnull_flgs_train, fillna_target_col] = fill_by_train\n        df_test.loc[isnull_flgs_test, fillna_target_col] = fill_by_test\n        \n    if fillna_by == 'minmax': # minmax\u306e\u5834\u5408(\u6700\u5c0f\u5024\u3068\u6700\u5927\u5024\u3067\u7d5e\u3089\u308c\u308b\u5019\u88dc\u306e\u5024\u7fa4\u304b\u3089\u30e9\u30f3\u30c0\u30e0\u62bd\u51fa\u3057\u3066\u88dc\u5b8c)\n        val_btwn_minmax_flgs = df_train[fillna_target_col].between(val_min, val_max)\n        val_btwn_minmax = df_train.loc[val_btwn_minmax_flgs, fillna_target_col].tolist()\n        fill_by_train = random.choices(val_btwn_minmax, k=len(df_train[isnull_flgs_train]))\n        fill_by_test = random.choices(val_btwn_minmax, k=len(df_test[isnull_flgs_test]))\n        df_train.loc[isnull_flgs_train, fillna_target_col] = fill_by_train\n        df_test.loc[isnull_flgs_test, fillna_target_col] = fill_by_test\n        \n    if fillna_by == 'median': # \u4e2d\u592e\u5024\u88dc\u5b8c\u306e\u5834\u5408\n        df_train.loc[isnull_flgs_train, fillna_target_col] = df_train[fillna_target_col].median()\n        df_test.loc[isnull_flgs_test, fillna_target_col] = df_test[fillna_target_col].median()\n        \n    # Adding isnull_flg\n    if add_flg:\n        col_isnull_flg = 'FLG:{}_isnull'.format(fillna_target_col)\n        df_train[col_isnull_flg] = 0\n        df_train.loc[isnull_flgs_train, col_isnull_flg] = 1\n        df_test[col_isnull_flg] = 0\n        df_test.loc[isnull_flgs_test, col_isnull_flg] = 1\n        update_cols(add_cols_flg=[col_isnull_flg])\n    \n    return ","e1dd91c4":"def update_cols(add_cols_qualitative=[], add_cols_quantitative=[], add_cols_flg=[], add_cols_unnecessary=[], show_detail=False):\n    global df_train, df_test, cols_qualitative, cols_quantitative, cols_flg, cols_unnecessary\n    if show_detail:\n        print(' - update_cols(add_cols_qualitative={}, add_cols_quantitative={}, add_cols_flg={}, add_cols_unnecessary={})'.format(add_cols_qualitative, add_cols_quantitative, add_cols_flg, add_cols_unnecessary))\n    \n    if add_cols_qualitative != []:\n        cols_qualitative = sorted(list(set(cols_qualitative) | set(add_cols_qualitative)))\n    \n    if add_cols_quantitative != []:\n        cols_quantitative = sorted(list(set(cols_quantitative) | set(add_cols_quantitative)))\n        \n    if add_cols_flg != []:\n        cols_flg = sorted(list(set(cols_flg) | set(add_cols_flg)))\n    \n    if add_cols_unnecessary != []:\n        cols_unnecessary = sorted(list(set(cols_unnecessary) | set(add_cols_unnecessary)))\n        \n    cols_qualitative = [col for col in cols_qualitative if col not in cols_unnecessary]\n    cols_quantitative = [col for col in cols_quantitative if col not in cols_unnecessary]\n    cols_flg = [col for col in cols_flg if col not in cols_unnecessary]\n    \n    if show_detail:\n        print(' - no use  : {}'.format(cols_unnecessary))\n        print(' > category: {}'.format(cols_qualitative))\n        print(' > numeric : {}'.format(cols_quantitative))\n        print(' > flag    : {}'.format(cols_flg))\n    \n    return ","0cb7307f":"def tfidf_pca_kmeans_cluster(extraction_target_col, n_dim_tfidf=500, n_dim_pca=50, n_cluster=100):\n    print('tfidf_pca_kmeans_cluster(\\'{}\\', n_dim_tfidf={}, n_dim_pca={}, n_cluster={})'.format(extraction_target_col, n_dim_tfidf, n_dim_pca, n_cluster))\n    global df_train, df_test\n    global tfidf_pca_kmeans_cluster_train, tfidf_pca_kmeans_cluster_test\n    \n    extraction_begin = time.time()\n    \n    if tfidf_pca_kmeans_cluster_train is not None: # 2\u56de\u76ee\u4ee5\u964d\u306f\u30ad\u30e3\u30c3\u30b7\u30e5\u306e\u30b3\u30d4\u30fc\u3092\u8fd4\u3059\n        print(' - already done, loading cache.')\n        col_text_cluster = 'TXT:kmeans_cluster_of_{}'.format(extraction_target_col)\n        df_train[col_text_cluster] = tfidf_pca_kmeans_cluster_train.copy()\n        df_test[col_text_cluster] = tfidf_pca_kmeans_cluster_test.copy()\n        df_train[col_text_cluster] = df_train[col_text_cluster].astype(object)\n        df_test[col_text_cluster] = df_test[col_text_cluster].astype(object)\n        update_cols(add_cols_qualitative=[col_text_cluster])\n\n    else:\n        print(' - run TfidfVectorizer')\n        tfidfv = TfidfVectorizer(max_features=n_dim_tfidf, use_idf=True, analyzer='word', ngram_range=(1, 2))\n        tfidfv.fit_transform(df_train[extraction_target_col])\n        f_text_train = tfidfv.transform(df_train[extraction_target_col]).todense()\n        f_text_test = tfidfv.transform(df_test[extraction_target_col]).todense()\n        del tfidfv\n\n        if n_dim_pca != -1:\n            print(' - run PCA')\n            pca = PCA(n_components=n_dim_pca)\n            pca.fit(f_text_train)\n            f_text_train = pca.transform(f_text_train)\n            f_text_test = pca.transform(f_text_test)\n            del pca\n\n        print(' - run MiniBatchKMeans')\n        kmeans = MiniBatchKMeans(n_clusters=n_cluster, batch_size=100)\n        clusters_train = kmeans.fit(f_text_train).labels_\n        del f_text_train\n        clusters_test = kmeans.fit(f_text_test).labels_\n        del f_text_test\n        del kmeans\n\n        col_text_cluster = 'TXT:kmeans_cluster_of_{}'.format(extraction_target_col)\n        df_train[col_text_cluster] = clusters_train\n        df_test[col_text_cluster] = clusters_test\n        df_train[col_text_cluster] = df_train[col_text_cluster].astype(object)\n        df_test[col_text_cluster] = df_test[col_text_cluster].astype(object)\n        update_cols(add_cols_qualitative=[col_text_cluster])\n        \n        tfidf_pca_kmeans_cluster_train = clusters_train\n        tfidf_pca_kmeans_cluster_test = clusters_test\n    \n    extraction_elapsed_time = time.time()-extraction_begin\n    extraction_elapsed_sec = int(extraction_elapsed_time%60)\n    extraction_elapsed_min = int((extraction_elapsed_time-extraction_elapsed_sec)\/60)\n    print(' > elapsed time: {}:{}'.format(extraction_elapsed_min, str(extraction_elapsed_sec).rjust(2, '0')))\n    \n    return ","bba34b19":"def tfidf_pca_logreg_prediction(extraction_target_col, n_dim_tfidf=500, n_dim_pca=50):\n    print('tfidf_pca_logreg_prediction(\\'{}\\', n_dim_tfidf={}, n_dim_pca={})'.format(extraction_target_col, n_dim_tfidf, n_dim_pca))\n    global df_train, df_test\n    global tfidf_pca_logreg_prediction_train, tfidf_pca_logreg_prediction_test\n    \n    extraction_begin = time.time()\n    \n    if tfidf_pca_logreg_prediction_train is not None: # 2\u56de\u76ee\u4ee5\u964d\u306f\u30ad\u30e3\u30c3\u30b7\u30e5\u306e\u30b3\u30d4\u30fc\u3092\u8fd4\u3059\n        print(' - already done, loading cache.')\n        col_logreg_prediction = 'TXT:logreg_prediction_on_{}'.format(extraction_target_col)\n        df_train[col_logreg_prediction] = tfidf_pca_logreg_prediction_train\n        df_test[col_logreg_prediction] = tfidf_pca_logreg_prediction_test\n        update_cols(add_cols_quantitative=[col_logreg_prediction])\n        \n    else:\n        # tfidf\n        print(' - run TfidfVectorizer')\n        tfidfv = TfidfVectorizer(max_features=n_dim_tfidf, use_idf=True, analyzer='word', ngram_range=(1, 2))\n        tfidfv.fit_transform(df_train[extraction_target_col])\n        f_text_train = tfidfv.transform(df_train[extraction_target_col]).todense()\n        f_text_test = tfidfv.transform(df_test[extraction_target_col]).todense()\n        del tfidfv\n\n        # pca\n        if n_dim_pca != -1:\n            print(' - run PCA')\n            pca = PCA(n_components=n_dim_pca)\n            pca.fit(f_text_train)\n            f_text_train = pca.transform(f_text_train)\n            f_text_test = pca.transform(f_text_test)\n            del pca\n\n        # LggisticRegression: CV\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3092\u8a66\u3057\u3066\u307f\u308b\n        print(' - run LogisticRegression')\n        X_train = f_text_train\n        y_train = df_train[COL_TARGET].values\n        X_test = f_text_test\n        del f_text_train, f_text_test\n\n        n_folds = 5\n        scores = []\n        y_preds_sum_train = [0 for i in range(len(X_train))]\n        y_preds_sum_test = [0 for i in range(len(X_test))]\n\n        clf = LogisticRegression(C=0.01)\n        skf = StratifiedKFold(n_splits=n_folds, random_state=81, shuffle=True)\n        for i, (train_ix, val_ix) in enumerate(skf.split(X_train, y_train)):\n            fold_begin = time.time()\n            _X_train, _y_train = X_train[train_ix], y_train[train_ix]\n            _X_val, _y_val = X_train[val_ix], y_train[val_ix]\n\n            clf.fit(_X_train, _y_train)\n            _y_pred_train = clf.predict_proba(X_train)[:, 1]\n            _y_pred_test = clf.predict_proba(X_test)[:, 1]\n            _score = roc_auc_score(y_train, _y_pred_train)\n            y_preds_sum_train += _y_pred_train\n            y_preds_sum_test += _y_pred_test\n            scores.append(_score)\n\n            elapsed_time = time.time()-fold_begin\n            elapsed_sec = int(elapsed_time%60)\n            elapsed_min = int((elapsed_time-elapsed_sec)\/60)\n            print(' - LogisticRegression CV Score of Fold {} is {:.5f} ({}:{})'.format(i, _score, elapsed_min, str(elapsed_sec).rjust(2, '0')))\n\n        score_cv_avr = np.mean(scores)\n        print(' > LogisticRegression Average CV Score is {:.5f}'.format(score_cv_avr))\n        del X_train, X_test\n\n        # \u4e88\u6e2c\u5024\u3092\u5217\u3068\u3057\u3066\u8ffd\u52a0\n        col_logreg_prediction = 'TXT:logreg_prediction_on_{}'.format(extraction_target_col)\n        df_train[col_logreg_prediction] = y_preds_sum_train \/ n_folds\n        df_test[col_logreg_prediction] = y_preds_sum_test \/ n_folds\n        update_cols(add_cols_quantitative=[col_logreg_prediction])\n        \n        # \u30ad\u30e3\u30c3\u30b7\u30e5\u3092\u4fdd\u5b58\n        tfidf_pca_logreg_prediction_train = y_preds_sum_train \/ n_folds\n        tfidf_pca_logreg_prediction_test = y_preds_sum_test \/ n_folds\n    \n    extraction_elapsed_time = time.time()-extraction_begin\n    extraction_elapsed_sec = int(extraction_elapsed_time%60)\n    extraction_elapsed_min = int((extraction_elapsed_time-extraction_elapsed_sec)\/60)\n    print(' > elapsed time: {}:{}'.format(extraction_elapsed_min, str(extraction_elapsed_sec).rjust(2, '0')))\n    \n    return ","89ce3d14":"# \u7dda\u5f62\u30e2\u30c7\u30eb\u7528\ndef tfidf_pca(extraction_target_col, n_dim_tfidf=500, n_dim_pca=10):\n    print('tfidf_pca(\\'{}\\', n_dim_tfidf={}, n_dim_pca={})'.format(extraction_target_col, n_dim_tfidf, n_dim_pca))\n    global df_train, df_test\n    global tfidf_pca_title_train, tfidf_pca_title_test, tfidf_pca_emp_title_train, tfidf_pca_emp_title_test\n    \n    extraction_begin = time.time()\n    \n    if extraction_target_col == 'title':\n        if tfidf_pca_title_train is not None: # 2\u56de\u76ee\u4ee5\u964d\u306f\u30ad\u30e3\u30c3\u30b7\u30e5\u306e\u30b3\u30d4\u30fc\u3092\u8fd4\u3059\n            print(' - already done, loading cache.')\n            df_train = df_train.reset_index(drop=True)\n            df_test = df_test.reset_index(drop=True)\n            df_train = pd.concat([df_train, tfidf_pca_title_train], axis=1)\n            df_test = pd.concat([df_test, tfidf_pca_title_test], axis=1)\n            update_cols(add_cols_quantitative=tfidf_pca_title_train.columns.tolist())\n            \n        else:\n            print(' - run TfidfVectorizer')\n            tfidfv = TfidfVectorizer(max_features=n_dim_tfidf, use_idf=True, analyzer='word', ngram_range=(1, 2))\n            tfidfv.fit_transform(df_train[extraction_target_col])\n            f_text_train = tfidfv.transform(df_train[extraction_target_col]).todense()\n            f_text_test = tfidfv.transform(df_test[extraction_target_col]).todense()\n            del tfidfv\n\n            print(' - run PCA')\n            pca = PCA(n_components=n_dim_pca)\n            pca.fit(f_text_train)\n            f_text_train = pca.transform(f_text_train)\n            f_text_test = pca.transform(f_text_test)\n            del pca\n\n            cols_tfidf_pca = ['{}_tfidf_pca_{}'.format(extraction_target_col, i) for i in range(n_dim_pca)]\n            tfidf_pca_title_train = pd.DataFrame(columns=cols_tfidf_pca, data=f_text_train)\n            tfidf_pca_title_test = pd.DataFrame(columns=cols_tfidf_pca, data=f_text_test)\n            tfidf_pca_title_train = tfidf_pca_title_train.reset_index(drop=True)\n            tfidf_pca_title_test = tfidf_pca_title_test.reset_index(drop=True)\n            del f_text_train, f_text_test\n\n            df_train = df_train.reset_index(drop=True)\n            df_test = df_test.reset_index(drop=True)\n            df_train = pd.concat([df_train, tfidf_pca_title_train], axis=1)\n            df_test = pd.concat([df_test, tfidf_pca_title_test], axis=1)\n            update_cols(add_cols_quantitative=tfidf_pca_title_train.columns.tolist())\n\n    else:\n        if tfidf_pca_emp_title_train is not None: # 2\u56de\u76ee\u4ee5\u964d\u306f\u30ad\u30e3\u30c3\u30b7\u30e5\u306e\u30b3\u30d4\u30fc\u3092\u8fd4\u3059\n            print(' - already done, loading cache.')\n            df_train = df_train.reset_index(drop=True)\n            df_test = df_test.reset_index(drop=True)\n            df_train = pd.concat([df_train, tfidf_pca_emp_title_train], axis=1)\n            df_test = pd.concat([df_test, tfidf_pca_emp_title_test], axis=1)\n            update_cols(add_cols_quantitative=tfidf_pca_emp_title_train.columns.tolist())\n            \n        else:\n            print(' - run TfidfVectorizer')\n            tfidfv = TfidfVectorizer(max_features=n_dim_tfidf, use_idf=True, analyzer='word', ngram_range=(1, 2))\n            tfidfv.fit_transform(df_train[extraction_target_col])\n            f_text_train = tfidfv.transform(df_train[extraction_target_col]).todense()\n            f_text_test = tfidfv.transform(df_test[extraction_target_col]).todense()\n            del tfidfv\n\n            print(' - run PCA')\n            pca = PCA(n_components=n_dim_pca)\n            pca.fit(f_text_train)\n            f_text_train = pca.transform(f_text_train)\n            f_text_test = pca.transform(f_text_test)\n            del pca\n\n            cols_tfidf_pca = ['{}_tfidf_pca_{}'.format(extraction_target_col, i) for i in range(n_dim_pca)]\n            tfidf_pca_emp_title_train = pd.DataFrame(columns=cols_tfidf_pca, data=f_text_train)\n            tfidf_pca_emp_title_test = pd.DataFrame(columns=cols_tfidf_pca, data=f_text_test)\n            tfidf_pca_emp_title_train = tfidf_pca_emp_title_train.reset_index(drop=True)\n            tfidf_pca_emp_title_test = tfidf_pca_emp_title_test.reset_index(drop=True)\n            del f_text_train, f_text_test\n\n            df_train = df_train.reset_index(drop=True)\n            df_test = df_test.reset_index(drop=True)\n            df_train = pd.concat([df_train, tfidf_pca_emp_title_train], axis=1)\n            df_test = pd.concat([df_test, tfidf_pca_emp_title_test], axis=1)\n            update_cols(add_cols_quantitative=tfidf_pca_emp_title_train.columns.tolist())\n    \n    extraction_elapsed_time = time.time()-extraction_begin\n    extraction_elapsed_sec = int(extraction_elapsed_time%60)\n    extraction_elapsed_min = int((extraction_elapsed_time-extraction_elapsed_sec)\/60)\n    print(' > elapsed time: {}:{}'.format(extraction_elapsed_min, str(extraction_elapsed_sec).rjust(2, '0')))\n    \n    return ","a7938c7b":"def year2decade(convert_target_col, remove_original=True):\n    print('year2decade({}, remove_original={})'.format(convert_target_col, remove_original))\n    global df_train, df_test, cols_qualitative, cols_quantitative\n    \n    col_decade_year = '{}_decade_year'.format(convert_target_col)\n    df_train[col_decade_year] = df_train[convert_target_col].dt.year.astype(int)\n    df_train[col_decade_year] -= df_train[col_decade_year] % 10\n    df_test[col_decade_year] = df_test[convert_target_col].dt.year.astype(int)\n    df_test[col_decade_year] -= df_test[col_decade_year] % 10\n    \n    if remove_original:\n        update_cols(add_cols_quantitative=[col_decade_year], add_cols_unnecessary=[convert_target_col])\n    else:\n        update_cols(add_cols_quantitative=[col_decade_year])\n    \n    return ","61e861dd":"# \u3081\u3061\u3083\u304f\u3061\u3083\u6642\u9593\u304b\u304b\u308b\u306e\u3067\u3084\u3089\u306a\u3044\u3002\ndef record_clustering_pca_tsne_kmeans(n_dim_pca=10, n_dim_tsne=2, n_cluster=100):\n    print('record_clustering_pca_tsne_kmeans(n_dim_pca={}, n_dim_tsne={}, n_cluster={})'.format(n_dim_pca, n_dim_tsne, n_cluster))\n    global df_train, df_test\n    global records_pca_tsne_kmeans_cluster_train, records_pca_tsne_kmeans_cluster_test\n    \n    extraction_begin = time.time()\n    \n    if records_pca_tsne_kmeans_cluster_train is not None: # 2\u56de\u76ee\u4ee5\u964d\u306f\u30ad\u30e3\u30c3\u30b7\u30e5\u306e\u30b3\u30d4\u30fc\u3092\u8fd4\u3059\n        print(' - already done, loading cache.')\n        col_record_cluster = 'CLSTR:pca_tsne_kmeans_cluster'\n        df_train[col_record_cluster] = records_pca_tsne_kmeans_cluster_train.copy()\n        df_test[col_record_cluster] = records_pca_tsne_kmeans_cluster_test.copy()\n        df_train[col_record_cluster] = df_train[col_record_cluster].astype(object)\n        df_test[col_record_cluster] = df_test[col_record_cluster].astype(object)\n        update_cols(add_cols_qualitative=[col_record_cluster])\n        \n    else:\n        print(' - run PCA')\n        pca = PCA(n_components=n_dim_pca)\n        pca.fit(df_train_linear)\n        f_pca_train = pca.transform(df_train_linear)\n        f_pca_test = pca.transform(df_test_linear)\n\n        print(' - run t-SNE')\n        tsne = TSNE(n_components=n_dim_tsne, init='random', random_state=0, perplexity=30, n_iter=250)\n        f_tsne_train = tsne.fit_transform(f_pca_train)\n        f_tsne_test = tsne.fit_transform(f_pca_test)\n        del f_pca_train, f_pca_test\n        \n        print(' - run MiniBatchKMeans')\n        kmeans = MiniBatchKMeans(n_clusters=n_cluster, batch_size=100)\n        clusters_train = kmeans.fit(f_tsne_train).labels_\n        del f_tsne_train\n        clusters_test = kmeans.fit(f_tsne_test).labels_\n        del f_tsne_test\n        del kmeans\n        \n        col_record_cluster = 'CLSTR:pca_tsne_kmeans_cluster'\n        df_train[col_record_cluster] = clusters_train\n        df_test[col_record_cluster] = clusters_test\n        df_train[col_record_cluster] = df_train[col_record_cluster].astype(object)\n        df_test[col_record_cluster] = df_test[col_record_cluster].astype(object)\n        update_cols(add_cols_qualitative=[col_record_cluster])\n        \n        records_pca_tsne_kmeans_cluster_train = clusters_train\n        records_pca_tsne_kmeans_cluster_test = clusters_test\n    \n    extraction_elapsed_time = time.time()-extraction_begin\n    extraction_elapsed_sec = int(extraction_elapsed_time%60)\n    extraction_elapsed_min = int((extraction_elapsed_time-extraction_elapsed_sec)\/60)\n    print(' > elapsed time: {}:{}'.format(extraction_elapsed_min, str(extraction_elapsed_sec).rjust(2, '0')))","5e78f7dc":"def record_clustering_kmeans(n_cluster=100):\n    print('record_clustering_kmeans(n_cluster={})'.format(n_cluster))\n    global df_train, df_test\n    global recoeds_kmeans_cluster_train, recoeds_kmeans_cluster_test\n    \n    extraction_begin = time.time()\n    \n    if recoeds_kmeans_cluster_train is not None: # 2\u56de\u76ee\u4ee5\u964d\u306f\u30ad\u30e3\u30c3\u30b7\u30e5\u306e\u30b3\u30d4\u30fc\u3092\u8fd4\u3059\n        print(' - already done, loading cache.')\n        col_record_cluster = 'CLSTR:kmeans_cluster'\n        df_train[col_record_cluster] = recoeds_kmeans_cluster_train.copy()\n        df_test[col_record_cluster] = recoeds_kmeans_cluster_test.copy()\n        df_train[col_record_cluster] = df_train[col_record_cluster].astype(object)\n        df_test[col_record_cluster] = df_test[col_record_cluster].astype(object)\n        update_cols(add_cols_qualitative=[col_record_cluster])\n        \n    else:\n        print(' - run MiniBatchKMeans')\n        kmeans = MiniBatchKMeans(n_clusters=n_cluster, batch_size=100)\n        clusters_train = kmeans.fit(df_train_linear).labels_\n        clusters_test = kmeans.fit(df_test_linear).labels_\n        del kmeans\n        \n        col_record_cluster = 'CLSTR:kmeans_cluster'\n        df_train[col_record_cluster] = clusters_train\n        df_test[col_record_cluster] = clusters_test\n        df_train[col_record_cluster] = df_train[col_record_cluster].astype(object)\n        df_test[col_record_cluster] = df_test[col_record_cluster].astype(object)\n        update_cols(add_cols_qualitative=[col_record_cluster])\n        \n        recoeds_kmeans_cluster_train = clusters_train\n        recoeds_kmeans_cluster_test = clusters_test\n    \n    extraction_elapsed_time = time.time()-extraction_begin\n    extraction_elapsed_sec = int(extraction_elapsed_time%60)\n    extraction_elapsed_min = int((extraction_elapsed_time-extraction_elapsed_sec)\/60)\n    print(' > elapsed time: {}:{}'.format(extraction_elapsed_min, str(extraction_elapsed_sec).rjust(2, '0')))","ffb2cb0b":"# \u4ea4\u4e92\u4f5c\u7528\u9805\ndef generate_interaction_terms(cols, n_conbinations=2):\n    print('generate_interaction_terms(n_conbinations={}, cols={})'.format(n_conbinations, cols))\n    global df_train, df_test\n    \n    for cols_conb in itertools.combinations(cols, n_conbinations):\n        cols_conb_sorted = sorted(cols_conb) # \u3053\u308c\u3092\u3057\u3066\u304a\u304b\u306a\u3044\u3068\u65b0\u3057\u304f\u4f5c\u3063\u305f\u7279\u5fb4\u91cf\u306e\u540d\u524d\u304c\u51e6\u7406\u3054\u3068\u306b\u5909\u308f\u3063\u305f\u308a\u3057\u3066\u56f0\u308b\n        sr_interaction_train = 1\n        sr_interaction_test = 1\n        for col in cols_conb_sorted:\n            # \u6700\u5c0f\u50240\u4ee5\u4e0b\u306a\u30891\u4ee5\u4e0a\u306b\u8abf\u6574\n            sr_tmp_train = df_train[col]\n            if sr_tmp_train.min() <= 0:\n                sr_tmp_train = sr_tmp_train-sr_tmp_train.min()+1\n            sr_tmp_test = df_test[col]\n            if sr_tmp_test.min() <= 0:\n                sr_tmp_test = sr_tmp_test-sr_tmp_test.min()+1\n            # \u639b\u3051\u7b97\u5b9f\u884c\n            sr_interaction_train *= sr_tmp_train\n            sr_interaction_test *= sr_tmp_test\n        col_interaction = 'INTRCTN:{}'.format(' (times) '.join(cols_conb_sorted))\n        df_train[col_interaction] = sr_interaction_train\n        df_test[col_interaction] = sr_interaction_test\n        update_cols(add_cols_quantitative=[col_interaction])\n        \n    return \n\n# \u5dee\u5206\u9805\ndef generate_difference_terms(cols):\n    print('generate_difference_terms(cols={})'.format(cols))\n    global df_train, df_test\n    \n    for cols_conb in itertools.combinations(cols, 2):\n        colA, colB = sorted(cols_conb) # \u3053\u308c\u3092\u3057\u3066\u304a\u304b\u306a\u3044\u3068\u65b0\u3057\u304f\u4f5c\u3063\u305f\u7279\u5fb4\u91cf\u306e\u540d\u524d\u304c\u51e6\u7406\u3054\u3068\u306b\u5909\u308f\u3063\u305f\u308a\u3057\u3066\u56f0\u308b\n        col_difference = 'DIFF:{} (minus) {}'.format(colA, colB)\n        df_train[col_difference] = df_train[colA] - df_train[colB]\n        df_test[col_difference] = df_test[colA] - df_test[colB]\n        update_cols(add_cols_quantitative=[col_difference])\n        \n    return \n\n# \u5272\u5408\u9805\ndef generate_division_terms(col_pairs): # \u4e8c\u6b21\u5143\u914d\u5217\u3067\u6e21\u3059\n    print('generate_division_terms(col_pairs={})'.format(col_pairs))\n    global df_train, df_test\n    \n    for dividend, divisor in col_pairs:\n        # \u6700\u5c0f\u50240\u4ee5\u4e0b\u306a\u30891\u4ee5\u4e0a\u306b\u8abf\u6574: train\n        sr_tmp_train_dividend = df_train[dividend]\n        sr_tmp_train_divisor = df_train[divisor]\n        if sr_tmp_train_dividend.min() <= 0:\n            sr_tmp_train_dividend = sr_tmp_train_dividend-sr_tmp_train_dividend.min()+1\n        if sr_tmp_train_divisor.min() <= 0:\n            sr_tmp_train_divisor = sr_tmp_train_divisor-sr_tmp_train_divisor.min()+1\n        # \u6700\u5c0f\u50240\u4ee5\u4e0b\u306a\u30891\u4ee5\u4e0a\u306b\u8abf\u6574: test\n        sr_tmp_test_dividend = df_test[dividend]\n        sr_tmp_test_divisor = df_test[divisor]\n        if sr_tmp_test_dividend.min() <= 0:\n            sr_tmp_test_dividend = sr_tmp_test_dividend-sr_tmp_test_dividend.min()+1\n        if sr_tmp_test_divisor.min() <= 0:\n            sr_tmp_test_divisor = sr_tmp_test_divisor-sr_tmp_test_divisor.min()+1\n        # \u5272\u308a\u7b97\u5b9f\u884c\n        col_division = 'DIV:{} (divby) {}'.format(dividend, divisor)\n        df_train[col_division] = sr_tmp_train_dividend \/ sr_tmp_train_divisor\n        df_test[col_division] = sr_tmp_test_dividend \/ sr_tmp_test_divisor\n        update_cols(add_cols_quantitative=[col_division])\n    \n    return ","60ae0cb6":"def adversarial_validation_lgb(rate_sampling=1.0, n_folds=3, plot_feature_importance=True, ignore_target_enc=True, ignore_txt=True):\n    print('adversarial_validation_lgb(rate_sampling={})'.format(rate_sampling))\n    global df_train, df_test\n    \n    cols_to_use = get_cols_to_use()\n    if ignore_target_enc:\n        cols_to_use = [col for col in cols_to_use if 'TARGET' not in col]\n    if ignore_txt:\n        cols_to_use = [col for col in cols_to_use if 'TXT' not in col]\n        \n    df_all = pd.concat([df_train[cols_to_use], df_test[cols_to_use]])\n    df_all['test'] = np.concatenate([np.zeros(len(df_train)), np.ones(len(df_test))])\n    X_all = df_all.drop(columns=['test']).values\n    y_all = df_all['test'].values\n    \n    df_subset = df_all.sample(n=int(len(df_all)*rate_sampling))\n    X_subset = df_subset.drop(columns=['test']).values\n    y_subset = df_subset['test'].values\n\n    # \u4ea4\u5dee\u691c\u5b9a\u3067\u7cbe\u5ea6\u3092\u628a\u63e1\u3059\u308b\n    scores = []\n    f_importances = []\n    skf = StratifiedKFold(n_splits=n_folds, random_state=81, shuffle=True) \n    for i, (train_ix, val_ix) in enumerate(skf.split(X_subset, y_subset)):\n        fold_begin = time.time()\n        _X_train, _y_train = X_subset[train_ix], y_subset[train_ix]\n        _X_val, _y_val = X_subset[val_ix], y_subset[val_ix]\n        \n        lgb_train = lgb.Dataset(_X_train, _y_train)\n        lgb_eval = lgb.Dataset(_X_val, _y_val, reference=lgb_train)\n\n        model = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval, verbose_eval=False)\n        _y_pred = model.predict(_X_val, num_iteration=model.best_iteration)\n        _score = roc_auc_score(_y_val, _y_pred)\n        scores.append(_score)\n        \n        elapsed_time = time.time()-fold_begin\n        elapsed_sec = int(elapsed_time%60)\n        elapsed_min = int((elapsed_time-elapsed_sec)\/60)\n        print(' - CV Score of Fold {} is {:.5f} ({}:{})'.format(i, _score, elapsed_min, str(elapsed_sec).rjust(2, '0')))\n    \n    print(' > Average CV Score is {:.5f}'.format(np.mean(scores)))\n    \n    # \u30c6\u30b9\u30c8\u3063\u307d\u3055\u306e\u5217\u3092\u8ffd\u52a0\n    lgb_train = lgb.Dataset(X_all, y_all)\n    model = lgb.train(lgbm_params, lgb_train)\n    df_all['test-like'] = model.predict(X_all, num_iteration=model.best_iteration)\n    df_train['test-like'] = df_all.loc[df_all['test'] == 0, 'test-like']\n    update_cols(add_cols_unnecessary=['test-like'])\n    del df_all\n    \n    # \u7279\u5fb4\u91cf\u306e\u91cd\u8981\u5ea6\u3092\u30d7\u30ed\u30c3\u30c8\n    if plot_feature_importance:\n        f_importance = model.feature_importance(importance_type='gain')\n        df_importance = pd.DataFrame(f_importance, index=cols_to_use, columns=['importance'])\n        df_importance = df_importance.sort_values('importance')\n        plt.figure()\n        df_importance.plot.barh(figsize=(8, 20), title='Feature Importances of Adversarial Validation')\n    \n    return ","e6ccb276":"def plot_similar_features(sh=0.5, df_target='df_train'):\n    print('plot_similar_features(sh={}, df_target=\\'{}\\')'.format(sh, df_target))\n    global df_train, df_test, df_holdout\n    \n    cols_to_use = get_cols_to_use()\n    df_corr = None\n    if df_target == 'df_train':\n        df_corr = df_train[cols_to_use].corr()\n    elif df_target == 'df_linear':\n        df_corr = df_train_linear[cols_to_use].corr()\n    else:\n        return \n    \n    corrs = []\n    for i, colA in enumerate(df_corr.columns):\n        for j, colB in enumerate(df_corr.index):\n            if j > i:\n                corr_tmp = df_corr.iloc[i, j]\n                corrs.append([corr_tmp, colA, colB])\n                \n    df_corr_pairs = pd.DataFrame(columns=['corr', 'colA', 'colB'], data=corrs)\n    df_corr_pairs['corr_abs'] = df_corr_pairs['corr'].abs()\n    df_corr_pairs = df_corr_pairs[df_corr_pairs['corr_abs'] >= sh]\n    \n    if len(df_corr_pairs) > 0:\n        df_corr_pairs = df_corr_pairs.sort_values('corr_abs', ascending=True)\n        sr_corr_pairs = pd.Series(df_corr_pairs['corr'].values, index=df_corr_pairs['colA']+' | '+df_corr_pairs['colB'])\n        del df_corr_pairs\n\n        plt.figure()\n        sr_corr_pairs.plot.barh(figsize=(8, 12), xlim=(-1.0, 1.0), title='Pairs of Highly Correlated Features')\n\n        print(' - correlation ranking')\n        print(' > {}'.format(sr_corr_pairs.index.tolist()))\n        \n    else:\n        print(' - correlation ranking')\n        print(' > {}'.format([]))\n    \n    return","09d2b194":"def count_encoding(encoding_target_cols, remove_original=True):\n    print('count_encoding({}, remove_original={})'.format(encoding_target_cols, remove_original))\n    global df_train, df_test\n\n    for col in encoding_target_cols:\n        col_encoded = 'COUNT:{}'.format(col)\n        freq = df_train[col].value_counts()\n        df_train[col_encoded] = df_train[col].map(freq)\n        df_train.loc[df_train[col].isnull(), col_encoded] = len(df_train[col].isnull())\n        df_test[col_encoded] = df_test[col].map(freq)\n        df_test.loc[df_test[col].isnull(), col_encoded] = len(df_test[col].isnull())\n        if remove_original:\n            update_cols(add_cols_quantitative=[col_encoded], add_cols_unnecessary=[col])\n        else:\n            update_cols(add_cols_quantitative=[col_encoded])\n\n    return ","0e4a619a":"def target_encoding(encoding_target_cols, remove_original=True):\n    print('target_encoding({}, remove_original={})'.format(encoding_target_cols, remove_original))\n    global df_train, df_test\n    \n    X_temp = df_train[encoding_target_cols+[COL_TARGET]]\n    y_train = df_train[COL_TARGET]\n    \n    for col in encoding_target_cols:\n        col_encoded = 'TARGET:{}'.format(col)\n        \n        # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306f\u8a13\u7df4\u30c7\u30fc\u30bf\u5168\u4f53\u306e\u5e73\u5747\u3067\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\n        target_mean_all = X_temp.groupby(col)[COL_TARGET].mean()\n        df_test[col_encoded] = df_test[col].map(target_mean_all)\n\n        # \u5b66\u7fd2\u30c7\u30fc\u30bf\u306foof\u306e\u5e73\u5747\u3067\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\n        skf = StratifiedKFold(n_splits=5, random_state=81, shuffle=True)\n        encoded_train = pd.Series(np.zeros(len(df_train)), index=df_train.index)\n        for train_ix, val_ix in skf.split(X_temp, y_train):\n            X_train_, _ = X_temp.iloc[train_ix], y_train.iloc[train_ix]\n            X_val, _ = X_temp.iloc[val_ix], y_train.iloc[val_ix]\n\n            target_mean = X_train_.groupby([col])[COL_TARGET].mean()\n            encoded_train.iloc[val_ix] = X_val[col].map(target_mean)\n\n        # \u5909\u63db\u5f8c\u306e\u5024\u3092\u4ee3\u5165\n        df_train[col_encoded] = encoded_train\n        \n        if remove_original:\n            update_cols(add_cols_quantitative=[col_encoded], add_cols_unnecessary=[col])\n        else:\n            update_cols(add_cols_quantitative=[col_encoded])\n        \n    return ","0268f131":"def onehot_encoding(encoding_target_cols):\n    print('onehot_encoding({})'.format(encoding_target_cols))\n    global df_train, df_test\n    \n    for col in encoding_target_cols:\n        ohe = ce.OneHotEncoder(cols=[col], handle_unknown='ignore', use_cat_names=True)\n        ohe.fit(df_train[col])\n        \n        df_ohe_train = ohe.transform(df_train[col])\n        df_ohe_test = ohe.transform(df_test[col])\n        \n        df_train = pd.concat([df_train, df_ohe_train], axis=1)\n        df_test = pd.concat([df_test, df_ohe_test], axis=1)\n        update_cols(add_cols_quantitative=df_ohe_train.columns.tolist(), add_cols_unnecessary=[col])\n\n    return","97c89dba":"# use_test_like_records=True\u306b\u3059\u308b\u3068\u3001Adversarial Validation\u306e\u4e88\u6e2c\u7d50\u679c(=\u30c6\u30b9\u30c8\u3063\u307d\u3055)\u306e\u4e0a\u4f4d\u304b\u3089\u62bd\u51fa\ndef pertition_holdout(rate_holdout=0.2, use_test_like_records=True, use_recent_records=False):\n    print('pertition_holdout(rate_holdout={}, use_test_like_records={}, use_recent_records={})'.format(rate_holdout, use_test_like_records, use_recent_records))\n    global df_train, df_test, df_holdout, cols_qualitative, cols_quantitative\n    \n    df_train = df_train.reset_index(drop=True)\n    n_holdout = len(df_train)*rate_holdout\n    true_rate = len(df_train[df_train[COL_TARGET] == 1])\/len(df_train)\n    \n    # holdout_ix\u306e\u6c7a\u5b9a\n    holdout_ix = None\n    if use_test_like_records: # Adversarial Validation\u5b9f\u65bd\u6e08\u307f\u306e\u5834\u5408\n        df_tmp = df_train[['test-like', COL_TARGET]].copy()\n        df_tmp = df_tmp.sort_values('test-like', ascending=False)\n        ix_true = df_tmp[df_tmp[COL_TARGET] == 1][:int(n_holdout*true_rate)].index.tolist()\n        ix_false = df_tmp[df_tmp[COL_TARGET] == 0][:int(n_holdout*(1-true_rate))].index.tolist()\n        holdout_ix = ix_true + ix_false\n        del df_tmp\n        \n    elif use_recent_records:\n        holdout_ix = df_train[df_train['issue_d'] >= dt.datetime(2015, 1, 1)].index.tolist()\n        \n    else:\n        ix_true = df_train[df_train[COL_TARGET] == 1].sample(n=int(n_holdout*true_rate)).index.tolist()\n        ix_false = df_train[df_train[COL_TARGET] == 0].sample(n=int(n_holdout*(1-true_rate))).index.tolist()\n        holdout_ix = ix_true + ix_false\n        \n    # df\u306e\u66f4\u65b0\n    df_holdout = df_train.iloc[holdout_ix].copy()\n    df_holdout = df_holdout.reset_index(drop=True)\n    df_train = df_train.drop(index=holdout_ix)\n    df_train = df_train.reset_index(drop=True)\n        \n    return ","672a1bd7":"# Cross Validation: LightGBM\u7528\ndef cross_validation_lgb(rate_sampling=1.0, n_folds=5, plot_feature_importance=True, print_topworst_features=True, n_top=10):\n    print('\\ncross_validation_lgb(rate_sampling={})'.format(rate_sampling))\n    global df_train, df_test\n    global score_cv_avr\n    \n    # \u30c7\u30fc\u30bf\u306e\u62bd\u51fa\n    cols_to_use = get_cols_to_use()\n    df_train_subset = df_train.sample(n=int(len(df_train)*rate_sampling))\n    X_train = df_train_subset[cols_to_use].values\n    y_train = df_train_subset[COL_TARGET].values\n    X_test = df_test[cols_to_use].values\n    del df_train_subset\n    \n    # \u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\n    scores = []\n    f_importances = []\n    skf = StratifiedKFold(n_splits=n_folds, random_state=81, shuffle=True)\n    for i, (train_ix, val_ix) in enumerate(skf.split(X_train, y_train)):\n        fold_begin = time.time()\n        _X_train, _y_train = X_train[train_ix], y_train[train_ix]\n        _X_val, _y_val = X_train[val_ix], y_train[val_ix]\n        \n        lgb_train = lgb.Dataset(_X_train, _y_train)\n        lgb_eval = lgb.Dataset(_X_val, _y_val, reference=lgb_train)\n        \n        model = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval, verbose_eval=False)\n        _y_pred = model.predict(_X_val, num_iteration=model.best_iteration)\n        _score = roc_auc_score(_y_val, _y_pred)\n        scores.append(_score)\n        \n        if plot_feature_importance or print_topworst_features:\n            f_importances.append(model.feature_importance(importance_type='gain'))\n            \n        elapsed_time = time.time()-fold_begin\n        elapsed_sec = int(elapsed_time%60)\n        elapsed_min = int((elapsed_time-elapsed_sec)\/60)\n        print(' - CV Score of Fold {} is {:.5f} ({}:{})'.format(i, _score, elapsed_min, str(elapsed_sec).rjust(2, '0')))\n\n    score_cv_avr = np.mean(scores)\n    print(' > Average CV Score is {:.5f}'.format(score_cv_avr))\n    \n    # \u7279\u5fb4\u91cf\u91cd\u8981\u5ea6\u306e\u8868\u793a\n    if plot_feature_importance:\n        mean_of_feature_importances = np.array(f_importances).mean(axis=0)\n        df_mean_importance_of_cv = pd.DataFrame(mean_of_feature_importances, index=cols_to_use, columns=['importance'])\n        df_mean_importance_of_cv = df_mean_importance_of_cv.sort_values('importance')\n        plt.figure()\n        df_mean_importance_of_cv.plot.barh(figsize=(8, 20), title='Feature Importances of Cross Validation')\n        del df_mean_importance_of_cv\n        \n    # \u5168\u30d5\u30a9\u30fc\u30eb\u30c9\u3067topN,worstN\u306b\u5171\u901a\u3057\u3066\u5165\u3063\u3066\u3044\u308b\u7279\u5fb4\u91cf\u3092\u8868\u793a\n    if print_topworst_features:\n\n        # \u5168\u7279\u5fb4\u91cf\u306e\u30e9\u30f3\u30ad\u30f3\u30b0\n        common_features_top_n = None\n        common_features_worst_n = None\n        for i in range(n_folds):\n            sr_importance_tmp = pd.Series(f_importances[i], index=cols_to_use)\n            sr_importance_tmp = sr_importance_tmp.sort_values(ascending=False)\n            features_top_n = sr_importance_tmp[:n_top].index\n            features_worst_n = sr_importance_tmp[::-1][:n_top].index\n            if common_features_top_n is None:\n                common_features_top_n = features_top_n\n                common_features_worst_n = features_worst_n\n            else:\n                common_features_top_n = list(set(common_features_top_n) & set(features_top_n))\n                common_features_worst_n = list(set(common_features_worst_n) & set(features_worst_n))\n\n        print('\\nPrint top and worst features (all)')\n        print(' - top {} features commonly in each fold:'.format(n_top))\n        print(' > {}'.format(common_features_top_n))\n        print(' - worst {} features commonly in each fold:'.format(n_top))\n        print(' > {}'.format(common_features_worst_n))\n        \n        # \u30bf\u30fc\u30b2\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3057\u305f\u5217\u3092\u306e\u305e\u3044\u305f\u7279\u5fb4\u91cf\u306e\u30e9\u30f3\u30ad\u30f3\u30b0\n        common_features_top_n = None\n        common_features_worst_n = None\n        for i in range(n_folds):\n            sr_importance_tmp = pd.Series(f_importances[i], index=cols_to_use)\n            cols_except_target_enc = [col for col in cols_to_use if 'TARGET' not in col]\n            sr_importance_tmp = sr_importance_tmp[cols_except_target_enc]\n            sr_importance_tmp = sr_importance_tmp.sort_values(ascending=False)\n            features_top_n = sr_importance_tmp[:n_top].index\n            features_worst_n = sr_importance_tmp[::-1][:n_top].index\n            if common_features_top_n is None:\n                common_features_top_n = features_top_n\n                common_features_worst_n = features_worst_n\n            else:\n                common_features_top_n = list(set(common_features_top_n) & set(features_top_n))\n                common_features_worst_n = list(set(common_features_worst_n) & set(features_worst_n))\n\n        print('\\nPrint top and worst features (ignore target enc)')\n        print(' - top {} features commonly in each fold (ignore target enc):'.format(n_top))\n        print(' > {}'.format(common_features_top_n))\n        print(' - worst {} features commonly in each fold (ignore target enc):'.format(n_top))\n        print(' > {}'.format(common_features_worst_n))\n    \n    return  ","2f01ee51":"# Cross Validation: XGBoost\u7528\ndef cross_validation_xgb(rate_sampling=1.0, n_folds=5):\n    print('\\ncross_validation_xgb(rate_sampling={})'.format(rate_sampling))\n    global df_train, df_test\n    global score_cv_avr\n    \n    # \u30c7\u30fc\u30bf\u306e\u62bd\u51fa\n    cols_to_use = get_cols_to_use()\n    df_train_subset = df_train.sample(n=int(len(df_train)*rate_sampling))\n    X_train = df_train_subset[cols_to_use].values\n    y_train = df_train_subset[COL_TARGET].values\n    X_test = df_test[cols_to_use].values\n    del df_train_subset\n    \n    # \u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\n    scores = []\n    skf = StratifiedKFold(n_splits=n_folds, random_state=81, shuffle=True)\n    for i, (train_ix, val_ix) in enumerate(skf.split(X_train, y_train)):\n        fold_begin = time.time()\n        _X_train, _y_train = X_train[train_ix], y_train[train_ix]\n        _X_val, _y_val = X_train[val_ix], y_train[val_ix]\n        \n        D_train = xgb.DMatrix(_X_train, label=_y_train)\n        D_val = xgb.DMatrix(_X_val, label=_y_val)\n        xgb_param = {'objective': 'binary:logistic', 'eval_metric': 'auc'}\n        model = xgb.train(xgb_param, D_train, verbose_eval=False)\n        _y_pred = model.predict(D_val, ntree_limit=model.best_ntree_limit)\n        _score = roc_auc_score(_y_val, _y_pred)\n        scores.append(_score)\n        \n        elapsed_time = time.time()-fold_begin\n        elapsed_sec = int(elapsed_time%60)\n        elapsed_min = int((elapsed_time-elapsed_sec)\/60)\n        print(' - CV Score of Fold {} is {:.5f} ({}:{})'.format(i, _score, elapsed_min, str(elapsed_sec).rjust(2, '0')))\n\n    score_cv_avr = np.mean(scores)\n    print(' > Average CV Score is {:.5f}'.format(score_cv_avr))\n    \n    return  ","2b4a5166":"# Cross Validation: sklearn\u306e\u30e2\u30c7\u30eb\u7528\ndef cross_validation(clf_name, rate_sampling=1.0, n_folds=5):\n    print('\\ncross_validation(clf_name=\\'{}\\', rate_sampling={})'.format(clf_name, rate_sampling))\n    global df_train, df_test\n    global score_cv_avr\n    \n    # \u30c7\u30fc\u30bf\u306e\u62bd\u51fa\n    cols_to_use = get_cols_to_use()\n    df_train_subset = df_train.sample(n=int(len(df_train)*rate_sampling))\n    X_train = df_train_subset[cols_to_use].values\n    y_train = df_train_subset[COL_TARGET].values\n    X_test = df_test[cols_to_use].values\n    del df_train_subset\n    \n    # \u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\n    scores = []\n    skf = StratifiedKFold(n_splits=n_folds, random_state=81, shuffle=True)\n    for i, (train_ix, val_ix) in enumerate(skf.split(X_train, y_train)):\n        fold_begin = time.time()\n        _X_train, _y_train = X_train[train_ix], y_train[train_ix]\n        _X_val, _y_val = X_train[val_ix], y_train[val_ix]\n                \n        # \u30e2\u30c7\u30eb\u751f\u6210\n        clf = None\n        if clf_name == 'LogisticRegression':\n            clf = LogisticRegression(C=0.01)\n        if clf_name == 'RandomForest':\n            clf = RandomForestClassifier()\n        if clf_name == 'HistGradientBoosting':\n            clf = HistGradientBoostingClassifier()\n        if clf_name == 'NeuralNetwork':\n            clf = Sequential()\n            clf.add(Dense(32, activation='relu'))\n            clf.add(Dense(16, activation='relu'))\n            clf.add(Dense(1, activation='sigmoid'))\n            clf.compile(loss='binary_crossentropy', optimizer='adam', metrics=[AUC()])\n            \n        # \u5b66\u7fd2\u3068\u4e88\u6e2c\n        _y_pred = None\n        if clf_name == 'NeuralNetwork':\n            clf.fit(_X_train, _y_train, epochs=5)\n            _y_pred = clf.predict(_X_val)[:, 0]\n        else:\n            clf.fit(_X_train, _y_train)\n            _y_pred = clf.predict_proba(_X_val)[:,1]\n            \n        # \u30b9\u30b3\u30a2\u3092\u8a18\u9332\n        _score = roc_auc_score(_y_val, _y_pred)\n        scores.append(_score)\n        \n        elapsed_time = time.time()-fold_begin\n        elapsed_sec = int(elapsed_time%60)\n        elapsed_min = int((elapsed_time-elapsed_sec)\/60)\n        print(' - CV Score of Fold {} is {:.5f} ({}:{})'.format(i, _score, elapsed_min, str(elapsed_sec).rjust(2, '0')))\n\n    score_cv_avr = np.mean(scores)\n    print(' > Average CV Score is {:.5f}'.format(score_cv_avr))\n    \n    return ","0647a11a":"# Holdout Validation: LightGBM\u7528\ndef holdout_validation_lgb(plot_feature_importance=True, print_topworst_features=True, print_topworst_features_by_conditions=True):\n    print('\\nholdout_validation_lgb()')\n    global df_train, df_test, df_holdout\n    global score_holdout\n    \n    validation_begin = time.time()\n    cols_to_use = get_cols_to_use()\n    \n    # \u30c7\u30fc\u30bf\u306e\u62bd\u51fa\n    X_train = df_train[cols_to_use].values\n    y_train = df_train[COL_TARGET].values\n    X_val = df_holdout[cols_to_use].values\n    y_val = df_holdout[COL_TARGET].values\n    \n    # Holdout\u3067\u691c\u5b9a\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)\n    model = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval, verbose_eval=False)\n    y_pred = model.predict(X_val, num_iteration=model.best_iteration)\n    score_holdout = roc_auc_score(y_val, y_pred)\n    f_importance = model.feature_importance(importance_type='gain')\n        \n    # \u30b9\u30b3\u30a2\u3068\u7d4c\u904e\u6642\u9593\u8868\u793a\n    elapsed_time = time.time()-validation_begin\n    elapsed_sec = int(elapsed_time%60)\n    elapsed_min = int((elapsed_time-elapsed_sec)\/60)\n    print(' - Score of Holdout is {:.5f} ({}:{})'.format(score_holdout, elapsed_min, str(elapsed_sec).rjust(2, '0')))\n    \n    # \u7279\u5fb4\u91cf\u91cd\u8981\u5ea6\u306e\u8868\u793a\n    if plot_feature_importance:\n        df_mean_importance_of_cv = pd.DataFrame(f_importance, index=cols_to_use, columns=['importance'])\n        df_mean_importance_of_cv = df_mean_importance_of_cv.sort_values('importance')\n        plt.figure()\n        df_mean_importance_of_cv.plot.barh(figsize=(8, 20), title='Feature Importances of Holdout Validation')\n        del df_mean_importance_of_cv\n        \n    # \u5168\u7279\u5fb4\u91cf\u306e\u91cd\u8981\u5ea6\u306e\u30e9\u30f3\u30ad\u30f3\u30b0\u3092\u8868\u793a\n    if print_topworst_features:\n        sr_importance_tmp = pd.Series(f_importance, index=cols_to_use)\n        sr_importance_tmp = sr_importance_tmp.sort_values(ascending=False)\n        print('\\nFeatures sorted by importance (all): {}'.format(sr_importance_tmp.index.tolist()))\n        \n    # \u5404\u6761\u4ef6\u3067\u7d5e\u3063\u3066\u306e\u30e9\u30f3\u30ad\u30f3\u30b0\n    if print_topworst_features_by_conditions:\n        conds = ['INTRCTN', 'DIFF', 'DIV']\n        for cond in conds:\n            sr_importance_tmp = pd.Series(f_importance, index=cols_to_use)\n            sr_importance_tmp = sr_importance_tmp[sr_importance_tmp.index.str.startswith(cond)]\n            sr_importance_tmp = sr_importance_tmp.sort_values(ascending=False)\n            print('\\nFeatures sorted by importance (including {}): {}'.format(cond, sr_importance_tmp.index.tolist()))\n    \n    return ","8818e2b2":"# Holdout Validation: XGBoost\u7528\ndef holdout_validation_xgb():\n    print('\\nholdout_validation_xgb()')\n    global df_train, df_test, df_holdout\n    global score_holdout\n    \n    validation_begin = time.time()\n    cols_to_use = get_cols_to_use()\n    \n    # \u30c7\u30fc\u30bf\u306e\u62bd\u51fa\n    X_train = df_train[cols_to_use].values\n    y_train = df_train[COL_TARGET].values\n    X_val = df_holdout[cols_to_use].values\n    y_val = df_holdout[COL_TARGET].values\n    \n    # Holdout\u3067\u691c\u5b9a\n    D_train = xgb.DMatrix(X_train, label=y_train)\n    D_val = xgb.DMatrix(X_val, label=y_val)\n    xgb_param = {'objective': 'binary:logistic', 'eval_metric': 'auc'}\n    model = xgb.train(xgb_param, D_train, verbose_eval=False)\n    y_pred = model.predict(D_val, ntree_limit=model.best_ntree_limit)\n    score_holdout = roc_auc_score(y_val, y_pred)\n        \n    # \u30b9\u30b3\u30a2\u3068\u7d4c\u904e\u6642\u9593\u8868\u793a\n    elapsed_time = time.time()-validation_begin\n    elapsed_sec = int(elapsed_time%60)\n    elapsed_min = int((elapsed_time-elapsed_sec)\/60)\n    print(' - Score of Holdout is {:.5f} ({}:{})'.format(score_holdout, elapsed_min, str(elapsed_sec).rjust(2, '0')))\n    \n    return ","4435b29a":"# Holdout Validation: sklearn\u306e\u30e2\u30c7\u30eb\u7528\ndef holdout_validation(clf_name, plot_feature_importance=False):\n    print('\\nholdout_validation(clf_name=\\'{}\\')'.format(clf_name))\n    global df_train, df_test, df_holdout\n    global score_holdout\n    \n    validation_begin = time.time()\n    cols_to_use = get_cols_to_use()\n    \n    # \u30c7\u30fc\u30bf\u306e\u62bd\u51fa\n    X_train = df_train[cols_to_use].values\n    y_train = df_train[COL_TARGET].values\n    X_val = df_holdout[cols_to_use].values\n    y_val = df_holdout[COL_TARGET].values\n    \n    # \u30e2\u30c7\u30eb\u751f\u6210\n    clf = None\n    if clf_name == 'LogisticRegression':\n        clf = LogisticRegression(C=0.01)\n    if clf_name == 'RandomForest':\n        clf = RandomForestClassifier()\n    if clf_name == 'HistGradientBoosting':\n        clf = HistGradientBoostingClassifier()\n    if clf_name == 'NeuralNetwork':\n        clf = Sequential()\n        clf.add(Dense(32, activation='relu'))\n        clf.add(Dense(16, activation='relu'))\n        clf.add(Dense(1, activation='sigmoid'))\n        clf.compile(loss='binary_crossentropy', optimizer='adam', metrics=[AUC()])\n\n    # \u5b66\u7fd2\u3068\u4e88\u6e2c\n    y_pred = None\n    if clf_name == 'NeuralNetwork':\n        clf.fit(X_train, y_train, epochs=5)\n        y_pred = clf.predict(X_val)[:, 0]\n    else:\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict_proba(X_val)[:,1]\n\n    # \u30b9\u30b3\u30a2\u3092\u8a18\u9332\n    score_holdout = roc_auc_score(y_val, y_pred)\n    \n    # Logistic Regression\u3067\u5909\u6570\u91cd\u8981\u5ea6\u3092\u898b\u305f\u3044\u5834\u5408\n    if plot_feature_importance and (clf_name == 'LogisticRegression'):\n        sr_importance = pd.Series(clf.coef_[0], index=cols_to_use)\n        sr_importance = sr_importance.abs()\n        sr_importance = sr_importance.sort_values()\n        plt.figure()\n        sr_importance.plot.barh(figsize=(8, 20), title='Importance of Logistic Regression Features')\n        print(' - ranking of important features: {}'.format(sr_importance.index.tolist()[::-1]))\n\n        \n    # \u30b9\u30b3\u30a2\u3068\u7d4c\u904e\u6642\u9593\u8868\u793a\n    elapsed_time = time.time()-validation_begin\n    elapsed_sec = int(elapsed_time%60)\n    elapsed_min = int((elapsed_time-elapsed_sec)\/60)\n    print(' - Score of Holdout is {:.5f} ({}:{})'.format(score_holdout, elapsed_min, str(elapsed_sec).rjust(2, '0')))\n    \n    return ","2dd85b52":"def modeling_and_prediction_lgb(cross_validation_ensemble=False, n_folds=5):\n    print('modeling_and_prediction_lgb(cross_validation_ensemble={})'.format(cross_validation_ensemble))\n    global df_train, df_test, df_holdout\n    global y_pred_final\n    \n    # \u30c7\u30fc\u30bf\u306e\u62bd\u51fa\n    cols_to_use = get_cols_to_use()\n    df_train = pd.concat([df_train, df_holdout]) # holdout\u3082\u5408\u308f\u305b\u308b\n    X_train = df_train[cols_to_use].values\n    y_train = df_train[COL_TARGET].values\n    X_test = df_test[cols_to_use].values\n    \n    # \u30e2\u30c7\u30ea\u30f3\u30b0\n    if cross_validation_ensemble:\n        y_preds_sum = [0 for i in range(len(X_test))]\n        skf = StratifiedKFold(n_splits=n_folds, random_state=81, shuffle=True)\n        for i, (train_ix, val_ix) in enumerate(skf.split(X_train, y_train)):\n            _X_train, _y_train = X_train[train_ix], y_train[train_ix]\n            _X_val, _y_val = X_train[val_ix], y_train[val_ix]\n            lgb_train = lgb.Dataset(_X_train, _y_train)\n            model = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_train, verbose_eval=False)\n            y_preds_sum = [pr1+pr2 for (pr1, pr2) in zip(y_preds_sum, \n                                                         model.predict(X_test, num_iteration=model.best_iteration))]\n        y_pred_final = [pr\/n_folds for pr in y_preds_sum]\n        \n    else:\n        lgb_train = lgb.Dataset(X_train, y_train)\n        model = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_train, verbose_eval=False)\n        y_pred_final = model.predict(X_test, num_iteration=model.best_iteration)\n    \n    return ","a74be9b1":"# XGBoost\u7528\ndef modeling_and_prediction_xgb(cross_validation_ensemble=False, n_folds=5):\n    print('modeling_and_prediction_xgb(cross_validation_ensemble={})'.format(cross_validation_ensemble))\n    global df_train, df_test, df_holdout\n    global y_pred_final\n    \n    # \u30c7\u30fc\u30bf\u306e\u62bd\u51fa\n    cols_to_use = get_cols_to_use()\n    df_train = pd.concat([df_train, df_holdout]) # holdout\u3082\u5408\u308f\u305b\u308b\n    X_train = df_train[cols_to_use].values\n    y_train = df_train[COL_TARGET].values\n    X_test = df_test[cols_to_use].values\n    \n    D_test = xgb.DMatrix(X_test)\n    \n    # \u30e2\u30c7\u30ea\u30f3\u30b0\n    if cross_validation_ensemble:\n        y_preds_sum = [0 for i in range(len(X_test))]\n        skf = StratifiedKFold(n_splits=n_folds, random_state=81, shuffle=True)\n        for i, (train_ix, val_ix) in enumerate(skf.split(X_train, y_train)):\n            _X_train, _y_train = X_train[train_ix], y_train[train_ix]\n            _X_val, _y_val = X_train[val_ix], y_train[val_ix]\n            \n            D_train = xgb.DMatrix(_X_train, label=_y_train)\n            xgb_param = {'objective': 'binary:logistic', 'eval_metric': 'auc'}\n            model = xgb.train(xgb_param, D_train, verbose_eval=False)\n            y_preds_sum = [pr1+pr2 for (pr1, pr2) in zip(y_preds_sum, \n                                                         model.predict(D_test, ntree_limit=model.best_ntree_limit))]\n        y_pred_final = y_preds_sum \/ n_folds\n        \n    else:\n        D_train = xgb.DMatrix(X_train, label=y_train)\n        xgb_param = {'objective': 'binary:logistic', 'eval_metric': 'auc'}\n        model = xgb.train(xgb_param, D_train, valid_sets=D_train, verbose_eval=False)\n        y_pred_final = model.predict(D_test, ntree_limit=model.best_ntree_limit)\n    \n    return ","df3dd58e":"# sklearn\u306e\u30e2\u30c7\u30eb\u7528\ndef modeling_and_prediction(clf_name, cross_validation_ensemble=False, n_folds=5):\n    print('modeling_and_prediction(clf_name=\\'{}\\', cross_validation_ensemble={})'.format(clf_name, cross_validation_ensemble))\n    global df_train, df_test, df_holdout\n    global y_pred_final\n    \n    # \u30c7\u30fc\u30bf\u306e\u62bd\u51fa\n    cols_to_use = get_cols_to_use()\n    df_train = pd.concat([df_train, df_holdout]) # holdout\u3082\u5408\u308f\u305b\u308b\n    X_train = df_train[cols_to_use].values\n    y_train = df_train[COL_TARGET].values\n    X_test = df_test[cols_to_use].values\n    \n    # \u30e2\u30c7\u30ea\u30f3\u30b0\n    if cross_validation_ensemble:\n        y_preds_sum = [0 for i in range(len(X_test))]\n        skf = StratifiedKFold(n_splits=n_folds, random_state=81, shuffle=True)\n        for i, (train_ix, val_ix) in enumerate(skf.split(X_train, y_train)):\n            _X_train, _y_train = X_train[train_ix], y_train[train_ix]\n            _X_val, _y_val = X_train[val_ix], y_train[val_ix]\n            \n            # \u30e2\u30c7\u30eb\u751f\u6210\n            clf = None\n            if clf_name == 'LogisticRegression':\n                clf = LogisticRegression(C=0.01)\n            if clf_name == 'RandomForest':\n                clf = RandomForestClassifier()\n            if clf_name == 'HistGradientBoosting':\n                clf = HistGradientBoostingClassifier()\n            if clf_name == 'NeuralNetwork':\n                clf = Sequential()\n                clf.add(Dense(32, activation='relu'))\n                clf.add(Dense(16, activation='relu'))\n                clf.add(Dense(1, activation='sigmoid'))\n                clf.compile(loss='binary_crossentropy', optimizer='adam', metrics=[AUC()])\n            \n            # \u5b66\u7fd2\u3068\u4e88\u6e2c\n            _y_pred = None\n            if clf_name == 'NeuralNetwork':\n                clf.fit(_X_train, _y_train, epochs=5)\n                y_preds_sum = [pr1+pr2 for (pr1, pr2) in zip(y_preds_sum, clf.predict(X_test)[:, 0])]\n            else:\n                clf.fit(_X_train, _y_train)\n                y_preds_sum = [pr1+pr2 for (pr1, pr2) in zip(y_preds_sum, clf.predict_proba(X_test)[:,1])]\n            \n        y_pred_final = [pr\/n_folds for pr in y_preds_sum]\n        \n    else:\n        clf = None\n        if clf_name == 'LogisticRegression':\n            clf = LogisticRegression(C=0.01)\n        if clf_name == 'RandomForest':\n            clf = RandomForestClassifier()\n        if clf_name == 'HistGradientBoosting':\n            clf = HistGradientBoostingClassifier()\n        if clf_name == 'NeuralNetwork':\n            clf = MLPClassifier()\n        clf.fit(X_train, y_train)\n        y_pred_final = clf.predict_proba(X_test)[:,1]\n    \n    return \n","99754bb0":"\ncols_qualitative, cols_quantitative, cols_flg, cols_unnecessary = [], [], [], []\n\n###\n### 1. Importing Data (Logistic Regression)\n###\n\nprint('\\n### 1. Importing Data\\n')\n\n# \u51e6\u7406\u5168\u4f53\u306e\u6642\u9593\u8a08\u6e2c\nnotebook_begin = time.time()\n\n# \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\nload_merged_df(cache=True)\nCOL_ID = 'ID'\nCOL_TARGET = 'loan_condition' \n\n\n###\n### 2. Clearning Data (Logistic Regression)\n###\n\nprint('\\n\\n### 2. Clearning Data\\n')\n\n# \u6b20\u640d\u6570\u5217\u8ffd\u52a0\nprint('Adding n_nan')\ndf_train['n_nan'] = df_train.isnull().sum(axis=1)\ndf_test['n_nan'] = df_test.isnull().sum(axis=1)\n\n# \u65e5\u4ed8\u3092\u8868\u3059\u5217\u3092\u65e5\u4ed8\u578b\u3078\nobject2date('issue_d') # \u6b20\u640d\u306a\u3057\nobject2date('earliest_cr_line') # \u3042\u3068\u3067\u30c7\u30fc\u30bf\u3092\u7d5e\u308b\u306e\u3067\u6b20\u640d\u306a\u3057\nextract_recent_data(oldest_year=2014) # 2014\u5e74\u4ee5\u964d\u306b\u30c7\u30fc\u30bf\u3092\u7d5e\u308b\ncols_unnecessary.append('issue_d') # issue_d\u306f\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3068\u88ab\u308b\u90e8\u5206\u304c\u306a\u3044\u306e\u3067\u524a\u9664\n\n# \u9806\u5e8f\u5c3a\u5ea6\u3092\u8868\u3059\u5217\u306f\u3053\u3053\u3067int\u578b\u3078\nordinal2int('grade')\nordinal2int('sub_grade')\nemplength2int() # \u88dc\u5b8c\u6e08\u307f\u6b20\u640d\u3042\u308a\n\n# \u6587\u5b57\u5217\u3092\u8868\u3059\u5217\u306fstr\u306b\u5909\u63db\u3057\u3064\u3064\u3053\u3053\u3067\u6b20\u640d\u5024\u88dc\u5b8c(str\u578b\u306b\u5909\u63db) \u203b\u5217\u3068\u3057\u3066\u306fobject\u306e\u307e\u307e\nobject2str('title') # \u88dc\u5b8c\u6e08\u307f\u6b20\u640d\u3042\u308a\nobject2str('emp_title') # \u88dc\u5b8c\u6e08\u307f\u6b20\u640d\u3042\u308a\npreprocess_text('title') # \u8a18\u53f7\u524a\u9664\u306a\u3069\u6700\u4f4e\u9650\u306e\u51e6\u7406\npreprocess_words_of_text('title') # \u7d30\u304b\u3044\u5358\u8a9e\u306e\u4fee\u6b63\n\n# \u305d\u306e\u4ed6: zip-code\nzipcode2str()\n\n\n###\n### 3. EDA (Logistic Regression)\n###\n\nprint('\\n\\n### 3. EDA\\n')\n\n# \u30bf\u30a4\u30d7\u5225\u306b\u7279\u5fb4\u91cf\u3092\u8aad\u307f\u8fbc\u307f\nload_cols_by_dtype()\n\n# \u7279\u5fb4\u91cf\u3054\u3068\u306b\u5206\u5e03\u3092\u307f\u308b\n#plot_histgrams(cols_quantitative)\n\n\n###\n### 4. Missing Value Completion\n###\n\nprint('\\n\\n### 4. Missing Value Completion\\n')\n\n# \u6587\u5b57\u5217\u3092\u8868\u3059\u5217\u306e\u6b20\u640d\u5024\u88dc\u5b8c(#\u3067\u88dc\u5b8c\u6e08\u307f)\nfillna_text('title', add_flg=False)\nfillna_text('emp_title', add_flg=False)\n\n# \u6570\u5024\u5217\u306e\u6b20\u640d\u5024\u88dc\u5b8c, \u6b20\u640d\u30d5\u30e9\u30b0\u4ed8\u4e0e \u203b\u57fa\u672c\u7684\u306b\u306f\u5168\u4f53\u306b\u7d1b\u308c\u3055\u305b\u308b\u3088\u3046\u306b\u88dc\u5b8c\u3057\u3066\u3001\u30d5\u30e9\u30b0\u306e\u65b9\u306b\u6b20\u640d\u60c5\u5831\u3092\u4e38\u6295\u3052\u3059\u308b\nfillna_numeric('dti', fillna_by=999.0) # dti: \u6700\u5927\u5024\u306e999.0\u3067\u88dc\u5b8c\nfillna_numeric('mths_since_last_delinq', fillna_by='minmax', val_min=0, val_max=85, add_flg=True) # mths_since_last_delinq: 0\u304b\u308985\u307e\u3067\u306e\u5024\u3067\u88dc\u5b8c(\u3053\u306e\u7bc4\u56f2\u306f\u30bf\u30fc\u30b2\u30c3\u30c8\u3078\u306e\u5f71\u97ff\u304c\u5e73\u5766)\nfillna_numeric('mths_since_last_record', fillna_by='random', add_flg=True) # mths_since_last_record: \u30e9\u30f3\u30c0\u30e0\u62bd\u51fa\u3067\u88dc\u5b8c\nfillna_numeric('revol_util', fillna_by='minmax', val_min=0, val_max=100, add_flg=True) # revol_util: 0\u304b\u3089100\u307e\u3067\u306e\u5024\u3067\u88dc\u5b8c(\u3053\u306e\u7bc4\u56f2\u306f\u30bf\u30fc\u30b2\u30c3\u30c8\u3078\u306e\u5f71\u97ff\u304c\u5e73\u5766)\nfillna_numeric('mths_since_last_major_derog', fillna_by='minmax', val_min=0, val_max=90, add_flg=True) # mths_since_last_major_derog: 0\u304b\u308990\u307e\u3067\u306e\u5024\u3067\u88dc\u5b8c(\u3053\u306e\u7bc4\u56f2\u306f\u30bf\u30fc\u30b2\u30c3\u30c8\u3078\u306e\u5f71\u97ff\u304c\u5e73\u5766)\nfillna_numeric('inq_last_6mths', fillna_by='median') # inq_last_6mths: \u4e2d\u592e\u5024\u3067\u88dc\u5b8c \u203b\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3060\u3051\u6b20\u640d\nfillna_numeric('EXTER:TotalWages', fillna_by='random', add_flg=True) # EXTER:TotalWages: \u30e9\u30f3\u30c0\u30e0\u62bd\u51fa\u3067\u88dc\u5b8c\n\n\n###\n### 5. Removing Outliers (Logistic Regression)\n###\n\nprint('\\n\\n### 5. Removing Outliers\\n')\n\n# \u6307\u6570\u95a2\u6570\u7684\u306a\u3084\u3064\u3060\u3051\u5bfe\u6570\u5909\u63db\u3002\u4e00\u7dd2\u306b\u5916\u308c\u5024\u9664\u53bb\u3082\u3084\u3063\u3068\u304f\u3002\nlog_transformation('loan_amnt', remove_outlier=False, add_flg=True)\nlog_transformation('installment', remove_outlier=False, add_flg=True)\nlog_transformation('annual_inc', remove_outlier=True, add_flg=True)\nlog_transformation('revol_bal', remove_outlier=True, add_flg=True)\nlog_transformation('tot_coll_amt', remove_outlier=True, add_flg=True)\nlog_transformation('tot_cur_bal', remove_outlier=True, add_flg=True)\nlog_transformation('EXTER:TotalWages', remove_outlier=False)\n\n    \n###\n### 6. Feature Engineering (1) (Logistic Regression)\n###\n\nprint('\\n\\n### 6. Feature Engineering (1)\\n')\n\n# \u30e1\u30e2\u30ea\u3092\u98df\u3046\u306e\u3067\u30c6\u30ad\u30b9\u30c8\u3060\u3051\u65e9\u3081\u306b\u7279\u5fb4\u91cf\u5316\u3057\u3066\u3057\u307e\u3046\n# tfidf\u3092(pca\u3057\u3066)kmeans\u3057\u305f\u30af\u30e9\u30b9\u30bf\u60c5\u5831\ntfidf_pca('emp_title', n_dim_tfidf=500, n_dim_pca=10)\n#tfidf_pca('title', n_dim_tfidf=500, n_dim_pca=5)\n# \u5143\u306e\u5217\u306f\u3082\u3046\u4f7f\u308f\u306a\u3044\nupdate_cols(add_cols_unnecessary=['title', 'emp_title'])\n\n# earliest_cr_line: 10\u5e74\u5358\u4f4d\u306b\u60c5\u5831\u91cf\u3092\u843d\u3068\u3057\u305f\u5f8c\u3001\u9806\u5e8f\u5c3a\u5ea6\u3068\u3057\u3066\u6570\u5024\u578b\u7279\u5fb4\u306b\u5909\u63db\u3002\nyear2decade('earliest_cr_line', remove_original=True)\nordinal2int('earliest_cr_line_decade_year')\n\n\n###\n### 7. Encoding (Logistic Regression)\n###\n\nprint('\\n\\n### 7. Encoding\\n')\n\n# OneHot\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u5b9f\u65bd\nupdate_cols(add_cols_unnecessary=['zip_code']) # \u30ab\u30c6\u30b4\u30ea\u6570\u591a\u3059\u304e\u308b\u306e\u3067\u6392\u9664\nonehot_encoding(cols_qualitative)\n\n\n###\n### 7.5 Scaling Numerics (Logistic Regression)\n###\n\nprint('\\n\\n### 7.5. Scaling Numerics\\n')\n\n# \u6a19\u6e96\u5316\nprint('Scaling with Standard Scaler')\nfor col in cols_quantitative:\n    scaler = StandardScaler()\n    scaler.fit(df_train[[col]])\n    df_train[col] = scaler.transform(df_train[[col]])\n    df_test[col] = scaler.transform(df_test[[col]])\n\n# \u7dda\u5f62\u30e2\u30c7\u30eb\u7528\u306eDataFrame\u306b\u30b3\u30d4\u30fc\u3057\u3066\u304a\u304f\nprint('Copying Dataframe to df_linear')\ncols_to_use = get_cols_to_use()\ndf_train_linear = df_train[cols_to_use].copy()\ndf_test_linear = df_test[cols_to_use].copy()\n\n# \u7279\u5fb4\u91cf\u6570\u306e\u78ba\u8a8d\nprint('number of features: {}'.format(len(get_cols_to_use())))\n\n# LogisticRegression\u7528\u306b\u5fc5\u8981\u306a\u3044\u5217\u3092\u524a\u9664\uff08\u4ed6\u3068\u76f8\u95a2\u306e\u9ad8\u3044\u3082\u306e\uff09 0.70493->0.70268\n#cols_unnecessary_tmp = ['pub_rec', 'EXTER:Population (million)', 'EXTER:Gross State Product', \n#                        'EXTER:LocationType_NOT ACCEPTABLE', 'addr_state_NJ', 'application_type_Joint App', \n#                        'initial_list_status_f']\n#cols_unnecessary_tmp += ['addr_state_MD', 'EXTER:Real State Growth %', 'tot_cur_bal', 'home_ownership_MORTGAGE', \n#                         'FLG:mths_since_last_delinq_isnull', 'purpose_credit_card', 'total_acc', 'addr_state_CA']\n#update_cols(add_cols_unnecessary=cols_unnecessary_tmp)\n    \n    \n###\n### 9. Cross Validation (Logistic Regression)\n###\n\nprint('\\n\\n### 9. Cross Validation\\n')\n\n# \u7279\u5fb4\u91cf\u6570\u306e\u78ba\u8a8d\nprint('number of features: {}'.format(len(get_cols_to_use())))\n\n# Holdout\u3092\u53d6\u3063\u3066\u304a\u304f(Adversarial Validation\u306f\u306a\u3057)\npertition_holdout(rate_holdout=0.2, use_test_like_records=False, use_recent_records=True)\n\n# \u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u5b9f\u884c\ncross_validation(clf_name='LogisticRegression', rate_sampling=1.0)\n\n\n###\n### 10. Holdout Validation\n###\n\nprint('\\n\\n### 10. Holdout Validation\\n')\n\n# Holdout\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\nholdout_validation(clf_name='LogisticRegression', plot_feature_importance=True)\n\n\n###\n### 11. Modeling for Submission\n###\n\nprint('\\n\\n### 11. Modeling for Submission\\n')\n\n# \u5168\u30c7\u30fc\u30bf\u30e2\u30c7\u30eb\u751f\u6210\nmodeling_and_prediction(clf_name='LogisticRegression', cross_validation_ensemble=True, n_folds=5)\n\n# \u4e88\u6e2c\u5024\u3092\u4fdd\u5b58\nprint('Appending model predictions to y_preds_list')\ny_preds_list_for_ensemble.append(y_pred_final)\ncvscore_list_for_ensemble.append(score_cv_avr)\nholdoutscore_list_for_ensemble.append(score_holdout)\n\n\n###\n### 12. Print Results\n###\n\nprint('\\n\\n### 12. Print Results (Logistic Regression)\\n')\n\n# \u8af8\u3005\u306e\u5024\u3092\u51fa\u529b\nprint('number of features: {}'.format(len(get_cols_to_use())))\n\n# \u7d50\u679c\u3092\u307e\u3068\u3081\u3066\u51fa\u529b\nprint('Print scores')\nprint(' > Avr of CV: {:.5f}'.format(score_cv_avr))\nprint(' > Holdout  : {:.5f}'.format(score_holdout))\n\n# \u7dcf\u7d4c\u904e\u6642\u9593\u306e\u51fa\u529b\nprint('Calculating total elapsed time')\nelapsed_time = time.time()-notebook_begin\nelapsed_sec = int(elapsed_time%60)\nelapsed_min = int((elapsed_time-elapsed_sec)\/60)\nprint(' > total elapsed time ... {}:{}'.format(elapsed_min, str(elapsed_sec).rjust(2, '0')))\n","76928250":"\ncols_qualitative, cols_quantitative, cols_flg, cols_unnecessary = [], [], [], []\nlgbm_params = {'objective': 'binary', 'metric': 'auc', 'learning_rate': 0.05}\n\n###\n### 1. Importing Data\n###\n\nprint('\\n### 1. Importing Data\\n')\n\n# \u51e6\u7406\u5168\u4f53\u306e\u6642\u9593\u8a08\u6e2c\nnotebook_begin = time.time()\n\n# \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\nload_merged_df(cache=True)\nCOL_ID = 'ID'\nCOL_TARGET = 'loan_condition' \n\n\n###\n### 2. Clearning Data\n###\n\nprint('\\n\\n### 2. Clearning Data\\n')\n\n# \u6b20\u640d\u6570\u5217\u8ffd\u52a0\nprint('Adding n_nan')\ndf_train['n_nan'] = df_train.isnull().sum(axis=1)\ndf_test['n_nan'] = df_test.isnull().sum(axis=1)\n\n# \u65e5\u4ed8\u3092\u8868\u3059\u5217\u3092\u65e5\u4ed8\u578b\u3078\nobject2date('issue_d') # \u6b20\u640d\u306a\u3057\nobject2date('earliest_cr_line') # \u3042\u3068\u3067\u30c7\u30fc\u30bf\u3092\u7d5e\u308b\u306e\u3067\u6b20\u640d\u306a\u3057\nextract_recent_data(oldest_year=2014) # 2014\u5e74\u4ee5\u964d\u306b\u30c7\u30fc\u30bf\u3092\u7d5e\u308b\ncols_unnecessary.append('issue_d') # issue_d\u306f\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3068\u88ab\u308b\u90e8\u5206\u304c\u306a\u3044\u306e\u3067\u524a\u9664\n\n# \u9806\u5e8f\u5c3a\u5ea6\u3092\u8868\u3059\u5217\u306f\u3053\u3053\u3067int\u578b\u3078\nordinal2int('grade')\nordinal2int('sub_grade')\nemplength2int() # \u88dc\u5b8c\u6e08\u307f\u6b20\u640d\u3042\u308a\n\n# \u6587\u5b57\u5217\u3092\u8868\u3059\u5217\u306fstr\u306b\u5909\u63db\u3057\u3064\u3064\u3053\u3053\u3067\u6b20\u640d\u5024\u88dc\u5b8c(str\u578b\u306b\u5909\u63db) \u203b\u5217\u3068\u3057\u3066\u306fobject\u306e\u307e\u307e\nobject2str('title') # \u88dc\u5b8c\u6e08\u307f\u6b20\u640d\u3042\u308a\nobject2str('emp_title') # \u88dc\u5b8c\u6e08\u307f\u6b20\u640d\u3042\u308a\npreprocess_text('title') # \u8a18\u53f7\u524a\u9664\u306a\u3069\u6700\u4f4e\u9650\u306e\u51e6\u7406\npreprocess_words_of_text('title') # \u7d30\u304b\u3044\u5358\u8a9e\u306e\u4fee\u6b63\n\n# \u305d\u306e\u4ed6: zip-code\nzipcode2str()\n\n\n###\n### 3. EDA\n###\n\nprint('\\n\\n### 3. EDA\\n')\n\n# \u30bf\u30a4\u30d7\u5225\u306b\u7279\u5fb4\u91cf\u3092\u8aad\u307f\u8fbc\u307f\nload_cols_by_dtype()\n\n# \u7279\u5fb4\u91cf\u3054\u3068\u306b\u5206\u5e03\u3092\u307f\u308b\n#plot_histgrams(cols_quantitative)\n\n\n###\n### 4. Removing Outliers\n###\n\nprint('\\n\\n### 4. Removing Outliers\\n')\n\n# \u6307\u6570\u95a2\u6570\u7684\u306a\u3084\u3064\u3060\u3051\u5bfe\u6570\u5909\u63db\u3002\u4e00\u7dd2\u306b\u5916\u308c\u5024\u9664\u53bb\u3082\u3084\u3063\u3068\u304f\u3002\nlog_transformation('loan_amnt', remove_outlier=False)\nlog_transformation('installment', remove_outlier=False)\nlog_transformation('annual_inc', remove_outlier=True, add_flg=True) # \u5916\u308c\u5024\u30d5\u30e9\u30b0\u306f\u82e5\u5e72\u306e\u52b9\u679c\u3042\u308a\nlog_transformation('revol_bal', remove_outlier=True, add_flg=True) # \u5916\u308c\u5024\u30d5\u30e9\u30b0\u306f\u82e5\u5e72\u306e\u52b9\u679c\u3042\u308a\nlog_transformation('tot_coll_amt', remove_outlier=True, add_flg=True) # \u5916\u308c\u5024\u30d5\u30e9\u30b0\u306f\u82e5\u5e72\u306e\u52b9\u679c\u3042\u308a\nlog_transformation('tot_cur_bal', remove_outlier=True, add_flg=True) # \u5916\u308c\u5024\u30d5\u30e9\u30b0\u306f\u82e5\u5e72\u306e\u52b9\u679c\u3042\u308a\nlog_transformation('EXTER:TotalWages', remove_outlier=False)\n\n\n###\n### 5. Missing Value Completion\n###\n\nprint('\\n\\n### 5. Missing Value Completion\\n')\n\n# \u6587\u5b57\u5217\u3092\u8868\u3059\u5217\u306e\u6b20\u640d\u5024\u88dc\u5b8c(#\u3067\u88dc\u5b8c\u6e08\u307f)\nfillna_text('title', add_flg=False)\nfillna_text('emp_title', add_flg=False)\n\n# \u6570\u5024\u5217\u306e\u6b20\u640d\u5024\u88dc\u5b8c, \u6b20\u640d\u30d5\u30e9\u30b0\u4ed8\u4e0e \u203b\u57fa\u672c\u7684\u306b\u306f\u5168\u4f53\u306b\u7d1b\u308c\u3055\u305b\u308b\u3088\u3046\u306b\u88dc\u5b8c\u3057\u3066\u3001\u30d5\u30e9\u30b0\u306e\u65b9\u306b\u6b20\u640d\u60c5\u5831\u3092\u4e38\u6295\u3052\u3059\u308b\nfillna_numeric('dti', fillna_by=-999)\nfillna_numeric('mths_since_last_delinq', fillna_by=-999)\nfillna_numeric('mths_since_last_record', fillna_by=-999)\nfillna_numeric('revol_util', fillna_by=-999)\nfillna_numeric('mths_since_last_major_derog', fillna_by=-999)\nfillna_numeric('inq_last_6mths', fillna_by='median') # inq_last_6mths: \u4e2d\u592e\u5024\u3067\u88dc\u5b8c \u203b\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3060\u3051\u6b20\u640d\nfillna_numeric('EXTER:TotalWages', fillna_by=-999) \n\n\n###\n### 6. Feature Engineering (1)\n###\n\nprint('\\n\\n### 6. Feature Engineering (1)\\n')\n\n# tfidf\u3092(pca\u3057\u3066)kmeans\u3057\u305f\u30af\u30e9\u30b9\u30bf\u60c5\u5831\ntfidf_pca_kmeans_cluster('emp_title', n_dim_tfidf=500, n_dim_pca=-1, n_cluster=100)\n\n# tfidf(\u3092pca\u3057\u305f\u3082\u306e)\u306b\u5bfe\u3059\u308bLogisticRegression\u306e\u4e88\u6e2c\u5024\ntfidf_pca_logreg_prediction('emp_title', n_dim_tfidf=500, n_dim_pca=-1) # \u30c6\u30ad\u30b9\u30c8\u306e\u524d\u51e6\u7406\u306a\u3057=0.57007, \u3042\u308a=0.56939\n\n# \u5143\u306e\u5217\u306f\u3082\u3046\u4f7f\u308f\u306a\u3044\nupdate_cols(add_cols_unnecessary=['emp_title'])\n    \n# earliest_cr_line: 10\u5e74\u5358\u4f4d\u306b\u60c5\u5831\u91cf\u3092\u843d\u3068\u3057\u305f\u5f8c\u3001\u9806\u5e8f\u5c3a\u5ea6\u3068\u3057\u3066\u6570\u5024\u578b\u7279\u5fb4\u306b\u5909\u63db\u3002\nyear2decade('earliest_cr_line', remove_original=True)\nordinal2int('earliest_cr_line_decade_year')\n\n# \u7dda\u5f62\u30e2\u30c7\u30eb\u7528\u306e\u6a19\u6e96\u5316\u3055\u308c\u305f\u30c6\u30fc\u30d6\u30eb\u3092\u5143\u306b\u3001\u30ec\u30b3\u30fc\u30c9\u5168\u4f53\u306e\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u60c5\u5831\u3092\u4ed8\u4e0e\n#record_clustering_pca_tsne_kmeans(n_dim_pca=10, n_dim_tsne=2, n_cluster=100) # \u6642\u9593\u304b\u304b\u308a\u3059\u304e\u308b\u306e\u3067\u3084\u3089\u306a\u3044\nrecord_clustering_kmeans(n_cluster=100)\n\n###\n### 7. Encoding\n###\n\nprint('\\n\\n### 7. Encoding\\n')\n    \n# \u30ab\u30a6\u30f3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3068\u30bf\u30fc\u30b2\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u4e21\u65b9\u5b9f\u65bd\ncols_qualitative_all = cols_qualitative.copy()\ncount_encoding(cols_qualitative_all, remove_original=True)\ncount_encoding(cols_qualitative_all, remove_original=False)\ntarget_encoding(cols_qualitative_all, remove_original=True) # \u30ab\u30c6\u30b4\u30ea\u5909\u65707\u500b\u304c\u5bfe\u8c61\ndel cols_qualitative_all\n\n# \u30ab\u30c6\u30b4\u30ea\u5909\u6570\u306b\u52a0\u3048\u3066\u30e6\u30cb\u30fc\u30af\u6570\u304c\u5c11\u306a\u304f(\u3068\u308a\u3042\u3048\u305a\u306f\u30e6\u30cb\u30fc\u30af\u657020\u4ee5\u4e0b)\u3042\u308b\u610f\u5473\u30ab\u30c6\u30b4\u30ea\u3068\u307f\u306a\u305b\u305d\u3046\u306a\u6570\u5024\u7279\u5fb4\u3082\u30a8\u30f3\u30b3\u30fc\u30c9\ncols_quantitative_few_unique = ['acc_now_delinq', 'collections_12_mths_ex_med', \n                                'earliest_cr_line_decade_year', 'emp_length', 'grade', \n                                'inq_last_6mths', 'n_nan'] # \u6570\u5024\u5909\u65707\u500b\u304c\u5bfe\u8c61\ntarget_encoding(cols_quantitative_few_unique, remove_original=False)\n\n# \u30bf\u30fc\u30b2\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u5f8c\u306b\u6b20\u640d\u5024\u304c\u3042\u308b\u3063\u307d\u3044\uff1f\uff1f\uff1f\nfor col in cols_quantitative:\n    if 'TARGET' in col:\n        df_train[col] = df_train[col].fillna(df_train[col].median())\n        df_test[col] = df_test[col].fillna(df_test[col].median())\n\n\n###\n### 8. Feature Engineering (2)\n###\n\nprint('\\n\\n### 8. Feature Engineering (2)\\n')\n\n# \u4ea4\u4e92\u4f5c\u7528\u9805\u306e\u8ffd\u52a0: \u5358\u4e00\u7279\u5fb4\u91cf\u306e\u307f\u3067\u8a13\u7df4\u3057\u305f\u969b\u306eHoldout\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306e\u91cd\u8981\u7279\u5fb4\u91cftop10\u306e\u7dcf\u5f53\u305f\u308a\u3092\u8ffd\u52a0\ngenerate_interaction_terms(['sub_grade', 'grade', 'TXT:logreg_prediction_on_emp_title', 'loan_amnt', 'dti', \n                            'COUNT:home_ownership', 'tot_cur_bal', 'open_acc', 'installment', 'emp_length'],\n                           n_conbinations=2) \nprint(' > {} interaction features added'.format(len([col for col in cols_quantitative if (col not in cols_unnecessary) and ('INTRCTN' in col)])))\n\n# \u5dee\u5206\u9805\u306e\u8ffd\u52a0: \u5358\u4e00\u7279\u5fb4\u91cf\u306e\u307f\u3067\u8a13\u7df4\u3057\u305f\u969b\u306eHoldout\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306e\u91cd\u8981\u7279\u5fb4\u91cftop10\u306e\u7dcf\u5f53\u305f\u308a\u3092\u8ffd\u52a0\ngenerate_difference_terms(['sub_grade', 'grade', 'TXT:logreg_prediction_on_emp_title', 'loan_amnt', 'dti', \n                           'COUNT:home_ownership', 'tot_cur_bal', 'open_acc', 'installment', 'emp_length']) \nprint(' > {} difference features added'.format(len([col for col in cols_quantitative if (col not in cols_unnecessary) and ('DIFF' in col)])))\n\n# \u5272\u5408\u9805\u306e\u8ffd\u52a0: \u52b9\u304d\u305d\u3046\u306a\u5272\u308a\u7b97\u3092\u52d8\u3067\u8ffd\u52a0\ngenerate_division_terms([['EXTER:State & Local Spending', 'annual_inc'], ['loan_amnt', 'annual_inc'],\n                         ['installment', 'annual_inc'], ['tot_coll_amt', 'annual_inc'], ['tot_cur_bal', 'annual_inc'],\n                         ['loan_amnt', 'revol_bal'], ['installment', 'revol_bal'], ['tot_coll_amt', 'revol_bal'],\n                         ['tot_cur_bal', 'revol_bal'], ['loan_amnt', 'tot_cur_bal'], ['installment', 'tot_cur_bal'],\n                         ['tot_coll_amt', 'tot_cur_bal']]) \nprint(' > {} division features added'.format(len([col for col in cols_quantitative if (col not in cols_unnecessary) and ('DIV' in col)])))\n\n# \u4e0a\u8a18\u3067\u3042\u307e\u308a\u52b9\u304b\u306a\u304b\u3063\u305f\u3082\u306e\u306a\u3069\u3092\u524a\u9664\ncols_unnecessary_tmp = ['INTRCTN:COUNT:home_ownership (times) emp_length', 'INTRCTN:emp_length (times) grade', \n                        'DIFF:COUNT:home_ownership (minus) emp_length', 'DIFF:dti (minus) loan_amnt', \n                        'INTRCTN:COUNT:home_ownership (times) grade', \n                        'DIFF:TXT:logreg_prediction_on_emp_title (minus) dti', 'DIFF:grade (minus) open_acc', \n                        'DIFF:COUNT:home_ownership (minus) grade', 'DIFF:emp_length (minus) grade', \n                        'DIFF:grade (minus) sub_grade', 'INTRCTN:grade (times) sub_grade'] # 11\u500b\ncols_unnecessary_tmp += [col for col in cols_quantitative if 'DIFF' in col] # \u5dee\u5206\u9805\u5168\u90e8\u6d88\u3057\u3066\u307f\u308b\nupdate_cols(add_cols_unnecessary=cols_unnecessary_tmp)\n\n# Adversarial Validation\u3067\u30c6\u30b9\u30c8\u3063\u307d\u3055\u5217\u306e\u8ffd\u52a0\nrate_sampling = 0.2\nadversarial_validation_lgb(rate_sampling=rate_sampling, plot_feature_importance=True)\n\n\n###\n### 9. Cross Validation\n###\n\nprint('\\n\\n### 9. Cross Validation\\n')\n\n# Holdout\u3092\u53d6\u3063\u3066\u304a\u304f\npertition_holdout(rate_holdout=0.2, use_test_like_records=False, use_recent_records=True)\n\n# \u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u5b9f\u884c\ncross_validation_lgb(rate_sampling=1.0, n_folds=5, plot_feature_importance=True, print_topworst_features=False, n_top=10)\n\n\n###\n### 10. Holdout Validation\n###\n\nprint('\\n\\n### 10. Holdout Validation\\n')\n\n# Holdout\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\nholdout_validation_lgb(plot_feature_importance=False, print_topworst_features=False, print_topworst_features_by_conditions=False)\n\n\n###\n### 11. Modeling for Submission\n###\n\nprint('\\n\\n### 11. Modeling for Submission\\n')\n\n# \u5168\u30c7\u30fc\u30bf\u30e2\u30c7\u30eb\u751f\u6210\nmodeling_and_prediction_lgb(cross_validation_ensemble=True, n_folds=5)\n\n# \u4e88\u6e2c\u5024\u3092\u4fdd\u5b58\nprint('Appending model predictions to y_preds_list')\ny_preds_list_for_ensemble.append(y_pred_final)\ncvscore_list_for_ensemble.append(score_cv_avr)\nholdoutscore_list_for_ensemble.append(score_holdout)\n\n\n###\n### 12. Print Results\n###\n\nprint('\\n\\n### 12. Print Results (LightGBM(1))\\n')\n\n# \u8af8\u3005\u306e\u5024\u3092\u51fa\u529b\nprint('lgbm_params: {}'.format(lgbm_params))\nprint('number of features: {}'.format(len(get_cols_to_use())))\n\n# \u7d50\u679c\u3092\u307e\u3068\u3081\u3066\u51fa\u529b\nprint('Print scores')\nprint(' > Avr of CV: {:.5f}'.format(score_cv_avr))\nprint(' > Holdout  : {:.5f}'.format(score_holdout))\n\n# \u7dcf\u7d4c\u904e\u6642\u9593\u306e\u51fa\u529b\nprint('Calculating total elapsed time')\nelapsed_time = time.time()-notebook_begin\nelapsed_sec = int(elapsed_time%60)\nelapsed_min = int((elapsed_time-elapsed_sec)\/60)\nprint(' > total elapsed time ... {}:{}'.format(elapsed_min, str(elapsed_sec).rjust(2, '0')))\n","e4720fe9":"\ncols_qualitative, cols_quantitative, cols_flg, cols_unnecessary = [], [], [], []\nlgbm_params = {'objective': 'binary', 'metric': 'auc', 'max bin': 127, 'learning_rate': 0.1}\n\n###\n### 1. Importing Data\n###\n\nprint('\\n### 1. Importing Data\\n')\n\n# \u51e6\u7406\u5168\u4f53\u306e\u6642\u9593\u8a08\u6e2c\nnotebook_begin = time.time()\n\n# \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\nload_merged_df(cache=True)\nCOL_ID = 'ID'\nCOL_TARGET = 'loan_condition' \n\n\n###\n### 2. Clearning Data\n###\n\nprint('\\n\\n### 2. Clearning Data\\n')\n\n# \u6b20\u640d\u6570\u5217\u8ffd\u52a0\nprint('Adding n_nan')\ndf_train['n_nan'] = df_train.isnull().sum(axis=1)\ndf_test['n_nan'] = df_test.isnull().sum(axis=1)\n\n# \u65e5\u4ed8\u3092\u8868\u3059\u5217\u3092\u65e5\u4ed8\u578b\u3078\nobject2date('issue_d') # \u6b20\u640d\u306a\u3057\nobject2date('earliest_cr_line') # \u3042\u3068\u3067\u30c7\u30fc\u30bf\u3092\u7d5e\u308b\u306e\u3067\u6b20\u640d\u306a\u3057\nextract_recent_data(oldest_year=2014) # 2014\u5e74\u4ee5\u964d\u306b\u30c7\u30fc\u30bf\u3092\u7d5e\u308b\ncols_unnecessary.append('issue_d') # issue_d\u306f\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3068\u88ab\u308b\u90e8\u5206\u304c\u306a\u3044\u306e\u3067\u524a\u9664\n\n# \u9806\u5e8f\u5c3a\u5ea6\u3092\u8868\u3059\u5217\u306f\u3053\u3053\u3067int\u578b\u3078\nordinal2int('grade')\nordinal2int('sub_grade')\nemplength2int() # \u88dc\u5b8c\u6e08\u307f\u6b20\u640d\u3042\u308a\n\n# \u6587\u5b57\u5217\u3092\u8868\u3059\u5217\u306fstr\u306b\u5909\u63db\u3057\u3064\u3064\u3053\u3053\u3067\u6b20\u640d\u5024\u88dc\u5b8c(str\u578b\u306b\u5909\u63db) \u203b\u5217\u3068\u3057\u3066\u306fobject\u306e\u307e\u307e\nobject2str('title') # \u88dc\u5b8c\u6e08\u307f\u6b20\u640d\u3042\u308a\nobject2str('emp_title') # \u88dc\u5b8c\u6e08\u307f\u6b20\u640d\u3042\u308a\npreprocess_text('title') # \u8a18\u53f7\u524a\u9664\u306a\u3069\u6700\u4f4e\u9650\u306e\u51e6\u7406\npreprocess_words_of_text('title') # \u7d30\u304b\u3044\u5358\u8a9e\u306e\u4fee\u6b63\n\n# \u305d\u306e\u4ed6: zip-code\nzipcode2str()\n\n\n###\n### 3. EDA\n###\n\nprint('\\n\\n### 3. EDA\\n')\n\n# \u30bf\u30a4\u30d7\u5225\u306b\u7279\u5fb4\u91cf\u3092\u8aad\u307f\u8fbc\u307f\nload_cols_by_dtype()\n\n# \u7279\u5fb4\u91cf\u3054\u3068\u306b\u5206\u5e03\u3092\u307f\u308b\n#plot_histgrams(cols_quantitative)\n\n\n###\n### 4. Removing Outliers\n###\n\nprint('\\n\\n### 4. Removing Outliers\\n')\n\n# \u6307\u6570\u95a2\u6570\u7684\u306a\u3084\u3064\u3060\u3051\u5bfe\u6570\u5909\u63db\u3002\u4e00\u7dd2\u306b\u5916\u308c\u5024\u9664\u53bb\u3082\u3084\u3063\u3068\u304f\u3002\nlog_transformation('loan_amnt', remove_outlier=False)\nlog_transformation('installment', remove_outlier=False)\nlog_transformation('annual_inc', remove_outlier=True, add_flg=True) # \u5916\u308c\u5024\u30d5\u30e9\u30b0\u306f\u82e5\u5e72\u306e\u52b9\u679c\u3042\u308a\nlog_transformation('revol_bal', remove_outlier=True, add_flg=True) # \u5916\u308c\u5024\u30d5\u30e9\u30b0\u306f\u82e5\u5e72\u306e\u52b9\u679c\u3042\u308a\nlog_transformation('tot_coll_amt', remove_outlier=True, add_flg=True) # \u5916\u308c\u5024\u30d5\u30e9\u30b0\u306f\u82e5\u5e72\u306e\u52b9\u679c\u3042\u308a\nlog_transformation('tot_cur_bal', remove_outlier=True, add_flg=True) # \u5916\u308c\u5024\u30d5\u30e9\u30b0\u306f\u82e5\u5e72\u306e\u52b9\u679c\u3042\u308a\nlog_transformation('EXTER:TotalWages', remove_outlier=False)\n\n\n###\n### 5. Missing Value Completion\n###\n\nprint('\\n\\n### 5. Missing Value Completion\\n')\n\n# \u6587\u5b57\u5217\u3092\u8868\u3059\u5217\u306e\u6b20\u640d\u5024\u88dc\u5b8c(#\u3067\u88dc\u5b8c\u6e08\u307f)\nfillna_text('title', add_flg=False)\nfillna_text('emp_title', add_flg=False)\n\n# \u6570\u5024\u5217\u306e\u6b20\u640d\u5024\u88dc\u5b8c, \u6b20\u640d\u30d5\u30e9\u30b0\u4ed8\u4e0e \u203b\u57fa\u672c\u7684\u306b\u306f\u5168\u4f53\u306b\u7d1b\u308c\u3055\u305b\u308b\u3088\u3046\u306b\u88dc\u5b8c\u3057\u3066\u3001\u30d5\u30e9\u30b0\u306e\u65b9\u306b\u6b20\u640d\u60c5\u5831\u3092\u4e38\u6295\u3052\u3059\u308b\nfillna_numeric('dti', fillna_by=-999)\nfillna_numeric('mths_since_last_delinq', fillna_by=-999)\nfillna_numeric('mths_since_last_record', fillna_by=-999)\nfillna_numeric('revol_util', fillna_by=-999)\nfillna_numeric('mths_since_last_major_derog', fillna_by=-999)\nfillna_numeric('inq_last_6mths', fillna_by='median') # inq_last_6mths: \u4e2d\u592e\u5024\u3067\u88dc\u5b8c \u203b\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3060\u3051\u6b20\u640d\nfillna_numeric('EXTER:TotalWages', fillna_by=-999) \n\n\n###\n### 6. Feature Engineering (1)\n###\n\nprint('\\n\\n### 6. Feature Engineering (1)\\n')\n\n# tfidf\u3092(pca\u3057\u3066)kmeans\u3057\u305f\u30af\u30e9\u30b9\u30bf\u60c5\u5831\ntfidf_pca_kmeans_cluster('emp_title', n_dim_tfidf=500, n_dim_pca=-1, n_cluster=20)\n\n# tfidf(\u3092pca\u3057\u305f\u3082\u306e)\u306b\u5bfe\u3059\u308bLogisticRegression\u306e\u4e88\u6e2c\u5024\ntfidf_pca_logreg_prediction('emp_title', n_dim_tfidf=500, n_dim_pca=-1) # \u30c6\u30ad\u30b9\u30c8\u306e\u524d\u51e6\u7406\u306a\u3057=0.57007, \u3042\u308a=0.56939\n\n# \u5143\u306e\u5217\u306f\u3082\u3046\u4f7f\u308f\u306a\u3044\nupdate_cols(add_cols_unnecessary=['emp_title'])\n    \n# earliest_cr_line: 10\u5e74\u5358\u4f4d\u306b\u60c5\u5831\u91cf\u3092\u843d\u3068\u3057\u305f\u5f8c\u3001\u9806\u5e8f\u5c3a\u5ea6\u3068\u3057\u3066\u6570\u5024\u578b\u7279\u5fb4\u306b\u5909\u63db\u3002\nyear2decade('earliest_cr_line', remove_original=True)\nordinal2int('earliest_cr_line_decade_year')\n\n# \u7dda\u5f62\u30e2\u30c7\u30eb\u7528\u306e\u6a19\u6e96\u5316\u3055\u308c\u305f\u30c6\u30fc\u30d6\u30eb\u3092\u5143\u306b\u3001\u30ec\u30b3\u30fc\u30c9\u5168\u4f53\u306e\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u60c5\u5831\u3092\u4ed8\u4e0e\n#record_clustering_pca_tsne_kmeans(n_dim_pca=10, n_dim_tsne=2, n_cluster=100) # \u6642\u9593\u304b\u304b\u308a\u3059\u304e\u308b\u306e\u3067\u3084\u3089\u306a\u3044\nrecord_clustering_kmeans(n_cluster=100)\n\n###\n### 7. Encoding\n###\n\nprint('\\n\\n### 7. Encoding\\n')\n    \n# \u30ab\u30a6\u30f3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3068\u30bf\u30fc\u30b2\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u4e21\u65b9\u5b9f\u65bd\ncols_qualitative_all = cols_qualitative.copy()\ncount_encoding(cols_qualitative_all, remove_original=True)\ncount_encoding(cols_qualitative_all, remove_original=False)\ntarget_encoding(cols_qualitative_all, remove_original=True) # \u30ab\u30c6\u30b4\u30ea\u5909\u65707\u500b\u304c\u5bfe\u8c61\ndel cols_qualitative_all\n\n# \u30ab\u30c6\u30b4\u30ea\u5909\u6570\u306b\u52a0\u3048\u3066\u30e6\u30cb\u30fc\u30af\u6570\u304c\u5c11\u306a\u304f(\u3068\u308a\u3042\u3048\u305a\u306f\u30e6\u30cb\u30fc\u30af\u657020\u4ee5\u4e0b)\u3042\u308b\u610f\u5473\u30ab\u30c6\u30b4\u30ea\u3068\u307f\u306a\u305b\u305d\u3046\u306a\u6570\u5024\u7279\u5fb4\u3082\u30a8\u30f3\u30b3\u30fc\u30c9\ncols_quantitative_few_unique = ['acc_now_delinq', 'collections_12_mths_ex_med', \n                                'earliest_cr_line_decade_year', 'emp_length', 'grade', \n                                'inq_last_6mths', 'n_nan'] # \u6570\u5024\u5909\u65707\u500b\u304c\u5bfe\u8c61\ntarget_encoding(cols_quantitative_few_unique, remove_original=False)\n\n# \u30bf\u30fc\u30b2\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u5f8c\u306b\u6b20\u640d\u5024\u304c\u3042\u308b\u3063\u307d\u3044\uff1f\uff1f\uff1f\nfor col in cols_quantitative:\n    if 'TARGET' in col:\n        df_train[col] = df_train[col].fillna(df_train[col].median())\n        df_test[col] = df_test[col].fillna(df_test[col].median())\n\n\n###\n### 8. Feature Engineering (2)\n###\n\nprint('\\n\\n### 8. Feature Engineering (2)\\n')\n\n# \u4ea4\u4e92\u4f5c\u7528\u9805\u306e\u8ffd\u52a0: \u5358\u4e00\u7279\u5fb4\u91cf\u306e\u307f\u3067\u8a13\u7df4\u3057\u305f\u969b\u306eHoldout\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306e\u91cd\u8981\u7279\u5fb4\u91cftop10\u306e\u7dcf\u5f53\u305f\u308a\u3092\u8ffd\u52a0\ngenerate_interaction_terms(['sub_grade', 'grade', 'TXT:logreg_prediction_on_emp_title', 'loan_amnt', 'dti', \n                            'COUNT:home_ownership', 'tot_cur_bal', 'open_acc', 'installment', 'emp_length'],\n                           n_conbinations=2) \nprint(' > {} interaction features added'.format(len([col for col in cols_quantitative if (col not in cols_unnecessary) and ('INTRCTN' in col)])))\n\n# \u5dee\u5206\u9805\u306e\u8ffd\u52a0: \u5358\u4e00\u7279\u5fb4\u91cf\u306e\u307f\u3067\u8a13\u7df4\u3057\u305f\u969b\u306eHoldout\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306e\u91cd\u8981\u7279\u5fb4\u91cftop10\u306e\u7dcf\u5f53\u305f\u308a\u3092\u8ffd\u52a0\ngenerate_difference_terms(['sub_grade', 'grade', 'TXT:logreg_prediction_on_emp_title', 'loan_amnt', 'dti', \n                           'COUNT:home_ownership', 'tot_cur_bal', 'open_acc', 'installment', 'emp_length']) \nprint(' > {} difference features added'.format(len([col for col in cols_quantitative if (col not in cols_unnecessary) and ('DIFF' in col)])))\n\n# \u5272\u5408\u9805\u306e\u8ffd\u52a0: \u52b9\u304d\u305d\u3046\u306a\u5272\u308a\u7b97\u3092\u52d8\u3067\u8ffd\u52a0\ngenerate_division_terms([['EXTER:State & Local Spending', 'annual_inc'], ['loan_amnt', 'annual_inc'],\n                         ['installment', 'annual_inc'], ['tot_coll_amt', 'annual_inc'], ['tot_cur_bal', 'annual_inc'],\n                         ['loan_amnt', 'revol_bal'], ['installment', 'revol_bal'], ['tot_coll_amt', 'revol_bal'],\n                         ['tot_cur_bal', 'revol_bal'], ['loan_amnt', 'tot_cur_bal'], ['installment', 'tot_cur_bal'],\n                         ['tot_coll_amt', 'tot_cur_bal']]) \nprint(' > {} division features added'.format(len([col for col in cols_quantitative if (col not in cols_unnecessary) and ('DIV' in col)])))\n\n# \u4e0a\u8a18\u3067\u3042\u307e\u308a\u52b9\u304b\u306a\u304b\u3063\u305f\u3082\u306e\u306a\u3069\u3092\u524a\u9664\ncols_unnecessary_tmp = ['INTRCTN:COUNT:home_ownership (times) emp_length', 'INTRCTN:emp_length (times) grade', \n                        'DIFF:COUNT:home_ownership (minus) emp_length', 'DIFF:dti (minus) loan_amnt', \n                        'INTRCTN:COUNT:home_ownership (times) grade', \n                        'DIFF:TXT:logreg_prediction_on_emp_title (minus) dti', 'DIFF:grade (minus) open_acc', \n                        'DIFF:COUNT:home_ownership (minus) grade', 'DIFF:emp_length (minus) grade', \n                        'DIFF:grade (minus) sub_grade', 'INTRCTN:grade (times) sub_grade'] # 11\u500b\ncols_unnecessary_tmp += [col for col in cols_quantitative if 'DIFF' in col] # \u5dee\u5206\u9805\u5168\u90e8\u6d88\u3057\u3066\u307f\u308b\nupdate_cols(add_cols_unnecessary=cols_unnecessary_tmp)\n\n\n###\n### 9. Cross Validation\n###\n\nprint('\\n\\n### 9. Cross Validation\\n')\n\n# Holdout\u3092\u53d6\u3063\u3066\u304a\u304f(Adversarial Validation\u306e\u7d50\u679c\u304b\u3089\u30c6\u30b9\u30c8\u3063\u307d\u3044\u3082\u306e\u3092Holdout\u306b\u3059\u308b)\npertition_holdout(rate_holdout=0.2, use_test_like_records=False, use_recent_records=True)\n\n# \u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u5b9f\u884c\ncross_validation_lgb(rate_sampling=1.0, n_folds=5, plot_feature_importance=True, print_topworst_features=False, n_top=10)\n\n\n###\n### 10. Holdout Validation\n###\n\nprint('\\n\\n### 10. Holdout Validation\\n')\n\n# Holdout\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\nholdout_validation_lgb(plot_feature_importance=False, print_topworst_features=False, print_topworst_features_by_conditions=False)\n\n\n###\n### 11. Modeling for Submission\n###\n\nprint('\\n\\n### 11. Modeling for Submission\\n')\n\n# \u5168\u30c7\u30fc\u30bf\u30e2\u30c7\u30eb\u751f\u6210\nmodeling_and_prediction_lgb(cross_validation_ensemble=True, n_folds=5)\n\n# \u4e88\u6e2c\u5024\u3092\u4fdd\u5b58\nprint('Appending model predictions to y_preds_list')\ny_preds_list_for_ensemble.append(y_pred_final)\ncvscore_list_for_ensemble.append(score_cv_avr)\nholdoutscore_list_for_ensemble.append(score_holdout)\n\n\n###\n### 12. Print Results\n###\n\nprint('\\n\\n### 12. Print Results (LightGBM(2))\\n')\n\n# \u8af8\u3005\u306e\u5024\u3092\u51fa\u529b\nprint('lgbm_params: {}'.format(lgbm_params))\nprint('number of features: {}'.format(len(get_cols_to_use())))\n\n# \u7d50\u679c\u3092\u307e\u3068\u3081\u3066\u51fa\u529b\nprint('Print scores')\nprint(' > Avr of CV: {:.5f}'.format(score_cv_avr))\nprint(' > Holdout  : {:.5f}'.format(score_holdout))\n\n# \u7dcf\u7d4c\u904e\u6642\u9593\u306e\u51fa\u529b\nprint('Calculating total elapsed time')\nelapsed_time = time.time()-notebook_begin\nelapsed_sec = int(elapsed_time%60)\nelapsed_min = int((elapsed_time-elapsed_sec)\/60)\nprint(' > total elapsed time ... {}:{}'.format(elapsed_min, str(elapsed_sec).rjust(2, '0')))\n","9f76dd1e":"\ncols_qualitative, cols_quantitative, cols_flg, cols_unnecessary = [], [], [], []\n\n###\n### 1. Importing Data\n###\n\nprint('\\n### 1. Importing Data\\n')\n\n# \u51e6\u7406\u5168\u4f53\u306e\u6642\u9593\u8a08\u6e2c\nnotebook_begin = time.time()\n\n# \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\nload_merged_df(cache=True)\nCOL_ID = 'ID'\nCOL_TARGET = 'loan_condition' \n\n\n###\n### 2. Clearning Data\n###\n\nprint('\\n\\n### 2. Clearning Data\\n')\n\n# \u6b20\u640d\u6570\u5217\u8ffd\u52a0\nprint('Adding n_nan')\ndf_train['n_nan'] = df_train.isnull().sum(axis=1)\ndf_test['n_nan'] = df_test.isnull().sum(axis=1)\n\n# \u65e5\u4ed8\u3092\u8868\u3059\u5217\u3092\u65e5\u4ed8\u578b\u3078\nobject2date('issue_d') # \u6b20\u640d\u306a\u3057\nobject2date('earliest_cr_line') # \u3042\u3068\u3067\u30c7\u30fc\u30bf\u3092\u7d5e\u308b\u306e\u3067\u6b20\u640d\u306a\u3057\nextract_recent_data(oldest_year=2014) # 2014\u5e74\u4ee5\u964d\u306b\u30c7\u30fc\u30bf\u3092\u7d5e\u308b\ncols_unnecessary.append('issue_d') # issue_d\u306f\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3068\u88ab\u308b\u90e8\u5206\u304c\u306a\u3044\u306e\u3067\u524a\u9664\n\n# \u9806\u5e8f\u5c3a\u5ea6\u3092\u8868\u3059\u5217\u306f\u3053\u3053\u3067int\u578b\u3078\nordinal2int('grade')\nordinal2int('sub_grade')\nemplength2int() # \u88dc\u5b8c\u6e08\u307f\u6b20\u640d\u3042\u308a\n\n# \u6587\u5b57\u5217\u3092\u8868\u3059\u5217\u306fstr\u306b\u5909\u63db\u3057\u3064\u3064\u3053\u3053\u3067\u6b20\u640d\u5024\u88dc\u5b8c(str\u578b\u306b\u5909\u63db) \u203b\u5217\u3068\u3057\u3066\u306fobject\u306e\u307e\u307e\nobject2str('title') # \u88dc\u5b8c\u6e08\u307f\u6b20\u640d\u3042\u308a\nobject2str('emp_title') # \u88dc\u5b8c\u6e08\u307f\u6b20\u640d\u3042\u308a\npreprocess_text('title') # \u8a18\u53f7\u524a\u9664\u306a\u3069\u6700\u4f4e\u9650\u306e\u51e6\u7406\npreprocess_words_of_text('title') # \u7d30\u304b\u3044\u5358\u8a9e\u306e\u4fee\u6b63\n\n# \u305d\u306e\u4ed6: zip-code\nzipcode2str()\n\n\n###\n### 3. EDA\n###\n\nprint('\\n\\n### 3. EDA\\n')\n\n# \u30bf\u30a4\u30d7\u5225\u306b\u7279\u5fb4\u91cf\u3092\u8aad\u307f\u8fbc\u307f\nload_cols_by_dtype()\n\n# \u7279\u5fb4\u91cf\u3054\u3068\u306b\u5206\u5e03\u3092\u307f\u308b\n#plot_histgrams(cols_quantitative)\n\n\n###\n### 4. Removing Outliers\n###\n\nprint('\\n\\n### 4. Removing Outliers\\n')\n\n# \u6307\u6570\u95a2\u6570\u7684\u306a\u3084\u3064\u3060\u3051\u5bfe\u6570\u5909\u63db\u3002\u4e00\u7dd2\u306b\u5916\u308c\u5024\u9664\u53bb\u3082\u3084\u3063\u3068\u304f\u3002\nlog_transformation('loan_amnt', remove_outlier=False)\nlog_transformation('installment', remove_outlier=False)\nlog_transformation('annual_inc', remove_outlier=True, add_flg=True) # \u5916\u308c\u5024\u30d5\u30e9\u30b0\u306f\u82e5\u5e72\u306e\u52b9\u679c\u3042\u308a\nlog_transformation('revol_bal', remove_outlier=True, add_flg=True) # \u5916\u308c\u5024\u30d5\u30e9\u30b0\u306f\u82e5\u5e72\u306e\u52b9\u679c\u3042\u308a\nlog_transformation('tot_coll_amt', remove_outlier=True, add_flg=True) # \u5916\u308c\u5024\u30d5\u30e9\u30b0\u306f\u82e5\u5e72\u306e\u52b9\u679c\u3042\u308a\nlog_transformation('tot_cur_bal', remove_outlier=True, add_flg=True) # \u5916\u308c\u5024\u30d5\u30e9\u30b0\u306f\u82e5\u5e72\u306e\u52b9\u679c\u3042\u308a\nlog_transformation('EXTER:TotalWages', remove_outlier=False)\n\n\n###\n### 5. Missing Value Completion\n###\n\nprint('\\n\\n### 5. Missing Value Completion\\n')\n\n# \u6587\u5b57\u5217\u3092\u8868\u3059\u5217\u306e\u6b20\u640d\u5024\u88dc\u5b8c(#\u3067\u88dc\u5b8c\u6e08\u307f)\nfillna_text('title', add_flg=False)\nfillna_text('emp_title', add_flg=False)\n\n# \u6570\u5024\u5217\u306e\u6b20\u640d\u5024\u88dc\u5b8c, \u6b20\u640d\u30d5\u30e9\u30b0\u4ed8\u4e0e \u203b\u57fa\u672c\u7684\u306b\u306f\u5168\u4f53\u306b\u7d1b\u308c\u3055\u305b\u308b\u3088\u3046\u306b\u88dc\u5b8c\u3057\u3066\u3001\u30d5\u30e9\u30b0\u306e\u65b9\u306b\u6b20\u640d\u60c5\u5831\u3092\u4e38\u6295\u3052\u3059\u308b\nfillna_numeric('dti', fillna_by=-999)\nfillna_numeric('mths_since_last_delinq', fillna_by=-999)\nfillna_numeric('mths_since_last_record', fillna_by=-999)\nfillna_numeric('revol_util', fillna_by=-999)\nfillna_numeric('mths_since_last_major_derog', fillna_by=-999)\nfillna_numeric('inq_last_6mths', fillna_by='median') # inq_last_6mths: \u4e2d\u592e\u5024\u3067\u88dc\u5b8c \u203b\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3060\u3051\u6b20\u640d\nfillna_numeric('EXTER:TotalWages', fillna_by=-999) \n\n\n###\n### 6. Feature Engineering (1)\n###\n\nprint('\\n\\n### 6. Feature Engineering (1)\\n')\n\n# tfidf\u3092(pca\u3057\u3066)kmeans\u3057\u305f\u30af\u30e9\u30b9\u30bf\u60c5\u5831\ntfidf_pca_kmeans_cluster('emp_title', n_dim_tfidf=500, n_dim_pca=-1, n_cluster=100)\n\n# tfidf(\u3092pca\u3057\u305f\u3082\u306e)\u306b\u5bfe\u3059\u308bLogisticRegression\u306e\u4e88\u6e2c\u5024\ntfidf_pca_logreg_prediction('emp_title', n_dim_tfidf=500, n_dim_pca=-1) # \u30c6\u30ad\u30b9\u30c8\u306e\u524d\u51e6\u7406\u306a\u3057=0.57007, \u3042\u308a=0.56939\n\n# \u5143\u306e\u5217\u306f\u3082\u3046\u4f7f\u308f\u306a\u3044\nupdate_cols(add_cols_unnecessary=['emp_title'])\n    \n# earliest_cr_line: 10\u5e74\u5358\u4f4d\u306b\u60c5\u5831\u91cf\u3092\u843d\u3068\u3057\u305f\u5f8c\u3001\u9806\u5e8f\u5c3a\u5ea6\u3068\u3057\u3066\u6570\u5024\u578b\u7279\u5fb4\u306b\u5909\u63db\u3002\nyear2decade('earliest_cr_line', remove_original=True)\nordinal2int('earliest_cr_line_decade_year')\n\n# \u7dda\u5f62\u30e2\u30c7\u30eb\u7528\u306e\u6a19\u6e96\u5316\u3055\u308c\u305f\u30c6\u30fc\u30d6\u30eb\u3092\u5143\u306b\u3001\u30ec\u30b3\u30fc\u30c9\u5168\u4f53\u306e\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u60c5\u5831\u3092\u4ed8\u4e0e\n#record_clustering_pca_tsne_kmeans(n_dim_pca=10, n_dim_tsne=2, n_cluster=100) # \u6642\u9593\u304b\u304b\u308a\u3059\u304e\u308b\u306e\u3067\u3084\u3089\u306a\u3044\nrecord_clustering_kmeans(n_cluster=100)\n\n###\n### 7. Encoding\n###\n\nprint('\\n\\n### 7. Encoding\\n')\n    \n# \u30ab\u30a6\u30f3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3068\u30bf\u30fc\u30b2\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u4e21\u65b9\u5b9f\u65bd\ncols_qualitative_all = cols_qualitative.copy()\ncount_encoding(cols_qualitative_all, remove_original=True)\ncount_encoding(cols_qualitative_all, remove_original=False)\ntarget_encoding(cols_qualitative_all, remove_original=True) # \u30ab\u30c6\u30b4\u30ea\u5909\u65707\u500b\u304c\u5bfe\u8c61\ndel cols_qualitative_all\n\n# \u30ab\u30c6\u30b4\u30ea\u5909\u6570\u306b\u52a0\u3048\u3066\u30e6\u30cb\u30fc\u30af\u6570\u304c\u5c11\u306a\u304f(\u3068\u308a\u3042\u3048\u305a\u306f\u30e6\u30cb\u30fc\u30af\u657020\u4ee5\u4e0b)\u3042\u308b\u610f\u5473\u30ab\u30c6\u30b4\u30ea\u3068\u307f\u306a\u305b\u305d\u3046\u306a\u6570\u5024\u7279\u5fb4\u3082\u30a8\u30f3\u30b3\u30fc\u30c9\ncols_quantitative_few_unique = ['acc_now_delinq', 'collections_12_mths_ex_med', \n                                'earliest_cr_line_decade_year', 'emp_length', 'grade', \n                                'inq_last_6mths', 'n_nan'] # \u6570\u5024\u5909\u65707\u500b\u304c\u5bfe\u8c61\ntarget_encoding(cols_quantitative_few_unique, remove_original=False)\n\n# \u30bf\u30fc\u30b2\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u5f8c\u306b\u6b20\u640d\u5024\u304c\u3042\u308b\u3063\u307d\u3044\uff1f\uff1f\uff1f\nfor col in cols_quantitative:\n    if 'TARGET' in col:\n        df_train[col] = df_train[col].fillna(df_train[col].median())\n        df_test[col] = df_test[col].fillna(df_test[col].median())\n\n\n###\n### 8. Feature Engineering (2)\n###\n\nprint('\\n\\n### 8. Feature Engineering (2)\\n')\n\n# \u4ea4\u4e92\u4f5c\u7528\u9805\u306e\u8ffd\u52a0: \u5358\u4e00\u7279\u5fb4\u91cf\u306e\u307f\u3067\u8a13\u7df4\u3057\u305f\u969b\u306eHoldout\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306e\u91cd\u8981\u7279\u5fb4\u91cftop10\u306e\u7dcf\u5f53\u305f\u308a\u3092\u8ffd\u52a0\ngenerate_interaction_terms(['sub_grade', 'grade', 'TXT:logreg_prediction_on_emp_title', 'loan_amnt', 'dti', \n                            'COUNT:home_ownership', 'tot_cur_bal', 'open_acc', 'installment', 'emp_length'],\n                           n_conbinations=2) \nprint(' > {} interaction features added'.format(len([col for col in cols_quantitative if (col not in cols_unnecessary) and ('INTRCTN' in col)])))\n\n# \u5dee\u5206\u9805\u306e\u8ffd\u52a0: \u5358\u4e00\u7279\u5fb4\u91cf\u306e\u307f\u3067\u8a13\u7df4\u3057\u305f\u969b\u306eHoldout\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306e\u91cd\u8981\u7279\u5fb4\u91cftop10\u306e\u7dcf\u5f53\u305f\u308a\u3092\u8ffd\u52a0\ngenerate_difference_terms(['sub_grade', 'grade', 'TXT:logreg_prediction_on_emp_title', 'loan_amnt', 'dti', \n                           'COUNT:home_ownership', 'tot_cur_bal', 'open_acc', 'installment', 'emp_length']) \nprint(' > {} difference features added'.format(len([col for col in cols_quantitative if (col not in cols_unnecessary) and ('DIFF' in col)])))\n\n# \u5272\u5408\u9805\u306e\u8ffd\u52a0: \u52b9\u304d\u305d\u3046\u306a\u5272\u308a\u7b97\u3092\u52d8\u3067\u8ffd\u52a0\ngenerate_division_terms([['EXTER:State & Local Spending', 'annual_inc'], ['loan_amnt', 'annual_inc'],\n                         ['installment', 'annual_inc'], ['tot_coll_amt', 'annual_inc'], ['tot_cur_bal', 'annual_inc'],\n                         ['loan_amnt', 'revol_bal'], ['installment', 'revol_bal'], ['tot_coll_amt', 'revol_bal'],\n                         ['tot_cur_bal', 'revol_bal'], ['loan_amnt', 'tot_cur_bal'], ['installment', 'tot_cur_bal'],\n                         ['tot_coll_amt', 'tot_cur_bal']]) \nprint(' > {} division features added'.format(len([col for col in cols_quantitative if (col not in cols_unnecessary) and ('DIV' in col)])))\n\n# \u4e0a\u8a18\u3067\u3042\u307e\u308a\u52b9\u304b\u306a\u304b\u3063\u305f\u3082\u306e\u306a\u3069\u3092\u524a\u9664\ncols_unnecessary_tmp = ['INTRCTN:COUNT:home_ownership (times) emp_length', 'INTRCTN:emp_length (times) grade', \n                        'DIFF:COUNT:home_ownership (minus) emp_length', 'DIFF:dti (minus) loan_amnt', \n                        'INTRCTN:COUNT:home_ownership (times) grade', \n                        'DIFF:TXT:logreg_prediction_on_emp_title (minus) dti', 'DIFF:grade (minus) open_acc', \n                        'DIFF:COUNT:home_ownership (minus) grade', 'DIFF:emp_length (minus) grade', \n                        'DIFF:grade (minus) sub_grade', 'INTRCTN:grade (times) sub_grade'] # 11\u500b\ncols_unnecessary_tmp += [col for col in cols_quantitative if 'DIFF' in col] # \u5dee\u5206\u9805\u5168\u90e8\u6d88\u3057\u3066\u307f\u308b\nupdate_cols(add_cols_unnecessary=cols_unnecessary_tmp)\n\n\n###\n### 9. Cross Validation\n###\n\nprint('\\n\\n### 9. Cross Validation\\n')\n\n# Holdout\u3092\u53d6\u3063\u3066\u304a\u304f\npertition_holdout(rate_holdout=0.2, use_test_like_records=False, use_recent_records=True)\n\n# \u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u5b9f\u884c\ncross_validation(clf_name='HistGradientBoosting', rate_sampling=1.0, n_folds=5)\n\n\n###\n### 10. Holdout Validation\n###\n\nprint('\\n\\n### 10. Holdout Validation\\n')\n\n# Holdout\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\nholdout_validation(clf_name='HistGradientBoosting')\n\n\n###\n### 11. Modeling for Submission\n###\n\nprint('\\n\\n### 11. Modeling for Submission\\n')\n\n# \u5168\u30c7\u30fc\u30bf\u30e2\u30c7\u30eb\u751f\u6210\nmodeling_and_prediction(clf_name='HistGradientBoosting', cross_validation_ensemble=True, n_folds=5)\n\n# \u4e88\u6e2c\u5024\u3092\u4fdd\u5b58\nprint('Appending model predictions to y_preds_list')\ny_preds_list_for_ensemble.append(y_pred_final)\ncvscore_list_for_ensemble.append(score_cv_avr)\nholdoutscore_list_for_ensemble.append(score_holdout)\n\n\n###\n### 12. Print Results\n###\n\nprint('\\n\\n### 12. Print Results (HistGradientBoosting)\\n')\n\n# \u8af8\u3005\u306e\u5024\u3092\u51fa\u529b\nprint('lgbm_params: {}'.format(lgbm_params))\nprint('number of features: {}'.format(len(get_cols_to_use())))\n\n# \u7d50\u679c\u3092\u307e\u3068\u3081\u3066\u51fa\u529b\nprint('Print scores')\nprint(' > Avr of CV: {:.5f}'.format(score_cv_avr))\nprint(' > Holdout  : {:.5f}'.format(score_holdout))\n\n# \u7dcf\u7d4c\u904e\u6642\u9593\u306e\u51fa\u529b\nprint('Calculating total elapsed time')\nelapsed_time = time.time()-notebook_begin\nelapsed_sec = int(elapsed_time%60)\nelapsed_min = int((elapsed_time-elapsed_sec)\/60)\nprint(' > total elapsed time ... {}:{}'.format(elapsed_min, str(elapsed_sec).rjust(2, '0')))\n","50427c34":"\ncols_qualitative, cols_quantitative, cols_flg, cols_unnecessary = [], [], [], []\n\n###\n### 1. Importing Data (Neural Network)\n###\n\nprint('\\n### 1. Importing Data\\n')\n\n# \u51e6\u7406\u5168\u4f53\u306e\u6642\u9593\u8a08\u6e2c\nnotebook_begin = time.time()\n\n# \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\nload_merged_df(cache=True)\nCOL_ID = 'ID'\nCOL_TARGET = 'loan_condition' \n\n\n###\n### 2. Clearning Data (Neural Network)\n###\n\nprint('\\n\\n### 2. Clearning Data\\n')\n\n# \u6b20\u640d\u6570\u5217\u8ffd\u52a0\nprint('Adding n_nan')\ndf_train['n_nan'] = df_train.isnull().sum(axis=1)\ndf_test['n_nan'] = df_test.isnull().sum(axis=1)\n\n# \u65e5\u4ed8\u3092\u8868\u3059\u5217\u3092\u65e5\u4ed8\u578b\u3078\nobject2date('issue_d') # \u6b20\u640d\u306a\u3057\nobject2date('earliest_cr_line') # \u3042\u3068\u3067\u30c7\u30fc\u30bf\u3092\u7d5e\u308b\u306e\u3067\u6b20\u640d\u306a\u3057\nextract_recent_data(oldest_year=2014) # 2014\u5e74\u4ee5\u964d\u306b\u30c7\u30fc\u30bf\u3092\u7d5e\u308b\ncols_unnecessary.append('issue_d') # issue_d\u306f\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3068\u88ab\u308b\u90e8\u5206\u304c\u306a\u3044\u306e\u3067\u524a\u9664\n\n# \u9806\u5e8f\u5c3a\u5ea6\u3092\u8868\u3059\u5217\u306f\u3053\u3053\u3067int\u578b\u3078\nordinal2int('grade')\nordinal2int('sub_grade')\nemplength2int() # \u88dc\u5b8c\u6e08\u307f\u6b20\u640d\u3042\u308a\n\n# \u6587\u5b57\u5217\u3092\u8868\u3059\u5217\u306fstr\u306b\u5909\u63db\u3057\u3064\u3064\u3053\u3053\u3067\u6b20\u640d\u5024\u88dc\u5b8c(str\u578b\u306b\u5909\u63db) \u203b\u5217\u3068\u3057\u3066\u306fobject\u306e\u307e\u307e\nobject2str('title') # \u88dc\u5b8c\u6e08\u307f\u6b20\u640d\u3042\u308a\nobject2str('emp_title') # \u88dc\u5b8c\u6e08\u307f\u6b20\u640d\u3042\u308a\npreprocess_text('title') # \u8a18\u53f7\u524a\u9664\u306a\u3069\u6700\u4f4e\u9650\u306e\u51e6\u7406\npreprocess_words_of_text('title') # \u7d30\u304b\u3044\u5358\u8a9e\u306e\u4fee\u6b63\n\n# \u305d\u306e\u4ed6: zip-code\nzipcode2str()\n\n\n###\n### 3. EDA (Neural Network)\n###\n\nprint('\\n\\n### 3. EDA\\n')\n\n# \u30bf\u30a4\u30d7\u5225\u306b\u7279\u5fb4\u91cf\u3092\u8aad\u307f\u8fbc\u307f\nload_cols_by_dtype()\n\n\n###\n### 4. Missing Value Completion\n###\n\nprint('\\n\\n### 4. Missing Value Completion\\n')\n\n# \u6587\u5b57\u5217\u3092\u8868\u3059\u5217\u306e\u6b20\u640d\u5024\u88dc\u5b8c(#\u3067\u88dc\u5b8c\u6e08\u307f)\nfillna_text('title', add_flg=False)\nfillna_text('emp_title', add_flg=False)\n\n# \u6570\u5024\u5217\u306e\u6b20\u640d\u5024\u88dc\u5b8c, \u6b20\u640d\u30d5\u30e9\u30b0\u4ed8\u4e0e \u203b\u57fa\u672c\u7684\u306b\u306f\u5168\u4f53\u306b\u7d1b\u308c\u3055\u305b\u308b\u3088\u3046\u306b\u88dc\u5b8c\u3057\u3066\u3001\u30d5\u30e9\u30b0\u306e\u65b9\u306b\u6b20\u640d\u60c5\u5831\u3092\u4e38\u6295\u3052\u3059\u308b\nfillna_numeric('dti', fillna_by=999.0) # dti: \u6700\u5927\u5024\u306e999.0\u3067\u88dc\u5b8c\nfillna_numeric('mths_since_last_delinq', fillna_by='minmax', val_min=0, val_max=85, add_flg=True) # mths_since_last_delinq: 0\u304b\u308985\u307e\u3067\u306e\u5024\u3067\u88dc\u5b8c(\u3053\u306e\u7bc4\u56f2\u306f\u30bf\u30fc\u30b2\u30c3\u30c8\u3078\u306e\u5f71\u97ff\u304c\u5e73\u5766)\nfillna_numeric('mths_since_last_record', fillna_by='random', add_flg=True) # mths_since_last_record: \u30e9\u30f3\u30c0\u30e0\u62bd\u51fa\u3067\u88dc\u5b8c\nfillna_numeric('revol_util', fillna_by='minmax', val_min=0, val_max=100, add_flg=True) # revol_util: 0\u304b\u3089100\u307e\u3067\u306e\u5024\u3067\u88dc\u5b8c(\u3053\u306e\u7bc4\u56f2\u306f\u30bf\u30fc\u30b2\u30c3\u30c8\u3078\u306e\u5f71\u97ff\u304c\u5e73\u5766)\nfillna_numeric('mths_since_last_major_derog', fillna_by='minmax', val_min=0, val_max=90, add_flg=True) # mths_since_last_major_derog: 0\u304b\u308990\u307e\u3067\u306e\u5024\u3067\u88dc\u5b8c(\u3053\u306e\u7bc4\u56f2\u306f\u30bf\u30fc\u30b2\u30c3\u30c8\u3078\u306e\u5f71\u97ff\u304c\u5e73\u5766)\nfillna_numeric('inq_last_6mths', fillna_by='median') # inq_last_6mths: \u4e2d\u592e\u5024\u3067\u88dc\u5b8c \u203b\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3060\u3051\u6b20\u640d\nfillna_numeric('EXTER:TotalWages', fillna_by='random', add_flg=True) # EXTER:TotalWages: \u30e9\u30f3\u30c0\u30e0\u62bd\u51fa\u3067\u88dc\u5b8c\n\n\n###\n### 5. Removing Outliers (Neural Network)\n###\n\nprint('\\n\\n### 5. Removing Outliers\\n')\n\n# \u6307\u6570\u95a2\u6570\u7684\u306a\u3084\u3064\u3060\u3051\u5bfe\u6570\u5909\u63db\u3002\u4e00\u7dd2\u306b\u5916\u308c\u5024\u9664\u53bb\u3082\u3084\u3063\u3068\u304f\u3002\nlog_transformation('loan_amnt', remove_outlier=False, add_flg=False)\nlog_transformation('installment', remove_outlier=False, add_flg=False)\nlog_transformation('annual_inc', remove_outlier=True, add_flg=False)\nlog_transformation('revol_bal', remove_outlier=True, add_flg=False)\nlog_transformation('tot_coll_amt', remove_outlier=True, add_flg=False)\nlog_transformation('tot_cur_bal', remove_outlier=True, add_flg=False)\nlog_transformation('EXTER:TotalWages', remove_outlier=False)\n\n    \n###\n### 6. Feature Engineering (1) (Neural Network)\n###\n\nprint('\\n\\n### 6. Feature Engineering (1)\\n')\n\n# tfidf\u3092(pca\u3057\u3066)kmeans\u3057\u305f\u30af\u30e9\u30b9\u30bf\u60c5\u5831\ntfidf_pca('emp_title', n_dim_tfidf=500, n_dim_pca=10)\n# \u5143\u306e\u5217\u306f\u3082\u3046\u4f7f\u308f\u306a\u3044\nupdate_cols(add_cols_unnecessary=['title', 'emp_title'])\n\n# earliest_cr_line: 10\u5e74\u5358\u4f4d\u306b\u60c5\u5831\u91cf\u3092\u843d\u3068\u3057\u305f\u5f8c\u3001\u9806\u5e8f\u5c3a\u5ea6\u3068\u3057\u3066\u6570\u5024\u578b\u7279\u5fb4\u306b\u5909\u63db\u3002\nyear2decade('earliest_cr_line', remove_original=True)\nordinal2int('earliest_cr_line_decade_year')\n\n\n###\n### 7. Encoding (Neural Network)\n###\n\nprint('\\n\\n### 7. Encoding\\n')\n\n# OneHot\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u5b9f\u65bd\nupdate_cols(add_cols_unnecessary=['zip_code']) # \u30ab\u30c6\u30b4\u30ea\u6570\u591a\u3059\u304e\u308b\u306e\u3067\u6392\u9664\nonehot_encoding(cols_qualitative)\n\n\n###\n### 7.5 Scaling Numerics (Neural Network)\n###\n\nprint('\\n\\n### 7.5. Scaling Numerics\\n')\n\n# RankGauss\nrank_gauss_transformation(n_split=5) # \u30e1\u30e2\u30ea\u3092\u98df\u3046\u306e\u3067n_split\u6570\u306e\u30ab\u30e9\u30e0\u3054\u3068\u306b\u5909\u63db\u3057\u3066\u3044\u304f\n\n\n###\n### 9. Cross Validation (Neural Network)\n###\n\nprint('\\n\\n### 9. Cross Validation\\n')\n\n# \u7279\u5fb4\u91cf\u6570\u306e\u78ba\u8a8d\nprint('number of features: {}'.format(len(get_cols_to_use())))\n\n# Holdout\u3092\u53d6\u3063\u3066\u304a\u304f(Adversarial Validation\u306f\u306a\u3057)\npertition_holdout(rate_holdout=0.2, use_test_like_records=False, use_recent_records=True)\n\n# \u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u5b9f\u884c\ncross_validation(clf_name='NeuralNetwork', rate_sampling=1.0)\ngc.collect() \n\n\n###\n### 10. Holdout Validation\n###\n\nprint('\\n\\n### 10. Holdout Validation\\n')\n\n# Holdout\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\nholdout_validation(clf_name='NeuralNetwork')\ngc.collect() \n\n\n###\n### 11. Modeling for Submission\n###\n\nprint('\\n\\n### 11. Modeling for Submission\\n')\n\n# \u5168\u30c7\u30fc\u30bf\u30e2\u30c7\u30eb\u751f\u6210\nmodeling_and_prediction(clf_name='NeuralNetwork', cross_validation_ensemble=True, n_folds=3)\ngc.collect() \n\n# \u4e88\u6e2c\u5024\u3092\u4fdd\u5b58\nprint('Appending model predictions to y_preds_list')\ny_preds_list_for_ensemble.append(y_pred_final)\ncvscore_list_for_ensemble.append(score_cv_avr)\nholdoutscore_list_for_ensemble.append(score_holdout)\n\n\n###\n### 12. Print Results\n###\n\nprint('\\n\\n### 12. Print Results (NeuralNetwork)\\n')\n\n# \u8af8\u3005\u306e\u5024\u3092\u51fa\u529b\nprint('number of features: {}'.format(len(get_cols_to_use())))\n\n# \u7d50\u679c\u3092\u307e\u3068\u3081\u3066\u51fa\u529b\nprint('Print scores')\nprint(' > Avr of CV: {:.5f}'.format(score_cv_avr))\nprint(' > Holdout  : {:.5f}'.format(score_holdout))\n\n# \u7dcf\u7d4c\u904e\u6642\u9593\u306e\u51fa\u529b\nprint('Calculating total elapsed time')\nelapsed_time = time.time()-notebook_begin\nelapsed_sec = int(elapsed_time%60)\nelapsed_min = int((elapsed_time-elapsed_sec)\/60)\nprint(' > total elapsed time ... {}:{}'.format(elapsed_min, str(elapsed_sec).rjust(2, '0')))\n","eb617b8b":"\nif False:\n\n    cols_qualitative, cols_quantitative, cols_flg, cols_unnecessary = [], [], [], []\n\n    ###\n    ### 1. Importing Data\n    ###\n\n    print('\\n### 1. Importing Data\\n')\n\n    # \u51e6\u7406\u5168\u4f53\u306e\u6642\u9593\u8a08\u6e2c\n    notebook_begin = time.time()\n\n    # \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\n    load_merged_df(cache=True)\n    COL_ID = 'ID'\n    COL_TARGET = 'loan_condition' \n\n\n    ###\n    ### 2. Clearning Data\n    ###\n\n    print('\\n\\n### 2. Clearning Data\\n')\n\n    # \u6b20\u640d\u6570\u5217\u8ffd\u52a0\n    print('Adding n_nan')\n    df_train['n_nan'] = df_train.isnull().sum(axis=1)\n    df_test['n_nan'] = df_test.isnull().sum(axis=1)\n\n    # \u65e5\u4ed8\u3092\u8868\u3059\u5217\u3092\u65e5\u4ed8\u578b\u3078\n    object2date('issue_d') # \u6b20\u640d\u306a\u3057\n    object2date('earliest_cr_line') # \u3042\u3068\u3067\u30c7\u30fc\u30bf\u3092\u7d5e\u308b\u306e\u3067\u6b20\u640d\u306a\u3057\n    extract_recent_data(oldest_year=2014) # 2014\u5e74\u4ee5\u964d\u306b\u30c7\u30fc\u30bf\u3092\u7d5e\u308b\n    cols_unnecessary.append('issue_d') # issue_d\u306f\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3068\u88ab\u308b\u90e8\u5206\u304c\u306a\u3044\u306e\u3067\u524a\u9664\n\n    # \u9806\u5e8f\u5c3a\u5ea6\u3092\u8868\u3059\u5217\u306f\u3053\u3053\u3067int\u578b\u3078\n    ordinal2int('grade')\n    ordinal2int('sub_grade')\n    emplength2int() # \u88dc\u5b8c\u6e08\u307f\u6b20\u640d\u3042\u308a\n\n    # \u6587\u5b57\u5217\u3092\u8868\u3059\u5217\u306fstr\u306b\u5909\u63db\u3057\u3064\u3064\u3053\u3053\u3067\u6b20\u640d\u5024\u88dc\u5b8c(str\u578b\u306b\u5909\u63db) \u203b\u5217\u3068\u3057\u3066\u306fobject\u306e\u307e\u307e\n    object2str('title') # \u88dc\u5b8c\u6e08\u307f\u6b20\u640d\u3042\u308a\n    object2str('emp_title') # \u88dc\u5b8c\u6e08\u307f\u6b20\u640d\u3042\u308a\n    preprocess_text('title') # \u8a18\u53f7\u524a\u9664\u306a\u3069\u6700\u4f4e\u9650\u306e\u51e6\u7406\n    preprocess_words_of_text('title') # \u7d30\u304b\u3044\u5358\u8a9e\u306e\u4fee\u6b63\n\n    # \u305d\u306e\u4ed6: zip-code\n    zipcode2str()\n\n\n    ###\n    ### 3. EDA\n    ###\n\n    print('\\n\\n### 3. EDA\\n')\n\n    # \u30bf\u30a4\u30d7\u5225\u306b\u7279\u5fb4\u91cf\u3092\u8aad\u307f\u8fbc\u307f\n    load_cols_by_dtype()\n\n    # \u7279\u5fb4\u91cf\u3054\u3068\u306b\u5206\u5e03\u3092\u307f\u308b\n    #plot_histgrams(cols_quantitative)\n\n\n    ###\n    ### 4. Removing Outliers\n    ###\n\n    print('\\n\\n### 4. Removing Outliers\\n')\n\n    # \u6307\u6570\u95a2\u6570\u7684\u306a\u3084\u3064\u3060\u3051\u5bfe\u6570\u5909\u63db\u3002\u4e00\u7dd2\u306b\u5916\u308c\u5024\u9664\u53bb\u3082\u3084\u3063\u3068\u304f\u3002\n    log_transformation('loan_amnt', remove_outlier=False)\n    log_transformation('installment', remove_outlier=False)\n    log_transformation('annual_inc', remove_outlier=True, add_flg=True) # \u5916\u308c\u5024\u30d5\u30e9\u30b0\u306f\u82e5\u5e72\u306e\u52b9\u679c\u3042\u308a\n    log_transformation('revol_bal', remove_outlier=True, add_flg=True) # \u5916\u308c\u5024\u30d5\u30e9\u30b0\u306f\u82e5\u5e72\u306e\u52b9\u679c\u3042\u308a\n    log_transformation('tot_coll_amt', remove_outlier=True, add_flg=True) # \u5916\u308c\u5024\u30d5\u30e9\u30b0\u306f\u82e5\u5e72\u306e\u52b9\u679c\u3042\u308a\n    log_transformation('tot_cur_bal', remove_outlier=True, add_flg=True) # \u5916\u308c\u5024\u30d5\u30e9\u30b0\u306f\u82e5\u5e72\u306e\u52b9\u679c\u3042\u308a\n    log_transformation('EXTER:TotalWages', remove_outlier=False)\n\n\n    ###\n    ### 5. Missing Value Completion\n    ###\n\n    print('\\n\\n### 5. Missing Value Completion\\n')\n\n    # \u6587\u5b57\u5217\u3092\u8868\u3059\u5217\u306e\u6b20\u640d\u5024\u88dc\u5b8c(#\u3067\u88dc\u5b8c\u6e08\u307f)\n    fillna_text('title', add_flg=False)\n    fillna_text('emp_title', add_flg=False)\n\n    # \u6570\u5024\u5217\u306e\u6b20\u640d\u5024\u88dc\u5b8c, \u6b20\u640d\u30d5\u30e9\u30b0\u4ed8\u4e0e \u203b\u57fa\u672c\u7684\u306b\u306f\u5168\u4f53\u306b\u7d1b\u308c\u3055\u305b\u308b\u3088\u3046\u306b\u88dc\u5b8c\u3057\u3066\u3001\u30d5\u30e9\u30b0\u306e\u65b9\u306b\u6b20\u640d\u60c5\u5831\u3092\u4e38\u6295\u3052\u3059\u308b\n    fillna_numeric('dti', fillna_by=-999)\n    fillna_numeric('mths_since_last_delinq', fillna_by=-999)\n    fillna_numeric('mths_since_last_record', fillna_by=-999)\n    fillna_numeric('revol_util', fillna_by=-999)\n    fillna_numeric('mths_since_last_major_derog', fillna_by=-999)\n    fillna_numeric('inq_last_6mths', fillna_by='median') # inq_last_6mths: \u4e2d\u592e\u5024\u3067\u88dc\u5b8c \u203b\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3060\u3051\u6b20\u640d\n    fillna_numeric('EXTER:TotalWages', fillna_by=-999) \n\n\n    ###\n    ### 6. Feature Engineering (1)\n    ###\n\n    print('\\n\\n### 6. Feature Engineering (1)\\n')\n\n    # tfidf\u3092(pca\u3057\u3066)kmeans\u3057\u305f\u30af\u30e9\u30b9\u30bf\u60c5\u5831\n    tfidf_pca_kmeans_cluster('emp_title', n_dim_tfidf=500, n_dim_pca=-1, n_cluster=100)\n\n    # tfidf(\u3092pca\u3057\u305f\u3082\u306e)\u306b\u5bfe\u3059\u308bLogisticRegression\u306e\u4e88\u6e2c\u5024\n    tfidf_pca_logreg_prediction('emp_title', n_dim_tfidf=500, n_dim_pca=-1) # \u30c6\u30ad\u30b9\u30c8\u306e\u524d\u51e6\u7406\u306a\u3057=0.57007, \u3042\u308a=0.56939\n\n    # \u5143\u306e\u5217\u306f\u3082\u3046\u4f7f\u308f\u306a\u3044\n    update_cols(add_cols_unnecessary=['emp_title'])\n\n    # earliest_cr_line: 10\u5e74\u5358\u4f4d\u306b\u60c5\u5831\u91cf\u3092\u843d\u3068\u3057\u305f\u5f8c\u3001\u9806\u5e8f\u5c3a\u5ea6\u3068\u3057\u3066\u6570\u5024\u578b\u7279\u5fb4\u306b\u5909\u63db\u3002\n    year2decade('earliest_cr_line', remove_original=True)\n    ordinal2int('earliest_cr_line_decade_year')\n\n    # \u7dda\u5f62\u30e2\u30c7\u30eb\u7528\u306e\u6a19\u6e96\u5316\u3055\u308c\u305f\u30c6\u30fc\u30d6\u30eb\u3092\u5143\u306b\u3001\u30ec\u30b3\u30fc\u30c9\u5168\u4f53\u306e\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u60c5\u5831\u3092\u4ed8\u4e0e\n    #record_clustering_pca_tsne_kmeans(n_dim_pca=10, n_dim_tsne=2, n_cluster=100) # \u6642\u9593\u304b\u304b\u308a\u3059\u304e\u308b\u306e\u3067\u3084\u3089\u306a\u3044\n    record_clustering_kmeans(n_cluster=100)\n\n    ###\n    ### 7. Encoding\n    ###\n\n    print('\\n\\n### 7. Encoding\\n')\n\n    # \u30ab\u30a6\u30f3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3068\u30bf\u30fc\u30b2\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u4e21\u65b9\u5b9f\u65bd\n    cols_qualitative_all = cols_qualitative.copy()\n    count_encoding(cols_qualitative_all, remove_original=True)\n    count_encoding(cols_qualitative_all, remove_original=False)\n    target_encoding(cols_qualitative_all, remove_original=True) # \u30ab\u30c6\u30b4\u30ea\u5909\u65707\u500b\u304c\u5bfe\u8c61\n    del cols_qualitative_all\n\n    # \u30ab\u30c6\u30b4\u30ea\u5909\u6570\u306b\u52a0\u3048\u3066\u30e6\u30cb\u30fc\u30af\u6570\u304c\u5c11\u306a\u304f(\u3068\u308a\u3042\u3048\u305a\u306f\u30e6\u30cb\u30fc\u30af\u657020\u4ee5\u4e0b)\u3042\u308b\u610f\u5473\u30ab\u30c6\u30b4\u30ea\u3068\u307f\u306a\u305b\u305d\u3046\u306a\u6570\u5024\u7279\u5fb4\u3082\u30a8\u30f3\u30b3\u30fc\u30c9\n    cols_quantitative_few_unique = ['acc_now_delinq', 'collections_12_mths_ex_med', \n                                    'earliest_cr_line_decade_year', 'emp_length', 'grade', \n                                    'inq_last_6mths', 'n_nan'] # \u6570\u5024\u5909\u65707\u500b\u304c\u5bfe\u8c61\n    target_encoding(cols_quantitative_few_unique, remove_original=False)\n\n    # \u30bf\u30fc\u30b2\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u5f8c\u306b\u6b20\u640d\u5024\u304c\u3042\u308b\u3063\u307d\u3044\uff1f\uff1f\uff1f\n    for col in cols_quantitative:\n        if 'TARGET' in col:\n            df_train[col] = df_train[col].fillna(df_train[col].median())\n            df_test[col] = df_test[col].fillna(df_test[col].median())\n\n\n    ###\n    ### 8. Feature Engineering (2)\n    ###\n\n    print('\\n\\n### 8. Feature Engineering (2)\\n')\n\n    # \u4ea4\u4e92\u4f5c\u7528\u9805\u306e\u8ffd\u52a0: \u5358\u4e00\u7279\u5fb4\u91cf\u306e\u307f\u3067\u8a13\u7df4\u3057\u305f\u969b\u306eHoldout\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306e\u91cd\u8981\u7279\u5fb4\u91cftop10\u306e\u7dcf\u5f53\u305f\u308a\u3092\u8ffd\u52a0\n    generate_interaction_terms(['sub_grade', 'grade', 'TXT:logreg_prediction_on_emp_title', 'loan_amnt', 'dti', \n                                'COUNT:home_ownership', 'tot_cur_bal', 'open_acc', 'installment', 'emp_length'],\n                               n_conbinations=2) \n    print(' > {} interaction features added'.format(len([col for col in cols_quantitative if (col not in cols_unnecessary) and ('INTRCTN' in col)])))\n\n    # \u5dee\u5206\u9805\u306e\u8ffd\u52a0: \u5358\u4e00\u7279\u5fb4\u91cf\u306e\u307f\u3067\u8a13\u7df4\u3057\u305f\u969b\u306eHoldout\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306e\u91cd\u8981\u7279\u5fb4\u91cftop10\u306e\u7dcf\u5f53\u305f\u308a\u3092\u8ffd\u52a0\n    generate_difference_terms(['sub_grade', 'grade', 'TXT:logreg_prediction_on_emp_title', 'loan_amnt', 'dti', \n                               'COUNT:home_ownership', 'tot_cur_bal', 'open_acc', 'installment', 'emp_length']) \n    print(' > {} difference features added'.format(len([col for col in cols_quantitative if (col not in cols_unnecessary) and ('DIFF' in col)])))\n\n    # \u5272\u5408\u9805\u306e\u8ffd\u52a0: \u52b9\u304d\u305d\u3046\u306a\u5272\u308a\u7b97\u3092\u52d8\u3067\u8ffd\u52a0\n    generate_division_terms([['EXTER:State & Local Spending', 'annual_inc'], ['loan_amnt', 'annual_inc'],\n                             ['installment', 'annual_inc'], ['tot_coll_amt', 'annual_inc'], ['tot_cur_bal', 'annual_inc'],\n                             ['loan_amnt', 'revol_bal'], ['installment', 'revol_bal'], ['tot_coll_amt', 'revol_bal'],\n                             ['tot_cur_bal', 'revol_bal'], ['loan_amnt', 'tot_cur_bal'], ['installment', 'tot_cur_bal'],\n                             ['tot_coll_amt', 'tot_cur_bal']]) \n    print(' > {} division features added'.format(len([col for col in cols_quantitative if (col not in cols_unnecessary) and ('DIV' in col)])))\n\n    # \u4e0a\u8a18\u3067\u3042\u307e\u308a\u52b9\u304b\u306a\u304b\u3063\u305f\u3082\u306e\u306a\u3069\u3092\u524a\u9664\n    cols_unnecessary_tmp = ['INTRCTN:COUNT:home_ownership (times) emp_length', 'INTRCTN:emp_length (times) grade', \n                            'DIFF:COUNT:home_ownership (minus) emp_length', 'DIFF:dti (minus) loan_amnt', \n                            'INTRCTN:COUNT:home_ownership (times) grade', \n                            'DIFF:TXT:logreg_prediction_on_emp_title (minus) dti', 'DIFF:grade (minus) open_acc', \n                            'DIFF:COUNT:home_ownership (minus) grade', 'DIFF:emp_length (minus) grade', \n                            'DIFF:grade (minus) sub_grade', 'INTRCTN:grade (times) sub_grade'] # 11\u500b\n    cols_unnecessary_tmp += [col for col in cols_quantitative if 'DIFF' in col] # \u5dee\u5206\u9805\u5168\u90e8\u6d88\u3057\u3066\u307f\u308b\n    update_cols(add_cols_unnecessary=cols_unnecessary_tmp)\n\n\n    ###\n    ### 9. Cross Validation\n    ###\n\n    print('\\n\\n### 9. Cross Validation\\n')\n\n    # Holdout\u3092\u53d6\u3063\u3066\u304a\u304f(Adversarial Validation\u306e\u7d50\u679c\u304b\u3089\u30c6\u30b9\u30c8\u3063\u307d\u3044\u3082\u306e\u3092Holdout\u306b\u3059\u308b)\n    pertition_holdout(rate_holdout=0.2, use_test_like_records=False)\n\n    # \u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u5b9f\u884c\n    cross_validation_xgb(rate_sampling=1.0, n_folds=5)\n\n\n    ###\n    ### 10. Holdout Validation\n    ###\n\n    print('\\n\\n### 10. Holdout Validation\\n')\n\n    # Holdout\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\n    holdout_validation_xgb()\n\n\n    ###\n    ### 11. Modeling for Submission\n    ###\n\n    print('\\n\\n### 11. Modeling for Submission\\n')\n\n    # \u5168\u30c7\u30fc\u30bf\u30e2\u30c7\u30eb\u751f\u6210\n    modeling_and_prediction_xgb(cross_validation_ensemble=True, n_folds=5)\n\n    # \u4e88\u6e2c\u5024\u3092\u4fdd\u5b58\n    print('Appending model predictions to y_preds_list')\n    y_preds_list_for_ensemble.append(y_pred_final)\n    cvscore_list_for_ensemble.append(score_cv_avr)\n\n\n    ###\n    ### 12. Print Results\n    ###\n\n    print('\\n\\n### 12. Print Results (XGBoost)\\n')\n\n    # \u8af8\u3005\u306e\u5024\u3092\u51fa\u529b\n    print('lgbm_params: {}'.format(lgbm_params))\n    print('number of features: {}'.format(len(get_cols_to_use())))\n\n    # \u7d50\u679c\u3092\u307e\u3068\u3081\u3066\u51fa\u529b\n    print('Print scores')\n    print(' > Avr of CV: {:.5f}'.format(score_cv_avr))\n    print(' > Holdout  : {:.5f}'.format(score_holdout))\n\n    # \u7dcf\u7d4c\u904e\u6642\u9593\u306e\u51fa\u529b\n    print('Calculating total elapsed time')\n    elapsed_time = time.time()-notebook_begin\n    elapsed_sec = int(elapsed_time%60)\n    elapsed_min = int((elapsed_time-elapsed_sec)\/60)\n    print(' > total elapsed time ... {}:{}'.format(elapsed_min, str(elapsed_sec).rjust(2, '0')))\n","ff9066d3":"model_names = ['LogisticRegression', 'LightGBM(1)', 'LightGBM(2)', 'HistGradientBoosting', 'NeuralNetwork']#, 'XGBoost']\ndf_results = pd.DataFrame({'Model': model_names, 'Avr Score of CV': cvscore_list_for_ensemble, 'Holdout Score': holdoutscore_list_for_ensemble})\ndf_results","d6a3b700":"### \u63d0\u51fa\u7528csv\u51fa\u529b\nprint('Saving model predictions')\n\n# \u5e73\u5747\nfname = 'submission_5models_avr_ensemble.csv'\nsubmission = pd.read_csv('{}\/sample_submission.csv'.format(DIR), index_col=0)\nsubmission.loan_condition = np.mean(y_preds_list_for_ensemble, axis=0)\nsubmission.to_csv(fname)\nprint(' > saved: {}'.format(fname))\n\n# CV\u306e\u7cbe\u5ea6\u3092\u898b\u3066\u91cd\u307f\u4ed8\u304d\u5e73\u5747(CV\u30b9\u30b3\u30a2\u306e\u5c0f\u6570\u70b9\u4ee5\u4e0b\u7b2c2\u4f4d\u7b2c3\u4f4d\u306e2\u6841\u3092\u91cd\u307f\u306b\u3057\u3066\u307f\u308b)\nweights = [3.9, 11.4, 11.0, 11.0, 5.6]\nfname = 'submission_5models_weighted_avr_ensemble.csv'\nsubmission = pd.read_csv('{}\/sample_submission.csv'.format(DIR), index_col=0)\nsubmission.loan_condition = np.average(y_preds_list_for_ensemble, weights=weights, axis=0)\nsubmission.to_csv(fname)\nprint(' > saved: {}'.format(fname))\n\n# \u7cbe\u5ea6\u306e\u826f\u3044LightGBM*2\u3068HistGradientBoosting\u306e\u5e73\u5747\nbest_3preds_plusNN = [y_preds_list_for_ensemble[1], y_preds_list_for_ensemble[2], \n                      y_preds_list_for_ensemble[3], y_preds_list_for_ensemble[4]]\nfname = 'submission_best3models_plusNN_avr_ensemble.csv'\nsubmission = pd.read_csv('{}\/sample_submission.csv'.format(DIR), index_col=0)\nsubmission.loan_condition = np.mean(best_3preds_plusNN, axis=0)\nsubmission.to_csv(fname)\nprint(' > saved: {}'.format(fname))\n\n# LightGBM*2\u306e\u5e73\u5747\nbest_2preds = [y_preds_list_for_ensemble[1], y_preds_list_for_ensemble[2]]\nfname = 'submission_2LightGBMs_avr_ensemble.csv'\nsubmission = pd.read_csv('{}\/sample_submission.csv'.format(DIR), index_col=0)\nsubmission.loan_condition = np.mean(best_2preds, axis=0)\nsubmission.to_csv(fname)\nprint(' > saved: {}'.format(fname))","160b2952":"### \u305d\u306e\u4ed6(zipcode)","7b4f0db5":"### RankGauss","c6acce9a":"### Target Encoding","8b45a878":"### Modeling for Submission","83a2bae8":"# Removing Outliers, Transformation","0875bb8e":"# Importing Data","c49760aa":"# Modeling","b2643de5":"# Model 3. LightGBM (max bin=127, learning rate=0.1, and defaults)","4399a23d":"### \u5bfe\u6570\u5909\u63db","58e7e17d":"### \u5024\u3092\u4e00\u5b9a\u9593\u9694\u306b\u5206\u5272","ad5a9957":"### \u6587\u5b57\u5217","cebb87af":"### \u9806\u5e8f\u5c3a\u5ea6\u5217","7b32bc6a":"# (\u5374\u4e0b) Model 6. XGBoost","da67d26e":"# Missing Value Completion","51271084":"### Cross Validation","867f5e1f":"### \u30bf\u30a4\u30d7\u5225\u306b\u8aad\u307f\u8fbc\u307f","56b6e0bb":"# Ensemble and Save Submission","fe2ff4ef":"### \u76f8\u95a2\u306e\u9ad8\u3044\u7279\u5fb4\u91cf\u3092\u30d7\u30ed\u30c3\u30c8","1fe40785":"# Cleaning Data","c7462311":"# Encoding","c0836d41":"### Count Encoding","6fee249a":"# Model 2. LightGBM (learning rate=0.05, and defaults)","f01f88af":"### OneHot Encoding","9ec09c36":"# Model 1. Logistic Regression","7b211553":"### Holdout\u3092\u53d6\u3063\u3066\u304a\u304f","e6ab8168":"### \u6587\u5b57\u5217","a142c858":"---","4168c43e":"# Model 4. HistGradientBoosting","1694c155":"# Feature Engineering","1dfc14a3":"### \u30ec\u30b3\u30fc\u30c9\u5168\u4f53\u3092\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0","8649639d":"# EDA","ec88a7a4":"### Holdout Validation","38979f80":"### \u6570\u5024\u5217","b6c36cef":"### \u4ea4\u4e92\u4f5c\u7528\u9805\u306e\u8ffd\u52a0","e182f0f4":"### \u7279\u5fb4\u91cf\u30ea\u30b9\u30c8\u3092\u66f4\u65b0\u3059\u308b\u3068\u304d\u306f\u3053\u3044\u3064\u3067\u3084\u308b","c8163255":"# Missing Value Completion","c399f6ed":"# Importing Libraries","b626d4ee":"### \u65e5\u4ed8\u5217","4e20b6b5":"### \u65e5\u4ed8\u7279\u5fb4\u309210\u5e74\u5358\u4f4d\u306b\u5909\u63db","f8f396b4":"### Adversarial Validation","de3103e8":"### \u30c6\u30ad\u30b9\u30c8\u306e\u7279\u5fb4\u91cf\u5316","e6e74d75":"# Model 5. NeuralNetwork","20d95310":"### \u7279\u5fb4\u91cf\u3054\u3068\u306b\u5206\u5e03\u3092\u8868\u793a"}}