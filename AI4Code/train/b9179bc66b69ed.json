{"cell_type":{"69608876":"code","f8c23a76":"code","3f95dd35":"code","fc40ecce":"code","b2d3cd23":"code","ac6430cd":"code","26205a28":"code","6d9097c8":"code","7965eba5":"code","d810d105":"code","1a59bcba":"code","4a356e9c":"code","be730350":"code","3c92d25a":"code","e174483e":"markdown","0160d51a":"markdown","ab162cce":"markdown","5793e401":"markdown","174e7fbd":"markdown","7360314e":"markdown","b37efdec":"markdown","a2685c18":"markdown","2fc48e54":"markdown","dae35260":"markdown","36017001":"markdown","7e5ac40d":"markdown"},"source":{"69608876":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport altair as alt\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f8c23a76":"data = pd.read_csv('..\/input\/bank-marketing-campaign-subscriptions\/Bank_Campaign.csv', sep=\";\", na_values=['unknown'])","3f95dd35":"print(f\"There are {data.shape[0]} rows and {data.shape[1]} columns.\")\nprint(f\"There are {data.isna().sum().sum()} missing values.\")","fc40ecce":"(data.isna().sum()[data.isna().sum() > 0] \/ data.shape[0] * 100).apply(lambda x: f\"{round(x, 2)}%\")","b2d3cd23":"TARGET = 'subscribed'\n\n(data[TARGET].value_counts() \/ data.shape[0] * 100).apply(lambda x: f\"{round(x, 1)}%\")","ac6430cd":"#\u00a0Get numerical columns\nnumericals = data.dtypes[data.dtypes != 'O'].index.tolist()","26205a28":"fig, ax = plt.subplots(3, 3, figsize=(22, 16))\nsns.histplot(data['age'], ax=ax[0][0])\nsns.histplot(data['duration'], ax=ax[0][1])\nsns.histplot(data['campaign'], ax=ax[0][2])\nsns.histplot(data['pdays'], ax=ax[1][0])\nsns.histplot(data['previous'], ax=ax[1][1])\nsns.histplot(data['emp.var.rate'], ax=ax[1][2])\nsns.histplot(data['cons.price.idx'], ax=ax[2][0])\nsns.histplot(data['euribor3m'], ax=ax[2][1])\nsns.histplot(data['nr.employed'], ax=ax[2][2])\nplt.show()","6d9097c8":"#\u00a0Get categorical variables\ncategoricals = data.dtypes[data.dtypes == 'O']","7965eba5":"fig, ax = plt.subplots(4, 3, figsize=(22, 30))\nsns.countplot(x=data['job'], ax=ax[0][0])\nax[0][0].tick_params(labelrotation=30)\nsns.countplot(x=data['marital'], ax=ax[0][1])\nsns.countplot(x=data['education'], ax=ax[0][2])\nax[0][2].tick_params(labelrotation=75)\nsns.countplot(x=data['default'], ax=ax[1][0])\nsns.countplot(x=data['housing'], ax=ax[1][1])\nsns.countplot(x=data['loan'], ax=ax[1][2])\nsns.countplot(x=data['contact'], ax=ax[2][0])\nsns.countplot(x=data['month'], ax=ax[2][1])\nsns.countplot(x=data['day_of_week'], ax=ax[2][2])\nsns.countplot(x=data['poutcome'], ax=ax[3][0])\nsns.countplot(x=data['subscribed'], ax=ax[3][1])\nplt.show()","d810d105":"fig, ax = plt.subplots(2, 2, figsize=(20, 10))\nsns.scatterplot(data=data, x='age', y='duration', hue='subscribed', ax=ax[0][0])\nsns.scatterplot(data=data, x='age', y='duration', hue='job', ax=ax[0][1])\nsns.scatterplot(data=data, x='age', y='duration', hue='education', ax=ax[1][0])\nsns.scatterplot(data=data, x='age', y='duration', hue='housing', ax=ax[1][1])\nplt.show()","1a59bcba":"types = {\n    \"job\":\"category\",\n    \"marital\":\"category\",\n    \"education\":\"category\",\n    \"default\":\"category\",\n    \"housing\":\"category\",\n    \"loan\":\"category\",\n    \"contact\":\"category\",\n    \"month\":\"category\",\n    \"day_of_week\":\"category\",\n    \"poutcome\":\"category\",\n}\n\ndata = data.astype(types)\ndata[TARGET] = data[TARGET].replace({\"yes\":1, \"no\":0})\n\nfor col in data.dtypes[data.dtypes == 'category'].index:\n    data[col] = data[col].fillna(data[col].mode()[0])","4a356e9c":"from sklearn.pipeline import make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import OneHotEncoder\n\n#\u00a0Separate target & features\nX = data.drop(columns=TARGET)\nX = X.fillna(X.median())\ny = data[TARGET]\n\n# One hot encode\nencoder = OneHotEncoder(drop='first')\nX = encoder.fit_transform(X)\n\n# Oversampling the data\noversample = SMOTE()\nX, y = oversample.fit_resample(X, y)","be730350":"from sklearn.model_selection import train_test_split\n\n#\u00a0Splitting\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","3c92d25a":"from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodels = [\n    LogisticRegression(max_iter=5000),\n    RandomForestClassifier(),\n    XGBClassifier(eval_metric='error', use_label_encoder=False)\n]\n\nSPACE = 30\n\nfor model in models:\n    #\u00a0Fit\n    model.fit(X_train, y_train)\n    \n    #\u00a0Scores\n    train_roc = roc_auc_score(y_train, model.predict(X_train))\n    test_roc  = roc_auc_score(y_test, model.predict(X_test))\n    \n    train_accuracy = accuracy_score(y_train, model.predict(X_train))\n    test_accuracy  = accuracy_score(y_test, model.predict(X_test))\n    \n    train_precision = precision_score(y_train, model.predict(X_train))\n    test_precision  = precision_score(y_test, model.predict(X_test))\n    \n    train_recall = recall_score(y_train, model.predict(X_train))\n    test_recall  = recall_score(y_test, model.predict(X_test))\n    \n    # Display\n    print(f\"{type(model).__name__.rjust(SPACE)}:\")\n    print(f'{\"ROC AUC\".rjust(SPACE)}: TRAIN({str(round(train_roc, 3))}) & TEST({str(round(test_roc, 3))})')\n    print(f'{\"Accuracy\".rjust(SPACE)}: TRAIN({str(round(train_accuracy, 3))}) & TEST({str(round(test_accuracy, 3))})')\n    print(f'{\"Precision\".rjust(SPACE)}: TRAIN({str(round(train_precision, 3))}) & TEST({str(round(train_precision, 3))})')\n    print(f'{\"Recall\".rjust(SPACE)}: TRAIN({str(round(train_recall, 3))}) & TEST({str(round(test_recall, 3))})')\n    print()","e174483e":"## Some quick check","0160d51a":"## Conclusion\n\n**Accuracy**\n: \"*Informally, accuracy is the fraction of predictions our model got right*\", among the models the random forest is the best model based on the accuracy score.\n\n**Precision**\n: \"*The precision is intuitively the ability of the classifier not to label as positive a sample that is negative*\", random forest is also the winner concerning this metric!\n\n**Recall**\n: \"*The recall is intuitively the ability of the classifier to find all the positive samples*\", random forest again!\n\nSo **random forest** is the big winner I guess!","ab162cce":"# Bank Marketing Campaign Subscriptions\n*A direct marketing campaign of a Portuguese banking institution to their clients*\n\n<div>\n    <img src=\"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/1153536\/1933923\/46aa21f35d5be16b1076fa0f75fded96\/dataset-cover.jpg?t=2021-02-12-05-59-55\">\n<\/div>\n\n<br>\n\n> **Context :** *The dataset contains information about marketing campaigns that were conducted via phone calls from a Portuguese banking institution to their clients. Purpose of these campaigns is to prompt their clients to subscribe for a specific financial product of the bank (term deposit). After each call was conducted, the client had to inform the institution about their intention of either subscribing to the product (indicating a successful campaign) or not (unsucessful campaign).\nThe final output of this survey will be a binary result indicating if the client subscribed ('yes') to the product or not ('no').*\n\nThe goals of this notebook are:\n- Create a good model for that case of imbalanced classes\n- Try some techniques to overcome imbalanced classes","5793e401":"## Processing","174e7fbd":"## Load data","7360314e":"## Numerical values","b37efdec":"## Models","a2685c18":"# EDA","2fc48e54":"## Basic insights","dae35260":"# Setup","36017001":"# Define types","7e5ac40d":"# Model"}}