{"cell_type":{"e0486cf4":"code","7e8204f2":"code","92f7ccc2":"code","f00fc0fd":"code","ff5b60d5":"code","0573047a":"code","fb7079ac":"code","8a8cf59b":"code","bdfc05e2":"code","ffbec935":"code","9897f859":"code","bb696755":"code","29e5d675":"code","7651ac57":"code","b43ae77e":"code","5a99249c":"code","c62e7c61":"code","c9907875":"code","5d40c1fd":"code","74d3c9a7":"code","d338ef1f":"markdown","3a0e5446":"markdown","e58f698d":"markdown","ff34480d":"markdown","8cca60d5":"markdown","6cd633cc":"markdown","8738720c":"markdown","45961589":"markdown","df0cbc07":"markdown","fd883ec5":"markdown","e11bf184":"markdown","c7fd80dc":"markdown","64e98f08":"markdown","014bc8d3":"markdown","8587bc6d":"markdown","23a1893f":"markdown","7ee8f986":"markdown","58ae2f3a":"markdown"},"source":{"e0486cf4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification,Trainer, TrainingArguments\nimport torch\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n        \n\n\n#\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7e8204f2":"if \"output\" not in os.listdir():\n    os.mkdir(\".\/output\")\nif \"logs\" not in os.listdir():\n    os.mkdir(\".\/logs\")\nif \"model\" not in os.listdir():\n    os.mkdir(\".\/model\") \n  ","92f7ccc2":"!pip install wandb","f00fc0fd":"import wandb\nwandb.login(key = \"a0f553a701b1c86e18b067324c61cdf1adcd410b\") ## Use your api key here","ff5b60d5":"data = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/train.csv\")\ndata.head()","0573047a":"cols = [\"excerpt\",\"target\"]\nmsk = np.random.rand(len(data)) < 0.8\n\ntrain_data = data[cols][msk]\nval_data = data[cols][~msk]","fb7079ac":"tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\ntrain_encodings = tokenizer(list(train_data[\"excerpt\"]), truncation=True, padding=True, return_tensors=\"pt\")\nval_encodings = tokenizer(list(val_data[\"excerpt\"]), truncation=True, padding=True, return_tensors=\"pt\")\n","8a8cf59b":"tokenizer.save_pretrained(\".\/model\")","bdfc05e2":"train_encodings.keys()","ffbec935":"import torch\n\nclass ReadabilityDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        \n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = ReadabilityDataset(train_encodings, list(train_data[\"target\"]))\nval_dataset = ReadabilityDataset(val_encodings, list(val_data[\"target\"]))","9897f859":"import transformers\ntraining_args = TrainingArguments(\n    output_dir='.\/output',          # output directory\n    num_train_epochs=8,              # total number of training epochs\n    per_device_train_batch_size=12,  # batch size per device during training\n    per_device_eval_batch_size=12,   # batch size for evaluation\n    warmup_steps=300,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='.\/logs',            # directory for storing logs\n    logging_steps=10,\n    load_best_model_at_end = True,\n    do_eval = True,\n    learning_rate = 1e-5, \n    lr_scheduler_type = \"cosine\"\n)","bb696755":"#1e5, 180 best, try 300","29e5d675":"model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels = 1)\n","7651ac57":"trainer = Trainer(\n    model=model,                         # the instantiated \ud83e\udd17 Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=val_dataset             # evaluation dataset\n)\n\ntrainer.train()","b43ae77e":"trainer.save_model(\".\/model\")","5a99249c":"from sklearn.metrics import mean_squared_error\nval_preds = trainer.predict(val_dataset)\nmean_squared_error(list(val_data[\"target\"]), list(val_preds.predictions.reshape(1,-1)[0]))**(1\/2)","c62e7c61":"test_data = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")\ntest_encodings = tokenizer(list(test_data[\"excerpt\"]), truncation=True, padding=True, return_tensors=\"pt\")\ntest_dataset = ReadabilityDataset(test_encodings,[0 for i in range(len(test_data[\"excerpt\"]))])\npreds = trainer.predict(test_dataset)","c9907875":"submit = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv\")\nsubmit[\"target\"] = list(preds.predictions.reshape(1,-1)[0])\nsubmit[\"id\"] = test_data[\"id\"]","5d40c1fd":"submit","74d3c9a7":"submit.to_csv(\"submission.csv\",index = None)","d338ef1f":"## Tokenize data","3a0e5446":"# Making a submission file","e58f698d":"## Now your turn. Upvote if you find this notebook helpful :-)","ff34480d":"# Predicting on test data","8cca60d5":"## Load Model","6cd633cc":"## Pytorch dataset reading class\n- prepare dataset for feeding into model","8738720c":"# Imports","45961589":"## Get wandb API key\nFor accessing a wandb \\<API KEY> you need to signup at [wandb](https:\/\/wandb.ai\/site) website. There are two ways to access the API key\n- As soon as you signup at [wandb](https:\/\/wandb.ai\/site) a key will automatically popup on webpage\n- If not go to [account settings](https:\/\/wandb.ai\/settings) you can find the api key\n\nGet the key and place it in the below code\n\n> \ud83d\udccc I have used my own key below. However, I will be deleting that key after making this notebook public. Please follow above steps and get access for your own key","df0cbc07":"# Validation scores","fd883ec5":"## Install wandb library .This helps to fetch weights of a given model","e11bf184":"## This notebook is about trainig a Roberta (or any transformer) model using Hugging Face library\nThings you need to know while using hugging face transformers library\n- There are 4 for finetuning a transformer library\n    1. Import a __tokenizer__ to tokenise the given text in a format the model understands\n    2. Feed the tokenized data to __model__\n    3. __Define training prarameters__ for _finetuning_ the model\n    4. __Train__ the model\n    \n    \n","c7fd80dc":"# Make some directories for Outputs, logs and model\n> \ud83d\udccc __.\/model__ directory is used for storing files of both tokeniser and model weights. Again its not just model parameters but also tokeniser parameters in the same path","64e98f08":"# How sumbit this model\n\n- In this notebook we have used the model by downloading from internet\n- But while submission we need to keep the model offline\n- Once you fine tune and generate a model commit the notebook\n- Then you will see something like this in the output section of commit\n- Click the button (circled in yellow) to add the model as a dataset and use it while inferencing\n- You can find the inferencing code [here](https:\/\/www.kaggle.com\/abhilashreddyy\/inference-transformer-model-using-hugging-face) (can be used for submitting the code)\n\n![](https:\/\/i.ibb.co\/4MSSMQT\/Whats-App-Image-2021-05-18-at-13-42-10.jpg)","014bc8d3":"## Split train test data","8587bc6d":"# Save model\n- saving the model so that it can be used later in offline settings","23a1893f":"## Load Dataset","7ee8f986":"# Train the model","58ae2f3a":"## Define training arguments for finetuning\n- We are using single cyclie learning rate for finetuning the model by using [cosine scheduler with warmup](https:\/\/huggingface.co\/transformers\/main_classes\/optimizer_schedules.html#transformers.get_cosine_schedule_with_warmup) function\n- You can read [this](https:\/\/medium.com\/dsnet\/the-1-cycle-policy-an-experiment-that-vanished-the-struggle-in-training-neural-nets-184417de23b9) article to learn more about one cycle LR\n- Change in learning rate looks something like this\n    - Initital climb in learning rate is called _warmup_steps_\n    \n![](https:\/\/i.ibb.co\/FD6fXFr\/warmup-cosine-schedule.png)\n\n> \ud83d\udccc I manually tried some good learning rates for training the model. Since hugging face has not yet implemented LR finder function"}}