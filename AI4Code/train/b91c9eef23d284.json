{"cell_type":{"8ad69464":"code","98b7d70c":"code","5182ae52":"code","0859a244":"code","09e119db":"code","af12ce2b":"code","fee5c9eb":"code","0e0c79aa":"code","c4e94498":"code","f75fb3f4":"code","e000f0dd":"code","5e530dd3":"code","2ae69d89":"code","0f398ff2":"code","e9930e01":"code","5ee7c1fa":"code","2e274bc6":"code","808bc1b0":"code","67724963":"code","f535ed5e":"code","b9638c81":"code","e2a5f2fe":"code","2f9e6fd4":"code","38759f93":"code","9ace6c98":"code","95916c61":"markdown","0edbab06":"markdown","589cca13":"markdown","b62986a6":"markdown"},"source":{"8ad69464":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","98b7d70c":"df = pd.read_csv('\/kaggle\/input\/stop-words-in-28-languages\/russian.txt')\ndf","5182ae52":"#Codes by Ragnar https:\/\/www.kaggle.com\/rowhitswami\/starter-load-stopwords\n\ndef get_stopwords_list(stop_file_path):\n    \"\"\"load stop words \"\"\"\n    \n    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n        stopwords = f.readlines()\n        stop_set = set(m.strip() for m in stopwords)\n        return list(frozenset(stop_set))","0859a244":"stopwords_path = \"\/kaggle\/input\/stop-words-in-28-languages\/russian.txt\"\nstopwords = get_stopwords_list(stopwords_path)","09e119db":"stopwords[0:10]","af12ce2b":"print(f\"Total number of stopwords: {len(stopwords)}\")","fee5c9eb":"from gensim.models import Word2Vec\nimport gensim","0e0c79aa":"corpus = ['\u041a\u043e\u043d\u0435\u0301\u0447\u043d\u043e, \u044d\u0301\u0442\u043e \u043d\u0435\u043f\u0440\u0430\u0301\u0432\u0434\u0430! \u041c\u0435\u0434\u0432\u0435\u0301\u0434\u0438 \u0436\u0438\u0432\u0443\u0301\u0442 \u0432 \u043b\u0435\u0441\u0443\u0301 \u0438 \u043d\u0435 \u043b\u044e\u0301\u0431\u044f\u0442 \u043b\u044e\u0434\u0435\u0301\u0439.', '\u0417\u0438\u043c\u043e\u0301\u0439 \u043c\u0435\u0434\u0432\u0435\u0301\u0434\u0438 \u043e\u0431\u044b\u0301\u0447\u043d\u043e \u0441\u043f\u044f\u0442, \u043f\u043e\u0442\u043e\u043c\u0443\u0301 \u0447\u0442\u043e \u043f\u043e\u0433\u043e\u0301\u0434\u0430 \u0441\u043b\u0438\u0301\u0448\u043a\u043e\u043c \u0445\u043e\u043b\u043e\u0301\u0434\u043d\u0430\u044f \u0438 \u043d\u0435\u0442 \u0435\u0434\u044b\u0301.', '\u041f\u043e\u044d\u0301\u0442\u043e\u043c\u0443 \u0440\u0443\u0301\u0441\u0441\u043a\u0438\u0435 \u043b\u044e\u0301\u0431\u044f\u0442 \u043c\u0435\u0434\u0432\u0435\u0301\u0434\u0435\u0439 \u0438 \u043d\u0430\u0437\u044b\u0432\u0430\u0301\u044e\u0442 \u0438\u0445 \"\u043c\u0438\u0301\u0448\u043a\u0438\"']","c4e94498":"stop_words = ['\u043e\u0434\u0438\u043d',\n '\u0431\u044b\u043b\u0438',\n '\u043c\u043e\u0440',\n '\u0448\u0435\u0441\u0442\u044c',\n '\u043f\u043e\u0441\u043b\u0435',\n '\u0431\u043e\u043b\u044c\u0448\u0435',\n '\u0440\u0430\u0437',\n '\u0442\u0432\u043e\u044f',\n '\u0435\u0449\u0451',\n '\u043e\u043d\u0438']","f75fb3f4":"def remove_stop_words(corpus):\n    results = []\n    for text in corpus:\n        tmp = text.split(' ')\n        for stop_word in stop_words:\n            if stop_word in tmp:\n                tmp.remove(stop_word)\n        results.append(\" \".join(tmp))\n        \n    return results","e000f0dd":"corpus = remove_stop_words(corpus)","5e530dd3":"words = []\nfor text in corpus:\n    for word in text.split(' '):\n        words.append(word)\n        \nwords = set(words)","2ae69d89":"words","0f398ff2":"\"\"\"Data Generation\"\"\"\n\nword2int = {}\n\nfor i,word in enumerate(words):\n    word2int[word] = i\n    \nsentences = []\nfor sentence in corpus:\n    sentences.append(sentence.split())\n    \nWINDOW_SIZE = 2\n\ndata = []\nfor sentence in sentences:\n    for idx, word in enumerate(sentence):\n        for neighbor in sentence[max(idx - WINDOW_SIZE, 0): min(idx + WINDOW_SIZE, len(sentence) + 1)]:\n            if neighbor !=word:\n                data.append([word, neighbor])","e9930e01":"for text in corpus:\n    print(text)\n\ndf = pd.DataFrame(data, columns = ['input', 'label'])","5ee7c1fa":"df.head(10)","2e274bc6":"df.shape","808bc1b0":"word2int","67724963":"import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nx = tf.placeholder(shape=[None, 2], dtype=tf.float32)","f535ed5e":"\"\"\"Define Tensorflow Graph\"\"\"\n\nONE_HOT_DIM = len(words)\n\n# function to convert numbers to one hot vectors\ndef to_one_hot_encoding(data_point_index):\n    one_hot_encoding = np.zeros(ONE_HOT_DIM)\n    one_hot_encoding[data_point_index] = 1\n    return one_hot_encoding\n\nX = [] # input word\nY = [] # target word\n\nfor x, y in zip(df['input'], df['label']):\n    X.append(to_one_hot_encoding(word2int[ x ]))\n    Y.append(to_one_hot_encoding(word2int[ y ]))\n\n# convert them to numpy arrays\nX_train = np.asarray(X)\nY_train = np.asarray(Y)\n\n# making placeholders for X_train and Y_train\nx = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\ny_label = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n\n# word embedding will be 2 dimension for 2d visualization\nEMBEDDING_DIM = 2 \n\n# hidden layer: which represents word vector eventually\nW1 = tf.Variable(tf.random_normal([ONE_HOT_DIM, EMBEDDING_DIM]))\nb1 = tf.Variable(tf.random_normal([1])) #bias\nhidden_layer = tf.add(tf.matmul(x,W1), b1)\n\n# output layer\nW2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, ONE_HOT_DIM]))\nb2 = tf.Variable(tf.random_normal([1]))\nprediction = tf.nn.softmax(tf.add( tf.matmul(hidden_layer, W2), b2))\n\n# loss function: cross entropy\nloss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), axis=[1]))\n\n# training operation\ntrain_op = tf.train.GradientDescentOptimizer(0.05).minimize(loss)","b9638c81":"\"\"\"Training\"\"\"\n\nsess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init) \n\niteration = 20000\nfor i in range(iteration):\n    # input is X_train which is one hot encoded word\n    # label is Y_train which is one hot encoded neighbor word\n    sess.run(train_op, feed_dict={x: X_train, y_label: Y_train})\n    if i % 3000 == 0:\n        print('iteration '+str(i)+' loss is : ', sess.run(loss, feed_dict={x: X_train, y_label: Y_train}))","e2a5f2fe":"# Now the hidden layer (W1 + b1) is actually the word look up table\nvectors = sess.run(W1 + b1)\nprint(vectors)","2f9e6fd4":"\"\"\"Word Vector in Table\"\"\"\n\nw2v_df = pd.DataFrame(vectors, columns = ['x1', 'x2'])\nw2v_df['word'] = words\nw2v_df = w2v_df[['word', 'x1', 'x2']]\nw2v_df","38759f93":"\"\"\"Word Vector in 2D Chart\"\"\"\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\nfor word, x1, x2 in zip(w2v_df['word'], w2v_df['x1'], w2v_df['x2']):\n    ax.annotate(word, (x1,x2 ))\n    \nPADDING = 1.0\nx_axis_min = np.amin(vectors, axis=0)[0] - PADDING\ny_axis_min = np.amin(vectors, axis=0)[1] - PADDING\nx_axis_max = np.amax(vectors, axis=0)[0] + PADDING\ny_axis_max = np.amax(vectors, axis=0)[1] + PADDING\n \nplt.xlim(x_axis_min,x_axis_max)\nplt.ylim(y_axis_min,y_axis_max)\nplt.rcParams[\"figure.figsize\"] = (20,20)\n\nplt.show()","9ace6c98":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('\u042d\u0301\u0442\u043e\u0442 \u0414\u0435\u043d\u044c \u041f\u043e\u0431\u0435\u0301\u0434\u044b. \u042d\u0301\u0442\u043e \u0440\u0430\u0301\u0434\u043e\u0441\u0442\u044c. @mpwolke was here' )","95916c61":"#Text by https:\/\/www.russianforfree.com\/text-in-russian-advanced-victory-day.php\n\n\u042d\u0301\u0442\u043e\u0442 \u0414\u0435\u043d\u044c \u041f\u043e\u0431\u0435\u0301\u0434\u044b. This Victory Day. (The 9th of May) \u042d\u0301\u0442\u043e \u0440\u0430\u0301\u0434\u043e\u0441\u0442\u044c. This is happiness.","0edbab06":"#Codes by Anil Govind https:\/\/www.kaggle.com\/anilreddy8989\/stopwords-word2vector","589cca13":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcSJVRFaLVDi0cF4rUCeXySkZowxVPgFQzpUGQ&usqp=CAU)sk.pinterest.com","b62986a6":"#Text by https:\/\/www.russianforfree.com\/text-russian-bear.php"}}