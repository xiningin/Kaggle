{"cell_type":{"43e3ac0f":"code","ba2e082c":"code","6be65a60":"code","00254606":"code","6ec55182":"code","5c289730":"code","ac8d625d":"code","cd5a7279":"code","703ef871":"code","0b4d95ef":"code","20c020b5":"code","b310d26f":"code","233c8f63":"code","81aea5d7":"code","a9ed3931":"code","c2c6e41e":"code","31a934e7":"code","5ea09540":"code","0cd2cca8":"code","9ba8f2e7":"markdown","8a366730":"markdown","67cf1c9e":"markdown","f9f07701":"markdown","872d0799":"markdown","683faf33":"markdown","7de1603f":"markdown","cb787029":"markdown","e53427ca":"markdown"},"source":{"43e3ac0f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ba2e082c":"#Read in the training data\ntrain_df = pd.read_csv('..\/input\/train.csv',\n                    dtype={'acoustic_data': np.int16,\n                           'time_to_failure': np.float64}) ","6be65a60":"#Look at the data and realize its not like a normal dataset\ntrain_df.head()","00254606":"train_df.shape","6ec55182":"#Define how long we want each sample to be\nsample_length = 150000\n\n#Divide length of our dataframe by how long we want each sample\nnum_samples = int((len(train_df) \/ sample_length)) ","5c289730":"#This is how many samples we will create\nnum_samples","ac8d625d":"#This is a list of features we will create\ncols = ['mean','median','std','max',\n        'min','var','ptp','10p',\n        '25p','50p','75p','90p']","cd5a7279":"#This creates an empty dataframe for now\n#Later we will fill it with values\nX_train = pd.DataFrame(index=range(num_samples), #The index will be each of our new samples\n                       dtype=np.float64, #Assign a datatype\n                       columns=cols) #The columns will be the features we listed above","703ef871":"#This creates a dataframe for our target variable 'time_to_failure'\ny_train = pd.DataFrame(index=range(num_samples),\n                       dtype=np.float64, \n                       columns=['time_to_failure']) #Our target variable","0b4d95ef":"#Now we create the samples\nfor i in range(num_samples):\n    \n    #i*sample_length = the starting index (from train_df) of the sample we create\n    #i*sample_length + sample_length = the ending index (from train_df)\n    sample = train_df.iloc[i*sample_length:i*sample_length+sample_length]\n    \n    #Converts to numpy array\n    x = sample['acoustic_data'].values\n    \n    #Grabs the final 'time_to_failure' value\n    y = sample['time_to_failure'].values[-1]\n    y_train.loc[i, 'time_to_failure'] = y\n    \n   #For every 150,000 rows, we make these calculations\n    X_train.loc[i, 'mean'] = np.mean(x)\n    X_train.loc[i, 'median'] = np.median(x)\n    X_train.loc[i, 'std'] = np.std(x)\n    X_train.loc[i, 'max'] = np.max(x)\n    X_train.loc[i, 'min'] = np.min(x)\n    X_train.loc[i, 'var'] = np.var(x)\n    X_train.loc[i, 'ptp'] = np.ptp(x) #Peak-to-peak is like range\n    X_train.loc[i, '10p'] = np.percentile(x,q=10) \n    X_train.loc[i, '25p'] = np.percentile(x,q=25) #We can also grab percentiles\n    X_train.loc[i, '50p'] = np.percentile(x,q=50)\n    X_train.loc[i, '75p'] = np.percentile(x,q=75)\n    X_train.loc[i, '90p'] = np.percentile(x,q=90)","20c020b5":"#Creates a simple train, test split\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train)","b310d26f":"#Fit a random forest\nfrom sklearn.ensemble import RandomForestRegressor\n\n#This creates the Randomforest with the given parameters\nrf = RandomForestRegressor(n_estimators=100, #100 trees (Default of 10 is too small)\n                          max_features=0.5, #Max number of features each tree can use \n                          min_samples_leaf=30, #Min amount of samples in each leaf\n                          random_state=11)\n\n#This trains the random forest on our training data\nrf.fit(X_train,y_train)","233c8f63":"#Score the model\nfrom sklearn.metrics import mean_absolute_error\n\nmean_absolute_error(y_val, rf.predict(X_val))","81aea5d7":"#Read in the sample submission. We can use that as a dataframe to grab the segment ids\nsubmission = pd.read_csv('..\/input\/sample_submission.csv',\n                         index_col = 'seg_id')","a9ed3931":"submission.head()","c2c6e41e":"#Creates a test dataframe\nX_test = pd.DataFrame(columns=X_train.columns, #Use the same columns as our X_train\n                      dtype=np.float64,\n                      index=submission.index) #Use the index ('seg_id') from the sample submission","31a934e7":"for i in X_test.index:\n    \n    #Read in that segments csv file\n    #By putting f before the string we can put any values between {} and it will be treated as a string\n    seg = pd.read_csv(f'..\/input\/test\/{i}.csv') \n                                            \n    #Grab the acoustic_data values\n    x = seg['acoustic_data'].values\n\n    #These are the same features we calcuted on the training data\n    X_test.loc[i, 'mean'] = np.mean(x)\n    X_test.loc[i, 'median'] = np.median(x)\n    X_test.loc[i, 'std'] = np.std(x)\n    X_test.loc[i, 'max'] = np.max(x)\n    X_test.loc[i, 'min'] = np.min(x)\n    X_test.loc[i, 'var'] = np.var(x)\n    X_test.loc[i, 'ptp'] = np.ptp(x)\n    X_test.loc[i, '10p'] = np.percentile(x,q=10) \n    X_test.loc[i, '25p'] = np.percentile(x,q=25)\n    X_test.loc[i, '50p'] = np.percentile(x,q=50)\n    X_test.loc[i, '75p'] = np.percentile(x,q=75)\n    X_test.loc[i, '90p'] = np.percentile(x,q=90)\n    ","5ea09540":"#Predict on the test data\ntest_predictions = rf.predict(X_test)\n\n#Assign the target column in our submission to be our predictions\nsubmission['time_to_failure'] = test_predictions","0cd2cca8":"#Output the predictions to a csv file\nsubmission.to_csv('submission.csv')","9ba8f2e7":"The goal of this kernel is to **establish a baseline **and **explain everything step by step** so anyone can get started. Code to read in and process the data is based on this kernel: <a href='https:\/\/www.kaggle.com\/inversion\/basic-feature-benchmark'>Basic Feature Benchmark.<\/a>\n\nAny questions, comments, or suggestions are always welcome.","8a366730":"How to submit predictions from the kernel:","67cf1c9e":"Now click Commit. After it runs, click Open Version. Scroll down to output and click submit to competition.","f9f07701":"By specifying the datatypes, pandas can read in the csv faster and use less memory. Without specifying the datatypes the kernel can crash because the file is so large","872d0799":"Now that you have a baseline, here are some next steps to explore:\n\n* Try to think of any other useful features you can extract for each sample. Here are more summary statics to try from <a href='https:\/\/docs.scipy.org\/doc\/scipy\/reference\/stats.html#summary-statistics'>SciPy<\/a>. SciPy also has different signal processing functions to check out.\n\n* Calculate the min, max, mean, etc over smaller chunks of the 150,000 rows per sample and add those as more features.\n\n* Create more samples. Instead of our training data being generated from seperate sections of 150,000 rows with no overlap, you can adjust the indices so that the samples overlap. If you do this be very careful to make sure no samples in your validation set overlap with your training data.\n\n* Try a LightGBM, XGBoost, or NN model.\n\n* Plot some samples to explore the data\n\n* Plot feature importances to see what your model finds important.","683faf33":"**n_estimators**: the number of decision trees in our random forest. \n\n**max_features**: The max amount of features each decision tree can use. This can be an integer (max_features=10 means use 10 features) or a ratio (0.5 means each tree is fit using half of the original features. Features are selected at random) \n\n**min_samples_leaf**: Minimum number of samples in each leaf of the decision trees","7de1603f":"Instead of each row being one sample, we have one long continous set of data. **We will split this one long sample into many different samples.**\n\nThe first column*** 'acoustic data'***  is our only feature. We use that feature to predict ***'time_to_failure'*.**\n\n**Each new sample will be created from 150,000 rows** of only the *'acoustic_data'* column. The target variable is the* 'time_to_failure' *at the last row of the sample. (We use 150,000 rows because thats how long the test samples are)","cb787029":"Our goal is to condense each sample into one row . **We create these features to extract information out of the 150,000 rows that make up each sample. After we extract those features, we can then represent each sample as one row in a dataframe.**","e53427ca":"In the test folder each segment is its own csv file. To eactract the features we loop through each segment id one at a time."}}