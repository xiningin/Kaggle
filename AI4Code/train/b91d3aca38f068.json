{"cell_type":{"66245fab":"code","7ffbe675":"code","859c8de6":"code","0ae02f34":"code","a5338654":"code","7e443745":"code","dbd14fe1":"code","5b76bf5a":"code","3765c8f6":"code","cad7a8ba":"code","305c9820":"code","6d9f871c":"code","cf4e76ad":"code","62af339b":"code","1f5edb0f":"code","0bebee8d":"code","2ccb2de6":"code","cc2fbc52":"code","c3653a66":"code","44436995":"code","cfbce598":"code","22c9af72":"code","3e7fb855":"code","b240f41e":"code","b7c28a4a":"markdown","016a6378":"markdown","4e6f3d8a":"markdown","61fabbf5":"markdown","dadfdf7e":"markdown","ebb2dd40":"markdown","967f9f71":"markdown","5d3ecf00":"markdown","b5fb394b":"markdown"},"source":{"66245fab":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy import ndimage \nfrom scipy.cluster import hierarchy \nfrom scipy.spatial import distance_matrix \nfrom matplotlib import pyplot as plt \nfrom sklearn import manifold, datasets \nfrom sklearn.cluster import AgglomerativeClustering \nfrom sklearn.datasets.samples_generator import make_blobs \n%matplotlib inline\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7ffbe675":"X1, y1 = make_blobs(n_samples=50, centers=[[4,4], [-2, -1], [1, 1], [10,4]], cluster_std=0.9)","859c8de6":"plt.scatter(X1[:, 0], X1[:, 1], marker='o') ","0ae02f34":"X1.shape","a5338654":"y1.shape","7e443745":"agglom = AgglomerativeClustering(n_clusters = 4, linkage = 'average')","dbd14fe1":"agglom.fit(X1, y1)","5b76bf5a":"# Create a figure of size 6 inches by 4 inches.\nplt.figure(figsize=(10,6))\n\n# These two lines of code are used to scale the data points down,\n# Or else the data points will be scattered very far apart.\n\n# Create a minimum and maximum range of X1.\nx_min, x_max = np.min(X1, axis=0), np.max(X1, axis=0)\n\n# Get the average distance for X1.\nX1 = (X1 - x_min) \/ (x_max - x_min)\n\n# This loop displays all of the datapoints.\nfor i in range(X1.shape[0]):\n    # Replace the data points with their respective cluster value \n    # (ex. 0) and is color coded with a colormap (plt.cm.spectral)\n    plt.text(X1[i, 0], X1[i, 1], str(y1[i]),\n             color=plt.cm.nipy_spectral(agglom.labels_[i] \/ 10.),\n             fontdict={'weight': 'bold', 'size': 9})\n    \n# Remove the x ticks, y ticks, x and y axis\n#plt.xticks([])\n#plt.yticks([])\n#plt.axis('off')\n\n\n\n# Display the plot of the original data before clustering\nplt.scatter(X1[:, 0], X1[:, 1], marker='.')\n# Display the plot\nplt.show()","3765c8f6":"dist_matrix = distance_matrix(X1,X1) \nprint(dist_matrix)","cad7a8ba":"Z = hierarchy.linkage(dist_matrix, 'complete')","305c9820":"plt.figure(figsize=(10,6))\ndendro = hierarchy.dendrogram(Z)","6d9f871c":"Z = hierarchy.linkage(dist_matrix, 'average')\nplt.figure(figsize=(10,6))\ndendro = hierarchy.dendrogram(Z)","cf4e76ad":"import pandas as pd\ndf = pd.read_csv(\"..\/input\/cars_clus.csv\")","62af339b":"print (\"Shape of dataset: \", df.shape)\n\ndf.head(5)","1f5edb0f":"# Quick Cleaning\n\nprint (\"Shape of dataset before cleaning: \", df.size)\ndf[[ 'sales', 'resale', 'type', 'price', 'engine_s',\n       'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap',\n       'mpg', 'lnsales']] = df[['sales', 'resale', 'type', 'price', 'engine_s',\n       'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap',\n       'mpg', 'lnsales']].apply(pd.to_numeric, errors='coerce')\ndf = df.dropna()\ndf = df.reset_index(drop=True)\nprint (\"Shape of dataset after cleaning: \", df.size)\ndf.head(5)","0bebee8d":"# Quick Feature Selection\n\nfeatureset = df[['engine_s',  'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap', 'mpg']]","2ccb2de6":"# Now we can normalize the feature set. __MinMaxScaler__ transforms features by scaling each feature to a given range. It is by default (0, 1). \nfrom sklearn.preprocessing import MinMaxScaler\nx = featureset.values #returns a numpy array\nmin_max_scaler = MinMaxScaler()\nfeature_mtx = min_max_scaler.fit_transform(x)\nfeature_mtx [0:5]","cc2fbc52":"from scipy.spatial import distance_matrix \ndist_matrix = distance_matrix(feature_mtx,feature_mtx) \nprint(dist_matrix)","c3653a66":"agglom = AgglomerativeClustering(n_clusters = 6, linkage = 'complete')\nagglom.fit(feature_mtx)\nagglom.labels_","44436995":"df['cluster_'] = agglom.labels_\ndf.head()","cfbce598":"import matplotlib.cm as cm\nn_clusters = max(agglom.labels_)+1\ncolors = cm.rainbow(np.linspace(0, 1, n_clusters))\ncluster_labels = list(range(0, n_clusters))\n\n# Create a figure of size 6 inches by 4 inches.\nplt.figure(figsize=(16,14))\n\nfor color, label in zip(colors, cluster_labels):\n    subset = df[df.cluster_ == label]\n    for i in subset.index:\n            plt.text(subset.horsepow[i], subset.mpg[i],str(subset['model'][i]), rotation=25) \n    plt.scatter(subset.horsepow, subset.mpg, s= subset.price*10, c=color, label='cluster'+str(label),alpha=0.5)\n#    plt.scatter(subset.horsepow, subset.mpg)\nplt.legend()\nplt.title('Clusters')\nplt.xlabel('horsepow')\nplt.ylabel('mpg')","22c9af72":"df.groupby(['cluster_','type'])['cluster_'].count()","3e7fb855":"agg_cars = df.groupby(['cluster_','type'])['horsepow','engine_s','mpg','price'].mean()\nagg_cars","b240f41e":"plt.figure(figsize=(16,10))\nfor color, label in zip(colors, cluster_labels):\n    subset = agg_cars.loc[(label,),]\n    for i in subset.index:\n        plt.text(subset.loc[i][0]+5, subset.loc[i][2], 'type='+str(int(i)) + ', price='+str(int(subset.loc[i][3]))+'k')\n    plt.scatter(subset.horsepow, subset.mpg, s=subset.price*20, c=color, label='cluster'+str(label))\nplt.legend()\nplt.title('Clusters')\nplt.xlabel('horsepow')\nplt.ylabel('mpg')\n","b7c28a4a":"Adding the labels back to the dataset for analysis","016a6378":"Using the <b> linkage <\/b> class from hierarchy, pass in the parameters:\n<ul>\n    <li> The distance matrix <\/li>\n    <li> 'complete' for complete linkage <\/li>\n<\/ul> <br>\nSave the result to a variable called <b> Z <\/b>","4e6f3d8a":"<h1 id=\"hierarchical_agglomerative\">Hierarchical Clustering - Agglomerative<\/h1>\n\nWe will be looking at a clustering technique, which is <b>Agglomerative Hierarchical Clustering<\/b>. Remember that agglomerative is the bottom up approach. <br> <br>\nWe will be looking at Agglomerative clustering, which is more popular than Divisive clustering. <br> <br>\nWe will also be using Complete Linkage as the Linkage Criteria. <br>\n<b> <i> NOTE: You can also try using Average Linkage wherever Complete Linkage would be used to see the difference! <\/i> <\/b>","61fabbf5":"<h1 id=\"clustering_vehicle_dataset\">Clustering on Vehicle dataset<\/h1>\n\nImagine that an automobile manufacturer has developed prototypes for a new vehicle. Before introducing the new model into its range, the manufacturer wants to determine which existing vehicles on the market are most like the prototypes--that is, how vehicles can be grouped, which group is the most similar with the model, and therefore which models they will be competing against.\n\nOur objective here, is to use clustering methods, to find the most distinctive clusters of vehicles. It will summarize the existing vehicles and help manufacturers to make decision about the supply of new models.","dadfdf7e":"\nIt is obvious that we have 3 main clusters with the majority of vehicles in those.\n\n__Cars__:\n- Cluster 1: with almost high mpg, and low in horsepower.\n- Cluster 2: with good mpg and horsepower, but higher price than average.\n- Cluster 3: with low mpg, high horsepower, highest price.\n    \n    \n    \n__Trucks__:\n- Cluster 1: with almost highest mpg among trucks, and lowest in horsepower and price.\n- Cluster 2: with almost low mpg and medium horsepower, but higher price than average.\n- Cluster 3: with good mpg and horsepower, low price.\n\n\nPlease notice that we did not use __type__ , and __price__ of cars in the clustering process, but Hierarchical clustering could forge the clusters and discriminate them with quite high accuracy.","ebb2dd40":"\n<h3 id=\"dendrogram\">Dendrogram Associated for the Agglomerative Hierarchical Clustering<\/h3>\nRemember that a <b>distance matrix<\/b> contains the <b> distance from each point to every other point of a dataset <\/b>. <br>\nUse the function <b> distance_matrix, <\/b> which requires <b>two inputs<\/b>. Use the Feature Matrix, <b> X2 <\/b> as both inputs and save the distance matrix to a variable called <b> dist_matrix <\/b> <br> <br>\nRemember that the distance values are symmetric, with a diagonal of 0's. This is one way of making sure your matrix is correct. <br> (print out dist_matrix to make sure it's correct)","967f9f71":"A Hierarchical clustering is typically visualized as a dendrogram as shown in the following cell. Each merge is represented by a horizontal line. The y-coordinate of the horizontal line is the similarity of the two clusters that were merged, where cities are viewed as singleton clusters. \nBy moving up from the bottom layer to the top node, a dendrogram allows us to reconstruct the history of merges that resulted in the depicted clustering. \n\nNext, we will save the dendrogram to a variable called <b>dendro<\/b>. In doing this, the dendrogram will also be displayed.\nUsing the <b> dendrogram <\/b> class from hierarchy, pass in the parameter:\n","5d3ecf00":"As you can see, we are seeing the distribution of each cluster using the scatter plot, but it is not very clear where is the centroid of each cluster. Moreover, there are 2 types of vehicles in our dataset, \"truck\" (value of 1 in the type column) and \"car\" (value of 1 in the type column). So, we use them to distinguish the classes, and summarize the cluster. First we count the number of cases in each group:","b5fb394b":"The <b> Agglomerative Clustering <\/b> class will require two inputs:\n<ul>\n    <li> <b>n_clusters<\/b>: The number of clusters to form as well as the number of centroids to generate. <\/li>\n    <ul> <li> Value will be: 4 <\/li> <\/ul>\n    <li> <b>linkage<\/b>: Which linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion. <\/li>\n    <ul> \n        <li> Value will be: 'complete' <\/li> \n        <li> <b>Note<\/b>: It is recommended you try everything with 'average' as well <\/li>\n    <\/ul>\n<\/ul> <br>"}}