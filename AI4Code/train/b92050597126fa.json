{"cell_type":{"bf5e4e13":"code","4aa987f3":"code","2d4f6a29":"code","9f60d1e5":"code","4ad4fc06":"code","7b444f3f":"code","7a343910":"code","511e8d45":"code","f5ba3ddd":"code","f7e8ca9f":"code","080296e8":"code","1b549f6b":"code","d220340e":"code","4b33f482":"code","b4507214":"code","54b86099":"code","32567215":"code","0d891356":"code","9277f390":"code","7803e8bb":"code","976b9505":"code","fcf0164a":"code","fb54ac00":"code","d22f43f1":"code","d0854a31":"code","717fcbf9":"code","949ffcc6":"code","01bb5bff":"code","c16a65d2":"code","7bf0001b":"code","cdad0a3d":"code","c8f8accf":"code","f6a01ca5":"code","8f954e4e":"code","8a62a633":"code","4092e15e":"markdown"},"source":{"bf5e4e13":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4aa987f3":"sample_submission_filepath = \"\/kaggle\/input\/quora-insincere-questions-classification\/sample_submission.csv\"\nembeddings_zippath = \"\/kaggle\/input\/quora-insincere-questions-classification\/embeddings.zip\"\ntrain_csv_path = \"\/kaggle\/input\/quora-insincere-questions-classification\/train.csv\"\ntest_csv_path = \"\/kaggle\/input\/quora-insincere-questions-classification\/test.csv\"","2d4f6a29":"train_df = pd.read_csv(train_csv_path)\ntest_df = pd.read_csv(test_csv_path)","9f60d1e5":"# unzip file.zip -d destination_folder\n!unzip \/kaggle\/input\/quora-insincere-questions-classification\/embeddings.zip -d \/kaggle\/working\/embeddings","4ad4fc06":"!ls \/kaggle\/working\/","7b444f3f":"# embeddings_list_available = [c.strip() for c in \"\"\"glove.840B.300d\/glove.840B.300d.txt  \n# GoogleNews-vectors-negative300\/GoogleNews-vectors-negative300.bin  \n# wiki-news-300d-1M\/wiki-news-300d-1M.vec  \n# paragram_300_sl999\/README.txt  \n# paragram_300_sl999\/paragram_300_sl999.txt \"\"\".split('\\n')]","7a343910":"embeddings_unzip_path = \"\"\"  \/kaggle\/working\/embeddings\/glove.840B.300d\/glove.840B.300d.txt  \n  \/kaggle\/working\/embeddings\/GoogleNews-vectors-negative300\/GoogleNews-vectors-negative300.bin  \n  \/kaggle\/working\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec  \n  \/kaggle\/working\/embeddings\/paragram_300_sl999\/README.txt  \n  \/kaggle\/working\/embeddings\/paragram_300_sl999\/paragram_300_sl999.txt \"\"\".split(\"\\n\")","511e8d45":"embeddings_list_available = [c.strip() for c in embeddings_unzip_path]","f5ba3ddd":"embeddings_list_available","f7e8ca9f":"embeddings_available_dict = {}\nfor idx, c in enumerate(embeddings_list_available):\n    print(f\"Embedings: {idx}. {c}\")\n    embeddings_available_dict[idx] = c","080296e8":"embeddings_available_dict","1b549f6b":"EMBEDDING_FILE = embeddings_available_dict[0]\nprint(EMBEDDING_FILE)","d220340e":"%%time\ndef get_coefs(word,*arr): \n    return word, np.asarray(arr, dtype='float32')\n\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))","4b33f482":"embeddings_index","b4507214":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]","54b86099":"import os\nimport time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers","32567215":"## split to train and val\ntrain_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2018)","0d891356":"train_df","9277f390":"## some config values \nembed_size = 300 # how big is each word vector\nmax_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a question to use\n\n## fill up the missing values\ntrain_X = train_df[\"question_text\"].fillna(\"_na_\").values\nval_X = val_df[\"question_text\"].fillna(\"_na_\").values\ntest_X = test_df[\"question_text\"].fillna(\"_na_\").values\n\n## Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n## Pad the sentences \ntrain_X = pad_sequences(train_X, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\n## Get the target values\ntrain_y = train_df['target'].values\nval_y = val_df['target'].values","7803e8bb":"train_df[\"question_text\"][0]","976b9505":"train_X[0]","fcf0164a":"word_index = tokenizer.word_index","fb54ac00":"nb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))","d22f43f1":"emb_mean#embedding_matrix.shape","d0854a31":"embedding_matrix","717fcbf9":"for word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n        ","949ffcc6":"inp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nx = Bidirectional(LSTM(64, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","01bb5bff":"model.fit(train_X, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))","c16a65d2":"pred_glove_val_y = model.predict([val_X], batch_size=1024, verbose=1)\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_glove_val_y>thresh).astype(int))))","7bf0001b":"test_X.shape","cdad0a3d":"test_df.shape","c8f8accf":"df_sub = pd.read_csv(sample_submission_filepath)","f6a01ca5":"pred_glove_test_y = model.predict([test_X], batch_size=1024, verbose=1)","8f954e4e":"df_sub['prediction'] = (pred_glove_test_y>0.5).astype('int')","8a62a633":"df_sub.to_csv(\"submission.csv\", index=False)","4092e15e":"We have four different types of embeddings.\n\n* GoogleNews-vectors-negative300 - https:\/\/code.google.com\/archive\/p\/word2vec\/\n* glove.840B.300d - https:\/\/nlp.stanford.edu\/projects\/glove\/\n* paragram_300_sl999 - https:\/\/cogcomp.org\/page\/resource_view\/106\n* wiki-news-300d-1M - https:\/\/fasttext.cc\/docs\/en\/english-vectors.html\n\nA very good explanation for different types of embeddings are given in this kernel. Please refer the same for more details..\n\n# Glove Embeddings:\n\n    In this section, let us use the Glove embeddings and rebuild the GRU model."}}