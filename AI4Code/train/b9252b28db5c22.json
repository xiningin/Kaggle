{"cell_type":{"6604926f":"code","ffedee3d":"code","3a791946":"code","f5826686":"code","5c7d7d8f":"code","643a4782":"code","b1c6f127":"code","1a102760":"code","c5c0b8ce":"code","d8d02675":"code","932e2b00":"code","d9d68ab6":"code","392f5f59":"code","b56e57a9":"code","c273656f":"code","5b56a4a0":"code","d36f2aab":"code","3116c998":"markdown","d061af67":"markdown","a2a2ceec":"markdown","ed7cb793":"markdown","b7e13e7e":"markdown","d6fb32f6":"markdown","c863d034":"markdown","940e48d5":"markdown","ba6e6880":"markdown","cbb4c288":"markdown","461e03d6":"markdown","11e9ef7b":"markdown","e8c7b4e8":"markdown","29ddf6eb":"markdown"},"source":{"6604926f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\ndata = pd.read_csv('..\/input\/glass.csv')\nfeatures = data.loc[:,data.columns != 'Type']\ntarget = data.loc[:,'Type']","ffedee3d":"features.describe()","3a791946":"corrmat = features.corr()\ncorrmat","f5826686":"corrmat.iloc[0,:].plot(kind='bar')","5c7d7d8f":"import matplotlib.pyplot as plt\nchartlocation = 0\nplt.figure(figsize=(15,12))\ncolumns = np.copy(corrmat.columns.values)\nfor index, row in corrmat.iterrows():\n    column_name = columns[chartlocation]\n    chartlocation = chartlocation + 1\n    plt.subplot(3,3,chartlocation)\n    row.drop(index).plot(kind='bar', title=column_name)","643a4782":"chartlocation = 0\nplt.figure(figsize=(15,12))\ncolumns = features.columns.values\nfor column in columns:\n    chartlocation = chartlocation + 1\n    plt.subplot(3,3,chartlocation)\n    features.boxplot(column=column)","b1c6f127":"fig = plt.figure(figsize=(15,12))\nax = fig.add_subplot(1, 1, 1)\nfeatures.hist(ax=ax)\nplt.show()","1a102760":"features.skew().plot(kind='bar')\nplt.show()","c5c0b8ce":"def find_outlier_fences_IQR(df_in, col_name):\n    q1 = df_in[col_name].quantile(0.25)\n    q3 = df_in[col_name].quantile(0.75)\n    iqr = q3-q1 #Interquartile range\n    fence_low  = q1-1.5*iqr\n    fence_high = q3+1.5*iqr\n    return [fence_low, fence_high]\n\nfences = {}\nfor column in features.columns.values:\n    fences[column] = find_outlier_fences_IQR(features, column)\nprint(fences)\n\n#lets find rows with more than one or two outliers and drop them.\noutliers_index = []\nfor index, row in features.iterrows():\n    outliers_detected = 0\n    for column in features.columns.values:\n        fence_low = fences[column][0]\n        fence_high = fences[column][1]\n        if row[column] < fence_low or row[column] > fence_high:\n            outliers_detected = outliers_detected + 1\n    \n    if outliers_detected > 1:\n        outliers_index.append(index)\n\nprint(\"\\nthere are %d rows found with more than 1 outlier\" %(len(outliers_index)))","d8d02675":"outliers_removed_featureset = features.drop(outliers_index)\noutliers_removed_targetset = target.drop(outliers_index)","932e2b00":"from sklearn.preprocessing import StandardScaler\nautoscaler = StandardScaler()\nfeatures_scaled = autoscaler.fit_transform(outliers_removed_featureset)","d9d68ab6":"from sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(1, figsize=(8, 6))\nax = Axes3D(fig)\nX_reduced = PCA(n_components=3).fit_transform(features_scaled.data)\nax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=outliers_removed_targetset)\nplt.title(\"Priciple components 3\")\nplt.show()","392f5f59":"X_reduced = PCA(n_components=2).fit_transform(features_scaled.data)\nplt.title(\"Priciple components 2\")\nplt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=outliers_removed_targetset)\nplt.show()","b56e57a9":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features_scaled,outliers_removed_targetset, test_size=0.20, random_state=42)","c273656f":"from sklearn.ensemble import GradientBoostingClassifier\nclf = GradientBoostingClassifier(n_estimators=100)\nclf = clf.fit(X_train, y_train)\nfeature_with_importance = pd.DataFrame()\nfeature_with_importance['columns'] = outliers_removed_featureset.columns\nfeature_with_importance['importance'] = clf.feature_importances_\nfeature_with_importance.sort_values(by=['importance'], ascending=True, inplace=True)\nfeature_with_importance.set_index('columns', inplace=True)\nfeature_with_importance.plot(kind='bar')\nplt.show()","5b56a4a0":"from sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodels = [\n    SVC(),\n    KNeighborsClassifier(),\n    GradientBoostingClassifier(n_estimators=100)\n]\n\nfor model in models:\n    clf = model.fit(X_train, y_train)\n    print('score:',clf.score(X_test,y_test))","d36f2aab":"from sklearn.model_selection import GridSearchCV\nparameter_grid = {\n    'C' :  [1, 10, 100, 1000, 1500],\n    'gamma' : [0.001, 0.01, 0.1, 1],\n    'kernel': [ 'rbf', 'sigmoid']\n}\n\ngsv = GridSearchCV(SVC(),parameter_grid)\ngsv = gsv.fit(X_train, y_train)\nprint('score:',gsv.score(X_test,y_test))\ngsv.best_params_","3116c998":"## Splitting the datasets","d061af67":"As you can see in the above graph, almost all elements are important in the making of glass. Now lets try out a few algorithms and see which one best suits our dataset.","a2a2ceec":"### Visualizing the dataset ","ed7cb793":"Before starting any datascience analysis, its important we understand the data we are dealing with. Whatever we can grab our hands at is going to help us build a better modal.\n\nThe current dataset is related to different types of Glass material. The goal of this analysis is to create a modal which can be used to identify the glass type if we can provide it with the variables that make up a Glass.\n\n## Loading the dataset and understanding it.\n","b7e13e7e":"Lets remove the found outliers from the dataset and then move on with modelling.","d6fb32f6":"As you can see in the plot above, the Refractive index of a glass is positively affected when there is an increase in Calcium and Iron content while for the rest of the elements the refractive index has a negative correlation.\n\n## Lets also see how other elements are correlated with each other.","c863d034":"So now that we know our dataset has a lot of outliers, its time we remove them. ","940e48d5":"### Standardization of a dataset\nThis is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).","ba6e6880":"The dimensions in our dataset are the various constituents of Glass. The variation of each quantity gives us a different kind of glass.\n\n### Lets understand what those dimensions really mean.\n\n1.  RI - Refractive index, In optics, the refractive index or index of refraction of a material is a dimensionless number that describes how light propagates through that medium. **This is an important factor that differentiates any glass type**.\n\n2. Na - Sodium\n\n3. Mg - Magnesium\nMagnesium and iron increase glass alteration, forming tri-octahedral smectites with the same (Fe + Mg)\/Si ratio. With iron, two kinds of silicates precipitate with the same composition but with a different morphology, whereas with magnesium alone, a single Mg-silicate forms. Moreover, it was found that the glass alteration rate drops when the pH stabilizes at a minimum value of 7.8 for Mg-silicates and 6.2 for Fe-silicates. At this point the secondary silicates stop precipitating. This result was confirmed by geochemical simulation and the solubility product of these silicates was estimated considering the presence or absence of aluminum in their structure.\nSource: https:\/\/www.sciencedirect.com\/science\/article\/pii\/S088329271630539X\n\n4. Al - Aluminium, Gives strength to the glass. Higher Al makes stronger glasses. \nSource: https:\/\/www.theregister.co.uk\/2015\/11\/04\/alumina_in_glass_could_make_busted_smartphones_a_thing_of_the_past\/\n\n5. Si - Silica is the primary ingredient in the production of most glass. So it wont be a surprise if we find Si having a major share in glass making.\n\n6. K - Potassium - For stronger glass. Source: https:\/\/en.wikipedia.org\/wiki\/Chemically_strengthened_glass\n\n7. Ca - Calcium - Soda\u2013lime glass, also called soda\u2013lime\u2013silica glass, is the most prevalent type of glass, used for windowpanes and glass containers (bottles and jars) for beverages, food, and some commodity items. Glass bakeware is often made of borosilicate glass. Soda\u2013lime glass accounts for about 90% of manufactured glass.\nSource: https:\/\/en.wikipedia.org\/wiki\/Soda%E2%80%93lime_glass\n\n8. Ba - Barium, is what is used in television screens to protects our eyes from the harmful X-Rays that could cause long term health issues.\n\n9. Fe - Iron as mentioned with Magnesium is used for glass alteration.\n\nNow that we have an understanding of our dimensions, lets find the correlations between the dimensions.\n","cbb4c288":"We see that Silica is the only element which is negatively correlated with every other element that make up the glass. Any increase in Silica will need a decrease in every other element and this increase mainly affects the refractive index as you can see in the graph.\n\nWe have a clarity on what the data is about and how they are all related to each other in the making of glass. Lets try now and see if there are any outliers in the dataset. The best way is to visualize each data dimension using boxplots. ","461e03d6":"As you can see in the histograms above, K, Ca, Fe and Ba have the highest skewness in their distribution. A distribution is skewed if one of its tails is longer than the other. The K, Ca, Fe and Ba distribution shown has a positive skew. You can see the collective skewness in our feature set in the below plot.","11e9ef7b":"### Hyperparameters tuning\nHyper-parameters are parameters that are not directly learnt within estimators. ","e8c7b4e8":"### Modelling\nWe will use one of the ensemble methods to find how important different elements are to the making of a glass. ","29ddf6eb":"As you can see in the above boxplots there are many datapoints which are outliers and may affect the goodness of our machine learning modal. We need to get rid of them. You can see the affect of outliers on the data distribution as well which we will see now."}}