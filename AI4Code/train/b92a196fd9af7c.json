{"cell_type":{"630f82fa":"code","cc1e28c7":"code","ab2dd7e7":"code","23353106":"code","3f066603":"code","1edffc39":"code","c7f4dc6c":"code","40f7fdd5":"code","000521ae":"code","9bf34ffa":"code","b2e2a453":"code","7437669c":"code","fceaa555":"code","829f63e5":"code","95afcf95":"code","e29f8bdb":"code","92f70da5":"code","19db89bf":"code","15083b93":"code","84401e07":"code","871b0a2c":"code","8ec617b2":"code","555fa597":"code","0e08dbda":"code","66d13ce4":"code","886537a6":"code","088016cb":"code","d48cfe43":"code","8e0e5d60":"code","cbada246":"code","b6563584":"code","b027d63f":"code","06fa0975":"code","c1b8457b":"code","ee7bbafb":"code","3f92da2c":"code","a7f0a524":"code","9a132fdc":"code","87415a5e":"code","000971f5":"code","59787057":"code","fc449167":"code","b1d514bd":"code","f054337a":"code","3c9d4b22":"code","63033beb":"code","4c0a7a36":"code","2cb44d11":"code","438b6177":"code","2f450929":"code","a57c45ca":"code","4d980dc6":"code","38e36cd7":"code","d22bf503":"code","581e7739":"code","305e6ef2":"code","371c8419":"code","2b34f349":"code","edd9d8a1":"code","6fcfd5ec":"code","35f2bd89":"code","172567a9":"markdown","8d16400c":"markdown","8d2667ed":"markdown","f7a92af8":"markdown","2bf233d6":"markdown","23371c8a":"markdown","6cacae9c":"markdown","3c7ff54f":"markdown","cedb7b2b":"markdown","ebb4e0fe":"markdown","6ca512d8":"markdown","767391ca":"markdown","68b20eaf":"markdown","e5bfdff1":"markdown","848cafbf":"markdown","1709b78a":"markdown","d108464b":"markdown"},"source":{"630f82fa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cc1e28c7":"!pip install bert-for-tf2\n!pip install sentencepiece","ab2dd7e7":"try:\n    %tensorflow_version 2.x\nexcept Exception:\n    pass\nimport tensorflow as tf\n\nimport tensorflow_hub as hub\n\nfrom tensorflow.keras import layers\nimport bert","23353106":"question_df = pd.read_csv(\"\/kaggle\/input\/quora-insincere-questions-classification\/train.csv\")\n\nquestion_df.isnull().values.any()\n\nquestion_df.shape","3f066603":"question_df = question_df[:50000]","1edffc39":"question_df.info","c7f4dc6c":"import re\ndef preprocess_text(sen):\n    # Removing html tags\n    sentence = remove_tags(sen)\n\n    # Remove punctuations and numbers\n    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n\n    # Single character removal\n    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n\n    # Removing multiple spaces\n    sentence = re.sub(r'\\s+', ' ', sentence)\n\n    return sentence\n\nTAG_RE = re.compile(r'<[^>]+>')\n\ndef remove_tags(text):\n    return TAG_RE.sub('', text)\n\nquestions = []\nsentences = list(question_df['question_text'])\nfor sen in sentences:\n    questions.append(preprocess_text(sen))","40f7fdd5":"print(question_df.columns.values)","000521ae":"question_df.drop(\"qid\", axis=1, inplace=True)","9bf34ffa":"print(question_df.columns.values)","b2e2a453":"question_df.target.unique()","7437669c":"# text tokenization using the BERT tokenizer\nBertTokenizer = bert.bert_tokenization.FullTokenizer\n\nbert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\",\n                            trainable=False)\nvocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\nto_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = BertTokenizer(vocabulary_file, to_lower_case)","fceaa555":"tokenizer.tokenize(\"don't be so judgmental\")","829f63e5":"tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"dont be so judgmental\"))","95afcf95":"def tokenize_questions(questions):\n    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(questions))","e29f8bdb":"tokenized_questions = [tokenize_questions(question) for question in questions]","92f70da5":"target = question_df['target']","19db89bf":"target = np.array(target)","15083b93":"questions_with_len = [[question, target[i], len(question)]\n                 for i, question in enumerate(tokenized_questions)]","84401e07":"import random\nrandom.shuffle(questions_with_len)","871b0a2c":"questions_with_len.sort(key=lambda x: x[2])","8ec617b2":"sorted_questions_labels = [(question_lab[0], question_lab[1]) for question_lab in questions_with_len]\n","555fa597":"processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_questions_labels, output_types=(tf.int32, tf.int32))\n","0e08dbda":"BATCH_SIZE = 32\nbatched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))","66d13ce4":"next(iter(batched_dataset))","886537a6":"import math\n\nTOTAL_BATCHES = math.ceil(len(sorted_questions_labels) \/ BATCH_SIZE)\nTEST_BATCHES = TOTAL_BATCHES \/\/ 10\nbatched_dataset.shuffle(TOTAL_BATCHES)\ntest_data = batched_dataset.take(TEST_BATCHES)\ntrain_data = batched_dataset.skip(TEST_BATCHES)","088016cb":"class TEXT_MODEL(tf.keras.Model):\n    \n    def __init__(self,\n                 vocabulary_size,\n                 embedding_dimensions=128,\n                 cnn_filters=50,\n                 dnn_units=512,\n                 model_output_classes=2,\n                 dropout_rate=0.1,\n                 training=False,\n                 name=\"text_model\"):\n        super(TEXT_MODEL, self).__init__(name=name)\n        \n        self.embedding = layers.Embedding(vocabulary_size,\n                                          embedding_dimensions)\n        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n                                        kernel_size=2,\n                                        padding=\"valid\",\n                                        activation=\"relu\")\n        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n                                        kernel_size=3,\n                                        padding=\"valid\",\n                                        activation=\"relu\")\n        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n                                        kernel_size=4,\n                                        padding=\"valid\",\n                                        activation=\"relu\")\n        self.pool = layers.GlobalMaxPool1D()\n        \n        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n        self.dropout = layers.Dropout(rate=dropout_rate)\n        if model_output_classes == 2:\n            self.last_dense = layers.Dense(units=1,\n                                           activation=\"sigmoid\")\n        else:\n            self.last_dense = layers.Dense(units=model_output_classes,\n                                           activation=\"softmax\")\n    \n    def call(self, inputs, training):\n        l = self.embedding(inputs)\n        l_1 = self.cnn_layer1(l) \n        l_1 = self.pool(l_1) \n        l_2 = self.cnn_layer2(l) \n        l_2 = self.pool(l_2)\n        l_3 = self.cnn_layer3(l)\n        l_3 = self.pool(l_3) \n        \n        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n        concatenated = self.dense_1(concatenated)\n        concatenated = self.dropout(concatenated, training)\n        model_output = self.last_dense(concatenated)\n        \n        return model_output","d48cfe43":"VOCAB_LENGTH = len(tokenizer.vocab)\nEMB_DIM = 200\nCNN_FILTERS = 100\nDNN_UNITS = 256\nOUTPUT_CLASSES = 2\n\nDROPOUT_RATE = 0.2\n#  hyperparameter that defines the number times that the learning algorithm will work through the entire training dataset.\nNB_EPOCHS = 10","8e0e5d60":"text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n                        embedding_dimensions=EMB_DIM,\n                        cnn_filters=CNN_FILTERS,\n                        dnn_units=DNN_UNITS,\n                        model_output_classes=OUTPUT_CLASSES,\n                        dropout_rate=DROPOUT_RATE)","cbada246":"if OUTPUT_CLASSES == 2:\n    text_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])\nelse:\n    text_model.compile(loss=\"sparse_categorical_crossentropy\",\n                       optimizer=\"adam\",\n                       metrics=[\"sparse_categorical_accuracy\"])","b6563584":"text_model.fit(train_data, epochs=NB_EPOCHS)","b027d63f":"test_loss, test_acc = text_model.evaluate(test_data)\n\nprint('Test Loss: {}'.format(test_loss))\nprint('Test Accuracy: {}'.format(test_acc))","06fa0975":"DROPOUT_RATE = 0.5\ntext_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n                        embedding_dimensions=EMB_DIM,\n                        cnn_filters=CNN_FILTERS,\n                        dnn_units=DNN_UNITS,\n                        model_output_classes=OUTPUT_CLASSES,\n                        dropout_rate=DROPOUT_RATE)\n\ntext_model.compile(loss=\"binary_crossentropy\",\n                       optimizer=\"adam\",\n                       metrics=[\"accuracy\"])\ntext_model.fit(train_data, epochs=NB_EPOCHS)\n","c1b8457b":"test_loss, test_acc = text_model.evaluate(test_data)\n\nprint('Test Loss: {}'.format(test_loss))\nprint('Test Accuracy: {}'.format(test_acc))","ee7bbafb":"#param_grid = dict(num_filters=[32, 64, 128],\n #                 kernel_size=[3, 5, 7],\n  #                vocab_size=[5000], \n   #               embedding_dim=[50],\n    #              maxlen=[100])","3f92da2c":"#VOCAB_LENGTH = len(tokenizer.vocab)\n#EMB_DIM = 200\n#CNN_FILTERS = 100\n#RNN_UNITS = 256\n","a7f0a524":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(VOCAB_LENGTH, 64),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1)\n])","9a132fdc":"model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])","87415a5e":"history = model.fit(train_data, epochs=10,\n                    validation_data=test_data, \n                    validation_steps=30)\n","000971f5":"test_loss, test_acc = model.evaluate(test_data)\n\nprint('Test Loss: {}'.format(test_loss))\nprint('Test Accuracy: {}'.format(test_acc))","59787057":"import matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, metric):\n  plt.plot(history.history[metric])\n  plt.plot(history.history['val_'+metric], '')\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(metric)\n  plt.legend([metric, 'val_'+metric])\n  plt.show()","fc449167":"plot_graphs(history, 'accuracy')","b1d514bd":"plot_graphs(history, 'loss')","f054337a":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(VOCAB_LENGTH, 64),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1)\n])","3c9d4b22":"model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])","63033beb":"history = model.fit(train_data, epochs=10,\n                    validation_data=test_data,\n                    validation_steps=30)","4c0a7a36":"test_loss, test_acc = model.evaluate(test_data)\n\nprint('Test Loss: {}'.format(test_loss))\nprint('Test Accuracy: {}'.format(test_acc))","2cb44d11":"plot_graphs(history, 'accuracy')","438b6177":"plot_graphs(history, 'loss')","2f450929":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(VOCAB_LENGTH, 64),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1)\n])","a57c45ca":"model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])","4d980dc6":"history = model.fit(train_data, epochs=10,\n                    validation_data=test_data,\n                    validation_steps=30)","38e36cd7":"test_loss, test_acc = model.evaluate(test_data)\n\nprint('Test Loss: {}'.format(test_loss))\nprint('Test Accuracy: {}'.format(test_acc))","d22bf503":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(VOCAB_LENGTH, 64),\n    tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'),\n    tf.keras.layers.MaxPooling1D(pool_size=2),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1)\n])\n","581e7739":"model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])","305e6ef2":"history = model.fit(train_data, epochs=10,\n                    validation_data=test_data,\n                    validation_steps=30)","371c8419":"test_loss, test_acc = model.evaluate(test_data)\n\nprint('Test Loss: {}'.format(test_loss))\nprint('Test Accuracy: {}'.format(test_acc))","2b34f349":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(VOCAB_LENGTH, 64),\n    tf.keras.layers.LSTM(200, activation='relu', return_sequences=True, input_shape=(1, 2)),\n    tf.keras.layers.LSTM(100, activation='relu', return_sequences=True),\n    tf.keras.layers.LSTM(50, activation='relu', return_sequences=True),\n    tf.keras.layers.LSTM(25, activation='relu'),\n    tf.keras.layers.Dense(20, activation='relu'),\n    tf.keras.layers.Dense(10, activation='relu'),\n    tf.keras.layers.Dense(1)\n   \n])","edd9d8a1":"model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])","6fcfd5ec":"history = model.fit(train_data, epochs=10,\n                    validation_data=test_data,\n                    validation_steps=30)","35f2bd89":"test_loss, test_acc = model.evaluate(test_data)\n\nprint('Test Loss: {}'.format(test_loss))\nprint('Test Accuracy: {}'.format(test_acc))","172567a9":"Future work: hyperparameter optim.","8d16400c":"Intermediate RNN layers should return full sequence of outputs; 3D tensor by specifying return_sequences=True.","8d2667ed":"## CNN\n\nThe neural net structure is copied from [the colab notebook](https:\/\/colab.research.google.com\/drive\/12noBxRkrZnIkHqvmdfFW2TGdOXFtNePM#scrollTo=VxONsFVHkFLU)\n\nThe first layer is an embedding layer that  is initialized with random weights and will learn an embedding for all of the words in the training dataset.                                                                                                                                                                                                                                    \n\nWe have 3 hidden layers with \"relu\" activation function.\n\nThe first layer has sliding window of size 2.\nThe second layer has sliding window of size 3.\nThe third layer has sliding window of size 4.\n\nThen we have a max pooling layer.\n\nThen we have a densely connected layer.\n\nThe dropout rate is 0.2.\n\n","f7a92af8":"Accuracy now is 95.6%. This is a tiny bit better than our RNN model with only 1 LSTM layer.","2bf233d6":"## RNN Models\nI referenced the models in [Text classification with a RNN](https:\/\/www.tensorflow.org\/tutorials\/text\/text_classification_rnn)","23371c8a":"### Playing with CNN+LSTM Hybrid","6cacae9c":"## Stacked Simple LSTMs","3c7ff54f":"Accuracy on the test dataset now is 96.5%.","cedb7b2b":"Test accuracy is 80% now -- seems like CNN models are not robust and sensitive to drop out rate.","ebb4e0fe":"It's painfully slow to operate on the original dataset. For the sake of efficiency, we only consider a subset of the entire question dataset.","6ca512d8":"Test Accuracy now is 92.5%. This is slightly worse than our CNN model.","767391ca":"## Stacked Three Bidirectional LSTM Layers","68b20eaf":"## Stacked two Bidirectional LSTM layers","e5bfdff1":"We have a pretty high accuracy of 96.6%. Simple LSTMs perform better than Bilateral LSTMs in this case, and seems to be more robust than our CNN model.","848cafbf":"Try a different dropout rate of 0.5.","1709b78a":"### One Bidirectional LSTM Layer","d108464b":"Accuracy now is 94.8%. This is not better than our RNN model with only 2 LSTM layer.\n\nAdding network depths does not seem ideal in improving performances."}}