{"cell_type":{"84abfe86":"code","0d66ec42":"code","edcae2fc":"code","2b9fadb9":"code","4ef12777":"code","e9caf661":"code","19d7939a":"code","505a15e2":"code","66842914":"code","de8f4a97":"code","1430414b":"code","dbac89dc":"code","51886197":"code","eb717657":"code","0070765d":"code","6a30e5ea":"code","e3f370a9":"code","c99cbe68":"code","64406263":"code","40d54284":"code","bd325bad":"code","bd4ba7b5":"code","02fad6a7":"code","9b5c2ff0":"code","441923d8":"code","adf5af7a":"code","309b8e10":"code","a63fde17":"code","80e4455d":"code","63952b7e":"code","c90c0474":"code","6e733928":"code","888c0bc4":"code","5813b60d":"code","36d9c86a":"code","543b78f0":"code","d23625f7":"code","6eca9364":"code","ec641548":"code","164202d0":"markdown","bff09d40":"markdown","1662bedf":"markdown","3b8a8fa5":"markdown","afb87f63":"markdown","7d146836":"markdown","7cac187f":"markdown","72e66ce7":"markdown","fbd2ae8c":"markdown","c4f28c3a":"markdown","1e184420":"markdown","f3fe52c6":"markdown","55f227f6":"markdown","9c701cec":"markdown","5e0bf23b":"markdown","4915a0f1":"markdown","0ecebcfa":"markdown","a79ff2f1":"markdown","22cc7289":"markdown","6f9dc266":"markdown","f3b6d828":"markdown","866e7d09":"markdown","83423729":"markdown","21ffaa01":"markdown","5a477cd7":"markdown","7fd5c160":"markdown"},"source":{"84abfe86":"# IMPORTING MAIN LIBRARIES\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n\n# Defining the main directory\n\nwork_dir = '..\/input\/titanic\/'","0d66ec42":"# IMPORTING DATA\n\ntrain = pd.read_csv(work_dir + 'train.csv')\ntest = pd.read_csv(work_dir + 'test.csv')\n\n# Looking at the data\ntrain.info()","edcae2fc":"train.head()","2b9fadb9":"train.duplicated().sum()","4ef12777":"# Deleting some completely useless features and creating the vector of labels\n\n#labels = train['Survived']\ntrain = train.drop(['PassengerId','Name'], axis = 1)\ntrain.head()","e9caf661":"# Looking at NaN % within the data\n\nnan = pd.DataFrame(train.isna().sum(), columns = ['NaN_sum'])\n#nan['feat'] = nan.index\nnan['Perc(%)'] = (nan['NaN_sum']\/1460)*100\nnan = nan[nan['NaN_sum'] > 0]\nnan = nan.sort_values(by = ['NaN_sum'])\nnan['Usability'] = np.where(nan['Perc(%)'] > 20, 'Discard', 'Keep')\nnan","19d7939a":"# Converting non-numeric predictors stored as numbers into string\n\ntrain['Pclass'] = train['Pclass'].apply(str)\ntrain['SibSp'] = train['SibSp'].apply(str)\ntrain['Parch'] = train['Parch'].apply(str)\n\ntrain['Age'] = train['Age'].fillna(train['Age'].median()) #Filling the missing value with the median\ntrain['Embarked'] = train['Embarked'].fillna('S')         #Filling the 2 missing value with the mode\n\ntrain.isna().sum()","505a15e2":"# Do we have balanced labels ?\n\nsns.set_context(\"paper\")\n\nfigure, ax = plt.subplots(1,3, figsize = (17,5))\nsns.countplot(train['Survived'], ax = ax[0])\nsns.countplot(train['Survived'], hue = train['Sex'], ax = ax[1])\nsns.countplot(train['Survived'], hue = train['Pclass'], ax = ax[2])\nplt.show()","66842914":"# Plotting more informative graphs\n\nsns.catplot(x = 'Pclass', y = 'Survived', hue = 'Sex',\n            data = train, kind = 'bar',height=6, aspect=2,margin_titles = True)\n\nplt.title('Survival Probability by Sex and Class', fontsize = (13))\nplt.ylabel('Survival Probability')\nplt.show()","de8f4a97":"# Checking Normality\n\ndef check_normality(data, name):\n    shap_t,shap_p = stats.shapiro(data)\n    print(name + ' parameters:')\n    print()\n    print(\"Skewness: %f\" % abs(data).skew())\n    print(\"Kurtosis: %f\" % abs(data).kurt())\n    print(\"Shapiro Test: %f\" % shap_t)\n    print(\"Shapiro p_value: %f\" % shap_p)\n    \ncheck_normality(train['Age'],'Age')\nprint('-------------------------')\ncheck_normality(train['Fare'],'Fare')","1430414b":"# Looking at numerical distributions\n\nfigure, ax = plt.subplots(2,2, figsize = (15,8))\n\nsm.qqplot(train['Fare'], stats.t, distargs=(4,),fit=True, line=\"45\", ax = ax[0,0])\nsns.distplot(train['Fare'], kde = True, hist=True, fit = norm, ax=ax[0,1])\nsm.qqplot(train['Age'], stats.t, distargs=(4,),fit=True, line=\"45\", ax = ax[1,0])\nsns.distplot(train['Age'], kde = True, hist=True, fit = norm, ax=ax[1,1])\nplt.show()","dbac89dc":"# Try to normlize with using SquareRoot & log-transformation\n\nfigure, ax = plt.subplots(2,2, figsize = (15,6))\n\nfare_sqrt = np.sqrt(train['Fare'])\nfare_log = np.log1p(train['Fare'])\n\nsm.qqplot(fare_sqrt, stats.t, distargs=(4,),fit=True, line=\"45\", ax = ax[0,0])\nsns.distplot(fare_sqrt, kde = True, hist=True, fit = norm, ax=ax[0,1])\nsm.qqplot(fare_log, stats.t, distargs=(4,),fit=True, line=\"45\", ax = ax[1,0])\nsns.distplot(fare_log, kde = True, hist=True, fit = norm, ax=ax[1,1])\nplt.show()","51886197":"check_normality(fare_sqrt,'Fare_sqrt')\nprint('-------------------------')\ncheck_normality(fare_log,'Fare_log')","eb717657":"# Try to normlize with using SquareRoot & log-transformation\n\nfigure, ax = plt.subplots(2,2, figsize = (15,6))\n\nage_sqrt = np.sqrt(train['Age'])\nage_log = np.log1p(train['Age'])\n\nsm.qqplot(age_sqrt, stats.t, distargs=(4,),fit=True, line=\"45\", ax = ax[0,0])\nsns.distplot(age_sqrt, kde = True, hist=True, fit = norm, ax=ax[0,1])\nsm.qqplot(age_log, stats.t, distargs=(4,),fit=True, line=\"45\", ax = ax[1,0])\nsns.distplot(age_log, kde = True, hist=True, fit = norm, ax=ax[1,1])\nplt.show()","0070765d":"check_normality(age_sqrt,'Age_sqrt')\nprint('-------------------------')\ncheck_normality(age_log,'Age_log')","6a30e5ea":"# Plotting AGE by our target\n\nfigure, ax = plt.subplots(1,3, figsize = (15,6))\n\nsns.stripplot(x = train['Survived'], y = train['Age'], ax = ax[0])\nsns.violinplot(x = train['Survived'], y = train['Age'], ax = ax[1])\nsns.boxplot(x = train['Survived'], y = train['Age'], ax = ax[2])\n\nplt.show()","e3f370a9":"figure, ax = plt.subplots(1,3, figsize = (15,6))\n\nsns.stripplot(x = train['Survived'], y = train['Fare'], ax = ax[0])\nsns.violinplot(x = train['Survived'], y = train['Fare'], ax = ax[1])\nsns.boxplot(x = train['Survived'], y = train['Fare'], ax = ax[2])\n\nplt.show()","c99cbe68":"# BASIC FEATURE ENGINEERING\n\nlabels = train['Survived']\n\ntrain['Age'] = age_log\ntrain['Fare'] = fare_log\n\ntrain['FareClass'] = train['Pclass'].astype('int64')*train['Fare']\ntrain['FareAge'] = train['Age']*train['Fare']\n\n#DELETING OTHER USELESS FEATURES\n\ntrain = train.drop(['Cabin','Ticket','Survived'],axis = 1)\n\n# DEALING WITH CATEGORICAL VARIABLES\n\ntrain_dummy = pd.get_dummies(train)\ntrain_dummy['Parch_9'] = np.zeros((891,), dtype='uint8')\n\n# SORTING COLUMNS ALPHABETICALLY\n\ntrain_dummy = train_dummy.reindex(sorted(train_dummy.columns), axis=1)\ntrain_dummy.head()","64406263":"# Outliers\n\nfrom sklearn.ensemble import IsolationForest\n\niso = IsolationForest(contamination=0.1)\noutliers = iso.fit_predict(train_dummy)\ntrain_dummy['outliers'] = outliers\n\ntrain_dummy.head()","40d54284":"# Plotting Outliers\n\nplt.figure(figsize = (10,5))\nplt.title('Isolation Forest Outliers', fontsize = 13)\nsns.scatterplot(x=np.expm1(train_dummy['Age']),y=np.expm1(train_dummy['Fare']),\n                hue = train_dummy['outliers'], palette=\"deep\")\nplt.show()","bd325bad":"# Removing the outliers\n\ntrain_dummy['labels'] = labels\ndata_cleaned = train_dummy.drop(train_dummy[train_dummy['outliers'] == -1].index)\nlabels = data_cleaned['labels']\ndata_cleaned = data_cleaned.drop(['outliers','labels'], axis = 1)\n","bd4ba7b5":"# MODELING\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nimport shap\nfrom catboost import Pool\nfrom sklearn.svm import SVC\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import StackingClassifier\n\nfrom sklearn.model_selection import RandomizedSearchCV","02fad6a7":"# PERFORMING 30-FOLD CROSS VALIDATION\n\ndef cross_val(data, labels, kf, model):\n    \n    cv_scores = []\n    \n    for i in range(0,kf):\n        \n        X_train,X_val,y_train,y_val = train_test_split(data,labels,\n                                               test_size = 0.1,\n                                               stratify = labels,\n                                               shuffle = True)\n        mod = model\n        model = mod.fit(X_train,y_train)\n        mod_pred = model.predict(X_val)\n        mod_score = model.score(X_val,y_val)\n        cv_scores.append(mod_score)\n        \n    return cv_scores\n\nlogistic_reg = cross_val(data_cleaned, labels, 30, LogisticRegression())\nSGD_Class = cross_val(data_cleaned, labels, 30, SGDClassifier())\ndecision_tree = cross_val(data_cleaned, labels, 30, DecisionTreeClassifier())\nrandom_forest = cross_val(data_cleaned, labels, 30, RandomForestClassifier(n_estimators=100))\nSVClass = cross_val(data_cleaned, labels, 30, SVC())\nGradBoost = cross_val(data_cleaned,labels, 30, GradientBoostingClassifier())\nCatBoost = cross_val(data_cleaned, labels, 30, CatBoostClassifier())\n\ncv_scores = pd.DataFrame(columns = ['Logistic_Class','SGDClassifier',\n                                    'DecisionTreeCl','RandomF_Class','SV_Class',\n                                    'GradientBoost','Catboost'])\n\ncv_scores['Logistic_Class'] = logistic_reg\ncv_scores['SGDClassifier'] = SGD_Class\ncv_scores['DecisionTreeCl'] = decision_tree\ncv_scores['RandomF_Class'] = random_forest\ncv_scores['SV_Class'] = SVClass\ncv_scores['GradientBoost'] = GradBoost\ncv_scores['Catboost'] = CatBoost\n","9b5c2ff0":"cv_scores","441923d8":"# CV_means\n\nlogistic_mean = np.mean(logistic_reg)\nSGD_mean = np.mean(SGD_Class)\ndecision_tree_mean = np.mean(decision_tree)\nrandomf_mean = np.mean(random_forest)\nSVC_mean = np.mean(SVClass)\nGradBoost_mean = np.mean(GradBoost)\nCatBoost_mean = np.mean(CatBoost)\n\ncv_means = [logistic_mean,SGD_mean,decision_tree_mean,randomf_mean,SVC_mean,GradBoost_mean,CatBoost_mean]\n\nmeans = pd.DataFrame(cv_scores.columns, columns = ['Algorithms'])\nmeans['cv_means'] = cv_means\nmeans = means.sort_values(by=['cv_means'], ascending=False)\nmeans","adf5af7a":"# Dividing the data into Training & Validation sets\n\nX_train,X_val,y_train,y_val = train_test_split(data_cleaned,labels,\n                                               test_size = 0.1,\n                                               stratify = labels,\n                                               random_state = 42)","309b8e10":"### Fitting Gradient Boost Classifier\n\ngbc = GradientBoostingClassifier(n_estimators=100,\n                                 learning_rate= 0.2,\n                                 max_depth=3,\n                                 random_state=42)\n\ngbc_model = gbc.fit(X_train,y_train)\ngbc_model.predict(X_val)\ngbc_score = gbc_model.score(X_val,y_val)\ngbc_score","a63fde17":"# Let's take a look at which are the most important features for the Gradient Boosting Classifier \n\nimp_feat_gbc = pd.DataFrame(sorted(zip(gbc_model.feature_importances_,\n                                       X_train.columns),reverse=True),columns = ['Values','Features'])\nimp_feat_gbc\n\nplt.figure(figsize = (14,8))\nplt.title(\"GradientBoost Features' importance\")\nsns.barplot(imp_feat_gbc['Values'],imp_feat_gbc['Features'], orient = 'h')\nplt.show()","80e4455d":"# Catboost Classifier\n\nparams = {'loss_function':'Logloss',\n          'eval_metric':'Accuracy',\n          #'cat_features': cat_features,\n          'verbose': 200,\n          'random_seed': 42\n         }\n\ncat = CatBoostClassifier(**params)\ncat_model = cat.fit(X_train,y_train,\n                     eval_set = (X_val,y_val),\n                     plot=True,\n                     use_best_model=True,\n                     verbose = 0)","63952b7e":"cat_pred = cat_model.predict(X_val)\ncat_model.score(X_val,y_val)","c90c0474":"# Features' importance of our model\n\nfeat_imp = cat_model.get_feature_importance(prettified=True)\nfeat_imp\n\n# Plotting top 20 features' importance\n\nplt.figure(figsize = (14,8))\nplt.title(\"CatBoost Features' importance\")\nsns.barplot(feat_imp['Importances'],feat_imp['Feature Id'], orient = 'h')\nplt.show()","6e733928":"# Feature importance Interactive Plot \n\ntrain_pool = Pool(X_train)\nval_pool = Pool(X_val)\n\nexplainer = shap.TreeExplainer(cat_model) # insert your model\nshap_values = explainer.shap_values(train_pool) # insert your train Pool object\n\nshap.summary_plot(shap_values, X_train)","888c0bc4":"# Stacked Classifier\n\nestimators = [('Catboost',CatBoostClassifier(**params)),\n              ('GradientBoost',GradientBoostingClassifier(n_estimators=100,\n                                                          learning_rate= 0.2,\n                                                          max_depth=3,\n                                                          random_state=42))]\n\nstack_gen = StackingClassifier(estimators = estimators,\n                              final_estimator = CatBoostClassifier(**params))\n\nstack_model = stack_gen.fit(X_train,y_train)","5813b60d":"stack_score = stack_model.score(X_val,y_val)\nstack_pred = stack_model.predict(X_val)\nstack_score","36d9c86a":"# Plotting the confusion matrix of the results\n\nconf_mx = confusion_matrix(y_val,stack_pred)\n\nheat_cm = pd.DataFrame(conf_mx, columns=np.unique(y_val), index = np.unique(y_val))\nheat_cm.index.name = 'Actual'\nheat_cm.columns.name = 'Predicted'\nplt.figure(figsize = (6,5))\nsns.set(font_scale=1.4) # For label size\nsns.heatmap(heat_cm, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g')# font size\nplt.show()","543b78f0":"# Classification Report\n\nprint(classification_report(y_val, stack_pred))","d23625f7":"# Test preparation\n\ntest = pd.read_csv(work_dir + 'test.csv')\ntest_id = test['PassengerId']\ntest = test.drop(['PassengerId','Name'],axis = 1)\n\ntest['Pclass'] = test['Pclass'].apply(str)\ntest['SibSp'] = test['SibSp'].apply(str)\ntest['Parch'] = test['Parch'].apply(str)\n\ntest['Age'] = test['Age'].fillna(test['Age'].median())\ntest['Fare'] = test['Fare'].fillna(test['Fare'].median())\n\ntest['Age'] = np.log1p(test['Age'])\ntest['Fare'] = np.log1p(test['Fare'])\n\n# FEATURE ENGINEERING\n\ntest['FareClass'] = test['Pclass'].astype('int64')*test['Fare']\ntest['FareAge'] = test['Age']*test['Fare']\n\n#DELETING OTHER USELESS FEATURES\n\ntest = test.drop(['Cabin','Ticket'],axis = 1)\ntest['Embarked'] = test['Embarked'].fillna('S') #Filling the 2 missing value with the mode\n\n# DEALING WITH CATEGORICAL VARIABLES\n\ntest_dummy = pd.get_dummies(test)\n#test_dummy['Parch_9'] = np.zeros((891,), dtype='uint8')\n\n# SORTING COLUMNS ALPHABETICALLY\n\ntest_dummy = test_dummy.reindex(sorted(test_dummy.columns), axis=1)\ntest_dummy.head()","6eca9364":"# Test CSV Submission\n\nsubmission = pd.DataFrame(test_id, columns = ['PassengerId'])\npred = stack_model.predict(test_dummy)\nsubmission['Survived'] = pred\nsubmission.head()","ec641548":"submission.to_csv(\"result.csv\", index = False, header = True)","164202d0":"Looking at the results of the stacked model we can see that the performance do not really increase and remains steady with a final accuracy of 87.6 %. THAT'S GOOD!","bff09d40":"**EDA**\n\nNow, let's use some other visualizations to gain more information:","1662bedf":"These visualizations represent the same comparison in different ways, all of them furnish us different information:\n\n- Boxplots (shows us the main statistic: median,min-max,1st & 3rd quartiles and the outliers)\n- Violin plot (shows us the same values of the boxplot plus the way they are distributed)\n- Stripplont (shows us the density of the observations)","3b8a8fa5":"Now let's see if the plots confirm what obtained using the test and the parameters:","afb87f63":"The log transformation results to be definitely better than the sqrt one. Nevertheless, the Shapiro test for normality says that our distributions are still not normally distributed. ","7d146836":"The above diagram represents each observation (x-axis) for the feature presented (y-axis). The x location of each dot on the x-axis reflects the impact of that feature on the model's predictions, while the color of the dot represents the value of that feature for that exact observation. Dots that pile up on the line show density. Here we can see how features such as 'SexFemale','SexMale' or 'Fare', differently from 'SibSp' and 'Parch', do not contribute significantly in producing the final predictions.","7cac187f":"The plots confirm that our numerical variables are not normally distributed. Considering that in general Ml algorithms work better with normal data, the best way to solve this problem is by applying transformations. Let's try using ***sqrt*** and ***log*** transformation and see which one works best.","72e66ce7":"Now, before starting with our Exploratory Data Analysis (EDA), we delete some features that won't help us in achieving our final aims. Note that the rule is that we can delete data only when we are SUPER SURE that we are not deleting informative data. In this case, the deleted features contain nominal values such as the name of the passenger, information that is completely useless to our aim.","fbd2ae8c":"Here I'll use Isolation Forest, a specific algorithm that identifies outliers. More on that here: https:\/\/en.wikipedia.org\/wiki\/Isolation_forest","c4f28c3a":"Now that we have an idea of which type of data we are working with, we can take a look at how they are presented:","1e184420":"Now we can investigate our numerical variables and their correlations. First, we want to see if our numerical variables are normally distributed. To do so we can use the **Shapiro test** (a statistical test to check normality) and look at the **Kurtosis** and **Skewness** parameters:\n\nN.B: In literature, acceptable values for skewness are between -0.5 and 0.5 while -2 and 2 for Kurtosis.","f3fe52c6":"### SUBMISSION","55f227f6":"Now that we have two good models, what we could try is to combine them to see if merged they can perform better than when taken individually.","9c701cec":"Now, how can we deal with these missing values? There is no golden rule, each dataset has its best solution, so it's up to us. Here we have two out of three features where we can deal with the problem and one where the percentage of missing data is just too high. Among the two with a reasonable number of missing data, I decided to impute the missing values using the median of values (for the numeric variable) and the mode (the most frequent observation) for the categorical one.","5e0bf23b":"The confusion matrix is one of the best ways to understand how the model is performing and where it makes mistakes. One of the limits of the confusion matrix is that, in multiclass classification tasks, if there are many classes it could be hard to interpret. For this reason, a better way to look at the performance metrics is to use the classification report.","4915a0f1":"![image.png](attachment:image.png)","0ecebcfa":"Now, before looking at the data we want to know if our variables contain missing data. We already know that from the info part, but we want to be a little more specific:\n- where are they?\n- which percentage of data are we missing per feature?","a79ff2f1":"Remember that there are more efficient ways to deal with missing data, but this is a beginner notebook and I want to keep it as easy as I can. Now let's investigate our labels!\n\n**EDA:**","22cc7289":"Let's see if we have duplicates in our data:","6f9dc266":"## WHO SURVIVED THE TITANIC DISASTER ?\n\nAhoy Folks! Welcome to my notebook! This is a beginner notebook aimed to show how to deal with the data and how to use some ML algorithms to complete the classification task. So, let's not waste time and start with the work!\n\nDon't forget to upvote and comment if you have questions or advice!","f3b6d828":"Now, in order to correctly predict the test data, we need to use the same pipeline (set of transformations) used for the training data. The only part skipped here is the dropping of outliers, simply because we cannot drop test observations. This is why our model will not perform too well on the test set.","866e7d09":"How to deal with outliers? There are different ways through which we can approach them: \n\n- Imputing them (treating them as further missing values)\n- Keeping them (and evaluating the consequences of the choice)\n- Dropping them (always last choice)\n\nHere, for sake of simplicity I decided to drop them. ","83423729":"**FEATURE ENGINEERING**\n\nThe idea behind feature engineering is to create new features that could help the model in completing the task. This becomes helpful especially when the dataset contains a lot of features and we can use the new features, deleting the raw ones and thus, reducing the dimensionality of data.","21ffaa01":"From our visualizations, we can obtain some important insights:\n\n- The number of saved people is higher (luckily) if compared with the one of dead people (This makes our dataset unbalanced).\n\n- Female passengers died in higher percentage and were saved in lower percentage respect to their male counterpart. Nevertheless, this result needs to be analyzed in relation to the number of males and females passengers on the ship (male passengers were double the number of females').\n\n- It seems that people in the 3rd class survived more than people in other classes, but also here, we need to consider that the number of people in the 3rd class is higher if compared with others.\n","5a477cd7":"What we can see looking at the features is that the features that have a higher impact on the model comprehend:\n\n- The gender of passenger\n- The Age of passenger\n- The Fare paid by passenger\n- The class\n\nall the others together explain a small part of the variance in the data. Interesting is also the fact that the features created in the part of feature engineering seem to be quite important for the model.","7fd5c160":"Now that we have an idea of our baseline values for each of the selected classifiers, we can take the most efficient and try to make them ever more efficient optimizing their hyperparameters:"}}