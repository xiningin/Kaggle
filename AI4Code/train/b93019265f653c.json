{"cell_type":{"a36f5476":"code","78cfb4ad":"code","4d6e3e93":"code","584be79a":"code","960d3caa":"code","d4804cd5":"code","47aed73f":"code","fbe99d5a":"code","7f661c00":"code","2bff66e4":"code","12d578de":"code","d2ca8331":"code","72f875d2":"code","2198618b":"code","649c87be":"code","9bee710a":"code","20adda88":"code","aee720ab":"code","0918f362":"markdown","52bb9fcf":"markdown","e96e8b99":"markdown","28a09f56":"markdown","63118bf7":"markdown","c65ffe06":"markdown","1ef530ae":"markdown","b427bad4":"markdown","2d7cde2a":"markdown","d9fb76c3":"markdown","4615160f":"markdown","a9fb3776":"markdown","50608170":"markdown","13ddf82c":"markdown","89661176":"markdown","c3e65897":"markdown","ea35476f":"markdown","6daa10a5":"markdown","3f3e7a4c":"markdown","25f6e75d":"markdown"},"source":{"a36f5476":"import numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport xgboost as xgb\n        \n%load_ext skip_kernel_extension_py","78cfb4ad":"#Read data into DataFrames.\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\", index_col=0)","4d6e3e93":"#Split features and target.\ny = train['claim']\nX = train.drop('claim', axis=1)","584be79a":"#Display samples of DataFrames.\nfor i in [train, test]:\n    display(i.head())","960d3caa":"#Dislpay Nulls.\nprint(f'\\nRows with NaNs in training set: {train.isnull().any(axis=1).sum()}')\nprint(f'Columns with NaNs in training set: {train.isnull().any(axis=0).sum()}')\nprint(f'Rows in training set: {len(train)}')\nprint(f'\\nRows with NaNs in testing set: {test.isnull().any(axis=1).sum()}')\nprint(f'Columns with NaNs in testing set: {test.isnull().any(axis=0).sum()}')\nprint(f'Rows in testing set: {len(test)}')","d4804cd5":"#Examine distribution of target values.\ndisplay(y.value_counts())","47aed73f":"#Create correlation map.\ncorrmap = train.corr()\ncorr_mask = np.triu(corrmap)\nf, ax = plt.subplots(figsize=(15, 10))\ncorr_plot = sns.heatmap(corrmap, square=True, mask=corr_mask, cmap=\"Blues\")\nplt.show()","fbe99d5a":"#Compare distributions for all features.\nplt.figure(figsize=(24, 6*(118\/4)))\nfor i in tqdm(range(len(X.columns.tolist()))):\n    plt.subplot(30, 4, i+1)\n    sns.histplot(X[f'f{i+1}'], kde=True)\n    sns.histplot(test[f'f{i+1}'], kde=True, color='green')\nplt.show()","7f661c00":"%%skip True\n\n#Create new feature as total NaNs in each row.\n#Thank you to BIZEN for this idea.\nX['nan_count'] = X.isna().sum(axis=1)\ntest['nan_count'] = test.isna().sum(axis=1)","2bff66e4":"%%skip True\n\n#Impute NaNs using SimpleImputer with mean.\nimputer = SimpleImputer(strategy='mean')\nX = pd.DataFrame(imputer.fit_transform(X), index=X.index, columns=X.columns)\ntest = pd.DataFrame(imputer.transform(test), index=test.index, columns=test.columns)\n\nX.to_csv('imputed_train_blending.csv',index=False)\ntest.to_csv('imputed_test_blending.csv', index=False)","12d578de":"#Load imputed datasets.\ntest = pd.read_csv(\"..\/input\/imputed-data-blended\/imputed_test_blending.csv\")\nX = pd.read_csv(\"..\/input\/imputed-data-blended\/imputed_train_blending.csv\")\ndisplay(X.head())\ndisplay(test.head())","d2ca8331":"#Split data into training and validation sets.\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=12345)","72f875d2":"models = [('dtc', DecisionTreeClassifier()),\n        ('gnb', ExtraTreesClassifier()),\n        ('ada', AdaBoostClassifier()),\n        ('rf', RandomForestClassifier())]","2198618b":"def fit_models(models, X_train, X_valid, y_train, y_valid):\n\n    #Create variable in which to store predictions for meta-model.\n    preds_for_meta = []\n    \n    #Loop through models in model list.\n    for name, model in tqdm(models):\n        \n        #Fit model and obtain predictions.\n        model.fit(X_train, y_train)\n        pred = model.predict_proba(X_valid)[:, 1]\n        \n        #Obtain base moedl roc score.\n        roc_base = roc_auc_score(y_valid, pred)\n        \n        print(f'{model} score: {roc_base}')\n        \n        #Reshape prediction into single-column matrix.\n        pred = pred.reshape(len(pred), 1)\n        \n        #Append prediction to varible for meta-model.\n        preds_for_meta.append(pred)\n        \n    #Create 2D array from predictions.\n    meta_features = np.hstack(preds_for_meta)\n    \n    #Define blender for model.\n    meta_model = xgb.XGBClassifier(n_estimators=7000,\n                                 #tree_method='gpu_hist', \n                                 #gpu_id = 0,\n                                 random_state = 5,\n                                 learning_rate=.03)\n    \n    #Fit meta model on predictions from base models.\n    meta_model.fit(meta_features, y_valid.values.ravel(),\n                 verbose=False,\n                 eval_set=[(meta_features, y_valid.values.ravel())],\n                 eval_metric='auc',\n                 early_stopping_rounds=300)\n    \n    print(f'Meta AUC: {roc_auc_score(y_valid, meta_model.predict_proba(meta_features)[:, 1])}')\n    \n    return meta_model","649c87be":"def meta_predict(models, meta_model, X_test):\n    \n    #Variable to store base models' test predictions.\n    preds_for_meta = []\n    \n    #Loop through models to make predictions.\n    for name, model in tqdm(models):\n        \n        #Make predictions with base models.\n        pred = model.predict(X_test)\n        \n        #Reshape prediction.\n        pred = pred.reshape(len(pred), 1)\n        \n        #Append predition to meta-model prediction list.\n        preds_for_meta.append(pred)\n        \n    #Reshape all predictions into 2D array.\n    meta_features = np.hstack(preds_for_meta)\n    \n    #Make prediction using meta-model.\n    meta_preds = meta_model.predict_proba(meta_features)[:, 1]\n    \n    return meta_preds","9bee710a":"%%time\nmeta_model = fit_models(models, X_train, X_valid, y_train, y_valid)","20adda88":"test_pred = meta_predict(models, meta_model, test)","aee720ab":"test = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv\", usecols=['id'], low_memory=False)\npredictions = pd.DataFrame()\npredictions[\"id\"] = test['id']\npredictions[\"claim\"] = test_pred\npredictions.to_csv('submission.csv', index=False, header=predictions.columns)","0918f362":"### **Build Blending Ensemble Model** ","52bb9fcf":"**Step 1: Create list of models to be used**\n\nHere we make a list of base models (aka level-0) models to be used for our blending ensemble. While we believe that blending the output of gradient boosting models would provide the best results, we are chosing these simpler models to preserve time, as this notebook is just meant to serve as an introducting to blending. In a later notebook, we will stack gradient boosting models using Sklearn's StackingClassifier; we will edit this notebook with a link to the other one in due course.\n\nNote, we are not providing any customized hyperparameter values for these models. If one were to use ensemble blending as their method of choice for a Kaggle competition, then it is highly recommended that one find the optimal hyperparameters for each base model using optuna or hyperopt.","e96e8b99":"**Import Necessary Libraries\/Modules**","28a09f56":"We can see that there is no real correlation between any of the features, nor between any feature and the target. Since there is no real inter-feature correlation, we don't have any redundant information, which means we can use all features. This will likely prove more useful than specifically selecting features with the highest correlations with the target, since none of the features show any significant correlation in this regard. ","63118bf7":"The overall aim of this notebook was to demonstrate how one would build a blending ensemble classification model. This involved preprocessing our data, feature engineering, and creating custom functions to train and evaluate base-models, as well as our meta model. The latter model was trained with features which were each comprised of predictions made by base models - to avoid long waits, we used simple Sklearn models as our base models. Overall, one can clearly see how blending models can provide a greater AUC-ROC score than any of the individual base models.\n\nIf one were to use this method for an actual competition, or for business purposes, then it is highly recommended that one try various base models and select those which provide the best results in the context of one's end goal. With the chosen base models, one should attempt to find the optimal hyperparameters for each model - using either hyperopt or optuna is highly recommended. Finally, one should use the optmized base models and try to determine the best hyperparameters for the meta-model.\n\nThank's for reading through this notebook. If anyone has any suggests for improvement, they would be much appreciated!","c65ffe06":"From what we can see here, there is nearly an equal distribution of classes in the training dataset. As such, we don't have to perform upsample\/downsampling or SMOTEing. ","1ef530ae":"As we can see, the AUC-ROC score for the meta-model is approximately .8682, which is better than any of the individual base models' scores; additionally, this score is not just the average of the base models' scores. While the difference between the meta-model's AUC-ROC score and the base models' scores seems, prima facie, quite small, such a difference may prove significant in the context of Kaggle competitions. ","b427bad4":"**Exploratory Data Analysis**","2d7cde2a":"**Step 4: Obtain meta-model from fit_models function**\n\nIn the following cell, we simply execute our fit_models function, which returns our meta-model.","d9fb76c3":"**Step 5: Obtain prediction from test dataset**\n\nFinally, with our meta-model developed, we simply execute the meta_predict function to obtain the predictions on the test set.","4615160f":"**Step 3: Create function to make predictions with meta-model on test set**\n\nThis function takes as its inputs the list of base models, our meta-model, and the test dataset. We obtain predictions from each base model, reshape the prediction, and append it to a list. The list is then stacked in order to serve as features for the meta-model. Finally, we obtain predictions from the meta-model on the test set, though indirectly by using the base-models' predictions as features. We return the meta-model's predictions.","a9fb3776":"**Submit Prediction**","50608170":"**Feature Engineering**","13ddf82c":"# Simple Blending Ensemble for Classification","89661176":"Blending and stacking have become increasingly popular methods in Kaggle competitions. These methods both involve using the predictions made by various base models as features for a meta-model, which provides a final prediction. In this notebook, we create a simple blended model for Kaggle's \"Tabular Playground Series - Sep 2021\" competition. This notebook is meant to serve as an example on how to blend models; as such, we will be utilizing less computationally expensive base models to generate predictions.  \n\nPrior to jumping in, it should be noted that there is a subtle difference between model blending and stacking, which can be summarized as follows:\n\n**Stacked Model**: An ensemble of models in which the meta-model is trained on out-of-fold predictions made by the base models during k-fold cross validation. \n\n**Blended Model**: An ensemble of models in which the meta-model is trained on predictions made by the base models on a holdout dataset (e.g., the validation dataset).","c3e65897":"**Import Datasets**","ea35476f":"Note: The above cells were originally run when attempting to fill NaNs while adding an extra feature that conveys how many NaNs were originally in each observation. In order to save time, we imported a custom script while allows us to use the '%%skip' cell-magic command to skip the execution of certain cells while preserving the notebook's appearance. Rather than running the above cells, we will instead import the pre-imputed data that we saved during the previous execution.","6daa10a5":"## Conclusion","3f3e7a4c":"The distributions of values for all features appears to be quite similar for both training and testing datasets. As such, there does not appear to be any dataset shift. Additionally, we expect there to be a similar distribution of target values between the training and testing datasets.","25f6e75d":"**Step 2: Ceate function to fit models**\n\nThis function fits each base model to the training dataset and obtains predictions from the valiadtion dataset. Unlike with stacking, where one uses out-of-fold predictions as features for one's meta-model, as well as predictions on the full test set with models fitted to the whole trainng set, with simple blending one only needs to use the base-models' predictions from the validation test set as the meta-models features. \n\nBelow, each model's predictions are reshaped to (len(prediction), 1) and appended to a list. The predictions are then stacked, thus forming an array with each individual base-model's predictions as a feature. Finally, we fit a meta-model (in this case an XGBoost classifier) to the array of base-model predictions and the actual validation target values, and we return the fitted model."}}