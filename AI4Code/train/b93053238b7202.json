{"cell_type":{"390ab2cd":"code","1f9d241d":"code","0b364434":"code","89bc7676":"code","7727121c":"code","c8d4b9d7":"code","3b12fbb3":"code","12e4daa8":"code","2b182edf":"code","0351fd0f":"code","47c751d5":"code","dff98e9a":"code","2200a9a2":"code","f3156167":"code","4840e12d":"code","0ddcab8b":"code","8d0ad869":"code","bbeaadc8":"code","71b9e949":"code","7dfa83cb":"code","5f75ccb9":"code","5d7e2f09":"code","62b5bacb":"code","eae67b40":"code","f41d0cb5":"code","729e8eca":"code","73a8a494":"code","e372f216":"code","0bdca425":"code","6b31d707":"code","210c6eb1":"code","a94f4fa3":"code","5a359b53":"code","7bbfd7e3":"code","fa788f9c":"code","67fe37dd":"code","864e4087":"code","873555e0":"code","2c8079c6":"code","d7b8474d":"code","e95feb7b":"markdown","978a64c5":"markdown","7850bbbb":"markdown","e1bd5756":"markdown","1d836a6b":"markdown","fa3720a4":"markdown","aca56088":"markdown","87b20086":"markdown","bb2da7b6":"markdown","49ff653c":"markdown","2bfd3ea2":"markdown"},"source":{"390ab2cd":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom matplotlib import pyplot as plt\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","1f9d241d":"%matplotlib inline","0b364434":"plt.style.use('seaborn-whitegrid')","89bc7676":"df = pd.read_csv('..\/input\/graduate-admissions\/Admission_Predict.csv')","7727121c":"pd.set_option('display.max_columns', len(df.columns))\npd.set_option('display.max_rows', 10)","c8d4b9d7":"df.head()","3b12fbb3":"df.shape","12e4daa8":"df.describe()","2b182edf":"df.info()","0351fd0f":"df.drop('Serial No.', axis = 1, inplace = True)","47c751d5":"df.columns","dff98e9a":"df.columns = df.columns.str.strip()","2200a9a2":"df.columns","f3156167":"for i in df.columns:\n    sns.distplot(df[i], fit = norm)\n    plt.show()","4840e12d":"research_exp = df['Research'].value_counts()\n\nplt.figure(figsize = (6,6))\n\nplt.pie(research_exp, labels = research_exp.index.map({0: 'No Experience', \n                                                       1: 'Experienced'}),\n        wedgeprops = {'edgecolor': 'black'}, \n        shadow = True, \n        autopct = '%1.2f%%', \n        textprops = {'fontsize': 15}, \n        explode = np.full(len(research_exp), 0.025), \n        colors = ['#73FC2C', '#FC2C2C'], \n        frame = False);\n\nplt.title('Experienced vs Non Experienced\\n', fontsize = 25)\nplt.show()","0ddcab8b":"uni_rat = df['University Rating'].value_counts()\n\nplt.figure(figsize = (7, 7))\n\nplt.pie(uni_rat, labels = uni_rat.index, \n        wedgeprops = {'edgecolor': 'black'}, \n        textprops = {'fontsize': 20, 'color': 'white'}, \n        shadow = True, \n        autopct = '%1.1f%%', \n        explode = np.insert( 0.5, 0, np.full(len(uni_rat) - 1, 0.025)), \n        startangle = 60, \n        colors = ['#2C51FF', '#506FFE', '#7C93FF', '#A0B1FF', '#002DFF'])\n\nplt.title('Percentage of universities of different ratings', fontsize = 25)\n\nplt.show()","8d0ad869":"sns.boxplot(x = \"Research\", y = \"GRE Score\",\n            palette = 'cool', saturation = 1,\n            data = df)\n\nplt.title('GRE Score according to research experience\\n', fontsize = 20)\n\nplt.xticks([0, 1], ['No Experience', 'Experienced'])\n\nsns.despine(offset = 10, trim = True)","bbeaadc8":"plt.figure(figsize = (7,7))\nsns.scatterplot(df['CGPA'], df['GRE Score'], s = 100, edgecolor = 'black', \n                color = 'red', alpha = 0.5)\nplt.title('GRE Score according to their CGPA\\n', fontsize = 20)\n\nplt.show()","71b9e949":"plt.figure(figsize = (7,7))\nsns.heatmap(df.corr(), \n            square=True, \n            linewidths = 1, \n            cbar_kws={\"shrink\": 0.9}, \n            annot = True)\n\nplt.show()","7dfa83cb":"sns.boxplot(x = \"University Rating\", y = \"GRE Score\",\n            palette = 'Blues', saturation = 1,\n            data = df)\n\nplt.title('GRE Score according to University Rating\\n', fontsize = 20)\n\nplt.show()","5f75ccb9":"plt.figure(figsize = (15, 8))\n\nplt.scatter(df['GRE Score'], df['TOEFL Score'], \n            c = df['CGPA'] * 10,  \n            s = df['Chance of Admit'] * 150, \n            edgecolor = 'black',\n            cmap = 'viridis')\n\nplt.title('Relationship between GRE and TOEFL Scores,\\\n CGPA & Chance of Admission\\n', fontsize = 25)\n\nplt.xlabel('\\nGRE Scores', fontsize = 20)\nplt.ylabel('TOEFL Scores\\n', fontsize = 20)\n\n\ncbar = plt.colorbar()\ncbar.solids.set_edgecolor(\"face\")","5d7e2f09":"from sklearn.model_selection import train_test_split\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, PolynomialFeatures\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\n\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nfrom sklearn.metrics import mean_squared_error\n","62b5bacb":"class Regressors():\n\n    lin_reg = LinearRegression()\n    dt_reg = DecisionTreeRegressor()\n    rf_reg = RandomForestRegressor()\n    svr_reg = SVR()\n    poly_reg  = PolynomialFeatures()\n\n\nclass Tuners():\n\n    random = RandomizedSearchCV\n    grid = GridSearchCV\n\n    \nclass Model(object):\n\n    def __init__(self, df):\n        \n        self.df = df\n        self.X = df.iloc[:, :-1].copy()\n        self.y = df.iloc[:, -1].copy()\n        \n    def train_test_set(self, test_size = 0.2, random_state = 0):\n\n        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, \n                                                            test_size = test_size, \n                                                            random_state = random_state)\n        return X_train, X_test, y_train, y_test\n\n\n\n\n        \nclass Transform_Column(Model):\n\n    # Code precisely written and made it a subclass \n    # in case if called manually\n    # otherwise simply fit() and transform() will work method \n    # without any other logic\n\n    tr = 0\n    temp = 0\n    \n    def __init__(self, df, ohe = ['University Rating'], oe = [], \n                 ss = ['GRE Score', 'TOEFL Score', 'CGPA', 'SOP', 'LOR']):\n        \n        super().__init__(df)\n        \n        self.X_train, self.X_test, self.y_train, self.y_test = self.train_test_set() \n\n        self.ohe = ohe\n        self.oe = oe\n        self.ss = ss\n        \n\n    def __get__(self, x, y):\n        \n        trans_col = make_column_transformer(\n            (OneHotEncoder(handle_unknown = 'ignore'), self.ohe), \n                                (OrdinalEncoder(), self.oe), \n                                (StandardScaler(), self.ss),\n                                remainder = 'passthrough')\n    \n    \n    # To fit this in pipeline we should have atleast fit and transform \n    # or fit_transform and transform method.\n\n    def fit(self, X):\n        Transform_Column.tr += 1\n        Transform_Column.temp = X\n        return self.column_transformer().fit(X)\n    \n    def transform(self, t):\n        self.fit(Transform_Column.temp).transform(t)\n        if Transform_Column.tr != 0:        \n            return self.fit(Transform_Column.temp).transform(t)\n        \n        else:\n            return print('Before calling transform(), call fit() method.')\n    \n    def fit_transform(self, X):\n        Transform_Column.temp = X\n        return self.column_transformer().fit_transform(X)\n\n\nclass Transform():\n    t = Transform_Column(df)\n\n\n\n\nclass Pipelines(Model): \n\n    def __init__(self, df):\n        super().__init__(df)\n\n        self.reg = Regressors()\n\n    def pipe_lin_reg(self):\n        return make_pipeline(Transform.t, self.reg.lin_reg)\n\n    def pipe_poly_reg(self):\n        return make_pipeline(Transform.t, self.reg.poly_reg, self.reg.lin_reg)\n    \n    def pipe_dt_reg(self):\n        return make_pipeline(Transform.t, self.reg.dt_reg)\n\n    def pipe_rf_reg(self):\n        return make_pipeline(Transform.t, self.reg.rf_reg)\n\n    def pipe_svr_reg(self):\n        return make_pipeline(Transform.t, self.reg.svr_reg)\n\n \nclass Hyperparameter_Optimization(Pipelines):\n\n    def ___init__(self, df):\n        super().__init__(df)\n        \n    def tune_lin_reg(self, \n                     tuner = None,\n                     fit_intercept = [True, False],\n                     normalize = [True, False]\n                     ):\n\n        params = {\n            'linearregression__fit_intercept': fit_intercept,\n            'linearregression__normalize': normalize\n        }\n\n\n        if tuner == None:\n            tuner = Tuners().grid\n        else:\n            tuner = tuner\n\n        tuned = tuner(self.pipe_lin_reg(), params, cv = 10, verbose = 10)\n        \n        return tuned\n\n    def tune_poly_reg(self, \n                      tuner = None,\n                      degree = np.arange(1,6),\n                      fit_intercept = [True, False],\n                      normalize = [True, False]\n                      ):\n\n        params = {\n            'polynomialfeatures__degree': degree,\n            'linearregression__fit_intercept': fit_intercept,\n            'linearregression__normalize': normalize\n        }\n\n\n        if tuner == None:\n            tuner = Tuners().grid\n        else:\n            tuner = tuner\n\n        tuned = tuner(self.pipe_poly_reg(), params, cv = 10, verbose = 10)\n        \n        return tuned\n\n\n\n    def tune_dt_reg(self,\n                    tuner = None, \n                    criterion = ['mse'],\n                    splitter = ['best'],\n                    max_depth = [2, 5, 10, 15, 20, 30, 50],\n                    min_samples_split = np.arange(2, 11, 2),\n                    min_samples_leaf = np.arange(1, 6, 1),\n                    min_weight_fraction_leaf = [0.0],\n                    max_features = [None],\n                    random_state = [None],\n                    max_leaf_nodes = [None],\n                    min_impurity_decrease = [0.0],\n                    min_impurity_split = [None],\n                    presort = ['deprecated'],\n                    ccp_alpha = [0.0]\n                    ):\n        \n        params = {\n            'decisiontreeregressor__criterion': criterion,\n            'decisiontreeregressor__splitter': splitter,\n            'decisiontreeregressor__max_depth': max_depth,\n            'decisiontreeregressor__min_samples_split': min_samples_split,\n            'decisiontreeregressor__min_samples_leaf': min_samples_leaf,\n            'decisiontreeregressor__min_weight_fraction_leaf': min_weight_fraction_leaf,\n            'decisiontreeregressor__max_features': max_features,\n            'decisiontreeregressor__random_state': random_state,\n            'decisiontreeregressor__max_leaf_nodes': max_leaf_nodes,\n            'decisiontreeregressor__min_impurity_decrease': min_impurity_decrease,\n            'decisiontreeregressor__min_impurity_split': min_impurity_split,\n            'decisiontreeregressor__presort': presort,\n            'decisiontreeregressor__ccp_alpha': ccp_alpha,\n        }\n\n        if tuner == None:\n            tuner = Tuners().grid\n        else:\n            tuner = tuner\n\n\n        tuned = tuner(self.pipe_dt_reg(), params, cv = 10, verbose = 10)\n        \n        return tuned\n\n\n\n\n    def tune_rf_reg(self, \n                    tuner = None,\n                    n_estimators = [10, 30, 50, 70, 100], \n                    criterion = ['mse'], \n                    max_depth = [None], \n                    min_samples_split = np.arange(2, 11, 2), \n                    min_samples_leaf = [1, 2, 3, 4, 5], \n                    min_weight_fraction_leaf = [0.0], \n                    max_features = ['auto'], \n                    max_leaf_nodes = [None], \n                    min_impurity_decrease = [0.0], \n                    min_impurity_split = [None], \n                    bootstrap = [True], \n                    oob_score = [False], \n                    n_jobs = [None], \n                    random_state = [None], \n                    verbose = [0], \n                    warm_start = [False], \n                    ccp_alpha = [0.0], \n                    max_samples = [None]\n                    ):\n        \n        params = {\n            'randomforestregressor__n_estimators': n_estimators,\n            'randomforestregressor__criterion': criterion,\n            'randomforestregressor__max_depth': max_depth,\n            'randomforestregressor__min_samples_split': min_samples_split,\n            'randomforestregressor__min_samples_leaf': min_samples_leaf,\n            'randomforestregressor__min_weight_fraction_leaf': min_weight_fraction_leaf,\n            'randomforestregressor__max_features': max_features,\n            'randomforestregressor__max_leaf_nodes': max_leaf_nodes,\n            'randomforestregressor__min_impurity_decrease': min_impurity_decrease,\n            'randomforestregressor__min_impurity_split': min_impurity_split,\n            'randomforestregressor__bootstrap': bootstrap,\n            'randomforestregressor__oob_score': oob_score,\n            'randomforestregressor__n_jobs': n_jobs,\n            'randomforestregressor__random_state': random_state,\n            'randomforestregressor__verbose': verbose,\n            'randomforestregressor__warm_start': warm_start,\n            'randomforestregressor__ccp_alpha': ccp_alpha,\n            'randomforestregressor__max_samples': max_samples,\n        }\n\n        \n        if tuner == None:\n            tuner = Tuners().grid\n        else:\n            tuner = tuner\n\n        tuned = tuner(self.pipe_rf_reg(), params, cv = 10, verbose = 10)\n        \n        return tuned\n\n\n\n\n    def tune_svr_reg(self, \n                     tuner = None,\n                     kernel = ['linear', 'rbf', 'poly'],\n                     degree = [3],\n                     gamma = ['scale'],\n                     coef0 = [0.0],\n                     tol = [0.0001, 0.001, 0.01, 1],\n                     C = [0.1, 1.0, 5, 10],\n                     epsilon = [0.1],\n                     shrinking = [True], \n                     cache_size = [200],\n                     verbose = [False],\n                     max_iter = [-1]\n                     ):\n        \n        params = {\n            'svr__kernel': kernel,\n            'svr__degree': degree,\n            'svr__gamma': gamma,\n            'svr__coef0': coef0,\n            'svr__tol': tol,\n            'svr__C': C,\n            'svr__epsilon': epsilon,\n            'svr__shrinking': shrinking,\n            'svr__cache_size': cache_size,\n            'svr__verbose': verbose,\n            'svr__max_iter': max_iter\n        }\n\n        if tuner == None:\n            tuner = Tuners().grid\n        else:\n            tuner = tuner\n\n\n        tuned = tuner(self.pipe_svr_reg(), params, cv = 10, verbose = 10)\n        \n        return tuned\n\n \n\nclass Evaluate(Model):\n\n    def __init__(self, df):\n        super().__init__(df)\n\n    def score_accuracy(self, model, x, y):\n\n        #res = 'Accuracy Score:- {}'.format(model.score(x, y))\n        res = model.score(x, y)\n        return res\n\n    def mse(self, model, x, y):\n        \n        y_pred = model.predict(x)\n        #res = 'Mean Squared Error:- {}'.format(mean_squared_error(y, y_pred))\n        res = mean_squared_error(y, y_pred)\n        return res\n    ","eae67b40":"model = Model(df)\nX1, X2, y1, y2 = model.train_test_set()\nprint(X1.shape, X2.shape, y1.shape, y2.shape)","f41d0cb5":"pipe = Pipelines(df)\nopt = Hyperparameter_Optimization(df)\nev = Evaluate(df)","729e8eca":"pipe_lin = pipe.pipe_lin_reg()\npipe_lin.fit(X1, y1)","73a8a494":"opt_lin = opt.tune_lin_reg()\nopt_lin.fit(X1, y1)","e372f216":"print('Before Hyperparameter Tuning:- \\n')\nprint('Score of Training Set:- {}'.format(ev.score_accuracy(pipe_lin, X1, y1)))\nprint('Score of Test Set:- {}'.format(ev.score_accuracy(pipe_lin, X2, y2)))\nprint('MSE of Training Set:- {}'.format(ev.mse(pipe_lin, X1, y1)))\nprint('MSE of Test Set:- {}\\n\\n'.format(ev.mse(pipe_lin, X2, y2)))\n\nprint('After Hyperparameter Tuning:- \\n')\nprint('Score of Training Set:- {}'.format(ev.score_accuracy(opt_lin, X1, y1)))\nprint('Score of Test Set:- {}'.format(ev.score_accuracy(opt_lin, X2, y2)))\nprint('MSE of Training Set:- {}'.format(ev.mse(opt_lin, X1, y1)))\nprint('MSE of Test Set:- {}\\n\\n'.format(ev.mse(opt_lin, X2, y2)))","0bdca425":"pipe_poly = pipe.pipe_poly_reg()\npipe_poly.fit(X1, y1)","6b31d707":"opt_poly = opt.tune_poly_reg()\nopt_poly.fit(X1, y1)","210c6eb1":"print('Before Hyperparameter Tuning:- \\n')\nprint('Score of Training Set:- {}'.format(ev.score_accuracy(pipe_poly, X1, y1)))\nprint('Score of Test Set:- {}'.format(ev.score_accuracy(pipe_poly, X2, y2)))\nprint('MSE of Training Set:- {}'.format(ev.mse(pipe_poly, X1, y1)))\nprint('MSE of Test Set:- {}\\n\\n'.format(ev.mse(pipe_poly, X2, y2)))\n\nprint('After Hyperparameter Tuning:- \\n')\nprint('Score of Training Set:- {}'.format(ev.score_accuracy(opt_poly, X1, y1)))\nprint('Score of Test Set:- {}'.format(ev.score_accuracy(opt_poly, X2, y2)))\nprint('MSE of Training Set:- {}'.format(ev.mse(opt_poly, X1, y1)))\nprint('MSE of Test Set:- {}\\n\\n'.format(ev.mse(opt_poly, X2, y2)))\n\n","a94f4fa3":"pipe_dt = pipe.pipe_dt_reg()\npipe_dt.fit(X1, y1)","5a359b53":"opt_dt = opt.tune_dt_reg()\nopt_dt.fit(X1, y1)","7bbfd7e3":"print('Before Hyperparameter tuninig:- \\n')\nprint('Score of Training Set:- {}'.format(ev.score_accuracy(pipe_dt, X1, y1)))\nprint('Score of Test Set:- {}'.format(ev.score_accuracy(pipe_dt, X2, y2)))\nprint('MSE of Training Set:- {}'.format(ev.mse(pipe_dt, X1, y1)))\nprint('MSE of Test Set:- {}\\n\\n'.format(ev.mse(pipe_dt, X2, y2)))\n\nprint('After Hyperparameter tuninig:- \\n')\nprint('Score of Training Set:- {}'.format(ev.score_accuracy(opt_dt, X1, y1)))\nprint('Score of Test Set:- {}'.format(ev.score_accuracy(opt_dt, X2, y2)))\nprint('MSE of Training Set:- {}'.format(ev.mse(opt_dt, X1, y1)))\nprint('MSE of Test Set:- {}\\n\\n'.format(ev.mse(opt_dt, X2, y2)))","fa788f9c":"pipe_rf = pipe.pipe_rf_reg()\npipe_rf.fit(X1, y1)","67fe37dd":"opt_rf = opt.tune_rf_reg()\nopt_rf.fit(X1, y1)","864e4087":"print('Before Hyperparameter Tuning:- \\n')\nprint('Score of Training Set:- {}'.format(ev.score_accuracy(pipe_rf, X1, y1)))\nprint('Score of Test Set:- {}'.format(ev.score_accuracy(pipe_rf, X2, y2)))\nprint('MSE of Training Set:- {}'.format(ev.mse(pipe_rf, X1, y1)))\nprint('MSE of Test Set:- {}\\n\\n'.format(ev.mse(pipe_rf, X2, y2)))\n\nprint('After Hyperparameter Tuning:- \\n')\nprint('Score of Training Set:- {}'.format(ev.score_accuracy(opt_rf, X1, y1)))\nprint('Score of Test Set:- {}'.format(ev.score_accuracy(opt_rf, X2, y2)))\nprint('MSE of Training Set:- {}'.format(ev.mse(opt_rf, X1, y1)))\nprint('MSE of Test Set:- {}'.format(ev.mse(opt_rf, X2, y2)))","873555e0":"pipe_svr = pipe.pipe_svr_reg()\npipe_svr.fit(X1, y1)","2c8079c6":"opt_svr = opt.tune_svr_reg()\nopt_svr.fit(X1, y1)","d7b8474d":"print('Before Hyperparameter Tuning:- \\n')\nprint('Score of Training Set:- {}'.format(ev.score_accuracy(pipe_svr, X1, y1)))\nprint('Score of Test Set:- {}'.format(ev.score_accuracy(pipe_svr, X2, y2)))\nprint('MSE of Training Set:- {}'.format(ev.mse(pipe_svr, X1, y1)))\nprint('MSE of Test Set:- {}\\n\\n'.format(ev.mse(pipe_svr, X2, y2)))\n\nprint('After Hyperparameter Tuning:- \\n')\nprint('Score of Training Set:- {}'.format(ev.score_accuracy(opt_svr, X1, y1)))\nprint('Score of Test Set:- {}'.format(ev.score_accuracy(opt_svr, X2, y2)))\nprint('MSE of Training Set:- {}'.format(ev.mse(opt_svr, X1, y1)))\nprint('MSE of Test Set:- {}'.format(ev.mse(opt_svr, X2, y2)))","e95feb7b":"##### 1) Linear Regression","978a64c5":"##### 5) Support Vector Machine","7850bbbb":"##### 2) Polynomial Regression","e1bd5756":"### Loading the data","1d836a6b":"### Importing all the important library files","fa3720a4":"#### Evaluating Models","aca56088":"##### 3) Decision Tree","87b20086":"# Admission Prediction","bb2da7b6":"##### 4) Random Forest ","49ff653c":"### Exploratory Data Analysis","2bfd3ea2":"### Creating Models"}}