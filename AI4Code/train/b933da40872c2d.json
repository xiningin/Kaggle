{"cell_type":{"c15003e7":"code","f01066b1":"code","ae3e6169":"code","1daa8e0c":"code","51861106":"code","62463e1d":"code","8049d805":"code","b5485685":"code","8d684ae4":"code","3781346a":"code","39b26de8":"code","c519551d":"code","64b8b911":"code","31db3e9a":"code","c5e779aa":"code","389eff83":"code","69b879c5":"code","b4598cf5":"code","bdf45186":"code","a1183bae":"code","cdfc387a":"code","3222d3c0":"code","bab527f6":"code","ca37e0e2":"markdown","67c7c4a8":"markdown","fdd01753":"markdown","1e9af331":"markdown","338022c1":"markdown","0f129a10":"markdown","8e44ea4b":"markdown","fbf8ee4e":"markdown","85b4b2b1":"markdown","4d94ce33":"markdown","615eb9d3":"markdown"},"source":{"c15003e7":"# modules we'll use\nimport pandas as pd\nimport numpy as np\n\n# for Box-Cox Transformation\nfrom scipy import stats\n\n# for min_max scaling\nfrom mlxtend.preprocessing import minmax_scaling\n\n# plotting modules\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# read in all our data\nkickstarters_2017 = pd.read_csv(\"..\/input\/kickstarter-projects\/ks-projects-201801.csv\")\n\n# set seed for reproducibility\nnp.random.seed(0)","f01066b1":"# generate 1000 data points randomly drawn from an exponential distribution\noriginal_data = np.random.exponential(size = 1000)\n\n# mix-max scale the data between 0 and 1\nscaled_data = minmax_scaling(original_data, columns = [0])\n\n# plot both together to compare\nfig, ax=plt.subplots(1,2)\nsns.distplot(original_data, ax=ax[0])\nax[0].set_title(\"Original Data\")\nsns.distplot(scaled_data, ax=ax[1])\nax[1].set_title(\"Scaled data\")","ae3e6169":"# normalize the exponential data with boxcox\nnormalized_data = stats.boxcox(original_data)\n\n# plot both together to compare\nfig, ax=plt.subplots(1,2)\nsns.distplot(original_data, ax=ax[0])\nax[0].set_title(\"Original Data\")\nsns.distplot(normalized_data[0], ax=ax[1])\nax[1].set_title(\"Normalized data\")","1daa8e0c":"# select the usd_goal_real column\nusd_goal = kickstarters_2017.usd_goal_real\n\n# scale the goals from 0 to 1\nscaled_data = minmax_scaling(usd_goal, columns = [0])\n\n# plot the original & scaled data together to compare\nfig, ax=plt.subplots(1,2)\nsns.distplot(kickstarters_2017.usd_goal_real, ax=ax[0])\nax[0].set_title(\"Original Data\")\nsns.distplot(scaled_data, ax=ax[1])\nax[1].set_title(\"Scaled data\")","51861106":"# Your turn! \n\n# We just scaled the \"usd_goal_real\" column. What about the \"goal\" column?\n","62463e1d":"# get the index of all positive pledges (Box-Cox only takes postive values)\nindex_of_positive_pledges = kickstarters_2017.usd_pledged_real > 0\n\n# get only positive pledges (using their indexes)\npositive_pledges = kickstarters_2017.usd_pledged_real.loc[index_of_positive_pledges]\n\n# normalize the pledges (w\/ Box-Cox)\nnormalized_pledges = stats.boxcox(positive_pledges)[0]\n\n# plot both together to compare\nfig, ax=plt.subplots(1,2)\nsns.distplot(positive_pledges, ax=ax[0])\nax[0].set_title(\"Original Data\")\nsns.distplot(normalized_pledges, ax=ax[1])\nax[1].set_title(\"Normalized data\")","8049d805":"# Your turn! \n# We looked as the usd_pledged_real column. What about the \"pledged\" column? Does it have the same info?\n","b5485685":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nfrom sklearn.preprocessing import minmax_scale\n\n\n# Any results you write to the current directory are saved as output.","8d684ae4":"weather_data=pd.read_csv(\"..\/input\/weatherww2\/Summary of Weather.csv\")","3781346a":"weather_data.head()","39b26de8":"weather_data.describe()","c519551d":"weather_data.describe(include=['O'])","64b8b911":"#There are 119040 rows and 31 columns.\nprint (weather_data.shape)\nprint ('--'*30)\n\n#Lets see what percentage of each column has null values\n#It means count number of nulls in every column and divide by total num of rows.\n\nprint (weather_data.isnull().sum()\/weather_data.shape[0] * 100)","31db3e9a":"cols = [col for col in weather_data.columns if (weather_data[col].isnull().sum()\/weather_data.shape[0] * 100 < 70)]\nweather_data_trimmed = weather_data[cols]\n\n#STA is more of station code, lets drop it for the moment\nweather_data_trimmed = weather_data_trimmed.drop(['STA'], axis=1)\n\nprint ('Legitimate columns after dropping null columns: %s' % weather_data_trimmed.shape[1])","c5e779aa":"weather_data_trimmed.isnull().sum()","389eff83":"weather_data_trimmed.sample(5)","69b879c5":"#Check dtypes and look for conversion if needed\nweather_data_trimmed.dtypes\n\n#Looks like some columns needs to be converted to numeric field\n\nweather_data_trimmed['Snowfall'] = pd.to_numeric(weather_data_trimmed['Snowfall'], errors='coerce')\nweather_data_trimmed['SNF'] = pd.to_numeric(weather_data_trimmed['SNF'], errors='coerce')\nweather_data_trimmed['PRCP'] = pd.to_numeric(weather_data_trimmed['PRCP'], errors='coerce')\nweather_data_trimmed['Precip'] = pd.to_numeric(weather_data_trimmed['Precip'], errors='coerce')\n\nweather_data_trimmed['Date'] = pd.to_datetime(weather_data_trimmed['Date'])","b4598cf5":"#Fill remaining null values. FOr the moment lts perform ffill\n\nweather_data_trimmed.fillna(method='ffill', inplace=True)\nweather_data_trimmed.fillna(method='bfill', inplace=True)\n\nweather_data_trimmed.isnull().sum()\n#Well no more NaN and null values to worry about","bdf45186":"print (weather_data_trimmed.dtypes)\nprint ('--'*30)\nweather_data_trimmed.sample(3)","a1183bae":"#weather_data_trimmed_scaled = minmax_scale(weather_data_trimmed.iloc[:, 1:])\n\nweather_data_trimmed['Precip_scaled'] = minmax_scale(weather_data_trimmed['Precip'])\nweather_data_trimmed['MeanTemp_scaled'] = minmax_scale(weather_data_trimmed['MeanTemp'])\nweather_data_trimmed['YR_scaled'] = minmax_scale(weather_data_trimmed['YR'])\nweather_data_trimmed['Snowfall_scaled'] = minmax_scale(weather_data_trimmed['Snowfall'])\nweather_data_trimmed['MAX_scaled'] = minmax_scale(weather_data_trimmed['MAX'])\nweather_data_trimmed['MIN_scaled'] = minmax_scale(weather_data_trimmed['MIN'])","cdfc387a":"#Plot couple of columns to see how the data is scaled\n\nfig, ax = plt.subplots(4, 2, figsize=(15, 15))\n\nsns.distplot(weather_data_trimmed['Precip'], ax=ax[0][0])\nsns.distplot(weather_data_trimmed['Precip_scaled'], ax=ax[0][1])\n\nsns.distplot(weather_data_trimmed['MeanTemp'], ax=ax[1][0])\nsns.distplot(weather_data_trimmed['MeanTemp_scaled'], ax=ax[1][1])\n\nsns.distplot(weather_data_trimmed['Snowfall'], ax=ax[2][0])\nsns.distplot(weather_data_trimmed['Snowfall_scaled'], ax=ax[2][1])\n\nsns.distplot(weather_data_trimmed['MAX'], ax=ax[3][0])\nsns.distplot(weather_data_trimmed['MAX_scaled'], ax=ax[3][1])\n","3222d3c0":"from scipy.stats import boxcox\n\nPrecip_norm = boxcox(weather_data_trimmed['Precip_scaled'].loc[weather_data_trimmed['Precip_scaled'] > 0])\nMeanTemp_norm = boxcox(weather_data_trimmed['MeanTemp_scaled'].loc[weather_data_trimmed['MeanTemp_scaled'] > 0])\nYR_norm = boxcox(weather_data_trimmed['YR_scaled'].loc[weather_data_trimmed['YR_scaled'] > 0])\nSnowfall_norm = boxcox(weather_data_trimmed['Snowfall_scaled'].loc[weather_data_trimmed['Snowfall_scaled'] > 0])\nMAX_norm = boxcox(weather_data_trimmed['MAX_scaled'].loc[weather_data_trimmed['MAX_scaled'] > 0])\nMIN_norm = boxcox(weather_data_trimmed['MIN_scaled'].loc[weather_data_trimmed['MIN_scaled'] > 0])","bab527f6":"fig, ax = plt.subplots(4, 2, figsize=(15, 15))\n\nsns.distplot(weather_data_trimmed['Precip_scaled'], ax=ax[0][0])\nsns.distplot(Precip_norm[0], ax=ax[0][1])\n\nsns.distplot(weather_data_trimmed['MeanTemp_scaled'], ax=ax[1][0])\nsns.distplot(MeanTemp_norm[0], ax=ax[1][1])\n\nsns.distplot(weather_data_trimmed['Snowfall_scaled'], ax=ax[2][0])\nsns.distplot(Snowfall_norm[0], ax=ax[2][1])\n\nsns.distplot(weather_data_trimmed['MAX_scaled'], ax=ax[3][0])\nsns.distplot(MAX_norm[0], ax=ax[3][1])","ca37e0e2":"It's not perfect (it looks like a lot pledges got very few pledges) but it is much closer to normal!","67c7c4a8":"You can see that scaling changed the scales of the plots dramatically (but not the shape of the data: it looks like most campaigns have small goals but a few have very large ones)","fdd01753":"And that's it for today! If you have any questions, be sure to post them in the comments below or [on the forums](https:\/\/www.kaggle.com\/questions-and-answers). \n\nRemember that your notebook is private by default, and in order to share it with other people or ask for help with it, you'll need to make it public. First, you'll need to save a version of your notebook that shows your current work by hitting the \"Commit & Run\" button. (Your work is saved automatically, but versioning your work lets you go back and look at what it was like at the point you saved it. It also lets you share a nice compiled notebook instead of just the raw code.) Then, once your notebook is finished running, you can go to the Settings tab in the panel to the left (you may have to expand it by hitting the [<] button next to the \"Commit & Run\" button) and setting the \"Visibility\" dropdown to \"Public\".\n\n# More practice!\n___\n\nTry finding a new dataset and pretend you're preparing to preform a [regression analysis](https:\/\/www.kaggle.com\/rtatman\/the-5-day-regression-challenge). ([These datasets are a good start!](https:\/\/www.kaggle.com\/rtatman\/datasets-for-regression-analysis)) Pick three or four variables and decide if you need to normalize or scale any of them and, if you think you should, practice applying the correct technique.","1e9af331":"# Practice scaling\n___\n\nTo practice scaling and normalization, we're going to be using a dataset of Kickstarter campaigns. (Kickstarter is a website where people can ask people to invest in various projects and concept products.)\n\nLet's start by scaling the goals of each campaign, which is how much money they were asking for.","338022c1":"# Get our environment set up\n________\n\nThe first thing we'll need to do is load in the libraries and datasets we'll be using. \n\n> **Important!** Make sure you run this cell yourself or the rest of your code won't work!","0f129a10":"# Practice normalization\n___\n\nOk, now let's try practicing normalization. We're going to normalize the amount of money pledged to each campaign.","8e44ea4b":"Notice that the *shape* of our data has changed. Before normalizing it was almost L-shaped. But after normalizing it looks more like the outline of a bell (hence \"bell curve\"). \n\n___\n## Your turn!\n\nFor the following example, decide whether scaling or normalization makes more sense. \n\n* You want to build a linear regression model to predict someone's grades given how much time they spend on various activities during a normal school week.  You notice that your measurements for how much time students spend studying aren't normally distributed: some students spend almost no time studying and others study for four or more hours every day. Should you scale or normalize this variable?\n* You're still working on your grades study, but you want to include information on how students perform on several fitness tests as well. You have information on how many jumping jacks and push-ups each student can complete in a minute. However, you notice that students perform far more jumping jacks than push-ups: the average for the former is 40, and for the latter only 10. Should you scale or normalize these variables?","fbf8ee4e":"# Scaling vs. Normalization: What's the difference?\n____\n\nOne of the reasons that it's easy to get confused between scaling and normalization is because the terms are sometimes used interchangeably and, to make it even more confusing, they are very similar! In both cases, you're transforming the values of numeric variables so that the transformed data points have specific helpful properties. The difference is that, in scaling, you're changing the *range* of your data while in normalization you're changing the *shape of the distribution* of your data. Let's talk a little more in-depth about each of these options. \n\n___\n\n## **Scaling**\n\nThis means that you're transforming your data so that it fits within a specific scale, like 0-100 or 0-1.  You want to scale data when you're using methods based on measures of how far apart data points, like [support vector machines, or SVM](https:\/\/en.wikipedia.org\/wiki\/Support_vector_machine) or [k-nearest neighbors, or KNN](https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm). With these algorithms, a change of \"1\" in any numeric feature is given the same importance. \n\nFor example, you might be looking at the prices of some products in both Yen and US Dollars. One US Dollar is worth about 100 Yen, but if you don't scale your prices methods like SVM or KNN will consider a difference in price of 1 Yen as important as a difference of 1 US Dollar! This clearly doesn't fit with our intuitions of the world. With currency, you can convert between currencies. But what about if you're looking at something like height and weight? It's not entirely clear how many pounds should equal one inch (or how many kilograms should equal one meter).\n\nBy scaling your variables, you can help compare different variables on equal footing. To help solidify what scaling looks like, let's look at a made-up example. (Don't worry, we'll work with real data in just a second, this is just to help illustrate my point.)\n","85b4b2b1":"**********Now that we're set up, let's learn about scaling & normalization. (If you like, you can take this opportunity to take a look at some of the data.)","4d94ce33":"### All days of the challange:\n\n* [Day 1: Handling missing values](https:\/\/www.kaggle.com\/rtatman\/data-cleaning-challenge-handling-missing-values)\n* [Day 2: Scaling and normalization](https:\/\/www.kaggle.com\/rtatman\/data-cleaning-challenge-scale-and-normalize-data)\n* [Day 3: Parsing dates](https:\/\/www.kaggle.com\/rtatman\/data-cleaning-challenge-parsing-dates\/)\n* [Day 4: Character encodings](https:\/\/www.kaggle.com\/rtatman\/data-cleaning-challenge-character-encodings\/)\n* [Day 5: Inconsistent Data Entry](https:\/\/www.kaggle.com\/rtatman\/data-cleaning-challenge-inconsistent-data-entry\/)\n___\nWelcome to day 2 of the 5-Day Data Challenge! Today, we're going to be looking at how to scale and normalize data (and what the difference is between the two!). To get started, click the blue \"Fork Notebook\" button in the upper, right hand corner. This will create a private copy of this notebook that you can edit and play with. Once you're finished with the exercises, you can choose to make your notebook public to share with others. :)\n\n> **Your turn!** As we work through this notebook, you'll see some notebook cells (a block of either code or text) that has \"Your Turn!\" written in it. These are exercises for you to do to help cement your understanding of the concepts we're talking about. Once you've written the code to answer a specific question, you can run the code by clicking inside the cell (box with code in it) with the code you want to run and then hit CTRL + ENTER (CMD + ENTER on a Mac). You can also click in a cell and then click on the right \"play\" arrow to the left of the code. If you want to run all the code in your notebook, you can use the double, \"fast forward\" arrows at the bottom of the notebook editor.\n\nHere's what we're going to do today:\n\n* [Get our environment set up](#Get-our-environment-set-up)\n* [Scaling vs. Normalization: What's the difference?](#Scaling-vs.-Normalization:-What's-the-difference?)\n* [Practice scaling](#Practice-scaling)\n* [Practice normalization](#Practice-normalization)\n\nLet's get started!","615eb9d3":"Notice that the *shape* of the data doesn't change, but that instead of ranging from 0 to 8ish, it now ranges from 0 to 1.\n\n___\n## Normalization\n\nScaling just changes the range of your data. Normalization is a more radical transformation. The point of normalization is to change your observations so that they can be described as a normal distribution.\n\n> **[Normal distribution:](https:\/\/en.wikipedia.org\/wiki\/Normal_distribution)** Also known as the \"bell curve\", this is a specific statistical distribution where a roughly equal observations fall above and below the mean, the mean and the median are the same, and there are more observations closer to the mean. The normal distribution is also known as the Gaussian distribution.\n\nIn general, you'll only want to normalize your data if you're going to be using a machine learning or statistics technique that assumes your data is normally distributed. Some examples of these include t-tests, ANOVAs, linear regression, linear discriminant analysis (LDA) and Gaussian naive Bayes. (Pro tip: any method with \"Gaussian\" in the name probably assumes normality.)\n\nThe method were  using to normalize here is called the [Box-Cox Transformation](https:\/\/en.wikipedia.org\/wiki\/Power_transform#Box%E2%80%93Cox_transformation). Let's take a quick peek at what normalizing some data looks like:"}}