{"cell_type":{"b52e07a4":"code","dcf8c9af":"code","8a937bd1":"code","a66e9fd2":"code","129a3b07":"code","65ae94c3":"code","1c16a033":"code","54e3fd5e":"code","af5b0535":"code","84630d7c":"code","ed9cf253":"code","a9e68bbb":"markdown","f1a9a136":"markdown","89e5c8a3":"markdown","8d10a3ef":"markdown"},"source":{"b52e07a4":"\nimport os\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom keras.models import Model\nfrom keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D\nfrom keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import Callback\nimport warnings\n\nLEN_WORDS = 30\nLEN_EMBEDDING = 300","dcf8c9af":"EMBEDDING_FILE = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')","8a937bd1":"X_train = train[\"question_text\"].fillna(\"fillna\").values\ny_train = train[\"target\"].values\nX_test = test[\"question_text\"].fillna(\"fillna\").values\n\nmax_features = 30000\nmaxlen = 40\nembed_size = 300\n\ntokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train) + list(X_test))\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\nx_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(X_test, maxlen=maxlen)","a66e9fd2":"def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","129a3b07":"class F1Evaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            y_pred = (y_pred > 0.5).astype(int)\n            score = f1_score(self.y_val, y_pred)\n            print(\"\\n F1 Score - epoch: %d - score: %.6f \\n\" % (epoch+1, score))","65ae94c3":"from keras.models import Model\nfrom keras.layers import Conv1D, Input, MaxPooling1D, Flatten, Dense, BatchNormalization, concatenate\nfrom keras.layers import LeakyReLU, Activation","1c16a033":"STRIDE_1 = 2\nSTRIDE_2 = 4\nSTRIDE_3 = 8\n\nFILTER_1 = 64\nFILTER_2 = 64\nFILTER_3 = 64\n\ninp = Input(shape=(maxlen, ))\nembed_layer = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nembed_layer = SpatialDropout1D(0.4)(embed_layer)\nembed_layer = Reshape((maxlen, embed_size))(embed_layer)\n\n# line1 = BatchNormalization()(Input_layer)\nline1 = Conv1D(FILTER_1, STRIDE_1)(embed_layer)\nline1 = Activation(LeakyReLU())(line1)\nline1 = MaxPooling1D(STRIDE_1)(line1)\nline1 = Conv1D(FILTER_1, STRIDE_1)(line1)\nline1 = Activation(LeakyReLU())(line1)\nline1 = MaxPooling1D(STRIDE_1)(line1)\nline1 = Conv1D(FILTER_1, STRIDE_1)(line1)\nline1 = Activation(LeakyReLU())(line1)\nline1 = MaxPooling1D(STRIDE_1*2)(line1)  # global max pooling\nline1 = Flatten()(line1)\n\n# line2 = BatchNormalization()(Input_layer)\nline2 = Conv1D(FILTER_2, STRIDE_1)(embed_layer)\nline2 = Activation(LeakyReLU())(line2)\nline2 = MaxPooling1D(STRIDE_1)(line2)\nline2 = Conv1D(FILTER_2, STRIDE_1)(line2)\nline2 = Activation(LeakyReLU())(line2)\nline2 = MaxPooling1D(STRIDE_1)(line2)\nline2 = Conv1D(FILTER_2, STRIDE_1)(line2)\nline2 = Activation(LeakyReLU())(line2)\nline2 = MaxPooling1D(STRIDE_1*2)(line2)  # global max pooling\nline2 = Flatten()(line2)\n\n# line3 = BatchNormalization()(Input_layer)\nline3 = Conv1D(FILTER_3, STRIDE_1)(embed_layer)\nline3 = Activation(LeakyReLU())(line3)\nline3 = MaxPooling1D(STRIDE_1)(line3)\nline3 = Conv1D(FILTER_3, STRIDE_1)(line3)\nline3 = Activation(LeakyReLU())(line3)\nline3 = MaxPooling1D(STRIDE_1)(line3)\nline3 = Conv1D(FILTER_3, STRIDE_1)(line3)\nline3 = Activation(LeakyReLU())(line3)\nline3 = MaxPooling1D(STRIDE_1*2)(line3)  # global max pooling\nline3 = Flatten()(line3)\n\nconcat_layer = concatenate([line1, line2, line3])\n\ntotal = Dense(1024, activation='relu')(concat_layer)\npreds = Dense(1, activation='sigmoid')(total)\n\nmodel = Model(inputs=inp, outputs=preds)","54e3fd5e":"model.summary()","af5b0535":"model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","84630d7c":"batch_size = 256\nepochs = 10\n\nX_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95,\n                                              random_state=1989)\nF1_Score = F1Evaluation(validation_data=(X_val, y_val), interval=1)\n\nhist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs,\n                 validation_data=(X_val, y_val),\n                 callbacks=[F1_Score], verbose=1)","ed9cf253":"y_pred = model.predict(x_test, batch_size=1024)\ny_pred = (y_pred > 0.5).astype(int)\nsubmission['prediction'] = y_pred\nsubmission.to_csv('submission.csv', index=False)","a9e68bbb":"# Setup","f1a9a136":"# Training","89e5c8a3":"# Inference","8d10a3ef":"- I forked and refered below helpful kernels. Thanks for the authors! If you think this kernel is helpful, please upvote them!\n- https:\/\/www.kaggle.com\/yekenot\/2dcnn-textclassifier\n- https:\/\/www.kaggle.com\/applecer\/use-f1-to-select-model-lstm-based"}}