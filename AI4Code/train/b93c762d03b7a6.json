{"cell_type":{"65ba82ed":"code","f2295e75":"code","74f9eeae":"code","7754fcfb":"code","6a5c2977":"code","fce9c2d1":"code","04370fbf":"code","e16ef979":"code","816c332f":"code","71d29692":"code","2a6504a9":"code","5b8d8999":"code","a5c16f87":"code","024ca737":"code","c8d7914f":"code","7bd2c111":"code","57db526b":"code","86375632":"code","0e0b4253":"code","56921122":"code","40a5d855":"code","7a23b391":"code","524ae604":"code","88a0aaef":"code","d66f9533":"code","4661dd19":"code","2e43367b":"markdown","f5a7aa95":"markdown","d1099797":"markdown","abcb9d23":"markdown","184c2549":"markdown","fdcda236":"markdown","d5ddeed2":"markdown","dec17fea":"markdown","ed0c40fe":"markdown","79081760":"markdown","321c955b":"markdown","5e91fa79":"markdown","61731353":"markdown","93210df9":"markdown","07ea7a5a":"markdown","29ae9c61":"markdown","d99584b4":"markdown"},"source":{"65ba82ed":"import os\nimport numpy as np \nimport pandas as pd \nimport json","f2295e75":"os.listdir('..\/input\/imet-2020-fgvc7')","74f9eeae":"train = pd.read_csv('..\/input\/imet-2020-fgvc7\/train.csv')\nlabels = pd.read_csv('..\/input\/imet-2020-fgvc7\/labels.csv')\nsubmission = pd.read_csv('..\/input\/imet-2020-fgvc7\/sample_submission.csv')","7754fcfb":"train.head()","6a5c2977":"labels.head()","fce9c2d1":"submission.head()","04370fbf":"labels['attribute_name'].nunique()","e16ef979":"from collections import Counter\n\ncls_counts = Counter(cls for classes in train['attribute_ids'].str.split() for cls in classes)\n\nprint(len(cls_counts))","816c332f":"label_map = dict(labels[['attribute_id', 'attribute_name']].values.tolist())\nnot_in_train_labels = set(labels['attribute_id'].astype(str).values) - set(list(cls_counts))\nfor _id in not_in_train_labels:\n    label = label_map[int(_id)]\n    print(f'attribute_id: {_id}  attribute_name: {label}')","71d29692":"# TOP 20 common attribute\nfor item in sorted(cls_counts.items(), key=lambda x: x[1], reverse=True)[:20]:\n    _id, count = item[0], item[1]\n    label = label_map[int(_id)]\n    print(f'attribute_name: {label}  count: {count}')","2a6504a9":"# Number of labels for each instance\nimport matplotlib.pyplot as plt\n\ndf_label_len = train.attribute_ids.str.split(\" \").apply(len)\nplt.figure(figsize=(25, 4))\ndf_label_len.value_counts().plot.bar()\nplt.title(f\"Number of labels for each instance\")","5b8d8999":"# ====================================================\n# Library\n# ====================================================\n\nimport sys\n\nimport gc\nimport os\nimport random\nimport time\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom collections import defaultdict, Counter\n\nimport cv2\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\n\nimport sklearn.metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom functools import partial\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam, SGD\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.models as models\n\nfrom albumentations import Compose, Normalize, Resize, RandomResizedCrop\nfrom albumentations.pytorch import ToTensorV2\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","a5c16f87":"# ====================================================\n# Utils\n# ====================================================\n\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    LOGGER.info(f'[{name}] start')\n    yield\n    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s.')\n\n    \ndef init_logger(log_file='train.log'):\n    from logging import getLogger, DEBUG, FileHandler,  Formatter,  StreamHandler\n    \n    log_format = '%(asctime)s %(levelname)s %(message)s'\n    \n    stream_handler = StreamHandler()\n    stream_handler.setLevel(DEBUG)\n    stream_handler.setFormatter(Formatter(log_format))\n    \n    file_handler = FileHandler(log_file)\n    file_handler.setFormatter(Formatter(log_format))\n    \n    logger = getLogger('Herbarium')\n    logger.setLevel(DEBUG)\n    logger.addHandler(stream_handler)\n    logger.addHandler(file_handler)\n    \n    return logger\n\nLOG_FILE = 'train.log'\nLOGGER = init_logger(LOG_FILE)\n\n\ndef seed_torch(seed=777):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nSEED = 777\nseed_torch(SEED)","024ca737":"N_CLASSES = 3474\n\n\nclass TrainDataset(Dataset):\n    def __init__(self, df, labels, transform=None):\n        self.df = df\n        self.labels = labels\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file_name = self.df['id'].values[idx]\n        file_path = f'..\/input\/imet-2020-fgvc7\/train\/{file_name}.png'\n        image = cv2.imread(file_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n            \n        label = self.labels.values[idx]\n        target = torch.zeros(N_CLASSES)\n        for cls in label.split():\n            target[int(cls)] = 1\n        \n        return image, target\n    \n\nclass TestDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file_name = self.df['id'].values[idx]\n        file_path = f'..\/input\/imet-2020-fgvc7\/test\/{file_name}.png'\n        image = cv2.imread(file_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n        \n        return image","c8d7914f":"HEIGHT = 128\nWIDTH = 128\n\n\ndef get_transforms(*, data):\n    \n    assert data in ('train', 'valid')\n    \n    if data == 'train':\n        return Compose([\n            #Resize(HEIGHT, WIDTH),\n            RandomResizedCrop(HEIGHT, WIDTH),\n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(),\n        ])\n    \n    elif data == 'valid':\n        return Compose([\n            Resize(HEIGHT, WIDTH),\n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(),\n        ])","7bd2c111":"def make_folds(df, n_folds, seed):\n    cls_counts = Counter(cls for classes in df['attribute_ids'].str.split() for cls in classes)\n    fold_cls_counts = defaultdict(int)\n    folds = [-1] * len(df)\n    for item in df.sample(frac=1, random_state=seed).itertuples():\n        cls = min(item.attribute_ids.split(), key=lambda cls: cls_counts[cls])\n        fold_counts = [(f, fold_cls_counts[f, cls]) for f in range(n_folds)]\n        min_count = min([count for _, count in fold_counts])\n        random.seed(item.Index)\n        fold = random.choice([f for f, count in fold_counts if count == min_count])\n        folds[item.Index] = fold\n        for cls in item.attribute_ids.split():\n            fold_cls_counts[fold, cls] += 1\n    df['fold'] = folds\n    return df","57db526b":"DEBUG = False\nN_FOLDS = 5\nFOLD = 0\n\nif DEBUG:\n    folds = train.sample(n=10000, random_state=SEED).reset_index(drop=True).copy()\n    folds = make_folds(folds, N_FOLDS, SEED)\nelse:\n    folds = train.copy()\n    folds = make_folds(folds, N_FOLDS, SEED)\n    \ntrn_idx = folds[folds['fold'] != FOLD].index\nval_idx = folds[folds['fold'] == FOLD].index\nprint(trn_idx.shape, val_idx.shape)","86375632":"train_dataset = TrainDataset(folds.loc[trn_idx].reset_index(drop=True), \n                             folds.loc[trn_idx]['attribute_ids'], \n                             transform=get_transforms(data='train'))\nvalid_dataset = TrainDataset(folds.loc[val_idx].reset_index(drop=True), \n                             folds.loc[val_idx]['attribute_ids'], \n                             transform=get_transforms(data='valid'))","0e0b4253":"batch_size = 128\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)","56921122":"model = models.resnet18(pretrained=False)\nweights_path = '..\/input\/resnet18\/resnet18.pth'\nmodel.load_state_dict(torch.load(weights_path))\n\nmodel.avgpool = nn.AdaptiveAvgPool2d(1)\nmodel.fc = nn.Linear(model.fc.in_features, N_CLASSES)","40a5d855":"from sklearn.metrics import fbeta_score\n\n\ndef get_score(targets, y_pred):\n    return fbeta_score(targets, y_pred, beta=2, average='samples')\n\n\n\ndef binarize_prediction(probabilities, threshold: float, argsorted=None,\n                        min_labels=1, max_labels=10):\n    \"\"\" \n    Return matrix of 0\/1 predictions, same shape as probabilities.\n    \"\"\"\n    assert probabilities.shape[1] == N_CLASSES\n    if argsorted is None:\n        argsorted = probabilities.argsort(axis=1)\n    max_mask = _make_mask(argsorted, max_labels)\n    min_mask = _make_mask(argsorted, min_labels)\n    prob_mask = probabilities > threshold\n    return (max_mask & prob_mask) | min_mask\n\n\ndef _make_mask(argsorted, top_n: int):\n    mask = np.zeros_like(argsorted, dtype=np.uint8)\n    col_indices = argsorted[:, -top_n:].reshape(-1)\n    row_indices = [i \/\/ top_n for i in range(len(col_indices))]\n    mask[row_indices, col_indices] = 1\n    return mask\n\n\ndef _reduce_loss(loss):\n    return loss.sum() \/ loss.shape[0]","7a23b391":"with timer('Train model'):\n    \n    n_epochs = 12\n    lr = 1e-4\n    \n    model.to(device)\n    \n    optimizer = Adam(model.parameters(), lr=lr, amsgrad=False)\n    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.75, patience=4, verbose=True, eps=1e-6)\n    \n    criterion = nn.BCEWithLogitsLoss(reduction='none')\n    best_score = 0.\n    best_thresh = 0.\n    best_loss = np.inf\n    \n    for epoch in range(n_epochs):\n        \n        start_time = time.time()\n\n        model.train()\n        avg_loss = 0.\n\n        optimizer.zero_grad()\n        tk0 = tqdm(enumerate(train_loader), total=len(train_loader))\n\n        for i, (images, labels) in tk0:\n\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            y_preds = model(images)\n            loss = _reduce_loss(criterion(y_preds, labels))\n            \n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            avg_loss += loss.item() \/ len(train_loader)\n            \n        model.eval()\n        avg_val_loss = 0.\n        preds = []\n        valid_labels = []\n        tk1 = tqdm(enumerate(valid_loader), total=len(valid_loader))\n\n        for i, (images, labels) in tk1:\n            \n            images = images.to(device)\n            labels = labels.to(device)\n            \n            with torch.no_grad():\n                y_preds = model(images)\n            \n            preds.append(torch.sigmoid(y_preds).to('cpu').numpy())\n            valid_labels.append(labels.to('cpu').numpy())\n\n            loss = _reduce_loss(criterion(y_preds, labels))\n            avg_val_loss += loss.item() \/ len(valid_loader)\n        \n        scheduler.step(avg_val_loss)\n            \n        preds = np.concatenate(preds)\n        valid_labels = np.concatenate(valid_labels)\n        argsorted = preds.argsort(axis=1)\n        \n        th_scores = {}\n        for threshold in [0.05, 0.06, 0.07, 0.08, 0.09, 0.10, 0.11, 0.12, 0.13, 0.14, 0.15]:\n            _score = get_score(valid_labels, binarize_prediction(preds, threshold, argsorted))\n            th_scores[threshold] = _score\n        \n        max_kv = max(th_scores.items(), key=lambda x: x[1])\n        th, score = max_kv[0], max_kv[1]\n\n        elapsed = time.time() - start_time\n        \n        LOGGER.debug(f'  Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n        LOGGER.debug(f'  Epoch {epoch+1} - threshold: {th}  f2_score: {score}')\n        \n        if score>best_score:\n            best_score = score\n            best_thresh = th\n            LOGGER.debug(f'  Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model  threshold: {best_thresh}')\n            torch.save(model.state_dict(), f'fold{FOLD}_best_score.pth')\n            \n        if avg_val_loss<best_loss:\n            best_loss = avg_val_loss\n            LOGGER.debug(f'  Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n            torch.save(model.state_dict(), f'fold{FOLD}_best_loss.pth')","524ae604":"batch_size = 128\n\ntest_dataset = TestDataset(submission, transform=get_transforms(data='valid'))\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","88a0aaef":"model = models.resnet18(pretrained=False)\nmodel.avgpool = nn.AdaptiveAvgPool2d(1)\nmodel.fc = nn.Linear(model.fc.in_features, N_CLASSES)\n\nweights_path = f'fold{FOLD}_best_score.pth'\nmodel.load_state_dict(torch.load(weights_path))","d66f9533":"with timer('inference'):\n    \n    model.to(device) \n    \n    preds = []\n    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))\n\n    for i, images in tk0:\n            \n        images = images.to(device)\n            \n        with torch.no_grad():\n            y_preds = model(images)\n            \n        preds.append(torch.sigmoid(y_preds).to('cpu').numpy())","4661dd19":"threshold = best_thresh\npredictions = np.concatenate(preds) > threshold\n\nfor i, row in enumerate(predictions):\n    ids = np.nonzero(row)[0]\n    submission.iloc[i].attribute_ids = ' '.join([str(x) for x in ids])\n    \nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","2e43367b":"# Library","f5a7aa95":"# train valid split","d1099797":"# Transforms","abcb9d23":"# Inference","184c2549":"# Model","fdcda236":"# Library","d5ddeed2":"# Dataset","dec17fea":"- PyTorch Resnet18 starter code  \n- 12 epochs not to exceed 4 hours (GPU Notebook <= 4 hours run-time)  \n\nIf this notebook is helpful, feel free to upvote :)  ","ed0c40fe":"- 3471 targets in train.csv","79081760":"# About this notebook","321c955b":"# About TARGET","5e91fa79":"# Train","61731353":"# Submission","93210df9":"# Data Loading","07ea7a5a":"- These labels are not in train","29ae9c61":"- 3474 targets in labels.csv","d99584b4":"# Utils"}}