{"cell_type":{"7e0e792e":"code","ad2e17d0":"code","d6447f5b":"code","3f0b3b39":"code","e7213ce8":"code","12afe40e":"code","38508943":"code","7bac3488":"code","653d3cfc":"code","31fe926b":"code","4bececdd":"code","ff018944":"code","99440349":"code","c876ba15":"code","bf5bd49b":"code","68db5ebe":"code","35ba87c3":"markdown","38c9edf5":"markdown","0d015e98":"markdown","06ab987a":"markdown","6f5b0657":"markdown","88cf5879":"markdown","bea32384":"markdown","00d2a075":"markdown","f93f02dd":"markdown","6c24b1a7":"markdown","c9267f13":"markdown","f68ca83a":"markdown","03f57111":"markdown","056279aa":"markdown","88127d97":"markdown","863469a3":"markdown"},"source":{"7e0e792e":"#Import the libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.cluster import KMeans","ad2e17d0":"def plot_clusters(coords, title, classes=None, labels=None, centroids=None):\n    if classes is None:\n        classes = [0] * coords.shape[0]\n    \n    plt.figure(figsize=(8,6))\n    plot = plt.scatter(coords[:, 0], coords[:, 1], c=classes, cmap='inferno')\n    \n    if labels is not None:\n        plt.legend(handles=plot.legend_elements()[0], labels=labels)\n    \n    if centroids is not None:\n        plt.scatter(centroids[:, 0], centroids[:, 1], c='black')\n\n    plt.title(title)","d6447f5b":"def find_clusters(model, dataset, label):\n    classes = model.fit_predict(dataset)\n    try:\n        centroids = model.cluster_centers_\n    except AttributeError:\n        centroids = None\n        \n    plot_clusters(dataset, label, classes=classes, centroids=centroids)","3f0b3b39":"pizza_df = pd.read_csv(\"..\/input\/nutrient-analysis-of-pizzas\/Pizza.csv\")\npizza_df","e7213ce8":"X = pizza_df.drop(labels=['brand','id'],axis=1)\nY = pizza_df.brand\n\nlabels = Y.unique().tolist()\nclasses = [labels.index(label) for label in Y]","12afe40e":"X","38508943":"#Convert columns of X to N(0, 1) distributions\nX_std = (X.values - np.mean(X.values, axis=0)) \/ np.std(X.values, axis=0)\n\n#Find the covariance matrix\nX_cov = np.cov(X_std, rowvar=False)\n\n#Create a covariance-matrix dataframe to plot the heatmap\nX_cov_df = pd.DataFrame(data=X_cov, columns=X.columns, index=X.columns)\nplt.figure(figsize=(8,8))\nsns.heatmap(X_cov_df, vmax=1, square=True, annot=True, cmap='winter')","7bac3488":"X_corr_df = X.corr()\nplt.figure(figsize=(8,8))\nsns.heatmap(X_corr_df, vmax=1, square=True, annot=True, cmap='winter')","653d3cfc":"np.isclose(X_cov_df, X_corr_df, atol=.01)","31fe926b":"e_values, e_vectors = np.linalg.eig(X_cov)\neigen_pairs = [(value, vector) for value, vector in zip(e_values, e_vectors)]\neigen_pairs.sort(key=lambda x: x[0], reverse=True)\n\ne_values = [value for value, vector in eigen_pairs]\ne_vectors = [vector for value, vector in eigen_pairs]\n\nfor value, vector in eigen_pairs:\n    print(f\"{np.around(value, 5)}\\t{np.around(vector, 3)}\")","4bececdd":"plt.figure(figsize=(8, 8))\nplt.plot(range(1, len(e_values) + 1), e_values, marker='o', label='Eigen values of components')\nplt.ylabel(\"Eigenvalues\")\nplt.xlabel(\"Principal components\")\nplt.title(\"Scree Plot\")\nplt.legend()","ff018944":"cumulative_values = []\ncum_sum = 0\nfor value in e_values:\n    cum_sum += 100 * value\/sum(e_values)\n    cumulative_values.append(cum_sum)\n\nplt.figure(figsize=(8, 8))\nplt.plot(range(1, len(e_values) + 1), cumulative_values, marker='o', label='Cumulative variance explained')\nplt.ylabel(\"Explained variance ratio\")\nplt.xlabel(\"Principal components\")\nplt.title(\"Cumulative explained variance\")\nplt.legend()","99440349":"pc_transformer = np.vstack((e_vectors[0], e_vectors[1])).T\npc_transformer","c876ba15":"X_transformed = X.values @ pc_transformer","bf5bd49b":"plot_clusters(X_transformed, \"Dimensionally-Reduced Pizza-Data\", classes=classes, labels=labels, centroids=None)","68db5ebe":"model = KMeans(n_clusters=10)\nfind_clusters(model, X_transformed, \"K-Means Clustered Pizza-Data (Reduced)\")","35ba87c3":"The first two components account for more than 90% of the variance.  \nThus, we can confidently discard the other components.","38c9edf5":"### Function for training and testing a model on a given dataset","0d015e98":"### Scree plot","06ab987a":"## Import the libraries","6f5b0657":"Thus, the correlation matrix of a set of variables is equivalent to the covariance matrix of the standardized \\[N(0, 1) normalized\\] variables.","88cf5879":"### Correlation matrix","bea32384":"Thus, we have performed dimensionality reduction on a 7-dimensional data set and have discovered clusters in two dimensions.","00d2a075":"## Capture the variance\n### Standardized Covariance Matrix","f93f02dd":"## Perform Clustering\n### Unclustered data","6c24b1a7":"### Projection Matrix\n\nNow we shall find the projection matrix which transforms the feature space into a new space with the first two principal components as our basis vectors.","c9267f13":"## PRINCIPAL COMPONENT ANALYSIS\n\n### Walkthrough of PCA using NumPy & ScikitLearn libraries","f68ca83a":"## Import the Pizza dataset","03f57111":"X stores the features and Y stores the brands of each of the pizzas.","056279aa":"### K-Means Clustering","88127d97":"## Utility functions\n### Function for plotting the clusters","863469a3":"## Perform PCA\nWe shall use the covariance matrix for the PCA because we have standardized the features.  \n\n### Eigen Values & Vectors"}}