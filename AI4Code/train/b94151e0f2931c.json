{"cell_type":{"9436c26b":"code","5024e83a":"code","a696e5ea":"code","3859e82b":"code","d234e9f3":"code","4438e0be":"code","9da8a60c":"code","e1e1a890":"code","99502fa1":"code","24db43cb":"code","f164e5eb":"code","01024505":"code","50a5cf88":"code","ad72d7a8":"code","a6014018":"code","b89f8a34":"code","987c871b":"code","8c517458":"code","3b6a87e1":"code","e9c6d175":"code","6489e122":"code","1501761d":"code","f461518f":"code","7eaf6ec2":"code","33478ab2":"code","920529fa":"code","3b8da4ef":"code","8ce11f15":"code","7dfad521":"code","8bb6bafe":"code","7f408c6e":"code","abf7f5ef":"code","8bc5f97d":"code","1fbc531e":"code","3bf71e14":"code","60471481":"code","f1d32308":"code","3451d1ac":"code","0a056441":"code","d59d9bb9":"code","a783dc59":"code","89ea613a":"markdown","ae1769dd":"markdown","1ec1a88f":"markdown","c5e838f6":"markdown","d508d1c4":"markdown","85d0cfc6":"markdown","ee517e31":"markdown","558e6b6e":"markdown","69cc012c":"markdown","1275ea11":"markdown","db035699":"markdown","e7f371c4":"markdown","e6942295":"markdown","90611ab1":"markdown"},"source":{"9436c26b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5024e83a":"# import the modules we'll need\nfrom IPython.display import HTML\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv(index = False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)","a696e5ea":"\ntest = pd.read_csv(\"..\/input\/loanprediction\/test_lAUu6dG.csv\")\ntrain = pd.read_csv(\"..\/input\/loanprediction\/train_ctrUa4K.csv\")","3859e82b":"train.head()","d234e9f3":"#Catergorical Variables\nplt.figure(1) \nplt.subplot(221) \ntrain['Gender'].value_counts(normalize=True).plot.bar(figsize=(20,10), title= 'Gender') \nplt.subplot(222) \ntrain['Married'].value_counts(normalize=True).plot.bar(title= 'Married') \nplt.subplot(223) \ntrain['Self_Employed'].value_counts(normalize=True).plot.bar(title= 'Self_Employed') \nplt.subplot(224) \ntrain['Credit_History'].value_counts(normalize=True).plot.bar(title= 'Credit_History') \nplt.show()","4438e0be":"#Ordinal Variables\nplt.figure(1)\nplt.subplot(131)\ntrain['Dependents'].value_counts().plot.bar(figsize=(24,6), title= 'Dependents') \nplt.subplot(132)\ntrain['Education'].value_counts().plot.bar(figsize=(24,6), title= 'Education') \nplt.subplot(133)\ntrain['Property_Area'].value_counts().plot.bar(figsize=(24,6), title= 'Property_Area') ","9da8a60c":"#Quantitative Variables\nplt.figure(1)\nplt.subplot(131)\ntrain['ApplicantIncome'].value_counts().plot.hist(figsize=(24,6), title= 'ApplicantIncome') \nplt.subplot(132)\ntrain['LoanAmount'].value_counts().plot.hist(figsize=(24,6), title= 'LoanAmount') \nplt.subplot(133)\ntrain['CoapplicantIncome'].value_counts().plot.hist(figsize=(24,6), title= 'CoapplicantIncome') ","e1e1a890":"#Catergorical Variables \ncat_var = ['Gender','Married','Self_Employed','Credit_History','Dependents','Education','Property_Area']\nfor col in cat_var:\n    Gender=pd.crosstab(train[col],train['Loan_Status']) \n    Gender.div(Gender.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4))\n    plt.show()","99502fa1":"quant_arr = ['ApplicantIncome','LoanAmount','CoapplicantIncome']\nfor col in quant_arr:\n    train.groupby('Loan_Status')[col].mean().plot.bar()\n    plt.xlabel(col)\n    plt.show()","24db43cb":"train.corr()","f164e5eb":"def survival_stacked_bar(variable):\n    approved=train[train[\"Loan_Status\"]==1][variable].value_counts()\/len(train[\"Loan_Status\"]==1)\n    NotApproved=train[train[\"Loan_Status\"]==0][variable].value_counts()\/len(train[\"Loan_Status\"]==0)\n    data=pd.DataFrame([approved,NotApproved])\n    data.index=[\"Approved\",\"NotApproved\"]\n    data.plot(kind=\"bar\",stacked=True,title=\"Percentage\")\n    return data.head()","01024505":"missing_data = train.isnull()\nfor column in missing_data.columns.values.tolist():\n    print(column)\n    print(missing_data[column].value_counts())\n    print(\"\")","50a5cf88":"def replceWithMode(array,data):\n    for col in array: \n        data[col].replace(np.nan,data[col].mode()[0],inplace = True)\nmode_array = ['Self_Employed','Dependents','Credit_History','Loan_Amount_Term','Married','Gender']\nreplceWithMode(mode_array,train)\nreplceWithMode(mode_array,test)\n\ntrain['LoanAmount'].fillna(train['LoanAmount'].median(), inplace=True)\ntest['LoanAmount'].fillna(test['LoanAmount'].median(), inplace=True)","ad72d7a8":"train['LoanAmount_log'] = np.log(train['LoanAmount']+1) \ntrain['LoanAmount_log'].hist(bins=20) \ntest['LoanAmount_log'] = np.log(test['LoanAmount']+1)","a6014018":"train[\"Loan_Amount_Term\"] = train[\"Loan_Amount_Term\"]\/12\ntest[\"Loan_Amount_Term\"] = test[\"Loan_Amount_Term\"]\/12","b89f8a34":"#Creating total income with applicants income and coapplicants income \ntrain['Total_Income']=train['ApplicantIncome']+train['CoapplicantIncome'] \ntest['Total_Income']=test['ApplicantIncome']+test['CoapplicantIncome']\n\ntrain['Total_Income_log'] = np.log(train['Total_Income']+1)\ntest['Total_Income_log'] = np.log(test['Total_Income']+1)","987c871b":"train['LoanAmount_log'] = np.log(train['LoanAmount']*1000+1)\ntest['LoanAmount_log'] = np.log(test['LoanAmount']*1000+1)","8c517458":"#Creating EMI\ntrain['EMI']=train['LoanAmount']\/train['Loan_Amount_Term'] \ntest['EMI']=test['LoanAmount']\/test['Loan_Amount_Term'] \n","3b6a87e1":"train['EMI']","e9c6d175":"train['Balance Income']=train['Total_Income']-(train['EMI']*1000) \ntest['Balance Income']=test['Total_Income']-(test['EMI']*1000)","6489e122":"#train=train.drop(['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term'], axis=1) \n#test=test.drop(['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term'], axis=1)","1501761d":"#Binary Data\nloan_map = {'Y':1,'N':0}\ntrain['Loan_Status'] = train['Loan_Status'].map(loan_map)","f461518f":"train[['Loan_Status']] = train[['Loan_Status']].apply(pd.to_numeric)\ntrain['Dependents'].replace('3+',3,inplace = True)\ntest['Dependents'].replace('3+',3,inplace = True)\ntrain[['Dependents']] = train[['Dependents']].apply(pd.to_numeric)\ntest[['Dependents']] = test[['Dependents']].apply(pd.to_numeric)","7eaf6ec2":"X = train.drop([\"Loan_Status\"],axis=1)\nX = X.drop([\"Loan_ID\"],axis=1)\n\n#X = pd.get_dummies(X)\ncolnames = ['Gender','Married','Education','Self_Employed','Property_Area']\nfor col in colnames:\n    X[col] = pd.factorize(X[col])[0]\ny = train[['Loan_Status']]","33478ab2":"X_test_data = test.drop([\"Loan_ID\"],axis=1)\ntest_colnames = ['Gender','Married','Education','Self_Employed','Property_Area']\nfor col in test_colnames:\n    X_test_data[col] = pd.factorize(X_test_data[col])[0] \n#X_test_data = pd.get_dummies(X_test_data)\nX_test_data.head()","920529fa":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier #For Classification\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n\ndef submission(model,feat):        \n    Y_pred = model.predict( X_test_data[feat])\n    Y_df = pd.DataFrame(test['Loan_ID'])\n    Y_df['Loan_Status'] = Y_pred\n    loan = {1:'Y',0:'N'}\n    Y_df['Loan_Status'] = Y_df['Loan_Status'].map(loan)\n    return Y_df\n\ndef confusion_matrix_model(model_used,x_test):\n    cm=confusion_matrix(y_test,model_used.predict(x_test))\n    col=[\"Predicted Not Approved\",\"Predicted Approved\"]\n    cm=pd.DataFrame(cm)\n    cm.columns=[\"Predicted Not Approved\",\"Predicted Approved\"]\n    cm.index=[\"Actual Not Approved\",\"Actual Approved\"]\n    cm[col]=np.around(cm[col].div(cm[col].sum(axis=1),axis=0),decimals=2)\n    return cm\n\nx_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.4,random_state=0)\n\ndef ModelDevelopement(model,feat):\n    fit = model.fit(x_train[feat],y_train)\n    pred = fit.predict(x_test[feat])\n    print(confusion_matrix_model(log_reg,x_test[feat]))\n    print(accuracy_score(y_test,pred))\n    fit2 =  model.fit(X[feat],y)\n    df = submission(fit2,feat)\n    return pred,df\n","3b8da4ef":"# Create a random forest classifier\nclf = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\n# Train the classifier\nclf.fit(X,y)\nnames = X.columns\n\nprint (\"Features sorted by their score:\")\nRF_feat = pd.DataFrame(sorted(zip(map(lambda x: round(x, 4), clf.feature_importances_), names),reverse=True))","8ce11f15":"RF_feat","7dfad521":"pd.DataFrame(X_res)","8bb6bafe":"feat = [\"Credit_History\",\"Balance Income\",\"ApplicantIncome\",\"Total_Income_log\",\"LoanAmount_log\",\"Property_Area\",\"Dependents\"]\n#feat = [\"Credit_History\",\"Balance Income\",\"Total_Income\",\"ApplicantIncome\",\"EMI\",\"Dependents\",\"Property_Area_Semiurban\",\"LoanAmount\",\"CoapplicantIncome\"]\nlog_reg=LogisticRegression(C = 4,penalty = 'l2')\nlog_pred,log_sub_df = ModelDevelopement(log_reg,feat)\ncreate_download_link(log_sub_df)","7f408c6e":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)\n{'bootstrap': [True, False],\n 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n 'max_features': ['auto', 'sqrt'],\n 'min_samples_leaf': [1, 2, 4],\n 'min_samples_split': [2, 5, 10],\n 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}","abf7f5ef":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(x_train[feat], y_train)","8bc5f97d":"rf_random.best_params_","1fbc531e":"RF = RandomForestClassifier(n_estimators = 800,min_samples_split = 10, random_state = 0, n_jobs = -1,min_samples_leaf = 4,max_depth = 50,max_features = 'sqrt',bootstrap = 'True') \nRF_pred,RF_sub_df = ModelDevelopement(RF,feat)\ncreate_download_link(RF_sub_df)","3bf71e14":"xgb = XGBClassifier(n_estimators=35, max_depth=3,learning_rate = 0.1)\nxgb_pred,xgb_sub_df = ModelDevelopement(xgb,feat)\ncreate_download_link(xgb_sub_df)","60471481":"\ndt = DecisionTreeClassifier() \nadB = AdaBoostClassifier(n_estimators=300, base_estimator=dt,learning_rate=0.1) \nadB_pred,adB_sub_df = ModelDevelopement(adB,feat)\ncreate_download_link(adB_sub_df)","f1d32308":"from sklearn import svm\n \nsvm = svm.SVC(kernel='linear', C=1, gamma=1) \nsvm_pred,svm_sub_df = ModelDevelopement(svm,feat)\ncreate_download_link(svm_sub_df)","3451d1ac":"import sklearn.exceptions","0a056441":"from mlxtend.classifier import StackingClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\n\nestimators = [('rf', RandomForestClassifier(n_estimators=10, random_state=42)),('svr', make_pipeline(StandardScaler(),LinearSVC(random_state=42)))]\nstack = StackingClassifier(classifiers=[RF, xgb],meta_classifier=log_reg)\nstack_pred,stack_sub_df = ModelDevelopement(stack,feat)\ncreate_download_link(stack_sub_df)","d59d9bb9":"from sklearn import model_selection\nfrom mlxtend.classifier import EnsembleVoteClassifier\n\neclf = EnsembleVoteClassifier(clfs=[log_reg,xgb,RF], weights=[1,1,1])\neclf_pred,eclf_sub_df = ModelDevelopement(eclf,feat)\ncreate_download_link(svm_sub_df)","a783dc59":"final_pred = 0.3*log_pred+ 0.1*adB_pred+0.4*RF_pred+0.2*xgb_pred\nfor i in range(len(final_pred)):\n    if(final_pred[i]  < 0.5):\n        final_pred[i] = 0\n    else:\n        final_pred[i]= 1\nprint(accuracy_score(y_test,final_pred))\nprint(confusion_matrix(y_test,final_pred))","89ea613a":"1.1 Univariate Analysis","ae1769dd":"1.2 Bivariate Analysis ","1ec1a88f":"**3. Feature Engineering**","c5e838f6":"**5.3 XGBoost**","d508d1c4":"**1. Understanding Data**","85d0cfc6":"**5. Model Development**","ee517e31":"**5.2. Random Forest**","558e6b6e":"**2. Data Cleaning and Processing **\n\n2. 1 Dealing with missing Data","69cc012c":"**5.1. Logistic Regression**","1275ea11":"**5.5 Support Vectore Machine**","db035699":"**5.6 Ensemble Methods**","e7f371c4":"**4. Dealing with Categorical Data**","e6942295":"2.2 Treating Outliers","90611ab1":"**5.4 AdaBoost Classifier**"}}