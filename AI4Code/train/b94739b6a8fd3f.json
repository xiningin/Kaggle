{"cell_type":{"d01935ff":"code","2480216a":"code","7b976f0a":"code","a3a041fe":"code","ed92045a":"code","905dab27":"code","02b09a2d":"code","0aa39fa4":"code","d1f13cc7":"code","8e150508":"code","34717736":"code","4ed2a8e3":"code","8d3f9277":"code","7fcc9fe4":"code","76090ad5":"code","ed9d8b53":"code","18b54e41":"code","aa4bc54c":"code","67c97dd5":"code","6901506f":"code","e1128165":"code","9c477dab":"code","cd6356ed":"code","a3494fb6":"code","ec7e6fdb":"code","7285518e":"code","fa9dada3":"markdown","df3a7cc9":"markdown","ca3db516":"markdown","9ad29803":"markdown","49800947":"markdown","c1dd3cac":"markdown","11b90736":"markdown","fa9d16c2":"markdown","833f3c5d":"markdown","26763e23":"markdown","0c9d4c03":"markdown","39cc73a4":"markdown","4bc64ed6":"markdown","3a5b4a6f":"markdown","1352f1a4":"markdown","d91f15e0":"markdown","1136e2ff":"markdown","30ee0fbe":"markdown"},"source":{"d01935ff":"import pandas as pd\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score\n\nfrom fastai import *\nfrom fastai.vision import *\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport cv2\n\nfrom tqdm import tqdm\nfrom skmultilearn.model_selection import iterative_train_test_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%load_ext autoreload\n%autoreload","2480216a":"!git clone https:\/\/github.com\/adambielski\/GrouPy.git\n\nimport sys\nsys.path.insert(0,'.\/GrouPy\/')\n\nimport groupy\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\nfrom groupy.gconv.pytorch_gconv import P4ConvZ2, P4ConvP4\nfrom groupy.gconv.pytorch_gconv.pooling import plane_group_spatial_max_pooling\n\n\nfrom torch.autograd import Variable\nfrom groupy.gconv.pytorch_gconv import P4MConvZ2, P4MConvP4M","7b976f0a":"class BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = P4MConvP4M(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.conv2 = P4MConvP4M(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm3d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                P4MConvP4M(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm3d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = P4MConvP4M(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.conv2 = P4MConvP4M(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.conv3 = P4MConvP4M(planes, self.expansion*planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm3d(self.expansion*planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                P4MConvP4M(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm3d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_planes = 23\n\n        self.conv1 = P4MConvZ2(3, 23, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm3d(23)\n        self.layer1 = self._make_layer(block, 23, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 45, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 91, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 181, num_blocks[3], stride=2)\n        self.linear = nn.Linear(181*8*block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        \n        out = self.layer2(out)\n        \n        out = self.layer3(out)\n        \n        out = self.layer4(out)\n        \n        outs = out.size()\n        print(out.size(),out.size(0))\n        \n        out = out.view(outs[0], outs[1]*outs[2], outs[3], outs[4])\n        print(out.size(),out.size(0))\n\n        out = F.avg_pool2d(out, 4)\n        print(out.size(),out.size(0))\n        out = out.view(out.size(0), -1)\n        print(out.size())\n        out = self.linear(out)\n        print(out.size())\n        return out\n\n\ndef ResNet18(pretrained=False):\n    return ResNet(BasicBlock, [2,2,2,2])\n\ndef ResNet34(pretrained=False):\n    return ResNet(BasicBlock, [3,4,6,3])\n\ndef ResNet50(pretrained=False):\n    return ResNet(Bottleneck, [3,4,6,3])\n\ndef ResNet101(pretrained=False):\n    return ResNet(Bottleneck, [3,4,23,3])\n\ndef ResNet152(pretrained=False):\n    return ResNet(Bottleneck, [3,8,36,3])","a3a041fe":"model_path='.'\npath='..\/input\/'\ntrain_folder=f'{path}train'\ntest_folder=f'{path}test'\ntrain_lbl=f'{path}train_labels.csv'\nORG_SIZE=96\n\nbs=32\nnum_workers=None # Apprently 2 cpus per kaggle node, so 4 threads I think\nsz=96","ed92045a":"df_trn=pd.read_csv(train_lbl)","905dab27":"tfms = get_transforms(do_flip=False, flip_vert=False, max_rotate=.0, max_zoom=.3,\n                      max_lighting=0.15, max_warp=0.10)","02b09a2d":"data = ImageDataBunch.from_csv(path,csv_labels=train_lbl,folder='train',bs=bs, ds_tfms=tfms, size=sz, suffix='.tif',test=test_folder);\nstats=data.batch_stats()        \ndata.normalize(stats)","0aa39fa4":"data.show_batch(rows=3, figsize=(12,9))","d1f13cc7":"from sklearn.metrics import roc_auc_score\ndef auc_score(y_pred,y_true):  \n    return roc_auc_score(to_np(y_true),to_np(F.sigmoid(y_pred))[:,1])","8e150508":"#f1=Fbeta_binary(beta2=1)\nf1=partial(fbeta,beta=1)","34717736":"class AdaptiveConcatPool3d(nn.Module):\n    \"Layer that concats `AdaptiveAvgPool3d` and `AdaptiveMaxPool3d`.\"\n    def __init__(self, sz:Optional[int]=None):\n        \"Output will be 2*sz or 2 if sz is None\"\n        super().__init__()\n        sz = sz or 1\n        self.ap,self.mp = nn.AdaptiveAvgPool3d(sz), nn.AdaptiveMaxPool3d(sz)\n    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)","4ed2a8e3":"def create_head_custom(nf:int, nc:int, lin_ftrs:Optional[Collection[int]]=None, ps:Floats=0.5, bn_final:bool=False):\n    \"Model head that takes `nf` features, runs through `lin_ftrs`, and about `nc` classes.\"\n    lin_ftrs = [nf, 512, nc] if lin_ftrs is None else [nf] + lin_ftrs + [nc]\n    ps = listify(ps)\n    if len(ps)==1: ps = [ps[0]\/2] * (len(lin_ftrs)-2) + ps\n    actns = [nn.ReLU(inplace=True)] * (len(lin_ftrs)-2) + [None]\n    layers = [AdaptiveConcatPool3d(), Flatten()]\n    for ni,no,p,actn in zip(lin_ftrs[:-1],lin_ftrs[1:],ps,actns):\n        layers += bn_drop_lin(ni,no,True,p,actn)\n    if bn_final: layers.append(nn.BatchNorm1d(lin_ftrs[-1], momentum=0.01))\n    return nn.Sequential(*layers)\n","8d3f9277":"learn = create_cnn(\n    data,\n    ResNet18,\n    path='.',    \n    metrics=[fbeta], \n    ps=0.5,\n    pretrained=False\n)","7fcc9fe4":"learn.model[1]=create_head_custom(362,2)\nlearn.model.to(learn.data.device)\nprint(learn.summary())\n","76090ad5":"learn.lr_find(num_it=500,end_lr=1000)\nlearn.recorder.plot()","ed9d8b53":"learn.fit_one_cycle(2,slice(5e-5,1e-4))\n","18b54e41":"learn.save('gCNN_1cycle')","aa4bc54c":"learn.recorder.plot()\ntry:\n    learn.recorder.plot_losses()\n    learn.recorder.plot_lr()\nexcept:\n    pass","67c97dd5":"preds,y=learn.get_preds()\npred_score=auc_score(preds,y)\npred_score","6901506f":"preds_test,y_test=learn.get_preds(ds_type=DatasetType.Test)","e1128165":"preds_test_tta,y_test_tta=learn.TTA(ds_type=DatasetType.Test)","9c477dab":"sub=pd.read_csv(f'{path}\/sample_submission.csv').set_index('id')\nsub.head()","cd6356ed":"clean_fname=np.vectorize(lambda fname: str(fname).split('\/')[-1].split('.')[0])\nfname_cleaned=clean_fname(data.test_ds.items)\nfname_cleaned=fname_cleaned.astype(str)","a3494fb6":"sub.loc[fname_cleaned,'label']=to_np(preds_test[:,1])\nsub.to_csv(f'submission_{pred_score}.csv')","ec7e6fdb":"sub.loc[fname_cleaned,'label']=to_np(preds_test_tta[:,1])\nsub.to_csv(f'submission_TTA_{pred_score}.csv')","7285518e":"!rm -r GrouPy\/","fa9dada3":"## And check for an appropriate learning rate\nThe maximum learning rate should be somewhere before the divergence\/minimum and the minimum of the learning rate about a 10th of the maximum","df3a7cc9":"\nIn Case I want to run quick tests use a subsample:\nI check if the last batch contains one element because otherwise batchnorm crashes.","ca3db516":"p=.1\nn=int(p*df_trn.shape[0])\nwhile True:\n    if p<1.0:\n        idx=np.random.choice(len(df_trn.index),n-np.random.randint(0,100))\n    else:\n        idx=range(len(df_trn.index))\n    X_train, y_train, X_val, y_val = iterative_train_test_split(df_trn.index[idx,None], df_trn.label[idx,None], test_size = 0.2)\n    print(X_train.shape,(X_train.shape[0]%bs),X_val.shape,(X_val.shape[0]%bs))\n    if  ((X_train.shape[0]%bs!=1) and (X_val.shape[0]%bs!=1)):\n        break\nX_train.shape,(X_train.shape[0]%bs),X_val.shape,(X_val.shape[0]%bs)\n\n\nsrc = (ImageItemList.from_csv(path,train_lbl, folder='train', suffix='.tif')\n        .split_by_idxs(X_train[:,0],X_val[:,0])\n        .label_from_lists(y_train.tolist(),y_val.tolist()))\n\ndata = (src.transform(tfms, size=sz)\n        .databunch(num_workers=0,bs=bs))\nstats=data.batch_stats()        \ndata.normalize(stats)","9ad29803":"Defining the metric that is used in the competition as well as the loss function","49800947":"## Now the data and model are merged in the fastai learner object\nSince pretrained is set to False the network is unfrozen so we dont need a warm up period where we only train the head of the network","c1dd3cac":"### Defining necessary variables and load the data","11b90736":"I read the stuff that was posted under Acknowledgements namely this paper: Veeling et al \"Rotation Equivariant CNNs for Digital Pathology\"\nI tried to implement it (mainly made it useable with the fastai library since it was already implemented and available).\n <img src=\"https:\/\/raw.githubusercontent.com\/basveeling\/keras-gcnn\/master\/model.png\" >\nMicroscopy pictures are usually rotation and flip invariant. So a CNN that uses the image and all rotations by 90\u00b0 as well as flipped left right and upside down, should be better suited than standard CNN.\nI largely used the code by [Adam Bielsky](https:\/\/github.com\/adambielski) and [Taco Cohen's groupy](https:\/\/github.com\/tscohen\/GrouPy) but changed the batchnorm from 2D to 3D (this might be wrong but otherwise I couldnt get it to work). \n\nI originally intended this for the [Human Protein Atlas Image Classification](https:\/\/www.kaggle.com\/c\/human-protein-atlas-image-classification) but since it uses 8 Layers to include the rotations the memory, and training time is just too much for that data set.\nTheres also a keras implementation of a  [gDensenet ](https:\/\/github.com\/basveeling\/keras-gcnn\/blob\/master\/keras_gcnn\/applications\/densenetnew.py) by Veeling online if you dont like the fastai stuff\n","fa9d16c2":"## Now add the custom head","833f3c5d":"### Otherwise the fulll dataset including the test set","26763e23":"### Now predict on test set","0c9d4c03":"Theres an error I don't understand yet in plot_loss() when only doing one epoch (I think)","39cc73a4":"# Definition of the group invariant Resnet (hidden)","4bc64ed6":"## I add the score to the name of the file so I can later plot the leaderboard score versus my validation score\nIn the fastai course Jeremy mentions that if you have a monotonic relation between validation and LB score the way you set up your validation set matches what the test set consists of.","3a5b4a6f":"# Imports (hidden)","1352f1a4":"## Define a custom head for the gCNN\nThe original resnet has al ot more categories than 2 so a new head is needed.\nThe fastai package already delivers an awesome function for this. I had to define 2 little functions to adapt it to the group equivariant, namely make the pooling 3D not 2D","d91f15e0":"### prepare submission\nI now load in the sample submission and put my predictions in the label column and save to a new file.","1136e2ff":"## Check how well gCNN are doing\nNow that we learned a lot we check how well we are doing by predicting on the validation set.\nFor just one epoch this is pretty good!\nI achieved a LB score >0.96 with this, without TTA.","30ee0fbe":"Sometimes its important in which order the ids in the submissions are so to make sure I don't mess up I put them in the same order. My first submission had a 50% score so I somewhere messed up the order oder the matching of id to label.\nsince fname_clean is the id we can just use that as index when adding the correct label in our dataframe. "}}