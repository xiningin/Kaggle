{"cell_type":{"db06f843":"code","9506ef3e":"code","acd68258":"code","8a3af6cf":"code","f82a6253":"code","04823cda":"code","0ba9c14a":"code","eeb1fcf9":"code","5796d2b2":"code","19f00f15":"code","0e6a384e":"code","a0ca4b10":"code","ff550fea":"code","5342d6ee":"code","f95bac79":"code","58ec122d":"code","91b37fbc":"code","e93f00ef":"code","c2b9b5a6":"code","1219dbbc":"code","17aa2a36":"code","faecb78b":"code","40c0f3e2":"code","ee6028f9":"code","a0bf192f":"code","7f82c181":"code","474ce5d3":"code","40f150c9":"code","1c97524f":"code","c11fe521":"code","ad80fe09":"code","41d9d64c":"code","58e2202f":"code","42098820":"code","1ce01ea2":"markdown","ac089821":"markdown"},"source":{"db06f843":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9506ef3e":"%config Completer.use_jedi = False            ","acd68258":"from IPython import display\n%cd ..\n!mkdir tmp\n%cd tmp\n!git clone https:\/\/github.com\/NyanSwanAung\/Object-Detection-Using-DETR-CustomDataset.git\n%cd Object-Detection-Using-DETR-CustomDataset\/\ndisplay.clear_output()","8a3af6cf":"ls datasets\/","f82a6253":"pwd","04823cda":"# Download train images\n!wget https:\/\/github.com\/NyanSwanAung\/Object-Detection-Using-DETR-CustomDataset\/releases\/download\/v1.0\/WIDER_train.zip -O datasets\/train.zip\n\n# Download val images\n!wget https:\/\/github.com\/NyanSwanAung\/Object-Detection-Using-DETR-CustomDataset\/releases\/download\/v1.0\/WIDER_val.zip -O datasets\/val.zip\n\n# Download annotations\n!wget https:\/\/github.com\/NyanSwanAung\/Object-Detection-Using-DETR-CustomDataset\/releases\/download\/v1.0\/wider_face_split.zip -O datasets\/annotations.zip\n\n#Download test images\n!wget https:\/\/github.com\/NyanSwanAung\/Object-Detection-Using-DETR-CustomDataset\/releases\/download\/v1.0\/WIDER_test.zip -O datasets\/test.zip\n        \ndisplay.clear_output()","0ba9c14a":"%cd datasets\/\n!unzip '*.zip'\n!rm -r annotations.zip train.zip val.zip test.zip\n%cd ..\ndisplay.clear_output()","eeb1fcf9":"ls datasets\/","5796d2b2":"!cp dataloaders\/face.py detr\/datasets \n\n# Make folder for trained-weights.\n!mkdir detr\/outputs","19f00f15":"!pip install -U 'git+https:\/\/github.com\/cocodataset\/cocoapi.git#subdirectory=PythonAPI'\ndisplay.clear_output()","0e6a384e":"phases = [\"train\", \"val\"]\nfor phase in phases:\n    root_path = \"datasets\/WIDER_{}\/images\/\".format(phase)\n    gt_path = os.path.join(\"datasets\/wider_face_split\/wider_face_{}.mat\".format(phase))\n","a0ca4b10":"%%writefile datasets\/face_to_coco.py\nimport re\nimport os\nimport cv2\nimport json\nimport itertools\nimport numpy as np\nfrom glob import glob\nimport scipy.io as sio\nfrom pycocotools import mask as cocomask\nfrom PIL import Image\n\n\nMAX_N = 10\n\ncategories = [\n    {\n        \"supercategory\": \"none\",\n        \"name\": \"face\",\n        \"id\": 0\n    }\n]\n\nphases = [\"train\", \"val\"]\nfor phase in phases:\n    root_path = \"datasets\/WIDER_{}\/images\/\".format(phase)\n    gt_path = os.path.join(\"datasets\/wider_face_split\/wider_face_{}.mat\".format(phase))\n    json_file = \"{}.json\".format(phase)\n\n    gt = sio.loadmat(gt_path)\n    event_list = gt.get(\"event_list\")\n    file_list = gt.get(\"file_list\")\n    face_bbox_list = gt.get(\"face_bbx_list\")\n\n    res_file = {\n        \"categories\": categories,\n        \"images\": [],\n        \"annotations\": []\n    }\n\n    annot_count = 0\n    image_id = 0\n    processed = 0\n    for event_idx, path in enumerate(event_list):\n        base_path = path[0][0]\n        for file_idx, img_name in enumerate(file_list[event_idx][0]):\n            file_path = img_name[0][0]\n            face_bbox = face_bbox_list[event_idx][0][file_idx][0]\n            num_boxes = face_bbox.shape[0]\n\n            if num_boxes > MAX_N:\n                continue\n\n            img_path = os.path.join(root_path, base_path, file_path + \".jpg\")\n            filename = os.path.join(base_path, file_path + \".jpg\")\n\n            img = Image.open(img_path)\n            img_w, img_h = img.size\n            img_elem = {\"file_name\": filename,\n                        \"height\": img_h,\n                        \"width\": img_w,\n                        \"id\": image_id}\n\n            res_file[\"images\"].append(img_elem)\n\n            for i in range(num_boxes):\n                xmin = int(face_bbox[i][0])\n                ymin = int(face_bbox[i][1])\n                xmax = int(face_bbox[i][2]) + xmin\n                ymax = int(face_bbox[i][3]) + ymin\n                w = xmax - xmin\n                h = ymax - ymin\n                area = w * h\n                poly = [[xmin, ymin],\n                        [xmax, ymin],\n                        [xmax, ymax],\n                        [xmin, ymax]]\n\n                annot_elem = {\n                    \"id\": annot_count,\n                    \"bbox\": [\n                        float(xmin),\n                        float(ymin),\n                        float(w),\n                        float(h)\n                    ],\n                    \"segmentation\": list([poly]),\n                    \"image_id\": image_id,\n                    \"ignore\": 0,\n                    \"category_id\": 0,\n                    \"iscrowd\": 0,\n                    \"area\": float(area)\n                }\n\n                res_file[\"annotations\"].append(annot_elem)\n                annot_count += 1\n\n            image_id += 1\n\n            processed += 1\n\n    with open(json_file, \"w\") as f:\n        json_str = json.dumps(res_file)\n        f.write(json_str)\n\n    print(\"Processed {} {} images...\".format(processed, phase))\nprint(\"Done.\")","ff550fea":"!python datasets\/face_to_coco.py\n\n# Move json files to datasets folder\n!mv train.json datasets && mv val.json datasets","5342d6ee":"ls datasets\/","f95bac79":"DATASET_PATH = 'datasets'\n\nTRAIN_IMG = 'datasets\/WIDER_train\/images'\nTRAIN_JSON = 'datasets\/train.json'\n\nVAL_IMG = 'datasets\/WIDER_val\/images'\nVAL_JSON = 'datasets\/val.json'","58ec122d":"!apt-get install software-properties-common -y\n!add-apt-repository ppa:ubuntu-toolchain-r\/test -y\n!apt-get update -y\n!apt-get install gcc-8 -y\n!apt-get upgrade libstdc++6 -y\ndisplay.clear_output()","91b37fbc":"ls detr\/","e93f00ef":"!python detr\/main.py --help","c2b9b5a6":"!python detr\/main.py \\\n    --batch_size=8 \\\n    --epochs=1 \\\n    --backbone='resnet101' \\\n    --num_classes=2 \\\n    --num_queries=100 \\\n    --dataset_file='face' \\\n    --data_path={DATASET_PATH} \\\n    --train_folder={TRAIN_IMG} \\\n    --train_json={TRAIN_JSON} \\\n    --val_folder={VAL_IMG} \\\n    --val_json={VAL_JSON} \\\n    --output_dir='detr\/outputs\/' \\\n    --resume='https:\/\/dl.fbaipublicfiles.com\/detr\/detr-r50-e632da11.pth' #detr-resnet50","1219dbbc":"!cp -r \/root\/..\/kaggle\/tmp\/Object-Detection-Using-DETR-CustomDataset\/detr\/outputs\/ \/root\/..\/kaggle\/working\/","17aa2a36":"pwd","faecb78b":"import os \nos.environ['KAGGLE_USERNAME'] = 'kyawlin'\nos.environ['KAGGLE_KEY'] = '24d3269babdaae2289340829b8434315'\n!kaggle datasets download -d kyawlin\/detr-w\n!unzip detr-w.zip\n!rm -r detr-w.zip","40c0f3e2":"# EvaluationCLASSES = ['face']\n\n# colors for visualization\n# COLORS = [[0.000, 0.447, 0.741]]","ee6028f9":"import torch\nimport torchvision.transforms as T\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport time \nimport math\nimport random\n%config InlineBackend.figure_format = 'retina'","a0bf192f":"ls","7f82c181":"%cd outputs\/\nTRAINED_CKPT_PATH = 'checkpoint.pth'\ncheckpoint = torch.load(TRAINED_CKPT_PATH, map_location='cpu')\nmodel = torch.hub.load('facebookresearch\/detr', 'detr_resnet50', pretrained=False, num_classes=2)\nmodel.load_state_dict(checkpoint['model'], strict=False)\n%cd ..","474ce5d3":"CLASSES = ['face']\n\n# colors for visualization\nCOLORS = [[0.000, 0.447, 0.741]]","40f150c9":"transform = T.Compose([\n    T.Resize(800),\n    T.ToTensor(),\n    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# for output bounding box post-processing\ndef box_cxcywh_to_xyxy(x):\n    x_c, y_c, w, h = x.unbind(1)\n    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n    return torch.stack(b, dim=1)\n\ndef rescale_bboxes(out_bbox, size):\n    img_w, img_h = size\n    b = box_cxcywh_to_xyxy(out_bbox)\n    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n    return b\n\ndef plot_results(pil_img, prob, boxes):\n    plt.figure(figsize=(16,10))\n    plt.imshow(pil_img)\n    ax = plt.gca()\n    colors = COLORS * 100\n    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n                                   fill=False, color=c, linewidth=3))\n        cl = p.argmax()\n        text = f'{CLASSES[cl]}: {p[cl]:0.2f}'\n        ax.text(xmin, ymin, text, fontsize=15,\n                bbox=dict(facecolor='yellow', alpha=0.5))\n    plt.axis('off')\n    plt.show()\n\ndef postprocess_img(img_path): \n  im = Image.open(img_path)\n\n  # mean-std normalize the input image (batch-size: 1)\n  img = transform(im).unsqueeze(0)\n\n  # propagate through the model\n  start = time.time()\n  outputs = model(img)\n  end = time.time()\n  print(f'Prediction time per image: {math.ceil(end - start)}s ', )\n\n  # keep only predictions with 0.7+ confidence\n  probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n  keep = probas.max(-1).values > 0.9\n\n  # convert boxes from [0; 1] to image scales\n  bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n  \n  plot_results(im, probas[keep], bboxes_scaled)","1c97524f":"# Load test image paths\n\nTEST_IMG_PATH = 'datasets\/WIDER_test\/images'\n\nimg_format = {'jpg', 'png', 'jpeg'}\npaths = list()\n\nfor obj in os.scandir(TEST_IMG_PATH):\n  if obj.is_dir():\n    paths_temp = [obj.path for obj in os.scandir(obj.path) if obj.name.split(\".\")[-1] in img_format]\n    paths.extend(paths_temp)\n\nprint('Total number of test images: ', len(paths))\nrandom.shuffle(paths)","c11fe521":"for i in paths[1:3]:\n  postprocess_img(i)","ad80fe09":"from detr.util.plot_utils import plot_logs\nfrom pathlib import Path\n\nlog_directory = [Path('outputs\/')]","41d9d64c":"fields_of_interest = (\n    'loss',\n    'mAP',\n    )\n\nplot_logs(log_directory,\n          fields_of_interest)","58e2202f":"fields_of_interest = (\n    'loss_ce',\n    'loss_bbox',\n    'loss_giou',\n    )\n\nplot_logs(log_directory,\n          fields_of_interest)","42098820":"fields_of_interest = (\n    'class_error',\n    'cardinality_error_unscaled',\n    )\n\nplot_logs(log_directory,\n          fields_of_interest)","1ce01ea2":"# Metrics Viz","ac089821":"# Training"}}