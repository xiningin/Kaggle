{"cell_type":{"397288ca":"code","cf5b4cf0":"code","5d9c59aa":"code","17805524":"code","cec80822":"code","54ea348b":"code","40da877d":"code","1146e531":"code","d104fc03":"code","96bfdd41":"code","df2ef6c7":"code","7e524916":"code","a2521361":"code","da369bc7":"code","7c59470a":"code","3a46f38c":"code","197fd508":"code","082338b7":"code","527ae4dd":"code","5180bf30":"code","d7b8dd8b":"markdown","04278894":"markdown","ff11b69e":"markdown","b95f5f7c":"markdown","f89b78fc":"markdown","11954c76":"markdown"},"source":{"397288ca":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport os.path\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport seaborn as sns\nfrom time import perf_counter\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom IPython.display import Markdown, display\n\ndef printmd(string):\n    # Print with Markdowns    \n    display(Markdown(string))","cf5b4cf0":"# Create a list with the filepaths for training and testing\ndir_ = Path('..\/input\/lego-brick-images\/dataset')\nfile_paths = list(dir_.glob(r'**\/*.png'))\nfile_paths = [str(x) for x in file_paths]\ndf = pd.DataFrame({'Filepath':file_paths})","5d9c59aa":"def get_label(string):\n    string  = ' '.join(string.split('\/')[-1].replace('.png', '').split(' ')[1:-1])\n    string = string.lower()\n    return string\n\n# Retrieve the label from the path of the pictures\ndf['Label'] = df['Filepath'].apply(lambda x: get_label(x))","17805524":"# Display some pictures of the dataset\nfig, axes = plt.subplots(nrows=4, ncols=6, figsize=(15, 7),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(df.Filepath[i]))\n    ax.set_title(df.Label[i], fontsize = 15)\nplt.tight_layout(pad=0.5)\nplt.show()","cec80822":"# Display the number of pictures of each category\nvc = df['Label'].value_counts()\nplt.figure(figsize=(20,5))\nsns.barplot(x = sorted(vc.index), y = vc, palette = \"rocket\")\nplt.title(\"Number of pictures of each category\", fontsize = 15)\nplt.xticks(rotation=90)\nplt.show()","54ea348b":"# Load the paths of the validation set\nvalidation = pd.read_csv('..\/input\/lego-brick-images\/validation.txt', names = ['Filepath'])\nvalidation['Filepath'] = validation['Filepath'].apply(lambda x: '..\/input\/lego-brick-images\/dataset\/' + x)\nvalidation.head()","40da877d":"# The paths of the validation set is already in the DataFrame df\n# Create a new column \"validation_set\" to indicate which\n# paths of picture should be in the training and validation set\ndf['validation_set'] = df['Filepath'].isin(validation['Filepath'])\ndf.head()","1146e531":"# Create a DataFrame for the training set\n# and for the test set. What is called \"validation set\"\n# will be used as test set in the workbook\n# Use only 30% of the data to speed up the model tests\ntrain_df = df[df['validation_set'] == False].sample(frac = 0.3)\ntest_df = df[df['validation_set'] == True].sample(frac = 0.3)","d104fc03":"a = 0\nprintmd(f'### Number of pictures in the train set: {train_df.shape[0]}')\nprintmd(f'### Number of pictures in the test set: {test_df.shape[0]}')","96bfdd41":"def create_gen():\n    # Load the Images with a generator and Data Augmentation\n    train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n        validation_split=0.1\n    )\n\n    test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n    )\n\n    train_images = train_generator.flow_from_dataframe(\n        dataframe=train_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=True,\n        seed=0,\n        subset='training',\n        rotation_range=30, # Uncomment to use data augmentation\n        zoom_range=0.15,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.15,\n        horizontal_flip=True,\n        fill_mode=\"nearest\"\n    )\n\n    val_images = train_generator.flow_from_dataframe(\n        dataframe=train_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=True,\n        seed=0,\n        subset='validation',\n        rotation_range=30, # Uncomment to use data augmentation\n        zoom_range=0.15,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.15,\n        horizontal_flip=True,\n        fill_mode=\"nearest\"\n    )\n\n    test_images = test_generator.flow_from_dataframe(\n        dataframe=test_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=False\n    )\n    \n    return train_generator,test_generator,train_images,val_images,test_images","df2ef6c7":"def get_model(model):\n# Load the pretained model\n    kwargs =    {'input_shape':(224, 224, 3),\n                'include_top':False,\n                'weights':'imagenet',\n                'pooling':'avg'}\n    \n    pretrained_model = model(**kwargs)\n    pretrained_model.trainable = False\n    \n    inputs = pretrained_model.input\n\n    x = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\n    x = tf.keras.layers.Dense(128, activation='relu')(x)\n\n    outputs = tf.keras.layers.Dense(46, activation='softmax')(x)\n\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n    model.compile(\n        optimizer='adam',\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model","7e524916":"# Dictionary with the models\nmodels = {\n    \"DenseNet121\": {\"model\":tf.keras.applications.DenseNet121, \"perf\":0},\n    \"MobileNetV2\": {\"model\":tf.keras.applications.MobileNetV2, \"perf\":0},\n    \"DenseNet169\": {\"model\":tf.keras.applications.DenseNet169, \"perf\":0},\n    \"DenseNet201\": {\"model\":tf.keras.applications.DenseNet201, \"perf\":0},\n    \"InceptionResNetV2\": {\"model\":tf.keras.applications.InceptionResNetV2, \"perf\":0},\n    \"InceptionV3\": {\"model\":tf.keras.applications.InceptionV3, \"perf\":0},\n    \"MobileNet\": {\"model\":tf.keras.applications.MobileNet, \"perf\":0},\n    \"MobileNetV2\": {\"model\":tf.keras.applications.MobileNetV2, \"perf\":0},\n    \"MobileNetV3Large\": {\"model\":tf.keras.applications.MobileNetV3Large, \"perf\":0},\n    \"MobileNetV3Small\": {\"model\":tf.keras.applications.MobileNetV3Small, \"perf\":0},\n    \"NASNetMobile\": {\"model\":tf.keras.applications.NASNetMobile, \"perf\":0},\n    \"ResNet101\": {\"model\":tf.keras.applications.ResNet101, \"perf\":0},\n    \"ResNet101V2\": {\"model\":tf.keras.applications.ResNet101V2, \"perf\":0},\n    \"ResNet152\": {\"model\":tf.keras.applications.ResNet152, \"perf\":0},\n    \"ResNet152V2\": {\"model\":tf.keras.applications.ResNet152V2, \"perf\":0},\n    \"ResNet50\": {\"model\":tf.keras.applications.ResNet50, \"perf\":0},\n    \"ResNet50V2\": {\"model\":tf.keras.applications.ResNet50V2, \"perf\":0},\n    \"VGG16\": {\"model\":tf.keras.applications.VGG16, \"perf\":0},\n    \"VGG19\": {\"model\":tf.keras.applications.VGG19, \"perf\":0},\n    \"Xception\": {\"model\":tf.keras.applications.Xception, \"perf\":0}\n}\n\n# Create the generators\ntrain_generator,test_generator,train_images,val_images,test_images=create_gen()\nprint('\\n')\n\n# Fit the models\nfor name, model in models.items():\n    \n    # Get the model\n    m = get_model(model['model'])\n    models[name]['model'] = m\n    \n    start = perf_counter()\n    \n    # Fit the model\n    history = m.fit(train_images,validation_data=val_images,epochs=1,verbose=1)\n    \n    # Sav the duration and the val_accuracy\n    duration = perf_counter() - start\n    duration = round(duration,2)\n    models[name]['perf'] = duration\n    print(f\"{name:20} trained in {duration} sec\")\n    \n    val_acc = history.history['val_accuracy']\n    models[name]['val_acc'] = [round(v,4) for v in val_acc]","a2521361":"# Create a DataFrame with the results\nmodels_result = []\n\nfor name, v in models.items():\n    models_result.append([ name, models[name]['val_acc'][-1], \n                          models[name]['perf']])\n    \ndf_results = pd.DataFrame(models_result, \n                          columns = ['model','val_accuracy','Training time (sec)'])\ndf_results.sort_values(by='val_accuracy', ascending=False, inplace=True)\ndf_results.reset_index(inplace=True,drop=True)\ndf_results","da369bc7":"plt.figure(figsize = (15,5))\nsns.barplot(x = 'model', y = 'val_accuracy', data = df_results)\nplt.title('Accuracy on the test set (after 1 epoch))', fontsize = 15)\nplt.ylim(0,1)\nplt.xticks(rotation=90)\nplt.show()","7c59470a":"plt.figure(figsize = (15,5))\nsns.barplot(x = 'model', y = 'Training time (sec)', data = df_results)\nplt.title('Training time for each model in sec', fontsize = 15)\nplt.xticks(rotation=90)\nplt.show()","3a46f38c":"# Use the whole data which is split into training and test datasets\n# Create a DataFrame for the training set\n# and for the test set. What is called \"validation set\"\n# will be used as test set in the workbook\ntrain_df = df[df['validation_set'] == False]\ntest_df = df[df['validation_set'] == True]\n\n# Create the generators\ntrain_generator,test_generator,train_images,val_images,test_images=create_gen()\n\n# Get the model with the highest validation score\nbest_model = df_results.iloc[0]\n\n# Create a new model\nmodel = get_model( eval(\"tf.keras.applications.\"+ best_model[0]) )\n\n# Train the model\nhistory = model.fit(train_images,\n                    validation_data=val_images,\n                    epochs=20,\n                    callbacks=[\n                        tf.keras.callbacks.EarlyStopping(\n                            monitor='val_loss',\n                            patience=3,\n                            restore_best_weights=True)]\n                    )","197fd508":"fig, axes = plt.subplots(2, 1, figsize=(15, 10))\nax = axes.flat\n\npd.DataFrame(history.history)[['accuracy','val_accuracy']].plot(ax=ax[0])\nax[0].set_title(\"Accuracy\", fontsize = 15)\nax[0].set_ylim(0,1.1)\n\npd.DataFrame(history.history)[['loss','val_loss']].plot(ax=ax[1])\nax[1].set_title(\"Loss\", fontsize = 15)\nplt.show()","082338b7":"# Predict the label of the test_images\npred = model.predict(test_images)\npred = np.argmax(pred,axis=1)\n\n# Map the label\nlabels = (train_images.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npred = [labels[k] for k in pred]\n\n# Get the accuracy on the test set\ny_test = list(test_df.Label)\nacc = accuracy_score(y_test,pred)\n\n# Display the results\nprintmd(f'## Best Model: {best_model[0]} with {acc*100:.2f}% accuracy on the test set')","527ae4dd":"# Display a confusion matrix\ncf_matrix = confusion_matrix(y_test, pred, normalize='true')\nplt.figure(figsize = (20,15))\nsns.heatmap(cf_matrix, annot=True, xticklabels = sorted(set(y_test)), yticklabels = sorted(set(y_test)),cbar=False)\nplt.title('Normalized Confusion Matrix', fontsize = 23)\nplt.xticks(fontsize=12,rotation=90)\nplt.yticks(fontsize=12)\nplt.show()","5180bf30":"# Display picture of the dataset with their labels\nfig, axes = plt.subplots(nrows=4, ncols=6, figsize=(20, 12),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(test_df.Filepath.iloc[i]))\n    ax.set_title(f\"True: {test_df.Label.iloc[i].split('_')[0]}\\nPredicted: {pred[i].split('_')[0]}\", fontsize = 15)\nplt.tight_layout()\nplt.show()","d7b8dd8b":"# 3. Test 20 canned architectures with pre-trained weights<a class=\"anchor\" id=\"3\"><\/a><a class=\"anchor\" id=\"1\"><\/a>\n\nMore info about the architectures under: [Module: tf.keras.applications](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications?hl=enhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications?hl=en)","04278894":"# 2. Load the Images with a generator and Data Augmentation<a class=\"anchor\" id=\"2\"><\/a><a class=\"anchor\" id=\"1\"><\/a>","ff11b69e":"# 5. Examples of prediction<a class=\"anchor\" id=\"5\"><\/a>\n","b95f5f7c":"# 1. Data preprocessing and visualization<a class=\"anchor\" id=\"1\"><\/a>","f89b78fc":"# Classification of Bricks\n## *Find the best model*\n\n![bricks](https:\/\/i.imgur.com\/UxJnflV.png)\n\n# Table of contents\n\n[<h3>1. Data preprocessing and visualization<\/h3>](#1)\n\n[<h3>2. Load the Images with a generator and Data Augmentation<\/h3>](#2)\n\n[<h3>3. Test 20 canned architectures with pre-trained weights<\/h3>](#3)\n\n[<h3>4. Train the best architecture<\/h3>](#4)\n\n[<h3>5. Examples of prediction<\/h3>](#5)","11954c76":"# 4. Train the architecture with the best result<a class=\"anchor\" id=\"4\"><\/a>\n*With more than one epoch*"}}