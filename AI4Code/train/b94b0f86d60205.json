{"cell_type":{"0354eb64":"code","60a8e0b9":"code","17e12474":"code","21324bdd":"code","f75233e6":"code","f9c17b1f":"code","af86f0ec":"code","c04a1897":"code","401de559":"code","1f1b224a":"code","c671695b":"code","cb7f71c7":"code","2d59e0cf":"code","71138385":"code","93837465":"code","de29e1dd":"code","400e9047":"code","a2b09f84":"code","13403027":"code","8dc9d92a":"code","2bfa132a":"code","1ed12c14":"code","308f925a":"code","1c90637c":"code","e9d4d737":"code","e5587464":"code","8938ae17":"code","618f2ada":"code","3141416b":"code","cb4dfa15":"code","65f934b0":"code","125b0eb6":"code","7222e532":"code","1c0d88fd":"code","2579f39c":"code","8b95ef28":"code","fa2b3b0c":"code","e4a0d110":"code","de85ef79":"code","7f5413c3":"code","d88af96f":"code","8c225f35":"code","dcc36a65":"code","6c1a0717":"code","bcf8aa82":"code","07cda888":"code","f11c0d6a":"code","10554e16":"code","8e2fd3be":"code","8b318ce2":"code","c27a69e0":"code","098e9be0":"code","8fe1d09d":"code","bc57589f":"code","866b50e1":"code","b9a2ca9c":"code","b8ebd5b2":"code","aff4b599":"code","a272ea2c":"code","53677aa3":"code","87892ed5":"code","1f102da4":"code","9a7ae981":"code","7b800c3d":"code","4b24e13c":"code","29a36000":"code","b5209031":"code","516eb19d":"code","9eb3c488":"code","9a101b40":"code","242c44ce":"code","c8622289":"code","a06f2b42":"code","3e45b002":"code","644a4335":"code","84613647":"code","a4c03976":"code","c87d24ad":"code","c5b33582":"code","2a5f20ec":"code","5801f0d6":"code","988ed206":"code","dd047efb":"code","2ebeae60":"code","4756b7d2":"code","9ad033a0":"code","1aab9077":"code","018ee781":"code","74828202":"code","35d06ffa":"code","bf9a1e0c":"code","a6ab132d":"code","cf063ea7":"code","8c48ce57":"code","008f68a4":"code","47321911":"code","315a8c8d":"markdown","4ad98a4f":"markdown","456e02ad":"markdown","295d6b5b":"markdown","4176af1b":"markdown","00fd7835":"markdown","13aa859f":"markdown","74bf7971":"markdown","d6d834b4":"markdown","84a56dcb":"markdown","0160fbc3":"markdown","f0ab1e28":"markdown","cab8047e":"markdown","77392692":"markdown","b3f992c5":"markdown","4c2e17b1":"markdown","42f77451":"markdown","0dc7ff1e":"markdown","b1c7d2f5":"markdown","7000ae3f":"markdown","c48230a8":"markdown","e010a72f":"markdown","ac1dab2e":"markdown","e47fdcfd":"markdown","a165881d":"markdown","8b1ecbf8":"markdown","da4499a9":"markdown","0446d3b1":"markdown","a23bc0dd":"markdown","273de27e":"markdown","58ce6c7d":"markdown","fdbfd38a":"markdown","c2555a70":"markdown","d9a79a02":"markdown","a82f41ad":"markdown","1be57a8d":"markdown","64792765":"markdown","5628a7f9":"markdown","0c5e82e3":"markdown","f50aea73":"markdown","2842ee9b":"markdown","96ab0bc5":"markdown","023c4075":"markdown","9012e9e1":"markdown","854900b9":"markdown","80415d15":"markdown","14a1f244":"markdown","4b168ac3":"markdown","84b69e21":"markdown","4c8e6396":"markdown","4d979177":"markdown","7df9e703":"markdown","2b8322d2":"markdown","7c8140d5":"markdown","ac2bf5e1":"markdown","b3e14904":"markdown","ae50c6e8":"markdown","8cb8e0b0":"markdown","e2ea3d3d":"markdown","47921c7a":"markdown","c94ed020":"markdown","009f4e76":"markdown","a57fbb1d":"markdown","5b440cec":"markdown","d8e176f5":"markdown","62037e8a":"markdown","d2797969":"markdown","2c5c72a0":"markdown","9f3f9db4":"markdown","2f0a9cce":"markdown","49849181":"markdown"},"source":{"0354eb64":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt # data visualization\nimport seaborn as sns # advanced data visualization\n\nfrom sklearn.model_selection import train_test_split # split data into train and test\nfrom sklearn.preprocessing import MinMaxScaler # scale the data between 0 - 1\nfrom tensorflow.keras.models import Sequential # initiate the mode \nfrom tensorflow.keras.layers import Dense, Activation, Dropout # add the layers\nfrom tensorflow.keras.optimizers import Adam # optimizer \n\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard # Early Stopping and Tensor Board\n\nfrom sklearn.metrics import classification_report,confusion_matrix # Model Evaluation \nfrom tensorflow.keras.models import load_model # load the model\n\nfrom imblearn.under_sampling import RandomUnderSampler # Balance the Data\n\nfrom datetime import datetime # Date and Time ","60a8e0b9":"# read in the file containing all the information about the dataset  \ndata_info = pd.read_csv('..\/input\/lending-club-loan-data-most-accurate\/lending_club_info.csv', index_col='LoanStatNew')\ndata_info","17e12474":"# Create the helper function \ndef feat_info(col_name):\n    print(data_info.loc[col_name]['Description'])","21324bdd":"feat_info('mort_acc')","f75233e6":"# load the data \ndf = pd.read_csv('..\/input\/lending-club-loan-data-most-accurate\/lending_club_loan_two.csv')","f9c17b1f":"# top 5 rows \ndf.head()","af86f0ec":"# column names\ndf.columns ","c04a1897":"# main info\ndf.info()","401de559":"# distribution of numerical features \ndf.describe().round(2)","1f1b224a":"# check the balance of the outcome feature\nplt.figure(figsize = (8,4), dpi = 100)\nsns.countplot(data = df, x = 'loan_status')","c671695b":"# check the balance of the outcome featur\nval_counts  = df['loan_status'].value_counts(normalize = True) * 100\npd.DataFrame(val_counts.round())","cb7f71c7":"# Inspect the distribution of loan amounts\nplt.figure(figsize = (8,4), dpi = 100)\nsns.histplot(data = df, x = 'loan_amnt', bins = 40)\nplt.show()","2d59e0cf":"# Correlation Matrix\nplt.figure(figsize = (12,8), dpi = 100)\nsns.heatmap(df.corr(), annot = True, cmap = 'viridis', vmin = -1, vmax = 1)\nplt.show()","71138385":"feat_info('installment')","93837465":"feat_info('loan_amnt')","de29e1dd":"# Inspect the relationship between loan amount and installment\nplt.figure(figsize = (8,4), dpi = 100)\nsns.scatterplot(data = df, x = 'loan_amnt', y = 'installment', alpha = 0.5)\nplt.show()","400e9047":"# The relationship between the loan_status and the Loan Amount\nplt.figure(figsize = (8,4), dpi = 100)\nsns.boxplot(x='loan_status',y='loan_amnt',data=df)\nplt.show()","a2b09f84":"#Calculate the summary statistics for the loan amount, grouped by the loan_status.\ndf.groupby('loan_status')['loan_amnt'].describe().round()","13403027":"# What are the unique possible grades and subgrades?\nsorted(df['grade'].unique())","8dc9d92a":"sorted(df['sub_grade'].unique())","2bfa132a":"# Create a countplot per grade. Set the hue to the loan_status label.\norder = sorted(df['grade'].unique())\nplt.figure(figsize = (8,4), dpi = 100)\nsns.countplot(x='grade',data=df,hue='loan_status', order = order)\nplt.show()","1ed12c14":"#Display a count plot per subgrade\nplt.figure(figsize=(12,4), dpi = 100)\norder = sorted(df['sub_grade'].unique())\nsns.countplot(x='sub_grade',data=df,order = order,palette='coolwarm')\nplt.show()","308f925a":"# Create a countplot per sub_grade. Set the hue to the loan_status label.\norder = sorted(df['sub_grade'].unique())\nplt.figure(figsize = (12,4), dpi = 100)\nsns.countplot(x='sub_grade',data=df,hue='loan_status', order = order, palette='Set2')\nplt.show()","1c90637c":"# Zoom in for F and G\nfiltered_f_g = df[(df.grade == 'F') | (df.grade == 'G')]\norder = sorted(filtered_f_g['sub_grade'].unique())\nplt.figure(figsize = (8,4), dpi = 100)\nsns.countplot(x='sub_grade', data=filtered_f_g, hue='loan_status', order = order, palette='Set2')\nplt.show()","e9d4d737":"# Create a new column called 'load_repaid' which will contain a 1 if the loan status was \"Fully Paid\" and a 0 if it was \"Charged Off\".\ndf['loan_repaid'] = df['loan_status'].apply(lambda x: 1 if x == 'Fully Paid' else 0)","e5587464":"# Create a bar plot showing the correlation of the numeric features to the new loan_repaid column\nplt.figure(figsize = (8,4), dpi = 100)\ndf.corr()['loan_repaid'].sort_values().drop('loan_repaid').plot(kind='bar')\nplt.show()","8938ae17":"# detecting missing values as count and percentages \ndef get_missing(df):\n    missing_values_count = df.isna().sum()\n    missing_values_percent = (df.isna().sum()\/len(df) * 100).round(1)\n    missing_values_percent_sign = missing_values_percent.astype(str) + '%'\n    missing_dataframe = pd.DataFrame({\"Missing Count\": missing_values_count,\n                                        'Missing Percent': missing_values_percent_sign})\n    return missing_dataframe[missing_dataframe['Missing Count'] > 0]","618f2ada":"get_missing(df)","3141416b":"feat_info('emp_title')\nprint('---------------')\nfeat_info('emp_length')","cb4dfa15":"# How many unique employment job titles are there?\ndf['emp_title'].nunique()","65f934b0":"# value counts \ndf['emp_title'].value_counts()","125b0eb6":"# drop emp_title\ndf = df.drop('emp_title', axis = 1)","7222e532":"sorted(df['emp_length'].dropna().unique())","1c0d88fd":"emp_length_order = [ '< 1 year',\n                      '1 year',\n                     '2 years',\n                     '3 years',\n                     '4 years',\n                     '5 years',\n                     '6 years',\n                     '7 years',\n                     '8 years',\n                     '9 years',\n                     '10+ years']","2579f39c":"# Zoom in for F and G\nplt.figure(figsize = (8,4), dpi = 100)\nsns.countplot(x='emp_length',data=df,order=emp_length_order, hue = 'loan_status')\nplt.xticks(rotation = 90)\nplt.show()","8b95ef28":"emp_co = df[df['loan_status']==\"Charged Off\"].groupby(\"emp_length\").count()['loan_status']\nemp_fp = df[df['loan_status']==\"Fully Paid\"].groupby(\"emp_length\").count()['loan_status']\nemp_len = emp_co\/(emp_fp+emp_co) * 100\npd.DataFrame(emp_len.round())","fa2b3b0c":"plt.figure(figsize = (8,4), dpi = 100)\nemp_len.round().plot(kind = 'bar')\nplt.xticks(rotation = 90)\nplt.show()","e4a0d110":"df = df.drop('emp_length',axis=1)","de85ef79":"get_missing(df)","7f5413c3":"# Review the title column vs the purpose column\ndf['purpose'].head(10)","d88af96f":"# Review the title column vs the purpose column\ndf['title'].head(10)","8c225f35":"df = df.drop('title', axis = 1)","dcc36a65":"feat_info('mort_acc')","6c1a0717":"# value counts \ndf['mort_acc'].value_counts()","bcf8aa82":"print(\"Correlation with the mort_acc column\")\ndf.corr()['mort_acc'].sort_values()","07cda888":"print(\"Mean of mort_acc column per total_acc\")\ndf.groupby('total_acc').mean()['mort_acc']","f11c0d6a":"total_acc_avg = df.groupby('total_acc').mean()['mort_acc']\ntotal_acc_avg[2.0]","10554e16":"def fill_mort_acc(total_acc,mort_acc):\n    '''\n    Accepts the total_acc and mort_acc values for the row.\n    Checks if the mort_acc is NaN , if so, it returns the avg mort_acc value\n    for the corresponding total_acc value for that row.\n    \n    total_acc_avg here should be a Series or dictionary containing the mapping of the\n    groupby averages of mort_acc per total_acc values.\n    '''\n    if np.isnan(mort_acc):\n        return total_acc_avg[total_acc]\n    else:\n        return mort_acc","8e2fd3be":"df['mort_acc'] = df.apply(lambda x: fill_mort_acc(x['total_acc'], x['mort_acc']), axis=1)","8b318ce2":"get_missing(df)","c27a69e0":"df = df.dropna()","098e9be0":"get_missing(df)","8fe1d09d":"df.select_dtypes(['object']).columns","bc57589f":"df['term'].value_counts()","866b50e1":"df['term'] = df['term'].apply(lambda term: int(term[:3]))","b9a2ca9c":"df = df.drop('grade',axis=1)","b8ebd5b2":"subgrade_dummies = pd.get_dummies(df['sub_grade'],drop_first=True)\ndf = pd.concat([df.drop('sub_grade',axis=1),subgrade_dummies],axis=1)","aff4b599":"df.columns","a272ea2c":"df.select_dtypes(['object']).columns","53677aa3":"dummies = pd.get_dummies(df[['verification_status', 'application_type','initial_list_status','purpose' ]],drop_first=True)\ndf = df.drop(['verification_status', 'application_type','initial_list_status','purpose'],axis=1)\ndf = pd.concat([df,dummies],axis=1)","87892ed5":"df['home_ownership'].value_counts()","1f102da4":"df['home_ownership']=df['home_ownership'].replace(['NONE', 'ANY'], 'OTHER')\n\ndummies = pd.get_dummies(df['home_ownership'],drop_first=True)\ndf = df.drop('home_ownership',axis=1)\ndf = pd.concat([df,dummies],axis=1)","9a7ae981":"df['address'].value_counts()","7b800c3d":"df['zip_code'] = df['address'].apply(lambda address:address[-5:])","4b24e13c":"df['zip_code'].value_counts()","29a36000":"dummies = pd.get_dummies(df['zip_code'],drop_first=True)\ndf = df.drop(['zip_code','address'],axis=1)\ndf = pd.concat([df,dummies],axis=1)","b5209031":"df = df.drop('issue_d',axis=1)","516eb19d":"df['earliest_cr_line'].value_counts()","9eb3c488":"df['earliest_cr_year'] = df['earliest_cr_line'].apply(lambda date:int(date[-4:]))\ndf = df.drop('earliest_cr_line',axis=1)","9a101b40":"df['earliest_cr_year'].value_counts()","242c44ce":"df.select_dtypes(['object']).columns","c8622289":"df = df.drop('loan_status',axis=1)","a06f2b42":"X = df.drop('loan_repaid',axis=1).values\ny = df['loan_repaid'].values","3e45b002":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=101)","644a4335":"scaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","84613647":"model = Sequential()\n\nmodel.add(Dense(78, activation = 'relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(39, activation = 'relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(19, activation = 'relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(1, activation = 'sigmoid'))\n\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam');","a4c03976":"model.fit(X_train, y_train, epochs = 25, batch_size = 256, validation_data = (X_test, y_test));","c87d24ad":"model.save('full_data_project_model.h5')  ","c5b33582":"losses = pd.DataFrame(model.history.history)\nlosses[['loss','val_loss']].plot();","2a5f20ec":"y_pred = model.predict(X_test)\npredictions = np.round(y_pred).astype(int)\nprint(classification_report(y_test,predictions))","5801f0d6":"df['loan_repaid'].value_counts(normalize = True) * 100","988ed206":"rus = RandomUnderSampler(sampling_strategy=1) # Numerical value\n# rus = RandomUnderSampler(sampling_strategy=\"not minority\") # String\nX_res, y_res = rus.fit_resample(X, y)","dd047efb":"pd.DataFrame(y).value_counts()","2ebeae60":"pd.DataFrame(y_res).value_counts()","4756b7d2":"# Train test split\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.20, random_state=101)\n\n# Scale the data\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Build the model\nmodel = Sequential()\nmodel.add(Dense(78, activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(39, activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(19, activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation = 'sigmoid'))\n\n# Compile the model\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam')\n\n# Early stopping\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)\n\n# Fit the model\nmodel.fit(X_train, y_train, epochs = 25, batch_size = 256, validation_data = (X_test, y_test));","9ad033a0":"losses = pd.DataFrame(model.history.history)\nlosses[['loss','val_loss']].plot();","1aab9077":"y_pred = model.predict(X_test)\npredictions = np.round(y_pred).astype(int)\nprint(classification_report(y_test,predictions))","018ee781":"datetime.now().strftime(\"%Y-%m-%d--%H%M\")","74828202":"# Get the working directory \npwd","35d06ffa":"# Create the call back\nlog_directory = 'logs\/fit'\n\nboard = TensorBoard(log_dir=log_directory,histogram_freq=1,\n    write_graph=True,\n    write_images=True,\n    update_freq='epoch',\n    profile_batch=2,\n    embeddings_freq=1)","bf9a1e0c":"# Train test split\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.20, random_state=101)\n\n# Scale the data\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Build the model\nmodel = Sequential()\nmodel.add(Dense(78, activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(39, activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(19, activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation = 'sigmoid'))\n\n# Compile the model\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam');\n\n# Early stopping\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25);\n\n# Fit the model after adding the call back named board\nmodel.fit(X_train, y_train, epochs = 25, batch_size = 256, validation_data = (X_test, y_test), callbacks = [board]);","a6ab132d":"print(log_directory)","cf063ea7":"pwd","8c48ce57":"%load_ext tensorboard\n%tensorboard --logdir logs\/fit","008f68a4":"from tensorboard import notebook\nnotebook.display(port=6006, height=1000) ","47321911":"notebook.list()","315a8c8d":"> Let's now go through all the string features to see what we should do with them.","4ad98a4f":"> It make sense to be highly correlated, because the higher loans means higher installments and vise versa. This relationship is governed by a fixed formula.","456e02ad":"# Tensorboard: Visualize the Model","295d6b5b":"> Convert these to dummy variables, but replace NONE and ANY with OTHER, so that we end up with just 4 categories, MORTGAGE, RENT, OWN, OTHER","4176af1b":"### earliest_cr_line","00fd7835":"> Let's feature engineer a zip code column from the address in the data set","13aa859f":"> Given historical data on loans given out with information on whether or not the borrower defaulted (charge-off), can we build a model that can predict wether or not a borrower will pay back their loan? This way in the future when we get a new potential customer we can assess whether or not they are likely to pay back the loan.","74bf7971":"### address","d6d834b4":"> Now make this zip_code column into dummy variables","84a56dcb":"> Create a count plot of the emp_length feature column. ","0160fbc3":"revol_util and the pub_rec_bankruptcies have missing data points, but they account for less than 0.5% of the total data. Go ahead and remove the rows that are missing those values in those columns with dropna().","f0ab1e28":"### emp_length","cab8047e":"## Creating the Tensorboard Callback","77392692":"> If the loan amount is higher we have a slight increase in the likelihood of being charged off. This makes sense, it is often harder to repay larger loans than smaller loans","b3f992c5":"# Creating the Model","4c2e17b1":"> The majority work for 10+ years.\n\n> This still doesn't really inform us if there is a strong relationship between employment length and being charged off, what we want is the percentage of charge offs per category. Essentially informing us what percent of people per employment category didn't pay back their loan.\n\n> If the ratio between the two bars is the same across all categories, then this feature will not be very informative and we can safely drop it","42f77451":"> By only balancing the data we were able to increase the F1 score on the \"0\" class from 60 percent to 80 percent! \n\n> Go a head and try to tune the model parameters and see how to further improve the results","0dc7ff1e":"### term","b1c7d2f5":"# Data Pre-Processing: Missing Data","7000ae3f":"### The relationship between loan_status and Loan Amount","c48230a8":"# Categorical and Dummy Variables","e010a72f":"Set X and y variables to the values of the features and label.","ac1dab2e":"> It is obvious that F and G grades have a high default rate. We will need to have a closer look at them.","e47fdcfd":"# Balance the Data and Re-Train","a165881d":"### Check the balance of the outcome feature","8b1ecbf8":"> If your sub grade is G5: the probability that you will default is almost 50%","da4499a9":"### Convert loan status to numeric","0446d3b1":"> The title column is simply a string subcategory\/description of the purpose column\n\n> We can safely drop it","a23bc0dd":"# Exploratory Data Analysis","273de27e":"# Goal ","58ce6c7d":"## Train the model again with the call back  ","fdbfd38a":"## Run TesorBoard ","c2555a70":"### title","d9a79a02":"> The data is clearly imbalanced. This is a typical scenario when dealing with problems related to loan default, spam or fraud detection. We can expect to do very well in terms of accuracy, but our precision and recall will be the true matrix that our model should be evaluated based on. We do not expect the model to perform very well on those metrics.","a82f41ad":"### verification_status, application_type, initial_list_status, purpose ","1be57a8d":"# Load and Check the Data ","64792765":"> drop the load_status column we created earlier, since its a duplicate of the loan_repaid column. We'll use the loan_repaid column since its already in 0s and 1s.","5628a7f9":">  Plot out the validation loss versus the training loss.","0c5e82e3":"> Create predictions from the X_test set and display a classification report and confusion matrix for the X_test set.","f50aea73":"### Inspect the distribution of loan amounts","2842ee9b":"### home_ownership","96ab0bc5":"> As interest rate goes up it is harder to pay off the loan ","023c4075":"> This feature will not be informative because half people (173k) have unique titles, so we will drop it","9012e9e1":"> Save the model","854900b9":"> This appears to be a historical time stamp feature. Extract the year from this feature using a .apply function, then convert it to a numeric feature.","80415d15":"### emp_title","14a1f244":"> There are many ways we could deal with this missing data. We could attempt to build a simple model to fill it in, such as a linear model, we could just fill it in based on the mean of the other columns, or you could even bin the columns into categories and then set NaN as its own category. There is no 100% correct approach! Let's review the other columsn to see which most highly correlates to mort_acc","4b168ac3":"### grade","84b69e21":"> Looks like the total_acc feature correlates with the mort_acc , this makes sense! Let's try this fillna() approach. We will group the dataframe by the total_acc and calculate the mean value for the mort_acc per total_acc entry","4c8e6396":"> Convert the term feature into either a 36 or 60 integer numeric data type","4d979177":">  Charge off rates are extremely similar across all employment lengths.\n\n> We can safely drop it ","7df9e703":"> This would be data leakage, we wouldn't know beforehand whether or not a loan would be issued when using our model, so in theory we wouldn't have an issue_date, drop this feature.","2b8322d2":"# Train Test Split and Scaling","7c8140d5":"### The relationship between Grades and loan_status","ac2bf5e1":"> We can see that \"loan amount\" has perfect correlation with \"installment\". Let's explore that feature further. They might not be distinct features but a duplicate version of each other.","b3e14904":"> Let's fill in the missing mort_acc values based on their total_acc value. If the mort_acc is missing, then we will fill in that missing value with the mean value corresponding to its total_acc value from the Series we created above.","ae50c6e8":"# Helper Function\nA function that returns the description of any given feature","8cb8e0b0":"> List all the columns that are currently non-numeric","e2ea3d3d":"> If I created a model that predicts that any loan will be repaid I will be 80 percent accurate. So our true metric is F1 score on \"0\" class: we are only 61 percent accurate when predicting those who does not pay their loans ","47921c7a":"> Let's examine emp_title and emp_length to see whether it will be okay to drop them.","c94ed020":"### issue_d","009f4e76":"# Evaluating Model Performance","a57fbb1d":"> Because the data is not well balanced, a model that predicts everything as loan repaid will have a resonable accuracy. Lets check that! ","5b440cec":"> We already know grade is part of sub_grade, so just drop the grade feature.","d8e176f5":"# Required Packages","62037e8a":"### Correlation matrix","d2797969":"> How many unique employment job titles are there?","2c5c72a0":"### mort_acc","9f3f9db4":"> Convert the subgrade into dummy variables.","2f0a9cce":"> Lets now rerun our model again ","49849181":"### The rest "}}