{"cell_type":{"01f2dd9e":"code","94cb2c56":"code","8c86d0e4":"code","03a4caad":"code","d5be85ca":"code","1c52b683":"code","f500bb4a":"code","a2216c1c":"code","9894e71c":"code","5198416e":"code","be6c0b1b":"code","14f0e536":"code","f4d9c608":"code","f86ac797":"code","e10edbfb":"code","9b713c38":"code","8b66a3bf":"code","fc646c84":"code","e5f45d51":"code","610bdf4a":"code","7adc6bf2":"code","50e62417":"code","7dc3de7a":"code","c1aab32f":"code","344dd015":"code","63f06aef":"code","accc202d":"code","f4cfcf5d":"code","e90a2ca1":"code","9f89d573":"code","86233c22":"code","0744b8fa":"code","1c8ef694":"code","71a0c8b7":"code","70a2d3e0":"code","6936f266":"code","fd67b67e":"code","337d9cc8":"code","371766ad":"code","d2e9cd32":"code","9f7ea798":"code","7a2be8df":"code","c4f601b3":"code","b7669730":"code","b1ba8472":"code","be230687":"code","5a2c275e":"code","bec5e93c":"code","4a8594f3":"code","58dda8af":"code","d33b5833":"code","c5343d35":"code","cf9e4b17":"code","74d90436":"code","b9effffb":"code","d785b44d":"code","3d5b7f27":"code","0b6efeb7":"code","8cc5cf99":"code","878330ed":"code","09ba9379":"markdown","ca827237":"markdown","bc652178":"markdown"},"source":{"01f2dd9e":"import sys\nimport os\n\nimport albumentations\nimport pandas as pd\nimport numpy as np\n\nimport gc\nfrom glob import glob\nimport pickle\nimport json\nimport subprocess\nfrom sklearn import metrics\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, RepeatedStratifiedKFold\n\n\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import patches, text, patheffects\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom torchvision.io import read_image\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.optimize import minimize\nimport seaborn as sns\n\nimport cv2\nfrom tqdm import tqdm\n\nimport ast","94cb2c56":"# !pip install --no-index --find-links ..\/input\/timmset2 tez\n# !pip install --no-index --find-links ..\/input\/timmset2 timm\n!pip install --no-index --find-links ..\/input\/timmset2 pytorch-lightning","8c86d0e4":"sys.path.append('..\/input\/timmset2\/pytorch-image-models')\nimport timm\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.utilities.seed import seed_everything\nfrom pytorch_lightning import callbacks\nfrom pytorch_lightning.callbacks.progress import ProgressBarBase\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning import LightningDataModule, LightningModule","03a4caad":"!pip install --no-index --find-links ..\/input\/yetanotherefficientdetpytorch webcolors","d5be85ca":"os.chdir('\/kaggle\/working')","1c52b683":"sys.path.append('..\/input\/yetanotherefficientdetpytorch')\nsys.path.append('..\/input\/yetanotherefficientdetpytorch\/Yet-Another-EfficientDet-Pytorch')\n# os.chdir('.\/yetanotherefficientdetpytorch\/Yet-Another-EfficientDet-Pytorch')\n# sys.path.append('.')\n# from backbone import EfficientDetBackbon\nfrom efficientdet.utils import BBoxTransform, ClipBoxes\nfrom utils.utils import preprocess, invert_affine, postprocess, STANDARD_COLORS, standard_to_bgr, get_index_label, plot_one_box\nfrom torch.backends import cudnn\nimport time","f500bb4a":"from efficientdet.model import BiFPN, Regressor, Classifier, EfficientNet\nfrom efficientdet.utils import Anchors\n\n\nclass EfficientDetBackbone(nn.Module):\n    def __init__(self, num_classes=80, compound_coef=0, load_weights=False, **kwargs):\n        super(EfficientDetBackbone, self).__init__()\n        self.compound_coef = compound_coef\n\n        self.backbone_compound_coef = [0, 1, 2, 3, 4, 5, 6, 6, 7]\n        self.fpn_num_filters = [64, 88, 112, 160, 224, 288, 384, 384, 384]\n        self.fpn_cell_repeats = [3, 4, 5, 6, 7, 7, 8, 8, 8]\n        self.input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536, 1536]\n        self.box_class_repeats = [3, 3, 3, 4, 4, 4, 5, 5, 5]\n        self.pyramid_levels = [5, 5, 5, 5, 5, 5, 5, 5, 6]\n        self.anchor_scale = [4., 4., 4., 4., 4., 4., 4., 5., 4.]\n        self.aspect_ratios = kwargs.get('ratios', [(1.0, 1.0), (1.4, 0.7), (0.7, 1.4)])\n        self.num_scales = len(kwargs.get('scales', [2 ** 0, 2 ** (1.0 \/ 3.0), 2 ** (2.0 \/ 3.0)]))\n        conv_channel_coef = {\n            # the channels of P3\/P4\/P5.\n            0: [40, 112, 320],\n            1: [40, 112, 320],\n            2: [48, 120, 352],\n            3: [48, 136, 384],\n            4: [56, 160, 448],\n            5: [64, 176, 512],\n            6: [72, 200, 576],\n            7: [72, 200, 576],\n            8: [80, 224, 640],\n        }\n\n        num_anchors = len(self.aspect_ratios) * self.num_scales\n\n        self.bifpn = nn.Sequential(\n            *[BiFPN(self.fpn_num_filters[self.compound_coef],\n                    conv_channel_coef[compound_coef],\n                    True if _ == 0 else False,\n                    attention=True if compound_coef < 6 else False,\n                    use_p8=compound_coef > 7)\n              for _ in range(self.fpn_cell_repeats[compound_coef])])\n\n        self.num_classes = num_classes\n        self.regressor = Regressor(in_channels=self.fpn_num_filters[self.compound_coef], num_anchors=num_anchors,\n                                   num_layers=self.box_class_repeats[self.compound_coef],\n                                   pyramid_levels=self.pyramid_levels[self.compound_coef])\n        self.classifier = Classifier(in_channels=self.fpn_num_filters[self.compound_coef], num_anchors=num_anchors,\n                                     num_classes=num_classes,\n                                     num_layers=self.box_class_repeats[self.compound_coef],\n                                     pyramid_levels=self.pyramid_levels[self.compound_coef])\n\n        self.anchors = Anchors(anchor_scale=self.anchor_scale[compound_coef],\n                               pyramid_levels=(torch.arange(self.pyramid_levels[self.compound_coef]) + 3).tolist(),\n                               **kwargs)\n\n        self.backbone_net = EfficientNet(self.backbone_compound_coef[compound_coef], load_weights)\n\n    def freeze_bn(self):\n        for m in self.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.eval()\n\n    def forward(self, inputs):\n        max_size = inputs.shape[-1]\n\n        _, p3, p4, p5 = self.backbone_net(inputs)\n\n        features = (p3, p4, p5)\n        features = self.bifpn(features)\n\n        regression = self.regressor(features)\n        classification = self.classifier(features)\n        anchors = self.anchors(inputs, inputs.dtype)\n\n        return features, regression, classification, anchors\n\n    def init_backbone(self, path):\n        state_dict = torch.load(path)\n        try:\n            ret = self.load_state_dict(state_dict, strict=False)\n            print(ret)\n        except RuntimeError as e:\n            print('Ignoring ' + str(e) + '\"')","a2216c1c":"!pip install --no-index --find-links ..\/input\/monaiset monai\nimport monai.transforms as mT","9894e71c":"global IMG_SIZE, BATCH_SIZE, MODEL_NAME\n# Constants\nIMG_SIZE = 224\nCHANNELS = 3\nQ = 30\nSEED = 2051\n\n# BATCH_SIZE = 8\nBATCH_SIZE = 32\n\nREPETE_NUMBER = 3\nLR = 0.000005\nWD = 0.0000005\nEPOCHS = 100\nEARRY_STOP = 3\n\n\nMODEL_NAME = \"model\"\n\nOPT_NAME = 'torch.optim.AdamW'\nOPT_PARAMS = {'lr': 1e-5}\n\nSCH_NAME = 'torch.optim.lr_scheduler.CosineAnnealingWarmRestarts'\nSCH_PARAMS = {\n  'T_0': 20,\n  'eta_min': 1e-4,\n  }\nTRAINER = {\n  'gpus': 1,\n  'accumulate_grad_batches': 1,\n  'progress_bar_refresh_rate': 1,\n  'fast_dev_run': False,\n  'num_sanity_val_steps': 0,\n  'resume_from_checkpoint': None,\n  }\nTRN_LOADER = {\n  'batch_size': BATCH_SIZE,\n  'shuffle': True,\n  'num_workers': 4,\n  'pin_memory': False,\n  'drop_last': True,\n  }\nVAL_LOADER = {\n  'batch_size': BATCH_SIZE,\n  'shuffle': False,\n  'num_workers': 4,\n  'pin_memory': False,\n  'drop_last': False\n  }\nLOSS = 'nn.BCEWithLogitsLoss'\n\nGRADCAM_BATCH_SIZE = 16\n\n\nMD_NAME = 'nb019-v001'\n\nSVR_PATH = '.'\n# SVR_PATH = '..\/input\/nb019-svr'\nSVR_RATIO = 0.4","5198416e":"# Check Val\nfrom tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n\ndef get_result(folder_names):\n    val_result = pd.DataFrame()\n\n    model_names = []\n    model_vals = []\n\n    for folder_name in folder_names:\n        paths = glob(f'{folder_name}\/version_*')\n        for path in paths:\n            print(path)\n            e_path = glob(f'{path}\/events*')[0]\n            event_acc = EventAccumulator(path, size_guidance={'scalars': 0})\n            event_acc.Reload()\n\n            scalars = {}\n            for tag in event_acc.Tags()['scalars']:\n                events = event_acc.Scalars(tag)\n                scalars[tag] = [event.value for event in events]\n\n            model_names.append(path)\n            model_vals.append(min(scalars['val_loss']))\n\n    val_result['name'] = model_names\n    val_result['val'] = model_vals\n    return val_result","be6c0b1b":"folder_names = [\n    '..\/input\/nb019-v005',\n    '..\/input\/nb019-v016',\n    '..\/input\/nb019-v018',\n    \n    '..\/input\/nb021-v013',\n    '..\/input\/nb021-v014',\n    '..\/input\/nb021nc-v004',\n    '..\/input\/nb021nc-v006',\n]\nresult_normal = get_result(folder_names)\nresult_normal = result_normal.sort_values('val')\nresult_normal = result_normal[:8]\n\nresult_normal\n\n\n# result_normal = pd.DataFrame()\n\n# model_names = []\n# model_vals = []\n\n# paths = [\n# #     '..\/input\/nb019-v005\/version_0',\n# #     '..\/input\/nb019-v016\/version_3',\n#     '..\/input\/nb019-v005\/version_4',\n#     '..\/input\/nb019-v018\/version_1',\n#     '..\/input\/nb019-v005\/version_3',\n#     '..\/input\/nb019-v005\/version_2',\n#     '..\/input\/nb019-v018\/version_0',\n    \n# #     '..\/input\/nb021-v013\/version_4',\n#     '..\/input\/nb021-v013\/version_0',\n#     '..\/input\/nb021-v013\/version_5',\n#     '..\/input\/nb021-v012\/version_5',\n    \n#     '..\/input\/nb021-v012\/version_2',\n#     '..\/input\/nb021-v013\/version_1',\n# ]\n# for path in paths:\n#     print(path)\n#     e_path = glob(f'{path}\/events*')[0]\n#     event_acc = EventAccumulator(path, size_guidance={'scalars': 0})\n#     event_acc.Reload()\n\n#     scalars = {}\n#     for tag in event_acc.Tags()['scalars']:\n#         events = event_acc.Scalars(tag)\n#         scalars[tag] = [event.value for event in events]\n\n#     model_names.append(path)\n#     model_vals.append(min(scalars['val_loss']))\n\n# result_normal['name'] = model_names\n# result_normal['val'] = model_vals\n# result_normal\n","14f0e536":"result_normal['nb'] = result_normal['name'].str.split('\/', expand=True)[2]\nresult_normal.groupby('nb')['val'].describe()","f4d9c608":"folder_names = [\n    '..\/input\/nb021nc-v010',\n    '..\/input\/nb021nc-v011',\n]\nresult_swin384 = get_result(folder_names)\nresult_swin384 = result_swin384.sort_values('val')\nresult_swin384 = result_swin384[:5]\nresult_swin384","f86ac797":"folder_names = [\n    '..\/input\/nb021-v017',\n    '..\/input\/nb021nc-v007',\n    '..\/input\/nb021nc-v005',\n]\nresult_xcit = get_result(folder_names)\nresult_xcit = result_xcit.sort_values('val')\nresult_xcit = result_xcit[:5]\nresult_xcit","e10edbfb":"# result_xcit['nb'] = result_xcit['name'].str.split('\/', expand=True)[2]\n# result_xcit.groupby('nb')['val'].describe()","9b713c38":"# folder_names = [  \n#     '..\/input\/nb021-v018',\n#     '..\/input\/nb021-v019',\n# ]\n# result_nfnet = get_result(folder_names)\n# # result_nfnet = result_nfnet.sort_values('val')\n# # result_nfnet = result_nfnet[:2]\n# result_nfnet","8b66a3bf":"folder_names = [  \n    '..\/input\/nb021nc-v009',\n]\nresult_nfnet2 = get_result(folder_names)\nresult_nfnet2 = result_nfnet2.sort_values('val')\nresult_nfnet2 = result_nfnet2[:5]\nresult_nfnet2","fc646c84":"print('normal')\nprint(result_normal['val'].describe())\nprint('\\nswin384')\nprint(result_swin384['val'].describe())\n\nprint('\\nxcit')\nprint(result_xcit['val'].describe())\n# print('\\nnfnet')\n# print(result_nfnet['val'].describe())\nprint('\\nnfnet2')\nprint(result_nfnet2['val'].describe())","e5f45d51":"# from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n# import seaborn as sns\n\n# paths = result_normal['name'].tolist()\n# paths.extend(result_xcit['name'].tolist())\n# paths.extend(result_nfnet['name'].tolist())\n\n# for path in paths:\n#     print(path)\n#     e_path = glob(f'{path}\/events*')[0]\n#     event_acc = EventAccumulator(path + '\/', size_guidance={'scalars': 0})\n#     event_acc.Reload()\n\n#     scalars = {}\n#     for tag in event_acc.Tags()['scalars']:\n#         events = event_acc.Scalars(tag)\n#         scalars[tag] = [event.value for event in events]\n#     sns.set()\n\n#     plt.figure(figsize=(8, 3))\n#     plt.subplot(1, 2, 1)\n#     plt.plot(range(len(scalars['lr-AdamW'])), scalars['lr-AdamW'])\n#     plt.xlabel('epoch')\n#     plt.ylabel('lr')\n#     plt.title('adamw lr')\n\n#     plt.subplot(1, 2, 2)\n#     plt.plot(range(len(scalars['train_loss'])), scalars['train_loss'], label='train_loss')\n#     plt.plot(range(len(scalars['val_loss'])), scalars['val_loss'], label='val_loss')\n#     plt.legend()\n#     plt.ylabel('rmse')\n#     plt.xlabel('epoch')\n#     plt.title('train\/val rmse')\n#     plt.show()\n#     print('best_val_loss', min(scalars['val_loss']))\n#     print()","610bdf4a":"def get_transform_value(train, field, width, height):\n    classes = np.array(field['classes'])\n    animal_indexes = np.where((classes == 16) | (classes == 17))[0]\n    minimum_crop = min(width, height)\n    if len(animal_indexes) == 0:\n        return {'cx': width \/ 2, 'cy': height \/ 2, 'ms': minimum_crop, 'msx': minimum_crop, 'msy': minimum_crop}\n\n    box = np.array(field['boxes']).astype(np.int)\n    box = box[animal_indexes]\n\n    width_x = (box[:, 2].max() - box[:, 0].min())\n    width_y = (box[:, 3].max() - box[:, 1].min())\n \n    center_x = (box[:, 0].min()) + (width_x \/ 2)\n    center_y = (box[:, 1].min()) + (width_y \/ 2)\n\n    min_size = max(width_x, width_y)\n    min_size = np.clip(min_size, 100, minimum_crop)\n    min_size_x = np.clip(width_x, 100, width)\n    min_size_y = np.clip(width_y, 100, height)\n\n    return {'cx': center_x, 'cy': center_y, 'ms': min_size, 'msx': min_size_x, 'msy': min_size_y}\n","7adc6bf2":"def str_to_list(colum_str):\n    string = colum_str.split(\"array(\")[1]\n    string = string.split(']')[0:-1]\n    string = ']'.join(string)\n    string += ']'\n    return ast.literal_eval(string)\n    \ndef fields_to_dict(field_text):\n    field = field_text\n    field = field[1:-1]\n\n    field = field.split('),')\n\n    boxes = field[1]\n    score = field[0]\n    classes = field[2]\n\n    boxes = str_to_list(boxes)\n    score = str_to_list(score)\n    classes = str_to_list(classes)\n    return {'boxes': boxes, 'score': score, 'classes': classes}\n\ndef set_boxes_info(df):\n    df['cx'] = 0\n    df['cy'] = 0\n    df['ms'] = 0\n    df['msx'] = 0\n    df['msy'] = 0\n    for index in df.index:\n        min_ms = min([df.loc[index, 'width'], df.loc[index, 'height']])\n        result = get_transform_value(False,df.loc[index, 'fields'], df.loc[index, 'width'], df.loc[index, 'height'])\n        cx = result['cx']\n        cy = result['cy']\n        ms = result['ms']\n        msx = result['msx']\n        msy = result['msy']\n\n        df.loc[index, ['cx', 'cy', 'ms', 'msx', 'msy']] = [cx, cy, ms, msx, msy]","50e62417":"# CPU or GPU\n# for colab\nDATA_DIR = '..\/input\/petfinder-pawpularity-score'\n\n# # Use only GPU\nIMG_PATH = DATA_DIR","7dc3de7a":"test_df=pd.read_csv(f'{DATA_DIR}\/test.csv')\ntest_df['image_path'] = f'{IMG_PATH}\/test\/' + test_df['Id'] + '.jpg'","c1aab32f":"dense_features = [\n    'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n    'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur',\n    'Persons', 'Cats', 'Dogs', 'Animals',\n]","344dd015":"IMAGENET_MEAN = [0.485, 0.456, 0.406]  # RGB\nIMAGENET_STD = [0.229, 0.224, 0.225]  # RGB\n\n\ndef get_default_transforms():\n    transform = {\n        \"train\": T.Compose([\n          # T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2),\n          T.RandomHorizontalFlip(),\n          T.RandomVerticalFlip(),\n          T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n\n          T.ConvertImageDtype(torch.float),\n          T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n        ]),\n        \"val\": T.Compose([\n          T.RandomHorizontalFlip(),\n\n          T.ConvertImageDtype(torch.float),\n          T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n        ]),\n    }\n    return transform","63f06aef":"class PetfinderDataset(Dataset):\n    def __init__(self, df, train=True):\n        self._X = df[\"Id\"].values\n        self._y = None\n        self._train = train\n        self._image_path = df[\"image_path\"].values\n        # self._dense_features = df[dense_features].values\n\n        self._fields = df['fields'].values\n        self._cats = df['Cats'].values\n        self._dogs = df['Dogs'].values\n        self._animalss = df['Animals'].values\n\n        self._width = df['width'].values\n        self._height = df['height'].values\n\n        self._cx = df['cx'].values\n        self._cy = df['cy'].values\n        \n        self._ms = df['ms'].values\n        self._msx = df['msx'].values\n        self._msy = df['msy'].values\n\n        if \"Pawpularity\" in df.keys():\n            self._y = df[\"Pawpularity\"].values\n\n    def __len__(self):\n        return len(self._X)\n\n    def __getitem__(self, idx):\n        image_path = self._image_path[idx]\n        image = read_image(image_path)\n\n        # Procese of get box\n        field = self._fields[idx]\n        width = self._width[idx]\n        height = self._height[idx]\n\n        image_size = min(width,height)\n\n        cx = self._cx[idx]\n        cy = self._cy[idx]\n        ms = self._ms[idx]\n        # Set msx and msy as big as possible, then crop\n        msx = self._msx[idx]\n        msy = self._msy[idx]\n     \n        if self._train:\n            transfomer = mT.Compose([          \n                T.RandomRotation(degrees=(-45, 45)),\n                T.RandomPerspective(distortion_scale=0.3, p=0.4),\n\n                mT.SpatialCrop(roi_center=[cy, cx], roi_size=[msy, msx]),\n                mT.Resize([IMG_SIZE, IMG_SIZE]),\n            ])\n        else:            \n            # TTA\n            if self._animalss[idx] > 1:\n                if torch.rand(1)[0] > 0.65:\n                    # Add columns by each crop\n                    classes = np.array(field['classes'])\n                    animal_indexes = np.where((classes == 16) | (classes == 17))[0]\n                    \n                    box = np.array(field['boxes']).astype(np.int)\n                    box = box[animal_indexes]\n                    animal_i = torch.randint(0, len(box), (1,))[0]\n                    \n                    msx = (box[animal_i, 2] - box[animal_i, 0])\n                    msy = (box[animal_i, 3] - box[animal_i, 1])\n\n                    cx = (box[animal_i, 0]) + (msx \/ 2)\n                    cy = (box[animal_i, 1]) + (msy \/ 2)\n                    \n                    ms = max(msx, msy)\n            \n            swift_rand = torch.rand(3)\n            \n            ms += (ms * (swift_rand[0] - 0.3) * 0.1)\n            msx += (msx * (swift_rand[0] - 0.3) * 0.1)\n            msy += (msy * (swift_rand[0] - 0.3) * 0.1)\n            \n            cx += (msx * (swift_rand[1] - 0.5) * 0.1)\n            cy += (msy * (swift_rand[2] - 0.5) * 0.1)\n            cx = np.clip(cx, 0, width)\n            cy = np.clip(cy, 0, height)\n            \n            ms = np.clip(ms, 100, image_size)\n            msx = ms\n            msy = ms  \n\n            transfomer = mT.Compose([\n                mT.SpatialCrop(roi_center=[cy, cx], roi_size=[msy, msx]),\n                mT.Resize([IMG_SIZE, IMG_SIZE]),\n            ])\n          \n        try:\n            image = transfomer(image)\n        except:\n            print(\"error\")\n            transfomer = mT.Compose([\n                mT.RandSpatialCrop(ms*0.99),\n                mT.Resize([IMG_SIZE, IMG_SIZE]),\n            ])\n            image = transfomer(image)\n\n        if self._y is not None:\n            label = self._y[idx]\n            return {\"image\": image, \"label\": label}\n        return {\"image\": image}\n\nclass PetfinderDataModule(LightningDataModule):\n    def __init__(\n        self,\n        train_df,\n        val_df,\n    ):\n        super().__init__()\n        self._train_df = train_df\n        self._val_df = val_df\n\n    def __create_dataset(self, train=True):\n        return (\n          PetfinderDataset(self._train_df, train)  if train else PetfinderDataset(self._val_df, train)\n        )\n\n    def train_dataloader(self):\n        dataset = self.__create_dataset(True)\n        return DataLoader(dataset, **TRN_LOADER)\n\n    def val_dataloader(self):\n        dataset = self.__create_dataset(False)\n        return DataLoader(dataset, **VAL_LOADER)\n","accc202d":"class PawpularModel(pl.LightningModule):\n    def __init__(self, model_name, model_type):\n        super().__init__()\n        self.__build_model(model_name, model_type)\n        self._criterion = eval(LOSS)()\n        self.transform = get_default_transforms()\n\n    def __build_model(self, model_name, model_type):\n        if model_type=='nb019':\n            self.base = timm.create_model(\n                model_name, pretrained=False, num_classes=0, in_chans=3\n            )\n            num_features = self.base.num_features\n            self.fc = nn.Sequential(\n                nn.Dropout(0.1),\n                nn.LazyLinear(1)\n            )\n        elif model_type=='onlyresize':\n            self.base = timm.create_model(\n                model_name, pretrained=False, num_classes=0, in_chans=3\n            )\n            num_features = self.base.num_features\n            self.fc = nn.Sequential(\n                nn.Dropout(0.1),\n                nn.LazyLinear(1)\n            )\n            \n        elif model_type=='ensemble':\n            self.base = timm.create_model(\n                model_name, pretrained=False, num_classes=0, in_chans=3\n            )\n            num_features = self.base.num_features\n            self.fc = nn.Sequential(\n                nn.Dropout(0.1),\n                nn.Linear(num_features, 128),\n                nn.Softmax(128, 10),\n            )\n\n    def forward(self, image):\n        f = self.base(image)\n        out = self.fc(f)\n        return out\n    def training_step(self, batch, batch_idx):\n        loss, pred, labels = self.__share_step(batch, 'train')\n        return {'loss': loss, 'pred': pred, 'labels': labels}\n        \n    def validation_step(self, batch, batch_idx):\n        loss, pred, labels = self.__share_step(batch, 'val')\n        return {'pred': pred, 'labels': labels}\n    \n    def test_step(self, batch, batch_idx):\n        # For transform, set mode val\n        loss, pred, labels = self.__share_step(batch, 'val')\n        return {'pred': pred, 'labels': labels}\n    \n    def __share_step(self, batch, mode):\n        images, labels = batch['image'], batch['label']\n        labels = labels \/ 100.0\n        images = self.transform[mode](images)\n        \n        if torch.rand(1)[0] < 0.5 and mode == 'train':\n            mix_images, target_a, target_b, lam = mixup(images, labels, alpha=0.5)\n            logits = self.forward(mix_images).squeeze(1)\n            loss = self._criterion(logits, target_a) * lam + \\\n                (1 - lam) * self._criterion(logits, target_b)\n        else:\n            logits = self.forward(images).squeeze(1)\n            loss = self._criterion(logits, labels)\n        \n        pred = logits.sigmoid().detach().cpu() * 100.\n        labels = labels.detach().cpu() * 100.\n        return loss, pred, labels\n        \n    def training_epoch_end(self, outputs):\n        self.__share_epoch_end(outputs, 'train')\n\n    def validation_epoch_end(self, outputs):\n        self.__share_epoch_end(outputs, 'val')\n        \n    def test_epoch_end(self, outputs):\n        result = self.__share_epoch_end(outputs, 'test')\n        \n    def __share_epoch_end(self, outputs, mode):\n        preds = []\n        labels = []\n        for out in outputs:\n            pred, label = out['pred'], out['labels']\n            preds.append(pred)\n            labels.append(label)\n        preds = torch.cat(preds)\n        labels = torch.cat(labels)\n        metrics = torch.sqrt(((labels - preds) ** 2).mean())\n        self.log(f'{mode}_loss', metrics)\n        \n    def predict_step(self, batch, batch_idx):\n        # For transform, set mode val\n        images = self.transform['val'](batch['image'])\n        logits = self.forward(images)  .squeeze(1)   \n        pred = logits.sigmoid().detach().cpu() * 100.\n        return {'pred': pred}\n    def on_predict_epoch_end(self, outputs):\n        preds = []\n        for out in outputs[0]:\n            preds.append(out['pred'])\n        result = torch.cat(preds)\n\n    def configure_optimizers(self):\n        optimizer = eval(OPT_NAME)(\n          self.parameters(), **OPT_PARAMS\n        )\n        scheduler = eval(SCH_NAME)(\n          optimizer,\n          **SCH_PARAMS\n        )\n        return [optimizer], [scheduler]\n\n    ","f4cfcf5d":"trainer = pl.Trainer(\n  max_epochs= EPOCHS,\n  callbacks=[],\n  **TRAINER\n)","e90a2ca1":"def predict_paths(model_name, model_type, paths, df):\n    versions_paths = []\n    versions_preds = []\n    versions_clf_preds = []\n    \n    datamodule = PetfinderDataModule(df, df)\n    val_dl = datamodule.val_dataloader()\n    \n    features = df[dense_features].values\n    \n    model = PawpularModel(model_name, model_type)\n    model = model.cuda().eval()\n    \n    for path in paths:\n        model.load_state_dict(torch.load(f'{path}\/checkpoints\/best_loss.ckpt')['state_dict'])\n        model = model.cuda().eval()\n        \n        s_preds = trainer.predict(model, val_dl)\n        preds = np.array([])\n        for row in s_preds:\n            preds = np.concatenate((preds, row['pred']), axis=0)\n        train_features = np.concatenate((preds[:, np.newaxis], features), axis=1)\n\n        # For no svr\n        clf_preds = [0]\n    \n        versions_paths.append(path)\n        versions_preds.append(preds)\n        versions_clf_preds.append(clf_preds)\n    return {'paths': versions_paths, 'preds': np.float32(versions_preds), 'clf_preds': np.float32(versions_clf_preds)}","9f89d573":"def normal_predict(df):\n    model_name = 'swin_large_patch4_window7_224'\n    model_type = 'nb019'\n    \n    paths = result_normal['name']\n\n    IMG_SIZE = 224\n    BATCH_SIZE = 32\n    \n    return predict_paths(model_name, model_type, paths, df)","86233c22":"def swin384_predict(df):\n    model_name = 'swin_large_patch4_window12_384_in22k'\n    model_type = 'nb019'\n    \n    paths = result_swin384['name']\n\n    IMG_SIZE = 384\n    BATCH_SIZE = 32\n    \n    return predict_paths(model_name, model_type, paths, df)","0744b8fa":"def xcit_predict(df):\n    model_name = 'xcit_large_24_p8_224_dist'\n    model_type = 'nb019'\n    \n    paths = result_xcit['name']\n    \n    IMG_SIZE = 224\n    BATCH_SIZE = 32\n    return predict_paths(model_name, model_type, paths, df)","1c8ef694":"def nfnet2_predict(df):\n    model_name = 'dm_nfnet_f2'\n    model_type = 'nb019'\n    \n    paths = result_nfnet2['name']\n    \n    IMG_SIZE = 352\n    BATCH_SIZE = 16\n    return predict_paths(model_name, model_type, paths, df)","71a0c8b7":"def multiple_ratio(preds):\n    y_pred = np.zeros(len(preds[0]))\n    best_ratios = np.array([1.0\/len(preds) for i in range(len(preds))])\n\n    for i, ratio in enumerate(best_ratios):\n        y_pred += (preds[i] * ratio)\n    return y_pred","70a2d3e0":"compound_coef = 8\nforce_input_size = None  # set None to use default size\n# img_path = train_df.loc[14, 'image_path']\n\n# replace this part with your project's anchor config\nanchor_ratios = [(1.0, 1.0), (1.4, 0.7), (0.7, 1.4)]\nanchor_scales = [2 ** 0, 2 ** (1.0 \/ 3.0), 2 ** (2.0 \/ 3.0)]\n\nthreshold = 0.2\niou_threshold = 0.2\n\nuse_cuda = True\nuse_float16 = False\ncudnn.fastest = True\ncudnn.benchmark = True\n\nobj_list = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n            'fire hydrant', '', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n            'cow', 'elephant', 'bear', 'zebra', 'giraffe', '', 'backpack', 'umbrella', '', '', 'handbag', 'tie',\n            'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n            'skateboard', 'surfboard', 'tennis racket', 'bottle', '', 'wine glass', 'cup', 'fork', 'knife', 'spoon',\n            'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut',\n            'cake', 'chair', 'couch', 'potted plant', 'bed', '', 'dining table', '', '', 'toilet', '', 'tv',\n            'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n            'refrigerator', '', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n            'toothbrush']\n\n\ncolor_list = standard_to_bgr(STANDARD_COLORS)\n# tf bilinear interpolation is different from any other's, just make do\ninput_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536, 1536]\ninput_size = input_sizes[compound_coef] if force_input_size is None else force_input_size\n\ndetect_model = EfficientDetBackbone(compound_coef=compound_coef, num_classes=len(obj_list),\n                             ratios=anchor_ratios, scales=anchor_scales)\ndetect_model.load_state_dict(torch.load(f'..\/input\/yetanotherefficientdetpytorch\/weights\/efficientdet-d{compound_coef}.pth', map_location='cuda'))\ndetect_model.requires_grad_(False)\ndetect_model.eval()\n\nif use_cuda:\n    detect_model = detect_model.cuda()\nif use_float16:\n    detect_model = detect_model.half()\n\nregressBoxes = BBoxTransform()\nclipBoxes = ClipBoxes()","6936f266":"def add_hwf(df):\n    # Object detec to train_confirm\n    df['width'] = 0\n    df['height'] = 0\n    df['fields'] = ''\n    \n    df['Persons'] = 0\n    df['Cats'] = 0\n    df['Dogs'] = 0\n    df['Animals'] = 0\n\n    for index in tqdm(df.index):\n        img_path = df.loc[index, 'image_path']\n        ori_imgs, framed_imgs, framed_metas = preprocess(img_path, max_size=input_size)\n        df.loc[index, 'width'] = framed_metas[0][2]\n        df.loc[index, 'height'] = framed_metas[0][3]\n    \n\n        if use_cuda:\n            x = torch.stack([torch.from_numpy(fi).cuda() for fi in framed_imgs], 0)\n        else:\n            x = torch.stack([torch.from_numpy(fi) for fi in framed_imgs], 0)\n\n        x = x.to(torch.float32 if not use_float16 else torch.float16).permute(0, 3, 1, 2)\n\n        features, regression, classification, anchors = detect_model(x)\n\n        out = postprocess(x,\n            anchors, regression, classification,\n            regressBoxes, clipBoxes,\n            threshold, iou_threshold)\n        out = invert_affine(framed_metas, out)[0]\n        out['boxes'] = out.pop('rois')\n        out['classes'] = out.pop('class_ids')\n        out['scores'] = out['scores']\n\n        df.loc[index, 'fields'] = str(out)\n\n        df.loc[index, 'Persons'] = out['classes'].tolist().count(0)\n        df.loc[index, 'Cats'] = out['classes'].tolist().count(16)\n        df.loc[index, 'Dogs'] = out['classes'].tolist().count(17)\n        df.loc[index, 'Animals'] = df.loc[index, 'Cats'] + df.loc[index, 'Dogs']","fd67b67e":"# train_df=pd.read_csv(f'..\/input\/petfinder2-detected-info\/train.csv')\n# train_df['image_path'] = f'{IMG_PATH}\/train\/' + train_df['Id'] + '.jpg'\n# train_df['fields'] = train_df['fields'].apply(fields_to_dict)\n# set_boxes_info(train_df)\n# train_df['Animals'] = train_df['Cats'] + train_df['Dogs']\n\n# confirm_index = []\n# with open('..\/input\/val-index\/val_index.pkl', 'rb') as web:\n#     confirm_index = pickle.load(web)\n# train_confirm = train_df.iloc[confirm_index]\n# train_df = train_df.drop(confirm_index)","337d9cc8":"# train_confirm['Pawpularity'].describe()","371766ad":"# %%time\n# normal_result = normal_predict(train_confirm)","d2e9cd32":"# for i in range(len(normal_result['preds'])):\n#     apply_index = [False for i in range(len(normal_result['preds']))]\n#     apply_index[i] = True\n\n#     preds = multiple_ratio(normal_result['preds'][apply_index])\n#     # preds = multiple_ratio(normal_preds_array)\n#     train_confirm['preds'] = preds\n#     print(normal_result['paths'][i])\n#     print(preds.mean(), preds.std(), np.sqrt(mean_squared_error(train_confirm['Pawpularity'], preds)))\n\n#     # Cats CV\n#     specific_data = train_confirm[(train_confirm['Cats']) > 0]\n#     print(len(specific_data))\n#     print(np.sqrt(mean_squared_error(specific_data['Pawpularity'], specific_data['preds'])))\n\n#     # Dogs CV\n#     specific_data = train_confirm[(train_confirm['Dogs']) > 0]\n#     print(len(specific_data))\n#     print(np.sqrt(mean_squared_error(specific_data['Pawpularity'], specific_data['preds'])))","9f7ea798":"# preds = multiple_ratio(normal_result['preds'])\n# train_confirm['preds'] = preds","7a2be8df":"# print(preds.mean(), preds.std(), np.sqrt(mean_squared_error(train_confirm['Pawpularity'], preds)))\n# # Cats CV\n# specific_data = train_confirm[(train_confirm['Cats']) > 0]\n# print(\"Cats\", len(specific_data), np.sqrt(mean_squared_error(specific_data['Pawpularity'], specific_data['preds'])))\n# # Dogs CV\n# specific_data = train_confirm[(train_confirm['Dogs']) > 0]\n# print(\"Dogs\", len(specific_data), np.sqrt(mean_squared_error(specific_data['Pawpularity'], specific_data['preds'])))\n\n# print(\"Animals\")\n# for i in range(8):\n#     specific_data = train_confirm[train_confirm['Animals'] == i]\n#     print(i, len(specific_data), np.sqrt(mean_squared_error(specific_data['Pawpularity'], specific_data['preds'])))","c4f601b3":"# one_animal = train_confirm[train_confirm['Animals']==1]\n\n# specific_data = one_animal[one_animal['width'] > one_animal['height']]\n# print(\"width > height\", len(specific_data), np.sqrt(mean_squared_error(specific_data['Pawpularity'], specific_data['preds'])))\n# print()\n\n# specific_data = one_animal[one_animal['width'] < one_animal['height']]\n# print(\"width < height\", len(specific_data), np.sqrt(mean_squared_error(specific_data['Pawpularity'], specific_data['preds'])))\n# print()\n\n# specific_data = one_animal[one_animal['width'] > (one_animal['height'] * 1.3)]\n# print(\"width> height * 1.3 \", len(specific_data), np.sqrt(mean_squared_error(specific_data['Pawpularity'], specific_data['preds'])))\n# print()\n# specific_data = one_animal[(one_animal['width'] * 1.3) < one_animal['height']]\n# print(\"width * 1.3 < height\", len(specific_data), np.sqrt(mean_squared_error(specific_data['Pawpularity'], specific_data['preds'])))\n# print()\n\n# specific_data = one_animal[one_animal['width'] > (one_animal['height'] * 1.5)]\n# print(\"width> height * 1.5 \", len(specific_data), np.sqrt(mean_squared_error(specific_data['Pawpularity'], specific_data['preds'])))\n# print()\n# specific_data = one_animal[(one_animal['width'] * 1.5) < one_animal['height']]\n# print(\"width * 1.5 < height\", len(specific_data), np.sqrt(mean_squared_error(specific_data['Pawpularity'], specific_data['preds'])))\n# print()","b7669730":"# ##time\n# xcit_result = xcit_predict(train_confirm) \n# xcit_preds = multiple_ratio(xcit_result['preds'])\n\n# print(xcit_preds.mean(), xcit_preds.std(), np.sqrt(mean_squared_error(train_confirm['Pawpularity'], xcit_preds)))","b1ba8472":"# ##time\n# nfnet2_result = nfnet2_predict(train_confirm) \n# nfnet2_preds = multiple_ratio(nfnet2_result['preds'])\n\n# print(nfnet_preds.mean(), nfnet_preds.std(), np.sqrt(mean_squared_error(train_confirm['Pawpularity'], nfnet2_preds)))","be230687":"# train_confirm.loc[:, 'preds'] = (train_confirm.loc[:, 'preds'] * 0.5) + (xcit_preds * 0.3) + (nfnet2_preds * 0.2)\n# preds = train_confirm['preds']","5a2c275e":"# print(preds.mean(), preds.std(), np.sqrt(mean_squared_error(train_confirm['Pawpularity'], preds)))\n# # Cats CV\n# specific_data = train_confirm[(train_confirm['Cats']) > 0]\n# print(\"Cats\", len(specific_data), np.sqrt(mean_squared_error(specific_data['Pawpularity'], specific_data['preds'])))\n# # Dogs CV\n# specific_data = train_confirm[(train_confirm['Dogs']) > 0]\n# print(\"Dogs\", len(specific_data), np.sqrt(mean_squared_error(specific_data['Pawpularity'], specific_data['preds'])))\n\n# print(\"Animals\")\n# for i in range(8):\n#     specific_data = train_confirm[train_confirm['Animals'] == i]\n#     print(i, len(specific_data), np.sqrt(mean_squared_error(specific_data['Pawpularity'], specific_data['preds'])))","bec5e93c":"# Object detect\nadd_hwf(test_df)\n# I mustn't connect to below code","4a8594f3":"gc.collect()\ntorch.cuda.empty_cache()","58dda8af":"test_df['fields'] = test_df['fields'].apply(fields_to_dict)\nset_boxes_info(test_df)","d33b5833":"# # Normal no svr\nIMG_SIZE = 224\nnormal_result = normal_predict(test_df)\nnormal_preds = multiple_ratio(normal_result['preds'])","c5343d35":"gc.collect()\ntorch.cuda.empty_cache()","cf9e4b17":"# # Normal no svr\nIMG_SIZE = 384\nswin384_result = swin384_predict(test_df)\nswin384_preds = multiple_ratio(swin384_result['preds'])","74d90436":"gc.collect()\ntorch.cuda.empty_cache()","b9effffb":"IMG_SIZE = 224\nxcit_result = xcit_predict(test_df) \nxcit_preds = multiple_ratio(xcit_result['preds'])","d785b44d":"gc.collect()\ntorch.cuda.empty_cache()","3d5b7f27":"IMG_SIZE = 352\nnfnet2_result = nfnet2_predict(test_df) \nnfnet2_preds = multiple_ratio(nfnet2_result['preds'])","0b6efeb7":"test_df.loc[:, 'preds'] = (normal_preds * 0.4) + (swin384_preds * 0.2) + (xcit_preds * 0.2) +  (nfnet2_preds * 0.2)","8cc5cf99":"test_df['Pawpularity'] = test_df['preds']\ntest_df = test_df[[\"Id\", \"Pawpularity\"]]\ntest_df.to_csv(\"submission.csv\", index=False)","878330ed":"test_df","09ba9379":"# Submit","ca827237":"# CV Using Confirm","bc652178":"# Define Model"}}