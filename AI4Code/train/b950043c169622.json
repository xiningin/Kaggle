{"cell_type":{"7b1a2063":"code","2110c084":"code","910003db":"code","602f5c3f":"code","c9e59aec":"code","2c89f7bd":"code","ea226697":"code","bec0ad12":"code","6f93e076":"code","51979391":"code","ee49c500":"code","e3c7dd73":"code","e02b9b2b":"code","ef9a0e1f":"code","f1e05c6a":"code","ab5b7274":"code","65ea9ca8":"code","b4056738":"code","f6a124c4":"code","849912d6":"code","a13f9312":"code","78c78544":"code","0e5ddf4a":"code","bf267466":"code","3dae85d9":"code","52b95f86":"code","4c131e6f":"code","a89507ec":"code","63165346":"code","9f8d0f0e":"code","3392f86c":"code","1910c40b":"code","70847d8e":"code","0784621d":"code","e369cee9":"code","8514acd3":"code","f4d0ee49":"code","a19c289a":"code","7ef1d801":"code","13bcfafe":"code","cda7f063":"code","a688a119":"code","ff991ab3":"code","deff8643":"code","5bec71be":"code","c85ff8f4":"markdown","6f1d3498":"markdown","54a928eb":"markdown","93696f07":"markdown","6c7ea700":"markdown","302bb230":"markdown","138e5cff":"markdown","4f809798":"markdown","6f43133a":"markdown","d0dfdfb2":"markdown","3c76e112":"markdown"},"source":{"7b1a2063":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","2110c084":"bangla_train = pd.read_csv(\"..\/input\/banglamct7-bangla-multiclass-text-dataset-7-tags\/BanglaMCT7\/train.csv\")\nbangla_test = pd.read_csv(\"..\/input\/banglamct7-bangla-multiclass-text-dataset-7-tags\/BanglaMCT7\/test.csv\")","910003db":"bangla_train.head(20)","602f5c3f":"bangla_train.tail(-20)","c9e59aec":"print(bangla_train.cleanText[100])","2c89f7bd":"bangla_test.head(20)","ea226697":"bangla_test.tail(-20)","bec0ad12":"bangla_train.info()","6f93e076":"bangla_train.describe()","51979391":"bangla_test.info()","ee49c500":"bangla_test.describe()","e3c7dd73":"bangla_train.isnull().sum()","e02b9b2b":"bangla_test.isnull().sum()","ef9a0e1f":"bangla_train['category'].value_counts()","f1e05c6a":"plt.figure(figsize=(20,10))\nplot = bangla_train[0:20000]\nsns.countplot(plot.category)","ab5b7274":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\nbangla_train.category = encoder.fit_transform(bangla_train['category'])\nbangla_test.category = encoder.fit_transform(bangla_test['category'])","65ea9ca8":"bangla_train.head(10)","b4056738":"max(bangla_train.category)","f6a124c4":"bangla_test.head(10)","849912d6":"bangla_train_x=bangla_train.cleanText[0:20000]\nbangla_test_x=bangla_test.cleanText[0:20000]\nbangla_train_y=bangla_train.category[0:20000]\nbangla_test_y=bangla_test.category[0:20000]","a13f9312":"bangla_train_x.head()","78c78544":"bangla_train_y.head()","0e5ddf4a":"print(\"type of text_train: {}\".format(type(bangla_train_x)))\nprint(\"length of text_train: {}\".format(len(bangla_train_x)))\nprint(\"text_train[1]:\\n{}\".format(bangla_train_x[2]))","bf267466":"#count vectorizer does tokenization and vocabulary building\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer().fit(bangla_train_x)\n#built-in transform method transforms the vector into bag-of-words-representation\nbangla_train_X = vect.transform(bangla_train_x)\nprint(\"X_train:\\n{}\".format(repr(bangla_train_X)))","3dae85d9":"print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\nprint(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))","52b95f86":"print(\"Dense representation of bag_of_words:\\n{}\".format(\nbangla_train_X.toarray()))","4c131e6f":"#print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n#print(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))","a89507ec":"feature_names = vect.get_feature_names()\nprint(\"Number of features: {}\".format(len(feature_names)))\nprint(\"First 20 features:\\n{}\".format(feature_names[:20]))\nprint(\"Features 20010 to 20030:\\n{}\".format(feature_names[10020:10040]))\nprint(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))","63165346":"!pip install scikit-learn  -U\n!pip install delayed","9f8d0f0e":"#import delayed\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nscores = cross_val_score(LogisticRegression(solver='liblinear'), bangla_train_X, bangla_train_y, cv=5)\nprint(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))","3392f86c":"from sklearn.model_selection import GridSearchCV\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 20]}\ngrid = GridSearchCV(LogisticRegression(solver='liblinear'), param_grid, cv=5)\ngrid.fit(bangla_train_X, bangla_train_y)\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\nprint(\"Best parameters: \", grid.best_params_)","1910c40b":"#count vectorizer does tokenization and vocabulary building\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect1 = CountVectorizer().fit(bangla_test_x)\n#built-in transform method transforms the vector into bag-of-words-representation\nbangla_test_X = vect.transform(bangla_test_x)\nprint(\"X_test:\\n{}\".format(repr(bangla_test_X)))","70847d8e":"vect = CountVectorizer(min_df=5).fit(bangla_train_x)\nX_train = vect.transform(bangla_train_x)\nprint(\"X_train with min_df: {}\".format(repr(X_train)))","0784621d":"feature_names = vect.get_feature_names()\nprint(\"First 50 features:\\n{}\".format(feature_names[:50]))\nprint(\"Features 20010 to 20030:\\n{}\".format(feature_names[10010:10030]))\nprint(\"Every 700th feature:\\n{}\".format(feature_names[::700]))","e369cee9":"grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\ngrid.fit(X_train, bangla_train_y)\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))","8514acd3":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\npipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None),\nLogisticRegression())\nparam_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}\ngrid = GridSearchCV(pipe, param_grid, cv=5)\ngrid.fit(bangla_train_x, bangla_train_y)\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))","f4d0ee49":"vectorizer = grid.best_estimator_.named_steps[\"tfidfvectorizer\"]\n# transform the training dataset\nbangla_train_X = vectorizer.transform(bangla_train_x)\n# find maximum value for each of the features over the dataset\nmax_value = bangla_train_X.max(axis=0).toarray().ravel()\nsorted_by_tfidf = max_value.argsort()\n# get feature names\nfeature_names = np.array(vectorizer.get_feature_names())\nprint(\"Features with lowest tfidf:\\n{}\".format(\nfeature_names[sorted_by_tfidf[:40]]))\nprint(\"Features with highest tfidf: \\n{}\".format(\nfeature_names[sorted_by_tfidf[-40:]]))","a19c289a":"sorted_by_idf = np.argsort(vectorizer.idf_)\nprint(\"Features with lowest idf:\\n{}\".format(\nfeature_names[sorted_by_idf[:100]]))","7ef1d801":"#grid.fit(bangla_train_X, bangla_train_y)\n#mglearn.tools.visualize_coefficients(\n#grid.best_estimator_.named_steps[\"logisticregression\"].coef_,\n#feature_names, n_top_features=40)","13bcfafe":"#finding the best setting of n-grams\npipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression())\n# running the grid search takes a long time because of the\n# relatively large grid and the inclusion of trigrams\nparam_grid = {\"logisticregression__C\": [0.001, 0.01, 0.1, 1, 10, 100],\n\"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3),(1, 4),(1, 5)]}\ngrid = GridSearchCV(pipe, param_grid, cv=5)\ngrid.fit(bangla_train_x, bangla_train_y)\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid.best_params_))","cda7f063":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(MultinomialNB(), X_train, bangla_train_y, cv=5)\nprint(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))","a688a119":"from sklearn.tree import DecisionTreeClassifier\nscores = cross_val_score(DecisionTreeClassifier(), X_train, bangla_train_y, cv=5)\nprint(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))","ff991ab3":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom lightgbm import LGBMClassifier","deff8643":"SGD=SGDClassifier()\nscores = cross_val_score(SGD, X_train, bangla_train_y, cv=5)\nprint(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))","5bec71be":"lgbm=LGBMClassifier()\nscores = cross_val_score(lgbm, bangla_train_X, bangla_train_y, cv=5)\nprint(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))","c85ff8f4":"**Now we will tune the regularization parameter 'C' of logistic regression to check if it increases the accuracy.**","6f1d3498":"> Therefore, we can conclude that so far logistic regression provides the best training accuracy of 91% when the data is preprocessed using tf-idf vectorizer and also n_gram modeliing is used along with cross validation to find the best parameters to produce the highest accuracy. It seems that there is an inconsistency in the dimension of test data for which it is getting difficult to see the test accuracy, but I think it is possible to still get the test accuracy if more time is spent on debugging and analysing the dataset. I will try to work on this dtataset more to find the problem. Also, applying deep learning models might give much better predictions.  ","54a928eb":"# Data preprocessing","93696f07":"**n-gram implementation**","6c7ea700":"# Exploratory Data Analysis","302bb230":"**Accuracy slightly improved after applying tf-idf. We need to try more preprocessing to increase the accuracy.Now finding words which are important as found by tf-idf**","138e5cff":"**Let's try to train now using logistic regression**","4f809798":"**Let's compare with other models such as - MUltinomial Naive Bayes, Decision Tree Classifier, Guassian Naive Bayes, SGD Classifier, etc**","6f43133a":"**We will now represent the text as a bag of words**","d0dfdfb2":"**We might get uninformative features still due to choosing wrong tokens. So we will now ue only those tokens which appear on atleast 5 documents, to make sure that they are the correct tokens which is specified by the min_df parameter**","3c76e112":"**Rescaling the data with tf-idf**"}}