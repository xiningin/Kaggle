{"cell_type":{"144c4332":"code","43227829":"code","f4c209b0":"code","23560aa6":"code","198f6c82":"code","3365798d":"code","192c3190":"code","8da3d7a7":"code","5976095d":"code","b736c65b":"code","6562df65":"code","b97b8820":"code","0937a079":"code","91579a58":"code","c84c7dd5":"code","cd17a97e":"code","a3c426cb":"code","aaebd8b5":"code","a3a0548e":"code","c0a339df":"code","6f96f6dc":"code","97961502":"code","a11b85ff":"code","04470ca8":"code","14a8a24a":"code","778261b5":"markdown","a04cd692":"markdown","a2b35536":"markdown","02b49863":"markdown","4950bd92":"markdown","505b0da8":"markdown","96ffd9ff":"markdown","7ef4399c":"markdown","2719c80d":"markdown","7f217cd0":"markdown","3f1e14a0":"markdown","3fae4e5e":"markdown","108d648f":"markdown","1c50ce7e":"markdown","892b5f3e":"markdown","4f05c66c":"markdown","e16dbd8b":"markdown","f7f77403":"markdown","15fa9eb5":"markdown","a27f2b49":"markdown"},"source":{"144c4332":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport matplotlib.pyplot as plt\nimport missingno as msno\n%matplotlib inline  \n\nfrom scipy import stats\nfrom sklearn import preprocessing\nfrom sklearn import feature_selection\nimport warnings\nwarnings.filterwarnings('ignore')\nSEED = 42","43227829":"def concat_df(train_data, test_data):\n    # Returns a concatenated df of training and test set\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n\ndef divide_df(all_data):\n    # Returns divided dfs of training and test set\n    return all_data.loc[:1459], all_data.loc[1460:]\n\ndf_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ny_train = df_train.SalePrice\nid_val = df_train.Id\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf_all = concat_df(df_train, df_test).drop(['SalePrice', 'Id'], axis=1)\n\ndf_train.name = 'Training Set'\ndf_test.name = 'Test Set'\ndf_all.name = 'All Set' \ndfs = [df_train, df_test]","f4c209b0":"df_all.head()","23560aa6":"for df in dfs:\n    print(f'Only features contained missing value in {df.name}')\n    temp = df.isnull().sum()\n    print(temp.loc[temp!=0], '\\n')","198f6c82":"null_features = df_all.isnull().sum()\n\n# For features having smaller than 100 missing values\nnull_100 = df_all.columns[list((null_features < 100) & (null_features != 0))]\nnum = df_all[null_100].select_dtypes(include=np.number).columns\nnon_num = df_all[null_100].select_dtypes(include='object').columns\n# Numerous features --> Fill with their median\ndf_all[num] = df_all[num].apply(lambda x: x.fillna(x.median()))\n# Object features --> Fill with value having the highest frequently in this feature\ndf_all[non_num] = df_all[non_num].apply(lambda x: x.fillna(x.value_counts().index[0]))\n\n\n# For features having larger than 1000 missing values --> I drop them\nnull_1000 = df_all.columns[list(null_features > 1000)]\ndf_all.drop(null_1000, axis=1, inplace=True)\ndf_all.drop(['GarageYrBlt', 'LotFrontage'], axis=1, inplace=True)\n\n\n# For other features having missing values --> Fill na value with \"Null\" \n# GarageCond\ndf_all['GarageCond'] = df_all['GarageCond'].fillna('Null')\n# GarageFinish\ndf_all['GarageFinish'] = df_all['GarageFinish'].fillna('Null')\n# GarageQual\ndf_all['GarageQual'] = df_all['GarageQual'].fillna('Null')\n# GarageType\ndf_all['GarageType'] = df_all['GarageType'].fillna('Null')","3365798d":"df_train, df_test = divide_df(df_all)\ndf_train = pd.concat([df_train, y_train], axis=1)  # Concatenate for analysis\n\n# Checking existing missing value or not\nprint(df_all.isnull().any().sum())","192c3190":"# Using binned technique for \"YearBuilt\", \"YearRemodAdd\" & \"YrSold\"\ndf_all['YearBuilt'] = pd.qcut(df_all['YearBuilt'], 10, duplicates='drop')\ndf_all['YearRemodAdd'] = pd.qcut(df_all['YearRemodAdd'], 10, duplicates='drop')\ndf_all['YrSold'] = pd.qcut(df_all['YrSold'], 10, duplicates='drop')","8da3d7a7":"# Encode categorical features to numeric feature\nfor cate_col in ['YearBuilt', 'YearRemodAdd', 'YrSold']:\n    df_all[cate_col] = preprocessing.LabelEncoder().fit_transform(df_all[cate_col].values)\n    \ndf_train, df_test = divide_df(df_all)","5976095d":"# Total square feet of porch in a house\ndf_all['TotalPorchSF'] = (df_all['OpenPorchSF'] + df_all['3SsnPorch'] +\n                          df_all['EnclosedPorch'] + df_all['ScreenPorch'] + df_all['WoodDeckSF'])\ndf_all['HasGarage'] = df_all['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n# Total number of bathroom\ndf_all['TotalBath'] = (df_all['FullBath'] + (0.5 * df_all['HalfBath']) +\n                       df_all['BsmtFullBath'] + (0.5 * df_all['BsmtHalfBath']))\n# House having the fire place or not\ndf_all['HasFireplace'] = df_all['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n# Total number of bathroom in basement\ndf_all['TotalBsmtbath'] = df_all['BsmtFullBath'] + (0.5 * df_all['BsmtHalfBath'])\n# Total square foot\ndf_all['TotalSF'] = df_all['BsmtFinSF1'] + df_all['BsmtFinSF2'] + df_all['1stFlrSF'] + df_all['2ndFlrSF']","b736c65b":"# These columns are used for generating above new features --> Drop the old features\ndf_all.drop(['OpenPorchSF', '3SsnPorch', 'EnclosedPorch', 'ScreenPorch', 'WoodDeckSF', 'FullBath', 'HalfBath',\n            'BsmtFullBath', 'BsmtHalfBath'], axis=1, inplace=True)","6562df65":"num_features = ['OverallQual', 'GrLivArea', 'TotalSF', 'GarageCars', 'TotalBath', 'GarageArea', 'TotalBsmtSF',\n '1stFlrSF', 'TotRmsAbvGrd', 'MasVnrArea', 'HasFireplace', 'Fireplaces', 'TotalPorchSF', '2ndFlrSF',\n 'LotArea', 'HasGarage', 'TotalBsmtbath', 'BsmtUnfSF', 'YearBuilt', 'YearRemodAdd', 'YrSold']\n\n# Drop the unused numeric columns also\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnum_cols = df_all.select_dtypes(include=numeric_dtypes).columns\ndrop_num = np.setdiff1d(num_cols, num_features)\n\ndf_all.drop(drop_num, axis=1, inplace=True)","b97b8820":"# Normalize skewness feature using Log function\nskew_features = df_all[num_features].apply(lambda x: stats.skew(x)).sort_values(ascending=False)\nskew_features = skew_features[abs(skew_features) > 0.5]\nprint(skew_features) \n\n# Apply Box cox for skewness > 0.75\nfor feat in skew_features.index:\n    df_all[feat] = np.log1p(df_all[feat])\n\ndf_train, df_test = divide_df(df_all)","0937a079":"df_train[num_features].head()","91579a58":"# \"Electrical\" features\ndf_train['Electrical'].loc[df_train['Electrical']=='Mix'] = 'SBrkr'\n# \"Exterior2nd\" features\ndf_train['Exterior2nd'].loc[df_train['Exterior2nd']=='Other'] = 'VinylSd'\n# \"Heating\" features\ndf_train['Heating'].loc[df_train['Heating']=='OthW'] = 'GasA'\ndf_train['Heating'].loc[df_train['Heating']=='Floor'] = 'GasA'\n# \"HouseStyle\" features\ndf_train['HouseStyle'].loc[df_train['HouseStyle']=='2.5Fin'] = '1.5Fin'","c84c7dd5":"cate_features = ['BldgType', 'BsmtExposure', 'BsmtFinType1', 'BsmtQual', 'CentralAir', 'Condition1', 'Electrical',\n 'ExterCond', 'ExterQual', 'Exterior2nd', 'Functional', 'GarageCond', 'GarageType', 'Heating', 'HouseStyle',\n 'KitchenQual', 'LandContour', 'LandSlope', 'LotShape', 'Neighborhood', 'PavedDrive', 'RoofStyle',\n 'SaleCondition', 'SaleType', 'Street', 'YearBuilt', 'YearRemodAdd', 'YrSold']\n\n# Drop the unused categorical columns by choosing the only set of columns above\ncols = df_train.select_dtypes(include=['object', 'category']).columns\n# Choose features only in \"cols\" but not in \"cate_features\"\ndrop_cate = np.setdiff1d(cols, cate_features)\n\ndf_train.drop(drop_cate, axis=1, inplace=True)\ndf_test.drop(drop_cate, axis=1, inplace=True)","cd17a97e":"print(df_train.shape, df_test.shape)","a3c426cb":"# Transform categorical feature to dummies features\nencoded_features = list()\n\nfor df in [df_train, df_test]:\n    for feature in cate_features:\n        # Change to array after encoding b.c want to add columns when change back to df\n        encoded_feat = preprocessing.OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n        # \"n\": Number of unique value in each feature\n        n = df[feature].nunique()\n        # \"feature_uniqueVal\" are the col's names in df after One-hot encoding\n        cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n        \n        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n        encoded_df.index = df.index\n        encoded_features.append(encoded_df)\n        \ndf_train = pd.concat([df_train, *encoded_features[:len(cate_features)]], axis=1)\ndf_test = pd.concat([df_test, *encoded_features[len(cate_features):]], axis=1)","aaebd8b5":"print(df_train.shape, df_test.shape)","a3a0548e":"# Drop original category features\ndf_train.drop(cate_features, axis=1, inplace=True)\ndf_test.drop(cate_features, axis=1, inplace=True)\n\ndf_all = concat_df(df_train, df_test)","c0a339df":"print(df_train.shape, df_test.shape)","6f96f6dc":"from sklearn.model_selection import KFold # for repeated K-fold cross validation\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_val_score # score evaluation","97961502":"# Repeated K-fold cross validation\nkfolds = KFold(n_splits=10, shuffle=True, random_state=SEED)\n\n# Return root mean square error applied cross validation (Used for training prediction)\ndef evaluate_model_cv(model, X, y):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)","a11b85ff":"# Base model\nxgboost = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n                       max_depth=3, min_child_weight=0,\n                       gamma=0, subsample=0.7,\n                       colsample_bytree=0.7, verbosity = 0,\n                       objective='reg:squarederror', nthread=-1,\n                       scale_pos_weight=1, seed=SEED, reg_alpha=0.00006)","04470ca8":"# Training model & find root mean square error (With cross validation technqiue)\nxgboost = xgboost.fit(np.array(df_train), np.array(y_train))\nprint('Finish training')\ncv_rmse_result = evaluate_model_cv(xgboost, np.array(df_train), np.array(y_train))\nprint(f'xgboost\\'s rmse (apply cv) after training: {np.mean(cv_rmse_result)}\\n')","14a8a24a":"# Testing ID\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest_id = test['Id']\n\n# When normalize the target\nsubmit = pd.concat((test_id, \n                    pd.Series(xgboost.predict(np.array(df_test)), name='SalePrice')), axis=1)\nsubmit.to_csv('Submission.csv', index=False)","778261b5":"#### Adding some important features","a04cd692":"#### Create the submission","a2b35536":"#### Encode the categorical features by using One-hot encoding technique","02b49863":"<a name='4'><\/a>\n# 4. Modeling ","4950bd92":"<a name='1'><\/a>\n# 1. Loading data","505b0da8":"#### Using \"Bin\" technique for all features having value representing \"year\" & encode them by label encoding technique","96ffd9ff":"<a name='3.2'><\/a>\n## 3.2 Categorical features","7ef4399c":"<a name='2'><\/a>\n# 2. Missing value\nI divide the set of missing value into 3 types: \n- (1) feature having below 100 missing values\n- (2) Feature having more than 1000 missing values\n- (3) The other missing value features","2719c80d":"#### Training the model","7f217cd0":"#### Initialize the xgboost model","3f1e14a0":"#### Choosing the appropriate categorical features","3fae4e5e":"## Kernel outline\n\n15\/8\/2021\n* [**1. Loading data**](#1)\n* [**2. Missing value**](#2)\n* [**3. Feature engineering**](#3)\n    * [3.1 Numeric features](#3.1)\n    * [3.2 Categorical features](#3.2)\n* [**4. Modeling**](#4)","108d648f":"<a name='3.1'><\/a>\n## 3.1 Numeric features","1c50ce7e":"We got nearly 0.13185 prediction score on the leaderboard, rank in the top 10% competitor, I think this score is not a bad start\n\nIf you like this notebook, please give it an upvote. Thank you!","892b5f3e":"# Quick start into house price competition\nThis kernel is created for beginners who want to have a quick journey through the a whole house price prediction project. It contains loading data step, dealing with missing values, preprocess both categorical and numeric features for training the model, and modeling steps also. \n\nBecause predicting the house's price is the regression problem, therefore there're many appropriate and powerful regression models can be used in this case. And after going through some of base model, the result shows that xgboost regression model might be more suitable for this problem. Eventhough it take some time to train the model, but this model will give us back the satisfied result for a quick start model. \n\nI encourage you to fork this kernel, play with the code and get an overview idea to jumping into this competitons. Good luck!\n\nIf you like this kernel, please give it an upvote. Thank you!","4f05c66c":"#### Some features having some values exist in training dataset but not in testing dataset --> We'll fix it","e16dbd8b":"#### Choosing numeric feature and normalize highly skewed features","f7f77403":"#### Drop original category features, we only use one-hot features to train the model","15fa9eb5":"## Model performance\nThe kernel results in nearly 0.13185 prediction score on the leaderboard, rank in the top 10% competitors","a27f2b49":"<a name='3'><\/a>\n# 3. Feature engineering"}}