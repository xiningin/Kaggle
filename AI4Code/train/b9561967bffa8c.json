{"cell_type":{"f3760012":"code","387dbc05":"code","50319d3a":"code","97a6f30e":"code","67dbec45":"code","625fe419":"code","33648567":"code","33263b20":"code","4273eb06":"code","4a82a4b7":"code","adf03a21":"code","ef20e0f1":"code","50983e30":"code","47fa3f14":"code","53537d4c":"code","838ea00b":"code","693bc47a":"code","ef58eaf5":"code","e9e2cb8b":"code","14341434":"code","73cd77bc":"code","7bd90dd4":"code","d90805b4":"code","f799a0dd":"code","0abfcb69":"code","7036bb37":"code","d2adfd90":"code","ec119a6c":"code","ccd6190d":"code","96470d00":"code","a3750b74":"markdown","ddb5d97f":"markdown","ae8cf8de":"markdown","45025b1a":"markdown","dc7bfe53":"markdown","56f45f0b":"markdown","0bd6b077":"markdown","cfa1f3e2":"markdown","78fdfd4d":"markdown","adf4570c":"markdown","85fb1735":"markdown","8e3aea34":"markdown","701b49ea":"markdown","3695d54b":"markdown","c3d4fc03":"markdown","8e700bc5":"markdown","99e490ae":"markdown","ce9a19b5":"markdown"},"source":{"f3760012":"import os\nfor dirname, _, filenames in os.walk('..\/input\/example-dataset-for-tutorial-notebook'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","387dbc05":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score\n\ndf_5_a = pd.read_csv('..\/input\/example-dataset-for-tutorial-notebook\/5_a.csv')\n\nprint(df_5_a.shape)\ndf_5_a.head()","50319d3a":"df_5_a['y_predicted'] = np.where(df_5_a['proba'] >= 0.5, float(1), float(0))\ndf_5_a.head()\n","97a6f30e":"# Checking to see if there's any 'proba' less than or equal to 0.5\n# And there is none. So all y_predicted will be classified as 1\n# df = df_5_a.loc[df_5_a['proba'] <= 0.5 ]\n# df.head()","67dbec45":"# print(df_5_a.to_numpy())\nactual_y_train_arr = df_5_a.iloc[:, 0].values\nprint('actual_y_train_arr ', actual_y_train_arr)\npredicted_y_arr = df_5_a.iloc[:, 2].values\nprint('predicted_y_arr ', predicted_y_arr)\ny_proba = df_5_a.iloc[:, 1].values","625fe419":"\ndef calculate_confusion_matrix(true_y_classes_array, predicted_y_classes_array):\n\n  # extract all unique classes from the train y class\n  unique_classes = np.unique(true_y_classes_array)\n  # print('unique', unique_classes)\n\n  # initialize a matrix with zero values that will be the final confusion matrix\n  confusion_matrix = np.zeros((len(unique_classes), len(unique_classes)))\n\n  for i in range(len(unique_classes)):\n    for j in range(len(unique_classes)):\n      confusion_matrix[i, j] = np.sum((true_y_classes_array == unique_classes[i]) & (predicted_y_classes_array == unique_classes[j]))\n\n  return confusion_matrix\n\n# actual_y_class_list = [1, 3, 3, 2, 5, 5, 3, 2, 1, 4, 3, 2, 1, 1, 2]\n# predicted_y_class_list = [1, 2, 3, 4, 2, 3, 3, 2, 1, 2, 3, 1, 5, 1, 1]\n\n# Only binary class dataset\nactual_y_class_list = [1, 1, 1, 0, 0]\npredicted_y_class_list =   [1, 1, 1, 0, 1]\n\nprint(calculate_confusion_matrix(actual_y_class_list, predicted_y_class_list))","33648567":"def get_confusion_matrix(true_y_classes_array, predicted_y_classes_array):\n  \n  unique_classes = np.unique(true_y_classes_array)\n  # For a binary class the above will give me [0 1] numpy array\n  # so top-left of confusion matrix will start from 0 i.e. 'True Negative'\n\n  # But the challenge here asks that the top left will be 'True Positive'\n  # Hence I need to reverse the above numpy array\n  unique_classes = unique_classes[::-1]\n  # print('reversed unique', unique_classes) # will convert the above array to [1 0]\n\n  # initialize a matrix with zero values that will be the final confusion matrix\n  # For the binary class-label dataset, this confusion matrix will be a 2*2 square matrix\n  confusion_matrix = np.zeros((len(unique_classes), len(unique_classes)))\n\n  for i in range(len(unique_classes)):\n    for j in range(len(unique_classes)):\n      confusion_matrix[i, j] = np.sum((true_y_classes_array == unique_classes[j]) & (predicted_y_classes_array == unique_classes[i]))\n\n  return confusion_matrix\n\nconfusion_matrix_5_a = get_confusion_matrix(actual_y_train_arr, predicted_y_arr)\nprint(confusion_matrix_5_a)\n\ntrue_negative, false_positive, false_negative, true_positive = int(confusion_matrix_5_a[1][1]), int(confusion_matrix_5_a[0][1]), int(confusion_matrix_5_a[1][0]), int(confusion_matrix_5_a[0][0])","33263b20":"# To check that the total num of elements of the original dataframe matches\n# with the counts captured in the confusion matrix\n# sum-all-the-elements-of-the confusion_matrix_5_a\nsum_all_elements_of_confusion_matrix = np.concatenate(confusion_matrix_5_a).sum()\nprint(sum_all_elements_of_confusion_matrix == df_5_a.shape[0] )","4273eb06":"# Testing my custom confusion_matrix result with scikit-learn\nfrom sklearn.metrics import confusion_matrix\nsklearn_confustion_matrix = confusion_matrix(actual_y_class_list, predicted_y_class_list)\nprint(sklearn_confustion_matrix)","4a82a4b7":"tn, fp, fn, tp = confusion_matrix(actual_y_class_list, predicted_y_class_list).ravel()\nprint(tn, fp, fn, tp)\nprint(true_negative, false_positive, false_negative, true_positive)","adf03a21":"# the below function will work only for\n# binary confusion matrix\ndef get_f1_score_accuracy_score(binary_conf_matrix):\n    true_negative  = binary_conf_matrix[1][1]    \n    false_positive = binary_conf_matrix[0][1]\n    false_negative = binary_conf_matrix[1][0]\n    true_positive = binary_conf_matrix[0][0]\n\n    precision = true_positive \/ (true_positive + false_positive)\n    recall = true_positive\/ (true_positive + false_negative)\n    \n    f1_score = (2 * (precision * recall)) \/ (precision + recall )\n    \n    sum_all_elements_of_confusion_matrix = np.concatenate(binary_conf_matrix).sum()\n    \n    accuracy_score = (true_positive + true_negative)\/sum_all_elements_of_confusion_matrix\n    \n    return f1_score, accuracy_score\n\n\nprint(\"My custom function's f1_score and accuracy_score \", get_f1_score_accuracy_score(confusion_matrix_5_a))  \n\n","ef20e0f1":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\n\nsklearn_f1_score = f1_score(actual_y_train_arr, predicted_y_arr)\nprint('sklearn_f1_score ', sklearn_f1_score)\n\nsklearn_accuracy_score = accuracy_score(actual_y_train_arr, predicted_y_arr)\nprint('sklearn_accuracy_score ', sklearn_accuracy_score)\n","50983e30":"\ndef get_single_tpr_fpr(df):\n\n    '''\n    Note, this implementation is only for binaly class labels (0 and 1)\n    :param df: the dataframe should have 'y' and 'y_predicted' as its labels\n    :return: a list containing tpr and fpr\n    '''\n\n    tp = ((df['y'] == 1.0 ) & (df['y_predicted'] == 1)).sum()\n    fp = ((df['y'] == 0.0 ) & (df['y_predicted'] == 1)).sum()\n    tn = ((df['y'] == 0.0 ) & (df['y_predicted'] == 0)).sum()\n    fn = ((df['y'] == 1.0 ) & (df['y_predicted'] == 0)).sum()\n\n    tpr = tp \/ (tp + fn )\n    fpr = fp \/ (fp + tn)\n\n    return [tpr, fpr]\n\n\n# While computing AUC score you need to calculate \"TP,\"FP\" at every threshold by using actual \"y\" and predicted \"y_pred\".\n\ndef calculate_all_thresholds_tpr_fpr_arr(df_original):\n\n    '''\n\n    :param df_original: the original dataframe, which should have a 'proba' label\n    :return: two arrays, tpr_arr_for_all_thresholds, fpr_arr_for_all_thresholds\n    '''\n\n    tpr_arr_for_all_thresholds = []\n    fpr_arr_for_all_thresholds = []\n\n    sorted_df = df_original.sort_values(by=['proba'], ascending=False)\n\n    unique_probability_thresholds = sorted_df['proba'].unique()\n\n    for threshold in tqdm(unique_probability_thresholds):\n        sorted_df['y_predicted'] = np.where(sorted_df['proba'] >= threshold, 1, 0)\n        tpr_fpr_arr = get_single_tpr_fpr(sorted_df)\n        tpr_arr_for_all_thresholds.append(tpr_fpr_arr[0])\n        fpr_arr_for_all_thresholds.append(tpr_fpr_arr[1])\n\n    return tpr_arr_for_all_thresholds, fpr_arr_for_all_thresholds\n","47fa3f14":"from tqdm import tqdm\nall_tpr_together_5_a, all_fpr_together_5_a = calculate_all_thresholds_tpr_fpr_arr(df_5_a)\nauc_score_5_a = np.trapz(all_tpr_together_5_a, all_fpr_together_5_a)\nprint('My Custom function ROC-AUC Score for 5_a.csv: ', auc_score_5_a)","53537d4c":"# Checking result with scikit-learn\nsklearn_roc_auc_score = roc_auc_score(actual_y_train_arr, y_proba)\nprint('sk-learn roc_auc_score for 5_a.csv: ', sklearn_roc_auc_score)\n# sk-learn roc_auc_score:  0.48829900000000004","838ea00b":"# Plotting of ROC-AUC Curve\n\nplt.plot(all_tpr_together_5_a, all_fpr_together_5_a, 'r', lw=2)\nplt.plot([0, 1], [0, 1], 'k-', lw=2)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('AUC={}'.format(round(auc_score_5_a, 4)))\n","693bc47a":"from sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\n\nx = np.random.randint(40, 400, 100).reshape(-1, 1)\ny = np.random.randint(0, 2, 100)\n\nmodel = LogisticRegression()\nmodel.fit(x, y)\nprobs = model.predict_proba(x)\nfpr, tpr, thresholds = metrics.roc_curve(y, probs[:,1])\n\n# %%\nplt.subplots(figsize=(10, 6))\nplt.plot(fpr, tpr, 'o-', label=\"ROC curve\")\nplt.plot(np.linspace(0,1,10), np.linspace(0,1,10), label=\"diagonal\")\nfor x, y, txt in zip(fpr[::5], tpr[::5], thresholds[::5]):\n    plt.annotate(np.round(txt,2), (x, y-0.04))\nrnd_idx = 27\nplt.annotate('this point refers to the tpr and the fpr\\n at a probability threshold of {}'.format(np.round(thresholds[rnd_idx], 2)),\n             xy=(fpr[rnd_idx], tpr[rnd_idx]), xytext=(fpr[rnd_idx]+0.2, tpr[rnd_idx]-0.25),\n             arrowprops=dict(facecolor='black', lw=2, arrowstyle='->'),)\nplt.legend(loc=\"upper left\")\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.show()","ef58eaf5":"df_5_c = pd.read_csv('..\/input\/example-dataset-for-tutorial-notebook\/5_c.csv')\nprint('df_5_c.shape ', df_5_c.shape)\ndf_5_c.head()","e9e2cb8b":"actual_y_train_arr_5_c = df_5_c.iloc[:, 0].values\nprint('actual_y_train_arr_5_c ', actual_y_train_arr_5_c)\n\ny_proba_5_c = df_5_c.iloc[:, 1].values\nprint('y_proba_5_c ', y_proba_5_c)\n\nunique_probability_thresholds_5_c = np.unique(df_5_c['prob'])\n","14341434":"# First I will modify the above get_tpr_fpr() function to return the value of A\n\ndef get_A_metric(y_actual, y_probabilities, threshold):\n    tp = 0\n    fp = 0\n    tn = 0\n    fn = 0\n\n    min_a = float('inf')\n\n    for i in range(len(y_probabilities)):\n        if y_probabilities[i] >= threshold :\n            if y_actual[i] == 1:\n                tp += 1\n            else:\n                fp += 1\n\n        if y_probabilities[i] < threshold:\n            if y_actual[i] == 0:\n                tn += 1\n            else:\n                fn += 1\n\n    A = (500 * fn) + (100 * fp)\n\n    return A\n","73cd77bc":"# Now a function to traverse the entire unique probability thresholds array\n# and return the minimum value of A and also the corresponding threshold\n\ndef get_minimized_a(y_actual, y_probabilities, total_threshold_arr ):\n  min_a = float('inf')\n  min_t = 0\n\n  for threshold in total_threshold_arr:\n    a = get_A_metric(y_actual, y_probabilities, threshold)\n    if a <= min_a:\n        min_a = min(a, min_a)\n        min_t = threshold\n\n  return min_a, min_t","7bd90dd4":"print(get_minimized_a(actual_y_train_arr_5_c, y_proba_5_c, unique_probability_thresholds_5_c ))\n# (141000, 0.2300390278970873)","d90805b4":"# from sklearn.metrics import r2_score\n\ndf_5_d = pd.read_csv('..\/input\/example-dataset-for-tutorial-notebook\/5_d.csv')\ndf_5_d.head()","f799a0dd":"# print(df_5_d.to_numpy())\nactual_y_train_arr_5d = df_5_d.iloc[:, 0].values\nprint('actual_y_train_arr_5d ', actual_y_train_arr_5d)\npredicted_y_arr_5d = df_5_d.iloc[:, 1].values\nprint('predicted_y_arr_5d ', predicted_y_arr_5d)","0abfcb69":"# Checking the r-square metric with sklearn\nfrom sklearn.metrics import r2_score\n\nsklearn_r2 = r2_score(actual_y_train_arr_5d, predicted_y_arr_5d)\nsklearn_r2","7036bb37":"def calculate_r2_score(y_train, y_predicted):\n    y_train_bar = y_train.mean()\n    # y_train_bar = np.mean(y_train)\n\n    sum_squared_residual = ((y_train - y_predicted)**2).sum()\n    sum_squared_total = ((y_train - y_train_bar)**2).sum()\n\n    return 1 - (sum_squared_residual\/sum_squared_total)\n\nprint(calculate_r2_score(actual_y_train_arr_5d, predicted_y_arr_5d))\n","d2adfd90":"def calculate_mse(y_actual, y_predicted):\n    mse = np.mean((y_actual - y_predicted)**2)\n    return mse\n\nprint(calculate_mse(actual_y_train_arr_5d, predicted_y_arr_5d))\n","ec119a6c":"# Checking the Mean Square Error metric with sklearn\nfrom sklearn.metrics import mean_squared_error\n\nsklearn_mse = mean_squared_error(actual_y_train_arr_5d, predicted_y_arr_5d)\nsklearn_mse","ccd6190d":"# The below will return the % value i.e 12.9 means 12.9%\ndef calculate_mean_absolute_percentage_error(y_actual, y_predicted):\n    mape = np.mean((np.abs(y_actual - y_predicted)) \/ np.mean(y_actual)) * 100\n    return mape\n\nprint(calculate_mean_absolute_percentage_error(actual_y_train_arr_5d, predicted_y_arr_5d))","96470d00":"def calculate_mean_absolute_percentage_error_2(y_actual, y_predicted):\n    mape = ((np.sum(np.abs(y_actual - y_predicted))) \/ np.sum(y_actual)) * 100\n    return mape\n\nprint(calculate_mean_absolute_percentage_error_2(actual_y_train_arr_5d, predicted_y_arr_5d))","a3750b74":"### When one of the actual data-point is zero\n\nhttps:\/\/en.wikipedia.org\/wiki\/Mean_absolute_percentage_error\n\nProblems can occur when calculating the MAPE value with a series of small denominators. A singularity problem of the form 'one divided by zero' and\/or the creation of very large changes in the Absolute Percentage Error, caused by a small deviation in error, can occur.\n\nAs an alternative, each actual value (At) of the series in the original formula can be replaced by the average of all actual values (\u0100t) of that series. This alternative is still being used for measuring the performance of models that forecast spot electricity prices.[2]\n\nNote that this is equivalent to dividing the sum of absolute differences by the sum of actual values, and is sometimes referred to as WAPE (weighted absolute percentage error).\n\nSo in that case the formulate becomes\n\n![Imgur](https:\/\/imgur.com\/KTg47Gk.png)\n\nThe derivation is as follows.\n\n![Imgur](https:\/\/imgur.com\/Lm45BRD.png)\n\nBasically,\n\n```python\n\nmean(actual_value) = sum(actual_value) \/ n\n\nhence n * mean(actual_value = sum(a)\n\n```\n","ddb5d97f":"### Explanations and notes on above Confusion matrix function\n\n![img](https:\/\/i.imgur.com\/1A3Izpg.png)\n\n#### Note `unique_classes[0]` is 1 and `unique_classes[1]` = 0\n\n### For first row of my final confusion_matrix\n\n`confusion_matrix[0,0]` => i.e. i, j = 0, 0 => will have the Total 'True' count (i.e. `np.sum()`) of following conditions\n\n`(true_y_classes_array == unique_classes[0]) & (predicted_y_classes_array == unique_classes[0])`\n\nSimilarly for `confusion_matrix[0, 1]` => i.e. i, j = 0, 1 => will have the Total 'True' count (i.e. `np.sum()`)  of following conditions\n\n`(true_y_classes_array == unique_classes[1]) & (predicted_y_classes_array == unique_classes[0])`\n\n---\n\n### Now second row\n\nAnd for second row of my final confusion_matrix\n\n`confusion_matrix[1,0]`  => i.e. i, j = 1, 0 => will have the Total 'True' count (i.e. `np.sum()`)  of following conditions\n\n`(true_y_classes_array == unique_classes[0]) & (predicted_y_classes_array == unique_classes[1])`\n\nSimilarly for `confusion_matrix[1, 1]`  => i.e. i, j = 1, 1  => will have the Total 'True' count (i.e. `np.sum()`) of following conditions\n\n\n`(true_y_classes_array == unique_classes[1]) & (predicted_y_classes_array == unique_classes[1])`","ae8cf8de":"So that thats the minimum value of **A** which is 141000\n\nand the corresponding threshold is 0.2300390278970873","45025b1a":"### From above we can see the values of the confution Matrix matches between scikit-learn and our custom-implementation\n\n\n\n---\n\n## F1 Score\n\n![img](https:\/\/i.imgur.com\/ZPntYB0.jpg)\n\n![Imgur](https:\/\/imgur.com\/qy5Fesd.jpg)\n","dc7bfe53":"# Computing performance metrics in Pure Python without scikit-learn\n\nHere I will compute following for the dataset 5_a.csv\n\n\n - Compute Confusion Matrix\n\n - Compute F1 Score\n\n - Compute AUC Score, we need to compute different thresholds and for each threshold compute tpr,fpr and then use\n\n  Note 1: in this data we can see number of positive points >> number of negatives points\n\n  Note 3: we need to derive the class labels from given score\n  \n  ## $$y^{pred} = [0 \\text{ if } y_{score} < 0.5  \\text{ else }  1]$$\n\n- Compute Accuracy Score","56f45f0b":"## As we can see above the roc_auc_score matches between my Custom function and that of sklearn","0bd6b077":"# What exactly is np.trapz() function\n\nnumpy.trapz() function integrate along the given axis using the composite trapezoidal rule. Say, I have one list of 100 numbers as height for Y axis, and as length for X axis: 1 to 100 with a constant step of 5. I need to calculate the Area that it is included by the curve of the (x,y) points, and the X axis. This is where I can use trapz() function\n\n```py\nimport numpy as np\n\n# The y values.  A numpy array is used here,\n# but a python list could also be used.\ny = np.array([5, 20, 4, 18, 19, 18, 7, 4])\n\n# Compute the area using the composite trapezoidal rule.\narea = np.trapz(y, dx=5)\nprint(\"area =\", area) # area = 452.5\n\n```\n\n### What is Composite Trapezoidal Rule\n\nThe trapezoidal rule gives us a technique to approximate the integral on a given interval [a, b], but we cannot reduce the error because the error depends on the width of the interval over which we are integrating.\n\nBy dividing the interval [a, b] into many smaller intervals, and applying the trapezoidal rule to each, this allows us to find a better approximation the integral.\n\nAn animation that shows what the trapezoidal rule is and how the error in approximation decreases as the step size decreases\n\n\n![Img](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/1\/10\/WikiTrap.gif\/220px-WikiTrap.gif)\n\n[Source](https:\/\/en.wikipedia.org\/wiki\/Trapezoidal_rule)\n\nRead more [here](https:\/\/secure.math.ubc.ca\/~pwalls\/math-python\/integration\/trapezoid-rule\/) and [here](https:\/\/ece.uwaterloo.ca\/~dwharder\/NumericalAnalysis\/13Integration\/comptrap\/complete.html) on Composite Trapezoidal Rule\n\nWhen passed two arguments (y and x both), trapz approximates\n\n![Imgur](https:\/\/imgur.com\/Cxr9X3o.png)\n\nAnd when you do not pass x, then trapz assumes $x(t) = t$ meaning $dx\/dt = 1$\n\nnamely, the area between a curve and the x axis. \n\nAn alternative for numpy `trapz()` function is `simps()` function from scipy. `simps()` function is used to get the integration of y(x) using samples along the axis and composite simpson\u2019s rule by using scipy.integrate.simps() method.\n\n```py\n\nimport numpy as np\nfrom scipy.integrate import simps\n\ny = np.array([5, 20, 4, 18, 20, 18, 3, 4])\noutput = np.trapz(y)\n\nprint(\"trapz() Output : \",output)\n\n# simps\n\narr = np.array([5, 20, 4, 18, 20, 18, 3, 4])\n\noutput = simps(arr)\nprint(\"simps() output =\", output)\n\n# trapz() Output :  87.5\n# simps() output = 88.66666666666666\n\n```\n","cfa1f3e2":"# Coefficient of determination or $R^2$ calculation\n\n[See Definitions of $R^2$ error](https:\/\/en.wikipedia.org\/wiki\/Coefficient_of_determination#Definitions)\n\nCoefficient of determination also called as $R^2$ score is used to evaluate the performance of a linear regression model. It is the amount of the variation in the output dependent attribute which is predictable from the input independent variable(s). It is used to check how well-observed results are reproduced by the model, depending on the ratio of total deviation of results described by the model.\n\nAssume R2 = 0.68\nIt can be referred that 68% of the changeability of the dependent output attribute can be explained by the model while the remaining 32 % of the variability is still unaccounted for.\nR2 indicates the proportion of data points which lie within the line created by the regression equation. A higher value of R2 is desirable as it indicates better results.\n\nR\u00b2 is calculated by taking one minus the sum of squares of residuals divided by the total sum of squares.\n\n![Imgur](https:\/\/imgur.com\/X0zWQho.png)","78fdfd4d":"## Finally the ROC-AOC Score","adf4570c":"\n# C. Computing the best threshold\n\n### Compute the best threshold (similarly to ROC curve computation) of probability which gives lowest values of metric A for the given data 5_c.csv\n\nwe will be predicting label of a data points like this:\n\n## $$ y^{pred} = [0 \\text{ if } y_{score} < \\text{ threshold }  \\text{ else } 1 $$\n\n$ A = 500 \\times \\text{number of false negative} + 100 \\times \\text{numebr of false positive}$\n\nNote 1: in this data we can see number of negative points > number of positive points","85fb1735":"## From above we can see the values of1_score, accuracy_score matches between scikit-learn and our custom-implementation\n\n# Note on ROC Curve\n\nThe ROC curve is plotted with TPR against the FPR where TPR is on y-axis and FPR is on the x-axis.\n\nIn Machine Learning, performance measurement is an essential task. So when it comes to a classification problem, we can count on an AUC - ROC Curve. When we need to check or visualize the performance of the multi - class classification problem, we use AUC (Area Under The Curve) ROC (Receiver Operating Characteristics) curve. It is one of the most important evaluation metrics for checking any classification model\u2019s performance. It is also written as AUROC (Area Under the Receiver Operating Characteristics)\n\n### What is AUC - ROC Curve?\n\nAUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease.\nThe ROC curve is plotted with TPR against the FPR where TPR is on y-axis and FPR is on the x-axis.\n\n\n\n---\n\n# AUC-ROC Score Calculation\n\nThe Receiver Operating Characetristic (ROC) curve is a graphical plot that allows us to assess the performance of binary classifiers. With imbalanced datasets, the Area Under the Curve (AUC) score is calculated from ROC and is a very useful metric in imbalanced datasets.\n\nTPR and FPR are defined as follows:\n\n- TPR = True Positives \/ All Positives\n- FPR = False Positives \/ All negatives","8e3aea34":"## So the r-square measures matches between my custom function and sklearn's inbuilt funcion\n\n---\n\n# Mean Squared Error (MSE) \/ Mean Squared Deviation (MSD)\n\nThe Mean Squared Error measures the average of the errors squared. It basically calculates the difference between the estimated and the actual value, squares these results and then computes their average.\n\nBecause the errors are squared, MSE can only assume non-negative values. Due to the intrinsic randomness and noise associated with most processes, MSE is usually positive and not zero.\n\n![Imgur](https:\/\/imgur.com\/GWH0ap5.png)","701b49ea":"## So the Mean Squared Error measures matches between my custom function and sklearn's inbuilt funcion\n\n---\n\n# Mean Absolute Percentage Error (MAPE)\n\nThe Mean Absolute Percentage Error measures the error between actual and forecasted values as a percentage. It achieves so by calculating it similarly to MAE, but also dividing it by the actual value, expressing the result as a percentage.\n\nBy expressing the error as a percentage, we can have a better understanding of how off our predictions are in relative terms. For instance, if we were to predict next year\u2019s spending, an MAE error of $50 could be both a relatively good or bad approximation.\n\n![Imgur](https:\/\/imgur.com\/zCFJTE6.png)\n","3695d54b":"### Now verifying the above F1-Score with that of sk-learn\n","c3d4fc03":"## Explanations on the above calculate_confusion_matrix() funcion\n\n[In above implementation, for a binary class-label (1 and 0 ) I will have 'true-nagative' at the top left of the final confusion matrix, as the traversing of the unique_classes array will start from 0 ]\n\n#### 1.np.zeros()  - the first arg of np.zeros() is the shape which is a tuple of ints or simple int\n\ne.g., (2, 3) or 2.\n\n---\n\n#### 2. Now for each row, I need to compare the values between true_y_classes_array and predicted_y_classes_array\n\nSo, I will implement this by comparing each element from true_y_classes_array with each of the unique array's elements and then the same for  predicted_y_classes_array\n\nIt will give me a list of True \/ False. Lets see this example\n\n```python\narr1 = [1, 1, 2, 6, 4]\narr2 = np.unique(arr1) # [1 2 4 6]\n\nprint(\"comparing \",  arr1 == arr2[0])\n# comparing [ True  True False False False]\n```\n\nThat is, the above will compare the whole of arr1 with arr2[0] which is 1\n\n---\n\n### 3. An now use np.sum() to count number of True value\n\n```python\nprint('np.sum of above True False numpy array ', np.sum(arr1 == arr2[0]))\n\n# np.sum of above True False numpy array  2\n```\n\nNote that above kind of operation will only work with `np.unique()` which returns a numpy array, and will not work with plain python list\n\n---\n\n### 4. So now for the confusion matrix, I need to fill up with matching counts of\n\n#### Note `unique_classes[0]` is 1 and `unique_classes[1]` = 0\n\n### For first row of my final confusion_matrix\n\nconfusion_matrix[0,0] => i.e. i, j = 0, 0 => will have the Total 'True' count (i.e. `np.sum()`)  of following conditions\n\n`(true_y_classes_array == unique_classes[0]) & (predicted_y_classes_array == unique_classes[0])`\n\nSimilarly for `confusion_matrix[0, 1]` => i.e. i, j = 0, 1  => will have the Total 'True' count (i.e. `np.sum()`)  of following conditions\n\n`(true_y_classes_array == unique_classes[0]) & (predicted_y_classes_array == unique_classes[1])`\n\n#### And for second row of my final confusion_matrix\n\nconfusion_matrix[1,0] => will have the Total 'True' count (i.e. `np.sum()`)  of following conditions\n\n`(true_y_classes_array == unique_classes[1]) & (predicted_y_classes_array == unique_classes[0])`\n\nSimilarly for confusion_matrix[1, 1] => will have the Total 'True' count (i.e. `np.sum()`)  of following conditions\n\n`(true_y_classes_array == unique_classes[1]) & (predicted_y_classes_array == unique_classes[1])`\n\n\"\"\"\n\n---\n\n# Confusion matrix of binary class-labels placing 'true-positive' \/ TF at the top left of Matrix\n\n### If for a binary class-label I have to place 'true-positive' \/ TF at the top left of the final confusion matrix\n\ni.e. the top-left will have 1 instead of 0 in the final confusion matrix then only need to reverse the unique_classes variable as below.","8e700bc5":"#### In below implementation, for a binary class-label (1 and 0 ) I will have 'true-nagative' at the top left of the final confusion matrix, as the traversing of the unique_classes array will start from 0\n\n![img](https:\/\/i.imgur.com\/MvapmPv.png[\/img])","99e490ae":"## Annotating threshold value in the RUC-AOC curve\n\nWhen we only plot the TPR and the FPR against each other we'll loose the threshold information. However, we can easily add them to the plot. In the below example with some randomly generated X and Y data, I annotated every 5th value but this should be enough the see the relationship (high confidence - bottom left, low confidence - top right).","ce9a19b5":"## After plotting ROC-Curve - next question is how the threshold relates back to the values of the variable (x) for identification of the cut off.\n\nSimple ans is we can not.\n\nX was our input matrix on which we performed the prediction. The thresholds are only related to the prediction from the classifier (\"probabilities\" values)."}}