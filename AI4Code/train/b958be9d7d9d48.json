{"cell_type":{"c4173ec4":"code","766a91aa":"code","efd42551":"code","5443cacd":"code","89b22d4f":"code","1da9016d":"code","0bad44af":"code","b981c7cb":"code","f30ea024":"code","1f368ac1":"code","38fb3622":"code","54a82289":"code","4cf9e82a":"code","d841591a":"code","2a91f777":"code","d06807ba":"code","9c7b23e3":"code","ae6b8a5e":"code","c2f81721":"code","d281edfc":"code","71b60589":"code","36324a23":"code","00bf5f13":"code","993881a4":"code","1a24d5a9":"code","8d61929b":"code","db0d0018":"code","f050837a":"markdown","06799fa6":"markdown","8a44a539":"markdown","22f8280f":"markdown","e40f49d6":"markdown","6d686377":"markdown","8104e983":"markdown","a95e3647":"markdown","84b7c994":"markdown"},"source":{"c4173ec4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport datetime\nimport xgboost as xgb\nfrom sklearn.model_selection import cross_val_score,train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\n\nimport os\n","766a91aa":"X = pd.read_csv(\"\/kaggle\/input\/production-quality\/data_X.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/production-quality\/sample_submission.csv\")\nY = pd.read_csv(\"\/kaggle\/input\/production-quality\/data_Y.csv\")","efd42551":"X[\"date_time\"]=pd.to_datetime(X[\"date_time\"])\nX[\"date_hour\"] = X[\"date_time\"].apply(lambda x: x.strftime(\"%d-%m-%Y-%H\"))","5443cacd":"X","89b22d4f":"# df = pd.DataFrame()\n# for date_hour in list(X.date_hour.unique()):\n#     temp = X.loc[X.date_hour==date_hour]\n#     ah = list(temp[\"AH_data\"].unique())[0]\n#     temp = temp[temp.columns[1:]]\n#     temp = pd.DataFrame(temp.set_index([\"date_hour\",\"AH_data\"]).stack()).reset_index()[[\"level_2\",0]].T.drop([\"level_2\"])\n#     temp[\"date_hour\"]= date_hour\n#     temp[\"AH_data\"]=ah\n#     df = pd.concat([df,temp])","1da9016d":"df = pd.read_csv(\"\/kaggle\/input\/training-frame\/training_frame.csv\")\ndf","0bad44af":"Y[\"date_shifted\"] = pd.to_datetime(Y[\"date_time\"]) - datetime.timedelta(hours=1)\nY[\"date_shifted\"] = pd.to_datetime(Y[\"date_shifted\"])\nY[\"date_shifted\"] = Y[\"date_shifted\"].apply(lambda x: x.strftime(\"%d-%m-%Y-%H\"))","b981c7cb":"training = pd.merge(df,Y[[\"date_shifted\",\"quality\"]],left_on=\"date_hour\",right_on=\"date_shifted\",how=\"inner\")","f30ea024":"submission[\"date_hour\"] = pd.to_datetime(submission[\"date_time\"]).apply(lambda x: x.strftime(\"%d-%m-%Y-%H\"))\nvalidation = pd.merge(df,submission[[\"date_hour\",\"quality\"]],left_on=\"date_hour\",right_on=\"date_hour\",how=\"inner\")","1f368ac1":"training","38fb3622":"xgb_model = xgb.XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)\ndt_model = DecisionTreeRegressor(random_state=0)\nX_val = training.drop([\"date_hour\",\"date_shifted\",\"quality\"],axis=1)\ny_val = training[\"quality\"]","54a82289":"xgb_cv =cross_val_score(xgb_model,X_val,y_val,cv=10,scoring=('neg_mean_absolute_error'))","4cf9e82a":"print(\"Average XGB Cross Validation MAE: {0}\".format(np.abs(xgb_cv.mean())))\nprint(\"Best XGB Cross Validation MAE: {0}\".format(np.abs(xgb_cv.max())))","d841591a":"cv_dt = cross_val_score(dt_model,X_val,y_val,cv=5,scoring=('neg_mean_absolute_error'))","2a91f777":"print(\"Average Decision Tree Cross Validation MAE: {0}\".format(np.abs(cv_dt.mean())))\nprint(\"Best Decision Tree Cross Validation MAE: {0}\".format(np.abs(cv_dt.max())))","d06807ba":"X_train,X_test,y_train,y_test = train_test_split(X_val, y_val, test_size=0.15, random_state=0)","9c7b23e3":"dt_model.fit(X_train,y_train)","ae6b8a5e":"xgb_model.fit(X_train,y_train)","c2f81721":"predictions_xgb = xgb_model.predict(X_test)\npredictions_dt = dt_model.predict(X_test)","d281edfc":"print(\"XGBoost Regressor MAE: {0}\".format(mean_absolute_error(y_test,predictions_xgb)))\nprint(\"DecisionTree Regressor MAE: {0}\".format(mean_absolute_error(y_test,predictions_dt)))","71b60589":"results = pd.DataFrame()\nresults[\"True\"] = y_test\nresults[\"Predicted XGB\"]=predictions_xgb\nresults[\"Predicted DecisionTree\"]=predictions_dt","36324a23":"results","00bf5f13":"model = xgb.XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)\nmodel.fit(X_val,y_val)","993881a4":"validation = validation.drop([\"date_hour\",\"quality\"],axis=1)","1a24d5a9":"quality = model.predict(validation)\nsubmission[\"quality\"]=quality","8d61929b":"submission = submission[[\"date_time\",\"quality\"]]","db0d0018":"submission","f050837a":"df is our transformed dataset. We can extract our training input and our submission input using date_hour.","06799fa6":"## SUMMARY\nThe best value on metrics we have achieved was MAE = 6.954. To improve upon this value we would need to work with GridSearch to get better parameters from our XGB Regression model. \nPreprocessing was quite limited since we didn't have any missing values and XGB doesn't require feature scalling or normalization (it can deal with null values too).\n\nIn the end the key to achiving good value is to pass entire process (1h of measurements) as single row since we do not know at which point process is losing its quality so we need to compare process as whole. This requires quite extensive transformation (if someone has way to do it faster than 9h, I would really love to know it).\n\nPerhaps someone could say why didn't I checked correlations between each feature and target variables to see if prediction is even possible and that would be correct assuption however in industrial problems I believe that engineering (and domain) knowledge are much more important, especially since there is so many features across entire manufacturing process (it would be hard to check if those are affecting quality).\n\n","8a44a539":"## Process DataFrame\nSo generally I commented this code out because it exceeds Kaggle notebook time limits. I downloaded data required in this transformation and did it locally on my computer. Feel free to uncomment this and use it, however it may take longer than 9hours. If you find way to do it more efficiently, please let me know.","22f8280f":"So after cross validation I can see that XGB performs better than DecisionTrees. I wouldn't assume that MAE = 7.012 is our final result as we used 90% of training data to train models, so we still can have a little bit more than that.","e40f49d6":"## So what we know and what can we assume?\n\n![image.png](attachment:f8ac608b-c664-4fe2-b2ec-5f7197aed5f0.png)\n\nWe have machine with 5 chambers (Chn) and each chambler has got 3 temperature sensors (TSn) and there are also humidity senors (HS) and layer height sensor (LHS).\nI can pressume that temperatures can affect roasting (overroast and underroast). <br>\n3 Sensors are probably located in different spots of chamber to ensure proper temperature in each part of chamber and ultimately even roasting of material.<br>\nHumidity is probably there to indicate required environment to roast material.<br>\nLayer height is more of a volume information. We can can assume that volume can affect roasting, also layer might be good indicator when material was changed.<br>\n\nSince entire process takes about 1 hour and seeds are sampled every hour. We can assume that quality of material sampled refers to measurements from previous hour.<br>\n\nAs for the measurements it is very difficult to assign one target variable to entire hour of measurements.\nIf we assign one value to all measurements we will literally have good conditions that show possibly lower quality material and it will amount to nothing while building model. <br>\nIn order to show entire hour of quality measurements I would suggest to transpose our data a little bit and get something that I would call process matrix (or table). Basically each minute of measurement would be a set of columns that make up one very high dimensional row. This would increase dimension of our data quite significantly (16 columns * 60 minutes). <br>\nOur single sample would have around 960 columns + layer height column as layer is very likely to stay the same. In order to decrease dimension of our dataset we can go machine learning route with PCA or tSNE... or we can also get average chamber temperature for each minute. However I wouldnt recommend doing anything of that, its better to leave data as it is for now.<br>\n\nAs for modeling. In my experience a lot of these problems were solved using Decision Trees or Gradient Boosting methods (see [Bosch Production Line Performance](https:\/\/www.kaggle.com\/c\/bosch-production-line-performance)). In that case I will pick XGBoost Regression model as my base and I will compare it to Sklearn DecisionTree Regressor.\n\n\n","6d686377":"Here we can see that MAE is sitting at 6.954.","8104e983":"## Submission file","a95e3647":"# Production Quality Prediction\n\n**Context<br>**\nYou need to build a model that, on the basis of data arriving every minute, determines the quality of products produced on a roasting machine. <br>\n\n**Content<br>**\nThe roasting machine is an aggregate consisting of 5 chambers of equal size, each chamber has 3 temperature sensors. In addition, for this task, you have collected data on the height of the raw material layer and its moisture content. Layer height and humidity are measured when raw materials enter the machine. Raw materials pass through the kiln in an hour.<br>\n\n**Acknowledgements<br>**\nProduct quality is measured in the laboratory by samples that are taken every hour, data on known analyzes are contained in the file data_Y.csv. The file indicates the time of sampling, the sample is taken at the exit of the roasting machine.<br>\n\n**Inspiration<br>**\nYou agreed with the customer that the model will be estimated by the MAE indicator, to evaluate the model, it is necessary to generate predictions for the period specified in the file sample_submission.csv (5808 predictions).<br>\n\n**Notebook Author** <br>\n**Aleksander Jakubowski**\n\n#### Example of Roasting machine \n![image.png](attachment:bcc472f1-925e-4906-acb1-e247075a51e0.png)","84b7c994":"To check how well model performs overall I will use cross-validation. "}}