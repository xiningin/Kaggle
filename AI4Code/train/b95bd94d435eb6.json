{"cell_type":{"14e11f83":"code","b5a4ad5b":"code","6ca7cafe":"code","3405ec22":"code","2f1fc912":"code","deffe53d":"code","f46dd174":"code","9bc90680":"code","858ecc2a":"code","a54111db":"code","2d80e892":"code","50c90ee9":"code","772751d7":"code","4edc619a":"code","c76914e1":"code","e49ac132":"code","62229d9c":"code","ac07e713":"markdown","130ca1ba":"markdown","3e29a6e9":"markdown","62510862":"markdown","7cc293d1":"markdown","8181ff3a":"markdown","e722bd2d":"markdown","c4908b20":"markdown","8f792dbe":"markdown","277b61cb":"markdown","a4a9e899":"markdown","1e9f5fc9":"markdown","e8f6a745":"markdown","3b91abc9":"markdown"},"source":{"14e11f83":"# Cloning the Github Repo for the Model.\n!git clone https:\/\/github.com\/DingXiaoH\/RepVGG.git","b5a4ad5b":"# Some Native Python Tools.\nimport glob\nfrom itertools import chain\nimport os\nimport random\n\n# These ones are for dealing with our Data.\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom PIL import Image\nimport numpy as np\n\n# PyTorch is like that one boss who actually does everything he can to help you.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets, transforms\n\n# Keeping Time in check.\nfrom tqdm.notebook import tqdm\n\n# Tools for Preprocessing and stuff.\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder\n","6ca7cafe":"# Heading to the directory with model files.\ncd .\/RepVGG","3405ec22":"# For Grabbing the RepVGG Model.\nfrom repvgg import repvgg_model_convert, create_RepVGG_B2","2f1fc912":"# Heading back to where the other data is stored.\ncd ..\/","deffe53d":"# Setting up the Hyperparameters\n\nbatch_size = 64\nepochs = 15\nlr = 3e-4\ngamma= 0.7\nseed = 42","f46dd174":"# Definign a function to seed everything.\ndef seed_it(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    \n# Running the Function.\nseed_it(seed)","9bc90680":"# Moving stuff to the GPU.\ndevice = \"cuda\"","858ecc2a":"# Defining the paths to files.\ntrainimages = \"..\/input\/cassava-leaf-disease-classification\/train_images\"\n\ntraindf = pd.read_csv(\"..\/input\/cassava-leaf-disease-classification\/train.csv\")\n\ntrain_list = []\n# Converting the Image IDs to their paths.\nfor i in traindf.index:\n    \n    a = traindf[\"image_id\"].loc[i]\n    \n    b = trainimages + \"\/\" + a\n    \n    train_list.append((b, traindf['label'].loc[i]))\n\n# Taking a look.\ntrain_list[:3]","a54111db":"random_idx = np.random.randint(1, len(train_list), size=16)\nfig, axes = plt.subplots(6,6, figsize=(13,13))\n\nfor idx, ax in enumerate(axes.ravel()):\n    img = Image.open(train_list[idx][0])\n    ax.set_title(train_list[idx][1])\n    ax.imshow(img)","2d80e892":"# Splitting the data into training and validation datasets.\ntrain, valid = train_test_split(train_list,\n                                test_size=0.2,\n                                random_state=42)","50c90ee9":"train_trans = transforms.Compose([\n                transforms.RandomResizedCrop(224),\n                transforms.RandomHorizontalFlip(p=0.3),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                         std=[0.229, 0.224, 0.225])\n            ])\n\nvalid_trans = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                         std=[0.229, 0.224, 0.225])\n            ])","772751d7":"class CassavaDataset(Dataset):\n    \n    # Grabbing the transforms presets.\n    def __init__(self, file_list, transform=None):\n        self.file_list = file_list\n        self.transform = transform\n        \n    def __len__(self):\n        self.filelength = len(self.file_list)\n        return self.filelength\n    \n    def __getitem__(self, idx):\n        \n        img_path = self.file_list[idx][0]\n        img = Image.open(img_path).convert(\"RGB\")\n        img_transformed = self.transform(img)\n        \n        # The second item in the tuple is the label.\n        label = self.file_list[idx][1]\n        \n        return img_transformed, label","4edc619a":"train_data = CassavaDataset(train, transform = train_trans)\nvalid_data = CassavaDataset(valid, transform = valid_trans)\n\ntrain_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\nvalid_loader = DataLoader(dataset = valid_data, batch_size = batch_size, shuffle = True)","c76914e1":"model = create_RepVGG_B2(deploy=False).to(device)","e49ac132":"# Defining some presets.\n\ncriterion = nn.CrossEntropyLoss()\n\noptimizer = optim.Adam(model.parameters(), lr = lr)\n\nscheduler = StepLR(optimizer, step_size=1, gamma=gamma)","62229d9c":"for epoch in range(epochs):\n    epoch_loss = 0.0\n    epoch_accuracy = 0.0\n    \n    for data, label in tqdm(train_loader):\n        data = data.to(device)\n        label = label.to(device)\n        \n        output = model(data)\n        \n        label = torch.nn.functional.one_hot(label, num_classes = 5)\n        label = label.squeeze_()\n        label = torch.argmax(label, axis=1)\n        \n        label = label.type_as(output)\n        \n        loss = criterion(output, label.long())\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        acc = (output.argmax(dim=1) == label).float().mean()\n        epoch_accuracy += acc \/ len(train_loader)\n        epoch_loss += loss \/ len(train_loader)\n\n        \n    with torch.no_grad():\n        epoch_val_accuracy = 0\n        epoch_val_loss = 0        \n        \n        for data, label in valid_loader:\n            data = data.to(device)\n            \n            label = label.to(device)\n            #label = label.squeeze_()\n            label = label.type_as(output)\n            \n            val_output = model(data)\n            val_loss = criterion(val_output, label.long())\n            \n            accc = (val_output.argmax(dim = 1) == label).float().mean()\n            epoch_val_accuracy += acc \/ len(valid_loader)\n            epoch_val_loss += val_loss \/ len(valid_loader)\n\n    print(\n        f\"Epoch: {epoch+1} - loss: {epoch_loss:.4f} - acc : {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy: .4f}\\n\"\n    )","ac07e713":"# Importing Libraries\n<p style=\"font-size: 20px; margin-right:50px\"> Importing all the necessary libraries.<\/p>","130ca1ba":"# Model\n<p style=\"font-size: 20px; margin-right:50px\">Grabbing the Model.<\/p>","3e29a6e9":"<h1 style=\"font-size: 27px; margin-right:50px\">Transforms<\/h1>\n<p style=\"font-size: 20px; margin-right:50px\">Producing some transformations for our training and validation datasets.<\/p>","62510862":"<p style=\"font-size: 27px; margin-right:50px\">Also, if you already upvoted the Notebook without even going through it: We are automatically Best Friends. \ud83d\ude0b<\/p>","7cc293d1":"<p style=\"font-size: 20px; margin-right:50px\">Finally collecting everthing into a two DataLoaders. One for the training data and another for the valid data.<\/p>\n","8181ff3a":"# Dataloader\n<p style=\"font-size: 20px; margin-right:50px\">Time to set up a DataLoader with the resources we have.<\/p>","e722bd2d":"<p style=\"font-size: 20px; margin-right:50px\">Okay, so let's get this right. Recently a paper popped up in <a href=\"paperswithcode.com\">paperswithcode's website<\/a>.<br> It talked about <a href=\"https:\/\/paperswithcode.com\/paper\/repvgg-making-vgg-style-convnets-great-again\">Making VGG-style ConvNets Great Again<\/a>.<br><br>\nNo, I am not kidding that is literally the title of that paper.<br>\nI kind of smirked when I read the title. Dude, it's high time we start replacing <b>ConvNets<\/b> with <b>Pretrained Transformer Models<\/b> because well they do an equally or sometimes even better of a job when it comes to <b>Image Classification<\/b>.<br> <br>\nYet here it seems like that Transformers might be met with some sort of competition? Maybe they don't?<br> No one knows let's jump into this.\n<br><br>\nThe Purpose of this Notebook is to help everyone use the RepVGG Model.<\/p>","c4908b20":"<h1 style=\"font-size: 27px; margin-right:50px\">Dataset Class<\/h1>\n<p style=\"font-size: 20px; margin-right:50px\">Defining the PyTorch Dataset Class.<\/p>","8f792dbe":"# Visualizing the Data\n<p style=\"font-size: 20px; margin-right:50px\">Time to take a look at our Data!<\/p>\n","277b61cb":"<h1 style=\"font-size: 27px; margin-right:50px\">Preprocessing<\/h1>\n<p style=\"font-size: 20px; margin-right:50px\">Here, we will make a list of all images' paths and put them in a tuple along with thier labels. Also, let's gather all those tuples and put them into a list.<\/p>","a4a9e899":"# Training the Model\n<p style=\"font-size: 20px; margin-right:50px\">Let's use everything that we have done above to finally train the model for 20 Epochs!<\/p>","1e9f5fc9":"<center><p style=\"font-size: 35px;float:center\">\ud83c\udfc3\ud83c\udffd\u200d\u2642\ufe0f Implementing the RepVGG \ud83c\udfc3\ud83c\udffd\u200d\u2640\ufe0f<\/p><\/center>","e8f6a745":"<h1 style=\"font-size: 27px; margin-right:50px\">Seeding Everything<\/h1>\n<p style=\"font-size: 20px; margin-right:50px\">Seeding makes things reproduceable so let's go!<\/p>","3b91abc9":"<img src=\"https:\/\/raw.githubusercontent.com\/DingXiaoH\/RepVGG\/main\/arch.PNG\" style=\"width:30%;float:right;display:inline-block\">\n<h1 style=\"font-size: 27px; margin-right:50px\">Briefing about this Model<\/h1>\n\n\n<p style=\"font-size: 20px; margin-right:100px\">A simple but powerful architecture of convolutional neural network, which has a VGG-like inference-time body composed of nothing but a stack of 3x3 convolution and ReLU, while the training-time model has a multi-branch topology. Such decoupling of the training-time and inference-time architecture is realized by a structural re-parameterization technique so that the model is named RepVGG.\n<br><br>\nFor any further reading: <a href=\"https:\/\/github.com\/DingXiaoH\/RepVGG\">Check out the researcher's Repo!<\/a><\/p>"}}