{"cell_type":{"5e94843b":"code","f1e18675":"code","cca4e00b":"code","25ccb4b1":"code","8378d10c":"code","8adceeb8":"code","e63a5cb3":"code","ebc77551":"code","b3e5b257":"code","436cdfcb":"code","1dcb0026":"code","6c1b2ea4":"code","8170a47e":"code","32fe074d":"code","f8bbb9f3":"code","4dadaad9":"code","e3f14b34":"code","35c5ed88":"markdown","ec7c6367":"markdown","fb01d1f6":"markdown"},"source":{"5e94843b":"## Importing the packeges\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","f1e18675":"# Importing the Dataset\nfrom sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()","cca4e00b":"cancer.data","25ccb4b1":"# Forming the DataFrame\ncancer_df = pd.DataFrame(cancer['data'],columns=cancer['feature_names'])\ncancer_df['target']=cancer['target']","8378d10c":"## Pairplot \nsns.pairplot(cancer_df,vars=['mean radius', 'mean texture', 'mean perimeter', 'mean area','mean smoothness']\n             ,hue='target')","8adceeb8":"sns.countplot(cancer_df['target'],label='Count')","e63a5cb3":"## Scatter Plot \nplt.figure(figsize=(8,8))\nplot=sns.scatterplot(x='mean radius',y='mean texture',data=cancer_df,hue='target')\n\n","ebc77551":"plt.figure(figsize=(30,20))\nsns.heatmap(cancer_df.corr(),annot=True)","b3e5b257":"## Differenciating the Variables\nX = cancer_df.drop(['target'],axis=1)\ny= cancer_df['target']","436cdfcb":"## Splitting the dataset into train and Test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","1dcb0026":"from sklearn.svm import SVC\nsvc_classifier = SVC()\nsvc_classifier.fit(X_train,y_train)\ny_pred = svc_classifier.predict(X_test)","6c1b2ea4":"from sklearn.metrics import confusion_matrix,classification_report\ncm = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm,annot=True)","8170a47e":"print(classification_report(y_test, y_pred))","32fe074d":"from sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler(feature_range=(0,1))\nX_train = mms.fit_transform(X_train)\nX_test = mms.fit_transform(X_test)","f8bbb9f3":"svc_classifier.fit(X_train,y_train)\ny_pred=svc_classifier.predict(X_test)\n\n","4dadaad9":"from sklearn.metrics import confusion_matrix,classification_report\ncm = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm,annot=True)","e3f14b34":"print(classification_report(y_test, y_pred))","35c5ed88":"We get bad precision and recall as feature scaling is not done","ec7c6367":"# Model Improvement by Scaling ","fb01d1f6":"We get better results because of better feature scaling. To improve the precsion and recall futher hyper parameter tuning is required"}}