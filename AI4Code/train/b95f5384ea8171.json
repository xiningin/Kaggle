{"cell_type":{"27317cdc":"code","3f234ec7":"code","7c5c64e4":"code","fb2abe48":"code","57a4cc4b":"code","f3f3d96a":"code","7b873c99":"code","06546034":"code","b52b17d1":"code","e81ae0ff":"code","f2a662d6":"code","7d4d216c":"code","2bea01aa":"code","be915845":"code","333f3102":"code","9bd18657":"code","e06b823f":"code","12ec6587":"code","53b160ba":"code","b8b3b752":"code","4e97942c":"code","615ee175":"code","9c571a99":"code","2b4e1bc3":"code","641528b9":"markdown","b1c6394d":"markdown"},"source":{"27317cdc":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import precision_score, recall_score, f1_score,\\\n                            accuracy_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n#from imblearn.over_sampling import SMOTE\n#from imblearn.pipeline import Pipeline\n\nimport os\nfrom collections import Counter\n\nnp.random.seed(34)\npath = '..\/input\/tabular-playground-series-sep-2021\/'","3f234ec7":"#","7c5c64e4":"import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndataset_train1 = pd.read_csv(f'{path}train.csv', index_col='id')\n#dataset_test1 = pd.read_csv(f'{path}test.csv', index_col='id')\n\ny = dataset_train1.claim\ny = pd.DataFrame(y)\n#dataset_train = dataset_train1.drop(['claim'], axis=1)\ndataset_train = dataset_train1.copy()\n\ndataset_train['nan'] = dataset_train.isnull().sum(axis=1)\ndataset_train['nan'] = dataset_train['nan']\/dataset_train['nan'].max()\n\n#dataset_test1['nan'] = dataset_test1.isnull().sum(axis=1)\n#dataset_test1['nan'] = dataset_test1['nan']\/dataset_test1['nan'].max()","fb2abe48":"dataset_train3 = dataset_train1.drop(['claim'], axis=1)\ndataset_train3['nan'] = dataset_train3.isnull().sum(axis=1)\ndataset_train3['nan'] = dataset_train3['nan']\/dataset_train3['nan'].max()","57a4cc4b":"from sklearn.preprocessing import QuantileTransformer, KBinsDiscretizer\n\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='mean')\ndataset_train = imputer.fit_transform(dataset_train)\n#dataset_test = imputer.transform(dataset_test1)","f3f3d96a":"from sklearn.preprocessing import MinMaxScaler\nx_scaler = MinMaxScaler()\ndataset_train_sc = x_scaler.fit_transform(dataset_train)\n#dataset_test_sc = x_scaler.transform(dataset_test1)","7b873c99":"N_split = int(0.2 * len(dataset_train_sc))\nX_train = dataset_train_sc[:-N_split, :]\nX_test = dataset_train_sc[-N_split:, :]\ny_train = y[:-N_split]\ny_test = y[-N_split:]","06546034":"from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, Concatenate\nfrom tensorflow.keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.initializers import RandomNormal\nimport tensorflow.keras.backend as K\nfrom sklearn.utils import shuffle","b52b17d1":"class cGAN():\n    def __init__(self):\n        self.latent_dim = 120\n        self.out_shape = 120\n        self.num_classes = 2\n        self.clip_value = 0.01\n        #optimizer = Adam(0.00001)\n        optimizer = Adam(0.00001, 0.5)\n        #optimizer = RMSprop(lr=0.00005)\n\n        # build discriminator\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss=['binary_crossentropy'],\n                                   optimizer=optimizer,\n                                   metrics=['accuracy'])\n\n        # build generator\n        self.generator = self.build_generator()\n\n        # generating new data samples\n        noise = Input(shape=(self.latent_dim,))\n        label = Input(shape=(1,))\n        gen_samples = self.generator([noise, label])\n\n        self.discriminator.trainable = False\n\n        # passing gen samples through disc. \n        valid = self.discriminator([gen_samples, label])\n\n        # combining both models\n        self.combined = Model([noise, label], valid)\n        self.combined.compile(loss=['binary_crossentropy'],\n                              optimizer=optimizer,\n                             metrics=['accuracy'])\n        self.combined.summary()\n\n    def wasserstein_loss(self, y_true, y_pred):\n        return K.mean(y_true * y_pred)\n\n    def build_generator(self):\n        init = RandomNormal(mean=0.0, stddev=0.02)\n        model = Sequential()\n\n        model.add(Dense(64, input_dim=self.latent_dim))\n        #model.add(Dropout(0.2))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n\n        model.add(Dense(128))\n        #model.add(Dropout(0.2))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n\n        model.add(Dense(256))\n        #model.add(Dropout(0.2))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n\n        model.add(Dense(self.out_shape, activation='tanh'))\n        model.summary()\n\n        noise = Input(shape=(self.latent_dim,))\n        label = Input(shape=(1,), dtype='int32')\n        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n        \n        model_input = multiply([noise, label_embedding])\n        gen_sample = model(model_input)\n\n        return Model([noise, label], gen_sample, name=\"Generator\")\n\n    \n    def build_discriminator(self):\n        init = RandomNormal(mean=0.0, stddev=0.02)\n        model = Sequential()\n\n        model.add(Dense(256, input_dim=self.out_shape, kernel_initializer=init))\n        model.add(LeakyReLU(alpha=0.2))\n        \n        model.add(Dense(128, kernel_initializer=init))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.3))\n        \n        model.add(Dense(64, kernel_initializer=init))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.3))\n        \n        model.add(Dense(1, activation='sigmoid'))\n        model.summary()\n        \n        gen_sample = Input(shape=(self.out_shape,))\n        label = Input(shape=(1,), dtype='int32')\n        label_embedding = Flatten()(Embedding(self.num_classes, self.out_shape)(label))\n\n        model_input = multiply([gen_sample, label_embedding])\n        validity = model(model_input)\n\n        return Model(inputs=[gen_sample, label], outputs=validity, name=\"Discriminator\")\n\n\n    def train(self, X_train, y_train, pos_index, neg_index, epochs, batch_size=32, sample_interval=50):\n\n        # Adversarial ground truths\n        valid = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n\n        for epoch in range(epochs):\n            \n            #  Train Discriminator with 8 sample from postivite class and rest with negative class\n            idx1 = np.random.choice(pos_index, 8)\n            idx0 = np.random.choice(neg_index, batch_size-8)\n            idx = np.concatenate((idx1, idx0))\n            samples, labels = X_train[idx], y_train[idx]\n            samples, labels = shuffle(samples, labels)\n            # Sample noise as generator input\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n\n            # Generate a half batch of new images\n            gen_samples = self.generator.predict([noise, labels])\n\n            # label smoothing\n            if epoch < epochs\/\/1.5:\n                valid_smooth = (valid+0.1)-(np.random.random(valid.shape)*0.1)\n                fake_smooth = (fake-0.1)+(np.random.random(fake.shape)*0.1)\n            else:\n                valid_smooth = valid \n                fake_smooth = fake\n                \n            # Train the discriminator\n            self.discriminator.trainable = True\n            d_loss_real = self.discriminator.train_on_batch([samples, labels], valid_smooth)\n            d_loss_fake = self.discriminator.train_on_batch([gen_samples, labels], fake_smooth)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n            # Train Generator\n            # Condition on labels\n            self.discriminator.trainable = False\n            sampled_labels = np.random.randint(0, 2, batch_size).reshape(-1, 1)\n            # Train the generator\n            g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)\n\n            # Plot the progress\n            if (epoch+1)%sample_interval==0:\n                print (f\"{epoch} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]}] [G loss: {g_loss}]\")","e81ae0ff":"y_train = np.array(y_train)\ny_train","f2a662d6":"y_train = y_train.reshape(-1,1)\npos_index = np.where(y_train==1)[0]\nneg_index = np.where(y_train==0)[0]","7d4d216c":"cgan = cGAN()","2bea01aa":"cgan.train(X_train, y_train, pos_index, neg_index, epochs=10000)","be915845":"# generating new samples\nnoise = np.random.normal(0, 1, (200000, 120))\nsampled_labels = np.ones(200000).reshape(-1, 1)\n\ngen_samples = cgan.generator.predict([noise, sampled_labels])\ngen_samples = x_scaler.inverse_transform(gen_samples)\nprint(gen_samples.shape)","333f3102":"len(dataset_train3.columns)","9bd18657":"for i in range(len(gen_samples)):\n    gen_samples[i,-1] = 1 if gen_samples[i,-1] >= 0.5 else 0","e06b823f":"gen_samples[210:350,-1]\n","12ec6587":"gen_samples[-1,:]","53b160ba":"dataset_train1 = pd.read_csv(f'{path}train.csv', index_col='id')","b8b3b752":"cols = [f'{i}' for i in range(118)]\ncols.append('nan')\ncols.append('claim')","4e97942c":"gen_df = pd.DataFrame(data = gen_samples, columns=cols)\ngen_df","615ee175":"len(gen_df)","9c571a99":"path = 'gen_dataset.csv'\ngen_df.to_csv(path, index=False)","2b4e1bc3":"noise = np.random.normal(0, 1, (200000, 120))\nnoise\n","641528b9":"# Using GANs to generate new data","b1c6394d":"# Data Exploration and Cleaning"}}