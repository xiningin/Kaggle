{"cell_type":{"6443330c":"code","4c77d861":"code","0cbd8591":"code","077973e7":"code","57fca928":"code","a7d92df1":"code","b01f5889":"code","0f82582a":"code","90b96a51":"code","edb0cc2f":"code","f9b7079e":"code","8e9de2fc":"code","1e96182e":"code","e0ad1026":"code","a9b10f08":"code","668213cd":"code","1a9ccc89":"code","48b9e50a":"code","e4185d3b":"code","a36721b7":"code","62ee942b":"code","730223d8":"code","c91b36e1":"code","362aeeaa":"code","b51c7356":"code","cd85f829":"code","c3164c0f":"code","f9c88abb":"code","7ec99bc4":"code","07197174":"code","508325e0":"code","7ff2d94f":"code","302cafa0":"code","dab511bd":"code","ea81eaac":"code","44708b6d":"code","76aebd01":"code","4b5b1944":"code","d2b31180":"code","d9fcb52e":"code","b5163110":"code","d0f2dcdf":"markdown","b001f5b9":"markdown","7291213a":"markdown","96b9acc0":"markdown","1bc5911e":"markdown","e12e8055":"markdown","6efdfaa6":"markdown","467d4dad":"markdown","f83ad103":"markdown","97a5dbcd":"markdown","79bfa33a":"markdown","b1b95f41":"markdown","e38f2248":"markdown","cfc9b50b":"markdown","fa3c4ce5":"markdown","970bf6b0":"markdown","0a121e26":"markdown","f358ccdc":"markdown","4f2cfb26":"markdown","6afaac20":"markdown","e71cc30b":"markdown","e6640443":"markdown","f15e3f70":"markdown","f3a8a269":"markdown","a2a68c4c":"markdown","5dafc8df":"markdown","a9a3f706":"markdown","36f19034":"markdown","e687489b":"markdown","0f8a1496":"markdown","48300dda":"markdown"},"source":{"6443330c":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nimport xgboost as xgb\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import roc_curve, auc","4c77d861":"data = pd.read_csv('..\/input\/creditcard.csv')","0cbd8591":"data.head()","077973e7":"data.describe().transpose()","57fca928":"data.info()","a7d92df1":"# Next, Class feature will be examined.\nplt.figure(figsize=(10,10))\nsns.countplot(\n    y=\"Class\", \n    data=data,\n    facecolor=(0, 0, 0, 0),\n    linewidth=5, \n    edgecolor=sns.color_palette(\"dark\", 2))\n\nplt.title('Fraudulent Transaction Summary')\nplt.xlabel('Count')\nplt.ylabel('Fraudulent Transaction   Non-Fraudulent Transaction', fontsize=12)","b01f5889":"data_value= data[\"Class\"].value_counts()","0f82582a":"print(data_value)\nprint(data_value\/284807)","90b96a51":"data['Class']= data['Class'].astype('category')","edb0cc2f":"#Distribution of Time\nplt.figure(figsize=(15,10))\nsns.distplot(data['Time'])","f9b7079e":"#Distribution of Amount\nplt.figure(figsize=(10,10))\nsns.distplot(data['Amount'])","8e9de2fc":"data['Hour'] = data['Time'].apply(lambda x: np.ceil(float(x)\/3600) % 24)","1e96182e":"#Class vs Amount vs Hour\npd.pivot_table(\n    columns=\"Class\", \n    index=\"Hour\", \n    values= 'Amount', \n    aggfunc='count', \n    data=data)","e0ad1026":"#Hour vs Class\nfig, axes = plt.subplots(2, 1, figsize=(15, 10))\n\nsns.countplot(\n    x=\"Hour\",\n    data=data[data['Class'] == 0], \n    color=\"#98D8D8\",  \n    ax=axes[0])\naxes[0].set_title(\"Non-Fraudulent Transaction\")\n\n\nsns.countplot(\n    x=\"Hour\",\n    data=data[data['Class'] == 1],\n    color=\"#F08030\", \n    ax=axes[1])\naxes[1].set_title(\"Fraudulent Transaction\")","a9b10f08":"#Amount vs Hour vs Class\nfig, axees = plt.subplots(2, 1, figsize=(15, 10))\n\nplt.title(\"Non-Fraudulent Transactions\")\nsns.barplot(\n    x='Hour',\n    y='Amount', \n    data=data[data['Class'] == 0], \n    palette=\"ocean\", \n    ax=axees[0])\n\nplt.title(\"Fraudulent Transactions\")\nsns.barplot(\n    x='Hour', \n    y='Amount', \n    data=data[data['Class'] == 1], \n    palette=\"Reds\", \n    ax=axees[1])","668213cd":"#Drop hour feature before continues next analysis.\ndata=data.drop(['Hour'], axis=1)","1a9ccc89":"data_nonfraud = data[data['Class'] == 0].sample(2000)\ndata_fraud  = data[data['Class'] == 1]\n\ndata_new = data_nonfraud.append(data_fraud).sample(frac=1)\nX = data_new.drop(['Class'], axis = 1).values\ny = data_new['Class'].values","48b9e50a":"tsne = TSNE(n_components=2, random_state=42)\nX_transformation = tsne.fit_transform(X)","e4185d3b":"plt.figure(figsize=(10, 10))\nplt.title(\"t-SNE Dimensionality Reduction\")\n\ndef plot_data(X, y):\n    plt.scatter(X[y == 0, 0], X[y == 0, 1], label=\"Non_Fraudulent\", alpha=0.5, linewidth=0.15, c='#17becf')\n    plt.scatter(X[y == 1, 0], X[y == 1, 1], label=\"Fraudulent\", alpha=0.5, linewidth=0.15, c='#d62728')\n    plt.legend()\n    return plt.show()\n\nplot_data(X_transformation, y)","a36721b7":"data[['Time', 'Amount']] = StandardScaler().fit_transform(data[['Time', 'Amount']])","62ee942b":"corr=data.corr(method='pearson')","730223d8":"plt.figure(figsize=(18, 18))\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(\n    corr, \n    xticklabels=corr.columns,\n    yticklabels=corr.columns, \n    cmap=\"coolwarm\", \n    annot=True, \n    fmt=\".2f\",\n    mask=mask, \n    vmax=.2, \n    center=0,\n    square=True, \n    linewidths=1, \n    cbar_kws={\"shrink\": .5})","c91b36e1":"# First train and label data created. \ntrain_data, label_data = data.iloc[:,:-1],data.iloc[:,-1]\n\n#Convert to matrix\ndata_dmatrix = xgb.DMatrix(data=train_data, label= label_data)","362aeeaa":"#Split data randomly to train and test subsets.\nX_train, X_test, y_train, y_test = train_test_split(\n                                    train_data, label_data, test_size=0.3,random_state=42)","b51c7356":"## Defining parameters\n\n#grid_param = {'n_estimators': [50, 100, 500],'max_depth': [4, 8], \n            #'max_features': ['auto', 'log2'], \n            #'criterion': ['gini', 'entropy'],\n            #'bootstrap': [True, False]}\n\n## Building Grid Search algorithm with cross-validation and F1 score.\n\n#grid_search = GridSearchCV(estimator=xg_class,  \n                     #param_grid=grid_param,\n                     #scoring='f1',\n                     #cv=5,\n                     #n_jobs=-1)\n\n## Lastly, finding the best parameters.\n\n#grid_search.fit(X_train, y_train)\n#best_parameters = grid_search.best_params_  \n#print(best_parameters)","cd85f829":"params = {\n    'objective':'reg:logistic',\n    'colsample_bytree': 0.3,\n    'learning_rate': 0.1,\n    'bootstrap': True, \n    'criterion': 'gini', \n    'max_depth': 4, \n    'max_features': 'auto', \n    'n_estimators': 50\n}\nxg_reg = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)\n\n#Feature importance graph\nplt.rcParams['figure.figsize'] = [20, 10]\nxgb.plot_importance(xg_reg)","c3164c0f":"data_model = data.drop(['V13', 'V25', 'Time', 'V20', 'V22', 'V8', 'V15', 'V19', 'V2'], axis=1)","f9c88abb":"data_under_nonfraud = data_model[data_model['Class'] == 0].sample(15000)\ndata_under_fraud  = data_model[data_model['Class'] == 1]\n\ndata_undersampling = data_under_nonfraud.append(data_under_fraud, \n                                                ignore_index=True, sort=False)","7ec99bc4":"plt.figure(figsize=(10,10))\nsns.countplot(y=\"Class\", data=data_undersampling,palette='Dark2')\nplt.title('Fraudulent Transaction Summary')\nplt.xlabel('Count')\nplt.ylabel('Fraudulent Transaction,        Non-Fraudulent Transaction')","07197174":"# New data will be split randomly to train and test subsets. Train data proportion is 70% and the test data proportion is 30%.\n\nmodel_train, model_label = data_undersampling.iloc[:,:-1],data_undersampling.iloc[:,-1]\nX_train, X_test, y_train, y_test = train_test_split(\n                                        model_train, model_label, test_size=0.3, random_state=42)","508325e0":"#5-fold Cross Validation method will be used.\n\nkfold_cv=KFold(n_splits=5, random_state=42, shuffle=True)\n\nfor train_index, test_index in kfold_cv.split(X,y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]","7ff2d94f":"# Define the model as the Random Forest\nmodelRF = RandomForestClassifier(\n    n_estimators=500, \n    criterion = 'gini', \n    max_depth = 4, \n    class_weight='balanced', \n    random_state=42\n).fit(X_train, y_train)\n\n# Obtain predictions from the test data \npredict_RF = modelRF.predict(X_test)","302cafa0":"# Define the model as the Support Vector Machine\nmodelSVM = svm.SVC(\n    kernel='rbf', \n    class_weight='balanced', \n    gamma='scale', \n    probability=True, \n    random_state=42\n).fit(X_train, y_train)\n\n# Obtain predictions from the test data \npredict_SVM = modelSVM.predict(X_test)","dab511bd":"# Define the model as the Logistic Regression\nmodelLR = LogisticRegression(\n    solver='lbfgs', \n    multi_class='multinomial',\n    class_weight='balanced', \n    max_iter=500, \n    random_state=42\n).fit(X_train, y_train)\n\n# Obtain predictions from the test data \npredict_LR = modelLR.predict(X_test)","ea81eaac":"# Define the model as the Multilayer Perceptron\nmodelMLP = MLPClassifier(\n    solver='lbfgs', \n    activation='logistic', \n    hidden_layer_sizes=(100,),\n    learning_rate='constant', \n    max_iter=1500, \n    random_state=42\n).fit(X_train, y_train)\n\n# Obtain predictions from the test data \npredict_MLP = modelMLP.predict(X_test)","44708b6d":"RF_matrix = confusion_matrix(y_test, predict_RF)\nSVM_matrix = confusion_matrix(y_test, predict_SVM)\nLR_matrix = confusion_matrix(y_test, predict_LR)\nMLP_matrix = confusion_matrix(y_test, predict_MLP) ","76aebd01":"fig, ax = plt.subplots(1, 2, figsize=(15, 8))\n\nsns.heatmap(RF_matrix, annot=True, fmt=\"d\",cbar=False, cmap=\"Paired\", ax = ax[0])\nax[0].set_title(\"Random Forest\", weight='bold')\nax[0].set_xlabel('Predicted Labels')\nax[0].set_ylabel('Actual Labels')\nax[0].yaxis.set_ticklabels(['Non-Fraud', 'Fraud'])\nax[0].xaxis.set_ticklabels(['Non-Fraud', 'Fraud'])\n\nsns.heatmap(SVM_matrix, annot=True, fmt=\"d\",cbar=False, cmap=\"Dark2\", ax = ax[1])\nax[1].set_title(\"Support Vector Machine\", weight='bold')\nax[1].set_xlabel('Predicted Labels')\nax[1].set_ylabel('Actual Labels')\nax[1].yaxis.set_ticklabels(['Non-Fraud', 'Fraud'])\nax[1].xaxis.set_ticklabels(['Non-Fraud', 'Fraud'])\n\n\nfig, axe = plt.subplots(1, 2, figsize=(15, 8))\n\nsns.heatmap(LR_matrix, annot=True, fmt=\"d\",cbar=False, cmap=\"Pastel1\", ax = axe[0])\naxe[0].set_title(\"Logistic Regression\", weight='bold')\naxe[0].set_xlabel('Predicted Labels')\naxe[0].set_ylabel('Actual Labels')\naxe[0].yaxis.set_ticklabels(['Non-Fraud', 'Fraud'])\naxe[0].xaxis.set_ticklabels(['Non-Fraud', 'Fraud'])\n\nsns.heatmap(MLP_matrix, annot=True, fmt=\"d\",cbar=False, cmap=\"Pastel1\", ax = axe[1])\naxe[1].set_title(\"Multilayer Perceptron\", weight='bold')\naxe[1].set_xlabel('Predicted Labels')\naxe[1].set_ylabel('Actual Labels')\naxe[1].yaxis.set_ticklabels(['Non-Fraud', 'Fraud'])\naxe[1].xaxis.set_ticklabels(['Non-Fraud', 'Fraud'])\n","4b5b1944":"print(\"Classification_RF:\")\nprint(classification_report(y_test, predict_RF))\nprint(\"Classification_SVM:\")\nprint(classification_report(y_test, predict_SVM))\nprint(\"Classification_LR:\")\nprint(classification_report(y_test, predict_LR))\nprint(\"Classification_MLP:\")\nprint(classification_report(y_test, predict_MLP))","d2b31180":"#RF AUC\nrf_predict_probabilities = modelRF.predict_proba(X_test)[:,1]\nrf_fpr, rf_tpr, _ = roc_curve(y_test, rf_predict_probabilities)\nrf_roc_auc = auc(rf_fpr, rf_tpr)\n\n#SVM AUC\nsvm_predict_probabilities = modelSVM.predict_proba(X_test)[:,1]\nsvm_fpr, svm_tpr, _ = roc_curve(y_test, svm_predict_probabilities)\nsvm_roc_auc = auc(svm_fpr, svm_tpr)\n\n#LR AUC\nlr_predict_probabilities = modelLR.predict_proba(X_test)[:,1]\nlr_fpr, lr_tpr, _ = roc_curve(y_test, lr_predict_probabilities)\nlr_roc_auc = auc(lr_fpr, lr_tpr)\n\n#MLP AUC\nmlp_predict_probabilities = modelMLP.predict_proba(X_test)[:,1]\nmlp_fpr, mlp_tpr, _ = roc_curve(y_test, mlp_predict_probabilities)\nmlp_roc_auc = auc(mlp_fpr, mlp_tpr)\n","d9fcb52e":"plt.figure()\nplt.plot(rf_fpr, rf_tpr, color='red',lw=2,\n         label='Random Forest (area = %0.2f)' % rf_roc_auc)\n\nplt.plot(svm_fpr, svm_tpr, color='blue',lw=2, \n         label='Support Vector Machine (area = %0.2f)' % svm_roc_auc)\n\nplt.plot(lr_fpr, lr_tpr, color='green',lw=2, \n         label='Logistic Regression (area = %0.2f)' % lr_roc_auc)\n\nplt.plot(mlp_fpr, mlp_tpr, color='orange',lw=2, \n         label='Multilayer Perceptron (area = %0.2f)' % mlp_roc_auc)\n\nplt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n","b5163110":"print(\"Average precision score of Logistic Regression\", average_precision_score(y_test, modelLR.predict_proba(X_test)[:,1]))\nprint(\"Average precision score of Random Forest\", average_precision_score(y_test, modelRF.predict_proba(X_test)[:,1]))\nprint(\"Average precision score of Multilayer Perceptron\", average_precision_score(y_test, modelMLP.predict_proba(X_test)[:,1]))","d0f2dcdf":"## **CONCLUSION**\n\nIn this project, I wanted to show how you can deal with imbalanced data and which performance metrics' particularly important compared to usual practices fairly balanced data. Also, I got a chance to use dimensionality reduction and expand on hyperparameter optimization techniques. I hope that this project can help other people who want to learn and work more about imbalanced data. \n\nAny comments and feedback are welcomed.","b001f5b9":"### **3.7. Neural Network - Multilayer Perceptron**","7291213a":"Another part is, ``Class`` structure will be converted to category and distribution of Time and Amount features will be examined. ","96b9acc0":"Above graph shows that the highest important feature is ``V16`` and this feature has a great difference with the second important one based on F score. Lowest importance parameters are ``V13``,``V25``,``Time``,``V20``,``V22``,``V8``,``V15``,``V19``, and ``V2``. These variables will be eliminated from data before model building. ","1bc5911e":"Below graphs show that the confusion matrix result of each ML algorithm. For imbalanced data, confusion matrix results can be incorrect. However, it is useful to say how many fraudulent transactions predicted correctly.\nBased on the graphs, Multilayer Perceptron, Random Forest and Logistic Regression models predict much the same Fraudulent transaction.  ","e12e8055":"Above graphs show that ``Time`` and ``Amount`` features needed to standardize.  Standardization  will be used to ``Time`` and ``Amount`` features for 0 mean and 1 std. This method preserves the shape of data and help to build features that have similar ranges to each other. \n\nBefore standardization, I want to create a feature namely ``Hour`` which will help to examine ``Time`` feature and its relationship with ``Class`` and ``Amount`` data in a better way. ","6efdfaa6":"Data exploration results and graphs show that feature size is big and class sizes imbalanced, so, dimensionality reduction helps to an interpretation of results easier. To achieve this, t-distributed stochastic neighbor embedding(t-SNE) method will be used. This method is one of the dimensionality reduction technique to make visualization in a low-dimensional space. Thus we can look details more smooth. This technique works well on high dimensional data and converts it to two- or three- dimensional spot.","467d4dad":"### ** 3.3. K-Fold Cross Validation Method**","f83ad103":"The correlation matrix shows that almost all parameters have no strong relationship with each other. The highest correlation is negative and 53%. These results shows that there is no need to take out any feature from model building on the ground of high correlation. ","97a5dbcd":"### ** 3.2. Data Splitting **","79bfa33a":"## **Introduction**\n\nThe aim of this project is detecting fraudulent or non-fraudulent transactions while dealing with imbalanced data. To achieve this, various supervised learning algorithms will be used and the results will be compared. \n\nImbalanced data refers to classification problems based on the binary class inequality. There are several methods for dealing with this problem like Re-Sampling, Generate Synthetic Samples, Anomaly Detection Methods or performance metrics instead of accuracy results. \n\nIn this project, the undersampling method will be implemented to the majority class and performance metrics such as Precision, Recall, F1 Score and AUC and some anomaly detection methods like one-class SVM and Neural Network will be used to find the best algorithm which highly predicted fraudulent or non-fraudulent transactions.\n\nThe project has 4 main topics:\n\n1. Data Exploration\n2. Hyperparameter Optimisation\n3. Model Building\n4. Comparing Performance Metrics","b1b95f41":"### ** 3.5. Support Vector Machine**","e38f2248":"### **Standardization**\nStandardization of Time and Amount features will be made. ","cfc9b50b":"Final comparing will be made with ROC Curve and AUC Score. ROC curve gives a good metric when the detection of both classes is equally important. With an AUC area, we can define the better classifier algorithm.","fa3c4ce5":"Above graph shows that fraudulent and non-fraudulent transactions aren't well separated into two different clusters in the two-dimensional space. This led to two types of transactions are very similar. Also, this graph demonstrates that accuracy results won't be enough for choosing the best algorithm. ","970bf6b0":"Based on the above ROC curve, we can say that, Logistic Regression, Random Forest  and Neural Network-Multilayer Perceptron algorithms have nearly similar AUC results. A great model has AUC near to the 1 which means it has a good measure of separability.\n\nThis conclusion can be demonstrated by ROC curve results as well. These algorithms leans towards True Positive Rate rather than False Positive Rate. As a result, we can say that these algorithms have better performance of classification. ","0a121e26":"## **4.Comparing Performance Metrics**\n\nIn this part, instead of accuracy results, other performance metrics will be compared. Because, the highest accuracy results in imbalanced data may be achieved from non-fraudulent transaction predictions, thus, the results can be misleading for predictive modeling.\n\nThe following metrics are in interest;\n\n* Confusion Matrix Fraud predictive part,\n* Precision,\n* Recall,\n* F1 Score and\n* AUC values \n\nBefore starting to compare, I would like to explain some performance metrics:\n\n* **Precision:** It explains, when the predicted value is 1, how often is it correct. \n* **Recall:** It explains, when the actual value is 1, how often does it predict 1.\n* **F1 Score:** It explains, the weighted average of the recall and precision.\n* **AUC:** It explains, which model predicts the best classification. . \n\nLastly, we can say that precision and recall are good metrics when the positive class is smaller. These metrics are good to detect positive samples accuratley.","f358ccdc":"Lastly, we can calculate, the average precision score for these 3 models. The results show all models have almost the same score.","4f2cfb26":"## **3.Model Building**\n\nIn this part, Random Forest, Support Vector Machine, Logistic Regression, and Multilayer Perceptron - Neural Network algorithms will be built.\n\n* Binary Support Vector Machine and Neural Network algorithms are one of the Anomaly Detection methods, therefore, they are chosen.\n* Since imbalance data have a predisposition to overfitting, Random Forest is one of the methods for preventing overfitting. For this reason, this method has been chosen. \n* Logistic Regression is one of the important models when the target variable is binary. \n\nIn this part, the important parameter is ``\"class_weight\"`` with balanced mode. It helps to adjust the model and this mode uses the values of y to automatically adjust weights. This adjustment method will help to get the best recall-precision trade-off.\n\nBefore the model building, the undersampling method will be applied. The output of this process will fed into model building phase.","6afaac20":"## **2. Hyperparameter Optimization **\n\nThis method helps to find the most optimal parameters for machine learning algorithms. It has crucial importance before proceeding to model training. The Grid Search algorithm will be used for the tuning hyperparameters. Then, XGBoost model will be built to achieve the feature importance graph. This graph helps to choose parameters which will be used on the training model. ","e71cc30b":"### **3.4. Random Forest**","e6640443":"As mention on the data information section, except ``Time``, ``Amount`` and ``Class`` features others can not interpret alone. And they don't give information about context. But we all know that features which are from ``V1`` to ``V28`` have been dimensionally reduction by PCA and no need to be standardized again. But the other features which we have meaning in that data, it can be expanded on. ","f15e3f70":"### **Pearson Correlation Matrix**\nThe final part is computing a correlation matrix by the Pearson method and analyze relationships between features.","f3a8a269":"### ** 3.1. Undersampling Method**\nOne of the most common ways of dealing with imbalanced data is undersampling method. This method helps to decrease the number of majority class. In this project, %5 out of non-fraudulent data have been chosen. ","a2a68c4c":"## **1. Data Exploratory**\n\nIn this part; the structure of the data, missing values, features distribution and the relationship between them and target value characteristics will be examined in detail.\n\nFirst data structure will be checked. ","5dafc8df":"Above graphs show that non-fraudulent and fraudulent transactions have been made in every hour. For the fraudulent transaction in third and twelfth hours have the highest record. On the other hand, after the eighth hour, non-fraudulent transaction counts are nearly close to each other.","a9a3f706":"Above graphs show fraudulent and non-fraudulent transactions' amounts at 1 hour granularity. Based on the error bar, the amount of variation of the non-fraudulent transaction in each hour is not widely. However, in the fraudulent transaction, some data points especially first, sixth, and eleventh hours, the range of amounts is visible large. This means that there is a high difference in the amount varies between upper and lower limits.","36f19034":"Based on the result of GridSearch parameters, parameters will be defined and XGBoost algorithm can build. ","e687489b":"### **3.6. Logistic Regression**","0f8a1496":"The graph and tables show that there is a huge difference between non-fraudulent and fraudulent data. This situation can interpretable as imbalanced data. \nImbalanced data can cause classification problems like incorrect high accuracy. There are some approaches to avoid imbalanced data like oversampling, undersampling or Synthetic Data Generation. But in this project, I will use the undersampling method and values of the majority class will be reduced. Then I will compare models by performance metrics. ","48300dda":"Above table shows, precision, recall, and F1-score results. \n\n* Logistic Regression model has the highest recall. This means that the Logistic Regression model has a better prediction of an actual fraudulent transaction as a fraudulent transaction. \n* However, when we look at the precision result, Logistic Regression is one of the lowest results. The highest one achieved with Random Forest. High precision relates to the low false positive rate, so we can say that Random Forest model predict the least false fraudulent transaction. \n* F1-Score gives a better explanation on the grounds that it is calculated from the harmonic mean of Precision and Recall. Especially, the highest recall and lower precision situations. F1 Score is mostly better metrics to choose the best-predicted model. In light of this information, we can say that Random Forest is the best-predicted algorithms in all models. "}}