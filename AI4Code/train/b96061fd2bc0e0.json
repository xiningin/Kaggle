{"cell_type":{"557b52c9":"code","b7d62f9f":"code","59086427":"code","66a55912":"code","ddb49059":"code","3388f31c":"code","89a7d672":"code","7787ba8c":"code","f5f8d56a":"code","30894649":"code","d6ecf214":"code","5743b0e3":"code","5595032e":"code","b5c6e7f1":"code","4b1d3c68":"code","260a0f03":"code","08f08b59":"code","e2253450":"code","c9d7c19f":"code","4553fa18":"code","4f0e2fbd":"code","53984bba":"code","ab1fe631":"code","1e714a46":"code","3aa19ff5":"code","dd765541":"code","9f2ae98f":"code","35d6b6ea":"code","0be861da":"code","05e1e129":"code","e94ac650":"code","9d5b2420":"code","3fca9b17":"code","e797139f":"code","66a50f6c":"code","4b53aaba":"code","e2834c4a":"code","a1d10233":"code","b13ce6ab":"code","b7c72389":"code","9d190d4a":"code","2aebbd52":"code","a9ae04b7":"code","406ae86c":"code","fa30bcb6":"code","ce1278d0":"code","d68eae40":"code","3ee55abf":"code","99fa486b":"code","c01ee401":"code","9d8ca9f5":"code","53ff96b7":"code","1d7252f6":"code","40c199bd":"code","cb44ca26":"code","360de327":"code","bca559d4":"code","f8277f6a":"code","47087905":"code","ed13194b":"code","6d181da0":"code","1e3cd512":"markdown","168a6392":"markdown","307d5c32":"markdown","98a4b1a6":"markdown","91aa2f5f":"markdown","c492ee46":"markdown","1e3e19e6":"markdown","e41afc73":"markdown"},"source":{"557b52c9":"import numpy as np # linear algebra\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('max_colwidth', None)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b7d62f9f":"train=pd.read_csv('\/kaggle\/input\/ames-housing-dataset\/AmesHousing.csv')\ntest=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain2=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')","59086427":"train.head()","66a55912":"train.columns = train.columns.str.replace(' ', '')\ntrain=train.rename(columns={\"YearRemod\/Add\": \"YearRemodAdd\"})","ddb49059":"train.head()","3388f31c":"test.head()","89a7d672":"train2.head()","7787ba8c":"print(\"Size of the Ames Dataset\",len(train))\nprint(\"Size of the Housing Dataset\",len(train2))\nprint(\"Size of the Housing Test Dataset\",len(test))","f5f8d56a":"data=pd.concat([train,train2,test], axis=0, sort=False)","30894649":"print(\"Size of the Housing Dataset\",len(data))","d6ecf214":"useless = ['Id','PID','Order','SalePrice'] \ndata = data.drop(useless, axis = 1)\n","5743b0e3":"duplicate = data[data.duplicated(keep='last')].index\nlen(duplicate)","5595032e":"duplicate[382:]","b5c6e7f1":"duplicate[390:]","4b1d3c68":"duplicate=duplicate[0:390]","260a0f03":"duplicate[386:]","08f08b59":"train = train.drop(duplicate, axis = 0)","e2253450":"print('Length of the Ames Dataset now',len(train))","c9d7c19f":"training=pd.concat([train,train2], axis=0, sort=False)","4553fa18":"useless = ['Id','PID','Order'] \ntraining = training.drop(useless, axis = 1)","4f0e2fbd":"from scipy.stats import norm\n(mu, sigma) = norm.fit(training['SalePrice'])\nplt.figure(figsize = (12,6))\nsns.distplot(training['SalePrice'], kde = True, hist=True, fit = norm)\nplt.title('SalePrice distribution vs Normal Distribution', fontsize = 13)\nplt.xlabel(\"House's sale Price in $\", fontsize = 12)\nplt.legend(['Normal dist ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma),'actual price dist'],loc='best')\nplt.show()","53984bba":"from scipy import stats\nshap = stats.shapiro(training['SalePrice'])\nprint('Skewness : %f' % abs(training['SalePrice']).skew())\nprint('Kurtosis : %f' % abs(training['SalePrice']).kurt())\nprint('Shapiro_Test_statistic : %f' % shap.statistic )\nprint('Shapiro_Test_pvalue : %f' % shap.pvalue )","ab1fe631":"f, ax = plt.subplots(figsize=(50, 35))\nmat = training.corr('pearson')\nmask = np.triu(np.ones_like(mat, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(mat, mask=mask, cmap=cmap, vmax=1, center=0, annot = True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","1e714a46":"# OverallQuall - SalePrice [Pearson = 0.8]\nfig,ax=plt.subplots(1,3,figsize=(20,10))\nsns.stripplot(data=training,x='OverallQual',y='SalePrice',ax=ax[1])\nsns.violinplot(data=training,x='OverallQual',y='SalePrice',ax=ax[2])\nsns.boxplot(data=training,x='OverallQual',y='SalePrice',ax=ax[0])\nplt.show()","3aa19ff5":"# OverallQuall - SalePrice [Pearson = -0.011\nfig,ax=plt.subplots(1,3,figsize=(20,10))\nsns.stripplot(data=train,x='BsmtFinSF2',y='SalePrice',ax=ax[1])\nsns.violinplot(data=train,x='BsmtFinSF2',y='SalePrice',ax=ax[2])\nsns.boxplot(data=train,x='BsmtFinSF2',y='SalePrice',ax=ax[0])\nplt.show()","dd765541":"# GrLivArea vs SalePrice [corr = 0.71]\n\nPearson_GrLiv = 0.71\nplt.figure(figsize = (12,6))\nsns.regplot(data=train, x = 'GrLivArea', y='SalePrice', scatter_kws={'alpha':0.2})\nplt.title('GrLivArea vs SalePrice', fontsize = 12)\nplt.legend(['$Pearson=$ {:.2f}'.format(Pearson_GrLiv)], loc = 'best')\nplt.show()","9f2ae98f":"# YearBuilt vs SalePrice\n\nPearson_YrBlt = 0.56\nplt.figure(figsize = (12,6))\nsns.regplot(data=train, x = 'YearBuilt', y='SalePrice', scatter_kws={'alpha':0.2})\nplt.title('YearBuilt vs SalePrice', fontsize = 12)\nplt.legend(['$Pearson=$ {:.2f}'.format(Pearson_YrBlt)], loc = 'best')\nplt.show()","35d6b6ea":"plt.figure(figsize=(15,10))\nsns.barplot(x='YrSold',y='SalePrice',data=train,estimator=np.median)\nplt.title('Median of Sale Price by Year')\nplt.xlabel('Year of Selling')\nplt.ylabel('Median of Price')\nplt.show()","0be861da":"# Separating Target and Features\n\ntarget = training['SalePrice']\ntest_id = test['Id']\ntest = test.drop(['Id'],axis = 1)\ntraining2 = training.drop(['SalePrice'], axis = 1)\n\n\n# Concatenating train & test set\n\ntrain_test = pd.concat([training2,test], axis=0, sort=False)","05e1e129":"len(train_test)","e94ac650":"nan=pd.DataFrame(train_test.isna().sum(),columns=['Nan_sum'])\nnan['feat']=nan.index\nnan=nan[nan['Nan_sum']>0]\nnan['Percentage']=(nan['Nan_sum']\/1460)*100\nnan['Percentage of total data']=(nan['Nan_sum']\/5459)*100\nnan=nan.sort_values(by=['Nan_sum'])\nnan.insert(0,'Serial No.',range(1,len(nan)+1))\nnan","9d5b2420":"plt.figure(figsize=(20,10))\nsns.barplot(x=nan['feat'],y=nan['Percentage'])\nplt.xticks(rotation=40)\nplt.title('Features Containing Nan')\nplt.xlabel('Features')\nplt.ylabel('% of Missing Data')\nplt.show()","3fca9b17":"# Converting non-numeric predictors stored as numbers into string\n\ntrain_test['MSSubClass'] = train_test['MSSubClass'].apply(str)\ntrain_test['YrSold'] = train_test['YrSold'].apply(str)\ntrain_test['MoSold'] = train_test['MoSold'].apply(str)\ntrain_test['OverallQual'] = train_test['OverallQual'].apply(str)\ntrain_test['OverallCond'] = train_test['OverallCond'].apply(str)","e797139f":"# Filling Categorical NaN (That we know how to fill due to the description file )\n\ntrain_test['Functional'] = train_test['Functional'].fillna('Typ')\ntrain_test['Electrical'] = train_test['Electrical'].fillna(\"SBrkr\")\ntrain_test['KitchenQual'] = train_test['KitchenQual'].fillna(\"TA\")\ntrain_test['Exterior1st'] = train_test['Exterior1st'].fillna(train_test['Exterior1st'].mode()[0])\ntrain_test['Exterior2nd'] = train_test['Exterior2nd'].fillna(train_test['Exterior2nd'].mode()[0])\ntrain_test['SaleType'] = train_test['SaleType'].fillna(train_test['SaleType'].mode()[0])\ntrain_test[\"PoolQC\"] = train_test[\"PoolQC\"].fillna(\"None\")\ntrain_test[\"Alley\"] = train_test[\"Alley\"].fillna(\"None\")\ntrain_test['FireplaceQu'] = train_test['FireplaceQu'].fillna(\"None\")\ntrain_test['Fence'] = train_test['Fence'].fillna(\"None\")\ntrain_test['MiscFeature'] = train_test['MiscFeature'].fillna(\"None\")\nfor col in ('GarageArea', 'GarageCars'):\n    train_test[col] = train_test[col].fillna(0)\n        \nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    train_test[col] = train_test[col].fillna('None')\n    \nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    train_test[col] = train_test[col].fillna('None')\n    \nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea','BsmtUnfSF', 'TotalBsmtSF'):\n    train_test[col] = train_test[col].fillna(0)\n\ntrain_test['LotFrontage'] = train_test['LotFrontage'].fillna(train['LotFrontage'].median())\n    \n    # Checking the features with NaN remained out\n\nfor col in train_test:\n    if train_test[col].isna().sum() > 0:\n        print(train_test[col][1])","66a50f6c":"train_test[\"SqFtPerRoom\"] = train_test[\"GrLivArea\"] \/ (train_test[\"TotRmsAbvGrd\"] +\n                                                       train_test[\"FullBath\"] +\n                                                       train_test[\"HalfBath\"] +\n                                                       train_test[\"KitchenAbvGr\"])\n\ntrain_test['Total_Home_Quality'] = train_test['OverallQual'] + train_test['OverallCond']\n\ntrain_test['Total_Bathrooms'] = (train_test['FullBath'] + (0.5 * train_test['HalfBath']) +\n                               train_test['BsmtFullBath'] + (0.5 * train_test['BsmtHalfBath']))\n\ntrain_test[\"HighQualSF\"] = train_test[\"1stFlrSF\"] + train_test[\"2ndFlrSF\"]\ntrain_test['renovated']=train_test['YearRemodAdd']+train_test['YearBuilt']","4b53aaba":"# Removing the useless variables\n\nuseless = ['GarageYrBlt','YearRemodAdd'] \ntrain_test = train_test.drop(useless, axis = 1)","e2834c4a":"# Creating dummy variables from categorical features\n\ntrain_test_dummy = pd.get_dummies(train_test)\nfrom scipy.stats import skew\nnumeric_features = train_test_dummy.dtypes[train_test_dummy.dtypes != object].index\nskewed_features = train_test_dummy[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skewed_features[skewed_features > 0.5]\nskew_index = high_skew.index","a1d10233":"# Normalize skewed features using log_transformation\n    \nfor i in skew_index:\n    train_test_dummy[i] = np.log1p(train_test_dummy[i] )","b13ce6ab":"nan=pd.DataFrame(train_test_dummy.isna().sum(),columns=['Nan_sum'])\nnan['feat']=nan.index\nnan=nan[nan['Nan_sum']>0]\nnan['Percentage']=(nan['Nan_sum']\/1460)*100\nnan['Perc']=(nan['Nan_sum']\/5459)*100\nnan=nan.sort_values(by=['Nan_sum'])\nnan.insert(0,'Serial No.',range(1,len(nan)+1))\nnan","b7c72389":"inf=pd.DataFrame(np.isinf(train_test_dummy).sum() ,columns=['Inf_sum'])\ninf['feat']=inf.index\ninf=inf[inf['Inf_sum']>0]\ninf=inf.sort_values(by=['Inf_sum'])\ninf.insert(0,'Serial No.',range(1,len(inf)+1))\ninf","9d190d4a":"import statsmodels.api as sm\n# SalePrice before transformation\n\nfig, ax = plt.subplots(1,2, figsize= (15,5))\nfig.suptitle(\" qq-plot & distribution SalePrice \", fontsize= 15)\n\nsm.qqplot(target, stats.t, distargs=(4,),fit=True, line=\"45\", ax = ax[0])\n#research sm \nsns.distplot(target, kde = True, hist=True, fit = norm, ax = ax[1])\nplt.show()","2aebbd52":"# SalePrice after transformation\n\ntarget_log = np.log1p(target)\n\nfig, ax = plt.subplots(1,2, figsize= (15,5))\nfig.suptitle(\"qq-plot & distribution SalePrice \", fontsize= 15)\n\nsm.qqplot(target_log, stats.t, distargs=(4,),fit=True, line=\"45\", ax = ax[0])\nsns.distplot(target_log, kde = True, hist=True, fit = norm, ax = ax[1])\nplt.show()","a9ae04b7":"import shap\nfrom xgboost import XGBRegressor\nfrom catboost import Pool\nfrom sklearn.svm import SVR\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeRegressor\nfrom mlxtend.regressor import StackingRegressor\nfrom sklearn.linear_model import LinearRegression, BayesianRidge\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error","406ae86c":"train_test.iloc[3999:4005]","fa30bcb6":"# Train-Test separation\n\nX_train = train_test_dummy[0:4000]\nX_test = train_test_dummy[4000:]\n\n# Creation of the RMSE metric:\n    \ndef rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model):\n    rmse = np.sqrt(-cross_val_score(model, X_train, target_log, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","ce1278d0":"nan=pd.DataFrame(X_train.isna().sum(),columns=['Nan_sum'])\nnan['feat']=nan.index\nnan=nan[nan['Nan_sum']>0]\nnan['Percentage']=(nan['Nan_sum']\/1460)*100\nnan['Perc']=(nan['Nan_sum']\/4000)*100\nnan=nan.sort_values(by=['Nan_sum'])\nnan.insert(0,'Serial No.',range(1,len(nan)+1))\nnan","d68eae40":"nan=pd.DataFrame(X_test.isna().sum(),columns=['Nan_sum'])\nnan['feat']=nan.index\nnan=nan[nan['Nan_sum']>0]\nnan['Percentage']=(nan['Nan_sum']\/1460)*100\nnan['Perc']=(nan['Nan_sum']\/2919)*100\nnan=nan.sort_values(by=['Nan_sum'])\nnan.insert(0,'Serial No.',range(1,len(nan)+1))\nnan","3ee55abf":"# 10 Fold Cross validation\n\nkf = KFold(n_splits=11, random_state=42, shuffle=True)\n\ncv_scores = []\ncv_std = []\n\nbaseline_models = ['Linear_Reg.','Bayesian_Ridge_Reg.','LGBM_Reg.','SVR',\n                   'Dec_Tree_Reg.','Random_Forest_Reg.', 'XGB_Reg.',\n                   'Grad_Boost_Reg.','Cat_Boost_Reg.','Stacked_Reg.','Stacked_Reg2']","99fa486b":"# Linear Regression\n\nlreg = LinearRegression()\nscore_lreg = cv_rmse(lreg)\ncv_scores.append(score_lreg.mean())\ncv_std.append(score_lreg.std())\n\n# Bayesian Ridge Regression\n\nbrr = BayesianRidge(compute_score=True)\nscore_brr = cv_rmse(brr)\ncv_scores.append(score_brr.mean())\ncv_std.append(score_brr.std())\n\n# Light Gradient Boost Regressor\n\nl_gbm = LGBMRegressor(objective='regression')\nscore_l_gbm = cv_rmse(l_gbm)\ncv_scores.append(score_l_gbm.mean())\ncv_std.append(score_l_gbm.std())\n\n# Support Vector Regression\n\nsvr = SVR()\nscore_svr = cv_rmse(svr)\ncv_scores.append(score_svr.mean())\ncv_std.append(score_svr.std())\n\n# Decision Tree Regressor\n\ndtr = DecisionTreeRegressor()\nscore_dtr = cv_rmse(dtr)\ncv_scores.append(score_dtr.mean())\ncv_std.append(score_dtr.std())\n\n# Random Forest Regressor\n\nrfr = RandomForestRegressor()\nscore_rfr = cv_rmse(rfr)\ncv_scores.append(score_rfr.mean())\ncv_std.append(score_rfr.std())\n\n# XGB Regressor\n\nxgb = XGBRegressor()\nscore_xgb = cv_rmse(xgb)\ncv_scores.append(score_xgb.mean())\ncv_std.append(score_xgb.std())\n\n# Gradient Boost Regressor\n\ngbr = GradientBoostingRegressor()\nscore_gbr = cv_rmse(gbr)\ncv_scores.append(score_gbr.mean())\ncv_std.append(score_gbr.std())\n\n# Cat Boost Regressor\n\ncatb = CatBoostRegressor()\nscore_catb = cv_rmse(catb)\ncv_scores.append(score_catb.mean())\ncv_std.append(score_catb.std())\n\n# Stacked Regressor\n\nstack_gen = StackingRegressor(regressors=(CatBoostRegressor(),\n                                          BayesianRidge()),\n                              meta_regressor = CatBoostRegressor(),\n                              use_features_in_secondary = True)","c01ee401":"score_stack_gen = cv_rmse(stack_gen)\ncv_scores.append(score_stack_gen.mean())\ncv_std.append(score_stack_gen.std())\n","9d8ca9f5":"# Stacked Regressor\n\nstack_gen2 = StackingRegressor(regressors=(CatBoostRegressor(),\n                                          XGBRegressor()),\n                              meta_regressor = CatBoostRegressor(),\n                              use_features_in_secondary = True)\n\nscore_stack_gen2 = cv_rmse(stack_gen2)\ncv_scores.append(score_stack_gen2.mean())\ncv_std.append(score_stack_gen2.std())\n\n","53ff96b7":"final_cv_score = pd.DataFrame(baseline_models, columns = ['Regressors'])\nfinal_cv_score['RMSE_mean'] = cv_scores\nfinal_cv_score['RMSE_std'] = cv_std","1d7252f6":"final_cv_score","40c199bd":"plt.figure(figsize = (12,8))\nsns.barplot(final_cv_score['Regressors'],final_cv_score['RMSE_mean'])\nplt.xlabel('Regressors', fontsize = 12)\nplt.ylabel('CV_Mean_RMSE', fontsize = 12)\nplt.xticks(rotation=40)\nplt.show()","cb44ca26":"cat = CatBoostRegressor()\ncat_model = cat.fit(X_train,target_log,\n                     plot=True,\n                     verbose = 0)","360de327":"feat_imp = cat_model.get_feature_importance(prettified=True)\nfeat_imp.head()","bca559d4":"# Plotting top 30 features' importance\n\nplt.figure(figsize = (12,8))\nsns.barplot(feat_imp['Importances'][:30],feat_imp['Feature Id'][:30], orient = 'h')\nplt.show()","f8277f6a":"params = {'iterations': 6000,\n          'learning_rate': 0.005,\n          'depth': 4,\n          'l2_leaf_reg': 1,\n          'eval_metric':'RMSE',\n          'early_stopping_rounds': 200,\n          'verbose': 200,\n          'random_seed': 42}\n         \ncat_f = CatBoostRegressor(**params)\ncat_model_f = cat_f.fit(X_train,target_log,\n                     plot=True,\n                     verbose = False)","47087905":"test_pred = cat_f.predict(X_test)\nsubmission = pd.DataFrame(test_id, columns = ['Id'])\ntest_pred = np.expm1(test_pred)\nsubmission['SalePrice'] = test_pred \nsubmission.head()\nsubmission.to_csv(\"cat.csv\", index = False, header = True)","ed13194b":"stack_f=stack_gen.fit(X_train,target_log)\ntest_stack = stack_gen.predict(X_test)\nsubmission = pd.DataFrame(test_id, columns = ['Id'])\ntest_pre = np.expm1(test_stack)\nsubmission['SalePrice'] = test_pre\n\nsubmission.to_csv(\"stack.csv\", index = False, header = True)","6d181da0":"stack_f2=stack_gen2.fit(X_train,target_log)\ntest_stack = stack_gen2.predict(X_test)\nsubmission = pd.DataFrame(test_id, columns = ['Id'])\ntest_pre = np.expm1(test_stack)\nsubmission['SalePrice'] = test_pre\n\nsubmission.to_csv(\"stack2.csv\", index = False, header = True)","1e3cd512":"checking for nan values in training set","168a6392":"Finding duplicates in data","307d5c32":"we want to delete duplicates till index 2902 which means only from the ames dataset","98a4b1a6":"transforming the sale price ","91aa2f5f":"checking if the values are in infinity or not after log transformation","c492ee46":"In literature, acceptable values for skewness are between -0.5 and 0.5 while -2 and 2 for Kurtosis. Looking at the plot, we can clearly see how the distribution does not seem to be normal, but highly right-skewed. The non-normality of our distribution is also supported by the Shapiro test for normality (p-value really small that allows us to reject the hypotesis of normality). Despite that, let's leave it like that for now, we'll deal with that later in the notebook.","1e3e19e6":"Checking for Nan values after dummy","e41afc73":"checking for nan values in test set"}}