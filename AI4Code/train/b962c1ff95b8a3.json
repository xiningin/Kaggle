{"cell_type":{"3a249b6e":"code","0d82e77c":"code","ef0ae0b6":"code","206d708e":"code","7bc83154":"code","47325447":"code","2e2b7a3e":"code","ca253033":"code","89e57d6a":"code","b84bc1f3":"code","8df1517f":"code","a9a78014":"code","006fa3ed":"code","bc461346":"code","b968716a":"code","08ed1c09":"code","4cd32b36":"code","133da0b6":"code","32c77e8f":"code","2a418b7e":"code","12badc36":"code","87cb2563":"code","50276f1d":"code","044baf8e":"code","18928d71":"code","9d386f7b":"code","d05d5557":"code","da5d11c2":"code","2a00cf5e":"code","756f6641":"code","757a190b":"code","3e46a656":"code","cbc8d304":"code","19294619":"code","85a87962":"code","553dbd15":"code","3fc959f0":"markdown","805d141a":"markdown","f786fa5c":"markdown","5a525c02":"markdown","6404c544":"markdown","1b9857f6":"markdown","ca4070a0":"markdown","17bb7ff6":"markdown","efc31d58":"markdown","72bee9b2":"markdown","68a1a00d":"markdown","188f1b05":"markdown","d8092f56":"markdown","73be090e":"markdown","fe4b183b":"markdown","dc05a1c4":"markdown","a8e40b7d":"markdown","8d3f151b":"markdown","340807ee":"markdown","3817d82d":"markdown","9f71520f":"markdown","2b21aec4":"markdown","2dafb5d8":"markdown","8feda4df":"markdown","905dac0e":"markdown","87a2ac85":"markdown","f5a4462a":"markdown","54c4179c":"markdown","81396348":"markdown","7ddfd840":"markdown","f2b75372":"markdown","6d735643":"markdown","07b4dad7":"markdown","1c37935a":"markdown","0cfc6f03":"markdown","bac08b2f":"markdown","6e624bf6":"markdown","f149a6c8":"markdown","39a47938":"markdown"},"source":{"3a249b6e":"import pandas as pd\n\n#Load the Ratings data\ndata = pd.read_csv('..\/input\/movielens-100k-dataset\/ml-100k\/u.data', sep=\"\\t\", header=None)\ndata.columns = ['user id', 'movie id', 'rating', 'timestamp']\ndata.head()","0d82e77c":"#Load the User data\nusers = pd.read_csv('..\/input\/movielens-100k-dataset\/ml-100k\/u.user', \n                    sep=\"|\", encoding='latin-1', header=None)\nusers.columns = ['user id', 'age', 'gender', 'occupation', 'zip code']\nusers.head()","ef0ae0b6":"#Load movie data\nitems = pd.read_csv('..\/input\/movielens-100k-dataset\/ml-100k\/u.item', \n                    sep=\"|\", encoding='latin-1', header=None)\nitems.columns = ['movie id', 'movie title' ,'release date','video release date', 'IMDb URL', \n                 'unknown', 'Action', 'Adventure', 'Animation', 'Children\\'s', 'Comedy', \n                 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', \n                 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nitems.head()","206d708e":"GENRES = pd.read_csv('..\/input\/movielens-100k-dataset\/ml-100k\/u.genre', \n                     sep=\"|\", header=None, usecols=[0])[0].tolist()\nGENRES","7bc83154":"print(\n    (f\"Number of users: {users['user id'].nunique()}\\n\" \n    f\"Nuber of movies: {items['movie id'].nunique()}\")\n)","47325447":"data['rating'].value_counts().sort_index().plot.bar()","2e2b7a3e":"users['age'].value_counts().sort_index().plot.bar(figsize=(12, 8))","ca253033":"users['gender'].value_counts().plot.bar()","89e57d6a":"users['occupation'].value_counts().plot.bar()","b84bc1f3":"dataset = data.sort_values(['user id', 'timestamp']).reset_index(drop=True)\ndataset['one'] = 1\ndataset['sample_num'] = dataset.groupby('user id')['one'].cumsum()\n\ndataset['target'] = dataset.groupby('user id')['movie id'].shift(-1)\ndataset['mean_rate'] = dataset.groupby('user id')['rating'].cumsum() \/ dataset['sample_num']\n\ndataset.head()","8df1517f":"dataset['prev movies'] = dataset['movie id'].apply(lambda x: str(x))\ndataset['prev movies'] = dataset.groupby('user id')['prev movies'].apply(lambda x: (x + ' ').cumsum().str.strip())\ndataset['prev movies'] = dataset['prev movies'].apply(lambda x: x.split())\ndataset.head()","a9a78014":"dataset = dataset.merge(items[['movie id'] + GENRES], on='movie id', how='left')\nfor genre in GENRES:\n    dataset[f'{genre}_rate'] = dataset[genre]*dataset['rating']\n    dataset[genre] = dataset.groupby('user id')[genre].cumsum()\n    dataset[f'{genre}_rate'] = dataset.groupby('user id')[f'{genre}_rate'].cumsum() \/ dataset[genre]\n\ndataset[GENRES] = dataset[GENRES].apply(lambda x: x \/ dataset['sample_num'])\ndataset.head()","006fa3ed":"dataset = dataset.merge(users, on='user id', how='left')\ndataset['gender'] = (dataset['gender'] == 'M').astype(int)\ndataset = pd.concat([dataset.drop('occupation', axis=1), pd.get_dummies(dataset['occupation'])], axis=1)\ndataset.drop('other', axis=1, inplace=True)\ndataset.drop('zip code', axis=1, inplace=True)\ndataset.head()","bc461346":"def get_coo_indexes(lil):\n    rows = []\n    cols = []\n    for i, el in enumerate(lil):\n        if type(el)!=list:\n            el = [el]\n        for j in el:\n            rows.append(i)\n            cols.append(j)\n    return rows, cols","b968716a":"from scipy.sparse import coo_matrix\nimport numpy as np\n\ndef get_sparse_features(series, shape):\n    coo_indexes = get_coo_indexes(series.tolist())\n    sparse_df = coo_matrix((np.ones(len(coo_indexes[0])), (coo_indexes[0], coo_indexes[1])), shape=shape)\n    return sparse_df","08ed1c09":"get_sparse_features(dataset['prev movies'], (len(dataset), dataset['movie id'].max()+1))","4cd32b36":"COLD_START_TRESH = 5\nTEST_SIZE = 0.2","133da0b6":"filtred_data = dataset[(dataset['sample_num'] >= COLD_START_TRESH) &\n                       ~(dataset['target'].isna())].sort_values('timestamp')\ntrain_data = filtred_data[:int(len(filtred_data)*(1-TEST_SIZE))]\ntest_data = filtred_data[int(len(filtred_data)*(1-TEST_SIZE)):]\ntrain_data.shape, test_data.shape","32c77e8f":"pd.concat([data['user id'].value_counts().describe(),\n           train_data['user id'].value_counts().describe(),\n           test_data['user id'].value_counts().describe()],\n         axis=1,\n         keys=['total', 'train', 'test'])","2a418b7e":"for df in [data, train_data, test_data]:\n    df.groupby('user id')['movie id'].count().plot.hist(bins=20)","12badc36":"print((\n    f\"Total movies: {data['movie id'].nunique()}\\n\"\n    f\"Movies in train: {train_data['movie id'].nunique()}\\n\"\n    f\"Movies in test: {test_data['movie id'].nunique()}\\n\"\n))  ","87cb2563":"X_train = train_data.drop(['user id', 'movie id', 'rating', 'timestamp', 'one', 'sample_num', 'target', 'prev movies'],\n                          axis=1)\nprev_movies_train = get_sparse_features(train_data['prev movies'], (len(train_data), dataset['movie id'].max()+1))\ny_train = train_data['target']\n\nX_test = test_data.drop(['user id', 'movie id', 'rating', 'timestamp', 'one', 'sample_num', 'target', 'prev movies'],\n                        axis=1)\nprev_movies_test = get_sparse_features(test_data['prev movies'], (len(test_data), dataset['movie id'].max()+1))\ny_test = test_data['target']","50276f1d":"import lightgbm as lgb\n\nparams = {\n    'objective': 'softmax',\n    'num_class': items['movie id'].nunique() + 1,\n    'num_iterations': 10,\n    'verbose': -1\n}\ntrain_data = lgb.Dataset(X_train.reset_index(drop=True), label=y_train, free_raw_data=False)\nmovies_data_train = lgb.Dataset(prev_movies_train, free_raw_data=False)\ntrain_data = train_data.construct()\nmovies_data_train = movies_data_train.construct()\ntrain_data = train_data.add_features_from(movies_data_train)\nmodel = lgb.train(params, train_data)","044baf8e":"test_data = lgb.Dataset(X_test.reset_index(drop=True), free_raw_data=False)\nmovies_data_test = lgb.Dataset(prev_movies_test, free_raw_data=False)\ntest_data = test_data.construct()\nmovies_data_test = movies_data_test.construct()\ntest_data = test_data.add_features_from(movies_data_test)\npreds_baseline = model.predict(test_data.get_data())\npreds_baseline.shape","18928d71":"def sparse_to_idx(data, pad_idx=-1):\n    indexes = data.nonzero()\n    indexes_df = pd.DataFrame()\n    indexes_df['rows'] = indexes[0]\n    indexes_df['cols'] = indexes[1]\n    mdf = indexes_df.groupby('rows').apply(lambda x: x['cols'].tolist())\n    max_len = mdf.apply(lambda x: len(x)).max()\n    return mdf.apply(lambda x: pd.Series(x + [pad_idx] * (max_len - len(x)))).values","9d386f7b":"def idx_to_sparse(idx, sparse_dim):\n    sparse = np.zeros(sparse_dim)\n    sparse[int(idx)] = 1\n    return pd.Series(sparse, dtype=int)","d05d5557":"import torch\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","da5d11c2":"# Train part\nPAD_IDX = 0\n# tensor with continious features\nX_train_tensor = torch.Tensor(X_train.fillna(0).values).to(device)\n# tensor with sequence of indexes\nmovies_train_tensor = torch.sparse_coo_tensor(\n    indices=prev_movies_train.nonzero(), \n    values=[1]*len(prev_movies_train.nonzero()[0]),\n    size=prev_movies_train.shape\n).to_dense().to(device)\n# tensor with binary features\nmovies_train_idx = torch.Tensor(\n    sparse_to_idx(prev_movies_train, pad_idx=PAD_IDX),\n).long().to(device)\n# target\ntarget_train = torch.Tensor(y_train.values).long().to(device)","2a00cf5e":"# tensor with continious features \nX_test_tensor = torch.Tensor(X_test.fillna(0).values).to(device)\n# tensor with continious features\nmovies_test_tensor = torch.sparse_coo_tensor(\n    indices=prev_movies_test.nonzero(), \n    values=[1]*len(prev_movies_test.nonzero()[0]),\n    size=prev_movies_test.shape\n).to_dense().to(device)\n# tensor with binary features\nmovies_test_idx = torch.Tensor(\n    sparse_to_idx(prev_movies_test, pad_idx=PAD_IDX),\n).long().to(device)\n# target\ntarget_test = torch.Tensor(y_test.values).long().to(device)","756f6641":"from torch import nn, cat, mean\n\nclass WideAndDeep(nn.Module):\n    def __init__(\n        self, \n        continious_feature_shape, # number of continious features\n        embed_size, # size of embedding for binary features\n        embed_dict_len, # number of unique binary features\n        pad_idx # padding index\n    ):\n        super(WideAndDeep, self).__init__()\n        self.embed = nn.Embedding(embed_dict_len, embed_size, padding_idx=pad_idx)\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(embed_size + continious_feature_shape, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU()\n        )\n        self.head = nn.Sequential(\n            nn.Linear(embed_dict_len + 256, embed_dict_len),\n        )\n\n    def forward(self, continious, binary, binary_idx):\n        # get embeddings for sequence of indexes\n        binary_embed = self.embed(binary_idx)\n        binary_embed_mean = mean(binary_embed, dim=1)\n        # get logits for \"deep\" part: continious features + binary embeddings\n        deep_logits = self.linear_relu_stack(cat((continious, binary_embed_mean), dim=1))\n        # get final softmax logits for \"deep\" part and raw binary features\n        total_logits = self.head(cat((deep_logits, binary), dim=1))\n        return total_logits","757a190b":"model = WideAndDeep(\n    X_train.shape[1], \n    16, \n    items['movie id'].nunique() + 1, \n    PAD_IDX\n).to(device)\nprint(model)","3e46a656":"EPOCHS = 10\nloss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nfor t in range(EPOCHS):\n    model.train()\n    pred_train = model(X_train_tensor, movies_train_tensor, movies_train_idx)\n    loss_train = loss_fn(pred_train, target_train)\n\n    # Backpropagation\n    optimizer.zero_grad()\n    loss_train.backward()\n    optimizer.step()\n\n    model.eval()\n    with torch.no_grad():\n        pred_test = model(X_test_tensor, movies_test_tensor, movies_test_idx)\n        loss_test = loss_fn(pred_test, target_test)\n    \n    print(f\"Epoch {t}\")\n    print(f\"Train loss: {loss_train:>7f}\")\n    print(f\"Test loss: {loss_test:>7f}\")","cbc8d304":"# mse\nfrom sklearn.metrics import mean_squared_error\n\ny_test_sparse = y_test.apply(lambda x: idx_to_sparse(x, items['movie id'].nunique() + 1))\nmse_baseline = mean_squared_error(y_test_sparse, preds_baseline)\nprint(f'Mean squared error for baseline: {mse_baseline:.4f}')","19294619":"loss = nn.MSELoss()\nsoftmax = nn.Softmax(dim=0)\nmse_wnd = loss(softmax(pred_test), torch.Tensor(y_test_sparse.values).to(device)).cpu().detach().numpy()\nprint(f'Mean squared error for Wide and Deep: {mse_wnd:.4f}')","85a87962":"# mean rank\nfrom scipy.stats import rankdata\n\nranks = pd.DataFrame(preds_baseline).apply(lambda x: pd.Series(rankdata(-x)), axis=1)\nranks_target = (ranks.values * y_test_sparse).sum(axis=1)\nmean_rank_baseline = ranks_target.mean()\nprint(f'Mean rank for baseline: {mean_rank_baseline:.0f}')","553dbd15":"preds_wnd = softmax(pred_test).cpu().detach().numpy()\nranks_wnd = pd.DataFrame(preds_wnd).apply(lambda x: pd.Series(rankdata(-x)), axis=1)\nranks_target_wnd = (ranks_wnd.values * y_test_sparse).sum(axis=1)\nmean_rank_wnd = ranks_target_wnd.mean()\nprint(f'Mean rank for Wide and Deep: {mean_rank_wnd:.0f}')","3fc959f0":"# Data split","805d141a":"To ensure Wide and Deep network is capable of solving recommendation task I compare it with baseline","f786fa5c":"The data which I choose for implementing this architecture is Movie Lens 100k dataset. It has some key advantages:\n- Popular. I bet you are already know or at leats hear about it\n- Simple. Just user rates for number of movies and a bit of meta information\n- Variative. It allows to construct binary features like previous watched films as long as some continious features important for deep part of network\n- Small size. It has only 100K rates and limited number of users and features, so the traing part won\\`t take to long \n\nAnd also one major drawback:\n- Tre dataset has no information to generate cross-product of userinstalled apps and impression apps as in original paper","5a525c02":"It\\`s time to define a target for this data. I choose to predict next watched movie. Also I use user mean rate as a feature","6404c544":"# Wide and Deep Learning for RecSys with Pytorch","1b9857f6":"And also I need continious features. Firstly I use movie meta information to generate features like user mean rate by genre and share of user watched movies by genre","ca4070a0":"And now define Wide and Deep architecture as a pytorch class","17bb7ff6":"Let\\`s take look at data a bit closer","efc31d58":"I need to define two functions:\n- First sparse_to_idx helps me to convert indexes of previous watched movies to series of films indexes. Also I pad this data with zeroes so I can later use it in embedding layer\n- Second is reverse to first idx_to_sparse helps me to convert target with index of movie to series of all zeros and one in place of index. I will use it later","72bee9b2":"# Features and target","68a1a00d":"Let\\`s train the network","188f1b05":"The users are mostly aged from 20 to 30","d8092f56":"So there is my implementatation and it seems to be working despite all limitations. Thumbs up!","73be090e":"## Simple basline","fe4b183b":"Now I construct input tensors for network. I need three tensors:\n- The tensor with continious features\n- The tensor with previous wathched films as sequence of indexes to feed into embedding layer\n- The tensor with previous wathched films as binary features","dc05a1c4":"I use multiclass LightGBM model with no parameters tuning as a baseline","a8e40b7d":"We have at least 5 films for each user in train. Movie count distribution in train reflects movie count distribution in total dataset","8d3f151b":"Movies are often rated as 3 or 4 stars from five","340807ee":"Not suprizingly the most popular occupatation for so young users is student","3817d82d":"# EDA","9f71520f":"This notebook was inspired by \"Wide & Deep Learning for Recommender Systems\" [paper](https:\/\/arxiv.org\/pdf\/1606.07792.pdf) by Google. In this paper authors propose an interesting NN arcitecture for Recommender Systems  \n![](https:\/\/miro.medium.com\/max\/875\/1*1jA7Qt71aMK_qG89tfUOoA.png)  \nI was strugguling to find realization of this arcitecture, so I decided to implement my own using Pytorch","2b21aec4":"Additional information about movie such as title, release date and genre","2dafb5d8":"All but 71 movies present in train data. So it won\\`t be possible to recommend them","8feda4df":"Let`s look how was the data splited between train and test","905dac0e":"# Data loading","87a2ac85":"## The Wide and Deep architecture","f5a4462a":"There are more than to male users for each female in this dataset","54c4179c":"The first metric I look at is Mean Squared Error\n  \nAs you see my implementation is twice better than baseline","81396348":"There are total 943 users and 1682 movies","7ddfd840":"Finaly, let\\`s get to the wide and deep network architecture\n  \nIn the original paper the cross-product of user installed apps and impression apps. As long as we don\\`t have any impressions working with movie lens data I use only information about previous watched filmes as features for wide component","f2b75372":"The next kind of features I need for wide and deep architecture is \"user history\" features, so I keep the list of previously watched films for every new film that user rated","6d735643":"Finaly I transform list of previous watched films to sparse format. For that I use scipy COO matrix","07b4dad7":"Secondly I use user meta information to generate features gender and one-hot encoded occupation","1c37935a":"Additional information about each user such as age, gender, occupation and zip code","0cfc6f03":"There is train test split in data provided by authors. But I won\\`t use it because it ignores timestamp. Otherwize I split data based on time label","bac08b2f":"**The data consist of:**  \nInformation about when and how user rated a movie","6e624bf6":"The second metric I look at is the mean rank next movie has in recommendations\n  \nBaseline puts it on a shy 841 place as the Wide and deep does 200 places better!","f149a6c8":"The list of all genres represented in dataset","39a47938":"## Compare metrics"}}