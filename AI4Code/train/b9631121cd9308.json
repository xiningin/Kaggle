{"cell_type":{"b006df35":"code","1aa2d7f2":"code","e96836bd":"code","859fbd24":"code","3ddf2954":"code","af790ffc":"code","1959a973":"code","2c2a11df":"code","875184c3":"code","39b94ed0":"code","d71d52cd":"code","c09ff14d":"code","d393b528":"code","20420cf4":"code","d0f6423d":"code","687b6ac6":"code","f43d4e19":"code","72dd3919":"code","90816956":"code","fe7cb0ca":"code","e125d3f2":"code","bbbee06b":"code","19a522b9":"markdown","0e57dd57":"markdown","0dbe71f2":"markdown","477be8d1":"markdown","6bcaf27b":"markdown","6d7443ae":"markdown","5f9aa687":"markdown","a710bc7d":"markdown","19526fbe":"markdown","453242d9":"markdown","6f6faf4a":"markdown","277f6482":"markdown","5ee0fc91":"markdown","f8c6da3e":"markdown","a0c7ccde":"markdown","46f9519e":"markdown","63675470":"markdown"},"source":{"b006df35":"## importing packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport tensorflow.keras.datasets.mnist as MNIST\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport warnings","1aa2d7f2":"# checking GPU availability\ncuda = torch.cuda.is_available()\nprint(\"GPU available:\", cuda)","e96836bd":"# setting the random seed\ntorch.manual_seed(1102)\nnp.random.seed(1102)","859fbd24":"# downloading the MNIST dataset\n(image_MNIST_train_set, label_MNIST_train_set), (image_MNIST_test_set, label_MNIST_test_set) = MNIST.load_data()","3ddf2954":"'''\nTitle:  Implementing a MLP and LeNet for MNIST classification\nAuthor: Soroush Arabshahi\nDate: 2021\nCode Version:\nAvailability: https:\/\/www.kaggle.com\/soroush361\/aoe-bme-lecture3-partc-mlp-lenetformnist\n'''\n#splitting the training set into a new training set and validation set by a ratio of 20%\nindices_train, indices_validation = train_test_split(range(len(image_MNIST_train_set)), test_size = 0.2)\nimage_train = image_MNIST_train_set[indices_train, :, :]\nlabel_train = label_MNIST_train_set[indices_train]\nimage_validation = image_MNIST_train_set[indices_validation, :, :]\nlabel_validation = label_MNIST_train_set[indices_validation]\n\nimage_test = image_MNIST_test_set\nlabel_test = label_MNIST_test_set\n","af790ffc":"# printing the number of samples, min and max intensity, and shape of the images for the sets\nimages = [image_train, image_validation, image_test]\nlabels = [label_train, label_validation, label_test]\nsets = [\"training set\",\"validation set\",\"test set\"]\n\nfor i in range(3):\n    print('The number of samples in the {s} is {num}'.format(s = sets[i], num = len(images[i])))\n    print('The number of labels in the {s} is {num}'.format(s = sets[i], num = len(labels[i])))\n    print('The minimum intensity in the {s} is {min}'.format(s = sets[i], min = images[i].min()))\n    print('The maximum intensity in the {s} is {max}'.format(s = sets[i], max = images[i].max()))\n    print('The shape of the matrix for the {s} is {shape}'.format(s = sets[i], shape = images[i].shape))\n    print()","1959a973":"\n# displays the first 24 samples from each set with true labels\n\nrows = 4\ncolumns = 6\n\nfor set_name, images, labels in zip(['Train', 'Validation', 'Test'], [image_train, image_validation, image_test], [label_train, label_validation, label_test]):\n    plt.figure(figsize=(12,8))\n    plt.suptitle(f'First 24 samples of {set_name} set')\n    for i in range(rows):\n        for j in range(columns):\n            idx = i*columns+j\n            plt.subplot(rows, columns, idx+1)\n            plt.title(f'True label is {labels[idx]}')\n            plt.imshow(images[idx], cmap='gray')\n            plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n    ","2c2a11df":"# reformats samples and creates a dataloaders of batch size 10 for each set\nimage_train_torch = torch.from_numpy(image_train).type(torch.FloatTensor).view(-1, 1, 28, 28)\nlabel_train_torch = torch.from_numpy(label_train).type(torch.LongTensor)\n\nimage_validation_torch = torch.from_numpy(image_validation).type(torch.FloatTensor).view(-1, 1, 28, 28)\nlabel_validation_torch = torch.from_numpy(label_validation).type(torch.LongTensor)\n\nimage_test_torch = torch.from_numpy(image_test).type(torch.FloatTensor).view(-1, 1, 28, 28)\nlabel_test_torch = torch.from_numpy(label_test).type(torch.LongTensor)\n\ntrain_data = TensorDataset(image_train_torch, label_train_torch)\ntrain_loader = DataLoader(train_data, batch_size = 100)\n\nvalidation_data = TensorDataset(image_validation_torch, label_validation_torch)\nvalidation_loader = DataLoader(validation_data, batch_size = 100)\n\ntest_data = TensorDataset(image_test_torch, label_test_torch)\ntest_loader = DataLoader(test_data, batch_size = 100)\n\nprint('Number of batches: train %d, validation %d, test %d' % \\\n      (len(train_loader), len(validation_loader), len(test_loader)))","875184c3":"'''\nTitle:  Implementing a MLP and LeNet for MNIST classification\nAuthor: Soroush Arabshahi\nDate: 2021\nCode Version:\nAvailability: https:\/\/www.kaggle.com\/soroush361\/aoe-bme-lecture3-partc-mlp-lenetformnist\n'''\n# defines a LeNet model\nclass LeNetModel(nn.Module):\n    \n    def __init__(self):\n        '''Define model modules.'''\n        super(LeNetModel, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 5, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.conv3 = nn.Conv2d(64, 128, 2, 1)\n        self.fc1 = nn.Linear(128*2*2, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        '''Define the model architecture (the sequence to place the model modules).'''\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv3(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 128*2*2)\n        x = F.relu(self.fc1(x))\n        x = F.sigmoid(self.fc2(x))\n        x = self.fc3(x)\n        return F.log_softmax(x, dim = 1)\n\nour_CNN = LeNetModel()\n\nour_CNN.cuda()\n\nprint(our_CNN)","39b94ed0":"# defines an SGD optimizer with learning rate of 0.001\noptimizer = SGD(our_CNN.parameters(), lr = 0.001)","d71d52cd":"# creates folder to save trained model\n!mkdir saved_models_CNN","c09ff14d":"\n# trains the network on a train set for 50 epochs\nEPOCHS = 50\n\ntrain_epoch_loss = list()\nvalidation_epoch_loss = list()\n\nfor epoch in range(EPOCHS):\n\n    train_loss = list()\n\n    our_CNN.train()\n    for batch_index, (train_image, train_label) in enumerate(train_loader):\n\n        train_label_predicted = our_CNN(train_image.cuda())\n\n        loss = F.cross_entropy(train_label_predicted, train_label.cuda())\n        train_loss.append(loss.cpu().data.item())\n\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        print(f'Epoch {epoch+1}\/{EPOCHS} - Train Batch {batch_index+1}\/{len(train_loader)} - Loss = {train_loss[-1]}', end='\\r')\n    \n    train_epoch_loss.append(np.mean(train_loss))\n\n    validation_loss = list()\n\n    our_CNN.eval()\n    for batch_index, (validation_image, validation_label) in enumerate(validation_loader):\n\n        validation_label_predicted = our_CNN(validation_image.cuda())\n\n        loss = F.cross_entropy(validation_label_predicted, validation_label.cuda())\n        validation_loss.append(loss.cpu().data.item())\n        \n        print(f'Epoch {epoch+1}\/{EPOCHS} - Validation Batch {batch_index+1}\/{len(validation_loader)} - Loss = {validation_loss[-1]}', end='\\r')\n    \n    validation_epoch_loss.append(np.mean(validation_loss))\n    \n    torch.save(our_CNN.state_dict(), '.\/checkpoint_epoch_%s.pth' % (epoch))\n    \n    print(f'Epoch {epoch+1}\/{EPOCHS} - Train Loss = {train_epoch_loss[-1]:.10f} - Validation Loss = {validation_epoch_loss[-1]:.10f}')\n    ","d393b528":"# plots the learning curve\nplt.figure(figsize = (12, 8))\nplt.plot(train_epoch_loss, '-o', label = 'training loss', markersize = 3)\nplt.plot(validation_epoch_loss, '-o', label = 'validation loss', markersize = 3)\nplt.legend(loc = 'upper right');\n","20420cf4":"# saves epoch with lowest validation loss\nbest_epoch = np.argmin(validation_epoch_loss)\nprint('best epoch: ', best_epoch)","d0f6423d":"'''\nTitle:  Implementing a MLP and LeNet for MNIST classification\nAuthor: Soroush Arabshahi\nDate: 2021\nCode Version:\nAvailability: https:\/\/www.kaggle.com\/soroush361\/aoe-bme-lecture3-partc-mlp-lenetformnist\n'''\n# loads the weights of the best epochs\nstate_dict = torch.load('.\/checkpoint_epoch_%s.pth' % (best_epoch))\nprint(state_dict.keys())\nour_CNN.load_state_dict(state_dict)","687b6ac6":"# test the model on the test set\ntest_loss = []\ntest_true_labels = []\ntest_pred_labels = []\n\nour_CNN.eval()\nfor batch_index, (test_image, test_label) in enumerate(test_loader):\n\n    test_label_predicted = our_CNN(test_image.cuda())\n    \n    test_true_labels.extend(list(test_label.numpy()))\n    test_pred_labels.extend(list(np.argmax(test_label_predicted.cpu().data.numpy(), axis=-1)))\n\n    loss = F.cross_entropy(test_label_predicted, test_label.cuda())\n    test_loss.append(loss.cpu().data.item())\n\n    print(f'Test Batch {batch_index+1}\/{len(test_loader)} - Loss = {test_loss[-1]}', end='\\r')\n\ntest_epoch_loss = np.mean(validation_loss)\n\ntest_true_labels = np.array(test_true_labels)\ntest_pred_labels = np.array(test_pred_labels)\n\nprint(f'\\nOverall test loss = {test_epoch_loss} - Number of samples {len(test_true_labels)}')","f43d4e19":"'''\nTitle:  Implementing a MLP and LeNet for MNIST classification\nAuthor: Soroush Arabshahi\nDate: 2021\nCode Version:\nAvailability: https:\/\/www.kaggle.com\/soroush361\/aoe-bme-lecture3-partc-mlp-lenetformnist\n'''\n# Function for viewing an image and it's predicted classes.\ndef view_classify(image, probabilities, version = \"MNIST\"):\n    ''' Function for viewing an image and it's predicted classes.\n    '''\n    probabilities = probabilities.data.numpy().squeeze()\n\n    fig, (ax1, ax2) = plt.subplots(figsize = (6, 9), ncols = 2)\n    ax1.imshow(image.resize_(1, 28, 28).numpy().squeeze(), cmap = 'gray')\n    ax1.set_title('Original Image')\n    ax1.axis('off')\n    ax2.bar(np.arange(10), probabilities)\n    ax2.set_aspect(10)\n    ax2.set_xticks(np.arange(10))\n        \n    ax2.set_title('Class Probability')\n    ax2.set_ylim(0, 1.1)\n\n    plt.tight_layout()","72dd3919":"image_test_torch.shape","90816956":"'''\nTitle:  Implementing a MLP and LeNet for MNIST classification\nAuthor: Soroush Arabshahi\nDate: 2021\nCode Version:\nAvailability: https:\/\/www.kaggle.com\/soroush361\/aoe-bme-lecture3-partc-mlp-lenetformnist\n'''\n# makes prediction on sample label\nour_CNN.eval()\n\nsample_test_image = image_test_torch[0, :, :, :][np.newaxis, :, :, :]\n\nif cuda:\n    sample_prediction = our_CNN(sample_test_image.cuda())\nelse:\n    sample_prediction = our_CNN(sample_test_image)","fe7cb0ca":"'''\nTitle:  Implementing a MLP and LeNet for MNIST classification\nAuthor: Soroush Arabshahi\nDate: 2021\nCode Version:\nAvailability: https:\/\/www.kaggle.com\/soroush361\/aoe-bme-lecture3-partc-mlp-lenetformnist\n'''\n# tests the the model on the first sample of the test set and plots the output probabilities\nwarnings.filterwarnings(\"ignore\")\n\nif cuda:\n    view_classify(sample_test_image, 2 ** sample_prediction.cpu())\nelse:\n    view_classify(sample_test_image, 2 ** sample_prediction)","e125d3f2":"# calculates test accuracy\ntest_accuracy = np.sum(test_true_labels == test_pred_labels)\/len(test_true_labels)\n\nprint(f'Overall test accuracy is {test_accuracy:.3%}')","bbbee06b":"'''\nTitle:  Implementing a MLP and LeNet for MNIST classification\nAuthor: Soroush Arabshahi\nDate: 2021\nCode Version:\nAvailability: https:\/\/www.kaggle.com\/soroush361\/aoe-bme-lecture3-partc-mlp-lenetformnist\n'''\n# plots confusion matrix for test set\nCM = confusion_matrix(test_true_labels, test_pred_labels)\n\nplt.figure(figsize = (12,10))\nsns.heatmap(CM, annot = True, annot_kws = {\"size\": 10})\nplt.ylim([0, 10]);\nplt.ylabel('True labels');\nplt.xlabel('predicted labels');","19a522b9":"<span style=\"color:red\">Your answer here<\/span>","0e57dd57":"#### <span style=\"color:red\">(8 pts.) Q3. <\/span> \n#### a. Similar to the lecture, reformat all samples and create a `DataLoader` of batch size 100 for each set. \n#### b. Print the total number of batches in each `DataLoader`. (You can use `len()` function on `DataLoader`)\n**Note:** Do not forget to add one axis for channels. The Pytorch's tensor format is (BatchSize, Channel, Width, Height).","0dbe71f2":"#### <span style=\"color:red\">(10 pts.) Q5. <\/span> Knowing the input shape of each image in the MNIST dataset is $28\\times28$, write the shape of the output tensor for each layer of the model.\n**Note:** You can use the following formula: \\\nAssuming the input shape is: $(B, Ch_{in}, W_{in}, H_{in})$, and the output shape is $(B, Ch_{out}, W_{out}, H_{out})$ \\\n1. For the `Conv2D` layers with $N$ number of $K\\times K$ kernels, zero-padding $P$, and stride $S$:\n$$\nW_{out} \\text{ or } H_{out} = \\lfloor {\\frac{W_{in} \\text{ or } H_{in} + 2P - K }{S}} + 1 \\rfloor\n$$\n$\\lfloor . \\rfloor$ means integer floor.\n\nAnd the number of channels for the output will be $Ch_{out} = N$\n\n2. Similarly for the `Maxpooling` layers of size $K \\times K$, zero-padding $P$, and stride $S$:\n$$\nW_{out} \\text{ or } H_{out} = \\lfloor {\\frac{W_{in} \\text{ or } H_{in} + 2P - K }{S}} + 1 \\rfloor\n$$\n\nThe only difference is that the number of channels for the output will be $Ch_{out} = Ch_{in}$\n\n3. For `Linear` (Also known as Fully-Connected) layers, the input is a vector of $(B, D_{in})$. Assuming the layer has $N$ number of neurons, the output is a vector of size $(B, N)$.\n\n**Note:** $B$ stands for the batch size.","477be8d1":"Jessica Zhang - jz3453","6bcaf27b":"#### <span style=\"color:red\">(12 pts.) Q4. <\/span> Using the lecture's notebook on LeNet, define a model as the following table:\n| Layer Number | Layer Type   | Number of kernels | Kernel size | Activation | Stride | Zero-Padding |\n| ------------ | ------------ | ----------------- | ----------- | ---------- | ------ | ------------ |\n| 1            | Conv2D       | 32                | 5x5         | ReLU       | 1      | 0            |\n| 2            | MaxPooling2D | NA                | 2x2         | NA         | 2      | 0            |\n| 3            | Conv2D       | 64                | 3x3         | ReLU       | 1      | 0            |\n| 4            | MaxPooling2D | NA                | 2x2         | NA         | 2      | 0            |\n| 5            | Conv2D       | 128               | 2x2         | ReLU       | 1      | 0            |\n| 6            | MaxPooling2D | NA                | 2x2         | NA         | 2      | 0            |\n| 7            | Faltten      | NA                | NA          | NA         | NA     | NA           |\n| 8            | FC (Linear)  | 128               | NA          | ReLU       | NA     | NA           |\n| 9            | FC (Linear)  | 64                | NA          | Sigmoid    | NA     | NA           |\n| 10           | FC (Linear)  | 10                | NA          | SoftMax    | NA     | NA           |\n**Note:** **NA** means not applicable.\n\n#### Then create an instance of the model and load it on the GPU by calling the `.cuda()` function.","6d7443ae":"<span style=\"color:red\">Your answer here<\/span>","5f9aa687":"#### <span style=\"color:red\">(25 pts.) Q7. <\/span> Similar to the example on LeNet, train your network on a train set for 50 epochs. Use the \"Cross-Entropy\" (`F.cross_entropy`) as the loss function. \n#### At the end of each epoch, save the model, test the model on the validation set, and keep the training and validation loss for later. \n**Note:** Remember that the loss is calculated for each batch; thus, you need to take the average loss over all batches as the epoch loss for both training and validation phases.","a710bc7d":"#### <span style=\"color:red\">(20 pts.) Q9. <\/span> \n#### a. Load the weights of the best epoch and test the model on the test set.\n#### b. Test the model on the first sample of the test set and plot the output probabilities.\n#### c. Print the overall accuracy of the model on the test set. Accuracy formula is $Accuracy=\\frac{\\text{Number of correct prediction}}{\\text{Total Number of samples}}$\n#### d. Plot the confusion matrix for the test set.\n\n**Note:** The testing phase is similar to the validation phase. You need to loop through batches of the test loader and keep the true labels and predicted labels for accuracy and confusion matrix.","19526fbe":"#### <span style=\"color:red\">(5 pts.) Q10. <\/span> Compare the performance of your architecture with the LeNet performance. Which one performs better on the MNIST classification? Can you suggest any method to improve either of the models?","453242d9":"#### <span style=\"color:red\">(5 pts.) Q8. <\/span> Plot the learning curve and indicate in which epoch the model achieved the lowest validation loss.\n**Note:** The learning curve is a plot that shows the training and validation loss for each epoch.","6f6faf4a":"#### <span style=\"color:red\">(5 pts.) Q2. <\/span> Display 24 first samples pulled from each set (train, validation, and test) and show the true labels as the title per each sample. \n**Note:** Display all 24 samples pulled from each set in one plot as in four rows and six columns. Thus, we will expect three plots for training, validation, and test set.","277f6482":"Input layer: (100, 1, 28, 28) \\\nConv2d 1: (100, 32, 24, 24) \\\nMax-pool 1: (100, 32, 12, 12) \\\nConv2d 2: (100, 64, 10, 10) \\\nMax-pool 2: (100, 64, 5, 5) \\\nConv2d 3: (100, 128, 4, 4) \\\nMax-pool 3: (100, 128, 2, 2) \\\nFlatten: (100,512) \\\nLinear 1: (100, 128) \\\nLinear 2: (100, 64) \\\nLinear 3: (100, 10) ","5ee0fc91":"#### <span style=\"color:red\">(5 pts.) Q6. <\/span> Define an SGD optimizer with a learning rate of $10^{-3}$ on the parameters of the model.","f8c6da3e":"# Art of Engineering\n## Biomedical Engineering Departmental Project\n\n### <span style=\"color:red\">Assignment 2<\/span> - MNIST Classification using LeNet\n\n#### Due date\/time: <span style=\"color:red\">November 5, 11:59 pm<\/span>\n\n#### Instructions:\n1. You may use any publicly available resources to answer the questions, but you need to ***cite*** them properly to prevent plagiarism.\n2. Using or copying other students' solutions is considered cheating, and you'll be graded **\"0\" for the entire assignment**.\n3. You may be asked to write Python code or to explain something in each question. To write your answers, please use code and markdown blocks, respectively. If you need to better organize your answers, you may add more blocks. In this notebook, we placed a code block with a comment as `# [Your code here]` and a markdown block with the text \"<span style=\"color:red\">Your answer here<\/span>\" for each question that needs them. If you wish to answer, please remove the comments first.\n4. Please ***use comments at the beginning of each code blocks*** to explain what you've implemented in that block.\n5. Please ***use LaTex formatting to write equations and formulas*** in markdown blocks wherever is needed.\n6. Please define your variables with ***short and meaningful names***.\n7. Please make sure ***internet access is granted*** on the settings panel.\n8. For this assignment, you ***need GPU access***. Please set the \"Accelerator\" as \"GPU\" on the \"Setting\" panel.\n\n#### How to submit:\nKaggle automatically saves the notebook after few seconds, and you may close the notebook and come back later to complete it. However, we need you to do the following steps to ensure your answers are visible to us for grading purposes. Thus, after you finished your answers, please:\n1. Rename the notebook as \"AoE_BME_Assignment2\".\n2. Click on the \"Save Version\" button on the top-right side of the window.\n3. On the save popup, save your notebook with: \\\n    3.1 If the scripts run fast, the \"Save & Run All (Commit)\" option, \\\n    3.2 If the scripts run slow, such as training networks, etc., and you are satisfied with the outputs of code blocks, \"Quick Save\" option.\n4. Click on the \"Save\" button and let the Kaggle saves your notebook.\n5. Then click on the \"Share\" button.\n6. On the share popup, change \"Private\" to \"Public\".\n7. Copy the \"Public url\", and click on the \"Save\" button.\n8. Paste the \"Public url\" address on the related assignment on the [CourseWorks](https:\/\/courseworks2.columbia.edu\/) and submit it.\n\n**Note:**\n1. You may save multiple versions of your notebook, but the latest version is considered as your final answer.\n2. If you wish us to grade your answers based on a different version of your notebook other than the latest version, please duplicate this template, copy your answers from your desired version, and follow the same procedure to submit. Again, make sure the name of the notebook remains as the current template.\n3. Do not change or save the notebook after the due date\/time as it will be considered a late submission and causes a decrease in your final grade.","a0c7ccde":"#### <span style=\"color:red\">(5 pts.) Q1. <\/span> \n#### a. Download the MNIST dataset using MNIST package. (Make sure the images and labels for both training set and test set are downloaded correctly)\n#### b. Using `train_test_split` function, split training set into a new training set and validation set by a ration of 20%.\n#### c. Print the number of samples, minimum and maximum intensity, and the shape of the images matrix for all three sets.","46f9519e":"My LeNet model performs better on the MNIST classification, as it has 98.54% accuracy rate, compared to the 97.2% accuracy rate of the LeNet5 model. To reduce over-fitting in the models, we could use regularization dropout to update the learning of weights over each iteration and improve generalization. For my model, increasing the number of epochs might improve the performance of the model, because it appears that the validation loss is still decreasing, and the best epoch has yet to be reached.","63675470":"#### <span style=\"color:red\">(0 pts.) Q0. <\/span> \n#### a. Import the packages you need similar to the notebook of the Lecture 3 - Part C. \n#### b. Check if the GPU is available.\n#### c. Set the random seed for both Pytorch and Numpy as 1102"}}