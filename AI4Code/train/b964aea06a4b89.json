{"cell_type":{"d7c6e6e3":"code","24a3809d":"code","5f07bd5a":"code","f425c5ce":"code","19429213":"code","e4a47115":"code","2d462fa4":"code","140d4917":"code","cc74e325":"code","e3b3784b":"code","740f66e8":"code","ab03adbe":"code","e76952ad":"code","4892f996":"code","a553d03e":"code","22443a7c":"code","dc478204":"code","cb07349b":"code","58cbfc10":"code","2a06c3d8":"code","dd15b6a2":"code","b6e60bf0":"code","3e69787b":"code","0cd937a9":"code","7bff857a":"code","761509b8":"code","fca24044":"code","cb9e54a0":"code","8aebf996":"code","c0385f19":"code","fbd54d9e":"code","3107bc9d":"code","9b2cfac6":"code","83020bd6":"code","9009d8b7":"code","d8eb03ea":"code","22486df3":"code","532fbf1b":"code","49f4151f":"code","472ba96a":"code","39ef518f":"code","030d9a14":"code","979ed843":"code","f38d8a22":"code","2800f498":"code","fe1b7e1a":"markdown","65362dd5":"markdown","b885f43e":"markdown","8f0708ed":"markdown","407c6bf6":"markdown","8c97dabc":"markdown","bced318d":"markdown","4f6e648e":"markdown","e8a7d5b6":"markdown","73bd04f5":"markdown","a48dd1c0":"markdown","6a6a7496":"markdown","6ad69ca8":"markdown","0780c88e":"markdown","04bda660":"markdown","802e5c9c":"markdown","ef53146c":"markdown","3a93ba5c":"markdown"},"source":{"d7c6e6e3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","24a3809d":"## import libraries\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nrcParams['figure.figsize'] = 12,7\nsns.color_palette(\"hls\", 8)","5f07bd5a":"df= pd.read_csv(\"\/kaggle\/input\/insurance\/insurance.csv\") # Reading dataset","f425c5ce":"df.head() # looking at the first 5 rows ","19429213":"df.shape # looking at the shape. we have 1338 rows and 7 columns","e4a47115":"df.info() # looking at information","2d462fa4":"df.describe().T # looking at statistical info","140d4917":"df.isnull().sum() # checking null values","cc74e325":"# plotting numerical features \n\nnum_variable = (df.dtypes==float) | (df.dtypes==\"int64\")\nnum_variable = df.columns[num_variable].tolist()\n\ndef plot_hist(train_df, variable):\n    plt.figure(figsize = (12,7))\n    plt.hist(train_df[variable], bins = 50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with hist\".format(variable))\n    plt.show()\n    \nfor i in num_variable:\n    plot_hist(df,i)","e3b3784b":"##plotting the categorical features\n\ncat_variable = df.dtypes==object\ncat_variable = df.columns[cat_variable].tolist()\n\n# Count of products per keys\ndef bar_plot(data,feature):\n    print(f'There are {len(set(data[feature]))} unique {feature}')\n    print('\\n')\n    sns.countplot(x = feature,\n              data = data,\n              order = data[feature].value_counts(ascending=False)[0:20].index)\n    plt.xticks(rotation=90)\n    print(f'Count of {feature}')\n    print('\\n')\n    print(data[feature].value_counts(ascending=False)[0:20])\n    plt.show()\n    print('\\n')\n    \nfor i in cat_variable:\n    bar_plot(df,i)","740f66e8":"sns.barplot(x='smoker', y='charges', hue='sex', data=df, palette='cool')","ab03adbe":"sns.barplot(x='children', y='charges', hue='sex', data=df, palette='viridis')","e76952ad":"df.groupby(\"age\")[[\"charges\"]].mean().sort_values(\"charges\", ascending = False)","4892f996":"# creating new feature by using age column\n\ndf[\"age_range\"] = 1000\nfor i in range(len(df[\"age\"])):\n    if df[\"age\"][i]<30:\n        df[\"age_range\"][i] = 1\n    elif df[\"age\"][i] >=30 and df[\"age\"][i]<45:\n        df[\"age_range\"][i] = 2\n    elif df[\"age\"][i] >=45:\n        df[\"age_range\"][i] = 3","a553d03e":"df","22443a7c":"df.groupby(\"age_range\")[[\"charges\"]].mean()","dc478204":"sns.barplot(x='age_range', y='charges', hue='children', data=df, palette='viridis')","cb07349b":"sns.barplot(x='region', y='charges', data=df, palette='viridis')","58cbfc10":"sns.barplot(x='region', y='charges', data=df, hue= \"age_range\" , palette='viridis')","2a06c3d8":"#creating new feature by using children \ndf[\"have_children\"] = [\"No\" if i == 0 else \"Yes\" for i in df[\"children\"]]","dd15b6a2":"df","b6e60bf0":"sns.barplot(x='have_children', y='charges', data=df , palette='viridis')","3e69787b":"cat_variable.append(\"have_children\") #Converting categorical variables to numeric variables","0cd937a9":"cat_variable","7bff857a":"from sklearn.preprocessing import LabelEncoder\nlb = LabelEncoder()\ndf[cat_variable] = df[cat_variable].apply(lambda col: lb.fit_transform(col.astype(str)))","761509b8":"df.head()","fca24044":"sns.heatmap(df.corr(),annot=True) #looking at correlation values","cb9e54a0":"X = df.drop(columns=[\"charges\",\"sex\"])\ny = df[\"charges\"]\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2)","8aebf996":"from sklearn.metrics import mean_squared_error","c0385f19":"def models():\n    #use logistic regression\n    from sklearn.linear_model import LinearRegression\n    lr = LinearRegression()\n    lr.fit(X_train,y_train)\n\n    #use Kneighbors\n    from sklearn.neighbors import KNeighborsRegressor\n    knn = KNeighborsRegressor()\n    knn.fit(X_train,y_train)\n\n    #use Support vector classifier (linear kernel)\n    from sklearn.svm import SVR\n    svc = SVR(kernel='linear')\n    svc.fit(X_train,y_train)\n\n    #use decision tree\n    from sklearn.tree import DecisionTreeRegressor\n    tree=DecisionTreeRegressor()\n    tree.fit(X_train,y_train)\n\n    #use Random Forest\n    from sklearn.ensemble import RandomForestRegressor\n    forest = RandomForestRegressor()\n    forest.fit(X_train,y_train)\n\n    #use GradientBoosting\n    from sklearn.ensemble import GradientBoostingRegressor\n    gb = GradientBoostingRegressor()\n    gb.fit(X_train,y_train)\n    \n    \n    from xgboost import XGBRegressor\n    xgb = XGBRegressor()\n    xgb.fit(X_train,y_train)\n\n    from lightgbm import LGBMRegressor\n    lgbm = LGBMRegressor()\n    lgbm.fit(X_train,y_train)\n    #Print the accuracy for ech model\n    print(\"Results\")\n    print('[0] Logistic Regression Test Error: ',np.sqrt(mean_squared_error(y_test,lr.predict(X_test))))\n    print('[1] K neighbors Regression Test Error: ',np.sqrt(mean_squared_error(y_test,knn.predict(X_test))))\n    print('[2] SVR linear Regression Test Error: ',np.sqrt(mean_squared_error(y_test,svc.predict(X_test))))\n    print('[3] Decision Tree Regression Test Error: ',np.sqrt(mean_squared_error(y_test,tree.predict(X_test))))\n    print('[4] Random Forest Regression Test Error: ',np.sqrt(mean_squared_error(y_test,forest.predict(X_test))))\n    print('[5] Gradient Boosting Regression Test Error: ',np.sqrt(mean_squared_error(y_test,gb.predict(X_test))))\n    print('[6] XGBoost Regression Test Error: ',np.sqrt(mean_squared_error(y_test,xgb.predict(X_test))))\n    print('[7] LightGBM Regression Test Error: ',np.sqrt(mean_squared_error(y_test,lgbm.predict(X_test)))) \n\n    return lr,knn,svc,tree,forest,gb,xgb,lgbm","fbd54d9e":"lr,knn,svc,tree,forest,gb,xgb,lgbm = models()","3107bc9d":"from sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score\nfrom sklearn.ensemble import GradientBoostingRegressor","9b2cfac6":"gb_params = {\n    'learning_rate': [0.001, 0.01, 0.1, 0.2],\n    'max_depth': [3, 5, 8,50,100],\n    'n_estimators': [200, 500, 1000, 2000],\n    'subsample': [1,0.5,0.75],\n}","83020bd6":"gb = GradientBoostingRegressor()\ngb_cv_model = GridSearchCV(gb, gb_params, cv = 10, n_jobs = -1, verbose = 0)\ngb_cv_model.fit(X_train, y_train)","9009d8b7":"gb_cv_model.best_params_","d8eb03ea":"gb_cv_model.best_estimator_","22486df3":"gb_tuned = gb_cv_model.best_estimator_\n\ngb_tuned = gb_tuned.fit(X_train,y_train)","532fbf1b":"y_pred = gb_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","49f4151f":"#create the mape function to evaluate the results\ndef mape(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","472ba96a":"mape(y_test,y_pred)","39ef518f":"result = pd.concat([pd.DataFrame(y_test).reset_index(), pd.DataFrame(y_pred,columns=[\"prediction\"])],axis=1)\ndel result[\"index\"]\nresult","030d9a14":"Importance = pd.DataFrame({\"Importance\": gb_tuned.feature_importances_*100},\n                         index = X_train.columns)","979ed843":"Importance.sort_values(by = \"Importance\", \n                       axis = 0, \n                       ascending = True).plot(kind =\"barh\", color = \"r\")\n\nplt.xlabel(\"De\u011fi\u015fken \u00d6nem D\u00fczeyleri\")","f38d8a22":"plt.figure(figsize=(20,10))\nplt.plot(result[\"charges\"], \"black\", linewidth=2)\nplt.plot(result[\"prediction\"], \"r--\", linewidth = 2)\nplt.legend([\"true\",\"predicted\"])\nplt.title(\"Results of the model \")\nplt.show()","2800f498":"plt.scatter(x=y_test,y=y_pred,c = 'c', marker = 'o', s = 35, alpha = 0.7)","fe1b7e1a":"* ***It is seen that the insurance costs of those who do not have children are less. I think that this newly created variable will affect the model.***","65362dd5":"# Insurance Prediction\nIn this notebook we will predict the insurance cost. This will include data analysis, data visualization, feature engineering and modelling. We will try multiple models and develop the best result. I hope you will like it :)","b885f43e":"* ***Let's start the model!***","8f0708ed":"* ***We created a new column with the name \"age_range\", with 1 for those younger than 30, 2 for those aged between 30 and 45, and 3 for those over 45.***","407c6bf6":"* ***Compared to other regions, those living in the Southeast region have higher insurance costs.****","8c97dabc":"* If you like it plase vote! ","bced318d":"# Feature Importance","4f6e648e":"* ***Looking at the average of insurance costs according to \"age_range\", those with a higher \"age range\" have higher insurance costs.***","e8a7d5b6":"* ***It is seen that the insurance costs of people who have 5 children are lower than their age groups.****","73bd04f5":"* ***When we look at the correlations of the variables, it is seen that the linear correlation with the smoker variable is quite high and the sex variables is quite low that's why we are going to drop this feature from the data for the model.***","a48dd1c0":"* Doing some analysis ","6a6a7496":"Beautiful! There are no nulls in our dataset.","6ad69ca8":"* ***Let's examine the effect of smoking on insurance costs according to the gender variable.Looking at the graph, it is understood that although there is no obvious difference between men and women, the insurance costs of smokers are higher.***","0780c88e":"* ***According to the results we obtained, our model works best with Gradient Boosting. Then let's move on to model tuning!***","04bda660":"* ***When the effect of the number of children on insurance costs is examined, it is interesting that the insurance costs of families with 5 children are lower.***","802e5c9c":"* ***We have created a new column as those who have children and those who do not. Thus, we will be able to examine the insurance costs of people with and without children.***","ef53146c":"* ***We decide to create a model function to see all model that I choose. This function is going to give us all models' error rate and we are going to pick up one to use as a main model which has the lowest error and we are going to use fine tuning to get best paremeters and we can get better result with it.***","3a93ba5c":"# Modelling"}}