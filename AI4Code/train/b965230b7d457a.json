{"cell_type":{"b44a90ac":"code","3bbf74c0":"code","fd0e6e2d":"code","45be1a5e":"code","92659195":"code","6a1be563":"code","dbddc802":"code","faabc191":"code","e432ae98":"code","f279791c":"code","00623d50":"code","52eab419":"code","6c29c378":"code","1c4fb863":"code","1bca4e08":"code","1d8d62a7":"code","97396721":"code","8b1abcca":"code","be71d49f":"code","d742dff7":"code","0e120aea":"code","d4f9bf4d":"code","fca4f7d3":"code","79e264d0":"code","95582ce2":"code","4dee3c8e":"code","ca335f58":"code","7370f4c4":"code","e765a706":"code","6c2e7b69":"code","0fb0ad0c":"code","8cf0cce4":"code","071d5961":"code","8ded81c6":"code","6813edd6":"code","38bb8ed8":"code","522f5294":"code","8da2de1d":"code","b7a4170a":"code","be769247":"code","f523e272":"code","3ba1cc11":"code","0478ac26":"code","7417485c":"code","8bf3278c":"code","02a1d462":"code","48c3dad9":"code","75983afb":"code","4d315a2f":"code","e1096e7f":"code","3d361756":"code","3e3122ab":"code","1abea01c":"code","5c3ae9dc":"markdown","1457eea4":"markdown","8d577ecb":"markdown","e7b83c4f":"markdown","c768e952":"markdown","07ad84e3":"markdown","1735379c":"markdown","b6a14436":"markdown","972bfd61":"markdown","f834e44a":"markdown","18f77522":"markdown","85c0be93":"markdown","767556b2":"markdown","a2569b42":"markdown","8ead471e":"markdown","93fae181":"markdown","0000d0ac":"markdown","c19f2f2d":"markdown","4966e56e":"markdown","ddf4dee9":"markdown","6c4d0c13":"markdown","4e6aef3e":"markdown","f7853374":"markdown","70c06ad1":"markdown"},"source":{"b44a90ac":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","3bbf74c0":"from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score,KFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import tree, metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LinearRegression, ElasticNet\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import recall_score, mean_squared_error, accuracy_score\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor","fd0e6e2d":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","45be1a5e":"ss = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv', parse_dates=True)","92659195":"train.head()","6a1be563":"train.info()","dbddc802":"test.info()","faabc191":"for col in train.select_dtypes('O').columns:\n    print('Unique values in {} column : {}'.format(col,train[col].unique()))","e432ae98":"corr = train.corr()\ncorr['SalePrice'].sort_values(ascending=False)[1:20].to_frame()","f279791c":"pl = pd.DataFrame()\npl['Sale Price'] = train['SalePrice']\npl['Log Sale Price'] = np.log1p(train['SalePrice'])\nsns.histplot(data=pl, x=\"Sale Price\", bins=30)","00623d50":"sns.histplot(data=pl, x=\"Log Sale Price\", bins=30)","52eab419":"train.drop(train[train['GrLivArea']>4500].index, inplace=True)\ntrain.drop(train[train['LotFrontage']>300].index, inplace=True)\ntrain.drop(train[train['LotArea']>100000].index, inplace=True)\ntrain.drop(train[train['MasVnrArea']>1500].index, inplace=True)\ntrain.drop(train[train['OpenPorchSF']>500].index, inplace=True)","6c29c378":"Y_train = train['SalePrice']\ndel train['SalePrice']\n\n#Converting the saleprice with Logarithms to overcome the high skewness and the outliers\nY_train = np.log1p(Y_train) \n\nfull_df = pd.concat([train.iloc[:,1:], test.iloc[:,1:]])\nntrain = len(train)\nntest = len(test)","1c4fb863":"#First of all make a list of categorical & numerical features of full data\nnum_features = ['LotFrontage', 'LotArea', 'YearBuilt','YearRemodAdd', 'MasVnrArea', \n            'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', \n            'GrLivArea','GarageYrBlt', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', \n            '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\n\ncat_features = ['MSSubClass', 'MSZoning', 'Street', 'LotShape', 'LandContour','LotConfig', 'LandSlope', 'Neighborhood',\n            'Condition1', 'Condition2','BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st','Exterior2nd', \n            'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation','BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n            'BsmtFinType2','Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual','TotRmsAbvGrd', 'Functional', \n            'GarageType','GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'BsmtFullBath', 'BsmtHalfBath','Fireplaces',\n            'MoSold', 'SaleType', 'SaleCondition','OverallCond', 'YrSold', 'Utilities', 'GarageCars', 'FullBath', 'HalfBath', \n                'BedroomAbvGr', 'KitchenAbvGr']","1bca4e08":"nd = pd.melt(full_df, value_vars = num_features)\nn1 = sns.FacetGrid(nd, col='variable', col_wrap=4, sharex=False, sharey=False)\nn1 = n1.map(sns.distplot, 'value')","1d8d62a7":"nd_cat = pd.melt(full_df, value_vars = cat_features)\nn2 = sns.FacetGrid(nd_cat, col='variable', col_wrap=4, sharex=False, sharey=False)\nn2 = n2.map(sns.countplot, 'value')","97396721":"plt.figure(figsize=(20,6))\nplt.title('Heatmap for the null values in each column')\nsns.heatmap(full_df.isnull(),cmap=\"YlGnBu\")","8b1abcca":"full_df.isnull().sum().sort_values(ascending=False).head(10)","be71d49f":"full_df.drop(['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis=1, inplace=True)","d742dff7":"#Start with numerical columns\nnum_col_na = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', \n          'BsmtFullBath','MasVnrArea','BsmtHalfBath','BsmtFinSF1', \n          'BsmtFinSF2', 'BsmtUnfSF','GarageYrBlt','GarageArea', 'GarageCars']\nfor col in num_col_na:\n    full_df[col].fillna(0, inplace=True)","0e120aea":"#Let's fill some categorical columns\ncat_col_na = ['MasVnrType', 'BsmtCond', 'BsmtExposure', 'BsmtQual', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType',\n             'GarageFinish','GarageType', 'GarageCond', 'GarageQual']\n\nfor col in cat_col_na:\n    full_df[col].fillna('Others', inplace=True)\n    \n#Let's finish filling last categorical columns\nfull_df['Functional'].fillna('Typ' ,inplace=True)\nfull_df['Electrical'].fillna('SBrkr' ,inplace=True)\nfull_df['SaleType'].fillna('Oth' ,inplace=True)  \nfull_df['KitchenQual'].fillna('TA' ,inplace=True)\nfull_df['SaleType'].fillna('Oth' ,inplace=True)      \nfull_df['Exterior1st'].fillna('Other' ,inplace=True) \nfull_df['Exterior2nd'].fillna('Other' ,inplace=True)\nfull_df['Utilities'].fillna('AllPub' ,inplace=True)\n\nlotfrontage_by_neighborhood = full_df.groupby('Neighborhood')['LotFrontage'].median()\n\nfor ngh in lotfrontage_by_neighborhood.index:\n    full_df.loc[(full_df['LotFrontage'].isnull()) & (full_df['Neighborhood']==ngh), ['LotFrontage']] = lotfrontage_by_neighborhood[ngh]\n\nfull_df['MSZoning'] = full_df['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))","d4f9bf4d":"remap = {180: 1, 30: 1, 45: 1, 190: 2, 50: 2, 90: 2, 85: 2, 40: 2, 160: 2, 70: 3, 20: 3, 75: 3, 80: 3, 120: 4, 60: 5}\nfull_df['MSSubClass'].replace(remap, inplace=True)","fca4f7d3":"full_df.isnull().sum().max()","79e264d0":"full_df['TtlPorchArea'] = full_df['OpenPorchSF'] + full_df['EnclosedPorch'] + full_df['3SsnPorch'] + full_df['ScreenPorch'] + full_df['WoodDeckSF']\n\nfull_df['TtlFootage'] = full_df['GrLivArea'] + full_df['BsmtFinSF1'] + full_df['BsmtFinSF2']\n\nfull_df['TtlBath'] = full_df['FullBath'] + 0.5*full_df['HalfBath'] + full_df['BsmtFullBath'] + 0.5*full_df['BsmtHalfBath']\n\nfull_df['Age'] = full_df['YrSold'] - full_df['YearBuilt']\n\nfull_df['Remod_Age'] = abs(full_df['YrSold'] - full_df['YearRemodAdd'])\n\nfull_df['RemodSinceBuilt'] = full_df['YearRemodAdd'] - full_df['YearBuilt']\n\nfull_df['LivAndBsmt'] = full_df['GrLivArea'] + full_df['TotalBsmtSF']*0.5\n\n#feature_new_num = ['TtlPorchArea', 'TtlFootage', 'TtlBath', 'Age', 'Remod_Age', 'RemodSinceBuilt', 'LivAndBsmt']\n#for x in feature_new_num:\n#    num_features.append(x)","95582ce2":"full_df.loc[full_df['OpenPorchSF']>0, 'HasOpenPorch'] = 1\nfull_df['HasOpenPorch'].fillna(0, inplace=True)\n\nfull_df.loc[full_df['2ndFlrSF']>0, 'Has2ndFlr'] = 1\nfull_df['Has2ndFlr'].fillna(0, inplace=True)\n\nfull_df['HasPool'] = full_df['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n\nfeature_new_cat = ['HasOpenPorch', 'Has2ndFlr', 'HasPool']\nfor x in feature_new_cat:\n    cat_features.append(x)","4dee3c8e":"# import labelencoder\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()","ca335f58":"full_df[cat_features].info()","7370f4c4":"# Convert YrSold, MoSold, MSSubClass, and OverallCond columns from numeric to string\n\nfull_df['Has2ndFlr'] = full_df['Has2ndFlr'].astype(int)\nfull_df['HasOpenPorch'] = full_df['HasOpenPorch'].astype(int)\nfull_df['GarageCars'] = full_df['GarageCars'].apply(int)\nfull_df['BsmtHalfBath'] = full_df['BsmtHalfBath'].astype(int)\nfull_df['BsmtFullBath'] = full_df['BsmtFullBath'].astype(int)","e765a706":"full_df['GarageCars'] = full_df['GarageCars'].apply(str)\nfull_df['BsmtHalfBath'] = full_df['BsmtHalfBath'].astype(str)\nfull_df['BsmtFullBath'] = full_df['BsmtFullBath'].astype(str)\n\nfull_df['YrSold'] = full_df['YrSold'].astype(str)\nfull_df['MoSold'] = full_df['MoSold'].astype(str)\nfull_df['MSSubClass'] = full_df['MSSubClass'].apply(str)\nfull_df['OverallCond'] = full_df['OverallCond'].astype(str)\nfull_df['TotRmsAbvGrd'] = full_df['TotRmsAbvGrd'].astype(str)","6c2e7b69":"# Apply le on categorical feature columns\n\nfor c in cat_features:\n    lbl = LabelEncoder() \n    full_df[c] = lbl.fit_transform(full_df[c])\nfull_df[cat_features].head(10)","0fb0ad0c":"full_df[cat_features].info()","8cf0cce4":"from scipy.stats import skew\nfrom scipy.special import boxcox1p","071d5961":"skewed = full_df[num_features].apply(lambda x: skew(x))\nplt.figure(figsize=(16,12))\nskewed.sort_values().plot(kind='bar')\nplt.subplots_adjust(bottom=0.3)\nplt.show()","8ded81c6":"skewed_high = skewed[abs(skewed) > 0.75]\nskewed_features = skewed_high.index\nlam = 0.3\nfor x in skewed_features:\n    full_df[x] = boxcox1p(full_df[x], lam)\nplt.figure(figsize=(16,12))\nskewed_rev = full_df[num_features].apply(lambda x: skew(x))\nskewed_rev.sort_values().plot(kind='bar')\nplt.subplots_adjust(bottom=0.3)\nplt.show()","6813edd6":"skewed_feats = train[num_features].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\nfull_df[skewed_feats] = np.log1p(full_df[skewed_feats])","38bb8ed8":"#Make sure we have done good work\nfull_df.info()","522f5294":"train_set = full_df[:ntrain].reset_index().drop('index',axis=1)\ntest_set = full_df[ntrain:].reset_index().drop('index',axis=1)","8da2de1d":"plt.figure(figsize=(10,6))\nsns.distplot(Y_train)\nplt.show()","b7a4170a":"X_train, X_test, y_train, y_test = train_test_split(train_set, Y_train, test_size=0.2, random_state=42)","be769247":"from sklearn.linear_model import Lasso","f523e272":"kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\nmodel_Lasso = Lasso(random_state=42)\ngrid_param_lasso = {'alpha': [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}\ngd_sr = GridSearchCV(estimator=model_Lasso, param_grid=grid_param_lasso, scoring='neg_mean_squared_log_error', cv=kfolds, n_jobs=-1)\ngd_sr.fit(X_train, y_train)\nbest_parameters = gd_sr.best_params_\nbest_parameters","3ba1cc11":"model_Lasso = Lasso(alpha=0.0005, random_state=42)\nall_accuracies = np.sqrt(-cross_val_score(estimator=model_Lasso, X=X_train, y=y_train, scoring='neg_mean_squared_error', cv=kfolds))\nall_accuracies.mean()","0478ac26":"model_Lasso.fit(X_train, y_train)","7417485c":"y_pred = model_Lasso.predict(X_test)","8bf3278c":"parametersGrid = {\"max_iter\": [1, 5, 10, 100],\n                  \"alpha\": [0.0005, 0.005, 0.001, 0.01, 0.1, 1, 10, 100],\n                  \"l1_ratio\": np.arange(0.0, 1.0, 0.1)}\n\nkfold = KFold(n_splits=10)\n\neNet  = ElasticNet()\neNet_grid = GridSearchCV(eNet, parametersGrid, scoring='r2', cv=kfold)\neNet_grid.fit(train_set, Y_train)","02a1d462":"eNet_grid.score(X_test, y_test)","48c3dad9":"y_pred2 = eNet_grid.predict(X_test)","75983afb":"GBoost = GradientBoostingRegressor(n_estimators=3000,learning_rate=0.005, max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10,loss='huber', random_state =5)\n#RMSE estimated through the partition of the train set\nGBoost.fit(X_train, y_train)","4d315a2f":"GBoost.score(X_test, y_test)","e1096e7f":"y_pred3 = GBoost.predict(X_test)","3d361756":"data = pd.DataFrame({\"Actual\":np.expm1(y_test), \"Predicted\" : np.expm1(eNet_grid.predict(X_test))})\nsns.set(rc={'figure.figsize':(10,7)})\nsns.scatterplot(data=data)","3e3122ab":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred2))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred2))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred2)))","1abea01c":"ss[\"SalePrice\"] = np.expm1(eNet_grid.predict(test_set))\nss.to_csv(\"submission.csv\", index=False)","5c3ae9dc":"I will choose _**ElasticNet**_ as the best model.\n\nLet's check some errors values:","1457eea4":"# 1. First steps","8d577ecb":"# 2. Cleaning data","e7b83c4f":"#### Back to train\/test datasets","c768e952":"### Begin the cleaning","07ad84e3":"We need to encode categorical features with encoder. I had a choice between LabelEncoder() and pd.get_dummies()\n\nAnd choose first one","1735379c":"Data Structure","b6a14436":"Using logarithms helps us get a normal distribution, which can help detect outliers.\n\nIn this data, we have a right-skewed distribution in which most sales are between 0 and 340K.","972bfd61":"Top 4 features have missing data > 70%. It is of course we have to remove them. \nFor FireplaceQu with 47% missing data, I decided to also remove it.","f834e44a":"Check features with missing data","18f77522":"Import data","85c0be93":"# 0. Background\n### The Challenge\n\nAsk a home buyer to describe their dream house, and they probably won\u2019t begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition\u2019s dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\n### Practice Skills\n\nCreative feature engineering\n\nAdvanced regression techniques like random forest and gradient boosting\n\n### Acknowledges\n\nThe Ames Housing dataset was compiled by Dean De Cock for use in data science education. It\u2019s an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset.","767556b2":"# 4. Machine learning","a2569b42":"## Scewness","8ead471e":"### Outliers","93fae181":"### Binary features:","0000d0ac":"## Feature creation\n### Summation\/subtraction of features:","c19f2f2d":"### Numeric features \u2014 Distribution plot:","4966e56e":"# 3. Preparing the data for ML","ddf4dee9":"First lets see the distribution of SalePrice","6c4d0c13":"### Categorical features \u2014 Count plot:","4e6aef3e":"#### Gotcha!","f7853374":"# Conclusion","70c06ad1":"_Reduce skewness for the numeric features._"}}