{"cell_type":{"264b3dd4":"code","20ae6a5a":"code","30bd0281":"code","fe233e85":"code","7e45db84":"code","fe85a9ec":"code","1e1381f5":"code","a94c02b4":"code","c35417fc":"code","2364d27d":"code","83d48ffe":"code","36f35824":"code","748da94d":"code","9aedee4b":"code","98ea8d04":"code","8fa9db67":"code","a9b6c42f":"code","601baa5e":"code","1c4371e3":"code","6c70f335":"code","be957fa1":"code","a19a98c3":"code","8bbd674f":"code","340aff3a":"code","d4531a05":"code","d303cd2c":"code","d817b559":"code","6be07627":"code","222ec3f8":"code","6967981b":"code","9ba561ba":"code","3a2f912c":"code","29d078e8":"code","058f43e2":"code","371f2d01":"code","c441e9bf":"code","b6c87b57":"markdown","3ca261b9":"markdown","908411da":"markdown","26a0b323":"markdown","102e579a":"markdown","f520017b":"markdown","5b450387":"markdown","6cfdf118":"markdown","7be548df":"markdown","69b73d6a":"markdown","45ab8391":"markdown","d6cdbfb6":"markdown","039175ce":"markdown","601afe14":"markdown","2906e136":"markdown","2c2602a8":"markdown","7d1a30b8":"markdown","c3d04c00":"markdown","319aeac9":"markdown"},"source":{"264b3dd4":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n\nfrom catboost import CatBoostClassifier\n\n%matplotlib inline","20ae6a5a":"df = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")","30bd0281":"df.head()","fe233e85":"df.info()","7e45db84":"df.describe()","fe85a9ec":"df.isnull().sum()","1e1381f5":"for col in df.columns:\n    print(f\"{col}: {len(df[col].value_counts())}\")","a94c02b4":"num_cols = [\"age\",\n            \"trestbps\",\n            \"chol\",\n            \"thalach\",\n            \"oldpeak\",\n            \"ca\"]\n\ncat_cols = [\"sex\",\n            \"cp\",\n            \"fbs\",\n            \"restecg\",\n            \"exang\",\n            \"slope\",\n            \"thal\"]\n\nX_cols = num_cols + cat_cols\ntarget_col = \"target\"","c35417fc":"X = df[X_cols]\ny = df[target_col]","2364d27d":"# checking balance: classes are balanced\ndf[target_col].value_counts()","83d48ffe":"f, axs = plt.subplots(2, 3, figsize=(12, 4))\nfor i, col  in enumerate(num_cols):\n    df[col].hist(ax=axs.reshape(-1)[i], bins=10)\n    axs.reshape(-1)[i].set_title(col)\n\nf.tight_layout()","36f35824":"f, axs = plt.subplots(2, 3, figsize=(10, 8))\nfor i, col  in enumerate(num_cols):\n    sns.boxplot(y=col, x=\"target\", data=df,  orient='v', ax=axs.reshape(-1)[i])\n\nf.tight_layout()","748da94d":"sns.pairplot(df, x_vars=set(num_cols), y_vars=set(num_cols), hue=target_col);","9aedee4b":"f, axs = plt.subplots(3, 3, figsize=(10, 8))\nfor i, col in enumerate(cat_cols):\n    sns.countplot(x=col, data=df, hue=target_col, ax=axs.reshape(-1)[i])\nf.tight_layout()","98ea8d04":"fig, ax = plt.subplots(figsize=(12, 12))  \nsns.heatmap(np.round(df.corr(), 2), annot=True, ax=ax);","8fa9db67":"# categorical columns \"sex\", \"fbs\", \"exang\" are already encoded\nohe_cols = [\"cp\", \"restecg\", \"slope\", \"thal\"]\n\nclass OHE_Transformer(BaseEstimator, TransformerMixin):\n    def __init__(self, ohe_cols):\n        self.ohe_cols = ohe_cols\n        self.ohe = OneHotEncoder(sparse=False)\n\n    def fit(self, X, y=None):\n        self.ohe.fit(X[self.ohe_cols])\n        return self\n\n    def transform(self, X):\n        encodedX = self.ohe.transform(X[self.ohe_cols])\n        encoded_cols = self.ohe.get_feature_names(self.ohe_cols)\n        encodedX_df = pd.DataFrame(encodedX, columns=encoded_cols, index=X.index)\n        newX = pd.concat([X[X.columns.difference(self.ohe_cols)], encodedX_df], axis=1)\n        return newX\n\nclass ColumnNameSaver(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if hasattr(X, \"columns\"):\n            self.cols = X.columns\n        else:\n            self.cols = [f\"col_{i}\" for i in range(X.shape[1])]\n        return X\n\ndef ColumnRemoveFunc(X, remove_cols=None):\n    if remove_cols is None:\n        return X\n    \n    newX = X[X.columns.difference(remove_cols)]\n    return newX\n    \ndef ColumnRemoveTransformer(remove_cols):\n    return FunctionTransformer(ColumnRemoveFunc, kw_args={\"remove_cols\":remove_cols})\n    \ndef ShowLogregCoef(pipe):\n    cols = pipe[\"cols\"].cols\n    coefs = pipe[\"logreg\"].coef_.reshape(-1)\n    coefs = np.abs(np.round(coefs, 4))\n    a = pd.DataFrame(coefs, index=cols)\n\n    fig, ax = plt.subplots(figsize=(4, 8)) \n    sns.heatmap(a, annot=True, ax=ax)\n\ndef PrintRocAucScore(model, X, y):\n    roc_aucs = cross_val_score(model, X, y, cv=10, scoring=\"roc_auc\", n_jobs=-1)\n    print(f\"scores: {' '.join(roc_aucs.round(5).astype('str'))}\")\n    print(f\"avg: {roc_aucs.mean().round(5)}\")\n    print(f\"std: {roc_aucs.std().round(5)}\")\n\ndef ShowRocCurvePlot(model, X_train, X_test, y_train, y_test):\n    y_test_proba = model.predict_proba(X_test)[:, 1]\n    y_train_proba = model.predict_proba(X_train)[:, 1]\n\n    train_auc = metrics.roc_auc_score(y_train, y_train_proba)\n    test_auc = metrics.roc_auc_score(y_test, y_test_proba)\n\n    plt.figure(figsize=(10,7))\n    plt.plot(*metrics.roc_curve(y_train, y_train_proba)[:2], label='train AUC={:.4f}'.format(train_auc))\n    plt.plot(*metrics.roc_curve(y_test, y_test_proba)[:2], label='test AUC={:.4f}'.format(test_auc))\n    legend_box = plt.legend(fontsize='large', framealpha=1).get_frame()\n    plt.show()\n","a9b6c42f":"clustering_pipeline = Pipeline(\n    [(\"ohe\", OHE_Transformer(ohe_cols)),\n     (\"rmcol\", ColumnRemoveTransformer([\"cp_1\", \"restecg_0\", \"slope_0\", \"thal_1\"])),\n     (\"cols\", ColumnNameSaver()),\n     (\"scaler\", StandardScaler())]\n)\nXPreprocessed = clustering_pipeline.fit_transform(X)\nXPreprocessed = pd.DataFrame(XPreprocessed, columns=clustering_pipeline[\"cols\"].cols)","601baa5e":"Z = linkage(XPreprocessed, 'ward')\nfig = plt.figure(figsize=(25, 10))\ndn = dendrogram(Z, color_threshold=23)\nplt.axhline(y=35, linestyle='--', color='b', label=\"main clusters\") \nplt.axhline(y=23, linestyle='--', color='r', label=\"outliers\")\nplt.legend()\nplt.show()","1c4371e3":"df[\"main_cluster\"] = fcluster(Z, 35, criterion=\"distance\")\ndf[\"outlier_cluster\"] = fcluster(Z, 23, criterion=\"distance\")\ndf[\"outlier_cluster\"].value_counts()","6c70f335":"df.drop(df[df[\"outlier_cluster\"].isin([1, 4])].index, inplace=True)\ndf.drop(columns=\"outlier_cluster\", inplace=True)\n# update X and y\nX = df[X_cols]\ny = df[target_col]","be957fa1":"df.groupby(\"main_cluster\").mean()","a19a98c3":"a = df.groupby(\"main_cluster\").mean()\nb = df.describe()\nclusters_df = pd.concat([df.groupby(\"main_cluster\").mean(), df.describe()], axis=0)\nclusters_df","8bbd674f":"clusters_df[[\"cp\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thalach\", \"thal\", \"target\"]]","340aff3a":"logreg_pipeline = Pipeline(\n    [(\"ohe\", OHE_Transformer(ohe_cols)),\n     (\"cols\", ColumnNameSaver()),\n     (\"scaler\", StandardScaler()),\n     (\"logreg\", LogisticRegression(max_iter=1000, penalty=\"l1\", solver=\"saga\"))]\n)\n\nlogreg_pipeline.fit(X, y)","d4531a05":"ShowLogregCoef(logreg_pipeline)","d303cd2c":"logreg_pipeline = Pipeline(\n    \n    [(\"ohe\", OHE_Transformer(ohe_cols)),\n     (\"rmcol\", ColumnRemoveTransformer([\"cp_1\", \"restecg_0\", \"slope_0\", \"thal_1\"])),\n     (\"cols\", ColumnNameSaver()),\n     (\"scaler\", StandardScaler()),\n     \n     (\"logreg\", LogisticRegression(max_iter=1000, penalty=\"l1\", solver=\"saga\"))]\n)\n\nlogreg_pipeline.fit(X, y)\nShowLogregCoef(logreg_pipeline)","d817b559":"preprocessing_pipeline = Pipeline(\n    [(\"ohe\", OHE_Transformer(ohe_cols)),\n     (\"cols\", ColumnNameSaver()),\n     (\"scaler\", StandardScaler())]\n)\nXPreprocessed = preprocessing_pipeline.fit_transform(X, y)\nXPreprocessed\n\nU, S, V = np.linalg.svd(XPreprocessed)  # svd \u0440\u0430\u0437\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \nprint(np.round(S, 2))\nplt.grid()\nplt.axis([0, len(S) + 1, -1, 38])\nplt.plot(range(1, len(S) + 1), S, '--o')\nplt.axvline(x=14, color='g', linestyle='--', label=\"little info threshold\")\nplt.axvline(x=17, color='r', linestyle='--', label=\"useless threshold\")\nplt.legend()\nplt.show()","6be07627":"logreg_pipeline = Pipeline(\n    [(\"ohe\", OHE_Transformer(ohe_cols)),\n     (\"rmcol\", ColumnRemoveTransformer([\"cp_1\", \"restecg_0\", \"slope_0\", \"thal_1\"])),\n     (\"scaler\", StandardScaler()),\n     (\"pca\", PCA(n_components=14)),\n     (\"cols\", ColumnNameSaver()),\n     (\"logreg\", LogisticRegression(max_iter=1000, penalty=\"l1\", solver=\"saga\"))]\n)\n\nlogreg_pipeline.fit(X, y)\nShowLogregCoef(logreg_pipeline)","222ec3f8":"# Testing logreg params\nparams = [\n    {\n        \"logreg__C\": [100, 10, 1, 0.1, 0.01, 0.001],\n        \"logreg__penalty\": [\"l2\"],\n        \"logreg__class_weight\": [\"balanced\", None],\n        \"logreg__l1_ratio\": [None],\n        \"pca__n_components\": [13, 14, 15, 16, 17],\n    },\n\n    {\n        \"logreg__C\": [100, 10, 1, 0.1, 0.01, 0.001],\n        \"logreg__penalty\": [\"l1\"],\n        \"logreg__class_weight\": [\"balanced\", None],\n        \"logreg__l1_ratio\": [None],\n        \"pca__n_components\": [13, 14, 15, 16, 17],\n    },\n\n    {\n        \"logreg__C\": [100, 10, 1, 0.1, 0.01, 0.001],\n        \"logreg__penalty\": [\"elasticnet\"],\n        \"logreg__class_weight\": [\"balanced\", None],\n        \"logreg__l1_ratio\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n        \"pca__n_components\": [13, 14, 15, 16, 17],\n    }\n]\n\n\nlogreg_grid = GridSearchCV(logreg_pipeline, params, cv=10, verbose=4, scoring='roc_auc', refit=True, n_jobs=-1)\nlogreg_grid.fit(X, y)\nprint(logreg_grid.best_params_)","6967981b":"best_params = {\n    'logreg__C': 0.01, \n    'logreg__class_weight': 'None', \n    'logreg__l1_ratio': None, \n    'logreg__penalty': 'l2', \n    'pca__n_components': 13}\n\nlogreg_pipeline.set_params(**best_params)\nPrintRocAucScore(logreg_pipeline, X, y)","9ba561ba":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\nlogreg_pipeline.fit(X_train, y_train)\nShowRocCurvePlot(logreg_pipeline, X_train, X_test, y_train, y_test)","3a2f912c":"logreg_pipeline.fit(X, y)\nShowLogregCoef(logreg_pipeline)","29d078e8":"# we don't preprocess categorical features here somehow, because catboost is able to do it better\ncatboost_pipeline = Pipeline(\n    [(\"cb\", CatBoostClassifier(loss_function='Logloss',\n                               verbose=False,\n                               cat_features=cat_cols,\n                               random_seed = 42,\n                               eval_metric='AUC'))]\n)\n\nPrintRocAucScore(catboost_pipeline, X, y)","058f43e2":"params = {\n    \"cb__depth\": [4, 6, 8],\n    \"cb__learning_rate\": [0.01, 0.1, 1],\n    \"cb__l2_leaf_reg\": [0.1, 1, 10, 50],\n    \"cb__iterations\": [100, 200, 400],\n}\n\ncb_grid = GridSearchCV(catboost_pipeline, params, cv=5, verbose=4, scoring='roc_auc', refit=True, n_jobs=-1)\ncb_grid.fit(X, y)\nprint(cb_grid.best_params_)","371f2d01":"best_params = {\"cb__depth\": 4, \n               \"cb__iterations\": 100, \n               \"cb__l2_leaf_reg\": 10, \n               \"cb__learning_rate\": 0.1}\n\ncatboost_pipeline.set_params(**best_params)\nPrintRocAucScore(catboost_pipeline, X, y)","c441e9bf":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\ncatboost_pipeline.fit(X_train, y_train)\nShowRocCurvePlot(catboost_pipeline, X_train, X_test, y_train, y_test)","b6c87b57":"\"ca\", \"oldpeak\", \"thalach\" seems to be significant features","3ca261b9":"First of all we should remove useless features gathered from categorical features after OHE (1 from each feature)\nbecause it could be calculated using others. These features don't give us any additional information.\nSo, removing \"cp_1\", \"restecg_0\", \"slope_0\", \"thal_1\" and rerun the model.","908411da":"Mean ROC AUC score on cv is ~0.91","26a0b323":"Dataframe was parsed correctly. All fields has numeric data type (int64\/float64). We don't see missed data. <br>\nFind out which features are numeric and which one are categorical.","102e579a":"\"ca\", \"oldpeak\", \"thalach\", \"cp\", \"thal\" are correlated with target","f520017b":"# Catboost","5b450387":"There are still several features with low coef: \"age\", \"fbs\", \"slope_2\", \"thal_2\". Try SVD method to ensure.","6cfdf118":"Clusters could be separated by features: cp, exang, oldpeak, slope, ca, thalach, thal. <br>\nCluster1 tends to be target class 0 (target mean = 0.21) <br>\nCluster2 tends to be target class 1 (target mean = 0.77) <br>\nMaybe there are more than 2 clusters but we don't have enough data to find them.","7be548df":"Cluster 1 and 4 are outliers. Remove them from the data.","69b73d6a":"SVD analysis confirms that removed features were totally useless. There is also a small step around N=14, so 3 more features could be removed too. Moreover, we could see that there is one feature which seems to be the most importnant (\"ca\"). The optimal feature number is between 13 and 17.","45ab8391":"Since we don't have much data (only 303 samples) it is hard to estimate quality of models. Depending on the split we could get underfitting or overfitting roc curves. The avarage ROC AUC of both models is about 0.91","d6cdbfb6":"# Cluster analysis","039175ce":"\"cp\", \"thal\" seems to be significant features","601afe14":"# Preprocessing","2906e136":"# Feature overview","2c2602a8":"# LogisticRegression","7d1a30b8":"There are 2 small outlier clusters and 2 main clusters","c3d04c00":"\"trestbps\", \"chol\", \"fbs\", \"restecg\" columns seems to be useless for classification because they are not correlate with target and there were no correlations with other features in pairplot.","319aeac9":"Mean ROC AUC score on cv is ~0.91. Weights shows that there is 1 main feature. Others don't influence much on target."}}