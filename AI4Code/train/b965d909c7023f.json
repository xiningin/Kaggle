{"cell_type":{"8a7f4749":"code","17c35cef":"code","e346c3c6":"code","27dad098":"code","70ef5890":"code","3c030553":"code","93f6ff8e":"code","be5549ff":"code","b044389d":"code","c449c5c0":"code","3875e90d":"code","4dbea096":"code","a72a71d9":"code","40e33003":"code","7bda4978":"code","0ebf8e36":"code","8eb80b79":"code","672ec275":"code","24474592":"code","7f43d44b":"code","29207e9c":"code","efeed1ae":"code","a94f2321":"code","8c3411fb":"code","25bfb60f":"code","e1459616":"code","ea2ee203":"markdown","163c84f7":"markdown","8aa9f4eb":"markdown","c1350441":"markdown","20354640":"markdown","fda11bae":"markdown","36f3f376":"markdown","9ce65c02":"markdown","665184d4":"markdown","9fd75c8d":"markdown"},"source":{"8a7f4749":"batch_size = 2048\nSTROKE_COUNT = 196\nTRAIN_SAMPLES = 750\nVALID_SAMPLES = 75\nTEST_SAMPLES = 50\nuse_gpu = True\nLEARNING_RATE = 1e-3\nEPOCHS = 50","17c35cef":"%matplotlib inline\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.utils.np_utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\nfrom glob import glob\nimport torch\nimport torch.utils.data as data\nimport torch.nn as nn\nimport gc\ngc.enable()","e346c3c6":"base_dir = os.path.join('..', 'input')\ntest_path = os.path.join(base_dir, 'test_simplified.csv')","27dad098":"device = torch.device(\"cuda\" if use_gpu else \"cpu\")\ntorch.manual_seed(42) # try and make the results more reproducible","70ef5890":"from ast import literal_eval\nALL_TRAIN_PATHS = glob(os.path.join(base_dir, 'train_simplified', '*.csv'))\nCOL_NAMES = ['countrycode', 'drawing', 'key_id', 'recognized', 'timestamp', 'word']\n\ndef _stack_it(raw_strokes):\n    \"\"\"preprocess the string and make \n    a standard Nx3 stroke vector\"\"\"\n    stroke_vec = literal_eval(raw_strokes) # string->list\n    # unwrap the list\n    in_strokes = [(xi,yi,i)  \n     for i,(x,y) in enumerate(stroke_vec) \n     for xi,yi in zip(x,y)]\n    c_strokes = np.stack(in_strokes)\n    # replace stroke id with 1 for continue, 2 for new\n    c_strokes[:,2] = [1]+np.diff(c_strokes[:,2]).tolist()\n    c_strokes[:,2] += 1 # since 0 is no stroke\n    # pad the strokes with zeros\n    return pad_sequences(c_strokes.swapaxes(0, 1), \n                         maxlen=STROKE_COUNT, \n                         padding='post').swapaxes(0, 1)\ndef read_batch(samples=5, \n               start_row=0,\n               max_rows = 1000):\n    \"\"\"\n    load and process the csv files\n    this function is horribly inefficient but simple\n    \"\"\"\n    out_df_list = []\n    for c_path in ALL_TRAIN_PATHS:\n        c_df = pd.read_csv(c_path, nrows=max_rows, skiprows=start_row)\n        c_df.columns=COL_NAMES\n        out_df_list += [c_df.sample(samples)[['drawing', 'word']]]\n    full_df = pd.concat(out_df_list)\n    full_df['drawing'] = full_df['drawing'].\\\n        map(_stack_it)\n    \n    return full_df","3c030553":"train_args = dict(samples=TRAIN_SAMPLES, \n                  start_row=0, \n                  max_rows=int(TRAIN_SAMPLES*1.5))\nvalid_args = dict(samples=VALID_SAMPLES, \n                  start_row=train_args['max_rows']+1, \n                  max_rows=VALID_SAMPLES+25)\ntest_args = dict(samples=TEST_SAMPLES, \n                 start_row=valid_args['max_rows']+train_args['max_rows']+1, \n                 max_rows=TEST_SAMPLES+25)\ntrain_df = read_batch(**train_args)\nvalid_df = read_batch(**valid_args)\ntest_df = read_batch(**test_args)\nword_encoder = LabelEncoder()\nword_encoder.fit(train_df['word'])\nprint('words', len(word_encoder.classes_), '=>', ', '.join([x for x in word_encoder.classes_]))","93f6ff8e":"def get_Xy(in_df):\n    X = np.stack(in_df['drawing'], 0)\n    y = to_categorical(word_encoder.transform(in_df['word'].values))\n    return X, y\ntrain_X, train_y = get_Xy(train_df)\nvalid_X, valid_y = get_Xy(valid_df)\ntest_X, test_y = get_Xy(test_df)\nprint(train_X.shape)","be5549ff":"fig, m_axs = plt.subplots(3,3, figsize = (16, 16))\nrand_idxs = np.random.choice(range(train_X.shape[0]), size = 9)\nfor c_id, c_ax in zip(rand_idxs, m_axs.flatten()):\n    test_arr = train_X[c_id]\n    test_arr = test_arr[test_arr[:,2]>0, :] # only keep valid points\n    lab_idx = np.cumsum(test_arr[:,2]-1)\n    for i in np.unique(lab_idx):\n        c_ax.plot(test_arr[lab_idx==i,0], \n                np.max(test_arr[:,1])-test_arr[lab_idx==i,1], '.-')\n    c_ax.axis('off')\n    c_ax.set_title(word_encoder.classes_[np.argmax(train_y[c_id])])","b044389d":"class LSTMNet(nn.Module):\n    def __init__(self, in_channels, classes):\n        \"\"\"Define the components of a LSTM Stroke Model\"\"\"\n        super(LSTMNet, self).__init__()    \n        self.relu = nn.ReLU(inplace=True)\n        self.conv_1 = nn.Conv1d(in_channels=in_channels, out_channels=48, kernel_size=5)\n        self.conv_2 = nn.Conv1d(in_channels=48, out_channels=64, kernel_size=5)\n        self.conv_3 = nn.Conv1d(in_channels=64, out_channels=96, kernel_size=3)\n        \n        self.lstm = nn.LSTM(input_size=96, hidden_size=128, \n                            batch_first=False,\n                            dropout=0.3, num_layers=2)\n        self.dropout = nn.Dropout(0.3)\n        self.feat_vec = nn.Linear(128, 512)\n        self.pred = nn.Linear(512, classes)\n\n    def forward(self,x):\n        \"\"\"Input x is expected to be a 3d tensor (N x seq_length x C)\n           N - Number of images in minibatch\n           seq_length - Length of the sequen\n           C - number of channels\"\"\"\n        x = self.relu(self.conv_1(x))\n        x = self.dropout(x)\n        x = self.relu(self.conv_2(x))\n        x = self.dropout(x)\n        x = self.relu(self.conv_3(x))\n        x = self.dropout(x)\n        x = x.permute(2, 0, 1) # prepare for the LSTM\n        output, (h_0, c_0) = self.lstm(x)\n        x = self.relu(self.feat_vec(output[-1]))\n        return self.pred(x)","c449c5c0":"model = LSTMNet(classes=len(word_encoder.classes_), in_channels = train_X.shape[2]).to(device)\nmodel","3875e90d":"criterion = nn.CrossEntropyLoss() #Use cross entropy loss\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)","4dbea096":"train_loader = torch.utils.data.DataLoader(list(zip(train_X.swapaxes(1, 2).astype('float32'), # pytorch and tf\/keras have different orders\n                                                    np.argmax(train_y, 1)\n                                                   )),\n                                           shuffle=True, batch_size=batch_size, \n                                           pin_memory=False)\ntest_loader = torch.utils.data.DataLoader(list(zip(valid_X.swapaxes(1, 2).astype('float32'), \n                                                   np.argmax(valid_y, 1)\n                                                  )),\n                                           shuffle=True, batch_size=batch_size, \n                                           pin_memory=False)","a72a71d9":"# nice wait bars\nfrom tqdm import tqdm_notebook, tnrange\nfrom collections import defaultdict\nfrom IPython.display import clear_output, display\ntrain_results = defaultdict(list)\ntrain_iter, test_iter, best_acc = 0,0,0\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize = (10, 10))\nax1.set_title('Train Loss')\nax2.set_title('Train Accuracy')\nax3.set_title('Test Loss')\nax4.set_title('Test Accuracy')\n\nfor i in tnrange(EPOCHS, desc='Epochs'):\n    clear_output(wait=True)\n    display(fig)\n    print(\"Epoch \",i)\n    ## Train Phase\n    #Model switches to train phase\n    model.train() \n    \n    # Running through all mini batches in the dataset\n    count, loss_val, correct, total = train_iter, 0, 0, 0\n    for data, target in tqdm_notebook(train_loader, desc='Training'):    \n        if use_gpu: #Using GPU & Cuda\n            data, target = data.to(device), target.to(device)\n\n        output = model(data) #FWD prop\n        loss = criterion(output, target) #Cross entropy loss\n        c_loss = loss.data.item()\n        ax1.plot(count, c_loss, 'r.')\n        loss_val += c_loss\n\n        optimizer.zero_grad() #Zero out any cached gradients\n        loss.backward() #Backward pass\n        optimizer.step() #Update the weights\n\n        #Compute accuracy\n        predicted = output.data.max(1)[1] #get index of max\n        total += target.size(0) #total samples in mini batch\n        c_acc = (predicted == target).sum().item()\n        ax2.plot(count, c_acc\/target.size(0), 'r.')\n        correct += c_acc\n        count +=1\n    train_loss_val, train_iter, train_acc = loss_val\/len(train_loader.dataset), count, correct\/float(total)\n    \n    print(\"Training loss: \", train_loss_val, \" train acc: \",train_acc)    \n    ## Test Phase\n    \n    #Model switches to test phase\n    model.eval()\n\n    #Running through all mini batches in the dataset\n    count, correct, total, lost_val = test_iter, 0, 0, 0\n    for data, target in tqdm_notebook(test_loader, desc='Testing'):\n        if use_gpu: #Using GPU & Cuda\n            data, target = data.to(device), target.to(device)\n        output = model(data)\n        loss = criterion(output, target) #Cross entropy loss\n        c_loss = loss.data.item()\n        ax3.plot(count, c_loss, 'b.')\n        loss_val += c_loss\n        #Compute accuracy\n        predicted = output.data.max(1)[1] #get index of max\n        total += target.size(0) #total samples in mini batch\n        c_acc = (predicted == target).sum().item()\n        ax4.plot(count, c_acc\/target.size(0), 'b.')\n        correct += c_acc\n        count += 1\n\n    #Accuracy over entire dataset\n    test_acc, test_iter, test_loss_val = correct\/float(total), count, loss_val\/len(test_loader.dataset)\n    print(\"Epoch: \",i,\" test set accuracy: \",test_acc)\n    \n    train_results['epoch'].append(i)\n    train_results['train_loss'].append(train_loss_val)\n    train_results['train_acc'].append(train_acc)\n    train_results['train_iter'].append(train_iter)\n    \n    train_results['test_loss'].append(test_loss_val)\n    train_results['test_acc'].append(test_acc)\n    train_results['test_iter'].append(test_iter)\n    \n    #Save model with best accuracy\n    if test_acc > best_acc:\n        best_acc = test_acc\n        torch.save(model.state_dict(), 'best_model.pth') \nplt.show()\nfig.savefig('train.png')","40e33003":"train_results_df = pd.DataFrame(train_results)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 5))\ntrain_results_df.plot('epoch', ['train_loss', 'test_loss'], ax=ax1)\nax1.set_title('Loss')\ntrain_results_df.plot('epoch', ['train_acc', 'test_acc'], ax=ax2)\nax2.set_title('Accuracy')\nfig.savefig('epochs.png')","7bda4978":"# load the best model\nmodel.load_state_dict(torch.load('best_model.pth'))","0ebf8e36":"#Model switches to test phase\ntop_k_count = 5\nmodel.eval()\n#Running through all mini batches in the dataset\ncount, correct, top_k_correct, total, lost_val = test_iter, 0, 0, 0, 0\npred_cat = []\ntest_cat = []\nfor data, target in tqdm_notebook(test_loader, desc='Testing'):\n    if use_gpu: #Using GPU & Cuda\n        data, target = data.to(device), target.to(device)\n    output = model(data)\n    loss = criterion(output, target) #Cross entropy loss\n    c_loss = loss.data.item()\n    loss_val += c_loss\n    #Compute accuracy\n    predicted = output.data.max(1)[1] #get index of max\n    # add outputs for better visualization\n    test_cat.append(output.data.max(1)[1].cpu().numpy())\n    pred_cat.append(target.data.cpu().numpy())\n        \n    total += target.size(0) #total samples in mini batch\n    c_acc = (predicted == target).sum().item()\n    top_k_correct += c_acc\n    for k in range(1, top_k_count+1):\n        top_k_correct += (output.data.argsort(1, descending=True)[:, k] == target).sum().item()\n    correct += c_acc\n    count += 1\n\n#Accuracy over entire dataset\ntest_acc, test_iter, test_loss_val, top_k_acc = correct\/float(total), count, loss_val\/len(test_loader.dataset), top_k_correct\/float(total)\ntest_cat = np.concatenate(test_cat, 0)\npred_cat = np.concatenate(pred_cat, 0)\nprint('Accuracy: %2.1f%%, Top %d Accuracy %2.1f%%' % (100*test_acc, top_k_count, 100*top_k_acc))","8eb80b79":"from sklearn.metrics import confusion_matrix, classification_report\nplt.matshow(confusion_matrix(test_cat, pred_cat))\nprint(classification_report(test_cat, pred_cat, \n                            target_names = [x for x in word_encoder.classes_]))","672ec275":"points_to_use = [5, 15, 20, 30, 40, 50]\npoints_to_user = [108]\nsamples = 12\nword_dex = lambda x: word_encoder.classes_[x]\nrand_idxs = np.random.choice(range(test_X.shape[0]), size = samples)\nfig, m_axs = plt.subplots(len(rand_idxs), len(points_to_use), figsize = (24, samples\/8*24))\nfor c_id, c_axs in zip(rand_idxs, m_axs):\n    res_idx = np.argmax(test_y[c_id])\n    goal_cat = word_encoder.classes_[res_idx]\n    for pt_idx, (pts, c_ax) in enumerate(zip(points_to_use, c_axs)):\n        test_arr = test_X[c_id, :].copy()\n        test_arr[pts:] = 0 # short sequences make CudnnLSTM crash, ugh \n        in_tensor = torch.Tensor(np.expand_dims(test_arr,0).swapaxes(1, 2)).to(device)\n        stroke_pred = model(in_tensor).data[0]\n        stroke_pred = stroke_pred.cpu().numpy()\n        top_10_idx = np.argsort(-1*stroke_pred)[:10]\n        top_10_sum = np.sum(stroke_pred[top_10_idx])\n        \n        test_arr = test_arr[test_arr[:,2]>0, :] # only keep valid points\n        lab_idx = np.cumsum(test_arr[:,2]-1)\n        for i in np.unique(lab_idx):\n            c_ax.plot(test_arr[lab_idx==i,0], \n                    np.max(test_arr[:,1])-test_arr[lab_idx==i,1], # flip y\n                      '.-')\n        c_ax.axis('off')\n        if pt_idx == (len(points_to_use)-1):\n            c_ax.set_title('Answer: %s (%2.1f%%) \\nPredicted: %s (%2.1f%%)' % (goal_cat, 100*stroke_pred[res_idx]\/top_10_sum, word_dex(top_10_idx[0]), 100*stroke_pred[top_10_idx[0]]\/top_10_sum))\n        else:\n            c_ax.set_title('%s (%2.1f%%), %s (%2.1f%%)\\nCorrect: (%2.1f%%)' % (word_dex(top_10_idx[0]), 100*stroke_pred[top_10_idx[0]]\/top_10_sum, \n                                                                 word_dex(top_10_idx[1]), 100*stroke_pred[top_10_idx[1]]\/top_10_sum, \n                                                                 100*stroke_pred[res_idx]\/top_10_sum))","24474592":"sub_df = pd.read_csv(test_path)\nsub_df['drawing'] = sub_df['drawing'].map(_stack_it)","7f43d44b":"sub_vec = np.stack(sub_df['drawing'].values, 0)","29207e9c":"submission_loader = torch.utils.data.DataLoader(list(zip(sub_vec.swapaxes(1, 2).astype('float32'), \n                                                   sub_df.index.values\n                                                  )),\n                                           shuffle=False, batch_size=batch_size, \n                                           pin_memory=False)","efeed1ae":"# make the predictions\nmodel.eval()\npred_out = []\nidx_out = []\nfor data, target in tqdm_notebook(submission_loader, desc='Testing'):\n    if use_gpu: #Using GPU & Cuda\n        data = data.to(device) \n    output = model(data)\n    pred_out += [output.data.cpu().numpy()]\n    idx_out += [target.numpy()]\npred_out = np.concatenate(pred_out, 0)\nidx_out = np.concatenate(idx_out, 0)","a94f2321":"top_3_pred = [word_encoder.classes_[np.argsort(-1*c_pred)[:3]] for c_pred in pred_out]","8c3411fb":"top_3_pred = [' '.join([col.replace(' ', '_') for col in row]) for row in top_3_pred]\ntop_3_pred[:3]","25bfb60f":"fig, m_axs = plt.subplots(3,3, figsize = (16, 16))\nrand_idxs = np.random.choice(range(sub_vec.shape[0]), size = 9)\nfor c_id, c_ax in zip(rand_idxs, m_axs.flatten()):\n    test_arr = sub_vec[c_id]\n    test_arr = test_arr[test_arr[:,2]>0, :] # only keep valid points\n    lab_idx = np.cumsum(test_arr[:,2]-1)\n    for i in np.unique(lab_idx):\n        c_ax.plot(test_arr[lab_idx==i,0], \n                np.max(test_arr[:,1])-test_arr[lab_idx==i,1], '.-')\n    c_ax.axis('off')\n    c_ax.set_title(top_3_pred[c_id])","e1459616":"sub_out_df = pd.DataFrame({'key_id': sub_df['key_id'][idx_out].values, \n                          'word': top_3_pred})\nsub_out_df[['key_id', 'word']].to_csv('submission.csv', index=False)\nsub_out_df.sample(5)","ea2ee203":"# Stroke-based Classification\nHere we use the stroke information to train a model and see if the strokes give us a better idea of what the shape could be. ","163c84f7":"## Show some predictions on the submission dataset","8aa9f4eb":"# LSTM to Parse Strokes\nThe model suggeted from the tutorial is\n\n![Suggested Model](https:\/\/www.tensorflow.org\/versions\/master\/images\/quickdraw_model.png)","c1350441":"## Prepare Training Data","20354640":"# Overview\nThe notebook is modified from one that was made for the [Quick, Draw Dataset](https:\/\/www.kaggle.com\/google\/tinyquickdraw), it would actually be interesting to see how beneficial a transfer learning approach using that data as a starting point could be.\n\n## This Notebook\nThe notebook takes and preprocesses the data from the QuickDraw Competition step (strokes) and trains an LSTM. The outcome variable (y) is always the same (category). The stroke-based LSTM. The model takes the stroke data and 'preprocesses' it a bit using 1D convolutions and then uses two stacked LSTMs followed by two dense layers to make the classification. The model can be thought to 'read' the drawing stroke by stroke.\n\n## Fun Models\n\nAfter the classification models, we try to build a few models to understand what the LSTM actually does. Here we experiment step by step to see how the prediction changes with each stop\n\n### Next Steps\nThe next steps could be\n- use more data to train\n- include the country code (different countries draw different things, different ways)\n- more complex models","fda11bae":"# Reading and Parsing\nSince it is too much data (23GB) to read in at once, we just take a portion of it for training, validation and hold-out testing. This should give us an idea about how well the model works, but leaves lots of room for improvement later","36f3f376":"# Reading Point by Point","9ce65c02":"### Model Parameters\nHere we keep track of the relevant parameters for the data preprocessing, model construction and training","665184d4":"# Submission\nWe can create a submission using the model","9fd75c8d":"## Import the Data\nRead and process the stroke data"}}