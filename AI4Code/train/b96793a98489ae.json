{"cell_type":{"b2b91f71":"code","bf87eb8e":"code","cd7f1152":"code","87601c8e":"code","a99e7ad2":"code","610feabc":"code","daa5fa90":"code","f92cfdd0":"code","dc27bafb":"code","aa9fe7b4":"code","5a6f698a":"code","3f1b8ff5":"code","f9c55073":"code","fa0bdb3f":"code","46d486f8":"code","ec9792c9":"code","c2a9523a":"code","991b7e97":"code","3c69977f":"code","53ff09a2":"code","b482e119":"code","c5698b92":"code","f0fd80a5":"code","c6857092":"code","c170557b":"code","407dc30c":"code","99d15ceb":"code","cb4d852b":"code","223ce7b7":"code","e46d904f":"code","da6f2295":"code","0e2b8c07":"code","c2a17a3d":"code","38646d4f":"code","e9e0d14a":"markdown","e026e357":"markdown","a518e5bf":"markdown","fc992580":"markdown","54ba4eb8":"markdown","26a7b6cb":"markdown","43d5b6e9":"markdown","600d5d21":"markdown","f3419461":"markdown","66ebdb51":"markdown","51b05ceb":"markdown","a57d13a5":"markdown","65298aba":"markdown"},"source":{"b2b91f71":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","bf87eb8e":"train_dir = '..\/input\/tabular-playground-series-jun-2021\/train.csv'\ntest_dir = '..\/input\/tabular-playground-series-jun-2021\/test.csv'","cd7f1152":"df=pd.read_csv(train_dir)\ndf.head()","87601c8e":"df.shape","a99e7ad2":"df.info()","610feabc":"df.describe()","daa5fa90":"corr_matrix = df.corr().abs()\nsns.heatmap(corr_matrix)","f92cfdd0":"len(df['target'].unique())","dc27bafb":"features_with_na=[features for features in df.columns if df[features].isnull().sum()>1]\n## 2- step print the feature name and the percentage of missing values\nlen(features_with_na)","aa9fe7b4":"unique_class= [features for features in df['target'].unique()]\ndata_in_eachclass = []\nfor x in unique_class:\n    data_in_eachclass.append(df[df['target']== x].shape[0])\npercentage_of_data= []\ntotal_values= np.sum(data_in_eachclass)\nfor x in data_in_eachclass:\n    percentage_of_data.append((x\/total_values) *100)\nfig, ax = plt.subplots(figsize =(16, 9))\n \n# Horizontal Bar Plot\n\nax.barh(unique_class, percentage_of_data,color= 'yellow')\n \n# Remove axes splines\nfor s in ['top', 'bottom', 'left', 'right']:\n    ax.spines[s].set_visible(False)\n \n# Remove x, y Ticks\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\n \n# Add padding between axes and labels\nax.xaxis.set_tick_params(pad = 5)\nax.yaxis.set_tick_params(pad = 10)\n \n# Add x, y gridlines\nax.grid(b = True, color ='red',\n        linestyle ='-.', linewidth = 0.5,\n        alpha = 0.2)\n \n# Show top values\nax.invert_yaxis()\n \n# Add annotation to bars\nfor i in ax.patches:\n    plt.text(i.get_width()+0.2, i.get_y()+0.5,\n             str(round((i.get_width()), 2)),\n             fontsize = 10, fontweight ='bold',\n             color ='red')\n \n# Add Plot Title\nax.set_title('PERCENTAGE OF TARGET CLASS PRESENT',\n             loc ='left', )\n \n# Add Text watermark\nfig.text(0.9, 0.15, 'Amartya_Bhattacharya', fontsize = 12,\n         color ='grey', ha ='right', va ='bottom',\n         alpha = 0.7)\n \n# Show Plot\nplt.show()\n\n","5a6f698a":"from sklearn import preprocessing\n  \n# label_encoder object knows how to understand word labels.\nlabel_encoder = preprocessing.LabelEncoder()\ndf['target']= label_encoder.fit_transform(df['target'])\n# Creating the training and testing data\ny=df['target']\n# df=df.drop('id',axis=1)\n\n","3f1b8ff5":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=40, svd_solver='auto')\n\nsc = StandardScaler()\nX=df\n\nX = X.drop(['id','target'],axis=1)\nfeatures= X.columns\nX[features] = sc.fit_transform(X[features])\npca.fit(X)","f9c55073":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import  BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import log_loss","fa0bdb3f":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.1)","46d486f8":"lr = LogisticRegression(random_state=42)\n\nknn = KNeighborsClassifier()\npara_knn = {'n_neighbors':np.arange(1, 50)}\n\ngrid_knn = GridSearchCV(knn, param_grid=para_knn, cv=5)\n\ndt = DecisionTreeClassifier()\npara_dt = {'criterion':['gini','entropy'],'max_depth':np.arange(1, 50), 'min_samples_leaf':[1,2,4,5,10,20,30,40,80,100]}\ngrid_dt = GridSearchCV(dt, param_grid=para_dt, cv=5)\n\nrf = RandomForestClassifier()\n\n# Define the dictionary 'params_rf'\nparams_rf = {\n    'n_estimators':[100, 350, 500],\n    'min_samples_leaf':[2, 10, 30]\n}\ngrid_rf = GridSearchCV(rf, param_grid=params_rf, cv=5)\ndt = DecisionTreeClassifier(criterion='gini', max_depth=9, min_samples_leaf=10, random_state=42)\nknn = KNeighborsClassifier(n_neighbors=3)\nrf = RandomForestClassifier(n_estimators=500, min_samples_leaf=2, random_state=42)","ec9792c9":"classifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt), ('Random Forest', rf)]","c2a9523a":"for clf_name, clf in classifiers:    \n \n    # Fit clf to the training set\n    clf.fit(X_train, y_train)    \n   \n    # Predict y_pred\n    y_pred = clf.predict_proba(X_test)\n    log_loss_score = log_loss(y_test,y_pred)\n    \n#     Calculate accuracy\n\n#     accuracy = accuracy_score(y_pred, y_test) \n#     roc_score= roc_auc_score(y_test,y_pred)\n   \n    # Evaluate clf's accuracy on the test set\n    print('{:s} : {:.3f}'.format(clf_name, log_loss_score))\n  ","991b7e97":"from sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import accuracy_score\n\ndef cross_val(X, y, model, params, folds=5):\n\n    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=21)\n    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n        print(f\"Fold: {fold}\")\n        x_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n        x_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n\n        alg = model(**params)\n        alg.fit(x_train, y_train,\n                eval_set=[(x_test, y_test)],\n                early_stopping_rounds=100,\n                verbose=400)\n\n        pred = alg.predict_proba(x_test)\n        log_loss_score = log_loss(y_test,pred)\n        \n#         accuracy = accuracy_score(y_test, pred)\n        print(f\" log_loss_score: {log_loss_score}\")\n        print(\"-\"*50)\n    return alg","3c69977f":"lgb_params= {'learning_rate': 0.045, \n             'n_estimators': 20000, \n             'max_bin': 94,\n             'num_leaves': 5, \n             'max_depth': 30, \n             'reg_alpha': 8.457, \n             'reg_lambda': 6.853, \n             'subsample': 0.749}","53ff09a2":"from lightgbm import LGBMClassifier\nlgb_model = cross_val(X, y, LGBMClassifier, lgb_params)\n","b482e119":"from xgboost import XGBClassifier\nclassifier = XGBClassifier(n_estimators = 10000,predictor = 'gpu_predictor',tree_method = 'gpu_hist',learning_rate = 0.01,max_depth=29,max_leaves = 31,eval_metric = 'mlogloss',verbosity = 3)\nclassifier.fit(X,y)","c5698b92":"y_pred=classifier.predict_proba(X_test)\nprint(\"log_loss_score_XGBOOST: \",log_loss(y_test,y_pred))","f0fd80a5":"features= X.columns\nfeatures","c6857092":"import missingno\nimport matplotlib as mpl\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier,plot_importance\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.model_selection import train_test_split,KFold, GroupKFold, StratifiedKFold\nimport warnings\nfrom sklearn.metrics import log_loss\nimport plotly.express as px\nfrom lightgbm import LGBMClassifier\n\nfrom catboost import CatBoostClassifier\ntest_preds = None\ntrain_rmse = 0\nval_rmse = 0\nn_splits = 5\n\nkf = KFold(n_splits = n_splits , shuffle = True , random_state = 0)\nfor fold, (tr_index , val_index) in enumerate(kf.split(X[features].values , y.values)):\n    \n    print(\"-\" * 50)\n    print(f\"Fold {fold + 1}\")\n    \n    x_train,x_val = X[features].values[tr_index] , X[features].values[val_index]\n    y_train,y_val = y.values[tr_index] , y.values[val_index]\n        \n    eval_set = [(x_val, y_val)]\n    cat_boost_model = CatBoostClassifier(depth=4,\n                               task_type=\"GPU\",\n            max_ctr_complexity=15,\n            iterations=17000,\n            od_wait=1000, od_type='Iter',\n            learning_rate=0.01,\n            min_data_in_leaf=1,\n            use_best_model=True,\n            loss_function='MultiClass')\n    cat_boost_model.fit(x_train, y_train, eval_set = eval_set, verbose = 500)\n    train_preds = cat_boost_model.predict(x_train)\n    train_rmse += mean_squared_error(y_train ,train_preds , squared = False)\n    print(\"Training RMSE : \" , mean_squared_error(y_train ,train_preds , squared = False))\n    \n    val_preds = cat_boost_model.predict(x_val)\n    val_rmse += mean_squared_error(y_val , val_preds , squared = False)\n    print(\"Validation RMSE : \" , mean_squared_error(y_val , val_preds , squared = False))\n    \n#     if test_preds is None:\n#         test_preds = model.predict_proba(test[cols].values)\n#     else:\n#         test_preds += model.predict_proba(test[cols].values)\nprint(\"-\" * 50)\nprint(\"Average Training RMSE : \" , train_rmse \/ n_splits)\nprint(\"Average Validation RMSE : \" , val_rmse \/ n_splits)\n\n# test_preds \/= n_splits\n","c170557b":"test_df= pd.read_csv(test_dir)\ntest_df.head()","407dc30c":"id_col= test_df['id']\ntest_df= test_df.drop('id',axis=1)","99d15ceb":"features_with_na=[features for features in test_df.columns if test_df[features].isnull().sum()>1]\n## 2- step print the feature name and the percentage of missing values\nlen(features_with_na)","cb4d852b":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX=test_df\nfeatures= X.columns\nX[features] = sc.fit_transform(X[features])\npca.fit(X)\nX","223ce7b7":"test_pred= cat_boost_model.predict_proba(X)\ntest_pred","e46d904f":"test_pred=pd.DataFrame(test_pred)\ntest_pred","da6f2295":"test_pred.columns = label_encoder.inverse_transform(test_pred.columns)\ntest_pred","0e2b8c07":"\n\nfinal_test= pd.concat([id_col,test_pred],axis=1)\nfinal_test.to_csv('result6.csv',index=False)","c2a17a3d":"df2= pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv')\ndf2.head()","38646d4f":"final_test","e9e0d14a":"# **EDA**","e026e357":"# Light BGM","a518e5bf":"**NO NULL VALUES FOUND IN EACH OF THE FEATURES**","fc992580":"# XGBOOST","54ba4eb8":"# **SUBMISSION**","26a7b6cb":"**MODELS USED LOGISTIC REGRESSION, KNN,DECISION TREE,RANDOM FOREST**","43d5b6e9":"**NO MISSING VALUES IN THE TEST DATASET TOO**","600d5d21":"# CATBOOST","f3419461":"# Finding Missing Values","66ebdb51":"# USING CATBOOST MODEL TO PREDICT","51b05ceb":"# Preparing the Data For the Models","a57d13a5":"**LET US LOOK AT THE DISTRIBUTION OF THE TARGET CLASS**","65298aba":"# **If you liked the Notebook Please Upvote**"}}