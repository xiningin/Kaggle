{"cell_type":{"a027b106":"code","c18cc71d":"code","8edfb3ca":"code","0ba6fa69":"code","c5aeff52":"code","cf820c5f":"code","82e3a02d":"code","e2676db6":"code","2cd17de8":"code","833797c4":"code","61929432":"code","ff975ad4":"code","3e488729":"code","f683e994":"code","8ddb00ae":"markdown","458bb785":"markdown","f482cac2":"markdown","8439b28c":"markdown","1e65ddfb":"markdown","00f58100":"markdown","60e54a4c":"markdown","c4f63488":"markdown","5e4315e0":"markdown","0511b373":"markdown","4450f849":"markdown","8b98000a":"markdown","20d24730":"markdown","b00c9fa5":"markdown","d99e2450":"markdown","63f4b615":"markdown"},"source":{"a027b106":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c18cc71d":"X_train = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv', index_col=\"Id\")\nX_test = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv', index_col=\"Id\")\n\ny = X_train['SalePrice']\nX_train.drop(axis=1, columns=['SalePrice'], inplace=True)","8edfb3ca":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\nnumeric_mean_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean'))])\n\nnumeric_zero_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value=0.0))])\n\ncategorical_none_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='none')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\ncategorical_mostfreq_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])","0ba6fa69":"num_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ['int64', 'float64']]\ncat_cols = [cname for cname in X_train.columns if X_train[cname].dtype == 'object']\n\nnum_cols_zeros, num_cols_mean, cat_cols_none, cat_cols_freq = [],[],[],[]\n\n# Here we decide whether to fill missing numeric values with 0.0 or the mean value\nfor column in num_cols:\n    if X_train[column].min() == 0:\n        num_cols_zeros.append(column)\n    else:\n        num_cols_mean.append(column)\n\n# Here we decide whether to fill missing categorical values with 'none' or the most frequent value\nfor column in cat_cols:\n    if X_train[column].isnull().sum() <= 30:\n        cat_cols_freq.append(column)\n    else:\n        cat_cols_none.append(column)\n\n# Here we define the parameters to be used in the column transformer\nfrom sklearn.compose import ColumnTransformer\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num_mean', numeric_mean_transformer, num_cols_mean),\n        ('num_zero', numeric_zero_transformer, num_cols_zeros),    \n        ('cat_none', categorical_none_transformer, cat_cols_none),\n        ('cat_freq', categorical_mostfreq_transformer, cat_cols_freq)])","c5aeff52":"from xgboost import XGBRegressor\n\nmodel = XGBRegressor()\n\nfull_pipe = Pipeline(steps=[\n    ('preprocessing', preprocessor),\n    ('model', model)])","cf820c5f":"full_pipe.fit(X_train, y)","82e3a02d":"preds = full_pipe.predict(X_test)","e2676db6":"params = { \n    'model__n_estimators': [200, 300, 500]\n}\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_search = GridSearchCV(full_pipe, params)\n                  \nparam_search.fit(X_train, y)\n\nprint(param_search.best_params_)    \nprint(param_search.best_score_)","2cd17de8":"from sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# This is just an example as a template for you\nclass Custom_Transformer_Example(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        return\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        return X","833797c4":"class Features(BaseEstimator, TransformerMixin):\n    \n    # Define parameters that can be used in the fit and transform method using 'self'\n    def __init__(self, year_bins=50, year_strategy='quantile', use_bins=True):\n        self.year_bins = year_bins\n        self.year_strategy = year_strategy\n        self.use_bins = use_bins\n    \n    def fit(self, X, y=None):\n        # Split year into bins\n        self.year_splitter = KBinsDiscretizer(n_bins=self.year_bins, encode='ordinal', strategy=self.year_strategy)\n        \n        # Fit the discretizer using the training data, where X will be X_train\n        self.year_splitter.fit(X[['YearBuilt']])\n        \n        # Return self, so that we can access the fitted discretizer in the transform method\n        return self\n    \n    def transform(self, X):\n        \n        # Make a copy of X so we don't modify the original (optional)\n        X_copy = X.copy()\n        \n        # The fitted discretizer can now be used to transform X (which can be train or test data)\n        if self.use_bins:\n            X_copy['YearBuiltBin'] = self.year_splitter.transform(X_copy[['YearBuilt']])\n            \n        return X_copy","61929432":"class Imputer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        \n        # Define the imputers and encoder\n        self.mean_imputer = SimpleImputer(strategy='mean')\n        self.zero_imputer = SimpleImputer(strategy='constant', fill_value=0.0)\n        self.none_imputer = SimpleImputer(strategy='constant', fill_value='none')\n        self.freq_imputer = SimpleImputer(strategy='most_frequent')\n    \n    def fit(self, X, y=None):\n        \n        # Fit the imputers\n        for cname in num_cols_mean:\n            self.mean_imputer.fit(X[[cname]])\n            \n        for cname in num_cols_zeros:\n            self.zero_imputer.fit(X[[cname]])\n            \n        for cname in cat_cols_none:\n            self.none_imputer.fit(X[[cname]])\n            \n        for cname in cat_cols_freq:\n            self.freq_imputer.fit(X[[cname]])\n        \n        return self\n    \n    def transform(self, X):\n        \n        # Make a copy so that X is not changed\n        X_copy = X.copy()\n           \n        # Transform the imputers\n        for cname in num_cols_mean:\n            X_copy[cname] = self.mean_imputer.transform(X[[cname]])\n            \n        for cname in num_cols_zeros:\n            X_copy[cname] = self.zero_imputer.transform(X[[cname]])\n            \n        for cname in cat_cols_none:\n            X_copy[cname] = self.none_imputer.transform(X[[cname]])\n            \n        for cname in cat_cols_freq:\n            X_copy[cname] = self.freq_imputer.transform(X[[cname]])\n        \n        return X_copy","ff975ad4":"class Encoder(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n    \n    def fit(self, X, y=None):\n        \n        # Fit the encoder\n        self.OH_encoder.fit(X[cat_cols])\n        \n        return self\n    \n    def transform(self, X):\n        \n        # Make a copy so that X is not changed\n        X_copy = X.copy()\n        \n        # Transform X using the fitted encoder\n        OH_X = pd.DataFrame(self.OH_encoder.transform(X[cat_cols]))\n        OH_X.index = X_copy.index\n        \n        X_copy = X_copy.drop(cat_cols, axis=1)\n        X_copy = pd.concat([X_copy, OH_X], axis=1)\n        \n        return X_copy","3e488729":"full_pipe = Pipeline(steps=[\n    ('impute', Imputer()),\n    ('encode', Encoder()),\n    ('add_features', Features()),\n    ('model', model)])","f683e994":"# Parameters to optimise\nparams = { \n    'model__n_estimators': [200, 300, 400],\n    'add_features__year_bins': [4, 5],\n    'add_features__year_strategy': ['quantile', 'uniform'],\n    'add_features__use_bins': [True, False]\n}\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_search = GridSearchCV(full_pipe, params)\n    \nparam_search.fit(X_train, y)\n\nprint(\"\\nGrid search:\")\ncv_results = param_search.cv_results_\nfor mean_score, params in zip(cv_results[\"mean_test_score\"], cv_results[\"params\"]):\n    print(-1*mean_score, params)\n\nprint(\"\\nBest score: {}, with params: {}\".format(param_search.best_score_, param_search.best_params_))    ","8ddb00ae":"Now let's add our our new transformers into the pipeline! ","458bb785":"Before building the pipeline I am loading both the training and test data. We will use cross-validation so we do not need to split the training data into training and validation sets.","f482cac2":"We can now use the pipeline we created to cross validate and optimise parameters using the pipeline. To adjust the model parameters we can use:\n\n    model__n_estimators: [200, 300]\n    \nHere's an exmample:","8439b28c":"Here we introduct the column transformer, which allows us to pass the column names to each of the transformers we defined above. We do some simple sorting to determine how to impute our values. \n\n**It's important to note that the ColumnTransformer class takes an input X and a list of columns, and then applies the transforms to the values of X based on the columns provided and then joins the output of each column transformer.**","1e65ddfb":"Now that the preprocessor and model have both had the .fit() method applied we can make a prediction on the test data. It's important to note here that the test data will have the mean and most frequent values from the training data applied (since we want to minimise data leakage, this is great).","00f58100":"And the encoder:","60e54a4c":"# Using a pipeline with custom transformers\nLet's say we want to now do some feature engineering, but we want to experiment with some parameters to see how they change the performance of the model. In this case we'll split a numeric feature into buckets. But how do we do this in a Pipeline I hear you cry? Fortunately SKlearn has got us covered. We'll create our own transformer.\n\nDon't get scared! I promise it's much simpler than it looks. This is a custom transformer used in a pipeline. The basic custom transformer is shown below, it's a class inheriting from the 'BaseEstimator' and 'TransformerMixin' SKLearn classes. ","c4f63488":"* We can use __init__ to define any parameters that we want to test. \n* We use fit to do any calculations needed to transform the data, in most cases the fit method is only called on the training data.\n* We use transform to transform the data, based on what we've calculated in the fit method.\n\nMaybe our model will perform better if we split some continuous data into buckets, and use it as a categorical feature. For example we'll try splitting 'YearBuilt' into categories. Here's the example of splitting data into buckets:","5e4315e0":"This is just an example, in this case it doesn't make any difference splitting the year the house was built into 4 bins (or groups).\n\nI hope you found this notebook useful and have a better idea for creating your own custom transformers in pipelines. Please let me know if there's anything that's not clear and feel free to ask any questions.\n\nAn upvote would be awesome! :)\n\nThanks,\nAli","0511b373":"The next step is to create a pipeline that combines the preprocessor created above with a regressor. In this case I have used a simple XGBoost to start with.","4450f849":"Now let's see how well it performs when we adjust the new feature parameters using GridSearch and cross-validation. Notice how we use the prefix the custom transformer followed by underscore, underscore, for example add_features__ for our new parameters.","8b98000a":"We're almost ready to add our new feature creation step into the pipeline! \n\nFrustratingly we lose the column names using ColumnTransformer, and it's not easy to get them back. So we'll quickly make 2 custom transformers that do the same thing as our original ColumnTransformer (only without scaling), but keeps the column names.","20d24730":"In this guide, i'll show you how to use the Pipeline and ColumnTransformer SKLearn classes, and then expand onto making your own custom transformers.","b00c9fa5":"# Using a pipeline and column transformer\n\nThe first steps in building a pipeline is to specify the transformer types. By looking at the data in the house prices data set we will need to impute with the following values:\n* The mean of the data, this can be used for continuous numeric data with an order, for example 'LotArea'\n* 0.0, this can be used where the feature has a minimum, for example \n* 'none', this is used for categorical data where NaN actually means none, for example 'BsmtQual' NA means no basement\n* The most frequent value in the data, this can be used for categorical data where there are a few missing values\n\nThe Pipeline class allows access to the .fit and fit_transform methods for the Imputers within the pipeline. For example:\n\n    num_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ['int64', 'float64']]\n    numeric_mean_transformer.fit(X_train[num_cols])\n    \nWould caluclate the mean for each of the numeric columns.\n\n    numeric_mean_transformer.transform(X_train[num_cols])\n    \nWould then fill any NaN values with the mean calculated in the .fit method. Now you could easily apply the same transform to the test data, and you've ensured you don't have any data leakage as you are using the mean of the training data. For example:\n\n    numeric_mean_transformer.transform(X_test[num_cols])\n    \nIn the code below I have created four transformers. The numeric_mean_transformer will fill missing values with the mean and then scale them. The numeric_zero_transformer will fill missing values with 0.0 and then scale them. The categorical_none_transformer will fill missing values with 'none' and then OneHot encode them, finally the categorical_mostfreq_transformer will fill missing values with the most frequent value and then OneHot encode them.\n\n**It's important to note that the Pipeline class, takes an input X, and then works its way down each transformer in the pipeline, passing the transformed value of X onto the next transformer.**","d99e2450":"We can now call the fit method on the training data. For every preprocessing step .fit_transform() will have been applied, and for the model .fit() has been applied.","63f4b615":"# Introduction\nIn most machine learning projects there are many transformational steps to prepare your data. A few common steps are imputing missing values, encoding categorical variables and scaling features. Luckily the [SciKit-learn preprocessing package](https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html#preprocessing) has built in functions for many of these common transformations.\n\nHowever, in a typical machine learning workflow you will need to apply all these transformations many times. First on the training data, again on any test data and again on new data you are using to make predictions. \n\nOf course you could write a function to apply the transformations but you would still need to run this first and then call the model separately. Scikit-learn pipelines are a tool to simplify this process. If you plan to use cross-validation then pipelines make it easy to apply transformations for every fold, reducing data leakage.\n\nThey have several key benefits:\n* They make your workflow much easier to read and understand.\n* They enforce the implementation and order of steps in your project.\n* They make it very easy to find the optimal model and preprocessing parameters.\n\n# Two approaches to pipelines\nWe'll go through the following different approaches and evaluate their performance.\n1. **Using a pipeline and column transformer**\n2. **Using a pipeline with custom transformers**"}}