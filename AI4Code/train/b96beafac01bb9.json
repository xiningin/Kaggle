{"cell_type":{"f647e086":"code","aa29218f":"code","caec862d":"code","4dcc1cb5":"code","0374fa10":"code","ef553560":"code","fa5f562c":"code","35707adc":"code","d92e22fc":"code","3046a5e9":"code","116dc4ab":"code","811f950a":"code","667c1ef4":"code","dfad5a3f":"code","fc9e8c21":"code","5cc7c070":"code","2ec16065":"code","b8ff0e66":"code","267c36da":"code","f41ff26a":"code","dca0cd3f":"code","13200f8d":"code","844a6c9c":"code","40f8fbc7":"code","0cbd4699":"code","996cee02":"code","0795fc53":"code","b6c2ac04":"code","b15e5764":"code","3e2dfa55":"code","d1dd93ef":"code","eaaffc11":"markdown","f87c969a":"markdown","f85ef64d":"markdown","2f3c2714":"markdown","dbb96189":"markdown","4985b006":"markdown","76abf4cc":"markdown","a112906a":"markdown","a60cc45d":"markdown","773a0630":"markdown","b39f3eb4":"markdown","d1018ee9":"markdown","f93a1e9a":"markdown","d60095c1":"markdown","d9ce260c":"markdown","e9c88aeb":"markdown","fc135059":"markdown","968291fa":"markdown","92ddfce1":"markdown","a1759851":"markdown","4a520720":"markdown","8c8abd0e":"markdown","7752aa3b":"markdown","19ca009e":"markdown","03325c8e":"markdown","616964d3":"markdown","0bb9ff8d":"markdown","914b4ad5":"markdown","34f03a9c":"markdown","7e11acbf":"markdown"},"source":{"f647e086":"import numpy as np\nimport pandas as pd\nimport os\nfrom scipy.signal import hilbert\nfrom scipy.signal import hann\nfrom scipy.signal import convolve\nimport copy\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error, f1_score\npd.options.display.precision = 15\nfrom collections import defaultdict\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cat\nimport time\nfrom collections import Counter\nimport datetime\nfrom catboost import CatBoostRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit, RepeatedStratifiedKFold\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn import linear_model\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom bayes_opt import BayesianOptimization\n# import eli5\nimport shap\nfrom IPython.display import HTML\nimport json\nimport altair as alt\nfrom category_encoders.ordinal import OrdinalEncoder\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom typing import List\n\nimport os\nimport time\nimport datetime\nimport json\nimport gc\nfrom numba import jit\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn import metrics\nfrom typing import Any\nfrom itertools import product\npd.set_option('max_rows', 500)\nimport re\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed","aa29218f":"path = '\/kaggle\/input\/m5-forecasting-accuracy'\ntrain_sales = pd.read_csv(f'{path}\/sales_train_validation.csv')\ncalendar = pd.read_csv(f'{path}\/calendar.csv')\nsubmission = pd.read_csv(f'{path}\/sample_submission.csv')\nsell_prices = pd.read_csv(f'{path}\/sell_prices.csv')","caec862d":"calendar","4dcc1cb5":"train_sales","0374fa10":"sell_prices","ef553560":"submission","fa5f562c":"train_sales.loc[train_sales['item_id'] == 'HOBBIES_1_001']","35707adc":"sell_prices.loc[sell_prices['item_id'] == 'HOBBIES_1_001']","d92e22fc":"plt.figure(figsize=(12, 4))\nfor i in range(10):\n    plt.plot(train_sales.loc[train_sales['item_id'] == 'HOBBIES_1_002'].iloc[i, 6:].values,\n             label=train_sales.loc[train_sales['item_id'] == 'HOBBIES_1_002'].iloc[i, 5]);\nplt.title('HOBBIES_1_002 sales')\nplt.legend();","3046a5e9":"plt.figure(figsize=(12, 4))\nfor i in range(10):\n    plt.plot(train_sales.loc[train_sales['item_id'] == 'HOBBIES_1_002'].iloc[i, 6:].rolling(30).mean().values,\n             label=train_sales.loc[train_sales['item_id'] == 'HOBBIES_1_002'].iloc[i, 5]);\nplt.title('HOBBIES_1_002 sales, rolling mean 30 days')\nplt.legend();\n\nplt.figure(figsize=(12, 4))\nfor i in range(10):\n    plt.plot(train_sales.loc[train_sales['item_id'] == 'HOBBIES_1_002'].iloc[i, 6:].rolling(60).mean().values,\n             label=train_sales.loc[train_sales['item_id'] == 'HOBBIES_1_002'].iloc[i, 5]);\nplt.title('HOBBIES_1_002 sales, rolling mean 60 days')\nplt.legend();\n\nplt.figure(figsize=(12, 4))\nfor i in range(10):\n    plt.plot(train_sales.loc[train_sales['item_id'] == 'HOBBIES_1_002'].iloc[i, 6:].rolling(90).mean().values,\n             label=train_sales.loc[train_sales['item_id'] == 'HOBBIES_1_002'].iloc[i, 5]);\nplt.title('HOBBIES_1_002 sales, rolling mean 90 days')\nplt.legend();\n","116dc4ab":"item_prices = sell_prices.loc[sell_prices['item_id'] == 'HOBBIES_2_001']\nfor s in item_prices['store_id'].unique():\n    small_df = item_prices.loc[item_prices['store_id'] == s]\n    plt.plot(small_df['wm_yr_wk'], small_df['sell_price'], label=s)\nplt.legend()\nplt.title('HOBBIES_2_001 sell prices');","811f950a":"train_sales.loc[train_sales['store_id'] == 'CA_1']","667c1ef4":"sell_prices.loc[sell_prices['store_id'] == 'CA_1']","dfad5a3f":"ca_1_sales = train_sales.loc[train_sales['store_id'] == 'CA_1']\npd.crosstab(ca_1_sales['cat_id'], ca_1_sales['dept_id'])","fc9e8c21":"plt.figure(figsize=(12, 4))\nfor d in ca_1_sales['dept_id'].unique():\n    store_sales = ca_1_sales.loc[ca_1_sales['dept_id'] == d]\n    store_sales.iloc[:, 6:].sum().rolling(30).mean().plot(label=d)\nplt.title('CA_1 sales by department, rolling mean 30 days')\nplt.legend(loc=(1.0, 0.5));","5cc7c070":"item_prices = sell_prices.loc[sell_prices['item_id'] == 'HOBBIES_2_001']\nfor s in item_prices['store_id'].unique():\n    small_df = item_prices.loc[item_prices['store_id'] == s]\n    plt.plot(small_df['wm_yr_wk'], small_df['sell_price'], label=s)\nplt.legend()\nplt.title('HOBBIES_2_001 sell prices');","2ec16065":"ca_1_prices = sell_prices.loc[sell_prices['store_id'] == 'CA_1']\nca_1_prices['dept_id'] = ca_1_prices['item_id'].apply(lambda x: x[:-4])\n","b8ff0e66":"plt.figure(figsize=(12, 6))\nfor d in ca_1_prices['dept_id'].unique():\n    small_df = ca_1_prices.loc[ca_1_prices['dept_id'] == d]\n    grouped = small_df.groupby(['wm_yr_wk'])['sell_price'].mean()\n    plt.plot(grouped.index, grouped.values, label=d)\nplt.legend(loc=(1.0, 0.5))\nplt.title('CA_1 mean sell prices by dept');","267c36da":"train_sales.loc[train_sales['dept_id'] == 'HOBBIES_1']","f41ff26a":"train_sales.loc[train_sales['dept_id'] == 'HOBBIES_1', 'item_id'].nunique()","dca0cd3f":"sell_prices.loc[sell_prices['item_id'].str.contains('HOBBIES_1')]","13200f8d":"hobbies_1_sales = train_sales.loc[train_sales['dept_id'] == 'HOBBIES_1']\nplt.figure(figsize=(12, 6))\nfor d in hobbies_1_sales['store_id'].unique():\n    store_sales = hobbies_1_sales.loc[hobbies_1_sales['store_id'] == d]\n    store_sales.iloc[:, 6:].sum().rolling(30).mean().plot(label=d)\nplt.title('HOBBIES_1 sales by stores, rolling mean 30 days')\nplt.legend(loc=(1.0, 0.5));","844a6c9c":"sell_prices.head()","40f8fbc7":"hobbies_1_prices = sell_prices.loc[sell_prices['item_id'].str.contains('HOBBIES_1')]\nplt.figure(figsize=(12, 6))\nfor d in hobbies_1_prices['store_id'].unique():\n    small_df = hobbies_1_prices.loc[hobbies_1_prices['store_id'] == d]\n    grouped = small_df.groupby(['wm_yr_wk'])['sell_price'].mean()\n    plt.plot(grouped.index, grouped.values, label=d)\nplt.legend(loc=(1.0, 0.5))\nplt.title('HOBBIES_1 mean sell prices by store');","0cbd4699":"train_sales.loc[train_sales['state_id'] == 'CA']","996cee02":"for col in ['item_id', 'dept_id', 'store_id']:\n    print(f\"{col} has {train_sales.loc[train_sales['state_id'] == 'CA', col].nunique()} unique values for CA state\")","0795fc53":"ca_sales = train_sales.loc[train_sales['state_id'] == 'CA']\nplt.figure(figsize=(12, 6))\nfor d in ca_sales['store_id'].unique():\n    store_sales = ca_sales.loc[ca_sales['store_id'] == d]\n    store_sales.iloc[:, 6:].sum().rolling(30).mean().plot(label=d)\nplt.title('CA sales by stores, rolling mean 30 days')\nplt.legend(loc=(1.0, 0.5));","b6c2ac04":"ca_prices = sell_prices.loc[sell_prices['store_id'].str.contains('CA')]\nplt.figure(figsize=(12, 6))\nfor d in ca_prices['store_id'].unique():\n    small_df = ca_prices.loc[ca_prices['store_id'] == d]\n    grouped = small_df.groupby(['wm_yr_wk'])['sell_price'].mean()\n    plt.plot(grouped.index, grouped.values, label=d)\nplt.legend(loc=(1.0, 0.5))\nplt.title('Mean sell prices by store in CA');","b15e5764":"train_sales.head()","3e2dfa55":"plt.figure(figsize=(12, 8))\ndept_grouped_sales = train_sales.groupby(['dept_id']).sum()\nfor i, row in dept_grouped_sales.iterrows():\n    plt.plot(row.values, label=i);\nplt.legend(loc=(1.0, 0.5))\nplt.title('Sales by departments');","d1dd93ef":"plt.figure(figsize=(12, 4))\nfor i, row in dept_grouped_sales.iterrows():\n    plt.plot(row.rolling(30).mean().values, label=i);\nplt.title('Sales by department, rolling mean 30 days')\nplt.legend(loc=(1.0, 0.5));\n\nplt.figure(figsize=(12, 4))\nfor i, row in dept_grouped_sales.iterrows():\n    plt.plot(row.rolling(60).mean().values, label=i);\nplt.title('Sales by department, rolling mean 60 days')\nplt.legend(loc=(1.0, 0.5));\n\nplt.figure(figsize=(12, 4))\nfor i, row in dept_grouped_sales.iterrows():\n    plt.plot(row.rolling(90).mean().values, label=i);\nplt.title('Sales by department, rolling mean 90 days')\nplt.legend(loc=(1.0, 0.5));\n","eaaffc11":"### All info about a single department\n\nNow we can analyse how different items of the same department are sold in different stores.","f87c969a":"Not surprisingly `calender` has data about dates: for each date we get info about dayofweek\/week\/month\/year, events and flags showing whether stores allowed purchases with SNAP food stamps. We have the data for all the dates.","f85ef64d":"### Aggregations over department\n\nNow let's look at various aggregations over the data","2f3c2714":"We can see that `FOOD_3` has much higher sales than any other department.","dbb96189":"We can see that prices on `HOBBIES_1` increase over time, but for other categories they are quite stable.","4985b006":"What a strange situation. I suppose there were some sales with lowered prices.","76abf4cc":"We have information about sales of 3049 various items, which belong to different categories and departments.","a112906a":"We can see that there were several points of time when the price increased.","a60cc45d":"### All info about a single state\n\nNow we can analyse how different items are sold in different stores of the same state.","773a0630":"We can see that there are 416 unique items in this department and they are sold in all stores.","b39f3eb4":"### All info about a single item\n\nLet's start with looking at all the data for a single item.","d1018ee9":"Well... this doesn't look pretty. Let's make is smoother - I'll plot rolling mean over 30 days","f93a1e9a":"This store (and I suppose other stores) have 3 categories: foods, hobbies and households, which have 2-3 departments.","d60095c1":"## Data exploration","d9ce260c":"And here we have information on sell prices for all items in all stores by weeks.submission","e9c88aeb":"Our submission file should contain the forecast for the next 56 days (28 public, 28 private). We make predictions for each item in each store.","fc135059":"Also we have the data about sell prices in all 10 stores.","968291fa":"We can see a lot of interesting things:\n* ca_3 store always has higher sales;\n* ca_1 has a little increasing trend;\n* ca_2 had a long decline and then had a very steep increase in sales;","92ddfce1":"## Import libraries","a1759851":"So what do we see here?\n* there is a definite seasonality with several peaks;\n* the sales are more or less constant and quite low (max sales per day in one store is 11);\n* as a result, it could be difficult to predict such low values;","4a520720":"Here we have information about item characteristics and sales for each day","8c8abd0e":"Not only sales grow over time, the prices also grow. We can see that there were several points of time when the price increased.","7752aa3b":"We can see a definite increase of sales over time. And we can see that CA_1 and CA_3 stores have higher sales than other stores.","19ca009e":"Interesting that `FOODS_1` has much higher sales than any other department.","03325c8e":"## Loading the data and overview","616964d3":"Now, let's make some plots. I'd like to see sales over time in different shops.","0bb9ff8d":"## M5 competition\n\nThis competition is forecasting competition, where we aim to make correct predictions for sales 28 days in the future. Or, more precisely, 28 days in public dataset and 28 days in private dataset. When one month is left before the deadline, the labels for the 28 public days will be released.\n\nIt is quite interesting that there are two competitions for the same data, the difference is in the metric and values to predict:\n* in this competition we make point forecasts and the metric is Weighted Root Mean Squared Scaled Error (WRMSSE);\n![](https:\/\/i.imgur.com\/uqhsf3d.png)\n![](https:\/\/i.imgur.com\/B1hglCf.png)\n* in another competition we make predictions for quantiles and the metric is Weighted Scaled Pinball Loss (WSPL)\n![](https:\/\/i.imgur.com\/J8XAQP4.png)\n![](https:\/\/i.imgur.com\/jzLckus.png)\n![](https:\/\/i.imgur.com\/3ihaSZO.png)\n\nI plan to do some extensive EDA as I'm interested in this data! And then, of course, we will build some models.\nThere is a lot of things in this data, so I'll go step by step.\n\n\n![](https:\/\/i.imgur.com\/C5hASXe.png)\n\n*Work in progress :)*","914b4ad5":"Also we have the data about sell prices for all items in this store.","34f03a9c":"### All info about a single store\n\nNow that we know how the data about a single item looks like, let's have a look at one store.","7e11acbf":"We can see that this item (as well as all other items in fact) is sold in 10 stores in 3 states. This item belongs to `HOBBIES` category and is sold in `HOBBIES_1` department."}}