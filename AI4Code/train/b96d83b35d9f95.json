{"cell_type":{"a44ed8f2":"code","f0a04ccb":"code","4ca1d016":"code","a7ec8f43":"code","ee8f7109":"code","a2c091d1":"code","dcbde131":"code","c29ad549":"code","c0d3d485":"code","800a9463":"code","675013e0":"code","2eaaa13d":"code","5bb4d819":"code","988b8730":"code","e6c0b2af":"code","aebef192":"code","ea62dcb8":"code","54017f8a":"code","e649eaec":"code","3871ec9c":"code","476f78a5":"code","ef4a9553":"code","2a75a5a6":"code","80d20e44":"code","fd55fa05":"code","6b4af611":"code","f4c175cc":"code","ed29290d":"code","99fc0656":"code","b846f4f9":"code","c62117c8":"code","67e6d25b":"code","b466b133":"code","8777e2c1":"code","1e737c1f":"code","ac087b9e":"code","c94065a0":"code","3af8e627":"code","dea70087":"code","a47908b7":"code","04f22242":"code","08e743be":"code","9e626bdc":"code","3593378f":"code","8eabe5f9":"code","8cab7fea":"code","aa31d410":"code","a90c5f43":"code","fe46d41a":"code","feb883d7":"code","431467de":"code","7a5ddd84":"code","86fa03b0":"code","e97c6e26":"code","99207624":"code","28a006f4":"code","5793b704":"code","893a9f18":"code","e83c8327":"code","f803127e":"code","cd144fe3":"code","006dd466":"code","6e77e622":"code","806b6fc2":"code","e2fcce62":"code","e757e1e0":"code","d94e799f":"code","d20167ee":"markdown","b96e51a0":"markdown","6d0c3fe0":"markdown","ac4f458f":"markdown","0ec59e74":"markdown","ecfd3e88":"markdown","3b8284aa":"markdown","4206d0e3":"markdown","75e69253":"markdown","3a04dab4":"markdown","37c39c90":"markdown","4cf2fcde":"markdown","150a431e":"markdown","bebba91f":"markdown","3a79fac4":"markdown","c28dd6a5":"markdown","0f80b98a":"markdown","0708de4d":"markdown","72a77ba4":"markdown","04339877":"markdown","81f1c950":"markdown","488ca228":"markdown","f5745ed6":"markdown","8a3eef8d":"markdown","7987d58d":"markdown","f9f06cec":"markdown","27b96c8c":"markdown","82b8cbfc":"markdown","a9ab3d33":"markdown","a82d7aaf":"markdown","2a1136b2":"markdown","8c070a0c":"markdown"},"source":{"a44ed8f2":"import warnings\nwarnings.filterwarnings(\"ignore\")","f0a04ccb":"#DF\nimport pandas as pd\nimport numpy as np\nfrom pandas.plotting import scatter_matrix\n\n#Common Model Algorithms\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeRegressor\n\n#Common Model Helpers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import roc_auc_score as auc\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import confusion_matrix\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')","4ca1d016":"from IPython.display import display_html \n\nsample_submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\n\nsample_head = sample_submission.head()\nsample_tail = sample_submission.tail()\nsample_sample = sample_submission.sample(5)\n\ndf1_styler = sample_head.style.set_table_attributes(\"style='display:inline'\").set_caption('Head')\ndf2_styler = sample_tail.style.set_table_attributes(\"style='display:inline'\").set_caption('Tail')\ndf3_styler = sample_sample.style.set_table_attributes(\"style='display:inline'\").set_caption('Random Sample')\n\ndisplay_html(df1_styler._repr_html_()+df2_styler._repr_html_()+ df3_styler._repr_html_(), raw=True)","a7ec8f43":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","ee8f7109":"train.shape","a2c091d1":"train.head()","dcbde131":"train.tail()","c29ad549":"train.sample(5)","c0d3d485":"train.info()","800a9463":"train.isna().sum().sort_values(ascending = False)","675013e0":"train.describe()","2eaaa13d":"survived = len(train[train['Survived'] ==1])\ndeaths = len(train[train['Survived'] ==0])\n\nprint(f\"The number of people who survived is {survived}.\")\nprint()\nprint(f\"The number of people who died is {deaths}.\")","5bb4d819":"num_cols = train[train.select_dtypes(exclude=['object']).columns]\ncat_cols = train[train.select_dtypes(include=['object']).columns]\n\nprint(f'The numerical cols are: {list(num_cols)} ({len(list(num_cols))}).\\n')\nprint(f'The categorical cols are: {list(cat_cols)} ({len(list(cat_cols))}).\\n')\nprint(f'Total number of cols: {len(list(train))}')","988b8730":"def tight():\n    plt.tight_layout()\n    plt.show()","e6c0b2af":"fig, (ax1,ax2) = plt.subplots(ncols = 2, nrows =1,figsize =(12,5))\n\nsns.countplot(train['Sex'], hue = train['Survived'],ax =ax1)\nax2.pie(train['Sex'].value_counts()\n        ,shadow = True,wedgeprops = {'edgecolor':'black'},autopct='%1.1f%%',labels =['M','F'])\n\ntight()","aebef192":"fig, ax = plt.subplots(figsize =(12,5))\n\nsns.countplot(train['Embarked'], hue = train['Survived'])\n\ntight()","ea62dcb8":"sns.catplot(data = train, kind = 'point', x = 'Pclass', y ='Survived', col ='Embarked', hue ='Sex')\n\ntight()","54017f8a":"corr_matrix = num_cols.corr()\n\ndisplay(corr_matrix['Survived'].sort_values(ascending = False))\nattributes = list(num_cols)\n\nscatter_matrix(num_cols[attributes],figsize = (12,8))\n\ntight()","e649eaec":"fig, ax = plt.subplots(figsize = (12,7))\ncolormap = sns.diverging_palette(220, 10, as_cmap = True)\n\nsns.heatmap(corr_matrix,linewidth = 0.01,vmax=1.0,square = True, cmap = colormap,annot = True)\n\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\n\ntight()","3871ec9c":"sns.lmplot(data = train, x ='Fare',y = 'Survived',hue = 'Sex',height = 5,aspect =2)\n\ntight()","476f78a5":"g = sns.FacetGrid( train, hue = 'Survived', aspect=4 )\n\ng.map(sns.kdeplot, 'Fare', shade= True )\ng.add_legend()\n\ntight()","ef4a9553":"fig = plt.figure(figsize=(12, 18))\n\nfor i in range(len(num_cols.columns)):\n    fig.add_subplot(10, 4, i+1)\n    sns.boxplot(y=num_cols.iloc[:,i])\n\ntight()","2a75a5a6":"fig = plt.figure(figsize=(12,18))\n\ntry:\n    for i in range(len(num_cols.columns)):\n        fig.add_subplot(10,4,i+1)\n        sns.distplot(num_cols.iloc[:,i].dropna())\n        plt.xlabel(num_cols.columns[i])\nexcept RuntimeError:\n    pass\n\ntight()","80d20e44":"fig, (ax1,ax2) = plt.subplots(ncols = 2, nrows =1, figsize = (12,5), sharex = True)\n\nsns.countplot(y= train['SibSp'],ax =ax1,palette = 'Reds_d')\nsns.countplot(y =train['Parch'],ax =ax2, palette ='Blues_d')\n\ntight()","fd55fa05":"sns.lmplot(data = train, x = 'Survived', y ='Pclass', hue = 'Sex')","6b4af611":"fig, (ax1,ax2,ax3) = plt.subplots(ncols = 3, nrows = 1, figsize = (14,5))\n\nsns.pointplot(x = 'Survived', y= 'Fare', hue = 'Sex', data = train,ax =ax1)\nsns.pointplot(x = 'Survived', y= 'Pclass', hue = 'Sex', data = train,ax= ax2)\nsns.pointplot(x = 'Survived', y= 'SibSp', hue = 'Sex', data = train,ax= ax3)\n\ntight()","f4c175cc":"sns.catplot(data = train, kind = 'bar', col ='Pclass', x ='Survived', y = 'Fare', hue = 'Sex')\nsns.catplot(data = train, kind = 'bar', col ='Pclass', x ='Survived', y = 'Age', hue = 'Sex')\n\ntight()","ed29290d":"X = train.copy()\ny = train['Survived']","99fc0656":"X = X.drop('Survived',axis=1)","b846f4f9":"X.isnull().sum().sort_values(ascending = False).head(1) #drop this useless as shit","c62117c8":"display(corr_matrix['Survived'].sort_values(ascending = False))","67e6d25b":"X = X.drop('Cabin',axis =1)","b466b133":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=42)","8777e2c1":"cat = list(X[X.select_dtypes(include=['object']).columns])\nnum = list(X[X.select_dtypes(exclude=['object']).columns])\nmy_cols = cat + num","1e737c1f":"X_train = X_train[my_cols].copy()\nX_valid = X_valid[my_cols].copy()\nX_test = test[my_cols].copy()","ac087b9e":"num_transformer = Pipeline(steps=[\n    ('num_imputer', SimpleImputer(strategy='median')),\n    ('std_scaler', StandardScaler())\n    ])\n\ncat_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_transformer, num),       \n        ('cat',cat_transformer,cat),\n        ])","c94065a0":"print(\"Data Shape: {}\".format(train.shape))\nprint(\"X_train Shape: {}\".format(X_train.shape))\nprint(\"y_train Shape: {}\".format(y_train.shape))","3af8e627":"X_train_prepared = preprocessor.fit_transform(X_train)\nX_valid_prepared = preprocessor.transform(X_valid)\n\nprint(X_train_prepared.shape)\nprint(X_valid_prepared.shape)","dea70087":"def display_scores(scores):\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())","a47908b7":"tree = DecisionTreeClassifier(max_depth =2)\n\ntree.fit(X_train_prepared, y_train)\ntree_predictions = tree.predict(X_train_prepared)\n\nprint(\"\\nAccuracy Score for Decision Tree Classifier is: \" + str(tree.score(X_train_prepared, y_train)))","04f22242":"tree_scores = cross_val_score(tree, X_train_prepared, y_train,scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-tree_scores)\n\ndisplay_scores(tree_rmse_scores)","08e743be":"forest = RandomForestClassifier(n_estimators = 10)\n\nforest.fit(X_train_prepared, y_train)\nforest_predictions = forest.predict(X_train_prepared)\n\nprint(\"\\nAccuracy Score for Random Forest Classifier is: \" + str(forest.score(X_train_prepared, y_train)))","9e626bdc":"forest_predictions = forest.predict(X_train_prepared)","3593378f":"forest_scores = cross_val_score(forest, X_train_prepared, y_train,scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\n\ndisplay_scores(forest_rmse_scores)","8eabe5f9":"param_grid = {'bootstrap': [True, False],\n 'max_depth': [1,2,3],\n 'max_features': ['auto', 'sqrt'],\n 'min_samples_leaf': [1, 2, 4],\n 'n_estimators': [10, 100]}\n\nforest_cv = GridSearchCV(forest, param_grid, n_jobs= 1, cv =10,scoring='roc_auc')\n                  \nforest_cv.fit(X_train_prepared, y_train)\nprint(forest_cv.best_params_)    \nprint(forest_cv.best_score_)","8cab7fea":"forest_cv_predictions = forest_cv.predict(X_train_prepared)","aa31d410":"svc = SVC(gamma ='auto')\n\nsvc.fit(X_train_prepared, y_train)\nsvc_predictions = forest.predict(X_train_prepared)\n\nprint(\"\\nAccuracy Score for SVC is: \" + str(svc.score(X_train_prepared, y_train)))","a90c5f43":"svc_scores = cross_val_score(svc, X_train_prepared, y_train,scoring=\"neg_mean_squared_error\", cv=10)\nsvc_rmse_scores = np.sqrt(-svc_scores)\n\ndisplay_scores(svc_rmse_scores)","fe46d41a":"gb = GradientBoostingClassifier(random_state = 10, learning_rate = 0.01)\n\ngb.fit(X_train_prepared, y_train)\ngb_predictions = gb.predict(X_train_prepared)\n\nprint(\"\\nAccuracy Score for Gradiant Boosting Classifieris: \" + str(gb.score(X_train_prepared, y_train)))","feb883d7":"gb_scores = cross_val_score(gb, X_train_prepared, y_train,scoring=\"neg_mean_squared_error\", cv=10)\ngb_rmse_scores = np.sqrt(-gb_scores)\n\ndisplay_scores(gb_rmse_scores)","431467de":"param_grid = { \n    \"learning_rate\": [0.01, 0.05, 0.1],\n    \"max_depth\":[3,5,8],\n    \"n_estimators\":[250,500]}\n\ngb_cv = GridSearchCV(gb, param_grid, n_jobs= 1, cv =5,scoring ='roc_auc')\n                \ngb_cv.fit(X_train_prepared, y_train)\nprint(gb_cv.best_params_)    \nprint(gb_cv.best_score_)","7a5ddd84":"logreg = LogisticRegression(solver = 'liblinear')\n\nlogreg.fit(X_train_prepared, y_train)\nlogreg_predictions = logreg.predict(X_train_prepared)\n\nprint(\"\\nAccuracy Score for Logistic Regression is: \" + str(logreg.score(X_train_prepared, y_train)))","86fa03b0":"logreg_scores = cross_val_score(logreg, X_train_prepared, y_train,scoring=\"neg_mean_squared_error\", cv=10)\nlogreg_rmse_scores = np.sqrt(-logreg_scores)\n\ndisplay_scores(logreg_rmse_scores)","e97c6e26":"param_grid = { \n    \"C\": [0.5,1.0,10.0,25.0,50.0],\n    \"solver\":['liblinear','lbfgs','sag'],\n    \"max_iter\":[1000,2500,5000],\n}\n\nlogreg_cv = GridSearchCV(logreg, param_grid, n_jobs= 1, cv =5,scoring = 'roc_auc')\n                \nlogreg_cv.fit(X_train_prepared, y_train)\nprint(logreg_cv.best_params_)    \nprint(logreg_cv.best_score_)","99207624":"knn = KNeighborsClassifier(n_neighbors = 5)\n\nknn.fit(X_train_prepared, y_train)\nknn_predictions = knn.predict(X_train_prepared)\n\nprint(\"\\nAccuracy Score for KNN Classifier is: \" + str(knn.score(X_train_prepared, y_train)))","28a006f4":"knn_scores = cross_val_score(knn, X_train_prepared, y_train,scoring=\"neg_mean_squared_error\", cv=10)\nknn_rmse_scores = np.sqrt(-knn_scores)\n\ndisplay_scores(knn_rmse_scores)","5793b704":"param_grid = { \n    \"n_neighbors\": [1,5,10,25],\n    \"weights\":['uniform','distance'],\n    \"leaf_size\":[30,50,100],\n    'algorithm':['auto','brute'],\n}\n\nknn_cv = GridSearchCV(knn, param_grid, n_jobs= 1, cv =10,iid = False,scoring ='roc_auc')\n                \nknn_cv.fit(X_train_prepared, y_train)\nprint(knn_cv.best_params_)    \nprint(knn_cv.best_score_)","893a9f18":"linear_svm = LinearSVC()\n\nlinear_svm.fit(X_train_prepared, y_train)\nlinear_svm_predictions = linear_svm.predict(X_train_prepared)\n\nprint(\"\\nAccuracy Score for Linear SVM Classifier is: \" + str(linear_svm.score(X_train_prepared, y_train)))","e83c8327":"linear_svm_scores = cross_val_score(linear_svm, X_train_prepared, y_train,scoring=\"neg_mean_squared_error\", cv=10)\nlinear_svm_rmse_scores = np.sqrt(-linear_svm_scores)\n\ndisplay_scores(linear_svm_rmse_scores)","f803127e":"param_grid = { \n    \"C\": [1.0,5.0,10.0,25.0],\n    \"max_iter\":[1000,2500,5000,10000,15000],\n    \"fit_intercept\":[True,False],\n}\n\nlinear_svm_cv = GridSearchCV(linear_svm, param_grid, n_jobs= 1, cv =10,iid = False,scoring = 'roc_auc')\n                \nlinear_svm_cv.fit(X_train_prepared, y_train)\nprint(linear_svm_cv.best_params_)    \nprint(linear_svm_cv.best_score_)","cd144fe3":"forest_predictions = forest_cv.predict(X_valid_prepared)\nknn_predictions = knn_cv.predict(X_valid_prepared)\ngb_predictions = gb_cv.predict(X_valid_prepared)\nlogreg_predictions = logreg_cv.predict(X_valid_prepared)\nsvc_predictions = svc.predict(X_valid_prepared)\n\nfig, ax = plt.subplots(2,3,figsize = (22,10),sharey = True)\n\nsns.heatmap(confusion_matrix(y_valid,forest_predictions),annot = True, fmt ='d',ax=ax[0,0],cmap = 'Greens',annot_kws={\"size\": 16},)\nsns.heatmap(confusion_matrix(y_valid,knn_predictions),annot = True, fmt = 'd', ax=ax[0,1], cmap = 'Reds',annot_kws={\"size\": 16},)\nsns.heatmap(confusion_matrix(y_valid,gb_predictions),annot = True, fmt = 'd', ax=ax[1,0], cmap = 'Blues',annot_kws={\"size\": 16},)\nsns.heatmap(confusion_matrix(y_valid,logreg_predictions),annot = True, fmt = 'd', ax=ax[1,1], cmap = 'Oranges',annot_kws={\"size\": 16},)\nsns.heatmap(confusion_matrix(y_valid,svc_predictions),annot = True, fmt = 'd', ax=ax[0,2], cmap = 'YlGnBu',annot_kws={\"size\": 16},)\n\nax[0,0].set_title('Confusion Matrix - Random Forest Classifier')\nax[0,1].set_title('Confusion Matrix - KNN')\nax[1,0].set_title('Confusion Matrix - Gradiant Boosting Classifier')\nax[1,1].set_title('Confusion Matrix - Logistic Regression')\nax[0,2].set_title('Confusion Matrix - SVC')\n\n\ntight()","006dd466":"fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_valid, forest_cv.predict_proba(X_valid_prepared)[:, 1])\nfpr_knn, tpr_knn, thresholds_knn = roc_curve(y_valid, knn_cv.predict_proba(X_valid_prepared)[:, 1])\nfpr_gbrt, tpr_gbrt, thresholds_gbrt = roc_curve(y_valid, gb_cv.predict_proba(X_valid_prepared)[:, 1])\nfpr_logreg, tpr_logreg, thresholds_logreg = roc_curve(y_valid, logreg_cv.predict_proba(X_valid_prepared)[:, 1])\nfpr_svc, tpr_svc, thresholds_svc = roc_curve(y_valid, svc.decision_function(X_valid_prepared))\nfpr_linearsvm, tpr_linearsvm, thresholds_linearsvm = roc_curve(y_valid, linear_svm_cv.decision_function(X_valid_prepared))\n\nroc_auc_forest = auc(y_valid,forest_cv.predict_proba(X_valid_prepared)[:, 1])\nroc_auc_knn = auc(y_valid,knn_cv.predict_proba(X_valid_prepared)[:, 1])\nroc_auc_gb = auc(y_valid,gb_cv.predict_proba(X_valid_prepared)[:, 1])\nroc_auc_logreg = auc(y_valid,logreg_cv.predict_proba(X_valid_prepared)[:, 1])\nroc_auc_svc = auc(y_valid,svc.decision_function(X_valid_prepared))\nroc_auc_linearsvm = auc(y_valid,linear_svm_cv.decision_function(X_valid_prepared))","6e77e622":"sns.set(style=\"darkgrid\")\n\nfig, ax = plt.subplots(figsize = (14,8))\n\nplt.plot(fpr_rf,tpr_rf,color='mediumseagreen',label = 'Random Forest Classifier (area = %0.2f)' % roc_auc_forest)\nplt.plot(fpr_knn,tpr_knn,color='red',label = 'KNN (area = %0.2f)' % roc_auc_knn)\nplt.plot(fpr_gbrt,tpr_gbrt,color='cyan',label = 'Gradient Boosting Classifier (area = %0.2f)' % roc_auc_gb)\nplt.plot(fpr_logreg,tpr_logreg,color='Orange',label = 'Logistic Regression (area = %0.2f)' % roc_auc_logreg)\nplt.plot(fpr_svc,tpr_svc,color='yellow',label = 'SVC (area = %0.2f)' % roc_auc_svc)\nplt.plot(fpr_linearsvm,tpr_linearsvm,color='slategrey',label = 'Linear SVM (area = %0.2f)' % roc_auc_linearsvm)\nplt.plot([0, 1], [0, 1],linestyle='--',color ='black')\n\nplt.legend()\nplt.title('ROC Curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate (Recall)')\n\ntight()","806b6fc2":"X_test_prepared = preprocessor.transform(X_test)","e2fcce62":"final_model = gb_cv.best_estimator_","e757e1e0":"final_predictions = final_model.predict(X_test_prepared)","d94e799f":"output = pd.DataFrame({'PassengerId': X_test.PassengerId,\n                       'Survived': final_predictions})\n\noutput.to_csv('submission.csv', index=False)","d20167ee":"### 10.1 Linear SVM CV Scores:","b96e51a0":"# 8. Logistic Regression:","6d0c3fe0":"### 10.2 Linear SVM GridSearchCV:","ac4f458f":"# 11. Evaluation:","0ec59e74":"# The Challenge:\n\n    The sinking of the Titanic is one of the most infamous shipwrecks in history.\n\n    On April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\n    While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\n    In this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc). ","ecfd3e88":"# Notes:\n\n| Variable | Definition                                   | Key                                              |\n|----------|----------------------------------------------|--------------------------------------------------|\n| survival | Survival                                     | 0 = No, 1 = Yes                                  |\n| pclass   | Ticket class                                 | 1 = 1st, 2 = 2nd, 3 = 3rd                        |\n| sex      | Sex                                          |                                                  |\n| Age      | Age in years                                 |                                                  |\n| sibsp    | # of siblings \/ spouses aboard   the Titanic |                                                  |\n| parch    | # of parents \/ children aboard   the Titanic |                                                  |\n| ticket   | Ticket number                                |                                                  |\n| fare     | Passenger fare                               |                                                  |\n| cabin    | Cabin number                                 |                                                  |\n| embarked | Port of Embarkation                          | C = Cherbourg, Q = Queenstown, S =   Southampton |","3b8284aa":"# 5. Random Forest Classifier:","4206d0e3":"### 9.1 KNN CV Scores:","75e69253":"# 10. Linear SVM:","3a04dab4":"                                        pclass: A proxy for socio-economic status (SES)\n                                        1st = Upper\n                                        2nd = Middle\n                                        3rd = Lower\n\n                                        age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n                                        sibsp: The dataset defines family relations in this way...\n                                        Sibling = brother, sister, stepbrother, stepsister\n                                        Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n                                        parch: The dataset defines family relations in this way...\n                                        Parent = mother, father\n                                        Child = daughter, son, stepdaughter, stepson\n                                        Some children travelled only with a nanny, therefore parch=0 for them","37c39c90":"### 6.1 SVC - CV Scores:","4cf2fcde":"### 9.2. KNN GridSearchCV:","150a431e":"### 11.1 Confusion Matrix:","bebba91f":"### 7.1 Gradient Boosting Classifier - CV Scores","3a79fac4":"### 8.1 Logistic Regression CV Scores:","c28dd6a5":"### 7.2 Gradient Boosting Classifier - GridSearchCV:","0f80b98a":"### 5.2 Random Forest Classifier - GridSearchCV","0708de4d":"## 2. Meet and Greet Data\n\n    This is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent\/feature variables(s)), what's its goals in life (dependent\/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.","72a77ba4":"### 4.1 Decision Tree Classifier - CV Scores:","04339877":"### 5.1 Random Forest Classifier - CV Scores","81f1c950":"# 4. Modelling:","488ca228":"# 4. Decesion Tree Classifier:","f5745ed6":"# 7. Gradient Boosting Classifier:","8a3eef8d":"# 9. KNN:","7987d58d":"# 6. SVC:","f9f06cec":"# 12. Final Model:","27b96c8c":"# 1. Import Libraries:","82b8cbfc":"### 11.3 ROC Curve:","a9ab3d33":"# 3. EDA Analysis:","a82d7aaf":"### 11.2 AUC Scores:","2a1136b2":"### 8.2 Logistic Regression GridSearchCV:","8c070a0c":"# 1A. Sample Submission:\n\n    Located below is a sample submission provided by Kaggle, you should align your results to something that looks like it but obviously with differen't answers. This gives users a good idea of how you think about answering the kaggle competition."}}