{"cell_type":{"a552d629":"code","27bfcd01":"code","ee427d4d":"code","d26e58de":"code","f13ff478":"code","db14bb54":"code","e855c455":"code","9cbc30c6":"code","e54c266d":"code","1b5b9605":"code","19dfc779":"code","2e4aec5a":"code","f5228655":"code","c8cda0df":"code","75630605":"code","44b3a176":"code","e7d46f11":"code","2224caa3":"code","7198b13b":"code","d99ba287":"code","441de2cb":"code","2a8677ca":"code","9174b46f":"code","4fda5b3a":"code","fe0010eb":"code","33da54aa":"code","976845b0":"code","cc912e40":"code","a224ece5":"code","8c2485ed":"code","c7978af8":"code","7f6d287c":"code","0a6f2bf8":"code","9178c369":"code","878383a5":"code","9ef5e943":"code","b617f2c1":"code","a0e69d53":"code","ad6f5ecf":"code","cf516dd6":"code","efa4456a":"code","c505effe":"code","8e293c8c":"code","5d55c898":"code","83a7c832":"code","f4c5cdf7":"code","531ae196":"code","81360b47":"code","72be724b":"code","a9e063a8":"code","52e83fb2":"code","a3f45800":"code","779ae318":"code","469fe995":"code","810c5113":"code","4fbc16d4":"code","52db59ae":"code","dab8525c":"markdown","070538e4":"markdown","0c5e948a":"markdown","586c3936":"markdown","a265754f":"markdown","f5fbcbda":"markdown","83941d07":"markdown","19b45712":"markdown","376c805d":"markdown","377a1b68":"markdown","3b9234f5":"markdown","be789054":"markdown","13f2543f":"markdown","9222c610":"markdown","e9a64bc2":"markdown","239e2664":"markdown","cb7fc007":"markdown","238826af":"markdown","3c98d1d4":"markdown","154cfe39":"markdown","2a0c94aa":"markdown","8d64a290":"markdown","c5ad9f15":"markdown","20e136da":"markdown","99e37eaa":"markdown","6d7e61b4":"markdown","86f6f4d7":"markdown","d7e918f7":"markdown","dd86fe61":"markdown","16c237ca":"markdown","551b884a":"markdown","951a83e5":"markdown","32ae0647":"markdown","3da8883f":"markdown","018779dd":"markdown","73379396":"markdown"},"source":{"a552d629":"import time\nimport warnings\nimport logging\nimport tensorflow as tf\nfrom datetime import datetime","27bfcd01":"@tf.function\ndef add(a, b):\n    return a + b\n\n@tf.function\ndef sub(a, b):\n    return a - b\n\n@tf.function\ndef mul(a, b):\n    return a * b\n\n@tf.function\ndef div(a, b):\n    return a \/ b","ee427d4d":"print(add(tf.constant(5), tf.constant(2)))","d26e58de":"print(sub(tf.constant(5), tf.constant(2)))","f13ff478":"print(mul(tf.constant(5), tf.constant(2)))","db14bb54":"print(div(tf.constant(5), tf.constant(2)))","e855c455":"@tf.function\ndef matmul(a, b):\n    return tf.matmul(a, b)","9cbc30c6":"@tf.function\ndef linear(m, x, c):\n    return add(matmul(m, x), c)","e54c266d":"m = tf.constant([[4.0, 5.0, 6.0]], tf.float32)\n\nm","1b5b9605":"x = tf.Variable([[100.0], [100.0], [100.0]], tf.float32)\n\nx","19dfc779":"c = tf.constant([[1.0]], tf.float32)\n\nc","2e4aec5a":"linear(m, x, c)","f5228655":"@tf.function\ndef pos_neg_check(x):\n    reduce_sum = tf.reduce_sum(x)\n\n    if reduce_sum > 0:\n        return tf.constant(1)\n\n    elif reduce_sum == 0:\n        return tf.constant(0)\n    \n    else:\n        return tf.constant(-1)","c8cda0df":"pos_neg_check(tf.constant([100, 100]))","75630605":"pos_neg_check(tf.constant([100, -100]))","44b3a176":"pos_neg_check(tf.constant([-100, -100]))","e7d46f11":"num = tf.Variable(7)","2224caa3":"@tf.function\ndef add_times(x):\n    # tf.range is a TensorFlow construct allowing the\n    # unrolling of this for loop within our static computation graph\n    for i in tf.range(x):\n        num.assign_add(x)","7198b13b":"add_times(5)","d99ba287":"print(num)","441de2cb":"a = tf.Variable(1.0)\n\nb = tf.Variable(2.0)","2a8677ca":"@tf.function\ndef f(x, y):\n    \n    a.assign(y * b)\n    \n    b.assign_add(x * a)\n    \n    return a + b","9174b46f":"f(1, 2)","4fda5b3a":"@tf.function\ndef square(a):\n    print(\"Input a: \", a)\n    return a * a","fe0010eb":"x = tf.Variable([[2, 2], [2, 2]], dtype = tf.float32)\n\nsquare(x)","33da54aa":"y = tf.Variable([[2, 2], [2, 2]], dtype = tf.int32)\n\nsquare(y)","976845b0":"z = tf.Variable([[3, 3], [3, 3]], dtype = tf.float32)\n\nsquare(z)","cc912e40":"concrete_int_square_fn = square.get_concrete_function(tf.TensorSpec(shape=None, dtype=tf.int32))\n\nconcrete_int_square_fn","a224ece5":"concrete_float_square_fn = square.get_concrete_function(tf.TensorSpec(shape=None, dtype=tf.float32))\n\nconcrete_float_square_fn","8c2485ed":"concrete_int_square_fn(tf.constant([[2, 2], [2, 2]], dtype = tf.int32))","c7978af8":"concrete_float_square_fn(tf.constant([[2.1, 2.1], [2.1, 2.1]], dtype = tf.float32))","7f6d287c":"# This will generate an exception or error when executed\nconcrete_float_square_fn(tf.constant([[2, 2], [2, 2]], dtype = tf.int32))","0a6f2bf8":"@tf.function\ndef f(x):\n    print(\"Python execution: \", x)\n    tf.print(\"Graph execution: \", x)","9178c369":"f(1)","878383a5":"f(1)","9ef5e943":"f(\"Hello tf.function!\")","b617f2c1":"arr = []\n\n@tf.function\ndef f(x):\n    for i in range(len(x)):\n        arr.append(x[i])    ","a0e69d53":"f(tf.constant([10, 20, 30]))","ad6f5ecf":"arr","cf516dd6":"@tf.function\ndef f(x):\n    tensor_arr = tf.TensorArray(dtype = tf.int32, size = 0, dynamic_size = True)\n    \n    for i in range(len(x)):\n        tensor_arr = tensor_arr.write(i, x[i])\n        \n    return tensor_arr.stack()","efa4456a":"result_arr = f(tf.constant([10, 20, 30]))\n\nresult_arr","c505effe":"external_list = []\n\ndef side_effect(x):\n    print('Python side effect')\n    external_list.append(x)\n\n@tf.function\ndef fn_with_side_effects(x):\n    tf.py_function(side_effect, inp=[x], Tout=[])","8e293c8c":"fn_with_side_effects(1)","5d55c898":"fn_with_side_effects(2)","83a7c832":"external_list","f4c5cdf7":"@tf.function\n\ndef some_tanh_fn(x):\n    while tf.reduce_sum(x) > 1:\n        x = tf.tanh(x)\n    \n    return x","531ae196":"some_tanh_fn(tf.random.uniform([10]))","81360b47":"def fn_with_variable_init_eager():\n\n    a = tf.constant([[10,10],[11.,1.]])\n    x = tf.constant([[1.,0.],[0.,1.]])\n    b = tf.Variable(12.)\n    \n    y = tf.matmul(a, x) + b\n\n    tf.print(\"tf_print: \", y)\n    \n    return y","72be724b":"fn_with_variable_init_eager()","a9e063a8":"@tf.function\ndef fn_with_variable_init_autograph():\n\n    a = tf.constant([[10,10],[11.,1.]])\n    x = tf.constant([[1.,0.],[0.,1.]])\n    b = tf.Variable(12.)\n    \n    y = tf.matmul(a, x) + b\n\n    tf.print(\"tf_print: \", y)\n    \n    return y","52e83fb2":"# This will generate an exception or error when executed\nfn_with_variable_init_autograph()","a3f45800":"class F():\n    def __init__(self):\n        self._b = None\n\n    @tf.function\n    def __call__(self):\n        a = tf.constant([[10, 10], [11., 1.]])\n        x = tf.constant([[1., 0.], [0., 1.]])\n        \n        if self._b is None:\n            self._b = tf.Variable(12.)\n            \n        y = tf.matmul(a, x) + self._b\n        print(y)\n\n        tf.print(\"tf_print: \", y)\n        return y\n\nfn_with_variable_init_autograph = F()\nfn_with_variable_init_autograph()","779ae318":"def f(x):\n    if x > 0:\n        x *= x\n    return x\n    \nprint(tf.autograph.to_code(f))    ","469fe995":"@tf.function\ndef g(x):\n    return x\n\nstart = time.time()\nfor i in tf.range(2000):\n    g(i)\nend = time.time()\n\nprint(\"tf.Tensor time elapsed: \", (end-start))","810c5113":"warnings.filterwarnings('ignore')\nlogging.getLogger('tensorflow').disabled = True","4fbc16d4":"start = time.time()\nfor i in range(2000):\n    g(i)\nend = time.time()\n\nprint(\"Native type time elapsed: \", (end-start))","52db59ae":"#End of code","dab8525c":"I have four helper methods that can perform addition, subtraction, multiplication, and division on two tensors. When I decorate these functions using @tf.function, Python will first convert these function computations into a static graph, and then execute these functions.","070538e4":"Converting a function that works in eager mode to its Graph representation requires to think about the Graph even though we are working in eager mode","0c5e948a":"### Trace a new graph with floating point inputs\n\nWe instantiate a variable x of type tf.float32, and then invoke the square method on x. Notice the print statement printing out the input a is executed, and the value of a of type float32 has been printed out to screen","586c3936":"### Use get_concrete_function() to get a concrete trace for a particular type of function\n\n For polymorphic functions that support different input types, for each input type a separate graph is retraced. If you want to get the concrete function associated with a specific type of input, you can invoke the get_concrete_function on our decorated method","a265754f":"When we invoke this using (1, 2), and you'll see that the result makes sense. \n\nThe operations are performed exactly in the order specified within our function, the order in which the code is written","f5fbcbda":"### Functions decorated using @tf.function maintain the order of dependencies that you specified in code. \n\nHere are two variables, a and b. \n\nI have a function which takes an x and y as inputs. This function first multiplies y by b and assigns the result to a, and once a has been updated, this updated value of a is multiplied by x, and this result is added to the value of b. \n\nWe then return a + b. This series of operations have dependencies between them, they have an inherent ordering, and this ordering is preserved by tf.function.","83941d07":" The function below, which calculates the square of a. \n \n We have a print statement within this decorated function. Python statements such as print, which have side effects.\n \n Which means they are executed only the first time the square function is invoked. This is when the computation graph is traced and generated. Subsequent invocations of the square function will use the already generated computation graph. At that point, the print statement will not be executed","19b45712":"### Convert regular Python code to TensorFlow constructs\n\nTo help users avoid having to rewrite their code when adding @tf.function, AutoGraph converts a subset of Python constructs into their TensorFlow equivalents.\n\nMay use data-dependent control flow, including if, for, while break, continue and return statements","376c805d":"### Python side effects only happen during tracing\n\nIn general, Python side effects (like printing or mutating objects) only happen during tracing. ","377a1b68":"For any function that you have defined, if you want to see the equivalent code in graph mode, you can use tf.autograph.to_code\n\nWhen you execute your functions in graph mode, TensorFlow offers you significant speedups","3b9234f5":"### Polymorphism and tracing\n\nPython's dynamic typing means that you can call functions with a variety of argument types, and Python will do something different in each scenario.\n\nOn the other hand, TensorFlow graphs require static dtypes and shape dimensions. tf.function bridges this gap by retracing the function when necessary to generate the correct graphs. Most of the subtlety of tf.function usage stems from this retracing behavior.","be789054":"### Use the tf.py_function() exit hatch to execute side effects","13f2543f":"### Decorating a function using @tf.function, \n\nAutoGraph in TensorFlow will take care of converting this to a static computation graph using TensorFlow constructs rather than the Pythonic control flow constructs that you have used.\n\nIf you execute this function, you'll see that it works as we expect it to","9222c610":"We use the tf.function decorator in order to make static computation graphs out of our TensorFlow programs\n\nWhile you're building your neural network, it's quite likely that you have a few helper methods that transform your tensor. \n\nAny helper method can be decorated using @tf.function, as you can see here. ","e9a64bc2":"Appending to Python lists is also a Python side-effect","239e2664":"### what actually happens under the hood is that the first time you invoke a function decorated using @tf.function, TensorFlow traces the computation performed within that function. \n\n### This tracing operation is performed only once during the first invocation, and this is what constructs the static computation graph. \n\n### Once the graph has been constructed, if you invoke that method once again, the same graph will be reused to perform the computation. \n\n### Subsequent invocations of the function will not create a new computation graph; it'll simply reuse the original one constructed. ","cb7fc007":"When we invoke the function f 2nd time with the same type of data. Notice that the Python print is not executed. Tf.print, which is part of our computation graph, is executed.","238826af":"#### Converting a function in eager mode to its Graph representation","3c98d1d4":"### Operations with side effects\n\nMay also use ops with side effects, such as tf.print, tf.Variable and others.","154cfe39":"When we invoke the function f for the first time. The print statement in Python is executed. Tf.print is also executed","2a0c94aa":"### In-order code execution\n\nDependencies in the code are automatically resolved based on the order in which the code is written","8d64a290":"### Control flow works\n\nfor\/while --> tf.while_loop (break and continue are supported)","c5ad9f15":"Python functions, are polymorphic, so if you now invoke this function with a different data type, TensorFlow retraces the computation graph and generates a new graph to deal with a different type of input. Notice that the Python print statement, as well as tf.print are both executed","20e136da":"### You can have functions decorated using @tf.function nested one within another. The linear function here, which operates on m, x ,and c, invokes the matmul and the add functions that we had defined earlier. \n\n### The result is the computation y is equal to mx+c.","99e37eaa":"#### AutoGraph is highly optimized and works well when the input is a tf.Tensor object","6d7e61b4":"### Functions decorated using @tf.function can invoke other TensorFlow operations as well. As you can see from this example here, matmul invokes tf.matmul","86f6f4d7":"When working in graph mode you should create variables exactly once, as I have done here. Notice this class F here, within the init method of the class I initialize the variable b to None (self._b = None)\n\nWe instantiate a new variable only the first time we invoke this function, if self._b is None, then self._b is equal to tf.Variable (self._b = tf.Variable(12.)). Any subsequent invocations to this method will not create a new variable. Instead, the previously instantiated variable will be reused","d7e918f7":"Now if you try to use the concrete function on a different data type below, such as the float function here on the data type of type int32, we get an exception. \n\nTensorFlow expected a floating point input for this concrete function. We specified an integer input, and that's why this exception occurred","dd86fe61":"### Re-trace the graph, now the inputs are of type integer\n\nTensorFlow has generated a static computation graph for this method for floating point input values. Python functions are polymorphic, which means the same square function can be used on input data that is of type int32 as well. For a different input type, this time int32, TensorFlow's autograph functionality will retrace another separate graph implementation, a separate static computation graph for integer inputs. Because the input type is different and Python retraced and generated a new graph, the print statement was executed, and you can see that the value of input a has been printed out to screen.","16c237ca":"This time the graph for floating point inputs is *NOT* traced, it is simply executed. This means that the print() statement is not executed. Since that is a Python side-effect. Python side-effects are executed only when the graph is traced","551b884a":"### Decorate functions with tf.function\n\nFunctions can be faster than eager code, especially for graphs with many small ops. But for graphs with a few expensive ops (like convolutions), you may not see much speedup.","951a83e5":"Using tf.range(2000) = 0.57 seconds\nUsing range(2000) = 12.9 seconds","32ae0647":"These functions execute in what is known as graph mode, and this can significantly speed up your graph execution, especially when you perform many small granular computations within your graph. Now when you invoke these functions, you'll be able to view the results right away","3da8883f":"### Operate on variables and tensors, invoke nested functions","018779dd":"When working with static computation graphs generated using the @tf.function decorator. Variables behave differently in eager mode and graph mode in TensorFlow\n\nWhen we define another function with exactly the same code that we had earlier, but this time I'm going to execute this function in graph mode. This function was decorated using @tf.function. Now if I try executing this function, it's a pretty straightforward function. We'll encounter an error.\n\nThe errors says tf.function\u2011decorated function tried to create variables on the non\u2011first call. \n\nWhat the error means is this:\n In graph mode, the variable that you create the first time the graph is traced is reused for every subsequent call. \n In eager mode, a new variable is created each time you execute the function","73379396":"### Regular Pythonic dynamic \n\nWhen we have regular Pythonic dynamic constructs within your tf.function, such as branching operations or for loops, AutoGraph will convert these two TensorFlow equivalents. \n\n### pos_neg_check function \n\nTakes an input tensor, x. It then computes the sum of elements of that tensor. If the sum is greater than 0, it returns the constant 1; if the sum is equal to 0, it returns 0; otherwise, it returns the constant \u20111"}}