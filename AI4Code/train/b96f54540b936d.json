{"cell_type":{"a563e3ee":"code","3c025c0a":"code","b0893fd1":"code","9032e879":"code","6b34d979":"code","b1fa28fd":"code","dd78654b":"code","46aadc10":"code","2cad1e96":"code","a729df0b":"code","38faaa2d":"code","83129d77":"code","e49eb1eb":"code","8f67571c":"code","5e60461f":"code","5d060256":"code","549209ee":"code","995b6848":"code","8d62dc2a":"code","3cfba67d":"code","fde67cf9":"code","e9b85545":"code","8f3d7c19":"code","ac9bbd8b":"code","dea26ade":"code","8c11206f":"code","70852561":"code","7d634a07":"code","03be6bf5":"code","baf4dc68":"code","45ca5347":"code","b0e7de17":"code","eb686463":"code","fd8abf6d":"code","0dfb1e6e":"code","363854d7":"code","46147d99":"code","b28a4f31":"code","b9b6983e":"code","f3f7750f":"code","0593a813":"code","ad7d3ad8":"code","266b9e35":"code","8c16b97c":"code","ed5db572":"code","a79af03a":"code","7c1311af":"code","278e4d1d":"code","de915e11":"code","2920cf4a":"code","e1f4cb5b":"code","c2d93666":"code","14b2cdbc":"code","115be731":"code","e52b6970":"code","bf1d3c29":"code","6b79fcc8":"code","6a1f39f0":"code","b5b966f0":"code","7b713924":"code","1cd33fcc":"code","2b1cf2f8":"code","dab6a761":"code","88df4652":"code","03ddb3f7":"code","d0710b98":"code","b96d8db1":"code","e0d35044":"code","00d52a97":"code","dce7b970":"code","aad7b65f":"code","54ae37bb":"code","a4918511":"code","327514d9":"code","f47fbff9":"code","b170c2df":"code","6c9296be":"code","7309a276":"code","6cca60c1":"code","aa76aa43":"code","57262244":"code","579f5854":"code","0ca15792":"code","8d991514":"code","775fa2c1":"code","bcf328f3":"code","356b242e":"code","d3f24956":"code","20bab00e":"code","fca31687":"code","60a4fae8":"code","efbafb6f":"markdown","3fde2150":"markdown","0e92d1b3":"markdown","2573b2c1":"markdown","c7b8cdfa":"markdown","bd1b4b15":"markdown","b4f039c9":"markdown","894fad19":"markdown","345c415d":"markdown","470a2549":"markdown","f19777b4":"markdown","6b8629a5":"markdown","4cfde4a5":"markdown","424124fd":"markdown","57df1255":"markdown","ee7ca1f1":"markdown","5b03a69e":"markdown","077dc4b3":"markdown","b58d795f":"markdown","9b2ecd56":"markdown","9b93004b":"markdown","21ec9c15":"markdown","dc32746e":"markdown","a066f133":"markdown","68178f67":"markdown","2bbd6ae4":"markdown","41671151":"markdown","d6e4a28c":"markdown","6a263fc9":"markdown","8077395f":"markdown","6c4966f5":"markdown","95aa08cc":"markdown","8965650d":"markdown","6059608a":"markdown","a7dc3d04":"markdown","7993ddfe":"markdown","2a77b974":"markdown","52eb78c9":"markdown","d2bffa51":"markdown","10567b9b":"markdown","d4be1e9a":"markdown","c1a9d89b":"markdown","20d50f7d":"markdown","8af78920":"markdown","de4443d6":"markdown","62e805c5":"markdown","a9dd3afd":"markdown","2d27b6d2":"markdown","5d6c8338":"markdown","a3361aca":"markdown","b7cc323c":"markdown","1de6ffa0":"markdown"},"source":{"a563e3ee":"import pandas as pd\nimport numpy as np\n\n# Regular expressions module.\nimport re\n\n# Data visualization and frame's visualization options.\nimport missingno as msno \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","3c025c0a":"# using pandas library to read the data frame\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n\n\ndf = pd.read_csv(\"\/kaggle\/input\/food-choices\/food_coded.csv\")","b0893fd1":"# taking look at the head of the dataset\ndf.head()","9032e879":"# looking at the data frame size\nprint(\"The size of the data frame is -\",df.shape)","6b34d979":"# looking at the column name  of the data frame\nprint(\"Columns in the data frame are - \\n\",df.columns)","b1fa28fd":"df[['comfort_food_reasons','comfort_food_reasons_coded',  'comfort_food_reasons_coded.1']]","dd78654b":"df[['comfort_food_reasons_coded',  'comfort_food_reasons_coded.1']].isnull().sum()","46aadc10":"df = df.drop('comfort_food_reasons_coded',axis =1)","2cad1e96":"# Check how many features have missing data\ndf.isnull().any().value_counts()","a729df0b":"# Amount of NaN values for each feature\ntotal = df.isnull().sum().sort_values(ascending=False)\n# Percentage part of total\npercent = (df.isnull().sum()\/df.isnull().count()*100).round(1).sort_values(ascending=False)\n# Merge series\nnan_data = pd.concat({\"# of NaN's\": total, '% of Total': percent}, axis=1)\nnan_data.head(10)","38faaa2d":"# Use missingno module for NaN's distribution\nmsno.matrix(df[df.columns[df.isnull().any()]])","83129d77":"# getting the unique values in the GPA column\n\ndf['GPA'].unique()","e49eb1eb":"'''\nUsing regex to remove the unwanted items and replacing them with\nwith nan values and then convert them into float\nFill these NaN values with mean value\n'''\n\n\ndf['GPA'] = df['GPA'].str.replace(r'[^\\d\\.\\d+]', '').replace( '',np.nan).astype(float).round(2)\n\ndf['GPA'] = df['GPA'].fillna(df['GPA'].mean())\n\n","8f67571c":"# plotting the box plot for GPA to look for any outliers \n\nfig, ax = plt.subplots(figsize=[12,6])\nsns.boxplot(df['GPA'])\nax.set_title(\"'GPA' distribution\")","5e60461f":"# getting the index of the row with GPA less than 2.5\nindex = df[df['GPA']<2.50].index.to_list()\n\n# drop those rows with GPA less than 2.5\ndf = df.drop(index)\n\n# resetting the index\ndf = df.reset_index(drop=True)","5d060256":"# plotting the box plot for GPA to look for any outliers \n\nfig, ax = plt.subplots(figsize=[12,6])\nsns.boxplot(df['GPA'])\nax.set_title(\"'GPA' distribution\")","549209ee":"# filtering the rwos with GPA below 3\nstudent_above_3_GPA = df[df['GPA']>3.0]","995b6848":"# New category names\nfood = ['greek_food', 'indian_food', 'italian_food', 'thai_food', 'persian_food', 'ethnic_food']\n# Set diverging palette\ncmap = sns.diverging_palette(50, 10, as_cmap=True)\n# Plot preparation\nplt.figure(figsize=(12,12))\nplt.title('Correlation between food preferences', y=1.01, size=20)\n\nwith sns.plotting_context(context='notebook', font_scale=1.5):\n    # Seaborn's heatmap creator\n    g = sns.heatmap(student_above_3_GPA[food].corr(method='spearman'),linewidths=0.5,vmax=1.0, square=True, center=0,\n                    cmap=cmap, annot=True, cbar_kws={\"shrink\": .75, \"orientation\": \"horizontal\"})\n    # Plot labels\n    loc, labels = plt.xticks()\n    g.set_xticklabels(labels, rotation=45)\n    g.set_yticklabels(labels, rotation=45)","8d62dc2a":"# getting the different diet types\n\ndiet_type = pd.get_dummies(df['diet_current_coded'])","3cfba67d":"# renaming the columns with different diet types\n\ndiet_type =diet_type.rename(columns={1:'healthy\/balanced\/moderated\/', 2:'unhealthy\/cheap\/too much\/random\/',3:'the same thing over and over',4:'unclear'})","fde67cf9":"# adding column with GPA\n\ndiet_type['GPA'] = df['GPA']","e9b85545":"cmap = sns.diverging_palette(50, 10, as_cmap=True)\n# Plot title and size\nplt.figure(figsize=(12,12))\nplt.title('Correlation between GPA and diet type', y=1.01, size=20)\n\nwith sns.plotting_context(context='notebook', font_scale=1.5):\n    # Seaborn's heatmap creator for different diet \n    g = sns.heatmap(diet_type.corr(method='spearman'),linewidths=0.5,vmax=1.0, square=True, center=0,\n                    cmap=cmap, annot=True, cbar_kws={\"shrink\": .75, \"orientation\": \"horizontal\"})\n    # Plot labels\n    loc, labels = plt.xticks()\n    g.set_xticklabels(labels, rotation=25)\n    g.set_yticklabels(labels, rotation=0)","8f3d7c19":"# looking at unique values\ndf['weight'].unique()","ac9bbd8b":"# using regex to remove unwanted strings\ndf['weight'] = df['weight'].str.replace(r'[^\\d\\d\\d]', '').replace('', np.nan).astype(float)","dea26ade":"# looking at the unique values\ndf['weight'].unique()","8c11206f":"# get the mean value of weight\ndf['weight'].mean()\n\n# filling Nan value with mean\ndf['weight'] = df['weight'].fillna(df['weight'].mean())","70852561":"# make a copy for future use\ndata_cp = df.copy(deep=True)","7d634a07":"# getting features of data frame\nfeatures = df.columns","03be6bf5":"# intialize a dictionary \nfeatures_by_dtype = {}\n\n# using for loop to get types and append them to the appropriate key\nfor f in features:\n    dtype = str(df[f].dtype)\n    \n    if dtype not in features_by_dtype.keys():\n        features_by_dtype[dtype] = [f]\n    else:\n        features_by_dtype[dtype] += [f]","baf4dc68":"# type object gives us text_features\ntext_features = features_by_dtype['object']\n\n#finding the unique values in text_features\nfor f in text_features:\n    l = len(df[f].unique())\n    print(f,' Number of Unique Values -',l)","45ca5347":"# plot GPA with Type sports\n\nimport seaborn as sns\n\n# plot using catplot\nax= sns.catplot(x=\"type_sports\", y=\"GPA\",data=df,ax = ax,height=9, aspect=2.5, kind='box')\nax.set_xticklabels(rotation=45,fontsize=20)","b0e7de17":"# defining the bins\nbins = [2,2.8, 3.2, 4.0]\n\n# defining the lables for the bins\nlabels =['sport_set_c','sport_set_b','sport_set_a']\n\n# genrating this bin and adding to the data frame\ndf['type_sport_binned'] = pd.cut(df['GPA'], bins,labels =labels)","eb686463":"# plot GPA with Type sports after binned\n\nimport seaborn as sns\n\n# plot using catplot\nax= sns.catplot(x=\"type_sport_binned\", y=\"GPA\",data=df,ax = ax,height=9, aspect=2.5, kind='box')\nax.set_xticklabels(rotation=45,fontsize=20)","fd8abf6d":"# plot GPA with fav_cuisine\ndf['fav_cuisine']\n\n# plot using catplot\nax= sns.catplot(x=\"fav_cuisine\", y=\"GPA\",data=df,ax = ax,height=9, aspect=2.5, kind='box')\nax.set_xticklabels(rotation=45,fontsize=20)","0dfb1e6e":"# defining the bins\nbins = [2,3, 4.0]\n\n# defining the lables for the bins\nlabels =['fav_cusine_set_b','fav_cusine__set_']\n\n# genrating this bin and adding to the data frame\ndf['fav_cusine_binned'] = pd.cut(df['GPA'], bins,labels =labels)","363854d7":"# plot GPA with father_profession\n\n# plot using catplot\nax= sns.catplot(x=\"father_profession\", y=\"GPA\",data=df,ax = ax,height=9, aspect=2.5, kind='box')\nax.set_xticklabels(rotation=45,fontsize=20)","46147d99":"# defining the bins\nbins = [2.0,2.5,3.0,3.5, 4.0]\n\n# defining the lables for the bins\nlabels =['father_prof_set_d','father_prof_set_c','father_prof_set_b','father_prof_set_a']\n\n# genrating this bin and adding to the data frame\ndf['father_prof_binned'] = pd.cut(df['GPA'], bins,labels =labels)","b28a4f31":"# plot GPA with mother_profession\n\n# plot using catplot\nax= sns.catplot(x=\"mother_profession\", y=\"GPA\",data=df,ax = ax,height=9, aspect=2.5, kind='box')\nax.set_xticklabels(rotation=45,fontsize=20)","b9b6983e":"# defining the bins\nbins = [2.0,2.5,3.0,3.5, 4.0]\n\n# defining the lables for the bins\nlabels =['mother_prof_set_d','mother_prof_set_c','mother_prof_set_b','mother_prof_set_a']\n\n# genrating this bin and adding to the data frame\ndf['mother_prof_binned'] = pd.cut(df['GPA'], bins,labels =labels)","f3f7750f":"# plot GPA with food_childhood\n\n# plot using catplot\nax= sns.catplot(x=\"food_childhood\", y=\"GPA\",data=df,ax = ax,height=12, aspect=2.5, kind='box')\nax.set_xticklabels(rotation=45,fontsize=20)","0593a813":"# defining the bins\nbins = [2.0,2.5,3.0,3.5, 4.0]\n\n# defining the lables for the bins\nlabels =['food_childhood_set_d','food_childhood_set_c','food_childhood_set_b','food_childhood_set_a']\n\n# genrating this bin and adding to the data frame\ndf['food_childhood_binned'] = pd.cut(df['GPA'], bins,labels =labels)","ad7d3ad8":"# plot GPA with meals_dinner_friend\n\n# plot using catplot\nax= sns.catplot(x=\"meals_dinner_friend\", y=\"GPA\",data=df,ax = ax,height=12, aspect=2.5, kind='box')\nax.set_xticklabels(rotation=45,fontsize=20)","266b9e35":"# defining the bins\nbins = [2.0,2.5,3.0,3.5, 4.0]\n\n# defining the lables for the bins\nlabels =['meals_dinner_friend_set_d','meals_dinner_friend_set_c','meals_dinner_friend_set_b','meals_dinner_friend_set_a']\n\n# genrating this bin and adding to the data frame\ndf['meals_dinner_friend_binned'] = pd.cut(df['GPA'], bins,labels =labels)","8c16b97c":"# getting the new set of features\nfeatures = []\n\n# converting the columns to list\nfeatures= df.columns.to_list()","ed5db572":"# getting the binned features \nbinned_features = [i for i in features if \"binned\" in i]","a79af03a":"# Subtract String Lists \n# using Counter() + elements() \nfrom collections import Counter\n\n# removing the binned features\nfeatures = list((Counter(features)-Counter(binned_features)).elements()) \n\n# removing the text_features\nfeatures = list((Counter(features)-Counter(text_features)).elements()) ","7c1311af":"ordinal = ['ethnic_food','fruit_day','greek_food','indian_food','italian_food','persian_food','thai_food','veggies_day'\n           ,'calories_day','healthy_feeling','income','nutritional_check','parents_cook','pay_meal_out','self_perception_weight',\n          'eating_out','exercise','self_perception_weight','life_rewarding','cook']","278e4d1d":"# getting the binar features\nbinary_category_features = ['Gender', 'vitamins']\n\nimage_features = [\"drink\",\"soup\",\"coffee\", \"fries\", \"breakfast\"]","de915e11":"binary_category_features += image_features","2920cf4a":"# removing the binary features\nfeatures = list((Counter(features)-Counter(ordinal)).elements()) \n\n# removing the binary features\nfeatures = list((Counter(features)-Counter(binary_category_features)).elements()) ","e1f4cb5b":"# features which are coded acroding to the code book of the data\n\ncoded_features =['comfort_food_reasons_coded.1','diet_current_coded','eating_changes_coded','eating_changes_coded1',\n                 'fav_cuisine_coded','ideal_diet_coded', \"cuisine\", \n                 \"employment\", \"fav_food\", \"marital_status\",\"on_off_campus\"]\n\n# removing the coded features\nfeatures = list((Counter(features)-Counter(coded_features)).elements()) ","c2d93666":"# numerical features \nnumerical_features = [\"weight\",\"GPA\"]\n\n# removing the numerical features\nfeatures = list((Counter(features)-Counter(numerical_features)).elements())","14b2cdbc":"# features which contain calorie \ncalorie_features = [i for i in features if \"calories\" in i]","115be731":"# finding unique values in the Calorie features\nfor c in calorie_features:\n    print('Number of categories in -',c,' is', len(df[c].unique()))\n    \nfeatures = list((Counter(features)-Counter(calorie_features)).elements())","e52b6970":"# remaing columns\n\nfeatures","bf1d3c29":"# finding unique values in the Calorie features\n\nfor f in features:\n    print('Number of categories in -',f,' is', len(df[f].unique()))","6b79fcc8":"# combining all the categorical columns\n\ncategorical_features = binned_features + binary_category_features + coded_features + calorie_features+features","6a1f39f0":"# total missing values in the data\ntotal = df[categorical_features].isnull().sum().sort_values(ascending=False)\nprint(\"Missing value\")\nprint(total)","b5b966f0":"'''\nto fill missing values with mode for categorical feature\nuse for loop through categorical featuresthen converting \ntype to category check if there is null find the mode of\nthat particular feature column and fill null value with it\n\n'''\n\nfor f in categorical_features:\n    df[f] = df[f].astype('category')\n    \n    if df[f].isnull().sum() !=0:\n        mode = df[f].mode().values[0]\n        df[f].fillna(value=mode, inplace=True)\n    ","7b713924":"# total missing values in the data\n\ntotal = df[ordinal].isnull().sum().sort_values(ascending=False)\nprint(\"Missing value\")\nprint(total)","1cd33fcc":"'''\nto fill missing values with mode for ordinal feature\nuse for loop through ordinal features then check if \nthere is null find the mode of that particular \nfeature column and fill null value with it\n\n'''\n\nfor f in ordinal:\n    if df[f].isnull().sum() !=0:\n        mode = df[f].mode().values[0]\n        df[f].fillna(value=mode, inplace=True)","2b1cf2f8":"# drop the orginal text features\n\ndf =df.drop(text_features, axis =1)","dab6a761":"# genrating dummy variable for all the categorical features\ndf = pd.get_dummies(data=df, columns=categorical_features)\n\n","88df4652":"from sklearn.preprocessing import StandardScaler\n\n# Create x, where x the 'scores' column's values as floats\nx = df['weight'].values.astype(float).reshape(-1, 1)\n\nscaler = StandardScaler()\n\n# Create an object to transform the data to fit minmax processor\nx_scaled = scaler.fit_transform(x)\n\n# Run the normalizer on the dataframe\ndf['weight'] = pd.DataFrame(x_scaled)","03ddb3f7":"# get the features of the data frame\nfeature = df.columns.tolist()\n\n# remove the target\nfeature.remove('GPA')\n\n# inputs for the model\nX = df[feature]\n\n# target\ny = df['GPA']","d0710b98":"# importing the Radom forest from sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import make_regression\n\n\nregr = RandomForestRegressor()\n\n# training the model\nregr.fit(X, y)","b96d8db1":"# get the improtance from the random forest\nimportances = regr.feature_importances_","e0d35044":"from pandas import DataFrame\n\n# getting features data frame \nfeature = df.columns.tolist()\nfeature.remove('GPA')\n\n# creating data frame with variable \ndf_varimp =  DataFrame (feature,columns=['Features'])\ndf_varimp[\"var_imp\"] =var_imp\n\n# sorting the varible importance\ndf_varimp_sorted=df_varimp.sort_values(by=['var_imp'],ascending=False)\nprint(df_varimp_sorted)","00d52a97":"# scaling the varibble improtance between 100 and 0 percentage\nhighest = df_varimp_sorted['var_imp'].iloc[0]\ndf_varimp_sorted['var_imp'] = df_varimp_sorted['var_imp'].div(highest)\ndf_varimp_sorted['% of -Var-imp'] = df_varimp_sorted['var_imp'] *100\ndf_varimp_sorted.head(20)","dce7b970":"# setting feature selection thresehold\nthres = 0.05\n\n# chossing features from the var_imp data frame\nimp_data = df_varimp_sorted[df_varimp_sorted['var_imp']>thres]\n\n# get those important feature for visulization\nfeat = imp_data['Features']\nprint(\"Feature selected from feature selection process\")\nprint(feat)","aad7b65f":"# select data of only important features\ndata_feature_select = df[feat]","54ae37bb":"# import train test split\nfrom sklearn.model_selection import  train_test_split\n\n\n\nX = data_feature_select\n\ny = df['GPA']\n\nX_t, X_test, y_t, y_test = train_test_split(X, y, test_size=0.05, random_state=1)\n\nX_train, X_val, y_train, y_val = train_test_split(X_t, y_t, test_size=0.15, random_state=1)","a4918511":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import make_regression\n\nregr = RandomForestRegressor(verbose=1,n_jobs=-1)\nregr.fit(X_train, y_train)","327514d9":"from sklearn.metrics import mean_squared_error\n\ny_pred = regr.predict(X_val)\n\nmean_squared_error(y_val, y_pred)\n","f47fbff9":"from sklearn.model_selection import cross_validate\n\ncv = cross_validate(regr, X, y, cv=5,n_jobs=-1)\nprint(cv['test_score'])\nprint(cv['test_score'].mean())","b170c2df":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 300, num = 3)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               }\nprint(random_grid)","6c9296be":"rf_random = RandomizedSearchCV(estimator = regr, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)","7309a276":"from sklearn.metrics import mean_squared_error\n\ny_pred = rf_random.predict(X_val)\n\nmean_squared_error(y_val, y_pred)","6cca60c1":"from sklearn.model_selection import cross_validate\n\ncv = cross_validate(rf_random, X, y, cv=5,n_jobs=-1)\nprint(cv['test_score'])\nprint(cv['test_score'].mean())","aa76aa43":"data_cp.columns","57262244":"# Plot with count of different type of employment\n\nax= sns.catplot(x=\"employment\",data=data_cp,ax = ax,height=12, aspect=2.5, kind='count',label='big')\nax.set_xticklabels(rotation=45,fontsize=40)\nax.set_yticklabels(fontsize=25)\n","579f5854":"# plotting the distrubtion of GPA with repect to the different employment\n\nax= sns.catplot(x=\"employment\", y=\"GPA\",data=data_cp,ax = ax,height=12, aspect=2.5, kind='box')\nax.set_xticklabels(rotation=45,fontsize=20)\nax.set_yticklabels(fontsize=25)","0ca15792":"# getting the empoyment into dummy varibale \n# has they are categorical variables\n\ndf_employment = pd.get_dummies(data_cp['employment'])\n\ndf_employment['GPA'] = data_cp['GPA']\n\n# rename the columns\ndf_employment = df_employment.rename(columns = {1.0:'yes full time',2.0:'yes part time',3.0:'no'})","8d991514":"# Set diverging palette\ncmap = sns.diverging_palette(50, 10, as_cmap=True)\n# Plot preparation\nplt.figure(figsize=(12,12))\nplt.title('Correlation between Employment and GPA', y=1.01, size=20)\n\nwith sns.plotting_context(context='notebook', font_scale=1.5):\n    # Seaborn's heatmap creator\n    g = sns.heatmap(df_employment.corr(method='spearman'),linewidths=0.5,vmax=1.0, square=True, center=0,\n                    cmap=cmap, annot=True, cbar_kws={\"shrink\": .75, \"orientation\": \"horizontal\"})\n    # Plot labels\n    loc, labels = plt.xticks()\n    g.set_xticklabels(labels, rotation=45)\n    g.set_yticklabels(labels, rotation=45)","775fa2c1":"data_cp.columns","bcf328f3":"# Plot with count of different type of Gender\n\nax= sns.catplot(x=\"Gender\",data=data_cp,ax = ax,height=12, aspect=2.5, kind='count',label='big')\nax.set_xticklabels( ['Female','Male'],rotation=45,fontsize=40)\nax.set_yticklabels(fontsize=25)\nplt.xlabel(\"Gender\", size=25)\nplt.ylabel(\"Count\", size=25)","356b242e":"# plotting the distrubtion of GPA with repect to the different Gender\n\nax= sns.catplot(x=\"Gender\", y=\"GPA\",data=data_cp,ax = ax,height=12, aspect=2.5, kind='box')\nax.set_xticklabels(['Female','Male'],rotation=45,fontsize=25)\nax.set_yticklabels(fontsize=25)\nplt.xlabel(\"GPA\", size=25)\nplt.ylabel(\"Gender\", size=25)","d3f24956":"df_gender = pd.get_dummies(data_cp['Gender'])\n\ndf_gender = df_gender.rename(columns = {1:'Female',2:'Male'})\n\ndf_gender['GPA'] = data_cp['GPA']","20bab00e":"# Set diverging palette\ncmap = sns.diverging_palette(50, 10, as_cmap=True)\n# Plot preparation\nplt.figure(figsize=(9,9))\nplt.title('Correlation between Gender and GPA', y=1.01, size=15)\n\nwith sns.plotting_context(context='notebook', font_scale=1.5):\n    # Seaborn's heatmap creator\n    g = sns.heatmap(df_gender.corr(method='spearman'),linewidths=0.5,vmax=1.0, square=True, center=0,\n                    cmap=cmap, annot=True, cbar_kws={\"shrink\": .75, \"orientation\": \"horizontal\"})\n    # Plot labels\n    loc, labels = plt.xticks()\n    g.set_xticklabels(labels, rotation=45)\n    g.set_yticklabels(labels, rotation=45)","fca31687":"# get GPA and weight for correlation dn heatmap\ndf_weight_GPA = data_cp[['GPA','weight']]","60a4fae8":"# Set diverging palette\ncmap = sns.diverging_palette(50, 10, as_cmap=True)\n# Plot preparation\nplt.figure(figsize=(8,8))\nplt.title('Correlation between Weight and GPA', y=1.01, size=20)\n\nwith sns.plotting_context(context='notebook', font_scale=1.5):\n    # Seaborn's heatmap creator\n    g = sns.heatmap(df_weight_GPA.corr(method='spearman'),linewidths=0.5,vmax=1.0, square=True, center=0,\n                    cmap=cmap, annot=True, cbar_kws={\"shrink\": .75, \"orientation\": \"horizontal\"})\n    # Plot labels\n    loc, labels = plt.xticks()\n    g.set_xticklabels(labels, rotation=45)\n    g.set_yticklabels(labels, rotation=45)","efbafb6f":"Adding GPA to the Diet_type data frame","3fde2150":"<a id=\"section_ID_4.3\"><\/a>\n\n## 4.3 Filter the GPA below 3 ","0e92d1b3":"## Model Evaluated with RMSE and  cross validation\n\n","2573b2c1":"<a id=\"section_ID_4.4\"><\/a>\n\n\n## 4.4. Correlation between different cuisines for students with GPA above 3.0\n\n### From the code book \n\n    1.ethnic_food\n    2.greek_food \n    3.indian_food \n    4.Italian_food \n    5.Persian_food \n    6.thai_food \n\n## How likely to eat these food when available?\n1 - very unlikely \n2 - unlikely \n3 - neutral \n4 - likely \n5 - very likely \n\nUsing these to identfy the coorelation different cuisines ","c7b8cdfa":"<a id=\"section_ID_6.9\"><\/a>\n\n## 6.9 Handling Missing values\n","bd1b4b15":"## Employment\n\n1 - yes full time\n\n2 - yes part time \n\n3 \u2013 no\n","b4f039c9":"The above picture shows the outliers in the GPA","894fad19":"<a id=\"section_ID_6.11\"><\/a>\n\n## 6.11 Scaling the weight feature","345c415d":"<a id=\"section_ID_h\"><\/a>\n\n### Copyright 2020 Abhishek Gargha Maheshwarappa\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","470a2549":"From it is clear the comfort_food_reasons_coded\tand comfort_food_reasons_coded.1 are one and the same and reduant, so dropping one of the column with less nan","f19777b4":"GPA contains many unwanted values like string and nan which has to be handled \n\nWe will replace the missing values with mean ","6b8629a5":"## Feature selection","4cfde4a5":"<a id=\"section_ID_6.7\"><\/a>\n\n## 6.7 Numerical Features","424124fd":"Looking at the above distrubution we can bin\/bucket them into four categories \n\n4.0 - 3.5 - mother_prof_set_a\n\n3.5 - 3.0 - mother_prof_set_b\n\n3.0 - 2.5 - mother_prof_set_c\n\n2.5 - 2.0 - mother_prof_set_d","57df1255":"Looking at the above distrubution we can bin\/bucket them into two categories \n\n4.0 - 3 - fav_cusine_set_a\n\n3 - 2 - fav_cusine_set_b","ee7ca1f1":"<a id=\"section_ID_6.1\"><\/a>\n\n## 6.1 Cleaning Weight column","5b03a69e":"Looking at the above distrubution we can bin\/bucket them into four categories \n\n4.0 - 3.5 - food_childhood_set_a\n\n3.5 - 3.0 - food_childhood_set_b\n\n3.0 - 2.5 - food_childhood_set_c\n\n2.5 - 2.0 - food_childhood_set_d","077dc4b3":"<a id=\"section_ID_8.2\"><\/a>\n\n## 8.2 Weight and GPA ","b58d795f":"<a id=\"section_ID_6.12\"><\/a>\n\n## 6.12 Define x and Y for buliding model","9b2ecd56":"## Train ,Validation and Test split\n\nData is split into 3 parts\n\nTaining data set = 80.75%\n\nValidation data set = 14.25%\n\nTest data set = 5%","9b93004b":"<a id=\"section_ID_9\"><\/a>\n\n## 9. Conclusion\n\n### Correlation between different cuisines for students with GPA above 3.0<br>\n\u2022\tA person who likes Greek food has 0.76, that is the highest correlation with Persian food<br>\n\u2022\tA person who likes Indian food has 0.74, highest correlation with Thai food<br>\n\u2022\tA person who likes Italian food has 0.33, highest correlation with Thai food<br>\n\u2022\tA person who likes Thai food has 0.74, highest correlation with Indian food<br>\n\u2022\tA person who likes Persian food has 0.83, highest correlation with Indian food<br>\n\u2022\tA person who likes Ethnic food has 0.69, highest correlation with Indian food<br>\n\n### Feature Engineering \n\u2022\tFeature engineering started with creating bin\/bucket for features containing huge amount of categorically data<br>\n\u2022\tOutliers were removed<br>\n\u2022\tThen recognizing the ordinal columns from the code book and partly by domain knowledge<br>\n\u2022\tFinally, other categorical feature were identified and dummy variable were created for them<br>\n\u2022\tWeight feature was scaled.<br>\n\u2022\tThe feature engineered columns performed very well in feature importance which proved the feature importance added value to the model prediction<br>\n\u2022\tThis was backed with good performance from the Random forest model, even on the cross validation<br>\n\n### GPA impacts employment<br>\n\u2022\tThe GPA and Part time has highest 0.074, means having higher GPA will lead higher chance of getting the part time job.<br>\n\u2022\tThe GPA and full time are also positively  correlated but not has high with part time employment<br>\n\u2022\tThe GPA and no employment are negatively correlated, that means if you get better GPA then there less chance of student getting not ending up any job <br>\n\n### Others\n\u2022\tThere is negative correlation between weight and the GPA, but correlation is very low also<br>\n\u2022\tThere less correlation between Gender and GPA. But the interesting fact is Male are negative correlated and Female are positive correlated. Which Female preform little better compared to male.<br>\n\u2022\tData has higher count of Female students<br>\n","21ec9c15":"<a id=\"section_ID_7\"><\/a>\n\n## 7. GPA impacts on employment","dc32746e":"<a id=\"section_ID_4\"><\/a>\n\n\n## 4. Correlation between different cuisines for students with GPA above 3.0?","a066f133":"<a id=\"section_ID_4.1\"><\/a>\n\n## 4.1 Cleaning GPA","68178f67":"<a id=\"section_ID_5\"><\/a>\n\n## 5. Correlation between diet type (healthy\/unhealthy etc.) and GPA","2bbd6ae4":"<a id=\"section_ID_3.1\"><\/a>\n\n## 3.1 Missing data ","41671151":"<a id=\"section_ID_3\"><\/a>\n\n## 3. Data Preprocessing","d6e4a28c":"<a id=\"section_ID_6\"><\/a>\n\n## 6. Model that forecasts student GPA using this data","6a263fc9":"After looking at the columns carefully we have duplicate data which can be dropped","8077395f":"<a id=\"section_ID_6.14\"><\/a>\n\n## 6.14 Training the model with important features","6c4966f5":"<a id=\"section_ID_6.10\"><\/a>\n\n## 6.10 Generating Dummy variables","95aa08cc":"<a id=\"section_ID_8.1\"><\/a>\n\n## 8.1 Gender and GPA impact","8965650d":"<a id=\"section_ID_6.6\"><\/a>\n\n## 6.6 Coded Features","6059608a":"Looking at the above distrubution we can bin\/bucket them into four categories \n\n4.0 - 3.5 - meals_dinner_friend_set_a\n\n3.5 - 3.0 - meals_dinner_friend_set_b\n\n3.0 - 2.5 - meals_dinner_friend_set_c\n\n2.5 - 2.0 - meals_dinner_friend_set_d","a7dc3d04":"<a id=\"section_ID_4.2\"><\/a>\n\n## 4.2 Outlier Removal","7993ddfe":"<a id=\"section_ID_8\"><\/a>\n\n## 8.valuable insights","2a77b974":"This is good accuracy.","52eb78c9":"<a id=\"section_ID_10\"><\/a>\n\n# 10.References \n\n1. [Seaborn](https:\/\/seaborn.pydata.org\/index.html)<br>\n2. [Geeks for Geeks](https:\/\/www.geeksforgeeks.org\/python-programming-language\/)<br>","d2bffa51":"From above box plot we see there some outliers which can be removed.\n\nFrom the figure it is clear value 2.50 are outliers which are is easy to remove from the data","10567b9b":"<a id=\"section_ID_6.4\"><\/a>\n\n## 6.4 Ordinal\n\nIt has features of both numerical and categorical data. But the numbers placed in categories have mathematical meaning. \n\nExample: movies rating on 1\u20135, etc. These values have a mathematical meaning.","d4be1e9a":"Looking at the above distrubution we can bin\/bucket them into three categories \n\n4.0 - 3.2 - sport_set_a\n\n3.1 - 2.8 - sport_set_b\n\n2.7 - 2   - sport_set_c\n","c1a9d89b":"<a id=\"section_ID_1\"><\/a>\n\n## 1.Introduction\n\nThe Aim of the notebook is anwer the the following question within the data\n\n    1.Is there any correlation between different cuisines for students with GPA above 3.0? \n    2.What is the correlation between diet type (healthy\/unhealthy etc.) and GPA? \n    3. Is it possible to create a model that forecasts student GPA using this data?If yes, how would you go about feature -  engineering? \n    4.How do you think GPA impacts employment? ","20d50f7d":"<a id=\"section_ID_6.5\"><\/a>\n\n## 6.5 Binary features","8af78920":"<a id=\"section_ID_6.3\"><\/a>\n\n## 6.3. Bin creation","de4443d6":"so we can drop the comfort_food_reasons_coded ","62e805c5":"# **Food choices and preferences of college students**","a9dd3afd":"<a id=\"section_ID_6.13\"><\/a>\n\n## 6.13. Modeling and Feature selection","2d27b6d2":"# Feature selection\nCurrently the threshold for feature selection is thres = 5% (0.05)","5d6c8338":"<a id=\"section_ID_2\"><\/a>\n\n## 2. Reading Data","a3361aca":"<a id=\"section_ID_6.8\"><\/a>\n\n## 6.8 Calorie Features","b7cc323c":"## Table of Contents\n\n\n1.[Introduction](#section_ID_1)<br> \n2.[Reading Data](#section_ID_2)<br>\n3.[Data Preprocessing](#section_ID_3)<br>\n > 3.1.[Missing data](#section_ID_3.1)<br>\n \n4.[Correlation between different cuisines for students with GPA above 3.0](#section_ID_4)<br>\n > 4.1[Cleaning GPA](#section_ID_4.1)<br>\n   4.2[Outlier Removal](#section_ID_4.2)<br>\n   4.3[Correlation between different cuisines for students](#section_ID_4.3)<br>\n\n5.[Correlation between diet type (healthy\/unhealthy etc.) and GPA](#section_ID_5)<br>\n6.[Model that forecasts student GPA using this data](#section_ID_6)<br>\n   > 6.1 [Cleaning Weight](#section_ID_6.1)<br>\n     6.2 [Feature Engineering](#section_ID_6.2)<br>\n     6.3 [Bin Creation](#section_ID_6.3)<br>\n     6.4 [Ordinal features](#section_ID_6.4)<br>\n     6.5 [Binary features](#section_ID_6.5)<br>\n     6.6 [Coded features](#section_ID_6.6)<br>\n     6.7 [Numerical features](#section_ID_6.7)<br>\n     6.8 [Calorie features](#section_ID_6.8)<br>\n     6.9 [Handling Missing values](#section_ID_6.9)<br>\n     6.10[Generating Dummy variables](#section_ID_6.10)<br>\n     6.11[Scaling the weight feature](#section_ID_6.11)<br>\n     6.12[Define x and Y for buliding model](#section_ID_6.12)<br>\n     6.13[Modeling and Feature selection](#section_ID_6.13)<br>\n     6.14[Training the model with important features](#section_ID_6.14)<br>\n\n7.[GPA impacts on employment](#section_ID_7)<br>\n8.[Insights](#section_ID_8)<br>\n >8.1[Gender and GPA impact](#section_ID_8.1)<br>\n  8.2[Correlation between GPA and Weight](#section_ID_8.2)<br>\n\n\n9. [Conclusion](#section_ID_9)<br>\n\n10.[References](#section_ID_10)<br>\n   ","1de6ffa0":"<a id=\"section_ID_6.2\"><\/a>\n\n## 6.2 Feature Engineering "}}