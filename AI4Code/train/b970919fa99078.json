{"cell_type":{"3afd1687":"code","2d516a08":"code","82ab288e":"code","c321dc68":"code","d6221177":"code","432fa2ee":"code","573ae494":"code","5b97484f":"code","35352b79":"code","9df7c269":"code","41b300a4":"code","60535ef2":"code","8ae70626":"code","1e3bab30":"code","7b2c2abc":"code","27a8e3d0":"code","4eecc8e6":"code","6a2d55bc":"markdown","575ff256":"markdown","53d14aea":"markdown","9588057d":"markdown","f09f5b69":"markdown","6ee607af":"markdown","fda2540e":"markdown","28b29c0e":"markdown","1667cd2a":"markdown","e9fb0723":"markdown","9d513c5d":"markdown","47d45270":"markdown","cf40f1dd":"markdown","97a8f619":"markdown","07e34dd3":"markdown","382a5718":"markdown"},"source":{"3afd1687":"import os\nimport shutil\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Dense, Activation,Dropout, MaxPooling2D,BatchNormalization\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model, load_model\nimport numpy as np\nimport time\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\nprint ('modules loaded')","2d516a08":"img_path=r'..\/input\/chess-pieces-detection-images-dataset\/KnightImages\/00000001.jpg'\nimg=plt.imread(img_path)\nprint ('Image shape is: ', img.shape)\nplt.axis('off')\nplt.imshow(img)","82ab288e":"sdir=r'..\/input\/chess-pieces-detection-images-dataset'\nclasslist=os.listdir(sdir)\nlabels=[]\nfilepaths=[]\nfor klass in classlist:\n    classpath=os.path.join(sdir, klass)\n    flist=os.listdir(classpath)\n    for f in flist:\n        fpath=os.path.join(classpath,f)\n        filepaths.append(fpath)\n        labels.append(klass)\nFseries=pd.Series(filepaths, name='filepaths')\nLseries = pd.Series(labels, name='labels')\ndf=pd.concat([Fseries, Lseries], axis=1)\nclass_count=len(list(train_df['labels'].unique()))\nprint('Number of classes in dataset is ', class_count)","c321dc68":"train_df, dummy_df =train_test_split(df, train_size=.9, shuffle=True, random_state=123, stratify= df['labels'])\nvalid_df, test_df = train_test_split(dummy_df, train_size=.5, shuffle=True, random_state=123, stratify =dummy_df['labels'])\nprint('train_df length: ', len(train_df), '  test_df length: ',len(test_df), '  valid_df length: ', len(valid_df))\nprint (train_df['labels'].value_counts())","d6221177":"def balance(train_df,max_samples, min_samples, column, working_dir, image_size):\n    train_df=train_df.copy()        \n    # make directories to store augmented images\n    aug_dir=os.path.join(working_dir, 'aug')\n    if os.path.isdir(aug_dir): # check if directory exists, if it does remove it and create a fresh empty directory\n        shutil.rmtree(aug_dir)\n    os.mkdir(aug_dir)\n    for label in train_df['labels'].unique():    \n        dir_path=os.path.join(aug_dir,label) # make sub directories in aug directory, one for each unique label (class)   \n        os.mkdir(dir_path)\n    # create and store the augmented images  \n    total=0\n    gen=ImageDataGenerator(horizontal_flip=True,  rotation_range=20, width_shift_range=.2,\n                                  height_shift_range=.2, zoom_range=.2)\n    groups=train_df.groupby('labels') # create dataframes one dataframe for each class\n    for label in train_df['labels'].unique():  # for every class               \n        group=groups.get_group(label)  # a dataframe holding only rows with the specified label \n        sample_count=len(group)   # determine how many samples there are in this class  \n        if sample_count< max_samples: # if the class has less than target number of images\n            aug_img_count=0\n            delta=max_samples-sample_count  # number of augmented images to create\n            target_dir=os.path.join(aug_dir, label)  # define where to write the images    \n            aug_gen=gen.flow_from_dataframe( group,  x_col='filepaths', y_col=None, target_size=image_size,\n                                            class_mode=None, batch_size=1, shuffle=False, \n                                            save_to_dir=target_dir, save_prefix='aug-', color_mode='rgb',\n                                            save_format='jpg')\n            while aug_img_count<delta:\n                images=next(aug_gen)            \n                aug_img_count += len(images)\n            total +=aug_img_count\n    print('Total Augmented images created= ', total)\n    # create aug_df and merge with train_df to create composite training set ndf\n    if total>0:\n        aug_fpaths=[]\n        aug_labels=[]\n        classlist=os.listdir(aug_dir)\n        for klass in classlist:\n            classpath=os.path.join(aug_dir, klass)     \n            flist=os.listdir(classpath)    \n            for f in flist:        \n                fpath=os.path.join(classpath,f)         \n                aug_fpaths.append(fpath)\n                aug_labels.append(klass)\n        Fseries=pd.Series(aug_fpaths, name='filepaths')\n        Lseries=pd.Series(aug_labels, name='labels')\n        aug_df=pd.concat([Fseries, Lseries], axis=1)\n        train_df=pd.concat([train_df,aug_df], axis=0).reset_index(drop=True)\n   \n    print (list(train_df['labels'].value_counts()) )\n    return train_df ","432fa2ee":"max_samples = 200\nmin_samples = 0\nimg_size = (224,224)\ncolumn = 'labels'  #make augmented images based onnumber of label samples\nworking_dir = r'.\/'\ntrain_dir=balance(train_df,max_samples, min_samples, column, working_dir, img_size)","573ae494":"\nbatch_size= 40\n# calculate test_batch_size and test_step so we go through test files exactly once\nlength=len(test_df)\ntest_batch_size=sorted([int(length\/n) for n in range(1,length+1) if length % n ==0 and length\/n<=80],reverse=True)[0]  \ntest_steps=int(length\/test_batch_size)\nprint ('test batch size= ', test_batch_size, '  test steps= ', test_steps)\ntrgen=ImageDataGenerator(horizontal_flip=True)\ntvgen=ImageDataGenerator()\ntrain_gen=trgen.flow_from_dataframe(train_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n                                    color_mode='rgb', shuffle=True, batch_size=batch_size)\nvalid_gen=tvgen.flow_from_dataframe(valid_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n                                    color_mode='rgb', shuffle=True, batch_size=batch_size)\ntest_gen=tvgen.flow_from_dataframe(test_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n                                    color_mode='rgb', shuffle=False, batch_size=test_batch_size)\n","5b97484f":"img_shape=(img_size[0], img_size[1], 3)\nmodel_name='EfficientNetB3'\nbase_model=tf.keras.applications.efficientnet.EfficientNetB3(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max') \nx=base_model.output\nx=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\nx = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\nx=Dropout(rate=.45, seed=123)(x)        \noutput=Dense(class_count, activation='softmax')(x)\nmodel=Model(inputs=base_model.input, outputs=output)\nmodel.compile(Adamax(learning_rate=.001), loss='categorical_crossentropy', metrics=['accuracy']) ","35352b79":"class ASK(keras.callbacks.Callback):\n    def __init__ (self, model, epochs,  ask_epoch): # initialization of the callback\n        super(ASK, self).__init__()\n        self.model=model               \n        self.ask_epoch=ask_epoch\n        self.epochs=epochs\n        self.ask=True # if True query the user on a specified epoch\n        \n    def on_train_begin(self, logs=None): # this runs on the beginning of training\n        if self.ask_epoch == 0: \n            print('you set ask_epoch = 0, ask_epoch will be set to 1', flush=True)\n            self.ask_epoch=1\n        if self.ask_epoch >= self.epochs: # you are running for epochs but ask_epoch>epochs\n            print('ask_epoch >= epochs, will train for ', epochs, ' epochs', flush=True)\n            self.ask=False # do not query the user\n        if self.epochs == 1:\n            self.ask=False # running only for 1 epoch so do not query user\n        else:\n            print('Training will proceed until epoch', ask_epoch,' then you will be asked to') \n            print(' enter H to halt training or enter an integer for how many more epochs to run then be asked again')  \n        self.start_time= time.time() # set the time at which training started\n        \n    def on_train_end(self, logs=None):   # runs at the end of training     \n        tr_duration=time.time() - self.start_time   # determine how long the training cycle lasted         \n        hours = tr_duration \/\/ 3600\n        minutes = (tr_duration - (hours * 3600)) \/\/ 60\n        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n        print (msg, flush=True) # print out training duration time\n        \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        if self.ask: # are the conditions right to query the user?\n            if epoch + 1 ==self.ask_epoch: # is this epoch the one for quering the user?\n                print('\\n Enter H to end training or  an integer for the number of additional epochs to run then ask again')\n                ans=input()\n                \n                if ans == 'H' or ans =='h' or ans == '0': # quit training for these conditions\n                    print ('you entered ', ans, ' Training halted on epoch ', epoch+1, ' due to user input\\n', flush=True)\n                    self.model.stop_training = True # halt training\n                else: # user wants to continue training\n                    self.ask_epoch += int(ans)\n                    if self.ask_epoch > self.epochs:\n                        print('\\nYou specified maximum epochs of as ', self.epochs, ' cannot train for ', self.ask_epoch, flush=True)\n                              \n                    else:\n                        print ('you entered ', ans, ' Training will continue to epoch ', self.ask_epoch, flush=True)","9df7c269":"rlronp=tf.keras.callbacks.ReduceLROnPlateau( monitor=\"val_loss\", factor=0.5,   patience=1,  verbose=1)\nestop=tf.keras.callbacks.EarlyStopping( monitor=\"val_loss\",   patience=4,  verbose=1,   restore_best_weights=True)\nepochs=40\nask_epoch=5 # at end of 5th epoch ask to enter H to halt training or an integer for how many more epochs to run then ask again\ncallbacks=[rlronp, estop, ASK( model,epochs, ask_epoch)]   \n","41b300a4":"history=model.fit(x=train_gen,  epochs=epochs, verbose=1, callbacks=callbacks,  validation_data=valid_gen,\n               validation_steps=None,  shuffle=False,  initial_epoch=0)","60535ef2":" acc= model.evaluate(test_gen, verbose= 1,  steps=test_steps)[1]  *100 \n print(f'Model accuracy on test set is {acc:6.2f}')","8ae70626":"working_dir=r'.\/'\nsave_path=os.path.join(working_dir, 'EfficientNetB3.h5')\nmodel.save(save_path, overwrite=True, include_optimizer=True, save_format='h5')","1e3bab30":"from sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\nsns.set_style('darkgrid')\nclasses=list(train_gen.class_indices.keys())\nclass_count=len(classes)\nlabels=test_gen.labels\nfiles=test_gen.filenames\nerror_file_list=[]\nindexes=[]\nerrors=0\npreds=model.predict(test_gen, steps=test_steps, verbose=1)\ntests=len(preds)\nfor i, p in enumerate (preds):\n    index=np.argmax(p) \n    indexes.append(index)\n    if index != labels[i]:\n        errors +=1\n        error_file_list.append(files[i])\nacc=( tests-errors)\/tests * 100\nprint(f'There were {errors}, errors in {tests} tests for an accuracy of {acc:6.2f} %' )\nif errors > 0:\n    print ('A list of files that were incorrectly predicted is shown below')\n    for i in range (len(error_file_list)):\n        print (error_file_list[i])\n\nclr = classification_report(labels, indexes, target_names=classes, digits= 4)\nprint(\"Classification Report:\\n----------------------\\n\", clr)\ncm = confusion_matrix(labels, indexes )        \nlength=len(classes)\nif length<8:\n    fig_width=8\n    fig_height=8\nelse:\n    fig_width= int(length * .5)\n    fig_height= int(length * .5)\nplt.figure(figsize=(fig_width, fig_height))\nsns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \nplt.xticks(np.arange(length)+.5, classes, rotation= 90)\nplt.yticks(np.arange(length)+.5, classes, rotation=0)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n","7b2c2abc":"def show_image_samples(gen ):\n    t_dict=gen.class_indices\n    classes=list(t_dict.keys())    \n    images,labels=next(gen) # get a sample batch from the generator \n    plt.figure(figsize=(20, 20))\n    length=len(labels)\n    if length<30:   #show maximum of 25 images\n        r=length\n    else:\n        r=40\n    for i in range(r):\n        plt.subplot(8, 5, i + 1)\n        image=images[i]\/255\n        plt.imshow(image)\n        index=np.argmax(labels[i])\n        class_name=classes[index]\n        plt.title(class_name, color='blue', fontsize=12)\n        plt.axis('off')\n    plt.show()","27a8e3d0":"show_image_samples (train_gen)","4eecc8e6":"import cv2\nimg=plt.imread(img_path) # read in the image shown earlier\nprint ('Input image shape is ', img.shape)\n# resize the image so it is the same size as the images the model was trained on\nimg=cv2.resize(img, img_size) # in earlier code img_size=(150,150) was used for training the model\nprint ('the resized image has shape ', img.shape)\n### show the resized image\nplt.axis('off')\nplt.imshow(img)\n# Normally the next line of code rescales the images. However the EfficientNet model expects images in the range 0 to 255\n# img= img\/255\n# plt.imread returns a numpy array so it is not necessary to convert the image to a numpy array\n# since we have only one image we have to expand the dimensions of img so it is off the form (1,150,150,3)\n# where the first dimension 1 is the batch size used by model.predict\nimg=np.expand_dims(img, axis=0)\nprint ('image shape after expanding dimensions is ',img.shape)\n# now predict the image\npred=model.predict(img)\nprint ('the shape of prediction is ', pred.shape)\n# this dataset has 15 classes so model.predict will return a list of 15 probability values\n# we want to find the index of the column that has the highest probability\nindex=np.argmax(pred[0])\n# to get the actual Name of the class earlier Imade a list of the class names called classes\nklass=classes[index]\n# lets get the value of the highest probability\nprobability=pred[0][index]*100\n# print out the class, and the probability \nprint(f'the image is predicted as being {klass} with a probability of {probability:6.2f} %')","6a2d55bc":"## Use transfer learning with EfficientNetB3 model","575ff256":"##  train_df is now balanced with 200 image samples per class, make train, test and valid generators","53d14aea":"## Actually rather surprised the F1 score is lower than I expected, not sure why. Only looked at 1 train image which\n## appears simple. Lets take a look at some images to see if they are really simple","9588057d":"## define a custom callback that after ask_epoch asks if you want to continue training or halt.\n## You can enter an integer for how many more\n## epochs to run then be asked again, or enter H to halt training","f09f5b69":"## train_df is not balanced  and has a limited number of samples. Define a function balance that creates augmented images\n## to bring each class up to max_samples images and creates a new dataframe where all classes have max_samples of image files","6ee607af":"## evaluate model on the test set","fda2540e":"## do predictions on the test set and generate classification report","28b29c0e":"## input an image and get the shape","1667cd2a":"## save the model","e9fb0723":"## set max_samples to value to 200 so each class will have 200 images\n## set image size = (224,224) do not need large image size for simple images\n## set working_dir as kaggle working directory where aug directory will be created","9d513c5d":"# split df into a train_df, a test_df and a valid df","47d45270":"## Can see some images are bad like all black bishop image. Looked at other images and found cases where\n## the image had two classes within the image which would confuse the model. ","cf40f1dd":"## train the model ","97a8f619":"## How to do predictions on a single image file","07e34dd3":"## create a dataframe of the form filepaths(path to the image file), labels (label of the file)","382a5718":"## Define an early stop callback and a reduce learning rate callback and instantiate the ASK callback"}}