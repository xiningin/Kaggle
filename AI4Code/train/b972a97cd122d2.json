{"cell_type":{"7993b933":"code","7f6de00e":"code","54aaf66e":"code","11276b27":"code","c5b02c00":"code","ec79a84b":"code","eba77262":"code","40e53f49":"code","a4f352e0":"code","dbcb3740":"code","79ae8da9":"code","e0dc3369":"code","705d8c79":"code","e642fca1":"code","84afaa5f":"code","a5aca3df":"code","239066a2":"code","2e292d7e":"code","2e13f158":"code","6cc39151":"code","4107bef0":"code","a06e1dc0":"code","6f81588c":"code","b6fe758c":"code","fdc1cb6f":"code","64827f1a":"code","de191772":"code","c3ac4c54":"code","03a69d1b":"code","d016f8f3":"code","aab71500":"code","a8b787f1":"code","0cc345bf":"code","d3b943e9":"code","0f685780":"code","167a2861":"code","f5bc10d7":"code","0b7280a2":"code","743f694f":"code","f950526e":"code","37512cd4":"code","7c411468":"code","2659ea9c":"code","16ccd76b":"code","4b56b3f2":"code","903ac488":"code","be22997b":"code","11b01b6e":"code","3473a88b":"code","89ae6685":"code","541bac9c":"code","4be56095":"code","af1097bf":"code","ee54d832":"code","32fe4dd3":"code","c55f0b0c":"code","830ac801":"code","f543d3fd":"code","dfe45ccf":"markdown","5a27a1ae":"markdown","33980e69":"markdown","6056501e":"markdown","44bd93bd":"markdown","f2775c8c":"markdown","46bd6c02":"markdown","a4c9473c":"markdown","55ddfb38":"markdown"},"source":{"7993b933":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7f6de00e":"from warnings import filterwarnings\nfilterwarnings('ignore')\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy as sp\nfrom sklearn.cluster import KMeans","54aaf66e":"df = pd.read_csv(\"..\/input\/usa-rests\/USArrests.csv\")\ndf.head()","11276b27":"df.index = df.iloc[:,0] # t\u00fcm g\u00f6zlemleri al 0.index de ki","c5b02c00":"df.index","ec79a84b":"df.head()","eba77262":"df = df.iloc[:,1:5]","40e53f49":"df.head()","a4f352e0":"df.isnull().sum()","dbcb3740":"df.info()","79ae8da9":"df.describe().T","e0dc3369":"df.hist(figsize = (10,10));","705d8c79":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters = 4)\nkmeans","e642fca1":"?kmeans","84afaa5f":"k_fit = kmeans.fit(df)","a5aca3df":"k_fit.n_clusters","239066a2":"k_fit.cluster_centers_","2e292d7e":"k_fit.labels_","2e13f158":"kmeans = KMeans(n_clusters = 2)\nk_fit = kmeans.fit(df)","6cc39151":"kumeler = k_fit.labels_","4107bef0":"plt.scatter(df.iloc[:,0], df.iloc[:,1], c = kumeler, s = 50, cmap = \"viridis\")\n\nmerkezler = k_fit.cluster_centers_\n\nplt.scatter(merkezler[:,0], merkezler[:,1], c = \"black\", s = 200, alpha = 0.5);","a06e1dc0":"df.head()","6f81588c":"df.iloc[:,0].head()","b6fe758c":"df.iloc[:,1].head()","fdc1cb6f":"from mpl_toolkits.mplot3d import Axes3D","64827f1a":"kmeans = KMeans(n_clusters = 3)\nk_fit = kmeans.fit(df)\nkumeler = k_fit.labels_\nmerkezler = kmeans.cluster_centers_","de191772":"plt.rcParams[\"figure.figsize\"] = (16,9)\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(df.iloc[:, 0], df.iloc[:, 1], df.iloc[:, 2]);","c3ac4c54":"fig = plt.figure()\nax = Axes3D(fig)\nax.scatter(df.iloc[:, 0], df.iloc[:, 1], df.iloc[:, 2], c=kumeler)\nax.scatter(merkezler[:, 0], merkezler[:, 1], merkezler[:, 2], \n           marker='*', \n           c='#050505', \n           s=1000);","03a69d1b":"kmeans = KMeans(n_clusters = 3)\nk_fit = kmeans.fit(df)\nkumeler = k_fit.labels_","d016f8f3":"pd.DataFrame({\"Eyaletler\" : df.index, \"Kumeler\": kumeler})[0:10]","aab71500":"df[\"kume_no\"] = kumeler","a8b787f1":"df.head()","0cc345bf":"df[\"kume_no\"] = df[\"kume_no\"] + 1","d3b943e9":"df.head()","0f685780":"from yellowbrick.cluster import KElbowVisualizer\nkmeans = KMeans()\nvisualizer = KElbowVisualizer(kmeans, k=(2,51))\nvisualizer.fit(df) \nvisualizer.poof()  ","167a2861":"kmeans = KMeans(n_clusters = 6)\nk_fit = kmeans.fit(df)\nkumeler = k_fit.labels_","f5bc10d7":"pd.DataFrame({\"Eyaletler\" : df.index, \"Kumeler\": kumeler})[0:10]","0b7280a2":"df = pd.read_csv(\"..\/input\/usa-rests\/USArrests.csv\").copy()\ndf.index = df.iloc[:,0]\ndf = df.iloc[:,1:5]\n#del df.index.name\ndf.head()","743f694f":"from scipy.cluster.hierarchy import linkage\nhc_complete = linkage(df, \"complete\")\nhc_average = linkage(df, \"average\")\nhc_single = linkage(df, \"single\")","f950526e":"dir(hc_complete)","37512cd4":"from scipy.cluster.hierarchy import dendrogram\n\nplt.figure(figsize=(15,10))\nplt.title(\"Hiyerar\u015fik K\u00fcmeleme - Dendogram\")\nplt.xlabel(\"Indexler\")\nplt.ylabel(\"Uzakl\u0131k\")\ndendrogram(hc_complete, leaf_font_size = 10);","7c411468":"from scipy.cluster.hierarchy import dendrogram\n\nplt.figure(figsize=(15, 10))\nplt.title('Hiyerar\u015fik K\u00fcmeleme - Dendogram')\nplt.xlabel('Indexler')\nplt.ylabel('Uzakl\u0131k')\ndendrogram(\n    hc_complete,\n    truncate_mode = \"lastp\",\n    p = 4,\n    show_contracted = True\n);","2659ea9c":"from scipy.cluster.hierarchy import dendrogram\n\nplt.figure(figsize = (15,10))\nplt.title(\"Hiyerar\u015fik K\u00fcmeleme - Dendrogram\")\nplt.xlabel(\"Indexler\")\nplt.ylabel(\"Uzakl\u0131k\")\nden = dendrogram(\n    hc_complete,\n    leaf_font_size=10\n);","16ccd76b":"?den","4b56b3f2":"?dendrogram","903ac488":"from sklearn.cluster import AgglomerativeClustering\ncluster = AgglomerativeClustering(n_clusters = 4,\n                                 affinity = \"euclidean\",\n                                 linkage = \"ward\")\n\ncluster.fit_predict(df)","be22997b":"pd.DataFrame({\"Eyaletler\" : df.index, \"Kumeler\": cluster.fit_predict(df)})[0:10]","11b01b6e":"df[\"kume_no\"] = cluster.fit_predict(df)","3473a88b":"df.head()","89ae6685":"df = pd.read_csv(\"..\/input\/usa-rests\/USArrests.csv\").copy()\ndf.index = df.iloc[:,0]\ndf = df.iloc[:,1:5]\n#del df.index.name\ndf.head()","541bac9c":"from warnings import filterwarnings\nfilterwarnings('ignore')","4be56095":"from sklearn.preprocessing import StandardScaler\n\ndf = StandardScaler().fit_transform(df)\ndf[0:5,0:5]","af1097bf":"from sklearn.decomposition import PCA\npca = PCA(n_components = 3)\npca_fit = pca.fit_transform(df)","ee54d832":"bilesen_df = pd.DataFrame(data = pca_fit, \n                          columns = [\"birinci_bilesen\",\"ikinci_bilesen\",\"ucuncu_bilesen\"])","32fe4dd3":"bilesen_df.head()","c55f0b0c":"pca.explained_variance_ratio_","830ac801":"pca = PCA().fit(df)","f543d3fd":"plt.plot(np.cumsum(pca.explained_variance_ratio_))","dfe45ccf":"# Hierarchical Clustering","5a27a1ae":"#### Visualization","33980e69":"# K-Means","6056501e":"### Optimum Number of Clusters","44bd93bd":"### Determining the Optimum Number of Clusters","f2775c8c":"### Model and Visualization on K-Means","46bd6c02":"# PCA","a4c9473c":"Bildi\u011finiz \u00fczere kNN gibi mesafeye dayal\u0131 y\u00f6ntemler kulland\u0131\u011f\u0131m\u0131zda, en geni\u015f aral\u0131\u011fa sahip olan \u00f6zellik, \u00e7\u0131kacak sonu\u00e7lara a\u011f\u0131r basacak ve daha az do\u011fru tahminler alaca\u011f\u0131z.\n\n\n\nBu nedenle hoca \"murder\"\u0131n a\u011f\u0131rl\u0131\u011f\u0131n\u0131 artt\u0131rmak gerekti\u011finden bahsediyor(\u00e7\u00fcnk\u00fc varyans\u0131 ve aral\u0131\u011f\u0131 di\u011ferlerine k\u0131yasla d\u00fc\u015f\u00fck). Bunu yapman\u0131n iki yolu var:\n\n\n\n1) \u00d6l\u00e7ek ile oynamak (feature scaling)\n\n- Standardizasyon(bizim verimiz i\u00e7in uygun de\u011fil)***\n\n- Normalizasyon\n\n- Min-Max d\u00f6n\u00fc\u015f\u00fcm\u00fc\n\n\n\n2) Yeni bir de\u011fi\u015fken eklemek \n-11'den fazla olanlara 1, az olanlara 0 de\u011feri verilir..\n\n-11'den daha fazla cinayet i\u015fleyen eyaletlere 10 de\u011feri verilir, min de\u011ferden daha az cinayet i\u015fleyenlere 1 de\u011feri verilir. Arada kalanlara direk mean de\u011feri verilir. B\u00f6ylelikle murder de\u011fi\u015fkeni a\u011f\u0131rl\u0131\u011f\u0131 veri setinde hissedilir hale getirilmi\u015f olur...\n\n-\u0130lave olarak min'den az olan de\u011ferlerin etkisini trashlay\u0131p, 11'den fazla olan de\u011ferlere 10 ekleyerek a\u011f\u0131rl\u0131\u011f\u0131n\u0131 artt\u0131rabiliriz. (yani burada yine yeni bir de\u011fi\u015fken eklemi\u015f oluyoruz.)\n\n\n\n\n\n***Standardizasyon(bizim verimiz i\u00e7in uygun de\u011fil):\n\n- Standartla\u015ft\u0131r\u0131p k\u00fcme olu\u015fturursak, etkileri, varyanslar\u0131, meanlerini ayn\u0131 kabul etmi\u015f olaca\u011f\u0131z, bu nedenle bu veri seti i\u00e7in bunu tercih etmiyoruz.\n\n-Standartla\u015ft\u0131rmadan k\u00fcme olu\u015fturacak olursak, \u00f6l\u00e7ek ve varyanslar\u0131 b\u00fcy\u00fck olan de\u011fi\u015fkenler otomatik olarak daha a\u011f\u0131rl\u0131kl\u0131 ya da etkili olarak kabul edilecekler.\n\n","55ddfb38":"### Clusters and Observation Units"}}