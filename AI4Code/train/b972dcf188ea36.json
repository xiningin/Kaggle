{"cell_type":{"47d09420":"code","adf3fe5d":"code","65eec1de":"code","aac1ebb5":"code","7a2ad689":"code","30755a6f":"code","52d535e0":"code","ac3493cd":"code","66e99426":"code","2ae1b103":"code","09cce924":"code","441ec375":"code","bbd45ebb":"code","0bd1902f":"code","c65fb6af":"code","9460d7bd":"code","905005c4":"code","8c1ef468":"code","25404219":"code","b224a025":"code","361344d3":"code","a7296c28":"code","f9af3588":"code","ad6bfc7a":"code","e545051c":"code","afb41084":"code","2d390f89":"code","413d803e":"code","c67e0578":"code","eac50eca":"code","0b0cbe17":"code","6eca86b1":"code","a5fc42fb":"code","96b47bd1":"code","43948d1f":"code","6520e399":"code","9509341f":"code","8d81255f":"code","855e617f":"code","68b0985e":"code","c40b73b3":"code","8911e5e6":"code","76576306":"code","c7ccbecd":"code","f7b1beff":"code","a4bcf851":"code","155710c6":"code","c55006cc":"code","cf913fb6":"code","28ce05de":"markdown","9eee1590":"markdown","7053bf71":"markdown","2bdbc14f":"markdown","2d392b4c":"markdown","2933050b":"markdown","77b45a36":"markdown","8cc6ffc6":"markdown","62d5751d":"markdown","d239c27e":"markdown","37413ddc":"markdown","5b562de7":"markdown","e8e5b9ed":"markdown","5a8ac09f":"markdown","8e09432f":"markdown","1025a589":"markdown","961d575d":"markdown","01a4c7f0":"markdown","c3a48c90":"markdown","551d3414":"markdown","96d78742":"markdown","7ef063a0":"markdown","b7ad4932":"markdown","26157275":"markdown","23f5668f":"markdown","5881fd5f":"markdown","1e081f45":"markdown","b357ab29":"markdown","27c24868":"markdown","fb068fef":"markdown","7c954d8d":"markdown","df200c29":"markdown","13d765d4":"markdown","63c8c161":"markdown","58a1e4f7":"markdown","5b14314d":"markdown","6402891b":"markdown","cb60eb00":"markdown","705d35b1":"markdown"},"source":{"47d09420":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import chi2_contingency\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n%matplotlib inline","adf3fe5d":"train=pd.read_csv(\"..\/input\/train.csv\")\ntest=pd.read_csv(\"..\/input\/test.csv\")\n\nprint(\"Train dataset has {} samples and {} attributes\".format(*train.shape))\nprint(\"Test dataset has {} samples and {} attributes\".format(*test.shape))","65eec1de":"train.head()","aac1ebb5":"fig , ax = plt.subplots(figsize=(6,4))\nsns.countplot(x='Survived', data=train)\nplt.title(\"Count of Survival\")\nplt.show()","7a2ad689":"n=len(train)\nsurv_0=len(train[train['Survived']==0])\nsurv_1=len(train[train['Survived']==1])\n\nprint(\"% of passanger survived in train dataset: \",surv_1*100\/n)\nprint(\"% of passanger not survived in train dataset: \",surv_0*100\/n)","30755a6f":"cat=['Pclass','Sex','Embarked']\nnum=['Age','SibSp','Parch','Fare']","52d535e0":"corr_df=train[num]  #New dataframe to calculate correlation between numeric features\ncor= corr_df.corr(method='pearson')\nprint(cor)","ac3493cd":"fig, ax =plt.subplots(figsize=(8, 6))\nplt.title(\"Correlation Plot\")\nsns.heatmap(cor, mask=np.zeros_like(cor, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax)\nplt.show()","66e99426":"csq=chi2_contingency(pd.crosstab(train['Survived'], train['Sex']))\nprint(\"P-value: \",csq[1])","2ae1b103":"csq2=chi2_contingency(pd.crosstab(train['Survived'], train['Embarked']))\nprint(\"P-value: \",csq2[1])","09cce924":"csq3=chi2_contingency(pd.crosstab(train['Survived'], train['Pclass']))\nprint(\"P-value: \",csq3[1])","441ec375":"fig, ax=plt.subplots(figsize=(8,6))\nsns.countplot(x='Survived', data=train, hue='Sex')\nax.set_ylim(0,500)\nplt.title(\"Impact of Sex on Survived\")\nplt.show()","bbd45ebb":"fig, ax=plt.subplots(figsize=(8,6))\nsns.countplot(x='Survived', data=train, hue='Embarked')\nax.set_ylim(0,500)\nplt.title(\"Impact of Embarked on Survived\")\nplt.show()","0bd1902f":"fig, ax=plt.subplots(figsize=(8,6))\nsns.countplot(x='Survived', data=train, hue='Pclass')\nax.set_ylim(0,400)\nplt.title(\"Impact of Pclass on Survived\")\nplt.show()","c65fb6af":"fig, ax=plt.subplots(1,figsize=(8,6))\nsns.boxplot(x='Survived',y='Fare', data=train)\nax.set_ylim(0,300)\nplt.title(\"Survived vs Fare\")\nplt.show()","9460d7bd":"print(train.isnull().sum())","905005c4":"print(test.isnull().sum())","8c1ef468":"train['Age'].describe()","25404219":"med=np.nanmedian(train['Age'])\ntrain['Age']=train['Age'].fillna(med)\ntest['Age']=test['Age'].fillna(med)","b224a025":"train['Cabin'].value_counts()","361344d3":"train['Cabin']=train['Cabin'].fillna(0)\ntest['Cabin']=test['Cabin'].fillna(0)","a7296c28":"train['Embarked'].value_counts()","f9af3588":"train['Cabin']=train['Cabin'].fillna(\"S\")","ad6bfc7a":"train['Fare'].describe()","e545051c":"med=np.nanmedian(train['Fare'])\ntest['Fare']=test['Fare'].fillna(med)","afb41084":"train['hasCabin']=train['Cabin'].apply(lambda x: 0 if x==0 else 1)\ntest['hasCabin']=test['Cabin'].apply(lambda x: 0 if x==0 else 1)","2d390f89":"train['FamilyMem']=train.apply(lambda x: x['SibSp']+x['Parch'], axis=1)\ntest['FamilyMem']=test.apply(lambda x: x['SibSp']+x['Parch'], axis=1)","413d803e":"def get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    if title_search:\n        return title_search.group(1)\n    return \"\"","c67e0578":"train['title']=train['Name'].apply(get_title)\ntest['title']=test['Name'].apply(get_title)","eac50eca":"title_lev1=list(train['title'].value_counts().reset_index()['index'])\ntitle_lev2=list(test['title'].value_counts().reset_index()['index'])","0b0cbe17":"title_lev=list(set().union(title_lev1, title_lev2))\nprint(title_lev)","6eca86b1":"train['title']=pd.Categorical(train['title'], categories=title_lev)\ntest['title']=pd.Categorical(test['title'], categories=title_lev)","a5fc42fb":"cols=['Pclass','Sex','Embarked','hasCabin','title']\nfcol=['Pclass','Sex','Embarked','hasCabin','title','Age','FamilyMem','Fare']","96b47bd1":"for c in cols:\n    train[c]=train[c].astype('category')\n    test[c]=test[c].astype('category')","43948d1f":"train_df=train[fcol]\ntest_df=test[fcol]","6520e399":"train_df=pd.get_dummies(train_df, columns=cols, drop_first=True)\ntest_df=pd.get_dummies(test_df, columns=cols, drop_first=True)","9509341f":"y=train['Survived']","8d81255f":"x_train, x_test, y_train, y_test = train_test_split(train_df, y, test_size=0.3, random_state=42)","855e617f":"rfc=RandomForestClassifier(random_state=42)","68b0985e":"param_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}","c40b73b3":"CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\nCV_rfc.fit(x_train, y_train)","8911e5e6":"CV_rfc.best_params_","76576306":"rfc1=RandomForestClassifier(random_state=42, max_features='auto', n_estimators= 200, max_depth=8, criterion='gini')","c7ccbecd":"rfc1.fit(x_train, y_train)","f7b1beff":"pred=rfc1.predict(x_test)","a4bcf851":"print(\"Accuracy for Random Forest on CV data: \",accuracy_score(y_test,pred))","155710c6":"op_rf=rfc1.predict(test_df)","c55006cc":"op=pd.DataFrame(test['PassengerId'])\nop['Survived']=op_rf","cf913fb6":"op.to_csv(\"Submission.csv\", index=False)","28ce05de":"## Visualization ","9eee1590":"Passengers from **Pclass 3** have lesser chances of Survival while passengers from **Pclass 1** have higher chances of survival","7053bf71":"#### Cabin","2bdbc14f":"## Model","2d392b4c":"So these features contribute by providing some information.","2933050b":"# Introduction\n- In this kernel you will learn what is Grid Search and how to implement it using <a href='https:\/\/scikit-learn.org\/stable\/'>SkLearn<\/a>\n- We are going to use the <a href='https:\/\/www.kaggle.com\/c\/titanic'>Titanic<\/a> dataset.","77b45a36":"#### Embarked****","8cc6ffc6":"## Titanic\n- The objective is to predict if passanger has survived or not.","62d5751d":"## Handling Missing Values","d239c27e":"Pclass, Sex and Embarked are **Categorical** Features while Age, SibSp, Parch and Fare are **continuous** variables.","37413ddc":"#### Age","5b562de7":"Ratio of Survived and Not Survived passangers for S and Q Embarked are similar but Passengers from C embarked have higer chances of survival.","e8e5b9ed":"Let's replace NaN by 0","5a8ac09f":"Let's replace the NaN by mode","8e09432f":" lets keep all the features as there is no strong evidence of data redundancy.","1025a589":"Only 4 features have missing values","961d575d":"* Let's check  features containing missing values","01a4c7f0":"We can say that Female passangers have higher probability of survival than Male passangers","c3a48c90":"We have 11 feature columns and target variable **Survived** which is binary.","551d3414":"## EDA","96d78742":"### Assigning datatypes","7ef063a0":"There's no strong correlation between any two variables. The strongest correlation is between **SibSp** and **Parch** features (0.414).","b7ad4932":"## Feature Engineering","26157275":"We will use Name, Ticket and Cabin variable in Feature Engineering","23f5668f":"Let's combine SibSp and Parch features to create new one **FamilyMem**","5881fd5f":"### Random Forest","1e081f45":"And even if we do nothing we would get approximately 61% accuracy by simple marking all passangers as not survived(**Accuracy Paradox**). So our aim should be to get accuracy higher than this.","b357ab29":"P values for features Sex, Embarked and Pclass are very low. So we can reject our Null Hypothesis which states that \" features are independent and have no relationship with target variable\"","27c24868":"from **cabin** let's create a new feature **hasCabin** ","fb068fef":"Average Fare for passangers who survived is higher than not survived.","7c954d8d":"### Let's use chi-square test to understand relationship between categorical variables and target variable","df200c29":"###  Let's create dummy variables","13d765d4":"#### Fare","63c8c161":"First Let's check the impact of feature **Sex** on **Survived**","58a1e4f7":"### Let's find correlation between Numeric Variable","5b14314d":"Let's use prefixes in the name to Create a new column **Title** ","6402891b":"Passanger who didn't  survived has edge over survived passanger.  ","cb60eb00":"## What is Grid Search\n- In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm.\n- The traditional way of performing hyperparameter optimization has been grid search, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set or evaluation on a held-out validation set.\n- Here's the official document of <a href='https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html'>Grid Search<\/a>.","705d35b1":"Let's replace missing values by median of Age."}}