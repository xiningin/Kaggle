{"cell_type":{"358770f1":"code","7ef1efbd":"code","7e3fd629":"code","d127cf3b":"code","a9d8488a":"code","bc9414fd":"code","89aefdc0":"code","f3a6cea0":"code","12b9d46d":"code","b4dc9d9c":"code","c89001c9":"code","b422f941":"code","4352e099":"code","7a7f17ab":"code","b6b42904":"code","b9e2b79f":"code","049df4ca":"code","7ccb82b3":"code","5848a819":"code","133ee517":"code","d69907cc":"code","0f097966":"code","3765b139":"code","595a8cf5":"code","727124f3":"code","4d2e53d3":"code","ed1febbc":"code","6fb7fb44":"code","ec1616cf":"markdown","334c1b08":"markdown","bbf35672":"markdown","2274c638":"markdown"},"source":{"358770f1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n","7ef1efbd":"#IMPORTING LIBRARIES\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport nltk.classify.util\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.classify import NaiveBayesClassifier\nimport numpy as np\nimport re\nimport string\nimport nltk\n%matplotlib inline","7e3fd629":"#importing dataset\ndf = pd.read_csv(r\"..\/input\/1429_1.csv\")\ndf.head()","d127cf3b":"dff = df[['reviews.rating' , 'reviews.text' , 'reviews.title' , 'reviews.username']]\nprint(dff.isnull().sum()) #Checking for null values\ndff.head()","a9d8488a":"#Filtering Null Values\ncheck =  dff[dff[\"reviews.rating\"].isnull()]\ncheck.head()","bc9414fd":"#Filtering not null values\nsenti=dff[dff[\"reviews.rating\"].notnull()]\ndff.head()","89aefdc0":"#Classifying text based on sentiments(positive or negative)\nsenti[\"senti\"] = senti[\"reviews.rating\"]>=4\nsenti[\"senti\"] = senti[\"senti\"].replace([True , False] , [\"pos\" , \"neg\"])","f3a6cea0":"#Count of reviews based on sentiments\nsenti[\"senti\"].value_counts().plot.bar(color=['cyan', 'blue'])","12b9d46d":"#Cleaning Text\nimport nltk.classify.util\nfrom nltk.classify import NaiveBayesClassifier\nimport numpy as np\nimport re\nimport string\nimport nltk\n\ncleanup_re = re.compile('[^a-z]+')\ndef cleanup(sentence):\n    sentence = str(sentence)\n    sentence = sentence.lower()\n    sentence = cleanup_re.sub(' ', sentence).strip()\n    #sentence = \" \".join(nltk.word_tokenize(sentence))\n    return sentence\n\nsenti[\"Summary_Clean\"] = senti[\"reviews.text\"].apply(cleanup)\ncheck[\"Summary_Clean\"] = check[\"reviews.text\"].apply(cleanup)","b4dc9d9c":"#Splitting training and testing dataset\nsplit = senti[[\"Summary_Clean\" , \"senti\"]]\ntrain=split.sample(frac=0.8,random_state=200)\ntest=split.drop(train.index)","c89001c9":"#Feature extracter for NLTK Naives Bayes\ndef word_feats(words):\n    features = {}\n    for word in words:\n        features [word] = True\n    return features","b422f941":"train[\"words\"] = train[\"Summary_Clean\"].str.lower().str.split()\ntest[\"words\"] = test[\"Summary_Clean\"].str.lower().str.split()\ncheck[\"words\"] = check[\"Summary_Clean\"].str.lower().str.split()\n\ntrain.index = range(train.shape[0])\ntest.index = range(test.shape[0])\ncheck.index = range(check.shape[0])\nprediction =  {} ## For storing results of different classifiers\n\ntrain_naive = []\ntest_naive = []\ncheck_naive = []\n\nfor i in range(train.shape[0]):\n    train_naive = train_naive +[[word_feats(train[\"words\"][i]) , train[\"senti\"][i]]]\nfor i in range(test.shape[0]):\n    test_naive = test_naive +[[word_feats(test[\"words\"][i]) , test[\"senti\"][i]]]\nfor i in range(check.shape[0]):\n    check_naive = check_naive +[word_feats(check[\"words\"][i])]\n\n\nclassifier = NaiveBayesClassifier.train(train_naive)\nprint(\"NLTK Naive bayes Accuracy : {}\".format(nltk.classify.util.accuracy(classifier , test_naive)))\nclassifier.show_most_informative_features(5)","4352e099":"#Predicting result for NLTK Classifier\ny =[]\nonly_words= [test_naive[i][0] for i in range(test.shape[0])]\nfor i in range(test.shape[0]):\n    y = y + [classifier.classify(only_words[i] )]\nprediction[\"Naive\"]= np.asarray(y)\n\ny1 = []\nfor i in range(check.shape[0]):\n    y1 = y1 + [classifier.classify(check_naive[i] )]\n\ncheck[\"Naive\"] = y1","7a7f17ab":"#BUILDING COUNTER AND TFIDF VECTOR FOR TEST<TRAIN AND CHECK DATA\nfrom wordcloud import STOPWORDS\n\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nstopwords = set(STOPWORDS)\nstopwords.remove(\"not\")\n\ncount_vect = CountVectorizer(min_df=2 ,stop_words=stopwords , ngram_range=(1,2))\ntfidf_transformer = TfidfTransformer()\n\nX_train_counts = count_vect.fit_transform(train[\"Summary_Clean\"])        \nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n\n\nX_new_counts = count_vect.transform(test[\"Summary_Clean\"])\nX_test_tfidf = tfidf_transformer.transform(X_new_counts)\n\ncheckcounts = count_vect.transform(check[\"Summary_Clean\"])\nchecktfidf = tfidf_transformer.transform(checkcounts)","b6b42904":"#Fitting Multinomial NB\nfrom sklearn.naive_bayes import MultinomialNB\nmodel1 = MultinomialNB().fit(X_train_tfidf , train[\"senti\"])\nprediction['Multinomial'] = model1.predict_proba(X_test_tfidf)[:,1]\nprint(\"Multinomial Accuracy : {}\".format(model1.score(X_test_tfidf , test[\"senti\"])))\n\ncheck[\"multi\"] = model1.predict(checktfidf)## Predicting Sentiment for Check which was Null values for rating","b9e2b79f":"#Fitting Bernouli NB\nfrom sklearn.naive_bayes import BernoulliNB\nmodel2 = BernoulliNB().fit(X_train_tfidf,train[\"senti\"])\nprediction['Bernoulli'] = model2.predict_proba(X_test_tfidf)[:,1]\nprint(\"Bernoulli Accuracy : {}\".format(model2.score(X_test_tfidf , test[\"senti\"])))\n\ncheck[\"Bill\"] = model2.predict(checktfidf)## Predicting Sentiment for Check which was Null values for rating","049df4ca":"#Fitting Logistic Regression\nfrom sklearn import linear_model\nlogreg = linear_model.LogisticRegression(solver='lbfgs' , C=1000)\nlogistic = logreg.fit(X_train_tfidf, train[\"senti\"])\nprediction['LogisticRegression'] = logreg.predict_proba(X_test_tfidf)[:,1]\nprint(\"Logistic Regression Accuracy : {}\".format(logreg.score(X_test_tfidf , test[\"senti\"])))\n\ncheck[\"log\"] = logreg.predict(checktfidf)## Predicting Sentiment for Check which was Null values for rating","7ccb82b3":"#Getting Most Occurence words in training test\nwords = count_vect.get_feature_names()\nfeature_coefs = pd.DataFrame(\n    data = list(zip(words, logistic.coef_[0])),\n    columns = ['feature', 'coef'])\nfeature_coefs.sort_values(by=\"coef\")","5848a819":"#To check out which classifier is doing what\ndef formatt(x):\n    if x == 'neg':\n        return 0\n    if x == 0:\n        return 0\n    return 1\nvfunc = np.vectorize(formatt)\n\ncmp = 0\ncolors = ['k', 'b', 'g', 'm', 'y']\nfor model, predicted in prediction.items():\n    if model not in 'Naive':\n        false_positive_rate, true_positive_rate, thresholds = roc_curve(test[\"senti\"].map(vfunc), predicted)\n        roc_auc = auc(false_positive_rate, true_positive_rate)\n        plt.plot(false_positive_rate, true_positive_rate, colors[cmp], label='%s: AUC %0.2f'% (model,roc_auc))\n        cmp += 1\n\nplt.title('Classifiers comparaison with ROC')\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.2])\nplt.ylim([-0.1,1.2])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","133ee517":"test.senti = test.senti.replace([\"pos\" , \"neg\"] , [True , False] )","d69907cc":"keys = prediction.keys()\nfor key in ['Multinomial', 'Bernoulli', 'LogisticRegression']:\n    print(\" {}:\".format(key))\n    print(metrics.classification_report(test[\"senti\"], prediction.get(key)>.5, target_names = [\"positive\", \"negative\"]))\n    print(\"\\n\")","0f097966":"#lets test our classifier with some samples\ndef test_sample(model, sample):\n    sample_counts = count_vect.transform([sample])\n    sample_tfidf = tfidf_transformer.transform(sample_counts)\n    result = model.predict(sample_tfidf)[0]\n    prob = model.predict_proba(sample_tfidf)[0]\n    print(\"Sample estimated as %s: negative prob %f, positive prob %f\" % (result.upper(), prob[0], prob[1]))\n\ntest_sample(logreg, \"The product was good and easy to  use\")\ntest_sample(logreg, \"the whole experience was just horrible and product is worst\")\ntest_sample(logreg, \"product is worst\")","3765b139":"check.head(5)","595a8cf5":"#WORDCLOUD for overall reviews\nfrom wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\n\n\nmpl.rcParams['font.size']=12                #10 \nmpl.rcParams['savefig.dpi']=100             #72 \nmpl.rcParams['figure.subplot.bottom']=.1 \n\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=300,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n        \n    ).generate(str(data))\n    \n    fig = plt.figure(1, figsize=(15, 15))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()\nshow_wordcloud(senti[\"Summary_Clean\"])","727124f3":"#WORDCLOUD based on positive reviews\nshow_wordcloud(senti[\"Summary_Clean\"][senti.senti == \"pos\"] , title=\"Postive Words\")","4d2e53d3":"#wordcloud based on negative reviews\nshow_wordcloud(senti[\"Summary_Clean\"][senti.senti == \"neg\"] , title=\"Negative Words\")","ed1febbc":"#pie plot\nsenti[\"senti\"].value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',shadow=True,figsize=(10,8))\nplt.show()","6fb7fb44":"#Statistical Summary of the data\nsenti.describe().plot(kind = \"area\",fontsize=27, figsize = (20,8), table = True,colormap=\"summer\")\nplt.xlabel('Statistics',)\nplt.ylabel('Value')\nplt.title(\"General Statistics of Products Dataset\")","ec1616cf":"This kernel notebook provides the insight of Amazon Products reviews, analyze the sentiments of customer,counts the number of of positive and negative reviews,summary of statistical data ,data visualization, formation of wordclouds based on overall reviews, positive reviews and negative reviews,fitting the dataset into different models to find out the accuracy score of the dataset.\nHope it will be useful for the beginner.","334c1b08":"About This Data:\nThis is a list of over 34,000 customer reviews for Amazon products like the Kindle, Fire TV Stick, and more provided by Datafiniti's Product Database. The dataset includes basic product information, rating, review text, and more for each product.","bbf35672":"Thanks for reading my kernel .If you found it helpful,then please upvote it","2274c638":"Lets see precision and recall of different classifiers"}}