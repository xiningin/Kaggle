{"cell_type":{"5e25aa91":"code","fc8319d9":"code","ec1eebda":"code","981768f8":"code","95e9dc1b":"code","7fba9e46":"code","adbf11cf":"code","fef1473c":"code","660ec6f8":"code","0afb330b":"code","b2ce302a":"code","0e6708f6":"code","f19f8224":"code","cf358874":"code","0a5a81a6":"code","1ca115e0":"code","12c62ca7":"code","e876f053":"code","6dbedce2":"markdown","b1021f01":"markdown","30da34fd":"markdown","a9adbefd":"markdown","0ecb6f40":"markdown","cd17db3a":"markdown","95ed244a":"markdown","da6dec1e":"markdown","5d1b1e8a":"markdown","482518e0":"markdown","6b5e9f81":"markdown"},"source":{"5e25aa91":"import matplotlib.patches as patches\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport os\n\nsns.set(style=\"darkgrid\")","fc8319d9":"DIR = \"..\/input\/global-wheat-detection\/\"\nTRAIN = \"train.csv\"\nSUBMISSION = \"sample_submission.csv\"\nTRAIN_IMAGES = \"train\"\nTEST_IMAGES = \"test\"\nWIDTH = 1024\nHEIGHT = 1024\n\nTRAIN_IMAGES = [os.path.join(DIR, \"train\", fname) for fname in os.listdir(os.path.join(DIR, \"train\"))]\nTEST_IMAGES = [os.path.join(DIR, \"test\", fname) for fname in os.listdir(os.path.join(DIR, \"test\"))]\n\ntrain_df = pd.read_csv(os.path.join(DIR, TRAIN))\nsubmission_df = pd.read_csv(os.path.join(DIR, TRAIN))","ec1eebda":"print(train_df.shape)\nprint(f\"Total training images: {len(TRAIN_IMAGES)}\")\nprint(f\"Total test images: {len(TEST_IMAGES)}\")\ntrain_df.head()","981768f8":"plt.figure(figsize=(12, 6))\nsns.countplot(train_df.source)\nplt.show()","95e9dc1b":"train_df.source.value_counts()","7fba9e46":"bbox_counts_by_source = train_df.groupby([\"source\"]).apply(lambda x:x[\"image_id\"].value_counts().mean())","adbf11cf":"plt.figure(figsize=(10, 6))\nbbox_counts_by_source.plot(kind='bar')\nplt.show()","fef1473c":"bbox_counts_by_source","660ec6f8":"box_count = train_df[\"image_id\"].value_counts()\nprint(f\"Min boxes: {box_count.min()}\")\nprint(f\"Max boxes: {box_count.max()}\")\nprint(f\"Mean boxes: {box_count.mean()}\")\nprint(f\"Std boxes: {box_count.std()}\")","0afb330b":"plt.figure(figsize=(10, 6))\nsns.distplot(box_count.values)\nplt.show()","b2ce302a":"bbox = lambda bbox: [float(x) for x in bbox[1:-1].split(\",\")]\ntrain_df.bbox = train_df.bbox.apply(bbox)\ntrain_df['xmin'] = train_df.bbox.apply(lambda x: x[0])\ntrain_df['ymin'] = train_df.bbox.apply(lambda x: x[1])\ntrain_df['width'] = train_df.bbox.apply(lambda x: x[2])\ntrain_df['height'] = train_df.bbox.apply(lambda x: x[3])","0e6708f6":"area_percent = train_df['width']*train_df['height'] \/ (WIDTH*HEIGHT)\nplt.figure(figsize=(10, 6))\nplt.title(\"Area % for whole dataset.\")\nprint(f\"Min area: {area_percent.min()}%\")\nprint(f\"Max area: {area_percent.max()}%\")\nprint(f\"Mean area: {area_percent.mean()}%\")\nprint(f\"Std area: {area_percent.std()}%\")\nsns.distplot(area_percent)\nplt.show()","f19f8224":"area_per_image = train_df.groupby(\"image_id\").apply(lambda x: (x[\"width\"]*x[\"height\"]).sum()\/(WIDTH*HEIGHT))\nplt.figure(figsize=(10, 6))\nplt.title(\"Area % for each image.\")\nprint(f\"Min area per image: {area_per_image.min()}%\")\nprint(f\"Max area per image: {area_per_image.max()}%\")\nprint(f\"Mean area per image: {area_per_image.mean()}%\")\nprint(f\"Std area per image: {area_per_image.std()}%\")\nsns.distplot(area_per_image)\nplt.show()","cf358874":"plt.figure(figsize=(10, 6))\nsns.distplot(train_df.height)\nplt.show()\n\nplt.figure(figsize=(10, 6))\nsns.distplot(train_df.width)\nplt.show()","0a5a81a6":"def get_boxes(df):\n    xmins, ymins, widths, heights = df['xmin'],df['ymin'], df['width'], df['height']\n    ps = []\n    for i in range(len(xmins)):\n        p = patches.Rectangle((xmins.iloc[i], ymins.iloc[i]),widths.iloc[i], heights.iloc[i], linewidth=2, edgecolor='c', facecolor='none')\n        ps.append(p)\n    return ps  \n\ndef show_img_bbox(rows=2, columns=2, source=None):\n    \"\"\"\n    source: Random selection only from images from `source`. \n    Thanks to https:\/\/www.kaggle.com\/devvindan\/wheat-detection-eda for the idea of this.\n    \"\"\"\n    fig = plt.figure(figsize=(int(8*columns), int(8*rows)))\n    if source is not None:\n        image_names = np.random.choice(train_df[train_df.source==source].image_id.unique(), columns*rows)\n    image_names = np.random.choice(train_df.image_id.unique(), columns*rows)\n    image_paths = [os.path.join(DIR, 'train', img+\".jpg\") for img in image_names]\n    for i in range(1, columns*rows +1):\n        img = Image.open(image_paths[i-1])\n        ax = fig.add_subplot(rows, columns, i)\n        plt.imshow(img)\n        df = train_df[train_df.image_id==image_names[i-1]]\n        bboxes = get_boxes(df)\n        [ax.add_patch(bbox) for bbox in bboxes]\n        plt.axis('off')\n\n    plt.show()","1ca115e0":"show_img_bbox()","12c62ca7":"sources = train_df.source.unique()\nfor source in sources:\n    print(f\"Images from: {source}\")\n    show_img_bbox(source=source)","e876f053":"!du ..\/input\/global-wheat-detection\/ -h","6dbedce2":"# Area\/Location of boxes","b1021f01":"621M of total data only <3\n\nBaseline model coming soon. ","30da34fd":"# DataSource","a9adbefd":"# Some thoughts:\n- bbox is used for bounding box \n- I am making dough here, these images shall be the wheat, pytorch dataloader shall be the grinder, convolutions shall be the water, Adam shall be the weather and public kernel tricks\/discussions shall be my magic ingredient.\n- I am so glad the data for this competition is under 1GB.","0ecb6f40":"## Number of bounding boxes in images provided by each source","cd17db3a":"# Making Dough\nLet's get our hands dirty and make some dough. \n\n\n<p align=\"center\">\n  <img width=\"460\" height=\"300\" src=\"https:\/\/cdn-a.william-reed.com\/var\/wrbm_gb_food_pharma\/storage\/images\/7\/0\/8\/1\/491807-1-eng-GB\/Freezing-dough-Understand-impact-on-glutenin-protein-say-researchers_wrbm_large.jpg\">\n<\/p>\n\n- Aim: Detect wheat heads (in form of multiple bounding boxes) of wheat plant images, all of size 1024x1024. \n- Data source: The [Global WHEAT dataset](http:\/\/www.global-wheat.com\/2020-challenge\/).\n- Metric: Mean average precision at different intersection over union (IoU) thresholds. Thresholds vary from 0.5 to 0.75 with a step size of 0.05.\n- Time: We have 3 months for now.","95ed244a":"Only 3422 train and 10 test images.","da6dec1e":"# Number of boxes per image","5d1b1e8a":"# Height and width distribution of bounding boxes","482518e0":"Wow, not only ethz_1 have given the maximum number of test images, but they also provide more number of bounding boxes per image. I expect their data to be of very high quality. Maybe I should fine tune my model only on their data.","6b5e9f81":"# Visualize images and their bounding boxes"}}