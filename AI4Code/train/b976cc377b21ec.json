{"cell_type":{"dd2b0c4a":"code","b63089ad":"code","6414ebf9":"code","dad7d5cb":"code","d7505588":"code","25f23ff0":"code","33b1062b":"code","5fa90d4d":"code","82c5dde6":"code","bfbd3b8e":"code","148848d8":"code","6e6174a5":"code","d198107d":"code","5fa88e1e":"markdown","dc663085":"markdown","fc21229f":"markdown","2429e3e9":"markdown","2aadfdbd":"markdown","b706750e":"markdown","7d622291":"markdown","653791bd":"markdown","1d53c524":"markdown","d3096ccf":"markdown","9f47be92":"markdown","cf5d42e7":"markdown"},"source":{"dd2b0c4a":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import recall_score\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom imblearn.over_sampling import SMOTE","b63089ad":"df = pd.read_csv (\"\/kaggle\/input\/credit-card-customers\/BankChurners.csv\")\ncolumns = df.columns.tolist()\ncols_to_use = columns[:len(columns) - 2]  # drop the last two columns\ndf = df[cols_to_use]\nprint(\"The data shape is : {} \".format(df.shape))","6414ebf9":"df.info()","dad7d5cb":"cat_columns = df.select_dtypes(include = ['object']).nunique(dropna=False)\nprint(cat_columns)","d7505588":"for col in cat_columns.index:\n    print(\"Feature: \", col)\n    print(\"   Vals: \", df[col].unique())","25f23ff0":"# Education level\n# Change 'College'=14 'Doctorate'=21 'Graduate'=16 'High School'=12 'Post-Graduate'=18 'Uneducated'=8 'Unknown'= Mode\ndf.loc[df['Education_Level'] == 'College',       'Education_Level'] = 14\ndf.loc[df['Education_Level'] == 'Doctorate',     'Education_Level'] = 21\ndf.loc[df['Education_Level'] == 'Graduate',      'Education_Level'] = 16\ndf.loc[df['Education_Level'] == 'High School',   'Education_Level'] = 12\ndf.loc[df['Education_Level'] == 'Post-Graduate', 'Education_Level'] = 18\ndf.loc[df['Education_Level'] == 'Uneducated',    'Education_Level'] = 8\ndf.loc[df['Education_Level'] == 'Unknown',       'Education_Level'] = df['Education_Level'].mode()","33b1062b":"# Income \ndf.loc[df['Income_Category'] == 'Less than $40K', 'Income_Category'] = 30\ndf.loc[df['Income_Category'] == '$40K - $60K',    'Income_Category'] = 50\ndf.loc[df['Income_Category'] == '$60K - $80K',    'Income_Category'] = 70\ndf.loc[df['Income_Category'] == '$80K - $120K',   'Income_Category'] = 100\ndf.loc[df['Income_Category'] == '$120K +',        'Income_Category'] = 200\ndf.loc[df['Income_Category'] == 'Unknown',        'Income_Category'] = 0\ndf.loc[df['Income_Category'] == 0, 'Income_Category'] = df['Income_Category'].mode()[0]","5fa90d4d":"# Card Category\ndf.loc[df['Card_Category'] == 'Blue', 'Card_Category'] = 1\ndf.loc[df['Card_Category'] == 'Silver', 'Card_Category'] = 2\ndf.loc[df['Card_Category'] == 'Gold', 'Card_Category'] = 3\ndf.loc[df['Card_Category'] == 'Platinum', 'Card_Category'] = 4","82c5dde6":"Y = df['Attrition_Flag'].to_numpy()\nY[Y=='Existing Customer'] = 1\nY[Y=='Attrited Customer'] = 2\nY = Y.astype('int')\n\nX = pd.get_dummies(df.drop(['CLIENTNUM', 'Attrition_Flag'], axis=1))\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1)","bfbd3b8e":"print(np.unique(Y, return_counts=True))","148848d8":"sm = SMOTE(random_state=42)\nx_res, y_res = sm.fit_resample(x_train, y_train)","6e6174a5":"print(np.unique(y_res, return_counts=True))","d198107d":"def ClassPrediction(classifier, mdl):\n  model = classifier.fit(x_res, y_res)\n  y_hat = model.predict(x_train)\n  acc = recall_score(y_train, y_hat, pos_label=2)\n  results.loc[mdl, 'Train'] = acc\n  y_hat = model.predict(x_test)\n  acc = recall_score(y_test, y_hat, pos_label=2)\n  results.loc[mdl, 'Test'] = acc\n\n# Storing Results\nresults = pd.DataFrame()\n\n# Models\nstage = 'Classification Model'\nClassPrediction(DecisionTreeClassifier(), 'Decision Tree')\nClassPrediction(RandomForestClassifier(), 'Random Forest')\nClassPrediction(GradientBoostingClassifier(), 'Gradient Boost')\nClassPrediction(AdaBoostClassifier(), 'AdaBoost')\nprint(results)","5fa88e1e":"Check that the data is now more balanced:","dc663085":"Let us read the data.\n\nNote: I'll delete the last two columns as the dataset description states.","fc21229f":"Let's take a look over the categorical columns:","2429e3e9":"## Data Preprocessing","2aadfdbd":"Let's handle categorical cols that can be interpreted as ordinal features:","b706750e":"Let's divide the data to features and label and split it to train-test accordingly:","7d622291":"Imports:","653791bd":"# Bank Churners\n\nIntro: In this notebook I'll use some ordinary ML algos in order to predict predict bank churners","1d53c524":"Conclusion: Gradient Boost had achieved the best Recall Score of ~88.9% with respect to the test set with no visible overfitting.","d3096ccf":"## Modeling\nI'll use serveral common classification ML algos. I'll use the Recall score as my main performance measurement since it's much more meaningful for this specific task of churn prediction.","9f47be92":"The label is imbalanced. Let's use SMOTE algo in order to balance the data:","cf5d42e7":"Let's check target balance:"}}