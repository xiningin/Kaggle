{"cell_type":{"8d64d357":"code","cf9711b4":"code","38204729":"code","9754835e":"code","c3f0988a":"markdown"},"source":{"8d64d357":"from sklearn import datasets\nimport numpy as np\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt","cf9711b4":"%matplotlib inline","38204729":"iris = datasets.load_iris()\nx = iris.data\n\ny = iris.target\nindex = np.where(y==1)\nindex = np.append(index, np.where(y==0))\nx = x[index]\ny = y[index].reshape(-1,1)\nprint(x.shape)\nprint(y.shape)\n\ntf.reset_default_graph()\nX = tf.placeholder(tf.float32, [None, x.shape[-1]], name='X')\nY = tf.placeholder(tf.float32, [None, 1], name='Y')\nW = tf.Variable(tf.zeros([x.shape[-1],1]), name='Weights')\nb = tf.Variable(tf.zeros([1]), name='bias')\nlogits = tf.matmul(X,W) + b\n\ncross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=Y))\ninit = tf.global_variables_initializer()\n\noptimizer = tf.train.GradientDescentOptimizer(0.06).minimize(cross_entropy)\npred = tf.round(tf.sigmoid(logits))\naccu = tf.reduce_mean(tf.cast(tf.equal(pred, Y), dtype=tf.float32))\nloss = []\naccuracy = []\nwith tf.Session() as sess:\n    sess.run(init)\n    file_writer = tf.summary.FileWriter(\"iris\/train\", sess.graph)\n    for i in range(1000):\n        sess.run(optimizer, feed_dict={X: x, Y:y})\n        loss.append(sess.run(cross_entropy, feed_dict={X:x, Y:y}))\n        accuracy.append(sess.run(accu, feed_dict={X:x, Y:y}))\n        if i%100==0:\n            print(sess.run(cross_entropy, feed_dict={X:x, Y:y}))\n            #print(sess.run(W))\n    print(sess.run(accu, feed_dict={X:x, Y:y}))       \nplt.plot(accuracy)\nplt.plot(loss)","9754835e":"from IPython.core.display import display, HTML\n\nhtml_string = \"\"\"\n<blockquote class=\"imgur-embed-pub\" lang=\"en\" data-id=\"UEsX76x\"><a href=\"\/\/imgur.com\/UEsX76x\"><\/a><\/blockquote><script async src=\"\/\/s.imgur.com\/min\/embed.js\" charset=\"utf-8\"><\/script>\n\"\"\"\nh = display(HTML(html_string))","c3f0988a":"Notice there is a very interesting behavior from the figure above,  the blue line is accuracy while the brownish yellow is the loss, the accuracy already reached 1 even when the loss is still decreasing. Logstic regression decision boundaries is not fixed as in SVM (linearly separable dataset).  Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. In fact, cross-entropy loss can be used in regression if the regression output is in between 0 and 1."}}