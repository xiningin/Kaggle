{"cell_type":{"db9298dc":"code","6fd5f7f6":"code","8c207bbe":"code","2de75da2":"code","27936069":"code","83762af2":"code","9fd74f58":"code","c66bb6ea":"code","6dfb53f4":"code","5c031bb6":"code","11b23812":"code","ce61248f":"code","58f695cd":"code","88b77919":"code","19e5f48b":"code","faca94b4":"code","ed23da7a":"code","42a0aa69":"code","62417947":"code","d01f305a":"code","6379984a":"code","6f833310":"code","fdd9127c":"code","3953c53f":"code","5e2d0654":"code","cabedf5e":"code","81ca2d33":"code","911ed949":"code","1f6b3911":"code","cd0f6752":"code","f3070066":"code","2c032e1f":"code","77e85a15":"code","3b9a9d62":"code","cb8ea621":"code","edc9c92d":"code","4fb82858":"code","66c63090":"code","7057c3e2":"code","e2d628cd":"code","e49bbf7d":"markdown","9a8d5063":"markdown","88e86e24":"markdown","061103b7":"markdown","eade14ac":"markdown","385ef17d":"markdown","f01a06d2":"markdown","60dc76e4":"markdown","392d45b5":"markdown","ffdc5547":"markdown","46337fff":"markdown","b45d7b08":"markdown","d505f2bd":"markdown","9885e529":"markdown","b5822bb8":"markdown","8ad0fb81":"markdown","71440852":"markdown","e2421462":"markdown","e1fae1f6":"markdown","56dbb18d":"markdown","ea94697a":"markdown","fa9c9816":"markdown","30198f3a":"markdown","a938d1cc":"markdown","aaa13d24":"markdown","873c4d42":"markdown","c1fc1566":"markdown","7e117d29":"markdown"},"source":{"db9298dc":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6fd5f7f6":"from datetime import timedelta\nimport matplotlib.dates as mdates\nimport datetime as dt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans","8c207bbe":"data = pd.read_csv('..\/input\/retail-store-sales-transactions\/scanner_data.csv')\ndata","2de75da2":"data.info()","27936069":"def get_day(x): \n    return dt.datetime(x.year, x.month, x.day)\n\ndef get_month(x):\n    return dt.datetime(x.year, x.month, 1)\n\ndata['Date']= pd.to_datetime(data['Date'])\ndata['invoice_date'] = data['Date'].apply(get_day)\ngroup_invoice = data.groupby('Customer_ID')['invoice_date']\ndata['first_date'] = group_invoice.transform('min')\ndata['last_date'] = group_invoice.transform('max')\ndata['invoice_month'] = data['Date'].apply(get_month)\ngroup_month = data.groupby('Customer_ID')['invoice_date']\ndata['first_month'] = group_month.transform('min').apply(get_month)\ndata.head()","83762af2":"def get_ymd (df, column):\n    year=df[column].dt.year\n    month=df[column].dt.month\n    day=df[column].dt.day\n    return year, month, day\n\ninvoice_year, invoice_month, _ = get_ymd(data, 'invoice_date')\nfirst_year, first_month, _ = get_ymd(data, 'first_date')\n\nyears_diff = invoice_year - first_year\nmonths_diff = invoice_month - first_month\n\ndata['duration_month'] = years_diff * 12 + months_diff * 1 + 1\ndata.head()","9fd74f58":"group_cohort = data.groupby(['first_month','duration_month'])\ncohort_data = group_cohort['Customer_ID'].apply(pd.Series.nunique).reset_index()\ncohort_data","c66bb6ea":"cohort_counts = cohort_data.pivot(index='first_month', columns='duration_month', values='Customer_ID')\ncohort_sizes = cohort_counts.iloc[:,0]\nretention = cohort_counts.divide(cohort_sizes, axis=0)\nretention.round(4)*100","6dfb53f4":"pd.to_datetime(retention.index)\nplt.subplots(figsize=(16, 8))\nsns.set()\nax = sns.heatmap(data=retention, annot=True, fmt='.2%', vmin=0.0, vmax=0.4, cmap='BuGn')\nax.set_yticklabels(retention.iloc[:].index.strftime('%b-%d-%Y'))\nplt.yticks()\nplt.title('Retention Rate')\nplt.show()","5c031bb6":"group_sales = data.groupby(['first_month','duration_month']) \ncohort_sales = group_sales['Sales_Amount'].mean()\ncohort_sales = cohort_sales.reset_index()\naverage_sales = cohort_sales.pivot(index='first_month', columns='duration_month', values='Sales_Amount')\naverage_sales.round(2)","11b23812":"pd.to_datetime(retention.index)\nplt.subplots(figsize=(16, 8))\nsns.set()\nax = sns.heatmap(data=average_sales, annot=True, fmt='.2f',cmap='Blues')\nax.set_yticklabels(retention.iloc[:].index.strftime('%b-%d-%Y'))\nplt.yticks()\nplt.title('Average Sales')\nplt.show()","ce61248f":"data","58f695cd":"print(data['invoice_date'].min())\nprint(data['invoice_date'].max())","88b77919":"def get_ymd (df, column):\n    year=df[column].dt.year\n    month=df[column].dt.month\n    day=df[column].dt.day\n    return year, month, day\n\nlast_year, last_month, last_day = get_ymd(data, 'last_date')\n\nyears_diff_last = 2016 - last_year\nmonths_diff_last = 12 - last_month\ndays_diff_last = 31 - last_day\n\ndata['recency'] = years_diff_last * 365 + months_diff_last * 30 + days_diff_last + 1\ndata.head()","19e5f48b":"data_recency=data.groupby('Customer_ID')['recency'].apply(min).reset_index()\ndata_frequency=data.groupby('Customer_ID').size().reset_index(name='count')\ndata_monetary=data.groupby('Customer_ID')['Sales_Amount'].apply(sum).reset_index()\n\ndata_rf = pd.merge(left=data_recency, right=data_frequency, how='inner', left_on='Customer_ID', right_on='Customer_ID')\ndata_rfm = pd.merge(left=data_rf, right=data_monetary, how='inner', left_on='Customer_ID', right_on='Customer_ID')\ndata_rfm.rename(columns={'count': 'frequency', 'Sales_Amount': 'monetary'}, inplace=True)\ndata_rfm = data_rfm.set_index('Customer_ID')\ndata_rfm","faca94b4":"print('Top 20% shopping frequency quantile:', data_rfm['frequency'].quantile(q = 0.8))\n\ndata_rfm['frequency'].quantile(q = 0.8)\ndata_20qf = data_rfm[data_rfm['frequency'] >= data_rfm['frequency'].quantile(q = 0.8)]\ndata_20qf_14d=data_20qf[data_20qf['recency']<=14]\ndata_20qf_14d","ed23da7a":"fig=go.Figure()\nfig.add_trace(\n    go.Scatter(x=data_rfm['frequency'], y=data_rfm['monetary'], name='All customers', mode='markers', opacity=0.5)\n)\nfig.add_trace(\n    go.Scatter(x=data_20qf_14d['frequency'], y=data_20qf_14d['monetary'], name='Top 20% frequency purchasing in 14 days', mode='markers')\n)\nfig.update_layout({  \n      'showlegend':True, 'legend':{'x':0.02, 'y':0.96, 'bgcolor':'rgb(246, 228, 129)'}\n      })\nfig.update_xaxes(\n        title_text = \"Purchase Frequency\",\n        title_font = {\"size\": 16},\n        title_standoff = 12)\nfig.update_yaxes(\n        title_text = \"Monetary\",\n        title_font = {\"size\": 16},\n        title_standoff = 12)\nfig.show()","42a0aa69":"print('Top 5% recency quantile:', data_rfm['recency'].quantile(q = 0.05))\nprint('Top 5% shopping frequency quantile:', data_rfm['frequency'].quantile(q = 0.95))\nprint('Top 5% purchase amount quantile:', data_rfm['monetary'].quantile(q = 0.95))\n\nsel_data_5qr = data_rfm['recency'] <= data_rfm['recency'].quantile(q = 0.05)\nsel_data_5qf = data_rfm['frequency'] >= data_rfm['frequency'].quantile(q = 0.95)\nsel_data_5qm = data_rfm['monetary'] >= data_rfm['monetary'].quantile(q = 0.95)\ndata_5rfm = data_rfm[sel_data_5qr & sel_data_5qf & sel_data_5qm]\ndata_5rfm","62417947":"fig=go.Figure()\nfig.add_trace(\n    go.Scatter(x=data_rfm['frequency'], y=data_rfm['monetary'], name='All customers', mode='markers', opacity=0.5)\n)\nfig.add_trace(\n    go.Scatter(x=data_5rfm['frequency'], y=data_5rfm['monetary'], name='Top 5% quantile in recency, frequency and spending', mode='markers')\n)\nfig.update_layout({  \n      'showlegend':True, 'legend':{'x':0.02, 'y':0.96, 'bgcolor':'rgb(246, 228, 129)'}\n      })\nfig.update_xaxes(\n        title_text = \"Purchase Frequency\",\n        title_font = {\"size\": 16},\n        title_standoff = 12)\nfig.update_yaxes(\n        title_text = \"Monetary\",\n        title_font = {\"size\": 16},\n        title_standoff = 12)\nfig.show()","d01f305a":"print('Top 1% recency quantile:', data_rfm['recency'].quantile(q = 0.01))\nprint('Top 1% shopping frequency quantile:', data_rfm['frequency'].quantile(q = 0.99))\nprint('Top 1% purchase amount quantile:', data_rfm['monetary'].quantile(q = 0.99))\n\nsel_data_1qr = data_rfm['recency'] <= data_rfm['recency'].quantile(q = 0.01)\nsel_data_1qf = data_rfm['frequency'] >= data_rfm['frequency'].quantile(q = 0.99)\nsel_data_1qm = data_rfm['monetary'] >= data_rfm['monetary'].quantile(q = 0.99)\ndata_sel_rfm = data_rfm[sel_data_1qr & sel_data_1qf & sel_data_1qm]\ndata_sel_rfm","6379984a":"data_top10 = data_sel_rfm.sort_values(by='monetary', ascending=False).head(10)\ndata_top10","6f833310":"fig=go.Figure()\nfig.add_trace(\n    go.Scatter(x=data_rfm['frequency'], y=data_rfm['monetary'], name='All customers', mode='markers', opacity=0.3)\n)\nfig.add_trace(\n    go.Scatter(x=data_top10['frequency'], y=data_top10['monetary'], name='Top 10 Customers', mode='markers')\n)\n\nannotaion_01={'x':'218', 'y':'3844.97', 'showarrow':True, 'arrowhead':4, 'xshift':-2,'yshift':8,'text':'ID-17450', 'textangle':-90, 'font':{'size':10, 'color':'green'}}\nannotaion_02={'x':'143', 'y':'2057.69', 'showarrow':True, 'arrowhead':4, 'xshift':-2,'yshift':8,'text':'ID-16029', 'textangle':-90,'font':{'size':10, 'color':'green'}}\nannotaion_03={'x':'66', 'y':'1690.49', 'showarrow':True, 'arrowhead':4, 'xshift':-2,'yshift':8,'text':'ID-13694', 'textangle':-90,'font':{'size':10, 'color':'green'}}\nannotaion_04={'x':'114', 'y':'1563.10', 'showarrow':True, 'arrowhead':4, 'xshift':-2,'yshift':8,'text':'ID-13089', 'textangle':-90,'font':{'size':10, 'color':'green'}}\nannotaion_05={'x':'72', 'y':'1515.32', 'showarrow':True, 'arrowhead':4, 'xshift':-2,'yshift':8,'text':'ID-17949', 'textangle':-90,'font':{'size':10, 'color':'green'}}\nannotaion_06={'x':'71', 'y':'1485.44', 'showarrow':False, 'arrowhead':4, 'xshift':0,'yshift':-30,'text':'ID-15061', 'textangle':-90,'font':{'size':10, 'color':'green'}}\nannotaion_07={'x':'228', 'y':'1429.91', 'showarrow':False, 'arrowhead':4, 'xshift':0,'yshift':-32,'text':'ID-14298', 'textangle':-90,'font':{'size':10, 'color':'green'}}\nannotaion_08={'x':'52', 'y':'1427.39', 'showarrow':True, 'arrowhead':4, 'xshift':-2,'yshift':8,'text':'ID-17841', 'textangle':-90,'font':{'size':10, 'color':'green'}}\nannotaion_09={'x':'83', 'y':'1063.42', 'showarrow':True, 'arrowhead':4, 'xshift':-2,'yshift':8,'text':'ID-13798', 'textangle':-90,'font':{'size':10, 'color':'green'}}\nannotaion_10={'x':'112', 'y':'1028.61', 'showarrow':False, 'arrowhead':4, 'xshift':0,'yshift':-32,'text':'ID-16422', 'textangle':-90,'font':{'size':10, 'color':'green'}}\n\nfig.update_layout({ \n    'annotations':[annotaion_01, annotaion_02, annotaion_03,\n      annotaion_04, annotaion_05, annotaion_06,\n      annotaion_07, annotaion_08, annotaion_09, annotaion_10], \n      'showlegend':True, 'legend':{'x':0.02, 'y':0.96, 'bgcolor':'rgb(246, 228, 129)'}\n      })\nfig.update_xaxes(\n        title_text = \"Purchase Frequency\",\n        title_font = {\"size\": 16},\n        title_standoff = 12)\nfig.update_yaxes(\n        title_text = \"Monetary\",\n        title_font = {\"size\": 16},\n        title_standoff = 12)\nfig.show()","fdd9127c":"range_labels = list(range(3, 0, -1))\ndata_q=data_rfm.copy(deep=True)\ndata_q['recency_quartile']=pd.qcut(data_q['recency'].rank(method='first'), q=3, labels=range_labels)\ndata_q['frequency_quartile']=pd.qcut(data_q['frequency'].rank(method='first'), q=3, labels=range(1,4))\ndata_q['monetary_quartile']=pd.qcut(data_q['monetary'].rank(method='first'), q=3, labels=range(1,4))\ndata_q.head()","3953c53f":"data_cat = (data_q['recency_quartile'].astype(str)).str.cat(data_q['frequency_quartile'].astype(str))\ndata_q['rfm_segment']=data_cat.str.cat(data_q['monetary_quartile'].astype(str))\ndata_q.head()","5e2d0654":"data_q['rfm_score'] = data_q[['recency_quartile','frequency_quartile','monetary_quartile']].sum(axis=1)\ndata_q.head()","cabedf5e":"def rfm_level(df):\n    if df['rfm_score'] >= 7:\n        return 'Top'\n    elif ((df['rfm_score'] >= 4) and (df['rfm_score'] < 7)):\n        return 'Middle'\n    else:\n        return 'Low'\ndata_q['rfm_level'] = data_q.apply(rfm_level, axis=1)\ndata_q.head()","81ca2d33":"rfm_level_agg = data_q.groupby('rfm_level').agg({'recency': 'mean','frequency': 'mean', 'monetary': ['mean', 'count']\n}).round(2)\nrfm_level_agg.head()","911ed949":"scaler=StandardScaler()\nscaler.fit(data_rfm)\ndata_normalized=scaler.transform(data_rfm)\ndata_normalized=pd.DataFrame(data_normalized, index=data_rfm.index, columns=data_rfm.columns)\nprint(data_normalized.describe().round(2))","1f6b3911":"plt.subplot(3, 1, 1); sns.kdeplot(data_rfm['recency'])\nplt.subplot(3, 1, 2); sns.kdeplot(data_rfm['frequency'])\nplt.subplot(3, 1, 3); sns.kdeplot(data_rfm['monetary'])\nplt.show()","cd0f6752":"data_log=np.log(data_rfm)\nscaler=StandardScaler()\ndata_log.replace([np.inf, -np.inf], np.nan, inplace=True)\nscaler.fit(data_log)\ndata_normalized=scaler.transform(data_log)\ndata_normalized = pd.DataFrame(data=data_normalized, index=data_log.index, columns=data_log.columns)\ndata_normalized.head()","f3070066":"data_normalized=data_normalized.fillna(0)\nkmeans = KMeans(n_clusters=3, random_state=1)\nkmeans.fit(data_normalized)\ncluster_labels = kmeans.labels_\n\ndata_rfm_k3 = data_rfm.assign(Cluster=cluster_labels)\ngrouped = data_rfm_k3.groupby(['Cluster'])\n\ngrouped.agg({\n    'recency': 'mean',\n    'frequency': 'mean',\n    'monetary': ['mean', 'count']\n  }).round(1)","2c032e1f":"plt.subplot(3, 1, 1); sns.kdeplot(data_log['recency'])\nplt.subplot(3, 1, 2); sns.kdeplot(data_log['frequency'])\nplt.subplot(3, 1, 3); sns.kdeplot(data_log['monetary'])\nplt.show()","77e85a15":"sse={}\nfor k in range(1, 21):\n    kmeans = KMeans(n_clusters=k, random_state=1)\n    kmeans.fit(data_normalized)\n    sse[k] = kmeans.inertia_\n\nsns.pointplot(x=list(sse.keys()), y=list(sse.values()))\nplt.title('The Elbow Method')\nplt.xlabel('k')\nplt.ylabel('SSE')\nplt.show()","3b9a9d62":"data_normalized['cluster']=data_rfm_k3['Cluster']\ndata_normalized.head()","cb8ea621":"data_melt = pd.melt(\n    data_normalized.reset_index(), \n    id_vars=['Customer_ID', 'cluster'],\n    value_vars=['recency', 'frequency', 'monetary'], \n    var_name='metric', value_name='value'\n)\ndata_melt.head()","edc9c92d":"sns.lineplot(data=data_melt, x='metric', y='value', hue='cluster')\nplt.title('Line plot of normalized variables')\nplt.xlabel('Metric')\nplt.ylabel('Value')\nplt.show()","4fb82858":"cluster_avg = data_rfm_k3.groupby(['Cluster']).mean() \npopulation_avg = data_rfm.mean()\nrelative_imp = cluster_avg \/ population_avg - 1\nprint(relative_imp.round(2))","66c63090":"plt.figure(figsize=(16, 6))\nsns.heatmap(data=relative_imp, annot=True, fmt='.2f', cmap='RdYlGn')\nplt.title('Relative importance of attributes')\nplt.show()","7057c3e2":"cluster2=data_rfm_k3[data_rfm_k3['Cluster']==2]\ncluster2","e2d628cd":"cluster2_list=[]\ncustomer=[cluster2_list.append(x) for x in cluster2.reset_index()['Customer_ID'] if x not in cluster2_list]\nprint(len(cluster2_list))\nprint(cluster2_list)","e49bbf7d":"- Running k-mean clustering on the normalized data. From the visulization, the elbow with a sharpest angel is the optimal number for clustering. Here shows 2 or 3 clusters is the optimal number.","9a8d5063":"- Identify the importance of each segment's attribute by calculating ratio beteen the average values of the clusters and the average of the population.","88e86e24":"- Plot the distribution of the 3 subjects. As the graphics show, these numbers are highly skewed.","061103b7":"- Here is to subset the data by the top 20% frequency quantile and the last activity in 2 weeks. The total number of customers in this group is 1,108.","eade14ac":"- Get the log scale in order to get a better distribution.","385ef17d":"- Point out the top 10 cutomers in a scatter plot","f01a06d2":"- Generate a new dataframe for recency, frequency, and monetary.","60dc76e4":"- Get the number of users in terms of how many month they have been with valid activities. Then, generate the matrix by using a pivot table.","392d45b5":"- K-Means clustering analysis starts from here. First, splits the customers into 3 levels to quickly review the numbers of these 3 levels.","ffdc5547":"- Plot a heat map to easily distinguash the most important cluster, cluster 2.","46337fff":"- Here is to subset the data by the top 5% quantile in recency, frequency, and monetary. The total number of these customers is 224.","b45d7b08":"- Add a 'Cluster' columns into the normalized datafram, and then melt the datafram into a long datafram by vertically stacking 3 subjects, recency, frequency, and monetry, one by one.","d505f2bd":"- Sum up the scores to rank the customers into 3 levels, the Top, Middle, and Low levels.","9885e529":"## 7. Conclusion\n\nIn sum, here are viewpoints as below.\n\n1. The customers who started purchase in the early 2016 have a higher retension rate. The customers who started purchase in Feb with 6 months duration and who started purchase in Aug with 2 months duration have higher average expenditure.\n\n2. 1,108 users are in the top 20% frequency quantile with an activity in the past two weeks. 224 users are in the top 5% quantile in recency, frequency and monetary. The IDs of the top 10 most valuable users are shown in the table above.\n\n3. There is a significant difference between the results of the K-Means clustering and the linear quantile clustering. The K-Means clustering brings 4,262 customers together as a group, which might be too huge to target some customers for a specific marketing campaign.\n\nSuggested further analysis.\n\n1. Use other machine learning methods for more deeper analysis and prediction.\n\n2. Optimize this KMeans model.\n\n3. Other visual types to explore more insights.","b5822bb8":"- Subset the data by the top 1% quantile in recency, frequency, and monetary to get 17 customers, and then sort the value by monetary to pick the top 10 valuable customers.","8ad0fb81":"- Figure out the average sales amount in terms of the customers splits into different duration months. Also, plot a heatmap to easily understand the situation.","71440852":"- Plot a heatmap to visualize the retention rates in terms of the dutation and the date that the account is created.","e2421462":"- Plot a line chart to compare different clusters. Obviously, the cluster 2 has higher frequncy and monetary with short recency, which is much better than the others.","e1fae1f6":"- Visualize the data to quickly view the distribution of these 1,108 customers.","56dbb18d":"## 5. Process\n- Determine whether there is an null value in the dataset.","ea94697a":"- Make sure the last date of the last transaction. The last date is Dec 31st, 2016. Thus, here is to set Jan 1st, 2017, as the date of this analysis is processing in order to calculate the recency. (The lower recency, the better)","fa9c9816":"- Plot the log numbers to see the distribution. Although the log recency is still slightly skewed, frequency and monetary look better than before.","30198f3a":"## 6. Analyze\n- Calculate the duration month","a938d1cc":"- Identify the customers in the cluster 2 and list out the customer IDs.","aaa13d24":"- Because K-Means works well on variables with the same mean and standard diviation, here is normalizing the data and reviwing the key statistics to verify whehter the data is good to use.","873c4d42":"## 1. Introduction\nThe given dataset contains sales records with 64.682 transactions and 22.625 customers IDs in 2016. The columns are Transaction, Customer ID, Transaction ID, Category, SKU, Quantity, and Sales Amount.\n\n## 2. Objectives\nThe questions for this analysis are as follows.\n1. What is the retention rate of the users in terms of the date since these users sign up.\n2. Who are the top 5% frequency quantile with valid activity in 14 days? Who are the top 0.5% quantile in recency, frequency, and influence? Who are the top 10 most valuable users?\n3. Who are the most valuable customers by using K-Means clustering?\n4. The outcome difference between the k-means clustering and the linear quantile method.\n\n## 3. Method\n- Python\nThe structure of this analysis would be split into two major chapters. The first section is to cluster the customers by using a linear quantile method. In this way, it will be clear who purchase on certain days and how much they spend recently.\n\nThe second section is a k-mean clustering analysis. This method splits the customers into 3 different clusters in different criteria, recency, frequency, and monetary. In this way, there will be a group of selected people marked as the most important customers for this business.\n\nLastly, this analysis campares the outcomes from the linear quantile method and the k-mean clustering method. There is a difference between these two methods to achieve customer segmentation.\n\n## 4. Prepare\n- First of all, First of all, import functions that will be used and take a look at the dataset.","c1fc1566":"- Here is to show the mean numbers of 3 criteria for the following comparison.","7e117d29":"- Again, visualize the data to see where these 752 customers are."}}