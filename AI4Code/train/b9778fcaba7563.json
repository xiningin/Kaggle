{"cell_type":{"97703b09":"code","ac4286e9":"code","ea3fb2b5":"code","ed41cab3":"code","a14fe6d7":"code","69ff6790":"code","c0fe7a62":"code","49ee08e7":"code","bbb8ded0":"code","6a55f124":"code","6c628e00":"code","eaf3a2b2":"code","f0ab568b":"code","aae0e849":"code","9c79fe58":"code","5a945de1":"code","74f0c188":"code","472a37e8":"code","dcb178f3":"code","8c9f9788":"code","f37d2ad7":"code","0c50f9a4":"code","cf9ce76d":"code","627d2132":"code","b6e82d67":"code","f4b3f942":"code","81cb298d":"code","35ecf0bb":"code","c8cbf5bf":"code","afe88551":"code","33dd80a0":"code","b1896ea1":"code","ef012e12":"code","3669bf17":"code","ca3704fe":"code","4c2e1df9":"code","e236d085":"code","34299fc9":"code","925902ce":"code","9d39b935":"code","e515f066":"code","e001bb7d":"code","ef63eadf":"code","32b84203":"code","09d268c9":"code","7fbca317":"code","c14923b2":"code","a44435bb":"code","2ded87ae":"code","db0934e3":"code","94f1d22b":"code","5eb4da7a":"code","c20fc6ad":"code","495866a4":"code","5df3f4fc":"code","103b59b0":"code","4a4ae348":"code","691d7330":"code","bf3948f5":"code","5b165a43":"code","c7935789":"markdown","fcbea416":"markdown","7e5f0750":"markdown","9472709c":"markdown","3ac2d091":"markdown","7c5eb93c":"markdown","ee2a8ab9":"markdown","ecf1dc5a":"markdown","fba42a5b":"markdown","39226a84":"markdown","0c5d9fc2":"markdown","9f41fb40":"markdown","60f16fb3":"markdown","89d5f9a2":"markdown","d01505fb":"markdown","99370eb2":"markdown","5df601f7":"markdown","9ca25fcd":"markdown","da44e6f9":"markdown","41f5558a":"markdown","413e42d8":"markdown","39fd6239":"markdown","9a24d206":"markdown","f8fe9851":"markdown","3a237c66":"markdown","127137bc":"markdown","b30e6a33":"markdown","8dc299b6":"markdown","13c4e068":"markdown","34236734":"markdown","3801d2c8":"markdown","50d1ebe9":"markdown","86f9fd03":"markdown","2e0ed6f6":"markdown","de5e03b5":"markdown","08b158d7":"markdown","7519694f":"markdown","721c0eb1":"markdown","bfd5d6fe":"markdown","ae2bcf4a":"markdown","6295415e":"markdown","0c508c91":"markdown","a688a254":"markdown","a8af10d6":"markdown","87952f41":"markdown","ae5e3f16":"markdown","641e4aa5":"markdown","6875f417":"markdown","4e9f2408":"markdown","fa7ef673":"markdown","291c49e4":"markdown","d193aa56":"markdown","6890b4a9":"markdown","79ab0e93":"markdown","3ceea802":"markdown","c4a553ec":"markdown","3f0872c5":"markdown","3da1ebce":"markdown","f3cc9f0b":"markdown","a8b98e51":"markdown","640c2654":"markdown","8efe417a":"markdown","3f89ccf3":"markdown","c0d8073a":"markdown"},"source":{"97703b09":"# for processing section\n\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nimport string\nimport warnings\n\n# for training section\nfrom transformers import BertTokenizer, BertModel\nfrom torch.nn import functional as F\nfrom sklearn.metrics import f1_score, accuracy_score\nimport time, os\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch\nimport numpy as pp\nfrom torch.utils import data\nfrom nltk.probability import FreqDist\nimport pandas as pd\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.width', 250)\npd.set_option('display.max_columns', 10)\npd.set_option('display.max_colwidth', 20)\nplt.figure(figsize=(55, 55))\nsb.set()\n\ntorch.manual_seed(42)\npp.random.seed(42)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","ac4286e9":"df = pd.DataFrame(pd.read_json('..\/input\/news-category-dataset\/News_Category_Dataset_v2.json', lines=True)).sample(frac=1.) # .sample(frac=1.) to shuffle\n\nprint('total data: ', len(df))\nprint(df.head())","ea3fb2b5":"df = df[['category', 'headline', 'short_description']]\nprint('data shape', df.shape)\ndf.drop_duplicates(inplace=True)\n\ndf['text'] = df['headline'] + ' ' + df['short_description']\ndf.drop(['headline', 'short_description'], axis=1, inplace=True)\n\nprint(df.head())\nprint('data shape', df.shape)","ed41cab3":"print(df['category'].value_counts())","a14fe6d7":"print(f\"total categories: {len(df['category'].unique())}\")\n\ncategories = {'TASTE': 'FOOD & DRINK', 'FOOD & DRINK': 'FOOD & DRINK', # FOOD & DRINK 2 classes\n              'EDUCATION': 'EDUCATION', 'COLLEGE': 'EDUCATION', # EDUCATION 2 classes\n              'PARENTING': 'PARENTING', 'PARENTS': 'PARENTING', # PARENTING  2 classes\n              'HEALTH': 'HEALTH', 'HEALTHY LIVING': 'HEALTH', 'WELLNESS': 'HEALTH', # HEALTH 3 classes\n              'WORLDPOST': 'WORLDPOST', 'THE WORLDPOST': 'WORLDPOST', # WORLDPOST 2 classes\n              'CULTURE & ARTS': 'CULTURE & ARTS', 'ARTS': 'CULTURE & ARTS', 'ARTS & CULTURE': 'CULTURE & ARTS', # 'CULTURE & ARTS 3 classes\n              'WEDDINGS': 'DIVORCE & WEDDINGS', 'DIVORCE': 'DIVORCE & WEDDINGS', # DIVORCE & WEDDINGS 2 classes\n              'BUSINESS': 'BUSINESS & ECONOMY', 'MONEY': 'BUSINESS & ECONOMY', # BUSINESS & ECONOMY 2 classes\n              'WEIRD NEWS': 'WEIRD NEWS', 'QUEER VOICES': 'WEIRD NEWS', # WEIRD NEWS 2 classes\n              'ENTERTAINMENT': 'ENTERTAINMENT', 'COMEDY': 'ENTERTAINMENT', # ENTERTAINMENT 2 classes\n              'POLITICS': 'POLITICS', 'WORLD NEWS': 'POLITICS', 'LATINO VOICES': 'POLITICS', # POLITICS 3 classes\n              'GOOD NEWS': 'CASUAL', 'FIFTY': 'CASUAL', 'HOME & LIVING': 'CASUAL', # CASUAL 3 classes\n              'ENVIRONMENT': 'ENVIRONMENT', 'GREEN': 'ENVIRONMENT', # ENVIRONMENT 2 classes\n              'STYLE & BEAUTY': 'STYLE & BEAUTY', 'STYLE': 'STYLE & BEAUTY'} # STYLE & BEAUTY 2 classes\n\ndf['category'] = df['category'].replace(categories)\nprint(f\"total categories after clustering: {len(df['category'].unique())}\")\n\nprint(df['category'].value_counts())","69ff6790":"categories_data = []\ntext_data = []\nbefore = len(df)\nprint('data shape ', df.shape)\nfor i, value in enumerate(df['category']):\n    if value in ['POLITICS', 'HEALTH', 'ENTERTAINMENT', 'PARENTING','STYLE & BEAUTY','TRAVEL',\n                 'WEIRD NEWS', 'FOOD & DRINK', 'BUSINESS & ECONOMY', 'DIVORCE & WEDDINGS']:\n        \n        categories_data.append(value)\n        text_data.append(df['text'].values[i])\n\ndf = pd.DataFrame({'text': text_data, 'category': categories_data})\nafter = len(df)\n\nprint('data shape after using top-10 classes', df.shape)\nprint('total removed data : ', before - after)","c0fe7a62":"df['n_upper'] = df['text'].apply(lambda x: sum([1 for w in x.split() if w.isupper()])) # WORD\ndf['n_lower'] = df['text'].apply(lambda x: sum([1 for w in x.split() if w.islower()])) # word\ndf['n_digit'] = df['text'].apply(lambda x: sum([1 for w in x.split() if w.isdigit()])) # 3346\ndf['n_title'] = df['text'].apply(lambda x: sum([1 for w in x.split() if w.istitle()])) # Word\ndf['n_special_ch'] = df['text'].apply(lambda x: sum([1 for w in x if w in string.punctuation])) # %@&()*-\ndf['n_words'] = df['text'].apply(lambda x: sum([1 for _ in x.split()])) # total words count\ndf['n_uniques'] = df['text'].apply(lambda x: len(set([w for w in x.split()]))) # total vocabs","49ee08e7":"def text_cleaner(text):\n    import re, nltk\n    from nltk.corpus import stopwords\n    from nltk.stem import wordnet\n    tweet = text.lower()\n\n    tweet = re.sub(\"\\?{1,}\", \" ? \", tweet)\n    tweet = re.sub(\"\\!{1,}\", \" ! \", tweet)\n    tweet = re.sub(\"\\-{1,}\", \" - \", tweet)\n    \n    tweet = re.sub(r\"janu[ae]ry|febru[ae]ry|march|april|may|june|july|august|september|october|november|december\", \" month \", tweet)\n    tweet = re.sub(r\"http\\S+\", \" url \", tweet)\n    tweet = re.sub(r\"\\S+\\@\\S+\", \" email \", tweet)\n    tweet = re.sub(r\" \\@\\S+\", \" username \", tweet)\n    tweet = re.sub(\"\\s{2,}\", \" \", tweet)\n    tweet = re.sub(r\"yr\", \" year \", tweet)\n\n    tweet = re.sub(r'[^a-z!?\\-]', ' ', tweet)\n    tweet = re.sub(r\"\\d*\/\\d*\/?\\d*?\", \" #\/#\/# \", tweet)\n    tweet = re.sub(\"\\d+:\\d+\", ' ##:##', tweet)\n    tweet = re.sub(r\"\\d{1}\", \"#\", tweet)\n    tweet = re.sub(r\"\\d{1,}\\.\\d{1,}\", \"#\", tweet)\n    tweet = re.sub(r\"\\d{2}\", \"##\", tweet)\n    tweet = re.sub(r\"\\d{3}\", \"###\", tweet)\n    tweet = re.sub(r\"\\d{4}\", \"####\", tweet)\n    tweet = re.sub(r\"\\d{5,}\", \"#####\", tweet)\n    tweet = re.sub(\" {2,}\", \" \", tweet)\n    \n    tweet = re.sub(r'(.)\\1\\1+', r'\\1', tweet) # yeeeees = yes\n\n    contraction_mapping = {'men': 'man', 'hom': 'home', 'txt': 'text',\n                           \"ain't\": \"is not\",\n                           \"aren't\": \"are not\",\n                           \"can't\": \"cannot\",\n                           \"'cause\": \"because\", 'hrs': 'hour',\n                           \"could've\": \"could have\", \"couldn't\": \"could not\",\n                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n                           \"hasn't\": \"has not\", \"haven't\": \"have not\",\n                           \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n                           \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n                           \"i'd\": \"i would\",\n                           \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\",\n                           \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\",\n                           \"let's\": \"let us\", \"ma'am\": \"madam\",\n                           \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\",\n                           \"mightn't've\": \"might not have\", \"must've\": \"must have\",\n                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n                           \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\",\n                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n                           \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\",\n                           \"she'll've\": \"she will have\", \"she's\": \"she is\",\n                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n                           \"so've\": \"so have\", \"so's\": \"so as\",\n                           \"this's\": \"this is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\",\n                           \"that's\": \"that is\", \"there'd\": \"there would\",\n                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n                           \"they'd\": \"they would\", \"they'd've\": \"they would have\",\n                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\",\n                           \"they've\": \"they have\", \"to've\": \"to have\",\n                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\",\n                           \"we'll've\": \"we will have\", \"we're\": \"we are\",\n                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\",\n                           \"what'll've\": \"what will have\", \"what're\": \"what are\",\n                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\",\n                           \"where'd\": \"where did\", \"where's\": \"where is\",\n                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n                           \"who's\": \"who is\", \"who've\": \"who have\",\n                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n                           \"won't've\": \"will not have\",\n                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n                           \"y'all\": \"you all\",\n                           \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\",\n                           \"y'all've\": \"you all have\",\n                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n                           \"you'll've\": \"you will have\",\n                           \"you're\": \"you are\", \"you've\": \"you have\"}\n    \n    tweet = [contraction_mapping[t] if t in contraction_mapping else t for t in tweet.split()]\n    stop_words = set(stopwords.words('english'))\n    not_stop_words = ['no', 'how', 'why', 'who', 'what', 'yes'] # these words are in 'stopwords', and we need it, so we will keep them\n    # drop words if only it's less then 2 ch and not in 'not_stop_words' and it's in 'stopwords'\n    tweet = [word for word in tweet if word not in stop_words and word.__len__() > 2 or word in not_stop_words]\n    # 'WordNetLemmatizer' is mapping the words depending on the 'POS' so we will take the 'POS' of every word and map it to the correct form\n    # please search in google if you don't understand it\n    pos_dict = {\n        'N': 'n',\n        'V': 'v',\n        'J': 'a',\n        'R': 'r'}\n    tuples = nltk.pos_tag(tweet)\n    tweet = ' '.join([wordnet.WordNetLemmatizer().lemmatize(tup[0], pos=pos_dict.get(tup[1][0], 'n')) for tup in tuples])\n    # tweet doesn't contains any word or is a 'Na' value; we will assign it to 'empty text' so we can avoid any error of 'float type' or 'key error'\n    if len(tweet.split()) == 0 or tweet == pp.nan: \n        tweet = 'empty text'\n    return tweet","bbb8ded0":"df['text'] = df['text'].apply(lambda x: text_cleaner(x))","6a55f124":"def extract_most_common(data, label, label_col, text_col, max_word=10):\n    from nltk.probability import FreqDist\n    data = data.loc[data[label_col] == label, text_col] # use just the 'text' of the specified 'label' \n    \n    # append all words of the specified 'label' text\n    all_words = []\n    for row in data:\n        for w in row.split():\n            all_words.append(w)\n    # get the frequency the words of specified 'label', sort it and extract the last 'max_word' words \n    # which are the top-10 most common word\n    freq = pd.DataFrame(FreqDist(all_words).items(), columns=['key', 'value']).sort_values('value', ascending=False)[:max_word]\n    freq = freq.set_index(freq['key']).drop('key', axis=1)\n    \n    return freq\n\n\ndef remove_words_if_in_remove_list(specified_list, remove_list):\n    \"\"\"\n    drop words of specified_list if it exist in remove_list\n    :param specified_list: the list that will be cleaned and returned\n    :param remove_list: the list that its words must not exist in the specified_list\n    :return: specified_list words clean from remove_list words\n    \"\"\"\n    specified_list_value = []\n    specified_list_index = []\n    # KEEP \n    for w, v in zip(specified_list.index, specified_list['value']):\n\n        # append word and its score\/frequency if it's special word for the label\n        if w not in remove_list.index: # if specified list word not in remove list:\n            specified_list_index.append(w)\n            specified_list_value.append(v)\n\n    # return specified list words as dataframe\n    specified_list = pd.DataFrame()\n    specified_list['value'] = specified_list_value\n    specified_list.index = specified_list_index\n    return specified_list\n\n# create 'most_common'\nmost_common = pd.DataFrame()\nfor category in df['category'].unique():\n    # extract extract most common words for 'category'\n    freq = extract_most_common(df, category, 'category', 'text')\n    \n    # count the ocurrance of each word of the category's special words, in the text\n    for word in freq.index:\n        word_count = []\n        for row in df['text']:\n            if word in row.split(): \n                word_count.append(1) # if word ocurred:\n            else:\n                word_count.append(0) # if not word ocurred:\n        # create a feature for the word counts,(its values are similar to 'WordVectorizer' class in sklearn)\n        most_common[word] = word_count\n\nmost_common = most_common.T.drop_duplicates()\n# here we transposed the words(cols) into index and used drop_duplicates because\n# there will be frequent words for more then one class so they will be duplicated.\n\n# then we will return the words to be columns(features).\nmost_common = most_common.T\nprint(most_common.head())\nprint(f'\\n most common words are: {len(most_common.columns)}')","6c628e00":"pca = PCA().fit(most_common) \n\n# plot PCA explained_variance_ratio_\nplt.plot(pp.cumsum(pca.explained_variance_ratio_))\nplt.axhline(y=.90, color = 'r', linestyle = '--') \nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\n\n# create dataframe of explained_variance_ratio_ values\ncumsum = pd.DataFrame(pp.cumsum(pca.explained_variance_ratio_))\nprint('cumulative explained variance values: ')\nprint(cumsum)\n\n# extract components the give 90% info of data, and use the first one\nbest_component = cumsum[(cumsum >= .90) & (cumsum  < .91)].dropna().index[0] \n# (cumsum >= .90) & (cumsum  < .92) is because we may not find a component of varaince .90, so we will take any\n# component between .90 and 91\n\n# Note: you can set the best_component manually, the above code is just to make the extraction of the 'best_component' automatically\n\nprint(f'\\n total most common words: {len(most_common.columns)}')\n\nprint('Best Component that keep 90% info:', best_component)\nprint(f'\\n total most common words after reduction: {best_component}')","eaf3a2b2":"pca = PCA(n_components=best_component).fit(most_common)\n\nfeatures = pd.DataFrame(pca.transform(most_common))  # I just put it as DataFrame to use pandas API\n\ndf = pd.concat([df, features], axis=1)\nprint(df.head())","f0ab568b":"null = pd.DataFrame(df.isnull().sum() \/ len(df) * 100)\nprint('Na % in each column: ')\nprint(null)\ndf.dropna(inplace=True)","aae0e849":"all_words = [w for row in df['text'] for w in row.split()]\n\nall_words_freq = pd.DataFrame(FreqDist(all_words).items(), columns=['key', 'value']).sort_values('value', ascending=False)[:50]\nplt.figure(figsize=(15, 10))\nplt.title('most common words top 50')\nplt.barh(all_words_freq['key'], all_words_freq['value'])","9c79fe58":"all_words = [w for row in df['text'] for w in row.split()]\n\nall_words_freq = pd.DataFrame(FreqDist(all_words).items(), columns=['key', 'value']).sort_values('value',\n                                                                                                 ascending=False)[:50]\nall_words_freq = all_words_freq.set_index(all_words_freq['key']).drop('key', axis=1)\n\nfor category in df['category'].unique():\n    freq = extract_most_common(df, category, 'category', 'text')\n    category_freq = remove_words_if_in_remove_list(freq, all_words_freq)\n    category_freq.plot()\n\n    plt.title(f'{category} Frequancy')\n    plt.xlabel('Key')\n    plt.ylabel('ocurred')\n    plt.xticks(rotation=90)\n    plt.plot(category_freq)","5a945de1":"plt.figure(figsize=(15, 10))\nsb.distplot(df['n_words'])\n\nplt.title('Length of News')\nplt.show()\nprint('min length: ', df['n_words'].min())\nprint('max length: ', df['n_words'].max())\nprint('mean length: ', df['n_words'].mean())","74f0c188":"plt.figure(figsize=(25, 15))\nplt.title('n_upper')\nsb.barplot(df['n_upper'], df['category'])","472a37e8":"plt.figure(figsize=(25, 15))\nplt.title('n_lower')\nsb.barplot(df['n_lower'], df['category'])","dcb178f3":"plt.figure(figsize=(25, 15))\nplt.title('n_digit')\nsb.barplot(df['n_digit'], df['category'])","8c9f9788":"plt.figure(figsize=(25, 15))\nplt.title('n_title')\nsb.barplot(df['n_title'], df['category'])","f37d2ad7":"plt.figure(figsize=(25, 15))\nplt.title('n_special_ch')\nsb.barplot(df['n_special_ch'], df['category'])","0c50f9a4":"plt.figure(figsize=(25, 15))\n\nplt.title('n_words')\nsb.barplot(df['n_words'], df['category'])","cf9ce76d":"train_set, test_set = train_test_split(df, test_size=.1, random_state=42)\nval_set, test_set = train_test_split(test_set, test_size=.5, random_state=42)\nprint(f'Train set length: {len(train_set)} ; categories: {train_set[\"category\"].nunique()}')\nprint(f'Val set length: {len(val_set)} ; categories: {val_set[\"category\"].nunique()}')\nprint(f'Test set length: {len(test_set)} ; categories: {test_set[\"category\"].nunique()}')","627d2132":"def prepare_text_dict(text, min_freq=3):\n    # split text\n    all_words = [w for row in text for w in row.split()]\n\n    # the frequencies of words in text\n    freq = FreqDist(all_words)\n    clean_words = []\n\n    # drop words occurred less min_freq\n    for row in text:\n        for word in row.split():\n            if freq[word] > min_freq:\n                clean_words.append(word)\n        \n    # vocabs set\n    clean_words = set(clean_words)\n    # convert words to indexes\n    word2index = {w: i for i, w in enumerate(clean_words, 2)}  # start at 2\n    word2index['<pad>'] = 1\n    word2index['<unk>'] = 0\n\n    # convert indexes to words\n    index2word = {i: w for i, w in enumerate(clean_words, 2)}  # start at 2\n    index2word[1] = '<pad>'\n    index2word[0] = '<unk>'\n    \n    del freq, clean_words # just to save some RAM memory\n    \n    return word2index, index2word\n# process text dict\ntext = pd.concat([train_set['text'], val_set['text']], axis=0)\nword2index, index2word = prepare_text_dict(text) # notice that we are won't use the 'test_set' vocabs\n\nprint('Total vocabs ', len(word2index))","b6e82d67":"# create dict for labels\nunique_classes = df['category'].unique()\n\nencoding2label = dict(enumerate(unique_classes))  # for every class there is a number. starting from 0. same as {0: 'politics', 1: 'sport', 2: 'parenting' ...}\n\nlabel2encoding = {value: key for key, value in encoding2label.items()} # the oppsite of {'politics': 0,  'sport': 1, 'parenting': 2 ...} \n\nfeatures_cols = df.drop(['text', 'category'], axis=1).columns # names of pca components' columns, we will use this list to extract the features we want to train with text later","f4b3f942":"# Dataset class\nclass Data(data.Dataset):\n    def __init__(self, df, word2index, label2encoding):\n        self.word2index = word2index # to index text\n\n        # sort text by length of total words\n        df['length'] = df['text'].apply(lambda x: len(x.split()))\n        df.sort_values(by='length', inplace=True)\n\n        # prepare data\n        self.target = df['category'].apply(lambda x: label2encoding[x]).values\n        self.text = df['text'].apply(lambda x: x.split()).values\n        self.features = df.loc[:, features_cols].values\n\n    def __len__(self):\n        return len(self.target)\n\n    def __getitem__(self, item):\n        sequence = [self.word2index[w] for w in self.text[item] if w in self.word2index]\n        features = self.features[item]\n        target = self.target[item]\n\n        return {'target': target, 'features': features, 'sequence': sequence}\n\n\ndef Bert_data(df, label2encoding):\n    # sort text by length of words\n    df['length'] = df['text'].apply(lambda x: len(x.split()))\n    df.sort_values(by='length', inplace=True)\n    target_uniques = df['category'].nunique()\n\n     # prepare data\n    target = df['category'].apply(lambda x: label2encoding[x]).values\n    text = df['text'] # the BertTokenizer will index them so we don't need for word2index\n    features = df.loc[:, features_cols].values\n\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    x_encoded = tokenizer.batch_encode_plus(text, pad_to_max_length=True)\n    \n    input_ids = torch.tensor(x_encoded['input_ids'],  device=device, dtype=torch.long)\n    attention_mask = torch.tensor(x_encoded[\"attention_mask\"],  device=device, dtype=torch.long) # to mark the text; if it was not a padding index,1 else 0\n    target = torch.tensor(target,  device=device, dtype=torch.long)\n    features = torch.tensor(features, device=device, dtype=torch.float)\n    \n    return data.TensorDataset(input_ids, attention_mask, features, target)\n\ntrain_data = Data(train_set, word2index, label2encoding)\nval_data = Data(val_set, word2index, label2encoding)\ntest_data = Data(test_set, word2index, label2encoding)\n\nbert_train_data = Bert_data(train_set, label2encoding)\nbert_val_data = Bert_data(val_set, label2encoding)\nbert_test_data = Bert_data(test_set, label2encoding)","81cb298d":"# pad text\n# you can use any padding function instead of this, don't worry if it's hard to understand,\n# it's like any padding function but with less code :)\ndef pad_text(list_text, seq_length):\n    paded_text = []\n    for text in list_text:\n\n        if len(text) == seq_length:\n            paded_text.append(text)\n        else:\n            paded_text.append(text + (seq_length - len(text)) * [1]) # '1' is the index for 'PAD' in vocabs dict\n\n    return paded_text\n\n\ndef collate_fn_padded(batch):\n    target = [b['target'] for b in batch]\n    features = [b['features'] for b in batch]\n    sequence = [b['sequence'] for b in batch]\n\n    # extract max_length\n    max_length = max([len(b) for b in sequence])\n    \n    # pad text\n    sequence = pad_text(sequence, max_length)\n\n    # convert list to torch.tensor\n    return {'target': torch.tensor(target, device=device, dtype=torch.long),\n            'features': torch.tensor(features, device=device, dtype=torch.float),\n            'sequence': torch.tensor(sequence, device=device, dtype=torch.long)\n            }","35ecf0bb":"train_loader = data.DataLoader(train_data, batch_size=32, collate_fn=collate_fn_padded)\nval_loader = data.DataLoader(val_data, batch_size=32, collate_fn=collate_fn_padded)\ntest_loader = data.DataLoader(test_data, batch_size=32, collate_fn=collate_fn_padded)\n\nbert_train_loader = data.DataLoader(bert_train_data, batch_size=12)\nbert_val_loader = data.DataLoader(bert_val_data, batch_size=12)\nbert_test_loader = data.DataLoader(bert_test_data, batch_size=12)","c8cbf5bf":"# --- word embedding files --- #\nvocab = word2index\n\n# every embedding file is of format '.txt', and every line contains the word as the first vlaue and the embedding vector as the rest of the values.\n# since the embeddings was trained on millions of words, there will be alot of lines, but we want just the embeddings of our vocabs \n\n# tweeter embeddings\ntweeter_embeddings = {}\nprint('\\ntweeter embeddings...')\n\nwith open('..\/input\/glovetwitter27b50d\/glove.twitter.27B.50d.txt', 'r') as f:\n    for line in f.readlines():\n        vector = line.split() # split by 'space'\n\n        word = vector[0] # word\n        dims = vector[1:] # embedding vector\n        if word in vocab.keys():  # if word is in our vocabs: \n            tweeter_embeddings[word] = pp.array(dims) # assign 'word' to its vector embedding in 'tweeter_embeddings' dict\n\nmissing = len(word2index) - len(tweeter_embeddings)\n\nprint(f'tweeter embeddings found for our vocabs: {len(tweeter_embeddings)} | missing: {missing} ; {missing \/ len(word2index) * 100:.1f}%')\n\n# glove embeddings\nprint('\\nglove embeddings...')\nglove_embeddings = {}\nwith open('..\/input\/glove6b50d\/glove.6B.50d.txt', 'r') as f:\n    for line in f.readlines():\n        vector = line.split()  # split by 'space'\n\n        word = vector[0] # word\n        dims = vector[1:] # embedding vector\n        if word in vocab.keys(): # if word is in our vocabs: \n            glove_embeddings[word] = pp.array(dims) # assign 'word' to its vector embedding in 'tweeter_embeddings' dict\n\nmissing = len(word2index) - len(glove_embeddings)\nprint(f'glove embeddings found for our vocabs: {len(glove_embeddings)} | missing: {missing} ; {missing \/ len(word2index) * 100:.1f}%')\n\nprint('total words of our vocabs: ', len(word2index))","afe88551":"# --- concatenate embeddings --- #\n\nd = pd.DataFrame(pd.concat([pd.DataFrame(tweeter_embeddings), pd.DataFrame(glove_embeddings)], axis=0, join='inner'))\n# 'inner' mean to not add any word embedding that does not contains dimentions from both files embeddings\n\ncombined_embedding = {}\n\nfor col in d.columns:\n    combined_embedding[col] = d[col].values","33dd80a0":"# --- embedding matrix --- #\n\n# we will give any word that has no embeddings a 100-dimension full of random numbers between -0.25 and 0.25\nfor word in vocab:\n    if word not in combined_embedding.keys():\n        combined_embedding[word] = pp.random.uniform(-0.25, 0.25, 100)  # 50d + 50d = 100d\n\n# create Look-Up table\ndef create_embedding_matrix(word_index, embedding_dict, dimension):\n    embedding_matrix = pp.zeros((len(vocab) + 1, dimension)) # (len_vocab, 100)\n\n    for word, index in word_index.items():\n        if word in embedding_dict:\n            embedding_matrix[index] = embedding_dict[word]\n\n    return embedding_matrix\n\nembedding_matrix = create_embedding_matrix(vocab, combined_embedding, 100)","b1896ea1":"class LSTM_Model(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, n_layers, output_size, p=.5, use_features=False,\n                 features_size=None):\n\n        super().__init__()  # This is needed to initialize the nn.Module properly\n\n        # 'use_features' if used, we supposed to give text and feature data to the model to make prediction.\n        # create 'use_features' as 'self.use_features' so we can use it in the hole class functions(e.g. 'forward') instead of just in '__init__' attrbute\n        self.use_features = use_features\n\n        # 'Embedding' layer works like a lookup table(dictionary). The words are the keys in this table, while the dense word vectors are the values.\n        self.Embedding = nn.Embedding(vocab_size, embedding_dim)\n\n        # LSTM is the Long Short-Term Memory layer that will take the vectors represntations and extract the semantics\/hidden_states\n        self.LSTM = nn.LSTM(embedding_dim, hidden_size, num_layers=n_layers, bidirectional=False, batch_first=True, dropout=p)\n        # if 'batch_first=True' 'hidden' shape will be (num_layers * num_directions, batch, hidden_size)\n        # if 'batch_first=False' 'hidden' shape will be (num_layers * num_directions, seq_len, hidden_size)\n        \n\n        # A Dense\/Linear layer that learn from last_hidden_state (optional layer; you can remove it)\n        self.Linear = nn.Linear(hidden_size, hidden_size)\n\n        # Dropout layer that help regularizing and prevent overfitting (optional layer)\n        # It randomly zeroes some of the elements of the input tensor with probability 'p'\n        self.Dropout = nn.Dropout(p)\n\n        # Dense\/Linear layer to predict the class of the text\/sequence\n        self.output = nn.Linear(hidden_size, output_size)\n\n        # add these layers if 'use_features'\n        if self.use_features:\n            # this dense layer will take the features and tune its parameters\n            self.f_Linear = nn.Linear(features_size, hidden_size)\n\n            # this dense layer will take the output from 'f_Linear' and predict the class of the text\/sequence\n            self.f_output = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, features=None):  # run the network\n\n        Embedding = self.Embedding(x)  # map the words to their vectors representations by the Embedding layer\n\n        output, (hidden, cell) = self.LSTM(Embedding)  # calculate the sequence\/text sementics\n        # 'output' comprises all the hidden states in the last layer\n        # (hidden, cell) comprises the hidden states after the last timestep\n\n        # 'cell' state contains info about wether to keep a hidden state or no (num_layers * num_directions, batch, hidden_size)\n        # 'output' state is a tensor of all the 'hidden' states from each time step  (seq_len, batch, num_directions * hidden_size)\n        # 'hidden' state is the hidden states from the last time step  (num_layers * num_directions, batch, hidden_size) \n\n        last_hidden_state = hidden[-1, :, :]  # last hidden state from the last time step (seq_len, hidden_size)\n\n        x = nn.functional.leaky_relu(self.Linear(last_hidden_state), .001)\n        # 'leaky_relu' is similar to relu activation function, it just let the values to be between '0.001' and any other number\n\n        x = self.Dropout(x)  # apply dropout\n\n        # combine the outputs of text and features if 'use_features' is 'True'.\n        # we will multiply each output by 0.5 to get half the original weights, and then combine the two halves\n        # to have a predictions from both outputs weights.\n        if self.use_features:\n\n            x = self.output(x) * 0.5  # half the weights\n\n            x_2 = nn.functional.leaky_relu(self.f_Linear(features), .001)\n            x_2 = self.f_output(x_2) * 0.5  # half the weights\n\n            final_x = x + x_2  # combine the two halves outputs\n\n            return final_x\n\n        else:\n            return self.output(x)  # if 'use_features' is 'False' then 'features' output won't be calculated","ef012e12":"# BI-LSTM\nclass BiLSTM_Model(nn.Module):\n\n    def __init__(self, vocab_size, embedding_dim, hidden_size, n_layers, output_size, p=.5, use_features=False,features_size=None):\n        super().__init__()\n        self.use_features = use_features\n        self.Embedding = nn.Embedding(vocab_size, embedding_dim)\n        \n        # if we make 'bidirectional'='True', we will\n        self.LSTM = nn.LSTM(embedding_dim, hidden_size, num_layers=n_layers, bidirectional=True, batch_first=True, dropout=p)\n        self.Linear = nn.Linear(hidden_size * 2, hidden_size) # 'hidden_size * 2' because we will concatenate last two hidden states\n        self.Dropout = nn.Dropout(p)\n        self.output = nn.Linear(hidden_size, output_size)\n\n        if self.use_features:\n            self.f_Linear = nn.Linear(features_size, hidden_size)\n            self.f_output = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, features=None):\n        Embedding = self.Embedding(x)\n        output, (hidden, cell) = self.LSTM(Embedding)\n\n        hidden = torch.cat([hidden[-2, :, :], hidden[-1, :, :]], dim=1) # concatenate left and right context\n        \n        x = nn.functional.leaky_relu(self.Linear(hidden), .001)\n        x = self.Dropout(x)\n        \n        if self.use_features:\n            x = self.output(x) * 0.5\n\n            x_2 = nn.functional.leaky_relu(self.f_Linear(features), .001)\n            x_2 = self.f_output(x_2) * 0.5\n\n            final_x = x + x_2\n            return final_x\n        else:\n            return self.output(x)","3669bf17":"class CNN_Model(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_size, p=.5, use_features=False,\n                 features_size=None):\n        super().__init__()\n        self.use_features = use_features\n        self.Embedding = nn.Embedding(vocab_size, embedding_dim)\n\n        # we will build 'Conv2d' layers as much as the amount of 'filter_sizes' list\n        # the 'kernel_size' will be (filter_size, embedding_dim),\n        # 'filter_size' is the number words to take as 'n-gram'\n        # 'embedding_dim' is the vector representation for the word (Embedding dimentions)\n        # 'ModuleList' helps in creating more then one layer at once\n        self.convs = nn.ModuleList(nn.Conv2d(1, n_filters, (ks, embedding_dim)) for ks in filter_sizes)  # 'convs' will be as much as 'filter_sizes' list\n\n        # note: There is no hidden layer between 'convs' layers and 'output' layer\n\n\n        # we will concatenate all 'convs' layers outputs and feed them to the 'output' layer\n        self.output = nn.Linear(len(filter_sizes) * n_filters, output_size)\n\n        self.Dropout = nn.Dropout(p)\n\n        if self.use_features:\n            self.f_Linear = nn.Linear(features_size, hidden_size)\n            self.f_output = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, features=None):\n        # the 'Conv2d layer' wants inputs of shape (batch, Channel, Height, Width)\n        # our text is of shape(batch, seq_lens)\n\n        # after feeding text to the 'Embedding' layer, we will have shape(batch, seq_lens, embedding_dim)\n        Embedding = self.Embedding(x)\n\n        # the convolutional layer wants the inputs to be of\n        # shape(batch,  Channel, Height, Width) and the 'Embedding' layer's output is shape(batch, seq_lens, embedding_dim)\n        # to make 'Embedding' output correct to feed a 'convs' layer, its shape must be same as the shape of a conv layer.\n\n        # the batch in conv shape exist in 'Embedding' shape.\n        # the Channel in conv shape will be the number of embedding layers; in our case it's 1 'Embedding' layer.\n        # the Height in conv shape  will be the 'seq_lens'(num words or n-gram)\n        # the Width in conv shape  will be the 'embedding_dim'(vector representation of a word)\n\n        # conv(batch, Channel, Height, Width) = Embedding(batch, num_embeddings, seq_lens, embedding_dim)\n        Embedding = Embedding.unsqueeze(1) # create Channel dimension\n\n        convs = [F.relu(c(Embedding)).squeeze(3) for c in self.convs]\n        # after applying a conv layer, the output will be of shape (batch, Height, Width, Channel)\n        # the 'max_pool1d' layer doesn't want the channel dimension(third dim), so we will remove\/squeeze it by using .squeeze(3)\n\n        convs = [F.max_pool1d(c, c.shape[2]).squeeze(2) for c in convs]\n        # the kernel_size is the word embedding_dim (c.shape[2]; which is the Width dimension(second dim) ).\n        # since the Width dimension (c.shape[2]) is pooled, the second dimension is empty.\n        # so we will remove\/squeeze it by using .squeeze(2)\n\n        # we will concatenate all 'convs' outputs (note that 'convs' is a list of all 'convs' layers\n        # outputs; e.g. [conv1, conv2, ...])\n\n        # the shape is, 'batch_size' and the sum of all the filters of all 'convs'\n        # layers (e.g. n_filter1 + n_filter2 ..., depending on how many conv layer there)\n        total_pools = torch.cat(convs, dim=1)  # (batch, total_filters)\n\n        x = self.Dropout(total_pools)\n\n        if self.use_features:\n            x = self.output(x) * 0.5\n            x_2 = nn.functional.leaky_relu(self.f_Linear(features), .001)\n            x_2 = self.f_output(x_2) * 0.5\n            final_x = x + x_2\n            return final_x\n        else:\n            return self.output(x)\n\n\n# We have to build a function that pad tha sequences that are less then the filter size\n# The model will throw an error if a sentence\/sequence is less then the kernel_size of certain conv, so we must pad it\n# to the maximum size of kernel_size.\ndef cnn_padding_to_match_filtersize(text):\n    features_ = []\n    for f in text:\n        f = f.cpu().numpy().tolist()\n        if len(f) < max(filter_sizes):\n            f += [1] * (max(filter_sizes) - len(f))\n            features_.append(f)\n        else:\n            features_.append(f)\n\n    return torch.LongTensor(features_).to(device)","ca3704fe":"\n# BI-LSTM all hidden_state concatenated\nclass RCNN_Model(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, n_layers, output_size, p=.5, use_features=False,\n                 features_size=None):\n        super().__init__()\n\n        self.use_features = use_features\n        self.Embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.LSTM = nn.LSTM(embedding_dim, hidden_size, num_layers=n_layers, dropout=p, bidirectional=True,\n                            batch_first=True)\n        # because the concatenated contexts will be pooled as in the paper, it's not recommended to put a hidden dense layer.\n        # (if you will, it's better to make its 'input' and 'output' shapes as the shape of the concatenated contexts)\n        self.Linear = nn.Linear(2 * hidden_size + embedding_dim, 2 * hidden_size + embedding_dim)\n\n        self.output = nn.Linear(2 * hidden_size + embedding_dim, output_size)\n        # 'output' input shape is:\n        # (2 * hidden_size + embedding_dim), '2 * hidden_size' is just because the inputs will be of shape\n        # (batch, left_context_h_state * embedding_dim * right_context_h_state)\n        # both 'left_context_h_state' and 'right_context_h_state' have same 'hidden_size', (because bi-lstm is of two directions\n        # with the same hidden size), when they are together, they will be of hidden size: 2 * hidden_size\n\n        if self.use_features:\n            # here we just changed the 'input' shape of the last layer, (because we didn't create any hidden layer\n            # above, like in the other models)\n            self.f_Linear = nn.Linear(features_size, 2 * hidden_size + embedding_dim)\n            self.f_output = nn.Linear(2 * hidden_size + embedding_dim, output_size)\n\n    def forward(self, x, features=None):\n        # x shape (batch, seq_lens)\n        inputs = self.Embedding(x)  # (batch, seq_lens, embedding_dim)\n\n        output, (hidden_state, cell_state) = self.LSTM(inputs)\n        # 'output' shape (batch, seq_len, num_directions * hidden_size)\n\n        # the hidden state values when bilstm 'bidirectional is True', will be like this [left_direction, right_direction].\n\n        # the 'output' contain all hidden states of both directions. so if we want to extract all the left_context(original direction)\n        # hidden states, we need to extract every 'first hidden' value(by skipping 'right_direction' in [left_direction, right_direction])\n\n        # # output[:, :, ::2] '2' here means take every 'second' index starting from 0\n        # example: | left|     right|       left|     right|  left|\n        #          |    0|         1|          2|         3|     4|\n        #          |start|     first|     second|     first|second|\n        #          | take|don't take|       take|don't take|  take|\n        left_context = output[:, :, ::2]  # ('batch', 'seq_len', hidden_size)\n\n        # and if we want to extract the right_context(opposite direction),\n        # we need to extract every 'second hidden' by skipping the first value (by skipping the left_direction,\n        # like in [left_direction, right_direction])\n\n        # example: output[:, :,1::2] 0th index will not be counted(start from 1)\n        # # output[:, :, 1::2] '2' here means take every 'second' index starting from 1\n        # example: |            left|right|      left| right|      left|\n        #          |               0|    1|         2|     3|         4|\n        #          | 0th index(skip)|start|     first|second|     first|\n        #          |don't take(skip)| take|don't take|  take|don't take|\n        right_context = output[:, :, 1::2]  # ('batch', 'seq_len', hidden_size)\n\n        # concatenate the three contexts (which are in 'num_directions * hidden_size' axis,  dim=2) as in the paper of RCNN\n        concatenated_contexts = torch.cat((left_context, inputs, right_context), dim=2)  # dim=2 is the  hidden_size\/context axis\n        # concatenated_contexts ('batch', 'seq_len', 'concatenated_contexts')\n\n        # apply linear transformation with tanh activation\n        concatenated_contexts = torch.tanh(self.Linear(concatenated_contexts))\n        # linear_transformation ('batch', 'seq_len', 'concatenated_contexts')\n\n        # permute the shape to be: ('batch', 'concatenated_contexts', 'seq_len')\n        # so we can to apply max-pooling to the hidden-states(contexts) by pooling 'seq_len' dim\n        # because max-pooling, as in docs, pool the second axis\n        concatenated_contexts = concatenated_contexts.permute(0, 2, 1)  # ('batch', 'concatenated_contexts', 'seq_len')\n\n        # extract the features by pooling each sentence\/sample in the batch (pooling it's hidden_states\/contexts)\n        # to one feature by a kernel_size of sentence length\n        pooled = F.max_pool1d(concatenated_contexts, concatenated_contexts.size()[2])\n        # 'pooled' shape ('batch', 'concatenated_contexts', 1)\n        # 'seq_len' dimension is '1', because 'final_contexts' pooled by a kernel of size 'seq_len' (which is the 2nd-axis)\n        \n        pooled = pooled.squeeze(2)  # squeeze 'seq_len'(2nd-axis)\n        # pooled shape(batch, 'concatenated_contexts') same as the original shape of text 'x' dimensions\n        # which is (batch, 'seq_len')\n\n        if self.use_features:\n            x = self.output(pooled) * 0.5\n            x_2 = nn.functional.leaky_relu(self.f_Linear(features), .001)\n            x_2 = self.f_output(x_2) * 0.5\n            final_x = x + x_2\n            return final_x\n        else:\n            return self.output(pooled)","4c2e1df9":"\n# BI-LSTM all hidden_state concatenated\nclass BiLSTM_concat_hiddens_Model(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, n_layers, output_size, p=.5, use_features=False,\n                 features_size=None):\n        super().__init__()\n        self.use_features = use_features\n        self.n_layers = n_layers\n\n        self.Embedding = nn.Embedding(vocab_size, embedding_dim)  # Initializing the look-up table.\n        self.dropout = 0.5\n        self.LSTM = nn.LSTM(embedding_dim, hidden_size, num_layers=n_layers, dropout=self.dropout, bidirectional=True,\n                            batch_first=True)\n\n        # the shape is (hidden_size * 2) because we will concatenate all the hidden states of the last lstm layer:\n        # and '2' is because we set the bidirectional=True which mean every layer has 2 directions.\n        self.Linear = nn.Linear(hidden_size * 2, hidden_size)\n\n        self.Dropout = nn.Dropout(self.dropout)\n        self.output = nn.Linear(hidden_size, output_size)\n\n        if self.use_features:\n            self.f_Linear = nn.Linear(features_size, hidden_size)\n            self.f_output = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, features=None):\n        batch_size = x.shape[0]  # assign variable that contains the length of batch_size\n        # we will need it when reshaping LSTM hidden_state dimensions in a way that can let us extract the last layer\n        inputs = self.Embedding(x)\n        output, (hidden_state, cell_state) = self.LSTM(inputs)\n\n        # because we only want the last layer, the docs say that you can separate\n        # the hidden state to (num_layers, num_directions, batch_size, hidden_size)\n        hidden_state = hidden_state.view(self.n_layers, 2, batch_size, hidden_size)\n        last_layer = hidden_state[-1]  # (num_directions, batch_size, hidden_size)\n        # [-1] is the last layer\n\n        # extract the hidden states values of both directions to a list, so we can concatenate this list of multiple directions\n        # to a tensor of all hidden states from both directions\n        last_layer_hidden = [last_layer[i, :, :] for i in range(last_layer.shape[0])] # the shape (batch_size, hidden_size * num_directions)\n        # last_layer.shape[0] is 'num_directions' dimension\n\n        # concatenate the 'last_layer_hidden' list so we can feed it to the dense layer\n        concatenated_hidden = torch.cat(last_layer_hidden, dim=1)  # (batch_size, hidden_size * num_directions)\n\n        x = F.leaky_relu(self.Linear(concatenated_hidden), 0.001)\n\n        x = self.Dropout(x)\n\n        if self.use_features:\n            x = self.output(x) * 0.5\n            x_2 = nn.functional.leaky_relu(self.f_Linear(features), 0.001)\n            x_2 = self.f_output(x_2) * 0.5\n            final_x = x + x_2\n            return final_x\n        else:\n            return self.output(x)","e236d085":"class BiLSTM_2DMAX_POOLING_2D_CNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, n_filters, n_layers, output_size, p=.5, k_size=2,use_features=False, features_size=None):\n        super().__init__()\n        self.use_features = use_features\n        self.Embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.LSTM = nn.LSTM(embedding_dim, hidden_size, num_layers=n_layers, bidirectional=True, batch_first=True, dropout=p)\n        self.Conv2d = nn.Conv2d(1, n_filters, kernel_size=k_size)  # 1 since there is one channel(embedding layer)\n        self.MaxPool2d = nn.MaxPool2d(kernel_size=k_size)\n        self.Dropout = nn.Dropout2d(p)\n\n        self.linear_shape = None\n        # to extract the 'input_shape' a linear layer wants after convs layers,\n        # torch.rand((1, 1, n_layers*2, hidden_size)) is just an example to apply 'convs' layers and extract the shape\n        # because 'self.convs' will apply all the 'convs' calculation, and the shape of it will be (batch, filters, Height, Width) \n        # which is the shape of the pooling layer (the last layer in 'convs' layers). A 'Linear' layer must take the \n        # following shape (batch, filters * Height * Width), so we will assign 'self.linear_shape' to the sum of (filters * Height * Width)\n        # that we will extract by applying 'convs' layers to the example\n        \n        \n        self.convs(torch.rand((1, 1, n_layers*2, hidden_size)))  # the example shape(1, 1, n_layers*2, hidden_size) is the formal shape a 2d-conv \n        # layer want (batch ,Channel, Height, Width) (because the first layer in 'convs' is a '2d-conv' layer).\n\n        self.linear = nn.Linear(self.linear_shape, hidden_size) \n        # DON'T WORRY IF YOU DIDN'T UNDERSTAND WHAT I MEAN HERE, BECAUSE IT'S NOT PART OF THE MODEL ARCHITECTURE;IT'S FOR THE EXPERIEMENT.\n        # Although this layer is optional; it's important for 'self.use_features' because when we will combine both predictions probabilities\n        # of features and text, we need to have same shape. We will make 'hidden_size' a common shape for both outputs layers.\n        # and for every method (text and feature), we will create a 'Linear' layer before the 'output' layers in each method,\n        # so it can give us an output of shape 'hidden_size'.\n        \n        self.output = nn.Linear(hidden_size, output_size)\n\n        if self.use_features:\n            self.f_Linear = nn.Linear(features_size, hidden_size)\n            self.f_output = nn.Linear(hidden_size, output_size)\n\n    def convs(self, hidden):\n        # Compute 'convs' layers\n        \n        # hidden shape(batch, Channel, num_layers * num_directions, hidden_size) same as 2d-conv formal shape(batch, Channel, Height, Width)\n        conv = F.leaky_relu(self.Conv2d(hidden), 0.01)  # output(features) shape (batch, filters, Height, Width)\n        pool = F.leaky_relu(self.MaxPool2d(conv), 0.01)  # output(features) shape (batch, filters, Height\/ k_size, Width\/ k_size) pooled\n        pool = self.Dropout(pool) # pool output shape(batch, filters, Height, Width)\n        \n        # extract the formal shape for 'Linear' layer input_shape.\n        linear_shape = pool.shape[1] * pool.shape[2] * pool.shape[3] # (filters * Height * Width)\n        if self.linear_shape is None:\n            self.linear_shape = linear_shape\n\n        # flatten pool \n        pool = pool.view(-1, linear_shape)  # (batch, filters * Height * Width)\n        return pool\n\n    def forward(self, x, features=None):\n        # x shape (batch , seq_length)\n        Embedding = self.Embedding(x) # (batch , seq_length, embedding_dim)\n        output, (hidden, cell) = self.LSTM(Embedding)\n        # 'hidden' shape = (num_layers * num_directions, batch, hidden_size)\n        \n        # the 'Conv2d' layer want its inputs to be of shape conv(batch, Channel, Height, Width). 'hidden' after permuting dims \n        # will be of shape:lstm(batch, num_layers * num_directions, hidden_size) -> conv(batch, Height, Width)\n        hidden = hidden.permute(1, 0, 2)  # convert the  shape to lstm(batch, num_layers * num_directions, hidden_size)\n\n        # the channel dim is missing. We need to make it by unsqueeze\/create a new dimension(axis) in \n        # the first-axis place so the 'hidden' tensor will have the correct input shape for a convolutional layer.\n\n        hidden = hidden.unsqueeze(1)  # lstm(batch, Channel, num_layers * num_directions, hidden_size) same as conv(batch, Channel, Height, Width)\n        \n        pool = self.convs(hidden) # apply 'convs' layers to 'hidden' values\n        \n        linear = self.linear(pool) # feed 'pool' to the linear layer\n\n        if self.use_features:\n            x = self.output(linear) * 0.5\n            x_2 = F.leaky_relu(self.f_Linear(features), .001)\n            x_2 = self.f_output(x_2) * 0.5\n            final_x = x + x_2\n            return final_x\n        else:\n            return self.output(linear)","34299fc9":"class SelfAttention(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, n_layers, p=.5, use_features=False,\n                 features_size=None):\n        # Note: I will speak in singular form just because the paper is explaining using example of one sentence; but in reality we have more then one sentence\n        super(SelfAttention, self).__init__()\n        self.use_features = use_features\n        self.Embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.LSTM = nn.LSTM(embedding_dim, hidden_size, num_layers=n_layers, dropout=p, bidirectional=True, batch_first=True)\n\n        # '2 * hidden_size' is same as 'da-by-2u' (e.g. 'any hyperparameter' * '2(num_direction)')\n        self.W_s1 = nn.Linear(hidden_size * 2, hidden_size, bias=False) # no bias as in paper\n\n        # 'hidden_size' is same as 'da' (e.g. 'any hyperparameter')\n        self.W_s2 = nn.Linear(hidden_size, hidden_size, bias=False) # no bias as in paper\n        self.fc_layer = nn.Linear(hidden_size * 2 * hidden_size, hidden_size) # optinal layer\n        self.output = nn.Linear(hidden_size, output_size)\n\n        if self.use_features:\n            self.f_Linear = nn.Linear(features_size, hidden_size)\n            self.f_output = nn.Linear(hidden_size, output_size)\n\n    def attention_weights(self, H):\n        # H shape(batch, seq_len, num_directions * hidden_size)\n        W_s1 = torch.tanh(self.W_s1(H))  # same shape as 'H'\n\n        W_s2 = self.W_s2(W_s1)  # same shape as 'H'\n        weight_matrix = W_s2.permute(0, 2, 1)  # (batch, num_directions * hidden_size, seq_len)\n\n        A = F.softmax(weight_matrix, dim=2) # apply softmax for second dim as in the paper (seq_len)\n        # the softmax() ensures all the computed weights sum up to 1.\n\n        # now 'A' have the same size as 'n' as in ('n-by-2', which is the shape of H), which means we created the attention_weights\n        # as expected from the paper (Since H is sized n-by-2u, the annotation vector 'A' will have a size 'n') step.\n        return A\n\n    def forward(self, x, features=None):\n        inputs = self.Embedding(x)\n        output, (hidden_state, cell_state) = self.LSTM(inputs)\n\n        H = output  # output is the (concatenate each \u2212\u2192ht with \u2190\u2212ht to obtain a hidden state 'H') step;\n        # because output contains all the hidden states.\n\n        attention_weights = self.attention_weights(H)  # apply self-attention mechanism step\n\n        M = torch.bmm(attention_weights, H) # multiply H by the attention weights; (the resulting matrix is the sentence embedding) step\n        \n        # flatten M; the (encode a variable length sentence into a fixed size embedding) step.\n        fixed_size_embedding = M.view(-1, M.size()[1] * M.size()[2]) # (batch, seq_len * hidden_size)\n\n        x = self.fc_layer(fixed_size_embedding) \n\n        if self.use_features:\n            x = self.output(x) * 0.5\n            x_2 = nn.functional.leaky_relu(self.f_Linear(features), .001)\n            x_2 = self.f_output(x_2) * 0.5\n            final_x = x + x_2\n            return final_x\n        else:\n            return self.output(x)","925902ce":"class BERT(nn.Module):\n    def __init__(self, hidden_size ,output_size,  p=.5, use_features=False, features_size=None):\n        super(BERT, self).__init__()\n        self.use_features = use_features\n        \n        self.bert = BertModel.from_pretrained(\"bert-base-uncased\") # pretrained BERT model\n        # Note: 'bert-base-uncased' is the small size bert model | 'bert-base-cased' is the large size bert model.\n        # I will use the small size bert because it will take too long to train the large one.\n\n        self.Dropout = nn.Dropout(p)\n        self.output = nn.Linear(768, output_size) # '768' is the hidden size of bert layer \n        \n        # freeze bert model\n        for parm in self.bert.parameters():\n            parm.requires_grad = False\n\n        if self.use_features:\n            self.f_Linear = nn.Linear(features_size, hidden_size)\n            self.f_output = nn.Linear(hidden_size, output_size)\n\n\n    def forward(self, ids, mask, features=None):\n        \n        # feeding the inputs to BERT model to obtain contextualized representations\n        context_reps = self.bert(ids, attention_mask = mask)[0] # BERT context repesentaion 'last_hidden_state' values\n        cls_token = context_reps[:, 0, :]  # extract CLS values,\n        # CLS stands for classification and its there to represent sentence-level classification.\n        # for more details:\n        # https:\/\/datascience.stackexchange.com\/questions\/66207\/what-is-purpose-of-the-cls-token-and-why-is-its-encoding-output-important\n        \n        if self.use_features:\n            x = self.output(cls_token) * 0.5\n            \n            x_2 = nn.functional.leaky_relu(self.f_Linear(features), .001)\n            x_2 = self.Dropout(x_2)\n            \n            x_2 = self.f_output(x_2) * 0.5\n\n            final_x = x + x_2\n            return final_x\n        else:\n            return self.output(cls_token)","9d39b935":"# --- Training loop --- #\n\ndef train_func(model, optimizer, criterion, iterator, using_features=False):\n    # create lists to append the scores of each metric from every batch, so we can take the mean\/average of each metric as the final scores\n    accuracy_list = []\n    f1_list = []\n    loss_list = []\n    \n    model.train() # train mode (has specific impact on some layers,e.g. Dropout layer)\n    \n    for batch in iterator: # loop over the whole data batch-by-batch\n        optimizer.zero_grad()\n        \n        # PREPARE DATA\n        if type(model).__name__ == 'BERT': # type(model).__name__ will tell us the name of the model class (e.g. BERT, CNN, BiLSTM)\n            # extract BERT data if the current model is BERT\n            ids, mask, features, label= batch[0], batch[1], batch[2], batch[3]\n        \n        else: \n            # extract non-BERT data if the current model is not BERT\n            text = batch['sequence']\n            features = batch['features']\n            label = batch['target']\n\n        # FEED MODEL\n        if type(model).__name__ == 'BERT':  # for BERT model\n            if using_features: # if we want to feed 'features' to the model:\n                output = model(ids, mask, features)\n            else: # if we don't want to feed 'features' to the model:\n                output = model(ids, mask)\n\n            \n        elif type(model).__name__ == 'CNN_Model':  # for CNN training\n            # CNN is using filter_sizes(e.g. 3, 4, 5), every conv layer has sepcific kernel size\n            # a conv layer kernel size is (kernel_size, embedding_dim), so we must pad short texts to the maximum size of filter_sizes list\n            # we will do this using the function we created before (in CNN archeticture).\n\n            if using_features:\n                output = model(cnn_padding_to_match_filtersize(text), features)\n            else:\n                output = model(cnn_padding_to_match_filtersize(text))\n\n        else:  # for other models\n            if using_features:\n                output = model(text, features)\n            else:\n                output = model(text)\n                \n        loss = criterion(output, label) # calcualte loss function\n        loss.backward() # backprobgate loss function\n\n        # Gradient clipping is a technique that tackles exploding gradients. \n        # The idea of gradient clipping is very simple: If the gradient gets too large, we rescale it to keep it small.\n        nn.utils.clip_grad_norm_(model.parameters(), 1) # optinal \n        \n        # calculate optimizer\n        optimizer.step()\n\n        # take the index class for the maximum prediction probability (e.g. [.2, .3, .5], output.argmax(1) will be index 2 is max arg)\n        output = output.argmax(1) # 1 will be the 1-axis\/dim\n        \n        # detach: to exclude elements of computation from gradient calculation(backward)\n        # cpu: convert the data from GPU cache to CPU cache\n        # detach: convert data from torch tensor to np array so we can use any metric(it's build with numpy API so it doesn't except torch API) \n        output = output.detach().cpu().numpy() \n        label = label.detach().cpu().numpy()\n\n        # append metrices values \n        loss_list.append(loss.detach().cpu().numpy().item()) # item will give you just the loss value\n        accuracy_list.append(accuracy_score(label, output)) # calculate accuracy_score\n        f1_list.append(f1_score(label, output, average='macro')) # calculate f1_score\n\n    # calculate the mean value of the whole epoch\n    loss = pp.mean(loss_list).round(4) # round(4) will give us the last 4 numbers after '.' (e.g. 2.9567)\n    f1 = pp.round(pp.mean(f1_list) * 100, 1) # multiply the value with 100 so it will look like percentage (e.g. 0.99 * 100 = 99)\n    accuracy = pp.round(pp.mean(accuracy_list) * 100, 1)  # multiply the value with 100 so it will look like percentage (e.g. 0.99 * 100 = 99)\n\n    return loss, f1, accuracy","e515f066":"# --- Validating loop --- #\n\ndef eval_func(model, criterion, iterator, using_features=False):\n    # create lists to append the scores of each metric from every batch, so we can take the mean\/average of each metric as the final scores\n    accuracy_list = []\n    f1_list = []\n    loss_list = []\n\n    model.eval() # train mode (has specific impact on some layers,e.g. Dropout layer)\n    \n    for batch in iterator:# loop over the whole data batch-by-batch\n        # PREPARE DATA\n        if type(model).__name__ == 'BERT': # type(model).__name__ will tell us the name of the model class (e.g. BERT, CNN, BiLSTM)\n            # extract BERT data if the current model is BERT\n            ids, mask, features, label= batch[0], batch[1], batch[2], batch[3]\n        \n        else: \n            # extract non-BERT data if the current model is not BERT\n            text = batch['sequence']\n            features = batch['features']\n            label = batch['target']\n            \n        # FEED MODEL\n        \n        with torch.no_grad(): # don't  calculate  gradients\n            \n            if type(model).__name__ == 'BERT':  # for BERT model\n                if using_features: # if we want to feed 'features' to the model:\n                    output = model(ids, mask, features)\n                else: # if we don't want to feed 'features' to the model:\n                    output = model(ids, mask)\n\n            elif type(model).__name__ == 'CNN_Model':  # for CNN model\n                # CNN is using filter_sizes(e.g. 3, 4, 5), every conv layer has sepcific kernel size\n                # conv layer kernel size is of shape(kernel_size, embedding_dim), so we must pad short texts to the maximum size filter_sizes\n                # we will do this using the function we created before (in CNN archeticture).\n                if using_features:\n                    output = model(cnn_padding_to_match_filtersize(text), features)\n                else:\n                    output = model(cnn_padding_to_match_filtersize(text))\n\n            else:  # for other models\n                if using_features:\n                    output = model(text, features)\n                else:\n                    output = model(text)\n\n            loss = criterion(output, label) # calcualte loss function\n\n        # take the index class for the maximum prediction probability (e.g. [.2, .3, .5], output.argmax(1) will be index 2 is max arg)\n        output = output.argmax(1) # 1 will be the 1-axis\/dim\n        \n        # detach: to exclude elements of computation from gradient calculation(backward)\n        # cpu: convert the data from GPU cache to CPU cache\n        # detach: convert data from torch tensor to np array so we can use any metric(it's build with numpy API so it doesn't except torch API) \n        output = output.detach().cpu().numpy() \n        label = label.detach().cpu().numpy()\n\n        # append metrices values \n        loss_list.append(loss.detach().cpu().numpy().item()) # item will give you just the loss value\n        accuracy_list.append(accuracy_score(label, output)) # calculate accuracy_score\n        f1_list.append(f1_score(label, output, average='macro')) # calculate f1_score\n\n    # calculate the mean value of the whole epoch\n    loss = pp.mean(loss_list).round(4) # round(4) will give us the last 4 numbers after '.' (e.g. 2.9567)\n    f1 = pp.round(pp.mean(f1_list) * 100, 1) # multiply the value with 100 so it will look like percentage (e.g. 0.99 * 100 = 99)\n    accuracy = pp.round(pp.mean(accuracy_list) * 100, 1)  # multiply the value with 100 so it will look like percentage (e.g. 0.99 * 100 = 99)\n\n    return loss, f1, accuracy","e001bb7d":"# --- pred loop --- #\ndef pred(model, iterator, using_features=False):\n    # create lists to append the scores of each metric from every batch, so we can take the mean\/average of each metric as the final scores\n    accuracy_list = []\n    f1_list = []\n    loss_list = []\n\n    model.eval() # train mode (has specific impact on some layers,e.g. Dropout layer)\n    \n    for batch in iterator:# loop over the whole data batch-by-batch\n        # PREPARE DATA\n        if type(model).__name__ == 'BERT': # type(model).__name__ will tell us the name of the model class (e.g. BERT, CNN, BiLSTM)\n            # extract BERT data if the current model is BERT\n            ids, mask, features, label= batch[0], batch[1], batch[2], batch[3]\n        \n        else: \n            # extract non-BERT data if the current model is not BERT\n            text = batch['sequence']\n            features = batch['features']\n            label = batch['target']\n\n        # FEED DATA\n        with torch.no_grad(): # don't calculate gradients\n            \n            if type(model).__name__ == 'BERT':  # for BERT model\n                if using_features: # if we want to feed 'features' to the model:\n                    output = model(ids, mask, features)\n                else: # if we don't want to feed 'features' to the model:\n                    output = model(ids, mask)\n\n\n            elif type(model).__name__ == 'CNN_Model':  # for CNN model\n                # CNN is using filter_sizes(e.g. 3, 4, 5), every conv layer has sepcific kernel size\n                # conv layer kernel size is of shape(kernel_size, embedding_dim), so we must pad short texts to the maximum size filter_sizes\n                # we will do this using the function we created before (in CNN archeticture).\n                if using_features:\n                    output = model(cnn_padding_to_match_filtersize(text), features)\n                else:\n                    output = model(cnn_padding_to_match_filtersize(text))\n\n            else:  # for other models\n                if using_features:\n                    output = model(text, features)\n                else:\n                    output = model(text)\n\n            loss = loss_function(output, label) # calcualte loss function\n        \n        # take the index class for the maximum prediction probability (e.g. [.2, .3, .5], output.argmax(1) will be index 2 is max arg)\n        output = output.argmax(1) # 1 will be the 1-axis\/dim\n        \n        # detach: to exclude elements of computation from gradient calculation(backward)\n        # cpu: convert the data from GPU cache to CPU cache\n        # detach: convert data from torch tensor to np array so we can use any metric(it's build with numpy API so it doesn't except torch API) \n        output = output.detach().cpu().numpy() \n        label = label.detach().cpu().numpy()\n\n        # append metrices values \n        loss_list.append(loss.detach().cpu().numpy().item()) # item will give you just the loss value\n        accuracy_list.append(accuracy_score(label, output)) # calculate accuracy_score\n        f1_list.append(f1_score(label, output, average='macro')) # calculate f1_score\n\n    # calculate the mean value of the whole epoch\n    loss = pp.mean(loss_list).round(4) # round(4) will give us the last 4 numbers after '.' (e.g. 2.9567)\n    f1 = pp.round(pp.mean(f1_list) * 100, 1) # multiply the value with 100 so it will look like percentage (e.g. 0.99 * 100 = 99)\n    accuracy = pp.round(pp.mean(accuracy_list) * 100, 1)  # multiply the value with 100 so it will look like percentage (e.g. 0.99 * 100 = 99)\n    name = type(model).__name__ # the name of the Model\n\n    return loss, f1, accuracy, name","ef63eadf":"# --- Running loop --- #\n\ndef training_model(model, optimizer, criterion, train_iterator, eval_iterator, epochs=1000, using_features=False):\n    print('\\nTraining Started...\\n')\n    # for early stopping\n    stop = 0  # increase this number if the model didn't become more accurate compared to the last best epoch\n    min_eval_acc = 0. # assign this number as the best evaluation accuracy score\n\n    for epoch in range(epochs): # Epochs Loop \n        \n        start_time = time.time()  # time after training started\n\n        loss, f1, acc = train_func(model, optimizer, criterion, train_iterator, using_features)  # train model\n        eval_loss, eval_f1, eval_acc = eval_func(model, criterion, eval_iterator, using_features)  # evaluate model\n\n        secs = int(time.time() - start_time)  # time after training finished\n        mins = secs \/ 60  # time in minutes\n\n        # save current-epoch model file, so we can coninue training later (fine-tuning the paremeters of last epoch). and delete\n        # the previous current-epoch model file, to avoid making lots of files(you can delete this section if you want)\n        \"\"\"\n        for f in os.listdir(): # loop over all files in directory\n            if 'current epoch' in f: # if a file contains 'current epoch', remove it\n                os.remove(f)\n        # create new file\n        torch.save(model.state_dict(),f'current epoch{epoch}_val_loss_{eval_loss}_val_acc{eval_acc}_val_f1_{eval_f1}.pt')\n        \"\"\"\n\n        # best model scores\n        if eval_acc > min_eval_acc: # current accuracy greater then the best accuracy \n            min_eval_acc = eval_acc # assign 'min_eval_acc' as the best accuracy\n            #training scores\n            best_loss = loss\n            best_acc = acc\n            best_f1 = f1\n            \n            #evaluation scores\n            best_eval_loss = eval_loss\n            best_eval_acc = eval_acc\n            best_eval_f1 = eval_f1\n\n            best_model = model.state_dict() # best model parameters\n            best_epoch = epoch + 1  # '+ 1' because 'epoch' starts from 0\n            stop = 0 # reset 'stop' counter\n            # print the current as the 'Best Epoch score'\n            print(\n                f'BEST Epoch({best_epoch} | {mins:.2f}\/min) Train(loss: {best_loss} | acc: {best_acc} | f1-score: {best_f1}) & Eval(loss: {best_eval_loss} | acc: {best_eval_acc} | f1-score: {best_eval_f1})\\n')\n\n        # if current accuracy wasn't greater then the best accuracy add 1 to 'stop'\n        else:\n            stop += 1\n            # print the current epoch as the 'Normal Epoch score'\n            print(\n                f'Epoch({epoch + 1} | {mins:.2f}\/min) Train(loss: {loss} | acc: {acc} | f1-score: {f1}) & Eval(loss: {eval_loss} | acc: {eval_acc} | f1-score: {eval_f1})\\n')\n        \n        # if 'stop' reached 5 it will stop training\n        if stop == 5:\n            print('EARLY STOPPING!')\n            # print the best epoch recorded during the whole training\n            print(\n                f'BEST EPOCH({best_epoch} | {mins:.2f}\/min) Train(loss:{best_loss} | acc:{best_acc} | f1-score:{best_f1}) & Eval(loss: {best_eval_loss} | acc: {best_eval_acc} | f1-score: {best_eval_f1})\\n')\n            # save best model\n            # torch.save(best_model, f'epoch_{best_epoch}_val_loss_{best_eval_loss}_val_acc_{best_eval_acc}_val_f1_{best_eval_f1}.pt')\n            break","32b84203":"\"\"\"vocab_size = len(word2index)\nembedding_dim = 100\noutput_size = len(unique_classes)\n\nhidden_size = 64\nn_filters = 64\nn_layers = 2\nfilter_sizes = [2, 3, 5, 7]\nprint('Size Of Vocab: ', vocab_size)\n\nmodel1 = SelfAttention(vocab_size, embedding_dim, hidden_size, output_size, n_layers)\nmodel2 = BiLSTM_2DMAX_POOLING_2D_CNN(vocab_size, embedding_dim, hidden_size, n_filters, n_layers, output_size)\nmodel3 = RCNN_Model(vocab_size, embedding_dim, hidden_size, n_layers, output_size)\nmodel4 = CNN_Model(vocab_size, embedding_dim, n_filters, filter_sizes, output_size)\nmodel5 = BiLSTM_concat_hiddens_Model(vocab_size, embedding_dim, hidden_size, n_layers, output_size)\nmodel6 = BiLSTM_Model(vocab_size, embedding_dim, hidden_size, n_layers, output_size)\nmodel7 = LSTM_Model(vocab_size, embedding_dim, hidden_size, n_layers, output_size)\nmodel8 = BERT(hidden_size, output_size)\n\nmodels = [model1, model2, model3, model4, model5, model6, model7, model8]\"\"\"\n#","09d268c9":"\"\"\"def count_parameters(model):\n    # to know how many parametres a 'trained' model has  \n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# set the training loss\nloss_function = nn.CrossEntropyLoss().to(device)\n\nfor clf in models:\n    print('----------------------')\n    print('----------------------')\n    print('model Name: ', type(clf).__name__) # print Model Name\n    print(f'{type(clf).__name__} has {count_parameters(clf):,} trainable parameters') # print Model Total Parameters \n    clf = clf.to(device) # convert model to GPU memory for fast training\n    optimizer = optim.AdamW(clf.parameters(), lr=.001) # assign AdamW as optimizer\n    \n    # use non-BERT data if model name is not 'BERT'\n    if type(clf).__name__ != 'BERT':\n        training_model(clf, optimizer, loss_function, train_loader, val_loader, epochs=100, using_features=False)\n    else: \n        # use BERT data if model name is 'BERT'\n        training_model(clf, optimizer, loss_function, bert_train_loader, bert_val_loader, epochs=4, using_features=False)\"\"\"\n#","7fbca317":"\"\"\"\n----------------------\n----------------------\nmodel Name:  SelfAttention\nSelfAttention has 3,091,610 trainable parameters\n\nTraining Started...\n\nBEST Epoch(1 | 0.97\/min) Train(loss: 0.9734 | acc: 69.3 | f1-score: 56.8) & Eval(loss: 1.0047 | acc: 71.3 | f1-score: 61.4)\n\nBEST Epoch(2 | 0.98\/min) Train(loss: 0.6599 | acc: 79.5 | f1-score: 70.2) & Eval(loss: 0.8866 | acc: 74.9 | f1-score: 64.7)\n\nBEST Epoch(3 | 0.98\/min) Train(loss: 0.5369 | acc: 83.4 | f1-score: 75.4) & Eval(loss: 0.8736 | acc: 76.8 | f1-score: 67.3)\n\nBEST Epoch(4 | 0.98\/min) Train(loss: 0.4462 | acc: 86.2 | f1-score: 79.1) & Eval(loss: 0.8971 | acc: 76.9 | f1-score: 67.4)\n\nEpoch(5 | 0.97\/min) Train(loss: 0.3687 | acc: 88.7 | f1-score: 82.8) & Eval(loss: 1.0104 | acc: 76.2 | f1-score: 67.0)\n\nEpoch(6 | 0.97\/min) Train(loss: 0.3034 | acc: 90.7 | f1-score: 85.7) & Eval(loss: 1.1462 | acc: 76.3 | f1-score: 67.6)\n\nEpoch(7 | 0.98\/min) Train(loss: 0.2477 | acc: 92.5 | f1-score: 88.4) & Eval(loss: 1.2817 | acc: 75.4 | f1-score: 66.2)\n\nEpoch(8 | 0.97\/min) Train(loss: 0.2065 | acc: 93.7 | f1-score: 90.0) & Eval(loss: 1.3617 | acc: 75.3 | f1-score: 66.5)\n\nEpoch(9 | 0.97\/min) Train(loss: 0.1762 | acc: 94.7 | f1-score: 91.7) & Eval(loss: 1.5247 | acc: 75.9 | f1-score: 66.8)\n\nEARLY STOPPING!\nBEST EPOCH(4 | 0.97\/min) Train(loss:0.4462 | acc:86.2 | f1-score:79.1) & Eval(loss: 0.8971 | acc: 76.9 | f1-score: 67.4)\n\n----------------------\n----------------------\nmodel Name:  BiLSTM_2DMAX_POOLING_2D_CNN\nBiLSTM_2DMAX_POOLING_2D_CNN has 2,682,330 trainable parameters\n\nTraining Started...\n\nBEST Epoch(1 | 1.00\/min) Train(loss: 1.0946 | acc: 65.2 | f1-score: 49.8) & Eval(loss: 0.9924 | acc: 69.2 | f1-score: 58.4)\n\nBEST Epoch(2 | 0.98\/min) Train(loss: 0.6957 | acc: 78.7 | f1-score: 68.7) & Eval(loss: 0.8895 | acc: 73.0 | f1-score: 62.5)\n\nBEST Epoch(3 | 1.00\/min) Train(loss: 0.5427 | acc: 83.5 | f1-score: 75.5) & Eval(loss: 0.8588 | acc: 75.4 | f1-score: 65.4)\n\nEpoch(4 | 0.98\/min) Train(loss: 0.4297 | acc: 87.1 | f1-score: 80.4) & Eval(loss: 0.8894 | acc: 75.4 | f1-score: 65.5)\n\nBEST Epoch(5 | 0.98\/min) Train(loss: 0.336 | acc: 90.0 | f1-score: 84.7) & Eval(loss: 0.9742 | acc: 75.8 | f1-score: 65.8)\n\nEpoch(6 | 0.98\/min) Train(loss: 0.2606 | acc: 92.2 | f1-score: 87.9) & Eval(loss: 1.0841 | acc: 75.4 | f1-score: 65.7)\n\nEpoch(7 | 0.97\/min) Train(loss: 0.2018 | acc: 94.1 | f1-score: 90.6) & Eval(loss: 1.2846 | acc: 75.2 | f1-score: 64.9)\n\nEpoch(8 | 0.97\/min) Train(loss: 0.1586 | acc: 95.3 | f1-score: 92.5) & Eval(loss: 1.4582 | acc: 74.5 | f1-score: 64.8)\n\nEpoch(9 | 1.02\/min) Train(loss: 0.1282 | acc: 96.2 | f1-score: 93.9) & Eval(loss: 1.4983 | acc: 75.0 | f1-score: 65.1)\n\nEpoch(10 | 0.97\/min) Train(loss: 0.1059 | acc: 96.8 | f1-score: 94.7) & Eval(loss: 1.7944 | acc: 74.6 | f1-score: 64.9)\n\nEARLY STOPPING!\nBEST EPOCH(5 | 0.97\/min) Train(loss:0.336 | acc:90.0 | f1-score:84.7) & Eval(loss: 0.9742 | acc: 75.8 | f1-score: 65.8)\n\n----------------------\n----------------------\nmodel Name:  RCNN_Model\nRCNN_Model has 2,608,822 trainable parameters\n\nTraining Started...\n\nBEST Epoch(1 | 0.93\/min) Train(loss: 0.9013 | acc: 71.5 | f1-score: 59.3) & Eval(loss: 0.8757 | acc: 72.5 | f1-score: 61.9)\n\nBEST Epoch(2 | 0.93\/min) Train(loss: 0.6062 | acc: 81.0 | f1-score: 72.0) & Eval(loss: 0.7992 | acc: 75.7 | f1-score: 65.1)\n\nBEST Epoch(3 | 0.93\/min) Train(loss: 0.4816 | acc: 84.9 | f1-score: 77.2) & Eval(loss: 0.8025 | acc: 76.8 | f1-score: 66.5)\n\nBEST Epoch(4 | 0.93\/min) Train(loss: 0.3843 | acc: 88.0 | f1-score: 81.6) & Eval(loss: 0.8367 | acc: 77.1 | f1-score: 67.2)\n\nEpoch(5 | 0.93\/min) Train(loss: 0.3016 | acc: 90.6 | f1-score: 85.4) & Eval(loss: 0.9089 | acc: 76.5 | f1-score: 66.8)\n\nEpoch(6 | 0.93\/min) Train(loss: 0.2323 | acc: 92.9 | f1-score: 88.8) & Eval(loss: 0.9794 | acc: 76.8 | f1-score: 67.0)\n\nEpoch(7 | 0.93\/min) Train(loss: 0.1785 | acc: 94.6 | f1-score: 91.4) & Eval(loss: 1.0859 | acc: 77.1 | f1-score: 67.9)\n\nEpoch(8 | 0.92\/min) Train(loss: 0.137 | acc: 95.8 | f1-score: 93.3) & Eval(loss: 1.1828 | acc: 76.8 | f1-score: 67.1)\n\nEpoch(9 | 0.92\/min) Train(loss: 0.1074 | acc: 96.7 | f1-score: 94.7) & Eval(loss: 1.2544 | acc: 77.0 | f1-score: 67.5)\n\nEARLY STOPPING!\nBEST EPOCH(4 | 0.92\/min) Train(loss:0.3843 | acc:88.0 | f1-score:81.6) & Eval(loss: 0.8367 | acc: 77.1 | f1-score: 67.2)\n\n----------------------\n----------------------\nmodel Name:  CNN_Model\nCNN_Model has 2,481,626 trainable parameters\n\nTraining Started...\n\nBEST Epoch(1 | 0.62\/min) Train(loss: 1.2512 | acc: 59.8 | f1-score: 45.1) & Eval(loss: 0.9857 | acc: 68.6 | f1-score: 57.7)\n\nBEST Epoch(2 | 0.62\/min) Train(loss: 0.8348 | acc: 74.4 | f1-score: 63.3) & Eval(loss: 0.8364 | acc: 74.4 | f1-score: 64.8)\n\nBEST Epoch(3 | 0.60\/min) Train(loss: 0.7137 | acc: 78.3 | f1-score: 68.5) & Eval(loss: 0.784 | acc: 76.5 | f1-score: 67.0)\n\nBEST Epoch(4 | 0.62\/min) Train(loss: 0.6352 | acc: 80.8 | f1-score: 71.7) & Eval(loss: 0.7755 | acc: 76.7 | f1-score: 67.5)\n\nBEST Epoch(5 | 0.60\/min) Train(loss: 0.5718 | acc: 82.6 | f1-score: 74.3) & Eval(loss: 0.7788 | acc: 77.4 | f1-score: 68.0)\n\nEpoch(6 | 0.60\/min) Train(loss: 0.5226 | acc: 84.2 | f1-score: 76.3) & Eval(loss: 0.7825 | acc: 77.1 | f1-score: 68.0)\n\nEpoch(7 | 0.60\/min) Train(loss: 0.4718 | acc: 85.7 | f1-score: 78.4) & Eval(loss: 0.8004 | acc: 77.3 | f1-score: 68.3)\n\nEpoch(8 | 0.60\/min) Train(loss: 0.4306 | acc: 86.9 | f1-score: 80.3) & Eval(loss: 0.8334 | acc: 77.3 | f1-score: 68.6)\n\nEpoch(9 | 0.62\/min) Train(loss: 0.3906 | acc: 88.1 | f1-score: 82.1) & Eval(loss: 0.8562 | acc: 77.1 | f1-score: 67.5)\n\nEpoch(10 | 0.62\/min) Train(loss: 0.3564 | acc: 89.0 | f1-score: 83.4) & Eval(loss: 0.9131 | acc: 76.9 | f1-score: 67.8)\n\nEARLY STOPPING!\nBEST EPOCH(5 | 0.62\/min) Train(loss:0.5718 | acc:82.6 | f1-score:74.3) & Eval(loss: 0.7788 | acc: 77.4 | f1-score: 68.0)\n\n----------------------\n----------------------\nmodel Name:  BiLSTM_concat_hiddens_Model\nBiLSTM_concat_hiddens_Model has 2,563,226 trainable parameters\n\nTraining Started...\n\nBEST Epoch(1 | 0.93\/min) Train(loss: 1.1491 | acc: 64.6 | f1-score: 48.6) & Eval(loss: 1.0221 | acc: 69.9 | f1-score: 59.2)\n\nBEST Epoch(2 | 0.95\/min) Train(loss: 0.7753 | acc: 77.6 | f1-score: 67.2) & Eval(loss: 0.8991 | acc: 74.6 | f1-score: 64.7)\n\nBEST Epoch(3 | 0.93\/min) Train(loss: 0.6418 | acc: 81.6 | f1-score: 72.7) & Eval(loss: 0.859 | acc: 76.5 | f1-score: 67.0)\n\nBEST Epoch(4 | 0.93\/min) Train(loss: 0.5473 | acc: 84.6 | f1-score: 76.7) & Eval(loss: 0.8603 | acc: 77.1 | f1-score: 67.9)\n\nEpoch(5 | 0.93\/min) Train(loss: 0.4714 | acc: 86.8 | f1-score: 79.8) & Eval(loss: 0.9318 | acc: 77.1 | f1-score: 67.6)\n\nBEST Epoch(6 | 0.92\/min) Train(loss: 0.4101 | acc: 88.7 | f1-score: 82.5) & Eval(loss: 0.9352 | acc: 77.6 | f1-score: 68.2)\n\nEpoch(7 | 0.93\/min) Train(loss: 0.3577 | acc: 90.2 | f1-score: 84.7) & Eval(loss: 1.0469 | acc: 77.6 | f1-score: 68.1)\n\nEpoch(8 | 0.93\/min) Train(loss: 0.3095 | acc: 91.6 | f1-score: 86.8) & Eval(loss: 1.1155 | acc: 77.6 | f1-score: 68.4)\n\nEpoch(9 | 0.92\/min) Train(loss: 0.2705 | acc: 92.7 | f1-score: 88.3) & Eval(loss: 1.1355 | acc: 77.5 | f1-score: 68.7)\n\nEpoch(10 | 0.92\/min) Train(loss: 0.238 | acc: 93.6 | f1-score: 89.8) & Eval(loss: 1.2253 | acc: 77.0 | f1-score: 67.5)\n\nEpoch(11 | 0.92\/min) Train(loss: 0.2123 | acc: 94.3 | f1-score: 90.9) & Eval(loss: 1.2749 | acc: 76.8 | f1-score: 67.4)\n\nEARLY STOPPING!\nBEST EPOCH(6 | 0.92\/min) Train(loss:0.4101 | acc:88.7 | f1-score:82.5) & Eval(loss: 0.9352 | acc: 77.6 | f1-score: 68.2)\n\n----------------------\n----------------------\nmodel Name:  BiLSTM_Model\nBiLSTM_Model has 2,563,226 trainable parameters\n\nTraining Started...\n\nBEST Epoch(1 | 0.93\/min) Train(loss: 1.1674 | acc: 63.7 | f1-score: 47.4) & Eval(loss: 1.0141 | acc: 71.1 | f1-score: 60.3)\n\nBEST Epoch(2 | 0.93\/min) Train(loss: 0.7863 | acc: 77.2 | f1-score: 66.4) & Eval(loss: 0.855 | acc: 75.7 | f1-score: 65.6)\n\nBEST Epoch(3 | 0.92\/min) Train(loss: 0.6485 | acc: 81.5 | f1-score: 72.7) & Eval(loss: 0.8615 | acc: 76.9 | f1-score: 67.3)\n\nBEST Epoch(4 | 0.93\/min) Train(loss: 0.5555 | acc: 84.3 | f1-score: 76.5) & Eval(loss: 0.8889 | acc: 77.1 | f1-score: 67.5)\n\nBEST Epoch(5 | 0.93\/min) Train(loss: 0.4797 | acc: 86.6 | f1-score: 79.7) & Eval(loss: 0.9217 | acc: 77.4 | f1-score: 68.2)\n\nEpoch(6 | 0.92\/min) Train(loss: 0.4153 | acc: 88.5 | f1-score: 82.4) & Eval(loss: 0.9863 | acc: 77.2 | f1-score: 67.9)\n\nEpoch(7 | 0.92\/min) Train(loss: 0.3595 | acc: 90.0 | f1-score: 84.5) & Eval(loss: 1.0619 | acc: 76.9 | f1-score: 67.6)\n\nEpoch(8 | 0.92\/min) Train(loss: 0.3156 | acc: 91.4 | f1-score: 86.6) & Eval(loss: 1.109 | acc: 77.3 | f1-score: 68.2)\n\nEpoch(9 | 0.92\/min) Train(loss: 0.2748 | acc: 92.6 | f1-score: 88.4) & Eval(loss: 1.171 | acc: 77.2 | f1-score: 68.0)\n\nEpoch(10 | 0.90\/min) Train(loss: 0.2412 | acc: 93.4 | f1-score: 89.5) & Eval(loss: 1.2531 | acc: 77.0 | f1-score: 67.9)\n\nEARLY STOPPING!\nBEST EPOCH(5 | 0.90\/min) Train(loss:0.4797 | acc:86.6 | f1-score:79.7) & Eval(loss: 0.9217 | acc: 77.4 | f1-score: 68.2)\n\n----------------------\n----------------------\nmodel Name:  LSTM_Model\nLSTM_Model has 2,450,586 trainable parameters\n\nTraining Started...\n\nBEST Epoch(1 | 0.65\/min) Train(loss: 1.2899 | acc: 59.1 | f1-score: 39.9) & Eval(loss: 1.1019 | acc: 67.3 | f1-score: 55.0)\n\nBEST Epoch(2 | 0.63\/min) Train(loss: 0.8659 | acc: 75.0 | f1-score: 63.0) & Eval(loss: 0.8939 | acc: 73.5 | f1-score: 62.4)\n\nBEST Epoch(3 | 0.63\/min) Train(loss: 0.7144 | acc: 79.7 | f1-score: 69.7) & Eval(loss: 0.8286 | acc: 75.8 | f1-score: 65.8)\n\nBEST Epoch(4 | 0.63\/min) Train(loss: 0.6175 | acc: 82.7 | f1-score: 74.0) & Eval(loss: 0.8102 | acc: 76.8 | f1-score: 67.2)\n\nBEST Epoch(5 | 0.65\/min) Train(loss: 0.5414 | acc: 85.0 | f1-score: 77.3) & Eval(loss: 0.8311 | acc: 77.1 | f1-score: 67.4)\n\nBEST Epoch(6 | 0.63\/min) Train(loss: 0.478 | acc: 86.9 | f1-score: 79.9) & Eval(loss: 0.8425 | acc: 77.5 | f1-score: 67.9)\n\nBEST Epoch(7 | 0.63\/min) Train(loss: 0.4253 | acc: 88.6 | f1-score: 82.3) & Eval(loss: 0.8682 | acc: 77.7 | f1-score: 68.1)\n\nEpoch(8 | 0.63\/min) Train(loss: 0.3807 | acc: 89.9 | f1-score: 84.2) & Eval(loss: 0.9001 | acc: 77.4 | f1-score: 68.4)\n\nEpoch(9 | 0.63\/min) Train(loss: 0.3409 | acc: 91.1 | f1-score: 86.0) & Eval(loss: 0.9429 | acc: 77.2 | f1-score: 67.8)\n\nEpoch(10 | 0.65\/min) Train(loss: 0.3041 | acc: 92.1 | f1-score: 87.4) & Eval(loss: 1.01 | acc: 76.4 | f1-score: 66.9)\n\nEpoch(11 | 0.63\/min) Train(loss: 0.2757 | acc: 92.9 | f1-score: 88.7) & Eval(loss: 1.0288 | acc: 77.4 | f1-score: 68.2)\n\nEpoch(12 | 0.63\/min) Train(loss: 0.2472 | acc: 93.7 | f1-score: 89.8) & Eval(loss: 1.0752 | acc: 76.7 | f1-score: 67.3)\n\nEARLY STOPPING!\nBEST EPOCH(7 | 0.63\/min) Train(loss:0.4253 | acc:88.6 | f1-score:82.3) & Eval(loss: 0.8682 | acc: 77.7 | f1-score: 68.1)\n\n----------------------\n----------------------\nmodel Name:  BERT\nBERT has 7,690 trainable parameters\n\nTraining Started...\n\nBEST Epoch(1 | 14.38\/min) Train(loss: 0.9453 | acc: 69.8 | f1-score: 56.2) & Eval(loss: 1.1201 | acc: 66.2 | f1-score: 52.9)\n\nBEST Epoch(2 | 14.35\/min) Train(loss: 0.8971 | acc: 71.5 | f1-score: 58.6) & Eval(loss: 1.073 | acc: 67.5 | f1-score: 54.5)\n\nBEST Epoch(3 | 14.37\/min) Train(loss: 0.8928 | acc: 71.7 | f1-score: 58.7) & Eval(loss: 1.0575 | acc: 67.7 | f1-score: 54.6)\n\nBEST Epoch(4 | 14.35\/min) Train(loss: 0.8916 | acc: 71.6 | f1-score: 58.8) & Eval(loss: 1.0496 | acc: 67.9 | f1-score: 54.8)\n\"\"\"\n#","c14923b2":"\"\"\"# create lists to append all models details\nloss_list = []\nf1_list = []\naccuracy_list = []\nname_list = []\n\n# use 'pred' function to extract: loss, f1, accuracy and name of every model\nfor clf in models:\n    if type(clf).__name__ != 'BERT': # use non-BERT data if model name is not 'BERT'\n        loss, f1, accuracy, name = pred(clf, test_loader, using_features=False)\n    else: \n        # use BERT data if model name is 'BERT'\n        loss, f1, accuracy, name = pred(clf, bert_test_loader, using_features=False)\n    \n    # append info values\n    loss_list.append(loss)\n    f1_list.append(f1)\n    accuracy_list.append(accuracy)\n    name_list.append(name)\n\n# Save models info to a DataFrame\nrecords_1 = pd.DataFrame({'loss': loss_list, 'f1-score': f1_list, 'accuracy-score': accuracy_list, 'name': name_list})\nrecords_1['training_type'] = 'no_pretrained_embedding'\n\nrecords_1.to_csv('records_1.csv', index=False)\"\"\"\nrecords_1 = pd.read_csv('..\/input\/records\/records_1 (2).csv')\nprint(records_1)","a44435bb":"\"\"\"vocab_size = len(word2index)\nembedding_dim = 100\noutput_size = len(unique_classes)\n\nhidden_size = 64\nn_filters = 64\nn_layers = 2\nfilter_sizes = [2, 3, 5, 7]\nprint('Size Of Vocab: ', vocab_size)\n\nmodel1 = SelfAttention(vocab_size, embedding_dim, hidden_size, output_size, n_layers)\nmodel2 = BiLSTM_2DMAX_POOLING_2D_CNN(vocab_size, embedding_dim, hidden_size, n_filters, n_layers, output_size)\nmodel3 = RCNN_Model(vocab_size, embedding_dim, hidden_size, n_layers, output_size)\nmodel4 = CNN_Model(vocab_size, embedding_dim, n_filters, filter_sizes, output_size)\nmodel5 = BiLSTM_concat_hiddens_Model(vocab_size, embedding_dim, hidden_size, n_layers, output_size)\nmodel6 = BiLSTM_Model(vocab_size, embedding_dim, hidden_size, n_layers, output_size)\nmodel7 = LSTM_Model(vocab_size, embedding_dim, hidden_size, n_layers, output_size)\n\nmodels = [model1, model2, model3, model4, model5, model6, model7]\"\"\"\n#","2ded87ae":"\"\"\"def count_parameters(model):\n    # to know how many parametres a 'trained' model has  \n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# set the training loss\nloss_function = nn.CrossEntropyLoss().to(device)\n\nfor clf in models:\n    print('----------------------')\n    print('----------------------')\n    print('model Name: ', type(clf).__name__) # print Model Name\n    print(f'{type(clf).__name__} has {count_parameters(clf):,} trainable parameters') # print Model Total Parameters \n    clf = clf.to(device) # convert model to GPU memory for fast training\n    optimizer = optim.AdamW(clf.parameters(), lr=.001) # assign AdamW as optimizer\n    \n    # use non-BERT data if model name is not 'BERT'\n    if type(clf).__name__ != 'BERT':\n        \n        # assign the pretraind embedding_matrix to the 'Embedding' layer\n        clf.Embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        # Freeze the 'Embedding' layer weights\n        clf.Embedding.weight.requires_grad = False\n        clf = clf.to(device) # convert model with Embedding to GPU memory for fast training\n\n        training_model(clf, optimizer, loss_function, train_loader, val_loader, epochs=100, using_features=False)\n\n        \n        # Since BERT is freezed, there is no embedding to train and test its prediction, so BERT will not be included in this section.\"\"\"\n#","db0934e3":"\"\"\"# create lists to append all models details\nloss_list = []\nf1_list = []\naccuracy_list = []\nname_list = []\n\n# use 'pred' function to extract: loss, f1, accuracy and name of every model\nfor clf in models:\n    if type(clf).__name__ != 'BERT': # use non-BERT data if model name is not 'BERT'\n        loss, f1, accuracy, name = pred(clf, test_loader, using_features=False)\n        \n        # Since BERT is freezed, there is no embedding to train and test its prediction, so BERT will not be included in this section.\n\n    # append info values\n    loss_list.append(loss)\n    f1_list.append(f1)\n    accuracy_list.append(accuracy)\n    name_list.append(name)\n\n# Save models info to a DataFrame\nrecords_2 = pd.DataFrame({'loss': loss_list, 'f1-score': f1_list, 'accuracy-score': accuracy_list, 'name': name_list})\nrecords_2['training_type'] = 'pretrained_embedding'\nrecords_2.to_csv('records_2.csv', index=False)\"\"\"\n\nrecords_2 = pd.read_csv('..\/input\/records\/records_2 (1).csv')\nprint(records_2)\n","94f1d22b":"\"\"\"vocab_size = len(word2index)\nembedding_dim = 100\noutput_size = len(unique_classes)\n\nhidden_size = 64\nn_filters = 64\nn_layers = 2\nfilter_sizes = [2, 3, 5, 7]\nprint('Size Of Vocab: ', vocab_size)\n\nmodel1 = SelfAttention(vocab_size, embedding_dim, hidden_size, output_size, n_layers)\nmodel2 = BiLSTM_2DMAX_POOLING_2D_CNN(vocab_size, embedding_dim, hidden_size, n_filters, n_layers, output_size)\nmodel3 = RCNN_Model(vocab_size, embedding_dim, hidden_size, n_layers, output_size)\nmodel4 = CNN_Model(vocab_size, embedding_dim, n_filters, filter_sizes, output_size)\nmodel5 = BiLSTM_concat_hiddens_Model(vocab_size, embedding_dim, hidden_size, n_layers, output_size)\nmodel6 = BiLSTM_Model(vocab_size, embedding_dim, hidden_size, n_layers, output_size)\nmodel7 = LSTM_Model(vocab_size, embedding_dim, hidden_size, n_layers, output_size)\n\nmodels = [model1, model2, model3, model4, model5, model6, model7]\"\"\"\n#","5eb4da7a":"\"\"\"def count_parameters(model):\n    # to know how many parametres a 'trained' model has  \n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# set the training loss\nloss_function = nn.CrossEntropyLoss().to(device)\n\nfor clf in models:\n    print('----------------------')\n    print('----------------------')\n    print('model Name: ', type(clf).__name__) # print Model Name\n    print(f'{type(clf).__name__} has {count_parameters(clf):,} trainable parameters') # print Model Total Parameters \n    clf = clf.to(device) # convert model to GPU memory for fast training\n    optimizer = optim.AdamW(clf.parameters(), lr=.001) # assign AdamW as optimizer\n    \n    # use non-BERT data if model name is not 'BERT'\n    if type(clf).__name__ != 'BERT':\n        # assign the pretraind embedding_matrix to the 'Embedding' layer\n        clf.Embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        # Unfreeze\/Tune the 'Embedding' layer weights\n        clf.Embedding.weight.requires_grad = True\n        clf = clf.to(device) # convert model with Embedding to GPU memory for fast training\n\n        training_model(clf, optimizer, loss_function, train_loader, val_loader, epochs=100, using_features=False)\n\n    \n        # Since bert BERT is freezed, there is no embedding to fine-tune\"\"\"\n# ","c20fc6ad":"\"\"\"\n# create lists to append all models details\nloss_list = []\nf1_list = []\naccuracy_list = []\nname_list = []\n\n# use 'pred' function to extract: loss, f1, accuracy and name of every model\nfor clf in models:\n\n    if type(clf).__name__ != 'BERT': # use non-BERT data if model name is not 'BERT'\n        loss, f1, accuracy, name = pred(clf, test_loader, using_features=False)\n\n    # Since bert BERT is freezed, there is no embedding to fine-tune and test its prediction\n\n    # append info values\n    loss_list.append(loss)\n    f1_list.append(f1)\n    accuracy_list.append(accuracy)\n    name_list.append(name)\n\n# Save models info to a DataFrame\nrecords_3 = pd.DataFrame({'loss': loss_list, 'f1-score': f1_list, 'accuracy-score': accuracy_list, 'name': name_list})\nrecords_3['training_type'] = 'pretrained_embedding_tuned'\n\nrecords_3.to_csv('records_3.csv', index=False)\n\"\"\"\nrecords_3 = pd.read_csv('..\/input\/records\/records_3.csv')\nprint(records_3)\n","495866a4":"\"\"\"vocab_size = len(word2index)\nembedding_dim = 100\noutput_size = len(unique_classes)\n\nhidden_size = 64\nn_filters = 64\nn_layers = 2\nfilter_sizes = [2, 3, 5, 7]\nfeatures_size = len(features_cols)\nprint('Size Of Vocab: ', vocab_size)\n\nmodel1 = BERT(hidden_size, output_size, features_size=features_size, use_features=True)\nmodel2 = SelfAttention(vocab_size, embedding_dim, hidden_size, output_size, n_layers, features_size=features_size, use_features=True)\nmodel3 = BiLSTM_2DMAX_POOLING_2D_CNN(vocab_size, embedding_dim, hidden_size, n_filters, n_layers, output_size, features_size=features_size, use_features=True)\nmodel4 = RCNN_Model(vocab_size, embedding_dim, hidden_size, n_layers, output_size, features_size=features_size, use_features=True)\nmodel5 = CNN_Model(vocab_size, embedding_dim, n_filters, filter_sizes, output_size, features_size=features_size, use_features=True)\nmodel6 = BiLSTM_concat_hiddens_Model(vocab_size, embedding_dim, hidden_size, n_layers, output_size, features_size=features_size, use_features=True)\nmodel7 = BiLSTM_Model(vocab_size, embedding_dim, hidden_size, n_layers, output_size, features_size=features_size, use_features=True)\nmodel8 = LSTM_Model(vocab_size, embedding_dim, hidden_size, n_layers, output_size, features_size=features_size, use_features=True)\n\nmodels = [model1, model2, model3, model4, model5, model6, model7, model8]\"\"\"","5df3f4fc":"\"\"\"def count_parameters(model):\n    # to know how many parametres a 'trained' model has  \n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# set the training loss\nloss_function = nn.CrossEntropyLoss().to(device)\n\nfor clf in models:\n    print('----------------------')\n    print('----------------------')\n    print('model Name: ', type(clf).__name__) # print Model Name\n    print(f'{type(clf).__name__} has {count_parameters(clf):,} trainable parameters') # print Model Total Parameters \n    clf = clf.to(device) # convert model to GPU memory for fast training\n    optimizer = optim.AdamW(clf.parameters(), lr=.001) # assign AdamW as optimizer\n    \n    # use non-BERT data if model name is not 'BERT'\n    if type(clf).__name__ != 'BERT':\n        # assign the pretraind embedding_matrix to the 'Embedding' layer\n        clf.Embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        # Unfreeze\/Tune the 'Embedding' layer weights\n        clf.Embedding.weight.requires_grad = True\n        clf = clf.to(device) # convert model with Embedding to GPU memory for fast training\n\n        training_model(clf, optimizer, loss_function, train_loader, val_loader, epochs=100, using_features=True) # set 'using_features' to True\n\n    else: \n        # use BERT data if model name is 'BERT'\n        training_model(clf, optimizer, loss_function, bert_train_loader, bert_val_loader, epochs=4, using_features=True)  # set 'using_features' to True\"\"\"\n#","103b59b0":"\"\"\"----------------------\n----------------------\nmodel Name:  BERT\nBERT has 11,092 trainable parameters\n\nTraining Started...\n\nBEST Epoch(1 | 14.43\/min) Train(loss: 0.9557 | acc: 69.5 | f1-score: 55.4) & Eval(loss: 1.0008 | acc: 68.4 | f1-score: 54.7)\n\nBEST Epoch(2 | 14.42\/min) Train(loss: 0.8667 | acc: 72.4 | f1-score: 59.7) & Eval(loss: 1.0008 | acc: 68.9 | f1-score: 56.0)\n\nBEST Epoch(3 | 14.42\/min) Train(loss: 0.8543 | acc: 72.8 | f1-score: 60.3) & Eval(loss: 0.9337 | acc: 71.0 | f1-score: 58.3)\n\nBEST Epoch(4 | 14.43\/min) Train(loss: 0.8489 | acc: 73.0 | f1-score: 60.5) & Eval(loss: 0.9241 | acc: 71.4 | f1-score: 58.5)\n\n----------------------\n----------------------\nmodel Name:  SelfAttention\nSelfAttention has 3,095,012 trainable parameters\n\nTraining Started...\n\nBEST Epoch(1 | 1.00\/min) Train(loss: 0.7243 | acc: 77.1 | f1-score: 66.7) & Eval(loss: 0.8331 | acc: 74.8 | f1-score: 65.2)\n\nBEST Epoch(2 | 1.02\/min) Train(loss: 0.6136 | acc: 80.5 | f1-score: 71.8) & Eval(loss: 0.7662 | acc: 77.5 | f1-score: 67.7)\n\nBEST Epoch(3 | 1.02\/min) Train(loss: 0.5706 | acc: 81.7 | f1-score: 73.3) & Eval(loss: 0.7306 | acc: 78.5 | f1-score: 69.2)\n\nBEST Epoch(4 | 1.02\/min) Train(loss: 0.5404 | acc: 82.7 | f1-score: 74.7) & Eval(loss: 0.709 | acc: 79.6 | f1-score: 70.6)\n\nEpoch(5 | 1.02\/min) Train(loss: 0.5162 | acc: 83.5 | f1-score: 75.7) & Eval(loss: 0.706 | acc: 79.6 | f1-score: 70.6)\n\nBEST Epoch(6 | 1.02\/min) Train(loss: 0.4953 | acc: 84.1 | f1-score: 76.7) & Eval(loss: 0.6934 | acc: 79.7 | f1-score: 70.8)\n\nEpoch(7 | 1.00\/min) Train(loss: 0.476 | acc: 84.8 | f1-score: 77.5) & Eval(loss: 0.7137 | acc: 79.7 | f1-score: 70.9)\n\nEpoch(8 | 1.02\/min) Train(loss: 0.4585 | acc: 85.4 | f1-score: 78.3) & Eval(loss: 0.7163 | acc: 79.7 | f1-score: 70.7)\n\nEpoch(9 | 1.00\/min) Train(loss: 0.4447 | acc: 85.7 | f1-score: 78.9) & Eval(loss: 0.7212 | acc: 79.5 | f1-score: 70.7)\n\nEpoch(10 | 1.02\/min) Train(loss: 0.4291 | acc: 86.2 | f1-score: 79.5) & Eval(loss: 0.7446 | acc: 79.7 | f1-score: 71.0)\n\nEpoch(11 | 1.00\/min) Train(loss: 0.4166 | acc: 86.6 | f1-score: 80.0) & Eval(loss: 0.7648 | acc: 79.2 | f1-score: 70.2)\n\nEARLY STOPPING!\nBEST EPOCH(6 | 1.00\/min) Train(loss:0.4953 | acc:84.1 | f1-score:76.7) & Eval(loss: 0.6934 | acc: 79.7 | f1-score: 70.8)\n\n----------------------\n----------------------\nmodel Name:  BiLSTM_2DMAX_POOLING_2D_CNN\nBiLSTM_2DMAX_POOLING_2D_CNN has 2,685,732 trainable parameters\n\nTraining Started...\n\nBEST Epoch(1 | 1.03\/min) Train(loss: 0.7646 | acc: 75.9 | f1-score: 64.8) & Eval(loss: 0.7753 | acc: 75.4 | f1-score: 65.4)\n\nBEST Epoch(2 | 1.02\/min) Train(loss: 0.618 | acc: 80.5 | f1-score: 71.8) & Eval(loss: 0.693 | acc: 78.2 | f1-score: 69.0)\n\nBEST Epoch(3 | 1.02\/min) Train(loss: 0.5718 | acc: 81.9 | f1-score: 73.6) & Eval(loss: 0.6745 | acc: 79.1 | f1-score: 70.0)\n\nBEST Epoch(4 | 1.03\/min) Train(loss: 0.5371 | acc: 83.1 | f1-score: 75.2) & Eval(loss: 0.6774 | acc: 79.2 | f1-score: 70.0)\n\nBEST Epoch(5 | 1.02\/min) Train(loss: 0.5081 | acc: 84.1 | f1-score: 76.6) & Eval(loss: 0.6691 | acc: 79.5 | f1-score: 70.5)\n\nEpoch(6 | 1.02\/min) Train(loss: 0.4837 | acc: 84.8 | f1-score: 77.7) & Eval(loss: 0.6813 | acc: 79.0 | f1-score: 69.7)\n\nEpoch(7 | 1.02\/min) Train(loss: 0.4625 | acc: 85.5 | f1-score: 78.5) & Eval(loss: 0.7012 | acc: 78.8 | f1-score: 69.5)\n\nEpoch(8 | 1.02\/min) Train(loss: 0.4416 | acc: 86.1 | f1-score: 79.5) & Eval(loss: 0.7028 | acc: 78.9 | f1-score: 69.6)\n\nEpoch(9 | 1.03\/min) Train(loss: 0.4226 | acc: 86.7 | f1-score: 80.3) & Eval(loss: 0.7428 | acc: 78.3 | f1-score: 69.0)\n\nEpoch(10 | 1.02\/min) Train(loss: 0.4075 | acc: 87.2 | f1-score: 81.0) & Eval(loss: 0.7465 | acc: 78.0 | f1-score: 68.7)\n\nEARLY STOPPING!\nBEST EPOCH(5 | 1.02\/min) Train(loss:0.5081 | acc:84.1 | f1-score:76.6) & Eval(loss: 0.6691 | acc: 79.5 | f1-score: 70.5)\n\n----------------------\n----------------------\nmodel Name:  RCNN_Model\nRCNN_Model has 2,620,916 trainable parameters\n\nTraining Started...\n\nBEST Epoch(1 | 0.97\/min) Train(loss: 0.6805 | acc: 78.2 | f1-score: 67.9) & Eval(loss: 0.7387 | acc: 77.1 | f1-score: 67.6)\n\nBEST Epoch(2 | 0.97\/min) Train(loss: 0.5699 | acc: 81.7 | f1-score: 73.4) & Eval(loss: 0.6862 | acc: 78.7 | f1-score: 69.3)\n\nBEST Epoch(3 | 0.97\/min) Train(loss: 0.5261 | acc: 83.1 | f1-score: 75.2) & Eval(loss: 0.6608 | acc: 79.6 | f1-score: 70.1)\n\nBEST Epoch(4 | 0.97\/min) Train(loss: 0.4936 | acc: 84.1 | f1-score: 76.6) & Eval(loss: 0.6559 | acc: 79.8 | f1-score: 70.6)\n\nBEST Epoch(5 | 0.97\/min) Train(loss: 0.4667 | acc: 85.0 | f1-score: 77.9) & Eval(loss: 0.6441 | acc: 80.0 | f1-score: 70.8)\n\nEpoch(6 | 0.95\/min) Train(loss: 0.4429 | acc: 85.8 | f1-score: 78.9) & Eval(loss: 0.668 | acc: 79.8 | f1-score: 70.7)\n\nEpoch(7 | 0.97\/min) Train(loss: 0.4226 | acc: 86.5 | f1-score: 79.9) & Eval(loss: 0.6769 | acc: 79.9 | f1-score: 70.6)\n\nEpoch(8 | 0.97\/min) Train(loss: 0.4022 | acc: 87.1 | f1-score: 80.7) & Eval(loss: 0.6901 | acc: 79.9 | f1-score: 70.8)\n\nEpoch(9 | 0.97\/min) Train(loss: 0.3861 | acc: 87.6 | f1-score: 81.4) & Eval(loss: 0.7173 | acc: 79.1 | f1-score: 69.7)\n\nEpoch(10 | 0.97\/min) Train(loss: 0.3698 | acc: 88.1 | f1-score: 82.1) & Eval(loss: 0.7255 | acc: 79.0 | f1-score: 69.9)\n\nEARLY STOPPING!\nBEST EPOCH(5 | 0.97\/min) Train(loss:0.4667 | acc:85.0 | f1-score:77.9) & Eval(loss: 0.6441 | acc: 80.0 | f1-score: 70.8)\n\n----------------------\n----------------------\nmodel Name:  CNN_Model\nCNN_Model has 2,485,028 trainable parameters\n\nTraining Started...\n\nBEST Epoch(1 | 0.65\/min) Train(loss: 0.8426 | acc: 73.2 | f1-score: 61.2) & Eval(loss: 0.8563 | acc: 73.9 | f1-score: 63.9)\n\nBEST Epoch(2 | 0.65\/min) Train(loss: 0.7464 | acc: 76.6 | f1-score: 66.7) & Eval(loss: 0.8278 | acc: 75.3 | f1-score: 65.8)\n\nBEST Epoch(3 | 0.65\/min) Train(loss: 0.7195 | acc: 77.4 | f1-score: 67.8) & Eval(loss: 0.7953 | acc: 76.1 | f1-score: 66.3)\n\nEpoch(4 | 0.65\/min) Train(loss: 0.699 | acc: 77.9 | f1-score: 68.5) & Eval(loss: 0.8044 | acc: 75.9 | f1-score: 65.8)\n\nEpoch(5 | 0.65\/min) Train(loss: 0.682 | acc: 78.6 | f1-score: 69.2) & Eval(loss: 0.792 | acc: 75.9 | f1-score: 65.8)\n\nEpoch(6 | 0.65\/min) Train(loss: 0.6703 | acc: 78.9 | f1-score: 69.8) & Eval(loss: 0.7842 | acc: 76.0 | f1-score: 66.3)\n\nBEST Epoch(7 | 0.67\/min) Train(loss: 0.6597 | acc: 79.2 | f1-score: 70.1) & Eval(loss: 0.7809 | acc: 76.2 | f1-score: 66.0)\n\nBEST Epoch(8 | 0.65\/min) Train(loss: 0.6531 | acc: 79.4 | f1-score: 70.3) & Eval(loss: 0.7813 | acc: 76.4 | f1-score: 66.2)\n\nEpoch(9 | 0.65\/min) Train(loss: 0.6396 | acc: 79.8 | f1-score: 70.9) & Eval(loss: 0.7738 | acc: 76.4 | f1-score: 66.3)\n\nEpoch(10 | 0.67\/min) Train(loss: 0.6369 | acc: 79.7 | f1-score: 70.9) & Eval(loss: 0.7888 | acc: 75.9 | f1-score: 65.8)\n\nEpoch(11 | 0.65\/min) Train(loss: 0.6297 | acc: 80.1 | f1-score: 71.2) & Eval(loss: 0.7809 | acc: 75.8 | f1-score: 65.8)\n\nEpoch(12 | 0.67\/min) Train(loss: 0.6227 | acc: 80.3 | f1-score: 71.5) & Eval(loss: 0.7786 | acc: 76.0 | f1-score: 65.9)\n\nEpoch(13 | 0.65\/min) Train(loss: 0.618 | acc: 80.4 | f1-score: 71.6) & Eval(loss: 0.7866 | acc: 75.4 | f1-score: 65.4)\n\nEARLY STOPPING!\nBEST EPOCH(8 | 0.65\/min) Train(loss:0.6531 | acc:79.4 | f1-score:70.3) & Eval(loss: 0.7813 | acc: 76.4 | f1-score: 66.2)\n\n----------------------\n----------------------\nmodel Name:  BiLSTM_concat_hiddens_Model\nBiLSTM_concat_hiddens_Model has 2,566,628 trainable parameters\n\nTraining Started...\n\nBEST Epoch(1 | 0.97\/min) Train(loss: 0.8739 | acc: 73.6 | f1-score: 60.9) & Eval(loss: 0.8475 | acc: 75.1 | f1-score: 64.9)\n\nBEST Epoch(2 | 0.97\/min) Train(loss: 0.6953 | acc: 79.3 | f1-score: 69.9) & Eval(loss: 0.7312 | acc: 78.1 | f1-score: 68.8)\n\nBEST Epoch(3 | 0.97\/min) Train(loss: 0.6382 | acc: 80.9 | f1-score: 72.1) & Eval(loss: 0.7049 | acc: 78.9 | f1-score: 70.2)\n\nBEST Epoch(4 | 0.97\/min) Train(loss: 0.6033 | acc: 81.9 | f1-score: 73.5) & Eval(loss: 0.6866 | acc: 79.6 | f1-score: 71.2)\n\nBEST Epoch(5 | 0.97\/min) Train(loss: 0.5739 | acc: 82.7 | f1-score: 74.6) & Eval(loss: 0.6798 | acc: 79.9 | f1-score: 71.4)\n\nBEST Epoch(6 | 0.95\/min) Train(loss: 0.5496 | acc: 83.5 | f1-score: 75.7) & Eval(loss: 0.6674 | acc: 80.2 | f1-score: 72.1)\n\nEpoch(7 | 0.97\/min) Train(loss: 0.5313 | acc: 84.0 | f1-score: 76.5) & Eval(loss: 0.6974 | acc: 80.1 | f1-score: 71.6)\n\nBEST Epoch(8 | 0.97\/min) Train(loss: 0.5132 | acc: 84.5 | f1-score: 77.0) & Eval(loss: 0.6808 | acc: 80.7 | f1-score: 72.7)\n\nEpoch(9 | 0.97\/min) Train(loss: 0.4977 | acc: 85.0 | f1-score: 77.7) & Eval(loss: 0.7006 | acc: 80.4 | f1-score: 72.2)\n\nEpoch(10 | 0.97\/min) Train(loss: 0.4839 | acc: 85.3 | f1-score: 78.0) & Eval(loss: 0.6988 | acc: 80.6 | f1-score: 72.2)\n\nEpoch(11 | 0.95\/min) Train(loss: 0.4694 | acc: 85.8 | f1-score: 78.9) & Eval(loss: 0.7105 | acc: 80.5 | f1-score: 72.2)\n\nEpoch(12 | 0.97\/min) Train(loss: 0.4589 | acc: 86.1 | f1-score: 79.2) & Eval(loss: 0.7357 | acc: 80.3 | f1-score: 72.4)\n\nEpoch(13 | 0.95\/min) Train(loss: 0.4473 | acc: 86.4 | f1-score: 79.6) & Eval(loss: 0.7249 | acc: 80.7 | f1-score: 72.7)\n\nEARLY STOPPING!\nBEST EPOCH(8 | 0.95\/min) Train(loss:0.5132 | acc:84.5 | f1-score:77.0) & Eval(loss: 0.6808 | acc: 80.7 | f1-score: 72.7)\n\n----------------------\n----------------------\nmodel Name:  BiLSTM_Model\nBiLSTM_Model has 2,566,628 trainable parameters\n\nTraining Started...\n\nBEST Epoch(1 | 0.97\/min) Train(loss: 0.882 | acc: 73.4 | f1-score: 60.4) & Eval(loss: 0.832 | acc: 75.2 | f1-score: 65.0)\n\nBEST Epoch(2 | 0.97\/min) Train(loss: 0.6947 | acc: 79.3 | f1-score: 70.0) & Eval(loss: 0.7344 | acc: 77.8 | f1-score: 68.6)\n\nBEST Epoch(3 | 0.95\/min) Train(loss: 0.6386 | acc: 81.0 | f1-score: 72.1) & Eval(loss: 0.7077 | acc: 78.9 | f1-score: 70.1)\n\nBEST Epoch(4 | 0.97\/min) Train(loss: 0.6016 | acc: 81.9 | f1-score: 73.5) & Eval(loss: 0.7021 | acc: 79.0 | f1-score: 70.1)\n\nBEST Epoch(5 | 0.97\/min) Train(loss: 0.5749 | acc: 82.8 | f1-score: 74.8) & Eval(loss: 0.6923 | acc: 79.8 | f1-score: 71.0)\n\nEpoch(6 | 0.97\/min) Train(loss: 0.5509 | acc: 83.4 | f1-score: 75.7) & Eval(loss: 0.6912 | acc: 79.6 | f1-score: 70.7)\n\nEpoch(7 | 0.95\/min) Train(loss: 0.5281 | acc: 84.0 | f1-score: 76.4) & Eval(loss: 0.7066 | acc: 79.8 | f1-score: 71.2)\n\nBEST Epoch(8 | 0.95\/min) Train(loss: 0.5113 | acc: 84.6 | f1-score: 77.1) & Eval(loss: 0.7125 | acc: 80.2 | f1-score: 71.7)\n\nEpoch(9 | 0.97\/min) Train(loss: 0.494 | acc: 85.0 | f1-score: 77.7) & Eval(loss: 0.6937 | acc: 80.2 | f1-score: 71.5)\n\nBEST Epoch(10 | 0.97\/min) Train(loss: 0.4789 | acc: 85.5 | f1-score: 78.4) & Eval(loss: 0.7176 | acc: 80.6 | f1-score: 72.1)\n\nEpoch(11 | 0.97\/min) Train(loss: 0.4671 | acc: 85.8 | f1-score: 78.8) & Eval(loss: 0.7254 | acc: 80.2 | f1-score: 71.6)\n\nEpoch(12 | 0.97\/min) Train(loss: 0.4547 | acc: 86.3 | f1-score: 79.5) & Eval(loss: 0.7233 | acc: 80.2 | f1-score: 71.7)\n\nEpoch(13 | 0.95\/min) Train(loss: 0.4456 | acc: 86.4 | f1-score: 79.6) & Eval(loss: 0.74 | acc: 79.9 | f1-score: 71.4)\n\nEpoch(14 | 0.97\/min) Train(loss: 0.4337 | acc: 86.8 | f1-score: 80.2) & Eval(loss: 0.7469 | acc: 79.7 | f1-score: 71.1)\n\nEpoch(15 | 0.97\/min) Train(loss: 0.4265 | acc: 87.1 | f1-score: 80.6) & Eval(loss: 0.7435 | acc: 80.1 | f1-score: 71.6)\n\nEARLY STOPPING!\nBEST EPOCH(10 | 0.97\/min) Train(loss:0.4789 | acc:85.5 | f1-score:78.4) & Eval(loss: 0.7176 | acc: 80.6 | f1-score: 72.1)\n\n----------------------\n----------------------\nmodel Name:  LSTM_Model\nLSTM_Model has 2,453,988 trainable parameters\n\nTraining Started...\n\nBEST Epoch(1 | 0.68\/min) Train(loss: 0.9457 | acc: 71.2 | f1-score: 57.1) & Eval(loss: 0.8884 | acc: 73.6 | f1-score: 63.2)\n\nBEST Epoch(2 | 0.72\/min) Train(loss: 0.7458 | acc: 77.8 | f1-score: 67.7) & Eval(loss: 0.8002 | acc: 75.9 | f1-score: 65.9)\n\nBEST Epoch(3 | 0.72\/min) Train(loss: 0.6859 | acc: 79.5 | f1-score: 70.0) & Eval(loss: 0.7267 | acc: 78.2 | f1-score: 69.1)\n\nBEST Epoch(4 | 0.70\/min) Train(loss: 0.6527 | acc: 80.6 | f1-score: 71.5) & Eval(loss: 0.7328 | acc: 78.3 | f1-score: 69.4)\n\nBEST Epoch(5 | 0.70\/min) Train(loss: 0.6263 | acc: 81.3 | f1-score: 72.5) & Eval(loss: 0.7151 | acc: 79.1 | f1-score: 70.3)\n\nEpoch(6 | 0.70\/min) Train(loss: 0.6055 | acc: 81.9 | f1-score: 73.2) & Eval(loss: 0.714 | acc: 78.9 | f1-score: 70.1)\n\nEpoch(7 | 0.68\/min) Train(loss: 0.5906 | acc: 82.2 | f1-score: 73.8) & Eval(loss: 0.6991 | acc: 79.1 | f1-score: 70.5)\n\nBEST Epoch(8 | 0.68\/min) Train(loss: 0.5765 | acc: 82.7 | f1-score: 74.5) & Eval(loss: 0.682 | acc: 79.9 | f1-score: 71.5)\n\nEpoch(9 | 0.70\/min) Train(loss: 0.5656 | acc: 82.9 | f1-score: 74.8) & Eval(loss: 0.6874 | acc: 79.3 | f1-score: 70.1)\n\nEpoch(10 | 0.68\/min) Train(loss: 0.5564 | acc: 83.3 | f1-score: 75.3) & Eval(loss: 0.6817 | acc: 79.9 | f1-score: 71.4)\n\nEpoch(11 | 0.70\/min) Train(loss: 0.5454 | acc: 83.6 | f1-score: 75.9) & Eval(loss: 0.6903 | acc: 79.8 | f1-score: 71.3)\n\nEpoch(12 | 0.70\/min) Train(loss: 0.5398 | acc: 83.7 | f1-score: 75.9) & Eval(loss: 0.6914 | acc: 79.5 | f1-score: 70.8)\n\nEpoch(13 | 0.70\/min) Train(loss: 0.5333 | acc: 84.0 | f1-score: 76.3) & Eval(loss: 0.6993 | acc: 79.5 | f1-score: 71.0)\n\nEARLY STOPPING!\nBEST EPOCH(8 | 0.70\/min) Train(loss:0.5765 | acc:82.7 | f1-score:74.5) & Eval(loss: 0.682 | acc: 79.9 | f1-score: 71.5)\"\"\"\n#","4a4ae348":"\"\"\"# create lists to append all models details\nloss_list = []\nf1_list = []\naccuracy_list = []\nname_list = []\n\n# use 'pred' function to extract: loss, f1, accuracy and name of every model\nfor clf in models:\n    if type(clf).__name__ != 'BERT': # use non-BERT data if model name is not 'BERT'\n        loss, f1, accuracy, name = pred(clf, test_loader, using_features=True) # set 'using_features' to True\n    else: \n        # use BERT data if model name is 'BERT'\n        loss, f1, accuracy, name = pred(clf, bert_test_loader, using_features=True) # set 'using_features' to True\n\n    # append info values\n    loss_list.append(loss)\n    f1_list.append(f1)\n    accuracy_list.append(accuracy)\n    name_list.append(name)\n\n# Save models info to a DataFrame\nrecords_4 = pd.DataFrame({'loss': loss_list, 'f1-score': f1_list, 'accuracy-score': accuracy_list, 'name': name_list})\nrecords_4['training_type'] = 'tuned_embedding & features'\nprint(records_4)\nrecords_4.to_csv('records_4.csv', index=False)\"\"\"\nrecords_4 = pd.read_csv('..\/input\/records-4\/records_4 (1).csv')\nprint(records_4)","691d7330":"# Concatenate All the Records to One DataFrame\nfull_records = pd.concat([records_1,records_2,records_3,records_4], axis=0)\n\nplt.figure(figsize=(15, 10))\nplt.xlim(0, 3)\n\nplt.title('Loss')\nsb.barplot(full_records['loss'], full_records['name'], hue=full_records['training_type'])","bf3948f5":"plt.figure(figsize=(15, 10))\nplt.xlim(0, 110)\n\nplt.title('F1-Score')\nsb.barplot(full_records['f1-score'], full_records['name'], hue =full_records['training_type'])","5b165a43":"plt.figure(figsize=(15, 10))\nplt.xlim(0, 110)\n\nplt.title('Accuracy-Score')\nsb.barplot(full_records['accuracy-score'], full_records['name'], hue =full_records['training_type'])","c7935789":"# 6.7- Self-Attention\n\nSelf-Attention, a mechanism that are used in the modern SOTA models(Transformer-based architectures), which is primarily used in modelling language understanding tasks, eschew the use of recurrence in neural network and instead trust entirely on self-attention mechanisms to draw global dependencies between inputs and outputs. A common approach in many of the methods consists of creating a simple vector representation by using the final hidden state of the RNN or the max\/average pooling from either RNNs hidden states or convolved n-grams. Carrying the semantics along all time steps of a recurrent model is relatively hard and not necessary. The self-attention mechanism is for these sequential models to replace the max pooling or averaging step.\n\nLet's first understand the original paper ([Neural Machine Translation by Jointly Learning to Align and Translate (2015)](https:\/\/arxiv.org\/abs\/1409.0473)) and then we will come back to self-attention.\n\n**Attention :**\n\nThe Attention mechanism was made to fix a problem that Neural Machine Translation models faces. Most of the proposed neural machine translation models belong to a family of encoder\u2013decoders. An encoder neural network reads and encodes a source sentence into a **fixed-length vector**. A decoder then outputs a translation from the encoded vector. The\nwhole encoder\u2013decoder system, which consists of the encoder and the decoder for a language pair, is jointly trained to maximize the probability of a correct translation given a source sentence. A potential issue with this encoder\u2013decoder approach is that a neural network needs to be able to **compress all the necessary information of a source sentence into a fixed-length vector**. This may make it difficult for the neural network to cope with long sentences, especially those that are longer than the sentences in the training corpus.\n\nIn order to address this issue, the paper introduces **aligning**. Each time the proposed model generates a word in a translation, it (soft-)searches for a set of positions in a source sentence where *the most relevant information is concentrated*. The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words.\n\nThe most important distinguishing feature of this approach from the basic encoder\u2013decoder is that it does not attempt to encode a whole input sentence into a single fixed-length vector. Instead, it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation. This frees a neural translation model from having to squash all the information of a source sentence, regardless of its length, into a fixed-length vector. We show this allows a model to cope better with long sentences. The improvement is more apparent with longer sentences.","fcbea416":"apply the function to every text row","7e5f0750":"If we examined the plots we will notice that for every category there is special words (e.g. 'trump' will be the most common word in 'Politics' category)\noccurring frequntly. So we will take these words and keep it as FEATURES for training later in the *Feature engineering* section.\n\n---\n\n# 3.3- The Average, Maximum and Minimum Length of the News Text","9472709c":"# 10.3- Test Models Accuracies","3ac2d091":"Let's see how many vocabs that are not in our vocabs","7c5eb93c":"---\n\n# 6- Model Archtectures\n\nI will explain **Model Archtectures**  Theoretcally(papers) and Practically(every code line).\n\n**Note**: The layers that doesn't belong to the orginal architecture I will mention it by saying: *optinal layer*\n\n**Note**: The layers that I explained and ocurred in more then one architecture, I will not explained anymore (e.g. embedding layer is used in almost every architecture, so I will explain it just one time), except if there is new things to explain. Just to make the explaination clear and avoid itrerating the explainations.\n\n**Note**: The **Model Archtectures** are created in a way that can be used with  an optional choice which is whether to use *feature data* or not. You can delete them because they are not part of the Archtecture (e.g. delete *use_features*)\n\n**Below is the explainaion of each parameter:**\n\n> **vocab_size =** vocab size \n>\n> **features_size =** features size\n>\n> **hidden_size =** hidden size\n>\n> **embedding_dim =** number of dimentions of the embedding matrix\n>\n> **output_size =** number of classes to predict\n>\n> **p =** dropout rate\n>\n> **n_layers =** number of layers (for RNN layers)\n>\n> **n_filters =** number of filters (for CNN layers)\n>\n> **filter_sizes =** ilter\/kernels sizes (for CNN layers)\n\n\n**Below is the parameters for each model:**\n\n> **SelfAttntion-Model parameters              :** (vocab_size, embedding_dim, hidden_size, output_size, p=.5)\n> \n> **BiLSTM_2DMAX_POOLING_2D_CNN-Model parameter:** (vocab_size, embedding_dim, hidden_size, n_filters, n_layers, output_size, p=.5)\n> \n> **BiLSTM_concat_hiddens-Model parameters     :** (vocab_size, embedding_dim, hidden_size, n_layers, output_size, p=.5)\n> \n> **RCNN-Model parameters                      :** (vocab_size, embedding_dim, hidden_size, n_layers, output_size, p=.5)\n> \n> **CNN-Model parameters                       :** (vocab_size, embedding_dim, n_filters, filter_sizes, output_size, p=.5)\n> \n> **BiLSTM-Model parameters                    :** (vocab_size, embedding_dim, hidden_size, n_layers, output_size, p=.5)\n> \n> **LSTM-Model parameters                      :** (vocab_size, embedding_dim, hidden_size, n_layers, output_size, p=.5)\n> \n> **BERT-Model parameters                      :** (hidden_size, output_size)\n\nFor learning purposes, we will use constant hyper-parameters for all the models.","ee2a8ab9":"---\n# 1- Create Data\n\n\n# 1.1- Import the Libraries","ecf1dc5a":"---\n# 5.5- Concatenate Embeddings\n\nWe will Combine both embeddings.","fba42a5b":"# 11.2- Training Models","39226a84":"---\n\n# 1.3- Use Just the Important Columns for the Classification\n\nNotice that there is dublicated news in the data which means that the text was categorized for more then one class. But since the dublicates are a small amount (few hundreds), we will remove them to avoid making it multi-label classification","0c5d9fc2":"---\n\n# 6.2- Bidirectional LSTM (BiLSTM)\n\n\nBidirectional LSTM(BiLSTM) model maintains two separate states for forward and backward inputs that are generated by two different LSTMs. The first LSTM is a regular sequence that starts from the beginning of the sentence, while in the second LSTM, the input sequence are fed in the opposite order(first lstm go from left to right and second lstm go from right to left). The idea behind bi-directional network is to capture information of surrounding inputs. It usually learns faster than one-directional approach Because it can see future words. \n\n![Detailed-structure-within-an-BiLSTM-cell](https:\/\/miro.medium.com\/max\/544\/1*sBZq013xGoFvyH5e0sXtdg.png)\n\nFOR MORE DETAILS READ THIS ARTICLE: [**Bi-directional RNN & Basics of LSTM and GRU**](https:\/\/medium.com\/analytics-vidhya\/bi-directional-rnn-basics-of-lstm-and-gru-e114aa4779bb)","9f41fb40":"---\n\n# 6.4- Recurrent Convolutional Neural Networks (RCNN)\n\nRecurrent Neural Networks (RNN) is a biased model, where later words are more dominant than earlier words. Thus, it could reduce the effectiveness when it is used to capture the semantics of a whole document, because key components could appear anywhere in a document rather than at the end. To tackle the bias problem, the Convolutional Neural Network (CNN), an unbiased model which can fairly determine discriminative phrases in a text with a max-pooling layer. Thus, the CNN may better capture the semantic of texts compared to recurrent neural networks. \n\nBut there is a problem with CNN models, when using such kernels, it is difficult to determine the window size: small window sizes may result in the loss of some critical information, whereas large windows result in an enormous parameter space (which could be difficult to train). Therefore, it raises a question: can we learn more contextual information than conventional window-based neural networks and represent the semantic of texts more precisely for text classification ?\n\nTo address the limitation of the above models, a [*Recurrent Convolutional Neural Network (RCNN)*](https:\/\/www.researchgate.net\/figure\/Recurrent-Convolutional-Neural-Network-Lai-et-al-2015-model-The-Figure-1-depicts-the_fig1_344013022) was proposed.\n\n![RNN structure](https:\/\/www.researchgate.net\/publication\/344013022\/figure\/fig1\/AS:930800774168576@1598931606345\/Recurrent-Convolutional-Neural-Network-Lai-et-al-2015-model-The-Figure-1-depicts-the.ppm)\n\n**First, we apply a bi-directional recurrent structure**, which may introduce considerably less noise compared to a traditional window-based neural network, to capture the contextual information to the greatest extent possible when learning word representations. Moreover, the model can reserve a larger range of the word ordering when learning representations of texts. **Second, we employ a max-pooling layer** that automatically judges which features play key roles in text classification, to capture the key component in the texts. By combining the recurrent structure and max-pooling layer, our model utilizes the advantage of both recurrent neural models and convolutional neural models.\n\nthe core idea: \n\nWe apply a **Bi-directional Recurrent structure**, which may introduce considerably less noise compared to a traditional windowbased (e.g. CNN) neural network, to capture the contextual information to the greatest extent possible when learning word representations.  Moreover, the model can reserve a larger range of the word ordering when learning representations of texts.\n\nAfter applying BiLSTM, We **concatenate the left-side context vector, the word embedding and the right-side context vector**.\n\nAfter we obtain the whole concatenation representation of a word , we **apply a linear transformation together with the tanh activation function** and send the result to the next layer\n\nWe **employ a max-pooling layer** that automatically judges which features play key roles in text classification, to capture the key component in the texts.\n\nBy combining the recurrent structure and max-pooling layer, our model utilizes the advantage of both recurrent neural models and convolutional neural models.\n\n**the model work as follow :**\n\nWe apply Bi-LSTM, one encodes the semantics of the left-side context along with all texts in the sentence, and another one encodes the semantics of the right-side context. Then, we define the representation of word, which is the *concatenation* of the *left-side context* vector, the *word embedding* and the *right-side context * vector.  In this manner, using this contextual information, our model may be better able to disambiguate the meaning of the word.\n\nFOR MORE DETAILS READ THE PAPER: [Recurrent Convolutional Neural Networks for Text Classification](https:\/\/www.deeplearningitalia.com\/wp-content\/uploads\/2018\/03\/Recurrent-Convolutional-Neural-Networks-for-Text-Classification.pdf)","60f16fb3":"---\n\n# 4- Data Preparation\n\nIf you are not familiar with using Dataset and Dataloader classes in Pytorch, I will recommend first checking out this [post](https:\/\/medium.com\/swlh\/how-to-use-pytorch-dataloaders-to-work-with-enormously-large-text-files-bbd672e955a0) which gives an overview of these APIs. In short, a Dataset class provides a method to sample data from a given dataset and a Dataloader object calls this method repeatedly to iterate through the dataset. The dataset class in our problem will look like\n\n# 4.1- Split Data\n\nWe will split data to three sets: train, validation and test sets.\n\nTrain set will be used to train models and tune there parameters.\n\nValidation set will be used to evaluate models, and its scores will be important to know if the models accuracies still increase; if not, we will stop it using Early Stopping.\n\nTest set will be the last section that will determine the accuracies of the models, and these scores will be used when we will analyze all models performances.","89d5f9a2":"Now we know what does *Attention* means; Let's go to *Self-Attention*\n\n\n**Self-Attention :**\n\nSelf-Attention([A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING](https:\/\/arxiv.org\/pdf\/1703.03130.pdf)) mechanism allows extracting different aspects of the sentence into multiple vector representations. It is performed on top of an LSTM in our sentence embedding model. This enables attention to be used in those cases when there are no extra inputs. In addition, due to its direct access to hidden representations from previous time steps, it relieves some long-term memorization burden from LSTM. As a side effect coming together with our proposed self-attentive sentence embedding, interpreting the extracted embedding becomes very easy and explicit.\n\nThe proposed sentence embedding model consists of two parts:  The first part is a **bidirectional LSTM**, and the second part is the **self-attention mechanism**, which **provides a set of summation weight vectors** for the LSTM hidden states. **These set of summation weight vectors are dotted with the LSTM hidden states**, and the resulting weighted LSTM hidden states are considered as an embedding for the sentence.  \n\n![Self-Attention](https:\/\/miro.medium.com\/max\/1200\/1*6c4-E0BRRLo197D_-vyXdg.png)\nthis figure shows an example when the proposed sentence embedding model is applied to sentiment analysis, combined with a fully connected layer and a softmax layer. Besides using a fully connected layer.\n\n\n**How It Works :**\n\nEach word in the sentence is independent. To gain some dependency between adjacent words within a single sentence, **we use a bidirectional LSTM** to process the sentence. And **we concatenate each hidden state of both directions to be as one hidden state(let's call it H)**. Let the hidden unit number for each\nunidirectional LSTM be *u*. Our aim is to encode a variable length sentence into a fixed size embedding. We achieve that by choosing a linear combination of the *n* LSTM hidden vectors in *H*. Computing the linear combination requires the self-attention mechanism. **The attention mechanism takes** the whole concatenated LSTM hidden states **H as input**, **and outputs a vector of weights** *a*.\n\n\n**a = Softmax(w_{s2}tanh(w_{s1}H^T))**\n\nHere *w_s1* is a weight matrix with a shape of *da-by-2u* ('any  hyperparameter' * '2(num_direction)'). and *w_s2* is a vector of parameters with size *da*, where *da* is a hyperparameter we can set arbitrarily. Since *H* is sized *n-by-2u* (e.g. 'hidden_size' * '2(num_direction)'), the annotation vector *a* will have a size *n*. the *softmax()* ensures all the computed weights sum up to 1. Then we sum up the LSTM hidden states *H* according to the weight provided by *a* to get a vector representation *m* of the input sentence.\n\n> This vector representation usually focuses on a specific component of the sentence, like a special set of related words or phrases. So it is expected to reflect an aspect, or component of the \nsemantics in a sentence. However, there can be multiple components in a sentence that together forms the overall semantics of the whole sentence, especially for long sentences. (For example, two clauses linked together by an \u201dand.\u201d) Thus, to represent the overall semantics of the sentence, we need multiple *m*s that focus on different parts of the sentence. Thus we need to perform multiple hops of attention. Say we want *r* different parts to be extracted from the sentence, with regard to this, we extend the ws_2 into a *r-by-da* matrix, note it as Ws2, and the resulting annotation vector a becomes annotation matrix *A* instead of *a*. Formally: *A = Softmax(W_{s2}tanh(W_{s1}H^T))*\n\n![Self-Attention equation](https:\/\/nlp.gluon.ai\/_images\/attention-nlp.png)\n\n\nHere the softmax() is performed along the second dimension. The embedding vector *m* then becomes an *r-by-2u* embedding matrix *M*. We compute the *r* weighted sums by multiplying the annotation matrix A and LSTM hidden states H, the resulting matrix is the sentence embedded\n\nFOR MORE DETAILS READ THIS ARTICLE: [Attention? Attention! - Lil'Log](https:\/\/lilianweng.github.io\/lil-log\/2018\/06\/24\/attention-attention.html)","d01505fb":"---\n# 12.3- Comparing accuracy-score\n\nThe accuracy metric is the fraction of predictions our model got right. The best model score is LSTM_concat_hiddens_Model. Its score is around 80.5%. \n\nNotice that F1-score is more accurate for imbalanced data than accuracy-score.\n\n","99370eb2":"---\n# 3- Text Analysis\nWe will do some **Text Analysis** to know info about the data; like how the words are spread among classes and mean lengths etc...\n\n# 3.1- Frequency of the Most Common Words\n\nWe will plot the Frequency of the Most Common 50 Words","5df601f7":"# 10.2- Training Models","9ca25fcd":"---\n# 9- Training Models Without Pre-Trained Embedding \n\nIn this section, we will expreiment the models performance *Without Pre-Trained Embedding*, that means the models will train their *Embedding Layer* only using the news data (which is around 200,000 samples).\n\nNotice that the models are not trained in the kernel, I trained them and saved the log of the test set scores, although I copied some logs for training if you are interested. I did that because in this case I will run 8 models for around 10 or more for 4 times\n\n# 9.1- Assigen Parameters\n\nIn this section, we will initiate models and set its parameters","da44e6f9":"---\n# 1.4- Combine Similar Classes for Balancing data(if possible)\nI did this clustering very carefully by looping through every class and see how they are relevant to each other by printing samples of texts for each class (but not in the notebook, to make it clean).\n\nFirst let's see the distribution of the classes:","41f5558a":"# 11.3- Test Models Accuracies","413e42d8":"# 12.2- Training Models","39fd6239":"---\n\n# 4.4- Create Data Class (for pytorch API)\n\nWe will feed our networks with *text* and the *features* we created above.\n\nWe will also sort the texts by their length so we can decrease the amount of padding and make training fast. For example if there is batch with texts of the following lengths 10,13 and 55. We need to pad them equaly to 55. But when we sort them, the text of length 55 will be batched with texts of similar length.\n\nAnd inside the __getitem__ attribute will be the indexing of the text. But padding will be in *collate function* which is a function that tells in which shape we want *DataLoader* to get the data (*collate function* is used to modify the data during the batch, the only thing we will use it for is to pad the sequence by batch max_length).\n\nWe created one class for all the model except BERT model, because BERT model need a special way to process the data by BertTokenizer, and has its own vocabs dictionary, so we will create a function for it. It's similar to *Data* class, just a little difference.","9a24d206":"---\n\n# 6.6- BiLSTM with 2D-MAX-POOLING AND 2D-CNN\n\nRNN can utilize distributed representations of words by first converting the tokens comprising each text into vectors, which form a matrix. And this matrix includes two dimensions: the time-step dimension and the feature vector dimension. Then most existing models usually utilize one-dimensional (1D) max pooling operation or attention-based operation only on the time-step dimension to obtain a fixed-length vector. However, the features on the feature vector dimension are not mutually independent, and simply applying 1D pooling operation over the time-step dimension independently may destroy the structure of the feature representation. On the other hand, applying two-dimensional (2D) pooling operation over the two dimensions may sample more meaningful features for sequence modeling tasks\n\nApplying two-dimensional (2D) pooling operation over the two dimensions may sample more meaningful features. To integrate the features on\nboth dimensions of the matrix, this paper **[Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling](https:\/\/www.aclweb.org\/anthology\/C16-1329.pdf)** explores applying 2D max pooling operation to obtain a fixed-length representation of the text. This paper also utilizes 2D convolution to sample more meaningful information of the matrix.\n\nthe paper of the architecture proposes Bidirectional Long Short-Term Memory Networks with Two-Dimensional Max Pooling (BiLSTM-2DPooling) to capture features on both the time-step dimension and the feature vector dimension. It first utilizes BiLSTM to transform the text into vectors. And then 2D max pooling operation is utilized to obtain a fixed-length vector. This paper also applies 2D convolution (BLSTM-2DCNN) to capture more meaningful features to represent the input text. \n\n**WARNNING**: *WE MUST USE ONLY A KERNEL-SIZE OF 2 OTHERWISE WE WILL GET ERRORS*\n\nThis model introduces two combined models **BiLSTM-2dPooling** and **BiLSTM-2dCNN**.\n\nThe overall model consists of four parts: **BiLSTM Layer**, **Two-dimensional Convolution Layer**, **Two dimensional max pooling Layer**, and **Output Layer**. \n\n\n![architecture](https:\/\/images.deepai.org\/converted-papers\/1611.06639\/x1.png)\nin the above Figure: A BLSTM-2DCNN for the seven word input sentence. Word embeddings have size 3, and\nBiLSTM has 5 hidden units. The height and width of convolution filters and max pooling operations are\n2, 2 respectively.\n\nI didn't find any article that implemented or explained the architecture of this paper; so I hope my explanation will be clear.\n\nFOR MORE DETAILS READ THE PAPER : [Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling](https:\/\/www.aclweb.org\/anthology\/C16-1329.pdf)","f8fe9851":"---\n\n# 2.4- Reduce the Most Common Words Using PCA\n\n\n# 2.4.1- What is PCA?\n\nPrincipal Component Analysis(PCA), is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information. \n\nFOR MORE DETAILS READ THIS ARTICLE: [How Exactly Does PCA Work](https:\/\/towardsdatascience.com\/how-exactly-does-pca-work-5c342c3077fe).\n\n# 2.4.2- How to Extract the Best Componet?\n\nThe amount of Values are the Principal Components that will keep the most information. But how do we choose the number of components? \n\nif you reed the [ARTICLE](https:\/\/towardsdatascience.com\/how-exactly-does-pca-work-5c342c3077fe), you will know that the informaion are cumulated in the first component till it was full and continue to cumulate in the next component till the last one. So we will plot the cumulative sum(percentage of the amount of data in the components) of *explained variance ratio* and choose the number of components that \"capture\" at least 90% of the variance(information). for more detail about *explained variance ratio* read this [stackoverflow](https:\/\/stackoverflow.com\/questions\/32857029\/python-scikit-learn-pca-explained-variance-ratio-cutoff#:~:text=The%20pca.explained_variance_ratio_%20parameter%20returns%20a%20vector%20of%20the,dimension.%20You%20probably%20want%20to%20do%20pca.explained_variance_ratio_.cumsum%20().).","3a237c66":"---\n\n# 1.2- Load Data","127137bc":"---\n# 11- Training Models With Pre-Trained Embedding (Fine-Tuned)\n\nIn this section, we will train every model with *Pre-Trained Embedding (Fine-Tuned) Layers*.\n\nSince BERT is freezed, there is no embedding to fine-tune and test its prediction, so BERT will not be included in this section.\n\n\n# 11.1- Assigen Parameters","b30e6a33":"---\n\n# 4.5- Create collate_fn to PAD Sorted Text by Length\n\nWe will create a padding function, and the collate function to pad the batches to the highes text length in the batch , instead of pading the hole data to the highest length in it.","8dc299b6":"# 7.3- Create Prediction Loop\n\n**Prediction Loop(pred),** This function will be used for statistics to know which model is better. It works as follow:\n\n\n1-Prepare\/Load the data \n\n2-Fead the model without computing gradients (to avoid train the parameters)\n\n3-calculate loss without computing gradients (to avoid train the parameters)\n\n4-calculate the scores","13c4e068":"---\n\n# 3.4- Plotting the Distribution of the Extracted Features(lexicon) among the classes\n\nHere we will notice that there are some classes effected by these features:\n\nNumber of lower letters","34236734":"---\n# 1.5- Use Top-10 Classes for Classification\n\nIt's still not good enaugh to classify all the classes. So because the data till now is imbalanced, we will just pick the top\/bigest 10 categories:\n\nPOLITICS: 36027\n\nHEALTH: 24511\n\nENTERTAINMENT: 21219\n\nPARENTING: 12546\n\nSTYLE & BEAUTY: 11763\n\nTRAVEL: 9884\n\nWEIRD NEWS: 8981\n\nFOOD & DRINK: 8322\n\nBUSINESS & ECONOMY: 7642\n\nDIVORCE & WEDDINGS: 7074","3801d2c8":"Number of title letters","50d1ebe9":"---\n# 2-Feature Engineering\n\n\n\n# 2.1- Extract Features(lexicon) from Text before Cleaning (words count, upper count ...)\n\nWe will extract some features from the text before cleaning the text. Some times these features make alot of sense to the classifier. We will see how each feature is has different distribution from class to another in *Text Analysis* section.","86f9fd03":"---\n\n# 4.3- Convert Categories to Index","2e0ed6f6":"Number of lower letters","de5e03b5":"Number of words","08b158d7":"---\n# 12- Training Models With Created Features\n\nIn this section, we will train every model with *Created Features* **+** *Pre-Trained Embedding (Fine-Tuned) layers*.\n\n# 12.1- Assigen Parameters\n\nNotice the **use_features=True**","7519694f":"# 12.3- Test Models Accuracies","721c0eb1":"# 13- Comparing Models and Techniques\n\n# 13.1- Comparing Loss\n\nRemember, Loss is the result of a bad prediction. A loss is a number indicating how bad the model's prediction was on a single example. But here it's the average loss of the whole test set.\n\nThe lower the Loss function, the better a model will be. all the models achieved the lowest loss with a *tuned pre-trained embedding and created features*. But notice that the *tuned pre-trained embedding* alone had a higher loss than without the features, which tell us that our created features are the reason for this good score not the tuning. Also there no need to say that a model with pre-trained embedding generalize better than trained one (if you have lots of data, I may change my words).\n\nThe best model that has the lowest loss is LSTM(*tuned pre-trained embedding*). Maybe because it's not complex compared with the other models. it's loss is around 0.5300 which is very good.\n\nNotice that BERT although it's the SOTA archeticure, but it didn't give us the result we expected. BERT parameters aren't tuned, we just added one Linear layer to train its parameters on our data. So maybe if we tune it or train it from scratch(so expensive) we will get better score.\n","bfd5d6fe":"---\n\n# 6.8- Bidirectional Encoder Representations from Transformers (BERT)\n\nBERT (Bidirectional Encoder Representations from Transformers), a recent paper called [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https:\/\/arxiv.org\/pdf\/1810.04805) published by researchers at Google AI Language. It became one of the most popular architectures for text classifications.\n\n![BERT structure](https:\/\/miro.medium.com\/max\/700\/0*ViwaI3Vvbnd-CJSQ.png)\n\nBERT relies on a Transformer (the attention mechanism that learns contextual relationships between words in a text). A basic Transformer consists of an encoder to read the text input and a decoder to produce a prediction for the task.\n\nAs opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that it\u2019s non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word).\n\nThe model is fully made of attention mechanism; no recurrance. It's too long to explain it clearly in this notebook, so I recommend to read the below article or the above paper. Notice that I walked with you half the way because you know what is attention ;)\n\nFor Sentiment-Analysis Classification Problem, BERT takes the output representation of the first token **[CLS]** then feeds it to a softmax classifier to get the prediction and use the prediction as model output for fine-tuning.\n\nThere are  two types of BERT:\n\n**BERT-Base**: Layers=12, Hidden Size=768, Self-Attention heads=12, Total Prameters=110M\n\n**BERT-Large**: Layers=24, Hidden Size=1024, Self-Attention heads=16, Total Prameters=340M\n\nWe will use BERT-Base since it's the smallest.\n\n**NOTE: ** We won't train BERT from 0, will Fine-Tune it, And it's recommended to for Fine-Tuning Bert, to train it for 4 epoch.\n\n\nFOR MORE DETAILS READ THIS ARTICLE: [**Painless Fine-Tuning of BERT in Pytorch**](https:\/\/medium.com\/swlh\/painless-fine-tuning-of-bert-in-pytorch-b91c14912caa)","ae2bcf4a":"In this NoteBook we will discuss and apply alot of algorithms and techniques for text classification. The goal of this NoteBook is to explain the implemetation and the code of every model archeticture in full details; because when someone reads an article to explain a model, if the code was provided, it's not explained in details, like how a convolutional layer takes the inputs of an embedding layer in case of CNN for text classification, how to give the correct shape, etc...\n\nThe implemetation of every model is applied directly from the original paper of the archeticture\n\nI will explain the techniques breifly and I will provide detailed articles for interested people.\n\nWe will use[ News Category Dataset](http:\/\/www.kaggle.com\/rmisra\/news-category-dataset)\nas a News Categorization Problem of 41 classes and 200853 samples. The data is real, which mean that it may be imbalanced, we will see how to fix this problem.\n\n![](https:\/\/storage.googleapis.com\/kaggle-datasets-images\/32526\/42326\/19f22224e873ca6c82d66e269ac02c0c\/dataset-card.jpg?t=2018-06-21-02-20-24)\n---\n# Outline:\n\n\n# 1- Create the Data\n\n**1.1- Import the Libraries**\n\n**1.2- Load Data**\n\n**1.3- Use Just the Important Columns for the Classification**\n\n**3.4- Combine Similar Classes to One Class (if possible)**\n\n**3.2- Use Top-10 Classes for Classification**\n\n\n# 2-Feature Engineering\n\n**2.1- Extract Features(lexicon) from Text before Cleaning (words count, upper count ...)**\n\n**2.2- Clean the Text (lower, stopwords, stemming, lammatizing, correcting, removing and more) Using a Big Function**\n\n**2.3- Extract the Most *n* Common Words for Every Class**\n\n**2.4- Reduce the Most Common Words Using PCA**\n\n**2.4.1- What is PCA?**\n\n**2.4.2- How to Extract the Best Componet?**\n\n**2.4.3- Reduce the Data**\n\n\n# 3- Text Analysis\n\n**3.1- Frequency of the Most Common Words**\n\n**3.2- Frequency of the Most Common Words for Every lass**\n\n**3.3- The Average, Maximum and Minimum Length of the News Text**\n\n**3.4- Plotting the Distribution of the Extracted Features(lexicon) Among the Classes**\n\n\n# 4- Data Preparation\n\n**4.1- Split Data**\n\n**4.2- Prepare Text: split text , drop non-frequent words(e.g. occurred 3 times), create word2index and index2word for converting text to numbers(index)**\n\n**4.3- Convert Categories to Index**\n\n**4.4- Create Data Class (for pytorch API)**\n\n**4.5- Create collate_fn to PAD Sorted Text by Length**\n\n**4.6- Use *DataLoader* to Batch the Data (and use collate_fn to pad the batch by length)**\n\n\n# 5- Load and Combine Multiple Pre-Trained Embeddings\n\n**5.1- What is Word Embedding**\n\n**5.2- What is Word2Vec Embedding**\n\n**5.3- What is GloVe Embedding**\n\n**5.4- Prepare Word Embedding Files**\n\n**5.5- Concatenate Embeddings**\n\n**5.6- Create Embedding Matrix**\n\n\n# 6- Model Archtectures\n\n**6.1- Long Short-Term Memory (LSTM)**\n\n**6.2- Bidirectional LSTM (BiLSTM)**\n\n**6.3- Convolutional Neural Network (CNN)**\n\n**6.4- Recurrent Convolutional Neural Networks (RCNN)**\n\n**6.5- BiLSTM with Last Layer's Hidden States Concatenated**\n\n**6.6- BiLSTM with 2D-MAX-POOLING AND 2D-CNN**\n\n**6.7- Self-Attention**\n\n**6.8- Bidirectional Encoder Representations from Transformers (BERT)**\n\n\n# 7- Create Training, Validating Prediction and Running Loops \n\n**7.1- Create Training Loop**\n\n**7.2- Create Validating Loop**\n\n**7.3- Create prediction Loop**\n\n**7.4- Create Running Loop**\n\n\n# 8- Explain the Metrices, Loss and Optimzer\n\n**8.1- Accuracy Score**\n\n**8.2- F1 Score**\n\n**8.3- AdamW Optimizer**\n\n**8.4- Cross-Entropy Loss Function**\n\n\n# 9- Training Models Without Pre-Trained Embedding \n\n**9.1- Assigen Parameters**\n\n**9.2- Training Models**\n\n**9.3- Test Models Accuracies**\n\n\n# 10- Training Models With Pre-Trained Embedding (freezed)\n\n**10.1- Assigen Parameters**\n\n**10.2- Training Models**\n\n**10.3- Test Models Accuracies**\n\n\n# 11- Training Models With Pre-Trained Embedding (Fine-Tuned)\n\n**11.1- Assigen Parameters**\n\n**11.2- Training Models**\n\n**11.3- Test Models Accuracies**\n\n\n# 12- Training Models With Created Features\n\n**12.1- Assigen Parameters**\n\n**12.2- Training Models**\n\n**12.3- Test Models Accuracies**\n\n\n# 13- Comparing Models and Techniques\n\n**13.1- Comparing Loss**\n\n**13.2- Comparing f1-score**\n\n**13.3- Comparing accuracy-score**\n\n\n# 14- Conclusion","6295415e":"---\n# 8- Explain the Metrices, the Loss and the Optimzer\n\nWe will explain every metrices, the Optimizer and the Loss Function:\n\n# 8.1- Accuracy Score\n\n**Accuracy** is a metric that evaluate classification\/prediction of a model. It is the number of original classes divided by the total number of predictions , multiplied by 100 to turn it into a percentage. It's good in a binary or multi class problems, but it won't be accuracte for Imdalanced classes distribution.\n\n**The formula for the Accuracy score is: TP+TN\/TP+FP+FN+TN**\n\nfor more details read this article: [Accuracy, Precision, Recall & F1 Score: Interpretation of Performance Measures](https:\/\/blog.exsilio.com\/all\/accuracy-precision-recall-f1-score-interpretation-of-performance-measures\/)\n\n---\n\n# 8.2- F1 Score\n\n**F1 score** can be interpreted as a weighted average of the **Precision** and **Recall**. This score takes both false positives and false negatives into account. F1 is usually more useful than accuracy, especially if you have an Imdalanced classes distribution \n\n**The formula for the F1 score is: 2 * (precision * recall) \/ (precision + recall)**\n\n\n**Precision :**\n\n**Precision** is the ratio of correctly predicted positive observations to the total predicted positive observations. The question that this metric answer is of all predictions that labeled as True, how many actually are True? High precision relates to the low false positive rate.\n\n**The formula for the Precision score is: TP\/TP+FP**\n\n\n**Recall (Sensitivity) : **\n\n**Recall** is the ratio of correctly predicted positive observations to the all observations in actual class - True. The question recall answers is: Of all the predictions that are truly True, how many did we label?\n\n**The formula for the Recall score is: TP\/TP+FN**\n\nfor more details read this article: [Accuracy, Precision, Recall & F1 Score: Interpretation of Performance Measures](https:\/\/blog.exsilio.com\/all\/accuracy-precision-recall-f1-score-interpretation-of-performance-measures\/)\n\n---\n\n# 8.3- AdamW Optimizer\n\nThe optimizer goal in a nutshell, is to associate loss function and model parameters together by updating the model, i.e. the weights and biases of each node based on the output of the loss function. So we can get lowest loss output. Gradient here refers to the slope of the equation in general. Gradients are partial derivatives and can be considered as the small change reflected in the loss function with respect to the small change in weights or parameters of the function. Now let's go **AdamW Optimizer**\n\nTo understand **AdamW**, let's first understand **Adam** : what Adam(adaptive moment estimation.) does is to *make Big steps when the gradients do not change much and small steps when they vary rapidly (adapting the step size for each weight individually, hence the name adaptive)*. **AdamW** yields better training loss and the models generalize much better than the models trained with Adam. AdamW uses L2 regularization or weight decay which help networks weights to overfit less and generalize better.\n\nfor more details read this article: [Why AdamW matters](https:\/\/towardsdatascience.com\/why-adamw-matters-736223f31b5d)\n\n---\n\n# 8.4- Cross-Entropy Loss Function\n\nCross-entropy loss measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.(In a neural network, you typically achieve this prediction by having the last layer activated by a softmax function; but in pytorch API, Cross-Entropy Loss doesn't need softmax activation function because it use softmax inside it)\n\nfor more details read this article: [Cross-entropy for classification](https:\/\/towardsdatascience.com\/cross-entropy-for-classification-d98e7f974451)","0c508c91":"---\n\n# 5.6- Create Embedding Matrix\n\nWe must create a look-up table so an Embedding layer can map the index of the words to their vectors representations.\n\nFor OOV words, instead of creating an array full of zeros (no correlation with any word), it's recommended for \nany word that has no vector representation be an array of random numbers between -0.25 and 0.25 (random correlation).","a688a254":"Number of special characters","a8af10d6":"# 2.4.3- Reduce the Data\n\nSo after knowing the *best_component*, we will re-*fit* PCA again but with the best number of components that keep 90% of information","87952f41":"---\n# 4.2- Prepare Text: split text , drop non-frequent words(e.g. occurred 3 times), create word2index and index2word for converting text to numbers(index)\n\nOur vocabs will be from train set vocabs and validation set vocabs. Although 'val_set' data will not train the model representaions for a word, when we extract the vectors representaions of a pre-trained Embedding, we can extract the vectors for 'val_set'vocabs and avoid OOV(out of vocabulary) probelm.\n\nSo we will have more words with vectors representaions.\n\nWe will drop any word that ocurred less then 3 times.","ae5e3f16":"---\n# 12.3- Comparing F1-score\n\nRemember F1-score is the mean of Precision and Recall and gives a better measure of the incorrectly classified cases than the Accuracy Metric.\nNow here we see some thin different from the loss scores. Notice that a loss function tells how the model parameters aren't sure about the prediction. But F1-score is more important than loss, because it measure the predictions in a way we human understand and be sure about model parameters.\n\nThe best model score is LSTM_concat_hiddens_Model. Its score is around 71.8% which is very good for imbalanced 10 classes.","641e4aa5":"---\n# 14- Conclusion\n\nAs I mentioned in the beginning of the NoteBook, The purpose of this NoteBook is to apply some archetictures and techniques, and not to reach a highest score. So we learned that the complexity doesn't mean a better result, and some features such as word count or punctuation count can have a huge advantages and effects over the accuracy. Not to mention that tuning the hyper-parameters may give better results, since we give all of them a constant hyper-parameters, it worth to play with them. \n\nI hope this work was useful and if it was, PLEASE UPVOTE THIS NOTEBOOK. I may make another Part with new archetictures and techniques for the same problem if I found lots of UPVOTEs.\n\n**Some papers archetictures that will be interesting to implement:**\n\n1. [*Word Emdeddings through Hellinger PCA*](https:\/\/arxiv.org\/abs\/1312.5542)\n\n2. [*Multichannel CNN with Attention for Text Classification*](https:\/\/arxiv.org\/abs\/2006.16174)\n\n3. [*Multi\u2010representational convolutional neural networks for text classification*](https:\/\/onlinelibrary.wiley.com\/doi\/abs\/10.1111\/coin.12225)\n\n**Some techniques that will be interesting to implement:**\n\n1. *Text Augmentation.*\n\n2. *n-grams for embedding vector ( same as n-grams for text, but we will average the embedding vector).*\n","6875f417":"---\n\n# 7.2- Create Validating Loop\n\n**Validating loop(eval_func)**, This function will validate\/test the model parameters. It works as follow:\n\n1-Prepare\/Load the data \n\n2-Fead\/Validate the model without computing gradients (to avoid train the parameters)\n\n3-calculate loss without computing gradients (to avoid train the parameters)\n\n4-calculate the scores","4e9f2408":"---\n\n# 6.1- Long Short-Term Memory (LSTM)\n\nLong Short Term Memory (LSTM) networks are a special kind of RNN, capable of learning long-term dependencies. They were introduced by [Hochreiter & Schmidhuber paper(1997)](https:\/\/www.researchgate.net\/publication\/13853244_Long_Short-term_Memory). They work tremendously well on a large variety of problems, and are now widely used. Recurrent Neural networks capture contextual information by maintaining a state of all previous inputs. This model analyzes a text word by word and stores the semantics of all the previous text in a fixed-sized hidden layer. In theory it seems like the RNN should learn to recognize that a certain type of sentence was important and would \u201cremember\u201d this until the end.\n\nLSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!\n![Detailed-structure-within-an-LSTM-cell](https:\/\/miro.medium.com\/max\/700\/1*aqUTrEWCmkxD90cv0qQwuw.png)\n\nAll recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.\n![Simple RNN structure](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-SimpleRNN.png)\n\nLSTMs also have this chain structure, but the repeating module has a different structure. A common architecture is composed of a cell (the memory part of the LSTM unit) and three \"regulators\", usually called gates, of the flow of information inside the LSTM unit: an input gate, an output gate and a forget gate.\n![LSTM structure](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-chain.png)\n\nFOR MORE DETAILS READ THIS ARTICLE: [**Understanding LSTM Networks**](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/#:~:text=%20Understanding%20LSTM%20Networks%20%201%20Recurrent%20Neural,are%20a%20special%20kind%20of%20RNN,...%20More)","fa7ef673":"---\n\n# 6.3- Convolutional Neural Network (CNN)\n\nConvolutional Neural Network (CNN), is a deep learning neural network designed for processing structured arrays of data such as images. Convolutional neural networks are widely used in computer vision and have become the state of the art for many visual applications such as image classification, and have also found success in Natural Language Processing for Text Classification Problems. CNNs can learn important words or phrases through selection via a max pooling layer.\n\nThe paper([Convolutional Neural Networks for Sentence Classification\n](http:\/\/arxiv.org\/abs\/1408.5882)) says, the result of each convolution will fire when a special pattern is detected. By varying the size of the kernels and concatenating their outputs, you\u2019re allowing yourself to detect patterns of multiples sizes (2, 3, or 5 adjacent words).Patterns could be expressions (word ngrams?) like \u201cI hate\u201d, \u201cvery good\u201d and therefore CNNs can identify them in the sentence regardless of their position.\n\n![Convolutional Neural Network (CNN) structure](https:\/\/miro.medium.com\/max\/700\/0*0efgxnFIaLTZ2qkY)\n\nCNNs are Very fast. Also efficient in terms of representation. With a large vocabulary, computing anything more than 3-grams can quickly become expensive. Even Google doesn\u2019t provide anything beyond 5-grams. *Convolutional Filters* learn good representations automatically, without needing to represent the whole vocabulary. \n\n**The model archeticture works as follow : **\n\nEach Word in a sentence are of n-dimensional vector representation. \n\nA convolution operation involves a filter which is applied to a window\/kernel of h words(height) to produce a new feature. This filter is applied to each possible window\/kernel of words in the sentence to produce a *feature map*.\n\nWe then apply a max-pooling operation over the feature map and take the maximum value as the feature corresponding to this particular filter. The idea is to capture the most important feature (with the highest value for each feature map).\n\nThe model uses multiple filters (with varying window sizes) to obtain multiple features (one feature is extracted from one filter).\n\nFOR MORE DETAILS READ THIS ARTICLE: [**Understanding Convolutional Neural Networks for NLP**](http:\/\/www.wildml.com\/2015\/11\/understanding-convolutional-neural-networks-for-nlp\/)","291c49e4":"---\n# 4.6- Use *DataLoader* to Batch the Data (and use collate_fn to pad the batch by length)\n\nWe will take set 'batch_size=12'for BERT model, because it's a huge model and the authors of BERT recommend batch to be like [8, 16, 32, 64],\nand 'batch_size=32' for all other models.","d193aa56":"---\n# 10- Training Models With Pre-Trained Embedding (Freezed)\n\nIn this section, we will train every model with *Pre-Trained Embedding (Freezed) layers*.\n\n\n# 10.1- Assigen Parameters","6890b4a9":"Now let's combine the similar classes and see if the distribution will be better.","79ab0e93":"# 9.2- Training Models\n\nIn this section, we will train every model.","3ceea802":"---\n# 7- Create Training, Validating and Prediction Loops \n\nWe will create the following functions for training a model:\n\n**Training Loop(train_func)**\n\n**Validating Loop(eval_func)**\n\n**Prediction Loop(pred_func)**\n\n**Running Loop(training_model)**\n\nEvery function will be explained in it's own section (the code will be explained clearly). \n\n---\n# 7.1- Create Training Loop\n\n**Training loop(train_func)**, This function will train\/tune the model parameters. It works as follow:\n\n1-Prepare\/Load the data \n\n2-Fead\/Train the model\n\n3-calculate loss and optimizer gradients\n\n4-calculate the scores","c4a553ec":"---\n\n# 7.4- Create Running Loop\n\n**Running Loop(training_model),** this function is different from the other functions, it will run train and eval loops. It works as follow: \n\n1-Run epoch\n\n2-Train the model\n\n3-Validate the model\n\n3-Save the current model (for fine-tuning later if we want); Optional\n\n4-Assign best model\n\n5-Apply Early Stopping if the model didn't improve for 5 consecutive epochs (the score we will use it for  Early Stopping is accuracy)\n\n6-Save best model; Optional","3f0872c5":"Number of digits characters","3da1ebce":"---\n# 2.2- Clean the text (lower, stopwords, stemming, lammatizing, correcting, removing and more) using a big function\n\nThe function will do lots of things, you can read it if you want, but it will just clean the text from any useless word.","f3cc9f0b":"---\n\n# 3.2- Frequency of the Most Common Words for Every Class\n\nWe will plot the most frequent words for every class. If there is no visualisation, then the class does not has special words that just ocurr for it.","a8b98e51":"# 9.3- Test Models Accuracies\n\nIn this section, we will predict 'val_loader' data and take every model scores with its name just to make statistics that tell us how good are the models later.","640c2654":"---\n# 2.3- Extract the Most *n* Common Words for Every Class\n\nThe **extract_most_common** function is used to extract the most common *n* words for every class. We will use just 10 words for every class, and will reduce the total amount of extracted features\/words by PCA later (maybe the features are not a huge amount that need the use of PCA, but for learning puposes, we will implement it).\n\nThe **remove_words_if_in_remove_list** function is used to remove the words that, occurred in the specified class (that the word extracted from), and any other class(the hole vocabs except the specified class's vocabs). Just to make sure that the extracted words are just occurring in the specified class.","8efe417a":"Now after the end of the section, let's see if we have any **Na** values that was caused by the processing.","3f89ccf3":"---\n# 5- Load and Combine Multiple Pre-Trained Embeddings\n\n# 5.1- What is Word Embedding\n\nWord Embedding are basically a form of word representation that bridges the human understanding of languages to that of a machine. They have learned representations of text in an n-dimensional space where words that have the same meaning have a similar representation. It means that two similar words are represented by almost similar vectors, which are very close to each other in a vector space.\n\nAn embedding is a mapping of a discrete \u2014 categorical \u2014 variable, to a vector of continuous numbers. In the context of neural networks, embeddings are low-dimensional, learned continuous vector representations of discrete variables. Neural network embeddings are useful, because they can reduce the dimensionality of categorical variables and meaningfully represent categories in the transformed space.\n\nFor More Details About Words Embeddings: [Neural Network Embeddings Explained](https:\/\/towardsdatascience.com\/neural-network-embeddings-explained-4d028e6f0526)\n\n![word embedding](https:\/\/th.bing.com\/th\/id\/R4532e9bea3d510079b8ac8dc3cf230e1?rik=6Auqj9SJXvCZNg&riu=http%3a%2f%2ftensorflow.classcat.com%2fwp-content%2fuploads%2f2018%2f08%2ftfml-text-classification3_WordEmbeddings.png&ehk=sfZ0h%2f1EKLXHmEFc3NEZw9o6uOVTHZoKWkPACO%2b5cMk%3d&risl=&pid=ImgRaw)\n\nEmbedding matrix is a randomly initialized matrix whose dimensions are **N * (Size of the vocabulary + 1)**, Where *N* is a number that we have to manually choose, and the size of the Vocabulary is the total number of unique words in our Document. So, each column of the embedding matrix represents a particular word from the document.\n\nThere is alot of methods for Word Embeddings, but we will stick to just two methods: **Word2Vec** and **Glove**\n\n---\n# 5.2- What is Word2Vec Embedding\n\nWord2vec is a method to efficiently create word embeddings by using a two-layer neural network. It was developed by Tomas Mikolov, et al. at Google in 2013 ,[Efficient Estimation of Word Representations in Vector Space](https:\/\/arxiv.org\/abs\/1301.3781), and it became the de facto standard for developing pre-trained word embedding. The main idea behind it is that you train a model on the context of each word; so, similar words will have similar numerical representations.\n\n**How It Works:**\n\nJust like a normal feed-forward densely connected neural network(NN) where you have a set of independent variables and a target dependent variable that you are trying to predict, you first break your sentence into words(tokenize) and create a number of pairs of words, depending on the *window size* (a hyper-parameter). So one of the combination could be a pair of words such as ('New','York').\n\nWe feed the 'New' into the NN through an embedding layer initialized with random weights, and pass it through the softmax layer with ultimate aim of predicting 'York'. An optimization method such as SGD, will minimize the loss function \"(target word | context words)\" which seeks to minimize the loss of predicting the target words given the context words. If we do this with enough epochs, the weights in the embedding layer would eventually represent the vocabulary of word vectors, which is the \"coordinates\" of the words in this geometric vector space.\n\n---\n\n# 5.3- What is GloVe Embedding\n\n[Glove: Global Vectors for Word Representation](https:\/\/www.researchgate.net\/publication\/284576917_Glove_Global_Vectors_for_Word_Representation), GloVe works similarly as Word2Vec. Unlike Word2Vec, GloVe does not rely just on local statistics (local context information of words), but incorporates global statistics (word co-occurrence). While you can see above that Word2Vec is a \"predictive\" model that predicts context given word, GLoVe learns by constructing a co-occurrence matrix (words X context) that basically count how frequently a word appears in a context. Since it's going to be a gigantic matrix, we factorize this matrix to achieve a lower-dimension representation. There's a lot of details that goes in GLOVE but that's the rough idea.\n\nfor more details read this question: [How is GloVe Different from Word2Vec](https:\/\/www.quora.com\/How-is-GloVe-different-from-word2vec)\n\n---\n# 5.4- Prepare Word Embedding Files\n\nFor training purposes, we will use two pre-trained embeddings of 50 dimensions.\n\nWe will use *glove.twitter.27B.50d* and *glove.6b.50d* for combining them into one array of 100 dimensions\n\n**Note**: It's recommended for the words that have no embeddings, to be an array with random numbers between **-0.25 and 0.25**","c0d8073a":"---\n\n# 6.5- BiLSTM with Last Layer's Hidden States Concatenated\n\nBiLSTM with Last Layer's Hidden States Concatenated structure is similar to BI-LSTM; but the only difference is that we will concatenate the hidden states of the last BiLSTM layer."}}