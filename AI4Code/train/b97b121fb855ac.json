{"cell_type":{"4823c9e3":"code","09044b75":"code","5dac13e8":"code","5a3bad62":"code","50f15388":"code","b7e7f11e":"code","f80a3e3a":"code","e6b91abd":"code","6936fd27":"code","969defff":"code","0e1ad3f4":"code","36a5da1d":"code","be32a30c":"code","325a4b92":"code","64500dab":"code","47373a9f":"code","4761bc4b":"code","5315dd30":"code","41803438":"code","079dfc8c":"code","5c10fd8e":"code","85a37392":"code","8e503959":"code","7ac6db35":"code","8d80cbae":"code","34c156d2":"code","862c5939":"code","70c97360":"code","86beda26":"code","8bc58975":"code","ff114f57":"code","87dad8a7":"markdown"},"source":{"4823c9e3":"!pip install -q transformers","09044b75":"import pandas as pd\nimport warnings\nimport pandas as pd\nfrom tqdm import tqdm\n\ntqdm.pandas()\nwarnings.filterwarnings(\"ignore\")\n\ndf = pd.read_csv(\"..\/input\/arabic-classification\/arabic_dataset_classifiction.csv\/arabic_dataset_classifiction.csv\")\ndf.head()","5dac13e8":"df.shape","5a3bad62":"set(df.targe.values.tolist())","50f15388":"# labels = ['autre', 'politique', 'sante', 'social', 'sport', '\u00e9conomique']\n# labels = ['politique', 'sante', 'social']\n# dfs = []\n# for i in range(5):\n#     print(i,df[df.targe == i].shape)\n#     dfs.append(df[df.targe == i].iloc[:2000])","b7e7f11e":"# df = pd.concat(dfs)","f80a3e3a":"df.shape","e6b91abd":"# df.rename({\"T\"})","6936fd27":"# Importing stock ml libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics\nimport transformers\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import BertTokenizer, BertModel, BertConfig\n\n\n# Preparing for TPU usage\n# import torch_xla\n# import torch_xla.core.xla_model as xm\n# device = xm.xla_device()","969defff":"# # Setting up the device for GPU usage\n\nfrom torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'","0e1ad3f4":"df[\"list\"] = df.targe.apply(lambda e:[(i==e)*1 for i in range(5)])\n\nnew_df = df[['text','targe',\"list\"]].copy()\n\nnew_df = new_df.rename({\"text\":\"comment_text\"},axis = 1)\nnew_df","36a5da1d":"# Sections of config\nfrom transformers import AutoTokenizer, AutoModel\n\n# Defining some key variables that will be used later on in the training\nMAX_LEN = 200\nTRAIN_BATCH_SIZE = 8*3\nVALID_BATCH_SIZE = 4*3\nEPOCHS = 1\nLEARNING_RATE = 1e-05\n\n# https:\/\/huggingface.co\/asafaya\/bert-base-arabic\nmodel_name = \"asafaya\/bert-base-arabic\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)","be32a30c":"class CustomDataset(Dataset):\n\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.comment_text = dataframe.comment_text\n        self.targets = self.data.list\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.comment_text)\n\n    def __getitem__(self, index):\n        comment_text = str(self.comment_text[index])\n        comment_text = \" \".join(comment_text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            comment_text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n        }","325a4b92":"# Creating the dataset and dataloader for the neural network\n\ntrain_size = 0.8\ntrain_dataset=new_df.sample(frac=train_size,random_state=200)\ntest_dataset=new_df.drop(train_dataset.index).reset_index(drop=True)\ntrain_dataset = train_dataset.reset_index(drop=True)\n\n\nprint(\"FULL Dataset: {}\".format(new_df.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset.shape))\n\ntraining_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\ntesting_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)","64500dab":"# train_dataset[train_dataset==1].shape,train_dataset[train_dataset==0].shape","47373a9f":"train_params = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntest_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntraining_loader = DataLoader(training_set, **train_params)\ntesting_loader = DataLoader(testing_set, **test_params)","4761bc4b":"# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n\nclass BERTClass(torch.nn.Module):\n    def __init__(self):\n        super(BERTClass, self).__init__()\n        self.l1 = transformers.AutoModelForMaskedLM.from_pretrained(model_name)\n        self.l1.cls.predictions.decoder = torch.nn.Linear(in_features=768, out_features= 5, bias=True)\n#         self.l2 = torch.nn.Dropout(0.1)\n#         self.l3 = torch.nn.Linear(32000, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)[0][:,0,:]\n#         output_2 = self.l2(output_1)\n#         output = self.l3(output_2)\n        return output_1\n\nmodel = BERTClass()\n\nmodel.to(device)","5315dd30":"# PATH = '..\/input\/solike-model\/aws_model.pt'\n# model.load_state_dict(torch.load(PATH))\n# model.to(device)\n# model.eval()","41803438":"def loss_fn(outputs, targets):\n    return torch.nn.BCEWithLogitsLoss()(outputs, targets)","079dfc8c":"optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)","5c10fd8e":"def train(epoch):\n    model.train()\n    for _,data in enumerate(training_loader, 0):\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        targets = data['targets'].to(device, dtype = torch.float)\n\n        outputs = model(ids, mask, token_type_ids)\n#         print(outputs)\n#         break\n        optimizer.zero_grad()\n        loss = loss_fn(outputs, targets)\n        if _%5000==0:\n            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n#             PATH = 'model.pt'\n#             torch.save(model.state_dict(), PATH)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()","85a37392":"for epoch in range(EPOCHS):\n    train(epoch)","8e503959":"PATH = 'model.pt'\ntorch.save(model.state_dict(), PATH)","7ac6db35":"def validation(epoch):\n    model.eval()\n    fin_targets=[]\n    fin_outputs=[]\n    with torch.no_grad():\n        for _, data in enumerate(testing_loader, 0):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n            targets = data['targets'].to(device, dtype = torch.float)\n            outputs = model(ids, mask, token_type_ids)\n            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n    return fin_outputs, fin_targets","8d80cbae":"# for epoch in range(EPOCHS):\noutputs, targets = validation(EPOCHS)\noutputs = np.array(outputs).argmax(axis = -1)\n\ntargets = np.array(targets).argmax(axis = -1)\n\n\n\naccuracy = metrics.accuracy_score(targets, outputs)\n# # f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n# # f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\nprint(f\"Accuracy Score = {accuracy}\")\n# # print(f\"F1 Score (Micro) = {f1_score_micro}\")\n# # print(f\"F1 Score (Macro) = {f1_score_macro}\")","34c156d2":"model.eval()\nwith torch.no_grad():\n    inputs = tokenizer.encode_plus(\n        \"text\",\n        None,\n        add_special_tokens=True,\n        max_length= MAX_LEN,\n        pad_to_max_length=True,\n        return_token_type_ids=True\n    )\n    ids = inputs['input_ids']\n    mask = inputs['attention_mask']\n    token_type_ids = inputs[\"token_type_ids\"]\n\n\n    data = {\n        'ids': torch.tensor(ids, dtype=torch.long),\n        'mask': torch.tensor(mask, dtype=torch.long),\n        'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)\n    }\n\n\n    ids = data['ids'].to(device, dtype = torch.long).unsqueeze(0)\n    mask = data['mask'].to(device, dtype = torch.long).unsqueeze(0)\n    token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n    outputs = model.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)","862c5939":"# outputs[0][0,0].shape\noutputs[0].shape","70c97360":"def predict(text):\n    model.eval()\n    with torch.no_grad():\n        inputs = tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length= MAX_LEN,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n        \n \n        data = {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)\n        }\n\n\n        ids = data['ids'].to(device, dtype = torch.long).unsqueeze(0)\n        mask = data['mask'].to(device, dtype = torch.long).unsqueeze(0)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        outputs = model(ids, mask, token_type_ids)\n        return torch.sigmoid(outputs).cpu().detach().numpy()","86beda26":"Review = \"Pour la seconde fois en quelques semaines nous sommes partis sans d\\xEEner. Il y a depuis peu un accueil d\\xE9testable. Une personne horriblement d\\xE9sagr\\xE9able est \\xE0 la r\\xE9ception et une h\\xF4tesse d\\xE9sabus\\xE9e nous place tout au fond du restaurant. Alors que les tables bien plac\\xE9es sont VIDES.\" #@param {type:\"string\"}\ntext = preprocess(Review)\n\n# print(text)\np1 = predict(text)[0][0]\np2 = p1>0.5\nprint(\"Positif (%.2f) N\u00e9gatif (%.2f)\"%(p1,1-p1))\nprint(\"%.2f\/5\"%(p1\/0.2))","8bc58975":"PATH = 'aws_model.pt'\ntorch.save(model.state_dict(), PATH)","ff114f57":"# from google.colab import drive\n# #https:\/\/www.tripadvisor.com\/Restaurant_Review-g187147-d715030-Reviews-Costes-Paris_Ile_de_France.html\n# drive.mount('\/content\/drive')","87dad8a7":"# "}}