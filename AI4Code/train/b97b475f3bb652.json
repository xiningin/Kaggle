{"cell_type":{"23e1ac35":"code","4d0e6448":"code","84bc86f8":"code","4231c23e":"code","6bdeb846":"code","088f9f53":"code","71ad544a":"code","7818fef6":"code","c924b0d4":"code","78f07208":"code","187a3fda":"code","e982e883":"code","eff25970":"code","ac432258":"code","11c39288":"code","abfe05a2":"code","c2e61cdd":"code","2857c814":"code","828d11db":"code","c63e06a1":"code","ec411382":"code","89586b0c":"code","dce36b18":"code","43d06d52":"code","7a6df0d3":"code","1236a83e":"code","cb3f8714":"code","e7b4ca06":"code","95e585bd":"code","9b41ec9b":"code","43955caa":"code","63167667":"code","05cec537":"code","cc3019a5":"code","d6550eeb":"code","3fa6350e":"code","205a00a5":"code","a29a8480":"code","419e8399":"code","a7dd3409":"code","fb712157":"code","437d8c14":"code","9390bf7f":"code","c108c7f3":"code","608fdb3b":"code","c4592618":"code","521dc9dc":"code","d83d4db1":"code","8d804751":"code","fcd9ca92":"code","a9eb994b":"code","244ee003":"code","27c96265":"code","1441bfc9":"code","acc720ca":"code","085c398c":"code","b5e66f00":"code","ee54ba5a":"code","02ac7a21":"code","73635011":"code","3e1e1e72":"code","98620e89":"code","c407262c":"code","068d6bc7":"code","40765db0":"code","40d0ab95":"code","fa0c472b":"code","88e6dbbb":"code","6fddba15":"code","bfb7be55":"markdown","1931f548":"markdown","455138f2":"markdown","7488c483":"markdown","434e03c1":"markdown","08f37314":"markdown","ecfdd6d1":"markdown","ea458aa0":"markdown","4e3c7787":"markdown","e57ee327":"markdown","c5c6d59b":"markdown","1ef3ceb5":"markdown","2a8faed2":"markdown","9bc8f1d7":"markdown","9f6ead76":"markdown","1bff5148":"markdown","d7848493":"markdown","2606f992":"markdown","0a22bdcb":"markdown","79f5bfc1":"markdown","2b900ec7":"markdown","811dcb8f":"markdown","f7af212a":"markdown","30fb1afb":"markdown","5cdc44a9":"markdown","dd7f9369":"markdown","74edcfa0":"markdown","3b82811d":"markdown","6426e9ac":"markdown","7cd81a72":"markdown","baa67518":"markdown","ccc8d21b":"markdown","f088e0e3":"markdown","3e8f4741":"markdown","a3f2fe8c":"markdown","35fb133b":"markdown","98cec07b":"markdown","0939d7d1":"markdown","d2134d8c":"markdown","792f4837":"markdown","79987ad5":"markdown","22512037":"markdown","f3ae9857":"markdown","b98b2d1b":"markdown","e03dd150":"markdown","6dc0bc77":"markdown","f86fe0f3":"markdown","8c66bd62":"markdown","c01d644b":"markdown","b7b0bd79":"markdown","0fe47dc0":"markdown","0c30cf88":"markdown","f269de0a":"markdown","7f252378":"markdown","9b0719e9":"markdown","43705a78":"markdown","a19f2c48":"markdown","21b4c5f1":"markdown","e05a699f":"markdown","db49139d":"markdown","a28b22dd":"markdown","6a70a665":"markdown","1469efbb":"markdown","34eacbad":"markdown","63b3a3a6":"markdown","f9172d44":"markdown","663af597":"markdown","3521912c":"markdown","886a92c6":"markdown","97dd99ea":"markdown","922ade27":"markdown","a22f3b79":"markdown","046dc931":"markdown","c7519ee9":"markdown","1d2e2281":"markdown","c8d7f5b2":"markdown","1bec028f":"markdown","d8eabf1a":"markdown"},"source":{"23e1ac35":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport nltk\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nimport re\nimport warnings  \nwarnings.filterwarnings('ignore') # to ignore the warnings","4d0e6448":"data = pd.read_csv('..\/input\/mercari-dataset\/train2.csv', encoding='latin1')\n# As most of the words are from Western European Language hence encoding='latin1'\n# Latin-1, also called ISO-8859-1, is an 8-bit character set endorsed by the \n# International Organization for Standardization (ISO) and represents the alphabets \n# of Western European languages. As its name implies, it is a subset of ISO-8859, \n# which includes several other related sets for writing systems like Cyrillic, Hebrew, and Arabic. \n# It is used by most Unix systems as well as Windows. DOS and Mac OS, however, use their own sets.","84bc86f8":"data.head()","4231c23e":"# Data Dimensions\ndata.shape","6bdeb846":"# Data Types\ndata.dtypes","088f9f53":"# Columns in the data\ndata.columns","71ad544a":"# Checking the NAs in the data\ndata.isnull().sum()","7818fef6":"# making a copy of the data as backup\ndata1 = data.copy()","c924b0d4":"for value in ['category_name']:\n    data1[value].fillna(value='Other', inplace=True) \n    # replacing by \"Other\" because there already exists a category by the same name\n\nfor value in ['brand_name']:\n    data1[value].fillna(value='Unknown', inplace=True)\n\n# Rechecking the null values\ndata1.isnull().sum()","78f07208":"data1.price.describe()","187a3fda":"plt.figure(figsize=(17,5))\nplt.title('Price Distribution', fontsize=15)\nsns.boxplot(data1.price, showfliers=False)\nplt.xlabel('Price',fontsize=15)\n\nplt.show()","e982e883":"plt.plot(figsize=(30,20))\nsns.distplot(data1['price'])","eff25970":"plt.subplot(1, 2, 1)\n(data1['price']).plot.hist(bins=50, figsize=(12, 6), edgecolor = 'white', range = [0, 250])\nplt.xlabel('price', fontsize=12)\nplt.title('Price Distribution', fontsize=12)\n\nplt.subplot(1, 2, 2)\nnp.log1p(data1['price']).plot.hist(bins=50, figsize=(12, 6), edgecolor='white')\nplt.xlabel('log(price)+1', fontsize=12)\nplt.title('Price Distribution', fontsize=12)","ac432258":"data1['shipping'].value_counts(normalize=True) # to show as a percentage of total values","11c39288":"index = ['Buyer','Seller']\nvalues =  data1['shipping'].value_counts()\nplt.figure(figsize=(7,4))\nplt.pie(values,startangle=90,autopct='%0.1f%%',explode=(0,0.1))\nplt.legend(title = \"Shipping Paid by\",loc = \"upper right\",labels= index,fontsize=10)\nplt.tight_layout()\nplt.title(\"Analysis on Shipping Paid\",fontsize=20)","abfe05a2":"buyer_charged = data1.loc[data1['shipping'] == 0, 'price']\nseller_charged = data1.loc[data1['shipping'] == 1, 'price']\n\n\nfig, ax = plt.subplots(figsize=(14, 8))\nax.hist(buyer_charged, bins=30, range=[0, 100], label='Price when buyer paid shipping', alpha=0.5, color='b')\nax.hist(seller_charged, bins=30, range=[0, 100], label='Price when seller paid shipping', alpha=0.5, color='r')\nplt.title('Price Distribution by shipping type', fontsize = 20)\nplt.xlabel('Price', fontsize = 15)\nplt.ylabel('No. of Items', fontsize = 15)\nplt.legend(fontsize = 15)\nplt.show()","c2e61cdd":"# average price for shipping for seller and buyer\nprint('The average price is {}'.format(round(seller_charged.mean(), 2)), 'if seller pays shipping')\nprint('The average price is {}'.format(round(buyer_charged.mean(), 2)), 'if buyer pays shipping')","2857c814":"print('There are', data1['category_name'].nunique(), 'unique values in category name column')","828d11db":"data1['category_name'].value_counts()[:10]","c63e06a1":"temp = data1[data1['brand_name']!='Unknown'] # remove Unknown coz it represents missing values\ndata1[['Main_categ','sub_categ1','sub_categ2']] = data1.category_name.str.split(\"\/\",expand = True,n= 2)\nfor i in ['sub_categ1','sub_categ2']:\n    data1[i].fillna(value = \"Label not given\", inplace=True)","ec411382":"print('There are', data1['Main_categ'].nunique(), 'unique values in Main category')\nprint('There are', data1['sub_categ1'].nunique(), 'unique values in Sub-category 1')\nprint('There are', data1['sub_categ2'].nunique(), 'unique values in Sub-category 2')","89586b0c":"maincat_count = data1.Main_categ.value_counts()\n\nplt.figure(figsize=(15, 10))\nsns.barplot(maincat_count.index[0:11], maincat_count[0:11])\nplt.title('Main category Analysis',fontsize = 20)\nplt.xlabel('Main category',fontsize = 15)\nplt.ylabel('Count',fontsize = 15)\nplt.xticks(rotation=15, fontsize=12)\nplt.show()","dce36b18":"temp = data1[data1['price'] < 80]\nplt.figure(figsize=(15, 10))\nsns.boxplot(temp['Main_categ'], temp['price'])\nplt.title('BoxPlot of Main category vs Price', fontsize = 20)\nplt.xlabel('Main Category',fontsize = 15)\nplt.ylabel('Price',fontsize = 15)\nplt.xticks(rotation = 15, fontsize=12)\nplt.show()","43d06d52":"temp.groupby(['Main_categ'])['price'].agg('median')","7a6df0d3":"index = []\n[index.append(key) for key, value in Counter(data1['sub_categ1']).most_common()]\ntop_10 = index[:10]\ntemp = data1[data1['sub_categ1'].isin(top_10)] \n# the corresponding top 10 indexes get their corresponding names from \"sub_categ1\"","1236a83e":"plt.figure(figsize=(20,10))\nsns.countplot(temp['sub_categ1'])\nplt.title('Sub-Category 1 Analysis',fontsize = 20)\nplt.xticks(rotation = 12,wrap = True,fontsize = 15)\nplt.xlabel('Top 10 Sub-Category1',fontsize = 15)\nplt.ylabel('Frequency',fontsize = 15)","cb3f8714":"temp2 = temp[temp['price']<80] # box plot\nplt.figure(figsize=(20,10))\nsns.boxplot(temp2['sub_categ1'],temp2['price'])\nplt.title('Sub-Category 1 vs Price',fontsize = 20)\nplt.xticks(rotation = 10,wrap = True,fontsize = 15)\nplt.xlabel('Top 10 Sub-Category 1',fontsize = 15)\nplt.ylabel('Price',fontsize = 15)","e7b4ca06":"temp2.groupby(['sub_categ1'])['price'].agg('median')","95e585bd":"index = []\n[index.append(key) for key, value in Counter(data1['sub_categ2']).most_common()]\ntop_10 = index[:10]\ntemp = data1[data1['sub_categ2'].isin(top_10)]\n\nplt.figure(figsize=(20,10))\nsns.countplot(temp['sub_categ2'])\nplt.title('Sub-Category 2 Analysis',fontsize = 20)\nplt.xticks(rotation = 7,wrap = True,fontsize = 14)\nplt.xlabel('Top 10 Sub-Category 2',fontsize = 15)\nplt.ylabel('Frequency',fontsize = 15)","9b41ec9b":"temp2 = temp[temp['price']<80]\nplt.figure(figsize=(20,10))\nsns.boxplot(temp2['sub_categ2'],temp2['price'])\nplt.title('Sub-Category 2 vs Price', fontsize = 20)\nplt.xticks(rotation = 10,wrap = True,fontsize = 14)\nplt.xlabel('Top 10 Sub-Category 2', fontsize = 15)\nplt.ylabel('Price', fontsize = 15)","43955caa":"temp2.groupby(['sub_categ2'])['price'].agg('median')","63167667":"print('There are', data1['brand_name'].nunique(), 'unique values in brand name column')","05cec537":"index = []\n[index.append(key) for key, value in Counter(data1['brand_name']).most_common()]\ntop_10 = index[:10]\ntemp = data1[data1['brand_name'].isin(top_10)]\n\nplt.figure(figsize=(20,20))\nplt.subplot(2,1,1)\nsns.countplot(temp['brand_name'])\nplt.title('Brand wise Analysis (with Unknown)', fontsize = 25)\nplt.xticks(rotation = 0,wrap = True,fontsize = 14)\nplt.xlabel('Brand Name',fontsize = 0)\nplt.ylabel('Frequency', fontsize = 20)\n\ntemp = data1[data1['brand_name']!='Unknown']\nindex = []\n[index.append(key) for key, value in Counter(temp['brand_name']).most_common()]\ntop_10 = index[:10]\ntemp2 = temp[temp['brand_name'].isin(top_10)]\n\nplt.subplot(2,1,2)\n# plt.figure(figsize=(40,20))\nsns.countplot(temp2['brand_name'])\nplt.title('Brand wise Analysis (without Unknown)', fontsize = 25)\nplt.xticks(rotation = 0,wrap = True,fontsize = 14)\nplt.xlabel('Brand Name',fontsize = 20)\nplt.ylabel('Frequency',fontsize = 20)\nplt.show()","cc3019a5":"temp3 = temp2[temp2['price']<80]\nplt.figure(figsize=(20,10))\nsns.boxplot(temp3['brand_name'],temp3['price'])\nplt.title(\"Top 10 brand wise Analysis (According to price)\",fontsize=20)\nplt.xlabel(\"Brand Name\",fontsize=15)\nplt.ylabel(\"Price\",fontsize=15)\nplt.xticks(rotation = 0,wrap = True,fontsize = 15)","d6550eeb":"temp3.groupby(['brand_name'])['price'].agg('median')","3fa6350e":"def length(description):\n    count = 0\n    for i in description.split():\n        count+=1\n    return count","205a00a5":"lol=[]\nfor i in data1['item_description']:\n    temp=[]\n    temp.append(i)\n    temp.append(length(str(i)))\n    lol.append(temp)","a29a8480":"print(lol[0:3])","419e8399":"mydf = pd.DataFrame(lol, columns=['Description', 'Description_length'])\nprint(mydf.head(2))","a7dd3409":"data1['Description_length'] = mydf['Description_length']","fb712157":"plt.figure(figsize=(10,5))\nsns.scatterplot(x=data1.Description_length, y=data1.Description_length.value_counts())\nplt.title('Scatter-plot of description length',fontsize=20)\nplt.xlabel(\"Description Length in words\",fontsize=15)\nplt.ylabel(\"Frequency\",fontsize=15)","437d8c14":"temp4=data1[data1['Description_length']<500]\nplt.figure(figsize=(15,5))\nsns.scatterplot(x=temp4['Description_length'], y=data1.price)\nplt.title('Description length vs Price',fontsize=20)\nplt.xlabel(\"Description Length in words\",fontsize=15)\nplt.ylabel(\"Price\",fontsize=15)","9390bf7f":"desc_len_count = data1.Description_length.value_counts()\nprint(desc_len_count[0:3])","c108c7f3":"plt.figure(figsize=(20, 10))\nsns.barplot(desc_len_count.index[0:11], desc_len_count[0:11])\nplt.title('Item Description having most frequent number of words',fontsize=20)\nplt.xticks(rotation = 0,wrap = True,fontsize = 15)\nplt.xlabel('Number of words',fontsize=15)\nplt.ylabel('Frequency',fontsize=15)\nplt.show()","608fdb3b":"from nltk.corpus import stopwords\nstopwords = set(stopwords.words(\"english\"))\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nfrom PIL import Image\n\n# checking length of stop words\nlen(stopwords)\nfrom string import punctuation\npunctuation = list(punctuation)\nstopwords.update(punctuation)\n# adding punctuation to stopwords\n\n\n# wordcloud for Item Description variable with 500 most occuring words\ndoc1 = []\nfor i in range(0,data1.shape[0]):\n    text = str(data1[\"item_description\"][i])\n    text = text.lower()\n    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n    text = nltk.word_tokenize(text)\n    text = [lemmatizer.lemmatize(word) for word in text if word not in stopwords and len(word)>2]\n    text = \" \".join(text)\n    doc1.append(text)\n\n# join string\ndoc2 = \"\".join(doc1)\n\n\n# wordcloud visualization\nimg = np.array(Image.open(\"..\/input\/images\/proj images\/cmt.png\")) \nwordcloud = WordCloud(width=1000,height= 500,relative_scaling=1.0,mask=img,max_words=1000,\n                      background_color='white',\n                      stopwords=stopwords,\n                      min_font_size=10).generate(doc2)\n\nplt.figure(figsize=(15,10), facecolor=None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","c4592618":"from nltk.corpus import stopwords\nfrom PIL import Image\nstp=set(stopwords.words('english'))\nj=0\nplt.figure(figsize=(8,10))\nfor i in data1[\"Main_categ\"].value_counts().index[:6]:\n    text = data1.loc[data1[\"Main_categ\"] == str(i)][\"item_description\"]\n    # text = data1.loc[data1['main_category']==str(i)]\n    # text = text['item_description']\n    text = \"\".join(text)\n    mask = np.array(Image.open('..\/input\/images\/proj images\/'+str(i)+'.png'))\n    wordcloud = WordCloud(width=2000, height=1500, relative_scaling=1.0, max_words=100, mask=mask,\n                          background_color='white',\n                          stopwords=stp,\n                          min_font_size=10,).generate(text)\n# # With relative_scaling=1, a word that is twice as frequent will have twice the size. \n# # If you want to consider the word frequencies and not only their rank, relative_scaling around .5 often looks good. \n# # If \u2018auto\u2019 it will be set to 0.5 unless repeat is true, in which case it will be set to 0.    \n    \n#     # plot the WordCloud image\n    if j < 6:\n        j = j + 1\n        plt.subplot(3,2,j)\n        plt.imshow(wordcloud)\n        plt.axis(\"off\")\n        plt.show()","521dc9dc":"data2=data1.copy()","d83d4db1":"from nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nfrom nltk.corpus import stopwords\nstp=set(stopwords.words('english'))\n\n\ndef tokenize(text):\n    text = str(text)\n    text = text.lower()\n    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n    text = nltk.word_tokenize(text)\n    text = [lemmatizer.lemmatize(word) for word in text if word not in stp and len(word) > 2]\n    return text\n\n\ndata2['tokens'] = data2['item_description'].map(tokenize)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df=10,\n                             max_features=180000,\n                             tokenizer=tokenize,\n                             )\n\ndata3 = data2.sample(n=15000)\nvz = vectorizer.fit_transform(list(data3['item_description']))\n\ndf_idf = pd.DataFrame(vectorizer.idf_, index=vectorizer.get_feature_names(),columns=[\"tfidf\"])\n\n\n# Given the high dimension of our tfidf matrix, we need to reduce their dimension using the \n# Singular Value Decomposition (SVD) technique. And to visualize our vocabulary, we could next use t-SNE to \n# reduce the dimension from 50 to 2. t-SNE is more suitable for dimensionality reduction to 2 or 3.\n\nfrom sklearn.decomposition import TruncatedSVD\n\nn_comp=30\nsvd = TruncatedSVD(n_components=n_comp, random_state=0)\nsvd_tfidf = svd.fit_transform(vz)\n\nfrom sklearn.manifold import TSNE\ntsne_model = TSNE(n_components=2, random_state=0, n_iter=1000,perplexity=75)\n\ntsne_tfidf= tsne_model.fit_transform(svd_tfidf)","8d804751":"from sklearn.decomposition import TruncatedSVD\n\nn_comp=30\nsvd = TruncatedSVD(n_components=n_comp, random_state=0)\nsvd_tfidf = svd.fit_transform(vz)","fcd9ca92":"from sklearn.manifold import TSNE\ntsne_model = TSNE(n_components=2, random_state=0, n_iter=1000,perplexity=75)\n\ntsne_tfidf= tsne_model.fit_transform(svd_tfidf)","a9eb994b":"from sklearn.cluster import KMeans\ntfidf_df = pd.DataFrame(tsne_tfidf, columns=['x', 'y'])\n\ntemp = tfidf_df.copy() \nmodel_kmeans = KMeans(n_clusters=10,random_state=10).fit(tfidf_df)\ntemp['description'] = data2['item_description']\ntemp['tokens'] = data2['tokens']\ntemp['category'] = data2['Main_categ']\ntemp['cluster'] = model_kmeans.predict(tfidf_df)","244ee003":"from bokeh.plotting import figure,show\nfrom bokeh.io import output_file, output_notebook\nfrom bokeh.transform import linear_cmap\nfrom bokeh.palettes import Category10\nfrom bokeh.resources import INLINE\nimport bokeh.io\nbokeh.io.output_notebook(INLINE)\n\n\n\ntemp.to_csv(\"tfidf_tsne.csv\")\nsample = pd.read_csv(\".\/tfidf_tsne.csv\")\ntooltip = [(\"Description\",\"@description\"),\n             (\"Tokens\",\"@tokens\"),\n            (\"Category\",\"@category\"),\n           (\"Cluster\",\"@cluster\")\n          ]\nmapper = linear_cmap(field_name='cluster', palette=Category10[10],low=0,high=9)\n#output_notebook()\np= figure(plot_width=900, plot_height=900,\n          title=\"K-Means clustering of the item description\",\n          tools=\"pan,wheel_zoom,box_zoom,reset,hover\",\n          tooltips = tooltip)\n\np.scatter(\"x\",\"y\",source = sample, alpha=0.7,color = mapper)\n# output_file(\"output_file_name.html\")\n# output_notebook()\n\n\n# from IPython.display import IFrame\n# IFrame(src='output_file_name.html', width=900, height=900)\nshow(p)","27c96265":"plt.figure(figsize=(15, 10))\nsns.boxplot(x=data1['item_condition_id'], y=np.log1p(data1.price))\nplt.title('Box Plot of item condition id VS Product price',fontsize=20)\nplt.xlabel('Item Condition ID',fontsize=15)\nplt.ylabel('log(price+1)',fontsize=15)\nplt.show()","1441bfc9":"data1.groupby(['item_condition_id'])['price'].agg('median')","acc720ca":"from collections import Counter\nindex = []\n[index.append(key) for key, value in Counter(data1['Main_categ']).most_common()]\n\n\nf,axes = plt.subplots(5, 2, figsize=(15, 30))\n\nfor i in range(data1['Main_categ'].nunique()):\n    sns.countplot(data1[data1['Main_categ'] == index[i]]['item_condition_id'], ax = axes[int(i\/2)][i%2])\n    axes[int(i \/ 2)][i % 2].set_title(index[i],fontsize=15)\n    axes[int(i \/ 2)][i % 2].set_xlabel('Item Condition ID')\n    axes[int(i \/ 2)][i % 2].set_ylabel('Frequency')","085c398c":"from collections import Counter\ntemp=data1[data1['brand_name']!='Unknown']\nindex = []\n[index.append(key) for key, value in Counter(temp['brand_name']).most_common()]\ntop_10 = index[:10]\ntemp2 = temp[temp['brand_name'].isin(top_10)]\n\nf,axes = plt.subplots(5, 2, figsize=(15, 30))\n\nfor i in range(temp2['brand_name'].nunique()):\n    sns.countplot(temp2[temp2['brand_name'] == index[i]]['item_condition_id'], ax = axes[int(i\/2)][i%2])\n    axes[int(i \/ 2)][i % 2].set_title(index[i],fontsize=15)\n    axes[int(i \/ 2)][i % 2].set_xlabel('Item Condition ID')\n    axes[int(i \/ 2)][i % 2].set_ylabel('Frequency')","b5e66f00":"data2 = data1.loc[data1['price'] > 0]","ee54ba5a":"data2.head()","02ac7a21":"feature_cols = ['item_condition_id', 'Main_categ', 'sub_categ1','sub_categ2', 'brand_name', 'shipping']\nx = data2[feature_cols] # Independent variables\ny = np.log(data2['price']) # Dependent variable","73635011":"cat_name = pd.get_dummies(data2[\"Main_categ\"], drop_first=True) # drop_first=True means dropping the redundant column\nbrd_name = pd.get_dummies(data2[\"brand_name\"], drop_first=True)\nsub1_name = pd.get_dummies(data2[\"sub_categ1\"], drop_first=True)\nsub2_name = pd.get_dummies(data2[\"sub_categ2\"], drop_first=True)\n\n# removing the categorical columns\nx.drop(['Main_categ', 'brand_name','sub_categ1', 'sub_categ2'], axis=1, inplace=True)\nx1 = pd.concat([x, cat_name, brd_name, sub1_name, sub2_name], axis=1)","3e1e1e72":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x1, y, test_size=0.3, random_state=10)","98620e89":"from statsmodels.api import OLS\nregr = OLS(y_train, x_train)\nresult = regr.fit()\nprint(\"Summary of OLS Model:-\")\nprint(result.summary())","c407262c":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(x_train, y_train)\n\ny_pred_train = model.predict(x_train) # Predicting the value of train to compare with actual value\ny_pred_test = model.predict(x_test) # Predicting the value of test data to compare with actual value\n\nfrom sklearn.metrics import mean_squared_error\nprint(\"RMSLE value of Training Data is {a}\".format(a=np.sqrt(mean_squared_error(y_train,y_pred_train))))\nprint(\"RMSLE value of Testing Data is {a}\".format(a=np.sqrt(mean_squared_error(y_test,y_pred_test))))","068d6bc7":"from sklearn.linear_model import Ridge\n# from sklearn.model_selection import GridSearchCV\n# parameters = {\"alpha\":[0.01,0.1,0,1,10,100]}\n# ridgeReg = Ridge(solver = \"lsqr\", fit_intercept=False)\n# lr_reg = GridSearchCV(ridgeReg,param_grid =parameters,n_jobs=-1)\n# lr_reg.fit(x_train, y_train)\n\n# By applying GridsearchCV, we get to know that the best parameter for the model is given by\n# lr_reg.best_params_\n# Output- {'alpha': 0.1}\n\nridgeReg = Ridge(alpha=0.1,solver=\"lsqr\",fit_intercept=False)\nridgeReg.fit(x_train,y_train)\n\ny_pred = ridgeReg.predict(x_test)\ny_pred_train = ridgeReg.predict(x_train)\nfrom sklearn.metrics import r2_score,mean_squared_error\nrmse_test_ridge = np.sqrt(mean_squared_error(y_test,y_pred))\nrmse_train_ridge = np.sqrt(mean_squared_error(y_train,y_pred_train))\nprint(\"RMSLE value of Training Data is {a}\".format(a=rmse_train_ridge))\nprint(\"RMSLE value of Testing Data is {a}\".format(a=rmse_test_ridge))","40765db0":"print('Min value of y_test is',  min(y_test))\nprint('Max value of y_test is',  max(y_test))","40d0ab95":"from sklearn.linear_model import SGDRegressor,Ridge\n\n# parameters = {\"alpha\": [0.01,0.1,0,1,10,100],\n#               \"l1_ratio\": [0.4,0.5,0.6,0.7,0.8],\n#               }\n#\n# model_sgd = SGDRegressor(loss=\"squared_loss\",penalty=\"l2\",\n#                          learning_rate=\"invscaling\",max_iter=100,\n#                          fit_intercept=False)\n#\n# model_SGD = GridSearchCV(model_sgd,param_grid=parameters)\n# model_SGD.fit(x_train,y_train)\n\n\n# By applying GridsearchCV, we get to know that the best parameter for the model is given by\n# print(model_SGD.best_params_)\n# Output -  {'alpha': 0, 'l1_ratio': 0.4}\n\nmodel_SGD = SGDRegressor(loss=\"squared_loss\",penalty=\"l2\",\n                         learning_rate=\"invscaling\",max_iter=100,\n                         fit_intercept=False,alpha=0,l1_ratio=0.4)\n\nmodel_SGD.fit(x_train,y_train)\ny_pred_train_sgd = model_SGD.predict(x_train)\ny_pred_test_sgd = model_SGD.predict(x_test)\n\nfrom sklearn.metrics import r2_score,mean_squared_error\n\nrmse_train = np.sqrt(mean_squared_error(y_train,y_pred_train_sgd))\nrmse_test = np.sqrt(mean_squared_error(y_test,y_pred_test_sgd))\nprint(\"RMSLE value of Training Data is {a}\".format(a=rmse_train))\nprint(\"RMSLE value of Testing Data is {a}\".format(a=rmse_test))","fa0c472b":"# from sklearn.model_selection import RandomizedSearchCV\n\n# param_dist = {'n_estimators': [10,50,100,150,160,200,300],\n#                'min_samples_split': [2,3,5,6],\n#               \"max_depth\":[None,10,20,40,60,80]\n#               }\n# regr1 = RandomForestRegressor()\n# n_iter_search = 50\n# regr1 = RandomizedSearchCV(regr1, param_distributions=param_dist,\n#                             n_iter=n_iter_search, cv=3,n_jobs=-1)\n\n# regr1.fit(x_train, y_train)\n\n# print(regr1.best_params_)\n# output - {'n_estimators': 200, 'min_samples_split': 3,'max_depth=40'}","88e6dbbb":"from sklearn.ensemble import RandomForestRegressor\n\nmodel_rf = RandomForestRegressor(n_jobs=-1,min_samples_split=3,n_estimators=200,max_depth=40)\nmodel_rf.fit(x_train,y_train)\n\ny_pred_train_rf = model_rf.predict(x_train)\ny_pred_test_rf = model_rf.predict(x_test)\n\nfrom sklearn.metrics import mean_squared_error\n\nrmse_train_rf = np.sqrt(mean_squared_error(y_train,y_pred_train_rf))\nrmse_test_rf = np.sqrt(mean_squared_error(y_test,y_pred_test_rf))\n\nprint(\"RMSLE value of Training Data is {a}\".format(a=rmse_train_rf))\nprint(\"RMSLE value of Testing Data is {a}\".format(a=rmse_test_rf))","6fddba15":"from prettytable import PrettyTable\n\nx = PrettyTable()\n\nx.field_names = [\"Models\", \"HyperParameters used\",  \"RMSLE\"]\n\nx.add_row([\"Ordinary Least Squares Regression\", \"---\", '---'])\nx.add_row([\"Linear Regression\", \"---\", 2023340540.2473302])\nx.add_row([\"Ridge Regression\", \"alpha\", 0.62])\nx.add_row([\"SGD Regressor\",\"alpha, l1_ratio\" , 0.64])\nx.add_row([\"Random Forest Regressor \",\"n_estimators, min_samples_split, max_depth\" , 0.61])\n\nprint(x)","bfb7be55":"**Let us check what length of item description occurs the most**","1931f548":"**Insight 1 from shipping:**\n- `0`- 82071 times (buyer charged) `55.53%`\n- `1`- 66182 times(seller charged) `44.64%`\n- Over 55% of items' shipping fee were paid by the buyers\n\n**Suggestion to the seller:**\n\n- The above insight matches with our perception that the sellers need to keep a lower price to compensate for the additional shipping","455138f2":"Let\u2019s find out which of the products rank the highest in terms of frequency of occurrence:","7488c483":"### Item Condition For every Main Category","434e03c1":"**As we know that nothing is free in this world so we can say that price of data being zero is not possible. We observed that there are some rows which have `price` as 0. Hence we remove those rows**","08f37314":"**Insight 2 from shipping:**\n- The price when buyer pays is high compared to when seller pays\n\nChekcing the Actual values - ","ecfdd6d1":"## 1. OLS(Ordinary Least Squares Regression) Model:","ea458aa0":"### Interpretation of RMSLE value:\n\n- It is a way of figuring out how much a model disagrees with the actual data\n- The lower this number is, the better the fit of the model\n- It is preferred that the value of RMSLE should be close to the minimum value of the response variable in test data ","4e3c7787":"### 6. Item Condition:","e57ee327":"First we create a function to return us a count of words from each Description","c5c6d59b":"### Brand name vs Price:","1ef3ceb5":"**Insight from Ridge Model:**\n\nAs we can see that **0.62** is *close* to **1.09**, we can conclude that our model is able to predict the value close to the actual value","2a8faed2":"**Insights from Linear Model:**\n- We can observe that training data has very less rmsle but the testing data the rmsle is extremely high.\n\n**Problem:**\n- The data is containing multi-collinearity which can be seen in the RMSLE of testing data\n\n**Solution:**\n\n- Almost 99% of our columns are the dummies representing the categorical data and we cannot remove them and it would be futile to check the VIF of over 3500 columns\n- Hence, we have to use algorithms which we can fine tune to optimize our model\n- This leads us to a conclusion that there is a need to build models which can help us apply techniques like Dimension reduction, Regularization, Boosting or Bagging. Let's check them out one by one\n- In ML, the ideal model is the one which has low bias i.e it can capture true relationship and low variability by producing consistent predictions across different datasets\n- The methods used to do this are -\n\n\t\u2022 Regularization\n\t\u2022 Boosting\n\t\u2022 Bagging \n","9bc8f1d7":"**Insight 2 from Main category:**\n\n- Although there are maximum products purchased in the `Women's` category, the price of `Men` category products(20 dollars) is almost as expensive as the products in `Women` category(19 dollars)","9f6ead76":"### 5. Item Description:","1bff5148":"## 3. Ridge Regression:\n \n**Overview:**\n\n- Ridge Regression comes under the Reglarization method\n- It provides a way to create a simple model with great explanatory power when number of predictor variables in a dataset are very high and not very significant or when a datset has multi-collinearity\n- It uses a type of Shrinkage Parameter called ridge estimator which belongs to a class of L2 regularization (\u03bb)\n- L2 regularization adds a penalty which equals the square of the magnitude of co-efficient\n- All co-effs are shrunk by the same factor\n- When \u03bb = 0, ridge regression equals least squares regression and when \u03bb = infinity, all co-effs are shrunk to 0\n- Our goal here is to find the perfect \u03bb value because as \u03bb increases, error due to bias increases and as \u03bb decreases, error due to variance increases\n\n**Note**:- In python, While modelling the **\u03bb** is considered as **alpha**","d7848493":"**Insight 3 from Item Description:**\n\n- This tells us that there are a lot of item descriptions having 3 words in it\n- The item descriptions with shorter length are siginificantly higher than longer description lengths","2606f992":"## Interactive Clustering graph with bokeh library:\n\n### Step 1- Pre-Processing: tf-idf\ntf-idf is the acronym for Term Frequency\u2013inverse Document Frequency. It quantifies the importance of a particular word in relative to the vocabulary of a collection of documents or corpus. The metric depends on two factors:\n\nTerm Frequency: the occurences of a word in a given document (i.e. bag of words)\nInverse Document Frequency: the reciprocal number of times a word occurs in a corpus of documents\nThink about of it this way: If the word is used extensively in all documents, its existence within a specific document will not be able to provide us much specific information about the document itself. So the second term could be seen as a penalty term that penalizes common words such as \"a\", \"the\", \"and\", etc. tf-idf can therefore, be seen as a weighting scheme for words relevancy in a specific document.","0a22bdcb":"**Insight 2 from Sub-Category 2:**\n\n- Yet again products in the `Athletic` category are bought moderately as they are expensive\n- Products in `Pants,Tights,Leggings` category are bought more as moderately expensive","79f5bfc1":"**Insight 1 from Price**\n- mean = 26, median = 17, min = 0, max = 2000\n- 25% of the products are priced below 10 dollars\n- 50% of products are priced below 17 dollars\n- 75% of products are priced below 29 dollars\n- Also, the maximum price that any product has is 2000 dollars","2b900ec7":"Checking the min and max values of y_test corresponding to our RMSLE value **0.62**","811dcb8f":"### Step 2- Singular Vector Decomposition:\n\n\n- Given the high dimension of our tfidf matrix, we need to reduce their dimension using the Singular Value Decomposition (SVD) technique. \n- And to visualize our vocabulary, we could next use t-SNE to reduce the dimension from 50 to 2. \n- t-SNE is more suitable for dimensionality reduction to 2 or 3.\n- In this case 2 coz we need to plot x-y axis","f7af212a":"Then we create a series which has the description and the length of description in words","30fb1afb":"#### Importing the libraries:","5cdc44a9":"**Insight from Random Forest Model:**\n\n- RMSLE value we obtained here **0.61** is a tiny bit, yet better than the RMSLE we got in Ridge model, thus the best model so far","dd7f9369":"### Main category vs Price:","74edcfa0":"### 3.3 Sub-category 2:","3b82811d":"**Insight 2 from Item Description:**\n    \n- As the item description gets longer the price starts decreasing","6426e9ac":"### Sub-category 1 vs Price: ","7cd81a72":"# Step 3: Final Conclusion","baa67518":"Top 10 most common category names","ccc8d21b":"**Insight 2 from Sub-category 1:**\n\n- Products in `Shoes` category is more expensive than `Women's Handbag` by 1 dollar\n- Products in `Women's Handbag` category is more expensive thus has low frequency \n- Hence its satisfies the logic that Expensive products are bought less number of times","f088e0e3":"### 3.1 Main Category:","3e8f4741":"# Step 2 : Applying Machine Learning Models\n\nComing to our actual problem, we will be now applying machine learing models to give us a price that we should suggest to the seller. As discussed earlier, this is a regression problem, so we will be applying some regressions models based on our data and other factors that we will face during solving.","a3f2fe8c":"Adding only 2nd coloumn `Description_length` from mydf to data1","35fb133b":"### 2. Shipping:","98cec07b":"![Figure_1.png](attachment:Figure_1.png)","0939d7d1":"#### Loading the data:","d2134d8c":"`As the orignal data had over 1.4 million observations, we only take the 10% out of it in the csv file above`","792f4837":"#### To check how shipping is related to the price:","79987ad5":"# Text Processing:\n\nMost of the time, the first steps of an NLP project is to \"tokenize\" your documents, which main purpose is to normalize our texts. The three fundamental stages will usually include:\n\n- break the descriptions into sentences and then break the sentences into tokens\n- remove punctuation and stop words\n- lowercase the tokens\n\n\n### Wordcloud for Item Description:\nTo check frequently occuring most **important words** in the description","22512037":"- We couldn't find `RMSLE` in OLS model and the Linear model had a very huge value\n- **Multi-collinearity** in the data lead us to build models where we could `fine tune` it according to the best parameters provided by **grid search mechanism**\n- Out of all the models we applied, **Random Forest Regressor** gave us the `best` and the `lowest` RMSLE value **0.61**\n- Hence we can deploy this model at the clients machine so that sellers can get an estimate of the price they should set for their products\n- Another solution would be making an app wherein the seller will enter the parameters one by one and after submitting he will get the result as the **Suggested Price** for the inputs provided by him\/her","f3ae9857":"### Important Note:\n1. Not always **R-squared** is the best parameter to judge our model as sometimes no matter what we do, we can never get a proper R-squared value because the way in which the data is built. Hence we look after other deciding parameter called RMSLE.\n2. We are calling **RMSLE** (Root Mean Squared log Error) instead of `RMSE` because we have taken the log of our dependent variable as it wasn't nomrally distributed.","b98b2d1b":"**General Observations from the above insights:**\n    \n- It is seen in this table that a subcategory in the category name is seperated by a slash \"\/\"\n- For ex. Beauty\/Makeup\/Face and Beauty\/Makeup\/Lips\n\n- This means `Beauty` category has a subcategory `MakeUp` and this sub category `MakeUp` is further divided into `Face` and `Lips`\n\n- It is also observed that `Women` apparel has the maximum number of items followed by any other category.\n- The category names are listed by \u2018\/\u2019 delimiter which tells about the main category, sub-category 1 and sub-category 2 of the products. \n- Therefore, to get better idea of each product, we will do feature engineering here and split the category name into 3 different columns namely, \u2018Main_categ\u2019, \u2018sub_categ1\u2019 and \u2018sub_categ2\u2019.\n\n\n**Divide category names into Main category, Sub-category 1 and Sub-category 2:**","e03dd150":"**Spliting the data into train and test to validate the model**","6dc0bc77":"## 5. Random Forest Regression Model:\n\n- Decision trees have the possibility of overfitting and have inaccuracy for predictive learning as not all the variables give less impurity at each split\n- Also they are not flexible with new data\n- This problem is easily overcome by random forests as it combines the simplicity of the decision trees with flexibility resulting in a vast improvement in accuracy\n- Bagging method is used to solve the problem we are facing\n- Variance decreases because of the sub samples created and 1\/3rd of the observations are left for validation purpose","f86fe0f3":"# Mercari's Price Suggestion and Modelling\n\n**Mercari** is an online shopping marketplace which is powered by one of the biggest community of Japan where users can sell pretty much anything.The community wants to offer price suggestions to the sellers but is a tough task as the sellers are enabled to put just about anything, or any bundle of things, on Mercari\u2019s marketplace.\n\n\n### Problem Statement:\n- It can be hard to know how much something\u2019s really worth. Small details can mean big differences in pricing. For example, one of these sweaters cost 335 dollars and the other cost 9.99 dollars. Can you guess which one\u2019s which?\n\n**Sweater A:** Vince, Long sleeve, Turtle neck pullover sweater, Black, Size L, Great condition\n\n**Sweater B:** St. John's Bay Long sleeve, Turtle neck Pullover sweater, Size L, Great condition\n\n- Hence, it's harder to guess or decide what price which product should have\n\n- Product pricing gets even harder at scale, considering just how many products are sold online. Clothing has strong seasonal pricing trends and is heavily influenced by brand names, while electronics have fluctuating prices based on product specs.\n\n\n\n### Challenges to Solve:\n\n- Given details about a product like product category name, brand name, and item condition, our challenge is to build an algorithm that automatically suggests the right product prices\n\n- But if solved rightly, it can eliminate human interference in giving price suggestions of a product and speed up efficiency of the shopping app. That\u2019s when Machine Learning comes to play.\n\n- The task of this Project is to build an algorithm that suggests the right product prices for shopping app from product name, user inputted text descriptions of the product, category name, brand name, item condition, and shipping information.\n\n- The goal is about creating a model that would help sellers to price their products.Pricing should be intermediate between sellers and buyers.\n\n\n### Our Objectives:\n\n1. \u2022 Data Cleaning\n    \u2022 Pre-processing\n\t\u2022 Feature Engineering \n2. Exploratory Data Analysis:\n    - Gathering as many insights as possible through Exploratory Data Analysis. \n    - Trying to see through Descriptive Statistics that what our data is trying to tell us and understand the significance of each and every variable. \n3. Text Processing:\n    - Natural Language Processing (NLP)\n    - Tokenizing and tf-idf algorithm\n    - K-means Clustering\n5. The given problem is exactly about predicting the price, which is a real-valued value, thus it falls into the Regression Category\n5. Trying to provide some interpretability and evolving our ways to handle the upcoming problems as we proceed through our project\n\n\n### Models used in the Project:\n1. Ordinary Least Squares Regression\n2. Linear Regression \n3. Ridge Regression  \n4. Stochastic Gradient Descent Regressor     \n5. Random Forest Regressor\n\n\n## Data Description:\n\n- `train_id` - the id of the product \n\n- `name` - the name of the product\n\n- `item_condition_id` - the condition of the product provided by the seller\n    \n According to information available on Mercari Website\n    - 1 stands for new\n    - 2 stands for fairly new\n    - 3 stands for Good\n    - 4 stands for Bad\n    - 5 stands for Very Poor\n    \n\n- `category_name` - category of the product\n\n- `brand_name` - brand name of the product\n\n- `price` - the price that the product was sold for. (This is the target variable that we will predict) The unit is USD\n\n- `shipping` - 1 if shipping fee is paid by seller and 0 by buyer\n\n- `item_description` - the full description of the item\n\n\n**Independent variables:** `train_id, name, item_condition_id, category_name, brand_name, shipping, item_description`\n\n**Target Variable:** `price`\n\nWe will build various supervised machine learning regression models and see which succeeds in solving the given mapping between the input variables and the price feature in the best way. Let\u2019s begin in a step-by-step manner\n\n\n## Step 1: Exploratory Data Analysis\n- The very first step in solving any case study in data science is to properly look and analyze the data you have. \n- It helps to give valuable insights into the pattern and information it has to convey. \n- Statistical tools have a big role in proper visualization of the data. \n- Even though it is considered not a very important part of solving a problem, but successful data scientists and ML engineers spend maximum part of solving a problem by analyzing the data they have. \n- Proper EDA gives interesting features of your data which in turn influences our data preprocessing and model selection criterion as well.","8c66bd62":"#### Handling missing values:","c01d644b":"### 3. Category Name:\nUnique Category Names","b7b0bd79":"- It can be concluded that the distribution of the `price` variable is heavily right-skewed.\n- The `price` variable follows a skewed distribution and in order to make errors on low price product more relevant than for higher prices, we take the log transform\n- log1p = log(p) + 1, we add 1  to avoid zero(log0=infinity) and negative values\n\nOur dependent variable should be normally distributed i.e. The Assumption of Normality should be satisfied\n\n**Let's compare both the graphs side by side**","0fe47dc0":"## 2. Linear Regression Model:","0c30cf88":"**Insight 3 from Item Condition ID:**\n\n- Brands like `Pink`, `Nike` and `Michael Kors` have more number of New(1) to Good(3) condition items as compared to other brands\n- `American Eagle` doesn't have a single product of poor quality(5)\n- `Apple` has the most number of poor quality(5) products","f269de0a":"### Item Description vs Price:","7f252378":"**Performing One hot encoding on categorical variables**","9b0719e9":"### 4. Brand Name:","43705a78":"**Insight 1 from Brand name:**\n\n- We ignore the Brand name marked as `Unknown` as it represents our missing values\n- Hence we make the graph again without considering the `Unknown`\n- We observe that `Pink`, `Nike`, and `Victoria's Secret` are top 3 most purchased brands\n- Also it is obvious that `Pink` and `Victoria's Secret` brand products are mostly bought from `Women` main category more frequently","a19f2c48":"**Insight from SGD model:**\n\n- We can see that the RMSLE is **0.64** which is slightly greater than the RMSLE in the Ridge Model of **0.62**","21b4c5f1":"After splitting the `category_name` column, the unique items that I have in each of the newly formed columns is listed below:","e05a699f":"**Insight 1 from Item Description:**\n\n- Most of the item descriptions are written in the range of 3-50 words","db49139d":"Checking for best Parameters using Grid search - ","a28b22dd":"### Sub-category 1:\nAs there are 114 unique sub categories, we won't be able to plot them all, hence, first we need to seperate the top 10 Sub-categories","6a70a665":"### Sub-category 2 vs Price:","1469efbb":"**Insight 1 from Sub-category 2:**\n\n- Products under category `Pants,Tights,Leggings` are bought the highest number of times","34eacbad":"As there are 2321 unique brand names, we lookout only for products purchased from the top 10 frequently occuring brands","63b3a3a6":"## 4. SGD(Stochastic Gradient Descent) Model:\n\n**Overview:**\n\n- In statistics, machine learning and data science fields, we optimize a lot of stuff\n- When we fit a line with linear regression, we optimize the slope and intercept\n-  In logistic regression, we optimize the squiggly line\n- In a similar way Gradient descent also helps optimize such things and much more\n- In this case we have almost half billion data points and hence computations can take a long time\n- This method uses a randomly selected subset of data at every step rather than full dataset\n- This reduces the time spent calculating derivatives of loss function (Sum of squared Residuals)","f9172d44":"**Insight 2 for Item Condition:**\n\n- Item condition marked as 1, 2 and 3 are most frequent in `Women`, `Kids`, `Men` and `Vintage` category\n- Item condition marked as 4 and 5 are bought significantly less amount of times\n- `Beauty` and `Handmade` categories have too many Item condition id as 1 as compared to other Item Condition ID\n- The most number of poor items which marked as 5 are found in the `Electronics` category\n\n### Item Condition for Top 10 brands","663af597":"**Insight 2 from Brand name:**\n\n- `Michael Kors` products cost high hence less bought\n- `Lululemon` has the 2nd highest median price \n- `Pink`, `Nike` and `Victoria's Secret` products have moderate pricing hence are more frequently bought","3521912c":"Making a new dataframe of the series we created - `lol`","886a92c6":"## Analysis on each Variable:\n### 1. Price:","97dd99ea":"### Step 3 - t Distributed Stochastic Neighbor Embedding (t-SNE)\n\n- t-SNE is a technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. \n- The goal is to take a set of points in a high-dimensional space and find a representation of those points in a lower-dimensional space, typically the 2D plane. \n- It is based on probability distributions with random walk on neighborhood graphs to find the structure within the data. But since t-SNE complexity is significantly high, usually we'd use other high-dimension reduction techniques before applying t-SNE.","922ade27":"### WordCloud for Item Description of Top 6 Categories","a22f3b79":"### Step 4- K means Clustering:\n\n- K-means clustering obejctive is to minimize the average squared Euclidean distance of the document \/ description from their cluster centroids.","046dc931":"Printing first three rows to check the string and the number of words in the string match or not","c7519ee9":"**Insights from OLS model:**\n\n- As we can see that the R-squared is almost close to 1 which means the predicted value of the training data is very similar to that of the test data\n- The 2nd warning tells us that there are strong multi-collinearity problems or that the design matrix is singular because of the one hot encoding and many unique values in our data columns\n- Hence this is one of the major drawbacks of this model in our case\n- Other factors like Skewness (0.5), Kurtosis (4.7) i.e. almost close to Mesokurtic curve and Auto-correlation (1.99) tell us that model is optimal, however we still can't use this model to know the RMSLE as well as to predict the values\n- For predicting the values for testing data we use linear regression and see if we can overcome the OLS model's drawbacks","1d2e2281":"**Insight 1 from Main Category:**\n- It can be said that `Women` products occur with the maximum frequency, followed by `Beauty` products. The 3rd largest general category is owned by `Kids` products","c8d7f5b2":"**Insight 1 from Sub-category 1:**\n\n- Most of the products from Sub-category 1 are bought from `Athletic Apparel` and `Makeup` category ","1bec028f":"**Seperating dependent and independent variables**","d8eabf1a":"**Insight 1 from Item Condition:**\n\n- The median of Item condition as 5 is higher than the others which states that items with item condition 5 were more expensive than others"}}