{"cell_type":{"f8b69ee8":"code","c16d98ed":"code","85a5229c":"code","e6b8ea7e":"code","ca83aa7a":"code","75808fd8":"code","e1122491":"code","26b57739":"code","10fd8e25":"code","86614939":"code","c98879b3":"code","f66cf9bf":"code","ed81602f":"code","adb39bbc":"code","40e4a2c9":"code","82d05a02":"code","c409ecc2":"code","7eeda1ca":"code","f94876c5":"code","ea0c09cb":"code","16d51a97":"code","b4d75d91":"code","c2e332fa":"code","af1cfc7f":"code","44b5ba20":"code","5d0b4fa3":"code","bce490a6":"code","0d34fd1a":"code","5e65b41d":"code","38589592":"code","db60e551":"code","379b6d97":"markdown","b52e984f":"markdown","fb5778d1":"markdown","a3b52c42":"markdown","6e973aff":"markdown","6c3ae96c":"markdown","4021a7b8":"markdown","8fdaec66":"markdown","048e49e2":"markdown","4091a672":"markdown","026a7a4c":"markdown","984b4245":"markdown","0fdec4c0":"markdown","406dc25a":"markdown"},"source":{"f8b69ee8":"from sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\nfrom scipy.misc import imread\nfrom scipy import sparse\nimport scipy.stats as ss\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec \nimport seaborn as sns\nfrom wordcloud import WordCloud ,STOPWORDS\nfrom PIL import Image\nimport matplotlib_venn as venn\n\nimport string\nimport re    #for regex\nimport nltk\nfrom nltk.corpus import stopwords\nimport spacy\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\n# Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import TweetTokenizer\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nimport gc\nimport time\nimport warnings\n\n#settings\nstart_time=time.time()\ncolor = sns.color_palette()\nsns.set_style(\"dark\")\neng_stopwords = set(stopwords.words(\"english\"))\nwarnings.filterwarnings(\"ignore\")\n\nlem = WordNetLemmatizer()\ntokenizer=TweetTokenizer()\n\n%matplotlib inline","c16d98ed":"!pip install transformers","85a5229c":"import sys\nimport copy\nimport torch \nfrom scipy.sparse import *\nfrom sklearn.metrics import roc_auc_score\nimport pyarrow as pa\n\nimport torch.nn as nn\nfrom torch.optim import lr_scheduler\nimport torch.nn.functional as F\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import Dataset,DataLoader\nfrom transformers import DistilBertConfig,DistilBertTokenizer,DistilBertModel\n\n\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")","e6b8ea7e":"train=pd.read_csv(\"..\/input\/toxic-comments\/train.csv\")\ntest=pd.read_csv(\"..\/input\/toxic-comments\/test.csv\")","ca83aa7a":"train.head()","75808fd8":"print(\"Check for missing values in Train dataset\")\nnull_check=train.isnull().sum()\nprint(null_check)\nprint(\"Check for missing values in Test dataset\")\nnull_check=test.isnull().sum()\nprint(null_check)\nprint(\"filling NA with \\\"unknown\\\"\")\ntrain[\"comment_text\"].fillna(\"unknown\", inplace=True)\ntest[\"comment_text\"].fillna(\"unknown\", inplace=True)","e1122491":"x=train.iloc[:,2:].sum()\n#marking comments without any tags as \"clean\"\nrowsums=train.iloc[:,2:].sum(axis=1)\ntrain['clean']=(rowsums==0)\n#count number of clean entries\ntrain['clean'].sum()\nprint(\"Total comments = \",len(train))\nprint(\"Total clean comments = \",train['clean'].sum())\nprint(\"Total tags =\",x.sum())","26b57739":"x=train.iloc[:,2:].sum()\n#plot\nplt.figure(figsize=(8,4))\nax= sns.barplot(x.index, x.values, alpha=0.8)\nplt.title(\"# per class\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('Type ', fontsize=12)\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()","10fd8e25":"x=rowsums.value_counts()\n\n#plot\nplt.figure(figsize=(8,4))\nax = sns.barplot(x.index, x.values, alpha=0.8,color=color[2])\nplt.title(\"Multiple tags per comment\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('# of tags ', fontsize=12)\n\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()","86614939":"merge=pd.concat([train.iloc[:,0:2],test.iloc[:,0:2]])\ndf=merge.reset_index(drop=True)","c98879b3":"#Sentense count in each comment:\n    #  '\\n' can be used to count the number of sentences in each comment\ndf['count_sent']=df[\"comment_text\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n#Word count in each comment:\ndf['count_word']=df[\"comment_text\"].apply(lambda x: len(str(x).split()))\n#Unique word count\ndf['count_unique_word']=df[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n#Letter count\ndf['count_letters']=df[\"comment_text\"].apply(lambda x: len(str(x)))\n#punctuation count\ndf[\"count_punctuations\"] =df[\"comment_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n#upper case words count\ndf[\"count_words_upper\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n#title case words count\ndf[\"count_words_title\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n#Number of stopwords\ndf[\"count_stopwords\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n#Average length of the words\ndf[\"mean_word_len\"] = df[\"comment_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","f66cf9bf":"#derived features\n#Word count percent in each comment:\ndf['word_unique_percent']=df['count_unique_word']*100\/df['count_word']\n#derived features\n#Punct percent in each comment:\ndf['punct_percent']=df['count_punctuations']*100\/df['count_word']","ed81602f":"#serperate train and test features\ntrain_feats=df.iloc[0:len(train),]\ntest_feats=df.iloc[len(train):,]\n#join the tags\ntrain_tags=train.iloc[:,2:]\ntrain_feats=pd.concat([train_feats,train_tags],axis=1)","adb39bbc":"train_feats['count_sent'].loc[train_feats['count_sent']>10] = 10 \nplt.figure(figsize=(12,6))\n## sentenses\nplt.subplot(121)\nsns.violinplot(y='count_sent',x='clean', data=train_feats,split=True)\nplt.xlabel('Clean?', fontsize=12)\nplt.ylabel('# of sentences', fontsize=12)\nplt.title(\"Number of sentences in each comment\", fontsize=15)\n# words\ntrain_feats['count_word'].loc[train_feats['count_word']>200] = 200\nplt.subplot(122)\nsns.violinplot(y='count_word',x='clean', data=train_feats,split=True,inner=\"quart\")\nplt.xlabel('Clean?', fontsize=12)\nplt.ylabel('# of words', fontsize=12)\nplt.title(\"Number of words in each comment\", fontsize=15)\n\nplt.show()","40e4a2c9":"train_feats['count_unique_word'].loc[train_feats['count_unique_word']>200] = 200\n#prep for split violin plots\n#For the desired plots , the data must be in long format\ntemp_df = pd.melt(train_feats, value_vars=['count_word', 'count_unique_word'], id_vars='clean')\n#spammers - comments with less than 40% unique words\nspammers=train_feats[train_feats['word_unique_percent']<30]","82d05a02":"plt.figure(figsize=(16,12))\ngridspec.GridSpec(2,2)\nplt.subplot2grid((2,2),(0,0))\nsns.violinplot(x='variable', y='value', hue='clean', data=temp_df,split=True,inner='quartile')\nplt.title(\"Absolute wordcount and unique words count\")\nplt.xlabel('Feature', fontsize=12)\nplt.ylabel('Count', fontsize=12)\n\nplt.subplot2grid((2,2),(0,1))\nplt.title(\"Percentage of unique words of total words in comment\")\n#sns.boxplot(x='clean', y='word_unique_percent', data=train_feats)\nax=sns.kdeplot(train_feats[train_feats.clean == 0].word_unique_percent, label=\"Bad\",shade=True,color='r')\nax=sns.kdeplot(train_feats[train_feats.clean == 1].word_unique_percent, label=\"Clean\")\nplt.legend()\nplt.ylabel('Number of occurances', fontsize=12)\nplt.xlabel('Percent unique words', fontsize=12)\n\nx=spammers.iloc[:,-7:].sum()\nplt.subplot2grid((2,2),(1,0),colspan=2)\nplt.title(\"Count of comments with low(<30%) unique words\",fontsize=15)\nax=sns.barplot(x=x.index, y=x.values,color=color[3])\n\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom')\n\nplt.xlabel('Threat class', fontsize=12)\nplt.ylabel('# of comments', fontsize=12)\nplt.show()","c409ecc2":"## Feature engineering to prepare inputs for BERT....\n\n\nY = train[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].astype(float)\nX = train['comment_text']\n\n\nX_train, X_test, y_train, y_test = train_test_split( X, Y, test_size=0.33, random_state=42)","7eeda1ca":"print('train_x shape is {}' .format({X_train.shape}))\nprint('test_x shape is {}' .format({X_test.shape}))\nprint('train_y shape is {}' .format({y_train.shape}))","f94876c5":"X_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","ea0c09cb":"def accuracy_thresh(y_pred, y_true, thresh:float=0.4, sigmoid:bool=True):\n    \"Compute accuracy when `y_pred` and `y_true` are the same size.\"\n    if sigmoid: y_pred = y_pred.sigmoid()\n#     return ((y_pred>thresh)==y_true.byte()).float().mean().item()\n    return np.mean(((y_pred>thresh).float()==y_true.float()).float().cpu().numpy(), axis=1).sum()\n#Expected object of scalar type Bool but got scalar type Double for argument #2 'other'\n\n","16d51a97":"config = DistilBertConfig(vocab_size_or_config_json_file=32000,dropout=0.1, num_labels=6, intermediate_size=3072 )","b4d75d91":"\n\nclass DistilBertForSequenceClassification(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.num_labels = config.num_labels\n\n        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n        self.pre_classifier = nn.Linear(config.hidden_size, config.hidden_size)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        self.dropout = nn.Dropout(config.seq_classif_dropout)\n\n        nn.init.xavier_normal_(self.classifier.weight)\n\n    def forward(self, input_ids=None, attention_mask=None, head_mask=None, labels=None):\n        distilbert_output = self.distilbert(input_ids=input_ids,\n                                            attention_mask=attention_mask,\n                                            head_mask=head_mask)\n        hidden_state = distilbert_output[0]                    \n        pooled_output = hidden_state[:, 0]                   \n        pooled_output = self.pre_classifier(pooled_output)   \n        pooled_output = nn.ReLU()(pooled_output)             \n        pooled_output = self.dropout(pooled_output)        \n        logits = self.classifier(pooled_output) \n        return logits\n\n","c2e332fa":"\n\nmax_seq_length = 256\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n\n\nclass text_dataset(Dataset):\n    def __init__(self,x,y, transform=None):\n        \n        self.x = x\n        self.y = y\n        self.transform = transform\n        \n    def __getitem__(self,index):\n        \n        tokenized_comment = tokenizer.tokenize(self.x[index])\n        \n        if len(tokenized_comment) > max_seq_length:\n            tokenized_comment = tokenized_comment[:max_seq_length]\n            \n        ids_review  = tokenizer.convert_tokens_to_ids(tokenized_comment)\n\n        padding = [0] * (max_seq_length - len(ids_review))\n        \n        ids_review += padding\n        \n        assert len(ids_review) == max_seq_length\n        \n        #print(ids_review)\n        ids_review = torch.tensor(ids_review)\n        \n        hcc = self.y[index] # toxic comment        \n        list_of_labels = [torch.from_numpy(hcc)]\n        \n        \n        return ids_review, list_of_labels[0]\n    \n    def __len__(self):\n        return len(self.x)\n \n\n","af1cfc7f":"text_dataset(X_train, y_train).__getitem__(6)[1]   ### Testing index 6 to see output","44b5ba20":"batch_size = 32\n\n\ntraining_dataset = text_dataset(X_train,y_train)\n\ntest_dataset = text_dataset(X_test,y_test)\n\ndataloaders_dict = {'train': torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=False),\n                   'val':torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n                   }\ndataset_sizes = {'train':len(X_train),\n                'val':len(X_test)}\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = DistilBertForSequenceClassification(config)\nmodel.to(device)\n\nprint(device)","5d0b4fa3":"def train_model(model, criterion, optimizer, scheduler, num_epochs=2):\n    model.train()\n    since = time.time()\n    print('starting')\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss = 100\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch+1, num_epochs))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                scheduler.step()\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            \n            beta_score_accuracy = 0.0\n            \n            micro_roc_auc_acc = 0.0\n            \n            \n            # Iterate over data.\n            for inputs, hcc in dataloaders_dict[phase]:\n                \n                inputs = inputs.to(device) \n                hcc = hcc.to(device)\n            \n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    \n                    loss = criterion(outputs,hcc.float())\n                    \n                    if phase == 'train':\n                        \n                        loss.backward()\n                        optimizer.step()\n\n                running_loss += loss.item() * inputs.size(0)\n                \n                micro_roc_auc_acc +=  accuracy_thresh(outputs.view(-1,6),hcc.view(-1,6))\n                \n                #print(micro_roc_auc_acc)\n\n                \n            epoch_loss = running_loss \/ dataset_sizes[phase]\n\n            \n            epoch_micro_roc_acc = micro_roc_auc_acc \/ dataset_sizes[phase]\n\n            print('{} total loss: {:.4f} '.format(phase,epoch_loss ))\n            print('{} micro_roc_auc_acc: {:.4f}'.format( phase, epoch_micro_roc_acc))\n\n            if phase == 'val' and epoch_loss < best_loss:\n                print('saving with loss of {}'.format(epoch_loss),\n                      'improved over previous {}'.format(best_loss))\n                best_loss = epoch_loss\n                best_model_wts = copy.deepcopy(model.state_dict())\n                torch.save(model.state_dict(), 'distilbert_model_weights.pth')\n         \n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(float(best_loss)))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model\n \nprint('done')","bce490a6":"lrlast = .001\nlrmain = 3e-5\n#optim1 = torch.optim.Adam(\n#    [\n#        {\"params\":model.parameters,\"lr\": lrmain},\n#        {\"params\":model.classifier.parameters(), \"lr\": lrlast},\n#       \n#   ])\n\noptim1 = torch.optim.Adam(model.parameters(),lrmain)\n\noptimizer_ft = optim1\ncriterion = nn.BCEWithLogitsLoss()\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.1)","0d34fd1a":"model_ft1 = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=8)","5e65b41d":"#y_test = test[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values\nx_test = test['comment_text']\ny_test = np.zeros(x_test.shape[0]*6).reshape(x_test.shape[0],6)\n\ntest_dataset = text_dataset(x_test,y_test)\nprediction_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n\ndef preds(model,test_loader):\n    predictions = []\n    for inputs, sentiment in test_loader:\n        inputs = inputs.to(device) \n        sentiment = sentiment.to(device)\n        with torch.no_grad():\n            outputs = model(inputs)\n            outputs = torch.sigmoid(outputs)\n            predictions.append(outputs.cpu().detach().numpy().tolist())\n    return predictions","38589592":"predictions = preds(model=model_ft1,test_loader=prediction_dataloader)\npredictions = np.array(predictions)[:,0]","db60e551":"submission = pd.DataFrame(predictions,columns=['toxic','severe_toxic','obscene','threat','insult','identity_hate'])\ntest[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]=submission\nfinal_sub = test[['id','toxic','severe_toxic','obscene','threat','insult','identity_hate']]\nfinal_sub.head()","379b6d97":"Conclusion: long sentences or more words do not seem to be a significant indicator of toxicity.","b52e984f":"Number of comments per class","fb5778d1":"Multiple tags per comment","a3b52c42":"**Reference**\n\n* https:\/\/www.kaggle.com\/jagangupta\/stop-the-s-toxic-comments-eda#Introduction\n* https:\/\/towardsdatascience.com\/building-a-multi-label-text-classifier-using-bert-and-tensorflow-f188e0ecdc5d\n* https:\/\/www.lyrn.ai\/2018\/11\/07\/explained-bert-state-of-the-art-language-model-for-nlp\/\n* https:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/\n* https:\/\/www.kaggle.com\/hpc8899\/distilbert\n","6e973aff":"Make prediction","6c3ae96c":"# 1. Import packages","4021a7b8":"# 2. Import dataset","8fdaec66":"Check for missing values in Train dataset","048e49e2":"# 4.DistilBERT","4091a672":"**Data**\n\nThe data were collected from wiki corpus dataset which was rated by human raters for toxicity. The corpus contains 63M comments from discussions relating to user pages and articles dating from 2004-2015.\n\nThe comments are tagged in the following five categories\n\n*     toxic\n*     severe_toxic\n*     obscene\n*     threat\n*     insult\n*     identity_hate\n","026a7a4c":"# 3. EDA","984b4245":"**If you find this notebook usefull, please upvoat.**","0fdec4c0":"**How BERT works**\n\nBERT makes use of Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. In its vanilla form, Transformer includes two separate mechanisms \u2013 an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERT\u2019s goal is to generate a language model, only the encoder mechanism is necessary. The detailed workings of Transformer are described in a paper by Google.\n\nAs opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that it\u2019s non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word).\n\nWhen training language models, there is a challenge of defining a prediction goal. Many models predict the next word in a sequence - e.g. \u201cThe child came home from ___ a directional approach which inherently limits context learning. To overcome this challenge, BERT uses two training strategies: \n\n**Masked LM (MLM)**\n\nBefore feeding word sequences into BERT, 15% of the words in each sequence are replaced with a [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. In technical terms, the prediction of the output words requires:\n\n*     Adding a classification layer on top of the encoder output.\n*     Multiplying the output vectors by the embedding matrix, transforming them into the vocabulary dimension.\n*     Calculating the probability of each word in the vocabulary with softmax.\n\n**Next Sentence Prediction (NSP)**\n\nIn the BERT training process, the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence.\n\nTo help the model distinguish between the two sentences in training, the input is processed in the following way before entering the model:\n\n* A [CLS] token is inserted at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.\n* A sentence embedding indicating Sentence A or Sentence B is added to each token. Sentence embeddings are similar in concept to token embeddings with a vocabulary of 2.\n* A positional embedding is added to each token to indicate its position in the sequence. The concept and implementation of positional embedding are presented in the Transformer paper\n\nTo predict if the second sentence is indeed connected to the first, the following steps are performed:\n\n*     The entire input sequence goes through the Transformer model.\n*     The output of the [CLS] token is transformed into a 2\u00d71 shaped vector, using a simple classification layer (learned matrices of weights and biases).\n*     Calculating the probability of IsNextSequence with softmax.","406dc25a":"****Objective****\n\nPrepare EDA to understand the data and build the multi-label text classifier using DistilBERT.\n\nDistilBERT is a smaller version of BERT developed and open sourced by the team at HuggingFace. It\u2019s a lighter and faster version of BERT that roughly matches its performance."}}