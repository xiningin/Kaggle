{"cell_type":{"53d62539":"code","d5a5d092":"code","10c62c6b":"code","79e1b5a7":"code","f8303dff":"code","67f9d64c":"code","2aeeb938":"code","cb0cc60d":"markdown"},"source":{"53d62539":"import urllib.request, json \nimport pandas as pd\nimport os\nfrom datetime import date\nimport matplotlib.pylab as plt\nimport re\nplt.style.use('ggplot')\npd.set_option('max_columns', 200)","d5a5d092":"!pip install ratelimit\nfrom ratelimit import limits, sleep_and_retry","10c62c6b":"@sleep_and_retry\n@limits(calls=10, period=1)\ndef readPage(link):\n    headers = {'user-agent': 'Impostor Engineer posta.gereksiz@gmail.com'}\n    urlRecent = link\n    try:\n        reqRecent = urllib.request.Request(urlRecent, headers=headers)\n        with urllib.request.urlopen(reqRecent) as response:\n            if response:\n                the_page = response.read()\n                df = pd.read_html(the_page)\n                df = df[0]\n        return df\n    except:\n        pass\n\ndef tranposeData(data, cik):\n    transposed = pd.DataFrame()\n    regex = r\"\\d{4}\"\n\n    try:\n        for i in range(1,2):\n            # Some tables have MultiIndex column names: \n            if len(data.columns[i]) == 2:\n                transposed.loc[i,'date'] = pd.to_datetime(data.columns[i][1][:re.search(regex, data.columns[i][1]).span()[1]]).date()\n            else:\n                transposed.loc[i,'date'] = pd.to_datetime(data.columns[i][:re.search(regex, data.columns[i]).span()[1]]).date()\n\n        for i, d in data.iterrows():\n            transposed['cik'] = cik\n            transposed.loc[1,d[data.columns[0]]] = d[data.columns[1]]\n\n        transposed = transposed.set_index(['date', 'cik'], drop=True)\n    except:\n        pass\n    return transposed\n\ndef generateDF(folder, cik):\n    dfm = pd.DataFrame()\n    filenames = ['R2.htm', 'R3.htm', 'R4.htm', 'R5.htm', 'R6.htm', 'R7.htm']\n\n    # try: \n    for i in range(6):\n        link = folder + '\/' + filenames[i]\n        try: \n            df = readPage(link)\n        except:\n            pass\n        globals()['dft%s' % i] = tranposeData(df, cik)\n\n    dflist = []\n    dfrawlist = [dft0, dft1, dft2, dft3, dft4, dft5]\n\n    for dft in dfrawlist:\n        if dft.shape[0]>0 and dft.shape[1]>1:\n            dflist.append(dft)\n\n    dfm = pd.concat(dflist, join='inner', axis=1)\n    return dfm\n\ndef combineDF(df):\n    finaldf = pd.DataFrame()\n    for i, d in df.iterrows():\n        df = generateDF(d['folder'], d['cik'])\n        finaldf = finaldf.append(df, ignore_index=True)\n    return finaldf","79e1b5a7":"def combo(df):\n    finanda = pd.DataFrame()\n    for d in df:\n        finanda = finanda.append(d)\n    return finanda","f8303dff":"df1 = generateDF('https:\/\/www.sec.gov\/Archives\/edgar\/data\/0000012040\/000117494721000803\/', '0000012040')\ndf2 = generateDF('https:\/\/www.sec.gov\/Archives\/edgar\/data\/0000012040\/000117494721000570\/', '0000012040')","67f9d64c":"df1cols = df1.columns\ndf2cols = df2.columns\ndf1cols","2aeeb938":"# The reason was there were duplicated column names. I wasn't expecting that. Cannot visually inspect 135 columns anyway. \n# Solution is to actually create a prefix_ for each column with the report name e.g. is_, bs_, cf_ etc. \n\ndf1dupes = [x for n, x in enumerate(df1cols) if x in df1cols[:n]]\ndf2dupes = [x for n, x in enumerate(df2cols) if x in df2cols[:n]]\n\nfor colm in df1cols:\n    if colm in df1dupes:\n        print(f'\\n *Duplicate: {colm} \\n')\n    else:\n        print(colm)","cb0cc60d":"# SEC EDGAR Financial Data Scraping\n- I wrote the sample script to scrape the financial statements, however ran into an issue with appending df. \n- Usually each statement have a unique list of items, i.e. items in Balance Sheet doesn't appear in Income Statement. \n- Some of the forms unfortunately included the same line item that are in other statements. e.g. first line item in Cash Flow Statement is Net Income, which is also found in Income Statement. \n- One way to solve this is to include a prefix_ for each statement to add them as a seperate column, and deal with them later. \n- This resulted in 8500 columns. \n- Another solution is to create a dictionary for all the columns merge as much as possible. e.g. `{'Accounts Receivable': 'rec', ...}`\n- COMPUSTAT database is a good start point. \n- Going through that many columns will be difficult. At the rate of 100 columns a day, will take 3 months. \n- Open to any suggestions, recommendations. "}}