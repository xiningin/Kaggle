{"cell_type":{"b0e63f41":"code","a4c1e5d3":"code","906f1d3d":"code","5921b20f":"code","01ea9484":"code","3422d050":"code","f0ef201b":"code","0e00a213":"code","90b3a1be":"code","a4667f26":"code","b19c66e0":"code","f006f610":"code","602959d5":"code","53ee0e73":"code","dd1efef4":"code","97988c8e":"code","23fa8253":"code","ca1f1f2f":"code","059dc1e0":"code","de325ccb":"code","470685b5":"code","c397a6a2":"code","90e053ea":"code","2871baf0":"code","9a3ba209":"code","0e644769":"code","ef1a2539":"code","7a7db3fc":"markdown","85638553":"markdown","e525b2e4":"markdown","ac2b2ec8":"markdown","2bef5ffc":"markdown","d84571a2":"markdown","8ad48012":"markdown","cf8071ce":"markdown","4bb53d00":"markdown","7e50167f":"markdown","9e9083e6":"markdown","225857d7":"markdown","ba9b67b4":"markdown","ec307192":"markdown","d680e8e8":"markdown","5ca60de1":"markdown","1f40150e":"markdown","883d0d43":"markdown","284ff71b":"markdown","2860652a":"markdown","60c09d98":"markdown","8e64f1b0":"markdown","dc430770":"markdown","987628f7":"markdown","60678a17":"markdown","d47dc20b":"markdown","1c37f7be":"markdown","55c35e87":"markdown","abf192da":"markdown","eba3eca4":"markdown","7aded9bc":"markdown","21a9496b":"markdown"},"source":{"b0e63f41":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# File system manangement\nimport os\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')","a4c1e5d3":"# List files available\nprint(os.listdir(\"..\/input\/\"))","906f1d3d":"# Training Data\n\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\nprint('Training data shape: ', train.shape)\ntrain.head()","5921b20f":"# Testing data features\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\nprint('Testing data shape: ', test.shape)\ntest.head()","01ea9484":"train['Survived'].value_counts()","3422d050":"sns.countplot(x = 'Survived',data = train)","f0ef201b":"sns.countplot(x = 'Survived',hue = 'Sex',data = train)","0e00a213":"sns.countplot(x = 'Survived',hue = 'Pclass',data = train)","90b3a1be":"sns.boxplot(x='Pclass',y='Age',data=train)","a4667f26":"print(\"Null in Training set\")\nprint(\"---------------------\")\nprint(train.isnull().sum())\nprint(\"---------------------\")\nprint(\"Null in Testing set\")\nprint(\"---------------------\")\nprint(test.isnull().sum())","b19c66e0":"def add_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    if pd.isnull(Age):\n        return int(train[train[\"Pclass\"] == Pclass][\"Age\"].mean())\n    else:\n        return Age","f006f610":"train['Age'] = train[['Age','Pclass']].apply(add_age,axis=1)\ntest['Age'] = test[['Age','Pclass']].apply(add_age,axis=1)","602959d5":"train.drop(\"Cabin\",inplace=True,axis=1)\ntest.drop(\"Cabin\",inplace=True,axis=1)","53ee0e73":"train['Embarked'].fillna(train['Embarked'].mode()[0],inplace=True)\ntest['Embarked'].fillna(test['Embarked'].mode()[0],inplace=True)\n","dd1efef4":"test['Fare'].fillna(test['Fare'].mean(),inplace=True)","97988c8e":"\ndef combine(df,col1,col2):\n    df[\"Family\"] = df[col1]+df[col2]\n    df.drop([col1,col2],inplace=True,axis=1)\n    return df\n\ntrain = combine(train,'SibSp','Parch')\ntest = combine(test,'SibSp','Parch')\n","23fa8253":"train['Age'].describe()","ca1f1f2f":"def process_age(df,cut_points,label_names):\n    df[\"Age\"] = df[\"Age\"].fillna(-0.5)\n    df[\"Age_categories\"] = pd.cut(df[\"Age\"],cut_points,labels=label_names)\n    return df\n\ncut_points = [-1,0,5,12,18,35,60,100]\nlabel_names = [\"Missing\",\"Infant\",\"Child\",\"Teenager\",\"Young Adult\",\"Adult\",\"Senior\"]\ntrain = process_age(train,cut_points,label_names)\ntest = process_age(test,cut_points,label_names)\n\npivot = train.pivot_table(index=\"Age_categories\",values='Survived')\npivot.plot.bar()\n","059dc1e0":"def create_dummies(df,column_name):\n    dummies = pd.get_dummies(df[column_name],prefix=column_name)\n    df = pd.concat([df,dummies],axis=1)\n    return df\n\nfor column in [\"Pclass\",\"Sex\",\"Age_categories\",'Embarked']:\n    train = create_dummies(train,column)\n    test = create_dummies(test,column)","de325ccb":"train.drop(['Name','Sex','Ticket','Pclass','Age_categories','Embarked'],inplace=True,axis=1)\ntest.drop(['Name','Sex','Ticket','Pclass','Age_categories','Embarked'],inplace=True,axis=1)","470685b5":"lr = LogisticRegression()\ncolumns = ['PassengerId', 'Age', 'Fare','Family',\n       'Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male',\n       'Age_categories_Missing', 'Age_categories_Infant',\n       'Age_categories_Child', 'Age_categories_Teenager',\n       'Age_categories_Young Adult', 'Age_categories_Adult',\n       'Age_categories_Senior']\n\nlr.fit(train[columns], train[\"Survived\"])\n","c397a6a2":"X = train[columns]\ny = train['Survived']\n\ntrain_X, val_X, train_y, val_y = train_test_split(\n    X, y, test_size=0.20,random_state=0)","90e053ea":"lr = LogisticRegression()\nlr.fit(train_X, train_y)\npredictions = lr.predict(val_X)\naccuracy = accuracy_score(val_y, predictions)\nprint(accuracy)\nfrom sklearn.metrics import classification_report\nprint(classification_report(val_y,predictions))","2871baf0":"lr = LogisticRegression()\nscores = cross_val_score(lr, X, y, cv=10)\nscores.sort()\naccuracy = scores.mean()\n\nprint(scores)\nprint(accuracy)","9a3ba209":"lr = LogisticRegression()\nlr.fit(X,y)\npredictions_test = lr.predict(test[columns])","0e644769":"submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsubmission_df = pd.DataFrame({'PassengerId' : test['PassengerId'],\n                              'Survived':predictions_test})\nsubmission_df.head()","ef1a2539":"submission_df.to_csv(\"submission.csv\",index=False)","7a7db3fc":"#### 3. Missing values in Frame column in Test Dataset\n\nSince there is one missing value, we shall impute them with the mean of the Fare column.","85638553":"#### Using cross validation for more robust error measurement\n\nUsing a Validation dataset has a drawback. Firstly, it decreases the training data and secondly since it is tested against a small amount of data, it has high chances of overfitting. To overcome this, there is a technique called **[cross validation](https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html)**. The most common form of cross validation, and the one we will be using, is called k-fold cross validation. \u2018Fold\u2019 refers to each different iteration that we train our model on, and \u2018k\u2019 just refers to the number of folds. In the diagram above, we have illustrated k-fold validation where k is 5.\n\n![](https:\/\/scikit-learn.org\/stable\/_images\/grid_search_cross_validation.png)\n\n[source](https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html)\n","e525b2e4":"The training data has 891 observations and 12 features (variables) including the TARGET (the label we want to predict).In this case we want to predict whether a passenger on Titanic **survived** or not.","ac2b2ec8":"## Creating new Features\n\n* WE shall create a new column called **Family** by combining Parch and SibSp columns\n","2bef5ffc":"# Logistic Regression in Python\n\n### Importing necessary libraries\n\n","d84571a2":"We can see that females survived in much higher proportions than males did. Now, Let\u2019s see how many people survived divided by class.","8ad48012":"Imagine you are working as a data scientist for an e-commerce company. One of the company\u2019s task is to send out e-mail offers to customers with a proposal to buy certain products. Your job as a data scientist is to determine whether the contacted person will buy the product or not. All you have is a sample of customers that were contacted recently, their age and a variable whether or not they took action.\n\n**So how do we do that?** \n\nThe only way that appears is to contact every person on the list and ask them whether they will buy the product or not. Although this appears to be the only solution, it isn\u2019t the best one.\nSo as a Data Scientist, you apply your knowledge of Machine Learning to the problem. Clearly, the Linear Regression algorithm will not work here since it only works for problems with a continuous outcome variable. On the other hand, the problem at hand is categorical i.e whether customers will buy a product( =1) or not( =0).\n\nInstead of trying to predict exactly whether the people will buy a product or not, you calculate the **probability or a likelihood** of the person saying yes. Basically you try to fit in probabilities between 0 and 1, which are the two possible outcomes. You also decide a cut off value\/threshold and then conclude that people with a probability higher than the threshold will buy the product and vice versa.\n\n**And how does it make the work of the company, easier?**\n\nSince it gives the probability of people who are more likely to buy a product, it enables the company, to focus only on the customers who are most likely to say Yes.","cf8071ce":"## Logistic Regression Implementation\n\nWe will use Logistic Regressionfrom [Scikit-Learn](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html) model. The only change we will make from the default model settings is to lower the [regularization parameter](https:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#logistic-regression), C, which controls the amount of overfitting (a lower value should decrease overfitting). This will get us slightly better results than the default Logistic Regression.\n\nThe .fit() method accepts two arguments: X and y. X must be a two dimensional array (like a dataframe) of the features that we wish to train our model on, and y must be a one-dimensional array (like a series) of our target, or the column we wish to predict.","4bb53d00":"Let\u2019s take a look at the Age column","7e50167f":"We can still can improve our model,however this notebook is intended to show how we can do some exploratory analysis, clean up data, perform predictions using Logistic regression Algorithm. In the Next notebook, I shall go in detail about Decision Trees and Random Forests.","9e9083e6":"# Describing the Performance of a Logistic model\n\n![](https:\/\/media.licdn.com\/dms\/image\/C5112AQHRd51OS1-Oaw\/article-cover_image-shrink_720_1280\/0?e=1575504000&v=beta&t=kzm0mOJoLPQeShqeFN6jgAQwblDum8S1Nc5I2FiPTBk)\n\n[source](https:\/\/medium.com\/google-design\/human-centered-machine-learning-a770d10562cd)\n\nA **confusion matrix** is a table that is often used to describe the performance of a classification model (or \u201cclassifier\u201d) on a set of test data for which the true values are known.Let us look at some of the important terms of confusion matrix.\n\n   ![](https:\/\/miro.medium.com\/max\/386\/1*GMlSubndVt3g7FmeQjpeMA.png)\n\n  confusion matrix whether employees will leave a company or not\n\n### The Confusion Matrix tells us the following:\n\n* There are two possible predicted classes: \u201cyes\u201d and \u201cno\u201d. If we were predicting that employees would leave an organisation, for example, \u201cyes\u201d would mean they will, and \u201cno\u201d would mean they won\u2019t leave the organisation.\n* The classifier made a total of 165 predictions (e.g., 165 employees were being studied).\n* Out of those 165 cases, the classifier predicted \u201cyes\u201d 110 times, and \u201cno\u201d 55 times.\n* In reality, 105 employees in the sample leave the organisation, and 60 do not.\n\n\n### Basic terms related to Confusion matrix:\n\n* **True positives (TP)**: These are cases in which we predicted yes (employees will leave the organisation), and employees actually leave i.e 100\n* **True negatives (TN)**: We predicted no(employees will not leave the organisation) and they don\u2019t leave i.e 50\n* **False positives (FP)**: We predicted yes they will leave, but they don\u2019t leave. (Also known as a \u201cType I error.\u201d) i.e 10\n* **False negatives (FN)**: We predicted no they will not leave, but they actually leave (Also known as a \u201cType II error.\u201d) i.e 5\n\n### Evaluating a Classification Model\n\n* **Accuracy** : (TP+TN)\/Total . Describes overall, how often the classifier correct. i.e 100+50\/165\nMeasures of Accuracy\n\nSensitivity and specificity are statistical measures of the performance of a binary classification test:\n\n* **Sensitivity\/Recall** = TP\/(TP + FN). When it\u2019s actually yes, how often does it predict yes? i.e 100\/(100+5)\n\n* **Specificity** = TN\/(TN + FP).When it\u2019s actually no, how often does it predict no?? i.e 50\/(50+10)\n* **Precision** = TP\/predicted yes. When it predicts yes, how often is it correct?100\/(10+100)\n\n### Evaluation metrics for a Classification model\u2019s performance.\n\n**ROC curve**\n\nA **ROC(Receiver Operator Characteristic Curve)** can help in deciding the best threshold value. It is generated by plotting the True Positive Rate (y-axis) against the False Positive Rate (x-axis) as you vary the threshold for assigning observations to a given class.ROC curve will always end at (1,1). The threshold at this point will be 0. This means that we will always classify these observations falling into the class 1(Specificity will be 0. False positive rate is 1).\n\nOne should select the best threshold for the trade off you want to make. According to the criticality of the business, we need to compare the cost of failing to detect positives vs cost of raising false alarms.\n\n![](https:\/\/miro.medium.com\/max\/469\/1*Y65IEOXvxLRKKqWxlQovsg.png)\n\nAn animation to demonstrate how an ROC curve relates to sensitivity and specificity for all possible cutoffs.\n\n![Alt Text](https:\/\/github.com\/dariyasydykova\/open_projects\/blob\/master\/ROC_animation\/animations\/ROC.gif?raw=true)\n\n[Source](https:\/\/github.com\/dariyasydykova\/open_projects\/blob\/master\/ROC_animation\/animations\/ROC.gif)\n\n**High Threshold:**\n* High specificity\n* Low sensitivity\n\n\n**Low Threshold**\n* Low specificity\n* High sensitivity\n\nThe area under ROC is called *Area Under the Curve(AUC)*. AUC gives the rate of successful classification by the logistic model. To get a more in-depth idea of what a ROC-AUC curve is and how is it calculated, here is a link to the [article](https:\/\/towardsdatascience.com\/understanding-the-roc-and-auc-curves-a05b68550b69) I wrote on the same topic.","225857d7":"#### 2. Missing values in Cabin \n\nSince we have lots of null values for Cabin column, so it is better to remove it.","ba9b67b4":"Logistic Regression an extension of Linear regression where the dependent variable is categorical and not continuous. It predicts the probability of the outcome variable.\n\n**Logistic regression**(*aka Logit Regression, maximum-entropy classification (MaxEnt) or the log-linear classifier*) can be binomial,ordinal or multinomial. In the binomial or binary logistic regression, the outcome can have only two possible types of values (e.g. \u201cYes\u201d or \u201cNo\u201d, \u201cSuccess\u201d or \u201cFailure\u201d). Multinomial logistic refers to cases where the outcome can have three or more possible types of values (e.g., \u201cgood\u201d vs. \u201cvery good\u201d vs. \u201cbest\u201d ). Generally, the outcome is coded as \u201c0\u2033 and \u201c1\u2033 in binary logistic regression.[Ordinal logistic](https:\/\/en.wikipedia.org\/wiki\/Ordered_logit) regression deals with dependent variables that are ordered.\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/8\/88\/Logistic-curve.svg\/320px-Logistic-curve.svg.png)\n\nSource : https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/8\/88\/Logistic-curve.svg\/320px-Logistic-curve.svg.png","ec307192":"the Age column needs to be treated slightly differently, as this is a continuous numerical column.we can separate this continuous feature into a categorical feature by dividing it into ranges.","d680e8e8":"Thus, around 549 people perished while 342 survived.","5ca60de1":"## Submission ","1f40150e":"#### 1. Age Column\n\nLet\u2019s create a function to impute ages regarding the corresponding age average per class.","883d0d43":"### Examining the Distribution of the Target Column ","284ff71b":"# Logistic Regression","2860652a":"## Exploratory Data Analysis(EDA)\n\nThe data page on Kaggle describes the columns in detail. It\u2019s always worth exploring this in detail to get a full understanding of the data.","60c09d98":"### Evaluating Accuracy of our model\n\nThe evaluation criteria given on the Titanic Data page is accuracy, i.e how many correct predictions we have made out of the total predictions. We have created our model but how will we know how accurate it is? We do have a Test dataset but since it doesn't have the Target column, everytime we optimize our model, we will have to submit our predictions to public Leaderboard to assess it accuracy. \n\n#### Creating a Validation set\n\nAnother option would be to create a validation set from the training set. We will hold out a part of the training set during the start of the experiment and use it for evaluating our predictions. We shall use the scikit-learn library's `model_selection.train_test_split()` function that we can use to split our data","8e64f1b0":"## Examining Missing Values\n\nNext we can look at the number and percentage of missing values in each column.","dc430770":"## Dropping Unnecessary columns","987628f7":"## Read in the data","60678a17":"The three columns i.e Age, cabin and Embarked have missing values which needs to be taken care of.","d47dc20b":"Distribution of survival rate class wise","1c37f7be":"#### Making Predictions on Test data","55c35e87":"## Encoding Categorical Variables","abf192da":"#### Making predictions and measuring accuracy","eba3eca4":"# Representation of Logistic regression\n\nJust like a lInear regression model, a logistic regression model also computes a weighted sum of input features(and adds a bias term to it). However, unlike linear regression, it calculates the logistic of the results so that the output is always between o and 1.\n\n**Logistic Regression model estimated probability (vectorized form)**\n\n![](https:\/\/i.imgur.com\/JNnqrrL.png)\n\nThe logisitc also called as logit is denoted by \u03c3 is a sigmoid function and it outputs a number between 0 and 1.\n\n![](https:\/\/imgur.com\/DNpFtRM.png)\n\n*source* : https:\/\/www.oreilly.com\/library\/view\/hands-on-machine-learning\/9781491962282\/\n\n\n* Positive values are predictive of class 1\n* Negative values are predictive of class 0\n\nThe output of a Logistic regression model is a probability. We can select a threshold value. If the probability is greater than this threshold value, the event is predicted to happen otherwise it is predicted not to happen.\n\nOnce the mpdel estimates the probabilities(p hat), it can then easily make the predictions as follows:\n\n![](https:\/\/imgur.com\/cm53tL8.png)","7aded9bc":"#### 3. Missing values in Embarked column\n\nSince there are just two missing values, we shall impute them with the mode of the Embarked column.","21a9496b":"We can use the pandas.get_dummies() function Now, we shall have to encode Sex, Embarked, Pclass and Age_categories. Name and Ticket columns have a lot of categories, hence we shall delete them."}}