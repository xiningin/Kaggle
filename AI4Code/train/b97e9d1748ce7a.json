{"cell_type":{"abd79e3f":"code","f38eed69":"code","1df9899e":"code","d774b59e":"code","091d5e75":"code","e9daa460":"code","10d34e84":"code","0d223c9b":"code","edbbb946":"code","783d0ef5":"code","fa1291ce":"code","62495c38":"code","b2da87a8":"code","ce7b1650":"code","cc87ea92":"code","ab0e1952":"code","307cfbcc":"code","3aacbdfe":"code","3e4cb264":"code","b2b11aa5":"code","e2369dd6":"markdown","e146c206":"markdown","3c313027":"markdown","a825cda2":"markdown","e65b292b":"markdown","9f2a8d87":"markdown","86e9538a":"markdown","06b91f05":"markdown","e13883b1":"markdown","9d12fb27":"markdown","0c64f5d0":"markdown","461eb609":"markdown","d0f036ba":"markdown","67f51242":"markdown","1ce43805":"markdown","9499476b":"markdown","46609864":"markdown","c9d5ec0f":"markdown","7fd822ff":"markdown"},"source":{"abd79e3f":"import numpy as np\nimport pandas as pd","f38eed69":"import sklearn.datasets\nds=sklearn.datasets.load_breast_cancer()\n#now our dataset has been loaded into dataframe ds\nds['data'],ds['target']","1df9899e":"#lets look at the columns of dataset\nprint(\"features :\",ds.feature_names)\nprint(\"\\ntotal number of features : \",len(ds.feature_names))\n","d774b59e":"X=ds['data']\ny=ds['target']\n#all of our feature variables are in X and target data corresponding to it in y.\nprint(\"shape of X : \",X.shape)\nprint(\"shape of y : \",y.shape)","091d5e75":"data=pd.DataFrame(ds.data,columns=ds.feature_names)\n#forming a dataframe \ndata['class']=ds.target\n#putting the target data by forming a new column called (class) in a dataframe data\n#Lets have a look at our data.\n\nprint(\"Two classes in the target data : \",ds.target_names)\ndata.head()","e9daa460":"data.describe()","10d34e84":"import seaborn as sns\ncols= [\"#86C2DE\",\"#F5C6BA\"]\nsns.countplot(x= data['class'], palette= cols)","0d223c9b":"data.info() #Tells us about the datatype of the features.","edbbb946":"# Correlation amongst numeric attributes\nimport matplotlib.pyplot as plt\ncorrmat = data.corr()\ncmap = sns.diverging_palette(260,-10,s=50, l=75, n=6, as_cmap=True)\nplt.subplots(figsize=(18,18))\nsns.heatmap(corrmat,cmap= cmap,annot=True, square=True)","783d0ef5":"for i in data.columns:\n    print(i, data[i].isnull().sum())\n","fa1291ce":"from sklearn.model_selection import train_test_split\nx=data.drop('class',axis=1) # drop will remove the column of 'class' from the data\ny=data['class'] #our target data\n\nx_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2,stratify=y,random_state=42)\n#keeping the test_size=0.2\nprint(\"shape of y_train\",y_train.shape,\"\\nshape of y :\",y.shape,\"\\nshape of y_test:\",y_train.shape)\n#random state helps to keep the mean approximately similar when you run the cell over and over again, because you dont\n#want to have a huge change in the performance of the model.\nz=[y_train.shape[0],y_test.shape[0]]\nfor k in range(y_train.shape[0]):\n    z.append(y_train.shape[0])\nfor k in range(y_test.shape[0]):\n    z.append(y_test.shape[0])\ncols= [\"#86C2DE\",\"#F5C6BA\",\"#86C2DE\",\"#F5C6BA\"]\nax=sns.countplot(x=z, palette= cols)\nax.set(xlabel=\"Test data size\/Train data size\", ylabel=\"count\")","62495c38":"x_bin_train=x_train.apply(pd.cut,bins=2,labels=[1,0])\nx_bintest=x_test.apply(pd.cut,bins=2,labels=[1,0])\n#The continuous values of data is converted into the categorical.","b2da87a8":"print(type(x_bin_train))\nx_bin_train=x_bin_train.values\n\nx_bin=pd.DataFrame(x_bin_train)\n","ce7b1650":"x_bin_train1=pd.DataFrame(x_bin_train)\nx_bin_train1=x_bin_train1.values\nx_bin_test=pd.DataFrame(x_bintest)\nx_bin_test=x_bin_test.values\nx_bin_train","cc87ea92":"b=10\ni=90\n#here i want the sum of all values of a row 90 and will compare with the bais.\n#if sum_of_row > bais only the the neuron will fire,(y_pred=1), otherwise(y_pred=0)\nx_bin_train1.shape","ab0e1952":"#lets find the sum of full row\ni=90\nb=10 #threshold\nif np.sum(x_bin_train1[i]) >= b:\n    y_pred=1\nelse:\n    y_pred=0\nprint(\"predicted value \",y_pred,\"\\nfor row :\",i,\"\\nbias :\",b)    ","307cfbcc":"test=[]\nbias=[]\nfor b in range(x_bin_train1.shape[1]+1):\n    acc_rows=0\n    for x,y in zip(x_bin_train1,y_train):\n      y_pred = (np.sum(x)>=b)\n      acc_rows+=(y == y_pred)\n    \n    print(\"value of b is :\",b,\":  \",acc_rows,\"  \",acc_rows\/x_bin_train.shape[0])\n    test.append(acc_rows\/x_bin_train.shape[0])\n    bias.append(b)\nindex=test.index(max(test))\nl1=max(test1)\nprint(\"\\nThe maximum Accuracy with Train data is :\",max(test),\" with bias : \",bias[index])    ","3aacbdfe":"plt.figure(figsize=(10,5))\nplt.title('Accuracy and Bias for Training data')\nplt.xlabel('Accuracy')\nplt.ylabel('Bias')\nsns.scatterplot(test,bias)\nplt.show()","3e4cb264":"test1=[]\nbias1=[]\nfor b in range(x_bin_test.shape[1]+1):\n    acc_rows=0\n    for x,y in zip(x_bin_test,y_test):\n      y_pred = (np.sum(x)>=b)\n      acc_rows+=(y == y_pred)\n    \n    print(\"value of b is :\",b,\":  \",acc_rows,\"  \",acc_rows\/x_bin_test.shape[0])\n    test1.append(acc_rows\/x_bin_test.shape[0])\n    bias1.append(b)\nindex=test1.index(max(test1))\nl=max(test1)\nprint(\"\\nThe maximum Accuracy with Test data is :\",max(test1),\" with bias : \",bias1[index]) ","b2b11aa5":"plt.figure(figsize=(10,5))\nplt.title('Accuracy and Bias for Testing data')\nplt.xlabel('Accuracy')\nplt.ylabel('Bias')\nsns.scatterplot(test1,bias1)\nplt.show()","e2369dd6":"we have successfully converted our data values into categorical 0,1. Now we are ready to calculate the b for which accuracy is highest.\n**The idea behind:**\n* The neuron will fire when the no of 1's in the row is greater than the threshold\/bais(b).\n* we will calculate the accuracy of the model by putting the value of bais such that we obtain highest accuracy\n* The value of bais will range from (1-30) as the number of columns\/features are 30.","e146c206":"# Checking for Missing values","3c313027":"# LOADING DATA\nFor the dataset we are using sklearn dataset so lets import it!","a825cda2":"Now, we aim to split our data into two parts column wise, one which contains the data and the second one(y) with target\n.It is a supervised learning technique so our model will get trained using the input data and target variable.","e65b292b":"From the count plot 114 is the count of test data and 455 being the count for train data","9f2a8d87":"**About the data :**\n* The data which we are using is continuous\n* The target class has two classes(0 and 1) namely malignant and benign\n* It is a classification problem","86e9538a":"# what does the shape tells us?\nHere, the shape of X is (569, 30) means that the dataframes consist of **569 rows** and **30 columns** \nfor Target class(dependent variable), has **569 rows** and the column is empty because it is in a form of single list.","06b91f05":"So now we will check for different bias values and for all the rows to get the accuracy","e13883b1":"<div style=\"color:#485053;\n           display:auto;\n           border-radius:0px;\n           background-color:#C2C4E2;\n           font-size:200%;\n           padding-left:40px; \n           font-family:Verdana;\n           font-weight:600; \n           letter-spacing:0.5px;\n           \">\n\n<p style=\"padding: 15px;\n          color:white;\n          text-align: center;\">\n\nCan we turn Continious values into categorical ?\n<\/p>\n<\/div>\n\n* Yes, Here we have continuous values. In order to find the loss and y_pred, we need our data in the form of (0 and 1) because we have two classes in the target data. So only after that we can calculate different values of b. \n\n* we use pd.cut() method for that purpose","9d12fb27":"<div style=\"color:#485053;\n           display:auto;\n           border-radius:0px;\n           background-color:#86C2DE;\n           font-size:200%;\n           padding-left:40px; \n           font-family:Verdana;\n           font-weight:600; \n           letter-spacing:0.5px;\n           \">\n\n<p style=\"padding: 15px;\n          color:white;\n          text-align: center;\">\n\nImplementation of Simple ML Model (20MAI0014) Krupa Gajjar\n<\/p>\n<\/div>\n\nIn the making of any machine learning model comes the recipe of preparation which includes, importing the necessary libraries, loading the dataset, preprocessing of data and then finally building the model, followed by testing and validation of the model to check the accurracy and other performance features!\n\nLets begin by first Importing the Libraries!","0c64f5d0":"# LIBRARIES\nwe are going to import the basic machine learning library NUMPY, as it can easily handle n-dimensional arrays.","461eb609":"The erroe is because the x_bin_train is converted into np array from the dataframe.","d0f036ba":"# Splitting the data\ntrain test split will split the data into train data and test data with the corresponding ratio.","67f51242":"<div style=\"color:#485053;\n           display:auto;\n           border-radius:0px;\n           background-color:#C2C4E2;\n           font-size:200%;\n           padding-left:40px; \n           font-family:Verdana;\n           font-weight:600; \n           letter-spacing:0.5px;\n           \">\n\n<p style=\"padding: 15px;\n          color:white;\n          text-align: center;\">\n\nIs the dataset Imbalanced? \n<\/p>\n<\/div>\n\nThere are two simple ways of checking the imbalaneness of data:\n\n1. By describing the data\n2. By represing it in a graph","1ce43805":"There are no missing values.","9499476b":"<div style=\"color:#485053;\n           display:auto;\n           border-radius:0px;\n           background-color:#C2C4E2;\n           font-size:200%;\n           padding-left:40px; \n           font-family:Verdana;\n           font-weight:600; \n           letter-spacing:0.5px;\n           \">\n\n<p style=\"padding: 15px;\n          color:white;\n          text-align: center;\">\n\nData Visualisation \n<\/p>\n<\/div>","46609864":"You can clearly see from this bar graph that the data is not Balanced!","c9d5ec0f":"The mean of the class is 0.627 which is not equal to 0.5 hence we can say that the Number of 1 is slightly greater than the Number of 0s in the target data.","7fd822ff":"# Conclusion\n* we had dataset with real inputs and binary output, we pursued a brute force approach for getting to the best threshold value for which the accuracy is high. \n* The accuracy of training set is higher than the testing\n* The model can be improved if we introduce weights as a parameter.\n* This model has harsh function for categorizing between 0 and 1 for y_pred. "}}