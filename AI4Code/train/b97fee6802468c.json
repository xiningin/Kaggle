{"cell_type":{"2d6e9987":"code","30ca54db":"code","b18ff4b1":"code","1ea7800b":"code","8cc4d405":"code","d7bbaf67":"code","9c61671a":"code","08c468ad":"code","82609e02":"code","1bd16048":"code","1a965a06":"code","d3df9cf7":"code","1b226dd2":"code","53bb8395":"code","27131453":"code","2efa2dc6":"markdown","baec27d3":"markdown","267b14ee":"markdown","e8580ac2":"markdown","ba889178":"markdown","5ffebf7f":"markdown","7ab084c1":"markdown","a3e85811":"markdown","6a93d833":"markdown","8a354b3a":"markdown","c24a3612":"markdown","4f232fca":"markdown","683ec53e":"markdown"},"source":{"2d6e9987":"%cd ..\/\n!mkdir tmp\n%cd tmp","30ca54db":"# Download YOLOv5\n!git clone https:\/\/github.com\/ultralytics\/yolov5  # clone repo\n%cd yolov5\n# Install dependencies\n%pip install -qr requirements.txt  # install dependencies\n\n%cd ..\/\nimport torch\nprint(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")","b18ff4b1":"# Install W&B \n# !pip install -q --upgrade wandb\n\n# Login \nimport wandb\nprint(wandb.__version__)\nwandb.login(anonymous='must')","1ea7800b":"# Necessary\/extra dependencies. \nimport os\nimport gc\nimport ast\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\ntqdm.pandas()\nfrom shutil import copyfile\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","8cc4d405":"%cd ..\/\nTRAIN_PATH = 'input\/tensorflow-great-barrier-reef\/train_images'\nIMG_SIZE = 256\nBATCH_SIZE = 16\nEPOCHS = 10","d7bbaf67":"df = pd.read_csv('input\/tensorflow-great-barrier-reef\/train.csv')\nprint('Number of training images: ', len(df))\ndf.head()","9c61671a":"def add_path(row):\n    return f\"{TRAIN_PATH}\/video_{row.video_id}\/{row.video_frame}.jpg\"\n\ndef num_boxes(annotations):\n    annotations = ast.literal_eval(annotations)\n    return len(annotations)\n\n# Add path\ndf['path'] = df.apply(lambda row: add_path(row), axis=1)\n# Find number of annotations per image\ndf['num_bbox'] = df['annotations'].apply(lambda x: num_boxes(x))\n\ndata = (df.num_bbox>0).value_counts()\/len(df)*100\nprint(f\"Number of images without bboxes: {data[0]:0.2f}% | with bboxes: {data[1]:0.2f}%\")","08c468ad":"# Remove images without annotations. \ndf = df.query(\"num_bbox>0\")\nprint(f\"Number of images with annotations: {len(df)}\")\ndf.head()","82609e02":"# Create train and validation split\ntrain_df, valid_df, = train_test_split(df, test_size=0.15, random_state=42)\n\ntrain_df.loc[:, 'split'] = 'train'\nvalid_df.loc[:, 'split'] = 'valid'\n\ndf = pd.concat([train_df, valid_df]).reset_index(drop=True)\nprint(f\"Number of train images: {len(train_df)}, validation images: {len(valid_df)}\")","1bd16048":"def add_new_path(row):\n    return f\"tmp\/barrier_reef\/dataset\/images\/{row.split}\/{row.video_frame}.jpg\"\n    \n# Add new path\ndf['new_path'] = df.apply(lambda row: add_new_path(row), axis=1)","1a965a06":"def copy_file(row):\n    os.makedirs(os.path.dirname(row.new_path), exist_ok=True)\n    copyfile(row.path, row.new_path)\n\n_ = df.progress_apply(lambda row: copy_file(row), axis=1)","d3df9cf7":"# Create .yaml file \nimport yaml\n\ndata_yaml = dict(\n    train = '..\/barrier_reef\/images\/train',\n    val = '..\/barrier_reef\/images\/valid',\n    nc = 1,\n    names = ['cots']\n)\n\n# Note that I am creating the file in the yolov5\/data\/ directory.\nwith open('tmp\/yolov5\/data\/data.yaml', 'w') as outfile:\n    yaml.dump(data_yaml, outfile, default_flow_style=True)\n    \n%cat tmp\/yolov5\/data\/data.yaml","1b226dd2":"IMG_WIDTH, IMG_HEIGHT = 1280, 720\n\ndef get_yolo_format_bbox(img_w, img_h, box):\n    \"\"\"\n    Convert the bounding boxes in YOLO format.\n    \n    Input:\n    img_w - Original\/Scaled image width\n    img_h - Original\/Scaled image height\n    box - Bounding box coordinates in the format, \"x_min, y_min, width, height\"\n    \n    Output:\n    Return YOLO formatted bounding box coordinates, \"x_center y_center width height\".\n    \"\"\"\n    w = box['width'] # width \n    h = box['height'] # height\n    xc = box['x'] + int(np.round(w\/2)) # xmin + width\/2\n    yc = box['y'] + int(np.round(h\/2)) # ymin + height\/2\n\n    return [xc\/img_w, yc\/img_h, w\/img_w, h\/img_h] # x_center y_center width height\n    \n\n# Iterate over each row and write the labels and bbox coordinates to a .txt file. \nfor i in tqdm(range(len(df))):\n    row = df.loc[i]\n    annotations = ast.literal_eval(row.annotations)\n    bboxes = []\n    for bbox in annotations:\n        # get bbox in YOLO format\n        bbox = get_yolo_format_bbox(IMG_WIDTH, IMG_HEIGHT, bbox)\n        bboxes.append(bbox)\n        \n    # Create .txt files for label\n    if row.split == 'train':\n        file_name = f\"tmp\/barrier_reef\/labels\/{row.split}\/{row.video_frame}.txt\"\n        os.makedirs(os.path.dirname(file_name), exist_ok=True)\n    else:\n        file_name = f\"tmp\/barrier_reef\/labels\/{row.split}\/{row.video_frame}.txt\"\n        os.makedirs(os.path.dirname(file_name), exist_ok=True)\n        \n    # Write the label as .txt file\n    with open(file_name, 'w') as f:\n        for i, bbox in enumerate(bboxes):\n            label = 0\n            bbox = [label]+bbox\n            bbox = [str(i) for i in bbox]\n            bbox = ' '.join(bbox)\n            f.write(bbox)\n            f.write('\\n')","53bb8395":"%cd tmp\/yolov5\/","27131453":"!python train.py --img 768\\\n                 --batch 16\\\n                 --epochs 10\\\n                 --data data.yaml\\\n                 --weights yolov5s.pt\\\n                 --project barrier-reef-yolo","2efa2dc6":"# \ud83d\udd28 Prepare Dataset\n\nThis is the most important section when it comes to training an object detector with YOLOv5. The directory structure, bounding box format, etc must be in the correct order. This section builds every piece needed to train a YOLOv5 model.\n\n* Find the number of images with annotations and remove the ones without annotations.\n* Create train-validation split.\n* Create required \/dataset folder structure and more the images to that folder.\n* Create `data.yaml` file needed to train the model.\n* Create bounding box coordinates in the required YOLO format.","baec27d3":"# WORK IN PROGRESS","267b14ee":"# \ud83d\ude85 Train with W&B","e8580ac2":"```\n--img {IMG_SIZE} \\ # Input image size.\n--batch {BATCH_SIZE} \\ # Batch size\n--epochs {EPOCHS} \\ # Number of epochs\n--data data.yaml \\ # Configuration file\n--weights yolov5s.pt \\ # Model name\n--save_period 1\\ # Save model after interval\n--project barrier-reef-yolo # W&B project name\n```","ba889178":"# \ud83c\udf5c Create .YAML file\n\nThe `data.yaml`, is the dataset configuration file that defines\n\n- an \"optional\" download command\/URL for auto-downloading,\n- a path to a directory of training images (or path to a *.txt file with a list of training images),\n- a path to a directory of validation images (or path to a *.txt file with a list of validation images), \n- the number of classes, \n- a list of class names\n\n\ud83d\udccd Important: There is just one class that's why nc=1. <br>\n\ud83d\udccd Note: The `data.yaml` is created in the yolov5\/data directory as required.","5ffebf7f":"\ud83d\udccd Notes:\n\n* There are three unique video_ids. These are 0, 1, and 2 and are three distinct directories (video_0, video_1, and video_2).\n* There are 20 unique sequences. ","7ab084c1":"\n\ud83d\udccd Note: W&B comes pre-installed with Kaggle kernel but to ensure the lastest version of W&B use pip install.\n\n","a3e85811":"## How to login to W&B?\n\nThere are two ways you can login to W&B in a Kaggle kernel setting:\n\n* Run a cell with `wandb.login()`. It will ask for the API key, which you can copy + paste in. This is ideal if you Quick Save your kernel.\n\n* You can also use Kaggle Secrets to store your API key and use the code snippet below to login. If you are not familiar with Kaggle Secrets check this [forum post](https:\/\/www.kaggle.com\/product-feedback\/114053). This is ideal if you do Run and Save All.\n\n```\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\n# I have saved my API token with \"wandb_api\" as Label. \n# If you use some other Label make sure to change the same below. \nwandb_api = user_secrets.get_secret(\"wandb_api\") \n\nwandb.login(key=wandb_api)\n\n```\n\nMore on W&B login [here](https:\/\/docs.wandb.ai\/ref\/cli\/wandb-login).\n","6a93d833":"# \u2600\ufe0f Imports and Setup\n\nAccording to the official Train Custom Data guide, YOLOv5 requires a certain directory structure.\n\n```\n\/parent_folder\n    \/dataset\n         \/images\n         \/labels\n    \/yolov5\n```\n\n* We thus will create a \/tmp directory.\n* Download YOLOv5 repository and pip install the required dependencies.\n* Install the latest version of W&B and login with your wandb account. You can create your free W&B account [here](https:\/\/wandb.ai\/signup).","8a354b3a":"# \ud83e\udd86 Hyperparameters\u00b6\n","c24a3612":"# \ud83c\udf5a Prepare Required Folder Structure\n\nThe required folder structure for the dataset directory is:\n\n```\n\/parent_folder\n    \/dataset\n         \/images\n             \/train\n             \/val\n         \/labels\n             \/train\n             \/val\n    \/yolov5\n```\n\nNote that I have named the directory `barrier_reef`.\n","4f232fca":"# \ud83c\udf6e Prepare Bounding Box Coordinated for YOLOv5\n\nFor every image with bounding box(es) a `.txt` file with the same name as the image will be created in the format shown below:\n\n* One row per object.\n* Each row is in `class x_center y_center width height` format.\n* Box coordinates must be in normalized xywh format (from 0 - 1). We can normalize by the boxes in pixels by dividing `x_center` and `width` by image width, and `y_center` and `height` by image `height`.\n* Class numbers are zero-indexed (start from 0).\n\n\ud83d\udccd Note: We don't have to remove the images without bounding boxes from the training or validation sets. But in our case we are not considering images without bboxes.","683ec53e":"This is a starter kernel to train a YOLOv5 model on Tensorflow - Help Protect the Great Barrier Reef dataset. Given an input image the task is to find the presence of crown-of-thorns starfish using bounding box detection.\n\n### Other works in this competition\n\nYou can check out my visualization kernel [here](https:\/\/www.kaggle.com\/ayuraj\/visualize-the-cots-interactively-with-w-b?scriptVersionId=80596542).\n\n# \ud83d\uddbc\ufe0f What is YOLOv5?\n\nYOLO an acronym for 'You only look once', is an object detection algorithm that divides images into a grid system. Each cell in the grid is responsible for detecting objects within itself.\n\n[Ultralytics' YOLOv5](https:\/\/github.com\/ultralytics\/yolov5) (\"You Only Look Once\") model family enables real-time object detection with convolutional neural networks.\n\n# \ud83e\udd84 What is Weights and Biases?\n\n[Weights & Biases](https:\/\/wandb.ai\/site) (W&B) is a set of machine learning tools that helps you build better models faster. Check out Experiment Tracking with Weights and Biases to learn more.\nWeights & Biases is directly integrated into YOLOv5, providing experiment metric tracking, model and dataset versioning, rich model prediction visualization, and more. You can learn more about W&B in this [kernel](https:\/\/www.kaggle.com\/ayuraj\/experiment-tracking-with-weights-and-biases)."}}