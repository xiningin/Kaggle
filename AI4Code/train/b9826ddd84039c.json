{"cell_type":{"2f192156":"code","590f190c":"code","805c34f0":"code","1a6b9cef":"code","cb219e42":"code","35b8dfbe":"code","0c022f70":"code","9e8512b0":"code","c8b30171":"code","06e18e88":"code","37677792":"code","045f53f4":"code","6e8d32a2":"code","a5e5f185":"code","2ce5894f":"code","9c7d1a4d":"code","709be9d6":"code","c921eabf":"code","bf9a9cdc":"code","75d7438e":"code","41760ac9":"code","5bd6b543":"code","7d20bf70":"code","e288c9a3":"code","1ff3c894":"code","05b9242f":"code","2cc2578a":"code","2a00776b":"markdown","231efb12":"markdown","46eed8ed":"markdown","4c84edcf":"markdown","be88646b":"markdown","61f9738e":"markdown","6c98ae6a":"markdown","ff947249":"markdown","341ed2d1":"markdown"},"source":{"2f192156":"import pandas as pd\n# Reading the train data set #\ndf = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\ndf.shape\n","590f190c":"# Reading the test data set #\ndf1= pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')\ndf1.shape\n","805c34f0":"# Train Data\ndf.head()\n# image size : 28*28 pixels\n# No. of Labels to be classified : 10","1a6b9cef":"print(df.shape) \nprint(df1.shape) ","cb219e42":"# Data cleaning #\n\nprint(df.isnull().sum().sum())\nprint(df1.isnull().sum().sum())\n\n# No Missing records in train and test dataset","35b8dfbe":"# CNN Architecture Visualization Library\n! pip install visualkeras","0c022f70":"import os\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nimport keras\nfrom keras.layers.core import Dense,Activation,Dropout\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Flatten,Dropout,Conv2D,MaxPooling2D, BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import EarlyStopping, Callback\nfrom keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport visualkeras\nfrom keras.utils import plot_model\nimport math\nfrom keras.optimizers import RMSprop\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau","9e8512b0":"\n# This function data_preprocessing will perform the following tasks :\n# a) Tf.keras.utlils.categorical will convert all the labels into dummy encodings for all the 10 classes\n# b) It will reshape the input data in the format x(len(x) - no.of records,pixel size(28*28), no. of channels(1) - black & white image)\n# c) We are dividing the entire independent variables by 255, to bring the entire variables into thje range (0,1), so thay it will help to converge the model faster and reach the global minima\n\ndef data_preprocessing(raw):\n    label = tf.keras.utils.to_categorical(raw.label, 10)\n    num_images = raw.shape[0]\n    x_as_array = raw.values[:,1:]\n    x_shaped_array = x_as_array.reshape(num_images, 28, 28, 1)\n    image = x_shaped_array \/ 255\n    return image, label\n\n# Applyomg this function on the training dataset #\n\nX,Y = data_preprocessing(df)\n\n# X - independent variables #\n# Y - dependent variable (labels)","c8b30171":"# Test Dataset\nX_test, Y_test = data_preprocessing(df1)","06e18e88":"# Splitting the training data into train and test split, to validate the model performance #\n\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.25, random_state=20)\n","37677792":"# Building the basic CNN Model without tuning any hyper parameters #\n\nmodel = tf.keras.Sequential()\n\n# First layer, which has a 2D Convolutional layer with kernel size as 3*3, Relu activation & Max pooling operation \nmodel.add(Conv2D(32, (3,3), padding='same', input_shape=(28,28,1),activation=tf.nn.relu))\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n\n# Second layer, which has a 2D Convolutional layer with kernel size as 3*3, ReLU activation and Max pooling operation \nmodel.add(Conv2D(64, (3,3), padding='same', activation=tf.nn.relu))\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n\n# Fully connected layer with ReLU activation function \nmodel.add(Flatten())\nmodel.add(Dense(128, activation=tf.nn.relu))\n\n# Output layer with softmax activation function\nmodel.add(Dense(10, activation=tf.nn.softmax))","045f53f4":"model.summary()","6e8d32a2":"visualkeras.layered_view(model)","a5e5f185":"# Optimizer specified here is adam, loss is categorical crossentrophy and metric is accuracy\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.categorical_crossentropy,\n              metrics=['accuracy'])","2ce5894f":"train_model = model.fit(X_train, Y_train,\n                  batch_size=128,\n                  epochs=50,\n                  verbose=1,\n                  validation_data=(X_val, Y_val))","9c7d1a4d":"score = model.evaluate(X_test, Y_test, steps=math.ceil(10000\/32))\n# checking the test loss and test accuracy\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","709be9d6":"score1 = model.evaluate(X_train, Y_train, steps=math.ceil(10000\/32))\n# checking the test loss and test accuracy\nprint('Train loss:', score1[0])\nprint('Train accuracy:', score1[1])\n\n# Getting the accuracy of 99.76% on training shows the model is getting overfitted #","c921eabf":"from kerastuner import RandomSearch\nfrom kerastuner.engine.hyperparameters import HyperParameters\nfrom keras.layers import LeakyReLU","bf9a9cdc":"# Learning rate, overfitting early stopping and plotting the accu. #\noptimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)","75d7438e":"def build_model(hp):  \n    model_2 = keras.Sequential()\n    model_2.add(Conv2D(\n        filters=hp.Int('conv_1_filter', min_value=32, max_value=128, step=16),\n        kernel_size=hp.Choice('conv_1_kernel', values = [3,4,5]),padding='same', \n        input_shape=(28,28,1))\n    )\n    model_2.add(BatchNormalization())\n    model_2.add(Activation(LeakyReLU(alpha=0.2)))\n    model_2.add(Conv2D(\n        filters=hp.Int('conv_2_filter', min_value=32, max_value=128, step=16),\n        kernel_size=hp.Choice('conv_2_kernel', values = [3,4,5]), padding='same')\n    )\n    model_2.add(BatchNormalization())\n    model_2.add(Activation(LeakyReLU(alpha=0.2)))\n    model_2.add(Conv2D(\n        filters=hp.Int('conv_3_filter', min_value=32, max_value=128, step=16),\n        kernel_size=hp.Choice('conv_3_kernel', values = [3,4,5]),padding='same')\n    )\n    model_2.add(BatchNormalization())  \n    model_2.add(Activation(LeakyReLU(alpha=0.2)))\n    model_2.add(MaxPooling2D(pool_size=(2, 2))\n    )\n    model_2.add(Flatten()\n    )\n    model_2.add(Dense(\n        units=hp.Int('dense_1_units', min_value=32, max_value=320, step=16),\n        activation=LeakyReLU(alpha=0.2))\n    )\n    model_2.add(Dropout(0.50))\n    model_2.add(Dense(\n        units=hp.Int('dense_2_units', min_value=32, max_value=240, step=16),\n        activation=LeakyReLU(alpha=0.2))\n    )\n    model_2.add(Dropout(0.50))\n    model_2.add(Dense(\n        units=hp.Int('dense_3_units', min_value=32, max_value=160, step=16),\n        activation=LeakyReLU(alpha=0.2))\n    )\n    model_2.add(Dropout(0.50))\n    model_2.add(Dense(10,activation='softmax')\n    )\n    model_2.compile(optimizer=optimizer,\n              loss='categorical_crossentropy',\n              metrics=['accuracy']\n    )\n  \n    return model_2","41760ac9":"# Building a checkpoint, which will try to save the model which performed the best on the validation dataset\ncheck_point = ModelCheckpoint(\"best_model.h5\", monitor=\"val_accuracy\", verbose=1, save_best_only=True)","5bd6b543":"tuner_search=RandomSearch(build_model,\n                          objective='val_accuracy',\n                          max_trials=3,directory='output',project_name=\"Mnist_Fashion\")","7d20bf70":"tuner_search.search(X_train,Y_train,epochs=85,validation_data=(X_val,Y_val),callbacks=[check_point])","e288c9a3":"# Based on the best validation accuracy, we are taking that model for prediction on X_test #\n\nmodel4=tuner_search.get_best_models(num_models=1)[0]\nmodel4.summary()\n\n# Below are the model parameters of the model which performed the best on validation set #","1ff3c894":"model4.fit(X_train,Y_train,validation_data=(X_val,Y_val),epochs=20)","05b9242f":"visualkeras.layered_view(model4)","2cc2578a":"score = model4.evaluate(X_test, Y_test)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","2a00776b":"# Adam Configuration Parameters #\n\nAlpha : learning rate or step size. The proportion that weights are updated (e.g. 0.001)\nLarger values (e.g. 0.3) results in faster initial learning before the rate is updated. Smaller values (e.g. 1.0E-5) slow learning right down during training\nbeta1. The exponential decay rate for the first moment estimates (e.g. 0.9).\nbeta2. The exponential decay rate for the second-moment estimates (e.g. 0.999). This value should be set close to 1.0 on problems with a sparse gradient (e.g. NLP and computer vision problems).\nepsilon. Is a very small number to prevent any division by zero in the implementation (e.g. 10E-8).","231efb12":"# Feature Enginnering #","46eed8ed":"# Building the CNN Model","4c84edcf":"# Hope you like this notebook..! Happy Learning","be88646b":"# Importing the required Libraries #","61f9738e":"![](https:\/\/i.imgur.com\/D2pBqYi.jpeg)","6c98ae6a":"# Image Classification using CNN architecture for the Fashion Mnist Dataset using Tensorflow Keras Tuner ","ff947249":"# Hyper Parameter Tuning with Keras Tuner and reducing the overfitting problem with batch normalization and adding dropout Layers#","341ed2d1":"# Problem Statement : Need to classify the images into the follow labels - T-shirt, Trouser, Pullover, Dress, Coat, Sandals, Shirt, Sneaker, Bag, Ankle boots"}}