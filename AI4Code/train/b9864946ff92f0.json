{"cell_type":{"053e6ed5":"code","5e302d5d":"code","33b19117":"code","b7b8be0b":"code","a11683a4":"code","f6c7998c":"code","2c2f6ac0":"markdown","ed14b9ff":"markdown","b85c2ba1":"markdown","5d5f8c8a":"markdown","7d73fe10":"markdown","84e7bf64":"markdown"},"source":{"053e6ed5":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport math\nimport pylab\nimport matplotlib.patches as patches\n\nnp.random.seed(410)\n\ndef sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\n\nx = (np.random.rand(15) * 20) - 10\ny = sigmoid(x)","5e302d5d":"y","33b19117":"# Now fit a model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom sklearn import metrics\n\nmodel = Sequential()\nmodel.add(Dense(20, input_dim=1, activation='relu'))\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(x, y, verbose=0, epochs=1000)\n\n# Predict and measure RMSE\npred = model.predict(x)\nscore = np.sqrt(metrics.mean_squared_error(pred, y))\nprint(f\"Score RMSE : {score}\")\n\n# important to remember it is learning the curve no the equations","b7b8be0b":"x2 = np.arange(-50.0, 50.0, 2.0)\ny_hat2 = model.predict(x2)\ny2 = sigmoid(x2)","a11683a4":"# Display the dataset\nimport pandas as pd\ndf = pd.DataFrame()\ndf['x'] = x2\ndf['y'] = y2\ndf['yHat'] = y_hat2\ndf","f6c7998c":"# Lets plot this\nplt.plot(x2, df['y'].tolist(), label='expected')\nplt.plot(x2, df['yHat'].tolist(), label='prediction')\nplt.ylabel('output')\nplt.legend()\nplt.savefig('nn-ext.png', dpi=300)\nplt.show()","2c2f6ac0":"Predict ","ed14b9ff":"sixteen selected engineered features:\n- Counts\n- Differences\n- Distance Between Quadratic Roots\n- Distance Formula\n- Logarithms\n- Max of Inputs\n- Polynomials\n- Power Ratio (such as BMI)\n- Powers\n- Ratio of a Product\n- Rational Differences\n- Rational Polynomials\n- Ratios\n- Root Distance\n- Root of a Ratio (such as Standard Deviation)\n- Square Roots","b85c2ba1":"Note:- Multi-variate feature engineering builds features from multiple inputs.<\/br>\nEnd to End ML : the model does all the work itself","5d5f8c8a":"## Many Models are Bad at Extrapolating\nSimple example, how well can a model learn to predict the sigmoid function. Predictions inside the training data go very well. Outside do not","7d73fe10":"# Why is Feature Engineering Even Needed?\n- End-to-End Machine learning is the dream of AutoML and Deep Learning\n- However, we are not there yet","84e7bf64":"Our model predict pretty well in the training set but outside the point it failed."}}