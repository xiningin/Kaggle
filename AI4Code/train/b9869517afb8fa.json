{"cell_type":{"0587247e":"code","cb579645":"code","e9927a07":"code","8ddd9b5c":"code","8b9c281b":"code","e3a74750":"code","15ff3c54":"code","21fe0fff":"code","ffe433e2":"code","800d7fde":"code","2d4dc9d5":"code","eb9fa67c":"code","55f94ac0":"code","c2450d01":"code","1a9cf679":"markdown","d7d63195":"markdown","852e8b83":"markdown","2de00b76":"markdown","a387c010":"markdown","d93eddc3":"markdown","808bd1d5":"markdown","84e2e5b1":"markdown","4d7fd0c9":"markdown","5b26ab0c":"markdown","c2f160ab":"markdown","fa50c9ec":"markdown","823b4991":"markdown","b7d5986e":"markdown"},"source":{"0587247e":"\n# Importing the required packages.\nfrom nltk.corpus import stopwords\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport sklearn\nimport re\n","cb579645":"\n# Read the CSV file using Pandas\ndf = pd.read_csv( '..\/input\/nlp-getting-started\/train.csv' )\ndf.head()\n","e9927a07":"\n# Truncate the dataFrame and use only `text` and `target` columns.\ndf = pd.read_csv( '..\/input\/nlp-getting-started\/train.csv' , usecols=[ 'text' , 'target' ] )\nprint( df.head() )\n\n# Store the entries in the above mentioned columns in NumPy arrays.\nraw_texts = df[ 'text' ].values\nraw_labels = df[ 'target' ].values\n","8ddd9b5c":"\n# Print the no. of samples for each class.\nprint( df[ 'target' ].value_counts() )\n\n# Compute the class weights\n# See this answer -> https:\/\/datascience.stackexchange.com\/a\/69302\/68023\nclass_weights = dict( zip( np.unique( raw_labels ), sklearn.utils.class_weight.compute_class_weight( 'balanced', np.unique( raw_labels ), raw_labels ))) \n","8b9c281b":"\n# Regex to remove non-alphabet characters\\\nr1 = re.compile( '[^a-zA-Z ]' )\n\n# Regex to remove hyperlinks ( \"https:\/\/...\" )\nr2 = re.compile( 'http:\/\/\\S+|https:\/\/\\S+' )\n\n# Regex to extract words starting with #\nr3 = re.compile( '#(\\w+)' )\n\n# Remove non-alphabet char from given sentence. See https:\/\/stackoverflow.com\/questions\/22520932\/python-remove-all-non-alphabet-chars-from-string\ndef remove_non_alphabet_char( sent ):\n    return r1.sub( '' , sent )\n\n# Remove hyperlinks from given sentence. See https:\/\/stackoverflow.com\/questions\/11331982\/how-to-remove-any-url-within-a-string-in-python\/11332580\ndef remove_links( sent ):\n    return r2.sub( '' , sent )\n\n# Remove emojis. See https:\/\/stackoverflow.com\/a\/33417311\/10878733\ndef remove_emojis( sent ):\n    emoji_pattern = re.compile(\"[\"\n        u\"\\U0001F600-\\U0001F64F\"\n        u\"\\U0001F300-\\U0001F5FF\" \n        u\"\\U0001F680-\\U0001F6FF\"\n        u\"\\U0001F1E0-\\U0001F1FF\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub( r'',sent )\n    \n\n# Clean the given sentence ( using the two methods above ) and tokenize it.\n# Also, remove stop words from the sentence.\ndef process_sent( sent ):\n    sent = text.lower()\n    sent = remove_links( sent )\n    sent = remove_non_alphabet_char( sent )\n    sent = remove_emojis( sent )\n    tokens = sent.split()\n    tokens = [ token.strip() for token in tokens if token not in stopwords.words( 'english' ) ]\n    return tokens\n    \n# Collect tokens and tokenized sentences in two arrays.\nprocessed_tokens = []\ntokenized_sentences = []\nfor text in raw_texts:\n    tokens = process_sent( text )\n    processed_tokens += tokens\n    tokenized_sentences.append( tokens )\n\n# Get unique tokens\nunique_tokens = list( set( processed_tokens ) )\nunique_tokens = np.array( unique_tokens )\n\n# Compute vocabulary size ( will be used for the Embedding layer )\nvocab_size = len( unique_tokens )\n# Create an array of indexed starting from 1 to vocab_size + 1 \n# For ex. [ 1 , 2 , 3 , ... , vocab_size ]\nindices = np.arange( 1 , vocab_size + 1 )\n\n# Zip unique_tokens and indices to create a dict with elements ( index , token ) where index has dtype=int and\n# token has dtype=str\n# This dict maps every index to its corresponding token.\nint_to_word = dict( zip( indices , unique_tokens ) )\n\n# This dict maps every token to its corresponding index.\nword_to_int = dict( zip( unique_tokens , indices ) )\n","e3a74750":"\n# Transform the given tokenized sentence to an integer sequence\n# For example, [ 'apple' , 'orange' ] --> [ 1023 , 1102]\ndef sent_to_int_seq( sent ):\n    int_seq = [ word_to_int[ token ] if token in unique_tokens else 0 for token in sent ]\n    return int_seq\n\n# Pad the given integer sequence with zeros ( from the end of the sequences )\n# For example, if maxlen=5,\n# [ 56 , 78 ] -> [ 56 , 78 , 0 , 0 , 0 ]\n# [ 34 , 56 , 78 , 23 , 13 , 12 ] -> [ 34 , 56 , 78 , 23 , 13 ]\ndef pad_sequence( seq , maxlen ):\n    out = np.zeros( shape=( maxlen , ) )\n    out[ 0 : len( seq ) ] = seq\n    return out\n\n# Compute the max length of the tokenized sentences.\n# Will be used for padding the sequences.\nmaxlen = max( [ len( arr ) for arr in tokenized_sentences ] )\nprint( f'Max Length for input sequences : {maxlen}')\n\n# Convert tokenized_sentences to integer sequences\n# Finally pad the integer sequence and store it in an array.\npadded_sentences = []\nfor sent in tokenized_sentences:\n    padded_sentences.append( pad_sequence( sent_to_int_seq( sent ) , maxlen ) )\n","15ff3c54":"\n# Convert list to ndarray\nx = np.array( padded_sentences )\nprint( x.shape )\n\n# Reshape raw_labels from shape ( num_labels , ) to ( num_labels , 1 )\n#y = raw_labels.reshape( -1 , 1) \ny = tf.keras.utils.to_categorical( raw_labels , num_classes=2 )\nprint( y.shape )\n","21fe0fff":"\n# Split the data into training and testing datasets.\ntrain_x , test_x , train_y , test_y = sklearn.model_selection.train_test_split( x , y , test_size=0.2 )\nprint( train_x.shape )\nprint( train_y.shape )\nprint( test_x.shape )\nprint( test_y.shape )\n","ffe433e2":"\n# Multilayer Perceptron with GeLU ( Gaussian Linear Units ) activation\ndef mlp( x , hidden_dims ):\n    y = tf.keras.layers.Dense( hidden_dims )( x )\n    y = tf.nn.gelu( y )\n    y = tf.keras.layers.Dense( x.shape[ -1 ] )( y )\n    y = tf.keras.layers.Dropout( 0.4 )( y )\n    return y\n\n# Token Mixing MLPs : Allow communication within tokens ( patches ) or, intuitively, between different parts\n# of the same sequence.\ndef token_mixing( x , token_mixing_mlp_dims ):\n    # x is a tensor of shape ( batch_size , num_patches , channels )\n    x = tf.keras.layers.LayerNormalization( epsilon=1e-6 )( x )\n    x = tf.keras.layers.Permute( dims=[ 2 , 1 ] )( x ) \n    # After transposition, shape of x -> ( batch_size , channels , num_patches )\n    x = mlp( x , token_mixing_mlp_dims )\n    return x\n\n# Channel Mixing MLPs : Allow communication within channels ( features of embeddings )\ndef channel_mixing( x , channel_mixing_mlp_dims ):\n    # x is a tensor of shape ( batch_size , num_patches , channels )\n    x = tf.keras.layers.LayerNormalization( epsilon=1e-6 )( x )\n    x = mlp( x , channel_mixing_mlp_dims )\n    return x\n\n# Mixer layer consisting of token mixing MLPs and channel mixing MLPs\n# input shape -> ( batch_size , channels , num_patches )\n# output shape -> ( batch_size , channels , num_patches )\ndef mixer( x , token_mixing_mlp_dims , channel_mixing_mlp_dims ):\n    # inputs x of are of shape ( batch_size , num_patches , channels )\n    # Note: \"channels\" is used instead of \"embedding_dims\"\n    \n    # Add token mixing MLPs\n    token_mixing_out = token_mixing( x , token_mixing_mlp_dims )\n    # Shape of token_mixing_out -> ( batch_size , channels , num_patches )\n\n    token_mixing_out = tf.keras.layers.Permute( dims=[ 2 , 1 ] )( token_mixing_out )\n    # Shape of transposition -> ( batch_size , num_patches , channels )\n    \n    #  Add skip connection\n    token_mixing_out = tf.keras.layers.Add()( [ x , token_mixing_out ] )\n\n    # Add channel mixing MLPs\n    channel_mixing_out = channel_mixing( token_mixing_out , channel_mixing_mlp_dims )\n    # Shape of channel_mixing_out -> ( batch_size , num_patches , channels )\n    \n    # Add skip connection\n    channel_mixing_out = tf.keras.layers.Add()( [ channel_mixing_out , token_mixing_out ] )\n    # Shape of channel_mixing_out -> ( batch_size , num_patches , channels )\n\n    return channel_mixing_out\n","800d7fde":"\n# These hyperparameters were searched with KerasTuner\nembedding_dims = 64\ntoken_mixing_mlp_dims = 32\nchannel_mixing_mlp_dims = 64\npatch_size = 5\nnum_mixer_layers = 8\nlearning_rate = 5e-3\n\nnum_classes = 2\nseq_input_shape = ( maxlen , )\n    \n# Model input layer\ninputs = tf.keras.layers.Input( shape=seq_input_shape )\n\n# Embedding layer which converts int sequences into dense vectors\nembedding = tf.keras.layers.Embedding( input_dim=vocab_size + 1 , output_dim=embedding_dims , input_length=maxlen )( inputs )\n    \n# Conv1D layer to produce patches from given sequences. \npatches = tf.keras.layers.Conv1D( embedding_dims ,\n                                 kernel_size=patch_size ,\n                                 strides=patch_size ,\n                                 use_bias=False , \n                                 trainable=False )( embedding )\n    \nx = patches\nfor _ in range( num_mixer_layers ):\n    x = mixer( x , token_mixing_mlp_dims , channel_mixing_mlp_dims )\n        \nx = tf.keras.layers.LayerNormalization( epsilon=1e-6 )( x )\nx = tf.keras.layers.GlobalAveragePooling1D()( x )\noutputs = tf.keras.layers.Dense( num_classes , activation='softmax' )( x )\n\nmodel = tf.keras.models.Model( inputs , outputs )\nmodel.summary()\n","2d4dc9d5":"\n# Batch size and epochs\nbatch_size = 32\nnum_epochs = 5\n\n# Compile the model and start the training\nmodel.compile(\n    loss='categorical_crossentropy' , \n    optimizer=tf.keras.optimizers.Adam( learning_rate ) ,\n    metrics=[ 'accuracy' ]\n)\n\nmodel.fit(train_x ,\n          train_y ,\n          batch_size=batch_size ,\n          validation_data=( test_x , test_y ) ,\n          epochs=num_epochs , \n          class_weight=class_weights ,\n        )\n","eb9fa67c":"\n# Fetch model predictions for test_x\npred_y = model.predict( test_x )\n\n# Print the classification report\nreport = sklearn.metrics.classification_report( np.argmax( test_y , axis=1 ) , np.argmax( pred_y , axis=1 ) , target_names=[ 'not disaster' , 'disaster' ] )\nprint( report )\n","55f94ac0":"\n# Plot the confusion matrix\nconf_matrix = sklearn.metrics.confusion_matrix( np.argmax( test_y , axis=1 ) ,np.argmax( pred_y , axis=1 ) )\ndisp = sklearn.metrics.ConfusionMatrixDisplay( conf_matrix , display_labels=[ 'not disaster' , 'disaster' ] )\ndisp.plot() \n","c2450d01":"\n# Read the test.csv file\ntest_df = pd.read_csv( '..\/input\/nlp-getting-started\/test.csv' , usecols=[ 'text' , 'id' ] )\n\n# Clean, tokenize, pad the sentences from test_df\ntest_inputs = []\nfor text in test_df[ 'text' ].values:\n    tokens = process_sent( text )\n    out = sent_to_int_seq( tokens )\n    out = pad_sequence( out , maxlen )\n    test_inputs.append( out )\n    \n# Fetch predictions for test_inputs\ntest_inputs = np.array( test_inputs )\npredicted_labels = np.argmax( model.predict( test_inputs ) , axis=1 )\n    \nids = test_df[ 'id' ].values\n\n# Create the submission.csv file from ids and predicted_labels\nsubmission_csv = { 'id' : ids , 'target' : predicted_labels }\nsubmission_csv = pd.DataFrame.from_dict( submission_csv )\nsubmission_csv.to_csv( 'submission.csv' , index=False )\n","1a9cf679":"\nAlongside, we plot the confusion matrix\n","d7d63195":"\nOur next step is to transform each tokenized sentence into a integer sequence. This conversion is performed by the `sent_to_int_seq()` method which takes in tokenized sentence like `[ 'apple' , 'orangle' ]` and maps it to a integer sequence ( consisting of tokens' indices retreived from `word_to_int` ) like `[ 1023 , 1102 ]`.\n\nAlso, we compute the maximum length of these sequences in order to perform zero padding using `pad_sequence()`\n\n```\nmaxlen = max( [ len( arr ) for arr in tokenized_sentences ] )\n```\n","852e8b83":"\n### b. \ud83e\uddf9 **Cleaning and tokenizing the textual data**\n\n![tokenization](https:\/\/www.kdnuggets.com\/wp-content\/uploads\/text-tokens-tokenization-manning.jpg)\n\n> Image Source: [Tokenization and Text Data Preparation - KDNuggets](https:\/\/www.kdnuggets.com\/2020\/03\/tensorflow-keras-tokenization-text-data-prep.html)\n\nAs you might have observed, the text ( of the tweets ) contains hashtags and URLs to images attached with the tweet. These are unnecessary features and hence we use regular expressions to filter each tweet. We perform a two-step filtration process:\n\n1. Convert the words in the tweet to lowercase. Also, remove all non-alphabet characters from the text, like numbers, punctuations. Refer to this [StackOverflow answer](https:\/\/stackoverflow.com\/questions\/22520932\/python-remove-all-non-alphabet-chars-from-string).\n\n2. Remove hyperlinks from the tweet. Refer to this [StackOverflow answer](https:\/\/stackoverflow.com\/questions\/11331982\/how-to-remove-any-url-within-a-string-in-python\/11332580)\n\n3. Remove \ud83d\ude00 emojis from the tweet. Refer to this [StackOverflow answer](https:\/\/stackoverflow.com\/a\/33417311\/10878733)\n\nFinally, we tokenize all the sentences and store these tokens in `processed_tokens`. After eliminating all duplicate tokens, we assign an index to each token and create two dictionaries which map each token to its index and vice-versa. See `word_to_index` and `index_to_word`.\n\n","2de00b76":"\nAs seen in the output of the code cell below, the CSV data contains 5 columns. Considering our problem, we'll truncate `df` and use `text` and `target` columns. The reason behind eliminating columns `keyword` and `location` is,\n\n* As we classifying tweets, the `location` \ud83c\udfe0 of a tweet is insignificant. The column also contains several null entries ( interpreted as `NaN` ).\n* The `keyword` column could be used as a *metadata* to the existing tweet. But we drop its use as it contains several `NaN` values. Even if we try to drop these rows containing `NaN` using `pandas.dropNa()`, we'll be left with insufficient data for training our model. ( Neural networks require larger amounts of data for training )\n\nAlso, we store the entries of these columns in NumPy arrays.\n","a387c010":"\nWe have completed the data preprocessing. We'll now discuss more on the training of our model.\n","d93eddc3":"\n<center><h1>\ud83d\udc24 <b><u>Tweet Classification with MLP-Mixers ( TF-Keras )<\/u><\/b><\/h1><\/center>\n\n![mlp_mixer_diagram](https:\/\/github.com\/shubham0204\/Google_Colab_Notebooks\/blob\/main\/data\/mlp_mixer_diagram.PNG?raw=true)\n\n> Image Source: [MLP-Mixer: An all-MLP Architecture for Vision](https:\/\/arxiv.org\/abs\/2105.01601)\n\n\nEntering the **\"Natural Language Processing with Disaster Tweets\"**, our goal is to design a ML model which can efficiently classify tweets into two categories, the ones which refer ( or describe ) to a \ud83d\ude31 disaster and others which don't contain any such context \ud83d\ude01.\n\n> In this notebook, you'll learn how to implement an MLP-Mixer architecture for classifying with TensorFlow Keras.\n\nHere's the list of all things we'll do ( and enjoy! ) in this notebook,\n\n* First, we process the raw texts in order to \ud83e\uddf9 eliminate unwanted characters and symbols.\n* We implement our MLP-Mixer model with TensorFlow and train it on the processed data.\n* Evaluate the model and produce the `submission.csv` file.\n","808bd1d5":"\n## 2. \ud83e\udd16 **Training the model**\n\nMLP Mixer is designed originally for image classification problems, as observed in their [research paper](https:\/\/arxiv.org\/abs\/2105.01601). We modify the architecture and produce patches from embeddings of shape `( num_patches , embedding_dims )`. In context of textual data, `embedding_dims` could be thought as `num_channels` in an image. \n\nThe rest of the components including MLPs and Mixer layers remain as they are.\n\n","84e2e5b1":"\n## 4. \u270d\ufe0f **Submitting the results**\n\nFor submitting our predictions to the competition, we need to write them in a CSV file called `submission.csv`. We parse the tweets from `test.csv`, clean them and generate perdictions for them. These predictions are stored with their corresponding ids in `submission.csv`.\n","4d7fd0c9":"\n## 3. \ud83e\uddbe **Evaluating the model**\n\n![Evaluation Metrics](https:\/\/static.packt-cdn.com\/products\/9781785282287\/graphics\/B04223_10_02.jpg)\n\n> Image Source: [Computing precision, recall, and F1-score - Packt](https:\/\/subscription.packtpub.com\/book\/big_data_and_business_intelligence\/9781785282287\/10\/ch10lvl1sec133\/computing-precision-recall-and-f1-score)\n\nAfter training our model, we'll like to evaluate it using our test data. Using `sklearn.metrics.classification_report` we examine the precision, recall and f1 scores for the two classes, `disaster` and `not disaster`.\n","5b26ab0c":"\n## 1. \ud83d\udee0 Processing the text data\n\n### a. \u2702\ufe0f **Reading and truncating the CSV data**\n\nThe data provided to us is in the CSV format. First, we parse the CSV file using `pandas.read_csv` which transform it into a `DataFrame` object, which eases data handling.\n","c2f160ab":"\nCompile the model with `SparseCategoricalFocalLoss` and `Adam` optimizer. We'll use the class weights, which we computed earlier, in the `model.fit()` method.\n","fa50c9ec":"\nWe split our data into training and testing datasets using `sklearn.model_selection.train_test_split()` using 20% of the data for evaluating our model.\n","823b4991":"\nWe have cleaned the data and now we are left with integer sequences of shape `( num_samples , maxlen )` and their corresponding labels of shape `( num_samples , 1 )`.\n","b7d5986e":"\nIn the output of the code cell below, you'll observe that we have unequal samples belonging to both the classes. This may affect our model's performance on any of the classes. Hence, we compute the class weights using `sklearn.utils.class_weight.compute_class_weight` which will be used for weighing the losses for each of the classes.\n    "}}