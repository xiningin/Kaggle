{"cell_type":{"fe5b88b5":"code","84890cc3":"code","da75bfa5":"code","2f08658c":"code","cb4fd6f9":"code","e3264633":"code","dad28544":"code","f9d13a83":"code","b1f60ee0":"code","ead630b5":"code","aa106a14":"code","e01f4218":"code","e94c03ff":"code","8775edc3":"code","9dbc54a5":"code","83a921a9":"code","ab2ce9c8":"code","7110608b":"code","bab2bc7e":"markdown","b66f4de9":"markdown","eb8db832":"markdown","9d2c0c4f":"markdown","64959573":"markdown","809218ea":"markdown","43f39f01":"markdown","0dc96ce4":"markdown","ec0c0650":"markdown","9a4f1b5a":"markdown","14ac720e":"markdown","c6e25e43":"markdown","39fff169":"markdown","748962bd":"markdown","fe0adf64":"markdown","938dfb89":"markdown","44610885":"markdown"},"source":{"fe5b88b5":"import os\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nimport traitlets\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nfrom tokenizers import BertWordPieceTokenizer\nfrom sklearn.metrics import roc_auc_score\n\nwarnings.simplefilter(\"ignore\")","84890cc3":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","da75bfa5":"def build_model(transformer, loss='binary_crossentropy', max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    x = tf.keras.layers.Dropout(0.35)(cls_token)\n    out = Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=3e-5), loss=loss, metrics=[tf.keras.metrics.AUC()])\n    \n    return model","2f08658c":"# https:\/\/stackoverflow.com\/questions\/8897593\/how-to-compute-the-similarity-between-two-text-documents\nimport nltk, string\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nnltk.download('punkt') # if necessary...\n\n\nstemmer = nltk.stem.porter.PorterStemmer()\nremove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n\ndef stem_tokens(tokens):\n    return [stemmer.stem(item) for item in tokens]\n\n'''remove punctuation, lowercase, stem'''\ndef normalize(text):\n    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n\nvectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')\n\ndef cosine_sim(text1, text2):\n    tfidf = vectorizer.fit_transform([text1, text2])\n    return ((tfidf * tfidf.T).A)[0,1]","cb4fd6f9":"AUTO = tf.data.experimental.AUTOTUNE\n\n# Create strategy from tpu\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# Data access\n#GCS_DS_PATH = KaggleDatasets().get_gcs_path('kaggle\/input\/') ","e3264633":"# First load the real tokenizer\ntokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Save the loaded tokenizer locally\nsave_path = '\/kaggle\/working\/distilbert_base_uncased\/'\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\ntokenizer.save_pretrained(save_path)\n\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('distilbert_base_uncased\/vocab.txt', lowercase=True)\nfast_tokenizer","dad28544":"train1 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv\")\ntrain2 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv\")\n\nvalid = pd.read_csv('\/kaggle\/input\/val-en-df\/validation_en.csv')\ntest1 = pd.read_csv('\/kaggle\/input\/test-en-df\/test_en.csv')\ntest2 = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-test-translated\/jigsaw_miltilingual_test_translated.csv')\nsub = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv')","f9d13a83":"test2.head()","b1f60ee0":"plt.figure(figsize=(12, 8))\n\nsns.distplot(train1.comment_text.str.len(), label='train')\nsns.distplot(test1.content_en.str.len(), label='test1')\nsns.distplot(test2.translated.str.len(), label='test2')\nplt.legend();","ead630b5":"plt.figure(figsize=(12, 8))\n\nsns.distplot(train1.comment_text.str.len(), label='train')\nsns.distplot(test1.content_en.str.len(), label='test1')\nsns.distplot(test2.translated.str.len(), label='test2')\nplt.xlim([0, 512])\nplt.legend();","aa106a14":"test_set_similarity = [cosine_sim(t1, t2) for t1, t2 in tqdm(zip(test1.content_en, test2.translated))]\n\nplt.figure(figsize=(12, 8))\n\nsns.distplot(test_set_similarity);","e01f4218":"x_train = fast_encode(train1.comment_text.astype(str), fast_tokenizer, maxlen=512)\nx_valid = fast_encode(valid.comment_text_en.astype(str), fast_tokenizer, maxlen=512)\nx_test1 = fast_encode(test1.content_en.astype(str), fast_tokenizer, maxlen=512)\nx_test2 = fast_encode(test2.translated.astype(str), fast_tokenizer, maxlen=512)\n\ny_train = train1.toxic.values\ny_valid = valid.toxic.values","e94c03ff":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(64)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(64)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = [(\n    tf.data.Dataset\n    .from_tensor_slices(x_test1)\n    .batch(64)\n),\n    (\n    tf.data.Dataset\n    .from_tensor_slices(x_test2)\n    .batch(64)\n)]","8775edc3":"from tensorflow.keras import backend as K\n\ndef focal_loss(gamma=2., alpha=.2):\n    def focal_loss_fixed(y_true, y_pred):\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n    return focal_loss_fixed","9dbc54a5":"%%time\nwith strategy.scope():\n    transformer_layer = transformers.TFBertModel.from_pretrained('bert-base-uncased')\n    model = build_model(transformer_layer, loss=focal_loss(gamma=1.5), max_len=512)\nmodel.summary()","83a921a9":"def build_lrfn(lr_start=0.000001, lr_max=0.000002, \n               lr_min=0.0000001, lr_rampup_epochs=7, \n               lr_sustain_epochs=0, lr_exp_decay=.87):\n    lr_max = lr_max * strategy.num_replicas_in_sync\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        return lr\n    \n    return lrfn","ab2ce9c8":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 7))\n\n_lrfn = build_lrfn()\nplt.plot([i for i in range(35)], [_lrfn(i) for i in range(35)]);","7110608b":"\nlrfn = build_lrfn()\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)\n\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=150,\n    validation_data=valid_dataset,\n    epochs=20,\n    verbose=1\n)","bab2bc7e":"## Build datasets objects","b66f4de9":"## Create fast tokenizer","eb8db832":"## Test dataset comparision","9d2c0c4f":"This notebook completely focuses on how to use BERT for classification. The library used is Huggingface. It is a popular library which helps us to use pre-trained transformer based architectures like BERT, T-5, RoBERTa, etc.. Here we will use tensorflow based BERT model imported from the library. \n\nThe aim of the model will be to classify wether a comment is toxic or not. \n\nhttps:\/\/www.kaggle.com\/c\/jigsaw-multilingual-toxic-comment-classification\/overview","64959573":"## TPU Configs","809218ea":"Cosine similarity calculates similarity by measuring the cosine of angle between two vectors. This is calculated as:\n![](https:\/\/miro.medium.com\/max\/426\/1*hub04IikybZIBkSEcEOtGA.png)\n\nCosine Similarity calculation for two vectors A and B [source]\nWith cosine similarity, we need to convert sentences into vectors. One way to do that is to use bag of words with either TF (term frequency) or TF-IDF (term frequency- inverse document frequency). The choice of TF or TF-IDF depends on application and is immaterial to how cosine similarity is actually performed \u2014 which just needs vectors. TF is good for text similarity in general, but TF-IDF is good for search query relevance.","43f39f01":"# Focal Loss","0dc96ce4":"# Reference\n\n\n* [Original Reference](https:\/\/www.kaggle.com\/miklgr500\/jigsaw-tpu-bert-with-huggingface-and-keras)","ec0c0650":"BERT has completely outperformed previous architectures. Introduction of BERT can be understood as the Inception stage of Natural Language Processing. \n\n![image.png](attachment:image.png)","9a4f1b5a":"## Fast encode","14ac720e":"# LrScheduler","c6e25e43":"Lets calculate cosine similarity two translated test datasets.","39fff169":"## Helper Functions","748962bd":"Clearly our model is overfitting but the point to notice here is that in just few epochs the model climbed the AUC score of 91%. This is the power of this State-of-the-Art model by Google.","fe0adf64":"## Load model into the TPU","938dfb89":"## Load text data into memory","44610885":"## Train Model"}}