{"cell_type":{"8440ccc2":"code","88266f8e":"code","1254ab1b":"code","eb00970f":"code","23c7d1a3":"code","e17371a2":"code","239ed934":"code","0ed0a0a3":"code","2b9abb75":"code","437542dd":"code","67bdbc9a":"code","8eb028a8":"code","c7cafec4":"code","98eb0f7e":"code","2abd135f":"code","15426158":"code","6f45f1de":"code","066b4945":"code","294c6e4c":"code","77803d2b":"code","2e9d33d4":"markdown","8c064417":"markdown","e9c4862a":"markdown","cef7f233":"markdown","a6d5b767":"markdown","aeae5541":"markdown","cd867d6c":"markdown","9fc1dfe3":"markdown","5fcf7299":"markdown","0561eea7":"markdown","0d6cb44a":"markdown","53082052":"markdown","c6ea1963":"markdown","7dfe1b65":"markdown"},"source":{"8440ccc2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","88266f8e":"df_census = pd.read_csv('..\/input\/census-cleanedcsv\/census_cleaned.csv')","1254ab1b":"df_census.head()","eb00970f":"df_census.info()","23c7d1a3":"df_census.describe()","e17371a2":"X = df_census.iloc[:,:-1]\ny = df_census.iloc[:,-1]","239ed934":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)","0ed0a0a3":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score","2b9abb75":"clf = DecisionTreeClassifier(random_state=2)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\naccuracy_score(y_pred, y_test)","437542dd":"from sklearn import tree\nimport graphviz \nimport matplotlib.pyplot as plt\n# plt.figure(figsize=(20,15))\n# tree.plot_tree(clf, max_depth=2);","67bdbc9a":"# dot_data = tree.export_graphviz(clf, out_file=None, max_depth=2) \n# graph = graphviz.Source(dot_data)\ndot_data = tree.export_graphviz(clf, out_file=None, \n                      feature_names=list(X.columns),\n                      class_names='50K',          \n                      filled=True, rounded=True,  \n                      special_characters=True, max_depth=2)  \ngraph = graphviz.Source(dot_data)\ngraph ","8eb028a8":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nimport warnings\nwarnings.filterwarnings('ignore')","c7cafec4":"X_census = df_census.iloc[:,:-1]\ny_census = df_census.iloc[:,-1]","98eb0f7e":"rf = RandomForestClassifier(n_estimators=10, random_state=2, n_jobs=-1)\nscores = cross_val_score(rf, X_census, y_census, cv=5)","2abd135f":"print('Accuracy:', np.round(scores, 3))\nprint('Accuracy mean: %0.3f' % (scores.mean()))","15426158":"rf = RandomForestClassifier(bootstrap=True, oob_score=True, n_estimators=100, warm_start=True, random_state=2, n_jobs=-1, verbose=0.5)\nscores = cross_val_score(rf, X_census, y_census, scoring='neg_mean_squared_error',cv=5)\nrf.fit(X_census, y_census)\nrf.oob_score_\n# rmse = np.sqrt(-scores)\n# print('RMSE:', np.round(rmse, 3))\n# print('RMSE mean: %0.3f' % (scores.mean()))","6f45f1de":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\noob_scores = []\nrf = RandomForestClassifier(oob_score=True, n_estimators=50, warm_start=True, random_state=2, n_jobs=-1)\nrf.fit(X_census, y_census)\noob_scores.append(rf.oob_score_)\nest = 50\nestimators = [est]\n\nfor i in range(9):\n    est += 50\n    estimators.append(est)\n    rf.set_params(n_estimators=est)\n    rf.fit(X_census, y_census)\n    oob_scores.append(rf.oob_score_)\n\nplt.figure(figsize=(15,7))\nplt.plot(estimators, oob_scores)\nplt.xlabel('Number of Trees')\nplt.ylabel('oob_score_')\nplt.title('Random Forest Warm Start', fontsize=15)\nplt.show()","066b4945":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_census, y_census, random_state=2)\n\ndef randomized_search_clf(params, oob_score=True, runs=16, clf=RandomForestClassifier(random_state=2, n_jobs=-1)):\n    rand_clf = RandomizedSearchCV(clf, params, n_iter=runs, scoring='neg_mean_squared_error', cv=10, n_jobs=-1, random_state=2)\n    \n    rand_clf.fit(X_train, y_train)\n    best_model = rand_clf.best_estimator_\n    best_params = rand_clf.best_params_\n    print('Best params:', best_params)\n    best_score = np.sqrt(-rand_clf.best_score_)\n    print('Training score:{:.3f}'.format(best_score))\n    y_pred = best_model.predict(X_test)\n    rmse_test = MSE(y_test, y_pred)**0.5\n    print('Test set score:{:.3f}'.format(rmse_test))","294c6e4c":"randomized_search_clf(params={'min_weight_fraction_leaf':[0.0, 0.0025,0.005, 0.0075, 0.01, 0.05],\n                             'min_samples_leaf':[1,2,4,6,8,10,20,30],\n                             'min_impurity_decrease':[0.0, 0.01, 0.05, 0.10, 0.15, 0.2],\n                             'max_leaf_nodes':[10,15,20,25,30,35,40,45,50,None],\n                             'max_features':['auto', 0.8,0.7,0.6,0.5,0.4],\n                             'max_depth':[None,2,4,6,8,10,20],\n                             'n_estimators':[100]})","77803d2b":"# to be continued...","2e9d33d4":"Next we build and fit model, make prediction and compare predictions with test set.","8c064417":"Next we need to calculate accuracy_score","e9c4862a":"Accuracy mean of Random Forest worse than Decision Tree. We will try to made it better.","cef7f233":"there is very strange - fine tuning did not help.","a6d5b767":"# Exploring random forests","aeae5541":"I have added parameters: oob_score, warm_start.\n\nIf we want to display more information when building model we can set **verbose** to a higher number.","cd867d6c":"# Accuracy score is more than 81% there is not bad!","9fc1dfe3":"The example is taken for educational purposes from a book by Corey Wade. \"Practical gradient enhancement with XGBoost and scikit-learn\"","5fcf7299":"We see that the peak in the number of trees is located at 300. We will try to fine-tune hyperparameters using RandonizedSearchCV","0561eea7":"# Today we will learn something new about descision trees","0d6cb44a":"Next step is train_test_split","53082052":"As usual we are starting from import new file and let's get to know it better.","c6ea1963":"\u00abWe start by building a decision tree to predict whether someone makes over 50K US dollars using the Census dataset\u00bb\n\nAfter uploading file we must to split columns to predictor and target.","7dfe1b65":"Now we will experiment with hyperparameters."}}