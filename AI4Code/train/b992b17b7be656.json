{"cell_type":{"b96d5b39":"code","599276de":"code","8c8e51be":"code","92c456b0":"code","12e731e5":"code","9a73c5a3":"code","81cef99e":"code","826cdd22":"code","cf85b419":"code","fd4c9f06":"code","bf21b6fd":"code","b0a1c3ac":"code","4d215dae":"code","becf775d":"code","10f60980":"code","e55bd248":"code","1bba1ffa":"code","81a10b93":"code","7a437e31":"code","10608bf8":"code","03e8468b":"code","26cc207e":"code","72639330":"code","dbaa8582":"code","e06b9dfa":"code","d201ea9f":"code","3c2500d0":"code","aae7ed06":"code","846a1efe":"code","66cdea8d":"code","c27236b1":"code","6ea14c1b":"code","233a0028":"code","8f94f91a":"code","0cb09617":"code","04c7e188":"code","06f38abf":"code","27c054ef":"code","44175195":"code","40afb00e":"code","c3f7d3b0":"code","6be35d46":"code","765710ac":"code","8c49bd8d":"code","b9e1d183":"code","67440bbf":"code","71583a6c":"code","687b4063":"code","3b60467e":"code","571219a3":"code","5671e94a":"code","4c736b3d":"code","1f3347f4":"code","75077c38":"code","c40ef7f4":"code","c7a20bd6":"code","caa19030":"code","4f44d0ef":"code","69a403f2":"code","2c20e94b":"code","d03373ee":"code","7b6f03dd":"code","3190c30b":"code","1e916ea6":"code","e5f3f382":"code","7c35acf0":"code","dcac030a":"code","83768ec8":"code","ccaa8de5":"code","d8bedfd6":"code","23227b75":"code","916cccad":"code","e34d83fe":"code","1581f860":"code","f11df952":"markdown","56c97f86":"markdown","9efcbb24":"markdown","7aa64a42":"markdown","4a3ec3fe":"markdown","931f168d":"markdown","80ff768f":"markdown","aac7d0c2":"markdown","b9c84f0f":"markdown","3dd7220a":"markdown","3207d679":"markdown"},"source":{"b96d5b39":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","599276de":"# Importing necessary libraries \n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","8c8e51be":"# Read the train and test datasets \ndf_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","92c456b0":"# checking the shapes of the train and test datasets\nprint(\"Shape of train: \", df_train.shape)\nprint(\"Shape of test: \", df_test.shape) # No target variable ","12e731e5":"# print the train records \ndf_train.head()","9a73c5a3":"# print the test records \ndf_test.head()","81cef99e":"df_train.info()","826cdd22":"df_train.describe([0.25,0.50,0.75,0.99])","cf85b419":"# Missing values  \ndf_train.isnull().sum()","fd4c9f06":"# list of numerical variables\nnumerical_features = [feature for feature in df_train.columns if df_train[feature].dtypes != 'O'] \n\n# list of variables that contain year information\nyear_feature = [feature for feature in numerical_features if 'Yr' in feature or 'Year' in feature]\n\nyear_feature","bf21b6fd":"# look into the content of these year variables\nfor feature in year_feature:\n    print(feature, df_train[feature].unique())","b0a1c3ac":"## Let us see whether there is a relation between year the house is sold and the sales price\n\ndf_train.groupby('YrSold')['SalePrice'].median().plot()\nplt.xlabel('Year Sold')\nplt.ylabel('Median House Price')\nplt.title(\"House Price vs YearSold\")","4d215dae":"## We will compare the difference between All years feature with SalePrice\n\nfor feature in year_feature:\n    if feature!='YrSold':\n\n        ## We will capture the difference between year variable and year the house was sold for\n        df_train[feature]=df_train['YrSold'] - df_train[feature]\n\n        plt.scatter(df_train[feature],df_train['SalePrice'],c = 'black')\n        plt.xlabel(feature)\n        plt.ylabel('SalePrice')\n        plt.show()","becf775d":"# list of numerical variables\nnumerical_features = [feature for feature in df_train.columns if df_train[feature].dtypes != 'O']\n\nprint('Number of numerical variables: ', len(numerical_features))\n\n# visualise the numerical variables\ndf_train[numerical_features].head()","10f60980":"# Visualising numerical variables with SalePrice\ndf_num = df_train.select_dtypes(include=['int64','float64'])\nfig,axs= plt.subplots(12,3,figsize=(20,80))\nfor i,ax in zip(df_num.columns,axs.flatten()):\n    sns.scatterplot(x=i, y='SalePrice', hue='SalePrice',data=df_num,ax=ax,palette='icefire')\n    plt.xlabel(i,fontsize=12)\n    plt.ylabel('SalePrice',fontsize=12)\n    ax.set_title('SalePrice'+' VS '+str(i))","e55bd248":"categorical_features=[feature for feature in df_train.columns if df_train[feature].dtypes=='O']\nprint('Number of categorical variables: ', len(categorical_features))\n\ndf_train[categorical_features].head()","1bba1ffa":"for feature in categorical_features:\n    print('The feature is {} and number of categories are {}'.format(feature,len(df_train[feature].unique())))","81a10b93":"# Plotting Categorical Features with Sale Price\ndef facetgrid_boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\n    \ncategorical = df_train.select_dtypes(exclude=['int64','float64'])\nf = pd.melt(df_train, id_vars=['SalePrice'], value_vars=sorted(df_train[categorical.columns]))\ng = sns.FacetGrid(f, col=\"variable\", col_wrap=3, sharex=False, sharey=False, size=5)\ng = g.map(facetgrid_boxplot, \"value\", \"SalePrice\")","7a437e31":"# Checking the corelation using heatmap\nplt.subplots(figsize = (25,20))\nsns.heatmap(round(df_train.corr(),2), cmap='coolwarm' , annot=True, center = 0)\nplt.show()","10608bf8":"# Most correlated features\ncorrmat = df_train.corr()\ntop_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0.5]\nplt.figure(figsize=(10,10))\na = sns.heatmap(df_train[top_corr_features].corr(),annot=True)","03e8468b":"df_train.head()","26cc207e":"## Missing values \n\nfeatures_nan=[feature for feature in df_train.columns if df_train[feature].isnull().sum() and df_train[feature].dtypes=='O']\n\nfor feature in features_nan:\n    print(\"{}: {}% missing values\".format(feature,np.round(df_train[feature].isnull().mean(),3)))","72639330":"## Replace missing value with a new label \ndef replace_cat_feature(dataset,features_nan):\n    df_train[features_nan]=df_train[features_nan].fillna('Missing')\n    return df_train\n\ndataset=replace_cat_feature(df_train,features_nan)\n\n# df_train[features_nan].isnull().sum()","dbaa8582":"df_train.head()","e06b9dfa":"## Numerical variables \n\n## Now lets check for numerical variables the contains missing values\nnumerical_with_nan=[feature for feature in df_train.columns if df_train[feature].isnull().sum() and df_train[feature].dtypes!='O']\n\n## We will print the numerical nan variables and percentage of missing values\n\nfor feature in numerical_with_nan:\n    print(\"{}: {}% missing value\".format(feature,np.around(df_train[feature].isnull().mean(),3)))","d201ea9f":"## Replacing the Numerical Missing Values\n\nfor feature in numerical_with_nan:\n    ## We will replace by using mean since there are outliers\n    mean_value = df_train[feature].mean()\n    \n    ## create a new feature to capture nan values\n    df_train[feature+'nan']=np.where(df_train[feature].isnull(),1,0)\n    df_train[feature].fillna(mean_value,inplace=True)\n    ","3c2500d0":"df_train.head()","aae7ed06":"## Date Time Variables\n\nfor feature in ['YearBuilt','YearRemodAdd','GarageYrBlt']:\n       \n    df_train[feature] = df_train['YrSold'] - df_train[feature]\n\ndf_train[['YearBuilt','YearRemodAdd','GarageYrBlt']].head()","846a1efe":"# Categorical features \ncategorical_features","66cdea8d":"for feature in categorical_features:\n    temp=df_train.groupby(feature)['SalePrice'].count()\/len(df_train)\n    temp_df=temp[temp>0.01].index\n    df_train[feature]=np.where(df_train[feature].isin(temp_df),df_train[feature],'Rare_var')","c27236b1":"df_train.head()","6ea14c1b":"for feature in categorical_features:\n    labels_ordered=df_train.groupby([feature])['SalePrice'].mean().sort_values().index\n    labels_ordered={k:i for i,k in enumerate(labels_ordered,0)}\n    df_train[feature]=df_train[feature].map(labels_ordered)\n\ndf_train.head(20)","233a0028":"# Handling outliers \nnum_col = list(df_train.dtypes[df_train.dtypes !='object'].index)\nnum_col = ['LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','TotalBsmtSF','1stFlrSF','GrLivArea','OpenPorchSF',\n           'EnclosedPorch','3SsnPorch','ScreenPorch' ,'PoolArea','MiscVal','SalePrice']\ndef drop_outliers(x):\n    list = []\n    for col in num_col:\n        Q1 = x[col].quantile(.25)\n        Q3 = x[col].quantile(.99)\n        IQR = Q3-Q1\n        x =  x[(x[col] >= (Q1-(1.5*IQR))) & (x[col] <= (Q3+(1.5*IQR)))] \n    return x   \n\ndf_train = drop_outliers(df_train)","8f94f91a":"from scipy import stats\nfrom scipy.stats import norm","0cb09617":"## Draw distribution plot with normal distribution fitted curve\n## Draw Quantile-Quantile plot \n  \ndef normality_plot(X):\n\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\n    sns.distplot(X, fit=norm, ax=axes[0])\n    axes[0].set_title('Distribution Plot')\n\n    axes[1] = stats.probplot((X), plot=plt)\n    plt.tight_layout()","04c7e188":"y = df_train.SalePrice\nnormality_plot(y)","06f38abf":"# One of the methods to normalize right-skewed data is using log transformation because big values will be pulled to the center. \n# However, log(0) is Nan, so will try using log(1+X) to fix skewness instead.\n\ny = np.log(1 + y)","27c054ef":"# SalePrice after log transformation, the skewness is fixed \n\nnormality_plot(y)","44175195":"# Checking the distribution before scaling \nplt.figure(figsize=(16,6))\nsns.distplot(df_train.SalePrice)\nplt.show()","40afb00e":"feature_scale=[feature for feature in df_train.columns if feature not in ['SalePrice']]\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\nscaler.fit(df_train[feature_scale])","c3f7d3b0":"scaler.transform(df_train[feature_scale])","6be35d46":"# transform the train dataset and add on the saleprice variables\ndf_train = pd.concat([df_train[['SalePrice']].reset_index(drop=True),\n                    pd.DataFrame(scaler.transform(df_train[feature_scale]), columns=feature_scale)],\n                    axis=1)\n\ndf_train.head()","765710ac":"# Distribution after scaling \n\nplt.figure(figsize=(16,6))\nplt.subplot(121)\nsns.distplot(df_train.SalePrice)","8c49bd8d":"df_test.head()","b9e1d183":"## Missing values \n\nfeatures_nan=[feature for feature in df_test.columns if df_test[feature].isnull().sum() and df_test[feature].dtypes=='O']\n\nfor feature in features_nan:\n    print(\"{}: {}% missing values\".format(feature,np.round(df_test[feature].isnull().mean(),3)))","67440bbf":"## Replace missing value with a new label \ndef replace_cat_feature(dataset,features_nan):\n    df_test[features_nan]=df_test[features_nan].fillna('Missing')\n    return df_train\n\ndataset=replace_cat_feature(df_test,features_nan)\n\ndataset[features_nan].isnull().sum()\ndf_test.head()","71583a6c":"## Numerical variables \n\n## Now lets check for numerical variables the contains missing values\nnumerical_with_nan=[feature for feature in df_test.columns if df_test[feature].isnull().sum() and df_test[feature].dtypes!='O']\n\n## We will print the numerical nan variables and percentage of missing values\n\nfor feature in numerical_with_nan:\n    print(\"{}: {}% missing value\".format(feature,np.around(df_test[feature].isnull().mean(),3)))","687b4063":"## Replacing the numerical Missing Values\n\nfor feature in numerical_with_nan:\n    ## We will replace by using mean \n    mean_value = df_test[feature].mean()\n    \n    ## create a new feature to capture nan values\n    df_test[feature+'nan']=np.where(df_test[feature].isnull(),1,0)\n    df_test[feature].fillna(mean_value,inplace=True)\n    \ndf_test[numerical_with_nan].isnull().sum()\ndf_test.head()","3b60467e":"# Categorical features --> Encoding \nfrom sklearn.preprocessing import LabelEncoder\n\ncols = ('MSZoning','Street','Alley','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood','Condition1','Condition2',\n        'BldgType','HouseStyle','RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','ExterQual','ExterCond','Foundation','BsmtQual',\n        'BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','Heating','HeatingQC','CentralAir','Electrical','KitchenQual','Functional',\n        'FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive','PoolQC','Fence','MiscFeature','SaleType','SaleCondition')\n\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(df_test[c].values)) \n    df_test[c] = lbl.transform(list(df_test[c].values))\n\ndf_test.head()","571219a3":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Load models\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import SVR\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom xgboost import XGBRegressor","5671e94a":"data = df_train.select_dtypes(include=[np.number]).interpolate().dropna()\ntest = df_test.select_dtypes(include=[np.number]).interpolate().dropna()","4c736b3d":"X = data.drop(['SalePrice'], axis=1)\ny = np.log(df_train.SalePrice)","1f3347f4":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","75077c38":"# Linear Regression\nfrom sklearn import linear_model\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n# Create linear regression object\nregr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_pred = regr.predict(X_test)\n\nprint('MSE is:', mean_squared_error(y_test, y_pred))\nrmse_lin_reg = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"RMSE for Linear Regression is:\", rmse_lin_reg)\nprint('The accuracy of the Linear Regression is',r2_score(y_test,y_pred))","c40ef7f4":"# Ridge \n\nridge = Ridge()\nparam_grid = {'alpha': [0.001, 0.01, 0.03, 0.05, 0.09, 0.7, 0.9, 5, 10, 20, 100]}\ngrid_search_ridge = GridSearchCV(ridge, param_grid, cv = 5)\ngrid_search_ridge.fit(X_train, y_train)\ny_pred = grid_search_ridge.predict(X_test)\n\nprint ('MSE is:', mean_squared_error(y_test, y_pred))\n\nrmse_ridge_reg = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"RMSE for Ridge Regression is:\", rmse_ridge_reg)\nprint('The accuracy of the Ridge Regression is',r2_score(y_test,y_pred))","c7a20bd6":"# Initial trial and error of GridSearch \nfrom sklearn.model_selection import train_test_split,GridSearchCV,KFold,cross_val_score\nfolds  = KFold(n_splits=10,shuffle=True,random_state=42)\n\nhyper_param = {'alpha':[0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n\nmodel = Ridge()\n\nmodel_cv = GridSearchCV(estimator=model,\n                        param_grid=hyper_param,\n                        scoring='r2',\n                        cv=folds,\n                        verbose=1,\n                        return_train_score=True)\n\nmodel_cv.fit(X_train,y_train)","caa19030":"cv_result_r = pd.DataFrame(model_cv.cv_results_)\ncv_result_r['param_alpha'] = cv_result_r['param_alpha'].astype('float32')\ncv_result_r.head()","4f44d0ef":"plt.figure(figsize=(16,8))\nplt.plot(cv_result_r['param_alpha'],cv_result_r['mean_train_score'])\nplt.plot(cv_result_r['param_alpha'],cv_result_r['mean_test_score'])\nplt.xlabel('Alpha')\n# plt.xscale('log')\nplt.ylabel('R2 Score')\nplt.show()","69a403f2":"grid_search_ridge.best_estimator_","2c20e94b":"# Lasso\n\nlasso = Lasso()\n# param_grid = {'alpha': [0.001, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 1, 2, 3, 5, 8, 10, 20, 50]}\nparam_grid = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n                        4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000]}\ngrid_search_las = GridSearchCV(lasso, param_grid, cv = 5)\ngrid_search_las.fit(X_train, y_train)\ny_pred = grid_search_las.predict(X_test)\n\nprint('MSE is:', mean_squared_error(y_test, y_pred))\n\nrmse_las_reg = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"The Root Mean Squared Error for Lasso Regression is:\", rmse_las_reg)\nprint('The accuracy of the Lasso Regression is',r2_score(y_test,y_pred))","d03373ee":"# Initial check \n\nfolds = KFold(n_splits=10,shuffle=True,random_state=42)\n\nhyper_param = {'alpha':[0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n\nmodel = Lasso()\n\nmodel_cv = GridSearchCV(estimator = model,\n                        param_grid=hyper_param,\n                        scoring='r2',\n                        cv=folds,\n                        verbose=1,\n                        return_train_score=True\n                       )\n\nmodel_cv.fit(X_train,y_train)","7b6f03dd":"cv_result_l = pd.DataFrame(model_cv.cv_results_)\ncv_result_l['param_alpha'] = cv_result_l['param_alpha'].astype('float32')\ncv_result_l.head()","3190c30b":"plt.figure(figsize=(16,8))\nplt.plot(cv_result_l['param_alpha'],cv_result_l['mean_train_score'])\nplt.plot(cv_result_l['param_alpha'],cv_result_l['mean_test_score'])\nplt.xscale('log')\nplt.ylabel('R2 Score')\nplt.xlabel('Alpha')\nplt.show()","1e916ea6":"grid_search_las.best_estimator_","e5f3f382":"# SVR \n\nsvr = SVR()\nparam_grid = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'gamma': ['scale', 'auto'],\n              'C': [1, 5, 10], 'epsilon': [0.1, 1, 10]}\ngrid_search_svr = GridSearchCV(svr, param_grid, cv = 5)\ngrid_search_svr.fit(X_train, y_train)\ny_pred = grid_search_svr.predict(X_test)\n\nprint('MSE is:', mean_squared_error(y_test, y_pred))\n\nrmse_svr_reg = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"RMSE of SVR is:\", rmse_svr_reg)\nprint('The accuracy of the Support Vector Regression is',r2_score(y_test,y_pred))","7c35acf0":"grid_search_svr.best_estimator_","dcac030a":"# Decision Tree Regressor \n\ndtr = DecisionTreeRegressor(random_state = 0)\nparam_grid = {'max_depth': list(range(2, 10)),\n              'splitter': ['best', 'random'],\n              'min_samples_leaf': list(range(1, 10)),\n              'max_leaf_nodes': list(range(5, 20))}\ngrid_search_dtr = GridSearchCV(dtr, param_grid, cv = 5)\ngrid_search_dtr.fit(X_train, y_train)\ny_pred = grid_search_dtr.predict(X_test)\n\n\nprint('MSE is:', mean_squared_error(y_test, y_pred))\nrmse_dtr_reg = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"RMSE of DT is:\", rmse_dtr_reg)\nprint('The accuracy of the Decision Tree Regressor is',r2_score(y_test,y_pred))\nprint(grid_search_dtr.best_params_)","83768ec8":"# Random Forest Regressor \n\nrfr = RandomForestRegressor()\nparam_grid = {'n_estimators': list(range(100, 200, 10)),\n             'max_depth': list(range(4, 7)),\n             'min_samples_split': list(range(2, 4))}\ngrid_search_rfr = GridSearchCV(rfr, param_grid, cv = 5)\ngrid_search_rfr.fit(X_train, y_train)\ny_pred = grid_search_rfr.predict(X_test)\n\nprint('MSE is:', mean_squared_error(y_test, y_pred))\n\nrmse_rfr_reg = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"RMSE of RF is:\", rmse_rfr_reg)\nprint('The accuracy of Random Forest Regression is',r2_score(y_test,y_pred))","ccaa8de5":"# XGBoost Regressor  \n\nxgb = XGBRegressor()\nparam_grid = {'n_estimators': list(range(500, 1000, 100)),\n             'learning_rate': [0.001, 0.01, 0.1]}\ngrid_search_xgb = GridSearchCV(xgb, param_grid, cv = 5)\ngrid_search_xgb.fit(X_train, y_train)\ny_pred = grid_search_xgb.predict(X_test)\n\nprint('MSE is:', mean_squared_error(y_test, y_pred))\n\nrmse_xgb_reg = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"RMSE of XGB is:\", rmse_xgb_reg)\nprint('The accuracy of the XGB is',r2_score(y_test,y_pred))","d8bedfd6":"# Adaboost Regressor \n\nabr = AdaBoostRegressor(random_state = 0)\nparam_grid = {'n_estimators': list(range(100, 1000, 100)),\n             'learning_rate': [0.001, 0.01, 0.1, 1, 10]}\ngrid_search_abr = GridSearchCV(abr, param_grid, cv = 5)\ngrid_search_abr.fit(X_train, y_train)\ny_pred = grid_search_abr.predict(X_test)\n\nprint('MSE is:', mean_squared_error(y_test, y_pred))\n\nrmse_abr_reg = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"RMSE of SVR is:\", rmse_abr_reg)\nprint('The accuracy of Adaboost Regressor is',r2_score(y_test,y_pred))","23227b75":"# Comparison of models \n\ndata = {'Linear Regression': rmse_lin_reg, 'Ridge Regression': rmse_ridge_reg, 'Lasso Regression': rmse_las_reg,\n        'Support Vector Regressor': rmse_svr_reg, 'Decision Tree Regressor': rmse_dtr_reg, 'Random Forest Regressor': rmse_rfr_reg,\n        'XGBoost Regressor': rmse_xgb_reg, 'Ada Boost Regressor': rmse_abr_reg}\ndata = dict(sorted(data.items(), key = lambda x: x[1], reverse = True))\nmodels = list(data.keys())\nRMSE = list(data.values())\nfig = plt.figure(figsize = (30, 10))\nsns.barplot(x = models, y = RMSE)\nplt.xlabel(\"Models Used\", size = 20)\nplt.xticks(rotation = 30, size = 15)\nplt.ylabel(\"RMSE\", size = 20)\nplt.yticks(size = 15)\nplt.title(\"RMSE for different models\", size = 25)\nplt.show()","916cccad":"# model = Ridge(alpha = 0.9)\nmodel = SVR(gamma = 'auto', C=10)\nmodel.fit(X_train,y_train)\n\ny_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\nprint(r2_score(y_true=y_train,y_pred=y_train_pred))\nprint(r2_score(y_true=y_test,y_pred=y_test_pred))","e34d83fe":"# Predictions --> For predicting saleprice from most correlated features \ntrain = df_train\ntest = df_test\n\n# Most correlated factors that will predict the price\ndesired_factors = ['OverallQual','GrLivArea','GarageCars','TotalBsmtSF','FullBath','YearBuilt']\n\n# Setting model to SVR \n\nmodel = SVR(gamma = 'auto',C = 10)\n\n# Set prediction data to factors that will predict, and set target to SalePrice\ntrain_data = train[desired_factors]\ntest_data = test[desired_factors]\ntarget = train.SalePrice\n\n# Fitting model with prediction data and telling it my target\nmodel.fit(train_data, target)\nmodel.predict(test_data.head())\n\na = pd.DataFrame({'Id': test['Id'], 'OverallQual':test['OverallQual'],'GrLivArea':test['GrLivArea'],\n                  'GarageCars':test['GarageCars'],'TotalBsmtSF':test['TotalBsmtSF'],'FullBath':test['FullBath'],\n                  'YearBuilt':test['YearBuilt'], 'SalePrice':target})\na.head()","1581f860":"# Submission \n\nsubmission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nfinal_submission = pd.DataFrame({'Id':submission['Id'], 'SalePrice':target})\nfinal_submission.to_csv('submission.csv', index = False)","f11df952":"## Final model --> Either Ridge or SVR (Based on the above results) ","56c97f86":"## Categorical Variables ","9efcbb24":"## Feature Scaling ","7aa64a42":"## Normalising variables ","4a3ec3fe":"We have four variables containing year info","931f168d":"## EDA ","80ff768f":"## Year Variables ","aac7d0c2":"## Performing some operations in test dataset for prediction ","b9c84f0f":"## Feature Engineering \n\n1. Handling missing values\n2. Handling year variables, numerical, categorical variables\n3. Handling outliers\n4. Feature Scaling ","3dd7220a":"## Numerical Variables ","3207d679":"## Modelling\n\n- Linear Regression \n- Ridge Regression \n- Lasso Regression \n- Support Vector Regression \n- Decision Tree\n- Random Forest \n- Adaboost \n- XGBoost"}}