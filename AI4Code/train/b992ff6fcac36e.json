{"cell_type":{"6538ce7c":"code","9520d8ec":"code","484fcf9a":"code","e2346106":"code","869e49e1":"code","c4d68d1a":"code","053cb2a6":"code","13b5fbc2":"code","6b78ea6d":"code","ff9d837b":"code","4083211a":"code","86361eaf":"code","06cf9ebc":"code","6ac0faf8":"code","94ca05fc":"code","0d9d43a5":"markdown","5c3ab0e7":"markdown","3e42e823":"markdown","715b99a9":"markdown","cb6d0b47":"markdown","4518aa39":"markdown","deb061dc":"markdown","d0c2f6bf":"markdown","41c90d2f":"markdown","4367463c":"markdown"},"source":{"6538ce7c":"import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/test.csv')\n\ntrain_y = train['target']\ntrain_data = train.drop(columns=['id', 'target'])\ntest_data = test.drop(columns=['id'])\n\ntrain_rows_len = train_data.shape[0]\n\ntrain_test_data = pd.concat([train_data, test_data])\n\nprint(train_data.shape)\nprint(test_data.shape)\nprint(train_test_data.shape)","9520d8ec":"import matplotlib.pyplot as plt\n\nplt.hist(train_y)\nplt.title(\"Target Histogram\")\nplt.show()","484fcf9a":"data_categorical_pd  = train_test_data.loc[:,train_test_data.dtypes==np.object]\ndata_numerical_pd  = train_test_data.loc[:,train_test_data.dtypes!=np.object]\nprint('data_categorical_pd.shape: ', data_categorical_pd.shape)\nprint('data_numerical_pd.shape: ', data_numerical_pd.shape)","e2346106":"categorical_missing_val_count = (data_categorical_pd.isnull().sum())\nnumerical_missing_val_count = (data_numerical_pd.isnull().sum())\n\nprint('categorical_missing_val_count')\nprint(categorical_missing_val_count[categorical_missing_val_count > 0])\nprint('numerical_missing_val_count')\nprint(numerical_missing_val_count[numerical_missing_val_count > 0])","869e49e1":"import seaborn as sns\nimport warnings\n\nwarnings.simplefilter(\"ignore\")\n\n# Numerical\ny_plot = train_y.copy()\ny_plot.columns = ['target']\nData_plot = pd.concat([data_numerical_pd[:][:len(y_plot)], y_plot], axis=1)\n\nfor feature in data_numerical_pd.columns:\n    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n    \n    plot10 = sns.distplot(Data_plot[Data_plot['target']==0][feature],ax=ax1, label='0')\n    sns.distplot(Data_plot[Data_plot['target']==1][feature],ax=ax1,color='red', label='1')\n    plot10.axes.legend()\n    ax1.set_title('Distribution of {name}'.format(name=feature))\n\n    sns.boxplot(x='target',y=feature,data=Data_plot,ax=ax2)\n    ax2.set_xlabel('Category') \n    ax2.set_title('Boxplot of {name}'.format(name=feature))\n\n    fig.show()","c4d68d1a":"data_plot = pd.concat([data_categorical_pd[:][:len(y_plot)], y_plot], axis=1)\nfeature_list = []\nfor col in data_categorical_pd.columns:\n    if len(data_categorical_pd[col].unique()) <= 20:\n        feature_list.append(col)\n\nn_cols = 3\nnrows = round(len(feature_list) \/ n_cols)\nfig, axes = plt.subplots(nrows, n_cols, figsize=(24, 12))\nplt.subplots_adjust(hspace=0.5)\n\nindex = 0\nfor row in range(nrows):\n    for col in range(n_cols):\n        feature = feature_list[index]\n        \n        sns.barplot(x=feature, y='target', data=data_plot, ax=axes[row][col])\n        axes[row][col].set_title(feature + ' Distribution', color = 'red')\n        \n        index += 1\nplt.show()","053cb2a6":"from sklearn.model_selection import train_test_split\n\nobject_cols = data_categorical_pd.columns\nX_train, X_valid, y_train, y_valid = train_test_split(data_categorical_pd[:][:len(train_y)], train_y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)\n\n# Columns that can be safely label encoded\ngood_label_cols = [col for col in object_cols if \n                   set(X_train[col]) == set(X_valid[col])]\n        \n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n\nprint('good_label_cols: ', len(good_label_cols))\nprint(good_label_cols)\nprint('bad_label_cols: ', len(bad_label_cols))\nprint(bad_label_cols)","13b5fbc2":"th = 100\n\ncat10_counts = data_categorical_pd['cat10'].value_counts()\ncat10_cut_values = cat10_counts[cat10_counts > th].index\nprint('cat10_cut_values: ', len(cat10_cut_values))\n\ncat10_cut_values_list = []\nfor value in data_categorical_pd['cat10']:\n    if value in cat10_cut_values:\n        cat10_cut_values_list.append(value)\n    else:\n        cat10_cut_values_list.append('others')\n\ndata_categorical_temp_pd = data_categorical_pd.copy()\ndata_categorical_temp_pd['cat10'] = cat10_cut_values_list\n\nobject_cols = data_categorical_temp_pd.columns\nX_train, X_valid, y_train, y_valid = train_test_split(data_categorical_temp_pd[:][:len(train_y)], train_y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)\n\nprint('cat10: ', set(X_train['cat10']) == set(X_valid['cat10']))","6b78ea6d":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\ndata_categorical_encoded_pd = data_categorical_temp_pd.copy()\nfor feature in data_categorical_encoded_pd.columns:\n        le = LabelEncoder()\n        data_categorical_encoded_pd[feature] = le.fit_transform(data_categorical_temp_pd[feature].astype(str))\n        \nprint(data_categorical_encoded_pd.info())\nprint(data_numerical_pd.info())","ff9d837b":"for feature in data_categorical_encoded_pd.columns:\n    value = len(data_categorical_encoded_pd[feature].unique())\n    print(feature, value)\n\nsame_len_19 = ['cat9', 'cat2']\nsame_len_4 = ['cat15', 'cat16', 'cat17', 'cat18']\nsame_len_2 = ['cat0', 'cat11', 'cat12', 'cat13', 'cat14']","4083211a":"data_categorical_FeaEng_pd = data_categorical_encoded_pd.copy()\ndata_numerical_FeaEng_pd = data_numerical_pd.copy()\n\ndata_categorical_FeaEng_pd['cat9cat2T'] = data_categorical_encoded_pd['cat9'] + data_categorical_encoded_pd['cat2']\ndata_categorical_FeaEng_pd['cat9cat2M'] = data_categorical_encoded_pd['cat9'] * data_categorical_encoded_pd['cat2']\n\ndata_categorical_FeaEng_pd['cat15cat16T'] = data_categorical_encoded_pd['cat15'] + data_categorical_encoded_pd['cat16']\ndata_categorical_FeaEng_pd['cat15cat17T'] = data_categorical_encoded_pd['cat15'] + data_categorical_encoded_pd['cat17']\ndata_categorical_FeaEng_pd['cat15cat18T'] = data_categorical_encoded_pd['cat15'] + data_categorical_encoded_pd['cat18']\ndata_categorical_FeaEng_pd['cat16cat17T'] = data_categorical_encoded_pd['cat16'] + data_categorical_encoded_pd['cat17']\ndata_categorical_FeaEng_pd['cat16cat18T'] = data_categorical_encoded_pd['cat16'] + data_categorical_encoded_pd['cat18']\ndata_categorical_FeaEng_pd['cat17cat18T'] = data_categorical_encoded_pd['cat17'] + data_categorical_encoded_pd['cat18']\ndata_categorical_FeaEng_pd['cat15cat16M'] = data_categorical_encoded_pd['cat15'] * data_categorical_encoded_pd['cat16']\ndata_categorical_FeaEng_pd['cat15cat17M'] = data_categorical_encoded_pd['cat15'] * data_categorical_encoded_pd['cat17']\ndata_categorical_FeaEng_pd['cat15cat18M'] = data_categorical_encoded_pd['cat15'] * data_categorical_encoded_pd['cat18']\ndata_categorical_FeaEng_pd['cat16cat17M'] = data_categorical_encoded_pd['cat16'] * data_categorical_encoded_pd['cat17']\ndata_categorical_FeaEng_pd['cat16cat18M'] = data_categorical_encoded_pd['cat16'] * data_categorical_encoded_pd['cat18']\ndata_categorical_FeaEng_pd['cat17cat18M'] = data_categorical_encoded_pd['cat17'] * data_categorical_encoded_pd['cat18']\n\ndata_categorical_FeaEng_pd['cat0cat11T'] = data_categorical_encoded_pd['cat0'] + data_categorical_encoded_pd['cat11']\ndata_categorical_FeaEng_pd['cat0cat12T'] = data_categorical_encoded_pd['cat0'] + data_categorical_encoded_pd['cat12']\ndata_categorical_FeaEng_pd['cat0cat13T'] = data_categorical_encoded_pd['cat0'] + data_categorical_encoded_pd['cat13']\ndata_categorical_FeaEng_pd['cat0cat14T'] = data_categorical_encoded_pd['cat0'] + data_categorical_encoded_pd['cat14']\ndata_categorical_FeaEng_pd['cat11cat12T'] = data_categorical_encoded_pd['cat11'] + data_categorical_encoded_pd['cat12']\ndata_categorical_FeaEng_pd['cat11cat13T'] = data_categorical_encoded_pd['cat11'] + data_categorical_encoded_pd['cat13']\ndata_categorical_FeaEng_pd['cat11cat14T'] = data_categorical_encoded_pd['cat11'] + data_categorical_encoded_pd['cat14']\ndata_categorical_FeaEng_pd['cat12cat13T'] = data_categorical_encoded_pd['cat12'] + data_categorical_encoded_pd['cat13']\ndata_categorical_FeaEng_pd['cat12cat14T'] = data_categorical_encoded_pd['cat12'] + data_categorical_encoded_pd['cat14']\ndata_categorical_FeaEng_pd['cat13cat14T'] = data_categorical_encoded_pd['cat13'] + data_categorical_encoded_pd['cat14']\ndata_categorical_FeaEng_pd['cat0cat11M'] = data_categorical_encoded_pd['cat0'] * data_categorical_encoded_pd['cat11']\ndata_categorical_FeaEng_pd['cat0cat12M'] = data_categorical_encoded_pd['cat0'] * data_categorical_encoded_pd['cat12']\ndata_categorical_FeaEng_pd['cat0cat13M'] = data_categorical_encoded_pd['cat0'] * data_categorical_encoded_pd['cat13']\ndata_categorical_FeaEng_pd['cat0cat14M'] = data_categorical_encoded_pd['cat0'] * data_categorical_encoded_pd['cat14']\ndata_categorical_FeaEng_pd['cat11cat12M'] = data_categorical_encoded_pd['cat11'] * data_categorical_encoded_pd['cat12']\ndata_categorical_FeaEng_pd['cat11cat13M'] = data_categorical_encoded_pd['cat11'] * data_categorical_encoded_pd['cat13']\ndata_categorical_FeaEng_pd['cat11cat14M'] = data_categorical_encoded_pd['cat11'] * data_categorical_encoded_pd['cat14']\ndata_categorical_FeaEng_pd['cat12cat13M'] = data_categorical_encoded_pd['cat12'] * data_categorical_encoded_pd['cat13']\ndata_categorical_FeaEng_pd['cat12cat14M'] = data_categorical_encoded_pd['cat12'] * data_categorical_encoded_pd['cat14']\ndata_categorical_FeaEng_pd['cat13cat14M'] = data_categorical_encoded_pd['cat13'] * data_categorical_encoded_pd['cat14']","86361eaf":"data_categorical_FeaEng_pd['cat15_16TotalTotal'] = (data_categorical_FeaEng_pd['cat15cat16T'] + data_categorical_FeaEng_pd['cat15cat17T'] +\n                                                    data_categorical_FeaEng_pd['cat15cat18T'] + data_categorical_FeaEng_pd['cat16cat17T'] + \n                                                    data_categorical_FeaEng_pd['cat16cat18T'] + data_categorical_FeaEng_pd['cat17cat18T'])\ndata_categorical_FeaEng_pd['cat15_16MulTotal'] = (data_categorical_FeaEng_pd['cat15cat16M'] + data_categorical_FeaEng_pd['cat15cat17M'] +\n                                                    data_categorical_FeaEng_pd['cat15cat18M'] + data_categorical_FeaEng_pd['cat16cat17M'] + \n                                                    data_categorical_FeaEng_pd['cat16cat18M'] + data_categorical_FeaEng_pd['cat17cat18M'])\n\ndata_categorical_FeaEng_pd['cat11_14TotalTotal'] = (data_categorical_FeaEng_pd['cat0cat11T'] + data_categorical_FeaEng_pd['cat0cat12T'] +\n                                                   data_categorical_FeaEng_pd['cat0cat13T'] + data_categorical_FeaEng_pd['cat0cat14T'] +\n                                                   data_categorical_FeaEng_pd['cat11cat12T'] + data_categorical_FeaEng_pd['cat11cat13T'] +\n                                                   data_categorical_FeaEng_pd['cat11cat14T'] + data_categorical_FeaEng_pd['cat12cat13T'] +\n                                                   data_categorical_FeaEng_pd['cat12cat14T'] + data_categorical_FeaEng_pd['cat13cat14T'])\ndata_categorical_FeaEng_pd['cat11_14MulTotal'] = (data_categorical_FeaEng_pd['cat0cat11M'] + data_categorical_FeaEng_pd['cat0cat12M'] +\n                                                   data_categorical_FeaEng_pd['cat0cat13M'] + data_categorical_FeaEng_pd['cat0cat14M'] +\n                                                   data_categorical_FeaEng_pd['cat11cat12M'] + data_categorical_FeaEng_pd['cat11cat13M'] +\n                                                   data_categorical_FeaEng_pd['cat11cat14M'] + data_categorical_FeaEng_pd['cat12cat13M'] +\n                                                   data_categorical_FeaEng_pd['cat12cat14M'] + data_categorical_FeaEng_pd['cat13cat14M'])","06cf9ebc":"from sklearn import preprocessing\n\ndata_numerical_norm_pd = data_numerical_FeaEng_pd.copy()\ndata_categorical_norm_pd = data_categorical_FeaEng_pd.copy()\n\nfor index, feature in enumerate(data_numerical_FeaEng_pd.columns):\n    min_max_scaler = preprocessing.MinMaxScaler()\n    \n    norm_list = min_max_scaler.fit_transform(\n        data_numerical_FeaEng_pd[data_numerical_FeaEng_pd.columns[index:index+1]])\n    \n    data_numerical_norm_pd[feature] = norm_list + 1","6ac0faf8":"from scipy.stats import skew, boxcox\nimport seaborn as sns\n\ndata_numerical_TR_pd = data_numerical_norm_pd.copy()\ndata_categorical_TR_pd = data_categorical_norm_pd.copy()\n\nskew_feature_list = []\nfor feature in data_numerical_norm_pd.columns:\n    skew_value = skew(data_numerical_norm_pd[feature])\n    if abs(skew_value) > 0.3:\n        _, fitted_lambda = boxcox(data_numerical_norm_pd[feature])\n        data_numerical_TR_pd[feature] = data_numerical_norm_pd[feature] ** fitted_lambda\n        skew_feature_list.append(feature)\n        \nn_rows = round(data_numerical_TR_pd.shape[1] \/ 4)\nn_rows = n_rows - 1\nfig, axs = plt.subplots(nrows=n_rows, ncols=4, figsize=(20, 10))\nindex = 0\nfor i in range(n_rows):\n    for j in range(4):\n        try:\n            sns.histplot(data_numerical_norm_pd[skew_feature_list[index]], kde = True, color = 'red', stat = 'count', ax=axs[i][j])\n            axs[i][j].title.set_text(skew_feature_list[index])\n        except:\n            break\n        index = index + 1\nplt.show()","94ca05fc":"X_train = pd.concat([data_numerical_TR_pd[:][:train_rows_len], data_categorical_TR_pd[:][:train_rows_len]], axis=1)\nX_test = pd.concat([data_numerical_TR_pd[:][train_rows_len:], data_categorical_TR_pd[:][train_rows_len:]], axis=1)\ny_train = train_y.copy()\n\nX_train.to_csv('x_train.csv',index=False)\nX_test.to_csv('x_test.csv',index=False)\ny_train.to_csv('y_train.csv',index=False)\n\nprint('X_train shape: ', X_train.shape)\nprint('X_test shape: ', X_test.shape)\nprint('y_train shape: ', y_train.shape)\nprint(X_train.head())","0d9d43a5":"# Prepare Train_Test Data","5c3ab0e7":"# Data cleaning - Dealing with outliers","3e42e823":"# Feature Engineering","715b99a9":"# Data cleaning - Label encoding, Drop Constant","cb6d0b47":"# Plot Target Data","4518aa39":"# Data cleaning - Dealing with null values","deb061dc":"# Feature Transformation","d0c2f6bf":"# Introduction\n\nHey, thanks for viewing my Kernel!\n\nIf you like my work, please, leave an upvote: it will be really appreciated and it will motivate me in offering more content to the Kaggle community ! \ud83d\ude0a\n\nThe objective of this notebook is to prepare the data for predictions. ","41c90d2f":"# Preparation Data For Training","4367463c":"# Split Numerical and Categorical"}}