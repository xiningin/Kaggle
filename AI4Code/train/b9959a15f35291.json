{"cell_type":{"600c6db5":"code","49982827":"code","2d357422":"code","96719310":"code","549b0e84":"code","072ddf91":"code","17084bef":"code","4b23e87d":"code","93baf57d":"code","40cd744a":"code","6275a5f9":"code","b08299b8":"code","99b77606":"code","d3a1a441":"code","7f77333c":"code","8b236c3c":"code","173d66d4":"code","6b63b133":"code","192ec9f2":"code","631fa8d6":"code","a0857e6c":"code","44935d41":"code","95696460":"code","38bb0e91":"code","40276841":"code","88d6e7b9":"code","5a4772a1":"code","87882a0a":"code","640f5259":"code","6ae99001":"code","d3fcb3e8":"code","034cd39b":"code","5b9518e1":"code","a3bc9e9f":"code","b3b37ef3":"code","82ec7e95":"code","831bc5ad":"code","74ce2001":"code","947da1c4":"markdown","6500d8ab":"markdown","395faeb1":"markdown","54770db8":"markdown","975af20a":"markdown","5e660f19":"markdown","d6e29662":"markdown","4042ed42":"markdown","00490717":"markdown","7ea707b0":"markdown","5730c0ee":"markdown","593fb61b":"markdown","c1d954dd":"markdown","ebf9b836":"markdown","fe001af3":"markdown","deeb45b2":"markdown","7608d5d3":"markdown","a1fda634":"markdown"},"source":{"600c6db5":"# Importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LassoCV, RidgeCV, ElasticNetCV\nfrom scipy import stats\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n%matplotlib inline","49982827":"# Load in dataset\ntrain = pd.read_csv('\/Users\/afsaja\/Desktop\/dsi3\/dsi3_projects\/project_2\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/Users\/afsaja\/Desktop\/dsi3\/dsi3_projects\/project_2\/house-prices-advanced-regression-techniques\/test.csv')","2d357422":"# Exploring the dataset\ntrain.head()","96719310":"test.head()","549b0e84":"train.shape, test.shape","072ddf91":"train.info(), test.info()","17084bef":"# Removing the 'Id' column from train and test dataset as it provides no additional value and savng it separately\ntrain_id = train[['Id']]\ntest_id = test[['Id']]\n\n#Dropping 'Id' column\ntrain.drop('Id', axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)","4b23e87d":"train.shape, test.shape","93baf57d":"sns.scatterplot(train['GrLivArea'], train['SalePrice']);","40cd744a":"# Seeing that we have two GrLivArea observations that are significantly higher than the average with low SalePrices, \n# we decide to remove them\ntrain.drop(train[(train['GrLivArea'] > 4000) & (train['SalePrice'] < 300000)].index, inplace=True)\ntrain.shape","6275a5f9":"# We realize a better scatter plot of SalePrice and GrLivArea after removing outliers \nsns.scatterplot(train['GrLivArea'], train['SalePrice']);","b08299b8":"# Below dist plot shows skewness in the target variable making it a candidate for log transformation\nsns.distplot(train['SalePrice'], kde=True, bins=20);","99b77606":"# Further investigating skewness (measures symmetry) and kurtosis (fat tails) shows that SalePrice is NOT normal\ntrain['SalePrice'].skew(), train['SalePrice'].kurt()","d3a1a441":"# log transforming our target variable and checking resulting histogram\ntrain['SalePrice'] = np.log(train['SalePrice'])\nsns.distplot(train['SalePrice'], bins=20, kde=True);","7f77333c":"# Going through the description on Kaggle, many of the NaN values in the features should be filled with 'None' or \n# ZERO with some to be categorized. Below, we go through each of them feature by feature\n\n# First, we combine the train and test sets to ensure that both sets are taken treated the same with \n# similar columns dropped (if any)\ncombined = pd.concat((train, test)).reset_index(drop=True)\ncombined.shape","8b236c3c":"# Setting y_train and removing 'SalePrice' from combined dataset so as not to lose 'SalePrice' column\ny_train = train['SalePrice']\ncombined.drop('SalePrice', axis=1, inplace=True)\ncombined.shape, y_train.shape","173d66d4":"# Now we go through each feature and drop or fill in NaNs based on documentation. We generate corr on train data given\n# that it still has the SalePrice column\ncorrmat = train.corr()\nfig, ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(corrmat, vmax=.8, square=True, cmap=\"PiYG\", annot=True, fmt='.1f');","6b63b133":"# PoolQC : data description says NA means \"No Pool\"\ncombined['PoolQC'].fillna('None', inplace=True)\n\n# MiscFeature: data description says NA means \"No misc value\"\ncombined['MiscFeature'].fillna('None', inplace=True)\n\n# Alley : data description says NA means \"No alley access\"\ncombined['Alley'].fillna('None', inplace=True)\n\n# Fence : data description says NA means \"No Fence\"\ncombined['Fence'].fillna('None', inplace=True)\n\n# FireplaceQu : data description says NA means \"No Fireplace\"\ncombined['FireplaceQu'].fillna('None', inplace=True)\n\n# we ran .describe() on LotFrontage and foudn the mean = median, we, hence, proceed to filling \n# NA values for LotFrontage with mean of column\ncombined.LotFrontage.fillna(value=combined['LotFrontage'].mean(), inplace=True)\n\n# Garage-cat : replacing NAs \"None\"\ngarage_list_cat = ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']\nfor col in garage_list_cat:\n    combined[col] = combined[col].fillna('None')\n\n# Garage_num : replacing NAs with 0s\ngarage_list_num = ['GarageArea', 'GarageCars', 'GarageYrBlt']\nfor col in garage_list_num:\n    combined[col] = combined[col].fillna(0)\n\n# Basement_num : Replacing basement numericals with 0s\nbasement_list_num = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']\nfor col in basement_list_num:\n    combined[col] = combined[col].fillna(0)\n\n# Basement_cat : Replacing basement categories with \"None\"\nbasement_list_num = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\nfor col in basement_list_num:\n    combined[col] = combined[col].fillna('None')\n    \n# Veneer data: replacing with 0s and 'None'\ncombined['MasVnrArea'].fillna(0, inplace=True)\ncombined['MasVnrType'].fillna('None', inplace=True)\n\n# Zoning: Replacing with most common value 'RL'\ncombined['MSZoning'].fillna('RL', inplace=True)\n\n# Utilities: Drop it since it has one type of observation and thus not helpful\ncombined.drop('Utilities', axis=1 ,inplace=True)\n\n# Functional: Description says NA = 'Typ'\ncombined['Functional'].fillna('Typ', inplace=True)\n\n# Electrical: Set missing value to 'SBrkr'\ncombined['Electrical'].fillna('SBrkr', inplace=True)\n\n# KitchenQual: Only one NA value, and same as Electrical, we set 'TA'\ncombined['KitchenQual'].fillna('TA', inplace=True)\n\n# Exterior1st and Exterior2nd : We will just substitute in the most common string\ncombined['Exterior1st'].fillna(combined['Exterior1st'].mode()[0], inplace=True)\ncombined['Exterior2nd'].fillna(combined['Exterior2nd'].mode()[0], inplace=True)\n\n# SaleType : Fill in again with most frequent which is \"WD\"\ncombined['SaleType'].fillna(combined['SaleType'].mode()[0], inplace=True)\n\n# SaleType : Fill in again with most frequent which is \"WD\"\ncombined['MSSubClass'].fillna('None', inplace=True)","192ec9f2":"# MSSubClass=The building class\ncombined['MSSubClass'] = combined['MSSubClass'].apply(str)\n\n\n# Changing OverallCond into a categorical variable\ncombined['OverallCond'] = combined['OverallCond'].astype(str)\n\n\n# Year and month sold are transformed into categorical features.\ncombined['YrSold'] = combined['YrSold'].astype(str)\ncombined['MoSold'] = combined['MoSold'].astype(str)","631fa8d6":"# Dropping part of total columns\n# Dropping basement and duplicated columns\ncolumns_kept_train = [c for c in combined.columns if c not in ['BsmtFinSF1', 'BsmtFinSF2','BsmtUnfSF','GarageCars']]\ncombined = combined[columns_kept_train]\n\n# Dropping GrLivArea parts\ncolumns_kept_train = [c for c in combined.columns if c not in ['1stFlrSF', '2ndFlrSF','LowQualFinSF',]]\ncombined = combined[columns_kept_train]","a0857e6c":"combined.shape","44935d41":"subset = combined.copy()\nskew_features = subset.skew(axis=0).sort_values(ascending=False)\nfig, ax = plt.subplots(figsize=(10, 10))\nplt.xticks(rotation='vertical')\nsns.barplot(x=skew_features.index, y=skew_features);","95696460":"# Plotting histograms of highly skewed numerical variables with SalesPrice to assess underlying distributions\nfig, ax = plt.subplots(ncols=2, nrows=2, figsize=(10, 10))\nsns.distplot(combined.MiscVal, bins=20, kde=True, ax=ax[0,0])\nsns.distplot(combined.PoolArea, bins=20, kde=True, ax=ax[0,1])\nsns.distplot(combined.LotArea, bins=20, kde=True, ax=ax[1,0])\nsns.distplot(combined['3SsnPorch'], bins=20, kde=True, ax=ax[1,1])\nfig.suptitle('Histograms of relevant numeric variables', fontsize=20)\nplt.show()","38bb0e91":"# Making sure no column has NAs\ncombined.isnull().sum()","40276841":"# Plotting correlationmap one more time\ncorrmat_combined = combined.corr()\nfig, ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(corrmat_combined, vmax=.8, square=True, cmap=\"PiYG\", annot=True, fmt='.1f');","88d6e7b9":"# Get dummy variables\nmodel_data = combined.copy()\nmodel_data = pd.get_dummies(model_data)\nmodel_data.shape","5a4772a1":"# Resplitting our data back into test and train\nX_train = model_data[:train.shape[0]]\nX_test = model_data[train.shape[0]:]\nX_test.shape, X_train.shape","87882a0a":"# Scaling the data using StandardScaler\n\nss = StandardScaler()\nX_train_ss = ss.fit_transform(X_train)\nX_test_ss = ss.transform(X_test)","640f5259":"# create an array of alpha values\nalpha_range = 10.**np.arange(-2, 3)\nalpha_range\n\n# select the best alpha with RidgeCV\nridgeregcv = RidgeCV(alphas=alpha_range, cv=5)\nridgeregcv.fit(X_train_ss, y_train)","6ae99001":"# Printing out RidgeCV results\nprint('RidgeCV alpha: ', str(ridgeregcv.alpha_))","d3fcb3e8":"# predict method uses the best alpha value\ny_pred_log = ridgeregcv.predict(X_test_ss)\ny_pred_log","034cd39b":"# Scaling back RidgeCV results to original SalePrice scale\ny_pred = np.expm1(y_pred_log)\ny_pred","5b9518e1":"# printing out CSV file to test\nridgecv_submission = pd.DataFrame({'Id': test_id.Id, 'SalePrice': y_pred})\nridgecv_submission.to_csv('ridgecv_submission.csv', index=False)","a3bc9e9f":"# select the best alpha with LassoCV\nlassoregcv = LassoCV(n_alphas=100, random_state=87, cv=5)\nlassoregcv.fit(X_train, y_train)","b3b37ef3":"# examine the coefficients\nprint('LASSO penalization: ', str(lassoregcv.alpha_))\nprint('LASSO coefs: ', str(lassoregcv.coef_[:5]))\nprint('LASSO alphas: ', str(lassoregcv.alphas_[:5]))\nprint('LASSO MES path: ', str(lassoregcv.mse_path_[:5]))","82ec7e95":"# predict method uses the best alpha value\ny_pred_log_lasso = lassoregcv.predict(X_test)\ny_pred_log_lasso","831bc5ad":"# Scaling back RidgeCV results to original SalePrice scale\ny_pred_lasso = np.expm1(y_pred_log_lasso)\ny_pred_lasso","74ce2001":"# printing out CSV file to test\nlassocv_submission = pd.DataFrame({'Id': test_id.Id, 'SalePrice': y_pred_lasso})\nlassocv_submission.to_csv('lassocv_submission.csv', index=False)","947da1c4":"# Modeling","6500d8ab":"### Dealing with missing values and feature engineering","395faeb1":"# Data exploration","54770db8":"### Removing outliers","975af20a":"We want to check now proceed with dropping variables that are parts of the total (to reduce multicollinearity between variables)","5e660f19":"# Data cleaning","d6e29662":"Instead of graphing a pairplot, we've taken advantage of Kaggle's 'Data' section that has summaries and distributions of each feature","4042ed42":"Last step is getting dummy variables! We proceed with changing all our categoricals to numeric in order to perform linear regression on them using ```pd.get_dummies()```","00490717":"## LASSO regression using LassoCV","7ea707b0":"We proceed to plotting the histograms of the 5 features we identified as highly skewed to have a look at their distributions","5730c0ee":"We now check the skewness of other features in the dataset that we think are important","593fb61b":"The plot above shows that some features are highly skewed. We will not transform all skewed data but focus ONLY on the features that exhibit a skewness > 5. No reason why we chose 5 specifically. This is just arbitrary and seems like a good cut-off based on the bar plot above","c1d954dd":"### Inspecting our target variable","ebf9b836":"Fixing categorical values that have some numerical values","fe001af3":"We have submitted both results to Kaggle and the output of both models was tested:\n- RidgeCV rmse = **0.13569**\n- LassoCV rmse = **0.19058**\n\nIt seems that **RidgeCV performs better than LassoCV** in this case. Why is that? I'm not quite sure.\n\nWe are not done yet! To test if we can get a better score, we **perform additional feature engineer** focused on the following:\n\n- Simplifying some of our categorical variables (ones with large number of categories)\n- Summing up ```GrLivArea``` and ```TotalBsmtSF``` to generate a total living space column and drop their parts accordingly\n- Summing up other features like ```Porch features```\n- Using Cox-Box transformation to transform some of our highly skewed data further\n\nFingers crossed! Let's do it!","deeb45b2":"## Resplitting and Scaling the data (preprocessing)","7608d5d3":"Hmmm... since the above distribution plots have shown that these features exhibit skewness due to the presence of zeros, we will not treat them given that log transformation in order to 'normalize' them would not work.","a1fda634":"## Ridge regression using RidgeCV"}}