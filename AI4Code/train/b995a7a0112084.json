{"cell_type":{"7b16d312":"code","905cd1b7":"code","e897dd6a":"code","89ff37c2":"code","ce2e86e5":"code","377d920c":"code","02b29a39":"code","842e1cc0":"code","794f0c8b":"code","f6f41e28":"code","17c5f9fa":"code","8313e92a":"code","103b6e32":"code","4f67535f":"code","07db61bf":"code","d9714bcf":"code","54edeb62":"code","03434094":"code","ce9e2672":"code","6538e438":"code","7001eef8":"code","40f0783f":"code","2ea37bd1":"code","2ffc8fdc":"code","6fdaad89":"code","a713e92e":"code","591491f3":"code","f10cd89e":"code","35fe43d9":"code","f681e918":"code","94730c45":"code","e63e1f1c":"code","632c5263":"code","b0e7f2da":"markdown","8ce18998":"markdown","06152da2":"markdown","afeef7a6":"markdown","83b4ac20":"markdown","0ffab6a3":"markdown","1a3b8eab":"markdown","d0304fdf":"markdown","2a8f3c05":"markdown","9e489116":"markdown","b0991b5a":"markdown","4f24a949":"markdown","4089a2dc":"markdown","34323477":"markdown","463944bb":"markdown","e74144a5":"markdown","093afcce":"markdown","d711a330":"markdown","6b494e49":"markdown","5d033dcd":"markdown","bd05d4fc":"markdown","715b315e":"markdown","c73634e8":"markdown","9ff9c4e1":"markdown"},"source":{"7b16d312":"import numpy as np\nimport pandas as pd\nimport sklearn \n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport platform\nprint('Versions:')\nprint('  python', platform.python_version())\nn = ('numpy', 'pandas', 'sklearn', 'matplotlib', 'seaborn')\nnn = (np, pd, sklearn, mpl, sns)\nfor a, b in zip(n, nn):\n    print('  --', str(a), b.__version__)","905cd1b7":"#pandas styling\npd.set_option('colheader_justify', 'left')\npd.set_option('precision', 0)\npd.options.display.float_format = '{:,.2f}'.format\npd.set_option('display.max_colwidth', -1)","e897dd6a":"#pd.reset_option('all')","89ff37c2":"#seaborn syling\nsns.set_style('whitegrid', { 'axes.axisbelow': True, 'axes.edgecolor': 'black', 'axes.facecolor': 'white',\n        'axes.grid': True, 'axes.labelcolor': 'black', 'axes.spines.bottom': True, 'axes.spines.left': True,\n        'axes.spines.right': False, 'axes.spines.top': False, 'figure.facecolor': 'white', \n        #'font.family': ['sans-serif'], 'font.sans-serif': ['Arial', 'DejaVu Sans', 'Liberation Sans', 'Bitstream Vera Sans', 'sans-serif'],\n        'grid.color': 'grey', 'grid.linestyle': ':', 'image.cmap': 'rocket', 'lines.solid_capstyle': 'round',\n        'patch.edgecolor': 'w', 'patch.force_edgecolor': True, 'text.color': 'black', \n        'xtick.top': False, 'xtick.bottom': True, 'xtick.color': 'navy', 'xtick.direction': 'out', \n        'ytick.right': False,    'ytick.left': True, 'ytick.color': 'navy', 'ytick.direction': 'out'})","ce2e86e5":"from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process \nfrom sklearn import feature_selection, model_selection, metrics\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom xgboost import XGBClassifier","377d920c":"train_raw = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_raw = pd.read_csv('..\/input\/titanic\/test.csv')\nlen(train_raw) + len(test_raw)","02b29a39":"df = pd.concat(objs=[train_raw, test_raw], axis=0)\ndf.shape","842e1cc0":"#ddict = pd.read_csv('..\/input\/Titanic_Data_Dictionary_ready.csv', index_col=0)\n#ddict","794f0c8b":"col1 = test_raw['PassengerId'] # will need for a submission\ndf.drop(['PassengerId', 'Cabin'], axis=1, inplace=True)","f6f41e28":"df['Age'] = df['Age'].fillna(df['Age'].median())\ndf['Fare'] = df['Fare'].fillna(df['Fare'].mean())\ndf['Embarked'] = df['Embarked'].fillna('S')\ndf.isnull().sum().to_frame().T","17c5f9fa":"#family sizes\ndf['Fsize'] =  df['Parch'] + df['SibSp'] + 1","8313e92a":"#titles\ndf['Surname'], df['Name'] = zip(*df['Name'].apply(lambda x: x.split(',')))\ndf['Title'], df['Name'] = zip(*df['Name'].apply(lambda x: x.split('.')))\n\ntitles = (df['Title'].value_counts() < 10)\ndf['Title'] = df['Title'].apply(lambda x: ' Misc' if titles.loc[x] == True else x)\ndf['Title'].value_counts().to_frame().T","103b6e32":"# ticket set (how many person in one ticket)\n# if one ticket has family members only, it's \"monotinic\"; if not - \"mixed\"\ndf['Tname'] = df['Ticket']\ndf['Tset']=0\n\nfor t in df['Tname'].unique():\n    if df['Surname'].loc[(df['Tname']==t)].nunique() != 1:\n        df['Tset'].loc[(df['Tname']==t)] = 'mixed'\n    else: \n        df['Tset'].loc[(df['Tname']==t)] = 'monotonic'\n\nfor t in df['Tname'].unique():\n    if df['Surname'].loc[(df['Tname']==t)].nunique() != 1:\n        df['Tset'].loc[(df['Tname']==t)] = 'mixed'\n    else: \n        df['Tset'].loc[(df['Tname']==t)] = 'monotonic'","4f67535f":"#price and \nfor t in df['Ticket'].unique():\n    df['Ticket'].loc[(df['Ticket']==t)] = len(df.loc[(df['Ticket']==t)]) \ndf['Price'] = df['Fare'] \/ df['Ticket']\n#renaming \"Ticket\"\ndf.rename(columns={'Ticket':'Tgroup'}, inplace=True)","07db61bf":"#deleting useless again\ndf.drop(['Parch', 'SibSp', 'Name', 'Surname', 'Tname', 'Fare'], axis=1, inplace=True)","d9714bcf":"df.head(2)","54edeb62":"df.dtypes.to_frame().sort_values([0]).T","03434094":"#code categorical data\nlabel = LabelEncoder()\ncols = df.dtypes[df.dtypes == 'object'].index.tolist()\nfor col in cols:\n    df[col] = label.fit_transform(df[col])","ce9e2672":"#binning\ndf['Price'] = pd.qcut(df['Price'], 4)\ndf['Age'] = pd.cut(df['Age'].astype(int), 5)","6538e438":"#code binning data\ndf['Age'] = label.fit_transform(df['Age'])\ndf['Price'] = label.fit_transform(df['Price'])","7001eef8":"df.head()","40f0783f":"a = len(train_raw)\ntrain = df[:a]\ntest = df[a:]","2ea37bd1":"train_raw.shape[0] == train.shape[0]","2ffc8fdc":"test.drop(['Survived'], axis=1, inplace=True)\ntest_raw.shape[0] == test.shape[0]","6fdaad89":"X = train.drop(['Survived'], axis=1).columns.to_list()\ny = ['Survived']","a713e92e":"#Machine Learning Algorithm initialization\nMLA = [ #Ensemble Methods\n        ensemble.AdaBoostClassifier(), ensemble.BaggingClassifier(), ensemble.ExtraTreesClassifier(),\n        ensemble.GradientBoostingClassifier(), ensemble.RandomForestClassifier(),\n        #Gaussian Processes\n        gaussian_process.GaussianProcessClassifier(),\n        #GLM\n        linear_model.LogisticRegressionCV(), linear_model.PassiveAggressiveClassifier(),\n        linear_model.RidgeClassifierCV(), linear_model.SGDClassifier(), linear_model.Perceptron(),\n        #Navies Bayes\n        naive_bayes.BernoulliNB(), naive_bayes.GaussianNB(),\n        #Nearest Neighbor\n        neighbors.KNeighborsClassifier(),\n        #SVM\n        svm.SVC(probability=True), svm.NuSVC(probability=True), svm.LinearSVC(),\n        #Trees    \n        tree.DecisionTreeClassifier(), tree.ExtraTreeClassifier(),\n        #Discriminant Analysis\n        discriminant_analysis.LinearDiscriminantAnalysis(), discriminant_analysis.QuadraticDiscriminantAnalysis(),\n        #xgboost\n        XGBClassifier() ]","591491f3":"cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0)\n\nmla = pd.DataFrame(columns=['Name','TestScore','ScoreTime','FitTime','Parameters'])\nprediction = train[y]\n\ni = 0\nfor alg in MLA:\n    name = alg.__class__.__name__\n    mla.loc[i, 'Name'] = name\n    mla.loc[i, 'Parameters'] = str(alg.get_params())\n    \n    cv_results = model_selection.cross_validate(alg, train[X], train[y], cv=cv_split)\n        \n    mla.loc[i, 'FitTime'] = cv_results['fit_time'].mean()\n    mla.loc[i, 'ScoreTime'] = cv_results['score_time'].mean()\n    mla.loc[i, 'TestScore'] = cv_results['test_score'].mean()\n\n    alg.fit(train[X], train[y])\n    prediction[name] = alg.predict(train[X])    \n    i += 1\n\nmla = mla.sort_values('TestScore', ascending=False).reset_index(drop=True)\nmla","f10cd89e":"# assing parameters\nparam_grid = {'criterion': ['gini', 'entropy'],  #default is gini\n              #'splitter': ['best', 'random'], #default is best\n              'max_depth': [2,4,6,8,10,None], #default is none\n              #'min_samples_split': [2,5,10,.03,.05], #default is 2\n              #'min_samples_leaf': [1,5,10,.03,.05], #default is 1\n              #'max_features': [None, 'auto'], #default none or all\n              'random_state': [0]}\n\n#choose best model with grid_search\ntune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring='roc_auc', cv=cv_split)\ntune_model.fit(train[X], train[y])\nprint('Parameters: ', tune_model.best_params_)","35fe43d9":"clf = tree.DecisionTreeClassifier()\nresults = model_selection.cross_validate(clf, train[X], train[y], cv=cv_split)\nclf.fit(train[X], train[y])\nresults['test_score'].mean()*100\n\n#feature selection\nfs = feature_selection.RFECV(clf, step=1, scoring='accuracy', cv=cv_split)\nfs.fit(train[X], train[y])\n\n#transform x and y to fit a new model\nX = train[X].columns.values[fs.get_support()]\nresults = model_selection.cross_validate(clf, train[X], train[y], cv=cv_split)\n\nprint('Shape New: ', train[X].shape) \nprint('Features to use: ', X)","f681e918":"#tune parameters\ntuned = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv=cv_split)\ntuned.fit(train[X], train[y])\n\nparam_grid = tuned.best_params_\nparam_grid","94730c45":"clf = ensemble.GradientBoostingClassifier()\nresults = model_selection.cross_validate(clf, train[X], train[y], cv=cv_split)\nclf.fit(train[X], train[y])\nresults['test_score'].mean()*100","e63e1f1c":"test['Survived'] = clf.predict(test[X])","632c5263":"submit = pd.DataFrame({ 'PassengerId' : col1, 'Survived': test['Survived'] }).set_index('PassengerId')\nsubmit['Survived'] = submit['Survived'].astype('int')\nsubmit.to_csv('submission.csv')","b0e7f2da":"# Modeling","8ce18998":"## Splitting back to train and test","06152da2":"I will be glad to see any your questions or suggestions. Criticism welcomed also.  \nMany thanks for your time.  \nBest,  \nLana  ","afeef7a6":"#### parameters tuning","83b4ac20":"# Imports and styling.","0ffab6a3":"Data Dictionary has main data set's feature's names as indexes and followed columns:\n- 'Definition' - meaning of feature\n- 'Description' - meaning of feature's values \n- '#Unique' - number of unique values in the columns, where NaN calculated as value also  \n- 'TopValue' - the most used value \n- '%UsedTop' - % of using top value\n- '%Missing' - % of missing values\n- 'Unit' - measurement units \n- 'Type' - measurement scales                  \n- 'Dtype' - column's python data type  ","1a3b8eab":"# Data engineering","d0304fdf":"*For some reason I can't use my data here, Kaggle support team still works with my problem (I hope so) - so, I've put an image of dictionary here.* \n\n*But everything is working on GitHub and [this notebook you can find here](https:\/\/github.com\/datalanas\/Jupyter_notebooks_to_share\/blob\/master\/Titanic_Prediction_of_binary_events.ipynb) and if it's difficult to you to create such kind of table, you can find the way I've made [this dictionary exactly ](https:\/\/github.com\/datalanas\/Jupyter_notebooks_to_share\/blob\/master\/Titanic_What_is_DataDictionary.ipynb) at GitHub also.*","2a8f3c05":"#### Transforming","9e489116":"We call feature \"useless\" if it:\n- has a single unique value\n- brings no information to algoritms, like \"PassengerID\"\n- has too many missing values, like \"Cabin\"\n- highly correlated\n- has a zero importance in a tree-based model","b0991b5a":"#### feature selection","4f24a949":"## Concatenating data sets","4089a2dc":"#### feature engineering","34323477":"#### missing values","463944bb":"This simple method gives me 0.799 score in Kaggle, what is a top 12% for today.","e74144a5":"![image.png](attachment:image.png)","093afcce":"#### useless features","d711a330":"# Working with Data Dictionary: simplier, faster and more accurate way to understand the data.","6b494e49":"Column \"Type\" may has followed values:\n1. Useless (useless for machine learning algorithms)\n2. Nominal (truly categorical, labels or groups without order)\n3. Binary (dichotomous - a type of nominal scales that contains only two categories)\n4. Ordinal (groups with order)\n5. Discrete  (count data, the number of occurrences)\n6. Cont (continuous with an absolute zero,but without a temporal component)\n7. Interval (continuous without an absolute zero, but without a temporal component)\n8. Time (cyclical numbers with a temporal component; continuous)\n9. Text\n10. Image\n11. Audio\n12. Video","5d033dcd":"## Submission","bd05d4fc":"Creating such kind of table takes not a lot, but save much more time and gives me feeling of TRUE understanding the data.\nHope you feel the same.","715b315e":"## Prediction","c73634e8":"## Open a Data Dictionary","9ff9c4e1":"This notebook not for very beginners but for entry and middle-level Data Scientists. I want to share here a simple way to make your job easy and produce a more accurate result.\n\nWhat you will NOT find here:\n1. EDA\n2. Plots\n3. Long explanations which is called in my country a \"text water\"\n\nWhat you will find:\n1. Data Dictionary\n2. Data Types explanation\n3. My choise of feature engineering (some of features you may not expect)\n4. Model selection\n5. Model tuning (feature also)\n6. Modeling\n\nLet's get started."}}