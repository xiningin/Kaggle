{"cell_type":{"821793cc":"code","18d6a54a":"code","6901d98d":"code","12415fb6":"code","0774bbe0":"code","0ec9bfcc":"code","b4767309":"code","71e1d143":"code","a7cdd4b9":"code","4d28ed9c":"code","74d0e25c":"code","b54c06d7":"code","42a71437":"code","7db9e105":"code","e4998494":"code","93a2ce42":"code","41f57ff8":"code","01c43909":"code","c442273f":"code","1cb36733":"markdown","7151c9b1":"markdown","329eade9":"markdown","ec4bc533":"markdown","d39febce":"markdown","a9f416d0":"markdown"},"source":{"821793cc":"# PACKAGES\n\n# standard\nimport numpy as np\nimport pandas as pd\nimport time\nimport random\n\n# plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# NLP\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nfrom collections import Counter\n\n# H2O\nimport h2o\nfrom h2o.estimators import H2OWord2vecEstimator\n\n# UMAP\nimport umap","18d6a54a":"# read data\nt1 = time.time()\ndf = pd.read_csv('..\/input\/political-though-work-corpus\/political_thought_works_corpus.csv')\nt2 = time.time()\nprint('Elapsed time: ', np.round(t2-t1,2))","6901d98d":"# clean up\ndf = df.drop('Unnamed: 0', axis=1)\ndf = df.drop('text', axis=1)\n# replace special character\ndf.loc[1,'authors'] = 'Niccolo Machiavelli'","12415fb6":"# add features\ndf['n_char'] = df.text_clean.str.len()\ndf['n_word'] = df.text_clean.str.split().map(lambda x : len(x))","0774bbe0":"# show overview\ndf","0ec9bfcc":"# plot number of words\nplt.figure(figsize=(12,4))\nplt.bar(df.book_title, df.n_word)\nplt.title('Number of words')\nplt.xticks(rotation=90)\nplt.grid()\nplt.show()","b4767309":"# extract list of authors\nauthors = df.authors.tolist()\nprint(authors)","71e1d143":"stopwords = set(STOPWORDS)\n\nt1 = time.time()\nfor a in authors:\n    df_temp = df[df.authors==a]\n    \n    print('Author = ', a.upper(), ':')\n    \n    # render wordcloud\n    text = ' '.join(txt for txt in df_temp.text_clean)\n    wordcloud = WordCloud(stopwords=stopwords, max_font_size=50, max_words=500,\n                          width = 600, height = 400,\n                          background_color='white').generate(text)\n    plt.figure(figsize=(12,8))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\nt2 = time.time()\nprint('Elapsed time: ', np.round(t2-t1,2))","a7cdd4b9":"for a in authors:\n    df_temp = df[df.authors==a]\n    df_temp = df_temp.reset_index()\n    txt = df_temp.text_clean[0]\n    \n    # token frequencies\n    tokens = word_tokenize(txt)\n    freq = Counter(tokens)\n    freq = sorted(freq.items(), key=lambda x: x[1], reverse=True)\n    freq10 = dict(freq[0:10]) # get top 10 and convert list => dict\n    # plot\n    plt.figure(figsize=(14,4))\n    plt.bar(freq10.keys(), freq10.values())\n    plt.title('Author='+a)\n    plt.grid()\n    plt.show()","4d28ed9c":"# start H2O\nh2o.init()","74d0e25c":"# utility function for tokenization\ndef tokenize(sentences, stop_word = stopwords): # use stop words from wordcloud package\n    tokenized = sentences.tokenize('\\\\W+')\n    tokenized_lower = tokenized.tolower()\n    tokenized_filtered = tokenized_lower[(tokenized_lower.nchar() >= 2) | (tokenized_lower.isna()),:]\n    tokenized_words = tokenized_filtered[tokenized_filtered.grep('[0-9]',invert=True,output_logical=True),:]\n    tokenized_words = tokenized_words[(tokenized_words.isna()) | (~ tokenized_words.isin(stop_word)),:]\n    return tokenized_words","b54c06d7":"# upload data to H2O environment\ntext_h2o = h2o.H2OFrame(df[['authors','text_clean']])","42a71437":"# tokenize text\nt1 = time.time()\nwords = tokenize(text_h2o['text_clean'])\nt2 = time.time()\nprint('Elapsed time:', np.round(t2-t1,2), 'secs')","7db9e105":"# train Word2Vec model\nrandom.seed(1234)\n\nt1 = time.time()\nw2v_model = H2OWord2vecEstimator(vec_size = 50,\n                                 window_size = 5,\n                                 sent_sample_rate = 0.001,\n                                 init_learning_rate = 0.025,\n                                 epochs = 10)\nw2v_model.train(training_frame=words)\nt2 = time.time()\nprint('Elapsed time:', np.round(t2-t1,2), 'secs')","e4998494":"# check model\nw2v_model.find_synonyms('knowledge', count = 5)","93a2ce42":"# create vector representation for each author\ntext_vec = w2v_model.transform(words, aggregate_method = 'AVERAGE')\n# and add authors to vectors\ntext_vec = text_vec.cbind(text_h2o['authors'])\ntext_vec","41f57ff8":"# vector features (columns w\/o the label)\nfeatures = text_vec.columns\nfeatures.remove('authors')\n\n# convert H2O frame to Pandas data frame\ndf_text_vec = text_vec.as_data_frame();\n\n# drop rows with missing values\ndf_text_vec = df_text_vec.dropna(axis=0)","01c43909":"# run UMAP algorithm to get a low dimensional (in our case 2D) representation\ndim_reducer = umap.UMAP(random_state=111, n_components=2,\n                        n_neighbors=4)\n\nt1 = time.time()\ntext_vec_umap = dim_reducer.fit_transform(df_text_vec[features])\nt2 = time.time()\nprint('Elapsed time:', np.round(t2-t1,2), 'secs')\n\n# convert result matrix to data frame\ndf_text_vec_umap = pd.DataFrame(text_vec_umap, columns=['x','y'])\n# and add school again\ndf_text_vec_umap['authors'] = df_text_vec.authors.tolist()","c442273f":"# plot author vectors\nplt.figure(figsize=(12,8))\nsns.scatterplot(data=df_text_vec_umap, x='x', y='y', \n                hue='authors', alpha=1,\n                s=1000)\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), ncol=1)\nplt.grid()\nplt.show()","1cb36733":"<a id='2'><\/a>\n# Word Frequencies by Author","7151c9b1":"<a id='4'><\/a>\n# Visualize using UMAP","329eade9":"# Table of Contents\n* [Wordcloud by Author](#1)\n* [Word Frequencies by Author](#2)\n* [Word2Vec - Word Embeddings](#3)\n* [Visualize using UMAP](#4)","ec4bc533":"#### Using code from: https:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/data-science\/word2vec.html\u00b6","d39febce":"<a id='1'><\/a>\n# Wordcloud by Author","a9f416d0":"<a id='3'><\/a>\n# Word2Vec - Word Embeddings"}}