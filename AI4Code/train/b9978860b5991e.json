{"cell_type":{"53c628cc":"code","a614ad17":"code","42359094":"code","2ce723d8":"code","738eaf70":"code","6b201873":"code","bf24f077":"code","334c471a":"code","4888f2ff":"code","754cc24f":"code","3a402111":"code","832820a3":"code","c944b01c":"code","c5f7d95f":"code","d18aa832":"code","7d1797ff":"code","0bf6e7bc":"code","f4b3dc79":"code","cd2cb0b2":"code","9ae38d16":"code","deddc690":"code","76bcb0f9":"code","04ffc857":"markdown","70532b2f":"markdown","2259981f":"markdown"},"source":{"53c628cc":"import numpy as np\nimport pandas as pd\n\nfrom datetime import datetime\nimport gc\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc\n\nimport json\nimport optuna\n\nimport lightgbm as lgbm\nimport optuna.integration.lightgbm as lgbo\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datatable as dt\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nfrom warnings import resetwarnings\n\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\n## Better check what version we are using as I may want to make use of init_model\nimport lightgbm as lgbm\nprint (lgbm.__version__)","a614ad17":"def reduce_mem_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","42359094":"%%time\ndf_train = reduce_mem_usage(dt.fread('..\/input\/tabular-playground-series-oct-2021\/train.csv').to_pandas())\ndf_test = reduce_mem_usage (dt.fread('..\/input\/tabular-playground-series-oct-2021\/test.csv').to_pandas())\n\nsample_submission = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv\")\n","2ce723d8":"## Always handy to know how many features we are dealing with\nnum_cols = [col for col in df_test.columns]  ## remember that this will include id which isn't really a feature","738eaf70":"# prepare dataframe for modeling\nX = df_train.drop(columns=[\"id\", \"target\"]).copy()\ny = df_train[\"target\"].copy()\n\ntest_data = df_test.drop(columns=[\"id\"]).copy()","6b201873":"cat_cols = X.columns[(X.nunique() < 5)]\ncon_cols = X.columns[(X.nunique() >= 5)]\ncat_cols_indices = [X.columns.get_loc(col) for col in cat_cols]","bf24f077":"f_pred = pd.DataFrame(np.zeros(df_train.shape[0]), columns = ['pred'] )","334c471a":"\ndel df_test, df_train\ngc.collect()\n","4888f2ff":"\n\nscaler = StandardScaler()\nX[con_cols] = reduce_mem_usage(pd.DataFrame(columns=con_cols, data=scaler.fit_transform(X[con_cols])))\ntest_data[con_cols] = reduce_mem_usage(pd.DataFrame(columns=con_cols, data=scaler.transform(test_data[con_cols])))\n","754cc24f":"# Hopefully there is nothing new above this line\ndef load_sp_data():\n    df_train_tmp = pd.read_csv(\"..\/input\/sandpit-data\/v6_f_pred_train.csv\")\n    df_test_tmp = pd.read_csv(\"..\/input\/sandpit-data\/v6_f_pred_test.csv\")\n    X['v6']  = df_train_tmp['pred']\n    test_data['v6'] = df_test_tmp['pred']\n\n    df_train_tmp = pd.read_csv(\"..\/input\/sandpit-data\/v7_f_pred_train.csv\")\n    df_test_tmp = pd.read_csv(\"..\/input\/sandpit-data\/v7_f_pred_test.csv\")\n    X['v7']  = df_train_tmp['pred']\n    test_data['v7'] = df_test_tmp['pred']\n    \n    df_train_tmp = pd.read_csv(\"..\/input\/sandpit-data\/v8_f_pred_train.csv\")\n    df_test_tmp = pd.read_csv(\"..\/input\/sandpit-data\/v8_f_pred_test.csv\")\n    X['v8']  = df_train_tmp['pred']\n    test_data['v8'] = df_test_tmp['pred']\n    \n    df_train_tmp = pd.read_csv(\"..\/input\/sandpit-data\/v10_f_pred_train.csv\")\n    df_test_tmp = pd.read_csv(\"..\/input\/sandpit-data\/v10_f_pred_test.csv\")\n    X['v10']  = df_train_tmp['pred']\n    test_data['v10'] = df_test_tmp['pred']\n    \n    df_train_tmp = pd.read_csv(\"..\/input\/sandpit-data\/v11_f_pred_train.csv\")\n    df_test_tmp = pd.read_csv(\"..\/input\/sandpit-data\/v11_f_pred_test.csv\")\n    X['v11']  = df_train_tmp['pred']\n    test_data['v11'] = df_test_tmp['pred']","3a402111":"load_sp_data ()\n# As I run this I realise that maybe I could just grap v12 now as I have also included the dataset of this notebook. So I have access to anything in last saved version\n\ndf_train_tmp = pd.read_csv(\"..\/input\/tps-oct-joes-sandpit\/f_pred_train.csv\" )\ndf_test_tmp = pd.read_csv(\"..\/input\/tps-oct-joes-sandpit\/f_pred_test.csv\")\nX['v12']  = df_train_tmp['pred']\ntest_data['v12'] = df_test_tmp['pred']\n\n### Just in case I want it again ... in next run.\ndf_train_tmp.to_csv(\".\/f_pred_train_12.csv\", index=True)\ndf_test_tmp.to_csv(\".\/f_pred_test_12.csv\", index=True)\n\n\nf_pred_cols = ['v6', 'v7', 'v8', 'v10', 'v11', \"v12\" ]\nprint (X[f_pred_cols])\n","832820a3":"# okay that looks decent but I forgot what we are dealing with. \n# From the header - I get the details\n# V6 LGBM1 Overall Training Validation Score | Blend: 0.8571127311293687 with LB score of 0.85633\n# V7 XGB0 Overall Training Validation Score | Blend: 0.8570048237818386 with LB score of 0.85645\n# V8 XGB1 Overall Training Validation Score | Blend: 0.8568621253215738 with LB score of 0.85621\n# V10 CATB0 Overall Training Validation Score | Blend: 0.8566436011399643 with LB score of 0.85597\n# V11 LGBM0 Overall Training Validation Score | Blend: 0.8569640619014858 with LB score of 0.85638\n# V12 LGBM0 Overall Training Validation Score | Blend: 0.8569158481801216 with LB score of 0.85639\n\n# I want one of each type so will definetly use v10\n# Then I will add v11 for my lgb choice and v7 for my xgb choice\n# So (double check) I should drop v6, v8, v12\n\n# V10 is not a great model. Hopefully not my doing and rather a 'function' of catboost on the gien dataset.\n# Should we include V10 or will it just be noise. Add FI later to check\n\nX = X.drop(columns=[\"v6\", \"v8\", \"v12\"])\n\n### apparently you can get much further into the notebook before it reminds you to do something simialr to test :(\ntest_data = test_data.drop(columns=[\"v6\", \"v8\", \"v12\"])\n\n\n###\n### What happens if we don't drop them and use all 6 new features ?\n### What happens if we add another 10 such new features ?\n### Well that always depends. I tend to think as computers of being able to use every last bit of data but there are always cases that it doens't work in your favour.\n### For this partcular case .... I don't know, I haven't tried. \n\n### But I am 95% sure that theh winner of this comp will have used more that one model and most probably lots of models!\n\nprint (X)","c944b01c":"del scaler\ngc.collect()","c5f7d95f":"# I still need to find where I got these params from and the try and credit the source\nlgbm_params_0 = {\n     'objective': 'binary',\n     'n_estimators':92700,        ## way to high but I have faith in early stopping criteria\n#      'importance_type': 'gain',\n     'metric':'auc',\n#      'boosting_type': 'gbdt',\n     'boosting_type': 'goss',\n     'n_jobs' : -1,\n    'learning_rate': 0.1, \n    'colsample_bytree': 0.3, \n    'reg_lambda': 0.011685550612519125, \n    'reg_alpha': 0.04502045156737212,\n#     Let the model know about the categorical features    \n#     'categorical_feature' : cat_features,\n#     'feature_name': 'auto',\n    'min_child_weight': 16.843316711276092, \n    'min_child_samples': 412, \n    'num_leaves': 546, \n    'max_depth': 5, \n    'cat_smooth': 36.40200359200525, \n    'cat_l2': 12.979520035205597\n    }\n\n# https:\/\/www.kaggle.com\/vishwas21\/tps-oct-21-eda-modeling\n# I'm not sure if that is the original source, but it is one of the earliest notebooks that mention these params\nxgb_params_0 = {\n    'max_depth': 6,\n    'n_estimators': 19500,\n    'subsample': 0.7,\n    'colsample_bytree': 0.2,\n    'colsample_bylevel': 0.6000000000000001,\n    'min_child_weight': 56.41980735551558,\n    'reg_lambda': 75.56651890088857,\n    'reg_alpha': 0.11766857055687065,\n    'gamma': 0.6407823221122686,\n    'booster': 'gbtree',\n    'eval_metric': 'auc',\n    'tree_method': 'hist',\n    'predictor': 'cpu_predictor',\n#     'tree_method': 'gpu_hist',\n#     'predictor': 'gpu_predictor',\n    'use_label_encoder': False\n    }\n\n\n\nxgb_params_1 = {\n    \"objective\": \"binary:logistic\",\n#     \"learning_rate\": 8e-3,\n    \"learning_rate\": 0.016,\n    \"subsample\": 0.6,\n    \"colsample_bylevel\": 0.9,\n    \"colsample_bytree\": 0.4,\n    \"n_estimators\": 20_000,\n    \"max_depth\": 8,\n    \"alpha\": 64,\n    \"lambda\": 32,\n    \"min_child_weight\": 8,\n    \"importance_type\": \"total_gain\",\n    \"tree_method\": \"hist\"#,\n#     \"predictor\": \"gpu_predictor\",\n}\n\nxgb_params_2 = {\n    \"objective\": \"binary:logistic\",\n    \"eval_metric\": \"auc\",\n    \"tree_method\": \"hist\",\n#     \"learning_rate\": 0.02,\n    \"learning_rate\": 0.1,\n    \"n_estimators\": 20_000,\n    \"random_state\": 42,\n    \"lambda\": 0.003731399945310043,\n    \"alpha\": 0.1660536107526955,\n    \"colsample_bytree\": 0.5164889907489927,\n    \"subsample\": 0.5869840790716806,\n    \"max_depth\": 18,\n    \"min_child_weight\": 142}\n\n\nlgb_params = {'n_estimators': 20000,\n          'max_depth': 3,\n          'learning_rate': 0.003,\n          'reg_alpha': 9.5,\n          'reg_lambda': 8.5,\n          'num_leaves': 109,\n          'min_data_per_group': 133,\n          'min_child_samples': 113,\n          'colsample_bytree': 0.2,\n          'objective': 'binary',\n          'random_state': 2001,\n          'metric': 'auc',}\n\n\n\n# Taken from https:\/\/www.kaggle.com\/dwoodlock\/tps-oct2021-don#Modeling\n## he reckons around 0.85673 on average over 2 seeds\nlgbm_params_1 =  {\n    'objective' : 'binary',\n    'metric' : 'auc',\n    'num_leaves' : 7,\n#    'learning_rate' : 0.08,\n    'learning_rate' : 0.1,\n#     'device' : 'gpu',\n    'feature_pre_filter': False, \n    'reg_alpha': 9.314037635261775, \n    'reg_lambda': 0.10613573572440353,\n    'num_leaves': 7,\n    'colsample_bytree': 0.4, \n    'subsample': 0.8391963650875751, \n    'subsample_freq': 5, \n    'min_child_samples': 100,\n#     'num_iterations': trees, #10000,\n    'n_estimators': 60000\n}\n\ncatb_params_0 = {\n    \"objective\": \"CrossEntropy\",\n    \"eval_metric\": \"AUC\",\n    \"grow_policy\": \"SymmetricTree\",\n    \"learning_rate\": 0.016,\n    \"n_estimators\": 30000,\n    \"random_strength\": 1.0,\n    \"max_bin\": 128,\n    \"l2_leaf_reg\": 0.002550319996478972,\n    \"max_depth\": 4,\n    \"min_data_in_leaf\": 193,\n    \"random_state\": 42\n}","d18aa832":"# ### Now what do we choose for modelling.\n\n# I'm really hoping that the model I choose will \n# 1. Identify my new features as very important\n# 2. Spot instances of my new feature giving wrong results\n# 3. Magically identify (and rectify) some comibnation of factors that are common for records in 2.\n\n# As I think through the things that I want, I wonder if kmeans, generating new features at this stage would be the right way to go.\n# My kmeans attempt at https:\/\/www.kaggle.com\/joecooper\/experiment-with-kmeans certainly showed that it added value. \n# Recap - I used the predictions from sandpit to generate clusters (or cluster predictions) and then added those to the training data. \n# In this one I'm adding sandpit predictions directly to training data. I believe that (or maybe just hope) that the predictions\n# themselves will be more handy than a cluster feature they generate.\n\n# I think combining the 2 together might make task2 and 3 above a little easier for computer to solve. If it was a 2 month competition I would certainly look at that.\n# Actually if I had 1 spare day I would look at that. Don't think it would take long to check.\n\n# Anyway - I just randomly choose lgb first and lets see what happens.\n\n# I should also point out that I'm expecting FI to be significant for new features.\n# How will that change the gradient of each boost? \n# Certainly a different set of params might be MUCH better - but optuna takes time and I'm only hoping for a little bump up the leaderboard.\n\n# So I just choose another set of lgb params from public notebooks. \n# Not sure having the exact same set as a feature your trained on already is a good idea?\n# Again haven't tried, but my instinct say diversify\n\n","7d1797ff":"%%time\n\n## Moving onto the time killer section\nmodels = [\n#     (\"lgbm3\", LGBMClassifier(**lgbm_params_3)),\n    (\"lgbm0\", LGBMClassifier(**lgbm_params_0)),\n#     (\"lgbm0a\", LGBMClassifier(**lgbm_params_0a)),\n#     (\"lgbm0b\", LGBMClassifier(**lgbm_params_0b)),\n#     (\"lgbm0c\", LGBMClassifier(**lgbm_params_0c)),\n#     (\"lgbm1\", LGBMClassifier(**lgbm_params_1)),\n#     (\"lgbm1a\", LGBMClassifier(**lgbm_params_1a)),\n#     (\"lgbm1b\", LGBMClassifier(**lgbm_params_1b)),\n#     (\"lgbm1c\", LGBMClassifier(**lgbm_params_1c)),\n#    (\"lgbm1\", LGBMClassifier(**lgbm_params_1)),\n#    (\"lgbm1\", LGBMClassifier(**lgbm_params_1)),\n#     (\"catb0\", CatBoostClassifier(**catb_params_0)),\n#     (\"catb1\", CatBoostClassifier(**catb_params_1)),\n#     (\"xgb0\", XGBClassifier(**xgb_params_0)),\n#     (\"xgb1\", XGBClassifier(**xgb_params_1)),\n]\n\noof_pred_tmp = dict()\ntest_pred_tmp = dict()\nscores_tmp = dict()\n\n### Another new addition so that I can see FI\nmod_importances = pd.DataFrame()\n\nprint (\"Start time :\", datetime.now() )\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X, y)):\n    print (\"Fold : \", fold, \" Start : \", datetime.now())\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n\n\n    for name, model in models:\n        start_time = datetime.now()\n        print (\"model name : \", name, \" Start : \", datetime.now())\n        if name not in scores_tmp:\n            oof_pred_tmp[name] = list()\n            oof_pred_tmp[\"y_valid\"] = list()\n            test_pred_tmp[name] = list()\n            scores_tmp[name] = list()\n\n        if name.startswith(\"lgbm0\"):\n            print (\"Name starts with lgbm0\")\n     \n            # I think that I will need to go much lower with my new features\n            lgb_params['learning_rate'] = 0.003\n            model = LGBMClassifier(**lgb_params)\n            model.fit( X_train, y_train,\n                eval_set=[(X_valid,y_valid)],\n                eval_metric='auc',\n                early_stopping_rounds=700,        \n                verbose=200,\n            )\n            \n            ### messing about with second fits is not needed\n            ### I was just messing about with it since I hadn't tried it before. \n            ### Can't say that I was overly impressed\n            \n            \n        \n        elif name.startswith(\"xgb0\"):\n            print (\"Name starts with xgb0\")\n            \n            xgb_params_0['learning_rate'] = 0.02\n            pre_model = XGBClassifier(**xgb_params_0)\n            pre_model.fit( X_train, y_train,\n                eval_set=[(X_valid,y_valid)],\n                eval_metric='auc',\n                early_stopping_rounds=700,        \n                verbose=1000,\n            )\n            \n            xgb_params_0['learning_rate'] *= 0.1\n            model = XGBClassifier(**xgb_params_0)\n            model.fit(X_train, y_train,\n                    eval_set=[(X_valid,y_valid)],\n                    eval_metric='auc',\n                    early_stopping_rounds=500,\n                    verbose=200,\n                    xgb_model=pre_model)\n            \n        elif name.startswith(\"catb0\"):\n            print (\"Name starts with catb0\")\n            ### catboost doens't like a boolean target \n            y_train = y_train.astype('float')\n            y_valid = y_valid.astype('float')\n            model.fit(\n                X_train, y_train,\n                eval_set=[(X_valid,y_valid)],\n                early_stopping_rounds=500,\n                verbose=1000)\n            \n                \n        else:\n            model.fit(\n                X_train, y_train,\n                eval_set=[(X_valid,y_valid)],\n                early_stopping_rounds=500,\n                verbose=1000\n            )\n        ## end if\n        \n        # Can't recall where I nicked this, but it is useful\n        fi_tmp = pd.DataFrame()\n        fi_tmp['feature'] = X_train.columns\n        fi_tmp['importance'] = model.feature_importances_\n        fi_tmp['fold'] = fold\n        fi_tmp['seed'] = 42    ## seed value within params\n        mod_importances = mod_importances.append(fi_tmp)\n        \n        ## This FI code is really handy. It can also be used for handling lopps of seeds as well.\n        ## Good practice for LB climbing but overkill for a sandpit\n    \n        ### Time to get the predictions for oof test data (valid data)\n        pred_valid = model.predict_proba(X_valid)[:, -1]\n        score = roc_auc_score(y_valid, pred_valid)\n        \n        scores_tmp[name].append(score)\n        oof_pred_tmp[name].extend(pred_valid)\n        \n        end_time = datetime.now()\n        duration = end_time - start_time\n        \n        f_pred.loc[idx_valid, 'pred'] = model.predict_proba(X_valid)[:, 1]\n        \n        print(f\"Fold: {fold} Model: {name} Score: {score}\", \"Duration :\", duration)\n\n        ### Time to get the predictions for competition test data (df_test descendandts)\n        y_hat = model.predict_proba(test_data)[:,1]\n        test_pred_tmp[name].append(y_hat)\n        \n    oof_pred_tmp[\"y_valid\"].extend(y_valid)\n        \n        \nfor name, model in models:\n    print(f\"Average Validation Score | {name}: {np.mean(scores_tmp[name])}\")","0bf6e7bc":"f_pred.to_csv(\".\/f_pred_train.csv\", index=True)","f4b3dc79":"# Time to unravel those dictionaries of models and predictions \ntest_predictions = pd.DataFrame(  \n    {name: np.mean(np.column_stack(test_pred_tmp[name]), axis=1) for name in test_pred_tmp.keys()}\n)\n\n# Always handy to save a copy for use later\ntest_predictions.to_csv(\".\/test_predictions.csv\", index=False)\n\n# get the average of all predictions\ntest_predictions[\"avg\"] = test_predictions.mean(axis=1)\n\n# overwrite target in sample_submission with our new and improved predictions \navg_submission = sample_submission.copy()\navg_submission[\"target\"] = test_predictions[\"avg\"]\navg_submission.to_csv(\".\/avg_submission.csv\", index=False)","cd2cb0b2":"f_pred_test = sample_submission.copy()\nf_pred_test['pred'] = test_predictions[\"avg\"]\nf_pred_test.to_csv(\".\/f_pred_test.csv\", index=True)","9ae38d16":"# So that oof_pred_tmp dictionary that we populated during training was so that\n# we can get the real validation score across the whole training dataset and not just avergae \n# individual folds. I have once before started a discussion within the competition detailing the \n# importance of this step\n\noof_predictions = pd.DataFrame({name:oof_pred_tmp[name] for name in oof_pred_tmp.keys()})\n\n# Again Always handy so save a copy for revisiting\noof_predictions.to_csv(\".\/oof_preds.csv\", index=False)\n\n# And now we get the all important validation score\ny_valid = oof_predictions[\"y_valid\"].copy()\ny_hat_blend = oof_predictions.drop(columns=[\"y_valid\"]).mean(axis=1)\nscore = roc_auc_score(y_valid, y_hat_blend)\n\nprint(f\"Overall Training Validation Score | Blend: {score}\")\n","deddc690":"import matplotlib.pyplot as plt\nimport seaborn as sns\norder = list(mod_importances.groupby('feature').mean().sort_values('importance', ascending=False).index)\n\n## I want the graph to be readable\nfor i in range(250):\n    order.pop()\n## Half embarrassed to put out such lazy code - but be fair ... it is doing the job I want it to.\n\n    \n\nfig = plt.figure(figsize=(16, 16), tight_layout=True)\nsns.barplot(x=\"importance\", y=\"feature\", data=mod_importances.groupby('feature').mean().reset_index(), order=order)\nplt.title(\"LGB feature importances\")\n\n#  From the original sandpit notebook we see \n#Results tracking\n# V3 LGBM0 Overall Training Validation Score | Blend: 0.8569692225223414 with LB score of 0.85633\n# V4 LGBM0 Overall Training Validation Score | Blend: 0.8570320102774184 with LB score of 0.85623\n# V5 LGBM0 Overall Training Validation Score | Blend: 0.8570190414332162 with LB score of 0.85627\n# V6 LGBM1 Overall Training Validation Score | Blend: 0.8571127311293687 with LB score of 0.85633\n# V7 XGB0 Overall Training Validation Score | Blend: 0.8570048237818386 with LB score of 0.85645\n# V8 XGB1 Overall Training Validation Score | Blend: 0.8568621253215738 with LB score of 0.85621\n# V10 CATB0 Overall Training Validation Score | Blend: 0.8566436011399643 with LB score of 0.85597\n# V11 LGBM0 Overall Training Validation Score | Blend: 0.8569640619014858 with LB score of 0.85638\n\n\n# When Running the LGB model the best feature is the xgb output \n# What does that tell us?\n\n","76bcb0f9":"print (f_pred)\nprint (X[['v7', 'v10', 'v11']])\n\n### So why do I look here ?\n\n# Well I'm only looking at row 0 to start. The original models had predictions of 0.62, 0.59 and 0.64\n# Averaging, ensembling, stacking (whatever you want to call that) would normally give a new prediction here of around 0.62\n# My new prediction is 0.57\n# Now for each row - new results are always closer to 0.5 regardless of direction. Statistically speaking f_pred has lower variance\n\n# So my new results are still the same in 'classification' but they are less confident\n\n# Could we add this new feature into the next version and just keep looping?\n# Would pred end up being 1 million rows of 0.5\n# Would it help throwing this through a Scaler ?\n# Will the stccking benefit of this output be less helpful to the leaderboard?\n\n# In answer to the original question. I look becuase I'm interested! \n# I want to understand and question what is going on. \n# Sometimes looking generates questions that lead to ideas that leads to innvoation.\n","04ffc857":"Results tracking\n\nV3  LGBM0 Overall Training Validation Score | Blend: 0.8569692225223414 with LB score of 0.85633\n\nV4  LGBM0 Overall Training Validation Score | Blend: 0.8570320102774184 with LB score of 0.85623\n\nV5  LGBM0 Overall Training Validation Score | Blend: 0.8570190414332162 with LB score of 0.85627\n\nV6  LGBM1 Overall Training Validation Score | Blend: 0.8571127311293687 with LB score of 0.85633\n\nV7   XGB0 Overall Training Validation Score | Blend: 0.8570048237818386 with LB score of 0.85645\n\nV8   XGB1 Overall Training Validation Score | Blend: 0.8568621253215738 with LB score of 0.85621\n\nV10 CATB0 Overall Training Validation Score | Blend: 0.8566436011399643 with LB score of 0.85597\n\nV11 LGBM0 Overall Training Validation Score | Blend: 0.8569640619014858 with LB score of 0.85638\n\nV12 LGBM0 Overall Training Validation Score | Blend: 0.8569158481801216 with LB score of 0.85639\n\n\n\nRecap of version 12\nThat v11 vs v12 is starting to annoy me.\nLet me double check my changes. Boolean features where changed to type 'category'. \nBoolean features where removed from Scaler. (another Q.  what happens to type catgeory within a scaler?)\nBoolean features where listed as such within the params (another Q. is that not default for the datatype?)\nSo really simple changes and the CV score decreases by 5 times as much as the LB increases.\nIgnoring the LB side of the problem doesn't help becuase I can't explain the CV side to start with.\nAnd I really don't want to trawl through LGB source code to search for an answer. \nAlways that same darn riddle. Do you trust LB or CV?\n\n\nI'll need to go read ROC notebooks again :(\nWasted my time. It seems that is hasn't changed in ages. \n\n\n","70532b2f":"Time to stimulate the thought processes.\n\n\nThere is NO NEW code here. You've seen similar code many times.\nI expect that the vast majority of which has been better written.\n\nMy thoughts are scattered around the notebook. Again you generally probably see (or have) better ones. \nMine are different in that I don't bother to spell check them!\n\n","2259981f":"Onto Version 13.\n\nSo some really difficult situations as I want one and only one LGBM model.\nWhich is the right one to choose.\n\n\nI'm going to choose V11 as I trust CV but obviously the reader can do whatever they want with whatever they may have to hand. Also I forgot to add v12 to the dataset.\nI'm too lazy to add it now. You know where it is if you really want it.\nBut yes - notice I added a new dataset which should be public. This is a dataset containing the key oof and submission data that this notebook has generated. \nSo it was effectively public already. I've just colleced and centralised it.\n\n\nOnly a couple days left and this notebook is about 20th on the list of public notebooks. An improvment of 0.0003 would put it in 1st place or public notebooks. \nIt would also move it into top 30-100 of the real LB. I wonder how many of those tied for 30th have teh exact same copied code.\n\nThis gives us something to aim for as the vast majority of better scoring code is generally just blending and lets be honest: If your code runs in less that 20 seconds it probably isn't going to be that original.\n\nI'm keen to try some ensemble with a twist. Too much of the standard stuff already published. In the past we sometimes tried adding a prediction set as a new feature and \nI'm going to give that a try today.\n\n\n\n"}}