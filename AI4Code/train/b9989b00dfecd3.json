{"cell_type":{"61ded8be":"code","f3362712":"code","237c32b3":"code","691b588e":"code","8574745f":"code","a4e97253":"code","14f3a4c6":"code","ef756c85":"code","a233307b":"code","57e689ab":"code","e4da6b2a":"code","0774f2cb":"code","36bc75d6":"code","c72a0356":"code","a86f66d8":"code","21ce2df2":"code","2ecc858a":"markdown","90655a30":"markdown","f11ed18c":"markdown","5092264c":"markdown","6911dca4":"markdown","a0065eb4":"markdown","69442c4d":"markdown","984d7967":"markdown","67891529":"markdown","1a421bee":"markdown"},"source":{"61ded8be":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","f3362712":"df_solar= pd.read_csv('..\/input\/SolarEnergy\/SolarPrediction.csv')\ndf_solar","237c32b3":"sns.distplot(df_solar['UNIXTime'])","691b588e":"df_solar=df_solar.drop(['Data','Time','TimeSunRise','TimeSunSet'],axis=1)\ndf_solar","8574745f":"figure= plt.figure(figsize=(10,10))\nsns.heatmap(df_solar.corr(),annot=True)","a4e97253":"figure= plt.figure(figsize=(20,10))\nsns.pairplot(df_solar)","14f3a4c6":"X= df_solar.drop('Radiation',axis=1)\ny=df_solar['Radiation']","ef756c85":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,random_state=101)","a233307b":"from sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\n\nresults_df = pd.DataFrame()\ncolumns = [\"Model\", \"Cross Val Score\", \"MAE\", \"MSE\", \"RMSE\", \"R2\"]\n\ndef evaluate(true, predicted):\n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    return mae, mse, rmse, r2_square\n\ndef append_results(model_name, model, results_df, y_test, pred):\n    results_append_df = pd.DataFrame(data=[[model_name, *evaluate(y_test, pred) , cross_val_score(model, X, y, cv=10).mean()]], columns=columns)\n    results_df = results_df.append(results_append_df, ignore_index = True)\n    return results_df","57e689ab":"from sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nada_reg= AdaBoostRegressor(RandomForestRegressor())\nada_reg.fit(X_train,y_train)","e4da6b2a":"pred = ada_reg.predict(X_test)","0774f2cb":"results_df= append_results(\"AdaBoost Regression\",AdaBoostRegressor(RandomForestRegressor()),results_df,y_test,pred)\nresults_df","36bc75d6":"sns.distplot((y_test,pred))","c72a0356":"from sklearn.ensemble import GradientBoostingRegressor\ngr_reg= GradientBoostingRegressor()\ngr_reg.fit(X_train,y_train)\npred = gr_reg.predict(X_test)","a86f66d8":"sns.distplot((y_test,pred))","21ce2df2":"results_df= append_results(\"Gradient Regression\",GradientBoostingRegressor(),results_df,y_test,pred)\nresults_df","2ecc858a":"### Data Frame to cotain regression Evaluation:\n","90655a30":"## 1- Regression AdaBoost :\n#### AdaBoost is one of the first boosting algorithms to be adapted in solving practices. Adaboost helps you combine multiple \u201cweak classifiers\u201d into a single \u201cstrong classifier\u201d\n####  \u2192 The weak learners in AdaBoost are decision trees with a single split, called decision stumps.\n#### \u2192 AdaBoost works by putting more weight on difficult to classify instances and less on those already handled well.\n#### \u2192 AdaBoost algorithms can be used for both classification and regression problem.","f11ed18c":"## 2- Gradient Boosting Regression\n#### Gradient Boosting trains many models in a gradual, additive and sequential manner. The major difference between AdaBoost and Gradient Boosting Algorithm is how the two algorithms identify the shortcomings of weak learners (eg. decision trees).\n#### While the AdaBoost model identifies the shortcomings by using high weight data points, gradient boosting performs the same by using gradients in the loss function (y=ax+b+e , e needs a special mention as it is the error term). The loss function is a measure indicating how good are model\u2019s coefficients are at fitting the underlying data.","5092264c":"### Quick visualization:","6911dca4":"source1 : https:\/\/towardsdatascience.com\/understanding-gradient-boosting-machines-9be756fe76ab\n\nsource2: https:\/\/towardsdatascience.com\/understanding-adaboost-2f94f22d5bfe","a0065eb4":"### 1- Importing data and libraries","69442c4d":"## When nothing works, Boosting does. Nowadays many people use either XGBoost or LightGBM or CatBoost to win competitions at Kaggle or Hackathons. AdaBoost is the first stepping stone in the world of Boosting.\n## So here in this notebook I will take about Adaboost and Gradient Boost.\n","984d7967":"### To see the normal distribution between y_test and prediction\n#### If it a correct model it will normally distributed with y_test","67891529":"### So that in our case Gradiet Boosting is better than AdaBoost!","1a421bee":"### To see the normal distribution between y_test and prediction\n#### If it a correct model it will normally distributed with y_test"}}