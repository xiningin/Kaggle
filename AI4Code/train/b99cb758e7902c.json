{"cell_type":{"4bd2c5cc":"code","cf7c197a":"code","8b8f17e4":"code","34c99d8f":"code","a758c476":"code","2b88c777":"code","cc74a584":"code","7d80c8f1":"code","7ebda0e3":"code","1d8625cf":"code","888e935a":"code","1160ca69":"code","8023515c":"code","95550d0d":"code","fce5d893":"code","ae3c485f":"code","bf55e889":"code","3cbb524a":"code","77dbb995":"code","9c1e0ed1":"code","bc08b549":"code","e500a84c":"code","aa8d4fd4":"code","6ed7a5c9":"code","a0362f07":"code","c100400d":"code","448f29da":"code","09dba119":"code","9a922aa3":"code","e5661ece":"code","cd2bbf71":"code","1af86ba3":"markdown","5d33681b":"markdown","32f46d15":"markdown","43e1d1c2":"markdown","3ce02e2e":"markdown","4d347e9c":"markdown","7ac8ebe7":"markdown","8fd30846":"markdown","c1ed67bf":"markdown","18372415":"markdown"},"source":{"4bd2c5cc":"#### Load packages\nimport pandas as pd\nimport numpy as np\nimport gc\n\n#### Read the training data\nfrom kaggle.competitions import twosigmanews\n\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()","cf7c197a":"(market_train_df, news_train_df) = env.get_training_data()","8b8f17e4":"market_train_df.head()","34c99d8f":"print(\" ---------------- Before removing missing values\")\nprint(market_train_df.isna().sum())\n\n# Remove missing values\nmarket_train_df.dropna(inplace= True)\n\nprint(\" \\n ---------------- After removing missing values \")\nprint(market_train_df.isna().sum())","a758c476":"market_train_df_1 = market_train_df[market_train_df.time.dt.year >= 2013]","2b88c777":"print(\"No. of observations (2013 to 2016): \", market_train_df_1.shape[0])\nprint(market_train_df_1.time.dt.year.value_counts())","cc74a584":"del market_train_df\ngc.collect()","7d80c8f1":"#Subset news data\nnews_train_df_1 = news_train_df[news_train_df.time.dt.year >= 2013].copy()","7ebda0e3":"del news_train_df\ngc.collect()","1d8625cf":"news_train_df_1.time.dt.year.value_counts()","888e935a":"news_train_df_1.head()","1160ca69":"news_train_df_1.info()","8023515c":"news_var = ['time','assetName', 'bodySize','companyCount','sentenceCount','wordCount',\n                                  'firstMentionSentence','relevance','sentimentClass','sentimentNegative',\n                                   'sentimentNeutral','sentimentPositive','sentimentWordCount','noveltyCount12H',\n                                   'noveltyCount24H','noveltyCount3D','noveltyCount5D','noveltyCount7D',\n                                   'volumeCounts12H','volumeCounts24H','volumeCounts3D','volumeCounts5D','volumeCounts7D'\n                                  ]","95550d0d":"news_train_df_1 = news_train_df_1[news_var]\n\nnews_train_df_1['date'] = news_train_df_1.time.dt.date","fce5d893":"news_train_df_1.groupby(['date','assetName']).size().head(15)","ae3c485f":"#Group to get day & assetName level data \nnews_train_df_grp = news_train_df_1.groupby(['date','assetName']).mean().reset_index()","bf55e889":"market_train_df_1['date'] = market_train_df_1.time.dt.date\n\nmarket_train_df_1 = pd.merge(market_train_df_1,news_train_df_grp,how='left',on = ['assetName','date'])\nmarket_train_df_1.head()","3cbb524a":"market_train_df_1.isna().sum()\/market_train_df_1.shape[0]","77dbb995":"#Fill 0 for NA's in News data\nmarket_train_df_1.fillna(0,inplace=True)","9c1e0ed1":"del news_train_df_grp\ngc.collect()","bc08b549":"#Find the correlations\ncorr_1 = market_train_df_1.corr()\n\nprint(corr_1['returnsOpenNextMktres10'].sort_values(ascending = False))\ndel corr_1","e500a84c":"#Removing rows with universe 0\nmarket_train_df_1 = market_train_df_1[market_train_df_1.universe == 1].copy()","aa8d4fd4":"# Train\/test split\nid_train = market_train_df_1.time.dt.year != 2016\nid_test = market_train_df_1.time.dt.year == 2016\n\ndep_var = 'returnsOpenNextMktres10'\nind_var = ['volume', 'close', 'open', 'returnsClosePrevRaw1',\n       'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n       'returnsOpenPrevMktres1', 'returnsClosePrevRaw10',\n       'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n       'returnsOpenPrevMktres10','bodySize', 'companyCount', 'sentenceCount', 'wordCount',\n       'firstMentionSentence', 'relevance', 'sentimentClass',\n       'sentimentNegative', 'sentimentNeutral', 'sentimentPositive',\n       'sentimentWordCount', 'noveltyCount12H', 'noveltyCount24H',\n       'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', 'volumeCounts12H',\n       'volumeCounts24H', 'volumeCounts3D', 'volumeCounts5D',\n       'volumeCounts7D']\n\ndf_train = market_train_df_1.loc[id_train,ind_var]\ndf_test = market_train_df_1.loc[id_test,ind_var]\n\nprint(\"{0} training rows and {1} testing rows\".format(df_train.shape[0],df_test.shape[0]))\n\n\ny_train = market_train_df_1.loc[id_train,dep_var]\ny_test = market_train_df_1.loc[id_test,dep_var]","6ed7a5c9":"import xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\n\n#-------------- XGboost (untuned)\nxg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 100)\n\nxg_reg.fit(df_train,y_train)","a0362f07":"import matplotlib.pyplot as plt\nxgb.plot_importance(xg_reg,max_num_features = 15)\nplt.rcParams['figure.figsize'] = [5, 5]\nplt.show()","c100400d":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\npred_train = xg_reg.predict(df_train)\nrms_train = sqrt(mean_squared_error(y_train, pred_train))\n\npred_test = xg_reg.predict(df_test)\nrms_test = sqrt(mean_squared_error(y_test, pred_test))\n\nprint('Train RMSE: {0} Test RMSE: {1}'.format(rms_train,rms_test))","448f29da":"pred_test_df = market_train_df_1.loc[id_test,['time','assetCode','universe','returnsOpenNextMktres10']]\npred_test_df['dayofyear'] = pred_test_df.time.dt.dayofyear\npred_test_df['confidence'] = [1 if pred >=0 else -1 for pred in pred_test]\npred_test_df['score'] = pred_test_df.universe * pred_test_df.returnsOpenNextMktres10 * pred_test_df.confidence\nprint(pred_test_df.confidence.value_counts())\n\nscore_1 = pred_test_df.groupby(['dayofyear']).score.sum()\nscore_2 = score_1.mean()\/ score_1.std()\nprint(\"\\n Competition Score: \",np.round(score_2,4))","09dba119":"# You can only iterate through a result from `get_prediction_days()` once \n# so be careful not to lose it once you start iterating.\ndays = env.get_prediction_days()","9a922aa3":"def make_predictions(market_obs_df,news_obs_df,predictions_df,ind_var,news_var,xg_reg):\n    \n    #Process news data\n    news_obs_df = news_obs_df.loc[:,news_var]\n    news_obs_df['date'] = news_obs_df.time.dt.date\n    news_train_df_grp = news_obs_df.groupby(['date','assetName']).mean().reset_index()\n    \n    #Merge the market and news data\n    market_obs_df['date'] = market_obs_df.time.dt.date\n    market_obs_df = pd.merge(market_obs_df,news_train_df_grp,how='left',on = ['assetName','date'])\n\n    #Fill 0 for NA's in News data\n    market_obs_df.fillna(0,inplace = True)\n    test = market_obs_df.loc[:,ind_var]\n    predictions_df.confidenceValue = [1 if  pred >=0 else -1 for pred in xg_reg.predict(test)]","e5661ece":"for (market_obs_df, news_obs_df, predictions_template_df) in days:\n    make_predictions(market_obs_df,news_obs_df,predictions_template_df,ind_var,news_var,xg_reg)\n    env.predict(predictions_template_df)\nprint('Done!')","cd2bbf71":"env.write_submission_file()","1af86ba3":"Selecting time, assetName and all the numeric variables","5d33681b":"\nIn a single day there can be multiple articles about an asset, so taking mean values of variables for now","32f46d15":"### Xgboost with only few years data\n* Building the model using only data from 2013 onwards, just checking how much the recent years data will help in making the prediction.\n<br\/>\n    * **Train:**  2013 to 2015\n    * **Test:** 2016\n\n* I'm using both marketing and news data, with no major feature engineering.\n* Using Xgboost for modeling, building it as an regression problem. For final submission, converting the final results to +1\/-1 based on their score.\n<br\/>\n<br\/>\nNote: This is just a simple model, there is lot of scope for improvement. This is my first public kernel, share your thoughts in the comments.\n","43e1d1c2":"Dropping the rows with missing values","3ce02e2e":"Merging  market and news data","4d347e9c":"Taking news data from 2013 onwards","7ac8ebe7":"#### Submisson creation","8fd30846":"Checking the competition metric","c1ed67bf":"Taking marketing data from 2013 onwards","18372415":"Create the submission file"}}