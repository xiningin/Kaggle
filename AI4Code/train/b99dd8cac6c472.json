{"cell_type":{"b1050371":"code","a55857e8":"code","93daa73c":"code","2572bdd7":"code","8664a8f4":"code","69d068f0":"code","37e57514":"code","e7b37aa6":"code","d1ff5ea7":"code","eb30e046":"code","2b7f0320":"code","066f9794":"code","32306c05":"code","1a0b0efd":"code","a070326d":"code","b7179ccf":"code","3d15ba65":"code","c4d768fc":"code","0f20d902":"code","2f3715b2":"code","fdcd125c":"code","01405629":"code","834be68e":"code","93bc8107":"code","502652e2":"code","8d2103d2":"code","4f7bde49":"code","75983e8e":"code","a40ba055":"code","3ce923d2":"code","90b981a1":"code","34113331":"code","c54215f6":"code","52c7f29c":"code","8fbae521":"code","638b592e":"code","ea07232e":"code","d7cdf57b":"markdown","84b466bd":"markdown","ed7ebc4f":"markdown","451a5134":"markdown","a71b9510":"markdown","14b1177a":"markdown","96e7ff8c":"markdown","0da34371":"markdown","681d6362":"markdown","c01c4138":"markdown","4a0eeb20":"markdown","6259103f":"markdown","8aad1eed":"markdown","236348bb":"markdown"},"source":{"b1050371":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport datetime\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom plotnine import *\nimport matplotlib.pyplot as plt\nimport plotnine\nimport lightgbm as lgb\nfrom mizani.breaks import date_breaks\n\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.","a55857e8":"train = pd.read_csv('..\/input\/train.csv', parse_dates =['first_active_month'])\ntest = pd.read_csv('..\/input\/test.csv', parse_dates =['first_active_month'])","93daa73c":"hist_trans = pd.read_csv('..\/input\/historical_transactions.csv', parse_dates=['purchase_date'])\nnew_trans = pd.read_csv('..\/input\/new_merchant_transactions.csv', parse_dates=['purchase_date'])\nmerchants = pd.read_csv('..\/input\/merchants.csv')","2572bdd7":"dfs = [train, test, hist_trans, new_trans, merchants]\nfor d in dfs:\n    print(d.shape)","8664a8f4":"train.describe(include='all')","69d068f0":"test.describe(include='all')","37e57514":"test['first_active_month'] = pd.to_datetime(test['first_active_month'].fillna('2017-09-01 00:00:00'))","e7b37aa6":"plotnine.options.figure_size = (14, 6)\nggplot(train, aes(x='target')) +\\\ngeom_histogram(bins=30, fill='blue', color='black') +\\\nscale_x_continuous(breaks=range(-40, 20, 10))","d1ff5ea7":"train['dayDiffActiveMonth'] = (train['first_active_month'] - pd.to_datetime(datetime.datetime.now().date())).dt.days\ntest['dayDiffActiveMonth'] = (test['first_active_month'] - pd.to_datetime(datetime.datetime.now().date())).dt.days","eb30e046":"plotnine.options.figure_size = (14, 6)\nggplot(train, aes(x='first_active_month')) +\\\ngeom_bar(fill='red', alpha=0.5, color='black') +\\\nscale_x_datetime(breaks=date_breaks('1 month')) +\\\ntheme(axis_text_x=element_text(rotation=90, hjust=1))","2b7f0320":"train.corr()","066f9794":"hist_trans.describe(include='all')","32306c05":"new_trans.describe(include='all')","1a0b0efd":"hist_trans.head()","a070326d":"hist_trans['authorized_flag'] = hist_trans['authorized_flag'].map({\"Y\" : 0, \"N\" : 1})\nnew_trans['authorized_flag'] = new_trans['authorized_flag'].map({\"Y\" : 0, \"N\" : 1})\n\nhist_trans['category_1'] = hist_trans['category_1'].map({\"Y\" : 0, \"N\" : 1})\nnew_trans['category_1'] = new_trans['category_1'].map({\"Y\" : 0, \"N\" : 1})","b7179ccf":"for c in ['authorized_flag', 'category_1','installments','month_lag','purchase_amount']:\n    hist_transt_amt_summ = hist_trans.groupby(['card_id']).agg({c : ['count', 'sum', 'mean', 'max', 'std']}).reset_index()\n    hist_transt_amt_summ.columns = ['card_id', 'hist_'+ c +'_cnt', 'hist_'+ c +'_sum', \n                                    'hist_'+ c +'_mean',  'hist_'+ c +'_max', 'hist_'+ c +'_std']\n    train = train.merge(hist_transt_amt_summ, how='left', on='card_id')\n    test = test.merge(hist_transt_amt_summ, how='left', on='card_id')\n    del(hist_transt_amt_summ)\n    \n    new_transt_amt_summ = new_trans.groupby(['card_id']).agg({c : ['count', 'sum', 'mean', 'max', 'std']}).reset_index()\n    new_transt_amt_summ.columns = ['card_id', 'new_'+ c +'_cnt', 'new_'+ c +'_sum', \n                                    'new_'+ c +'_mean',  'new_'+ c +'_max', 'new_'+ c +'_std']\n    train = train.merge(new_transt_amt_summ, how='left', on='card_id')\n    test = test.merge(new_transt_amt_summ, how='left', on='card_id')\n    del(new_transt_amt_summ)","3d15ba65":"#train.sort_values(by='target', ascending=True)\n#C_ID_a4e600deef, C_ID_7e285a535a\ntrain[train.card_id.isin(['C_ID_a4e600deef', 'C_ID_7e285a535a'])].head().T","c4d768fc":"ggplot(train, aes(x='hist_authorized_flag_mean', y='target')) +\\\ngeom_point(color='red', alpha=0.4, size=0.2)","0f20d902":"ggplot(train[train.hist_purchase_amount_sum<2000000], aes(x='hist_purchase_amount_sum', y='target')) +\\\ngeom_point(color='red', alpha=0.4, size=0.2)","2f3715b2":"train_corr = train.corr()","fdcd125c":"plt.figure(figsize=(8,13))\ntrain_corr['target'] = train_corr['target'].abs().sort_values(ascending=False)\ntrain_corr.drop('target', axis=0)['target'].plot(kind='barh')\nplt.show()","01405629":"train = train.fillna(0)\ntest = test.fillna(0)","834be68e":"X_train = train.drop(['target', 'first_active_month', 'card_id'], axis=1, inplace=False)\ny_train = train.target\nX_test = test.drop([ 'first_active_month', 'card_id'], axis=1, inplace=False)","93bc8107":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)","502652e2":"print(X_train.shape, X_val.shape, X_test.shape)","8d2103d2":"lr = LinearRegression()\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_val)\nnp.sqrt(mean_squared_error(y_val, y_pred)) #3.8346","4f7bde49":"dtr = DecisionTreeRegressor(max_depth = 6, min_samples_leaf=10)\ndtr.fit(X_train, y_train)\ny_pred = dtr.predict(X_val)\nnp.sqrt(mean_squared_error(y_val, y_pred)) # 3.8272","75983e8e":"'''for i in range(1,30):\n    knn = KNeighborsRegressor(n_neighbors=i, n_jobs=12)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    print(i, np.sqrt(mean_squared_error(y_test, y_pred)) )\n'''    ","a40ba055":"'''rfr = RandomForestRegressor(max_depth=6, n_estimators=150, min_samples_leaf=5)\nrfr.fit(X_train, y_train)\nrf_pred = rfr.predict(X_val)\nrf_train_pred = rfr.predict(X_train)\nprint(np.sqrt(mean_squared_error(y_val, rf_pred))) # 3.8027 #3.854 on LB\nprint(np.sqrt(mean_squared_error(y_train, rf_train_pred))) # 3.7342\n'''","3ce923d2":"train_columns = X_train.columns\nlgb_train = lgb.Dataset(X_train, y_train, feature_name=list(train_columns))\nlgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train, feature_name=list(train_columns))","90b981a1":"params = {\n    'objective': 'regression_l2',\n    'metric': { 'rmse'},\n    'num_leaves': 2000,\n    'learning_rate': 0.1,\n    'feature_fraction': 1,\n    'verbose': 0,\n    'max_depth' : 6,\n    'min_data_in_leaf' : 8\n}","34113331":"gbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=200000,\n                valid_sets=lgb_eval,\n                early_stopping_rounds=50) #3.76446","c54215f6":"test_lgbm_pred = gbm.predict(X_test, num_iteration = gbm.best_iteration)","52c7f29c":"submission = pd.DataFrame(test_lgbm_pred, index=X_test.index)\nsubmission.columns=['target']\nsubmission = pd.concat([test.card_id, submission], axis=1)\nsubmission.to_csv('submission_lgbm.csv', index=False)","8fbae521":"lgb_fi = pd.DataFrame(gbm.feature_importance(), index=train_columns).reset_index()\nlgb_fi.columns = ['column','score']","638b592e":"plotnine.options.figure_size = (14, 20)\nggplot(lgb_fi, aes(x='column', y='score')) +\\\ngeom_bar(stat='identity') +\\\ntheme(axis_text_x=element_text(rotation=90, hjust=1)) +\\\nplotnine.coord_flip()","ea07232e":"lgb_fi.sort_values(by='score', ascending=False)","d7cdf57b":"Loading all libraries and reading files. We will also parse dates while reading through files.","84b466bd":"**Random Forest Regression:**\n\nAgain with some of my favorite parameters for first submission","ed7ebc4f":"Let's run few algorithams with default\/my favorite parameters\n\n**Linear Regression:**","451a5134":"Now let's look at pearson co-relation between variables.","a71b9510":"Both historical and new merchant data got similar structure. In first phase we will just count number of transactions\nand average of amount. I just want to include numeric features into consideration for now for simplicity. I will add\ncategorical feature's details in later part of kernel(if needed)\n\nFollowing features seem numeric :  installments, month_lag, purchase_amount \n\nI would also like to convert to flags into number so we can consider mean of those binary features.","14b1177a":"Most of the values belong to range between -10 and 10.\n\nIt is also good to know whether age of card dependancy on target. For this we need to create new featuer by substracting current date from activation date. We will also explore first active month feature with simple bar chart.","96e7ff8c":"This shows our manufactured feature - \"dayDiffActiveMonth\" is having highest co-relation with target variable. However there are no strong co-relations. and we need to manufacture other features to make our model strong. So let's explore how we can use historical and new merchant transactions to create new features.","0da34371":"Let's have a look at shape","681d6362":"It looks like there is one null value for first activation column for test dataset. Let's fill it with most common date","c01c4138":"**KNN Regression:**","4a0eeb20":"Also I would like to explore train and test set with describe command. Describe is my favorite command and it gives really nice glimpse of data. ","6259103f":"Let's also look at target value distribution in test dataset","8aad1eed":"Creating validation set for cross validation. \n* train set size : 80%\n* validation set size : 20%","236348bb":"Decision Tree Regression:\nTaking max depth value as 5 and minimum sample leaf as 10. I almost always choose this parameter for first traila with decision tree based on my past experience."}}