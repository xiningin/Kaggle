{"cell_type":{"a992740e":"code","de03bb92":"code","7cffda10":"code","a581f930":"code","8cedfa05":"code","65ccbe10":"code","45d8b726":"code","157320f1":"code","bea89885":"code","1a896748":"code","4c150948":"code","48774cfa":"code","425897c5":"code","6901358e":"code","4a3de475":"code","55199c29":"code","d8db6ae7":"code","d18cc792":"code","1d9f3ae7":"code","c529e326":"code","8358d527":"code","aac6c489":"code","505e3a95":"code","ec89e3cd":"code","6e0025f8":"code","b6b3f0f6":"code","db97530e":"code","2e4e1346":"code","b73e2bd5":"markdown","c34a48ab":"markdown","8499dded":"markdown","2fc3393f":"markdown","47ac56a4":"markdown","bba89040":"markdown","d695cc13":"markdown","c8b010d1":"markdown","705d7c2c":"markdown","4fc6ccb6":"markdown"},"source":{"a992740e":"import pandas as pd\nimport numpy\nimport cv2\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split","de03bb92":"path = '..\/input\/chinese-mnist\/data\/data\/'\n\ndf = pd.read_csv('..\/input\/chinese-mnist\/chinese_mnist.csv')\ndf.head()","7cffda10":"char_values = numpy.unique(df['character'].values)\nidx_to_character = {i:c for i,c in enumerate(char_values)}\ncharacter_to_idx = {c:i for i,c in enumerate(char_values)}","a581f930":"# install d2l for visaulization\n!pip install --quiet d2l","8cedfa05":"from d2l import mxnet as d2l\nfrom mxnet import autograd, gluon, init, lr_scheduler, np, npx\nimport mxnet as mx\nfrom mxnet.gluon import nn\nnpx.set_np()","65ccbe10":"# index extracted: suite_id: 1, sample_id: 3, code: 4\n# resulted file name: input_1_3_4.jpg\nfeatures = []\nlabels = []\n\nfor i in range(len(df)):\n    image_path = path + 'input_' + str(df.iloc[i][0]) + \"_\" + str(df.iloc[i][1]) + \"_\" + str(df.iloc[i][2]) + \".jpg\"\n    image_arr = cv2.imread(image_path)\n    features.append(image_arr)\n    labels.append(character_to_idx[df.iloc[i]['character']])\n    \nfeatures = np.array(features)\nlabels = np.array(labels)","45d8b726":"# shuffle\nfeatures, labels = shuffle(features, labels, random_state=0)","157320f1":"# print one label and the image\nprint('Label:', idx_to_character[int(labels[0])])\n\nImage.fromarray(features[0].asnumpy().astype('uint8'), 'RGB')","bea89885":"# normalize\nfeatures = features \/ 255.","1a896748":"# move depth\nfeatures = np.moveaxis(features, 3, 1)","4c150948":"# split between train and test\nX_train, y_train = features[:12000], labels[:12000]\nX_test, y_test = features[12000:], labels[12000:]","48774cfa":"dataset_train = mx.gluon.data.dataset.ArrayDataset(X_train, y_train)\ntrain_iter = gluon.data.DataLoader(dataset_train, batch_size=128, shuffle=True, num_workers=2)\n\ndataset_test = mx.gluon.data.dataset.ArrayDataset(X_test, y_test)\ntest_iter = gluon.data.DataLoader(dataset_test, batch_size=128, shuffle=True, num_workers=2)","425897c5":"net = nn.HybridSequential()\nnet.add(nn.Conv2D(channels=6, kernel_size=5, padding=2, activation='relu'),\n        nn.MaxPool2D(pool_size=2, strides=2),\n        nn.Conv2D(channels=16, kernel_size=5, activation='relu'),\n        nn.MaxPool2D(pool_size=2, strides=2),\n        nn.Dense(120, activation='relu'),\n        nn.Dense(84, activation='relu'),\n        nn.Dense(10))\nnet.hybridize()","6901358e":"loss = gluon.loss.SoftmaxCrossEntropyLoss()\ndevice = d2l.try_gpu()","4a3de475":"def train(net, train_iter, test_iter, num_epochs, loss, trainer, device):\n    net.initialize(force_reinit=True, ctx=device, init=init.Xavier())\n    animator = d2l.Animator(xlabel='epoch', xlim=[0, num_epochs],\n    legend=['train loss', 'train acc', 'test acc'])\n    for epoch in range(num_epochs):\n        metric = d2l.Accumulator(3) # train_loss, train_acc, num_examples\n        for i, (X, y) in enumerate(train_iter):\n            X, y = X.as_in_ctx(device), y.as_in_ctx(device)\n            with autograd.record():\n                y_hat = net(X)\n                l = loss(y_hat, y)\n            l.backward()\n            trainer.step(X.shape[0])\n            metric.add(l.sum(), d2l.accuracy(y_hat, y), X.shape[0])\n            train_loss = metric[0] \/ metric[2]\n            train_acc = metric[1] \/ metric[2]\n            if (i + 1) % 50 == 0:\n                animator.add(epoch + i \/ len(train_iter),\n                            (train_loss, train_acc, None))\n        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n        animator.add(epoch + 1, (None, None, test_acc))\n    print(f'train loss {train_loss:.3f}, train acc {train_acc:.3f}, '\n          f'test acc {test_acc:.3f}')","55199c29":"lr, num_epochs = 0.3, 30\nnet.initialize(force_reinit=True, ctx=device, init=init.Xavier())\ntrainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})","d8db6ae7":"train(net, train_iter, test_iter, num_epochs, loss, trainer, device)","d18cc792":"class SquareRootScheduler:\n    def __init__(self, lr=0.1):\n        self.lr = lr\n    def __call__(self, num_update):\n        return self.lr * pow(num_update + 1.0, -0.5)","1d9f3ae7":"trainer.set_learning_rate(0.1)\nprint(f'learning rate is now {trainer.learning_rate:.2f}')","c529e326":"scheduler = SquareRootScheduler(lr=0.1)\nd2l.plot(np.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])","8358d527":"trainer = gluon.Trainer(net.collect_params(), 'sgd',\n                        {'lr_scheduler': scheduler})\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device)","aac6c489":"class FactorScheduler:\n    def __init__(self, factor=1, stop_factor_lr=1e-7, base_lr=0.1):\n        self.factor = factor\n        self.stop_factor_lr = stop_factor_lr\n        self.base_lr = base_lr\n\n    def __call__(self, num_update):\n        self.base_lr = max(self.stop_factor_lr, self.base_lr * self.factor)\n        return self.base_lr","505e3a95":"scheduler = FactorScheduler(factor=0.9, stop_factor_lr=1e-2, base_lr=2.0)\nd2l.plot(np.arange(50), [scheduler(t) for t in range(50)])","ec89e3cd":"trainer = gluon.Trainer(net.collect_params(), 'sgd',\n                        {'lr_scheduler': scheduler})\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device)","6e0025f8":"scheduler = lr_scheduler.MultiFactorScheduler(step=[15, 30], factor=0.5,\n                                              base_lr=0.5)\nd2l.plot(np.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])","b6b3f0f6":"trainer = gluon.Trainer(net.collect_params(), 'sgd',\n                        {'lr_scheduler': scheduler})\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device)","db97530e":"scheduler = lr_scheduler.CosineScheduler(max_update=20, base_lr=0.3,\n                                         final_lr=0.01)\nd2l.plot(np.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])","2e4e1346":"trainer = gluon.Trainer(net.collect_params(), 'sgd',\n                        {'lr_scheduler': scheduler})\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device)","b73e2bd5":"# Definition\n\n\nSchedulers are used to adjust the learning rate explicitly each learning step. This is conveniently\nachieved by the set_learning_rate method. We could adjust it downward after every epoch (or\neven after every minibatch), e.g., in a dynamic manner in response to how optimization is pro-\ngressing.","c34a48ab":"# Training without Scheduler","8499dded":"## Multi Factor Scheduler","2fc3393f":"## Factor Scheduler","47ac56a4":"## Square Root Scheduler","bba89040":"# Training with Schedulers","d695cc13":"# Prepare Data","c8b010d1":"## Cosine Scheduler","705d7c2c":"# Model","4fc6ccb6":"# Schedulers for beginners using Chinesse MNIST"}}