{"cell_type":{"4b58d601":"code","8a2ae50a":"code","3dc1e56b":"code","ce4f5aa8":"code","771241cc":"code","d9b94e01":"code","c6d26a1b":"code","fe989f46":"code","a664a86b":"code","da749832":"code","ad66d3b5":"code","83182754":"code","4313f6f5":"code","742c5e69":"code","73083c21":"code","30dadf29":"code","2d8209c8":"code","93fe1d72":"code","d54a3c92":"code","9d4a8b1a":"code","72b06507":"code","e796dac6":"code","824878f9":"code","cbe068d4":"code","f3331b4e":"code","b4d7b8a1":"code","2ddf06be":"code","8ba580be":"code","11214ac9":"code","2d9ce50f":"code","f09ff621":"code","a8216bd6":"code","3a91b59e":"code","fb170c33":"code","251cdd03":"code","ab96f165":"code","6c6575e2":"code","fc8fe4f7":"code","f56e2469":"code","8a6db3f2":"code","97cbd860":"code","02178431":"code","b8ea6429":"code","6a14dc91":"code","fb7ef571":"code","0ee3f47b":"code","a36fec70":"code","365710ee":"markdown","707bb4f3":"markdown","ded43e5a":"markdown"},"source":{"4b58d601":"!pip install ..\/input\/datrie-python-prefix-tree\/datrie-0.8.2-cp37-cp37m-manylinux1_x86_64.whl","8a2ae50a":"!pip install ..\/input\/efficientnetkerasapplications\/Keras_Applications-1.0.8-py3-none-any.whl\n!pip install ..\/input\/efficientnetkerasapplications\/efficientnet-1.1.1-py3-none-any.whl\nimport sys\nsys.path.append('..\/input\/timm-pytorch-image-models\/pytorch-image-models-master')\nimport numpy as np\nimport pandas as pd\nimport string\nimport datrie\nimport gc\nimport matplotlib.pyplot as plt\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml import PCA\nfrom cuml.neighbors import NearestNeighbors\nimport tensorflow as tf\nimport efficientnet.tfkeras as efn\nimport math\nfrom shutil import copyfile\ncopyfile(src = \"..\/input\/bert-and-tokenization\/tokenization.py\", dst = \"..\/working\/tokenization.py\")\nimport tokenization\nimport tensorflow_hub as hub\n\nimport os\nimport cv2\nimport random\nfrom tqdm import tqdm\n\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport timm\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\nimport fasttext as ft","3dc1e56b":"fullWordsSet = [] \ntrie = [] \njointReplacements = []\nfuzzRepDct = []\n\ndef stringReplacer(st):\n    st = st.replace('(','').replace(')','')\n    st = st.replace('[','').replace(']','')\n    st = st.replace('{','').replace('}','')\n    st = st.replace('!','')\n    st = st.replace('#',' ')\n    st = st.replace('-',' ')\n    st = st.replace(',',' ')\n    st = st.replace('\\\\',' ')\n    st = st.replace('\/',' ')\n    st = st.replace('|',' ')\n    st = st.replace(' THE ',' ')\n    st = st.replace('\"',' ')\n    st = st.replace(' X ',' ')\n    st = st.replace('=',' ')\n    \n    if len(st)>0:\n        if st[-1]=='.':\n            st = st[:-1]\n    \n    ########\n    \"\"\"\n    allSWs = ['ORIGINAL','&','1','EXTRA','PACKING','IMPORT','+','FREE','COD','SELLER','FASHION','SIZE','COLOR','NEW','FULL','FRESH','STOCK','PACK','BEST','REAL','MEMBERITAHU','WAS','MAYROON','EXTRA','BY','NOON AY','BE','MAY','APA','NYA','KAMI','KUNG PAANO','TEM','MEMILIKI','ONE','WHICH','KEPADA','O QUE','IT','TEMPO','ISANG','ILANG','TO','ARE','\u00c9','THIS','DE','OR','JIKA','SA','N\u00d3S','UMA','MAAARI','DAN','BAGAIMANA','S\u00c3O','PAMANAHAN','ON','AKO','PACKING','THAT','SAID','QUE','SE','SALITA','NG','AS','WITH','WERE','TIDAK','SEBUAH','FOR','PALAVRA','WE','IBANG','DO','KANYANG','AKAN','&','KUNG','DARIPADA','POR','UNTUK','FROM','MASING-MASING','THE','INI','MENJADI','SER','NGUNIT','DALAM','KUNG ANO','SAYA','HOW','NA','BAWAT','SEBAGAI','PACK','FASHION','CAN','DIZER','ANG','AY','MEMPUNYAI','ITU','GUMAGANA','MAS','OTHER','CADA','DOES','ATAU','TEVE','SABIHIN','ESTE','COLOR','SETIAP','SEORANG','OUTRO','SIYA','LAINNYA','MAGING','YANG','HAD','ORIGINAL','MY','SELLER','YOU','A PARTIR DE','IF','WHAT','DISSE','MELAKUKAN','PADA','OF','THEIR','COM','SOME','BILANG','DARI','KUNG SAAN','O','ANDA','AT','SINABI','QUENTE','VOC\u00ca','DI','KATA','TAPI','FAZ','LATA','FAZER','FOI','LAIN','PARA','COMO','ORAS','1','ANG KANILANG','OLEH','IMPORT','ITO','THEY','PARA SA','BAHWA','BEST','HAVE','SATU','STOCK','HE','IS','FORAM','SA PAMAMAGITAN NG','BERKATA','NEW','GAWIN',\"IT'S\",'PANAS','OU','NAGKAROON','DENGAN','KELUAR','MULA SA','E','NAMIN','ADALAH','MEREKA','HOT','FRESH','SEU','BEBERAPA','IA','A','TIME','AND','BUT','WILL','BOLEH','MASA','FULL','OUT','SILA','EACH','PERKATAAN','FORA','ISA','HIS','FREE','+','SIZE','I','REAL','ELE','MAINIT','AN','TELL','BISA','TETAPI','DIA','WAKTU','VONTADE','SA IYO','ELES','ALGUNS','EM','BELIAU','WORD','COD']\n    for sw in allSWs:\n        st = st.replace(' '+sw+' ',' ')\n    \"\"\"\n    #########\n    st = st.replace('    ','   ')\n    st = st.replace('   ','  ')\n    st = st.replace('  ',' ')\n    return st\n\ndef cleanSlashX(st):\n    st2 = ''\n    i=0\n\n    while i<len(st):\n        if st[i:i+2]=='\\X':\n            i+=4\n        else:\n            st2 += st[i]\n            i+=1\n    return st2\n\ndef cleanup_text(text):\n    text = cleanSlashX(text)\n    text = stringReplacer(text)\n    text = \"\".join([c if ord(c) < 128 else \"\" for c in text]).strip() \n    return text\n\n\ndef stringReplacer_Joined(st):\n    global jointReplacements\n    for k,v in jointReplacements.items():\n        if k in st:\n            st = st.replace(k,v)\n\n    return st\n\ndef stringReplacer_Fuzzy(st):\n    global fuzzRepDct\n    for k,v in fuzzRepDct.items():\n        if k in st:\n            st = st.replace(k,v)\n\n    return st","ce4f5aa8":"def tokenizeTitle(ttl):\n    tl = ttl.split(' ')\n    tl = [wd for wd in tl if len(wd)>0]\n    #return set(tl[:30])\n    return set(tl)\n\ndef read_dataset():\n    global fullWordsSet, trie, jointReplacements, fuzzRepDct\n    \n    df = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\n    \n    df['title'] = df['title'].str.upper()\n    df['title'] = df['title'].apply(cleanup_text)\n    df['tokens'] = df['title'].apply(tokenizeTitle)\n\n    fullWordsSet = set()\n    for i in tqdm(range(df.shape[0])):\n        for wrd in df['tokens'].iloc[i]:\n            if len(wrd) > 2:\n                if not any(char.isdigit() for char in wrd):\n                    fullWordsSet.add(wrd)\n    fullWordsSet = list(fullWordsSet)\n    \n    trie = datrie.Trie(string.ascii_uppercase)\n    for i,wrd in tqdm(enumerate(fullWordsSet)):\n        trie[wrd] = i\n        \n    jointCnt = 0\n    jointReplacements = {}\n    for i,wrd in tqdm(enumerate(fullWordsSet)):\n        prefxs = trie.prefixes(wrd)\n        for j,wrd2 in enumerate(prefxs):\n            if wrd2 != wrd:\n                wrd3 = wrd[ len(wrd2): ]\n                if wrd3 in trie:\n                    #print(wrd2,' + ',wrd3,' = ',wrd)\n                    #print('*******')\n                    jointCnt += 1\n                    jointReplacements[ wrd2+' '+wrd3 ] = wrd\n    \n    \n    df['title'] = df['title'].apply(stringReplacer_Joined)\n    \n    df['tokens'] = df['title'].apply(tokenizeTitle)\n    \n    fullWordsSet = set()\n    for i in tqdm(range(df.shape[0])):\n        for wrd in df['tokens'].iloc[i]:\n            if len(wrd) > 2:\n                if not any(char.isdigit() for char in wrd):\n                    fullWordsSet.add(wrd)\n    fullWordsSet = list(fullWordsSet)\n    \n    azDict = {}\n\n    for wrd in tqdm(fullWordsSet):\n        if len(wrd) > 1:\n            ak = wrd[0]\n            zk = wrd[-1]\n            azk = ak+zk\n            if azk not in azDict:\n                azDict[azk] = []\n            else:\n                azDict[azk].append(wrd)\n                \n    #1,6 1,7 2,7 2,8 1,8\n    mmMax = 1\n    minLen = 6\n    fuzzRepDct = {}\n\n    for k,v in tqdm(azDict.items()):\n        L = v\n        used = []\n        grps = []\n        for w in L:\n            if w not in used:\n                grp = []\n                for w2 in L:\n                    if len(w)>=minLen and len(w)==len(w2):\n                        mm = 0\n                        for i,c in enumerate(w):\n                            if c != w2[i]:\n                                mm += 1\n                        if mm<=mmMax:\n                            grp.append(w2)\n                            used.append(w2)\n                if len(grp)>1:\n                    grps.append(grp)   \n                    pvt = grp[0]\n                    for n in range(1,len(grp)):\n                        fuzzRepDct[ grp[n] ] = pvt\n                        \n    df['title'] = df['title'].apply(stringReplacer_Fuzzy)\n    \n    del df['tokens']\n    \n    df_cu = cudf.DataFrame(df)\n    image_paths = '..\/input\/shopee-product-matching\/test_images\/' + df['image']\n    return df, df_cu, image_paths\n\ndef read_datasetTrain():\n    global fullWordsSet, trie, jointReplacements, fuzzRepDct\n    df = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\n    \n    df['title'] = df['title'].str.upper()\n    df['title'] = df['title'].apply(cleanup_text)\n    df['tokens'] = df['title'].apply(tokenizeTitle)\n    \n    fullWordsSet = set()\n    for i in tqdm(range(df.shape[0])):\n        for wrd in df['tokens'].iloc[i]:\n            if len(wrd) > 2:\n                if not any(char.isdigit() for char in wrd):\n                    fullWordsSet.add(wrd)\n    fullWordsSet = list(fullWordsSet)\n    \n    trie = datrie.Trie(string.ascii_uppercase)\n    for i,wrd in tqdm(enumerate(fullWordsSet)):\n        trie[wrd] = i\n        \n    jointCnt = 0\n    jointReplacements = {}\n    for i,wrd in tqdm(enumerate(fullWordsSet)):\n        prefxs = trie.prefixes(wrd)\n        for j,wrd2 in enumerate(prefxs):\n            if wrd2 != wrd:\n                wrd3 = wrd[ len(wrd2): ]\n                if wrd3 in trie:\n                    #print(wrd2,' + ',wrd3,' = ',wrd)\n                    #print('*******')\n                    jointCnt += 1\n                    jointReplacements[ wrd2+' '+wrd3 ] = wrd\n    \n    \n    df['title'] = df['title'].apply(stringReplacer_Joined)\n    \n    \n    df['tokens'] = df['title'].apply(tokenizeTitle)\n    \n    fullWordsSet = set()\n    for i in tqdm(range(df.shape[0])):\n        for wrd in df['tokens'].iloc[i]:\n            if len(wrd) > 2:\n                if not any(char.isdigit() for char in wrd):\n                    fullWordsSet.add(wrd)\n    fullWordsSet = list(fullWordsSet)\n    \n    azDict = {}\n\n    for wrd in tqdm(fullWordsSet):\n        if len(wrd) > 1:\n            ak = wrd[0]\n            zk = wrd[-1]\n            azk = ak+zk\n            if azk not in azDict:\n                azDict[azk] = []\n            else:\n                azDict[azk].append(wrd)\n                \n    #1,6 1,7 2,7 2,8 1,8\n    mmMax = 1\n    minLen = 8\n    fuzzRepDct = {}\n\n    for k,v in tqdm(azDict.items()):\n        L = v\n        used = []\n        grps = []\n        for w in L:\n            if w not in used:\n                grp = []\n                for w2 in L:\n                    if len(w)>=minLen and len(w)==len(w2):\n                        mm = 0\n                        for i,c in enumerate(w):\n                            if c != w2[i]:\n                                mm += 1\n                        if mm<=mmMax:\n                            grp.append(w2)\n                            used.append(w2)\n                if len(grp)>1:\n                    grps.append(grp)   \n                    pvt = grp[0]\n                    for n in range(1,len(grp)):\n                        fuzzRepDct[ grp[n] ] = pvt\n                        \n    df['title'] = df['title'].apply(stringReplacer_Fuzzy)\n    \n    del df['tokens']\n    \n    df_cu = cudf.DataFrame(df)\n    image_paths = '..\/input\/shopee-product-matching\/train_images\/' + df['image']\n    return df, df_cu, image_paths","771241cc":"# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Configuration\nBATCH_SIZE = 8\nIMAGE_SIZE = [512, 512]\n# Seed\nSEED = 42\n# Verbosity\nVERBOSE = 1\n# Number of classes\nN_CLASSES = 11011","d9b94e01":"# RESTRICT TENSORFLOW TO 2GB OF GPU RAM\n# SO THAT WE HAVE 14GB RAM FOR RAPIDS\nLIMIT = 2.0\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_virtual_device_configuration(\n            gpus[0],\n            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        #print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n        print(e)\nprint('We will restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\nprint('then RAPIDS can use %iGB GPU RAM'%(16-LIMIT))","c6d26a1b":"GET_CV = False\n# Flag to check ram allocations (debug)\nCHECK_SUB = False\n\ndf = cudf.read_csv('..\/input\/shopee-product-matching\/test.csv')\n# If we are comitting, replace train set for test set and dont get cv\nif len(df) > 3:\n    GET_CV = False\ndel df\n\n# Function to get our f1 score\ndef f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection \/ (len_y_pred + len_y_true)\n    return f1\n\n\n# Function to combine predictions\ndef combine_predictions(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions'], row['oof_text'], row['oof_hash']])\n    return ' '.join( np.unique(x) )\n\n\"\"\"\n# Function to read out dataset\ndef read_dataset():\n    if GET_CV:\n        df = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\n        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n        df['matches'] = df['label_group'].map(tmp)\n        df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n        if CHECK_SUB:\n            df = pd.concat([df, df], axis = 0)\n            df.reset_index(drop = True, inplace = True)\n        df_cu = cudf.DataFrame(df)\n        image_paths = '..\/input\/shopee-product-matching\/train_images\/' + df['image']\n    else:\n        df = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\n        df_cu = cudf.DataFrame(df)\n        image_paths = '..\/input\/shopee-product-matching\/test_images\/' + df['image']\n        \n    return df, df_cu, image_paths\n\n\"\"\"\n# Function to decode our images\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels = 3)\n    image = tf.image.resize(image, IMAGE_SIZE)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    return image\n\n# Function to read our test image and return image\ndef read_image(image):\n    image = tf.io.read_file(image)\n    image = decode_image(image)\n    return image\n\n# Function to get our dataset that read images\ndef get_dataset(image):\n    dataset = tf.data.Dataset.from_tensor_slices(image)\n    dataset = dataset.map(read_image, num_parallel_calls = AUTO)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset","fe989f46":"# Arcmarginproduct class keras layer\nclass ArcMarginProduct(tf.keras.layers.Layer):\n    '''\n    Implements large margin arc distance.\n\n    Reference:\n        https:\/\/arxiv.org\/pdf\/1801.07698.pdf\n        https:\/\/github.com\/lyakaap\/Landmark2019-1st-and-3rd-Place-Solution\/\n            blob\/master\/src\/modeling\/metric_learning.py\n    '''\n    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n                 ls_eps=0.0, **kwargs):\n\n        super(ArcMarginProduct, self).__init__(**kwargs)\n\n        self.n_classes = n_classes\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps\n        self.easy_margin = easy_margin\n        self.cos_m = tf.math.cos(m)\n        self.sin_m = tf.math.sin(m)\n        self.th = tf.math.cos(math.pi - m)\n        self.mm = tf.math.sin(math.pi - m) * m\n        \n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'n_classes': self.n_classes,\n            's': self.s,\n            'm': self.m,\n            'ls_eps': self.ls_eps,\n            'easy_margin': self.easy_margin,\n        })\n        return config\n\n    def build(self, input_shape):\n        super(ArcMarginProduct, self).build(input_shape[0])\n\n        self.W = self.add_weight(\n            name='W',\n            shape=(int(input_shape[0][-1]), self.n_classes),\n            initializer='glorot_uniform',\n            dtype='float32',\n            trainable=True,\n            regularizer=None)\n    \n    def call(self, inputs):\n        X, y = inputs\n        y = tf.cast(y, dtype=tf.int32)\n        cosine = tf.matmul(\n            tf.math.l2_normalize(X, axis=1),\n            tf.math.l2_normalize(self.W, axis=0)\n        )\n        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = tf.where(cosine > 0, phi, cosine)\n        else:\n            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n        one_hot = tf.cast(\n            tf.one_hot(y, depth=self.n_classes),\n            dtype=cosine.dtype\n        )\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps \/ self.n_classes\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n        return output\n    \n    \n    ","a664a86b":"# Function to get the embeddings of our images with the fine-tuned model\ndef get_image_embeddings(image_paths):\n    embeds = []\n    \n    margin = ArcMarginProduct(\n            n_classes = N_CLASSES, \n            s = 30, \n            m = 0.5, \n            name='head\/arc_margin', \n            dtype='float32'\n            )\n\n    inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n    label = tf.keras.layers.Input(shape = (), name = 'inp2')\n    x = efn.EfficientNetB3(weights = None, include_top = False)(inp)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = margin([x, label])\n        \n    output = tf.keras.layers.Softmax(dtype='float32')(x)\n\n    model = tf.keras.models.Model(inputs = [inp, label], outputs = [output])\n    model.load_weights('..\/input\/efficientnet\/EfficientNetB3_512_42.h5')\n    model = tf.keras.models.Model(inputs = model.input[0], outputs = model.layers[-4].output)\n    chunk = 5000\n    iterator = np.arange(np.ceil(len(df) \/ chunk))\n    for j in iterator:\n        a = int(j * chunk)\n        b = int((j + 1) * chunk)\n        image_dataset = get_dataset(image_paths[a:b])\n        image_embeddings = model.predict(image_dataset)\n        embeds.append(image_embeddings)\n    del model\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return image_embeddings\n\n# Return tokens, masks and segments from a text array or series\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","da749832":"# Function to get our text title embeddings using a pre-trained bert model\ndef get_text_embeddings(df, max_len = 70):\n    embeds = []\n    module_url = \"..\/input\/bert-en-uncased-l24-h1024-a16-1\"\n    bert_layer = hub.KerasLayer(module_url, trainable = True)\n    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n    text = bert_encode(df['title'].values, tokenizer, max_len = max_len)\n    \n    margin = ArcMarginProduct(\n            n_classes = 11014, \n            s = 30, \n            m = 0.5, \n            name='head\/arc_margin', \n            dtype='float32'\n            )\n    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n    label = tf.keras.layers.Input(shape = (), name = 'label')\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    x = margin([clf_output, label])\n    output = tf.keras.layers.Softmax(dtype='float32')(x)\n    model = tf.keras.models.Model(inputs = [input_word_ids, input_mask, segment_ids, label], outputs = [output])\n    \n    model.load_weights('..\/input\/bert-and-tokenization\/Bert_123.h5')\n    model = tf.keras.models.Model(inputs = model.input[0:3], outputs = model.layers[-4].output)\n    chunk = 5000\n    iterator = np.arange(np.ceil(len(df) \/ chunk))\n    for j in iterator:\n        a = int(j * chunk)\n        b = int((j + 1) * chunk)\n        text_chunk = ((text[0][a:b], text[1][a:b], text[2][a:b]))\n        text_embeddings = model.predict(text_chunk, batch_size = BATCH_SIZE)\n        embeds.append(text_embeddings)\n    del model\n    text_embeddings = np.concatenate(embeds)\n    print(f'Our text embeddings shape is {text_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return text_embeddings","ad66d3b5":"# Function to get 50 nearest neighbors of each image and apply a distance threshold to maximize cv\ndef get_neighbors(df, embeddings, KNN = 50, image = True):\n    \n    if len(df) <= 3:\n        KNN = 3\n    \n    model = NearestNeighbors(n_neighbors = KNN)\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n    # Iterate through different thresholds to maximize cv, run this in interactive mode, then replace else clause with a solid threshold\n    if GET_CV:\n        if image:\n            thresholds = list(np.arange(4.5, 5.0, 0.1))\n        else:\n            thresholds = list(np.arange(31, 35, 1))\n        scores = []\n        for threshold in thresholds:\n            predictions = []\n            for k in range(embeddings.shape[0]):\n                idx = np.where(distances[k,] < threshold)[0]\n                ids = indices[k,idx]\n                posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n                predictions.append(posting_ids)\n            df['pred_matches'] = predictions\n            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n            score = df['f1'].mean()\n            print(f'Our f1 score for threshold {threshold} is {score}')\n            scores.append(score)\n        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n        best_threshold = max_score['thresholds'].values[0]\n        best_score = max_score['scores'].values[0]\n        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n        \n        # Use threshold\n        predictions = []\n        for k in range(embeddings.shape[0]):\n            ids = np.array([])\n            # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n            if image:\n                idx = np.where(distances[k,] < 3.3)[0]\n                ids = indices[k,idx]\n            else:\n                idx = np.where(distances[k,] < 20.0)[0]\n                ids = indices[k,idx]\n                if (len(idx)>1):\n                    arr = distances[k,np.where(distances[k,]<20)[0]][1:]\n                    mean = np.mean(arr)\n                    standard_deviation = np.std(arr)\n                    if(standard_deviation>0):\n                        distance_from_mean = abs(arr - mean)\n                        max_deviations = 2\n                        not_outlier = distance_from_mean < max_deviations * standard_deviation\n                        max_dist = arr[not_outlier][-1]\n                        idx = np.where(distances[k,] <= max_dist)[0]\n                        ids = indices[k,idx]\n            posting_ids = df['posting_id'].iloc[ids].values\n            predictions.append(posting_ids)\n    \n    # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n    else:\n        predictions = []\n        for k in tqdm(range(embeddings.shape[0])):\n            ids = np.array([])\n            if image:\n                idx = np.where(distances[k,] < 3.3)[0]\n                ids = indices[k,idx]\n            else:\n                idx = np.where(distances[k,] < 16.0)[0]\n                ids = indices[k,idx]\n                if (len(idx)>1):\n                    arr = distances[k,np.where(distances[k,]<16.0)[0]][1:]\n                    mean = np.mean(arr)\n                    standard_deviation = np.std(arr)\n                    if(standard_deviation>0):\n                        distance_from_mean = abs(arr - mean)\n                        max_deviations = 2\n                        not_outlier = distance_from_mean < max_deviations * standard_deviation\n                        max_dist = arr[not_outlier][-1]\n                        idx = np.where(distances[k,] <= max_dist)[0]\n                        ids = indices[k,idx]\n            \n            posting_ids = df['posting_id'].iloc[ids].values\n            predictions.append(posting_ids)\n        \n    del model, distances, indices\n    gc.collect()\n    return df, predictions","83182754":"#df,df_cu,image_paths = read_datasetTrain()\ndf,df_cu,image_paths = read_dataset()\ndf.head()","4313f6f5":"image_embeddings = get_image_embeddings(image_paths)\n","742c5e69":"text_embeddings = get_text_embeddings(df)\ngc.collect()","73083c21":"df, image_predictions = get_neighbors(df, image_embeddings, KNN = 27, image = True)","30dadf29":"df, text_predictions = get_neighbors(df, text_embeddings, KNN = 22, image = False)","2d8209c8":"from cuml.feature_extraction.text import TfidfVectorizer\n\nmodel = TfidfVectorizer(stop_words=None, binary=True, max_features=25000)\ntext_embeddings2 = model.fit_transform(df_cu.title).toarray()\nprint('text embeddings shape',text_embeddings2.shape)","93fe1d72":"preds = []\nCHUNK = 1024*4\n\nprint('Finding similar titles...')\nCTS = len(df_cu)\/\/CHUNK\nif len(df_cu)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(df_cu))\n    print('chunk',a,'to',b)\n    \n    # COSINE SIMILARITY DISTANCE\n    # cts = np.dot( text_embeddings, text_embeddings[a:b].T).T\n    cts = cupy.matmul(text_embeddings2, text_embeddings2[a:b].T).T\n    \n    for k in range(b-a):\n        IDX = np.where(cts[k,]>0.74)[0] #0.738\n        #IDX = cupy.where(cts[k,]>0.75)[0] # Orig\n        o = df_cu.iloc[cupy.asnumpy(IDX)].posting_id.to_pandas().values\n        preds.append(o)\n        \ndel model, text_embeddings2","d54a3c92":"df_cu['oof_text'] = preds","9d4a8b1a":"import albumentations as A \nfrom albumentations.pytorch.transforms import ToTensorV2","72b06507":"class CFG:\n    \n    img_size = 512\n    batch_size = 12\n    seed = 2020\n    \n    device = 'cuda'\n    classes = 11014\n    \n    model_name = 'eca_nfnet_l0'\n    model_path = '..\/input\/shopee-pytorch-models\/arcface_512x512_nfnet_l0 (mish).pt'\n    \n    scale = 30 \n    margin = 0.5","e796dac6":"def read_dataset():\n    df = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\n    df_cu = cudf.DataFrame(df)\n    image_paths = '..\/input\/shopee-product-matching\/test_images\/' + df['image']\n    return df, df_cu, image_paths\n\ndef read_datasetTrain():\n    df = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\n    df_cu = cudf.DataFrame(df)\n    image_paths = '..\/input\/shopee-product-matching\/train_images\/' + df['image']\n    return df, df_cu, image_paths","824878f9":"def seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_torch(CFG.seed)","cbe068d4":"def get_image_predictions(df, embeddings,threshold = 0.0):\n    \n    if len(df) > 3:\n        KNN = 50\n    else : \n        KNN = 3\n    \n    model = NearestNeighbors(n_neighbors = KNN, metric = 'cosine')\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n    predictions = []\n    for k in tqdm(range(embeddings.shape[0])):\n        idx = np.where(distances[k,] < threshold)[0]\n        ids = indices[k,idx]\n        posting_ids = df['posting_id'].iloc[ids].values\n        predictions.append(posting_ids)\n        \n    del model, distances, indices\n    gc.collect()\n    return predictions","f3331b4e":"def get_test_transforms():\n\n    return A.Compose(\n        [\n            A.Resize(CFG.img_size,CFG.img_size,always_apply=True),\n            A.Normalize(),\n        ToTensorV2(p=1.0)\n        ]\n    )","b4d7b8a1":"class ShopeeDataset(Dataset):\n    def __init__(self, image_paths, transforms=None):\n\n        self.image_paths = image_paths\n        self.augmentations = transforms\n\n    def __len__(self):\n        return self.image_paths.shape[0]\n\n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        \n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']       \n    \n        return image,torch.tensor(1)","2ddf06be":"class ArcMarginProduct_Image(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct_Image, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n        \n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps \/ self.out_features\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.scale\n        \n        return output","8ba580be":"class ShopeeModel(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = CFG.classes,\n        model_name = CFG.model_name,\n        fc_dim = 512,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = True,\n        pretrained = False):\n\n\n        super(ShopeeModel,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n        \n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n\n        if model_name == 'resnext50_32x4d':\n            final_in_features = self.backbone.fc.in_features\n            self.backbone.fc = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif model_name == 'efficientnet_b3':\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif model_name == 'tf_efficientnet_b5_ns':\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n            \n        elif model_name == 'eca_nfnet_l0':\n            final_in_features = self.backbone.head.fc.in_features\n            self.backbone.head.fc = nn.Identity()\n            self.backbone.head.global_pool = nn.Identity()\n\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n\n        self.use_fc = use_fc\n\n        self.dropout = nn.Dropout(p=0.0)\n        self.fc = nn.Linear(final_in_features, fc_dim)\n        self.bn = nn.BatchNorm1d(fc_dim)\n        self._init_params()\n        final_in_features = fc_dim\n\n        self.final = ArcMarginProduct_Image(\n            final_in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n        \n    def forward(self, image, label):\n        feature = self.extract_feat(image)\n        #logits = self.final(feature,label)\n        return feature\n\n    def extract_feat(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n        return x","11214ac9":"class Mish_func(torch.autograd.Function):\n    \n    \"\"\"from: https:\/\/github.com\/tyunist\/memory_efficient_mish_swish\/blob\/master\/mish.py\"\"\"\n    \n    @staticmethod\n    def forward(ctx, i):\n        result = i * torch.tanh(F.softplus(i))\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n  \n        v = 1. + i.exp()\n        h = v.log() \n        grad_gh = 1.\/h.cosh().pow_(2)\n        \n        # Note that grad_hv * grad_vx = sigmoid(x)\n        #grad_hv = 1.\/v  \n        #grad_vx = i.exp()\n        \n        grad_hx = i.sigmoid()\n\n        grad_gx = grad_gh *  grad_hx #grad_hv * grad_vx \n        \n        grad_f =  torch.tanh(F.softplus(i)) + i * grad_gx \n        \n        return grad_output * grad_f \n    \nclass Mish(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__()\n        pass\n    def forward(self, input_tensor):\n        return Mish_func.apply(input_tensor)\n    \n    \ndef replace_activations(model, existing_layer, new_layer):\n    \n    \"\"\"A function for replacing existing activation layers\"\"\"\n    \n    for name, module in reversed(model._modules.items()):\n        if len(list(module.children())) > 0:\n            model._modules[name] = replace_activations(module, existing_layer, new_layer)\n\n        if type(module) == existing_layer:\n            layer_old = module\n            layer_new = new_layer\n            model._modules[name] = layer_new\n    return model","2d9ce50f":"def get_image_embeddings1(image_paths, model_name = CFG.model_name):\n    embeds = []\n    \n    model = ShopeeModel(model_name = model_name)\n    model.eval()\n    \n    if model_name == 'eca_nfnet_l0':\n        model = replace_activations(model, torch.nn.SiLU, Mish())\n\n    model.load_state_dict(torch.load(CFG.model_path))\n    model = model.to(CFG.device)\n    \n\n    image_dataset = ShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n    image_loader = torch.utils.data.DataLoader(\n        image_dataset,\n        batch_size=CFG.batch_size,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=4\n    )\n    \n    with torch.no_grad():\n        for img,label in tqdm(image_loader): \n            img = img.cuda()\n            label = label.cuda()\n            feat = model(img,label)\n            image_embeddings = feat.detach().cpu().numpy()\n            embeds.append(image_embeddings)\n    \n    \n    del model\n    \n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return image_embeddings","f09ff621":"df_image,df_image_cu,image_paths = read_dataset()\n#df_image,df_image_cu,image_paths = read_datasetTrain()\ndf_image.head()","a8216bd6":"def findDifference(f1, f2):\n    return np.linalg.norm(f1-f2)\n \ndef cosine_distance(a, b):\n    if a.shape != b.shape:\n        raise RuntimeError(\"array {} shape not match {}\".format(a.shape, b.shape))\n    if a.ndim==1:\n        a_norm = np.linalg.norm(a)\n        b_norm = np.linalg.norm(b)\n    elif a.ndim==2:\n        a_norm = np.linalg.norm(a, axis=1, keepdims=True)\n        b_norm = np.linalg.norm(b, axis=1, keepdims=True)\n    else:\n        raise RuntimeError(\"array dimensions {} not right\".format(a.ndim))\n    similiarity = np.dot(a, b.T)\/(a_norm * b_norm) \n    dist = 1. - similiarity\n    return dist","3a91b59e":"def breakPostfixQuants(ttl,psfx):\n    ttlB = []\n    qTpls = []\n    for wrd in ttl:\n        found = False\n        for fx in psfx:\n            if wrd[-len(fx):]==fx:\n                n = len(wrd)-len(fx)\n                if n > 0:\n                    prv = wrd[:n]\n                    if prv.isnumeric():\n                        found = True\n                        ttlB.append(prv)\n                        ttlB.append(fx)\n        if not found:\n            ttlB.append(wrd)\n            \n    psfx = set(psfx)\n    for i,wrd in enumerate(ttlB):\n        if i>0:\n            if wrd in psfx:\n                prv = ttlB[i-1]\n                if prv.isnumeric():\n                    qTpls.append( (prv,wrd) )\n                    \n    for tpl in qTpls:\n        ttlB.remove(tpl[0])\n        ttlB.remove(tpl[1])\n            \n    return (ttlB,qTpls)","fb170c33":"def preprocessTitle(ttl):\n    tl = ttl.split(' ')\n    #tl = [wd for wd in tl if wd not in allSW and len(wd)>0]\n    tl = [wd for wd in tl if len(wd)>0]\n    psfx = ['ML','LTR','KG','GM','GR','GRM','GRAM','PC','PCS','L','LBS','PAKET','PACKET', \\\n            'X','W','V','IN','G','CM','MM','M','OZ','GB','INCHES','FT','PIECES','SHEETS',\\\n           'PACK','PACKS']\n    tl,qtpl = breakPostfixQuants(tl,psfx)\n    #btl = set(zip(tl,tl[1:]))\n    #return (tl,qtpl,btl)\n    return (tl,qtpl)","251cdd03":"def quantMismatched(Q1,Q2):\n    Q1 = set([q[0] for q in Q1 ])\n    Q2 = set([q[0] for q in Q2 ])\n    nf = False\n    if len(Q1)>0 and len(Q2)>0:\n        QL = Q1\n        QS = Q2\n        if len(QS)>len(QL):\n            QS = Q1\n            QL = Q2\n        nf = False\n        for q in QS:\n            if q not in QL:\n                nf = True\n                break\n    return nf","ab96f165":"pptDict = {}\nfor i in tqdm(range(df.shape[0])):\n    ttl = df['title'].iloc[i]\n    tl,qtpl = preprocessTitle(ttl)\n    pid = df['posting_id'].iloc[i]\n    pptDict[pid] = (tl,qtpl)","6c6575e2":"image_embeddings1 = get_image_embeddings1(image_paths.values)\nimage_predictions1 = get_image_predictions(df_image, image_embeddings1, threshold = 0.314) #0.3","fc8fe4f7":"# Function to combine predictions\ndef combine_predictions(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions'], row['oof_text'], row['oof_hash']])\n    return ' '.join( np.unique(x) )","f56e2469":"# Function to combine predictions\ndef combine_predictions_oof(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions'], row['oof_text'], row['oof_hash'],row['oof_image']])\n    return ' '.join( np.unique(x) )","8a6db3f2":"# Concatenate image predctions with text predictions\ntmp = df.groupby('image_phash').posting_id.agg('unique').to_dict()\ndf['oof_hash'] = df.image_phash.map(tmp)\ndf['image_predictions'] = image_predictions\ndf['oof_text'] = df_cu['oof_text'].to_pandas().values\ndf['text_predictions'] = text_predictions\ndf['oof_image']=image_predictions1","97cbd860":"df.head(5)","02178431":"addPhaSure = True\naddTitleSure = True\nuseJIThresh = True\nJIThresh = 0.416 \n\nfor i in tqdm(range(df.shape[0])):\n    pID = df['posting_id'].iloc[i]\n    matchesTL = list(df['text_predictions'].iloc[i])\n    matchesIL = list(df['image_predictions'].iloc[i])\n    tl1 = pptDict[pID][0]\n    lst2 = []\n    for mtID in matchesTL:\n        if mtID != pID and useJIThresh:\n            tl2 = pptDict[mtID][0]\n            aUbU = len( set(tl1).union(set(tl2)) )\n            aIbU = len( set(tl1).intersection(set(tl2)) )\n            Jid = aIbU \/ aUbU\n            if Jid > JIThresh:\n                lst2.append(mtID)\n        else:\n            lst2.append(mtID)\n            \n    if addTitleSure:\n        qTitle = df['title'].iloc[i]\n        titleLst = list(df[ df['title'] == qTitle  ].index)\n        titleLst = [ df['posting_id'].iloc[tid] for tid in titleLst ]\n        titleSet = set(titleLst)\n        lst2Set = set(lst2)\n        lst2Set2 = titleSet.union(lst2Set)\n        lst2 = list(lst2Set2)\n        \n    lst2 = np.array(lst2).astype(object)\n    df['text_predictions'].iloc[i] = lst2\n    \n    lst3 = []\n    \n    for mtID in matchesIL:\n        lst3.append(mtID)\n        \n    if addPhaSure:\n        qHash = df['image_phash'].iloc[i]\n        phLst = list( df[ df['image_phash'] == qHash ].index)\n        phLst = [ df['posting_id'].iloc[tid] for tid in phLst ]\n        phSet = set(phLst)\n        lst3Set = set(lst3)\n        lst3Set2 = phSet.union(lst3Set)\n        lst3 = list(lst3Set2)\n        \n    lst3 = np.array(lst3).astype(object)\n    df['image_predictions'].iloc[i] = lst3\n        ","b8ea6429":"df['matches'] = df.apply(combine_predictions_oof, axis = 1)","6a14dc91":"sub_df = df[['posting_id', 'matches']] ","fb7ef571":"pidToIndx = {}\nfor i in tqdm(range(df.shape[0])):\n    pID = df['posting_id'].iloc[i]\n    pidToIndx[pID] = i","0ee3f47b":"\ndelCnt = 0\ndoRec = False\nfeatsDict = {'TL1':[],'TL2':[], 'JI':[],'SE':[],'isDup':[]}\ntestMode = False\nfor i in tqdm(range(sub_df.shape[0])):\n    pID = sub_df['posting_id'].iloc[i]\n    idIndx = pidToIndx[pID]\n    matchesStr = sub_df['matches'].iloc[i]\n    matchesLst = matchesStr.split(' ')\n    matchesLst2 = [] \n    if len(matchesLst)>1:\n        #print(pID)\n        #print(matchesStr)\n        #print(matchesLst)\n        \n        Q1 = pptDict[pID][1]\n        tl1 = pptDict[pID][0]\n        \n        for mtID in matchesLst:\n            if mtID != pID:\n                Q2 = pptDict[mtID][1]\n                \n                tl2 = pptDict[mtID][0]\n                \n                qMM = quantMismatched(Q1,Q2)\n                if qMM:\n                    qMM=1\n                else:\n                    qMM=0\n                    \n                ###############\n                #f1 = image_embeddings[idIndx]\n                #idIndx2 = pidToIndx[mtID]\n                #f2 = image_embeddings[idIndx2]\n                #se = cosine_distance(f1,f2)\n                \n                ###############\n\n                if qMM==0: \n                    matchesLst2.append(mtID)\n                    \n\n                    if testMode:\n                        idIndx2 = pidToIndx[mtID]\n                        lbl1 = df['label_group'][idIndx]\n                        lbl2 = df['label_group'][idIndx2]\n                        duplicTruth = lbl1==lbl2\n                        if duplicTruth == False:\n                            print(tl1,tl2)\n                    \n                else:\n                    delCnt += 1\n                    #print(delCnt,' deleted')\n                    \n                \n                    \n                if doRec:\n                    aUbU = len( set(tl1).union(set(tl2)) )\n                    aIbU = len( set(tl1).intersection(set(tl2)) )\n                    Jid = aIbU \/ aUbU\n                    f1 = image_embeddings[idIndx]\n                    idIndx2 = pidToIndx[mtID]\n                    f2 = image_embeddings[idIndx2]\n                    se = cosine_distance(f1,f2)\n                    lbl1 = df['label_group'][idIndx]\n                    lbl2 = df['label_group'][idIndx2]\n                    duplicTruth = lbl1==lbl2\n                    featsDict['JI'].append(Jid)\n                    featsDict['TL1'].append(tl1)\n                    featsDict['TL2'].append(tl2)\n                    featsDict['SE'].append(se)\n                    if duplicTruth:\n                        featsDict['isDup'].append(1)\n                    else:\n                        featsDict['isDup'].append(0)\n                    \n            else:\n                matchesLst2.append(mtID)\n                \n        mS2 = ''\n        for m2 in matchesLst2:\n            mS2 += m2+' '\n        mS2 = mS2[:-1]\n        sub_df['matches'].iloc[i] = mS2\n                \n        #print('*****')","a36fec70":"sub_df.to_csv('submission.csv', index = False)","365710ee":"This approach uses following:\n\n* RAPIDs KNN Search using TFIDF for Text and EfficientNet+NFNet Ensemble with ArcFace for Images - (copied from public kernel) \n* Product quantity detection and filtering, in order to differentiate between same product name and different size\/quantity.\n* Initial cleaning of text by replacing Strings.\n* Using Prefix-Tree data structure to find and join words which are sometimes broken in two parts and sometimes not.\n* Making all similar mispelled big words same with first and last letter same and max 1 letter difference.\n* Adding all posts with same title or phash.\n* Filtering text matches using Jaccard Index.","707bb4f3":"Images","ded43e5a":" Thanks to previous Notebooks \n https:\/\/www.kaggle.com\/prashantchandel1097\/ensemble-of-multiple-models-lb0-733\n https:\/\/www.kaggle.com\/prashantchandel1097\/ensemble-of-multiple-models-lb0-733\n https:\/\/www.kaggle.com\/cdeotte\/part-2-rapids-tfidfvectorizer-cv-0-700  \n https:\/\/www.kaggle.com\/muhammad4hmed\/b3-tfidf-knn-boom-p  \n https:\/\/www.kaggle.com\/parthdhameliya77\/pytorch-eca-nfnet-l0-image-tfidf-inference\n "}}