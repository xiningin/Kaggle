{"cell_type":{"237f40f2":"code","4c6e3f6d":"code","23f83962":"code","28e60b0a":"code","f3b9e963":"code","40f039c2":"code","b6fb2b70":"code","eafb9c74":"code","75251d06":"code","c32b1ce5":"code","43c3d2cb":"code","28d7b430":"code","d075540e":"code","2a69f0e2":"code","2d43a3b4":"code","38c7ecbc":"code","303c2d73":"code","53dc8be5":"code","e809d1f5":"code","c40546fc":"code","f3c8ab05":"code","120b99fd":"code","43a198a0":"code","2eb2a809":"code","00d221c9":"code","e5325733":"code","fc4e0fe2":"code","e18fcdf9":"code","31255c7a":"code","7a8e4ead":"code","69897786":"code","902d0847":"code","81037a5f":"code","e21551d8":"code","04ad6037":"code","9be6a0ae":"code","c31d0a29":"code","7ecf7c78":"code","5e66c108":"code","3ab08fc5":"code","4dcab97c":"code","98408969":"code","b355713f":"code","1abf5ff8":"code","9651c1a3":"code","132baa33":"code","4fc97c87":"code","f4c0eeb3":"code","f0499419":"code","63c65e5a":"code","9f4dadda":"code","d6dedd21":"code","8b6d4adc":"code","e824c58b":"code","484b66cb":"code","e9469cd6":"code","2e63a130":"code","4fb62cbb":"code","e8ddf57d":"code","4c263d62":"code","da616264":"code","567fbebe":"code","6f50f8f6":"code","d2453fb4":"code","2d15948a":"code","0f90ad7d":"code","178ee0ca":"markdown","2ee1f12d":"markdown","0f20de09":"markdown","dac02c21":"markdown","0a77aabe":"markdown","3b296e25":"markdown","e9b95c1b":"markdown","2b8e74c7":"markdown","f366c4dc":"markdown","556da680":"markdown","663b2b07":"markdown","d5113846":"markdown","e08c6070":"markdown","4106fc36":"markdown","e3e91da3":"markdown","69fcb603":"markdown","7ab52a6d":"markdown","9ad1a1ca":"markdown","b9971496":"markdown","45312d7f":"markdown","b316cfc8":"markdown","04b0d8c8":"markdown","cf3cbd5e":"markdown","cd442c79":"markdown","f3ef61e9":"markdown","b1315bef":"markdown","d0389244":"markdown","f495332c":"markdown","9a0e8d90":"markdown","8553f3b0":"markdown","16d7d703":"markdown","583c3a33":"markdown","7a65e9ec":"markdown","3d7f611c":"markdown","f9caeb24":"markdown","658735c1":"markdown","c3e23fd9":"markdown","9f4ad331":"markdown","8174fdf3":"markdown","f080c664":"markdown","7c597f9c":"markdown","ee131ef9":"markdown","4a1f53a0":"markdown","efcfc700":"markdown","5d59c6ae":"markdown","94a8b332":"markdown","c7962284":"markdown","19579dd2":"markdown","0955ef47":"markdown","b1959d68":"markdown","f47227d3":"markdown","f7b99ab0":"markdown","cb9bae4c":"markdown","b2c44054":"markdown","b085767f":"markdown","ef5ba7c4":"markdown","f8c5cfe3":"markdown","c66cdeb9":"markdown"},"source":{"237f40f2":"!pip install -q efficientnet","4c6e3f6d":"import os\nimport gc\nimport re\n\nimport cv2\nimport math\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\n\nimport tensorflow as tf\nfrom IPython.display import SVG\nimport efficientnet.tfkeras as efn\nfrom keras.utils import plot_model\nimport tensorflow.keras.layers as L\nfrom keras.utils import model_to_dot\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Model\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow.keras.applications import DenseNet121\n\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.cm as cm\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MultiLabelBinarizer\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\n\ntqdm.pandas()\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nnp.random.seed(0)\ntf.random.set_seed(0)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","23f83962":"EPOCHS = 20\nSAMPLE_LEN = 100\nIMAGE_PATH = \"..\/input\/plant-pathology-2021-fgvc8\/train_images\/\"\n#TEST_PATH = \"..\/input\/plant-pathology-2021-fgvc8\/test.csv\"\nTRAIN_PATH = \"..\/input\/plant-pathology-2021-fgvc8\/train.csv\"\nSUB_PATH = \"..\/input\/plant-pathology-2021-fgvc8\/sample_submission.csv\"\n\n\n\nsub = pd.read_csv(SUB_PATH)\n#test_data = pd.read_csv(TEST_PATH)\ntrain_data = pd.read_csv(TRAIN_PATH)","28e60b0a":"train_data.head()","f3b9e963":"#test_data.head()","40f039c2":"train_data['labels'].value_counts()","b6fb2b70":"plt.figure(figsize=(20,12))\nlabels = sns.barplot(train_data.labels.value_counts().index,train_data.labels.value_counts())\nfor item in labels.get_xticklabels():\n    item.set_rotation(45)","eafb9c74":"train_data['labels'] = train_data['labels'].apply(lambda string: string.split(' '))\ntrain_data","75251d06":"s = list(train_data['labels'])\nmlb = MultiLabelBinarizer()\ntrainx = pd.DataFrame(mlb.fit_transform(s), columns=mlb.classes_, index=train_data.index)\ntrainx","c32b1ce5":"labels = pd.concat([train_data['image'], trainx], axis=1)\nlabels.head()","43c3d2cb":"def visualize_leaves(cond=[0, 0, 0, 0, 0, 0], cond_cols=[\"healthy\"], is_cond=True):\n    if not is_cond:\n        cols, rows = 3, min([3, len(train_images)\/\/3])\n        fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(30, rows*20\/3))\n        for col in range(cols):\n            for row in range(rows):\n                ax[row, col].imshow(train_images.loc[train_images.index[-row*3-col-1]])\n        return None\n        \n    cond_0 = \"complex == {}\".format(cond[0])\n    cond_1 = \"frog_eye_leaf_spot == {}\".format(cond[1])\n    cond_2 = \"healthy == {}\".format(cond[2])\n    cond_3 = \"powdery_mildew == {}\".format(cond[3])\n    cond_4 = \"rust == {}\".format(cond[4])\n    cond_5 = \"scab == {}\".format(cond[5])\n    cond_list = []\n    for col in cond_cols:\n        if col == \"complex\":\n            cond_list.append(cond_0)\n        if col == \"frog_eye_leaf_spot\":\n            cond_list.append(cond_1)\n        if col == \"healthy\":\n            cond_list.append(cond_2)\n        if col == \"powdery_mildew\":\n            cond_list.append(cond_3)\n        if col == \"rust\":\n            cond_list.append(cond_4)\n        if col == \"scab\":\n            cond_list.append(cond_5)\n    \n    data = labels.loc[:100]\n    for cond in cond_list:\n        data = data.query(cond)\n        \n    images = train_images.loc[list(data.index)]\n    cols, rows = 3, min([3, len(images)\/\/3])\n    \n    fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(30, rows*20\/3))\n    for col in range(cols):\n        for row in range(rows):\n            ax[row, col].imshow(images.loc[images.index[row*3+col]])\n    plt.show()","28d7b430":"fig = px.parallel_categories(labels[['complex', 'frog_eye_leaf_spot', 'healthy', 'powdery_mildew', 'rust','scab']], color=\"healthy\", color_continuous_scale=\"sunset\",\\\n                             title=\"Parallel categories plot of targets\")\nfig","d075540e":"def edge_and_cut(img):\n    emb_img = img.copy()\n    edges = cv2.Canny(img, 100, 200)\n    edge_coors = []\n    for i in range(edges.shape[0]):\n        for j in range(edges.shape[1]):\n            if edges[i][j] != 0:\n                edge_coors.append((i, j))\n    \n    row_min = edge_coors[np.argsort([coor[0] for coor in edge_coors])[0]][0]\n    row_max = edge_coors[np.argsort([coor[0] for coor in edge_coors])[-1]][0]\n    col_min = edge_coors[np.argsort([coor[1] for coor in edge_coors])[0]][1]\n    col_max = edge_coors[np.argsort([coor[1] for coor in edge_coors])[-1]][1]\n    new_img = img[row_min:row_max, col_min:col_max]\n    \n    emb_img[row_min-10:row_min+10, col_min:col_max] = [255, 0, 0]\n    emb_img[row_max-10:row_max+10, col_min:col_max] = [255, 0, 0]\n    emb_img[row_min:row_max, col_min-10:col_min+10] = [255, 0, 0]\n    emb_img[row_min:row_max, col_max-10:col_max+10] = [255, 0, 0]\n    \n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(30, 20))\n    ax[0].imshow(img, cmap='gray')\n    ax[0].set_title('Original Image', fontsize=24)\n    ax[1].imshow(edges, cmap='gray')\n    ax[1].set_title('Canny Edges', fontsize=24)\n    ax[2].imshow(emb_img, cmap='gray')\n    ax[2].set_title('Bounding Box', fontsize=24)\n    plt.show()","2a69f0e2":"edge_and_cut(train_images[3])\nedge_and_cut(train_images[4])\nedge_and_cut(train_images[5])","2d43a3b4":"def invert(img):\n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(30, 20))\n    ax[0].imshow(img)\n    ax[0].set_title('Original Image', fontsize=24)\n    ax[1].imshow(cv2.flip(img, 0))\n    ax[1].set_title('Vertical Flip', fontsize=24)\n    ax[2].imshow(cv2.flip(img, 1))\n    ax[2].set_title('Horizontal Flip', fontsize=24)\n    plt.show()","38c7ecbc":"invert(train_images[3])\ninvert(train_images[4])\ninvert(train_images[5])","303c2d73":"def conv(img):\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 20))\n    kernel = np.ones((7, 7), np.float32)\/25\n    conv = cv2.filter2D(img, -1, kernel)\n    ax[0].imshow(img)\n    ax[0].set_title('Original Image', fontsize=24)\n    ax[1].imshow(conv)\n    ax[1].set_title('Convolved Image', fontsize=24)\n    plt.show()","53dc8be5":"conv(train_images[3])\nconv(train_images[4])\nconv(train_images[5])","e809d1f5":"def blur(img):\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 20))\n    ax[0].imshow(img)\n    ax[0].set_title('Original Image', fontsize=24)\n    ax[1].imshow(cv2.blur(img, (100, 100)))\n    ax[1].set_title('Blurred Image', fontsize=24)\n    plt.show()","c40546fc":"blur(train_images[3])\nblur(train_images[4])\nblur(train_images[5])","f3c8ab05":"AUTO = tf.data.experimental.AUTOTUNE\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('plant-pathology-2021-fgvc8')\n#GCS_DS_PATH = '..\/input\/plant-pathology-2021-fgvc8\/train_images'","120b99fd":"KaggleDatasets().get_gcs_path()","43a198a0":"labels","2eb2a809":"labels[labels['healthy'] == 1]","00d221c9":"def format_path(st):\n#     return GCS_DS_PATH + '\/images\/' + st \n    return GCS_DS_PATH + '\/train_images\/' + st \n#test_paths = test_data.image_id.apply(format_path).values\ntrain_paths = labels.image.apply(format_path).values\n\ntrain_labels = (labels.loc[:, 'complex':'scab'].values)","e5325733":"def decode_image(filename, label=None, image_size=(512, 512)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    image = tf.image.resize(image, image_size)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n\ndef data_augment(image, label=None):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    \n    if label is None:\n        return image\n    else:\n        return image, label","fc4e0fe2":"train_paths","e18fcdf9":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_paths, train_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .map(data_augment, num_parallel_calls=AUTO)\n    .repeat()\n    .shuffle(512)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\n# valid_dataset = (\n#     tf.data.Dataset\n#     .from_tensor_slices((valid_paths, valid_labels))\n#     .map(decode_image, num_parallel_calls=AUTO)\n#     .batch(BATCH_SIZE)\n#     .cache()\n#     .prefetch(AUTO)\n# )\n\n\n# test_dataset = (\n#     tf.data.Dataset\n#     .from_tensor_slices(test_paths)\n#     .map(decode_image, num_parallel_calls=AUTO)\n#     .batch(BATCH_SIZE)\n# )\n","31255c7a":"def build_lrfn(lr_start=0.00001, lr_max=0.00005, \n               lr_min=0.00001, lr_rampup_epochs=5, \n               lr_sustain_epochs=0, lr_exp_decay=.8):\n    lr_max = lr_max * strategy.num_replicas_in_sync\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) *\\\n                 lr_exp_decay**(epoch - lr_rampup_epochs\\\n                                - lr_sustain_epochs) + lr_min\n        return lr\n    return lrfn","7a8e4ead":"lrfn = build_lrfn()\nSTEPS_PER_EPOCH = train_labels.shape[0] \/\/ BATCH_SIZE\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)","69897786":"with strategy.scope():\n    model = tf.keras.Sequential([DenseNet121(input_shape=(512, 512, 3),\n                                             weights='imagenet',\n                                             include_top=False),\n                                 L.GlobalAveragePooling2D(),\n                                 L.Dense(train_labels.shape[1],\n                                         activation='softmax')])\n        \n    model.compile(optimizer='adam',\n                  loss = 'categorical_crossentropy',\n                  metrics=['categorical_accuracy'])\n    model.summary()","902d0847":"SVG(tf.keras.utils.model_to_dot(Model(model.layers[0].input, model.layers[0].layers[13].output), dpi=70).create(prog='dot', format='svg'))","81037a5f":"SVG(tf.keras.utils.model_to_dot(model, dpi=70).create(prog='dot', format='svg'))","e21551d8":"history = model.fit(train_dataset,\n                    epochs=EPOCHS,\n                    callbacks=[lr_schedule],\n                    steps_per_epoch=STEPS_PER_EPOCH,\n                    validation_data=valid_dataset)","04ad6037":"def display_training_curves(training, validation, yaxis):\n    if yaxis == \"loss\":\n        ylabel = \"Loss\"\n        title = \"Loss vs. Epochs\"\n    else:\n        ylabel = \"Accuracy\"\n        title = \"Accuracy vs. Epochs\"\n        \n    fig = go.Figure()\n        \n    fig.add_trace(\n        go.Scatter(x=np.arange(1, EPOCHS+1), mode='lines+markers', y=training, marker=dict(color=\"dodgerblue\"),\n               name=\"Train\"))\n    \n    fig.add_trace(\n        go.Scatter(x=np.arange(1, EPOCHS+1), mode='lines+markers', y=validation, marker=dict(color=\"darkorange\"),\n               name=\"Val\"))\n    \n    fig.update_layout(title_text=title, yaxis_title=ylabel, xaxis_title=\"Epochs\", template=\"plotly_white\")\n    fig.show()","9be6a0ae":"display_training_curves(\n    history.history['categorical_accuracy'], \n    history.history['val_categorical_accuracy'], \n    'accuracy')","c31d0a29":"acc_df = pd.DataFrame(np.transpose([[*np.arange(1, EPOCHS+1).tolist()*3], [\"Train\"]*EPOCHS + [\"Val\"]*EPOCHS + [\"Benchmark\"]*EPOCHS,\n                                     history.history['categorical_accuracy'] + history.history['val_categorical_accuracy'] + [1.0]*EPOCHS]))\nacc_df.columns = [\"Epochs\", \"Stage\", \"Accuracy\"]\nfig = px.bar(acc_df, x=\"Accuracy\", y=\"Stage\", animation_frame=\"Epochs\", title=\"Accuracy vs. Epochs\", color='Stage',\n       color_discrete_map={\"Train\":\"dodgerblue\", \"Val\":\"darkorange\", \"Benchmark\":\"seagreen\"}, orientation=\"h\")\n\nfig.update_layout(\n    xaxis = dict(\n        autorange=False,\n        range=[0, 1]\n    )\n)\n\nfig.update_layout(template=\"plotly_white\")","7ecf7c78":"def process(img):\n    return cv2.resize(img\/255.0, (512, 512)).reshape(-1, 512, 512, 3)\ndef predict(img):\n    return model.layers[2](model.layers[1](model.layers[0](process(img)))).numpy()[0]\n\nfig = make_subplots(rows=4, cols=2)\npreds = predict(train_images[2])\n\ncolors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0]}\nif list.index(preds.tolist(), max(preds)) == 0:\n    pred = \"Healthy\"\nif list.index(preds.tolist(), max(preds)) == 1:\n    pred = \"Scab\"\nif list.index(preds.tolist(), max(preds)) == 2:\n    pred = \"Rust\"\nif list.index(preds.tolist(), max(preds)) == 3:\n    pred = \"Multiple diseases\"\n\ncolors[pred] = px.colors.qualitative.Plotly[1]\ncolors[\"Healthy\"] = \"seagreen\"\ncolors = [colors[val] for val in colors.keys()]\nfig.add_trace(go.Image(z=cv2.resize(train_images[2], (205, 136))), row=1, col=1)\nfig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=1, col=2)\nfig.update_layout(height=1200, width=800, title_text=\"DenseNet Predictions\", showlegend=False)\n\npreds = predict(train_images[0])\ncolors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0]}\nif list.index(preds.tolist(), max(preds)) == 0:\n    pred = \"Healthy\"\nif list.index(preds.tolist(), max(preds)) == 1:\n    pred = \"Multiple diseases\"\nif list.index(preds.tolist(), max(preds)) == 2:\n    pred = \"Rust\"\nif list.index(preds.tolist(), max(preds)) == 3:\n    pred = \"Scab\"\n    \ncolors[pred] = px.colors.qualitative.Plotly[1]\ncolors[\"Multiple diseases\"] = \"seagreen\"\ncolors = [colors[val] for val in colors.keys()]\nfig.add_trace(go.Image(z=cv2.resize(train_images[0], (205, 136))), row=2, col=1)\nfig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=2, col=2)\n\npreds = predict(train_images[3])\ncolors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0]}\nif list.index(preds.tolist(), max(preds)) == 0:\n    pred = \"Healthy\"\nif list.index(preds.tolist(), max(preds)) == 1:\n    pred = \"Multiple diseases\"\nif list.index(preds.tolist(), max(preds)) == 2:\n    pred = \"Rust\"\nif list.index(preds.tolist(), max(preds)) == 3:\n    pred = \"Scab\"\n    \ncolors[pred] = px.colors.qualitative.Plotly[1]\ncolors[\"Rust\"] = \"seagreen\"\ncolors = [colors[val] for val in colors.keys()]\nfig.add_trace(go.Image(z=cv2.resize(train_images[3], (205, 136))), row=3, col=1)\nfig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=3, col=2)\n\npreds = predict(train_images[1])\ncolors = {\"Healthy\":px.colors.qualitative.Plotly[0], \"Scab\":px.colors.qualitative.Plotly[0], \"Rust\":px.colors.qualitative.Plotly[0], \"Multiple diseases\":px.colors.qualitative.Plotly[0]}\nif list.index(preds.tolist(), max(preds)) == 0:\n    pred = \"Healthy\"\nif list.index(preds.tolist(), max(preds)) == 1:\n    pred = \"Multiple diseases\"\nif list.index(preds.tolist(), max(preds)) == 2:\n    pred = \"Rust\"\nif list.index(preds.tolist(), max(preds)) == 3:\n    pred = \"Scab\"\n    \ncolors[pred] = px.colors.qualitative.Plotly[1]\ncolors[\"Scab\"] = \"seagreen\"\ncolors = [colors[val] for val in colors.keys()]\nfig.add_trace(go.Image(z=cv2.resize(train_images[1], (205, 136))), row=4, col=1)\nfig.add_trace(go.Bar(x=[\"Healthy\", \"Multiple diseases\", \"Rust\", \"Scab\"], y=preds, marker=dict(color=colors)), row=4, col=2)\n\nfig.update_layout(template=\"plotly_white\")","5e66c108":"test_paths = []\ntest_paths.append(GCS_DS_PATH + '\/test_images\/' + '85f8cb619c66b863.jpg')\ntest_paths.append(GCS_DS_PATH + '\/test_images\/' + 'ad8770db05586b59.jpg')\ntest_paths.append(GCS_DS_PATH + '\/test_images\/' + 'c7b03e718489f3ca.jpg')","3ab08fc5":"test_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_paths)\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n)","4dcab97c":"probs_efn = model.predict(test_dataset, verbose=1)\nprint(probs_efn)\nSUB_PATH = \"..\/input\/plant-pathology-2021-fgvc8\/sample_submission.csv\"\nsub = pd.read_csv(SUB_PATH)","98408969":"arr = ['complex','frog_eye_leaf_spot','healthy','powdery_mildew','rust','scab']","b355713f":"sub.loc[:, 'labels':] = [arr[np.argmax(aa)] for aa in probs_efn][0]\n#sub.loc[:, 'labels':] = [arr[np.argmax(aa)] for aa in probs_efn][1]\n#sub.loc[:, 'labels':] = [arr[np.argmax(aa)] for aa in probs_efn][2]\nsub.to_csv('.\/submission.csv', index=False)\nsub.head()","1abf5ff8":"# probs_dnn = model.predict(test_dataset, verbose=1)\n# sub.loc[:, 'healthy':] = probs_dnn\n# sub.to_csv('submission_dnn.csv', index=False)\n# sub.head()","9651c1a3":"with strategy.scope():\n    model = tf.keras.Sequential([efn.EfficientNetB7(input_shape=(512, 512, 3),\n                                                    weights='imagenet',\n                                                    include_top=False),\n                                 L.GlobalAveragePooling2D(),\n                                 L.Dense(train_labels.shape[1],\n                                         activation='softmax')])\n    \n    \n        \n    model.compile(optimizer='adam',\n                  loss = 'categorical_crossentropy',\n                  metrics=['categorical_accuracy'])\n    model.summary()","132baa33":"SVG(tf.keras.utils.model_to_dot(Model(model.layers[0].input, model.layers[0].layers[11].output), dpi=70).create(prog='dot', format='svg'))","4fc97c87":"SVG(tf.keras.utils.model_to_dot(model, dpi=70).create(prog='dot', format='svg'))","f4c0eeb3":"history = model.fit(train_dataset,\n                    epochs=EPOCHS,\n                    callbacks=[lr_schedule],\n                    steps_per_epoch=STEPS_PER_EPOCH)","f0499419":"model.save(\"EfficientNet_epoch20_2.h5\")","63c65e5a":"a","9f4dadda":"history = a","d6dedd21":"display_training_curves(\n    history.history['categorical_accuracy'], \n    history.history['val_categorical_accuracy'], \n    'accuracy')","8b6d4adc":"acc_df = pd.DataFrame(np.transpose([[*np.arange(1, EPOCHS+1).tolist()*3], [\"Train\"]*EPOCHS + [\"Val\"]*EPOCHS + [\"Benchmark\"]*EPOCHS,\n                                     history.history['categorical_accuracy'] + history.history['val_categorical_accuracy'] + [1.0]*EPOCHS]))\nacc_df.columns = [\"Epochs\", \"Stage\", \"Accuracy\"]\nfig = px.bar(acc_df, x=\"Accuracy\", y=\"Stage\", animation_frame=\"Epochs\", title=\"Accuracy vs. Epochs\", color='Stage',\n       color_discrete_map={\"Train\":\"dodgerblue\", \"Val\":\"darkorange\", \"Benchmark\":\"seagreen\"}, orientation=\"h\")\n\nfig.update_layout(\n    xaxis = dict(\n        autorange=False,\n        range=[0, 1]\n    )\n)\n\nfig.update_layout(template=\"plotly_white\")","e824c58b":"test_paths","484b66cb":"test_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_paths)\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n)","e9469cd6":"probs_efn = model.predict(test_dataset, verbose=1)\nprint(probs_efn)\nSUB_PATH = \"..\/input\/plant-pathology-2021-fgvc8\/sample_submission.csv\"\nsub = pd.read_csv(SUB_PATH)\n","2e63a130":"arr = ['complex','frog_eye_leaf_spot',\t'healthy'\t,'powdery_mildew'\t,'rust',\t'scab']\narr","4fb62cbb":"sub.loc[:, 'labels':] = [arr[np.argmax(aa)] for aa in probs_efn][0]\nsub.to_csv('.\/submission.csv', index=False)\nsub.head()","e8ddf57d":"[arr[np.argmax(aa)] for aa in probs_efn]","4c263d62":"sub.loc[:, 'labels':]","da616264":"with strategy.scope():\n    model = tf.keras.Sequential([efn.EfficientNetB7(input_shape=(512, 512, 3),\n                                                    weights='noisy-student',\n                                                    include_top=False),\n                                 L.GlobalAveragePooling2D(),\n                                 L.Dense(train_labels.shape[1],\n                                         activation='softmax')])\n    \n    \n        \n    model.compile(optimizer='adam',\n                  loss = 'categorical_crossentropy',\n                  metrics=['categorical_accuracy'])\n    model.summary()","567fbebe":"SVG(tf.keras.utils.model_to_dot(Model(model.layers[0].input, model.layers[0].layers[11].output), dpi=70).create(prog='dot', format='svg'))","6f50f8f6":"SVG(tf.keras.utils.model_to_dot(model, dpi=70).create(prog='dot', format='svg'))","d2453fb4":"history = model.fit(train_dataset,\n                    epochs=EPOCHS,\n                    callbacks=[lr_schedule],\n                    steps_per_epoch=STEPS_PER_EPOCH,\n                    validation_data=valid_dataset)","2d15948a":"display_training_curves(\n    history.history['categorical_accuracy'], \n    'accuracy')","0f90ad7d":"acc_df = pd.DataFrame(np.transpose([[*np.arange(1, EPOCHS+1).tolist()*3], [\"Train\"]*EPOCHS + [\"Val\"]*EPOCHS + [\"Benchmark\"]*EPOCHS,\n                                     history.history['categorical_accuracy'] + history.history['val_categorical_accuracy'] + [1.0]*EPOCHS]))\nacc_df.columns = [\"Epochs\", \"Stage\", \"Accuracy\"]\nfig = px.bar(acc_df, x=\"Accuracy\", y=\"Stage\", animation_frame=\"Epochs\", title=\"Accuracy vs. Epochs\", color='Stage',\n       color_discrete_map={\"Train\":\"dodgerblue\", \"Val\":\"darkorange\", \"Benchmark\":\"seagreen\"}, orientation=\"h\")\n\nfig.update_layout(\n    xaxis = dict(\n        autorange=False,\n        range=[0, 1]\n    )\n)\n\nfig.update_layout(template=\"plotly_white\")","178ee0ca":"### Generate submission","2ee1f12d":"### Animation (click \u25b6\ufe0f)","0f20de09":"The model predicts the leaf diseases with great accuracy. The level of performance is similar to that of DenseNet, as the green bars are very common. The red and blue bars are more prominent in the last (fourth) leaf labeled \"multiple diseases\". This is probably because leaves with multiple diseases may show symptoms of rust and scab as well, thus slightly confusing the model.","dac02c21":"The transformation clearly blurs the image by removing detailed, low-level features, while retaining the major, high-level features. This is once again a great way to augment images and train more robust models.","0a77aabe":"### Visualize model architecture\n\nThe model consists of the EfficientNet NoisyStudent head (without the top), followed by global average pooling and a dense layer (with softmax) to generate probabilities.","3b296e25":"### Train model","e9b95c1b":"From the animations above, we can see the volatility in validation metrics a lot more clearly. The validation metrics oscillate in an erratic fashion until it reaches the 7th epoch and starts to generalize properly.","2b8e74c7":"The above image shows the fundamental block in the EfficientNet NoisyStudent architecture. This model has the same architecture as EfficientNet. Only the weights are different, as they are obtained through semi-supervision.","f366c4dc":"### EfficientNet NoisyStudent","556da680":"## Preparing the ground <a id=\"3.1\"><\/a>\n\nBefore we move on to building the models, I will explain the major building blocks in pretrained CV models. Every major ImageNet model has a different architecture, but each one has the common building blocks: **Conv2D, MaxPool, ReLU**. I have already explained the mechanism behind convolution in the previous section, so I will now explain MaxPool and ReLU.\n\n### MaxPool\n\nMax pooling is very similar to convolution, except it involves finding the maximum value in a window instead of finding the dot product of the window with a kernel. Max pooling does not require a kernel and it is very useful in reducing the dimensionality of convolutional feature maps in CNNs. The image below demonstrates the working of MaxPool:\n\n\n<center><img src=\"https:\/\/i.imgur.com\/rBNMsfi.png\" width=\"400px\"><\/center>\n<br><\/br>\n\nThe above example demonstrates max pooling with a window size of *(2, 2)*. This process can be represented with the equation below:\n<br><\/br>\n.\n\n<center><img src=\"https:\/\/i.imgur.com\/FRyMNhI.png\" width=\"650px\"><\/center>\n<br><\/br>\n\nIn the above equation, the window moves across the image and the maximum value in each winow is calculated. Once again, this process is very important in reducing the complexity of CNNs while retaining features.","663b2b07":"## Convolution <a id=\"2.3\"><\/a>\n\nConvolution is a rather simple algorithm which involves a kernel (a 2D matrix) which moves over the entire image, calculating dot products with each window along the way. The GIF below demonstrates convolution in action.\n\n<center><img src=\"https:\/\/i.imgur.com\/wYUaqR3.gif\" width=\"450px\"><\/center>\n\nThe above process can be summarized with an equation, where *f* is the image and *h* is the kernel. The dimensions of *f* are *(m, n)* and the kernel is a square matrix with dimensions smaller than *f*:\n\n<center><img src=\"https:\/\/i.imgur.com\/9scTOGv.png\" width=\"350px\"><\/center>\n<br>\n\nIn the above equation, the kernel *h* is moving across the length and breadth of the image. The dot product of *h* with a sub-matrix or window of matrix *f* is taken at each step, hence the double summation (rows and columns). Below I demonstrate the effect of convolution on leaf images.","d5113846":"### Generate submission","e08c6070":"### Animation (click \u25b6\ufe0f)","4106fc36":"### Load sample images","e3e91da3":"From the animations above, we can see that the validation and training metrics do not show great volatility. They steadily rise towards 1.0.","69fcb603":"From the above plots, we can see that the losses decrease and accuracies increase quite consistently. The training metrics settle down very fast (after 1 or 2 epochs), whereas the validation metrics much greater volatility and start to settle down only after 7-8 epochs. This is expected because validation data is unseen and more diffcult to make predictions on than training data. ","7ab52a6d":"### Install and import necessary libraries","9ad1a1ca":"### Scatter plots","b9971496":"### ReLU\n\nReLU is an activation function commonly used in neural network architectures. *ReLU(x)* returns 0 for *x < 0* and *x* otherwise. This function helps introducenon-linearity in the neural network, thus increasing its capacity ot model the image data. The graph and equation of *ReLU* are:\n\n<center><img src=\"https:\/\/i.imgur.com\/eiRVQBh.png\" width=\"400px\"><\/center>\n\n<center><img src=\"https:\/\/i.imgur.com\/0mBFAH0.png\" width=\"400px\"><\/center>\n<br><\/br>\n\nAs mentioned earlier, this function is non-linear and helps increase the modeling capacity of the CNN models. Now since we understand the basic building blocks of pretrained images models, let us finetune some pretained ImageNet models on TPU and visualize the results!","45312d7f":"## Flipping <a id=\"2.2\"><\/a>\n\nFlipping is a simple transformation that involves index-switching on the image channels. In vertical flipping, the order of rows is exchanged, whereas in vertical flipping, the order of rows is exchanged. Let us assume that *A<sub>ijk<\/sub>* (of size *(m, n, 3)*) is the image we want to flip. Horizontal and vertical flipping can be represented by the transformations below:\n\n<center><img src=\"https:\/\/i.imgur.com\/B9y5apl.png\" width=\"135px\"><\/center>\n<center><img src=\"https:\/\/i.imgur.com\/eQ1dyvN.png\" width=\"305px\"><\/center>\n<center><img src=\"https:\/\/i.imgur.com\/i30LQgq.png\" width=\"305px\"><\/center>\n<br>\n\nWe can see that the order of columns is exchanged in horizontal flipping. While the *i* and *k* indices remain the same, the *j* index reverses. Whereas, in vertical flipping, the order of rows is exchanged in horizontal flipping. While the *j* and *k* indices remain the same, the *i* index reverses.\n\n","b316cfc8":"### Helper functions","04b0d8c8":"In the above plot, we can see the relationship between all four categories. As expected, it is impossible for a healthy leaf (<code>healthy == 1<\/code>) to have scab, rust, or multiple diseases. Also, every unhealthy leaf has one of either scab, rust, or multiple diseases. The frequency of each combination can be seen by hovering over the plot.","cf3cbd5e":"### Visualize results","cd442c79":"### DenseNet fundamental block","f3ef61e9":"From the above plots, we can see that the losses decrease and accuracies increase quite consistently. The training metrics settle down very fast (after 1 or 2 epochs), whereas the validation metrics much greater volatility and start to settle down only after 12-13 epochs (similar to DenseNet). This is expected because validation data is unseen and more diffcult to make predictions on than training data. ","b1315bef":"## EfficientNet <a id=\"3.3\"><\/a>\n\nEfficientNet is another popular (more recent) CNN-based ImageNet model which achieved the SOTA on several image-based tasks in 2019. EfficientNet performs model scaling in an innovative way to achieve excellent accuracy with significantly fewer parameters. It achieves the same if not greater accuracy than ResNet and DenseNet with a mcuh shallower architecture. Now let us train EfficientNet on leaf images and evaluate its performance.","d0389244":"### Sample predictions\n\nNow, I will visualize some sample predictions made by the DenseNet model. The <font color=\"red\">red<\/font> bars represent the model's prediction (maximum probability), the <font color=\"green\">green<\/font> represent the ground truth (label), and the rest of the bars are <font color=\"blue\">blue<\/font>. When the model predicts correctly, the prediction bar is <font color=\"green\">green<\/font>.","f495332c":"## DenseNet <a id=\"3.2\"><\/a>\n\nDensely Connected Convolutional Networks (DenseNets), are a popular CNN-based ImageNet used for a variety of applications, inclusing classification, segmentation, localization, etc. Most models before DenseNet relied solely on network depth for representational power. **Instead of drawing representational power from extremely deep or wide architectures, DenseNets exploit the potential of the network through feature reuse.** This was the main motivation behind the DenseNet architecture. Now let us train DenseNet on leaf images and evaluate its performance.","9a0e8d90":"### Setup TPU Config","8553f3b0":"### Create Dataset objects","16d7d703":"### Visualize model architecture\n\nThe model consists of the EfficientNet head (without the top), followed by global average pooling and a dense layer (with softmax) to generate probabilities.","583c3a33":"## Blurring <a id=\"2.4\"><\/a>\n\nBlurring is simply the addition of noise to the image, resulting in a less-clear image. The noise can be sampled from any distribution of choice, as long as the main content in the image does not become invisible. Only the minor details get obfuscated due to blurring. The blurring transformation can be represented using the equation below. \n\n<center><img src=\"https:\/\/i.imgur.com\/zVM8HCU.png\" width=\"220px\"><\/center>\n<br>\n\nThe example uses a Gaussian distribution with mean 0 and variance 0.1. Below I demonstrate the effect of blurring on a few leaf images:","7a65e9ec":"### Scatter plots","3d7f611c":"### Define hyperparameters and callbacks","f9caeb24":"### Visualize model architecture\n\nThe model consists of the DenseNet head (without the top), followed by global average pooling and a dense layer (with softmax) to generate probabilities.","658735c1":"# EDA <a id=\"1\"><\/a>","c3e23fd9":"### Load the data and define hyperparameters","9f4ad331":"### Load labels and paths","8174fdf3":"The convolution operator seems to have an apparent \"sunshine\" effect of the images. This may also serve the purpose of augmenting the data, thus helping to build more robust and accurate models. ","f080c664":"### Train model","7c597f9c":"### Scatter plots","ee131ef9":"# Image processing and augmentation <a id=\"2\"><\/a>","4a1f53a0":"### Visualize results","efcfc700":"# Modeling <a id=\"3\"><\/a>","5d59c6ae":"We can see that DenseNet predicts leaf diseases with great accuracy. No red or blue bars are seen. The probabilities are very polarized (one very high and the rest very low), indicating that the model is making these predictions with great confidence.","94a8b332":"## EfficientNet NoisyStudent <a id=\"3.4\"><\/a>\n\nEfficientNet NoisyStudent, released in 2020, is based on EfficientNet and uses semi-supervised learning on noisy images to learn rich visual representation. It outperformed EfficientNet on several tasks and is the SOTA at the time of writing (March 2020). Now let us train EfficientNet NoisyStudent on leaf images and evaluate its performance.","c7962284":"We can see that the images are simply flipped. All major features in the image remain the same, but to a computer algorithm, the flipped images look completely different. These transformations can be used for data augmentation, making models more robust and accurate.","19579dd2":"### Visualize results","0955ef47":"### EfficientNet fundamental block","b1959d68":"The above image shows the fundamental block in the DenseNet architecture. The architecture mainly involves Convolution, Maxpooling, ReLU, and concatenation.","f47227d3":"The above image shows the fundamental block in the EfficientNet architecture. This architecture involves more addition and multiplication-based operators than DenseNet. These operations are less parameter-intensive than concatenation, which is much more common in DenseNet. Such transformations help EfficientNet achieve great efficiency (in terms of performance per parameter).","f7b99ab0":"### Animation (click \u25b6\ufe0f)","cb9bae4c":"## Visualize sample leaves <a id=\"1.4\"><\/a>\n\nNow, I will visualize sample leaves beloning to different categories in the dataset.","b2c44054":"From the above plots, we can once again see that the losses decrease and accuracies increase quite consistently. The training metrics settle down very fast (after 1 or 2 epochs). In this case, the validation metrics do not show high volatility as compared to the DenseNet model.","b085767f":"The second column of images above contains the Canny edges and the third column contains cropped images. I have taken the Canny edges and used it to predict a bounding box in which the actual leaf is contained. The most extreme edges at the four corners of the image are the vertices of the bounding box. This red box is likely to contain most of if not all of the leaf. These edges and bounding boxes can be used to build more accurate models.","ef5ba7c4":"## Canny edge detection <a id=\"2.1\"><\/a>\n\nCanny is a popular edge detection algorithm, and as the name suggests, it detects the edges of objects present in an image. It was developed by John F. Canny in 1986. The algorithm involves several steps.\n\n1. **Noise reduction:** Since edge detection is susceptible to noise in an image, we remove the noise in the image using a 5x5 Gaussian filter.\n\n\n2. **Finding Intensity Gradient of the Image**: The smoothened image is then filtered with a Sobel kernel in both horizontal and vertical directions to get the first derivative in the horizontal (*G<sub>x<\/sub>*) and vertical (*G<sub>y<\/sub>*) directions. From these two images, one can find the edge gradient and direction for each pixel:\n\n<center><img src=\"https:\/\/i.imgur.com\/ntyjTep.png\" width=\"300px\"><\/center>\n<center><img src=\"https:\/\/i.imgur.com\/75qDjv6.png\" width=\"260px\"><\/center>\n\n<br>\n\n3. **Rounding:** The gradient is always perpendicular to edges. So, it is rounded to one of the four angles representing vertical, horizontal and two diagonal directions.\n\n4. **Non-maximum suppression:** After getting the gradient magnitude and direction, a full scan of the image is done to remove any unwanted pixels which may not constitute the edge. For this, we check every pixel for being a local maximum in its neighborhood in the direction of the gradient.\n\n5. **Hysteresis Thresholding:** This stage decides which parts are edges and which are not. For this, we need two threshold values, *minVal* and *maxVal*. Any edges with intensity gradient greater than *maxVal* are considered edges and those lesser than *minVal* are considered non-edges, and discarded. Those who lie between these two thresholds are classified edges or non-edges based on their neighborhood. If they are near \u201csure-edge\u201d pixels, they are considered edges, and otherwise, they are discarded.\n\nThe result of these five steps is a two-dimensional binary map (0 or 255) indicating the location of edges on the image. Canny edge is demonstrated below with a few leaf images:","f8c5cfe3":"### Train model","c66cdeb9":"## Preparing the ground <a id=\"1.1\"><\/a>"}}