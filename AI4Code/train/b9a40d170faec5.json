{"cell_type":{"f47bd677":"code","486479f0":"code","1b5bcc3b":"code","8da11bc1":"code","26067cdf":"code","d32cda20":"code","f29c8b0c":"code","f0d7f1bf":"code","eee62a77":"code","0ee3309c":"code","52f654ab":"code","3cefa564":"code","76815c90":"code","6832124a":"code","8c0b9622":"markdown","1ae70f91":"markdown","40dd8971":"markdown","0e51cd57":"markdown","13ef0faf":"markdown","13d2d8ac":"markdown","991004e5":"markdown","1d85aa63":"markdown","7e03f5c9":"markdown","bcd6e9cd":"markdown","cf06b941":"markdown","79f8c1e0":"markdown","ecf5d88c":"markdown","1e19e37f":"markdown","e9bbe5cd":"markdown","9b9d9edc":"markdown"},"source":{"f47bd677":"# Familiar imports\nimport numpy as np\nimport pandas as pd\nfrom numpy import mean\nfrom numpy import std\nimport time\nfrom matplotlib import pyplot\n\n# For ordinal encoding categorical variables, splitting data\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.model_selection import KFold\nfrom tqdm import tqdm\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom lightgbm import LGBMRegressor\n# REF: https:\/\/www.kaggle.com\/svyatoslavsokolov\/tps-feb-2021-lgbm-simple-version","486479f0":"# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\n\n# Preview the data\ntrain.head()","1b5bcc3b":"# Separate target from features\ny = train['target']\nfeatures = train.drop(['target'], axis=1)\n\n# Preview features\nfeatures.head()","8da11bc1":"# List of categorical columns\nobject_cols = [col for col in features.columns if 'cat' in col]\n\n# ordinal-encode categorical columns\nX = features.copy()\nX_test = test.copy()\nordinal_encoder = OrdinalEncoder()\nX[object_cols] = ordinal_encoder.fit_transform(features[object_cols])\nX_test[object_cols] = ordinal_encoder.transform(test[object_cols])\n\n# Preview the ordinal-encoded features\nX.head()","26067cdf":"lgbm_parameters = {\n    'metric': 'rmse', \n    'n_jobs': -1,\n    'n_estimators': 50000,\n    'reg_alpha': 10.924491968127692,\n    'reg_lambda': 17.396730654687218,\n    'colsample_bytree': 0.21497646795452627,\n    'subsample': 0.7582562557431147,\n    'learning_rate': 0.009985133666265425,\n    'max_depth': 18,\n    'num_leaves': 63,\n    'min_child_samples': 27,\n    'max_bin': 523,\n    'cat_l2': 0.025083670064082797\n}","d32cda20":"lgbm_val_pred = np.zeros(len(y))\nlgbm_test_pred = np.zeros(len(X_test))\nmse = []\nkf = KFold(n_splits=10, shuffle=True)\n\nfor trn_idx, val_idx in tqdm(kf.split(X,y)):\n    x_train_idx = X.iloc[trn_idx]\n    y_train_idx = y.iloc[trn_idx]\n    x_valid_idx = X.iloc[val_idx]\n    y_valid_idx = y.iloc[val_idx]\n\n    lgbm_model = LGBMRegressor(**lgbm_parameters)\n    lgbm_model.fit(x_train_idx, y_train_idx, eval_set = ((x_valid_idx,y_valid_idx)),verbose = -1, early_stopping_rounds = 100)  \n    lgbm_test_pred += lgbm_model.predict(X_test)\/10\n    mse.append(mean_squared_error(y_valid_idx, lgbm_model.predict(x_valid_idx)))\n    \nnp.mean(mse)\npd.DataFrame({'id':X_test.index,'target':lgbm_test_pred}).to_csv('submission.csv', index=False)","f29c8b0c":"# # define the model\n# model = LGBMRegressor()\n\n# # evaluate the model\n# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# n_scores = cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1, error_score='raise')\n\n# # report performance\n# print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))","f0d7f1bf":"# # explore lightgbm number of trees effect on performance\n\n# # get a list of models to evaluate\n# def get_models():\n#     models = dict()\n#     trees = [10, 50, 100, 500, 1000, 5000]\n#     for n in trees:\n#         models[str(n)] = LGBMRegressor(n_estimators=n)\n#     return models\n\n# # evaluate a give model using cross-validation\n# def evaluate_model(model):\n#     cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n#     scores = cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1, error_score='raise')\n#     return scores\n\n# # get the models to evaluate\n# models = get_models()\n# # evaluate the models and store results\n# results, names = list(), list()\n# for name, model in models.items():\n#     t1 = time.time()\n#     scores = evaluate_model(model)\n#     results.append(scores)\n#     names.append(name)\n#     t2 = time.time()\n#     print('>%s %.3f (%.3f) time taken: %.3f' % (name, mean(scores), std(scores), t2-t1))\n# # plot model performance for comparison\n# pyplot.boxplot(results, labels=names, showmeans=True)\n# pyplot.show()","eee62a77":"# # explore lightgbm tree depth effect on performance\n \n# # get a list of models to evaluate\n# def get_models():\n#     models = dict()\n#     for i in range(1,11):\n#         models[str(i)] = LGBMRegressor(max_depth=i, num_leaves=2**i)\n#     return models\n \n# # evaluate a give model using cross-validation\n# def evaluate_model(model):\n#     cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n#     scores = cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1, error_score='raise')\n#     return scores\n \n# # get the models to evaluate\n# models = get_models()\n# # evaluate the models and store results\n# results, names = list(), list()\n# for name, model in models.items():\n#     t1 = time.time()\n#     scores = evaluate_model(model)\n#     results.append(scores)\n#     names.append(name)\n#     t2 = time.time()\n#     print('>%s %.3f (%.3f) time taken: %.3f' % (name, mean(scores), std(scores), t2-t1))\n# # plot model performance for comparison\n# pyplot.boxplot(results, labels=names, showmeans=True)\n# pyplot.show()","0ee3309c":"# # explore lightgbm learning rate effect on performance\n\n# # get a list of models to evaluate\n# def get_models():\n#     models = dict()\n#     rates = [0.0001, 0.001, 0.01, 0.1, 1.0]\n#     for r in rates:\n#         key = '%.4f' % r\n#         models[key] = LGBMRegressor(learning_rate=r)\n#     return models\n\n# # evaluate a give model using cross-validation\n# def evaluate_model(model):    \n#     cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n#     scores = cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1, error_score='raise')\n#     return scores\n\n# models = get_models()\n# # evaluate the models and store results\n# results, names = list(), list()\n# for name, model in models.items():\n#     t1 = time.time()\n#     scores = evaluate_model(model)\n#     results.append(scores)\n#     names.append(name)\n#     t1 = time.time()\n#     print('>%s %.3f (%.3f) time taken: %.2f' % (name, mean(scores), std(scores), t2-t1))\n# # plot model performance for comparison\n# pyplot.boxplot(results, labels=names, showmeans=True)\n# pyplot.show()","52f654ab":"# # explore lightgbm boosting type effect on performance\n\n# # get a list of models to evaluate\n# def get_models():\n#     models = dict()\n#     types = ['gbdt', 'dart', 'goss']\n#     for t in types:\n#         models[t] = LGBMRegressor(boosting_type=t)\n#     return models\n\n# # evaluate a give model using cross-validation\n# def evaluate_model(model):\n#     cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n#     scores = cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1, error_score='raise')\n#     return scores\n\n# # get the models to evaluate\n# models = get_models()\n# # evaluate the models and store results\n# results, names = list(), list()\n# for name, model in models.items():\n#     t1 = time.time()\n#     scores = evaluate_model(model)\n#     results.append(scores)\n#     names.append(name)\n#     t1 = time.time()\n#     print('>%s %.3f (%.3f) time taken: %.2f' % (name, mean(scores), std(scores), t2-t1))\n# # plot model performance for comparison\n# pyplot.boxplot(results, labels=names, showmeans=True)\n# pyplot.show()","3cefa564":"# # define the final model\n# model = LGBMRegressor(n_estimators=500, max_depth=7, num_leaves=2**7, learning_rate=0.1, boosting_type='gbdt')\n\n# # evaluate a give model using cross-validation\n# t1 = time.time()\n# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# scores = cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1, error_score='raise')\n# t2 = time.time()\n# print('>Final model performance: %.3f (%.3f) time taken: %.2f' % (mean(scores), std(scores), t2-t1))","76815c90":"# # Train the final model with whole dataset\n# model.fit(X, y)","6832124a":"# # Use the model to generate predictions\n# predictions = model.predict(X_test)\n\n# # Save the predictions to a CSV file\n# output = pd.DataFrame({'Id': X_test.index,\n#                        'target': predictions})\n# output.to_csv('submission.csv', index=False)","8c0b9622":"#### Explore Boosting Type\nA feature of LightGBM is that it supports a number of different boosting algorithms, referred to as boosting types.\n\nThe boosting type can be specified via the \u201cboosting_type\u201d argument and take a string to specify the type. The options include:\n\n*     \u2018gbdt\u2018: Gradient Boosting Decision Tree (GDBT).\n*     \u2018dart\u2018: Dropouts meet Multiple Additive Regression Trees (DART).\n*     \u2018goss\u2018: Gradient-based One-Side Sampling (GOSS).\n\nThe default is GDBT, which is the classical gradient boosting algorithm.","1ae70f91":"Welcome to the **[30 Days of ML competition](https:\/\/www.kaggle.com\/c\/30-days-of-ml\/overview)**!  In this notebook, you'll learn how to make your first submission.\n\nBefore getting started, make your own editable copy of this notebook by clicking on the **Copy and Edit** button.\n\n# Step 1: Import helpful libraries\n\nWe begin by importing the libraries we'll need.  Some of them will be familiar from the **[Intro to Machine Learning](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning)** course and the **[Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)** course.","40dd8971":"#### Explore Tree Depth\nVarying the depth of each tree added to the ensemble is another important hyperparameter for gradient boosting. The tree depth controls how specialized each tree is to the training dataset: how general or overfit it might be. \n\nTree depth is controlled via the \u201cmax_depth\u201d argument and defaults to an unspecified value as the default mechanism for controlling how complex trees are is to use the number of leaf nodes.\n\nThere are two main ways to control tree complexity: the max depth of the trees and the maximum number of terminal nodes (leaves) in the tree. In this case, we are exploring the number of leaves so we need to increase the number of leaves to support deeper trees by setting the \u201cnum_leaves\u201d argument.\n\nThe example below explores tree depths between 1 and 10 and the effect on model performance.","0e51cd57":"### LightGBM Hyperparameters\nIn this section, we will take a closer look at some of the hyperparameters you should consider tuning for the LightGBM ensemble and their effect on model performance.\n\nThere are many hyperparameters we can look at for LightGBM, although in this case, we will look at the number of trees and tree depth, the learning rate, and the boosting type.","13ef0faf":"The next code cell separates the target (which we assign to `y`) from the training features (which we assign to `features`).","13d2d8ac":"#### Now we will create final LightGBM model based on above observations:","991004e5":"# Step 6: Keep Learning!\n\nIf you're not sure what to do next, you can begin by trying out more model types!\n1. If you took the **[Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)** course, then you learned about **[XGBoost](https:\/\/www.kaggle.com\/alexisbcook\/xgboost)**.  Try training a model with XGBoost, to improve over the performance you got here.\n\n2. Take the time to learn about **Light GBM (LGBM)**, which is similar to XGBoost, since they both use gradient boosting to iteratively add decision trees to an ensemble.  In case you're not sure how to get started, **[here's a notebook](https:\/\/www.kaggle.com\/svyatoslavsokolov\/tps-feb-2021-lgbm-simple-version)** that trains a model on a similar dataset.","1d85aa63":"# Step 5: Submit to the competition\n\nWe'll begin by using the trained model to generate predictions, which we'll save to a CSV file.","7e03f5c9":"Next, we break off a validation set from the training data.","bcd6e9cd":"# Step 2: Load the data\n\nNext, we'll load the training and test data.  \n\nWe set `index_col=0` in the code cell below to use the `id` column to index the DataFrame.  (*If you're not sure how this works, try temporarily removing `index_col=0` and see how it changes the result.*)","cf06b941":"Once you have run the code cell above, follow the instructions below to submit to the competition:\n1. Begin by clicking on the **Save Version** button in the top right corner of the window.  This will generate a pop-up window.  \n2. Ensure that the **Save and Run All** option is selected, and then click on the **Save** button.\n3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the **Submit** button to submit your results to the leaderboard.\n\nYou have now successfully submitted to the competition!\n\nIf you want to keep working to improve your performance, select the **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.","79f8c1e0":"#### Explore Learning Rate\nLearning rate controls the amount of contribution that each model has on the ensemble prediction.\n\nSmaller rates may require more decision trees in the ensemble.\n\nThe learning rate can be controlled via the \u201clearning_rate\u201d argument and defaults to 0.1.\n\nThe example below explores the learning rate and compares the effect of values between 0.0001 and 1.0.","ecf5d88c":"#### Explore Number of Trees\nAn important hyperparameter for the LightGBM ensemble algorithm is the number of decision trees used in the ensemble.\n\nRecall that decision trees are added to the model sequentially in an effort to correct and improve upon the predictions made by prior trees. As such, more trees are often better.\n\nThe number of trees can be set via the \u201cn_estimators\u201d argument and defaults to 100.\n\nThe example below explores the effect of the number of trees with values between 10 to 5,000.","1e19e37f":"# Step 4: Train a model\n\nNow that the data is prepared, the next step is to train a model.  \n\nIf you took the **[Intro to Machine Learning](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning)** courses, then you learned about **[Random Forests](https:\/\/www.kaggle.com\/dansbecker\/random-forests)**.  In the code cell below, we fit a random forest model to the data.","e9bbe5cd":"Slightly tuned the LGBM parameters from this kernel https:\/\/www.kaggle.com\/awwalmalhi\/extreme-fine-tuning-lgbm-using-7-step-training","9b9d9edc":"# Step 3: Prepare the data\n\nNext, we'll need to handle the categorical columns (`cat0`, `cat1`, ... `cat9`).  \n\nIn the **[Categorical Variables lesson](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables)** in the Intermediate Machine Learning course, you learned several different ways to encode categorical variables in a dataset.  In this notebook, we'll use ordinal encoding and save our encoded features as new variables `X` and `X_test`."}}