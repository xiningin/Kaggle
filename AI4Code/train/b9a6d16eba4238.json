{"cell_type":{"493a5745":"code","e33ab6cf":"code","26379321":"markdown"},"source":{"493a5745":"\"\"\"\n## Setup\n\"\"\"\nimport os\nimport re\nimport json\nimport string\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn import preprocessing\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizer, TFBertModel, BertConfig\n\nmax_len = 384\nconfiguration = BertConfig()\ndata_csv = \"..\/input\/entity-annotated-corpus\/ner_dataset.csv\"\n\n\n\"\"\"\n## Setup Tokenizers\n\"\"\"\n\n# Save the slow pretrained tokenizer\nslow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nsave_path = \"bert_base_uncased\/\"\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\nslow_tokenizer.save_pretrained(save_path)\n\n# Load the fast tokenizer from saved file\ntokenizer = BertWordPieceTokenizer(\"bert_base_uncased\/vocab.txt\", lowercase=True)\n\n\"\"\"\n## Define model\n\"\"\"\n\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=False, reduction=tf.keras.losses.Reduction.NONE\n)\n\ndef masked_ce_loss(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 17))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)\n\ndef create_model(num_tags):\n    ## BERT encoder\n    encoder = TFBertModel.from_pretrained(\"bert-base-uncased\")\n\n    ## NER Model\n    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n    embedding = encoder(\n        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n    )[0]\n    embedding = layers.Dropout(0.3)(embedding)\n    tag_logits = layers.Dense(num_tags+1, activation='softmax')(embedding)\n    \n    model = keras.Model(\n        inputs=[input_ids, token_type_ids, attention_mask],\n        outputs=[tag_logits],\n    )\n    optimizer = keras.optimizers.Adam(lr=3e-5)\n    model.compile(optimizer=optimizer, loss=masked_ce_loss, metrics=['accuracy'])\n    return model\n\n\"\"\"\n## Preprocess dataset\n\"\"\"\n\ndef process_csv(data_path):\n    df = pd.read_csv(data_path, encoding=\"latin-1\")\n    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n    enc_tag = preprocessing.LabelEncoder()\n    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"])\n    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n    tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n    return sentences, tag, enc_tag\n\n\ndef create_inputs_targets(data_csv):\n    dataset_dict = {\n        \"input_ids\": [],\n        \"token_type_ids\": [],\n        \"attention_mask\": [],\n        \"tags\": []\n    }\n    sentences, tags, tag_encoder = process_csv(data_csv)\n    \n    for sentence, tag in zip(sentences, tags):\n        input_ids = []\n        target_tags = []\n        for idx, word in enumerate(sentence):\n            ids = tokenizer.encode(word, add_special_tokens=False)\n            input_ids.extend(ids.ids)\n            num_tokens = len(ids)\n            target_tags.extend([tag[idx]] * num_tokens)\n        \n        \n        # Pad truncate\n        input_ids = input_ids[:max_len - 2]\n        target_tags = target_tags[:max_len - 2]\n\n        input_ids = [101] + input_ids + [102]\n        target_tags = [16] + target_tags + [16]\n        token_type_ids = [0] * len(input_ids)\n        attention_mask = [1] * len(input_ids)\n        padding_len = max_len - len(input_ids)\n\n        input_ids = input_ids + ([0] * padding_len)\n        attention_mask = attention_mask + ([0] * padding_len)\n        token_type_ids = token_type_ids + ([0] * padding_len)\n        target_tags = target_tags + ([17] * padding_len)\n        \n        dataset_dict[\"input_ids\"].append(input_ids)\n        dataset_dict[\"token_type_ids\"].append(token_type_ids)\n        dataset_dict[\"attention_mask\"].append(attention_mask)\n        dataset_dict[\"tags\"].append(target_tags)\n        assert len(target_tags) == max_len, f'{len(input_ids)}, {len(target_tags)}'\n        \n    for key in dataset_dict:\n        dataset_dict[key] = np.array(dataset_dict[key])\n\n    x = [\n        dataset_dict[\"input_ids\"],\n        dataset_dict[\"token_type_ids\"],\n        dataset_dict[\"attention_mask\"],\n    ]\n    y = dataset_dict[\"tags\"]\n    return x, y, tag_encoder\n\n\n\"\"\"\n## Create model\n\"\"\"\n\nnum_tags = pd.read_csv(data_csv, encoding=\"latin-1\")[\"Tag\"].nunique()\n\nuse_tpu = None\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    use_tpu = True\nexcept:\n    use_tpu = False\n\nif use_tpu:\n    # Create distribution strategy\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n    # Create model\n    with strategy.scope():\n        model = create_model(num_tags)\nelse:\n    model = create_model(num_tags)\n    \nmodel.summary()\n\n\"\"\"\n## Train\n\"\"\"\n\nx_train, y_train, tag_encoder = create_inputs_targets(data_csv)\n\nbs = 64 if use_tpu else 16\n\nmodel.fit(\n    x_train,\n    y_train,\n    epochs=1,\n    verbose=1,\n    batch_size=bs,\n    validation_split=0.1\n)","e33ab6cf":"\"\"\"\n## Inference\n\"\"\"\n\ndef create_test_input_from_text(texts):\n    dataset_dict = {\n        \"input_ids\": [],\n        \"token_type_ids\": [],\n        \"attention_mask\": []\n    }\n    for sentence in texts:\n        input_ids = []\n        for idx, word in enumerate(sentence.split()):\n            ids = tokenizer.encode(word, add_special_tokens=False)\n            input_ids.extend(ids.ids)\n            num_tokens = len(ids)\n            \n        # Pad and create attention masks.\n        # Skip if truncation is needed\n        input_ids = input_ids[:max_len - 2]\n\n        input_ids = [101] + input_ids + [102]\n        n_tokens = len(input_ids)\n        token_type_ids = [0] * len(input_ids)\n        attention_mask = [1] * len(input_ids)\n        padding_len = max_len - len(input_ids)\n\n        input_ids = input_ids + ([0] * padding_len)\n        attention_mask = attention_mask + ([0] * padding_len)\n        token_type_ids = token_type_ids + ([0] * padding_len)\n        \n        dataset_dict[\"input_ids\"].append(input_ids)\n        dataset_dict[\"token_type_ids\"].append(token_type_ids)\n        dataset_dict[\"attention_mask\"].append(attention_mask)\n        \n    for key in dataset_dict:\n        dataset_dict[key] = np.array(dataset_dict[key])\n\n    x = [\n        dataset_dict[\"input_ids\"],\n        dataset_dict[\"token_type_ids\"],\n        dataset_dict[\"attention_mask\"],\n    ]\n    return x, n_tokens\n\n\ntest_inputs = [\"alex lives in london\"]\nx_test, n_tokens = create_test_input_from_text(test_inputs)\nprint('input tokens')\nprint(x_test[0][0][:n_tokens])\npred_test = model.predict(x_test)\npred_tags = np.argmax(pred_test,2)[0][:n_tokens]  # ignore predictions of padding tokens\n\n# create dictionary of tag and its index\nle_dict = dict(zip(tag_encoder.transform(tag_encoder.classes_), tag_encoder.classes_))\nprint('predicted tags')\nprint([le_dict.get(_, '[pad]') for _ in pred_tags])\n","26379321":"# Entity Recognition with BERT\nPlease refer to [this blog post](https:\/\/apoorvnandan.github.io\/2020\/08\/02\/bert-ner\/) for details.\n\n- Copied some preprocessing code from this awesome notebook. https:\/\/www.kaggle.com\/abhishek\/entity-extraction-model-using-bert-pytorch"}}