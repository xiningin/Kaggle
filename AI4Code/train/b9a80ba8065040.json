{"cell_type":{"3662a65c":"code","f9c81331":"code","1e3f0625":"code","b52f2564":"code","bffc882f":"code","53907134":"code","1fc20b99":"code","8760f315":"code","22a8f105":"code","f91866d7":"code","af800a61":"code","269a0145":"markdown","d4b6bc31":"markdown","9958c2ea":"markdown","8188605a":"markdown"},"source":{"3662a65c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f9c81331":"#import the libraries\nimport glob\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport cv2\nimport torch\nimport PIL","1e3f0625":"#loading the annotations file\nkey = pd.read_csv('..\/input\/input-key\/training_frames_keypoints.csv')\nkey.head()\n","b52f2564":"#UNCOMMENT BELOW COMMAND TO INSTALL imgaug lib\n#!pip install imgaug\nfrom imgaug import augmenters as iaa\nimport imgaug as ia","bffc882f":"class ImgAugTransform:\n  def __init__(self):\n    sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n    self.aug = iaa.Sequential([\n        iaa.Sometimes(0.25, iaa.GaussianBlur(sigma=(0, 2.0))),#sigma was 3.0 initially\n        iaa.Fliplr(0.4),# flip was 0.5 initially\n        iaa.Affine(rotate=(-20, 20), mode='symmetric'),\n        iaa.Sometimes(0.3,\n                      iaa.OneOf([iaa.Dropout(p=(0, 0.1)), # in place of 0.3, 0.2 was placed\n                                 iaa.CoarseDropout(0.1, size_percent=0.5)])),\n        iaa.AddToHueAndSaturation(value=(-10, 10), per_channel=True),\n         # Apply affine transformations to some of the images\n        # - scale to 80-120% of image height\/width (each axis independently)\n        # - translate by -20 to +20 relative to height\/width (per axis)\n        # - rotate by -45 to +45 degrees\n        # - shear by -16 to +16 degrees\n        # - order: use nearest neighbour or bilinear interpolation (fast)\n        # - mode: use any available mode to fill newly created pixels\n        #         see API or scikit-image for which modes are available\n        # - cval: if the mode is constant, then use a random brightness\n        #         for the newly created pixels (e.g. sometimes black,\n        #         sometimes white)\n        sometimes(iaa.Affine(\n            scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n            translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n            rotate=(-45, 45),\n            shear=(-16, 16),\n            order=[0, 1],\n            cval=(0, 255),\n            mode=ia.ALL\n        )),\n\n        #\n        # Execute 0 to 5 of the following (less important) augmenters per\n        # image. Don't execute all of them, as that would often be way too\n        # strong.\n        #\n        iaa.SomeOf((0, 5),\n            [\n                # Convert some images into their superpixel representation,\n                # sample between 20 and 200 superpixels per image, but do\n                # not replace all superpixels with their average, only\n                # some of them (p_replace).\n                sometimes(\n                    iaa.Superpixels(\n                        p_replace=(0, 1.0),\n                        n_segments=(20, 200)\n                    )\n                ),\n\n                # Blur each image with varying strength using\n                # gaussian blur (sigma between 0 and 3.0),\n                # average\/uniform blur (kernel size between 2x2 and 7x7)\n                # median blur (kernel size between 3x3 and 11x11).\n                iaa.OneOf([\n                    iaa.GaussianBlur((0, 3.0)),\n                    iaa.AverageBlur(k=(2, 7)),\n                    iaa.MedianBlur(k=(3, 11)),\n                ]),\n\n                # Sharpen each image, overlay the result with the original\n                # image using an alpha between 0 (no sharpening) and 1\n                # (full sharpening effect).\n                iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)),\n\n                # Same as sharpen, but for an embossing effect.\n                iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)),\n\n                # Search in some images either for all edges or for\n                # directed edges. These edges are then marked in a black\n                # and white image and overlayed with the original image\n                # using an alpha of 0 to 0.7.\n                sometimes(iaa.OneOf([\n                    iaa.EdgeDetect(alpha=(0, 0.7)),\n                    iaa.DirectedEdgeDetect(\n                        alpha=(0, 0.7), direction=(0.0, 1.0)\n                    ),\n                ])),\n\n                # Add gaussian noise to some images.\n                # In 50% of these cases, the noise is randomly sampled per\n                # channel and pixel.\n                # In the other 50% of all cases it is sampled once per\n                # pixel (i.e. brightness change).\n                iaa.AdditiveGaussianNoise(\n                    loc=0, scale=(0.0, 0.05*255), per_channel=0.5\n                ),\n\n                # Either drop randomly 1 to 10% of all pixels (i.e. set\n                # them to black) or drop them on an image with 2-5% percent\n                # of the original size, leading to large dropped\n                # rectangles.\n                iaa.OneOf([\n                    iaa.Dropout((0.01, 0.1), per_channel=0.5),\n                    iaa.CoarseDropout(\n                        (0.03, 0.15), size_percent=(0.02, 0.05),\n                        per_channel=0.2\n                    ),\n                ]),\n\n                # Invert each image's channel with 5% probability.\n                # This sets each pixel value v to 255-v.\n                iaa.Invert(0.05, per_channel=True), # invert color channels\n\n                # Add a value of -10 to 10 to each pixel.\n                iaa.Add((-10, 10), per_channel=0.5),\n\n                # Change brightness of images (50-150% of original value).\n                iaa.Multiply((0.5, 1.5), per_channel=0.5),\n\n\n                # Convert each image to grayscale and then overlay the\n                # result with the original with random alpha. I.e. remove\n                # colors with varying strengths.\n                iaa.Grayscale(alpha=(0.0, 1.0)),\n\n                # In some images move pixels locally around (with random\n                # strengths).\n                sometimes(\n                    iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)\n                ),\n\n                # In some images distort local areas with varying strength.\n                sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05)))\n            ],\n            # do all of the above augmentations in random order\n            random_order=True\n        )\n    ],\n    # do all of the above augmentations in random order\n    random_order=True\n)\n\n    \n      \n  def __call__(self, sample):\n    image, key_pts = sample['image'], sample['keypoints']\n    img = np.array(image)\n    return {'image': self.aug.augment_image(img), 'keypoints': key_pts}\n    #return self.aug.augment_image(img)","53907134":"\nclass ToTensor(object):\n\n    def __call__(self, sample):\n        image, key_pts = sample['image'], sample['keypoints']\n         \n        # if image has no grayscale color channel, add one\n        if(len(image.shape) == 2):\n            # add that third color dim\n            image = image.reshape(image.shape[0], image.shape[1], 3)\n            \n        # swap color axis because\n        # numpy image: H x W x C\n        # torch image: C X H X W\n        image = image.transpose((2, 0, 1))\n        \n        return {'image': torch.from_numpy(image),\n                'keypoints': torch.from_numpy(key_pts)}\n\n#FUNCTION TO PERFORM NORMALIZATION\nclass Normalize(object):      \n\n    def __call__(self, sample):\n        image, key_pts = sample['image'], sample['keypoints']\n        \n        image_copy = np.copy(image)\n        key_pts_copy = np.copy(key_pts)\n\n        # convert image to grayscale\n        #image_copy = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        \n        # scale color range from [0, 255] to [0, 1]\n        image_copy=  image_copy\/255.0\n        #image_copy = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n        # scale keypoints to be centered around 0 with a range of [-1, 1]\n        # mean = 100, sqrt = 50, so, pts should be (pts - 100)\/50\n        key_pts_copy = (key_pts_copy - 100)\/50.0\n\n\n        return {'image': image_copy, 'keypoints': key_pts_copy}\n    \n#RESCALING FUNCTION    \nclass Rescale(object):\n    \n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        self.output_size = output_size\n\n    def __call__(self, sample):\n        image, key_pts = sample['image'], sample['keypoints']\n\n        h, w = image.shape[:2]\n        if isinstance(self.output_size, int):\n            if h > w:\n                new_h, new_w = self.output_size * h \/ w, self.output_size\n            else:\n                new_h, new_w = self.output_size, self.output_size * w \/ h\n        else:\n            new_h, new_w = self.output_size\n\n        new_h, new_w = int(new_h), int(new_w)\n\n        img = cv2.resize(image, (new_w, new_h))\n        \n        # scale the pts, too\n        key_pts = key_pts * [new_w \/ w, new_h \/ h]\n\n        return {'image': img, 'keypoints': key_pts}\n    \n    \n#FUNCTION TO RANDOMLY CROP IMAGE \nclass RandomCrop(object):\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        if isinstance(output_size, int):\n            self.output_size = (output_size, output_size)\n        else:\n            assert len(output_size) == 2\n            self.output_size = output_size\n\n    def __call__(self, sample):\n        image, key_pts = sample['image'], sample['keypoints']\n\n        h, w = image.shape[:2]\n        new_h, new_w = self.output_size\n\n        top = np.random.randint(0, h - new_h)\n        left = np.random.randint(0, w - new_w)\n\n        image = image[top: top + new_h,\n                      left: left + new_w]\n\n        key_pts = key_pts - [left, top]\n\n        return {'image': image, 'keypoints': key_pts}\n\n","1fc20b99":"#ist column contain name of the file image and all other columns \n#contain the x and y axis of keypoints\n#so we will separate them\n#create a function to make a dataset of form A sample of our dataset will be a dictionary {'image': image, 'keypoints': key_pts}\nfrom torch.utils.data import DataLoader , Dataset\nfrom torchvision import transforms, utils\n\nclass facialdataset(Dataset):\n    def __init__(self,csv_file , root_dir , transform  = None):\n        self.key_csv = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n    #function to return the length of keypoints dictionary\n    def __len__(self):\n        return len(self.key_csv)\n    def __getitem__(self,idx):\n        #append root dir and image name from csv to fetch image\n        image_name = os.path.join(self.root_dir , self.key_csv.iloc[idx,0])\n        image = mpimg.imread(image_name)\n        #remove last channel if image have 4 channels instead of 3\n        if image.shape[2] == 4:\n            image = image[:,:,0:3]\n        #now convert the cords in matrix and the reshape\n        key_cords = self.key_csv.iloc[idx,1:].to_numpy()\n        key_cords = key_cords.astype('float').reshape(-1,2)\n        dictionary = {'image':image , 'keypoints':key_cords}\n        \n        #applying transform is not none:\n        if self.transform:\n            dictionary= self.transform(dictionary)\n        return dictionary\n# define the data tranform\n# order matters! i.e. rescaling should come before a smaller crop\ntrain_transforms = transforms.Compose([Rescale(250),\n                                       ImgAugTransform(),#add the custom augmetations in the pytorch transform pipeline\n                                        RandomCrop(224),\n                                         Normalize(),\n                                      ToTensor()])","8760f315":"#after creating the class, we will pass the images and labels directory\n#in the class to get the dictionary\nface_dataset_train = facialdataset(csv_file='..\/input\/input-key\/training_frames_keypoints.csv',\n                            root_dir='..\/input\/input-key\/training\/training\/',transform=train_transforms)\nface_dataset_test = facialdataset(csv_file = '..\/input\/input-key\/test_frames_keypoints.csv',\n                                 root_dir = '..\/input\/input-key\/test\/test\/',\n                                 transform=train_transforms)","22a8f105":"print('length of train data' , len(face_dataset_train))\nprint('length of test data' , len(face_dataset_test))","f91866d7":"print(\"train\" , face_dataset_train)","af800a61":"print('number of images in train' , len(face_dataset_train))\nprint('number of images in test' , len(face_dataset_test))\nfor i in range(1,5):\n    sample = face_dataset_test[i]\n    print(i , sample['image'].size() , sample['keypoints'].size() )","269a0145":"# PROCESSING THE DATASET AND CONVERTING INTO A PROPER FORMAT\n****MAKING DICTIONARY CONTAINING IMAGES WITH THIER KEYPOINTS IN (X,Y) FORM\n****HERE X AND Y ARE THE CORDINATES OF A KEYPOINT","d4b6bc31":"# ******BUILD MODEL AS PER THE TASK.\n","9958c2ea":"# APPLY THE ABOVE DEFINED TRANSFORMATION ON THE DATASET\nHERE I AM USING A DATASET CONTAINING IMAGES OF FACES WITH THEIR RESPECTIVE KEYPOINTS AS (X,Y) FORM IN A CSV FILE","8188605a":"# DATA AUGMENTATION PIPELINE USING IMGAUG LIB FROM PYPI\n"}}