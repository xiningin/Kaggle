{"cell_type":{"1a032116":"code","8d861e6c":"code","5218ba58":"code","ab67463f":"code","e91dccfc":"code","9e148e7f":"code","07afcb76":"code","0b22288a":"code","81054e60":"code","7598cbc8":"code","b5c68b9f":"code","4151b612":"code","1fec4545":"code","270ee24d":"code","008380b2":"code","a31065db":"code","ff432aaf":"code","313a0eb4":"code","0d2fd5be":"code","47f4c9d7":"code","65bd03cc":"code","0df7cc87":"code","91157f2e":"code","2d11af48":"code","659b5ece":"code","600d9a6c":"code","1231a962":"markdown","d4d56ff9":"markdown","6582e315":"markdown","cdd82e88":"markdown","a447b69f":"markdown","427565d3":"markdown","f142af57":"markdown","edabeb9a":"markdown","7ce63672":"markdown","4612da5c":"markdown","67800d44":"markdown","709aee76":"markdown","dfba0b20":"markdown","0eb5e11a":"markdown","28a7d751":"markdown","a43de6aa":"markdown","3fc7995e":"markdown","6a1771e2":"markdown","da7c99a0":"markdown","016a950e":"markdown","f46181ee":"markdown","e051afee":"markdown","03b371aa":"markdown","8e1e33d2":"markdown"},"source":{"1a032116":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\n\nimport re \nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport nltk \nfrom sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\nimport nltk.corpus\nfrom nltk.corpus import stopwords\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('stopwords')\nimport string\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom nltk.stem import PorterStemmer \nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import WordCloud\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pickle\nimport statsmodels.api as sm","8d861e6c":"train_data = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_data  = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntrain_data.info()","5218ba58":"train_data.head(10)","ab67463f":"train_data.info()","e91dccfc":"test_data.info()","9e148e7f":"train_data.dtypes","07afcb76":"test_data.dtypes","0b22288a":"x=train_data.target.value_counts()\nsns.barplot(x.index,x)\nplt.title('number of tweet')","81054e60":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train_data[train_data['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='blue')\nax1.set_title('disaster tweets')\ntweet_len=train_data[train_data['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='CRIMSON')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","7598cbc8":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train_data[train_data['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='blue')\nax1.set_title('disaster tweets')\ntweet_len=train_data[train_data['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='CRIMSON')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweet')\nplt.show()","b5c68b9f":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=train_data[train_data['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('disaster')\nword=train_data[train_data['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Not disaster')\nfig.suptitle('Average word length in each tweet')","4151b612":"corpus=[]\n    \nfor x in train_data['text'].str.split():\n    for i in x:\n        corpus.append(i)","1fec4545":"dic=defaultdict(int)\ndic=defaultdict(int)\nfor word in corpus:\n    if word not in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:30] \n    \n\n\nx,y=zip(*top)\nplt.rcParams[\"figure.figsize\"] = (20,10)\nplt.bar(x,y , color ='red')","270ee24d":"from nltk.corpus import stopwords\nstop = stopwords.words('english')\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:30] \n    \n\n\nx,y=zip(*top)\nplt.rcParams[\"figure.figsize\"] = (20,10)\n#There is also this workaround in case you want to change the size without using the figure environment.\n#So in case you are using plt.plot() for example, you can set a tuple with width and height.\nplt.bar(x,y , color ='green')","008380b2":"plt.figure(figsize=(10,5))\nimport string\ndic=defaultdict(int)\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n      \nx,y=zip(*dic.items())\nplt.barh(x,y ,color = 'purple')","a31065db":"contractions = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"i'd\": \"i would\",\n\"i'll\": \"i will\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'll\": \"it will\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"needn't\": \"need not\",\n\"oughtn't\": \"ought not\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"she'd\": \"she would\",\n\"she'll\": \"she will\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"that'd\": \"that would\",\n\"that's\": \"that is\",\n\"there'd\": \"there had\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'll\": \"they will\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'll\": \"we will\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"who'll\": \"who will\",\n\"who's\": \"who is\",\n\"won't\": \"will not\",\n\"wouldn't\": \"would not\",\n\"you'd\": \"you would\",\n\"you'll\": \"you will\",\n\"you're\": \"you are\"\n}","ff432aaf":"from nltk.corpus import stopwords\nstop = stopwords.words('english')","313a0eb4":"def Expand_contraction(text):\n  if True:\n        text = text.split()\n        new_text = []\n        for word in text:\n            if word in contractions:\n                new_text.append(contractions[word])\n            else:\n                new_text.append(word)\n        text = \" \".join(new_text)\n  return text\n","0d2fd5be":"def  clean_text(df, text_field, new_text_field_name):\n    df[new_text_field_name] = df[text_field].str.lower() #Convert strings in the Series\/Index to lowercase.\n    \n    # remove numbers\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text: re.sub(r\"\\d+\", \"\", text))\n    #Expandsion text\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text : Expand_contraction(text))\n    #removestop word\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n    #remove unwanted character\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text: re.sub(r\"https?:\/\/\\S+|www\\.\\S+\", \"\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text: re.sub(r'\\<a href', ' ', text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text: re.sub(r'&amp;', '', text) )\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text: re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]\/]', ' ', text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text: re.sub(r'<br \/>', ' ', text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text: re.sub(r'\\'', ' ', text))\n    #remove HTML tags\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text: re.sub(r\"<.*?>\", \"\", text))\n    #remove emojis \n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text: re.sub(r\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", \"\", text))\n        # Hashtags and usernames\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"IranDeal\", \"Iran Deal\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ArianaGrande\", \"Ariana Grande\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"camilacabello97\", \"camila cabello\", text)) \n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"RondaRousey\", \"Ronda Rousey\", text))     \n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"MTVHottest\", \"MTV Hottest\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"TrapMusic\", \"Trap Music\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"PantherAttack\", \"Panther Attack\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"StrategicPatience\", \"Strategic Patience\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"socialnews\", \"social news\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"NASAHurricane\", \"NASA Hurricane\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"onlinecommunities\", \"online communities\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"humanconsumption\", \"human consumption\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Meat-Loving\", \"Meat Loving\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"facialabuse\", \"facial abuse\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"LakeCounty\", \"Lake County\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"BeingAuthor\", \"Being Author\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"withheavenly\", \"with heavenly\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"thankU\", \"thank you\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"iTunesMusic\", \"iTunes Music\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"OffensiveContent\", \"Offensive Content\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"NASASolarSystem\", \"NASA Solar System\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"animalrescue\", \"animal rescue\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"aRmageddon\", \"armageddon\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Throwingknifes\", \"Throwing knives\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"GodsLove\", \"God's Love\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"bookboost\", \"book boost\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ibooklove\", \"I book love\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"NestleIndia\", \"Nestle India\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"realDonaldTrump\", \"Donald Trump\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"CecilTheLion\", \"Cecil The Lion\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"weathernetwork\", \"weather network\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Hostage&2\", \"Hostage & 2\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"GOPDebate\", \"GOP Debate\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"RickPerry\", \"Rick Perry\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"frontpage\", \"front page\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"NewsInTweets\", \"News In Tweets\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ViralSpell\", \"Viral Spell\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"til_now\", \"until now\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"volcanoinRussia\", \"volcano in Russia\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ZippedNews\", \"Zipped News\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"MicheleBachman\", \"Michele Bachman\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"53inch\", \"53 inch\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"KerrickTrial\", \"Kerrick Trial\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"abstorm\", \"Alberta Storm\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Beyhive\", \"Beyonce hive\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"IDFire\", \"Idaho Fire\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"DETECTADO\", \"Detected\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"RockyFire\", \"Rocky Fire\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Listen\/Buy\", \"Listen \/ Buy\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"NickCannon\", \"Nick Cannon\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"FaroeIslands\", \"Faroe Islands\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"yycstorm\", \"Calgary Storm\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"IDPs:\", \"Internally Displaced People :\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ArtistsUnited\", \"Artists United\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"jimmyfallon\", \"jimmy fallon\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"justinbieber\", \"justin bieber\", text))  \n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"UTC2015\", \"UTC 2015\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Time2015\", \"Time 2015\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"djicemoon\", \"dj icemoon\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"LivingSafely\", \"Living Safely\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"FIFA16\", \"Fifa 2016\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"bbcnews\", \"bbc news\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"c4news\", \"c4 news\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"OBLITERATION\", \"obliteration\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"MUDSLIDE\", \"mudslide\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"NoSurrender\", \"No Surrender\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"NotExplained\", \"Not Explained\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"greatbritishbakeoff\", \"great british bake off\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"LondonFire\", \"London Fire\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"KOTAWeather\", \"KOTA Weather\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"LuchaUnderground\", \"Lucha Underground\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"KOIN6News\", \"KOIN 6 News\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"LiveOnK2\", \"Live On K2\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"nikeplus\", \"nike plus\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"david_cameron\", \"David Cameron\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"peterjukes\", \"Peter Jukes\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"JamesMelville\", \"James Melville\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"megynkelly\", \"Megyn Kelly\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"cnewslive\", \"C News Live\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"TweetLikeItsSeptember11th2001\", \"Tweet like it is september 11th 2001\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"cbplawyers\", \"cbp lawyers\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"fewmoretweets\", \"few more tweets\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"cjoyner\", \"Chris Joyner\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ENGvAUS\", \"England vs Australia\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ScottWalker\", \"Scott Walker\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"MikeParrActor\", \"Michael Parr\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"realmandyrain\", \"Mandy Rain\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"GraysonDolan\", \"Grayson Dolan\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ApolloBrown\", \"Apollo Brown\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"saddlebrooke\", \"Saddlebrooke\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"TontitownGrape\", \"Tontitown Grape\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"AbbsWinston\", \"Abbs Winston\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ShaunKing\", \"Shaun King\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"MeekMill\", \"Meek Mill\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"GRupdates\", \"GR updates\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"SouthDowns\", \"South Downs\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"braininjury\", \"brain injury\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"auspol\", \"Australian politics\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"calgaryweather\", \"Calgary Weather\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"weallheartonedirection\", \"we all heart one direction\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"edsheeran\", \"Ed Sheeran\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"TrueHeroes\", \"True Heroes\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"S3XLEAK\", \"sex leak\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ComplexMag\", \"Complex Magazine\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"CityofCalgary\", \"City of Calgary\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"SummerFate\", \"Summer Fate\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"RAmag\", \"Royal Academy Magazine\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"offers2go\", \"offers to go\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"foodscare\", \"food scare\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"MNPDNashville\", \"Metropolitan Nashville Police Department\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"TfLBusAlerts\", \"TfL Bus Alerts\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"GamerGate\", \"Gamer Gate\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"IHHen\", \"Humanitarian Relief\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"spinningbot\", \"spinning bot\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ModiMinistry\", \"Modi Ministry\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"TAXIWAYS\", \"taxi ways\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Calum5SOS\", \"Calum Hood\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"po_st\", \"po.st\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"scoopit\", \"scoop.it\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"UltimaLucha\", \"Ultima Lucha\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"JonathanFerrell\", \"Jonathan Ferrell\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"aria_ahrary\", \"Aria Ahrary\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"rapidcity\", \"Rapid City\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"OutBid\", \"outbid\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"lavenderpoetrycafe\", \"lavender poetry cafe\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"EudryLantiqua\", \"Eudry Lantiqua\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"15PM\", \"15 PM\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"OriginalFunko\", \"Funko\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"rightwaystan\", \"Richard Tan\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"CindyNoonan\", \"Cindy Noonan\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"RT_America\", \"RT America\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"narendramodi\", \"Narendra Modi\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"BakeOffFriends\", \"Bake Off Friends\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"TeamHendrick\", \"Hendrick Motorsports\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"alexbelloli\", \"Alex Belloli\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"itsjustinstuart\", \"Justin Stuart\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"gunsense\", \"gun sense\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"DebateQuestionsWeWantToHear\", \"debate questions we want to hear\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"RoyalCarribean\", \"Royal Carribean\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"samanthaturne19\", \"Samantha Turner\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"JonVoyage\", \"Jon Stewart\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"renew911health\", \"renew 911 health\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"SuryaRay\", \"Surya Ray\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"pattonoswalt\", \"Patton Oswalt\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"minhazmerchant\", \"Minhaz Merchant\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"TLVFaces\", \"Israel Diaspora Coalition\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"pmarca\", \"Marc Andreessen\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"pdx911\", \"Portland Police\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"jamaicaplain\", \"Jamaica Plain\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Japton\", \"Arkansas\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"RouteComplex\", \"Route Complex\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"INSubcontinent\", \"Indian Subcontinent\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"NJTurnpike\", \"New Jersey Turnpike\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Politifiact\", \"PolitiFact\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Hiroshima70\", \"Hiroshima\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"GMMBC\", \"Greater Mt Moriah Baptist Church\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"versethe\", \"verse the\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"TubeStrike\", \"Tube Strike\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"MissionHills\", \"Mission Hills\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ProtectDenaliWolves\", \"Protect Denali Wolves\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"NANKANA\", \"Nankana\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"SAHIB\", \"Sahib\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"PAKPATTAN\", \"Pakpattan\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Newz_Sacramento\", \"News Sacramento\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"gofundme\", \"go fund me\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"pmharper\", \"Stephen Harper\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"IvanBerroa\", \"Ivan Berroa\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"LosDelSonido\", \"Los Del Sonido\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"bancodeseries\", \"banco de series\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"timkaine\", \"Tim Kaine\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"IdentityTheft\", \"Identity Theft\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"AllLivesMatter\", \"All Lives Matter\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"mishacollins\", \"Misha Collins\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"BillNeelyNBC\", \"Bill Neely\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"BeClearOnCancer\", \"be clear on cancer\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Kowing\", \"Knowing\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ScreamQueens\", \"Scream Queens\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"AskCharley\", \"Ask Charley\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"BlizzHeroes\", \"Heroes of the Storm\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"BradleyBrad47\", \"Bradley Brad\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"HannaPH\", \"Typhoon Hanna\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"meinlcymbals\", \"MEINL Cymbals\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Ptbo\", \"Peterborough\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"cnnbrk\", \"CNN Breaking News\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"IndianNews\", \"Indian News\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"savebees\", \"save bees\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"GreenHarvard\", \"Green Harvard\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"StandwithPP\", \"Stand with planned parenthood\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"hermancranston\", \"Herman Cranston\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"WMUR9\", \"WMUR-TV\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"RockBottomRadFM\", \"Rock Bottom Radio\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ameenshaikh3\", \"Ameen Shaikh\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ProSyn\", \"Project Syndicate\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Daesh\", \"ISIS\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"s2g\", \"swear to god\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"listenlive\", \"listen live\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"CDCgov\", \"Centers for Disease Control and Prevention\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"FoxNew\", \"Fox News\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"CBSBigBrother\", \"Big Brother\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"JulieDiCaro\", \"Julie DiCaro\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"theadvocatemag\", \"The Advocate Magazine\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"RohnertParkDPS\", \"Rohnert Park Police Department\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"THISIZBWRIGHT\", \"Bonnie Wright\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Popularmmos\", \"Popular MMOs\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"WildHorses\", \"Wild Horses\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"FantasticFour\", \"Fantastic Four\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"HORNDALE\", \"Horndale\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"PINER\", \"Piner\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"BathAndNorthEastSomerset\", \"Bath and North East Somerset\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"thatswhatfriendsarefor\", \"that is what friends are for\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"residualincome\", \"residual income\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"YahooNewsDigest\", \"Yahoo News Digest\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"MalaysiaAirlines\", \"Malaysia Airlines\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"AmazonDeals\", \"Amazon Deals\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"MissCharleyWebb\", \"Charley Webb\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"shoalstraffic\", \"shoals traffic\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"GeorgeFoster72\", \"George Foster\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"pop2015\", \"pop 2015\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"_PokemonCards_\", \"Pokemon Cards\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"DianneG\", \"Dianne Gallagher\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"KashmirConflict\", \"Kashmir Conflict\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"BritishBakeOff\", \"British Bake Off\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"FreeKashmir\", \"Free Kashmir\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"mattmosley\", \"Matt Mosley\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"BishopFred\", \"Bishop Fred\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"EndConflict\", \"End Conflict\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"EndOccupation\", \"End Occupation\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"UNHEALED\", \"unhealed\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"CharlesDagnall\", \"Charles Dagnall\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Latestnews\", \"Latest news\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"KindleCountdown\", \"Kindle Countdown\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"NoMoreHandouts\", \"No More Handouts\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"datingtips\", \"dating tips\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"charlesadler\", \"Charles Adler\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"twia\", \"Texas Windstorm Insurance Association\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"txlege\", \"Texas Legislature\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"WindstormInsurer\", \"Windstorm Insurer\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Newss\", \"News\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"hempoil\", \"hemp oil\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"CommoditiesAre\", \"Commodities are\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"tubestrike\", \"tube strike\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"JoeNBC\", \"Joe Scarborough\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"LiteraryCakes\", \"Literary Cakes\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"TI5\", \"The International 5\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"thehill\", \"the hill\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"3others\", \"3 others\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"stighefootball\", \"Sam Tighe\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"whatstheimportantvideo\", \"what is the important video\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ClaudioMeloni\", \"Claudio Meloni\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"DukeSkywalker\", \"Duke Skywalker\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"carsonmwr\", \"Fort Carson\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"offdishduty\", \"off dish duty\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"andword\", \"and word\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"rhodeisland\", \"Rhode Island\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"easternoregon\", \"Eastern Oregon\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"WAwildfire\", \"Washington Wildfire\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"57am\", \"57 am\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"JacobHoggard\", \"Jacob Hoggard\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"newnewnew\", \"new new new\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"under50\", \"under 50\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"getitbeforeitsgone\", \"get it before it is gone\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"freshoutofthebox\", \"fresh out of the box\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"amwriting\", \"am writing\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Bokoharm\", \"Boko Haram\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Nowlike\", \"Now like\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"seasonfrom\", \"season from\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"epicente\", \"epicenter\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"epicenterr\", \"epicenter\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"sicklife\", \"sick life\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"yycweather\", \"Calgary Weather\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"calgarysun\", \"Calgary Sun\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"approachng\", \"approaching\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"evng\", \"evening\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Sumthng\", \"something\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"EllenPompeo\", \"Ellen Pompeo\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"shondarhimes\", \"Shonda Rhimes\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ABCNetwork\", \"ABC Network\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"SushmaSwaraj\", \"Sushma Swaraj\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"pray4japan\", \"Pray for Japan\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"hope4japan\", \"Hope for Japan\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Illusionimagess\", \"Illusion images\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"SummerUnderTheStars\", \"Summer Under The Stars\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ShallWeDance\", \"Shall We Dance\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"TCMParty\", \"TCM Party\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"marijuananews\", \"marijuana news\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"onbeingwithKristaTippett\", \"on being with Krista Tippett\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Beingtweets\", \"Being tweets\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"newauthors\", \"new authors\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"remedyyyy\", \"remedy\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"44PM\", \"44 PM\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"HeadlinesApp\", \"Headlines App\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"40PM\", \"40 PM\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"myswc\", \"Severe Weather Center\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ithats\", \"that is\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"icouldsitinthismomentforever\", \"I could sit in this moment forever\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"FatLoss\", \"Fat Loss\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"02PM\", \"02 PM\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"MetroFmTalk\", \"Metro Fm Talk\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Bstrd\", \"bastard\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"bldy\", \"bloody\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"MetrofmTalk\", \"Metro Fm Talk\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"terrorismturn\", \"terrorism turn\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"BBCNewsAsia\", \"BBC News Asia\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"BehindTheScenes\", \"Behind The Scenes\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"GeorgeTakei\", \"George Takei\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"WomensWeeklyMag\", \"Womens Weekly Magazine\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"SurvivorsGuidetoEarth\", \"Survivors Guide to Earth\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"incubusband\", \"incubus band\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Babypicturethis\", \"Baby picture this\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"BombEffects\", \"Bomb Effects\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"win10\", \"Windows 10\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"idkidk\", \"I do not know I do not know\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"TheWalkingDead\", \"The Walking Dead\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"amyschumer\", \"Amy Schumer\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"crewlist\", \"crew list\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Erdogans\", \"Erdogan\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"BBCLive\", \"BBC Live\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"TonyAbbottMHR\", \"Tony Abbott\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"paulmyerscough\", \"Paul Myerscough\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"georgegallagher\", \"George Gallagher\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"JimmieJohnson\", \"Jimmie Johnson\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"pctool\", \"pc tool\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"DoingHashtagsRight\", \"Doing Hashtags Right\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ThrowbackThursday\", \"Throwback Thursday\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"SnowBackSunday\", \"Snowback Sunday\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"LakeEffect\", \"Lake Effect\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"RTphotographyUK\", \"Richard Thomas Photography UK\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"BigBang_CBS\", \"Big Bang CBS\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"writerslife\", \"writers life\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"NaturalBirth\", \"Natural Birth\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"UnusualWords\", \"Unusual Words\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"wizkhalifa\", \"Wiz Khalifa\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"acreativedc\", \"a creative DC\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"vscodc\", \"vsco DC\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"VSCOcam\", \"vsco camera\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"TheBEACHDC\", \"The beach DC\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"buildingmuseum\", \"building museum\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"WorldOil\", \"World Oil\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"redwedding\", \"red wedding\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"AmazingRaceCanada\", \"Amazing Race Canada\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"WakeUpAmerica\", \"Wake Up America\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"\\\\Allahuakbar\\\\\", \"Allahu Akbar\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"bleased\", \"blessed\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"nigeriantribune\", \"Nigerian Tribune\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"HIDEO_KOJIMA_EN\", \"Hideo Kojima\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"FusionFestival\", \"Fusion Festival\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"50Mixed\", \"50 Mixed\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"NoAgenda\", \"No Agenda\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"WhiteGenocide\", \"White Genocide\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"dirtylying\", \"dirty lying\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"SyrianRefugees\", \"Syrian Refugees\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"changetheworld\", \"change the world\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Ebolacase\", \"Ebola case\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"mcgtech\", \"mcg technologies\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"withweapons\", \"with weapons\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"advancedwarfare\", \"advanced warfare\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"letsFootball\", \"let us Football\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"LateNiteMix\", \"late night mix\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"PhilCollinsFeed\", \"Phil Collins\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"RudyHavenstein\", \"Rudy Havenstein\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"22PM\", \"22 PM\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"54am\", \"54 AM\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"38am\", \"38 AM\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"OldFolkExplainStuff\", \"Old Folk Explain Stuff\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"BlacklivesMatter\", \"Black Lives Matter\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"InsaneLimits\", \"Insane Limits\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"youcantsitwithus\", \"you cannot sit with us\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"2k15\", \"2015\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"TheIran\", \"Iran\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"JimmyFallon\", \"Jimmy Fallon\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"AlbertBrooks\", \"Albert Brooks\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"defense_news\", \"defense news\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"nuclearrcSA\", \"Nuclear Risk Control Self Assessment\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Auspol\", \"Australia Politics\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"NuclearPower\", \"Nuclear Power\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"WhiteTerrorism\", \"White Terrorism\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"truthfrequencyradio\", \"Truth Frequency Radio\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ErasureIsNotEquality\", \"Erasure is not equality\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ProBonoNews\", \"Pro Bono News\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"JakartaPost\", \"Jakarta Post\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"toopainful\", \"too painful\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"melindahaunton\", \"Melinda Haunton\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"NoNukes\", \"No Nukes\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"curryspcworld\", \"Currys PC World\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ineedcake\", \"I need cake\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"blackforestgateau\", \"black forest gateau\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"BBCOne\", \"BBC One\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"AlexxPage\", \"Alex Page\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"jonathanserrie\", \"Jonathan Serrie\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"SocialJerkBlog\", \"Social Jerk Blog\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ChelseaVPeretti\", \"Chelsea Peretti\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"irongiant\", \"iron giant\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"RonFunches\", \"Ron Funches\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"TimCook\", \"Tim Cook\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"sebastianstanisaliveandwell\", \"Sebastian Stan is alive and well\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Madsummer\", \"Mad summer\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"NowYouKnow\", \"Now you know\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"concertphotography\", \"concert photography\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"TomLandry\", \"Tom Landry\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"showgirldayoff\", \"show girl day off\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Yougslavia\", \"Yugoslavia\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"QuantumDataInformatics\", \"Quantum Data Informatics\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"FromTheDesk\", \"From The Desk\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"TheaterTrial\", \"Theater Trial\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"CatoInstitute\", \"Cato Institute\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"EmekaGift\", \"Emeka Gift\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"LetsBe_Rational\", \"Let us be rational\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Cynicalreality\", \"Cynical reality\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"FredOlsenCruise\", \"Fred Olsen Cruise\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"NotSorry\", \"not sorry\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"UseYourWords\", \"use your words\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"WordoftheDay\", \"word of the day\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Dictionarycom\", \"Dictionary.com\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"TheBrooklynLife\", \"The Brooklyn Life\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"jokethey\", \"joke they\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"nflweek1picks\", \"NFL week 1 picks\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"uiseful\", \"useful\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"JusticeDotOrg\", \"The American Association for Justice\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"autoaccidents\", \"auto accidents\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"SteveGursten\", \"Steve Gursten\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"MichiganAutoLaw\", \"Michigan Auto Law\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"birdgang\", \"bird gang\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"nflnetwork\", \"NFL Network\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"NYDNSports\", \"NY Daily News Sports\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"RVacchianoNYDN\", \"Ralph Vacchiano NY Daily News\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"EdmontonEsks\", \"Edmonton Eskimos\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"david_brelsford\", \"David Brelsford\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"TOI_India\", \"The Times of India\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"hegot\", \"he got\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"SkinsOn9\", \"Skins on 9\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"sothathappened\", \"so that happened\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"LCOutOfDoors\", \"LC Out Of Doors\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"NationFirst\", \"Nation First\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"IndiaToday\", \"India Today\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"HLPS\", \"helps\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"HOSTAGESTHROSW\", \"hostages throw\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"SNCTIONS\", \"sanctions\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"BidTime\", \"Bid Time\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"crunchysensible\", \"crunchy sensible\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"RandomActsOfRomance\", \"Random acts of romance\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"MomentsAtHill\", \"Moments at hill\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"eatshit\", \"eat shit\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"liveleakfun\", \"live leak fun\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"SahelNews\", \"Sahel News\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"abc7newsbayarea\", \"ABC 7 News Bay Area\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"facilitiesmanagement\", \"facilities management\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"facilitydude\", \"facility dude\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"CampLogistics\", \"Camp logistics\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"alaskapublic\", \"Alaska public\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"MarketResearch\", \"Market Research\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"AccuracyEsports\", \"Accuracy Esports\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"TheBodyShopAust\", \"The Body Shop Australia\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"yychail\", \"Calgary hail\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"yyctraffic\", \"Calgary traffic\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"eliotschool\", \"eliot school\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"TheBrokenCity\", \"The Broken City\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"OldsFireDept\", \"Olds Fire Department\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"RiverComplex\", \"River Complex\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"fieldworksmells\", \"field work smells\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"IranElection\", \"Iran Election\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"glowng\", \"glowing\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"kindlng\", \"kindling\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"riggd\", \"rigged\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"slownewsday\", \"slow news day\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"abc7chicago\", \"ABC 7 Chicago\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"copolitics\", \"Colorado Politics\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"AdilGhumro\", \"Adil Ghumro\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"netbots\", \"net bots\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"byebyeroad\", \"bye bye road\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"massiveflooding\", \"massive flooding\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"EndofUS\", \"End of United States\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"35PM\", \"35 PM\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"greektheatrela\", \"Greek Theatre Los Angeles\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"76mins\", \"76 minutes\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"publicsafetyfirst\", \"public safety first\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"livesmatter\", \"lives matter\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"myhometown\", \"my hometown\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"tankerfire\", \"tanker fire\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"MEMORIALDAY\", \"memorial day\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"MEMORIAL_DAY\", \"memorial day\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"instaxbooty\", \"instagram booty\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Jerusalem_Post\", \"Jerusalem Post\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"WayneRooney_INA\", \"Wayne Rooney\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"VirtualReality\", \"Virtual Reality\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"OculusRift\", \"Oculus Rift\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"OwenJones84\", \"Owen Jones\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"jeremycorbyn\", \"Jeremy Corbyn\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"paulrogers002\", \"Paul Rogers\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"mortalkombatx\", \"Mortal Kombat X\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"mortalkombat\", \"Mortal Kombat\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"FilipeCoelho92\", \"Filipe Coelho\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"OnlyQuakeNews\", \"Only Quake News\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"kostumes\", \"costumes\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"YEEESSSS\", \"yes\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ToshikazuKatayama\", \"Toshikazu Katayama\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"IntlDevelopment\", \"Intl Development\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ExtremeWeather\", \"Extreme Weather\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"WereNotGruberVoters\", \"We are not gruber voters\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"NewsThousands\", \"News Thousands\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"EdmundAdamus\", \"Edmund Adamus\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"EyewitnessWV\", \"Eye witness WV\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"PhiladelphiaMuseu\", \"Philadelphia Museum\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"DublinComicCon\", \"Dublin Comic Con\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"NicholasBrendon\", \"Nicholas Brendon\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Alltheway80s\", \"All the way 80s\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"FromTheField\", \"From the field\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"NorthIowa\", \"North Iowa\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"WillowFire\", \"Willow Fire\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"MadRiverComplex\", \"Mad River Complex\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"feelingmanly\", \"feeling manly\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"stillnotoverit\", \"still not over it\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"FortitudeValley\", \"Fortitude Valley\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"CoastpowerlineTramTr\", \"Coast powerline\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ServicesGold\", \"Services Gold\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"NewsbrokenEmergency\", \"News broken emergency\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Evaucation\", \"evacuation\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"leaveevacuateexitbe\", \"leave evacuate exit be\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"P_EOPLE\", \"PEOPLE\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Tubestrike\", \"tube strike\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"CLASS_SICK\", \"CLASS SICK\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"localplumber\", \"local plumber\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"awesomejobsiri\", \"awesome job siri\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"PayForItHow\", \"Pay for it how\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ThisIsAfrica\", \"This is Africa\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"crimeairnetwork\", \"crime air network\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"KimAcheson\", \"Kim Acheson\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"cityofcalgary\", \"City of Calgary\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"prosyndicate\", \"pro syndicate\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"660NEWS\", \"660 NEWS\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"BusInsMagazine\", \"Business Insurance Magazine\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"wfocus\", \"focus\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"ShastaDam\", \"Shasta Dam\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"go2MarkFranco\", \"Mark Franco\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"StephGHinojosa\", \"Steph Hinojosa\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Nashgrier\", \"Nash Grier\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"NashNewVideo\", \"Nash new video\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"IWouldntGetElectedBecause\", \"I would not get elected because\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"SHGames\", \"Sledgehammer Games\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"bedhair\", \"bed hair\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"JoelHeyman\", \"Joel Heyman\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"viaYouTube\", \"via YouTube\", text))\n           \n    # Urls\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"https?:\\\/\\\/t.co\\\/[A-Za-z0-9]+\", \"\", text))\n        \n\n          \n        \n    # Acronyms\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"m\u00cc\u00bcsica\", \"music\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"okwx\", \"Oklahoma City Weather\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"arwx\", \"Arkansas Weather\", text))    \n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"gawx\", \"Georgia Weather\", text))  \n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"scwx\", \"South Carolina Weather\", text))  \n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"cawx\", \"California Weather\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"tnwx\", \"Tennessee Weather\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"azwx\", \"Arizona Weather\", text))  \n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"alwx\", \"Alabama Weather\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"wordpressdotcom\", \"wordpress\", text))    \n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"usNWSgov\", \"United States National Weather Service\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Suruc\", \"Sanliurfa\", text))   \n    # Grouping same words without embeddings\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"Bestnaijamade\", \"bestnaijamade\", text))\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda text:re.sub(r\"SOUDELOR\", \"Soudelor\", text))\n    \n    return df\ndata_clean = clean_text(train_data, 'text', 'text_clean')\ndata_clean_test = clean_text(test_data,'text', 'text_clean')\ndata_clean","47f4c9d7":"traindata = list(np.array(data_clean.iloc[:,5])) #Extracting the text feature alone from the train data\ntestdata = list(np.array(data_clean_test.iloc[:,4]))#Extracting the text feature alone from the test data\ny = np.array(data_clean.iloc[:,4]).astype(int)#Extracting the target varaible from the train data\n\nX_all = traindata + testdata #combining both the test and train data\nlentrain = len(data_clean)\nprint('for Check!')\nprint(traindata[:1])\nprint(y)\nprint('count train data:' ,lentrain)\nprint(len(X_all))","65bd03cc":"data_clean_test","0df7cc87":"# Implementing TFIDF to extract the features from the text\ntfidf = TfidfVectorizer(min_df=3,  max_features=None, strip_accents='unicode',  \n        analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1)\n\nprint(\"Implementing TFIDF to both the test and train data\")\ntfidf.fit(X_all)\nprint(\"Transforming the data\")\nX_all = tfidf.transform(X_all)","91157f2e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score","2d11af48":"\nX = X_all[:lentrain] # Seperating the train data from the entire data\nX_test_data = X_all[lentrain:] # Seperating the test data from the entire data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    stratify=y, \n                                                    test_size=0.2)\nlog = LogisticRegression(penalty='l2',dual=False, tol=0.0001, \n                             C=0.5, fit_intercept=True, intercept_scaling=1.0, \n                             class_weight=None, random_state=None) #initialising the logistic regression function with the respective parameters\n\nprint(\"Training on the train data\")\nlog.fit(X_train,y_train)\n\n#Evaluating with the train data's target variable to obatin the training accuracy!\ny_pred_X=log.predict(X_test)\nprint('Training accuracy is {}'.format(accuracy_score(y_test, y_pred_X)))\n\npredictions = log.predict(X_test_data) #Prediciting the target for the test data\n","659b5ece":"print(classification_report(y_test, y_pred_X))","600d9a6c":"test_ids=test_data['id']\nsubmission = pd.DataFrame(predictions,index=test_ids,columns=['target'])\nsubmission.to_csv('submission.csv')\nprint(\"submission file created..\")","1231a962":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  \u0e1b\u0e23\u0e30\u0e01\u0e32\u0e28\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23 dic \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 defaultdict \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e1b\u0e49\u0e2d\u0e07\u0e01\u0e31\u0e19 key error \u0e19\u0e31\u0e1a\u0e08\u0e33\u0e19\u0e27\u0e19\u0e04\u0e33\u0e17\u0e38\u0e01\u0e04\u0e33\u0e17\u0e35\u0e48\u0e40\u0e1b\u0e47\u0e19 punctuation \u0e43\u0e19 corpus \u0e42\u0e14\u0e22\u0e43\u0e2b\u0e49 key \u0e40\u0e1b\u0e47\u0e19 \u0e04\u0e33\u0e28\u0e31\u0e1e\u0e17\u0e4c \u0e41\u0e25\u0e30 value \u0e40\u0e1b\u0e47\u0e19\u0e08\u0e33\u0e19\u0e27\u0e19\u0e04\u0e33 \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49\u0e01\u0e32\u0e23 for loop \u0e41\u0e25\u0e30\u0e40\u0e1e\u0e34\u0e48\u0e21 value \u0e2d\u0e35\u0e011 \u0e40\u0e21\u0e37\u0e48\u0e2d\u0e04\u0e33\u0e14\u0e31\u0e07\u0e01\u0e25\u0e48\u0e32\u0e27\u0e40\u0e1b\u0e47\u0e19\u0e44\u0e1b\u0e15\u0e32\u0e21\u0e40\u0e07\u0e37\u0e48\u0e2d\u0e19\u0e44\u0e02 \u0e04\u0e37\u0e2d \u0e40\u0e1b\u0e47\u0e19 punctuation \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e19\u0e33\u0e21\u0e32\u0e1e\u0e25\u0e47\u0e2d\u0e15\u0e01\u0e23\u0e32\u0e1f \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e14\u0e39\u0e27\u0e48\u0e32\u0e04\u0e33\u0e43\u0e14\u0e1a\u0e49\u0e32\u0e07\u0e17\u0e35\u0e48\u0e40\u0e1b\u0e47\u0e19 punctuation \u0e41\u0e25\u0e30 \u0e21\u0e35\u0e08\u0e33\u0e19\u0e27\u0e19\u0e40\u0e17\u0e48\u0e32\u0e44\u0e23\u0e43\u0e19 corpus ","d4d56ff9":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \u0e2a\u0e23\u0e49\u0e32\u0e07\u0e44\u0e1f\u0e25\u0e4c CSV \u0e17\u0e35\u0e48\u0e08\u0e30\u0e2a\u0e48\u0e07\u0e42\u0e14\u0e22\u0e40\u0e01\u0e47\u0e1a id \u0e02\u0e2d\u0e07 test_data \u0e44\u0e27\u0e49\u0e43\u0e19\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23\u0e0a\u0e37\u0e48\u0e2d test_ids \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e2a\u0e23\u0e49\u0e32\u0e07 DataFrame \u0e0a\u0e37\u0e48\u0e2d submission \u0e42\u0e14\u0e22\u0e21\u0e35 index \u0e40\u0e1b\u0e47\u0e19 test_ids \u0e41\u0e25\u0e30 column \u0e40\u0e1b\u0e47\u0e19 target \u0e42\u0e14\u0e22\u0e21\u0e35\u0e04\u0e48\u0e32\u0e43\u0e19 column \u0e40\u0e1b\u0e47\u0e19 predictions \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e41\u0e1b\u0e25\u0e07 file \u0e40\u0e1b\u0e47\u0e19 file csv \u0e40\u0e1b\u0e47\u0e19\u0e2d\u0e31\u0e19\u0e40\u0e2a\u0e23\u0e47\u0e08\u0e2a\u0e34\u0e49\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33\u0e42\u0e1b\u0e23\u0e40\u0e08\u0e47\u0e04","6582e315":"<center><h1><span style=\"color:Red\">Real or Not? NLP with Disaster Tweets<\/span><\/h1>\n    <h2> DSI 206 Multimedia Representation Management<\/h2>\n     <h3>BY Sukonlaphat Chinnawong 6209656203 <br\/>\n        Apiwat Yokyuenyong 6209656237 <br\/>\n         Kittisak Thongsi 6209656286<br\/><\/h3><\/center>","cdd82e88":"# **Introduction**\n\n\n> \u0e43\u0e19\u0e1b\u0e31\u0e08\u0e08\u0e38\u0e1a\u0e31\u0e19 \u0e2a\u0e37\u0e48\u0e2d\u0e2d\u0e2d\u0e19\u0e44\u0e25\u0e19\u0e4c\u0e40\u0e1b\u0e47\u0e19\u0e0a\u0e48\u0e2d\u0e07\u0e17\u0e32\u0e07\u0e17\u0e35\u0e48\u0e1a\u0e38\u0e04\u0e04\u0e25\u0e17\u0e31\u0e48\u0e27\u0e44\u0e1b\u0e19\u0e34\u0e22\u0e21\u0e43\u0e0a\u0e49\u0e43\u0e19\u0e01\u0e32\u0e23\u0e01\u0e23\u0e30\u0e08\u0e32\u0e22\u0e02\u0e48\u0e32\u0e27\u0e01\u0e31\u0e19\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e41\u0e1e\u0e23\u0e48\u0e2b\u0e25\u0e32\u0e22 \u0e0b\u0e36\u0e48\u0e07\u0e17\u0e27\u0e34\u0e15\u0e40\u0e15\u0e2d\u0e23\u0e4c\u0e01\u0e25\u0e32\u0e22\u0e40\u0e1b\u0e47\u0e19\u0e0a\u0e48\u0e2d\u0e07\u0e17\u0e32\u0e07\u0e01\u0e32\u0e23\u0e2a\u0e37\u0e48\u0e2d\u0e2a\u0e32\u0e23\u0e17\u0e35\u0e48\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e19\u0e34\u0e22\u0e21\u0e41\u0e25\u0e30\u0e2a\u0e33\u0e04\u0e31\u0e0d\u0e43\u0e19\u0e22\u0e32\u0e21\u0e09\u0e38\u0e01\u0e40\u0e09\u0e34\u0e19 \u0e1c\u0e39\u0e49\u0e04\u0e19\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e1b\u0e23\u0e30\u0e01\u0e32\u0e28\u0e40\u0e2b\u0e15\u0e38\u0e09\u0e38\u0e01\u0e40\u0e09\u0e34\u0e19\u0e17\u0e35\u0e48\u0e1e\u0e27\u0e01\u0e40\u0e02\u0e32\u0e01\u0e33\u0e25\u0e31\u0e07\u0e40\u0e1d\u0e49\u0e32\u0e2a\u0e31\u0e07\u0e40\u0e01\u0e15\u0e44\u0e14\u0e49\u0e41\u0e1a\u0e1a\u0e40\u0e23\u0e35\u0e22\u0e25\u0e44\u0e17\u0e21\u0e4c \u0e19\u0e33\u0e21\u0e32\u0e0b\u0e36\u0e48\u0e07\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19\u0e17\u0e35\u0e48\u0e17\u0e49\u0e32\u0e17\u0e32\u0e22\u0e43\u0e19\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19\u0e1b\u0e23\u0e30\u0e21\u0e27\u0e25\u0e1c\u0e25 NLP","a447b69f":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  \u0e15\u0e31\u0e49\u0e07 list \u0e27\u0e48\u0e32\u0e07\u0e0a\u0e37\u0e48\u0e2d corpus \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e40\u0e15\u0e23\u0e35\u0e22\u0e21\u0e40\u0e01\u0e47\u0e1a\u0e04\u0e33\u0e17\u0e35\u0e48\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23 \u0e43\u0e0a\u0e49 nest loop \u0e42\u0e14\u0e22 for loop \u0e43\u0e19 column \u0e0a\u0e37\u0e48\u0e2d text \u0e17\u0e38\u0e01\u0e41\u0e16\u0e27\u0e41\u0e25\u0e30\u0e19\u0e33\u0e04\u0e33\u0e17\u0e38\u0e01\u0e04\u0e33\u0e44\u0e1b\u0e40\u0e01\u0e47\u0e1a\u0e44\u0e27\u0e49\u0e43\u0e19 list \u0e0a\u0e37\u0e48\u0e2d corpus \u0e1c\u0e48\u0e32\u0e19\u0e01\u0e32\u0e23 append","427565d3":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\u0e1b\u0e23\u0e30\u0e01\u0e32\u0e28\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23 dic \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 defaultdict \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e1b\u0e49\u0e2d\u0e07\u0e01\u0e31\u0e19 key error \u0e19\u0e31\u0e1a\u0e08\u0e33\u0e19\u0e27\u0e19\u0e04\u0e33\u0e17\u0e38\u0e01\u0e04\u0e33\u0e17\u0e35\u0e48\u0e44\u0e21\u0e48\u0e43\u0e0a\u0e48 stop word \u0e43\u0e19 corpus \u0e42\u0e14\u0e22\u0e43\u0e2b\u0e49 key \u0e40\u0e1b\u0e47\u0e19\u0e04\u0e33\u0e28\u0e31\u0e1e\u0e17\u0e4c\u0e41\u0e25\u0e30 value \u0e40\u0e1b\u0e47\u0e19\u0e08\u0e33\u0e19\u0e27\u0e19\u0e04\u0e33\u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49\u0e01\u0e32\u0e23 for loop \u0e41\u0e25\u0e30\u0e40\u0e1e\u0e34\u0e48\u0e21 value \u0e2d\u0e35\u0e01 1 \u0e40\u0e21\u0e37\u0e48\u0e2d\u0e04\u0e33\u0e14\u0e31\u0e07\u0e01\u0e25\u0e48\u0e32\u0e27\u0e40\u0e1b\u0e47\u0e19\u0e44\u0e1b\u0e15\u0e32\u0e21\u0e40\u0e07\u0e37\u0e48\u0e2d\u0e19\u0e44\u0e02\u0e04\u0e37\u0e2d \u0e44\u0e21\u0e48\u0e40\u0e1b\u0e47\u0e19 stop word \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e19\u0e33\u0e21\u0e32\u0e40\u0e23\u0e35\u0e22\u0e07\u0e42\u0e14\u0e22\u0e01\u0e32\u0e23 sorted \u0e08\u0e32\u0e01\u0e21\u0e32\u0e01\u0e44\u0e1b\u0e19\u0e49\u0e2d\u0e22\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e43\u0e2b\u0e49\u0e07\u0e48\u0e32\u0e22\u0e15\u0e48\u0e2d\u0e01\u0e32\u0e23\u0e14\u0e39\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e42\u0e14\u0e22\u0e40\u0e23\u0e35\u0e22\u0e01\u0e14\u0e39\u0e40\u0e1e\u0e35\u0e22\u0e07\u0e41\u0e04\u0e48 30 \u0e15\u0e31\u0e27\u0e41\u0e23\u0e01 \u0e41\u0e25\u0e30\u0e19\u0e33\u0e21\u0e32\u0e1e\u0e25\u0e47\u0e2d\u0e15\u0e01\u0e23\u0e32\u0e1f \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e14\u0e39\u0e27\u0e48\u0e32\u0e04\u0e33\u0e43\u0e14\u0e1a\u0e49\u0e32\u0e07\u0e17\u0e35\u0e48\u0e44\u0e21\u0e48\u0e21\u0e35\u0e43\u0e19 stopword \u0e41\u0e25\u0e30\u0e21\u0e35\u0e08\u0e33\u0e19\u0e27\u0e19\u0e40\u0e17\u0e48\u0e32\u0e43\u0e14\u0e43\u0e19 corpus","f142af57":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  \u0e2a\u0e23\u0e49\u0e32\u0e07 function \u0e0a\u0e37\u0e48\u0e2d clean_text \u0e21\u0e35 argument 3 \u0e15\u0e31\u0e27\u0e44\u0e14\u0e49\u0e41\u0e04\u0e48 df, text_field , new_text_field_name  \u0e2b\u0e25\u0e31\u0e01\u0e01\u0e32\u0e23\u0e04\u0e37\u0e2d\u0e41\u0e1b\u0e25\u0e07 text \u0e43\u0e2b\u0e49\u0e40\u0e1b\u0e47\u0e19 lower \u0e17\u0e31\u0e49\u0e07\u0e2b\u0e21\u0e14\u0e01\u0e48\u0e2d\u0e19 \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e43\u0e0a\u0e49 module \u0e2b\u0e25\u0e31\u0e01 \u0e04\u0e37\u0e2d apply lambda \u0e21\u0e32\u0e0a\u0e48\u0e27\u0e22\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19\u0e41\u0e1b\u0e25\u0e07\u0e04\u0e48\u0e32\u0e43\u0e19\u0e41\u0e15\u0e48\u0e25\u0e30\u0e41\u0e16\u0e27\u0e43\u0e2b\u0e49\u0e40\u0e1b\u0e47\u0e19\u0e04\u0e48\u0e32 format \u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23 \u0e42\u0e14\u0e22\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49 regular expression \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19 string \u0e43\u0e19 format \u0e41\u0e1a\u0e1a\u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e2a\u0e19\u0e43\u0e08\u0e44\u0e1b\u0e40\u0e1b\u0e47\u0e19 format \u0e41\u0e1a\u0e1a\u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23 \u0e42\u0e14\u0e22\u0e2a\u0e34\u0e48\u0e07\u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e17\u0e33\u0e21\u0e35\u0e14\u0e31\u0e07\u0e19\u0e35\u0e49 <br\/><br\/>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  1.\u0e25\u0e1a\u0e15\u0e31\u0e27\u0e40\u0e25\u0e02\u0e17\u0e31\u0e49\u0e07\u0e2b\u0e21\u0e14 <br\/>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  2.Expand_contraction <br\/>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  3.\u0e40\u0e2d\u0e32 stop word \u0e2d\u0e2d\u0e01 <br\/>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  4.\u0e40\u0e2d\u0e32\u0e2a\u0e34\u0e48\u0e07\u0e17\u0e35\u0e48\u0e44\u0e21\u0e48\u0e43\u0e0a\u0e48\u0e40\u0e19\u0e37\u0e49\u0e2d\u0e2b\u0e32 \u0e41\u0e25\u0e30 punctuation \u0e2d\u0e2d\u0e01 <br\/>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  5.\u0e25\u0e1a html tag <br\/>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  6.\u0e25\u0e1a emoji <br\/>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  7.\u0e41\u0e1b\u0e25\u0e07\u0e04\u0e33\u0e15\u0e48\u0e32\u0e07\u0e46\u0e40\u0e1b\u0e47\u0e19 format \u0e41\u0e1a\u0e1a\u0e40\u0e14\u0e35\u0e22\u0e27\u0e01\u0e31\u0e19 \u0e43\u0e0a\u0e49 function clean_text \u0e40\u0e1e\u0e37\u0e48\u0e2d clean \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e35\u0e48\u0e08\u0e30\u0e43\u0e0a\u0e49\u0e17\u0e33 model \u0e41\u0e25\u0e30\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e35\u0e48\u0e08\u0e30\u0e17\u0e33\u0e44\u0e1b\u0e17\u0e14\u0e2a\u0e2d\u0e1a <br\/>","edabeb9a":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  model \u0e21\u0e35\u0e04\u0e48\u0e32 accuracy \u0e42\u0e14\u0e22\u0e23\u0e27\u0e21\u0e40\u0e1b\u0e47\u0e19\u0e17\u0e35\u0e48\u0e19\u0e48\u0e32\u0e1e\u0e36\u0e07\u0e1e\u0e2d\u0e43\u0e08 \u0e41\u0e15\u0e48 model \u0e44\u0e14\u0e49\u0e17\u0e33\u0e01\u0e32\u0e23 predict \u0e04\u0e48\u0e32 target \u0e17\u0e35\u0e48\u0e40\u0e1b\u0e47\u0e19 0 \u0e44\u0e14\u0e49\u0e41\u0e21\u0e48\u0e19\u0e22\u0e33\u0e01\u0e27\u0e48\u0e32\u0e04\u0e48\u0e32 target \u0e17\u0e35\u0e48\u0e40\u0e1b\u0e47\u0e19 1","7ce63672":"\n\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \u0e41\u0e22\u0e01\u0e2a\u0e34\u0e48\u0e07\u0e17\u0e35\u0e48\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e40\u0e1b\u0e47\u0e19 2 \u0e01\u0e25\u0e38\u0e48\u0e21 \u0e44\u0e14\u0e49\u0e41\u0e01\u0e48 word (target==1 \u0e40\u0e1b\u0e47\u0e19 boolean) \u0e41\u0e25\u0e30 word (target==0 \u0e40\u0e1b\u0e47\u0e19 boolean) \u0e41\u0e25\u0e49\u0e27\u0e40\u0e01\u0e47\u0e1a\u0e04\u0e27\u0e32\u0e21\u0e22\u0e32\u0e27\u0e02\u0e2d\u0e07\u0e41\u0e15\u0e48\u0e25\u0e30 word \u0e44\u0e27\u0e49\u0e43\u0e19 list \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e21\u0e32\u0e2b\u0e32\u0e04\u0e48\u0e32\u0e40\u0e09\u0e25\u0e35\u0e48\u0e22\u0e02\u0e2d\u0e07\u0e04\u0e27\u0e32\u0e21\u0e22\u0e32\u0e27\u0e15\u0e31\u0e27\u0e2d\u0e31\u0e01\u0e29\u0e23\u0e02\u0e2d\u0e07\u0e04\u0e33\u0e43\u0e19\u0e41\u0e15\u0e48\u0e25\u0e30 index \u0e1c\u0e48\u0e32\u0e19 numpy \u0e19\u0e33\u0e04\u0e48\u0e32\u0e40\u0e09\u0e25\u0e35\u0e48\u0e22\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e21\u0e32\u0e1e\u0e25\u0e47\u0e2d\u0e15\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e14\u0e39\u0e04\u0e27\u0e32\u0e21\u0e22\u0e32\u0e27\u0e40\u0e09\u0e25\u0e35\u0e48\u0e22\u0e02\u0e2d\u0e07\u0e04\u0e33\u0e43\u0e19\u0e41\u0e15\u0e48\u0e25\u0e30\u0e01\u0e25\u0e38\u0e48\u0e21 \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e14\u0e39\u0e04\u0e27\u0e32\u0e21\u0e41\u0e15\u0e01\u0e15\u0e48\u0e32\u0e07\u0e02\u0e2d\u0e07\u0e04\u0e27\u0e32\u0e21\u0e22\u0e32\u0e27\u0e40\u0e09\u0e25\u0e35\u0e48\u0e22\u0e02\u0e2d\u0e07\u0e04\u0e33\u0e43\u0e19\u0e41\u0e15\u0e48\u0e25\u0e30\u0e01\u0e25\u0e38\u0e48\u0e21\u0e27\u0e48\u0e32\u0e21\u0e35\u0e08\u0e33\u0e19\u0e27\u0e19\u0e41\u0e25\u0e30\u0e01\u0e32\u0e23\u0e01\u0e23\u0e30\u0e08\u0e32\u0e22\u0e41\u0e15\u0e01\u0e15\u0e48\u0e32\u0e07\u0e01\u0e31\u0e19\u0e2b\u0e23\u0e37\u0e2d\u0e44\u0e21\u0e48\n\n","4612da5c":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  \u0e41\u0e22\u0e01\u0e2a\u0e34\u0e48\u0e07\u0e17\u0e35\u0e48\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e40\u0e1b\u0e47\u0e19 2 \u0e01\u0e25\u0e38\u0e48\u0e21 \u0e44\u0e14\u0e49\u0e41\u0e01\u0e48 tweet_len(target==1 \u0e40\u0e1b\u0e47\u0e19 boolean) \u0e41\u0e25\u0e30 tweet_len (target==0 \u0e40\u0e1b\u0e47\u0e19 boolean) \u0e41\u0e25\u0e49\u0e27\u0e19\u0e31\u0e1a\u0e08\u0e33\u0e19\u0e27\u0e19 character \u0e17\u0e31\u0e49\u0e07\u0e2b\u0e21\u0e14\u0e1c\u0e48\u0e32\u0e19\u0e1f\u0e31\u0e07\u0e01\u0e4c\u0e0a\u0e31\u0e48\u0e19 len \u0e19\u0e31\u0e1a\u0e08\u0e33\u0e19\u0e27\u0e19 string character \u0e19\u0e33\u0e21\u0e32\u0e1e\u0e25\u0e47\u0e2d\u0e15\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e14\u0e39\u0e08\u0e33\u0e19\u0e27\u0e19 character \u0e43\u0e19\u0e41\u0e15\u0e48\u0e25\u0e30\u0e01\u0e25\u0e38\u0e48\u0e21 \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e14\u0e39\u0e04\u0e27\u0e32\u0e21\u0e41\u0e15\u0e01\u0e15\u0e48\u0e32\u0e07\u0e02\u0e2d\u0e07\u0e08\u0e33\u0e19\u0e27\u0e19 character \u0e43\u0e19\u0e41\u0e15\u0e48\u0e25\u0e30\u0e01\u0e25\u0e38\u0e48\u0e21\u0e27\u0e48\u0e32\u0e41\u0e15\u0e01\u0e15\u0e48\u0e32\u0e07\u0e01\u0e31\u0e19\u0e2b\u0e23\u0e37\u0e2d\u0e44\u0e21\u0e48\n\n\n","67800d44":"![picture](https:\/\/www.magisto.com\/blog\/wp-content\/uploads\/2019\/03\/Twitter.jpg)","709aee76":"\u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01\u0e1e\u0e25\u0e47\u0e2d\u0e15\u0e01\u0e23\u0e32\u0e1f\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e14\u0e39\u0e25\u0e31\u0e01\u0e29\u0e13\u0e30\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e41\u0e25\u0e49\u0e27\u0e44\u0e14\u0e49\u0e02\u0e49\u0e2d\u0e2a\u0e23\u0e38\u0e1b\u0e14\u0e31\u0e07\u0e19\u0e35\u0e49 <br\/><br\/>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  1.\u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33 NLP \u0e08\u0e30\u0e44\u0e21\u0e48\u0e19\u0e33\u0e04\u0e27\u0e32\u0e21\u0e22\u0e32\u0e27\u0e02\u0e2d\u0e07\u0e08\u0e33\u0e19\u0e27\u0e19 Character \u0e08\u0e33\u0e19\u0e27\u0e19\u0e04\u0e33\u0e04\u0e27\u0e32\u0e21\u0e22\u0e32\u0e27\u0e40\u0e09\u0e25\u0e35\u0e48\u0e22\u0e02\u0e2d\u0e07\u0e04\u0e33 \u0e21\u0e32\u0e43\u0e0a\u0e49\u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33 model \u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01\u0e21\u0e35\u0e01\u0e32\u0e23\u0e01\u0e23\u0e30\u0e08\u0e32\u0e22\u0e41\u0e25\u0e30\u0e08\u0e33\u0e19\u0e27\u0e19\u0e17\u0e35\u0e48\u0e44\u0e21\u0e48\u0e41\u0e15\u0e01\u0e15\u0e48\u0e32\u0e07\u0e01\u0e31\u0e19 \u0e43\u0e19\u0e01\u0e25\u0e38\u0e48\u0e21\u0e17\u0e35\u0e48 target \u0e40\u0e1b\u0e47\u0e19 1 \u0e41\u0e25\u0e30 target \u0e40\u0e1b\u0e47\u0e19 0 \u0e0b\u0e36\u0e48\u0e07\u0e15\u0e2d\u0e19\u0e41\u0e23\u0e01\u0e19\u0e31\u0e49\u0e19\u0e1c\u0e39\u0e49\u0e08\u0e31\u0e14\u0e17\u0e33\u0e04\u0e32\u0e14\u0e01\u0e32\u0e23\u0e27\u0e48\u0e32\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23\u0e40\u0e2b\u0e25\u0e48\u0e32\u0e19\u0e35\u0e49\u0e2d\u0e32\u0e08\u0e08\u0e30\u0e21\u0e35\u0e1c\u0e25\u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01 \u0e2b\u0e32\u0e01\u0e40\u0e1b\u0e47\u0e19\u0e01\u0e32\u0e23\u0e41\u0e0a\u0e23\u0e4c\u0e02\u0e48\u0e32\u0e27\u0e20\u0e31\u0e22\u0e1e\u0e34\u0e1a\u0e31\u0e15\u0e34\u0e01\u0e47\u0e04\u0e27\u0e23\u0e08\u0e30\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e22\u0e32\u0e27\u0e02\u0e2d\u0e07 character \u0e2b\u0e23\u0e37\u0e2d\u0e08\u0e33\u0e19\u0e27\u0e19\u0e04\u0e33\u0e17\u0e35\u0e48\u0e21\u0e32\u0e01\u0e01\u0e27\u0e48\u0e32 tweet \u0e17\u0e35\u0e48\u0e41\u0e04\u0e48\u0e1e\u0e34\u0e21\u0e1e\u0e4c\u0e17\u0e31\u0e48\u0e27\u0e44\u0e1b\u0e41\u0e25\u0e30\u0e04\u0e32\u0e14\u0e27\u0e48\u0e32\u0e2b\u0e32\u0e01\u0e40\u0e1b\u0e47\u0e19 tweet \u0e17\u0e35\u0e48\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e01\u0e31\u0e1a\u0e20\u0e31\u0e22\u0e1e\u0e34\u0e1a\u0e31\u0e15\u0e34\u0e2d\u0e32\u0e08\u0e08\u0e30\u0e21\u0e35\u0e04\u0e33\u0e28\u0e31\u0e1e\u0e17\u0e4c\u0e40\u0e09\u0e1e\u0e32\u0e30\u0e17\u0e35\u0e48\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e22\u0e32\u0e27\u0e40\u0e09\u0e25\u0e35\u0e48\u0e22\u0e15\u0e48\u0e32\u0e07\u0e08\u0e32\u0e01 tweet \u0e17\u0e31\u0e48\u0e27\u0e44\u0e1b <br\/><br\/>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  2.\u0e43\u0e19\u0e01\u0e32\u0e23 cleansing data \u0e08\u0e30\u0e15\u0e49\u0e2d\u0e07\u0e15\u0e31\u0e14 punctuation \u0e41\u0e25\u0e30 stop word \u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01\u0e21\u0e35\u0e08\u0e33\u0e19\u0e27\u0e19\u0e21\u0e32\u0e01\u0e43\u0e19 corpus \u0e0b\u0e36\u0e48\u0e07\u0e2d\u0e32\u0e08\u0e08\u0e30\u0e21\u0e35\u0e1c\u0e25\u0e01\u0e31\u0e1a\u0e08\u0e33\u0e19\u0e27\u0e19 feature \u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33 tfidf\n","dfba0b20":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  \u0e41\u0e22\u0e01\u0e2a\u0e34\u0e48\u0e07\u0e17\u0e35\u0e48\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e40\u0e1b\u0e47\u0e19 2 \u0e01\u0e25\u0e38\u0e48\u0e21 \u0e44\u0e14\u0e49\u0e41\u0e01\u0e48 tweet_len(target==1 \u0e40\u0e1b\u0e47\u0e19 boolean) \u0e41\u0e25\u0e30 tweet_len (target==0 \u0e40\u0e1b\u0e47\u0e19 boolean) \u0e41\u0e25\u0e49\u0e27\u0e19\u0e31\u0e1a\u0e08\u0e33\u0e19\u0e27\u0e19 word \u0e43\u0e19\u0e41\u0e15\u0e48\u0e25\u0e30\u0e01\u0e25\u0e38\u0e48\u0e21\u0e42\u0e14\u0e22\u0e01\u0e32\u0e23\u0e19\u0e33 string \u0e21\u0e32 split \u0e40\u0e1b\u0e47\u0e19 list \u0e02\u0e2d\u0e07\u0e04\u0e33\u0e19\u0e31\u0e1a\u0e08\u0e33\u0e19\u0e27\u0e19\u0e04\u0e33\u0e43\u0e19 list \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 len \u0e19\u0e33\u0e21\u0e32\u0e1e\u0e25\u0e47\u0e2d\u0e15\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e14\u0e39\u0e08\u0e33\u0e19\u0e27\u0e19 word \u0e43\u0e19\u0e41\u0e15\u0e48\u0e25\u0e30\u0e01\u0e25\u0e38\u0e48\u0e21 \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e14\u0e39\u0e04\u0e27\u0e32\u0e21\u0e41\u0e15\u0e01\u0e15\u0e48\u0e32\u0e07 word \u0e43\u0e19\u0e41\u0e15\u0e48\u0e25\u0e30\u0e01\u0e25\u0e38\u0e48\u0e21\u0e27\u0e48\u0e32\u0e21\u0e35\u0e08\u0e33\u0e19\u0e27\u0e19\u0e41\u0e25\u0e30\u0e01\u0e32\u0e23\u0e01\u0e23\u0e30\u0e08\u0e32\u0e22\u0e41\u0e15\u0e01\u0e15\u0e48\u0e32\u0e07\u0e01\u0e31\u0e19\u0e2b\u0e23\u0e37\u0e2d\u0e44\u0e21\u0e48\n","0eb5e11a":"# Cleansing Data\n\n   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \u0e1b\u0e23\u0e30\u0e01\u0e32\u0e28 dictionary \u0e0a\u0e37\u0e48\u0e2d contraction \u0e42\u0e14\u0e22\u0e43\u0e2b\u0e49 key \u0e40\u0e1b\u0e47\u0e19\u0e04\u0e33\u0e22\u0e48\u0e2d\u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e2a\u0e19\u0e43\u0e08 value \u0e40\u0e1b\u0e47\u0e19\u0e04\u0e33\u0e40\u0e15\u0e47\u0e21\u0e02\u0e2d\u0e07\u0e04\u0e33\u0e19\u0e31\u0e49\u0e19 \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e40\u0e15\u0e23\u0e35\u0e22\u0e21\u0e17\u0e35\u0e48\u0e08\u0e30\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19 format \u0e02\u0e2d\u0e07\u0e04\u0e33\u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e2a\u0e19\u0e43\u0e08\u0e43\u0e19\u0e02\u0e31\u0e49\u0e19\u0e15\u0e2d\u0e19\u0e15\u0e48\u0e2d\u0e44\u0e1b\n","28a7d751":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \u0e17\u0e33 tfidf \u0e42\u0e14\u0e22\u0e1b\u0e23\u0e30\u0e01\u0e32\u0e28 object \u0e0a\u0e37\u0e48\u0e2d tfidf \u0e42\u0e14\u0e22\u0e21\u0e35\u0e04\u0e38\u0e13\u0e2a\u0e21\u0e1a\u0e31\u0e15\u0e34\u0e40\u0e1b\u0e47\u0e19 TfidfVectorizer \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19 \u0e19\u0e33 X_all \u0e44\u0e1b fit \u0e43\u0e19 tfidf \u0e41\u0e25\u0e30 transform \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e40\u0e15\u0e23\u0e35\u0e22\u0e21\u0e19\u0e33\u0e44\u0e1b\u0e17\u0e33 model \u0e42\u0e14\u0e22\u0e40\u0e01\u0e47\u0e1a\u0e04\u0e48\u0e32\u0e44\u0e27\u0e49\u0e43\u0e19\u0e0a\u0e37\u0e48\u0e2d\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23 X_all","a43de6aa":"# Model\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \u0e40\u0e01\u0e47\u0e1a feature text \u0e08\u0e32\u0e01 data_clean \u0e44\u0e27\u0e49\u0e43\u0e19\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23\u0e0a\u0e37\u0e48\u0e2d traindata \u0e40\u0e01\u0e47\u0e1a feature text \u0e08\u0e32\u0e01 data_clean_test \u0e44\u0e27\u0e49\u0e43\u0e19 \u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23\u0e0a\u0e37\u0e48\u0e2d testdata \u0e40\u0e01\u0e47\u0e1a feature target \u0e08\u0e32\u0e01 data_clean \u0e44\u0e27\u0e49\u0e43\u0e19\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23\u0e0a\u0e37\u0e48\u0e2d y \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e19\u0e33 traindata \u0e21\u0e32\u0e23\u0e27\u0e21\u0e01\u0e31\u0e1a testdata \u0e40\u0e1b\u0e47\u0e19\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23 X_all \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e19\u0e33\u0e44\u0e1b\u0e17\u0e33 tdidf \u0e15\u0e48\u0e2d\u0e44\u0e1b \u0e42\u0e14\u0e22\u0e17\u0e35\u0e48\u0e19\u0e33\u0e21\u0e32\u0e23\u0e27\u0e21\u0e01\u0e31\u0e19\u0e01\u0e48\u0e2d\u0e19\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e1b\u0e49\u0e2d\u0e07\u0e01\u0e31\u0e19\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e40\u0e23\u0e37\u0e48\u0e2d\u0e07 shape \u0e02\u0e2d\u0e07 train \u0e41\u0e25\u0e30 test \u0e44\u0e21\u0e48\u0e40\u0e17\u0e48\u0e32\u0e01\u0e31\u0e19 \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e1b\u0e23\u0e30\u0e01\u0e32\u0e28\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23 lentrain \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e19\u0e31\u0e1a\u0e08\u0e33\u0e19\u0e27\u0e19\u0e41\u0e16\u0e27\u0e02\u0e2d\u0e07 data_clean \u0e0b\u0e36\u0e48\u0e07\u0e01\u0e47\u0e04\u0e37\u0e2d\u0e08\u0e33\u0e19\u0e27\u0e19 traindata \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e43\u0e0a\u0e49\u0e40\u0e1b\u0e47\u0e19\u0e15\u0e31\u0e27\u0e2d\u0e49\u0e32\u0e07\u0e2d\u0e34\u0e07\u0e43\u0e19\u0e01\u0e32\u0e23\u0e41\u0e1a\u0e48\u0e07\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e43\u0e19\u0e20\u0e32\u0e22\u0e2b\u0e25\u0e31\u0e07\n\n\n\n","3fc7995e":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; import \u0e0a\u0e38\u0e14\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e40\u0e02\u0e49\u0e32\u0e21\u0e32\u0e42\u0e14\u0e22 train_data \u0e04\u0e37\u0e2d\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e35\u0e48\u0e43\u0e0a\u0e49\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e1d\u0e36\u0e01 model \u0e41\u0e25\u0e30 test_data \u0e04\u0e37\u0e2d\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e35\u0e48\u0e43\u0e0a\u0e49\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e17\u0e14\u0e2a\u0e2d\u0e1a model \u0e0b\u0e36\u0e48\u0e07\u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01\u0e17\u0e35\u0e48 import \u0e0a\u0e38\u0e14\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e40\u0e02\u0e49\u0e32\u0e21\u0e32\u0e41\u0e25\u0e49\u0e27 \u0e08\u0e36\u0e07\u0e17\u0e33\u0e01\u0e32\u0e23\u0e40\u0e0a\u0e47\u0e04\u0e25\u0e31\u0e01\u0e29\u0e13\u0e30\u0e02\u0e2d\u0e07\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e42\u0e14\u0e22\u0e23\u0e27\u0e21 \u0e41\u0e25\u0e30\u0e0a\u0e19\u0e34\u0e14\u0e02\u0e2d\u0e07\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e31\u0e49\u0e07 2 \u0e0a\u0e38\u0e14\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25","6a1771e2":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \u0e19\u0e33\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23 X_all \u0e21\u0e32\u0e41\u0e1a\u0e48\u0e07\u0e42\u0e14\u0e22\u0e01\u0e32\u0e23 slicing \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e41\u0e22\u0e01\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e35\u0e48\u0e08\u0e30\u0e19\u0e33\u0e21\u0e32\u0e17\u0e33\u0e42\u0e21\u0e40\u0e14\u0e25\u0e41\u0e25\u0e30\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e35\u0e48\u0e08\u0e30\u0e19\u0e33\u0e44\u0e1b\u0e1b\u0e23\u0e30\u0e22\u0e38\u0e01\u0e15\u0e4c\u0e43\u0e0a\u0e49\u0e42\u0e21\u0e40\u0e14\u0e25\u0e15\u0e48\u0e2d\u0e44\u0e1b \u0e42\u0e14\u0e22\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e35\u0e48\u0e08\u0e30\u0e19\u0e33\u0e21\u0e32\u0e17\u0e33\u0e42\u0e21\u0e40\u0e14\u0e25\u0e0a\u0e37\u0e48\u0e2d X \u0e41\u0e25\u0e30\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e35\u0e48\u0e08\u0e30\u0e19\u0e33\u0e44\u0e1b\u0e1b\u0e23\u0e30\u0e22\u0e38\u0e01\u0e15\u0e4c\u0e43\u0e0a\u0e49 model \u0e0a\u0e37\u0e48\u0e2d X_test_data \u0e19\u0e33 X \u0e41\u0e25\u0e30 y \u0e21\u0e32\u0e41\u0e1a\u0e48\u0e07\u0e40\u0e1b\u0e47\u0e19\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a test \u0e41\u0e25\u0e30 train model \u0e1c\u0e48\u0e32\u0e19 sklearn.model_selection.train_test_split \u0e42\u0e14\u0e22\u0e41\u0e1a\u0e48\u0e07\u0e40\u0e1b\u0e47\u0e19\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e17\u0e33\u0e42\u0e21\u0e40\u0e14\u0e25 80% \u0e41\u0e25\u0e30\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e14\u0e2a\u0e2d\u0e1a 20% \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e1b\u0e23\u0e30\u0e01\u0e32\u0e28\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23 log \u0e0b\u0e36\u0e48\u0e07\u0e21\u0e35\u0e04\u0e38\u0e13\u0e2a\u0e21\u0e1a\u0e31\u0e15\u0e34\u0e40\u0e1b\u0e47\u0e19 logistic regression model \u0e41\u0e25\u0e30\u0e19\u0e33\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e17\u0e14\u0e2a\u0e2d\u0e1a\u0e43\u0e2a\u0e48\u0e40\u0e02\u0e49\u0e32\u0e44\u0e1b\u0e43\u0e19\u0e42\u0e21\u0e40\u0e14\u0e25\u0e40\u0e01\u0e47\u0e1a\u0e44\u0e27\u0e49\u0e43\u0e19\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23\u0e0a\u0e37\u0e48\u0e2d y_pred_X \u0e19\u0e33\u0e44\u0e1b\u0e15\u0e23\u0e27\u0e08\u0e2a\u0e2d\u0e1a accuracy_score  \u0e42\u0e14\u0e22\u0e40\u0e17\u0e35\u0e22\u0e1a\u0e23\u0e30\u0e2b\u0e27\u0e48\u0e32\u0e07 y_test \u0e41\u0e25\u0e30 y_pred_X \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e19\u0e33\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e21\u0e32\u0e17\u0e33\u0e19\u0e32\u0e22\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e35\u0e48\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23 \u0e42\u0e14\u0e22\u0e40\u0e01\u0e47\u0e1a\u0e1c\u0e25\u0e25\u0e31\u0e1e\u0e18\u0e4c\u0e44\u0e27\u0e49\u0e43\u0e19\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23\u0e0a\u0e37\u0e48\u0e2d prediction","da7c99a0":"# Method\n\n   import module \u0e17\u0e35\u0e48\u0e43\u0e0a\u0e49\u0e08\u0e33\u0e40\u0e1b\u0e47\u0e19\u0e15\u0e48\u0e2d\u0e01\u0e32\u0e23\u0e27\u0e34\u0e40\u0e04\u0e23\u0e32\u0e30\u0e2b\u0e4c","016a950e":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \u0e14\u0e39 classification report","f46181ee":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  \u0e2a\u0e23\u0e49\u0e32\u0e07 function \u0e0a\u0e37\u0e48\u0e2d Expand_contraction \u0e42\u0e14\u0e22\u0e21\u0e35 argument \u0e40\u0e1b\u0e47\u0e19\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21(text) \u0e43\u0e19 function \u0e08\u0e30\u0e21\u0e35\u0e01\u0e32\u0e23 split \u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e41\u0e22\u0e01\u0e04\u0e33\u0e2d\u0e2d\u0e01\u0e21\u0e32 \u0e41\u0e25\u0e49\u0e27\u0e2a\u0e23\u0e49\u0e32\u0e07 list \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e40\u0e01\u0e47\u0e1a\u0e04\u0e33\u0e0a\u0e37\u0e48\u0e2d new_text \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19 for loop \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e14\u0e39\u0e27\u0e48\u0e32\u0e04\u0e33\u0e43\u0e14\u0e1a\u0e49\u0e32\u0e07\u0e17\u0e35\u0e48\u0e40\u0e1b\u0e47\u0e19\u0e15\u0e23\u0e07\u0e01\u0e31\u0e1a\u0e04\u0e48\u0e32 key \u0e43\u0e19 dictionary contractions \u0e2b\u0e32\u0e01\u0e15\u0e23\u0e07\u0e08\u0e30\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19\u0e43\u0e2b\u0e49\u0e40\u0e1b\u0e47\u0e19 value \u0e17\u0e35\u0e48\u0e01\u0e33\u0e2b\u0e19\u0e14\u0e44\u0e27\u0e49\u0e41\u0e25\u0e49\u0e27\u0e19\u0e33\u0e44\u0e1b\u0e40\u0e01\u0e47\u0e1a\u0e44\u0e27\u0e49\u0e43\u0e19 litst \u0e0a\u0e37\u0e48\u0e2d new_text \u0e2b\u0e32\u0e01\u0e44\u0e21\u0e48\u0e15\u0e23\u0e07\u0e08\u0e30\u0e43\u0e0a\u0e49\u0e04\u0e48\u0e32\u0e40\u0e14\u0e34\u0e21 \u0e41\u0e25\u0e49\u0e27\u0e19\u0e33\u0e44\u0e1b\u0e40\u0e01\u0e47\u0e1a\u0e44\u0e27\u0e49\u0e43\u0e19 list \u0e0a\u0e37\u0e48\u0e2d new_text \u0e19\u0e33\u0e04\u0e33\u0e43\u0e19 list \u0e0a\u0e37\u0e48\u0e2d new text \u0e21\u0e32 join \u0e01\u0e25\u0e31\u0e1a\u0e40\u0e1b\u0e47\u0e19 string \u0e2d\u0e35\u0e01\u0e04\u0e23\u0e31\u0e49\u0e07 \u0e44\u0e14\u0e49\u0e40\u0e1b\u0e47\u0e19\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e17\u0e35\u0e48\u0e41\u0e1b\u0e25\u0e07\u0e04\u0e33\u0e17\u0e35\u0e48\u0e40\u0e1b\u0e47\u0e19 contraction word \u0e41\u0e25\u0e49\u0e27","e051afee":"# Data Set\n\u0e17\u0e32\u0e07\u0e1c\u0e39\u0e49\u0e08\u0e31\u0e14\u0e17\u0e33\u0e44\u0e14\u0e49\u0e43\u0e2b\u0e49\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e21\u0e32\u0e17\u0e31\u0e49\u0e07\u0e2b\u0e21\u0e14 3 \u0e0a\u0e38\u0e14\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e1b\u0e23\u0e30\u0e01\u0e2d\u0e1a\u0e44\u0e1b\u0e14\u0e49\u0e27\u0e22 <br\/><br\/>\n    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.) train.csv \u0e15\u0e31\u0e27\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e1b\u0e23\u0e30\u0e01\u0e2d\u0e1a\u0e44\u0e1b\u0e14\u0e49\u0e27\u0e22\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e31\u0e49\u0e07\u0e2b\u0e21\u0e14 7613 \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25 \u0e41\u0e25\u0e30\u0e1b\u0e23\u0e30\u0e01\u0e2d\u0e1a\u0e44\u0e1b\u0e14\u0e49\u0e27\u0e22 attributes \u0e14\u0e31\u0e07\u0e19\u0e35\u0e49 <br\/><br\/>\n    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.1) id \u0e04\u0e37\u0e2d \u0e25\u0e33\u0e14\u0e31\u0e1a\u0e40\u0e25\u0e02\u0e44\u0e2d\u0e14\u0e35\u0e02\u0e2d\u0e07\u0e1c\u0e39\u0e49\u0e43\u0e0a\u0e49 <br\/>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.2) keyword \u0e04\u0e37\u0e2d \u0e04\u0e33\u0e40\u0e09\u0e1e\u0e32\u0e30\u0e17\u0e35\u0e48\u0e43\u0e0a\u0e49\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22\u0e08\u0e32\u0e01\u0e43\u0e19 tweet \u0e19\u0e31\u0e49\u0e19\u0e46 <br\/>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.3) location \u0e04\u0e37\u0e2d \u0e1e\u0e34\u0e01\u0e31\u0e14\u0e02\u0e2d\u0e07\u0e1c\u0e39\u0e49\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19 <br\/>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.4) text \u0e04\u0e37\u0e2d \u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e17\u0e35\u0e48\u0e1c\u0e39\u0e49\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\u0e42\u0e1e\u0e2a\u0e15\u0e4c\u0e25\u0e07 twitter <br\/>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.5) target \u0e04\u0e37\u0e2d \u0e1c\u0e25\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22\u0e27\u0e48\u0e32\u0e40\u0e1b\u0e47\u0e19\u0e1b\u0e23\u0e30\u0e42\u0e22\u0e04\u0e17\u0e35\u0e48\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e02\u0e49\u0e2d\u0e07\u0e01\u0e31\u0e1a\u0e20\u0e31\u0e22\u0e1e\u0e34\u0e1a\u0e31\u0e15\u0e34\u0e2b\u0e23\u0e37\u0e2d\u0e44\u0e21\u0e48 \u0e42\u0e14\u0e22\u0e17\u0e35\u0e48 0 \u0e2b\u0e21\u0e32\u0e22\u0e16\u0e36\u0e07 \u0e44\u0e21\u0e48\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e02\u0e49\u0e2d\u0e07 \u0e41\u0e25\u0e30 1 \u0e2b\u0e21\u0e32\u0e22\u0e16\u0e36\u0e07 \u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e02\u0e49\u0e2d\u0e07 <br\/><br\/>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.) test.csv \u0e15\u0e31\u0e27\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e1b\u0e23\u0e30\u0e01\u0e2d\u0e1a\u0e44\u0e1b\u0e14\u0e49\u0e27\u0e22\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e31\u0e49\u0e07\u0e2b\u0e21\u0e14 3623 \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25 \u0e41\u0e25\u0e30\u0e1b\u0e23\u0e30\u0e01\u0e2d\u0e1a\u0e44\u0e1b\u0e14\u0e49\u0e27\u0e22 attributes \u0e14\u0e31\u0e07\u0e19\u0e35\u0e49 <br\/><br\/>\n    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.1) id \u0e04\u0e37\u0e2d \u0e25\u0e33\u0e14\u0e31\u0e1a\u0e40\u0e25\u0e02\u0e44\u0e2d\u0e14\u0e35\u0e02\u0e2d\u0e07\u0e1c\u0e39\u0e49\u0e43\u0e0a\u0e49 <br\/>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2) keyword \u0e04\u0e37\u0e2d \u0e04\u0e33\u0e40\u0e09\u0e1e\u0e32\u0e30\u0e17\u0e35\u0e48\u0e43\u0e0a\u0e49\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22\u0e08\u0e32\u0e01\u0e43\u0e19 tweet \u0e19\u0e31\u0e49\u0e19\u0e46 <br\/>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3) location \u0e04\u0e37\u0e2d \u0e1e\u0e34\u0e01\u0e31\u0e14\u0e02\u0e2d\u0e07\u0e1c\u0e39\u0e49\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19 <br\/>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.4) text \u0e04\u0e37\u0e2d \u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e17\u0e35\u0e48\u0e1c\u0e39\u0e49\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\u0e42\u0e1e\u0e2a\u0e15\u0e4c\u0e25\u0e07 twitter <br\/><br\/>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.) sample_submission.csv \u0e0a\u0e38\u0e14\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e19\u0e35\u0e49\u0e17\u0e33\u0e2b\u0e19\u0e49\u0e32\u0e40\u0e01\u0e47\u0e1a\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25 id \u0e41\u0e25\u0e30 target \u0e02\u0e2d\u0e07\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21 \u0e41\u0e15\u0e48\u0e25\u0e30\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e27\u0e48\u0e32\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e02\u0e49\u0e2d\u0e07\u0e01\u0e31\u0e1a\u0e1b\u0e23\u0e30\u0e42\u0e22\u0e04\u0e20\u0e31\u0e22\u0e1e\u0e34\u0e1a\u0e31\u0e15\u0e34\u0e2b\u0e23\u0e37\u0e2d\u0e44\u0e21\u0e48","03b371aa":"\n\n   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \u0e14\u0e39\u0e08\u0e33\u0e19\u0e27\u0e19\u0e02\u0e2d\u0e07\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e27\u0e48\u0e32\u0e40\u0e1b\u0e47\u0e19 0 (\u0e1b\u0e23\u0e30\u0e42\u0e22\u0e04\u0e44\u0e21\u0e48\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e01\u0e31\u0e1a\u0e20\u0e31\u0e22\u0e1e\u0e34\u0e1a\u0e31\u0e15\u0e34) \u0e41\u0e25\u0e30 1 (\u0e1b\u0e23\u0e30\u0e42\u0e22\u0e04\u0e17\u0e35\u0e48\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e01\u0e31\u0e1a\u0e20\u0e31\u0e22\u0e1e\u0e34\u0e1a\u0e31\u0e15\u0e34) \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 funtion value count \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e19\u0e31\u0e1a\u0e08\u0e33\u0e19\u0e27\u0e19 value 0 \u0e01\u0e31\u0e1a 1 \u0e43\u0e19\u0e04\u0e2d\u0e25\u0e31\u0e21\u0e19\u0e4c target\n\u0e41\u0e25\u0e49\u0e27 plot \u0e42\u0e14\u0e22 seaborn barplot\n\n\n","8e1e33d2":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  \u0e1b\u0e23\u0e30\u0e01\u0e32\u0e28\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23 dic \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 defaultdict \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e1b\u0e49\u0e2d\u0e07\u0e01\u0e31\u0e19 key error \u0e19\u0e31\u0e1a\u0e08\u0e33\u0e19\u0e27\u0e19\u0e04\u0e33\u0e17\u0e38\u0e01\u0e04\u0e33\u0e17\u0e35\u0e48 \u0e40\u0e1b\u0e47\u0e19 stop word \u0e43\u0e19 corpus \u0e42\u0e14\u0e22\u0e43\u0e2b\u0e49 key \u0e40\u0e1b\u0e47\u0e19\u0e04\u0e33\u0e28\u0e31\u0e1e\u0e17\u0e4c\u0e41\u0e25\u0e30 value \u0e40\u0e1b\u0e47\u0e19\u0e08\u0e33\u0e19\u0e27\u0e19\u0e04\u0e33\u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49\u0e01\u0e32\u0e23 for loop \u0e41\u0e25\u0e30\u0e40\u0e1e\u0e34\u0e48\u0e21 value \u0e2d\u0e35\u0e011 \u0e40\u0e21\u0e37\u0e48\u0e2d\u0e04\u0e33\u0e14\u0e31\u0e07\u0e01\u0e25\u0e48\u0e32\u0e27\u0e40\u0e1b\u0e47\u0e19\u0e44\u0e1b\u0e15\u0e32\u0e21\u0e40\u0e07\u0e37\u0e48\u0e2d\u0e19\u0e44\u0e02\u0e04\u0e37\u0e2d \u0e40\u0e1b\u0e47\u0e19 stop word \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e19\u0e33\u0e21\u0e32\u0e40\u0e23\u0e35\u0e22\u0e07\u0e42\u0e14\u0e22\u0e01\u0e32\u0e23 sorted \u0e08\u0e32\u0e01\u0e21\u0e32\u0e01\u0e44\u0e1b\u0e19\u0e49\u0e2d\u0e22\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e43\u0e2b\u0e49\u0e07\u0e48\u0e32\u0e22\u0e15\u0e48\u0e2d\u0e01\u0e32\u0e23\u0e14\u0e39\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e40\u0e23\u0e35\u0e22\u0e01\u0e14\u0e39\u0e41\u0e04\u0e48 30 \u0e15\u0e31\u0e27\u0e41\u0e23\u0e01 \u0e41\u0e25\u0e30\u0e19\u0e33\u0e21\u0e32\u0e1e\u0e25\u0e47\u0e2d\u0e15\u0e01\u0e23\u0e32\u0e1f \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e14\u0e39\u0e27\u0e48\u0e32\u0e04\u0e33\u0e43\u0e14\u0e1a\u0e49\u0e32\u0e07\u0e17\u0e35\u0e48\u0e40\u0e1b\u0e47\u0e19 stopword \u0e41\u0e25\u0e30 \u0e21\u0e35\u0e08\u0e33\u0e19\u0e27\u0e19\u0e40\u0e17\u0e48\u0e32\u0e44\u0e23\u0e43\u0e19 corpus "}}