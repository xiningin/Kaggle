{"cell_type":{"7616f57a":"code","e90d27ff":"code","c17889bb":"code","5a1f4858":"code","a97666c5":"code","831132bd":"code","298e9068":"code","3153d1f4":"code","93d863c5":"code","cc8599e9":"code","4f8cecaf":"code","2726de51":"code","e8afc57a":"code","ce2aff29":"code","49b30d7e":"code","aad7413e":"code","15826b0f":"code","9d20134b":"code","5422f477":"code","aba53c0c":"code","940323f2":"code","b0416d5f":"code","7babe85c":"code","9928f659":"code","0bce618d":"code","755bf761":"code","f604bf6d":"code","032f7b8a":"code","0ed7f28a":"code","15ac8298":"code","9bc72c13":"code","50351fd7":"code","85e35d2e":"code","74291a4b":"code","5f82efec":"code","98b1d800":"code","119dd8d4":"code","df8679e6":"code","89e3f6ec":"code","2251a899":"code","76bc866a":"code","7e2a3374":"code","91834c11":"code","2aa3fda7":"code","a80b52b5":"code","ca41c03f":"code","95a9322d":"code","a247acb9":"code","9dc56f1c":"code","f115ee8f":"code","c157e08a":"code","2c4e2786":"code","19a189c6":"code","5f9b4e36":"code","588dc87a":"code","d5e16897":"code","8b48e571":"code","6c837791":"code","c703b1dc":"code","e9f596df":"code","34475226":"markdown","5480baf3":"markdown","48504ea5":"markdown","2499a646":"markdown","7e110995":"markdown","5aebd4af":"markdown","6ecae9ce":"markdown","d16e3d2e":"markdown","d71cefd4":"markdown","09daf76f":"markdown","de68402f":"markdown","b3e5ff7b":"markdown","8bfed462":"markdown","7b0b12ea":"markdown","3537492d":"markdown","e70b07e0":"markdown","c9f4dedb":"markdown","fb8a63a8":"markdown","2132cedf":"markdown","f923fe92":"markdown","da1e7d61":"markdown","310cc125":"markdown","afad268a":"markdown","31c1fde6":"markdown","377deef1":"markdown","cbfd173b":"markdown"},"source":{"7616f57a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport warnings\nimport scipy.linalg\nimport scipy.stats\nimport seaborn as sns\nimport warnings\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, Normalizer\nfrom sklearn.metrics import roc_auc_score","e90d27ff":"def fxn():\n    warnings.warn(\"deprecated\", DeprecationWarning)\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    fxn()","c17889bb":"jitter = 1e-10","5a1f4858":"train_path = \"..\/input\/santander-customer-transaction-prediction\/train.csv\"","a97666c5":"df = pd.read_csv(train_path)\ndf.drop('ID_code',inplace=True, axis=1)\n#print(df.describe())\n#print(df.head())","831132bd":"ones = len(df[df['target'] == 1])\nzeros = len(df[df['target'] == 0])\n\nclasses = ['Target 0','Target 1']\n\nplt.bar(classes,[zeros,ones],color='blue',edgecolor='black')\nplt.xticks(classes)\n\nplt.bar([0,1],[zeros,ones])\nplt.xlabel('Class', fontsize=16)\nplt.ylabel('Count', fontsize=16)\nplt.title(\"Diagram of imbalance of the two classes\")\nplt.show()\n\nclass_imbalance = ones\/len(df)\n\nprint(\"Class imbalance : \", class_imbalance*100, \"% of positive targets\")","298e9068":"#Features histograms\n\ndf.hist(figsize = (50,50))\nplt.show()","3153d1f4":"threshold = 0.112\ndf = df[(np.random.rand(df.shape[0]) < threshold) | (df[\"target\"] == 1)]","93d863c5":"ones = len(df[df['target'] == 1])\nzeros = len(df[df['target'] == 0])\n\nclasses = ['Target 0','Target 1']\n\nplt.bar(classes,[zeros,ones],color='blue',edgecolor='black')\nplt.xticks(classes)\n\nplt.bar([0,1],[zeros,ones])\nplt.xlabel('Class', fontsize=16)\nplt.ylabel('Count', fontsize=16)\nplt.title(\"Classes after undersampling\")\nplt.show()\n\nprint(\"Class undersampling : \",ones\/len(df)*100, \"% of positive targets\")","cc8599e9":"numpy_data = df.to_numpy() \n\ntargets = numpy_data[:,0]\nfeatures = numpy_data[:,1:len(numpy_data) - 1]\n\nones_count = np.count_nonzero(targets == 1)\nzeros_count = np.count_nonzero(targets == 0)\nunbalanced = ones_count \/ (zeros_count + ones_count)\n\nX_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.1, random_state=42)","4f8cecaf":"# build the design matrix\n\ndef build_X(X):    \n    init = np.ones(len(X))\n    \n    matrix = init\n        \n    for i in range(200):\n        column = []\n        for j in range(len(X)):\n            column.append(X[j,i])\n        matrix = np.column_stack((matrix,column))\n\n    return matrix","2726de51":"def compute_posterior(X, y, sigma2priorweights, sigma2noise):\n            \n    Sigma_inverse = X.T @ X * (1\/sigma2noise) + np.linalg.inv(sigma2priorweights)\n    \n    posterior_Sigma = np.linalg.inv(Sigma_inverse)\n        \n    posterior_mu = (1\/sigma2noise) * posterior_Sigma @ X.T @ y\n\n    return posterior_mu, posterior_Sigma","e8afc57a":"sc = StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","ce2aff29":"sigma2priorweights = 1\nsigma2noise = 1\n\nbigX = build_X(X_train)\n\nw_posterior_mu, w_posterior_Sigma = compute_posterior(bigX, y_train, np.identity(201)*sigma2priorweights, sigma2noise)","49b30d7e":"print(w_posterior_Sigma)\n\nfig, ax = plt.subplots()\nax.matshow(w_posterior_Sigma)\nax.grid(None)\nplt.title(\"Posterior variance of the weights\")\nplt.show()","aad7413e":"def compute_predictive(Xnew, w_posterior_mu, w_posterior_Sigma, sigma2noise):\n    \n    Xnew = build_X(Xnew)\n    \n    y_posterior_mu = Xnew @ w_posterior_mu \n    y_posterior_Sigma = sigma2noise + Xnew @ w_posterior_Sigma @ Xnew.T\n    \n    return y_posterior_mu, y_posterior_Sigma","15826b0f":"y_posterior_mu, y_posterior_Sigma = compute_predictive(X_test,w_posterior_mu,w_posterior_Sigma,1)","9d20134b":"def result_analysis(prediction,control):\n    TP = 0\n    FP = 0\n    TN = 0\n    FN = 0\n\n    for i in range(len(prediction)):\n        if(prediction[i] == 1 and control[i] == 1):\n            TP += 1\n        if(prediction[i] == 0 and control[i] == 1):\n            FN += 1\n        if(prediction[i] == 1 and control[i] == 0):\n            FP += 1\n        else:\n            TN += 1\n            \n    accuracy = (TP + TN) \/ (TP + TN + FP + FN)\n\n    se = TP \/ (TP + FN + jitter)\n    sp = TN \/ (TN + FP + jitter)\n\n    error_rate = 1 - accuracy\n        \n    auc_score = roc_auc_score(control, prediction)\n        \n    return TP, TN, FP, FN, accuracy, error_rate, se, sp, auc_score","5422f477":"def threshold(prediction,value):\n    rounded_prediction = []\n    for i in range(len(prediction)):\n        if(prediction[i] < value):\n            rounded_prediction.append(0)\n        else:\n            rounded_prediction.append(1)\n    return rounded_prediction","aba53c0c":"def confusion_matrix(prediction,control,title):\n    \n    TP, TN, FP, FN, accuracy, error_rate, se, sp, auc = result_analysis(prediction,control)\n            \n    confusion_matrix = [[FP,TP],[TN,FN]]\n    df_cm = pd.DataFrame(confusion_matrix, [1,0], [0,1])\n    # plt.figure(figsize=(10,7))\n    sns.set(font_scale=1.4) # for label size\n    sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n    \n    plt.xlabel(\"True values\")\n    plt.ylabel(\"Predicted values\")\n    plt.title(title)\n    \n    plt.show()\n    \n    print(\"accuracy : \",accuracy)\n    print(\"error rate : \", error_rate)\n    print(\"Sensitivity : \", se)\n    print(\"Specificity : \", sp)\n    print(\"AUC : \", auc)","940323f2":"BLR_prediction = threshold(y_posterior_mu,0.5)","b0416d5f":"confusion_matrix(BLR_prediction,y_test,\"Bayesian Linear Regression Confusion Matrix\")","7babe85c":"def MH_logistic(z):\n    return 1\/(1 + np.exp(-z))","9928f659":"class BernoulliLikelihood():\n    def logdensity(self, y, p):\n        value = np.sum(y*np.log(p + jitter) + (1-y) * np.log(1-p + jitter))\n        return value\n\nclass NormalPrior():\n    def __init__(self, sigma2x):\n        self.sigma2x = sigma2x\n        \n    def logdensity(self, x):\n        first_term = 1\/((2*np.pi) * np.linalg.det(self.sigma2x)**(1\/2)) \n        second_term = np.exp(-1\/2 * x.T @ np.linalg.inv(self.sigma2x) @ x)\n        value = np.log(first_term * second_term + jitter)\n        return value","0bce618d":"class MHSampler():\n    @property\n    def samples(self):\n        return self._samples\n    @samples.getter    \n    def samples(self):\n        return np.asarray(self._samples)\n    \n    def __init__(self, initial_sample, likelihood, prior):\n        self.likelihood = likelihood\n        self.prior = prior\n        self._samples = [initial_sample]\n    \n    def unnormalized_logposterior(self, w, X, y):\n        p = MH_logistic(X@w)\n        log_likelihood = self.likelihood.logdensity(y,p)\n        log_prior = self.prior.logdensity(w)\n        return log_likelihood + log_prior\n\n    def step(self, X, y, step_proposal):\n        w_prev = self._samples[-1]\n        w_proposal = step_proposal(w_prev)\n        \n        log_gw_prev = self.unnormalized_logposterior(w_prev, X, y)\n        log_gw_proposal = self.unnormalized_logposterior(w_proposal, X, y)\n\n        acceptance_ratio = np.exp(log_gw_proposal - log_gw_prev)\n        \n        if acceptance_ratio >= 1:\n            self._samples.append(w_proposal)\n        else:\n            u = random.uniform(0.0,1.0)\n            if u <= acceptance_ratio:\n                self._samples.append(w_proposal)\n            else:\n                self._samples.append(w_prev)\n        \n        return min(acceptance_ratio, 1)","755bf761":"likelihood = BernoulliLikelihood()\nprior = NormalPrior(np.identity(200))\n\n#starting_point = np.random.randn(1, 200)\n#sampler = MHSampler(starting_point[0,:],likelihood,prior)\n\n#starting_point = np.zeros((200))\n#sampler = MHSampler(starting_point,likelihood,prior)\n\nsampler = MHSampler(X_train[0],likelihood,prior)","f604bf6d":"def step_proposal(sample):\n    new_sample = np.random.randn(1, 200) * 0.1 + sample\n    return new_sample[0,:]\n\nnum_iterations = 5000\nfor step in range(num_iterations):\n    acceptance = sampler.step(X_train,y_train,step_proposal)\n    print('Metropolis Hastings Training : Epoch [{}\/{}]'.format(step, num_iterations),end=\"\\r\")","032f7b8a":"def _rhat_base(ary):\n    \"\"\"Compute the rhat for a 2d array.\"\"\"\n    _, num_samples = ary.shape\n\n    # Calculate chain mean\n    chain_mean = np.mean(ary, axis=1)\n    # Calculate chain variance\n    chain_var = np.var(ary, axis=1, ddof=1)\n    # Calculate between-chain variance\n    between_chain_variance = num_samples * np.var(chain_mean, axis=None, ddof=1)\n    # Calculate within-chain variance\n    within_chain_variance = np.mean(chain_var)\n    # Estimate of marginal posterior variance\n    rhat_value = np.sqrt(\n        (between_chain_variance \/ within_chain_variance + num_samples - 1) \/ (num_samples)\n    )\n    return rhat_value\n\n\ndef _rhat_rank(ary):\n    \"\"\"Compute the rank normalized rhat. \n    Computation follows https:\/\/arxiv.org\/abs\/1903.08008\n    \"\"\"\n    \n    def _z_scale(ary):\n        rank = scipy.stats.rankdata(ary, method=\"average\")\n        z = scipy.stats.norm.ppf((rank - 0.5) \/ ary.size)\n        return z.reshape(ary.shape)\n    \n    \n    def _split_chains(ary):\n        \"\"\"Split and stack chains.\"\"\"\n        _, n_draw = ary.shape\n        half = n_draw \/\/ 2\n        return np.vstack((ary[:, :half], ary[:, -half:]))\n    \n    split_ary = _split_chains(ary)\n    rhat_bulk = _rhat_base(_z_scale(split_ary))\n\n    split_ary_folded = abs(split_ary - np.median(split_ary))\n    rhat_tail = _rhat_base(_z_scale(split_ary_folded))\n\n    rhat_rank = max(rhat_bulk, rhat_tail)\n    return rhat_rank\n\ndef compute_rhat(samples):\n    \"\"\"Compute the rhat statistics from samples. Samples needs to be a tensor \n    with dimensions [num_of_chain, num_of_samples, num_of_variables]. \"\"\"\n    \n    samples = np.atleast_3d(samples)\n    return np.asarray([_rhat_rank(samples[...,i]) for i in range(samples.shape[-1]) ])","0ed7f28a":"rhat = compute_rhat(sampler.samples)[0]\n\nprint(\"rhat of Metropolis Hastings : \", rhat)","15ac8298":"%config InlineBackend.figure_format = 'retina'\nimport numpy as np\nimport scipy as scipy\nimport scipy.spatial\nimport time\nimport random\n\nimport matplotlib \nimport matplotlib.font_manager\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#matplotlib.rc_file('~\/.config\/matplotlib\/matplotlibrc')\nimport warnings\nimport pandas as pd\nimport torch\nimport torch.nn as nn\n\nwarnings.filterwarnings(\"ignore\")\ndef set_seed(seed: int=0):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    \ndef args_as_tensors(*index):\n    \"\"\"A simple decorator to convert numpy arrays to torch tensors\"\"\"\n    def decorator(method):\n        def wrapper(*args, **kwargs):\n            converted_args = [torch.tensor(a).float() \n                              if i in index and type(a) is np.ndarray else a \n                              for i, a in enumerate(args)]\n            return method(*converted_args, **kwargs)\n        return wrapper  \n    return decorator","9bc72c13":"class Distribution(nn.Module):  \n    pass\n\nclass Bernoulli(Distribution):\n    @args_as_tensors(1, 2)\n    def logdensity(self, y, p):\n        return y * torch.log(p + jitter) + (1-y) * torch.log(1 - p + jitter)","50351fd7":"class NormalDiagonal(Distribution):\n\n    @property\n    def var(self):\n        return self.logvar.exp()\n    \n    def extra_repr(self):\n        return 'train=%s' % self.train\n    \n    def __init__(self, d, train=True):\n        super(NormalDiagonal, self).__init__()\n        self.train = train\n        self.d = d\n        self.mean = torch.nn.Parameter(torch.zeros(d), requires_grad=train)\n        self.logvar = torch.nn.Parameter(torch.zeros(d), requires_grad=train)\n                                    \n    def sample(self, n=1):\n\n        eps = torch.randn(n,self.d,requires_grad=self.train) \n        samples = self.mean + eps * torch.sqrt(self.var)\n                                                        \n        return samples","85e35d2e":"from functools import total_ordering\n\n_KL_REGISTRY = {}  # Source of truth mapping a few general (type, type) pairs to functions.\n_KL_MEMOIZE = {}  # Memoized version mapping many specific (type, type) pairs to functions.\n\n@total_ordering\nclass _Match(object):\n    __slots__ = ['types']\n\n    def __init__(self, *types):\n        self.types = types\n\n    def __eq__(self, other):\n        return self.types == other.types\n\n    def __le__(self, other):\n        for x, y in zip(self.types, other.types):\n            if not issubclass(x, y):\n                return False\n            if x is not y:\n                break\n        return True\n\ndef _dispatch_kl(type_q, type_p):\n    matches = [(super_q, super_p) for super_q, super_p in _KL_REGISTRY\n               if issubclass(type_q, super_q) and issubclass(type_p, super_p)]\n    if not matches:\n        return NotImplemented\n    left_q, left_p = min(_Match(*m) for m in matches).types\n    right_p, right_q = min(_Match(*reversed(m)) for m in matches).types\n    left_fun = _KL_REGISTRY[left_q, left_p]\n    right_fun = _KL_REGISTRY[right_q, right_p]\n    if left_fun is not right_fun:\n        logger.warning('Ambiguous kl_divergence({}, {}). Please register_kl({}, {})'.format(\n            type_q.__name__, type_p.__name__, left_q.__name__, right_p.__name__))\n    return left_fun\n\n\ndef register_kl(type_q, type_p):\n    \"\"\"\n    Decorator to register a pairwise function with kl_divergence.\n    Usage:\n\n        @register_kl(Normal, Normal)\n        def kl_normal_normal(q, p):\n            # insert implementation here\n    \"\"\"\n    if not isinstance(type_q, type) and issubclass(type_q, BaseDistribution):\n        raise TypeError('Expected type_q to be a Distribution subclass but got {}'.format(type_q))\n    if not isinstance(type_p, type) and issubclass(type_p, BaseDistribution):\n        raise TypeError('Expected type_p to be a Distribution subclass but got {}'.format(type_p))\n    \n    def decorator(fun):\n        _KL_REGISTRY[type_q, type_p] = fun\n        _KL_MEMOIZE.clear()  # reset since lookup order may have changed\n        print('KL divergence between \\'%s\\' and \\'%s\\' registered.' % (type_q.__name__, type_p.__name__))\n        return fun\n    return decorator\n\n\ndef kl_divergence(q, p):\n    r\"\"\"Compute Kullback-Leibler divergence KL(p|q) between two distributions.\"\"\"\n    try:\n        fun = _KL_MEMOIZE[type(q), type(p)]\n    except KeyError:\n        fun = _dispatch_kl(type(q), type(p))\n        _KL_MEMOIZE[type(q), type(p)] = fun\n    if fun is NotImplemented:\n        raise NotImplementedError('KL divergence for pair %s - %s not registered' % (type(q).__name__,\n                                                                                     type(p).__name__))\n    return fun(q, p)","74291a4b":"@register_kl(NormalDiagonal, NormalDiagonal)\ndef _normaldiagonal_normaldiagonal(q, p):\n\n    kl = torch.log(p.var \/ q.var) + (q.var + torch.square((q.mean - p.mean))) \/ p.var  - 1\n    \n    return 1\/2 * torch.sum(kl)","5f82efec":"def VI_logistic(z):\n    return 1\/(1 + torch.exp(-z))\n\nclass LogisticRegression(nn.Module):\n    def __init__(self, input_dim):\n        super(LogisticRegression, self).__init__()\n        \n        self.prior_w = NormalDiagonal(input_dim,False)\n        self.posterior_w = NormalDiagonal(input_dim)\n\n    @args_as_tensors(1)\n    def predict_y(self, X, mc_samples=1):\n        \n        w_samples = self.posterior_w.sample(mc_samples)\n        w_samples = torch.unsqueeze(w_samples, 2)\n        y_samples = VI_logistic(X @ w_samples)\n        \n        return y_samples\n    \n    def predict_vector(self, X, mc_samples=1):\n        \n        w_samples = self.posterior_w.sample(mc_samples)\n        y_samples = X.float() @ w_samples.T\n        prediction = torch.sum(y_samples) \/ mc_samples\n        prediction = logistic(prediction.clone())\n        \n        return prediction","98b1d800":"class VariationalObjective(nn.Module):    \n    def __init__(self, model, likelihood, N, mc_samples=1):\n        super(VariationalObjective, self).__init__()\n        self.N = N\n        self.model = model\n        self.likelihood = likelihood\n        self.mc_samples = mc_samples\n        \n    def expected_loglikelihood(self, Xbatch, ybatch):\n        \n        ypred = self.model.predict_y(Xbatch,self.mc_samples)\n        \n        logliks = self.likelihood.logdensity(ybatch,ypred)\n        \n        logliks = torch.sum(logliks.clone())\n                \n        return self.N\/self.mc_samples * logliks\n    \n    def kl(self):\n        return _normaldiagonal_normaldiagonal(self.model.posterior_w,self.model.prior_w)\n    \n    def compute_objective(self, Xbatch, ybatch):\n        logliks = self.expected_loglikelihood(Xbatch,ybatch)\n                \n        kl = self.kl()\n        \n        result = - logliks + kl\n        \n        return result","119dd8d4":"class Dataset():\n    def __init__(self, X, y, minibatch_size):\n        self.X = X\n        self.y = y \n        self.minibatch_size = min(minibatch_size, len(self.X))\n        self._i = 0  \n    def next_batch(self):  \n        if len(self.X) <= self._i + self.minibatch_size:\n            shuffle = np.random.permutation(len(self.X))\n            self.X = self.X[shuffle]\n            self.y = self.y[shuffle]\n            Xbatch = self.X[self._i:]\n            ybatch = self.y[self._i:]\n            self._i = 0\n            return Xbatch, ybatch\n\n        Xbatch = self.X[self._i:self._i + self.minibatch_size]\n        ybatch = self.y[self._i:self._i + self.minibatch_size]\n        self._i += self.minibatch_size\n        return Xbatch, ybatch","df8679e6":"dataset = Dataset(X_train, y_train, minibatch_size=1)","89e3f6ec":"likelihood = Bernoulli()\nmodel = LogisticRegression(200)\n\nnelbo = VariationalObjective(model,likelihood,200,200)","2251a899":"class Summary:\n    @property\n    def data(self):\n        data = pd.DataFrame(self._data, columns=['step', self.name, 'time'])\n        data.time = data.time - data.time.iloc[0]\n        return data\n    \n    def __init__(self, name):\n        \"\"\"A simple class to store some values during optimization\"\"\"\n        self.name = str(name)\n        self._data = []\n    \n    def append(self, step, value):\n        #self._data.append([step, float(value.detach().numpy()), time.time()])\n        self._data.append([step, float(value), time.time()])","76bc866a":"nelbo_summary = Summary('nelbo')\nnll_summary = Summary('expected_loglik')\nkl_summary = Summary('kl')\n\noptimizer = torch.optim.SGD(nelbo.parameters(), lr=1e-7, momentum=0.9)\n#optimizer = torch.optim.Adam(nelbo.parameters())\n\nnum_iterations = 20000 #50000\n\nfor step in range(num_iterations):\n    \n    optimizer.zero_grad()\n    \n    Xbatch, ybatch = dataset.next_batch()\n    loss = nelbo.compute_objective(Xbatch,ybatch)\n        \n    nelbo_summary.append(step, loss.detach().numpy())    \n    nll_summary.append(step, loss.detach().numpy() - nelbo.kl().detach().numpy())\n    kl_summary.append(step, nelbo.kl().detach().numpy())\n    \n    loss.backward()\n    \n    optimizer.step()\n    \n    print('Epoch [{}\/{}], Loss: {:.4f}'.format(step, num_iterations, loss.item()),end=\"\\r\")","7e2a3374":"fig, axs = plt.subplots(1, 2, figsize=[10, 3])\n\nnelbo_summary.data.plot(x='step', y='nelbo', ax=axs[0]);\nnll_summary.data.plot(x='step', y='expected_loglik', ax=axs[1], c='C1');\nkl_summary.data.plot(x='step', y='kl', ax=axs[1], c='C2');\naxs[1].semilogy();\nfig.suptitle('Optimization of the NELBO', y=1.02)\naxs[0].margins(0, 0.05)","91834c11":"def log_likelihood(y,p):\n    value = 0\n    for i in range(len(p)):\n        value += y[i] * np.log(p[i] + jitter) + (1 - y[i]) * np.log(1 - p[i] + jitter)\n    return value\/len(p)","2aa3fda7":"def MH_predict(x_new, w_samples):\n    p = 0\n    for w in w_samples[-200:]:\n        p += MH_logistic(w.T@x_new)\n    return p\/len(w_samples[-200:])","a80b52b5":"MH_prediction = []\nfor step in range(len(X_test)):\n    MH_prediction.append(MH_predict(X_test[step],sampler.samples))\n    print('Metropolis Hastings Validation : Epoch [{}\/{}]'.format(step, len(X_test)),end=\"\\r\")","ca41c03f":"MH_rounded_prediction = threshold(MH_prediction,0.5)","95a9322d":"confusion_matrix(MH_rounded_prediction,y_test,\"Metropolis Hastings Confusion Matrix\")\nprint(\"log-likelihood : \", log_likelihood(y_test,MH_prediction))","a247acb9":"with torch.no_grad():\n    VI_prediction = nelbo.model.predict_y(X_test,200).detach().numpy()[0]","9dc56f1c":"VI_rounded_prediction = threshold(VI_prediction,0.5)","f115ee8f":"confusion_matrix(VI_rounded_prediction,y_test, \"Variational Inference Confusion Matrix\")\nprint(\"log-likelihood : \", log_likelihood(y_test,VI_prediction)[0])","c157e08a":"def plot_predictive_distribution(prediction,title):\n    plt.hist(prediction, bins = 100)\n    \n    plt.xlabel(\"Predicted values\")\n    plt.ylabel(\"Count\")\n    plt.title(title)\n    \n    plt.show()","2c4e2786":"plot_predictive_distribution(y_posterior_mu,\"Bayesian Linear Regression predictive distribution\")","19a189c6":"plot_predictive_distribution(MH_prediction,\"Metropolis Hastings predictive distribution\")","5f9b4e36":"plot_predictive_distribution(VI_prediction, \"Variational Inference predictive distribution\")","588dc87a":"def uncertainties(prediction,rounded_prediction,control):\n    \n    correct = 0\n    incorrect = 0\n    \n    correct_uncertainties = []\n    incorrect_uncertainties = []\n    \n    for i in range(len(prediction)):\n                \n        if(abs(prediction[i] - 0.5) < 0.2):\n            if(rounded_prediction[i] == control[i]):\n                correct_uncertainties.append(prediction[i])\n                correct += 1\n            else:\n                incorrect_uncertainties.append(prediction[i])\n                incorrect += 1\n        \n    correct_rate = correct \/ (correct + incorrect)\n    incorrect_rate = incorrect \/ (correct + incorrect)\n    \n        \n    print(\"Correct uncertainties rate : \", correct_rate)\n    print(\"Incorrect uncertainties rate : \", incorrect_rate)\n\n    \n    return correct_uncertainties, incorrect_uncertainties","d5e16897":"correct_MH_uncertainties, incorrect_MH_uncertainties = uncertainties(MH_prediction,MH_rounded_prediction,y_test)","8b48e571":"correct_VI_uncertainties, incorrect_VI_uncertainties = uncertainties(VI_prediction,VI_rounded_prediction,y_test)","6c837791":"def plot_double_predictive_distribution(correct_prediction,incorrect_prediction,title):\n    plt.hist(correct_prediction, bins = 20, color=\"green\", alpha=0.5, label=\"correct\")\n    plt.hist(incorrect_prediction, bins = 20, color=\"red\", alpha=0.5, label=\"incorrect\")\n    plt.legend(loc='upper right')\n    plt.xlabel(\"Predicted values\")\n    plt.ylabel(\"Count\")\n    plt.title(title)\n    \n    plt.show()","c703b1dc":"plot_double_predictive_distribution(np.array(correct_MH_uncertainties), np.array(incorrect_MH_uncertainties), \"Metropolis-Hastings uncertainties distribution\")","e9f596df":"plot_double_predictive_distribution(np.array(correct_VI_uncertainties), np.array(incorrect_VI_uncertainties), \"Variational Inference uncertainties distribution\")","34475226":"### Dataset Split","5480baf3":"## Question F\n(text) Report the error metrics implemented in point 2.B. above and the confusion matrix\non the test data. Discuss logistic regression performance with respect to the\nperformance of Bayesian linear regression. [5]\n\n\n|     | Bayesian Linear Regression | Metropolis-Hastings | Variational Inference |\n| --- | --- | --- | --- |\n| Accuracy | 0.85 | 0.82 | 0.75 |\n| Sensitivity | 0.85 | 0.74 | 0.64 |\n| Specificity | 0.85 | 0.86 | 0.80 |\n| AUC | 0.77 | 0.73 | 0.63 |\n\n<span style=\"color:black\">\nAccording to these results, the Bayesian linear regression is slightly more precise than the logistic regressions. However with a longer training for Metropolis-Hastings and Variational Inference I get similar results (around 0.8 of accuracy and 0.75 of auc score). I would therefore say that in terms of cost in time and in computational complexity, Bayesian linear regression seems more interesting on this dataset. However, with an even larger dataset and less Gaussian distributed features, logistic regressions could be more competitive.\n<br\/><br\/>\nWe can see that the three confusion matrices present a satisfactory prediction diagonal with a limited number of false positives and false negatives.\n<\/span>","48504ea5":"### Posterior variance of the weights","2499a646":"## 2. Logistic Regression\n## Question A\n(code) The goal is to implement a Bayesian logistic regression classifier; assume a\nGaussian prior on the parameters. As a first step, implement a Markov chain Monte\nCarlo inference algorithm to infer parameters (you should already have an\nimplementation of the Metropolis-Hastings algorithm from the lab sessions). [10]","7e110995":"## Classification metrics","5aebd4af":"## Question B\n(text) Comment on the distribution of class labels and the dimensionality of the input and how\nthese may affect the analysis. [7]\n\n<span style=\"color:black\">\nThis dataset presents a very high dimensionality compared to the one in the labs (200 vs 2). Overall it will be more difficult for the models to converge and to achieve a high accuracy compared to the labs.\n<br\/><br\/>\nThe targets are highly unbalanced as we can see above on the bar plot (10% of positive targets). To help the models learn correctly, I used an undersampling method (reducing the number of 0 target in the dataset) and it greatly improved the AUC scores of the three models. I used the precision, specificity and AUC scores, which are better than simple accuracy to monitor model performance on unbalanced datasets. Indeed the accuracy of a naive model predicting only 0 on a 90\/10 unbalanced dataset is 0.9 but the auc is 0.5.\n<br\/><br\/>\nThe features are gaussian distributed as we can see in the features histogram. It is useful for the logistic regressions where we assume a gaussian prior on the parameters. Therefore we do not have to make any major features transformation.\n<\/span>","6ecae9ce":"### Variational Inference Prediction","d16e3d2e":"## Question D\n(text) Suggest a way to discretize predictions and display the confusion matrix on the\ntest data and report accuracy [5]\n\n<span style=\"color:black\">\nTo discretize the predictions, we round all predictions greater than 0.5 to class 1 and all predictions less than 0.5 to class 0. It could also be possible to use an adaptative threshold by calculating it on a training dataset, however this is not necessary here since we have rebalanced the classes by undersampling the dataset.\n<br\/><br\/>\nWith the code below, We can plot the confusion matrix with useful metrics like accuracy, sensitivity, specificity and auc.\nThe bayesian linear regression scores pretty well with 0.85 of accuracy and 0.77 of AUC score.  I compared its results with the ones of the Bayesian Linear Regression of sklearn and they are absolutely identical. The only optimization I could implement is too replace the matrix inversion algorithm to accelerate the computations.\n<\/span>","d71cefd4":"### Posteriors computation","09daf76f":"Hugo Danet, EURECOM, Advanced Statistical Inference [ASI], May 2021\n# ASI Assessed exercise\n\n## Santander Customer Transaction Prediction\n\n## Question A\n(code) Download and import the Santander dataset. The labels of the test data are not\npublicly available, so create your own test set by randomly choosing half of the instances in\nthe original training set. [3]","de68402f":"## Question G\n(text) Compare the uncertainties on predictions obtained by the Metropolis-Hastings\nalgorithm and variational inference. First, compare the log-likelihood on test data as a \nglobal metric to assess which inference method yields better uncertainty quantification.\nSecond, pick a few test points for which the mean of the predictive distribution is (a)\naround 0.5 (b) giving a correct prediction (c) giving a wrong prediction, and visualize\/\ndiscuss what the predictive distribution looks like. Discuss the difference between the\nMetropolis-Hastings algorithm and variational inference. [15]\n\n\n |    | Metropolis-Hastings | Variational Inference |\n | --- | --- | --- |\n | log-likelihood | -0.94 | -2.51 |\n    \n<span style=\"color:black\">\nFirst of all, the more steps we carry out with the two models, the less uncertainties there are. With the parameters I defined, Metropolis is a bit more uncertain than Variational Inference according to the histograms I plotted even if the log-likelihoods are -0.94 versus -2.51. Metropolis-Hastings has a better likelihood due to a better accuracy. We can observe on the distributions, that the logistic regressions predict values close to 0 and 1, unlike the Bayesian Linear Regression which is a Gaussian around 0.5. By centering the search around 0.5. I do not notice any differences between MH and VI on the incorrect and correct uncertainty distributions. Perhaps with more training for the two models, some differences would appear.\n<br\/><br\/>\nMore generally, Variational Inference appears to be more scalable since training can be fast to achieve a reasonable result and tuning was glabally easier than with Metropolis-Hastings. However, with the current settings, ie with a slightly longer training, Metropolis-Hastings is more precise as seen in the question F. This dataset is probably not large enough to show that Variational Inference can be more scalable. The other advantage of Variational Inference is that we can try many different models (here a multivariate normal model) to represent the distribution of the posterior. In MCMC, after a correct amount of samples and with a high computational and time cost, any generic distribution could be fitted.\n<\/span>","b3e5ff7b":"### Plotting features distribution","8bfed462":"## Question D\n(text) Comment on the tuning of the Metropolis-Hastings algorithm, and how to\nguarantee that samples are representative of samples of the posterior over model\nparameters. [5] \n\n<span style=\"color:black\">\nTo tune the Metropolis-Hestings algorithm, I first reduced a little the step size compared to the lab (from 0.5 to 0.1). The number of steps is not very important for my implementation since it is quite fast and it achieve a low $\\hat R$ value with less than 1000 steps. The tuning was principally focused on the data preprocessing (undersampling + standardization) and on the starting point which has a huge impact on the convergence speed.\n<br\/><br\/>\nI tried 3 different starting points : the null one, a random one centered on 0 with a variance of 1 on each parameter and the first sample of the train set. The first sample of the train set was the one that gave the more consistent results with a little number of steps.\n<br\/><br\/>\nTo guarantee that samples are representative of samples of the posterior over model parameters we could run multiple iterations of MH with different starting points and verify that it converges each time. I am using the burn in trick by discarding the first samples and therefore giving time to the Markov chain to reach the equilibrum distribution.\n<br\/><br\/>\nAnother possible way to garuantee that samples are representative of samples of the posterior over model parameters is to run independent chains from different starting points and check the obtained distributions. Due to the high dimensionality, visualizing the path of the chains was not the best solution to ensure representativity of samples.\n<br\/><br\/>\nI finally used the potential scale reduction factor $\\hat R$ and verified that its value was less than 1.05 to ensure the chains have been fully mixed.\n<\/span>","7b0b12ea":"## Predictive distribution\n\n### Metropolis-Hastings Prediction","3537492d":"### Variational Inference Training","e70b07e0":"## Question B\n(text) Describe any pre-processing that you suggest for this data [5] \n\n<span style=\"color:black\">\nAccording to the features plot, there is no outliers in the dataset, so a standard scaler is sufficient enough for this work. I also had a problem of exploding gradients conducting to a NaN loss with variational inference and non scaled data.\n<br\/><br\/>\nThe standard scaler is removing the mean and scaling to unit variance. If a feature has a variance with orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n<\/span>\n\n## Data Scaling","c9f4dedb":"## Question C\n(code) Treat class labels as continuous and apply regression to the training data. Also,\ncalculate and report the posterior variance of the weights [10]","fb8a63a8":"### Metropolis-Hastings training","2132cedf":"## Question C\n(code) Based on samples from the posterior over model parameters, write a function\nthat computes the predictive distribution, and write the necessary functions to evaluate\nclassification metrics such as the log-likelihood on test data and error rate. [10]","f923fe92":"### $\\hat{R}$ - statistics","da1e7d61":"### Plotting target imbalance","310cc125":"## Question B\n(code) Implement the variational approximation we studied in the course to obtain an\napproximation to the posterior over model parameters (you should already have an\nimplementation of the from the lab sessions). [10] ","afad268a":"### Bayesian Linear Regression Validation","31c1fde6":"## Question E\n(text) Comment on the tuning of the variational inference algorithm, and discuss the\nbehavior of the optimization with respect to the choice of the optimizer\/step-size. [5]\n\n<span style=\"color:black\">\nAs mentioned in the lab, it is necessary to greatly reduce the number of mc_samples and batch size in order to scale the Variational Inference method on a large dataset. I finally used a batch size of 1 (which explains why the loss is noisy) and a number of mc_samples of 200 which achieve a good trade-off between speed of convergence and accuracy.\n<br\/><br\/>\nSince we are using the SGD optimizer, which has a fixed learning rate unlike Adam, we need to carefully tweak the learning rate. After several tries, I chose 1e-7 for the learning rate. The convergence is relatively slow but necessary in order to not suffer from a problem of explosion of the gradients leading to a NaN loss caused by learning rates greater than 1e-7.\n<\/span>","377deef1":"## 1. Bayesian Linear Regression\n\n## Question A\n(code) Implement Bayesian linear regression (you should already have an\nimplementation from the lab sessions) [10]","cbfd173b":"### Undersampling training set"}}