{"cell_type":{"3db52aab":"code","9c4c27d0":"code","10845249":"code","d20dc66e":"code","1b258a9a":"code","b2e92b05":"code","6101d53c":"code","da836627":"code","d2d19af0":"code","d9dc2d1b":"code","88d3f29a":"code","4aadaecf":"code","10f3fdca":"code","e5dcb968":"code","ef48a737":"code","eea0ccf3":"code","9d46053d":"code","657291ac":"code","c6486bdc":"code","6d638522":"code","c8950dd5":"code","14487456":"code","a5492f47":"code","7ef1ba1e":"code","76fff324":"code","e9dfeb91":"code","f452d020":"code","89ef04c8":"code","5319d50c":"code","ff3cc711":"code","75e772f9":"code","12be46d4":"code","f53b1b5e":"code","ec6064f3":"code","5fd15344":"code","b19a52ad":"code","1d5a27fe":"code","d93c9306":"code","95549fb3":"code","cb5d3b07":"code","56016331":"code","0b52bae1":"code","ce7e8a5b":"code","2f9ca897":"code","50ede0a0":"code","6424723d":"code","414b1721":"code","e86d0a9a":"code","4fb81ece":"markdown","5ecd16ee":"markdown","255a4815":"markdown","5b486552":"markdown","e0853913":"markdown","803721d9":"markdown","27649ab8":"markdown","409f60dd":"markdown","3a22c698":"markdown","7f50eb03":"markdown","67fb5185":"markdown","90737046":"markdown","2a1b675a":"markdown"},"source":{"3db52aab":"!pip install ..\/input\/sacremoses\/sacremoses-master\/ > \/dev\/null\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow_hub as hub\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport gc\nimport sys\nimport os\nimport sys\nimport glob\nimport torch\nimport re \nimport math\nimport pickle\nimport datetime\nimport string \nimport nltk \nimport spacy\nimport tensorflow.keras.backend as K\n\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import GroupKFold\nfrom scipy import spatial\nfrom nltk.tokenize import sent_tokenize\nfrom nltk import wordpunct_tokenize\nfrom sklearn.linear_model import MultiTaskElasticNet\nfrom scipy.stats import spearmanr, rankdata\n\nsys.path.insert(0, \"..\/input\/transformers\/transformers-master\/\")\n\nimport transformers as ppb\n\n\n","9c4c27d0":"DEVICE = torch.device(\"cuda\")","10845249":"root_path = '..\/input\/google-quest-challenge\/'\nss = pd.read_csv(root_path + '\/sample_submission.csv')\ntrain = pd.read_csv(root_path + '\/train.csv')\ntest = pd.read_csv(root_path + '\/test.csv')","d20dc66e":"# train.columns","1b258a9a":"# train['full_text']","b2e92b05":"# technology=train[train.category == \"TECHNOLOGY\"]","6101d53c":"train[['question_title', 'question_body', 'answer']]","da836627":"train['question_title'] = train['question_title'] + '?'\ntrain['question_body'] = train['question_body'] + '?'\ntrain['answer'] = train['answer'] + '.'\n","d2d19af0":"train['full_question'] = train['question_title'] + \" [SEP] \" + train['question_body']\ntest['full_question'] = test['question_title'] + \" [SEP] \" + test['question_body']","d9dc2d1b":"# count = 0\n# for i in train.answer:\n#   print(count)\n#   print(i)\n#   print(\"-\"*100)\n#   count += 1\n#   if count == 10:\n#     break","88d3f29a":"DEVICE = torch.device(\"cuda\")","4aadaecf":"bert_model_config = '..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased\/bert_config.json'\nbert_config = ppb.BertConfig.from_json_file(bert_model_config)\ntokenizer = ppb.BertTokenizer.from_pretrained('..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased-vocab.txt')\nbert_model = ppb.BertModel.from_pretrained('..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased\/', config=bert_config)\nbert_model.to(DEVICE)","10f3fdca":"#text = 'i love you embedding'\n#print(tokenizer.tokenize(text))\n#print(tokenizer.vocab)","e5dcb968":"# -*- coding: utf-8 -*-\nimport re\nalphabets= \"([A-Za-z])\"\nprefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\nsuffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\nstarters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\nacronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\nwebsites = \"[.](com|net|org|io|gov)\"\n\ndef split_into_sentences(text):\n    text = \" \" + text + \"  \"\n    text = text.replace(\"\\n\",\" \")\n    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n    text = re.sub(websites,\"<prd>\\\\1\",text)\n    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n    if \"\u201d\" in text: text = text.replace(\".\u201d\",\"\u201d.\")\n    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n    text = text.replace(\".\",\".<stop>\")\n    text = text.replace(\"?\",\"?<stop>\")\n    text = text.replace(\"!\",\"!<stop>\")\n    text = text.replace(\"<prd>\",\".\")\n    sentences = text.split(\"<stop>\")\n    sentences = sentences[:-1]\n    sentences = [s.strip() for s in sentences]\n    return sentences\n","ef48a737":"import itertools\nwords = set(nltk.corpus.words.words())\n\ndef remove_non_english(text):\n    return \" \".join(w for w in nltk.wordpunct_tokenize(text) if w.lower() in words or not w.isalpha())\n\n#     doc = spacy_nlp(x) \n#     tokens = [token.text for token in doc]\n#     preprocessed_doc = \" \".join(w for w in tokens if w.lower() in words)\n#     return preprocessed_doc\n\n\ndef add_token_url(text):\n    URL_REGEX = r\"\"\"(?i)\\b((?:https?:(?:\/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b\/?(?!@)))\"\"\"\n    urls = re.findall(URL_REGEX,text)\n    count = 0\n    for url in urls:\n        text = text.replace(url, '<URL>')\n    text = sent_tokenize(text)\n    text = [x for x in text if x not in string.punctuation]\n    result = []\n    text = [x.splitlines() for x in text]\n    text = list(itertools.chain.from_iterable(text))\n    text = list(filter(None, text))\n\n    text = [remove_non_english(x) for x in text]\n    text = [x for x in text if x not in string.punctuation]\n    text = [re.sub(r'[^\\w\\s]','',x) for x in text]\n    text = [re.sub(' +', ' ', x) for x in text]\n    text = [x.strip() for x in text]\n    text = list(filter(None, text))\n\n    return ' [SEP] '.join(text)\n\n","eea0ccf3":"train['preprocessed_full_question'] = [add_token_url(x) for x in train['full_question']]\ntrain['preprocessed_answer'] = [add_token_url(x) for x in train['answer']]\n\ntest['preprocessed_full_question'] = [add_token_url(x) for x in test['full_question']]\ntest['preprocessed_answer'] = [add_token_url(x) for x in test['answer']]\n","9d46053d":"print(len(tokenizer))  # 28997\ntokenizer.add_tokens([\"<URL>\"])\nprint(len(tokenizer))  # 28997\n\nbert_model.resize_token_embeddings(len(tokenizer)) \n","657291ac":"def convert_text_to_vector(df, col, tokenizer, model):\n    df[col] = [x[:512] for x in df[col]]\n    tokenized = df[col].apply(lambda x: tokenizer.encode(x, add_special_tokens = True))\n    max_len= 512 \n    padded = [i + [0]*(max_len - len(i)) for i in tokenized]\n\n    for i in tqdm(range(len(tokenized))):\n        tokenized[i].extend([0]*(max_len - len(tokenized[i])))\n    tokenized = [np.array(x) for x in tokenized]\n    tokenized = np.array(tokenized)\n    attention_mask = np.where(tokenized != 0,1,0)\n    input_ids = torch.tensor(tokenized).to(DEVICE)\n    attention_mask = torch.tensor(attention_mask).to(DEVICE)\n    \n    segments = []\n    for tokens in tqdm(tokenized):\n      segment = []\n      current_segment_id = 0\n      for token in tokens:\n          segment.append(current_segment_id)\n          if token == 102:\n            current_segment_id += 1\n      segment = segment + [current_segment_id+1] * (512 - len(tokens))\n      segments.append(segment)\n    segments = torch.tensor(segments).to(DEVICE)\n    return input_ids, attention_mask, segments\n    \n\n","c6486bdc":"batch_size = 64\n\n","6d638522":"targets = [\n        'question_asker_intent_understanding',\n        'question_body_critical',\n        'question_conversational',\n        'question_expect_short_answer',\n        'question_fact_seeking',\n        'question_has_commonly_accepted_answer',\n        'question_interestingness_others',\n        'question_interestingness_self',\n        'question_multi_intent',\n        'question_not_really_a_question',\n        'question_opinion_seeking',\n        'question_type_choice',\n        'question_type_compare',\n        'question_type_consequence',\n        'question_type_definition',\n        'question_type_entity',\n        'question_type_instructions',\n        'question_type_procedure',\n        'question_type_reason_explanation',\n        'question_type_spelling',\n        'question_well_written',\n        'answer_helpful',\n        'answer_level_of_information',\n        'answer_plausible',\n        'answer_relevance',\n        'answer_satisfaction',\n        'answer_type_instructions',\n        'answer_type_procedure',\n        'answer_type_reason_explanation',\n        'answer_well_written'    \n    ]\n\ny = train[targets].values\n","c8950dd5":"# ----","14487456":"def chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]\n\ndef splitDataFrameIntoSmaller(df, chunkSize = 10000): \n    listOfDf = list()\n    numberChunks = len(df) \/\/ chunkSize + 1\n    for i in range(numberChunks):\n        listOfDf.append(df[i*chunkSize:(i+1)*chunkSize])\n    return listOfDf\n\n","a5492f47":"import sklearn\nle = sklearn.preprocessing.LabelEncoder()\nle.fit(train['category'])\ncategory_features_train = le.fit_transform(train['category'])\ncategory_features_test = le.fit_transform(test['category'])\n","7ef1ba1e":"module_url = \"..\/input\/universalsentenceencoderlarge4\/\"\nembed = hub.load(module_url)\n\ndef encoding_sentence(df, col, batch_size, model):   \n    all_features = []\n    for tokenized_batch in tqdm(splitDataFrameIntoSmaller(df[col].values, chunkSize = batch_size)):\n        all_features.append(model(tokenized_batch)[\"outputs\"].numpy())\n    all_features = np.vstack(all_features)\n    return all_features\n\ndef calculate_text_distance(question_title_features,question_body_features,answer_features):\n\n    dist1 = list(map(lambda x, y: np.linalg.norm(x-y), question_title_features, question_body_features))\n    dist2 = list(map(lambda x, y: np.linalg.norm(x-y), question_body_features,answer_features))\n    dist3 =list(map(lambda x, y: np.linalg.norm(x-y), answer_features,question_title_features))\n    cosdist = np.array([dist1, dist2, dist3])\n    cosdist = cosdist.T\n\n    dist1 = list(map(lambda x, y: spatial.distance.cosine(x,y), question_title_features, question_body_features))\n    dist2 = list(map(lambda x, y: spatial.distance.cosine(x,y), question_body_features,answer_features))\n    dist3 = list(map(lambda x, y: spatial.distance.cosine(x,y), answer_features,question_title_features))\n    l2dist = np.array([dist1, dist2, dist3])\n    l2dist = l2dist.T\n\n    distance = np.hstack([cosdist,l2dist])\n    return distance\n\n\n\n\nquestion_title_encoding = encoding_sentence(train, 'question_title', 32, embed)\nquestion_body_encoding = encoding_sentence(train, 'question_body', 32, embed)\nanswer_encoding  = encoding_sentence(train, 'answer', 32, embed)\n\nquestion_title_encoding_test = encoding_sentence(test, 'question_title', 32, embed)\nquestion_body_encoding_test = encoding_sentence(test, 'question_body', 32, embed)\nanswer_encoding_test  = encoding_sentence(test, 'answer', 32, embed)\n\ntrain_distance = calculate_text_distance(question_title_encoding,question_body_encoding,answer_encoding)\ntest_distance = calculate_text_distance(question_title_encoding_test,question_body_encoding_test,answer_encoding_test)\n\n\n\n\n","76fff324":"def compute_spearmanr(trues, preds):    \n    rhos = []\n    for col_trues, col_pred in zip(trues.T, preds.T):\n        rhos.append(\n            spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n    return np.mean(rhos)\n\n\nclass CustomCallback(tf.keras.callbacks.Callback):\n    \n    def __init__(self, valid_data, test_data, batch_size=16, fold=None):\n\n        self.valid_inputs = valid_data[0]\n        self.valid_outputs = valid_data[1]\n        self.test_inputs = test_data\n        \n        self.batch_size = batch_size\n        self.fold = fold\n        \n    def on_train_begin(self, logs={}):\n        self.valid_predictions = []\n        self.test_predictions = []\n        \n    def on_epoch_end(self, epoch, logs={}):\n        self.valid_predictions = self.model.predict(self.valid_inputs)\n        \n        rho_val = compute_spearmanr(self.valid_outputs, self.valid_predictions)\n        print('\\n Epoch {}, Validation score {}'.format(epoch,rho_val))\n\n        \n        if self.fold is not None:\n            self.model.save_weights(f'bert-base-{fold}-{epoch}.h5py')\n        \n        #self.test_predictions = self.model.predict(self.test_inputs)\n","e9dfeb91":"# BERT_PATH = '..\/input\/bert-base-uncased-huggingface-transformer\/'\n# from transformers import *\n\nclass BertClassification(tf.keras.Model):\n    def __init__(self,flag_distance = False, flag_cat = False,flag_lstm = False, trainable = True):\n        super().__init__(name='BertClassification')\n        self.bert_layer = hub.KerasLayer('..\/input\/bert-base-from-tfhub\/bert_en_uncased_L-12_H-768_A-12', trainable=trainable)\n#         config = BertConfig() # print(config) to see settings\n#         config.output_hidden_states = False # Set to True to obtain hidden states\n#         config.trainable = True\n#         self.bert_layer = TFBertModel.from_pretrained(BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n\n        self.global_avarage = tf.keras.layers.GlobalAveragePooling1D()\n        self.dense_out = tf.keras.layers.Dense(30, activation=\"sigmoid\", name=\"dense_output\")\n        self.embed =  tf.keras.layers.Embedding(500, 64, input_length=1)\n        self.dropout = tf.keras.layers.Dropout(0.25)\n        self.flag_distance = flag_distance\n        self.flag_cat = flag_cat\n        self.flag_lstm = flag_lstm\n\n\n    def call(self, inputs):\n        max_len = 512\n        inputs = [tf.cast(x, tf.int32) for x in inputs]\n\n        input_word_ids_title, input_masks_title, input_segments_title = inputs[0],inputs[1],inputs[2]\n        input_word_ids_answer, input_masks_answer, input_segments_answer =  inputs[3],inputs[4],inputs[5]     \n\n        features_cat = inputs[6]\n        distance_features = tf.cast(inputs[7], tf.float32)  \n\n        _, sequence_output_title = self.bert_layer([input_word_ids_title, input_masks_title, input_segments_title])\n        global_title = self.global_avarage(sequence_output_title)\n\n\n        _, sequence_output_answer = self.bert_layer([input_word_ids_answer, input_masks_answer, input_segments_answer])\n        global_answer = self.global_avarage(sequence_output_answer)\n\n\n        embedding_cat = self.embed(features_cat)\n        embedding_cat = self.global_avarage(embedding_cat)\n        embedding_cat = self.dropout(embedding_cat)\n        distance_features = self.dropout(distance_features)\n\n        concat = tf.keras.layers.concatenate([global_title,\n                                              global_answer,\n                                              embedding_cat,\n                                              distance_features])\n\n        concat = self.dropout(concat)\n        out = self.dense_out(concat)\n        return out\n\n# model = BertClassification()","f452d020":"# def training(X_train,y_train,X_val,y_val,X_test):  \n#   batch_size  =  2\n#   custom_callback = CustomCallback(valid_data=(X_val,y_val),test_data=X_test, batch_size=batch_size)\n#   learning_rate = 3e-5\n#   epochs = 20\n#   loss_function = 'binary_crossentropy'\n#   optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n#   model = BertClassification()\n#   model.compile(loss=loss_function, optimizer=optimizer)\n#   model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,callbacks=[custom_callback])\n#   return model\n\n\n# def training_2(X_train,y_train,X_val,y_val,X_test):  \n#   batch_size  =  2\n#   custom_callback = CustomCallback(valid_data=(X_val,y_val),test_data=X_test, batch_size=batch_size)\n#   learning_rate = 3e-5\n#   epochs = 3\n#   loss_function = 'binary_crossentropy'\n#   optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n#   model = question_answer_model()\n#   model.compile(loss=loss_function, optimizer=optimizer)\n#   model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,callbacks=[custom_callback])\n#   return model  ","89ef04c8":"#inputs_ids, attention_masks, segments = convert_text_to_vector(train, 'full_text', tokenizer, bert_model)\n#inputs_ids_test, attention_masks_test, segments_test = convert_text_to_vector(test, 'full_text', tokenizer, bert_model)\n\n\ninputs_ids_title, attention_masks_title, segments_title = convert_text_to_vector(train, 'preprocessed_full_question', tokenizer, bert_model)\ninputs_ids_test_title, attention_masks_test_title, segments_test_title = convert_text_to_vector(test, 'preprocessed_full_question', tokenizer, bert_model)\n\ninputs_ids_answer, attention_masks_answer, segments_answer = convert_text_to_vector(train, 'preprocessed_answer', tokenizer, bert_model)\ninputs_ids_test_answer, attention_masks_test_answer, segments_test_answer = convert_text_to_vector(test, 'preprocessed_answer', tokenizer, bert_model)\n\n\n\nX = [inputs_ids_title.cpu().data.numpy(), \n     attention_masks_title.cpu().data.numpy(), \n     segments_title.cpu().data.numpy(),\n     inputs_ids_answer.cpu().data.numpy(),\n     attention_masks_answer.cpu().data.numpy(),\n     segments_answer.cpu().data.numpy(),\n     category_features_train,\n     train_distance\n     ]\n\nX_test = [inputs_ids_test_title.cpu().data.numpy(), \n     attention_masks_test_title.cpu().data.numpy(), \n     segments_test_title.cpu().data.numpy(),\n     inputs_ids_test_answer.cpu().data.numpy(),\n     attention_masks_test_answer.cpu().data.numpy(),\n     segments_test_answer.cpu().data.numpy(),\n     category_features_test,\n     test_distance\n     ]\n\n\n","5319d50c":"import timeit\n\nbatch_size  = 4\nlearning_rate = 3e-5\nepochs = 3\nloss_function = 'binary_crossentropy'\n\ngkf = GroupKFold(n_splits=5).split(X=train.category, groups=train.category)\n\nvalid_preds = []\ntest_preds = []\nvalidation_score = []\n\n\nfor fold, (train_idx, valid_idx) in tqdm(enumerate(gkf)):\n    if fold in [1, 2]:\n        print(\"Fold {}\".format(fold))\n\n        start = timeit.default_timer()\n\n        X_train = [X[i][train_idx] for i in range(len(X))]\n        y_train = y[train_idx]\n        X_val = [X[i][valid_idx] for i in range(len(X))]\n        y_val = y[valid_idx]   \n        K.clear_session()\n        \n        custom_callback = CustomCallback(valid_data=(X_val,y_val),test_data=X_test, batch_size=batch_size)\n        \n        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n        \n        model = BertClassification()\n        model.compile(loss=loss_function, optimizer=optimizer)\n\n        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n        \n        test_prediction = model.predict(X_test)\n        valid_preds.append(model.predict(X_val))\n        test_preds.append(test_prediction)       \n        rho_val = compute_spearmanr(y_val, valid_preds[-1])\n        validation_score.append(rho_val)\n        \n        #print(\"Spearman score {}\".format(rho_val))\n        \n        stop = timeit.default_timer()\n        training_time = stop  -  start  \n        \n        print(\"Training time {}\".format(training_time))\n        \n        \n        del model\n        del X_train\n        del y_train\n        del X_val\n        del y_val \n\nprint(\"Validation score {}\".format(np.mean(validation_score)))\n\n","ff3cc711":"test_preds[0]","75e772f9":"test_preds[1].shape","12be46d4":"np.mean(test_preds, axis=0)","f53b1b5e":"\"\"\"\n\n\nfrom sklearn.model_selection import train_test_split\nX_train,y_train, X_val, y_val = train_test_split(X,y,random_state = 1,test_size = 0.25)\nprint(\"Validation score {}\".format(compute_spearmanr(y_val, model.predict(X_val)))\n\"\"\"\n# test_preds[0]","ec6064f3":"# batch_size  = 2\n# learning_rate = 3e-5\n# epochs = 2\n# loss_function = 'binary_crossentropy'\n\n# X_val = [X[i][:500] for i in range(len(X))]\n# y_val = y[:500]   \n        \n\n# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n# model = BertClassification()\n# model.compile(loss=loss_function, optimizer=optimizer)\n\n# # custom_callback = CustomCallback(valid_data=(X_val,y_val),test_data=X_test, batch_size=batch_size)\n# model.fit(X, y, epochs=epochs, batch_size=batch_size)\n    \n","5fd15344":"train['category'].values","b19a52ad":"from sklearn.preprocessing import OneHotEncoder\nonehotencoder = OneHotEncoder()\nonehotencoder.fit(category_features_train.reshape(-1,1))\n    \ncategory_onehot_train = onehotencoder.transform(category_features_train.reshape(-1,1)).toarray()\ncategory_onehot_test = onehotencoder.transform(category_features_test.reshape(-1,1)).toarray()","1d5a27fe":"category_onehot_train.shape","d93c9306":"train_distance.shape","95549fb3":"def sigmoid(X):\n    return 1\/(1+np.exp(-X))","cb5d3b07":"X = np.hstack([\n     category_onehot_train,\n     train_distance\n     ])\n\nX_test = np.hstack([\n     category_onehot_test,\n     test_distance\n     ])\n\nelastic_model = MultiTaskElasticNet(alpha=0.001, random_state=42, l1_ratio=0.5)\nelastic_model.fit(X, y)\n","56016331":"elastic_prediction = sigmoid(elastic_model.predict(X_test))","0b52bae1":"elastic_prediction","ce7e8a5b":"test_preds.append(elastic_prediction)","2f9ca897":"test_preds = np.mean(test_preds, axis=0)","50ede0a0":"test_preds","6424723d":"submission = pd.DataFrame(columns = list(ss.columns))\nsubmission['qa_id'] = test['qa_id']\nsubmission[targets] = test_preds\nsubmission.to_csv(\"submission.csv\", index = False)","414b1721":"submission","e86d0a9a":"# https:\/\/mccormickml.com\/2019\/05\/14\/BERT-word-embeddings-tutorial\/","4fb81ece":"### Categpry features","5ecd16ee":"## Features Engineering","255a4815":"## Elastic Model","5b486552":"## Import library","e0853913":"https:\/\/mccormickml.com\/2019\/05\/14\/BERT-word-embeddings-tutorial\/ \\\nhttps:\/\/medium.com\/miq-tech-and-analytics\/how-to-detect-non-english-language-words-and-remove-them-from-your-keyword-insights-599b91916071 \\\nhttps:\/\/trituenhantao.io\/lap-trinh\/huong-dan-fine-tuning-bert-voi-pytorch\/","803721d9":"### Concat sentences","27649ab8":"## Convert to Bert inputs","409f60dd":"## Read data","3a22c698":"## Preprocessing","7f50eb03":"### Universal Google Encoding","67fb5185":"## Exploratory","90737046":"## Build model ","2a1b675a":"## Load Bert model"}}