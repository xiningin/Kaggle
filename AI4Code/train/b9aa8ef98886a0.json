{"cell_type":{"b106ac36":"code","1396787b":"code","4f5f3a53":"code","8e5327a2":"code","99feb37e":"code","f56f8479":"code","bf2656aa":"code","62bcbf41":"code","7ae706af":"code","d06bb498":"code","e6e037d2":"code","3b8865a9":"code","84ba2528":"code","2a37b47e":"code","6900281c":"markdown","4311a209":"markdown","f2862c8c":"markdown","33f186e7":"markdown","e620b71e":"markdown","345d738a":"markdown","9dd45077":"markdown","d909cfd2":"markdown","2950747f":"markdown","dcc9d888":"markdown","999c9169":"markdown"},"source":{"b106ac36":"import numpy as np\nimport pandas as pd\nimport random\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.manifold import TSNE\nfrom sklearn.pipeline import FeatureUnion\nimport string\n\nimport IPython.display as ipd\nimport librosa.display\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport pandas as pd","1396787b":"items = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\nitems_english = pd.read_csv(\"..\/input\/predict-future-sales-supplementary\/item_category.csv\")\n\nitems = list(items[\"item_name\"])\nitems_english = list(items_english[\"item_name_translated\"])","4f5f3a53":"_ = [print(items_english[i]) for i in range(10)] ","8e5327a2":"items_english = [''.join(sign for sign in item if sign not in string.punctuation) for item in items_english]\n_ = [print(items_english[i]) for i in range(10)] ","99feb37e":"np.random.seed(123)\nind = np.arange(len(items_english))\nnp.random.shuffle(ind)\nind = ind[:1000]\nitems_english = np.array(items_english)[ind]","f56f8479":"tfidf_word_vectorizer = TfidfVectorizer(max_features=None, analyzer='word', ngram_range=(1, 1))\ntfidf_word_features = tfidf_word_vectorizer.fit_transform(items_english)\nlen(tfidf_word_vectorizer.get_feature_names())","bf2656aa":"tfidf_char_vectorizer = TfidfVectorizer(max_features=None, analyzer='char', ngram_range=(3, 3))\ntfidf_char_features = tfidf_char_vectorizer.fit_transform(items_english)","62bcbf41":"len(tfidf_char_vectorizer.get_feature_names())","7ae706af":"projector = TSNE(n_components=2, perplexity=5, random_state=42)\ntfidf_projection = projector.fit_transform(tfidf_char_features.toarray())","d06bb498":"# Create a trace\ntrace = go.Scatter(\n    text=items_english,\n    x=tfidf_projection[:, 0],\n    y=tfidf_projection[:, 1],\n    mode='markers',\n)\nlayout = go.Layout(title=\"Char tf-idf projection of items names\", hovermode='closest')\nfigure = go.Figure(data=[trace], layout=layout)\npy.iplot(figure, filename='projection.html')\n","e6e037d2":"fasttext_features = pd.read_csv(\"..\/input\/predictfuturesalesitemnametranslatedfasttext\/fasttext_features.csv\")\nfasttext_features.drop('item_id', axis=1, inplace=True)\nfasttext_features.drop('Unnamed: 0', axis=1, inplace=True)\nfasttext_features = fasttext_features.values","3b8865a9":"projector = TSNE(n_components=2, perplexity=60.0, random_state=42)\nfasttext_projection = projector.fit_transform(fasttext_features)","84ba2528":"fasttext_projection = fasttext_projection[ind]","2a37b47e":"# Create a trace\ntrace = go.Scatter(\n    text=items_english,\n    x=fasttext_projection[:, 0],\n    y=fasttext_projection[:, 1],\n    mode='markers',\n)\nlayout = go.Layout(title=\"FastText projection of items names\", hovermode='closest')\nfigure = go.Figure(data=[trace], layout=layout)\npy.iplot(figure, filename='projection.html')\n","6900281c":"### Word-embeddings\n\nNext possible step is to use word-embeddings. This is, however, non-trivial because we don't have a constant length of items names, it's hard to get a good representation of domain words, it requires a lot of memory, and, in my opinion, the semantics of the items names can be covered by the categories. But we have to check it!\n\nI used fastText library to load official Facebook model trained on Wikipedia. Then, I used get_sentence_vector() to infer vector with constant length for each item name, to get rid of a problem with variable sentence length. The result of this operation is uploaded [here](https:\/\/www.kaggle.com\/davids1992\/predictfuturesalesitemnametranslatedfasttext)\n\nLoading:","4311a209":"Sentence embeddings don't look good at all. I checked the embeddings generation and shuffling a few times and I can't see any bug, so It's hard to say why no patterns can be seen. One guess is that I lowercased everything, but \"Pink Floyd\" can have much different representation than \"pink floyd\" in FastText. Also, the variable length and Russian characters can disturb models.\n\nWill it work with the regressors? It's always worth checking.\n\n---\nIf you find my kernel interesting, please don't forget to upvote :)","f2862c8c":"To make kernel feasible, I'm gonna use 1000 examples","33f186e7":"Let's clean the text a little bit","e620b71e":"### Problem - sparse matrix\n\nI can see that people tend to represent text features using Bag-of-words i.e. Tf-idf transform. Because there's a lot of words in item's names, the resulting matrix is very sparse and has thousands of dimensions. Choosing only N most important features is some solution, although obviously the rest of the words also can be crucial.\n\nIn this kernel, I'd like to propose a few methods of representing text features in a more compact way. I will focus on items names, but it can be applied cities and categories as well.\n\nLet's start with the imports:","345d738a":"The amount of features is huge!\n\nIt's not a very efficient way to represent the text. We could use CSR matrix to store it better, but handling mixed types of matrices or two matrices is brittle.\n\nWe can, however, compress the data and represent it in continuous space, based on their syntax (not semantics!). Let's try to do that","9dd45077":"Thanks to the **deargle** and **K\u00e2z\u0131m An\u0131l Eren** we can read the translated items names directly to the notebook from Kaggle database. Let's read it together with original names.\n","d909cfd2":"### Starting with bag-of-words\n\nOne of the easiest ways to represent the text is to convert it to Bag-of-words.\n\nBag-of-words is a very sparse vector with elements number equal to vocabulary size and a few 1's in the places. As a feature, we can use character or words, and their n-grams. ","2950747f":"We can see that the data is now represented in an interesting way. For example, many PINK FLOYD albums are grouped together, and there are clear clusters for audiobooks, board games, XBOX etc.\n\nProbably the representation is much better than using only top n features from tf-idf vectorizer, because it consists all the words in it. In two dimensions we can show some words are more similar syntactically than other. We can suspect, that this similarity has some semantical meaning and that the obtained features will correlate with features derived for the competition well.\n","dcc9d888":"We can visualize this features in the same space","999c9169":"There are 15258 words in the items names. Which are the most informative? Is it enough to choose first N to improve our models?\n\nBecause the text is still messy and the Russian language has morphology, we can use characters in some range (characters n-grams) instead of words, to catch all the important features."}}