{"cell_type":{"865a409d":"code","62fb0e61":"code","4a9f05ab":"code","cf30acd9":"code","1732a0ac":"code","6ac0be4d":"code","85e421a5":"code","cc20e271":"code","2a2cf512":"code","02434e9b":"code","cb9ec5f8":"code","d741aa52":"code","14641242":"code","bcdad21f":"code","d5bcaeae":"code","464edfdf":"code","33a9df2e":"code","7cbd28a8":"code","bb08f904":"code","a843ae2e":"code","e4297dee":"code","ca4d7cbc":"code","9bb6886f":"code","a1e85ff0":"code","58dc2948":"code","490a6945":"code","35b008ff":"code","7387fac2":"code","787b8d44":"code","483d180c":"code","b409649b":"code","a36c1c00":"code","4155ffa6":"markdown","9314694a":"markdown","e981df33":"markdown","95f7e879":"markdown","53fdbc28":"markdown","147c30b7":"markdown","72243eab":"markdown","493c5cfe":"markdown","d8804c06":"markdown","3de50e51":"markdown","8922b334":"markdown","070b3775":"markdown","9e80b908":"markdown","58f0dea7":"markdown","b5614975":"markdown","9da07e10":"markdown","0544f13c":"markdown","d1316396":"markdown","01cd213b":"markdown","f73d2041":"markdown","1620acba":"markdown","ce7004a2":"markdown","133324a0":"markdown","4123dedf":"markdown"},"source":{"865a409d":"# Reading the input directory files\nimport os\nprint(os.listdir(\"..\/input\/\"))\n# Importing Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport seaborn as sns\nimport math\n%matplotlib inline","62fb0e61":"# Reading the Bank Customers file using pandas function read.csv()\ncustomers_data = pd.read_csv('..\/input\/Churn_Modelling.csv')","4a9f05ab":"# Displaying the top rows of the dataset for a quick visualization of the data\nprint(customers_data.head())","cf30acd9":"# running a script on customers data  file: customers_data.describe() to run the descriptive statistics on the data\n#in order to screen outliers and potential bad data.\n\ncustomers_data.describe(include=\"all\")","1732a0ac":"# analyzing the data, to know the number of rows and columns and see if there are any missing data\ncustomers_data.shape\nprint(\" The number of null values is: \" , customers_data.isnull().values.sum())\nprint(customers_data.isnull().sum())","6ac0be4d":"# Running customers_data.info () command to check if there are no missing values in any of the fields or NaN \n# and if all columns types were consistent with the data they contains. All were complete and consistent.\ncustomers_data.info () \n","85e421a5":"#Creating helper functions to see visualy the distributon of the the different predictor variables\n\ndef visual_exploratory(x):\n    \n    for var in x.select_dtypes(include = [np.number]).columns :\n        print( var + ' : ')\n        x[var].plot('hist')\n        plt.show()\n        \nvisual_exploratory(customers_data)\n\n# ploting the box plot to visually inspect numeric data\n\ndef boxPlot_exploratory(x):\n    \n    for var in x.select_dtypes(include = [np.number]).columns :\n        print( var + ' : ')\n        x.boxplot(column = var)\n        plt.show()\n        \nboxPlot_exploratory(customers_data)","cc20e271":"#Creating a variable of Categorical features\n\ncat_df_customers = customers_data.select_dtypes(include = ['object']).copy()\nprint(cat_df_customers.head()) \nprint(\" The number of null values is: \" , cat_df_customers.isnull().values.sum())","2a2cf512":"#Plotting categorical features\n\n## 1. Plot for Geographical location\n\nlocation_count = cat_df_customers['Geography'].value_counts()\nsns.barplot(location_count.index, location_count.values)\nplt.title('Geographical location Distribution of Bank Customers')\nplt.ylabel('Frequency', fontsize=11)\nplt.xlabel('Geography', fontsize=11)\nplt.show()\n\n\n## 2. Plot for Gender \n\nlocation_count = cat_df_customers['Gender'].value_counts()\nsns.barplot(location_count.index, location_count.values)\nplt.title('Gender Distribution of Bank Customers')\nplt.ylabel('Frequency', fontsize=11)\nplt.xlabel('Gender', fontsize=11)\nplt.show()","02434e9b":"#gradient boosting decision tree algorithm\nimport xgboost as xgb\nimport sklearn as skt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder","cb9ec5f8":"new_customers_data=customers_data.copy()","d741aa52":"# encode string class values as integers\n\nGender = new_customers_data['Gender']\nlabel_encoder = LabelEncoder()\nlabel_encoder = label_encoder.fit(Gender)\nlabel_encoded =label_encoder.transform(Gender)\nnew_customers_data['Gender']=label_encoded","14641242":"#print(new_customers_data.head())\n#Gend = new_customers_data['Gender']\n#print(Gend)","bcdad21f":"temp_customers_data=new_customers_data.copy()\ntemp_customers_data = pd.get_dummies(temp_customers_data, columns=['Geography'], prefix = ['Geography'])\nprint(temp_customers_data.head())","d5bcaeae":"# Appending the new column to the new_customers_data dataframe\n\nnew_customers_data.insert(13, 'Geography_France' , temp_customers_data['Geography_France'])\nnew_customers_data.insert(14, 'Geography_Germany' , temp_customers_data['Geography_Germany'])\nnew_customers_data.insert(15, 'Geography_Spain' , temp_customers_data['Geography_Spain'])\nprint(new_customers_data.head())","464edfdf":"# Helper function that will create and add a new column tof credit score range the data frame\ndef creditscore(data):\n    score = data.CreditScore\n    score_range =[]\n    for i in range(len(score)) : \n        if (score[i] < 600) :  \n            score_range.append(1) # 'Very Bad Credit'\n        elif ( 600 <= score[i] < 650) :  \n            score_range.append(2) # 'Bad Credit'\n        elif ( 650 <= score[i] < 700) :  \n            score_range.append(3) # 'Good Credit'\n        elif ( 700 <= score[i] < 750) :  \n            score_range.append(4) # 'Very Good Credit'\n        elif score[i] >= 750 : \n            score_range.append(5) # 'Excellent Credit'\n    return score_range\n\n# converting the returned list into a dataframe\nCreditScore_category = pd.DataFrame({'CreditScore_range': creditscore(new_customers_data)})\n\n# Appending the new column to the new_customers_data dataframe\nnew_customers_data.insert(16, 'CreditScore_range' , CreditScore_category['CreditScore_range'])","33a9df2e":"# Helper function that will create and add a new column of age group to the data frame\ndef agegroup(data):\n    age = data.Age\n    age_range =[]\n    for i in range(len(age)) : \n        if (age[i] < 30) :  \n            age_range.append(1) # 'Between 18 and 30 year'   \n        elif ( 30 <= age[i] < 40) :  \n            age_range.append(2) # 'Between 30 and 40 year'\n        elif ( 40 <= age[i] < 50) :  \n            age_range.append(3) # 'Between 40 and 50 year'\n        elif ( 50 <= age[i] < 60) :  \n            age_range.append(4) # ''Between 50 and 60 year'\n        elif ( 60 <= age[i] < 70) :  \n            age_range.append(5) # 'Between 60 and 70 year'\n        elif ( 70 <= age[i] < 80) :  \n            age_range.append(6) # 'Between 70 and 80 year'\n        elif age[i] >= 80 : \n            age_range.append(7) # ''Above 80 year'\n    return age_range\n\n# converting the returned list into a dataframe\nAgeGroup_category = pd.DataFrame({'age_group': agegroup(new_customers_data)})\n\n# Appending the new column to the new_customers_data dataframe\nnew_customers_data.insert(17, 'age_group' , AgeGroup_category['age_group'])","7cbd28a8":"print(new_customers_data.head())","bb08f904":"new_customers_data_xgboost=new_customers_data.copy()\nTarget = 'Exited'\nSurname = 'Surname'\nGeography = 'Geography'\n#Gender= 'Gender'\nID= 'RowNumber'\nCustomerId = 'CustomerId'\n#Choose all predictors except Target, Surname, Geography, CustomerId & ID and also separate the response variable\nX = [x for x in new_customers_data_xgboost.columns if x not in [Surname,Geography, Target, ID, CustomerId]]\nY = new_customers_data_xgboost.iloc[:,-1]\n","a843ae2e":"predictors = new_customers_data_xgboost[X] #predictor variable\nresponse = Y # response variable\nprint(predictors.head())\nprint(response.head())","e4297dee":"# split data into train and test sets\nseed = 7\ntest_size = 0.33\nX_train, X_test, y_train, y_test = train_test_split(predictors, response, test_size=test_size,\nrandom_state=seed)\n# fit model on training data\n\n## xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n##                max_depth = 5, alpha = 10, n_estimators = 10)\n\n#model = xgb.XGBClassifier(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,\n                #max_depth = 5, alpha = 10, n_estimators = 10)\nmodel = xgb.XGBClassifier()\nmodel.fit(X_train, y_train)\n# make predictions for test data\npredictions = model.predict(X_test)\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\n\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","ca4d7cbc":"# plotting important features for a quick idea of which contribute to the model perfromance better\nimport matplotlib.pyplot as plt\nparams = {\"objective\":\"binary:logistic\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n                'max_depth': 5, 'alpha': 10}\nxgb.plot_importance(model)\nplt.rcParams['figure.figsize'] = [5, 5]\nplt.show()","9bb6886f":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold # to nforce the same distribution of classes in each fold\nfrom sklearn.model_selection import cross_val_score","a1e85ff0":"# testing the cross validated model\nmodel2 = xgb.XGBClassifier()\nkfold = StratifiedKFold(n_splits=10, random_state=7)\nresults = cross_val_score(model2, predictors, response, cv=kfold)\nprint(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))","58dc2948":"print(results) # These results represent the accuracy at each fold in the cross validated model","490a6945":"print(model.feature_importances_) # the inbuild method from the model  display the importance score according to the input order of the predictors","35b008ff":"# plot feature importance\nfrom matplotlib import pyplot\nxgb.plot_importance(model)\npyplot.show()","7387fac2":"print(np.sort(model.feature_importances_)) # Sorting them according to the importance order of the features","787b8d44":"from sklearn.feature_selection import SelectFromModel\nthresholds = np.sort(model.feature_importances_)\nfor thresh in thresholds:\n    selection = SelectFromModel(model, threshold=thresh, prefit=True)\n    select_X_train = selection.transform(X_train)\n    # train model\n    selection_model = xgb.XGBClassifier()\n    selection_model.fit(select_X_train, y_train)\n    # eval model\n    select_X_test = selection.transform(X_test)\n    y_pred = selection_model.predict(select_X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1],accuracy*100.0))","483d180c":"from sklearn.feature_selection import SelectFromModel\n# select features using threshold\nthresh= 0.04582651\n\nselection = SelectFromModel(model, threshold=thresh, prefit=True)\nselect_X_train = selection.transform(X_train)\n# train model\nselection_model = xgb.XGBClassifier()\nselection_model.fit(select_X_train, y_train)\n# eval model\nselect_X_test = selection.transform(X_test)\ny_pred = selection_model.predict(select_X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1],accuracy*100.0))","b409649b":"pred = pd.DataFrame(y_pred)\nprint (pred.head())\nwith open('churns_predict.csv', 'w') as f:\n    print( pred, file=f) ","a36c1c00":"predictions = [round(value) for value in y_pred]\nprint(predictions)\npreds = pd.DataFrame(predictions)\nprint(preds)","4155ffa6":"## 1. Exploratory Analysis","9314694a":"This is to avoid including redundant features in our training dataset as they do not  contribute to the improvemenet of the model","e981df33":"#### 2.5.2 plotting important features identified by the model","95f7e879":"Here one-hot encoding  convert each geographic name  into a new column and assign it a 1 or 0 and for this we will use \n.get_dummies(), a pandas method.This will make our model not interprete the values as weight since we will have only 1 and 0 instead of 0,1 and 2 for the case of label encoder","53fdbc28":"### 2.3 Creating new transformed features and adding them to the dataset","147c30b7":"# Bank Customer Churn Prediction","72243eab":"#### From the above analysis from .info() we identify 3 columns with object dtype, in which two Geography and Gender are categorical features, the 3rd one surname is just a string data but not categorical\n#### And all data are complete, there are no missing values, as we have in all columns the total number of rows which is 10000","493c5cfe":"For training and testing the performance of our XGBoost model, we will base on the principle of using 67% of the data as training dataset and 33% as testing dataset.","d8804c06":"### 2.1. Label Encode string values in the dataset","3de50e51":"#### 2.5.1 Inspecting the model parameters","8922b334":"In this phase 1, we will explore data, to have an understanding of its format, content and see if there is need to clean them before using them in our model prediction","070b3775":"This split the data into folds. The algorithm is trained on k \u2212 1 folds with one held back and tested on the held back fold.\nThis is repeated so that each fold of the dataset is given a chance to be the held back test set\n\nit is more accurate because the algorithm is trained and evaluated multiple times on different data","9e80b908":"### 2.2 Transform Geography string values with one-Hot Encoding","58f0dea7":"Since XGBoost models takes only numeric values as input as it considers problems as regression modelling problem we will transform all string features values of gender into numerical value. Here we use label encoder as we have only two choices for gender which is ok as it will not create any wrong intrepretation of weighting.","b5614975":"#### Plotting the distribution of the above categorical features","9da07e10":"### 2.7 Re-run the model and prediction with the optimal threshold of 0.04582651, taking only 7 predictors","0544f13c":"We decided to use XGBoost as it s a strong model which tries to create a strong learner from an ensemble of weak learners (models)\nhence from the ensemble of weak models it learns from their error and combine all together to build a combination of them and keep only the parts where they performed well\nit has the advantages of combining different models into one and apply regularization, Penalisation of trees, performance, speed all in one model:hence has an inbuilt optimization\nit reduce the collinearity amongs features for a better performing model.","d1316396":"## 2. Building classification Model with Extreme Gradient Boosting(XGBoost) algorithm","01cd213b":"### 2.6 Training the model with k-fold cross validation technique","f73d2041":"### 2.4 Training and Building the XGBoost model","1620acba":"In this exercice, we are going to build and train a model that predict which customers\nmay churn in future so that they can take steps to incentivise those customers to stay. \nWe will classify the predictions of those customers in either exited or stayed in binary classification (0 and 1)\n","ce7004a2":"#### 2.6.1 Selecting features based on  their respective feature importance scores","133324a0":"From the results, we can see that there are no missing data ","4123dedf":"### 2.5 Training the model with train and test technique"}}