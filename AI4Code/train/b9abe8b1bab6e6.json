{"cell_type":{"b9d2e166":"code","7cbbc6ce":"code","54985bf0":"code","6796e2ee":"code","c225734d":"code","73bc4acb":"code","5d0b506f":"code","1541478c":"code","25a2cb01":"code","80110451":"code","be4e1ffd":"code","77bab36b":"code","a9503a53":"code","c3e6737e":"code","2d444ec2":"code","24101f62":"code","ce640105":"code","fc637cd9":"code","9592dd2c":"code","69c5eb7d":"code","3ed482a2":"code","c0552167":"code","ca558a9e":"code","4e53017e":"code","ac07c157":"code","61f7d757":"code","2324b6f7":"code","6096900a":"code","00f81642":"code","ec15bf43":"code","50937aa7":"code","31217b18":"code","cc2b96b6":"code","d17c02d5":"code","d0348a38":"code","1aa7966a":"code","99afb813":"markdown","3850f850":"markdown","eb03910b":"markdown","e5845440":"markdown","b234c3a3":"markdown","085b9425":"markdown","094711a6":"markdown","38fdc8f3":"markdown","79b2c445":"markdown","46236aec":"markdown"},"source":{"b9d2e166":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm","7cbbc6ce":"data = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')","54985bf0":"data.head(1)","6796e2ee":"print(f'data length: {len(data)}\\n')\nprint(f'missing values by column:\\n{data.isnull().sum()}\\n')\nprint(f'data duplication(all columns): {data.duplicated().sum()}')","c225734d":"## visualization function\n\ndef value_counts_plot_bar(df, column, fontsize=15):\n    label_counts = df[column].value_counts()\n    \n    plt.figure(figsize=(15,10))\n    plt.style.use('seaborn')\n    bar_colors = cm.rainbow(np.linspace(0,1,len(df[column].unique())))\n    plt.bar(label_counts.index.values,\n            label_counts.values,\n            color=bar_colors,\n            linewidth=0,\n            alpha=0.6)\n    plt.tick_params(labelsize=fontsize)\n    plt.xlabel(column, fontsize=fontsize)\n    plt.ylabel(f'{column} value counts', fontsize=fontsize)\n    plt.show()\n    \n    \ndef value_counts_plot_line(df, column, fontsize=15):\n    plt.figure(figsize=(15,10))\n    plt.style.use('seaborn')\n    df[column].value_counts().sort_index().plot(\n        color='orangered', \n        linewidth=4\n    )\n    plt.tick_params(labelsize=fontsize)\n    plt.xlabel(column, fontsize=fontsize)\n    plt.ylabel(f'{column} value counts', fontsize=fontsize)\n    plt.show()\n    \n    \ndef plot_bar_by_gender(df, column, fontsize=15, bar_width=0.6):\n    plt.figure(figsize=(15,10))\n    plt.style.use('seaborn')\n    bar_colors = cm.rainbow(np.linspace(0,1,len(df['Sex'].unique())))\n\n    for bar_color, (key, df_by_key) in zip(bar_colors, df.groupby('Sex')):\n        df_index = df_by_key[column].value_counts().index\n        df_values = df_by_key[column].value_counts().values\n        set_align = 'edge' if key is 'M' else 'center'\n        plt.bar(\n            df_index, \n            df_values,\n            width=bar_width,\n            linewidth=0, \n            color=bar_color, \n            label=key, \n            alpha=0.6,\n            align=set_align\n        )\n        plt.legend(title='Sex', fontsize=fontsize)\n        plt.tick_params(labelsize=fontsize)\n        plt.xticks(df[column].unique())\n        plt.xlabel(column, fontsize=fontsize)\n        plt.ylabel(f'{column} value counts', fontsize=fontsize)\n    plt.show()\n    \n    \ndef plot_line_by_gender(df, column, fontsize=15):\n    plt.figure(figsize=(15,10))\n    plt.style.use('seaborn')\n    line_colors = cm.rainbow(np.linspace(0,1,len(df['Sex'].unique())))\n    \n    for line_color, (key, df_by_key) in zip(line_colors, df.groupby('Sex')):\n        \n        df_by_key[column].value_counts().sort_index().plot(\n            color=line_color,\n            label=key,\n            linewidth=4\n        )\n        plt.legend(title='Sex', fontsize=fontsize)\n        plt.tick_params(labelsize=fontsize)\n        plt.xlabel(column, fontsize=fontsize)\n        plt.ylabel(f'{column} value counts', fontsize=fontsize)\n    plt.show()","73bc4acb":"value_counts_plot_line(data, 'Age')","5d0b506f":"value_counts_plot_bar(data, 'Sex')","1541478c":"value_counts_plot_bar(data, 'ChestPainType')","25a2cb01":"value_counts_plot_line(data, 'RestingBP')","80110451":"value_counts_plot_line(data, 'Cholesterol')","be4e1ffd":"value_counts_plot_bar(data, 'FastingBS')","77bab36b":"value_counts_plot_bar(data, 'RestingECG')","a9503a53":"value_counts_plot_bar(data, 'HeartDisease')","c3e6737e":"plot_line_by_gender(data, 'Age')","2d444ec2":"plot_bar_by_gender(data, 'ChestPainType')","24101f62":"plot_line_by_gender(data, 'RestingBP')","ce640105":"plot_line_by_gender(data, 'Cholesterol')","fc637cd9":"plot_bar_by_gender(data, 'FastingBS')","9592dd2c":"plot_bar_by_gender(data, 'RestingECG')","69c5eb7d":"plot_bar_by_gender(data, 'HeartDisease')","3ed482a2":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\ndef categorical_encoder(df):\n    new_df = df.copy()\n    le = LabelEncoder()\n    new_df['Sex'] = le.fit_transform(new_df['Sex'].values)\n    new_df['ChestPainType'] = le.fit_transform(new_df['ChestPainType'].values)\n    new_df['RestingECG'] = le.fit_transform(new_df['RestingECG'].values)\n    new_df['ExerciseAngina'] = le.fit_transform(new_df['ExerciseAngina'].values)\n    new_df['ST_Slope'] = le.fit_transform(new_df['ST_Slope'].values)\n    return new_df\n\ndef feature_scaler(df):\n    new_df = df.copy()\n    sc = StandardScaler()\n    # standardize on including some category columns as well\n    new_df['Sex'] = sc.fit_transform(new_df['Sex'].values.reshape(-1, 1))\n    new_df['RestingBP'] = sc.fit_transform(new_df['RestingBP'].values.reshape(-1, 1))\n    new_df['Cholesterol'] = sc.fit_transform(new_df['Cholesterol'].values.reshape(-1, 1))\n    new_df['FastingBS'] = sc.fit_transform(new_df['FastingBS'].values.reshape(-1, 1))\n    new_df['RestingECG'] = sc.fit_transform(new_df['RestingECG'].values.reshape(-1, 1))\n    new_df['MaxHR'] = sc.fit_transform(new_df['MaxHR'].values.reshape(-1, 1))\n    new_df['Oldpeak'] = sc.fit_transform(new_df['Oldpeak'].values.reshape(-1, 1))  \n    return new_df\n\n\ndata = categorical_encoder(data) \ndata = feature_scaler(data)","c0552167":"data.head()","ca558a9e":"def feature_engineering(df):\n    new_df = df.copy()\n    # apply to standardized columns\n    columns = ['Sex','RestingBP','Cholesterol','FastingBS','RestingECG','MaxHR','Oldpeak']\n    new_df['standardized_columns_mean'] = new_df[columns].mean(axis=1)\n    return new_df\n\ndata = feature_engineering(data)","4e53017e":"data.head()","ac07c157":"!pip install pytorch-lightning==1.4.5 -q","61f7d757":"import gc\nimport warnings\nimport multiprocessing\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import roc_auc_score\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport pytorch_lightning as pl\nfrom pytorch_lightning.utilities.seed import seed_everything\n\nwarnings.filterwarnings('ignore')","2324b6f7":"class Cfg:\n    seed = 42\n    lr = 3e-3 \n    epochs = 100\n    n_folds = 5\n    train_batch_size = 32\n    val_batch_size = 512\n    input_size = len(data.columns) - 1\n    hidden_size = 256\n    dropout_ratio = 0.1\n    num_classes = 2\n    target_name = 'HeartDisease'\n    n_gpus = torch.cuda.device_count()\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","6096900a":"seed_everything(Cfg.seed, workers=True)","00f81642":"class HeartFailureDataset(Dataset):\n    def __init__(self, data):\n        super().__init__()\n        self.data = data\n        \n        self.prepare_data()\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def prepare_data(self):\n        self.labels = np.array(self.data['HeartDisease'].values.tolist())\n        self.inputs = np.array(self.data.drop('HeartDisease',axis=1).values.tolist())\n    \n    def __getitem__(self, index):\n        return {\n            'inputs': torch.tensor(self.inputs[index], dtype=torch.float),\n            'labels': torch.tensor(self.labels[index], dtype=torch.long)\n        }","ec15bf43":"class TanhExp(nn.Module):\n    '''\n    TanhExp: https:\/\/arxiv.org\/abs\/2003.09855  \n    '''\n    def __init__(self):\n        super().__init__()\n\n    @torch.jit.script\n    def tanhexp(x):\n        return x * torch.tanh(torch.exp(x))\n\n    def forward(self, input):\n        output = self.tanhexp(input)\n        return output","50937aa7":"class CustomModel(pl.LightningModule):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        \n        self.mlp = nn.Sequential(\n            nn.utils.weight_norm(\n                nn.Linear(self.config.input_size, self.config.hidden_size)\n            ),\n            TanhExp(),\n            nn.utils.weight_norm(\n                nn.Linear(self.config.hidden_size, self.config.hidden_size)\n            ),\n            TanhExp(),\n            nn.utils.weight_norm(\n                nn.Linear(self.config.hidden_size, self.config.hidden_size)\n            ),\n            TanhExp(),\n            nn.AlphaDropout(self.config.dropout_ratio),\n            nn.Linear(self.config.hidden_size, self.config.num_classes)\n        )\n        \n        self.criterion = nn.CrossEntropyLoss()\n        \n    def forward(self, inputs, labels=None):\n        preds = self.mlp(inputs)\n        if labels is not None:\n            loss = self.criterion(preds, labels)\n            return loss\n        else:\n            return preds\n        \n    def training_step(self, batch, batch_idx):\n        loss = self.forward(inputs=batch['inputs'], \n                          labels=batch['labels'])\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        val_loss = self.forward(inputs=batch['inputs'], \n                              labels=batch['labels'])\n        self.log('val_loss', val_loss)\n        return val_loss\n    \n    def test_step(self, batch, batch_idx):\n        return self.validation_step(batch, batch_idx)\n    \n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.config.lr)","31217b18":"def inference(model, data_loader, config):\n    all_outputs = []\n    all_labels = []\n    \n    for batch_idx, batch in enumerate(data_loader):\n        inputs = batch['inputs']\n        labels = batch['labels']\n        \n        inputs = inputs.to(config.device)\n        labels = labels.to(config.device)\n        \n        model.to(config.device)\n        model.eval()\n        with torch.no_grad():\n            outputs = model(\n                inputs=inputs, \n                labels=None\n                )\n        outputs = outputs.cpu().detach().numpy()[:,1].tolist()\n        labels = labels.cpu().detach().numpy().tolist() \n        all_outputs.extend(outputs)\n        all_labels.extend(labels)\n    \n    return all_outputs, all_labels","cc2b96b6":"def run_train(fold, data, tr_idx, val_idx, config):\n    print(f'     ----- Fold: {fold} -----')\n    data = data.drop('cluster_id', axis=1)\n    train, val = data.iloc[tr_idx], data.iloc[val_idx]\n    \n    train_ds = HeartFailureDataset(train)\n    val_ds = HeartFailureDataset(val)\n    \n    train_loader = DataLoader(train_ds, \n                              batch_size=config.train_batch_size, \n                              num_workers=multiprocessing.cpu_count(), \n                              pin_memory=True, \n                              drop_last=True,\n                              shuffle=True)\n    \n    val_loader = DataLoader(val_ds, \n                            batch_size=config.val_batch_size, \n                            num_workers=multiprocessing.cpu_count(), \n                            pin_memory=True, \n                            drop_last=False,\n                            shuffle=False)\n\n    checkpoint = pl.callbacks.ModelCheckpoint(monitor='val_loss', \n                                              mode='min', \n                                              save_top_k=1,  \n                                              save_weights_only=True, \n                                              dirpath=f'model_fold{fold}\/')\n    \n    tb_logger = pl.loggers.TensorBoardLogger(f'model_fold{fold}_logs\/')\n\n    trainer = pl.Trainer(max_epochs=config.epochs,\n                         gpus=config.n_gpus,\n                         logger=tb_logger,\n                         callbacks=[checkpoint],\n                         progress_bar_refresh_rate=0)\n    \n    model = CustomModel(config)\n    model.to(config.device)\n    trainer.fit(model, train_loader, val_loader)\n    \n    model.load_state_dict(torch.load(checkpoint.best_model_path)['state_dict'])\n    outputs, labels = inference(model, val_loader, config)\n\n    del train_ds, val_ds, train_loader, val_loader, model\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    return outputs, labels","d17c02d5":"# COPY: https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/844b4be24\/sklearn\/model_selection\/_split.py#L751\n\nfrom sklearn.model_selection._split import _BaseKFold, _RepeatedSplits\nfrom sklearn.utils.validation import check_random_state, column_or_1d\nfrom sklearn.utils.multiclass import type_of_target\nfrom collections import defaultdict\n\nclass StratifiedGroupKFold(_BaseKFold):\n    \"\"\"Stratified K-Folds iterator variant with non-overlapping groups.\n    This cross-validation object is a variation of StratifiedKFold attempts to\n    return stratified folds with non-overlapping groups. The folds are made by\n    preserving the percentage of samples for each class.\n    The same group will not appear in two different folds (the number of\n    distinct groups has to be at least equal to the number of folds).\n    The difference between GroupKFold and StratifiedGroupKFold is that\n    the former attempts to create balanced folds such that the number of\n    distinct groups is approximately the same in each fold, whereas\n    StratifiedGroupKFold attempts to create folds which preserve the\n    percentage of samples for each class as much as possible given the\n    constraint of non-overlapping groups between splits.\n    Read more in the :ref:`User Guide <cross_validation>`.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of folds. Must be at least 2.\n    shuffle : bool, default=False\n        Whether to shuffle each class's samples before splitting into batches.\n        Note that the samples within each split will not be shuffled.\n        This implementation can only shuffle groups that have approximately the\n        same y distribution, no global shuffle will be performed.\n    random_state : int or RandomState instance, default=None\n        When `shuffle` is True, `random_state` affects the ordering of the\n        indices, which controls the randomness of each fold for each class.\n        Otherwise, leave `random_state` as `None`.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import StratifiedGroupKFold\n    >>> X = np.ones((17, 2))\n    >>> y = np.array([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n    >>> groups = np.array([1, 1, 2, 2, 3, 3, 3, 4, 5, 5, 5, 5, 6, 6, 7, 8, 8])\n    >>> cv = StratifiedGroupKFold(n_splits=3)\n    >>> for train_idxs, test_idxs in cv.split(X, y, groups):\n    ...     print(\"TRAIN:\", groups[train_idxs])\n    ...     print(\"      \", y[train_idxs])\n    ...     print(\" TEST:\", groups[test_idxs])\n    ...     print(\"      \", y[test_idxs])\n    TRAIN: [1 1 2 2 4 5 5 5 5 8 8]\n           [0 0 1 1 1 0 0 0 0 0 0]\n     TEST: [3 3 3 6 6 7]\n           [1 1 1 0 0 0]\n    TRAIN: [3 3 3 4 5 5 5 5 6 6 7]\n           [1 1 1 1 0 0 0 0 0 0 0]\n     TEST: [1 1 2 2 8 8]\n           [0 0 1 1 0 0]\n    TRAIN: [1 1 2 2 3 3 3 6 6 7 8 8]\n           [0 0 1 1 1 1 1 0 0 0 0 0]\n     TEST: [4 5 5 5 5]\n           [1 0 0 0 0]\n    Notes\n    -----\n    The implementation is designed to:\n    * Mimic the behavior of StratifiedKFold as much as possible for trivial\n      groups (e.g. when each group contains only one sample).\n    * Be invariant to class label: relabelling ``y = [\"Happy\", \"Sad\"]`` to\n      ``y = [1, 0]`` should not change the indices generated.\n    * Stratify based on samples as much as possible while keeping\n      non-overlapping groups constraint. That means that in some cases when\n      there is a small number of groups containing a large number of samples\n      the stratification will not be possible and the behavior will be close\n      to GroupKFold.\n    See also\n    --------\n    StratifiedKFold: Takes class information into account to build folds which\n        retain class distributions (for binary or multiclass classification\n        tasks).\n    GroupKFold: K-fold iterator variant with non-overlapping groups.\n    \"\"\"\n\n    def __init__(self, n_splits=5, shuffle=False, random_state=None):\n        super().__init__(n_splits=n_splits, shuffle=shuffle,\n                         random_state=random_state)\n\n    def _iter_test_indices(self, X, y, groups):\n        # Implementation is based on this kaggle kernel:\n        # https:\/\/www.kaggle.com\/jakubwasikowski\/stratified-group-k-fold-cross-validation\n        # and is a subject to Apache 2.0 License. You may obtain a copy of the\n        # License at http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n        # Changelist:\n        # - Refactored function to a class following scikit-learn KFold\n        #   interface.\n        # - Added heuristic for assigning group to the least populated fold in\n        #   cases when all other criteria are equal\n        # - Swtch from using python ``Counter`` to ``np.unique`` to get class\n        #   distribution\n        # - Added scikit-learn checks for input: checking that target is binary\n        #   or multiclass, checking passed random state, checking that number\n        #   of splits is less than number of members in each class, checking\n        #   that least populated class has more members than there are splits.\n        rng = check_random_state(self.random_state)\n        y = np.asarray(y)\n        type_of_target_y = type_of_target(y)\n        allowed_target_types = ('binary', 'multiclass')\n        if type_of_target_y not in allowed_target_types:\n            raise ValueError(\n                'Supported target types are: {}. Got {!r} instead.'.format(\n                    allowed_target_types, type_of_target_y))\n\n        y = column_or_1d(y)\n        _, y_inv, y_cnt = np.unique(y, return_inverse=True, return_counts=True)\n        if np.all(self.n_splits > y_cnt):\n            raise ValueError(\"n_splits=%d cannot be greater than the\"\n                             \" number of members in each class.\"\n                             % (self.n_splits))\n        n_smallest_class = np.min(y_cnt)\n        if self.n_splits > n_smallest_class:\n            warnings.warn((\"The least populated class in y has only %d\"\n                           \" members, which is less than n_splits=%d.\"\n                           % (n_smallest_class, self.n_splits)), UserWarning)\n        n_classes = len(y_cnt)\n        \n        \n        _, groups_inv, groups_cnt = np.unique(\n            groups, return_inverse=True, return_counts=True)\n        y_counts_per_group = np.zeros((len(groups_cnt), n_classes))\n        for class_idx, group_idx in zip(y_inv, groups_inv):\n            y_counts_per_group[group_idx, class_idx] += 1\n\n        y_counts_per_fold = np.zeros((self.n_splits, n_classes))\n        groups_per_fold = defaultdict(set)\n\n        if self.shuffle:\n            rng.shuffle(y_counts_per_group)\n\n        # Stable sort to keep shuffled order for groups with the same\n        # class distribution variance\n        sorted_groups_idx = np.argsort(-np.std(y_counts_per_group, axis=1),\n                                       kind='mergesort')\n\n        for group_idx in sorted_groups_idx:\n            group_y_counts = y_counts_per_group[group_idx]\n            best_fold = self._find_best_fold(\n                y_counts_per_fold=y_counts_per_fold, y_cnt=y_cnt,\n                group_y_counts=group_y_counts)\n            y_counts_per_fold[best_fold] += group_y_counts\n            groups_per_fold[best_fold].add(group_idx)\n\n        for i in range(self.n_splits):\n            test_indices = [idx for idx, group_idx in enumerate(groups_inv)\n                            if group_idx in groups_per_fold[i]]\n            yield test_indices\n\n    def _find_best_fold(\n            self, y_counts_per_fold, y_cnt, group_y_counts):\n        best_fold = None\n        min_eval = np.inf\n        min_samples_in_fold = np.inf\n        for i in range(self.n_splits):\n            y_counts_per_fold[i] += group_y_counts\n            # Summarise the distribution over classes in each proposed fold\n            std_per_class = np.std(\n                y_counts_per_fold \/ y_cnt.reshape(1, -1),\n                axis=0)\n            y_counts_per_fold[i] -= group_y_counts\n            fold_eval = np.mean(std_per_class)\n            samples_in_fold = np.sum(y_counts_per_fold[i])\n            is_current_fold_better = (\n                fold_eval < min_eval or\n                np.isclose(fold_eval, min_eval)\n                and samples_in_fold < min_samples_in_fold\n            )\n            if is_current_fold_better:\n                min_eval = fold_eval\n                min_samples_in_fold = samples_in_fold\n                best_fold = i\n        return best_fold","d0348a38":"kmeans = KMeans(n_clusters=30, random_state=Cfg.seed)\n\nkmeans.fit(np.array(data, dtype=np.float32))\ndata['cluster_id'] = kmeans.labels_","1aa7966a":"sgkf = StratifiedGroupKFold(n_splits=Cfg.n_folds, shuffle=True, random_state=Cfg.seed)\nauc_scores = 0.0\n\nfor i, (tr_idx, val_idx) in enumerate(sgkf.split(data, data[Cfg.target_name], data['cluster_id'])):\n    outputs, labels = run_train(i, data, tr_idx, val_idx, Cfg)\n    outputs = np.array(outputs)\n    labels = np.array(labels)\n    \n    auc_score = roc_auc_score(labels, outputs)\n    auc_scores += auc_score\n    \n    del outputs, labels\n    gc.collect()\n    print(f'FOLD{i} ROC AUC SCORE: {auc_score:.5f}')\nprint(f'{Cfg.n_folds}FOLDS CV ROC AUC SCORE: {auc_scores\/Cfg.n_folds:.5f}')","99afb813":"## Model Training","3850f850":"## Preprocess","eb03910b":"# Heart Failure Prediction using  CustomNN\n\n## Introduction\n\nThis notebook challenges the task of predicting Heart Failure  using an interesting dataset published by [fedesoriano](https:\/\/www.kaggle.com\/fedesoriano).","e5845440":"#### Next, check by gender.","b234c3a3":"## Feature Engineering","085b9425":"## Check The Dataset","094711a6":"### Dataset Overview\n\n<br>\n\n`Age`: Age of the patient\n\n<br>\n\n`Sex`: Sex of the patient (M: Male or F: Female)\n\n<br>\n\n`ChestPainType`: Chest pain type\n              \n                 TA: Typical Angina, \n              \n                 ATA: Atypical Angina, \n              \n                 NAP: Non-Anginal Pain, \n              \n                 ASY: Asymptomatic\n              \n<br>\n\n`RestingBP`: Resting blood pressure (0 ~ 200)\n\n<br>\n\n`Cholesterol`: Serum cholestrol (0 ~ 603)\n\n<br>\n\n`FastingBS`: Fasting blood sugar (1 or 0)\n\n<br>\n\n`RestingECG`: Resting electrocardiogram results\n\n              Normal: Normal, \n              \n              ST: having ST-T wave abnormality, \n              \n              LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria\n\n<br>\n\n`MaxHR`: Maximum heart rate achieved (60 ~ 202)\n\n<br>\n\n`ExerciseAngina`: Exercise-induced angina (Yes or No)\n\n<br>\n\n`Oldpeak`: oldpeak = ST [Numeric value measured in depression] (-2.6 ~  6.2)\n\n<br>\n\n`ST_Slope`: The slope of the peak exercise ST segment\n            \n            Up: upsloping, \n            \n            Flat: flat, \n            \n            Down: downsloping\n\n<br>\n\n`HeartDisease`: Heart disease(1 or 0)","38fdc8f3":"### Metrics: Area under the ROC curve (ROC AUC)\n\n<br>\n\n### CV: StratifiedGroupKFold\n\nCreate a cluster with KMeans and use that cluster as the group id.","79b2c445":"#### We'll check at the data by column.","46236aec":"## Thanks for reading!!"}}