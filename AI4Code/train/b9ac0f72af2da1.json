{"cell_type":{"905159c6":"code","2e526617":"code","5d059430":"code","b106e649":"code","c9c857a7":"code","266caecc":"code","5141401f":"code","683e519c":"code","ac1c80ff":"code","c6d08d9f":"code","d2c1355a":"code","b7a6c795":"code","f2918f58":"code","c883ec9c":"code","78484865":"code","57b7b4e0":"code","f6149b71":"code","1e8541cc":"code","b87f45b1":"markdown","ff64bfa5":"markdown","24629462":"markdown","af5f7340":"markdown","4197093e":"markdown","ba0076b4":"markdown","d9e45736":"markdown","81285554":"markdown","a3829d85":"markdown","74f1e835":"markdown","08b79e54":"markdown","1642ceff":"markdown","3dd30675":"markdown","ec1e3451":"markdown","141927c6":"markdown","14a8f966":"markdown","67dc17b9":"markdown","29b6ea19":"markdown","b80394ac":"markdown","9483ddf8":"markdown","0ee50eb7":"markdown","fb45294a":"markdown","dc260d39":"markdown"},"source":{"905159c6":"import numpy as np \nimport pandas as pd \nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.impute import KNNImputer\nimport pandas_profiling as pp\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2e526617":"data = pd.read_csv('\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ndata.head(3)","5d059430":"pp.ProfileReport(data)","b106e649":"le = LabelEncoder()\n\ndata['city'] = le.fit_transform(data['city'])\ndata['city'].head(5)","c9c857a7":"gender_map = {\n    'Female' : 2,\n    'Male' : 1,\n    'Other' : 0\n    }\n\nrelevent_experience_map = {\n    'Has relevent experience' : 1,\n    'No relevent experience' : 0\n    }\n\nenrolled_university_map = {\n    'no_enrollment' : 0,\n    'Part time course' : 1,\n    'Full time course' : 2\n    }\n\neducation_level_map = {\n    'Primary School' :    0,\n    'Graduate'       :    2,\n    'Masters'        :    3, \n    'High School'    :    1, \n    'Phd'            :    4\n    } \n\nmajor_map = {\n    'STEM' : 0,\n    'Business Degree' : 1,\n    'Humanities' : 2,\n    'Arts' : 3,\n    'Other' : 4,\n    'No Major' : 5\n    }\n\nexperience_map = {\n    '<1' : 0,\n    '1' : 1,\n    '2' : 2,\n    '3' : 3,\n    '4' : 4,\n    '5' : 5,\n    '6' : 6,\n    '7' : 7,\n    '8' : 8,\n    '9' : 9,\n    '10' : 10,\n    '11' : 11,\n    '12' : 12,\n    '13' : 13,\n    '14' : 14,\n    '15' : 15,\n    '16' : 16,\n    '17' : 17,\n    '18' : 18,\n    '19' : 19,\n    '20' : 20,\n    '>20' : 21\n    }\n\nsize_map = {\n    '<10' : 0,\n    '10\/49' : 1,\n    '50-99' : 2,\n    '100-500' :3,\n    '500-999' :4,\n    '1000-4999': 5,\n    '5000-9999' : 6,\n    '10000+' : 7\n    }\n\ncompany_type_map = {\n    'Pvt Ltd'               :    0,\n    'Funded Startup'        :    1, \n    'Early Stage Startup'   :    2, \n    'Other'                 :    3, \n    'Public Sector'         :    4, \n    'NGO'                   :    5\n}\n\nlast_new_job_map = {\n    'never'        :    0,\n    '1'            :    1, \n    '2'            :    2, \n    '3'            :    3, \n    '4'            :    4, \n    '>4'           :    5\n}\n\ndata.loc[:,'education_level'] = data['education_level'].map(education_level_map)\ndata.loc[:,'company_size'] = data['company_size'].map(size_map)\ndata.loc[:,'company_type'] = data['company_type'].map(company_type_map)\ndata.loc[:,'last_new_job'] = data['last_new_job'].map(last_new_job_map)\ndata.loc[:,'major_discipline'] = data['major_discipline'].map(major_map)\ndata.loc[:,'enrolled_university'] = data['enrolled_university'].map(enrolled_university_map)\ndata.loc[:,'relevent_experience'] = data['relevent_experience'].map(relevent_experience_map)\ndata.loc[:,'gender'] = data['gender'].map(gender_map)\ndata.loc[:,'experience'] = data['experience'].map(experience_map)","266caecc":"knn_imputer = KNNImputer()\n#making a copy just in case\ncopy = data.copy()\n\ncopy = knn_imputer.fit_transform(copy)\n#rounding the knn values\ncopy[:, 3:] = np.round(copy[:, 3:])\ndata = pd.DataFrame(copy, columns = data.columns)","5141401f":"numeric = data[[\"city_development_index\", \"training_hours\", \"target\"]].copy()\ncategory = data[[\"city\", \"gender\", \"relevent_experience\", \"enrolled_university\", \"education_level\", \"major_discipline\", \"experience\", \"company_size\", \"company_type\", \"last_new_job\"]].copy()\n\n#using the previously manual encoded columns\ncategory_ordinalencoded = category[['education_level', 'experience', 'company_size', 'last_new_job']]\n","683e519c":"#columns that need to be one hot encoded\none_how_columns = [ col for col in category.columns if col not in ['education_level', 'experience', 'company_size', 'last_new_job']]\n\n#onehotencoder\nohe = OneHotEncoder(sparse=False).fit(category.loc[:, one_how_columns])\ncategory_onehotEncoded = ohe.transform(category.loc[:, one_how_columns])\n\n#joining all the category columns\ncategory_preprocessed = np.concatenate([category_onehotEncoded, category_ordinalencoded], axis=1)\n\n#joining all the features\nX = np.concatenate([numeric.drop('target', axis=1).values, category_preprocessed], axis=1)\ny = numeric['target'].values","ac1c80ff":"X, y = SMOTE(random_state = 99).fit_resample(X, y)","c6d08d9f":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state = 99)","d2c1355a":"dtc = DecisionTreeClassifier(criterion='entropy',random_state = 99)\ndtc.fit(X_train, y_train)\n\ny_dtc = dtc.predict(X_test)\naccuracy_score(y_test, y_dtc)","b7a6c795":"#svm needs scaled data\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\ntrain_copy = X_train.copy()\ntest_copy = X_test.copy()\n\ntrain_copy = sc_X.fit_transform(train_copy)\ntest_copy = sc_X.transform(test_copy)\nsvm = SVC(kernel = 'rbf', random_state=0)\nsvm.fit(train_copy, y_train)\n\n#predicting test set\ny_svm = svm.predict(test_copy)\naccuracy_score(y_test, y_svm)","f2918f58":"rf1 = RandomForestClassifier(random_state = 0)\nrf1.fit(X_train, y_train)\n\n#predicting test set\ny_rf1 = rf1.predict(X_test)\naccuracy_score(y_test, y_rf1)","c883ec9c":"rf = RandomForestClassifier(random_state = 99)\nrf.fit(X_train, y_train)\n\ny_base = rf.predict(X_test)\n\ncm_base =  confusion_matrix(y_test, y_base)\ncm_base","78484865":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\n# Use the random grid to search for best hyperparameters\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)\n\nrf_random.best_params_","57b7b4e0":"best_random = rf_random.best_estimator_\ny_rand = best_random.predict(X_test)\n\n#confusion matrix\ncm_rand = confusion_matrix(y_test, y_rand)\ncm_rand","f6149b71":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [False],\n    'max_depth': [None, 20, 50, 70, 100],\n    'max_features': ['auto', 'sqrt'],\n    'min_samples_leaf': [2],\n    'min_samples_split': [5],\n    'n_estimators': [400, 600, 800, 1200, 1400, 1600]\n}\n# Create a base model\nrfc = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rfc, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\n\ngrid_search.fit(X_train, y_train)\ngrid_search.best_params_\n\ny_grid = grid_search.predict(X_test)\ncm_grid = confusion_matrix(y_test, y_grid)\ncm_grid\n","1e8541cc":"print(classification_report(y_test, y_grid))\naccuracy_score(y_test, y_grid)","b87f45b1":"## Data Exploration\n\nFor EDA, I used this library that provides almost all the relevent information we need. So we don't have to manually look around the data ( Saves time:) )\n","ff64bfa5":"Decision tree gives us a **79.2%** accuracy\n### SVM","24629462":"# Handling missing data with knn\nAs we saw in the EDA report, there is a lot of missing data which we can deal with the help of KNN Imputer","af5f7340":"# Preprocessing\n**Label encoding \"city\" feature**","4197093e":"**Manual encoding ordinal features**\nSome of the categorical features in this dataset are ordinal, i.e,there is a clear ordering of the categories. So I have manually encoded these.","ba0076b4":"SVM gives us a **79.1%** accuracy\n### Random Forest","d9e45736":"## Using RandomSearch to find the optimal parameters\n\nRandomizedSearchCV randomly goes through the combination of parameters and gives the best one found. It does not give the absolute best parameters but its usually pretty close and helps in reducing the iteratons in Gridsearch","81285554":"## Base Model","a3829d85":"# The Dataset\n","74f1e835":"**NOTE: I used random search twice and created the param_grid based on those two results (Yes, I got two different best_params_!)**","08b79e54":"# Comparing models\n### Decision tree","1642ceff":"# Tuning the Random Forest Model","3dd30675":" **One Hot encoding the rest categorical columns**","ec1e3451":"## Grid Search","141927c6":"## Train-test split","14a8f966":"# One Hot Encoding \nNow, the categorical features which are not ordinal will be one hot encoded","67dc17b9":"Random forest performs the best among these with an accuracy of **84.2%**\n\nSo, we'll finetune the random forest model to increase the performance.","29b6ea19":"* enrollee_id : Unique ID (Useless here)\n* city: City code\n* city_ development _index : Developement index of the city (scaled)\n* gender: Gender of candidate\n* relevent_experience: Relevant experience of candidate\n* enrolled_university: Type of University course enrolled if any\n* education_level: Education level of candidate\n* major_discipline :Education major discipline of candidate\n* experience: Total experience in years\n* company_size: No of employees in current company\n* company_type : Type of current company\n* lastnewjob: Difference in years between previous job and current job\n* training_hours: training hours completed\n\n    **target: 0 \u2013 Not looking for job change, 1 \u2013 Looking for a job change**","b80394ac":"## Imbalanced data\nAs we can see clearly, the data is very imbalanced which needs to be dealt with if we want our model to train properly.\nFor this, I used SMOTE(Synthetic Minority Oversampling Technique) to make the data balanced. \n> If you want to know more about SMOTE, check this [link](https:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/)","9483ddf8":"Comparing the Random search model","0ee50eb7":"## Accuracy","fb45294a":" **Dividing numeric and categorical data**","dc260d39":"## Description\nWith the help of these features we have to predict which data scientist is looking for a job change and thus, is a better prospect in terms of hiring."}}