{"cell_type":{"49c8e559":"code","0b3a8a60":"code","a723cc64":"code","6b93f791":"code","82d9e69a":"code","de5f1944":"code","1f89b0cb":"code","48171f3b":"markdown","ada00b89":"markdown","8718b856":"markdown","034c9558":"markdown","d29761af":"markdown"},"source":{"49c8e559":"from IPython.display import clear_output\n!pip install mglearn \nclear_output()","0b3a8a60":"import numpy as np\nimport pandas as pd \nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score,train_test_split,GridSearchCV,ParameterGrid, StratifiedKFold\nfrom sklearn.datasets import load_iris\nimport mglearn\nimport matplotlib.pyplot as plt","a723cc64":"def nested_cv(X, y, inner_cv, outer_cv, Classifier, parameter_grid):\n    outer_scores = []\n    # for each split of the data in the outer cross-validation\n    # (split method returns indices of training and test parts)\n    for training_samples, test_samples in outer_cv.split(X, y):\n        # find best parameter using inner cross-validation\n        best_parms = {}\n        best_score = -np.inf\n        # iterate over parameters\n        for parameters in parameter_grid:\n            # accumulate score over inner splits\n            cv_scores = []\n            # iterate over inner cross-validation\n            for inner_train, inner_test in inner_cv.split(\n                    X[training_samples], y[training_samples]):\n                # build classifier given parameters and training data\n                clf = Classifier(**parameters)\n                clf.fit(X[inner_train], y[inner_train])\n                # evaluate on inner test set\n                score = clf.score(X[inner_test], y[inner_test])\n                cv_scores.append(score)\n            # compute mean score over inner folds\n            mean_score = np.mean(cv_scores)\n            if mean_score > best_score:\n                # if better than so far, remember parameters\n                best_score = mean_score\n                best_params = parameters\n        # build classifier on best parameters using outer training set\n        clf = Classifier(**best_params)\n        clf.fit(X[training_samples], y[training_samples])\n        # evaluate\n        outer_scores.append(clf.score(X[test_samples], y[test_samples]))\n    return np.array(outer_scores)","6b93f791":"SEED = 42\nNFOLD = 5\ngamma_list = [0.001, 0.01, 0.1, 1, 10, 100]\nC_list = [0.001, 0.01, 0.1, 1, 10, 100]\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n              'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}","82d9e69a":"iris = load_iris()\nprint(\"Iris labels:\\n{}\".format(iris.target))","de5f1944":"scores = cross_val_score(GridSearchCV(SVC(), param_grid, cv=NFOLD),\n                         iris.data, iris.target, cv=NFOLD)\nprint(\"Cross-validation scores: \", scores)\nprint(\"Mean cross-validation score: \", scores.mean())","1f89b0cb":"scores = nested_cv(iris.data, iris.target, StratifiedKFold(5),\n                   StratifiedKFold(5), SVC, ParameterGrid(param_grid))\nprint(\"Cross-validation scores: {}\".format(scores))\nprint(\"Mean cross-validation score: \", scores.mean())","48171f3b":"# load data","ada00b89":"# same result ","8718b856":"# 1.Nested cross-validation (using GridSearch) - very simple","034c9558":"# 2.Nested cross-validation (using Complex Coding) ","d29761af":"# nested cross validation functions"}}