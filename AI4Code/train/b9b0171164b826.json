{"cell_type":{"757335ed":"code","25812f39":"code","900d9b51":"code","cee67556":"code","6cdf7ada":"code","13e6abad":"code","b024e585":"code","3924e821":"code","defc574e":"code","0fcdaf9a":"code","b5d08d93":"code","8973caef":"code","0dbb047a":"code","1dff1ab1":"code","10ed5c4b":"code","838784ff":"code","2ca6ebc8":"code","3e8ab3a4":"code","6b6d276a":"code","7e5f097b":"code","f5da828a":"markdown","5e2a4e67":"markdown","d72aaec5":"markdown","83c726be":"markdown","d8f668cb":"markdown","d122d8e1":"markdown","4ab00eae":"markdown","554281f4":"markdown","7d172db8":"markdown","1bcb07b4":"markdown","c01ae93f":"markdown","f8da7bc3":"markdown","6e3165ed":"markdown","147684ff":"markdown","8505ee4f":"markdown","991bf7b4":"markdown","ce6d108a":"markdown","8b4ebf65":"markdown","690eba29":"markdown","63b64167":"markdown","23081e2f":"markdown"},"source":{"757335ed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","25812f39":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom dateutil import parser\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.svm import SVC \nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nimport pickle\nfrom lightgbm import LGBMClassifier\nimport warnings\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings('ignore')","900d9b51":"df=pd.read_csv('..\/input\/banknote-authentication-uci\/BankNoteAuthentication.csv')\ndf.head()","cee67556":"X = df.iloc[:,:-1]\ny = df.iloc[:,-1]","6cdf7ada":"df.shape","13e6abad":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)","b024e585":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nrf = RandomForestClassifier(max_features=4,n_estimators=100)","3924e821":"rf.fit(X_train,y_train)","defc574e":"y_pred = rf.predict(X_test)","0fcdaf9a":"rf.score(X_test,y_test)","b5d08d93":"# rf.score(X_test,y_test)\naccuracy_score(y_pred,y_test)","8973caef":"from sklearn.model_selection import GridSearchCV\n\nmax_features_range = np.arange(1,5,1)\nn_estimators_range = np.arange(10,201,10)\nparam_grid = dict(max_features=max_features_range,n_estimators=n_estimators_range)\n\nrd = RandomForestClassifier()\n    \ngrid = GridSearchCV(estimator=rf,param_grid=param_grid,cv=5)","0dbb047a":"grid.fit(X_train,y_train)","1dff1ab1":"print(\"The best parameters are %s with a score of %0.2f\"\n      % (grid.best_params_,grid.best_score_))","10ed5c4b":"grid_results = pd.concat([pd.DataFrame(grid.cv_results_[\"params\"]),pd.DataFrame(grid.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])],axis=1)\ngrid_results.head()","838784ff":"# Creating Data Frame\n\ngrid_contour = grid_results.groupby(['max_features','n_estimators']).mean()\ngrid_contour","2ca6ebc8":"# Pivoting Data \ngrid_reset = grid_contour.reset_index()\ngrid_reset.columns = ['max_features', 'n_estimators', 'Accuracy']\ngrid_pivot = grid_reset.pivot('max_features', 'n_estimators')\ngrid_pivot","3e8ab3a4":"x = grid_pivot.columns.levels[1].values\ny = grid_pivot.index.values\nz = grid_pivot.values","6b6d276a":"import plotly.graph_objects as go\n\n# X and Y axes labels\nlayout = go.Layout(\n            xaxis=go.layout.XAxis(\n              title=go.layout.xaxis.Title(\n              text='n_estimators')\n             ),\n             yaxis=go.layout.YAxis(\n              title=go.layout.yaxis.Title(\n              text='max_features') \n            ) )\n\nfig = go.Figure(data = [go.Contour(z=z, x=x, y=y)], layout=layout )\n\nfig.update_layout(title='Hyperparameter tuning', autosize=False,\n                  width=500, height=500,\n                  margin=dict(l=65, r=50, b=65, t=90))\n\nfig.show()","7e5f097b":"import plotly.graph_objects as go\n\n\nfig = go.Figure(data= [go.Surface(z=z, y=y, x=x)], layout=layout )\nfig.update_layout(title='Hyperparameter tuning',\n                  scene = dict(\n                    xaxis_title='n_estimators',\n                    yaxis_title='max_features',\n                    zaxis_title='Accuracy'),\n                  autosize=False,\n                  width=800, height=800,\n                  margin=dict(l=65, r=50, b=65, t=90))\nfig.show()","f5da828a":"### 3.1 Making Predictions","5e2a4e67":"### ","d72aaec5":"### 2.3 Test Train Split","83c726be":"### 3.3 Accuracy Score ","d8f668cb":"# 6.Result Visualization","d122d8e1":"# 2.Data Preparation  ","4ab00eae":"We can see that the region with light Yellow have the best hyperparameters.We can select hyperparameters fro this area.Our Grid Search has give us the best values for max_features and n_estimators as 1 and 30.If we look at the 2D contour plot carefully we can see that a region is marked in light yellow indicating the best hyper parameters.","554281f4":"# 5.Hyperparameter's and Accuracy ","7d172db8":"# 7.Conclusion \n\n1.We have build a Random Forest Model to Predict the Fake Notes\n\n2.We have used Grid Search to find out the best hyperameter for our machine learning model\n\n3.We have displayed the hyperparameters with model accuracy with 2D and 3D plots.\n\n### You can refer to my other notebooks from https:\/\/www.kaggle.com\/binuthomasphilip\/code\u00b6","1bcb07b4":"### 6.2 3D Surface Plot","c01ae93f":"### 3.2 R Square Error ","f8da7bc3":"### ### Importing Python Modules ","6e3165ed":"### 2.1 Creating Matrix of Features","147684ff":"### 2.2 Examining the Shape of Data","8505ee4f":"# 4.Hypeparameter Tuning ","991bf7b4":"# 1.Importing and Data Exploration data","ce6d108a":"Using the second approach we have access to predicted data values.","8b4ebf65":"### Recently I published a self help book titled Inspiration: Thoughts on Spirituality, Technology, Wealth, Leadership and Motivation. The preview of the book can be read from the Amazon link https:\/\/lnkd.in\/gj7bMQA\n\nAny machine learning algorithm has many hyperparameters.Many people generally use the default values for the hyper parameters.But if we can optimise these hyper parameters then we wll be in a position to further improve the performance of our machine learning models.In this notebooks we will cover following things\n\n1.Data Import\n\n2.Data Preparation\n\n3.Building a Random Forest Model\n\n4.Hyper Parameter tuning using Grid Search\n\n5.Creating dataframe and Pivot table of hyperparameter and accuracy\n\n6.Result Visualization\n\n7.Conclusion\n\n### You can refer to my other notebooks from https:\/\/www.kaggle.com\/binuthomasphilip\/code","690eba29":"# 3.Model Build\n\nWe will be building up Random Forest model to classify Bank notes.We will be considering n_estimators and max_features hyper parameters while building up our Random Forest Model.","63b64167":"### 6.1 2D Contour Plot","23081e2f":"### Importing Data"}}