{"cell_type":{"2b8cad34":"code","0d432a0f":"code","3f483b72":"code","896cff5d":"code","c5dbb0eb":"code","3cfea30d":"code","0b6a7ef9":"code","38e170e2":"code","99b935ef":"code","ae845f91":"code","93950101":"code","9ac206d4":"code","ec384b55":"code","e933f8a0":"code","9b8324f4":"code","ce37a424":"code","15577746":"code","979917ac":"code","b5f7b56a":"code","21b8e856":"code","7efb875f":"code","1a5fd707":"code","d7532250":"code","f8fa2b5b":"code","5876b149":"code","5c71574f":"code","b2baf947":"code","e5314b1b":"code","1b19398c":"code","1ef698e2":"code","75c4e40d":"code","b8372fe6":"code","8e1767bf":"markdown","e9e91547":"markdown","d6d2b3cd":"markdown","d3799019":"markdown","d7cd9d1d":"markdown","7e66a407":"markdown","0051cf4b":"markdown","04d6b065":"markdown","21d9ad50":"markdown","adfeabc8":"markdown","7e7652e3":"markdown","175593aa":"markdown","8d923d62":"markdown","b468310a":"markdown","dd0cfa7a":"markdown"},"source":{"2b8cad34":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sun Nov  1 12:07:10 2020\n@author: jaket\n\"\"\"","0d432a0f":"import math\nimport pandas as pd\nimport numpy as np\nimport os\nimport warnings \nimport calendar","3f483b72":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler","896cff5d":"from sklearn.model_selection import KFold\nfrom category_encoders import CountEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import log_loss","c5dbb0eb":"from sklearn.multioutput import MultiOutputClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier","3cfea30d":"warnings.filterwarnings('ignore')","0b6a7ef9":"SEED = 42\nnp.random.seed(SEED)","38e170e2":"train = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntest = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\ntargets = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')","99b935ef":"scaler = StandardScaler()","ae845f91":"numeric = train.drop(['sig_id', 'cp_type', 'cp_time', 'cp_dose'], axis=1)\ncats= train.filter(items=['sig_id', 'cp_type', 'cp_time', 'cp_dose'])","93950101":"scaler.fit(numeric) # Fit Scaler\nnumeric_sc = scaler.transform(numeric) # Scale data","9ac206d4":"pca = PCA(.90)\npca.fit(numeric_sc) # Do PCA\npca.n_components_  # How many dimensions?","ec384b55":"train_pc =pca.transform(numeric_sc)\ntrain_pc = pd.DataFrame(data=train_pc)\ntrain=pd.concat([train_pc, cats], axis=1) # Add back to cats","e933f8a0":"print ('90% of the variance is explained in ',pca.n_components_,  ' components.') ","9b8324f4":"numeric_test = test.drop(['sig_id', 'cp_type', 'cp_time', 'cp_dose'], axis=1)\ncats_test= test.filter(items=['sig_id', 'cp_type', 'cp_time', 'cp_dose'])\nscaler.fit(numeric_test)\nnumeric_sc_test = scaler.transform(numeric_test) # Scale data","ce37a424":"test_pc =pca.transform(numeric_sc_test)\ntest_pc = pd.DataFrame(data=test_pc)\ntest= pd.concat([test_pc, cats_test], axis=1)","15577746":"train[['cp_type', 'cp_dose']]=train[['cp_type', 'cp_dose']].astype('category')\ntest[['cp_type', 'cp_dose']]=test[['cp_type', 'cp_dose']].astype('category')","979917ac":"dummies_train=pd.get_dummies(train[['cp_type', 'cp_dose']])\ndummies_test=pd.get_dummies(test[['cp_type', 'cp_dose']])","b5f7b56a":"train=train.drop(['cp_type', 'cp_dose'], axis=1) # Delete uncoded cats\ntest=test.drop(['cp_type', 'cp_dose'], axis=1)","21b8e856":"train=pd.concat([train, dummies_train], axis=1) # Add encoded cats\ntest=pd.concat([test, dummies_test], axis=1)","7efb875f":"X = train.drop('sig_id', axis=1).to_numpy()\nX_test = test.drop('sig_id', axis=1).to_numpy()\ny = targets.drop('sig_id', axis=1).to_numpy() ","1a5fd707":"del train\ndel test\ndel numeric\ndel cats\ndel numeric_sc\ndel train_pc\ndel numeric_test\ndel cats_test\ndel numeric_sc_test\ndel test_pc\ndel dummies_train\ndel dummies_test\n","d7532250":"kf = KFold(n_splits=5)\ndef model_validation_loop (X, y, clf):\n    log_loss_list = []\n    for n, (train_idx, pred_idx) in enumerate(kf.split(X, y)):\n        print('Starting fold: ', n)\n        X_train, X_test = X[train_idx], X[pred_idx]\n        y_train, y_test = y[train_idx], y[pred_idx]\n        \n        clf.fit(X_train, y_train)\n        \n        # Get Log Loss\n        \n        preds = clf.predict_proba(X_test) # list of preds per class\n        preds = np.array(preds)[:,:,1].T # take the positive class\n        \n        loss = log_loss(np.ravel(y_test), np.ravel(preds))\n        print('Log Loss for this fold:', loss)\n        log_loss_list.append(loss)\n    \n    print('Mean Log Loss:', np.mean(log_loss_list))\n    print('_'*50)\n    \n","f8fa2b5b":"classifiers = [\n    MultiOutputClassifier(XGBClassifier()),\n    MultiOutputClassifier(MLPClassifier(random_state = 42))    \n    ]","5876b149":"clf_names= ['GradientBoost', 'NeuralNet']","5c71574f":"params = {'colsample_bytree': 0.6522,\n          'gamma': 3.6975,\n          'learning_rate': 0.0503,\n          'max_delta_step': 2.0706,\n          'max_depth': 10,\n          'min_child_weight': 31.5800,\n          'n_estimators': 166,\n          'subsample': 0.8639}","b2baf947":"tuned_xgb=MultiOutputClassifier(XGBClassifier().set_params(**params))","e5314b1b":"model_validation_loop(X, y, tuned_xgb)","1b19398c":"sample_sub =  pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","1ef698e2":"final_preds=pd.DataFrame(data=tuned_xgb.predict(X_test))","75c4e40d":"ids=pd.DataFrame(data=sample_sub['sig_id'])[0:100]\nfinal_preds=pd.concat([ids, final_preds], axis=1)\nfinal_preds.columns=sample_sub.columns","b8372fe6":"final_preds.to_csv('submission.csv', index=False)","8e1767bf":"Lets do the same with the test data...","e9e91547":"Now that we have X, y and X_test we dont need anything else in the environment. Kaggle throws an error 137 (OOM) without deleting all objects - so I do this for space.","d6d2b3cd":"### Encode categorical variables","d3799019":"The XGB model has a slight performance bonus on the Neural Net. Now we can tune the XGB model. Instead of doing a parameter search I have looked on Kaggle for parametes to save many hours. Thanks to: https:\/\/www.kaggle.com\/fchmiel\/xgboost-baseline-multilabel-classification","d7cd9d1d":"Drop id col and turn to np","7e66a407":"# Mechanisms of Action - A XGb and NN Comparison\n","0051cf4b":"We'll compare these 2 multioutput clfs","04d6b065":"## Set up cross-fold validation","21d9ad50":"Write","adfeabc8":"Improved model observed. Lets now predict the test data","7e7652e3":"Run validation fn to test improvement in performance in the basic vs tuned model.","175593aa":"Predict","8d923d62":"## PCA on train features<br>\n\n#### First scale the numerical data, then fit a PCA to it","b468310a":"### On Kaggle Neural nets and XGB classifiers have been the most commonly used approaches to this multioutput classification problem.<br>\n### Here, I compare the log loss of both and tune\/use the best for submission. EDA has already been extensively done so I will go straight to modelling. I use PCA to reduce the variables included as the factor number is significant and highly covaried.<br>\n","dd0cfa7a":"Format"}}