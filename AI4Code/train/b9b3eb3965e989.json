{"cell_type":{"f575cc34":"code","7cedfecd":"code","ee3b6c1f":"code","b6ef4c4c":"code","07396f56":"code","81b03aa1":"code","a734949c":"code","2a6118a1":"code","e741bde0":"code","25d015e2":"code","366fd175":"code","c8aaba5a":"code","beb6bcb0":"code","32b3e349":"code","3778bd86":"code","04d143f6":"code","5d545c3f":"code","11f9bd46":"code","547bccd2":"code","88d9446d":"code","d2fc54de":"code","e6d255ad":"code","d23a1355":"code","469751bd":"code","22c81507":"code","25068282":"code","4049d3fc":"code","04d6521b":"code","e8e70159":"code","205daaaa":"code","6db495cb":"code","d7dd6e08":"code","eb847ab2":"code","1666c252":"code","088aa767":"code","b22cf093":"code","187c150d":"code","325bf8f8":"code","1b71e583":"code","7afffe9e":"code","665e81a0":"code","5658ef10":"code","0fbe6bde":"code","e0a091af":"code","60db4352":"code","bd047e45":"code","e943d310":"code","6411e669":"code","d8be9b62":"code","9d170f87":"code","03560716":"code","f1fc6831":"code","546fa2e4":"code","d3497aaf":"code","10003265":"code","a58da315":"code","831cb70a":"code","52a3e14f":"code","c9fbfb68":"code","4cc6e9b8":"code","873e8d9a":"code","a7faf5cc":"code","c5ff688c":"code","27f2b4db":"code","c521ae8a":"code","759b39c9":"code","ff2ced44":"code","0e333f4d":"markdown","2417101d":"markdown","b24a0410":"markdown","0a510950":"markdown","9d30f010":"markdown","a0265010":"markdown","8436a7e4":"markdown","7b4ff642":"markdown","ce861211":"markdown","405d7e60":"markdown","54039aa1":"markdown","79f24f39":"markdown","2c12aaf8":"markdown","33312199":"markdown","9df5d9f4":"markdown","ef5b9d66":"markdown","b6054f21":"markdown","591f0775":"markdown","909eec61":"markdown","6d5a29cc":"markdown","6f9b373b":"markdown","169058c4":"markdown","956ea0ea":"markdown","a25bb73f":"markdown","51351ba3":"markdown","353a86fd":"markdown","acbc740e":"markdown","87f59a90":"markdown","612ef159":"markdown","db4288e3":"markdown","7c4028a2":"markdown","d5eec74d":"markdown","f10f88ef":"markdown","3872756f":"markdown","1908a96f":"markdown","4afc3d56":"markdown","e3b0095c":"markdown","4ee2650a":"markdown","b6f747c8":"markdown","97af1218":"markdown","465a4521":"markdown","c61d3e4a":"markdown","dc7b043f":"markdown","127bfebc":"markdown","d26b1d68":"markdown","edc8886f":"markdown","f63f8a3f":"markdown","0f075ccc":"markdown","abe9caf8":"markdown"},"source":{"f575cc34":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7cedfecd":"# Data Loading and Numerical Operations\nimport pandas as pd\nimport numpy as np\n# Data Visualizations\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n# Data Resampling\nfrom sklearn.utils import resample\n# Data Feature Selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n# Data Splitting\nfrom sklearn.model_selection import train_test_split\n# Data Scaling\nfrom sklearn.preprocessing import MinMaxScaler\n# Data Modeling\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, classification_report\n# Hyperparameter Tuning\nfrom sklearn.model_selection import RandomizedSearchCV","ee3b6c1f":"# Reading Dataset\ndata = pd.read_csv(\"..\/input\/heart-disease-prediction-using-logistic-regression\/framingham.csv\")\ndata.shape # Calculating the dimensions of the dataset","b6ef4c4c":"data.head(10)","07396f56":"data.info()","81b03aa1":"data.isnull().sum()","a734949c":"data.duplicated().sum()","2a6118a1":"print((data[\"glucose\"].mode())[0])","e741bde0":"data[\"glucose\"].fillna((data[\"glucose\"].mode())[0], inplace=True)","25d015e2":"data.dropna(inplace=True)\ndata.isnull().sum()","366fd175":"plt.figure(figsize=(20,10), facecolor='w')\nsns.boxplot(data=data)\nplt.show()","c8aaba5a":"print(data['totChol'].max())\nprint(data['sysBP'].max())","beb6bcb0":"data = data[data['totChol']<600.0]\ndata = data[data['sysBP']<295.0]\ndata.shape","32b3e349":"data.describe()","3778bd86":"#Checking relationship between variables\ncor=data.corr()\nplt.figure(figsize=(20,10), facecolor='w')\nsns.heatmap(cor,xticklabels=cor.columns,yticklabels=cor.columns,annot=True)\nplt.title(\"Correlation among all the Variables of the Dataset\", size=20)\ncor","04d143f6":"categorical_features = ['male', 'education', 'currentSmoker', 'BPMeds', 'prevalentStroke', 'prevalentHyp', 'diabetes']","5d545c3f":"for feature in categorical_features:\n    print(feature,':')\n    print(data[feature].value_counts())\n    print(\"-----------------\")","11f9bd46":"num_plots = len(categorical_features)\ntotal_cols = 2\ntotal_rows = num_plots\/\/total_cols + 1\nfig, axs = plt.subplots(nrows=total_rows, ncols=total_cols,\n                        figsize=(7*total_cols, 7*total_rows), facecolor='w', constrained_layout=True)\nfor i, var in enumerate(categorical_features):\n    row = i\/\/total_cols\n    pos = i % total_cols\n    plot = sns.countplot(x=var, data=data, ax=axs[row][pos])","547bccd2":"numeric_features = ['cigsPerDay', 'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']\nfor feature in numeric_features:\n    plt.figure(figsize=(18, 10), facecolor='w')\n    sns.distplot(data[feature])\n    plt.title('{} Distribution'.format(feature), fontsize=20)\n    plt.show()","88d9446d":"num_plots = len(numeric_features)\ntotal_cols = 2\ntotal_rows = num_plots\/\/total_cols + 1\ncolor = ['m', 'g', 'b', 'r', 'y', 'v', 'o']\nfig, axs = plt.subplots(nrows=total_rows, ncols=total_cols,\n                        figsize=(7*total_cols, 7*total_rows), facecolor='w', constrained_layout=True)\nfor i, var in enumerate(numeric_features):\n    row = i\/\/total_cols\n    pos = i % total_cols\n    plot = sns.violinplot(y=var, data=data, ax=axs[row][pos], linewidth=2)","d2fc54de":"#Distribution of outcome variable, Heart Disease\nplt.figure(figsize=(12, 10), facecolor='w')\nplt.subplots_adjust(right=1.5)\nplt.subplot(121)\nsns.countplot(x=\"TenYearCHD\", data=data)\nplt.title(\"Count distribution of TenYearCHD\", size=20)\nplt.subplot(122)\nlabels=[0,1]\nplt.pie(data[\"TenYearCHD\"].value_counts(),autopct=\"%1.1f%%\",labels=labels,colors=[\"lime\",\"red\"])\nplt.show()","e6d255ad":"#Grouping education and cigsPerDay\n\ngraph_1 = data.groupby(\"education\", as_index=False).cigsPerDay.mean()","d23a1355":"plt.figure(figsize=(12,8), facecolor='w')\nsns.regplot(x=graph_1[\"education\"], y=graph_1[\"cigsPerDay\"])\nplt.title(\"Graph showing cigsPerDay in every level of education.\", size=20)\nplt.xlabel(\"education\", size=20)\nplt.ylabel(\"cigsPerDay\", size=20)\nplt.xticks(size=12)\nplt.yticks(size=12)","469751bd":"#checking for which gender has more risk of coronary heart disease CHD\n\ngraph_2 = data.groupby(\"male\", as_index=False).TenYearCHD.sum()","22c81507":"#Ploting the above values\n\nplt.figure(figsize=(12,8), facecolor='w')\nsns.barplot(x=graph_2[\"male\"], y=graph_2[\"TenYearCHD\"])\nplt.title(\"Graph showing which gender has more risk of coronary heart disease CHD\", size=20)\nplt.xlabel(\"Gender\\n0 is female and 1 is male\",size=20)\nplt.ylabel(\"TenYearCHD cases\", size=20)\nplt.xticks(size=12)\nplt.yticks(size=12)","25068282":"#Distribution of current smokers with respect to age\nplt.figure(figsize=(30,15), facecolor='w')\nsns.countplot(x=\"age\",data=data,hue=\"currentSmoker\")\nplt.title(\"Graph showing which age group has more smokers.\", size=30)\nplt.xlabel(\"age\", size=20)\nplt.ylabel(\"age Count\", size=20)\nplt.xticks(size=12)\nplt.yticks(size=12)","4049d3fc":"plt.figure(figsize=(30,12), facecolor='w')\nsns.countplot(x=\"TenYearCHD\",data=data,hue=\"cigsPerDay\")\nplt.legend(title='cigsPerDay', fontsize='large')\nplt.title(\"Graph showing the relation between cigsPerDay and risk of coronary heart disease.\", size=30)\nplt.xlabel(\"Risk of TenYearCHD\", size=20)\nplt.ylabel(\"Count of TenYearCHD\", size=20)\nplt.xticks(size=12)\nplt.yticks(size=12)\nplt.show()","04d6521b":"# Grouping up the data and ploting it\n\ngraph_3 = data.groupby(\"TenYearCHD\", as_index=False).sysBP.mean()\n\nplt.figure(figsize=(12,8), facecolor='w')\nsns.barplot(x=graph_3[\"TenYearCHD\"], y=graph_3[\"sysBP\"])\nplt.title(\"Graph showing the relation between sysBP and risk of CHD\", size=20)\nplt.xlabel(\"Risk of CHD\", size=20)\nplt.ylabel(\"sysBP\", size=20)\nplt.xticks(size=12)\nplt.yticks(size=12)","e8e70159":"plt.figure(figsize=(12,8), facecolor='w')\nsns.regplot(x=graph_3[\"TenYearCHD\"], y=graph_3[\"sysBP\"])\nplt.title(\"Distribution of sysBP in relation to the risk of CHD\", size=20)\nplt.xticks(size=12)\nplt.yticks(size=12)","205daaaa":"# Grouping up the data and ploting it\n\ngraph_4 = data.groupby(\"TenYearCHD\", as_index=False).diaBP.mean()\n\nplt.figure(figsize=(12,8), facecolor='w')\nsns.barplot(x=graph_4[\"TenYearCHD\"], y=graph_4[\"diaBP\"])\nplt.title(\"Graph showing the relation between diaBP and risk of CHD\", size=20)\nplt.xlabel(\"Risk of CHD\", size=20)\nplt.ylabel(\"diaBP\", size=20)\nplt.xticks(size=12)\nplt.yticks(size=12)","6db495cb":"plt.figure(figsize=(12,8), facecolor='w')\nsns.regplot(x=graph_4[\"TenYearCHD\"], y=graph_4[\"diaBP\"])\nplt.title(\"Distribution of diaBP in relation to the risk of CHD\", size=20)\nplt.xticks(size=12)\nplt.yticks(size=12)","d7dd6e08":"plt.figure(figsize=(20,10), facecolor='w')\nsns.boxplot(x=\"age\",y=\"totChol\",data=data)\nplt.title(\"Distribution of age with respect to totChol\", size=20)\nplt.show()","eb847ab2":"#Plotting a linegraph to check the relationship between age and cigsPerDay, totChol, glucose.\n\ngraph_5 = data.groupby(\"age\").cigsPerDay.mean()\ngraph_6 = data.groupby(\"age\").totChol.mean()\ngraph_7 = data.groupby(\"age\").glucose.mean()\n\nplt.figure(figsize=(16,10), facecolor='w')\nsns.lineplot(data=graph_5, label=\"cigsPerDay\")\nsns.lineplot(data=graph_6, label=\"totChol\")\nsns.lineplot(data=graph_7, label=\"glucose\")\nplt.title(\"Graph showing totChol and cigsPerDay in every age group.\", size=20)\nplt.xlabel(\"age\", size=20)\nplt.ylabel(\"count\", size=20)\nplt.xticks(size=12)\nplt.yticks(size=12)","1666c252":"#sysBP vs diaBP with respect to currentSmoker and male attributes\n#plt.figure(figsize=(18, 9), facecolor='w')\nsns.lmplot('sysBP', 'diaBP', \n           data=data,\n           hue=\"TenYearCHD\",\n           col=\"male\",row=\"currentSmoker\")\nplt.show()","088aa767":"target1=data[data['TenYearCHD']==1]\ntarget0=data[data['TenYearCHD']==0]","b22cf093":"target1=resample(target1,replace=True,n_samples=len(target0),random_state=40)","187c150d":"target=pd.concat([target0,target1])","325bf8f8":"target['TenYearCHD'].value_counts() ","1b71e583":"data=target\nnp.shape(data)","7afffe9e":"#Distribution of heart disease cases in the balanced dataset, the outcome variable\nplt.figure(figsize=(12, 10), facecolor='w')\nplt.subplots_adjust(right=1.5)\nplt.subplot(121)\nsns.countplot(x=\"TenYearCHD\", data=data)\nplt.title(\"Count of TenYearCHD column\", size=20)\nplt.subplot(122)\nlabels=[0,1]\nplt.pie(data[\"TenYearCHD\"].value_counts(),autopct=\"%1.1f%%\",labels=labels,colors=[\"red\",\"lime\"])\nplt.show()","665e81a0":"#To idenfify the features that have larger contribution towards the outcome variable, TenYearCHD\nX=data.iloc[:,0:15]\ny=data.iloc[:,-1]\nprint(\"X - \", X.shape, \"\\ny - \", y.shape)","5658ef10":"#Apply SelectKBest and extract top 10 features\nbest=SelectKBest(score_func=chi2, k=10)","0fbe6bde":"fit=best.fit(X,y)","e0a091af":"data_scores=pd.DataFrame(fit.scores_)\ndata_columns=pd.DataFrame(X.columns)","60db4352":"#Join the two dataframes\nscores=pd.concat([data_columns,data_scores],axis=1)\nscores.columns=['Feature','Score']\nprint(scores.nlargest(11,'Score'))","bd047e45":"#To visualize feature selection\nscores=scores.sort_values(by=\"Score\", ascending=False)\nplt.figure(figsize=(20,7), facecolor='w')\nsns.barplot(x='Feature',y='Score',data=scores,palette='BuGn_r')\nplt.title(\"Plot showing the best features in descending order\", size=20)\nplt.show()","e943d310":"#Select 10 features\nfeatures=scores[\"Feature\"].tolist()[:10]\nfeatures","6411e669":"data=data[['sysBP','glucose','age','cigsPerDay','totChol','diaBP','prevalentHyp','male','BPMeds','diabetes','TenYearCHD']]\ndata.head()","d8be9b62":"y = data['TenYearCHD']\nX = data.drop(['TenYearCHD'], axis=1)\ntrain_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.4, random_state=1)","9d170f87":"scaler = MinMaxScaler()\ntrain_x = scaler.fit_transform(train_x)\ntest_x = scaler.transform(test_x)","03560716":"m1 = 'LogisticRegression'\nlr = LogisticRegression(random_state=1, max_iter=1000)\nmodel = lr.fit(train_x, train_y)\nlr_predict = lr.predict(test_x)\nlr_conf_matrix = confusion_matrix(test_y, lr_predict)\nlr_acc_score = accuracy_score(test_y, lr_predict)\nprint(\"confusion matrix\")\nprint(lr_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Logistic Regression:\",lr_acc_score*100,'\\n')\nprint(classification_report(test_y,lr_predict))","f1fc6831":"m2 = 'KNeighborsClassifier'\nknn = KNeighborsClassifier(n_neighbors=1)\nmodel = knn.fit(train_x, train_y)\nknn_predict = knn.predict(test_x)\nknn_conf_matrix = confusion_matrix(test_y, knn_predict)\nknn_acc_score = accuracy_score(test_y, knn_predict)\nprint(\"confusion matrix\")\nprint(knn_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of k-NN Classification:\",knn_acc_score*100,'\\n')\nprint(classification_report(test_y, knn_predict))","546fa2e4":"m3 = 'Random Forest Classfier'\nrf = RandomForestClassifier(n_estimators=200, random_state=0,max_depth=12)\nrf.fit(train_x,train_y)\nrf_predicted = rf.predict(test_x)\nrf_conf_matrix = confusion_matrix(test_y, rf_predicted)\nrf_acc_score = accuracy_score(test_y, rf_predicted)\nprint(\"confusion matrix\")\nprint(rf_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Random Forest:\",rf_acc_score*100,'\\n')\nprint(classification_report(test_y,rf_predicted))","d3497aaf":"m4 = 'DecisionTreeClassifier'\ndt = DecisionTreeClassifier(criterion = 'entropy',random_state=0,max_depth = 30)\ndt.fit(train_x,train_y)\ndt_predicted = dt.predict(test_x)\ndt_conf_matrix = confusion_matrix(test_y, dt_predicted)\ndt_acc_score = accuracy_score(test_y, dt_predicted)\nprint(\"confusion matrix\")\nprint(dt_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of DecisionTreeClassifier:\",dt_acc_score*100,'\\n')\nprint(classification_report(test_y,dt_predicted))","10003265":"m5 = 'Gradient Boosting Classifier'\ngvc =  GradientBoostingClassifier()\ngvc.fit(train_x,train_y)\ngvc_predicted = gvc.predict(test_x)\ngvc_conf_matrix = confusion_matrix(test_y, gvc_predicted)\ngvc_acc_score = accuracy_score(test_y, gvc_predicted)\nprint(\"confusion matrix\")\nprint(gvc_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Gradient Boosting Classifier:\",gvc_acc_score*100,'\\n')\nprint(classification_report(test_y,gvc_predicted))","a58da315":"m6 = 'Support Vector Classifier'\nsvc = SVC(kernel = 'linear')\nsvc.fit(train_x,train_y)\nsvc_predicted = svc.predict(test_x)\nsvc_conf_matrix = confusion_matrix(test_y, svc_predicted)\nsvc_acc_score = accuracy_score(test_y, svc_predicted)\nprint(\"confusion matrix\")\nprint(svc_conf_matrix)\nprint(\"\\n\")\nprint(\"Support Vector Classifier:\",svc_acc_score*100,'\\n')\nprint(classification_report(test_y,svc_predicted))\n\n# kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n","831cb70a":"m7 = 'Naive Bayes Classifier'\nnbc = GaussianNB()\nnbc.fit(train_x,train_y)\nnbc_predicted = nbc.predict(test_x)\nnbc_conf_matrix = confusion_matrix(test_y, nbc_predicted)\nnbc_acc_score = accuracy_score(test_y, nbc_predicted)\nprint(\"confusion matrix\")\nprint(nbc_conf_matrix)\nprint(\"\\n\")\nprint(\"Naive Bayes Classifier:\",nbc_acc_score*100,'\\n')\nprint(classification_report(test_y,nbc_predicted))\n","52a3e14f":"m8 = 'LGBMClassifier'\nlg=LGBMClassifier(boosting_type='gbdt',n_estimators=5000,learning_rate=0.05,objective='binary',metric='accuracy',is_unbalance=True,\n                 colsample_bytree=0.7,reg_lambda=3,reg_alpha=3,random_state=500,n_jobs=-1,num_leaves=35)\nlg.fit(train_x,train_y)\nlg_predicted = lg.predict(test_x)\nlg_conf_matrix = confusion_matrix(test_y, lg_predicted)\nlg_acc_score = accuracy_score(test_y, lg_predicted)\nprint(\"confusion matrix\")\nprint(lg_conf_matrix)\nprint(\"\\n\")\nprint(\"LGBMClassifier:\",lg_acc_score*100,'\\n')\nprint(classification_report(test_y,lg_predicted))\n","c9fbfb68":"m9 = 'XGBClassifier'\nxg = XGBClassifier(learning_rate=0.05, n_estimators=100,max_depth=4, subsample = 0.9,colsample_bytree = 0.1, gamma=1,random_state=42)\nxg.fit(train_x,train_y)\nxg_predicted = xg.predict(test_x)\nxg_conf_matrix = confusion_matrix(test_y, xg_predicted)\nxg_acc_score = accuracy_score(test_y, xg_predicted)\nprint(\"confusion matrix\")\nprint(xg_conf_matrix)\nprint(\"\\n\")\nprint(\"XGBClassifier:\",xg_acc_score*100,'\\n')\nprint(classification_report(test_y,xg_predicted))\n","4cc6e9b8":"m10 = 'MLPClassifier'\nmlp=MLPClassifier(solver='adam', learning_rate_init = 0.0005, learning_rate = 'adaptive', activation=\"relu\", max_iter=3000, random_state=10)\nmlp.fit(train_x,train_y)\nmlp_predicted = mlp.predict(test_x)\nmlp_conf_matrix = confusion_matrix(test_y, mlp_predicted)\nmlp_acc_score = accuracy_score(test_y, mlp_predicted)\nprint(\"confusion matrix\")\nprint(mlp_conf_matrix)\nprint(\"\\n\")\nprint(\"MLPClassifier:\",mlp_acc_score*100,'\\n')\nprint(classification_report(test_y,mlp_predicted))\n","873e8d9a":"lr_false_positive_rate,lr_true_positive_rate,lr_threshold = roc_curve(test_y,lr_predict)\nknn_false_positive_rate,knn_true_positive_rate,knn_threshold = roc_curve(test_y,knn_predict)\nrf_false_positive_rate,rf_true_positive_rate,rf_threshold = roc_curve(test_y,rf_predicted)                                                             \ndt_false_positive_rate,dt_true_positive_rate,dt_threshold = roc_curve(test_y,dt_predicted)\ngvc_false_positive_rate,gvc_true_positive_rate,gvc_threshold = roc_curve(test_y,gvc_predicted)\nsvc_false_positive_rate,svc_true_positive_rate,svc_threshold = roc_curve(test_y,svc_predicted)\nnbc_false_positive_rate,nbc_true_positive_rate,nbc_threshold = roc_curve(test_y,nbc_predicted)\nlg_false_positive_rate,lg_true_positive_rate,lg_threshold = roc_curve(test_y,lg_predicted)\nxg_false_positive_rate,xg_true_positive_rate,xg_threshold = roc_curve(test_y,xg_predicted)\nmlp_false_positive_rate,mlp_true_positive_rate,mlp_threshold = roc_curve(test_y,mlp_predicted)\n\n\n\n\nsns.set_style('whitegrid')\nplt.figure(figsize=(15,8), facecolor='w')\nplt.title('Reciever Operating Characterstic Curve')\nplt.plot(lr_false_positive_rate,lr_true_positive_rate,label='Logistic Regression')\nplt.plot(knn_false_positive_rate,knn_true_positive_rate,label='K-Nearest Neighbor')\nplt.plot(rf_false_positive_rate,rf_true_positive_rate,label='Random Forest')\nplt.plot(dt_false_positive_rate,dt_true_positive_rate,label='Desion Tree')\nplt.plot(gvc_false_positive_rate,gvc_true_positive_rate,label='Gradient Boosting Classifier')\nplt.plot(svc_false_positive_rate,svc_true_positive_rate,label='Support Vector Classifier')\nplt.plot(nbc_false_positive_rate,nbc_true_positive_rate,label='Naive Bayes Classifier')\nplt.plot(lg_false_positive_rate,lg_true_positive_rate,label='LGBMClassifier')\nplt.plot(xg_false_positive_rate,xg_true_positive_rate,label='XGBClassifier')\nplt.plot(mlp_false_positive_rate,mlp_true_positive_rate,label='MLPClassifier')\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.legend()\nplt.show()","a7faf5cc":"model_ev = pd.DataFrame({'Model': ['Logistic Regression','KNN','Random Forest',\n                                   'Decision Tree','Gradient Boosting','SVC','Naive Bayes','LGBMClassifier','XGBClassifier','MLPClassifier'], \n                         'Accuracy': [lr_acc_score*100, knn_acc_score*100, rf_acc_score*100, dt_acc_score*100,gvc_acc_score*100,\n                                      svc_acc_score*100,nbc_acc_score*100,lg_acc_score*100,xg_acc_score*100,mlp_acc_score*100]})\nmodel_ev","c5ff688c":"colors = ['red','green','blue','gold','purple','silver','orange','brown','lightblue']\nplt.figure(figsize=(15,8), facecolor='w')\nplt.title(\"Barplot Representing Accuracy of different models\")\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Models\")\nplt.bar(model_ev['Model'],model_ev['Accuracy'],color = colors)\nplt.show()","27f2b4db":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","c521ae8a":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestClassifier()\n\n# Random search of parameters, using 3 fold cross validation, \n# search across 20 different combinations\nrf_random = RandomizedSearchCV(estimator = rf, \n                               param_distributions = random_grid, \n                               n_iter = 20, \n                               cv = 3, \n                               verbose=2, \n                               random_state=7, \n                               n_jobs = -1)\n\n# Fit the random search model\nrf_random.fit(train_x,train_y)","759b39c9":"rf_random.best_estimator_","ff2ced44":"model = RandomForestClassifier(bootstrap=False, max_depth=80, max_features='sqrt',\n                       n_estimators=1400)\n\nmodel.fit(train_x, train_y)\n\npredicted = model.predict(test_x)\n\nscore = model.score(test_x, test_y)\n\nprint(\"The accuracy of our model is \", score)\nprint(\"\\nPrediction:\",predicted)","0e333f4d":"## Final Random Forest Classifier Model","2417101d":"Only these features have strongest influence over the target variable. They are, in particular order:\n* sysBP\n* glucose\n* age\n* cigsPerDay\n* totChol\n* diaBP\n* prevalentHyp\n* male\n* BPMeds\n* diabetes\n","b24a0410":"For the same numerical features:\n* `cigsPerDay` has uneven distribution although most of the data is concentrated on `0`\n* The majority portions of the following columns lie in the range: \n    * `totChol`: 150 to 300\n    * `sysBP`: 100 to 150\n    * `diaBP`: 60 to 100\n    * `BMI`: 20 to 30\n    * `heartRate`: 50 to 100\n    * `glucose`: 50 to 150","0a510950":"#### Relationship between age and cigsPerDay, totChol, glucose.","9d30f010":"* There is no linear relationship observed.\n* Level 3 `education` shows the lowest mean.","a0265010":"The number of positive and negative cases are equal. Hence the classes are now balanced for model fitting","8436a7e4":"#### Relation between age and totChol","7b4ff642":"### Random Forest Classifier","ce861211":"### 1. Univariate Analysis\n#### Categorical Features","405d7e60":"#### Which age group has more smokers.","54039aa1":"Among the categorical features:\n* `BPmeds`, `prevalentStroke` and `diabetes` are highly imbalanced.\n* There are four levels of `education` whereas the rest categorical features are all binary\n* The number of Smokers and non-Smokers in `currentSmoker` is almost the same","79f24f39":"According to this dataset, `males` have shown a slighly higher risk of coronary heart disease `TenYearCHD`.","2c12aaf8":"* Low `cigsPerDay` comes with lower risk of CHD.\n* Those who don't smoke, i.e., with a `cigsPerDay` of 0.0 has a really low risk of contracting the disease\n* Although that is the case, low `cigsPerDay` doesn't actually guarantee a much lower risk of CHD","33312199":"Filling the missing spaces of `glucose`column with the mode of the data (Mode = 75) present to reduce the number of missing data in our dataset","9df5d9f4":"### 2. Bivariate Analysis\n\n#### Relationship between education and cigsPerDay\n","ef5b9d66":"Among the numerical features:\n* `totChol`, `sysBP`, `diaBP`and `BMI` has an uniform distribution and the rest are unevenly distributed\n* `cigsPerDay` has a highly uneven distribution with the most data present in 0 \n* `cigsPerDay` and `sysBP` shows quite a bit and slight right skewness respectively.","b6054f21":"# Predictive Modeling","591f0775":"It shows the number of np.nan or null values or missing values are present in the dataset:\n* education: 105\n* cigsPerDay: 29\n* BPMeds: 53\n* totChol: 50\n* BMI: 19\n* heartRate: 1\n* glucose: 388","909eec61":"The above graph plots the relationship between systolic blood pressure and diastolic blood pressure for patients based on their gender and whether they are current smokers or not and plots the best fit line","6d5a29cc":"### ROC Curve To Compare All Models","6f9b373b":"#### Relation between sysBP and risk of CHD.","169058c4":"The distribution is highly imbalanced. As in, the number of negative cases outweigh the number of positive cases. This would lead to class imbalance problem while fitting our models. Therefore, this problem needs to be addressed and taken care of.","956ea0ea":"An overall Statistical Information is shown\n* It is clearly evident that none of the data is missing in columns.\n* It also shows the mean, standard deviation and other statistical metrices of the dataset \n* It also shows the categorical data of the dataset since they were already converted into discrete numerical values","a25bb73f":"### Resampling imbalanced dataset by oversampling positive cases","51351ba3":"#### Relation between diaBP and risk of CHD","353a86fd":"* Minor relation found between higher risk of `TenYearCHD` with higher `diaBP` similar to the previous one\n* Majority of people with `diaBP` ranging upto 80.0 has lower chance of contracting the disease.","acbc740e":"### 3. Multivariate Analysis","87f59a90":"We use the following different machine learning models for the dataset:\n\n1. Logistic Regressor\n2. K-Nearest Neighbour Classifier\n3. Random Forest Classifier\n4. Decision Tree Classifier\n5. Gradient Boosting Classifier\n6. Support Vector Classifier\n7. Naive Bayes Classifier\n8. LGBMClassifier\n9. XGBClassifier\n10. MLPClassifier","612ef159":"This plot shows the `Features` and their respective `chi-square test` scores","db4288e3":"* There is a minor relation between `totChol` and `glucose`.\n* `totChol` has a steep, linear and inverse graph for lower ranges of `age`\n* `cigsPerDay` has a fairly parallel relationship with `age`","7c4028a2":"### **Model Evaluation**","d5eec74d":"### Feature Selection","f10f88ef":"We divide the dataset into training and test sub-datasets for predictive modeling","3872756f":"* Mid-age groups ranging from the age of 38 - 46 have more number of `currentSmokers`\n* No `currentSmokers` observed below the `age` of 32 \n* maximum age for a `currentSmokers` is 70 ","1908a96f":"## Finding Best Hyperparameters for Random Forest Model\n#### Using Randomized Search Cross Validation\n","4afc3d56":"#### Target Variable","e3b0095c":"### **Conclusions**\nRandom Forest Classifier Model performs best among all models","4ee2650a":"#### Distribution of sysBP vs diaBP with respect to currentSmoker and male attributes","b6f747c8":"* Minor relation of higher risk of `TenYearCHD` found with higher `sysBP`\n* Majority of people with `sysBP` ranging from 72 - 130 has lower chance of contracting the disease.","97af1218":"### Feature Splitting and Scaling","465a4521":"#### Relation between cigsPerDay and risk of coronary heart disease.","c61d3e4a":"#### Which gender has more risk of coronary heart disease CHD","dc7b043f":"Compared to all the independent data, the correlation coefficient between education and and target variable TenYearCHD is very low and actually negative.","127bfebc":"The Outlier present in `totChol` is 600.\n\nThe Outlier present in `sysBP` is 295.\n","d26b1d68":"Now with the missing values, outliers and duplicate values dealt with, we proceed to perform EDA","edc8886f":"#### Numerical Features","f63f8a3f":"The boxplots are shifted in an upwardly manner suggesting that aged people have more cholesterol (bad cholesterol in general)","0f075ccc":"### Objective: To build a classification model that predicts heart disease. (note the target column to predict is 'TenYearCHD' where CHD = Coronary heart disease) \n\n### We are going to perform the following steps: \n\n1. Read the file and display columns.\n2. Handle missing values, Outliers and Duplicate Data\n3. Calculate basic statistics of the data (count, mean, std, etc) and exploratory analysts and describe observations.\n4. Select columns that will be probably important to predict heart disease.\n5. Create training and testing sets (use 60% of the data for the training and reminder for testing).\n6. Build a machine learning model to predict TenYearCHD\nEvaluate the model (f1 score, Accuracy, Precision ,Recall and Confusion Matrix)\n7. Find Best Model (Model which is giving best f1 score,roc curve and best test score).\n8. Finding Best Hyperparameters for Best Model.\n9. Conclude final model.\n\n","abe9caf8":"Removable Outliers are detected in `totChol` and `sysBP` columns of our dataset. Outliers in all other numerical columns are important and thus cannot be removed."}}