{"cell_type":{"7dfd1820":"code","6cc1d192":"code","6cc73f99":"code","b49a3f14":"code","7aff0679":"code","15bd3569":"code","95062d32":"code","9d4a3c70":"code","3a736729":"code","3021e02b":"code","1bf8c2a2":"code","6d8047f9":"code","190ba595":"code","6d9fbc83":"code","6dbedb28":"code","a07da3ca":"code","02748148":"code","7284346e":"code","8923a7b8":"code","72abdecb":"code","19f4bc97":"code","5e2316d9":"code","bffdc002":"code","bcc2815c":"code","add139b9":"code","156f1db1":"code","d0dff4b8":"code","28db60de":"code","1d7222f1":"code","723820bc":"code","5f2fcd43":"code","512b6eb4":"code","217cacfc":"code","7761c87b":"code","d3393083":"code","35df5539":"code","4896655f":"code","53171975":"code","bddccc2b":"code","0d744c5c":"code","1d8a478e":"code","3a9ffaf6":"code","954b0545":"code","251da642":"code","9e9a4bef":"code","5e7c8231":"code","64a3a262":"code","5d8dc8f8":"code","3c4d1b77":"code","946f8260":"code","2131f740":"code","77034fec":"code","6433c55c":"code","8bcfde5e":"markdown","9c402838":"markdown","d84aa22c":"markdown","61e73fbd":"markdown","42a9f67b":"markdown","868b5d95":"markdown","2ecf0296":"markdown","79dff8c2":"markdown","9475d962":"markdown","3d09df39":"markdown","9a987af4":"markdown","ae8fd869":"markdown","16738631":"markdown","24bb97cb":"markdown","a0787fc6":"markdown","89e9b078":"markdown","a5a87ec6":"markdown","9f435790":"markdown","0cfbcceb":"markdown","175b64b5":"markdown","424d9760":"markdown","1472a664":"markdown","3c5e1f60":"markdown","8c165ff1":"markdown","064ad0b7":"markdown","b890aad0":"markdown","d74aae60":"markdown","5e813a83":"markdown","efc935ba":"markdown","bc3330df":"markdown","ed02d1ee":"markdown","bce9496a":"markdown","65b4de16":"markdown","7812d666":"markdown","378f581d":"markdown","4baee81f":"markdown","74e821cd":"markdown"},"source":{"7dfd1820":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import auc\nfrom sklearn.svm import SVC\nimport warnings \nwarnings.filterwarnings(\"ignore\")","6cc1d192":"df=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndt=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf.head(5)","6cc73f99":"df.info()","b49a3f14":"df.describe()","7aff0679":"categorical_features = df.select_dtypes(include=['object']).columns\nprint('Categorical')\nprint(categorical_features)\nprint('Numerical')\nnumerical_features = df.select_dtypes(exclude = [\"object\"]).columns\nprint(numerical_features)\ndf_num=df[numerical_features]\ndf_cat=df[categorical_features]","15bd3569":"sns.distplot(df.skew(),color='red',axlabel ='Skewness')","95062d32":"plt.figure(figsize = (12,8))\nsns.distplot(df.kurt(),color='blue',axlabel ='Kurtosis',norm_hist= False, kde = True,rug = False)\nplt.show()","9d4a3c70":"%matplotlib inline\nimport scipy.stats as st\ny=df['SalePrice']\nplt.style.use('fivethirtyeight')\nplt.figure(1);\nplt.title('johnson su')\nsns.distplot(y,kde=False,fit=st.johnsonsu)\nplt.figure(2);\nplt.title('normal distrbutation')\nsns.distplot(y,kde=False,fit=st.norm)\nplt.figure(3);\nplt.title('log normal')\nsns.distplot(y,kde=False,fit=st.lognorm)","3a736729":"import seaborn as sns\nplt.figure(figsize=(12,12), dpi= 80)\nsns.heatmap(df.corr(), cmap='RdYlGn', center=0)\n\n# Decorations\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()","3021e02b":"df_corr=df.corr()['SalePrice'][:-1]\nfeature_list=df_corr[abs(df_corr)>0.5].sort_values(ascending=False)\nfeature_list","1bf8c2a2":"year_feature = [feature for feature in df_num if 'Yr' in feature or 'Year' in feature]\ndiscrete_feature=[feature for feature in df_num if len(df[feature].unique())<25 and feature not in year_feature+['Id']]\nfor feature in discrete_feature:\n    data=df.copy()\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()","6d8047f9":"continuous_feature=[feature for feature in df_num if feature not in discrete_feature+year_feature+['Id']]\nfor feature in continuous_feature:\n    data=df.copy()\n    data[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()","190ba595":"dy=pd.DataFrame(df.groupby('YrSold')['SalePrice'].mean().reset_index().values,\n                    columns=[\"YrSold\",\"SalePrice\"])\nfig = go.Figure(layout={'title':\"Average SalePrice over the Year Sold\",'xaxis':{'title':\"Year Sold\"}\n                        ,'yaxis':{'title':\"Average SalePrice\"}})\n# Add traces\nfig.add_trace(go.Bar(x=dy.YrSold, y=dy.SalePrice,marker=dict(color=\"blue\")))\nfig.show()","6d9fbc83":"dy=pd.DataFrame(df.groupby('MoSold')['SalePrice'].mean().reset_index().values,\n                    columns=[\"MoSold\",\"SalePrice\"])\nfig = go.Figure(layout={'title':\"Average SalePrice over the Month Sold\",'xaxis':{'title':\"Month Sold\"}\n                        ,'yaxis':{'title':\"Average SalePrice\"}})\n# Add traces\nfig.add_trace(go.Scatter(x=dy.MoSold, y=dy.SalePrice))\nfig.show()","6dbedb28":"dy=pd.DataFrame(df.groupby('SaleType')['SalePrice'].mean().reset_index().values,\n                    columns=[\"SaleType\",\"SalePrice\"])\nfig = go.Figure(layout={'title':\"Average SalePrice with Respect to SaleType\",'xaxis':{'title':\"Sale Type\"}\n                        ,'yaxis':{'title':\"Average SalePrice\"}})\n\n# Add traces\nfig.add_trace(go.Bar(x=dy.SaleType, y=dy.SalePrice,marker=dict(color=\"brown\")))\nfig.show()","a07da3ca":"dy=pd.DataFrame(df.groupby('YearBuilt')['SalePrice'].mean().reset_index().values,\n                    columns=[\"YearBuilt\",\"SalePrice\"])\nfig = go.Figure(layout={'title':\"Average SalePrice with Respect to Year Built\",'xaxis':{'title':\"Year Built\"}\n                        ,'yaxis':{'title':\"Average SalePrice\"}})\n\n# Add traces\nfig.add_trace(go.Scatter(x=dy.YearBuilt, y=dy.SalePrice,mode='lines+markers',marker=dict(color=\"red\")))\nfig.show()","02748148":"dy=pd.DataFrame(df.groupby('Neighborhood')['SalePrice'].mean().reset_index().values,\n                    columns=[\"Neighborhood\",\"SalePrice\"])\nfig = go.Figure(layout={'title':\"Average SalePrice over the Neighborhood\",'xaxis':{'title':\"Neighborhood\"}\n                        ,'yaxis':{'title':\"Average SalePrice\"},'xaxis_tickangle':-45})\n# Add traces\nfig.add_trace(go.Scatter(x=dy.Neighborhood, y=dy.SalePrice,mode='lines+markers'))\nfig.show()","7284346e":"dy=pd.DataFrame(df.groupby('HouseStyle')['SalePrice'].mean().reset_index().values,\n                    columns=[\"HouseStyle\",\"SalePrice\"])\nfig = go.Figure(layout={'title':\"Average SalePrice with Respect to House Style\",'xaxis':{'title':\"House Style\"}\n                        ,'yaxis':{'title':\"Average SalePrice\"}})\n\n# Add traces\nfig.add_trace(go.Bar(x=dy.HouseStyle, y=dy.SalePrice,marker=dict(color=\"green\")))\nfig.show()","8923a7b8":"dy=pd.DataFrame(df.groupby('SaleCondition')['SalePrice'].mean().reset_index().values,\n                    columns=[\"SaleCondition\",\"SalePrice\"])\nfig = go.Figure(layout={'title':\"Average SalePrice with Respect to SaleType\",'xaxis':{'title':\"Sale Condition\"}\n                        ,'yaxis':{'title':\"Average SalePrice\"}})\n\n# Add traces\nfig.add_trace(go.Bar(x=dy.SaleCondition, y=dy.SalePrice,marker=dict(color=\"black\")))\nfig.show()","72abdecb":"df['saletype'] = 'Low Range'\ndf.loc[(df['SalePrice'] >= 143000) & (df['SalePrice'] <= 254000), 'saletype'] = 'Medium Range'\ndf.loc[(df['SalePrice'] > 254000), 'saletype'] = 'High Range'\n\ndf_flight = pd.DataFrame(df['saletype'].value_counts().reset_index().values, columns=[\"saletype\", \"AggregateType\"])\nlabels = [\"Low Range\",\"Medium Range\",\"High Range\"]\nvalue = [df_flight['AggregateType'][0],df_flight['AggregateType'][1],df_flight['AggregateType'][2]]\n# colors=['lightcyan','cyan','royalblue']\nfigs = go.Figure(data=[go.Pie(labels=labels, values=value, pull=[0, 0, 0.2],textinfo = 'label+percent', hole = 0.35, \n                              hoverinfo=\"label+percent\")],layout={'title':\"SalePrice by Range\",\n                                                'annotations':[dict(text='<b>Saleprice<b>', x=0.5, y=0.5, font_size=11, showarrow=False)]})\nfigs.update_traces( textinfo='label + percent', textfont_size=10)\nfigs.show()","19f4bc97":"from scipy import stats\nfrom scipy.stats import norm, skew\nfig = plt.figure()\nres = stats.probplot(df['SalePrice'], plot=plt)\nplt.show()","5e2316d9":"x=df\nx['SalePrice']=np.log1p(x['SalePrice'])\nfig = plt.figure()\nres = stats.probplot(x['SalePrice'], plot=plt)\nplt.show()","bffdc002":"%matplotlib inline\ntarget=np.log1p(df[\"SalePrice\"])\nplt.figure(4);\nplt.scatter(x=df['GarageArea'],y=target)","bcc2815c":"df=df[df['GarageArea']<1200]\ntarget=np.log1p(df[\"SalePrice\"])\nplt.figure(9);\nplt.scatter(x=df['GarageArea'],y=target)\nplt.xlim(-200,1600)","add139b9":"target=np.log1p(df[\"SalePrice\"])\nplt.figure(5);\nplt.scatter(x=df['GrLivArea'],y=target)\nplt.figure(6);\nplt.scatter(x=df['YearBuilt'],y=target)\nplt.figure(7);\nplt.scatter(x=df['TotalBsmtSF'],y=target)\nplt.figure(8);\nplt.scatter(x=df['1stFlrSF'],y=target)\n","156f1db1":"import missingno as msno\nmsno.heatmap(df)","d0dff4b8":"total=df.isnull().sum().sort_values(ascending=False)\npercent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","28db60de":"df.shape","1d7222f1":"data=pd.concat((df.loc[:,'MSSubClass':'SaleCondition'],\n                dt.loc[:,'MSSubClass':'SaleCondition']))","723820bc":"var1=['Exterior1st','Exterior2nd','SaleType','Electrical','KitchenQual']\nfor v in var1:\n  data[v]=data[v].fillna(data[v].mode()[0])\ndata['MSZoning'] = data.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\nfor col in ['GarageType','GarageFinish','GarageQual','GarageCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\"PoolQC\",'Alley','Fence','MiscFeature','FireplaceQu','MasVnrType','Utilities']:\n  data[col]=data[col].fillna('None')\n\nfor col in ['GarageYrBlt','GarageArea','GarageCars','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BsmtUnfSF','TotalBsmtSF']:\n  data[col]=data[col].fillna(0)\n\ndata['LotFrontage'] = data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\ndata['Functional']=data['Functional'].fillna('Typ')\n\ndata=pd.get_dummies(data)\ndata.shape","5f2fcd43":"print(data.isnull().sum().sum())\ndata.head(3)\n","512b6eb4":"for c in df_cat:\n    df[c] = df[c].astype('category')\n    if df[c].isnull().any():\n        df[c] = df[c].cat.add_categories(['MISSING'])\n        df[c] = df[c].fillna('MISSING')\n\ndef boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\nf = pd.melt(df, id_vars=['SalePrice'], value_vars=categorical_features)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False, size=5)\ng = g.map(boxplot, \"value\", \"SalePrice\")","217cacfc":"import numpy as np\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nmodel = smf.ols('SalePrice ~ MSSubClass+LotArea+OverallQual+OverallCond+YearBuilt+YearRemodAdd+BsmtFinSF1+BsmtFinSF2+BsmtUnfSF+TotalBsmtSF+LowQualFinSF+GrLivArea+BsmtFullBath+BsmtHalfBath+FullBath+HalfBath+BedroomAbvGr+KitchenAbvGr+TotRmsAbvGrd+Fireplaces+GarageCars+GarageArea+WoodDeckSF+OpenPorchSF+EnclosedPorch+ScreenPorch+PoolArea+MiscVal+MoSold+YrSold', data=df_num).fit()\nprint(model.summary()) ","7761c87b":"corr=df.corr()[\"SalePrice\"]\ncorr[np.argsort(corr, axis=0)[::-1]]","d3393083":"#creating matrices for sklearn:\nX = data[:df.shape[0]]\ny = df.SalePrice\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)","35df5539":"import numpy as np\n\n\nfrom yellowbrick.model_selection import ValidationCurve\n\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Load a regression dataset\n\nviz = ValidationCurve(\n    DecisionTreeRegressor(), param_name=\"max_depth\",\n    param_range=np.arange(1, 11), cv=10, scoring=\"r2\"\n)\n\n# Fit and show the visualizer\nviz.fit(X, y)\nviz.show()","4896655f":"import xgboost as xgb\n# model_xgb = xgb.XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1) \nmodel_xgb = xgb.XGBRegressor(n_estimators=2000, max_depth=6, learning_rate=0.1, \n                             verbosity=1, silent=None, objective='reg:linear', booster='gbtree', \n                             n_jobs=-1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, \n                             subsample=0.8, colsample_bytree=0.8, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0.2, reg_lambda=1.2, \n                             scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, importance_type='gain') \n","53171975":"model_xgb.fit(X_train, y_train)","bddccc2b":"from yellowbrick.regressor import PredictionError\nvisualizer = PredictionError(model_xgb)\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()","0d744c5c":"from yellowbrick.regressor import ResidualsPlot\nvisualizer = ResidualsPlot(model_xgb)\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()","1d8a478e":"from sklearn.ensemble import RandomForestRegressor\nfrom yellowbrick.regressor import PredictionError\nrdf_r=RandomForestRegressor(n_estimators=600,random_state=0, n_jobs= -1)\nvisualizer = PredictionError(rdf_r)\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()","3a9ffaf6":"from yellowbrick.regressor import ResidualsPlot\nvisualizer = ResidualsPlot(rdf_r)\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()","954b0545":"#import scikitplot as skplt\n#skplt.estimators.plot_learning_curve(rdf_r, X, y)\n#plt.show()","251da642":"from sklearn.linear_model import Ridge\nfrom yellowbrick.regressor import PredictionError\nmodel_rdg=Ridge(alpha=3.181)\nvisualizer = PredictionError(model_rdg)\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()","9e9a4bef":"from yellowbrick.regressor import ResidualsPlot\nvisualizer = ResidualsPlot(model_rdg)\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()","5e7c8231":"import scikitplot as skplt\nskplt.estimators.plot_learning_curve(model_rdg, X, y)\nplt.show()","64a3a262":"from yellowbrick.regressor import PredictionError\nfrom sklearn.tree import DecisionTreeRegressor\ndtree = DecisionTreeRegressor(random_state=0)\nvisualizer = PredictionError(dtree)\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()","5d8dc8f8":"from yellowbrick.regressor import ResidualsPlot\nvisualizer = ResidualsPlot(dtree)\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()","3c4d1b77":"import scikitplot as skplt\nskplt.estimators.plot_learning_curve(dtree, X, y)\nplt.show()","946f8260":"from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nmodel_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y_train)\nfrom yellowbrick.regressor import PredictionError\nvisualizer = PredictionError(model_lasso)\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()","2131f740":"from yellowbrick.regressor import ResidualsPlot\nvisualizer = ResidualsPlot(model_lasso)\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()","77034fec":"import scikitplot as skplt\nskplt.estimators.plot_learning_curve(model_lasso, X, y)\nplt.show()","6433c55c":"X_test = X[:50]\ny_test = y[:50]\npred1 = model_xgb.predict(X_test)\npred2 = rdf_r.predict(X_test)\npred3 = model_lasso.predict(X_test)\npred4 = dtree.predict(X_test)\npred5=model_rdg.predict(X_test)\nplt.style.use('ggplot')\nplt.figure(figsize=(8,6))\nplt.plot(y_test, 'b*', label='ActualValue' )\nplt.plot(pred1, 'gd', label='XGBoost')\nplt.plot(pred2, 'b^', label='RandomForestRegressor')\nplt.plot(pred3, 'ys', label='Lasso')\nplt.plot(pred4, 'p', label='DecisionTree')\nplt.plot(pred5, 'r*', ms=10, label='Ridge')\n\nprint('XGBoost :',model_xgb.score(X_test, y_test))\nprint('RandomForestRegressor :',rdf_r.score(X_test, y_test))\nprint('Lasso :',model_lasso.score(X_test, y_test))\nprint('DecisionTree :',dtree.score(X_test, y_test))\nprint('Ridge :',model_rdg.score(X_test, y_test))\n# plt.tick_params(axis='x', which='both', bottom=False, top=False,\n#                 labelbottom=False)\nplt.tick_params(axis='x', which='both', bottom=True, top=True,\n                labelbottom=True)\nplt.ylabel('predicted Value')\nplt.xlabel('Test samples')\nplt.legend(loc=\"best\")\nplt.title('Model predictions and their average')\nplt.show()","8bcfde5e":"### Hypothesis Testing\n> A statistical hypothesis, sometimes called confirmatory data analysis, is a hypothesis that is testable on the basis of observing a process that is modeled via a set of random variables. A statistical hypothesis test is a method of statistical inference","9c402838":"As we can see that some houses have zero garage area indicates that they did not have garage. there are also some outliers which affects our regression model so we remove that outliers from our data. ","d84aa22c":"With this information we can see that the prices are skewed right and some outliers lies above ~500,000.","61e73fbd":"**Bivariate analysis**","42a9f67b":"### Missing data\nImportant questions when thinking about missing data:\n\n* How prevalent is the missing data?\n* Is missing data random or does it have a pattern? \n>  The answer to these questions is important for practical reasons because missing data can imply a reduction of the sample size. This can prevent us from proceeding with the analysis. Moreover, from a substantive perspective, we need to ensure that the missing data process is not biased and hidding an inconvenient truth.","868b5d95":"> Random Forest is a flexible, easy to use machine learning algorithm that produces great results most of the time with minimum time spent on hyper-parameter tuning. It has gained popularity due to its simplicity and the fact that it can be used for both classification and regression tasks.","2ecf0296":"**Handling Missing Values**","79dff8c2":"### **Residual plot**\nA residual is a difference between the target and predicted values, i.e. the error of the prediction. The ResidualsPlot Visualizer shows the difference between residuals on the vertical axis and the dependent variable on the horizontal axis","9475d962":"It is apparent that SalePrice doesn't follow normal distribution, so before performing regression it has to be transformed. While log transformation does pretty good job, best fit is unbounded Johnson distribution.","3d09df39":"## Lasso","9a987af4":"> Lasso regression is a type of linear regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, like the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters).","ae8fd869":"There is 10 strongly correlated values with SalePrice, now we have strongly correlated values but this is incomplete without removing outliers.","16738631":"##### Low Range  -> SalePrice less than 143000\n##### Medium Range -> SalePrice between 143000 and 254000\n##### High Range -> SalePrice greater than 25400","24bb97cb":"### The relationship between these discrete features and Sale Price","a0787fc6":"**For the visual comparison of models performance, total 50 random test samples are selected from the test dataset, after scaling the test values the house prices are predicted using the trained model. In fig above the predicted price values of each model are plotted with actual price to show the performance of each model.** ","89e9b078":"We'll consider that when more than 20% of the data is missing, we should delete the corresponding variable and pretend it never existed. This means that we will not try any trick to fill the missing data in these cases.","a5a87ec6":"### Feature Selection\n1.Univariate Selection\n\n2.Feature Importance\n\n3.Corellation matrix using Heatmap\n\n### Filter method\nselect the best subset\n  \n  -chi square\n  \n  -anova test\n\n  -corellation coefficient(pearson [-1,1])\n\n### wrapper meathod\n\n\n**1.Forward Selection**: Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model.\n\n**2.Backward Elimination:** In backward elimination, we start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features.\n\n**3.Recursive Feature elimination:** It is a greedy optimization algorithm which aims to find the best performing feature subset. It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the next model with the left features until all the features are exhausted. It then ranks the features based on the order of their elimination.\n\n\n### Embedded techniques\ntries all the permutation of different subsets\n\n\n\n### Univariate Selection\nStatistical tests can be used to select those features that have the strongest relationship with the output variable.\n\n\nThe scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features.\n\n\nThe example below uses the chi-squared (chi\u00b2) statistical test for non-negative features to select 10 of the best features from the Mobile Price Range Prediction Dataset.\n\n\n\n","9f435790":"We have 1460 observations of 80 variables in the training dataframe. The variables are described below:\n\nSalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n\n* MSSubClass: The building class\n* MSZoning: The general zoning classification\n* LotFrontage: Linear feet of street connected to property\n* LotArea: Lot size in square feet\n* Street: Type of road access\n* Alley: Type of alley access\n* LotShape: General shape of property\n* LandContour: Flatness of the property\n* Utilities: Type of utilities available\n* LotConfig: Lot configuration\n* LandSlope: Slope of property\n* Neighborhood: Physical locations within Ames city limits\n* Condition1: Proximity to main road or railroad\n* Condition2: Proximity to main road or railroad (if a second is present)\n* BldgType: Type of dwelling\n* HouseStyle: Style of dwelling\n* OverallQual: Overall material and finish quality\n* OverallCond: Overall condition rating\n* YearBuilt: Original construction date\n* YearRemodAdd: Remodel date\n* RoofStyle: Type of roof\n* RoofMatl: Roof material\n* Exterior1st: Exterior covering on house\n* Exterior2nd: Exterior covering on house (if more than one material)\n* MasVnrType: Masonry veneer type\n* MasVnrArea: Masonry veneer area in square feet\n* ExterQual: Exterior material quality\n* ExterCond: Present condition of the material on the exterior\n* Foundation: Type of foundation\n* BsmtQual: Height of the basement\n* BsmtCond: General condition of the basement\n* BsmtExposure: Walkout or garden level basement walls\n* BsmtFinType1: Quality of basement finished area\n* BsmtFinSF1: Type 1 finished square feet\n* BsmtFinType2: Quality of second finished area (if present)\n* BsmtFinSF2: Type 2 finished square feet\n* BsmtUnfSF: Unfinished square feet of basement area\n* TotalBsmtSF: Total square feet of basement area\n* Heating: Type of heating\n* HeatingQC: Heating quality and condition\n* CentralAir: Central air conditioning\n* Electrical: Electrical system\n* 1stFlrSF: First Floor square feet\n* 2ndFlrSF: Second floor square feet\n* LowQualFinSF: Low quality finished square feet (all floors)\n* GrLivArea: Above grade (ground) living area square feet\n* BsmtFullBath: Basement full bathrooms\n* BsmtHalfBath: Basement half bathrooms\n* FullBath: Full bathrooms above grade\n* HalfBath: Half baths above grade\n* Bedroom: Number of bedrooms above basement level\n* Kitchen: Number of kitchens\n* KitchenQual: Kitchen quality\n* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n* Functional: Home functionality rating Fireplaces: Number of fireplaces\n* FireplaceQu: Fireplace quality\n* GarageType: Garage location\n* GarageYrBlt: Year garage was built\n* GarageFinish: Interior finish of the garage\n* GarageCars: Size of garage in car capacity\n* GarageArea: Size of garage in square feet\n* GarageQual: Garage quality\n* GarageCond: Garage condition\n* PavedDrive: Paved driveway\n* WoodDeckSF: Wood deck area in square feet\n* OpenPorchSF: Open porch area in square feet\n* EnclosedPorch: Enclosed porch area in square feet\n* 3SsnPorch: Three season porch area in square feet\n* ScreenPorch: Screen porch area in square feet\n* PoolArea: Pool area in square feet\n* PoolQC: Pool quality\n* Fence: Fence quality\n* MiscFeature: Miscellaneous feature not covered in other categories\n* MiscVal: $Value of miscellaneous feature\n* MoSold: Month Sold\n* YrSold: Year Sold\n* SaleType: Type of sale\n* SaleCondition: Condition of sale\n","0cfbcceb":"there's no missing value and generating the dummy variables for categorical data we got the total 302 attributes or columns so we drop first to reduce the coumns and the complexcity of our model this descreases the running time.","175b64b5":"**Data Visualization**","424d9760":"**Univariate analysis**","1472a664":"## XGBoost \nXGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. In prediction problems involving unstructured data (images, text, etc.) artificial neural networks tend to outperform all other algorithms or frameworks. Can be used to solve regression, classification, ranking, and user-defined prediction problems.","3c5e1f60":"<font size=\"+1\" color=blue ><b>Visualization tools.<\/b><\/font>\n* Matplotlib\n* Seaborn\n* Yellowbrick\n* Plotly\n\n<font size=\"+1\" color=brown ><b>ML Algorithms.<\/b><\/font>\n* Xgboost\n* Lasso\n* RandomForest\n* DecisionTree\n* Ridge","8c165ff1":"## DecisionTree","064ad0b7":"### Correlation values","b890aad0":"## RandomForest","d74aae60":"## Model Training and Testing","5e813a83":"> Decision Tree - Regression. Decision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes","efc935ba":"<font size=\"+3\" color=purple ><b> <center><u>EDA on House Prices with python<\/u><\/center><\/b><\/font>\n \n","bc3330df":"<font size=\"+2\" color=green ><b>Please Upvote my kernel if you like my work.<\/b><\/font>","ed02d1ee":"> Ridge regression is a way to create a parsimonious model when the number of predictor variables in a set exceeds the number of observations, or when a data set has multicollinearity (correlations between predictor variables).","bce9496a":"**Developing a prediction model for house prices is much needed for socio-economic development and national lives. In this notebook, a diverse set of machine learning algorithms such as Decision Tree, Random Forest, XgBoost, Lasso etc. are being employed to predict the housing price. The main aim of this work is to predict a house price for the given features to maximize the prediction accuracy by using the proposed methodology.**","65b4de16":"**Merging the training and testing dataset**","7812d666":"* Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string and also the 'RL' is by far the most common value. So we can fill in missing values with 'RL'\n* for all the catagorical variable mentioned above data description says NA means \"No Pool\",\"No Basement\" etc.. majority of houses have no Pool at all in general.\n* Replacing missing data with 0 (Since No garage = no cars in such garage)\n* group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\n  In functional data description says NA means typical","378f581d":"### **PredictionError plot**\nThe Prediction Error Visualizer visualizes prediction errors as a scatterplot of the predicted and actual values. We can then visualize the line of best fit and compare it to the 45\u00ba line.","4baee81f":"## Model Performance Plot","74e821cd":"## Ridge"}}