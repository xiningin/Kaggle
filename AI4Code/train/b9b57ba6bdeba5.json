{"cell_type":{"075f555c":"code","68b63ef2":"code","9b641908":"code","e118f301":"code","efbe40f9":"code","c0c63e77":"code","b4d37ae0":"code","eb214814":"code","db7cdc86":"code","55b83a44":"code","f6190d80":"code","584df7d8":"code","5162a3f9":"code","99d4e72e":"code","26489679":"code","fe3fe216":"code","ff6efd45":"code","8718e4c9":"code","fadc0d08":"code","1e25f672":"code","39fc1032":"code","c3e4675e":"code","706f4141":"code","fc8f4bad":"code","4b20dfd3":"code","ee25b39c":"code","6dc5c5f7":"code","c240c323":"code","cdd782ea":"code","c8685228":"code","78179da5":"code","4fc6c7ba":"code","de005384":"code","d7723817":"code","8172da6e":"code","875dfa8c":"code","681d049f":"code","48f533e5":"code","e8d124e7":"code","3d83b7a0":"markdown","6e009b81":"markdown","da6d60c1":"markdown","6c968306":"markdown","fdb4649c":"markdown","085c7858":"markdown","d2b8a24d":"markdown","338b7d69":"markdown","2ac475f1":"markdown","bbcf31fe":"markdown","265928c6":"markdown","dca8541f":"markdown","c845812a":"markdown","25117551":"markdown","41eb7ebf":"markdown","0fea1706":"markdown","b2ab2529":"markdown","1806ff9c":"markdown","849397af":"markdown","38864fd4":"markdown","90bdbe80":"markdown","2abbb65c":"markdown","e05e0b42":"markdown","bbdbc305":"markdown","b79b7d5a":"markdown","1968cea7":"markdown","6c1e11fc":"markdown","8b16b532":"markdown","24d21ef4":"markdown","604c858a":"markdown","2c7c1d2d":"markdown","c074dc0c":"markdown","d756be37":"markdown","28738697":"markdown","8a273d24":"markdown","da87d98d":"markdown","825d2939":"markdown","84ac176e":"markdown","28581903":"markdown","3181be08":"markdown","fbe6f7f9":"markdown","20c6283c":"markdown","37dce76b":"markdown","f8f8da48":"markdown","f836d533":"markdown","a059215d":"markdown","aa5ab606":"markdown","e24d1216":"markdown","a11dfbf5":"markdown"},"source":{"075f555c":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import r2_score\nfrom matplotlib.pyplot import figure\nfigure(figsize=(15, 6), dpi=80)\n%config Completer.use_jedi=False","68b63ef2":"X=np.linspace(0,20,1000)\ny= -1*X+2","9b641908":"plt.plot(X,y)\nplt.scatter(X,y)\nplt.show()","e118f301":"def Adagrad(X,y,MaxEpochs,epsilon,alpha,errorCritria,gradCritria):\n    loss=[]\n    thetasL0=[]\n    thetasL1=[]\n    yPredict_Epochs=[]\n    x=np.column_stack((np.ones(len(X),dtype=int), X))\n    y= y.reshape(-1,1)\n    m=(x.shape)[0]\n    thetas=np.zeros((x.shape[1],1))\n    count=0\n    epoch=0\n    v=0\n    while epoch < MaxEpochs:\n        count+=1\n        yPredict= x@thetas\n        \n        MSEold=(np.sum((yPredict-y)**2))\/(2*m)\n        \n        Grad= (np.transpose(x)@(yPredict-y))\/m\n        \n        v= v + (Grad)**2\n        LR= ((alpha)\/((v**0.5)+epsilon))\n        thetas= thetas - LR*Grad\n        thetasL0.append(thetas[0])\n        thetasL1.append(thetas[1])\n        \n        yPredict= x@thetas\n        \n\n        MSEnew=(np.sum((yPredict-y)**2))\/(2*m)\n        \n        loss.append(MSEnew)\n        yPredict_Epochs.append(yPredict)\n        if abs(MSEold-MSEnew)< errorCritria and round(np.linalg.norm(Grad,2),5) < gradCritria :\n            print(f\" Our Model Converge after {count} iterations\")\n            print(\"The norm for our Gradient \",np.linalg.norm(Grad,2))\n            print(\"Error Difference: \",abs(MSEold-MSEnew))\n            return r2_score(y,yPredict), thetas, yPredict,loss,thetasL1,thetasL0,yPredict_Epochs\n        \n        epoch+=1\n    print(\"Gradient Norm: \",np.linalg.norm(Grad,2))\n    print(\"Error Difference: \",abs(MSEold-MSEnew))\n    print(f\"We Reach {MaxEpochs} iterations\")\n    return r2_score(y,yPredict), thetas, yPredict,loss,thetasL1,thetasL0,yPredict_Epochs","efbe40f9":"print(\"First Trail with Learning Rate = 0.001 \\n\\n\")\nS1,ths1,yP1,loss1,th1_1,th0_1,yP_Epochs1=Adagrad(X,y,1000,1e-8,0.001,0.001,0.0001)\nprint(\"First Trail Score: \\n\", S1)\nprint(\"Second Trail with Learning Rate = 0.01 \\n\\n\")\nS2,ths2,yP2,loss2,th1_2,th0_2,yP_Epochs2=Adagrad(X,y,1000,1e-8,0.01,0.001,0.0001)\nprint(\"Second Trail Score: \\n\", S2)\nprint(\"Third Trail with Learning Rate = 0.1 \\n\\n\")\nS3,ths3,yP3,loss3,th1_3,th0_3,yP_Epochs3=Adagrad(X,y,1000,1e-8,0.1,0.001,0.0001)\nprint(\"Third Trail Score: \\n\", S3)\nprint(\"Fourth Trail with Learning Rate = 0.2 \\n\\n\")\nS4,ths4,yP4,loss4,th1_4,th0_4,yP_Epochs4=Adagrad(X,y,1000,1e-8,0.2,0.001,0.0001)\nprint(\"Fourth Trail Score: \\n\", S4)\nprint(\"Fifth Trail with Learning Rate = 0.6 \\n\\n\")\nS5,ths5,yP5,loss5,th1_5,th0_5,yP_Epochs5=Adagrad(X,y,1000,1e-8,0.6,0.001,0.0001)\nprint(\"Fifth Trail Score: \\n\", S5)","c0c63e77":"plt.plot(loss1 )\nplt.xlabel('Epochs')\nplt.ylabel('Loss1')\nplt.title(\"Plot Loss1 Vs Epochs\")\nplt.show()\nplt.plot(loss2 )\nplt.xlabel('Epochs')\nplt.ylabel('Loss2')\nplt.title(\"Plot Loss2 Vs Epochs\")\nplt.show()\nplt.plot(loss3 )\nplt.xlabel('Epochs')\nplt.ylabel('Loss3')\nplt.title(\"Plot Loss3 Vs Epochs\")\nplt.show()\nplt.plot(loss4 )\nplt.xlabel('Epochs')\nplt.ylabel('Loss4')\nplt.title(\"Plot Loss4 Vs Epochs\")\nplt.show()\nplt.plot(loss5)\nplt.xlabel('Epochs')\nplt.ylabel('Loss5')\nplt.title(\"Plot Loss5 Vs Epochs\")\nplt.show()","b4d37ae0":"plt.scatter(X,y, color = 'red')\nplt.plot(X,yP1 , color = 'black')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title(\"Plot LinearRegression1\")\nplt.show()\nplt.scatter(X,y, color = 'red')\nplt.plot(X,yP2 , color = 'black')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title(\"Plot LinearRegression2\")\nplt.show()\nplt.scatter(X,y, color = 'red')\nplt.plot(X,yP3 , color = 'black')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title(\"Plot LinearRegression3\")\nplt.show()\nplt.scatter(X,y, color = 'red')\nplt.plot(X,yP4 , color = 'black')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title(\"Plot LinearRegression4\")\nplt.show()\nplt.scatter(X,y, color = 'red')\nplt.plot(X,yP5 , color = 'black')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title(\"Plot LinearRegression5\")\nplt.show()","eb214814":"Score,thetas,yP,loss,thetasL1,thetasL0,yPredict_Epochs=Adagrad(X,y,1000,1e-8,0.6,0.001,0.0001)","db7cdc86":"Score","55b83a44":"print(f\"The Values for best Thetas are {np.round(thetas[0][0],8)} for Theta0 and {np.round(thetas[1][0],8)} for Theta1\")","f6190d80":"plt.plot(loss , color = 'blue')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title(\"Plot Loss Vs Epochs\")\nplt.show()","584df7d8":"plt.plot(thetasL0 ,loss,color = 'black')\nplt.xlabel('theta0')\nplt.ylabel('Loss')\nplt.title(\"Plot Theta0 Vs Loss\")\nplt.show()\nplt.plot(thetasL1 ,loss,color = 'black')\nplt.xlabel('theta1')\nplt.ylabel('Loss')\nplt.title(\"Plot Theta1 Vs Loss\")\nplt.show()","5162a3f9":"for p in yPredict_Epochs:\n    plt.scatter(X,y, color = 'green')\n    plt.plot(X, p,color=\"red\")\nplt.show()","99d4e72e":"plt.scatter(X,y, color = 'blue')\nplt.plot(X,yP , color = 'red')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title(\"Plot LinearRegression Line for best fitted model\")\nplt.show()","26489679":"def RMSProp(X,y,MaxEpochs,epsilon,B,alpha,errorCritria,gradCritria):\n    loss=[]\n    thetasL0=[]\n    thetasL1=[]\n    yPredict_Epochs=[]\n    x=np.column_stack((np.ones(len(X),dtype=int), X))\n    y= y.reshape(-1,1)\n    m=(x.shape)[0]\n    thetas=np.zeros((x.shape[1],1))\n    count=0\n    epoch=0\n    v=0\n    while epoch < MaxEpochs:\n        count+=1\n        yPredict= x@thetas\n        \n        MSEold=(np.sum((yPredict-y)**2))\/(2*m)\n        \n        Grad= (np.transpose(x)@(yPredict-y))\/m\n        \n        v= B*v + (1-B)*((Grad)**2)\n        LR= ((alpha)\/((v**0.5)+epsilon))\n        thetas= thetas - LR*Grad\n        thetasL0.append(thetas[0])\n        thetasL1.append(thetas[1])\n        \n        yPredict= x@thetas\n        \n\n        MSEnew=(np.sum((yPredict-y)**2))\/(2*m)\n        \n        loss.append(MSEnew)\n        yPredict_Epochs.append(yPredict)\n        if abs(MSEold-MSEnew)< errorCritria and round(np.linalg.norm(Grad,2),5) < gradCritria :\n            print(f\"Convergence occure at {count} iterations\")\n            print(\"Gradient Norm: \",np.linalg.norm(Grad,2))\n            print(\"Error Difference: \",abs(MSEold-MSEnew))\n            return r2_score(y,yPredict), thetas, yPredict,loss,thetasL1,thetasL0,yPredict_Epochs\n        \n        epoch+=1\n        \n    print(\"Gradient Norm: \",np.linalg.norm(Grad,2))\n    print(\"Error Difference: \",abs(MSEold-MSEnew))\n    print(f\"We Reach {MaxEpochs} iterations\")\n    return r2_score(y,yPredict), thetas, yPredict,loss,thetasL1,thetasL0,yPredict_Epochs","fe3fe216":"print(\"First Trail with Learning Rate = 0.0001 and \u03b2=0.9 \")\nS1,ths1,yP1,loss1,th1_1,th0_1,yP_Epochs1=RMSProp(X,y,1000,1e-8,0.9,0.0001,0.001,0.0001)\nprint(\"First Trail Score: \", S1)\nprint()\nprint(\"Second Trail with Learning Rate = 0.001 and \u03b2=0.8 \")\nS2,ths2,yP2,loss2,th1_2,th0_2,yP_Epochs2=RMSProp(X,y,1000,1e-8,0.8,0.001,0.001,0.0001)\nprint(\"Second Trail Score: \", S2)\nprint()\nprint(\"Third Trail with Learning Rate = 0.001 and \u03b2=0.99 \")\nS3,ths3,yP3,loss3,th1_3,th0_3,yP_Epochs3=RMSProp(X,y,1000,1e-8,0.99,0.001,0.001,0.0001)\nprint(\"Third Trail Score: \", S3)\nprint()\nprint(\"Fourth Trail with Learning Rate = 0.0001 and \u03b2=0.99 \")\nS4,ths4,yP4,loss4,th1_4,th0_4,yP_Epochs4=RMSProp(X,y,1000,1e-8,0.99,0.00001,0.001,0.0001)\nprint(\"Fourth Trail Score: \", S4)\nprint()\nprint(\"Fifth Trail with Learning Rate = 0.01 and \u03b2=0.99 \")\nS5,ths5,yP5,loss5,th1_5,th0_5,yP_Epochs5=RMSProp(X,y,1000,1e-8,0.99,0.01,0.001,0.0001)\nprint(\"Fifth Trail Score: \", S5)\n","ff6efd45":"plt.plot(loss1)\nplt.xlabel('Epochs')\nplt.ylabel('Loss1')\nplt.title(\"Plot Loss1 Vs Epochs\")\nplt.show()\nplt.plot(loss2 )\nplt.xlabel('Epochs')\nplt.ylabel('Loss2')\nplt.title(\"Plot Loss2 Vs Epochs\")\nplt.show()\nplt.plot(loss3 )\nplt.xlabel('Epochs')\nplt.ylabel('Loss3')\nplt.title(\"Plot Loss3 Vs Epochs\")\nplt.show()\nplt.plot(loss4 )\nplt.xlabel('Epochs')\nplt.ylabel('Loss4')\nplt.title(\"Plot Loss4 Vs Epochs\")\nplt.show()\nplt.plot(loss5 )\nplt.xlabel('Epochs')\nplt.ylabel('Loss5')\nplt.title(\"Plot Loss5 Vs Epochs\")\nplt.show()","8718e4c9":"plt.scatter(X,y)\nplt.plot(X,yP1 , color = 'red')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title(\"Plot LinearRegression1\")\nplt.show()\nplt.scatter(X,y)\nplt.plot(X,yP2 , color = 'red')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title(\"Plot LinearRegression2\")\nplt.show()\nplt.scatter(X,y)\nplt.plot(X,yP3 , color = 'red')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title(\"Plot LinearRegression3\")\nplt.show()\nplt.scatter(X,y)\nplt.plot(X,yP4 , color = 'red')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title(\"Plot LinearRegression4\")\nplt.show()\nplt.scatter(X,y)\nplt.plot(X,yP5 , color = 'red')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title(\"Plot LinearRegression5\")\nplt.show()","fadc0d08":"Score,thetas,yP,loss,thetasL1,thetasL0,yPredict_Epochs=RMSProp(X,y,1000,1e-8,0.99,0.01,0.001,0.0001)","1e25f672":"Score","39fc1032":"print(f\"The Values for best Thetas are {np.round(thetas[0][0],8)} for Theta0 and {np.round(thetas[1][0],8)} for Theta1\")","c3e4675e":"plt.plot(loss , color = 'blue')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title(\"Plot Loss Vs Epochs\")\nplt.show()","706f4141":"plt.plot(thetasL0 ,loss)\nplt.xlabel('theta0')\nplt.ylabel('Loss')\nplt.title(\"Plot Theta0 Vs Loss\")\nplt.show()\nplt.plot(thetasL1 ,loss)\nplt.xlabel('theta1')\nplt.ylabel('Loss')\nplt.title(\"Plot Theta1 Vs Loss\")\nplt.show()","fc8f4bad":"for p in yPredict_Epochs:\n    plt.scatter(X,y,s=50,color='blue')\n    plt.plot(X, p,color='red')\nplt.show()","4b20dfd3":"plt.scatter(X,y, color = 'blue')\nplt.plot(X,yP , color = 'red',linewidth=4.0)\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title(\"Plot LinearRegression\")\nplt.show()","ee25b39c":"def Adam(X,y,MaxEpochs,epsilon,B1,B2,alpha,errorCritria,gradCritria):\n    loss=[]\n    thetasL0=[]\n    thetasL1=[]\n    yPredict_Epochs=[]\n    x=np.column_stack((np.ones(len(X),dtype=int), X))\n    y= y.reshape(-1,1)\n    m=(x.shape)[0]\n    thetas=np.zeros((x.shape[1],1))\n    count=0\n    epoch=1\n    vt=0\n    mt=0\n    while epoch < MaxEpochs+1:\n        count+=1\n        yPredict= x@thetas\n        \n        MSEold=(np.sum((yPredict-y)**2))\/(2*m)\n        \n        Grad= (np.transpose(x)@(yPredict-y))\/m\n        \n        mt= B1*mt + (1-B1)*(Grad)\n        vt= B2*vt + (1-B2)*((Grad)**2)\n        \n        mt=mt\/(1-(B1**epoch))\n        vt=vt\/(1-(B2**epoch))\n        \n        LR= (alpha)\/(((vt**0.5)+epsilon))\n        \n        thetas= thetas - LR*mt\n        thetasL0.append(thetas[0])\n        thetasL1.append(thetas[1])\n        \n        yPredict= x@thetas\n        \n\n        MSEnew=(np.sum((yPredict-y)**2))\/(2*m)\n        \n        loss.append(MSEnew)\n        yPredict_Epochs.append(yPredict)\n        if abs(MSEold-MSEnew)< errorCritria and round(np.linalg.norm(Grad,2),5) < gradCritria :\n            print(f\"Convergence occure at {count} iterations\")\n            print(\"Gradient Norm: \",np.linalg.norm(Grad,2))\n            print(\"Error Difference: \",abs(MSEold-MSEnew))\n            return r2_score(y,yPredict), thetas, yPredict,loss,thetasL1,thetasL0,yPredict_Epochs\n        \n        epoch+=1\n    print(\"Gradient Norm: \",np.linalg.norm(Grad,2))\n    print(\"Error Difference: \",abs(MSEold-MSEnew))\n    print(f\"We Reach {MaxEpochs} iterations\")\n    return r2_score(y,yPredict), thetas, yPredict,loss,thetasL1,thetasL0,yPredict_Epochs","6dc5c5f7":"print(\"First Trail with Learning Rate = 0.0001 and \u03b21=0.9, \u03b22=0.99 \\n\")\nS1,ths1,yP1,loss1,th1_1,th0_1,yP_Epochs1=Adam(X,y,1000,1e-8,0.9,0.99,0.0001,0.001,0.0001)\nprint(\"First Trail Score: \", S1)\nprint()\nprint(\"Second Trail with Learning Rate = 0.0001 and \u03b21=0.8, \u03b22=0.9 \\n\")\nS2,ths2,yP2,loss2,th1_2,th0_2,yP_Epochs2=Adam(X,y,1000,1e-8,0.8,0.9,0.0001,0.001,0.0001)\nprint(\"Second Trail Score: \", S2)\nprint()\nprint(\"Third Trail with Learning Rate = 0.001 and \u03b21=0.8, \u03b22=0.99 \\n\")\nS3,ths3,yP3,loss3,th1_3,th0_3,yP_Epochs3=Adam(X,y,1000,1e-8,0.8,0.99,0.001,0.001,0.0001)\nprint(\"Third Trail Score: \", S3)\nprint()\nprint(\"Fourth Trail with Learning Rate = 0.01 and \u03b21=0.9, \u03b22=0.9 \\n\")\nS4,ths4,yP4,loss4,th1_4,th0_4,yP_Epochs4=Adam(X,y,1000,1e-8,0.9,0.9,0.01,0.001,0.0001)\nprint(\"Fourth Trail Score: \", S4)\nprint()\nprint(\"Fifth Trail with Learning Rate = 0.01 and \u03b21=0.7, \u03b22=0.8 and 3000 Iterations \\n\")\nS5,ths5,yP5,loss5,th1_5,th0_5,yP_Epochs5=Adam(X,y,3000,1e-8,0.4,0.7,0.08,0.01,0.0001)\nprint(\"Fifth Trail Score: \", S5)\nprint()\nprint(\"Sixth Trail with Learning Rate = 0.01 and \u03b21=0.7, \u03b22=0.88 and 2000 Iterations \\n\")\nS6,ths6,yP6,loss6,th1_6,th0_6,yP_Epochs6=Adam(X,y,2000,1e-8,0.7,0.88,0.01,0.001,0.0001)\nprint(\"Sixth Trail Score: \", S6)","c240c323":"plt.plot(loss1)\nplt.xlabel('Epochs')\nplt.ylabel('Loss1')\nplt.title(\"Plot Loss1 Vs Epochs\")\nplt.show()\nplt.plot(loss2 )\nplt.xlabel('Epochs')\nplt.ylabel('Loss2')\nplt.title(\"Plot Loss2 Vs Epochs\")\nplt.show()\nplt.plot(loss3 )\nplt.xlabel('Epochs')\nplt.ylabel('Loss3')\nplt.title(\"Plot Loss3 Vs Epochs\")\nplt.show()\nplt.plot(loss4 )\nplt.xlabel('Epochs')\nplt.ylabel('Loss4')\nplt.title(\"Plot Loss4 Vs Epochs\")\nplt.show()\nplt.plot(loss5 )\nplt.xlabel('Epochs')\nplt.ylabel('Loss5')\nplt.title(\"Plot Loss5 Vs Epochs\")\nplt.show()\nplt.plot(loss6)\nplt.xlabel('Epochs')\nplt.ylabel('Loss6')\nplt.title(\"Plot Loss6 Vs Epochs\")\nplt.show()","cdd782ea":"plt.scatter(X,y)\nplt.plot(X,yP1 , color = 'red')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title(\"Plot LinearRegression1\")\nplt.show()\nplt.scatter(X,y)\nplt.plot(X,yP2 , color = 'red')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title(\"Plot LinearRegression2\")\nplt.show()\nplt.scatter(X,y)\nplt.plot(X,yP3 , color = 'red')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title(\"Plot LinearRegression3\")\nplt.show()\nplt.scatter(X,y)\nplt.plot(X,yP4 , color = 'red')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title(\"Plot LinearRegression4\")\nplt.show()\nplt.scatter(X,y)\nplt.plot(X,yP5 , color = 'red')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title(\"Plot LinearRegression5\")\nplt.show()\nplt.scatter(X,y)\nplt.plot(X,yP6 , color = 'red')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title(\"Plot LinearRegression6\")\nplt.show()","c8685228":"Score,thetas,yP,loss,thetasL1,thetasL0,yPredict_Epochs=Adam(X,y,2000,1e-8,0.7,0.88,0.01,0.001,0.0001)","78179da5":"Score","4fc6c7ba":"print(f\"The Values for best Thetas are {np.round(thetas[0][0],8)} for Theta0 and {np.round(thetas[1][0],8)} for Theta1\")","de005384":"plt.plot(loss )\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title(\"Plot Loss Vs Epochs\")\nplt.show()","d7723817":"plt.plot(thetasL0 ,loss,color = 'black')\nplt.xlabel('theta0')\nplt.ylabel('Loss')\nplt.title(\"Plot Theta0 Vs Loss\")\nplt.show()\nplt.plot(thetasL1 ,loss,color = 'black')\nplt.xlabel('theta1')\nplt.ylabel('Loss')\nplt.title(\"Plot Theta1 Vs Loss\")\nplt.show()","8172da6e":"for p in yPredict_Epochs:\n    plt.scatter(X,y)\n    plt.plot(X, p,color='red')\nplt.show()","875dfa8c":"plt.scatter(X,y, color = 'red')\nplt.plot(X,yP , color = 'black')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title(\"Plot LinearRegression\")\nplt.show()","681d049f":"S_ada,ths_ada,yP_ada,loss_ada,th1_ada,th0_ada,yP_Epochs_ada=Adagrad(X,y,1000,1e-8,0.01,0.001,0.0001)\nprint('\\n')\nS_rms,ths_rms,yP_rms,loss_rms,th1_rms,th0_rms,yP_Epochs_rms=RMSProp(X,y,1000,1e-8,0.9,0.01,0.001,0.0001)\nprint('\\n')\nS_adam,ths_adam,yP_adam,loss_adam,th1_adam,th0_adam,yP_Epochs_adam=Adam(X,y,1000,1e-8,0.8,0.9,0.01,0.001,0.0001)","48f533e5":"print(f\"The R2 Score for adagrad algorithm with learning rate 0.01 is {S_ada}\")\nprint(f\"The R2 Score for RMSprop algorithm with learning rate 0.01 and Beta 0.9 is {S_rms}\")\nprint(f\"The R2 Score for adam algorithm with learning rate 0.01, Beta1 0.9 and Beta2 0.9 is {S_adam}\")","e8d124e7":"plt.scatter(X,y)\nplt.plot(X,yP_ada,'Green')\nplt.plot(X,yP_rms ,'black')\nplt.plot(X,yP_adam ,'red')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend(['Adagrad','RMSProp','Adam'])\nplt.title(\"Plot LinearRegression\")\nplt.show()","3d83b7a0":"### Calculate r2 score","6e009b81":"![image.png](attachment:image.png)","da6d60c1":"### Plot loss vs. epochs","6c968306":"### Check The thetas","fdb4649c":"### Update the previos implementation to be RMSProp.\n#### Compare your results with Adagrad results.","085c7858":"### Using The Best HyperParameter with Learning Rate = 0.01 and \u03b2=0.99.","d2b8a24d":"For this practical work, the student will have to develop a Python program that is able to implement the accelerated gradient descent methods with adaptive learning rate <b>(Adagrad, RMSProp, and Adam)<\/b> in order to achieve the linear regression of a set of datapoints.","338b7d69":"## Adagrad","2ac475f1":"## RMSProp","bbcf31fe":"### Plot loss vs. epochs","265928c6":"### Check The thetas","dca8541f":"### Plot The Best Regression Line","c845812a":"#### Try different values of the huperparameters and see the differnce in your results.","25117551":"# Comparison between 3 different algorithms using same hyperprarameters\n\n\n## Adagrad \n\n1- learning rate 0.01\n\n## RMSProp\n\n1- Learning Rate = 0.01\n2- \u03b2=0.9 \n\n## Adam\n\n1- Learning Rate = 0.01\n\n2- \u03b21=0.8 \n\n3- \u03b22=0.9\n","41eb7ebf":"### Plot loss vs. epochs","0fea1706":"### Using The Best HyperParameter with Learning Rate = 0.6","b2ab2529":"### For a single variable linear regression ML model, build a function to find the optimum Theta_0 and Theta_1 parameters using Adagrad optimization algorithm.\n#### The funtion should have the following input parameters:\n##### 1. Input data as a matrix (or vector based on your data).\n##### 2. Target label as a vector.\n##### 3. Learning rate.\n##### 4. Epsilon.\n##### 5. Maximum number of iterations (Epochs).\n#### The funtion should return the following outputs:\n##### 1. All predicted Theta_0 in all iterations.\n##### 2. All predicted Theta_1 in all iterations.\n##### 3. Corresponding loss for each Theta_0 and Theta_1 predictions.\n##### 4.All hypothesis outputs (prdicted labels) for each Theta_0 and Theta_1 predictions.\n##### 5.Final Optimum values of Theta_0 and Theta_1.\n#### Choose the suitable number of iterations, learning rate, Epsilon, and stop criteria.\n#### Calculate r2 score. Shouldn't below 0.9\n#### Plot the required curves (loss-epochs, loss-theta0, loss-theta1, all fitted lines per epoch (single graph), best fit line)\n#### Try different values of the huperparameters and see the differnce in your results.","1806ff9c":"### Using The Best HyperParameter with Learning Rate = 0.01 and \u03b21=0.7 and \u03b22=0.8. .","849397af":"![image.png](attachment:image.png)","38864fd4":"### Update the previos implementation to be Adam.\n#### Compare your results with Adagrad and RMSProp results.","90bdbe80":"### Plot loss vs. epochs","2abbb65c":"### Plot Theta_0 vs. loss and Theta_1 vs. loss","e05e0b42":"______________________________________________","bbdbc305":"### Calculate r2 score","b79b7d5a":"### Calculate r2 score","1968cea7":"### Plot The Best Regression Line","6c1e11fc":"### Plot loss vs. epochs","8b16b532":"## Adam","24d21ef4":"## Congratulations \n![image.png](attachment:image.png)","604c858a":"### Plot all regression lines till converge","2c7c1d2d":"![image-4.png](attachment:image-4.png)","c074dc0c":"________________________________________________________","d756be37":"To have a dataset or set of data points, the student must generate a pair of arrays <b>X<\/b> and <b>y<\/b> with the values in <b>X<\/b> equally distributed between <b>0<\/b> and <b>20<\/b> and the values in <b>y<\/b> such that: \n<b>yi = a*xi + b (and a = -1, b = 2)<\/b>\n","28738697":"### Plot The Best Regression Line","8a273d24":"### Plot Theta_0 vs. loss and Theta_1 vs. loss","da87d98d":"### Plot The Best Regression Line","825d2939":"### Plot Theta_0 vs. loss and Theta_1 vs. loss","84ac176e":"### Plot The Best Regression Line","28581903":"#### Try different values of the huperparameters and see the differnce in your results.","3181be08":"### Plot loss vs. epochs","fbe6f7f9":"#### Import numpy, matplotlib.pyplot and make it inline","20c6283c":"#### Plot your data points. ","37dce76b":"## Practical Work 4 ","f8f8da48":"#### Try different values of the huperparameters and see the differnce in your results.","f836d533":"### Plot The Best Regression Line","a059215d":"### Plot all regression lines till converge","aa5ab606":"### Plot all regression lines till converge","e24d1216":"### Check The thetas","a11dfbf5":"## Calculate R2 Score for each one of previous algorithms using this hyperparameter settings"}}