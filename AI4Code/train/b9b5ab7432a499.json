{"cell_type":{"ce489456":"code","3c41b496":"code","08e5cae8":"code","e697f036":"code","d7f4473f":"code","791cb30c":"code","99b06672":"code","0c3d6d0b":"code","eefab530":"code","3e0df5bb":"code","0509b729":"code","c58845dd":"code","b688423c":"code","97a3fde5":"code","e770778b":"code","99c65875":"code","623a8ec1":"code","d75e1c18":"code","2d6555cd":"code","7ef69368":"code","8d0ed491":"code","15de4cf6":"code","ca69e7a1":"code","a15a92ff":"code","49c91d9b":"code","e65f1295":"code","664f3603":"code","cb85966e":"code","73893a23":"code","7e85e209":"code","c872e2bf":"code","80d5cfea":"code","7f9e6a98":"code","ab5d852b":"code","e8054bd8":"code","b930e318":"code","b3f3938d":"code","b2b5acf4":"code","b09f1c7f":"code","14be989a":"code","0428e34b":"code","816fa551":"code","aa219137":"code","70adc897":"code","17ce7e64":"code","62dde8df":"code","42f9fbba":"code","fa794c67":"markdown","22973d89":"markdown","162f755a":"markdown","df7e4426":"markdown","2fdf0665":"markdown","5d57a098":"markdown","a0defc1a":"markdown","4fbc6a09":"markdown","a88cedfd":"markdown","2a617c23":"markdown","c38a5983":"markdown","ea376ed5":"markdown","0e30c093":"markdown","031561d4":"markdown","205da9f6":"markdown","84553bbe":"markdown","acdf6b41":"markdown","7693cd9d":"markdown","800d7d60":"markdown","61036cd6":"markdown","2497777b":"markdown","af4fd580":"markdown"},"source":{"ce489456":"#importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno\nfrom scipy import stats\nfrom sklearn import preprocessing,model_selection\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score,classification_report","3c41b496":"#reading data\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest =  pd.read_csv(\"..\/input\/test.csv\")\nprint(train.shape,test.shape)","08e5cae8":"#combining data for preprocessing\ncomb = train.append(test)\ncomb.shape","e697f036":"#getting information about datatypes and null values\ncomb.info()\nprint(comb.isnull().sum())","d7f4473f":"#Converting categorical variables to type category\ncat_col = ['Type_of_Cab','Confidence_Life_Style_Index','Destination_Type','Gender','Surge_Pricing_Type']\ncomb[cat_col]= comb[cat_col].astype('category')","791cb30c":"comb.info()","99b06672":"missingno.matrix(comb)","0c3d6d0b":"#Checking for any duplicate data\nif len(comb[comb.duplicated()]) > 0:\n    print(\"No. of duplicated entries: \", len(comb[comb.duplicated()]))\n    print(comb[comb.duplicated(keep=False)].sort_values(by=list(comb.columns)).head())\nelse:\n    print(\"No duplicated entries found\")","eefab530":"comb.describe()","3e0df5bb":"comb.hist(figsize=(10,10))","0509b729":"col_log =['Trip_Distance','Var1','Var2','Var3']\ncomb[col_log]=np.log(comb[col_log])","c58845dd":"pip install impyute\n","b688423c":"col = ['Customer_Since_Months','Life_Style_Index','Var1']","97a3fde5":"from impyute.imputation.cs import mice\n\nX = comb[col]\n\nimputed = mice(X.values,verbose=1)\n\n#mice_ages = imputed[:, 2]","e770778b":"imputed.shape","99c65875":"mice_csm = imputed[:, 0]\nmice_lsi = imputed[:, 1]\nmice_var1 = imputed[:, 2]","623a8ec1":"sns.distplot(mice_csm, hist = False, kde = True,\n                 kde_kws = {'shade': False, 'linewidth': 3})\nsns.distplot(comb['Customer_Since_Months'], hist = False, kde = True,\n                 kde_kws = {'shade': False, 'linewidth': 3})","d75e1c18":"sns.distplot(mice_lsi, hist = False, kde = True,\n                 kde_kws = {'shade': False, 'linewidth': 3})\nsns.distplot(comb['Life_Style_Index'], hist = False, kde = True,\n                 kde_kws = {'shade': False, 'linewidth': 3})","2d6555cd":"sns.distplot(mice_var1, hist = False, kde = True,\n                 kde_kws = {'shade': False, 'linewidth': 3})\nsns.distplot(comb['Var1'], hist = False, kde = True,\n                 kde_kws = {'shade': False, 'linewidth': 3})","7ef69368":"comb['Customer_Since_Months'] = mice_csm\ncomb['Life_Style_Index'] = mice_lsi\ncomb['Var1'] = mice_var1","8d0ed491":"comb.isnull().sum()","15de4cf6":"comb['Type_of_Cab'] = comb['Type_of_Cab'].cat.add_categories('Unknown')\ncomb['Type_of_Cab'].fillna(\"Unknown\", inplace=True)","ca69e7a1":"comb['Confidence_Life_Style_Index'] = comb['Confidence_Life_Style_Index'].cat.add_categories('Unknown')\ncomb['Confidence_Life_Style_Index'].fillna(\"Unknown\", inplace=True)","a15a92ff":"cat_col = ['Type_of_Cab','Confidence_Life_Style_Index','Destination_Type','Gender','Surge_Pricing_Type']\nfor i in cat_col:\n  print(\"=================\")\n  print(comb[i].value_counts())","49c91d9b":"comb['Destination_Type'].replace({\"A\":1,\"B\":2,\"C\":3,\"D\":4,\"E\":5,\"F\":6,\"G\":7,\"H\":8,\"I\":9,\"J\":10,\"K\":11,\"L\":12,\"M\":13,\"N\":14},inplace = True)\ncomb['Destination_Type']","e65f1295":"bins= [1,4,12,14]\nlabels = ['High_vol','Med_vol','Low_vol']\ncomb['Destination_Type'] = pd.cut(comb['Destination_Type'], bins=bins, labels=labels, right=False)\n","664f3603":"comb['Destination_Type'].value_counts()","cb85966e":"comb.info()","73893a23":"dummies_toc = pd.get_dummies(comb['Type_of_Cab'],prefix='toc',prefix_sep='_')\ndummies_clsi = pd.get_dummies(comb['Confidence_Life_Style_Index'],prefix = 'clsi',prefix_sep='_')","7e85e209":"dummies_dt = pd.get_dummies(comb['Destination_Type'])\ndummies_gender = pd.get_dummies(comb['Gender'])","c872e2bf":"merged = pd.concat([comb,dummies_toc,dummies_clsi,dummies_dt,dummies_gender],axis = 'columns')\ncomb = merged.drop(['Type_of_Cab','Confidence_Life_Style_Index','Destination_Type','Gender'],axis = 'columns')\n\ncomb.shape","80d5cfea":"comb.head()","7f9e6a98":"train_features = comb[comb['Surge_Pricing_Type'].isnull()!=True].drop(['Trip_ID','Surge_Pricing_Type'], axis=1)\ntrain_label = comb[comb['Surge_Pricing_Type'].isnull()!=True]['Surge_Pricing_Type']\n\ntest_features = comb[comb['Surge_Pricing_Type'].isnull()==True].drop(['Trip_ID','Surge_Pricing_Type'], axis=1)\n\ntrain_features.shape,train_label.shape,test_features.shape\n","ab5d852b":"from sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(train_features, train_label, test_size=0.25,random_state=42,stratify = train_label)","e8054bd8":"train = x_train.append(x_val)\nval = y_train.append(y_val)","b930e318":"#xgboost\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier(base_score=0.6,learning_rate=0.05,objective='multi:softmax',n_estimators=300,max_depth=8,n_jobs=-1)\nxgb.fit(x_train,y_train)","b3f3938d":"print(xgb.score(x_val,y_val))","b2b5acf4":"# light gradient boost\nfrom lightgbm import LGBMClassifier\n\nlgbc=LGBMClassifier(n_estimators=425, learning_rate=0.08,objective= 'multiclass', reg_lambda=3,max_depth=8,min_child_weight=0.1)\nlgbc.fit(x_train,y_train)","b09f1c7f":"lgbc.score(x_val,y_val)","14be989a":"#best parameter selection for Perceptron\nfrom sklearn.model_selection import GridSearchCV\n\n\nmlp = MLPClassifier(max_iter=100)\nparameter_space = {\n    'activation': ['tanh', 'relu'],\n    'solver': ['sgd', 'adam'],\n    'alpha': [0.001, 0.01,0.1],\n    'learning_rate': ['constant','adaptive'],\n}\nclf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\nclf.fit(x_train, y_train)\n\nprint('Best parameters found:\\n', clf.best_params_)\n","0428e34b":"from sklearn.neural_network import MLPClassifier\np = MLPClassifier(random_state=42,\n              max_iter=200,activation='relu',solver ='adam',learning_rate='adaptive')\np.fit(x_train,y_train)","816fa551":"p.score(x_val,y_val)","aa219137":"xgb.fit(train,val)\nlgbc.fit(train,val)\np.fit(train,val)","70adc897":"from sklearn.ensemble import VotingClassifier\nestimator = []\nestimator.append(('MLP', p))\nestimator.append(('XGB', xgb )) \nestimator.append(('LGBM', lgbc)) \n \n  \n# Voting Classifier with hard voting \nvot_hard = VotingClassifier(estimators = estimator, voting ='hard') \nvot_hard.fit(train, val) ","17ce7e64":"print(vot_hard.score(x_val,y_val))","62dde8df":"pred_vot_hard = vot_hard.predict(test_features)\nunique_elements, counts_elements = np.unique(pred_vot_hard, return_counts=True)\nprint(unique_elements,counts_elements)","42f9fbba":"pd.DataFrame(pred_vot_hard, columns=['Surge_Pricing_Type']).to_csv('\/prediction_vot_final.csv')","fa794c67":"**The final score recieved on the scoreboard was 0.7012.It is a general agreement that ensembles are more stable and accurate than stand-alone models**","22973d89":"## Working on numerical data","162f755a":"**We can see there is a big variance in the values of some of the features(difference in min and max values),lets apply log transformation on them**","df7e4426":"# Making Predictions","2fdf0665":"**Time for One-Hot Encoding**","5d57a098":"**This is how our final data looks. Data looks pretty normalized so we will not be performing nirmalization separately**","a0defc1a":"# Model Training","4fbc6a09":"# Conclusion","a88cedfd":"\n\n> The approach for this problem will be to choose some models which will provide stable and good accuracies and finally apply hard voting on these to get the results\n\n**Following models have been chosen for this purpose:**\n\n*   XGBoost\n*   Light Gradient Boost\n*   Multi Layer Perceptron\n\n\n\n\n\n\n\n\n\n","2a617c23":"\n\n*In this notebook we successfully predicted the Surge_Pricing_Type for cab aggregators.*\n\n**Some important concepts learned:**\n\n*   MICE helps in imputation of missing data for numerical features with minimal effect on variance of the distribution.\n*   If number of NaN's are very large(larger than observations in one of the category) it is better to make a separate category out of them.\n\n\n*   To remove skewness\/outliers in features we can apply log transformation or normalization.\n*   Building an ensemble is an effective way to get a stable model.\n\n\n\n\n\n","c38a5983":"**Hard-Voting Ensemble**","ea376ed5":"***Imputing values using MICE(Multivariate Imputation by Chained Equation)***","0e30c093":"# Problem Statement\n\n**To build a predictive model, which could help cab-aggregators in predicting the '*surge_pricing_type*' pro-actively. This would in turn help them in matching the right cabs with the right customers quickly and efficiently.**","031561d4":"***In hard voting (also known as majority voting), every individual classifier votes for a class, and the majority wins. In statistical terms, the predicted target label of the ensemble is the mode of the distribution of individually predicted labels.***","205da9f6":"# Data Analysis","84553bbe":"**We will start by imputing values in features having Null values.For this we will create a separate category 'Unknown' as the number of unknowns is greater than number of observations in some other categories and filling it with some test statistics may cause biasing.**","acdf6b41":"*Fitting our models to entire dataset*","7693cd9d":"## Working with categorical features","800d7d60":"## Understanding data","61036cd6":"\n\n> As you can see in above plots, the variance in distribution of data after imputing is done is not very large.This is the reason this method is preferred over imputing by other test statistics.\n\n","2497777b":"\n\n> The model performs quite well on the validation dataset. Let us now use it for predictions on test dataset.\n\n\n\n\n\n\n","af4fd580":"**We can see there are a lot of categories in destination type.This will lead to many extra columns after one-hot encoding which may impact our model accuracy**\n\n*We will try to minimize the categories by making bins ['High_vol','Med_Vol','Low_Vol'] based on the volumes*"}}