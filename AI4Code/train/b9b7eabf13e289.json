{"cell_type":{"3ae2cfde":"code","444f9aff":"code","c29d9813":"code","aab93974":"code","f05e8a12":"code","5073eefb":"code","7abdddf6":"code","d02239aa":"code","e90275d2":"code","ed169bc2":"code","ca1d2a22":"code","542e2e89":"code","8e59f539":"code","1555b1cc":"code","8e0993eb":"code","f44bda6e":"code","030362c8":"code","c268c181":"code","ae888527":"code","17336d8b":"code","a8de64cc":"code","32059076":"code","77002234":"code","6a802a0f":"code","28a5c14a":"code","3e003568":"code","00f7a2b8":"code","2f133aa4":"code","d76f6b73":"code","739391d8":"markdown","9c0cb963":"markdown","5358f312":"markdown"},"source":{"3ae2cfde":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","444f9aff":"# Ignore warning\n\nimport warnings\nwarnings.filterwarnings('ignore')","c29d9813":"FILEPATH = '\/kaggle\/input\/creditcardfraud\/creditcard.csv'","aab93974":"df = pd.read_csv(FILEPATH)","f05e8a12":"df.info()","5073eefb":"df.head()","7abdddf6":"df.isnull().sum()","d02239aa":"import missingno as miss\n\nmiss.matrix(df)","e90275d2":"import seaborn as sns\n\nsns.heatmap(df.corr())","ed169bc2":"X = df.iloc[:, :-1]\ny = df.iloc[:, -1]","ca1d2a22":"# Show X Columns\n\nX.columns","542e2e89":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state = 56, stratify=y)","8e59f539":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC","1555b1cc":"import matplotlib.pyplot as plt\n\ndef show_confusion_matrix(_model_cm, title = None):\n    \n    f, ax = plt.subplots(figsize = (5, 5))\n    \n    sns.heatmap(_model_cm, annot = True, linewidth = 0, linecolor = 'red', fmt = 'g', ax = ax, cmap = 'Greens')\n    \n    # cmap colors:\n    # YlGnBu, Blues, BuPu, Greens\n    \n    plt.title(title + ' Confusion Matrix')\n    plt.xlabel('y Predict')\n    plt.ylabel('y test')\n    \n    plt.show()","8e0993eb":"def predict_with_model(model):\n    \n    model = model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    return y_pred, accuracy","f44bda6e":"def show_metrics(model_cm):\n\n    total = sum(sum(model_cm))\n    \n    accuracy = (model_cm[0, 0] + model_cm[1, 1]) \/ total\n    accuracy = float(\"{:.2f}\".format(accuracy))\n\n    sensitivity = model_cm[0, 0] \/ (model_cm[0, 0] + model_cm[0, 1])\n    sensitivity = float(\"{:.2f}\".format(sensitivity))\n\n    specificity = model_cm[1, 1]\/(model_cm[1, 0] + model_cm[1, 1])\n    specificity = float(\"{:.2f}\".format(specificity))\n    \n    print(f'accuracy : {accuracy}, sensitivity : {sensitivity}, specificity : {specificity}')","030362c8":"best_model_accuracy = 0\nbest_model = None\n\nmodels = [\n#     MLPClassifier(),\n    RandomForestClassifier(),\n    KNeighborsClassifier(),\n    LogisticRegression(solver = \"liblinear\"),\n    DecisionTreeClassifier(),\n    GaussianNB()\n]\n\nresults = pd.DataFrame(columns = ['Accuracy'])\n\nfor model in models:\n    \n    model_name = model.__class__.__name__\n\n    y_pred, accuracy = predict_with_model(model)\n    \n    print(\"-\" * 30)\n    print(model_name + \": \" )\n    \n    current_model_cm = confusion_matrix(y_test, y_pred)\n    show_metrics(current_model_cm)\n    \n    results.loc[model_name] = accuracy\n    \n    if(accuracy > best_model_accuracy):\n        best_model_accuracy = accuracy\n        best_model = model_name\n    \n    print(\"Accuracy: {:.2%}\".format(accuracy))\n    \n    show_confusion_matrix(current_model_cm, model_name)","c268c181":"print(\"Best Model : {}\".format(best_model))\nprint(\"Best Model Accuracy : {:.2%}\".format(best_model_accuracy))","ae888527":"results","17336d8b":"error_rate_list = []\nrange_min = 1\nrange_max = 10\n\nbest_k_value = 0\nbest_k_error_rate = 100\n\nfor i in range(range_min, range_max):\n    \n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(X_train, y_train)\n    \n    pred_i = knn.predict(X_test)\n    current_error_rate = np.mean(pred_i != y_test)\n    error_rate_list.append(current_error_rate)\n    \n    if(best_k_error_rate > current_error_rate):\n        best_k_error_rate = current_error_rate\n        best_k_value = i","a8de64cc":"current_error_rate, best_k_value","32059076":"plt.figure(figsize = (16, 6))\nplt.plot(range(range_min,range_max), error_rate_list, color='green', linestyle = 'dotted', marker = 'o',\n         markerfacecolor = 'green', markersize = 10)\nplt.title('Error Rate vs K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","77002234":"knn = KNeighborsClassifier(n_neighbors = i)\ny_pred, accuracy = predict_with_model(knn)\n\ncurrent_model_cm = confusion_matrix(y_test, y_pred)\nshow_metrics(current_model_cm)\n\nresults.loc[model_name] = accuracy","6a802a0f":"results","28a5c14a":"!pip install python-vivid","3e003568":"from vivid.utils import timer, get_logger\nlogger = get_logger(__name__)","00f7a2b8":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score","2f133aa4":"def calculate_score(y_true, y_pred):\n    \n    _functions = [\n        accuracy_score,\n        f1_score,\n        precision_score,\n        recall_score\n    ]\n    \n    score_map = {}\n    \n    for func in _functions:\n        score_map[func.__name__] = func(y_true, y_pred)\n        \n    return score_map\n","d76f6b73":"with timer(logger, prefix = '\\ntime taken for svc : '):\n    \n    svc_model = SVC()\n    \n    svc_model.fit(X_train, y_train)\n    y_pred = svc_model.predict(X_test)\n    \n    score = calculate_score(y_test, y_pred)\n    \n    logger.info(score)","739391d8":"### Improve K Neighbors by Hyper Parameter Tuning","9c0cb963":"<font color=\"blue\" size=+1.5><b>Check out my other kernels<\/b><\/font>\n\n<table style=\"font-family: 'Trebuchet MS', Arial, Helvetica, sans-serif;border-collapse: collapse;width: 100%;\">\n  <tr>\n    <th style=\"border: 1px solid #ddd;padding: 8px; padding-top: 12px;padding-bottom: 12px;text-align: left;background-color: #2987E7;color: white;\">Notebook<\/th>\n    <th style=\"border: 1px solid #ddd;padding: 8px; padding-top: 12px;padding-bottom: 12px;text-align: left;background-color: #2987E7;color: white;\">Tags<\/th>\n  <\/tr>\n  <tr>\n    <td style=\"text-align: left\"><a href=\"https:\/\/www.kaggle.com\/rajacsp\/sof-questions-eda-and-visual\">SOF Questions - EDA and Visual<\/a> <\/td>\n    <td style=\"text-align: left\">Data Visual, Plotly<\/td>\n  <\/tr>\n  <tr>\n    <td style=\"background-color: #f2f2f2;text-align: left\"><a href=\"https:\/\/www.kaggle.com\/rajacsp\/netflix-visualization-plotly-plots-treemap\">Netflix - Visualization, Plotly, Plots, and Treemap<\/a> <\/td>\n    <td style=\"background-color: #f2f2f2;text-align: left\">Data Visual, Data Cleaning, Plotly<\/td>\n  <\/tr>\n  <tr>\n    <td style=\"text-align: left\"><a href=\"https:\/\/www.kaggle.com\/rajacsp\/prediction-with-various-algorithms\">Prediction with various Algorithms<\/a> <\/td>\n    <td style=\"text-align: left\">Random Forest, Logistic Regression<\/td>\n  <\/tr>\n  <tr>\n    <td style=\"background-color: #f2f2f2;text-align: left\"><a href=\"https:\/\/www.kaggle.com\/rajacsp\/eda-and-visualization\">EDA and Visualization<\/a> <\/td>\n    <td style=\"background-color: #f2f2f2;text-align: left\">Data Cleaning, Data Visual<\/td>\n  <\/tr>\n  <tr>\n    <td style=\"text-align: left\"><a href=\"https:\/\/www.kaggle.com\/rajacsp\/job-analysis-eda-visual\">Job Analysis - EDA and Visual<\/a> <\/td>\n    <td style=\"text-align: left\">Data Visual, EDA, Plotly<\/td>\n  <\/tr>   \n  <tr>\n    <td style=\"background-color: #f2f2f2;text-align: left\"><a href=\"https:\/\/www.kaggle.com\/rajacsp\/estonia-disaster-visualization\">Estonia Disaster - Visualization<\/a> <\/td>\n    <td style=\"background-color: #f2f2f2;text-align: left\">Data Visual, EDA, Data Cleaning<\/td>\n  <\/tr>\n    \n\n  <tr>\n    <td style=\"background-color: #f2f2f2;text-align: left\"><a href=\"https:\/\/www.kaggle.com\/rajacsp\/prediction-with-various-algorithms\">Credit Card Fraud - Prediction with various algorithms<\/a><\/td>\n    <td style=\"background-color: #f2f2f2;text-align: left\">Various ML Algorithms<\/td>\n  <\/tr>  \n  <tr>\n    <td style=\"text-align: left\"><a href=\"https:\/\/www.kaggle.com\/rajacsp\/linear-equations-real-time\">Linear Equations - Real Time<\/a> <\/td>\n    <td style=\"text-align: left\">Linear Equation<\/td>\n  <\/tr>  \n<\/table>\n","5358f312":"### Use python-vivid model as an experiment"}}