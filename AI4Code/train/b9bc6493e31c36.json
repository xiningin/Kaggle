{"cell_type":{"be5cb47f":"code","b2580586":"code","2f102a6c":"code","f1ae615e":"code","2dd980b3":"code","6ad4407d":"code","9a1137c4":"code","6c4632be":"code","5a3126eb":"code","076ea0f9":"code","51e0260c":"code","8456b75b":"code","023b961c":"code","27ec327c":"code","882cebe2":"code","d5bab38a":"code","7c0779f5":"code","0dc3869d":"code","56b660cb":"code","07176671":"code","e12e57c2":"code","ed4acca3":"code","679171cf":"code","5002542a":"code","5c4fda7c":"code","0b80fed8":"code","b3a19438":"code","947023e8":"code","5f73b588":"markdown","08faeb2b":"markdown","8eed7d89":"markdown","45ae8c2f":"markdown","738fd704":"markdown","372f6d47":"markdown","53653d0a":"markdown","cd66454c":"markdown","c6fc8d2a":"markdown","dccbd5ca":"markdown","def960a8":"markdown","5827b345":"markdown","9a8bb613":"markdown","f15b50cb":"markdown","00b5119c":"markdown","2b7818d1":"markdown"},"source":{"be5cb47f":"!pip install transformers","b2580586":"import random\nimport numpy as np\nfrom tqdm import tqdm_notebook as tqdm\nimport time\nimport logging\nfrom sklearn.model_selection import StratifiedKFold\nimport os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n                              TensorDataset)\nfrom sklearn.metrics import accuracy_score, f1_score\n\nfrom transformers import *","2f102a6c":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","f1ae615e":"seed_everything()","2dd980b3":"!ls ..\/input\/nlp-getting-started","6ad4407d":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsubmit = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","9a1137c4":"print('Train size:', train.shape)\nprint('Test size:', test.shape)","6c4632be":"train.head()","5a3126eb":"train.target.value_counts()","076ea0f9":"test.head()","51e0260c":"class InputExample(object):\n    \"\"\"A single training\/test example for simple sequence classification.\"\"\"\n\n    def __init__(self, id, text, label=None):\n        \"\"\"Constructs a InputExample.\n        Args:\n            id: Unique id for the example.\n            text: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        \"\"\"\n        self.id = id\n        self.text = text\n        self.label = label\n\n\nclass InputFeatures(object):\n    def __init__(self,\n                 example_id,\n                 choices_features,\n                 label\n\n                 ):\n        self.example_id = example_id\n        _, input_ids, input_mask, segment_ids = choices_features[0]\n        self.choices_features = {\n            'input_ids': input_ids,\n            'input_mask': input_mask,\n            'segment_ids': segment_ids\n        }\n        self.label = label","8456b75b":"def read_examples(df, is_training):\n    if not is_training:\n        df['target'] = np.zeros(len(df), dtype=np.int64)\n    examples = []\n    for val in df[['id', 'text', 'target']].values:\n        examples.append(InputExample(id=val[0], text=val[1], label=val[2]))\n    return examples, df\n\n\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n\n    # This is a simple heuristic which will always truncate the longer sequence\n    # one token at a time. This makes more sense than truncating an equal percent\n    # of tokens from each, since if one sequence is very short then each token\n    # that's truncated likely contains more information than a longer sequence.\n\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()","023b961c":"\ndef convert_examples_to_features(examples, tokenizer, max_seq_length,\n                                 is_training):\n    features = []\n    for example_index, example in enumerate(examples):\n\n        text = tokenizer.tokenize(example.text)\n        MAX_TEXT_LEN = max_seq_length - 2 \n        text = text[:MAX_TEXT_LEN]\n\n        choices_features = []\n\n        tokens = [\"[CLS]\"] + text + [\"[SEP]\"]  \n        segment_ids = [0] * (len(text) + 2) \n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n        input_mask = [1] * len(input_ids)\n\n        padding_length = max_seq_length - len(input_ids)\n        input_ids += ([0] * padding_length)\n        input_mask += ([0] * padding_length)\n        segment_ids += ([0] * padding_length)\n        choices_features.append((tokens, input_ids, input_mask, segment_ids))\n\n        label = example.label\n        if example_index < 1 and is_training:\n            logger.info(\"*** Example ***\")\n            logger.info(\"idx: {}\".format(example_index))\n            logger.info(\"id: {}\".format(example.id))\n            logger.info(\"tokens: {}\".format(' '.join(tokens).replace('\\u2581', '_')))\n            logger.info(\"input_ids: {}\".format(' '.join(map(str, input_ids))))\n            logger.info(\"input_mask: {}\".format(len(input_mask)))\n            logger.info(\"segment_ids: {}\".format(len(segment_ids)))\n            logger.info(\"label: {}\".format(label))\n\n        features.append(\n            InputFeatures(\n                example_id=example.id,\n                choices_features=choices_features,\n                label=label\n            )\n        )\n    return features\n\n\ndef select_field(features, field):\n    return [\n        feature.choices_features[field] for feature in features\n    ]\n\ndef metric(y_true, y_pred):\n    acc = accuracy_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred, average='macro')\n    return acc, f1","27ec327c":"# hyperparameters\nmax_seq_length = 512  \nlearning_rate = 1e-5  \nnum_epochs = 3  \nbatch_size = 8  \npatience = 2  \nfile_name = 'model'  ","882cebe2":"logger = logging.getLogger('mylogger')\nlogger.setLevel(logging.DEBUG)\ntimestamp = time.strftime(\"%Y.%m.%d_%H.%M.%S\", time.localtime())\nfh = logging.FileHandler('log_model.txt')\nfh.setLevel(logging.DEBUG)\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\nformatter = logging.Formatter('[%(asctime)s][%(levelname)s] ## %(message)s')\nfh.setFormatter(formatter)\nch.setFormatter(formatter)\nlogger.addHandler(fh)\nlogger.addHandler(ch)","d5bab38a":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","7c0779f5":"train_examples, train_df = read_examples(train, is_training=True)\nlabels = train_df['target'].astype(int).values\ntrain_features = convert_examples_to_features(\n    train_examples, tokenizer, max_seq_length, True)\nall_input_ids = np.array(select_field(train_features, 'input_ids'))\nall_input_mask = np.array(select_field(train_features, 'input_mask'))\nall_segment_ids = np.array(select_field(train_features, 'segment_ids'))\nall_label = np.array([f.label for f in train_features])","0dc3869d":"test_examples, test_df = read_examples(test, is_training=False)\ntest_features = convert_examples_to_features(\n    test_examples, tokenizer, max_seq_length, True)\ntest_input_ids = torch.tensor(select_field(test_features, 'input_ids'), dtype=torch.long)\ntest_input_mask = torch.tensor(select_field(test_features, 'input_mask'), dtype=torch.long)\ntest_segment_ids = torch.tensor(select_field(test_features, 'segment_ids'), dtype=torch.long)","56b660cb":"class NeuralNet(nn.Module):\n    def __init__(self, hidden_size=768, num_class=2):\n        super(NeuralNet, self).__init__()\n\n        self.bert = BertModel.from_pretrained('bert-base-uncased',  \n                                        output_hidden_states=True,\n                                        output_attentions=True)\n        for param in self.bert.parameters():\n            param.requires_grad = True\n        self.weights = nn.Parameter(torch.rand(13, 1))\n        self.dropouts = nn.ModuleList([\n            nn.Dropout(0.5) for _ in range(5)\n        ])\n        self.fc = nn.Linear(hidden_size, num_class)\n\n    def forward(self, input_ids, input_mask, segment_ids):\n        all_hidden_states, all_attentions = self.bert(input_ids, token_type_ids=segment_ids,\n                                                                attention_mask=input_mask)[-2:]\n        batch_size = input_ids.shape[0]\n        ht_cls = torch.cat(all_hidden_states)[:, :1, :].view(\n            13, batch_size, 1, 768)\n        atten = torch.sum(ht_cls * self.weights.view(\n            13, 1, 1, 1), dim=[1, 3])\n        atten = F.softmax(atten.view(-1), dim=0)\n        feature = torch.sum(ht_cls * atten.view(13, 1, 1, 1), dim=[0, 2])\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                h = self.fc(dropout(feature))\n            else:\n                h += self.fc(dropout(feature))\n        h = h \/ len(self.dropouts)\n        return h","07176671":"skf = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\n# off: out-of-fold\noof_train = np.zeros((len(train_df), 2), dtype=np.float32)\noof_test = np.zeros((len(test_df), 2), dtype=np.float32)","e12e57c2":"for fold, (train_index, valid_index) in enumerate(skf.split(all_label, all_label)):\n    \n    # remove this line if you want to train for all 7 folds\n    if fold == 2:\n        break # due to kernel time limit\n\n    logger.info('================     fold {}        ==============='.format(fold))\n\n    train_input_ids = torch.tensor(all_input_ids[train_index], dtype=torch.long)\n    train_input_mask = torch.tensor(all_input_mask[train_index], dtype=torch.long)\n    train_segment_ids = torch.tensor(all_segment_ids[train_index], dtype=torch.long)\n    train_label = torch.tensor(all_label[train_index], dtype=torch.long)\n\n    valid_input_ids = torch.tensor(all_input_ids[valid_index], dtype=torch.long)\n    valid_input_mask = torch.tensor(all_input_mask[valid_index], dtype=torch.long)\n    valid_segment_ids = torch.tensor(all_segment_ids[valid_index], dtype=torch.long)\n    valid_label = torch.tensor(all_label[valid_index], dtype=torch.long)\n\n    train = torch.utils.data.TensorDataset(train_input_ids, train_input_mask, train_segment_ids, train_label)\n    valid = torch.utils.data.TensorDataset(valid_input_ids, valid_input_mask, valid_segment_ids, valid_label)\n    test = torch.utils.data.TensorDataset(test_input_ids, test_input_mask, test_segment_ids)\n\n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n\n    model = NeuralNet()\n    model.cuda()\n    loss_fn = torch.nn.CrossEntropyLoss()\n\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n\n    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=1e-6)\n    model.train()\n\n    best_f1 = 0.\n    valid_best = np.zeros((valid_label.size(0), 2))\n\n    early_stop = 0\n    for epoch in range(num_epochs):\n        train_loss = 0.\n        for batch in tqdm(train_loader):\n            batch = tuple(t.cuda() for t in batch)\n            x_ids, x_mask, x_sids, y_truth = batch\n            y_pred = model(x_ids, x_mask, x_sids)\n            loss = loss_fn(y_pred, y_truth)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() \/ len(train_loader)\n    \n        model.eval()\n        val_loss = 0.\n        valid_preds_fold = np.zeros((valid_label.size(0), 2))\n        with torch.no_grad():\n            for i, batch in tqdm(enumerate(valid_loader)):\n                batch = tuple(t.cuda() for t in batch)\n                x_ids, x_mask, x_sids, y_truth = batch\n                y_pred = model(x_ids, x_mask, x_sids).detach()\n                val_loss += loss_fn(y_pred, y_truth).item() \/ len(valid_loader)\n                valid_preds_fold[i * batch_size:(i + 1) * batch_size] = F.softmax(y_pred, dim=1).cpu().numpy()\n    \n        acc, f1 = metric(all_label[valid_index], np.argmax(valid_preds_fold, axis=1))\n        if best_f1 < f1:\n            early_stop = 0\n            best_f1 = f1\n            valid_best = valid_preds_fold\n            torch.save(model.state_dict(), 'model_fold_{}.bin'.format(fold))\n        else:\n            early_stop += 1\n        logger.info(\n            'epoch: %d, train loss: %.8f, valid loss: %.8f, acc: %.8f, f1: %.8f, best_f1: %.8f\\n' %\n            (epoch, train_loss, val_loss, acc, f1, best_f1))\n        torch.cuda.empty_cache()  \n    \n        if early_stop >= patience:\n            break\n\n    test_preds_fold = np.zeros((len(test_df), 2))\n    valid_preds_fold = np.zeros((valid_label.size(0), 2))\n    model.load_state_dict(torch.load('model_fold_{}.bin'.format(fold)))\n    model.eval()\n    with torch.no_grad():\n        for i, batch in tqdm(enumerate(valid_loader)):\n            batch = tuple(t.cuda() for t in batch)\n            x_ids, x_mask, x_sids, y_truth = batch\n            y_pred = model(x_ids, x_mask, x_sids).detach()\n            valid_preds_fold[i * batch_size:(i + 1) * batch_size] = F.softmax(y_pred, dim=1).cpu().numpy()\n    with torch.no_grad():\n        for i, batch in tqdm(enumerate(test_loader)):\n            batch = tuple(t.cuda() for t in batch)\n            x_ids, x_mask, x_sids = batch\n            y_pred = model(x_ids, x_mask, x_sids).detach()\n            test_preds_fold[i * batch_size:(i + 1) * batch_size] = F.softmax(y_pred, dim=1).cpu().numpy()\n    valid_best = valid_preds_fold\n    oof_train[valid_index] = valid_best\n    acc, f1 = metric(all_label[valid_index], np.argmax(valid_best, axis=1))\n    logger.info('epoch: best, acc: %.8f, f1: %.8f, best_f1: %.8f\\n' %\n                (acc, f1, best_f1))\n    \n    \n    #oof_test += test_preds_fold \/ 7 # uncomment this for 7 folds\n    oof_test += test_preds_fold \/ 2 # comment this line when training for 7 folds\n","ed4acca3":"logger.info(f1_score(labels, np.argmax(oof_train, axis=1)))\ntrain_df['pred_target'] = np.argmax(oof_train, axis=1)","679171cf":"train_df.head()","5002542a":"test_df['target'] = np.argmax(oof_test, axis=1)\nlogger.info(test_df['target'].value_counts())","5c4fda7c":"submit['target'] = np.argmax(oof_test, axis=1)\nsubmit.to_csv('submission_3fold.csv', index=False)","0b80fed8":"submit.head()","b3a19438":"offline_sub = pd.read_csv('..\/input\/bertsubmission\/submission.csv')\noffline_sub.head()","947023e8":"offline_sub.to_csv('offline_submission.csv', index=False)","5f73b588":"BERT, which stands for Bidirectional Encoder Representations from Transformers was introduced in this [paper](https:\/\/arxiv.org\/abs\/1810.04805). BERT is often regarded(and used) like ImageNet but for NLP tasks. Like in vision tasks, we start with pretrained models on ImageNet and then finetune for our task at hand; for most of the NLP task, we start with BERT(and its variants) and finetune for our tasks. There are other ongoing NLP competitions where BERT and its variants are ruling the leaderboards. Make sure to check them out, especially their kernels and discussion forums if you want to learn more about NLP and BERT.  ","08faeb2b":"## Tokenization","8eed7d89":"In this kernel, we will use the popular [transformer](https:\/\/github.com\/huggingface\/transformers) library from [hugging face](https:\/\/huggingface.co\/). They have implemented various state-of-the-art NLP models like `BERT`, `XLNet`, `Roberta`, `CRTL`, etc. So make sure to check out their [documentation](https:\/\/huggingface.co\/transformers\/) and try using them in competition. \nWe will start by first installing the library with `pip install transformers`","45ae8c2f":"This kernel can be improved in many ways:\n1. Train for more epochs and see if it improves the result\n2. Use models from best fold score to make predictions for the test set\n3. Try changing random seed and see how the score(LB) changes and use it during ensemble","738fd704":"> I hope this kernel was useful to you in some ways. I am, by no means, expert of NLP. I am still learning about this field. So feel free to discuss about the content of this kernel in the commment section","372f6d47":"Similarly for `test` set.","53653d0a":"`BERT` expects three kinds of input: `input_ids`(of tokens), `segment_ids`(to distinguish different sentences), and `input_mask`(to indicate which elements in the sequence are tokens and which are padding elements). The code below gets all three inputs for `train` set. We will be using `bert-base-uncased` in this kernel but you can experiment with other variants as well.","cd66454c":"Due to kernel time limit, we will only train for 2 folds and use offline submission to make submission. You can train for all folds offline or using multiple kernels","c6fc8d2a":"In this kernel, I want to show you how to finetune `BERT`. Originally I had no plans of publishing this kernel as I wanted to improve on this baseline but after dataleak I decided to publish. I hope people learn from this and improve on this.","dccbd5ca":"We will be using `bert-base-uncased` as our base model and add a linear layer with [multisample dropout](https:\/\/arxiv.org\/abs\/1905.09788). This is based on [8th place solution](https:\/\/www.kaggle.com\/c\/jigsaw-unintended-bias-in-toxicity-classification\/discussion\/100961#latest-593873) of [Jigsaw Competition](https:\/\/www.kaggle.com\/c\/jigsaw-unintended-bias-in-toxicity-classification\/overview). \n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F1528571%2F7822f529cc19735e7412765aa593aeca%2Fmultisample.png?generation=1575802762069664&amp;alt=media)\n","def960a8":"I trained for all folds offline and selected the models from best folds to make predictions on test-set. The LB score was `0.83640`","5827b345":"We will use `StratifiedKFold` to split our data into `7 folds`. Multifold splitting is a popular validation strategy in kaggle competitions. ","9a8bb613":"We will be using the same tokenization process to tokenize our `train` and `test` data. Let's write the code for that","f15b50cb":"For NLP tasks, the data are in human readable form i.e. `text` type, so we need to convert in computer readable form. This is where tokenization comes into play. Tokenization involves two steps: breaking words into `tokens` and converting them into vectors. There is a great [blog post](http:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/) about this and using BERT for the first time. An image from the blog is shown below\n![](http:\/\/jalammar.github.io\/images\/distilBERT\/bert-distilbert-tokenization-2-token-ids.png)","00b5119c":"Since we want to make sure that the results are reproducible every time this kernel runs, we will `seed everything` and fix the randomness","2b7818d1":"What are `[CLS]` and `[SEP]`?\n\nThey are special tokens and `BERT` uses them to mark the beginning(`[CLS]`) and separation\/end of sentence(`[SEP]`). In usage, it would look something like this:\n> `[CLS] a visually stunning rumination on love [SEP]`"}}