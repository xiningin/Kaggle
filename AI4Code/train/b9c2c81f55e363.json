{"cell_type":{"4ddbba1d":"code","11de4970":"code","c725adb5":"code","ee41c42e":"code","d57733f6":"code","c7c14812":"code","c5ff05f5":"code","f80a4822":"code","0f0ea817":"code","971036c4":"code","39020e92":"code","c545f022":"code","31d1be01":"code","1668cb79":"code","22d6be94":"code","bb0b89b4":"code","2eb4c066":"code","2a037c61":"code","57ab09b2":"code","cb3b480a":"code","21e9ba07":"code","9c5b61c3":"code","9e773993":"code","d378e95c":"code","3dc581bd":"code","0a01a3c0":"code","c1eee701":"code","8f8fdda4":"code","1067a3c1":"code","b2685f29":"code","ef809326":"code","f2a85fd1":"code","91609baa":"code","3c20e175":"code","151766fe":"code","913a4bd7":"code","c2815c68":"code","46e8a9be":"code","dbd94743":"code","92db04b8":"code","d9294b03":"markdown"},"source":{"4ddbba1d":"from IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#11ff66','#6611ff','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))","11de4970":"dhtml('Code Modules, Setting, & Functions')","c725adb5":"import numpy as np,pylab as pl\nimport tensorflow as tf\nimport tensorflow.keras.layers as tkl\nimport tensorflow_datasets as tfds\nfrom collections import Counter\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.preprocessing \\\nimport sequence as tksequence","ee41c42e":"def get_weights_shape(layer):\n    w_xh,w_oo,b_h=layer.weights\n    print('w_xh shape: %s'%w_xh.shape)\n    print('w_oo shape: %s'%w_oo.shape)\n    print('b_h shape: %s'%b_h.shape) \ndef compare_calc(layer,x):\n    n,inputs=x.shape[0],x.shape[1]\n    w_xh,w_oo,b_h=layer.weights\n    out_calc=[]\n    output=layer(\n        tf.reshape(x,shape=(1,n,inputs)))\n    pl.figure(figsize=(10,5))\n    for t in range(n):\n        xt=tf.reshape(x_seq[t],(1,inputs))\n        print('time step {} =>'.format(t))\n        print(5*' '+'input'+13*' '+': '+\n              str(xt.numpy()))\n        ht=tf.matmul(xt,w_xh)+b_h \n        print(5*' '+'hidden'+11*' '+': '+   \n              str(ht.numpy()))   \n        if t>0:\n            prev_o=out_calc[t-1]\n        else:\n            prev_o=tf.zeros(shape=(ht.shape))        \n        ot=ht+tf.matmul(prev_o,w_oo)\n        ot=tf.math.tanh(ot)\n        out_calc.append(ot)\n        print(5*' '+'calculated output: '+\n              str(ot.numpy()))\n        print(5*' '+'SimpleRNN output:   '.format(t)+\n              str(output[0][t].numpy())+'\\n')\n        pl.plot(output[0][t].numpy(),'-o',\n                label='time step %d'%t)\n    pl.grid(); pl.legend(); pl.show()","d57733f6":"def encode(text_tensor,label):\n    text=text_tensor.numpy()\n    encoded_text=encoder.encode(text)\n    return encoded_text,label\ndef encode_fmap(text,label):\n    return tf.py_function(encode,inp=[text,label], \n                          Tout=(tf.int64,tf.int64))","c7c14812":"dhtml('Data Exploration')","c5ff05f5":"# 25,000 movies reviews from IMDB, \n# labeled by sentiment (positive\/negative)\nnum_words=10000; max_length=1000\nembedding_vector_length=32\n(x_train,y_train),(x_test,y_test)=\\\nimdb.load_data(path=\"imdb_full.pkl\",\n               num_words=num_words,\n               skip_top=0,seed=113,\n               maxlen=max_length,\n               start_char=1,oov_char=2,\n               index_from=3)","f80a4822":"print(x_train.shape,x_test.shape)\nprint(y_train.shape,y_test.shape)\nx=np.vstack([x_train.reshape(-1,1),\n             x_test.reshape(-1,1)])\ny=np.vstack([y_train.reshape(-1,1),\n             y_test.reshape(-1,1)])\nx=x.reshape(-1); y=y.reshape(-1)\nprint(x.shape,y.shape)","0f0ea817":"word_to_id=imdb.get_word_index()\nword_to_id=\\\n{k:(v+3) for k,v in word_to_id.items()}\nsw=[\"<PAD>\",\"<START>\",\"<UNK>\",\"<UNUSED>\"]\nfor i in range(4): word_to_id[sw[i]]=i\nid_to_word=\\\n {value:key for key,value in word_to_id.items()}\ndef get_string(x,i):\n    return ' '.join(id_to_word[id] for id in x[i] if id>3)","971036c4":"features=[get_string(x,i) \n          for i in range(x.shape[0])]\nfeatures=np.array(features)\ntargets=y\ndhtml(features[0],c2,f2,fs2)","39020e92":"dhtml('Data Building')","c545f022":"ds=tf.data.Dataset.\\\nfrom_tensor_slices((features,targets))\nfor ex in ds.take(3):\n    tf.print(ex[0].numpy()[:60],ex[1])","31d1be01":"tf.random.set_seed(123)\nds=ds.shuffle(50000,reshuffle_each_iteration=False)\nds_test=ds.take(10000)\nds_train_valid=ds.skip(10000)\nds_valid=ds_train_valid.take(10000)\nds_train=ds_train_valid.skip(10000)","1668cb79":"tokenizer=tfds.features.text.Tokenizer()\ntoken_counts=Counter()\nfor example in ds_train:\n    tokens=tokenizer.tokenize(example[0].numpy())\n    token_counts.update(tokens)\nprint('vocabulary size:',len(token_counts))","22d6be94":"encoder=tfds.features.text\\\n.TokenTextEncoder(token_counts)\nexample_str='hi this is an example of sentences'\nencoder.encode(example_str)","bb0b89b4":"tf.random.set_seed(123)\ntrain=ds_train.map(encode_fmap)\nvalid=ds_valid.map(encode_fmap)\ntest=ds_test.map(encode_fmap)","2eb4c066":"tf.random.set_seed(1)\nfor example in train.shuffle(1000).take(3):\n    print('sequence length:',example[0].shape)\n    print(example[0].numpy())","2a037c61":"ds_example=train.take(20)\nprint('individual sizes:')\nfor example in ds_example:\n    print(example[0].shape)\nds_batched_example=ds_example\\\n.padded_batch(4,padded_shapes=([-1],[]))\nprint('batch dimensions:')\nfor batch in ds_batched_example:\n    print(batch[0].shape)","57ab09b2":"train_data=train.padded_batch(\n    32,padded_shapes=([-1],[]))\nvalid_data=valid.padded_batch(\n    32,padded_shapes=([-1],[]))\ntest_data=test.padded_batch(\n    32,padded_shapes=([-1],[]))","cb3b480a":"for example in train_data.take(1):\n    print(example[0].numpy()[0],'\\n',\n          example[1].numpy())","21e9ba07":"dhtml('Embedding, RNN, LSTM, & GRU Layers')","9c5b61c3":"model=tf.keras.Sequential(\n    name='embedding_structure')\nmodel.add(tkl.Embedding(\n    input_dim=256,output_dim=5,\n    input_length=32,name='embedding_1'))\nmodel.summary()","9e773993":"def rnn_layer(inputs,units):\n    rnn_layer=tkl.SimpleRNN(\n        units=units,use_bias=True,\n        return_sequences=True)\n    rnn_layer.build(\n        input_shape=(None,None,inputs))\n    return rnn_layer","d378e95c":"m=5; inputs=7; units=4\ntf.random.set_seed(123)\nrnn_layer74=rnn_layer(inputs,units)    \nget_weights_shape(rnn_layer74)","3dc581bd":"x_seq=tf.convert_to_tensor(\n    [[1.*(i+1)]*inputs for i in range(m)],\n    dtype=tf.float32)\ncompare_calc(rnn_layer74,x_seq)","0a01a3c0":"model=tf.keras.Sequential(\n    name='simple_rnn_structure')\nmodel.add(tkl.Embedding(1000,32))\nmodel.add(\n    tkl.SimpleRNN(32,return_sequences=True))\nmodel.add(tkl.SimpleRNN(32))\nmodel.add(tkl.Dense(1))\nmodel.summary()","c1eee701":"model=tf.keras.Sequential(\n    name='lstm_structure')\nmodel.add(tkl.Embedding(10000,32))\nmodel.add(\n    tkl.LSTM(32,return_sequences=True))\nmodel.add(tkl.LSTM(32))\nmodel.add(tkl.Dense(1))\nmodel.summary()","8f8fdda4":"model=tf.keras.Sequential(\n    name='gru_structure')\nmodel.add(tkl.Embedding(10000,32))\nmodel.add(\n    tkl.GRU(32,return_sequences=True))\nmodel.add(tkl.GRU(32))\nmodel.add(tkl.Dense(1))\nmodel.summary()","1067a3c1":"dhtml('Predicting Sentiments')","b2685f29":"dhtml(' '.join(\n    [list(token_counts)[i]\n     for i in range(32)]),c2,f2,fs2)","ef809326":"embedding_dim=32\nvocabulary_size=len(token_counts)+2\nmodel=tf.keras.Sequential(\n    name='bi_lstm_model')\nmodel.add(tkl.Embedding(\n    input_dim=vocabulary_size,\n    output_dim=embedding_dim,\n    name='embedding_layer'))\nmodel.add(tkl.Bidirectional(\n    tkl.LSTM(64,name='lstm_layer'),\n    name='bidirect_lstm_layer'))\nmodel.add(tkl.Dense(64,activation='relu',\n                    name='dense_64'))\nmodel.add(tkl.Dense(1,activation='sigmoid',\n                    name='out'))\noptimizer=tf.keras.optimizers.Adam(1e-3)\nloss_fun=tf.keras.losses\\\n.BinaryCrossentropy(from_logits=False)\nmodel.compile(\n    optimizer=optimizer,loss=loss_fun,\n    metrics=['accuracy'])","f2a85fd1":"history=model.fit(\n    train_data,epochs=5,\n    validation_data=valid_data)","91609baa":"model.evaluate(test_data)","3c20e175":"dhtml('Functions in Construction Process')","151766fe":"def preprocess_datasets(\n    ds_train,ds_valid,ds_test,\n    max_seq_len=None,batch_size=32):\n    tokenizer=tfds.features.text.Tokenizer()\n    token_counts=Counter()\n    for ds in [ds_train,ds_valid,ds_test]:\n        for example in ds:\n            tokens=tokenizer.tokenize(\n                example[0].numpy())\n            if max_seq_len is not None:\n                tokens=tokens[-max_seq_len:]\n            token_counts.update(tokens)\n    print('vocabulary size: ',len(token_counts))\n    encoder=tfds.features.text\\\n    .TokenTextEncoder(token_counts)\n    def encode(text_tensor,label):\n        text=text_tensor.numpy()\n        encoded_text=encoder.encode(text)\n        if max_seq_len is not None:\n            encoded_text=encoded_text[-max_seq_len:]\n        return encoded_text,label\n    def encode_fmap(text,label):\n        return tf.py_function(\n            encode,inp=[text,label], \n            Tout=(tf.int64,tf.int64))\n    train=ds_train.map(encode_fmap)\n    valid=ds_valid.map(encode_fmap)\n    test=ds_test.map(encode_fmap)\n    train_data=train.padded_batch(\n        batch_size,padded_shapes=([-1],[]))\n    valid_data=valid.padded_batch(\n        batch_size,padded_shapes=([-1],[]))\n    test_data=test.padded_batch(\n        batch_size,padded_shapes=([-1],[]))\n    return (train_data,valid_data,test_data,\n            len(token_counts))","913a4bd7":"def build_rnn_model(\n    embedding_dim,vocabulary_size,\n    recurrent_type='SimpleRNN',\n    n_rnn_units=64,n_rnn_layers=1,\n    bidirect=True):\n    tf.random.set_seed(123)\n    model=tf.keras.Sequential()\n    model.add(tkl.Embedding(\n        input_dim=vocabulary_size,\n        output_dim=embedding_dim,\n        name='embedding_layer'))\n    for i in range(n_rnn_layers):\n        return_sequences=(i<n_rnn_layers-1)    \n        if recurrent_type=='SimpleRNN':\n            recurrent_layer=tkl.SimpleRNN(\n                units=n_rnn_units, \n                return_sequences=return_sequences,\n                name='simple_rnn_layer{}'.format(i))\n        elif recurrent_type=='LSTM':\n            recurrent_layer=tkl.LSTM(\n                units=n_rnn_units, \n                return_sequences=return_sequences,\n                name='lstm_layer{}'.format(i))\n        elif recurrent_type=='GRU':\n            recurrent_layer=tkl.GRU(\n                units=n_rnn_units, \n                return_seq=return_sequences,\n                name='gru_layer{}'.format(i))     \n        if bidirect:\n            recurrent_layer=tkl.Bidirectional(\n                recurrent_layer,\n                name='bidirect_'+recurrent_layer.name)       \n        model.add(recurrent_layer)\n    model.add(tkl.Dense(64,activation='relu'))\n    model.add(tkl.Dense(1,activation='sigmoid'))\n    return model","c2815c68":"batch_size=32\nmax_seq_len=100\ntrain_data,valid_data,test_data,n=\\\npreprocess_datasets(\n    ds_train,ds_valid,ds_test,\n    max_seq_len=max_seq_len,\n    batch_size=batch_size)","46e8a9be":"vocabulary_size=n+2\nembedding_dim=32\nrnn_model=build_rnn_model(\n    embedding_dim,vocabulary_size,\n    recurrent_type='SimpleRNN', \n    n_rnn_units=64,n_rnn_layers=3,\n    bidirect=True)\nrnn_model.summary()","dbd94743":"optimizer=tf.keras.optimizers.Adam(1e-3)\nloss_fun=tf.keras.losses\\\n.BinaryCrossentropy(from_logits=False)\nrnn_model.compile(\n    optimizer=optimizer,loss=loss_fun,\n    metrics=['accuracy'])\nhistory=rnn_model.fit(\n    train_data,epochs=10,\n    validation_data=valid_data)","92db04b8":"rnn_model.evaluate(test_data)","d9294b03":"<details>\n<summary>\ud83d\udcd3 &nbsp; Modeling Sequential Data Using Recurrent Neural Networks\n<\/summary><br\/>Github Links<br\/>    \n<a href=\"https:\/\/github.com\/rasbt\/python-machine-learning-book-3rd-edition\/blob\/master\/ch16\/ch16_part1.ipynb\">Part 1<\/a><br\/>\n<a href=\"https:\/\/github.com\/rasbt\/python-machine-learning-book-3rd-edition\/blob\/master\/ch16\/ch16_part2.ipynb\">Part 2<\/a><br\/>\n<\/details>"}}