{"cell_type":{"387352e4":"code","1e286f0a":"code","c2f79255":"code","01652d54":"code","c8a766aa":"code","be27806d":"code","23ac9cf0":"code","787f35ee":"code","fc30e060":"code","d575d652":"code","a66ba7ac":"code","95f97257":"code","06f3eda4":"code","d618aca8":"code","afe52231":"code","63d3c44e":"code","7a941c2c":"code","646b039b":"code","7e088a11":"markdown","caa5967f":"markdown","04830298":"markdown","068eff05":"markdown","4fea649e":"markdown","971ee15b":"markdown","0910eeb2":"markdown","80370f3b":"markdown","79c0b6f4":"markdown","641dbabc":"markdown","df882438":"markdown","3ddf6e7c":"markdown","9819d47d":"markdown","a7658e31":"markdown","bbd712d2":"markdown","66b5ade5":"markdown","e426622a":"markdown","4fbb306c":"markdown","73aa3d7a":"markdown","00eddffc":"markdown","3d9288ee":"markdown","2e378ec1":"markdown","49d91c3b":"markdown","9f834241":"markdown","9b3eb962":"markdown","6c65050c":"markdown","44e669f6":"markdown","15ccef96":"markdown","119499b1":"markdown","6734f666":"markdown","336701a6":"markdown","0f3b75b8":"markdown","9d2a8f0d":"markdown","e0f4190b":"markdown","6e56b0df":"markdown","b5afe0e0":"markdown","86e9f09a":"markdown","df070059":"markdown","939a0032":"markdown","fd1f8cf1":"markdown"},"source":{"387352e4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","1e286f0a":"data = pd.read_csv('..\/input\/used-car-dataset-ford-and-mercedes\/skoda.csv')","c2f79255":"data.dtypes","01652d54":"data.describe()","c8a766aa":"g = sns.PairGrid(data, y_vars=[\"price\"], x_vars=[\"year\", \"mileage\", \"tax\", \"mpg\", \"engineSize\"])\ng.map(plt.scatter)","be27806d":"sorted_data= data\n\nfor col in [\"price\", \"year\", \"mileage\", \"tax\", \"mpg\", \"engineSize\"]:\n    sorted_data= sorted_data.sort_values(by=[col])\n    num_of_outliers= len(sorted_data.index)*0.02\n    selected_values= len(sorted_data.index)-round(num_of_outliers,0)\n    sorted_data= sorted_data[:int(selected_values)]\n    \nclean_data= sorted_data\nclean_data","23ac9cf0":"g = sns.PairGrid(clean_data, y_vars=[\"price\"], x_vars=[\"year\", \"mileage\", \"tax\", \"mpg\", \"engineSize\"])\ng.map(plt.scatter)","787f35ee":"clean_coded_data= pd.get_dummies(clean_data, columns=['transmission','model', 'fuelType'])\nclean_coded_data","fc30e060":"# code for scaling values with MinMaxScaler\n\n#from sklearn.preprocessing import MinMaxScaler\n#scaler= MinMaxScaler()\n\n#def scaleColumns(df, cols_to_scale):\n#    for col in cols_to_scale:\n#        df[col] = pd.DataFrame(scaler.fit_transform(pd.DataFrame(clean_coded_data[col])),columns=[col])\n#    return df\n\n#clean_coded_data = scaleColumns(clean_coded_data, ['year','price', 'mileage', 'tax', 'mpg', 'engineSize'])\n#clean_coded_data\n\n# code for standardizing values with StandardScaler\n\n#from sklearn.preprocessing import StandardScaler\n#scaler= StandardScaler()\n\n#def scaleColumns(df, cols_to_scale):\n#    for col in cols_to_scale:\n#        df[col] = pd.DataFrame(scaler.fit_transform(pd.DataFrame(clean_coded_data[col])),columns=[col])\n#    return df\n\n#clean_coded_data = scaleColumns(clean_coded_data, ['year','price', 'mileage', 'tax', 'mpg', 'engineSize'])\n#clean_coded_data","d575d652":"x= clean_coded_data.drop(columns='price')\ny= clean_coded_data['price'].copy()","a66ba7ac":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nxTrain, xTest, yTrain, yTest= train_test_split(x, y, random_state=0, test_size=.30)","95f97257":"from sklearn.linear_model import LinearRegression\n\nregressor= LinearRegression()\nregressor.fit(xTrain, yTrain)\nyPredict= regressor.predict(xTest)\n\nm11= metrics.r2_score(yTest, yPredict)*100\nm21= metrics.mean_absolute_error(yTest, yPredict)\n\nprint('R Square %: ', m11)\nprint('Mean Absolute Error: ', m21)","06f3eda4":"from sklearn.ensemble import RandomForestRegressor\nregressor= RandomForestRegressor(n_estimators=150)\nregressor.fit(xTrain, yTrain)\nyPredict= regressor.predict(xTest)\n\nm12= metrics.r2_score(yTest, yPredict)*100\nm22= metrics.mean_absolute_error(yTest, yPredict)\n\nprint('R Square %: ', m12)\nprint('Mean Absolute Error: ', m22)","d618aca8":"from sklearn.ensemble import GradientBoostingRegressor\nregressor= GradientBoostingRegressor()\nregressor.fit(xTrain, yTrain)\nyPredict= regressor.predict(xTest)\n\nm13= metrics.r2_score(yTest, yPredict)*100\nm23= metrics.mean_absolute_error(yTest, yPredict)\n\nprint('R Square %: ', m13)\nprint('Mean Absolute Error: ', m23)","afe52231":"import random\n\nresults= []\nfor i in range(100):\n    n_est= random.randrange(100,200)\n    max_dep= round(random.uniform(3,10),1)\n    \n    regressor= GradientBoostingRegressor(n_estimators=n_est, max_depth=max_dep)\n    regressor.fit(xTrain, yTrain)\n    yPredict= regressor.predict(xTest)\n    \n    r2= metrics.r2_score(yTest, yPredict)\n    \n    output=[r2, n_est, max_dep]\n    results.append(output)\n    \nresults[:5]","63d3c44e":"r2, n_estimators,max_depth= max(results)\n\nregressor= GradientBoostingRegressor(n_estimators=n_estimators, max_depth=max_depth)\nregressor.fit(xTrain, yTrain)\nyPredict= regressor.predict(xTest)\n\nm13= metrics.r2_score(yTest, yPredict)*100\nm23= metrics.mean_absolute_error(yTest, yPredict)\n\nprint('R Square %: ', m13)\nprint('Mean Absolute Error: ', m23)","7a941c2c":"metric_results= {'Model': ['linear Regression', 'Random Forest', 'Gradient Boosting'], \n                 'R Square': [m11, m12, m13], \n                 'MAE': [m21, m22, m23]}\nmetrics= pd.DataFrame(metric_results)\nmetrics","646b039b":"prediction= pd.DataFrame({'actual price': yTest, 'predicted price': yPredict})\nsns.relplot(data=prediction, x='actual price', y='predicted price')","7e088a11":"Since, we have stored the benchmarks for all models, we now create a pandas data frame to store them in a table.","caa5967f":"We see a clear difference in this plot from the earlier scatter plot. the outliers are removed and the data is now ready for feature engineering.","04830298":"# 3c) Feature Engineering","068eff05":"This project uses database of used Skoda cars in UK @ https:\/\/www.kaggle.com\/adityadesai13\/used-car-dataset-ford-and-mercedes\n\nOur objective is to use machine learning alogrithms to predict the price of a used Skoda car in UK based on features like, Model, Year of introduction, Engine size etc. To achieve the objective we will require the following python libraries:\n1. scikit-learn for devloping prediction models\n2. pandas for data management\n3. seaborn for data visualization\n\nFurther, we will be using the following machine learning logarithms form scikit-learn library:\n1. LinearRegression\n2. RandomForestRegression\n3. GradientBoostingRegressor\n\nMoreover, to evaluate the models we will use two metrics:\n1. R square\n2. Mean Average Error","4fea649e":"Now we will use RandomForestRegressor module form sci-kit learn. RandomForestRegressor works on developing decision trees for prediction. It has single parimeter, n_estimators, which sets the number of trees\/nodes which will be formed for prediction. The higher the trees, the more accurate the prediction will be.\n\n1. regressor variable sets the function RandomForestRegressor() at n_estimators=150. \n2. fit() function is used to train xTrain and yTrain variables which hold our training data.\n3. predict() function is used to predict the price based on xTest which holds our testing data.\n4. predicted values of prices by the logarithm is stored in yPredict column.\n\nm12 and m22 holds the banchmarking valules (r square and mean average error).","971ee15b":"Let's see the data types of values for each columns of the data frame using dtypes method in pandas.","0910eeb2":"We need to configure a pair of n_estimators and max_depth parimeters for GradientBoostingRegressor logarithm to bring maximum prediction accuracy.\n\nOur approach is to run a loop for 100 times which will pick random values of n_estimators and max_depth. These random values will be used to run the logarithm, and the corrosponding r square will be stored in a list named results. The results list will contain another list of 3 values, r square, n_estimators value and max_depth value.","80370f3b":"# 5) Further Scope","79c0b6f4":"Based on this table, we now conclude GradientBoostingRegressor to be efficient in bringing accurate predictions in used car prices. It has explained 95% of the variation in price of used Skoda cars in UK with an average error of below 910 dollars.","641dbabc":"**Parimeter Optimization for Gradient Boosting Regressor**","df882438":"There are 6,000+ entries in the database as well as the means and quantiles for each colum of data can be seen form the stastical summary.\nLet's visualize the scatterplots for each of these columns plotted against price.","3ddf6e7c":"Importing the database of skoda used cars using pandas.\nThe \"data\" variable holds all of our data in pandas data frame.","9819d47d":"We will be using the following logarithms form sci-kit to tain the data:\n1. LinearRegression\n2. RandomForestRegression\n3. GradientBoostingRegressor\n\nFor each logarithm, I have printed the R square and Average Mean Error as benchmarks to campare their results.","a7658e31":"The model can also be used to predict prices for other brands like BMW, Nissan, Toyota etc. \n\nAlthough we have removed 95% of the uncertaninty related to price variations, our mean average error can be improved. An inflated MAE with a high R square can make a model unhelpful.\n\nFinally, as already discussed, our model does a good job for prediction less expensive cars. However, it can be improved in price prediction for expensive used cars. This can be done either adding more data rows on expensive cars or dividing the data into expensive and less expensive cars, and building two different prediction models.","bbd712d2":"Before we being using models, we must split the data into dependent and independent variables. x variable holds the independent colummns and y holds the price column.","66b5ade5":"There are a total of 9 columns in our data frame, 3 of them are catagorical.\nWe now use pandas describe() function for stastical summary of the data.","e426622a":"let's split the data into taining and testing subsets. We have imported the train_test_split module form sci-kit. Further, the size of testing data is set to 30% which is not randomly seleted from data as random_state parimeter is set to 0.\n\nAnother module we have imported is metrics, which will be used for capturing R squre and Mean Average Error for every logarithm we will run.","4fbb306c":"**Using Linear Regression**","73aa3d7a":"Our objective is to predict the price of a used Skoda car in UK based on the following features:\n1. Model of the Skoda car\n2. Year\tof introduction\n3. Transmission\ttype\n4. Mileage of the car\n5. Fuel Type used by the car\n6. Tax on the car\n7. Miles Per Gallon\t\n8. Engine Size of the car","00eddffc":"**Using Random Forest Regressor**","3d9288ee":"We have imported the LinearRegression module form sci-kit learn. LinearRegression alogarithm works on the priciple of minizing the distance of each data point to the line of best fit.\n\n1. regressor variable sets the function LinearRegression() offered by LinearRegression module. \n2. fit() function is used to train xTrain and yTrain variables which hold our training data.\n3. predict() function is used to predict the price based on xTest which holds our testing data.\n4. predicted values of prices by the logarithm is stored in yPredict column.\n\nm11 and m21 holds the banchmarking valules (r square and mean average error).","2e378ec1":"I have used get_dummies() function in pandas to code the catagorical data columns in our clean_data variable.","49d91c3b":"The scatterplots show a linear relationship of variables with price. Hence, this data can be used to feed to a supervised-regression machine learning logarithm for developing predictions.\n\nAdditionally, we also see some outliers in the data which needs to be removed. For this purpose we must proceed with the data cleaning to get rid of them first.","9f834241":"With outliers removed, we have 5552 rows now. To see the difference, let us see the scatter plots for each variables against price using seaborn.","9b3eb962":"# 3) Detailed methodology","6c65050c":"# 3a) Data Description and Visualization","44e669f6":"The Model, Transmission and Fuel type columns are coded properly.\n\nWe observe form the data that some columns have higher values on average than others. For instance, the engizeSize column has values form 0 to 2.5 while the price column has values form 995 to 91874.\n\nMachine learning prediction models work best if the data is scaled or standardized. I used the MinMaxScaler() from sci-kit learn to scale the values of each numerical column. It sets the values between 0-1 hence, devlpoping homoginity of scale in the whole data.\n\nHowever, interestingly, it did not improve my prediction in any way. I then, moved to use StandardScaler(), another function from sci-kit which transforms the values of columns into a normal distribution with mean of 0. Nevertheless, this again, did not bring any imporvement to the prediction.\n\nTherefore, I decided to move on without scaling and standardizing the data. However, I have left the following cell commented to show the logic I used for it.","15ccef96":"Finally, we have used GradientBoostingRegressor module form sci-kit learn. GradientBoostingRegressor is a powerful logarithm based on several weak regression logarithms which are combined to bring accuracy in prediction. It has several parameters but, we will use n_estimators and max-depth in the next steps to optamize the prediction model.\n\n1. regressor variable sets the function GradientBoostingRegressor at default n_estimators=100 and max_depth=3.\n2. fit() function is used to train xTrain and yTrain variables which hold our training data.\n3. predict() function is used to predict the price based on xTest which holds our testing data.\n4. predicted values of prices by the logarithm is stored in yPredict column.\n\nm13 and m23 holds the banchmarking valules (r square and mean average error).","119499b1":"We can observe form the above graph that we can do better to predict the prices of more expensive cars. Our model brings good results in the less expenisve cars.\n","6734f666":"# 2) Problem statement","336701a6":"# 3d) Model Development and Testing\n","0f3b75b8":"We are now able to explain the variation in Prices of used Skoda cars in UK. Our model have removed 95% of the uncertanity in Prices of these cars. Further, with an average error of 910$, we will be able to predict the prices using the following features:\n* Model of the Skoda car\n* Year of introduction\n* Transmission type\n* Mileage of the car\n* Fuel Type used by the car\n* Tax on the car\n* Miles Per Gallon\n* Engine Size of the car\n\nPrice prediction is an important metric for car dealers. It helps them to manage their profits. A car dealer dealing in Skoda cars can use this data to set buying price and manage his profit margin. Further, it also helps in inventory management. As expensive cars are bought less, a dealer can use this prediction model to optamize his inventory of expensive and affordable cars thus, minimizing his inventory costs.\n\nAnother use can be in car modification and repair service. Car modification businesses can set their prices based on features added and removed. For instance, an owner of workshop might be able to see the market value of converting a manual transmission car to automatic. The cofficients of transmission feature will help him see how much transmission type influence price of a car. Thus, he will be able to manage his comissions and profits.\n\nOn the other hand, a customer can make use of this model to know the market value of a used Skoda car. This will help him to pay the true market price for the car. Further, he will be aware of dealers comission and can bargain better while buying.","9d2a8f0d":"# 4) Business Value of Results","e0f4190b":"We need to slect the list in results which has highest r square which has been done using max() function. This brings our optimal configuration of n_estimators and max_depth. Finally, for one last time, we will run the GradientBoostingRegression with these optimal parameter values and update the benchmak values m13 and m23.","6e56b0df":"# 3b) Data Cleaning","b5afe0e0":"Lets import the required libraries which include:\n1. numpy for numerical analysis and as a requirement for working with pandas\n2. pandas for data management\n3. mathplotlib and seaborn for data visualization","86e9f09a":"**Using Gradient Boosting Regressor**","df070059":"In the following cell, I have devloped a loop which takes non-string columns (price, year, mileage, tax, mpg, engineSize). For every column, I have sorted the data and have removed the last 2% of entries. Meaning, I have removed the top 2% of data. Top 2%, because, the outliers are mostly concentrated in higher percentiles. lower percentiles have not been removed.\n\nOnce, the loop deletes top 2% of data for every column, the modified data is stored in new variable called clean_data.","939a0032":"# 1) Introduction","fd1f8cf1":"# 3e) Model Results"}}