{"cell_type":{"2ceecca6":"code","af36d04a":"code","c0819b29":"code","3d94f577":"code","cf236e2f":"code","f68af483":"code","8854e429":"code","cb6cd41a":"code","62198ae8":"code","6b186bb3":"code","f231613a":"code","49fca6db":"markdown","6c63e5fc":"markdown","31c913c6":"markdown","cfc9c80f":"markdown","832678ef":"markdown"},"source":{"2ceecca6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport seaborn as sns\nimport time\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Any results you write to the current directory are saved as output.","af36d04a":"def logloss(y,yp):\n    yp = np.clip(yp,1e-5,1-1e-5)\n    return -y*np.log(yp)-(1-y)*np.log(1-yp)\n    \ndef reverse(tr,te):\n    reverse_list = [0,1,2,3,4,5,6,7,8,11,15,16,18,19,\n                22,24,25,26,27,41,29,\n                32,35,37,40,48,49,47,\n                55,51,52,53,60,61,62,103,65,66,67,69,\n                70,71,74,78,79,\n                82,84,89,90,91,94,95,96,97,99,\n                105,106,110,111,112,118,119,125,128,\n                130,133,134,135,137,138,\n                140,144,145,147,151,155,157,159,\n                161,162,163,164,167,168,\n                170,171,173,175,176,179,\n                180,181,184,185,187,189,\n                190,191,195,196,199]\n    reverse_list = ['var_%d'%i for i in reverse_list]\n    for col in reverse_list:\n        tr[col] = tr[col]*(-1)\n        te[col] = te[col]*(-1)\n    return tr,te\n\ndef scale(tr,te):\n    for col in tr.columns:\n        if col.startswith('var_'):\n            mean,std = tr[col].mean(),tr[col].std()\n            tr[col] = (tr[col]-mean)\/std\n            te[col] = (te[col]-mean)\/std\n    return tr,te\n\ndef getp_vec_sum(x,x_sort,y,std,c=0.5):\n    # x is sorted\n    left = x - std\/c\n    right = x + std\/c\n    p_left = np.searchsorted(x_sort,left)\n    p_right = np.searchsorted(x_sort,right)\n    p_right[p_right>=y.shape[0]] = y.shape[0]-1\n    p_left[p_left>=y.shape[0]] = y.shape[0]-1\n    return (y[p_right]-y[p_left])\n\ndef get_pdf(tr,col,x_query=None,smooth=3):\n    std = tr[col].std()\n    df = tr.groupby(col).agg({'target':['sum','count']})\n    cols = ['sum_y','count_y']\n    df.columns = cols\n    df = df.reset_index()\n    df = df.sort_values(col)\n    y,c = cols\n    \n    df[y] = df[y].cumsum()\n    df[c] = df[c].cumsum()\n    \n    if x_query is None:\n        rmin,rmax,res = -5.0, 5.0, 501\n        x_query = np.linspace(rmin,rmax,res)\n    \n    dg = pd.DataFrame()\n    tm = getp_vec_sum(x_query,df[col].values,df[y].values,std,c=smooth)\n    cm = getp_vec_sum(x_query,df[col].values,df[c].values,std,c=smooth)+1\n    dg['res'] = tm\/cm\n    dg.loc[cm<500,'res'] = 0.1\n    return dg['res'].values\n\ndef get_pdfs(tr):\n    y = []\n    for i in range(200):\n        name = 'var_%d'%i\n        res = get_pdf(tr,name)\n        y.append(res)\n    return np.vstack(y)\n\ndef print_corr(corr_mat,col,bar=0.97):\n    print(col)\n    cols = corr_mat.loc[corr_mat[col]>bar,col].index.values\n    cols_ = ['var_%s'%(i.split('_')[-1]) for i in cols]\n    print(cols)\n    return cols","c0819b29":"%%time\npath = '..\/input\/'\ntr = pd.read_csv('%s\/train.csv'%path)\nte = pd.read_csv('%s\/test.csv'%path)","3d94f577":"%%time\ntr,te = reverse(tr,te)\ntr,te = scale(tr,te)","cf236e2f":"%%time\nprob = get_pdf(tr,'var_0')\nplt.plot(prob)","f68af483":"%%time\npdfs = get_pdfs(tr)","8854e429":"%%time\ndf_pdf = pd.DataFrame(pdfs.T,columns=['var_prob_%d'%i for i in range(200)])\ncorr_mat = df_pdf.corr(method='pearson')","cb6cd41a":"corr_mat.head()","62198ae8":"plt.figure(figsize=(15,10))\nsns.heatmap(corr_mat, cmap='RdBu_r', center=0.0) \nplt.title('PDF Correlations',fontsize=16)\nplt.show() ","6b186bb3":"plt.figure(figsize=(10,5))\nplt.plot(pdfs[0],color='b',label='var_0')\nplt.plot(pdfs[2],color='r',label='var_2')\nplt.legend(loc='upper right')","f231613a":"cols = print_corr(corr_mat,'var_prob_12')\ncorr_mat.loc[cols,cols]","49fca6db":"**We can group features using this correlation matrix. For example, var_0 and var_2's pdfs is 0.97+ correlated. We can confirm it using the figure below.**","6c63e5fc":"In this kernel, I implement vectorized PDF caculation (without for loop) to get their correlation matrix. This is helpful to study feature grouping.\ncredits to @sibmike https:\/\/www.kaggle.com\/sibmike\/are-vars-mixed-up-time-intervals","31c913c6":"**load data & group vars**","cfc9c80f":"**We can find the group of a var using the following functions.**","832678ef":"**Functions**"}}