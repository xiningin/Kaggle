{"cell_type":{"00e85986":"code","e4693c6e":"code","c4aeaf39":"code","79d980a7":"code","d9f0a98f":"code","25e18bdd":"code","9ce7d357":"code","508bbdbe":"code","790af38d":"code","aeb8faef":"code","5befea2b":"code","8ea37b7a":"code","ad6fb856":"code","f8adc9e5":"code","f7b4a9ab":"code","f425ac50":"code","ea13fc33":"code","409a40fc":"code","b895b9a4":"code","49994db9":"code","ca071ff9":"code","c24cd0e4":"code","bedbaa44":"code","555d1f6e":"code","1522f1f1":"code","99bc98b8":"code","8ce5f680":"code","59991dba":"code","fb266ae4":"code","ba82bcf7":"code","0f7a9157":"code","73f021ac":"code","f511283c":"code","3e41432e":"code","fa5c22be":"code","5855bfd0":"code","19621af8":"code","2dde88ca":"code","193e8e29":"code","5afac887":"code","2e6eab26":"code","627f4bee":"code","9ef01a65":"code","3c977ae5":"code","955b3867":"code","81c56fd5":"code","e1464464":"code","8dbfbfd3":"code","e166b981":"code","97d629de":"code","0240422a":"code","df875a2c":"code","07f54eab":"code","6ee90150":"markdown","90bfe790":"markdown","4e8e83d0":"markdown","325c51d2":"markdown","825670d4":"markdown","d9973c1e":"markdown","662ba56a":"markdown","162cf936":"markdown","8dd8432a":"markdown","32cfe69c":"markdown","e2fb7eae":"markdown","2c482acc":"markdown","58cc22f1":"markdown","0abe2bfd":"markdown","d5cb471f":"markdown","27d8ca6f":"markdown","92e822e8":"markdown","4772673f":"markdown","ff57ef8a":"markdown","8490f346":"markdown","473df8d2":"markdown","039057d3":"markdown","40326a68":"markdown","30170133":"markdown","a666bbf2":"markdown","5fbbcf02":"markdown","aba024a7":"markdown","9352b4ab":"markdown","7df22271":"markdown","aee6da3e":"markdown","a0e8d1b1":"markdown"},"source":{"00e85986":"import os\n# Thanks to https:\/\/www.kaggle.com\/dirktheeng\/anserini-bert-squad-for-semantic-corpus-search\/\n# for the code on how to setup Java 11\n!curl -O https:\/\/download.java.net\/java\/GA\/jdk11\/9\/GPL\/openjdk-11.0.2_linux-x64_bin.tar.gz\n!mv openjdk-11.0.2_linux-x64_bin.tar.gz \/usr\/lib\/jvm\/; cd \/usr\/lib\/jvm\/; tar -zxvf openjdk-11.0.2_linux-x64_bin.tar.gz\n!update-alternatives --install \/usr\/bin\/java java \/usr\/lib\/jvm\/jdk-11.0.2\/bin\/java 1\n!update-alternatives --set java \/usr\/lib\/jvm\/jdk-11.0.2\/bin\/java\nos.environ[\"JAVA_HOME\"] = \"\/usr\/lib\/jvm\/jdk-11.0.2\"","e4693c6e":"!pip install pyserini==0.8.1.0\n!pip install transformers\n!pip install geopandas\n!pip install pyLDAvis","c4aeaf39":"!jupyter nbextension enable --py --sys-prefix widgetsnbextension\n","79d980a7":"%%capture\n\n\nfrom IPython.core.display import display, HTML\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport re\nimport json\nimport os\n\nfrom pyserini.search import pysearch\n\n\nfrom IPython.html.widgets import interactive\nfrom ipywidgets import interact, interact_manual\nfrom ipywidgets import Layout, Button, Box, FloatText, Textarea, Dropdown, Label, IntSlider\n\nfrom matplotlib import cm\nimport seaborn as sns\nimport matplotlib.patches as mpatches\nimport ipywidgets as widgets\nfrom mpl_toolkits.axes_grid1 import AxesGrid\nimport geopandas as gpd\n\nimport nltk\nnltk.download('stopwords')","d9f0a98f":"%%capture\n!wget https:\/\/www.dropbox.com\/s\/d6v9fensyi7q3gb\/lucene-index-covid-2020-04-03.tar.gz\n!tar xvfz lucene-index-covid-2020-04-03.tar.gz","25e18bdd":"!du -h lucene-index-covid-2020-04-03","9ce7d357":"searcher = pysearch.SimpleSearcher('lucene-index-covid-2020-04-03\/')\n\ndef search(search_strings, topk=5):\n  columns = ['search', 'rank', 'title', 'score']\n  results_df = pd.DataFrame()\n  for search in search_strings:\n    hits = searcher.search(search)\n    #label_hits.append(hits)\n    print(\"\")\n    print(\"Search term: \", search)\n    print(\"  hits:\", len(hits))\n    scores = [h.score for h in hits]\n    print(\"  mean score:\", np.mean(scores))\n    print(\"\")\n    for i in range(0, min(topk, len(hits))):\n      print(f'{i+1:2} {hits[i].docid} {hits[i].score:.5f} {hits[i].lucene_document.get(\"title\")} {hits[i].lucene_document.get(\"doi\")}')\n      row_df = pd.DataFrame([[search, i+1, hits[i].lucene_document.get(\"title\"), hits[i].score]], columns=columns)\n      results_df = results_df.append(row_df)\n\n  return results_df","508bbdbe":"%%html\n<div class='tableauPlaceholder' id='viz1587056618048' style='position: relative'><noscript><a href='https:&#47;&#47;www.bsg.ox.ac.uk&#47;research&#47;publications&#47;variation-government-responses-covid-19'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ox&#47;Oxford-COVID-19&#47;Geo&#47;1_rss.png' style='border: none' \/><\/a><\/noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' \/> <param name='embed_code_version' value='3' \/> <param name='site_root' value='' \/><param name='name' value='Oxford-COVID-19&#47;Geo' \/><param name='tabs' value='no' \/><param name='toolbar' value='yes' \/><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ox&#47;Oxford-COVID-19&#47;Geo&#47;1.png' \/> <param name='animate_transition' value='yes' \/><param name='display_static_image' value='yes' \/><param name='display_spinner' value='yes' \/><param name='display_overlay' value='yes' \/><param name='display_count' value='yes' \/><param name='filter' value='publish=yes' \/><\/object><\/div>                <script type='text\/javascript'>                    var divElement = document.getElementById('viz1587056618048');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https:\/\/public.tableau.com\/javascripts\/api\/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                <\/script>","790af38d":"!wget https:\/\/raw.githubusercontent.com\/jajsmith\/COVID19NonPharmaceuticalInterventions\/master\/npi_full.csv","aeb8faef":"full_df = pd.read_csv('npi_full.csv')\nfull_df['start_date'] = pd.to_datetime(full_df['start_date'])\nfull_df['end_date'] = pd.to_datetime(full_df['end_date'])\nfull_df['oxford_fiscal_measure_cad'] = full_df['oxford_fiscal_measure_cad'].replace('[\\$,]', '', regex=True).astype(float)","5befea2b":"!wget https:\/\/flatteningthecurve.herokuapp.com\/data\/covid","8ea37b7a":"full_cases = pd.read_csv('covid')","ad6fb856":"intervention_categories = ['S1 School Closing',\n                           'S2 Workplace closing',\n                           'S3 Cancel public events',\n                           'S4 Close public transport',\n                           'S5 Public info campaigns',\n                           'S6 Restrictions on internal movements',\n                           'S7 International travel controls',\n                           'S8 Fiscal measures',\n                           'S9 Monetary measures (interest rate)',\n                           'S10 Emergency investment in health care',\n                           'S11 Investment in vaccines',\n                           'S12 Testing policy',\n                           'S13 Contact tracing']","f8adc9e5":"def parse_rate(string):\n    if type(string) == float:\n        return string\n    cad = string[:-1]\n    return float(cad)\n\ndef impute_intervention(prov):\n    for interv_cat in intervention_categories:\n        prov[interv_cat] = 0\n    closure_geo = ['S1', 'S2', 'S3', 'S4', 'S6']\n    public_geo = ['S5']\n    travel = ['S7']\n    rate = ['S9']\n    fiscal = ['S8', 'S10', 'S11']\n    test = ['S12']\n    trace = ['S13']\n    for idx, row in prov.iterrows():\n        interv = row['oxford_government_response_category']\n        if interv in intervention_categories:\n            interv_prefix = str(interv).split(' ')[0]\n            subset = prov.iloc[:idx+1]\n            subset = subset[subset['oxford_government_response_category'] == interv]\n            if interv_prefix in closure_geo:\n                prov.at[idx, interv] = (np.nanmax(subset['oxford_closure_code']) + np.nanmax(subset['oxford_geographic_target_code'])) * 100 \/ 3\n            elif interv_prefix in public_geo:\n                prov.at[idx, interv] = (np.nanmax(subset['oxford_geographic_target_code']) + np.nanmax(subset['oxford_public_info_code'])) * 100 \/ 2\n            elif interv_prefix in travel:\n                prov.at[idx, interv] = (subset['oxford_travel_code'].max()) * 100 \/ 3\n            elif interv_prefix in rate:\n                prov.at[idx, interv] = subset['oxford_monetary_measure'].apply(parse_rate).sum()\n            elif interv_prefix in fiscal:\n                prov.at[idx, interv] = pd.to_numeric(subset['oxford_fiscal_measure_cad']).sum()\n            elif interv in test:\n                prov.at[idx, interv] = subset['oxford_testing_code'].max() * 100 \/ 2\n            elif interv in trace:\n                prov.at[idx, interv] = subset['oxford_tracing_code'].max() * 100 \/ 2\n            if idx > 0:\n                for i in intervention_categories:\n                    if i != interv:\n                        prov.at[idx, i] = prov.at[idx-1, i]\n        else:\n            if idx > 0:\n                for i in intervention_categories:\n                    prov.at[idx, i] = prov.at[idx-1, i]\n    return prov","f7b4a9ab":"def construct_positive(prov):\n    prov['Cumulative Cases'] = 0\n    for idx, row in prov.iterrows():\n        if idx == 0:\n            prov.at[0, 'Cumulative Cases'] = prov.iloc[0]['Daily Cases']\n        else:\n            prov.at[idx, 'Cumulative Cases'] = prov.iloc[idx-1]['Cumulative Cases'] + prov.iloc[idx]['Daily Cases']\n    \n    return prov","f425ac50":"def generate_cases_province(full_npi, pn, pn_short):\n    prov = full_npi[(full_npi['region'] == pn)]\n    \n    # mb_list = ['Region', pn, pn+'.1', pn+'.2', pn+'.3', pn+'.4', pn+'.5']\n    # mb = mobility[mb_list]\n    # mb = mb.rename(columns={mb_list[0]: 'start_date',\n    #                         mb_list[1]: mobility_list[0],\n    #                         mb_list[2]: mobility_list[1],\n    #                         mb_list[3]: mobility_list[2],\n    #                         mb_list[4]: mobility_list[3],\n    #                         mb_list[5]: mobility_list[4],\n    #                         mb_list[6]: mobility_list[5]\n    #                         })\n    # mb = mb.iloc[1:]\n    # mb['start_date'] = pd.to_datetime(mb['start_date'], format='%d-%m-%Y')\n    # for mb_type in mobility_list:\n    #     mb[mb_type] = pd.to_numeric(mb[mb_type])\n    #     mb[mb_type] = mb[mb_type].apply(lambda x: scale_mob(x, mb[mb_type].min(), mb[mb_type].max()))\n       \n    prov = prov[['start_date', 'region', 'end_date', 'oxford_government_response_category', 'oxford_closure_code',\n       'oxford_public_info_code', 'oxford_travel_code',\n       'oxford_geographic_target_code', 'oxford_fiscal_measure_cad',\n       'oxford_monetary_measure', 'oxford_testing_code', 'oxford_tracing_code']]\n    \n    prov['start_date'] =  pd.to_datetime(prov['start_date'], infer_datetime_format='%m\/%d\/%Y')\n    \n    cs = full_cases[full_cases['province'] == pn_short]\n    cs['start_date'] = pd.to_datetime(cs['date'], format='%Y-%m-%d')\n    cs = cs.groupby('start_date')['id'].agg('count').reset_index().rename(columns={'id': 'Daily Cases'})\n    cs = construct_positive(cs)\n    \n    # prov = pd.merge(prov, mb[['start_date'] + mobility_list], on='start_date', how='outer')\n    prov = pd.merge(prov, cs, on='start_date', how='outer')\n    prov = prov.sort_values(by='start_date',ascending=True).reset_index(drop=True)\n    \n    prov = impute_intervention(prov)\n    prov = prov.drop_duplicates(subset =\"start_date\", \n                     keep = 'last').reset_index(drop=True) \n    \n    \n    return prov","ea13fc33":"full_viz = full_df.copy()\nfull_viz['oxford_geographic_target_code'].fillna(0, inplace=True)\nfull_viz['oxford_closure_code'].fillna(0, inplace=True)\nfull_viz['oxford_public_info_code'].fillna(0, inplace=True)","409a40fc":"on = generate_cases_province(full_viz, 'Ontario', 'Ontario')\nqb = generate_cases_province(full_viz, 'Quebec', 'Quebec')\nbc = generate_cases_province(full_viz, 'British Columbia', 'BC')\nsk = generate_cases_province(full_viz, 'Saskatchewan', 'Saskatchewan')\nnb = generate_cases_province(full_viz, 'New Brunswick', 'New Brunswick')\nns = generate_cases_province(full_viz, 'Nova Scotia', 'Nova Scotia')\nmb = generate_cases_province(full_viz, 'Manitoba', 'Manitoba')\nab = generate_cases_province(full_viz, 'Alberta', 'Alberta')\npei = generate_cases_province(full_viz, 'Prince Edward Island', 'PEI')\nnwt = generate_cases_province(full_viz, 'Northwest Territories', 'NWT')\nnl = generate_cases_province(full_viz, 'Newfoundland and Labrador', 'NL')\nyt = generate_cases_province(full_viz, 'Yukon', 'Yukon')\nnv = generate_cases_province(full_viz, 'Nunavut', 'Nunavut')","b895b9a4":"prov_dict = {'Ontario': on,\n             'Quebec': qb,\n             'British Columbia': bc,\n             'Saskatchewan': sk,\n             'New Brunswick': nb,\n             'Nova Scotia': ns,\n             'Manitoba' : mb,\n             'Alberta' : ab,\n             'Prince Edward Island': pei,\n             'Northwest Territories' : nwt,\n             'Newfoundland and Labrador' : nl,\n             'Yukon': yt,\n             'Nunavut': nv}\nprov_list = ['Ontario', 'Quebec', 'British Columbia', 'Saskatchewan', \n             'New Brunswick', 'Nova Scotia', 'Manitoba', 'Alberta',\n             'Prince Edward Island', 'Northwest Territories', 'Newfoundland and Labrador', 'Yukon', 'Nunavut']","49994db9":"w_prov1 = widgets.Select(description=\"Province 1\", options=prov_list)\nw_prov2 = widgets.Select(description=\"Province 2\", options=prov_list)\nw_intervention_multi = widgets.SelectMultiple(description=\"Different intervention types\",\n                                             options=intervention_categories)\nw_stats = widgets.Select(description=\"Different COVID-19 indicators\",\n                         options=['Daily Cases', 'Cumulative Cases'])\nw_stringency = widgets.Select(description=\"Stringency Index\", options=[1, 33, 50, 66, 100])","ca071ff9":"def compare_provinces_cases(prov1_str, prov2_str, interv_type, stat, stringency_idx):\n    prov1 = prov_dict[prov1_str]\n    prov2 = prov_dict[prov2_str]\n    \n    if stat == \"Daily Cases\":\n        height = 50\n    else:\n        height = 500\n    \n    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, sharey=True, figsize=(15, 10))\n    colors = sns.color_palette(\"Set2\", n_colors=8)\n    legend_labels = [mpatches.Patch(color=colors[0], label=stat)]\n    \n    begin_date = np.datetime64('2020-02-15')\n    \n    if prov1['start_date'].values[-1] > prov2['start_date'].values[-1]:\n        end_date = prov2['start_date'].values[-1]\n    else:\n        end_date = prov1['start_date'].values[-1]\n        \n    ax1.set_xlim(left=begin_date, right=end_date)\n    \n    ax1.bar(prov1['start_date'], prov1[stat], color=colors[0])\n    ax1.set(ylabel=stat, title=prov1_str)\n    \n    ax2.bar(prov2['start_date'], prov2[stat], color=colors[0])\n    ax2.set(xlabel = 'Date', ylabel=stat, title=prov2_str)\n    \n    for idx in range(len(interv_type)):\n        if len(prov1[prov1[interv_type[idx]] >= stringency_idx]['start_date']) :\n            first_date_1 = prov1[prov1[interv_type[idx]] >= stringency_idx]['start_date'].values[0]\n            second_date_1 = first_date_1 + np.timedelta64(14,'D')\n            \n            ax1.axvline(x=first_date_1, linestyle='-', color=colors[1+idx])\n            ax1.text(first_date_1+ np.timedelta64(8, 'h'), height, interv_type[idx],rotation=90)\n            ax1.axvspan(second_date_1, end_date, facecolor= colors[1+idx], alpha=0.4)\n            ax1.text(second_date_1+ np.timedelta64(8, 'h'), height,\n                    interv_type[idx].split(' ')[0] + ' - after 14 days',rotation=90)\n        \n        if len(prov2[prov2[interv_type[idx]] >= stringency_idx]['start_date']) :\n            first_date_2 = prov2[prov2[interv_type[idx]] >= stringency_idx]['start_date'].values[0]\n            second_date_2 = first_date_2 + np.timedelta64(14,'D')\n            \n            ax2.axvline(x=first_date_2, linestyle='-', color=colors[1+idx])\n            ax2.text(first_date_2+ np.timedelta64(8, 'h'),height, interv_type[idx],rotation=90)\n            \n            ax2.axvspan(second_date_2, end_date, facecolor= colors[1+idx], alpha=0.4)\n            ax2.text(second_date_2+ np.timedelta64(8, 'h'), height,\n                    interv_type[idx].split(' ')[0] + ' - after 14 days',rotation=90)\n        \n    \n        legend_labels.append(mpatches.Patch(color=colors[1+idx], label=interv_type[idx]))  \n\n    ax1.legend(handles=legend_labels, loc=2)\n    ax2.legend(handles=legend_labels, loc=2)\n    \n    plt.show()","c24cd0e4":"def view_case_provinces(p1, p2, i, nb, s):\n    display(compare_provinces_cases(p1, p2, i, nb, s))","bedbaa44":"interactive(view_case_provinces, p1=w_prov1, p2=w_prov2, i=w_intervention_multi,\n            nb=w_stats, s=w_stringency)","555d1f6e":"%%html\n<div class='tableauPlaceholder' id='viz1586995220180' style='position: relative'  ><noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;On&#47;OntarioInterventions&#47;Sheet1&#47;1_rss.png' style='border: none' \/><\/a><\/noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' \/> <param name='embed_code_version' value='3' \/> <param name='site_root' value='' \/><param name='name' value='OntarioInterventions&#47;Sheet1' \/><param name='tabs' value='no' \/><param name='toolbar' value='yes' \/><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;On&#47;OntarioInterventions&#47;Sheet1&#47;1.png' \/> <param name='animate_transition' value='yes' \/><param name='display_static_image' value='yes' \/><param name='display_spinner' value='yes' \/><param name='display_overlay' value='yes' \/><param name='display_count' value='yes' \/><param name='filter' value='publish=yes' \/><\/object><\/div>                <script type='text\/javascript'>                    var divElement = document.getElementById('viz1586995220180');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='90%';vizElement.style.height=(divElement.offsetWidth*0.65)+'px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https:\/\/public.tableau.com\/javascripts\/api\/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                <\/script>\n","1522f1f1":"%%html\n<div class='tableauPlaceholder' id='viz1587073081316' style='position: relative'><noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;In&#47;Interventions_All_Provinces&#47;Sheet1&#47;1_rss.png' style='border: none' \/><\/a><\/noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' \/> <param name='embed_code_version' value='3' \/> <param name='site_root' value='' \/><param name='name' value='Interventions_All_Provinces&#47;Sheet1' \/><param name='tabs' value='no' \/><param name='toolbar' value='yes' \/><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;In&#47;Interventions_All_Provinces&#47;Sheet1&#47;1.png' \/> <param name='animate_transition' value='yes' \/><param name='display_static_image' value='yes' \/><param name='display_spinner' value='yes' \/><param name='display_overlay' value='yes' \/><param name='display_count' value='yes' \/><param name='filter' value='publish=yes' \/><\/object><\/div>                <script type='text\/javascript'>                    var divElement = document.getElementById('viz1587073081316');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https:\/\/public.tableau.com\/javascripts\/api\/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                <\/script>","99bc98b8":"full_df['oxford_government_response_category'].astype(str).unique()","8ce5f680":"results_df = search(full_df['oxford_government_response_category'].astype(str).unique())","59991dba":"import os\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\nimport gensim\nfrom gensim import corpora, models\nfrom gensim.models.coherencemodel import CoherenceModel\nfrom gensim.models.ldamodel import LdaModel\nfrom gensim.models.phrases import Phrases, Phraser\nfrom gensim.utils import simple_preprocess\nfrom nltk.corpus import stopwords\nimport pandas\nimport re\nimport pandas\nfrom pprint import pprint","fb266ae4":"engl_df = full_df[full_df['region'] != 'Quebec']\nfull_text = engl_df['source_full_text'].drop_duplicates().astype(str)\ndata = full_text.values\ndata = [re.sub('\\s+', ' ', text) for text in data] # remove new lines\ndata = [re.sub(\"\\'\", \"\", text) for text in data] # remove quotes\nprint(\"Total number of documents: \", len(data))","ba82bcf7":"words = [row.split() for row in data]\n\nprint(words[13:14])\n\n","0f7a9157":"\nbigram = Phrases(words, min_count=30, progress_per=10000)\ntrigram = Phrases(bigram[words], threshold=100)\nbigram_mod = Phraser(bigram)\ntrigram_mod = Phraser(trigram)\n\nbigrams = [b for l in data for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n","73f021ac":"\nstop_words = stopwords.words('english')\n#stop_words.extend(['school', 'Nonpharmaceutical Interventions', 'education'])\n","f511283c":"r = [x.split() for x in full_df['region'].dropna().unique().tolist()]\nr = np.hstack([np.array(x) for x in r])\nsr = [x.split() for x in full_df['subregion'].dropna().unique().tolist()]\nsr = np.hstack([np.array(x) for x in sr])\ngeo_stop_words = np.append(r, sr)\ngeo_stop_words = [x.lower() for x in geo_stop_words]\ngeo_stop_words = [x.replace('(','').replace(')','') for x in geo_stop_words]\n","3e41432e":"stop_words.extend(geo_stop_words)\n","fa5c22be":"def remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\nwords_nostops = remove_stopwords(words)\n\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\nwords_bigrams = make_bigrams(words_nostops)","5855bfd0":"id2word = corpora.Dictionary(words_bigrams)\n\n","19621af8":"texts = words_bigrams\ncorpus = [id2word.doc2bow(text) for text in texts]\n\nlda_model = gensim.models.LdaModel(corpus=corpus,\n                                       id2word=id2word,\n                                       num_topics=35, \n                                       random_state=10,\n                                       chunksize=100,\n                                       passes=1,\n                                       per_word_topics=True)\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","2dde88ca":"Perplexity = lda_model.log_perplexity(corpus)\nprint (\"Perplexity:\", Perplexity)","193e8e29":"coherence_model_lda = CoherenceModel(model=lda_model, texts=words_bigrams, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","5afac887":"start=1\nlimit=50\nstep=2\ncoherence_values = []\nmodel_list = []\nfor num_topics in range(start, limit, step):\n    print('Topics: ', num_topics)\n    lda_model = gensim.models.LdaModel(corpus=corpus,\n                                       id2word=id2word,\n                                       num_topics=num_topics,\n                                       random_state=10,\n                                       chunksize=100,\n                                       passes=4,\n                                       per_word_topics=True)\n    model_list.append(lda_model)\n    coherence_model_lda_c_v = CoherenceModel(model=lda_model, texts=words_bigrams, corpus=corpus, dictionary=id2word, coherence=\"c_v\")\n    coherence_values.append(coherence_model_lda_c_v.get_coherence())\n\n\n","2e6eab26":"# Show graph\nimport matplotlib.pyplot as plt\nx = range(start, limit, step)\nplt.plot(x, coherence_values)\nplt.xlabel(\"Num Topics\")\nplt.ylabel(\"Coherence score\")\nplt.legend((\"CoherenceValues\"), loc='best')\nplt.show()\n\n","627f4bee":"PT=coherence_values[10:40]\nprint(PT)","9ef01a65":"Optimal_N_Topic=35#PT.index(max(PT))+10\nprint(Optimal_N_Topic)","3c977ae5":"texts = words_bigrams\ncorpus = [id2word.doc2bow(text) for text in texts]\n\nOptimal_lda_model = gensim.models.LdaModel(corpus=corpus,\n                                       id2word=id2word,\n                                       num_topics=Optimal_N_Topic, \n                                       random_state=10,\n                                       chunksize=100,\n                                       passes=1,\n                                       per_word_topics=True)\n\npprint(Optimal_lda_model.print_topics())\ndoc_lda = Optimal_lda_model[corpus]","955b3867":"import pyLDAvis\nimport pyLDAvis.gensim  # don't skip this\nimport matplotlib.pyplot as plt\n\n","81c56fd5":"pyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(Optimal_lda_model, corpus, id2word)\nvis","e1464464":"logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.DEBUG)\n","8dbfbfd3":"topics = Optimal_lda_model.print_topics(num_topics=Optimal_N_Topic, num_words=20)\ntopics = [topic[1] for topic in topics]\ntopics = [topic.split('\"') for topic in topics]\nsearch_inputs = [\" \".join(topic_keys[1::2]) for topic_keys in topics]\nsearch_inputs = [''.join([i if ord(i) < 128 else ' ' for i in text]) for text in search_inputs]","e166b981":"results_lda_df = search(search_inputs)","97d629de":"common_results = np.intersect1d(results_df['title'].unique(), results_lda_df['title'].unique())\nprint(\"results in common: \", len(common_results))\ncommon_results","0240422a":"print(\"Baseline average score: \", results_df['score'].mean())\nprint(\"NPI-Context average score: \", results_lda_df['score'].mean())","df875a2c":"new_results = results_lda_df[~results_lda_df['title'].isin(results_df['title'].unique())]\n\nprint(\"Baseline # results: \", results_df.shape[0])\nprint(\"NPI-Context results: \", results_lda_df.shape[0])\nprint(\"New results above baseline: \", new_results.shape[0])","07f54eab":"new_results.sample(4)","6ee90150":"## Read Data and Preprocessing\nIn this step we read the dataset and select the titles ","90bfe790":"## Create the Dictionary and Corpus needed for Topic Modeling\nThe LDA function receives *Dictionary* and *Corpus* as inpuut and provides *Topics* as output\n","4e8e83d0":"# Comparison of Search Results\n","325c51d2":"## New Research Discovered through Context-aware search\n\nThe below results were all additional discoveries through the addition of NPI-context to the document search process that were not found with the baseline method.","825670d4":"### Intervention Stringency by Geography","d9973c1e":"# NPI-Context: Using intervention context to inform literature search with case study\n\n1. We make it easier to conduct literature search for new interventions in specific environments by incorporating the context for each intervention in our search.\n\n2. We demonstrate this method on a newly constructed dataset of Canadian NPIs. To show the benefit we compare using general Oxford intervention categories as search terms with our method and demonstrate that new relevant research is surfaced.\n\n## Introduction\n\nThe competition named \"the COVID-19 Open Research Dataset (CORD-19)\" has been launched to support experts in the healthcare domain quickly and accurately receive answers to their scientific questions related to coronaviruses. We can take advantage of NLP and ML tools to develop improved ways of finding relevant research to guide policy actions taken by governments and organizations around the world. CORD-19 encompasses 40,000 articles about coronaviruses. For the competition, 10 tasks have been proposed. Each task covers some fundamental questions related to COVID 19. In this submission, we focus on answering the questions in the task related to non-pharmaceutical interventions. In particular we aim to answer:\n\n- What do we know about the effectiveness of non-pharmaceutical interventions?\n- What is known about equity and barriers to compliance for non-pharmaceutical interventions?\n\n## Method\n\n### Searching the CORD-19 Dataset\n\nWe make use of the [covidex.io](https:\/\/covidex.io) project using the Anserini information retrieval toolkit via pyserini. All the documents in CORD-19 are indexed in Lucene. We build off the demonstration notebook found [here](https:\/\/colab.research.google.com\/drive\/1mrapJp6-RIB-3u6FaJVa4WEwFdEBOcTe) to setup the lucene index and search functionality.\n\nThanks to [Jimmy Lin](https:\/\/cs.uwaterloo.ca\/~jimmylin\/) from the University of Waterloo and [Kyunghyun Cho](http:\/\/www.kyunghyuncho.me\/) from NYU and their team for building this.\n\n### Building a Dataset of Intervention Events\n\nPolicy makers and researchers around the world use literature review to help each team, organization, and country understand the effectiveness of non-pharmaceutical interventions and barriers to compliance *for their specific circumstances*. Observing the leading countries in COVID-19 response like South Korea and China we see drastically different methods used to intervene. **Making use of country-specific context is an important part of improving search quality.**\n\nIn order to show the effectivenss of this it is important to have an up-to-date and thorough picture of each countries current interventions and how they are being implemented. One has been created for Canada to use as a case study.\n\nThe [howsmyflattening.ca](https:\/\/howsmyflattening.ca) team has compiled a dataset of non-pharmaceutical interventions in Canada with 60 intervention labels, 1838 events, and more than 900 unique information sources. Some of the authors of this notebook are contributors to the Canadian non-pharmaceutical interventions dataset. The dataset can be retrieved on Kaggle [here](https:\/\/www.kaggle.com\/howsmyflattening\/covid19-challenges#npi_canada.csv).\n\n### Intervention Context using Topic Modeling\n\nWe use Latent Dirichlet Allocation [(Blei, et. al., 2003)](http:\/\/www.jmlr.org\/papers\/volume3\/blei03a\/blei03a.pdf) to find topics in the full text announcements recorded for all recorded interventions in the input dataset. We then use keywords from these topics to guide search of relevant documents, comparing and augmenting the search results of the labeled interventions themselves. **Crucially, we are not just modeling topics in existing research, but also in actual interventions to understand the relationships between them.**\n\n\n## Putting it all together - A Case Study in Canada\n\nBelow we use Canadian intervention data as a case study for our approach. We apply topic modeling to the intervention text and compare the search results with our baseline approach and show that the context-keyword generation leads to new, relevant results.","662ba56a":"### Interactive Visualization Setup","162cf936":"Sanity check of index size (should be 1.5G):","8dd8432a":"# Oxford Government Response Tracking","32cfe69c":"## visualization\n*pyLDAvis* is a python package to provide interactive web-based visualization to describe the topics that have been provided by the LDA model. ","e2fb7eae":"## Understanding Canadian Interventions\n\nTODO: write a bit here about the Oxford intervention types and how they are tracked. https:\/\/www.bsg.ox.ac.uk\/research\/research-projects\/coronavirus-government-response-tracker","2c482acc":"# Keyword Selection from LDA Topics on Canadian NPI Full Text (gensim)","58cc22f1":"We creat dictionary using *corpora.Dictionary*","0abe2bfd":"The following map summerizes the intervention policies taken in each county. Selecting an intervention type and a specific day from Jan 1st 2020 to April 2020 would show the level of government response on the map. moving the pointer on each country would show the number of positive cases and intervention level.","d5cb471f":"Let's grab the pre-built index:","27d8ca6f":"## Building the Topic Model\nTo train the LDA model, we need to define 1) the corpus, 2) dictionary and 3)the number of topics. We also need to determine the values of hyperparameters such as *alpha* and *eta*. The defult values of these parametters are $1\/#topic$. Another parameter is *chunksize* that determines the number of documents to be used in each training chunk. Finally *passes* is the total number of training passes. \n","92e822e8":"### Run LDA with the optimal number of topics ","4772673f":"\n## Import Packages\nThe main package that we have used are *gensim*, *nltk* and *pandas*","ff57ef8a":"# Run Search","8490f346":"The topic modeling algorithm that we have utilized in this approach is one well-known generative probabilistic model that is referred to as **Latent Dirichlet Allocation (LDA)**\n\nLDA receives words as an input vector and generates topics which are probability distribution over words based on a generative process. LDA uses a joint probability distribution over both the observed and hidden random variables and compute the posterior distribution (conditional distribution) of the hidden variables given the observed variables. **The fundamental assumption of LDA is that documents can be assigned to multiple topics**. Another assumption is that topics are hidden variables, and words in documents are visible variables. Thus, LDA performs a generative process by receiving words (*apparent variables*) as an input vector to provide topics (*hidden variables*) which are **probability distribution over words**","473df8d2":"### Interactive Interventions and Case Data","039057d3":"\n# Compute Coherence Score\nA good LDA model can provide coherent topics. So its topic coherence is high. ","40326a68":"# Baseline Search: Oxford Response Labels\n\nThese are the oxford indicators found in the Canadian NPI dataset. It can be expected that they would provide a strong baseline for search results that can inform the Canadian response to COVID-19.","30170133":"# Canadian non-pharmaceutical interventions dataset\n\nWe seek to improve on the Oxford dataset by finding individual intervention announcements at the city, province, and national level. This provides the content we can learn from to add context to our search.\n\nTo this end we aided in the construction of the Canadian non-pharmaceutical interventions dataset covers compiled information from **January 1st to March 31st, 2020** across 13 provinces and territories as well as the 20 largest census metropolitan areas in Canada.\nThe 60 individual types of intervention contained in this dataset includes (but not restricted to) government announcements, initiatives, and orders, such as social distancing measures or social and fiscal measures. They are tagged with the appropriate oxford interention indicators when appropriate. The rest of the notebook will use this as the reference dataset for creating visualization and topic modelling.","a666bbf2":" ## Bigram and Trigram \nWe need to provide Bigram and Trigram from the orginal texts. ","5fbbcf02":"## What is an optimal LDA model ? \nHow can we find the best values of Hyperparameters such as number of topics ( Model Hyperparameters) and alpha and beta that are reffered to as Document-Topic Density and and Word-Topic Density, respectivly and known as \"Dirichlet hyperparameters\". ","aba024a7":"# Setup","9352b4ab":"\n## NLTK Stop words","7df22271":"In order to aid efforts to fight the pandemic, we aim to use a standard metric for analyzing the governments responses. The Oxford COVID-19 Government Response Tracker (OxCGRT) has collected the Coronavirus Government Response Tracker Dataset. The dataset is collected and updated in real time by a team of dozens of students and staff at Oxford University. This Dataset is available at this [link](https:\/\/www.bsg.ox.ac.uk\/research\/research-projects\/coronavirus-government-response-tracker).\n They have provided 13 indicators of such responses. 9 of these metrics (S1-S7, S12, and S13) are non-financial policies such as event cancelation and the others (S8-S11) are financial indicators such as monetary measures. The Canadian NPI dataset linked all eligible interventions type to one of the Oxford categories. \n A list of these indicators\n\nEach indicator contains a range of values. For more information on the list of the indicators and the encodings visit the [Encodings](https:\/\/www.bsg.ox.ac.uk\/sites\/default\/files\/2020-04\/BSG-WP-2020-031-v4.0_0.pdf).\nAveraging the stringency numbers gives a composite index and allows us to understand how quickly and strinctly different areas of Canada reacted to COVID-19 over time. The Stringency Index is calculated using only the policy indicators S1 \u2013 S7. This metric is calculated by averaging the normalized values from each indicator. Further details on calculation of this index is provided at: [Calculation details](bsg.ox.ac.uk\/sites\/default\/files\/Calculation%20and%20presentation%20of%20the%20Stringency%20Index.pdf)\n\nNote that this index simply records the number and strictness of government policies and should not be interpreted as \u2018scoring\u2019 the appropriateness or effectiveness of a country\u2019s response.","aee6da3e":"### 1. The number of Topics ","a0e8d1b1":"### Intervention Impact on Mobility Explorer"}}