{"cell_type":{"980ce4f0":"code","69a6049a":"code","5b34b805":"code","6a7dc4d7":"code","fe072679":"code","3341ec7a":"code","bb683e17":"code","a8827d27":"code","c8ae8ced":"code","d5e280e9":"code","054ed61a":"code","ea78f351":"code","16554f17":"code","0a72f37d":"code","dae0c07d":"code","a42eb7c5":"code","efdff642":"code","a30a7417":"code","e62002fb":"code","7c3ba3ab":"code","48ae55e7":"code","fe4bd5c5":"code","f82bd1ce":"markdown","4f49e6ee":"markdown"},"source":{"980ce4f0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n#https:\/\/towardsdatascience.com\/end-to-end-case-study-bike-sharing-demand-dataset-53201926c8db\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","69a6049a":"import pandas as pd\ndf = pd.read_csv('\/kaggle\/input\/bikesfortutorial\/Bikes.csv')","5b34b805":"df.head()","6a7dc4d7":"df.rename(columns = {'hum':'humidity','weathersit':'weather','cnt':'count'},inplace = True)","fe072679":"df.columns","3341ec7a":"df.shape","bb683e17":"from datetime import datetime\ntime = []\nmonth = []\nfor i in df['datetime']:\n    dt_object2 = datetime.strptime(i, \"%Y-%m-%d %H:%M:%S\")\n    time.append(dt_object2.hour)\n    month.append(dt_object2.month)\ndf['time'] = pd.DataFrame(time)\ndf['time'] = df['time'].astype(float)\ndf['month'] = pd.DataFrame(month)\ndf['month'] = df['month'].astype(float)","a8827d27":"df.drop('datetime',axis=1,inplace=True) \ndf.drop('holiday',axis=1,inplace=True) \ndf = df.drop('atemp',axis=1)\ndf = pd.get_dummies(df,columns=['season','weather'],drop_first=True)","c8ae8ced":"for i in df.columns:\n    df[i].fillna(value = df[i].median())","d5e280e9":"df","054ed61a":" work_day = df[df['workingday']==1] \nnon_work_day = df[df['workingday']==0]\n    # Model for registered\nX = work_day.drop(['casual','registered','count'],axis = 1)\ny = work_day.registered\n    # Dividing the data into train and test\n","ea78f351":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X,y,test_size = 0.30 ,random_state = 2)","16554f17":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(x_train,y_train)\ny_pred = regressor.predict(x_test)","0a72f37d":"from sklearn.metrics import mean_squared_error\nmse=mean_squared_error(y_pred,y_test)\nrmse=np.sqrt(mse)\nprint('RMLSE for the data:',rmse)","dae0c07d":"# Finding best parameters for decision tree\nfrom sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor(random_state=0)\ndt_params = {'max_depth':np.arange(1,50,2),'min_samples_leaf':np.arange(2,15)}\n    \nfrom sklearn.model_selection import GridSearchCV\ngs_dt = GridSearchCV(dt,dt_params,cv=3)\ngs_dt.fit(x_train,y_train)\na = gs_dt.best_params_","a42eb7c5":"a","efdff642":"# Training with best parameters\nfrom sklearn.tree import DecisionTreeRegressor\ndtr=DecisionTreeRegressor(max_depth=a['max_depth'],min_samples_leaf= a['min_samples_leaf'])\nmodel = dtr.fit(x_train,y_train)\ny_pred = model.predict(x_test)\n\nfrom sklearn.metrics import mean_squared_log_error\nmsle=mean_squared_log_error(y_pred,y_test)\nrmsle=np.sqrt(msle)\nprint('RMLSE for the data:',rmsle) # For decision tree\n\nfrom sklearn.metrics import mean_squared_error\nmse=mean_squared_error(y_pred,y_test)\nrmse=np.sqrt(mse)\nprint('RMLSE for the data:',rmse)","a30a7417":"# Plotting important features\nimport matplotlib.pyplot as plt\nimportances = dtr.feature_importances_\nplt.title('Registered Feature Importances')\nplt.barh(range(len(importances)), importances, color='g', align='center')\nplt.yticks(range(len(importances)), X.columns)\nplt.xlabel('Relative Importance')\nplt.show()","e62002fb":"# Finding best parameters for RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(random_state=0)\nrf_params = {'n_estimators':np.arange(25,150,25),'max_depth':np.arange(1,11,2),'min_samples_leaf':np.arange(2,15,3)}\n\nfrom sklearn.model_selection import GridSearchCV\ngs_rf = GridSearchCV(rf,rf_params,cv=3)\ngs_rf.fit(x_train,y_train)\nb = gs_rf.best_params_","7c3ba3ab":"# Fitting the model with best params\nRF = RandomForestRegressor(n_estimators=b['n_estimators'],max_depth=b['max_depth'],min_samples_leaf=b['min_samples_leaf'],random_state=0)\nmodel = RF.fit(x_train,y_train)\ny_pred = model.predict(x_test)\n\nfrom sklearn.metrics import mean_squared_log_error\nmsle=mean_squared_log_error(y_pred,y_test)\nrmsle=np.sqrt(msle)\nprint('RMLSE for the data:',rmsle) # For random forest","48ae55e7":"# Plotting important features\nimport matplotlib.pyplot as plt\nimportances = RF.feature_importances_\nplt.title('Registered Feature Importances')\nplt.barh(range(len(importances)), importances, color='g', align='center')\nplt.yticks(range(len(importances)), X.columns)\nplt.xlabel('Relative Importance')\nplt.show()","fe4bd5c5":"x = non_work_day.drop(['casual','registered','count'],axis = 1)\ny = non_work_day.casual\n    # Dividing the data into train and test\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.30 ,random_state = 2)","f82bd1ce":"from sklearn.ensemble import AdaBoostRegressor\nar = AdaBoostRegressor(base_estimator=RF,random_state=0)\nar_params = {'n_estimators':np.arange(25,200,25)}\nfrom sklearn.model_selection import GridSearchCV\ngs_ar = GridSearchCV(ar,ar_params,cv=3)\ngs_ar.fit(x_train,y_train)\nc = gs_ar.best_params_","4f49e6ee":"ab_rf = AdaBoostRegressor(base_estimator=RF,n_estimators=c['n_estimators'],random_state=0)\nmodel = ab_rf.fit(x_train,y_train)\ny_pred = model.predict(x_test)\n\nfrom sklearn.metrics import mean_squared_log_error\nmsle=mean_squared_log_error(y_pred,y_test)\nrmsle=np.sqrt(msle)\nprint('RMLSE for the data:',rmsle) # For Ada-Boost\n"}}