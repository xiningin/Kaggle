{"cell_type":{"fc9d610e":"code","3cbf50da":"code","cb07be75":"code","d92725c6":"code","f7797c46":"code","8fd13a7a":"code","04bc96e5":"code","f7b12ba8":"code","622baade":"code","337148e1":"code","563aced5":"code","9920816b":"code","debb320c":"code","cc661025":"code","c83b169b":"code","3fefb56b":"code","6f230d29":"markdown"},"source":{"fc9d610e":"!pip install -U efficientnet","3cbf50da":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom efficientnet.tfkeras import *\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.applications import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.utils import Sequence,to_categorical\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.losses import *\nfrom tensorflow.keras.regularizers import l1\nimport matplotlib.pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.metrics import roc_auc_score\nfrom tensorflow.keras.metrics import AUC\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.utils import shuffle\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split,StratifiedKFold\nfrom tensorflow.keras.utils import plot_model","cb07be75":"train_images_path='\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'\ntest_images_path='\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/test\/'\ntrain_df=pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/train.csv')\nsample_sub=pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/sample_submission.csv')","d92725c6":"print('Train Data Shape: {}'.format(train_df.shape))\ntrain_df.head()","f7797c46":"print(\"Tensorflow version \" + tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\nprint('Running on TPU ', tpu.master())\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync","8fd13a7a":"gcs_path = KaggleDatasets().get_gcs_path()\ndef format_train_path(st):\n    return gcs_path + '\/jpeg\/train\/' + st + '.jpg'\n\ndef format_test_path(st):\n    return gcs_path + '\/jpeg\/test\/' + st + '.jpg'\n\ntrain_data,val_data=train_test_split(train_df,test_size=0.1)\ntrain_paths = train_data.image_name.apply(format_train_path).values\nval_paths = val_data.image_name.apply(format_train_path).values\n\ntrain_labels = train_data['target'].values\nval_labels = val_data['target'].values\n","04bc96e5":"DIMS=(256,256,3)\nEPOCHS=8","f7b12ba8":"def decode_image(filename,label=None,image_size=(DIMS[0],DIMS[1])):\n    bits=tf.io.read_file(filename)\n    img=tf.image.decode_jpeg(bits,channels=3)\n    img=tf.cast(img,tf.float32)\/255.0\n    img=tf.image.central_crop(img,0.3)\n    img=tf.image.resize(img,image_size)\n    if label is None:\n        return img\n    else:\n        return img, label\n    \ndef data_augment(image, label=None):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    image = tf.image.rot90(image)\n    if label is None:\n        return image\n    else:\n        return image, label\n","622baade":"train_dataset=(tf.data.Dataset.from_tensor_slices((train_paths,train_labels)).map(decode_image,num_parallel_calls=AUTO)\n               .map(data_augment,num_parallel_calls=AUTO).repeat()\n              .shuffle(13)\n              .batch(BATCH_SIZE).prefetch(AUTO))\n\nval_dataset=(tf.data.Dataset.from_tensor_slices((val_paths,val_labels))\n             .map(decode_image,num_parallel_calls=AUTO)\n             .shuffle(13)\n             .batch(BATCH_SIZE)\n             .cache()\n             .prefetch(AUTO))","337148e1":"with strategy.scope():\n    inp=Input(DIMS)\n    x=EfficientNetB7(include_top=False,input_tensor=inp)\n    gap=SeparableConv2D(2048,2,activation='relu',padding='same')(x.output)\n    \n    x_0=Conv2D(512,(1,1),1,padding='same')(x.output)\n    x_0=BatchNormalization()(x_0)\n    x_0=Activation('relu')(x_0)\n    \n    x_1=Conv2D(512,(2,2),1,padding='same')(x.output)\n    x_1=BatchNormalization()(x_1)\n    x_1=Activation('relu')(x_1)\n    \n    x_2=Conv2D(512,(3,3),1,padding='same')(x.output)\n    x_2=BatchNormalization()(x_2)\n    x_2=Activation('relu')(x_2)\n    \n    x=Concatenate()([x_0,x_1,x_2,gap])\n    x=Conv2D(2000,(2,2),strides=2,padding='same')(x)\n    x=BatchNormalization()(x)\n    x=Activation('relu')(x)\n    x=GlobalAveragePooling2D()(x)\n    \n    out=Dense(1,activation='sigmoid')(x)    \n    model=Model(inp,out) \n        \n    model.compile(\n        optimizer=Adam(0.001),\n        loss = 'binary_crossentropy' ,\n        metrics=[AUC()]\n    )\n","563aced5":"STEPS_PER_EPOCH = len(train_labels) \/\/ BATCH_SIZE\nmc=ModelCheckpoint('classifier.h5',monitor='val_loss',save_best_only=True,verbose=1,period=1)\nrop=ReduceLROnPlateau(monitor='val_loss',min_lr=0.0000001,patience=2,mode='min')","9920816b":"history=model.fit(train_dataset,epochs=EPOCHS,steps_per_epoch=STEPS_PER_EPOCH,\n                  validation_data=val_dataset,\n                 callbacks=[mc])","debb320c":"def plot_metrics(metrics,name=['loss','AUC']):\n    epochs = range(1, len(metrics[0]) + 1)\n    plt.plot(epochs, metrics[0], 'b',color='red', label='Training '+name[0])\n    plt.plot(epochs, metrics[1], 'b',color='blue', label='Validation '+name[0])\n    plt.title('Metric Plot')\n    plt.legend()\n    plt.figure()\n    plt.plot(epochs, metrics[2], 'b', color='red', label='Training '+name[1])\n    plt.plot(epochs, metrics[3], 'b',color='blue', label='Validation '+name[1])\n    plt.legend()\n    plt.show()","cc661025":"plot_metrics([history.history['loss'],history.history['val_loss'],\n              history.history['auc'],history.history['val_auc']])","c83b169b":"test_paths = sample_sub.image_name.apply(format_test_path).values\ntest_dataset=(tf.data.Dataset.from_tensor_slices(test_paths)\n             .map(decode_image,num_parallel_calls=AUTO)\n             .batch(BATCH_SIZE))","3fefb56b":"model=load_model('classifier.h5')\npreds=model.predict(test_dataset,verbose=1)\nsample_sub['target'] = preds\nsample_sub.to_csv('submission.csv', index=False)\nsample_sub.head()","6f230d29":"**Set Path and Read DataFrames**"}}