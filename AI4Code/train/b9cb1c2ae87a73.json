{"cell_type":{"9084918c":"code","5faf5d8e":"code","03dc84f5":"code","581dcd9c":"code","744e478e":"code","52db4d8f":"code","54b79cc0":"code","3af09e4c":"code","66d1bd5d":"code","75c4c773":"code","2712c5cb":"code","91a370f8":"code","1e794d9f":"code","c795246f":"code","ca892cd2":"code","10ab7a31":"code","0c08015f":"code","f51fc4a0":"code","b6af7575":"markdown","dd2ddfa2":"markdown","9c5eea40":"markdown","c29ee92e":"markdown","5ea4b51b":"markdown","dc53b04e":"markdown","cab080d3":"markdown","5bd08273":"markdown","aae9822c":"markdown","a7f04062":"markdown","f65c1af0":"markdown","9469ba77":"markdown","df9d6627":"markdown","11ec0cd3":"markdown","ebfa4163":"markdown"},"source":{"9084918c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, AvgPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.callbacks import LearningRateScheduler","5faf5d8e":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ntrain.head(5)","03dc84f5":"missing_data = train.isnull().sum()\nprint(missing_data[missing_data > 0])","581dcd9c":"train_y = train['label']\ntrain_x = train.drop('label', axis = 1)","744e478e":"train_x = train_x \/ 255.0\ntest = test \/ 255.0","52db4d8f":"train_x = train_x.values.reshape(-1, 28, 28, 1)\ntest = test.values.reshape(-1, 28, 28, 1)","54b79cc0":"train_y = to_categorical(train_y)","3af09e4c":"train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size = 0.1, random_state = 2)","66d1bd5d":"plt.imshow(train_x[0][:,:,0], cmap = 'gray')","75c4c773":"model = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', activation = 'relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', activation = 'relu'))\nmodel.add(MaxPool2D(pool_size = (2, 2)))\n# model.add(AvgPool2D(pool_size = (2, 2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))\nmodel.add(MaxPool2D(pool_size = (2, 2), strides = (2,2)))\n# model.add(AvgPool2D(pool_size = (2, 2), strides = (2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))\n\nprint(model.summary())","2712c5cb":"datagen = ImageDataGenerator(zoom_range = 0.1,\n                            height_shift_range = 0.1,\n                            width_shift_range = 0.1,\n                            rotation_range = 10)","91a370f8":"model.compile(loss='categorical_crossentropy', optimizer = Adam(lr=1e-4), metrics=[\"accuracy\"])","1e794d9f":"annealer = LearningRateScheduler(lambda x: 1e-3 * 0.75 ** x)\nfrom keras.callbacks import ReduceLROnPlateau\n# annealer = ReduceLROnPlateau(monitor='val_acc', \n#                             patience=3, \n#                             verbose=1, \n#                             factor=0.5, \n#                             min_lr=0.00001)","c795246f":"history = model.fit_generator(datagen.flow(train_x,train_y, batch_size=16),\n                              epochs = 1, \n                              validation_data = (val_x,val_y),\n                              verbose = 2, \n                              steps_per_epoch = 500, \n                              callbacks=[annealer])","ca892cd2":"final_loss, final_acc = model.evaluate(val_x, val_y, verbose=0)\nprint(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss, final_acc))","10ab7a31":"def show_train_history(train_history, title, train, validation):\n    plt.plot(train_history.history[train])\n    plt.plot(train_history.history[validation])\n    plt.title(title)\n    plt.ylabel(train)\n    plt.xlabel('Epoch')\n    plt.legend(['train', 'validation'], loc = 'upper left')\n    plt.show()\nshow_train_history(history, 'Accuracy', 'acc', 'val_acc')\nshow_train_history(history, 'Loss', 'loss', 'val_loss')","0c08015f":"prediction = model.predict(test)\n# select the indix with the maximum probability\nprediction = np.argmax(prediction, axis = 1)","f51fc4a0":"submission = pd.DataFrame({\"ImageId\": list(range(1,len(prediction)+1)),\n                         \"Label\": prediction})\nsubmission.to_csv(\"submission.csv\", index=False)","b6af7575":"Compile model","dd2ddfa2":"Missing data :","9c5eea40":"Load data :","c29ee92e":"Let's take a look how the img looks like :","5ea4b51b":"Reshape to 28 x 28 x 1 :","dc53b04e":"Train the model:\n* adjusting epochs will get better result. (about 25~30)\n* adjusting batch_size makes the result better\n    * 16 => 0.9874\n    * 50 => 0.9895\n    * 80 => 0.9921","cab080d3":"get train_x and train_y :","5bd08273":"**data genorator :**\nTo generate more image data by rotating, shifting the center, zooming in or out...etc.","aae9822c":"**Train History:**","a7f04062":"**Predict and submit :**","f65c1af0":"Make train_y array of form [...0, 0, ....., 1, 0, 0...] :","9469ba77":"Split validation set :","df9d6627":"Normalization :\n*  make sure each feature to have a similar range so that our gradients don't go out of control, and it'll converge faster.","11ec0cd3":"Set learning rate :\n* If val_acc doesn't get better in 3 epoch, reduce learning rate:\n    * without adjusting learning rate (20 epoch): 0.9883\n    * with adjusting learning rate (20 epoch): 0.9874\n    * without adjusting learning rate (30 epoch): 0.9905\n    * with adjusting learning rate (30 epoch): 0.9910, ReduceLROnPlateau only appeared once.\n* reduce learning rate by 10% every epoch:\n    * 30 epoch => 0.9943\n    * much better\n* reduce learning rate by 25% every epoch:\n    * 30 epoch => 0.9912","ebfa4163":"> **CNN :**\n* convolution: to get local features. increasing convolution layer may get more complicated features.\n* pooling: to scale picture data and get those who are important.\n    * MaxPooling (20 epoch) => 0.9874\n    * AvgPooling (20 epoch) => 0.9817\n    * Max pooling is better for extracting the extreme features\n    * Avgerage pooling sometimes can\u2019t extract good features because it takes all into count \n* drop: to prevent overfit\n* flattern: data here is 28X28, but we need to make it back to one-dimensional data in order to connect them\n* conntect: fully connected"}}