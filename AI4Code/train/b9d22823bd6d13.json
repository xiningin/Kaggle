{"cell_type":{"42fab751":"code","1b4af498":"code","23781661":"code","ae2765a7":"code","f127ccf3":"code","11fa3ca7":"code","f89efb0b":"code","07e1de9b":"code","d90d2b9a":"code","757f776a":"code","78a8f82e":"code","6693388e":"code","1afdb9ef":"code","8f8ef68c":"code","1bda04cc":"code","1b63f8ea":"code","4e1714c0":"code","3a47beb7":"code","1550be69":"code","a23f7740":"code","91495b2d":"code","b4cda4f3":"code","c8cb6995":"code","06aa1923":"code","c53f3ad7":"code","0cb1f6ad":"code","934cc5ad":"code","c4006ed2":"code","f40a1f31":"code","ce3b7261":"code","4f5b8887":"code","a6f838a1":"code","6ca2294e":"code","4aa12bfb":"code","411f0ef3":"code","3279f5ed":"code","f4eab194":"code","2fbb5069":"code","b566ae51":"code","58104a74":"code","3d7c5f54":"code","7e99a956":"code","e120635c":"code","53441464":"code","1819d0ac":"code","b65e6d59":"code","c0b6b3f2":"code","9ee40d45":"code","723ef348":"code","bd708475":"code","115964d9":"code","f14e9607":"code","ca367cb0":"code","40c779cf":"code","81a2db56":"code","04ba4a3f":"code","15bb0721":"code","6d779831":"code","1d44dd48":"code","d7ba3f21":"code","fd08a6dc":"code","2713f12a":"code","4e3a8636":"code","1be88ed1":"code","4d9b0d59":"code","eda33467":"code","990a6ddd":"code","73f0185a":"code","c4af3521":"code","46e57d47":"code","1fda9b0c":"code","9d78eec3":"code","14a467b9":"code","4a562196":"code","a55f362b":"code","456e3155":"code","691aedb7":"code","6237c66a":"code","7c2c766c":"code","91037304":"code","7c7b304e":"code","eb098d3d":"code","de2571dd":"code","15bd3aec":"code","b8c5dc4a":"code","c9d85b5c":"code","41a77547":"code","737b4ec0":"code","577e583a":"code","2ef7ea94":"markdown","3a8fe1c1":"markdown","0f118342":"markdown","e0b78d8d":"markdown","d537c037":"markdown","419fcad4":"markdown","ac9f7fdd":"markdown","57c0002d":"markdown","ed440a2b":"markdown","c9f9bd88":"markdown","67172389":"markdown","080c96f6":"markdown","4e6c952e":"markdown","4c3bef09":"markdown","c89c1801":"markdown","ed80e9f0":"markdown","529a9176":"markdown","feeddf8d":"markdown","d689a2a1":"markdown","26f4c2c9":"markdown","eae6d429":"markdown","21404210":"markdown","d9490361":"markdown","99b11bc6":"markdown","d2f05199":"markdown","fe05dd8f":"markdown","d178f871":"markdown","c41598bf":"markdown","be8fc00f":"markdown","23fc088a":"markdown","de0f4bb4":"markdown","7f2d3f72":"markdown","f4e50c9c":"markdown","8aa62b11":"markdown","86dd6771":"markdown","eff62a30":"markdown","4f027688":"markdown","510129a8":"markdown","ada90769":"markdown","5a89b4d5":"markdown","a989f595":"markdown","9e8be15a":"markdown","2b41d098":"markdown","a3c93e6f":"markdown","34e850d0":"markdown","fe0f60b6":"markdown","b59c7607":"markdown","03f1414d":"markdown","01ad85f4":"markdown","332faa24":"markdown","705c8d5b":"markdown","1d90cbec":"markdown","2621d67a":"markdown","eb1cea7d":"markdown","ac757419":"markdown","aafb289a":"markdown","a6c6b1d9":"markdown","f2d1abe4":"markdown","de44ac05":"markdown","1e2afcfc":"markdown","e829c150":"markdown","0c343402":"markdown","47552820":"markdown","4abc3f03":"markdown","b3dbc3b3":"markdown","f355fc5d":"markdown","61670130":"markdown","3b8b5deb":"markdown","47d5e38b":"markdown","3da3378d":"markdown","e14c351c":"markdown","bde207be":"markdown","483e2aeb":"markdown","ad15afba":"markdown","121f286f":"markdown","23556f77":"markdown","4c179c25":"markdown"},"source":{"42fab751":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom bayes_opt import BayesianOptimization","1b4af498":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\ntrain.set_index('id', inplace=True)\ntest.set_index('id', inplace=True)","23781661":"train.revenue = np.log1p(train.revenue)","ae2765a7":"train.describe(include='all')","f127ccf3":"test.describe(include='all')","11fa3ca7":"train_completeness = pd.DataFrame({'filled': [train.loc[:, col].dropna().count() for col in train.columns],\n                                   'total': [len(train)]*len(train.columns)},\n                                  index=train.columns)\ntrain_completeness.drop('revenue')\ntest_completeness = pd.DataFrame({'filled': [test.loc[:, col].dropna().count() for col in test.columns],\n                                   'total': [len(test)]*len(test.columns)},\n                                 index=test.columns)\n\nf, (ax1, ax2) = plt.subplots(1, 2, sharex=True, figsize=(15,5))\ntrain_completeness.filled.plot.bar(title='train sample', ax=ax1)\ntest_completeness.filled.plot.bar(title='test sample', color='orange', ax=ax2)","f89efb0b":"featurelist = []","07e1de9b":"train['in_collection'] = train.belongs_to_collection.agg(lambda x: 1)\ntrain.loc[train.belongs_to_collection.isna(), 'in_collection'] = 0","d90d2b9a":"train.groupby('in_collection').revenue.plot.hist(alpha=0.3, bins=np.linspace(0, 10, 20), legend=True)","757f776a":"test['in_collection'] = test.belongs_to_collection.agg(lambda x: 1)\ntest.loc[test.belongs_to_collection.isna(), 'in_collection'] = 0\n\nfeaturelist += ['in_collection']","78a8f82e":"train['nmovies_in_collection'] = train.belongs_to_collection.apply(\n    lambda x: train.loc[train.belongs_to_collection == x].belongs_to_collection.count())\ntrain.loc[train.belongs_to_collection.isna(), 'nmovies_in_collection'] = 1\ntrain.boxplot(by='nmovies_in_collection', column='revenue', figsize=(8,5))","6693388e":"train.groupby('belongs_to_collection').revenue.std().plot.hist(alpha=0.3)","1afdb9ef":"train_collections = set(train.belongs_to_collection.unique())\ntest_collections = set(test.belongs_to_collection.unique())\nprint('number of unique collections:  train: {}, test: {}'.format(len(train_collections), len(test_collections)))","8f8ef68c":"print('number of collections present in both samples: {}'.format(len(train_collections.intersection(test_collections))))","1bda04cc":"train['median_revenue_collection'] = train.belongs_to_collection.apply(\n    lambda x: train.loc[train.belongs_to_collection == x].revenue.median())\ntrain.median_revenue_collection.fillna(train.loc[train.in_collection == 0].revenue.median(), inplace=True)\n\ntest['median_revenue_collection'] = test.belongs_to_collection.apply(\n    lambda x: train.loc[train.belongs_to_collection == x].revenue.median())\ntest.median_revenue_collection.fillna(train.loc[train.in_collection == 0].revenue.median(), inplace=True)","1b63f8ea":"train.plot.scatter('median_revenue_collection', 'revenue', alpha=0.2)\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(train.median_revenue_collection, train.revenue)))","4e1714c0":"featurelist += ['median_revenue_collection']","3a47beb7":"train.budget = np.log1p(train.budget.replace(0, pd.concat([train, test], sort=False).budget.median()))\ntest.budget = np.log1p(test.budget.replace(0, pd.concat([train, test], sort=False).budget.median()))","1550be69":"train.plot.scatter(x='budget', y='revenue', alpha=0.1)\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(train.budget, train.revenue)))","a23f7740":"featurelist += ['budget']","91495b2d":"genres = set([genre['name'] for genre in eval(\"+\".join(\n    [g for g in pd.concat([train, test], sort=False).genres.fillna(\n    \"[{'id':999, 'name':'NoGenre'}]\")]))])\nprint(len(genres), 'different genres')","b4cda4f3":"# extract only those genres members that occur in train\ngenre_train = set([genre['name'] for genre in eval(\"+\".join(\n    [c for c in train.genres.fillna(\n    \"[{'id':999, 'name':'NoGenre'}]\")]))])\n\n# calculate cumulative revenue per genre\ncum_rev_genre = [train.loc[train.genres.fillna(\n    \"[{'id':999, 'name':'NoGenre'}]\").str.contains(\"'name': '{}'\".format(c)), 'revenue'].agg('exp').sum()\n                  for c in genre_train]\n\n# sort genres based on revenue\ncum_rev_genre = pd.Series(cum_rev_genre, index=genre_train).sort_values(ascending=False)\n\n# scale the cumulative revenue\ncumulative_revenue = cum_rev_genre.cumsum()\/cum_rev_genre.cumsum()[-1]","c8cb6995":"f, ax = plt.subplots(figsize=(12,5))\n\ncumulative_revenue.plot.line(rot=90, ax=ax)\n\nax.set_xticks(range(len(cumulative_revenue)))\nax.set_xticklabels(list(cumulative_revenue.index))\nax.set_ylabel('cumulative revenue')","06aa1923":"genre_weights = []\n\n# define tiers and weighting scheme; high-revenue = high weight\nweight = 10\nthreshold = 0.1\nfor i in range(len(cumulative_revenue)): \n    if cumulative_revenue[i] > threshold:\n        weight -= 1\n        threshold += 0.1\n    genre_weights.append(weight)\n\ngenre_weights = pd.Series(genre_weights + [0], \n                          index=list(cum_rev_genre.index) + [\"NoGenre\"])","c53f3ad7":"train['genre_weight'] = train.genres.fillna(\n    \"[{'id':999, 'name':'NoGenre'}]\").apply(\n    lambda x: np.sqrt(np.sum([genre_weights[c['name']]**2 for c in eval(x)])))\n\n# genre members that occur in test but not in train get zero weights assigned (no revenue information available)\ntest['genre_weight'] = test.genres.fillna(\n    \"[{'id':999, 'name':'NoGenre'}]\").apply(\n    lambda x: np.sqrt(np.sum([genre_weights[c['name']]**2 \n                              if c['name'] in genre_train else 0 \n                              for c in eval(x)])))","0cb1f6ad":"train.loc[[15, 24], ['original_title', 'genre_weight', 'revenue']]","934cc5ad":"f, ax = plt.subplots()\n\nax.scatter(train.genre_weight, train.revenue, alpha=0.1)\nax.set_xlabel('genre_weight')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(train.genre_weight, train.revenue)))","c4006ed2":"featurelist += ['genre_weight']","f40a1f31":"train.homepage.fillna(0, inplace=True)\ntrain.loc[train.homepage != 0, 'homepage'] = 1","ce3b7261":"train.groupby('homepage').revenue.plot.hist(alpha=0.3, bins=np.linspace(0, 10, 20), legend=True)","4f5b8887":"train.loc[train.homepage == 1, 'revenue'].agg(['mean', 'std'])","a6f838a1":"train.loc[train.homepage == 0, 'revenue'].agg(['mean', 'std'])","6ca2294e":"languages = pd.concat([train, test], sort=False).original_language.unique()\nprint(len(languages), 'different languages')","4aa12bfb":"language_counts = pd.concat([train, test], sort=False).original_language.value_counts()\n\n# rename languages with less than 10 occurences to 'ot' for 'other'\nfor lang in language_counts[language_counts < 10].index:\n    train.loc[train.original_language == lang, 'original_language'] = 'ot'\n    test.loc[test.original_language == lang, 'original_language'] = 'ot'\n    \nlanguages = pd.concat([train, test], sort=False).original_language.unique()\nprint('languages used in more than 10 movies:', languages)","411f0ef3":"# extract only those languages that occur in train\nolang_train = train.original_language.unique()\n\n# calculate cumulative revenue per olang member\ncum_rev_olang = [train.loc[train.original_language == l, 'revenue'].agg('exp').sum()\n                  for l in olang_train]\ncum_rev_olang = pd.Series(cum_rev_olang, index=olang_train).sort_values(ascending=False)\n\ncumulative_revenue = cum_rev_olang.cumsum()\/cum_rev_olang.cumsum()[-1]\n\nolang_weights = []\n\n# define tiers and weighting scheme; high-revenue = high weight\nweight = 10\nthreshold = 0.1\nfor i in range(len(cumulative_revenue)): \n    if cumulative_revenue[i] > threshold:\n        weight -= 1\n        threshold += 0.1\n    olang_weights.append(weight)\n\nolang_weights = pd.Series(olang_weights + [0], \n                          index=list(cum_rev_olang.index) + [\"xx\"])\n\ntrain['olang_weight'] = train.original_language.map(olang_weights)\n\n# olang members that occur in test but not in train get zero weights assigned (no revenue information available)\ntest['olang_weight'] = test.original_language.map(olang_weights).fillna(0)","3279f5ed":"f, ax = plt.subplots()\n\nax.scatter(train.olang_weight, train.revenue, alpha=0.1)\nax.set_xlabel('olang_weight')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(train.olang_weight, train.revenue)))","f4eab194":"featurelist += ['olang_weight']","2fbb5069":"titlelen = train.original_title.apply(lambda x: len(x))\n\nf, ax = plt.subplots()\nax.scatter(titlelen, train.revenue, alpha=0.1)\nax.set_xlabel('title length in characters')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(titlelen, train.revenue)))","b566ae51":"overviewlen = train.overview.fillna('notext').apply(lambda x: len(x))\n\nf, ax = plt.subplots()\nax.scatter(overviewlen, train.revenue, alpha=0.1)\nax.set_xlabel('overview length in characters')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(overviewlen, train.revenue)))","58104a74":"f, ax = plt.subplots()\n\nax.scatter(np.log(train.popularity), train.revenue, alpha=0.1)\nax.set_xlabel('log(popularity)')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(np.log(train.popularity), train.revenue)))","3d7c5f54":"train.popularity = np.log1p(train.popularity)\ntest.popularity = np.log1p(test.popularity)\nfeaturelist += ['popularity']","7e99a956":"companies = set([company['name'] for company in eval(\"+\".join(\n    [c for c in pd.concat([train, test], sort=False).production_companies.dropna()]))])\nprint(len(companies), 'different production companies')","e120635c":"# calculate cumulative revenue per company\ncum_rev_companies = [train.loc[train.production_companies.fillna('').str.contains(c), 'revenue'].agg('exp').sum()\n                     for c in companies]\ncum_rev_companies = pd.Series(cum_rev_companies, index=companies).sort_values(ascending=False)\n\ncumulative_revenue = cum_rev_companies.cumsum()\/cum_rev_companies.cumsum()[-1]\n\ncompany_weights = []\n\n# define tiers and weighting scheme; high-revenue = high weight\nweight = 10\nthreshold = 0.1\nfor i in range(len(cumulative_revenue)): \n    if cumulative_revenue[i] > threshold:\n        weight -= 1\n        threshold += 0.1\n    company_weights.append(weight)\n\ncompany_weights = pd.Series(company_weights + [0], index=list(cum_rev_companies.index) + [\"nocompany\"])\n\ntrain['production_companies_weight'] = train.production_companies.fillna(\"[{'name': 'nocompany'}]\").apply(\n    lambda x: np.sqrt(np.sum([company_weights[company['name']]**2 for company in eval(x)])))\n\ntest['production_companies_weight'] = test.production_companies.fillna(\"[{'name': 'nocompany'}]\").apply(\n    lambda x: np.sqrt(np.sum([company_weights[company['name']]**2 for company in eval(x)])))","53441464":"f, ax = plt.subplots()\n\nax.scatter(train.production_companies_weight, train.revenue, alpha=0.1)\nax.set_xlabel('production_companies_weight')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(train.production_companies_weight, train.revenue)))","1819d0ac":"featurelist += ['production_companies_weight']","b65e6d59":"countries = set([company['name'] for company in eval(\"+\".join(\n    [c for c in pd.concat([train, test], sort=False).production_countries.dropna()]))])\nprint(len(countries), 'different production countries')","c0b6b3f2":"# calculate cumulative revenue per country\ncum_rev_countries = [train.loc[train.production_countries.fillna('').str.contains(c), 'revenue'].agg('exp').sum()\n                     for c in countries]\ncum_rev_countries = pd.Series(cum_rev_countries, index=countries).sort_values(ascending=False)\n\ncumulative_revenue = cum_rev_countries.cumsum()\/cum_rev_countries.cumsum()[-1]\n\ncountry_weights = []\n\n# define tiers and weighting scheme; high-revenue = high weight\nweight = 10\nthreshold = 0.1\nfor i in range(len(cumulative_revenue)): \n    if cumulative_revenue[i] > threshold:\n        weight -= 1\n        threshold += 0.1\n    country_weights.append(weight)\n\ncountry_weights = pd.Series(country_weights + [0], index=list(cum_rev_countries.index) + [\"nocountry\"])\n\ntrain['production_countries_weight'] = train.production_countries.fillna(\"[{'name': 'nocountry'}]\").apply(\n    lambda x: np.sqrt(np.sum([country_weights[country['name']]**2 for country in eval(x)])))\n\ntest['production_countries_weight'] = test.production_countries.fillna(\"[{'name': 'nocountry'}]\").apply(\n    lambda x: np.sqrt(np.sum([country_weights[country['name']]**2 for country in eval(x)])))","9ee40d45":"f, ax = plt.subplots()\n\nax.scatter(train.production_countries_weight, train.revenue, alpha=0.1)\nax.set_xlabel('production_countries_weight')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(train.production_countries_weight, train.revenue)))","723ef348":"featurelist += ['production_countries_weight']","bd708475":"train.release_date = train.release_date.apply(\n        lambda x: \"{}\/{}\/{}\".format(\n            x.split('\/')[0],\n            x.split('\/')[1],\n            (str(20)+x.split('\/')[2] if (float(x.split('\/')[2]) < 20) \n                                     else str(19)+x.split('\/')[2])))\n\ntest.release_date = test.release_date.fillna(test.release_date.iloc[1000]).apply(\n        lambda x: \"{}\/{}\/{}\".format(\n            x.split('\/')[0],\n            x.split('\/')[1],\n            (str(20)+x.split('\/')[2] if (float(x.split('\/')[2]) < 20) \n                                     else str(19)+x.split('\/')[2])))","115964d9":"train.release_date = pd.to_datetime(train.release_date)\ntest.release_date = pd.to_datetime(test.release_date)","f14e9607":"release_year = train.release_date.apply(lambda x: x.year)\nf, ax = plt.subplots()\n\nax.scatter(release_year, train.revenue, alpha=0.1)\nax.set_xlabel('release_year')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(release_year, train.revenue)))","ca367cb0":"train['release_year'] = pd.cut(train.release_date.apply(lambda x: x.year), \n                               bins=np.arange(1920, 2030, 10), right=True, labels=False)\ntest['release_year'] = pd.cut(test.release_date.apply(lambda x: x.year), \n                               bins=np.arange(1920, 2030, 10), right=True, labels=False)","40c779cf":"release_month = train.release_date.apply(lambda x: x.month)\nf, ax = plt.subplots()\n\nax.scatter(release_month, train.revenue, alpha=0.1)\nax.set_xlabel('release_month')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(release_month, train.revenue)))","81a2db56":"release_season = train.release_date.apply(lambda x: np.sin(x.month\/12*np.pi))\n# winter is 0, summer is 1 (northern hemisphere)\n\nf, ax = plt.subplots()\n\nax.scatter(release_season, train.revenue, alpha=0.1)\nax.set_xlabel('release_month')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(release_season, train.revenue)))","04ba4a3f":"release_day = train.release_date.apply(lambda x: x.day)\n\nf, ax = plt.subplots()\n\nax.scatter(release_day, train.revenue, alpha=0.1)\nax.set_xlabel('release_day')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(release_day, train.revenue)))","15bb0721":"pd.concat([train, test], sort=False).runtime.replace(0, np.nan).fillna(train.runtime.median()).plot.hist(\n    alpha=0.3, bins=np.arange(0, 400, 30), logy=True)","6d779831":"f, ax = plt.subplots()\n\nax.scatter(train.runtime.replace(0, np.nan).fillna(train.runtime.median()), train.revenue, alpha=0.1)\nax.set_xlabel('runtime (min)')\nax.set_ylabel('log(revenue)')\nax.set_xticks([0, 60, 90, 120, 180, 360])\nax.grid()","1d44dd48":"train['runtime_bin'] = pd.cut(train.runtime.replace(0, np.nan).fillna(train.runtime.median()), \n                              bins=[0, 60, 90, 120, 180, 360], right=True, include_lowest=True, labels=False)\ntest['runtime_bin'] = pd.cut(test.runtime.replace(0, np.nan).fillna(train.runtime.median()), \n                             bins=[0, 60, 90, 120, 180, 360], right=True, include_lowest=True, labels=False)","d7ba3f21":"featurelist += ['runtime_bin']","fd08a6dc":"languages = set([lang['name'] for lang in eval(\"+\".join(\n    [l for l in pd.concat([train, test], sort=False).spoken_languages.dropna()]))])\nprint(len(languages), 'different spoken languages')","2713f12a":"# extract only those languages that occur in train\nlanguages_train = set([lang['name'] for lang in eval(\"+\".join(\n    [l for l in train.spoken_languages.dropna()]))])\n\n# calculate cumulative revenue per language\ncum_rev_languages = [train.loc[train.spoken_languages.fillna('').str.contains(l), 'revenue'].agg('exp').sum()\n                     for l in languages_train]\ncum_rev_languages = pd.Series(cum_rev_languages, index=languages_train).sort_values(ascending=False)\n\ncumulative_revenue = cum_rev_languages.cumsum()\/cum_rev_languages.cumsum()[-1]\n\nlanguage_weights = []\n\n# define tiers and weighting scheme; high-revenue = high weight\nweight = 10\nthreshold = 0.1\nfor i in range(len(cumulative_revenue)): \n    if cumulative_revenue[i] > threshold:\n        weight -= 1\n        threshold += 0.1\n    language_weights.append(weight)\n\nlanguage_weights = pd.Series(language_weights + [0], \n                             index=list(cum_rev_languages.index) + [\"nolang\"])\n\ntrain['spoken_languages_weight'] = train.spoken_languages.fillna(\"[{'name': 'nolang'}]\").apply(\n    lambda x: np.sqrt(np.sum([language_weights[lang['name']]**2 for lang in eval(x)])))\n\n# languages that occur in test but not in train get zero weights assigned (no revenue information available)\ntest['spoken_languages_weight'] = test.spoken_languages.fillna(\"[{'name': 'nolang'}]\").apply(\n    lambda x: np.sqrt(np.sum([language_weights[lang['name']]**2 \n                              if lang['name'] in languages_train else 0 \n                              for lang in eval(x)])))","4e3a8636":"f, ax = plt.subplots()\n\nax.scatter(train.spoken_languages_weight, train.revenue, alpha=0.1)\nax.set_xlabel('spoken_languages_weight')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(train.spoken_languages_weight, train.revenue)))","1be88ed1":"featurelist += ['spoken_languages_weight']","4d9b0d59":"pd.concat([train, test], sort=False).status.unique()","eda33467":"train = train.loc[train.status == 'Released']\nlen(train)","990a6ddd":"train.tagline.fillna(0, inplace=True)\ntrain.loc[train.tagline != 0, 'tagline'] = 1\n\ntrain.groupby('tagline').revenue.plot.hist(alpha=0.3, bins=np.linspace(0, 10, 20), legend=True)","73f0185a":"titlelen = train.title.apply(lambda x: len(x))\n\nf, ax = plt.subplots()\nax.scatter(titlelen, train.revenue, alpha=0.1)\nax.set_xlabel('title length in characters')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(titlelen, train.revenue)))","c4af3521":"keywords_train = set([keyword['name'] for keyword in eval(\"+\".join([k for k in train.Keywords.dropna()]))])\nprint('{} different keywords like {}...'.format(len(keywords_train), \", \".join(list(keywords_train)[:10])))","46e57d47":"cum_rev_keywords = [train.loc[train.Keywords.fillna('').str.contains(l), 'revenue'].agg('exp').sum()\n                    for l in keywords_train]\ncum_rev_keywords = pd.Series(cum_rev_keywords, index=keywords_train).sort_values(ascending=False)\ncumulative_revenue = cum_rev_keywords.cumsum()\/cum_rev_keywords.cumsum()[-1]","1fda9b0c":"cumulative_revenue.plot.line()","9d78eec3":"len(cumulative_revenue[cumulative_revenue < 0.5])","14a467b9":"train['hot_keyword'] = [1 if any([keyword['name'] in cumulative_revenue[cumulative_revenue < 0.5] \n                                  for keyword in eval(x)]) else 0\n                        for x in train.Keywords.fillna(\"''\")]\n\ntest['hot_keyword'] = [1 if any([keyword['name'] in cumulative_revenue[cumulative_revenue < 0.5] \n                                  for keyword in eval(x)]) else 0\n                        for x in test.Keywords.fillna(\"''\")]","4a562196":"train.groupby('hot_keyword').revenue.plot.hist(alpha=0.3, bins=np.linspace(0, 10, 20), legend=True)","a55f362b":"actors = set([actor['name'] for actor in eval(\"+\".join(\n    [a for a in pd.concat([train, test], sort=False).cast.dropna()]))])\nprint(len(actors), 'different actors')","456e3155":"# extract only those actors that occur in train\nactors_train = set([actor['name'] for actor in eval(\"+\".join(\n    [a for a in train.cast.dropna()]))])\n\n# calculate cumulative revenue per actor\ncum_rev_actors = [train.loc[train.cast.fillna('').str.contains(\"'name': '{}'\".format(a)), 'revenue'].agg('exp').sum()\n                  for a in actors_train]\ncum_rev_actors = pd.Series(cum_rev_actors, index=actors_train).sort_values(ascending=False)\n\ncumulative_revenue = cum_rev_actors.cumsum()\/cum_rev_actors.cumsum()[-1]\n\nactor_weights = []\n\n# define tiers and weighting scheme; high-revenue = high weight\nweight = 10\nthreshold = 0.1\nfor i in range(len(cumulative_revenue)): \n    if cumulative_revenue[i] > threshold:\n        weight -= 1\n        threshold += 0.1\n    actor_weights.append(weight)\n\nactor_weights = pd.Series(actor_weights + [0], \n                          index=list(cum_rev_actors.index) + [\"noactor\"])\n\ntrain['cast_weight'] = train.cast.fillna(\"[{'name': 'noactor'}]\").apply(\n    lambda x: np.sqrt(np.sum([actor_weights[a['name']]**2 for a in eval(x)])))\n\n# actors that occur in test but not in train get zero weights assigned (no revenue information available)\ntest['cast_weight'] = test.cast.fillna(\"[{'name': 'noactor'}]\").apply(\n    lambda x: np.sqrt(np.sum([actor_weights[a['name']]**2 \n                              if a['name'] in actors_train else 0 \n                              for a in eval(x)])))","691aedb7":"cum_rev_actors.iloc[:10]","6237c66a":"f, ax = plt.subplots()\n\nax.scatter(np.log1p(train.cast_weight), train.revenue, alpha=0.1)\nax.set_xlabel('cast_weight')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(train.cast_weight, train.revenue)))","7c2c766c":"featurelist += ['cast_weight']","91037304":"crew = set([crew['name'] for crew in eval(\"+\".join(\n    [c for c in pd.concat([train, test], sort=False).crew.dropna()]))])\nprint(len(crew), 'different crew members')","7c7b304e":"# extract only those crew members that occur in train\ncrew_train = set([crew['name'] for crew in eval(\"+\".join(\n    [c for c in train.crew.dropna()]))])\n\n# calculate cumulative revenue per crew member\ncum_rev_crew = [train.loc[train.crew.fillna('').str.contains(\"'name': '{}'\".format(c)), 'revenue'].agg('exp').sum()\n                  for c in crew_train]\ncum_rev_crew = pd.Series(cum_rev_crew, index=crew_train).sort_values(ascending=False)\n\ncumulative_revenue = cum_rev_crew.cumsum()\/cum_rev_crew.cumsum()[-1]\n\ncrew_weights = []\n\n# define tiers and weighting scheme; high-revenue = high weight\nweight = 10\nthreshold = 0.1\nfor i in range(len(cumulative_revenue)): \n    if cumulative_revenue[i] > threshold:\n        weight -= 1\n        threshold += 0.1\n    crew_weights.append(weight)\n\ncrew_weights = pd.Series(crew_weights + [0], \n                          index=list(cum_rev_crew.index) + [\"nocrew\"])\n\ntrain['crew_weight'] = train.crew.fillna(\"[{'name': 'nocrew'}]\").apply(\n    lambda x: np.sqrt(np.sum([crew_weights[c['name']]**2 for c in eval(x)])))\n\n# crew members that occur in test but not in train get zero weights assigned (no revenue information available)\ntest['crew_weight'] = test.crew.fillna(\"[{'name': 'noactor'}]\").apply(\n    lambda x: np.sqrt(np.sum([crew_weights[c['name']]**2 \n                              if c['name'] in crew_train else 0 \n                              for c in eval(x)])))","eb098d3d":"f, ax = plt.subplots()\n\nax.scatter(np.log1p(train.crew_weight), train.revenue, alpha=0.1)\nax.set_xlabel('crew_weight')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(train.crew_weight, train.revenue)))","de2571dd":"featurelist += ['crew_weight']","15bd3aec":"random_state = 1\nn_folds = 5\n\ndef prepare_params(n_estimators, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features,\n                  max_leaf_nodes, min_impurity_decrease):\n    params = {'n_jobs': -1, 'random_state': 42}\n    params['n_estimators'] = max(min(int(n_estimators), 10000), 0)\n    params['max_depth'] = max(min(int(max_depth), 100), 1)\n    params['min_samples_split'] = max(min(int(min_samples_split), 10), 2)\n    params['min_samples_leaf'] = max(min(int(min_samples_leaf), 20), 1)\n    params['min_weight_fraction_leaf'] = max(min(min_weight_fraction_leaf, 1), 0)\n    params['max_features'] = max(min(int(max_features), len(featurelist)), 1)\n    params['max_leaf_nodes'] = max(min(max_leaf_nodes, 1), 10000)\n    params['min_impurity_decrease'] = max(min(min_impurity_decrease, 1), 0)\n    return params\n    \ndef eval_model(n_estimators, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf,\n               max_features, max_leaf_nodes, min_impurity_decrease):\n    params = prepare_params(n_estimators, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf,\n               max_features, max_leaf_nodes, min_impurity_decrease)\n\n    model = RandomForestRegressor(**params)\n    \n    cv_result = np.mean(cross_val_score(model, train.loc[:, featurelist], train.revenue, cv=n_folds, \n                                    scoring='neg_mean_squared_error'))\n    \n    return -np.sqrt(-cv_result)","b8c5dc4a":"rf_optimizer = BayesianOptimization(f=eval_model, \n                                    pbounds={'n_estimators': (800, 1200), \n                                             'max_depth': (7, 10), \n                                             'min_samples_split': (2, 5),\n                                             'min_samples_leaf': (1, 2), \n                                             'min_weight_fraction_leaf': (0.01, 0.03),\n                                             'max_features': (3, 6),\n                                             'max_leaf_nodes': (8, 12),\n                                             'min_impurity_decrease': (0.003, 0.008)}, #len(featurelist))},\n                                    random_state=random_state)","c9d85b5c":"# rf_optimizer.maximize(init_points=100, n_iter=20, alpha=1e-4)\n# rf_optimizer.max","41a77547":"bestfit = {'params': {'max_depth': 9.873668590451505,\n  'max_features': 4.599495854919051,\n  'max_leaf_nodes': 10.767508455801893,\n  'min_impurity_decrease': 0.004577578155030315,\n  'min_samples_leaf': 1.6865009276815837,\n  'min_samples_split': 4.503877015692119,\n  'min_weight_fraction_leaf': 0.010365765546883836,\n  'n_estimators': 950.0288629889935},\n 'target': -2.1252538162396446}","737b4ec0":"params = bestfit['params']\nparams = prepare_params(params['n_estimators'], params['max_depth'], params['min_samples_split'], \n                        params['min_samples_leaf'], params['min_weight_fraction_leaf'], \n                        params['max_features'], params['max_leaf_nodes'], params['min_impurity_decrease'])\n\n# train a model using the best-fit parameters\nmodel = RandomForestRegressor(**params).fit(train.loc[:, featurelist], train.revenue)\n\n# make the prediction\npred = model.predict(test.loc[:, featurelist])\npred = pd.DataFrame({'revenue': np.expm1(pred).astype(np.int64)}, index=test.index)\npred.to_csv('submission.csv')","577e583a":"fimp = pd.Series(model.feature_importances_, index=featurelist)\nfimp.plot.bar()","2ef7ea94":"### original_language\n\nThe movie's language is most likely to have a large impact on the revenue. How many languages are present in both data sets?","3a8fe1c1":"Let's have a look at those 10 actors that produce the highest revenue:","0f118342":"The month barely plays a role, the correlation is not significant enough to be considered. What about the season? We can use a $\\sin$ transformation to map the month on the season (0 for winter, 1 for summer; for the northern hemisphere).","e0b78d8d":"### title\n\nWe skip NLP again and simply check for a correlation between the length of a movie title and `revenue`.","d537c037":"### tagline\n\nAnother opportunity for NLP, but again, we simply check if the existence of a tagline has any impact on `revenue`.","419fcad4":"# Data Inspection","ac9f7fdd":"Yes, the majority has relatively small standard deviations in revenue. This implies that other movies from the same collection are likely to have large revenues, too.\n\nAre there movies in the `test` sample that belong to the same collections as those in the `train` sample?","57c0002d":"The actual distribution rises steeply for a small fraction of keywords and then flattens out. Only a small fraction of all keywords make up 50% of the total revenue.","ed440a2b":"Yes, there is an overlap of 229 collections. We take advantage of this situation and create a new feature `median_revenue_collection` that assigns the median revenue of the corresponding collection to its member movies - movies that are not part of any collection get assigned the median revenue across all movies that are not part of a collection.","c9f9bd88":"This plot shows that there is a clear preference for some genres to have higher revenues than others. For instance, the subset of [action, adventure, drama, comedy] movies make up 50% of the total revenue of all movies as a function of genre.\n\nWe take advantage of this fact and divide the field of genres into tiers, each of which covers a 10% quantile of the total cumulative revenue (technically, the total cumulative is a multiple of the real total revenue since we are assigning the same revenue to multiple companies). The idea behing this approach is that the most successful genres that earned 10% of the total cumulative revenue are assigned to tier 10, the next most successful 10% into tier 9, etc. The tier number can then subsequently be used as a weight to express the company's success.","67172389":"# Feature Engineering, EDA, and Completion\n\nWe discuss individual features, create new features, complete feature data, and define `featurelist`, a preliminary list of features that will be used in the modeling.","080c96f6":"There are some systematics, but the effect seems rather small. Let's ignore this feature for now.","4e6c952e":"There are different regimes: movies with runtimes between 2 and 3 hours seem to have consistenly higher revenues than shorter movies. We transform `runtime` into discrete buckets based on the grid structure chosen above, which is supposed to capture significant changes in the revenue as a function of movie runtime.","4c3bef09":"Is there a correlation with `revenue`?","c89c1801":"Movies can be assigned to multiple genres. How many unique genre categories are there?","ed80e9f0":"There is a significant correlation.","529a9176":"Is there a correlation between `production_companies_weights` and `revenue`?","feeddf8d":"Yes, there is a significant correlation between `budget` and `revenue`. We add `budget` as a model feature.","d689a2a1":"There is a rather weak (~5% significance) correlation between the release year and revenue. Let's keep this feature for future use, but we will not use it in the immediate modeling.","26f4c2c9":"This kernel provides a simple solution to the TMDB Box Office Prediction competition. A few characteristics of this kernel:\n* no external data are used; data have not been modified in any way\n* the number of features used in the model has been minimized to simplify interpretation of the model\n* categorical features are weighted using a revenue-based weighting scheme\n* a simple Random Forest model is used\n* best-fit model parameters are found using Bayesian Optimization","eae6d429":"### overview\n\nAgain, NLP would be definitely useful here, but for the sake of simplicity, let's repeat the length-analysis previously applied to the movie titles:","21404210":"### status\n\nAll movies in the data set should be released. Let's check.","d9490361":"Let's have a look at the correlation between `genre_weight` and revenue across all movies in the training sample:","99b11bc6":"Next question: does the number of movies per collection matter?","d2f05199":"Some movies are actually not yet released. This is no big deal for the `test` sample, but all data points in `train` should have a reliable measurement of `revenue` and this should not be available if the movie is not yet released. \n\nWe drop all data points from the `train` data sample with a `status` flag that differs from `Released`.","fe05dd8f":"If all keywords were equally important for `revenue`, their cumulative distribution would follow a straight line. ","d178f871":"### cast\n\nWe apply our weighting schema once again.","c41598bf":"### spoken_languages\n\nAgain, we take advantage of our weighting schema.","be8fc00f":"## Inventory\n\n* `train`: 22 columns, 3000 rows of data\n* `test`: 21 columns, 4398 rows of data\n\n## Completeness\n\nThe following bar plots show the completeness levels of the individual features:","23fc088a":"No, this is not significant. What about the day of release in the respective month?","de0f4bb4":"### original_title\n\nNLP would be useful to extract keywords from movie titles which can then be used as features. Information on the language could be involved as well.\n\nHere, we simply check if the title length somehow correlates with the movie's revenue.","7f2d3f72":"There is a strong correlation between `cast_weight` and `revenue`.","f4e50c9c":"There is a clear correlation. We apply the rescaling to both data samples.","8aa62b11":"There is a weak but statistically significant correlation. We keep this feature and apply the same weighting schema to other categorical features.","86dd6771":"### poster_path\n\nThis feature provides a filename and is hence not diagnostic of `revenue`.","eff62a30":"Yes, movies in collections are more likely to generate higher revenue. This is useful information and we add this feature to the list.","4f027688":"### genres","510129a8":"Are revenues within movie collections consistent?","ada90769":"Interestingly, there is a weak but significant correlation: revenues seem to be higher when a movie is released closer to the end of the month. Maybe people are more likely to spend money on movie tickets at the end of the month? \n\nAlthough interesting, this feature is not strong enough to make it into our model.","5a89b4d5":"There seem to be only 5 major features that drive a movies revenue. Interesting...","a989f595":"Well, there might be a small effect in the sense that having a homepage increases the likelihood of high revenues, but is it significant?","9e8be15a":"Again, there's a weak correlation. Let's keep this feature for now.","2b41d098":"### homepage\n\nThe existence of a homepage might have an effect on `revenue`. Let's check:","a3c93e6f":"There is an extremely weak trend at the 2.6% level between the length of `overview` and `revenue` - not significant enough. We drop this feature.","34e850d0":"Finally, the genre weights are applied to the data samples by forming the geometric average weight of all genres that are assigned to an individual movie. The geometric average is chosen to favor genres in the top-tier and to put less weight on genres in the lower tiers.","fe0f60b6":"The vast majority of movies runs between 1 and 2 hours, but there are some outlines. Let's check whether the runtime affects the revenue.","b59c7607":"Many of these languages should be extremely rare. Let's clip all languages that are used in less than 10 movies each across both samples and then apply our weighting schema.","03f1414d":"There is no significant correlation between the movie's title length and its revenue. We ignore this feature. ","01ad85f4":"# Box office prediction","332faa24":"### release_date\n\nWe turn `release_date` into a datetime object. Before we can do that, we have to split the two-digit years into year belonging into the 20th century and years belonging into the 21st century.","705c8d5b":"### production_countries\n\nWe take advantage of our weighting schema again.","1d90cbec":"Out of curiosity: what is the distribution of feature importances?","2621d67a":"No, it's not siginificant. We will not use `homepage` in the model learning.","eb1cea7d":"### Keywords","ac757419":"That's a large number of keywords. We repeat part of the `production_companies` analysis to see if there are keywords that have an unusually high impact on `revenue`.","aafb289a":"### imdb_id\n\nThis feature represents a unique identifier for each movie and will hence be ignored.","a6c6b1d9":"We will label these keywords as *hot* keywords that potentially lead to higher revenues. We introduce a feature `hot_keyword` that flags  whether either of the keywords for one movie is among these hot keywords.","f2d1abe4":"Again, both distributions look rather similar. We ignore this feature.","de44ac05":"Let's see if the year a movie was released affects its revenue.","1e2afcfc":"`crew_weight` correlates with `revenue`.","e829c150":"We turn the `revenue` feature into its logarithm to reduce the weight of the highest earning movies in the modeling (and because it's required by the competition).","0c343402":"### runtime\n\nThe runtime of a movie should affect its success. Let's look at the distribution of runtimes in both samples.","47552820":"### budget\n\nSince we applied a logarithmic transformation to `revenue`, we have to apply the same transformation to `budget` to keep both quantities aligned. At the same time, we fill missing data points with the median `budget` value across both samples. \n\nIs there a significant correlation between `budget` and `revenue`? We plot the data and use a Pearson $r$ correlation test to check for significance.","4abc3f03":"### popularity\n\nWe rescale the `popularity` on a logarithmic scale and find a clear correlation with `revenue`.","b3dbc3b3":"This measure drops four data points from the `train` data sample.","f355fc5d":"### belongs_to_collection\n\nFirst, let's check if being part of a collection correlates with higher revenues.","61670130":"A quick check:","3b8b5deb":"### production_companies\n\nWe apply our weighting schema to this feature.","47d5e38b":"# Modeling","3da3378d":"Although this is a rather limited number of genres, we introduce a weighting schema that we will use for most categorical features throughout this data set. \n\nIn this schema, we add up the total revenue (on a linear - not logarithmic - scale multiplied with some constant factor) of each of the categories.","e14c351c":"What about the release month? Is there a correlation with `revenue`?","bde207be":"We use a simple Random Forest Regressor to predict the revenues of the test sample. \n\nBest-fit parameters are found using a Bayesian Optimizer. ","483e2aeb":"### crew","ad15afba":"The two distributions look very similar. We ignore this feature.","121f286f":"There is a significant correlation of `spoken_languages_weight` with `revenue`.","23556f77":"There is no significant correlation between the length of a movie's title and `revenue`. We ignore this feature.","4c179c25":"We use the best-fit set of parameters and predict revenues for the test set movies."}}