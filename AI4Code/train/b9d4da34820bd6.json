{"cell_type":{"e1ac3d27":"code","f598d09f":"code","6a706806":"code","841f35dd":"code","68858752":"code","d02a5134":"code","de2fcf49":"code","67d6f1d4":"code","b013500a":"code","22b70eef":"code","85008115":"code","361b3860":"code","9775904b":"code","2211745e":"code","09503fef":"code","fea993ee":"code","93b0ba08":"code","42c4538b":"code","999f6af4":"code","21142d48":"code","a3d29039":"code","521de694":"code","7ff75f4f":"code","982bdf51":"code","46022622":"code","8b7c9d33":"markdown","88d91300":"markdown","0e886ca4":"markdown","ca0d0648":"markdown","9e888f50":"markdown","a677841e":"markdown","fa54d8ab":"markdown","d160d257":"markdown","edb6413f":"markdown","a5e96c9e":"markdown","4c207492":"markdown","73d6efbc":"markdown","bae23551":"markdown","71acdb9c":"markdown","2b1e4850":"markdown","023f3320":"markdown","fdd28f05":"markdown","f4915251":"markdown","71f4e98a":"markdown","6da038c0":"markdown","f6c724a5":"markdown","acf6e73b":"markdown","fd0edbd3":"markdown","ddab2fd3":"markdown","d66d8acb":"markdown","504c7876":"markdown","c21ab55f":"markdown","90f26068":"markdown","de9e6f7a":"markdown","775f1749":"markdown"},"source":{"e1ac3d27":"import re\nimport numpy as np\nimport pandas as pd \n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.datasets import imdb\n\nfrom keras.utils.np_utils import to_categorical\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport ssl\nssl._create_default_https_context = ssl._create_unverified_context","f598d09f":"max_features = 1000\nmaxlen = 80  # cut texts after this number of words (among top max_features most common words)\nbatch_size = 32\n\n# save np.load\n#np_load_old = np.load\n\n# modify the default parameters of np.load\n#np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n\nprint('Loading data...')\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\nprint(len(x_train), 'train sequences')\nprint(len(x_test), 'test sequences')\n\n#np.load = np_load_old\n\nprint('Pad sequences (samples x time)')\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)","6a706806":"x_train[0]","841f35dd":"INDEX_FROM=3   # word index offset\n\nword_to_id = imdb.get_word_index()\nword_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\nword_to_id[\"<PAD>\"] = 0\nword_to_id[\"<START>\"] = 1\nword_to_id[\"<UNK>\"] = 2\n\nid_to_word = {value:key for key,value in word_to_id.items()}\nprint(' '.join(id_to_word[id] for id in x_train[10] ))","68858752":"print('Build model...')\nmodel = Sequential()\nmodel.add(Embedding(max_features, 8))\nmodel.add(LSTM(16, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","d02a5134":"# try using different optimizers and different optimizer configs\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Write the training input and output, batch size, and testing input and output\n\nmodel.fit(x_train, y_train, \n          batch_size=batch_size, \n          epochs=1, \n          validation_data=(x_test, y_test))","de2fcf49":"score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)","67d6f1d4":"prediction = model.predict(x_test[22220:22221])\nprint('Prediction value:',prediction[0])\nprint('Test Label:',y_test[22220:22221])","b013500a":"# Credits to Peter Nagy","22b70eef":"!wget https:\/\/notebooks.azure.com\/vipulmishra\/projects\/labgail\/raw\/Senti.csv","85008115":"import pandas as pd\ndata = pd.read_csv('Senti.csv')\n# Keeping only the neccessary columns\ndata = data[['text','sentiment']]","361b3860":"data.head(10)","9775904b":"data = data[data.sentiment != \"Neutral\"]\ndata['text'] = data['text'].apply(lambda x: x.lower())\ndata['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n\nfor idx,row in data.iterrows():\n    row[0] = row[0].replace('rt',' ')\n    \nmax_fatures = 2000\ntokenizer = Tokenizer(nb_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(data['text'].values)\nX = tokenizer.texts_to_sequences(data['text'].values)\nX = pad_sequences(X)","2211745e":"Y = pd.get_dummies(data['sentiment']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\nprint('Shape of training samples:',X_train.shape,Y_train.shape)\nprint('Shape of testing samples:',X_test.shape,Y_test.shape)","09503fef":"model = Sequential()\nmodel.add(Embedding(max_fatures, 128 ,input_length = X.shape[1], dropout=0.2))\nmodel.add(LSTM(128))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","fea993ee":"batch_size = 32\nmodel.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 2)","93b0ba08":"score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\nprint(\"Score: %.2f\" % (score))\nprint(\"Accuracy: %.2f\" % (acc))","42c4538b":"text = 'We are going to Delhi'\ntester = np.array([text])\ntester = pd.DataFrame(tester)\ntester.columns = ['text']\n\ntester['text'] = tester['text'].apply(lambda x: x.lower())\ntester['text'] = tester['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n\nmax_fatures = 2000\ntest = tokenizer.texts_to_sequences(tester['text'].values)\ntest = pad_sequences(test)\n\nif X.shape[1]>test.shape[1]:\n    test = np.pad(test[0], (X.shape[1]-test.shape[1],0), 'constant')\n    \ntest = np.array([test])\n\nprediction = model.predict(test)\nprint('Prediction value:',prediction[0])","999f6af4":"model = Sequential()\nmodel.add(Embedding(max_features, 8))\nmodel.add(LSTM(8, dropout=0.0, recurrent_dropout=0.0))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=1, validation_data=(x_test, y_test))\n\nscore, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)","21142d48":"# Write your code here \n\n# Use the same layer design from the above cell ","a3d29039":"model = Sequential()\nmodel.add(Embedding(max_features, 4))\nmodel.add(LSTM(16, dropout=0.0, recurrent_dropout=0.0))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=1, validation_data=(x_test, y_test))\n\nscore, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)","521de694":"model = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(LSTM(8, dropout=0.5, recurrent_dropout=0.5))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=1, validation_data=(x_test, y_test))\n\nscore, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)","7ff75f4f":"# Write your code here \n\n# Use the same model design from the above cell ","982bdf51":"model = Sequential()\nmodel.add(Embedding(max_features, 8))\nmodel.add(LSTM(8, dropout=0.0, recurrent_dropout=0.0, return_sequences=True))\nmodel.add(LSTM(8, dropout=0.0, recurrent_dropout=0.0, return_sequences=True))\nmodel.add(LSTM(8, dropout=0.0, recurrent_dropout=0.0))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=1, validation_data=(x_test, y_test))\n\nscore, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)","46022622":"# Write your code here \n\n# Use the same node design from the above cell ","8b7c9d33":"### Prediction","88d91300":"### Model Training","0e886ca4":"### LSTM with 8 nodes","ca0d0648":"###  Importing packages","9e888f50":"### Dropout with probability 0.9","a677841e":"# Part 3: RNN Design Choices","fa54d8ab":"### Training ","d160d257":"### Visualize the data","edb6413f":"#  Part 1:  Recurrent Neural Network ","a5e96c9e":"### LSTM with 16 nodes","4c207492":"### RNN with 2 layer LSTM","73d6efbc":"### Validation","bae23551":"### RNN with 3 layer LSTM","71acdb9c":"### Testing","2b1e4850":"### Formatting Test Example","023f3320":"### Training set","fdd28f05":"## Influence of Dropout","f4915251":"### What are your findings?","71f4e98a":"### Dropout with probability 0.5","6da038c0":"### Other RNN Layers\n\n* keras.layers.RNN(cell, return_sequences=False)\n* keras.layers.SimpleRNN(units, activation='tanh')\n* keras.layers.GRU(units, activation='tanh', recurrent_activation='hard_sigmoid')\n* keras.layers.ConvLSTM2D(filters, kernel_size, strides=(1, 1), padding='valid', )\n* keras.layers.SimpleRNNCell(units, activation='tanh')\n* keras.layers.GRUCell(units, activation='tanh', recurrent_activation='hard_sigmoid')\n* keras.layers.LSTMCell(units, activation='tanh', recurrent_activation='hard_sigmoid')\n* keras.layers.CuDNNGRU(units, kernel_initializer='glorot_uniform')\n* keras.layers.CuDNNLSTM(units, kernel_initializer='glorot_uniform')","f6c724a5":"### Format data","acf6e73b":"### Building a Model","fd0edbd3":"## Influence of number of nodes","ddab2fd3":"### Preparing Dataset","d66d8acb":"### Load data","504c7876":"## Influence of Embedding","c21ab55f":"# Part 2: Recurrent Neural Network with Custom Dataset","90f26068":"### Visualize data","de9e6f7a":"## Multilayered RNNs","775f1749":"### Design a model"}}