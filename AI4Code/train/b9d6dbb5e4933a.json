{"cell_type":{"f4bef5ba":"code","b35da5f3":"code","acc21d5b":"code","91c52115":"code","f868e121":"code","1e849206":"code","a4139e42":"code","b3a981dc":"code","9ebf2d4d":"code","41ef53ca":"code","b66dbaea":"code","4cdba319":"code","afee29eb":"code","0aacdfb6":"code","82345e6e":"code","d4bf0df0":"code","230445e2":"code","53555f55":"code","c7aa8091":"code","16262609":"code","f8b6f0a2":"code","0753734a":"code","f78ef969":"code","27f2579a":"code","aade543f":"code","3bbd0c7d":"code","99b824f4":"code","d797c8b0":"code","7e5fd18c":"code","344a65bd":"code","e918236f":"code","28be1956":"code","31bd3cf0":"code","72a398d3":"code","eaf91b78":"code","cb440fe6":"code","f970eafe":"code","54c04b82":"code","50bf303b":"code","9b013859":"code","e80457aa":"code","6c5c6fa1":"code","9bbcb674":"code","91af0e8f":"code","db967c05":"code","eb19beb2":"code","869ffcbd":"code","0dc4efbc":"code","2435afd1":"code","95f39469":"code","5c45a070":"code","0502754c":"code","9309a72f":"code","8dc26aee":"code","869671ec":"code","95a86aca":"markdown"},"source":{"f4bef5ba":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gensim\nimport nltk\nimport os\nprint(os.listdir(\"..\/input\/embeddings\/GoogleNews-vectors-negative300\/\"))\n\n# Any results you write to the current directory are saved as output.","b35da5f3":"path = \"..\/input\/embeddings\/GoogleNews-vectors-negative300\/GoogleNews-vectors-negative300.bin\"\nembeddings = gensim.models.KeyedVectors.load_word2vec_format(path,binary=True)","acc21d5b":"list(embeddings['modi'][:5])","91c52115":"pd.Series(embeddings['modi'][:5])","f868e121":"embeddings.most_similar('modi',topn=10)","1e849206":"url = 'https:\/\/bit.ly\/2S2yXEd'\ndata = pd.read_csv(url)\ndata.head()","a4139e42":"doc1 = data.iloc[0,0]\nprint(doc1)\nprint(nltk.word_tokenize(doc1.lower()))","b3a981dc":"docs = data['review']\ndocs.head()","9ebf2d4d":"words = nltk.word_tokenize(doc1.lower())\ntemp = pd.DataFrame()\nfor word in words:\n    try:\n        print(word,embeddings[word][:5])\n        temp = temp.append(pd.Series(embeddings[word][:5]),ignore_index=True)\n    except:\n        print(word,'is not there')","41ef53ca":"temp","b66dbaea":"docs = docs.str.lower().str.replace('[^a-z ]','')\ndocs.head()","4cdba319":"from nltk.stem import PorterStemmer\nstemmer = PorterStemmer()\nstopwords  = nltk.corpus.stopwords.words('english')\n\ndef clean_doc(doc):\n    words = doc.split(' ')\n    words_clean = [word for word in words if word not in stopwords]\n    doc_clean= ' '.join(words_clean)\n    return doc_clean\n\ndocs_clean = docs.apply(clean_doc)\ndocs_clean.head()","afee29eb":"docs_clean.shape","0aacdfb6":"docs_vectors =  pd.DataFrame()\n\nfor doc in docs_clean:\n    words = nltk.word_tokenize(doc)\n    temp =  pd.DataFrame()\n    for word in words:\n        try:\n            word_vec = embeddings[word]\n            temp = temp.append(pd.Series(word_vec),ignore_index=True)\n        except:\n            pass\n    docs_vectors=docs_vectors.append(temp.mean(),ignore_index=True)   \ndocs_vectors.shape    ","82345e6e":"docs_vectors.head()","d4bf0df0":"pd.isnull(docs_vectors).sum(axis=1).sort_values(ascending=False).head()","230445e2":"X = docs_vectors.drop([64,590])\nY = data['sentiment'].drop([64,590])","53555f55":"from sklearn.model_selection import train_test_split\nxtrain,xtest,ytrain,ytest = train_test_split(X,Y,test_size=.2,random_state=100)","c7aa8091":"from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\nfrom sklearn.metrics import accuracy_score\nmodel = RandomForestClassifier(n_estimators=800)\nmodel.fit(xtrain,ytrain)\ntest_pred =  model.predict(xtest)\naccuracy_score(ytest,test_pred)","16262609":"model = AdaBoostClassifier(n_estimators=800)\nmodel.fit(xtrain,ytrain)\ntest_pred =  model.predict(xtest)\naccuracy_score(ytest,test_pred)","f8b6f0a2":"url = 'https:\/\/bit.ly\/2W21FY7'\ndata = pd.read_csv(url)\ndata.shape","0753734a":"data.head()","f78ef969":"docs = data.loc[:,'Lower_Case_Reviews']\nprint(docs.shape)\ndocs.head()","27f2579a":"Y = data['Sentiment_Manual']\nY.head()","aade543f":"Y.value_counts()","3bbd0c7d":"docs = docs.str.lower().str.replace('[^a-z ]','')\ndocs.head()","99b824f4":"from nltk.stem import PorterStemmer\nstemmer = PorterStemmer()\nstopwords  = nltk.corpus.stopwords.words('english')\n\ndef clean_doc(doc):\n    words = doc.split(' ')\n    words_clean = [stemmer.stem(word) for word in words if word not in stopwords]\n    doc_clean= ' '.join(words_clean)\n    return doc_clean\n\ndocs_clean = docs.apply(clean_doc)\ndocs_clean.head()","d797c8b0":"X = docs_clean \nX.shape,Y.shape","7e5fd18c":"from sklearn.model_selection import train_test_split\n\nxtrain,xtest,ytrain,ytest = train_test_split(X,Y,test_size=.2,random_state=100)","344a65bd":"from sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(min_df=5)\ncv.fit(X)","e918236f":"XTRAIN = cv.transform(xtrain)\nXTEST = cv.transform(xtest)","28be1956":"XTRAIN = XTRAIN.toarray()\nXTEST = XTEST.toarray()","31bd3cf0":"from sklearn.tree import DecisionTreeClassifier as dtc\nfrom sklearn.metrics import accuracy_score\nmodel = dtc(max_depth=10)\nmodel.fit(XTRAIN,ytrain)\nyp= model.predict(XTEST)\naccuracy_score(ytest,yp)","72a398d3":"from sklearn.naive_bayes import MultinomialNB as mnb\nm1=mnb()\nm1.fit(XTRAIN,ytrain)\nyp1=m1.predict(XTEST)\naccuracy_score(ytest,yp1)","eaf91b78":"from sklearn.naive_bayes import BernoulliNB as bnb\nm2=bnb()\nm2.fit(XTRAIN,ytrain)\nyp2=m2.predict(XTEST)\naccuracy_score(ytest,yp2)","cb440fe6":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntv = TfidfVectorizer(min_df=5)\ntv.fit(X)","f970eafe":"XTRAIN = tv.transform(xtrain)\nXTEST = tv.transform(xtest)","54c04b82":"XTRAIN = XTRAIN.toarray()\nXTEST = XTEST.toarray()","50bf303b":"from sklearn.naive_bayes import MultinomialNB as mnb\nmod=mnb()\nmod.fit(XTRAIN,ytrain)\nypred=mod.predict(XTEST)\naccuracy_score(ytest,ypred)","9b013859":"stopwords  = nltk.corpus.stopwords.words('english')\n\ndef clean_doc(doc):\n    words = doc.split(' ')\n    words_clean = [word for word in words if word not in stopwords]\n    doc_clean= ' '.join(words_clean)\n    return doc_clean\n\ndocs_clean = docs.apply(clean_doc)\ndocs_clean.head()","e80457aa":"docs_vectors =  pd.DataFrame()\n\nfor doc in docs_clean:\n    words = nltk.word_tokenize(doc)\n    temp =  pd.DataFrame()\n    for word in words:\n        try:\n            word_vec = embeddings[word]\n            temp = temp.append(pd.Series(word_vec),ignore_index=True)\n        except:\n            pass\n    docs_vectors=docs_vectors.append(temp.mean(),ignore_index=True)   \ndocs_vectors.shape    ","6c5c6fa1":"Y.shape","9bbcb674":"df = pd.concat([docs_vectors,Y],axis=1)\ndf.head(3)","91af0e8f":"df[df.iloc[:,0].isnull()].shape","db967c05":"df = df.dropna(axis=0)","eb19beb2":"df.shape","869ffcbd":"X = df.drop(['Sentiment_Manual'],axis=1)\nY = df['Sentiment_Manual']","0dc4efbc":"X.shape,Y.shape","2435afd1":"from sklearn.model_selection import train_test_split\nxtrain,xtest,ytrain,ytest = train_test_split(X,Y,test_size=.2,random_state=100)","95f39469":"xtrain.shape,ytrain.shape","5c45a070":"from sklearn.tree import DecisionTreeClassifier as dtc\nfrom sklearn.metrics import accuracy_score\nmodel = dtc(max_depth=10)\nmodel.fit(xtrain,ytrain)\nyp= model.predict(xtest)\naccuracy_score(ytest,yp)","0502754c":"data.head()","9309a72f":"data.Sentiment_Manual.shape","8dc26aee":"docs_clean.shape","869671ec":"from nltk.sentiment import SentimentIntensityAnalyzer\nanalyser = SentimentIntensityAnalyzer()\n\ndef get_sentiment(sentence,analyser=analyser):\n    score = analyser.polarity_scores(sentence)['compound']\n    if score > 0:\n        return 1\n    else:\n        return 0","95a86aca":"**HOTSTAR - GO SOLO review Analysis**"}}