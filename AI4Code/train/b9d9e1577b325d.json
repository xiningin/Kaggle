{"cell_type":{"08026b7d":"code","fa7b1bcc":"code","1176de8b":"code","ff40e9a2":"code","fb90cd51":"code","ce6177d2":"code","e2a9ab62":"code","09013483":"code","fd000c76":"code","638a7df3":"code","0f130c35":"code","7e61b561":"code","a1a23d0a":"code","ff82f266":"code","8a867454":"code","92c361f6":"code","8fe3e9ed":"code","502ee087":"code","0fa4ea91":"code","f2e0a367":"code","5bc049df":"code","790c39a9":"code","d1541b71":"code","203de746":"code","5ad1cf37":"code","69712953":"code","c596f108":"code","ed826bf2":"code","93c6380e":"code","ffaca2f5":"code","8f720a9d":"code","f0402216":"code","e70b4b2a":"code","deaa0242":"code","8293807c":"code","037e66c5":"code","26a9e2c0":"code","fe862977":"code","d7ed7c86":"code","3b3dbaf9":"code","ae8a0679":"code","204c18af":"code","4b770d53":"code","c212dde8":"code","a28361c1":"code","b8bbe406":"code","5e6274a2":"code","512efcf1":"code","9fb41050":"code","28efdd5b":"code","428867e2":"code","c2b6ae53":"code","901afdb1":"code","a99de033":"code","b4a4e2fb":"code","c6ea4dd6":"markdown","5baef2e2":"markdown","2653af6a":"markdown","e7b60850":"markdown","cb3e51b3":"markdown","3940c103":"markdown","4cdbd810":"markdown","2e351276":"markdown","8b3dd867":"markdown","1c68ff64":"markdown","db8d3365":"markdown","bd5dbb7a":"markdown","d04afa50":"markdown","c428b155":"markdown","b482643e":"markdown","39da7d1e":"markdown","87c06fcf":"markdown","71edf94a":"markdown","344be8f6":"markdown","566c3900":"markdown","ae4f5090":"markdown","18841559":"markdown","21ccc4ac":"markdown","23027a47":"markdown","85139661":"markdown","14656372":"markdown","a8546c77":"markdown","d39797fe":"markdown","61898629":"markdown","30e08002":"markdown","f0e4a8e0":"markdown","d0fadea5":"markdown","b93a96ea":"markdown","976b8615":"markdown","56919336":"markdown","883a4c0a":"markdown","d98169c4":"markdown","95ad2925":"markdown","49db6be1":"markdown","084e1cda":"markdown","7d1f06ae":"markdown","53a638dd":"markdown","3e00d449":"markdown","76ec97f5":"markdown","e54564d8":"markdown","a071a3fb":"markdown","33bc6ac0":"markdown","9ac1dfe1":"markdown","8ae2a0c8":"markdown","e6cf3a64":"markdown","cc510c75":"markdown"},"source":{"08026b7d":"# import necessary packages\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# machine learning packages\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score","fa7b1bcc":"# Train and Test datasets\ninput_train = '..\/input\/train.csv'\ninput_test = '..\/input\/test.csv'","1176de8b":"# Reading the datasets\ndf_test = pd.read_csv(input_test)\ndf_train = pd.read_csv(input_train)\ncombine = [df_train, df_test]\ndf_train.head()","ff40e9a2":"df_test.head()","fb90cd51":"## Checking for missing data\ndf_train.isna().sum()","ce6177d2":"# some initial analysis \nlen(df_train[df_train[\"Sex\"]=='male'])\nlen(df_train[df_train['Age']>60])\nset(df_train['Survived'])\nlen(df_train[df_train[\"Survived\"]==1])\nlen(df_train[df_train[\"Pclass\"]==3])\nlen(df_train[df_train[\"SibSp\"]>0])\nlen(df_train[df_train[\"Parch\"]>0])\nset(df_train[\"Fare\"])","e2a9ab62":"df_train.describe()","09013483":"df_train[['Pclass', 'Survived']].groupby(['Pclass']).mean().sort_values(by='Survived', ascending=False)","fd000c76":"df_train[['Sex', 'Survived']].groupby(['Sex']).mean().sort_values(by='Survived', ascending=False)","638a7df3":"df_train[['SibSp', 'Survived']].groupby(['SibSp']).mean().sort_values(by='Survived', ascending=False)","0f130c35":"df_train[['Parch', 'Survived']].groupby(['Parch']).mean().sort_values(by='Survived', ascending=False)","7e61b561":"grid = sns.FacetGrid(df_train, col='Survived')\ngrid.map(plt.hist, 'Age', bins=20)","a1a23d0a":"grid = sns.FacetGrid(df_train, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","ff82f266":"#len(df_train[df_train['Pclass']==1])\nlen(df_train[df_train['Pclass']==2])\n#len(df_train[df_train['Pclass']==3])\n","8a867454":"grid = sns.catplot(x='Embarked',y='Survived',kind='point',data=df_train)\ngrid.add_legend();","92c361f6":"f,ax=plt.subplots(2,2,figsize=(15,15))\nsns.countplot('Embarked',data=df_train,ax=ax[0,0])\nax[0,0].set_title('No. Of Passengers Boarded')\nsns.countplot('Embarked',hue='Sex',data=df_train,ax=ax[0,1])\nax[0,1].set_title('Male-Female Split for Embarked')\nsns.countplot('Embarked',hue='Survived',data=df_train,ax=ax[1,0])\nax[1,0].set_title('Embarked vs Survived')\nsns.countplot('Embarked',hue='Pclass',data=df_train,ax=ax[1,1])\nax[1,1].set_title('Embarked vs Pclass')\nplt.subplots_adjust(wspace=0.2,hspace=0.5)\nplt.show()","8fe3e9ed":"sns.factorplot('Pclass','Survived',hue='Sex',col='Embarked',data=df_train)\nplt.show()","502ee087":"grid = sns.FacetGrid(df_train, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","0fa4ea91":"print(\"Before dropping\", df_train.shape, df_test.shape, combine[0].shape, combine[1].shape)\n\ndf_train = df_train.drop(['Ticket', 'Cabin'], axis=1)\ndf_test = df_test.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [df_train, df_test]\n\n\"After dropping\", df_train.shape, df_test.shape, combine[0].shape, combine[1].shape","f2e0a367":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-z]+)\\.', expand=False)\n\npd.crosstab(df_train['Title'], df_train['Sex'])","5bc049df":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ndf_train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","790c39a9":"dataset['Title']","d1541b71":"all_titles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(all_titles)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ndf_train.head()","203de746":"df_train = df_train.drop(['Name', 'PassengerId'], axis=1)\ndf_test = df_test.drop(['Name'], axis=1)\ncombine = [df_train, df_test]\ndf_train.shape, df_test.shape","5ad1cf37":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ndf_train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","69712953":"grid = sns.FacetGrid(df_train, row='Pclass', col='Sex', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","c596f108":"missing_ages = np.zeros((2,3))\nmissing_ages","ed826bf2":"gender=['male','female']\nfor dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == gender[i]) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n            #print(guess_df)\n            age_guess = guess_df.median()\n            #print(age_guess)\n            # Convert random age float to nearest .5 age\n            missing_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == gender[i]) & (dataset.Pclass == j+1),\\\n                    'Age'] = missing_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ndf_train.head()","93c6380e":"df_train['AgeBand'] = pd.cut(df_train['Age'], 5)\ndf_train[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","ffaca2f5":"for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ndf_train.head()","8f720a9d":"df_train = df_train.drop(['AgeBand'], axis=1)\ncombine = [df_train, df_test]\ndf_train.head()","f0402216":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ndf_train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","e70b4b2a":"df_train = df_train.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ndf_test = df_test.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [df_train, df_test]\n\ndf_train.head()","deaa0242":"freq_port = df_train.Embarked.dropna().mode()[0]\nfreq_port","8293807c":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ndf_train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n","037e66c5":"df_test['Fare'].fillna(df_test['Fare'].dropna().median(), inplace=True)\ndf_test.head()","26a9e2c0":"df_train['FareBand'] = pd.qcut(df_train['Fare'], 4)\ndf_train[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","fe862977":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ndf_train = df_train.drop(['FareBand'], axis=1)\ncombine = [df_train, df_test]\n    \ndf_train.head(10)","d7ed7c86":"df_test.head(10)","3b3dbaf9":"for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ndf_train.head()","ae8a0679":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ndf_train.head()","204c18af":"def split_train_test(data,size=0.3):\n        arr = np.arange(len(data))\n        np.random.shuffle(arr)\n        train = data.iloc[arr[0:int(len(data)*(1-size))]]\n        test = data.iloc[arr[int(len(data)*(1-size)):len(data)]]\n        return train,test","4b770d53":"dtrain,dtest=split_train_test(df_train,size = 0.3)\nX_train=dtrain[dtrain.columns[1:]]\nY_train=dtrain[dtrain.columns[:1]]\nX_test=dtest[dtest.columns[1:]]\nY_test=dtest[dtest.columns[:1]]\nXX=df_train[df_train.columns[1:]]\nYY=df_train['Survived']\nXT=df_test[df_test.columns[1:]]","c212dde8":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nlogreg_acc = round(accuracy_score(Y_test,Y_pred) * 100, 2)\nlogreg_acc","a28361c1":"coeff_df = pd.DataFrame(df_train.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","b8bbe406":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nknn_acc = round(accuracy_score(Y_test,Y_pred) * 100, 2)\nknn_acc","5e6274a2":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nsvc_acc = round(accuracy_score(Y_test,Y_pred) * 100, 2)\nsvc_acc","512efcf1":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\ngauss_acc = round(accuracy_score(Y_test,Y_pred) * 100, 2)\ngauss_acc","9fb41050":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nperc_acc = round(accuracy_score(Y_test,Y_pred) * 100, 2)\nperc_acc","28efdd5b":"# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nlinsvc_acc = round(accuracy_score(Y_test,Y_pred) * 100, 2)\nlinsvc_acc","428867e2":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nsgd_acc = round(accuracy_score(Y_test,Y_pred) * 100, 2)\nsgd_acc","c2b6ae53":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\ndtree_acc = round(accuracy_score(Y_test,Y_pred) * 100, 2)\ndtree_acc","901afdb1":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nran_forest_acc = round(accuracy_score(Y_test,Y_pred) * 100, 2)\nran_forest_acc","a99de033":"models = pd.DataFrame({\n    'Model': ['Logistic Regression','KNN','Support Vector Machines', \n              'Naive Bayes','Perceptron','Linear SVC', \n              'Stochastic Gradient Decent','Decision Tree','Random Forest'],\n    'Score': [logreg_acc,knn_acc, svc_acc,gauss_acc, \n              perc_acc,linsvc_acc,sgd_acc,dtree_acc,ran_forest_acc]})\nmodels.sort_values(by='Score', ascending=False)","b4a4e2fb":"Y_pred = logreg.predict(XT)\nsubmission = pd.DataFrame({\n        \"PassengerId\": df_test[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submission.csv', index=False)","c6ea4dd6":"**Fare Feature - **","5baef2e2":"We now convert the Fare feature to ordinal values based on the FareBand.","2653af6a":"**Incomplete\/Complete features - **\n\n*  There are three columns with missing data - Cabin, Age, Embarked. The cabin column has maximum of missing data (~77%) followed by Age column (~20%) and Embarked column (~0.002%). There is way too much missing enteries in cabin column. Let's see if we can use it later for a prediction on a subset of data. We will definitley complete the 'Age' and 'Embarked' because they might be related to the survival rate. \n* We can drop PassengerID from our analysis as it does not contribute to survival rate.\n* We can also drop 'Name' column as it may not contribute to survival rate. However, we will create a new column 'Title' which might be related to survival. ","e7b60850":"**Logistic Regression**\n\nLogistic Regression predicts the relationship between independent features and dependent categorical feature by estimating the probabilities using a logistic function.\n\nNote the confidence score generated by the model based on our training dataset.","cb3e51b3":"**Pclass Feature - **\n\nFeature Pclass has three different values. We can see its correlation with Survival in a combine grid. ","3940c103":"Observations:\n\n* The survival chances are always high for Pclass 1 and Pclass 2 than Pclass 3.\n\n* Pclass passengers of Port S (irrespective of gender) has very low survival rate in comparison to other ports.\n\n* The males boarded from Port Q has the lowest survival rate.","4cdbd810":"**Data Characteristics - **\n\n*  The given data consists of information about 891 passengers. Out of them, 577 (64.7%) are males and rest 314 (35.2%) are females. \n\n* Mean Age is 30 years. Mostly young people are travelling. There are only 22 (2.4%) senior citizens.\n\n* There are three different class, namely 1,2,3 with 1 being upper-class (24.2%), 2 being middle-class (20.6%) and 3 lower-class (55.1%). \n\n* Survived columm has two enteries, namely 0 (not survived) and 1 (survived). According to this dataset, 342 (38.4%) persons had suvived the tragedy.\n\n* SibSp feature tells number of siblings \/ spouses aboard the Titanic. 283 (31.8%) had siblings\/spouses aboard.\n\n* Parch feature tells of parents \/ children aboard the Titanic. 213 (24%) had parents\/children abroad the Titanic.\n\n* Passenger Fare varies significantly from 0 to 512. The mean fare is 32, however, the distribution of fare is quite skewed. ","2e351276":"The highest score is from Logistic Regression model. Thus submitting Y_pred for that model.","8b3dd867":"There are another two features which can be combined to make one feature - Parch and SibSp. This new feature will tell us the total family members aboarded the Ship. \n\nAfter creating this new feature, we can drop Parch and SibSp.","1c68ff64":"Observations - \n* Age varies from 0.42 years to 80 years with most passengers belong to age of 15 to 40 years.\n* Young children (age <=5) had high survival rate.\n* The oldest passengers (age ) had survived.\n* A large number of passengers between age 16 and 30 years did not survive.","db8d3365":"**Model Comparison**\n\nWe now rank all the above models to choose the best one for our problem. ","bd5dbb7a":"Observations - \n\n* Pclass 3 has maximum passengers (55%), followed by Pclass 1 (24%) and rest is Pclass 2 (20%). However, the Pclass 3 passengers did not survive much. Pclass 1 has maximum survival passengers.\n* Children below age 5 mostly survived.","d04afa50":"**Dropping some features - **\n\nBy dropping, we can eliminate useless features. It will increase the speed and eases the analsis because the data has reduced by dropping some features.\n\nBased on our assumptions and decisions, we would like to drop the Cabin and Ticket features as they don't correlate much with the survival rate.","c428b155":"**Sex Feature - **","b482643e":"We can convert the categorical titles to ordinal.","39da7d1e":"Let us drop Parch, SibSp, and FamilySize features in favor of IsAlone.","87c06fcf":"** To Complete a numerical continuous feature - **\n\nThere are three features with missing enteries - Cabin, Age and Embarked. We are dropping Cabin feature. So, we should start with completing other two features - Age and Embarked. \n\nThere are three methods to complete a numerical continuous feature.\n\n1. To generate random numbers between mean and standard deviation. This is simple way without worrying about the correlation between different features. \n\n2. To use other correlated features which is more accurate way of guessing missing values. In our case, we note correlation among Age, Gender, and Pclass. We can replace missing Age values using median values for Age across sets of Pclass and Gender feature combinations. So, median Age for Pclass=1 and Gender=0, Pclass=1 and Gender=1, and so on...\n\n3. To combine methods 1 and 2. So instead of guessing age values based on median, use random numbers between mean and standard deviation, based on sets of Pclass and Gender combinations.\n\nMethod 1 and 3 will introduce random noise into our models. The results from multiple executions might vary. We will prefer method 2.","71edf94a":"**Converting a categorical feature into numerical one - **\n\nFeatures like Sex and Embarked have alphabetic values. We can convert these strings values into numerical values. This is required by most model algorithms. \n\nLet us start by converting alphabetic values of Sex feature to a new feature  where female=1 and male=0.","344be8f6":"We can now convert the Embarked feature by creating a new numeric Port feature. However, before that we should complete this features as there were two missing enteries in this column.","566c3900":"**Features Study - **\n\nNext, we will study all features to decide which are important for this problem. We will consider features one by one and make some decisions whether to include them in Model predictions.\n\nHere are some basic analysis - ","ae4f5090":"**Naive Bayes classifier**\n\nNaive Bayes classifiers is based on applying Bayes' theorem with strong (naive) independence assumptions\/hypothesis between the features. In this, the best hypothesis is chosen by comparing the conditional probabilities based on different hypothesis.","18841559":"We can replace many titles with a more common name or classify them as Rare.","21ccc4ac":"We can create FareBand.","23027a47":"We can create another feature called IsAlone","85139661":"**Observations in a Nutshell for all features - **\n\n* Females have better survival rates.\n* Survival rates for Pclass - 1>2>3\n* Embarkation Port correlates with survival rate. \n* Port C has better survival rate than ports S and Q.\n* Higher fare paying passengers had better survival rates.\n* Siblings\/Parch feature does not correlate much with survival rate. ","14656372":"Observations:\n\n* Maximum passengers boarded from S. The majority of them are from Pclass 3. However, this port has maximum passengers of all three classes in comparison to other ports.\n\n* Inspite of higher number of passengers from Pclass 1, the survival rate is still bit low because many passengers from Pclass 3 around 81% didn't survive.\n\n* Since small number passengers boarded from port C and they mainly belong to Pclass1, the survival rate is better than other two ports.\n\n* Port Q had almost 95% of the passengers from Pclass 3 and hence the low survival rate.","a8546c77":"Observations - \n\n* Higher fare paying passengers had better survival rates. Thus, we can create fare ranges for our model.\n\n* Port of embarkation correlates with survival rates. \n","d39797fe":"Hello,\n\nWelcome aboard! The rule of Sea states that  \"Women and children first\" (or to a lesser extent) is a code of conduct dating from 1852, whereby the lives of women and children were to be saved first in a life-threatening situation, typically abandoning ship, when survival resources such as lifeboats were limited. https:\/\/en.wikipedia.org\/wiki\/Women_and_children_first\n\nIn this notebook, we will predict what kind of of people were likely to survive.","61898629":"**Feature Enginnering - **\n\nWe have analysed the dataset and made some assumptions and decisions regarding the features. Now, we will apply those decisions. This addition\/extraction\/removal of features is termed as feature engineering.\n\nAn example would be binning the Fare feature to use for Predictive Modeling.\n\nBelow we apply Feature Engineering to this dataset -","30e08002":"* There is a very high correlation between Pclass and Survived features. The upper-class passengers has 62.9% survival rate.\n* Women have more survival rate (74%) than men.\n* Parch\/Sib - It has mix correlation rate for various values of Parch\/Sib. Some are not correlated at all and some are highly correlated. ","f0e4a8e0":"** Model, predict and solve -  **\n\nNow its time to model and predict the survived passengers. This problem is a classification and regression type. We have to predict the relationship between the output (survived or not) with the other features (Pclass, Sex, Age etc). Here, we will perform a supervised machine learning techinque as we are training our model with a given dataset. \n\nBased on the current machine learning problem, i.e., Classification, Supervised Learning and Regression, we can choose following models - \n\n* Logistic Regression\n* KNN or k-Nearest Neighbors\n* Support Vector Machines\n* Naive Bayes classifier\n* Decision Tree\n* Random Forrest\n* Perceptron\n* Artificial neural network\n* RVM or Relevance Vector Machine","d0fadea5":"Observations.\n\nEarlier we found that there is a correlation between Age and Survival feature. In order to bring out the correlation, we can band the age into different bands and these are similar to Titles.\n\nFor example: Master title has Age mean of 5 years. There is a slight variation in survival rates among Title age bands.\nCertain titles mostly survived (Mme, Lady, Sir) and certain did not survive (Don, Rev, Jonkheer).\n\nThus, we keep the new Title feature for model training.\n","b93a96ea":"First, we separate the input and output data. We drop the Survived column (output) from the data and make it a separate Y column.\n","976b8615":"We can use Logistic Regression to validate our assumptions and decisions for feature creating and completing goals. This can be done by calculating the coefficient of the features in the decision function.\n\nPositive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).\n\n* Sex correlates positively with Survival, being the highest positivie coefficient. Implies - As the Sex value increases (male: 0 to female: 1), the probability of Survived=1 increases the most.\n* Inversely as Pclass increases, probability of Survived=1 decreases the most.\n* Also Title as second highest positive correlation.","56919336":"We replace Age with numericals based on these bands.","883a4c0a":"We can create another feature called IsAlone","d98169c4":"The survival rate for Port C is the highest (~0.55) and it is the lowest for S (~0.34).\n\nWe can further split its distribution with respect to Pclass and Sex features. ","95ad2925":"The first step is to check whether the data is complete or not and then decide if it is worth to complete the missing features in the dataset.","49db6be1":"**k-Nearest Neighbors algorithm**\n\nThe k-Nearest Neighbors algorithm (k-NN) is basically a classification by finding the most similar data points in the training data, and making an educated guess based on their classifications. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.","084e1cda":"**Support Vector Machines (SVM)**\n\nA Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. This algorithm is also used in Supervised learning. Given the labeled training data, the algorithm outputs an optimal hyperplane which categorizes new examples. In two dimentional space this hyperplane is a line dividing a plane in two parts where in each class lay in either side.","7d1f06ae":"Let us start by preparing an empty array to contain fill up missing Age values based on Pclass x Gender combinations.","53a638dd":"The Perceptron algorithm is the simplest type of artificial neural network. It is a model of a single neuron that can be used for two-class classification problems. ","3e00d449":"**To complete a categorical feature - **\n\nEmbarked feature takes S, Q, C values based on port of embarkation. Our training dataset has two missing values. We simply fill these with the most common occurance.","76ec97f5":"Now we can safely drop the Name feature from training and testing datasets. We also do not need the PassengerId feature in the training dataset.","e54564d8":"**To complete and convert a numeric feature into bands - **\n\nThe test data has one missing entry of Fare feature. We can replace this with the most common entry in this feature.\n\nWe will also round off the fare to two decimals as it represents currency.","a071a3fb":"We can further create Age bands and determine correlations with Survived.","33bc6ac0":"**Age Feature - **\n\nAs Age feature is continous one, we should see its distribution for survival and non-survivals.","9ac1dfe1":"**To Create a new feature from existing - **\n\nName feature is itself not a useful one. However, we can extract the Title of the passengers from it and use it to correlate with the Survival rate. \n\nBelow we extract Title feature using regular expressions. The RegEx pattern (\\w+\\.) matches the first word which ends with a dot character within Name feature. The expand=False flag returns a DataFrame.","8ae2a0c8":"**Embarked Feature -** \n\nEmbarked Feature has three different entires - S, Q and C ports. We can just see its distribution with survival as - ","e6cf3a64":"We can remove the AgeBand feature.","cc510c75":"Now we iterate over Sex (0 or 1) and Pclass (1, 2, 3) to calculate guessed values of Age for the six combinations."}}