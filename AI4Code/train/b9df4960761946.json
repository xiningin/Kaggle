{"cell_type":{"3ef55e66":"code","078e4100":"code","cc6574e5":"code","3bbea820":"code","32847a25":"code","eb52d863":"code","b3bb9d74":"code","26fa5f11":"code","86bc1d64":"code","6d7a2469":"code","89bf7835":"code","7484a929":"code","4577a3e6":"code","178121fd":"code","4050bcb5":"code","c4231b71":"code","f0255cf4":"code","924d6317":"code","15ab4865":"code","a5a09b84":"code","a580d912":"code","db4e75d5":"code","eb515268":"code","6174c6e0":"code","fa049a73":"code","73474258":"code","e2483cb1":"code","1e33492c":"code","55419495":"code","0a3f5470":"code","78297e65":"code","751866bc":"code","9677c344":"code","88c190a7":"code","03d1fc65":"code","961dddb3":"code","ad5254c0":"code","d9e593af":"code","3ac30256":"code","276bc287":"code","a69e362c":"code","300bd2c7":"code","4657308a":"code","36b5075a":"code","05aead52":"code","802a10bc":"code","405e4710":"code","d3bc8017":"code","90e02794":"code","b81c4f84":"code","aab2351a":"code","58b8b010":"code","1443130d":"code","0a63c96c":"code","7eb2f950":"code","4893edb0":"code","fa4d705b":"code","263171d1":"code","339b53c4":"code","bcff46d2":"code","a37ce8f1":"code","a195e283":"code","6976a668":"code","3dd1acaa":"code","dae2a11b":"code","b4f8fe1e":"code","b2d55b24":"code","92e35966":"code","a5344764":"code","6c7040bd":"code","53d12981":"code","e493b357":"code","6bdf96f7":"code","75732fcc":"code","f055f6e4":"code","c7ab0541":"code","ffb3cdd2":"code","8710ca86":"code","35f92875":"code","8f0f2b7b":"code","16d7c037":"code","6376643d":"markdown","296cea3f":"markdown","76cf6e61":"markdown","65e92c58":"markdown","76bb12a3":"markdown","126008b5":"markdown","b211094c":"markdown","92efc54d":"markdown","a192e197":"markdown","0129f68b":"markdown","23715af4":"markdown","0962f40b":"markdown","cbb1c0dc":"markdown","fdc49d4d":"markdown","c18f7b45":"markdown","8ebdf78e":"markdown","39d0cead":"markdown","616f2d85":"markdown","a24cebd2":"markdown","309db209":"markdown","a542d2ac":"markdown","980f98ff":"markdown","4437f0bd":"markdown","d0e6c84f":"markdown","b9a860b8":"markdown","214f2445":"markdown","e7ae95bf":"markdown","eb6c3e34":"markdown","97ee5bf3":"markdown","d7e777bc":"markdown","bdbc4dda":"markdown","1d1da29d":"markdown","0b52002e":"markdown","bd07d808":"markdown","bd6af4f7":"markdown","8828bf8d":"markdown","b59bbf60":"markdown","5f05f7cc":"markdown","73789185":"markdown","303278e9":"markdown","6d40706a":"markdown","939ec5b5":"markdown","37409864":"markdown","320ffa11":"markdown","05656365":"markdown","2840f9db":"markdown","9552136e":"markdown","e3456bc7":"markdown"},"source":{"3ef55e66":"\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        ","078e4100":"training = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nIDtest=test[\"PassengerId\"]\n\ntraining['train_test'] = 1\ntest['train_test'] = 0\ntest['Survived'] = np.NaN\nall_data = pd.concat([training,test])\n\n%matplotlib inline\nall_data.columns","cc6574e5":"#quick look at the dataset\ntraining.info()\ntraining.describe()","3bbea820":"#1.Datasets for EDA for numeric and categorical variables seperately  \ndf_num = training[['Age','SibSp','Parch','Fare']]\ndf_cat = training[['Survived','Pclass','Sex','Ticket','Cabin','Embarked']]","32847a25":"#2. Distribution of variables - numeric\nfor i in df_num.columns:\n    plt.hist(df_num[i])\n    plt.title(i)\n    plt.show()","eb52d863":"#3. Correlation between variables - numeric\nprint(df_num.corr())\nsns.heatmap(df_num.corr())","b3bb9d74":"# Survival status across variables - numeric  \npd.pivot_table(training, index = 'Survived', values = ['Age','SibSp','Parch','Fare'])","26fa5f11":"for i in df_cat.columns:\n    sns.barplot(df_cat[i].value_counts().index,df_cat[i].value_counts()).set_title(i)\n    plt.show()","86bc1d64":"#2. Distribution of variables - categorical\n#for i in df_cat.columns:\n    #plt.hist(df_cat[i])\n    #plt.title(i)\n    #plt.show()","6d7a2469":"training[\"Name\"].head(20)","89bf7835":"name=training[\"Name\"]\ntraining[\"Title\"]=[i.split(\".\")[0].split(\",\")[-1].strip() for i in name]\n#training[\"Title\"].head(20)","7484a929":"sns.countplot(x=\"Title\", data = training)\nplt.xticks(rotation = 60)\nplt.show()","4577a3e6":"#convert the title's to categorical values\n# convert to categorical\ntraining[\"Title\"] = training[\"Title\"].replace([\"Lady\",\"the Countess\",\"Capt\",\"Col\",\"Don\",\"Dr\",\"Major\",\"Rev\",\"Sir\",\"Jonkheer\",\"Dona\"],\"other\")\ntraining[\"Title\"] = [0 if i == \"Master\" else 1 if i == \"Miss\" or i == \"Ms\" or i == \"Mlle\" or i == \"Mrs\" else 2 if i == \"Mr\" else 3 for i in training[\"Title\"]]\n","178121fd":"training[\"Title\"].head(20)","4050bcb5":"# survival probability based on title\ng = sns.factorplot(x = \"Title\", y = \"Survived\", data = training, kind = \"bar\")\ng.set_xticklabels([\"Master\",\"Mrs\",\"Mr\",\"Other\"])\ng.set_ylabels(\"Survival Probability\")\nplt.show()","c4231b71":"training.drop(labels = [\"Name\"], axis = 1, inplace = True)\n","f0255cf4":"training.head()","924d6317":"training = pd.get_dummies(training,columns=[\"Title\"])\n\n","15ab4865":"training.head()","a5a09b84":"training[\"Fsize\"] = training[\"SibSp\"] + training[\"Parch\"] + 1\ntraining[\"Fsize\"].head()","a580d912":"g = sns.factorplot(x = \"Fsize\", y = \"Survived\", data = training, kind = \"bar\")\ng.set_ylabels(\"Survival\")\nplt.show()","db4e75d5":"training[\"family_size\"] = [1 if i < 5 else 0 for i in training[\"Fsize\"]]\ntraining[\"family_size\"].head()","eb515268":"g = sns.factorplot(x = \"family_size\", y = \"Survived\", data = training, kind = \"bar\")\ng.set_ylabels(\"Survival\")\n","6174c6e0":"training[\"Embarked\"].head()\nsns.countplot(x = \"Embarked\", data = training)\n","fa049a73":"training = pd.get_dummies(training, columns=[\"Embarked\"])\ntraining.head()","73474258":"training[\"Ticket\"].head(20)\na = \"A\/5. 2151\"\na.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(\" \")[0]","e2483cb1":"tickets = []\nfor i in list(training.Ticket):\n    if not i.isdigit():\n        tickets.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(\" \")[0])\n    else:\n        tickets.append(\"x\")\ntraining[\"Ticket\"] = tickets\n\ntraining[\"Ticket\"].head(20)","1e33492c":"training.head(20)\n#get dummies based on tickets type\ntraining = pd.get_dummies(training, columns= [\"Ticket\"], prefix = \"T\")\ntraining.head(10)","55419495":"sns.countplot(x = \"Pclass\", data = training)\n","0a3f5470":"training[\"Pclass\"] = training[\"Pclass\"].astype(\"category\")\ntraining = pd.get_dummies(training, columns= [\"Pclass\"])\ntraining.head()","78297e65":"print(pd.crosstab(training.Survived,training.Sex))\n\nplt.figure(figsize=(12,5))\nax=sns.countplot(x=\"Sex\", data=training, hue=\"Survived\",palette=\"hls\")\ntitanic = sns.load_dataset(\"titanic\")\ntotal = float(len(titanic)) # one person per row \nplt.title('Sex Distribuition by survived or not', fontsize=20)\nplt.xlabel('Sex Distribuition',fontsize=17)\nplt.ylabel('percent', fontsize=17)\n\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height\/total),\n            ha=\"center\") \n\nplt.show()","751866bc":"training[\"Sex\"] = training[\"Sex\"].astype(\"category\")\ntraining = pd.get_dummies(training, columns=[\"Sex\"])\ntraining.head()","9677c344":"#Drop unrelated variables\ntraining.drop(labels = [\"PassengerId\", \"Cabin\"], axis = 1, inplace = True)","88c190a7":"training.columns\ntraining.head()","03d1fc65":"# Drop Name variable\n#Training.drop(labels = [\"Name\"], axis = 1, inplace = True)","961dddb3":"g = sns.FacetGrid(training, col='Survived')\ng.map(plt.hist, 'Age', bins=25)","ad5254c0":"import pandas as pd\npd.set_option('max_columns', None)\ndf = pd.read_csv(\"..\/input\/titanic\/train.csv\", encoding='latin-1').set_index(\"PassengerId\")\n\nimport missingno as msno\nimport matplotlib.pyplot as plt\nmsno.bar(df, figsize=(12, 6), fontsize=12, color='steelblue')","d9e593af":"#Other way to check missing values for each column\ntraining.loc[:, training.isnull().any()].columns #suggest that only age column is missing","3ac30256":"training","276bc287":"all_data[\"Sex\"] =all_data[\"Sex\"].map({\"male\": 0, \"female\":1})","a69e362c":"dataset=all_data","300bd2c7":"index_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\n\nfor i in index_NaN_age :\n    age_med = dataset[\"Age\"].median()\n    age_pred = dataset[\"Age\"][((dataset['SibSp'] == dataset.iloc[i][\"SibSp\"]) & (dataset['Parch'] == dataset.iloc[i][\"Parch\"]) & (dataset['Pclass'] == dataset.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred) :\n        dataset['Age'].iloc[i] = age_pred\n    else :\n        dataset['Age'].iloc[i] = age_med","4657308a":"g = sns.catplot(x=\"Survived\", y = \"Age\",data = training, kind=\"box\")\ng = sns.catplot(x=\"Survived\", y = \"Age\",data = training, kind=\"violin\")","36b5075a":"from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\n","05aead52":"# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=10)","802a10bc":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom collections import Counter\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n\nsns.set(style='white', context='notebook', palette='deep')","405e4710":"training_len=len(training)\ntraining_len","d3bc8017":"#a=training #living a copy of dataset for use late\ntraining","90e02794":"y_train = training[\"Survived\"]","b81c4f84":"y_train","aab2351a":"train = training[:training_len]\nX_train = train.drop(labels = \"Survived\", axis = 1)\n#y_train = train[\"Survived\"]","58b8b010":"training_len","1443130d":"test = training[:training_len]\ntest","0a63c96c":"training","7eb2f950":"test.drop(labels=[\"Survived\"],axis = 1,inplace=True)\n","4893edb0":"test","fa4d705b":"X_train","263171d1":"from xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nxgb_model = XGBClassifier(\n        objective = 'binary:logistic',\n        colsample_bytree = 0.8,\n        learning_rate = 0.3,\n        max_depth = 7,\n        min_child_weight = 3,\n        n_estimators = 100,\n        subsample = 0.6)\n\n# {'colsample_bytree': 0.5,\n#  'learning_rate': 0.1,\n#  'max_depth': 7,\n#  'min_child_weight': 3,\n#  'n_estimators': 100,\n#  'objective': 'binary:logistic',\n#  'subsample': 0.7}\n\n%time xgb_model.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_train, y_train)], verbose=False)\ny_pred_xgb = xgb_model.predict(X_train)\na_xgb = accuracy_score(y_train, y_pred_xgb)\n\nprint(\"AS: \", a_xgb)","339b53c4":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator=xgb_model,X=X_train,y=y_train,cv=5)\nprint(\"Accuracy: {:.2f}%\".format(accuracies.mean()*100))","bcff46d2":"y_train","a37ce8f1":"X_train.fillna(method='ffill', inplace=True)","a195e283":"y_train.fillna(method='ffill', inplace=True)","6976a668":"X_train","3dd1acaa":"# Modeling step Test differents algorithms \nrandom_state = 2\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(LinearDiscriminantAnalysis())\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train, y = y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","dae2a11b":"### META MODELING  WITH ADABOOST, RF, EXTRATREES and GRADIENTBOOSTING\n\n# Adaboost\nDTC = DecisionTreeClassifier()\n\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\n\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsadaDTC.fit(X_train,y_train)\n\nada_best = gsadaDTC.best_estimator_","b4f8fe1e":"gsadaDTC.best_score_","b2d55b24":"#ExtraTrees \nExtC = ExtraTreesClassifier()\n\n\n## Search grid for optimal parameters\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsExtC.fit(X_train,y_train)\n\nExtC_best = gsExtC.best_estimator_\n\n# Best score\ngsExtC.best_score_","92e35966":"# RFC Parameters tunning \nRFC = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC.fit(X_train,y_train)\n\nRFC_best = gsRFC.best_estimator_\n\n# Best score\ngsRFC.best_score_","a5344764":"# Gradient boosting tunning\n\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsGBC.fit(X_train,y_train)\n\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngsGBC.best_score_","6c7040bd":"### SVC classifier\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsSVMC.fit(X_train,y_train)\n\nSVMC_best = gsSVMC.best_estimator_\n\n# Best score\ngsSVMC.best_score_","53d12981":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"b\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"r\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"r\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(gsRFC.best_estimator_,\"RF mearning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(gsSVMC.best_estimator_,\"SVC learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(gsadaDTC.best_estimator_,\"AdaBoost learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(gsGBC.best_estimator_,\"GradientBoosting learning curves\",X_train,y_train,cv=kfold)","e493b357":"nrows = ncols = 2\nfig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,15))\n\nnames_classifiers = [(\"AdaBoosting\", ada_best),(\"ExtraTrees\",ExtC_best),(\"RandomForest\",RFC_best),(\"GradientBoosting\",GBC_best)]\n\nnclassifier = 0\nfor row in range(nrows):\n    for col in range(ncols):\n        name = names_classifiers[nclassifier][0]\n        classifier = names_classifiers[nclassifier][1]\n        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n        g = sns.barplot(y=X_train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row][col])\n        g.set_xlabel(\"Relative importance\",fontsize=10)\n        g.set_ylabel(\"Features\",fontsize=6)\n        g.tick_params(labelsize=12)\n        g.set_title(name + \" feature importance\")\n        nclassifier += 1","6bdf96f7":"y_train","75732fcc":"X_train","f055f6e4":"list(X_train)","c7ab0541":"test.fillna(method='ffill', inplace=True)","ffb3cdd2":"exists=\"Kelly, Mr. James\" in X_train[0:]\nprint(exists)","8710ca86":"test_Survived_RFC = pd.Series(RFC_best.predict(test), name=\"RFC\")\ntest_Survived_ExtC = pd.Series(ExtC_best.predict(test), name=\"ExtC\")\ntest_Survived_SVMC = pd.Series(SVMC_best.predict(test), name=\"SVC\")\ntest_Survived_AdaC = pd.Series(ada_best.predict(test), name=\"Ada\")\ntest_Survived_GBC = pd.Series(GBC_best.predict(test), name=\"GBC\")\n\n\n# Concatenate all classifier results\nensemble_results = pd.concat([test_Survived_RFC,test_Survived_ExtC,test_Survived_AdaC,test_Survived_GBC, test_Survived_SVMC],axis=1)\n\n\ng= sns.heatmap(ensemble_results.corr(),annot=True)","35f92875":"votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best),\n('svc', SVMC_best), ('adac',ada_best),('gbc',GBC_best)], voting='soft', n_jobs=4)\n\nvotingC = votingC.fit(X_train, y_train)","8f0f2b7b":"test_Survived = pd.Series(votingC.predict(test), name=\"Survived\")\n\nresults = pd.concat([IDtest,test_Survived],axis=1)\n\nresults.to_csv(\"Final_ML_results.csv\",index=False)","16d7c037":"#lets have a glimpse of the prediction we have got:\n\nimport pandas\ndf = pandas.read_csv(\"Final_ML_results.csv\")\ndf.head(100)","6376643d":"# 3.7 Missing values imputation ","296cea3f":"> Best way to check for missing values in dataset.","76cf6e61":"<a id = \"5\"><\/a><br>\n# 3.3 Embarked","65e92c58":"<center><img \nsrc=\"https:\/\/media1.tenor.com\/images\/ed9c5b036d76821adbc74201dc1619d0\/tenor.gif?itemid=3386898\" width=\"700\" height=\"700\"><\/img><\/center>\n> \n<br>","76bb12a3":"<a id = \"12\"><\/a><br>\n# Ensemble modeling","126008b5":"# 1. Gathering data","b211094c":"<a id = \"3\"><\/a>\n# 3.1 Title of passenger","92efc54d":"> We finally pick SVC, AdaBoost, RandomForest , ExtraTrees and the GradientBoosting classifiers for our ensemble modeling.","a192e197":"> Interpretation\n\n   > The figure above shows, overall GradientBoosting and Adaboost classifiers tend to overfit in the training set, by quite a margin. This could be improved by providing more training examples.\n\n   > For SVC and ExtraTrees classifiers, cross-validation curves were close together which suggest that the training set performed as close or similar to the cross-validation score. The margins were bigger if the training sample size was <300.","0129f68b":"# 3. Feature engineering","23715af4":"# 3.6 Age ","0962f40b":"> for simplicity we will later replace missing with most common occurance and 'NaN' whichever more appropriate.","cbb1c0dc":"**Above we can see that:**\n    1. Higher mortalities in children aged <5 years.\n    2. Higher mortality in youths an younger adults aged 20-40 years.\n    3. Most passengers were in 15-60 years (lowest and highest extremes being ~1 and 80 years).","fdc49d4d":"# Hyperparameter tunning for best models\n> Here we use the grid search optimization for AdaBoost, ExtraTrees , RandomForest, GradientBoosting and SVC classifiers.","c18f7b45":"**Above we can see that:**\n    1. Nearly 53% men died in the accident compared to 9% of women.","8ebdf78e":"> Here, the violin plot suggest that the those who were relatively young had higher survival rates, comapred to their older counterparts.\n\n> Also, you may prefer to try multiple imputations (e.g. it worth doign if there are more than one missing variables) using this blog [@athi94](https:\/\/www.kaggle.com\/athi94\/investigating-imputation-methods).","39d0cead":"RMS Titanic was one of the largest ocean liners built between 1909-11 in the UK. The infamous voyage that led to sinking and disappearence happend on 14th April 1912. Reports suggest that the ocean liners struck an iceberg which caused a considerable damage on the side and the ship began to flood. Within the two hours of this happening, Titanic sank into deep ocean, never to be recovered.  \n\nHowever, it was only after the [James Cameroon](https:\/\/en.wikipedia.org\/wiki\/James_Cameron)'s Movie [Tatanic](https:\/\/en.wikipedia.org\/wiki\/Titanic_(1997_film)) releasing in 1997, people (around the world) began to noticing about this incident. The film was a commercial success grossing 2.2 Billion USD in global market and was nominated for 14 ocars, of which it won 11 of them. People loved Jack and Rose who were main charaters in the movie. These charaters still occupies special place in our childhood memories,and reminds us about the tragic accident that happened a hundread years ago.\n\n**Charateristics of Titanic - in the lead up to the accident.**\n    1. In todays terms, the ship worths 190 million USD. The same price as a Boeing's 767 would cost, carrying ~250 passengers. \n    2. 46,000 horse power engine supporting 52,310 tons of weight (length: 269m, height: 53, depth: 20m).\n    3. 2435 passengers and 892 crews (1 crew for every 3 passengers).\n    4. Surprisingly it had only 20 lifeboats which could support upto 1178 people. \n    \n**Accident: why it worth a detailed look and even a ML based model?**\n<br>\nThere are important lessons to be learnt from the ship's accident and sinking. The number of deaths and casualties from the disaster clearly suggest a preponderance of higher mortality among children, women and men from low socio-economic status (those buying third class tickets) compared to those from second and first class. Nearly 52 children (66%), 89 women (54%) and 387 men (84%) lost thier life from third class compared to their counterparts. A significant number of male crews: 693(78%) lost their life compared to 3(13%) of female crews.\n\n<center><iframe width=\"660\" height=\"400\" src=\"https:\/\/www.youtube-nocookie.com\/embed\/05o7sOAjtXE\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe><\/center>\n<br>\nTherefore, using the data provided in this Kaggle competition, I will build series of models (basic and ensemble) to predict the risk of fatalities and bring important insights for future consideration. My focus will be *EDA, visualization, model building and validation* while also focusing on presenting the results nicely and succintly. Please leave your comments and suggestions below and if possible, *Upvote* my notebook which will massively help me to remain motivated and bring awesome contents in future.\n","616f2d85":"<a id = \"10\"><\/a><br>\n# K-folds cross validation","a24cebd2":"<a id = \"4\"><\/a><br>\n# 3.2 Family size","309db209":"> Here, we bring some feature engineering: recoding the variables differently and manipulating with missing values to improve the prediction of mortality outcomes in Titanic dataset.","a542d2ac":"I would like to massively thank following Kagglers namely:[@bhartiprasad17](https:\/\/www.kaggle.com\/bhartiprasad17\/titanic-survival-model), [@adamml ](https:\/\/www.kaggle.com\/adamml\/titanic-to-beginner)and [@kenjee](https:\/\/www.kaggle.com\/kenjee\/titanic-project-example) for their inspiring and very thorough notebooks - which has immensely helped me in understand both the methodological as well as pratical aspects of building a ML-based risk prediction model. \n\n> I will try to further improve the current model as well apply the technique in cardiovascular disease risk prediction or disease (risk) prediction in general, so if you have a similar interest then we can potentially team up.","980f98ff":"<a id=\"9\"><\/a><h1 style='background:#7ad16d; border:0; color:white'><center> Basic model building <\/center><\/h1>","4437f0bd":"**Above we can see that:**\n1. Smaller families (<5 members) had nearly two times higher survival probability compared to larger families (>=5 members)","d0e6c84f":"<a id=\"14\"><\/a><h1 style='background:#7ad16d; border:0; color:white'><center> Conclusions <\/center><\/h1>","b9a860b8":"# Cross validation of model","214f2445":"<a id = \"7\"><\/a><br>\n# 3.4 Pclass","e7ae95bf":"1. [Introduction](#1)\n1. [Data cleaning, exploration and preprocessing](#2)\n     * [Title of passenger](3)\n     * [Family size](4)\n     * [Embarked](5)\n     * [Ticket](6)\n     * [Pclass](7)\n     * [Sex](8)\n1. [Basic model building](#9)\n     * [K folds cross validation](10)\n     * [Fine tuning the Xab model](11)\n     * [Ensemble modeling](12)\n1. [Key results (summary](#13)\n1. [Conclusions](#14)","eb6c3e34":"> Interpretation\n> Showing a modest correlation between these classifiers (overall ~.80 ): with lowest between Adaboost and ExtraTrees feature importance. Given these difference, one might consider to opt for a ensemble modeling, which is shown in the steps below:","97ee5bf3":"# Plot learning curves\nHere, we are trying to plot the learning curves which shows the fitting effect of the model on the training set and the model accuracy.","d7e777bc":"Here, we compare the relative performance of each of classifiers (below) by using a stratified kfold cross validation procedure.\n    * SVC\n    * Decision Tree\n    * AdaBoost\n    * Random Forest\n    * Extra Trees\n    * Gradient Boosting\n    * Multiple layer perceprton (neural network)\n    * KNN\n    * Logistic regression\n    * Linear Discriminant Analysis","bdbc4dda":"# 2. Assessment of data","1d1da29d":"The model performed sufficiently well, correctly descriminating ~80% of fatalities. Further improvement in the performance will require a deeper feature engineering - some of which has been covered in previous notebooks published in Kaggle, namely:[@bhartiprasad17](https:\/\/www.kaggle.com\/bhartiprasad17\/titanic-survival-model), [@adamml ](https:\/\/www.kaggle.com\/adamml\/titanic-to-beginner)and [@kenjee](https:\/\/www.kaggle.com\/kenjee\/titanic-project-example) .","0b52002e":"**Prediction**","bd07d808":"The results here showed which features (of the dataset) are important in predicting the fatalities based on tree based classifiers.","bd6af4f7":"> Interpretation\n\n> These graphs share some important features: such as Title, sex, fare and age emerged as a strong driver of fatatalities though, important differences remain in the ranking of these features based on their relative importance. Clearly, we can suggest that fatalities in Titanic disaster was based on the relative social and economic standing of the passenger on board. Those lucky to have been survived were probably: older\/children, female sex, and from higher economic status (based on tickets they purchased e.g. fares, titles).","8828bf8d":"<a id = \"8\"><\/a><br>\n# 3.5 Sex","b59bbf60":"<a id=\"4\"><\/a><h1 style='background:#7ad16d; border:0; color:white'><center> Table of contents <\/center><\/h1>","5f05f7cc":"<a id = \"11\"><\/a><br>\n# Important features of tree based classifiers","73789185":"<a id=\"1\"><\/a><h1 style='background:#7ad16d; border:0; color:white'><center> Introduction <\/center><\/h1>","303278e9":"<a id=\"2\"><\/a><h1 style='background:#7ad16d; border:0; color:white'><center> Data cleaning, exploration and preprocessing <\/center><\/h1>","6d40706a":"**Above we can see that:**\n    1. Only three columns have missing values: age, cabin and embarked.\n    2. Cabin - this variable is not going to be used. No need to impute\n    3. For age and embarked - we can do imputation.","939ec5b5":"<a id = \"5.1\"><\/a><br>\n# After working in Titanic project (for hours), and waiting for upvote from peers is like: \n<br>\n<center><img \nsrc=\"https:\/\/media1.tenor.com\/images\/bbf211a950b564ab385e880ab436eff9\/tenor.gif?itemid=5781405\" width=\"700\" height=\"700\"><\/img><\/center>\n \n<br>\n\n\n\n*Guys upvote and we will sail our Kaggly boat together.*","37409864":"Information about variables\n* **PassengerId**: unique id number to each passenger\n* **Survived**: passenger survive(1) or died(0)\n* **Pclass**: passenger class\n* **Name**: name\n* **Sex**: gender of passenger\n* **Age**: age of passenger\n* **SibSp**: number of siblings\/spouses\n* **Parch**: number of parents\/children\n* **Ticket**: ticket number\n* **Fare**: amount of money spent on ticket\n* **Cabin**: cabin category\n* **Embarked**: port where passenger embarked (C = Cherbourg, Q = Queenstown, S = Southampton)","320ffa11":"<a id=\"14\"><\/a><h1 style='background:#7ad16d; border:0; color:white'><center> Acknowledgement <\/center><\/h1>","05656365":"Here, we download the files: (train.csv) and (test.csv) and read them using read_csv.","2840f9db":"\n<a id=\"7\"><\/a><h1 style='background:#0f9124; border:18; color:azure'><center> ML based predictions from the Titanic disaster: revelations  <\/center><\/h1>","9552136e":"<a id = \"6\"><\/a><br>\n# 3.4 Ticket","e3456bc7":"**Combined model**"}}