{"cell_type":{"b57704e1":"code","4128ac55":"code","fcab0954":"code","805435bc":"code","855f6787":"code","eb0a4c60":"code","4557ec05":"code","38817eb9":"code","d975796e":"code","fe8811a6":"code","ffd74024":"code","1d780cea":"code","944775b0":"code","eab41b16":"code","a977d7f7":"code","fcd7a5fe":"code","f8f9b191":"code","951a915b":"code","7001f0a5":"code","ed47c6c3":"code","98e2a9d7":"code","08d8aaa3":"code","93e4f49b":"code","347c17df":"code","347e5371":"code","df7a94a5":"code","ca2e1336":"code","dccd7f86":"code","f421ffdd":"code","73914fc0":"code","7b610e66":"code","426a86a2":"code","6e42579a":"code","839960ad":"code","29099266":"code","74e7f0bd":"code","3c9acf74":"markdown","d69a930f":"markdown","ac4d28dd":"markdown","ddc7479f":"markdown","7c2afa8e":"markdown","401bc618":"markdown"},"source":{"b57704e1":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgbm\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.cluster import KMeans\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold","4128ac55":"train = pd.read_csv(\"..\/input\/turkiye-is-bankasi-machine-learning-challenge-4\/MLChallenge4\/train.csv\")\ntest = pd.read_csv(\"..\/input\/turkiye-is-bankasi-machine-learning-challenge-4\/MLChallenge4\/test.csv\")","fcab0954":"test.drop(columns=\"ID\", inplace=True)\ntest.head(2)","805435bc":"test[\"TARGET\"] = 2\nall_data = pd.concat([train,test],axis=0)\nall_data.shape","855f6787":"all_data.drop(columns=[\"TXN_TRM\",\"MC_NAME\"], inplace=True)","eb0a4c60":"cat_cols = ['CST_NR', 'CC_NR', 'DAY_OF_MONTH',\n       'TXN_SOURCE', 'TXN_ENTRY', 'CITY', 'COUNTRY',\n   'MC_ID', 'MCC_CODE']","4557ec05":"all_data[cat_cols] = all_data[cat_cols].fillna(\"missing\")","38817eb9":"# Label encoding categorical features\nfrom sklearn import preprocessing\nfor col in cat_cols:\n    print(col)\n    le = preprocessing.LabelEncoder()\n    all_data[col] = le.fit_transform(all_data[col].values)","d975796e":"all_data = all_data.sort_values(by=[\"DAY_OF_MONTH\",\"TXN_TIME\"])\nall_data[\"top_islem1\"] = 1\nall_data['date_block_num'] = range(933739)","fe8811a6":"def lag_feature(all_data, lags, col):\n    tmp = all_data[['CST_NR','DAY_OF_MONTH', 'CITY', 'COUNTRY','date_block_num',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['CST_NR','DAY_OF_MONTH', 'CITY', 'COUNTRY','date_block_num', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        all_data = pd.merge(all_data, shifted, on=['CST_NR','DAY_OF_MONTH', 'CITY', 'COUNTRY','date_block_num'], how='left')\n    return all_data","ffd74024":"all_data = lag_feature(all_data, [1],\"top_islem1\")\nall_data[\"loc_sum\"] = all_data.groupby(['DAY_OF_MONTH','CST_NR'])['top_islem1_lag_1'].transform('sum') ","1d780cea":"all_data['TXN_AMNT'] = StandardScaler().fit_transform(all_data['TXN_AMNT'].values.reshape(-1, 1))","944775b0":"all_data[\"top_islem1\"] = 1\nall_data[\"weekend\"]  = np.where((all_data[\"DAY_OF_WEEK\"] < 5.5)&(all_data[\"DAY_OF_WEEK\"] > 1.5),0 ,1)\nall_data['top_isyeri_islem'] = all_data.groupby(['DAY_OF_MONTH','MC_ID'])['top_islem1'].transform('sum') #\u00fcye i\u015fyeri ba\u015f\u0131na g\u00fcnl\u00fck toplam i\u015flem\n#all_data['top_isyeri_saniyelik_islem'] = all_data.groupby(['DAY_OF_MONTH','TXN_TIME', 'MC_ID'])['top_islem1'].transform('sum')  #\u00fcye i\u015fyeri ba\u015f\u0131na saniyelik toplam i\u015flem\nall_data['top_mus_islem'] = all_data.groupby(['DAY_OF_MONTH','CST_NR'])['top_islem1'].transform('sum') #m\u00fc\u015fteri no ba\u015f\u0131na g\u00fcnl\u00fck toplam i\u015flem\nall_data['top_kart_islem'] = all_data.groupby(['DAY_OF_MONTH','CC_NR'])['top_islem1'].transform('sum') #kart no ba\u015f\u0131na g\u00fcnl\u00fck toplam i\u015flem\nall_data['isyeri_top_tutar'] = all_data.groupby(['DAY_OF_MONTH','MC_ID'])['TXN_AMNT'].transform('sum') #\u00fcye i\u015fyeri ba\u015f\u0131na g\u00fcnl\u00fck toplam tutar\nall_data['top_mussaniyelik_islem'] = all_data.groupby(['DAY_OF_MONTH','TXN_TIME','CST_NR'])['top_islem1'].transform('sum') #m\u00fc\u015fteri no ba\u015f\u0131na saniyelik toplam i\u015flem\nall_data['top_mussaniyelik_ort_tutar'] = all_data.groupby(['DAY_OF_MONTH','TXN_TIME','CST_NR'])['TXN_AMNT'].transform('mean') #m\u00fc\u015fteri no ba\u015f\u0131na saniyelik ortalama tutar\nall_data['top_mussaniyelik_tutar'] = all_data.groupby(['DAY_OF_MONTH','TXN_TIME','CST_NR'])['TXN_AMNT'].transform('sum') #m\u00fc\u015fteri no ba\u015f\u0131na saniyelik toplam tutar\n\nall_data['isyeri_weekend_ort_tutar'] = all_data.groupby(['weekend','MC_ID'])['TXN_AMNT'].transform('mean') #\u00fcye i\u015fyeri ba\u015f\u0131na hsonu veya hi\u00e7i ortalama tutar\nall_data['isyeri_weekendsaniye_ort_tutar'] = all_data.groupby(['weekend','TXN_TIME','MC_ID'])['TXN_AMNT'].transform('mean')\nall_data['isyeri_weekend_ort_islem'] = all_data.groupby(['weekend','MC_ID'])['top_islem1'].transform('mean') #\u00fcye i\u015fyeri ba\u015f\u0131na hsonu veya hi\u00e7i ortalama islem\n\nall_data['isyeri_ort_tutar'] = all_data.groupby(['DAY_OF_MONTH','MC_ID'])['TXN_AMNT'].transform('mean')  #\u00fcye i\u015fyeri ba\u015f\u0131na g\u00fcnl\u00fck ortalama tutar\nall_data['isyeri_ort_saniyetutar'] = all_data.groupby(['DAY_OF_MONTH','TXN_TIME', 'MC_ID'])['TXN_AMNT'].transform('mean')\nall_data['mus_saniyeislem_yuku'] = all_data['top_mussaniyelik_tutar'] \/ all_data['top_mussaniyelik_ort_tutar']  # ki\u015finin tutar sapmas\u0131\nall_data['isyeri_saniyetutar_yuku'] = all_data['TXN_AMNT'] \/ all_data['isyeri_weekendsaniye_ort_tutar']\nall_data['isyeri_tutar_yuku'] = all_data['TXN_AMNT'] \/ all_data['isyeri_weekend_ort_tutar']\nall_data['isyeri_islem_yuku'] = all_data['top_isyeri_islem'] \/ all_data['isyeri_weekend_ort_islem']\nall_data['isyeri_tutar\/ort'] = all_data['TXN_AMNT'] \/ all_data['isyeri_ort_tutar'] # ki\u015finin tutar\u0131 \/ \u00fcye i\u015fyeri ba\u015f\u0131na g\u00fcnl\u00fck ortalama tutar\n#all_data['isyeri_saniye_tutar\/ort'] = all_data['TXN_AMNT'] \/ all_data['isyeri_ort_saniyetutar']\n#all_data['mus\/kart'] = all_data['top_mus_islem'] \/ all_data['top_kart_islem'] # m\u00fc\u015fteri no i\u015flem say\u0131s\u0131\/ kart no i\u015flem say\u0131s\u0131\nall_data['top_mus_islem'] = all_data.groupby(['CC_NR'])['top_islem1'].transform('sum') #m\u00fc\u015fteri no ba\u015f\u0131na  toplam i\u015flem\nall_data['top_isyeri_islem'] = all_data.groupby(['MC_ID'])['top_islem1'].transform('sum') #\u00fcye i\u015fyeri ba\u015f\u0131na  toplam i\u015flem\nall_data['m\u00fcsteri_isyerinde_islem'] = all_data.groupby(['CC_NR','MC_ID'])['top_islem1'].transform('sum') #M\u00fc\u015fterinin o i\u015fyerinde ka\u00e7 i\u015flemi var\n#all_data['kacinci_islem'] = all_data.groupby(['CC_NR','MC_ID'])['top_islem1'].transform('sum') #M\u00fc\u015fterinin o i\u015fyerindeki ka\u00e7\u0131nc\u0131 i\u015flemi\nall_data['mus_tutar_top'] = all_data.groupby(['CC_NR'])['TXN_AMNT'].transform('sum') #m\u00fc\u015fteri tutar topmal\u0131\nall_data['mus_tutar_top'].fillna(0, inplace = True)\nall_data['mus_tutar\/ort'] = all_data['TXN_AMNT'] \/ all_data['mus_tutar_top'] # ki\u015finin tutar\u0131 \/ m\u00fc\u015fteri tutar topmal\u0131\nall_data['mus_tutar\/ort'].fillna(0, inplace = True)\nall_data['isyeri_ort_tutar'] = all_data.groupby(['MC_ID'])['TXN_AMNT'].transform('mean')  #\u00fcye i\u015fyeri ba\u015f\u0131na ortalama tutar\nall_data['isyeri_ort_tutar'].fillna(0, inplace = True)\nall_data['gunlultop_mus_islem'] = all_data.groupby(['DAY_OF_MONTH','CST_NR'])['top_islem1'].transform('sum') #m\u00fc\u015fteri no ba\u015f\u0131na g\u00fcnl\u00fck toplam i\u015flem\nall_data['gunluktop_kart_islem'] = all_data.groupby(['DAY_OF_MONTH','CC_NR'])['top_islem1'].transform('sum') #kart no ba\u015f\u0131na g\u00fcnl\u00fck toplam i\u015flem\nall_data['gunluktop_isyeri_islem'] = all_data.groupby(['DAY_OF_MONTH','MC_ID'])['top_islem1'].transform('sum') #\u00fcye i\u015fyeri ba\u015f\u0131na g\u00fcnl\u00fck toplam i\u015flem","eab41b16":"all_data.drop(columns = [\"DAY_OF_WEEK\",\"DAY_OF_MONTH\"],  inplace=True)\nall_data['TARGET'] = all_data['TARGET'].apply(np.int32)","a977d7f7":"# Splitting back to train and test\ntest = all_data.iloc[len(train):]\ntrain = all_data.iloc[:len(train)]\ntrain.shape,test.shape","fcd7a5fe":"# get class distribution\nprint (\"Normal transaction:\", train['TARGET'][train['TARGET']==0].count()) #class = 0\nprint (\"Fraudulent transaction:\", train['TARGET'][train['TARGET']==1].count()) #class = 1","f8f9b191":"sns.countplot(train['TARGET'])","951a915b":"# separate classes into different datasets\nnormal_class = train.query('TARGET == 0')\nfraudulent_class = train.query('TARGET == 1')\n\n# randomize the datasets\nnormal_class = normal_class.sample(frac=1,random_state=1210)\nfraudulent_class = fraudulent_class.sample(frac=1,random_state=1210)","7001f0a5":"resampled = normal_class.sample(n=int(len(fraudulent_class)*4.4), random_state=1210)","ed47c6c3":"len(resampled)","98e2a9d7":"train = pd.concat([fraudulent_class,resampled])","08d8aaa3":"sns.countplot(train['TARGET'])","93e4f49b":"train","347c17df":"train.columns","347e5371":"# Selecting which features to use during modeling.\nfeatures = ['TXN_SOURCE',\"CC_NR\",\"CST_NR\",\n       'TXN_ENTRY', 'TXN_AMNT', 'CITY', 'COUNTRY', 'MC_ID', 'MCC_CODE',\n       'top_islem1', 'weekend', 'top_isyeri_islem',\n        'top_mus_islem', 'top_kart_islem','loc_sum',\n       'isyeri_top_tutar', 'top_mussaniyelik_islem',\n       'top_mussaniyelik_ort_tutar', 'top_mussaniyelik_tutar',\n       'isyeri_weekend_ort_tutar', 'isyeri_weekendsaniye_ort_tutar',\n       'isyeri_weekend_ort_islem', 'isyeri_ort_tutar',\n       'isyeri_ort_saniyetutar', 'mus_saniyeislem_yuku',\n       'isyeri_saniyetutar_yuku', 'isyeri_tutar_yuku', 'isyeri_islem_yuku',\n       'isyeri_tutar\/ort', \n       'm\u00fcsteri_isyerinde_islem', 'mus_tutar_top', 'mus_tutar\/ort',\n       'gunlultop_mus_islem', 'gunluktop_kart_islem',\n       'gunluktop_isyeri_islem']\nlen(features)","df7a94a5":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import fbeta_score","ca2e1336":"from sklearn.metrics import f1_score\n\ndef lgb_f1_score(y_hat, data):\n    y_true = data.get_label()\n    y_hat = np.where(y_hat < 0.5, 0, 1)   # scikits f1 doesn't like probabilities\n    return 'f1', f1_score(y_true, y_hat), True","dccd7f86":"y = train[\"TARGET\"]\nkf = KFold(n_splits=3, shuffle=True,random_state=3)\noof = np.zeros(len(train))\noofed = np.zeros(len(train))\nscore_list1 = []\nscore_list2 = []\nevals_result = {}\nfold = 1\ntest_preds = []\n\n\nfor train_index, test_index in kf.split(train):\n    X_train, X_val = train.iloc[train_index], train.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    \n    print(X_train.shape,X_val.shape)\n\n    \n    y_pred_list = []\n    for seed in [None]: # Add more values to this list if you want to have a multiple seed average.\n        dtrain = lgbm.Dataset(X_train[features], y_train)\n        dvalid = lgbm.Dataset(X_val[features], y_val)\n        print(seed)\n        params = {\"objective\": \"binary\",\n              \"metric\": \"binary_error\",\n              \"verbosity\": -1,\n              \"boosting_type\": \"gbdt\",\n                \"max_depth\" : 20,\n                \"num_leaves\" : 60,\n                \"learning_rate\" : 0.07,\n                    \"lambda_l1\":0,\n                  \"lambda_l2\": 0,\n               \"bagging_fraction\":0.4,\n                     \"feature_fraction\":0.4,\n                  \"early_stopping_round\":17,\n              \"max_bin\":35\n             }\n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                        dtrain,\n                        valid_sets=[dtrain, dvalid],\n                           feval=lgb_f1_score, evals_result=evals_result,\n                        verbose_eval=100,\n                        num_boost_round=1700,\n                        categorical_feature = ['COUNTRY', 'TXN_SOURCE',\"CC_NR\",\"CST_NR\",\n       'TXN_ENTRY',  'CITY',  'MC_ID', 'MCC_CODE','weekend']\n)\n\n    \n        y_pred_list.append(model.predict(X_val[features]))\n        print(roc_auc_score(y_val,   np.mean(y_pred_list,axis=0)))\n        y_val_f1 = np.where(y_val >= 0.5, 1, 0)\n        y_pred_f1 = np.where(np.mean(y_pred_list,axis=0)>= 0.5, 1, 0)\n        print(fbeta_score(y_val_f1,   y_pred_f1, average='binary', beta=0.5))\n        test_preds.append(model.predict(test[features]))\n        lgbm.plot_metric(evals_result, metric='binary_error')\n        \n    \n    oof[test_index] = np.mean(y_pred_list,axis=0) \n    oofed[test_index] = np.where(np.mean(y_pred_list,axis=0) >= 0.5, 1, 0)\n    score1 = roc_auc_score(y_val_f1,oof[test_index])\n    score2 = fbeta_score(y_val_f1,oofed[test_index], average='binary', beta=0.5)\n\n    print(f\"AUC Fold-{fold} : {score1}\")\n    print(f\"F1 Fold-{fold} : {score2}\")\n    score_list1.append(score1)\n    score_list2.append(score2)\n    fold+=1\n\nnp.mean(score_list1)\nnp.mean(score_list2)","f421ffdd":"# Full oof score.\nroc_auc_score(y,oof)","73914fc0":"# Fold scores.\nscore_list2","7b610e66":"# Feature importance\nlgbm.plot_importance(model,figsize=(10,10))","426a86a2":"test = test.reset_index().rename(columns={'index': 'Id'})","6e42579a":"test[\"Predicted\"] = np.mean(test_preds,axis=0)\ntest[[\"Id\",\"Predicted\"]].head()","839960ad":"test[\"binary\"]  = np.where(test[\"Predicted\"] >= 0.5, 'bir', 'sifir')","29099266":"test.binary.value_counts()","74e7f0bd":"test[[\"Id\",\"Predicted\"]].to_csv(\"submission.csv\",index=False)","3c9acf74":"# Undersampling","d69a930f":"# Groupby Features","ac4d28dd":"The above graph shows that most of the fraudulent transactions are of very low amount","ddc7479f":"# Lag Features","7c2afa8e":"# F1 Score def for LGBM","401bc618":"# LGBM"}}