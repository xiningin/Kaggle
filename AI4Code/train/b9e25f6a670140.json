{"cell_type":{"ad318876":"code","f0f1dd43":"code","e27c82be":"code","00bdfe29":"code","3245f27c":"code","9b44f4aa":"code","171da075":"code","f391dd60":"markdown","305537cf":"markdown","36ddd8a5":"markdown","dc8e532a":"markdown","c02c2dce":"markdown"},"source":{"ad318876":"import pandas as pd","f0f1dd43":"book_train = pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet')","e27c82be":"book_train.info()","00bdfe29":"! ls ..\/input\/optiver-realized-volatility-prediction\/book_train.parquet | head -n 5","3245f27c":"! ls ..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id=0\/","9b44f4aa":"book_train_0 = pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id=0\/c439ef22282f412ba39e9137a3fdabac.parquet')\nbook_train_0.info()","171da075":"import glob\nsubset_paths = glob.glob('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id=11*\/*')\nbook_train_subset = pd.read_parquet(subset_paths)\nbook_train_subset.info()","f391dd60":"Note that because we loaded a single partition, **the partition column was not included**. We could remedy that manually if we need the stock ID or just load a larger subset of the data by passing a list of paths. This will load all of the stock IDs 110-119, reducing memory usesage without implicitly dropping the partition column:","305537cf":"[Apache Parquet](https:\/\/arrow.apache.org\/docs\/python\/parquet.html) is an efficient columnar storage format. Compared to saving this dataset in csvs using parquet:\n- Greatly reduces the necessary disk space\n- Loads the data into Pandas with memory efficient datatypes\n- Enables fast reads from disk\n- Allows us to easily work with partitions of the data\n\nPandas has a parquet integration that makes loading data into a dataframe trivial; we'll try that now.","36ddd8a5":"The one exception is the `stock_id` column, which has been converted to the category type as it is [the partition column](https:\/\/arrow.apache.org\/docs\/python\/parquet.html#reading-from-partitioned-datasets). The parquet files in this dataset are all paritioned by `stock_id` so that it's not necessary to load the entire file at once. In fact, if you examine the parquet files you'll see that they are actually directories.","dc8e532a":"Those are in turn also directories, which would be relevant if the data were partitioned by more than one column.","c02c2dce":"If this data were stored as a csv, the numeric types would all default to the 64 bit versions. Parquet retains the more efficient types I specified while saving the data.\n\n**Expect memory usage to spike to roughly double the final dataframe size while parquet loads a file. Consider loading your largest dataset first or using partitions to mitigate this.**"}}