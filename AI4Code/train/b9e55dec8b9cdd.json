{"cell_type":{"e37e3e04":"code","9e7db0c7":"code","5c795bd8":"code","175a83a9":"code","6d80b5c0":"code","65bff704":"code","bf8e257b":"code","900b2d7f":"code","461222bb":"code","b83786b8":"code","3d02c4a5":"code","5fa66d98":"code","d92633b7":"code","2351e5a0":"code","ebb85f71":"code","a8868f41":"code","7a05c622":"code","b20bfe72":"code","df3c461c":"markdown","a93c42be":"markdown","6a13df50":"markdown","90ea59f5":"markdown","02a41ad3":"markdown","20852244":"markdown","4d629242":"markdown","ca71ae46":"markdown","84eab8da":"markdown","09bbaa40":"markdown"},"source":{"e37e3e04":"import gc\nimport re\nimport string\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport plotly.express as px\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.feature_extraction.text import CountVectorizer","9e7db0c7":"input_folder = \"\/kaggle\/input\/tweet-sentiment-extraction\/\"\ntrain = pd.read_csv(f'{input_folder}train.csv')\ntest = pd.read_csv(f'{input_folder}test.csv')\nss = pd.read_csv(f'{input_folder}sample_submission.csv')","5c795bd8":"print(\"Shape of train data\",train.shape)\nprint(\"Shape of test data\",test.shape)","175a83a9":"train.head()","6d80b5c0":"train = train.dropna()\ntest = test.dropna()\nprint(\"Number of missing value in the train data\",train.isnull().sum())\nprint(\"Number of missing value in the test data\",test.isnull().sum())\n","65bff704":"#We are calculating the meta features only for the test column as it is common in both train and test\n\n#word_count\ntrain['word_count'] = train['text'].apply(lambda x:len(str(x).split()))\ntest['word_count'] = test['text'].apply(lambda x:len(str(x).split()))\n\n# unique_word_count\ntrain['unique_word_count'] = train['text'].apply(lambda x: len(set(str(x).split())))\ntest['unique_word_count'] = test['text'].apply(lambda x: len(set(str(x).split())))\n\n# stop_word_count\ntrain['stop_word_count'] = train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\ntest['stop_word_count'] = test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n# url_count\ntrain['url_count'] = train['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\ntest['url_count'] = test['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n\n# mean_word_length\ntrain['mean_word_length'] = train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest['mean_word_length'] = test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\n# char_count\ntrain['char_count'] = train['text'].apply(lambda x: len(str(x)))\ntest['char_count'] = test['text'].apply(lambda x: len(str(x)))\n\n# punctuation_count\ntrain['punctuation_count'] = train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ntest['punctuation_count'] = test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n# hashtag_count\ntrain['hashtag_count'] = train['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\ntest['hashtag_count'] = test['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n\n# mention_count\ntrain['mention_count'] = train['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\ntest['mention_count'] = test['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n\n#print the train and test data\ntrain.head()","bf8e257b":"METAFEATURES = ['word_count', 'unique_word_count', 'stop_word_count', 'mean_word_length',\n                'char_count', 'punctuation_count']\n\n\nfig, axes = plt.subplots(ncols=1, nrows=len(METAFEATURES), figsize=(20, 50), dpi=100)\n\nfor i,feat in enumerate(METAFEATURES):\n    #Distribution for the training set w.r.t target category\n    sns.distplot(train.loc[train['sentiment'] == 'positive'][feat], label=\"Positive\", ax=axes[i], color='green')\n    sns.distplot(train.loc[train['sentiment'] == 'neutral'][feat], label=\"Neutral\", ax=axes[i], color='blue')\n    sns.distplot(train.loc[train['sentiment'] == 'negative'][feat], label=\"Negative\", ax=axes[i], color='red')\n    \n    axes[i].set_xlabel('')\n    axes[i].tick_params(axis='x', labelsize=12)\n    axes[i].tick_params(axis='y', labelsize=12)\n    axes[i].legend()\n    \n    axes[i].set_title(f'{feat} Target Distribution in Training Set', fontsize=20)\n\nplt.show()\n    ","900b2d7f":"METAFEATURES = ['word_count', 'unique_word_count', 'stop_word_count', 'mean_word_length',\n                'char_count', 'punctuation_count']\n\nfig, axes = plt.subplots(ncols=1, nrows=len(METAFEATURES), figsize=(20, 50), dpi=100)\n\nfor i,feat in enumerate(METAFEATURES):\n    #Distribution of train and test data set\n    sns.distplot(train[feat], label='Training',ax=axes[i])\n    sns.distplot(test[feat], label='Test',ax=axes[i])\n    \n    axes[i].set_xlabel('')\n    axes[i].tick_params(axis='x', labelsize=12)\n    axes[i].tick_params(axis='y', labelsize=12)\n    axes[i].legend()\n    \n    axes[i].set_title(f'{feat} Training & Test set Distribution', fontsize=20)\n\nplt.show()","461222bb":"sentiment_count = train.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\nplt.figure(figsize=(12,6))\nplt.title(\"Count of Target category \")\nsns.countplot(x='sentiment',data=train)\n\nfig = px.pie(sentiment_count, values='text', names='sentiment', title='Distribution of Target category', color_discrete_sequence=px.colors.sequential.RdBu)\nfig.show()","b83786b8":"def plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), color = 'white',\n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color=color,\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=600, \n                    height=300,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(train.loc[train['sentiment'] == 'neutral', 'text'], title=\"Word Cloud of Neutral tweets on train data\",color = 'white')","3d02c4a5":"plot_wordcloud(train.loc[train['sentiment'] == 'positive', 'text'], title=\"Word Cloud of Positive tweets on train data\",color = 'white')","5fa66d98":"plot_wordcloud(train.loc[train['sentiment'] == 'negative', 'text'], title=\"Word Cloud of Negative tweets on train data\",color = 'white')","d92633b7":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","2351e5a0":"train['clean_text'] = train['text'].apply(lambda x:clean_text(x))\ntest['clean_text'] = test['text'].apply(lambda x:clean_text(x))","ebb85f71":"def get_top_n_remove_stop_words(corpus, n=None,ngram=1):\n    vec = CountVectorizer(ngram_range=(ngram,ngram), stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","a8868f41":"#Distribution of ngrams in train and test set\n#We are removing the stop words also\nfig, axes = plt.subplots(ncols=3, figsize=(10, 15), dpi=100)\nplt.tight_layout()\n\ncommon_words = get_top_n_remove_stop_words(train[train['sentiment'] == 'positive']['clean_text'], 20)\ndf2 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\nsns.barplot(y='ReviewText', x='count', ax=axes[0], color='green', data=df2)\n\ncommon_words = get_top_n_remove_stop_words(train[train['sentiment'] == 'neutral']['clean_text'], 20)\ndf1 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\nsns.barplot(y='ReviewText', x='count', ax=axes[1], color='blue', data=df1)\n\ncommon_words = get_top_n_remove_stop_words(train[train['sentiment'] == 'negative']['clean_text'], 20)\ndf = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\nsns.barplot(y='ReviewText', x='count', ax=axes[2], color='red', data=df)\n\nfor i in range(3):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=10)\n    axes[i].tick_params(axis='y', labelsize=10)\n\naxes[0].set_title(f'Top {20} positive words', fontsize=10)\naxes[1].set_title(f'Top {20} neutral words', fontsize=10)\naxes[2].set_title(f'Top {20} negative words', fontsize=10)","7a05c622":"#Distribution of ngrams in train and test set\n#We are removing the stop words also\nfig, axes = plt.subplots(ncols=3, figsize=(10, 15), dpi=100)\nplt.tight_layout()\n\ncommon_words = get_top_n_remove_stop_words(train[train['sentiment'] == 'positive']['clean_text'], 20, 2)\ndf2 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\nsns.barplot(y='ReviewText', x='count', ax=axes[0], color='green', data=df2)\n\ncommon_words = get_top_n_remove_stop_words(train[train['sentiment'] == 'neutral']['clean_text'], 20, 2)\ndf1 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\nsns.barplot(y='ReviewText', x='count', ax=axes[1], color='blue', data=df1)\n\ncommon_words = get_top_n_remove_stop_words(train[train['sentiment'] == 'negative']['clean_text'], 20, 2)\ndf = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\nsns.barplot(y='ReviewText', x='count', ax=axes[2], color='red', data=df)\n\nfor i in range(3):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=10)\n    axes[i].tick_params(axis='y', labelsize=10)\n\naxes[0].set_title(f'Top {20} positive words', fontsize=10)\naxes[1].set_title(f'Top {20} neutral words', fontsize=10)\naxes[2].set_title(f'Top {20} negative words', fontsize=10)","b20bfe72":"#Distribution of ngrams in train and test set\n#We are removing the stop words also\nfig, axes = plt.subplots(ncols=3, figsize=(10, 15), dpi=100)\nplt.tight_layout()\n\ncommon_words = get_top_n_remove_stop_words(train[train['sentiment'] == 'positive']['clean_text'], 20, 3)\ndf2 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\nsns.barplot(y='ReviewText', x='count', ax=axes[0], color='green', data=df2)\n\ncommon_words = get_top_n_remove_stop_words(train[train['sentiment'] == 'neutral']['clean_text'], 20, 3)\ndf1 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\nsns.barplot(y='ReviewText', x='count', ax=axes[1], color='blue', data=df1)\n\ncommon_words = get_top_n_remove_stop_words(train[train['sentiment'] == 'negative']['clean_text'], 20, 3)\ndf = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\nsns.barplot(y='ReviewText', x='count', ax=axes[2], color='red', data=df)\n\nfor i in range(3):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=10)\n    axes[i].tick_params(axis='y', labelsize=10)\n\naxes[0].set_title(f'Top {20} positive words', fontsize=10)\naxes[1].set_title(f'Top {20} neutral words', fontsize=10)\naxes[2].set_title(f'Top {20} negative words', fontsize=10)","df3c461c":"> # **5. Ngram and Cleaning of text**\n\nTo analyse the text column we will be extracting the N-Gram features.\n\nN-gram are used to describe the number of words used as observation points, e.g unigram means singly-worded , bigram means 2-worded phrase , and trigram means 3-word phrase","a93c42be":"All the metafeatures have very similar distributions in training and test set , so both of the set are taken from the same sample.<br\/>\n\nAll of the meta features have information about the target as well . But few features are bot so good like url_count, hastag_count and mention_count. Whereas other meta features looks very important.","6a13df50":"> # **1.Read Data**","90ea59f5":"> # **2. Meta Features**\n\nDistribution of meta features in train and test data can be useful to identify the sentiments of the tweets. \n\n1. word_count number of words in text\n2. unique_word_count number of unique words in text\n3. stop_word_count number of stop words in text\n4. url_count number of urls in text\n5. mean_word_length average character count in words\n6. char_count number of characters in text\n7. punctuation_count number of punctuations in text\n8. hashtag_count number of hashtags (#) in text\n9. mention_count number of mentions (@) in text\n","02a41ad3":"> # **4.Word cloud**","20852244":"> # **3. Target**\n\nClass distributions are **31.2%** for neutral , **40.4%** for positive and **28.3%** for negative . Classes are almost balanced. ","4d629242":"> # **0.Introduction and references**\n\nThis is my first kernel in kaggle, as now i got some free time to learn about NLP. If you have any idea that might imporve the kernel , please be sure to comment. And please upvote in case you like the kernel.\n\n**Problem Statement**\n\nWith all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.\n\n*\"My ridiculous dog is amazing.\"* [sentiment: positive]\n\n**Evaluation**\n\nJaccard Similarity intersection over union is defined as size of intersection divided by size of union of two sets. \nLet\u2019s take example of two sentences:\n\n**Sentence 1**: AI is our friend and it has been friendly<br\/>\n**Sentence 2**: AI and humans have always been friendly\n\nIn order to calculate similarity using Jaccard similarity, we will first perform **lemmatization** to reduce words to the same root word. In our case, \u201cfriend\u201d and \u201cfriendly\u201d will both become \u201cfriend\u201d, \u201chas\u201d and \u201chave\u201d will both become \u201chas\u201d. Drawing a Venn diagram of the two sentences we get:\n\n![Venn Diagram of the two sentences for jaccard similarity](https:\/\/miro.medium.com\/max\/926\/1*u2ZZPh5er5YbmOg7k-s0-A.png)\n\nFor the above two sentences, we get Jaccard similarity of **5\/(5+3+2) = 0.5** which is size of intersection of the set divided by total size of set.\n\nThe kernel includes codes and ideas from the below kernel.Please view these kernel also for more information.\n\n> [https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert](http:\/\/)<br\/>\n> [https:\/\/towardsdatascience.com\/overview-of-text-similarity-metrics-3397c4601f50](http:\/\/)<br\/>\n> [https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes](http:\/\/)<br\/>\n> [https:\/\/www.kaggle.com\/tanulsingh077\/twitter-sentiment-extaction-analysis-and-eda](http:\/\/)<br\/>\n> [https:\/\/towardsdatascience.com\/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a](http:\/\/)<br\/>\n","ca71ae46":"> # **5.3 Trigrams**","84eab8da":"> # **1.1 Missing Values**\n\nTraining data have missing values(nan) in selected_text and sentiment.<br\/>\nNow i am removing the row containg the missing value , as it is only one values.","09bbaa40":"# **5.2 Bigrams**\n\nMost common words in neutral are already present in the negative category also . It will be very hard to use some of those words in other contexts. Like \"just got\" , \"im going\" ."}}