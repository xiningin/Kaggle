{"cell_type":{"37f32392":"code","cc3fa1cc":"code","4186dcde":"code","ae04271c":"code","91611c92":"code","5f4bf10b":"code","d9c13395":"code","370b3501":"code","c59b3bfd":"code","fff5ffb5":"code","a4a00216":"code","638a004d":"code","ad782b6e":"code","bf00d5a8":"code","2b3820a3":"code","6dd5cfba":"code","9a241b19":"code","54ca9eb5":"markdown","9762440d":"markdown","df94e0e4":"markdown","43ba725c":"markdown","b0113c94":"markdown","3815c99f":"markdown","0972719a":"markdown","80b5aeae":"markdown"},"source":{"37f32392":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nprint(os.listdir(\".\/\"))\n# Any results you write to the current directory are saved as output.","cc3fa1cc":"import numpy as np\nimport pandas as pd\nimport os\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import preprocessing, model_selection, neighbors, svm\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer, MultiLabelBinarizer\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MaxAbsScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.impute import SimpleImputer\n\nfrom catboost import CatBoostRegressor, Pool\n\nfrom tqdm import tqdm\nimport json\nimport ast\n\nfrom datetime import datetime","4186dcde":"TRAIN_DATA_PATH = \"..\/input\/train.csv\"\nTEST_DATA_PATH = \"..\/input\/test.csv\"\nSUBMISSON_PATH = \"..\/input\/sample_submission.csv\"\nLABEL_COL_NAME = \"revenue\"","ae04271c":"def date(x):\n    x=str(x)\n    year=x.split('\/')[2]\n    if int(year)<19:\n        return x[:-2]+'20'+year\n    else:\n        return x[:-2]+'19'+year\n\ndef isNaN(x):\n    return str(x) == str(1e400 * 0)\n\ndef getIsoListFormJson(data, isoKey='id', forceInt=False):\n    datas = data.values.flatten()\n    ids = []\n    for c in (datas):    \n        ccc = []\n        if isNaN(c) == False:\n            c = json.dumps(ast.literal_eval(c))        \n            c = json.loads(c)            \n            for cc in c:\n                if forceInt:\n                    ccStr = int(cc[isoKey])\n                else:\n                    ccStr = str(cc[isoKey])\n                ccc.append(ccStr)\n        else:\n            if forceInt:\n                ccc.append(0)\n            else:\n                ccc.append('0')\n        ids.append(ccc)    \n    return np.array(ids)\n\ndef distributeIdsOverData(data, colName, isoKey='id', forceInt=True):\n    arr = getIsoListFormJson(data[colName], isoKey, forceInt)    \n\n    gsi = -1\n    for gs in tqdm(arr):\n        gsi += 1\n        gs.sort()\n        for g in gs:\n            gi = gs.index(g)\n            try:\n                data.loc[gsi, f\"{colName}_{gi}\"] = float(g)                \n            except :\n                data.loc[gsi, f\"{colName}_{gi}\"] = g                \n            \n    data.drop(colName, axis=1, inplace=True)\n    print(f\"{colName} distributed over data, cols: {len(data.columns)}\")\n\ndef imput_title(df):\n    for index, row in df.iterrows():\n        if row['title'] == \"none\":\n            df.at[index,'title'] = df.loc[index]['original_title']\n    return df    \n    \ndef prepareData(data):    \n    data = imput_title(data)\n\n    data[\"different_title\"] = data[\"original_title\"] != data[\"title\"]\n\n    data.drop(\"overview\", axis=1, inplace=True)\n    data.drop(\"poster_path\", axis=1, inplace=True)\n    data.drop('imdb_id', axis=1, inplace=True)    \n\n    data[\"belongs_to_collection\"] = getIsoListFormJson(data[\"belongs_to_collection\"])\n\n    cast = data['cast'].fillna('none')\n    cast = cast.apply(lambda x: {} if x == 'none' else ast.literal_eval(x))\n    data['num_cast'] = cast.apply(lambda x: len(x) if x != {} else 0)\n    # Get the sum of each of the cast genders in a film: 0 `unknown`, 1 `female`, 2 `male`\n    data['genders_0_cast'] = cast.apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\n    data['genders_1_cast'] = cast.apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\n    data['genders_2_cast'] = cast.apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\n    distributeIdsOverData(data,'cast','cast_id')\n\n    crew = data['crew'].fillna('none')\n    crew = crew.apply(lambda x: {} if x == 'none' else ast.literal_eval(x))    \n    data['num_crew'] = crew.apply(lambda x: len(x) if x != {} else 0)    \n    # Get the sum of each of the cast genders in a film: 0 `unknown`, 1 `female`, 2 `male`\n    data['genders_0_crew'] = crew.apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\n    data['genders_1_crew'] = crew.apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\n    data['genders_2_crew'] = crew.apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\n    distributeIdsOverData(data,'crew','name',False) \n\n    distributeIdsOverData(data,'genres')\n    \n    keywords = data['Keywords'].fillna('none')\n    keywords = keywords.apply(lambda x: {} if x == 'none' else ast.literal_eval(x))\n    data['num_keywords'] = keywords.apply(lambda x: len(x) if x != {} else 0)\n    distributeIdsOverData(data,'Keywords')\n\n    data[\"Has_HomePage\"] = list(map(lambda c: float(c is not np.nan), data[\"homepage\"]))\n    data.drop('homepage', axis=1, inplace=True)\n\n    data[\"IsReleased\"] = list(map(lambda c: float(c == \"Released\"), data[\"status\"]))\n    data.drop(\"status\", axis=1, inplace=True)\n  \n    data[\"original_title_len\"] = list(map(lambda c: float(len(str(c))), data[\"original_title\"]))\n    data.drop(\"original_title\", axis=1, inplace=True)\n    \n    production_companies = data['production_companies'].fillna('none')\n    production_companies = production_companies.apply(lambda x: {} if x == 'none' else ast.literal_eval(x))\n    data['num_production_companies'] = production_companies.apply(lambda x: len(x) if x != {} else 0)\n    distributeIdsOverData(data,'production_companies')    \n\n    production_countries = data['production_countries'].fillna('none')\n    production_countries = production_countries.apply(lambda x: {} if x == 'none' else ast.literal_eval(x))\n    data['num_production_countries'] = production_countries.apply(lambda x: len(x) if x != {} else 0)    \n    distributeIdsOverData(data,'production_countries','iso_3166_1',False)\n\n    data['release_date']=data['release_date'].fillna('1\/1\/90').apply(lambda x: date(x))\n    data['release_date']=data['release_date'].apply(lambda x: datetime.strptime(x,'%m\/%d\/%Y'))\n    data['release_day']=data['release_date'].apply(lambda x:x.weekday())\n    data['release_month']=data['release_date'].apply(lambda x:x.month)\n    data['release_year']=data['release_date'].apply(lambda x:x.year)\n    data.drop('release_date', axis=1, inplace=True)\n    \n    spoken_languages = data['spoken_languages'].fillna('none')\n    spoken_languages = spoken_languages.apply(lambda x: {} if x == 'none' else ast.literal_eval(x))\n    data['num_spoken_languages'] = spoken_languages.apply(lambda x: len(x) if x != {} else 0)\n    distributeIdsOverData(data,'spoken_languages','iso_639_1',False)\n\n    data[\"tagline_len\"] = list(map(lambda c: float(len(str(c))), data[\"tagline\"]))\n    data.drop(\"tagline\", axis=1, inplace=True)\n\n    data[\"title_len\"] = list(map(lambda c: float(len(str(c))), data[\"title\"]))\n    data.drop(\"title\", axis=1, inplace=True)    \n\n    data.fillna(0, inplace=True)\n    data[\"budget\"] = np.log1p(SimpleImputer(missing_values=0, strategy=\"median\", verbose=1).fit_transform(data[\"budget\"].values.reshape(-1,1)))\n    #data[\"budget\"] = np.log1p(data[\"budget\"])\n\n    data[LABEL_COL_NAME] = np.log1p(data[LABEL_COL_NAME])","91611c92":"train = pd.read_csv(TRAIN_DATA_PATH, index_col='id')\nprint(\"Train Data Loaded\")\ntest = pd.read_csv(TEST_DATA_PATH, index_col = 'id')\nprint(\"Test Data Loaded\")","5f4bf10b":"if not os.path.exists(\"all_data.pickle\"):   \n    ##FILLING MISSIN BUDGET DATA\n    train.loc[16,'revenue'] = 192864          # Skinning\n    train.loc[90,'budget'] = 30000000         # Sommersby          \n    train.loc[118,'budget'] = 60000000        # Wild Hogs\n    train.loc[149,'budget'] = 18000000        # Beethoven\n    train.loc[313,'revenue'] = 12000000       # The Cookout \n    train.loc[451,'revenue'] = 12000000       # Chasing Liberty\n    train.loc[464,'budget'] = 20000000        # Parenthood\n    train.loc[470,'budget'] = 13000000        # The Karate Kid, Part II\n    train.loc[513,'budget'] = 930000          # From Prada to Nada\n    train.loc[797,'budget'] = 8000000         # Welcome to Dongmakgol\n    train.loc[819,'budget'] = 90000000        # Alvin and the Chipmunks: The Road Chip\n    train.loc[850,'budget'] = 90000000        # Modern Times\n    train.loc[1112,'budget'] = 7500000        # An Officer and a Gentleman\n    train.loc[1131,'budget'] = 4300000        # Smokey and the Bandit   \n    train.loc[1359,'budget'] = 10000000       # Stir Crazy \n    train.loc[1542,'budget'] = 1              # All at Once\n    train.loc[1542,'budget'] = 15800000       # Crocodile Dundee II\n    train.loc[1571,'budget'] = 4000000        # Lady and the Tramp\n    train.loc[1714,'budget'] = 46000000       # The Recruit\n    train.loc[1721,'budget'] = 17500000       # Cocoon\n    train.loc[1865,'revenue'] = 25000000      # Scooby-Doo 2: Monsters Unleashed\n    train.loc[2268,'budget'] = 17500000       # Madea Goes to Jail budget\n    train.loc[2491,'revenue'] = 6800000       # Never Talk to Strangers\n    train.loc[2602,'budget'] = 31000000       # Mr. Holland's Opus\n    train.loc[2612,'budget'] = 15000000       # Field of Dreams\n    train.loc[2696,'budget'] = 10000000       # Nurse 3-D\n    train.loc[2801,'budget'] = 10000000       # Fracture\n\n    test.loc[3889,'budget'] = 15000000       # Colossal\n    test.loc[6733,'budget'] = 5000000        # The Big Sick\n    test.loc[3197,'budget'] = 8000000        # High-Rise\n    test.loc[6683,'budget'] = 50000000       # The Pink Panther 2\n    test.loc[5704,'budget'] = 4300000        # French Connection II\n    test.loc[6109,'budget'] = 281756         # Dogtooth\n    test.loc[7242,'budget'] = 10000000       # Addams Family Values\n    test.loc[7021,'budget'] = 17540562       #  Two Is a Family\n    test.loc[5591,'budget'] = 4000000        # The Orphanage\n    test.loc[4282,'budget'] = 20000000       # Big Top Pee-wee\n\n    train.loc[391,'runtime'] = 86 #Il peor natagle de la meva vida\n    train.loc[592,'runtime'] = 90 #\u0410 \u043f\u043e\u0443\u0442\u0440\u0443 \u043e\u043d\u0438 \u043f\u0440\u043e\u0441\u043d\u0443\u043b\u0438\u0441\u044c\n    train.loc[925,'runtime'] = 95 #\u00bfQui\u00e9n mat\u00f3 a Bambi?\n    train.loc[978,'runtime'] = 93 #La peggior settimana della mia vita\n    train.loc[1256,'runtime'] = 92 #Cipolla Colt\n    train.loc[1542,'runtime'] = 93 #\u0412\u0441\u0435 \u0438 \u0441\u0440\u0430\u0437\u0443\n    train.loc[1875,'runtime'] = 86 #Vermist\n    train.loc[2151,'runtime'] = 108 #Mechenosets\n    train.loc[2499,'runtime'] = 108 #Na Igre 2. Novyy Uroven\n    train.loc[2646,'runtime'] = 98 #\u540c\u684c\u7684\u59b3\n    train.loc[2786,'runtime'] = 111 #Revelation\n    train.loc[2866,'runtime'] = 96 #Tutto tutto niente niente\n    \n    test.loc[4074,'runtime'] = 103 #Shikshanachya Aaicha Gho\n    test.loc[4222,'runtime'] = 93 #Street Knight\n    test.loc[4431,'runtime'] = 100 #\u041f\u043b\u044e\u0441 \u043e\u0434\u0438\u043d\n    test.loc[5520,'runtime'] = 86 #Glukhar v kino\n    test.loc[5845,'runtime'] = 83 #Frau M\u00fcller muss weg!\n    test.loc[5849,'runtime'] = 140 #Shabd\n    test.loc[6210,'runtime'] = 104 #Le dernier souffle\n    test.loc[6804,'runtime'] = 145 #Chaahat Ek Nasha..\n    test.loc[7321,'runtime'] = 87 #El truco del manco\n\n    all_data = train.append(test)\n    print(\"Preparing All Data\")\n    prepareData(all_data)    \n    all_data.to_pickle(\"all_data.pickle\")\n    print(\"saved all data\")\nelse: \n    all_data = pd.read_pickle(\"all_data.pickle\")\n    print(\"saved all data\")","d9c13395":"train = all_data[:len(train)]\ntest = all_data[len(train):]","370b3501":"train.head()","c59b3bfd":"train.describe()","fff5ffb5":"test.head()","a4a00216":"test.describe()","638a004d":"X_tr = train.drop(LABEL_COL_NAME, axis = 1)\ny_tr = train[LABEL_COL_NAME]\nnumerical_features = ['budget',\n                      'popularity', \n                      'runtime', \n                      'title_len', \n                      'original_title_len', \n                      'tagline_len',\n                      'num_crew',\n                      'num_cast',\n                      'num_keywords',\n                      'num_production_companies',\n                      'num_production_countries',\n                      'num_spoken_languages']\ncat_features = set(X_tr.columns) - set(numerical_features)\ncat_features = [list(X_tr.columns).index(c) for c in cat_features]","ad782b6e":"#import required packages\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\nimport gc\nfrom hyperopt import hp, tpe, Trials, STATUS_OK\nfrom hyperopt.fmin import fmin\nfrom hyperopt.pyll.stochastic import sample\n#optional but advised\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#GLOBAL HYPEROPT PARAMETERS\nNUM_EVALS = 100 #number of hyperopt evaluation rounds\nN_FOLDS = 3 #number of cross-validation folds on data in each evaluation round\n\n#LIGHTGBM PARAMETERS\nLGBM_MAX_LEAVES = 2**11 #maximum number of leaves per tree for LightGBM\nLGBM_MAX_DEPTH = 25 #maximum tree depth for LightGBM\nEVAL_METRIC_LGBM_REG = 'mae' #LightGBM regression metric. Note that 'rmse' is more commonly used \nEVAL_METRIC_LGBM_CLASS = 'auc'#LightGBM classification metric\n\n#XGBOOST PARAMETERS\nXGB_MAX_LEAVES = 2**12 #maximum number of leaves when using histogram splitting\nXGB_MAX_DEPTH = 25 #maximum tree depth for XGBoost\nEVAL_METRIC_XGB_REG = 'mae' #XGBoost regression metric\nEVAL_METRIC_XGB_CLASS = 'auc' #XGBoost classification metric\n\n#CATBOOST PARAMETERS\nCB_MAX_DEPTH = 6 #maximum tree depth in CatBoost\nOBJECTIVE_CB_REG = 'RMSE' #CatBoost regression metric\nOBJECTIVE_CB_CLASS = 'Logloss' #CatBoost classification metric\n\ndef quick_hyperopt(data, labels, package='lgbm', num_evals=NUM_EVALS, diagnostic=False, cat_features=[]):\n    \n    #==========\n    #LightGBM\n    #==========\n    \n    if package=='lgbm':\n        \n        print('Running {} rounds of LightGBM parameter optimisation:'.format(num_evals))\n        #clear space\n        gc.collect()\n        \n        integer_params = ['max_depth',\n                         'num_leaves',\n                          'max_bin',\n                         'min_data_in_leaf',\n                         'min_data_in_bin']\n        \n        def objective(space_params):\n            \n            #cast integer params from float to int\n            for param in integer_params:\n                space_params[param] = int(space_params[param])\n            \n            #extract nested conditional parameters\n            if space_params['boosting']['boosting'] == 'goss':\n                top_rate = space_params['boosting'].get('top_rate')\n                other_rate = space_params['boosting'].get('other_rate')\n                #0 <= top_rate + other_rate <= 1\n                top_rate = max(top_rate, 0)\n                top_rate = min(top_rate, 0.5)\n                other_rate = max(other_rate, 0)\n                other_rate = min(other_rate, 0.5)\n                space_params['top_rate'] = top_rate\n                space_params['other_rate'] = other_rate\n            \n            subsample = space_params['boosting'].get('subsample', 1.0)\n            space_params['boosting'] = space_params['boosting']['boosting']\n            space_params['subsample'] = subsample\n            \n            #for classification, set stratified=True and metrics=EVAL_METRIC_LGBM_CLASS\n            cv_results = lgb.cv(space_params, train, nfold = N_FOLDS, stratified=False,\n                                early_stopping_rounds=100, metrics=EVAL_METRIC_LGBM_REG, seed=42)\n            \n            best_loss = cv_results['l1-mean'][-1] #'l2-mean' for rmse\n            #for classification, comment out the line above and uncomment the line below:\n            #best_loss = 1 - cv_results['auc-mean'][-1]\n            #if necessary, replace 'auc-mean' with '[your-preferred-metric]-mean'\n            return{'loss':best_loss, 'status': STATUS_OK }\n        \n        train = lgb.Dataset(data, labels)\n                \n        #integer and string parameters, used with hp.choice()\n        boosting_list = [{'boosting': 'gbdt',\n                          'subsample': hp.uniform('subsample', 0.5, 1)},\n                         {'boosting': 'goss',\n                          'subsample': 1.0,\n                         'top_rate': hp.uniform('top_rate', 0, 0.5),\n                         'other_rate': hp.uniform('other_rate', 0, 0.5)}] #if including 'dart', make sure to set 'n_estimators'\n        metric_list = ['MAE', 'RMSE'] \n        #for classification comment out the line above and uncomment the line below\n        #modify as required for other classification metrics classification\n        #metric_list = ['auc']\n        objective_list_reg = ['huber', 'gamma', 'fair', 'tweedie']\n        objective_list_class = ['logloss', 'cross_entropy']\n        #for classification set objective_list = objective_list_class\n        objective_list = objective_list_reg\n\n        space ={'boosting' : hp.choice('boosting', boosting_list),\n                'num_leaves' : hp.quniform('num_leaves', 2, LGBM_MAX_LEAVES, 1),\n                'max_depth': hp.quniform('max_depth', 2, LGBM_MAX_DEPTH, 1),\n                'max_bin': hp.quniform('max_bin', 32, 255, 1),\n                'min_data_in_leaf': hp.quniform('min_data_in_leaf', 1, 256, 1),\n                'min_data_in_bin': hp.quniform('min_data_in_bin', 1, 256, 1),\n                'lambda_l1' : hp.uniform('lambda_l1', 0, 5),\n                'lambda_l2' : hp.uniform('lambda_l2', 0, 5),\n                'learning_rate' : hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n                'metric' : hp.choice('metric', metric_list),\n                'objective' : hp.choice('objective', objective_list),\n                'feature_fraction' : hp.quniform('feature_fraction', 0.5, 1, 0.01),\n                'bagging_fraction' : hp.quniform('bagging_fraction', 0.5, 1, 0.01)\n            }\n        \n        #optional: activate GPU for LightGBM\n        #follow compilation steps here:\n        #https:\/\/www.kaggle.com\/vinhnguyen\/gpu-acceleration-for-lightgbm\/\n        #then uncomment lines below:\n        #space['device'] = 'gpu'\n        #space['gpu_platform_id'] = 0,\n        #space['gpu_device_id'] =  0\n\n        trials = Trials()\n        best = fmin(fn=objective,\n                    space=space,\n                    algo=tpe.suggest,\n                    max_evals=num_evals, \n                    trials=trials)\n                \n        #fmin() will return the index of values chosen from the lists\/arrays in 'space'\n        #to obtain actual values, index values are used to subset the original lists\/arrays\n        best['boosting'] = boosting_list[best['boosting']]['boosting']#nested dict, index twice\n        best['metric'] = metric_list[best['metric']]\n        best['objective'] = objective_list[best['objective']]\n        \n        #cast floats of integer params to int\n        for param in integer_params:\n            best[param] = int(best[param])\n            \n        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n        if diagnostic:\n            return(best, trials)\n        else:\n            return(best)\n    \n    #==========\n    #XGBoost\n    #==========\n    \n    if package=='xgb':\n        \n        print('Running {} rounds of XGBoost parameter optimisation:'.format(num_evals))\n        #clear space\n        gc.collect()\n        \n        integer_params = ['max_depth']\n        \n        def objective(space_params):\n            \n            for param in integer_params:\n                space_params[param] = int(space_params[param])\n                \n            #extract multiple nested tree_method conditional parameters\n            #libera te tutemet ex inferis\n            if space_params['tree_method']['tree_method'] == 'hist':\n                max_bin = space_params['tree_method'].get('max_bin')\n                space_params['max_bin'] = int(max_bin)\n                if space_params['tree_method']['grow_policy']['grow_policy']['grow_policy'] == 'depthwise':\n                    grow_policy = space_params['tree_method'].get('grow_policy').get('grow_policy').get('grow_policy')\n                    space_params['grow_policy'] = grow_policy\n                    space_params['tree_method'] = 'hist'\n                else:\n                    max_leaves = space_params['tree_method']['grow_policy']['grow_policy'].get('max_leaves')\n                    space_params['grow_policy'] = 'lossguide'\n                    space_params['max_leaves'] = int(max_leaves)\n                    space_params['tree_method'] = 'hist'\n            else:\n                space_params['tree_method'] = space_params['tree_method'].get('tree_method')\n                \n            #for classification replace EVAL_METRIC_XGB_REG with EVAL_METRIC_XGB_CLASS\n            cv_results = xgb.cv(space_params, train, nfold=N_FOLDS, metrics=[EVAL_METRIC_XGB_REG],\n                             early_stopping_rounds=100, stratified=False, seed=42)\n            \n            best_loss = cv_results['test-mae-mean'].iloc[-1] #or 'test-rmse-mean' if using RMSE\n            #for classification, comment out the line above and uncomment the line below:\n            #best_loss = 1 - cv_results['test-auc-mean'].iloc[-1]\n            #if necessary, replace 'test-auc-mean' with 'test-[your-preferred-metric]-mean'\n            return{'loss':best_loss, 'status': STATUS_OK }\n        \n        train = xgb.DMatrix(data, labels)\n        \n        #integer and string parameters, used with hp.choice()\n        boosting_list = ['gbtree', 'gblinear'] #if including 'dart', make sure to set 'n_estimators'\n        metric_list = ['MAE', 'RMSE'] \n        #for classification comment out the line above and uncomment the line below\n        #metric_list = ['auc']\n        #modify as required for other classification metrics classification\n        \n        tree_method = [{'tree_method' : 'exact'},\n               {'tree_method' : 'approx'},\n               {'tree_method' : 'hist',\n                'max_bin': hp.quniform('max_bin', 2**3, 2**7, 1),\n                'grow_policy' : {'grow_policy': {'grow_policy':'depthwise'},\n                                'grow_policy' : {'grow_policy':'lossguide',\n                                                  'max_leaves': hp.quniform('max_leaves', 32, XGB_MAX_LEAVES, 1)}}}]\n        \n        #if using GPU, replace 'exact' with 'gpu_exact' and 'hist' with\n        #'gpu_hist' in the nested dictionary above\n        \n        objective_list_reg = ['reg:linear', 'reg:gamma', 'reg:tweedie']\n        objective_list_class = ['reg:logistic', 'binary:logistic']\n        #for classification change line below to 'objective_list = objective_list_class'\n        objective_list = objective_list_reg\n        \n        space ={'boosting' : hp.choice('boosting', boosting_list),\n                'tree_method' : hp.choice('tree_method', tree_method),\n                'max_depth': hp.quniform('max_depth', 2, XGB_MAX_DEPTH, 1),\n                'reg_alpha' : hp.uniform('reg_alpha', 0, 5),\n                'reg_lambda' : hp.uniform('reg_lambda', 0, 5),\n                'min_child_weight' : hp.uniform('min_child_weight', 0, 5),\n                'gamma' : hp.uniform('gamma', 0, 5),\n                'learning_rate' : hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n                'eval_metric' : hp.choice('eval_metric', metric_list),\n                'objective' : hp.choice('objective', objective_list),\n                'colsample_bytree' : hp.quniform('colsample_bytree', 0.1, 1, 0.01),\n                'colsample_bynode' : hp.quniform('colsample_bynode', 0.1, 1, 0.01),\n                'colsample_bylevel' : hp.quniform('colsample_bylevel', 0.1, 1, 0.01),\n                'subsample' : hp.quniform('subsample', 0.5, 1, 0.05),\n                'nthread' : -1\n            }\n        \n        #optional: activate GPU for XGBoost\n        #uncomment line below\n        #space['tree_method'] = 'gpu_hist'\n        \n        trials = Trials()\n        best = fmin(fn=objective,\n                    space=space,\n                    algo=tpe.suggest,\n                    max_evals=num_evals, \n                    trials=trials)\n        \n        best['tree_method'] = tree_method[best['tree_method']]['tree_method']\n        best['boosting'] = boosting_list[best['boosting']]\n        best['eval_metric'] = metric_list[best['eval_metric']]\n        best['objective'] = objective_list[best['objective']]\n        \n        #cast floats of integer params to int\n        for param in integer_params:\n            best[param] = int(best[param])\n        if 'max_leaves' in best:\n            best['max_leaves'] = int(best['max_leaves'])\n        if 'max_bin' in best:\n            best['max_bin'] = int(best['max_bin'])\n        \n        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n        \n        if diagnostic:\n            return(best, trials)\n        else:\n            return(best)\n    \n    #==========\n    #CatBoost\n    #==========\n    \n    if package=='cb':\n        \n        print('Running {} rounds of CatBoost parameter optimisation:'.format(num_evals))\n        \n        #clear memory \n        gc.collect()\n            \n        integer_params = ['depth',\n                          'one_hot_max_size', #for categorical data\n                          'min_data_in_leaf',\n                          'max_bin']\n        \n        def objective(space_params):\n                        \n            #cast integer params from float to int\n            for param in integer_params:\n                space_params[param] = int(space_params[param])\n                \n            #extract nested conditional parameters\n            if space_params['bootstrap_type']['bootstrap_type'] == 'Bayesian':\n                bagging_temp = space_params['bootstrap_type'].get('bagging_temperature')\n                space_params['bagging_temperature'] = bagging_temp\n                \n            if space_params['grow_policy']['grow_policy'] == 'LossGuide':\n                max_leaves = space_params['grow_policy'].get('max_leaves')\n                space_params['max_leaves'] = int(max_leaves)\n                \n            space_params['bootstrap_type'] = space_params['bootstrap_type']['bootstrap_type']\n            space_params['grow_policy'] = space_params['grow_policy']['grow_policy']\n                           \n            #random_strength cannot be < 0\n            space_params['random_strength'] = max(space_params['random_strength'], 0)\n            #fold_len_multiplier cannot be < 1\n            space_params['fold_len_multiplier'] = max(space_params['fold_len_multiplier'], 1)\n                       \n            #for classification set stratified=True\n            cv_results = cb.cv(train, space_params, fold_count=N_FOLDS, \n                             early_stopping_rounds=25, stratified=False, partition_random_seed=42)\n           \n            #best_loss = cv_results['test-MAE-mean'].iloc[-1] \n            best_loss = cv_results['test-RMSE-mean'].iloc[-1] \n            \n            #for classification, comment out the line above and uncomment the line below:\n            #best_loss = cv_results['test-Logloss-mean'].iloc[-1]\n            #if necessary, replace 'test-Logloss-mean' with 'test-[your-preferred-metric]-mean'\n            \n            return{'loss':best_loss, 'status': STATUS_OK}\n        \n        train = cb.Pool(data, labels.astype('float32'), cat_features=cat_features)\n        \n        #integer and string parameters, used with hp.choice()\n        bootstrap_type = [\n                          {'bootstrap_type':'Poisson'}, \n                          {'bootstrap_type':'Bayesian', 'bagging_temperature' : hp.loguniform('bagging_temperature', np.log(1), np.log(50))},\n                          {'bootstrap_type':'Bernoulli'}] \n        LEB = ['No', 'AnyImprovement', 'Armijo'] #remove 'Armijo' if not using GPU\n        #score_function = ['Correlation', 'L2', 'NewtonCorrelation', 'NewtonL2']\n        grow_policy = [{'grow_policy':'SymmetricTree'},\n                       {'grow_policy':'Depthwise'},\n                       {'grow_policy':'Lossguide',\n                        'max_leaves': hp.quniform('max_leaves', 2, 32, 1)}]\n        eval_metric_list_reg = ['MAE', 'RMSE', 'Poisson']\n        eval_metric_list_class = ['Logloss', 'AUC', 'F1']\n        #for classification change line below to 'eval_metric_list = eval_metric_list_class'\n        eval_metric_list = eval_metric_list_reg\n                \n        space ={'depth': hp.quniform('depth', 2, CB_MAX_DEPTH, 1),\n                'max_bin' : hp.quniform('max_bin', 1, 32, 1), #if using CPU just set this to 254\n                #'max_bin': 254,\n                'l2_leaf_reg' : hp.uniform('l2_leaf_reg', 0, 5),\n                'min_data_in_leaf' : hp.quniform('min_data_in_leaf', 1, 50, 1),\n                'random_strength' : hp.loguniform('random_strength', np.log(0.005), np.log(5)),\n                'one_hot_max_size' : hp.quniform('one_hot_max_size', 2, 16, 1), #uncomment if using categorical features\n                'bootstrap_type' : hp.choice('bootstrap_type', bootstrap_type),\n                'learning_rate' : hp.uniform('learning_rate', 0.05, 0.25),\n                'eval_metric' : hp.choice('eval_metric', eval_metric_list),\n                'objective' : OBJECTIVE_CB_REG,\n                #'score_function' : hp.choice('score_function', score_function), #crashes kernel - reason unknown\n                'leaf_estimation_backtracking' : hp.choice('leaf_estimation_backtracking', LEB),\n                'grow_policy': hp.choice('grow_policy', grow_policy),\n                #'colsample_bylevel' : hp.quniform('colsample_bylevel', 0.1, 1, 0.01),# CPU only\n                'fold_len_multiplier' : hp.loguniform('fold_len_multiplier', np.log(1.01), np.log(2.5)),\n                'od_type' : 'Iter',\n                'od_wait' : 25,\n                'task_type' : 'GPU',\n                'verbose' : 0,\n                'cat_features': cat_features\n            }\n        \n        #optional: run CatBoost without GPU\n        #uncomment line below\n        #space['task_type'] = 'CPU'\n            \n        trials = Trials()\n        best = fmin(fn=objective,\n                    space=space,\n                    algo=tpe.suggest,\n                    max_evals=num_evals, \n                    trials=trials)\n        \n        #unpack nested dicts first\n        best['bootstrap_type'] = bootstrap_type[best['bootstrap_type']]['bootstrap_type']\n        best['grow_policy'] = grow_policy[best['grow_policy']]['grow_policy']\n        best['eval_metric'] = eval_metric_list[best['eval_metric']]\n        \n        #best['score_function'] = score_function[best['score_function']] \n        #best['leaf_estimation_method'] = LEM[best['leaf_estimation_method']] #CPU only\n        best['leaf_estimation_backtracking'] = LEB[best['leaf_estimation_backtracking']]        \n        \n        #cast floats of integer params to int\n        for param in integer_params:\n            best[param] = int(best[param])\n        if 'max_leaves' in best:\n            best['max_leaves'] = int(best['max_leaves'])\n        \n        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n        \n        if diagnostic:\n            return(best, trials)\n        else:\n            return(best)\n    \n    else:\n        print('Package not recognised. Please use \"lgbm\" for LightGBM, \"xgb\" for XGBoost or \"cb\" for CatBoost.')     ","bf00d5a8":"cb_params = quick_hyperopt(X_tr, y_tr, 'cb', 15, cat_features=cat_features)\nnp.save('cb_params.npy', cb_params)\nprint(cb_params)","2b3820a3":"try:\n    model = CatBoostRegressor(**cb_params, task_type='GPU')\n    model.fit(X_tr, y_tr, cat_features=cat_features)    \nexcept:\n    print(\"GPU grow_policy error, just remove it\")\n    cb_params.pop('grow_policy')\n    model = CatBoostRegressor(**cb_params, task_type='GPU')\n    model.fit(X_tr, y_tr, cat_features=cat_features)","6dd5cfba":"test = test.drop(LABEL_COL_NAME, axis = 1)\ny_test = np.expm1(model.predict(test))","9a241b19":"submission = pd.read_csv(SUBMISSON_PATH, index_col='id')\nsubmission[LABEL_COL_NAME] = y_test[:-1]\nsubmission.to_csv(f'submission.csv')\nprint(submission)","54ca9eb5":"There are some missing values in the test and train data. If we know them from imdb then we could fill them manually.","9762440d":"<h3>Imports and Setup<\/h3>","df94e0e4":"<h1>TMDB Features for Catboost and Catboost Optimization<\/h1>\n<h2>What is Catboost<\/h2>\n\n<p>A fast, scalable, high performance Gradient Boosting on Decision Trees library, used for ranking, classification, regression and other machine learning tasks for Python, R, Java, C++. Supports computation on CPU and GPU.<\/p>\n\n<p>Provided by Yandex and basicly it is the russian tensorflow and focused on Gradient Boosting insteed of neural network. Also has a lot of GPU features.<\/p>\n","43ba725c":"<h3>Loading the test and train data<\/h3>","b0113c94":"Now we have our train and test data, we should define the categorical data for catboost. Because catboost could also handle string categorical data if defined. No need to make some label encoding for catboost.","3815c99f":"<h3>Cross Validator Functions and Hyper Parameter Optimizations.<\/h3>","0972719a":"<h3>Paths and Definitions<\/h3>","80b5aeae":"<h3>Functions and Feature Generation<\/h3>\n<p>Here we are pasing json data and distribute over pandas dataframe. Like Crew, Cast, Genre, Production Company etc... . We are defining every unique value as column in our dataframe. If the movie has that value row value of that column will be 1.<\/p> \n<p>Also by the way we try to find use full features too. They could be important somehow.<\/p>\n<ul>\n    <li>Is the title is different of the original title.<\/li>\n    <li>Count of casts<\/li>\n    <li>Count of crews<\/li>\n    <li>Count of casts gender<\/li>\n    <li>Count of crews gender<\/li>\n    <li>Has a home page or not<\/li>\n    <li>Is released or not<\/li>\n    <li>Count of keywords<\/li>\n    <li>Count of production companies and countries<\/li>\n    <li>Release Day, Month and Year as seperate features.<\/li>\n    <li>title and original title length<\/li>\n<\/ul>\n\n<p>Budget and Revenues are so big skewed values. Not good for machine learning. We are using log of them.<\/p>\n<p>More over we are imputing the budget with median strategy. It may increase the score.<\/p>"}}