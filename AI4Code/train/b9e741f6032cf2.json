{"cell_type":{"5b0f4306":"code","b0fab358":"code","c7dac3d8":"code","c350015f":"code","c1145614":"markdown"},"source":{"5b0f4306":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b0fab358":"train1 = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\ntest1 = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\nbook_train = pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet')\nbook_test = pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/book_test.parquet')\ntrade_train = pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/trade_train.parquet')\ntrade_test = pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/trade_test.parquet')\nsubmission = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/sample_submission.csv')","c7dac3d8":"train1.head()","c350015f":"# LSTM for international airline passengers problem with regression framing\nimport numpy \n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport math\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n# convert an array of values into a dataset matrix\ndef create_dataset(dataset, look_back=1):\n dataX, dataY = [], []\n for i in range(len(dataset)-look_back-1):\n  a = dataset[i:(i+look_back), 0]\n  dataX.append(a)\n  dataY.append(dataset[i + look_back, 0])\n  return numpy.array(dataX), numpy.array(dataY)\n# fix random seed for reproducibility\nnumpy.random.seed(7)\n# load the dataset\ndataset = train1['target']\ndataset1= test1['row_id']\nenc = OneHotEncoder(handle_unknown='ignore')\ndataset1 = test1.replace('-','.')\nprint(\"replaced - successfully\")\n#dataset1 = dataset1.astype(float)\n#dataset1 = numpy.reshape(dataset1,1,[dataset1.shape[0],dataset1.shape[1]])\ndataset1 = enc.fit_transform(dataset1).toarray()\n#dataset = dataset.astype('float32')\n#dataset1[\"row_id\"] = dataset1['row_id'].astype(float)\n#dataset1 = dataset1.astype('float32')\n# normalize the dataset\nscaler = MinMaxScaler(feature_range=(0, 1))\ndataset = numpy.array(dataset)\nprint(\" array created \")\ndataset = dataset.reshape(-1,1)\ndataset = scaler.fit_transform(dataset)\ndataset1 = scaler.fit_transform(dataset1)\n#dataset1 = dataset.reshape(-1,1)\n#dataset1 = scaler.fit_transform(dataset1)\n# split into train and test sets\n#train_size = int(len(dataset) * 0.67)\n#test_size = len(dataset) - train_size\ntrain, test = dataset, dataset1\n# reshape into X=t and Y=t+1\nlook_back = 1\ntrainX, trainY = create_dataset(train, look_back)\ntestX, testY = create_dataset(test, look_back)\n# reshape input to be [samples, timesteps , features]\ntrainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n# create and fit the LSTM network\nmodel = Sequential()\nmodel.add(LSTM(4, input_shape=(1, look_back)))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(trainX, trainY, epochs=10, batch_size=1, verbose=2)\n# make predictions\ntrainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\n# invert predictions[2D input array]\n#trainPredict = scaler.inverse_transform(trainPredict)\n#trainY = scaler.inverse_transform([trainY])\n#testPredict = scaler.inverse_transform(testPredict)\n#testY = scaler.inverse_transform([testY])\n# calculate root mean squared error\n'''trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))\n# shift train predictions for plotting\ntrainPredictPlot = numpy.empty_like(dataset)\ntrainPredictPlot[:, :] = numpy.nan\ntrainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n# shift test predictions for plotting\ntestPredictPlot = numpy.empty_like(dataset)\ntestPredictPlot[:, :] = numpy.nan\ntestPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict '''\nimport csv\nOutput = {\"row_id\" : test1['row_id'],\"target\" : testPredict}\nwith open('submission.csv', 'w') as f:\n    for key in Output.keys():\n        f.write(\"%s,%s\\n\"%(key,Output[key]))\n\n\nprint(\"Output saved successfully!!!\")","c1145614":"train.head()"}}