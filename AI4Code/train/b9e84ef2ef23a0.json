{"cell_type":{"1a5604cc":"code","34188f42":"code","d45c48d0":"code","d00baa7b":"code","27be7c3d":"code","1bfc5727":"code","c0c49e30":"code","3f5b6c97":"code","0ba82f52":"code","3bcd0d28":"code","fbc07d41":"code","18dc17b3":"code","e6900ad3":"code","ad633f28":"code","b4102357":"code","56a4480f":"code","1f85e6e5":"code","e6ee8a21":"code","72b69ded":"code","f7e7bcef":"code","cfcea952":"code","ab9f9a3e":"code","c8b36899":"code","325fc2d3":"code","37f78f4e":"markdown","e10a757b":"markdown","dcbabc28":"markdown","b1941ce1":"markdown","ba26e544":"markdown","6eaa6aec":"markdown","a26387ca":"markdown","a5503e99":"markdown","acbea218":"markdown","c3775ae1":"markdown","6e8bbd90":"markdown","ab4bc7b5":"markdown","311b3999":"markdown","13faf8d6":"markdown","49f0ec84":"markdown","b3f9106d":"markdown","f134451b":"markdown","8d5c37e3":"markdown","f0e9661a":"markdown","06056ee9":"markdown"},"source":{"1a5604cc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport warnings\nwarnings.filterwarnings('ignore')\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","34188f42":"# Load data set\nx_1=np.load('\/kaggle\/input\/X.npy')\ny_1=np.load('\/kaggle\/input\/Y.npy')\nimg_size=64\nplt.subplot(1,2,1)\nplt.imshow(x_1[260].reshape(img_size,img_size))\nplt.axis('off')\nplt.subplot(1,2,2)\nplt.imshow(x_1[900].reshape(img_size,img_size))\nplt.axis('off')","d45c48d0":"# Join a seuence of arrays along an row ais\nX=np.concatenate((x_1[204:409],x_1[822:1027]),axis=0) # from 0 to 204 is zero sign and from 205 to 410 is one sign\nz=np.zeros(205)\no=np.ones(205)\nY=np.concatenate((z,o),axis=0).reshape(X.shape[0],1)\nprint(\"X shape:\",X.shape)\nprint(\"Y Shape:\",Y.shape)","d00baa7b":"# Then lets create x_test,x_train,y_test and y_train arrays\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.15,random_state=42)\nnumber_of_train=x_train.shape[0]\nnumber_of_test=x_test.shape[0]\n","27be7c3d":"x_train_flatten=x_train.reshape(number_of_train,x_train.shape[1]*x_train.shape[2]) #convert to 2D\nx_test_flatten=x_test.reshape(number_of_test,x_test.shape[1]*x_test.shape[2])      #convert to 2D\nprint(\"x train flatten\",x_train_flatten.shape)\nprint(\"x test flatten\",x_test_flatten.shape)","1bfc5727":"x_train=x_train_flatten.T  \nx_test=x_test_flatten.T\ny_train=y_train.T\ny_test=y_test.T\nprint(\"x train:\",x_train.shape)\nprint(\"x test:\",x_test.shape)\nprint(\"y train:\",y_train.shape)\nprint(\"y test:\",y_test.shape)","c0c49e30":"# so what we need is dimension 4096 that is number of pizels as a parameter for our initialize me thod(def)\ndef initialize_weights_and_bias(dimension):\n    w=np.full((dimension,1),0.01)\n    b=0.0\n    return w,b\nw,b = initialize_weights_and_bias(4096)\nprint(b)\nprint(w)","3f5b6c97":"# calculation z\n#z =np.dot(w.T,x_train)+b\ndef sigmoid(z):\n    y_head=1\/(1+np.exp(-z))\n    return y_head\n","0ba82f52":"y_head=sigmoid(10)\nprint(y_head)","3bcd0d28":"# Forward propagation steps:\n#find z=w.T*x+b\n# y_head=sigmoid(z)\n#loss(error)=loss(y,y_head)\n#cost =sum(loss)\ndef forward_propagation(w,b,x_train,y_train):\n    z=np.dot(w.T,x_train)+b\n    y_head=sigmoid(z)   # probabilistic value 0-1\n    loss=-y_train*np.log(y_head)-(1-y_train)\/np.log(1-y_head)\n    cost=(np.sum(loss))\/x_train.shape[1]  #x_train.shape[1] is for scaling\n    return cost","fbc07d41":"def backward_propagation(w,b,x_train,y_train):\n    derivative_weight=(np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1]\n    derivative_bias=np.sum(y_head-y_train)\/x_train.shape[1]\n    gradients={\"derivative_weight\":derivative_weight,\"derivative_bias\":derivative_bias}\n    return gradients","18dc17b3":"def update (w,b,x_train,y_train,learning_rate,number_of_iteration):\n    cost_list=[]\n    cost_list2=[]\n    index=[]\n    # updating parameters is number_of_iteration\n    for i in range(number_of_iteration):\n        # make forward and backward propagation and find cost and gradients\n        cost = forward_propagation(w,b,x_train,y_train)\n        gradients=backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w=w-learning_rate*gradients[\"derivative_weight\"]\n        b=b-learning_rate*gradients[\"derivative_bias\"]\n        if i % 10==0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"cost after iteraiton %i: %f\" %(i,cost))\n        # we update parameters weights and bias\n    parameters={\"weight\":w,\"bias\":b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients ,cost_list","e6900ad3":"def predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z=sigmoid(np.dot(w.T,x_test)+b)\n    y_prediction=np.zeros((1,x_test.shape[1]))\n    #if z is bigger than 0.5, our prediction is sign one(y_head=1),\n    #if z is smalller than 0.5, our prediction is sign zero (y_head=0).\n    for i in range (z.shape[1]):\n        if z[0,i]<=0.5:\n            y_prediction[0,i]=0\n        else:\n            y_prediction[0,i]=1\n    return y_prediction\n# predict(parameters[\"weight\"],parameters[\"bias\"],x_test)","ad633f28":"def logistig_regression (x_train,y_train,x_test,y_test, learning_rate,num_iterations):\n    #initialize\n    dimension=x_train.shape[0] # that is 4096\n    w,b=initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters,gradients,cost_list=update(w,b,x_train,y_train,learning_rate,num_iterations)\n    \n    y_prediction_test=predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train=predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n    \n    # Print train\/test Errors\n\n    print(\"train accuracy:{} % \".format(100-np.mean(np.abs(y_prediction_train-y_train))*100))\n    print(\"test accuracy:{} % \".format(100-np.mean(np.abs(y_prediction_test-y_test))*100))\n    \nlogistig_regression(x_train,y_train,x_test,y_test,learning_rate=0.0001,num_iterations=50)\n\n    ","b4102357":"from sklearn import linear_model\nlogreg=linear_model.LogisticRegression(random_state=42,max_iter=50)\nprint(\"test accuracy:{}\".format(logreg.fit(x_train.T,y_train.T).score(x_test.T,y_test.T)))\nprint(\"train accuracy:{}\".format(logreg.fit(x_train.T,y_train.T).score(x_train.T,y_train.T)))","56a4480f":"#Initialize parameters and layer size\ndef initialize_parameters_and_layer_size_NN(x_train,y_train):\n    parameters ={\"weight1\":np.random.randn(3,x_train.shape[0])*0.1,\n                 \"bias1\":np.zeros((3,1)),\n                 \"weight2\":np.random.randn(y_train.shape[0],3)*0.1,\n                 \"bias2\":np.zeros((y_train.shape[0],1)),\n        \n    }\n    return parameters","1f85e6e5":"def forward_propagation_NN(x_train,parameters):\n    z1=np.dot(parameters[\"weight1\"],x_train)+parameters[\"bias1\"]\n    a1=np.tanh(z1)\n    z2=np.dot(parameters[\"weight2\"],a1)+parameters[\"bias2\"]\n    a2=sigmoid(z2)  # a2 mean y_head is probabilistic value\n    cache={\"z1\":z1,\n          \"a1\":a1,\n          \"z2\":z2,\n          \"a2\":a2}\n    return a2, cache","e6ee8a21":"# Compute cost\ndef compute_cost_NN(a2,y,parameters):\n    logprobs=np.multiply(np.log(a2),Y)\n    cost=-np.sum(logprobs)\/y.shape[1]\n    return cost","72b69ded":"def backward_propagation_NN(parameters,cache,x,y):\n    dz2=cache[\"a2\"]-y\n    dw2=np.dot(dz2,cache[\"a1\"].T)\/x.shape[1]\n    db2=np.sum(dz2,axis=1,keepdims=True)\/x.shape[1]\n    dz1=np.dot(parameters[\"weight2\"].T,dz2)*(1-np.power(cache[\"a1\"],2))\n    dw1=np.dot(dz1,x.T)\/x.shape[1]\n    db1=np.sum(dz1,axis=1,keepdims=True)\/x.shape[1]\n    grads={\"dweight1\":dw1,\n           \"dbias1\":db1,\n           \"dweight2\":dw2,\n           \"dbias2\":db2}\n    return grads \n    \n    \n    \n    \n    \n    \n    \n    \n    ","f7e7bcef":"def update_parameters_NN(parameters,grads,learning_rate=0.01):\n    parameters = {\"weight1\":parameters[\"weight1\"]-learning_rate*grads[\"dweight2\"],\n                 \"bias1\":parameters[\"bias1\"]-learning_rate*grads[\"dbias1\"],\n                 \"weight2\":parameters[\"weight2\"]-learning_rate*grads[\"dweight2\"],\n                 \"bias2\":parameters[\"bias2\"]-learning_rate*grads[\"dbias2\"]}\n    return parameters","cfcea952":"# prediction\ndef predict_NN(parameters,x_test):\n    # x_test is a input for forward propagation\n    a2, cache=forward_propagation_NN(x_test,parameters)\n    y_prediction=np.zeros((1,x_test.shape[1]))\n    #if z is bigger than 0,5 our prediction is sign one (y_head=1)\n    # if z is smaller than 0.5 our prediction is sign zero(y_head=0)\n    for i in range(a2.shape[1]):\n        if a2[0,i]<=0.5:\n            y_prediction[0,i]=0\n        else:\n            y_prediction[0,i]=1\n    return y_prediction","ab9f9a3e":"# 2 - Layer neurak network\ndef two_layer_neural_network(x_train,y_train,x_test,y_test,num_iterations):\n    cost_list=[]\n    index_list=[]\n    # intialize paremeters and layer sizes\n    parameters=initialize_parameters_and_layer_size_NN(x_train,y_train)\n    for i in range(0,num_iterations):\n        # forward propagation\n        a2, cache = forward_propagation_NN(x_train,parameters)\n        # compute cost\n        cost=compute_cost_NN(a2,y_train,parameters)\n        #backward propagation\n        grads=backward_propagation_NN(parameters,cache,x_train,y_train)\n        #update parameters\n        parameters=update_parameters_NN(parameters,grads)\n        if i % 100==0:\n            cost_list.append(cost)\n            index_list.append(i)\n            print(\"cost after iteration %i: %f\" %(i,cost))\n    plt.plot(index_list,cost_list)\n    plt.xticks(index_list,rotation='vertical')\n    plt.xlabel(\"Number of iteration\")\n    plt.ylabel(\"cost\")\n    plt.show()\n    # predict\n    y_prediction_test=predict_NN(parameters,x_test)\n    y_prediction_train=predict_NN(parameters,x_train)\n    # print test\/train errors\n    print(\"train accuracy: {} %\".format(100-np.mean(np.abs(y_prediction_train-y_train))*100))\n    print(\"test accuracy: {} %\".format(100-np.mean(np.abs(y_prediction_test-y_test))*100))\n    return parameters\nparamaters=two_layer_neural_network(x_train,y_train,x_test,y_test,num_iterations=50)","c8b36899":"#reshaping \nx_train,x_test,y_train,y_test=x_train.T,x_test.T,y_train.T,y_test.T","325fc2d3":"# Evaluating the ANN\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # intialize neural network library\nfrom keras.layers import Dense # build our layers library\ndef build_classifier():\n    classifier=Sequential() # initialize neural network\n    classifier.add(Dense(units=8,kernel_initializer='uniform',activation='relu',input_dim=x_train.shape[1]))\n    classifier.add(Dense(units=4,kernel_initializer='uniform',activation='relu'))\n    classifier.add(Dense(units=1,kernel_initializer='uniform',activation='sigmoid'))\n    classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n    return classifier\nclassifier=KerasClassifier(build_fn=build_classifier,epochs=100)\naccuracies=cross_val_score(estimator=classifier,X=x_train,y=y_train,cv=3)\nmean=accuracies.mean()\nvariance=accuracies.std()\nprint(\"accuracy mean:\"+ str(mean))\nprint(\"accuracy variance:\"+ str(variance))\n   ","37f78f4e":"###  Backward Propagation","e10a757b":"### L-Layer Neural Network","dcbabc28":"### Logistic Regression with Sklearn","b1941ce1":"### Updating(learning) parameters","ba26e544":"# Logistic Regression","6eaa6aec":"###  Backward Propagation","a26387ca":"* Up to this point we learn\n * Initializing parameters (implemented)\n * Finding cost with forward propagation and cost function(implemented)\n * Updating(learning) parameters (weights and bias). Now lets implement it.","a5503e99":"### Loss and Cost fuction","acbea218":"### Update Parameters","c3775ae1":"### Create ANN Model","6e8bbd90":"### Initializing Parameters","ab4bc7b5":"# Artificial Neural Network**","311b3999":"### Implementing with keras library","13faf8d6":"### Prediction","49f0ec84":"###  2-Layer Neural Network","b3f9106d":"### Forward Propagation","f134451b":"### Prediction with learn parameters weight and bias","8d5c37e3":"# Overwiev the Data Set","f0e9661a":"### Forward Propagation","06056ee9":"What we did up to this point:\n* Choose our labels(classes) that are sign zeros and sign one\n* Create and flatten train and test sets\n* Our final inputs(images) and outputs(labels or classes) created"}}