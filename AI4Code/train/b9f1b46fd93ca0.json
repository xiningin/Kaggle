{"cell_type":{"4ea7932f":"code","b13274a8":"code","c50d651a":"code","a5dbfbb1":"code","dccf59b7":"code","1663e8ee":"code","c1488a80":"code","30d5089e":"code","91c01ec4":"code","f44a2e8a":"code","6d3561ae":"code","4e64fb5c":"code","c4041dfb":"code","6958d668":"code","2b8f0f15":"code","2374714b":"code","3b53954f":"code","a2d44c63":"markdown","51bb5b0b":"markdown","4be0035c":"markdown","90ac8edd":"markdown","738a92ce":"markdown","b838231b":"markdown","eba1e822":"markdown","afaa543a":"markdown","0a8c30ad":"markdown","9339af2e":"markdown","ce24c006":"markdown","3957b66d":"markdown","4c3b6600":"markdown"},"source":{"4ea7932f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\ntrain_df = pd.read_csv('..\/input\/train.csv')\n\ny_train = train_df.label.values\nX_train = train_df.drop(columns=[\"label\"]).values\nX_train = X_train \/ 255.0\n\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.4, random_state=42)\nprint(X_train.shape, X_test.shape)\nprint(train_df.shape)","b13274a8":"import tensorflow as tf\nprint(tf.__version__)\n\nfrom sklearn.metrics import accuracy_score","c50d651a":"def incomplete_model(X, y, X_test, epochs=1):\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Flatten(input_shape=(784,)),\n        #tf.keras.layers.Dense(256, activation=tf.nn.relu),\n        #tf.keras.layers.Dense(128, activation=tf.nn.relu),\n        tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n    ])\n    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n    model.fit(X, y, epochs=epochs, verbose=0)\n    predictions = model.predict(X_test)\n    return np.argmax(predictions, axis=1)\n\nclassifications = incomplete_model(X_train, y_train, X_test, 2) ","a5dbfbb1":"binary_classifications = np.absolute(y_test - classifications)\nbinary_classifications = np.clip(binary_classifications, 0, 1)\naccuracy = 1 - (np.sum(binary_classifications) \/ y_test.shape[0])\nprint(accuracy)\n","dccf59b7":"NUM_TO_ANALYSE = 9\nSAMPLE_COUNT = 3000\n\nsample_indices = np.random.randint(low=0, high=y_test.shape[0], size=SAMPLE_COUNT)\n\nactual_sample = y_test[sample_indices]\npredicted_sample = classifications[sample_indices]","1663e8ee":"def confusion_matrix(predicted, actual, klass=NUM_TO_ANALYSE):\n    \n    actual_indices = np.where(actual == klass)[0]   \n    predicted_indices = np.where(predicted == klass)[0]    \n    not_in_predicted_indices = np.where(predicted != klass)[0]    \n    not_in_actual_indices = np.where(actual != klass)[0]\n    \n    # True Positives: TPs are count of rightly predicted \n    true_positive_indices = np.where(actual[predicted_indices] == klass)[0]\n    TRUE_POSITIVES = len(true_positive_indices)\n    \n    # False Positives: Failed to predict correctly\n    false_positive_indices = np.where(actual[predicted_indices] != klass)[0]\n    FALSE_POSITIVES = len(false_positive_indices)\n    \n    # True Negatives: Predicted as not part of the class and they are true in the actuals\n    true_negative_indices = predicted[not_in_actual_indices]\n    TRUE_NEGATIVES = len(np.where(true_negative_indices != klass)[0])\n    \n    # False Negatives: False negatives are not predicted as the class of interest but they are actually belongs to the class\n    false_negative_indices = actual[not_in_predicted_indices]\n    FALSE_NEGATIVES = len(np.where(false_negative_indices == klass)[0])\n    \n    return {'TP': TRUE_POSITIVES, 'FP': FALSE_POSITIVES, 'TN': TRUE_NEGATIVES, 'FN': FALSE_NEGATIVES}\n\n\nmetrices = confusion_matrix(predicted_sample, actual_sample, NUM_TO_ANALYSE)\n\nprint(metrices['TP'], metrices['FP'], metrices['TN'], metrices['FN'])\nprint(sum([metrices['TP'], metrices['FP'], metrices['TN'], metrices['FN']]))","c1488a80":"class Metrics:\n    metrics = None\n    precision = None\n    recall = None\n    f1_score = None\n    tpr = None\n    fpr = None\n    \n    def __init__(self, metrics):\n        self.metrics = metrics\n        \n    def calculate(self):\n        self.precision = self.metrics['TP'] \/ (self.metrics['TP'] + self.metrics['FP'])\n        self.recall = self.metrics['TP'] \/ (self.metrics['TP'] + self.metrics['FN'])\n        self.f1_score = 2 * self.precision * self.recall \/ (self.precision + self.recall)\n        self.tpr = self.metrics['TP'] \/ (self.metrics['TP'] + self.metrics['FN'])\n        self.fpr = self.metrics['FP'] \/ (self.metrics['FP'] + self.metrics['TN'])\n        \nmetrics = Metrics(metrices)\nmetrics.calculate()\nprint(metrics.precision)\n","30d5089e":"def precision(metrics):\n    return metrics['TP'] \/ (metrics['TP'] + metrics['FP'])\n\nmetrices['precision'] = precision(metrices)\nmetrices['precision']","91c01ec4":"def recall(metrices):\n    return metrices['TP'] \/ (metrices['TP'] + metrices['FN'])\n\nmetrices['recall'] = recall(metrices)\nmetrices['recall']","f44a2e8a":"def f1_score(metrices):\n    return 2 * metrices['precision'] * metrices['recall'] \/ (metrices['precision'] + metrices['recall'])\n\nmetrices['f1_score'] = f1_score(metrices)\nmetrices['f1_score']","6d3561ae":"def tpr(metrics):\n    return metrics['TP'] \/ (metrics['TP'] + metrics['FN'])\n\nmetrices['TPR'] = tpr(metrices)\nmetrices['TPR']","4e64fb5c":"def fpr(metrics):\n    return metrics['FP'] \/ (metrics['FP'] + metrics['TN'])\n\nmetrices['FPR'] = fpr(metrices)\nmetrices['FPR']","c4041dfb":"def run(iterations=5):    \n    outcomes = {'precision': [], 'recall': [], 'f1_score':[], 'tpr': [], 'fpr': []}\n    \n        \n    for index in np.arange(1,iterations):\n        print(\"Iternation {0}\".format(index))\n        classification = incomplete_model(X_train, y_train, X_test, index)\n        sample_indices = np.random.randint(low=0, high=y_test.shape[0], size=index * 50)\n\n        actual_sample = y_test[sample_indices]\n        predicted_sample = classifications[sample_indices]\n\n        metrices = confusion_matrix(predicted_sample, actual_sample)\n        metrics = Metrics(metrices)\n        metrics.calculate()\n        outcomes['precision'].append(metrics.precision)\n        outcomes['recall'].append(metrics.recall)\n        outcomes['f1_score'].append(metrics.f1_score)\n        outcomes['tpr'].append(metrics.tpr)\n        outcomes['fpr'].append(metrics.fpr)\n    return outcomes\n    \noutcomes = run(15)","6958d668":"df = pd.DataFrame(data=outcomes)\ndf","2b8f0f15":"import matplotlib.pyplot as plt\nfrom PIL import Image\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 9999\npd.options.display.float_format = '{:20, .2f}'.format","2374714b":"df = df.sort_values(by ='recall')\n\ntrace = go.Scatter(x = df.recall, y = df.precision)\ndata = [trace]\npy.iplot(data, filename='AUC')","3b53954f":"df = df.sort_values(by ='fpr')\n \ntrace = go.Scatter(x = df.fpr, y = df.tpr)\ndata = [trace]\npy.iplot(data, filename='ROC')","a2d44c63":"## True Positive Rate (TPR or Recall or Sensitivity)\nTo scribe in words, TPR is \"If it is TRUE, how often the model predicts TRUE\"\n\n$$\\frac{True \\ Positives}{True \\ Positives + False \\ Negatives}$$","51bb5b0b":"## Accuracy\nTo scribe in words, \"How good is the classification model?\"\n\n$$\\frac{True \\ Positives + True \\ Negatives}{Total \\ No. \\ of \\ Samples}$$","4be0035c":"## Objective\nThe objective of this kernel is to walk through the validation metrices to consider for an imbalanced classification problem.\nIntent is to go deeper into the following metrices,\n- **Accuracy**\n- **Precision** \n- **Recall** \n- **True Positive Rate**\n- **False Positive Rate**\n- **Receiver Operating Curve - ROC**\n- **Precision Recall Curve - PRC**\n- **F1 Score**\n- **Confusion Matrix**\n\n### Terminologies\nWe shall come across the following terminologies while getting deeper in to the above said metrices are calculated\n1. True Positives(**TP**), False Positives(**FP**), True Negatives(**TN**), False Negatives(**FN**)\n2. Binary Cross Entropy\n3. Sparse Categorical Cross Entropy\n\nI found it is hard to remember the definition of these terms and often bumped into confusion\n\n\n## Approach\nApproach is to introduce the above said concepts and terms by doing step by step on digit recognition problem. Since the digit recognizer has 10 classes that is 0-9 digits of numeric system, We shall simplify the classes into 2(Recognized, Not Recognized) eventually.\nTo achieve this, we shall split the dataset into train, validation and test sets. Here we shall calculate the metrices on the test set.\n**Since we are focusing on the concepts, let us ignore the actual test data because we are not aware what class they belong to.**","90ac8edd":"## False Positive Rate (FPR)\nTo scribe in words, FPR is \"When it is FALSE, how often the model predicts TRUE\"\n\n$$\\frac{False \\ Positives}{False \\ Positives + True \\ Negatives}$$","738a92ce":"## PR Curve (Work In Progress)\n- Precision - Recall curves gives more informative picture of an algorithm's performance\n- Visibly it looks like convex hull of the ROC curve's space\n\n$$Recall(FPR) \\ vs \\ Precision$$\n\n- In PR Space, the goal is to e in the upper right hand corner","b838231b":"## F1 Score\nF1 Score is the weighted average of the true postive rate(recall) and precision\n\n$$2 * \\frac{Precision \\times Recall}{Precision + Recall}$$","eba1e822":"Let us run the test multiple times and observe how the metrices we have learnt comes out for digit recognition dataset.","afaa543a":"## Precision\nTo scribe in words, \"When it predicts TRUE, how often the model is correct\"\n\n$$\\frac{True \\ Positives}{True \\ Positives + False \\ Positives}$$","0a8c30ad":"## Shallow Net\nLet us build an extremely shallow network with no hidden layers or convolution to just get some predictions on the dataset.","9339af2e":"## Recall(or Sensitivity\/TPR)\nTo scribe in words, TPR is \"If it is TRUE, how often the model predicts TRUE\"\n\n$$\\frac{True \\ Positives}{True \\ Positives + False \\ Negatives}$$","ce24c006":"## ROC Curve (Work In Progress)\nROC stands for **Receiver Operator Characteristics**, ROC curves are used to present results of binary decision problems. \n- ROC curves shows the number of correctly classified positive samples varies with the number of incorrectly classified negative examples. ie \n\n$$False \\ Positive \\ Rate(FPR) \\ vs \\ True \\ Positive \\ Rate(TPR)$$\n\n- ROC Curves can present an overly optimistic view of an algorithm's performance\n- In ROC space, the goal is to be in the upper left hand corner\n","3957b66d":"## Confusion Matrix\n\nA confusion matrix is used to descriribe the performance of a binary classification model. There are four basic terms to be pondered\n\n- **True Positives**: These are the samples predicted correctly\n- **True Negatives**: Predicted as **FALSE** but they are actually **FALSE**\n- **False Positives or Type 1 Error**: Predicted as **TRUE** but they are actually **FALSE**\n- **False Negatives or Type 2 Error**: Predicted as **FALSE** but they are actually **TRUE**\n\n|   \t|   Actual Positive\t|   Actual Negative\t| \n|---\t|---\t|---\t|\n|Predicted Positive\t|   True Positives\t|   False Positives\t|\n|Predicted Negative\t|   False Negatives\t|   True Negatives\t|","4c3b6600":"## Transform Multi-Class to Binary Classification\nFor clear illustration of the topics of interest, we shall transform this multi-class classification into binary classification problem. ie Our end goal of this section is to make digit recognizer into TRUE or FALSE problem.\n- Take a sample of 3000 observations from our test set as our universe\n- Consider predicting only the digit 9 from the universe.\n- Since 9 alone is our relevant data, a multi class problem simplified to a binary class one"}}