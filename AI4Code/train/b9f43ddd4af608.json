{"cell_type":{"6a581101":"code","cfb47c2a":"code","1e1d1a42":"code","5ffd93a3":"code","45a83c42":"code","b4f4ec3d":"code","372947f7":"code","3b6c457e":"code","cb754c95":"code","28556e44":"code","e1636c34":"code","a1e0a6dd":"code","b67d815e":"code","a2ca0d39":"code","d7b30c94":"code","a4d91278":"code","5e357a35":"code","a40bfe4a":"code","7dd27102":"code","243677c9":"code","7774e8c0":"code","689aa50d":"code","7cf0bc5d":"code","8518e4fa":"code","1f3679de":"code","4fe66336":"code","cc837fb4":"code","a5727438":"markdown","c23ec563":"markdown","213a0b17":"markdown","99c53f29":"markdown","f255e89b":"markdown","95c0d035":"markdown","3b0ad716":"markdown","09266ccd":"markdown","e5e21a69":"markdown","fe4f4c1d":"markdown","02e3abf4":"markdown","bd5c83c9":"markdown","a2ab6422":"markdown","e083452a":"markdown","7c294cd8":"markdown"},"source":{"6a581101":"import pandas as pd\nfrom scipy.io import arff\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nimport numpy as np\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score, roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nimport xgboost as xgb\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","cfb47c2a":"oasis_longitudinal = '\/kaggle\/input\/mri-and-alzheimers\/oasis_longitudinal.csv'\noasis_longitudinal = pd.read_csv (oasis_longitudinal)\noasis_longitudinal.head()","1e1d1a42":"oasis_longitudinal.info()","5ffd93a3":"oasis_cross_sectional = '\/kaggle\/input\/mri-and-alzheimers\/oasis_cross-sectional.csv'\noasis_cross_sectional = pd.read_csv (oasis_cross_sectional)\noasis_cross_sectional.rename(columns={'Educ': 'EDUC'}, inplace=True)\noasis_cross_sectional.head()","45a83c42":"oasis_cross_sectional.info()","b4f4ec3d":"oasis_cross_sectional['CDR'].isnull().sum()","372947f7":"oasis_cross_sectional.dropna(subset=['CDR'], inplace=True)","3b6c457e":"oasis_cross_sectional.info()","cb754c95":"oasis_longitudinal.drop(['Subject ID'], axis = 1, inplace = True, errors = 'ignore')\noasis_longitudinal.drop(['MRI ID'], axis = 1, inplace = True, errors = 'ignore')\noasis_longitudinal.drop(['Visit'], axis = 1, inplace = True, errors = 'ignore')\noasis_longitudinal.drop(['Group'], axis = 1, inplace = True, errors = 'ignore')\noasis_longitudinal.drop(['Hand'], axis = 1, inplace = True, errors = 'ignore')\noasis_longitudinal.drop(['MR Delay'], axis = 1, inplace = True, errors = 'ignore')\noasis_longitudinal.head()","28556e44":"oasis_cross_sectional.drop(['ID'], axis = 1, inplace = True, errors = 'ignore')\noasis_cross_sectional.drop(['Delay'], axis = 1, inplace = True, errors = 'ignore')\noasis_cross_sectional.drop(['Hand'], axis = 1, inplace = True, errors = 'ignore')\noasis_cross_sectional.head()","e1636c34":"frames = [oasis_longitudinal, oasis_cross_sectional]\ndataset_final = pd.concat(frames)\ndataset_final.head()","a1e0a6dd":"dataset_final.info()","b67d815e":"data_na = (dataset_final.isnull().sum() \/ len(dataset_final)) * 100\ndata_na = data_na.drop(data_na[data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Lost proportion (%)' :round(data_na,2)})\nmissing_data.head(20)","a2ca0d39":"from sklearn.impute  import SimpleImputer\n# We perform it with the most frequent value \nimputer = SimpleImputer ( missing_values = np.nan,strategy='most_frequent')\n\nimputer.fit(dataset_final[['SES']])\ndataset_final[['SES']] = imputer.fit_transform(dataset_final[['SES']])\n\n# We perform it with the median\nimputer = SimpleImputer ( missing_values = np.nan,strategy='median')\n\nimputer.fit(dataset_final[['MMSE']])\ndataset_final[['MMSE']] = imputer.fit_transform(dataset_final[['MMSE']])","d7b30c94":"# 1= M, 0 = F\ndataset_final['M\/F'] = dataset_final['M\/F'].replace(['M', 'F'], [1,0])  \ndataset_final.head(3)","a4d91278":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(dataset_final['CDR'].values)\nle.classes_","5e357a35":"dataset_final['CDR'] = le.transform(dataset_final['CDR'].values) ","a40bfe4a":"# The classes are heavily skewed we need to solve this issue later.\nprint('Label 0 :', round(dataset_final['CDR'].value_counts()[0]\/len(dataset_final) * 100,2), '% of the dataset')\nprint('Label 0.5 :', round(dataset_final['CDR'].value_counts()[1]\/len(dataset_final) * 100,2), '% of the dataset')\nprint('Label 1 :', round(dataset_final['CDR'].value_counts()[2]\/len(dataset_final) * 100,2), '% of the dataset')\nprint('Label 2 :', round(dataset_final['CDR'].value_counts()[3]\/len(dataset_final) * 100,2), '% of the dataset')","7dd27102":"dataset_final = dataset_final.drop(dataset_final[dataset_final['CDR']==3].index)\ndataset_final.info()","243677c9":"X = dataset_final.drop([\"CDR\"],axis=1)\ny = dataset_final[\"CDR\"].values # 0,0.5=1,1=2,2=3","7774e8c0":"# We divide our data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    random_state = 42)","689aa50d":"print(\"{0:0.2f}% Train\".format((len(X_train)\/len(dataset_final.index)) * 100))\nprint(\"{0:0.2f}% Test\".format((len(X_test)\/len(dataset_final.index)) * 100))","7cf0bc5d":"print(len(X_train))\nprint(len(X_test))","8518e4fa":"FOLDS =10\n\nparametros_gb = {\n    \"loss\":[\"deviance\"],\n    \"learning_rate\": [0.01, 0.025, 0.005,0.5, 0.075, 0.1, 0.15, 0.2,0.3,0.8,0.9],\n    \"min_samples_split\": [0.01, 0.025, 0.005,0.4,0.5, 0.075, 0.1, 0.15, 0.2,0.3,0.8,0.9],\n    \"min_samples_leaf\": [1,2,3,5,8,10,15,20,40,50,55,60,65,70,80,85,90,100],\n    \"max_depth\":[3,5,8,10,15,20,25,30,40,50],\n    \"max_features\":[\"log2\",\"sqrt\"],\n    \"criterion\": [\"friedman_mse\",  \"mae\"],\n    \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n    \"n_estimators\":range(1,100)\n    }\n\nmodel_gb= GradientBoostingClassifier()\n\n\ngb_random = RandomizedSearchCV(estimator = model_gb, param_distributions = parametros_gb, n_iter = 100, cv = FOLDS, \n                               verbose=2, random_state=42,n_jobs = -1, scoring='f1_macro')\ngb_random.fit(X_train, y_train)\n\ngb_random.best_params_","1f3679de":"model_gb = GradientBoostingClassifier(subsample = 1.0,n_estimators= 23,\n                 min_samples_split = 0.025,\n                 min_samples_leaf = 3,\n                 max_features = 'log2',\n                 max_depth =10,\n                 loss = 'deviance',\n                 learning_rate = 0.8,\n                 criterion= 'mae')\nmodel_gb.fit(X_train,y_train)\n\nPredicted_gb= model_gb.predict(X_test)\nPredicted_gb_tr= model_gb.predict(X)\n\n\nacc = accuracy_score(Predicted_gb, y_test)\nacc_tr = accuracy_score(Predicted_gb_tr, y)\n\ntest_score = cross_val_score(model_gb, X_train, y_train, cv=FOLDS, scoring='accuracy').mean()\ntest_f1 = cross_val_score(model_gb, X_train, y_train, cv=FOLDS, scoring='f1_macro').mean()\n","4fe66336":"print(\"Accuracy Test\",acc)\nprint(\"Accuracy Training\",acc_tr)\nprint(\"Accuracy Cross_validate\",test_score)\nprint(\"F1 Cross_validate\",test_f1)\nprint(\"F1 Macro:\",f1_score(y_test, Predicted_gb, average='macro'))\nprint(\"F1 Micro:\",f1_score(y_test, Predicted_gb, average='micro'))  \nprint(\"F1 Weighted:\",f1_score(y_test, Predicted_gb, average='weighted'))\n\n#print(f1_score(y, Predicted_gb_tr, average='macro'))\n#print(f1_score(y, Predicted_gb_tr, average='micro'))  \n#print(f1_score(y, Predicted_gb_tr, average='weighted'))\n\nprint(\"\\nMatrix of confusion\")\nPredicted_gb= model_gb.predict(X)\nconfusion_matrix(y, Predicted_gb)\n","cc837fb4":"from joblib import dump, load\ndump(model_gb, 'model_gb.joblib') ","a5727438":"# Save model","c23ec563":"### Target","213a0b17":"# Read Datasets","99c53f29":"## Remove unnecessary columns from the 2 datasets","f255e89b":"## Delete Data without CDR ","95c0d035":"### Remove label with label 2","3b0ad716":"### Hyperparameter Gradient Boosting Classifier","09266ccd":"# Statistics","e5e21a69":"### Imputation of missing values","fe4f4c1d":"### Model building","02e3abf4":"## Join the two datasets into one","bd5c83c9":"# Pre Processing","a2ab6422":"### Label encoder","e083452a":"### Review null data","7c294cd8":"# Models"}}