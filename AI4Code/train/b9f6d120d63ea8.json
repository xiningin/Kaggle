{"cell_type":{"7ed4164d":"code","653a5542":"code","b145c884":"code","cc3308d8":"code","35f33b29":"code","61f1a2f9":"code","f4e73c34":"code","207bb335":"code","f0481270":"code","ca1950fe":"code","32e0977a":"code","6507ec59":"code","1f22c04d":"code","beab1f66":"code","40020744":"code","f24bd460":"code","822eaf61":"code","13b4c16d":"code","db31406b":"code","1fedc1a0":"code","2aec050e":"code","a815cb40":"code","0177816e":"code","f882d0ca":"code","1e6952b2":"code","74dcb0c1":"code","d32890cb":"code","2d0ada30":"code","c1490523":"code","b0696ccc":"code","2bb9569b":"code","35b58475":"code","30d5c186":"code","8d335a3c":"code","0342889d":"code","97f3ef70":"code","d1a88dcd":"code","b04f7bcb":"code","ae50f43b":"code","6e820c57":"code","3e386e4a":"code","24f3db90":"code","a274073d":"code","c85e17e6":"code","742c0ac0":"code","0ab9e5ed":"code","a8e7ad65":"code","7b46f7b1":"code","1aff883d":"code","a18950a7":"code","9f2f832e":"code","aa1ae5bf":"code","1d7a4097":"code","592f6fe8":"code","43b703dc":"code","977df4b8":"code","0ba92424":"code","d28ac524":"code","79402463":"code","2f753315":"code","9b163d2f":"code","5e153441":"code","bb182beb":"code","c1d9bd38":"code","52c5964f":"code","f0852d16":"code","0dcc8eb7":"code","8915c0ca":"code","34198d5f":"code","bf568b53":"code","f8de8174":"code","31ff1700":"code","c824b16c":"code","5e9d2d5a":"code","3beadae8":"code","3f499d7c":"code","551b33bc":"code","e9343dfa":"code","51d5bc6c":"code","d8a9da82":"code","76759ff6":"code","b395b87e":"code","475dffcc":"code","ba61c9bc":"code","9752604a":"code","f539c9e4":"code","569592b1":"code","875ce100":"code","74277b75":"code","bd375736":"code","6042cf53":"code","691f93d5":"code","d55d372c":"code","248913f3":"code","2ca9922c":"code","dc73c354":"code","725d237c":"code","327716ee":"code","f240295a":"code","ee03f28d":"code","6f2468fd":"code","19d091c8":"code","2de992ba":"code","296e6c27":"code","52f33d2b":"code","8bf5369b":"code","b48c8cb2":"code","beffb5ae":"code","a55bfd2c":"code","0dd8aebf":"code","c56bd661":"code","e70ed635":"code","da0471b4":"code","8a9d70b3":"code","edeedf4b":"code","57ea3d02":"code","1780699f":"code","1e9bec5a":"code","4795d83c":"code","1ba35453":"code","b2ebe984":"code","9e1d4fad":"code","ee26741b":"code","4bf7413c":"code","da44ad0b":"code","326d28de":"code","9aa83395":"code","da86c74a":"code","3d3f47ba":"code","047b1258":"code","067214a4":"code","12bec952":"code","a913e6f7":"code","9a152a18":"code","288083b0":"code","a8a5e40d":"code","ad23beeb":"code","b676b613":"code","10d10ee8":"code","e39d1f2a":"code","aa83a6bb":"code","54392cba":"code","c08f2261":"code","3ffbabd5":"code","e4e9d789":"code","f028c7ac":"code","488af8c2":"code","904d50fe":"code","a7f19098":"code","e769fb2f":"code","fe688911":"code","7fd98724":"code","44ac2bcf":"code","103fd04c":"code","4726f8b2":"code","75be3791":"code","2325135b":"code","19b278c5":"code","41c3a3e4":"code","f0105d82":"code","1fb4c053":"code","b59355e2":"code","5cfdcb94":"code","ea3ff18e":"code","a8de8cc0":"code","fd81bd50":"code","4e245267":"code","bde31ee1":"code","cc9427e3":"code","b15730bd":"code","a71545bf":"code","6e5c0860":"code","6200861d":"code","c80e0b9b":"code","6185bdb1":"code","f8b82fa0":"code","5ad82289":"code","7e39d28a":"code","0db5adef":"code","aef1b003":"code","f3ed8c23":"code","071a1015":"code","64c24177":"code","3f42b2fa":"code","c262a3cd":"code","7e7cec95":"code","82293a3c":"code","dd7ad85a":"code","1eab79e2":"code","54e59e98":"code","df066bdc":"code","3d5c5afb":"code","22891013":"code","24b58967":"code","1b3b8486":"code","91283cea":"code","26efb0f3":"code","7d89e204":"code","58d4a7a7":"code","3575ae33":"code","4ded0b7b":"code","e8c436fa":"code","ca8667f6":"code","27210553":"code","db99facd":"code","3b677cc1":"code","1b195530":"code","f3225a32":"code","3d3c2bac":"code","f2befd33":"code","e6360c57":"code","4f8c4d10":"code","b3e63230":"code","a47692c1":"code","936aa4ff":"code","94e79091":"code","98f6dbb9":"code","eeb0d4ed":"code","bb8b6c8a":"code","88143f36":"code","a5bb9ffc":"code","b51f30c0":"code","fe4d02d6":"code","88c32856":"code","df142c93":"code","c7455273":"code","cf2b3a8f":"code","dbcb80a3":"code","367a1dba":"code","4f0ede99":"code","6f2f5356":"code","1aa06fc2":"code","584cb8f2":"code","78f1629a":"code","3ca3983e":"code","d9e22372":"code","dccc841e":"code","b68fb194":"code","8d25ced2":"code","2c90b9b8":"code","00d6e742":"code","bb1837fe":"code","c38d6fc8":"markdown","e62daa87":"markdown","d5dbbaa4":"markdown","4408339f":"markdown","bdb07da2":"markdown","1d9d3aa2":"markdown","703ff9dc":"markdown","1f65aacb":"markdown","6d55fb6e":"markdown","c9fa5777":"markdown","8aa0fef9":"markdown","bad8d984":"markdown","3164bce8":"markdown","54d9030e":"markdown","828d91c2":"markdown","5fa24206":"markdown","be7563cc":"markdown","b35be472":"markdown","24c21bc2":"markdown","7518f06b":"markdown","e5d29dc1":"markdown","6e1bd6ad":"markdown","0cf2a925":"markdown","cee014cc":"markdown","5f021748":"markdown","0905cbdd":"markdown","0d58eeca":"markdown","5f82afda":"markdown","f8248bd7":"markdown","18dc128e":"markdown","22cb8106":"markdown","c9e1fed0":"markdown","b14d60df":"markdown","8821c7d2":"markdown","93540e05":"markdown","ae081964":"markdown","8e147181":"markdown","756de167":"markdown","6d72f056":"markdown","a8163332":"markdown","323bf1c0":"markdown","61c9084a":"markdown","cb098c50":"markdown","7fb155e1":"markdown","ad36f675":"markdown","ebfeb1b2":"markdown","982d0610":"markdown","7b0587ac":"markdown","d00d9320":"markdown","f5c2d5d1":"markdown","b13b5264":"markdown","0395f496":"markdown","c9783e9f":"markdown","2b6478e9":"markdown","347a6c4f":"markdown","a8f04447":"markdown","84080731":"markdown","b8807511":"markdown","82362c92":"markdown","f144d2d2":"markdown","18dffd48":"markdown","d90119e5":"markdown","8b42c190":"markdown","739e02df":"markdown","13b8d49a":"markdown","c3aae519":"markdown","68d703c4":"markdown","4b3e00a9":"markdown","9f3d0be1":"markdown","7ef1505a":"markdown","09a4c07d":"markdown","bc1c4617":"markdown","8f0bc3c8":"markdown","b179c79b":"markdown","b135e38a":"markdown","f13532c6":"markdown","b6154670":"markdown","17d3e5ba":"markdown","91dceb0c":"markdown","d10022d5":"markdown","8d9dd1e5":"markdown","769c2fa5":"markdown","c8924575":"markdown","cd519ae6":"markdown","e08c0de3":"markdown","d72f00d9":"markdown","ab294579":"markdown","d0dca3a0":"markdown","62aaea63":"markdown","d4d3ed9b":"markdown","2f8e061e":"markdown","8517c419":"markdown","36c668cb":"markdown","cf567620":"markdown","5ce546da":"markdown","077941d1":"markdown","c2a27e56":"markdown","ed324b33":"markdown","acfdb834":"markdown","eaca356c":"markdown","4b6c10f9":"markdown","197718ac":"markdown","8437789a":"markdown","e7a094a6":"markdown","4de141e9":"markdown","87d54cdb":"markdown","47c413ab":"markdown","adbbc93c":"markdown","5279cb1a":"markdown"},"source":{"7ed4164d":"# creating a reuseable function that will help us in ploting our barplots for analysis\n\ndef BarPlot(df, Variable, plotSize):\n    fig, axs = plt.subplots(figsize = plotSize)\n    plt.xticks(rotation = 45)\n    ax = sns.countplot(x=Variable, data=df)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}'.format(height\/len(df) * 100),\n                ha=\"center\")","653a5542":"# creating a reuseable function that will help us in Bivariate for analysis\ndef CountPlot(df,Variable,title,plotsize,hue=None):\n    plt.figure(figsize=plotsize)\n    plt.xticks(rotation=90)\n    plt.title(title)\n    sns.countplot(data = df, x=Variable, order=df[Variable].value_counts().index,hue = hue)\n    plt.show()\n    \n    convertcount=df.pivot_table(values='Lead Number',index=Variable,columns='Converted', aggfunc='count').fillna(0)\n    convertcount[\"Conversion(%)\"] =round(convertcount[1]\/(convertcount[0]+convertcount[1]),2)*100\n    return print(convertcount.sort_values(ascending=False,by=\"Conversion(%)\"))\n","b145c884":"#Importing Libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport re\nfrom scipy import stats \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Importing RFE and LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nimport statsmodels.api as sm  \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_curve","cc3308d8":"# Reading File\ndf= pd.read_csv(\"..\/input\/lead-scoring-x-online-education\/Leads X Education.csv\")","35f33b29":"df.head(20)","61f1a2f9":"# Getting Original onversion rate for the data set\norgConversionRate = round(100*(sum(df['Converted'])\/len(df['Converted'].index)), 2)\nprint(\"The conversion rate of leads is: \",orgConversionRate)","f4e73c34":"# Checking shape of dataframe\ndf.shape","207bb335":"# Checking columns name\ndf.columns","f0481270":"# Checking columns type in dataframe\ndf.info()","ca1950fe":"# checking attributes for continuous variables\ndf.describe()","32e0977a":"# AS Value  select represent that User has not selecte any values for that, Hence it can be converted to Null\n# so that it can be treated as Null\ndf = df.replace('Select', np.nan)\ndf.head(20)","6507ec59":"# Checking if any duplicate value in Lead Number and Prospect ID\nprint(sum(df.duplicated(subset= 'Lead Number'))!=0)\nprint(sum(df.duplicated(subset= 'Prospect ID'))!=0)","1f22c04d":"# Checking Null Values\nprint(df.isnull().sum(axis=0))","beab1f66":"# Checking column-wise null percentages here\nprint(round(100*(df.isnull().sum()\/len(df)).sort_values(ascending= False), 2))","40020744":"# Droping columns having null percentage >50%\ndf = df.drop(df.columns[df.apply(lambda col: col.isnull().sum()\/len(df) > 0.70)], axis=1)\ndf","f24bd460":"## Checking number of unique values per column \ndf.nunique()","822eaf61":"df = df.drop(df.columns [df.apply(lambda col: col.nunique()==1)], axis=1)\ndf","13b4c16d":"### We can also drop the column 'Prospect ID' as we already have an identifying column with unique values: 'Lead Number'\ndf = df.drop('Prospect ID', axis=1)\ndf.head()","db31406b":"plt.figure(figsize=(20,10))\nplt.subplot(2,2,1)\nsns.countplot(df['Asymmetrique Activity Index'])\n\nplt.subplot(2,2,2)\nsns.boxplot(df['Asymmetrique Activity Score'])\n\nplt.subplot(2,2,3)\nsns.countplot(df['Asymmetrique Profile Index'])\n\nplt.subplot(2,2,4)\nsns.boxplot(df['Asymmetrique Profile Score'])","1fedc1a0":"colsToDrop = ['Asymmetrique Activity Index','Asymmetrique Activity Score','Asymmetrique Profile Index','Asymmetrique Profile Score']\ndf =df.drop(colsToDrop, axis=1)","2aec050e":"# Checking Lead Quality\ndf['Lead Quality'].value_counts()","a815cb40":"# Since 'Lead Quality' is based on an employees intuition, let us inpute any NAN values with 'Not Sure'\ndf['Lead Quality'] = df['Lead Quality'].fillna('Not Sure')\ndf['Lead Quality'].value_counts()","0177816e":"BarPlot(df, 'Lead Quality', (10,10))","f882d0ca":"df =df.drop('Lead Quality', axis=1)\ndf","1e6952b2":"df['City'].value_counts()","74dcb0c1":"df.City.describe()","d32890cb":"BarPlot(df, 'City', (10,10))","2d0ada30":"df.City= df.City.fillna('Mumbai')","c1490523":"df['City'].value_counts(normalize= True)","b0696ccc":"### Exploring 'Specialization' column which hs 36.58% null values\ndf.Specialization.describe()","2bb9569b":"BarPlot(df, 'Specialization', (10,10))","35b58475":"df.Specialization= df['Specialization'].fillna('Others')\ndf.Specialization.value_counts(normalize=True)","30d5c186":"df.Tags.describe()","8d335a3c":"BarPlot(df,'Tags', (10,10))","0342889d":"df = df.drop(['Tags'], axis = 1)","97f3ef70":"# Checking again the Null values\nprint(round(100*(df.isnull().sum(axis=0)\/len(df.index)).sort_values(ascending=False),2))","d1a88dcd":"df['What matters most to you in choosing a course'].value_counts()","b04f7bcb":"BarPlot(df, 'What matters most to you in choosing a course',(8,5))","ae50f43b":"# Dropping 'What matters most to you in choosing a course'\ndf= df.drop(['What matters most to you in choosing a course'], axis =1)","6e820c57":"df['What is your current occupation'].value_counts(normalize= True)","3e386e4a":"BarPlot(df, 'What is your current occupation',(10,10))","24f3db90":"df['What is your current occupation']=df['What is your current occupation'].fillna('Unemployed')","a274073d":"BarPlot(df, 'What is your current occupation',(10,10))","c85e17e6":"df['Country'].value_counts()","742c0ac0":"df['Country'].describe()","0ab9e5ed":"df['Country']= df['Country'].fillna('India')\ndf['Country'].value_counts()","a8e7ad65":"df=df.drop(['Country'], axis=1)","7b46f7b1":"df.head()","1aff883d":"# Checking Null Values Again\n\nprint(round(100*(df.isnull().sum()\/len(df)).sort_values(ascending= False), 2))","a18950a7":"### Imputing missing values in Lead Source\ndf['Lead Source'].value_counts()","9f2f832e":"### Imputing missing values with 'Google'\n\ndf['Lead Source'] = df['Lead Source'].fillna('Google')\ndf['Lead Source'].value_counts()","aa1ae5bf":"df['Lead Source'] =  df['Lead Source'].apply(lambda x: x.capitalize())\ndf['Lead Source'].value_counts()","1d7a4097":"df['Page Views Per Visit'].value_counts()","592f6fe8":"df['Page Views Per Visit'].describe()","43b703dc":"df['Page Views Per Visit'].median()","977df4b8":"#### Imputing the missing values with '2.0' which is the median value\ndf['Page Views Per Visit']= df['Page Views Per Visit'].fillna(2.0)","0ba92424":"df['Page Views Per Visit']","d28ac524":"df.TotalVisits.describe()","79402463":"### We will impute this value with the meadian value since the \n### mean and the median values are relatively close to each other\ndf.TotalVisits = df.TotalVisits.fillna(3.0)\ndf.TotalVisits.value_counts()","2f753315":"df['Last Activity'].value_counts()","9b163d2f":"#### Imputing the missing values with 'Email Opened'\ndf['Last Activity'] = df['Last Activity'].fillna('Email Opened')\ndf['Last Activity'].value_counts()","5e153441":"df.shape","bb182beb":"print(round(100*(df.isnull().sum()\/len(df)).sort_values(ascending= False), 2))","c1d9bd38":"# Getting shape of dataframe after cleanup\ndf.shape","52c5964f":"df.info()","f0852d16":"features = ['TotalVisits','Total Time Spent on Website','Page Views Per Visit']\n# Plotting Box plot for continuous columns\nplt.figure(figsize = (20,12))\nfor i in enumerate(features):\n    plt.subplot(2,2,i[0]+1)\n    sns.boxplot(df[i[1]])","0dcc8eb7":"### Caping data at the 1% & 95% mark so as to not lose any values or drop rows\nq1 = df['Page Views Per Visit'].quantile(0.01)\ndf['Page Views Per Visit'][df['Page Views Per Visit']<= q1] = q1\n\nq3 = df['Page Views Per Visit'].quantile(0.95)\ndf['Page Views Per Visit'][df['Page Views Per Visit']>= q3] = q3","8915c0ca":"### Caping data at the 1% & 95% mark so as to not lose any values or drop rows\nq1 = df['TotalVisits'].quantile(0.01)\ndf['TotalVisits'][df['TotalVisits']<= q1] = q1\n\nq3 = df['TotalVisits'].quantile(0.95)\ndf['TotalVisits'][df['TotalVisits']>= q3] = q3","34198d5f":"# Plotting Box plot for continuous columns to check after caping outliers\nplt.figure(figsize = (20,12))\nfor i in enumerate(features):\n    plt.subplot(2,2,i[0]+1)\n    sns.boxplot(df[i[1]])","bf568b53":"df['Lead Origin'].value_counts()","f8de8174":"BarPlot(df,'Lead Origin', (15,10))","31ff1700":"CountPlot(df,'Lead Origin','Conversion based on Lead Origin',(15,10),hue='Converted')","c824b16c":"df['Lead Source'].value_counts()","5e9d2d5a":"BarPlot(df,'Lead Source', (15,4))","3beadae8":"plt.figure(figsize=(20,30))\nsns.countplot(data = df, x= 'Lead Source', order=df['Lead Source'].value_counts().index,hue = 'Converted')                      \nplt.xticks(rotation=45)\nplt.show()\n","3f499d7c":"## Printing % of converted Lead with respect to Lead Source\nconvertcount=df.pivot_table(values='Lead Number',index='Lead Source',columns='Converted', aggfunc='count').fillna(0)\nconvertcount[\"Conversion(%)\"] =round(convertcount[1]\/(convertcount[0]+convertcount[1]),2)*100\nprint(convertcount.sort_values(ascending=False,by=\"Conversion(%)\"))","551b33bc":"cols=['Click2call', 'Live chat', 'Nc_edm', 'Pay per click ads', 'Press_release',\n  'Social media', 'Welearn', 'Bing', 'Blog', 'Testone', 'Welearnblog_home', 'Youtubechannel']\ndf['Lead Source'] = df['Lead Source'].replace(cols, 'Others')","e9343dfa":"BarPlot(df,'Lead Source', (15,10))","51d5bc6c":"plt.figure(figsize=(20,30))\nsns.countplot(data = df, x= 'Lead Source', order=df['Lead Source'].value_counts().index,hue = 'Converted')                      \nplt.xticks(rotation=45)\nplt.show()\n\nconvertcount=df.pivot_table(values='Lead Number',index='Lead Source',columns='Converted', aggfunc='count').fillna(0)\nconvertcount[\"Conversion(%)\"] =round(convertcount[1]\/(convertcount[0]+convertcount[1]),2)*100\nprint(convertcount.sort_values(ascending=False,by=\"Conversion(%)\"))","d8a9da82":"df['Do Not Email'].value_counts()","76759ff6":"BarPlot(df,'Do Not Email', (15,10))","b395b87e":"CountPlot(df,'Do Not Email','Do Not Email',(15,10),hue='Converted')","475dffcc":"df['Do Not Call'].value_counts()","ba61c9bc":"BarPlot(df,'Do Not Call', (15,10))","9752604a":"CountPlot(df,'Do Not Call','Conversion based on Do Not Call',(15,10),hue='Converted')","f539c9e4":"df = df.drop(['Do Not Call', 'Do Not Email'], axis=1)\ndf.shape","569592b1":"df.info()","875ce100":"df['Last Activity'].value_counts()","74277b75":"BarPlot(df,'Last Activity', (15,5))","bd375736":"CountPlot(df,'Last Activity','Conversion based on Last Activity',(15,10),hue='Converted')","6042cf53":"features = ['Search', 'Newspaper Article', 'X Education Forums', 'Newspaper' , 'Digital Advertisement','Through Recommendations']\nfor i in enumerate(features):\n    print(df[i[1]].value_counts())","691f93d5":"features = ['Search', 'Newspaper Article', 'X Education Forums', 'Newspaper' , 'Digital Advertisement','Through Recommendations']\nplt.figure(figsize = (20,20))\nfor i in enumerate(features):\n    plt.subplot(3,2,i[0]+1)\n    sns.countplot(x = df[i[1]], data = df) \nplt.show()","d55d372c":"Cols = ['Search', 'Newspaper', 'X Education Forums', 'Newspaper Article' , 'Digital Advertisement','Through Recommendations']\ndf = df.drop(Cols,axis=1)\ndf.head()","248913f3":"df['A free copy of Mastering The Interview'].value_counts()","2ca9922c":"BarPlot(df,'A free copy of Mastering The Interview', (15,10))","dc73c354":"CountPlot(df,'A free copy of Mastering The Interview','Conversion based on A free copy of Mastering The Interview',(15,10),hue='Converted')","725d237c":"df['Last Notable Activity'].value_counts()","327716ee":"BarPlot(df,'Last Notable Activity', (15,10))","f240295a":"## Droping Last Notable Activity \ndf= df.drop(['Last Notable Activity'], axis=1)\ndf.head()","ee03f28d":"df.shape","6f2468fd":"BarPlot(df,'Specialization', (15,10))","19d091c8":"CountPlot(df,'Specialization','Conversion based on Specialization',(15,10),hue='Converted')","2de992ba":"BarPlot(df,'City', (15,10))","296e6c27":"CountPlot(df,'City','Conversion based on City',(15,10),hue='Converted')","52f33d2b":"df['What is your current occupation'].value_counts()","8bf5369b":"BarPlot(df,'What is your current occupation', (15,10))","b48c8cb2":"CountPlot(df,'What is your current occupation','Conversion based on What is your current occupation',(15,10),hue='Converted')","beffb5ae":"BarPlot(df,'TotalVisits', (15,10))","a55bfd2c":"CountPlot(df,'TotalVisits','Conversion based on Total Visit',(15,10),hue='Converted')","0dd8aebf":"BarPlot(df,'Page Views Per Visit', (25,20))","c56bd661":"CountPlot(df,'Page Views Per Visit','Page Views Per Visit vs Conversion',(20,20),hue='Converted')","e70ed635":"df['Total Time Spent on Website'].value_counts()","da0471b4":"df['Total Time Spent on Website'] = df['Total Time Spent on Website'].apply(lambda x: round((x\/60), 2))\ndf.head()","8a9d70b3":"sns.distplot(df['Total Time Spent on Website'])\nplt.show()","edeedf4b":"# Let us split our dataframe to perform better analysis\ndf1=df[df['Total Time Spent on Website']>=1.0]\ndf1[\"Hours Spent\"]= df1[\"Total Time Spent on Website\"].astype(int)\n\ndf1.head()","57ea3d02":"CountPlot(df1,'Hours Spent','Conversion based on Last Activity',(15,10),hue='Converted')","1780699f":"plt.figure(figsize=(20,20))\nplt.xticks(rotation=45)\nplt.yscale('log')\nsns.boxplot(data =df1, x='TotalVisits',y='Total Time Spent on Website', hue ='Converted',orient='v')\nplt.title('Total Time Spent Vs Total Visits based on Conversion')\nplt.show()","1e9bec5a":"plt.figure(figsize=(20, 20))\nsns.heatmap(df.corr(), cmap='YlGnBu',annot=True)","4795d83c":"# Final Dataframe\ndf.head()","1ba35453":"### First we will convert the Yes\/No values in the 'A free copy of Mastering The Interview' column to 1\/0\n\ndf['A free copy of Mastering The Interview'] = df['A free copy of Mastering The Interview'].map(dict(Yes=1, No=0))\ndf.head()","b2ebe984":"dummy_Cols = ['Lead Origin', 'Lead Source', 'Last Activity', 'Specialization','What is your current occupation','City']\ndummy = pd.get_dummies(df[dummy_Cols],drop_first=True)\ndummy.head()","9e1d4fad":"combined = df.copy()\ncombined.shape","ee26741b":"combined = pd.concat([combined, dummy], axis=1)\ncombined.head()","4bf7413c":"### We will now drop the original columns and the columns that have 'Others' as a sub heading since we had \n### combined various values to create those columns\n\ncols = ['Lead Origin', 'Lead Source', 'Last Activity', 'Specialization','What is your current occupation','City',\n                     'Lead Source_Others','Specialization_Others']\ncombined = combined.drop(cols, axis=1)\ncombined.head()","da44ad0b":"combined.shape","326d28de":"combined.info()","9aa83395":"### First we will drop the Converted & Lead Number columns \nX = combined.drop(['Converted','Lead Number'], axis=1)\nX.head()","da86c74a":"X.shape","3d3f47ba":"### Adding the target variable 'Converted' to y\ny = combined['Converted']\n\ny.head()","047b1258":"# Splitting the data into train and test\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","067214a4":"X_train.shape","12bec952":"X_test.shape","a913e6f7":"X_train.head()","9a152a18":"X_train.shape","288083b0":"## Scaling numeric Variables\nscaler = StandardScaler()\n\nX_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.fit_transform(X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])\n\n\nX_train.head()","a8a5e40d":"X_train.shape","ad23beeb":"combined.corr()","b676b613":"plt.figure(figsize=(20,20))\nsns.heatmap(combined.corr(),cmap='YlGnBu',annot=True)","10d10ee8":"### Dropping highly correlated variables\nX_train = X_train.drop(['Lead Origin_Lead Add Form', 'Lead Source_Facebook'], axis=1)\nX_test = X_test.drop(['Lead Origin_Lead Add Form', 'Lead Source_Facebook'], axis=1)","e39d1f2a":"X_train.head()","aa83a6bb":"X_train.shape","54392cba":"plt.figure(figsize=(30,20))\nsns.heatmap(combined[X_train.columns].corr(),cmap='YlGnBu',annot=True)","c08f2261":"X_train.info()","3ffbabd5":"## Creating Logistic Regression Model\nlogisticRegressionModel = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogisticRegressionModel.fit().summary()","e4e9d789":"logreg = LogisticRegression()\n\nrfe = RFE(logreg, 15)\nrfe= rfe.fit(X_train,y_train)\nrfe.support_","f028c7ac":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","488af8c2":"col = X_train.columns[rfe.support_]","904d50fe":"X_train.columns[~rfe.support_]","a7f19098":"X_train1= X_train[col]\nX_train1","e769fb2f":"X_train_sm = sm.add_constant(X_train1)\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","fe688911":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","7fd98724":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","44ac2bcf":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Lead_Score_Prob':y_train_pred})\ny_train_pred_final['Lead'] = y_train.index\ny_train_pred_final.head()","103fd04c":"y_train_pred_final['Final_Predicted_Hot_Lead'] = y_train_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","4726f8b2":"from sklearn import metrics\nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted_Hot_Lead)\nprint(confusion)","75be3791":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted_Hot_Lead))","2325135b":"### Checking VIF values\nvif = pd.DataFrame()\nvif['Features'] = X_train1.columns\nvif['VIF'] = [variance_inflation_factor(X_train1.values, i) for i in range(X_train1.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","19b278c5":"X_train2 = X_train1.drop('Last Activity_Resubscribed to emails', axis=1)\nX_train2","41c3a3e4":"X_train_sm = sm.add_constant(X_train2)\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","f0105d82":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm).values.reshape(-1)","1fb4c053":"y_train_pred[:10]","b59355e2":"y_train_pred_final['Lead_Score_Prob'] = y_train_pred","5cfdcb94":"y_train_pred_final['Final_Predicted_Hot_Lead'] = y_train_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","ea3ff18e":"confusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted_Hot_Lead)\nprint(confusion)","a8de8cc0":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted_Hot_Lead))","fd81bd50":"vif = pd.DataFrame()\nvif['Features'] = X_train2.columns\nvif['VIF'] = [variance_inflation_factor(X_train2.values, i) for i in range(X_train2.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","4e245267":"X_train3 = X_train2.drop('What is your current occupation_Housewife', axis=1)\nX_train3.columns","bde31ee1":"X_train_sm = sm.add_constant(X_train3)\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","cc9427e3":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)","b15730bd":"y_train_pred[:10]","a71545bf":"y_train_pred_final['Lead_Score_Prob'] = y_train_pred","6e5c0860":"y_train_pred_final['Final_Predicted_Hot_Lead'] = y_train_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","6200861d":"confusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted_Hot_Lead)\nprint(confusion)","c80e0b9b":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted_Hot_Lead))","6185bdb1":"vif = pd.DataFrame()\nvif['Features'] = X_train3.columns\nvif['VIF'] = [variance_inflation_factor(X_train3.values, i) for i in range(X_train3.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","f8b82fa0":"X_train4 = X_train3.drop('What is your current occupation_Working Professional', axis=1)\nX_train4.columns ","5ad82289":"X_train_sm = sm.add_constant(X_train4)\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","7e39d28a":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)","0db5adef":"y_train_pred[:10]","aef1b003":"y_train_pred_final['Lead_Score_Prob'] = y_train_pred","f3ed8c23":"y_train_pred_final['Final_Predicted_Hot_Lead'] = y_train_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","071a1015":"confusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted_Hot_Lead)\nprint(confusion)","64c24177":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted_Hot_Lead))","3f42b2fa":"vif = pd.DataFrame()\nvif['Features'] = X_train4.columns\nvif['VIF'] = [variance_inflation_factor(X_train4.values, i) for i in range(X_train4.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","c262a3cd":"plt.figure(figsize=(20,20))\nsns.heatmap(X_train_sm.corr(),cmap='YlGnBu',annot=True)","7e7cec95":"y_train_pred = res.predict(X_train_sm)\ny_train_pred.head()","82293a3c":"y_train_pred_final['Final_Predicted_Hot_Lead'] = y_train_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","dd7ad85a":"confusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted_Hot_Lead)\nprint(confusion)","1eab79e2":"# Let's check the overall accuracy.\nprint(round(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted_Hot_Lead),2))","54e59e98":"y_train_pred_final['Lead_Score'] = round((y_train_pred_final['Lead_Score_Prob'] * 100),0)\n\ny_train_pred_final['Lead_Score'] = y_train_pred_final['Lead_Score'].astype('int')\n\n# Let's see the head\ny_train_pred_final.head()","df066bdc":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","3d5c5afb":"# Let's see the sensitivity of our logistic regression model\nround((TP \/ float(TP+FN)),2)","22891013":"# Let us calculate specificity\nround((TN \/ float(TN+FP)),2)","24b58967":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","1b3b8486":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Lead_Score_Prob, drop_intermediate = False )","91283cea":"draw_roc(y_train_pred_final.Converted, y_train_pred_final.Lead_Score_Prob)","26efb0f3":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","7d89e204":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['Probability','Accuracy','Sensitivity','Specificty'])\n\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","58d4a7a7":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='Probability', y=['Accuracy','Sensitivity','Specificty'])\nplt.show()","3575ae33":"y_train_pred_final['Final_Predicted_Hot_Lead'] = y_train_pred_final.Lead_Score_Prob.map( lambda x: 1 if x > 0.33 else 0)\n\ny_train_pred_final.head()","4ded0b7b":"# Accuracy\nround(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted_Hot_Lead),2)","e8c436fa":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted_Hot_Lead )\nconfusion2","ca8667f6":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","27210553":"# Let's see the sensitivity of our logistic regression model\nround(TP \/ float(TP+FN),2)","db99facd":"# Let us calculate specificity\nround(TN \/ float(TN+FP),2)","3b677cc1":"### Calculating Precision\nprecision =round(TP\/float(TP+FP),2)\nprecision","1b195530":"### Calculating Recall\nrecall = round(TP\/float(TP+FN),2)\nrecall","f3225a32":"### Let us generate the Precision vs Recall tradeoff curve \np ,r, thresholds=precision_recall_curve(y_train_pred_final.Converted,y_train_pred_final['Lead_Score_Prob'])\nplt.title('Precision vs Recall tradeoff')\nplt.plot(thresholds, p[:-1], \"g-\")    # Plotting precision\nplt.plot(thresholds, r[:-1], \"r-\")    # Plotting Recall\nplt.show()","3d3c2bac":"### The F statistic is given by 2 * (precision * recall) \/ (precision + recall)\n## The F score is used to measure a test's accuracy, and it balances the use of precision and recall to do it.\n### The F score can provide a more realistic measure of a test's performance by using both precision and recall\nF1 =2 * (precision * recall) \/ (precision + recall)\nround(F1,2)","f2befd33":"X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.transform(X_test[['TotalVisits',\n                                'Total Time Spent on Website','Page Views Per Visit']])","e6360c57":"X_train4.shape","4f8c4d10":"X_test = X_test[X_train4.columns]\n\nX_test.shape","b3e63230":"X_test.head()","a47692c1":"X_test_sm = sm.add_constant(X_test)","936aa4ff":"y_test_pred = res.predict(X_test_sm)","94e79091":"y_test_pred[:10]","98f6dbb9":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)\ny_pred_1.head()","eeb0d4ed":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","bb8b6c8a":"y_test_df['Lead'] = y_test_df.index","88143f36":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","a5bb9ffc":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","b51f30c0":"y_pred_final.shape","fe4d02d6":"# Renaming the column \n\ny_pred_final= y_pred_final.rename(columns={ 0 : 'Lead_Score_Prob'})","88c32856":"# Rearranging the columns\n\ny_pred_final = y_pred_final.reindex(['Lead','Converted','Lead_Score_Prob'], axis=1)","df142c93":"# Adding Lead_Score column\n\ny_pred_final['Lead_Score'] = round((y_pred_final['Lead_Score_Prob'] * 100),0)\n\ny_pred_final['Lead_Score'] = y_pred_final['Lead_Score'].astype(int)","c7455273":"# Let's see the head of y_pred_final\ny_pred_final.head()","cf2b3a8f":"y_pred_final['Final_Predicted_Hot_Lead'] = y_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > 0.33 else 0)","dbcb80a3":"y_pred_final.head()","367a1dba":"# Let's check the overall accuracy.\nround(metrics.accuracy_score(y_pred_final.Converted, y_pred_final.Final_Predicted_Hot_Lead),2)","4f0ede99":"confusion3 = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.Final_Predicted_Hot_Lead )\nconfusion3","6f2f5356":"TP = confusion3[1,1] # true positive \nTN = confusion3[0,0] # true negatives\nFP = confusion3[0,1] # false positives\nFN = confusion3[1,0] # false negatives","1aa06fc2":"# Let's see the sensitivity of our logistic regression model\nround((TP \/ float(TP+FN)),2)","584cb8f2":"# Let us calculate specificity\nround(TN \/ float(TN+FP),2)","78f1629a":"y_train_pred_final = y_train_pred_final.reindex(['Lead','Converted','Lead_Score_Prob','Lead_Score','Final_Predicted_Hot_Lead'], axis=1)\ny_train_pred_final","3ca3983e":"### Generating table\nresultingTable1 = pd.merge(y_train_pred_final,df,how='inner',left_on='Lead',right_index=True)\nresultingTable1[['Lead Number','Lead_Score']].head()","d9e22372":"### Generating table\nresultingTable2 = pd.merge(y_pred_final,df,how='inner',left_on='Lead',right_index=True)\nresultingTable2[['Lead Number','Lead_Score']].head()","dccc841e":"result_df= pd.concat([resultingTable1, resultingTable2])","b68fb194":"result_df","8d25ced2":"### renaming Converted_x to Converted and droping Converted_y as both are same\nresult_df=result_df.rename(columns={'Converted_x' : 'Converted'})\nresult_df= result_df.drop(['Converted_y'], axis=1)","2c90b9b8":"result_df","00d6e742":"# coefficients of our final model \n\npd.options.display.float_format = '{:.2f}'.format\nnew_params = res.params[1:]\nnew_params","bb1837fe":"# Getting a relative coeffient value for all the features wrt the feature with the highest coefficient\n\nfeature_importance = new_params\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nfeature_importance","c38d6fc8":"#### Observation\nThe p value for What is your current occupation_Working Professional is above the threshold at 0.440\n\nAccuracy is same as previous model 79.80\n\nWe will be dropping this variable in the next model","e62daa87":"### Observation\nFrom the resultant shape we can confirm that the number of rows in the final dataset are the same as it's in original. Therefore we can use the values from the 'result_df' dataset to pursue the leads.based on the key insights identified above.","d5dbbaa4":"### Final Model Reporting & Equation","4408339f":"#### Observation\nFrom our analysis we see that this column holds similar data represented in the **Last Activity column**. We will drop this one and keep the Last Activity column.","bdb07da2":"##### Observation\nFrom the above bar plot it looks like we have a lot of small categories within the tags section. Moreover these tags are added by the sales team. We can safely drop this column as this doesn't provide a lot of insight.","1d9d3aa2":"### Creating Dummy Values","703ff9dc":"#### Observation\n- The VIF values for all the variables in the model look to be under control\n- The p value for What is your current occupation_Housewife is very high at 0.999 & above the threshold 0.05\n- We will be dropping What is your current occupation_Housewife in the next model\n- Accuracy is same as previous model 79.80","1f65aacb":"### Scaling","6d55fb6e":"#### Observation\nFrom the above curve we can see that the optimal cutoff is at 0.33. This is the point where all the parameters are equally balanced","c9fa5777":"### Observations\nBased on the above we can see that the data is highly skewed. Therefore we will be dropping this column eventually\n\nNote\nWe will be dropping the columns Do Not Email & Do Not Call as the data is highly skewed towards the No section. These two columns can be take as safe assumptions by company X that 99% of their prospective customers do not like to be called or receive emails.","8aa0fef9":"##### Observation\nThere are lot of outliers in **TotalVisits** and **Page Views per Visit**","bad8d984":"**log odds = 1.3837 +(1.0659 Total Time Spent on Website) + (1.1280 Lead Source_Olark chat) + (3.5984 Lead Source_Reference) + (5.4963 Lead Source_Welingak website) + (-1.2127 Last Activity_Converted to Lead) + (-1.7984 Last Activity_Email Bounced) + (2.1604 Last Activity_Had a Phone Conversation) + (-1.4009 Last Activity_Olark Chat Conversation) + (1.1884 Last Activity_SMS Sent)+(-2.8435 What is your current occupation_Other)+(-2.3752 What is your current occupation_Student)+(-2.7984 * What is your current occupation_Unemployed)**","3164bce8":"### Finally Checking Dataset","54d9030e":"#### Observation\nThe data feels highly skewed in these columns and confirms our assumption that it is correctly represented in the Lead Source Column\n\nSince the data for these columns is already correctly represented in the 'Lead Source' column, we will drop these columns","828d91c2":"### Checking Continuous Variables","5fa24206":"## Step 3: EDA\nSteps Perfomed in this section\n- Outlier Treatment\n- Univariate Analysis\n- Bivariate Analysis","be7563cc":"### Feature Selection Using RFE","b35be472":"### Model 4","24c21bc2":"### For testing set","7518f06b":"#### Lead Quality","e5d29dc1":"#### Observation\nWe have successfully removed the highly corelated variables from the traiing and test datasets.","6e1bd6ad":"### Checking Categorical Variables","0cf2a925":"#### Total  Visit","cee014cc":"#### Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0","5f021748":"#### Observation\nThere are a lot of columns with 1 or two unique values.\n\nBelow are the columns that have only one unique value, since they won't have anything to contribute to the model significantly,  we will remove these columns.\n\n- Get updates on DM Content\n-  Update me on Supply Chain Content\n- I agree to pay the amount through cheque\n- Receive More Updates About Our Courses\n- Magazine\n\nNote: There are no null values in these columns as seen from the null values table above.","0905cbdd":"#### Observation\nBased on the above heatmap we can see that we don't have any highly correlated features. Therefore there is no multicollinearity in the dataset.","0d58eeca":"#### Page Views Per Visit  ","5f82afda":"#### Observations\nBased on the plots above we observe that:\n\n- Most students found X education via 'Google' search\n- However, most of the google search leads weren't converted to actual students of the platform\n- References had the highest number of conversions at 92%\n- Welingak website also had a significantly high number of conversions at 99%\n- Welearn & Nc_edm had 100% conversion but due to their low numbers overall it might not be a correct picture of the situation\n- No conversions were made through the youtube channel, blog, press releases, pay per click ads or Welearnblog_home\n\n**Note:** Let us merge the columns with low numbers into a common category: 'Others'","f8248bd7":"#### A free copy of Mastering The Interview","18dc128e":"### Generating Leads Table\nAssigning Lead score to the respective lead numbers present in our original dataset.","22cb8106":"##### Observation\nWe can clearly see that this column is heavily skewed towards better career prospects. Since it doesn't really provide any more information, we can drop this column and keep note that all candidates that take this course are looking to have a better career.","c9e1fed0":"## Step 5 Modeling","b14d60df":"### Specialization","8821c7d2":"#### Observation\nNo Duplicates value for Prospect ID and Lead Number","93540e05":"### Tags","ae081964":"### Model 3","8e147181":"### Checking  Corelation b\/w Variables","756de167":"##### Note:\nSince there are 36% null values that haven't yet been accounted for, we will replace those with 'Others'. This is being done because the NaN values have the highest percentage of values that haven't been shown above. It simply means that the user did not have any option relevant to them in this field.","6d72f056":"\n\nLet us now look at the following columns: Asymmetrique Activity Index, Asymmetrique Profile Index,Asymmetrique Profile Score and Asymmetrique Activity Score.\n\nWe know from the data dictionary that these are scores assigned to a customer based on their activity and profile Via X- Education employee after calling to Lead.","a8163332":"### Specialization","323bf1c0":"### Observation\nFrom the above plots it is safe to infer that:\n\n- People who dont visit any pages have the highest count of conversion overall\n- Less than half the people who visit 2 pages on average convert to students","61c9084a":"### Making Predictions on test set","cb098c50":"### Model 2","7fb155e1":"#### Checking all columns individually having null values in order to decide to impute\/drop the column\nAs we can see some of the columns have substantial number of null or missing values. If we drop all these columns we will lose a lot of information so instead of dropping them, for some of the feature variables we will create a new value as 'Unknown' ","ad36f675":"## Step 4 Data Preparation","ebfeb1b2":"## Step 2: Data Cleaning","982d0610":"### What is your current occupation","7b0587ac":"### Model 5","d00d9320":"#### Observation\nThe precision vs recall tradeoff value from the above graph is at 0.4","f5c2d5d1":"### Country","b13b5264":"#### Observation\n- Mumbai is having 37% Conversion rate","0395f496":"#### Do Not Email","c9783e9f":"#### Observation\nFrom the ROC curve we can say that the model will be able to provide us with a good result overall.","2b6478e9":"#### Observation\nTo imporve the conversion rate X education should focus on providing incentives to referrals as well as improve the lead conversion through olark chat, organic search, direct traffic, and google leads and generate more leads from reference and welingak website.","347a6c4f":"##### Observation\nWe still have a few columns that have a high number of null values.","a8f04447":"#### Generating predicted values on the training set","84080731":"Observation\nThere is a lot of variation in the data and the number of null values is also very high at 45.65% and its assigned to leas aster calling them via Employee of X-education. Therefore we will drop these columns.\n\nLet us now drop all the 4 columns: **Asymmetrique Activity Index, Asymmetrique Activity Score, Asymmetrique Profile Index and Asymmetrique Profile Score**.","b8807511":"### What matters most to you in choosing a course","82362c92":"#### Observation:\nThere are some columns with over 50% of null values.\n","f144d2d2":"### What is your current occupation","18dffd48":"#### Observation\nFrom the above scores we note that our model has a good overall relevancy, defined by Precision, at 70% & a great return of relevant results, defined by Recall, at 80%.\n\nFor the purposes of our model we will focus on the Recall result as we would not like to miss out on any hot leads that are willing to be converted.","d90119e5":"#### Observation\n- Housewife are having 100% Conversion rate but their count are very less\n- Unemployed are having 34% Conversion rate\n- Student are having 37% Conversion rate","8b42c190":"##### Observation\nWe will impute the missing values with 'Mumbai' since it has the highest count.","739e02df":"## Step 1: Reading and Understanding the Data","13b8d49a":"### Last Notable Activity ","c3aae519":"### Insights\n- Hot Leads are identified as 'Customers having lead score of 33 or above'\n- Sales Team of the company should first focus on the 'Hot Leads'\n- Higher the Lead Score, higher the chances of conversion of 'Hot Leads' into 'Paying Customers'\n- The 'Cold Leads'(Customer having lead score < 33) should be focused after the Sales Team is done with the 'Hot Leads'","68d703c4":"### TotalVisits","4b3e00a9":"### Observation\nFrom the above bar plot we can infer that:\n\n- The highest number of conversions happen when people are spending around 18 hours or above on the website\n- People who spent around 3 hours on the website didn't opt for any courses.\n- From the boxplot we can see better that the longer you stay on the website, the higher your chances of conversion as well.\n \n Overall more time the user spends on the website, the better their chances of becoming a student.","9f3d0be1":"### Observation\nNow that our dataset is clear of all the null values we can begin performing analysis on the remaining columns","7ef1505a":"##### Note\nSince a mode value for this column is India, we can replace the missing values with India. Since this will potentially skew the data heavily in the model we will drop this column.","09a4c07d":"##### Observation\nWe have reduced a lot of columns and imputed missing values in a few of them. In the remaining columns we can safely impute the missing value with the mode value since it has less than 5% missing values.","bc1c4617":"#### Total Time Spent on Website","8f0bc3c8":"#### Creating new column 'Lead_Score'\nLead_Score would be equal to (Lead_Score_Prob * 100)","b179c79b":"### City","b135e38a":"##### Observation\nFrom the abover barplot we see that the number of values for 'Not Sure' are considerable high at 63.14%. We will drop this column","f13532c6":"#### Observation\nAs we can see above, when we are selecting the optimal cutoff = 0.33, the various performance parameters Accuracy, Sensitivity & Specificity are all around 80%\n\nThis meets our objective of getting a highly sensitive model with 80% sensitivity","b6154670":"#### Total Time Spent on Website","17d3e5ba":"### Precision and Recall Curve","91dceb0c":"#### Observation And Conclusion (on Test Set)\nAs we can see above when cut-off = 0.33, the various Model Performance parameters on test set are as per below\n- Sensitivity = 80%\n- Specificity = 80%\n- Accuracy = 80%%\n\nAll the 3 performance parameters on test set appear to be almost same with no much variation, so we are good with the modeling now.","d10022d5":"#### Lead Origin","8d9dd1e5":"#### Observation\nThe above looks like time spent was recorded in minutes. We will convert the entire column into hours for ease of analysis","769c2fa5":"### Observations\nFrom the above analysis we can conclude that:\n\n- People who make more than 10 visits are almost 50% likely to be converted\n- Only 15% of people who visited the website once converted to student. This could imply that people weren't able to gather all the information they needed easily. Hence, decided not to opt for any course.","c8924575":"#### Plotting the ROC Curve","cd519ae6":"##### Observation\nThe data in this column looks skewed, however, it also defines potential target market for company X. We will impute the missing values with 'Unemployed' and drop the column if analysis further down seems it necessary","e08c0de3":"#### Observation\n- The VIF values for all the variables in the model look to be under control\n- The p value for **Last Activity_Resubscribed to emails is very high** at 1 & above the threshold 0.05\n- We will be dropping Last Activity_Resubscribed to emails in the next model\n- Model's accuracy is 79%","d72f00d9":"#### Do Not Call","ab294579":"#### Observation\nA large number of candidates, 64% didnt opt for any course even though the would like a free copy. 60% of the candidates didn't opt for any course or the free book.","d0dca3a0":"### City","62aaea63":"#### Note:\n**Search, Newspaper Article, Education Forums, Newspaper , Digital Advertisement and Through Recommendations**, are already represented in the 'Lead Source' column.\n\nWe will carry out basic univariate analysis on them and make a decision on if we need to drop them or not.","d4d3ed9b":"### Merging both resulting tabel","2f8e061e":"#### Observation\nBased on the above statistics for Accuracy(80%), Sensitivity(64%) and Specificity(89%) we can say that our trained model is currently highly specific but not very sensitive. Our objective is to create a highly sensitive model with 80% sensitivity. Let us find cut-off values using ROC curves to improve this.","8517c419":"#### Observation\nThe shape of the dataset is 9240x37\n\nOriginal conversion rate of company X is 38.54%.\n\nLarge number of 'Select' values present for Lead Profile and City in the dataset. These values correspond to the user having   not made any selection.\n\nThere are 7 numerical columns and 30 categorical columns","36c668cb":"#### Observation\nAs we can confirm with our heatmap, there is no multicollinearity in the model.","cf567620":"### Last Activity","5ce546da":"### Performing Train - Test Split","077941d1":"### Finding Optimal Cutoff Point\nOptimal cutoff probability is that prob where we get balanced sensitivity and specificity","c2a27e56":"#### Lead Source","ed324b33":"##### Assessing the model with StatsModels","acfdb834":"##### Making predictions on the test set","eaca356c":"#### Observation\n- Healthcare Management specialization is having 50% conversion rate\n- Finance Management specialization is having 45% conversion rate","4b6c10f9":"### Observation\nModel 5 meets all our criteria:\n\n- The VIF values are under 3\n- The p values are under 0.05\n- The 12 selected features look significant\n\nLet us generate a heatmap to confirm that there is no multicollinearity","197718ac":"#### Observation\nBased on the plots above we can infer that:\n\n- People interacting with the portal usually send sms the most\n- Only 24% of people who visit the website convert to actual students","8437789a":"#### Last Activity","e7a094a6":"#### Observation\nBased on the above we can see that the data is highly skewed. Therefore we will be dropping this column eventually","4de141e9":"### For training set","87d54cdb":"### Page Views Per Visit","47c413ab":"#### Observation\nBased on the F1 score we can say that our model is fairly accurate. Let us test this accuracy on the test set.","adbbc93c":"#### Asymmetrique Activity\/Profile Index Activity and Score","5279cb1a":"### Observation\nFrom the barplot above we can infer that:\n\n- Lead Add Form has the highest conversion rate at 92%\n- Quick Add Form has 100% conversion rate but it has only 1 entry, so it might not be that reliable as a lead to go on\n- API has the least amount of conversions"}}