{"cell_type":{"faeb4727":"code","500a343b":"markdown"},"source":{"faeb4727":"import numpy as np\nimport pandas as pd\n\npd.reset_option('^display.', silent=True)\npd.set_option('mode.chained_assignment', None)\n\n# Load the full dataset\nX = pd.read_csv('\/kaggle\/input\/passenger-list-for-the-estonia-ferry-disaster\/estonia-passenger-list.csv')\n\n# Split X and y\ny = X.Survived\nX = X.drop(['Survived'], axis=1) \n\n# Make a naive classifer that says nobody survived\nfrom sklearn.base import BaseEstimator\nclass NobodySurvivedClassifier(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X),1), dtype=bool)\n\nnobody_survived_classifier = NobodySurvivedClassifier()\nnobody_survived_classifier.fit(X[:1], y[:1])\ny_pred = nobody_survived_classifier.predict(X[1:])\n\n# Lets get an amazing accuracy score!\nfrom sklearn.metrics import accuracy_score\nprint(f'Accuracy score for baseline classifier: {accuracy_score(y[1:], y_pred)}')","500a343b":"This is just the naive baseline classifier that I mentioned in my dataset task. It basically predicts that nobody survived the Estonia disaster and gets a pretty good score because only about 15% of the passengers actually survived. Predicting nobody survived means we're right 85% of the time, brilliant! This demonstrates why accuracy is generally not the preferred performance metric for classifiers, especially when we're dealing with an imbalanced dataset, which is the case for this ferry disaster."}}