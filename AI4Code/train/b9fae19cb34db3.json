{"cell_type":{"7fbbcaae":"code","0a76956e":"code","84420075":"code","007e6082":"code","1fd86f98":"code","c965e9c1":"code","d2939785":"code","e9f354db":"code","b9a57263":"code","443a9856":"code","f885b2fc":"code","ae73d806":"code","ab0a9ec2":"code","e775873c":"code","7212bbcf":"code","298b4342":"code","8a6129e9":"code","761f50da":"code","5ca843ab":"code","47883ca6":"code","a4d056b0":"code","4d6a9bf9":"code","af3c0fdb":"code","382c8639":"code","cb2fe04e":"code","5f4c020c":"code","fced599e":"code","1468c463":"code","5cfc3d09":"code","006cc361":"code","3fa87512":"code","5c3305e8":"code","de3e1d4f":"code","81adbb11":"code","6d7d33f7":"code","7c4dd5e5":"code","16fc8a7a":"code","4548e6af":"code","3caa344e":"code","c8279da8":"code","53fc8613":"code","88a9a9a4":"code","122c12e1":"code","fadf332e":"code","78a8e0d4":"code","93644a97":"code","083dda62":"code","8501bdad":"code","b90076ec":"code","c8f92297":"code","340d9fdf":"code","1c36ce8e":"code","597fa13a":"code","06ab6761":"code","04bf6152":"code","18ad69de":"code","6dad295e":"code","f58fabbf":"code","cabd1b6e":"code","604bc5e4":"code","c310bc78":"code","cd713615":"markdown","ee442d5a":"markdown","cce8a03a":"markdown","55a53931":"markdown","49ea0f6e":"markdown","675b5919":"markdown","c98e4dab":"markdown","c239c4ae":"markdown","6a2c864a":"markdown","4a38e994":"markdown","8d7ee82e":"markdown","b913e7e6":"markdown","2b14722f":"markdown","c0c4234c":"markdown","a490bf2a":"markdown","1cd9c589":"markdown","3b6ed3b4":"markdown","a41d2de9":"markdown","e42806bd":"markdown","c06869ff":"markdown","7d964123":"markdown","3a60c07a":"markdown","9adfcaee":"markdown","a7945b53":"markdown","47ef086c":"markdown","86f010c7":"markdown","f6e9d981":"markdown","e6491892":"markdown","69de1fb4":"markdown","c8985664":"markdown","f4a232a1":"markdown","cbcab727":"markdown"},"source":{"7fbbcaae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current sessio","0a76956e":"#importing basic modules\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# %matplotlib inline: This is known as magic inline function.\n#When using the 'inline' backend, our matplotlib graphs will be included in our notebook, next to the code. ","84420075":"test_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\ntrain_data = pd.read_csv('..\/input\/titanic\/train.csv')","007e6082":"train_df = train_data.copy() #copy of train_data as train_df\ntrain_df.head() #train_df preview","1fd86f98":"test_df = test_data.copy()#copy of train_data as train_df\ntest_df.head()","c965e9c1":"train_df.shape","d2939785":"test_df.shape","e9f354db":"train_df.isnull().sum() #isnull is used for checking missing values.","b9a57263":"test_df.isnull().sum()","443a9856":"test_df.Fare.fillna(test_df[\"Fare\"].mean(skipna=True), inplace=True)","f885b2fc":"plt.figure(figsize=(15,6))\nax = sns.distplot(train_df.Age)\nax.set_title('Age', fontsize=14)","ae73d806":"train_df.Age.mean(skipna = True)","ab0a9ec2":"train_df.Age.median(skipna = True)","e775873c":"train_df.Age.fillna(train_df[\"Age\"].median(skipna=True), inplace=True)","7212bbcf":"train_df['Embarked'].value_counts()","298b4342":"train_df.Embarked.fillna(train_df['Embarked'].value_counts().idxmax(), inplace=True)","8a6129e9":"test_df.Age.fillna(test_df[\"Age\"].median(skipna=True), inplace=True)","761f50da":"train_df.drop([\"Cabin\"], axis=1, inplace=True)\ntrain_df.head()","5ca843ab":"test_df.drop(['Cabin'], axis=1, inplace=True)\ntest_df.head()","47883ca6":"train_df['PassengerId'].nunique()","a4d056b0":"train_df['Name'].nunique()","4d6a9bf9":"train_df['Ticket'].nunique()","af3c0fdb":"train_df['Pclass'].nunique()","382c8639":"train_df['SibSp'].nunique()","cb2fe04e":"train_df['Parch'].nunique()","5f4c020c":"train_df['Fare'].nunique()","fced599e":"train_df.drop(['Name','PassengerId','Ticket'], axis=1, inplace=True)","1468c463":"#Similarly with test_df\ntest_df.drop(['Name','PassengerId','Ticket'], axis=1, inplace=True)","5cfc3d09":"plt.figure(figsize=(15,6))\nax = sns.barplot(x=train_df['Pclass'], y=train_df['Fare'])\nax.set_xticklabels(ax.get_xticklabels(), rotation=0)\nax.set_title('Pclass Vs Fare', fontsize=14)","006cc361":"train_df.head()","3fa87512":"plt.figure(figsize=(15,6))\nax = sns.barplot(x=train_df['Pclass'], y=train_df['Survived'])\nax.set_xticklabels(ax.get_xticklabels(), rotation=0)\nax.set_title('Pclass Vs Survived', fontsize=14)","5c3305e8":"ax = sns.catplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", kind=\"bar\", data=train_df)","de3e1d4f":"ax = sns.catplot(x=\"Sex\", y=\"Age\", hue=\"Survived\", kind=\"violin\", split=True, data=train_df)","81adbb11":"print(train_df.Age.max())\nprint(train_df.Age.min())\n\nage_gap = []\n\nfor i in range(0,81,5):\n    age_gap.append(i)\n    \ntrain_df['Age'].groupby(pd.cut(train_df['Age'], age_gap)).count()","6d7d33f7":"plt.figure(figsize=(18,10))\nax = sns.barplot(x='Age', y='Survived', data=train_df)\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"right\");","7c4dd5e5":"train_df['Age'] = np.where((train_df['Age'] > 17) & (train_df['Age'] < 49),0,1)","16fc8a7a":"#similar changes with test data\ntest_df['Age'] = np.where((test_df['Age'] > 17) & (test_df['Age'] < 49),0,1)","4548e6af":"plt.figure(figsize=(15,6))\nax = sns.barplot(x=train_df['Age'], y=train_df['Survived'])\nax.set_xticklabels(ax.get_xticklabels(), rotation=0)\nax.set_title('Age Vs Survived', fontsize=14)","3caa344e":"plt.figure(figsize=(15,6))\nax = sns.kdeplot(train_df[\"Fare\"][train_df.Survived == 1],shade= True)\nax1 = sns.kdeplot(train_df[\"Fare\"][train_df.Survived == 0], shade= True)\nplt.legend(['Survived', 'Died'])\nplt.xlim(-20,200);","c8279da8":"plt.figure(figsize=(15,6))\nax = sns.barplot(x=train_df['Embarked'], y=train_df['Survived'])\nax.set_xticklabels(ax.get_xticklabels(), rotation=0)\nax.set_title('Embarked Vs Survived', fontsize=14)","53fc8613":"train_df.SibSp.value_counts()","88a9a9a4":"train_df.Parch.value_counts()","122c12e1":"# Combining result of SibSp and Parch and storing result in new column travel\ntrain_df['Travel'] = train_df['SibSp'] + train_df['Parch']","fadf332e":"#Changing values according to our analysis.\ntrain_df['Travel'] = np.where((train_df.Travel == 0 ), 0, 1)","78a8e0d4":"plt.figure(figsize=(15,6))\nax = sns.barplot(x=train_df['Travel'], y=train_df['Survived'])\nax.set_xticklabels(ax.get_xticklabels(), rotation=0)\nax.set_title('Travel Vs Survived', fontsize=14)","93644a97":"train_df.drop([\"SibSp\",'Parch'], axis=1, inplace=True)\ntrain_df.head()","083dda62":"#Similar chnages to test data\n\ntest_df['Travel'] = test_df['SibSp'] + test_df['Parch']\ntest_df['Travel'] = np.where((test_df.Travel == 0 ), 0, 1)\ntest_df.drop([\"SibSp\",'Parch'], axis=1, inplace=True)\ntest_df.head()","8501bdad":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\ntrain_df['Sex'] = le.fit_transform(train_df['Sex'])\ntrain_df.head()","b90076ec":"#Similarly with test data.\ntest_df['Sex'] = le.fit_transform(test_df['Sex'])\ntest_df.head()","c8f92297":"train_df = pd.get_dummies(train_df, columns=['Embarked'], prefix = ['Embarked'])\ntrain_df.head()","340d9fdf":"#Similarly with test data\ntest_df = pd.get_dummies(test_df, columns=['Embarked'], prefix = ['Embarked'])\ntest_df.head()","1c36ce8e":"X1 = train_df.drop(['Embarked_C','Embarked_Q', 'Embarked_S'],axis=1)","597fa13a":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n\nX_vif = add_constant(X1)\n\npd.Series([variance_inflation_factor(X_vif.values, i) \n               for i in range(X_vif.shape[1])], \n              index=X_vif.columns)","06ab6761":"#Checking for correlation.\n\nplt.figure(figsize=(15,6))\nax = sns.heatmap(train_df.corr(),annot = True)\nax.set_title('CORRELATION MATRIX', fontsize=14)","04bf6152":"X = train_df.drop([\"Survived\"], axis=1)\ny = train_df['Survived']","18ad69de":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)\n\n# instantiate the model (using the default parameters)\nlogreg = LogisticRegression(max_iter=1000)\n\n# fit the model with data\nlogreg.fit(X_train,y_train)\n\ny_pred=logreg.predict(X_test)","6dad295e":"# import the metrics class\nfrom sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\ncnf_matrix","f58fabbf":"class_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","cabd1b6e":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))","604bc5e4":"y_pred_proba = logreg.predict_proba(X_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","c310bc78":"X1 = ['Pclass','Sex','Age','Fare','Travel','Embarked_C','Embarked_Q','Embarked_S']\n\ntest_df['Survived'] = logreg.predict(test_df[X1])\ntest_df['PassengerId'] = test_data['PassengerId']\n\nPredicted_outcome=  test_df[['PassengerId','Survived']]\n\nPredicted_outcome.to_csv(\"Predicted_outcome.csv\", index=False)\n\nPredicted_outcome.head()","cd713615":"I'll go with median as it is by default in round figure so no need to work on it.","ee442d5a":"As you can also notice that Suvived column is missing from the test data. As, this is out outcome variable. Otherwise I would have merged these two csv. Well, no issues we'll first perfrom all the operations with train_df and similar changes to test_df.\n\nSo, we'll train train our data with train_df and then predict the test data. ","cce8a03a":"So, most of the people have Port of Embarkation is S which stands for Southampton. Let's quickly fill the na with S.","55a53931":"These are the final Columns of test and train data.","49ea0f6e":"**Understanding**: Age raging between 25-35 has the maximun number of people travelled on the Titanic. Let's check mean and median of the entries without missing values.","675b5919":"Let's dive into our dataset.","c98e4dab":"**Understanding**: \n- 1. 0-16 aged kids have survived more than the people belonging to group 17-48.\n- 2. Again, 48-80 aged pople have survived more.\n\nSo they preffered to save kids, olds and women first. Good work!! \n\nBut, I'll divide the age between two groups below 17 & above 48 in 1 and 17-48 in 0.","c239c4ae":"![image.png](attachment:image.png)","6a2c864a":"**Understanding**:\n\nAs we discussed above the more you pay the more your life is secured. Here, is the same thing like the feature Pclass.","4a38e994":"**Understanding**: \n- 1. Females have survived more than Males.\n- 2. Superior Passenger class people survived in higher number than there preceeding class.\n- 3. Age has a little differnce but noticible point is childrens survived more than died.\n- 4. Death in male is higher than survival among age ranges ~20-35.\n","8d7ee82e":"#### Overview about the data:\n\nThe data has been split into two groups:\n\n- training set (train.csv)\n- test set (test.csv)\n\nThe **training set** should be used to build your machine learning models. For the training set, we provide the outcome (also known as the \u201cground truth\u201d) for each passenger. Your model will be based on \u201cfeatures\u201d like passengers\u2019 gender and class. You can also use feature engineering to create new features.\n\nThe **test set** should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n\n**Columns:**\n\n- survival\t Survival\t 0 = No, 1 = Yes\n- pclass\t Ticket class\t 1 = 1st, 2 = 2nd, 3 = 3rd\n- sex\t Sex\t\n- Age\t Age in years\t\n- sibsp\t # of siblings \/ spouses aboard the Titanic\t\n- parch\t # of parents \/ children aboard the Titanic\t\n- ticket\t Ticket number\t\n- fare\t Passenger fare\t\n- cabin\t Cabin number\t\n- embarked\t Port of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton\n\n**Variable notes:**\n\n- pclass: A proxy for socio-economic status (SES)\n- 1st = Upper\n- 2nd = Middle\n- 3rd = Lower\n\n- age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n- sibsp: The dataset defines family relations in this way...\n- Sibling = brother, sister, stepbrother, stepsister\n- Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n- parch: The dataset defines family relations in this way...\n- Parent = mother, father\n- Child = daughter, son, stepdaughter, stepson\n- Some children travelled only with a nanny, therefore parch=0 for them.","b913e7e6":"`Task: Task is to predict survival of the Passengers in the test data.`","2b14722f":"**Analysis**: \n- train_df has three columns which is having missing values: age, cabin and embarked\n- test_df has two columns which is having missing values: age and cabin and Fare\n\nWe can use many funtions like fillna (backward or forward) or replacing it by using central tendency. If we notice carefully, for train_df Age column has 177 missing values out of 891 entries which is ~19-20% of the overall data and test_df has 86 out of 418 which is ~20-21% of the data, which can be handled but the issue is with Cabin column. In cabin column of both test_df and train_df ~77-78% of the data is missing which is quite huge. So, we will ignore them and there is missing value in Embarked column only for train_df which is just 2 entries, ~0.2-0.3% of the data. So, can easily be treated. So we left with Fare of test_df only 1 entry has null value.\n\nLet's first replace Fare null value with mean.","c0c4234c":"Now its time to convert Embarked caterogical using One Hot Encoding or get_dummines, and label encode Sex column.","a490bf2a":"**Analsis**:\n\nNone of the columns have high VIF. Hence, less multicolinearity. Great!!","1cd9c589":"**Analysis**:\n\nSo, people who were travelling alone had less chance to surive in comparison with travelling with someone.","3b6ed3b4":"###### Importing and preview of data","a41d2de9":"**Logistic Regression**: Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression).","e42806bd":"**Analysis**:\n\nBoth column have a similarity that if 0 means they all are travelling alone and if any other value, they are either with parents or sibilings or say they aren't travelling alone. So, let's make it little simple.\n\nPepople travelling Alone would be 0, or travelling with someone would be 1. Let's make these changes.","c06869ff":"**Understanding**: Ah! so peoples who paid more had more secured life than those who paid less. So, it's clear that these are orinal categories so it's not needed to encode it using One Hot Encoding.","7d964123":"Dealing with cateorical Terms.","3a60c07a":"**Analysis**: Now, PassengerId and name of every entries are unique, as well as ticket number except those who came with some person. So these features are not going to help us in further process. So, let's drop it.","9adfcaee":"As Cabin has a lot of missing values we have ignored it and here we're done with missing values in train_df. Similar step in test_df for Age column.","a7945b53":"**Understanding**: So clearly, these are ordinal categories as 1st class passengers have paid more than 2nd trailing by 3rd class passengers.","47ef086c":"Now, will drop SibSp and Parch because they are left with no use.","86f010c7":"I'm going with the central tendency capping. But before choosing between mean median or mode let's check for the distribution of Age column.","f6e9d981":"##### Understanding our data","e6491892":"Now, we are left with columns like Travelling with Sibiling or Parents.","69de1fb4":"Let's drop cabin column from both train_df and test_df","c8985664":"Checking for multicollinearity among the continuous columns using VIF methods.\n\n**Multicollinearity**: Multicollinearity occurs when two or more independent variables are highly correlated with one another in a regression model.\n\n**Why not Multicollinearity?**: Multicollinearity can be a problem in a regression model because we would not be able to distinguish between the individual effects of the independent variables on the dependent variable.\n\n**Detection of Multicollinearity**: Multicollinearity can be detected via various methods. One of the popular method is using VIF.\n\n**VIF**: VIF stands for Variable Inflation Factors. VIF determines the strength of the correlation between the independent variables. It is predicted by taking a variable and regressing it against every other variable.","f4a232a1":"As you can see 0 has less chace for survival incomparison to 1","cbcab727":"So. people from Cheobarg survived more than Queensland and Southampton. One of the reason I assume of S being low because majority of people embark at S or may be more males embark at S or may be Pclass plays any role."}}