{"cell_type":{"b802387d":"code","1169f775":"code","f395fb81":"code","7d1cedc0":"code","eaca7aeb":"code","72e263e7":"code","4c9c2237":"code","883fc168":"code","1001f720":"code","48f9f544":"code","8bad8275":"code","94479eab":"code","63b20a8d":"code","192316c8":"code","f9bad0b4":"code","22514bb3":"code","635a7455":"code","42cd651a":"code","ac8fdba9":"code","dc5ad07b":"code","71a506fc":"code","0f86ebe2":"code","99ac1c6e":"code","3dae0d2b":"markdown","91fc9fd8":"markdown","f3f95ed6":"markdown","01a67e5f":"markdown","b31d39ae":"markdown","c89295cd":"markdown","2ff58a73":"markdown","f0e90f78":"markdown","d14a87d8":"markdown","7b97df65":"markdown","71e7ee3b":"markdown","a82eee67":"markdown","43513628":"markdown","1ce936c6":"markdown","ad4a5610":"markdown","1b85a653":"markdown"},"source":{"b802387d":"# Data Import on Kaggle\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Importing processing libraries\nimport numpy as np\nimport pandas as pd\n\n# Importing Visualisation libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Importing libraries for the metrics\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold\n\n# Importing libraries for the model\nimport xgboost as xgb \nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor","1169f775":"data = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest_data = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')","f395fb81":"memory_usage = data.memory_usage(deep=True) \/ 1024 ** 2\nprint('memory usage of features: \\n', memory_usage.head(7))\nprint('memory usage sum: ',memory_usage.sum())","7d1cedc0":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df\n\nreduced_df = reduce_memory_usage(data, verbose=True)","eaca7aeb":"reduced_df.describe()","72e263e7":"sample_df = reduced_df.sample(int(len(reduced_df) * 0.2))\nsample_df.shape\n\nsample_df = sample_df.drop(['id'], axis=1)","4c9c2237":"# Let's confirm if the sampling is retaining the feature distributions\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nsns.histplot(\n    data=reduced_df, x=\"f6\", label=\"Original data\", color=\"red\", alpha=0.3, bins=15\n)\nsns.histplot(\n    data=sample_df, x=\"f6\", label=\"Sample data\", color=\"green\", alpha=0.3, bins=15\n)\n\nplt.legend()\nplt.show();","883fc168":"f, ax = plt.subplots(figsize=(8, 6))\ncorr = reduced_df.iloc[:,:20].corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax)\nplt.show()","1001f720":"fig = plt.figure(figsize = (15, 50))\nfor i in range(len(sample_df.columns.tolist()[:100])):\n    plt.subplot(20,5,i+1)\n    sns.set_style(\"white\")\n    plt.title(sample_df.columns.tolist()[:100][i], size = 12, fontname = 'monospace')\n    a = sns.kdeplot(sample_df[sample_df.columns.tolist()[:100][i]], color = '#1a5d57', shade = True, alpha = 0.9, linewidth = 1.5, edgecolor = 'black')\n    plt.ylabel('')\n    plt.xlabel('')\n    plt.xticks(fontname = 'monospace')\n    plt.yticks([])\n    for j in ['right', 'left', 'top']:\n        a.spines[j].set_visible(False)\n        a.spines['bottom'].set_linewidth(1.2)\n        \nfig.tight_layout(h_pad = 3)\n\nplt.show()","48f9f544":"# The results from the tests were not useful so I've deleted them.\n# I have kept my code for ANOVA below if you want to refer to it.\n\n\n\n# import statsmodels.api as sm\n# from statsmodels.formula.api import ols\n\n# all_columns = \"+\".join(sample_df.columns[:-1])\n# my_formula = \"loss~\" + all_columns\n\n# mod = ols(formula=my_formula,\n#                 data=sample_df, family=sm.families.Gaussian()).fit()\n                \n# aov_table = sm.stats.anova_lm(mod, typ=2)\n# print(aov_table)","8bad8275":"x = sample_df.drop(['loss'], axis=1)\ny = sample_df.loss\n\nx_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.33, random_state=42)","94479eab":"scaler = MinMaxScaler()\nx_scaled = scaler.fit_transform(x_train)","63b20a8d":"x_scaled","192316c8":"model_dict = {\n#     'Random Forest Regressor': RandomForestRegressor(random_state=0, verbose=10),\n#     'Gradient Boosting Regressor': GradientBoostingRegressor(random_state=0, verbose=10),\n#     'Support Vector Machine': SVR(),\n#     'Decison Tree': DecisionTreeRegressor(random_state=0),\n    'XGB': xgb.XGBRegressor(random_state=0, verbose=10),\n    'Light GBM': lgb.LGBMRegressor(random_state=0, verbose=10)\n            }\nmodel_list = []\ntrain_acc_list = []\ntest_acc_list = []\ncounter_list = []\nprediction_list = []\nmetric_scores_list = []\n\nfor model, clf in model_dict.items():\n    clf.fit(x_train, y_train)\n    test_preds = clf.predict(x_test)\n    test_rmse = np.sqrt(mean_squared_error(y_test, test_preds))\n    \n    train_pred =  clf.predict(x_train)\n    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n    \n    model_list.append(model)\n    train_acc_list.append(train_rmse)\n    test_acc_list.append(test_rmse)  \n    print('{} training'.format(model), 'completed')\n\nresults = pd.DataFrame({\"model\": model_list, \"train_rmse\": train_acc_list, \"test_rmse\": test_acc_list})\n","f9bad0b4":"results","22514bb3":"params = {\n                       \"learning_rate\":[0.003, 0.008],\n                       \"subsample\":[0.84],\n                       'booster': ['gbtree'],\n                       'tree_method': ['gpu_hist'],\n 'colsample_bytree':[0.70],\n    'max_depth': [7],\n    'n_estimators': [2500],\n}\n\nxgb_estimator = xgb.XGBRegressor(random_state=42)\ngrid = GridSearchCV(xgb_estimator, param_grid=params, scoring='neg_root_mean_squared_error', cv=5, verbose=100)\nxgb_model = grid.fit(x_scaled, y_train)\n\nprint(xgb_model.best_params_, xgb_model.best_score_)\n","635a7455":"xgb_model = xgb.XGBRegressor(random_state=42, booster='gbtree', colsample_bytree= 0.7, learning_rate= 0.003, max_depth=7, n_estimators=2500, subsample= 0.84, tree_method= 'gpu_hist')\nxgb_model.fit(x_train, y_train)\noof_pred1 = xgb_model.predict(x_test)\noof_pred1 = np.clip(oof_pred1, y.min(), y.max())\n\nprint(f'Mean Error: {np.sqrt(mean_squared_error(y_test, oof_pred1))}')","42cd651a":"params = {\n    'num_leaves': [50],\n    'learning_rate': [0.003],\n    'max_depth': [-1],\n    'n_estimators': [2500],\n}\n\nlgb_estimator = lgb.LGBMRegressor(random_state=42)\n\ngrid = GridSearchCV(lgb_estimator, param_grid=params, scoring='neg_root_mean_squared_error', cv=5, verbose=100)\nlgb_model = grid.fit(x_scaled, y_train)\n\nprint(lgb_model.best_params_, lgb_model.best_score_)\n","ac8fdba9":"lgb_model = lgb.LGBMRegressor(learning_rate=0.003, max_depth=-1, n_estimators=1000, num_leaves=50, random_state=42)\nlgb_model.fit(x_train, y_train)\n\noof_pred1 = lgb_model.predict(x_test)\noof_pred1 = np.clip(oof_pred1, y.min(), y.max())\n\nfrom sklearn.metrics import mean_squared_error\nprint(f'Mean Error: {np.sqrt(mean_squared_error(y_test, oof_pred1))}')","dc5ad07b":"from sklearn.preprocessing import minmax_scale\n\na1 = lgb_model.feature_importances_\na2 = xgb_model.feature_importances_\n\naxis_x  = x_train.columns.values\naxis_y1 = minmax_scale(a1)\naxis_y2 = minmax_scale(a2)\n\nplt.style.use('seaborn-whitegrid') \nplt.figure(figsize=(16, 6))\nplt.title(f'XGBoost vs Light GBM Feature Importances', fontsize=12)  \n\nplt.scatter(axis_x, axis_y1, s=20, label='Light GBM') \nplt.scatter(axis_x, axis_y2, s=20, label='XGBoost')\n\nplt.legend(fontsize=12, loc=2)\nplt.show()","71a506fc":"# Let's first take the non-sampled data\n\nreduced_df = reduced_df.drop('id', axis=1)\nx_final = reduced_df.drop('loss', axis=1)\ny_final = reduced_df.loss\n\nx_train,x_test,y_train,y_test = train_test_split(x_final, y_final, test_size=0.33, random_state=42)","0f86ebe2":"lgb_model = lgb.LGBMRegressor(learning_rate=0.003, max_depth=-1, n_estimators=1000, num_leaves=50, random_state=42)\nlgb_model.fit(x_train, y_train)\n\noof_pred1 = lgb_model.predict(x_test)\noof_pred1 = np.clip(oof_pred1, y.min(), y.max())\n\nfrom sklearn.metrics import mean_squared_error\nprint(f'Mean Error: {np.sqrt(mean_squared_error(y_test, oof_pred1))}')","99ac1c6e":"final_preds = lgb_model.predict(test_data.drop('id', axis=1))\nnew_df = pd.DataFrame({'id': test_data['id'], 'loss': final_preds})\n\n# Submission\nnew_df.to_csv(\"submission2.csv\",index=False)","3dae0d2b":"## Sampling Data\n\nNow that we have reduced the memory usage by over 70%, let's sample the data. We are doing this to reduce the model training time. The sampling would preserve the distributions of each feature while taking only 20% of the entire dataset. We can then perform EDA, modelling, hyperparameter tuning and other steps on this sampled data.\n\nOnce we decide on the model we want to use, we can train the final model on the entire dataset again.","91fc9fd8":"### XGBoost\n\nThe parameters we set for grid search were:\n\n- learning_rate: 0.003, 0.008\n- max_depth: 3, 5, 7\n- n_estimators: 500, 1000, 2500\n\nand the top performing parameters after gridsearchcv were:\n- learning_rate: 0.003\n- max_depth: 7\n- n_estimators: 2500\n\nwith an RMSE of (-7.915772914886475)","f3f95ed6":"## Feature Importance\n\nLet's take a look at Feature Importance for both our models ","01a67e5f":"### Initial Model Training","b31d39ae":"## Plan\n\nMoving forward this is the plan we are going to be following. Keep in mind, this is not a concrete plan and I might change it as we move through the notebook. This will show you my process on how I approach these datasets.\n\n- *Memory Reduction*\n- *Sampling to Reduce Training Time*\n- *EDA*\n- *Model Development*\n- *Hyperparameter Tuning*\n- *Feature Importance from top models*\n- *Selecting the best Model*","c89295cd":"## EDA\n\nLet's start looking at any correlations that might exist among the features.\nWe will also be looking at the densities of every feature.","2ff58a73":"## Imports \n\nLet's import some of the libraries we will be using throughout the notebook","f0e90f78":"### LightGBM\n\nThe parameters we set for grid search were:\n\n- learning_rate: 0.003, 0.009\n- max_depth: -1, 3, 5\n- n_estimators: 500, 1000\n- num_leaves: 28, 31, 50, 75\n\nand the top performing parameters were\n- learning_rate: 0.003\n- max_depth: -1\n- n_estimators: 1000,\n- num_leaves: 50\n\nwith an RMSE of (-7.9347)","d14a87d8":"### Scaling\n\nHere we will be scaling the train data to normalise it between 0 and 1. This will not have any effect for most of our models since they are boosting but it is needed for the Support Vector Machine (SVM).","7b97df65":"## Model Selection\n\nIn this section, we will use some statistical methods and regressions to find siginificant features and possible interactions between them that might be important. For this, we will test out ANOVA, linear regression and GAM and see the results we get.\n\nFollowing that, we will start training some basic models on the data to make some predictions and see which ones to move forward with.\nWe will test SVM, XGBoost, LightGBM and Random Forrest.  ","71e7ee3b":"## Best Fit\n\nNow that we have performed hyperparameter tuning for our two top models, XGB and LGBM, we can start taking a deeper look at them and considering the best model to use.","a82eee67":"## Memory Reduction\n\nHere, we will take a look at the memory consumption by the current data and each feature following which we will try to reduce it to some extent. ","43513628":"### Train - Test Split\n\nLet's use our sample to split the data into train and test sets","1ce936c6":"## Hyperparameter Tuning\n\nIn this step, we are selecting our XGBoost and LightGBM models to perform Hyperparameter tuning on. We'll start off by using GridSearchCV on both these models with various parameters and selecting the best performing ones based on rmse score.","ad4a5610":"### Initial Model Selection\n\nNow that we've trained our first batch of models on default parameters, we can eliminate a few which don't do well.\n\nThe XGBoost and the lightGBM models performed the best so we will keep those and perform hyperparamter tuning on them.\n\n","1b85a653":"# Tabular Playground Series - Aug 21\n\nThis month, our data consists of 99 feature variables and our target variable is loss. We will first perform some basic EDA to take a better look at this data following which we will start working on our models. "}}