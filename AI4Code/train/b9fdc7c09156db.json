{"cell_type":{"5883bdc9":"code","77520fc5":"code","e629bfb7":"code","0032aed4":"code","7ffdb0d5":"code","0896c842":"code","c85cb33d":"code","1981b24e":"code","60cfe607":"code","0d06f1f8":"code","aa2ef776":"code","b1dd8c2c":"code","e7a92d7a":"code","f0f57f05":"code","d1cb482c":"code","b7e196f0":"code","a68a1141":"code","11f3f74a":"code","805f20b8":"code","82c20dc0":"code","b25bfafe":"code","dba95a41":"code","33417d61":"code","863f0bf8":"code","b2b05a9c":"markdown","905f7fd6":"markdown","f79db137":"markdown","186a8c7f":"markdown","deb3d433":"markdown","5447466c":"markdown"},"source":{"5883bdc9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport cv2\n# set the matplotlib backend so figures can be saved in the background\nimport matplotlib .pyplot as plt\n# import the necessary packages\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense\nfrom keras.optimizers import SGD\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.layers.core import Activation\nfrom keras.layers.core import Flatten\nfrom keras.layers.core import Dropout\nfrom keras import backend as K\nimport random\nimport os\n\n# Any results you write to the current directory are saved as output.","77520fc5":"data = []\nlabels = []\n\nfor path, _, files in os.walk('..\/input\/nonsegmentedv2'):\n    imagesPaths =  sorted([image for image in files])\n    #print(imagesPaths)\n    for imagePath in imagesPaths:\n        image = cv2.imread('..\/input\/nonsegmentedv2\/'+ path.split('\/')[3] + '\/' + imagePath)\n        image = cv2.resize(image, (32,32)).flatten()\n        data.append(image)\n        labels.append(path.split('\/')[3])\n        \ndata\n","e629bfb7":"data = np.array(data, dtype='float32') \/ 255.0\n# since the pixel intensities lies from 0 to 255, thus we normalized the data to 0 to 1\nlabels = np.array(labels)","0032aed4":"data","7ffdb0d5":"data.shape","0896c842":"labels.shape","c85cb33d":"# partition the data into training and testing splits using 75% of\n# the data for training and the remaining 25% for testing\n(trainX, testX, trainY, testY) = train_test_split(data,labels, test_size=0.25, random_state=42)","1981b24e":"## Here We are just doing the one-hot encoding for the labels.\nlb = LabelBinarizer()\ntrainY = lb.fit_transform(trainY)\ntestY = lb.transform(testY)","60cfe607":"trainY.shape","0d06f1f8":"trainY","aa2ef776":"model = Sequential()\n# First hidden layer , with 1024 nodes and the input shape would be 3072 ,\n# since we have made the images into 32x32 shape.\n# thus 32x32x3 = 3072 since the images have 3 colour channels , RGB.\n# So there are 5539 images each image with 3072 columns or dimentions.\nmodel.add(Dense(1024, input_shape=(3072,), activation = 'sigmoid'))\n# the second hidden layer will have 512 nodes\nmodel.add(Dense(512, activation='sigmoid'))\n# Now since there are 12 classes, thus , the number of output nodes needs to be the number of classes i.e. 12\nmodel.add(Dense(len(lb.classes_), activation=\"softmax\"))","b1dd8c2c":"INIT_LR = 0.01\nEPOCHS = 501\n# compile the model using SGD as our optimizer and categorical\n# cross-entropy loss \nprint(\"[INFO] training network...\")\nopt = SGD(lr=INIT_LR)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])","e7a92d7a":"h = model.fit(trainX, trainY, validation_data=(testX, testY), epochs=EPOCHS, batch_size = 32)","f0f57f05":"print(\"[INFO] evaluating network...\")\npredictions = model.predict(testX, batch_size=32)\nprint(classification_report(testY.argmax(axis=1),predictions.argmax(axis=1), target_names=lb.classes_))\n \n# plot the training loss and accuracy\nN = np.arange(0, EPOCHS)\nplt.figure()\nplt.plot(N, h.history[\"loss\"], label=\"train_loss\")\nplt.plot(N, h.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(N, h.history[\"acc\"], label=\"train_acc\")\nplt.plot(N, h.history[\"val_acc\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy (Simple NN)\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss\/Accuracy\")\nplt.legend()","d1cb482c":"image = cv2.imread('..\/input\/nonsegmentedv2\/Maize\/1.png')\noutput = image.copy()\nimage = cv2.resize(image, (32,32))\nimage = image.astype('float') \/ 255\nimage = image.flatten()\nimage = image.reshape((1, image.shape[0]))\nimage.shape","b7e196f0":"preds = model.predict(image)\ni = preds.argmax(axis=1)[0]\nlabel = lb.classes_[i]\nprint(preds, label)","a68a1141":"class SmallVGGNet:\n    @staticmethod\n    def build(width, height, depth, classes):\n        # initialize the model along with the input shape to be\n        # \"channels last\" and the channels dimension itself\n        model = Sequential()\n        inputShape = (height, width, depth)\n        chanDim = -1\n        \n        # If the we are using 'channels first', then we need to update the input shape\n        # and channels dimentions\n        if K.image_data_format() == 'channels_first':\n            inputShape = (depth, height, width)\n            chanDim = 1\n            \n        model.add(Conv2D(32, (3,3), padding = 'same' ,input_shape = inputShape))\n        model.add(Activation('relu'))\n        ## To normalize the data along the channel dimention, to reduce training time and stabilize\n        ## the network.\n        model.add(BatchNormalization(axis=chanDim))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n        \n        # (CONV => RELU) * 2 => POOL layer set\n        model.add(Conv2D(64, (3, 3), padding=\"same\"))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=chanDim))\n        model.add(Conv2D(64, (3, 3), padding=\"same\"))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=chanDim))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n        \n        # (CONV => RELU) * 3 => POOL layer set\n        model.add(Conv2D(128, (3, 3), padding=\"same\"))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=chanDim))\n        model.add(Conv2D(128, (3, 3), padding=\"same\"))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=chanDim))\n        model.add(Conv2D(128, (3, 3), padding=\"same\"))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=chanDim))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n        \n        # first (and only) set of FC => RELU layers\n        model.add(Flatten())\n        model.add(Dense(512))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization())\n        model.add(Dropout(0.5))\n\n        # softmax classifier\n        model.add(Dense(classes))\n        model.add(Activation(\"softmax\"))\n\n        # return the constructed network architecture\n        return model","11f3f74a":"data = []\nlabels = []\n\nfor path, _, files in os.walk('..\/input\/nonsegmentedv2'):\n    imagesPaths =  sorted([image for image in files])\n    #print(imagesPaths)\n    for imagePath in imagesPaths:\n        image = cv2.imread('..\/input\/nonsegmentedv2\/'+ path.split('\/')[3] + '\/' + imagePath)\n        image = cv2.resize(image, (64,64))\n        data.append(image)\n        labels.append(path.split('\/')[3])\n        \ndata","805f20b8":"data = np.array(data, dtype='float32') \/ 255.0\n# since the pixel intensities lies from 0 to 255, thus we normalized the data to 0 to 1\nlabels = np.array(labels)","82c20dc0":"# partition the data into training and testing splits using 75% of\n# the data for training and the remaining 25% for testing\n(trainX, testX, trainY, testY) = train_test_split(data,labels, test_size=0.25, random_state=42)","b25bfafe":"## Here We are just doing the one-hot encoding for the labels.\nlb = LabelBinarizer()\ntrainY = lb.fit_transform(trainY)\ntestY = lb.transform(testY)","dba95a41":"# construct the image generator for data augmentation\n# construct the image generator for data augmentation\naug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n                        height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n                        horizontal_flip=True, fill_mode=\"nearest\")\n \n\"\"\"\nImage augmentation allows us to construct \u201cadditional\u201d training data from our existing training data by randomly rotating, shifting, shearing, zooming, and flipping.\n\nData augmentation is often a critical step to:\n\n1) Avoiding overfitting\n2) Ensuring your model generalizes well\n\"\"\"\n    \n# initialize our VGG-like Convolutional Neural Network\nmodel = SmallVGGNet.build(width=64, height=64, depth=3,classes=len(lb.classes_))","33417d61":"# initialize our initial learning rate, # of epochs to train for,\n# and batch size\nINIT_LR = 0.01\nEPOCHS = 100\nBS = 32\n# initialize the model and optimizer \nprint(\"[INFO] training network...\")\nopt = SGD(lr=INIT_LR, decay=INIT_LR \/ EPOCHS)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n\n# train the network\nh = model.fit_generator(aug.flow(trainX, trainY, batch_size=BS),\n                                validation_data=(testX, testY), \n                                steps_per_epoch=len(trainX) \/\/ BS,\n                                epochs=EPOCHS)","863f0bf8":"# evaluate the network\nprint(\"[INFO] evaluating network...\")\npredictions = model.predict(testX, batch_size=32)\nprint(classification_report(testY.argmax(axis=1),\n                            predictions.argmax(axis=1), target_names=lb.classes_))\n \n# plot the training loss and accuracy\nN = np.arange(0, EPOCHS)\nplt.figure()\nplt.plot(N, h.history[\"loss\"], label=\"train_loss\")\nplt.plot(N, h.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(N, h.history[\"acc\"], label=\"train_acc\")\nplt.plot(N, h.history[\"val_acc\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy (SmallVGGNet)\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss\/Accuracy\")\nplt.legend()","b2b05a9c":"#### As we can see here , we got a 82% accuracy by using a CNN Model .\n","905f7fd6":"#### As we can see that the model produces the right result for the image , which is maize. But still using a Simple feedforward neural network might not be the ideal solution, thus we are going to use CNN, VGGNet architechture  to be precise.","f79db137":"#### Now since , we have made the dataset and their labels, we need to do one more very common preprocessing step, that is to normalize the data , i.e. make the data points between 0 and 1 to optimize the training process and make the models learn faster.","186a8c7f":"## Here without using any CNN or any complicated model structure , we saw that, the model produces a 50% accuracy, which is descent for this type of model.","deb3d433":"# First making a toy model.\n","5447466c":"#### here we can see that there are 5539 images and 5539 labels."}}