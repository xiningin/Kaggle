{"cell_type":{"ac758afd":"code","0d4ca1a2":"code","8c83af8c":"code","b8131613":"code","2e9f15d5":"code","8cfb058f":"code","398db702":"code","5ceac976":"code","da473666":"code","0dfc4d6f":"code","1a0618b2":"code","074bfdab":"code","6cdaf19e":"code","ac0df9da":"code","53db0003":"code","da72e044":"code","fe1d8f88":"code","70753433":"code","3730a9a3":"code","292a917f":"markdown","8dccf286":"markdown","f12beada":"markdown","315325ec":"markdown","6b193959":"markdown","ed6e33d0":"markdown","7553113e":"markdown","d3e342f0":"markdown","334d6b31":"markdown"},"source":{"ac758afd":"import os\nimport pandas as pd\nfrom kaggle.competitions import nflrush\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport random\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport gc\nimport pickle\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nimport tqdm","0d4ca1a2":"env = nflrush.make_env()\ntrain_df = pd.read_csv('\/kaggle\/input\/nfl-big-data-bowl-2020\/train.csv', low_memory=False)","8c83af8c":"unused_columns = [\"GameId\",\"PlayId\",\"Team\",\"Yards\",\"TimeHandoff\",\"TimeSnap\"]","b8131613":"#\u5c06\u4e0d\u5728unused_columns\u4e2d\u7684\u4e14\u6bcf\u4e2a\u7403\u5458\u4f1a\u6709\u4e0d\u540c\u6570\u503c\u7684\u6570\u636e\u89c6\u4e3aunique_column\nunique_columns = []\nfor c in train_df.columns:\n    if c not in unused_columns+[\"PlayerBirthDate\"] and len(set(train_df[c][:11]))!= 1:\n        unique_columns.append(c)\n        print(c,\" is unique\")\nunique_columns+=[\"BirthY\"]","2e9f15d5":"#\u5224\u65ad\u6570\u636e\u662f\u5426\u6309\u7167playID-Team\u6765\u6392\u5e8f\nok = True\nfor i in range(0,509762,22):\n    p=train_df[\"PlayId\"][i]\n    for j in range(1,22):\n        if(p!=train_df[\"PlayId\"][i+j]):\n            ok=False\n            break\nprint(\"train data is sorted by PlayId.\" if ok else \"train data is not sorted by PlayId.\")\nok = True\nfor i in range(0,509762,11):\n    p=train_df[\"Team\"][i]\n    for j in range(1,11):\n        if(p!=train_df[\"Team\"][i+j]):\n            ok=False\n            break\nprint(\"train data is sorted by Team.\" if ok else \"train data is not sorted by Team.\")","8cfb058f":"#\u5c06\uff08\u6bcf\u4e00\u884c\u7684\u6570\u636e\u90fd\u76f8\u540c\uff09\u975e\u7279\u6b8a\u5217\u5408\u5e76\u8d77\u6765\uff0c\u5e76\u5c06\u7279\u6b8a\u5217\u9010\u4e00\u6dfb\u52a0\u5230\u672b\u5c3e\uff08\u5bf9\u5176\u4e2d\u4e09\u5217\u9700\u8981\u5728\u4e4b\u540e\u4f5c\u7279\u6b8a\u5904\u7406\uff0c\u56e0\u6b64\u4e0d\u5728\u6b64\u5904\u5408\u5e76\uff09\nall_columns = []\nfor c in train_df.columns:\n    if c not in unique_columns + unused_columns+[\"DefensePersonnel\",\"GameClock\",\"PlayerBirthDate\"]:\n        all_columns.append(c)\nall_columns.append(\"DL\")\nall_columns.append(\"LB\")    \nall_columns.append(\"DB\")\nall_columns.append(\"GameHour\")   \nfor c in unique_columns:\n    for i in range(22):\n        all_columns.append(c+str(i))","398db702":"lbl_dict = {}\nfor c in train_df.columns:\n    if c == \"DefensePersonnel\":#\u8bb0\u5f55\u9632\u5b88\u4eba\u5458\u914d\u7f6e\uff08\u6bcf\u4e2a\u4f4d\u7f6e\u4e0a\u7684\u9632\u5b88\u4eba\u5458\u6570\uff09\n        arr = [[int(s[0]) for s in t.split(\", \")] for t in train_df[\"DefensePersonnel\"]]\n        train_df[\"DL\"] = np.array([a[0] for a in arr])\n        train_df[\"LB\"] = np.array([a[1] for a in arr])\n        train_df[\"DB\"] = np.array([a[2] for a in arr])\n    elif c == \"GameClock\":#\u6bd4\u8d5b\u65f6\u95f4\u4ec5\u8bb0\u5f55\u5f53\u524d\u7684\u5c0f\u65f6\uff08\u4e3b\u8981\u7528\u6765\u533a\u5206\u6bd4\u8d5b\u8fdb\u884c\u5728\u4e00\u5929\u5185\u7684\u54ea\u4e00\u4e2a\u65f6\u95f4\u6bb5\uff09\n        arr = [[int(s) for s in t.split(\":\")] for t in train_df[\"GameClock\"]]\n        train_df[\"GameHour\"] = pd.Series([a[0] for a in arr])\n    elif c == \"PlayerBirthDate\":#\u7403\u5458\u751f\u65e5\u4ec5\u8bb0\u5f55\u5176\u51fa\u751f\u5e74\uff08\u4e3b\u8981\u7528\u4e8e\u4ee3\u8868\u7403\u5458\u5e74\u9f84\uff09\n        arr = [[int(s) for s in t.split(\"\/\")] for t in train_df[\"PlayerBirthDate\"]]\n        train_df[\"BirthY\"] = pd.Series([a[2] for a in arr])\n    # elif c == \"PlayerHeight\":\n    #     arr = [float(s.split(\"-\")[0]) * 30.48 + float(s.split(\"-\")[1]) * 2.54\n    #         for s in list(train_df[\"PlayerHeight\"])]\n    #     train_df[\"PlayerHeight\"] = pd.Series(arr)\n    elif train_df[c].dtype=='object' and c not in unused_columns: \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train_df[c].values))\n        lbl_dict[c] = lbl\n        train_df[c] = lbl.transform(list(train_df[c].values))\n","5ceac976":"#\u5220\u53bb\u4e0d\u9700\u8981\u7684\u884c\ndrop_columns = [\"Team\", \"GameClock\", \"DefensePersonnel\", \"TimeHandoff\", \"TimeSnap\", \"PlayerBirthDate\"]\ntrain_df = train_df.drop(drop_columns, axis=1)#\u4e22\u5f03\u5217","da473666":"#\u5c0622\u884c\u6570\u636e\u538b\u7f29\u5230\u4e00\u884c\u5185\ntrain_data=np.zeros((509762\/\/22,len(all_columns)))\nfor i in tqdm.tqdm(range(0,509762,22)):\n    count=0\n    for c in all_columns:\n        if c in train_df:\n            train_data[i\/\/22][count] = train_df[c][i]\n            count+=1\n    for c in unique_columns:\n        for j in range(22):\n            train_data[i\/\/22][count] = train_df[c][i+j]\n            count+=1        ","0dfc4d6f":"#\u53d6\u51fa\u8bad\u7ec3\u7528\u7684\u6807\u7b7e\uff0c\u5e76\u88c5\u597d\u8bad\u7ec3\u6570\u636e\ny_train_ = np.array([train_df[\"Yards\"][i] for i in range(0,509762,22)])\nX_train = pd.DataFrame(data=train_data,columns=all_columns)\ntrain_data_df = pd.DataFrame(data=train_data)\ntrain_data_df.to_csv(\"train_data_cdropped.csv\")","1a0618b2":"data = [0 for i in range(199)]\nfor y in y_train_:#\u5c06\u5176\u8f6c\u6362\u6210\u6982\u7387\u6807\u7b7e\n    data[int(y+99)]+=1\nplt.plot([i-99 for i in range(199)],data)","074bfdab":"X_train.head","6cdaf19e":"try_clf = 1#\u9009\u62e9\u8dd1\u7b56\u7565\u6811\u7b97\u6cd5\uff080\uff09\u8fd8\u662f\u8dd1\u56de\u5f52\u6a21\u578b\u795e\u7ecf\u7f51\u7edc\u7b97\u6cd5\uff081\uff09\u8fd8\u662f\u8dd1\u5206\u7c7b\u6a21\u578b\u795e\u7ecf\u7f51\u7edc\u7b97\u6cd5\uff082\uff09","ac0df9da":"# scaler = preprocessing.StandardScaler()\n# scaler.fit([[y] for y in y_train_])\n# y_train = np.array([y[0] for y in scaler.transform([[y] for y in y_train_])])\n#\u5bf9\u6570\u636e\u8fdb\u884c\u6807\u51c6\u5316\n\nscaler = preprocessing.StandardScaler()\nscaler.fit(y_train_.reshape(-1, 1))#\u83b7\u5f97\u5747\u503c\u548c\u65b9\u5dee\ny_train = scaler.transform(y_train_.reshape(-1, 1)).flatten()\n\nscaler1 = preprocessing.StandardScaler()\nscaler1.fit(X_train)#\u83b7\u5f97\u5747\u503c\u548c\u65b9\u5dee\nX_train = pd.DataFrame(data=scaler1.transform(X_train))","53db0003":"y_valid_pred = np.zeros(X_train.shape[0])\nmodels = []\nfolds = 10\nseed = 222\nif try_clf == 0:\n    kf = KFold(n_splits = folds, shuffle = True, random_state=seed)\n\n\n    for tr_idx, val_idx in kf.split(X_train, y_train):#\u5206\u5272\u6570\u636e\u96c6\u3001\u6821\u9a8c\u96c6\n        tr_x, tr_y = X_train.iloc[tr_idx,:], y_train[tr_idx]\n        vl_x, vl_y = X_train.iloc[val_idx,:], y_train[val_idx]\n\n        print(len(tr_x),len(vl_x))\n        tr_data = lgb.Dataset(tr_x, label=tr_y)\n        vl_data = lgb.Dataset(vl_x, label=vl_y)  \n        clf = lgb.LGBMRegressor(n_estimators=200,learning_rate=0.01)#\u4f7f\u7528\u56de\u5f52\u6a21\u578b\n        clf.fit(tr_x, tr_y,\n            eval_set=[(vl_x, vl_y)],\n            early_stopping_rounds=20,\n            verbose=False)\n        y_valid_pred[val_idx] += clf.predict(vl_x, num_iteration=clf.best_iteration_)\n        models.append(clf)\n\n    gc.collect()\nelif try_clf == 1:\n    from sklearn import neural_network\n    X_train.head\n    X_train[np.isnan(X_train) == True] = 0#\u628a\u6570\u636e\u96c6\u91cc\u9762\u7684NaN\u6570\u636e\u5168\u90e8\u66ff\u6362\u4e3a0\n    kf = KFold(n_splits = folds, shuffle = True, random_state=seed)\n\n    r_y_train = scaler.inverse_transform(y_train)\n    for tr_idx, val_idx in kf.split(X_train, r_y_train):#\u5206\u5272\u6570\u636e\u96c6\u3001\u6821\u9a8c\u96c6\n        tr_x, tr_y = X_train.iloc[tr_idx,:], r_y_train[tr_idx]\n        vl_x, vl_y = X_train.iloc[val_idx,:], r_y_train[val_idx]\n\n        print(len(tr_x),len(vl_x))\n        mlp = neural_network.MLPRegressor(hidden_layer_sizes=(256, 128), activation=\"logistic\",\n                     solver='adam', alpha=0.0001,\n                     batch_size='auto', learning_rate=\"adaptive\",\n                     learning_rate_init=0.001,\n                     power_t=0.5, max_iter=200,tol=1e-4)\n        mlp.fit(tr_x, tr_y)\n        \n        y_valid_pred[val_idx] += scaler.transform(mlp.predict(vl_x).reshape(-1, 1)).flatten()\n        \n        y_p = mlp.predict(vl_x)\n        \n        #\u8ba1\u7b97\u8bc4\u4f30\u51fd\u6570\uff08\u5c40\u90e8\u7ed3\u679c\uff09\n        y_pred = np.zeros((len(val_idx),199))\n        y_ans = np.zeros((len(val_idx),199))\n        for i,p in enumerate(np.round(y_p)):\n            p+=99\n            for j in range(199):\n                if j>=p+10:\n                    y_pred[i][j]=1.0\n                elif j>=p-10:\n                    y_pred[i][j]=(j+10-p)*0.05\n\n        for i,p in enumerate(r_y_train[val_idx]):\n            p+=99\n            for j in range(199):\n                if j>=p:\n                    y_ans[i][j]=1.0\n        print(\"validation score:\",np.sum(np.power(y_pred-y_ans,2))\/(199*(len(val_idx))))\n        #\u8ba1\u7b97\u8bc4\u4f30\u51fd\u6570\uff08\u5c40\u90e8\u7ed3\u679c\uff09\n        \n        \n        #for i in range(100):\n        #    print(y_p[i], r_y_train[i])\n        mlp_loss_curve = mlp.loss_curve_\n        plt.plot(range(1, len(mlp_loss_curve)+1), mlp_loss_curve)\n        plt.show()\n        models.append(mlp)\n\n    gc.collect()\nelif try_clf == 2:\n    from sklearn import neural_network\n    X_train.head\n    X_train[np.isnan(X_train) == True] = 0#\u628a\u6570\u636e\u96c6\u91cc\u9762\u7684NaN\u6570\u636e\u5168\u90e8\u66ff\u6362\u4e3a0\n    kf = KFold(n_splits = folds, shuffle = True, random_state=seed)\n\n    r_y_train = scaler.inverse_transform(y_train)\n    tr_y_1 = np.zeros((len(r_y_train), 199))\n    for i, d in enumerate(r_y_train):\n        for j in range(199):\n            if j == d:\n                tr_y_1[i][j] = 1\n    \n    print(\"begin training...\")\n    for tr_idx, val_idx in kf.split(X_train, r_y_train):#\u5206\u5272\u6570\u636e\u96c6\u3001\u6821\u9a8c\u96c6\n        tr_x, tr_y = X_train.iloc[tr_idx,:], r_y_train[tr_idx].astype(\"int\")\n        vl_x = X_train.iloc[val_idx,:]\n        vl_y = tr_y_1[val_idx]\n            \n        print(len(tr_x),len(vl_x))\n        mlp = neural_network.MLPClassifier(hidden_layer_sizes=(256, 128), activation=\"logistic\",\n                     solver='adam', alpha=0.0001,\n                     batch_size='auto', learning_rate=\"adaptive\",\n                     learning_rate_init=0.001,\n                     power_t=0.5, max_iter=200,tol=1e-4)\n        mlp.fit(tr_x, tr_y)\n        y_pred = mlp.predict_proba(vl_x)\n        for i in range(50):\n            print(\"pred:\", y_pred[i,:])\n            print(\"truth:\", vl_y[i,:])\n        print(\"validation score:\",np.sum(np.power(y_pred-vl_y,2))\/(199*(len(val_idx))))#\u8bc4\u4f30\uff08\u5c40\u90e8\uff09\n        mlp_loss_curve = mlp.loss_curve_\n        plt.plot(range(1, len(mlp_loss_curve)+1), mlp_loss_curve)\n        plt.show()\n        \n    gc.collect()","da72e044":"r_y_valid_pred = scaler.inverse_transform(y_valid_pred)\nr_y_train = scaler.inverse_transform(y_train)\nfor i in range(100):\n    print(r_y_valid_pred[i], r_y_train[i])","fe1d8f88":"y_pred = np.zeros((509762\/\/22,199))\ny_ans = np.zeros((509762\/\/22,199))\nfor i,p in enumerate(np.round(scaler.inverse_transform(y_valid_pred))):\n    p+=99\n    for j in range(199):\n        if j>=p+10:\n            y_pred[i][j]=1.0\n        elif j>=p-10:\n            y_pred[i][j]=(j+10-p)*0.05\n\nfor i,p in enumerate(scaler.inverse_transform(y_train)):\n    p+=99\n    for j in range(199):\n        if j>=p:\n            y_ans[i][j]=1.0\nprint(\"validation score:\",np.sum(np.power(y_pred-y_ans,2))\/(199*(509762\/\/22)))","70753433":"y_pred_demo = np.zeros(199)\ny_pred_sigmoid_demo = np.zeros(199)\np = 5 + 99\n\nfor j in range(199):\n    y_pred_sigmoid_demo[j] = 1 \/ (1 + np.exp((p-j)\/3))\nplt.plot(range(199), y_pred_sigmoid_demo)","3730a9a3":"index = 0\nfor (test_df, sample_prediction_df) in tqdm.tqdm(env.iter_test()):\n    for c in test_df.columns:\n        if c == \"DefensePersonnel\":\n            try:\n                arr = [[int(s[0]) for s in t.split(\", \")] for t in test_df[\"DefensePersonnel\"]]\n                test_df[\"DL\"] = [a[0] for a in arr]\n                test_df[\"LB\"] = [a[1] for a in arr]\n                test_df[\"DB\"] = [a[2] for a in arr]\n            except:\n                test_df[\"DL\"] = [np.nan for i in range(22)]\n                test_df[\"LB\"] = [np.nan for i in range(22)]\n                test_df[\"DB\"] = [np.nan for i in range(22)]\n        elif c == \"GameClock\":\n            try:\n                arr = [[int(s) for s in t.split(\":\")] for t in test_df[\"GameClock\"]]\n                test_df[\"GameHour\"] = pd.Series([a[0] for a in arr])\n            except:\n                test_df[\"GameHour\"] = [np.nan for i in range(22)]\n        elif c == \"PlayerBirthDate\":\n            try:\n                arr = [[int(s) for s in t.split(\"\/\")] for t in test_df[\"PlayerBirthDate\"]]\n                test_df[\"BirthY\"] = pd.Series([a[2] for a in arr])\n            except:\n                test_df[\"BirthY\"] = [np.nan for i in range(22)]\n        # elif c == \"PlayerHeight\":\n        #     try:\n        #         arr = [float(s.split(\"-\")[0]) * 30.48 + float(s.split(\"-\")[1]) * 2.54\n        #             for s in list(test_df[\"PlayerHeight\"])]\n        #         test_df[\"PlayerHeight\"] = pd.Series(arr)\n        #     except:\n        #         test_df[\"PlayerHeight\"] = [np.nan for i in range(22)]\n        elif c in lbl_dict and test_df[c].dtype=='object'and c not in unused_columns\\\n            and not pd.isnull(test_df[c]).any():\n            try:\n                test_df[c] = lbl_dict[c].transform(list(test_df[c].values))\n            except:\n                test_df[c] = [np.nan for i in range(22)]\n    #\u5220\u53bb\u4e0d\u9700\u8981\u7684\u884c\n    #drop_columns = [\"GameClock\", \"DefensePersonnel\", \"TimeHandoff\", \"TimeSnap\", \"PlayerBirthDate\"]\n    #test_df.drop(drop_columns, axis=1)#\u4e22\u5f03\u5217\n    count=0\n    test_data = np.zeros((1,len(all_columns)))\n\n    for c in all_columns:\n        if c in test_df:\n            try:\n                test_data[0][count] = test_df[c][index]\n            except:\n                if try_clf:\n                    test_data[0][count] = np.nan\n                else:\n                    test_data[0][count] = 0\n            count+=1\n    for c in unique_columns:\n        for j in range(22):\n            try:\n                test_data[0][count] = test_df[c][index + j]\n            except:\n                if try_clf:\n                    test_data[0][count] = np.nan\n                else:\n                    test_data[0][count] = 0\n            count+=1\n    #test_data[np.isnan(test_data) == True] = 0\n    if try_clf == 1:\n        test_data = scaler1.transform(test_data)\n    elif try_clf == 2:\n        test_data = scaler1.transform(test_data)\n    y_pred = np.zeros(199)\n    \n    #\u4e24\u4e2a\u6a21\u578b\u7684\u8f93\u51fa\u5f62\u5f0f\u4e0d\u540c\uff0c\u56e0\u6b64\u9700\u8981\u4e0d\u540c\u7684\u5904\u7406\u65b9\u5f0f\n    if try_clf == 0 or try_clf == 1:\n        if try_clf == 0:\n            y_pred_p = np.sum(np.round(scaler.inverse_transform(\n                [model.predict(test_data)[0] for model in models])))\/folds\n        elif try_clf == 1:\n            y_pred_p = np.sum(np.round([model.predict(test_data)[0] for model in models]))\/folds\n        y_pred_p += 99\n        for j in range(199):\n            if j>=y_pred_p+10:\n                y_pred[j]=1.0\n            elif j>=y_pred_p-10:\n                y_pred[j]=(j+10-y_pred_p)*0.05\n    env.predict(pd.DataFrame(data=[y_pred],columns=sample_prediction_df.columns))\n    index += 22\nenv.write_submission_file()","292a917f":"## import\nLoad the necessary libraries.","8dccf286":"## train\nI used LGBMRegressor.\nI wanted to use multi-class classification, but the number of datasets was small and it was difficult to split them including all labels.","f12beada":"Since the training data was sorted, preprocessing can be done easily.","315325ec":"## evaluation\nContinuous Ranked Probability Score (CRPS) is derived based on the predicted scalar value.\nThe CRPS is computed as follows:\n$$\nC=\\frac{1}{199N}\\sum_{m=1}^N\\sum_{n=-99}^{99}(P(y\\geq n)-H(n-Y_m))^2\n$$\n$H(x)=1$ if $x\\geq 0$ else $0$","6b193959":"Since the variance is small, I standardized the objective variable.","ed6e33d0":"The organizers seemed to expect to predict one by one, so I did. \nHowever, it seems that it is likely to be faster to predict at once after all the evaluation data is acquired by dummy input.\n\n\nThis model is a simple one that has not been tuned, so I think we can still expect a better score.\nPlease let me know if you have any opinions or advice.","7553113e":"## train data\nThe shape of train data is 509762 \u00d7 49.\nBut, since one set consists of 22 lines, the actual number of data is 23171.\nI converted it to a format that is easy to use.","d3e342f0":"## make submission\n\nWhen there is a label that does not exist in the training data, it is handled as nan.\nIf you can check the error one by one and complement it, you will get better score.","334d6b31":"## Introduction\nI will introduce a simple method using lightGBM as a starter."}}