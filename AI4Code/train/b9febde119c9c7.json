{"cell_type":{"eec91a4b":"code","21f77c19":"code","6afae064":"code","01641566":"code","03f0a775":"code","70907ebd":"code","d2217bbf":"code","c3766d13":"code","deb30928":"code","16728510":"code","9bc5c38b":"code","2bc0aa43":"code","e2ab0981":"code","c8c4cb41":"code","bfd89234":"code","4b612058":"code","d5b5bf99":"code","3b928bef":"code","2b3e0afa":"code","03a9fa05":"code","f9a9ef01":"code","0e9ee4d1":"code","147c4176":"code","d1fd0f0c":"code","cb3fff9f":"code","bce5d783":"code","f50eb1c9":"code","1cd4cea6":"code","f083d37a":"code","7d7abde5":"code","6d2737e9":"code","d6c5badf":"code","57b485f4":"code","8d4a8cf6":"code","4e3b8b62":"code","52bd6827":"code","136cd1fb":"code","82cd12bf":"code","87033635":"code","7c303776":"markdown","4a6d6508":"markdown","65a955c7":"markdown","4e2b1a9d":"markdown","73fcebf0":"markdown","f5acec92":"markdown","9987b2d5":"markdown","2fd461e6":"markdown","781626c9":"markdown","95dcb253":"markdown","bd97a39b":"markdown","efa591b5":"markdown","dfc96918":"markdown","737d1378":"markdown","9aad1463":"markdown","60fd0996":"markdown","fbba1ec1":"markdown"},"source":{"eec91a4b":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom random import randint\nfrom sklearn import preprocessing\nimport tensorflow as tf \nfrom tensorflow import keras\nimport itertools\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split","21f77c19":"df = pd.read_csv(r'..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')","6afae064":"#starting an EDA\ndf.describe()\n","01641566":"#looking only at the persons who does not had a stroke\nnot_stroke = df.loc[(df['stroke']==0)]\nnot_stroke.describe()","03f0a775":"#looking at the age of the persons \nplt.figure(figsize = (12,6))\nplt.title('Age Non Stroke')\nplt.hist(not_stroke['age'],bins = range(0,100,10))\nplt.show()","70907ebd":"#looking at bmis \n\nbmis = not_stroke['bmi']\nplt.figure(figsize=(12,6))\nplt.title('BMIs of non Stroke')\nplt.hist(bmis,bins = range(10,int(1+ max(list(bmis))),5))\nplt.show()","d2217bbf":"glucose = not_stroke['avg_glucose_level'] \nplt.figure(figsize=(12,6))\nplt.title('Avg Glucose Level Non Stroke')\nplt.hist(glucose)\nplt.show()","c3766d13":"plt.title('Gender Non Stroke')\nnot_stroke.gender.value_counts().plot(kind = 'bar')","deb30928":"plt.title('Hypertension Non Stroke')\nnot_stroke.hypertension.value_counts().plot(kind = 'bar')","16728510":"plt.title('Heart disease Non Stroke')\nnot_stroke.heart_disease.value_counts().plot(kind = 'bar')","9bc5c38b":"plt.title('Ever Married Non stroke')\nnot_stroke.ever_married.value_counts().plot(kind = 'bar')","2bc0aa43":"plt.title('Work type Non Stroke')\nnot_stroke.work_type.value_counts().plot(kind = 'bar')","e2ab0981":"plt.title('Residence type Non Stroke')\nnot_stroke.Residence_type.value_counts().plot(kind = 'bar')","c8c4cb41":"stroke = df.loc[(df['stroke']==1)]\nstroke.describe()","bfd89234":"age = stroke['age']\nplt.figure(figsize =(12,6))\nplt.title(\"Age of people with stroke\")\nplt.hist(age,bins = range(0,100,10) )\n# removing outliers \n","4b612058":"bmis = stroke['bmi']\nplt.figure(figsize=(12,6))\nplt.title('BMIs of Stroke')\nplt.hist(bmis,bins = range(10,int(1+ max(list(bmis))),5))\nplt.show()","d5b5bf99":"glucose = stroke['avg_glucose_level'] \nplt.figure(figsize=(12,6))\nplt.title('Avg Glucose Level Stroke')\nplt.hist(glucose)\nplt.show()","3b928bef":"plt.title('Gender Stroke')\nstroke.gender.value_counts().plot(kind = 'bar')","2b3e0afa":"plt.title('Hypertension Stroke')\nstroke.hypertension.value_counts().plot(kind = 'bar')","03a9fa05":"plt.title('Heart disease Stroke')\nstroke.heart_disease.value_counts().plot(kind = 'bar')","f9a9ef01":"plt.title('Ever Married Stroke')\nstroke.ever_married.value_counts().plot(kind = 'bar')","0e9ee4d1":"plt.title('Work type Stroke')\nstroke.work_type.value_counts().plot(kind = 'bar')","147c4176":"plt.title('Residence type Stroke')\nstroke.Residence_type.value_counts().plot(kind = 'bar')","d1fd0f0c":"df.pop('id')\ndef arrume(df,tipo):\n    unicos = np.unique(df[tipo])\n    df[tipo] = df[tipo].map(dict(zip(unicos,range(len(unicos)))))\n\n\n\narrume(df,'work_type')\narrume(df,'gender')\narrume(df,'smoking_status')\narrume(df,'Residence_type')\narrume(df,'ever_married')\n","cb3fff9f":"print (df.isna().any(axis=0))","bce5d783":"df['bmi']= df['bmi'].fillna(np.mean(df['bmi']))","f50eb1c9":"q_low = df[\"age\"].quantile(0.01)\nq_hi  = df[\"age\"].quantile(0.99)\n\ndf = df[(df[\"age\"] < q_hi) & (df[\"age\"] > q_low)]","1cd4cea6":"#see data if it is balanced \ntotal = len(df['stroke'])\nis_stroke = list(df['stroke']).count(1)\nprint(f'Number of negative cases {total-is_stroke}')\nprint(f'Number of positive cases {is_stroke}')\n#hint is not balanced ","f083d37a":"y = df.pop('stroke')\nx = df.values\ny =np.array(y)\n\n#undersampling with random data \nx_under = []\ny_under = []\n\nwhile len(y_under)<240:\n    i = randint(0,len(x)-1)\n    if y[i]==0:\n        x_under.append(x[i])\n        y_under.append(0)\nfor i in range(len(y)):\n    if y[i]==1:\n        x_under.append(x[i])\n        y_under.append(1)\nprint(f'Number of strokes: {y_under.count(1)}\\nNumber of non-Strokes : {y_under.count(0)}')\nx_under,x_test_u,y_under,y_test_u = train_test_split(x_under,y_under,test_size=0.1)\nx_over,x_test_o,y_over,y_test_o = train_test_split(x,y,test_size=0.3)    ","7d7abde5":"#doing oversampling\noversample = RandomOverSampler(sampling_strategy='minority')\nx_over, y_over = oversample.fit_resample(x_over, y_over)\n\n\ncounter=(np.count_nonzero(y_over))\n\nprint(f'Number of strokes : {counter}\\nNumber of non-strokes : {len(y_over)-counter}')","6d2737e9":"def plot_confusion_matrix(cm, classes,\n                        normalize=False,\n                        title='Confusion matrix',\n                        cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","d6c5badf":"model = keras.Sequential([\n    keras.layers.Dense(64,input_shape=(10,),activation='relu'),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dense(64,activation='relu'),\n    keras.layers.Dense(2, activation='softmax')\n])\nmodel.compile(optimizer='Adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\nhistorico = model.fit(np.array(x_under),np.array(y_under),epochs = 300,validation_split = 0.1,verbose = 0)\n","57b485f4":"val=historico.history\nplt.figure(0)\nplt.plot(np.arange(1,len(val['accuracy'])+1),val['accuracy'],label='acc')\nplt.plot(np.arange(1,len(val['val_accuracy'])+1),val['val_accuracy'],label='val_acc')\nplt.legend()\nplt.show()\nplt.figure(1)\nplt.plot(np.arange(1,len(val['val_loss'])+1),val['val_loss'],label='val_loss')\nplt.plot(np.arange(1,len(val['val_loss'])+1),val['loss'],label='loss')\nplt.legend()\nplt.show()","8d4a8cf6":"def arrume2(x):\n    x = np.array(x)\n    return x\n    ","4e3b8b62":"x_test_u = arrume2(x_test_u)\nx_test_o = arrume2(x_test_o)\ncomp = model.predict(x_test_u)\ncomp = [np.argmax(l) for l in comp]\ncm = confusion_matrix(y_true= y_test_u, y_pred=comp)\nplot_confusion_matrix(cm=cm, classes = ['Non Stroke','Stroke'])\nplt.plot()","52bd6827":"#retraining the model with the oversample\nhistorico = model.fit(np.array(x_over),np.array(y_over),epochs=1000,validation_split = 0.3,verbose =0)\nval=historico.history\nplt.figure(0)\nplt.plot(np.arange(1,len(val['accuracy'])+1),val['accuracy'],label='acc')\nplt.plot(np.arange(1,len(val['val_accuracy'])+1),val['val_accuracy'],label='val_acc')\nplt.legend()\nplt.show()\nplt.figure(1)\nplt.plot(np.arange(1,len(val['val_loss'])+1),val['val_loss'],label='val_loss')\nplt.plot(np.arange(1,len(val['val_loss'])+1),val['loss'],label='loss')\nplt.legend()\nplt.show()","136cd1fb":"comp = model.predict(np.array(x_test_o))\ncomp = [np.argmax(l) for l in comp]\ncm = confusion_matrix(y_true=y_test_o, y_pred=comp)\nplot_confusion_matrix(cm=cm, classes = ['Non Stroke','Stroke'])\nplt.plot()","82cd12bf":"x_100 = []\ny_100 = [] \nfor i in range(len(y)):\n    if y[i]==1:\n        y_100.append(1)\n        x_100.append(x[i])\n    if len(y_100)==100:\n        break\nwhile len(y_100)<200:\n    i = randint(0,len(x)-1)\n    if y[i]==0:\n        y_100.append(0)\n        x_100.append(x[i])\n\nx_100 = np.array(x_100)\ny_100 = np.array(y_100)","87033635":"comp = model.predict(np.array(x_100))\ncomp = [np.argmax(l) for l in comp]\ncm = confusion_matrix(y_true=y_100, y_pred=comp)\nplot_confusion_matrix(cm=cm, classes = ['Non Stroke','Stroke'])\nplt.plot()","7c303776":"# What are the objectives of the work \n* Build a neural net using tensorflow\n* Get an accuracy bigger than 90 % \n* Can we do it ? ","4a6d6508":"# Is the Data balanced ? \n - A hint. It's not. \n - No lets see how the data is organized.\n - How many of the data is cases of stroke ? How many are not ? ","65a955c7":"Above are the results of the loss and accuracy of the training process of the model. ","4e2b1a9d":"# NOW FINALLY A NEURAL NET !!!!!!!","73fcebf0":"Lets test the model with 100 cases of stroke and 100 cases of not stroke let's see what is the f1 score. ","f5acec92":"* Now the same model but being trained with oversampled data ","9987b2d5":"# Removing Outliers \n We have outliers in the dataframe we need to remove so we can get better results","2fd461e6":"Now looking at the cases of Stroke","781626c9":"# Why can't we just use the data as it is ? \nWe have a problem. The data is not balanced. But what this means ? It means we have a lot of negatives and few positives cases of stroke. If we use the way it is, our model would predict, much likely, that all cases are not strokes and thats not what we want. \nBut OMG what we can do to solve this problem ? The answer is undersampling or oversampling. But what the hell is undersampling and oversampling ?\n* Undersampling : is reducing the amount of data with random cases of not stroke(negatives), in our case,  and all cases of strokes (positives) this way we have a balanced dataset. \n* Oversampling : Is raising the amount of cases of stroke (positives) and keeping the number of negatives there are different ways you can do this.  \n","95dcb253":"# Organizing the data \n* What categories we have in the data ? \n* What categorical data do we have ?\n* How we are going to deal with categorical data ? \n# Dealing with categorical data \n* We are going to use label encoding \n* What the hell is label encoding ? \nImagine that you have the simple categorical data that are divided in 3 categories we are going to change each category with an integer. Simple.\n* Why are we using label encoding ? \n One of the reasons is the simplicity of it. \n* Do we have other alternatives ? \n Yes we do. Such as Onehot encoding and others but they are more difficult to apply feel free to make any changes on the notebook \n \n \n","bd97a39b":"Below we can see the model in tensorflow that we are using.\nWe create the sequential model and compile it.","efa591b5":"# Do we have NaN ? \n* Yes we do !!!\n* Where? \n* How to fix this without losing data ? \n\n ","dfc96918":"\nThe function above was taken out of the site: https:\/\/deeplizard.com\/learn\/video\/km7pxKy4UHU","737d1378":"# Let's get the useful libraries \nAbove are the libraries used in this project to predict the cases of stroke","9aad1463":"# Fixing our problems with Nan \n* We only have Nan in the BMI part of the data. \n* We are going to change the Nan with the average of the BMIs. \n","60fd0996":"We are undersampling using random numbers so we can get random data from the negatives (non strokes) ","fbba1ec1":"# Results\nThe F-score of the model gets compromised given the low number of positives in the test set so the number of negatives is very high. \nThe mistake i was making that let the f1 Score be to high and was showed by Vmaia is that i waas oversampling the raw data and testing it with the raw data, but given the nature of the problem the f1 score is low. if we do in an balanced dataset the prediction is better but there are still a lot of room for improvement please let me know what i ca n imporve\n\nWe can see that the oversampled data have a better f1 score but i am a little bit suspicious about it because he is to high if you can see some mistake please show me  \n\n**Thank you for your attention and if you liked the notebook give it an up, also put your ideas and if i made it some mistake say it on the comments **"}}