{"cell_type":{"ee57b586":"code","d5157b20":"code","79e27f21":"code","fdf1096b":"code","e9893707":"code","4823b655":"code","7430559b":"code","13559706":"code","8b8fb9c3":"code","abdcf67a":"code","419129ce":"code","aeb14e9f":"code","9c1aa12b":"code","27f3c4d0":"code","f3f8be73":"code","b4da51dd":"code","97e6ea4f":"code","45466c16":"code","f0b2bee4":"code","cf26e1b8":"markdown","33ec6e4a":"markdown","1daeca37":"markdown"},"source":{"ee57b586":"# Load packages\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.base import TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.utils import shuffle\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import classification_report \nfrom nltk.corpus import stopwords\nimport statistics\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.stem.porter import PorterStemmer\nimport string\nimport spacy\nfrom spacy.lang.en import English\nimport numpy as np\nimport scipy.stats as stats\nimport math\nspacy.load('en')\nparser = English()","d5157b20":"# load dataset\nbbc_df = pd.read_csv('..\/input\/bbc-text.csv')","79e27f21":"bbc_df.head(10)","fdf1096b":"def wc_count(text):\n    if isinstance(text,str):\n        return len(text.split())\n    else:\n        return 0\nbbc_df['word_count'] = bbc_df['text'].apply(wc_count)","e9893707":"bbc_df.columns","4823b655":"print('Size of corpus',len(bbc_df['word_count']))\nprint('max word count',max(bbc_df['word_count']))\nprint('min word count',min(bbc_df['word_count']))","7430559b":"# Category Distribution\nbbc_df.groupby('category').count()","13559706":"# Plotting the normal distribution\nstd = statistics.stdev(bbc_df['word_count'])\nmean = statistics.mean(bbc_df['word_count'])\nvariance = statistics.variance(bbc_df['word_count'])\nx_min = min(bbc_df['word_count'])\nx_max = max(bbc_df['word_count'])\nx= bbc_df['word_count']\n\nmu = mean\nvariance = 1\nsigma = math.sqrt(variance)\nx = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\nplt.plot(x, stats.norm.pdf(x, mu, sigma))\nplt.show()","8b8fb9c3":"bbc_df.sort_values('word_count',inplace = True)","abdcf67a":"bbc_df['word_count'].tail(15)","419129ce":"bbc_df['word_count'].head(15)","aeb14e9f":"bbc_df = bbc_df.iloc[:-15]","9c1aa12b":"# Plotting the Normal Distribution after removing the outliers\n\nstd = statistics.stdev(bbc_df['word_count'])\nmean = statistics.mean(bbc_df['word_count'])\nvariance = statistics.variance(bbc_df['word_count'])\nx_min = min(bbc_df['word_count'])\nx_max = max(bbc_df['word_count'])\nx= bbc_df['word_count']\n\nmu = mean\nvariance = 1\nsigma = math.sqrt(variance)\nx = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\nplt.plot(x, stats.norm.pdf(x, mu, sigma))\nplt.show()\n","27f3c4d0":"# Round to the nearest 100th value\nimport math\ndef roundup(x):\n    return int(math.ceil(x \/ 100.0)) * 100\n\nbbc_df['word_count'] = bbc_df['word_count'].apply(roundup)\n\n# plotting the histogram\nscore_india = bbc_df['word_count']\nlegend = 'Word Count'\nplt.hist(score_india, color='green')\nplt.xlabel(\"Article Size\")\nplt.ylabel(\"Frequency\")\nplt.legend(legend)\nplt.xticks(range(0, 1500, 200))\nplt.yticks(range(0, 1000, 100))\nplt.title('Word count for articles in BBC')\nplt.figure(figsize=(3,7))\nplt.show()","f3f8be73":"bbc_df.shape","b4da51dd":"bbc_df.info()","97e6ea4f":"bbc_df['category'].unique()","45466c16":"bbc_df['category'].value_counts()","f0b2bee4":"sns.countplot(bbc_df['category'])","cf26e1b8":"# Text Classification - OneClass Classificaiton","33ec6e4a":"The one-class algorithms are based on recognition since their aim is to recognize data from a particular class, and reject data from all other classes. This is accomplished by creating a boundary that encompasses all the data belonging to the target class within itself, so when a new sample arrives the algorithm only has to check whether it lies within the boundary or outside and accordingly classify the sample as belonging to the target class or the outlier.","1daeca37":"Things we are going to discuss:\n\n1. Data Preparation \n2. Cleaning and Tokenization\n3. Feature Extraction\n4. Train One-class classificaiton model\n5. Predict one-class on test data"}}