{"cell_type":{"63e7ecb5":"code","a12babf6":"code","b9be9db2":"code","110de290":"code","9e38a8bd":"code","ffe19a61":"code","947a8e46":"code","22a13238":"code","af958a9d":"code","14efc651":"code","b5801547":"code","16fbfb36":"code","82774cc5":"code","be8427c1":"code","110a7df8":"code","3d88e9fa":"code","bc7094bf":"code","c877c5f7":"code","aa1721bb":"code","e9a324a1":"code","af6ce610":"code","8eeeddf1":"code","7c899389":"code","c4dc1f59":"code","a2894116":"code","7bc8d6fa":"code","ea39e868":"code","a7d0b852":"code","34b6858f":"code","e206bea5":"code","0ef278ad":"code","f430e50d":"code","f98b9373":"code","60ec7e57":"code","b5880334":"code","ab3f2a1a":"code","872fc30d":"code","36d929f7":"code","2d5559d7":"code","c1044bbc":"code","3281f6ae":"code","7db88ecb":"code","ad8b3119":"code","0af558d0":"code","a9b7b833":"code","2a0b05e1":"code","9302e9cc":"code","93454d08":"code","0e3f4bd6":"code","69513eb2":"code","fdfecbeb":"markdown","858a5674":"markdown","2a7b52fa":"markdown","70610d77":"markdown","71d441fe":"markdown","f4a1a812":"markdown","a352bed9":"markdown","def2a1f4":"markdown","24de523b":"markdown","00fef322":"markdown","cd89f47d":"markdown","fbbd467b":"markdown","e050102c":"markdown","c2857ced":"markdown","c565d941":"markdown","1963cc9f":"markdown","727dff2e":"markdown","56b62361":"markdown","15e71eda":"markdown","e78cb47d":"markdown","d8f87da4":"markdown","5c0dc99a":"markdown","d07d0a5a":"markdown","31e09481":"markdown","0bb932fd":"markdown","530e1a3b":"markdown"},"source":{"63e7ecb5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    #print(dirname)\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a12babf6":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","b9be9db2":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","110de290":"# women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\n# rate_women = sum(women)\/len(women)\n\n# print(\"% of women who survived:\", rate_women)","9e38a8bd":"# men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\n# rate_men = sum(men)\/len(men)\n\n# print(\"% of men who survived:\", rate_men)","ffe19a61":"# from sklearn.ensemble import RandomForestClassifier\n\n# y = train_data[\"Survived\"]\n\n# features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n# X = pd.get_dummies(train_data[features])\n# X_test = pd.get_dummies(test_data[features])\n\n# model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n# model.fit(X, y)\n# predictions = model.predict(X_test)\n\n# output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n# output.to_csv('my_submission.csv', index=False)\n# print(\"Your submission was successfully saved!\")","947a8e46":"from sklearn.model_selection import train_test_split","22a13238":"train_data.shape","af958a9d":"train_data.isnull().sum()","14efc651":"test_data.isnull().sum()","b5801547":"# out of 891 total rows 687 values are NaN for Cabin column\n\n# PassengerId, Name, Ticket, Cabin intutively seem not very much related to the survival\n\ndropFeatures = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked']\n\ntrain_data_drop = train_data.drop(columns = dropFeatures)\n\n# Same for test_data\ntest_data_drop = test_data.drop(columns = dropFeatures)","16fbfb36":"# Shape of our training data after dropping columns\n# PassengerId, Name, Ticket, Cabin\nprint(f\"train_data_drop shape: {train_data_drop.shape}\")\nprint(f\"test_data_drop shape: {test_data_drop.shape}\")","82774cc5":"# # Set mean where ever the value == NaN in Age Column\n# # For now we have NaNs only in Age Column\n# train_data_drop['Age'].fillna(train_data['Age'].mean(), inplace=True)\n\n# # Same for test_data\n# test_data_drop['Age'].fillna(test_data['Age'].mean(), inplace=True)\n# test_data_drop['Fare'].fillna(test_data['Fare'].mean(), inplace=True)","be8427c1":"# # Set mode where ever the value == NaN in Age Column\n# # For now we have NaNs in Age Column and Embarked Column\n# train_data_drop['Age'].fillna(train_data['Age'].mode()[0], inplace=True)\n# train_data_drop['Embarked'].fillna(train_data['Embarked'].mode()[0], inplace=True)\n\n# # Same for test_data\n# test_data_drop['Age'].fillna(test_data['Age'].mode()[0], inplace=True)\n# test_data_drop['Fare'].fillna(test_data['Fare'].mode()[0], inplace=True)","110a7df8":"# Mean imputation in the 'Age' field in the train_data_after_drop\ntrain_data_drop['Age'].fillna(train_data['Age'].mean(), inplace=True)\n\n# Mean imputation in the 'Age' field in the test_data_after_drop\ntest_data_drop['Age'].fillna(test_data['Age'].mean(), inplace=True)\n\n# Mode imputation in the 'Fare' field in the test_data_after_drop\ntest_data_drop['Fare'].fillna(test_data['Fare'].mode()[0], inplace=True)","3d88e9fa":"# from sklearn.preprocessing import MinMaxScaler\n# scaler = MinMaxScaler()\n\n# # For train data\n# train_data_drop[['Age','SibSp', 'Parch', 'Fare']] = scaler.fit_transform(train_data_drop[['Age','SibSp', 'Parch', 'Fare']])\n\n# # For test data\n# test_data_drop[['Age','SibSp', 'Parch', 'Fare']] = scaler.fit_transform(test_data_drop[['Age','SibSp', 'Parch', 'Fare']])","bc7094bf":"# from sklearn.preprocessing import StandardScaler\n# scaler = StandardScaler()\n\n# # For train data\n# train_data_drop[['Age','SibSp', 'Parch', 'Fare']] = scaler.fit_transform(train_data_drop[['Age','SibSp', 'Parch', 'Fare']])\n\n# # For test data\n# test_data_drop[['Age','SibSp', 'Parch', 'Fare']] = scaler.fit_transform(test_data_drop[['Age','SibSp', 'Parch', 'Fare']])","c877c5f7":"#train_data_drop.describe()","aa1721bb":"#test_data_drop.describe()","e9a324a1":"# Code to check if NaNs exist in any Column\ntrain_data_drop.isnull().sum()","af6ce610":"# Code to check if NaNs exist in any Column\ntest_data_drop.isnull().sum()","8eeeddf1":"# Splits the training data into train\/validation by 70-30\nX_train, X_validation, y_train, y_validation = train_test_split(train_data_drop.drop(columns = ['Survived']), \n                                                    train_data_drop['Survived'], \n                                                    stratify=train_data_drop['Survived'], \n                                                    test_size = 0.3)\n\n# Code below converts the categorical columns 'Sex' and 'Embarked' to one hot encoding\nX_train = pd.get_dummies(X_train)\nX_validation = pd.get_dummies(X_validation)\n\n# Similar for test data set\nX_test = pd.get_dummies(test_data_drop)","7c899389":"# 70% of 891 = 623, of the data to be used for training\nprint(f\"X_train Shape: {X_train.shape}\")\n\n# 30% of 891 = 268, of the data to be used for validation\nprint(f\"X_validation Shape: {X_validation.shape}\")\n\nprint(f\"X_test Shape: {X_test.shape}\")","c4dc1f59":"import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax","a2894116":"from sklearn.ensemble import RandomForestClassifier\n\n# instantiate the Random Forest Classifier\nrfc = RandomForestClassifier(n_estimators=100, max_depth=7, random_state=1)\n\n# fit(train) the model with the training dataset\nrfc.fit(X_train, y_train)\n\n# make predictions on the validation dataset\ny_pred = rfc.predict(X_validation)","7bc8d6fa":"# Import accuracy_score and classification report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\n# Compute validation dataset accuracy  \nacc = accuracy_score(y_pred, y_validation)\nprint(\"Test set accuracy: {:.2f} percent\".format(acc*100))\n\n# Calculate Precision, Recall and F1 Score\nprint(classification_report(y_validation, y_pred))","ea39e868":"np.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplot_confusion_matrix(y_validation, y_pred, classes=np.array(['Died', 'Survived']),\n                      title='Confusion matrix, without normalization')\nplt.show()","a7d0b852":"predictions = rfc.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('Submission_File_1.csv', index=False)\nprint(\"Your submission was successfully saved!\")","34b6858f":"# Import DecisionTreeClassifier from sklearn.tree\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6\ndt = DecisionTreeClassifier(max_depth=6)\n\n# Fit dt to the training set\ndt.fit(X_train, y_train)\n\n# Predict test set labels\ny_pred = dt.predict(X_validation)","e206bea5":"# Import accuracy_score and classification report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\n# Predict test set labels\n#y_pred = dt.predict(X_validation)\n\n# Compute test set accuracy  \nacc = accuracy_score(y_pred, y_validation)\nprint(\"Test set accuracy: {:.2f} percent\".format(acc*100))\n\n# Calculate Precision, Recall and F1 Score\nprint(classification_report(y_validation, y_pred))","0ef278ad":"np.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplot_confusion_matrix(y_validation, y_pred, classes=np.array(['Died', 'Survived']),\n                      title='Confusion matrix, without normalization')\nplt.show()","f430e50d":"# Create a pd.Series of features importances\nimportances = pd.Series(data=dt.feature_importances_,\n                        index= X_train.columns)\n\n# Sort importances\nimportances_sorted = importances.sort_values()\n\n# Draw a horizontal barplot of importances_sorted\nimportances_sorted.plot(kind='barh', color='lightgreen')\nplt.title('Features Importances')\nplt.show()","f98b9373":"# predictions = dt.predict(X_test)\n\n# output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n# output.to_csv('my_submission.csv', index=False)\n# print(\"Your submission was successfully saved!\")","60ec7e57":"# Import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import BaggingClassifier\nfrom sklearn.ensemble import BaggingClassifier\n\n# Instantiate dt\ndt = DecisionTreeClassifier(random_state=1)\n\n# Instantiate bc\nbc = BaggingClassifier(base_estimator=dt, n_estimators=50, random_state=1)","b5880334":"# Fit bc to the training set\nbc.fit(X_train, y_train)\n\n# Predict test set labels\ny_pred = bc.predict(X_validation)\n\n# Evaluate acc_test\nacc_test = accuracy_score(y_validation, y_pred)\nprint('Test set accuracy of bc: {:.2f} percent'.format(acc_test*100))\n\n# Evaluate Precision, Recall and F1 Score\nprint(classification_report(y_validation, y_pred))","ab3f2a1a":"np.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplot_confusion_matrix(y_validation, y_pred, classes=np.array(['Died', 'Survived']),\n                      title='Confusion matrix, without normalization')\nplt.show()","872fc30d":"# predictions = bc.predict(X_test)\n\n# output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n# output.to_csv('my_submission.csv', index=False)\n# print(\"Your submission was successfully saved!\")","36d929f7":"# Import KNN and Logistic Regression\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Set seed for reproducibility\nSEED=1\n\n# Instantiate lr\nlr = LogisticRegression(random_state=SEED)\n\n# Instantiate knn\nknn = KNN(n_neighbors=27)\n\n# Instantiate dt\ndt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state=SEED)\n\n# Define the list classifiers\nclassifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt)]","2d5559d7":"# Import VotingClassifier from sklearn.ensemble\nfrom sklearn.ensemble import VotingClassifier\n\n# Import accuracy_score and classification report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\n# Instantiate a VotingClassifier vc\nvc = VotingClassifier(estimators=classifiers)     \n\n# Fit vc to the training set\nvc.fit(X_train, y_train)   \n\n# Evaluate the test set predictions\ny_pred = vc.predict(X_validation)\n\n# Calculate accuracy score\naccuracy = accuracy_score(y_pred, y_validation)\nprint('Voting Classifier: {:.3f}'.format(accuracy*100))\n\n# Calculate Precision, Recall and F1 Score\nprint(classification_report(y_validation, y_pred))","c1044bbc":"np.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplot_confusion_matrix(y_validation, y_pred, classes=np.array(['Died', 'Survived']),\n                      title='Confusion matrix, without normalization')\nplt.show()","3281f6ae":"# Import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import AdaBoostClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Instantiate dt\ndt = DecisionTreeClassifier(max_depth=8, random_state=1)\n\n# Instantiate ada\nada = AdaBoostClassifier(base_estimator=dt, n_estimators=200, random_state=1)\n\n# Fit ada to the training set\nada.fit(X_train, y_train)\n\n# Compute the probabilities of obtaining the positive class\ny_pred = ada.predict(X_validation)","7db88ecb":"# Import accuracy_score and classification report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\n# Compute test set accuracy  \nacc = accuracy_score(y_pred, y_validation)\nprint(\"Test set accuracy: {:.2f} percent\".format(acc*100))\n\n# Calculate Precision, Recall and F1 Score\nprint(classification_report(y_validation, y_pred))","ad8b3119":"np.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplot_confusion_matrix(y_validation, y_pred, classes=np.array(['Died', 'Survived']),\n                      title='Confusion matrix, without normalization')\nplt.show()","0af558d0":"from sklearn.svm import LinearSVC\n\nsvm = LinearSVC()\nsvm.fit(X_train, y_train)\n\n# Predict test set labels\ny_pred = svm.predict(X_validation)","a9b7b833":"# Import accuracy_score and classification report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\n# Evaluate acc_test\nacc_test = accuracy_score(y_validation, y_pred)\nprint('Test set accuracy of bc: {:.2f} percent'.format(acc_test*100))\n\n# Evaluate Precision, Recall and F1 Score\nprint(classification_report(y_validation, y_pred))","2a0b05e1":"np.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplot_confusion_matrix(y_validation, y_pred, classes=np.array(['Died', 'Survived']),\n                      title='Confusion matrix, without normalization')\nplt.show()","9302e9cc":"# Import GradientBoostingRegressor\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Instantiate gb\ngb = GradientBoostingClassifier(max_depth=2, \n            n_estimators=200,\n            random_state=2)\n\n# Fit gb to the training set\ngb.fit(X_train, y_train)\n\n# Predict test set labels\ny_pred = gb.predict(X_validation)","93454d08":"# Import accuracy_score and classification report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\n# Evaluate acc_test\nacc_test = accuracy_score(y_validation, y_pred)\nprint('Test set accuracy of bc: {:.2f} percent'.format(acc_test*100))\n\n# Evaluate Precision, Recall and F1 Score\nprint(classification_report(y_validation, y_pred))","0e3f4bd6":"np.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplot_confusion_matrix(y_validation, y_pred, classes=np.array(['Died', 'Survived']),\n                      title='Confusion matrix, without normalization')\nplt.show()","69513eb2":"# predictions = bc.predict(X_test)\n\n# output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n# output.to_csv('my_submission.csv', index=False)\n# print(\"Your submission was successfully saved!\")","fdfecbeb":"# Code for Plotting Confusion Matrix","858a5674":"# Bagging Classifier","2a7b52fa":"# AdaBoost Classifier","70610d77":"## Min Max Scaling for Age, Sibsp, Parch, Fare","71d441fe":"# Ensemble Method using: K Nearest Neighbor, Logistic Regression and Decision Tree Classifier and then voting classifier","f4a1a812":"# Gradient Boosting","a352bed9":"# Predict on test data set using Random Forest Classifier and create csv file for submission","def2a1f4":"> ## Mode Imputation","24de523b":"# Results after Bagging Classifier submission (No Normalizing, No Imputation)\n![leaderboard01.png](attachment:leaderboard01.png)","00fef322":"# Creation of Train and Validation splits and cleaning data","cd89f47d":"# Results Gradient Boosting submission (No normalization, No Imputation)\n![leaderboard02.png](attachment:leaderboard02.png)","fbbd467b":"# Random Forest Classifier","e050102c":"# Visualise feature importance for Decision Tree Classifier","c2857ced":"## Standard Scaling","c565d941":"## Mix Imputation","1963cc9f":"# Predict on test data set using Decision Tree Classifier and create csv file for submission","727dff2e":"# Inputation","56b62361":"# Predict on test data set using Bagging Classifier and create csv file for submission","15e71eda":"# Scaling","e78cb47d":"# Support Vector Classifier","d8f87da4":"# Random Forest Classifier","5c0dc99a":"# Results after Gradient Boosting submission (No normalization, Mode Imputation)\n![leaderboard.png](attachment:leaderboard.png)","d07d0a5a":"# Decision Tree Classifier","31e09481":"# Predict on test data set Gradient Boosting and create csv file for submission","0bb932fd":"## Mean Imputation","530e1a3b":"# Results after Decision Tree Classifier submission (No Normalization, No Imputation)\n![leaderboard.png](attachment:leaderboard.png)"}}