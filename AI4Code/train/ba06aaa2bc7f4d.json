{"cell_type":{"83f2d7b6":"code","0adb86a4":"code","02b00c1d":"code","441ca968":"code","16f3c45c":"code","5ea48a7e":"code","023313a3":"code","90c036bb":"code","3b132324":"code","a360c6c8":"code","681fb9cd":"code","47252aae":"code","1402a82b":"code","e6d9abd1":"code","49fabe8d":"code","43aae3a3":"code","3b83f5bf":"code","fe93f881":"code","16f47b5d":"code","77f434ee":"code","886cbbb8":"code","634bb127":"code","7d2407d4":"code","5b273575":"code","a9bce301":"code","176e0382":"code","794d22a6":"code","32000505":"code","d32691b8":"code","c5d1c700":"code","dcbc0ece":"code","055fa1cc":"code","6c4d13a0":"code","84d465bd":"code","c686da56":"code","281785a6":"code","0861c97e":"code","14b95334":"code","e9145459":"code","1da2598f":"code","beba2163":"code","407ea0de":"code","5ca7ac86":"code","b23865f9":"code","ae28bed3":"markdown","1004e550":"markdown","7d67dcaa":"markdown","d5fcdc45":"markdown","561bf396":"markdown","20ac6a20":"markdown","3ce05c4d":"markdown","32cc6a4e":"markdown","41114c0d":"markdown","b74e243a":"markdown","131e3628":"markdown","95d90beb":"markdown","0a19bd3f":"markdown","f9bfe50c":"markdown","e0b93cee":"markdown","b7176d91":"markdown","bb08c9b8":"markdown","58ce2d12":"markdown","286fa7c1":"markdown","b11d8f49":"markdown","abb71c7d":"markdown","bab98807":"markdown","66b59e20":"markdown","b9f90976":"markdown","8137ed41":"markdown"},"source":{"83f2d7b6":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom tensorflow import keras\nimport tensorflow as tf\nfrom tensorflow.keras import regularizers\nfrom keras_tuner import RandomSearch\n","0adb86a4":"train_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntrain_data.head()","02b00c1d":"test_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntest_data.head()","441ca968":"train_data.info()","16f3c45c":"test_data.info()","5ea48a7e":"train_len = len(train_data)\ncombined_data =  pd.concat(objs=[train_data, test_data], axis=0).reset_index(drop=True)","023313a3":"combined_data[combined_data['Embarked'].isna()]","90c036bb":"combined_data[(combined_data['Pclass']==1) & (combined_data['Survived']==1)]['Embarked'].value_counts()","3b132324":"combined_data['Embarked'].fillna('S', inplace=True)","a360c6c8":"combined_data[combined_data['Fare'].isna()]","681fb9cd":"combined_data['Fare'].describe()","47252aae":"combined_data[combined_data['Fare']==0][['Pclass', 'SibSp', 'Parch', 'Embarked']]","1402a82b":"for cl in range(1,4):\n    avg_fare = combined_data[((combined_data['Fare'] != 0)  &(combined_data['Embarked'] == 'S')  & (combined_data['Pclass'] == cl) & (combined_data['SibSp'] == 0) & (combined_data['Parch'] == 0))]['Fare'].mean()\n    combined_data.loc[((combined_data['Pclass']==cl) & (combined_data['Fare'] == 0)), 'Fare'] = avg_fare","e6d9abd1":"avg_fare = combined_data[((combined_data['Fare'] != 0)  &(combined_data['Embarked'] == 'S')  & (combined_data['Pclass'] == 3) & (combined_data['SibSp'] == 0) & (combined_data['Parch'] == 0))]['Fare'].mean()\ncombined_data['Fare'].fillna(avg_fare, inplace=True)","49fabe8d":"combined_data['Fare__'] = (combined_data['Fare'] - combined_data['Fare'].min()) \/ (combined_data['Fare'].max() - combined_data['Fare'].min())","43aae3a3":"combined_data['Title'] = combined_data['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\ncombined_data['Title'] = combined_data['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n\ncombined_data['Title'].value_counts()","3b83f5bf":"combined_data['Title'].replace(to_replace=['Ms', 'Mlle'], value='Miss', inplace=True)\ncombined_data['Title'].replace(to_replace=['Rev', 'Dr', 'Col', 'Major', 'Sir', 'Don', 'Capt', 'Jonkheer'], value='Mr', inplace=True)\ncombined_data['Title'].replace(to_replace=['Lady', 'Mme', 'the Countess', 'Dona'], value='Mrs', inplace=True)","fe93f881":"missing_age_idx = list(combined_data[\"Age\"][combined_data[\"Age\"].isnull()].index)\n\n#combined_data['Age__'] = combined_data[\"Age\"]\n\nfor i in missing_age_idx :\n    age_avrg = combined_data[\"Age\"][((combined_data['SibSp'] == combined_data.iloc[i][\"SibSp\"]) & (combined_data['Parch'] == combined_data.iloc[i][\"Parch\"]) & (combined_data['Title'] == combined_data.iloc[i][\"Title\"]))].median()\n    \n    if not np.isnan(age_avrg) :\n        combined_data.loc[i, 'Age'] = age_avrg\n    else :\n        age_avrg = combined_data[\"Age\"][(combined_data['Title'] == combined_data.iloc[i][\"Title\"])].median()\n        combined_data.loc[i, 'Age'] = age_avrg\n","16f47b5d":"#combined_data['Age__'] = (combined_data['Age__'] - combined_data['Age__'].min()) \/ (combined_data['Age__'].max() - combined_data['Age__'].min())","77f434ee":"combined_data.info()","886cbbb8":"combined_data.head()","634bb127":"sns.countplot(x=\"Title\", data=combined_data[:train_len], hue=\"Survived\")","7d2407d4":"sns.countplot(x=\"Embarked\", data=combined_data[:train_len], hue=\"Survived\")","5b273575":"sns.countplot(x=\"Sex\", data=combined_data[:train_len], hue=\"Survived\")","a9bce301":"sns.countplot(x=\"Pclass\", data=combined_data[:train_len], hue=\"Survived\")","176e0382":"combined_data = pd.get_dummies(combined_data, columns = [\"Title\"], prefix=\"Ttl_\")\ncombined_data = pd.get_dummies(combined_data, columns = [\"Embarked\"], prefix=\"Em_\")\ncombined_data = pd.get_dummies(combined_data, columns = [\"Sex\"], prefix=\"Sex_\")\ncombined_data = pd.get_dummies(combined_data, columns = [\"Pclass\"], prefix=\"Pclass_\")","794d22a6":"combined_data[\"FamilySize\"] = combined_data[\"SibSp\"] + combined_data[\"Parch\"] + 1","32000505":"sns.countplot(x=\"FamilySize\", hue=\"Survived\",data = combined_data[:train_len])","d32691b8":"combined_data['Single__'] = combined_data['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n#combined_data['SmallF__'] = combined_data['Fsize'].map(lambda s: 1 if  s == 2  else 0)\ncombined_data['SmallFamily__'] = combined_data['FamilySize'].map(lambda s: 1 if  2 <= s <= 4  else 0)\n#combined_data['MedF__'] = combined_data['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ncombined_data['LargeFamily__'] = combined_data['FamilySize'].map(lambda s: 1 if s >= 5 else 0)","c5d1c700":"g = sns.kdeplot(combined_data[:train_len][\"Fare\"][combined_data[:train_len][\"Survived\"] == 0], color=\"Red\", shade = True)\ng = sns.kdeplot(combined_data[:train_len][\"Fare\"][combined_data[:train_len][\"Survived\"] == 1], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel(\"Fare\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","dcbc0ece":"g = sns.kdeplot(combined_data[:train_len][\"Age\"][combined_data[:train_len][\"Survived\"] == 0], color=\"Red\", shade = True)\ng = sns.kdeplot(combined_data[:train_len][\"Age\"][combined_data[:train_len][\"Survived\"] == 1], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel(\"Age\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","055fa1cc":"combined_data.loc[ combined_data['Age'] <= 10, 'Age'] = 0\ncombined_data.loc[(combined_data['Age'] > 10) & (combined_data['Age'] <= 30), 'Age'] = 1\ncombined_data.loc[(combined_data['Age'] > 30) & (combined_data['Age'] <= 60), 'Age'] = 2\ncombined_data.loc[(combined_data['Age'] > 60) , 'Age'] = 3\ncombined_data = pd.get_dummies(combined_data, columns = [\"Age\"], prefix=\"Age_\")","6c4d13a0":"features = [col_name for col_name in combined_data.columns if '__' in col_name]\nfeatures","84d465bd":"\n\n#combined_data['Sex'] = combined_data['Sex'].map( {'female': 1, 'male': 0} )\n#combined_data['Title'] = combined_data['Title'].map({\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4})\n#combined_data['Embarked'] = combined_data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} )\n\ntrain_data = combined_data[:train_len]\ntest_data = combined_data[train_len:]\n\ntrain_targets = train_data.Survived\n\ntest_data = test_data[features]\ntrain_data = train_data[features]\n\n","c686da56":"\ndef build_model(hp):\n    model = keras.Sequential()\n    model.add(keras.layers.Flatten())\n    model.add(\n        keras.layers.Dense(\n            units=hp.Int(\"units\", min_value=32, max_value=256, step=32),\n            activation=\"relu\",\n        )\n    )\n    model.add(\n        keras.layers.Dense(\n            units=hp.Int(\"units\", min_value=32, max_value=256, step=32),\n            activation=\"relu\",\n        )\n    )\n    model.add(\n        keras.layers.Dropout(\n            rate = hp.Choice('rate', values=[0.1, 0.3, 0.5, 0.7]),\n        )\n    )\n    \n    model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n    model.compile(\n        optimizer=keras.optimizers.Adam(\n            hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n        ),\n        loss=\"binary_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n    return model","281785a6":"tuner = RandomSearch(\n    build_model,\n    objective=\"accuracy\",\n    max_trials=10,\n    executions_per_trial=2,\n    overwrite=True,\n    directory=\"my_dir\",\n    project_name=\"Titanic\",\n)","0861c97e":"early_stop = tf.keras.callbacks.EarlyStopping(monitor='binary_accuracy', patience=5, restore_best_weights=True)","14b95334":"tuner.search(train_data, \n             train_targets, \n            # batch_size=32,\n             epochs=200, \n             shuffle=True,\n            # validation_split=0.2, \n             callbacks=[early_stop,],\n            )","e9145459":"tuner.get_best_hyperparameters()[0].values\n\nbest_model = tuner.get_best_models()[0]","1da2598f":"best_model.evaluate(train_data, train_targets) ","beba2163":"gender_submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\npredict =  (best_model.predict(test_data) > 0.5).astype('int')","407ea0de":"gender_submission['Survived'] = predict","5ca7ac86":"original_test = test_data = combined_data[train_len:].copy()\noriginal_test['Survived'] = predict\noriginal_test[:20]","b23865f9":"gender_submission.to_csv('submission.csv', index=False)","ae28bed3":"As before let's normalize Age.","1004e550":"As we can see both passengers were traveling together in 1st class. Let's check the most common 'Embarked' for all survived 1st class passengers.","7d67dcaa":"AS we can see Family with 2, 3, 4 persons has highest chance to survive. So, let's introduce new features Single, SmallFamily, Largefamily","d5fcdc45":"<a id=\"2\"><\/a> <br>\n# 2. Data cleaning\nAt first let's review the datasets to determine which features need to be cleaned.","561bf396":"The simple solution is to calculate average Fare for passengers with same Pclass, SibSp, Parch, and Embarked. But before we do that let's look at Fare distribution across all passengers.","20ac6a20":"The last step in cleaning Fare is normalizing the value for better conversion of the model.","3ce05c4d":"As we can see the following features need to be cleaned: Embarked, Fare, Age and Cabin.\nWe suggest ignoring Cabin since more that 75% are missing.\n\nBefore we start cleaning Embarked, Fare, Age let's combine both sets into one DataFrame to use all available data for better approximation of missing entries.","32cc6a4e":"Now we can fill missing Age with average values for passengers with similar features.","41114c0d":"To clean missing entries for 'Age' let's intrucude new feature Title which can be extarcetd from the passanger name.","b74e243a":"As we can see all entries have the same SibSp, Parch, Embarked but different Pclass. All passengers were travelling alone but in different class so we can assume that the Fare was not recorder correctly for these passengers. \nLet's just replace it with average Fare pre class.","131e3628":"Great! We clean all missing values.\nLet's do some feature engineering now.","95d90beb":"<a id=\"3\"><\/a> <br>\n# 3. Feature engineering","0a19bd3f":"Let's take closer look at entries with Fare = 0.","f9bfe50c":"Looking at the data above we suggest to kepp only four most common Titles  [Mr, Miss, Mrs, Master].","e0b93cee":"Let's check entries with missing Embarked.","b7176d91":"<a id=\"4\"><\/a> <br>\n# 4. Model Prediction","bb08c9b8":"WOW! We got accuracy 86%!!!\nNow we can use the model to predict surviving chance for test data.","58ce2d12":"Let's see how Embarked, Sex, Pclass are correlated with Surviving chance.","286fa7c1":"Content:**Content:**\n1. [Introduction](#1)\n1. [Data cleaning](#2)\n1. [Feature engineering](#3)\n1. [Model prediction](#4)","b11d8f49":"For SibSp and Parch ","abb71c7d":"Let's take look at passanger with missing Fare.","bab98807":"So, we can assume that that both ladies boarded at Southampton.","66b59e20":"And now we can do the same for entry with missing Fare.","b9f90976":"Looking at plots above let's keep these featurs as is and just convert them into categorical variable indicators.","8137ed41":"<a id=\"1\"><\/a> <br>\n# 1. Introduction\nThe main goal of this notebook is to show Deep learning approach using Keras turner to select the best model from predefined hyperparameter space. \n\nAs Andrew Ng said \"AI is akin to building a rocket ship. You need a huge engine and a lot of fuel. The rocket engine is the learning algorithms, but the fuel is the huge amounts of data we can feed to these algorithms\". Since we do not have a lot of data in Titanic dataset (less than 1000 in DL considered as low) we have to introduce new features to compliment the lack of data.\n\nOur model reaches 88% accuracy on train data which is very good result considering that there were a lot of pure luck for passengers to survive. \n\n\nThe notebook is created by Mariya Goliyad."}}