{"cell_type":{"9a0aa1f5":"code","95493910":"code","d768092f":"code","85977a1e":"code","3015e9b6":"code","daaca9f2":"code","8f7d139f":"code","278a9408":"code","402905c7":"code","51c6174f":"code","3d4e7640":"code","26231901":"code","3dc123f8":"code","6954a93d":"code","f2fdc964":"code","28c1738d":"code","8a1cbabc":"code","b11ae559":"code","27a3d1f4":"code","46af558b":"code","85a996e4":"code","3710a0c3":"code","19f68b2e":"code","67834184":"code","e0dcc9f5":"code","f36296c9":"code","e6986028":"code","4e3efce1":"code","7c98747e":"code","17d3e2b0":"code","99e4a08f":"code","25aba296":"code","4cb1346b":"code","e565aa1b":"code","95610de6":"code","c856c863":"code","e6958698":"code","9ffc8934":"code","6ec411b0":"code","6d504ac6":"code","30eb3014":"code","4e25c7e7":"code","a628b5c3":"code","6c190482":"markdown","5cdca9b6":"markdown","cac96c32":"markdown","696948bc":"markdown"},"source":{"9a0aa1f5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","95493910":"TARGET_COL = 'diabetes_mellitus'\n\n#Load Data\ndf_train = pd.read_csv('\/kaggle\/input\/widsdatathon2021\/TrainingWiDS2021.csv',index_col=0)\ndf_test = pd.read_csv('\/kaggle\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv',index_col=0)\ndf_dict = pd.read_csv('\/kaggle\/input\/widsdatathon2021\/DataDictionaryWiDS2021.csv')\ndf_sample = pd.read_csv('\/kaggle\/input\/widsdatathon2021\/SampleSubmissionWiDS2021.csv')\ndf_template = pd.read_csv('\/kaggle\/input\/widsdatathon2021\/SolutionTemplateWiDS2021.csv')\n\n#Display\ndisplay(df_train.head())","d768092f":"print(df_train.shape)\nprint(df_test.shape)","85977a1e":"print(df_train.nunique())\nprint(df_train.dtypes)","3015e9b6":"# Target Variable Distribution in Training\ndf_train.groupby('diabetes_mellitus').encounter_id.count()","daaca9f2":"#Drop the id columns\nid_cols = ['encounter_id','hospital_id','icu_id']\ntest_id = df_test['encounter_id']\ndf_train.drop(id_cols, axis=1, inplace=True)\ndf_test.drop(id_cols, axis=1, inplace=True)","8f7d139f":"# Drop columns that have over 80% missing value\ndrop_cols = ['h1_diasbp_invasive_max', 'h1_diasbp_invasive_min',\n       'h1_mbp_invasive_max', 'h1_mbp_invasive_min',\n       'h1_sysbp_invasive_max', 'h1_sysbp_invasive_min', 'h1_albumin_max',\n       'h1_albumin_min', 'h1_bilirubin_max', 'h1_bilirubin_min',\n       'h1_bun_max', 'h1_bun_min', 'h1_calcium_max', 'h1_calcium_min',\n       'h1_creatinine_max', 'h1_creatinine_min', 'h1_hco3_max',\n       'h1_hco3_min', 'h1_lactate_max', 'h1_lactate_min',\n       'h1_platelets_max', 'h1_platelets_min', 'h1_wbc_max', 'h1_wbc_min',\n       'h1_arterial_pco2_max', 'h1_arterial_pco2_min',\n       'h1_arterial_ph_max', 'h1_arterial_ph_min', 'h1_arterial_po2_max',\n       'h1_arterial_po2_min', 'h1_pao2fio2ratio_max',\n       'h1_pao2fio2ratio_min']\ndf_train.drop(drop_cols, axis=1, inplace=True)\ndf_test.drop(drop_cols, axis=1, inplace=True)","278a9408":"print(df_train['readmission_status'].unique())\nprint(df_test['readmission_status'].unique())\n\n# Drop readmission_status column\ndf_train.drop(\"readmission_status\", axis=1, inplace=True)\ndf_test.drop(\"readmission_status\", axis=1, inplace=True)","402905c7":"# Drop Correlated Columns\ncor_cols = ['paco2_for_ph_apache', 'h1_inr_max', 'h1_inr_min']\ndf_train.drop(cor_cols, axis=1, inplace=True)\ndf_test.drop(cor_cols, axis=1, inplace=True)","51c6174f":"df_train.describe().T","3d4e7640":"missing_count = df_train.isna().sum()\nmissing_df = (pd.concat([missing_count.rename('Missing count'),\n                     missing_count.div(len(df_train))\n                          .rename('Missing ratio')],axis = 1)\n             .loc[missing_count.ne(0)])\nmissing_df","26231901":"\"\"\"\nplt.figure(figsize = (16,8))\nplt.title('Hospital Admit Source', size = 20)\nsns.countplot(y ='hospital_admit_source', data = df_train); \n\"\"\"","3dc123f8":"\"\"\"\nplt.figure(figsize = (16,8))\nplt.title('ICU Admit Source', size = 20)\nsns.countplot(y ='icu_admit_source', data = df_train)\n\"\"\"","6954a93d":"## Print the categorical columns\nprint([col for col in df_train.columns if (1<df_train[col].nunique()) & (df_train[col].dtype != np.number)& (df_train[col].dtype != int) ])","f2fdc964":"categorical_cols =  ['ethnicity', 'gender', 'hospital_admit_source', 'icu_admit_source', 'icu_stay_type', 'icu_type']\n\n## Handle na values\ndf_train[categorical_cols] = df_train[categorical_cols].fillna(\"NA\")\ndf_test[categorical_cols] = df_test[categorical_cols].fillna(\"NA\")\n\ndf_train[categorical_cols].isna().sum()","28c1738d":"for i in range(len(df_dict)):\n    if str(df_dict['Data Type'][i])==\"string\":\n        print(df_dict['Variable Name'][i])","8a1cbabc":"print(df_train['apache_2_diagnosis'].nunique())\nprint(df_train['apache_3j_diagnosis'].nunique())","b11ae559":"diagnosis_cols = ['apache_2_diagnosis','apache_3j_diagnosis']","27a3d1f4":"for i in range(len(df_dict)):\n    if str(df_dict['Data Type'][i])==\"binary\":\n        print(df_dict['Variable Name'][i])","46af558b":"binary_cols = ['elective_surgery',\n'apache_post_operative',\n'arf_apache',\n'gcs_unable_apache',\n'intubated_apache',\n'ventilated_apache',\n'aids',\n'cirrhosis',\n'hepatic_failure',\n'immunosuppression',\n'leukemia',\n'lymphoma',\n'solid_tumor_with_metastasis']","85a996e4":"#impute weight.\ndf_train.groupby('gender')[\"weight\"].mean()","3710a0c3":"df_test.groupby('gender')[\"weight\"].mean()","19f68b2e":"\"\"\"\n# impute weight wrt gender\ndf_train[\"weight\"] = np.where(df_train['gender'] == 'F', df_train[\"weight\"].fillna(77.04), df_train[\"weight\"])\ndf_train[\"weight\"] = np.where(df_train['gender'] == 'M', df_train[\"weight\"].fillna(89.49), df_train[\"weight\"])\ndf_train[\"weight\"] = np.where(df_train['gender'] == 'NA', df_train[\"weight\"].fillna(83.73), df_train[\"weight\"])\n\ndf_test[\"weight\"] = np.where(df_test['gender'] == 'F', df_test[\"weight\"].fillna(77.11), df_test[\"weight\"])\ndf_test[\"weight\"] = np.where(df_test['gender'] == 'M', df_test[\"weight\"].fillna(88.84),df_test[\"weight\"])\ndf_test[\"weight\"] = np.where(df_test['gender'] == 'NA', df_test[\"weight\"].fillna(98.64),df_test[\"weight\"])\n\"\"\"","67834184":"#impute height.\ndf_train.groupby('gender')[\"height\"].mean()","e0dcc9f5":"df_test.groupby('gender')[\"height\"].mean()","f36296c9":"\"\"\"\n# impute height wrt gender\ndf_train[\"height\"] = np.where(df_train['gender'] == 'F', df_train[\"height\"].fillna(161.631668), df_train[\"height\"])\ndf_train[\"height\"] = np.where(df_train['gender'] == 'M', df_train[\"height\"].fillna(176.340292), df_train[\"height\"])\ndf_train[\"height\"] = np.where(df_train['gender'] == 'NA', df_train[\"height\"].fillna(171.425581), df_train[\"height\"])\n\ndf_test[\"height\"] = np.where(df_test['gender'] == 'F', df_test[\"height\"].fillna(161.534660), df_test[\"height\"])\ndf_test[\"height\"] = np.where(df_test['gender'] == 'M', df_test[\"height\"].fillna(175.836729),df_test[\"height\"])\ndf_test[\"height\"] = np.where(df_test['gender'] == 'NA', df_test[\"height\"].fillna(175.160000),df_test[\"height\"])\n\"\"\"","e6986028":"# we can fill the missing values of d1_pao2fio2ratio_max with pao2_apache\/fio2_apache\n\ndf_train[\"d1_pao2fio2ratio_max\"] = np.where((df_train[\"pao2_apache\"].notna() \n                                             & df_train[\"fio2_apache\"].notna()\n                                             & df_train[\"d1_pao2fio2ratio_max\"].isna() ), \n                                            df_train[\"pao2_apache\"] \/ df_train[\"fio2_apache\"], \n                                            df_train[\"d1_pao2fio2ratio_max\"])\ndf_test[\"d1_pao2fio2ratio_max\"] = np.where((df_test[\"pao2_apache\"].notna() \n                                             & df_test[\"fio2_apache\"].notna()\n                                             & df_test[\"d1_pao2fio2ratio_max\"].isna() ), \n                                            df_test[\"pao2_apache\"] \/ df_test[\"fio2_apache\"], \n                                            df_test[\"d1_pao2fio2ratio_max\"])","4e3efce1":"all_features = df_test[[col for col in df_test if TARGET_COL != col]]\ncat_cols = categorical_cols + binary_cols + diagnosis_cols\nnumerical_cols = [col for col in all_features if col not in cat_cols]","7c98747e":"from sklearn.impute import SimpleImputer\n# impute remaining categorical (binary and diagnosis) columns by mode\n\ndef cat_imputation(df):    \n    imputer_cat = SimpleImputer(strategy='most_frequent')\n    df[cat_cols] = imputer_cat.fit_transform(df[cat_cols])\n    return df\n\ndf_train = cat_imputation(df_train)\ndf_test = cat_imputation(df_test)\n","17d3e2b0":"# impute remaining numerical columns by mean\nfor n in numerical_cols:\n    df_train[n] = df_train.groupby(['ethnicity','gender'])[n].apply(lambda x: x.fillna(x.mean()))\n    df_test[n] = df_test.groupby(['ethnicity','gender'])[n].apply(lambda x: x.fillna(x.mean()))","99e4a08f":"df_train.groupby('gender')[\"height\"].mean()","25aba296":"from sklearn.preprocessing import OrdinalEncoder\n\nfor c in categorical_cols:\n    df_train[c] = OrdinalEncoder(dtype=\"int\").fit_transform(df_train[[c]])\n    df_test[c] = OrdinalEncoder(dtype=\"int\").fit_transform(df_test[[c]])","4cb1346b":"\"\"\"\nfrom fancyimpute import KNN, NuclearNormMinimization, SoftImpute, IterativeImputer, BiScaler\n\nfill_knn_train = KNN(k=3).fit_transform(df_train)\nfill_knn_test = KNN(k=3).fit_transform(df_test)\ndf_train = pd.DataFrame(fill_knn_train)\ndf_test = pd.DataFrame(fill_knn_test)\n\"\"\"","e565aa1b":"\"\"\"\n!pip install fancyimpute\nfrom fancyimpute import KNN, NuclearNormMinimization, SoftImpute, BiScaler\ndef imputation(df):\n    all_features = df[[col for col in df if TARGET_COL != col]]\n    numerical_cols = [col for col in all_features if col not in categorical_cols]\n    df = KNN(k=3).fit_transform(df)\n    return df\n\ndf_train = imputation(df_train)\ndf_test = imputation(df_test)\n\"\"\"","95610de6":"\ncompression_opts = dict(method='zip',\n                        archive_name='out.csv')  \ndf_train.to_csv('tarin_out.zip', index=False,\n          compression=compression_opts)  \ndf_test.to_csv('test_out.zip', index=False,\n          compression=compression_opts)  \n","c856c863":"import lightgbm as lgb\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score,roc_curve,auc, mean_squared_error\nfrom sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.over_sampling import RandomOverSampler\n\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom imblearn.ensemble import BalancedBaggingClassifier, BalancedRandomForestClassifier, RUSBoostClassifier","e6958698":"X_train, X_valid, y_train, y_valid = train_test_split(\n     df_train[[c for c in df_train if TARGET_COL != c]], df_train[TARGET_COL], test_size=0.20, random_state=999)\nprint(X_train.shape,X_valid.shape)","9ffc8934":"rdm = RandomOverSampler(random_state=999)\nX_rdm, y_rdm = rdm.fit_resample(X_train, y_train)","6ec411b0":"\"\"\"\nmodel = XGBClassifier(tree_method = 'gpu_hist',learning_rate=0.45,n_estimators=164, max_depth=8, min_child_weight=2, random_state=999) \nmodel.fit(X_rdm, y_rdm)\nfeatures = X_train.columns\nprint(f\"AUC is {roc_auc_score(y_valid, model.predict_proba(X_valid)[:, 1])}\")\nprint(metrics.classification_report(y_valid, model.predict(X_valid), labels=[0, 1]))\n\"\"\"","6d504ac6":"model = LGBMClassifier(random_state=999, tree_method = 'gpu_hist',boosting_type='gbdt', \n                       num_leaves=64, learning_rate = 0.1,n_estimators = 125, max_depth =10,\n                       min_child_samples=20, min_child_weight=0.002,\n                       reg_alpha=0.0001, reg_lambda=0.0001)\nmodel.fit(X_rdm, y_rdm)\n\nfeatures = X_train.columns\nprint(f\"AUC is {roc_auc_score(y_valid, model.predict_proba(X_valid)[:, 1])}\")\nprint(metrics.classification_report(y_valid, model.predict(X_valid), labels=[0, 1]))","30eb3014":"X_test = df_test\ny_test = model.predict_proba(X_test)[:,1]\nprint(y_test)\nsubmission = pd.DataFrame({\n        \"encounter_id\": test_id,\n        \"diabetes_mellitus\": y_test\n        })\nsubmission = submission.sort_values(by=['encounter_id'])\nprint(submission)\nsubmission.to_csv('Submission.csv', index=False)","4e25c7e7":"\"\"\"\nfrom sklearn.model_selection import GridSearchCV\nparam_1 ={'n_estimators': [10,20,50,80,90,100,120,130], 'learning_rate': [0.08, 0.1,0.12]}\nparam_2 ={'num_leaves':[60,64,70,80,90,100],'max_depth':[8,9,10,11]}\ngbm = LGBMClassifier(random_state=999, tree_method = 'gpu_hist',boosting_type='gbdt', num_leaves=64)\ngrid_search = GridSearchCV(gbm, param_1, scoring='roc_auc',n_jobs=-1)\ngrid_search.fit(X_rdm,y_rdm)\ngrid_search.best_params_\n\"\"\"","a628b5c3":"\"\"\"\nfeatures = X_train.columns\nprint(f\"AUC is {roc_auc_score(y_valid, grid_search.predict(X_valid))}\")\nprint(metrics.classification_report(y_valid, grid_search.predict(X_valid), labels=[0, 1]))\n\"\"\"","6c190482":"# Deal with Imblanaced Data","5cdca9b6":"# Encoding & Imputation\n","cac96c32":"# **Load Dataset**","696948bc":"# EDA"}}