{"cell_type":{"533db594":"code","2c549560":"code","653de72b":"code","3a3a1ce4":"code","5a42ad95":"code","27d9769d":"code","01ec0bb3":"code","ab3560c0":"code","10c25bcf":"code","43924c5b":"code","50336dd0":"code","fb3ef151":"code","33e20506":"code","25b80f92":"code","b3d23b58":"code","d38a47c6":"code","22533277":"markdown","e2cbe1d9":"markdown"},"source":{"533db594":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2c549560":"import gc\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.stats import skew, kurtosis,linregress\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom sklearn.model_selection import   GroupKFold\nimport lightgbm as lgb\nfrom sklearn.decomposition import PCA\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns","653de72b":"#Calculate a linear least-squares regression for the values of the time series  \ndef sales_features(x):\n    x=np.array(x)\n    linReg = linregress(range(len(x)), x)\n    return  getattr(linReg, 'rvalue') , getattr(linReg, 'intercept') , getattr(linReg, 'slope') , getattr(linReg, 'stderr') , kurtosis(x) , skew(x)\n\n#just to expand the series to df      \ndef expand_series_to_df(df):\n    extra_features = [col for col in df.columns if df[col].dtype == 'object' and 'sku' not in col]\n    for col in extra_features :\n        tags = df[col].apply(pd.Series)\n        tags = tags.rename(columns = lambda x : 'extra_' + col + '_'+ str(x))\n        df = pd.concat([df[:], tags[:]], axis=1)\n        del df[col]\n    return df\n\n#to plot the feature importange of LGBM by gain\ndef plot_importances(importances_):\n    mean_gain = importances_[['gain', 'feature']].groupby('feature').mean()\n    importances_['mean_gain'] = importances_['feature'].map(mean_gain['gain'])\n    plt.figure(figsize=(8, 18))\n    data_imp = importances_.sort_values('mean_gain', ascending=False)\n    sns.barplot(x='gain', y='feature', data=data_imp[:2000])\n    plt.tight_layout()\n    plt.savefig('importances.png')\n    plt.show()\n    \n#train LGBM with GroupKF\ndef train_classifiers(df=None, y=None, feature_to_keep=None):\n    folds = GroupKFold(n_splits=5)\n    clfs = []\n    importances = pd.DataFrame()\n    oof_preds =np.zeros(df.shape[0])\n\n    for fold_, (trn_, val_) in enumerate(folds.split(df, y, df['en_US_description'].values)):\n        full_train = df[feature_to_keep].copy()\n        print(full_train.shape)\n        trn_x, trn_y = full_train.loc[trn_], y[trn_]\n        val_x, val_y = full_train.loc[val_], y[val_]\n\n        clf = lgb.LGBMRegressor(boosting_type='gbdt', \n                                class_weight=None, \n                                colsample_bytree=0.9, \n                                learning_rate=0.01, \n                                max_depth=5, \n                                metric='rmse',\n                                min_child_samples=20, \n                                min_child_weight=10, \n                                min_split_gain=0.01,\n                                n_estimators=10000, \n                                n_jobs=-1, num_leaves=31,\n                                objective='regression', \n                                random_state=None, \n                                reg_alpha=0.01,\n                                reg_lambda=0.01, silent=-1, \n                                subsample=0.9, \n                                subsample_for_bin=200000,\n                                subsample_freq=1, \n                                verbose=-1)\n        print(clf)\n        clf.fit(\n            trn_x, trn_y,\n            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n            eval_metric='rmse',\n            verbose=100,\n            early_stopping_rounds=100\n        )\n        oof_preds[val_] = clf.predict(val_x, num_iteration=clf.best_iteration_)\n        \n        imp_df = pd.DataFrame()\n        imp_df['feature'] = full_train.columns\n        imp_df['gain'] = clf.feature_importances_\n        imp_df['fold'] = fold_ + 1\n        importances = pd.concat([importances, imp_df], axis=0, sort=False)\n        clfs.append(clf)\n    \n    err = sqrt(mean_squared_error(y, oof_preds))\n    print('{} LGBM RMSE: {}'.format(cnt + 1, err))\n    \n    return clfs, importances, err, oof_preds","3a3a1ce4":"data_path = '..\/input\/kaggledays-paris\/'\ntrain = pd.read_csv(data_path + 'train.csv',encoding='UTF-8')\ntest = pd.read_csv(data_path + 'test.csv',encoding='UTF-8')\n\ncolorfile = pd.read_csv('..\/input\/color-feature\/color_transcode2.csv',encoding='UTF-8')\ntrain = train.merge(colorfile, on='color', how='left')\ntest  = test.merge(colorfile,  on='color', how='left')\n\ntrain['color'] = train['new_color']\ntest['color']  = test['new_color']\n\nprint(train.shape, test.shape)\nprint(train.head())\n\ntrain['is_test'] = 0\ntest['is_test'] = 1\ndf_all = pd.concat([train, test], axis=0)\nprint('train ',train.shape)\nprint('test ',test.shape)","5a42ad95":"train.head()","27d9769d":"#we Preprocess all the en_US_description with tfidf\nprint('Preprocessing text...')\ntfidf = TfidfVectorizer(max_features=5,norm='l2',)\ntfidf.fit(df_all[ 'en_US_description'].astype(str).values)\ntfidf_all = np.array(tfidf.transform(df_all[ 'en_US_description'].astype(str).values).toarray(), dtype=np.float16)\n\nfor i in tqdm(range(5)):\n        df_all['en_US_description_tfidf_' + str(i)] = tfidf_all[:, i]\n        \ndel tfidf, tfidf_all\ngc.collect()\n\nprint('Done.')\nprint('train ',train.shape)\nprint('test ',test.shape)","01ec0bb3":"#we label encode all the categorial feature but at the end we didnt use all of them\nprint('Label Encoder...')\ncols = [f_ for f_ in df_all.columns if df_all[f_].dtype == 'object' and 'sku' not in f_ and 'ID' not in f_]\nprint(cols)\ncnt = 0\nfor c in tqdm(cols):\n        le = LabelEncoder()\n        df_all[c] = le.fit_transform(df_all[c].astype(str))\n        cnt += 1\n\n        del le\nprint('len(cols) = {}'.format(cnt))\n\ntrain = df_all.loc[df_all['is_test'] == 0].drop(['is_test'], axis=1)\ntest = df_all.loc[df_all['is_test'] == 1].drop(['is_test'], axis=1)\n\nprint('train ',train.shape)\nprint('test ',test.shape)","ab3560c0":"# the images features = PCA  on the resnet embedings\nvimages = pd.read_csv(data_path + 'vimages.csv')\n\npca = PCA(n_components=10)\nvpca = pca.fit_transform(vimages.drop('sku_hash', axis=1).values)\nvimages_pca = vimages[['sku_hash']]\nfor i in tqdm(range(vpca.shape[1])):\n    vimages_pca['dim_pca_{}'.format(i)] = vpca[:, i]\n    \ntrain = train.merge(vimages_pca, on='sku_hash', how='left')\ntest = test.merge(vimages_pca, on='sku_hash', how='left')\nprint('train ',train.shape)\nprint('test ',test.shape)","10c25bcf":"sales = pd.read_csv(data_path + 'sales.csv')\nsales = sales.merge(df_all[['sku_hash','model','function']], on='sku_hash', how='left')\nsales = sales.sort_values('Date', ascending=True)\nsalesg = sales.groupby('sku_hash').agg({\n    'sales_quantity': ['sum', 'mean'],\n    'Month_transaction': ['last'],\n     }).reset_index()\nsalesg.columns = ['%s%s' % (a, '_%s' % b if b else '') for a, b in salesg.columns]\nprint(salesg.head())\n\ntrain = train.merge(salesg, on='sku_hash', how='left')\ntest  = test.merge(salesg, on='sku_hash', how='left')\n\naggs={}\naggs['sales_quantity']= {'sum':'sum'}\n\n## by item \nagg_tmp = sales.groupby(['sku_hash','Date']).agg(aggs)\nagg_tmp.columns = [ '{}_{}'.format(k, agg) for k in aggs.keys() for agg in aggs[k]]\nagg_tmp=agg_tmp.reset_index()\n\nagg_tmp = agg_tmp.sort_values('Date', ascending=True)\n\nXX_sales =  agg_tmp.groupby(['sku_hash'])['sales_quantity_sum'].apply(lambda x: sales_features(x))\nXX_sales  = expand_series_to_df(XX_sales.reset_index())\nXX_sales.columns = ['sku_hash', \n                    'sales_quantity_sum_linear_trend_rvalue',\n                    'sales_quantity_sum_linear_trend_intercept',\n                    'sales_quantity_sum_linear_trend_attr_slope',\n                    'sales_quantity_sum_linear_trend_attr_stderr',\n                    'sales_quantity_sum_kurtosis',\n                    'sales_quantity_sum_skewness'\n                    ]\n\ntrain = train.merge(XX_sales, on='sku_hash', how='left')\ntest = test.merge(XX_sales,   on='sku_hash', how='left')\nprint('train ',train.shape)\nprint('test ',test.shape)","43924c5b":"# by model\nagg_tmp = sales.groupby(['model','Date']).agg(aggs)\nagg_tmp.columns = [ '{}_{}'.format(k, agg) for k in aggs.keys() for agg in aggs[k]]\nagg_tmp=agg_tmp.reset_index()\nagg_tmp = agg_tmp.sort_values('Date', ascending=True)\nXX_sales_BYMODEL =  agg_tmp.groupby(['model'])['sales_quantity_sum'].apply(lambda x: sales_features(x))\nXX_sales_BYMODEL  = expand_series_to_df(XX_sales_BYMODEL.reset_index())\nXX_sales_BYMODEL.columns = ['model', \n                    'BYMODELsales_quantity_sum_linear_trend_rvalue',\n                    'BYMODELsales_quantity_sum_linear_trend_intercept',\n                    'BYMODELsales_quantity_sum_linear_trend_attr_slope',\n                    'BYMODELsales_quantity_sum_linear_trend_attr_stderr',\n                    'BYMODELsales_quantity_sum_kurtosis',\n                    'BYMODELsales_quantity_sum_skewness'\n                    ]\n\ntrain = train.merge(XX_sales_BYMODEL, on='model', how='left')\ntest  = test.merge(XX_sales_BYMODEL,  on='model', how='left')\nprint('train ',train.shape)\nprint('test ',test.shape)","50336dd0":"def extract_features(df):\n    df['month_real'] = (df['month']+df['Month_transaction_last']).apply(lambda x: x%12)\n    pass\n\nprint('Extracting month_real features for train:')\nextract_features(train)\nprint('Extracting month_real features for test:')\nextract_features(test)\nprint('train ',train.shape)\nprint('test ',test.shape)","fb3ef151":"train.columns","33e20506":"y = np.log1p(train['target'].values)\n\n# the list of feature to keep \nf = [\n     'en_US_description_tfidf_0', 'en_US_description_tfidf_1', 'en_US_description_tfidf_2', 'en_US_description_tfidf_3', 'en_US_description_tfidf_4',\n     'dim_pca_0', 'dim_pca_1', 'dim_pca_2', 'dim_pca_3','dim_pca_4',\n      'dim_pca_5', 'dim_pca_6', 'dim_pca_7', 'dim_pca_8','dim_pca_9',\n\n    'sales_quantity_sum',\n    'sales_quantity_mean',\n    # 'sales_quantity_std',\n    # 'last_sales_quantity',\n    # 'addtocart_sum',\n    'fr_FR_price',\n    'product_type',\n    'month',\n    'month_real',\n    'product_gender',\n    'macro_function',\n    'function',\n    # 'sub_function',\n    #'model',\n    'macro_material',\n    'color',\n    # 'en_US_description',\n    'Month_transaction_last',\n    'sales_quantity_sum_linear_trend_rvalue',\n    'sales_quantity_sum_linear_trend_intercept',\n    'sales_quantity_sum_linear_trend_attr_slope',\n    'sales_quantity_sum_linear_trend_attr_stderr',\n    'sales_quantity_sum_kurtosis',\n    'sales_quantity_sum_skewness',\n     'BYMODELsales_quantity_sum_linear_trend_rvalue',\n     'BYMODELsales_quantity_sum_linear_trend_intercept',\n     'BYMODELsales_quantity_sum_linear_trend_attr_slope',\n     'BYMODELsales_quantity_sum_linear_trend_attr_stderr',\n     'BYMODELsales_quantity_sum_kurtosis',\n     'BYMODELsales_quantity_sum_skewness',\n       ]","25b80f92":"clfs, importances, err, oof_predict = train_classifiers(train,y,f)\n","b3d23b58":"plot_importances(importances)","d38a47c6":"preds_ = None\nfor clf in clfs:\n        if preds_ is None:\n            preds_  = clf.predict(test[f], num_iteration=clf.best_iteration_)\n        else:\n            preds_ += clf.predict(test[f], num_iteration=clf.best_iteration_)\n            \npreds_ = preds_ \/ len(clfs)\npreds_ = np.expm1(preds_)\n\n# Prepare submission\nsubm = pd.DataFrame()\nsubm['ID'] = test['ID'].values\nsubm['target'] = preds_\nsubm.to_csv('submission{}.csv'.format(err), index=False) \n","22533277":"Here is more or less what our best single model was. \\n (our best submission was a blend of different runs.)\n\n\nWe fight a lot to get a correct Cross Validation scheme and finally Oleg got something while removing almost all features and using a GroupKfold by description which actually acts as proxy to the sku_hash.\n\n\nWe forgot to include back the TFIDF and PCA pictures features in our submission this is what cause the difference in score between this kernel and our LB, too bad for the 0.00024 difference between the 5th rank...\n\n\nUsefull features were the quantity, price, real month and result of linear least-squares regression of quantity for the 7 days available . \n\n\nThanks again to Lu and Oleg and also to the organisers and mentors .\n","e2cbe1d9":"Sales features are based on the sum of quantity by product by date , and also on the sum of quantity by model  by date"}}