{"cell_type":{"520d2777":"code","2a2a2e91":"code","ee71e9f3":"code","a657f39e":"code","078e0293":"code","ac6f00ee":"code","db5b7a40":"code","ac67c29d":"code","c0f51d6b":"code","59e7ac2a":"code","47d4c9ca":"code","3ff44530":"code","53cbb5ba":"code","78a14eac":"code","2058f7f8":"code","3bdb3040":"code","b8332b4e":"code","b31439c0":"code","683dee7c":"code","369bd8a9":"code","2e4e78a4":"code","a677a6bb":"code","aef86acc":"code","dec3e2b8":"code","405f53d0":"code","d73a3234":"code","705be582":"code","de8c4895":"code","07b37a48":"code","8d3b6402":"code","cc9dc2cc":"code","46a9d79f":"code","37305e8a":"code","d37f6991":"code","e0c79018":"code","7b5dfe9a":"code","99b7fc69":"code","52294757":"code","5655fbb5":"code","f68b5ed8":"code","bba3a741":"code","b8ddb09c":"code","c78d6cc3":"code","b73123d7":"code","79c511e1":"code","d1d595af":"code","3725f540":"code","798bd618":"code","50505d44":"code","f7616fca":"code","fe416a00":"code","6dacaa2b":"code","51a64ba6":"code","1e868423":"code","72f28b7c":"markdown","4b4cbe77":"markdown","457d8a73":"markdown","698be311":"markdown","aa660e17":"markdown","6ed5b71d":"markdown","cd56d674":"markdown","5f2f8a00":"markdown","6b4ef595":"markdown","2070b65e":"markdown","a735aced":"markdown","a2c0a104":"markdown","ed6e0adc":"markdown"},"source":{"520d2777":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport sklearn\nimport seaborn as sns \n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","2a2a2e91":"housing_data=pd.read_csv('..\/input\/california-housing-prices\/housing.csv')","ee71e9f3":"housing_data.head()","a657f39e":"housing_data.describe()","078e0293":"housing_data.columns","ac6f00ee":"housing_data.shape","db5b7a40":"housing_data.hist(bins=50, figsize=(20,15))\nplt.show()","ac67c29d":"fig, axes = plt.subplots(3, 3, figsize=(18, 10))\n\nsns.boxplot(ax=axes[0, 0], data=housing_data, x='longitude')\nsns.boxplot(ax=axes[0, 1], data=housing_data, x='latitude')\nsns.boxplot(ax=axes[0, 2], data=housing_data, x='housing_median_age')\nsns.boxplot(ax=axes[1, 0], data=housing_data, x='total_rooms')\nsns.boxplot(ax=axes[1, 1], data=housing_data, x='total_bedrooms')\nsns.boxplot(ax=axes[1, 2], data=housing_data, x='population')\nsns.boxplot(ax=axes[2, 0], data=housing_data, x='households')\nsns.boxplot(ax=axes[2, 1], data=housing_data, x='median_income')","c0f51d6b":"fig, axes = plt.subplots(3, 3, figsize=(18, 10))\n\nsns.scatterplot(ax=axes[0, 0], data=housing_data, x='longitude', y ='median_house_value',hue='ocean_proximity')\nsns.scatterplot(ax=axes[0, 1], data=housing_data, x='latitude', y ='median_house_value',hue='ocean_proximity')\nsns.scatterplot(ax=axes[0, 2], data=housing_data, x='housing_median_age', y ='median_house_value',hue='ocean_proximity')\nsns.scatterplot(ax=axes[1, 0], data=housing_data, x='total_rooms', y ='median_house_value',hue='ocean_proximity')\nsns.scatterplot(ax=axes[1, 1], data=housing_data, x='total_bedrooms', y ='median_house_value',hue='ocean_proximity')\nsns.scatterplot(ax=axes[1, 2], data=housing_data, x='population', y ='median_house_value',hue='ocean_proximity')\nsns.scatterplot(ax=axes[2, 0], data=housing_data, x='households', y ='median_house_value',hue='ocean_proximity')\nsns.scatterplot(ax=axes[2, 1], data=housing_data, x='median_income', y ='median_house_value',hue='ocean_proximity')","59e7ac2a":"housing_data.corr(method ='pearson')","47d4c9ca":"for column in (housing_data.columns):\n    print(\"null data in \" , column , \"= \",housing_data[column].isnull().sum())","3ff44530":"housing_data=housing_data.dropna()\nhousing_data[column].isnull().sum()","53cbb5ba":"housing_data[\"ocean_proximity\"].unique()","78a14eac":"housing_data[\"ocean_proximity\"].value_counts()","2058f7f8":"sns.catplot(x='ocean_proximity',data=housing_data,kind=\"count\")","3bdb3040":"housing_data[\"ocean_proximity\"]=housing_data[\"ocean_proximity\"].replace(\"NEAR BAY\",0)\nhousing_data[\"ocean_proximity\"]=housing_data[\"ocean_proximity\"].replace(\"NEAR OCEAN\",0)\nhousing_data[\"ocean_proximity\"]=housing_data[\"ocean_proximity\"].replace(\"<1H OCEAN\",1)\nhousing_data[\"ocean_proximity\"]=housing_data[\"ocean_proximity\"].replace(\"INLAND\",2)\nhousing_data[\"ocean_proximity\"]=housing_data[\"ocean_proximity\"].replace(\"ISLAND\",3)","b8332b4e":"housing_data = housing_data.drop('ocean_proximity', axis=1)\n#without droping this column the error is do big 67774.38988264417","b31439c0":"filt=housing_data[\"households\"]<6000\nhousing_data=housing_data.loc[filt]\nhousing_data.shape ","683dee7c":"filt=housing_data[\"population\"]<20000\nhousing_data=housing_data.loc[filt]\nhousing_data.shape ","369bd8a9":"filt=housing_data[\"total_bedrooms\"]<6000\nhousing_data=housing_data.loc[filt]\nhousing_data.shape ","2e4e78a4":"filt=housing_data[\"total_rooms\"]<35000\nhousing_data=housing_data.loc[filt]\nhousing_data.shape \n# dropping the outliers gave a better resultes","a677a6bb":"\nX=housing_data.iloc[:,:9]\nY=housing_data.iloc[:,-1]\nprint(X.shape)\nprint(Y.shape)\n","aef86acc":"from sklearn.model_selection import train_test_split\nX_train, X_test,Y_train,Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 20)\nX_train.shape\nX_train.head()","dec3e2b8":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler(copy=True, with_mean=True, with_std=True)\nX_train_scale = scaler.fit_transform(X_train)\nX_test_scale = scaler.transform(X_test)\nX_test_scale","405f53d0":"from sklearn.linear_model import Ridge\nridge_reg = Ridge(alpha=0.01)\nridge_reg.fit(X_train_scale, Y_train)","d73a3234":"Y_predict=ridge_reg.predict(X_test_scale)","705be582":"score=ridge_reg.score(X_test_scale,Y_predict)\nprint(\"accuracy is : \",score*100 , \"%\")","de8c4895":"from sklearn.metrics import mean_squared_error\nMSE = mean_squared_error(Y_test, Y_predict)\nprint(np.sqrt(MSE)) \n#alpha=1  => 9.037097028992218e-10\n#alpha=5  => 4.551486323293706e-09  # the best\n#alpha=10 => 9.032141529746062e-09","07b37a48":"Y_predict_train=ridge_reg.predict(X_train_scale)","8d3b6402":"MSE = mean_squared_error(Y_train, Y_predict_train)\nprint(np.sqrt(MSE)) # the model don't suffer from overfitting or under fitting","cc9dc2cc":"from sklearn.metrics import r2_score\nprint(r2_score(Y_test, Y_predict))","46a9d79f":"from sklearn.linear_model import Lasso\nlasso_reg = Lasso(alpha=4.7)\nlasso_reg.fit(X_train_scale, Y_train)","37305e8a":"Y_predict=lasso_reg.predict(X_test_scale)","d37f6991":"score=lasso_reg.score(X_test_scale,Y_predict)\nprint(\"accuracy is : \",score*100 , \"%\")","e0c79018":"MSE = mean_squared_error(Y_test, Y_predict)\nprint(np.sqrt(MSE))\n#alpha=1  => 0.1681449647887946\n#alpha=5  => 0.09263927846218097\n#alpha=9  => 0.09731598695208972\n#alpha=10 => 0.08287387008903957 the best\n#alpha=15 => 0.11404621947351543\n#alpha=20 => 0.14574983885366918","7b5dfe9a":"Y_predict_train=lasso_reg.predict(X_train_scale)\nMSE = mean_squared_error(Y_train, Y_predict_train)\nprint(np.sqrt(MSE)) # the model don't suffer from overfitting or under fitting","99b7fc69":"print(r2_score(Y_test, Y_predict))","52294757":"print(r2_score(Y_train, Y_predict_train))","5655fbb5":"from sklearn.linear_model import ElasticNet\nelastic_reg=ElasticNet(alpha=1, l1_ratio=1)\nelastic_reg.fit(X_train_scale, Y_train)","f68b5ed8":"Y_predict=elastic_reg.predict(X_test_scale)","bba3a741":"score=elastic_reg.score(X_test_scale,Y_predict)\nprint(\"accuracy is : \",score*100 , \"%\")","b8ddb09c":"MSE = mean_squared_error(Y_test, Y_predict)\nprint(np.sqrt(MSE))\n#alpha=1  => 0.18001525491166995 ,ratio=0.5\n#alpha=5  => 0.1333989080984809  ,ratio=0.5\n#alpha=9  => 0.09738099019410122 ,ratio=0.5\n#alpha=10 => 0.0924007783214709  ,ratio=0.5\n#alpha=15 => 0.08677372255100453 ,ratio=0.5\n#alpha=20 => 0.08504492603835609 ,ratio=0.5  \n#alpha=21 => 0.08034563217845028 ,ratio=0.4\n#alpha=22 => 0.08032991435000437 ,ratio=0.3  the best\n#alpha=23 => 0.09430418539961244 ,ratio=0.2","c78d6cc3":"Y_predict_train=elastic_reg.predict(X_train_scale)\nMSE = mean_squared_error(Y_train, Y_predict_train)\nprint(np.sqrt(MSE)) # the model don't suffer from overfitting or under fitting","b73123d7":"print(r2_score(Y_test, Y_predict))","79c511e1":"print(r2_score(Y_train, Y_predict_train))","d1d595af":"import xgboost as xgb","3725f540":"data_dmatrix = xgb.DMatrix(data=X,label=Y)","798bd618":"#xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 1, learning_rate = 1,\n #               max_depth = 13, alpha = 10, n_estimators = 17)\n\nxg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 1, learning_rate = 0.9,\n                max_depth = 7, alpha = 10, n_estimators =20)","50505d44":"xg_reg.fit(X_train_scale,Y_train)\n\nY_predict = xg_reg.predict(X_test_scale)","f7616fca":"score=xg_reg.score(X_test_scale,Y_predict)\nprint(\"accuracy is : \",score*100,\"%\")","fe416a00":"MSE = mean_squared_error(Y_test, Y_predict)\nprint(np.sqrt(MSE))","6dacaa2b":"Y_predict_train=xg_reg.predict(X_train_scale)\nMSE = mean_squared_error(Y_train, Y_predict_train)\nprint(np.sqrt(MSE)) # the model don't suffer from overfitting or under fitting","51a64ba6":"print(r2_score(Y_test, Y_predict))","1e868423":"print(r2_score(Y_train, Y_predict_train))","72f28b7c":"# better understating the relation between features and the output price","4b4cbe77":"# spliting the train_set and test_set","457d8a73":"# dropping outliers","698be311":"# Visualizing distributions of data\n","aa660e17":"# XGBoost","6ed5b71d":"# using Lasso ","cd56d674":"# using Ridge","5f2f8a00":"# scale the input ","6b4ef595":"# Visualizing boxplot to find data outliers","2070b65e":"### . plain lnear regression is the normal regression that uses gradient decent and with no Regularizing term and i can use it with univariate linear regression\n### . Ridge  linear regression that uses gradient decent and with Regularizing term = 1\/2 sum(theta^2)  \n### . Lasso linear regression that uses gradient decent and with Regularizing term = l1 norm\n### . ElasticNet lasso linear regression that uses gradient decent and with Regularizing term that is mix of Ridge and Lasso and there is parameter to control it\n## .Ridge is a good default, but if we suspect that only a few features are useful, we should prefer Lasso or Elastic Net because they tend to reduce the useless features\u2019 weights down to zero,","a735aced":"# using elastic","a2c0a104":"# split the input and the output","ed6e0adc":"# data cleaing"}}