{"cell_type":{"0b92a005":"code","914d80a6":"code","d681db0f":"code","d07f375b":"code","6bfd3be2":"code","c23da0ee":"code","bd795bd6":"code","ba22aecf":"code","90211e96":"code","8b61cf2f":"markdown","12c67a4f":"markdown","ed1aa8f3":"markdown"},"source":{"0b92a005":"import numpy as np","914d80a6":"CLASSES_LIST = [\n    \"Lead\",\n    \"Position\",\n    \"Claim\",\n    \"Counterclaim\",\n    \"Rebuttal\",\n    \"Evidence\",\n    \"Concluding Statement\"\n]","d681db0f":"# Code adapted from Rob Mulla (@robikscube) (https:\/\/www.kaggle.com\/robikscube\/student-writing-competition-twitch)\ndef is_match(row):\n    \"\"\"\n    Returns  if prediction and ground truth are matching.\n    Only used internally in get_scores.\n    \"\"\"\n    set_pred = set(row.predictionstring_pred.split(\" \"))\n    set_gt = set(row.predictionstring_gt.split(\" \"))\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter \/ len(set_gt)\n    overlap_2 = inter \/ len(set_pred)\n    return overlap_1 >= 0.5 and overlap_2 >= 0.5","d07f375b":"def get_scores(pred_df, gt_df):\n    \"\"\"\n    Returns precision, recall and f1 scores. Only used internally in kaggle_score\n    for one class at a time.\n    \"\"\"\n\n    # Checking DataFrames emptiness before proceeding with calculations:\n    nan_metrics_nb = 0\n    \n    if pred_df.empty:\n        precision = np.nan # Precision has no mathematical meaning in that case\n        recall = 0\n        nan_metrics_nb += 1\n    \n    if gt_df.empty:\n        precision = 0\n        recall = np.nan # Recall has no mathematical meaning in that case\n        nan_metrics_nb += 1\n    \n    if nan_metrics_nb > 0:\n        return {\n            \"precision\" : precision,\n            \"recall\" : recall,\n            \"f1\" : np.nan if nan_metrics_nb == 2 else 0\n        }\n    \n    # If no DataFrame is empty, we proceed:\n    gt_df = gt_df[[\"id\", \"discourse_type\", \"predictionstring\"]].reset_index(drop=True).copy()\n    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n    pred_df[\"pred_id\"] = pred_df.index\n    gt_df[\"gt_id\"] = gt_df.index\n\n    # All ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(\n        gt_df,\n        left_on=[\"id\", \"class\"],\n        right_on=[\"id\", \"discourse_type\"],\n        how=\"outer\",\n        suffixes=(\"_pred\", \"_gt\"),\n    ).fillna(\"\")\n\n    # Purposedly ignoring multiple match possibilty (very unlikely) for efficiency\n    tp_df = joined[joined.apply(is_match, axis=1)]\n\n    TP = tp_df.shape[0]\n    FP = pred_df.drop(tp_df[\"pred_id\"]).shape[0]\n    FN = gt_df.drop(tp_df[\"gt_id\"]).shape[0]\n    \n    # Returning metrics\n    return {\n        \"precision\" : TP \/ (TP + FP),\n        \"recall\" : TP \/ (TP + FN),\n        \"f1\" : TP \/ (TP + 0.5 * (FP + FN))\n    }","6bfd3be2":"def kaggle_score(pred_df, gt_df, return_details=False):\n    \"\"\"\n    A function that scores for the kaggle Student Writing Competition\n\n    Uses the steps in the evaluation page, with a simplified (= random)\n    calculation when 2 matches exist for the same discourse element. \n    See https:\/\/www.kaggle.com\/c\/feedback-prize-2021\/overview\/evaluation\n    \"\"\"\n    scores = [\n        get_scores(\n            pred_df[pred_df[\"class\"] == class_],\n            gt_df[gt_df[\"discourse_type\"] == class_]\n        )\n        for class_ in CLASSES_LIST   \n    ]\n    f1_score = np.nanmean([class_scores[\"f1\"] for class_scores in scores])\n    if return_details:\n        return f1_score, dict(zip(CLASSES_LIST, scores))\n    return f1_score","c23da0ee":"import pandas as pd\npd.options.mode.chained_assignment = None # Disabling pandas warnings triggered by display_classes\nfrom IPython.core.display import HTML","bd795bd6":"CLASSES_COLORS = {\n    \"Lead\": \"Grey\",\n    \"Position\": \"YellowGreen\",\n    \"Claim\": \"#F1C40F\",\n    \"Counterclaim\": \"#E67E22\",\n    \"Rebuttal\": \"#873600\",\n    \"Evidence\": \"#3498DB\",\n    \"Concluding Statement\": \"Green\"\n}","ba22aecf":"def display_classes(essay_id, train_df):\n    '''\n    Typically takes train_df or a model prediction as input.\n    Prints the essay text (keeping its exact original formatting) using colors to highlight discourse elements and their classes.\n    Uses only `predictionstring`, which is useful to display models predictions.\n    '''\n    \n    # Handling submission format :\n    discourse_type = \"class\" if \"discourse_type\" not in train_df.columns else \"discourse_type\"\n    \n    elements_df = train_df[train_df[\"id\"] == essay_id]\n    essay_text = open(f'..\/..\/raw_data\/train\/{essay_id}.txt').read()\n    essay_words = essay_text.split()\n    formatted_essay = \"\"\n    \n    # First we make sure discourse elements are in the text order\n    elements_df[\"prediction_list\"] = elements_df[\"predictionstring\"].map(lambda x : x.split())\n    elements_df[\"start_word_index\"] = elements_df[\"prediction_list\"].map(lambda x : int(x[0]))\n    elements_df.sort_values(\"start_word_index\", inplace=True)\n\n    # Then for each discourse element, we go word by word trough the original essay text\n    # and then we highlight the exact part of the essay corresponding to the discourse class.\n    end_char = 0\n    for i, element in elements_df.iterrows():\n        start_word = essay_words[element[\"start_word_index\"]] \n        start_char = essay_text[end_char:].find(start_word) + len(essay_text[:end_char])\n        formatted_essay += essay_text[end_char:start_char]\n        for word_index in element[\"prediction_list\"]:\n            word = essay_words[int(word_index)]\n            word_position = essay_text[end_char:].find(word)\n            if word_position == -1:\n                return \"Formatting failed\"\n            end_char = word_position + len(essay_text[:end_char]) + len(word)\n        formatted_essay += f\"|<span style='color:{CLASSES_COLORS[element[discourse_type]]}'>{essay_text[start_char:end_char]}<\/span>\"\n    formatted_essay = formatted_essay.replace(\"\\n\", \"<br>\")\n    color_labels = \" \".join([\n        f\"|<span style='color:{CLASSES_COLORS[class_]}'>{class_}<\/span>\"\n        for class_ in CLASSES_COLORS.keys()])\n    return HTML(color_labels + \"<br><br>\" + formatted_essay)","90211e96":"def generate_predictionstring(discourse_start, discourse_end, essay_text):\n    '''\n    The following snippet of code was copy pasted from this Kaggle thread\n    https:\/\/www.kaggle.com\/c\/feedback-prize-2021\/discussion\/297591\n    It is the \"official\" way `predictionstring` is computed from `discourse_start\/end`.\n    It can be useful for models that output a prediction with character index.\n    '''\n    char_start = discourse_start\n    char_end = discourse_end\n    word_start = len(essay_text[:char_start].split())\n    word_end = word_start + len(essay_text[char_start:char_end].split())\n    word_end = min( word_end, len(essay_text.split()) )\n    predictionstring = \" \".join( [str(x) for x in range(word_start,word_end)] )\n    return predictionstring","8b61cf2f":"# Metric calculation","12c67a4f":"# Generate `predictionstring`","ed1aa8f3":"# Visualize discourse elements"}}