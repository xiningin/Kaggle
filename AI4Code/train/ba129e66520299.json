{"cell_type":{"15d0ed86":"code","2df3948c":"code","0c32ab68":"code","b54ca014":"code","ba1761d4":"code","ed5d3b4a":"code","3d8835c8":"code","3a9c6ca9":"code","1cf1944c":"code","b8c71d01":"code","ee9a680d":"code","ad6e1d79":"code","af21c7a6":"code","01c67b01":"code","414cf9a8":"code","8645cc50":"code","89904e32":"code","66eafe80":"code","14ca84c3":"code","8b0d802d":"code","f1d5177c":"code","26f1b723":"code","005fcace":"code","0f3d38a3":"code","e55140a3":"code","aea6cd99":"code","cd9f590f":"code","4ca32571":"code","8711e4cc":"code","c65b02a2":"markdown","a2e6e94e":"markdown","3aaafa4f":"markdown","acfa09f4":"markdown","d5231de5":"markdown","d51a6e59":"markdown","622a061f":"markdown","053304c9":"markdown","196bd073":"markdown","2ab9ac47":"markdown","9e861d0a":"markdown","0638c82e":"markdown","9a552ee9":"markdown","7afa15c7":"markdown"},"source":{"15d0ed86":"# Exercise 10.2\n%matplotlib inline\nimport numpy as np\nfrom matplotlib import pyplot","2df3948c":"x = np.arange(0, 1, 0.02)\nfig = pyplot.plot(x, 4*np.sqrt(1-x**2))","0c32ab68":"# Hit and miss Monte Carlo integration\nngroups = 16\nN = np.zeros(ngroups)\nI = np.zeros(ngroups)\nE = np.zeros(ngroups)\n\nn0 = 100\nfor i in range(ngroups):\n    N[i] = n0\n    x = np.random.random(n0)\n    y = np.random.random(n0)\n    I[i] = 0\n    Nin = 0\n    for j in range(n0):\n        if(y[j]<np.sqrt(1-x[j]**2)):\n            Nin += 1\n    I[i] = 4*float(Nin)\/float(n0)\n    E[i] = abs(I[i] - np.pi)\n    print(n0, Nin, I[i], E[i])\n    n0 *= 2\n\npyplot.plot(N, E, ls='-', c='red',lw =3);\npyplot.plot(N, 0.8\/np.sqrt(N), ls='-', c='blue', lw=3)\npyplot.xscale('log')\npyplot.yscale('log')","b54ca014":"# Simple Monte Carlo Integration\nngroups = 16\nN = np.zeros(ngroups)\nI = np.zeros(ngroups)\nE = np.zeros(ngroups)\n\nn0 = 100\nfor i in range(ngroups):\n    N[i] = n0\n    r = np.random.random(n0)\n    I[i] = 0.\n    for j in range(n0):\n        x = r[j]\n        I[i] += np.sqrt(1-x**2)\n        \n    I[i] *=  4.\/float(n0)\n    E[i] = abs(I[i] - np.pi)\n    print(n0, I[i], E[i])\n    n0 *= 2\n\npyplot.plot(N, E, ls='-', c='red', lw = 3)\npyplot.plot(N, 0.8\/np.sqrt(N), ls='-', c='blue', lw=3)\npyplot.xscale('log')\npyplot.yscale('log')","ba1761d4":"n0 = 100000\nI = np.zeros(n0)\nr = np.random.random(n0)\nfor j in range(n0):\n    x = r[j]\n    I[j] = 4 * np.sqrt(1-x**2)\n    \ndef group_measurements(ngroups):\n    global I, n0\n    nmeasurements = n0\/\/ngroups;\n    for n in range(ngroups):\n        Ig = 0.\n        Ig2 = 0.\n        for i in range (n*nmeasurements, (n+1)*nmeasurements):\n            Ig += I[i]\n            Ig2 += I[i]**2\n        Ig \/= nmeasurements\n        Ig2 \/= nmeasurements\n        sigma = Ig2 - Ig**2\n        print(Ig, Ig2, sigma)\ngroup_measurements(10)\nprint(\"-----------------------------------\")\ngroup_measurements(20)\nprint(\"-----------------------------------\")\ngroup_measurements(1)","ed5d3b4a":"pyplot.xlim(0, 10)\npyplot.ylim(0, 1)\nx = np.arange(0, 10, 0.1)\npyplot.plot(x, np.exp(-x), label='e^{-x}')\npyplot.plot(x, np.exp(-x**2), label='e^{-x^2}')\npyplot.plot(x, x**1.5*np.exp(-x), label='x^{3\/2}e^{-x}')\npyplot.legend()","3d8835c8":"# Trapezoidal integration\ndef trapezoids(func, xmin, xmax, nmax):\n    Isim = func(xmin) + func(xmax)\n    h = (xmax-xmin)\/nmax\n    for i in range(1, nmax):\n        x = xmin + i *h\n        Isim += 2*func(x)\n    Isim *= h\/2\n    return Isim","3a9c6ca9":"def f(x):\n    return x**1.5*np.exp(-x)","1cf1944c":"print(\"Trapezoids: \", trapezoids(f, 0., 20., 100000))","b8c71d01":"# Simple Monte arlo integration\nn0 = 100000\nr = np.random.random(n0)\n\nItot = np.sum(r**1.5*np.exp(-r))\nprint(\"Simple Monte Carlo: \", Itot\/n0)","ee9a680d":"# Importance sampling\nx = -np.log(r)\nItot = np.sum(x**1.5)\nprint(\"Importance Sampling: \", Itot\/n0)","ad6e1d79":"pyplot.xlim(0, np.pi)\nx = np.arange(0, np.pi, 0.05)\npyplot.plot(x, 1.\/(x**2 + np.cos(x)**2), label='one')\npyplot.plot(x, np.exp(-x), label='two')\npyplot.plot(x, np.exp(-2*x), label='three')\npyplot.plot(x, np.exp(-0.2*x), label='four')\npyplot.legend()","af21c7a6":"# Trapezoidal integration\ndef g(x):\n    return 1.\/(x**2+np.cos(x)**2)\n\nprint(\"Trapezoids: \", trapezoids(g, 0., np.pi, 1000000))\n\n","01c67b01":"# Simple Monte Carlo integeration\nn0 = 1000000\na = np.arange(0.1, 2.1, 0.1)\nI = np.arange(0.1, 2.1, 0.1)\n\nr = np.random.random(n0)\nI0 = np.sum(1.\/((r*np.pi)**2 +np.cos(r*np.pi)**2))\nprint(\"Simple Monte Carlo: \", I0\/n0*np.pi)","414cf9a8":"# importance sampling\nprint(\"Importance Sampling:\")\nx = -np.log(r)\ni = 0\nfor ai in a:\n    norm = (1.-np.exp(-ai*np.pi))\/ai\n    x1 = norm*x\/ai\n    Itot = 0.\n    Nin = 0\n    I2 = 0.\n    for xi in x1:\n        if(xi <= np.pi):\n            Nin += 1\n            Itot += g(xi)*np.exp(xi*ai)\n            I2 += (g(xi)*np.exp(xi*ai))**2\n    Itot *= norm\n    I2 *= norm\n    \n    I[i] = Itot\/Nin\n    i+=1\n    print(ai, Itot\/Nin, np.sqrt(abs(Itot**2\/Nin**2-I2\/Nin))\/np.sqrt(Nin))\npyplot.plot(a, I, ls='-', marker='o', c = 'red', lw=3)","8645cc50":"delta = 2\nxmin = 0.\nxmax = 4.0\ndef f(x):\n    return x**2*np.exp(-x)\n\ndef P(x):\n    global xmin, xmax\n    if(x<xmin or x> xmax):\n        return 0.\n    return np.exp(-x)\n\ndef metropolis(xold):\n    global delta\n    xtrial = np.random.random()\n    xtrial = xold +(2*xtrial-1)*delta\n    weight = P(xtrial)\/P(xold)\n    xnew = xold\n    if weight >= 1:\n        xnew = xtrial\n    elif(weight != 0 ):\n        r = np.random.random()\n        if(r<=weight):\n            xnew = xtrial\n    return xnew\nxwalker = (xmax + xmin)\/2.\nfor i in range(100000):\n    xwalker = metropolis(xwalker)\n\nI0 = 0.\nN = 300000\nx = np.zeros(N)\nx[0] = xwalker\nfor i in range(1, N):\n    for j in range(20):\n        xwalker = metropolis(xwalker)\n    x[i] = xwalker\n    I0 += x[i]**2\n\nbinwidth = 0.1\npyplot.hist(x, bins= np.arange(xmin-1, xmax+1, 0.1), normed= True)\n\nprint(\"Trapezoids: \", trapezoids(f, xmin, xmax, 100000))\nprint(\"Metropolis: \", I0*(1.0-np.exp(-4.0))\/N)","89904e32":"fig=pyplot.hist(x**2, bins=np.arange(xmin**2-1, xmax**2+1, 0.1), normed=True)","66eafe80":"import numpy as np\nimport math\nimport random\nfrom matplotlib import pyplot as plt\nfrom IPython.display import clear_output\nPI = 3.1415926\ne = 2.71828","14ca84c3":"# uniform random value from a range\ndef get_rand_number(min_value, max_value):\n    range_value = max_value - min_value\n    choice = random.uniform(0, 1)\n    return min_value + range_value*choice","8b0d802d":"def f_of_x(x):\n    \"\"\"\n    The function that we want to integerate over\n    \"\"\"\n    return (e**(-1*x))\/(1+(x-1)**2)","f1d5177c":"def crude_monte_carlo(num_samples=5000):\n    lower_bound = 0\n    upper_bound = 50\n    \n    sum_of_samples = 0\n    for i in range(num_samples):\n        x = get_rand_number(lower_bound, upper_bound)\n        sum_of_samples += f_of_x(x)\n    return (upper_bound - lower_bound)*float(sum_of_samples\/num_samples)","26f1b723":"print(crude_monte_carlo(100000))","005fcace":"def get_crude_MC_variance(num_samples):\n    \"\"\"\n    This function returns the variance of the Crude Monte Carlo.\n    Note that the inputed number of samples does not necessarily need to correpsond to the number of samples used in the Monte Carlo Simulation.\n    Args:\n    - num_samples (int)\n    Return:\n    - Variance for Crude Monte Carlo approximation of f(x) (float)\n    \"\"\"\n    int_max = 5 # this is th emax of our integration range\n    # get the average of squares\n    running_total = 0\n    for i in range(num_samples):\n        x = get_rand_number(0, int_max)\n        running_total += f_of_x(x)**2\n    sum_of_sqs = running_total*int_max\/num_samples\n    \n    # get square of average\n    running_total = 0\n    for i in range(num_samples):\n        x = get_rand_number(0, int_max)\n        running_total += f_of_x(x)\n    sq_ave = (int_max*running_total\/num_samples)**2\n    \n    return sum_of_sqs - sq_ave","0f3d38a3":"def step_f(x):\n    return 1 if x<=2 else 0\nfor i in range(10):\n    x = get_rand_number(0, 6)\n    print(x, step_f(x))","e55140a3":"(4.0\/10.0)*6","aea6cd99":"xs = [float(i\/50) for i in range(int(50*PI*2))]\nys = [f_of_x(x) for x in xs]\nplt.xlim([0, 6])\nplt.ylim([0, 0.5])\nplt.plot(xs, ys)\nplt.title(\"f(x)\");","cd9f590f":"# finding the optimal lambda\ndef g_of_x(x, A, lamda):\n    e = 2.71828\n    return A*math.pow(e, -1*lamda*x)\n\ndef inverse_G_of_r(r, lamda):\n    return (-1 * math.log(float(r)))\/lamda\n\ndef get_IS_variance(lamda, num_samples):\n    \"\"\"\n    This function calculates the variance if a Monte Carlo using importance sampling.\n    Args:\n    - lamda (float): lambda value of g(x) being tested\n    Return:\n    - Variance\n    \"\"\"\n    A = lamda\n    int_max = 5\n    # get sum of squares\n    running_total = 0\n    for i in range(num_samples):\n        x = get_rand_number(0, int_max)\n        running_total += (f_of_x(x)\/g_of_x(x, A, lamda)\/g_of_x(x, A, lamda))**2\n        \n    sum_of_sqs = running_total\/num_samples\n    \n    # get squared average\n    running_total = 0\n    for i in range(num_samples):\n        x = get_rand_number(0, int_max)\n        running_total += f_of_x(x)\/g_of_x(x, A, lamda)\n    sq_ave = (running_total\/num_samples)**2\n    \n    return sum_of_sqs - sq_ave\n\n# get variance as a funciton of lambda by testing many different lambdas\ntest_lambdas = [i*.05 for i in range (1, 61)]\nvariances = []\nfor i, lamda in enumerate(test_lambdas):\n    print(f\"lambda {i+1}\/{len(test_lambdas)}: {lamda}\")\n    A = lamda\n    variances.append(get_IS_variance(lamda, 10000))\n    clear_output(wait=True)\n\noptimal_lambda = test_lambdas[np.argmin(np.asarray(variances))]\nIS_variance = variances[np.argmin(np.asarray(variances))]\n\nprint(f\"Optimal Lambda: {optimal_lambda}\")\nprint(f\"Optimal Variance: {IS_variance}\")\nprint(f\"Error: {(IS_variance\/10000)**0.5}\")","4ca32571":"def importance_sampling_MC(lamda, num_samples):\n    A  = lamda\n    running_total = 0\n    for i in range(num_samples):\n        r = get_rand_number(0, 1)\n        running_total += f_of_x(inverse_G_of_r(r, lamda=lamda))\/g_of_x(inverse_G_of_r(r, lamda=lamda), A, lamda)\n    approximation = float(running_total\/num_samples)\n    return approximation","8711e4cc":"# run simulation\nnum_samples = 10000\napprox = importance_sampling_MC(optimal_lambda, num_samples)\nvariance = get_IS_variance(optimal_lambda, num_samples)\nerror = (variance\/num_samples)**0.5\n\n# Display results\nprint(f\"Importance samping approximation: {approx}\")\nprint(f\"Variance: {variance}\")\nprint(f\"Error: {error}\")","c65b02a2":"OK, so we can see that our function is mostly active in the rough rangeof $[0, 3-ish]$ and is mostly inactive on the range $[4-ish, \\infty]$. So, let's see if we can find a function template that can be parameterized to replicate this quality. Deb's proposes this function:\n\\begin{equation}\n    g(x) = Ae^{-\\lambda x}\n\\end{equation}\n\nAfter we find the ideal values for $A$ and $\\lambda$, we'll be able to construct this plot of $f(x)$ and our optimal weight function $g(x)$.\n\nYou can see that inmany ways $g(x)$ does n ot ideally replicate the shape of $f(x)$. This is ok. A crude $g(x)$ can still do the marvels for decreasing your estimation variance. Feel free to experiment with other weight function $g(x)$ to see if you can find even better solutions.\n\n\n## Parameterize g(x)\nBefore we can perform the simulation, we will need to find the optimal parameters $\\lambda$ and $A$. We can find $A(\\lambda)$ using the normalization restriction on $g(x)$:\n\\begin{equation}\n    \\mathcal{1} = \\int_0^{\\infty} g(x)dx \\rightarrow A = \\lambda\n\\end{equation}\n\nNow, all we need to do is find the ideal $\\lambda$, and we'll have our ideal $g(x)$.\n\nTo do this, let's calculate the variance for different $\\lambda$ on the range $[0.05, 3.0]$ in increament of 0.5, and use the $\\lambda$ with the lowest variance.\n\nWhen using importance sampling, we calculate the variance of the ratio between f(x) and g(x).\n\n\\begin{equation}\n    \\sigma^2 = [\\frac{1}{N}\\sum_i^N\\frac{f^2(x_i)}{g^2(x_i)}] - [\\sum_j^N\\frac{1}{N}\\frac{f(x_j)}{g(x_j)}]^2\n\\end{equation}\nAnd we'll want to use this equation to approximate the integral:\n\n\\begin{equation}\n    I \\simeq \\frac{1}{N} \\sum_i^N \\frac{f(G^{-1}(r_i))}{g(G^{-1}(r_i))}\n\\end{equation}\n\nWe'll recalculate the variance for different $\\lambda$, changing the weight function accordingly each time. After, we'll use our optimal $\\lambda$ to calcualte the integreal with minimal variance.\n\nThe algorithm will look like this:\n1. Start at $\\lambda = 0.05$\n2. Calculate the variance\n3. Increment $\\lambda$\n4. Repeat steps 2 and 3 until you reach the last $\\lambda$\n5. Pick the $\\lambda$ with the lowest variance - this is your optimal $\\lambda$\n6. Use importance sampling Monte Carlo with this $\\lambda$ to calculate the integral","a2e6e94e":"## Importance Sampling: The Intuition\nImportance sampling is a method for reducing the variance of a Monte Carlo simulation without increasing the number of samples. The idea is that instead of randomly sampling from the whole function, let's just sample from a distribution of points similarly shaped to the function.\n\nLet's say you have a step function active on the range $[0, 2]$ and inactive from $[2, 6]$. Sampling 10 times might yield estimates like this:","3aaafa4f":"# Monte Carlo Methods\nLearned from: https:\/\/towardsdatascience.com\/monte-carlo-simulations-with-python-part-1-f5627b7d60b0","acfa09f4":"## Variance reduction\nIf the function being integrated does nto fluctuate too much in the interval of integration, and does not differ much from the average value, then the standard Monte Carlo mean-value method should work well with a reasonable number of points. Otherwise, we will find that the variance is very large, meaning that some points will make small contributions, while others will make large contributions to the integeral. If this is the case, the algorithm will be very inefficient. The method can be improved by splitting the function $f(x)$ in two $f(x) = f_1(x) + f_2(x)$, such that the integral of $f_1(x)$ is known, and $f2(x)$ as a small variance. The \"variance reduction\" technique, consists then in evaluating th eintegral of $f_2(x)$ to obtain:\n\\begin{equation}\n\\int_a^bf(x)dx = \\int_a^bf_1(x)dx + \\int_a^bf_2(x)dx = \\int_a^bf_1(x)dx + J\n\\end{equation}\n\n## Importance of Sampling\nImagine that we want to sample the function $f(x) = e^{-x^2}$ in the interval $[0, 1]$. It is evident that most of our points will fall in the region where value of $f(x)$ is very small, and therefore we will need a large number of values to achieve a decent accuracy. A way to improve the measurement by reducing the variance is obtained by \"importance sampling\". As the name says, the idea is to sample the regions with large contributions to the integral. For this goal, we introduce a probability distribution $P(x)$ normalized in the interval of integration:\n\\begin{equation}\n\\int_a^bP(x)dx = 1\n\\end{equation}\n\nThen, we can rewrite the integral of $f(x)$ as:\n\\begin{equation}\n    I = \\int_a^b\\frac{f(x)}{P(x)}P(x)dx\n\\end{equation}\nWe can evaluate this integral, by sampling according to the probability distribution $P(x)$ and evaluting the sum:\n\\begin{equation}\n    I(N) = \\frac{1}{N}\\sum_{i=1}^N\\frac{f(x_i)}{P(x_i)}.\n\\end{equation}\n\nNote that for the uniform case $P(x) = 1\/(b-a)$, the expression reduces to the simple Monte Carlo integral.\n\nWe are free to choose $P(x)$ now. We wish to do it in a way to reduce and minimize the variance of the integrand $f(x)\/P(x)$. The way to do this is picking a $P(x)$ that mimics $f(x)$ where $f(x)$ is large. If we are able to determine an appropriate $P(x)$, the integrand will be slowly varying, and hence the variance will be reduced. Another consideration is that the generation of points according to the distribution $P(x)$ should be simple task. As an example, let us consider again the integeral\n\\begin{equation}\n    I = \\int_0^1e^{-x^2}dx.\n\\end{equation}\n\nA reasonable choice for a weight function is $P(x) = Ae^{-x}$, where A is a normalization constant.\n\nNotice that for $P(x) = f(x)$ the variance is zero! This is known as the zero variance property. There is a catch, though: The probability function $P(x)$ needs to be normalized, implying that inreality, $P(x) = f(x)\/\\int f(x)dx$, which assumes that we know in advance precisely the integeral that we are trying to calculate!\n\n\n### Exercise 10.3: Importance sampling\n1. Choose the weight function $P(x) = e^{-x}$ and evaluate the integral\n\\begin{equation}\n       \\int_0^{\\infty}x^{3\/2}e^{-x}dx.\n\\end{equation}\n2. Choose $P(x) = e^{-ax}$ and estimate\n\\begin{equation}\n    \\int_0^{\\pi}\\frac{dx}{x^2 + cos^2x}\n\\end{equation}\nDetermine the value of $a$ that  minimizes the variance of the integral","d5231de5":"# Monte Carlo integration\nlearned from: https:\/\/github.com\/afeiguin\/comp-phys\/blob\/master\/10_01_montecarlo_integration.ipynb\n## The idea\nImagine tha twe want to measure the area of a pond with arbitrary shape. Suppose that this pond is in the middle of a field with known area $A$. If we throw $N$ stones randomly, such that they alnd within the boundaries of the field, and we count the number of stones that fall in the pond $N_{in}$, the area of the pond willbe approximately proportional to the fraction of stones that make a splash, multiplied by $A$:\n\\begin{equation}\n    A_{pond} = \\frac{N_{in}}{N}A\n\\end{equation}\n\nThis simple procedure is an example of the \"Monte Carlo\" method.\n\n## Simple Monte Carlo integration\nMore generally, iagine a rectangle of height H in the integration interval $[a, b]$, such that the function $f(x)$ is within its boundaries. Compute $n$ pairs of random numbers $(x_i, y_i)$ such that they are uniformly distributed inside this rectangle. The faction of points that falls within the area contained below $f(x)$, *i.e.,* that satisfy $y_i \\leq f(x_i)$ is an estimate of the ratio of the integral of $f(x)$ and the area of the rectangle. Hence, the estimate of the integral will be given by:\n\\begin{equation}\n\\int_a^bf(x)dx \\simeq I(N) = \\frac{N_{in}}{N}H(b-a)\n\\end{equation}\n\nAnother Monte Carlo procedure is based on the definiton of the average of a function $f(x)$:\n\\begin{equation}\n\\langle g \\rangle = \\frac{1}{(b-a)}\\int_a^bf(x)dx\n\\end{equation}\n\nIn order to determine this average, we sample the value of $f(x)$:\n\\begin{equation}\n\\langle f \\rangle \\simeq \\frac{1}{N}\\sum_{i=1}^Nf(x_i)\n\\end{equation}\n\nThus:\n\\begin{equation}\n\\frac{1}{(b-a)}\\int_a^bf(x)dx = \\frac{1}{N}\\sum_{i=1}^Nf(x_i)\n\\end{equation}\n\nOr:\n\\begin{equation}\n\\int_a^bf(x)dx = \\frac{(b-a)}{N}\\sum_{i=1}^Nf(x_i)\n\\end{equation}\nwhere the $N$ values $x_i$ are distributed uniformly in the interval $[a, b]$. The integral will be given by:\n\\begin{equation}\nI(N) = (b-a)\\langle f \\rangle = \\frac{b-a}{N} \\sum_{i=1}^Nf(x_i) \\text{   }(1)\n\\end{equation}\n\n## Monte Carlo error analysis\nThe Monte Carlo method clearly yields approximate results. The accuracy depends on the number of values N that we use for the average. A possible measure of the error is the \"variance\" $\\sigma^2$ defined by:\n\\begin{equation}\n\\sigma^2 = \\langle f^2 \\rangle - {\\langle f \\rangle}^2\n\\end{equation}\nwhere\n\\begin{equation}\n\\langle f \\rangle = \\frac{1}{N}\\sum_{i=1}^N f(x_i)\n\\end{equation}\nand\n\\begin{equation}\n\\label{eq:sigma}\n\\langle f^2 \\rangle = \\frac{1}{N}\\sum_{i=1}^N {f(x_i)}^2\n\\end{equation}\n\nThe \"standard deviation\" is $\\sigma$. However, we should expect that the error decreases with the number of points $N$, and the quantity $\\sigma$ defines by (\\ref{eq:sigma}) does not. Hence, this cannot be a good measure of the error.\n\nImagine that we perform several measurements of the integral, each of them yielding a resuult $I_n$. These values have been obtained with different sequences of N random numbers. According to the central limit theorem, these values would be normally distributed around a mean $\\langle I \\rangle$. Suppose that we have a set of M of such measurements $I_n$. A convenient measure of the differences of these measurements is the \"standard deviation of the means\" $\\sigma_M$:\n\\begin{equation}\n\\sigma^2_M = \\langle I^2 \\rangle - {\\langle f \\rangle}^2 \\text{   }(2),\n\\end{equation}\nwhere \n\\begin{equation}\n\\langle I \\rangle = \\frac{1}{M}\\sum_{n=1}^M I_n  \\text{   }(3),\n\\end{equation}\nand\n\\begin{equation}\n\\langle I^2 \\rangle = \\frac{1}{M}\\sum_{n=1}^M I_n^2 \\text{   }(4)\n\\end{equation}\nIt can be proven that\n\\begin{equation}\n\\sigma_M = \\sigma\/\\sqrt{N} \\text{   }(5)\n\\end{equation}\nThis relation becomes exact in the limit of a very large number of measurements. Note that this expression implies that the error decreases with the square root of th enumber of trials, meaning that if we want to reduce error by a factor 10, we need 100 times more points for the average.\n\n## Excercise 10.1: One dimensional integration\n1. Write a program that implements the \"hit and miss\" Monte Carlo integration algorithm. Find the estimate $I(N)$ for the integral of:\n\\begin{equation}\nf(x) = 4\\sqrt{1-x^2}\n\\end{equation}\nas a function of $N$, in the interval (0, 1). Choose H = 1, and sample only the x-dependant part $\\sqrt{1-x^2}$, and multiply the result by 4. Calculate the difference between $I(N)$ and the exact result $\\pi$. This difference is a measure of the error associated with the Monte Carlo estimate. Make a log-log plot of the error as a function of $N$. What is the approximate functional dependence of the error on $N$ for large $N$?\n2. Estimate the integral of $f(x)$ using the simple Monte Carlo integration by averaging over $N$ points, using integral formula $(3)$, and compute the error as a funciton of N, for N up to 10,000. Determine the approximate functional dependence of the error on $N$ for large $N$. How many trials are necessary to determine $I_N$ to two decimal places?\n3. Perform 10 measurements I_n(N), with N = 10,000 using different random sequences. Show in a table the values of $I_n$ and $\\sigma$ according to the formlulas for integral $(1)$ and sigma $(2)$. Use the formula $(5)$ to estimate the standard deviation of the means, and cmopare to the values obtained from the formula $(2)$ using 100,000 values.\n4. To verify that your result for the error is independent of the number of sets you used to divide your data, repeat the previous item grouping your results in 20 groups of 5,000 points each.\n\n## Exercise 10.2: Importance of randomness\nTo examine the efects of a poor random number generator, modify your program to use the linear congruential random number generator using the parameters $a=5, c = 0$ and the seed $x_1 = 1$. Repeat the integral of the previous exercise and compare your results.","d51a6e59":"This implementation of the Crude Monte Carlo gives us a variance of 0.266 which corresponds to an error of 0.005. For a quick, back of the envelop estimate, this isn't bad at all, but what if we need a lot more precision? We could always jsut increase teh nuber of samples, but then our computation time will increase as well. What if, instead of using random sampling, we cleverly sampled from the right distribution of points ... called, *importance sampling*.","622a061f":"# Wrapping things up\nIn this tutorial, we learned how to perform Monte Carlo simulations for the estimation of a definite integreal. We used both the crude method and the importance sampling method, and found that importance sampling provded a significant decrease in the error.","053304c9":"### Exercise 10.4: The Metropolis algorithm\nUse the Metropolis algorithm to sample points according to a distribution and estiamte the integral\n\\begin{equation}\n    \\int_0^4 x^2 e^{-x}dx\n\\end{equation}\nwith $P(x) = e^{-x}$ for $0\\leq x \\leq4$. Plot the number of times the walker is at point $x_0, x_1, x_2, ...$ Is the integrand sampled uniformly? If not, what is the approximte region of $x$ where the integrand is sampled more often?","196bd073":"### Determine the Variance of the estimation\nRun the algorithm for several times and find the variance\n$\\sigma^2 = <I^2> - <I>^2$, where $I$ is the integral value.\nOr\n\\begin{equation}\n\\sigma^2 = \\mathcal{[}\\frac{b-a}{N}\\sum_i^N f^2(x_i) \\mathcal{]} - \\mathcal{[}\\sum_j^N\\frac{b-a}{N}f(x_j)\\mathcal{]}^2\n\\end{equation}\nThis is the equation for variance we'll use in our simulations. Let's see how to do this in Python.","2ab9ac47":"## The Math\nLet's see if we can find a $g(x)$ such that:\n\\begin{equation}\n    \\frac{f(x)}{g(x)} \\simeq k\\text{, where k is some constant}\n\\end{equation}\nBasically, we want g(x) to look like a scaled version of $f(x)$.\n\nWe'll also need $g(x)$ to satisfy a few criteria:\n1. $g(x)$ is integrable\n2. g(x) is non-negative on $[a, b]$\n3. The indefinite integral of $g(x)$, which we'll call $G(x)$, has a real inverse\n4. The integral of $g(x)$ in the range $[a, b]$ must equal 1\n\nIn ideal case, $f(x) = k *g(x)$, where $k$ is a constant. However, if $f(x) = k*g(x)$, then $f(x)$ would be integrable and we would have no need to perform a Monte Carlo simulation; we could just solve the problem analytically!\n\nSo, we'll settle for $f(x) \\simeq k*g(x)$. We won't get a perfect estimate of course, but you'll find it performs better than our crude estimation from earlier.\n\nWe'll define $G(x)$ as follows, and we'll also perform a change of variables to $r$.\n\\begin{equation}\n    G(x) = \\int_0^x g(x) d(x) \\\\\n    r = G(x)\n\\end{equation}\n\n$r$ will be restricted to the range $[0, 1]$.Sicne the integral of $g(x)$ was defined to be 1, $G(x)$ can never be greater than 1, and therefore $r$ can never be greater than one. This is important becasue later, we will randomly sample from $r$ in the range $[0, 1]$ when performing the simulation.\n\nUsing these definitions, we can produce the following estimation:\n\\begin{equation}\n    I \\simeq \\frac{1}{N}\\sum_i^N \\frac{f(G^{-1}(r_i))}{g(G^{-1}(r_i))}\n\\end{equation}\n\nThis sum is what we will be calculating when we perform the Monte Carlo. We'll randomly sample from r in order to produce our estimate.\n\nSimple right? Don't be intimated if this doesn't make sense at first glance. I intentionally focused o the intuition and breezed through the math quite a bit. If you're confued, or you want more mathematical rigor, check out the resource I talked about earlier until you believe that final equation.\n\n## Importance Sampling: Python Implementation.\nOk, now that we understand the math behind importance sampling, let's go back to our problem from before. Remember, we're trying to estimate the following integral as precisely as we can:\n\\begin{equation}\n    I = \\int_0^\\infty \\frac{e^{-x}}{1 + (x - 1)^2} dx\n\\end{equation}\n\n## Visualizing our problem\nLet's start by generating a template for our $g(x)$ weight function. I'd like to visualize my function $f(x)$, so we'll do that using <code>matplotlib<\/code>:\n","9e861d0a":"These samples correspond to a most likely distribution of samples, and yield an integral estimation of 2.4. But, what if instead, we estimate the ratio between our function $f(x)$ and some special weight function $g(x)$ whose value is almost always about 1\/2 the value of $f(x)$ for any given x? What if we also bias our samples to appear in teh most acive ranges of our function (which we'll find to minimize the error). You'll see that the average of these ratios is a lot closer the real value of our integral, which is 2. The importance sampling method is used to determine this optimal function $g(x)$.","0638c82e":"## Run the simulation\nNow, all we ahve to do is run the simulation with our optimized g(x) function, and we're good to go. Here is what it looks like in ode:","9a552ee9":"## The Crude Monte Carlo: Implementation\n1. Get a random input value from the integration range\n2. Evaluate the integrand\n3. Repeat Steps 1 and 2 for as long as you like\n4. Determine the average of all these samples and multiply by the range","7afa15c7":"### The algorithm"}}