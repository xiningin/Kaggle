{"cell_type":{"88f85b34":"code","f75ef098":"code","840086ab":"code","4d5184ed":"code","46d554cf":"code","21fe4955":"code","4f910b86":"code","7b0cd956":"code","091510cb":"code","ee0266f2":"code","544cbb20":"code","8d41ff20":"code","f0b6f503":"code","4ca1c092":"code","09029765":"code","eb8f2540":"code","8e2c1a50":"code","e7cb7e86":"code","4360c4bb":"code","a0c062f1":"code","87953f40":"code","b11f19b8":"code","e90eda74":"code","f7e2f9ba":"code","009ca4b9":"code","b7f82c5b":"code","61832f5a":"code","642d4401":"code","1314103d":"code","e8d6a67f":"code","44d548cd":"code","de14c63a":"code","f9d02c6f":"code","2d0b4d77":"code","fba04c50":"markdown","cd30caca":"markdown","37ca0d83":"markdown","0bc617cb":"markdown","a3666ffa":"markdown","3d45c997":"markdown","31bd2cbe":"markdown","4f03cd54":"markdown","f99ab204":"markdown","ea2387a5":"markdown","628703c1":"markdown","f221f46b":"markdown","c4eaa136":"markdown","76964fe5":"markdown","0400a23f":"markdown","a8ab405b":"markdown","c9e717d9":"markdown","e7327fae":"markdown","96d526b1":"markdown","cfb71168":"markdown","46fbc462":"markdown","f0ec3893":"markdown","95b14ac8":"markdown","8e2a6455":"markdown","d2c438ac":"markdown"},"source":{"88f85b34":"# Principal libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline\nsns.set()","f75ef098":"# Loading the dataset\n\ndata = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndata_raw = data.copy() #Just in case\n\ndata.head()","840086ab":"# Column names\n\nprint(\"The column names are:\", data.columns)","4d5184ed":"# First look to the missing data\n\ntotal = data.isnull().sum().sort_values(ascending = False)\nporcentage = (data.isnull().sum()\/data.isnull().count()).sort_values(ascending = False)\n\nmissing_data = pd.concat([total, porcentage], axis = 1, keys = [\"Total\", \"Porcentage\"])\nmissing_data","46d554cf":"# How many people survived plot\n\nfig, ax = plt.subplots(1, 2, figsize = (15,5))\nsns.countplot(data[\"Survived\"], ax = ax[0])\nax[0].set_title(\"How many people survived?\")\nax[0].set_ylabel(\"Count\")\nsns.countplot(\"Sex\", hue = \"Survived\", data = data, ax = ax[1])\nax[1].set_title(\"Survived by Sex\")\nax[1].set_ylabel(\"Count\")\n\nplt.show()","21fe4955":"# Pclass analysis\n\nfig, ax = plt.subplots(1, 2, figsize = (15,5))\nsns.countplot(data[\"Pclass\"], ax = ax[0])\nax[0].set_title(\"Pclass Analysis\")\nax[0].set_ylabel(\"Count\")\nsns.barplot(x = \"Pclass\", y = \"Survived\", data = data, ax = ax[1])\nax[1].set_title(\"Survived by Pclass\")\nax[1].set_ylabel(\"Porcentage of total\")\n\nplt.show()","4f910b86":"# Crosstab \n\npd.crosstab(data[\"Pclass\"], data[\"Survived\"], margins = True)","7b0cd956":"# Pivot Table\n\ndata.pivot_table(\"Survived\", index = \"Sex\", columns = \"Pclass\")","091510cb":"# Survived by Sex and Pclass\n\nfig, ax = plt.subplots(1, 2, figsize = (15,5))\nsns.countplot(\"Pclass\", hue = \"Survived\", data = data, ax = ax[0])\nax[0].set_title(\"Pclass Analysis\")\nax[0].set_ylabel(\"Count\")\nsns.countplot(\"Sex\", hue = \"Pclass\", data = data, ax = ax[1])\nax[1].set_title(\"Sex by Pclass\")\nax[1].set_ylabel(\"Count\")\n\nplt.show()","ee0266f2":"# Crosstab \n\npd.crosstab([data[\"Survived\"], data[\"Sex\"]], data[\"Pclass\"], margins = True)","544cbb20":"# Lets try to get some extra info about the age\n\ndata[\"Age\"].describe()","8d41ff20":"# Violin and Box Plots\n\nfig, ax = plt.subplots(1, 2, figsize = (15, 5))\nsns.boxplot(\"Sex\", \"Age\", hue = \"Survived\", data = data, ax = ax[0])\nax[0].set_title(\"Box Plot\")\nsns.violinplot(\"Sex\", \"Age\", hue = \"Survived\", data = data, split = True, ax = ax[1])\nax[1].set_title(\"Violin Plot\")\n\nplt.show()","f0b6f503":"# Violin plot for Age, Pclass and Survived\n\nfig = sns.violinplot(\"Pclass\", \"Age\", hue = \"Survived\", split = True, data = data)\nfig.set_title(\"Pclass and Age survirval\")\nplt.show()","4ca1c092":"#\u00a0Extract the salutations (THANKS TO ash316)\n\ndata[\"Initial\"] = 0\nfor i in data:\n    data[\"Initial\"] = data[\"Name\"].str.extract('([A-Za-z]+)\\.')\n    \ndata.head()","09029765":"# Extract all the salutations\n\nprint(data[\"Initial\"].unique())","eb8f2540":"#\u00a0Now we can replace them\n\ndata[\"Initial\"].replace([\"Mlle\", \"Mme\", \"Ms\", \"Dr\", \"Major\", \"Lady\", \"Countess\",\n                        \"Jonkheer\", \"Col\", \"Rev\", \"Capt\", \"Sir\", \"Don\"], \n                        [\"Miss\", \"Miss\", \"Miss\", \"Mr\", \"Mr\", \"Mrs\", \"Mrs\", \"Other\",\n                        \"Other\", \"Other\", \"Mr\", \"Mr\", \"Mr\"], inplace = True)\n\ndata.groupby(\"Initial\")[\"Age\"].mean()","8e2c1a50":"# Assign the new values\n\ndata.loc[(data[\"Age\"].isnull())&(data[\"Initial\"]==\"Mr\"), \"Age\"] = 33\ndata.loc[(data[\"Age\"].isnull())&(data[\"Initial\"]==\"Miss\"), \"Age\"] = 22\ndata.loc[(data[\"Age\"].isnull())&(data[\"Initial\"]==\"Master\"), \"Age\"] = 5\ndata.loc[(data[\"Age\"].isnull())&(data[\"Initial\"]==\"Mrs\"), \"Age\"] = 36\ndata.loc[(data[\"Age\"].isnull())&(data[\"Initial\"]==\"Other\"), \"Age\"] = 46","e7cb7e86":"# Take a look now into the missing data\n\ntotal = data.isnull().sum().sort_values(ascending = False)\nporcentage = (data.isnull().sum()\/data.isnull().count()).sort_values(ascending = False)\n\nmissing_data = pd.concat([total, porcentage], axis = 1, keys = [\"Total\", \"Porcentage\"])\nmissing_data","4360c4bb":"# Plot Embarked and Survival\n\nfig, ax = plt.subplots(1, 2, figsize = (15, 5))\nsns.countplot(\"Embarked\", hue = \"Survived\", data = data, ax = ax[0])\nax[0].set_title(\"Embarked and survived\")\nax[0].set_ylabel(\"Count\")\nsns.countplot(\"Embarked\", hue = \"Sex\", data = data, ax = ax[1])\nax[1].set_title(\"Embarked by Sex\")\nax[1].set_ylabel(\"Count\")\n\nplt.show()","a0c062f1":"#\u00a0Crosstab\n\npd.crosstab([data[\"Survived\"], data[\"Embarked\"]], data[\"Pclass\"], margins = True)","87953f40":"#\u00a0Filling missing values\n\ndata[\"Embarked\"].fillna(\"S\", inplace = True)","b11f19b8":"# Take a look now into the missing data\n\ntotal = data.isnull().sum().sort_values(ascending = False)\nporcentage = (data.isnull().sum()\/data.isnull().count()).sort_values(ascending = False)\n\nmissing_data = pd.concat([total, porcentage], axis = 1, keys = [\"Total\", \"Porcentage\"])\nmissing_data","e90eda74":"# SibSp plot\n\nfig = sns.barplot(\"SibSp\", \"Survived\", data = data)\nfig.set_title(\"SibSp and Survived\")\n\nplt.show()","f7e2f9ba":"# SibSp plot with Pclass\n\nfig = sns.countplot(\"SibSp\", hue = \"Pclass\", data = data)\nfig.set_title(\"Pclass with SibSp\")\nfig.set_ylabel(\"Count\")\n\nplt.show()","009ca4b9":"# A brief summary of Fare\n\ndata[\"Fare\"].describe()","b7f82c5b":"# Correlation Plot\n\nsns.heatmap(data.corr(), annot = True, linewidths = 0.1)\nplt.show()","61832f5a":"# Removing non-relevant features\n\nnon_relevant_f = [\"PassengerId\", \"Cabin\", \"Name\", \"Ticket\", \"Initial\"]\ndata = data.drop(non_relevant_f, axis = 1)\n\ndata.head()","642d4401":"# Split the data\n\nX = data.iloc[:, 1:].values\ny = data.iloc[:, 0].values","1314103d":"# Encoding categorical features\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nlabelencoder_X_1 = LabelEncoder()\nX[:, 1] = labelencoder_X_1.fit_transform(X[:, 1]) # \"Sex\"\nlabelencoder_X_2 = LabelEncoder()\nX[:, 6] = labelencoder_X_2.fit_transform(X[:, 6]) # \"Embarked\"\n\ntransformer = ColumnTransformer(\n    transformers=[\n        (\"Titanic\",\n        OneHotEncoder(categories=\"auto\"),\n        [1]\n        )\n    ], remainder=\"passthrough\"\n)\nX = transformer.fit_transform(X)\nX = X[:, 1:]","e8d6a67f":"# Last but not least..\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                   y,\n                                                   test_size=0.2,\n                                                   random_state = 42)","44d548cd":"#\u00a0Its important to scale the data to make the model better\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)","de14c63a":"# Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nmodel_lr = LogisticRegression()\nmodel_lr.fit(X_train, y_train)\nprediction_lr = model_lr.predict(X_test)\n\nprint(\"The accuracy of the Logistic Regression is:\", metrics.accuracy_score(prediction_lr, y_test))","f9d02c6f":"# Random Forests\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel_rf = RandomForestClassifier(n_estimators=100)\nmodel_rf.fit(X_train, y_train)\nprediction_rf = model_rf.predict(X_test)\n\nprint(\"The accuracy of the Random Forests Classifier is:\", metrics.accuracy_score(prediction_rf, y_test))","2d0b4d77":"# Lighgt GBM\n\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score\n\ntraining_data = lgb.Dataset(data = X_train, label = y_train)\nparams = {'num_leaves': 31, 'num_trees': 100, 'objective':'binary'}\nparams['metric'] = ['auc', 'binary_logloss']\nclassifier = lgb.train(params = params,\n                      train_set = training_data,\n                      num_boost_round=10)\n\nprob_pred = classifier.predict(X_test)\ny_pred=np.zeros(len(prob_pred))\nfor i in range(0, len(prob_pred)):\n    if prob_pred[i] >= 0.5:\n        y_pred[i] = 1\n    else:\n        y_pred[i] = 0\n        \naccuracy = accuracy_score(y_pred, y_test) * 100\nprint(\"Accuracy: {:.0f} %\".format(accuracy))","fba04c50":"As we can see, not many people survived the disaster. Furthermore, there were more men in the ship than women, but more women survived the accident. This looks interesting for us, because we can build our model depending on this kind  of features. Let's continue exploring the data!\n\nLooking the data head we made before, it's clear that the next feature we could go into detail is Pclass. ","cd30caca":"## FIRST PART - EDA & DATA PREPROCESS","37ca0d83":"There are a lot of different ways to approach a classification problem. Here, we'll see some of them. The first one, a classic, is the logistic regression.","0bc617cb":"# Titanic: Machine Learning from Disaster","a3666ffa":"So now we're pretty sure about people between 20 and 40 survived more. This is another important feature in order to build a model. As we should remember, we had 177 missing values in age. There are many ways to deal with this problem, but I've learned one really clever way to do it (Really thankful to the Kaggle user ash316, look his notebook in this approach: [EDA To Prediction(DieTanic)](kaggle.com\/ash316\/eda-to-prediction-dietanic#Part3:-Predictive-Modeling)). The thing is we could fill the blanks with the mean age, but this could result in some problems. The solution is in the name (this is why keep some features which at first look don't seem interesting is really important). We have the salutations, so we can come up with the idea of their age.","3d45c997":"## SECOND PART - PREDICTION MODELS","31bd2cbe":"Another important feature seems to be SibSp, which tell us if the person is alone or with his\/her family. Let's take a look into it.","4f03cd54":"As we expected, it seems that people who went in FirstClass survived more than people who travelled in ThirdClass. Over the 50% of people took the ship in ThirdClass meanwhile about the 25% took it in FirstClass. But, let's go a bit deeper on it. ","f99ab204":"We can see that families with 5 or more members had 0% survirval rate. This is very interesting. Let's try to figure it out why!","ea2387a5":"Now we have a brief summary of the data. There are some missing values in Embarked, Age and Cabin features, so we'll have to deal with them. There are many different approaches to deal with missing values, so we'll decide later which one is better for us. Anyway, we can go into more detail from here.\n\nOne of the first questions we have to answer is: how many people survived the disaster? Let's make a quick visualization.","628703c1":"Another classic model is the Random Forest.","f221f46b":"Wow! That was a clever solution by ash316, we solved the problem of the age! Let's take a look to some other features. Maybe, in importance order, where they embarked  could be important.","c4eaa136":"Let's see what happens if we try it with more complex models like Light GBM.","76964fe5":"Now we can clearly see that is better to travel in FirstClass, as we expected ^^\n\nMaybe the name of the people who went in the Titanic, with our objectives, is not relevant, we'll deal with this later. For now, the next feature we can take a look is the age. Being young is better in order to survive?","0400a23f":"Ok, now we can see that there are not correlation about we said, but we explored it a little bit more!\n\nRemember we had 2 missing values in Embarked. We could fill the gaps with the mean but, we are working now with a categorical feature, so we cannot work like we do with numbers. Most of the people embarked in S, so let's fill the gaps with S.","a8ab405b":"So we can see that people who embarked in C survived more than in the other ports. It could be interesting to see if those people travelled in FirstClass.","c9e717d9":"Jonathan Lices Mart\u00edn\n\n<hr>\n\nThis notebook is a recopilation of what I've learned during my training process in python. Some of the functions I'll use here may not be totally mine. So I have to thanks everyone in Kaggle for the help, the functions and models templates :)\n\n<hr>\n\nTitanic was a British passenger liner operated by the White Star Line that sank in the North Atlantic Ocean in the early morning hours of 15 April 1912, after striking an iceberg during her maiden voyage from Southampton to New York City. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making the sinking one of modern history's deadliest peacetime commercial marine disasters [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Titanic). In this notebook I'll try to predict if the survival of this disaster depends on the characteristics we have in the dataset with some different models.","e7327fae":"As we can see, families with more of 5 members were all in ThirdClass, so this may be the reason of their survirval rate.\n\nAnother feature that seems to be important is the fare, maybe because of its relation with Pclass.","96d526b1":"Now we can see it better. Of 491 people who travelled in ThirdClass just 119 survived meanwhile of 216 who travelled in FirstClass just 80 died. Moreover, women who travelled in FirstClass have a survirval rate of 0.968%, that is to say, just one or two women died on the accident in FirstClass. We can visualize this.","cfb71168":"Well, the minimum value is 0!, I think I wouldn't have taken this trip even for free ^^' \n\nLast but not least.. we have 687 missing values in Cabin. We'll have to deal with this. But first, let's see the correlation between vars.","46fbc462":"### Exploratory Data Analysis\n\nSo, let's analyse the data. Before we go into the detail, it's always great to have a general idea of what we're going to work with. We have to understand the problem and the data, so print the column names and try to have a brief description of each feature could help us. There are also categorical features so we'll have to deal with them later and, of course, we have non-relevant information. We can expect missing data too, so we could take a first look on it.","f0ec3893":"The first thing we have to do is to load the principal libraries to make a great data exploratoroy analysis.","95b14ac8":"So the oldest person in the ship had 80 years, and the yougest one... 0.42 years? Well this is actually not a problem for us.","8e2a6455":"### Data Preprocess\n\nNow, we're ready to preprocess the data in order to build our model. We'll prepare the data, so we can drop those features that are non-relevant at all.","d2c438ac":"We've tried enough models for today. We had a 84% accuracy which is good but we could improve it more. \n\nThanks to all for having a look at this notebook. Hope you like it and found it useful! :)"}}