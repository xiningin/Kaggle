{"cell_type":{"97944edf":"code","6d93c1f5":"code","2b2c7561":"code","0dcf026a":"code","035800e6":"code","54ab402f":"code","0c5cf575":"code","96cb570d":"code","5e10445f":"code","cf0b83cd":"code","dc195409":"code","89f9283c":"code","d1b8a149":"code","9d064cb8":"code","0fcb97e8":"code","44252f94":"code","d5602f78":"code","53aec035":"code","e30115f6":"code","ba05dc64":"code","68b59e6f":"code","08388b64":"code","fdeec989":"code","61ef13e5":"code","195033b3":"code","f877b29e":"code","5396faac":"code","b106e6d4":"code","c6bec500":"code","1b05a549":"code","2441d43f":"code","baeb39f7":"code","442db633":"code","870e62de":"code","1677b910":"code","97a9b75f":"code","0d5f4935":"code","9cdb934e":"code","53f69d35":"code","45ff875d":"code","ea6599f1":"code","f4058d8b":"code","7cf2153d":"code","2328aad1":"code","eee11371":"code","6b18e92b":"code","7c9b09af":"code","7c89fb3a":"markdown","4635bc2b":"markdown","e4c6ac6a":"markdown","1eb67637":"markdown","876fbb30":"markdown","8296866a":"markdown","1e373ff2":"markdown","fe66bb57":"markdown","b0d38da2":"markdown","5590d3f6":"markdown","7f4478a8":"markdown","22d0c273":"markdown","322449e0":"markdown","fd39f27a":"markdown","4a5d10d3":"markdown","54e5ccdf":"markdown","bb44d784":"markdown","043ad17a":"markdown","485c50d7":"markdown","e6904290":"markdown"},"source":{"97944edf":"!pip install gensim==3.8.3","6d93c1f5":"from shutil import copytree\n\ncopytree(src=\"..\/input\/softserve-ai-hackathon-2021\/baseline\/utils\", dst=\".\/utils\/\")","2b2c7561":"import os\nimport json\nimport multiprocessing\nfrom multiprocessing import Pool\nfrom functools import partial\n\nimport pandas as pd\nfrom tqdm import tqdm\nfrom spacy.lang.en import English\nfrom gensim.models import Word2Vec\n\nfrom utils.dataset import create_dataframe, split_sentence, split_words\nfrom utils.modelling import preparing_data_w2v\nfrom utils.prediction import search_similar_text\nfrom utils.processing import get_text, get_files_path\nfrom utils.vectors import get_avg_vector, postprocess_vectors","0dcf026a":"cores = multiprocessing.cpu_count()\nprint('There are {} CPU cores.'. format(cores))","035800e6":"# spacy rule-based matching \nnlp_eng = English()\nsentencizer = nlp_eng.create_pipe(\"sentencizer\")\nnlp_eng.add_pipe(sentencizer)","54ab402f":"folder_list = [\n    'Ashcroft', \n    'Density&Viscosity', \n    'Flow', \n    'Gas_analysis', \n    'Level', \n    'Liquid _analysis',\n    'Pressure', \n    'Temperature', \n    'Valves_actuators'\n]\n\nfolder_data = '..\/input\/softserve-ai-hackathon-2021\/baseline'\nfolder_created_data = '.\/'\n\nfolder_json = 'text_json'\nfolder_json_processed = 'data_text_processed'\n\ndataset_texts = 'data_texts.pkl'\ndataset_names = 'device2document_map.pkl'\ndataset_vectors = 'data_vectors.pkl'\nmodel_name = 'word2vec.model'\n\npath_jsons = os.path.join(folder_data, folder_json)\npath_json_processed = os.path.join(folder_created_data, folder_json_processed)\n\npath_dataset_texts = os.path.join(folder_created_data, dataset_texts)\npath_dataset_names = os.path.join(folder_data, dataset_names)\npath_dataset_vectors = os.path.join(folder_created_data, dataset_vectors)\npath_model = os.path.join(folder_created_data, model_name)","0c5cf575":"# path to random json file\nrandom_file = 'Pressure\/manual-rosemount-2051-profibus-pa-protocol-en-88384.pdf\/output-31-to-40.json'\nrandom_file_path = os.path.join(path_jsons, random_file)\nrandom_file_path","96cb570d":"with open(random_file_path) as f:\n    data_json = json.load(f)\ndata_json.keys()","5e10445f":"data_json['inputConfig']","cf0b83cd":"# text chunk\ntext_chunk = ''\nfor paragraph in data_json['responses'][0]['fullTextAnnotation']['pages'][0]['blocks'][0]['paragraphs']:\n    for word in paragraph['words']:\n        text_chunk = text_chunk + ''.join([symbol['text'] for symbol in word['symbols']]) + ' '\nprint(text_chunk)","dc195409":"# coordinates chunk\ndata_json['responses'][0]['fullTextAnnotation']['pages'][0]['blocks'][0]['boundingBox']['normalizedVertices']","89f9283c":"# checking how many files in respective folders\nfor fold, f_list in get_files_path(path_jsons, folder_list).items():\n    print('Folder: {}.\\nNumber files for processing: {} \\n'.format(fold, len(f_list)))","d1b8a149":"number_processes = 3\nfiles_list_dict = get_files_path(path_jsons, folder_list)\npartial_args = partial(get_text, folder_save=path_json_processed, lang_threshold=0.5)\nwith Pool(processes=number_processes) as pool:\n    %time pool.map(partial_args, list(files_list_dict.values()))","9d064cb8":"dataframe_list = []\nfolder_exist_list = [item.split('.')[0] for item in os.listdir(path_json_processed)]\n\nfor folder in tqdm(folder_exist_list):\n    if folder in folder_list:\n        file_processed_path = '{}\/{}.json'.format(path_json_processed, folder)\n        dataframe_list.append(create_dataframe(file_processed_path, doc_path_folder='s3:\/\/hackathon-baseline\/duai_docs\/'))\ndata = pd.concat(dataframe_list).reset_index(drop=True)\nprint('There are {} rows in created DataFrame'.format(data.shape[0]))","0fcb97e8":"file_processed_path","44252f94":"data.head()","d5602f78":"# split text into sentences\n%time data['text_sentences'] = data['text'].apply(lambda t: split_sentence(t, nlp_eng, 10))","53aec035":"# split sentences into words\n%time data['text_words'] = data['text_sentences'].apply(lambda t: [split_words(sent, nlp_eng, 2) for sent in t])","e30115f6":"# remove records without sentences\ndata = data[data['text_words'].apply(len)!=0].reset_index(drop=True)\nprint('There are {} rows after removing records without sentences...'.format(data.shape[0]))\ndata.head()","ba05dc64":"data.to_pickle(path_dataset_texts)","68b59e6f":"# dataset initialization\ndata = pd.read_pickle(path_dataset_texts)\nprint('There are {} rows in loaded DataFrame'.format(data.shape[0]))","08388b64":"%time sentences_list = preparing_data_w2v(data)","fdeec989":"# example of sentences for modelling \nprint(sentences_list[:5])","61ef13e5":"model = Word2Vec(min_count=10,\n                 window=4,\n                 size=300,\n                 sample=6e-5, \n                 alpha=0.025, \n                 min_alpha=0.0001, \n                 negative=20,\n                 workers=cores-2)","195033b3":"%time model.build_vocab(sentences_list, progress_per=10000)","f877b29e":"%time model.train(sentences_list, total_examples=model.corpus_count, epochs=30, report_delay=1)","5396faac":"model.wv['water'].shape","b106e6d4":"model.wv.most_similar(positive=[\"water\"])","c6bec500":"model.save(path_model)","1b05a549":"# dataset initialization\ndata = pd.read_pickle(path_dataset_texts)\nprint('There are {} rows in loaded DataFrame'.format(data.shape[0]))\ndata.head()","2441d43f":"# model initialization\nmodel = Word2Vec.load(path_model)\nprint('Model Initialization: {}'.format(model))","baeb39f7":"# creating vectors\n%time data['text_vectors'] = data['text_words'].apply(lambda x: [get_avg_vector(sent, model) for sent in x])\n%time data['sentences'] = data.apply(lambda x: list(zip(x['text_sentences'], x['text_vectors'])), axis=1)","442db633":"# checking if number of sentences equal to number of vectors\ndata[data['text_sentences'].apply(len)!=data['text_vectors'].apply(len)]","870e62de":"%time data_vectors = postprocess_vectors(data)","1677b910":"data_vectors['page_class_coordinate'].head().values","97a9b75f":"data_vectors.to_pickle(path_dataset_vectors)","0d5f4935":"# device names dataset initialization\ndata_names = pd.read_pickle(path_dataset_names)\nprint('The dimensionality of the DataFrame: {}'.format(data_names.shape))\ndata_names.head()","9cdb934e":"data_names.head().values","53f69d35":"# vectors dataset initialization\ndata_vectors = pd.read_pickle(path_dataset_vectors)\nprint('The dimensionality of the DataFrame: {}'.format(data_vectors.shape))\ndata_vectors.head()","45ff875d":"# model initialization\nmodel = Word2Vec.load(path_model)\nprint('Model Initialization: {}'.format(model))","ea6599f1":"# creating possible search dataset\ninput_dict = [\n    {'manufacturer':'micro motion', 'device_model':7827, 'text':'instrumentation cable requirements'},\n    {'manufacturer':'micro motion', 'device_model':5700, 'text':'setting flow units'},\n    {'manufacturer':'rosemount', 'device_model':2051, 'text':'power module installation'},\n    {'manufacturer':'rosemount', 'device_model':2051, 'text':'how to set device tag'},\n    {'manufacturer':'rosemount', 'device_model':2051, 'text':'communicator connection'},\n    {'manufacturer':'rosemount', 'device_model':3410, 'text':'CPU module replacement'},\n            ]\ndata_input = pd.DataFrame(input_dict)\ndata_input","f4058d8b":"# search parameters \ntop = 5\nquery_input = data_input.iloc[3]\n\n%time data_output = search_similar_text(query_input, data_vectors, data_names, 'text_vectors', model, nlp_eng, top)\ndata_output","7cf2153d":"data_output['page_class_coordinate'].values","2328aad1":"def process_output(output_df: pd.DataFrame, query: str):\n    doc_page = output_df['page_class_coordinate'].apply(lambda x: x[0][0])\n    answers = ' '.join(doc_page.tolist())\n    return pd.DataFrame([[query, answers]], columns=['query', 'answers'])","eee11371":"query = query_input.manufacturer + ' ' + query_input.device_model.astype(str) + ' ' + query_input.text\nsubmission_df = process_output(data_output, query)\nsubmission_df","6b18e92b":"submission_df.iloc[0].answers","7c9b09af":"import csv\nsubmission_df.to_csv(os.path.join(folder_created_data, 'submission.csv'), index=False, quoting=csv.QUOTE_NONE)","7c89fb3a":"# Data Processing\n\nAfter passing flow  PDF documents are converted into following data sets:\n<li>documents (pdf);<\/li>\n<li>document pages (images);<\/li>\n<li>document texts with coordinates (json files);<\/li>\n<li>texts embeddings (pickle file);<\/li>\n<li>company and device names (pickle file).<\/li>","4635bc2b":"# DATA SCIENCE HACKATHON: INTELLIGENT SEARCH ENGINE\nThis guide demonstrates a basic solution for our competition. Go through the steps and see which things you can change to get a better score on the leaderboard.","e4c6ac6a":"#### File Usage\n\nThere are [3 files](https:\/\/www.kaggle.com\/c\/softserve-ds-hackathon-2021\/data) provided for the baseline solution:\n<li>text_json - folder contains parsed text. The text was parsed by Cloud Vision API. Apart from texts the model also returns text coordinates on page;<\/li>\n<li>documents - folder contains PDF documents;<\/li>\n<li>decive2document_map.pkl - pandas DataFrame contains the mapping of a document, device class, device manufacturer, and device model.<\/li>","1eb67637":"### Download Data","876fbb30":"#### Convert output to Kaggle submission format","8296866a":"## Initialization","1e373ff2":"### Vectors","fe66bb57":"### Data & Model Initialization","b0d38da2":"## Import","5590d3f6":"### Evaluation\nYour model is going to be evaluated on MAP score. You can find [here](https:\/\/www.kaggle.com\/c\/softserve-ds-hackathon-2021\/overview\/evaluation) more information about this metric.\n\n### Submission\nUsing your chosen model, you are going to make predictions on unseen competition data. Then you are going to download and submit your predictions and see your team name in the Leaderboard.","7f4478a8":"Text preprocessing steps for Word2Vec models include:\n<li>removing stop words;<\/li>\n<li>removing numbers and words with numbers;<\/li>\n<li>removing short words.<\/li>","22d0c273":"### Word2vec","322449e0":"## Inference","fd39f27a":"Let\u2019s try to find relevant places in different documents for a user's queries. To retrieve best similar texts for a new query we would have to:\n<li>Split the query into: text, company and device name.<\/li>\n<li>Encode query text into the same model we used for vectors creating.<\/li>\n<li>Filter texts based on company and device names.<\/li>\n<li>Retrieve most similar text chunks and IDs.<\/li>","4a5d10d3":"Each document from the dataset contains information about single or multiple  devices and manufacturer companies. In most cases this information is located on the first pages. But very often device models appear in text a lot.\n\n![image.png](attachment:17dbab0d-c902-4bb4-a049-ca3c49c4c7f9.png)\n<center>Company and device name (random dataset page): red - company name, purple - device name<\/center>","54e5ccdf":"### Processing all files","bb44d784":"### Search","043ad17a":"### Exploring Random JSON file","485c50d7":"### Creating Dataset","e6904290":"## Vector Encoder"}}