{"cell_type":{"823c0ea9":"code","defff581":"code","2301ba67":"code","1a40650c":"code","101507a1":"code","bd8a4e48":"code","33763b2b":"code","a3f17f4c":"code","681b6466":"code","9c44bb0a":"code","8fe517b1":"code","693f0679":"code","c538f7af":"code","2558da45":"code","10d8fe7d":"code","6b5dab8f":"code","cf958e3e":"code","502c2802":"code","bb7ca606":"code","99112017":"code","d6af5ef6":"code","7918eb05":"code","3cbede1c":"code","64c81e2e":"code","ac11d794":"code","a46ec8d2":"code","5d4cdabe":"code","161cc8f1":"code","24c2baca":"code","ab65b10c":"code","48a0321e":"markdown","01b021c2":"markdown","7bd71a57":"markdown","92637143":"markdown","f9c344b4":"markdown","586a0f90":"markdown","718a2855":"markdown","aa779cb7":"markdown","a451da68":"markdown","121b2ee8":"markdown","c0f9e4fa":"markdown","e70a1d3a":"markdown","b1fdab81":"markdown","e0c9508e":"markdown","a3b85d22":"markdown","fa0e7b30":"markdown","d815f24e":"markdown","2d6809d6":"markdown","a9a89892":"markdown"},"source":{"823c0ea9":"NORMALIZE_Y = True","defff581":"# Common\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Model helpers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\n\n# Model algorithms\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\n\n# Etc.\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2301ba67":"df_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_train.head()","1a40650c":"len(df_train)","101507a1":"df_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf_test.head()","bd8a4e48":"# Descriptive statistics summary\ndf_train['SalePrice'].describe()","33763b2b":"# Histogram\nplt.title('Normal')\nsns.distplot(df_train['SalePrice'], fit=stats.norm)","a3f17f4c":"# Skewness and kurtosis\n# See https:\/\/medium.com\/@atanudan\/kurtosis-skew-function-in-pandas-aa63d72e20de\nprint('Skewness: {:.2f}'.format(df_train['SalePrice'].skew()))\nprint('Kurtosis: {:.2f}'.format(df_train['SalePrice'].kurt()))","681b6466":"# Scatter plot GrLivArea\/SalePrice\nvar = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","9c44bb0a":"# Scatter plot TotalBsmtSF\/SalePrice\nvar = 'TotalBsmtSF'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","8fe517b1":"# Box plot OverallQual\/SalePrice\nvar = 'OverallQual'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y='SalePrice', data=data)\nfig.axis(ymin=0, ymax=800000);","693f0679":"var = 'YearBuilt'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","c538f7af":"# Correlation matrix\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","2558da45":"# SalePrice correlation matrix\nk = 10  # number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","10d8fe7d":"# Scatterplot\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(df_train[cols], size = 2.5)\nplt.show();","6b5dab8f":"# numeric_cols = df_train.select_dtypes(include = ['int64', 'float64']).columns.tolist()\n# categorical_cols = df_train.select_dtypes('object').columns.tolist()","cf958e3e":"# Finding numeric features\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in df_train.columns:\n    if df_train[i].dtype in numeric_dtypes:\n        if i in ['TotalSF', 'Total_Bathrooms', 'Total_porch_sf', 'haspool', 'hasgarage', 'hasbsmt', 'hasfireplace']:\n            pass\n        else:\n            numeric.append(i)     \n# Visualising some more outliers in the data values\nfig, axs = plt.subplots(ncols=2, figsize=(12, 120))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\nsns.color_palette(\"husl\", 8)\nfor i, feature in enumerate(list(df_train[numeric]), 1):\n    if(feature=='MiscVal'):\n        break\n    plt.subplot(len(list(numeric)), 3, i)\n    sns.scatterplot(x=feature, y='SalePrice', hue='SalePrice', palette='Blues', data=df_train)\n        \n    plt.xlabel('{}'.format(feature), size=15, labelpad=12.5)\n    plt.ylabel('SalePrice', size=15, labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(loc='best', prop={'size': 10})\n        \nplt.show()","502c2802":"# Missing data\ntotal = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data[missing_data['Total'] > 0].head(20)","bb7ca606":"# Dealing with missing data - drop missing columns\ndf_train = df_train.drop((missing_data[missing_data['Total'] > 1]).index,1)\ndf_train = df_train.drop(df_train.loc[df_train['Electrical'].isnull()].index)\ndf_train.isnull().sum().max()  # Just checking that there's no missing data missing...","99112017":"# Remove outliers\ndf_train.drop(df_train[(df_train['OverallQual']<5) & (df_train['SalePrice']>200000)].index, inplace=True)\ndf_train.drop(df_train[(df_train['GrLivArea']>4500) & (df_train['SalePrice']<300000)].index, inplace=True)\ndf_train.reset_index(drop=True, inplace=True)","d6af5ef6":"if NORMALIZE_Y:\n    # log(1+x) transform\n    df_train['SalePrice'] = np.log1p(df_train['SalePrice'])\n\n    # df_train['SalePrice'] = np.log(df_train['SalePrice'])","7918eb05":"if NORMALIZE_Y:\n    sns.set_style('white')\n    sns.set_color_codes(palette='deep')\n    f, ax = plt.subplots(figsize=(8, 7))\n    # Check the new distribution \n    sns.distplot(df_train['SalePrice'] , fit=stats.norm, color='b');\n\n    # Get the fitted parameters used by the function\n    (mu, sigma) = stats.norm.fit(df_train['SalePrice'])\n    print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n    # Now plot the distribution\n    plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n                loc='best')\n    ax.xaxis.grid(False)\n    ax.set(ylabel='Frequency')\n    ax.set(xlabel='SalePrice')\n    ax.set(title='SalePrice distribution')\n    sns.despine(trim=True, left=True)\n\n    plt.show()","3cbede1c":"if NORMALIZE_Y:\n    # OR Transformed histogram and normal probability plot\n    sns.distplot(df_train['SalePrice'], fit=stats.norm);\n    fig = plt.figure()\n    res = stats.probplot(df_train['SalePrice'], plot=plt)","64c81e2e":"numeric_features = ['TotalBsmtSF', 'GrLivArea', 'YearBuilt', 'YrSold']\ncategorical_features = ['OverallQual', 'MSZoning']\nfeatures = numeric_features + categorical_features\n\n# pd_train.dropna()\nX = df_train[features]\nX_final_test = df_test[features] # For submission\ny = df_train['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nX_train.head()","ac11d794":"# See https:\/\/stackoverflow.com\/questions\/48507651\/multiple-classification-models-in-a-scikit-pipeline-python\n# https:\/\/stackoverflow.com\/questions\/51695322\/compare-multiple-algorithms-with-sklearn-pipeline\n\nfrom sklearn.base import BaseEstimator\nfrom sklearn.linear_model import SGDRegressor\n\nclass RegSwitcher(BaseEstimator):\n\n    def __init__(\n        self, \n        estimator = SGDRegressor(),\n    ):\n        \"\"\"\n        A Custom BaseEstimator that can switch between classifiers.\n        :param estimator: sklearn object - The classifier\n        \"\"\"\n        self.estimator = estimator\n\n    def fit(self, X, y=None, **kwargs):\n        self.estimator.fit(X, y)\n        return self\n\n    def predict(self, X, y=None):\n        return self.estimator.predict(X)\n\n    def predict_proba(self, X):\n        return self.estimator.predict_proba(X)\n\n    def score(self, X, y):\n        return self.estimator.score(X, y)","a46ec8d2":"# Feature transformation\n# See https:\/\/scikit-learn.org\/stable\/auto_examples\/compose\/plot_column_transformer_mixed_types.html\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('ohe', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)],\n    remainder='passthrough'\n)","5d4cdabe":"# Estimators\nestimators = [\n    ('preprocessor', preprocessor),\n    ('reg', RegSwitcher()),\n]\n\n# Pipeline\npipeline = Pipeline(steps=estimators)","161cc8f1":"# Tune hyperparamters and find the champion model.\n\nparameters = [\n    {\n        'reg__estimator': [RandomForestRegressor(random_state=1)],\n        'reg__estimator__n_estimators': [100, 200, 1000],\n        'reg__estimator__max_depth': [5, 6],\n    },\n     {\n        'reg__estimator': [GradientBoostingRegressor(random_state=1)],\n        'reg__estimator__n_estimators': [100, 200, 1000],\n        'reg__estimator__max_depth': [5, 6],\n    }\n]\n\ngsearch = GridSearchCV(estimator=pipeline, \n                       param_grid=parameters, \n                       cv=5, \n                       n_jobs=12, \n                       return_train_score=False, \n                       verbose=1)\ngsearch.fit(X_train, y_train)\n\nprint('Best score(R2): {:.4f}'.format(gsearch.best_score_))\nprint('Best params: {}'.format(gsearch.best_params_))\nprint('Test score(R2): {:.4f}'.format(gsearch.score(X_test, y_test)))\n\nif NORMALIZE_Y:\n    # print('Residual sum of squares: {:.4f}'.format(np.mean((gsearch.predict(X_test) - y_test) ** 2)))\n    rmse = np.sqrt(mean_squared_error(y_test, gsearch.predict(X_test)))\n    print('RMSE: {:.4f}'.format(rmse))\nelse:\n    rmsle = np.sqrt(mean_squared_log_error(y_test, gsearch.predict(X_test)))\n    print('RMSLE: {:.4f}'.format(rmsle))","24c2baca":"predictions = gsearch.predict(X_final_test)\nif NORMALIZE_Y:\n    predictions = np.floor(np.expm1(predictions))\n\nsubmission = pd.DataFrame({'Id': df_test.Id, 'SalePrice': predictions})\nsubmission","ab65b10c":"# Fix outlier predictions\n# q1 = submission['SalePrice'].quantile(0.0045)\n# q2 = submission['SalePrice'].quantile(0.99)\n# submission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\n# submission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\n\nsubmission.to_csv('submission.csv', index=False)\nprint('Your submission was successfully saved!')","48a0321e":"# Prediction for Submission","01b021c2":"#### Relationship with numerical variables","7bd71a57":"# Feature Engineering","92637143":"### SalePrice","f9c344b4":"The SalePrice is skewed to the right as seen in the EDA. This is a problem because most ML models don't do well with non-normally distributed data. We can apply a log(1+x) tranform to fix the skew.","586a0f90":"### Outliers\nThe outliers may cause overfitting of our model.","718a2855":"# Data Preparation","aa779cb7":"### Missing Data","a451da68":"# Exploratory Data Analysis","121b2ee8":"#### Relationship with categorical features","c0f9e4fa":"### Skewness and Normality","e70a1d3a":"Let's plot the SalePrice again. The SalePrice is now normally distributed.","b1fdab81":"### Correlation matrix","e0c9508e":"# Base Models","a3b85d22":"### Feature Selection and Transformation","fa0e7b30":"#### 'SalePrice' correlation matrix (zoomed heatmap style)","d815f24e":"# References\n- https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n- https:\/\/www.kaggle.com\/lavanyashukla01\/how-i-made-top-0-3-on-a-kaggle-competition\n- https:\/\/www.kaggle.com\/jesucristo\/1-house-prices-solution-top-1","2d6809d6":"# Import Libraries","a9a89892":"#### Scatter plots between 'SalePrice' and correlated variables"}}