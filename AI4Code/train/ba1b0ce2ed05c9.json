{"cell_type":{"45fde083":"code","6c1eefe8":"code","1b2e94e4":"code","65ae622d":"code","ff118204":"code","a1350439":"code","3ff1d0e4":"code","14e63424":"code","8067bcea":"code","4c79af57":"code","0fd5b2f5":"code","4e62e719":"code","05ab8680":"code","2685be39":"code","06557e79":"code","26b4337e":"code","7291cc17":"code","a4a5f214":"code","726d8438":"code","767040fd":"code","27c0f9e9":"code","b0abf2cd":"code","0c7652c6":"code","9b4a1567":"code","8ee49255":"code","b56ff4ee":"code","2f038ef7":"code","c513a165":"code","9ae5f2c6":"code","24b0fd9e":"code","ca5cbb44":"code","9c0245ab":"code","edafd6dd":"code","3148ffbb":"code","a6b1d4ba":"code","ea13909f":"code","d0ceb5bb":"code","e06e1738":"code","ed515fdf":"code","0c86dda9":"code","815c530a":"code","8a5bc69b":"code","533d95b3":"code","42ec675d":"code","6bc4bc37":"code","5da18f77":"code","e49b4f01":"code","2fd688e2":"code","a89cc992":"code","a6e06f7f":"code","7cde1030":"code","bb657eee":"code","431cafdb":"code","225c7362":"code","94583d56":"code","96a6e79e":"code","58697a7a":"code","361f24f4":"code","b610e6ca":"code","a2cb114d":"code","1e909395":"code","8accf1dd":"code","b85c743d":"code","db1c4f18":"code","dd630d5f":"markdown","6613effc":"markdown","d8f05c11":"markdown","981730f3":"markdown","e261f294":"markdown","b3faafa7":"markdown","79ba232b":"markdown","dcc1e0cc":"markdown","349d2d1d":"markdown","8e889d34":"markdown","61f1f800":"markdown","7b8279a3":"markdown","ab8d9e47":"markdown","d7e90561":"markdown","931f95dc":"markdown","7bd58eaa":"markdown","2be2255f":"markdown","d55a8d68":"markdown","4f31cf3d":"markdown","33cfc7e8":"markdown","8389a112":"markdown","75fecfae":"markdown","58a70546":"markdown","c841e184":"markdown","8f2b2dd6":"markdown","9b345b29":"markdown","6016fbf3":"markdown","2e636ee9":"markdown","e2b4347f":"markdown","1a314f69":"markdown","7ad229ad":"markdown","2881f9d7":"markdown","ed30770d":"markdown"},"source":{"45fde083":"import numpy as np\n\nimport pandas as pd \npd.set_option('display.max_rows', 500)\n\nimport gc\n\n# sklearn preprocessing \nfrom sklearn.metrics import confusion_matrix, roc_auc_score ,roc_curve,auc\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\n\n# LightGBM is a gradient boosting framework that uses tree based learning algorithms.\nimport lightgbm as lgb\n\n# File system manangement\nimport os\n\n#eda\n!pip install klib\nimport klib\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nseed = 1234\nnp.random.seed(seed)","6c1eefe8":"# Training data\ndf_train = pd.read_csv('..\/input\/widsdatathon2021\/TrainingWiDS2021.csv')\nprint('Training data shape: ', df_train.shape)\ndf_train.head()","1b2e94e4":"# Dropping the first column\ndf_train.drop('Unnamed: 0',axis=1,inplace=True)\nprint('Training data shape: ', df_train.shape)\ndf_train.head()","65ae622d":"# Testing data features\ndf_test = pd.read_csv('..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv')\ndf_test.drop('Unnamed: 0',axis=1,inplace=True)\nprint('Testing data shape: ', df_test.shape)\ndf_test.head()","ff118204":"df_train.info(verbose=True, null_counts=True)","a1350439":"# Number of each type of column\ndf_train.dtypes.value_counts()","3ff1d0e4":"# Number of unique classes in each object column\ndf_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","14e63424":"# Function to calculate missing values by column# Funct \n\ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","8067bcea":"# Missing values for training data\nmissing_values_train = missing_values_table(df_train)\nmissing_values_train[:160].style.background_gradient(cmap='Greens')","4c79af57":"# Missing values for testing data\nmissing_values_test = missing_values_table(df_test)\nmissing_values_test[:159].style.background_gradient(cmap='Greens')","0fd5b2f5":"df_cleaned_train = klib.data_cleaning(df_train, drop_threshold_cols=0.3)\nklib.missingval_plot(df_cleaned_train)","4e62e719":"df_train['diabetes_mellitus'].value_counts(normalize=True)","05ab8680":"df_train['diabetes_mellitus'].astype(int).plot.hist();","2685be39":"plt.figure(figsize=(10,6))\nplt.title(\"Age vs people suffering from Diabetes\")\nsns.lineplot(df_train[\"age\"],df_train[\"diabetes_mellitus\"])\nplt.xlabel(\"Age\")\nplt.ylabel(\"Number of people Suffering\")","06557e79":"plt.figure(figsize=(10,6))\nplt.title(\"Age vs people suffering from Diabetes\")\nsns.lineplot(data=df_train, x=\"age\", y=\"diabetes_mellitus\", hue=\"gender\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Number of people Suffering\")","26b4337e":"# Ethnicity vs Diabetes \ng = sns.catplot(x=\"ethnicity\", hue=\"gender\", col=\"diabetes_mellitus\",\n                data=df_train, kind=\"count\",\n                height=5, aspect=1.5);","7291cc17":"#Display correlation with a target variable of interest\nklib.corr_plot(df_train, target='diabetes_mellitus')","a4a5f214":"train_copy = df_train.copy()\ntest_copy = df_test.copy()\n\n# drop columns with a lot of missing values\ntrain_copy = klib.data_cleaning(train_copy, drop_threshold_cols=0.3)\ntrain_copy.columns\n","726d8438":"selected_columns = train_copy.columns\nselected_columns = np.delete(selected_columns, 104)\ntest_copy = test_copy[selected_columns]\ntest_copy","767040fd":"train_copy.drop(['encounter_id', 'icu_stay_type', 'icu_type'],axis=1,inplace=True)\ntest_copy.drop(['encounter_id', 'icu_stay_type', 'icu_type'],axis=1,inplace=True)","27c0f9e9":"df_train['hospital_id'].isin(df_test['hospital_id']).value_counts()","b0abf2cd":"# Dropping hospital id also\ntrain_copy.drop('hospital_id',axis=1,inplace=True)\ntest_copy.drop('hospital_id',axis=1,inplace=True)","0c7652c6":"# Drop missing age values on the training set\ntrain_copy.dropna(subset=['age'],inplace=True)\nlen(train_copy)","9b4a1567":"train_copy['height'].fillna(train_copy['height'].mean(), inplace = True)\ntest_copy['height'].fillna(train_copy['height'].mean(), inplace = True)","8ee49255":"train_copy['weight'].fillna(train_copy['weight'].mean(), inplace = True)\ntest_copy['weight'].fillna(train_copy['weight'].mean(), inplace = True)","b56ff4ee":"#function returning Series\ndef calculate_bmi(x):\n    return round(x.weight\/(x.height*x.height\/10000),3)\n\ntrain_copy['bmi'] = train_copy.apply(calculate_bmi, axis=1)","2f038ef7":"train_copy.head()","c513a165":"test_copy['bmi'] = test_copy.apply(calculate_bmi, axis=1)\ntest_copy.head()","9ae5f2c6":"train_copy['ethnicity'].fillna('Other\/Unknown', inplace = True)\ntest_copy['ethnicity'].fillna('Other\/Unknown', inplace = True)","24b0fd9e":"# Performing one-hot encoding\ntrain_copy = pd.concat([train_copy,pd.get_dummies(train_copy['ethnicity'], prefix='ethnicity')],axis=1)\ntrain_copy.drop(['ethnicity'],axis=1, inplace=True)","ca5cbb44":"test_copy = pd.concat([test_copy,pd.get_dummies(test_copy['ethnicity'], prefix='ethnicity')],axis=1)\ntest_copy.drop(['ethnicity'],axis=1, inplace=True)","9c0245ab":"train_copy.dropna(subset=['gender'],inplace=True)\ntest_copy['gender'].fillna('M', inplace = True)","edafd6dd":"# Performing one-hot encoding\ntrain_copy = pd.concat([train_copy,pd.get_dummies(train_copy['gender'], prefix='gender')],axis=1)\ntrain_copy.drop(['gender'],axis=1, inplace=True)\n\ntest_copy = pd.concat([test_copy,pd.get_dummies(test_copy['gender'], prefix='gender')],axis=1)\ntest_copy.drop(['gender'],axis=1, inplace=True)","3148ffbb":"train_copy['hospital_admit_source'].fillna('Other', inplace = True)\ntest_copy['hospital_admit_source'].fillna('Other', inplace = True)","a6b1d4ba":"train_copy = pd.concat([train_copy,pd.get_dummies(train_copy['hospital_admit_source'], prefix='has')],axis=1)\ntrain_copy.drop(['hospital_admit_source'],axis=1, inplace=True)\n\ntest_copy = pd.concat([test_copy,pd.get_dummies(test_copy['hospital_admit_source'], prefix='has')],axis=1)\ntest_copy.drop(['hospital_admit_source'],axis=1, inplace=True)","ea13909f":"test_copy['has_PACU']=0\ntest_copy['has_Acute Care\/Floor']=0\ntest_copy['has_ICU']=0\ntest_copy['has_Observation']=0\ntest_copy['has_Other']=0","d0ceb5bb":"train_copy['icu_admit_source'].fillna('Accident & Emergency', inplace = True)\ntest_copy['icu_admit_source'].fillna('Accident & Emergency', inplace = True)","e06e1738":"train_copy = pd.concat([train_copy,pd.get_dummies(train_copy['icu_admit_source'], prefix='ias')],axis=1)\ntrain_copy.drop(['icu_admit_source'],axis=1, inplace=True)\n\ntest_copy = pd.concat([test_copy,pd.get_dummies(test_copy['icu_admit_source'], prefix='ias')],axis=1)\ntest_copy.drop(['icu_admit_source'],axis=1, inplace=True)","ed515fdf":"train_copy.isnull().sum()","0c86dda9":"train_copy.fillna(-9999, inplace=True)\ntrain_copy.isnull().sum()","815c530a":"test_copy.fillna(-9999, inplace=True)\ntest_copy.isnull().sum()","8a5bc69b":"# split the all-data DF into training and testing again\ntraining = train_copy\ntesting = test_copy","533d95b3":"print(training.shape)\nprint(testing.shape)","42ec675d":"TARGET = 'diabetes_mellitus'\ntrain_labels = training[TARGET]\ntrain = training.drop(columns = [TARGET])\nfeatures = list(train.columns)\ntest = testing\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","6bc4bc37":"train.info(verbose=True, null_counts=True)","5da18f77":"# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C = 0.0001)\n\n# Train on the training data\nlog_reg.fit(train, train_labels)","e49b4f01":"# Make predictions\n# Make sure to select the second column only\nlog_reg_pred = log_reg.predict_proba(test)[:, 1]","2fd688e2":"log_reg_pred ","a89cc992":"# Submission dataframe\nsubmit = df_test[['encounter_id']]\nsubmit['diabetes_mellitus'] = log_reg_pred\nsubmit.to_csv('logreg_baseline.csv',index=False)\nsubmit.head()","a6e06f7f":"# Make the random forest classifier\nrf = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","7cde1030":"# Train on the training data\nrf.fit(train, train_labels)\n\n# Extract feature importances\nfeature_importance_values = rf.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n\n# Make predictions on the test data\npredictions = rf.predict_proba(test)[:, 1]","bb657eee":"# Make a submission dataframe\n# Submission dataframe\nsubmit = df_test[['encounter_id']]\nsubmit['diabetes_mellitus'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline_domain.csv', index = False)","431cafdb":"def plot_feature_importances(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n        \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","225c7362":"# Show the feature importances for the default features\nfeature_importances_sorted = plot_feature_importances(feature_importances)","94583d56":"numerical_columns = train.columns[~train.columns.isin(categorical_columns)]","96a6e79e":"X = train\ny = train_labels\n\ntrain_X, val_X, train_y, val_y = train_test_split( X, y, test_size=0.20,random_state=0)","58697a7a":"# Logistic Regression\n\nlr = LogisticRegression()\nlr.fit(train_X, train_y)\npredictions2 = lr.predict_proba(val_X)[:,1]\nroc_auc_score(val_y, predictions2)\n\n\n","361f24f4":"# RF\n\nrf.fit(train_X, train_y)\npredictions3 = rf.predict_proba(val_X)[:,1]\nroc_auc_score(val_y, predictions3)\n","b610e6ca":"# Light GBM\n\nd_train=lgb.Dataset(train_X, label=train_y)\n\n#Specifying the parameter\nparams={}\nparams['learning_rate']=0.03\nparams['boosting_type']='gbdt' #GradientBoostingDecisionTree\nparams['objective']='binary' #Binary target feature\nparams['metric']='binary_logloss' #metric for binary classification\nparams['max_depth']=10\n\n#train the model \nclf=lgb.train(params,d_train,100) #train the model on 100 epocs\n\npredictions_lgb = clf.predict(val_X)\nroc_auc_score(val_y, predictions_lgb)","a2cb114d":"# Log Reg baseline with CV\nscores_log = cross_val_score(lr, X, y, cv=10, scoring='roc_auc')\nscores_log.sort()\nprint('Mean Absolute Score %2f' %(scores_log.mean()))\n","1e909395":"# RF baseline with CV\n\nscores_rf = cross_val_score(rf, X, y, cv=10, scoring='roc_auc')\nscores_rf.sort()\nprint('Mean Absolute Score %2f' %(scores_rf.mean()))","8accf1dd":"kf = StratifiedKFold(n_splits=3,shuffle=False,random_state=seed)\npred_test_full = 0\ncv_score =[]\ni=1\nfor train_index,test_index in kf.split(X,y):\n    print('{} of KFold {}'.format(i,kf.n_splits))\n    xtr,xvl = X.loc[train_index],X.loc[test_index]\n    ytr,yvl = y[train_index],y[test_index]\n    \n    #model\n    lr = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n    #lr = LogisticRegression(C = 0.0001)\n    lr.fit(xtr,ytr)\n    score = roc_auc_score(yvl,lr.predict_proba(xvl)[:,1])\n    print('ROC AUC score:',score)\n    cv_score.append(score)    \n    pred_test = lr.predict_proba(test)[:,1]\n    pred_test_full +=pred_test\n    i+=1","b85c743d":"print('Confusion matrix\\n',confusion_matrix(yvl,lr.predict(xvl)))\nprint('Cv',cv_score,'\\nMean cv Score',np.mean(cv_score))","db1c4f18":"proba = lr.predict_proba(xvl)[:,1]\nfrp,trp, threshold = roc_curve(yvl,proba)\nroc_auc_ = auc(frp,trp)\n\nplt.figure(figsize=(10,6))\nplt.title('Reciever Operating Characteristics')\nplt.plot(frp,trp,'r',label = 'AUC = %0.2f' % roc_auc_)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'b--')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')","dd630d5f":"# Baseline\n\n### 1. Logistic Regression","6613effc":"let's check if there is overlap between the hospitals in the labelled dataset and the hospitals in the unlabelled dataset","d8f05c11":"The test set is considerably smaller and lacks a TARGET column.","981730f3":"The predictions must be in the format shown in the sample_submission.csv file, where there are only two columns: encounter_id and diabetes_mellitus are present. We will create a dataframe in this format from the test set and the predictions called submit.\n\n","e261f294":"### split the all-data DF into training and testing again","b3faafa7":"# Exploratory Data Analysis\n\n##  Column Types","79ba232b":"#### Ethnicity","dcc1e0cc":"## Correlation Plot\n\n","349d2d1d":"# Feature Engineering\n\nDepending on your model, your dataset may contain too many features for it to handle.Hence you can eliminate the ones which donot appear very high on the feature importance leaderboard.","8e889d34":"The training data has 130157 observations and 180 variables including the TARGET (the label we want to predict).","61f1f800":"# Using cross validation for more robust error measurement\nUsing a Validation dataset has a drawback. Firstly, it decreases the training data and secondly since it is tested against a small amount of data, it has high chances of overfitting. To overcome this, there is a technique called cross validation. The most common form of cross validation, and the one we will be using, is called k-fold cross validation. \u2018Fold\u2019 refers to each different iteration that we train our model on, and \u2018k\u2019 just refers to the number of folds. In the diagram above, we have illustrated k-fold validation where k is 5.\n\n![](https:\/\/scikit-learn.org\/stable\/_images\/grid_search_cross_validation.png)\n\n\nsource: https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html","7b8279a3":"## Missing values","ab8d9e47":"### Dropping unnecessary columns\nThere were no recurring patient visits, so the encounter_id would not be relevant to our models","d7e90561":"#### Hospital admit source feature","931f95dc":"#### Height, weight and BMI","7bd58eaa":"Depending upon the percentage of missing values, you can decide to drop the columns which have a very high percentage of missing values and see if that improves the results.Let's drop the empty and single valued columns as well as empty and duplicate rows and visualise the clean dataframe. I'll do it for train but the same needs to be done for the test dataframe as well.","2be2255f":"The graph above gives a high-level overview of the missing values in a dataset. It pinpoints which columns and rows to examine in more detail.\n\nTop portion of the plot shows the aggregate for each column. Summary statistics is displayed on the right most side.\n\nBottom portion of the plot shows the missing values (black colors) in the DataFrame.","d55a8d68":"#### ICU admit source feature","4f31cf3d":"### 2. Random Forest","33cfc7e8":"### This notebook was strongly based on the example presented during the WiDS Datathon 2021 Workshop held on 13th Jan,2021. https:\/\/www.kaggle.com\/parulpandey\/starter-code-with-baseline","8389a112":"We can also visualise the missign values instead of looking at the numbers. ","75fecfae":"## Cross-Validation for Imbalanced Classification\nA better way of splitting data would be to split it in such a way that maintains the same class distribution in each subset.we can use a version of k-fold cross-validation that preserves the imbalanced class distribution in each fold. It is called stratified k-fold cross-validation and will enforce the class distribution in each split of the data to match the distribution in the complete training dataset.\n\nLogistic regression is used for modelling. The data set is split using Stratified Kfold. In each split model is created and predicted using that model. The final predicted value is average of all model.","58a70546":"## Read in Data","c841e184":"## The Target Column","8f2b2dd6":"## Preprocessing Data\n\n","9b345b29":"### Handling Missing Values and Encoding Categorical Variables","6016fbf3":"## Reciever Operating Characteristics","2e636ee9":"#### Age feature","e2b4347f":"## Some things that you could try to improve performance:\n\n* Check for outliers\n* Efficient Missing value handling\n* Hyperparameter tuning\n\nWhen we do hyperparameter tuning, it's crucial to not tune the hyperparameters on the testing data. We can only use the testing data a single time when we evaluate the final model that has been tuned on the validation data.\n\n* Ensembling\n","1a314f69":"#### Gender","7ad229ad":"This is an example of imbalanced class problem i.e we where the total number of a class of data (0) is far less than the total number of another class of data (1). Examples include:\n* Fraud detection.\n* Outlier detection.\n","2881f9d7":"# Evaluating the model\n\nWe have created our model but how will we know how accurate it is? We do have a Test dataset but since it doesn't have the Target column, everytime we optimize our model, we will have to submit our predictions to public Leaderboard to assess it accuracy.\n\n### Creating a Validation set\nAnother option would be to create a validation set from the training set. We will hold out a part of the training set during the start of the experiment and use it for evaluating our predictions. We shall use the scikit-learn library's model_selection.train_test_split() function that we can use to split our data","ed30770d":"### All other features"}}