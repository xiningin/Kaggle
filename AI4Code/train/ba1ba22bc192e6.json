{"cell_type":{"3d925674":"code","647cc496":"code","866b3ea1":"code","408c94c1":"code","09e12b2b":"code","791a98fb":"code","2ceb7ec6":"code","eb49a4b4":"code","1364316d":"code","74c3f009":"code","a5b6e834":"code","6468223d":"code","7b0bc382":"code","c5f4747a":"code","0aa1d72e":"code","2d65b768":"code","a26d2247":"code","4be3c2e4":"code","a53303c7":"code","0766500e":"code","a86cd304":"code","ef86bc5d":"code","04a839c9":"code","38a92068":"code","4bdc4a14":"code","a1e2e1cd":"code","f11be51d":"code","00f21a3d":"code","455464fe":"code","aae82cef":"code","e77d41fd":"code","8056ffc6":"markdown","cb60bec5":"markdown","a702fd03":"markdown","4cb6ca6e":"markdown","aca5dd5c":"markdown","85b1c155":"markdown","435cf8f4":"markdown","04cd3d26":"markdown"},"source":{"3d925674":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","647cc496":"#import the required libraries\nimport gc\nimport os\nimport logging\nimport datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport lightgbm as lgb\nfrom scipy.signal import hann\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nfrom scipy.signal import hilbert\nfrom scipy.signal import convolve\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold, KFold\nwarnings.filterwarnings('ignore')","866b3ea1":"train_data = pd.read_csv('..\/input\/train.csv', dtype = {'acoustic_data': np.int16, 'time_to_failure': np.float32})","408c94c1":"#let's look at the training data\nprint(train_data.head())\nprint(100*'-')\nprint(train_data.shape)","09e12b2b":"#let's prepare our data\nseg_length = 150000\ntotal_samples = int(np.floor((train_data.shape[0]) \/ seg_length))\n\n#we will be using a total of nine different features as given below for making our predictions\nCOLUMNS = ['AVERAGE', 'STD', 'MAX', 'MIN', 'SUM', \n           'AVERAGE_FIRST_10000', 'AVERAGE_LAST_10000', 'AVERAGE_FIRST_50000', 'AVERAGE_LAST_50000',\n           'STD_FIRST_10000', 'STD_LAST_10000', 'STD_FIRST_50000', 'STD_LAST_50000',\n           'ABS_AVERAGE', 'ABS_STD', 'ABS_MAX', 'ABS_MIN',\n           '10Q', '25Q', '50Q', '75Q', '90Q', \n           'ABS_1Q', 'ABS_5Q', 'ABS_30Q', 'ABS_60Q', 'ABS_95Q', 'ABS_99Q',\n           'KURTOSIS', 'SKEW', 'MEDIAN',\n           'HILBERT_MEAN', 'HANN_WINDOW_MEAN']\ncols = COLUMNS #our features used for the prediction\nx_train = pd.DataFrame(index = range(total_samples), columns = cols, dtype = np.float64) #an empty dataframe holding our feature values\ny_train = pd.DataFrame(index = range(total_samples), columns = ['time_to_failure'], dtype = np.float64) #an empty dataframe holding our target labels","791a98fb":"for value in tqdm_notebook(range(total_samples)):\n    sample = train_data.iloc[value*seg_length : value*seg_length + seg_length]\n    x = pd.Series(sample['acoustic_data'].values)\n    y = sample['time_to_failure'].values[-1]\n    \n    y_train.loc[value, 'time_to_failure'] = y\n    \n    x_train.loc[value, 'AVERAGE'] = x.mean()\n    x_train.loc[value, 'STD'] = x.std()\n    x_train.loc[value, 'MAX'] = x.max()\n    x_train.loc[value, 'MIN'] = x.min() \n    x_train.loc[value, 'SUM'] = x.sum()\n    \n    x_train.loc[value, 'AVERAGE_FIRST_10000'] = x[:10000].mean()\n    x_train.loc[value, 'AVERAGE_LAST_10000']  =  x[-10000:].mean()\n    x_train.loc[value, 'AVERAGE_FIRST_50000'] = x[:50000].mean()\n    x_train.loc[value, 'AVERAGE_LAST_50000'] = x[-50000:].mean()\n    \n    x_train.loc[value, 'STD_FIRST_10000'] = x[:10000].std()\n    x_train.loc[value, 'STD_LAST_10000']  =  x[-10000:].std()\n    x_train.loc[value, 'STD_FIRST_50000'] = x[:50000].std()\n    x_train.loc[value, 'STD_LAST_50000'] = x[-50000:].std()\n    \n    x_train.loc[value, 'ABS_AVERAGE'] = np.abs(x).mean()\n    x_train.loc[value, 'ABS_STD'] = np.abs(x).std()\n    x_train.loc[value, 'ABS_MAX'] = np.abs(x).max()\n    x_train.loc[value, 'ABS_MIN'] = np.abs(x).min()\n    \n    x_train.loc[value, '10Q'] = np.percentile(x, 0.10)\n    x_train.loc[value, '25Q'] = np.percentile(x, 0.25)\n    x_train.loc[value, '50Q'] = np.percentile(x, 0.50)\n    x_train.loc[value, '75Q'] = np.percentile(x, 0.75)\n    x_train.loc[value, '90Q'] = np.percentile(x, 0.90)\n    \n    x_train.loc[value, 'ABS_1Q'] = np.percentile(x, np.abs(0.01))\n    x_train.loc[value, 'ABS_5Q'] = np.percentile(x, np.abs(0.05))\n    x_train.loc[value, 'ABS_30Q'] = np.percentile(x, np.abs(0.30))\n    x_train.loc[value, 'ABS_60Q'] = np.percentile(x, np.abs(0.60))\n    x_train.loc[value, 'ABS_95Q'] = np.percentile(x, np.abs(0.95))\n    x_train.loc[value, 'ABS_99Q'] = np.percentile(x, np.abs(0.99))\n    \n    x_train.loc[value, 'KURTOSIS'] = x.kurtosis()\n    x_train.loc[value, 'SKEW'] = x.skew()\n    x_train.loc[value, 'MEDIAN'] = x.median()\n    \n    x_train.loc[value, 'HILBERT_MEAN'] = np.abs(hilbert(x)).mean()\n    x_train.loc[value, 'HANN_WINDOW_MEAN'] = (convolve(x, hann(150), mode = 'same') \/ sum(hann(150))).mean()","2ceb7ec6":"x_train.head() #our training dataframe holding our features","eb49a4b4":"y_train.head() #our training dataframe holding our output labels","1364316d":"print(x_train.shape)\nprint(y_train.shape)","74c3f009":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error","a5b6e834":"#normalizing the data\nscaler = StandardScaler()\nscaler.fit(x_train)\nx_train_scaled = scaler.transform(x_train)\n\ny_train_flatten = y_train.values.ravel() #flattening the y_train","6468223d":"#let's look at the mormalized data\nx_train_dataframe = pd.DataFrame(x_train_scaled)\nx_train_dataframe.head(10)","7b0bc382":"#import the libraries for building the neural net\nimport keras\nfrom keras.layers import Dense\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Activation\nfrom keras.layers import Dropout\nfrom keras.models import Sequential\nfrom keras import optimizers","c5f4747a":"kernel_init = 'he_normal'\ndef Model(input_dim, activation, classes):\n    model = Sequential()\n\n#     model.add(Dense(512, kernel_initializer = kernel_init, input_dim = input_dim))\n#     model.add(Activation(activation))\n#     model.add(BatchNormalization())\n#     model.add(Dense(32, kernel_initializer = kernel_init, input_dim = input_dim))\n#     model.add(Activation(activation))\n#     model.add(BatchNormalization())\n#     model.add(Dropout(0.2))\n\n    model.add(Dense(32, kernel_initializer = kernel_init)) \n    model.add(Activation(activation))\n    model.add(BatchNormalization())\n    model.add(Dense(32, kernel_initializer = kernel_init)) \n    model.add(Activation(activation))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n\n    model.add(Dense(32, kernel_initializer = kernel_init))    \n    model.add(Activation(activation))\n    model.add(BatchNormalization())\n    model.add(Dense(32, kernel_initializer = kernel_init))    \n    model.add(Activation(activation))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n    \n    model.add(Dense(32, kernel_initializer = kernel_init))    \n    model.add(Activation(activation))\n    model.add(BatchNormalization())\n    model.add(Dense(32, kernel_initializer = kernel_init))    \n    model.add(Activation(activation))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n\n    model.add(Dense(classes, kernel_initializer = kernel_init))    \n    model.add(Activation('linear'))\n    \n    return model","0aa1d72e":"input_dim = x_train.shape[1]\nactivation = 'tanh'\nclasses = 1 #the output labels\nmodel = Model(input_dim = input_dim, activation = activation, classes = classes)\n\n#model.summary()","2d65b768":"#compile the model\noptim = optimizers.Adam(lr = 0.001)\nmodel.compile(loss = 'mean_absolute_error', optimizer = optim, metrics = ['mean_absolute_error'])","a26d2247":"submission = pd.read_csv('..\/input\/sample_submission.csv', index_col = 'seg_id')\nX_test = pd.DataFrame(columns = x_train.columns, dtype = np.float64, index = submission.index)\n\nfor id in X_test.index:\n    seg = pd.read_csv('..\/input\/test\/' + id + '.csv')\n    \n    x = pd.Series(seg['acoustic_data'].values)\n    \n    X_test.loc[id, 'AVERAGE'] = x.mean()\n    X_test.loc[id, 'STD'] = x.std()\n    X_test.loc[id, 'MAX'] = x.max()\n    X_test.loc[id, 'MIN'] = x.min() \n    X_test.loc[id, 'SUM'] = x.sum()\n    \n    X_test.loc[id, 'AVERAGE_FIRST_10000'] = x[:10000].mean()\n    X_test.loc[id, 'AVERAGE_LAST_10000']  =  x[-10000:].mean()\n    X_test.loc[id, 'AVERAGE_FIRST_50000'] = x[:50000].mean()\n    X_test.loc[id, 'AVERAGE_LAST_50000'] = x[-50000:].mean()\n    \n    X_test.loc[id, 'STD_FIRST_10000'] = x[:10000].std()\n    X_test.loc[id, 'STD_LAST_10000']  =  x[-10000:].std()\n    X_test.loc[id, 'STD_FIRST_50000'] = x[:50000].std()\n    X_test.loc[id, 'STD_LAST_50000'] = x[-50000:].std()\n    \n    X_test.loc[id, 'ABS_AVERAGE'] = np.abs(x).mean()\n    X_test.loc[id, 'ABS_STD'] = np.abs(x).std()\n    X_test.loc[id, 'ABS_MAX'] = np.abs(x).max()\n    X_test.loc[id, 'ABS_MIN'] = np.abs(x).min()\n    \n    X_test.loc[id, '10Q'] = np.percentile(x, 0.10)\n    X_test.loc[id, '25Q'] = np.percentile(x, 0.25)\n    X_test.loc[id, '50Q'] = np.percentile(x, 0.50)\n    X_test.loc[id, '75Q'] = np.percentile(x, 0.75)\n    X_test.loc[id, '90Q'] = np.percentile(x, 0.90)\n    \n    X_test.loc[id, 'ABS_1Q'] = np.percentile(x, np.abs(0.01))\n    X_test.loc[id, 'ABS_5Q'] = np.percentile(x, np.abs(0.05))\n    X_test.loc[id, 'ABS_30Q'] = np.percentile(x, np.abs(0.30))\n    X_test.loc[id, 'ABS_60Q'] = np.percentile(x, np.abs(0.60))\n    X_test.loc[id, 'ABS_95Q'] = np.percentile(x, np.abs(0.95))\n    X_test.loc[id, 'ABS_99Q'] = np.percentile(x, np.abs(0.99))\n    \n    X_test.loc[id, 'KURTOSIS'] = x.kurtosis()\n    X_test.loc[id, 'SKEW'] = x.skew()\n    X_test.loc[id, 'MEDIAN'] = x.median()\n    \n    X_test.loc[id, 'HILBERT_MEAN'] = np.abs(hilbert(x)).mean()\n    X_test.loc[id, 'HANN_WINDOW_MEAN'] = (convolve(x, hann(150), mode = 'same') \/ sum(hann(150))).mean()","4be3c2e4":"print(X_test.shape)","a53303c7":"#normalizing our testing data\nX_test_scaled = scaler.transform(X_test)\nprint(X_test_scaled.shape)","0766500e":"input_dim = x_train.shape[1]\nactivation = 'tanh'\nclasses = 1\n\nhistory = dict() #dictionery to store the history of individual models for later visualization\nprediction_scores = dict() #dictionery to store the predicted scores of individual models on the test dataset\n\n#here we will be training the same model for a total of 10 times and will be considering the mean of the output values for predictions\nfor i in np.arange(0, 10):\n    optim = optimizers.Adam(lr = 0.001)\n    ensemble_model = Model(input_dim = input_dim, activation = activation, classes = classes)\n    ensemble_model.compile(loss = 'mean_absolute_error', optimizer = optim, metrics = ['mean_absolute_error'])\n    print('TRAINING MODEL NO : {}'.format(i))\n    H = ensemble_model.fit(x_train_scaled, y_train_flatten,\n                  batch_size = 64,\n                  epochs = 200,\n                  verbose = 1)\n    history[i] = H\n    \n    ensemble_model.save('MODEL_{}.model'.format(i))\n    \n    predictions = ensemble_model.predict(X_test_scaled, verbose = 1, batch_size = 64)\n    prediction_scores[i] = predictions","a86cd304":"#making predictions\nprediction1 = np.hstack([p.reshape(-1,1) for p in prediction_scores.values()]) #taking the scores of all the trained models\nprediction1 = np.mean(prediction1, axis = 1)\n\nprint(prediction1.shape)","ef86bc5d":"import lightgbm as lgb\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV","04a839c9":"# fixed parameters\nparams = {'boosting_type': 'gbdt',\n          'objective': 'regression_l1',\n          'nthread': 4,\n          'num_leaves': 64,\n          'learning_rate': 0.05,\n          'max_bin': 512,\n          'colsample_bytree': 1,\n          'reg_alpha': 0.1,\n          'reg_lambda': 0.1,\n          'min_split_gain': 0.1,\n          'min_child_weight': 0.1,\n          'min_child_samples': 20,\n          'metric' : 'mae'}","38a92068":"gbm = lgb.LGBMRegressor(boosting_type= 'gbdt',\n                        objective = 'regression_l1',\n                        n_jobs = 4,\n                        silent = True,\n                        max_bin = params['max_bin'],\n                        min_split_gain = params['min_split_gain'],\n                        min_child_weight = params['min_child_weight'],\n                        min_child_samples = params['min_child_samples'])","4bdc4a14":"folds = KFold(n_splits = 12, shuffle = True, random_state = 101)\n\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(x_train)):\n    print('Fold', fold_n)\n    X_train_f, X_valid = x_train.iloc[train_index], x_train.iloc[valid_index]\n    y_train_f, y_valid = y_train.iloc[train_index], y_train.iloc[valid_index]\n\n    gbm.fit(X_train_f, y_train_f.values.flatten(),\n            eval_set=[(X_valid, y_valid.values.flatten())],\n            eval_metric = 'mae',\n            early_stopping_rounds = 200)\n\n    print('Starting predicting...')\n    # predict\n    y_pred = gbm.predict(X_valid, num_iteration = gbm.best_iteration_)\n    # eval\n    print('The mae of prediction is:', mean_absolute_error(y_valid.values.flatten(), y_pred))\n\n# feature importances\nprint('Feature importances:', list(gbm.feature_importances_))","a1e2e1cd":"#sunmitting the file\nsubmission['time_to_failure'] = prediction1\nsubmission.to_csv('ensemble_submission.csv')","f11be51d":"lgbm_predictions = gbm.predict(X_test)","00f21a3d":"submission['time_to_failure'] = lgbm_predictions\nprint(submission.head())\nsubmission.to_csv('lgbm_submission.csv')","455464fe":"submission['time_to_failure'] = 0.5 * lgbm_predictions + 0.5 * prediction1\nsubmission.to_csv('lgbm_ensemble_submission.csv')","aae82cef":"submission['time_to_failure'] = 0.2 * lgbm_predictions + 0.8 * prediction1\nsubmission.to_csv('0.2lgbm_ensemble_0.8submission.csv')","e77d41fd":"submission['time_to_failure'] = 0.8 * lgbm_predictions + 0.2 * prediction1\nsubmission.to_csv('0.8lgbm_ensemble_0.2submission.csv')","8056ffc6":"### MAKING SUBMISSIONS","cb60bec5":"## SIMPLE NEURAL NETWORK","a702fd03":"3. LGBM and ENSEMBLE","4cb6ca6e":"**Here we will be using two types of neural networks. The first network is a simple feed forward neural network with the following architecture:**\n\n***1. BLOCK1:  ((DENSE(32 filters) ---> ACTIVATION ---> BN) x 2) -----> DROPOUT***\n\n***2. BLOCK2 : ((DENSE(64 filters) ---> ACTIVATION ---> BN) x 2) -----> DROPOUT***\n\n***3. BLOCK3:  ((DENSE(128 filters) ---> ACTIVATION ---> BN) x 2) -----> DROPOUT***\n\n***4. BLOCK4(output layer):  (DENSE(classes) ---> ACTIVATION 2)***\n\n**The second type of the network will also be a feed forward neural net but unlike in the first one where we trained our dataset on a single neural net, here we will be training our dataset on multiplt neural networks. Each neural net will have its own output scores. The mean of all the individual scores will be considered to be the final otuput score. This type of model is called as Ensemble Model. Ensemble learning ususally helps us to achieve high performance of the model on our input data, hence such models are generally used instead of a sinlge model of neural network.**","aca5dd5c":"## ENSEMBLE NEURAL NETWROK","85b1c155":"1. Emsemble model","435cf8f4":"## LightGBM","04cd3d26":"2. LGBM model"}}