{"cell_type":{"ced3b14f":"code","37cd00c0":"code","d9cd6b05":"code","04f90a87":"code","6c10e06f":"code","8de546e5":"code","288a5c31":"code","7fe29cde":"code","c22e0e7c":"code","54a0df94":"code","be76e7cd":"code","7414a735":"code","ad0f5800":"code","c43d581c":"code","86662e17":"code","797bdb2a":"code","fc6f4f99":"code","e6a2a988":"code","0f1531e8":"code","975463eb":"code","67073824":"code","aba99625":"code","4b1cccd0":"code","32979585":"code","f4bc3fbd":"code","5d08bf28":"code","8556bc1f":"code","a0ff7133":"code","2b5cc55c":"code","5aabeb34":"code","0b678b58":"code","21bf11d7":"code","6c45bfb7":"code","69cd9a9a":"code","4e1323f5":"code","a9c23510":"code","b2fa0d85":"markdown","998d145c":"markdown","9acc4a13":"markdown","046bd42d":"markdown","a55160ea":"markdown","cf3f7fcd":"markdown","ff0151ea":"markdown","3ba324a9":"markdown","8959f236":"markdown","fd6c8410":"markdown","2b5c0fe8":"markdown","cb1da3fd":"markdown","d5fc06cd":"markdown","f93e6f71":"markdown","7d68f876":"markdown","a8b67c5d":"markdown","b5563b94":"markdown","702fb1e4":"markdown"},"source":{"ced3b14f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib as plt\nimport seaborn as sns\nimport string\n%matplotlib inline \n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","37cd00c0":"dataset = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', encoding =\"ISO-8859-1\" ,\n                 names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])","d9cd6b05":"dataset.head()","04f90a87":"dataset.info()","6c10e06f":"dataset.describe(include = 'O')","8de546e5":"# Printing the length of the dataset\nprint(\"Dataset length : {}\".format(len(dataset)))","288a5c31":"print(\"Dataset shape : {}\".format(dataset.shape))","7fe29cde":"#Checking for null values\ndataset.isnull().any()","c22e0e7c":"dataset.isnull().sum()","54a0df94":"# Checking the target values\ndataset.target.value_counts()","be76e7cd":"g = sns.countplot(dataset['target'], data=dataset)\ng.set_xticklabels([\"Negative\", \"Positive\"], rotation=0)","7414a735":"dataset.user.value_counts()","ad0f5800":"import re","c43d581c":"def remove_noise(text):\n    # Dealing with Punctuation\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","86662e17":"# Applying the remove_noise function on the dataset\n\ndataset['text'] = dataset['text'].apply(lambda x : remove_noise(x))","797bdb2a":"dataset.head()","fc6f4f99":"dataset['text'] = dataset['text'].apply(lambda x : x.lower())","e6a2a988":"dataset.head()","0f1531e8":"dataset['text'][12314]","975463eb":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nstop = stopwords.words('english')","67073824":"# Removing stopwords\n\ndef remove_stopwords(text):\n    text = [item for item in text.split() if item not in stop]\n    return ' '.join(text)\n\ndataset['cleaned_data'] = dataset['text'].apply(remove_stopwords)","aba99625":"dataset.head()","4b1cccd0":"from nltk.stem.porter import PorterStemmer\n\nstemmer = PorterStemmer()\n\ndef stemming(text):\n    text = [stemmer.stem(word) for word in text.split()]\n    return ' '.join(text)\n\ndataset['stemed_text'] = dataset['cleaned_data'].apply(stemming)\n\ndataset.head()","32979585":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nfig, (ax1) = plt.subplots(1, figsize=[7, 7])\nwordcloud = WordCloud( background_color='white', width=600, height=600).generate(\" \".join(dataset['stemed_text']))\n\nax1.imshow(wordcloud)\nax1.axis('off')\nax1.set_title('Frequent Words',fontsize=16);","f4bc3fbd":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n\ntfidf = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n\nx = tfidf.fit_transform(dataset['stemed_text'])","5d08bf28":"tfidf.get_feature_names()[:20]","8556bc1f":"tfidf.get_params()","a0ff7133":"print(x[0].todense())","2b5cc55c":"y = dataset['target']","5aabeb34":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    See full source and example: \n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n    \n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","0b678b58":"from sklearn.model_selection import train_test_split\nimport itertools\nimport numpy as np\nfrom sklearn.metrics import precision_score, roc_auc_score, recall_score, confusion_matrix, roc_curve, precision_recall_curve, accuracy_score, classification_report\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=42)","21bf11d7":"from sklearn.naive_bayes import MultinomialNB\n\nmultinb = MultinomialNB(alpha=1.9)\nmultinb.fit(x_train, y_train)\n\nmulti_predict = multinb.predict(x_test)\nmultinb_accuracy_score = accuracy_score(y_test, multi_predict)\nprint(\"The Accuracy score for MultinomialNB is : {}\".format(multinb_accuracy_score))\n\nmultinb_conf_mat = confusion_matrix(y_test, multi_predict)\nplot_confusion_matrix(multinb_conf_mat, classes = ['FAKE', 'REAL'])","6c45bfb7":"%%time\n\nmultinb_classifier = MultinomialNB(alpha=0.1)\n\nprevious_score = 0\n\n# We are taking values from 0 to 1 with an increament of 0.1 \n\nfor alpha in np.arange(0,2,0.1):\n    sub_classifier = MultinomialNB(alpha=alpha)\n    sub_classifier.fit(x_train, y_train)\n    y_pred = sub_classifier.predict(x_test)\n    score = accuracy_score(y_test, y_pred)\n    \n    if score> previous_score:\n        classifier = sub_classifier\n        print(\"Alpha is : {} & Accuracy is : {}\".format(alpha, score))","69cd9a9a":"from sklearn.naive_bayes import BernoulliNB\n\nbernoullinb = BernoulliNB(alpha=2)\nbernoullinb.fit(x_train, y_train)\n\nbernoulli_pred = bernoullinb.predict(x_test)\n\nbernoulli_acc_score = accuracy_score(y_test, bernoulli_pred)\nprint(\"The Accuracy score for BernoulliNB is {} : \".format(bernoulli_acc_score))\n\nprint(\"=======================================================================================\")\n\nbernoullinb_conf_mat = confusion_matrix(y_test, bernoulli_pred)\nplot_confusion_matrix(bernoullinb_conf_mat, classes = ['FAKE', 'REAL'])\n\n\nbernoullinb_class_report = classification_report(y_test, bernoulli_pred)\nprint(bernoullinb_class_report)","4e1323f5":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression()\nlog_reg.fit(x_train, y_train)\nlog_pred = log_reg.predict(x_test)\n\nlog_reg_conf_mat = confusion_matrix(y_test, log_pred)\nplot_confusion_matrix(log_reg_conf_mat, classes = ['FAKE', 'REAL'])\n\n\nlog_reg_class_report = classification_report(y_test, log_pred)\nprint(log_reg_class_report)","a9c23510":"import pickle\n\nfile = open('vectoriser-ngram-(1,2).pickle','wb')\npickle.dump(tfidf, file)\nfile.close()\n\nfile = open('Sentiment-LR.pickle','wb')\npickle.dump(log_reg, file)\nfile.close()\n\nfile = open('Sentiment-BNB.pickle','wb')\npickle.dump(bernoullinb, file)\nfile.close()","b2fa0d85":"* We can clearly see that the Logistic Regression Model performs the best out of all the different models that we tried. It achieves nearly 80% accuracy while classifying the sentiment of a tweet.\n\n* Although it should also be noted that the BernoulliNB Model is the fastest to train and predict on. It also achieves 78% accuracy while calssifying.\n","998d145c":"### 1. Importing dependencies","9acc4a13":"* Converting the uppercase letters to lower case","046bd42d":"## Saving the models","a55160ea":"### Applying BernoulliNB","cf3f7fcd":"### 2.     Importing dataset","ff0151ea":"### 6. Creating the models","3ba324a9":"* Plotting the distribution for the dataset","8959f236":"## Hyperparameter tuning for MultinomialNB","fd6c8410":"### The tweets have been annotated **(0 = Negative, 4 = Positive)** and they can be used to detect sentiment.","2b5c0fe8":"\nIntroduction\n\n   * **Natural Language Processing (NLP)**: The discipline of computer science, artificial intelligence and linguistics that is concerned with the creation of computational models that process and understand natural language. These include: making the computer understand the semantic grouping of words (e.g. cat and dog are semantically more similar than cat and spoon), text to speech, language translation and many more\n\n   * **Sentiment Analysis**: It is the interpretation and classification of emotions (positive, negative and neutral) within text data using text analysis techniques. Sentiment analysis allows organizations to identify public sentiment towards certain words or topics.\n\nIn this notebook, we'll develop a **Sentiment Analysis** model to categorize a tweet as **Positive or Negative**.\n\n### Table of Contents:\n\n    1.     Importing dependencies\n    2.     Importing dataset\n    3.     Preprocessing Text\n    4.     Analysing data\n    5.     Splitting data\n    6.     TF-IDF Vectoriser\n    7.     Transforming Dataset\n    8.     Creating and Evaluating Models\n    9.         * BernoulliNB Model\n    10.        * LinearSVC Model\n    11.        * Logistic Regression Model\n    12.     Saving the Models\n    13.     Using the Model\n \n\n","cb1da3fd":"## Applying MultinomialNB\n\n* Na\u00efve Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong independence assumptions between the features. They are among the simplest Bayesian network models","d5fc06cd":"###     4.     Analysing data\n","f93e6f71":"###    5.     TF-IDF Vectorizer\n\n\n### Using TF-IDF Vectorizer to convert tweets into vectors\n\nTF-IDF indicates what the importance of the word is in order to understand the document or dataset. Let us understand with an example. Suppose you have a dataset where students write an essay on the topic, My House. In this dataset, the word a appears many times; it\u2019s a high frequency word compared to other words in the dataset. The dataset contains other words like home, house, rooms and so on that appear less often, so their frequency are lower and they carry more information compared to the word. This is the intuition behind TF-IDF.\n\nTF-IDF Vectoriser converts a collection of raw documents to a matrix of TF-IDF features. The Vectoriser is usually trained on only the X_train dataset.\n\nngram_range is the range of number of words in a sequence. [e.g \"very expensive\" is a 2-gram that is considered as an extra feature separately from \"very\" and \"expensive\" when you have a n-gram range of (1,2)]\n\nmax_features specifies the number of features to consider. [Ordered by feature frequency across the corpus].","7d68f876":"###    5.     Splitting data\n\n* Stemming the data","a8b67c5d":"### Applying Logistic Regression","b5563b94":"##     3.     Preprocessing Text\n \n\n* Removing punctuations from the dataset\n* Replacing punctuation with blank spaces\n* Removing html tags\n* Removing symbols\n* Removing numbers, etc.","702fb1e4":"## Removing the stopwords from the dataset"}}