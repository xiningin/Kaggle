{"cell_type":{"6303f783":"code","80da805c":"code","136f7f09":"code","bfe3020f":"code","dd5fddf5":"code","0eaacdf7":"code","26459d4b":"code","ca53167d":"code","ac436aa3":"code","0de221be":"code","efc75c0f":"code","dfb23198":"code","ffed8812":"code","73d4d9c9":"code","f852645d":"code","ff15f822":"code","98a10dcd":"code","9f5015c3":"code","6eb75ea2":"code","e284228b":"code","9cd82498":"code","609f68f2":"code","a6fe4c4e":"code","e23d7a5c":"code","21344015":"code","18fb630c":"code","180640c6":"code","d73d743e":"code","6ddc5c5c":"code","b830a8d4":"code","d18eca34":"code","2acb5aa5":"code","04865995":"code","42ed9008":"code","e295a06e":"code","d31287d4":"code","50e0ad1e":"code","b30b551b":"code","f8e7e4fd":"code","63b22055":"code","da6b61be":"code","5d0d3fcf":"code","bcc53529":"code","6b1b2a79":"code","fefa1541":"code","1e0baa17":"markdown","b01acdd3":"markdown","769fa985":"markdown","e7fd1cb1":"markdown","1d682b95":"markdown","b05f236d":"markdown","b6b48e74":"markdown","751dadc2":"markdown","78201225":"markdown","b1a94b34":"markdown","eaf6f580":"markdown","0f348811":"markdown","201c7fa9":"markdown","17b448fd":"markdown","0a028d9e":"markdown","6fc2d3ca":"markdown","d5456318":"markdown","0f71b176":"markdown","009e5b2b":"markdown","72c4337c":"markdown","6d9f0fc0":"markdown","eecdbf06":"markdown","4a5f563b":"markdown","ae5914fc":"markdown","9cc094c9":"markdown","e5331e7b":"markdown","64e2b05d":"markdown","363de1cc":"markdown","4653db68":"markdown","8be3fb50":"markdown","b7ca3e29":"markdown","02dc79bf":"markdown","38ab48e7":"markdown","47a2e8c7":"markdown","9aa55566":"markdown"},"source":{"6303f783":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom statistics import mode\nimport re\nfrom xgboost import XGBClassifier","80da805c":"df = pd.read_csv('..\/input\/h1b_kaggle.csv')","136f7f09":"df.info()\ndf.head()\ndf.describe()","bfe3020f":"df.rename( columns={'Unnamed: 0':'CASE_ID'}, inplace=True )","dd5fddf5":"df['CASE_STATUS'].unique()","0eaacdf7":"import warnings\nwarnings.filterwarnings(\"ignore\")\ndf.CASE_STATUS[df['CASE_STATUS']=='REJECTED'] = 'DENIED'\ndf.CASE_STATUS[df['CASE_STATUS']=='INVALIDATED'] = 'DENIED'\ndf.CASE_STATUS[df['CASE_STATUS']=='PENDING QUALITY AND COMPLIANCE REVIEW - UNASSIGNED'] = 'DENIED'\ndf.CASE_STATUS[df['CASE_STATUS']=='CERTIFIED-WITHDRAWN'] = 'CERTIFIED'","26459d4b":"##Drop rows with withdrawn\ndf.EMPLOYER_NAME.describe()\ndf = df.drop(df[df.CASE_STATUS == 'WITHDRAWN'].index)\n\n## Storing non null in df w.r.t. case status\ndf = df[df['CASE_STATUS'].notnull()]\ndf['CASE_STATUS'].value_counts()","ca53167d":"94364\/(94364+2818282)","ac436aa3":"##check count of NAN\ncount_nan = len(df) - df.count()\nprint(count_nan)","0de221be":"## Filling na in employer name with mode\ndf['EMPLOYER_NAME'] = df['EMPLOYER_NAME'].fillna(df['EMPLOYER_NAME'].mode()[0])","efc75c0f":"assert pd.notnull(df['EMPLOYER_NAME']).all().all()","dfb23198":"%matplotlib notebook\ndf.boxplot(column='PREVAILING_WAGE')","ffed8812":"df.PREVAILING_WAGE.max()","73d4d9c9":"np.nanpercentile(df.PREVAILING_WAGE,98)","f852645d":"df.PREVAILING_WAGE.median()","ff15f822":"np.nanpercentile(df.PREVAILING_WAGE,2)","98a10dcd":"## replacing min and max with 2 and 98 percentile\ndf.loc[df.PREVAILING_WAGE < 34028, 'PREVAILING_WAGE']= 34028\ndf.loc[df['PREVAILING_WAGE'] > 138611, 'PREVAILING_WAGE']= 138611\ndf.PREVAILING_WAGE.fillna(df.PREVAILING_WAGE.mean(), inplace = True)","9f5015c3":"## Filling na in JOB_TITLE and FULL_TIME_POSITION with mode\ndf['JOB_TITLE'] = df['JOB_TITLE'].fillna(df['JOB_TITLE'].mode()[0])\ndf['FULL_TIME_POSITION'] = df['FULL_TIME_POSITION'].fillna(df['FULL_TIME_POSITION'].mode()[0])\ndf['SOC_NAME'] = df['SOC_NAME'].fillna(df['SOC_NAME'].mode()[0])","6eb75ea2":"df['FULL_TIME_POSITION'].value_counts()","e284228b":"foo1 = df['FULL_TIME_POSITION']=='Y'\nfoo2 = df['CASE_STATUS']=='CERIFIED'\nlen(df[foo1])\/len(df)*100","9cd82498":"df = df.drop('lat', axis = 1)\ndf = df.drop('lon', axis = 1)","609f68f2":"df['EMPLOYER_NAME'].value_counts()","a6fe4c4e":"df['NEW_EMPLOYER'] = np.nan\ndf.shape","e23d7a5c":"warnings.filterwarnings(\"ignore\")\n\ndf['EMPLOYER_NAME'] = df['EMPLOYER_NAME'].str.lower()\ndf.NEW_EMPLOYER[df['EMPLOYER_NAME'].str.contains('university')] = 'university'\ndf['NEW_EMPLOYER']= df.NEW_EMPLOYER.replace(np.nan, 'non university', regex=True)","21344015":"df['SOC_NAME'].value_counts()","18fb630c":"# Creating occupation and mapping the values\nwarnings.filterwarnings(\"ignore\")\n\ndf['OCCUPATION'] = np.nan\ndf['SOC_NAME'] = df['SOC_NAME'].str.lower()\ndf.OCCUPATION[df['SOC_NAME'].str.contains('computer','programmer')] = 'Computer Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('software','web developer')] = 'Computer Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('database')] = 'Computer Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('math','statistic')] = 'Mathematical Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('predictive model','stats')] = 'Mathematical Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('teacher','linguist')] = 'Education Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('professor','Teach')] = 'Education Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('school principal')] = 'Education Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('medical','doctor')] = 'Medical Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('physician','dentist')] = 'Medical Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('Health','Physical Therapists')] = 'Medical Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('surgeon','nurse')] = 'Medical Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('psychiatr')] = 'Medical Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('chemist','physicist')] = 'Advance Sciences'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('biology','scientist')] = 'Advance Sciences'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('biologi','clinical research')] = 'Advance Sciences'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('public relation','manage')] = 'Management Occupation'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('management','operation')] = 'Management Occupation'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('chief','plan')] = 'Management Occupation'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('executive')] = 'Management Occupation'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('advertis','marketing')] = 'Marketing Occupation'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('promotion','market research')] = 'Marketing Occupation'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('business','business analyst')] = 'Business Occupation'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('business systems analyst')] = 'Business Occupation'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('accountant','finance')] = 'Financial Occupation'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('financial')] = 'Financial Occupation'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('engineer','architect')] = 'Architecture & Engineering'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('surveyor','carto')] = 'Architecture & Engineering'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('technician','drafter')] = 'Architecture & Engineering'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('information security','information tech')] = 'Architecture & Engineering'\ndf['OCCUPATION']= df.OCCUPATION.replace(np.nan, 'Others', regex=True)","180640c6":"df.OCCUPATION.value_counts()","d73d743e":"## Splitting city and state and capturing state in another variable\ndf['state'] = df.WORKSITE.str.split('\\s+').str[-1]","6ddc5c5c":"df.state.value_counts()","b830a8d4":"from sklearn import preprocessing\nclass_mapping = {'CERTIFIED':0, 'DENIED':1}\ndf[\"CASE_STATUS\"] = df[\"CASE_STATUS\"].map(class_mapping)","d18eca34":"df.head()","2acb5aa5":"test1 = pd.Series(df['JOB_TITLE'].ravel()).unique()\npd.DataFrame(test1)","04865995":"# dropping these columns\ndf = df.drop('EMPLOYER_NAME', axis = 1)\ndf = df.drop('SOC_NAME', axis = 1)\ndf = df.drop('JOB_TITLE', axis = 1)\ndf = df.drop('WORKSITE', axis = 1)\ndf = df.drop('CASE_ID', axis = 1)","42ed9008":"df1 = df.copy()","e295a06e":"df1[['CASE_STATUS', 'FULL_TIME_POSITION', 'YEAR','NEW_EMPLOYER','OCCUPATION','state']] = df1[['CASE_STATUS', 'FULL_TIME_POSITION', 'YEAR','NEW_EMPLOYER','OCCUPATION','state']].apply(lambda x: x.astype('category'))","d31287d4":"df1.info()","50e0ad1e":"X = df.drop('CASE_STATUS', axis=1)\ny = df.CASE_STATUS\n\nseed = 7\ntest_size = 0.40\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\nX_train.columns","b30b551b":"X_train.isnull().sum()","f8e7e4fd":"X_train_encode = pd.get_dummies(X_train)\nX_test_encode = pd.get_dummies(X_test)","63b22055":"y_train.head()","da6b61be":"X_train_encode.head()","5d0d3fcf":"import xgboost\ntrain_X = X_train_encode.as_matrix()\ntrain_y = y_train.as_matrix()","bcc53529":"\ngbm=xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=1, gamma=0, learning_rate=0.5, max_delta_step=0,\n       max_depth=3, max_features='sqrt', min_child_weight=1, missing=None,\n       n_estimators=100, n_jobs=1, nthread=None,\n       objective='binary:logistic', random_state=10, reg_alpha=0,\n       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n       subsample=0.8).fit(train_X, train_y)","6b1b2a79":"y_pred = gbm.predict(X_test_encode.as_matrix())","fefa1541":"print(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","1e0baa17":"There are lot of values associated with SOC_NAME, so you might want to create a new feature that will contain the important occupation of the applicant, mapping it with the SOC_NAME value. You can create a new variable called OCCUPATION. For example computer, programmer and software are all computer ocupations. This will cover the top 80% of the occupations, and minor and remaining occupations will be categorized as others.","b01acdd3":"For the the JOB_TITLE, FULL_TIME_POSITION and SOC_NAME columns, we could fill the missing values with the mode(most occuring value).","769fa985":"# Predicting the Status of H-1B Visa Applications","e7fd1cb1":"18 Missing Values for Employer name\n\nFilling Mode value for the Missing Employer name","1d682b95":"What if the the University string is in upper case in some of the rows and in lower case in some other rows? If you are mapping lowercase university it would then miss the uppercase UNIVERSITY and vice-versa. So in order to correctly map and check the University string, you should first convert all the strings into the same case; either lowercase or uppercase.\n\nAll the strings in EMPLOYER_NAME containing the keyword university will have 'university' as value in the NEW_EMPLOYER column. All the remaining empty rows will be filled with 'non university'.","b05f236d":"The next feature is FULL_TIME_POSITION","b6b48e74":"Changing dtype to category: Now before moving to the modeling part, you should definitely check the data types of the variables. For instance, over here, a few variables should have been used as categories or factors, but instead they are in object string fromat.\n\nSo, you have to change the data type of these variables from object to category, as these are categorical features.","751dadc2":"Now, in order to calculate the probabilities, you need to convert the target classes to binary, i.e. 0 and 1. You can use CERTIFIED and DENIED to map it to 0 and 1. ","78201225":"Y indicates the petitioner has a full time role and N indicates a part time role.","b1a94b34":"# XGBoost","eaf6f580":"Since visa applications majorly depend on State location, you should split the state information from the WORKSITE variable.","0f348811":"## Feature Creation","201c7fa9":"Working on Feature Prevailing wage","17b448fd":"Since, you have now generated new features from the variables below, we can drop them, running horizontally across columns, as axis = 1.","0a028d9e":"EMPLOYER_NAME contains the names of the employers and there are lot of unique employers. It is the company which submits the application for its employee. You cannot use EMPLOYER_NAME directly in the model because it has got many unique string values or categories; more than 500 employers. These employers act as factors or levels. It is not advisable to use this many factors in a single column.\n\nThe top 5 companies submitting the application for their employees are Infosys, TCS, Wipro, Deloitte and IBM. However, if any University is submitting an application then it is more likely to be accepted.","6fc2d3ca":"Around 85% of the jobs applied for are full time jobs.","d5456318":"## Info about dataset","0f71b176":"If the employer name contains the string 'University' (for instance if a US university is filing a visa petition, then it has more chances of approval for the employee).\n\nSo, if the EMPLOYER_NAME contains 'University', then NEW_EMPLOYER contains the university value.","009e5b2b":"California has the highest number of petitions cementing its place as the base of choice for IT ops, followed by Texas, New York and New Jersey.","72c4337c":"The target variable contains 6 different classes\n\nConverting to binary class classification, we would classify either Certified or Denied. So the first thing that we should do is to convert remaining classes into either denied or certified.","6d9f0fc0":"## Dropping lat and lon columns","eecdbf06":"Checking the percentage of Certified and Denied classes in the Dataset.","4a5f563b":"Convert categorical variables into numeric ones using one hot encoding.\nFor classification, if the dependent variable belongs to the class factor, convert it to numeric\nas_matrix is quick enough to implement one hot encoding. You can convert the dataset to a matrix format, as shown below.","ae5914fc":"ncode X_train and X_test to get them ready for Xgboost, as it only works on numeric data. The function pd.get_dummies() is used to encode the categorical values to integers. It will create a transpose of all the categorical values and then map 1 wherever the value is present or 0 if it's not present. You should definitely try at your end to to print the X_train_encode below to check the transpose.","9cc094c9":"## Mapping Target Variables ","e5331e7b":"Similarly for SOC_NAME feature","64e2b05d":"Exploring Employer_Name Feature","363de1cc":"Check if there are any null values in the training set. There should not be any.","4653db68":"To check if any employers are still not null","8be3fb50":"## Dropping columns","b7ca3e29":"## Splitting Data in Training and Test Sets","02dc79bf":"## H1B VISA and Dataset Kaggle ","38ab48e7":"## Treating missing and NA values ","47a2e8c7":"The decline class is only 3.2% of the total dataset, that means you now have approx 96.8% Certified cases in your dataset. This shows that the datset is highly imbalanced. ","9aa55566":"This dataset has 11 columns, from which 1 is a target variable, which in this case is a case_status. So, this data has 1 target variable and 10 independent or explanatory variables."}}