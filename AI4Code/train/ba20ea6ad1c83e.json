{"cell_type":{"42c9f37a":"code","5d0bc180":"code","a4c3af8f":"code","4efa8da7":"code","6a3367f4":"code","c4807b12":"code","cc713980":"code","a1eddc03":"code","84734af1":"code","74c6604a":"code","e1126e77":"code","24957718":"code","699866b1":"code","e6fa6beb":"code","24a46161":"code","71b1b747":"code","b5e49cb5":"code","d76d7c79":"code","e2c946cb":"code","37073b00":"code","3ff96ba7":"code","637767ba":"code","7541fa24":"code","5d4e241a":"code","4cab08a5":"code","538c763a":"code","ad963073":"code","b8563fd5":"code","3df5c637":"code","72a4c751":"code","cb17bd89":"code","bd34114c":"markdown","9f676819":"markdown","1adbb36e":"markdown","8f0a44d8":"markdown","f2a7e88d":"markdown","3b27be4b":"markdown","ca58c029":"markdown","e2d928f3":"markdown","1c1a3739":"markdown","62896d2b":"markdown","d88391b4":"markdown","58b67c09":"markdown","9815a0e4":"markdown","0bddf636":"markdown","d903464b":"markdown","e5284bdb":"markdown","37b625f2":"markdown","54f976e9":"markdown","6173db54":"markdown","719b61e7":"markdown","199bef84":"markdown","c37ab51a":"markdown","abc9a773":"markdown","293279f8":"markdown","3d70d9ee":"markdown","a0caf2f9":"markdown","4deea358":"markdown","2a4ac3fb":"markdown","b4eba7fb":"markdown"},"source":{"42c9f37a":"# Data Processing\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold, cross_validate, train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Data Visualizing\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\nfrom IPython.display import display, HTML\n\n# Data Modeling\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC, SVC, NuSVC\n\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# Data Validation\nfrom sklearn import metrics\n\n# Math\nfrom scipy import stats  # Computing the t and p values using scipy \nfrom statsmodels.stats import weightstats \nimport math\nfrom scipy.stats import norm\n\n# Warning Removal\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)","5d0bc180":"df = pd.read_csv('..\/input\/Momo_Secret_Finding.csv')\n","a4c3af8f":"df.head()","4efa8da7":"zero_count = (df.isnull()).sum() # (df1 == 0).sum()\nzero_count_df = pd.DataFrame(zero_count)\nzero_count_df.drop('STATUS', axis=0, inplace=True)\nzero_count_df.columns = ['Count_Missing_Value']\n\n# https:\/\/stackoverflow.com\/questions\/31859285\/rotate-tick-labels-for-seaborn-barplot\/60530167#60530167\nsns.set(style='whitegrid')\nplt.figure(figsize=(13,8))\nsns.barplot(x=zero_count_df.index, y=zero_count_df['Count_Missing_Value'])\nplt.xticks(rotation=70)","6a3367f4":"cats = ['CITY', 'AGENT', 'STATUS', 'SHOP_ID']\n\ndef plotFrequency(cats):\n    #\"A plot for visualize categorical data, showing both absolute and relative frequencies\"\n    fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n    axes = axes.flatten()\n\n    for ax, cat in zip(axes, cats):\n        total = float(len(df[cat]))\n        sns.countplot(df[cat], palette='plasma', ax=ax)\n\n        for p in ax.patches:\n            height = p.get_height()\n            ax.text(p.get_x() + p.get_width() \/ 2.,\n                    height + 10,\n                    '{:1.2f}%'.format((height \/ total) * 100),\n                    ha=\"center\")\n\n        plt.ylabel('Count', fontsize=15, weight='bold')","c4807b12":"plotFrequency(cats)","cc713980":"def plotStatus(cats):\n    #\"A plot for visualize categorical data, showing both absolute and relative frequencies\"\n    fig, axes = plt.subplots(2, 2, figsize=(25, 20))\n    axes = axes.flatten()\n\n    for ax, cat in zip(axes, cats):\n        total = float(len(df[cat]))\n        sns.countplot(df[cat], palette='plasma',hue=df['STATUS'], ax=ax)\n        \n        for p in ax.patches:\n            height = p.get_height()\n            ax.text(p.get_x() + p.get_width() \/ 2.,\n                    height + 10,\n                    '{:1.2f}%'.format((height \/ total) * 100),\n                    ha=\"center\")\n        \n        \n        ax.legend(title='STATUS?',loc='upper right',labels=['Churn', 'Retain'])\n        \n        plt.ylabel('Count', fontsize=15, weight='bold')\n        display","a1eddc03":"plotStatus(cats)","84734af1":"plt.figure(figsize=(13,5))\nsns.scatterplot(x=df['TIME_TO_CONVERT'], y=df['STATUS'])","74c6604a":"plt.figure(figsize=(13,5))\nsns.scatterplot(x=df['TIME_TO_CONVERT'], y=df['CITY'], hue=df['STATUS'])","e1126e77":"print(df['TRAN_ID'].duplicated().sum())\nprint(df['USER_ID'].duplicated().sum())","24957718":"df = df[df['CITY'].notnull()]","699866b1":"# Time_to_convert can't be negative value as following the data description, \n# it is a period that the first payment made by user to the day that the user made the first offline payment\ndf = df[df['TIME_TO_CONVERT']>0]","e6fa6beb":"fig, axes = plt.subplots(1, 2, figsize=(18, 5))\naxes = axes.flatten()\n\nChurn_Convert_Time = df[df['STATUS']=='churn']['TIME_TO_CONVERT']\nsns.distplot(Churn_Convert_Time, ax=axes[0]).set_title(\"Churn_Convert_Time\")\n\nRetain_Convert_Time = df[df['STATUS']=='retain']['TIME_TO_CONVERT']\nsns.distplot(Retain_Convert_Time, ax=axes[1]).set_title(\"Retain_Convert_Time\")","24a46161":"# Median value of TIME_TO_CONVERT for churn customers\nprint(df[df['STATUS']=='churn']['TIME_TO_CONVERT'].median())\n\n# Median value of TIME_TO_CONVERT for retain customers\nprint(df[df['STATUS']=='retain']['TIME_TO_CONVERT'].median())","71b1b747":"# Filling the missing values\ndf.loc[(df['STATUS'] == 'churn' ) & (df['TIME_TO_CONVERT'].isnull()), 'TIME_TO_CONVERT'] = 168\ndf.loc[(df['STATUS'] == 'retain' ) & (df['TIME_TO_CONVERT'].isnull()), 'TIME_TO_CONVERT'] = 144.5","b5e49cb5":"def plot_3chart(feature):\n    fig = plt.figure(constrained_layout=True, figsize=(12, 8))\n    # creating a grid of 3 cols and 3 rows.\n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n\n    # Customizing the histogram grid.\n    ax1 = fig.add_subplot(grid[0, :2])\n    # Set the title.\n    ax1.set_title('Histogram')\n    # plot the histogram.\n    sns.distplot(df[feature], hist=True, kde=True, fit=norm, color='#e74c3c', ax=ax1)\n    ax1.legend(labels=['Normal', 'Actual'])\n    \n    # customizing the QQ_plot.\n    ax2 = fig.add_subplot(grid[1, :2])\n    # Set the title.\n    ax2.set_title('Probability Plot')\n    # Plotting the QQ_Plot.\n    stats.probplot(df[feature].fillna(np.median(df.loc[:, feature])), plot=ax2)\n    #ax2.get_lines()[0].set_markerfacecolor('#e74c3c')\n    ax2.get_lines()[0].set_markersize(12.0)\n    \n     # Customizing the Box Plot.\n    ax3 = fig.add_subplot(grid[:, 2])\n    # Set title.\n    ax3.set_title('Box Plot')\n    # Plotting the box plot.\n    sns.boxplot(df[feature], orient='v', color='#e74c3c', ax=ax3)\n    ax3.yaxis.set_major_locator(MaxNLocator(nbins=24))\n\n    plt.suptitle(f'{feature}', fontsize=24)","d76d7c79":"plot_3chart('TIME_TO_CONVERT')","e2c946cb":"df['TIME_TO_CONVERT'] = np.log1p(df['TIME_TO_CONVERT'])\n\nfig, axes = plt.subplots(1, 2, figsize=(18, 5))\naxes = axes.flatten()\n\nsns.distplot(df['TIME_TO_CONVERT'], ax=axes[0]).set_title(\"Churn_Convert_Time\")\n\n#Get also the QQ-plot\nres = stats.probplot(df['TIME_TO_CONVERT'], plot=axes[1])\nplt.show()","37073b00":"df.drop(['TRAN_ID', 'USER_ID'], axis=1, inplace=True)","3ff96ba7":"df['SHOP_ID'] = df['SHOP_ID'].astype('str')","637767ba":"df = pd.get_dummies(df, drop_first=True)\ndf.head()","7541fa24":"x = df.drop('STATUS_retain', axis=1)\ny = df['STATUS_retain']\n\n# Usually we need to bring all features to the same scale using StandardScaler or Normalizer or Boxcox. \n# But in this case, it won't yield better result\n# x = StandardScaler().fit_transform(df.drop('STATUS_retain', axis=1))\n# y = df['STATUS_retain']","5d4e241a":"cv = StratifiedKFold(10, shuffle=True, random_state=0)\n\ndef model_check(X, y, estimators, cv):\n    model_table = pd.DataFrame()\n    \n    row_index = 0\n    for est in estimators:\n\n        MLA_name = est.__class__.__name__\n        model_table.loc[row_index, 'Model Name'] = MLA_name\n        #    model_table.loc[row_index, 'MLA Parameters'] = str(est.get_params())\n\n        cv_results = cross_validate(est,\n                                    X,\n                                    y,\n                                    cv=cv,\n                                    scoring='accuracy',\n                                    return_train_score=True,\n                                   )\n\n        model_table.loc[row_index, 'Train Accuracy Mean'] = cv_results['train_score'].mean()\n        model_table.loc[row_index, 'Test Accuracy Mean'] = cv_results['test_score'].mean()\n        model_table.loc[row_index, 'Test Std'] = cv_results['test_score'].std()\n        model_table.loc[row_index, 'Time'] = cv_results['fit_time'].mean()\n\n        row_index += 1\n\n        model_table.sort_values(by=['Test Accuracy Mean'],\n                            ascending=False,\n                            inplace=True)\n\n    return model_table","4cab08a5":"logreg = LogisticRegression(n_jobs=-1, solver='newton-cg')\n\nknn = KNeighborsClassifier(n_neighbors=13)\n\ngnb = GaussianNB()\n\nlinearSVC = LinearSVC()\n\nRbfSVC = SVC()\n\ndt = DecisionTreeClassifier(max_depth=10)\n\nrf = RandomForestClassifier(random_state=0,n_jobs=-1,verbose=0)\n\nadab = AdaBoostClassifier(random_state=0)\n\ngb = GradientBoostingClassifier(random_state=0)\n\nxgb = XGBClassifier(random_state=0)\n\nlgbm = LGBMClassifier(random_state=0)\n\nvotingC = VotingClassifier(estimators=[(\"XGB\", xgb), (\"GB\", gb), (\"DecisionTree\", dt),('LightGBM', lgbm)], \n                           voting='soft', n_jobs=4)","538c763a":"estimators = [logreg,knn,gnb,linearSVC,RbfSVC,dt,rf,gb,xgb,lgbm,votingC]","ad963073":"raw_models = model_check(x, y, estimators, cv)\ndisplay(raw_models.style.background_gradient(cmap='summer_r'))","b8563fd5":"xgb = XGBClassifier(random_state=0).fit(x, y)\ngb = GradientBoostingClassifier(random_state=0).fit(x, y)\ndt = DecisionTreeClassifier(max_depth=10).fit(x, y)\nrf = RandomForestClassifier(random_state=0,n_jobs=-1,verbose=0).fit(x, y)","3df5c637":"nrows = 2\nncols = 2\nfig, axes = plt.subplots(nrows = nrows, ncols = ncols, figsize=(15,15))\n\nnames_classifiers = [(\"XGB\", xgb), (\"GB\", gb), ('DT', dt), ('RF', rf)]\n\nnclassifier = 0\nfor row in range(nrows):\n    for col in range(ncols):\n        name = names_classifiers[nclassifier][0]\n        classifier = names_classifiers[nclassifier][1]\n        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n        g = sns.barplot(y=x.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row][col])\n        g.set_xlabel(\"Relative importance\",fontsize=12)\n        g.set_ylabel(\"Features\",fontsize=12)\n        g.tick_params(labelsize=9)\n        g.set_title(name + \" feature importance\")\n        nclassifier += 1","72a4c751":"from pdpbox import pdp, get_dataset, info_plots\n\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=gb, dataset=x, model_features=x.columns, feature='TIME_TO_CONVERT')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'TIME_TO_CONVERT')\nplt.show()","cb17bd89":"row_to_show = 1\ndata_for_prediction = x.iloc[row_to_show]\n\nimport shap\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(dt)\n# Calculate Shap values\nshap_values = explainer.shap_values(data_for_prediction)\nshap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)","bd34114c":"### Split Independent and Dependent Variables","9f676819":"### Check duplicated TRAIN_ID and USER_ID\n- If Yes, remove those rows","1adbb36e":"#### Observation\n- City and Time_To_Convert have missing values","8f0a44d8":"### Models Evaluation\n- It seems like the models do not really perform better than the baseline model (79% churn, 21% retain)","f2a7e88d":"# MOMO SECRET FINDING\n## Author: Vu Duong\n### Date: July, 07, 2020","3b27be4b":"### SHAP Values\n- Helps to break down a prediction to show the impact of each feature.\n- Let's pick a random patient, here is 1, from our dataset to see how our model predict based on individual feature\n- TIME_TO_CONVERT has the biggest impact on prediction of being churn\n- CITY has the biggest impact on prediction of being retain","ca58c029":"### Remove outliers in TIME_TO_CONVERT","e2d928f3":"### Define a cross validation function","1c1a3739":"#### Observation\n- The variable is right-skwed. As (linear) models love normal distributed date, we need to transform this variable and make it more normally distributed.\n- Applying log1p on the TIME_TO_CONVERT to center the data. As the result, the curve blue line seems to align with the red line.","62896d2b":"### Apply one-hot-encoding on data prior to ML models","d88391b4":"# INTRODUCTION\nThe aim of the campaign is to accquire and retain users, convert them into frequent users of Momo. Could you evaluate the performance of the campaign? Is there any interesting pattern in the data? And finally can we do anything about the secret you just found? \n\n## Data Description\n- TRAN_ID:\tThe identity of  the transaction\n- USER_ID:\tThe identity of  the user\n- CITY:\tThe city where the transaction took place, 'Others' is include all the provinces\/city other than HCM and Hanoi\n- AGENT:\tStore category of the transaction\n- STATUS:\tThe retaining status of the user. If the user continue to user Momo in the week after, then the user is labeled as retain, else churn\n- SHOP_ID:\tIdentity of the shop in which the transaction took place\n- TIME_TO_CONVERT:\tMeasure in day. How long it's been since the first payment made by user to the day that the user made the first offline payment","58b67c09":"# LIBRARY","9815a0e4":"# MODELING\n- The purpose of model is to predict if a user churns or retains","0bddf636":"#### Observation\n- The variable is right-skwed. As (linear) models love normal distributed date, we need to transform this variable and make it more normally distributed.\n- Applying log1p on the TIME_TO_CONVERT to center the data. As the result, the curve blue line seems to align with the red line.","d903464b":"### Remove unnecessary columns","e5284bdb":"#### Observation\n- We can't really predict a customer churn or retain our service based on TIME_TO_CONVERT against Status, as we can see the data point overlap.","37b625f2":"### Explore 3 most Important Features in each models\n- In XGBoost: City, Shop_id, Agent\n- In Gradient Decent: Time_to_convert, City, Shop_id\n- In Decision Tree: Time_to_convert, Shop_id, City,\n- In Random Forest: Time_to_convert","54f976e9":"#### Observation\n- The City graph shows customers come from HCM are more likely to retain, following those from Others, and those from Ha Noi least retaining.\n- The Agent graph indicates Convenience Store tend to attract and keep customers with us more than Super Market channel.\n- The Shop_ID gives an interesting information. Shop_id 1 and 2 attract most consumers but with least retention rate. Shop_id 3 comes from City of Others and Agent of Convience Store only outperform other shop_id, at 0.45 (1.85\/4.11), following by Shop_id 5 with retention rate of 0.4 (1.96\/4.84)","6173db54":"#### Observation\n- In City feature: Most customers come from HCM, while customers from Ha Noi and Other are equal at lower level.\n- In Agent feature: the channels through Super Market and Convience Store are almost similar.\n- In Status feature: 78.92% of churn customers, while only 21.08% of retain customers.\n- In Shop_id: 2 most popular shop are Shop_id 1 and 2 accounting for 33.68% and 37.71% respectively. While Shop_Id 3, 4, 5, 6 share the same market portion around 6%.","719b61e7":"## Results from the study\n- There are 78.92% of churn customers, 21.08% of retain customers.\n- Shop_id 1 and 2 drive most customers at 33.68% and 37.71% of total.\n- HCM Customers are more likely to retain, compared to those from Ha Noi and Others\n- Shop_id 3 and 5 achieve the highest retention rate of 0.45 and 0.4, however they just account for 5.96% and 6.8 % of customers.\n- Shop_id 6 is the only one to inherite both characteristics from Super Market and Convience Store.\n- We may use the combination of TIME_TO_CONVERT and CITY to derive the insight from customers' STATUS.\n- Clean valueS in CITY, TIME_TO_CONVERT. Remove unnecessary columns like TRAIN_ID, USER_ID\n- The important features for prediction capability are CITY, SHIP_ID, AGENT, TIME_TO_CONVERT\n- The Partial Dependence Plots shows the longer people make their first offline payment, the less likely are they truely become our customers.\n- Use SHAP helps us realize how variables impact on customer STATUS.","199bef84":"### Remove missing values in CITY","c37ab51a":"# DATA MANIPULATION\n- Check duplicated values in TRAIN_ID, USER_ID\n- Clean CITY feature\n- Clean TIME_TO_CONVERT feature","abc9a773":"### Replace missing values in TIME_TO_CONVERT","293279f8":"#### Observation\n- Convert-time for categories of Churn and Retain shares the same pattern with the long right tail, called right-skwed distribution.\n- Thus, we impute missing values with median according to their types","3d70d9ee":"### Partial Dependence Plots\n- The Y-axis represents the change in prediction from what it would be predicted at the baseline or leftmost value.\n- Blue area denotes the confidence interval\n- For the \u2018TIME_TO_CONVERT\u2019 graph, we observe that probability of a person having churn steeply decreases as the data passed level grows.","a0caf2f9":"### Models","4deea358":"### Transform datatype","2a4ac3fb":"#### Observation\n- Ploting a graph among 3 variable yeild better outcome. \n- People from Other City tend to retain at the second half of TIME_TO_CONVERT.\n- People from HCM City tend to retain at the first quarter and the second half of TIME_TO_CONVERT.\n- People from Ha Noi are unpredictable.","b4eba7fb":"# DATA EXPLORATION"}}