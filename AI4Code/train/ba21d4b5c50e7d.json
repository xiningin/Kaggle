{"cell_type":{"d7842a09":"code","3c07d09a":"code","130bab54":"code","52191735":"code","1ecf6ebf":"code","0c825952":"code","a7b335b4":"code","7f312408":"code","290d57ba":"code","55144a5b":"code","2cfcee0d":"code","978fb7ab":"code","2408b6d0":"code","b7b2b801":"code","4c3e1866":"code","6eefa510":"code","26777fa7":"code","53db08ce":"code","b6149b71":"markdown","b9da1eed":"markdown","d2d3123b":"markdown"},"source":{"d7842a09":"# Libraries - We need to import the libraries for the classes, methods which we may using in any notebook - this is general concept like any oops language \n\nimport numpy as np \nimport pandas as pd \nimport os\nimport gc\n\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn import preprocessing\n\n# To list the directories in ..\/input, using this we can use the directory\/file name(s) to read the train,test and any additional data\nprint(os.listdir('..\/input'))\n\n# Pandas's dispaly  settings - to view all column data without \"...\" when we are using pandas library to read and dispaly data to anlayse \npd.set_option('display.max_colwidth', -1)\npd.set_option('display.max_rows', 100)  \npd.set_option('display.max_columns', 500)","3c07d09a":"# Seed - to maintian uniqueness and stability of predictions throughout the developement\n# Without seed settings we may getting different results each time when we train and predict\nrandom_state = 1337\nnp.random.seed(random_state)","130bab54":"# Read Data using pandas\n# Competietion having 2 types(trasaction and identities) data for train and test. \n# After read both files for each we need to merge the data by using join.\n\ntrain_ts = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_transaction.csv\", index_col='TransactionID')\ntrain_id = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_identity.csv\", index_col='TransactionID')\n\ntest_ts = pd.read_csv(\"..\/input\/ieee-fraud-detection\/test_transaction.csv\", index_col='TransactionID')\ntest_id = pd.read_csv(\"..\/input\/ieee-fraud-detection\/test_identity.csv\", index_col='TransactionID')\n\nsubmission = pd.read_csv(\"..\/input\/ieee-fraud-detection\/sample_submission.csv\")\n\nprint(train_ts.shape,train_id.shape,test_ts.shape,test_id.shape,submission.shape)","52191735":"# trasactions and identities data mearging using join\n\ntrain = train_ts.merge(train_id, how='left', left_index=True, right_index=True)\ntest = test_ts.merge(test_id, how='left', left_index=True, right_index=True)\nprint(train.shape,test.shape)","1ecf6ebf":"# Data having huge size, to run the notebook without intereptions we need to clean unused datasets to release memory\n# Here we created new datasets, so old datasets need to dispose\n\ndel(train_ts, train_id, test_ts, test_id)\ngc.collect()","0c825952":"# Memory reduction function - Memory is high for original data. We need to reduce memory to process features. \ndef reduce_mem_usage(df):\n    start_mem_usg = df.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in df.columns:\n        if df[col].dtype != object:  # Exclude strings                      \n            # make variables for Int, max and min\n            IsInt = False\n            mx = df[col].max()\n            mn = df[col].min()\n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(df[col]).all(): \n                NAlist.append(col)\n                df[col].fillna(0,inplace=True) \n                   \n            # test if column can be converted to an integer\n            asint = df[col].fillna(0).astype(np.int64)\n            result = (df[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True            \n            # Make Integer\/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        df[col] = df[col].astype(np.uint8)\n                    elif mx < 65535:\n                        df[col] = df[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        df[col] = df[col].astype(np.uint32)\n                    else:\n                        df[col] = df[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)    \n            # Make float datatypes 32 bit\n            else:\n                df[col] = df[col].astype(np.float32)\n            \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = df.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg\/start_mem_usg,\"% of the initial size\")\n    return df","a7b335b4":"# Memory reducing for train and test datasets\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","7f312408":"# In both train and test datasets having few categorical columns. We need to convert any strain data into numericl as best practice\n# and also we can process numerical categorical columns also. \n# Here I listed out the categorical columns(string and numerical) seperately\ncat_cols_str=['ProductCD', 'card4', 'card6','P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']\ncat_cols_int=[ 'card1','card2','card3','card5','addr1', 'addr2']\n","290d57ba":"# Processing categorical data function - I will convert categorical columns in to sequesntial numerical data\n# Filled 0 or NA for empty data values\n\ndef process_categorical_columns(temp_ds):\n    for col in cat_cols_str: \n        temp_ds[col] = temp_ds[col].fillna('NA')\n        temp_ds[col]= temp_ds[col].replace(0,'NA')\n        le = preprocessing.LabelEncoder()\n        le.fit(temp_ds[col])\n        temp_ds[col] = le.transform(temp_ds[col]) \n\n    for col in cat_cols_int: \n        temp_ds[col] = temp_ds[col].fillna(0) \n        temp_ds[col]= temp_ds[col].replace('NA',0)\n        le = preprocessing.LabelEncoder()\n        le.fit(temp_ds[col])\n        temp_ds[col] = le.transform(temp_ds[col]) \n    return temp_ds","55144a5b":"# Processing train and test categorical data\ntrain = process_categorical_columns(train)\ntest = process_categorical_columns(test)","2cfcee0d":"# Prepare dataset to train \n# In this problem we need to predict probabillity in between 0 and 1. \n# So, this is clasifier problem and I am using LGBM clasifier wiht Kfold validation. And using predict_proba() method\n# For LGBM we need to provide  Train data(X_data), Train target(y_data) and test data(X_test)\n\nX_data = train.drop('isFraud',axis=1)\ny_data = train['isFraud'].values\nX_test = test\nprint(X_data.shape, y_data.shape, X_test.shape)","978fb7ab":"# Clean unused datasets to release memory\ndel(train, test)\ngc.collect()","2408b6d0":"# LGB Parameters - initial - need to fine tune\nlgb_params = {\n    \"objective\" : \"binary\",\n    \"metric\" : \"auc\",\n    \"boosting\": 'gbdt',\n    \"max_depth\" : -1,\n    \"num_leaves\" : 13,\n    \"learning_rate\" : 0.0085,\n    \"bagging_freq\": 5,\n    \"bagging_fraction\" : 0.4,\n    \"feature_fraction\" : 0.05,\n    \"min_data_in_leaf\": 80,\n    \"min_sum_heassian_in_leaf\": 10,\n    \"tree_learner\": \"serial\",\n    \"boost_from_average\": \"false\",\n    \"bagging_seed\" : random_state,\n    \"verbosity\" : 1,\n    \"seed\": random_state\n}","b7b2b801":"# LGB training method\ndef train_and_predict(n_splits=5, n_estimators=100):\n    y_pred = np.zeros(X_test.shape[0], dtype='float32')\n    cv_score = 0\n\n    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n\n    for fold, (train_index, val_index) in enumerate(kfold.split(X_data, y_data)):\n        if fold >= n_splits:\n            break\n        \n        # Split the data for train and validation in each fold\n        X_train, X_val = X_data.iloc[train_index, :], X_data.iloc[val_index, :]\n        y_train, y_val = y_data[train_index], y_data[val_index]\n        \n        # Using LGBM classifier algorthm\n        model = LGBMClassifier(**lgb_params, n_estimators=n_estimators, n_jobs = -1)\n        model.fit(X_train, y_train, \n            eval_set=[(X_train, y_train), (X_val, y_val)], eval_metric='auc',\n            verbose=50, early_stopping_rounds=200)\n\n        # in predict_proba we will get 2 values for each prediction, so we need to use 2nd value\n        y_val_pred = model.predict_proba(X_val)[:,1]\n        \n        # Calcualte cross validation score using metrics\n        val_score = roc_auc_score(y_val, y_val_pred)\n        print(f'Fold {fold}, AUC {val_score}')\n        \n        # Averaging cross validation score and test predictions for all folds\n        cv_score += val_score \/ n_splits\n        y_pred += model.predict_proba(X_test)[:,1] \/ n_splits\n    \n    # Assign final test predictions to submission dataset\n    submission['isFraud'] = y_pred\n    return cv_score","4c3e1866":"# Setting folds and start training - as I said I am using KFold, in this we need to set number of folds.\n# Based on the folds count full dataset splited into smaller sets \n# and each set used for training seperately and predict the results. \n# And by averaging the results we can get final results with better accuracy\n# I am using 5000 iterations,verbore as 50 and early stopping rounds as 200 (in above funtion)\n# Verbose - cross validate for each 50 iterations \n# Early stopping rounds - Stop training early if results not improve for last 200 iteration\n\nN_FOLDS = 5\nn_estimators = 5000\ncv_score = train_and_predict(n_splits=N_FOLDS, n_estimators=n_estimators)\nprint(cv_score)","6eefa510":"submission['isFraud'].sum()","26777fa7":"# create the output file for submission\nsubmission.to_csv('submission.csv',index=False)","53db08ce":"# list the files in output folder to download the output file if we run the notebook as interactive session instead of commit\nfrom IPython.display import FileLink, FileLinks\nFileLinks('.')","b6149b71":" LB : 0.9121\n \n We can slightly improve by increaseing estimators to 10K or 20K\n ","b9da1eed":"Few steps to solve,\n\n1. Data analysing\n2. Data cleaning\n3. Feature engineering\n4. Feature filtering by identifying important features\n5. Algorithm selection\n6. Parameter selection\/fine tune\n7. Train & predict\n8. Cross Validation\n\nAs a initial step, I am covering few steps(2,5,6,7,8) which are needed to create baseline notebook as complete cycle. Later I will other steps(1,3,4,6-fine tune) one by one as update\n\nIn addition these I am adding few which are needed like libraries, seed, memory, etc.\n\nNext step I will add Feature engineering(3rd step) ","d2d3123b":"Here I am explaing small steps even I know that every one knows these in this platform. Very basic steps, but I am feeling beginers can use\/check this to learn"}}