{"cell_type":{"7e70b1e0":"code","cf1fcdc1":"code","e8a95090":"code","4a5d426e":"code","f9411b52":"code","ca8eea63":"code","bf39003f":"code","efb71ddc":"code","fd133db0":"code","08771fd6":"code","8af6b7be":"code","f915646f":"code","3fe469b7":"code","41ab1dc1":"code","e7db133c":"code","755fac42":"markdown","f4c8030e":"markdown","4317c8a4":"markdown","19b967e7":"markdown","8444e2a5":"markdown","e040c7e8":"markdown","5ac53ff3":"markdown","949a0395":"markdown","a69a3ce1":"markdown","e8a5d32f":"markdown"},"source":{"7e70b1e0":"import numpy as np\nimport pandas as pd\npd.options.mode.chained_assignment = None\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","cf1fcdc1":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom imblearn.over_sampling import ADASYN\nfrom imblearn.under_sampling import EditedNearestNeighbours\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFECV\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score, classification_report","e8a95090":"dataset = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ndataset","4a5d426e":"dataset.describe()","f9411b52":"z = np.abs(stats.zscore(dataset))\ndataset = dataset[((z < 3)).all(axis=1)]","ca8eea63":"dataset.isnull().sum()","bf39003f":"minmax = MinMaxScaler()\ndataset[['age','creatinine_phosphokinase','ejection_fraction',\n            'platelets','serum_creatinine','serum_sodium','time']] = minmax.fit_transform(dataset[['age','creatinine_phosphokinase','ejection_fraction',\n            'platelets','serum_creatinine','serum_sodium','time']])","efb71ddc":"y = dataset['DEATH_EVENT']\nx = dataset.drop('DEATH_EVENT', axis=1)\ny.value_counts()","fd133db0":"resample = ADASYN(sampling_strategy='all', random_state=42)\nx_resample, y_resample = resample.fit_resample(x,y)\n\ny_resample.value_counts()","08771fd6":"x_train, x_test, y_train, y_test = train_test_split(x_resample, y_resample, test_size=0.2, random_state=42)","8af6b7be":"estimator = RandomForestClassifier()\nfeature_selection = RFECV(estimator, step=2, cv=5)\nfeature_selection = feature_selection.fit(x_train, y_train)\nmask = np.array(feature_selection.support_)","f915646f":"x_train = x_train.loc[:, mask]\nx_test = x_test.loc[:, mask]\nx_train","3fe469b7":"run_gs = True\n\nif run_gs:\n    parameter_grid = {\n                 'max_depth' : [1,2, 3, 6, 8],\n                 'gamma': [0,0.2, 0.4, 0.8, 1.5],\n                 'use_label_encoder' : [False], \n                 'random_state' : [1], \n                 'eval_metric' : ['logloss']\n                 }\n    model = XGBClassifier()\n    cross_validation = StratifiedKFold(n_splits=5)\n\n    grid_search = GridSearchCV(model,\n                               scoring='accuracy',\n                               param_grid=parameter_grid,\n                               cv=cross_validation,\n                               verbose=1\n                              )\n\n    grid_search.fit(x_train, y_train)\n    model = grid_search\n    parameters = grid_search.best_params_\n    \n    print('Best score: {}'.format(grid_search.best_score_))\n    print('Best parameters: {}'.format(grid_search.best_params_))","41ab1dc1":"clf = XGBClassifier(gamma=0.8, max_depth=3, eval_metric ='logloss' ,use_label_encoder=False, random_state=1)\nclf.fit(x_train, y_train)\ny_pred = clf.predict(x_test)","e7db133c":"target_names = ['class 0', 'class 1']\nprint(classification_report(y_test, y_pred, target_names=target_names))","755fac42":"**XGBoost**\n\nXGBoost is a variant of gradient boosting. Recently it has proven to be a great success. Learn more about it here : https:\/\/arxiv.org\/pdf\/1603.02754.pdf","f4c8030e":"**FEATURE ENGINEERING**\n\n**RECURSIVE FEATURE ELIMINATION**\n\n\nRecursive feature elimination is a wrapper method used for feature selection and engineering. It uses a classifier as base estimator, based on which the features are recursively eliminated","4317c8a4":"**DATA RESAMPLING**\n\n**ADASYN**\n\nThis is a oversampling technique of minority class to address the class imbalance issue. This method is similar to SMOTE but it generates different number of samples depending on an estimate of the local distribution of the class to be oversampled. Read the related paper to understand more on this : https:\/\/www.researchgate.net\/publication\/224330873_ADASYN_Adaptive_Synthetic_Sampling_Approach_for_Imbalanced_Learning\n","19b967e7":"**HEART FAILURE PREDICTION USING XGBOOST**\n\nThis notebook include the following techniques for predicting the heart failure:\n\n- Outlier detection and removal using Z-SCORE\n- Feature Scaling using MINMAX\n- Data Resampling using ADASYN\n- Feature Engineering\n- Hyperparameter tuning\n- Ensemble XGBoost Model","8444e2a5":"Checking for missing data reveals that there is no data point missing in this dataset, a good thing!","e040c7e8":"**FEATURE SCALING**\n\n**MIN MAX SCALING**\n\nA normalization technique is required to scale the continuous features to a range. For this the minmax scaler has been used. Learn more about min max scaler here https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MinMaxScaler.html","5ac53ff3":"Splitting the data as 80% for training and 20% for testing","949a0395":"The description of the dataset reveals the basic statistics and percentiles of the each numeric column. Using this, the outliers can identified by taking a look at the mean and std-deviation of the columns and comparing it to the 75th percentile and the max value. If the max value is much larger than the standard deviation of the column for the 75th percentile then there are outliers definite outliers in the dataset, which is the case here ","a69a3ce1":"**OUTLIER DETECTION AND REMOVAL**\n\n**Z SCORE** \n\nThe Z-Score can be used to identify and remove outliers in a dataset. It indicates how many standard deviations away a data point is from the mean. The formula to find the Z-Score for a feature is:\n\n  Z = (x-\u03bc)\/\u03c3\n                                                        \nIf the z score of a data point is more than 3, it indicates that the data point is different from the others. Such a data point is an outlier and should be removed","e8a5d32f":"On investigating the dataset balance, we find that the dataset is imbalanced"}}