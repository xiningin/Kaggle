{"cell_type":{"75632247":"code","cb1f9a72":"code","ce92d0f0":"code","e3f99a3e":"code","ee6a73e6":"code","f189b6ea":"code","b925ab70":"markdown","b8397af4":"markdown","3848e3bd":"markdown","d89579b9":"markdown","4941651e":"markdown"},"source":{"75632247":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cb1f9a72":"from keras.models import Sequential\nfrom keras.layers import Dense,Embedding, LSTM\nfrom keras.datasets import imdb\nfrom keras.preprocessing import sequence","ce92d0f0":"max_words = 10240\nmaxlen = 32\n\n(x_train, y_train),(_, _) = imdb.load_data(num_words = max_words)\n\nx_train = sequence.pad_sequences(x_train, maxlen = maxlen)\n","e3f99a3e":"print(y_train[:10])\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, 16, input_length=maxlen))\nmodel.add(LSTM(128))\nmodel.add(Dense(1, activation= 'sigmoid' ))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics= ['acc'])\n\nhistory = model.fit(x_train, y_train, batch_size=128, epochs=15, verbose=1) \n","ee6a73e6":"print(model.summary())","f189b6ea":"print(\"Accuracy: %\",history.history[\"acc\"][-1]*100)\n\nplt.figure()\nplt.plot(history.history[\"acc\"], label = \"Train\")\nplt.title(\"Acc\")\nplt.ylabel(\"Acc\")\nplt.xlabel(\"Epochs\")\nplt.legend()\nplt.show()\n\nplt.figure()\nplt.plot(history.history[\"loss\"], label = \"Train\")\nplt.title(\"loss\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"Epochs\")\nplt.legend()\nplt.show()","b925ab70":"<a id=\"7\"><\/a> <br>\n## Visualizing","b8397af4":"<a id=\"1\"><\/a> <br>\n## Introduction\n\n<a id=\"2\"><\/a> <br>\n## What are LSTM ?\n\nLong Short Term Memory networks \u2013 usually just called \u201cLSTMs\u201d \u2013 are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter & Schmidhuber (1997), and were refined and popularized by many people in following work.1 They work tremendously well on a large variety of problems, and are now widely used.\n\nLSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!\n\nAll recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.\n\n![Ekran%20Al%C4%B1nt%C4%B1s%C4%B1.PNG](attachment:Ekran%20Al%C4%B1nt%C4%B1s%C4%B1.PNG)\n\nDon\u2019t worry about the details of what\u2019s going on. We\u2019ll walk through the LSTM diagram step by step later. For now, let\u2019s just try to get comfortable with the notation we\u2019ll be using.\n\n![11.PNG](attachment:11.PNG)\n\n\nIn the above diagram, each line carries an entire vector, from the output of one node to the inputs of others. The pink circles represent pointwise operations, like vector addition, while the yellow boxes are learned neural network layers. Lines merging denote concatenation, while a line forking denote its content being copied and the copies going to different locations.\n\n<a id=\"3\"><\/a> <br>\n## The Core Idea Behind LSTMs\n\n\nThe key to LSTMs is the cell state, the horizontal line running through the top of the diagram.\nThe cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It\u2019s very easy for information to just flow along it unchanged.\n\n\n![Ekran%20Al%C4%B1nt%C4%B1s%C4%B1.PNG](attachment:Ekran%20Al%C4%B1nt%C4%B1s%C4%B1.PNG)\n\nThe LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.\nGates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.\n\n![Ekran%20Al%C4%B1nt%C4%B1s%C4%B11.PNG](attachment:Ekran%20Al%C4%B1nt%C4%B1s%C4%B11.PNG)\n\nThe sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means \u201clet nothing through,\u201d while a value of one means \u201clet everything through!\u201d\nAn LSTM has three of these gates, to protect and control the cell state.","3848e3bd":"<a id=\"6\"><\/a> <br>\n## Create LSTM Model","d89579b9":"<font color='red'>\n<br>Content:\n    \n* [Introduction](#1)\n    * [What are LSTM ?](#2)\n    * [The Core Idea Behind LSTMs](#3)\n* [Long Short Term Memory(LSTM)](#4)\n    * [Dataset EDA](#5)\n    * [Create Autoencoders Model](#6)\n    * [Visualizing](#7)","4941651e":"<a id=\"4\"><\/a> <br>\n## Long Short Term Memory(LSTM)\n<a id=\"5\"><\/a> <br>\n## DATASET EDA"}}