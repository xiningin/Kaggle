{"cell_type":{"ad9775e4":"code","c8e7bc06":"code","a939aa04":"code","a0447a25":"code","8bb513ea":"code","b344759e":"code","2f71f6cd":"code","76cd8726":"code","01542915":"code","e8695433":"code","4765bd51":"code","619260f1":"code","c2f3892a":"code","9251ad9c":"code","285f1224":"code","3cb3484a":"code","c3ff8d72":"code","75017902":"code","70a3dec4":"code","f4116ec0":"code","965be7d3":"code","f6775ea0":"code","0b9f5cdc":"code","343e5d63":"code","689632d2":"code","9ddaeaef":"code","3b170a27":"code","f9552df2":"code","5419d725":"code","7fe0a468":"code","e338f8ac":"code","e56e95da":"code","09e1e01f":"markdown","de9dcd41":"markdown","7f03be1b":"markdown","0bde18e1":"markdown","a108c2d5":"markdown","75e8da08":"markdown","ab2049a8":"markdown","702fd13c":"markdown","fc207873":"markdown","9a4ebf2e":"markdown","e624b0e2":"markdown","c940e5cf":"markdown","3cae3ed4":"markdown","ac118c9e":"markdown","da34c7c7":"markdown","0106ae01":"markdown","5d19fc30":"markdown","3ca8d0ca":"markdown","a5ec7e70":"markdown","48897391":"markdown","e07e7f1b":"markdown","162f9ec7":"markdown","910a76de":"markdown","420f3a9e":"markdown","cea7ec48":"markdown"},"source":{"ad9775e4":"# File check\nimport os","c8e7bc06":"dir_name = os.listdir(\"\/kaggle\/input\/intel-image-classification\/seg_test\/seg_test\/\")\ndir_name","a939aa04":"# Basic library\nimport numpy as np \nimport pandas as pd \n\n# Data preprocessing\nimport cv2 # Open cv\nfrom sklearn.model_selection import train_test_split\n\n# Visualization\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\n# Machine learning library\nimport keras\nfrom keras.models import Sequential, Model, load_model\nfrom keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization, Activation, Input\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# Validation\nfrom sklearn.metrics import roc_auc_score","a0447a25":"# Create empty dataframe and list\ndf = pd.DataFrame({})\nlists = []\ncate = []\n\n# get the filenames\nfor dir_ in dir_name:\n    # file name\n    list_ = os.listdir(\"\/kaggle\/input\/intel-image-classification\/seg_test\/seg_test\/\"+dir_+\"\/\")\n    lists = lists+list_\n    # category name\n    cate_ = np.tile(dir_,len(list_))\n    cate = np.concatenate([cate,cate_])\n\n# insert dataframe\ndf[\"cate\"] = cate\ndf[\"name\"] = lists","8bb513ea":"# Define data size\nsize = 128","b344759e":"# data loading\n# Create image data list\nimg_data = []\n\n# Data loading\nfor dir_ in dir_name:\n    for name in df[df[\"cate\"]==dir_][\"name\"]:\n        path = \"\/kaggle\/input\/intel-image-classification\/seg_test\/seg_test\/\"+dir_+\"\/\"+name+\"\"\n        img = cv2.imread(path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Change to color array, BGR\u21d2RGB\n        image = cv2.resize(img, (size,size), interpolation=cv2.INTER_AREA)\n        img_data.append(image)\n\n# Add to dataframe\ndf[\"img\"] = img_data","2f71f6cd":"# datasize\nprint(\"data set size:{}\".format(df.shape))","76cd8726":"# Null check\nprint(\"Null data:{}\".format(df.isnull().sum()))","01542915":"# unique categorise\nprint(\"unique category:\\n{}\".format(df[\"cate\"].value_counts()))","e8695433":"# image check\ncate_name = df[\"cate\"].value_counts().index\n\nfig, ax = plt.subplots(6,4, figsize=(20,30))\nfor i in range(6):\n    for j in range(4):\n        ax[i,j].imshow(df[df[\"cate\"]==cate_name[i]][\"img\"].values[j])\n        ax[i,j].set_title(cate_name[i])","4765bd51":"# Create each category dataframe\ngla_df = pd.DataFrame({})\nmou_df = pd.DataFrame({})\nsea_df = pd.DataFrame({})\nstr_df = pd.DataFrame({})\nfor_df = pd.DataFrame({})\nbui_df = pd.DataFrame({})","619260f1":"# Define function\ndef create_rgb_df(sample_df):\n    create_df = pd.DataFrame({})\n    # Create each list\n    red_mean = []\n    red_std = []\n    green_mean = []\n    green_std = []\n    blue_mean = []\n    blue_std = []\n    \n    for i in range(len(sampling_df )):\n        red_m = sampling_df[\"img\"].values[i][:,:,0].mean()\n        red_s = sampling_df[\"img\"].values[i][:,:,0].std()\n        green_m = sampling_df[\"img\"].values[i][:,:,1].mean()\n        green_s = sampling_df[\"img\"].values[i][:,:,1].std()\n        blue_m = sampling_df[\"img\"].values[i][:,:,2].mean()\n        blue_s = sampling_df[\"img\"].values[i][:,:,2].std()\n        # Append to list\n        red_mean.append(red_m)\n        red_std.append(red_s)\n        green_mean.append(green_m)\n        green_std.append(green_s)\n        blue_mean.append(blue_m)\n        blue_std.append(blue_s)\n\n    create_df[\"red_mean\"] = red_mean\n    create_df[\"red_std\"] = red_std\n    create_df[\"green_mean\"] = green_mean\n    create_df[\"green_std\"] = green_std\n    create_df[\"blue_mean\"] = blue_mean\n    create_df[\"blue_std\"] = blue_std\n    \n    return create_df","c2f3892a":"# Define sampling_df of glacier\nsampling_df = df[df[\"cate\"]==\"glacier\"]\ngla_df = create_rgb_df(sampling_df)\n\n# Define sampling_df of mountain\nsampling_df = df[df[\"cate\"]==\"mountain\"]\nmou_df = create_rgb_df(sampling_df)\n\n# Define sampling_df of sea\nsampling_df = df[df[\"cate\"]==\"sea\"]\nsea_df = create_rgb_df(sampling_df)\n\n# Define sampling_df of street\nsampling_df = df[df[\"cate\"]==\"street\"]\nstr_df = create_rgb_df(sampling_df)\n\n# Define sampling_df of forest\nsampling_df = df[df[\"cate\"]==\"forest\"]\nfor_df = create_rgb_df(sampling_df)\n\n# Define sampling_df of building\nsampling_df = df[df[\"cate\"]==\"buildings\"]\nbui_df = create_rgb_df(sampling_df)","9251ad9c":"# Visualization of Red mean distribution\nplt.figure(figsize=(10,6))\nsns.distplot(gla_df[\"red_mean\"], label=\"glacier\")\nsns.distplot(mou_df[\"red_mean\"],label=\"mountain\")\nsns.distplot(sea_df[\"red_mean\"], label=\"sea\")\nsns.distplot(str_df[\"red_mean\"], label=\"street\")\nsns.distplot(for_df[\"red_mean\"], label=\"forest\")\nsns.distplot(bui_df[\"red_mean\"], label=\"buildings\")\nplt.title(\"Red color distribution\")\nplt.legend()","285f1224":"# Visualization of Green mean distribution\nplt.figure(figsize=(10,6))\nsns.distplot(gla_df[\"green_mean\"], label=\"glacier\")\nsns.distplot(mou_df[\"green_mean\"],label=\"mountain\")\nsns.distplot(sea_df[\"green_mean\"], label=\"sea\")\nsns.distplot(str_df[\"green_mean\"], label=\"street\")\nsns.distplot(for_df[\"green_mean\"], label=\"forest\")\nsns.distplot(bui_df[\"green_mean\"], label=\"buildings\")\nplt.title(\"Green color distribution\")\nplt.legend()","3cb3484a":"# Visualization of Blue mean distribution\nplt.figure(figsize=(10,6))\nsns.distplot(gla_df[\"blue_mean\"], label=\"glacier\")\nsns.distplot(mou_df[\"blue_mean\"],label=\"mountain\")\nsns.distplot(sea_df[\"blue_mean\"], label=\"sea\")\nsns.distplot(str_df[\"blue_mean\"], label=\"street\")\nsns.distplot(for_df[\"blue_mean\"], label=\"forest\")\nsns.distplot(bui_df[\"blue_mean\"], label=\"buildings\")\nplt.title(\"Blue color distribution\")\nplt.legend()","c3ff8d72":"# create dummy variables\ndf = pd.concat([df,pd.get_dummies(df[\"cate\"])], axis=1)\ndf.head()","75017902":"# Define parameters\nX = df[\"img\"]\ny = df[[\"buildings\", \"forest\", \"glacier\", \"mountain\", \"sea\", \"street\"]]\ncategory = [\"buildings\", \"forest\", \"glacier\", \"mountain\", \"sea\", \"street\"]\n\n# Separate the train and test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\nX_val, y_val = X_test.head(450), y_test.head(450)\nX_test, y_test = X_test.tail(450), y_test.tail(450)\n\nconsid_index = X_test.index","70a3dec4":"# Image data\n# Data shaping & Change to ndarray\nX_train_array = np.ndarray(shape=(len(X_train),size,size,3), dtype=np.float32)\nX_val_array = np.ndarray(shape=(len(X_val),size,size,3), dtype=np.float32)\nX_test_array = np.ndarray(shape=(len(X_test),size,size,3), dtype=np.float32)\n\n# Define function\ndef change_array(input_array, output_array):\n    for i in range(len(input_array)):\n        output_array[i] = input_array.values[i]\n    return output_array\n\nX_train_array = change_array(X_train, X_train_array)\nX_val_array = change_array(X_val, X_val_array)\nX_test_array = change_array(X_test, X_test_array)\n\n# Scaling\nX_train = X_train_array\/255\nX_val = X_val_array\/255\nX_test = X_test_array\/255\n\n# Checking dimension\nprint(\"train data shape:{}\".format(X_train.shape))\nprint(\"train data shape:{}\".format(X_val.shape))\nprint(\"test data shape:{}\".format(X_test.shape))","f4116ec0":"# Traget data\ny_train1 = y_train[\"buildings\"]\ny_train2 = y_train[\"forest\"]\ny_train3 = y_train[\"glacier\"]\ny_train4 = y_train[\"mountain\"]\ny_train5 = y_train[\"sea\"]\ny_train6 = y_train[\"street\"]\n\n# Val data\ny_val1 = y_val[\"buildings\"]\ny_val2 = y_val[\"forest\"]\ny_val3 = y_val[\"glacier\"]\ny_val4 = y_val[\"mountain\"]\ny_val5 = y_val[\"sea\"]\ny_val6 = y_val[\"street\"]\n\n# Test data\ny_test1 = y_test[\"buildings\"]\ny_test2 = y_test[\"forest\"]\ny_test3 = y_test[\"glacier\"]\ny_test4 = y_test[\"mountain\"]\ny_test5 = y_test[\"sea\"]\ny_test6 = y_test[\"street\"]","965be7d3":"# Convert class vectors to binary class metrics\ny_train1 = keras.utils.to_categorical(y_train1,2)\ny_train2 = keras.utils.to_categorical(y_train2,2)\ny_train3 = keras.utils.to_categorical(y_train3,2)\ny_train4 = keras.utils.to_categorical(y_train4,2)\ny_train5 = keras.utils.to_categorical(y_train5,2)\ny_train6 = keras.utils.to_categorical(y_train6,2)\n\n# Val data\ny_val1 = keras.utils.to_categorical(y_val1,2)\ny_val2 = keras.utils.to_categorical(y_val2,2)\ny_val3 = keras.utils.to_categorical(y_val3,2)\ny_val4 = keras.utils.to_categorical(y_val4,2)\ny_val5 = keras.utils.to_categorical(y_val5,2)\ny_val6 = keras.utils.to_categorical(y_val6,2)\n\n# Test data\ny_test1 = keras.utils.to_categorical(y_test1,2)\ny_test2 = keras.utils.to_categorical(y_test2,2)\ny_test3 = keras.utils.to_categorical(y_test3,2)\ny_test4 = keras.utils.to_categorical(y_test4,2)\ny_test5 = keras.utils.to_categorical(y_test5,2)\ny_test6 = keras.utils.to_categorical(y_test6,2)\n\n# shape check\nprint(\"y_train data shape:{}\".format(y_train1.shape))\nprint(\"y_val data shape:{}\".format(y_val1.shape))\nprint(\"y_test data shape:{}\".format(y_test1.shape))","f6775ea0":"def define_model():\n    # Model\n    inputs = Input(shape=(size, size, 3))\n    \n    # 1st layer\n    x = BatchNormalization()(inputs)\n    x = Conv2D(filters=128, kernel_size=(3,3), strides=(1,1))(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Conv2D(filters=128, kernel_size=(3,3), strides=(1,1))(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = MaxPool2D(pool_size=(2,2))(x)\n    x = Dropout(0.2)(x)\n    \n    # 2nd layer\n    x = Conv2D(filters=256, kernel_size=(3,3), strides=(1,1))(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Conv2D(filters=256, kernel_size=(3,3), strides=(1,1))(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = MaxPool2D(pool_size=(2,2))(x)\n    x = Dropout(0.2)(x)\n    \n    # 3rd layer\n    x = Conv2D(filters=512, kernel_size=(3,3), strides=(1,1))(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Conv2D(filters=512, kernel_size=(3,3), strides=(1,1))(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = MaxPool2D(pool_size=(2,2))(x)\n    x = Dropout(0.2)(x)\n    \n    # Flatten\n    x = Flatten()(x)\n    \n    # Dens layer\n    x = Dense(1024, activation=\"relu\")(x)\n    x = Dropout(0.2)(x)\n    x = Dense(512, activation=\"relu\")(x)\n    \n    output1 = Dense(2, activation=\"softmax\", name='output1')(x)\n    output2 = Dense(2, activation=\"softmax\", name='output2')(x)\n    output3 = Dense(2, activation=\"softmax\", name='output3')(x)\n    output4 = Dense(2, activation=\"softmax\", name='output4')(x)\n    output5 = Dense(2, activation=\"softmax\", name='output5')(x)\n    output6 = Dense(2, activation=\"softmax\", name='output6')(x)\n    \n    multiModel = Model(inputs, [output1, output2, output3, output4, output5, output6])\n    \n    # initiate Adam optimizer\n    opt = keras.optimizers.adam(lr=0.001, decay=0.00001)\n    \n    # Compile\n    multiModel.compile(loss={\"output1\":\"categorical_crossentropy\",\n                            \"output2\":\"categorical_crossentropy\",\n                            \"output3\":\"categorical_crossentropy\",\n                            \"output4\":\"categorical_crossentropy\",\n                            \"output5\":\"categorical_crossentropy\",\n                            \"output6\":\"categorical_crossentropy\"},\n                      optimizer = opt,\n                      metrics=[\"accuracy\"])\n    return multiModel","0b9f5cdc":"# Define early stoppint\nes_cb = EarlyStopping(monitor=\"val_loss\",\n                     patience=10,\n                     verbose=1)\ncp_cb = ModelCheckpoint(\"cnn_model_01_h1\",\n                       monitor=\"val_loss\",\n                       verbose=1,\n                       save_best_only=True)\n\n# parameters\nbatch_size=10\nepochs=100\n\n# train model fitting\nmodel_cnn = define_model()\nhistry = model_cnn.fit(X_train,\n                  {\"output1\":y_train1,\n                  \"output2\":y_train2,\n                  \"output3\":y_train3,\n                  \"output4\":y_train4,\n                  \"output5\":y_train5,\n                  \"output6\":y_train6},\n                  batch_size=batch_size,\n                  epochs=epochs,\n                  validation_data=(X_val,\n                                   {\"output1\":y_val1,\n                                   \"output2\":y_val2,\n                                   \"output3\":y_val3,\n                                   \"output4\":y_val4,\n                                   \"output5\":y_val5,\n                                   \"output6\":y_val6,\n                                   }),\n                   callbacks=[es_cb, cp_cb])","343e5d63":"# train_loss\ntrain1_loss = histry.history[\"output1_loss\"]\ntrain2_loss = histry.history[\"output2_loss\"]\ntrain3_loss = histry.history[\"output3_loss\"]\ntrain4_loss = histry.history[\"output4_loss\"]\ntrain5_loss = histry.history[\"output5_loss\"]\ntrain6_loss = histry.history[\"output6_loss\"]\n\n# val_loss\nval1_loss = histry.history[\"val_output1_loss\"]\nval2_loss = histry.history[\"val_output2_loss\"]\nval3_loss = histry.history[\"val_output3_loss\"]\nval4_loss = histry.history[\"val_output4_loss\"]\nval5_loss = histry.history[\"val_output5_loss\"]\nval6_loss = histry.history[\"val_output6_loss\"]\n\n\n# train_accuracy\ntrain1_acc = histry.history[\"output1_accuracy\"]\ntrain2_acc = histry.history[\"output2_accuracy\"]\ntrain3_acc = histry.history[\"output3_accuracy\"]\ntrain4_acc = histry.history[\"output4_accuracy\"]\ntrain5_acc = histry.history[\"output5_accuracy\"]\ntrain6_acc = histry.history[\"output6_accuracy\"]\n\n\n# val_accuracy\nval1_acc = histry.history[\"val_output1_accuracy\"]\nval2_acc = histry.history[\"val_output2_accuracy\"]\nval3_acc = histry.history[\"val_output3_accuracy\"]\nval4_acc = histry.history[\"val_output4_accuracy\"]\nval5_acc = histry.history[\"val_output5_accuracy\"]\nval6_acc = histry.history[\"val_output6_accuracy\"]\n\n\n# Visualization\nfig, ax = plt.subplots(4,3,figsize=(25,25))\nplt.subplots_adjust(wspace=0.3)\n\n# train1 loss\nax[0,0].plot(range(len(train1_loss)), train1_loss, label='train1_loss')\nax[0,0].plot(range(len(val1_loss)), val1_loss, label='val1_loss')\nax[0,0].set_xlabel('epoch', fontsize=16)\nax[0,0].set_ylabel('loss', fontsize=16)\nax[0,0].set_yscale('log')\nax[0,0].legend(fontsize=16)\n\n# train2 loss\nax[0,1].plot(range(len(train2_loss)), train2_loss, label='train2_loss')\nax[0,1].plot(range(len(val2_loss)), val2_loss, label='val2_loss')\nax[0,1].set_xlabel('epoch', fontsize=16)\nax[0,1].set_ylabel('loss', fontsize=16)\nax[0,1].set_yscale('log')\nax[0,1].legend(fontsize=16)\n\n# train3 loss\nax[0,2].plot(range(len(train3_loss)), train3_loss, label='train3_loss')\nax[0,2].plot(range(len(val3_loss)), val3_loss, label='val3_loss')\nax[0,2].set_xlabel('epoch', fontsize=16)\nax[0,2].set_ylabel('loss', fontsize=16)\nax[0,2].set_yscale('log')\nax[0,2].legend(fontsize=16)\n\n# train4 loss\nax[1,0].plot(range(len(train4_loss)), train4_loss, label='train4_loss')\nax[1,0].plot(range(len(val4_loss)), val4_loss, label='val4_loss')\nax[1,0].set_xlabel('epoch', fontsize=16)\nax[1,0].set_ylabel('loss', fontsize=16)\nax[1,0].set_yscale('log')\nax[1,0].legend(fontsize=16)\n\n# train5 loss\nax[1,1].plot(range(len(train5_loss)), train5_loss, label='train5_loss')\nax[1,1].plot(range(len(val5_loss)), val5_loss, label='val5_loss')\nax[1,1].set_xlabel('epoch', fontsize=16)\nax[1,1].set_ylabel('loss', fontsize=16)\nax[1,1].set_yscale('log')\nax[1,1].legend(fontsize=16)\n\n# train6 loss\nax[1,2].plot(range(len(train6_loss)), train6_loss, label='train6_loss')\nax[1,2].plot(range(len(val6_loss)), val6_loss, label='val6_loss')\nax[1,2].set_xlabel('epoch', fontsize=16)\nax[1,2].set_ylabel('loss', fontsize=16)\nax[1,2].set_yscale('log')\nax[1,2].legend(fontsize=16)\n\n\n\n# train1 accuracy\nax[2,0].plot(range(len(train1_acc)), train1_acc, label='train1_accuracy')\nax[2,0].plot(range(len(val1_acc)), val1_acc, label='val1_accuracy')\nax[2,0].set_xlabel('epoch', fontsize=16)\nax[2,0].set_ylabel('accuracy', fontsize=16)\nax[2,0].set_yscale('log')\nax[2,0].legend(fontsize=16)\n\n# train2 accuracy\nax[2,1].plot(range(len(train2_acc)), train2_acc, label='train2_accuracy')\nax[2,1].plot(range(len(val2_acc)), val2_acc, label='val2_accuracy')\nax[2,1].set_xlabel('epoch', fontsize=16)\nax[2,1].set_ylabel('accuracy', fontsize=16)\nax[2,1].set_yscale('log')\nax[2,1].legend(fontsize=16)\n\n# train3 accuracy\nax[2,2].plot(range(len(train3_acc)), train3_acc, label='train3_accuracy')\nax[2,2].plot(range(len(val3_acc)), val3_acc, label='val3_accuracy')\nax[2,2].set_xlabel('epoch', fontsize=16)\nax[2,2].set_ylabel('accuracy', fontsize=16)\nax[2,2].set_yscale('log')\nax[2,2].legend(fontsize=16)\n\n# train4 accuracy\nax[3,0].plot(range(len(train4_acc)), train4_acc, label='train4_accuracy')\nax[3,0].plot(range(len(val4_acc)), val4_acc, label='val4_accuracy')\nax[3,0].set_xlabel('epoch', fontsize=16)\nax[3,0].set_ylabel('accuracy', fontsize=16)\nax[3,0].set_yscale('log')\nax[3,0].legend(fontsize=16)\n\n# train5 accuracy\nax[3,1].plot(range(len(train5_acc)), train5_acc, label='train5_accuracy')\nax[3,1].plot(range(len(val5_acc)), val5_acc, label='val5_accuracy')\nax[3,1].set_xlabel('epoch', fontsize=16)\nax[3,1].set_ylabel('accuracy', fontsize=16)\nax[3,1].set_yscale('log')\nax[3,1].legend(fontsize=16)\n\n# train6 accuracy\nax[3,2].plot(range(len(train6_acc)), train6_acc, label='train6_accuracy')\nax[3,2].plot(range(len(val6_acc)), val6_acc, label='val6_accuracy')\nax[3,2].set_xlabel('epoch', fontsize=16)\nax[3,2].set_ylabel('accuracy', fontsize=16)\nax[3,2].set_yscale('log')\nax[3,2].legend(fontsize=16)","689632d2":"# with val data, confirming confusion matrix\ny_pred_cnn = load_model(\"cnn_model_01_h1\").predict(X_test)","9ddaeaef":"# Result of predicted dataframe\ncnn_pred = pd.DataFrame({category[0]:[i[1] for i in y_pred_cnn[0]],\n                         category[1]:[i[1] for i in y_pred_cnn[1]],\n                         category[2]:[i[1] for i in y_pred_cnn[2]],\n                         category[3]:[i[1] for i in y_pred_cnn[3]],\n                         category[4]:[i[1] for i in y_pred_cnn[4]],\n                         category[5]:[i[1] for i in y_pred_cnn[5]]})\n\n# change to float to flag from max value.\ndef flag_df(df):\n    for i in range(df.shape[0]):\n        max_ = df.iloc[i,:].max()\n        for j in range(df.shape[1]):\n            max_ = df.iloc[i,j].max()\n\ncnn_pred.head(10)","3b170a27":"# print roc_auc score\nprint(\"roc_auc score:{}\".format(roc_auc_score(y_true=y_test, y_score=cnn_pred, average=\"weighted\").round(3)))","f9552df2":"fig, ax = plt.subplots(4,6, figsize=(25,20))\n\nfor i in range(12):\n    if i < 6:\n        ax[0,i].imshow(X_test[i])\n        ax[1,i].bar(category, y_test.iloc[i,:], label=\"True label\")\n        ax[1,i].bar(category, cnn_pred.iloc[i,:], label=\"Predicted probability\")\n        ax[1,i].xaxis.set_tick_params(rotation=90)\n        ax[1,i].legend()\n    else:\n        ax[2,i-6].imshow(X_test[i])\n        ax[3,i-6].bar(category, y_test.iloc[i,:], label=\"True label\")\n        ax[3,i-6].bar(category, cnn_pred.iloc[i,:], label=\"Predicted probability\")\n        ax[3,i-6].xaxis.set_tick_params(rotation=90)\n        ax[3,i-6].legend()","5419d725":"# Cinfirming the test data index\nprint(\"Sea and sky image:{}\".format(consid_index[2]))\nprint(\"Street image:{}\".format(consid_index[8]))\nprint(\"forest image:{}\".format(consid_index[11]))","7fe0a468":"# The argument is data, filter size\nsample_img = img_data[609]\n\nfig, ax = plt.subplots(2,3, figsize=(20,10))\n\nax[0,0].imshow(sample_img)\nax[0,0].set_title(\"row data\")\n\nax[0,1].imshow(cv2.filter2D(sample_img,-1,\n                            np.ones((3,3),np.float32)\/9))\nax[0,1].set_title(\"filter 3*3\")\n               \nax[0,2].imshow(cv2.filter2D(sample_img,-1,\n                            np.ones((5,5),np.float32)\/25))\nax[0,2].set_title(\"filter 5*5\")\n               \nax[1,0].imshow(cv2.filter2D(sample_img,-1,\n                            np.ones((7,7),np.float32)\/49))\nax[1,0].set_title(\"filter 7*7\")\n               \nax[1,1].imshow(cv2.filter2D(sample_img,-1,\n                            np.ones((10,10),np.float32)\/100))\nax[1,1].set_title(\"filter 10*10\")\n               \nax[1,2].imshow(cv2.filter2D(sample_img,-1,\n                            np.ones((15,15),np.float32)\/175))\nax[1,2].set_title(\"filter 15*15\")","e338f8ac":"# The argument is data, filter size, stdv\nsample_img = img_data[695]\n\nfig, ax = plt.subplots(2,3, figsize=(20,10))\n\nax[0,0].imshow(sample_img)\nax[0,0].set_title(\"row data\")\n\nax[0,1].imshow(cv2.GaussianBlur(sample_img,(3,3),0))\nax[0,1].set_title(\"filter 3*3, 0\")\n               \nax[0,2].imshow(cv2.GaussianBlur(sample_img,(5,5),0))\nax[0,2].set_title(\"filter 5*5, 0\")\n               \nax[1,0].imshow(cv2.GaussianBlur(sample_img,(7,7),0))\nax[1,0].set_title(\"filter 7*7, 0\")\n               \nax[1,1].imshow(cv2.GaussianBlur(sample_img,(5,5),1))\nax[1,1].set_title(\"filter 5*5, 1\")\n               \nax[1,2].imshow(cv2.GaussianBlur(sample_img,(7,7),1))\nax[1,2].set_title(\"filter 7*7, 1\")","e56e95da":"# The argument is data, minVal, maxVal\nsample_img = img_data[1468]\n\nfig, ax = plt.subplots(2,3, figsize=(20,10))\n\nax[0,0].imshow(sample_img)\nax[0,0].set_title(\"row data\")\n\nax[0,1].imshow(cv2.Canny(sample_img, 100, 200))\nax[0,1].set_title(\"minVal:100, maxVal:200\")\n               \nax[0,2].imshow(cv2.Canny(sample_img, 100, 400))\nax[0,2].set_title(\"minVal:100, maxVal:300\")\n               \nax[1,0].imshow(cv2.Canny(sample_img, 100, 600))\nax[1,0].set_title(\"minVal:100, maxVal:150\")\n               \nax[1,1].imshow(cv2.Canny(sample_img, 50, 200))\nax[1,1].set_title(\"minVal:150, maxVal:200\")\n               \nax[1,2].imshow(cv2.Canny(sample_img, 10, 200))\nax[1,2].set_title(\"minVal:50, maxVal:200\")","09e1e01f":"### I tried to classify the image of the intel lmage using CNN. Which category does the result relatively often belong to? It turns out that can be predicted. On the other hand, the results were unpredictable for images in which feature lines were difficult to extract, images with too many feature lines, and images that were difficult to judge due to overlapping classification definitions. It was found that how to deal with these features, which are trade-offs in the CNN filtering process, is an important point for improving accuracy.","de9dcd41":"## GaussianBlur","7f03be1b":"- roc_auc_score is 0.918, it turns out that it can be predicted with high numbers.","0bde18e1":"## Difine CNN model","a108c2d5":"### Color distribution\n\nComparison each average and standard deviation RGB of image, with category.","75e8da08":"- Even if you look at the individual photos, you can distinguish between those that are well predicted and those that are not, but the characteristic shape is captured.\n- Looking at things that could not be done, for example, the example of predicting the sky and sea figure as a glacier may have missed the prediction because there are no characteristic lines and the colors are close.\nIn addition, \n- In the example where a road is called a building, there is a building in the photo, and people can make a mistake when thinking about it.\n- The prediction result is not clear in the example of a tree photograph. Probably the straight line got lost on the road or building.","ab2049a8":"### File name dataframe","702fd13c":"## Convlution\nIn order to confirm the filtering state in the convolutional layer, I made an image by simulating with cv2. <br>\nLooking at the results, it is difficult to predict the sea. It can be seen that it becomes difficult to visually recognize that it is the sea when the filter is applied. Since there is no particular characteristic contour, it is predicted that identification will be difficult.\n\nReference) https:\/\/docs.opencv.org\/master\/d4\/d86\/group__imgproc__filter.html#gaabe8c836e97159a9193fb0b11ac52cf1","fc207873":"## Edge detection with Canny[](http:\/\/)","9a4ebf2e":"## Image data EDA","e624b0e2":"## Validation","c940e5cf":"### Data confirming","3cae3ed4":"train, test data splitting","ac118c9e":"# CNN prediction","da34c7c7":"* Maybe my guess is wrong. Next, for deeper understanding, I would like to deepen my understanding by analyzing the results of the latest prediction methods such as EfficientNet and other themes.","0106ae01":"- Data loading and image check\n- Image data EDA, color distribution check\n- CNN prediction\n- Validation\n- Consideration simulation filtering image with cv2","5d19fc30":"### Image datas","3ca8d0ca":"## Data [](http:\/\/)preporcessing","a5ec7e70":"- Color distributions are different each categories. Color can have cateogorical information.\n- For some colors, the differences between categories are small. For example, in red color, mountain and road, and in glaciers and sea have similar colors.","48897391":"This result is the result of filtering by the technique called GaussianBlur, but as the clouds in the distance are filtered, the shadows become sharp and look like mountains. It can be considered that such a feature is the reason for erroneous recognition.\n\nReference) https:\/\/docs.opencv.org\/master\/d4\/d86\/group__imgproc__filter.html#gaabe8c836e97159a9193fb0b11ac52cf1","e07e7f1b":"## Data loading","162f9ec7":"Finally, using the example of forest, let's check the extraction of feature lines using Canny. As we can see, the result is something to not understand. Although a straight image can be read, it is difficult to distinguish whether this is a straight line of another target building or road.\n\nReference) https:\/\/docs.opencv.org\/master\/da\/d22\/tutorial_py_canny.html","910a76de":"## Conclusion","420f3a9e":"# Intel img, EDA and CNN prediction","cea7ec48":"## Consideration simulation filtering image with cv2"}}