{"cell_type":{"3d6bc9f2":"code","dd034ff7":"code","27a82fa7":"code","5432742f":"code","678ff56b":"code","c583a6ba":"code","ed6bf8e9":"code","c20aba67":"code","50e1dbc8":"code","bf50fb74":"code","efc6af9f":"code","f3863c88":"code","4725781d":"code","78569c8a":"markdown","3bcf8359":"markdown","def1d138":"markdown","3ebc090c":"markdown","951d702c":"markdown","f46b1f1b":"markdown","bbe49069":"markdown","b47dcf5c":"markdown","d40e9d0a":"markdown","87d9f4f4":"markdown","da039f81":"markdown","87112f07":"markdown","6db02cb8":"markdown","0646640f":"markdown","0db439f4":"markdown","aecb0c35":"markdown","d41ce9be":"markdown","9d3b16ef":"markdown","f4a1471d":"markdown","2c2c72a8":"markdown","93c0a536":"markdown","125a96b8":"markdown","938eb0b6":"markdown","ad0d8b39":"markdown","bcc10f7c":"markdown","bfab7c5a":"markdown"},"source":{"3d6bc9f2":"import json\nimport pandas as pd","dd034ff7":"from bs4 import BeautifulSoup\nimport requests\nimport re\n\ndef extract_job_description(offer_url):\n    \"\"\" Scan through page content and extract the text needed \"\"\"\n    \n    html_doc = requests.get(offer_url).text\n    soup = BeautifulSoup(html_doc, 'html.parser')\n    description = \"\"\n\n    if \"educarriere.ci\" in offer_url:\n        selector = \".detailsOffre\"\n\n    if \"agenceemploijeunes.ci\" in offer_url:\n        selector = \".aej_art\"\n    \n    if \"atoo.ci\" in offer_url:\n        selector = \".job-desc\"\n    \n\n    for x in soup.select(selector):\n        description += x.get_text()\n\n    return description.replace(\"\\n\\n\", \"\\n\")\n","27a82fa7":"# Installing the depencency\n!python -m spacy download fr_core_news_md","5432742f":"%%time\nimport re\nimport pandas as pd\nimport spacy\nimport fr_core_news_md\n\ndataset = pd.read_csv('..\/input\/job-offers-in-cte-divoire-november-2020\/offers.csv')\n\n# lower case, remove digits, stopwords, and punctuations\nnlp = fr_core_news_md.load()\nstop_words = list(\"\\n cabinet type dipl\u00f4me cl\u00f4ture date envoyez niveau espace offre emploi entreprise cabinets oct connecter emplois fcfa prix espaces articles tax an cv entretien embauche motivation lettre conseils questions mail travail r\u00e9ponses recruteurs expir\u00e9e stage entreprise envoyer faire activit\u00e9 secteur avoir profil dossiers entreprise ans adresse sens cv lieu\".split())\ndata = [re.sub(r'[\\d?,.;:!\/-]', '', x).strip().lower() for x in dataset['content']]\ndata = [doc for doc in list(nlp.pipe(data))]\n\n# keep only nouns and verbs\nfor x in data:\n    i = data.index(x)\n    data[i] = \" \".join([word.text for word in data[i] if word.pos_ in ['NOUN', 'VERB'] and word.text not in stop_words ])\n","678ff56b":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(max_df=0.50, min_df=3)\ndoc_term_matrix = vectorizer.fit_transform(data)","c583a6ba":"%%time\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.model_selection import GridSearchCV\n\n# Options to try with our LDA\nsearch_params = {\n  'n_components': [7, 8, 9, 10],\n  'learning_decay': [.5]\n}\n\n# Set up LDA with the options we'll keep static\nmodel = LatentDirichletAllocation(learning_method='online')\n\n# Try all of the options\ngridsearch = GridSearchCV(model, param_grid=search_params, n_jobs=-1, verbose=1)\ngridsearch.fit(doc_term_matrix)\n\n# What did we find?\nprint(\"Best Model's Params: \", gridsearch.best_params_)\nprint(\"Best Log Likelihood Score: \", gridsearch.best_score_)\n","ed6bf8e9":"import random\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport pyLDAvis\nimport pyLDAvis.sklearn\n\n# arbitrary number of categories\ntopics_n = 10\nLDAmodel = LatentDirichletAllocation(n_components=topics_n, max_iter=10, learning_method='online', learning_decay=0.5)\n\nLDAmodel.fit(doc_term_matrix)\nwords = vectorizer.get_feature_names()\n\n# limit is top ten words\ntop_n = 10\n\n# print top recurring words\nfor idx, topic in enumerate(LDAmodel.components_):\n        print(\"Top {} words in topic #{}:\".format(top_n, idx))\n        print(\", \".join([words[i]\n                        for i in topic.argsort()[-top_n:]][::-1]))\n","c20aba67":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\n\nvectorizer = TfidfVectorizer(max_df=0.8, min_df=10)\ndoc_term_matrix = vectorizer.fit_transform(data)\n\nnmfModel = NMF(n_components=10, random_state=42)\nnmfModel.fit(doc_term_matrix )\n\n# print top recurring words\nfor i, topic in enumerate(nmfModel.components_):\n    print(f'Top 20 words for topic #{i}:')\n    print([vectorizer.get_feature_names()[i] for i in topic.argsort()[-20:]])\n    print('\\n')","50e1dbc8":"labels = {\n    0: 'Marketing',\n    1: 'Finance',\n    2: 'Informatique',\n    3: 'Assistanat',\n    4: 'Transport',\n    5: 'Marketing',\n    6: 'Management',\n    7: 'Communication',\n    8: 'Technicien',\n    9: 'Entrepreneuriat'\n}","bf50fb74":"import random\nrandom_id = random.randint(0, len(dataset)-1)\ntitle, url = (dataset['title'].iloc[random_id], dataset['url'].iloc[random_id])\nprint('title: {}, URL: {}'.format(title, url))\nsample_text = extract_job_description(url)\n# transform method returns a matrix with one line per document, columns being topics weight\ntopic_values = nmfModel.transform(vectorizer.transform([sample_text]))\ndominant_topic_id = topic_values.argmax(axis=1)[0]\nprint(labels[dominant_topic_id])","efc6af9f":"url = 'http:\/\/www.atoo.ci\/emploi\/jobs\/analyste-des-risques-industriels-stagiaire-environnementaliste\/'\nsample_text = extract_job_description(url)\n# transform method returns a matrix with one line per document, columns being topics weight\ntopic_values = nmfModel.transform(vectorizer.transform([sample_text]))\ndominant_topic_id = topic_values.argmax(axis=1)[0]\nprint(labels[dominant_topic_id])","f3863c88":"import pickle\n\nmodel_path = '\/kaggle\/working\/job_classification_model.pickle'\nvect_path = '\/kaggle\/working\/job_classification_vect.pickle'\n\npickle.dump(nmfModel, open(model_path, 'wb')) \npickle.dump(vectorizer, open(vect_path, 'wb')) ","4725781d":"import pkg_resources\nimport types\ndef get_imports():\n    for name, val in globals().items():\n        if isinstance(val, types.ModuleType):\n            # Split ensures you get root package, \n            # not just imported function\n            name = val.__name__.split(\".\")[0]\n\n        elif isinstance(val, type):\n            name = val.__module__.split(\".\")[0]\n\n        # Some packages are weird and have different\n        # imported names vs. system names\n        if name == \"PIL\":\n            name = \"Pillow\"\n        elif name == \"sklearn\":\n            name = \"scikit-learn\"\n\n        yield name\nimports = list(set(get_imports()))\n\nrequirements = []\nfor m in pkg_resources.working_set:\n    if m.project_name in imports and m.project_name!=\"pip\":\n        requirements.append((m.project_name, m.version))\n\nfor r in requirements:\n    print(\"{}=={}\".format(*r))","78569c8a":"### Explore the JSON file","3bcf8359":"```python\nimport pandas as pd   \ndataset = []\nfor offer_url in data:\n    print(\"Saving item \" + str(data.index(offer_url)+1) + \": \" \n          + offer_url)\n    content = extract_job_description(offer_url)\n    item = { \"url\": offer_url, \"content\": content }\n    dataset.append(item)\n    \n# save items to CSV file\ndf = pd.DataFrame(dataset)\ndf.to_csv(\"\/kaggle\/working\/training-data.csv\")\n```","def1d138":"### Predict on training data","3ebc090c":"```shell\ncurl -d 'url=http:\/\/www.atoo.ci\/emploi\/jobs\/developpeur-web-php-laravel-2\/' 'https:\/\/classification-nlp-model.herokuapp.com' \n```","951d702c":"### Apply LDA method with the right number of topics\nThe parameter `n_components` specifies the number of categories we intend to extract from the text.","f46b1f1b":"### Prepare helper method\nThe scraping will be done using [Beautiful Soup](https:\/\/pypi.org\/project\/beautifulsoup4\/), a convenient Python library widely used for extracting content from web pages. Let's start by creating a helper method.","bbe49069":"```python\ndata = ''\nf = open('\/kaggle\/input\/job-offers-in-cte-divoire-november-2020\/job-offers.json')\nfor x in f:\n    data += x\n    \n# convert data to JSON object and extract links\ndata = [x[4] for x in json.loads(data)['values']]\n\n# get an overview of links\ndf = pd.DataFrame(data,columns=['URL'])\ndf.head()\n```","b47dcf5c":"## Export packages with specific versions","d40e9d0a":"# References\n\n1. Python for NLP: Topic modeling. https:\/\/stackabuse.com\/python-for-nlp-topic-modeling\/\n2. Topic Modeling in Python: Latent Dirichlet Allocation (LDA). https:\/\/towardsdatascience.com\/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n3. pyLDAvis.sklearn https:\/\/github.com\/bmabey\/pyLDAvis\/blob\/master\/notebooks\/sklearn.ipynb\n4. Picking the \"right\" number of topics for a scikit-learn topic model https:\/\/investigate.ai\/text-analysis\/choosing-the-right-number-of-topics-for-a-scikit-learn-topic-model\/\n5. Saving Sklearn Model to Pickle. https:\/\/medium.com\/@pemagrg\/saving-sklearn-model-to-pickle-595da291ec1c\n\n\n","87d9f4f4":"# Re-use model as a webservice\nOnce exported, the model can be embedded into a REST webservice built using Flask. Users will then be able to send HTTP POST requests to the API and receive a prediction.","da039f81":"# Context\nThe idea of this text classification came after I built a webservice that periodically collects job offers from [C\u00f4te d'Ivoire](https:\/\/en.wikipedia.org\/wiki\/Ivory_Coast) and renders them [through a REST API](http:\/\/job.samuelguebo.ci\/api\/offer). However, because websites rarely categorize job offers, the webservice doesn't include any taxonomy system such as tags, topics, or categories \u2014 those terms will be used interchangeably here. This is where natural language processing (NLP) comes in handy.\n\nThe current Notebook explores how to generate category-like classification out of a set of texts using NLP models. Generating the initial classification groups is aslo known as topic modeling. In this instance, two topic modeling methods will be explored: the\nLatent Dirichlet Allocation (LDA) method the Non-Negative Matrix Factorization (NMF). Once the models are generated, they will be used to predict the relevant classification of each job offer.\n\nAs a bonus, this notebook describes some ways to export and reuse the NLP model as an API endpoint to enable future classifications.","87112f07":"### Predict on unseen data","6db02cb8":"# Generate categories using Latent Dirichlet Allocation (LDA) method\nThe aim of this method is to represent each document (job offer description) as a probability distribution of words. This will surface some clusters which will then be used to determine the categories or tags associated with each document. A crucial step will be to choose the number of categories to be generated.","0646640f":"# Map topics to human-readable labels\nAfter visualizing the top reccurent words for each topic, one should use domain knowlege to assign each topic a name. In this case the names will be in French as the API will be used by a French-speaking audience.","0db439f4":"# Classify using Non-Negative Matrix Factorization (NMF) method\nNon-negative matrix factorization is a supervised learning technique which performs topic modeling by using a process of clustering. We'll use the `TfidfVectorizer` class from `sklearn.feature_extraction.text` and only consider those words that show up in  less than 80% of the document and appear in at least 10 job offers.","aecb0c35":"# Housekeeping\nLet's import the dependencies and set up the applicable parameters.\n","d41ce9be":"# Test the webservice\nThe model was deployed as a REST API at `https:\/\/classification-nlp-model.herokuapp.com`. It only supports POST request and accepts the `url` parameter. \n\nOnce provided, the API takes care of fetching the job description based on its URI, as long as it points to a job offer posted on one of the main providers of job offers in C\u00f4te d'Ivoire: `agenceemploijeunes.ci`, `educarriere.ci`, `atoo.ci`.","9d3b16ef":"### Search for the right parameters\nThere is no easy way to choose the right number of topics. How can we tell whether there are 5, 10, or 20 topics across the dataset? It boils down to trying all these parameters and comparing how the model performs. Doing so manually would be time-consuming. This is where `GridSearchCV` comes into play. It is a Sklearn function that will automate the process by trying a variety of parameters combination, comparing how the model performs, and suggesting the best result. \n\n<span color=\"crimson\">Please in mind that trying all combinations may take some time. You'll probably need to go for a walk or make yourself some tea.<\/span>","f4a1471d":"# Predict topic for unseen texts\nNow that we have our classification system in place, it's time to test its prediction or a real-life scenario.","2c2c72a8":"```python\nfrom flask import Flask\nfrom flask import request\nfrom bs4 import BeautifulSoup\nimport pickle\nimport requests\n\napp = Flask(__name__)\n\n\n# entry point\n@app.route('\/', methods= ['POST'])\ndef predict_category():\n\n    # collect the text submited\n    job_url = request.form['url']\n\n    # load saved model and vectorizer\n    model_path = '.\/model\/job_classification_model.pickle'\n    vect_path = '.\/model\/job_classification_vect.pickle'\n\n    saved_model = pickle.load(open(model_path, 'rb'))\n    saved_vect = pickle.load(open(vect_path, 'rb'))\n     \n    # Custom categories based on specific domain knowledge \n    categories = {\n        0: 'Transport \/ Logistique',\n        1: 'Entrepeneuriat',\n        2: 'Marketing et communication',\n        3: 'Gestion de projets',\n        4: 'Management \/ Ressources humaines',\n        5: 'Comptabilit\u00e9 \/ Finance',\n        6: 'Communication',\n        7: 'Informatique',\n        8: 'Affaires juridiques'\n    }\n    \n    job_description = extract_job_description(job_url)\n    category_values = saved_model.transform(saved_vect.transform([job_description]))\n    \n    # do the prediction\n    dominant_category_id = category_values.argmax(axis=1)[0]\n    predicted_category = categories[dominant_category_id]\n\n    return predicted_category\n\n\ndef extract_job_description(offer_url):\n    \"\"\" Scan through page content and extract the text needed \"\"\"\n    \n    html_doc = requests.get(offer_url).text\n    soup = BeautifulSoup(html_doc, 'html.parser')\n    description = \"\"\n\n    # Remove HTML tags\n    if \"educarriere.ci\" in offer_url:\n        selector = \".detailsOffre\"\n\n    if \"agenceemploijeunes.ci\" in offer_url:\n        selector = \".aej_art\"\n    \n    if \"atoo.ci\" in offer_url:\n        selector = \".job-desc\"\n    \n\n    for x in soup.select(selector):\n        description += x.get_text()\n\n    return description.replace(\"\\n\\n\", \"\\n\")\n```","93c0a536":"# Build a dataset of job offers\nIn order to begin the classification, a decent amount of text can be helpful to get accurate results. As we don't currently have such texts available, a dataset will be generated using web scraping. To speed things up, a JSON file containing URLs of job offers will be used.\n","125a96b8":"### Compile the content and save it a CSV file\nUpon completion of the process, a dataset containaing a list of job offers and their corresponding descriptions will be built. <span style=\"color:crimson\">**Note:** please bear in mind that combing through the text of thousands of web pages might take some time.<\/span>","938eb0b6":"# Data cleaning and processing\nWith the dataset now ready, text processing can begin. We'll be using [Spacy, a mainstream NLP library](spacy.io\/) that is both efficient and intuitive. Since the text to be processed is in French, the relevant Spacy language model `fr_core_news_md` will be installed and used.","ad0d8b39":"### Convert texts to vectors\nIn order to analyze the content and extract patterns, texts needs to be converted into vectors a document-term matrix, representing the distribution of term across the document. We'll use the `CountVectorizer` class from `sklearn.feature_extraction.text` and only consider those words that show up in  less than 80% of the document and appear in at least 3 documents.","bcc10f7c":"### Remove stop words and unneeded characters\nTo improve NLP performance, words that are too frequent while not having much significance can be dropped. Similarly, [stop words](https:\/\/en.wikipedia.org\/wiki\/Stop_word) can be skipped for better accuracy.","bfab7c5a":"# Save model for later use\nAs explained earlier, the end goal for our model is to be deployed as a REST API which is able to receive an input \u2014 a link to a job offer \u2014 and output a prediction: the category of job offer. To that end, it needs to be exported as a serialized file using the [pickle library]()"}}