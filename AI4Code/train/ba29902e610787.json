{"cell_type":{"8a8e93a6":"code","998a5707":"code","82521a96":"code","87ed9806":"code","03f0fc8b":"code","a37cb016":"code","eceda5a6":"code","eeeb9918":"code","c05d0047":"code","9c0e9d1b":"code","dde1d326":"code","9116fc8e":"code","6dabbcc8":"code","99c80271":"code","4765ad0a":"code","87f43b5b":"code","8ea44ed9":"code","a5cf4206":"code","1e03b50d":"code","fd6650fa":"code","105c9e7e":"code","33025822":"code","c54cce17":"code","80604e44":"code","aded4ae7":"code","53e25857":"code","5c41f447":"code","20c0f378":"code","13c574b3":"code","17e2bce7":"code","9a9b48b3":"code","2eec910d":"code","424cc817":"code","4b775e23":"code","bf48c3a0":"code","613925d9":"code","112b5bb4":"code","c77e9105":"code","be1fe617":"code","555b4629":"code","402027a4":"markdown","43997f6a":"markdown","65d452c5":"markdown","eff680df":"markdown","8247b45a":"markdown","3472ce51":"markdown","2b25008d":"markdown","a939d708":"markdown","c7e7e660":"markdown","6c48e514":"markdown","4fb618df":"markdown","fac5d1dc":"markdown","0b674820":"markdown","3c25a8aa":"markdown"},"source":{"8a8e93a6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","998a5707":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nfrom sklearn.metrics import f1_score as f1\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport plotly.graph_objs as go\nimport plotly.express as ex","82521a96":"df_train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","87ed9806":"df_train.head(4)","03f0fc8b":"df_train.shape[0]","a37cb016":"df_train.isna().sum()","eceda5a6":"def remove_ht(sir):\n    idx = sir.find('%20')\n    if idx ==  -1:\n        return sir\n    else:\n        return sir[0:idx]+' '+sir[(idx+3):]\n\n\ndf_train['keyword'].fillna(df_train['keyword'].mode()[0],inplace=True)\ndf_train['keyword'] = df_train['keyword'].apply(remove_ht)\n\ndf_test['keyword'].fillna(df_test['keyword'].mode()[0],inplace=True)\ndf_test['keyword'] = df_test['keyword'].apply(remove_ht)\n","eeeb9918":"def number_of_hashtags(sir):\n    splited = sir.split(' ')\n    ht = 0\n    for word in splited:\n        if len(word) > 1 and word[0] == '#':\n            ht+=1\n    return ht\n\n\nsid = SentimentIntensityAnalyzer()\n\ndef pos_sentiment(sir):\n    r = sid.polarity_scores(sir)\n    return (r['pos'])\ndef neg_sentiment(sir):\n    r = sid.polarity_scores(sir)\n    return (r['neg'])\n\ndef number_of_words(sir):\n    splited = sir.split(' ')\n    words = 0\n    for word in splited:\n        if len(word) > 1 and word[0] != '#':\n            words+=1\n    return words\ndef number_of_exclamation_marks(sir):\n    ex = 0\n    for char in sir:\n        if char == '!':\n            ex+=1\n    return ex\n\ndef average_word_length(sir):\n    splited = sir.split(' ')\n    no_hash = [word for word in splited if '#' not in word]\n    length = 0\n    for word in no_hash:\n        length+=len(word)\n    return length\/len(no_hash)\n\ndef contains_mentions(sir):\n    splited = sir.split(' ')\n    for word in splited:\n        if '@' in word:\n            return 1\n    return 0\n\n\ndisasters = ['fire','storm','flood','tornado','earthquake','volcano','hurricane',\n            'tornado','cyclone','famine','epidemic','war','dead','collapse','crash','hostages','terror']\n\ndef contains_disaster_tag(sir):\n    splited = sir.split(' ')\n    for word in splited:\n        if word.lower() in disasters:\n            return 1\n    return 0","c05d0047":"df_train['Number_Of_Hashtags'] = df_train['text'].apply(number_of_hashtags)\ndf_train['Pos_Sentiment'] = df_train['text'].apply(pos_sentiment)\ndf_train['Neg_Sentiment'] = df_train['text'].apply(neg_sentiment)\ndf_train['Number_Of_Words'] = df_train['text'].apply(number_of_words)\ndf_train['Exc_Marks'] = df_train['text'].apply(number_of_exclamation_marks)\ndf_train['Avg_Word_Length'] = df_train['text'].apply(average_word_length)\ndf_train['Has_Mention'] = df_train['text'].apply(contains_mentions)\ndf_train['Has_Disaster_Word'] = df_train['text'].apply(contains_disaster_tag)\n\ndf_test['Number_Of_Hashtags'] = df_test['text'].apply(number_of_hashtags)\ndf_test['Pos_Sentiment'] = df_test['text'].apply(pos_sentiment)\ndf_test['Neg_Sentiment'] = df_test['text'].apply(neg_sentiment)\ndf_test['Number_Of_Words'] = df_test['text'].apply(number_of_words)\ndf_test['Exc_Marks'] = df_test['text'].apply(number_of_exclamation_marks)\ndf_test['Avg_Word_Length'] = df_test['text'].apply(average_word_length)\ndf_test['Has_Mention'] = df_test['text'].apply(contains_mentions)\ndf_test['Has_Disaster_Word'] = df_test['text'].apply(contains_disaster_tag)","9c0e9d1b":"from sklearn.preprocessing import LabelEncoder\nlabel_e = LabelEncoder()\nlabel_e.fit(df_train['keyword'])\ndf_train['Keyword'] = label_e.transform(df_train['keyword'])\n\nlabel_e = LabelEncoder()\nlabel_e.fit(df_test['keyword'])\ndf_test['Keyword'] = label_e.transform(df_test['keyword'])","dde1d326":"from wordcloud import WordCloud,STOPWORDS\nimport re\nstopwords = list(STOPWORDS)\n#find top 10 words in disasters \ndf_dis = df_train[df_train['target']==1]\n\ndis_word_freq = dict()\n\nfor sample in df_dis.text:\n    tokens = sample.lower()\n    tokens = re.findall(r'\\b[A-Za-z]+\\b',tokens)\n    tokens = [tok for tok in tokens if len(tok) > 2]\n    no_hash = [tok for tok in tokens if '#' not in tok and tok.find('http') == -1]\n    clean_tokens = [tok for tok in no_hash if tok not in stopwords]\n    for tok in clean_tokens:\n        if tok not in dis_word_freq:\n            dis_word_freq[tok] = 1\n        else:\n            dis_word_freq[tok] += 1\n\ndis_word_freq = {k: v for k, v in sorted(dis_word_freq.items(), key=lambda item: item[1])}\nwl_d = list(dis_word_freq.keys())\nwl_d = list(reversed(wl_d))\n\n","9116fc8e":"df_not_dis = df_train[df_train['target']==0]\n\nnot_dis_word_freq = dict()\n\nfor sample in df_not_dis.text:\n    tokens = sample.lower()\n    tokens = re.findall(r'\\b[A-Za-z]+\\b',tokens)\n    tokens = [tok for tok in tokens if len(tok) > 2]\n    no_hash = [tok for tok in tokens if '#' not in tok and tok.find('http') == -1]\n    clean_tokens = [tok for tok in no_hash if tok not in stopwords]\n    for tok in clean_tokens:\n        if tok not in not_dis_word_freq:\n            not_dis_word_freq[tok] = 1\n        else:\n            not_dis_word_freq[tok] += 1\n\nnot_dis_word_freq = {k: v for k, v in sorted(not_dis_word_freq.items(), key=lambda item: item[1])}\nwl = list(not_dis_word_freq.keys())\nwl = list(reversed(wl))\n\n\n\ntop_50_dist_words = wl_d[:50]\ntop_50_non_dist_words = wl[:50]\n\nlen(set())","6dabbcc8":"def amount_of_dis_tokens(sir):\n    tok = sir.split(' ')\n    tok = set(tok)\n    cont = set(top_50_dist_words).intersection(tok)\n    return len(cont)\n    \ndef amount_of_non_dis_tokens(sir):\n    tok = sir.split(' ')\n    tok = set(tok)\n    cont = set(top_50_non_dist_words).intersection(tok)\n    return len(cont)\n\ndf_train['Contains_Top50_Dist_Words'] = df_train.text.apply(amount_of_dis_tokens)\ndf_train['Contains_Top50_Non_Dist_Words'] = df_train.text.apply(amount_of_dis_tokens)\n\ndf_test['Contains_Top50_Dist_Words'] = df_test.text.apply(amount_of_dis_tokens)\ndf_test['Contains_Top50_Non_Dist_Words'] = df_test.text.apply(amount_of_dis_tokens)","99c80271":"features = df_train.columns[5:]\nY = df_train['target']\n","4765ad0a":"\n\nwords = ''\n\nfor sample in df_train.text:\n    tokens = sample.lower().split(' ')\n    no_hash = [tok for tok in tokens if '#' not in tok and tok.find('http') == -1]\n    words += ' '.join(no_hash)+' '\n\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() ","87f43b5b":"plt.figure(figsize=(20,11))\nax = sns.boxplot(x=df_train['target'],y=df_train['Avg_Word_Length'])\nax.set_xlabel('Target',fontsize=18)\nax.set_ylabel('Average Word Length',fontsize=18)\nax.set_title('Average Word Length Effect On Outcome',fontsize=18)","8ea44ed9":"plt.figure(figsize=(20,11))\nax = sns.kdeplot(df_train[df_train['target']==1]['Number_Of_Words'],label='Number Of Words Target=1')\nax = sns.kdeplot(df_train[df_train['target']==0]['Number_Of_Words'],label='Number Of Words Target=0')\nplt.legend(prop={'size':20})","a5cf4206":"plt.figure(figsize=(20,11))\npivot = df_train.pivot_table(index='Number_Of_Words',columns='Contains_Top50_Dist_Words',values='target')\nsns.heatmap(pivot,cmap='coolwarm',annot=True)\n","1e03b50d":"plt.figure(figsize=(20,11))\nax=sns.boxplot(x=df_train['target'],y=df_train['Neg_Sentiment'])","fd6650fa":"plt.figure(figsize=(20,11))\nax=sns.boxplot(x=df_train['target'],y=df_train['Avg_Word_Length'])","105c9e7e":"plt.figure(figsize=(20,11))\nax=sns.countplot(df_train['Contains_Top50_Dist_Words'])","33025822":"df_train = df_train[df_train['Avg_Word_Length'] < 11]\ndf_train = df_train[df_train['Avg_Word_Length'] > 2]\ndf_train = df_train[df_train['Neg_Sentiment'] < 0.7]\nlns = df_train[(df_train['Neg_Sentiment'] > 0.6) & (df_train['target'] == 0)]\ndf_train = df_train.drop(lns.index)","c54cce17":"df_train[features]","80604e44":"from sklearn.feature_selection import SelectKBest,chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,f1_score,classification_report\nfrom sklearn.metrics import accuracy_score as ascore\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\n \nY = df_train['target']\nselector = SelectKBest(chi2,k=4)\nX = selector.fit_transform(df_train[features],Y)\n\n\n\ntrain_x,test_x,train_y,test_y = train_test_split(X,Y)\n\ns_fet = [fet for index,fet in enumerate(features) if selector.get_support()[index]==True ]\ns_fet","aded4ae7":"def optimal_n(train_x,test_x,train_y,test_y,n_list):\n    results = []\n    for n in n_list:\n        model = KNeighborsClassifier(n_neighbors = n)\n        model.fit(train_x,train_y)\n        pred = model.predict(test_x)\n        results.append(f1(pred,test_y))\n    return results","53e25857":"n_list = [10,20,30,50,80,130,210,350,560]\nresult = optimal_n(train_x,test_x,train_y,test_y,n_list)\nplt.figure(figsize=(20,11))\nax =sns.lineplot(x=np.arange(len(n_list)),y=result)\nn_list.insert(0,1)\nax.set_xticklabels(n_list)\nax.set_title('KNN Accuracy Depending On Number Of Neighbors',fontsize=16)\nax.set_xlabel('N Value',fontsize=16)\nax.set_ylabel('Accuracy Score',fontsize=16)\nplt.show()","5c41f447":"def optimal_e(train_x,test_x,train_y,test_y,n_list):\n    results = []\n    for n in n_list:\n        model = RandomForestClassifier(max_leaf_nodes = n,random_state=42)\n        model.fit(train_x,train_y)\n        pred = model.predict(test_x)\n        results.append(f1(pred,test_y))\n    return results","20c0f378":"n_list = [2,3,5,8,13,21,35,56,91,147,200]\nresult = optimal_e(train_x,test_x,train_y,test_y,n_list)\nplt.figure(figsize=(20,11))\nax = sns.lineplot(x=np.arange(0,11),y=result)\n#n_list.insert(0,1)\nax.set_xticklabels(labels = n_list)\nax.set_title('RandomForest Accuracy Depending On Number Of Estimators',fontsize=16)\nax.set_xlabel('N Value',fontsize=16)\nax.set_ylabel('Accuracy Score',fontsize=16)\nplt.show()","13c574b3":"def optimal_n(train_x,test_x,train_y,test_y,n_list):\n    results = []\n    for n in n_list:\n        model = AdaBoostClassifier(n_estimators = n,random_state=42,learning_rate=0.05)\n        model.fit(train_x,train_y)\n        pred = model.predict(test_x)\n        results.append(f1(pred,test_y))\n    return results","17e2bce7":"ee_list = [2,3,5,8,13,21,35,56,91,147,300]\nresult = optimal_n(train_x,test_x,train_y,test_y,ee_list)\nplt.figure(figsize=(20,11))\nax =sns.lineplot(x=np.arange(len(ee_list)),y=result)\nn_list.insert(0,1)\nax.set_xticklabels(labels = ee_list)\nax.set_title('AdaBoost Accuracy Depending On Number Of Max Leaf Nodes',fontsize=16)\nax.set_xlabel('N Value',fontsize=16)\nax.set_ylabel('Accuracy Score',fontsize=16)\nplt.show()","9a9b48b3":"def optimal_n(train_x,test_x,train_y,test_y,n_list):\n    results = []\n    for n in n_list:\n        model = DecisionTreeClassifier(max_leaf_nodes = n,random_state=42,criterion='entropy')\n        model.fit(train_x,train_y)\n        pred = model.predict(test_x)\n        results.append(f1(pred,test_y))\n    return results","2eec910d":"ee_list = [2,3,5,8,13,21,35,56,91,147,300]\nresult = optimal_n(train_x,test_x,train_y,test_y,ee_list)\nplt.figure(figsize=(20,11))\nax =sns.lineplot(x=np.arange(len(ee_list)),y=result)\nn_list.insert(0,1)\nax.set_xticklabels(labels = ee_list)\nax.set_title('Decision Tree Accuracy Depending On Number Of Max Leaf Nodes',fontsize=16)\nax.set_xlabel('N Value',fontsize=16)\nax.set_ylabel('Accuracy Score',fontsize=16)\nplt.show()","424cc817":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom wordcloud import STOPWORDS\nimport re \nimport string\nfrom nltk.stem import PorterStemmer\nstop_words = list(STOPWORDS)\nstemmer = PorterStemmer()\ndf_train.text = df_train.text.apply(lambda x : re.sub(r'^RT[\\s]+', '', x))\ndf_train.text = df_train.text.apply(lambda x : re.sub(r'#', '', x))\ndf_train.text = df_train.text.apply(lambda x : re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', x))\ndf_train.text = df_train.text.apply(lambda x : ' '.join([word for word in x.split(' ') if word not in stop_words]))\ndf_train.text =df_train.text.str.lower()\ndf_train.text = df_train.text.apply(lambda x : ' '.join([ stemmer.stem(word) for word in x.split(' ')]) )\n\ndf_train\n\n\nvect = CountVectorizer()\nvect.fit(df_train.text)\n\ntf_matrix = vect.transform(df_train.text)\nsvd = TruncatedSVD(n_components=1500)\ntrain_sparse = svd.fit_transform(tf_matrix)\n","4b775e23":"cum_var = np.cumsum(svd.explained_variance_ratio_)\ntr1 = go.Scatter(x=np.arange(len(cum_var)),y=cum_var)\ngo.Figure(data=[tr1],layout={'title':'explained variance ratio train data','xaxis_title':\"Number Of Components\",'yaxis_title':\"explained variance\"})","bf48c3a0":"df_test.text = df_test.text.apply(lambda x : re.sub(r'^RT[\\s]+', '', x))\ndf_test.text = df_test.text.apply(lambda x : re.sub(r'#', '', x))\ndf_test.text = df_test.text.apply(lambda x : re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', x))\ndf_test.text = df_test.text.apply(lambda x : ' '.join([word for word in x.split(' ') if word not in stop_words]))\ndf_test.text = df_test.text.str.lower()\ndf_test.text = df_test.text.apply(lambda x : ' '.join([ stemmer.stem(word) for word in x.split(' ')]) )\n\ndf_test\n\n\nvect = CountVectorizer()\nvect.fit(df_test.text)\n\ntf_matrix = vect.transform(df_test.text)\nsvd = TruncatedSVD(n_components=1500)\ntest_sparse = svd.fit_transform(tf_matrix)\n","613925d9":"cum_var = np.cumsum(svd.explained_variance_ratio_)\ntr1 = go.Scatter(x=np.arange(len(cum_var)),y=cum_var)\ngo.Figure(data=[tr1],layout={'title':'explained variance ratio test data','xaxis_title':\"Number Of Components\",'yaxis_title':\"explained variance\"})","112b5bb4":"df_train","c77e9105":"from sklearn.naive_bayes import GaussianNB\ntr_sp = pd.DataFrame(train_sparse.copy())\ntr_sp['Pos_Sentiment'] = df_train.Pos_Sentiment\ntr_sp['Neg_Sentiment'] = df_train.Neg_Sentiment\ntr_sp['Number_Of_Hashtags'] = df_train.Number_Of_Hashtags\ntr_sp.Number_Of_Hashtags = tr_sp.Number_Of_Hashtags.fillna(tr_sp.Number_Of_Hashtags.mean())\ntr_sp.Pos_Sentiment = tr_sp.Pos_Sentiment.fillna(tr_sp.Pos_Sentiment.mean())\ntr_sp.Neg_Sentiment = tr_sp.Neg_Sentiment.fillna(tr_sp.Neg_Sentiment.mean())\ntrain_x,test_x,train_y,test_y = train_test_split(tr_sp,df_train.target)\nNB = GaussianNB()\nNB.fit(train_x,train_y)\npred = NB.predict(test_x)\nconf = confusion_matrix(pred,test_y)\nplt.figure(figsize=(20,11))\nax = sns.heatmap(conf,annot=True,cmap='mako',fmt='d')\nax.set_title('Naive Bayes Confusion Matrix ')","be1fe617":"from sklearn.metrics import f1_score as f1\nf1(pred,test_y)","555b4629":"selector = SelectKBest(chi2,k=4)\nX = selector.fit_transform(df_train[features],Y)\ns_fet = [fet for index,fet in enumerate(features) if selector.get_support()[index]==True ]\n\nrfc = RandomForestClassifier(max_leaf_nodes = 200,random_state=42)\ndtc = DecisionTreeClassifier(max_leaf_nodes = 21,random_state=42,criterion='entropy')\n\nrfc.fit(X,Y)\ndtc.fit(X,Y)\n\nsub = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\nprediction = np.round(rfc.predict(df_test[s_fet])*0.5+dtc.predict(df_test[s_fet])*0.5).astype('int64')\n\ncf_mat = (confusion_matrix(prediction,sub['target']))\nplt.figure(figsize=(20,11))\nax = sns.heatmap(cf_mat,cmap='coolwarm',annot=True,fmt='d')\n\n\nresult = sub.copy()\nresult['target'] = prediction\nresult.to_csv('submission.csv',index=False)","402027a4":"# So our final prediction got an exccelent accuracy score we predict with great accuracy tweets which are not disasters but in the same time our model was not able to classify a real disaster properly .","43997f6a":"### In our train dataset are 7613 samples of which 2533 samples are missing a value for the location feature and 61 are missing the value for the keyword feature.","65d452c5":"### Count Vectorezation","eff680df":"# Final Prediction And Results","8247b45a":"### Its quite clear that the scores we got so far are far from being high, our next approach will be vectorezation with dimension reductuion","3472ce51":"### our distribution in actual disasters number of words is approching a bimodal distribution","2b25008d":"### We can see that words that contain 3 or more words that are classified as the top 30 words for disaster tweets are usually real disasters!","a939d708":"# EDA","c7e7e660":"## Removing Outliers","6c48e514":"# Feature Engineering","4fb618df":"# Different Model Evaluation","fac5d1dc":"### We see that tweets where the average word length is higher usually indicate on a real disasters","0b674820":"![](https:\/\/static1.squarespace.com\/static\/54db7b69e4b00a5e4b11038c\/54db7d17e4b0c0e085a1149c\/54fcce81e4b03e45c63dac02\/1585477312935\/Twitter_logo_white.jpg?format=1500w)\n\n\n\n\n# Introduction\nTwitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programmatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\nBut as we can see the data we have tells us that many tweets that may look like disaster tweets are 'spam' and are being wrongly classified.\nOur main goal is to create a model that can give us a stable classification for real disasters and possible help emergency forces cope better with disasters.\n\n\n## Feature Engineering Goals:\n1) Extract the hashtags \n\n2) Extract the number of words\n\n3) Extract average word length\n\n4) Extract the number of chars\n\n5) Extract the sentiment of the text\n\n6) Extract the number of exclamation marks\n\n7) Extract the presence of the '@' tag\n\n8) Extract the amount of words that are tagged in the top 30 words for each classification\n\n\n\n\n\nI think we are ready now to dive into our noodle bowl of information!","3c25a8aa":"# Feature Selection"}}