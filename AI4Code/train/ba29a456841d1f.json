{"cell_type":{"7a4a5527":"code","166f77a7":"code","109d52a9":"code","0e0f6f28":"code","f4305891":"code","c624c4bb":"code","299e428c":"code","5997a490":"code","a723079a":"code","b5cd18e4":"code","0efe5aa5":"code","b1ea8379":"code","aa74c414":"code","37a9943d":"code","4339ca8b":"markdown","b2e59aa3":"markdown","00628a74":"markdown","7f657986":"markdown","6cc31b96":"markdown","38435b56":"markdown","91de7583":"markdown","51bf7398":"markdown","d52d45d5":"markdown","8d9bc6cd":"markdown","2256346e":"markdown","c480e58f":"markdown","e6fa8ee5":"markdown","e9fbdabe":"markdown","d7022ad5":"markdown","2a9be75d":"markdown"},"source":{"7a4a5527":"import numpy as np\nimport pandas as pd\nimport random","166f77a7":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\npredicted_results = pd.read_csv('..\/input\/sample_submission.csv')","109d52a9":"train_X = train.drop('label', axis=1)\/255\ntrain_y = train['label']\n\nsamples = test\/255\n\nsamples.head()","0e0f6f28":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(train_X, train_y, test_size=.20, random_state=1)\nprint(\"number of test samples :\", x_test.shape[0])\nprint(\"number of training samples:\",x_train.shape[0])","f4305891":"def vectorized_result(j):\n    e = np.zeros((10, 1))\n    e[j] = 1.0\n    return e","c624c4bb":"training_inputs = []   \ntest_inputs = []\nsample_inputs = []\n\nfor row in x_train.iterrows():\n    training_inputs.append((row[1].values.reshape((784,1))))\n                        \ntraining_results = [vectorized_result(y) for y in y_train]\ntraining_data = list(zip(training_inputs, training_results))\n    \nfor row in x_test.iterrows():\n    test_inputs.append((row[1].values.reshape((784,1)))) \n\ntest_data = list(zip(test_inputs, y_test))\n\nfor row in samples.iterrows():\n    sample_inputs.append((row[1].values.reshape((784,1))))","299e428c":"class Network(object): #constructing network\n    def __init__(self,sizes): # sizes is an array of number of neurons per layer\n        self.num_layers = len(sizes) #number of hidden layers based on size array chosen\n        self.sizes = sizes\n        self.biases = [np.random.randn(y,1) for y in sizes[1:]] #randomly generate initial biases\n        self.weights = [np.random.randn(y,x) for x,y in zip(sizes[:-1], sizes[1:])] #randomly generate initial weights\n\n    def sigmoid(z): #build the sigmoid function for output transformation\n        return 1.0\/(1.0+np.exp(-z))\n\n    def feedforward(self, a):\n        for b, w in zip(self.biases, self.weights):\n            a = sigmoid(np.dot(w, a)+b) #find the updated 'a' value for each, dot multiply vectors and add biases\n        return a\n\n    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n    # Train the neural network using mini-batch stochastic gradient descent.\n    # The \"training_data\" is a list of tuples (x, y) representing the training inputs and the desired\n    # epochs is the number of 'learning loops' the program runs before accepting values\n    # eta is the learning rate\n        if test_data:\n            n_test = sum(1 for _ in test_data) # this is the test data used for scoring after the training is done\n        n = sum(1 for _ in training_data)  # training\n        for j in range(epochs):  #changed from xrange to range, xrange is depreciated\n            random.shuffle(training_data)\n            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)] #Uses randomly selected mini batches instead of entire dataset\n            for mini_batch in mini_batches:\n                self.update_mini_batch(mini_batch, eta) # local gradient descent for each mini batch\n            if test_data:\n                print (\"Epoch {}: {} \/ {} --> {:2f}%\".format(j, self.evaluate(test_data), n_test,(100*self.evaluate(test_data)\/n_test)))\n            else:\n                print (\"Epoch {} complete\".format(j))\n\n    def update_mini_batch(self, mini_batch, eta):\n        nabla_b = [np.zeros(b.shape) for b in self.biases] #every bias gets a np array of 0s entire size of biases \n        nabla_w = [np.zeros(w.shape) for w in self.weights] #every weight gets a np array of 0s the entire size of weights\n        for x, y in mini_batch: #update the values in the network through backprop algorithm method\n            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n        self.weights = [w-(eta\/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] #new weights\n        self.biases = [b-(eta\/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] #new biases\n\n    def backprop(self, x, y):\n        nabla_b = [np.zeros(b.shape) for b in self.biases] # layer by layer vector of biases\n        nabla_w = [np.zeros(w.shape) for w in self.weights]# layer by layer vector of weights\n    # feedforward\n        activation = x\n        activations = [x] # list to store all the activations, layer by layer\n        zs = [] # list to store all the z vectors, layer by layer\n        for b, w in zip(self.biases, self.weights):\n            z = np.dot(w, activation)+b\n            zs.append(z)\n            activation = sigmoid(z)\n            activations.append(activation)\n        # backward pass\n        delta = (activations[-1]-y) * sigmoid_prime(zs[-1])\n        nabla_b[-1] = delta\n        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n\n        #l is negative index of list since it is backpropagation\n        for l in range(2, self.num_layers):\n            z = zs[-l]\n            sp = sigmoid_prime(z)\n            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n            nabla_b[-l] = delta\n            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n        return (nabla_b, nabla_w)\n\n    def evaluate(self, test_data):\n        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data] #how does generated test result match up with actual\n        return sum(int(x == y) for (x, y) in test_results)\n    \n    def predict(self, samples):\n        predicts = [(np.argmax(self.feedforward(x))) for x in samples] #generate results\n        return predicts\n    \ndef sigmoid(z): #build the sigmoid function for output transformation\n        return 1.0\/(1.0+np.exp(-z))\n    \ndef sigmoid_prime(z):\n    #Derivative of the sigmoid function\n    return sigmoid(z)*(1-sigmoid(z))","5997a490":"mindbrain = Network([784,64,64,10]) # 64 nodes per hidden layer","a723079a":"mindbrain.SGD(training_data, 30, 7, 1.2, test_data=test_data) # each number is 784 pixels, 7 datapoint mini batches, 30 epochs, 1.2 learning rate","b5cd18e4":"results = mindbrain.predict(sample_inputs)","0efe5aa5":"print('Results, frequency per number: \\n')\nprint('0:', results.count(0))\nprint('1:', results.count(1))\nprint('2:', results.count(2))\nprint('3:', results.count(3))\nprint('4:', results.count(4))\nprint('5:', results.count(5))\nprint('6:', results.count(6))\nprint('7:', results.count(7))\nprint('8:', results.count(8))\nprint('9:', results.count(9))","b1ea8379":"predicted_results.Label = results","aa74c414":"predicted_results.head(20)","37a9943d":"predicted_results.to_csv('submission.csv', index=False)","4339ca8b":"![Cat Tax](https:\/\/images.unsplash.com\/photo-1529778873920-4da4926a72c2?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=676&q=80)","b2e59aa3":"### Constructing the network","00628a74":"Basic train-test split at 20% test data, 80% training data.","7f657986":"Lets train this puppy.  I chose to use 7 datapoint minibatches since the pixel data is cleanly split into 112 batches.  30 epochs were used because I saw a slower increase in performance after about 20 cycles.  At around 30 I notice a plateau at about 95% which I am okay with.  1.2 learning rate was used because after testing 1.2 seemed ok and felt it was adequate.","6cc31b96":"## Handwriting Number Recognition\n### My First Neural Network","38435b56":"Normalize the pixel intensity values between 0-1, split the x and y data for fitting ","91de7583":"Load the code","51bf7398":"Import necessary libraries.","d52d45d5":"## Fill in the submissions table","8d9bc6cd":"Zip the inputs together so that the network uses the whole set of X , y values for learning and prepare the test data as well as sample submission data","2256346e":"# Cat Tax","c480e58f":"This is the bulk of the code.  Here the network class is built.  This network uses a sigmoid activation function, feed forward and backward propagation.  I use stochastic gradient descent to optimize (minimize) the quadratic cost.  \n  \n#### This network takes a few inputs:  \n* eta - Learning rate, how quickly and aggressively the model learns  \n* epochs - Number of learning cycles\n* sizes - Size of each layer, -> input, hidden, and output layers  \n* mini-batch size - Size of each minibatch that the training set is broken up into for training.\n* training data - Data used for fitting the model\n* test data - Data used for the model to evaluate itself against to see how accurate predictions from training set are on the test data\n* evaluates test data if any is provided and prints out the accuracy of each epoch.  \n    \n#### The network builds in some variables while trainining:  \n* weights and biases for each neuron  \n* activation function for each neuron (using weights\/biases)  \n  \n#### The network outputs scoring using the evaluate() function and predictions using the predict() function.","e6fa8ee5":"## Predict the results","e9fbdabe":"## Generate the output","d7022ad5":"### Generating the model itself using the defined class ","2a9be75d":"This is done to perform one-hot style encoding so results change from number to array with a 1 in location of the number.  \n  \n  For instance: [3] becomes  [ 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 ]"}}