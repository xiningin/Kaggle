{"cell_type":{"a1d7a47f":"code","ec141e04":"code","81632c7e":"code","cab6e582":"code","b1ef1676":"code","ea0aac54":"code","bf74eed1":"code","8d1f3101":"code","aea83001":"code","bccefa74":"code","e5de4b7b":"code","b5c8e569":"code","8cb00c20":"code","e98933b8":"code","7ecc98c5":"code","ac8d998d":"code","f778c240":"code","748c44cd":"code","062cb1ae":"code","d4f85e0e":"code","6b2b3959":"code","8a8206d3":"code","ec2c11b4":"code","612f0871":"code","e1a0b534":"code","926c0315":"code","ebe9a7e1":"code","ca5e8dd7":"code","91348f51":"code","b9db9b7f":"code","22ffc2ba":"code","e52cdbdd":"code","71e86d6d":"code","719a067f":"code","981cd33c":"code","346f9345":"code","097a64c6":"code","4558333a":"code","11681d53":"code","010ac4f5":"code","55c0da60":"code","e54af061":"code","80e21f5b":"code","66fdca46":"code","6c3f73a3":"code","312c6bba":"markdown","169d6f28":"markdown","c4c9e0ce":"markdown","df5b9a9a":"markdown"},"source":{"a1d7a47f":"import sys\nsys.path.append('..\/input\/iterativestratification')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","ec141e04":"import numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\n\n%mkdir model\n%mkdir interim\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA,FactorAnalysis\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport warnings\nwarnings.filterwarnings('ignore')","81632c7e":"from sklearn.preprocessing import QuantileTransformer","cab6e582":"NB = '25'\n\nIS_TRAIN = True\nMODEL_DIR = \"model\" # \"..\/model\"\nINT_DIR = \"interim\" # \"..\/interim\"\n","b1ef1676":"os.listdir('..\/input\/lish-moa')","ea0aac54":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\ntrain_drugs = pd.read_csv('..\/input\/lish-moa\/train_drug.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\ntrain_features = train_drugs.merge(train_features,on='sig_id',how='left')\n#train_target_scored = train_target_scored.merge(train_drug,on='sig_id')","bf74eed1":"train_features","8d1f3101":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","aea83001":"#RankGauss\n\nfor col in (GENES + CELLS):\n\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    if IS_TRAIN:\n        transformer = QuantileTransformer(n_quantiles=100, random_state=123, output_distribution=\"normal\")\n        transformer.fit(raw_vec)\n        pd.to_pickle(transformer, f'{MODEL_DIR}\/{NB}_{col}_quantile_transformer.pkl')\n    else:\n        transformer = pd.read_pickle(f'{MODEL_DIR}\/{NB}_{col}_quantile_transformer.pkl')        \n\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","bccefa74":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","e5de4b7b":"# GENES\nn_comp = 90  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n#data2 = (FactorAnalysis(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\nif IS_TRAIN:\n    fa = FactorAnalysis(n_components=n_comp, random_state=1903).fit(data[GENES])\n    pd.to_pickle(fa, f'{MODEL_DIR}\/{NB}_factor_analysis_g.pkl')\n    #umap = UMAP(n_components=n_dim, random_state=1903).fit(data[GENES])\n    #pd.to_pickle(umap, f'{MODEL_DIR}\/{NB}_umap_g.pkl')\nelse:\n    fa = pd.read_pickle(f'{MODEL_DIR}\/{NB}_factor_analysis_g.pkl')\n    #umap = pd.read_pickle(f'{MODEL_DIR}\/{NB}_umap_g.pkl')\ndata2 = fa.transform(data[GENES])\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","b5c8e569":"#CELLS\nn_comp = 50  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\nif IS_TRAIN:\n    fa = FactorAnalysis(n_components=n_comp, random_state=1903).fit(data[CELLS])\n    pd.to_pickle(fa, f'{MODEL_DIR}\/{NB}_factor_analysis_c.pkl')\n    #umap = UMAP(n_components=n_dim, random_state=1903).fit(data[GENES])\n    #pd.to_pickle(umap, f'{MODEL_DIR}\/{NB}_umap_g.pkl')\nelse:\n    fa = pd.read_pickle(f'{MODEL_DIR}\/{NB}_factor_analysis_c.pkl')\n    #umap = pd.read_pickle(f'{MODEL_DIR}\/{NB}_umap_g.pkl')\ndata2 = fa.transform(data[CELLS])\n#data2 = (FactorAnalysis(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","8cb00c20":"train_features.shape","e98933b8":"train_features","7ecc98c5":"from sklearn.feature_selection import VarianceThreshold\n\n\n#var_thresh = VarianceThreshold(0.8)  #<-- Update\nvar_thresh = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n\ndata = train_features.append(test_features)\nif IS_TRAIN:\n    transformer = QuantileTransformer(n_quantiles=100, random_state=123, output_distribution=\"normal\")\n    transformer.fit(data.iloc[:,5:])\n    pd.to_pickle(transformer, f'{MODEL_DIR}\/{NB}_{col}_quantile_transformer2.pkl')\nelse:\n    transformer = pd.read_pickle(f'{MODEL_DIR}\/{NB}_{col}_quantile_transformer2.pkl')  \ndata_transformed = transformer.transform(data.iloc[:, 5:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\ntrain_features.shape\n","ac8d998d":"from sklearn.cluster import KMeans\ndef fe_cluster(train, test, n_clusters_g = 45, n_clusters_c = 15, SEED = 123):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    \n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n        train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n        test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\n#train_features ,test_features=fe_cluster(train_features,test_features)","f778c240":"def fe_stats(train, test):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    \n    for df in train, test:\n        df['g_sum'] = df[features_g].sum(axis = 1)\n        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n        df['g_skew'] = df[features_g].skew(axis = 1)\n        df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n        \n    return train, test\n\ntrain_features,test_features=fe_stats(train_features,test_features)","748c44cd":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget = train[train_targets_scored.columns]","062cb1ae":"train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","d4f85e0e":"train","6b2b3959":"target_cols = target.drop('sig_id', axis=1).columns.values.tolist()","8a8206d3":"#folds = train.copy()\n\n#mskf = MultilabelStratifiedKFold(n_splits=5)\n\n#for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n#    folds.loc[v_idx, 'kfold'] = int(f)\n\n#folds['kfold'] = folds['kfold'].astype(int)\n#folds","ec2c11b4":"train","612f0871":"train = train_drugs.merge(train,on='sig_id')","e1a0b534":"train[['sig_id','drug_id']]","926c0315":"scored = (train[['sig_id','drug_id']]).merge(train[train_targets_scored.columns],on='sig_id')\nscored","ebe9a7e1":"targets = scored.columns[2:]","ca5e8dd7":"FOLDS = 5\nvc = scored.drug_id.value_counts()\nvc1 = vc.loc[vc<=18].index\nvc2 = vc.loc[vc>18].index\nfolds = train.copy()\n# STRATIFY DRUGS 18X OR LESS\ndct1 = {}; dct2 = {}\nskf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=34)\ntmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\nfor fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n    dd = {k:fold for k in tmp.index[idxV].values}\n    dct1.update(dd)\n\n# STRATIFY DRUGS MORE THAN 18X\nskf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=34)\ntmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop=True)\nfor fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n    dd = {k:fold for k in tmp.sig_id[idxV].values}\n    dct2.update(dd)\n                \nfolds['kfold'] = folds.drug_id.map(dct1)\nfolds.loc[folds.kfold.isna(),'kfold'] =\\\n    folds.loc[folds.kfold.isna(),'sig_id'].map(dct2)\nfolds.kfold = folds.kfold.astype('int8')","91348f51":"print(train.shape)\nprint(folds.shape)\nprint(test.shape)\nprint(target.shape)\nprint(sample_submission.shape)","b9db9b7f":"folds","22ffc2ba":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n    ","e52cdbdd":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n#         print(inputs.shape)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n   \n    ","71e86d6d":"import torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","719a067f":"class Model(nn.Module):      # <-- Update\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.25)\n        self.dense2 = nn.Linear(hidden_size, hidden_size)\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.25)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","981cd33c":"def process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data","346f9345":"feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id','drug_id']]\nlen(feature_cols)","097a64c6":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\nBATCH_SIZE = 128\nLEARNING_RATE = 5e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 5            #<-- Update\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=2048\n","4558333a":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(test)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=5e-3, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n    \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n        print(f\"SEED: {seed}, FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"SEED: {seed} ,FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"model\/{NB}-scored2-SEED{seed}-FOLD{fold}_.pth\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n\n    )\n    \n    model.load_state_dict(torch.load(f\"model\/{NB}-scored2-SEED{seed}-FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n #   if not IS_TRAIN:\n   # valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n   # oof[val_idx] = valid_preds     \n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions","11681d53":"def run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ \/ NFOLDS\n        oof += oof_\n        \n    return oof, predictions","010ac4f5":"# Averaging on multiple SEEDS\n\nSEED = [940, 1513, 1269,1392,1119,1303]  #<-- Update\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    \n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ \/ len(SEED)\n    predictions += predictions_ \/ len(SEED)\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions\n","55c0da60":"train_targets_scored","e54af061":"len(target_cols)\n","80e21f5b":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ \/ target.shape[1]\n    \nprint(\"CV log_loss: \", score)\n    ","66fdca46":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","6c3f73a3":"sub.shape","312c6bba":"# **If U find it helpful and consider forking , please do Upvote** :)","169d6f28":"# This is an Updated version of my previous public kernel <https:\/\/www.kaggle.com\/kushal1506\/moa-pytorch-0-01859-rankgauss-pca-nn>","c4c9e0ce":"## Updates -\n\n* Implementing Feature Engineering \n* Implementing Label Smoothing","df5b9a9a":"# Dataset Classes"}}