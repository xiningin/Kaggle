{"cell_type":{"56675635":"code","a86ae3c1":"code","c9c70678":"code","eaeb4aa3":"code","aa60af35":"code","1d14a141":"code","f92b8424":"code","a74103b8":"code","63a7efe7":"code","a39ef431":"code","6c2823ae":"code","e5e93a1d":"code","ce75e25b":"code","0f55df76":"code","d33ab760":"code","aa897bf4":"code","6885faba":"code","27b36fb7":"code","9c25c338":"code","546da9fb":"code","7ff77fc4":"code","b9fd15c4":"code","19c9b201":"code","6aa7390f":"code","0990887c":"code","f7ba3685":"code","37445aca":"code","55d7a0c7":"code","98b445e9":"code","12541532":"code","0e1e1990":"code","c0b41cee":"code","7cbed006":"code","7737dd88":"code","b9b7224c":"code","fc1e035d":"code","8345fcd1":"code","41053715":"code","6b2274f4":"code","ada19db5":"code","922104d0":"code","30f71e25":"code","d69fbdbb":"code","6eecff4c":"code","fdf962af":"code","74e52ba9":"code","ae70c4e7":"code","b6a0ce6f":"code","c2c7fd57":"code","540f9409":"code","1371aed3":"code","99bf28b8":"code","fa193535":"code","ddd15841":"code","656984c5":"code","a2f2a524":"code","251c472e":"code","83080fe2":"code","a4f0f064":"code","d8f83088":"code","9eb8fa80":"code","4a40c6b9":"code","241ed1ed":"code","512e9428":"code","406e83d6":"markdown","0829788c":"markdown","498b708f":"markdown","75457664":"markdown","4a905fee":"markdown","c5328b3b":"markdown","25eff4a6":"markdown","176e5236":"markdown","416d7a06":"markdown","93fc35f7":"markdown","83dcab54":"markdown","2fb1f280":"markdown","bd1d568d":"markdown","14e4c29b":"markdown","307ee486":"markdown","3eefae96":"markdown","3c1fcb39":"markdown","b2f41dad":"markdown","96282b8d":"markdown","c99bdee5":"markdown","8a29f086":"markdown","a3684db8":"markdown","e17c3af0":"markdown","ad2937e2":"markdown","92c2e735":"markdown","34ff4697":"markdown","d3e9b746":"markdown","51dbfea5":"markdown","2c40de1b":"markdown","2df3a6e0":"markdown","176564d6":"markdown","79ab79ac":"markdown","143025f5":"markdown","e8af1ecc":"markdown","73c516dd":"markdown","0f567749":"markdown","5aea564c":"markdown","bee9e39c":"markdown","dbb01628":"markdown","f6a7b237":"markdown","710e9918":"markdown","7ceb0346":"markdown","a7719d88":"markdown","4f945d8c":"markdown","e9c73600":"markdown","3bc02445":"markdown","f4d51e8a":"markdown","2b5fc23d":"markdown","ab882823":"markdown","fd75429b":"markdown","7b9c6bf0":"markdown","53005d67":"markdown","7d792690":"markdown","05de7c95":"markdown"},"source":{"56675635":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a86ae3c1":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n%matplotlib inline\n\n# setting plot style for all the plots\nplt.style.use('fivethirtyeight')","c9c70678":"df = pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\ndf.head()","eaeb4aa3":"print('Number of rows in the dataset: ',df.shape[0])\nprint('Number of columns in the dataset: ',df.shape[1])","aa60af35":"df.info()","1d14a141":"df.describe().round(decimals=3)","f92b8424":"plt.figure(figsize=(10, 6))\nsns.countplot(x='quality', data=df)\nplt.title('Number of wines present in the dataset of a given quality')\nplt.show()","a74103b8":"# Function to plot barplot and boxplot of a given feature\ndef plot(x_val, y_val, palette='pastel'):\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n    sns.barplot(x=x_val, y=y_val, data=df, ax=ax[0], palette=palette)\n    sns.boxplot(x= x_val, y= y_val, data=df, ax=ax[1],palette=palette, linewidth=3)\n    plt.tight_layout(w_pad=2)\n    plt.show()","63a7efe7":"plot('quality','fixed acidity')","a39ef431":"plot('quality', 'volatile acidity')","6c2823ae":"plot('quality', 'citric acid')","e5e93a1d":"plot('quality', 'residual sugar')","ce75e25b":"plot('quality', 'chlorides')","0f55df76":"plt.figure(figsize=(12,8))\ncorr = df.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr,mask=mask, annot=True, linewidths=1, cmap='YlGnBu')\nplt.show()","d33ab760":"bins = (2, 6.5, 8)\ngroup_names = ['bad', 'good']\ndf['quality'] = pd.cut(df['quality'], bins = bins, labels = group_names)","aa897bf4":"df.head()","6885faba":"plt.figure(figsize=(7,6))\nsns.countplot(x='quality', data=df, palette='pastel')\nplt.title('Number of good and bad quality wines')\nplt.show()","27b36fb7":"from sklearn.preprocessing import LabelEncoder","9c25c338":"label_encoder = LabelEncoder()\ndf['quality'] = label_encoder.fit_transform(df['quality'])\ndf.head(3)","546da9fb":"X = df.drop('quality', axis=1)\ny = df['quality']","7ff77fc4":"from sklearn.preprocessing import scale","b9fd15c4":"X_scaled = scale(X)","19c9b201":"from sklearn.model_selection import train_test_split","6aa7390f":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, stratify=y, random_state=41)","0990887c":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV","f7ba3685":"knn = KNeighborsClassifier()","37445aca":"params = {\n    'n_neighbors':list(range(1,15)),\n    'p':[1, 2, 3, 4],\n    'leaf_size':list(range(1,50)),\n    'weights':['uniform', 'distance']\n}","55d7a0c7":"# Doing Gridsearch to find optimal parameters\nknn_grid = GridSearchCV(estimator=knn, param_grid=params, scoring='accuracy',cv=5,n_jobs=-1)\nknn_grid.fit(X_train, y_train)","98b445e9":"knn_grid.best_params_","12541532":"knn_grid.best_score_","0e1e1990":"knn_predict = knn_grid.predict(X_test)","c0b41cee":"from sklearn.metrics import accuracy_score,confusion_matrix\nprint('Accuracy Score: ',accuracy_score(y_test,knn_predict))\nprint('Using k-NN we get an accuracy score of: ',\n      round(accuracy_score(y_test,knn_predict),5)*100,'%')","7cbed006":"from sklearn.metrics import confusion_matrix\n\n\n# Fucntion to create confusion Matrix\ndef conf_matrix(actual, predicted, model_name):\n    cnf_matrix = confusion_matrix(actual, predicted)\n#     cnf_matrix\n    class_names = [0,1]\n    fig,ax = plt.subplots()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks,class_names)\n    plt.yticks(tick_marks,class_names)\n\n    #create a heat map\n    sns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'YlGnBu',\n               fmt = 'g')\n    ax.xaxis.set_label_position('top')\n    plt.tight_layout()\n    plt.title('Confusion matrix for ' + model_name + ' Model', y = 1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    plt.show()","7737dd88":"conf_matrix(y_test, knn_predict, 'k-Nearest Neighbors')","b9b7224c":"from sklearn.metrics import classification_report","fc1e035d":"print(classification_report(y_test, knn_predict))","8345fcd1":"from sklearn.metrics import roc_auc_score,roc_curve","41053715":"y_probabilities = knn_grid.predict_proba(X_test)[:,1]\n\n#Create true and false positive rates\nfalse_positive_rate_knn,true_positive_rate_knn,threshold_knn = roc_curve(y_test,y_probabilities)\n\n#Plot ROC Curve\nplt.figure(figsize=(10,6))\nplt.title('Revceiver Operating Characterstic')\nplt.plot(false_positive_rate_knn,true_positive_rate_knn, linewidth=2)\nplt.plot([0,1],ls='--', linewidth=2)\nplt.plot([0,0],[1,0],c='.5', linewidth=2)\nplt.plot([1,1],c='.5',linewidth=2)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","6b2274f4":"#Calculate area under the curve\nroc_auc_score(y_test,y_probabilities)","ada19db5":"from sklearn.linear_model import LogisticRegression","922104d0":"logreg = LogisticRegression()","30f71e25":"params = {'C':[0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n             'class_weight': [{1:0.5, 0:0.5}, {1:0.4, 0:0.6},{1:0.6, 0:0.4}, {1:0.7, 0:0.3},{1:0.3, 0:0.7}],\n             'penalty': ['l1', 'l2'],\n             'solver': ['liblinear', 'saga'],\n             'max_iter':[50,100,150,200]\n             }","d69fbdbb":"# Doing Gridsearch to find optimal parameters\nlog_grid = GridSearchCV(estimator=logreg, param_grid=params, scoring='accuracy', cv=5, n_jobs=-1)\nlog_grid.fit(X_train, y_train)","6eecff4c":"log_grid.best_params_","fdf962af":"log_grid.best_score_","74e52ba9":"log_predict = log_grid.predict(X_test)","ae70c4e7":"print('Accuracy Score: ',accuracy_score(y_test,log_predict))\nprint('Using k-NN we get an accuracy score of: ',\n      round(accuracy_score(y_test,log_predict),5)*100,'%')","b6a0ce6f":"conf_matrix(y_test, log_predict, 'Logistic Regression')","c2c7fd57":"print(classification_report(y_test, knn_predict))","540f9409":"y_probabilities = log_grid.predict_proba(X_test)[:,1]\n\n#Create true and false positive rates\nfalse_positive_rate_log,true_positive_rate_log,threshold_log = roc_curve(y_test,y_probabilities)\n\n#Plot ROC Curve\nplt.figure(figsize=(10,6))\nplt.title('Revceiver Operating Characterstic')\nplt.plot(false_positive_rate_log,true_positive_rate_log, linewidth=2)\nplt.plot([0,1],ls='--', linewidth=2)\nplt.plot([0,0],[1,0],c='.5', linewidth=2)\nplt.plot([1,1],c='.5', linewidth=2)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","1371aed3":"#Calculate area under the curve\nroc_auc_score(y_test,y_probabilities)","99bf28b8":"from sklearn.tree import DecisionTreeClassifier","fa193535":"dt = DecisionTreeClassifier()","ddd15841":"param_grid = {\n    'criterion': ['gini','entropy'],\n    'max_depth': [None, 1, 2, 3, 4, 5, 6],\n    'max_features': ['auto', 'sqrt','log2'],\n    'max_leaf_nodes': [None, 1, 2, 3, 4, 5, 6],\n    'min_samples_leaf': [1,2,3,4,5,6,7],\n    'min_samples_split': [2,3,4,5,6,7,8,9,10]\n}","656984c5":"# Doing Gridsearch to find optimal parameters\ndt_grid = GridSearchCV(estimator=dt, param_grid=param_grid, scoring='accuracy',cv=5, n_jobs=-1)\ndt_grid.fit(X_train, y_train)","a2f2a524":"dt_grid.best_params_","251c472e":"dt_grid.best_score_","83080fe2":"dt_predict = dt_grid.predict(X_test)","a4f0f064":"print('Accuracy Score: ',accuracy_score(y_test,dt_predict))\nprint('Using Decision Tree Classifier we get an accuracy score of: ',\n      round(accuracy_score(y_test,dt_predict),5)*100,'%')","d8f83088":"conf_matrix(y_test, log_predict, 'Decision Tree')","9eb8fa80":"print(classification_report(y_test, dt_predict))","4a40c6b9":"y_probabilities = dt_grid.predict_proba(X_test)[:,1]\n\n#Create true and false positive rates\nfalse_positive_rate_dt,true_positive_rate_dt,threshold_dt = roc_curve(y_test,y_probabilities)\n\n#Plot ROC Curve\nplt.figure(figsize=(10,6))\nplt.title('Revceiver Operating Characterstic')\nplt.plot(false_positive_rate_dt,true_positive_rate_dt, linewidth=2)\nplt.plot([0,1],ls='--', linewidth=2)\nplt.plot([0,0],[1,0],c='.5', linewidth=2)\nplt.plot([1,1],c='.5', linewidth=2)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","241ed1ed":"#Calculate area under the curve\nroc_auc_score(y_test,y_probabilities)","512e9428":"#Plot ROC Curve\nplt.figure(figsize=(10,6))\nplt.title('Reciver Operating Characterstic Curve')\nplt.plot(false_positive_rate_knn,true_positive_rate_knn,linewidth=2, label='k-Nearest Neighbor')\nplt.plot(false_positive_rate_log,true_positive_rate_log, linewidth=2, label='Logistic Regression')\nplt.plot(false_positive_rate_dt,true_positive_rate_dt, linewidth=2, label='Decision Tree')\nplt.plot([0,1],ls='--', linewidth=2)\nplt.plot([0,0],[1,0],c='.5', linewidth=2)\nplt.plot([1,1],c='.5', linewidth=2)\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.legend()\nplt.show()","406e83d6":"#### Classification report","0829788c":"### Loading the data","498b708f":"### 5. Residual Sugar vs. Quality","75457664":"### 4. Applying ML Algorithms","4a905fee":"## Implementing Machine Learing Algorithms","c5328b3b":"#### Receiver Operating Characterstic(ROC) Curve","25eff4a6":"#### Confusion Matrix","176e5236":"#### Best score for the model","416d7a06":"#### Making predictions","93fc35f7":"### Number of good and bad quality wines in the dataset","83dcab54":"### Dimensions of the dataset","2fb1f280":"### 2. Assigning a label(numerical value) to the quality variable.","bd1d568d":"In this kernel I have performed Exploratory Data Analysis on the **Red Wine Quality** dataset and tried to identify relationship between heart the quality of wine and various other features. After EDA data pre-processing is done I have applied **k-NN(k-Nearest Neighbors)**,  **Logistic Regression**  and **Decision Tree** Algorithm to make the predictions. I will use various other algorithms for predictions in future and add them in this kernel.\n\nI hope you find this kernel helpful and some **<font color='red'>UPVOTES<\/font>** would be very much appreciated","14e4c29b":"### ii. Logistic Regression","307ee486":"## Exploratory Data Analysis(EDA)","3eefae96":"#### Making predictions","3c1fcb39":"#### Best score for the model","b2f41dad":"### Importing required libraries","96282b8d":"#### Best parameters for the model","c99bdee5":"### 7. Correlation Heatmap between various features","8a29f086":"### 3. Splitting the dataset into Training and Testing sets","a3684db8":"## Preporcessing the data before applying Machine Learning algorithms","e17c3af0":"#### Accuracy","ad2937e2":"### 6. Chlorides vs. Quality","92c2e735":"### iii. Decision Tree","34ff4697":"### 4. Citric Acid vs. Quality","d3e9b746":"### 1. Dividing the wine quality as good or bad to make it a binary classification problem","51dbfea5":"#### Classification report","2c40de1b":"#### Best score for the model","2df3a6e0":"#### Making predictions","176564d6":"### Comparing ROC Curve of k-Nearest Neighbors, Logistic Regression and Decision Tree\n","79ab79ac":"### 1. Number of wines of a given quality in the dataset","143025f5":"### 2. Scaling the features","e8af1ecc":"#### Plotting the relationship between quality of wine and various other features","73c516dd":"#### Confusion Matrix","0f567749":"### 3. Volatile Acidity vs. Quality","5aea564c":"### 1. Splitting the features and target variables","bee9e39c":"### Features in the data set","dbb01628":"**The features described in the above data set are:**\n\n**1. Count** tells us the number of NoN-empty rows in a feature.\n\n**2. Mean** tells us the mean value of that feature.\n\n**3. Std** tells us the Standard Deviation Value of that feature.\n\n**4. Min** tells us the minimum value of that feature.\n\n**5. 25%, 50%, and 75%** are the percentile\/quartile of each features.\n\n**6. Max** tells us the maximum value of that feature.","f6a7b237":"#### Receiver Operating Characterstic(ROC) Curve","710e9918":"The **'quality'** column now contains values 0 and 1. Although Label encoder assigns incremental values i.e 1, 2, 3, 4, ... it can be used here in place of OneHot Encoder since there are only two values in the quality column.","7ceb0346":"The quality column in the dataset now has only two values i.e. good and bad.","a7719d88":"#### Best parameters for the model","4f945d8c":"### i. K Nearest Neighbors","e9c73600":"#### Accuracy","3bc02445":"### Basic statistical details about the dataset","f4d51e8a":"#### Accuracy","2b5fc23d":"#### Classification report","ab882823":"### 2. Fixed Acidity vs. Quality","fd75429b":"**Suggestions are welcome**\n\n**<font color='red'>UPVOTE<\/font>** if you found the notebook helpful.","7b9c6bf0":"#### Receiver Operating Characterstic(ROC) Curve","53005d67":"**What's next?**\n1. Applying SVM and Random Forest Algorithms\n2. Applying various ensemble methods such as bagging, boosting.\n3. Compare the models on the basis of their accuracy score.","7d792690":"#### Best parameters for the model","05de7c95":"#### Confusion Matrix"}}