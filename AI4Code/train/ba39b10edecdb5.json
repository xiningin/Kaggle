{"cell_type":{"d326000e":"code","aed6c6b8":"code","026a4392":"code","d4ea8f6b":"code","ba296844":"code","1f7745b1":"code","d49e73d0":"code","1cc338f2":"code","93bf4b32":"code","ef917861":"code","f592a733":"code","7045845b":"code","f1f083b1":"code","5e569f5e":"code","9638c6e5":"code","88d8b3e1":"code","7ed8acb2":"code","f10d295e":"code","871b4d63":"code","2056ccb9":"code","a8863ea2":"code","aec81861":"code","da6b1757":"code","12bca2c8":"code","bc78a1e6":"code","252c9372":"code","81a4623a":"code","73ddf9c4":"code","183afa47":"code","2d46d5a2":"code","66a7b4a8":"code","fd6ee955":"markdown","eb15ba35":"markdown","63a2d75a":"markdown","e05adfde":"markdown","592d24e9":"markdown","90ad302b":"markdown","2ee6f10f":"markdown","6a4ec52e":"markdown","09d9beb9":"markdown"},"source":{"d326000e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport itertools\nfrom random import shuffle\nimport matplotlib.pyplot as plt\nimport cv2\nfrom PIL import Image as pil_image\nfrom math import sqrt\nimport random\nfrom keras.utils import Sequence\nfrom keras.layers import Input\n# from lapjv import lapjv\nfrom tqdm import tqdm_notebook as tqdm\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.resnet50 import ResNet50,preprocess_input\nfrom keras.applications.xception import Xception\nfrom keras.layers.core import Activation, Dense, Dropout, Flatten, Lambda\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.layers import *\nfrom keras.models import Sequential, Model\nfrom collections import OrderedDict\nfrom keras.callbacks import ModelCheckpoint,ReduceLROnPlateau\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import model_from_json, load_model\nfrom keras import regularizers\n\n\nprint(os.listdir(\"..\/input\"))","aed6c6b8":"DATA=\"..\/input\/humpback-whale-identification\"\nTRAIN_IMG=\"..\/input\/humpback-whale-identification\/train\"\nTEST_IMG=\"..\/input\/humpback-whale-identification\/test\"\nSUBMISSION_DF = '..\/input\/pretrained-model\/submission_siamese.csv'\nBB_DATA=\"..\/input\/bounding-boxes-using-image-processing\"\n\nSIAMESE_MOD=\"..\/input\/siamese-trained-new\/siamese_trained_bb.h5\"\nSIAMESE_MOD_MID=\"..\/input\/siamese-trained-new\/siamese_mid.hdf5\"\n\nSIMPLE_CNN_SUB=\"..\/input\/cnn-outputs\/submission_top100.csv\"\nTEST_PARTS_DIR=\"..\/input\/pretrained-model\"\nIM_SIZE=100\n\n","026a4392":"TRAIN_FLG=True\n# if os.path.isfile(SIAMESE_MOD):\n#     TRAIN_FLG=False\n#     print(\"model will be loaded from \"+ SIAMESE_MOD)","d4ea8f6b":"test_df= pd.DataFrame({\"Image\":  os.listdir(TEST_IMG)})\nprint(\"test images:\"+ str(len(test_df)))","ba296844":"train_df = pd.read_csv(os.path.join(DATA, 'train.csv'))\ntrain_df=train_df[train_df['Image']!='859e1399e.jpg']\ntrain_lbl=train_df.copy()\nprint(\"train images:\"+ str(len(train_lbl)))\nprint(\"total unique class:\"+ str(len(np.unique(train_lbl['Id']))))\ntrain_lbl.head()","1f7745b1":"#take out whales with a single train example (2072 examples)\ndf=train_lbl.groupby(['Id']).size().reset_index(\n    name='train_examples')\ndf=df[df['train_examples']>=2]\nsingle_whale_set= set(df.Id.values)\nprint(\"number of classes with more than 1 examples:\"+ str(len(df)))\n\n# print(\"number of classes with more than 1 examples:\"+ str(len(df)))\ntrain_lbl=train_lbl[train_lbl['Id'].isin(df['Id'])]\nprint(\"number of train instances :\"+ str(len(train_lbl)))","d49e73d0":"no_new_whale=train_lbl[train_lbl['Id']!='new_whale']\nprint(\"number of train instances :\"+ str(len(no_new_whale)))","1cc338f2":"def fetch_whale_img_list(image_dir,labels):\n    img_groups = {}\n    for img_file in tqdm(labels[\"Image\"].values,desc='fetch_whale_img_list'):\n        pid=img_file\n        train_itms=labels[labels['Image']==pid]\n        if train_itms is not None and len(train_itms)>0:\n            gid=labels[labels['Image']==pid].values[0][1] #this is the ralevant whale group\n            if gid in img_groups:\n                img_groups[gid].append(pid)\n            else:\n                img_groups[gid] = [pid]\n    return img_groups\n\nwhales_train_list=fetch_whale_img_list(TRAIN_IMG,no_new_whale)","93bf4b32":"def get_random_image(img_groups, gname):\n    photos = img_groups[gname]\n    pname = np.random.choice(photos, size=1)[0]\n    return pname\n    \ndef create_triples(image_dir,labels,data_set='train',img_groups=whales_train_list):\n#     img_groups = fetch_whale_img_list(image_dir,labels)\n    # creat equal number of negative examples\n    group_names = list(img_groups.keys())\n    triples = []\n    # positive pairs are any combination of images in same group\n    for key in img_groups.keys():\n        combs=itertools.combinations(img_groups[key], 2)\n        triple_pos = [(x[0] , x[1] , 1) \n                 for x in combs]\n        idx= np.random.choice(np.arange(len(triple_pos)),size=len(img_groups[key])-1,replace=False)\n        triple_pos=[triple_pos[i] for i in idx]\n        triples.extend(triple_pos)\n        \n        triple_neg = []\n        for x in triple_pos:\n            flg=True\n            while flg:\n                neg_w = np.random.choice(group_names, size=1, replace=False)[0]\n                if neg_w!=key: \n                    flg=False\n            right = get_random_image(img_groups, neg_w)\n            triple_neg.append((x[0], right, 0))\n        triples.extend(triple_neg)\n#         print(\"added neg examples:\"+str(len(triple_neg)))\n#     for i in tqdm(range(len(pos_triples))):\n#         g1, g2 = np.random.choice(np.arange(len(group_names)), size=2, replace=False)\n#         left = get_random_image(img_groups, group_names, g1)\n#         right = get_random_image(img_groups, group_names, g2)\n#         neg_triples.append((left, right, 0))\n#     pos_triples.extend(neg_triples)\n    shuffle(triples)\n    return triples","ef917861":"if TRAIN_FLG:\n    triples_data = create_triples(TRAIN_IMG,no_new_whale)\n    print(len(triples_data))\n    print(\"triplets examples:\")\n    print(triples_data[0:5])","f592a733":"if TRAIN_FLG:\n    #look at some examples\n    imgs=triples_data[0:10]\n    per_row=2\n    rows=5\n    cols = 2\n    fig, axes = plt.subplots(rows,cols, figsize=(24\/\/per_row*cols,24\/\/per_row*rows))\n    for ax in axes.flatten(): \n        ax.axis('off')\n\n    left=0\n    j=0\n    for i,ax in enumerate(axes.flatten()):\n        img=imgs[j][left]\n        label=imgs[j][2]\n        image_path=os.path.join(TRAIN_IMG, img)\n        left=(left+1)%2\n        if (i+1)%2==0:\n            j=j+1\n        ax.imshow(cv2.imread(image_path))\n        ax.set_title(label)","7045845b":"#read BB data fo train and test\ntrain_bb=pd.read_csv(os.path.join(BB_DATA,\"boxs_train.csv\"))\ntest_bb=pd.read_csv(os.path.join(BB_DATA,\"boxs_test.csv\"))\nbb_all=train_bb.append(test_bb, ignore_index=True)\n\nbb = {}\nfor i in range(len(bb_all)):\n    image=bb_all['Image'].iloc[i].split(\"\/\")[-1]\n    x0=float(bb_all['x0'].iloc[i])\n    y0=float(bb_all['y0'].iloc[i])\n    x1=float(bb_all['x1'].iloc[i])\n    y1=float(bb_all['y1'].iloc[i])\n    box=(x0,y0,x1,y1)\n    #save to labels file and dir\n#   croped_image= crop_img(image,box)\n    bb[image] = box","f1f083b1":"# expnd boxes to compensate for bounding box errors\ncrop_margin = 0.1\ndef expand_bb(image,box,margin=crop_margin):\n    size_x,size_y=image.shape[1],image.shape[0]\n    x0, y0, x1, y1 = box[0],box[1],box[2],box[3]\n    dx = x1 - x0\n    dy = y1 - y0\n    x0 = max(0,x0-dx * crop_margin)\n    x1 = min(size_x,x1+ dx * crop_margin + 1)\n    y0 = max(0,y0-dy * crop_margin)\n    y1 = min(size_y, y1+dy * crop_margin + 1)\n    return x0,y0,x1,y1\n\ndef crop_img(image,box):\n    new_box=expand_bb(image,box)\n    if len(image.shape)==3:\n        new_image = image[int(new_box[1]):int(new_box[3]), int(new_box[0]):int(new_box[2]),:]\n    else:\n        new_image = image[int(new_box[1]):int(new_box[3]), int(new_box[0]):int(new_box[2])]\n    return new_image","5e569f5e":"RESIZE_IMG = IM_SIZE\nfrom skimage.transform import resize\nfrom keras.utils import np_utils\nimport cv2\n\ndef read_crop_resize(image_path, boxes=bb):\n    image = cv2.imread(image_path)\n    if image is not None:\n        if image_path.split(\"\/\")[-1] in boxes:\n            box=boxes[image_path.split(\"\/\")[-1]]\n            croped_image= crop_img(image,box)\n            image=croped_image\n        try:\n            image = cv2.resize(image, (RESIZE_IMG, RESIZE_IMG)) \n        except cv2.error as e:\n            print(\"error resizing image:\"+image_path )\n            return None\n    return image\n\ndef preprocess_images(image_names, seed, datagen,directory=TRAIN_IMG):\n    np.random.seed(seed)\n#     X = np.zeros((len(image_names), RESIZE_IMG, RESIZE_IMG, 3))\n    X = np.zeros((len(image_names), RESIZE_IMG, RESIZE_IMG,1))\n    for i, image_name in enumerate(image_names):\n        if os.path.isfile(image_name):\n            image = read_crop_resize(image_name)\n        else:\n            image = read_crop_resize(os.path.join(directory, image_name))\n        if image is not None:\n            if datagen is not None:\n                image = datagen.random_transform(image)\n            else:\n                image = image\n            image=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n            image=np.expand_dims(image, axis=2)\n            X[i]=image\n#             X[i]=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        else:\n            print(\"error reading image :\"+image_name)\n    return X\n\ndef image_triple_generator(image_triples, directory, batch_size,augment=True,shuffle=True):\n    datagen_args = dict(rescale=1.\/255,rotation_range=10,\n                        horizontal_flip=True)\n    if not augment:\n        datagen_args = dict(rescale=1.\/255)\n    datagen_left = ImageDataGenerator(**datagen_args)\n    datagen_right = ImageDataGenerator(**datagen_args)\n#     image_cache = {}\n    \n    while True:\n        # loop once per epoch\n        num_recs = len(image_triples)\n        if shuffle:\n            indices = np.random.permutation(np.arange(num_recs))\n        else:\n            indices = np.arange(num_recs)\n        num_batches = num_recs \/\/ batch_size\n#         if num_recs % batch_size > 0: num_batches=+1\n        for bid in range(num_batches):\n            # loop once per batch\n            batch_indices = indices[bid * batch_size : min(num_recs,(bid + 1) * batch_size)]\n            batch = [image_triples[i] for i in batch_indices]\n            # make sure image data generators generate same transformations\n            seed = np.random.randint(low=0, high=1000, size=1)[0]\n            Xleft = preprocess_images([b[0] for b in batch], seed, \n                                      datagen_left,directory)\n            Xright = preprocess_images([b[1] for b in batch],seed,\n                                       datagen_right, directory)\n            Y = np.array([b[2] for b in batch]) # 0 or 1\n            yield ([Xleft.astype(np.uint8), Xright.astype(np.uint8)], Y)\n","9638c6e5":"if TRAIN_FLG:\n    triples_batch_gen = image_triple_generator(triples_data,TRAIN_IMG, 32)\n    ([Xleft, Xright], Y) = triples_batch_gen.__next__()\n    print(\"generator output shapes:\")\n    print(Xleft.shape, Xright.shape, Y.shape)","88d8b3e1":"# plt.imshow(Xright[2])","7ed8acb2":"def create_base_network(input_shape):\n    '''Base network to be shared (eq. to feature extraction).\n    '''\n    seq = Sequential()\n    # CONV => RELU => POOL\n    seq.add(Conv2D(20, kernel_size=5, padding=\"same\", input_shape=input_shape,data_format=\"channels_last\",activation='relu'))\n    seq.add(BatchNormalization())\n    seq.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n    seq.add(BatchNormalization())\n    # CONV => RELU => POOL\n    seq.add(Conv2D(50, kernel_size=5, padding=\"same\",activation='relu'))\n    seq.add(BatchNormalization())\n    seq.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n    seq.add(BatchNormalization())\n    seq.add(Flatten())\n    seq.add(Dense(500,kernel_regularizer=regularizers.l2(0.01)))\n    \n    return seq","f10d295e":"if TRAIN_FLG:\n    image_size=IM_SIZE\n#     input_shape = (image_size,image_size, 3)\n    input_shape = (image_size,image_size,1)\n    base_network = create_base_network(input_shape)\n\n#     input_shape = (image_size,image_size, 3)\n    input_shape = (image_size,image_size,1)\n    vector_left =Input(shape=base_network.output_shape[1:])\n    vector_right = Input(shape=base_network.output_shape[1:])\n    img_l = Input(shape=input_shape)\n    img_r = Input(shape=input_shape)\n    x_l         = base_network(img_l)\n    x_r         = base_network(img_r)","871b4d63":"if TRAIN_FLG:\n    #layer to merge two encoded inputs with the l1 distance between them\n    mid        = 32\n    L_prod = Lambda(lambda x : x[0]*x[1])([vector_left, vector_right])\n    L_sum = Lambda(lambda x : x[0] + x[1])([vector_left, vector_right])\n    L1_distance= Lambda(lambda x : K.abs(x[0] - x[1]))([vector_left, vector_right])\n    L2_distance= Lambda(lambda x : K.square(x[0] - x[1]))([vector_left, vector_right])\n    distance= Concatenate()([L_prod, L_sum, L1_distance, L2_distance])\n    distance= Reshape((4, base_network.output_shape[1], 1), name='reshape1')(distance)\n    x = Conv2D(mid, (4, 1), activation='relu', padding='valid')(distance)\n    x = BatchNormalization()(x)\n    x = Reshape((base_network.output_shape[1], mid, 1))(x)\n    x = Conv2D(1, (1, mid), activation='linear', padding='valid')(x)\n    x = BatchNormalization()(x)\n    x = Flatten(name='flatten')(x)\n    pred = Dense(1, use_bias=True, activation='sigmoid', name='weighted-average',kernel_initializer=\"random_normal\")(x)\n    head_model = Model([vector_left, vector_right], outputs=pred, name='head')\n\n    x = head_model([x_l, x_r])\n    siamese_model = Model(inputs=[img_l, img_r], outputs= x)\n    siamese_model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])","2056ccb9":"if TRAIN_FLG:\n    from keras.utils.vis_utils import plot_model\n    plot_model(base_network, to_file='branch_plot.png', show_shapes=True, show_layer_names=True,expand_nested=True)\n    pil_image.open('branch_plot.png')","a8863ea2":"if TRAIN_FLG:\n    plot_model(head_model, to_file='head_plot.png', show_shapes=True, show_layer_names=True,expand_nested=True)\n    pil_image.open('head_plot.png')","aec81861":"if TRAIN_FLG:\n    triples_train,triples_test =train_test_split(triples_data, test_size=0.2, random_state=42)\n\n    callbacks=[\n        ReduceLROnPlateau(monitor='val_loss',patience=10,min_lr=1e-9,verbose=1,mode='min'),\n        ModelCheckpoint('siamese_mid.hdf5',monitor='val_loss',save_best_only=True,verbose=1)\n    ]\n\n    BATCH_SIZE=32\n    NUM_EPOCHS=30\n\n    train_gen = image_triple_generator(triples_train,TRAIN_IMG, BATCH_SIZE)\n    val_gen = image_triple_generator(triples_test,TRAIN_IMG, BATCH_SIZE)\n\n    num_train_steps = len(triples_train) \/\/ BATCH_SIZE\n    num_val_steps = len(triples_test) \/\/ BATCH_SIZE\n#     num_train_steps = 100\n#     num_val_steps = 30\n\n    siamese_model.save('siamese_trained_bb.h5')\n    history = siamese_model.fit_generator(train_gen,\n                                  steps_per_epoch=num_train_steps,\n                                  epochs=NUM_EPOCHS,\n                                  validation_data=val_gen,\n                                  validation_steps=num_val_steps,\n                                          callbacks=callbacks)\n    siamese_model.save('siamese_trained_bb.h5')\nelse: #load from file\n    siamese_model=load_model(SIAMESE_MOD)\n    siamese_model.load_weights(SIAMESE_MOD_MID)","da6b1757":"PREDICT=False","12bca2c8":"# create pairs on test image,train image to gain similarity score between them\ndef create_pairs(test_img,train_imgs):\n    pairs = []\n    for img in train_imgs:\n#         pair = (os.path.join(TEST_IMG,test_img) , os.path.join(TRAIN_IMG,img[0]) , 0)\n        pair = (os.path.join(TEST_IMG,test_img) , os.path.join(TRAIN_IMG,img) , 0)\n        pairs.append(pair)\n    return pairs","bc78a1e6":"import sys, os\n\n# Disable\ndef blockPrint():\n    sys.stdout = open(os.devnull, 'w')\n\n# Restore\ndef enablePrint():\n    sys.stdout = sys.__stdout__","252c9372":"test_paths=pd.read_csv(os.path.join(TEST_PARTS_DIR,\"test_all.txt\"),header=None, names=['Image'])\ntest_paths=test_paths['Image'].values\ntrain_paths=train_df[\"Image\"].values","81a4623a":"if PREDICT:\n    # scores is an array with number of claases dim\n    #given a scores vector of size (len(train)), group train labels to top 5 scored whales \n    new_whale='new_whale'\n    def get_top5whales(scores,train_paths=train_paths,threshold=0.99):\n        vhigh = 0\n        pos = [0, 0, 0, 0, 0, 0]\n\n        top_w = []\n        top5_submission=[]\n        s = set()\n        a = scores\n        for j in list(reversed(np.argsort(scores))): # j is an encoded label of some whale\n            img = train_paths[j] # get image value of train example j\n\n            if a[j] < threshold and new_whale not in s: # if score is lower than threshold and we didn't put new whale yet, than put new whale in the list\n                pos[len(top5_submission)] += 1\n                s.add(new_whale)\n                top5_submission.append(new_whale)\n            if len(top5_submission) == 5: break;\n            whales=no_new_whale[no_new_whale['Image']==img][\"Id\"]\n            if whales is not None and len(whales)>0:\n                for w in whales.values:\n                    if w not in s: # if we didn't yet added this whale\n                        if a[j] >= threshold:\n                            vhigh += 1\n                        s.add(w)\n                        top5_submission.append(w)\n            if len(top5_submission) == 5: break;\n            if new_whale not in s: pos[5] += 1\n        assert len(top5_submission) == 5 \n        assert len(s) == 5\n        return top5_submission","73ddf9c4":"if PREDICT:\n    # for each test image gain similarity core for each one of the train images, and group them to top 5 whales for submission\n    batch_size=64\n\n    submissions={}\n    # test on small set:\n    # test_paths_t=test_paths[:10]\n    for i in tqdm(range(len(test_paths)),desc='scores'):\n    #     print(i)\n        tst_img=test_paths[i]\n        pairs= create_pairs(tst_img,train_paths)\n        pairs1=pairs[:batch_size*(len(pairs)\/\/batch_size)]\n        test_generator = image_triple_generator(image_triples=pairs1,directory=None, batch_size=batch_size,augment=False,shuffle=False)\n        scors1= siamese_model.predict_generator(test_generator,verbose = 1,steps=len(pairs)\/\/batch_size , workers=1) \n        scores=scors1.flatten()\n\n        pairs2=pairs[batch_size*(len(pairs)\/\/batch_size):]\n        test_generator = image_triple_generator(image_triples=pairs2,directory=None, batch_size=len(pairs2),augment=False,shuffle=False)\n        scors2= siamese_model.predict_generator(test_generator,verbose = 1,steps=1 , workers=1) \n\n        scores=np.concatenate((scores,scors2.flatten()))\n        submissions[tst_img]=' '.join(get_top5whales(scores,train_paths=train_paths,threshold=0.99))\n","183afa47":"if PREDICT:\n    df_whales=pd.DataFrame.from_dict(submissions, orient='index', columns=['Id'])\n    df_whales['Image'] = df_whales.index\n    df_whales.to_csv(\"whales_pred_siamese.csv\", index = False) ","2d46d5a2":"# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\nif PREDICT:\n    # function that takes in a dataframe and creates a text link to  \n    # download it (will only work for files < 2MB or so)\n    # def create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    def create_download_link(df, title = \"Download CSV file\",input_file='whales_pred_siamese.csv', filename = \"data.csv\"):  \n        csv = pd.read_csv(input_file)\n        csv= csv.to_csv(index = False)\n        b64 = base64.b64encode(csv.encode())\n        payload = b64.decode()\n        html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n        html = html.format(payload=payload,title=title,filename=filename)\n        return HTML(html)\n\n    # create a random sample dataframe\n\n    # create a link to download the dataframe\n    create_download_link()","66a7b4a8":"#load submission from file (calc offline)\nif not PREDICT:\n    df_whales=pd.read_csv(SUBMISSION_DF)\n    df_whales.to_csv(\"whales_pred_siamese.csv\", index = False) ","fd6ee955":"# Load\/ Train model","eb15ba35":"generate methods for preprocessing the images: resize, crop using bounding boxes,etc.","63a2d75a":"# TRAIN\n","e05adfde":"##  head model","592d24e9":"## branch model ","90ad302b":"define image data generator to be used whild training, together with preprocessing","2ee6f10f":"# load data and prepare training set ","6a4ec52e":"create triplets of (img1,img2, similarity), where similarity is 1 for same class images and 0 otherwise.","09d9beb9":"# Evaluation & submission"}}