{"cell_type":{"c0c44e3e":"code","dacf86e9":"code","4c22dc4d":"code","12ffc818":"code","830b59a2":"code","6b8caac5":"code","312e45c0":"code","fb36589d":"code","558c6c3f":"code","44989cdd":"code","2b036edf":"code","aaa3ceb2":"code","da6456bd":"code","5a1b0ecf":"code","689e81d5":"code","e5bb27a8":"code","48b7703f":"code","e421c8c5":"code","9fb0f1c9":"code","de5adfbe":"code","6e4ca77a":"code","c1148e2d":"code","5568ba01":"code","9e62e392":"markdown","ed52122e":"markdown","75a30d40":"markdown","af1c65f4":"markdown","4a858a12":"markdown","b67630d0":"markdown","6bea4aa4":"markdown","cb26fa2f":"markdown","950095ce":"markdown","5b39f1f2":"markdown","67090095":"markdown","cb71453a":"markdown"},"source":{"c0c44e3e":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import model_selection","dacf86e9":"data_full = pd.read_csv(\"..\/input\/wine-quality\/winequalityN.csv\")","4c22dc4d":"total_entries = (data_full.shape[0] * data_full.shape[1])\nmissing_entries_max = data_full.isnull().sum().sum()\nmissing_entries_max_percentage = (missing_entries_max \/ total_entries) * 100\n\nprint(f\"Total entries in the dataset: %i\" % total_entries)\nprint(f\"Maximum missing values in the dataset: {missing_entries_max}\")\nprint(f\"Percentage of maximum missing values in the dataset: {missing_entries_max_percentage:.2f}%\")","12ffc818":"data_full = data_full.dropna(axis=0)\ndata_full.shape","830b59a2":"data_full = data_full.replace('red', 0)\ndata_full = data_full.replace('white', 1)\n\ndata_full.describe()","6b8caac5":"plt.tight_layout()\ndata_full.hist(bins = 100, figsize = (24, 16))\nplt.show()","312e45c0":"plt.figure(figsize = (24, 16))\nsns.heatmap(data_full.corr(), annot = True)\nplt.show()","fb36589d":"data_full = data_full.drop('total sulfur dioxide',axis=1)\ndata_full = pd.get_dummies(data_full, drop_first=True)\ndata_full.head()","558c6c3f":"scaler = MinMaxScaler()\n\n# A numpy array containing the normalized values\ndata_fit = scaler.fit(data_full)\ndata_fit = data_fit.transform(data_full)","44989cdd":"data_full = pd.DataFrame(data_fit, columns=['type', 'fixed acidity','volatile acidity','citric acid','residual sugar','chlorides','free sulfur dioxide','density','pH','sulphates','alcohol','quality'])\ndata_full.head()","2b036edf":"X = data_full[['type', 'fixed acidity','volatile acidity','citric acid','residual sugar','chlorides','free sulfur dioxide','density','pH','sulphates','alcohol']]\ny = data_full.quality\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 42)","aaa3ceb2":"def get_rmse(y_test, predictions):\n    return mean_squared_error(y_test, predictions) ** 0.5","da6456bd":"n_estimators = [50 + i*50 for i in range(10)]\nmodels_n_estimators = [RandomForestRegressor(n_estimators = n_estimators[i], random_state = 42) for i in range(10)]\n\nn_estimators_rmses = []\n                       \nfor model in models_n_estimators:                   \n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    rmse = get_rmse(y_test, predictions)\n    n_estimators_rmses += [rmse]\n    print(f\"RMSE of model with n_estimators={model.n_estimators}: {rmse:.5f}\")\n\nplt.plot(n_estimators, n_estimators_rmses)\nplt.title(\"RMSE vs n_estimators\")\nplt.xlabel(\"n_estimators\")\nplt.ylabel(\"rmse\")\nplt.show()\n    \n    ","5a1b0ecf":"max_depths = [10 + i*10 for i in range(10)]\nmodels_max_depths = [RandomForestRegressor(max_depth = max_depths[i], random_state = 42) for i in range(10)]\n\nmax_depths_rmses = []\n                       \nfor model in models_max_depths:                   \n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    rmse = get_rmse(y_test, predictions)\n    max_depths_rmses += [rmse]\n    print(f\"RMSE of model with max_depth={model.max_depth}: {rmse:.5f}\")\n\nplt.plot(max_depths, max_depths_rmses)\nplt.title(\"RMSE vs max_depth\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"rmse\")\nplt.show()","689e81d5":"min_samples_splits = [50 + i*50 for i in range(10)] + [500 + i*500 for i in range(10)]\nmodels_min_samples_splits = [RandomForestRegressor(min_samples_split = min_samples_splits[i], random_state = 42) for i in range(20)]\n\nmin_samples_splits_rmses = []\n                       \nfor model in models_min_samples_splits:                   \n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    rmse = get_rmse(y_test, predictions)\n    min_samples_splits_rmses += [rmse]\n    print(f\"RMSE of model with min_samples_split={model.min_samples_split}: {rmse:.5f}\")\n\nplt.plot(min_samples_splits, min_samples_splits_rmses)\nplt.title(\"RMSE vs min_samples_split\")\nplt.xlabel(\"min_samples_split\")\nplt.ylabel(\"rmse\")\nplt.show()","e5bb27a8":"max_leaf_nodes = [200 + i*200 for i in range(10)]\nmodels_max_leaf_nodes = [RandomForestRegressor(max_leaf_nodes = max_leaf_nodes[i], random_state = 42) for i in range(10)]\n\nmax_leaf_nodes_rmses = []\n                       \nfor model in models_max_leaf_nodes:                   \n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    rmse = get_rmse(y_test, predictions)\n    max_leaf_nodes_rmses += [rmse]\n    print(f\"RMSE of model with max_leaf_nodes={model.max_leaf_nodes}: {rmse:.5f}\")\n\nplt.plot(max_leaf_nodes, max_leaf_nodes_rmses)\nplt.title(\"RMSE vs max_leaf_nodes\")\nplt.xlabel(\"max_leaf_nodes\")\nplt.ylabel(\"rmse\")\nplt.show()","48b7703f":"min_samples_leaves =  [25 + i*25 for i in range (10)] + [250 + i*250 for i in range(10)]\nmodels_min_samples_leaves = [RandomForestRegressor(min_samples_leaf = min_samples_leaves[i], random_state = 42) for i in range(20)]\n\nmin_samples_leaves_rmses = []\n                       \nfor model in models_min_samples_leaves:                   \n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    rmse = get_rmse(y_test, predictions)\n    min_samples_leaves_rmses += [rmse]\n    print(f\"RMSE of model with min_samples_leaf={model.min_samples_leaf}: {rmse:.5f}\")\n\nplt.plot(min_samples_leaves, min_samples_leaves_rmses)\nplt.title(\"RMSE vs min_samples_leaf\")\nplt.xlabel(\"min_samples_leaf\")\nplt.ylabel(\"rmse\")\nplt.show()","e421c8c5":"max_samples =  [0.05 + i*0.049 for i in range(20)]\nmodels_max_samples = [RandomForestRegressor(max_samples = max_samples[i], random_state = 42) for i in range(20)]\n\nmax_samples_rmses = []\n                       \nfor model in models_max_samples:                   \n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    rmse = get_rmse(y_test, predictions)\n    max_samples_rmses += [rmse]\n    print(f\"RMSE of model with max_samples={model.max_samples}: {rmse:.5f}\")\n\nplt.plot(max_samples, max_samples_rmses)\nplt.title(\"RMSE vs max_samples\")\nplt.xlabel(\"max_samples\")\nplt.ylabel(\"rmse\")\nplt.show()","9fb0f1c9":"def get_best_parameter(params, rmses):\n    return params[rmses.index(min(rmses))]","de5adfbe":"best_n_estimators = get_best_parameter(n_estimators, n_estimators_rmses)\nbest_max_depth = get_best_parameter(max_depths, max_depths_rmses)\nbest_min_samples_split = get_best_parameter(min_samples_splits, min_samples_splits_rmses)\nbest_max_leaf_nodes = get_best_parameter(max_leaf_nodes, max_leaf_nodes_rmses)\nbest_min_samples_leaf = get_best_parameter(min_samples_leaves, min_samples_leaves_rmses)\nbest_max_samples = get_best_parameter(max_samples, max_samples_rmses)\n\nprint(f\"Best n_estimators: {best_n_estimators}\\nBest max_depth: {best_max_depth}\\nBest min_samples_split: {best_min_samples_split}\\nBest max_leaf_nodes: {best_max_leaf_nodes}\\nBest min_samples_leaf: {best_min_samples_leaf}\\nBest max_samples: {best_max_samples}\")","6e4ca77a":"param_grid = {'n_estimators': [200, 300, 400],\n              'max_depth': [40, 60, 80],\n              'min_samples_split': [2000, 3000, 4000],\n              'max_leaf_nodes': [800, 1200, 1600],\n              'min_samples_leaf': [1000, 1500, 2000]}\n\nbest_model = model_selection.GridSearchCV(estimator = RandomForestRegressor(),\n                                          param_grid = param_grid,\n                                          scoring = 'neg_root_mean_squared_error',\n                                          verbose=10,\n                                          n_jobs=-1,\n                                          cv=2)\n\nbest_model.fit(X_train, y_train)","c1148e2d":"params = best_model.get_params()\nprint(params)","5568ba01":"\"\"\"\nbest_model = RandomForestRegressor(n_estimators = best_n_estimators,\n                                   max_depth = best_max_depth, \n                                   min_samples_split = best_min_samples_split, \n                                   max_leaf_nodes = best_max_leaf_nodes, \n                                   min_samples_leaf = best_min_samples_leaf, \n                                   max_samples = best_max_samples, \n                                   random_state = 42)\n\nbest_model.fit(X_train, y_train)\nbest_predictions = best_model.predict(X_test)\nbest_rmse = get_rmse(y_test, best_predictions)\nprint(f\"RMSE of the optimized model: {best_rmse:.5f}\")\n\"\"\"","9e62e392":"Because our dataset in unbalance(i.e. there are features which have high ranges in respect of others), we will normalize our values to be from 0 to 1.","ed52122e":"As we can see, most of the features (except the \"type\" feature, which is binary feature) are normally distributed.\n\nNow, we will show the correlation between each feature with another, using heatmap(correlation matrix).","75a30d40":"# According to the above results, we will build a model with the parameters that gave us the best results:","af1c65f4":"# Splitting the data","4a858a12":"As we set that if there are corrlation above 0.7 between features which are not the same, we say that the correlation is high enough so we can remove one of the correlated features. Hence, we decided to remove 'total sulfur dioxide'.","b67630d0":"Applying ordinal encoding to the wine types:","6bea4aa4":"Since the percentage of missing entries is negligable, we can drop each entry that contains a null without affecting the accuracy of the prediction.","cb26fa2f":"# 3. Preprocessing","950095ce":"## Now, let us see how may missing entries are in the dataset:","5b39f1f2":"# 1. Imports","67090095":"# 2. Loading the Data","cb71453a":"# Developing the models"}}