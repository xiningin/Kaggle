{"cell_type":{"fd8b4a80":"code","9f18d2dd":"code","40ac45d7":"code","440bcea6":"code","7409e841":"code","9ae0406f":"code","23b7700c":"code","c1364fc8":"code","da67ac6e":"code","5043da5b":"code","7e432b07":"code","ecb4ad04":"code","4faf194b":"code","36cf3c1c":"code","1f75ed3b":"code","966276f2":"code","61b2f5fa":"code","c7b55b65":"code","f42c7ee8":"code","6e034acf":"code","549e6b40":"code","9cdc41c4":"markdown","bc7a6ccb":"markdown","116446c7":"markdown","556031e2":"markdown","cc2748dd":"markdown","d219c482":"markdown","3d570572":"markdown"},"source":{"fd8b4a80":"!pip install openmim\n!mim install mmdet\n!git clone https:\/\/github.com\/open-mmlab\/mmdetection.git\n%cd mmdetection\n!pip install -q -e .\n%cd ..","9f18d2dd":"import sys\nsys.path.insert(0, \".\/mmdetection\")\nimport os\n# Check Pytorch installation\nimport torch, torchvision\nprint(torch.__version__, torch.cuda.is_available())\n\n# Check mmcv installation\nfrom mmcv.ops import get_compiling_cuda_version, get_compiler_version\nfrom mmcv import Config\nprint(get_compiling_cuda_version())\nprint(get_compiler_version())\n\n# Check MMDetection installation\nfrom mmdet.apis import set_random_seed\n\n# Imports\nimport mmdet\nfrom mmdet.apis import set_random_seed\nfrom mmdet.datasets import build_dataset, build_dataloader\nfrom mmdet.models import build_detector\nfrom mmdet.apis import train_detector, single_gpu_test\nfrom mmdet.apis import init_detector, inference_detector, show_result_pyplot\n\nimport mmcv\nfrom mmcv.runner import load_checkpoint\nfrom mmcv.parallel import MMDataParallel\n\nimport random\nimport numpy as np\nfrom pathlib import Path\nimport datetime\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nimport cv2\nimport json\nimport matplotlib.pyplot as plt\nfrom IPython.core.display import Video, display\nimport subprocess\nimport gc\nimport shutil\n","40ac45d7":"# seed everything\n\nglobal_seed = 0\n\ndef set_seed(seed=global_seed):\n    \"\"\"Sets the random seeds.\"\"\"\n    set_random_seed(seed, deterministic=False)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed()","440bcea6":"# baseline models\n# cfg = Config.fromfile('\/kaggle\/working\/mmdetection\/configs\/vfnet\/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco.py')\n# cfg = Config.fromfile(\"\/kaggle\/working\/mmdetection\/configs\/vfnet\/vfnet_r50_fpn_mstrain_2x_coco.py\")\n# cfg = Config.fromfile(\"\/kaggle\/working\/mmdetection\/configs\/gfl\/gfl_r50_fpn_mstrain_2x_coco.py\")\n# baseline_cfg_path = \"\/kaggle\/working\/mmdetection\/configs\/cascade_rcnn\/cascade_rcnn_r50_fpn_20e_coco.py\"\nbaseline_cfg_path = \"\/kaggle\/working\/mmdetection\/configs\/cascade_rcnn\/cascade_rcnn_x101_32x4d_fpn_1x_coco.py\"\ncfg = Config.fromfile(baseline_cfg_path)","7409e841":"# model_name = 'vfnet_r50_fpn'\n# model_name = 'cascade_rcnn_r50_fpn'\nmodel_name = 'cascade_rcnn_x101_32x4d_fpn_1x'\nfold = 0\njob = 4\n\n# Folder to store model logs and weight files\njob_folder = f'\/kaggle\/working\/job{job}_{model_name}_fold{fold}'\ncfg.work_dir = job_folder\n\n# Set seed thus the results are more reproducible\ncfg.seed = global_seed\n\nif not os.path.exists(job_folder):\n    os.makedirs(job_folder)\n\nprint(\"Job folder:\", job_folder)","9ae0406f":"import pandas as pd\n\n# Load image level csv file\nextra_df = pd.read_csv('..\/input\/nfl-health-and-safety-helmet-assignment\/image_labels.csv')\nprint('Number of ground truth bounding boxes: ', len(extra_df))\n\n# Number of unique labels\nlabel_to_id = {label: i for i, label in enumerate(extra_df.label.unique())}\nprint('Unique labels: ', label_to_id)","23b7700c":"# Set the number of classes\nfor head in cfg.model.roi_head.bbox_head:\n    head.num_classes = 5\n\ncfg.gpu_ids = [0]\n\n# Setting pretrained model in the init_cfg which is required \n# for transfer learning as per the latest MMdetection update\ncfg.model.backbone.init_cfg=dict(type='Pretrained', checkpoint='torchvision:\/\/resnet50')\ncfg.model.backbone.init_cfg=dict(type='Pretrained', checkpoint='open-mmlab:\/\/resnext101_32x4d')\ncfg.model.pop('pretrained', None)\n\n# Epochs for the runner that runs the workflow \n# Consider increase number of epochs for better performance\ncfg.runner.max_epochs = 1 \ncfg.total_epochs = 1 \n\n# Learning rate of optimizers. \n# The LR is divided by 8 since the config file is originally for 8 GPUs\ncfg.optimizer.lr = 0.02\/8\n\n# Learning rate scheduler config used to register LrUpdater hook\ncfg.lr_config = dict(\n    policy='CosineAnnealing', # The policy of scheduler, also support CosineAnnealing, Cyclic, etc. Refer to details of supported LrUpdater from https:\/\/github.com\/open-mmlab\/mmcv\/blob\/master\/mmcv\/runner\/hooks\/lr_updater.py#L9.\n    by_epoch=False,\n    warmup='linear', # The warmup policy, also support `exp` and `constant`.\n    warmup_iters=500, # The number of iterations for warmup\n    warmup_ratio=0.001, # The ratio of the starting learning rate used for warmup\n    min_lr=1e-07)\n\n# config to register logger hook\ncfg.log_config.interval = 20 # Interval to print the log\n\n# Config to set the checkpoint hook, Refer to https:\/\/github.com\/open-mmlab\/mmcv\/blob\/master\/mmcv\/runner\/hooks\/checkpoint.py for implementation.\ncfg.checkpoint_config.interval = 1 # The save interval is 1","c1364fc8":"def create_ann_file(df, category_id):\n    \n    now = datetime.datetime.now()\n\n    data = dict(\n        info=dict(\n            description='NFL-Helmet-Assignment',\n            url=None,\n            version=None,\n            year=now.year,\n            contributor=None,\n            date_created=now.strftime('%Y-%m-%d %H:%M:%S.%f'),\n        ),\n        licenses=[dict(\n            url=None,\n            id=0,\n            name=None,\n        )],\n        images=[\n            # license, url, file_name, height, width, date_captured, id\n        ],\n        type='instances',\n        annotations=[\n            # segmentation, area, iscrowd, image_id, bbox, category_id, id\n        ],\n        categories=[\n            # supercategory, id, name\n        ],\n    )\n    \n    class_name_to_id = {}\n    labels =  [\"__ignore__\",\n                'Helmet',\n              'Helmet-Blurred', \n              'Helmet-Difficult', \n              'Helmet-Sideline',\n              'Helmet-Partial']\n\n    for i, each_label in enumerate(labels):\n        class_id = i - 1  # starts with -1\n        class_name = each_label\n        if class_id == -1:\n            assert class_name == '__ignore__'\n            continue\n        class_name_to_id[class_name] = class_id\n        data['categories'].append(dict(\n            supercategory=None,\n            id=class_id,\n            name=class_name,\n        ))\n    \n    box_id = 0\n    for i, image in tqdm(enumerate(os.listdir(TRAIN_PATH))):\n\n        img = cv2.imread(TRAIN_PATH+'\/'+image)\n        height, width, _ = img.shape\n\n        data['images'].append({\n            'license':0, \n            'url': None,\n            'file_name': image,\n            'height': height,\n            'width': width,\n            'date_camputured': None,\n            'id': i\n        })\n\n        df_temp = df[df.image == image]\n        for index, row in df_temp.iterrows():\n\n            area = round(row.width*row.height, 1)\n            bbox =[row.left, row.top, row.width, row.height]\n\n            data['annotations'].append({\n                'id': box_id,\n                'image_id': i,\n                'category_id': category_id[row.label],\n                'area': area,\n                'bbox':bbox,\n                'iscrowd':0\n            })\n            box_id+=1\n    \n    return data","da67ac6e":"TRAIN_PATH = '..\/input\/nfl-health-and-safety-helmet-assignment\/images'\nextra_df = pd.read_csv('..\/input\/nfl-health-and-safety-helmet-assignment\/image_labels.csv')\n\ncategory_id = {'Helmet':0, 'Helmet-Blurred':1,\n               'Helmet-Difficult':2, 'Helmet-Sideline':3,\n               'Helmet-Partial':4}\n\ndf_train, df_val = train_test_split(extra_df, test_size=0.2, random_state=42)\nann_file_train = create_ann_file(df_train, category_id)\nann_file_val = create_ann_file(df_val, category_id)","5043da5b":"# save data sets\nos.makedirs('..\/tmp', exist_ok=True)\n\nwith open('..\/tmp\/ann_file_train.json', 'w') as f:\n    json.dump(ann_file_train, f, indent=4)\n        \nwith open('..\/tmp\/ann_file_val.json', 'w') as f:\n    json.dump(ann_file_val, f, indent=4)","7e432b07":"cfg.dataset_type = 'CocoDataset' # Dataset type, this will be used to define the dataset\ncfg.classes = ('Helmet', 'Helmet-Blurred', 'Helmet-Difficult', 'Helmet-Sideline',\n               'Helmet-Partial')\n\ncfg.data.train.img_prefix = TRAIN_PATH # Prefix of image path\ncfg.data.train.classes = cfg.classes\ncfg.data.train.ann_file = '..\/tmp\/ann_file_train.json'\ncfg.data.train.type='CocoDataset'\n\ncfg.data.val.img_prefix = TRAIN_PATH # Prefix of image path\ncfg.data.val.classes = cfg.classes\ncfg.data.val.ann_file = '..\/tmp\/ann_file_val.json'\ncfg.data.val.type='CocoDataset'\n\ncfg.data.test.img_prefix = TRAIN_PATH # Prefix of image path\ncfg.data.test.classes = cfg.classes\ncfg.data.test.ann_file =  '..\/tmp\/ann_file_val.json'\ncfg.data.test.type='CocoDataset'\n\ncfg.data.samples_per_gpu = 4 # Batch size of a single GPU used in testing\ncfg.data.workers_per_gpu = 2 # Worker to pre-fetch data for each single GPU","ecb4ad04":"# The config to build the evaluation hook, refer to https:\/\/github.com\/open-mmlab\/mmdetection\/blob\/master\/mmdet\/core\/evaluation\/eval_hooks.py#L7 for more details.\ncfg.evaluation.metric = 'bbox' # Metrics used during evaluation\n\n# Set the epoch intervel to perform evaluation\ncfg.evaluation.interval = 1\n\n# Set the iou threshold of the mAP calculation during evaluation\ncfg.evaluation.iou_thrs = [0.5]\n\n# cfg.evaluation.save_best='bbox_mAP_50'","4faf194b":"# consider including other transformations for training\n\nalbu_train_transforms = [\n    dict(type='ShiftScaleRotate', shift_limit=0.0625,\n        scale_limit=0.15, rotate_limit=15, p=0.4),\n    dict(type='RandomBrightnessContrast', brightness_limit=0.2,\n         contrast_limit=0.2, p=0.5),\n#     dict(type='IAAAffine', shear=(-10.0, 10.0), p=0.4),\n# #     dict(type='MixUp', p=0.2, lambd=0.5),\n#     dict(type=\"Blur\", p=1.0, blur_limit=7),\n#     dict(type='CLAHE', p=0.5),\n#     dict(type='Equalize', mode='cv', p=0.4),\n#     dict(\n#         type=\"OneOf\",\n#         transforms=[\n#             dict(type=\"GaussianBlur\", p=1.0, blur_limit=7),\n#             dict(type=\"MedianBlur\", p=1.0, blur_limit=7),\n#         ],\n#         p=0.4,\n#     ),\n    \n#     dict(type='MixUp', p=0.2, lambd=0.5),\n#     dict(type='RandomRotate90', p=0.5),\n#     dict(type='CLAHE', p=0.5),\n#     dict(type='InvertImg', p=0.5),\n#     dict(type='Equalize', mode='cv', p=0.4),\n#     dict(type='MedianBlur', blur_limit=3, p=0.1)\n    ]\n\n\ncfg.train_pipeline = [\n    dict(type='LoadImageFromFile'), # First pipeline to load images from file path\n    dict(type='LoadAnnotations',\n         with_bbox=True,# Whether to use bounding box, True for detection\n         with_mask=True, # Whether to use instance mask, True for instance segmentation\n# Whether to convert the polygon mask to instance mask, set False for acceleration and to save memory\n         poly2mask=False), \n    dict(type='Resize', # Augmentation pipeline that resize the images and their annotations\n         img_scale=(1333, 800), # The largest scale of image\n         keep_ratio=True), # whether to keep the ratio between height and width.\n    dict(type='RandomFlip', flip_ratio=0.5), # Augmentation pipeline that flip the images and their annotations\n    dict(\n        type='Albu',\n        transforms=albu_train_transforms, # transformations defined above\n        bbox_params=dict(\n        type='BboxParams',\n        format='pascal_voc',\n        label_fields=['gt_labels'],\n        min_visibility=0.0,\n        filter_lost_elements=True),\n        keymap=dict(img='image', gt_bboxes='bboxes'),\n        update_pad_shape=False,\n        skip_img_without_anno=True),\n    dict(\n        type='Normalize', # Augmentation pipeline that normalize the input images\n        mean=[123.675, 116.28, 103.53], # These keys are the same of img_norm_cfg since the\n        std=[58.395, 57.12, 57.375], # keys of img_norm_cfg are used here as arguments\n        to_rgb=True),\n    dict(type='Pad', size_divisor=32),# Padding config, The number the padded images should be divisible\n    dict(type='DefaultFormatBundle'), # Default format bundle to gather data in the pipeline\n    dict(type='Collect',# Pipeline that decides which keys in the data should be passed to the detector\n         keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n]\ncfg.test_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug', # An encapsulation that encapsulates the testing augmentations\n        img_scale=(1333, 800), # Decides the largest scale for testing, used for the Resize pipeline\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'), # Thought RandomFlip is added in pipeline, it is not used because flip=False\n            dict(\n                type='Normalize',\n                mean=[123.675, 116.28, 103.53],\n                std=[58.395, 57.12, 57.375],\n                to_rgb=True),\n            dict(type='Pad', size_divisor=32), # Padding config to pad images divisable by 32.\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']) # Collect pipeline that collect necessary keys for testing.\n        ])\n]","36cf3c1c":"cfg_path = f'{job_folder}\/job{job}_{Path(baseline_cfg_path).name}'\nprint(cfg_path)\n\n# Save config file for inference later\ncfg.dump(cfg_path)\nprint(f'Config:\\n{cfg.pretty_text}')","1f75ed3b":"# Build Dataset and Start Training\n\nmodel = build_detector(cfg.model,\n                       train_cfg=cfg.get('train_cfg'),\n                       test_cfg=cfg.get('test_cfg'))\nmodel.init_weights()\ndatasets = [build_dataset(cfg.data.train)]","966276f2":"train_detector(model, datasets[0], cfg, distributed=False, validate=True)","61b2f5fa":"# Get the best epoch number\nimport json\nfrom collections import defaultdict\n\nlog_file = f'{job_folder}\/None.log.json'\n\n# Source: mmdetection\/tools\/analysis_tools\/analyze_logs.py \ndef load_json_logs(json_logs):\n    # load and convert json_logs to log_dict, key is epoch, value is a sub dict\n    # keys of sub dict is different metrics, e.g. memory, bbox_mAP\n    # value of sub dict is a list of corresponding values of all iterations\n    log_dicts = [dict() for _ in json_logs]\n    for json_log, log_dict in zip(json_logs, log_dicts):\n        with open(json_log, 'r') as log_file:\n            for line in log_file:\n                log = json.loads(line.strip())\n                # skip lines without `epoch` field\n                if 'epoch' not in log:\n                    continue\n                epoch = log.pop('epoch')\n                if epoch not in log_dict:\n                    log_dict[epoch] = defaultdict(list)\n                for k, v in log.items():\n                    log_dict[epoch][k].append(v)\n    return log_dicts\n\nlog_dict = load_json_logs([log_file])\n# [(print(inner['bbox_mAP']) for inner in item) for item in log_dict]\n# [print(item) for item in log_dict[0]]\nbest_epoch = np.argmax([item['bbox_mAP'][0] for item in log_dict[0].values()])+1\nbest_epoch","c7b55b65":"data_dir = '\/kaggle\/input\/nfl-health-and-safety-helmet-assignment\/'\nexample_video = f'{data_dir}\/test\/57906_000718_Endzone.mp4'\n\nfrac = 0.65\ndisplay(Video(example_video, embed=True, height=int(720*frac), width=int(1280*frac)))","f42c7ee8":"# create frames \nimg_ext = 'png'\nimage_name = '57906_000718_Endzone'\nframe_dir = '\/kaggle\/tmp\/mp4_img\/'\nos.makedirs(frame_dir, exist_ok=True)\n\ncmd = 'ffmpeg -i \\\"{}\\\" -qscale:v 2 \\\"{}\/{}_%d.{}\\\"'.format(example_video, frame_dir, image_name, img_ext)\nprint(cmd)\nsubprocess.call(cmd, shell=True)\n\nframe_bbox_dir = '\/kaggle\/tmp\/mp4_img_bbox\/'\nos.makedirs(frame_bbox_dir, exist_ok=True)\ncheckpoint = f'{job_folder}\/epoch_{best_epoch}.pth'\nprint(\"Loading weights from:\", checkpoint)\ncfg = Config.fromfile(cfg_path)\n\nfor f in tqdm(os.listdir(frame_dir)):\n    img = f'{frame_dir}\/{f}'\n    # the model is initialized and deleted each time because of RAM usage\n    model = init_detector(cfg, checkpoint, device='cuda:0')\n    # get results\n    result = inference_detector(model, img)\n    # save image with bboxes into out_file\n    model.show_result(img, result, out_file=os.path.join(frame_bbox_dir,f))\n    del result, model\n    gc.collect()\n    \n# make video from frames\nvideo_name = '57906_000718_Endzone_fps60.mp4'\ntmp_video_path = os.path.join('\/kaggle\/working\/', f'tmp_{video_name}')\nvideo_path = os.path.join('\/kaggle\/working\/', video_name)\n\nframe_rate = 60\n\nimages = [img for img in os.listdir(frame_bbox_dir)]\nimages.sort(key = lambda x: int(x.split('_')[-1][:-4]))\n\nframe = cv2.imread(os.path.join(frame_bbox_dir, images[0]))\nheight, width, layers = frame.shape\n\nvideo = cv2.VideoWriter(tmp_video_path, cv2.VideoWriter_fourcc(*'MP4V'),\n                        frame_rate, (width,height))\n\nfor f in images:\n    img = cv2.imread(os.path.join(frame_bbox_dir, f))\n    video.write(img)\n\nvideo.release()\n\n# Not all browsers support the codec, we will re-load the file at tmp_video_path\n# and convert to a codec that is more broadly readable using ffmpeg\n\nif os.path.exists(video_path):\n    os.remove(video_path)\n    \nsubprocess.run([\"ffmpeg\", \"-i\", tmp_video_path, \"-crf\", \"18\", \"-preset\", \"veryfast\",\n                \"-vcodec\",\"libx264\", video_path,])\n\nos.remove(tmp_video_path)","6e034acf":"frac = 0.65\ndisplay(Video(video_path, embed=True, height=int(720*frac), width=int(1280*frac)))","549e6b40":"# remove directories with frames (optional)\n\nfor path in [frame_dir, frame_bbox_dir]:\n    try:\n        shutil.rmtree(path)\n    except OSError as e:\n        print (\"Error: %s - %s.\" % (e.filename, e.strerror))","9cdc41c4":"# MMDetection CascadeRCNN \n\n#### **The purpse of this notebook is to explore method for object detection, concretly helmet detection, using `MMDetection` package and CascadeRCNN model.**\n\nWe'll use labeled data set provided for training.","bc7a6ccb":"The process of converting video into frames, predicting bboxes and converting back into video is described here https:\/\/www.kaggle.com\/eneszvo\/mmdetection-player-tracking-for-beginners","116446c7":"### References\n\n* MMDetection repository https:\/\/github.com\/open-mmlab\/mmdetection\n* SIIM MMDetection+CascadeRCNN+Weight&Bias https:\/\/www.kaggle.com\/sreevishnudamodaran\/siim-mmdetection-cascadercnn-weight-bias\n* NFL Helmet Assignment - Getting Started Guide https:\/\/www.kaggle.com\/robikscube\/nfl-helmet-assignment-getting-started-guide\n\n**Plese check my related notebook**\n* MMDetection player tracking for beginners https:\/\/www.kaggle.com\/eneszvo\/mmdetection-player-tracking-for-beginners\n* https:\/\/zhuanlan.zhihu.com\/p\/385702286","556031e2":"# MMDdetection\n\nMMDetection is an open source object detection toolbox based on PyTorch.\n\n## Major features\n\n* **Modular Design**\n\nThe detection framework consist of different components and one can easily construct a customized object detection framework by combining different modules.\n\n* **Support of multiple frameworks out of box**\n\nThe toolbox directly supports popular and contemporary detection frameworks, e.g. Faster RCNN, Mask RCNN, RetinaNet, etc.\n\n* **High efficiency**\n\nAll basic bbox and mask operations run on GPUs. The training speed is faster than or comparable to other codebases, including Detectron2, maskrcnn-benchmark and SimpleDet.\n\n* **State of the art**\n\nThe toolbox stems from the codebase developed by the MMDet team, who won COCO Detection Challenge in 2018, and we keep pushing it forward.\n","cc2748dd":"I'm not sure how to change label above boxes, but anyway, we can see that results are good even with one epoch of training.","d219c482":"Create COCO format data set which is the standard format in object detection.","3d570572":"It is recommended to install MMDetection with MIM, which automatically handle the dependencies of OpenMMLab projects, including mmcv and other python packages."}}