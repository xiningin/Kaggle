{"cell_type":{"a2ab3c6d":"code","68175c5a":"code","bcf7f7c8":"code","84863e3f":"code","2a81d978":"code","b3633f73":"code","8bd5a941":"code","b5593aa9":"code","40b1a489":"code","00f2883f":"code","437e47b6":"code","8ec6286d":"code","19bae565":"code","98d81825":"code","d591baea":"code","f361562a":"code","2152de21":"code","d32aacae":"code","9e48a366":"code","3959a82b":"code","77132141":"code","097e45ff":"code","8f032903":"code","9b15e9d4":"code","00ac3482":"code","065ee680":"code","42cc1da0":"code","33a52c2a":"code","04de48af":"code","1870886a":"code","f0d7fbc5":"code","30f59775":"code","ba4bbf7e":"code","157e02fc":"code","a12c1a2a":"code","65836f7e":"code","0ab5fca4":"code","113e8674":"code","adaff948":"code","d9156fe0":"code","a0aa6446":"code","edda0dc2":"code","331bd65c":"code","3fd11a79":"code","f8f78471":"code","7063fb31":"code","a33053e8":"code","f59b67f7":"code","3109c113":"code","698c12b4":"code","62f80be6":"code","528a774a":"code","966b7052":"code","5df5c8f1":"code","3456a239":"code","83808dc6":"code","8f1413fc":"code","a68be9f3":"code","2c1c53b8":"code","8fc1e5be":"code","0a24eaae":"code","538a2ecf":"code","659d2609":"code","211f62db":"code","b2a4c2ad":"markdown","1b752560":"markdown","7486855d":"markdown","de2a1f67":"markdown","2feab6ca":"markdown","10fa17ea":"markdown","8183abf5":"markdown","f6daa153":"markdown","2d08287b":"markdown","55914193":"markdown","ded3ec98":"markdown"},"source":{"a2ab3c6d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","68175c5a":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport string\nimport re\nimport nltk\nfrom sklearn.preprocessing import StandardScaler\nfrom nltk.corpus import stopwords \nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import Counter\nfrom collections import defaultdict\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import RidgeClassifier\n\n# Import elements for multilayer percepton \nfrom keras import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nimport tensorflow as tf","bcf7f7c8":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsample = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","84863e3f":"train.head()","2a81d978":"test.head()","b3633f73":"print(\"Shape of training set : {}\".format(train.shape))\nprint(\"Shape of test set : {}\".format(test.shape))","8bd5a941":"### Lets check for the mssing values \nprint(\"NO OF MISSING VALUES :\");print(\"------Training set-------\")\nprint(\"keyword : {}\".format(train.keyword.isnull().sum()))\nprint(\"location : {}\".format(train.location.isnull().sum()))\nprint(\"-------Test data--------\")\nprint(\"keyword : {}\".format(test.keyword.isnull().sum()))\nprint(\"location : {}\".format(test.location.isnull().sum()))\nprint(\"\");print(\"\")\nprint(\"PROPORTION OF MISSING VALUES :\");print(\"------Training set-------\")\nprint(\"keyword : {}\".format(train.keyword.isnull().sum()\/train.shape[0]*100))\nprint(\"location : {}\".format(train.location.isnull().sum()\/train.shape[0]*100))\nprint(\"-------Test data--------\")\nprint(\"keyword : {}\".format(test.keyword.isnull().sum()\/test.shape[0]*100))\nprint(\"location : {}\".format(test.location.isnull().sum()\/test.shape[0]*100))","b5593aa9":"### remove location\ntrain.drop(columns=['location','keyword'],inplace=True)\ntest.drop(columns=['location','keyword'],inplace=True)","40b1a489":"### class distribution of target###\nsns.set(style='darkgrid', context='notebook')\nsns.countplot(x='target', data=train).set_title('target distribution')\n#plt.title('target distribution')","00f2883f":"### lets take a quick look at some of the texts to guess what sort of cleaning needs to be done\nfor i in [5,12,44,22,45,67,99,122,455,78,2225,558,111,5578,546,447,944,6557,1115,447,6552,4177,700,4999,425]:\n    print(train.text[i])\n    print('\\n')","437e47b6":"## Lets see a random piece of text\ntrain.text[455]","8ec6286d":"### Visualising through wordclouds \ntexts = train.text.tolist()\ntexts_combined = ' '.join(texts)\nplt.figure(figsize=(14,14))\nplt.imshow(WordCloud().generate(texts_combined))\nplt.axis(\"off\")","19bae565":"### Visualising for positive texts\npositive = train.text[train.target == 1]\npositive_texts = positive.tolist()\npositive_texts_combined = ' '.join(positive_texts)\nplt.figure(figsize=(14,14))\nplt.imshow(WordCloud().generate(positive_texts_combined))\nplt.axis(\"off\")\nplt.title(\"positive texts\")","98d81825":"### Visualising for negative texts\nnegative = train.text[train.target == 0]\nnegative_texts = negative.tolist()\nnegative_texts_combined = ' '.join(negative_texts)\nplt.figure(figsize=(14,14))\nplt.imshow(WordCloud().generate(negative_texts_combined))\nplt.axis(\"off\")\nplt.title(\"Negative targets\")","d591baea":"#text = \"model love u take u time ur\u00f0\\x9f\\x93\u00b1 \u00f0\\x9f\\x98\\x99\u00f0\\x9f\\x98\\x8e\u00f0\\x9f\\x91\\x84\u00f0\\x9f\\x91 \u00f0\\x9f\\x92\u00a6\u00f0\\x9f\\x92\u00a6\u00f0\\x9f\\x92\u00a6\"\n#print(text)\n#text = remove_irrelevant(text)\n#text = remove_punc(text)\n#text = remove_emoji(text)\n#text = re.sub('[@#]*[@\\w]*\\d{1,}[@\\w]*','',text)\n#text = re.sub('\\d+','',text)\n#text = re.sub('_*',' ',text)\n#text = re.sub('[[\\w*]]*','',text)\n#text = re.sub('[\\t\\n\\r\\f\\v]+','',text)\n#text = re.sub('@\\w*','',text)\n#text = re.sub('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',text)\n#text = remove_punc(text)\n#print(\"\\n\");print(text)\n#print(text)","f361562a":"## example demonstrating decoding into ASCII characters\n\nstring_with_nonASCII = \"\u00e0a string with\u00e9 all the fu\u00fcnny characters\u00df.\"\n\nencoded_string = string_with_nonASCII.encode(\"ascii\", \"ignore\")\ndecode_string = encoded_string.decode()\n\nprint(decode_string)","2152de21":"## example demonstrating removal of unicode encoded characters\n\nstring = \"jbj\\x9f gn9 blasts\"\nprint(string)\n\nstring_unicode_removed = string.encode(\"ascii\", \"ignore\").decode()\nprint(string_unicode_removed)","d32aacae":"def remove_irrelevant(text):\n    \n    ## convert non-ASCII to ASCII characters\n    text = text.encode(\"ascii\",\"ignore\").decode()\n    ## remove emails\n    text = re.sub('[\\w\\.-]+@[\\w\\.-]+','',text)\n    ## remove words within brackets\n    text = re.sub('\\[.*?\\]', '', text)\n    ## remove words containing nos. & special characters in between\n    text = re.sub('[@#-]*[@\\w]*\\d{1,}[-@\\w]*','',text)\n    ## remove numbers\n    text = re.sub('\\d+','',text)\n    ## remove blankspace characters\n    text = re.sub('[\\t\\n\\r\\f\\v]+','',text)\n    ## remove callouts (starting with @)\n    text = re.sub('@\\w*','',text)\n    ## remove urls\n    text = re.sub('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',text)\n    ## convert to lowercase\n    text = text.lower()\n    \n    return text\n\n### remove punctuations\ndef remove_punc(text):\n    not_punc = [w for w in text if w not in string.punctuation]\n    text_punc_removed = ''.join(not_punc)\n    return text_punc_removed\n\ndef remove_emoji(text): ## remove emojis\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)    \n \n### remove repetitive characters in a word\ndef rep(text):\n    grp = text.group(0)\n    if len(grp) > 1:\n        return grp[0:2] # can change the value here on repetition\ndef unique_char(rep,sentence):\n    convert = re.sub(r'(\\w)\\1+', rep, sentence) \n    return convert\n\n\n### Lemmatization\ndef lemmatize(text):\n    \n    lemmatizer = WordNetLemmatizer()\n    return \" \".join([lemmatizer.lemmatize(i) for i in text.split()])","9e48a366":"### example removal of irrelevant characters\nimport string\ntext = \"fdf @nj23jnj-jn nkn! @ddsk kkm [sssd23_dd!]\"\n\ntext = remove_irrelevant(text)\ntext = remove_punc(text)\n\nprint(text)","3959a82b":"### example showing conversion of repetitive characters to 2 characters\nsentence=\"heyyy givvve meee the address\"\nunique_char(rep,sentence)","77132141":"### cleaning the texts\ntrain[\"text\"] = train[\"text\"].apply(lambda x:remove_irrelevant(x))\ntrain[\"text\"] = train[\"text\"].apply(lambda x:remove_punc(x))\ntrain[\"text\"] = train[\"text\"].apply(lambda x:remove_emoji(x))\ntrain[\"text\"] = train[\"text\"].apply(lambda x:unique_char(rep,x))\ntrain[\"text\"] = train[\"text\"].apply(lambda x:lemmatize(x))\n\npositive = train.text[train.target == 1]\nnegative = train.text[train.target == 0]\n\ntrain.head()","097e45ff":"### Visualising cleaned texts for identifying stopwords\n\nwords_combined = []\nfor i in range(train.shape[0]):\n    for w in train.text[i].split():\n        words_combined.append(w)\n#words_combined\n#print(len(words_combined))\n\ndict = Counter(words_combined).most_common()\ndict[:60]","8f032903":"## Adding more stopwords(unigram)\n\nstopwords = stopwords.words(\"english\")\n\nadd = [word for word,count in dict if count>200]+['ve','rs','ll','d','t','s']\n\nkeep = ['i','you','my','with','like','as','me','your','not','its','out','after','all','no','fire','we','get','new'\n       ,'now','more','dont','about']\n\nif add not in stopwords:\n    stopwords_added = stopwords + add\nstopwords_added  \n\nstopwords_unigram = [w for w in stopwords_added if w not in keep]\nprint(stopwords_unigram)","9b15e9d4":"### demo showing to generate n-grams\n\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(' ')]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]\n\ngenerate_ngrams(\"ccc zxxc  asd asca  dvdd asfa\",n_gram=2)## bigrams","00ac3482":"# Bigrams\n\ndisaster_bigrams = defaultdict(int)\nnondisaster_bigrams = defaultdict(int)\n\nfor tweet in positive:\n    for word in generate_ngrams(tweet, n_gram=2):\n        disaster_bigrams[word] += 1\n        \nfor tweet in negative:\n    for word in generate_ngrams(tweet, n_gram=2):\n        nondisaster_bigrams[word] += 1\n        \ndf_disaster_bigrams = pd.DataFrame(sorted(disaster_bigrams.items(), key=lambda x: x[1])[::-1])\ndf_nondisaster_bigrams = pd.DataFrame(sorted(nondisaster_bigrams.items(), key=lambda x: x[1])[::-1])\n\n# Trigrams\ndisaster_trigrams = defaultdict(int)\nnondisaster_trigrams = defaultdict(int)\n\nfor tweet in positive:\n    for word in generate_ngrams(tweet, n_gram=3):\n        disaster_trigrams[word] += 1\n        \nfor tweet in negative:\n    for word in generate_ngrams(tweet, n_gram=3):\n        nondisaster_trigrams[word] += 1\n        \ndf_disaster_trigrams = pd.DataFrame(sorted(disaster_trigrams.items(), key=lambda x: x[1])[::-1])\ndf_nondisaster_trigrams = pd.DataFrame(sorted(nondisaster_trigrams.items(), key=lambda x: x[1])[::-1])","065ee680":"fig, axes = plt.subplots(ncols=2, figsize=(18, 50), dpi=100)\nplt.tight_layout()\n\nsns.barplot(y=df_disaster_bigrams[0].values[:100], x=df_disaster_bigrams[1].values[:100], ax=axes[0], color='red')\nsns.barplot(y=df_nondisaster_bigrams[0].values[:100], x=df_nondisaster_bigrams[1].values[:100], ax=axes[1], color='green')\n\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\nN = 100\naxes[0].set_title(f'Top {N} most common bigrams in Disaster Tweets', fontsize=15)\naxes[1].set_title(f'Top {N} most common bigrams in Non-disaster Tweets', fontsize=15)\n\nplt.show()","42cc1da0":"## adding bigram stopwords\n\nadd_bigrams_positive = ['in the','of the','on the','by the','at the','and the','to be','to the','by a','for the',\n              'like a','of a','is a','from the','for a','as from','is from','for a','under a','in under','to a']\n\nadd_bigrams_negative = ['for a','i wa','do you','of a','have to',\n                   'with the','be a','to get','it wa','are you','if i','full re','wa a']\n\nstopwords_bigram = add_bigrams_positive + add_bigrams_negative\nprint(stopwords_bigram)","33a52c2a":"fig, axes = plt.subplots(ncols=2, figsize=(22, 50), dpi=100)\nplt.tight_layout()\n\nsns.barplot(y=df_disaster_trigrams[0].values[:100], x=df_disaster_trigrams[1].values[:100], ax=axes[0], color='red')\nsns.barplot(y=df_nondisaster_trigrams[0].values[:100], x=df_nondisaster_trigrams[1].values[:100], ax=axes[1], color='green')\n\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\nN = 100\naxes[0].set_title(f'Top {N} most common trigrams in Disaster Tweets', fontsize=15)\naxes[1].set_title(f'Top {N} most common trigrams in Non-disaster Tweets', fontsize=15)\n\nplt.show()","04de48af":"### adding trigram stopwords\n\nadd_trigrams_positive = ['in under a','up by a']\nadd_trigrams_negative = ['china stock market','cross body bag','reddit will now']\n\nstopwords_trigram = add_trigrams_positive + add_trigrams_negative\nprint(stopwords_trigram)","1870886a":"xtrain,xvalid,ytrain,yvalid = train_test_split(train.text.values, train.target.values, test_size=0.2, \n                                              random_state=22)\nprint(xtrain.shape)\nprint(xvalid.shape)","f0d7fbc5":"pd.DataFrame(xtrain).head()","30f59775":"stopwords = stopwords_unigram + stopwords_bigram + stopwords_trigram\nctv = CountVectorizer(stop_words=stopwords ,ngram_range=(1,3))\n\n# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\nctv.fit(list(xtrain) + list(xvalid))\nxtrain_ctv =  ctv.transform(xtrain) \nxvalid_ctv = ctv.transform(xvalid)","ba4bbf7e":"# Fitting a simple Logistic Regression on Counts\n\nclf = LogisticRegression(C=1.0)\nclf.fit(xtrain_ctv, ytrain)\npredictions = clf.predict(xvalid_ctv)\n\nprint('Accuracy of Logistic Regression classifier on training set: {:.2f}'\n     .format(clf.score(xtrain_ctv, ytrain)))\nprint('Accuracy of Logistic Regression classifier on test set: {:.2f}'\n     .format(clf.score(xvalid_ctv, yvalid)))\nprint('F1-score of Logistic Regression is: {:.2f}'.format(f1_score(yvalid,predictions)))\ncm = confusion_matrix(yvalid, predictions)\ncm","157e02fc":"tfv = TfidfVectorizer(min_df=2,  max_features=None, \n            strip_accents='unicode', analyzer='word',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n\n# Fitting TF-IDF to both training and test sets (semi-supervised learning)\ntfv.fit(list(xtrain) + list(xvalid))\nxtrain_tfv =  tfv.transform(xtrain) \nxvalid_tfv = tfv.transform(xvalid)","a12c1a2a":"# Fitting a simple Logistic Regression on TFIDF\nclf = LogisticRegression(C=1.0)\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict(xvalid_tfv)\n\nprint('Accuracy of Logistic Regression classifier(with tfidf) on training set: {:.2f}'\n     .format(clf.score(xtrain_tfv, ytrain)))\nprint('Accuracy of Logistic Regression(with tfidf) classifier on test set: {:.2f}'\n     .format(clf.score(xvalid_tfv, yvalid)))\nprint('F1-score of Logistic Regression is: {:.2f}'.format(f1_score(yvalid,predictions)))\ncm = confusion_matrix(yvalid, predictions)\ncm","65836f7e":"# Fitting a simple Naive Bayes on TFIDF\nclf = MultinomialNB()\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict(xvalid_tfv)\n\nprint('Accuracy of Naive Bayes classifier(with tfidf) on training set: {:.2f}'\n     .format(clf.score(xtrain_tfv, ytrain)))\nprint('Accuracy of Naive Bayes classifier(with tfidf) classifier on validation set: {:.2f}'\n     .format(clf.score(xvalid_tfv, yvalid)))\nprint('F1-score of Naive Bayes (with tf-idf) is: {:.2f}'.format(f1_score(yvalid,predictions)))\ncm = confusion_matrix(yvalid, predictions)\ncm","0ab5fca4":"# Fitting a Naive Bayes on Counts\n\nclf = MultinomialNB()\nclf.fit(xtrain_ctv, ytrain)\npredictions = clf.predict(xvalid_ctv)\n\nprint('Accuracy of Naive Bayes classifier(with CountVectorizer) on training set: {:.2f}'\n     .format(clf.score(xtrain_ctv, ytrain)))\nprint('Accuracy of Naive Bayes classifier(with CountVectorizer) on test set: {:.2f}'\n     .format(clf.score(xvalid_ctv, yvalid)))\nprint('F1-score of Naive Bayes is: {:.2f}'.format(f1_score(yvalid,predictions)))\ncm = confusion_matrix(yvalid, predictions)\ncm","113e8674":"### There are more than 1lakh features obtained after CountVectorising\n### We need to reduce it to make SVM run efficiently\nsvd = TruncatedSVD(n_components=120)\nsvd.fit(xtrain_tfv)\nxtrain_svd = svd.transform(xtrain_tfv)\nxvalid_svd = svd.transform(xvalid_tfv)\n\n# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\nscl = StandardScaler()\nscl.fit(xtrain_svd)\nxtrain_svd_scl = scl.transform(xtrain_svd)\nxvalid_svd_scl = scl.transform(xvalid_svd)","adaff948":"# Fitting a simple SVM( with tf-idf values)\nclf = SVC(C=1.0) \nclf.fit(xtrain_svd_scl, ytrain)\npredictions = clf.predict(xvalid_svd_scl)\n\nprint('Accuracy of SVC classifier(with TfidfVectorizer) on training set: {:.2f}'\n     .format(clf.score(xtrain_svd_scl, ytrain)))\nprint('Accuracy of SVC classifier(with TfidfVectorizer) on test set: {:.2f}'\n     .format(clf.score(xvalid_svd_scl, yvalid)))\nprint('F1-score of SVC classifier is: {:.2f}'.format(f1_score(yvalid,predictions)))\ncm = confusion_matrix(yvalid, predictions)\ncm","d9156fe0":"#### Fitting a simple SVM( with CountVectorizer fit)\n\nsvd.fit(xtrain_ctv)\nxtrain_svd = svd.transform(xtrain_ctv)\nxvalid_svd = svd.transform(xvalid_ctv)\n\n# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\nscl = StandardScaler()\nscl.fit(xtrain_svd)\nxtrain_svd_scl = scl.transform(xtrain_svd)\nxvalid_svd_scl = scl.transform(xvalid_svd)\n\nclf = SVC(C=1.0) \nclf.fit(xtrain_svd_scl, ytrain)\npredictions = clf.predict(xvalid_svd_scl)\n\nprint('Accuracy of SVC classifier(with CountVectorizer) on training set: {:.2f}'\n     .format(clf.score(xtrain_svd_scl, ytrain)))\nprint('Accuracy of SVC classifier(with CountVectorizer) on test set: {:.2f}'\n     .format(clf.score(xvalid_svd_scl, yvalid)))\nprint('F1-score of SVC classifier(over counts) is: {:.2f}'.format(f1_score(yvalid,predictions)))\ncm = confusion_matrix(yvalid, predictions)\ncm","a0aa6446":"# Fitting a simple xgboost on tf-idf\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf.fit(xtrain_tfv.tocsc(), ytrain)\npredictions = clf.predict(xvalid_tfv.tocsc())\n\nprint('Accuracy of XGB classifier(with TfidfVectorizer) on training set: {:.2f}'\n     .format(clf.score(xtrain_tfv.tocsc(), ytrain)))\nprint('Accuracy of XGB classifier(with TfidfVectorizer) on test set: {:.2f}'\n     .format(clf.score(xvalid_tfv.tocsc(), yvalid)))\nprint('F1-score of XGB classifier is: {:.2f}'.format(f1_score(yvalid,predictions)))\ncm = confusion_matrix(yvalid, predictions)\ncm","edda0dc2":"# Fitting a simple xgboost on Counts\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf.fit(xtrain_ctv.tocsc(), ytrain)\npredictions = clf.predict(xvalid_ctv.tocsc())\n\nprint('Accuracy of XGB classifier(with TfidfVectorizer) on training set: {:.2f}'\n     .format(clf.score(xtrain_ctv.tocsc(), ytrain)))\nprint('Accuracy of XGB classifier(with TfidfVectorizer) on test set: {:.2f}'\n     .format(clf.score(xvalid_ctv.tocsc(), yvalid)))\nprint('F1-score of XGB classifier is: {:.2f}'.format(f1_score(yvalid,predictions)))\ncm = confusion_matrix(yvalid, predictions)\ncm","331bd65c":"### parameter(C) tuning using Grid Search for Logistic regression over counts\n\ngsc = GridSearchCV(estimator=LogisticRegression(max_iter=1000), \n                  param_grid = {'C': [0.01, 0.1, 1, 10, 100, 1000]},\n                  cv=5, scoring='f1')\n\ngrid_result = gsc.fit(xtrain_ctv, ytrain)\nbest_params = grid_result.best_params_\nbest_params","3fd11a79":"### parameter(C) tuning using Grid Search for NaiveBayes over counts\n\ngsc = GridSearchCV(estimator=MultinomialNB(), \n                  param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]},\n                  cv=5, scoring='f1')\n\ngrid_result = gsc.fit(xtrain_ctv, ytrain)\nbest_params = grid_result.best_params_\nbest_params","f8f78471":"# Initialize SVD\nsvd = TruncatedSVD()\n    \n# Initialize the standard scaler \nscl = StandardScaler()\n\n# We will use logistic regression here..\nlr_model = LogisticRegression(max_iter=1000)\n\n# Create the pipeline \nclf = Pipeline([('svd', svd),\n                         ('scl', scl),('lr',lr_model)])","7063fb31":"param_grid = {'svd__n_components' : [120, 180],\n              'lr__C': [0.1, 1.0, 10], \n              'lr__penalty': ['l1', 'l2']}","a33053e8":"model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1',\n                     verbose=10, n_jobs=-1, iid=True, refit=True, cv=5)\n\nresult = model.fit(xtrain_ctv, ytrain)\nbest_params = result.best_params_","f59b67f7":"# hence the best fit happens with 180 svd components\nbest_params","3109c113":"svd = TruncatedSVD(n_components=180)\nsvd.fit(xtrain_ctv)\nxtrain_svd = svd.transform(xtrain_ctv)\nxvalid_svd = svd.transform(xvalid_ctv)\n\nclf = LogisticRegression(C=1.0, max_iter=1000, penalty='l2')\nclf.fit(xtrain_svd, ytrain)\npredictions = clf.predict(xvalid_svd)\n\nprint('Accuracy of Logistic Regression classifier(with counts) on training set: {:.2f}'\n     .format(clf.score(xtrain_svd, ytrain)))\nprint('Accuracy of Logistic Regression(with counts) classifier on test set: {:.2f}'\n     .format(clf.score(xvalid_svd, yvalid)))\nprint('F1-score of Logistic Regression is: {:.2f}'.format(f1_score(yvalid,predictions)))\ncm = confusion_matrix(yvalid, predictions)\ncm","698c12b4":"# Model tuning a RandomForestClassifier with 180 SVD components\n\ngsc = GridSearchCV(estimator=RandomForestClassifier(), \n                  param_grid = {'n_estimators': [10, 100, 200]},\n                  cv=5, scoring='f1')\n\ngrid_result = gsc.fit(xtrain_svd, ytrain)\nbest_params = grid_result.best_params_\nbest_params","62f80be6":"# Fitting the RandomForest classifier with 180 svd components (over counts)\n\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(xtrain_svd, ytrain)\npredictions = clf.predict(xvalid_svd)\nprint('F1-score of RandomForest is: {:.2f}'.format(f1_score(yvalid,predictions)))","528a774a":"# Model tuning a RidgeClassifier with 180 SVD components\n\ngsc = GridSearchCV(estimator=RidgeClassifier(), \n                  param_grid = {'alpha': [0.01, 0.1, 1, 10, 50, 100]},\n                  cv=5, scoring='f1')\n\ngrid_result = gsc.fit(xtrain_ctv, ytrain)\nbest_params = grid_result.best_params_\nbest_params","966b7052":"clf = RidgeClassifier(alpha=10)\nclf.fit(xtrain_ctv, ytrain)\npredictions = clf.predict(xvalid_ctv)\n\nprint('Accuracy of Ridge Regression classifier(with counts) on training set: {:.2f}'\n     .format(clf.score(xtrain_ctv, ytrain)))\nprint('Accuracy of Ridge Regression classifier(with counts) classifier on test set: {:.2f}'\n     .format(clf.score(xvalid_ctv, yvalid)))\nprint('F1-score of Ridge Regression is: {:.2f}'.format(f1_score(yvalid,predictions)))","5df5c8f1":"# Input shape for model\ninput_shape = xtrain_tfv.shape[1]\n\n# Set callbacks to avoid overfitting\ncallbacks = [EarlyStopping(patience=3), ReduceLROnPlateau(patience=2)]","3456a239":"# Construct multilayer net\n\nmlp_model = Sequential([\n    Dense(64, input_dim=input_shape, activation='relu'),\n    Dropout(0.5),\n    Dense(32, activation='relu'),\n    Dropout(0.5),\n    Dense(16, activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid'),   \n])\n\nmlp_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","83808dc6":"pd.DataFrame(xvalid_ctv.toarray())","8f1413fc":"# Train model\n#train_sparse = pd.concat(pd.DataFrame(xtrain_ftv))\n#tf.sparse.reorder(xtrain.tfv);tf.\nmlp_model.fit(xtrain_tfv.toarray(), ytrain,\n             batch_size=16,\n             epochs=30, \n             verbose=2,\n             callbacks=callbacks,\n             validation_data=(xvalid_tfv.toarray(), yvalid))","a68be9f3":"# Check model on test validation_set\ntest_score = mlp_model.evaluate(xvalid_tfv.toarray(), yvalid)\nprint(test_score)","2c1c53b8":"### cleaning the test texts\ntest[\"text\"] = test[\"text\"].apply(lambda x:remove_irrelevant(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x:remove_punc(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x:remove_emoji(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x:unique_char(rep,x))\ntest[\"text\"] = test[\"text\"].apply(lambda x:lemmatize(x))\n","8fc1e5be":"my_final_submission = pd.DataFrame({'id':test.id.values, 'text':test.text.values})","0a24eaae":"xtest_tfv =  tfv.transform(my_final_submission.text.values)\nmy_final_submission[\"target\"] = mlp_model.predict_classes(xtest_tfv.toarray())\nmy_final_submission.drop(columns=['text'], inplace=True)\nmy_final_submission.head()","538a2ecf":"#clf = LogisticRegression(max_iter=1000, C=1.0)\n#clf.fit(xtrain_ctv, ytrain)","659d2609":"#xtest_ctv =  ctv.transform(my_final_submission.text.values)\n#my_final_submission[\"target\"] = clf.predict(xtest_ctv)\n#my_final_submission.drop(columns=['text'], inplace=True)\n#my_final_submission.head()","211f62db":"my_final_submission.to_csv(\"submission.csv\", index=False)","b2a4c2ad":"Lets have a quick look into the dataset","1b752560":"Logistic regression works better with CountVectorizer than TfidfVectorizer","7486855d":"There lies a slight imbalance in the distribution of '0' & '1'","de2a1f67":"Looks like Svm didnot work well here. \nLets apply the xgboost","2feab6ca":"As we noted ,the performance doesnot  improves with Naive Bayes.\nNow lets check the performance of SVM.","10fa17ea":"Proportion of missing values for both 'keyword','location' is roughly the same in both training & test data where\n'keyword' : 0.8% in both train & test data\n    'location' : 33% in both train & test data","8183abf5":"Application of SVD for logit model is worthless.","f6daa153":"**A lot of stopwords, https, needs to be cleaned !!**","2d08287b":"Lets experiment with the SVD components ,after then grid search for logit model parameters","55914193":"Since SVM's take a lot of time to run, so we will reduce the no. of features using Singular Value Decomposition.\nAlso we have to perform Feature Scaling before applying SVM.","ded3ec98":"So C=1 is the optimal value for logit model over counts. Lets turn to Naive Bayes model. "}}