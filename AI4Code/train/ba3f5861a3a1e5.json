{"cell_type":{"01cd76bb":"code","be59187e":"code","352c53f3":"code","718e4078":"code","9c1bab96":"code","cea5879e":"code","5d97dbf7":"code","f14e0f63":"code","1c8331be":"code","a98e5e52":"code","6e4a88bf":"code","528e5e8d":"code","d8d189cb":"code","11ce713a":"code","88240db7":"code","b33226e3":"code","53793aa4":"code","498bc494":"code","6d930575":"code","6bf9c546":"code","2eb72bcb":"code","2e49af45":"code","fe9587c0":"code","bf48b4d2":"code","f076f739":"code","d23b9b6a":"code","cfc30f82":"code","773a5339":"code","fcffe8c5":"code","06f210fd":"code","a1c813c6":"code","567fabc1":"code","c3965b2e":"code","b46cb007":"code","e3674e29":"code","abccfcb7":"code","792005b4":"code","e76c4c82":"code","166de906":"code","d94c1e49":"code","65455d5c":"code","9c39f863":"code","b5bf47a6":"code","9e3eea2f":"code","7f210e7f":"markdown","c34450a6":"markdown","7f7456c3":"markdown","dbe63e8c":"markdown","a15b7531":"markdown","6d079da6":"markdown","fa67aada":"markdown","a2eae53f":"markdown","b8cb955e":"markdown","a67cce29":"markdown","e302922e":"markdown","4e0dab8f":"markdown","f395f66a":"markdown","7492ee18":"markdown","eafb5775":"markdown","999073b2":"markdown","7029cbf1":"markdown","ec5bf736":"markdown","cd8e211e":"markdown","e899a68f":"markdown","a9ccf3bc":"markdown","33a3c0ec":"markdown","20403122":"markdown","429aa5f0":"markdown","4fca7d31":"markdown","26dcba5e":"markdown","fc722e3c":"markdown"},"source":{"01cd76bb":"# Dependencies\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","be59187e":"# Reading Dataset\n\ndata = pd.read_csv('Gold_Daily.csv')\nprint(data.shape)\ndata.head()","352c53f3":"# Pre-processing Dataset\n\ndata = data[['Date', 'Price']]\ndata.Date = pd.to_datetime(data.Date, format='%b %d, %Y')\ndata.set_index('Date', drop=True, inplace=True)\nprint(data.shape)\ndata.head()","718e4078":"import datetime\nfrom datetime import timedelta\n\nbase = datetime.datetime(data.index.min().year, data.index.min().month, data.index.min().day)\narr = np.array([base + datetime.timedelta(days=i) for i in range((data.index.max() - data.index.min()).days+1)])\n\ndata_temp = pd.DataFrame(arr, columns=['date']).set_index('date', drop=True)\nprint(data_temp.shape)\ndata_temp.head(10)","9c1bab96":"data = pd.merge(left=data_temp, right=data,\n                     left_on=data_temp.index, right_on=data.index,\n                     how='left')\ndata.head(10)\ndata.set_index('key_0', drop=True, inplace=True)","cea5879e":"data.fillna(method='ffill', inplace=True)","5d97dbf7":"data.info()","f14e0f63":"data.head(10)","1c8331be":"plt.plot(data)","a98e5e52":"# Box plot\n\nimport seaborn as sns\nfig = plt.subplots(figsize=(12, 2))\nax = sns.boxplot(x=data['Price'],whis=1.5)","6e4a88bf":"# Histogram\n\nfig = data['Price'].hist(figsize = (12,4))","528e5e8d":"# Train-Test split\n\ntrain_len = int(data.shape[0]*0.8) # 80-20 split\ntrain = data[0:train_len] \ntest = data[train_len:] ","d8d189cb":"# Mapping to previous value\ny_hat_naive = test.copy()\ny_hat_naive['naive_forecast'] = train['Price'][train_len-1]\n\n\n# Plotting\nplt.figure(figsize=(12,4))\nplt.plot(train['Price'], label='Train')\nplt.plot(test['Price'], label='Test')\nplt.plot(y_hat_naive['naive_forecast'], label='Naive forecast')\nplt.legend(loc='best')\nplt.title('Naive Method')\nplt.show()\n\n\n# Calculate RMSE and MAPE\nfrom sklearn.metrics import mean_squared_error\n\nrmse = np.sqrt(mean_squared_error(test['Price'], y_hat_naive['naive_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['Price']-y_hat_naive['naive_forecast'])\/test['Price'])*100,2)\n\nresults = pd.DataFrame({'Method':['Naive method'], 'MAPE': [mape], 'RMSE': [rmse]})\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","11ce713a":"y_hat_avg = test.copy()\ny_hat_avg['avg_forecast'] = train['Price'].mean()\n\n\n# Plotting\nplt.figure(figsize=(12,4))\nplt.plot(train['Price'], label='Train')\nplt.plot(test['Price'], label='Test')\nplt.plot(y_hat_avg['avg_forecast'], label='Simple average forecast')\nplt.legend(loc='best')\nplt.title('Simple Average Method')\nplt.show()\n\n\n# Calculate RMSE and MAPE\nrmse = np.sqrt(mean_squared_error(test['Price'], y_hat_avg['avg_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['Price']-y_hat_avg['avg_forecast'])\/test['Price'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Simple average method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","88240db7":"y_hat_sma = data.copy()\nma_window = 12\ny_hat_sma['sma_forecast'] = data['Price'].rolling(ma_window).mean()\ny_hat_sma['sma_forecast'][train_len:] = y_hat_sma['sma_forecast'][train_len-1]\n\n\n# Plotting\nplt.figure(figsize=(12,4))\nplt.plot(train['Price'], label='Train')\nplt.plot(test['Price'], label='Test')\nplt.plot(y_hat_sma['sma_forecast'][train_len:], label='Simple moving average forecast')\nplt.legend(loc='best')\nplt.title('Simple Moving Average Method')\nplt.show()\n\n\n# Calculate RMSE and MAPE\nrmse = np.sqrt(mean_squared_error(test['Price'], y_hat_sma['sma_forecast'][train_len:])).round(2)\nmape = np.round(np.mean(np.abs(test['Price']-y_hat_sma['sma_forecast'][train_len:])\/test['Price'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Simple moving average forecast'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","b33226e3":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\n\nmodel = ExponentialSmoothing(np.asarray(train['Price']) ,seasonal_periods=12 ,trend='additive', seasonal=None)\nmodel_fit = model.fit(optimized=True)\nprint(model_fit.params)\ny_hat_holt = test.copy()\ny_hat_holt['holt_forecast'] = model_fit.forecast(len(test))\n\n\n# Plotting\nplt.figure(figsize=(12,4))\nplt.plot( train['Price'], label='Train')\nplt.plot(test['Price'], label='Test')\nplt.plot(y_hat_holt['holt_forecast'], label='Holt\\'s exponential smoothing forecast')\nplt.legend(loc='best')\nplt.title('Holt\\'s Exponential Smoothing Method')\nplt.show()\n\n\n# Calculate RSME and MAPE\nrmse = np.sqrt(mean_squared_error(test['Price'], y_hat_holt['holt_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['Price']-y_hat_holt['holt_forecast'])\/test['Price'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Holt\\'s exponential smoothing method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","53793aa4":"y_hat_hwa = test.copy()\nmodel = ExponentialSmoothing(np.asarray(train['Price']), seasonal_periods=12 ,trend='add', seasonal='add')\nmodel_fit = model.fit(optimized=True)\n# print(model_fit.params)\ny_hat_hwa['hw_forecast'] = model_fit.forecast(test.shape[0])\n\n\n# Plotting\nplt.figure(figsize=(12,4))\nplt.plot( train['Price'], label='Train')\nplt.plot(test['Price'], label='Test')\nplt.plot(y_hat_hwa['hw_forecast'], label='Holt Winters\\'s additive forecast')\nplt.legend(loc='best')\nplt.title('Holt Winters\\' Additive Method')\nplt.show()\n\n\n# Calculate RMSE and MAPE\nrmse = np.sqrt(mean_squared_error(test['Price'], y_hat_hwa['hw_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['Price']-y_hat_hwa['hw_forecast'])\/test['Price'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Holt Winters\\' additive method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","498bc494":"y_hat_hwm = test.copy()\nmodel = ExponentialSmoothing(np.asarray(train['Price']) ,seasonal_periods=12 ,trend='add', seasonal='mul')\nmodel_fit = model.fit(optimized=True)\n# print(model_fit.params)\ny_hat_hwm['hw_forecast'] = model_fit.forecast(test.shape[0])\n\n\n# Plotting\nplt.figure(figsize=(12,4))\nplt.plot( train['Price'], label='Train')\nplt.plot(test['Price'], label='Test')\nplt.plot(y_hat_hwm['hw_forecast'], label='Holt Winters\\'s mulitplicative forecast')\nplt.legend(loc = 'best')\nplt.title('Holt Winters\\' Mulitplicative Method')\nplt.show()\n\n\n# Calculate RMSE and MAPE\nrmse = np.sqrt(mean_squared_error(test['Price'], y_hat_hwm['hw_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['Price']-y_hat_hwm['hw_forecast'])\/test['Price'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Holt Winters\\' multiplicative method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","6d930575":"from statsmodels.tsa.stattools import adfuller\nadf_test = adfuller(data['Price'])\n\nprint('ADF Statistic: %f' % adf_test[0])\nprint('Critical Values @ 0.05: %.2f' % adf_test[4]['5%'])\nprint('p-value: %f' % adf_test[1])","6bf9c546":"from scipy.stats import boxcox\ndata_boxcox = pd.Series(boxcox(data['Price'], lmbda=0), index = data.index)\n\nplt.figure(figsize=(12,4))\nplt.plot(data_boxcox, label='After Box Cox tranformation')\nplt.legend(loc='best')\nplt.title('After Box Cox transform')\nplt.show()","2eb72bcb":"data_boxcox_diff = pd.Series(data['Price'] - data['Price'].shift(2), data.index)\nplt.figure(figsize=(12,4))\nplt.plot(data_boxcox_diff, label='After Box Cox tranformation and differencing')\nplt.legend(loc='best')\nplt.title('After Box Cox transform and differencing')\nplt.show()","2e49af45":"data_boxcox_diff.dropna(inplace=True)","fe9587c0":"adf_test = adfuller(data_boxcox_diff)\n\nprint('ADF Statistic: %f' % adf_test[0])\nprint('Critical Values @ 0.05: %.2f' % adf_test[4]['5%'])\nprint('p-value: %f' % adf_test[1])","bf48b4d2":"from statsmodels.graphics.tsaplots import plot_acf\nplt.figure(figsize=(12,4))\nplot_acf(data_boxcox_diff, ax=plt.gca(), lags = 30)\nplt.show()","f076f739":"from statsmodels.graphics.tsaplots import plot_pacf\nplt.figure(figsize=(12,4))\nplot_pacf(data_boxcox_diff, ax=plt.gca(), lags = 30)\nplt.show()","d23b9b6a":"# Train-test split (after box-cox)\n\ntrain_data_boxcox = data_boxcox[:train_len]\ntest_data_boxcox = data_boxcox[train_len:]","cfc30f82":"from statsmodels.tsa.statespace.sarimax import SARIMAX\n\nmodel = SARIMAX(train_data_boxcox, order=(1, 2, 1), seasonal_order=(0, 0, 0, 0)) \nmodel_fit = model.fit()\n# print(model_fit.params)\n\n\n# Recover Original Series\ny_hat_arima = test_data_boxcox.copy()\ny_hat_arima['arima_forecast_boxcox'] = model_fit.predict(test_data_boxcox.index.min(), test_data_boxcox.index.max())\ny_hat_arima['arima_forecast'] = np.exp(y_hat_arima['arima_forecast_boxcox'])\n\n\n# Plotting\nplt.figure(figsize=(12,4))\nplt.plot(train['Price'], label='Train')\nplt.plot(test['Price'], label='Test')\nplt.plot(y_hat_arima['arima_forecast'][test.index.min():], label='ARIMA forecast')\nplt.legend(loc='best')\nplt.title('Autoregressive integrated moving average (ARIMA) method')\nplt.show()\n\n\n# Calculate RMSE and MAPE\nrmse = np.sqrt(mean_squared_error(test['Price'], y_hat_arima['arima_forecast'][test.index.min():])).round(2)\nmape = np.round(np.mean(np.abs(test['Price']-y_hat_arima['arima_forecast'][test.index.min():])\/test['Price'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Autoregressive integrated moving average (ARIMA) method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","773a5339":"model = SARIMAX(train_data_boxcox, order=(1, 2, 1), seasonal_order=(1, 0, 1, 7)) \nmodel_fit = model.fit()\n# print(model_fit.params)\n\n\n# Recover Original Series\ny_hat_sarima = test_data_boxcox.copy()\ny_hat_sarima['sarima_forecast_boxcox'] = model_fit.predict(test_data_boxcox.index.min(), test_data_boxcox.index.max())\ny_hat_sarima['sarima_forecast'] = np.exp(y_hat_sarima['sarima_forecast_boxcox'])\n\n\n# Plotting\nplt.figure(figsize=(12,4))\nplt.plot(train['Price'], label='Train')\nplt.plot(test['Price'], label='Test')\nplt.plot(y_hat_sarima['sarima_forecast'][test.index.min():], label='SARIMA forecast')\nplt.legend(loc='best')\nplt.title('Seasonal autoregressive integrated moving average (SARIMA) method')\nplt.show()\n\n\n# Calculate RMSE and MAPE\nrmse = np.sqrt(mean_squared_error(test['Price'], y_hat_sarima['sarima_forecast'][test.index.min():])).round(2)\nmape = np.round(np.mean(np.abs(test['Price']-y_hat_sarima['sarima_forecast'][test.index.min():])\/test['Price'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Seasonal autoregressive integrated moving average (SARIMA) method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","fcffe8c5":"# data_1 = data.reset_index().drop('key_0', axis=1)\n# data_1.head()","06f210fd":"# train_data_1 = data_1[:train_len]\n# test_data_1 = data_1[train_len:]","a1c813c6":"# from sklearn.preprocessing import MinMaxScaler\n\n# scaler = MinMaxScaler()\n\n# train_scaled = scaler.fit_transform(train_data_1)\n# test_scaled = scaler.transform(test_data_1)","567fabc1":"# # Create input dataset\n# def create_dataset (X, look_back = 1):\n#     Xs, ys = [], []\n \n#     for i in range(len(X)-look_back):\n#         v = X[i:i+look_back]\n#         Xs.append(v)\n#         ys.append(X[i+look_back])\n \n#     return np.array(Xs), np.array(ys)\n\n\n# LOOK_BACK = 30 # Monthly look back","c3965b2e":"# X_train, y_train = create_dataset(train_scaled,LOOK_BACK)\n# X_test, y_test = create_dataset(test_scaled,LOOK_BACK)\n\n# # Print data shape\n# print('X_train.shape:', X_train.shape)\n# print('y_train.shape:', y_train.shape)\n# print('X_test.shape:', X_test.shape) \n# print('y_test.shape:', y_test.shape)","b46cb007":"# import tensorflow as tf\n# from tensorflow import keras\n# from tensorflow.keras import Sequential, layers, callbacks\n# from tensorflow.keras.layers import Dense, Dropout, GRU\n\n# model = Sequential()\n# model.add(GRU (units = 64, return_sequences = True, input_shape = [X_train.shape[1], X_train.shape[2]]))\n# model.add(Dropout(0.2)) \n# model.add(GRU(units = 64)) \n# model.add(Dropout(0.2))\n# model.add(Dense(units = 1)) \n\n# model.compile(optimizer='adam',loss='mse')\n# model.summary()","e3674e29":"# early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss',\n#                                            patience = 10)\n\n# history = model.fit(X_train, y_train, epochs = 100,  \n#                     validation_split = 0.2,\n#                     batch_size = 16, \n#                     shuffle = False, \n#                     callbacks = [early_stop])\n","abccfcb7":"# # Ploting results\n\n# loss = history.history['loss']\n# val_loss = history.history['val_loss']\n\n# epochs_range = range(len(loss))\n\n# plt.plot(epochs_range, loss, label='Training Loss')\n# plt.plot(epochs_range, val_loss, label='Validation Loss')\n# plt.legend(loc='upper right')\n# plt.title('Training and Validation Loss')\n# plt.show()","792005b4":"# y_test_ = scaler.inverse_transform(y_test)\n# y_train_ = scaler.inverse_transform(y_train)","e76c4c82":"# # Make prediction\n# prediction = model.predict(X_test)\n# prediction = scaler.inverse_transform(prediction)\n\n# prediction_train = model.predict(X_train)\n# prediction_train = scaler.inverse_transform(prediction_train)\n\n\n# # Plot test data vs prediction\n\n# plt.figure(figsize=(10, 6))\n# range_future = len(prediction)\n# # plt.plot(np.arange(range_future), np.array(y_test), label='Test data')\n# plt.plot(np.concatenate((y_train_, y_test_), axis=0), label='Actual')\n# plt.plot(np.concatenate((prediction_train, prediction), axis=0), label='Prediction_GRU')\n# plt.legend(loc='upper left')\n","166de906":"# rmse = np.sqrt(mean_squared_error(y_test_, prediction)).round(2)\n# mape = np.round(np.mean(np.abs(y_test_ - prediction)\/y_test_)*100,2)\n\n# tempResults = pd.DataFrame({'Method':['Deep Learning: GRU'], 'RMSE': [rmse],'MAPE': [mape] })\n# results = pd.concat([results, tempResults])\n# results = results[['Method', 'RMSE', 'MAPE']]\n# results","d94c1e49":"# inputs = keras.Input(shape=(X_train.shape[1], X_train.shape[2]))\n# x = GRU(units = 64, return_sequences = True)(inputs)\n# x = Dropout(0.2)(x)\n# output_tensor = tf.keras.layers.MultiHeadAttention(num_heads=1, key_dim=64, dropout=0.2, output_shape=1)(x, x, return_attention_scores=False)\n# x = tf.squeeze(output_tensor, axis=-1)\n# outputs = Dense(units = 1)(x)\n# model = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n# model.compile(optimizer='adam',loss='mse')\n# model.summary()","65455d5c":"# early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss',\n#                                            patience = 10)\n\n# history = model.fit(X_train, y_train, epochs = 100,  \n#                     validation_split = 0.2,\n#                     batch_size = 16, \n#                     shuffle = False, \n#                     callbacks = [early_stop])","9c39f863":"# # Ploting results\n\n# loss = history.history['loss']\n# val_loss = history.history['val_loss']\n\n# epochs_range = range(len(loss))\n\n# plt.plot(epochs_range, loss, label='Training Loss')\n# plt.plot(epochs_range, val_loss, label='Validation Loss')\n# plt.legend(loc='upper right')\n# plt.title('Training and Validation Loss')\n# plt.show()","b5bf47a6":"# # Make prediction\n# prediction = model.predict(X_test)\n# prediction = scaler.inverse_transform(prediction)\n\n# prediction_train = model.predict(X_train)\n# prediction_train = scaler.inverse_transform(prediction_train)\n\n\n# # Plot test data vs prediction\n\n# plt.figure(figsize=(10, 6))\n# range_future = len(prediction)\n# # plt.plot(np.arange(range_future), np.array(y_test), label='Test data')\n# plt.plot(np.concatenate((y_train_, y_test_), axis=0), label='Actual')\n# plt.plot(np.concatenate((prediction_train, prediction), axis=0), label='Prediction_GRU')\n# plt.legend(loc='upper left')","9e3eea2f":"# rmse = np.sqrt(mean_squared_error(y_test_, prediction)).round(2)\n# mape = np.round(np.mean(np.abs(y_test_ - prediction)\/y_test_)*100,2)\n\n# tempResults = pd.DataFrame({'Method':['Deep Learning: GRU + Attention'], 'RMSE': [rmse],'MAPE': [mape]})\n# results = pd.concat([results, tempResults])\n# results = results[['Method', 'RMSE', 'MAPE']]\n# results","7f210e7f":"## Autocorrelation function (ACF)","c34450a6":"## 4.6. Holt Winter's multiplicative method with trend and seasonality","7f7456c3":"### Differencing to remove trend","dbe63e8c":"## Partial autocorrelation function (PACF)","a15b7531":"Observations,\n\n- Series differencing is required to remove the trend.\n- Auto-correlation suggests 4\/5th order dependency.","6d079da6":"## 4.1. Naive Method","fa67aada":"## 4.5. Holt Winters' additive method with trend and seasonality","a2eae53f":"There are no outliers present as per `box plot` and `histogram`.","b8cb955e":"## 4.4. Holt's Method with Trend","a67cce29":"## Comparing Different Time Series Forecasting Methods","e302922e":"## 4.3. Simple Moving Average Method","4e0dab8f":"### Box Cox transformation to make variance constant","f395f66a":"## Outlier detection","7492ee18":"## 4.8. Seasonal auto regressive integrated moving average (SARIMA)","eafb5775":"# 1. Reading Dataset","999073b2":"Conclusion:  \n\n- `Holt's` methods give similar performance output as `S+AR+I+MA` methods.\n- `Deep learning` based methods are dominating in terms of performance with increased computational cost.","7029cbf1":"# 4. Time Series Forecasting","ec5bf736":"## 4.10. Deep Learning: GRU + Attention","cd8e211e":"# 3. Time Series Analysis","e899a68f":"### Augmented Dickey-Fuller (ADF) test","a9ccf3bc":"## 4.2. Simple Average Method","33a3c0ec":"# 2. Pre-processing Dataset","20403122":"## 4.7. Auto regressive integrated moving average (ARIMA)","429aa5f0":"-----------","4fca7d31":"# Time series forecasting: Gold Price ","26dcba5e":"### Augmented Dickey-Fuller (ADF) test","fc722e3c":"## 4.9. Deep Learning: GRU"}}