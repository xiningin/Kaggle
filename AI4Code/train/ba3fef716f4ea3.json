{"cell_type":{"9068f2b9":"code","b2e48c45":"code","a2cb4fab":"code","ff50b120":"code","1298e05a":"code","d25ff7f4":"code","03a15be3":"code","cae959db":"code","f936c865":"code","b467e1bd":"code","9ec2f00a":"code","6c032a89":"code","459445aa":"code","6cd2cf83":"code","2e09469b":"code","1f55a8ba":"code","b9d8aad0":"code","7dfdae21":"code","f80fe027":"code","5c48840d":"code","1e427e88":"code","dc49923e":"code","e7c2b93b":"code","0ac1d03f":"code","771b0e72":"code","7732a379":"markdown","200c2ead":"markdown","a0b9b0a9":"markdown","8d6c737d":"markdown","95a0dad7":"markdown","379cf3a3":"markdown","12f4a32a":"markdown","63f0a5b4":"markdown","5bec8750":"markdown","1b706cf8":"markdown"},"source":{"9068f2b9":"n_hidden_layers = 1 # number of hidden layers\n# hidden_dim = 32 # dimensions of hidden layers\nl1 = 0.0 # penalty on output\nl2 = 0.1 # weights l2 regularization parameter\ndropout = 0.025 # dropout parameter [0,1]\nbatch_size = 128 # batch_size\nepochs = 100 # number of epochs\nshallow = True","b2e48c45":"import os\nimport time\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom tqdm.notebook import tqdm\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\nfrom torch.utils.data import TensorDataset, DataLoader","a2cb4fab":"df = pd.read_csv('..\/input\/cmsnewsamples\/new-smaples.csv').drop(columns = 'Unnamed: 0')\ndf = df.drop(columns = [i for i in df.columns if '_1' in i])\ndf['non_hits'] = df[[i for i in df.columns if 'mask' in i]].sum(axis=1)\ndf = df[df['non_hits']==0].reset_index(drop=True)\n\ndf['1\/pT'] = df['q\/pt'].abs()\n\nfeatures = ['emtf_phi_'+str(i) for i in [0,2,3,4]] + ['emtf_theta_'+str(i) for i in [0,2,3,4]] + ['old_emtf_phi_'+str(i) for i in [0,2,3,4]]\n\nnew_features = []\nfor i in range(len(features)-1):\n    for j in range(i+1, (i\/\/4+1)*4):\n        new_features.append('delta_'+'_'.join(features[i].split('_')[:-1])+'_'+str((j)%4)+'_'+str(i%4))\n        df[new_features[-1]]=df[features[j]]-df[features[i]]\n\nfeatures = new_features[:]\n\nfeatures+=['fr_0', 'fr_2', 'fr_3', 'fr_4']\n\nlabels_1 = ['1\/pT']\n\nscaler_1 = MinMaxScaler()\ndf[features] = scaler_1.fit_transform(df[features])\n\ndf_features = df[features].copy()\n\nfeature_hidden_dim = [min(1000, 2*len(df[features[i]].unique())) for i in range(len(features))]","ff50b120":"print(feature_hidden_dim)","1298e05a":"df[features+labels_1].head()","d25ff7f4":"class ExU(torch.nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ExU, self).__init__()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        self.weight = Parameter(torch.Tensor(out_dim, in_dim))\n        self.bias = Parameter(torch.Tensor(in_dim))\n        self.reset_parameters()\n        \n    def reset_parameters(self):\n        self.weight = torch.nn.init.normal_(self.weight, mean=3.5, std=0.5)\n        self.bias = torch.nn.init.normal_(self.bias, mean=3.5, std=0.5)\n        \n    def forward(self, inp):\n        output = inp-self.bias\n        output = output.matmul(torch.exp(self.weight.t()))\n#         output = output.matmul(self.weight.t())\n        output = F.relu(output)\n        \n        return output","03a15be3":"class NAM(torch.nn.Module):\n    def __init__(self, in_dim, n_hidden_layers, feature_hidden_dim ,dropout = 0, shallow=True):\n        super(NAM, self).__init__()\n        self.dropout = dropout\n        self.model = []\n        for i in range(in_dim):\n            if n_hidden_layers==0:\n                layers = [ExU(1, 1)]\n            else:\n                layers = [ExU(1, feature_hidden_dim[i]), torch.nn.Dropout(self.dropout)]\n                if not shallow:\n                    layers+=[torch.nn.Linear(feature_hidden_dim[i], 64), torch.nn.Dropout(self.dropout)]\n                    layers+=[torch.nn.Linear(64, 32), torch.nn.Dropout(self.dropout)]\n                    layers+=[torch.nn.Linear(32, 1, bias=False)]\n                if shallow:\n                    layers+=[ExU(feature_hidden_dim[i], 1)]\n            self.model.append(torch.nn.Sequential(*layers))\n            \n        self.model = torch.nn.ModuleList(self.model)\n\n        self.in_dim = in_dim\n        self.n_hidden_layers = n_hidden_layers\n        self.feature_hidden_dim = feature_hidden_dim\n        \n        self.summation_params = []\n        for i in range(in_dim + 1):\n            self.summation_params.append(torch.nn.init.normal_(Parameter(torch.Tensor(1)), mean=0.5, std=0.5))\n        self.summation_params = torch.nn.ParameterList(self.summation_params)\n            \n    def forward(self, x):\n        \n        output = self.summation_params[0]*self.model[0](x[:,0].reshape(-1,1))\n        for i in range(1,self.in_dim):\n            output += self.summation_params[i]*self.model[i](x[:,i].reshape(-1,1))\n        output += self.summation_params[self.in_dim]\n        \n#         output = torch.cat([self.summation_params[i]*self.model[i](x[:,i].reshape(-1,1)) for i in range(self.in_dim)], axis=1)\n# #         partial_output = output.detach().cpu().numpy()\n#         output = output.sum(axis=1)\n#         output += self.summation_params[self.in_dim]\n    \n        return output","cae959db":"def criterion(outputs, labels, weights, l1=0):\n    loss0 = torch.sqrt(torch.mean((labels-outputs)**2))\n    loss1 = torch.sqrt(torch.mean(outputs**2))\n    \n    return loss0+loss1*l1","f936c865":"def train_nam(model, X_train, Y_train, X_test, Y_test, l1, l2, fold=0, epochs=50, batch_size=128, results_path='.\/', progress_bar=False):\n    \n    test_index = list(X_test.index)\n    X_val = torch.Tensor(X_train.reset_index(drop=True).iloc[:int(len(X_train)*0.1)].to_numpy())\n    Y_val = torch.Tensor(Y_train.reset_index(drop=True).iloc[:int(len(Y_train)*0.1)].to_numpy())\n    X_train = torch.Tensor(X_train.reset_index(drop=True).iloc[int(len(X_train)*0.1):].reset_index(drop=True).to_numpy())\n    Y_train = torch.Tensor(Y_train.reset_index(drop=True).iloc[int(len(Y_train)*0.1):].reset_index(drop=True).to_numpy())\n    X_test = torch.Tensor(X_test.reset_index(drop=True).to_numpy())\n    Y_test = torch.Tensor(Y_test.reset_index(drop=True).to_numpy())\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    \n    train_loader = DataLoader(TensorDataset(X_train, Y_train), batch_size=batch_size, shuffle=True, num_workers = 4) \n    val_loader = DataLoader(TensorDataset(X_val, Y_val), batch_size=batch_size) \n    test_loader = DataLoader(TensorDataset(X_test, Y_test), batch_size=batch_size)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=l2)\n    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=2, factor=0.2)\n    \n    l1=torch.tensor(l1)\n    \n    m_train_loss = []\n    m_val_loss = []\n    m_test_loss = []\n    min_val_loss = float('inf')\n    \n    for epoch in range(epochs):\n      train_loss = 0\n      val_loss = 0\n      if progress_bar:\n          pbar = tqdm(train_loader)\n      else:\n          pbar = train_loader\n      for data in pbar:\n        optimizer.zero_grad()\n        outputs = model(data[0].to(device))\n        labels = data[1].to(device)\n        loss = criterion(outputs, labels, model.parameters(), l1)\n        l2 = criterion(outputs, labels, model.parameters())\n        loss.backward()\n        optimizer.step()\n        if progress_bar:\n          pbar.set_description('Loss: '+str(l2.detach().cpu().numpy()))\n        train_loss += l2.detach().cpu().numpy()\/len(train_loader)\n\n      for data in val_loader:\n        optimizer.zero_grad()\n        outputs = model(data[0].to(device))\n        labels = data[1].to(device)\n        loss = criterion(outputs, labels, model.parameters())\n        val_loss += loss.detach().cpu().numpy()\/len(val_loader)\n      if val_loss<min_val_loss:\n        min_val_loss = val_loss\n        torch.save(model.state_dict(), 'model.pth')\n      lr_scheduler.step(val_loss)\n      print('Epoch: ', str(epoch+1)+'\/'+str(epochs),'| Training Loss: ', train_loss, '| Validation Loss: ', val_loss)\n      m_train_loss.append(train_loss)\n      m_val_loss.append(val_loss)\n\n    model.load_state_dict(torch.load('model.pth'))\n    test_loss = 0\n    true = []\n    preds = []\n    \n    for data in test_loader:\n      optimizer.zero_grad()\n      outputs = model(data[0].to(device))\n      labels = data[1].to(device)\n      true += list(labels.detach().cpu().numpy().flatten())\n      preds += list(outputs.detach().cpu().numpy().flatten())\n      loss = criterion(outputs, labels, model.parameters()).detach().cpu().numpy()\n      test_loss += loss\/len(test_loader)\n    \n    print('Test Loss: ', test_loss)\n    \n    OOF_preds = pd.DataFrame()\n    OOF_preds['true_value'] = true\n    OOF_preds['preds'] = preds\n    OOF_preds['row'] = test_index\n    OOF_preds.to_csv(os.path.join(results_path, 'OOF_preds_'+str(fold)+'.csv'), index=False)\n    \n    return m_train_loss, m_val_loss","b467e1bd":"df = df.sample(frac=1, random_state=242).reset_index(drop=True)","9ec2f00a":"model = NAM(len(features), n_hidden_layers, feature_hidden_dim, dropout)\nX_train = df[features].iloc[:int(len(df)*0.8)]\nX_test = df[features].iloc[int(len(df)*0.8):]\nY_train = df[labels_1].iloc[:int(len(df)*0.8)]\nY_test = df[labels_1].iloc[int(len(df)*0.8):]","6c032a89":"m_train_loss, m_val_loss = train_nam(model, X_train, Y_train, X_test, Y_test, l1, l2, batch_size=batch_size, epochs=epochs)","459445aa":"files = os.listdir('\/kaggle\/working')\ndf = pd.concat([pd.read_csv('\/kaggle\/working\/'+i) for i in files if 'OOF_preds_' in i])\ndf.to_csv('OOF_preds.csv')","6cd2cf83":"df = pd.read_csv('OOF_preds.csv').drop(columns = ['Unnamed: 0'])\ndf = df.sort_values(by = 'row').reset_index(drop = True)\ndf['True_pT'] = 1\/df['true_value']\ndf['Predicted_pT'] = 1\/df['preds']","2e09469b":"df_fcnn = pd.read_csv('..\/input\/1-pt-regression-swiss-activation-new-data\/OOF_preds.csv').drop(columns = ['Unnamed: 0'])\ndf_fcnn = df_fcnn.sort_values(by = 'row').reset_index(drop = True)\ndf_fcnn['True_pT'] = 1\/df_fcnn['true_value']\ndf_fcnn['Predicted_pT'] = 1\/df_fcnn['preds']","1f55a8ba":"from sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import mean_absolute_error as mae\n\ndef MAE(df):\n    MAE1 = []\n    dx = 0.5\n    for i in range(int(2\/dx),int(150\/dx)):\n        P = df[(df['True_pT']>=(i-1)*dx)&(df['True_pT']<=(i+1)*dx)]\n        try:\n            p = mae(P['True_pT'],P['Predicted_pT'])\n        except:\n            p=0\n        MAE1.append(p)\n    MAE1 = MAE1[:196]\n    return MAE1","b9d8aad0":"dx = 0.5\nMAE1 = MAE(df)\nplt.plot([i*dx for i in range(4,200)],MAE1,label = 'NAM')\nplt.plot([i*dx for i in range(4,200)],MAE(df_fcnn),label = 'FCNN')\nplt.xlabel('pT -->')\nplt.ylabel('MAE -->')\nplt.legend()\nplt.show()","7dfdae21":"class NAM(torch.nn.Module):\n    def __init__(self, in_dim, n_hidden_layers, feature_hidden_dim ,dropout = 0, shallow=True):\n        super(NAM, self).__init__()\n        self.dropout = dropout\n        self.model = []\n        for i in range(in_dim):\n            if n_hidden_layers==0:\n                layers = [ExU(1, 1)]\n            else:\n                layers = [ExU(1, feature_hidden_dim[i]), torch.nn.Dropout(self.dropout)]\n                if not shallow:\n                    layers+=[torch.nn.Linear(feature_hidden_dim[i], 64), torch.nn.Dropout(self.dropout)]\n                    layers+=[torch.nn.Linear(64, 32), torch.nn.Dropout(self.dropout)]\n                    layers+=[torch.nn.Linear(32, 1, bias=False)]\n                if shallow:\n                    layers+=[ExU(feature_hidden_dim[i], 1)]\n            self.model.append(torch.nn.Sequential(*layers))\n            \n        self.model = torch.nn.ModuleList(self.model)\n\n        self.in_dim = in_dim\n        self.n_hidden_layers = n_hidden_layers\n        self.feature_hidden_dim = feature_hidden_dim\n        \n        self.summation_params = []\n        for i in range(in_dim + 1):\n            self.summation_params.append(torch.nn.init.normal_(Parameter(torch.Tensor(1)), mean=0.5, std=0.5))\n        self.summation_params = torch.nn.ParameterList(self.summation_params)\n            \n    def forward(self, x):\n        \n        output = torch.cat([self.summation_params[i]*self.model[i](x[:,i].reshape(-1,1)) for i in range(self.in_dim)], axis=1)\n        partial_output = output.detach().cpu().numpy()\n        output = output.sum(axis=1)\n        output += self.summation_params[self.in_dim]\n        \n        return output, partial_output","f80fe027":"max_values = df_features.max().to_numpy()\nmin_values = df_features.min().to_numpy()","5c48840d":"model = NAM(len(features), n_hidden_layers, feature_hidden_dim, dropout)\nmodel.load_state_dict(torch.load('model.pth'))\ninput_to_model = torch.Tensor([[ min_values[j]+(max_values[j]-min_values[j])*i\/1000 for i in range(1000)] for j in range(len(features))]).t()","1e427e88":"# list(model.parameters())","dc49923e":"_, partial_output = model(input_to_model)","e7c2b93b":"partial_output.shape","0ac1d03f":"for i in range(len(features)):\n    plt.plot([min_values[i]+(max_values[i]-min_values[i])*j\/1000 for j in range(1000)], partial_output[:,i].flatten())\n    plt.title('Partial_dependence__of_1\/pT_on_'+features[i])\n    plt.xlabel(features[i])\n    plt.ylabel('Componenet in predicted pT')\n    plt.show()","771b0e72":"for i in range(len(features)):\n    plt.plot([min_values[i]+(max_values[i]-min_values[i])*j\/1000 for j in range(1000)], 1\/partial_output[:,i].flatten())\n    plt.title('Partial_dependence__of_pT_on_'+features[i])\n    plt.xlabel(features[i])\n    plt.ylabel('Componenet in predicted pT')\n    plt.show()","7732a379":"# ExU layer","200c2ead":"# Partial Dependence Visualization","a0b9b0a9":"# NAM","8d6c737d":"# Training Fn","95a0dad7":"# Experiment","379cf3a3":"# Results","12f4a32a":"# Imports","63f0a5b4":"# Read & Pre-process","5bec8750":"# Parameters","1b706cf8":"# Loss function"}}