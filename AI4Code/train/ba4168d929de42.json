{"cell_type":{"f84bbe88":"code","cb78d659":"code","d60279bb":"code","8a77a053":"code","e384bc9b":"code","26ac6778":"code","4ca2fb49":"code","59b0a9ba":"code","d99b4625":"code","df614cf2":"code","de27a662":"code","8d0522aa":"code","0009db3c":"code","71b5d646":"code","208fba8b":"code","d5e0cddc":"code","b14b00b0":"code","223f69d6":"code","8a7cdd0b":"code","b0ccb390":"code","326705e9":"code","44b9f76f":"code","c5d2c692":"code","b2d58421":"code","e298016b":"code","9b6739b3":"code","40d13404":"code","b5ecd372":"code","ebde3acf":"code","f86bee2a":"code","83b30c80":"code","c5c0896b":"code","f88b0508":"code","f762f4e1":"code","ba1dfbcf":"code","0fca83fb":"code","18e1de36":"code","68ac9fb7":"code","a2ce6210":"code","8e62be09":"code","2193dfd3":"code","1d753a8b":"code","40dd8cb5":"code","171b3825":"code","3526ed65":"code","dff485b2":"code","b40fba1c":"code","d7576710":"code","14cf9468":"code","d94db4ab":"code","8e8a9e39":"code","06e20caa":"code","31942ad7":"code","e050b80c":"code","8b0bba38":"code","a1b61949":"code","a3347ab6":"markdown","3f483b9e":"markdown","0185ff4c":"markdown","7d2ed8ed":"markdown","bece50a8":"markdown","7b30779c":"markdown","3ddf95d9":"markdown","f9d683ea":"markdown","572e0737":"markdown","3c829ada":"markdown","69b61cdf":"markdown","3be53713":"markdown","207bec18":"markdown","76d70803":"markdown","32dc7b3d":"markdown","32c9b1a7":"markdown","ad4e900a":"markdown"},"source":{"f84bbe88":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cb78d659":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d60279bb":"data = pd.read_csv(r'\/kaggle\/input\/covid19s-impact-on-airport-traffic\/covid_impact_on_airport_traffic.csv')","8a77a053":"data.head(2)","e384bc9b":"data.tail(2)","26ac6778":"data.shape","4ca2fb49":"data.info()","59b0a9ba":"data['Date'] = data['Date'].astype('datetime64[ns]')","d99b4625":"data.describe(include='all')","df614cf2":"data.duplicated().sum() #No duplicates are present.","de27a662":"data = data.drop(columns = ['AggregationMethod','Version','Centroid','ISO_3166_2','Geography'])","8d0522aa":"data.head()","0009db3c":"plt.figure(figsize=(20,6))\nfig1 = sns.countplot(x = 'AirportName', data = data , palette='rainbow_r')\nfig1.set_xticklabels(fig1.get_xticklabels(), rotation=90)\nfig1.set_title(\"Count for various Airports\")\nplt.show();\n# This shows that there are around 250 data points on each airport except Santiago International Airport and Edmonton International","71b5d646":"plt.figure(figsize=(20,6))\nfig2 = sns.countplot(x = 'City', data = data , palette='viridis')\nfig2.set_xticklabels(fig2.get_xticklabels(), rotation=90)\nfig2.set_title(\"Count for various City\")\nplt.show();\n# This shows that all cities have more or less equal counts in data except New York. The most likely reason would be that it has more airports.","208fba8b":"plt.figure(figsize=(20,6))\nfig3 = sns.countplot(x = 'State', data = data , palette='cividis')\nfig3.set_xticklabels(fig3.get_xticklabels(), rotation=90)\nfig3.set_title(\"Count for various State\")\nplt.show();\n# Here, all states have equal counts in data except Alberta,Quebec,California and New York. Again most likely reason must be the number of airports. We'll come again on this in in-depth analysis of countries.","d5e0cddc":"plt.figure(figsize=(8,4))\nfig4 = sns.countplot(x = 'Country', data = data , palette='summer')\nfig4.set_xticklabels(fig4.get_xticklabels())\nfig4.set_title(\"Count for various Country\")\nplt.show();\n# Maximum data points are for US followed by Canada. This is because the number of airports in US and Canada are probably more than Australia and Chile.","b14b00b0":"data.groupby(\"Country\")[['State','City','AirportName']].nunique()","223f69d6":"sns.kdeplot(data['PercentOfBaseline'],shade=True)\nplt.title(\"Distribution of Percent of Baseline for full data\")\nplt.show();","8a7cdd0b":"fig5 = sns.pairplot(data,hue='Country',height=5,palette='husl',aspect=1)\nfig5._legend.remove()\nplt.title(\"Distribution of Percent of Baseline for different countries\")\nplt.legend(loc = 'upper right',bbox_to_anchor=(1.2, 0., 0.5, 0.5))\nplt.show();","b0ccb390":"data_chile = data[data['Country']=='Chile']","326705e9":"data_chile.head()","44b9f76f":"data_chile.info()","c5d2c692":"data_chile.nunique()","b2d58421":"data_chile = data_chile.sort_values(by=\"Date\")","e298016b":"data_chile.set_index('Date',inplace=True)","9b6739b3":"data_chile.head()","40d13404":"data_chile = data_chile.drop(columns=['AirportName','City','State','Country'])","b5ecd372":"data_chile.head()","ebde3acf":"plt.figure(figsize=(20,10))\nplt.plot(data_chile['PercentOfBaseline'])\nplt.title(\"Plot for PercentOfBaseline Vs Time for Chile\")\nplt.show();","f86bee2a":"from statsmodels.tsa.stattools import adfuller\nprint('Results of Dickey-Fuller Test:')\ndftest = adfuller(data_chile['PercentOfBaseline'], autolag='AIC')\ndfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\nfor key,value in dftest[4].items():\n    dfoutput['Critical Value (%s)'%key] = value\nprint(dfoutput)","83b30c80":"#define function for kpss test\nfrom statsmodels.tsa.stattools import kpss\n#define KPSS\nprint ('Results of KPSS Test:')\nkpsstest = kpss(data_chile['PercentOfBaseline'], regression='c')\nkpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic','p-value','Lags Used'])\nfor key,value in kpsstest[3].items():\n    kpss_output['Critical Value (%s)'%key] = value\nprint (kpss_output)","c5c0896b":"data_chile['diff'] = data_chile['PercentOfBaseline'] - data_chile['PercentOfBaseline'].shift(1)","f88b0508":"plt.figure(figsize=(20,10))\nplt.plot(data_chile['diff'])\nplt.title(\"Plot for lagged PercentOfBaseline Vs Time for Chile\")\nplt.show();\n","f762f4e1":"from statsmodels.tsa.stattools import adfuller\nprint('Results of Dickey-Fuller Test:')\ndftest = adfuller(data_chile['diff'].dropna(), autolag='AIC')\ndfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\nfor key,value in dftest[4].items():\n    dfoutput['Critical Value (%s)'%key] = value\nprint(dfoutput)","ba1dfbcf":"#define function for kpss test\nfrom statsmodels.tsa.stattools import kpss\n#define KPSS\nprint ('Results of KPSS Test:')\nkpsstest = kpss(data_chile['diff'].dropna(), regression='c')\nkpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic','p-value','Lags Used'])\nfor key,value in kpsstest[3].items():\n    kpss_output['Critical Value (%s)'%key] = value\nprint (kpss_output)","0fca83fb":"from statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(x=data_chile['PercentOfBaseline'].dropna(),model='multiplicative',period=9)\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\nplt.figure(figsize=(10,10))\nplt.subplot(411)\nplt.plot(data_chile['PercentOfBaseline'], label='Original')\nplt.legend(loc='best')\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality')\nplt.legend(loc='best')\nplt.subplot(414)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\nplt.tight_layout();","18e1de36":"from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\nplot_acf(data_chile['diff'].dropna(),zero=False)\nplt.xlim(0,20)\nplt.xticks(np.arange(0,20,1))\nplt.show(); #q=1 or q=0","68ac9fb7":"plot_pacf(data_chile['diff'].dropna(),zero=False,lags=40,method='ols',alpha=0.05)\nplt.xticks(np.arange(0,40,2))\nplt.show(); # p=3,5","a2ce6210":"df = pd.DataFrame(data_chile['diff'])","8e62be09":"df.dropna(inplace=True)","2193dfd3":"from statsmodels.tsa.arima_model import ARMA","1d753a8b":"# Create Training and Test\ntrain = df.iloc[:212]\ntest = df.iloc[212:]\n# Build Model\nmodel = ARMA(train, order=(6,0))  \nfitted = model.fit()  \nprint(fitted.summary())\n\n# Forecast\nfc, se, conf = fitted.forecast(25, alpha=0.05);  # 95% conf","40dd8cb5":"# Make as pandas series\nfc_series = pd.Series(fc, index=test.index)\nlower_series = pd.Series(conf[:, 0], index=test.index)\nupper_series = pd.Series(conf[:, 1], index=test.index)\n\n# Plot\nplt.figure(figsize=(12,5), dpi=100)\nplt.plot(train, label='training')\nplt.plot(test, label='actual',color='r')\nplt.plot(fc_series, label='forecast',color='g')\nplt.fill_between(lower_series.index, lower_series, upper_series,color='g', alpha=.05)\nplt.title('Forecast vs Actuals')\nplt.legend(loc='best', fontsize=8)\nplt.show()\n","171b3825":"data_US = data[data['Country']=='United States of America (the)']","3526ed65":"data_US.shape","dff485b2":"df1 = pd.DataFrame(data_US.groupby('Date',as_index=True)['PercentOfBaseline'].mean())","b40fba1c":"df1.head()","d7576710":"plt.figure(figsize=(20,10))\nplt.plot(df1)\nplt.title(\"Plot of USA's average PercentOfBaseline Vs Time\")\nplt.show()","14cf9468":"from statsmodels.tsa.stattools import adfuller\nprint('Results of Dickey-Fuller Test:')\ndftest = adfuller(df1['PercentOfBaseline'], autolag='AIC')\ndfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\nfor key,value in dftest[4].items():\n    dfoutput['Critical Value (%s)'%key] = value\nprint(dfoutput)","d94db4ab":"#define function for kpss test\nfrom statsmodels.tsa.stattools import kpss\n#define KPSS\nprint ('Results of KPSS Test:')\nkpsstest = kpss(df1['PercentOfBaseline'].dropna(), regression='c')\nkpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic','p-value','Lags Used'])\nfor key,value in kpsstest[3].items():\n    kpss_output['Critical Value (%s)'%key] = value\nprint (kpss_output)","8e8a9e39":"from statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(x=df1['PercentOfBaseline'],model='multiplicative',period=9)\n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\nplt.figure(figsize=(10,10))\nplt.subplot(411)\nplt.plot(df1['PercentOfBaseline'], label='Original')\nplt.legend(loc='best')\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality')\nplt.legend(loc='best')\nplt.subplot(414)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\nplt.tight_layout()","06e20caa":"from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\nplot_acf(df1['PercentOfBaseline'],zero=False)\nplt.xlim(0,10)\nplt.xticks(np.arange(0,10,1))\nplt.show() #q=2","31942ad7":"plot_pacf(df1['PercentOfBaseline'],lags=20,zero=False)\nplt.xticks(np.arange(0,20,1))\nplt.show() # p=1","e050b80c":"from statsmodels.tsa.arima_model import ARMA","8b0bba38":"# Create Training and Test\nsize = int(len(df1['PercentOfBaseline'])*0.8)\ntrain = df1['PercentOfBaseline'].iloc[:size]\ntest = df1['PercentOfBaseline'].iloc[size:]\n# Build Model\nmodel = ARMA(train, order=(1,2))  \nfitted = model.fit() \nprint(fitted.summary())\n\n# Forecast\nfc, se, conf = fitted.forecast(len(test), alpha=0.05)  # 95% conf\n\n# Make as pandas series\nfc_series = pd.Series(fc, index=test.index)\nlower_series = pd.Series(conf[:, 0], index=test.index)\nupper_series = pd.Series(conf[:, 1], index=test.index)","a1b61949":"# Plot\nplt.figure(figsize=(12,5), dpi=100)\nplt.plot(train, label='training')\nplt.plot(test, label='actual',color='r')\nplt.plot(fc_series, label='forecast',color='g')\nplt.fill_between(lower_series.index, lower_series, upper_series,color='g', alpha=.05)\nplt.title('Forecast vs Actuals')\nplt.legend(loc='best', fontsize=8)\nplt.show()","a3347ab6":"# Analysis For CHILE","3f483b9e":"# Bivariate Analysis","0185ff4c":"Dickey-Fuller Test: This is one of the statistical tests for checking stationarity. Here the null hypothesis is that the TS is non-stationary. The test results comprise of a Test Statistic and some Critical Values for difference confidence levels. If the \u2018Test Statistic\u2019 is less than the \u2018Critical Value\u2019, we can reject the null hypothesis and say that the series is stationary.\n\nWe can conclude that our data is not stationary, hence, we need to make it stationary because all timeseries models are stationary.","7d2ed8ed":"# Analysis for USA","bece50a8":"We reject null hypothesis and say series has become stationary now.","7b30779c":"# Getting and Knowing Data","3ddf95d9":"## Removing features that aren't important\n\nThe geography is a polygon feature meaning it resembles shape. So, we can conclude that geography tells us the shape of an airport. Since, we don't require it for our analysis. We'll drop it.\n\nSame reasoning applies for centroid. Centroid probably tells us the latitude and longitude of the centre of the airport. Since, we don't have any use for that. We'll drop it too.\n\nISO_3166_2 is some unique value for every state. We won't be requiring it for time series analysis.\n\nAggregationMethod is always 'Daily', so, it doesn't provide any information. We can remove it.\n\nNo information on version is provided. So, we'll leave that from our analysis.","f9d683ea":"## Distribution of PercentOfBaseline","572e0737":"# Importing Libraries","3c829ada":"# Univariate Analysis","69b61cdf":"# Hope you all liked it!\n# I'd appreciate if you'll leave comments below for any suggestions.","3be53713":"The authors of the KPSS test have defined the null hypothesis as the process is trend stationary, to an alternate hypothesis of a unit root series.\n\nKPSS test also suggests that our series is NOT stationary.\nhttps:\/\/www.analyticsvidhya.com\/blog\/2018\/09\/non-stationary-time-series-python\/\n\nPlease see this to understand the warning https:\/\/stats.stackexchange.com\/questions\/286124\/kpss-test-in-python-with-statsmodels\/286167","207bec18":"Data on Chile is based on only one state which has one city with one airport named Santiago International airport","76d70803":"There are 4 countries, 23 states,27 cities and 28 airports in the dataset. Data has been collected for 262 distinct days starting from 16\/03\/20 to 02\/12\/20.\n\nCentroid and Country are POINT and POLYGON structures, telling us that they are geographical locations.\n\nAlso, from count we can see, that all features have no missing value.","32dc7b3d":"The warning is given letting the user know that the index is not a date\/time index. It won't have any effect on predictions.","32c9b1a7":"We accept reject hypothesis and say that series is stationary.","ad4e900a":"## Now, we have made the series to be strictly stationary, so we move onto modelling. Since, there is no seasonal component, we can use ARIMA Model. Let's first find out value of p, d and q"}}