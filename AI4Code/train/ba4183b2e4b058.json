{"cell_type":{"2c17b3e4":"code","1fb2e6fa":"code","fb0c35c1":"code","27f951bd":"code","bfcb0d81":"code","69cb541a":"code","99c9c98e":"code","1600338d":"code","32c18ed7":"code","6e08f1e2":"code","cf4b5ce3":"code","cc894c6d":"code","9b213c23":"code","872481f2":"code","1aa8af03":"code","55c03517":"code","34ea06d3":"code","40bda0cd":"code","fb826602":"code","3febdf9f":"code","e6854496":"code","3073d62b":"code","f7fa8cfa":"code","fa665183":"code","b893dba2":"code","f77ac927":"code","3a2d75c3":"code","945909be":"code","f4ec828d":"code","8b7641d6":"code","f4e82afb":"code","2bc62070":"markdown","470b641e":"markdown","27a57670":"markdown","1b0a4976":"markdown","7388fd48":"markdown","9abace86":"markdown","f809910b":"markdown","f0c177a2":"markdown","d76b144c":"markdown","3a87acf7":"markdown","c90895c7":"markdown","7a26d535":"markdown","529499f2":"markdown","e02d965e":"markdown","fea10ffa":"markdown","e430de2f":"markdown","1230c32a":"markdown","0ddc78b4":"markdown","b2f36253":"markdown","0301bb7b":"markdown","86940906":"markdown","fc65668c":"markdown"},"source":{"2c17b3e4":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# import spacy\nimport string\nfrom collections import Counter\n\nimport os\nimport shutil\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras.layers import LSTM, GRU, Dense, Embedding, Dropout\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow import keras\nimport kerastuner as kt\n","1fb2e6fa":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fb0c35c1":"df = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ndf_topredict = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\ndf_sample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')","27f951bd":"df.head()","bfcb0d81":"# df.apply(lambda row: row.name, axis=1)\ndf['text_length'] = df.apply(lambda row: len(df.loc[row.name, 'excerpt'].split()), axis=1)","69cb541a":"def maxword_len(row_idx):\n    words = df.loc[row_idx, 'excerpt'].split()\n    max_len = len(max(words, key=len))\n    return max_len","99c9c98e":"df['maxword_length'] = df.index.map(lambda row_idx: maxword_len(row_idx))","1600338d":"def maxsent_len(row_idx):\n    paragraph = df.loc[row_idx, 'excerpt']\n    num_words = [len(sentence.split()) for sentence in paragraph.split('.')]\n    return max(num_words)","32c18ed7":"df['maxsent_length'] = df.index.map(lambda row_idx: maxsent_len(row_idx))","6e08f1e2":"df['ntext_length'] = (df.text_length - df.text_length.min()) \/ (df.text_length.max()-df.text_length.min())\ndf['nmaxword_length'] = (df.maxword_length - df.maxword_length.min()) \/ (df.maxword_length.max()-df.maxword_length.min())\ndf['nmaxsent_length'] = (df.maxsent_length - df.maxsent_length.min()) \/ (df.maxsent_length.max()-df.maxsent_length.min())","cf4b5ce3":"df.head()","cc894c6d":"plt.figure(figsize=(10,4))\n\nplt.subplot(1,2,1)\nplt.hist(x=df.target, color='tab:cyan', bins=40, edgecolor='k')\nplt.xlabel('Text Difficulty Score(target)')\nplt.ylabel('Count')\nplt.title('Distribution of Target Score')\n\nplt.subplot(1,2,2)\nplt.hist(x=df.standard_error, color='tab:purple',bins=40, edgecolor='k')\nplt.xlabel('Standard Error')\nplt.ylabel('Count')\nplt.title('Distribution of Error')\n\nplt.tight_layout()\nplt.show()","9b213c23":"# set the display to show more text\npd.options.display.max_colwidth = 100\n\n# print out the text to exam the difference between high score and low score\nmin_target = df.loc[df.target==df.target.min(),['excerpt','target','text_length','maxword_length','maxsent_length']]\nprint('Min Target Score:',min_target.target, '-'*20, 'TEXT', '-'*20)\nprint(f'\"{min_target.excerpt}\"')\nprint(f'Text Length: {min_target.text_length}')\nprint(f'Longest Word Length: {min_target.maxword_length}')\nprint(f'Longest Sentence Length: {min_target.maxsent_length}')\n\nprint()\n\nmax_target = df.loc[df.target==df.target.max(),['excerpt','target','text_length','maxword_length','maxsent_length']]\nprint('Max Target Score:',max_target.target, '-'*20, 'TEXT', '-'*20)\nprint(f'\"{max_target.excerpt}\"')\nprint(f'Text Length: {max_target.text_length}')\nprint(f'Longest Word Length: {max_target.maxword_length}')\nprint(f'Longest Sentence Length: {max_target.maxsent_length}')\n\n# shorten the text display\npd.options.display.max_colwidth = 50\n","872481f2":"corr_list = ['text_length','maxword_length','maxsent_length']\n\nsns.set_theme(style=\"white\", color_codes=True)\n\nplt.figure(figsize=(15,5))\nfor i in range(len(corr_list)):\n    plt.subplot(1,3,i+1)\n    sns.regplot(x=df[corr_list[i]] ,y=df.target, marker='+')\n    \nplt.show()","1aa8af03":"SEED = 5","55c03517":"dataset = tf.data.Dataset.from_tensor_slices(\n            (tf.cast(df['excerpt'].values, tf.string),\n             tf.cast(df['target'].values, tf.float16)))\ndataset.shuffle(SEED)\n\nprint(dataset)","34ea06d3":"# Let's print out an instance in the dataset\nfor example, label in dataset.take(1):\n    print('Text: ', example.numpy(), sep='\\n')\n    print()\n    print('Label: ', label.numpy(),sep='\\n')","40bda0cd":"TRAIN_SIZE = int(len(dataset)*0.7)\n\ntrain_dataset = dataset.take(TRAIN_SIZE)\ntest_dataset = dataset.skip(TRAIN_SIZE) ","fb826602":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 4\n\ntrain_dataset = train_dataset.cache().batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_dataset.cache().batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n","3febdf9f":"total_words = df['excerpt'].str.split()\ntotal_word_set = set()\ntotal_words.apply(total_word_set.update)\ncount_dict = Counter(total_word_set)\nVOCAB_SIZE = len(count_dict)\n\nprint('total unique words number:', VOCAB_SIZE)","e6854496":"encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n    max_tokens=VOCAB_SIZE)\nencoder.adapt(train_dataset.map(lambda text, label: text))","3073d62b":"embedding_layer = Embedding(\n    input_dim=len(encoder.get_vocabulary()),\n    output_dim=128,\n    mask_zero=True\n    )","f7fa8cfa":"def model_builder(hp):\n    model = Sequential()\n    model.add(encoder)\n    model.add(embedding_layer)\n    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)))\n    \n    hp_units = hp.Int('units', min_value=96, max_value=512, step=32)\n    model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n    model.add(tf.keras.layers.Dropout(0.5))\n    \n    model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n    model.add(tf.keras.layers.Dropout(0.5))\n    \n    model.add(tf.keras.layers.Dense(1))\n    \n    hp_learning_rate = hp.Choice('learning_rate', values = [1e-4, 1e-5])\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n                  loss=tf.keras.losses.MeanSquaredError())\n    \n    return model","fa665183":"tuner = kt.Hyperband(model_builder,\n                     objective='val_loss',\n                     max_epochs=10,\n                     factor=3)","b893dba2":"stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\ntuner.search(train_dataset, validation_data=test_dataset,\n             epochs=30, callbacks=[stop_early])","f77ac927":"# Display the optimal hyperparameters\nbest_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n\nprint(f\"\"\"\nThe hyperparameter search is complete... \nThe optimal number of neurons in the dense layers is {best_hps.get('units')};\nThe optimal learning rate is {best_hps.get('learning_rate')}.\n\"\"\")","3a2d75c3":"model = tuner.hypermodel.build(best_hps)\nhistory = model.fit(train_dataset, epochs=50,\n                    validation_data=test_dataset, \n                    callbacks = [stop_early])\n\nval_loss_per_epoch = history.history['val_loss']\nbest_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1\nprint(f'Best epoch: {best_epoch}')","945909be":"hypermodel = tuner.hypermodel.build(best_hps)\n\n# Retrain the model\nhypermodel.fit(train_dataset, validation_data=test_dataset,\n             epochs=best_epoch, callbacks=[stop_early])","f4ec828d":"dataset = dataset.cache().batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\nhypermodel.fit(dataset, epochs=best_epoch)","8b7641d6":"def make_prediction(row_idx):\n    result = hypermodel.predict(np.array([df_topredict.excerpt[row_idx]]))\n    return result[0][0]\n\ndf_topredict['target'] = df_topredict.index.map(lambda row_idx: make_prediction(row_idx))","f4e82afb":"df_sub = df_topredict.loc[:, ['id','target']]\ndf_sub.to_csv('submission.csv', index=False)","2bc62070":"# Project Introduction\n\n**Project Goal:** Using machine learning to identify the appropriate reading level of a passage of text for grades 3-12 students.\n\n**Data:** \n- train.csv size: 2834 rows, 6 columns:\n    - id\n    - url_legal\n    - license\n    - excerpt (feature)\n    - target (** dependent variable)\n    - standard_error\n- test.csv size: 7 rows, 4 columns\n\n#### In this project, I did not use any pre-trained models. Therefore, there is no need to turn on the Internet toggle in the kernel in order to download anything.","470b641e":"## Add length of the longest sentence in the text as a column","27a57670":"## Add a longest word in the text as a column","1b0a4976":"# Feature Engineering\n- Add some columns\n    - text length\n    - length of the longest word in the text\n    - length of the longest sentence in the text","7388fd48":"## Tuning the train, test dataset to feed into tensorflow","9abace86":"### Build the model with the optimal hyperparameters and train it on the data for 50 epochs","f809910b":"## Add a text_length column","f0c177a2":"## Normalize the 3 created columns using min\/max normalization\nformula:  (df-df.min())\/(df.max()-df.min())","d76b144c":"# Re-train the model with the entire dataset","3a87acf7":"### Instantiate the tuner and perform hypertuning","c90895c7":"## Visualize the relationship between target and \n- text_length\n- maxword_length\n- maxsent_length","7a26d535":"### Take a look at the engineered df:","529499f2":"## Create tokenize(encoder) and vectorize(embedding) layers","e02d965e":"# EDA \n\n## Check the target and standard_error distribution:","fea10ffa":"### Re-instantiate the hypermodel and train it with the optimal number of epochs from above.","e430de2f":"# Tune the LSTM Model\n### Define the model","1230c32a":"# Loading Libraries","0ddc78b4":"# Loading Datasets into Memory","b2f36253":"## Check the text with lowest and highest target score:","0301bb7b":"# Make Prediction","86940906":"## Train test split","fc65668c":"# RNN Model\n## Preprocessing: convert the dataframe into tf dataset"}}