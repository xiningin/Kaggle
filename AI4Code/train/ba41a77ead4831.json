{"cell_type":{"9ed86144":"code","4380f189":"code","d24053ce":"code","6bf7f86f":"code","6cbbce5d":"code","8b69d8f9":"code","de0868a6":"code","66b2f874":"code","d376d766":"code","edd818e2":"code","b08fe7ed":"code","c3b24cea":"code","811e0c3d":"markdown","ec2d8160":"markdown","c2ce79f9":"markdown","210f672c":"markdown","aff29b24":"markdown","cb886b98":"markdown","062ac1f3":"markdown","bf07777e":"markdown","52aa3ad4":"markdown","efa0b172":"markdown"},"source":{"9ed86144":"import pandas as pd\nimport numpy as np\nimport math\nimport os\nimport plotly.graph_objects as go\nfrom keras.models import Sequential\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom keras.layers import Dense\nfrom keras.layers import LSTM","4380f189":"os.chdir(\"\/kaggle\/input\/wallmart-sales\")","d24053ce":"df = pd.read_csv(\"single_item.csv\")\ndf = df.drop(['Unnamed: 0'], axis = 1)\ndf.head()","6bf7f86f":"dataset = pd.DataFrame(df.iloc[:,0])\nscaler = MinMaxScaler(feature_range=(0, 1))\ndataset = scaler.fit_transform(dataset)","6cbbce5d":"test_size = 30\ntrain_size = int(len(df) - test_size)\ntrain, test  = dataset[:train_size,0],dataset[train_size:len(dataset),0] ","8b69d8f9":"def create_dataset(dataset, time_step=7):\n    dataX, dataY = [], []\n    m = len(dataset)\n    for i in range(time_step, m):\n        dataX.append(dataset[i-time_step:i])\n        dataY.append(dataset[i])\n    return np.array(dataX), np.array(dataY)\n\ntime_step = 7\ntrainX, trainY = create_dataset(train, time_step)\ntestX, testY = create_dataset(test, time_step)\n\ntrainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\ntestX = np.reshape(testX, (testX.shape[0], testX.shape[1],1))","de0868a6":"model = Sequential()\nmodel.add(LSTM(4, input_shape=(time_step, 1)))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(trainX, trainY, epochs=5, batch_size=1, verbose=2)","66b2f874":"testPredict = model.predict(testX)\n\ntestPredict = scaler.inverse_transform(testPredict)\ntestY = scaler.inverse_transform([testY])\n\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","d376d766":"testPredictPlot = np.empty_like(dataset)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(dataset)-len(testPredict):, :] = testPredict\n\nData = pd.DataFrame(scaler.inverse_transform(dataset))\nData_test = pd.DataFrame(testPredictPlot)","edd818e2":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=df.date, y=Data.iloc[:,0], name=\"Dataset\",\n                         line_color='deepskyblue'))\nfig.add_trace(go.Scatter(x=df.date,y=Data_test.iloc[:,0], name=\"Dataset test\",\n                         line_color='red'))\nfig.show()","b08fe7ed":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=df['date'].iloc[-23:,], y=Data.iloc[-23:,0], name=\"Dataset\",\n                         line_color='deepskyblue'))\nfig.add_trace(go.Scatter(x=df['date'].iloc[-23:,],y=Data_test.iloc[-23:,0], name=\"Dataset test\",\n                         line_color='red'))\nfig.show()","c3b24cea":"pred = pd.concat([Data,Data_test], axis =1)\npred = pred[-23:]\npred.columns = ['Actual sales', 'Predicted sales']\npred.head()","811e0c3d":"## Creating and training the model","ec2d8160":"## Making predictions on test data","c2ce79f9":"### Sales predictions from 2016-04-02 to 2016-04-24","210f672c":"The model could not predict the spike(almost 9 times) in sales on April 17th which happened beacuse of \tOrthodox Easter. Even though the \tOrthodox Easter was on 20th April, people purchased this particular item on 17th. Now the challenge is to incorporate these event related spikes in our predictions. ","aff29b24":"## Scaling the data","cb886b98":"## Splitting data into test and train","062ac1f3":"## Importing libraries","bf07777e":"## Importing data","52aa3ad4":"## Making dependant and independant faetures","efa0b172":"# Single item sales prediction"}}