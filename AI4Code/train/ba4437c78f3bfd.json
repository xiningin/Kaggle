{"cell_type":{"2ae1b6ff":"code","e1620674":"code","3c0712b7":"code","dcf94211":"code","b8e2e3d3":"code","48233b7d":"code","e425e25e":"code","aae0a5c3":"code","b57924e9":"code","47cb0f5c":"code","f76e94cc":"code","fe68d73d":"code","5011ffb7":"code","a43f6bae":"code","2cb194ca":"code","e0507d86":"code","e4a2d03c":"code","3d44e96d":"code","bb95e3f4":"code","98cdd68c":"code","eaa58c91":"code","4a6f401f":"code","987284aa":"markdown","4dbcd269":"markdown","029a0d81":"markdown","f004ac84":"markdown","8f0216fd":"markdown","1ac7f8c1":"markdown","1523999b":"markdown","a3edf889":"markdown","8684142b":"markdown","d7d926d2":"markdown","929dc4e2":"markdown","709fbb53":"markdown"},"source":{"2ae1b6ff":"!nvidia-smi","e1620674":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport re","3c0712b7":"data = pd.read_csv('..\/input\/all-trumps-twitter-insults-20152021\/trump_insult_tweets_2014_to_2021.csv')\ndata.head()","dcf94211":"tweets = data['tweet'].copy()\ndel data","b8e2e3d3":"from transformers import GPT2Tokenizer\n\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2', \n                                          bos_token='<|sos|>', \n                                          eos_token='<|eos|>', \n                                          pad_token='<|pad|>')\n\n\ntokenizer.encode(\"Sample Text\")","48233b7d":"max_tweet = max([len(tokenizer.encode(tweet)) for tweet in tweets])\n\nprint(f'The longest tweet is {max_tweet} tokens long.')","e425e25e":"batch_size = 32\n\nimport torch\nfrom torch.utils.data import Dataset\n\nclass TweetDataset(Dataset):\n    def __init__(self,tweets,tokenizer,gpt2_type=\"gpt2\",max_length=max_tweet):\n        self.tokenizer = tokenizer\n        self.input_ids = []\n        self.attention_masks = []\n        \n        for tweet in tweets:\n            encoding_dict = tokenizer('<|sos|>'+ tweet +'<|eos|>',truncation=True,\n                                     max_length=max_length,\n                                     padding='max_length')\n            \n            self.input_ids.append(torch.tensor(encoding_dict['input_ids']))\n            self.attention_masks.append(torch.tensor(encoding_dict['attention_mask']))\n        \n    def __len__(self):\n        return len(self.input_ids)\n    \n    def __getitem__(self,idx):\n        return self.input_ids[idx], self.attention_masks[idx]","aae0a5c3":"from torch.utils.data import random_split\ndataset = TweetDataset(tweets,tokenizer,max_length=max_tweet)","b57924e9":"train_size = int(0.9 * len(dataset))\nval_size = len(dataset)-train_size\n\ntrain,val = random_split(dataset,[train_size,val_size])\nprint(f'No of train samples = {train_size} and Number of validation samples = {val_size}')","47cb0f5c":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\ntrain_dataloader = DataLoader(train,sampler = RandomSampler(train),\n                             batch_size = batch_size)\n\nval_dataloader = DataLoader(val,sampler = SequentialSampler(val),\n                           batch_size = batch_size)","f76e94cc":"import random\nfrom transformers import GPT2LMHeadModel, GPT2Config","fe68d73d":"configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\nmodel.resize_token_embeddings(len(tokenizer))\n\ndevice = torch.device(\"cuda\")\nmodel.cuda()\n\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","5011ffb7":"# the warmup steps are steps at the start of training that are ignored\n# every x steps we will sample the model to test the output\n\nepochs = 6\nwarmup_steps = 1e2\nsample_every = 100","a43f6bae":"from transformers import AdamW\n\noptimizer = AdamW(model.parameters(),\n                  lr = 5e-4,\n                  eps = 1e-8\n                )","2cb194ca":"from transformers import get_linear_schedule_with_warmup\n\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = warmup_steps, \n                                            num_training_steps = total_steps)","e0507d86":"import random\nimport time\nimport datetime\n\ndef format_time(elapsed):\n    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n\ntotal_t0 = time.time()\n\ntraining_stats = []\n\nmodel = model.to(device)\n\nfor epoch_i in range(0, epochs):\n\n    print(f'Beginning epoch {epoch_i + 1} of {epochs}')\n\n    t0 = time.time()\n\n    total_train_loss = 0\n\n    model.train()\n\n    for step, batch in enumerate(train_dataloader):\n\n        b_input_ids = batch[0].to(device)\n        b_labels = batch[0].to(device)\n        b_masks = batch[1].to(device)\n\n        model.zero_grad()        \n\n        outputs = model(  b_input_ids,\n                          labels=b_labels, \n                          attention_mask = b_masks,\n                          token_type_ids=None\n                        )\n\n        loss = outputs[0]  \n\n        batch_loss = loss.item()\n        total_train_loss += batch_loss\n\n        # Get sample every 100 batches.\n        if step % sample_every == 0 and not step == 0:\n\n            elapsed = format_time(time.time() - t0)\n            print(f'Batch {step} of {len(train_dataloader)}. Loss:{batch_loss}. Time:{elapsed}')\n\n            model.eval()\n\n            sample_outputs = model.generate(\n                                    bos_token_id=random.randint(1,30000),\n                                    do_sample=True,   \n                                    top_k=50, \n                                    max_length = 200,\n                                    top_p=0.95, \n                                    num_return_sequences=1\n                                )\n            for i, sample_output in enumerate(sample_outputs):\n                  print(f'Example output: {tokenizer.decode(sample_output, skip_special_tokens=True)}')\n            \n            model.train()\n\n        loss.backward()\n\n        optimizer.step()\n\n        scheduler.step()\n\n    # Calculate the average loss over all of the batches.\n    avg_train_loss = total_train_loss \/ len(train_dataloader)       \n    \n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n\n    print(f'Average Training Loss: {avg_train_loss}. Epoch time: {training_time}')\n\n    t0 = time.time()\n\n    model.eval()\n\n    total_eval_loss = 0\n    nb_eval_steps = 0\n\n    # Evaluate data for one epoch\n    for batch in val_dataloader:\n        \n        b_input_ids = batch[0].to(device)\n        b_labels = batch[0].to(device)\n        b_masks = batch[1].to(device)\n        \n        with torch.no_grad():        \n\n            outputs  = model(b_input_ids,  \n                             attention_mask = b_masks,\n                             labels=b_labels)\n          \n            loss = outputs[0]  \n            \n        batch_loss = loss.item()\n        total_eval_loss += batch_loss        \n\n    avg_val_loss = total_eval_loss \/ len(val_dataloader)\n    \n    validation_time = format_time(time.time() - t0)    \n\n    print(f'Validation loss: {avg_val_loss}. Validation Time: {validation_time}')\n\n    # Record all statistics from this epoch.\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n\nprint(f'Total training took {format_time(time.time()-total_t0)}')","e4a2d03c":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\npd.set_option('precision', 2)\ndf_stats = pd.DataFrame(data=training_stats)\ndf_stats = df_stats.set_index('epoch')\n\nsns.set(style='darkgrid')\n\nsns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nplt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\nplt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n\nplt.title(\"Training & Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nxt = [i for i in range(1,epochs+1)]\nplt.xticks(xt)\n\nplt.show()","3d44e96d":"import os\nout_dir = '\/TrumpTweet'\nmodel_to_save = model.module if hasattr(model, 'module') else model\nmodel_to_save.save_pretrained('.\/')\ntokenizer.save_pretrained('.\/')","bb95e3f4":"model.eval()\n\nprompt = \"<|sos|>\"\n\ngenerated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\ngenerated = generated.to(device)\n\nsample_outputs = model.generate(\n                                generated, \n                                do_sample=True,   \n                                top_k=50, \n                                max_length = 300,\n                                top_p=0.95, \n                                num_return_sequences=5\n                                )\n\nfor i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))","98cdd68c":"#Load Saved Model\n\nmodel = GPT2LMHeadModel.from_pretrained('.\/')\ntokenizer = GPT2Tokenizer.from_pretrained('.\/')\nmodel.to(device)\n","eaa58c91":"tweet_2012 = 'it makes me feel so good to hit sleazebags back much better than seeing a psychiatrist which i never have'","4a6f401f":"model.eval()\n\nprompt = tweet_2012\n\ngenerated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\ngenerated = generated.to(device)\n\nsample_outputs = model.generate(\n                                generated, \n                                do_sample=True,   \n                                top_k=50, \n                                max_length = 300,\n                                top_p=0.95, \n                                num_return_sequences=5\n                                )\n\nfor i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))","987284aa":"**Part two of my fun project to train a model over tweets of Donald Trump and try to create new tweets. As a start, I tried out simple LSTM model and done some experimentations with that by applying\/removing embedding layers, increasing hidden units, adding more layers but model had a tendency to repeat text after some time. So as a conclusion for this small project In this notebook I try to use Transformers to generate new tweets.**\n\n![download.jpg](attachment:download.jpg)\n\nNote: As at the time of of this notebook, I am only learning transformers so please correct me in the comments if you have some better suggestion.","4dbcd269":"**Let's see how well our model performs**","029a0d81":"You can have a look at the [LSTM Notebook](https:\/\/www.kaggle.com\/ajay19\/trump-tweet-generation).","f004ac84":"**Let's use a tweet from Trump's account from 2012.**","8f0216fd":"**I loaded the dataset and then only used the tweet column for text generation**","1ac7f8c1":"**We can clearly see how well Transformers perform when compared to LSTM model which produced something like this:**\n\n![lstm_output.jpg](attachment:lstm_output.jpg)","1523999b":"**Here I added three new tokens in the pre-trained GPT2 tokenizer: \\\n<|sos|> : start of sentence \\\n<|eos|> : end of sentence \\\n<|pad|> : padding token**","a3edf889":"**Next we create a custom dataloader for our tweets using torch Dataset. \\\nEach entry in the dataset will be two tensors, one which is the encoding for the string and one which is the attention mask**","8684142b":"**Total training steps is the number of data points, times the number of epochs. \nEssentially, epochs are training cycles, how many times each point will be seen by the model.**\n\n**We can set a variable learning rate which will help scan larger areas of the \nproblem space at higher LR earlier, then fine tune to find the exact model minima \nat lower LR later in training.**","d7d926d2":"**We use 90% dataset for training and 10% for validation.**","929dc4e2":"**Algorithm for AdamW optimizer** \\\n![AdamW.png](attachment:AdamW.png)","709fbb53":"**Next I will try this model on some other famous personalities on twitter.**"}}