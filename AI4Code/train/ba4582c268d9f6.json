{"cell_type":{"c1fd055d":"code","d162d277":"code","04cbcbf3":"code","cfcca84e":"code","5eb3fc2f":"code","b761d42a":"code","33c0ec47":"code","070a9510":"code","9cb5d0dc":"code","09deb013":"code","48470f8a":"code","a6161e8c":"code","30e3296d":"code","7590980e":"code","3d070971":"code","73e39e42":"code","77e0e8d8":"code","f57668bf":"code","edda160f":"code","4aff3a86":"code","48d67abd":"code","664d730a":"code","f4e38b5c":"code","7b94f2b8":"code","e1095cdd":"code","bc61744f":"code","6d8ac2d8":"code","6aaa3b63":"code","025308c1":"code","b4f78690":"code","2149c532":"code","99583a7b":"code","72a2a9cc":"code","e7959a84":"code","6f91e282":"code","3628b547":"code","097c3e13":"code","db048c8c":"markdown","f498abd5":"markdown"},"source":{"c1fd055d":"!pip install tensorflow-text==2.6.0","d162d277":"!pip install -q tf-models-official==2.6.0","04cbcbf3":"# !pip freeze","cfcca84e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nfrom official.nlp import optimization  # to create AdamW optimizer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# from official.nlp import optimization  # to create AdamW optmizer\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import accuracy_score\n\ntf.get_logger().setLevel('ERROR')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5eb3fc2f":"val_data = pd.read_csv('\/kaggle\/input\/friends-classification\/val_data.csv', index_col=0)\ntrain_data = pd.read_csv('\/kaggle\/input\/friends-classification\/train_data.csv', index_col=0)\ntest = pd.read_csv('\/kaggle\/input\/friends-classification\/test.csv', index_col=0)","b761d42a":"val_data.shape, train_data.shape, test.shape","33c0ec47":"train, valid = train_test_split(\n    train_data,\n    train_size=0.7,\n    random_state=0,\n    stratify=train_data['Category'])\n\ny_train, X_train = \\\n    train['Category'], train.drop(['Category'], axis=1)\ny_valid, X_valid = \\\n    valid['Category'], valid.drop(['Category'], axis=1)","070a9510":"y_train_c = tf.keras.utils.to_categorical(\n    y_train.astype('category').cat.codes.values, num_classes=6)\ny_valid_c = tf.keras.utils.to_categorical(\n    y_valid.astype('category').cat.codes.values, num_classes=6)","9cb5d0dc":"X_train.fillna('', inplace=True)\nval_data['other_speaker'] = ''","09deb013":"tfhub_handle_encoder = \\\n    \"https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-512_A-8\/1\"\ntfhub_handle_preprocess = \\\n    \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3\"","48470f8a":"EPOCHS = 30\nSAVE_PERIOD = 10\nBATCH_SIZE = 128\nSTEPS_PER_EPOCH_ = len(y_train_c) \/ BATCH_SIZE","a6161e8c":"epochs = 30\n# steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\nsteps_per_epoch = len(y_train_c)\nnum_train_steps = steps_per_epoch * epochs\nnum_warmup_steps = int(0.1*num_train_steps)\n\ninit_lr = 3e-5\noptimizer = optimization.create_optimizer(init_lr=init_lr,\n                                          num_train_steps=num_train_steps,\n                                          num_warmup_steps=num_warmup_steps,\n                                          optimizer_type='adamw')","30e3296d":"# Set up epochs and steps\n# epochs = 3\n# batch_size = 32\n# eval_batch_size = 32\n\n# train_data_size = len(glue_train_labels)\n# steps_per_epoch = int(train_data_size \/ batch_size)\n# num_train_steps = steps_per_epoch * epochs\n# warmup_steps = int(epochs * train_data_size * 0.1 \/ batch_size)\n\n# creates an optimizer with learning rate schedule\n# optimizer = nlp.optimization.create_optimizer(\n#     2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)","7590980e":"# string join\ndef build_classifier_model():\n\n  input1 = tf.keras.layers.Input(\n      shape=(), dtype=tf.string, name='text')\n\n  input2 = tf.keras.layers.Input(\n      shape=(), dtype=tf.string, name='text1')\n  text_input = tf.strings.join([input1, input2])\n\n  preprocessing_layer = hub.KerasLayer(\n      tfhub_handle_preprocess, name='preprocessing')\n\n  encoder_inputs = preprocessing_layer(text_input)\n  encoder = hub.KerasLayer(\n      tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n\n  outputs = encoder(encoder_inputs)\n  net = outputs['pooled_output']\n  net = tf.keras.layers.Dropout(0.1)(net)\n  output = tf.keras.layers.Dense(\n      6, activation='softmax', name='classifier')(net)\n  model = tf.keras.Model([input1, input2], output)\n\n  loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False) # (from_logits=True)\n  metric = tf.metrics.CategoricalAccuracy('accuracy')\n  optimizer = Adam(\n      learning_rate=5e-05, epsilon=1e-08, decay=0.01, clipnorm=1.0)\n  model.compile(\n      optimizer=optimizer, loss=loss, metrics=metric)\n  model.summary()\n  return model","3d070971":"# enc output concat\ndef build_classifier_model_cat():\n\n  input1 = tf.keras.layers.Input(\n      shape=(), dtype=tf.string, name='text')\n\n  input2 = tf.keras.layers.Input(\n      shape=(), dtype=tf.string, name='text1')\n\n  preprocessing_layer = hub.KerasLayer(\n      tfhub_handle_preprocess, name='preprocessing')\n\n  encoder_input1 = preprocessing_layer(input1)\n  encoder_input2 = preprocessing_layer(input2)\n  encoder = hub.KerasLayer(\n      tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n\n  output1 = encoder(encoder_input1)\n  output2 = encoder(encoder_input2)\n\n  net = tf.keras.layers.Concatenate(axis=-1)([output1['pooled_output'], output2['pooled_output']])\n  net = tf.keras.layers.Dropout(0.1)(net)\n  output = tf.keras.layers.Dense(\n      6, activation='softmax', name='classifier')(net)\n  model = tf.keras.Model([input1, input2], output)\n\n  loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False) # (from_logits=True)\n  metric = tf.metrics.CategoricalAccuracy('accuracy')\n  optimizer = Adam(\n      learning_rate=5e-05, epsilon=1e-08, decay=0.01, clipnorm=1.0)\n  model.compile(\n      optimizer=optimizer, loss=loss, metrics=metric)\n  model.summary()\n  return model","73e39e42":"classifier_model = build_classifier_model()","77e0e8d8":"classifier_model_cat = build_classifier_model_cat()","f57668bf":"# Create a callback that saves the model's weights every 10 epochs\ncheckpoint_path = '.\/'\ncp_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_path, \n    verbose=1, \n    save_weights_only=True,\n    save_freq= int(SAVE_PERIOD * STEPS_PER_EPOCH_))","edda160f":"history = classifier_model.fit(\n    x=[X_train['other_speaker'].values, X_train['friend_response'].values],\n    y=y_train_c,\n    validation_data=([X_valid['other_speaker'].values, X_valid['friend_response'].values], y_valid_c),\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n#     steps_per_epoch=steps_per_epoch,\n    callbacks=[cp_callback],\n)","4aff3a86":"history1 = classifier_model_cat.fit(\n    x=[X_train['other_speaker'].values, X_train['friend_response'].values],\n    y=y_train_c,\n    validation_data=([X_valid['other_speaker'].values, X_valid['friend_response'].values], y_valid_c),\n    epochs=4,\n    batch_size=BATCH_SIZE,\n#     steps_per_epoch=STEPS_PER_EPOCH,\n    callbacks=[cp_callback],\n)","48d67abd":"# classifier_model.save('partly_trained.h5') \nclassifier_model_cat.save('model_cat_final.h5') ","664d730a":"!ls","f4e38b5c":"from keras.models import load_model\nclassifier_model_cat = tf.keras.models.load_model('.\/model_cat.h5',custom_objects={'KerasLayer':hub.KerasLayer})","7b94f2b8":"classifier_model_cat = tf.keras.experimental.load_from_saved_model('.\/model_cat.h5', custom_objects={'KerasLayer':hub.KerasLayer})","e1095cdd":"history = history1","bc61744f":"sns.set()\nsns.set_context(\"paper\", font_scale=2) \n\nfig = plt.figure(figsize=(20,10))\nax1 = fig.add_subplot(121)\nsns.lineplot(ax=ax1, data=history.history['accuracy'])\nsns.lineplot(ax=ax1, data=history.history['val_accuracy'])\nax1.set(\n    title=f\"Model accuracy\",\n    xlabel=\"epoch\",\n    ylabel=\"accuracy\"\n)\nax1.legend(['train', 'test'], loc='upper left')\nax2 = fig.add_subplot(122)\nsns.lineplot(ax=ax2, data=history.history['loss'])\nsns.lineplot(ax=ax2, data=history.history['val_loss'])\nax2.set(\n    title=f\"Model loss\",\n    xlabel=\"epoch\",\n    ylabel=\"loss\"\n)\nax2.legend(['train', 'test'], loc='upper left')\n\nfig.tight_layout()\nplt.show(fig) ","6d8ac2d8":"y_test_c = tf.keras.utils.to_categorical(\n    val_data['label'].astype('category').cat.codes.values, num_classes=6)\n\ne = classifier_model.evaluate(x=[val_data['other_speaker'].values ,val_data['friend_response'].values], y=y_test_c)","6aaa3b63":"print(f\"BERT Accuracy: {e[1]}\")","025308c1":"y_proba_bert = classifier_model.predict(val_data['friend_response'].values)\ny_pred_bert = np.argmax(y_proba_bert, axis=1)","b4f78690":"cat_dict = dict( enumerate(val_data['label'].astype('category').cat.categories ) )","2149c532":"y_true = val_data['label'].astype('category').cat.codes.values\ntarget_names = ['1', '2', '3', '4', '5', '6']","99583a7b":"#Confusion Matrix\ncf_matrix = confusion_matrix(y_true, y_pred_bert)\nsns.heatmap(\n    cf_matrix\/np.sum(cf_matrix),\n    annot=True, fmt='.2%', cmap='Blues',\n    xticklabels=target_names,\n    yticklabels=target_names)","72a2a9cc":"#Classification Report for BERT\ntarget_names = ['1', '2', '3', '4', '5', '6']\nprint(classification_report(y_true, y_pred_bert, target_names=target_names))","e7959a84":"# y_proba_bert_ = classifier_model.predict(test['friend_response'].values)\ny_proba_bert_ = classifier_model_cat.predict([test['other_speaker'].values, test['friend_response'].values])\ny_pred_bert_ = np.argmax(y_proba_bert_, axis=1)","6f91e282":"predict_submit = [cat_dict[item] for item in y_pred_bert_]","3628b547":"# predict_submit = model.predict(test)\nsubmit = test.copy()\nsubmit['Category'] = predict_submit\nsubmit.drop(columns=['friend_response', 'other_speaker'], inplace=True)\nsubmit.to_csv('submit.csv')\nprint('')","097c3e13":"submit","db048c8c":"### submit","f498abd5":"### Model"}}