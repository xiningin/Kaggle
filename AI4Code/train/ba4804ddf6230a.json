{"cell_type":{"68959317":"code","9aee3825":"code","8ca8bc1b":"code","4f1586d6":"code","fc73ab78":"code","bdeb9a8d":"code","1bfc850a":"code","8e83971a":"code","8c4f26d2":"code","4f440335":"code","a763d562":"code","07ea57a2":"code","cce0b60c":"code","72b9ccbc":"code","695d4455":"code","69cffd02":"code","787e1bd1":"code","de52f82a":"code","1cd270ef":"code","0ca69f52":"code","7f5bc47c":"code","959a4f4a":"code","68bf4a93":"code","0c7b7354":"code","e4763452":"code","5efc3215":"code","857754ea":"code","d273e1a3":"code","b72cf531":"code","1df05840":"code","11659cca":"code","3501a046":"code","8a824704":"code","c9bfc5f8":"code","bb60ec7b":"code","5253fac0":"code","9494e222":"code","2c1a7126":"code","b6499dca":"code","e90f9abf":"code","3a5176a7":"code","8bb77a19":"code","b675dffd":"code","b8afd38c":"code","3951438f":"code","aa8747c1":"code","b80465a4":"code","9887ddb7":"code","23dc58b8":"code","ec369373":"code","356ca8ca":"code","c460d001":"code","09b143cc":"code","f2c0b77e":"code","c09433a3":"code","038a3756":"code","d0bae5bf":"code","3f73498e":"code","62dd964b":"code","3c9ca1c6":"code","a94ebe99":"code","879204e8":"code","b743dd9e":"code","e7d74ec5":"code","a2da39d4":"code","20f95357":"code","e7538e9f":"code","0808ef5f":"code","f777ea3c":"code","e68eef7d":"code","e2b2c5f7":"code","ba8611f6":"code","7e273dc9":"code","0431f454":"code","0e2f9a99":"code","0faf8fc3":"markdown","9b981aa5":"markdown","00641127":"markdown","34af710e":"markdown","9e39d542":"markdown","69e23719":"markdown","1e207aa7":"markdown","15de5479":"markdown","5315e623":"markdown","c8c00a1a":"markdown","52642bad":"markdown","c28c6167":"markdown","c40668cd":"markdown","8b79ad98":"markdown","e873be10":"markdown","a0ca59c1":"markdown","ed59cd0b":"markdown","61f9d2e1":"markdown","3b3db209":"markdown"},"source":{"68959317":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom collections import namedtuple\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom datetime import datetime\nimport re\nimport os\nimport itertools # mainly for `chain`\nimport spacy # for dependency trees, word vectors, etc.\nimport plotnine as plt\nfrom tqdm import tqdm\n\n# print(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9aee3825":"census_dir = os.listdir('..\/input\/los-angeles-census-data\/')\ncensus_path = '..\/input\/los-angeles-census-data\/'\n# census_dir","8ca8bc1b":"census_df = pd.read_csv(\n    os.path.join(census_path, 'census-data-by-council-district.csv')\n) # use 'council-district' because Hispanic data is intact\n# census_df.head()","4f1586d6":"gender_fracs = (\n    census_df.select(lambda x: 'pop' in x and 'male' in x.lower(), axis = 'columns')\n     .aggregate(sum)\n     \/\n        census_df['Pop2010'].aggregate(sum)\n).rename('gender_fracs')\n# print(gender_fracs)\ngender_fracs.plot.bar()","fc73ab78":"# # hacking together something to make ggplot be agreeable -- faux DF\n# test_df = pd.DataFrame(gender_fracs)\n# test_df['index'] = test_df.index\n\n# (plt.ggplot(test_df, plt.aes('index','gender_fracs')) +\n#  plt.geom_bar(stat = 'identity')\n# )","bdeb9a8d":"# get breakdown by race\nrace_counts = (census_df.select(lambda x: 'pop' in x and not 'male' in x.lower(), axis = 'columns')\n .sum(axis='index') # counts by race\n).rename('race_counts')\n\n# have both counts and fractions in same dataframe\nrace_df = pd.DataFrame(data = {'race_counts': race_counts, \n                     'race_fracs': race_counts\/census_df['Pop2010'].aggregate(sum)\n                    }\n            )\n# print(race_df)\nrace_df['race_fracs'].plot.bar()","1bfc850a":"applicant_dir = os.path.join('..','input', 'city-of-los-angeles-job-applicant-composition')\nraw_applicant_df = pd.read_csv(os.path.join(applicant_dir, 'rows.csv'))\n# remove those with missing entries, see how big the difference is\napplicant_df = raw_applicant_df.dropna()\nprint(\"The ratio of cleaned-up entries (no NaN's) to raw entries is {0:.4f}.\".\n      format(len(applicant_df)\/len(raw_applicant_df)))\napplicant_df","8e83971a":"# split string column via 'extract'\n# ala https:\/\/stackoverflow.com\/a\/21296915\nregexp_extractor = r'.*?(?P<Job_Title>[0-9]*[a-zA-Z ]+).*?(?P<Job_Number>\\d+)'\n# test = applicant_df['Job Description'].str.extract(regexp_extractor)\n\n\napplicant_df = (applicant_df['Job Description'].str.extract(regexp_extractor)\n .join(\n     applicant_df.drop(['Fiscal Year', 'Job Number', 'Job Description'], axis = 'columns'),\n      how = 'inner'\n     )\n)\n# merge lines with same job title *and* job number\napplicant_df = applicant_df.groupby(['Job_Title', 'Job_Number'], as_index = False).sum()","8c4f26d2":"ethnicity_indices = ['Black', 'Hispanic', 'Asian',\n       'Caucasian', 'American Indian\/ Alaskan Native', 'Filipino',\n       'Unknown_Ethnicity']\ngender_indices = ['Male', 'Female', 'Unknown_Gender']\n\nethnicity_nums = (applicant_df\n .groupby(applicant_df['Job_Number']\n         )\n#  .apply(matcher))\n .sum()\n [ethnicity_indices]\n .aggregate(sum, axis = 'index')\n) \nethnicity_nums.plot.bar()\n# probably dubious; if diversity (wrt ethnicity) has been an issue","4f440335":"n = 10\ndf_popular = applicant_df.sort_values('Apps Received', axis = 'index', ascending = False).head(n)\nprint(\"The top {0} most popular positions account for {1:.1%} of all applications.\"\n     .format(n, df_popular['Apps Received'].sum()\/applicant_df['Apps Received'].sum()))\ndf_popular.plot(x='Job_Title', y = 'Apps Received', kind='bar')","a763d562":"ethnicity_df_popular = df_popular.select(lambda x: x in ethnicity_indices, axis = 'columns').sum()\n(ethnicity_df_popular\/ethnicity_df_popular.sum()).plot(kind = 'bar')","07ea57a2":"cityofla_root = '..\/input\/data-science-for-good-city-of-los-angeles\/cityofla\/CityofLA'\nbulletins_dir = os.path.join(cityofla_root, 'Job Bulletins')\nadditional_dir = os.path.join(cityofla_root, 'Additional data')\n# os.listdir(bulletins_dir)[:10]","cce0b60c":"# let's see what fields we need to fill\n# the specification itself\nspec_csv = pd.read_csv(os.path.join(additional_dir, 'kaggle_data_dictionary.csv'))\n# the provided example\nexample_csv = pd.read_csv(os.path.join(additional_dir, 'sample job class export template.csv'))\nentries = list(spec_csv['Field Name'].unique())\nentries","72b9ccbc":"job_titles = []\nwith open(os.path.join(additional_dir, 'job_titles.csv')) as f:\n    job_titles = [job.strip().title() for job in f.readlines()]\njob_titles[:10]","695d4455":"# example_csv.head()","69cffd02":"n_char = ' #newline# ' #to replace newlines\nt_char = ' #tab# ' #to replace tab\n\ndef load_file(fn,root_dir = bulletins_dir,  newline_repl = n_char, tab_repl = t_char, encoding = 'utf-8'):\n    \"\"\"\n    Load text from filename.\n    Replace newlines ('\\r\\n' or '\\n' or '\\r') and tabs ('\\t') with newline_repl and tab_repl, respectively.\n    Return dictionary with fields:\n    - 'fn' : original filename\n    - 'text': text of file, as substituted above\n    \"\"\"\n    with open(os.path.join(bulletins_dir, fn), mode = 'r', encoding = encoding) as f:\n        text = (f.read()\n                .replace('\\r\\n', '\\n').replace('\\r', '\\n').replace('\\n', newline_repl)\n                .replace('\\t', tab_repl))\n    \n    return {'fn': fn, 'text': text}\n\ntest_fn = \"ADMINISTRATIVE ANALYST 1590 060118.txt\"\nd = load_file(test_fn, encoding = 'latin-1')","787e1bd1":"marker_newheading = n_char + '[A-Z ]{4,}' + n_char # probably if all-caps are floating on their own, it's a heading\n    # it seems related notes follow each heading with \"NOTES:\" (with a colon ':')\n    # we'll keep these under the same heading for now, perhaps will separate later\n\npattern_job_title = '(?P<text>^[A-Z ]+)' # because one contiguous string and job is always first, this is enough\npattern_class_code = '(?P<label>[cC]lass [cC]ode)\\s*:?\\s*(?P<text>[0-9]+)' #'\\s' = whitespace (includes ':' in this case)\npattern_open_date = '(?P<label>[oO]pen [dD]ate)\\s*:?\\s*(?P<text>[0-9-]+)' # should parse via datetime\n\npattern_chunk_duties = 'DUTIES(?P<text>.*?)' + marker_newheading\npattern_chunk_salary = 'SALARY\\s*?(' + n_char + ')*(?P<text>.*?)' + marker_newheading # further processing if DWP is mentioned\npattern_chunk_requirements =  ('(?P<label>REQUIREMENT(S?)(\\\/MINIMUM QUALIFICATION(S?))?).*?' \n                        + n_char \n                        + '(?P<text>.*?)'\n                        + marker_newheading)\npattern_chunk_process_notes = 'REQUIREMENT(S?)(.*?)NOTE(S?)(:?)(?P<text>.*?)WHERE TO APPLY' # Requirement Notes follow requirements section\n\npattern_is_college = '((Graduation)|(degree from)) .*? (?P<text>(college)|(university))'\n\n\n# pattern_exp_length_unspecified = r'(?P<unit>(year)|(month)|(day)|(hour))(s?).*?(experience)'\n\n\n# a dictionary we'll be able to iterate through later\n# re.compile() for some optimization benefits\npattern_dict = {k:re.compile(v) for (k,v) in globals().items() if k.startswith('pattern') and type(v) == str}","de52f82a":"def split_list(l, start_func, end_func = None):\n    \"\"\"Given list, return three slices of the list (pre,mid,post):\n    - first element of mid is the first element where start_func evaluates to True\n    - last element of mid is the element *preceding* the elemtn where end_func evaluates to True,\n    or the end of the list if no such element exists.\n    \n    If instead end_func is set to None (default), will add to mid\n    only for the contiguous elements for which start_func evaluates to True\n    \n    e.g.: start_func = (lambda x: x > 2), end_func = (lambda x: x < 7)\n    split_list([1,2,9,13,9,4],start_func,end_func) -> [[1,2],[9,13,9],[4]]\n    \"\"\"\n    if end_func is None:\n        def end_func(item):\n            return not start_func(item)\n    \n    pre,mid,post = [],[],[]\n    start_mid = False\n    stop_mid = False\n    # stop_mid is a \"higher-priority\" flag than start_mid\n    # so check\/set that flag first in loop\n    for item in l:\n        if stop_mid or (start_mid and end_func(item)):\n            stop_mid = True\n            post.append(item)\n        elif not start_mid:\n            if not start_func(item):\n                pre.append(item)\n            elif (not stop_mid and start_func(item)):\n                start_mid = True\n                mid.append(item)\n        elif (start_mid and not end_func(item)):\n            mid.append(item)\n\n    return [pre,mid,post]\n            \n# unit test\nstart_func = lambda x: x > 2\nend_func = lambda x: x < 7\nassert split_list([1,2,9,13,9,4], start_func, end_func) == [[1, 2], [9, 13, 9], [4]]\n\nend_func = None\nassert split_list([1,2,9,12,0,-1,12], start_func) == [[1, 2], [9, 12], [0, -1, 12]]","1cd270ef":"def _process_simple(text, key, pattern):\n    \"\"\"Basic method for simple regex processing.\n    Returns a dictionary of {key: result of regexp.groupdict()['text'].strip()}\n    after reverting 'text' to unsubstituted version\n    \"\"\"\n    try:\n        match_text = re.search(pattern, text).groupdict()['text']\n        m = match_text.replace(n_char, '\\n')\n        return {key:m.strip()}\n    except AttributeError: # no match! So `groupdict` is not an attribute\n        return {key:None}\n\ndef process_job_title(text):\n    return _process_simple(text, 'job_title', pattern_job_title)\n\ndef process_class_code(text):\n    return _process_simple(text, 'class_code', pattern_class_code)\n\ndef process_duties(text):\n    return _process_simple(text, 'duties', pattern_chunk_duties)\n\ndef process_is_college_required(text):\n    res = _process_simple(text, 'is_college_required', pattern_is_college)\n    res['is_college_required'] = True if res['is_college_required'] else False\n    return res","0ca69f52":"def process_open_date(text, in_fmt = \"%m-%d-%y\", out_fmt = '%m\/%d\/%Y'):\n    try:\n        date_str = re.search(pattern_open_date, text).groupdict()['text']\n        date = datetime.strptime(date_str, in_fmt)\n        # desired output format: 10\/27\/2017\n        return {'open_date':date.strftime(out_fmt)}\n    except:\n        # it's a different format than expected\n        # we're going to be lazy for now\n        return {'open_date':None}","7f5bc47c":"def process_salary(text):\n    \"\"\"\n    Given full text,\n    Return dict of\n    - keys: 'la' or 'dwp'\n    - values: list of tuples for start and end ranges (as ints)\n    \n    (Would break if salaries are mentioned in more than two lines.)\n    \"\"\"\n    sal_match_str = (r'\\$(?P<salary_start>[0-9,]+) to \\$(?P<salary_end>[0-9,]+)')\n    dwp_str = 'Department of Water and Power'\n    \n    pre_dict = _process_simple(text, 'sal', pattern_chunk_salary)\n    salary_text = pre_dict['sal']\n    if not salary_text: # no salary given\n        return None\n    \n#     salary_text = pattern_dict['pattern_chunk_salary'].search(text).groupdict()['text']\n#     text_list = salary_text.split(n_char)\n    text_list = salary_text.split('\\n') # _process_simple replaced n_char with '\\n'\n    text_list = [s for s in text_list if '$' in s]\n    sal_tups_dict = {s:re.findall(sal_match_str, s) for s in text_list}\n    sal_tups_dict = {k:v for k,v in sal_tups_dict.items() if v != []}\n    \n    temp = {}\n    # clean up into dictionary of lists\n    for k,sal_list in sal_tups_dict.items():\n        # 'clean up' sal_list\n        nums_list = []\n        for tup in sal_list:\n            num_range = [int(re.sub(',','',t)) for t in tup]\n            nums_list.append(num_range)\n        \n        if dwp_str in k:\n            temp['dwp'] = nums_list\n        else:\n            temp['la'] = nums_list\n            \n    # now we flatten into dictionary of \"simple\" key-value pairs\n    # for inclusion into a dataframe\n    res = {}\n    label_prefixes = ['start', 'end']\n    for key, l_val in temp.items():\n        for i,tup in enumerate(l_val):\n            for j,val in enumerate(tup):\n                label = 'salary_{0}_{1}_listing_{2}'.format(label_prefixes[j], key, i+1)\n                res[label] = val\n                \n    return res","959a4f4a":"def preprocess_requirements(text):\n    \"\"\"\n    Separate full text into list of sentences corresponding to different requirements or subrequirements.\n    Returns a dictionary of\n    requirement_subheading:sent (e.g. '1a': (subsection 1a))\n    as well as a key 'misc' containing a string not tied to a specific (sub)heading.\n    \"\"\"\n    sents = [s for s in \n             pattern_dict['pattern_chunk_requirements'].search(text).groupdict()['text'].split(n_char) \n             if len(s) > 0]\n    \n    num_str = '123456789'\n    split_sents = {}\n    pre = sents\n    \n    for s in num_str:\n        split_str = '\\A(?P<label>[{}a-z])\\.'.format(s) # match with strings start with e.g. '{s}.' and also any 'a.','b.', \n        splitter = re.compile(split_str)\n        pre,mid,post = split_list(pre, start_func = splitter.search)\n        # split_sents[s] = mid\n        # build specific labels (some repeated searches but oh well) \n        labels = [(s + splitter.search(sent).groupdict()['label']) \n                  if s != splitter.search(sent).groupdict()['label']\n                  else s for sent in mid\n                 ]\n        split_sents.update(zip(labels, mid))\n        pre.extend(post)\n    \n    split_sents['misc'] = '\\n'.join(pre) # keep any \"uncaptured\" lines as a long str\n\n    \n    \n    return split_sents\n\n\ndef process_majors(text):\n    \"\"\"\n    Returns a string of majors relevant to the listing, separated by '|'\n    ('' if no relevant majors).\n    \"\"\"\n    dict_preprocessed = preprocess_requirements(text)\n    \n#     dict_processed = {}\n#     for key, val in dict_preprocessed.items():\n#         try:\n#             dict_processed[key] = get_major_requirement(val)\n#         except:\n#             print('key ={}, val = {}'.format(key, val))\n#             raise\n    \n    dict_processed = {key:get_major_requirement(val) for key,val in dict_preprocessed.items()}\n    temp = [v for v in list(dict_processed.values()) if v]\n    majors_list = ['|'.join(v) for v in temp]\n    # flatten list\n#     majors_list = itertools.chain.from_iterable(majors_list)\n    majors_list = [m for m in majors_list if m != '']\n    return {'majors':'|'.join(majors_list)}\n        \n","68bf4a93":"file = load_file('SYSTEMS ANALYST 1596 102717.txt')\n# file = load_file('ADMINISTRATIVE ANALYST 1590 060118.txt')\ntext = file['text']","0c7b7354":"import spacy","e4763452":"# en_nlp = spacy.load('en')\nen_nlp = spacy.load('en_core_web_lg')","5efc3215":"s = \"\"\"1. Graduation from an accredited four-year college or university with a major in Computer Science, Philosophy of Science, Information Systems, or Geographical Information Systems; or\"\"\"\ndoc = en_nlp(s)\nspacy.displacy.render(doc)","857754ea":"def print_node(tok):\n    \"\"\"Print human-readable information on Spacy Token.\"\"\"\n    print(\"Text:{}, POS:{}, tag:{}, dep:{}\".format(tok.orth_, tok.pos_, tok.tag_, tok.dep_))","d273e1a3":"def text_to_num(textnum):\n    \"\"\"\n    Converts text to number.\n    Currently a brute-force\/dictionary-based implementation for some\n    single-token words (handles \"zero\" through \"twenty\")\n    (Should handle current use-case.)\n    \"\"\"\n    num_dict = {\n        'zero': 0,\n        'one':1,\n        'two':2,\n        'three':3,\n        'four':4,\n        'five':5,\n        'six':6,\n        'seven':7,\n        'eight':8,\n        'nine':9,\n        'ten':10,\n        'eleven':11,\n        'twelve':12,\n        'thirteen':13,\n        'fourteen':14,\n        'fifteen':15,\n        'sixteen':16,\n        'seventeen':17,\n        'eighteen':18,\n        'nineteen':19,\n        'twenty':20\n    }\n    if textnum in num_dict.keys():\n        return num_dict[textnum]\n    \n    # maybe a str'd number like \"1,000\"\n    s = re.sub(',','',textnum)\n    try:\n        return int(s)\n    except ValueError:\n        # still no good, huh? ...\n        print(\"text_to_num didn't work! Number: {}\".format(textnum))\n    return None","b72cf531":"test = \"\"\"\n1. Graduation from an accredited four-year college or university with a major in Computer Science, Information Systems, or Geographical Information Systems; or\n2. Graduation from an accredited four-year college or university and two years of full-time paid experience in a class at the level of Management Assistant which provides experience in:\na. the development, analysis, implementation or major modification of new or existing computer-based information systems or relational databases; or\nb. performing cost benefit, feasibility and requirements analysis for a large-scale computer-based information system; or\nc. performing system implementation and support activities including software and hardware acquisition, installation, modifications to system configuration, system and application upgrade installation; or\n3. Two years of full-time paid experience as a Systems Aide with the City of Los Angeles; and\na. Satisfactory completion of four courses, of at least three semester or four quarter units each, in Information Systems, Systems Analysis, or a closely related degree program, professional designation, or certificate program from an accredited college or university.\nb. At least three of the courses must be from the core courses required in the program, and one course may be from either the required core courses or the prescribed elective courses of the program.  A course in systems analysis and design is especially desired, but not required.\n\"\"\"\n\ntest_doc = en_nlp(test)\n\ns = \"\"\"1. Graduation from an accredited four-year college or university with a major in Computer Science, Philosophy of Science, Information Systems, or Geographical Information Systems; or\"\"\"\ndoc = en_nlp(s)","1df05840":"def find_lemma(lemma, doc, start = 0):\n    \"\"\"\n    NOTE: Must use the lemma form.\n    type(lemma) == str, \n    type(doc) == spacy.Doc (or some other iterable containing spacy.Tokens)\n    (if str, doc is converted to a Doc)\n    \n    Returns the *first* instance of token with lemma from `lemma` in the \n    document itself if found, else None.\n    'start' determines the index from which token of the doc to start looking\n    (this is a search *to the right*).\n    \"\"\"\n    \n    for token in doc[start:]:\n        if token.lemma_ == lemma:\n            return token\n    return None\n\ndef is_proper_context(lemmas, doc, need_all = False):\n    \"\"\"\n    Sees whether lemmas (str) are found in doc.\n    Returns boolean True if yes.\n    If need_all flag is True, only return True if all lemmas are in doc.\n    \"\"\"\n    if need_all:\n        for lemma in lemmas:\n            if find_lemma(lemma, doc) is None:\n                return False\n        return True\n    else:\n        for lemma in lemmas:\n            if find_lemma(lemma, doc):\n                return True\n        return False\n        ","11659cca":"# for seeing if a driver's license is required\n# TODO: Seems that with a bit more abstractness, could be used to\n# deal with more questions regarding context\ndef get_drivers_license_requirement(sent):\n    \"\"\"\n    Checks if sent (a spacy.Doc or str) suggests a driver's license is required.\n    Returns None if unknown from sentence, 'P' if possibly required, 'R' if required, 'N' if not required.\n    \"\"\"\n    if type(sent) == str:\n        sent = en_nlp(sent)\n    \n    ret = None\n    \n    target = find_lemma('require', sent)\n    if target is None or not is_proper_context([\"driver\", \"license\"], doc = sent, need_all = True):\n        return None # may not be relevant\n    \n    \n    is_negated = ((sum(['neg' == t.dep_ for t in target.children]) % 2) == 1)\n    is_ambiguous = (sum(['may' == t.lemma_ for t in target.children]) > 0)\n    \n    if is_ambiguous:\n        ret = 'P'\n    else:\n        ret = 'N' if is_negated else 'R'\n    \n    return ret\n\n# unit test\nassert get_drivers_license_requirement(sent = en_nlp(\"This position may not require a driver's license.\")) == 'P'\n\ndef process_drivers_license_requirement(text):\n    try:\n        sents = re.search(pattern_chunk_process_notes,text).groupdict()['text']\n    except:\n        return {'drivers_license_req':''}\n    sents = sents.split(n_char)\n    temp = [get_drivers_license_requirement(s) for s in sents]\n    temp = [t for t in temp if t]\n#     print(temp)\n    # TODO: really hacky\/questionable\n    try:\n        temp = temp[0:1] # only keep first mention\n    except: # nothing in list -- ''.join(temp) will return '' without extra processing\n        pass\n    return {'drivers_license_req':''.join(temp)}\n\n# process_drivers_license_requirement(text)","3501a046":"def traverse_tree(start_node, is_match, process_node, top_down = True):\n    \"\"\"\n    Traverse tree, starting from start_node.\n    If is_match(node) returns True, perform process_node(node).\n    Otherwise, keep searching.\n    If top_down is True (default), traverse top-down (root to leaves).\n    If False, travel up the tree (from passed-in leaf toward root).\n    \n    Returns a list of process_node(node) results.\n    \"\"\"\n    # traverse down from 'major' until the first proper-noun tag (t.tag_ == 'NNP') is found\n    # then use children's (t.dep_ == 'compound') [merge tokens] and (t.dep_ == 'conj') [separate entity]\n    # to build spans for the majors.\n    # NOTE: This will almost surely fail for long majors with commas and \"and\"s\n    # Will try triaging with whether 'or' vs 'and' conj is in children of \"major\"\n    \n    # going down or up?\n    cont = ''\n    if top_down:\n        cont = 'children'\n    else:\n        cont = 'ancestors'\n    \n    stack = [start_node]\n    result = []\n    cur = None\n    while len(stack) > 0:\n        cur = stack.pop()   \n        if is_match(cur): # \"start\" of interesting span\n            result.append(process_node(cur))\n        else:\n            stack.extend(list(cur.__getattribute__(cont)))\n#             stack.extend(cur.children) # keep hunting\n    return result","8a824704":"# target = find_lemma('major', doc)\n# ## have moved functions into major_requirement\n# # res = traverse_tree(target, is_match, process_node) \n# # res # one too many layers\n# # list(itertools.chain.from_iterable(res))","c9bfc5f8":"def get_major_requirement(sent):\n    \"\"\"\n    (If type(sent) == str, will convert to spacy.Doc first)\n    \n    Check which majors are mentioned as satisfying the requirement.\n    Returns None if irrelevant sentence, otherwise a list of the majors (list of str).\n    \"\"\"\n    if type(sent) == str:\n        sent = en_nlp(sent)\n    \n    if not (is_proper_context(['graduation'], sent) and is_proper_context(['college', 'university'], sent)):\n        return None\n    \n    node_major = find_lemma('major', sent)\n    \n    if not node_major:\n        return None\n    # have a \"major *in*\" {major}\n#     target = node_major\n    try:\n        target = [n for n in node_major.children if n.orth_ == 'in'][0]\n    except IndexError: # no specific major\n        return None\n    \n    # Specific is_match and process_node functions for traverse_tree, for this use-case\n    \n    def is_match(node):\n        return (node.tag_ == 'NNP')\n\n    def process_node(cur):\n        # NOTE: With the spacy builtins token.conjuncts and token.subtree, this is more procedural\/less recursive\n\n    #             print(\"cur:{}\".format(cur))\n        ll_res = []\n        conj_list = [cur]\n        conj_list.extend(cur.conjuncts)\n    #             print(conj_list)\n        for conj in conj_list:\n            chunk_list = [child for child in conj.children if child not in conj_list]  # make chunks mutually exclusive\n    #                 print(\"Chunk List:{}\".format(chunk_list))\n            res = [list(child.subtree) for child in chunk_list] # get ME subtrees\n            res.append([conj])\n    #                 print(temp)\n            res = list(itertools.chain.from_iterable(res)) # flatten list\n    #                 print(\"After:{}\".format(temp))\n                # add coordinating conjunctive if exists\n            for node in res:\n                if node.dep_ == 'cc' and node.orth_ == 'or':\n                    ll_res.append('OR')\n                # need to remove separately -- can't modify list during above iteration\n            res = [node for node in res if node.orth_ != 'or'] \n            res = sorted(res, key= lambda tok: tok.i)\n    #                 print(\"finally:{}\".format(temp))\n            ll_res.append(res)\n        return ll_res\n    \n    temp = traverse_tree(target, is_match, process_node) # has one too many layers\n    temp = list(itertools.chain.from_iterable(temp))\n        # TODO: buggy... May need to clean up process_node. \n        # Currently, list representation works though\n#     return [sent[t[0].i:t[-1].i+1] if type(t) == list else t for t in temp] \n    \n    def postprocess(node_list):\n        temp = []\n        for l in node_list:\n            if l == 'OR':\n                continue\n            l_t = [t.orth_ for t in l]\n            temp.append(' '.join(l_t))\n        return [t.strip(' ,;') for t in temp]\n    \n    return postprocess(temp)","bb60ec7b":"s = \"\"\"1. Graduation from an accredited four-year college or university with a major in Computer Science, Philosophy of Science, Information Systems, or Geographical Information Systems; or\"\"\"\ndoc = en_nlp(s)\n\n# get_major_requirement(doc)","5253fac0":"job_fields = ['node', # a reference back to the Token in the Doc\n             'name', # plaintext (may not match node.orth_, which is only one word)\n             'experience_type', # type of experience\n             'duration' # how long the experience must be\n                        ]\n\nJob = namedtuple('Job', job_fields)","9494e222":"\n\ndef get_node_modifiers(node):\n    \"\"\"\n    Pass in Spacy.tokens.\n    Return whether job\/experience must be full-time or part-time.\n    Returns Span of Spacy.tokens relevant to type of experience, or None if no such tokens exist.\n    \n    (Note: Quality of output depends on accuracy of Spacy's dependency tree)\n    \"\"\"\n    \n    # we'll want the full subtrees of relevant nodes\n    # will use traverse_tree again, but will filter nodes beforehand\n    \n    # want to return relevant nodes\n    def has_wanted_dep(node):\n        return (node.dep_ == 'compound' or node.dep_ == 'amod')\n    \n    def get_subtree_and_node(node):\n        temp = list(node.subtree)\n        temp.append(node)\n        return temp\n    \n    # we assume that relevant information are left children of 'experience'\n    # if dep_ is \"compound\" or \"amod\"\n    res = []\n    for tok in node.lefts:\n        res.extend(\n            traverse_tree\n                   (tok, is_match = has_wanted_dep, process_node = get_subtree_and_node)\n                  )\n    if len(res) == 0:\n        return None\n    # flatten and sort\n    res = sorted(list(set(itertools.chain.from_iterable(res))), key = lambda tok: tok.i)\n    res = node.doc[res[0].i:res[-1].i+1]\n    return res\n\n\n\ndef get_associated_time(node):\n    \"\"\"Returns time (number of years) as a float.\n    \n    If no explicit time is attached to node, return None.\n    \"\"\"\n    #something like\n    \n    time_periods = {'year': 1, \n                    'month': 1\/12, \n#                     'week': 1\/(12*4),\n#                     'day': 1\/(12*4*5), \n#                     'hour': 1\/(12*4*5*8)\n                   }\n    # find the node associated with the time (associated with job)\n    \n#     print_node(node)\n    nodes = [tok for tok in node.ancestors if tok.lemma_ in time_periods.keys()]\n    # maybe node itself is a time period?\n    if node.orth_ in time_periods.keys():\n        nodes.append(node)\n    if not nodes: # maybe the dependency tree is broken (has happened)\n        nodes = [find_lemma(time, node.doc) for time in time_periods.keys()]\n        nodes = [n for n in nodes if n]\n#     print(nodes)\n#     nodes = [find_lemma(tok.lemma_, tok.doc) \n#              for tok in list(time_periods.keys())\n#              if tok.lemma_ in list(job.ancestors)]\n#     nodes = [n for n in nodes if n is not None]\n    # how to choose \"most important\" time?\n    # the one that's closest to the root\n    # and because parent.ancestors is a subset of child.ancestors\n    # we can simply choose the node with the smallest len(ancestors)\n    try:\n        time_node = sorted(nodes, key = lambda tok: len(list(tok.ancestors)))[0]\n    except: # no explicit period of time stated\n#         print(\"Problem in get_associated_time! Sentence: {}\".format(node.doc))\n        return None\n    \n    # now check whether there are numerical modifiers attached to this node\n    prefactor = get_associated_number(time_node)\n    # multiply this * time_periods[time.orth_] to get number of years\n    return prefactor * time_periods[time_node.lemma_]\n\n\n\ndef get_associated_number(node, verbose = False):\n    \"\"\"Use Spacy dependency tree to match number with node. Return number as int or float.\n    Return 0 if 'no' is attached to node.\n    Return 1 if no explicit number is attached to node (presumably 'a','an','the' is attached).\n    (*This is an assumption*)\n    \n    Else return number.\n    \"\"\"\n    # first see if there's a determiner\n    try:\n        det_nodes = [n for n in node.children if n.dep_ == 'det']\n        det_node = det_nodes[0]\n        if det_node.lemma_ == 'no':\n            return 0\n    except: # no det match\n        pass\n    try:\n        number_nodes = [n for n in node.children if n.dep_ == 'nummod']\n        num_node = number_nodes[0]\n        return text_to_num(num_node.lemma_)\n    except:\n        pass\n    # other cases, just return 1\n    return 1\n\n# unit tests\ns = \"\"\"1. Six months of perfectly normal and not at all weird experience with the City of Los Angeles as a Management Assistant or Management Aide interpreting and applying State and Federal regulations, City ordinances, the City Administrative Code and\/or the City Charter; or\"\"\"\ndoc = en_nlp(s)\ntest = find_lemma('experience', doc)\nm = get_node_modifiers(test)\nassert m.orth_ == 'perfectly normal and not at all weird'\n\ns = \"\"\"1. Six months of unpaid volunteer experience with the City of Los Angeles as a Management Assistant or Management Aide interpreting and applying State and Federal regulations, City ordinances, the City Administrative Code and\/or the City Charter; or\"\"\"\ndoc = en_nlp(s)\ntest = find_lemma('Assistant', doc)\nassert get_associated_time(test) == 0.5\n\ns = \"\"\"1. Two years of unpaid volunteer experience with the City of Los Angeles as a Management Assistant or Management Aide interpreting and applying State and Federal regulations, City ordinances, the City Administrative Code and\/or the City Charter; or\"\"\"\ndoc = en_nlp(s)\ntest = find_lemma('year', doc)\nassert get_associated_number(test) == 2\n\ns = \"\"\"1. Years of unpaid volunteer experience with the City of Los Angeles as a Management Assistant or Management Aide interpreting and applying State and Federal regulations, City ordinances, the City Administrative Code and\/or the City Charter; or\"\"\"\ndoc = en_nlp(s)\ntest = find_lemma('year', doc)","2c1a7126":"# # currently breaks below...\n# # probably because of sorting list in func,\n# # but making key from unsorted list in first instance\n# # so should have sorted list passed in\/do so here first\n# def memoize(func):\n#     memoize_dict = {}\n#     def mem_func(*args, **kwargs):\n#         # build a key by converting everything into a bunch of tuples\n#         key = []\n#         for arg in args:\n#             if type(arg) == list:\n#                 key.append(tuple(arg))\n#                 continue\n#             key.append(arg)\n#         for k in sorted(kwargs):\n#             val = kwargs[k]\n#             if type(val) == list:\n#                 val = tuple(val)\n#             key.append((k, val))\n#         key = tuple(key)\n            \n#         if key in memoize_dict:\n#             return memoize_dict[key]\n#         else:\n#             memoize_dict[key] = func(*args,**kwargs)\n#     return mem_func\n\n# @memoize\ndef keep_largest_discrete_intervals(vals, start_func, end_func, crit, acc = 0):\n    \"\"\"\n    Given a list (vals) and a function which, \n    when acted on each entry in vals,\n    provides the val's start (start_func) and end (end_func) interval portions,\n    \n    Return a list of non-overlapping values with maximal total value\n    (as assessed by crit()).\n    \"\"\"\n    # sounds like a greedy dynamic programming algorithm to me\n    if len(vals) == 0:\n        return vals, acc\n    elif len(vals) == 1:\n        return vals, acc + crit(vals[0])\n    \n    # sort by start time\n    vals = sorted(vals, key = lambda val: start_func(val))\n    val_0 = crit(vals[0])\n    \n    vals_skip, acc_skip = (keep_largest_discrete_intervals\n                    (vals[1:], start_func, end_func, crit, acc)\n                          )\n    vals_keep, acc_keep = (\n                 keep_largest_discrete_intervals\n                     ([v for v in vals if start_func(v)>= end_func(vals[0])],\n                         start_func, end_func, crit, acc + val_0)\n                           )\n    if acc_skip > acc_keep: # better to skip\n        return vals_skip, acc_skip\n    else: # tack on vals[0] along vals_keep before returning\n        vals_keep.insert(0, vals[0])\n        return vals_keep, acc_keep\n\nll = [(0,2),(8, 15), (0,3), (1,7), (2,9)]\nll= sorted(ll, key = lambda val: val[0])\n\n# # test case\n# keep_largest_discrete_intervals(vals = ll, \n#                                 start_func = lambda x: x[0], \n#                                 end_func = lambda x: x[1], \n#                                 crit = lambda x: x[1] - x[0], \n#                                 )","b6499dca":"s = \"\"\"2.  Graduation from an accredited four-year college or university with any major and six months of full-time paid experience (1,000 hours) providing recreation and leisure services for an agency or organization that conducts professional recreation programs; or\n\"\"\"\ndoc = en_nlp(s)\n# spacy.displacy.render(doc)","e90f9abf":"def get_previous_experience(sent):\n    \"\"\"\n    sent = Spacy.Doc pertaining to job (if not relevant, returns None)\n    \n    Return a Job NamedTuple with the following fields:\n    - node: a node comprising part of the job\n    - name: a string of the full job name\n    - experience_type: a Spacy.Span of type of experience\n    - time: the length (in years) of required time\n    \"\"\"\n    if type(sent) == str:\n        sent = en_nlp(sent)\n    \n    within_la = True\n    context_words = ['experience']\n    if not is_proper_context(context_words, sent, need_all=True):\n        return None\n    \n    # now just regex for the position\n    s = sent.text\n    \n    # regexp search plus cleanup\n    # if there was overlapping segments, keep only the largest one\n    prev_jobs = [re.search(job, s) for job in job_titles if re.search(job, s)]\n    prev_jobs, _ = keep_largest_discrete_intervals(prev_jobs, \n                                start_func = lambda x: x.start(), \n                                end_func = lambda x: x.end(), \n                                crit = lambda x: x.end() - x.start()\n                                                  )\n    # now get just the strings\n    prev_jobs = [m.group() for m in prev_jobs]\n    within_la = bool(prev_jobs) # if no job titles were found, maybe generic experience\n    #     print(\"prev_jobs = {}\".format(prev_jobs))\n    if within_la:\n        # match jobs to Spacy.tokens to look for time-period\n        toks_dict = {}\n        for job in prev_jobs:\n            # TODO: really should retokenize sentence...\n            lemma = job.split(' ')[-1] # just pick a word -- will be going up subtree\n            cur_i = 0\n            temp = find_lemma(lemma, sent)\n            while temp in toks_dict.keys(): # collision -- keep looking\n                cur_i = temp.i + 1 # start from the word following the previous collision\n                temp = find_lemma(lemma, sent, start = cur_i)\n            if temp:\n                toks_dict[temp] = job\n    \n#     print(toks_dict)\n    # build Job namedtuples\n    job_tuples_list = []\n    if within_la:\n        for job_node, job_name in toks_dict.items():\n            exp_type = get_node_modifiers(find_lemma('experience', sent))\n            time = get_associated_time(job_node)\n            job_tuples_list.append(Job(job_node,job_name,exp_type,time))\n    else: # will have dummy job_names\/job_nodes\n        job_node = find_lemma('experience', sent)\n        job_name = 'generic'\n        exp_type = get_node_modifiers(job_node)\n        time = get_associated_time(job_node)\n        job_tuples_list.append(Job(job_node,job_name,exp_type,time))\n            \n    return job_tuples_list","3a5176a7":"s = \"\"\"1. Two years of full-time paid professional experience as a Customer Service Representative in budgetary analysis and control, administrative analysis and research, systems and procedures analysis, or personnel administration; or\"\"\"\ndoc = en_nlp(s)\n# get_previous_experience(doc)","8bb77a19":"def process_experience(text, verbose = False):\n    \"\"\"From text, get CSV-addable dict of experience information\"\"\"\n    rel_text = preprocess_requirements(text)\n\n    temp = [get_previous_experience(sent) for sent in rel_text.values() if sent]\n    temp = [t for t in temp if t] # remove empties\n    temp = itertools.chain.from_iterable(temp)\n\n    # now iterate to make some key-value pairs\n    res = {}\n    j_fields = job_fields.copy()\n    j_fields.remove('node') # not interested in token when making dict\n    \n    for i, job_tuple in enumerate(temp):\n        for field in j_fields:\n            label = 'prev_job_{num}_{field}'.format(num=i+1, field=field)\n            try:\n                val = job_tuple.__getattribute__(field).orth_ # convert Node to str\n            except:\n                val = job_tuple.__getattribute__(field)\n            res[label] = val\n            \n    return res\n        ","b675dffd":"# process_experience(text)","b8afd38c":"s = \"\"\"1. Three years of full-time paid experience with the City of Los Angeles as a Management Assistant or Management Aide interpreting and applying State and Federal regulations, City ordinances, the City Administrative Code and\/or the City Charter; or\"\"\"\ndoc = en_nlp(s)\njob_node = get_previous_experience(doc)[0]","3951438f":"# text = file['text']\n# pattern_dict['pattern_chunk_salary'].search(text).groupdict()","aa8747c1":"# TODO: does this work for all listings?\ndef perform_processing(fn):\n    \"\"\"\n    For a job listing (given via filename),\n    extract all relevant information and place into dictionary.\n    \"\"\"\n    entry = load_file(fn, encoding = 'latin-1')\n    text = entry['text']\n    # now a slew of function calls and updates to `entry`\n    funs = [fun for key,fun in globals().items() if key.startswith('process_')]\n    \n    # some preprocessing\n    text = text.replace(\"-time paid experience\", \"-time experience\")\n    \n    for fun in funs:\n        entry.update(fun(text))\n    \n    # untransform whitespace\n    entry['text'] = text.replace(n_char, '\\n').replace(t_char, '\\t') \n    \n    return entry","b80465a4":"job_fns = os.listdir(bulletins_dir)\ndf_list = []\nfor fn in tqdm(job_fns):\n    try:\n        df_list.append(perform_processing(fn))\n    except:\n        print(\"problem_fn:{}\".format(fn))\n#         raise\nbulletins_df = pd.DataFrame.from_records(df_list)","9887ddb7":"# from collections import namedtuple\n\n# NamedDict = namedtuple('NamedDict',['name','dict'])\n\n# # close = ['education', 'college', 'school', 'university', 'apprenticeship', 'apprentice']\n# # far = ['banana', 'information', 'systems', 'bye', 'dean']\n\n\n\n# def get_sims(target, word_list, nlp = en_nlp, verbose = True):\n#     \"\"\"\n#     Get similarities between target and word_list.\n#     If either are str, they are converted to spacy.Doc's using nlp model.\n#     Returns a NamedTuple with name=target, dict={words in wordlist: similarity_scores}\n#     \"\"\"\n#     if type(word_list) == str:\n#         word_list = nlp(word_list)\n#     if type(target) == str:\n#         target = nlp(target)\n    \n# #     sim_scores = [target.similarity(word) for word in word_list]\n# #     if verbose:\n# #         print(\"Similarity with {0}: {1}\".format(target[0].orth_, sim_scores))\n#     d = {word.orth_:target.similarity(word) for word in word_list}\n#     if verbose:\n#         print(\"Similarity with '{0}': {1}\".format(target[0].orth_, d))\n# #     d['_TARGET'] = target[0].orth_\n    \n#     nd = NamedDict(name = target[0].orth_, dict = d)\n\n#     return nd\n\n# # sims = get_sims(\"education\", test_doc)\n# # max(sims.dict.values())\n\n# # print(show_)\n\n# # sims_close = [en_nlp('college').similarity(en_nlp(word)) for word in close]\n# # print(\"Close words: {}\".format(sims_close))\n\n# # sims_far = [en_nlp('college').similarity(en_nlp(word)) for word in far]\n# # print(\"Far words: {}\".format(sims_far))","23dc58b8":"def calc_min_experience(df):\n    return (df.select(lambda x: 'prev_job' in x and 'duration' in x, axis = 'columns')\n     .fillna(100000) # temporary remove nans for min operation\n     .aggregate(min, axis = 'columns')\n     .replace(100000, np.nan) # reinstate nan's\n     .rename('min_experience_duration')\n    )","ec369373":"# def calc_salary_mean(df, edge, loc):\n#     \"\"\"\n#     Calculate salary mean\n#     edge in ['start', 'end'],\n#     loc in ['la','dwp']\n#     \"\"\"\n#     s = 'salary_{}_{}'.format(edge, loc)\n#     return (\n#         df.select(lambda x: s in x, axis = 'columns').\n#      fillna(0).aggregate(sum, axis = 'columns') # sum all non-null values in a row\n#     \/\n#     (~df.select(lambda x: s in x, axis = 'columns').isnull())\n#      .apply(sum, axis = 'columns') # ...and divide by the number of non-null values in that row\n#     ).rename('mean_{}'.format(s))\n\n# edges = ['start','end']\n# locs = ['la','dwp']\n\n# temp_list = []\n# # temp_df = pd.DataFrame()\n\n# temp_list.append(calc_min_experience(bulletins_df))\n\n# for edge in edges:\n#     for loc in locs:\n#         temp_list.append(calc_salary_mean(bulletins_df, edge, loc))\n# #         label = 'salary_mean_{}_{}'.format(edge, loc)\n# #         temp_df.insert(loc = len(temp_df.columns), column = label, value = calc_salary_mean(bulletins_df,edge,loc))\n# # temp_list.append(temp_df)\n# temp_df = pd.concat(temp_list, axis = 'columns') # pd.concat() more efficient than repeated updates","356ca8ca":"bulletins_df = (bulletins_df.assign(\n    has_dwp_listing = lambda x: x['salary_start_dwp_listing_1'].notna())\n)","c460d001":"def calc_salary_mean(df, edge):\n    \"\"\"\n    Calculate salary mean\n    edge in ['start', 'end']\n    \"\"\"\n    s = 'salary_{}'.format(edge)\n    return (\n        df.select(lambda x: s in x and not 'mean' in x, axis = 'columns').\n     fillna(0).aggregate(sum, axis = 'columns') # sum all non-null values in a row\n    \/\n    (~df.select(lambda x: s in x and not 'mean' in x, axis = 'columns').isnull())\n     .apply(sum, axis = 'columns') # ...and divide by the number of non-null values in that row\n    ).rename('mean_{}'.format(s))\n\nedges = ['start','end']\n\ntemp_list = []\ntemp_list.append(calc_min_experience(bulletins_df))\n\nfor edge in edges:\n    temp_list.append(calc_salary_mean(bulletins_df, edge))\n\ntemp_df = pd.concat(temp_list, axis = 'columns') # pd.concat() more efficient than repeated updates","09b143cc":"temp_df = (temp_df.assign(\n    mean_salary_range = lambda x:x['mean_salary_end'] - x['mean_salary_start'])\n)","f2c0b77e":"bulletins_df = (bulletins_df.assign(text_length = lambda x: x['text'].str.len()))\nbulletins_df = bulletins_df.merge(temp_df, left_index = True, right_index = True, how = 'outer')","c09433a3":"# calculate the response variable \nurm_labels = ['Black','Hispanic','American Indian\/ Alaskan Native', 'Filipino']\napplicant_df['frac_URM'] = applicant_df.apply(lambda x: sum(x[urm_labels])\/x['Apps Received'], axis = 'columns')","038a3756":"features = ['min_experience_duration', 'is_college_required', 'text_length', \n            'mean_salary_start','mean_salary_range']\nresponses = ['frac_URM']\n\n\n# dataframe with only features we'll want to use in our model (and job title as index)\ndf_model = applicant_df.merge(bulletins_df, how = 'left', left_on = 'Job_Number', right_on = 'class_code')\n# df_model = applicant_df.merge(bulletins_df, how = 'left', left_on = 'Job_Title', right_on = 'job_title')\n\n\n# a slightly cleaner way to deal with duplicates (multiple job listings in bulletins)\n# could be to figure out a way to \"average\" the relevant values\n# we shall go with a simpler approach and just drop the duplicate listing\n#\n# Future directions could be to see how the listing changed and why,\n# but unfortunately we do not have information about applicant information coregistered with date,\n# so there wouldn't be much more analysis we could do (e.g. pairwise comparison)\ndf_model = df_model.drop_duplicates(subset = ['Job_Number'])\napplicant_df = applicant_df.drop_duplicates(subset = ['Job_Number'])\n\n\nprint(\"After merging, we have {} entries, which is {:.2%} of the total number of provided job postings.\".format(\n    len(df_model), len(df_model)\/len(bulletins_df)))","d0bae5bf":"(plt.ggplot(df_model, plt.aes(x = 'text_length'))\n + plt.geom_histogram(bins = 20, fill = 'gray', color = 'black') # doesn't perfectly follow ggplot syntax :'(\n + plt.ggtitle(\"Job Posting Length Distribution\")\n + plt.labs(x = \"Length of Posting\", y = \"Frequency\")\n)","3f73498e":"import fancyimpute","62dd964b":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation","3c9ca1c6":"num_topics = 10\nlda = LatentDirichletAllocation(n_components = num_topics, random_state = 42)\n\nvectorizer = CountVectorizer()\ntext_vecs = vectorizer.fit_transform(bulletins_df['text']) # convert docs to document-word matrix\nlda_model = lda.fit(text_vecs) # use document-word matrix to create n-category generative model\ntext_sims = lda_model.transform(text_vecs) # use generative model to predict probability of doc falling into topic\n\n# take argmax for each list, assign that as the text's \"category\"\n# group by category\ndf_sims = pd.DataFrame.from_records(text_sims)\n# df_sims.apply([max, np.mean, np.var], axis = 'rows') # see how informative categories are vs others","a94ebe99":"df_impute = df_sims.merge(df_model[features + responses \n#                                    + ['Apps Received'] # newly added; impute for statsmodels\n                                   # turns out, no missing values to impute!\n                                  ], left_index = True, right_index = True)","879204e8":"# first, how much are we imputing?\nfor label in features:\n    print(\"Fraction of missing values in {}: {:.3f}\".format(\n        label, sum(df_model[label].isna())\/len(df_model))\n         )","b743dd9e":"imputer = fancyimpute.IterativeImputer()\n\n\ndf_imputed = pd.DataFrame(imputer.fit_transform(df_impute), columns = df_impute.columns)\n# the imputation considered 'is_college_required' a float rather than a factor\n# replace via cutoff\ndf_imputed['is_college_required'] = df_imputed.apply(lambda x: x['is_college_required'] > 0.5, axis = 'columns')","e7d74ec5":"# # sklearn attempt\n\n# from sklearn.linear_model import LassoLarsCV, LarsCV, LassoCV, LinearRegression\n# from sklearn.metrics import mean_squared_error, r2_score\n\n# models = ['LassoLarsCV', 'LarsCV', 'LassoCV', 'LinearRegression']\n# models_dict = {}\n\n# for model in models:\n#     reg = globals()[model]() #default\n#     reg = reg.fit(df_imputed[features], np.ravel(df_imputed[responses]))\n#     models_dict[model] = reg\n\n# for key, model in results_dict.items():\n#     print(\"{} R2 value: {}\".format(key, model.score(df_imputed[features], np.ravel(df_imputed[responses]))))\n    \n\n# model_str = 'LinearRegression'\n# reg = models_dict[model_str]\n\n# print(\"The {} model has a mean-squared-error (MSE) of {:.4f} and a coefficient-of-determination (R^2) of {:.2%}\"\n#      .format(model_str, \n#             mean_squared_error(np.ravel(df_imputed[responses]), reg.predict(df_imputed[features])),\n#              r2_score(np.ravel(df_imputed[responses]), reg.predict(df_imputed[features]))\n# ))\n# dict(zip(features, reg.coef_))","a2da39d4":"import statsmodels.api as sm\nimport statsmodels.formula.api as smf","20f95357":"# from sklearn.preprocessing import MinMaxScaler\n\n# scaler = MinMaxScaler()\n# df_tomerge = pd.DataFrame(scaler.fit_transform(df_imputed[features].values), columns = df_imputed[features].columns)\n\n# df_stat = sm.tools.add_constant(df_tomerge)\n# df_stat['frac_URM'] = df_imputed['frac_URM']","e7538e9f":"# # analyze only those observations with with no missing values\n# df_stat = df_model[features + responses].dropna()\n\n# analyze dataset using imputed values\ndf_stat = df_imputed.drop(labels =[0,1,2,3,4,5,6,7,8,9], axis = 1)","0808ef5f":"df_stat = sm.tools.add_constant(df_stat)\n# df_stat = df_stat.drop('const', axis = 'columns')\ndf_stat['is_college_required'] = df_stat.apply(lambda x: int(x['is_college_required']), axis = 1)\ndf_stat['Apps_Received'] = applicant_df['Apps Received']","f777ea3c":"df_stat = df_stat.dropna()","e68eef7d":"# df_stat.corr()","e2b2c5f7":"# \"successes\" and \"failures\" of applicant diversity\n# to be used to weight the different observations appropriately in GLM\nbinom_counts = np.array([applicant_df.apply(lambda x: sum(x[urm_labels]), axis = 'columns'),\n applicant_df.apply(lambda x: x['Apps Received'] - sum(x[urm_labels]), axis = 'columns')\n]).T","ba8611f6":"lm_apps_received = smf.ols(formula = \n        \"Apps_Received ~ text_length + is_college_required \\\n        + min_experience_duration + \\\n        mean_salary_start + mean_salary_range\",\n        data = df_stat\n       ).fit()\nprint(lm_apps_received.summary())","7e273dc9":"df_stat.keys()","0431f454":"lm_frac_urm = (smf.GLM(\n#                 endog = binom_counts\n                endog = df_stat[responses]\n              , exog = df_stat[features + ['const']]\n              , family = sm.families.Binomial()\n#               , var_weights = df_stat['Apps_Received']  # weight job by number of received apps \n#                                                         # (maybe questionable? Probably gives overconfident estimates?)\n             )\n      .fit())\nprint(\"Weighting each listing equally...\")\nprint(lm_frac_urm.summary())","0e2f9a99":"lm_frac_urm_weighted = (smf.GLM(\n#                 endog = binom_counts\n                endog = df_stat[responses]\n              , exog = df_stat[features + ['const']]\n              , family = sm.families.Binomial()\n              , var_weights = df_stat['Apps_Received']  # weight job by number of received apps \n                                                        # (maybe questionable? Probably gives overconfident estimates?)\n             )\n      .fit())\nprint(\"Weighting each listing by number of applicants...\")\nprint(lm_frac_urm_weighted.summary())","0faf8fc3":"# Merging our datasets\n\nPhew! Structuring data is exhausting work! (And certainly can be continued further, but we shall leave it here for now.)\n\nLet's combine this newly minted DataFrame with our dataset regarding applicant data, create some relevant features, and see what we notice.\n\nThe goal will be to calculate a generalized linear model for a Bernoulli\/Binomial family to predict what fraction of a job postings will have applicants from underrepresented ethnic backgrounds (URMs), based on:\n\n- a job posting's length (`text_length`)\n- the minimum required previous job experience (`min_experience_duration`)\n- whether a college degree requirement is mentioned in the listing (`is_college_required`)\n\n... and a few other factors (such as average salary offered) in an attempt to mitigate [omitted variable bias](https:\/\/en.wikipedia.org\/wiki\/Omitted-variable_bias). We will then investigate the estimates for the parameter coefficients corresponding to the above features to see their effect on applicant composition. (That is, we'll see how much the above features affect who applies.)\n\n**In simple English**: We're going to use a model that maps the variables above to a fraction in $(0,1)$. We'll use that model to see how much each variable contributes.","9b981aa5":"## Statsmodels","00641127":"The above may make measures of diversity (with respect to having a high number of applicants from underrepresented minorities) seem good. But it's worth remembering that\n\n1. This is incomplete data from circa 2014. \n2. [Simpson's Paradox](https:\/\/en.wikipedia.org\/wiki\/Simpson%27s_paradox) may be at play, where things may look \"good\" in the aggregate but are in fact \"bad\" at the stratified level, where it actually matters (or vice-versa). For example, perhaps URMs are overrepresented in applying to less influential positions. Indeed, it's worth noting the most popular applications:","34af710e":"Well, some positions still contain missing values in the features we're interested in. Rather than drop these jobs from the analysis (every observation counts!), we will impute \"reasonable\" values for these values by averaging other jobs with similar topic and filled-in value. (This will be done via a combination of [Latent Dirichlet Allocation](http:\/\/www.jmlr.org\/papers\/volume3\/blei03a\/blei03a.pdf) to separate the postings into related groups, and a k-nearest-neighbors imputation using `fancyimpute`.)","9e39d542":"# Model building\n\nNow we build our model! For greater explainability, we use `statsmodels` to generate our model.","69e23719":"# The City of Los Angeles Job Posting Diversity Analysis\n\nAs many of its employees become eligible for retirement, the City of Los Angeles is interested in finding ways to improve diversity among their applicant pool.\n\nCompared to other data science competitions (where the objective function we are trying to optimize is \"given from on high\"), the question of how to improve recruitment requires a different thought pattern -- we must consider how people interact with job postings and prioritize actionable\/interpretable results when constructing our model and making recommendations.","1e207aa7":"# Previous Job Applicant Data\n\nThese come from the LA City website as listed above.\n\nThere may even be some correspondence between job postings here and job postings in the provided data! Worth looking into, especially when looking for disparities in desired applicant makeup. (Of course, we do not have information about who was eventually hired.)","15de5479":"So the sample CSV would like to have multiple rows for each observation (job listing).\n\nWe will first collect as much information as we can in the most convenient manner into one row for the purposes of analysis.","5315e623":"## Text Extraction: Dependency tree parsing (A ton of tree traversal)\n\nWe'll work with the dependency trees generated by Spacy to aid in grabbing more complex fields.","c8c00a1a":"Given the more subjective nature of \"goodness of a piece of text\", a cursory look through the postings may suggest some good lines of inquiry. After having taken a look, there are some immediate thoughts\/suggestions\/lines of inquiry that may be worth pursuing:\n\n1. The typical job posting's length and diction feel like a barrier to entry.\n\nSome off-the-cuff recommendations:\n\n1. *Show information that is relevant and actionable to applicants to reduce friction.* For example, for relevant mutually exclusive information, there could be two modes: (1) promotional applicants (within City of LA); (2) non-City of LA prospecitve applicants. Also, perhaps have a collapsed \"FAQ\" portion to the job listing in lieu of the potentially opaque \"Notes\" listed underneath headings. If there are liability concerns, the original text can perhaps be indicated as a page prior to application submission. But for many, it is not critical to know that, for example, the examination is based on a validation study. Such information would likely be ignored, or make one wonder what is meant by \"validation study\", or else simply add to the length of the job listing (increasing the likelihood of applicants either skimming the listing or forgoing the application process altogether).\n\nAfter further analysis, I aim to show whether and with what effect size post diction and (especially) length, among other factors, change application rates across different groups.","52642bad":"# Initial Plan\n\nThe City of Los Angeles (LA) is interested in greater diversity. Using race\/ethnicity as a (tenuous) proxy, we can see how job application proportions correspond to local 2010 census data. (This of course assumes that we are interested in having the employee composition reflect that of the 2010 local census data; populations may have shifted in the meantime and applicants are not limited to being\/having been in the LA area.)\n\nCensus dataset is available on Kaggle (https:\/\/www.kaggle.com\/cityofLA\/los-angeles-census-data). Job applicant data is from https:\/\/data.lacity.org\/A-Prosperous-City\/Job-Applicants-by-Gender-and-Ethnicity\/mkf9-fagf and contains information on some applications during Fiscal Years (FY) 2013-2014 and 2014-2015.","c28c6167":"# Discussion and Future Directions\n\nHmm, curious... \n\nFor our diversity prediction model, if we weight each job listing evenly, the specified features do not appear strongly associated with the fraction of URM applicants, except for a *negative* association with `mean_salary_start` (the larger the mean salary, the smaller the fraction of URM applicants). Weighting each job listing by number of applicants gives similar-magnitude weights to the features but is probably overconfident in its estimates.\n\nI am hesitant to make conclusions based on this analysis, based on a warning made when seeing how well we could predict the number of applications by the previously mentioned features. There is likely collinearity issues with the chosen features (hence a \"high condition number\") or some other issue in the specification, leading to questionable estimates. We've attempted different data curation (dropping observations with missing values), investigating correlations between variables, trying different regression variables (dropping different variables off the regression), but no luck.\n\nGiven more time, it would be worth investigating the cause of the high condition number before making any conclusions. Despite this issue, it would also be worth seeing how different clusters of job postings (clustered via, for example, topic) differ from one another in required previous experience, education requirements, salary, etc.\n\nI look forward to seeing what other Kagglers have been able to pull off with this data! I hope the City of Los Angeles finds use in these analyses.","c40668cd":"If properly loaded, Spacy comes with pre-trained dependency parsers. These dependency trees (while not perfect) do a pretty good job on the sentence level and will be helpful in extracting the more complicated pieces of information requested for our final CSV.\n\nAn example of a parsed sentence is shown below:","8b79ad98":"## Text Extraction: Regular Expression City\n\nLet's try and pull out useful chunks, i.e., chunks that \n\n1. chunks that are directly related to a field requested in the specification; or\n2. chunks which imply a certain context in which some assumptions can be made (for example: if under the \"Requirements\" tab, a mention of \"experience\" probably means that whatever is mentioned in that sentence is a requirement for the position)","e873be10":"# Los Angeles Census Data\n\nBut first, for something completely different!\n\nThis focuses on census data from 2010 to see what the relative gender\/ethnicity breakdowns of the city are.","a0ca59c1":"The self-reported ethnicity data will be used to compare against the composition of applicants to previous job postings. (**Note** that these fractions need not sum to $1$.)\n\nFrom the plots, we see that \n1. there was a fairly even split between males and females, and\n2. the most commonly reported ethnicities were (1) white; (2) Hispanic; (3) Asian; (4) black.","ed59cd0b":"Many of the most applied-for positions (Customer Service Representative, Meter Reader) do not have much impact on larger operations of the City's decision-making, and we find an overrepresentation of underrepresented minorities within these positions. Indeed, it looks like Simpson's paradox strikes, and we will have to be wary of mindless aggregation.","61f9d2e1":"# City of LA Job Bulletins: A Whole Lot of Cleanup\n\nThere is quite a bit of data wrangling to be done in order to build our desired structured database\/CSV. We shall use regular expression matching (using `re`) and dependency tree parsing (using a `spacy` model).\n\nNB: We could perhaps use a deep-learning model like [BERT](https:\/\/arxiv.org\/abs\/1810.04805) to try and pull out the semantic understandings in a more black-box manner. But given the highly structured nature of the provided job postings, BERT seems to bring in more complexity and compute requirements than is really warranted (especially if fine-tuning is required), and we may end up having to massage BERT's outputs afterward regardless.","3b3db209":"A quick view at the distribution of job posting lengths:"}}