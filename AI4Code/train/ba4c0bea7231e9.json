{"cell_type":{"13105908":"code","16090c63":"code","03776241":"code","79ffd567":"code","24ec2a8c":"code","d899e5bb":"code","7fbeded9":"code","6b770da1":"code","6391eb26":"code","515bfcaa":"code","06fd3587":"code","530e4e3b":"code","203d2acc":"code","176d0f58":"code","9db07606":"code","34e563af":"code","9227845d":"code","b0a0b6ad":"code","03f72ee1":"code","05b14253":"markdown","027be2eb":"markdown","9c5f2be1":"markdown","168f7e64":"markdown","bf9b15f5":"markdown","75945ea9":"markdown","e7be25b1":"markdown","1eadbc39":"markdown","dc152398":"markdown","15a37d07":"markdown","b3c99288":"markdown","2b807f09":"markdown"},"source":{"13105908":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","16090c63":"item_categories= pd.read_csv(filepath_or_buffer = \"..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\nitems = pd.read_csv(filepath_or_buffer = \"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\nsales_train =  pd.read_csv(filepath_or_buffer =\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\nshops = pd.read_csv(filepath_or_buffer =\"..\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\ntest =  pd.read_csv(filepath_or_buffer =\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")","03776241":"import datetime as dt\n\n# Convert the date column to pandas datetime type\nsales_train['date'] = pd.to_datetime(sales_train['date'])\n\n# sales_train.drop('date',axis=1,inplace=True)\nsales_train['date'] = sales_train['date'].apply(lambda x: x.strftime('%Y-%m'))\n\nsales_train.head()","79ffd567":"sales_train.drop(['date_block_num','item_price'] , axis =1, inplace= True)\n\nsales_train.sort_values(by='date')","24ec2a8c":"# Aggregate the data by date,shopid and item id\ntrain_data = sales_train.groupby(['date','shop_id','item_id']).sum()\n \ntrain_data","d899e5bb":"train_data = sales_train.pivot_table(index=['shop_id','item_id'], columns='date', values='item_cnt_day', fill_value=0)\n\ntrain_data.reset_index(inplace=True)\n\ntrain_data.head()","7fbeded9":"test_data = pd.merge( test , train_data , on = ['shop_id', 'item_id'], how = 'left')\ntest_data.drop(['ID', '2013-01'], axis =1, inplace=True)\ntest_data= test_data.fillna(0)\ntest_data.head()","6b770da1":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error","6391eb26":"Y_train = train_data['2015-10'].values\nX_train = train_data.drop(['2015-10'], axis = 1)\nX_test = test_data","515bfcaa":"x_train, x_test, y_train, y_test = train_test_split( X_train, Y_train, test_size=0.2, random_state=101)","06fd3587":"from sklearn.linear_model import LinearRegression\nLR = LinearRegression()\nLR.fit(x_train,y_train)\n\nprint('Train set mse:', mean_squared_error(y_train, LR.predict(x_train)))\nprint('Test set mse:', mean_squared_error(y_test, LR.predict(x_test)))\nprint('Test set score:', LR.score(x_train,y_train))","530e4e3b":"from sklearn import linear_model\nrid = linear_model.Ridge(alpha=.5)\n\nrid.fit(x_train,y_train)\n\nprint('Train set mse:', mean_squared_error(y_train, rid.predict(x_train)))\nprint('Test set mse:', mean_squared_error(y_test, rid.predict(x_test)))\nprint('Test set score:', rid.score(x_train,y_train))","203d2acc":"lasso = linear_model.Lasso(alpha=0.1)\n\nlasso.fit(x_train,y_train)\n\nprint('Train set mse:', mean_squared_error(y_train, lasso.predict(x_train)))\nprint('Test set mse:', mean_squared_error(y_test, lasso.predict(x_test)))\nprint('Test set score:', lasso.score(x_train,y_train))","176d0f58":"from sklearn.ensemble import RandomForestRegressor\nRFR = RandomForestRegressor(n_estimators = 100)\nRFR.fit(x_train,y_train)\n\nprint('Train set mse:', mean_squared_error(y_train, RFR.predict(x_train)))\nprint('Test set mse:', mean_squared_error(y_test, RFR.predict(x_test)))\nprint('Test set score:', RFR.score(x_train,y_train))","9db07606":"from sklearn.ensemble import VotingRegressor\n\nmodel= VotingRegressor([(\"Linear Regression\",LR),\n                        (\"Ridge Regression\",rid),\n                        (\"Lasso Regression\",lasso),\n                        (\"Random Forest Reressor\",RFR)\n                        ])\n\nmodel.fit(x_train,y_train)\n\nprint('Train set mse:', mean_squared_error(y_train, model.predict(x_train)))\nprint('Test set mse:', mean_squared_error(y_test, model.predict(x_test)))\nprint('Test set score:', model.score(x_train,y_train))","34e563af":"from sklearn.ensemble import GradientBoostingRegressor\ngbr = GradientBoostingRegressor()\n\ngbr.fit(x_train, y_train)\n\nprint('Train set mse:', mean_squared_error(y_train, gbr.predict(x_train)))\nprint('Test set mse:', mean_squared_error(y_test, gbr.predict(x_test)))\nprint('Test set score:', gbr.score(x_train,y_train))","9227845d":"from sklearn.model_selection import RandomizedSearchCV\nparams = {\n    \"learning_rate\": [0.05,0.10,0.15,0.20,0.25,0.30],\n    \"max_depth\": range(5,21,2),\n    \"n_estimators\" : range(20,101,10),\n    'min_samples_split':range(200,1001,200),\n    'min_samples_leaf':range(30,71,10)\n}\n\nfrom sklearn.ensemble import GradientBoostingRegressor\ngbr = GradientBoostingRegressor()\n\nrandom_search = RandomizedSearchCV(gbr, param_distributions =params ,n_iter =5 ,n_jobs=-1,cv=5,verbose = 3)\nrandom_search.fit(x_train,y_train)\n\nprint('Train set mse:', mean_squared_error(y_train, random_search.predict(x_train)))\nprint('Test set mse:', mean_squared_error(y_test, random_search.predict(x_test)))\nprint('Test set score:', random_search.score(x_train,y_train))\n","b0a0b6ad":"prediction = random_search.predict(X_test)","03f72ee1":"prediction = list(map(round, prediction))\nsubmission = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')\nsubmission['item_cnt_month'] = prediction\nsubmission.to_csv('prediction.csv', index=False)\nsubmission.head()","05b14253":"**Train data**","027be2eb":"#### **Gradient Boosting Regressor**","9c5f2be1":"#### **Linear Regression**","168f7e64":"## **Dataset**","bf9b15f5":"### **Submission file**","75945ea9":"#### **Ridge Regression**","e7be25b1":"#### **Random Forest Regressor**","1eadbc39":"#### **Lasso Regression**","dc152398":"### Model Building","15a37d07":"**Test data:**","b3c99288":"#### **Hyper Parameter Tuning of Gradient Boosting Regressor**","2b807f09":"#### **Voting Regressor**"}}