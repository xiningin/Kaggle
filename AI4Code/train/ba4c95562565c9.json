{"cell_type":{"488a2b3f":"code","1441e9d1":"code","b981e92d":"code","5b50918f":"code","0397d8ce":"code","ea6f5439":"code","dc2f903c":"code","cf749195":"code","e3689d30":"code","92b18df1":"code","e5d64461":"code","4da1aabb":"code","da9171ba":"code","3349c2b4":"code","59765907":"code","a8bf5861":"code","8b13768f":"markdown","4d702fed":"markdown","514da761":"markdown","50d5aab4":"markdown","cee10742":"markdown","ca02b08b":"markdown","ffe7a9cb":"markdown","e04401f3":"markdown","383d1aac":"markdown","18c715ba":"markdown","41fe9709":"markdown","aefcad6d":"markdown","4af109f1":"markdown","92f6e42f":"markdown"},"source":{"488a2b3f":"# First, you should install TorchUtils (see README.md)","1441e9d1":"import os\nimport cv2\nimport time\nimport math\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom torch.optim import Adam, AdamW\nfrom torch.nn.parameter import Parameter\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn import metrics\nimport urllib\nimport pickle\nimport torch.nn.functional as F\nimport seaborn as sns\nimport random\nimport sys\nimport gc\nimport shutil\nfrom tqdm.autonotebook import tqdm\nimport albumentations\nfrom albumentations import pytorch as AT\n\nimport scipy.special\nsigmoid = lambda x: scipy.special.expit(x)\nfrom scipy.special import softmax\n\nimport torch_utils as tu \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b981e92d":"SEED = 42\nbase_dir = '..\/input\/'\ntu.tools.seed_everything(SEED, deterministic=False)\ntu.tools.set_gpus('0') # gpu ids","5b50918f":"EXP = 1\nwhile os.path.exists('..\/exp\/exp%d'%EXP):\n    EXP+=1\nos.makedirs('..\/exp\/exp%d'%EXP)","0397d8ce":"CLASSES = 176\nFOLD = 5\nBATCH_SIZE = 64\nACCUMULATE = 1\nLR = 3e-4\nEPOCH = 72\nDECAY_SCALE = 20.0\nMIXUP = 0.1 # 0 to 1","ea6f5439":"train_transform = albumentations.Compose([\n    albumentations.RandomRotate90(p=0.5),\n    albumentations.Transpose(p=0.5),\n    albumentations.Flip(p=0.5),\n    albumentations.HorizontalFlip(p=0.5),\n    albumentations.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.0625, rotate_limit=45, border_mode=1, p=0.5),\n    tu.randAugment(N=2,M=6,p=1,cut_out=True),\n    albumentations.Normalize(),\n    AT.ToTensorV2(),\n    ])\n    \ntest_transform = albumentations.Compose([\n    albumentations.Normalize(),\n    AT.ToTensorV2(),\n    ])\n\n\nclass LeavesDataset(Dataset):\n    \n    def __init__(self, df, label_encoder, data_path='..\/input', transform = train_transform): \n        self.df = df \n        self.data_path = data_path\n        self.transform = transform\n        self.df.label = self.df.label.apply(lambda x: label_encoder[x])\n\n    def __len__(self):\n        return self.df.shape[0]\n\n    def __getitem__(self, idx):\n        img_path, label = self.df.image[idx], self.df.label[idx]\n        img_path = os.path.join(self.data_path, img_path)\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = self.transform(image = img)['image']\n        return img, label","dc2f903c":"train_df = pd.read_csv(os.path.join(base_dir, 'train.csv'))","cf749195":"train_df.head()","e3689d30":"sfolder = StratifiedKFold(n_splits=FOLD,random_state=SEED,shuffle=True)\ntr_folds = []\nval_folds = []\nfor train_idx, val_idx in sfolder.split(train_df.image, train_df.label):\n    tr_folds.append(train_idx)\n    val_folds.append(val_idx)\n    print(len(train_idx), len(val_idx))","92b18df1":"from torch.optim.lr_scheduler import CosineAnnealingLR\nscaler = torch.cuda.amp.GradScaler() # for AMP training","e5d64461":"def train_model(epoch, verbose=False):\n    model_conv.train()         \n    avg_loss = 0.\n    optimizer.zero_grad()\n    if verbose:\n        bar = tqdm(total=len(train_loader))\n    mixup_fn = tu.Mixup(prob=MIXUP, switch_prob=0.0, onehot=True, label_smoothing=0.05, num_classes=CLASSES)\n    for idx, (imgs, labels) in enumerate(train_loader):\n        imgs_train, labels_train = imgs.float().cuda(), labels.cuda()\n        if MIXUP:\n            imgs_train, labels_train = mixup_fn(imgs_train, labels_train)\n        with torch.cuda.amp.autocast():\n            output_train, _ = model_conv(imgs_train)\n            loss = criterion(output_train, labels_train)\n        scaler.scale(loss).backward()\n        if ((idx+1)%ACCUMULATE==0): # Gradient Accumulate\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            scheduler.step()\n        avg_loss += loss.item() \/ len(train_loader) \n        if verbose:\n            bar.update(1)\n    if verbose:\n        bar.close()\n    return avg_loss\n\ndef test_model():    \n    avg_val_loss = 0.\n    model_conv.eval()\n    y_true_val = np.zeros(len(valset))\n    y_pred_val = np.zeros((len(valset), CLASSES))\n    with torch.no_grad():\n        for idx, (imgs, labels) in enumerate(val_loader):\n            imgs_vaild, labels_vaild = imgs.float().cuda(), labels.cuda()\n            output_test, _ = model_conv(imgs_vaild)\n            avg_val_loss += (criterion_test(output_test, labels_vaild).item() \/ len(val_loader)) \n            a = labels_vaild.detach().cpu().numpy().astype(np.int)\n            b = softmax(output_test.detach().cpu().numpy(), axis=1)\n\n            y_true_val[idx*BATCH_SIZE:idx*BATCH_SIZE+b.shape[0]] = a\n            y_pred_val[idx*BATCH_SIZE:idx*BATCH_SIZE+b.shape[0]] = b\n            \n    metric_val = sum(np.argmax(y_pred_val, axis=1) == y_true_val) \/ len(y_true_val)\n    return avg_val_loss, metric_val","4da1aabb":"def train(fold):\n    best_avg_loss = 100.0\n    best_acc = 0.0\n\n    avg_val_loss, avg_val_acc = test_model()\n    print('pretrain val loss %.4f precision %.4f'%(avg_val_loss, avg_val_acc))       \n\n    ### training\n    for epoch in range(EPOCH):   \n        print('lr:', optimizer.param_groups[0]['lr']) \n        np.random.seed(SEED+EPOCH*999)\n        start_time = time.time()\n        avg_loss = train_model(epoch)\n        avg_val_loss, avg_val_acc = test_model()\n        elapsed_time = time.time() - start_time \n        print('Epoch {}\/{} \\t train_loss={:.4f} \\t val_loss={:.4f} \\t val_precision={:.4f} \\t time={:.2f}s'.format(\n            epoch + 1, EPOCH, avg_loss, avg_val_loss, avg_val_acc, elapsed_time))\n\n        if avg_val_loss < best_avg_loss:\n            best_avg_loss = avg_val_loss\n\n        if avg_val_acc > best_acc:\n            best_acc = avg_val_acc\n            torch.save(model_conv.module.state_dict(), '..\/exp\/exp' + str(EXP) + '\/model-best' + str(fold) + '.pth')\n            print('model saved!')\n\n        print('=================================')   \n\n    print('best loss:', best_avg_loss)\n    print('best precision:', best_acc)\n    return best_avg_loss, best_acc","da9171ba":"log = open('..\/exp\/exp' + str(EXP) +'\/log.txt', 'w')\nlog.write('SEED%d\\n'%SEED)\ncv_losses = []\ncv_metrics = []\n\nfor fold in range(FOLD):\n    print('\\n ********** Fold %d **********\\n'%fold)\n    ###################### Dataset #######################\n    if os.path.exist('..\/exp\/exp' + str(EXP) + '\/model-best{}.pth'.format(fold)):\n        print('fold{} has been trained!'.format(fold))\n        continue\n    labels = train_df.label.unique()\n    label_encoder = {}\n    for idx, name in enumerate(labels):\n        label_encoder.update({name:idx})\n    \n    trainset = LeavesDataset(train_df.iloc[tr_folds[fold]].reset_index(), label_encoder, base_dir, train_transform)\n    train_loader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, num_workers=16, shuffle=True, drop_last=True, worker_init_fn=tu.tools.worker_init_fn)\n    \n    valset = LeavesDataset(train_df.iloc[val_folds[fold]].reset_index(), label_encoder, base_dir, test_transform)\n    val_loader = torch.utils.data.DataLoader(valset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)\n    \n    ####################### Model ########################\n    model_conv = tu.ImageModel(name='resnest50d', pretrained=True, feature=2048, classes=CLASSES)\n    model_conv.cuda()\n    model_conv = torch.nn.DataParallel(model_conv)\n\n    ###################### Optim ########################\n    optimizer = tu.RangerLars(model_conv.parameters(), lr=LR, weight_decay=2e-4)\n\n    if MIXUP:\n        criterion = tu.SoftTargetCrossEntropy()\n    else:\n        criterion = tu.LabelSmoothingCrossEntropy()\n        \n    criterion_test = nn.CrossEntropyLoss()\n\n    T = len(train_loader)\/\/ACCUMULATE * EPOCH # cycle\n    scheduler = CosineAnnealingLR(optimizer, T_max=T, eta_min=LR\/DECAY_SCALE)\n    \n    val_loss, val_acc = train(fold)\n    \n    cv_losses.append(val_loss)\n    cv_metrics.append(val_acc)\n    torch.cuda.empty_cache()\n    log.write('fold{} best precision{} \\n'.format(fold,val_acc))\n\ncv_loss = sum(cv_losses) \/ FOLD\ncv_acc = sum(cv_metrics) \/ FOLD\nprint('CV loss:%.6f  CV precision:%.6f'%(cv_loss, cv_acc))\nlog.write('CV loss:%.6f  CV precision:%.6f\\n\\n'%(cv_loss, cv_acc))","3349c2b4":"log.close()\ntu.tools.backup_folder('.', '..\/exp\/exp%d\/src'%EXP)  # backup code","59765907":"class LeavesDataset_test(Dataset):\n    \n    def __init__(self, df, label_encoder, data_path='..\/input', transform = test_transform): \n        self.df = df \n        self.data_path = data_path\n        self.transform = transform\n        #self.df.label = self.df.label.apply(lambda x: label_encoder[x])\n\n    def __len__(self):\n        return self.df.shape[0]\n\n    def __getitem__(self, idx):\n        img_path = self.df.image[idx]\n        img_path = os.path.join(self.data_path, img_path)\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = self.transform(image = img)['image']\n        return img\n\ndef encode_label(df):\n    labels = df.label.unique()\n    class_to_num = {}\n    for idx, name in enumerate(labels):\n        class_to_num.update({name: idx})\n    num_to_class = {v: k for k, v in class_to_num.items()}\n    return class_to_num,num_to_class","a8bf5861":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\ntrain_path = '..\/input\/classify-leaves\/train.csv'\ntest_path = '..\/input\/classify-leaves\/train.csv'\nimg_path = '..\/input\/classify-leaves\/images'\nsaveFileName = '..\/exp\/exp{}\/model_all'.format(EXP)\nmodel_path = '..\/exp\/exp{}\/model-best'.format(EXP)\n\nFOLD = 5\n\n\ntrain_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)\nclass_to_num, num_to_class = encode_label(train_df)\n\n\ntest_dataset = LeavesDataset_test(test_df)\n\n\ntest_loader = torch.utils.data.DataLoader(\n        dataset=test_dataset,\n        batch_size=64,\n        shuffle=False,\n        num_workers=8\n    )\nprint(len(test_loader))\n## predict\n#model = res_model(176)\nmodel = tu.ImageModel(name='resnest50d', pretrained=True, feature=2048, classes=176)\n# create model and load weights from checkpoint\nmodel = model.to(device)\npredictions_folder = []\nfor fold in range(FOLD):\n\n    model.load_state_dict(torch.load(model_path+'{}.pth'.format(fold)))\n    model.eval()\n\n    predictions = []\n    for batch in tqdm(test_loader): \n\n        imgs = batch\n        with torch.no_grad():\n            logits,_ = model(imgs.to(device))\n            #print(logits)\n        predictions.extend(logits.argmax(dim=-1).cpu().numpy().tolist())\n    print(predictions)\n    predictions_folder.append(predictions)\nprediction = []\npredictions_folder = np.array(predictions_folder)\nfor i in range(len(predictions_folder[0])):\n    max_label = np.argmax(np.bincount(predictions_folder[:,i]))\n    prediction.append(max_label)\nprint(prediction)\npreds = []\nfor j in prediction:\n    preds.append(num_to_class[j])\n\ntest_data = pd.read_csv(test_path)\ntest_data['label'] = pd.Series(preds)\nsubmission = pd.concat([test_data['image'], test_data['label']], axis=1)\nsubmission.to_csv(saveFileName, index=False)","8b13768f":"## Training Loop","4d702fed":"## Prepare Lib","514da761":"## Param","50d5aab4":"\u6211\u53ea\u662f\u7b80\u5355\u7684\u4f7f\u7528\u4e865\u6298\u4ea4\u53c9\u9a8c\u8bc1\u7136\u540e\u7ed3\u679c\u53d6\u51fa\u73b0\u6700\u591a\u7684label","cee10742":"## Dataset","ca02b08b":"\u6211\u7684base-line\u662f\u57fa\u4e8eresnest50\u4e0a\u5f97\u5230\u7684\u7ed3\u679c\uff0c\u6ca1\u6709\u505amixup\u548ctorch-utils\u7684randAugment\uff08\u4e5f\u5c31\u662f\u8bf4\u6570\u636e\u589e\u5f3a\u4e0d\u591f\u5f3a\uff09\uff0c\u6ca1\u6709\u7528K\u6298\u4ea4\u53c9\u9a8c\u8bc1\u3002\u6700\u540e\u7684\u516c\u699c\u5206\u65700.97000\uff0c\u79c1\u699c\u5206\u65700.97204\u3002\u770b\u5230seefun\u7684discussion\u540e\u91c7\u7528\u5176\u6846\u67b6\uff0cscore\u76f4\u63a5\u6da8\u4e860.005\uff0c\u592a\u9999\u4e86\uff0c\u9042\u7528\u5176\u6846\u67b6\u8fdb\u884c\u5fae\u8c03\u3002","ffe7a9cb":"12th--Public Score Score:0.98659\uff0cPrivate Score:0.98704, reference to seefun","e04401f3":"## Inference","383d1aac":"**\u58f0\u660e\uff1a\u4ee5\u4e0b\u4ee3\u7801\u5747\u4e3aseefun\u7684\u4ee3\u7801\uff0c\u6211\u53ea\u662f\u5728\u4e0a\u9762\u8fdb\u884c\u4e86\u5fae\u8c03\u3002**\nhttps:\/\/github.com\/seefun\/TorchUtils","18c715ba":"\u5728seefun\u4ee3\u7801\u57fa\u7840\u4e0a\uff0c\u5728randAugment\u4e2d\u52a0\u5165cut_out\uff0c\u6548\u679c\u5f88\u7384\u5b66\uff0c\u6709\u4e00\u70b9\u70b9\u7684\u63d0\u5347","41fe9709":"\u6211\u89c9\u5f9736\u4e2aepoch\u8fd8\u6ca1\u6709\u5b8c\u5168\u6536\u655b\uff0c\u4e8e\u662f\u5c06epoch\u589e\u52a0\u523072. \u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\u540e\u7684score\u76f8\u5bf9\u4e8e36\u4e2aepoch\u589e\u52a0\u4e860.001.\n\n\u52a0\u5165mixup\uff0c\u8bbe\u4e3a0.1\uff08\u592a\u5927\u4f1a\u4f7f\u6a21\u578b\u6027\u80fd\u4e0b\u964d\uff09","aefcad6d":"\u53d6\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\u7684\u7ed3\u679c\u662f0.98704\uff08\u79c1\u699c\uff09\uff0c0.98659\uff08\u516c\u699c\uff09\u3002\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\u4f1a\u4f7f\u6a21\u578b\u6027\u80fd\u63d0\u9ad80.003","4af109f1":"\u9996\u5148\u611f\u8c22seefun\uff0c\u6700\u540e\u7684\u7ed3\u679c\u90fd\u662f\u57fa\u4e8eseefun\u7684\u4ee3\u7801\u548ctorch\u2014utils\u4e0a\u5fae\u8c03\u5f97\u5230\u7684\u3002","92f6e42f":"resnest\u7684\u72c2\u70ed\u7c89\u4e1d\uff0c\u4f7f\u7528resnest50d\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff1ahttps:\/\/hangzhang.org\/files\/resnest.pdf\n\nmixup + Label Smoothing\n\n\u4f7f\u7528Ranger([RAdam](arxiv.org\/abs\/1908.03265)+[Lookahead](https:\/\/arxiv.org\/abs\/1907.08610)+[GC](https:\/\/arxiv.org\/abs\/2004.01461))\/ RangerLars(Ranger+Lars)\u4f18\u5316\u5668(\u8fd9\u4e2a\u4f18\u5316\u5668\u611f\u89c9\u6536\u655b\u7684\u66f4\u5feb\uff09"}}