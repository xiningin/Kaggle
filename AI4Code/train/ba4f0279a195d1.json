{"cell_type":{"fcc9b132":"code","133a21f1":"code","e25044e3":"code","4489093d":"code","443772e5":"code","df115ed2":"code","ddce1ecd":"code","e01b3c4a":"code","005de246":"code","70cda9ba":"code","195a87ae":"code","1572aeec":"code","c161a3b3":"code","b761d977":"code","f0e19b90":"code","a547adc7":"code","601cc942":"code","d019369b":"code","538b1e14":"code","5400e2a2":"code","68c32992":"code","c4672bd8":"code","4b127a80":"code","0ff816ce":"code","9baa1420":"code","9c37f400":"code","fa1e5aa8":"code","a3882ad5":"code","20d52239":"code","cfb0b68b":"code","e708c371":"code","1a46c611":"code","f847fd61":"code","b77ab3a3":"code","aca7cb31":"code","bfd32324":"code","ef3376ce":"code","d0b843db":"code","67c01881":"code","012b5c28":"code","49db0ea5":"code","09611fd0":"code","6d5e2690":"code","41d680c8":"code","61feb46d":"code","824bb836":"code","54ad1977":"code","ce9b43d8":"code","ec1b8973":"code","0716b19e":"code","9c8cfba8":"code","114b23d2":"code","14b77262":"code","59960b6f":"code","332babac":"code","447dd88a":"code","68133a2c":"code","cfb9eb13":"code","1bf6d38b":"code","07054d9b":"code","d7cc4443":"code","8af2ed09":"code","da5e186f":"markdown","f1b1efe5":"markdown","9bd9fdb9":"markdown","37df0dde":"markdown","d05c6c11":"markdown","0c90ae2d":"markdown","614c5b63":"markdown","7c3db180":"markdown","12c51d95":"markdown","4c9b4b08":"markdown","5264f676":"markdown","b7bd2c6b":"markdown","f7e163b6":"markdown","45acb461":"markdown","881b050f":"markdown","5db2feb2":"markdown","3501d5db":"markdown","f0956939":"markdown","76983905":"markdown","568bf040":"markdown"},"source":{"fcc9b132":"import pandas as pd\nimport numpy as np \nimport seaborn as sns\nimport matplotlib.pyplot as plt","133a21f1":"df = pd.read_csv('..\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv')","e25044e3":"df.head()","4489093d":"df.tail()","443772e5":"df.info()","df115ed2":"df.isnull().sum()","ddce1ecd":"df.describe()","e01b3c4a":"plt.figure(figsize=(10,5))\nsns.countplot('gender',data = df)\nplt.title(\"Distribution of Males and Females in our Data\",fontsize = 15)","005de246":"plt.figure(figsize=(10,5))\nsns.countplot('hsc_s',data = df)\nplt.title(\"Distribution of the Streams that students chose in High school\",fontsize = 15)","70cda9ba":"plt.figure(figsize=(10,5))\nsns.countplot('degree_t',data = df)\nplt.title(\"Distribution of the Type of College Degrees\",fontsize = 15)","195a87ae":"plt.figure(figsize=(10,5))\nsns.countplot('workex',data = df,)\nplt.title(\"Distribution of how many students have prior work experience\",fontsize = 15)","1572aeec":"plt.figure(figsize=(10,5))\nsns.countplot('specialisation',data = df)\nplt.title(\"Distribution of the Types of Specialisation\",fontsize = 15)","c161a3b3":"plt.figure(figsize=(10,5))\nsns.countplot('status',data = df,palette = 'inferno')\nplt.title(\"Distribution of the Placements\",fontsize = 15)","b761d977":"plt.figure(figsize=(10,5))\nsns.countplot('gender',data = df,hue = 'status')\nplt.title(\"Distribution of Placements in Males and Females\",fontsize = 15)","f0e19b90":"plt.figure(figsize=(10,5))\nsns.countplot('workex',data = df,hue = 'status')\nplt.title(\"Distribution of Placements in Males and Females\",fontsize = 15)","a547adc7":"plt.figure(figsize=(10,5))\nsns.boxplot('status','mba_p',data = df)\nplt.title(\"Relation between the Students that were placed and their score during MBA\",fontsize = 15)","601cc942":"plt.figure(figsize=(10,5))\nsns.boxplot('status','degree_p',data = df)\nplt.title(\"Relation between the Students that were placed and their degree percentage\",fontsize = 15)","d019369b":"plt.figure(figsize=(10,5))\nsns.countplot('degree_t',data = df,hue = 'status')\nplt.title(\"Relation between the degree types that students chose and their placement\",fontsize = 15)","538b1e14":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix ,accuracy_score,recall_score,precision_score","5400e2a2":"df1 = df.copy()","68c32992":"df1.drop(['sl_no','salary'],axis = 1,inplace = True)","c4672bd8":"df1.head()","4b127a80":"df1['status']= df1['status'].map({'Placed':1,'Not Placed':0})\ndf1['workex']= df1['workex'].map({'Yes':1,'No':0})\ndf1['gender']= df1['gender'].map({'M':1,'F':0})\ndf1['hsc_b']= df1['hsc_b'].map({'Central':1,'Others':0})\ndf1['ssc_b']= df1['ssc_b'].map({'Central':1,'Others':0})\ndf1['degree_t']= df1['degree_t'].map({'Sci&Tech':0,'Comm&Mgmt':1,'Others':2})\ndf1['specialisation']= df1['specialisation'].map({'Mkt&HR':1,'Mkt&Fin':0})\ndf1['hsc_s']= df1['hsc_s'].map({'Commerce':0,'Science':1,'Arts':2})","0ff816ce":"df1.head()","9baa1420":"#separating our target column and the predictor columns\nX = df1[['ssc_p','hsc_p','degree_p','workex','mba_p','etest_p','gender','degree_t','specialisation']]\ny = df1['status']","9c37f400":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 101)","fa1e5aa8":"#we are scaling down our features so that we can get a higher accuracy and better results\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","a3882ad5":"lr = LogisticRegression()\nlr.fit(X_train,y_train)\npred = lr.predict(X_test)","20d52239":"print(\"Accuracy:\",accuracy_score(y_test, pred)*100)\nprint(\"Precision:\",precision_score(y_test, pred)*100)\nprint(\"Recall:\",recall_score(y_test, pred)*100)","cfb0b68b":"cm = confusion_matrix(y_test, pred)\nprint(cm)","e708c371":"tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\nsens = tp\/(tp+fn)\nspec = tn\/(tn+fp)\n\nprint(f\"True Positive Rate: \", 100*sens)\nprint(f\"True Negative Rate: \", 100*spec)\n","1a46c611":"from sklearn.metrics import roc_curve, auc, roc_auc_score\n\nfpr, tpr, thresholds = roc_curve(y_test, pred)\nauc = roc_auc_score(y_test, pred)\nprint('AUC: %.3f' % auc)","f847fd61":"plt.figure()\nplt.title('ROC for Logistic regression')\nplt.plot(fpr, tpr, color='darkorange',marker='.',\n         lw=2, label='ROC curve (area = %0.2f)')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=\"lower right\")\nplt.show()","b77ab3a3":"from sklearn.svm import SVC","aca7cb31":"svc = SVC()\nsvc.fit(X_train,y_train)\nsvc_pred = svc.predict(X_test)","bfd32324":"print(\"Accuracy:\",accuracy_score(y_test, svc_pred)*100)\nprint(\"Precision:\",precision_score(y_test, svc_pred)*100)\nprint(\"Recall:\",recall_score(y_test, svc_pred)*100)","ef3376ce":"cm = confusion_matrix(y_test, svc_pred)\nprint(cm)","d0b843db":"from sklearn.metrics import roc_curve, auc, roc_auc_score\n\nfpr, tpr, thresholds = roc_curve(y_test, svc_pred)\nauc = roc_auc_score(y_test, svc_pred)\nprint('AUC: %.3f' % auc)","67c01881":"plt.figure()\nplt.title('ROC for SVM with linear kernel')\nplt.plot(fpr, tpr, color='darkorange',marker='.',\n         lw=2, label='ROC curve (area = %0.2f)')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=\"lower right\")\nplt.show()","012b5c28":"svclassifier = SVC(kernel='poly', degree=8)\nsvclassifier.fit(X_train, y_train)","49db0ea5":"y_pred = svclassifier.predict(X_test)","09611fd0":"from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","6d5e2690":"from sklearn.metrics import roc_curve, auc, roc_auc_score\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nauc = roc_auc_score(y_test, y_pred)\nprint('AUC: %.3f' % auc)\nplt.figure()\nplt.title('ROC for SVM with polynomial kernel')\nplt.plot(fpr, tpr, color='darkorange',marker='.',\n         lw=2, label='ROC curve (area = %0.2f)')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=\"lower right\")\nplt.show()\n","41d680c8":"from sklearn.model_selection import GridSearchCV","61feb46d":"param_grid = {'C':[0.1,1,10,100,1000],'gamma':[1,0.1,0.01,0.001,0.0001]}","824bb836":"grid = GridSearchCV(SVC(),param_grid,verbose = 3)","54ad1977":"grid.fit(X_train,y_train)","ce9b43d8":"grid.best_estimator_","ec1b8973":"grid_pred = grid.predict(X_test)","0716b19e":"print(\"Accuracy:\",accuracy_score(y_test, grid_pred)*100)\nprint(\"Precision:\",precision_score(y_test, grid_pred)*100)\nprint(\"Recall:\",recall_score(y_test, grid_pred)*100)","9c8cfba8":"print(confusion_matrix(y_test, grid_pred))\nprint(classification_report(y_test, grid_pred))","114b23d2":"from sklearn.metrics import roc_curve, auc, roc_auc_score\n\nfpr, tpr, thresholds = roc_curve(y_test, grid_pred)\nauc = roc_auc_score(y_test, grid_pred)\nprint('AUC: %.3f' % auc)\nplt.figure()\nplt.title('ROC for SVM with RBF kernel')\nplt.plot(fpr, tpr, color='darkorange',marker='.',\n         lw=2, label='ROC curve (area = %0.2f)')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=\"lower right\")\nplt.show()","14b77262":"from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.model_selection import StratifiedShuffleSplit, cross_val_score, cross_val_predict, GridSearchCV","59960b6f":"def classification(classifier):\n    # Training The Model\n    classifier.fit(X_train, y_train)\n\n    # KFold Accuracies on Training Data\n    kfold_accuracy = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10, n_jobs=-1)\n    print(\"K-Fold Accuracy Score:\\n\", kfold_accuracy, \"\\n\")\n    print(\"Avg K-Fold Accuracy Score:\", kfold_accuracy.mean(), \"\\n\")\n\n    # Prediction on Testing Data\n    y_pred = cross_val_predict(estimator = classifier, X = X_test, y = y_test, cv = 10, n_jobs=-1)\n            \n    # Accuracy for y_test and y_pred\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy Score:\", accuracy, \"\\n\")\n\n    # Confusion Matrix\n    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred), \"\\n\")\n\n    # Classification Report\n    print(classification_report(y_test, y_pred))\n\n    #ROC curve\n    from sklearn.metrics import roc_curve, auc, roc_auc_score\n\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n    auc = roc_auc_score(y_test, y_pred)\n    print('AUC: %.3f' % auc)\n    plt.figure()\n    plt.title('ROC for {}'.format(classifier))\n    plt.plot(fpr, tpr, color='darkorange',marker='.',\n            lw=2, label='ROC curve (area = %0.2f)')\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.legend(loc=\"lower right\")\n    plt.show()","332babac":"rf_clf = ensemble.RandomForestClassifier()\n\nclassification(rf_clf)","447dd88a":"bag_clf = ensemble.BaggingClassifier()\n\nclassification(bag_clf)","68133a2c":"grad_boost_clf=ensemble.GradientBoostingClassifier()\nclassification(grad_boost_clf)","cfb9eb13":"#obtained from the codes above\nlr_accuracy= 88.37\nsvm_linear_accuracy= 79.06\nsvm_poly_accuracy= 70\nsvm_rbf_accuracy= 83.72\nensemble_RF_accuracy= 86.04\nensemble_bagging_accuracy= 88.37\nensemble_GradBoost_accuracy= 95.34","1bf6d38b":"d = {'lr_accuracy': 88.37, 'svm_linear_accuracy': 79.06, 'csvm_poly_accuracy': 70.0,'svm_rbf_accuracy':83.72,'ensemble_RF_accuracy':86.04,'ensemble_bagging_accuracy':88.37,'ensemble_GradBoost_accuracy':95.34}\nser = pd.Series(data=d, index=['lr_accuracy', 'svm_linear_accuracy', 'svm_poly_accuracy','svm_rbf_accuracy','ensemble_RF_accuracy','ensemble_bagging_accuracy','ensemble_GradBoost_accuracy']).sort_values(ascending=False)","07054d9b":"feature_imp=ser","d7cc4443":"feature_imp['svm_poly_accuracy']=70","8af2ed09":"feature_imp = ser\n# Creating a bar plot\nsns.barplot(x=feature_imp, y=feature_imp.index,palette = 'inferno')\n# Add labels to your graph\nplt.xlabel('Accuracy Score')\nplt.ylabel('Algorithms')\nplt.title(\"Visualizing All Algorithms accuracy\")","da5e186f":"**As you can see, only salary has Null values, but that is because certain students have not been placed, hence it is not a redundant null value and still conveys important information to us**","f1b1efe5":"As you can see below, our data is slightly imbalanced with a higher number of males compared to females ","9bd9fdb9":"Below, we can see the s=distribution of the streams that students have chosen when they were in high school","37df0dde":"We won't use all of these values. We will use the 'ssc_p','hsc_p','degree_p','workex','mba_p','etest_p','gender','degree_t' and 'specialisation' column as our features to predict whether a candidate will be placed or not. We will do this because not all parameters are uselful in our prediction","d05c6c11":"in the SVM classifiers we can see that the rbf kernel has the highest accuracy, followed by the linear kernel and lastly the polynomial kernel.\n\n1. since there are many parameters that are affecting the status(result\/target value) it is not possible to have a single linear \/ polynomial kernel \n2. it is hard to form a line\/ plane that can easily form a classifier for the target data \n3. since the rbf kernel forms the decision boundary with C and gamma as parameters, we have used the SVC grid to show us which c and gamma values give us the best result \n4. thus even though we used a high polynomial degree we did not get a satisfactory value. ","0c90ae2d":"On seeing the results, we have obtained the expected results, ie the ensemble methods are performing better than the traditional methods.","614c5b63":"# ANALYSIS OF RESULTS","7c3db180":"#a brief explanation of the columns\n1. SL- Serial number \n2. Gender- M or F\n3. ssc_p- 10th board percentage\n4. ssc_b- central or private board\n5. hsc_p- 12th grade percentage \n6. hsc_b- central or private board\n7. hsc_s- specialisation in 12th grade\n8. degree_p- degree percentage \n9. degree_t- degree type, field of education\n10. workex- Work Experience (Y\/N)\n11. etest - Employibility test percentage ( conducted by the college) \n12. specialisation - MBA specialisation\n12. mba_p - MBA percentage\n13. status - placed or not\n14. salary - salary if placed else NaN\n","12c51d95":"# ENSEMBLE LEARNING","4c9b4b08":"**What is the result of varying kernels in SVM?**\n","5264f676":"Encoding our data so that it is easier to use in all the upcoming models as well as for scaling the df","b7bd2c6b":"# SVM classifier\n","f7e163b6":"# LOGISTIC REGRESSION","45acb461":"linear kernel","881b050f":"# CLEANING OF DATA AND PREPROCESSING","5db2feb2":"As you can clearly see, workex is a very strong paramter to indicate whether a student will get placed or not","3501d5db":"polynomial kernel","f0956939":"surprisingly, there is no clear relation between the percentage in mba and the status of the students if they were placed or not","76983905":"RBF kernel + finding out the best parameter","568bf040":"We will drop the sl_no and salary column as they wont help us in any way. The salary also has null values so dropping it means we dint have to impute these values and it won't help us in predicting if a person gets placed or not."}}