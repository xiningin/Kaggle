{"cell_type":{"4467c42a":"code","c2248dcf":"code","5f22d15a":"code","7fa16766":"code","6485b10b":"code","6f6e8274":"code","7ebdf4a9":"code","65c2e729":"code","d6f5154d":"code","4cd7144e":"code","8bdc79fe":"code","02097926":"code","60b089b0":"code","d347367e":"code","a633e8d4":"code","c4a7ed09":"code","f77b8a8d":"code","cd4530a1":"code","53035aad":"code","2238066b":"code","99c3ede2":"code","d7fd7c5b":"code","6dd12142":"code","12f27a8f":"code","f17b7562":"code","223d58d7":"code","0e5f479c":"code","f3646c43":"code","4f87cbff":"code","d06ea933":"code","470cf9a5":"code","83c61e1b":"code","03461631":"code","782e7fd7":"code","9d3a53d1":"code","24a593ec":"code","dc6543cd":"code","6c48f528":"code","fc7e03a3":"code","2deda4a9":"code","96bc3561":"code","c7c692cd":"code","d2c59a41":"code","79e8130a":"code","2ae77bdc":"code","6acab557":"markdown","ee776448":"markdown"},"source":{"4467c42a":"!nvidia-smi","c2248dcf":"import torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport sys\nimport time\nimport numpy as np\nimport math\nimport pandas as pd\nfrom PIL import Image, ImageOps, ImageFilter\nfrom datetime import datetime\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom torchvision import datasets, models, transforms\nimport random\nimport datetime\nimport os\n\nfrom sklearn import preprocessing \nfrom sklearn.model_selection import KFold\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)","5f22d15a":"# os.chdir(\"..\/input\/pretrained_PyTorch\/\")\n# os.getcwd()","7fa16766":"train = pd.read_csv(\"..\/input\/imet-2019-fgvc6\/train.csv\")\nlable = pd.read_csv(\"..\/input\/imet-2019-fgvc6\/labels.csv\")\ntest = pd.read_csv(\"..\/input\/imet-2019-fgvc6\/sample_submission.csv\")","6485b10b":"lable_length = len(lable)\ntrain_length = len(train)\ntest_length = len(test)\nprint(train_length)\nprint(lable_length)\nprint(test_length)","6f6e8274":"print(np.array(lable)[397])\nprint(np.array(lable)[398])\nc_length = len(np.array(lable)[:398])\nt_length = len(np.array(lable)[398:])\nprint(c_length)\nprint(t_length)","7ebdf4a9":"#np.array(test)","65c2e729":"test.head","d6f5154d":"#np.array(train)","4cd7144e":"def creatData(train,lable_length):\n    train = np.array(train)\n    trainA_data = []\n    lab_data = []\n    #trainC_data = []\n    #trainT_data = []\n    for t in range(train_length):\n        v = np.zeros(lable_length)\n        #print(train[t,1])\n        lab = []\n        for s in train[t,1].split(\" \"):\n            #print(s)\n            v[int(s)] = 1\n            lab.append(int(s))\n        trainA_data.append([train[t,0],v])\n        lab_data.append([train[t,0],np.array(lab)])\n        #trainC_data.append([train[t,0],v[:c_length]])\n        #trainT_data.append([train[t,0],v[c_length:]])\n    return np.array(trainA_data),np.array(lab_data)\n    #return np.array(trainA_data)#,np.array(trainC_data),np.array(trainT_data)","8bdc79fe":"train_a,train_lab = creatData(train,lable_length)\n#train_a,train_c,train_t = creatData(train,lable_length)\n#train_t.shape()","02097926":"train_lab[0]","60b089b0":"def dfilter(data,th):\n    dfilter = []\n    for i in range(len(data)):\n        if train_a[i][1].sum()>th:\n            dfilter.append(train_a[i])\n    return np.array(dfilter)","d347367e":"datafilter = dfilter(train_a,1) # remain feature great then 1","a633e8d4":"print(f\"amount of image before: {len(train_a)}\")\n\nprint(f\"amount of image after: {len(datafilter)}\")","c4a7ed09":"# not use\ndatafilter = train_a","f77b8a8d":"image_resize = 200\ndata_transforms2 = transforms.Compose([\n    transforms.Resize((image_resize,image_resize)),\n    #transforms.RandomResizedCrop(250),\n    #transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(\n            [0.485, 0.456, 0.406], \n            [0.229, 0.224, 0.225])\n    ])\n\n\ndata_transforms = transforms.Compose([\n    transforms.Resize((image_resize,image_resize)),\n    #transforms.RandomResizedCrop(250),\n    #transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    #transforms.Normalize(\n    #        [0.485, 0.456, 0.406], \n    #        [0.229, 0.224, 0.225])\n    ])\n\ntrain_transformer = transforms.Compose([\n    transforms.Resize((128,128)),              # resize the image to \n    #transforms.RandomHorizontalFlip(),  # randomly flip image horizontally\n    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    transforms.ToTensor(),           # transform it into a PyTorch Tensor\n    #transforms.Normalize(mean = (0.5, 0.5, 0.5), std = (0.5, 0.5, 0.5))\n])","cd4530a1":"class trainDataset(Dataset):\n    def __init__(self, train_lib, transform,transform2):\n        self.filenames = train_lib[:,0]\n        self.labels = train_lib[:,1]\n        self.transform = transform\n        self.transform2 = transform2\n        #self.new_feature = \n\n    def __len__(self):\n        return len(self.filenames)\n\n    def __getitem__(self, idx):\n        img = Image.open(\"..\/input\/imet-2019-fgvc6\/train\/\"+format(self.filenames[idx])+'.png')  # PIL image\n        #image= image.filter(ImageFilter.EDGE_ENHANCE)\n        #image2 = image.filter(ImageFilter.FIND_EDGES)\n        image = self.transform(img)\n        image2 = self.transform2(img)\n        #return image, self.labels[idx]\n        return image,image2, self.labels[idx]\n","53035aad":"class testDataset(Dataset):\n    def __init__(self, test_lib, transform,transform2):\n        test_lib = np.array(test_lib)\n        self.filenames = test_lib[:,0]\n        #self.labels = test_lib[:,1]\n        self.transform = transform\n        self.transform2 = transform2\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def __getitem__(self, idx):\n        img = Image.open(\"..\/input\/imet-2019-fgvc6\/test\/\"+format(self.filenames[idx])+'.png')  # PIL image\n        #image= image.filter(ImageFilter.EDGE_ENHANCE)\n        #image2 = image.filter(ImageFilter.FIND_EDGES)\n        image = self.transform(img)\n        image2 = self.transform2(img)\n        #return image,self.filenames[idx]\n        return image,image2,self.filenames[idx]","2238066b":"trainA_dataloader = DataLoader(trainDataset(datafilter, data_transforms,data_transforms2),batch_size=32, shuffle=True,num_workers=2, pin_memory=True)\n#trainC_dataloader = DataLoader(trainDataset(train_c, data_transforms,data_transforms2),batch_size=32, shuffle=True,num_workers=2, pin_memory=True)\n#trainT_dataloader = DataLoader(trainDataset(train_t, data_transforms,data_transforms2),batch_size=32, shuffle=True,num_workers=2, pin_memory=True)","99c3ede2":"score_dataloader = DataLoader(trainDataset(datafilter[:1000], data_transforms,data_transforms2),batch_size=32, shuffle=False,num_workers=2, pin_memory=True)","d7fd7c5b":"test_dataloader = DataLoader(testDataset(test, data_transforms,data_transforms2),batch_size=32,shuffle=False,num_workers=2, pin_memory=True)","6dd12142":"plt.figure(figsize=(10,10))\nplt.subplot(2,2,1)\nplt.imshow(transforms.ToPILImage()(trainA_dataloader.dataset[15][0]).convert('RGB'))\nplt.subplot(2,2,2)\nplt.imshow(transforms.ToPILImage()(trainA_dataloader.dataset[15][1]).convert('RGB'))\nplt.subplot(2,2,3)\nplt.imshow(transforms.ToPILImage()(trainA_dataloader.dataset[29][0]).convert('RGB'))\nplt.subplot(2,2,4)\nplt.imshow(transforms.ToPILImage()(trainA_dataloader.dataset[29][1]).convert('RGB'))\n","12f27a8f":"#(np.array(trainC_dataloader.dataset[4][0]).shape)","f17b7562":"# https:\/\/github.com\/Cadene\/pretrained-models.pytorch\/blob\/master\/pretrainedmodels\/models\/senet.py\n\"\"\"\nResNet code gently borrowed from\nhttps:\/\/github.com\/pytorch\/vision\/blob\/master\/torchvision\/models\/resnet.py\n\"\"\"\nfrom __future__ import print_function, division, absolute_import\nfrom collections import OrderedDict\nimport math\n\nimport torch.nn as nn\nfrom torch.utils import model_zoo\n\n__all__ = ['SENet', 'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152',\n           'se_resnext50_32x4d', 'se_resnext101_32x4d']\n\npretrained_settings = {\n    'senet154': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/senet154-c7b49a05.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnet50': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnet50-ce0d4300.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnet101': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnet101-7e38fcc6.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnet152': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnet152-d17c99b7.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnext50_32x4d': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnext50_32x4d-a260b3a4.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnext101_32x4d': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnext101_32x4d-3b2fe3d8.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n}\n\n\nclass SEModule(nn.Module):\n\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels \/\/ reduction, kernel_size=1,\n                             padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels \/\/ reduction, channels, kernel_size=1,\n                             padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n\n\nclass Bottleneck(nn.Module):\n    \"\"\"\n    Base class for bottlenecks that implements `forward()` method.\n    \"\"\"\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.se_module(out) + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(Bottleneck):\n    \"\"\"\n    Bottleneck for SENet154.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes * 2)\n        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n                               stride=stride, padding=1, groups=groups,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(planes * 4)\n        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNetBottleneck(Bottleneck):\n    \"\"\"\n    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n    (the latter is used in the torchvision implementation of ResNet).\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n                               stride=stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n                               groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNeXtBottleneck(Bottleneck):\n    \"\"\"\n    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None, base_width=4):\n        super(SEResNeXtBottleneck, self).__init__()\n        width = math.floor(planes * (base_width \/ 64)) * groups\n        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n                               stride=1)\n        self.bn1 = nn.BatchNorm2d(width)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n                               padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(width)\n        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SENet(nn.Module):\n\n    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n                 downsample_padding=1, num_classes=1000):\n        \"\"\"\n        Parameters\n        ----------\n        block (nn.Module): Bottleneck class.\n            - For SENet154: SEBottleneck\n            - For SE-ResNet models: SEResNetBottleneck\n            - For SE-ResNeXt models:  SEResNeXtBottleneck\n        layers (list of ints): Number of residual blocks for 4 layers of the\n            network (layer1...layer4).\n        groups (int): Number of groups for the 3x3 convolution in each\n            bottleneck block.\n            - For SENet154: 64\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models:  32\n        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n            - For all models: 16\n        dropout_p (float or None): Drop probability for the Dropout layer.\n            If `None` the Dropout layer is not used.\n            - For SENet154: 0.2\n            - For SE-ResNet models: None\n            - For SE-ResNeXt models: None\n        inplanes (int):  Number of input channels for layer1.\n            - For SENet154: 128\n            - For SE-ResNet models: 64\n            - For SE-ResNeXt models: 64\n        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n            a single 7x7 convolution in layer0.\n            - For SENet154: True\n            - For SE-ResNet models: False\n            - For SE-ResNeXt models: False\n        downsample_kernel_size (int): Kernel size for downsampling convolutions\n            in layer2, layer3 and layer4.\n            - For SENet154: 3\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models: 1\n        downsample_padding (int): Padding for downsampling convolutions in\n            layer2, layer3 and layer4.\n            - For SENet154: 1\n            - For SE-ResNet models: 0\n            - For SE-ResNeXt models: 0\n        num_classes (int): Number of outputs in `last_linear` layer.\n            - For all models: 1000\n        \"\"\"\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        if input_3x3:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n                                    bias=False)),\n                ('bn1', nn.BatchNorm2d(64)),\n                ('relu1', nn.ReLU(inplace=True)),\n                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n                                    bias=False)),\n                ('bn2', nn.BatchNorm2d(64)),\n                ('relu2', nn.ReLU(inplace=True)),\n                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n                                    bias=False)),\n                ('bn3', nn.BatchNorm2d(inplanes)),\n                ('relu3', nn.ReLU(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n                                    padding=3, bias=False)),\n                ('bn1', nn.BatchNorm2d(inplanes)),\n                ('relu1', nn.ReLU(inplace=True)),\n            ]\n        # To preserve compatibility with Caffe weights `ceil_mode=True`\n        # is used instead of `padding=1`.\n        layer0_modules.append(('pool', nn.MaxPool2d(3, stride=2,\n                                                    ceil_mode=True)))\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        self.layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1,\n            downsample_padding=0\n        )\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.avg_pool = nn.AvgPool2d(7, stride=1)\n        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n                    downsample_kernel_size=1, downsample_padding=0):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=downsample_kernel_size, stride=stride,\n                          padding=downsample_padding, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n                            downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups, reduction))\n\n        return nn.Sequential(*layers)\n\n    def features(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def logits(self, x):\n        x = self.avg_pool(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.logits(x)\n        return x\n\n\ndef initialize_pretrained_model(model, num_classes, settings):\n    assert num_classes == settings['num_classes'], \\\n        'num_classes should be {}, but is {}'.format(\n            settings['num_classes'], num_classes)\n    model.load_state_dict(model_zoo.load_url(settings['url']))\n    model.input_space = settings['input_space']\n    model.input_size = settings['input_size']\n    model.input_range = settings['input_range']\n    model.mean = settings['mean']\n    model.std = settings['std']\n\n\ndef senet154(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n                  dropout_p=0.2, num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['senet154'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet50(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNetBottleneck, [3, 4, 6, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnet50'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet101(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNetBottleneck, [3, 4, 23, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnet101'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet152(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNetBottleneck, [3, 8, 36, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnet152'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext50_32x4d(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnext50_32x4d'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext101_32x4d(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnext101_32x4d'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model","223d58d7":"from shutil import copyfile\nfrom os.path import isfile, join, abspath, exists, isdir, expanduser\nfrom os import listdir, makedirs, getcwd, remove\n\ncache_dir = expanduser(join('~', '.torch'))\nif not exists(cache_dir):\n    makedirs(cache_dir)\nmodels_dir = join(cache_dir, 'models\/')\nif not exists(models_dir):\n    makedirs(models_dir)\n    \ncopyfile(\"..\/input\/pretrained-pytorch\/densenet201-c1103571.pth\", models_dir+\"densenet201-c1103571.pth\")\ncopyfile(\"..\/input\/pretrained-pytorch\/resnet50-19c8e357.pth\", models_dir+\"resnet50-19c8e357.pth\")\ncopyfile(\"..\/input\/pretrained-se-resnet\/se_resnext101_32x4d-3b2fe3d8.pth\", models_dir+\"se_resnext101_32x4d-3b2fe3d8.pth\")","0e5f479c":"!ls ~\/.torch\/models ","f3646c43":"class Ensemble(nn.Module):\n    def __init__(self, modelA, modelB,input_length,output_length):\n        super(Ensemble, self).__init__()\n        self.modelA = modelA\n        self.modelB = modelB\n        self.classifier = nn.Linear(input_length, output_length)\n        \n    def forward(self, xin,xin2):\n        x1 = self.modelA(xin)\n        x2 = self.modelB(xin2)\n        x = torch.cat((x1, x2), dim=1)\n        x = self.classifier(F.relu(x))\n        return x\n\n    \n# efficientNet = EfficientNet.from_name('efficientnet-b7') \n# efficientNet._fc = nn.Linear(in_features=1280, out_features=lable_length)\n\n\nseresnext_model = se_resnext101_32x4d(pretrained='imagenet')\nseresnext_model.last_linear = nn.Linear(in_features=2048, out_features=lable_length, bias=True)\n\n#densenet_model = models.densenet201(pretrained=True)\n#densenet_model.load_state_dict(torch.load( models_dir+\"densenet201-c1103571.pth\"))\n#densenet_model.classifier= nn.Linear(in_features=1920,out_features=lable_length)\n\n#efficientNet = EfficientNet(width_coeff=2.0, depth_coeff=3.1,drop_connect_rate=0.5,num_classes = lable_length)\n\nresnet_model = models.resnet50(pretrained=True)\n#resnet_model.load_state_dict(torch.load(\"..\/models\/resnet50.pth\"))\nresnet_model.fc= nn.Linear(in_features=2048, out_features=lable_length)\n\n#model = Ensemble(seresnext_model, resnet_model,lable_length*2,lable_length)\n#model.to(device)\n\nmodel = seresnext_model\nmodel.to(device)","4f87cbff":"def train(epoch,fnum,dloader):\n    model.train()\n    for step, (x,x2,y) in enumerate(dloader):\n        data = Variable(x).cuda()   # batch x\n        #data2 = Variable(x2).cuda()\n        target = Variable(y).cuda()   # batch y\n        #print(data.size())\n        #print(data2.size())\n        #print(target.size())\n        #output = model(data,data2)\n        output = model(data)\n\n        loss = loss_func(output, target.float())   # cross entropy loss\n        optimizer.zero_grad()           # clear gradients for this training step\n        loss.backward()                 # backpropagation, compute gradients\n        optimizer.step()                # apply gradients\n        if step==0:\n            start = time.time()\n            #break\n            ti = 0\n        elif step==100:\n            ti = time.time()-start #total time = ti*(length\/100)\n            #print(ti)\n            ti = ti*(len(dloader)\/100)\n        if step % 100 == 0:\n            second = ti*(((len(dloader)-step)\/len(dloader)))#*(5-epoch)*(4-fnum)\n            print('Train Fold:{}\/4  Ep: {}\/3 [{}\/{} ({:.0f}%)]\\t Loss: {:.6f}\\t Remain : {} '.\n                     format(fnum+1,\n                            epoch+1, \n                            step * len(data), \n                            len(dloader.dataset),\n                            100.*step\/len(dloader), \n                            loss.data.item(),\n                            datetime.timedelta(seconds = int(second))))\n        data.cpu()\n        #data2.cpu()\n        target.cpu()\n        torch.cuda.empty_cache()\n    print(\"Finish\")","d06ea933":"def val(dloader):\n    model.eval()\n    los = []\n    for step, (x,x2, y) in enumerate(dloader):\n        data = Variable(x).cuda()\n        #data2 = Variable(x2).cuda()\n        target = Variable(y).cuda()\n        \n        #output = model(data,data2)\n        output = model(data)\n        \n        loss = loss_func(output, target.float())\n        los.append(loss.item())\n        \n        \n        if step %100 == 0:\n            print('[{}\/{} ({:.1f}%)]'.format(step * len(data), \n                                        len(dloader.dataset),\n                                        100.*step\/len(dloader)))\n        data.cpu()\n        #data2.cpu()\n        target.cpu()\n        torch.cuda.empty_cache()\n    los = np.array(los)\n    avg_val_loss = los.sum()\/len(los)\n    print(f\"Avg val loss: {avg_val_loss:.8f}\")\n    #return ans,out\n    ","470cf9a5":"class FocalLoss(nn.Module):\n    def __init__(self, gamma):\n        super().__init__()\n        self.gamma = gamma\n        \n    def forward(self, input, target):\n        # Inspired by the implementation of binary_cross_entropy_with_logits\n        if not (target.size() == input.size()):\n            raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(target.size(), input.size()))\n\n        max_val = (-input).clamp(min=0)\n        loss = input - input * target + max_val + ((-max_val).exp() + (-input - max_val).exp()).log()\n\n        # This formula gives us the log sigmoid of 1-p if y is 0 and of p if y is 1\n        invprobs = F.logsigmoid(-input * (target * 2 - 1))\n        loss = (invprobs * self.gamma).exp() * loss\n        \n        return loss.mean()","83c61e1b":"fold = KFold(n_splits = 4, random_state = 10)\nfor fold_num, (trn_idx, val_idx) in enumerate(fold.split(datafilter)):\n    Ftrain_dataloader = datafilter[trn_idx, :]\n    Fval_dataloader = datafilter[val_idx, :]\n\n    val_dataloader = DataLoader(trainDataset(Fval_dataloader, data_transforms,data_transforms2),batch_size=32, shuffle=False,num_workers=2, pin_memory=True)\n    train_dataloader = DataLoader(trainDataset(Ftrain_dataloader, data_transforms,data_transforms2),batch_size=32, shuffle=True,num_workers=2, pin_memory=True)\n    for epoch in range(3):\n        ###########################################\n        if epoch==0:\n            optimizer = torch.optim.Adam(model.parameters(), lr=0.0001\/(2**epoch))\n        else:\n            optimizer = torch.optim.SGD(model.parameters(), lr=0.0001,momentum=0.9, weight_decay=1e-4)\n        #optimizer = torch.optim.Adam(model.parameters(), lr=0.00002\/(2**epoch))\n        #optimizer = torch.optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=1e-4)\n        loss_func = FocalLoss(0.4)\n        #loss_func = torch.nn.MSELoss()\n        ###########################################\n        train(epoch,fold_num,train_dataloader) \n        val(val_dataloader)","03461631":"torch.cuda.empty_cache()\n!nvidia-smi","782e7fd7":"torch.save(model, 'net.pkl')\n#model = torch.load('net.pkl')","9d3a53d1":"def findPre(output,gate):\n    a = ''\n    output = np.array(output)\n    m = np.max(output)\n\n    for i in range(len(output)):\n        #s = np.where(v[i] > 0.95, 1, 0)\n        if output[i]>gate:\n            #print(output[i])\n            a = a + format(i)+' '\n            \n    #print(a)\n    return a\n    ","24a593ec":"def test(model,dloader,threshold):\n    model = model.eval().cuda()\n    #lengthD = len(dloader.dataset)\n    ans = []\n    out = []\n    for step, (x,x2, y) in enumerate(dloader):\n        data = Variable(x).cuda()\n        #data2 = Variable(x2).cuda()\n        #data = Variable(x)\n        target = y\n        #output = model(data,data2)\n        output = model(data).detach()\n        \n        #data.cpu()   # batch x\n        #data2.cpu()   # batch x\n        #target.cpu()   # batch y\n        #torch.cuda.empty_cache()\n        \n        v = output.cpu()\n        v = torch.sigmoid(v)\n        \n        v = torch.sigmoid(v)\n        v = np.array(v)\n        v = preprocessing.minmax_scale(v, feature_range=(0,1),axis=1)\n        for i in range(len(v)):\n            out.append(np.where(v[i] > threshold, 1, 0))\n            s = findPre(v[i],threshold)\n            ans.append([target[i],s])\n        if step %10 == 0:\n            print('[{}\/{} ({:.1f}%)]'.format(step * len(data), \n                                        len(dloader.dataset),\n                                        100.*step\/len(dloader)))\n            \n        data.cpu()\n        #data2.detach()\n        #target.detach()\n        torch.cuda.empty_cache()\n    print(\"Finish\")\n    return ans,out\n    ","dc6543cd":"from sklearn.metrics import fbeta_score\n\ndef makeScore(pre,ans):\n    pre = np.array(pre)\n    va = fbeta_score(y_pred=pre, y_true=ans, beta=2, average=\"samples\")\n    print(\"Score : {:.5f}\".format(va))\n    return va\n    ","6c48f528":"def findThreshold():\n    score = []\n    candidates = np.arange(0, 1.0, 0.01)\n    for th in candidates:\n        print(\"Threshold : {:.2f}\".format(th))\n        _,pre = test(model = model,dloader = score_dataloader,threshold = th)\n        #return pre\n        score.append(makeScore(np.array(pre),np.array(train_a[:1000,1].tolist())))\n        print(\"=============================\")\n    pm = np.array(score).argmax()\n    best_th, best_score = candidates[pm], score[pm]\n    return best_th, best_score","fc7e03a3":"bt, bs = findThreshold()\nprint(\"Best Threshold : {:.2f}\".format(bt))\nprint(\"Best Score : {:.5f}\".format(bs))","2deda4a9":"torch.cuda.empty_cache()\n!nvidia-smi","96bc3561":"sub,_ = test(model = model,dloader = test_dataloader,threshold = bt)","c7c692cd":"sub =  pd.DataFrame(sub)","d2c59a41":"sub = sub.rename(index=str, columns={0: \"id\", 1: \"attribute_ids\"})","79e8130a":"sub.head","2ae77bdc":"sub.to_csv('submission.csv', index=False)","6acab557":"## pytorch_seresnext101-32x4d+kfold+focal loss v3.0\n\n\n* fold = 4\n* epoch = 3\n* \u03b3=0.4","ee776448":"### Remove the item that only have one feature (not work well)"}}