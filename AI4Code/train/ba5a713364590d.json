{"cell_type":{"0cc0fa19":"code","084ff006":"code","a04691b1":"code","f3dc8b26":"code","ea126613":"code","c32aa4b3":"code","d1a70177":"code","46a6813e":"code","45dfae52":"code","eaae8d52":"code","f22cc67c":"code","a8addc1a":"code","f287d7da":"code","550adcfe":"code","ff421819":"code","ffc365ee":"code","70324ba9":"code","4dfff7aa":"code","1baf1667":"code","e5ba0030":"code","a97b94d6":"code","2eeefd3c":"code","09357e32":"code","e1024303":"code","9d1838d2":"code","6c073588":"code","4d557644":"code","65abbf20":"code","cd61fde4":"code","51d9186e":"code","1b8340fb":"code","4a4f1fad":"code","149b4b0a":"code","64e0349e":"code","893d246c":"code","931fb9d0":"code","9fd54c51":"code","724f0912":"code","d57470f3":"code","5e4b2271":"markdown","c8e27c18":"markdown","8dee4e2c":"markdown","280e515f":"markdown","eac8977d":"markdown","9ca5c9b5":"markdown","2561bdb3":"markdown","abcb0a7e":"markdown","823f50ff":"markdown","34a242fe":"markdown","d8f2a9d9":"markdown","0cc63d5e":"markdown","3246ebc9":"markdown","3745192c":"markdown","d67aad9a":"markdown","d7e76585":"markdown","7e1a0e7e":"markdown","5061894b":"markdown","bf7dbaed":"markdown"},"source":{"0cc0fa19":"# Jupyter Candies \n\n# Run the notebook readable w\/o wanrings.\n# P\/S: Not a good habit to do this, but squelching warnings \n#      so that the notebook is easier to demo.\nimport warnings\nwarnings.filterwarnings('ignore')","084ff006":"# Install the additional libaries.\n! pip install -U git+https:\/\/github.com\/huggingface\/transformers\n## ! pip install torch","a04691b1":"from itertools import chain\nfrom collections import namedtuple\n\nimport numpy as np\nimport torch\nfrom transformers import BertTokenizer, BertModel, BertForMaskedLM\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n","f3dc8b26":"from transformers import BertTokenizer, BertModel, BertForMaskedLM\n\n# Load pre-trained model tokenizer (vocabulary)\n# A tokenizer will split the text into the appropriate sub-parts (aka. tokens).\n# Depending on how the pre-trained model is trained, the tokenizers defers.\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n\n# Example of a tokenized input after WordPiece Tokenization.\ntext = \"[CLS] my dog is cute [SEP] he likes playing [SEP]\"\nprint(tokenizer.wordpiece_tokenizer.tokenize(text))","ea126613":"\"playing\" in tokenizer.wordpiece_tokenizer.vocab","c32aa4b3":"print(\"slacking\" in tokenizer.wordpiece_tokenizer.vocab)","d1a70177":"text = \"[CLS] my dog is cute [SEP] he likes slacking [SEP]\"\ntokenized_text = tokenizer.wordpiece_tokenizer.tokenize(text) # There, we see the ##ing token!\nprint(tokenized_text)","46a6813e":"token_indices = tokenizer.convert_tokens_to_ids(tokenized_text)\ntoken_indices","45dfae52":"import numpy as np\n\n# We need to create an array that indicates the end of sentences, delimited by [SEP]\ntext = \"[CLS] my dog is cute [SEP] he likes slacking [SEP]\"\ntokenized_text = tokenizer.wordpiece_tokenizer.tokenize(text)  # There, we see the ##ing token!\n\n# First we find the indices of `[SEP]`, and incrementally adds it up. \n# Here's some Numpy gymnastics... \n# Thanks to @divakar https:\/\/stackoverflow.com\/a\/58316889\/610569\nm = np.asarray(tokenized_text) == \"[SEP]\"\nsegments_ids = m.cumsum()-m\n\ntokens_tensor, segments_tensors = torch.tensor([token_indices]), torch.tensor([segments_ids])\n\n# See the type change?\nprint(tokens_tensor.shape, type(token_indices), type(tokens_tensor))\nprint(segments_tensors.shape, type(segments_ids), type(segments_tensors))","eaae8d52":"tokens_tensor, segments_tensors = torch.tensor([token_indices]), torch.tensor([segments_ids])\n\n# See the type change?\nprint(tokens_tensor.shape, type(token_indices), type(tokens_tensor))\nprint(segments_tensors.shape, type(segments_ids), type(segments_tensors))","f22cc67c":"import torch \n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# When using the BERT model for \"encoding\", i.e. convert string to array of floats, \n# we use the `BertModel` object from pytorch transformer library.\nmodel = BertModel.from_pretrained('bert-base-uncased')\nmodel.eval(); model.to(device)","a8addc1a":"# Predict hidden states features for each layer\nwith torch.no_grad():\n    encoded_layers, _ = model(tokens_tensor.to(device), segments_tensors.to(device))\n    \nprint(encoded_layers)","f287d7da":"encoded_layers.shape","550adcfe":"# Load the model.\nmodel = BertForMaskedLM.from_pretrained('bert-base-uncased')\nmodel.eval(); model.to(device)","ff421819":"# We need to create an array that indicates the end of sentences, delimited by [SEP]\ntext = \"[CLS] please don't let the [MASK] out of the [MASK] . [SEP]\"\ntokenized_text = tokenizer.wordpiece_tokenizer.tokenize(text)\ntoken_indices = tokenizer.convert_tokens_to_ids(tokenized_text)\n\n# Create the segment indices.\nm = np.asarray(tokenized_text) == \"[SEP]\"\nsegments_ids = m.cumsum()-m\n\n# Convert them to the arrays to pytorch tensors.\ntokens_tensor, segments_tensors = torch.tensor([token_indices]), torch.tensor([segments_ids])\n\n# Apply the model to the inputs.\nwith torch.no_grad(): # You can take this context manager to mean that we're not training.\n    outputs, *_ = model(tokens_tensor.to(device), \n                        token_type_ids=segments_tensors.to(device))\n\noutputs.shape","ffc365ee":"# Apply the model to the inputs.\nwith torch.no_grad(): # You can take this context manager to mean that we're not training.\n    outputs, *_ = model(tokens_tensor.to(device), token_type_ids=segments_tensors.to(device))","70324ba9":"outputs.shape","4dfff7aa":"print(tokenized_text)","1baf1667":"# Lets remember our original masked sentence.\nprint(tokenized_text)\n# We have to check where the masked token is from the original text. \nmask_index = tokenized_text.index('[MASK]') \nassert mask_index == 7 # The 7th token.\n\n# Then we fetch the vector for the 7th value, \n# The [0, mask_index] refers to accessing vector of vocab_size for\n# the 0th sentence, mask_index-th token.\noutput_value = outputs[0, mask_index]\n\n# As a sanity check we can see that the shape of the output_value\n# is the same as the `vocab_size` from the outputs' shape.\nassert int(output_value.shape[0]) == len(tokenizer.wordpiece_tokenizer.vocab)","e5ba0030":"# Lets recap the original sentence with the masked word.\nprint(text)\n\n# We have to check where the first masked token is from the original text. \nmask_index = tokenized_text.index('[MASK]') \noutput_value = outputs[0, mask_index]\n\n## We use torch.argmax to get the index with the highest value.\nmask_word_in_vocab = int(torch.argmax(output_value))\nprint(tokenizer.convert_ids_to_tokens([mask_word_in_vocab]))","a97b94d6":"# Lets recap the original sentence with the masked word.\nprint(text)\n\n# We have to check where the masked tokens are from the original text. \nfor mask_index, token in enumerate(tokenized_text):\n    if token == '[MASK]':\n        output_value = outputs[0, mask_index]\n        mask_word_in_vocab = int(torch.argmax(output_value))\n        print(tokenizer.convert_ids_to_tokens([mask_word_in_vocab]))","2eeefd3c":"def fill_in_the_blanks(text, model, tokenizer, return_str=False):\n    tokenized_text = tokenizer.wordpiece_tokenizer.tokenize(text)\n    token_indices = tokenizer.convert_tokens_to_ids(tokenized_text)\n    # Create the segment indices.\n    m = np.asarray(tokenized_text) == \"[SEP]\"\n    segments_ids = m.cumsum()-m\n    # Convert them to the arrays to pytorch tensors.\n    tokens_tensor = torch.tensor([token_indices]).to(device)\n    segments_tensors = torch.tensor([segments_ids]).to(device)\n    \n    # Apply the model to the inputs.\n    with torch.no_grad(): # You can take this context manager to mean that we're not training.\n        outputs, *_ = model(tokens_tensor, token_type_ids=segments_tensors)\n    \n    output_tokens = []\n    for mask_index, token_id in enumerate(token_indices):\n        token = tokenizer.convert_ids_to_tokens([token_id])[0]\n        if token == '[MASK]':\n            output_value = outputs[0, mask_index]\n            # The masked word index in the vocab.\n            mask_word_in_vocab = int(torch.argmax(output_value))\n            token = tokenizer.convert_ids_to_tokens([mask_word_in_vocab])[0]\n        output_tokens.append(token)\n        \n    return \" \".join(output_tokens).replace(\" ##\", \"\").replace(\" ' t \", \"'t \") if return_str else output_tokens","09357e32":"# Load the model.\nmodel = BertForMaskedLM.from_pretrained('bert-base-uncased')\nmodel.eval(); model.to(device)\n\ntext = \"[CLS] please don't let the [MASK] out of the [MASK] . [SEP]\"\nprint(fill_in_the_blanks(text, model, tokenizer, return_str=True))","e1024303":"text = \"[CLS] i like to drink beer and eat [MASK] . [SEP]\"\nprint(fill_in_the_blanks(text, model,tokenizer, return_str=True))","9d1838d2":"text = \"[CLS] i like to drink coffee and eat [MASK] . [SEP]\"\nprint(fill_in_the_blanks(text, model, tokenizer, return_str=True))","6c073588":"phoenix_turtle = \"\"\"Truth may seem but cannot be;\\nBeauty brag but \u2019tis not she;\\nTruth and beauty buried be.\"\"\"\nsonnet20 = \"\"\"A woman\u2019s face with Nature\u2019s own hand painted\\nHast thou, the master-mistress of my passion;\\nA woman\u2019s gentle heart, but not acquainte\\nWith shifting change, as is false women\u2019s fashion;\"\"\"\nsonnet1 = \"\"\"From fairest creatures we desire increase,\\nThat thereby beauty\u2019s rose might never die,\\nBut as the riper should by time decease,\\nHis tender heir might bear his memory:\"\"\"\nsonnet73 = \"\"\"In me thou see\u2019st the glowing of such fire,\\nThat on the ashes of his youth doth lie,\\nAs the death-bed whereon it must expire,\\nConsum\u2019d with that which it was nourish\u2019d by.\"\"\"\nvenus_adonis = \"\"\"It shall be cause of war and dire events,\\nAnd set dissension \u2018twixt the son and sire;\\nSubject and servile to all discontents,\\nAs dry combustious matter is to fire:\\nSith in his prime Death doth my love destroy,\\nThey that love best their loves shall not enjoy\\n\"\"\"\nsonnet29 = \"\"\"When, in disgrace with fortune and men\u2019s eyes,\\nI all alone beweep my outcast state,\\nAnd trouble deaf heaven with my bootless cries,\\nAnd look upon myself and curse my fate,\"\"\"\nsonnet130 = \"\"\"I have seen roses damask\u2019d, red and white,\\nBut no such roses see I in her cheeks;\\nAnd in some perfumes is there more delight\\nThan in the breath that from my mistress reeks.\"\"\"\nsonnet116 = \"\"\"Love\u2019s not Time\u2019s fool, though rosy lips and cheeks\\nWithin his bending sickle\u2019s compass come;\\nLove alters not with his brief hours and weeks,\\nBut bears it out even to the edge of doom.\"\"\"\nsonnet18 = \"\"\"But thy eternal summer shall not fade\\nNor lose possession of that fair thou ow\u2019st;\\nNor shall Death brag thou wander\u2019st in his shade,\\nWhen in eternal lines to time thou grow\u2019st;\\nSo long as men can breathe or eyes can see,\\nSo long lives this, and this gives life to thee.\"\"\"\nanthony_cleo = \"\"\"She made great Caesar lay his sword to bed;\\nHe plowed her, and she cropped.\"\"\"\n\nshakespeare = [phoenix_turtle, sonnet20, sonnet1, sonnet73, venus_adonis,\n              sonnet29, sonnet130, sonnet116, sonnet18, anthony_cleo]","4d557644":"from transformers import BertConfig, BertForMaskedLM, BertTokenizer\n\n# Load the BERT model.\nmodel = BertForMaskedLM.from_pretrained('bert-base-uncased')\nmodel.eval()\nmodel.to(device)\n# Load the BERT Tokenizer.\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n# Load the BERT Config.\nconfig = BertConfig.from_pretrained('bert-large-uncased')","65abbf20":"truth = \"that on the ashes of his youth doth lie\"\nmasked_text = \"[CLS] that on the ashes of his youth [MASK] lie\"\nprint(fill_in_the_blanks(masked_text, model, tokenizer, return_str=True))","cd61fde4":"from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\nimport torch.nn.functional as F\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, tokenizer):\n        \"\"\"\n        :param texts: A list of documents, each document is a list of strings.\n        :rtype texts: list(string)\n        \"\"\"\n        tokenization_process = lambda s: tokenizer.build_inputs_with_special_tokens(\n                                             tokenizer.convert_tokens_to_ids(\n                                                 tokenizer.tokenize(s.lower())))\n        pad_sent = lambda x: np.pad(x, (0,tokenizer.max_len_single_sentence - len(x)), 'constant', \n                                    constant_values=tokenizer.convert_tokens_to_ids(tokenizer.pad_token))\n        self.examples = torch.tensor([pad_sent(tokenization_process(doc)) for doc in texts])\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, item):\n        return torch.tensor(self.examples[item])\n","51d9186e":"# Initialize the Dataset object.\ntrain_dataset = TextDataset(shakespeare, tokenizer)\n# Initalize the DataLoader object, `batch_size=2` means reads 2 poems at a time.\ndataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=2)","1b8340fb":"# 10 poems with 510 tokens per poems, \n# if poem has <510, pad with the 0th index.\ntrain_dataset.examples.shape\n\n# For each batch, we read 2 poems at a time.\nprint(next(iter(dataloader)).shape)","4a4f1fad":"# An example of a batch.\nnext(iter(dataloader))","149b4b0a":"def mask_tokens(inputs, tokenizer, mlm_probability=0.8):\n    \"\"\" Prepare masked tokens inputs\/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"\n\n    if tokenizer.mask_token is None:\n        raise ValueError(\n            \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the --mlm flag if you want to use this tokenizer.\"\n        )\n\n    labels = inputs.clone()\n    # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert\/RoBERTa)\n    probability_matrix = torch.full(labels.shape, mlm_probability)\n    special_tokens_mask = [\n        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n    ]\n    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n    if tokenizer._pad_token is not None:\n        padding_mask = labels.eq(tokenizer.pad_token_id)\n        probability_matrix.masked_fill_(padding_mask, value=0.0)\n    masked_indices = torch.bernoulli(probability_matrix).bool()\n    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n\n    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n\n    # 10% of the time, we replace masked input tokens with random word\n    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n    inputs[indices_random] = random_words[indices_random]\n\n    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n    return inputs, labels","64e0349e":"from transformers.optimization import AdamW\nfrom transformers.optimization import get_linear_schedule_with_warmup as WarmupLinearSchedule\n\nArguments = namedtuple('Arguments', ['learning_rate', 'weight_decay', 'adam_epsilon', 'num_warmup_steps', \n                                     'max_steps', 'num_train_epochs'])\n\nargs = Arguments(learning_rate=5e-3, weight_decay=0.0, adam_epsilon=1e-8, num_warmup_steps=0, # Optimizer arguments\n                 max_steps=20, num_train_epochs=50  # Training routine arugments\n                )  \n\n# Prepare optimizer and schedule (linear warmup and decay)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n\noptimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\nscheduler = WarmupLinearSchedule(optimizer, num_warmup_steps=args.num_warmup_steps, num_training_steps=args.max_steps)","893d246c":"for _e in range(args.num_train_epochs):\n    print('Epoch:', _e, '\\t' ,end='')\n    for step, batch in enumerate(iter(dataloader)):\n        print(step, end=', ')\n\n        optimizer.zero_grad()\n        # Randomly mask the tokens 80% of the time. \n        inputs, labels = mask_tokens(batch, tokenizer)\n        inputs, labels = inputs.to(device), labels.to(device)\n        # Initialize the model to train mode.\n        model.train()\n        # Feed forward the inputs through the models.\n        loss, _ = model(inputs, masked_lm_labels=labels)\n\n        # Backpropagate the loss.\n        loss.backward()\n        # Step through the optimizer.\n        optimizer.step()\n    print()","931fb9d0":"truth = \"That on the ashes of his youth doth lie\"\nmasked_text = \"[CLS] That on the ashes of his youth [MASK] lie [SEP]\"\nprint(fill_in_the_blanks(masked_text, model, tokenizer, return_str=True))","9fd54c51":"labels","724f0912":"labels.to(device)","d57470f3":"inputs[1][0]","5e4b2271":"# Lets convert our input text to an array of number!!!","c8e27c18":"### Background: BERT Basics\n\nLets start with the elephant in the zoo!\n\n<img src=\"https:\/\/lilianweng.github.io\/lil-log\/assets\/images\/BERT-input-embedding.png\" alt=\"BERTInputs\" style=\"width:700px;\"\/>\n\nFirst the input string needs to prepended with the `[CLS]` token, this special token is used to allocate some placeholder than can be used to produce the labels for classification task. \n\nThen, for each sentence that's inside the text string, explicit `[SEP]` tokens need to be added to indicate on of a sentence. \n\n\nThen string input needs to be converted to three components before passing them to Transformer model:\n\n - **WordPiece tokenization**: The text (string) input would be split into tokens segmented using the WordPiece model that may split natural words further into sub-words units to handle rare\/unknown words. \n \n - **Segment Indices**: This part indicates the start and end of the sentences in the string inputs, delimited by the special `[SEP]` token.\n \n - **Position Indices**: This part simply enumerates the index of WordPiece tokens. ","8dee4e2c":"We see that the shape is 3-Dimension, i.e. (`batch_size`, `sequence_length`, `hidden_dimension`), where\n\n - `batch_size` corresponds to \"no. of sentences\"\n - `sequence_length` corresponds to \"no. of tokens\"\n - `hidden_dimensions` refers to the \"information for each word provided by the pre-trained model\"","280e515f":"# Introduction\n\nThe first section of this notebook will first ***very*** briefly introduce some background concepts that's good to know about \n\n- The ImageNet Moment in NLP\n- A Zoo of Pre-trained Models\n- BERT (Bidirectional Encoder Representation from Transformers) Basics, one of the more popular transfer learning models for NLP and \n\n\nThe second section** demonstrates how you can the BERT model from `pytorch_transformer` library to: \n\n1. **Convert text to array\/list of floats** \n2. **Fill in the blanks** \n\n<!--\n3. **Fine-tune the pre-trained model** based on the data you want to use for a specific task\n4. **Apply the fine-tuned model** to a couple of downstream tasks\n-->\n\n### References\n\nI'll strongly recommend these readings to better understand\/appreciate the first part of the notebook =)\n\n - [Rush (2018) blogpost](https:\/\/nlp.seas.harvard.edu\/2018\/04\/03\/attention.html) on \"The Annotated Transformer\" that explains the explaining the Transformer architecture \n - [Ruder et al. (2019) tutorial](http:\/\/ruder.io\/state-of-transfer-learning-in-nlp\/index.html) on \"Transfer Learning in NLP\" @ NAACL\n - [Weng (2019) blogpost](https:\/\/lilianweng.github.io\/lil-log\/2019\/01\/31\/generalized-language-models.html) on \"Generalized Language Models\"\n - https:\/\/github.com\/huggingface\/transformers\n - https:\/\/github.com\/explosion\/spacy-transformers\n ","eac8977d":"### Gotcha! The output is differen from texample in the image above!!\n\nThat's because the full word `playing` is inside the `BertTokenizer`'s WordPiece vocabulary.","9ca5c9b5":"Now we see that the output tensor shape is different. The dimensions now refers to the (`batch_size`, `sequence_length`, `vocab_size`), where: \n\n - `batch_size` corresponds to \"no. of sentences\"\n - `sequence_length` corresponds to \"no. of tokens\"\n - `vocab_size` is the no. of wordpiece tokens in the tokenizer's vocabulary, we'll use this to fetch the correct word that we want to use to fill in the `[MASK]` symbol.","2561bdb3":"## Background: A Zoo of Pre-trained Models\n\nThere's a whole variety of transfer learning pre-trained models in the wild. [Sanh et al. (2019)](https:\/\/arxiv.org\/abs\/1910.01108) puts them nicely into a chart of the no. of parameters* of the model with respect to the dates the models were released: \n\n<img src=\"https:\/\/miro.medium.com\/max\/4140\/1*IFVX74cEe8U5D1GveL1uZA.png\" alt=\"DistilBERT\" style=\"width:700px;\"\/>\n\n***Note:** \"*Parameters*\" approximates to how much \"*memory*\"\/\"*information*\" the model is storing after pre-training.\n\n\n<!-- \nHere's a summary inspired by [Weng's (2019) blogpost](https:\/\/lilianweng.github.io\/lil-log\/2019\/01\/31\/generalized-language-models.html):\n\n| Name | Architecture | Autoregressive | No. of Parameters | Release Date | Pre-training | Downstream tasks | Downstream Model | \n|:-|:-|:-:|:-:|:-:|:-:|:-:|:-|\n| [ELMo](https:\/\/allennlp.org\/elmo) | 2-layers BiLSTM | Yes | 94M | Apr 2018 | Unsupervised | Feature-based | Task-agnostic | None | \n| [ULMFit](http:\/\/nlp.fast.ai\/classification\/2018\/05\/15\/introducing-ulmfit.html) | AWD-LSTM | Yes | ?? | Apr 2018 | Unsupervised | Feature-based | Task-agnostic | None | \n| [GPT](https:\/\/openai.com\/blog\/language-unsupervised\/) | Transformer Decoder | Yes | 110M | Jul 2018 | Unsupervised | Model-baed | Task-agnostic | Pre-trained layers + Task layers | \n| [BERT](https:\/\/ai.googleblog.com\/2018\/11\/open-sourcing-bert-state-of-art-pre.html) | Transformer Encoder | No | 340M | Oct 2018 | Unsupervised | Model-based | Task-agnostic | Pre-trained layers + Task layers | \n| [Transfomer ElMo](https:\/\/github.com\/allenai\/allennlp\/blob\/master\/tutorials\/how_to\/training_transformer_elmo.md) | Transformer Decoder | Yes | 465M | Jan 2019 | Unsupervised | Task-agnostic | Pre-trained layers + Task layers | \n| [GPT-2](https:\/\/openai.com\/blog\/better-language-models\/) | Transformer Decoder | Yes | 1500M | Feb 2019 | Unsupervised | Model-baed | Task-agnostic | Pre-trained layers + Task layers | \n\n-->","abcb0a7e":"# It'll take some time to digest this `mask_tokens` function. \n\nBut all it does is that 80% of the time, it'll mask the tokens, 10% of the time it'll random replace the masked word with another word, and 10% of the time it'll keep the original. You can read more in the original implementation at https:\/\/github.com\/huggingface\/transformers\/blob\/master\/examples\/run_lm_finetuning.py#L143","823f50ff":"### Now, we convert the list and numpy arrays to PyTorch's Tensor objects","34a242fe":"# Fine-tuning BERT models. \n\nBy default, the pre-trained model is trained on the\n\n - BookCorpus, ~800M words\n - English Wikipedia, ~2500M words\n \nIf we want the model to adapt to a specific domain, we need to ***fine-tune*** the model. This section demonstrate how this can be done with the same PyTorch Transformer Library.\n\n### Lets use some Shakespeare poems...\n","d8f2a9d9":"### Corresponding to the text input, we need to create the \"segment indices\"","0cc63d5e":"# Lets make the fill-in-the-blank feature into a function.","3246ebc9":"### First, we load the pre-trained model","3745192c":"# The BERT model is very good at fill-in-the-blank task\n\nThe BERT model is trained using a \"cloze\" task where words are randomly replaced with the `[MASK]` symbols and the model learns to adjust its parameters such that it learns which words are most probable to fit into the `[MASK]` symbols.\n\nWhen using the BERT model for \"guessing missing words\", we use the `BertForMaskedLM` object from pytorch transformer library. Here's an example if we blank out words in the sentence, BERT is able to find the appropriate word to fill it in.","d67aad9a":"### We fetch the index of these words from the model's vocabulary. ","d7e76585":"### Lets try another verb that's not in the vocabulary.","7e1a0e7e":"## Hmmmm, thou hast not read Shakespeare enough.\n\nLets try to fine-tuned our model to shakespearian text. \n\nTo do so, here's some boilerplates to munge data into an object use by the model training routine.\n","5061894b":"### And if we try \"fill-in-the-blanks\" on Shakepearian text...","bf7dbaed":"## Background: The ImageNet Moment for NLP \n\n\nTransfer learning gained traction in Computer Vision, made popular by the [ImageNet](http:\/\/www.image-net.org) and [CIFAR](https:\/\/www.cs.toronto.edu\/~kriz\/cifar.html) image classification task. Similarly, transfer learning gained popularity when a wave of Transformer based models, with the BERT model being the more popular one from the zoo.\n"}}