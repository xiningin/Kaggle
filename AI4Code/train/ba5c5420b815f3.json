{"cell_type":{"69cbb522":"code","c3068160":"code","b5a8f67e":"code","5f79ed4e":"code","242f893c":"code","17c9a912":"code","8efe94fa":"code","b1a1a6c8":"code","aadae03e":"code","e3a12c8e":"code","d3d1b51a":"code","241fa7a1":"code","122f1085":"code","54ffc039":"code","151d76b1":"code","a4297c71":"code","764827d7":"code","88485e1d":"code","8584cb13":"code","cfda93ca":"code","36b38601":"code","fc4b3f66":"code","3dfd1d5e":"code","a4e90b0e":"code","2936fd48":"code","cb01cb08":"code","f8f2076d":"code","cb5c49e8":"code","e7959abc":"code","447a5007":"code","dbdec6fb":"code","a2c05091":"code","36ffa142":"code","d5f1f033":"code","b1ff8957":"code","864b6ce5":"markdown","615719bd":"markdown","8f0e820f":"markdown","03ff02dc":"markdown","e1bd6eec":"markdown","681dc0cd":"markdown","eaf07526":"markdown","7a97c354":"markdown","ec2ba85c":"markdown","b4eb46b5":"markdown","1aa5810e":"markdown","4be5cb10":"markdown","8ab7500d":"markdown","1da20b91":"markdown"},"source":{"69cbb522":"# General imports\nimport numpy as np\nimport pandas as pd\nimport os, sys, gc, time, warnings, pickle, psutil, random\n\nfrom math import ceil\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport time\n\nwarnings.filterwarnings('ignore')","c3068160":"## Simple \"Memory profilers\" to see memory usage\ndef get_memory_usage():\n    return np.round(psutil.Process(os.getpid()).memory_info()[0]\/2.**30, 2) \n        \ndef sizeof_fmt(num, suffix='B'):\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f%s%s\" % (num, unit, suffix)\n        num \/= 1024.0\n    return \"%.1f%s%s\" % (num, 'Yi', suffix)","b5a8f67e":"## Memory Reducer\n# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n# :verbose                                        # type: bool\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                       df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","5f79ed4e":"## Merging by concat to not lose dtypes\ndef merge_by_concat(df1, df2, merge_on):\n    merged_gf = df1[merge_on]\n    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n    return df1","242f893c":"########################### Vars\n#################################################################################\nTARGET = 'sales'         # Our main target\nEND_TRAIN = 1913+28         # Last day in train set\nMAIN_INDEX = ['id','d']  # We can identify item by these columns","17c9a912":"########################### Load Data\n#################################################################################\nprint('Load Main Data')\n\n# Here are reafing all our data \n# without any limitations and dtype modification\ntrain_df = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv')\nprices_df = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sell_prices.csv')\ncalendar_df = pd.read_csv('..\/input\/m5-forecasting-accuracy\/calendar.csv')","8efe94fa":"########################### Make Grid\n#################################################################################\nprint('Create Grid')\n\n# We can tranform horizontal representation \n# to vertical \"view\"\n# Our \"index\" will be 'id','item_id','dept_id','cat_id','store_id','state_id'\n# and labels are 'd_' coulmns\n\nindex_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\ngrid_df = pd.melt(train_df, \n                  id_vars = index_columns, \n                  var_name = 'd', \n                  value_name = TARGET)\n\n# If we look on train_df we se that \n# we don't have a lot of traning rows\n# but each day can provide more train data\nprint('Train rows:', len(train_df), len(grid_df))\n\n# To be able to make predictions\n# we need to add \"test set\" to our grid\nadd_grid = pd.DataFrame()\nfor i in range(1,29):\n    temp_df = train_df[index_columns]\n    temp_df = temp_df.drop_duplicates()\n    temp_df['d'] = 'd_'+ str(END_TRAIN+i)\n    temp_df[TARGET] = np.nan\n    add_grid = pd.concat([add_grid,temp_df])\n\ngrid_df = pd.concat([grid_df,add_grid])\ngrid_df = grid_df.reset_index(drop=True)\n\n# Remove some temoprary DFs\ndel temp_df, add_grid\n\n# We will not need original train_df\n# anymore and can remove it\ndel train_df\n\n# You don't have to use df = df construction\n# you can use inplace=True instead.\n# like this\n# grid_df.reset_index(drop=True, inplace=True)\n\n# Let's check our memory usage\nprint(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n\n# We can free some memory \n# by converting \"strings\" to categorical\n# it will not affect merging and \n# we will not lose any valuable data\nfor col in index_columns:\n    grid_df[col] = grid_df[col].astype('category')\n\n# Let's check again memory usage\nprint(\"{:>20}: {:>8}\".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))","b1a1a6c8":"########################### Product Release date\n#################################################################################\nprint('Release week')\n\n# It seems that leadings zero values\n# in each train_df item row\n# are not real 0 sales but mean\n# absence for the item in the store\n# we can safe some memory by removing\n# such zeros\n\n# Prices are set by week\n# so it we will have not very accurate release week \nrelease_df = prices_df.groupby(['store_id','item_id'])['wm_yr_wk'].agg(['min']).reset_index()\nrelease_df.columns = ['store_id','item_id','release']\n\n# Now we can merge release_df\ngrid_df = merge_by_concat(grid_df, release_df, ['store_id','item_id'])\ndel release_df\n\n# We want to remove some \"zeros\" rows\n# from grid_df \n# to do it we need wm_yr_wk column\n# let's merge partly calendar_df to have it\ngrid_df = merge_by_concat(grid_df, calendar_df[['wm_yr_wk','d']], ['d'])\n                      \n# Now we can cutoff some rows \n# and safe memory \ngrid_df = grid_df[grid_df['wm_yr_wk']>=grid_df['release']]\ngrid_df = grid_df.reset_index(drop=True)\n\n# Let's check our memory usage\nprint(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n\n# Should we keep release week \n# as one of the features?\n# Only good CV can give the answer.\n# Let's minify the release values.\n# Min transformation will not help here \n# as int16 -> Integer (-32768 to 32767)\n# and our grid_df['release'].max() serves for int16\n# but we have have an idea how to transform \n# other columns in case we will need it\ngrid_df['release'] = grid_df['release'] - grid_df['release'].min()\ngrid_df['release'] = grid_df['release'].astype(np.int16)\n\n# Let's check again memory usage\nprint(\"{:>20}: {:>8}\".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))","aadae03e":"########################### Save part 1\n#################################################################################\nprint('Save Part 1')\n\n# We have our BASE grid ready\n# and can save it as pickle file\n# for future use (model training)\ngrid_df.to_pickle('grid_part_1.pkl')\n\nprint('Size:', grid_df.shape)","e3a12c8e":"########################### Prices\n#################################################################################\nprint('Prices')\n\n# We can do some basic aggregations\nprices_df['price_max'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('max')\nprices_df['price_min'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('min')\nprices_df['price_std'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('std')\nprices_df['price_mean'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('mean')\n\n# and do price normalization (min\/max scaling)\nprices_df['price_norm'] = prices_df['sell_price']\/prices_df['price_max']\n\n# Some items are can be inflation dependent\n# and some items are very \"stable\"\nprices_df['price_nunique'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('nunique')\nprices_df['item_nunique'] = prices_df.groupby(['store_id','sell_price'])['item_id'].transform('nunique')\n\n# I would like some \"rolling\" aggregations\n# but would like months and years as \"window\"\ncalendar_prices = calendar_df[['wm_yr_wk','month','year']]\ncalendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\nprices_df = prices_df.merge(calendar_prices[['wm_yr_wk','month','year']], on=['wm_yr_wk'], how='left')\ndel calendar_prices\n\n# Now we can add price \"momentum\" (some sort of)\n# Shifted by week \n# by month mean\n# by year mean\nprices_df['price_momentum'] = prices_df['sell_price']\/prices_df.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1))\nprices_df['price_momentum_m'] = prices_df['sell_price']\/prices_df.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\nprices_df['price_momentum_y'] = prices_df['sell_price']\/prices_df.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')\n\ndel prices_df['month'], prices_df['year']","d3d1b51a":"# added because kaggle notebook canont allocate more memory than it's available at this point\ngrid_df = reduce_mem_usage(grid_df)\nprices_df = reduce_mem_usage(prices_df)","241fa7a1":"########################### Merge prices and save part 2\n#################################################################################\nprint('Merge prices and save part 2')\n\n# Merge Prices\noriginal_columns = list(grid_df)\ngrid_df = grid_df.merge(prices_df, on=['store_id','item_id','wm_yr_wk'], how='left')\nkeep_columns = [col for col in list(grid_df) if col not in original_columns]\ngrid_df = grid_df[MAIN_INDEX+keep_columns]\ngrid_df = reduce_mem_usage(grid_df)\n\n# Save part 2\ngrid_df.to_pickle('grid_part_2.pkl')\nprint('Size:', grid_df.shape)\n\n# We don't need prices_df anymore\ndel prices_df\n\n# We can remove new columns\n# or just load part_1\ngrid_df = pd.read_pickle('grid_part_1.pkl')","122f1085":"########################### Merge calendar\n#################################################################################\ngrid_df = grid_df[MAIN_INDEX]\n\n# Merge calendar partly\nicols = ['date',\n         'd',\n         'event_name_1',\n         'event_type_1',\n         'event_name_2',\n         'event_type_2',\n         'snap_CA',\n         'snap_TX',\n         'snap_WI']\n\ngrid_df = grid_df.merge(calendar_df[icols], on=['d'], how='left')\n\n# Minify data\n# 'snap_' columns we can convert to bool or int8\nicols = ['event_name_1',\n         'event_type_1',\n         'event_name_2',\n         'event_type_2',\n         'snap_CA',\n         'snap_TX',\n         'snap_WI']\nfor col in icols:\n    grid_df[col] = grid_df[col].astype('category')\n\n# Convert to DateTime\ngrid_df['date'] = pd.to_datetime(grid_df['date'])\n\n# Make some features from date\ngrid_df['tm_d'] = grid_df['date'].dt.day.astype(np.int8)\ngrid_df['tm_w'] = grid_df['date'].dt.week.astype(np.int8)\ngrid_df['tm_m'] = grid_df['date'].dt.month.astype(np.int8)\ngrid_df['tm_y'] = grid_df['date'].dt.year\ngrid_df['tm_y'] = (grid_df['tm_y'] - grid_df['tm_y'].min()).astype(np.int8)\ngrid_df['tm_wm'] = grid_df['tm_d'].apply(lambda x: ceil(x\/7)).astype(np.int8)\n\ngrid_df['tm_dw'] = grid_df['date'].dt.dayofweek.astype(np.int8)\ngrid_df['tm_w_end'] = (grid_df['tm_dw']>=5).astype(np.int8)\n\n# Remove date\ndel grid_df['date']","54ffc039":"########################### Save part 3 (Dates)\n#################################################################################\nprint('Save part 3')\n\n# Safe part 3\ngrid_df.to_pickle('grid_part_3.pkl')\nprint('Size:', grid_df.shape)\n\n# We don't need calendar_df anymore\ndel calendar_df\ndel grid_df","151d76b1":"########################### Some additional cleaning\n#################################################################################\n\n## Part 1\n# Convert 'd' to int\ngrid_df = pd.read_pickle('grid_part_1.pkl')\ngrid_df['d'] = grid_df['d'].apply(lambda x: x[2:]).astype(np.int16)\n\n# Remove 'wm_yr_wk'\n# as test values are not in train set\ndel grid_df['wm_yr_wk']\ngrid_df.to_pickle('grid_part_1.pkl')\n\ndel grid_df","a4297c71":"########################### Summary\n#################################################################################\n\n# Now we have 3 sets of features\ngrid_df = pd.concat([pd.read_pickle('grid_part_1.pkl'),\n                     pd.read_pickle('grid_part_2.pkl').iloc[:,2:],\n                     pd.read_pickle('grid_part_3.pkl').iloc[:,2:]],\n                     axis=1)\n                     \n# Let's check again memory usage\nprint(\"{:>20}: {:>8}\".format('Full Grid',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\nprint('Size:', grid_df.shape)\n\n# 2.5GiB + is is still too big to train our model\n# (on kaggle with its memory limits)\n# and we don't have lag features yet\n# But what if we can train by state_id or shop_id?\nstate_id = 'CA'\ngrid_df = grid_df[grid_df['state_id']==state_id]\nprint(\"{:>20}: {:>8}\".format('Full Grid',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n#           Full Grid:   1.2GiB\n\nstore_id = 'CA_1'\ngrid_df = grid_df[grid_df['store_id']==store_id]\nprint(\"{:>20}: {:>8}\".format('Full Grid',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n#           Full Grid: 321.2MiB\n\n# Seems its good enough now\n# In other kernel we will talk about LAGS features\n# Thank you.","764827d7":"########################### Final list of features\n#################################################################################\ngrid_df.info()","88485e1d":"pd.set_option('display.max_columns', None)\ngrid_df.tail()","8584cb13":"grid_df.head()","cfda93ca":"del grid_df","36b38601":"########################### Apply on grid_df\n#################################################################################\n# lets read grid from \n# https:\/\/www.kaggle.com\/kyakovlev\/m5-simple-fe\n# to be sure that our grids are aligned by index\ngrid_df = pd.read_pickle('grid_part_1.pkl')\n\n# We need only 'id','d','sales'\n# to make lags and rollings\ngrid_df = grid_df[['id','d','sales']]\nSHIFT_DAY = 28\n\n# Lags\n# with 28 day shift\nstart_time = time.time()\nprint('Create lags')\n\nLAG_DAYS = [col for col in range(SHIFT_DAY,SHIFT_DAY+15)]\ngrid_df = grid_df.assign(**{\n        '{}_lag_{}'.format(col, l): grid_df.groupby(['id'])[col].transform(lambda x: x.shift(l))\n        for l in LAG_DAYS\n        for col in [TARGET]\n    })\n\n# Minify lag columns\nfor col in list(grid_df):\n    if 'lag' in col:\n        grid_df[col] = grid_df[col].astype(np.float16)\n\nprint('%0.2f min: Lags' % ((time.time() - start_time) \/ 60))\n\n# Rollings\n# with 28 day shift\nstart_time = time.time()\nprint('Create rolling aggs')\n\nfor i in [7,14,30,60,180]:\n    print('Rolling period:', i)\n    grid_df['rolling_mean_'+str(i)] = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).mean()).astype(np.float16)\n    grid_df['rolling_std_'+str(i)]  = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).std()).astype(np.float16)\n\n# Rollings\n# with sliding shift\nfor d_shift in [1,7,14]: \n    print('Shifting period:', d_shift)\n    for d_window in [7,14,30,60]:\n        col_name = 'rolling_mean_tmp_'+str(d_shift)+'_'+str(d_window)\n        grid_df[col_name] = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(d_shift).rolling(d_window).mean()).astype(np.float16)\n    \n    \nprint('%0.2f min: Lags' % ((time.time() - start_time) \/ 60))","fc4b3f66":"########################### Export\n#################################################################################\nprint('Save lags and rollings')\ngrid_df.to_pickle('lags_df_'+str(SHIFT_DAY)+'.pkl')","3dfd1d5e":"########################### Final list of new features\n#################################################################################\ngrid_df.info()","a4e90b0e":"del grid_df","2936fd48":"########################### Apply on grid_df\n#################################################################################\n# lets read grid from \n# https:\/\/www.kaggle.com\/kyakovlev\/m5-simple-fe\n# to be sure that our grids are aligned by index\ngrid_df = pd.read_pickle('.\/grid_part_1.pkl')\ngrid_df[TARGET][grid_df['d']>(1913)] = np.nan\nbase_cols = list(grid_df)\n\nicols =  [\n            ['state_id'],\n            ['store_id'],\n            ['cat_id'],\n            ['dept_id'],\n            ['state_id', 'cat_id'],\n            ['state_id', 'dept_id'],\n            ['store_id', 'cat_id'],\n            ['store_id', 'dept_id'],\n            ['item_id'],\n            ['item_id', 'state_id'],\n            ['item_id', 'store_id']\n            ]\n\nfor col in icols:\n    print('Encoding', col)\n    col_name = '_'+'_'.join(col)+'_'\n    grid_df['enc'+col_name+'mean'] = grid_df.groupby(col)[TARGET].transform('mean').astype(np.float16)\n    grid_df['enc'+col_name+'std'] = grid_df.groupby(col)[TARGET].transform('std').astype(np.float16)\n\nkeep_cols = [col for col in list(grid_df) if col not in base_cols]\ngrid_df = grid_df[['id','d']+keep_cols]","cb01cb08":"#################################################################################\nprint('Save Mean\/Std encoding')\ngrid_df.to_pickle(processed_data_dir+'mean_encoding_df.pkl')","f8f2076d":"grid_df.info()","cb5c49e8":"del grid_df","e7959abc":"########################### Helpers\n#################################################################################\n## Seeder\n# :seed to make all processes deterministic     # type: int\ndef seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n\n    \n## Multiprocess Runs\ndef df_parallelize_run(func, t_split):\n    num_cores = np.min([N_CORES,len(t_split)])\n    pool = Pool(num_cores)\n    df = pd.concat(pool.map(func, t_split), axis=1)\n    pool.close()\n    pool.join()\n    return df","447a5007":"########################### Helper to load data by store ID\n#################################################################################\n# Read data\ndef get_data_by_store(store):\n    \n    # Read and contact basic feature\n    df = pd.concat([pd.read_pickle('.\/grid_part_1.pkl'),\n                    pd.read_pickle('.\/grid_part_2.pkl').iloc[:,2:],\n                    pd.read_pickle('.\/grid_part_3.pkl').iloc[:,2:]],\n                    axis=1)\n    \n    # Leave only relevant store\n    df = df[df['store_id']==store]\n\n    # With memory limits we have to read \n    # lags and mean encoding features\n    # separately and drop items that we don't need.\n    # As our Features Grids are aligned \n    # we can use index to keep only necessary rows\n    # Alignment is good for us as concat uses less memory than merge.\n    df2 = pd.read_pickle(MEAN_ENC)[mean_features]\n    df2 = df2[df2.index.isin(df.index)]\n    \n    df3 = pd.read_pickle(LAGS).iloc[:,3:]\n    df3 = df3[df3.index.isin(df.index)]\n    \n    df = pd.concat([df, df2], axis=1)\n    del df2 # to not reach memory limit \n    \n    df = pd.concat([df, df3], axis=1)\n    del df3 # to not reach memory limit \n    \n    # Create features list\n    features = [col for col in list(df) if col not in remove_features]\n    df = df[['id','d',TARGET]+features]\n    \n    # Skipping first n rows\n    df = df[df['d']>=START_TRAIN].reset_index(drop=True)\n    \n    return df, features\n\n# Recombine Test set after training\ndef get_base_test():\n    base_test = pd.DataFrame()\n\n    for store_id in STORES_IDS:\n        temp_df = pd.read_pickle('test_'+store_id+'.pkl')\n        temp_df['store_id'] = store_id\n        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n    \n    return base_test\n\n\n########################### Helper to make dynamic rolling lags\n#################################################################################\ndef make_lag(LAG_DAY):\n    lag_df = base_test[['id','d',TARGET]]\n    col_name = 'sales_lag_'+str(LAG_DAY)\n    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n    return lag_df[[col_name]]\n\n\ndef make_lag_roll(LAG_DAY):\n    shift_day = LAG_DAY[0]\n    roll_wind = LAG_DAY[1]\n    lag_df = base_test[['id','d',TARGET]]\n    col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n    return lag_df[[col_name]]","dbdec6fb":"########################### Model params\n#################################################################################\nimport lightgbm as lgb\nlgb_params = {\n                    'boosting_type': 'gbdt',\n                    'objective': 'tweedie',\n                    'tweedie_variance_power': 1.1,\n                    'metric': 'rmse',\n                    'subsample': 0.5,\n                    'subsample_freq': 1,\n                    'learning_rate': 0.03,\n                    'num_leaves': 2**11-1,\n                    'min_data_in_leaf': 2**12-1,\n                    'feature_fraction': 0.5,\n                    'max_bin': 100,\n                    'n_estimators': 1400,\n                    'boost_from_average': False,\n                    'verbose': -1,\n                } \n\n# Let's look closer on params\n\n## 'boosting_type': 'gbdt'\n# we have 'goss' option for faster training\n# but it normally leads to underfit.\n# Also there is good 'dart' mode\n# but it takes forever to train\n# and model performance depends \n# a lot on random factor \n# https:\/\/www.kaggle.com\/c\/home-credit-default-risk\/discussion\/60921\n\n## 'objective': 'tweedie'\n# Tweedie Gradient Boosting for Extremely\n# Unbalanced Zero-inflated Data\n# https:\/\/arxiv.org\/pdf\/1811.10192.pdf\n# and many more articles about tweediie\n#\n# Strange (for me) but Tweedie is close in results\n# to my own ugly loss.\n# My advice here - make OWN LOSS function\n# https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/discussion\/140564\n# https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/discussion\/143070\n# I think many of you already using it (after poisson kernel appeared) \n# (kagglers are very good with \"params\" testing and tuning).\n# Try to figure out why Tweedie works.\n# probably it will show you new features options\n# or data transformation (Target transformation?).\n\n## 'tweedie_variance_power': 1.1\n# default = 1.5\n# set this closer to 2 to shift towards a Gamma distribution\n# set this closer to 1 to shift towards a Poisson distribution\n# my CV shows 1.1 is optimal \n# but you can make your own choice\n\n## 'metric': 'rmse'\n# Doesn't mean anything to us\n# as competition metric is different\n# and we don't use early stoppings here.\n# So rmse serves just for general \n# model performance overview.\n# Also we use \"fake\" validation set\n# (as it makes part of the training set)\n# so even general rmse score doesn't mean anything))\n# https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/discussion\/133834\n\n## 'subsample': 0.5\n# Serves to fight with overfit\n# this will randomly select part of data without resampling\n# Chosen by CV (my CV can be wrong!)\n# Next kernel will be about CV\n\n##'subsample_freq': 1\n# frequency for bagging\n# default value - seems ok\n\n## 'learning_rate': 0.03\n# Chosen by CV\n# Smaller - longer training\n# but there is an option to stop \n# in \"local minimum\"\n# Bigger - faster training\n# but there is a chance to\n# not find \"global minimum\" minimum\n\n## 'num_leaves': 2**11-1\n## 'min_data_in_leaf': 2**12-1\n# Force model to use more features\n# We need it to reduce \"recursive\"\n# error impact.\n# Also it leads to overfit\n# that's why we use small \n# 'max_bin': 100\n\n## l1, l2 regularizations\n# https:\/\/towardsdatascience.com\/l1-and-l2-regularization-methods-ce25e7fc831c\n# Good tiny explanation\n# l2 can work with bigger num_leaves\n# but my CV doesn't show boost\n                    \n## 'n_estimators': 1400\n# CV shows that there should be\n# different values for each state\/store.\n# Current value was chosen \n# for general purpose.\n# As we don't use any early stopings\n# careful to not overfit Public LB.\n\n##'feature_fraction': 0.5\n# LightGBM will randomly select \n# part of features on each iteration (tree).\n# We have maaaany features\n# and many of them are \"duplicates\"\n# and many just \"noise\"\n# good values here - 0.5-0.7 (by CV)\n\n## 'boost_from_average': False\n# There is some \"problem\"\n# to code boost_from_average for \n# custom loss\n# 'True' makes training faster\n# BUT carefull use it\n# https:\/\/github.com\/microsoft\/LightGBM\/issues\/1514\n# not our case but good to know cons","a2c05091":"########################### Vars\n#################################################################################\nVER = 1                          # Our model version\nSEED = 42                        # We want all things\nseed_everything(SEED)            # to be as deterministic \nlgb_params['seed'] = SEED        # as possible\nN_CORES = psutil.cpu_count()     # Available CPU cores\n\n\n#LIMITS and const\nTARGET      = 'sales'            # Our target\nSTART_TRAIN = 0                  # We can skip some rows (Nans\/faster training)\nEND_TRAIN   = 1913+28            # End day of our train set\nP_HORIZON   = 28                 # Prediction horizon\nUSE_AUX     = False               # Use or not pretrained models\n\n#FEATURES to remove\n## These features lead to overfit\n## or values not present in test set\nremove_features = ['id','state_id','store_id',\n                   'date','wm_yr_wk','d',TARGET]\nmean_features   = ['enc_cat_id_mean','enc_cat_id_std',\n                   'enc_dept_id_mean','enc_dept_id_std',\n                   'enc_item_id_mean','enc_item_id_std'] \n\n#PATHS for Features\nORIGINAL = '..\/input\/m5-forecasting-accuracy\/'\nBASE     = '.\/grid_part_1.pkl'\nPRICE    = '.\/grid_part_2.pkl'\nCALENDAR = '.\/grid_part_3.pkl'\nLAGS     = '.\/lags_df_28.pkl'\nMEAN_ENC = '.\/mean_encoding_df.pkl'\n\n\n# AUX(pretrained) Models paths\n#AUX_MODELS = '..\/input\/m5-aux-models\/'\n\n\n#STORES ids\nSTORES_IDS = pd.read_csv(ORIGINAL+'sales_train_validation.csv')['store_id']\nSTORES_IDS = list(STORES_IDS.unique())\n\n\n#SPLITS for lags creation\nSHIFT_DAY  = 28\nN_LAGS     = 15\nLAGS_SPLIT = [col for col in range(SHIFT_DAY,SHIFT_DAY+N_LAGS)]\nROLS_SPLIT = []\nfor i in [1,7,14]:\n    for j in [7,14,30,60]:\n        ROLS_SPLIT.append([i,j])","36ffa142":"########################### Train Models\n#################################################################################\nfor store_id in STORES_IDS:\n    print('Train', store_id)\n    \n    # Get grid for current store\n    grid_df, features_columns = get_data_by_store(store_id)\n    \n    # Masks for \n    # Train (All data less than 1913)\n    # \"Validation\" (Last 28 days - not real validatio set)\n    # Test (All data greater than 1913 day, \n    #       with some gap for recursive features)\n    train_mask = grid_df['d']<=END_TRAIN\n    valid_mask = train_mask&(grid_df['d']>(END_TRAIN-P_HORIZON))\n    preds_mask = grid_df['d']>(END_TRAIN-100)\n    \n    # Apply masks and save lgb dataset as bin\n    # to reduce memory spikes during dtype convertations\n    # https:\/\/github.com\/Microsoft\/LightGBM\/issues\/1032\n    # \"To avoid any conversions, you should always use np.float32\"\n    # or save to bin before start training\n    # https:\/\/www.kaggle.com\/c\/talkingdata-adtracking-fraud-detection\/discussion\/53773\n    train_data = lgb.Dataset(grid_df[train_mask][features_columns], \n                       label=grid_df[train_mask][TARGET])\n    train_data.save_binary('train_data.bin')\n    train_data = lgb.Dataset('train_data.bin')\n    \n    valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], \n                       label=grid_df[valid_mask][TARGET])\n    \n    # Saving part of the dataset for later predictions\n    # Removing features that we need to calculate recursively \n    grid_df = grid_df[preds_mask].reset_index(drop=True)\n    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n    grid_df = grid_df[keep_cols]\n    grid_df.to_pickle('test_'+store_id+'.pkl')\n    del grid_df\n    \n    # Launch seeder again to make lgb training 100% deterministic\n    # with each \"code line\" np.random \"evolves\" \n    # so we need (may want) to \"reset\" it\n    seed_everything(SEED)\n    estimator = lgb.train(lgb_params,\n                          train_data,\n                          valid_sets = [valid_data],\n                          verbose_eval = 100,\n                          )\n    \n    # Save model - it's not real '.bin' but a pickle file\n    # estimator = lgb.Booster(model_file='model.txt')\n    # can only predict with the best iteration (or the saving iteration)\n    # pickle.dump gives us more flexibility\n    # like estimator.predict(TEST, num_iteration=100)\n    # num_iteration - number of iteration want to predict with, \n    # NULL or <= 0 means use best iteration\n    model_name = 'lgb_model_'+store_id+'_v'+str(VER)+'.bin'\n    pickle.dump(estimator, open(model_name, 'wb'))\n\n    # Remove temporary files and objects \n    # to free some hdd space and ram memory\n    #!rm train_data.bin\n    del train_data, valid_data, estimator\n    gc.collect()\n    \n    # \"Keep\" models features for predictions\n    MODEL_FEATURES = features_columns","d5f1f033":"########################### Predict\n#################################################################################\n\n# Create Dummy DataFrame to store predictions\nall_preds = pd.DataFrame()\n\n# Join back the Test dataset with \n# a small part of the training data \n# to make recursive features\nbase_test = get_base_test()\n\n# Timer to measure predictions time \nmain_time = time.time()\n\n# Loop over each prediction day\n# As rolling lags are the most timeconsuming\n# we will calculate it for whole day\nfor PREDICT_DAY in range(1,29):    \n    print('Predict | Day:', PREDICT_DAY)\n    start_time = time.time()\n\n    # Make temporary grid to calculate rolling lags\n    grid_df = base_test.copy()\n    grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1)\n        \n    for store_id in STORES_IDS:\n        \n        # Read all our models and make predictions\n        # for each day\/store pairs\n        model_path = 'lgb_model_'+store_id+'_v'+str(VER)+'.bin' \n        if USE_AUX:\n            model_path = AUX_MODELS + model_path\n        \n        estimator = pickle.load(open(model_path, 'rb'))\n        \n        day_mask = base_test['d']==(END_TRAIN+PREDICT_DAY)\n        store_mask = base_test['store_id']==store_id\n        \n        mask = (day_mask)&(store_mask)\n        base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])\n    \n    # Make good column naming and add \n    # to all_preds DataFrame\n    temp_df = base_test[day_mask][['id',TARGET]]\n    temp_df.columns = ['id','F'+str(PREDICT_DAY)]\n    if 'id' in list(all_preds):\n        all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n    else:\n        all_preds = temp_df.copy()\n        \n    print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) \/ 60),\n                  ' %0.2f min total |' % ((time.time() - main_time) \/ 60),\n                  ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n    del temp_df\n    \nall_preds = all_preds.reset_index(drop=True)\nall_preds","b1ff8957":"########################### Export\n#################################################################################\n# Reading competition sample submission and\n# merging our predictions\n# As we have predictions only for \"_validation\" data\n# we need to do fillna() for \"_evaluation\" items\nsubmission = pd.read_csv(ORIGINAL+'sample_submission.csv')[['id']]\nsubmission = submission.merge(all_preds, on=['id'], how='left').fillna(0)\nsubmission.to_csv('submission_v'+str(VER)+'.csv', index=False)","864b6ce5":"### 2-1. Melt train_df (sales_train_evaluation.cvs file)","615719bd":"#### \ucc38\uace0 \ubb38\ud5cc : \nhttps:\/\/tsfresh.readthedocs.io\/en\/latest\/text\/forecasting.html\n\nhttps:\/\/medium.com\/making-sense-of-data\/time-series-next-value-prediction-using-regression-over-a-rolling-window-228f0acae363\n\nhttps:\/\/link.springer.com\/content\/pdf\/10.1007%2F978-0-387-32348-0.pdf  (\ucc45: modeling financial time series with s-plus chapter 9 : rolling analtsis of time series )\n\nhttps:\/\/note.nkmk.me\/python-pandas-rolling\/\n<\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u306b\u304a\u3051\u308brolling()\u3068resample()>\n\n","8f0e820f":"## 1. Import, define Memory reducer, Load csv data","03ff02dc":"### 2-3. Merge calendar data ('calendar.csv') with train data (grid_df)","e1bd6eec":"# 2. Create Grid","681dc0cd":"Copyright 2020 Konstantin Yakovlev\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n\n   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","eaf07526":"## 4. Modeling","7a97c354":"### 2-2. Extract Prices data('sell_prices.csv') and merge it with train data (grid_df)","ec2ba85c":"## 3. Lag & Rolling","b4eb46b5":"## < Table of Content >\n1. Import, define Memory reducer, Load csv data\n2. Create Grid (Merge sale_train_evalaution.csv, calendar.csv, sell_prices.csv into grid_df)\n3. \n4.\n5.\n6.\n","1aa5810e":"## 0. About the Code\nWe used the code provided in the Kaggle M5 discussion board made by @kyakovlev :\nhttps:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/discussion\/138881\nto \n1. Reduce the Memory size of the data \n2. Use Lags and rollings to get additional information from the provided csv data\n3. Build our model using LigthBGM","4be5cb10":"\ucd9c\ucc98 : https:\/\/www.kaggle.com\/kyakovlev\/m5-three-shades-of-dark-darker-magic","8ab7500d":"https:\/\/qiita.com\/takaaki_inada\/items\/3f822737cf306a7bbce9\n\ub300\ud68c \ubd84\uc11d \uc815\ub9ac\nhttps:\/\/www.kaggle.com\/tnmasui\/m5-wrmsse-evaluation-dashboard\nWRMSSE \uac12 \uc815\ub9ac \ubc29\ubc95","1da20b91":"## 4. Modeling"}}