{"cell_type":{"ca4ea4e7":"code","d7b83199":"code","525e1940":"code","d919c454":"code","5c5b73bd":"code","79ed38c9":"code","8d8c5003":"code","440baf2f":"code","ee03b71b":"code","e4c2c99b":"code","e21f0f98":"code","8a23f4cd":"code","9dd15c33":"code","d189c7d1":"code","36ce006d":"code","2ed2d709":"code","f0b93da5":"code","a7db29a0":"code","be2590cb":"code","267a3db4":"code","27c48da7":"code","0b4582fd":"code","040eb7f9":"code","6b59114d":"code","f5716ce4":"code","dd289602":"code","3d822df4":"code","fecb82f4":"code","e8554318":"code","bf049b78":"code","2d70a0e1":"code","886192d5":"code","8126bdda":"code","be4c4379":"code","bf12fd21":"code","ad10c7f5":"code","0dd2f36f":"code","27d98c9f":"code","bf504cea":"code","84c366fe":"code","70958d71":"code","51e51ed8":"code","545642f7":"code","4c241dab":"code","3c147932":"code","7d17083f":"code","ffd1393a":"code","9c957dcc":"code","f0bc9985":"code","04e75a75":"code","17512b2b":"code","1f3ea14f":"code","b3c3d5f0":"code","c6a46be7":"code","f195c837":"code","67d7c8e3":"code","b1c6fb1c":"code","c0955adc":"code","8a69031c":"code","bea88a26":"code","bb8bf4eb":"code","fdadb45f":"code","76795b44":"code","6c387a6e":"code","3a9c3672":"code","9bc63065":"code","bbbcdd67":"code","aea07b98":"code","2e0bfbbd":"code","f2a58c81":"code","6a6dcd5c":"code","472842b8":"code","7a4f58d4":"code","a8ba9981":"code","bca297be":"code","01d37df5":"code","29afe532":"code","1e59a991":"code","90757ed6":"code","88422c89":"code","d3ec48e7":"code","67cbbb28":"code","ea4d84fe":"code","88b25697":"code","cbeb3a74":"code","6a1d4b24":"code","3237f089":"code","0475119e":"code","637252bf":"code","2357d069":"code","16833839":"markdown","8a40ae94":"markdown","d3a78115":"markdown","e7209b7a":"markdown","6302c052":"markdown","39c9681c":"markdown","53ff2354":"markdown","64175944":"markdown","7df81eda":"markdown","4b581709":"markdown","0f4ce9cb":"markdown","77990611":"markdown","346c96fe":"markdown","2d81dd15":"markdown","12b75f51":"markdown","fc87f7bc":"markdown","1f0fc359":"markdown","9baea8da":"markdown","b39116a4":"markdown","6e4a5faa":"markdown","d985edb5":"markdown","8860fbbd":"markdown","91e1105b":"markdown","78a8f07e":"markdown","5a587a0f":"markdown"},"source":{"ca4ea4e7":"'''!pip install gensim --upgrade\n!pip install keras --upgrade\n!pip install pandas --upgrade'''","d7b83199":"# DataFrame\nimport pandas as pd\nimport gc\n# Matplot\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Scikit-learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\nfrom keras import utils\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\n# nltk\nimport nltk\nfrom nltk.corpus import stopwords\nfrom  nltk.stem import SnowballStemmer\n\n# Word2vec\nimport gensim\n\n# Utility\nimport re\nimport numpy as np\nimport os\nfrom collections import Counter\nimport logging\nimport time\nimport pickle\nimport itertools\n\n# Set log\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","525e1940":"nltk.download('stopwords')","d919c454":"# DATASET\nDATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\nDATASET_ENCODING = \"ISO-8859-1\"\nTRAIN_SIZE = 0.8\n\n# TEXT CLENAING\nTEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n\n# WORD2VEC \nW2V_SIZE = 300\nW2V_WINDOW = 7\nW2V_EPOCH = 32\nW2V_MIN_COUNT = 10\n\n# KERAS\nSEQUENCE_LENGTH = 300\nEPOCHS = 8\nBATCH_SIZE = 1024\n\n# SENTIMENT\nPOSITIVE = \"POSITIVE\"\nNEGATIVE = \"NEGATIVE\"\nNEUTRAL = \"NEUTRAL\"\nSENTIMENT_THRESHOLDS = (0.4, 0.7)\n\n# EXPORT\nKERAS_MODEL = \"model.h5\"\nWORD2VEC_MODEL = \"model.w2v\"\nTOKENIZER_MODEL = \"tokenizer.pkl\"\nENCODER_MODEL = \"encoder.pkl\"","5c5b73bd":"dataset_filename = os.listdir(\"..\/input\")[0]\n#dataset_path = '..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv'\ndataset_path = '..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv'\nprint(\"Open file:\", dataset_path)\ndf = pd.read_csv(dataset_path, encoding =DATASET_ENCODING , names=DATASET_COLUMNS)","79ed38c9":"print(\"Dataset size:\", len(df))","8d8c5003":"df.head(5)","440baf2f":"decode_map = {0: \"NEGATIVE\", 2: \"NEUTRAL\", 4: \"POSITIVE\"}\ndef decode_sentiment(label):\n    return decode_map[int(label)]","ee03b71b":"%%time\ndf.target = df.target.apply(lambda x: decode_sentiment(x))","e4c2c99b":"target_cnt = Counter(df.target)\n\nplt.figure(figsize=(16,8))\nplt.bar(target_cnt.keys(), target_cnt.values())\nplt.title(\"Dataset labels distribuition\")","e21f0f98":"stop_words = stopwords.words(\"english\")\nstemmer = SnowballStemmer(\"english\")","8a23f4cd":"def preprocess(text, stem=False):\n    # Remove link,user and special characters\n    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n    tokens = []\n    for token in text.split():\n        if token not in stop_words:\n            if stem:\n                tokens.append(stemmer.stem(token))\n            else:\n                tokens.append(token)\n    return \" \".join(tokens)","9dd15c33":"%%time\ndf.text = df.text.apply(lambda x: preprocess(x))","d189c7d1":"df_train, df_test = train_test_split(df, test_size=1-TRAIN_SIZE, random_state=42)\nprint(\"TRAIN size:\", len(df_train))\nprint(\"TEST size:\", len(df_test))","36ce006d":"%%time\ndocuments = [_text.split() for _text in df_train.text] \nimport os\nmodel_exists = os.path.isfile('..\/input\/trained-model\/model.w2v')","2ed2d709":"if model_exists:\n    from gensim.models import Word2Vec\n    w2v_model=Word2Vec.load(\"..\/input\/trained-model\/model.w2v\")\n    print(\"w2v model exists, existing model loaded.\")\nelse:\n    w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE, \n                                            window=W2V_WINDOW, \n                                            min_count=W2V_MIN_COUNT, \n                                            workers=8)","f0b93da5":"'''if model_present:\n    w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE, \n                                                window=W2V_WINDOW, \n                                                min_count=W2V_MIN_COUNT, \n                                                workers=8)'''","a7db29a0":"if not model_exists:\n    w2v_model.build_vocab(documents)","be2590cb":"words = w2v_model.wv.vocab.keys()\nvocab_size = len(words)\nprint(\"Vocab size\", vocab_size)","267a3db4":"w2v_model.wv['model'].shape","27c48da7":"%%time\nif model_exists==False:\n    w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)\nelse:\n    print(\"Trained model loaded\")","0b4582fd":"w2v_model.most_similar(\"love\")","040eb7f9":"%%time\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df_train.text)\n\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"Total words\", vocab_size)","6b59114d":"%%time\nx_train = pad_sequences(tokenizer.texts_to_sequences(df_train.text), maxlen=300)\nx_test = pad_sequences(tokenizer.texts_to_sequences(df_test.text), maxlen=300)","f5716ce4":"labels = df_train.target.unique().tolist()\nlabels.append(NEUTRAL)\nlabels","dd289602":"encoder = LabelEncoder()\nencoder.fit(df_train.target.tolist())\n\ny_train = encoder.transform(df_train.target.tolist())\ny_test = encoder.transform(df_test.target.tolist())\n\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)\n\nprint(\"y_train\",y_train.shape)\nprint(\"y_test\",y_test.shape)","3d822df4":"print(\"x_train\", x_train.shape)\nprint(\"y_train\", y_train.shape)\nprint()\nprint(\"x_test\", x_test.shape)\nprint(\"y_test\", y_test.shape)","fecb82f4":"y_train[:10]","e8554318":"embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\nfor word, i in tokenizer.word_index.items():\n  if word in w2v_model.wv:\n    embedding_matrix[i] = w2v_model.wv[word]\nprint(embedding_matrix.shape)","bf049b78":"embedding_layer = Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=W2V_SIZE, trainable=False)","2d70a0e1":"print(vocab_size,W2V_SIZE,SEQUENCE_LENGTH)","886192d5":"model_exists=os.path.isfile('..\/input\/trained-model\/model.h5')\nif model_exists:\n    from keras.models import load_model\n    model = load_model('..\/input\/trained-model\/model.h5')\n    print(\"Keras model file found and loaded.\")\nelse:\n    model = Sequential()\n    model.add(embedding_layer)\n    model.add(Dropout(0.5))\n    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n    model.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()","8126bdda":"if not model_exists:\n    model.compile(loss='binary_crossentropy',\n                  optimizer=\"adam\",\n                  metrics=['accuracy'])\nelse:\n    print(\"Compiled Model Loaded.\")","be4c4379":"callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]","bf12fd21":"%%time\nif not model_exists:#if model doesn't exist, fit the model on the training set\n    from keras.callbacks import CSVLogger\n\n    csv_logger = CSVLogger('training.log', separator=',', append=False)\n    history = model.fit(x_train, y_train,\n                        batch_size=BATCH_SIZE,\n                        epochs=EPOCHS,\n                        validation_split=0.1,\n                        verbose=1,\n                        callbacks=callbacks)\n    #storing history for visualization\n    pickle_out = open(\"..\/input\/trained-model\/history.pickle\",\"wb\")\n    pickle.dump(history.histpry, pickle_out)\n    pickle_out.close()\nelse:\n    if os.path.isfile('..\/input\/trained-model\/history.pickle'):\n        history=pickle.load('..\/input\/trained-model\/history.pickle')\n        print(\"History file found and loaded.\")","ad10c7f5":"%%time\nscore = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\nprint()\nprint(\"ACCURACY:\",score[1])\nprint(\"LOSS:\",score[0])","0dd2f36f":"try:\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(len(acc))\n\n    plt.plot(epochs, acc, 'b', label='Training acc')\n    plt.plot(epochs, val_acc, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n\n    plt.figure()\n\n    plt.plot(epochs, loss, 'b', label='Training loss')\n    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n\n    plt.show()\nexcept:\n    print(\"Fit history not found, so visualizations are not shown.\")","27d98c9f":"def decode_sentiment(score, include_neutral=True):\n    if include_neutral:        \n        label = NEUTRAL\n        if score <= SENTIMENT_THRESHOLDS[0]:\n            label = NEGATIVE\n        elif score >= SENTIMENT_THRESHOLDS[1]:\n            label = POSITIVE\n\n        return label\n    else:\n        return NEGATIVE if score < 0.5 else POSITIVE","bf504cea":"def predict(text, include_neutral=True):\n    start_at = time.time()\n    # Tokenize text\n    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n    # Predict\n    score = model.predict([x_test])[0]\n    # Decode sentiment\n    label = decode_sentiment(score, include_neutral=include_neutral)\n\n    return {\"label\": label, \"score\": float(score),\n       \"elapsed_time\": time.time()-start_at}  ","84c366fe":"predict(\"I love the music\")","70958d71":"predict(\"I hate the rain\")","51e51ed8":"predict(\"i don't know what i'm doing\")","545642f7":"%%time\ny_pred_1d = []\ny_test_1d = list(df_test.target)\nscores = model.predict(x_test, verbose=1, batch_size=8000)\ny_pred_1d = [decode_sentiment(score, include_neutral=False) for score in scores]","4c241dab":"def plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n\n    cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=30)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90, fontsize=22)\n    plt.yticks(tick_marks, classes, fontsize=22)\n\n    fmt = '.2f'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label', fontsize=25)\n    plt.xlabel('Predicted label', fontsize=25)","3c147932":"%%time\n\ncnf_matrix = confusion_matrix(y_test_1d, y_pred_1d)\nplt.figure(figsize=(12,12))\nplot_confusion_matrix(cnf_matrix, classes=df_train.target.unique(), title=\"Confusion matrix\")\nplt.show()","7d17083f":"print(classification_report(y_test_1d, y_pred_1d))","ffd1393a":"accuracy_score(y_test_1d, y_pred_1d)","9c957dcc":"model.save(KERAS_MODEL)\nw2v_model.save(WORD2VEC_MODEL)\npickle.dump(tokenizer, open(TOKENIZER_MODEL, \"wb\"), protocol=0)\npickle.dump(encoder, open(ENCODER_MODEL, \"wb\"), protocol=0)","f0bc9985":"predict(\"lol\")","04e75a75":"tweets=pd.read_csv('..\/input\/twitterdiamond\/diamond.csv')","17512b2b":"tweets=tweets.rename(columns={'7:50 pm - 23 May 2019':'Time','This guy told not celebrate the #PulwamaAttack has it hurts  some community  Now people answered him of giving   seat and a defeat of his father and son  Karma hits you  badly  pic twitter com rx eO CK P':'Tweet'})","1f3ea14f":"#!pip install --upgrade pip\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ntweets.head()","b3c3d5f0":"'''pclass=[]\nscore=[]\ncount=0\nfor i in tweets['Tweet']:\n    p=predict(i)\n    pclass.append(p['label'])\n    score.append(p['score'])\n    if count%100==0 or count==0:\n        print(\"Tweets done:\",count)\n    count+=1'''","c6a46be7":"#tweets[\"Class\"]=pclass\n#tweets['Predicted Class Score']=score","f195c837":"tweets.head()","67d7c8e3":"'''from IPython.display import HTML\nimport base64\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)'''","b1c6fb1c":"#create_download_link(tweets)","c0955adc":"#in development\n\nmodel_exists=os.path.isfile('..\/input\/model-with-dropout\/bidirectional_model(with dropout).h5')\nif model_exists:\n    from keras.models import load_model\n    model = load_model('..\/input\/model-with-dropout\/bidirectional_model(with dropout).h5')\n    print(\"Keras model file found and loaded.\")\nelse:\n    import keras\n    from keras.layers import Lambda\n    import tensorflow as tf\n    model = Sequential()\n    model.add(embedding_layer)\n    model.add(Dropout(0.5))\n    #model.add(Flatten())\n    #model.add(Lambda(lambda x: tf.expand_dims(model.output, axis=-1)))\n    #model.add(Lambda(lambda x: tf.expand_dims(model.output, axis=-1)))\n    #model.add(keras.layers.Bidirectional((keras.layers.CuDNNLSTM(100,(3,3),padding='same',input_shape=(300,300), dropout=0.2, recurrent_dropout=0.2))))\n    model.add(keras.layers.Bidirectional((keras.layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2))))\n    #model.add(keras.layers.ConvLSTM2D(100, (3,3), strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, go_backwards=False, stateful=False, dropout=0.0, recurrent_dropout=0.0))\n    model.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()","8a69031c":"predict('The concert was boring for the first 15 minutes while the band warmed up but then was terribly exciting')","bea88a26":"if not model_exists:\n    model.compile(loss='binary_crossentropy',\n                  optimizer=\"adam\",\n                  metrics=['accuracy'])\nelse:\n    print(\"Compiled Model Loaded.\")","bb8bf4eb":"callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]","fdadb45f":"%%time\nif not model_exists:#if model doesn't exist, fit the model on the training set\n    from keras.callbacks import CSVLogger\n\n    csv_logger = CSVLogger('training.log', separator=',', append=False)\n    history = model.fit(x_train, y_train,\n                        batch_size=BATCH_SIZE,\n                        epochs=EPOCHS,\n                        validation_split=0.1,\n                        verbose=1,\n                        callbacks=callbacks)\n    #storing history for visualization\n    pickle_out = open(\"..\/input\/trained-model\/convhistory.pickle\",\"wb\")\n    pickle.dump(history.histpry, pickle_out)\n    pickle_out.close()\n'''else:\n    if os.path.isfile('..\/input\/trained-model\/convhistory.pickle'):\n        history=pickle.load('..\/input\/trained-model\/convhistory.pickle')\n        print(\"History file found and loaded.\")'''","76795b44":"model.save('bidirectional_model.h5')","6c387a6e":"%%time\ny_pred_1d = []\ny_test_1d = list(df_test.target)\nscores = model.predict(x_test, verbose=1, batch_size=8000)\ny_pred_1d = [decode_sentiment(score, include_neutral=False) for score in scores]","3a9c3672":"def plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n\n    cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=30)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90, fontsize=22)\n    plt.yticks(tick_marks, classes, fontsize=22)\n\n    fmt = '.2f'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label', fontsize=25)\n    plt.xlabel('Predicted label', fontsize=25)","9bc63065":"%%time\n\ncnf_matrix = confusion_matrix(y_test_1d, y_pred_1d)\nplt.figure(figsize=(12,12))\nplot_confusion_matrix(cnf_matrix, classes=df_train.target.unique(), title=\"Confusion matrix\")\nplt.show()","bbbcdd67":"print(classification_report(y_test_1d, y_pred_1d))","aea07b98":"'''%%time\nscore = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\nprint()\nprint(\"ACCURACY:\",score[1])\nprint(\"LOSS:\",score[0])'''","2e0bfbbd":"# in development\n'''\nmodel_exists=os.path.isfile('..\/input\/model-with-dropout\/conv_model.h5')\nif model_exists:\n    from keras.models import load_model\n    model = load_model('..\/input\/model-with-dropout\/conv_model.h5')\n    print(\"Keras model file found and loaded.\")\nelse:\n    import keras\n    from keras.layers import Lambda\n    import tensorflow as tf\n    model = Sequential()\n    model.add(embedding_layer)\n    model.add(Dropout(0.5))\n    #model.add(Flatten())\n    model.add(Lambda(lambda x: tf.expand_dims(model.output, axis=-1)))\n    model.add(Lambda(lambda x: tf.expand_dims(model.output, axis=-1)))\n    #model.add(keras.layers.Bidirectional((keras.layers.CuDNNLSTM(100,(3,3),padding='same',input_shape=(300,300), dropout=0.2, recurrent_dropout=0.2))))\n    model.add(keras.layers.ConvLSTM2D(100,(3,3),padding='same',strides=1, dropout=0.2, recurrent_dropout=0.2,))\n    #model.add(keras.layers.ConvLSTM2D(100, (3,3), strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, go_backwards=False, stateful=False, dropout=0.0, recurrent_dropout=0.0))\n    #model.add(keras.layers.Reshape((300,100)))\n    #model.add(Flatten())\n    #model.add(Dense(100,activation='sigmoid'))\n    model.add(Dense(1,activation='relu'))\n    model.add(Flatten())\n    model.add(Dense(1,activation='sigmoid'))\n    \n\n\nmodel.summary()'''","f2a58c81":"'''# in development\n\nmodel_exists=os.path.isfile('..\/input\/model-with-dropout\/conv_model.h5')\nif model_exists:\n    from keras.models import load_model\n    model = load_model('..\/input\/model-with-dropout\/conv_model.h5')\n    print(\"Keras model file found and loaded.\")\nelse:\n    import keras\n    from keras.layers import Lambda\n    import tensorflow as tf\n    model = Sequential()\n    model.add(embedding_layer)\n    model.add(Dropout(0.5))\n    #model.add(Flatten())\n    #model.add(Lambda(lambda x: tf.expand_dims(model.output, axis=-1)))\n    #model.add(Lambda(lambda x: tf.expand_dims(model.output, axis=-1)))\n    #model.add(keras.layers.Bidirectional((keras.layers.CuDNNLSTM(100,(3,3),padding='same',input_shape=(300,300), dropout=0.2, recurrent_dropout=0.2))))\n    #model.add(keras.layers.ConvLSTM2D(100,(3,3),padding='same',strides=1, dropout=0.2, recurrent_dropout=0.2,))\n    #model.add(keras.layers.ConvLSTM2D(100, (3,3), strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, go_backwards=False, stateful=False, dropout=0.0, recurrent_dropout=0.0))\n    model.add(keras.layers.Conv1D(32, 3, strides=1, padding='valid',activation='relu'))\n    model.add(keras.layers.MaxPooling1D(pool_size=2, strides=None, padding='valid'))\n    model.add(keras.layers.Bidirectional((keras.layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2))))\n    #model.add(keras.layers.Reshape((300,100)))\n    #model.add(Flatten())\n    #model.add(Dense(100,activation='sigmoid'))\n    #model.add(Dense(1,activation='relu'))\n    #model.add(Flatten())\n    model.add(Dense(1,activation='sigmoid'))\n    \n\n\nmodel.summary()'''","6a6dcd5c":"'''#if not model_exists:\nmodel.compile(loss='binary_crossentropy',\n              optimizer=\"adam\",\n              metrics=['accuracy'])\n#else:\n    #print(\"Compiled Model Loaded.\")'''","472842b8":"'''callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]'''","7a4f58d4":"#creating smaller train size\n#small_df=pd.concat([pd.DataFrame(x_train).iloc[:10000,:],(pd.DataFrame(y_train).iloc[:100000,:])])","a8ba9981":"'''x_train=pd.DataFrame(x_train).iloc[:40000,:]\ny_train=pd.DataFrame(y_train).iloc[:40000,:]\nx_test=pd.DataFrame(x_test).iloc[:7500,:]\ny_test=pd.DataFrame(y_test).iloc[:7500,:]\n#x_train.shape'''","bca297be":"!export CUDA_VISIBLE_DEVICES=1","01d37df5":"'''%%time\n#BATCH_SIZE = 16\nif not model_exists:#if model doesn't exist, fit the model on the training set\n    #from keras.callbacks import CSVLogger\n\n    #csv_logger = CSVLogger('training.log', separator=',', append=False)\n    history = model.fit(x_train, y_train,\n                        batch_size=1024,\n                        epochs=EPOCHS,\n                        validation_split=0.1,\n                        verbose=1,\n                        callbacks=callbacks)\n    #storing history for visualization\n    pickle_out = open(\"..\/convhistory.pickle\",\"wb\")\n    pickle.dump(history.history, pickle_out)\n    pickle_out.close()\n    model.save('conv_model.h5')\nelse:\n    if os.path.isfile('..\/input\/trained-model\/convhistory.pickle'):\n        history=pickle.load('..\/input\/trained-model\/convhistory.pickle')\n        print(\"History file found and loaded.\")'''","29afe532":"model_exists","1e59a991":"from keras import backend as K\nK.tensorflow_backend._get_available_gpus()","90757ed6":"'''import keras\nimport tensorflow as tf\n\n\nconfig = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 4} ) \nsess = tf.Session(config=config) \nkeras.backend.set_session(sess)\ninit = tf.global_variables_initializer()\nsess.run(init)'''","88422c89":"!export CUDA_VISIBLE_DEVICES=1","d3ec48e7":"model_exists","67cbbb28":"predict(\"This sucks\")","ea4d84fe":"'''%%time\nscore = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\nprint()\nprint(\"ACCURACY:\",score[1])\nprint(\"LOSS:\",score[0])'''","88b25697":"df=pd.read_csv('..\/input\/final370data\/Article370_Platinum.csv')\ndf=df.drop(columns='Unnamed: 0')\ndf.head()","cbeb3a74":"!pip install ray","6a1d4b24":"%%time\n#import ray\n#ray.init()\n#@ray.remote\ndef getsentiments():\n    lst=[]\n    for val in df['Text'][150000:]:\n        lst.append(predict(val))\n    return lst\nlst=getsentiments()\n#lst=ray.get(getsentiments.remote())","3237f089":"import pickle\nwith open('SentimentsFrom150k.pickle','wb') as f:\n    pickle.dump(lst,f)\n'''df['Sentiment']=lst\ndf.to_csv(\"a370senti.csv\")'''","0475119e":"df.head()","637252bf":"len(lst)","2357d069":"len(df['Text'])","16833839":"### Map target label to String\n* **0** -> **NEGATIVE**\n* **2** -> **NEUTRAL**\n* **4** -> **POSITIVE**","8a40ae94":"Creating Bidirectional LSTM Model","d3a78115":"### Callbacks","e7209b7a":"### Settings","6302c052":"### Split train and test","39c9681c":"# Twitter Sentiment Analysis","53ff2354":"Confusion Matrix","64175944":"### Predict","7df81eda":"### Word2Vec ","4b581709":"### Label Encoder ","0f4ce9cb":"### Classification Report","77990611":"### Tokenize Text","346c96fe":"Convolutional LSTM","2d81dd15":"### Pre-Process dataset","12b75f51":"### Train","fc87f7bc":"Train","1f0fc359":"### Save model","9baea8da":"### Accuracy Score","b39116a4":"### Dataset details\n* **target**: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n* **ids**: The id of the tweet ( 2087)\n* **date**: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n* **flag**: The query (lyx). If there is no query, then this value is NO_QUERY.\n* **user**: the user that tweeted (robotickilldozr)\n* **text**: the text of the tweet (Lyx is cool)","6e4a5faa":"### Confusion Matrix","d985edb5":"### Build Model","8860fbbd":"### Read Dataset","91e1105b":"### Embedding layer","78a8f07e":"### Compile model","5a587a0f":"### Evaluate"}}