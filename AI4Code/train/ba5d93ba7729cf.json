{"cell_type":{"a8bd3dc9":"code","15eac518":"code","3b22b618":"code","91246179":"code","d3722026":"code","2db588f7":"code","6cfa5cf7":"code","25320a3c":"code","7c069cea":"code","89f66167":"code","569539de":"code","7220a83f":"code","7bd9c983":"code","b6a9854f":"code","6500de50":"code","cdf008f7":"code","587c3f51":"code","71190b52":"code","c2ca1cc8":"markdown","51648b95":"markdown","ae593dda":"markdown","284cd045":"markdown","5f06a640":"markdown","84dab770":"markdown","0e370d93":"markdown","ea8a3cb8":"markdown","502d2b55":"markdown","a595c5e9":"markdown"},"source":{"a8bd3dc9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy import spatial\nfrom collections import Counter\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nds_path = \"\/kaggle\/input\/complete-poetryfoundationorg-dataset\/\"\nglove_path = \"\/kaggle\/input\/glove-global-vectors-for-word-representation\/glove.6B.%dd.txt\"","15eac518":"import keras.backend as K\nfrom keras.models import Model\nfrom keras.optimizers import Adam, SGD\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Embedding, Input, LSTM, GRU\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","3b22b618":"# read the poems dataset\npoems_df = pd.read_csv(os.path.join(ds_path, \"kaggle_poem_dataset.csv\"))\npoems_df.head(2)","91246179":"# groupby the Author and print the count of number of poems written by each of them\npoems_df.groupby(\"Author\").agg({\"Content\": \"count\"}).sort_values(\"Content\", ascending=False).head(5)","d3722026":"# since Shakespeare has the most poems I am using that for training our model\nwilliam_poems = poems_df[poems_df[\"Author\"] == \"William Shakespeare\"]\nprint(\"Some of the lines are: \")\nprint(william_poems.iloc[0, 4].split('\\n')[:4])","2db588f7":"# combine all the poems by seperating them with \\n\npoems_combined = \"\\n\".join(william_poems.iloc[:, 4].values)\nprint(\"Total number of characters: \", len(poems_combined))","6cfa5cf7":"# now we will be splitting the entire document into lines\npoem_lines = poems_combined.split('\\n')\nprint(\"Number of lines in the dataset: \", len(poem_lines))","25320a3c":"# prepare the input and target lines\ninput_lines = [\"<sos> \"+line for line in poem_lines] # in each of the input we add <sos> token idicating the begining of a line\ntarget_lines = [line+ \" <eos>\" for line in poem_lines] # while target lines are appended with with <eos> token indicating end of the line","7c069cea":"# there could be lines having a lot of words in them. But since we have to define the length of the sequene before hand \n# and we already know that LSTM\/GRU are slow due to their sequential nature, we will try to keep the sequence length as small as possible \n\n# so, in the below snippet, we are calculating the frequencies of different sequence lengths\ntokenized_lines = map(str.split, input_lines)\nlen_of_lines = map(len, tokenized_lines)\nlen_frequencies = Counter(list(len_of_lines))\n\n# output the frequencies\nsorted(len_frequencies.items())","89f66167":"EPOCHS = 500 # number of times the model is trained on the entire training dataset\nBATCH_SIZE = 64 # number of data points to consider to train at a single point of time\nLATENT_DIM = 200 # the size of the hidden state\/vector\nEMBEDDING_DIM = 200 # size of the word embeddings - comes into various sizes 50, 100 or 200\nMAX_VOCAB_SIZE = 30000 # the maximum number of words to consider\nVALIDATION_SPLIT = 0.2 # % of validation dataset","569539de":"# the process to get the word embeddings and tokenize the sentences is a repeatable part\n# hence, I've created a class for easy management across the projects\n\nclass SequenceGenerator():\n    \n    # takes as input an input and output sequence\n    def __init__(self, input_lines, target_lines, max_seq_len=None, max_vocab_size=10000, embedding_dim=200):\n        \"\"\"\n            This is a class constructor.\n            \n            Parameters:\n                input_lines (list): list of input sentences\n                target_lines (list): list of target sentences\n                max_seq_len (int): the maximum length of a single sequence\n                max_vocab_size (int): the maximum number of words in the entire input & output datasets\n                embedding_dim (int): length of a vector of a single word embedding\n        \"\"\"\n        \n        self.input_lines = input_lines\n        self.target_lines = target_lines\n        \n        self.MAX_SEQ_LEN = max_seq_len\n        self.MAX_VOCAB_SIZE = max_vocab_size\n        self.EMBEDDING_DIM = embedding_dim\n    \n    \n    def initialize_embeddings(self):\n        \"\"\"Reads the GloVe word-embeddings and creates embedding matrix and word to index and index to word mapping.\"\"\"\n        \n        # load the word embeddings\n        self.word2vec = {}\n        with open(glove_path%self.EMBEDDING_DIM, 'r') as file:\n            for line in file:\n                vectors = line.split()\n                self.word2vec[vectors[0]] = np.asarray(vectors[1:], dtype=\"float32\")\n\n                \n        # get the embeddings matrix\n        self.num_words = min(self.MAX_VOCAB_SIZE, len(self.word2idx)+1)\n        self.embeddings_matrix = np.zeros((self.num_words, self.EMBEDDING_DIM))\n        \n        for word, idx in self.word2idx.items():\n            if idx <= self.num_words:\n                word_embeddings = self.word2vec.get(word)\n                if word_embeddings is not None:\n                    self.embeddings_matrix[idx] = word_embeddings\n                    \n        self.idx2word = {v:k for k,v in self.word2idx.items()}\n    \n    \n    def prepare_sequences(self, filters=''):\n        \"\"\"\n            Initialize a tokenizer and train it on the input and target sentences.\n            Also, it prepares the sequences by tokenizing and padding the sequences.\n            \n            Parameters:\n                filters (str): mentioned characters in the list are removed from the sentences\n        \"\"\"\n        \n        # train the tokenizer\n        self.tokenizer = Tokenizer(num_words=self.MAX_VOCAB_SIZE, filters='')\n        self.tokenizer.fit_on_texts(self.input_lines+self.target_lines)\n        \n        # get the word-index mapping and initialize embeddings\n        self.word2idx = self.tokenizer.word_index\n        self.initialize_embeddings()\n        \n        # tokenize the input and target lines\n        self.input_sequences = self.tokenizer.texts_to_sequences(self.input_lines)\n        self.target_sequences = self.tokenizer.texts_to_sequences(self.target_lines)\n        \n        # get the max sequence len from the data\n        max_seq_len = max(list(map(len, self.input_lines+self.target_lines)))\n        if self.MAX_SEQ_LEN:\n            self.MAX_SEQ_LEN = min(self.MAX_SEQ_LEN, max_seq_len)\n        else:\n            self.MAX_SEQ_LEN = max_seq_len\n            \n        # pad the sequences\n        self.input_sequences = pad_sequences(self.input_sequences, maxlen=self.MAX_SEQ_LEN, padding=\"post\")\n        self.target_sequences = pad_sequences(self.target_sequences, maxlen=self.MAX_SEQ_LEN, padding=\"post\")\n        \n        print(\"1st input sequence: \", self.input_sequences[0])\n        print(\"1st target sequence: \", self.target_sequences[0])\n        \n        \n    def one_hot_encoding(self):\n        \"\"\"Creates the One-hot encoding for the target sequence.\"\"\"\n        \n        # it will be a 3 dimensional array where\n        # first-dim is the number of target lines\n        # second-dim is the size of the sequences\n        # third-dim is the number of words in the dataset\n        self.one_hot_targets = np.zeros((len(self.target_sequences), self.MAX_SEQ_LEN, self.num_words))\n        \n        for seq_idx, seq in enumerate(self.target_sequences):\n            for word_idx, word_id in enumerate(self.target_sequences[seq_idx]):\n                if word_id > 0:\n                    self.one_hot_targets[seq_idx, word_idx, word_id] = 1\n    \n    \n    def get_closest_word(self, word_vec):\n        \"\"\"\n            Find the nearest word to the provided vector. The distance between the vectors is \n            calculated using the cosine-distance.\n            \n            Parameters:\n                word_vec (np.array): a vector of size EMBEDDING_DIM\n                \n            Returns:\n                Str: the closest word to the provided vector\n        \"\"\"\n        \n        max_dist = 9999999999\n        closest_word = \"NULL\"\n        \n        # iterate overall the words and find the closest one\n        for word, vec in self.word2vec.items():\n            \n            # get the cosine distance between the words\n            dist = spatial.distance.cosine(word_vec, vec)\n            \n            # compare the distance and keep the minimum\n            if dist < max_dist:\n                max_dist = dist\n                closest_word = word\n        \n        return closest_word\n\n\n# create an object of the class\nsg_obj = SequenceGenerator(input_lines, target_lines, max_seq_len=12, \n                           max_vocab_size=MAX_VOCAB_SIZE, embedding_dim=EMBEDDING_DIM)\n\n# prepare the input & target sequences\nsg_obj.prepare_sequences()\n# create the One-hot encoding on the target sequences\nsg_obj.one_hot_encoding()\n\n# make sure the tokenized words contains <sos> & <eos>\nassert '<sos>' in sg_obj.word2idx\nassert '<eos>' in sg_obj.word2idx","7220a83f":"# add the embedding layer\n# input_dim: is the size of the embedding matrix\n# output_dim: is the size of the embedding vector of a word\n# weights: is the embedding_matrix we created in the SequenceGenerator class\nembedding = Embedding(\n    input_dim=sg_obj.num_words,\n    output_dim=sg_obj.EMBEDDING_DIM,\n    weights=[sg_obj.embeddings_matrix]\n)\n\n# since we are using the LSTM architecture, it has two internal states\n# first is the hidden state (state_h), which is also the output of a LSTM cell\n# second is the cell state (state_c)\n# here we are initializing it as empty for the encoder\nstate_h = Input(shape=(LATENT_DIM,))\nstate_c = Input(shape=(LATENT_DIM,))\n\n# this layer takes as input a single sequence\nsequence_input = Input(shape=(sg_obj.MAX_SEQ_LEN,))\n\n# the below layer gets the embeddings for the words in the sequence\nembedding_ = embedding(sequence_input)\n\n# once we have the embeddings these are passed to the LSTM units\n# return_state: ensures that both the states are returned \n# return_sequences: ensures that the output (hidden state) of each of the cell is returned, \n# if return_sequences=false, only the hidden state of the last cell of LSTM is returned\nlstm = LSTM(LATENT_DIM, return_state=True, return_sequences=True)\n\n# also, we pass the embeddings as an input to the LSTM and also the initial states like below\nx, h_, c_ = lstm(embedding_, initial_state=[state_h, state_c])\n\n# as we discussed above, the output of the LSTM cells are fed to a dense layer\n# the size of the dense layer is - number of words in the vocabulary\n# 'softmax' activation is used, as this gives the probabilities for each of the word in the vocabulary\ndense = Dense(sg_obj.num_words, activation=\"softmax\")\noutput = dense(x)\n\n# the Model brings all the inputs and the outputs under a single umbrella\n# this makes the entire encoder model\nEncoder = Model([sequence_input, state_h, state_c], output)","7bd9c983":"# decoder takes as input a single word (index of the word)\ndeco_inp = Input(shape=(1,))\n\n# the input word is converted to its embeddings\n# we will use the same embedding layer as above\ndeco_embed = embedding(deco_inp)\n\n# here also we will use the same LSTM (lstm) layer, since it is already trained\n# only the new inputs will be passed to this\ndeco_x, h, c = lstm(deco_embed, initial_state=[state_h, state_c])\ndeco_output = dense(deco_x)\n\n# combining all the layers to create Decoder model\nDecoder = Model([deco_inp, state_h, state_c], [deco_output, h, c])","b6a9854f":"# since this is a classification problem, we will use 'categorical_crossentropy' as the loss function\n# optimizer used is the Adam optimizer with the learning rate of 0.01\nEncoder.compile(\n    loss='categorical_crossentropy',\n    optimizer=Adam(lr=0.01),\n    metrics=['accuracy']\n)\n\n# initial hidden\/cell state vector containing all zeros\n# this will be passed into the LSTM model\ninitial_state = np.zeros((len(sg_obj.input_sequences), LATENT_DIM))\n\n# train the model\nhistory = Encoder.fit(\n    [sg_obj.input_sequences, initial_state, initial_state], # pass the input sequences and the state vectors\n    sg_obj.one_hot_targets, # the one-hot encoding of the target sequences\n    batch_size=BATCH_SIZE, # the batch size\n    epochs=EPOCHS, # number of times to train the model\n    validation_split=VALIDATION_SPLIT, # % of data for validation\n    verbose=0 # to suppress the information printed for each epoch\n)","6500de50":"# though here the accuracies does not matter to us, since\n# we are not going to try to produce the inputs as it is\n\n# plot the losses for both training and testing\nplt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label='val_loss')\nplt.legend()\nplt.show()\n\n# plot the accuracies for both training and testing\nplt.plot(history.history['accuracy'], label='acc')\nplt.plot(history.history['val_accuracy'], label='val_acc')\nplt.legend()\nplt.show()","cdf008f7":"def get_context(sequences, query_word):\n    \"\"\"\n        This function takes as input multiple lines generated by the model so far and a query_word or the theme of the poem.\n        \n        So, the approach is we will add the embeddings of all the words in a sentence to get the sentence embeddings and will\n        create the sentence embeddings for all the sentences created so far.\n        \n        Now, to summarize all the sentence embeddings into a single vector, we will calculate the distance of all the sentence\n        from the query_word. These weights are normalized and will be used as the weights to combine the sentence embeddings.\n        \n        This final embedding vector or the context will be passed to the Decoder as a hidden state and a new line is generated from it.\n    \"\"\"\n    \n    assert query_word in sg_obj.word2idx\n    \n    # null vector containing all zeroes\n    query_word_embed = sg_obj.word2vec.get(query_word, np.zeros(shape=(EMBEDDING_DIM)))\n    \n    if sequences == []:\n        return query_word_embed\n    \n    # to keep all the sentence embeddings\n    seq_embeddings = []\n    for seq in sequences:\n        \n        # add up all the word embeddings of a sequence\n        zero_vector = np.zeros(shape=(EMBEDDING_DIM))\n        for word in seq:\n            zero_vector += sg_obj.word2vec.get(word, np.zeros(shape=(EMBEDDING_DIM)))\n            \n        seq_embeddings.append(zero_vector)\n    seq_embeddings = np.array(seq_embeddings)\n            \n    weights = []\n    for seq_embed in seq_embeddings:\n        # get the distance between the query word and the sentence embeddings\n        dist = spatial.distance.cosine(seq_embed, query_word_embed)\n        weights.append(np.array([dist]))\n        \n    # normalize the distances\n    weights = np.array(weights\/max(weights))\n        \n    # get the final weighted context\n    context = sum(weights * seq_embeddings)\n    \n    return context","587c3f51":"def get_sample_line(context):\n    \"\"\"\n        Get a single line using the provided context as a hidden state\n        \n        Parameters:\n            context (np.array): generated context of the same size as the word_embedding\n    \"\"\"\n    \n    # sentence start token\n    sos_token = np.array([[sg_obj.word2idx.get(\"<sos>\")]])\n    \n    # create the empty lstm state vectors\n    h = np.array([context])    \n    c = np.zeros(shape=(1, LATENT_DIM))\n    \n    # so we know when to quit\n    eos_token = sg_obj.word2idx['<eos>']\n    \n    output_sequence = []\n    \n    # limit the length of the generated line\n    for i in range(sg_obj.MAX_SEQ_LEN):\n        \n        # predict the first word\n        # the outputed stated are passed to the lstm to generate the next word in the sequence\n        o, h, c = Decoder.predict([sos_token, h, c])\n        \n        # get the probabilities generated from the dense layer\n        probs = o[0,0]\n        \n        if np.argmax(probs) ==0:\n            print(\"Something went wrong!!\")\n        \n        probs = np.nan_to_num(probs)\n        # the word-indices starts from 1 so 1st value does not count\n        probs[0] = 0 \n        \n        # normalize the probabilities\n        probs \/= probs.sum()\n        \n        # select a random word with provided probability of being selected\n        selected_idx = np.random.choice(len(probs), p=probs)\n        \n        # if the generated word is equal to eos_token, terminate\n        if selected_idx == eos_token:\n            break\n        \n        # append the generated word to the output_sequence\n        output_sequence.append(sg_obj.idx2word.get(selected_idx, \"Error <%d>\" % selected_idx))\n        \n        # the word generated will be used as an input to generated the new word\n        sos_token[0][0] = selected_idx\n    \n    # return the sequence\n    return output_sequence","71190b52":"# the theme of the poem - only single word (for simplicity)\nquery_word = \"love\"\n\n# to append the generated poem lines\npoem_lines = []\n\n# first sequence containing only ones, this will be used to generate the context\nsequences = []\n\n# we will be generating 8 lines, you can play around with this\nfor line_no in range(8):\n    \n    # get the context, for the first line the context will contain the embeddings of the theme words itself\n    context = get_context(sequences, query_word)\n    \n    try:\n        # generate a new line and append it\n        sequences.append(get_sample_line(context))\n    except:\n        pass\n    \n    poem_lines.append(\" \".join(sequences[-1]))\n    \nprint(\"\\n\\n\")\nprint(\"\\n\".join(poem_lines))","c2ca1cc8":"## Create the Encoder","51648b95":"# 6. Generate a Poem\n\nWe can pass a starting word to our decoder and start generating the words from it, until <eos> token is generated and the process is repeated to generate as many lines as we want.\n    \nBut, the problem with this is, there may not be any relation between the words. So, here I am using a hack (taking from the principles of a Memory network) to try to provide some context before generating a new line.","ae593dda":"### Finally, we will use the above created get_context and get_sample_lines functions to generate the poem","284cd045":"# Poetry Generation using Seq2Seq\nIn this notebook, I am exploring the poems data by Poetryfoundation.org and creating a RNN based model - Seq2Seq to generate the poems. The Seq2Seq model as the name suggests, consists of two RNN (LSTM or GRU) models. \n\nFirst is called an Encoder, which takes sentence as an input and outputs a summary of the sentence aka hidden state or vector.\n\na new sequence and is trained individually than Decoder unit. So, a single cell\/unit in an encoder takes a single word as an input, processes it and outputs a vector (hidden state). This output vector is fed into a Dense layer with 'Softmax' as an activation function, which gives a probability against the words in the dictionary and the one with the highest probability is the next word in the sequence. \n\nWhile, the second is a Decoder, which generates an output sequence by taking two inputs; One the final hidden state from the Encoder and second a starting word or '<sos>' (start of sentence token). Each cell in the decoder also outputs a hidden state which is again then fed to a dense layer with 'Softmax' as an activation function, giving the probabilities against the words in the dictionary. The word with the highest probability is selected\/outputed. The hidden state from this cell and the output word is fed to the next cell and the process is repeated until a '<eos>' (end of sentence token is generated).\n\n![image.png](attachment:image.png)\n\nBut, for Poem generation, since we don't have an input when we generate new ones, so, we will tweak the apporach a little bit. We will enhance the Encoder, that is we will train it seperately with its hidden vectors feeding into a dense layer with 'Softmax' as decribed above. Once we've trained the encoder, we will use the trained layers which will act as decoder for us. This approach is elaborated below.\n\n**This Seq2Seq model is better than an ordinary RNN model because of the two primary reasons:**\n* The output generated can be of variable length, i.e. not need to be of fixed size\n* When we pass the hidden state of the Encoder to the Decoder we pass a summary of the entire sentence (context) to the decoder unit, which is really helpful while generating a new text.\n\n**The brief of the process is mentioned below.**\n1. **Reading the data:** The dataset contains a lot of poems from different poets. It's upto you to either use all the peoms to train the model or to use peoms by a specific poet. In this notebook I'll be using the ones by William Shakespeare\n2. **Tokenize the data and generate sequences:** In this step we will be splitting the poems into tokens(words) and since lines in the poems can be of varying length we will also pad the sequences to make them of same length\n3. **Prepare Word-embeddings:** Since, we cannot directly use the tokens\/words in the neural networks we need a way to represent these into some numerical form. One of the intuitive method is to create the one-hot encodings but over the researchers have created a much much better method which has been named as embeddings. There are different methods to create the same and hence various pre-trained word-embeddings are available for our use. In this notebook, I've used GloVe by Stanford. I've already added these embeddings into the workspace and you can do the same in any of your projects by using the '+ Add Data' option present on the top right corner.\n4. **Create the Seq2Seq model:** As described above we will create two LSTM\/GRU models - an encoder and a decoder.\n5. **Train the model:** The entire Seq2Seq model is trained\n6. **Generate a Poem:** Finally, in this step we will generate our own poem :) Here, I've used a hack - adapting from the principles of a Memory Network to try to generate lines which have some relation amonst them. The detailed approach is mentioned below.","5f06a640":"# 2. Tokenize the data and generate the sequences\n# 3. Prepare Word-Embeddings\n\nThese two steps are mentioned together since, I've created a class to process the input\/output sentences, prepare word-embeddings and generate the output senquences. Putting the repeatable code into a class helps to maintain the code and saves time in re-writing the repeatable codelines. Even you can pick this class and use in your own projects.","84dab770":"# 1. Reading the data","0e370d93":"## Create the Decoder","ea8a3cb8":"As we can see, we only have 2 such lines where the sequence lengths 41 or 149, so, we will cap the sequence length to 12","502d2b55":"# 4. Create Seq2Seq model","a595c5e9":"# 5. Train the model"}}