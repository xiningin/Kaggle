{"cell_type":{"1edc2d22":"code","9aca9498":"code","f4dd3fc4":"code","c8221d71":"code","06018c20":"code","81216f27":"code","4896d534":"code","aef7c624":"code","2f0b47ec":"code","05254226":"code","6357d047":"code","1e341a0e":"code","f5a50350":"code","a4cf260a":"code","a714af39":"code","c1926920":"code","1e3cfde1":"code","1c3457a8":"code","f2e71200":"code","5b058979":"code","d51d1c04":"code","4aa9b6c4":"code","9ad41482":"code","9697f255":"markdown","58d00933":"markdown","1866f763":"markdown","afd54db6":"markdown","abe5bbc4":"markdown","9c070a21":"markdown","85df623d":"markdown","76afbbdf":"markdown","c1169223":"markdown","4a01c0d1":"markdown"},"source":{"1edc2d22":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","9aca9498":"df_train = pd.read_csv('..\/input\/cat-in-the-dat-ii\/train.csv')\ndf_test = pd.read_csv('..\/input\/cat-in-the-dat-ii\/test.csv')","f4dd3fc4":"df_train.head()","c8221d71":"df_test.head()","06018c20":"df_train.info()","81216f27":"df_train.describe()","4896d534":"df_train.isna().sum()","aef7c624":"df_test.isna().sum()","2f0b47ec":"df_train['bin_3'] = df_train['bin_3'].apply(lambda x: 1 if x=='T' else 0)\ndf_train['bin_4'] = df_train['bin_4'].apply(lambda x:1 if x =='Y' else 0)\ndf_test['bin_3'] = df_test['bin_3'].apply(lambda x:1 if x=='T' else 0)\ndf_test['bin_4'] = df_test['bin_4'].apply(lambda x:1 if x == 'Y' else 0)","05254226":"def replace_nan(data):\n    for column in data.columns:\n        if data[column].isna().sum() > 0:\n            data[column] = data[column].fillna(data[column].mode()[0])\n\n\nreplace_nan(df_train)\nreplace_nan(df_test)","6357d047":"features = []\n\nfor col in df_train.columns[:-1]:\n    rd = LabelEncoder()\n    rd.fit_transform( df_train[col].append( df_test[col] ) )\n    df_train[col] = rd.transform( df_train[col] )\n    df_test [col] = rd.transform( df_test [col] )\n    features.append(col)","1e341a0e":"df_train.head()","f5a50350":"df_train.info()","a4cf260a":"import pandas_profiling as pp\npp.ProfileReport(df_train)","a714af39":"X = df_train.drop('target', axis=1)\nY = df_train['target']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.20, random_state = 0)","c1926920":"print(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","1e3cfde1":"\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\nX_train","1c3457a8":"from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(random_state = 0)\nforest.fit(X_train, Y_train)","f2e71200":"from sklearn.metrics import confusion_matrix, accuracy_score\ny_pred = forest.predict(X_test)\ncm = confusion_matrix(Y_test, y_pred)\nprint(cm)\naccuracy_score(Y_test, y_pred)","5b058979":"conf_matrix = confusion_matrix(y_pred, Y_test)\n\nprint(f'Confussion Matrix: \\n{conf_matrix}\\n')\n\nsns.heatmap(conf_matrix, annot=True)","d51d1c04":"tn = conf_matrix[0,0]\nfp = conf_matrix[0,1]\ntp = conf_matrix[1,1]\nfn = conf_matrix[1,0]\n\ntotal = tn + fp + tp + fn\nreal_positive = tp + fn\nreal_negative = tn + fp","4aa9b6c4":"accuracy  = (tp + tn) \/ total # Accuracy Rate\nprecision = tp \/ (tp + fp) # Positive Predictive Value\nrecall    = tp \/ (tp + fn) # True Positive Rate\nf1score  = 2 * precision * recall \/ (precision + recall)\nspecificity = tn \/ (tn + fp) # True Negative Rate\nerror_rate = (fp + fn) \/ total # Missclassification Rate\nprevalence = real_positive \/ total\nmiss_rate = fn \/ real_positive # False Negative Rate\nfall_out = fp \/ real_negative # False Positive Rate\n\nprint(f'Accuracy    : {accuracy}')\nprint(f'Precision   : {precision}')\nprint(f'Recall      : {recall}')\nprint(f'F1 score    : {f1score}')\nprint(f'Specificity : {specificity}')\nprint(f'Error Rate  : {error_rate}')\nprint(f'Prevalence  : {prevalence}')\nprint(f'Miss Rate   : {miss_rate}')\nprint(f'Fall Out    : {fall_out}')","9ad41482":"from sklearn.metrics import classification_report\nprint(classification_report(y_pred, Y_test))","9697f255":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.1em; font-weight: 200;\">Load Required Libraries <\/span>","58d00933":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.1em; font-weight: 200;\">Feature Scaling<\/span>","1866f763":" <h1 style='background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;' >Random Forest <\/h1>\n\n\nRandom forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean\/average prediction (regression).\n\n\n<img src=\"https:\/\/www.frontiersin.org\/files\/Articles\/284242\/fnagi-09-00329-HTML\/image_m\/fnagi-09-00329-g001.jpg\" width=\"700px\">","afd54db6":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.1em; font-weight: 200;\">Random Forest Classifier<\/span>","abe5bbc4":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.1em; font-weight: 200;\">Change boolean value to int so as to encode<\/span>","9c070a21":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.1em; font-weight: 200;\">Classification Report<\/span>","85df623d":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.1em; font-weight: 200;\">Performance Measures<\/span>","76afbbdf":" <h1 style='background-color:LimeGreen; font-family:newtimeroman; font-size:180%; text-align:center; border-radius: 15px 50px;' > Data Description <\/h1>\n\n\n\nIn this competition, you will be predicting the probability [0, 1] of a binary target column.\n\nThe data contains binary features (bin_*), nominal features (nom_*), ordinal features (ord_*) as well as (potentially cyclical) day (of the week) and month features. The string ordinal features ord_{3-5} are lexically ordered according to string.ascii_letters.\n\nSince the purpose of this competition is to explore various encoding strategies. Unlike the first Categorical Feature Encoding Challenge, the data for this challenge has missing values and feature interactions.\n\n### Files\n* train.csv - the training set\n* test.csv - the test set; you must make predictions against this data\n* sample_submission.csv - a sample submission file in the correct format\n\n\n#### Dataset Link :\n[Here](https:\/\/www.kaggle.com\/c\/cat-in-the-dat-ii\/data)\n","c1169223":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.1em; font-weight: 200;\">Split into train and test sets<\/span>","4a01c0d1":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.1em; font-weight: 200;\">Read Data<\/span>"}}