{"cell_type":{"de3690c4":"code","a807331b":"code","5959d5be":"code","d79da76e":"code","8bf36e84":"code","c6180ef8":"code","1e1d91ab":"code","77a49f60":"code","c8446168":"code","3d36103f":"code","14c5eb3e":"code","aefc2734":"code","60d222f2":"code","71014757":"code","f3fb5ba6":"code","b0164728":"code","6d84cda5":"code","c1996f98":"code","f0730513":"code","2f06c2d7":"code","20a7ea72":"markdown","5497b861":"markdown","5e4008fc":"markdown","c8f3d901":"markdown","8060837f":"markdown","ae7534d2":"markdown","1c3668a4":"markdown","4f8acaae":"markdown","745bb29c":"markdown","edf59112":"markdown","02d6cfd5":"markdown","9d3bca27":"markdown","f6809265":"markdown"},"source":{"de3690c4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a807331b":"df=pd.read_csv('\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv')\ndf.head()","5959d5be":"df.describe(include='all')","d79da76e":"df.isnull().sum()","8bf36e84":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.style.use('ggplot')\nfig,ax=plt.subplots(1,2,figsize=(15,5))\nsns.countplot(x='gender',data=df,hue='gender',ax=ax[0])\nsns.countplot(x='gender',data=df,hue='status',ax=ax[1])\nax[0].set_title('Total no of female and male students')\nax[1].set_title('Total no of female and male students who have been placed and not placed')\n\nfig.tight_layout(pad=5.0)\n","c6180ef8":"# lets plot \nsns.set(style=\"dark\")\nfig,ax=plt.subplots(1,3,figsize=(25,5))\nsns.countplot(x='degree_t',data=df,hue='status',palette=\"Set2\",ax=ax[0])\nsns.countplot(x='hsc_b',data=df,hue='status',ax=ax[1],palette=\"GnBu\")\nsns.countplot(x='workex',data=df,hue='status',ax=ax[2],palette=\"deep\")\n\nax[0].set_title('Degree on placement')\nax[1].set_title('Board on placement')\nax[2].set_title('Previous work experience on placement')\n\n\n","1e1d91ab":"#plotting pair plot bw diff data\nplt.figure(figsize=(25,20))\nsns.pairplot(df[[column for column in df.columns if column not in ['sl_no']]])\nplt.tight_layout(pad=2.0)","77a49f60":"df.head()","c8446168":"# first of all we will drop salary and sl_no as they are not the requi9red feature for our model creations\ndf.drop(['sl_no','salary'],axis=1,inplace=True)\ndf.head(2)","3d36103f":"# lets seperate independent feature with dependent feature\nind_df=df.drop('status',axis=1)\ndep_df=df[['status']]\n","14c5eb3e":"#creating dummy variable and dropping first row\nind_df=pd.get_dummies(ind_df,drop_first=True)\nind_df.head(2)","aefc2734":"from sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix","60d222f2":"# splitting data into train part and test part\nX_train,X_test,y_train,y_test=train_test_split(ind_df,dep_df,test_size=0.2,random_state=0)","71014757":"#model creation SVC\nmodel=SVC()\nmodel.fit(X_train,y_train)","f3fb5ba6":"# predicting score \ny_pred=model.predict(X_test)\nmodel.score(X_test,y_test)","b0164728":"cm=confusion_matrix(y_test,y_pred)\n# lets plot heat map to visulize this\nsns.heatmap(cm)","6d84cda5":"# lets import library for doing hyperparameter tuning there are two ways \n# 1. Grid search cv\n# 2. Randomized search cv\nfrom sklearn.model_selection import GridSearchCV\n","c1996f98":"# lets set our params\nparameter=[{'C':[50,100,150,200],'kernel':['linear']}\n          ,{'C':[100,200,300,400],'kernel':['rbf'],'gamma':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]}]\ngrid_srch=GridSearchCV(estimator=model,param_grid=parameter,scoring='accuracy',cv=10,n_jobs=-1)\ngrid_srch.fit(X_train,y_train)","f0730513":"# now check our best accuracy\ngrid_srch.best_score_","2f06c2d7":"# these parameters are the best fit for our model\ngrid_srch.best_params_","20a7ea72":"\n# Machine learning model creation to check if student is placed or not","5497b861":"*Lets plot confusion matrix we get very bad accuracy*","5e4008fc":"## As we can see now our accuracy improves a lot after hyperparameter tuning\n# Now i am ending this kernel Right here\n# If you like my way. please do an upvote.","c8f3d901":"salary have some none value it means the null value must belong to some unplaced student.","8060837f":"When creating a machine learning model, you'll be presented with design choices as to how to define your model architecture. Often times, we don't immediately know what the optimal model architecture should be for a given model, and thus we'd like to be able to explore a range of possibilities. In true machine learning fashion, we'll ideally ask the machine to perform this exploration and select the optimal model architecture automatically. Parameters which define the model architecture are referred to as hyperparameters and thus this process of searching for the ideal model architecture is referred to as hyperparameter tuning.\n\nThese hyperparameters might address model design questions such as:\n\nWhat degree of polynomial features should I use for my linear model?\nWhat should be the maximum depth allowed for my decision tree?\nWhat should be the minimum number of samples required at a leaf node in my decision tree?","ae7534d2":"# Feature engineering","1c3668a4":"# Hyperparameter tuning for machine learning models.","4f8acaae":"### As we can see our score is very low and it does not perform better so we would try to do hyperparameter tuning","745bb29c":"## There are some categorical variable so lets perform encoding in them by One hot encoding","edf59112":"## Lets plot data of male student and female.","02d6cfd5":"## Lets plot countplot to degree and status\nIt will help us in analyzing which degree have higher placement ratio","9d3bca27":"# Lets create various Data science pipeline\n### 1.Exploratory Data Analysis \n### 2.Data visulization \n### 3.Data cleaning and Feature Engineering \n### 4.Feature Selection \n### 5.Model Selection\n","f6809265":"# Exploratory Data Analysis"}}