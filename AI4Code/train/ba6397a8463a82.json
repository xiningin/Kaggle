{"cell_type":{"81d11981":"code","c9d1c2ca":"code","9e855899":"code","bc9c505b":"code","6052ebb4":"code","f295e67d":"code","2e58f47c":"code","5daef1e5":"code","f8acac49":"markdown","28fe9616":"markdown","13278f22":"markdown","4542cda3":"markdown","85b4e5f6":"markdown","6e664a5f":"markdown","30839018":"markdown","bf0b5fc2":"markdown"},"source":{"81d11981":"# Imports\nimport numpy as np\nimport keras\nfrom keras.datasets import imdb\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.preprocessing.text import Tokenizer\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(42)","c9d1c2ca":"# Loading the data (it's preloaded in Keras)\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=1000)\n\nprint(x_train.shape)\nprint(x_test.shape)","9e855899":"print(x_train[0])\nprint(y_train[0])","bc9c505b":"# One-hot encoding the output into vector mode, each of length 1000\ntokenizer = Tokenizer(num_words=1000)\nx_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\nx_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\nprint(x_train[0])","6052ebb4":"# One-hot encoding the output\nnum_classes = 2\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\nprint(y_train.shape)\nprint(y_test.shape)","f295e67d":"# TODO: Build the model architecture\nmodel = Sequential()\nmodel.add(Dense(512,activation='relu',input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\n# TODO: Compile the model using a loss function and an optimizer.\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n","2e58f47c":"# TODO: Run the model. Feel free to experiment with different batch sizes and number of epochs.\nhist = model.fit(x_train, y_train,\n          batch_size=32,\n          epochs=10,\n          validation_data=(x_test, y_test), \n          verbose=2)","5daef1e5":"score = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Accuracy: \", score[1])","f8acac49":"## 2. Examining the data\nNotice that the data has been already pre-processed, where all the words have numbers, and the reviews come in as a vector with the words that the review contains. For example, if the word 'the' is the first one in our dictionary, and a review contains the word 'the', then there is a 1 in the corresponding vector.\n\nThe output comes as a vector of 1's and 0's, where 1 is a positive sentiment for the review, and 0 is negative.","28fe9616":"## 6. Evaluating the model\nThis will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?","13278f22":"## 5. Training the model\nRun the model here. Experiment with different batch_size, and number of epochs!","4542cda3":"And we'll also one-hot encode the output.","85b4e5f6":"## 1. Loading the data\nThis dataset comes preloaded with Keras, so one simple command will get us training and testing data. There is a parameter for how many words we want to look at. We've set it at 1000, but feel free to experiment.","6e664a5f":"## 3. One-hot encoding the output\nHere, we'll turn the input vectors into (0,1)-vectors. For example, if the pre-processed vector contains the number 14, then in the processed vector, the 14th entry will be 1.","30839018":"# Analyzing IMDB Data in Keras\n\nThis notebook is a part of my learning journey which I've been documenting from Udacity's Natural Language Processing Nanodegree program, which helped me a lot to learn and excel advanced data science stuff such as PySpark. Thank you so much Udacity for providing such quality content. \n","bf0b5fc2":"## 4. Building the  model architecture\nBuild a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting."}}