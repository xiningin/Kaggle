{"cell_type":{"bdfa7ca8":"code","226229de":"code","65cd1b2c":"code","1f0ec0f3":"code","dba18cd5":"code","32ee1a78":"code","c03fa36e":"code","c8a3bdf8":"code","dbb466ec":"code","551d6154":"code","5eb2f75b":"code","aa1ecb03":"code","1236a5ef":"code","4e3898f1":"code","9b318a52":"code","0fbcd3df":"code","f1f65466":"code","f48c54cc":"code","6223d783":"code","504b74aa":"code","9ba3f5f1":"code","62fd9e53":"code","c8228cfd":"code","4a3e68a7":"code","f31b4937":"code","da2e22e7":"code","3c389764":"code","a3438cba":"code","a5b149e6":"code","2e8a8047":"code","1f536c72":"code","cb5dfcaa":"code","62c6703a":"code","d073ab54":"code","fb914fac":"code","49b4fef9":"code","0dc19e41":"code","f5cb78fb":"code","0c5534b1":"code","fa8f58af":"code","ed3ad303":"code","5bb64cbc":"code","789b69be":"code","9a622b16":"code","58968e5a":"code","25732aa6":"code","71e043f5":"code","86deee6d":"code","16cc97fc":"code","1073d9aa":"code","4c2d65e4":"code","9de47a12":"code","fd42f882":"code","b008a323":"code","f7f8af61":"code","bd543c0a":"code","71393b04":"code","ce4be7af":"code","10af2a18":"code","49abd2e4":"markdown","9a0f2bdd":"markdown","187756c4":"markdown","a9c89997":"markdown","bfd3b68d":"markdown","a5f74d43":"markdown","add74539":"markdown","c926e6d5":"markdown","6a8007dc":"markdown","044b6a04":"markdown","cef8ed00":"markdown","579fcd62":"markdown","9fd9f955":"markdown","8e0f6906":"markdown","4eb8b2c9":"markdown","6866c39c":"markdown","970f66f4":"markdown","816cc8e5":"markdown","4ce7d8b0":"markdown","80bfd7ed":"markdown","2773978a":"markdown","69f08c65":"markdown","48e27529":"markdown","e5d8847e":"markdown","3542b759":"markdown","1de7dc9e":"markdown","6df17181":"markdown","5fc1a841":"markdown","3e6dc9f6":"markdown","bd93ea94":"markdown","8db7fc80":"markdown","c8daa6ef":"markdown","ef69dc05":"markdown","3b580c23":"markdown","90b07404":"markdown","e96a7a11":"markdown","f3c03fa6":"markdown","bee2a17d":"markdown","f9f864a2":"markdown","880be285":"markdown","f014f1a1":"markdown","200dae34":"markdown","93e2327f":"markdown","a34313ee":"markdown","c11e20c2":"markdown","be39d883":"markdown"},"source":{"bdfa7ca8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nimport gc\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","226229de":"DATA_FOLDER = '..\/input\/competitive-data-science-predict-future-sales'\nitems = pd.read_csv(os.path.join(DATA_FOLDER, 'items.csv'))\nitem_cats = pd.read_csv(os.path.join(DATA_FOLDER, 'item_categories.csv'))\nshops = pd.read_csv(os.path.join(DATA_FOLDER, 'shops.csv'))\ntrain = pd.read_csv(os.path.join(DATA_FOLDER, 'sales_train.csv'))\ntest = pd.read_csv(os.path.join(DATA_FOLDER, 'test.csv'))\nsubmission = pd.read_csv(os.path.join(DATA_FOLDER, 'sample_submission.csv'))","65cd1b2c":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","1f0ec0f3":"print(\"Items\")\nprint(items.head(2))\nprint(\"\\nItem Catageries\")\nprint(item_cats.tail(2))\nprint(\"\\nShops\")\nprint(shops.sample(n=2))\nprint(\"\\nTraining Data Set\")\nprint(train.sample(n=3,random_state=1))\nprint(\"\\nTest Data Set\")\nprint(test.sample(n=3,random_state=1))","dba18cd5":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price)","32ee1a78":"train = train[train.item_price<90001]\ntrain = train[train.item_cnt_day<901]","c03fa36e":"median = train[(train.shop_id==32)&(train.item_id==2973)&(train.date_block_num==4)&(train.item_price>0)].item_price.median()\ntrain.loc[train.item_price<0, 'item_price'] = median","c8a3bdf8":"import math\n\ngrouped_shop_id = pd.DataFrame(train.groupby(['shop_id', 'date_block_num'])['item_cnt_day'].sum().reset_index())\nfig, axes = plt.subplots(nrows=5,ncols=2,sharex=False,sharey=False,figsize=(16,20))\ncount = 0\nnum_graph = 10\nid_per_graph = math.ceil(grouped_shop_id.shop_id.max()\/num_graph)\n\nfor i in range(5):\n    for j in range(2):\n        sns.pointplot(x='date_block_num', y='item_cnt_day', hue='shop_id', data=grouped_shop_id[np.logical_and(count*id_per_graph <= grouped_shop_id['shop_id'], grouped_shop_id['shop_id'] < (count+1)*id_per_graph)], ax=axes[i][j])\n        count += 1","dbb466ec":"del grouped_shop_id","551d6154":"train_join_item = pd.merge(train, items, how='left', on=['item_id'])\ntrain_join_item = train_join_item.drop('item_name', axis=1) # no need to use column item_name\ntrain_join_item.head(10)","5eb2f75b":"grouped_item_category_id = pd.DataFrame(train_join_item.groupby(['item_category_id', 'date_block_num'])['item_cnt_day'].sum().reset_index())\nfig, axes = plt.subplots(nrows=5,ncols=2,sharex=False,sharey=False,figsize=(16,20))\ncount = 0\nnum_graph = 10\nid_per_graph = math.ceil(grouped_item_category_id.item_category_id.max()\/num_graph)\n\nfor i in range(5):\n    for j in range(2):\n        sns.pointplot(x='date_block_num', y='item_cnt_day', hue='item_category_id', data=grouped_item_category_id[np.logical_and(count*id_per_graph <= grouped_item_category_id['item_category_id'], grouped_item_category_id['item_category_id'] < (count+1)*id_per_graph)], ax=axes[i][j])\n        count += 1","aa1ecb03":"del grouped_item_category_id","1236a5ef":"from itertools import product\n\n# Create \"grid\" with columns\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# For every month we create a grid from all shops\/items combinations from that month\ngrid = [] \nfor block_num in train['date_block_num'].unique():\n    cur_shops = train.loc[train['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = train.loc[train['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n# Turn the grid into a dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n# Groupby data to get shop-item-month aggregates\ngb = train.groupby(['date_block_num', 'shop_id', 'item_id']).agg({'item_cnt_day':'sum','item_price':np.mean}).reset_index()\n\n# Join it to the grid\nall_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\nall_data = pd.merge(all_data, items, how='left', on=['item_id'])\nall_data = all_data.drop('item_name', axis=1) # no need to use column item_name\n\nprint(all_data.head())\n\nfor type_id in ['item_id', 'shop_id', 'item_category_id']:\n    for column_id, aggregator, aggtype in [('item_price',np.mean,'avg'),('item_cnt_day',np.sum,'sum'),('item_cnt_day',np.mean,'avg')]:\n        \n        gb = train_join_item.groupby([type_id,'date_block_num']).aggregate(aggregator).reset_index()[[column_id,type_id,'date_block_num']]\n        gb.columns = [type_id+'_'+column_id+'_'+aggtype,type_id,'date_block_num']\n        \n        all_data = pd.merge(all_data, gb, on=['date_block_num',type_id], how='left')\n\nall_data","4e3898f1":"del grid\ndel gb\ndel cur_shops\ndel cur_items","9b318a52":"reduce_mem_usage(all_data, verbose=True)","0fbcd3df":"lag_variables  = list(all_data.columns[6:])+['item_cnt_day']\nlags = [1, 2, 3, 6]\nfor lag in lags:\n    sales_new_df = all_data.copy()\n    sales_new_df.date_block_num += lag\n    sales_new_df = sales_new_df[['date_block_num','shop_id','item_id']+lag_variables]\n    sales_new_df.columns = ['date_block_num','shop_id','item_id']+ [lag_feat+'_lag_'+str(lag) for lag_feat in lag_variables]\n    all_data = pd.merge(all_data, sales_new_df,on=['date_block_num','shop_id','item_id'] ,how='left')\n    \nall_data","f1f65466":"del sales_new_df","f48c54cc":"for feat in all_data.columns:\n    if 'item_cnt' in feat:\n        all_data[feat]=all_data[feat].fillna(0)\n    elif 'item_price' in feat:\n        all_data[feat]=all_data[feat].fillna(all_data[feat].median())","6223d783":"cols_to_drop = lag_variables[:-1] + ['item_price']\ntraining = all_data.drop(cols_to_drop,axis=1)","504b74aa":"test.head()","9ba3f5f1":"test['date_block_num'] = 34","62fd9e53":"test = pd.merge(test, items, on='item_id', how='left')","c8228cfd":"for lag in lags:\n\n    sales_new_df = all_data.copy()\n    sales_new_df.date_block_num += lag\n    sales_new_df = sales_new_df[['date_block_num','shop_id','item_id']+lag_variables]\n    sales_new_df.columns = ['date_block_num','shop_id','item_id']+ [lag_feat+'_lag_'+str(lag) for lag_feat in lag_variables]\n    test = pd.merge(test, sales_new_df,on=['date_block_num','shop_id','item_id'] ,how='left')","4a3e68a7":"test = test.drop(['ID', 'item_name'], axis=1)","f31b4937":"test_columns = test.columns\ntraining_columns = set(training.drop('item_cnt_day',axis=1).columns)\nprint(len(test_columns))\nprint(len(training_columns))\nfor i in test_columns:\n    assert i in training_columns\nfor i in training_columns:\n    assert i in test_columns","da2e22e7":"for feat in test.columns:\n    if 'item_cnt' in feat:\n        test[feat]=test[feat].fillna(0)\n    elif 'item_price' in feat:\n        test[feat]=test[feat].fillna(test[feat].median())","3c389764":"test[['shop_id','item_id']+['item_cnt_day_lag_'+str(x) for x in [1,2,3]]].head()","a3438cba":"print(training[training['shop_id'] == 5][training['item_id'] == 5233][training['date_block_num'] == 33]['item_cnt_day'])\nprint(training[training['shop_id'] == 5][training['item_id'] == 5233][training['date_block_num'] == 32]['item_cnt_day'])\nprint(training[training['shop_id'] == 5][training['item_id'] == 5233][training['date_block_num'] == 31]['item_cnt_day'])","a5b149e6":"X_train = training[training.date_block_num < 33].drop(['item_cnt_day'], axis=1)\nY_train = training[training.date_block_num < 33]['item_cnt_day']\nX_valid = training[training.date_block_num == 33].drop(['item_cnt_day'], axis=1)\nY_valid = training[training.date_block_num == 33]['item_cnt_day']\nX_test = test","2e8a8047":"#X_train.to_csv('X_train.csv', index=False)\n#Y_train.to_csv('Y_train.csv', index=False)\n#X_valid.to_csv('X_valid.csv', index=False)\n#Y_valid.to_csv('Y_valid.csv', index=False)\n#X_test.to_csv('X_test.csv', index=False)","1f536c72":"del training\ndel test\ngc.collect()","cb5dfcaa":"from xgboost import XGBRegressor\nfrom xgboost import plot_importance\nimport xgboost as xgb\n\nxgbtrain = xgb.DMatrix(X_train, Y_train)\n\nparam = {'max_depth':10, \n         'subsample':1,\n         'min_child_weight':0.5,\n         'eta':0.3, \n         'num_round':1000, \n         'seed':1,\n         'silent':0,\n         'eval_metric':'rmse'} # random parameters\nmodel = xgb.train(param, xgbtrain)","62c6703a":"x=plot_importance(model)\nx.figure.set_size_inches(10, 30) ","d073ab54":"score = model.get_score(importance_type='weight')\n\n# list out keys and values separately \nkey_list = list(score.keys()) \nval_list = list(score.values())\n\ntop_score = list(filter(lambda x: x >= 200, val_list))\n\ntop_feat = []\n\nfor i in top_score:\n    feat = key_list[val_list.index(i)]\n    top_feat += [feat]\n    \nprint(top_feat)","fb914fac":"xgbpredict = xgb.DMatrix(X_test)","49b4fef9":"pred_xgb = model.predict(xgbpredict).clip(0, 20)","0dc19e41":"del model\ndel xgbpredict\ngc.collect()","f5cb78fb":"pd.Series(pred_xgb).describe()","0c5534b1":"sub_df_xgb = pd.DataFrame({'ID':X_test.index,'item_cnt_month': pred_xgb })","fa8f58af":"sub_df_xgb.head()","ed3ad303":"sub_df_xgb.to_csv('submission_xgb.csv',index=False)","5bb64cbc":"from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(X_train, Y_train)","789b69be":"pred_lr = lr.predict(X_test).clip(0, 20)","9a622b16":"pd.Series(pred_lr).describe()","58968e5a":"sub_df_lr = pd.DataFrame({'ID':X_test.index,'item_cnt_month': pred_lr })","25732aa6":"sub_df_lr.head()","71e043f5":"sub_df_lr.to_csv('submission_lr.csv',index=False)","86deee6d":"import lightgbm as lgb\n\nlgb_params = {\n               'feature_fraction': 0.75,\n               'metric': 'rmse',\n               'nthread':1, \n               'min_data_in_leaf': 2**7, \n               'bagging_fraction': 0.75, \n               'learning_rate': 0.03, \n               'objective': 'mse', \n               'bagging_seed': 2**7, \n               'num_leaves': 2**7,\n               'bagging_freq':1,\n               'verbose':0 \n              }","16cc97fc":"model_lgb = lgb.train(lgb_params, lgb.Dataset(X_train, label=Y_train), 100)","1073d9aa":"pred_lgb = model_lgb.predict(X_test).clip(0, 20)","4c2d65e4":"pd.Series(pred_lgb).describe()","9de47a12":"sub_df_lgb = pd.DataFrame({'ID':X_test.index,'item_cnt_month': pred_lgb })","fd42f882":"sub_df_lgb.head()","b008a323":"sub_df_lgb.to_csv('submission_lgb.csv',index=False)","f7f8af61":"pred_w_avg = 0.7*pred_xgb + 0.15*pred_lr + 0.15*pred_lgb","bd543c0a":"pd.Series(pred_w_avg).describe()","71393b04":"sub_df_w_avg = pd.DataFrame({'ID':X_test.index,'item_cnt_month': pred_w_avg })","ce4be7af":"sub_df_w_avg.head()","10af2a18":"sub_df_w_avg.to_csv('submission_w_avg.csv',index=False)","49abd2e4":"Some variables are cleared to save memory","9a0f2bdd":"We delete the training and test datasets because we don't need it","187756c4":"***Predictions***","a9c89997":"Let's see an overview of the predictions","bfd3b68d":"We drop the 'ID' and 'iten_name' columns because we don't need it","a5f74d43":"We will validate that the training and test data apparently correspond","add74539":"New features are created using mean encoding.","c926e6d5":"We fill the na of some characteristics with 0 and others with the median","6a8007dc":"We load the datasets","044b6a04":"Some variables are cleared to save memory.","cef8ed00":"We need to prepare the features.","579fcd62":"We create Lag Features for test data","9fd9f955":"It is possible to observe that there is a sales peak in most of shops at the end of each year. Therefore, the date_block_num column will be relevant when generating the models.","8e0f6906":"Outliers.","4eb8b2c9":"We use the function to reduce memory since the dataset cannot be eliminated.","6866c39c":"We check that all the columns that are in the training set are also in the test set and vice versa.","970f66f4":"Let's see an overview of the predictions","816cc8e5":"WEIGHTED AVERAGING","4ce7d8b0":"It was necessary for me to use this function since the 16 GB of RAM was not being enough","80bfd7ed":"We graph the weight of each of the variables to know which are the most important for the model.","2773978a":"We fill the na of some characteristics with 0 and others with the median","69f08c65":"In the item_id graph you can see some peaks at times that coincide with those of the previous (end of the year) or at different times. This consolidates the importance of the date_block_num in the dataset.","48e27529":"Predictions","e5d8847e":"# LINEAR MODEL","3542b759":"We review which are the characteristics that have a weight greater than 200.","1de7dc9e":"Let's see an overview of the predictions","6df17181":"Some variables are cleared to save memory.","5fc1a841":"We are going to drop the variables that we will not be able to have at prediction time, which are the columns that are not lagged, therefore we will have to drop all the lag_variables except \"item_cnt_day\" because it is our target.","3e6dc9f6":"# XGB Model","bd93ea94":"Next, I would like to add some new features using shop and item categories.","8db7fc80":"We merged train and ietms datasets to group training data by item_category_id","c8daa6ef":"We save the datasets in .csv files to use them in another notebook","ef69dc05":"Let's see an overview of the predictions","3b580c23":"We train the XGB model. After several runs, these were the parameters that delivered the best results","90b07404":"In the plot is possible to see that there are items with strange prices and sales. I decided to remove items with price > 90000 and sales > 900 because it can affect final results.","e96a7a11":"# Ensemble","f3c03fa6":"Some variables are cleared to save memory.","bee2a17d":"We print the first 5 rows of test data","f9f864a2":"Create Lag Features for training data","880be285":"The lagged value for shop_id = 5 and item_id = 5233 correspond, therefore it looks ok.","f014f1a1":"We split the training set into training and validation.\n- < 33 months for the train\n- 33 month for the validation set ","200dae34":"We join test and items data using item_id column","93e2327f":"There is one item with price below zero. It will be fill with median.","a34313ee":"I print some rows for each dataset","c11e20c2":"# LightGBM","be39d883":"Since the greater value of date_block_num in the training data was 33, date_block_num for the test data must be 34"}}