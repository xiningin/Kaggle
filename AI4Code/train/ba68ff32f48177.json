{"cell_type":{"10828098":"code","51fad7e4":"code","e81fee39":"code","10e92732":"code","134059d9":"code","7fa64785":"code","488e410b":"code","074a82b1":"code","7610c4dd":"code","e7e09fcc":"code","1a3032ac":"code","206a8596":"code","c7303716":"code","34202cf8":"code","37330926":"code","a445271d":"code","e7ba7d4e":"code","ff99fe41":"code","25035c4f":"code","8d797e53":"code","445026b9":"code","4c0d22ae":"code","a191f950":"code","dc64518d":"code","b545ead9":"code","e8fd9cbe":"code","1a26f68d":"code","c7033ac5":"code","a955c648":"code","ddef1e8d":"code","1c291e78":"code","bd561701":"code","c90d070f":"code","b9abb30d":"code","251d550c":"code","b48ac253":"code","6e6de831":"code","7dffd6f2":"code","3380e236":"code","4387eb08":"code","3d675436":"code","e5ff019b":"code","d8fc192c":"code","0bf52e82":"code","f5458af4":"code","efc4d9c3":"code","9c548d7b":"code","2a0b4930":"code","b0692e19":"code","fdf729d4":"code","34416311":"code","cb72d88b":"code","45a80d67":"code","c601bb76":"code","8fc014e8":"code","ccadc96d":"code","f1744b85":"code","e1a5b7f4":"code","a2effe58":"markdown","6e5461a0":"markdown","a6b3abd0":"markdown","28dd218f":"markdown","bdd9364b":"markdown","a8fc5ed8":"markdown","fe7ef8fa":"markdown","5151c05b":"markdown","93458263":"markdown","e90de52c":"markdown","650a4a15":"markdown","93973fe4":"markdown","19b17831":"markdown","c7f95aab":"markdown","8e04f188":"markdown","4d60c63d":"markdown","f822ff7e":"markdown","623d342a":"markdown","3febf460":"markdown","cbda6601":"markdown","1cadb64c":"markdown","fad08ee2":"markdown"},"source":{"10828098":"def readFile(fileName):\n    file = pd.read_csv(fileName)\n    return file","51fad7e4":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","e81fee39":"titanic = readFile('..\/input\/titanic\/train.csv')","10e92732":"titanic","134059d9":"titanic.describe()","7fa64785":"titanic['Embarked'].value_counts()","488e410b":"titanic['Sex'].value_counts()","074a82b1":"titanic['Survived'].value_counts()","7610c4dd":"titanic['Pclass'].value_counts()","e7e09fcc":"titanic.info()","1a3032ac":"titanicCorr = titanic.corr()\ntitanicCorr['Survived'].sort_values(ascending=False)","206a8596":"titanic['Relatives'] = titanic['SibSp'] + titanic['Parch']\ntitanic['Age Bracket'] = titanic['Age'] \/\/ 15 * 15","c7303716":"titanicCorr = titanic.corr()\ntitanicCorr['Survived'].sort_values(ascending=False)","34202cf8":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass dataFrameSelector(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, attributeNames):\n        self.attributeNames = attributeNames\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        return X[self.attributeNames]","37330926":"class mostFrequentImputer(BaseEstimator, TransformerMixin):\n    \n    def fit(self, X, y=None):\n        self.mostFrequent = pd.Series(\n            [X[c].value_counts().index[0] for c in X], index=X.columns)\n        return self\n    \n    def transform(self, X, y=None):\n        return X.fillna(self.mostFrequent)","a445271d":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler","e7ba7d4e":"yTrain = titanic['Survived'].copy()\nyTrain","ff99fe41":"XTrain = titanic.drop(['Survived'], axis=1)\nXTrain","25035c4f":"catPipeline = Pipeline([\n    ('cat', dataFrameSelector(['Sex', 'Embarked'])),\n    ('imputer', mostFrequentImputer()),\n    ('encoder', OneHotEncoder(sparse=False))\n])","8d797e53":"catPipeline.fit_transform(titanic)","445026b9":"numPipeline = Pipeline([\n    ('num', dataFrameSelector(['Age', 'SibSp', 'Parch', 'Fare',\n                               'Pclass', 'Age Bracket', 'Relatives'])),\n    ('imputer', SimpleImputer(strategy='median')),\n    ('stdScaler', StandardScaler())\n])","4c0d22ae":"numPipeline.fit_transform(titanic)","a191f950":"fullPipeline = FeatureUnion(transformer_list=[\n    ('numPipeline', numPipeline),\n    ('catPipeline', catPipeline)\n])","dc64518d":"XTrain = fullPipeline.fit_transform(XTrain)\nXTrain","b545ead9":"XTrainTr = pd.DataFrame(XTrain)\nXTrainTr","e8fd9cbe":"test = readFile('..\/input\/titanic\/test.csv')\ntest","1a26f68d":"test['Relatives'] = test['SibSp'] + test['Parch']\ntest['Age Bracket'] = test['Age'] \/\/ 15 * 15","c7033ac5":"XTest = fullPipeline.transform(test)\nXTest","a955c648":"XTestTr = pd.DataFrame(XTest)\nXTestTr","ddef1e8d":"def score(model):\n    model.fit(XTrain, yTrain)\n    cvs = cross_val_score(model, XTrain, yTrain, cv=10)\n    yTrainPred = cross_val_predict(model, XTrain, yTrain, cv=3)\n    confusionMatrix = confusion_matrix(yTrain, yTrainPred)\n    precisionScore = precision_score(yTrain, yTrainPred)\n    recallScore = recall_score(yTrain, yTrainPred)\n    total = print('cvs:', cvs.mean(), ' ps:', precisionScore, ' rs:', recallScore,\n                 ' cm:', confusionMatrix)\n    return total","1c291e78":"def plot(model):\n    yTrainPred = cross_val_predict(model, XTrain, yTrain, cv=3)\n    precisionScore = precision_score(yTrain, yTrainPred)\n    recallScore = recall_score(yTrain, yTrainPred)\n    yScores = cross_val_predict(model, XTrain, yTrain, cv=3, method='decision_function')\n    precisions, recalls, thresholds = precision_recall_curve(yTrain, yScores)\n    \n    plt.style.use('ggplot')\n    fig, (ax0, ax1) = plt.subplots(nrows=2, ncols=1, figsize=(10,10));\n    plot = ax0.plot(thresholds, recalls[:-1], 'b--', label='Recall', linewidth=2);\n    ax0.plot(thresholds, precisions[:-1], 'g-', label='Precisions', linewidth=2);\n    ax0.set(title='Precision, Recall vs Threshold', xlabel='Threshold',\n           ylabel='Precision\/Recall');\n    ax0.legend(title='Plot');\n    \n    plot = ax1.plot(recalls, precisions, linewidth=2);\n    ax1.set(title='Precison vs Recall', xlabel='Recall', ylabel='Precision');\n    \n    fig.suptitle('Precision-Recall Trade Off', fontsize=16, fontweight='bold');\n    \n    fig.savefig(f'..\/{model}.png')","bd561701":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import precision_recall_curve","c90d070f":"from sklearn.linear_model import SGDClassifier","b9abb30d":"sgdClf = SGDClassifier(random_state=42)","251d550c":"score(sgdClf)","b48ac253":"plot(sgdClf)","6e6de831":"from sklearn.ensemble import RandomForestClassifier","7dffd6f2":"forestClf = RandomForestClassifier(random_state=42, n_estimators=100)","3380e236":"score(forestClf)","4387eb08":"from sklearn.svm import SVC\nsvClf = SVC()","3d675436":"score(svClf)","e5ff019b":"plot(svClf)","d8fc192c":"from sklearn.naive_bayes import GaussianNB\nbayesClf = GaussianNB()","0bf52e82":"score(bayesClf)","f5458af4":"svClfScores = cross_val_score(svClf, XTrain, yTrain, cv=10)\nforestClfScores = cross_val_score(forestClf, XTrain, yTrain, cv=10)\nsgdClfScores = cross_val_score(sgdClf, XTrain, yTrain, cv=10)\nbayesClfScores = cross_val_score(bayesClf, XTrain, yTrain, cv=10)","efc4d9c3":"plt.style.use('seaborn')\nfig, ax = plt.subplots(figsize=(10,10));\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.yaxis.set_ticks_position('none')\n\nax.grid(color='grey', axis='y', linestyle='-', linewidth=0.25, alpha=0.5)\nax.plot([1]*10, svClfScores, \".\")\nax.plot([2]*10, forestClfScores, \".\")\nax.plot([3]*10, sgdClfScores, \".\")\nax.plot([4]*10, forestClfScores, \".\")\nax.boxplot([svClfScores, forestClfScores, sgdClfScores, bayesClfScores],\n           labels=('SVM','Random Forest', 'SGD', 'Bayes'))\nax.set(ylabel=\"Model Accuracy\", title='Model Accuracy Comparison')\nfig.savefig('..\/Model Comparison.png')","9c548d7b":"from sklearn.model_selection import GridSearchCV","2a0b4930":"paramGrid = [\n    {'C': [0.1, 1, 10, 100, 1000]},\n    {'gamma': [1, 0.1, 0.01, 0.0001]},\n    {'kernel': ['rbf']}\n]","b0692e19":"gridSearch = GridSearchCV(svClf, paramGrid, cv=5, scoring = 'neg_mean_squared_error',\n                         return_train_score = True)","fdf729d4":"gridSearch.fit(XTrain, yTrain)","34416311":"model = gridSearch.best_estimator_","cb72d88b":"score(model)","45a80d67":"predictions = model.predict(XTest)","c601bb76":"titanicPredict = pd.DataFrame(predictions, columns=['Survived'])","8fc014e8":"testId = test['PassengerId']\ntestId","ccadc96d":"testId = pd.DataFrame(testId, columns=['PassengerId'])\ntestId","f1744b85":"titanicP = pd.concat([testId, titanicPredict], axis=1)","e1a5b7f4":"titanicP.to_csv('titanic_predictions.csv', index=False)","a2effe58":"Python Function to showcase the recall, precision trade off","6e5461a0":"Data Processing and Pipeline Construction for ML Algorithms","a6b3abd0":"Python Function to ascertain the cross validation score, precision score, recall score and confusion matrix of the models","28dd218f":"Stochastic Gradient Descent Classifier model as an accuracy score of 79%","bdd9364b":"The higher the fare, the greater the chance of survival and the lower the passenger class, the lower the chance of survival. This shows that the wealthy class were given more spaces in the life boats than the poor class. Feature engineering will be carried out to find out if it'll have a more significant impact on the chance of survival.","a8fc5ed8":"Hypertuning SVM Model with Grid Search","fe7ef8fa":"python function to read csv files","5151c05b":"Only 38% of the passengers survived the trip","93458263":"Loading, Data Processing and Feature Engineering of the Test Set for ML Algorithms","e90de52c":"C=Cherbourg, Q=Queenstown, S=Southampton.\nRepresentative of the different places Titanic picked up passengers.","650a4a15":"Loading and Exploring the data set","93973fe4":"The hypertuned model's accuracy is the same as the model tested earlier","19b17831":"Random Forest Classifier has an accuracy score of 80%","c7f95aab":"This class fills the empty non integer column spaces with the most frequent attribute.","8e04f188":"This class selects only the desired columns for data transformation.","4d60c63d":"Graphical demonstration of different model accuracy","f822ff7e":"Age, Cabin and Embarked columns have missing data. The Cabin column will be ignored during data preprocessing while the age and embarked will be filled up with the mean and common occurence respectively. Name also will be ignored as hot encoding such volume may cause problems.","623d342a":"Training and Testing Models","3febf460":"Support Vector Machine has an accuracy score of 82%. The highest so far.","cbda6601":"Bayes Classifier has an accuracy score of 79%.","1cadb64c":"The data is self explanatory apart from SibSp indicating how many siblings & spouses and Parcho which indicates how many children & parents were onboard the Titanic.","fad08ee2":"Based on passenger details from the popular titanic that sunk on it's maiden voyage. This project predicts the passenger that survived based on available data. The data set has already been splitted into a training and test set. The goal is to train a suitable algorithm on the training set and use it to predict the survival of passengers in the test set.\n\nNB: Some of the codes were sourced from Aurelien Geron's Hands on Machine Learning with Scikit-Learn, Keras and TensorFlow. "}}