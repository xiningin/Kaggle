{"cell_type":{"3201ae1a":"code","9a763870":"code","3d15e7b3":"code","8ddc2e1c":"code","3f4e5cb8":"code","890837cd":"code","d15e2290":"code","55e85357":"code","47eeeeea":"code","f7ddacb3":"code","ff61e0e9":"code","c3eefb6e":"code","2c256600":"code","9fb5425b":"code","ae9fbb72":"code","58a48647":"code","7b9a2e7e":"code","b8820214":"code","946b184e":"code","2b470b82":"code","2e10b819":"code","aa2e9a5d":"code","9359a38b":"code","8dada144":"code","3f818389":"code","e8a7eed0":"code","d700ce37":"code","400736a9":"code","22e26ae4":"markdown","65262adb":"markdown","99aafeeb":"markdown","56f1e9b5":"markdown","5e50985a":"markdown","58ee2fca":"markdown","d9d8fcbd":"markdown","e89ff548":"markdown","11b438fc":"markdown","64b3490d":"markdown","1d37e466":"markdown","dcbb1a11":"markdown","7b9f0369":"markdown","c28a3de1":"markdown","fefb2569":"markdown","1e5b5ddf":"markdown","67dbbf4b":"markdown","c8a3cada":"markdown","f922f684":"markdown"},"source":{"3201ae1a":"import seaborn as sns\nimport pandas as pd # (e.g. pd.read_csv)\nimport matplotlib.pylab as plt\nimport numpy as np # linear algebra\nimport warnings\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nimport lightgbm as lgb\nimport gc, datetime, random\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n\n#from ml import simple\n\nwarnings.simplefilter(\"ignore\")\nplt.style.use('ggplot')\ncolor_pal = [x['color'] for x in plt.rcParams['axes.prop_cycle']]","9a763870":"transaction = pd.read_csv('..\/input\/anomaly-detection\/creditcard.csv')","3d15e7b3":"print('transaction shape is {}'.format(transaction.shape))\ntransaction = transaction.sort_values(\"Time\")\ntransaction.head(20)","8ddc2e1c":"# plot all transaction\nsns.lmplot( x=\"Time\", y=\"Amount\", data=transaction, fit_reg=False, hue='Class', height=8, aspect=17\/8.27)\nplt.title(\"Transaction Amount during Time\")\n\n# plot only fraudulent transaction\ntransaction[(transaction['Class'] == 1)].plot(x='Time', y='Amount', style='.', figsize=(15, 3), label='Fraudulent Transaction', color = 'orange')","3f4e5cb8":"transaction.info()","890837cd":"transaction = transaction.reset_index()\ntransaction.head(20)","d15e2290":"total = len(transaction)\nplt.figure(figsize=(12,5))\nplt.subplot(121)\nplot_tr = sns.countplot(x='Class', data=transaction)\nplot_tr.set_title(\"Fraud Transactions Distribution \\n 0: No Fraud | 1: Fraud\", fontsize=18)\nplot_tr.set_xlabel(\"Is fraud?\", fontsize=16)\nplot_tr.set_ylabel('Count', fontsize=16)\nfor p in plot_tr.patches:\n    height = p.get_height()\n    plot_tr.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\", fontsize=15) ","55e85357":"transaction.groupby('Class').size()","47eeeeea":"data_null = transaction.isnull().sum()\/len(transaction) * 100\ndata_null","f7ddacb3":"np.warnings.filterwarnings('ignore')\nsns.set(rc={'figure.figsize':(15,50)})\nfor num, alpha in enumerate(list(transaction.drop(columns =['index', 'Class'], axis = 1).columns)):\n    plt.subplot(10,3,num+1)\n    yes = transaction[(transaction['Class'] == 1)][alpha]\n    no = transaction[(transaction['Class'] == 0) ][alpha]\n    plt.hist(yes[yes>0], alpha=0.75, label='Fraud', color='r')\n    plt.hist(no[no>0], alpha=0.25, label='Not Fraud', color='g')\n    plt.ylim(0, 1000)\n    plt.legend(loc='upper right')\n    plt.ylabel(\"Amount is limit to 1000 (250000)\")\n    plt.title('Histogram of values  in column ' + str(alpha) )\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n   ","ff61e0e9":"#we are going to divide 70\/30 the data into training and testing set proportion\ntrain,test=train_test_split(transaction,test_size=0.3)","c3eefb6e":"print(train.shape)\nprint(test.shape)","2c256600":"sns.lmplot( x=\"Time\", y=\"Amount\", data=train, fit_reg=False, hue='Class', height=8, aspect=17\/8.27)\nplt.title(\"Training distribution\")\ntrain.groupby('Class').size()","9fb5425b":"sns.lmplot( x=\"Time\", y=\"Amount\", data=test, fit_reg=False, hue='Class', height=8, aspect=17\/8.27)\nplt.title(\"Testing distribution\")\ntest.groupby('Class').size()","ae9fbb72":"# Extract xTrain\nxTrain = train.drop(columns= ['index', 'Class'])\n# Extract xTest\nxTest = test.drop(columns= ['index', 'Class'])\n# Extract yTrain\nyTrain = train[('Class')]\n# Extract y Test\nyTest = test[('Class')]\n\nxTrain.sort_values('Time')","58a48647":"print('xTrain:', xTrain.shape)\nprint('yTrain:', yTrain.shape)\nprint('xTest:', xTest.shape)\nprint('yTest:', yTest.shape)","7b9a2e7e":"def makePredictions(train, test, target, lgb_params, NFOLDS=6):\n    folds = KFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n\n    X,y = train, target   \n    P = test\n\n    predictions = np.zeros(len(test))\n    \n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n        print('Fold:',fold_)\n        tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]\n        vl_x, vl_y = X.iloc[val_idx,:], y[val_idx]\n            \n        print(len(tr_x),len(vl_x))\n        tr_data = lgb.Dataset(tr_x, label=tr_y)\n\n        vl_data = lgb.Dataset(vl_x, label=vl_y)  \n\n        estimator = lgb.train(\n            lgb_params,\n            tr_data,\n            valid_sets = [tr_data, vl_data],\n            verbose_eval = 200,\n        )   \n        \n        pp_p = estimator.predict(P)\n        predictions += pp_p\/NFOLDS\n        \n        del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data\n        gc.collect()\n        \n    test['prediction'] = predictions\n    \n    return test","b8820214":"def make_predictions(train, test, features_columns, target, lgb_params, NFOLDS=2):\n    \n    folds = GroupKFold(n_splits=NFOLDS)\n\n    X,y = train[features_columns], train[target]    \n    P,P_y = test[features_columns], test[target]  \n    split_groups =  train['Time']\n\n    test = test[['index',target]]    \n    predictions = np.zeros(len(test))\n    oof = np.zeros(len(train))\n    \n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y, groups=split_groups)):\n        print('Fold:',fold_)\n        tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]\n        vl_x, vl_y = X.iloc[val_idx,:], y[val_idx]\n            \n        print(len(tr_x),len(vl_x))\n        tr_data = lgb.Dataset(tr_x, label=tr_y)\n        vl_data = lgb.Dataset(vl_x, label=vl_y)  \n\n        estimator = lgb.train(\n            lgb_params,\n            tr_data,\n            valid_sets = [tr_data, vl_data],\n            verbose_eval = 200,\n        )   \n        \n        pp_p = estimator.predict(P)\n        predictions += pp_p\/NFOLDS\n        \n        oof_preds = estimator.predict(vl_x)\n        oof[val_idx] = (oof_preds - oof_preds.min())\/(oof_preds.max() - oof_preds.min())\n\n        if LOCAL_TEST:\n            feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n            print(feature_imp)\n        \n        del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data\n        gc.collect()\n        \n    test['prediction'] = predictions\n    print('OOF AUC:', metrics.roc_auc_score(y, oof))\n    if LOCAL_TEST:\n        print('Holdout AUC:', metrics.roc_auc_score(test[TARGET], test['prediction']))\n    \n    return test","946b184e":"lgb_params3 = {\n                'objective':'binary',\n                'boosting_type':'gbdt',\n                'metric':'auc',\n                'n_jobs':-1,\n                'learning_rate':0.007,\n                'num_leaves': 2**8,\n                'max_depth':-1,\n                'tree_learner':'serial',\n                'colsample_bytree': 0.85,\n                'subsample_freq':1,\n                'subsample':0.85,\n                'n_estimators':1800,\n                'max_bin':255,\n                'verbose':-1,\n                'seed': 42,\n                #'early_stopping_rounds':100,\n                'reg_alpha':0.3,\n                'reg_lamdba':0.243\n            } \n","2b470b82":"best_params = {'objective':'binary',\n               'boosting_type':'gbdt',\n               'metric':'auc',\n               'reg_lambda': 0.4,\n               'reg_alpha': 0.30000000000000004,\n               'num_leaves': 500,\n               'min_data_in_leaf': 120,\n               'learning_rate': 0.05,\n               'feature_fraction': 0.4,\n               'bagging_fraction': 0.2,\n               'class_weight':None,\n               'colsample_bytree':1.0,\n               'importance_type':'split',\n               'max_depth':-1,\n               'min_child_samples':20,\n               'min_child_weight':0.001,\n               'min_split_gain':0.0,\n               'n_estimators':1800, \n               'n_jobs':-1,\n               'num_leaves':31,\n               'learning_rate': 0.05,\n               'pre_dispatch':'2*n_jobs',\n               'random_state':None, \n               'refit':True,\n               'return_train_score':False, \n               'scoring':'roc_auc',\n               'tree_learner':'serial',\n               'seed': 42,\n               'verbose': -1}","2e10b819":"variables = list(train.drop(columns= ['index', 'Class']).columns)\nvariables","aa2e9a5d":"list(test.columns)\n#test = test.drop(columns= ['index', 'Class'])\ntest.head()","9359a38b":"TARGET = 'Class'\nLOCAL_TEST = True\npredictions_1 = make_predictions(train, test, variables ,TARGET, best_params, NFOLDS=6)\n","8dada144":"predictions_2 = makePredictions(xTrain, xTest, yTrain, best_params, NFOLDS=6)","3f818389":"Mat1 = pd.DataFrame({\"Class\":yTest, \"Prediction\": predictions_1['prediction']})\nMat2 = pd.DataFrame({\"Class\":yTest, \"Prediction\": predictions_2['prediction']})\nMat1","e8a7eed0":"Mat2","d700ce37":"def plot_conf_Mat(Matrix):\n# Creates a confusion matrix\n    cm = confusion_matrix(Matrix['Class'].astype(np.int64), Matrix['Prediction'].astype(np.int64)) \n\n# Transform to df for easier plotting\n    cm_df = pd.DataFrame(cm,\n                     index = ['Class','Prediction'], \n                     columns = ['Class','Prediction'])\n\n    plt.figure(figsize=(5.5,4))\n    sns.heatmap(cm, annot=True)\n    plt.title('CM \\nAccuracy:{0:.3f}'.format(accuracy_score(Matrix['Class'].astype(np.int64), Matrix['Prediction'].astype(np.int64))))\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n\nplot_conf_Mat(Mat1)\nplot_conf_Mat(Mat2)","400736a9":"train_drp = train.drop(columns=[ 'V3', 'V6', 'V7', 'V9', 'V10', 'V12', 'V14', 'V16'], axis = 1)\ntest_drp = train.drop(columns=['V3', 'V6', 'V7', 'V9', 'V10', 'V12', 'V14', 'V16'], axis = 1)\n\n\n#indices = (0, 2, 5, 6, 8, 9, 11, 13, 15)\n#for i in (indices):\n#    variables.pop(i)\nvariables_drp = variables\n#variables_drp.remove('Time')  \nvariables_drp.remove('V3')  \nvariables_drp.remove('V6')\nvariables_drp.remove('V7') \nvariables_drp.remove('V9')  \nvariables_drp.remove('V10')\nvariables_drp.remove('V12')\nvariables_drp.remove('V14')\nvariables_drp.remove('V16')  \n\n\nTARGET = 'Class'\nLOCAL_TEST = True\npredictions_1 = make_predictions(train_drp, test_drp, variables_drp ,TARGET, best_params, NFOLDS=6)","22e26ae4":"As expected, we have a small ratio of fraud case compared to normal transaction. Most supervised machine learning classification algorithms are sensitive to unbalance in the predictor classes, and special techniques would have to be used to account for this unbalance (Under-sampling the normal transaction, Over-sampling the frand transaction). but now we will continu we our raw data.","65262adb":"**Preprocessing**","99aafeeb":"**Add Index to each transaction**","56f1e9b5":"** UMBALANCED Proportion of classes**","5e50985a":"Good! seems to be clean.","58ee2fca":"The fraudulent transaction occur mainly on small  `Amount` which do not exceed 2000.\nIt seems no specific Time that we show a cluster of fraudes. Maybe near to Time : 40000.","d9d8fcbd":"We can delete some non importante column to improve model. Time, V3, V6, V7, V9, V10, V12, V14, V16 seem to be not important.","e89ff548":"**Summary classes**","11b438fc":"**Load packages**","64b3490d":"**WHEN WE REMOVE NO IMPORTANTE VARIABLES**","1d37e466":"**Split Training and Testing Data**","dcbb1a11":"**Check for missing Data**","7b9f0369":"**LGBM**","c28a3de1":"**Recognize categorical and numerical Variables**","fefb2569":"**Explore the must important Variables**","1e5b5ddf":"**Confusion Marix**","67dbbf4b":"**Visualize Transaction Distribution by `Amount` during `Time`**","c8a3cada":"**Load Data**","f922f684":"**Visualize distribution of trainning and testing datasets**"}}