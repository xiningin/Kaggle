{"cell_type":{"e90602eb":"code","f89d3c93":"code","6fc5be8b":"code","42d17bd9":"code","e059a4c3":"code","36fbe440":"code","9e7c0b40":"code","d224f13e":"code","2e16920e":"code","1dbc0f27":"code","b131622f":"code","0ff46376":"code","1356d50a":"code","70f67264":"code","c2b51d5a":"code","e32ea98d":"code","4e115d0c":"code","2909f53e":"code","aa46844f":"code","e62018e4":"code","64e54f26":"code","a416d7a3":"code","a26ce1be":"code","c38d6b30":"code","2736b16a":"code","62b9f0b2":"code","e1da44d5":"code","b0fc501a":"code","27515646":"code","78a03f95":"markdown","d09f7c7d":"markdown","4051f8da":"markdown","d1e682d9":"markdown","13e06442":"markdown","281450f4":"markdown","6efded59":"markdown","bf6a51fb":"markdown","86063459":"markdown","615f0abc":"markdown"},"source":{"e90602eb":"import numpy as np\nimport pandas as pd\nimport os, time, re\nimport pickle, gzip, datetime\n\nfrom datetime import datetime\n\nnow = datetime.now()\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nimport matplotlib as mpl\n%matplotlib inline\n\n\nfrom sklearn import preprocessing as pp\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score, mean_squared_error\nimport lightgbm as lgb\n\n\nimport tensorflow as tf\nimport keras\n\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Activation, Dense, Dropout\nfrom tensorflow.keras.layers import BatchNormalization, Input, Lambda\nfrom tensorflow.keras.layers import Embedding, Flatten, dot\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.losses import mse, binary_crossentropy","f89d3c93":"ratingDF = pd.read_csv('..\/input\/movielens-20m-dataset\/ratings.csv')","6fc5be8b":"ratingDF.userId = ratingDF.userId.astype(str).astype(int)\nratingDF.movieId = ratingDF.movieId.astype(str).astype(int)\nratingDF.rating = ratingDF.rating.astype(str).astype(float)\nratingDF.timestamp = ratingDF.timestamp.apply(lambda x: now.strftime(\"%m\/%d\/%Y, %H:%M:%S\"))","42d17bd9":"n_users = ratingDF.userId.unique().shape[0]\nn_movies = ratingDF.movieId.unique().shape[0]\nn_ratings = len(ratingDF)\navg_ratings_per_user = n_ratings\/n_users","e059a4c3":"print('Number of unique users: ', n_users)\nprint('Number of unique movies: ', n_movies)\nprint('Number of total ratings: ', n_ratings)\nprint('Average number of ratings per user: ', avg_ratings_per_user)","36fbe440":"movieIndex = ratingDF.groupby(\"movieId\").count().sort_values(by= \\\n\"rating\",ascending=False)[0:1000].index\nratingDFX2 = ratingDF[ratingDF.movieId.isin(movieIndex)]\nratingDFX2.count()","9e7c0b40":"userIndex = ratingDFX2.groupby(\"userId\").count().sort_values(by= \\\n\"rating\",ascending=False).sample(n=1000, random_state=2018).index\nratingDFX3 = ratingDFX2[ratingDFX2.userId.isin(userIndex)]\nratingDFX3.count()","d224f13e":"movies = ratingDFX3.movieId.unique()\nmoviesDF = pd.DataFrame(data=movies,columns=['originalMovieId'])\nmoviesDF['newMovieId'] = moviesDF.index+1","2e16920e":"users = ratingDFX3.userId.unique()\nusersDF = pd.DataFrame(data=users,columns=['originalUserId'])\nusersDF['newUserId'] = usersDF.index+1","1dbc0f27":"ratingDFX3 = ratingDFX3.merge(moviesDF,left_on='movieId', \\\nright_on='originalMovieId')\nratingDFX3.drop(labels='originalMovieId', axis=1, inplace=True)\nratingDFX3 = ratingDFX3.merge(usersDF,left_on='userId', \\\nright_on='originalUserId')\nratingDFX3.drop(labels='originalUserId', axis=1, inplace=True)","b131622f":"n_users = ratingDFX3.userId.unique().shape[0]\nn_movies = ratingDFX3.movieId.unique().shape[0]\nn_ratings = len(ratingDFX3)\navg_ratings_per_user = n_ratings\/n_users","0ff46376":"print('Number of unique users: ', n_users)\nprint('Number of unique movies: ', n_movies)\nprint('Number of total ratings: ', n_ratings)\nprint('Average number of ratings per user: ', avg_ratings_per_user)","1356d50a":"X_train, X_test = train_test_split(ratingDFX3,\ntest_size=0.10, shuffle=True, random_state=2018)\nX_validation, X_test = train_test_split(X_test,\ntest_size=0.50, shuffle=True, random_state=2018)","70f67264":"print('Shape of train set:', X_train.shape)\nprint('Shape of validation set:',X_validation.shape)\nprint('Shape of test set: ',X_test.shape)","c2b51d5a":"print('Size of train set:', X_train.size)\nprint('Size of validation set:',X_validation.size)\nprint('Size of test set: ',X_test.size)","e32ea98d":"# Generate ratings matrix for train\nratings_train = np.zeros((n_users, n_movies))\nfor row in X_train.itertuples():\n    ratings_train[row[6]-1, row[5]-1] = row[3]","4e115d0c":"sparsity = float(len(ratings_train.nonzero()[0]))\nsparsity \/= (ratings_train.shape[0] * ratings_train.shape[1])\nsparsity *= 100\nprint('Sparsity: {:4.2f}%'.format(sparsity))","2909f53e":"# Generate ratings matrix for validation\nratings_validation = np.zeros((n_users, n_movies))\nfor row in X_validation.itertuples():\n    ratings_validation[row[6]-1, row[5]-1] = row[3]","aa46844f":"# Generate ratings matrix for test\nratings_test = np.zeros((n_users, n_movies))\nfor row in X_test.itertuples():\n    ratings_test[row[6]-1, row[5]-1] = row[3]","e62018e4":"actual_validation = ratings_validation[ratings_validation.nonzero()].flatten()","64e54f26":"pred_validation = np.zeros((len(X_validation),1))\npred_validation[pred_validation==0] = 3.5\npred_validation","a416d7a3":"print(\"Mean  Squared Error is : \", mean_squared_error(pred_validation, actual_validation))","a26ce1be":"ratings_validation_prediction = np.zeros((n_users, n_movies))\ni = 0\nfor row in ratings_train:\n    ratings_validation_prediction[i][ratings_validation_prediction[i]==0] = np.mean(row[row>0])\n    i += 1","c38d6b30":"pred_validation = ratings_validation_prediction  [ratings_validation.nonzero()].flatten()\nuser_average = mean_squared_error(pred_validation, actual_validation)\nprint('Mean squared error using user average:', user_average)","2736b16a":"ratings_validation_prediction = np.zeros((n_users, n_movies)).T\ni = 0\nfor row in ratings_train.T:\n    ratings_validation_prediction[i][ratings_validation_prediction[i]==0] = np.mean(row[row>0])\n    i += 1","62b9f0b2":"ratings_validation_prediction = ratings_validation_prediction.T\npred_validation = ratings_validation_prediction[ratings_validation.nonzero()].flatten()\nmovie_average = mean_squared_error(pred_validation, actual_validation)\nprint('Mean squared error using movie average:', movie_average)","e1da44d5":"n_latent_factors = 1\nuser_input = Input(shape=[1], name='user')\nuser_embedding = Embedding(input_dim=n_users + 1, output_dim=n_latent_factors,\nname='user_embedding')(user_input)\nuser_vec = Flatten(name='flatten_users')(user_embedding)\nmovie_input = Input(shape=[1], name='movie')\nmovie_embedding = Embedding(input_dim=n_movies + 1,\noutput_dim=n_latent_factors,\nname='movie_embedding')(movie_input)\nmovie_vec = Flatten(name='flatten_movies')(movie_embedding)\nproduct = dot([movie_vec, user_vec], axes=1)\nmodel = Model(inputs=[user_input, movie_input], outputs=product)\nmodel.compile('adam', 'mean_squared_error')","b0fc501a":"history = model.fit(x=[X_train.newUserId, X_train.newMovieId],\n                    y=X_train.rating, epochs=100,\n                    validation_data=([X_validation.newUserId,\n                                      X_validation.newMovieId], X_validation.rating),\n                    verbose=1)","27515646":"pd.Series(history.history['val_loss'][10:]).plot(logy=False)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Validation Error\")\nprint('Minimum MSE: ', min(history.history['val_loss']))","78a03f95":"# Collaborative Filtering\n\nContent-based filtering is not commonly used because it is a rather difficult\ntask to learn the distinct properties of items\u2014this level of understanding is\nvery challenging for artificial machines to achieve currently. It is much\neasier to collect and analyze a large amount of information on users\u2019\nbehaviors and preferences and make predictions based on this. Therefore,\ncollaborative filtering is much more widely used and is the type of\nrecommender system we will focus on here.\n\nCollaborative filtering requires no knowledge of the underlying items\nthemselves. Rather, collaborative filtering assumes that users that agreed in\nthe past will agree in the future and that user preferences remain stable over\ntime. By modeling how similar users are to other users, collaborative\nfiltering can make pretty powerful recommendations. Moreover,\ncollaborative filtering does not have to rely on explicit data (i.e., ratings that\nusers provide). Rather, it can work with implicit data such as how long or\nhow often a user views or clicks on a particular item. For example, in the\npast Netflix asked users to rate movies but now uses implicit user behavior\nto make inferences about user likes and dislikes.\n\nHowever, collaborative filtering has its challenges. First, it requires a lot of\nuser data to make good recommendations. Second, it is a very\ncomputationally demanding task. Third, the datasets are generally very\nsparse since users will have exhibited preferences for only a small fraction\nof all the items in the universe of possible items. Assuming we have enough\ndata, there are techniques we can use to handle the sparsity of the data and\nefficiently solve the problem, which we will cover in this chapter.","d09f7c7d":"Let\u2019s also reindex movieID and userID to a range of 1 to 1,000 for our\nreduced dataset:","4051f8da":"Let\u2019s generate a test set and a validation set from this reduced dataset so\nthat each holdout set is 5% of the reduced dataset:","d1e682d9":"# Recommender Systems\n\nwe will use RBMs to build a recommender system, one of\nthe most successful applications of machine learning to date and widely\nused in industry to help predict user preferences for movies, music, books,\nnews, search, shopping, digital advertising, and online dating.\nThere are two major categories of recommender systems\u2014collaborative\nfiltering recommender systems and content-based filtering recommender\nsystems. Collaborative filtering involves building a recommender system\nfrom a user\u2019s past behavior and those of other users to which the user is\nsimilar to. This recommender system can then predict items that the user\nmay have an interest in even though the user has never expressed explicit\ninterest. Movie recommendations on Netflix rely on collaborative filtering.\nContent-based filtering involves learning the distinct properties of an item\nto recommend additional items with similar properties. Music\nrecommendations on Pandora rely on content-based filtering.","13e06442":"# Perform Baseline Experiments","281450f4":"# Convert fields into appropriate data types","6efded59":"## Define the Cost Function: Mean Squared Error","bf6a51fb":"Let\u2019s calculate the number of unique users, unique movies, total ratings, and\naverage number of ratings per user for our reduced dataset:","86063459":"# Load the data","615f0abc":"# MovieLens Dataset\n\nInstead of the 100 million ratings Netflix dataset, we will use a smaller\nmovie ratings dataset known as the MovieLens 20M Dataset, provided by\nGroupLens, a research lab in the Department of Computer Science and\nEngineering at the University of Minnesota, Twin Cities. The data contains\n20,000,263 ratings across 27,278 movies created by 138,493 users from\nJanuary 9, 1995 to March 31, 2015. Of users who rated at least 20 movies\neach, we will select a subset at random.\nThis dataset is more manageable to work with than the 100 million ratings\ndataset from Netflix. Because the file size exceeds one hundred megabytes,\nthe file is not accessible on GitHub. You will need to download the file\ndirectly from the MovieLens website."}}