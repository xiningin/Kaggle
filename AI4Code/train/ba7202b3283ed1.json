{"cell_type":{"9fec27f0":"code","815f2e1a":"code","4fdd7714":"code","772f28af":"code","59fa6ff4":"code","5d2deaf8":"code","957a145d":"code","6b310d2f":"code","21422369":"code","80b5f099":"code","af41d03a":"code","61690f3b":"code","f3d58bee":"code","d9febae6":"code","a2401d46":"code","834aa289":"code","56148503":"code","528f7b98":"code","d34087d1":"code","21446d56":"code","f398a921":"code","e3cfd802":"code","00aa64af":"code","86c8ec3f":"code","b500d0eb":"code","cfba3e81":"code","25a0cb53":"code","34ad2029":"code","61e1d39f":"code","ee5df975":"code","d9f4b327":"code","88352e31":"code","5d0cc2d4":"code","09c24bb4":"code","8f6ca570":"code","5073ec2a":"code","b6e721a3":"code","a1a9d1d0":"code","6d859fa6":"code","0730a36a":"code","01aea5ee":"code","c6121a79":"code","4be57e5a":"code","56e6eca7":"code","4a230fae":"code","e4f18175":"code","9e1cea3c":"code","c5eed65a":"code","a850c04e":"code","c6b96d92":"code","da02dad2":"code","9a06aa31":"code","cc0904e9":"code","e9036ec8":"code","b28d2ce3":"code","b6c076c4":"code","8ecd4158":"code","3622d0c5":"code","43f0342d":"code","5bc4768b":"code","7c418223":"code","16eea42c":"code","b0f540d2":"code","614248ca":"code","37a3f78a":"code","c12ae82f":"code","383079b1":"code","72fb9652":"code","676013bc":"code","00cf19e2":"code","77fd6674":"code","d0250f61":"code","42c788ac":"code","7db49510":"code","8b854697":"code","3032a25f":"code","900a049d":"code","940728ab":"code","8f2b3b09":"code","0de2159f":"code","2eca9fe4":"code","70f84d9b":"code","7e43f6b0":"code","7055d765":"code","57dc94ba":"code","c97dd478":"code","a0b2bc49":"code","aa7cd95c":"code","d939d9c8":"code","8ae72cef":"code","dd67ceb5":"code","fd139cd8":"code","8df1a831":"code","c4247e36":"code","60e6f8b8":"code","11156e59":"code","47a55971":"code","9e33f878":"code","ca4ed7c9":"code","294d6054":"code","8182b29c":"code","0d3970cb":"code","ae60bfdb":"code","f2bcd33c":"code","da0fcf0c":"code","f641dbeb":"code","38d28fbb":"code","645c7e5b":"code","3e5910e6":"code","84788949":"code","f24c97dd":"code","9f8e90a1":"code","c0514c88":"code","cd8419fe":"code","db95fa56":"code","754f35f3":"code","87f40a04":"code","466a064a":"code","5528099b":"code","31253bd0":"code","62b6f065":"code","e1b03422":"code","325bde87":"code","7b788b91":"code","2afd8454":"code","56f583d7":"code","0aa36208":"code","397b973f":"code","76859092":"code","81204a3b":"code","6fcd79b5":"code","7a26a45e":"code","b39afc38":"code","f44ed858":"code","748f03a4":"code","56ef0401":"code","7fa568fb":"code","f5477974":"code","42dc4720":"code","390a6a22":"code","ee2198ea":"code","598c84e5":"code","68e4882d":"code","7fa0aa33":"code","674cb3c6":"code","b58b1696":"code","206ab4eb":"code","d8063f21":"code","2de9a6dc":"code","0bf3e794":"markdown","84eb7671":"markdown","3291826a":"markdown","2a2a740c":"markdown","9d38e0cf":"markdown","f109dbd0":"markdown","ba9e150d":"markdown","adac13d4":"markdown","369d6cf5":"markdown","4aa7081b":"markdown","7ad26a7e":"markdown","9e527540":"markdown","f17e02b1":"markdown","e91a6695":"markdown","cd50b540":"markdown","713d7d2e":"markdown","961925d9":"markdown","e5d4b964":"markdown","62c38c4c":"markdown","06798d59":"markdown","175a8844":"markdown","af4909b3":"markdown","05297f91":"markdown","13afc218":"markdown","64199180":"markdown","40e499be":"markdown","16520799":"markdown","52a550f4":"markdown","0c2770e4":"markdown","d6d6db64":"markdown","ac810149":"markdown","80763560":"markdown","80602819":"markdown","137587cd":"markdown","531f0a1d":"markdown","106ca43b":"markdown","d727e1a6":"markdown","1742e7cd":"markdown","7d5af087":"markdown","a05ba8fa":"markdown","ace86419":"markdown","96809daa":"markdown","375b2eff":"markdown","44c23eb9":"markdown","206c9f06":"markdown","1ec1c89a":"markdown","157cf4be":"markdown","25837dd7":"markdown","7df8a6b8":"markdown","80ed3477":"markdown","983d60e8":"markdown","cbaa2023":"markdown","421d90e6":"markdown","a61b0f11":"markdown","e736d83a":"markdown","27137e97":"markdown","99aec3eb":"markdown","e3938d17":"markdown","37db5173":"markdown","7c4e5939":"markdown","4c9eaa7e":"markdown","ba331d7c":"markdown","8374c66b":"markdown","cbfbf99d":"markdown","073fae60":"markdown","afc85563":"markdown","48dbd3c9":"markdown","384a4509":"markdown","4d645acc":"markdown","ae20e51f":"markdown","1d47b8de":"markdown","76d3bd00":"markdown","ebe684e5":"markdown","0a0e1a88":"markdown","298b643b":"markdown","3a25366c":"markdown","756ba313":"markdown","44a73675":"markdown","73b1ceb4":"markdown","b01635aa":"markdown","67ef2470":"markdown","663f4149":"markdown","099f1fb8":"markdown","73ce98f3":"markdown","2255b180":"markdown","9017d249":"markdown"},"source":{"9fec27f0":"# Input data files are available in the \"..\/input\/\" directory.\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import randint\nimport seaborn as sns # used for plot interactive graph. \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom io import StringIO\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import chi2\nfrom IPython.display import display\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.model_selection import cross_val_score,GridSearchCV\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.exceptions import ConvergenceWarning\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.tools as tls\nimport plotly.plotly as py\nimport plotly.figure_factory as ff\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)","815f2e1a":"# loading data\ndf = pd.read_csv('..\/input\/consumer-complaints-train-dataset\/Edureka_Consumer_Complaints_train.csv')\ndf.shape","4fdd7714":"df.head(2).T # Columns are shown in rows for easy reading","772f28af":"fig = plt.figure(figsize=(20,12))\ndf.groupby(['State'])['Complaint ID'].count().sort_values().plot.barh(\n    ylim=0, color='blue', title= 'NUMBER OF COMPLAINTS IN EACH PRODUCT CATEGORY\\n')\nplt.xlabel('Number of ocurrences', fontsize = 10);","59fa6ff4":"df[df['State'] == 'CA']['Product'].value_counts()","5d2deaf8":"df[df['State'] == 'CA']['Product'].value_counts().head(5).plot.pie(explode=[0.2,0,0,0,0],shadow=True)\n# Unsquish the pie.\nimport matplotlib.pyplot as plt\nplt.gca().set_aspect('equal')","957a145d":"df[df['State'] == 'CA']['Issue'].value_counts().head(10)","6b310d2f":"df[df['State'] == 'CA']['Issue'].value_counts().head(5).plot.pie(explode=[0.2,0,0,0,0],shadow=True)\n# Unsquish the pie.\nimport matplotlib.pyplot as plt\nplt.gca().set_aspect('equal')","21422369":"p_product_discussions = round(df[\"Product\"].value_counts() \/ len(df[\"Product\"]) * 100,2)\n\nprint(p_product_discussions)\n\nlabels = list(p_product_discussions.index)\nvalues = p_product_discussions.values.tolist()\ncolors = ['#F78181', '#F5A9BC', '#2E9AFE', '#58FA58', '#FAAC58', '#088A85', '#8A0808', '#848484', '#F781F3', '#D7DF01', '#2E2EFE']\n\n\nproduct_pie = go.Pie(labels=labels, values=values, \n                          marker=dict(colors=colors,\n                         line=dict(color='#000000', width=2)))\n\nlayout = go.Layout(title='Product Types')\n\nfig = go.Figure(data=[product_pie], layout=layout)\niplot(fig)","80b5f099":"# Building the variables for our graphs\ndisputed = df['Consumer disputed?'].value_counts()\ncompany_response = df['Company response to consumer'].value_counts()\ntop5_disputed = df['Company'].loc[df['Consumer disputed?'] == 'Yes'].value_counts()[:5]\ntop5_nodispute = df['Company'].loc[df['Consumer disputed?'] == 'No'].value_counts()[:5]","af41d03a":"\n# Top three subplots\ntotal_complaints_plotly = go.Bar(\n            x=disputed.index.values,\n            y=disputed.values,\n    text = 'Complaints',\n    showlegend=False,\n    marker=dict(\n        color=['#40FF00', '#FF0000'])\n    )\n\ntop5_disputes_plotly = go.Bar(\n            x=top5_disputed.index.values,\n            y=top5_disputed.values,\n    text='Disputes',\n    showlegend=False,\n    marker=dict(\n        color=top5_disputed.values,\n        colorscale='Reds')\n    )\n\ntop5_nodisputes_plotly = go.Bar(\n            x=top5_nodispute.index.values,\n            y=top5_nodispute.values,\n    text='No Disputes',\n    showlegend=False,\n    marker=dict(\n        color=top5_nodispute.values,\n        colorscale='Blues')\n    )\n\n# Lower Subplot\ncustomer_res_plotly = go.Bar(\n            x=company_response.index.values,\n            y=company_response.values,\n    text='Customer Response',\n    showlegend=False,\n        marker=dict(\n        color=df['Company response to consumer'].value_counts().values,\n        colorscale = [[0.0, 'rgb(165,0,38)'], [0.1111111111111111, 'rgb(215,48,39)'], [0.2222222222222222, 'rgb(244,109,67)'], \n                      [0.3333333333333333, 'rgb(253,174,97)'], [0.4444444444444444, 'rgb(254,224,144)'], \n                      [0.5555555555555556, 'rgb(224,243,248)'], [0.6666666666666666, 'rgb(171,217,233)'], \n                      [0.7777777777777778, 'rgb(116,173,209)'], [0.8888888888888888, 'rgb(69,117,180)'], \n                      [1.0, 'rgb(49,54,149)']],\n        reversescale = True\n        )\n)\n\nfig = tls.make_subplots(rows=2, cols=3, specs=[[{}, {}, {}], [{'colspan': 3}, None, None]],\n                          subplot_titles=('Did the Customer Disputed?',\n                                          'Disputes',\n                                         'No Disputes',\n                                         'Company response to consumer'))\n\n# First three Subplots\nfig.append_trace(total_complaints_plotly, 1, 1)\nfig.append_trace(top5_disputes_plotly , 1, 2)\nfig.append_trace(top5_nodisputes_plotly , 1, 3)\n\n# Lower Subplot\nfig.append_trace(customer_res_plotly, 2, 1)\n\n\n\nfig['layout'].update(showlegend=True, height=600, width=800, title='Sectors')\niplot(fig, filename='Complaints')","61690f3b":"df['Date received'] = pd.to_datetime(df['Date received'])\ndf['year_received'], df['month_received'] = df['Date received'].dt.year, df['Date received'].dt.month\ndf.head()","f3d58bee":"# Create a Line Plot by the top 5 companies  by year who had more customer interaction cases (Disputes and No Disputes)\nsorting_groups = df.groupby(['year_received', 'Consumer disputed?'])['Company'].apply(lambda x: x.value_counts())\nd = {'CRM': sorting_groups}\n\n\nyear_crm = pd.DataFrame(data=d).reset_index()\nyear_crm.sort_values(by='CRM', ascending=False)\n\ncrm_df = year_crm.rename(columns={\"level_2\": \"Company\"})\n\n# Conditionals Top 5 Companies with dispues (Bank of America, Wells Fargo, JP Morgan, Equifax, CitiBank)\nboa_disputes = crm_df.loc[(crm_df['Company'] == 'Bank of America') & (crm_df['Consumer disputed?'] == 'Yes')]\nwfc_disputes = crm_df.loc[(crm_df['Company'] == 'Wells Fargo & Company') & (crm_df['Consumer disputed?'] == 'Yes')]\njp_disputes = crm_df.loc[(crm_df['Company'] == 'JPMorgan Chase & Co.') & (crm_df['Consumer disputed?'] == 'Yes')]\nequi_disputes = crm_df.loc[(crm_df['Company'] == 'Equifax') & (crm_df['Consumer disputed?'] == 'Yes')]\nciti_disputes = crm_df.loc[(crm_df['Company'] == 'Citibank') & (crm_df['Consumer disputed?'] == 'Yes')]\n\n# Establish the year (Continue Here tomorrow!)\nyears = boa_disputes['year_received'].values.tolist()","d9febae6":"crm_df.head()","a2401d46":"# Implementing line chart (top 5 companies with complaints)\nboa_disputes_amount = boa_disputes['CRM'].values.tolist()\nwfc_disputes_amount = wfc_disputes['CRM'].values.tolist()\njp_disputes_amount = jp_disputes['CRM'].values.tolist()\nequi_disputes_amount = equi_disputes['CRM'].values.tolist()\nciti_disputes_amount = citi_disputes['CRM'].values.tolist()\n\n# Text to add\nboa_text = [str(dis) + '\\n Disputes'  for dis in boa_disputes_amount]\nwfc_text = [str(wfc) + '\\n Disputes'  for wfc in wfc_disputes_amount]\njp_text = [str(jp) + '\\n Disputes' for jp in jp_disputes_amount]\nequi_text = [str(equi) + '\\n Disputes' for equi in equi_disputes_amount]\nciti_text = [str(citi) + '\\n Disputes' for citi in citi_disputes_amount]\n\nboa_disputes_chart = go.Scatter(\n    x=years,\n    y=boa_disputes_amount,\n    text=boa_text,\n    name='Bank of America', \n    hoverinfo='x+text',\n    mode='lines',\n    line=dict(width=1,\n             color='rgb(0, 22, 235)',\n             ),\n    fill='tonexty'\n)\n\nwfc_disputes_chart = go.Scatter(\n    x=years,\n    y=wfc_disputes_amount,\n    text=wfc_text,\n    name=\"Wells Fargo & Company\", \n    hoverinfo='x+text',\n    mode='lines',\n    line=dict(width=1,\n             color='rgb(275, 170, 0)',\n             ),\n    fill='tonexty'\n)\n\n\njp_disputes_chart = go.Scatter(\n    x=years,\n    y=jp_disputes_amount,\n    text=jp_text,\n    name='JP Morgan Chase & Co.',\n    hoverinfo='x+text',\n    mode='lines',\n    line=dict(width=1,\n             color='rgb(128, 128, 128)',\n             ),\n    fill='tonexty'\n)\n\nequi_disputes_chart = go.Scatter(\n    x=years,\n    y=equi_disputes_amount,\n    text=equi_text,\n    name='Equifax',\n    hoverinfo='x+text',\n    mode='lines',\n    line=dict(width=1,\n             color='rgb(175, 0, 0)',\n             ),\n    fill='tonexty'\n)\n\nciti_disputes_chart = go.Scatter(\n    x=years,\n    y=citi_disputes_amount,\n    text=citi_text,\n    name='CitiBank',\n    hoverinfo='x+text',\n    mode='lines',\n    line=dict(width=1,\n             color='rgb(0, 215, 215)',\n             ),\n    fill='tonexty'\n)\n\ndata = [boa_disputes_chart, wfc_disputes_chart, jp_disputes_chart, equi_disputes_chart, citi_disputes_chart]\n\nlayout = dict(title = 'Number of Disputes <br> (Top 5 Companies)',\n              xaxis = dict(title = 'Year'),\n              yaxis = dict(title = 'Number of Disputes')\n             )\n\n\nfig = dict(data=data, layout=layout)\n\n\niplot(fig, filename='basic-area-no-bound')","834aa289":"# Months with the highest disputes (We will make a barplot)\ndef customerservice_per_month(month, dispute):\n    result = df.loc[(df['month_received'] == month) & (df['Consumer disputed?'] == dispute)]\n    return result\n\n# Monthly Disputes\ndis_january = len(customerservice_per_month(month=1, dispute='Yes'))\ndis_february = len(customerservice_per_month(month=2, dispute='Yes'))\ndis_march = len(customerservice_per_month(month=3, dispute='Yes'))\ndis_april = len(customerservice_per_month(month=4, dispute='Yes'))\ndis_may = len(customerservice_per_month(month=5, dispute='Yes'))\ndis_june = len(customerservice_per_month(month=6, dispute='Yes'))\ndis_july = len(customerservice_per_month(month=7, dispute='Yes'))\ndis_august = len(customerservice_per_month(month=8, dispute='Yes'))\ndis_september = len(customerservice_per_month(month=9, dispute='Yes'))\ndis_october = len(customerservice_per_month(month=10, dispute='Yes'))\ndis_november = len(customerservice_per_month(month=11, dispute='Yes'))\ndis_december = len(customerservice_per_month(month=12, dispute='Yes'))\n\n# Monthly No-Disputes\nnodis_january = len(customerservice_per_month(month=1, dispute='No'))\nnodis_february = len(customerservice_per_month(month=2, dispute='No'))\nnodis_march = len(customerservice_per_month(month=3, dispute='No'))\nnodis_april = len(customerservice_per_month(month=4, dispute='No'))\nnodis_may = len(customerservice_per_month(month=5, dispute='No'))\nnodis_june = len(customerservice_per_month(month=6, dispute='No'))\nnodis_july = len(customerservice_per_month(month=7, dispute='No'))\nnodis_august = len(customerservice_per_month(month=8, dispute='No'))\nnodis_september = len(customerservice_per_month(month=9, dispute='No'))\nnodis_october = len(customerservice_per_month(month=10, dispute='No'))\nnodis_november = len(customerservice_per_month(month=11, dispute='No'))\nnodis_december = len(customerservice_per_month(month=12, dispute='No'))","56148503":"# Most active months\nmonths = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September',\n         'October', 'November', 'December']\ndisputes_by_month = [dis_january, dis_february, dis_march, dis_april, dis_may, dis_june, dis_july, dis_august, dis_september,\n                    dis_october, dis_november, dis_december]\n\nnodisputes_by_month = [nodis_january, nodis_february, nodis_march, nodis_april, nodis_may, nodis_june, nodis_july, \n                       nodis_august, nodis_september, nodis_october, nodis_november, nodis_december]\n\n\ndisputes_chart = go.Bar(\n    y=months,\n    x=disputes_by_month,\n    orientation='h',\n    name='Disputes',\n    text='Disputes',\n    marker=dict(\n        color='#FF6464',\n    line=dict(\n        color='#CD3232',\n        width=1.5\n    ))\n)\n\nnodisputes_chart = go.Bar(\n    y=months,\n    x=nodisputes_by_month,\n    orientation='h',\n    name='No Disputes',\n    text='No Disputes',\n    marker=dict(\n        color='#A9FFA9',\n    line=dict(\n        color='#59AF59',\n        width=1.5\n    ))\n)\n\nfig = tls.make_subplots(rows=1, cols=2, specs=[[{}, {}]],\n                          subplot_titles=('Dispute Chart per Month',\n                                          'No Dispute Chart per Month'))\n\nfig.append_trace(disputes_chart, 1, 1)\nfig.append_trace(nodisputes_chart, 1, 2)\n\nfig['layout'].update(showlegend=True, title=\"Level of Activity by Month\")\niplot(fig)","528f7b98":"df.groupby(['Company'])['Complaint ID'].count().sort_values(ascending=False).head(5)","d34087d1":"dispute_presence = df.loc[df['Consumer disputed?'] == 'Yes']\ncross_month = pd.crosstab(dispute_presence['State'], dispute_presence['Company']).apply(lambda x: x\/x.sum() * 100)","21446d56":"#  Share of Most disputes for Bank of America.\ndf_boa = pd.DataFrame(cross_month['Bank of America']).reset_index().sort_values(by=\"Bank of America\", ascending=False).round(2)\ndf_boa = df_boa.rename(columns={'Bank of America': 'share of complaints'})\n\nfor col in df_boa.columns:\n    df_boa[col] = df_boa[col].astype(str)\n    \n    \nscl = [[0.0, 'rgb(202, 202, 202)'],[0.2, 'rgb(253, 205, 200)'],[0.4, 'rgb(252, 169, 161)'],\n            [0.6, 'rgb(247, 121, 108  )'],[0.8, 'rgb(255, 39, 39)'],[1.0, 'rgb(219, 0, 0)']]\n\n\ndf_boa['text'] = \"State Code: \" + df_boa['State'] + '<br>'\n\n\ndata = [ dict(\n        type='choropleth',\n        colorscale = scl,\n        autocolorscale = False,\n        locations = df_boa['State'],\n        z = df_boa['share of complaints'], \n        locationmode = 'USA-states',\n        text = df_boa['text'],\n        marker = dict(\n            line = dict (\n                color = 'rgb(255,255,255)',\n                width = 2\n            ) ),\n        colorbar = dict(\n            title = \"%\")\n        ) ]\n\nlayout = dict(\n    title = 'Most Complaints by State <br> Bank of America',\n    geo = dict(\n        scope = 'usa',\n        projection=dict(type='albers usa'),\n        showlakes = True,\n        lakecolor = 'rgb(255, 255, 255)')\n)\n\nfig = dict(data=data, layout=layout)\niplot(fig, filename='d3-cloropleth-map')","f398a921":"df[df['Company'] == 'Bank of America']['Product'].value_counts()","e3cfd802":"len(df[df['Timely response?'] == 'Yes'])\/len(df['Timely response?'])","00aa64af":"df[(df['Timely response?'] == 'Yes') & (df['Consumer disputed?'] == 'Yes')]['Company'].value_counts().head(10)","86c8ec3f":"len(df[(df['Timely response?'] == 'Yes') & (df['Consumer disputed?'] == 'Yes')])\/len(df[df['Consumer disputed?'] == 'Yes'])","b500d0eb":"df[(df['Timely response?'] == 'No') & (df['Consumer disputed?'] == 'Yes')]['Company'].value_counts().head(10)","cfba3e81":"len(df[(df['Timely response?'] == 'No') & (df['Consumer disputed?'] == 'Yes')])\/len(df[df['Consumer disputed?'] == 'Yes'])","25a0cb53":"Complaints_by_products = df.groupby(['Product'])['Complaint ID'].count().sort_values(ascending=False)\nfig = plt.figure(figsize=(8,6))\nComplaints_by_products.plot.barh(ylim=0,color='blue',title= 'Customer Complaints By Products\\n')\nplt.xlabel('No of Complaints')","34ad2029":"df[df['Product'] == 'Mortgage']['Issue'].value_counts()","61e1d39f":"submitted_via_df = df['Submitted via'].value_counts()\nfig = plt.figure(figsize=(8,6))\nsubmitted_via_df.plot.pie(title= 'How the Customers Complaints were submitted\\n',explode=[0.2,0,0,0,0,0],shadow=True)\nplt.xlabel('')","ee5df975":"Issue_df = df['Issue'].value_counts().head(10)\nfig = plt.figure(figsize=(8,6))\nIssue_df.plot.barh(ylim=0, color='blue', title= 'Consumer Complaint Issues\\n')\nplt.xlabel('No of Complaints')","d9f4b327":"# Create a new dataframe with two columns\ndf1 = df[['Product', 'Consumer complaint narrative']].copy()\n\n# Remove missing values (NaN)\ndf1 = df1[pd.notnull(df1['Consumer complaint narrative'])]\n\n# Renaming second column for a simpler name\ndf1.columns = ['Product', 'Consumer_complaint'] \n\ndf1.shape","88352e31":"# Percentage of complaints with text\ntotal = df1['Consumer_complaint'].notnull().sum()\nround((total\/len(df)*100),1)","5d0cc2d4":"pd.DataFrame(df.Product.unique()).values","09c24bb4":"# Because the computation is time consuming (in terms of CPU), the data was sampled\ndf2 = df1.sample(10000, random_state=1).copy()","8f6ca570":"# Renaming categories\ndf2.replace({'Product': \n             {'Credit reporting, credit repair services, or other personal consumer reports': \n              'Credit reporting, repair, or other', \n              'Credit reporting': 'Credit reporting, repair, or other',\n             'Credit card': 'Credit card or prepaid card',\n             'Prepaid card': 'Credit card or prepaid card',\n             'Payday loan': 'Payday loan, title loan, or personal loan',\n             'Money transfer': 'Money transfer, virtual currency, or money service',\n             'Virtual currency': 'Money transfer, virtual currency, or money service'}}, \n            inplace= True)","5073ec2a":"pd.DataFrame(df2.Product.unique())","b6e721a3":"# Create a new column 'category_id' with encoded categories \ndf2['category_id'] = df2['Product'].factorize()[0]\ncategory_id_df = df2[['Product', 'category_id']].drop_duplicates()\n\n\n# Dictionaries for future use\ncategory_to_id = dict(category_id_df.values)\nid_to_category = dict(category_id_df[['category_id', 'Product']].values)\n\n# New dataframe\ndf2.head()","a1a9d1d0":"fig = plt.figure(figsize=(8,6))\ncolors = ['grey','grey','grey','grey','grey','grey','grey','grey','grey',\n    'grey','darkblue','darkblue','darkblue']\ndf2.groupby('Product').Consumer_complaint.count().sort_values().plot.barh(\n    ylim=0, color='Blue', title= 'NUMBER OF COMPLAINTS IN EACH PRODUCT CATEGORY\\n')\nplt.xlabel('Number of ocurrences', fontsize = 10);","6d859fa6":"tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n                        ngram_range=(1, 2), \n                        stop_words='english')\n\n# We transform each complaint into a vector\nfeatures = tfidf.fit_transform(df2.Consumer_complaint).toarray()\n\nlabels = df2.category_id\n\nprint(\"Each of the %d complaints is represented by %d features (TF-IDF score of unigrams and bigrams)\" %(features.shape))","0730a36a":"# Finding the three most correlated terms with each of the product categories\nN = 3\nfor Product, category_id in sorted(category_to_id.items()):\n  features_chi2 = chi2(features, labels == category_id)\n  indices = np.argsort(features_chi2[0])\n  feature_names = np.array(tfidf.get_feature_names())[indices]\n  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n  print(\"\\n==> %s:\" %(Product))\n  print(\"  * Most Correlated Unigrams are: %s\" %(', '.join(unigrams[-N:])))\n  print(\"  * Most Correlated Bigrams are: %s\" %(', '.join(bigrams[-N:])))","01aea5ee":"X = df2['Consumer_complaint'] # Collection of documents\ny = df2['Product'] # Target or the labels we want to predict (i.e., the 13 different complaints of products)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25,\n                                                    random_state = 0)","c6121a79":"models = [\n    RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0),\n    LinearSVC(),\n    MultinomialNB(),\n    LogisticRegression(random_state=0),\n]\n\n# 5 Cross-validation\nCV = 5\ncv_df = pd.DataFrame(index=range(CV * len(models)))\n\nentries = []\nfor model in models:\n  model_name = model.__class__.__name__\n  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n  for fold_idx, accuracy in enumerate(accuracies):\n    entries.append((model_name, fold_idx, accuracy))\n    \ncv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])","4be57e5a":"mean_accuracy = cv_df.groupby('model_name').accuracy.mean()\nstd_accuracy = cv_df.groupby('model_name').accuracy.std()\n\nacc = pd.concat([mean_accuracy, std_accuracy], axis= 1, \n          ignore_index=True)\nacc.columns = ['Mean Accuracy', 'Standard deviation']\nacc","56e6eca7":"plt.figure(figsize=(8,5))\nsns.boxplot(x='model_name', y='accuracy', \n            data=cv_df, \n            color='lightblue', \n            showmeans=True)\nplt.title(\"MEAN ACCURACY (cv = 5)\\n\", size=14);","4a230fae":"X_train, X_test, y_train, y_test,indices_train,indices_test = train_test_split(features, \n                                                               labels, \n                                                               df2.index, test_size=0.25, \n                                                               random_state=1)\nmodel = LinearSVC()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)","e4f18175":"df2.head()","9e1cea3c":"df2['category_id'].unique()","c5eed65a":"print('\\t\\t\\t\\tCLASSIFICATIION METRICS\\n')\nprint(metrics.classification_report(y_test, y_pred, \n                                    target_names= df2['Product'].unique()))","a850c04e":"conf_mat = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(8,8))\nsns.heatmap(conf_mat, annot=True, cmap=\"Blues\", fmt='d',\n            xticklabels=category_id_df.Product.values, \n            yticklabels=category_id_df.Product.values)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title(\"CONFUSION MATRIX - LinearSVC\\n\", size=16);","c6b96d92":"for predicted in category_id_df.category_id:\n  for actual in category_id_df.category_id:\n    if predicted != actual and conf_mat[actual, predicted] >= 20:\n      print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual], \n                                                           id_to_category[predicted], \n                                                           conf_mat[actual, predicted]))\n    \n      display(df2.loc[indices_test[(y_test == actual) & (y_pred == predicted)]][['Product', \n                                                                'Consumer_complaint']])\n      print('')","da02dad2":"model.fit(features, labels)\n\nN = 4\nfor Product, category_id in sorted(category_to_id.items()):\n  indices = np.argsort(model.coef_[category_id])\n  feature_names = np.array(tfidf.get_feature_names())[indices]\n  unigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 1][:N]\n  bigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 2][:N]\n  print(\"\\n==> '{}':\".format(Product))\n  print(\"  * Top unigrams: %s\" %(', '.join(unigrams)))\n  print(\"  * Top bigrams: %s\" %(', '.join(bigrams)))","9a06aa31":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25,\n                                                    random_state = 0)\n\ntfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n                        ngram_range=(1, 2), \n                        stop_words='english')\n\nfitted_vectorizer = tfidf.fit(X_train)\ntfidf_vectorizer_vectors = fitted_vectorizer.transform(X_train)\n\nmodel = LinearSVC().fit(tfidf_vectorizer_vectors, y_train)","cc0904e9":"new_complaint = \"\"\"I have been enrolled back at XXXX XXXX University in the XX\/XX\/XXXX. Recently, i have been harassed by \\\nNavient for the last month. I have faxed in paperwork providing them with everything they needed. And yet I am still getting \\\nphone calls for payments. Furthermore, Navient is now reporting to the credit bureaus that I am late. At this point, \\\nNavient needs to get their act together to avoid me taking further action. I have been enrolled the entire time and my \\\ndeferment should be valid with my planned graduation date being the XX\/XX\/XXXX.\"\"\"\nprint(model.predict(fitted_vectorizer.transform([new_complaint])))","e9036ec8":"new_complaint_2 = \"\"\"I have been getting robo calls from a debt collection agency called \" Alliance 1 \\'\\' for over XXXX months. \nThe calls average XXXX times per week. They are attempting to collect a debt for someone whose name sounds \nlike \" XXXX XXXX \\'\\'. I am sick and tired of their harrassement and want the calls to stop. \\n\"\"\"\nprint(model.predict(fitted_vectorizer.transform([new_complaint_2])))","b28d2ce3":"test_df = pd.read_csv('..\/input\/consumer-complaints-test-dataset\/Edureka_Consumer_Complaints_test.csv')\ntest_df.shape","b6c076c4":"test_df.head().T","8ecd4158":"# Create a new dataframe with two columns\ndf3 = test_df[['Product', 'Consumer complaint narrative']].copy()\n\n# Remove missing values (NaN)\ndf3 = df3[pd.notnull(df3['Consumer complaint narrative'])]\n\n# Renaming second column for a simpler name\ndf3.columns = ['Product', 'Consumer_complaint'] \n\ndf3.shape","3622d0c5":"# Percentage of complaints with text\ntotal = df3['Consumer_complaint'].notnull().sum()\nround((total\/len(test_df)*100),1)","43f0342d":"pd.DataFrame(df3.Product.unique()).values","5bc4768b":"pd.DataFrame(df2.Product.unique()).values","7c418223":"# Renaming categories\ndf3.replace({'Product': \n             {'Credit reporting, credit repair services, or other personal consumer reports': \n              'Credit reporting, repair, or other', \n              'Credit reporting': 'Credit reporting, repair, or other',\n             'Credit card': 'Credit card or prepaid card',\n             'Prepaid card': 'Credit card or prepaid card',\n             'Payday loan': 'Payday loan, title loan, or personal loan',\n             'Money transfer': 'Money transfer, virtual currency, or money service',\n             'Virtual currency': 'Money transfer, virtual currency, or money service'}}, \n            inplace= True)","16eea42c":"pd.DataFrame(df2.Product.unique())","b0f540d2":"df3.head()","614248ca":"Predictions = model.predict(fitted_vectorizer.transform(df3['Consumer_complaint']))\nPredictions.shape","37a3f78a":"Predictions_df = pd.DataFrame(Predictions,columns=['Predictions']).reset_index()\nPredictions_df.to_csv('predictions.csv',index=False)","c12ae82f":"print('\\t\\t\\t\\tCLASSIFICATIION METRICS\\n')\nprint(metrics.classification_report(df3['Product'], Predictions, \n                                    target_names= df3['Product'].unique()))","383079b1":"conf_mat = confusion_matrix(df3['Product'], Predictions)\nfig, ax = plt.subplots(figsize=(8,8))\nsns.heatmap(conf_mat, annot=True, cmap=\"Blues\", fmt='d',\n            xticklabels=category_id_df.Product.values, \n            yticklabels=category_id_df.Product.values)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title(\"CONFUSION MATRIX - LinearSVC\\n\", size=16);","72fb9652":"df3['Consumer_complaint'].iloc[0]","676013bc":"df3[df3['Consumer_complaint'] == df3['Consumer_complaint'].iloc[0]]","00cf19e2":"complaint1 = \"\"\"I WANT TO REQUEST A CREDIT LINE INCREASE OF XXXX I FELL I HAVE THE RIGHT CREDIT LINE DO TO MY INCOME AND THE LADY FROM CAPITAL ONE SAID IT IS BASED OFF INCOME \nAND ALSO DUE TO THE FACT THEY HAVE NOT MAILED MY CREDIT CARD TO ME YET AFTER I CALLED AND TOLD THEM TO. \\n\"\"\"","77fd6674":"print(model.predict(fitted_vectorizer.transform([complaint1])))","d0250f61":"df3[df3['Consumer_complaint'] == df3['Consumer_complaint'].iloc[16]]","42c788ac":"df3['Consumer_complaint'].iloc[16]","7db49510":"complaint2 = \"\"\"I have been actively filing loan modifications with B of A sinceXXXX. I am a XXXX Veteran and I did use my VA certificateto secure my home in XXXX. My home was finance thruCountry Wide before B of A bought my loan. B of A didservice my home for foreclosure while I was serving in theXXXX XXXX about XXXX XXXX and the loan was noteven 2 months behind. I came home in XXXX XXXX andrequested another loan modification but no response soI obtained an attorney. My attorney did respond to B ofA and requested all correspondence concerning my homebe sent to them and they were actively involved with myloan modification. In early XXXX I was notified by areal-estate agent that my home was listed for foreclosureand public auction. No XXXX service me or my attorney. Theforeclosure was listed in the local newspaper. This wasnews to me and my attorney since we both had no ideaand was actively involve with a loan modification at thetime. I have since been working with Attorney Generaloffice in Florida. I was given guidance from the AG tofile this complaint with CFPB along with their inquireabout my case. I have been told my request formodification has been denied for reasons such as \\'\\' I was not living in the home and it is a VA require-ment \\'\\' which is not true, \" insufficient income \\'\\', mycurrent husband signed a form stating he wouldcontribute {$1500.00} additional income to my household to help cover the cost and the list goes on. \\nI am currently enrolled in a XXXX programwith the XXXX VA and is receivingtreatment. My condition is described as \" XXXX \\'\\' at this time but with support from my husbandand adult children I am trying to do what isnecessary to keep my home. I am filing thiscomplaint because I too believe I am a victimof \" dual tracking \\'\\' with B of A. They are notowning up to their constant mistakes. I don\\'tunderstand why they will not work with me tohelp me keep my home vice foreclosing. I amnot trying to run from my responsibility of payingfor my home. I just need help with lowering the payment so I can afford it. Is that too much to ask for? \\nB of A pulled back the foreclosure because they justrealized we were working on a loan modification with mebut recently filed a petition in court another foreclosure. \\nI have no other option but to file bankruptcy to keepmy home. This not fair to me the consumer because allI wanted was a lower payment. \\n\"\"\"","8b854697":"print(model.predict(fitted_vectorizer.transform([complaint2])))","3032a25f":"df = pd.read_csv('\/kaggle\/input\/consumer-complaints-train-dataset\/Edureka_Consumer_Complaints_train.csv')","900a049d":"df.head(5).T","940728ab":"df.shape","8f2b3b09":"df1 = df.copy()","0de2159f":"df1.drop(['Date received','Date sent to company','Sub-product','Issue', \n          'Sub-issue','Consumer complaint narrative','ZIP code','Complaint ID'],axis=1,inplace=True)","2eca9fe4":"df1.shape","70f84d9b":"df1.isnull().sum()","7e43f6b0":"df1['Company public response'].value_counts()","7055d765":"df1['Consumer consent provided?'].value_counts()","57dc94ba":"df1['Tags'].value_counts()","c97dd478":"df1.drop(['Tags'],axis=1,inplace=True)","a0b2bc49":"df1.shape","aa7cd95c":"df1['Company public response'].notnull().sum()","d939d9c8":"df1['Consumer consent provided?'].fillna('Other',inplace=True)","8ae72cef":"df1['Consumer consent provided?'].isnull().sum()","dd67ceb5":"df1.isnull().sum()","fd139cd8":"df1['Company public response'].fillna('Company chooses not to provide a public response',inplace=True)","8df1a831":"df1.isnull().sum()","c4247e36":"df1.dropna(inplace=True)","60e6f8b8":"df1.shape","11156e59":"df1.head(5).T","47a55971":"from sklearn.preprocessing import LabelEncoder","9e33f878":"df1 = df1.apply(LabelEncoder().fit_transform)","ca4ed7c9":"df1.info()","294d6054":"df1.head()","8182b29c":"df2 = df1.sample(10000, random_state=1).copy()","0d3970cb":"df2['Consumer disputed?'].value_counts()","ae60bfdb":"X = df2.drop(['Consumer disputed?'],axis=1)\ny = df2['Consumer disputed?']","f2bcd33c":"from sklearn.model_selection import train_test_split","da0fcf0c":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25,\n                                                    random_state = 0)","f641dbeb":"log_reg = LogisticRegression(random_state=101,class_weight='balanced')","38d28fbb":"log_reg.fit(X_train,y_train)","645c7e5b":"predictions = log_reg.predict(X_test)","3e5910e6":"from sklearn.metrics import classification_report","84788949":"import seaborn as sns\nprint(metrics.accuracy_score(y_test,predictions))\nprint('********************************************')\nprint('Confusion matrix')\nlr_cfm=metrics.confusion_matrix(y_test, predictions)\n\n\nlbl1=[\"Predicted 1\", \"Predicted 2\"]\nlbl2=[\"Actual 1\", \"Actual 2\"]\n\nsns.heatmap(lr_cfm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=lbl1, yticklabels=lbl2)\nplt.show()\n\nprint('**********************************************')\nprint(metrics.classification_report(y_test,predictions))","f24c97dd":"from sklearn.metrics import roc_curve, auc\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,predictions)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nprint (roc_auc)","9f8e90a1":"import matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.2])\nplt.ylim([-0.1,1.2])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","c0514c88":"from sklearn.ensemble import RandomForestClassifier\nrandom_grid = {'n_estimators': range(5,20,3),\n              'max_features' : ['auto', 'sqrt'],\n              'max_depth' : [5,10,20,30],\n              'min_samples_split':[2,5,10],\n              'criterion':['entropy'],\n              'min_samples_leaf':[1,2,4]}\n\nrf=RandomForestClassifier(oob_score=True,class_weight='balanced')\nrf_gs = GridSearchCV(rf, random_grid, cv = 5, n_jobs=-1, verbose=2)\n\nrf_gs.fit(X_train, y_train)\ny_pred = rf_gs.predict(X_test)","cd8419fe":"print(metrics.accuracy_score(y_test,y_pred))\nprint('*******************************************')\nprint('Confusion matrix')\nrf_cfm=metrics.confusion_matrix(y_test, y_pred)\n\nlbl1=[\"Predicted 1\", \"Predicted 2\"]\nlbl2=[\"Actual 1\", \"Actual 2\"]\n\nsns.heatmap(rf_cfm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=lbl1, yticklabels=lbl2)\nplt.show()\n\nprint('********************************************')\nprint(metrics.classification_report(y_test,y_pred))","db95fa56":"# Actual Values of y_test\nprint (y_test.value_counts())\nprint (\"Null Accuracy:\",y_test.value_counts().head(1) \/ len(y_test))","754f35f3":"predict_probabilities = rf_gs.predict_proba(X_test)\nfinal_metric = roc_auc_score(y_test, predict_probabilities[:,1])\nprint (final_metric)","87f40a04":"from xgboost.sklearn import XGBClassifier","466a064a":"xgb_model = XGBClassifier(num_class = 2,\n                           objective=\"multi:softprob\",\n                           eval_metric=\"mlogloss\",\n                           seed=42) ","5528099b":"xgb_model.fit(X_train, y_train)","31253bd0":"xgboost_pred=xgb_model.predict(X_test)","62b6f065":"print(metrics.accuracy_score(y_test,xgboost_pred))\nprint('************')\nprint('Confusion matrix')\nxgboost_cm=metrics.confusion_matrix(y_test, xgboost_pred)\n\n\nlbl1=[\"Predicted 1\", \"Predicted 2\"]\nlbl2=[\"Actual 1\", \"Actual 2\"]\n\nsns.heatmap(xgboost_cm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=lbl1, yticklabels=lbl2)\n\nplt.show()\n\nprint('************\\n Classification report')\nprint(metrics.classification_report(y_test,xgboost_pred))","e1b03422":"predict_probabilities = xgb_model.predict_proba(X_test)\nfrom sklearn.metrics import roc_auc_score\nfinal_metric = roc_auc_score(y_test, predict_probabilities[:,1])\nprint (final_metric)","325bde87":"xgb_gs_clf = XGBClassifier(num_class = 2,\n                           objective=\"multi:softprob\",\n                           eval_metric=\"mlogloss\",\n                           seed=42)                         \n                        \nparam_grid = {\"max_depth\": [10,15,20],\n              \"n_estimators\": range(5,20,5) , \n              \"gamma\": [0.03,0.05], \n              \"learning_rate\": [0.01,0.05]}\n#              \"min_child_weight\": [5,10], \n#              \"colsample_bytree\": [0.4,0.8], \n#              \"subsample\": [0.50,0.85]} \n\ngrid_search = GridSearchCV(xgb_gs_clf, \n                           param_grid=param_grid,\n                           cv = 5,\n                           n_jobs=-1,\n                           scoring='neg_log_loss',\n                           verbose=2)\ngrid_search.fit(X_train,y_train)\ngrid_search.best_params_","7b788b91":"xgboost_y_pred=grid_search.predict(X_test)    ","2afd8454":"print(metrics.accuracy_score(y_test,xgboost_y_pred))\nprint('*************************************************')\nprint('Confusion matrix')\nxgboost_cfm=metrics.confusion_matrix(y_test, xgboost_y_pred)\n\n\nlbl1=[\"Predicted 1\", \"Predicted 2\"]\nlbl2=[\"Actual 1\", \"Actual 2\"]\n\nsns.heatmap(xgboost_cfm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=lbl1, yticklabels=lbl2)\n\nplt.show()\n\nprint('***************************************************\\n Classification report')\nprint(metrics.classification_report(y_test,xgboost_y_pred))","56f583d7":"predict_probabilities = grid_search.predict_proba(X_test)\nfinal_metric = roc_auc_score(y_test, predict_probabilities[:,1])\nprint (final_metric)","0aa36208":"import lightgbm as lgb\n\nd_train= lgb.Dataset(X_train, label = y_train) ","397b973f":"params = {}\nparams['learning_rate'] = 0.001                                             # Learning rate\nparams['boosting_type'] = 'gbdt'                                           # gbdt = gradient boosted decision tree\nparams['objective'] = 'multiclass'                                         # Multi class classification\nparams['metric'] = 'multi_logloss'\nparams['num_classes'] = 5 \nparams['eval_metric']='auc', 'binary_logloss'\n#params['sub_feature'] = 0.5\n#params['min_data'] = 50\n#params['max_depth'] = 10\n\n\nclf = lgb.train(params, d_train)     ","76859092":"#Prediction\ny_pred=clf.predict(X_test)","81204a3b":"clf.params","6fcd79b5":"predictions = []\npredictions_proba = []\nfor x in y_pred:\n    predictions.append(np.argmax(x))\n    predictions_proba.append(max(x))","7a26a45e":"lgb_y_pred = np.array(predictions)","b39afc38":"print(metrics.accuracy_score(y_test,lgb_y_pred))\nprint('*************************************************')\nprint('Confusion matrix')\nlgb_cfm=metrics.confusion_matrix(y_test, lgb_y_pred)\n\n\nlbl1=[\"Predicted 1\", \"Predicted 2\"]\nlbl2=[\"Actual 1\", \"Actual 2\"]\n\nsns.heatmap(lgb_cfm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=lbl1, yticklabels=lbl2)\n\nplt.show()\n\nprint('***************************************************\\n Classification report')\nprint(metrics.classification_report(y_test,lgb_y_pred))","f44ed858":"final_metric = roc_auc_score(y_test, predictions_proba)\nprint (final_metric)","748f03a4":"test_df = pd.read_csv('..\/input\/consumer-complaints-test-dataset\/Edureka_Consumer_Complaints_test.csv')","56ef0401":"test_df.shape","7fa568fb":"test_df.drop(['Date received','Date sent to company','Sub-product','Issue', \n          'Sub-issue','Consumer complaint narrative','ZIP code','Complaint ID'],axis=1,inplace=True)","f5477974":"test_df.isnull().sum()","42dc4720":"test_df.drop(['Tags'],axis=1,inplace=True)","390a6a22":"test_df['Consumer consent provided?'].fillna('Other',inplace=True)","ee2198ea":"test_df['Company public response'].fillna('Company chooses not to provide a public response',inplace=True)","598c84e5":"test_df.isnull().sum()","68e4882d":"test_df.dropna(inplace=True)","7fa0aa33":"test_df.shape","674cb3c6":"test_df1 = test_df.apply(LabelEncoder().fit_transform)","b58b1696":"test_df1.head()","206ab4eb":"test_predictions = xgb_model.predict(test_df1)","d8063f21":"test_predictions.shape","2de9a6dc":"test_predictions_df = pd.DataFrame(test_predictions,columns=['Consumer Disputed Predictions'])\ntest_predictions_df.to_csv('Cust_Dispute_predictions.csv',\"w\",index=False)","0bf3e794":"# LightGBM :","84eb7671":"There are 12 different classes or categories (target). However; it is observed that some classes are contained in others. For instance, \u2018Credit card\u2019 and \u2018Prepaid card\u2019 are contained in \u2018Credit card or prepaid card\u2019 category. Now, imagine there is a new complaint about Credit card and we want to classify it. The algorithm can either classify this complaint as 'Credit card' or 'Credit card or prepaid' and it would be correct. Nevertheless, this would affect model performance. In order to avoid this problem, the names of some categories were renamed.","3291826a":"Filling the NA values in Company public response column with **'Company chooses not to provide a public response'**.","2a2a740c":"Since it is an **imbalanced dataset**, we will set the <b>class_weight<\/b> parameter to 'balanced'","9d38e0cf":"July receives the most number of disputes during the year while March receives the lowest number of disputes.","f109dbd0":"<b>LightGBM<\/b> performance is not upto the mark.It is unable to correctly classify even a single datapoint that belonged to class 1. We can conclude that <b>Consumer Complaints<\/b> can use the <b>XGboost<\/b> model to identify if the Consumer Disputed or not.","ba9e150d":"We are at a pretty good accuracy of 0.80 which is slightly higher than our Null Accuracy of 0.79 though the recall and f1-score of 1 is 0.","adac13d4":"The model is predicting most of categories correctly with an accuracy of around 0.80.","369d6cf5":"The accuracy can be low due to the fact that there are a lot of <b>categorical variables<\/b> in the dataset. As there is a lot of <b>non-linearity<\/b>, let us use <b>tree<\/b> algorithms.","4aa7081b":"The overall accuracy is less and the precision and f1-score is really low for 1. Let's try out other methods.","7ad26a7e":"# Consumer Complaints Dataset","9e527540":"Transferring the Predictions to predictions.csv.","f17e02b1":"# Part 3 : CLASSIFICATION MODELS AND FEATURE ENGINEERING","e91a6695":"<a id='sum'><\/a>\n## Comparison of model performance\n\nThe best mean acuracy was obtained with LinearSVC.","cd50b540":"Overall the number of Complaints have gone down for all the top 5 Companies.","713d7d2e":"Let's understand how the No of Complaints changed over the years and months.","961925d9":"We can see that there a lot of null values in the Dataset.","e5d4b964":"# Random Forest :","62c38c4c":"From more than 358810 complaints, there are about 56180 cases with text (~ 15.7% of the original dataset is not null). This is still a good number to work with. Now let's have a look at the categories we want to classify each complaint.","06798d59":"* Let's define our **X** and **Y** and start working on the training the model.","175a8844":"Making a copy of the dataset to perform the operations and prepare a good dataset for training and testing.","af4909b3":"Again, the algorithm correctly classified the caomplaint as __\"Debt collection\"__. \nAlthough our model is not going to be all the time correct when classifying new complaints, it does a good job.","05297f91":"<a id='cm'><\/a>\n## Confusion Matrix : \n\nA Confusion Matrix is a table which rows represent the actual class and columns represents the predicted class.<br><br>\nIf we had a perfect model that always classifies correctly a new complaint, then the confusion matrix would have values in the diagonal only (where predicted label = actual label).","13afc218":"<a id='pre'><\/a>\n## Text Preprocessing :\n\nThe text needs to be transformed to vectors so as the algorithms will be able make predictions. In this case it will be used the Term Frequency \u2013 Inverse Document Frequency (TFIDF) weight to evaluate __how important a word is to a document in a collection of documents__.\n\nAfter removing __punctuation__ and __lower casing__ the words, importance of a word is determined in terms of its frequency.","64199180":"Thus, most of the complaints that Bank of America receives are regarding Mortgages.\n","40e499be":"<a id='ml'><\/a>\n## Multi-Classification models : \n\nThe classification models evaluated are: \n* Random Forest\n* Linear Support Vector Machine\n* Multinomial Naive Bayes \n* Logistic Regression.","16520799":"There are 118670 after the Data Cleaning, which is a good number to fit the best model.","52a550f4":"The information available is very less and there is no neutral value that we can use to fill the NA values. ","0c2770e4":"The accuracy has surely increased ! But hold on...we need to remember that it is an imbalanced dataset. Let us calculate the null accuracy.","d6d6db64":"<a id='m'><\/a>\n## Models:","ac810149":"The primary issues for all the complaints in California are Loan modification,collection,foreclosure,Loan servicing, payments, escrow account.","80763560":"We will try the <b>Bagging<\/b> technique <b>Random Forest<\/b> on the dataset. Since we do not know the optimal hyperparameters for the forest, let us use GridSearch cross-validation to identify them.","80602819":"# Grid Search :\n\nHere are the parameters for <a href='https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV'>GridSearchCV<\/a>:\n- __estimator__: `model`\n- __param_grid__: `dist or list of dictionaries` Parameters to the estimator\/model\n- __scoring__: `string,callable, list\/tuple, dict or None, default: None` Evaluating metrics\n- __cv__: `int or callable` Cross validation","137587cd":"It has predicted the second complaint correctly as well. Thus, the model is doing a good job in predicting the product category using the Consumer Complaint narratives.","531f0a1d":"In Consumer consent provided? column Other seems to be a neutral column that can be used to fill in the NA values.","106ca43b":"We have 3,58,810 (rows) and 18 features (columns).","d727e1a6":"We can conclude that the data isn't balanced.","1742e7cd":"Shape of the Predictions is same as the test date i.e no of rows.","7d5af087":"# Part 1: DATA EXPLORATION","a05ba8fa":"<a id='pred'><\/a>\n# Predictions :\n\nNow let's make a few predictions on unseen data.<br>","ace86419":"The number of disputes after a delay in response in really low. However, this does not signify that late responses will not attract dispute. It may mean that the consumers issue is either resolved or he is no longer interested in the issue.","96809daa":"# XgBoost with Grid Search :","375b2eff":"# Part 2 : TEXT BASED MODELLING","44c23eb9":"The number of classes were reduced from 12 to 10. <br><br>Now we need to represent each class as a number, so as our predictive model can better understand the different categories.","206c9f06":"Let's have a look at the dataset after conversion.","1ec1c89a":"There are NA values in the state columns and it is something we can't just fill in randomly. Hence, dropping those NA values.","157cf4be":"# Table of Content\n\n* [Goal](#obj)\n* [Importing packages and loading data](#imp)\n* [Exploratory Data Analysis (EDA) and Feature Engineering](#eda)\n* [Text Preprocessing](#pre)\n* [Multi-Classification models](#ml)\n    * [Spliting the data: train and test](#sp)\n    * [Models](#m)\n* [Comparison of model performance](#sum)\n* [Model Evaluation](#ev)\n    * [Precision, Recall, F1-score](#f1)\n    * [Confusion Matrix](#cm)\n* [Predictions](#pred)\n* [Classification Models and Feature Engineering](#pred)\n* [Exploratory Data Analysis (EDA) and Feature Engineering](#pred)\n* [Model Building](#ml)\n    * [Logistic Regression](#sp)\n    * [Random Forest Classifier](#ml)\n        * [GridSearchCV](#sp)\n        * [XGBoost](#ml)\n        * [XGBoost with GridSearchCV](#ml)\n        * [LightGBM](#ml)\n* [Predictions](#pred)","25837dd7":"Hurray! <b>Xgboost<\/b> gave us a higher ROC_AUC score compared to Random Forest. Let us see if we can make this better by tuning the real strengths of Xgboost - its <b>Hyperparameters.<\/b> ","7df8a6b8":"The most common company response is Company chooses not to provide a public response which we can use to fill the remaining NA values.","80ed3477":"There is no improvement over the earlier Xgboost model. Let's try out with <b>more hyperparameter combinations<\/b> and see if there is a jump in the roc_auc value.","983d60e8":"#### Most correlated terms with each category : ","cbaa2023":"The algorithm has classified this text as a \"Student loan\" complaint. The complaint is regarding Student Loans and it has predicted it correctly.","421d90e6":"Because the computation is time consuming (in terms of CPU), the data was sampled.","a61b0f11":"<a id='ev'><\/a>\n## Model Evaluation :","e736d83a":"<a id='m'><\/a>\n## Precision, Recall, F1-score :","27137e97":"Let's clean the data in test dataset.","99aec3eb":"The state of California mainly has most complaints around Mortgage. Let's find out what kind of issues are raised for this particular product.","e3938d17":"Null accuracy itself is **0.79**. So our accuracy of **0.72** is less than that. Also, as discussed earlier, **f1-score** is more important in these scenarios. But even that is pretty low. By the way, is f1-score still the right metric in this scenario?","37db5173":"Let's take a look at the various product types.","7c4e5939":"<a id='obj'><\/a>\n## Goal:<br>\nClassification of Consumer Narratives to their respective Product Category and Predicting if a consumer will dispute or not based on the features.<br><br>\nClassification algorithms: Linear Support Vector Machine (LinearSVM), Random Forest, Multinomial Naive Bayes and Logistic Regression.<br><br>\nNote: Text classification is an example of supervised machine learning since we train the model with labelled data (complaints about and specific finance product is used for train a classifier.","4c9eaa7e":"__Data__: Consumer complaints received about financial products and services<br>\n\nThese are real world complaints received about financial products and services. Each complaint has been labeled with a specific product; therefore, this is a supervised text classification problem. With the aim to classify future complaints based on its content, we used different machine learning algorithms can make more accurate predictions (i.e., classify the complaint in one of the product categories).","ba331d7c":"Let's see what is the classification that our model gives to this new complaint.","8374c66b":"### \u201cTerm Frequency \u2013 Inverse Document Frequency \n\n__TF-IDF__ is the product of the __TF__ and __IDF__ scores of the term.<br><br> $$\\text{TF-IDF}=\\frac{\\text{TF}}{\\text{IDF}}$$<br>\n\n__Term Frequency :__ This summarizes how often a given word appears within a document.\n\n$$\\text{TF} = \\frac{\\text{Number of times the term appears in the doc}}{\\text{Total number of words in the doc}}$$<br><br>\n__Inverse Document Frequency:__ This downscales words that appear a lot across documents. A term has a high IDF score if it appears in a few documents. Conversely, if the term is very common among documents (i.e., \u201cthe\u201d, \u201ca\u201d, \u201cis\u201d), the term would have a low IDF score.<br>\n\n$$\\text{IDF} = \\ln\\left(\\frac{\\text{Number of docs}}{\\text{Number docs the term appears in}} \\right)$$<br>\n\nTF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents. The higher the TFIDF score, the rarer the term is. For instance, in a Mortgage complaint the word _mortgage_ would be mentioned fairly often. However, if we look at other complaints, _mortgage_ probably would not show up in many of them. We can infer that _mortgage_ is most probably an important word in Mortgage complaints as compared to the other products. Therefore, _mortgage_ would have a high TF-IDF score for Mortgage complaints.\n\nTfidfVectorizer class can be initialized with the following parameters:\n* __min_df__: remove the words from the vocabulary which have occurred in less than \u2018min_df\u2019 number of files.\n* __max_df__: remove the words from the vocabulary which have occurred in more than _\u2018max_df\u2019 * total number of files in corpus_.\n* __sublinear_tf__: set to True to scale the term frequency in logarithmic scale.\n* __stop_words__: remove the predefined stop words in 'english'.\n* __use_idf__: weight factor must use inverse document frequency.\n* __ngram_range__: (1, 2) to indicate that unigrams and bigrams will be considered.","cbfbf99d":"More than half of the Users submitted their complaints via web.","073fae60":"In general, the confusion matrix looks good (clear diagonal that represents correct classifications). Nevertheless, there are cases were the complaint was classified in a wrong class.\n\n## Misclassified complaints :\nLet\u2019s have a look at the cases that were wrongly classified.","afc85563":"# Model Building :","48dbd3c9":"Dropping the Tags column.","384a4509":"Bank of America has the highest number of complaints among all the other companies. Lets see where are these complains from.","4d645acc":"Dropping Sub-product,Issue, Sub-issue as all of them are related to the Product Column. Dropping the Date columns as they don't have much contribution and the same goes for ZIP code and Complaint ID.","ae20e51f":"We can see that for most of the companies the Consumer disputed after the timely response.","1d47b8de":"# Predictions : ","76d3bd00":"For imbalanced datasets, ROC_AUC is considered to be a more relevant metric than f1-score and accuracy as it is independent of threshold value.","ebe684e5":"Now, this is a metric that makes sense. We should get the **ROC_AUC** score as close to 1 as possible. Let us now see how Boosting algorithms fare.","0a0e1a88":"Converting all the object columns using ****Label Encoder()****.","298b643b":"There are **355907** after cleaning which a really good number to work on.","3a25366c":"As we can see, the Timely response rate is quite high. Let's how it impacts the Consumer Disputes.","756ba313":"It has predicted the first complaint correctly.","44a73675":"We can see that most of the complaints for Mortgage were regarding Loan modification,collection,foreclosure,Loan servicing, payments, escrow account. ","73b1ceb4":"The dataset contains features that are not necessary to solve our multi-classification problem. For this text classification problem, we are going to build another dataframe that contains \u2018Product\u2019 and \u2018Consumer complaint narrative\u2019 (renamed as 'Consumer_complaint').","b01635aa":"Let's transfer our predictions to an external csv file.","67ef2470":"California has the highest number of complaints as compared to others. Let's see what are these complaints about.","663f4149":"Filling the NA values in **Consumer consent provided?** with **'Other'**.","099f1fb8":"<a id='imp'><\/a>\n## Importing packages and loading data","73ce98f3":"Let's try out our best model to fit our Test Dataset.","2255b180":"# XGBoost : ","9017d249":"<a id='sp'><\/a>\n### Spliting the data into train and test sets\nThe original data was divided into features (X) and target (y), which were then splitted into train (75%) and test (25%) sets. Thus, the algorithms would be trained on one set of data and tested out on a completely different set of data (not seen before by the algorithm)."}}