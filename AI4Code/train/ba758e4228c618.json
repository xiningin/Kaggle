{"cell_type":{"fcd9c81f":"code","98ad7ec5":"code","c14c6127":"code","469ac05d":"code","c1fbd0b2":"code","37621959":"code","54f24d25":"code","bf7fb3db":"code","e5755721":"code","258b4b95":"code","f4a3a4f8":"code","e4e343cb":"code","2e5ad34f":"code","ffd21e91":"code","6b0eda50":"code","3bbaa9fc":"code","6e428e84":"code","d979a0bc":"code","53981391":"code","582d95ba":"code","c73e3fe9":"code","ecf47915":"code","a4495efe":"code","fc289b22":"code","38430b60":"markdown","505773d1":"markdown","064836fd":"markdown","b1328f3f":"markdown","b525f41e":"markdown","4ddd6902":"markdown","c4e5c3e8":"markdown","8db62686":"markdown","9026768f":"markdown","933278f7":"markdown","894a3c97":"markdown","f796af22":"markdown","e6b52a11":"markdown","7620b5cd":"markdown","631bea82":"markdown","9e51238f":"markdown","510dae22":"markdown","9db9c217":"markdown","424c25b1":"markdown","7f826b01":"markdown","ee18114f":"markdown","01be8676":"markdown","7518b657":"markdown","fa88a808":"markdown","5b3fb537":"markdown"},"source":{"fcd9c81f":"# import libraries\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport keras\nfrom tensorflow.keras.layers import Dense,Flatten,Conv2D,Dropout,BatchNormalization,MaxPooling2D,AveragePooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.utils import plot_model\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot,iplot\ninit_notebook_mode(connected=True)\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport plotly","98ad7ec5":"img_height= 28\nimg_width = 28\nnum_classes= 10\nbatch_size= 128\nepoch = 50\nval_size=0.2\nrandom_state = 42","c14c6127":"train_data = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\ntest_data = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')","469ac05d":"train_data.head(2)","c1fbd0b2":"print(\"Fashion MNIST train -  rows:\",train_data.shape[0],\" columns:\", train_data.shape[1])\nprint(\"Fashion MNIST test -  rows:\",test_data.shape[0],\" columns:\", test_data.shape[1])","37621959":"# create a dictionary. for each type of label\nlabels = {0: 'T-shirt\/top',1:'Trouser', 2: 'Pullover', 3:'Dress', 4:'Coat', 5:'Sandal',6:'Shirt',7:'Sneaker',8:'Bag',\\\n         9:'Ankle Boot'}\ndef get_classes_distribution(data):\n    # get label count\n    label_counts = data['label'].value_counts()\n    # no of samples or data points\n    total_samples = len(data)\n    # count the number of items in each case\n    for i in range(num_classes):\n        label = labels[label_counts.index[i]]\n        count = label_counts.values[i]\n        percent = (count\/total_samples)*100\n        print(\"{:<20s}:    {} or {}%\".format(label, count, percent))\nget_classes_distribution(train_data)","54f24d25":"get_classes_distribution(test_data)","bf7fb3db":"plt.figure(figsize=(9,5))\nax= sns.countplot(x='label',data=train_data,palette='Pastel1')\nxtickslocs = ax.get_xticks()\nplt.xticks(ticks=xtickslocs, labels=labels.values())\nplt.show()","e5755721":"def sample_images_data(data):\n    sample_images = []\n    sample_labels = []\n    for i in labels.keys():\n        # get four samples for each category\n        samples= data[data['label'] == i].head(4)\n        # append the samples to the sample list\n        for j,s in enumerate(samples.values):\n            # first column contain labels hence index should start from 1\n            img= np.array(samples.iloc[j,1:]).reshape(img_height,img_width)\n            sample_images.append(img)\n            sample_labels.append(samples.iloc[j,0])\n    print('Total number of samples to plot:', len(sample_images))\n    return sample_images,sample_labels\ntrain_sample_images, train_sample_labels = sample_images_data(train_data)\n\n","258b4b95":"def plot_sample_images(data_sample_images, data_sample_labels,cmap='Blues'): \n    # plot the sample images now\n    f, ax= plt.subplots(5,8, figsize=(18,14))\n    \n    for i, img in enumerate(data_sample_images):\n        ax[i\/\/8, i%8].imshow(img, cmap=cmap)\n        ax[i\/\/8,i%8].axis=('off')\n        ax[i\/\/8,i%8].set_title(labels[train_sample_labels[i]])\n    plt.show()\n    \nplot_sample_images(train_sample_images,train_sample_labels)\n    ","f4a3a4f8":"test_sample_images, test_sample_labels = sample_images_data(test_data)\nplot_sample_images(test_sample_images,test_sample_labels)","e4e343cb":"# data preprocessing\n\ndef data_preprocessing(raw):\n    out_y= tf.keras.utils.to_categorical(raw.label, num_classes)\n    num_images = raw.shape[0]\n    x_as_array = raw.values[:,1:] #excluding ist column which is label\n    x_shaped_array = x_as_array.reshape(num_images,img_height,img_width,1)\n    out_x= x_shaped_array\/255\n    return out_x,out_y","2e5ad34f":"# prepare the data\nX,y = data_preprocessing(train_data)\nX_test,y_test=data_preprocessing(test_data)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_size, random_state=random_state)","ffd21e91":"print(\"Fashion MNIST train -  rows:\",X_train.shape[0],\" columns:\", X_train.shape[1:3],\"channels:\", X_train.shape[3])\nprint(\"Fashion MNIST valid -  rows:\",X_val.shape[0],\" columns:\", X_val.shape[1:3],\"channels:\", X_val.shape[3])\nprint(\"Fashion MNIST test -  rows:\",X_test.shape[0],\" columns:\", X_test.shape[1:3],\"channels:\", X_test.shape[3])","6b0eda50":"def plot_count_per_class(yd):\n    ydf = pd.DataFrame(yd)\n    f, ax = plt.subplots(1,1, figsize=(12,6))\n    g = sns.countplot(ydf[0], order = np.arange(0,10),palette='Pastel1')\n    g.set_title(\"Number of items for each class\")\n    g.set_xlabel(\"Category\")\n    \n    for p, label in zip(g.patches, np.arange(0,10)):\n        g.annotate(labels[label], (p.get_x(), p.get_height()+50))\n        \n    plt.show()  \n    \n\n\ndef get_count_per_class(yd):\n    ydf = pd.DataFrame(yd)\n    # Get the count for each label\n    label_counts = ydf[0].value_counts()\n\n    # Get total number of samples\n    total_samples = len(yd)\n\n\n    # Count the number of items in each class\n    for i in range(len(label_counts)):\n        label = labels[label_counts.index[i]]\n        count = label_counts.values[i]\n        percent = (count \/ total_samples) * 100\n        print(\"{:<20s}:   {} or {}%\".format(label, count, percent))\n\nplot_count_per_class(np.argmax(y_train,axis=1))\nget_count_per_class(np.argmax(y_train,axis=1))","3bbaa9fc":"plot_count_per_class(np.argmax(y_val,axis=1))\nget_count_per_class(np.argmax(y_val,axis=1))","6e428e84":"'''\nLet's apply some data augmentation.\n\nData augmentation is a set of techniques used to generate new training samples from the original ones\nby applying jitters and perturbations such that the classes labels are not changed.\nIn the context of computer vision, these random transformations can be translating,\nrotating, scaling, shearing, flipping etc.\n\nData augmentation is a form of regularization because the training algorithm is being\nconstantly presented with new training samples,\nallowing it to learn more robust and discriminative patterns\nand reducing overfitting.\n'''\n\n#data_aug= ImageDataGenerator(\n#    featurewise_center=False,\n#    samplewise_center=False,\n#    featurewise_std_normalization=False,\n#   samplewise_std_normalization=False,\n#   zca_whitening=False,\n#   rotation_range=False,\n#   zoom_range = False, \n#   width_shift_range=0.005,\n#   height_shift_range=0.005,\n#   horizontal_flip=True,\n#   vertical_flip=False\n#)","d979a0bc":"# Model\nmodel = Sequential()\n\n# Add convolution 2D\nmodel.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=(img_height,img_width,1),kernel_initializer='he_normal'))\n# Add batch normalisation layer\nmodel.add(BatchNormalization())\n# Add maxpooling layer\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n# Add dropouts tot the model\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(64, kernel_size=(3,3),activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(128,(3,3),activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.3))\nmodel.add(Flatten())\nmodel.add(Dense(512,activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(num_classes,activation='softmax'))\n\n#compile model\nmodel.compile(loss=keras.losses.categorical_crossentropy,optimizer='adam', metrics=['accuracy'])","53981391":"model.summary()","582d95ba":"# Callbacks\ncallback= EarlyStopping(monitor='val_loss', patience=1,verbose=1,min_delta=0)\n","c73e3fe9":"train_model = model.fit(X_train,y_train, batch_size=batch_size,\n                  epochs=epoch,\n                  verbose=1,\n                  validation_data=(X_val, y_val))","ecf47915":"score = model.evaluate(X_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","a4495efe":"def create_trace(x,y,ylabel,color):\n        trace = go.Scatter(\n            x = x,y = y,\n            name=ylabel,\n            marker=dict(color=color),\n            mode = \"markers+lines\",\n            text=x\n        )\n        return trace\n    \ndef plot_accuracy_and_loss(train_model):\n    hist = train_model.history\n    acc = hist['accuracy']\n    val_acc = hist['val_accuracy']\n    loss = hist['loss']\n    val_loss = hist['val_loss']\n    epochs = list(range(1,len(acc)+1))\n    \n    trace_ta = create_trace(epochs,acc,\"Training accuracy\", \"Green\")\n    trace_va = create_trace(epochs,val_acc,\"Validation accuracy\", \"Red\")\n    trace_tl = create_trace(epochs,loss,\"Training loss\", \"Blue\")\n    trace_vl = create_trace(epochs,val_loss,\"Validation loss\", \"Magenta\")\n   \n    fig = plotly.subplots.make_subplots(rows=1,cols=2, subplot_titles=('Training and validation accuracy',\n                                                             'Training and validation loss'))\n    fig.append_trace(trace_ta,1,1)\n    fig.append_trace(trace_va,1,1)\n    fig.append_trace(trace_tl,1,2)\n    fig.append_trace(trace_vl,1,2)\n    fig['layout']['xaxis'].update(title = 'Epoch')\n    fig['layout']['xaxis2'].update(title = 'Epoch')\n    fig['layout']['yaxis'].update(title = 'Accuracy', range=[0,1])\n    fig['layout']['yaxis2'].update(title = 'Loss', range=[0,1])\n\n    \n    iplot(fig, filename='accuracy-loss')\n\nplot_accuracy_and_loss(train_model)","fc289b22":"plot_accuracy_and_loss(train_model)","38430b60":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:cyan' role=\"tab\" aria-controls=\"home\"><center>Fashion MNIST Tensorflow: Convoluton Neural Networks<\/center><\/h2>\n","505773d1":"# <a id=\"41\">Class distribution<\/a>\n\nLet's see how many number of images are in each class. We start with the train set.\n\n### Train set images class distribution","064836fd":"Let's check the class imbalance for the resulted training set.","b1328f3f":"Test accuracy is  around  0.92.\n\nWe evaluated the model accuracy based on the predicted values for the test set.  Let's check the validation value during training.\n\n","b525f41e":"And, as well, for the validation set.","4ddd6902":"Both the train and validation set are unbalanced with respect of distribution of classes. ","c4e5c3e8":"Let's now plot the images.   \nThe labels are shown above each image.","8db62686":"## <a id=\"42\">Sample images<\/a>\n\n### Train set images\n\nLet's plot some samples for the images.   \nWe add labels to the train set images, with the corresponding fashion item category.  ","9026768f":"`The classes are equaly distributed in the train set (10% each). Let's check the same for the test set.` \n\n`Let's also plot the class distribution.`\n\n","933278f7":"### Test set images\n\nLet's plot now a selection of the test set images.  \nLabels are as well added (they are known).  ","894a3c97":"### Inspect the model\n\nLet's check the model we initialized.","f796af22":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:cyan' role=\"tab\" aria-controls=\"home\"><center>If you found this notebook helpful , some upvotes would be very much appreciated - That will keep me motivated \ud83d\ude0a<\/center><\/h2>\n","e6b52a11":"## <a id=\"53\">Validation accuracy and loss<\/a>\n\nLet's plot the train and validation accuracy and loss, from the train history.","7620b5cd":"### Test  images class distribution","631bea82":"# <a id=\"4\">Data exploration<\/a>","9e51238f":"## <a id=\"53\">Test prediction accuracy<\/a>\n\nWe calculate the test loss and accuracy.","510dae22":"## <a id=\"51\">Prepare the model<\/a>\n\n## Data preprocessing\n\nFirst we will do a data preprocessing to prepare for the model.\n\nWe reshape the columns  from (784) to (28,28,1). We also save label (target) feature as a separate vector.","9db9c217":"# <a id=\"3\">Read the data<\/a>\n\nThere are 10 different classes of images, as following: \n\n* **0**: **T-shirt\/top**;   \n* **1**: **Trouser**;   \n* **2**: **Pullover**;   \n* **3**: **Dress**;\n* **4**: **Coat**;\n* **5**: **Sandal**;\n* **6**: **Shirt**;\n* **7**: **Sneaker**;\n* **8**: **Bag**;\n* **9**: **Ankle boot**.\n\nImage dimmensions are **28**x**28**.   \n\nThe train set and test set are given in two separate datasets.\n","424c25b1":"## <a id=\"52\">Train the model<\/a>\n\n### Build the model   \n\n\n\nWe will use a **Sequential** model.\n* The **Sequential** model is a linear stack of layers. It can be first initialized and then we add layers using **add** method or we can add all layers at init stage. The layers added are as follows:\n\n* **Conv2D** is a 2D Convolutional layer (i.e. spatial convolution over images). The parameters used are:\n * filters - the number of filters (Kernels) used with this layer; here filters = 32;\n * kernel_size - the dimmension of the Kernel: (3 x 3);\n * activation - is the activation function used, in this case `relu`;\n * kernel_initializer - the function used for initializing the kernel;\n * input_shape - is the shape of the image presented to the CNN: in our case is 28 x 28\n The input and output of the **Conv2D** is a 4D tensor.\n \n* **MaxPooling2D** is a Max pooling operation for spatial data. Parameters used here are:\n * *pool_size*, in this case (2,2), representing the factors by which to downscale in both directions;\n \n * **Conv2D** with the following parameters:\n * filters: 64;\n * kernel_size : (3 x 3);\n * activation : `relu`;\n \n* **MaxPooling2D** with parameter:\n * *pool_size* : (2,2);\n\n* **Conv2D** with the following parameters:\n * filters: 128;\n * kernel_size : (3 x 3);\n * activation : `relu`;\n \n* **Flatten**. This layer Flattens the input. Does not affect the batch size. It is used without parameters;\n\n* **Dense**. This layer is a regular fully-connected NN layer. It is used without parameters;\n * units - this is a positive integer, with the meaning: dimensionality of the output space; in this case is: 128;\n * activation - activation function : `relu`;\n \n* **Dense**. This is the final layer (fully connected). It is used with the parameters:\n * units: the number of classes (in our case 10);\n * activation : `softmax`; for this final layer it is used `softmax` activation (standard for multiclass classification)\n \n\nThen we compile the model, specifying as well the following parameters:\n* *loss*;\n* *optimizer*;\n* *metrics*. \n","7f826b01":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:cyan' role=\"tab\" aria-controls=\"home\"><center>Thank You \ud83d\ude0a \ud83d\ude4f<\/center><\/h2>\n","ee18114f":"The dimmension of the processed train, validation and test set are as following:","01be8676":"We process both the train_data and the test_data","7518b657":"### Run the model\n\nWe run the model with the training set. We are also using the validation set (a subset from the orginal training set) for validation.","fa88a808":"`Fashion-MNIST is a dataset of Zalando's article images\u2014consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes:`\n","5b3fb537":"## Parameters"}}