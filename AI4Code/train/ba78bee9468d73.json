{"cell_type":{"3d24f91e":"code","fb439b4f":"code","e8e21cc4":"code","fbc26bb5":"code","6ad7bd90":"code","e18c7ed9":"code","e7bebab3":"code","0a8f6963":"code","45278752":"code","5a6b6ac2":"code","1ea338ec":"code","ad205dd0":"code","ff36c41a":"code","6b644e3c":"code","e20f0e60":"code","94a441b0":"code","0a2ad0e9":"code","964c0a47":"code","b48ee4df":"code","11530e8a":"code","612021e8":"code","230d5ccb":"code","b061ab74":"code","2f0df458":"code","8b00e91d":"code","cfa662b7":"markdown","1dc3cd7c":"markdown","0e21e1ea":"markdown","a75bd81e":"markdown","c583ad52":"markdown","0b3259f6":"markdown","da442e3f":"markdown","05b9be7e":"markdown"},"source":{"3d24f91e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fb439b4f":"data = pd.read_csv(\"\/kaggle\/input\/gender-classification-dataset\/gender_classification_v7.csv\")","e8e21cc4":"data.columns","fbc26bb5":"data.info()","6ad7bd90":"data.head(10)","e18c7ed9":"data.describe()","e7bebab3":"#data.drop(\"nose_wide\",inplace = True, axis=1)\n#data.drop(\"nose_long\",inplace = True, axis = 1)\n#data.drop(\"lips_thin\",inplace = True, axis = 1)\n#data.drop(\"distance_nose_to_lip_long\",inplace = True, axis = 1)","0a8f6963":"data","45278752":"y = data.gender.values\n\nx = data[[\"long_hair\",\"forehead_width_cm\",\"forehead_height_cm\",\"nose_wide\",\"nose_long\",\"lips_thin\",\"distance_nose_to_lip_long\"]].values","5a6b6ac2":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"x_train : \",x_train)\nprint(\"x_test : \",x_test)\nprint(\"y_train : \",y_train)\nprint(\"y_test : \",y_test)","1ea338ec":"from sklearn.model_selection import train_test_split\nx_test, x_train, y_test, y_train = train_test_split(x, y, test_size = 0.15, random_state = 42)\nfrom sklearn import linear_model\n\nlogreg = linear_model.LogisticRegression(random_state = 42, max_iter = 150)\n\nprint(\"Test Accuracy : \", (logreg.fit(x_train, y_train).score(x_test, y_test)))\nprint(\"Train Accuracy : \", (logreg.fit(x_train, y_train).score(x_train, y_train)))","ad205dd0":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\nknn = KNeighborsClassifier(n_neighbors = 9)\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nf, ax = plt.subplots(figsize = (5,5))\ncm = confusion_matrix(y_test, knn.predict(x_test))\nsns.heatmap(cm,annot = True, linewidth = 0.5, linecolor = \"red\", fmt = \".0f\", ax = ax)\nplt.show()\nprint(\"Confusion Matrix : \", cm)\nprint(\" {}nn Score: {} \".format(3, knn.score(x_test,y_test)))\n\nscore_list = []\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train, y_train)\n    score_list.append(knn2.score(x_test, y_test))\n\nplt.plot(range(1,15), score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"Accuracy\")\nplt.show()","ff36c41a":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.15, random_state = 1)\n\nfrom sklearn.svm import SVC\nsvm = SVC(random_state = 1)\nsvm.fit(x_train, y_train)\nprint(\"Accuracy of SVM Algo : \", svm.score(x_test, y_test))\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, svm.predict(x_test))\n\nprint(\"Confusion Matrix : \", cm)\nimport seaborn as sns\nf, ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm, annot = True, linewidth = 0.5, linecolor = \"red\", fmt = \".0f\", ax = ax)\nplt.show()","6b644e3c":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1)\n\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train, y_train)\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, nb.predict(x_test))\n\nprint(\"Accuracy of Naive Bayes Algo : \", nb.score(x_test, y_test))\nprint(\"Confusion Matrix : \", cm)\n\nimport seaborn as sns\nf, ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm, annot = True, linewidth = 0.5, linecolor = \"red\", fmt = \".0f\", ax = ax)\nplt.show()","e20f0e60":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.15, random_state = 1)\n\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train, y_train)\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, dt.predict(x_test))\nprint(\"Score : \", dt.score(x_test, y_test))\nprint(\"Confusion Matrix : \", cm)\n\nimport seaborn as sns\nf ,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm, annot = True, linewidth = 0.5, linecolor = \"red\", fmt = \".0f\", ax = ax)\nplt.show()","94a441b0":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.15, random_state = 42)\n\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 100, random_state = 1)\nrf.fit(x_train, y_train)\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, rf.predict(x_test))\n\nprint(\"Random Forest Algo Result : \", rf.score(x_test, y_test))\nprint(\"Confusion Matrix : \", cm)\n\nimport seaborn as sns\nf, ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm, annot = True, linewidth = 0.5, linecolor = \"red\", fmt = \".0f\", ax = ax)\nplt.show()","0a2ad0e9":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = 42)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"x_train : \",x_train)\nprint(\"x_test : \",x_test)\nprint(\"y_train : \",y_train)\nprint(\"y_test : \",y_test)","964c0a47":"def initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b","b48ee4df":"#Calculation of Z\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","11530e8a":"#Forward Propagation\ndef forward_propagation(w, b, x_train, y_train):\n    z = np.dot(w.T, x_train,) + b\n    y_head = sigmoid(z)\n    loss = -y_train * np.log(y_head) - (1 - y_train) * np.log(1 - y_head)\n    cost = (np.sum(loss)) \/ x_train.shape[1] #x_train.shape[1] is for scaling(optimizing)\n    \n    return cost","612021e8":"#In Backward Propagation, We will use y_head that found in Forward Propagation. \n#For this reason, instead of writing Backward Propagation method, Let's combine Forward and Backward Propagation\n\ndef forward_backward_propagation(w, b, x_train, y_train):\n    #Forward Propagation\n    z = np.dot(w.T, x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train * np.log(y_head) - (1 - y_train) * np.log(1 - y_head)\n    cost = (np.sum(loss)) \/ x_train.shape[1] #x_train.shape[1] is for scaling(optimizing)\n    \n    #Backward Propagation\n    derivative_weight = (np.dot(x_train, ((y_head - y_train).T))) \/ x_train.shape[1] # x_train.shape[1] another scaling\n    derivative_bias = np.sum(y_head - y_train) \/ x_train.shape[1] # x_train.shape[1] another scaling\n    \n    gradients = {\"derivative_weight\" : derivative_weight, \"derivative_bias\" : derivative_bias}\n    \n    return cost, gradients","230d5ccb":"#In Backward Propagation, We will use y_head that found in Forward Propagation. \n#For this reason, instead of writing Backward Propagation method, Let's combine Forward and Backward Propagation\n\ndef forward_backward_propagation(w, b, x_train, y_train):\n    #Forward Propagation\n    z = np.dot(w.T, x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train * np.log(y_head) - (1 - y_train) * np.log(1 - y_head)\n    cost = (np.sum(loss)) \/ x_train.shape[1] #x_train.shape[1] is for scaling(optimizing)\n    \n    #Backward Propagation\n    derivative_weight = (np.dot(x_train, ((y_head - y_train).T))) \/ x_train.shape[1] # x_train.shape[1] another scaling\n    derivative_bias = np.sum(y_head - y_train) \/ x_train.shape[1] # x_train.shape[1] another scaling\n    \n    gradients = {\"derivative_weight\" : derivative_weight, \"derivative_bias\" : derivative_bias}\n    \n    return cost, gradients","b061ab74":"#Updating (learning) Parameters\n\ndef update(w, b, x_train, y_train, learning_rate, number_of_iteration):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    #Updating (learning) parameters is number_of_iteration times\n    for i in range(number_of_iteration):\n        \n        #Make forward and backward propagation and find cost and gradients\n        cost, gradients = forward_backward_propagation(w, b, x_train, y_train)\n        cost_list.append(cost)\n        \n        #Let's Update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        \n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n        \n        #We update(learn) parameters weights and bias\n        parameters = {\"weight\": w,\"bias\": b}\n        plt.plot(index,cost_list2)\n        plt.xticks(index,rotation='vertical')\n        plt.xlabel(\"Number of Iterarion\")\n        plt.ylabel(\"Cost\")\n        plt.show()\n        \n        return parameters, gradients, cost_list","2f0df458":"#Prediction\ndef predict(w, b, x_test):\n    \n    #x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T, x_test) + b)\n    Y_prediction = np.zeros((1, x_test.shape[1]))\n    \n    # if z is bigger than 0.5, our prediction is Male (y_head = 1)\n    # if z is lower tan 0.5, our prediction is Female (y_head = 0)\n    for i in range(z.shape[1]):\n        if z[0,i] <= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n    return Y_prediction","8b00e91d":"#We have made our prediction\n#Now, we're putting them together\n\ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate, num_iteration):\n    #Initialize\n    dimension = x_train.shape[0] #Which 5001\n    w, b = initialize_weights_and_bias(dimension)\n    \n    #Do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate, num_iteration)\n    \n    y_prediction_test = predict(parameters[\"weight\"], parameters[\"bias\"], x_test)\n    y_prediction_train = predict(parameters[\"weight\"], parameters[\"bias\"], x_train)\n    \n    #Print Train\/Test Errors\n    print(\"Train Accuracy : \", (100 - np.mean(np.abs(y_prediction_train - y_train) * 100)))\n    print(\"Test Accuracy : \", (100 - np.mean(np.abs(y_prediction_test - y_test) * 100)))\n    \nlogistic_regression(x_train, y_train, x_test, y_test, learning_rate = 30, num_iteration = 150)","cfa662b7":"### Logistic Regression","1dc3cd7c":"### Random Forest Classification","0e21e1ea":"### Decision Tree Learning","a75bd81e":"### SVM (Support Vector Machine)","c583ad52":"### KNN Classification","0b3259f6":"### Without Sklearn Library","da442e3f":"### Train - Test Split","05b9be7e":"### Naive Bayes"}}