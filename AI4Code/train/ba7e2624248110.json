{"cell_type":{"5c0a0126":"code","82546545":"code","cae69e36":"code","408f1997":"code","bd50506b":"code","7d1f11fd":"code","74ecac2b":"code","f241c001":"code","24938902":"code","de756d93":"code","fbf40095":"code","7741add0":"code","74fc0404":"code","1ad3f810":"code","e2e4f9ad":"code","4103cfb7":"code","7d74c4f6":"code","028e1cb8":"code","144f0d3f":"code","e91d2a90":"code","106a9b11":"code","c972ba96":"code","ba0106d0":"code","b3a2723d":"code","58bae83c":"code","7bc1bab9":"code","833c79c1":"code","7dcb0da0":"code","0cebbb08":"code","cd33e1b0":"code","d688c05c":"code","1c708273":"code","c974cd64":"code","dc469f96":"code","862106f1":"code","ad9e0ada":"code","aec6ea47":"code","f50b2b21":"code","4f385fb5":"code","39e6615a":"code","760ed187":"code","ececfbd4":"code","8e3d51cf":"code","3dda7b21":"code","a95b274b":"code","ea2e4f7d":"code","df0f90cd":"code","600cb8fc":"code","d2912dc3":"code","6a07dd3f":"code","a3473531":"code","43da2f26":"code","8ccea15c":"code","77619690":"code","455e6680":"code","599c2d2f":"code","fd09b086":"code","6da04f59":"code","04c48101":"markdown","6344af81":"markdown","7bbf7058":"markdown","81fe64d8":"markdown","7188baaf":"markdown","61d318af":"markdown","08b64973":"markdown","5c98c961":"markdown","f09b9eed":"markdown","d77a01f2":"markdown","638a24f4":"markdown","5c96a1c4":"markdown","b144a212":"markdown","912d59fa":"markdown","770fcef2":"markdown","780d6f9b":"markdown","5407f142":"markdown","c0a854df":"markdown","e643d4a7":"markdown","d597bc48":"markdown","e38f2deb":"markdown","49c005e7":"markdown","7fd56169":"markdown","fb9273d1":"markdown","f5273f03":"markdown","86fbb60e":"markdown","44556785":"markdown","ac726cbe":"markdown","567b860e":"markdown","a9eea6f0":"markdown","38f103f1":"markdown","177a5699":"markdown","e7fd970d":"markdown","e16f4956":"markdown","f5dd3d1d":"markdown","20550adc":"markdown","0e4abbf4":"markdown","00e20aff":"markdown","abfeb20a":"markdown","6955a0e0":"markdown","3f26f0e6":"markdown","08861a6a":"markdown","e11fe39f":"markdown","bb7dcee0":"markdown"},"source":{"5c0a0126":"import pandas as pd \nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\n\n\ndata = pd.read_csv(\"..\/input\/cs178-wine-quality\/winequality-white.csv\", sep=\";\") \ndata.head()","82546545":"import plotly.express as px\nfig = px.pie(data, values='quality', names='quality')\nfig.show()","cae69e36":"# Guardamos la label como \"etiquetas originales\" si no lo est\u00e1 ya\nif 'quality' in data.columns:\n    data['original_labels'] = data['quality']\n    del data['quality']","408f1997":"data.info()","bd50506b":"data.describe().T","7d1f11fd":"sns.kdeplot(data['fixed acidity'], shade=True);","74ecac2b":"data['fixed acidity'] = np.log1p(data['fixed acidity'])\ndata[['fixed acidity']] = MinMaxScaler().fit_transform(data[['fixed acidity']])\nsns.distplot(data['fixed acidity'], fit = stats.norm)\nprint(data['fixed acidity'].skew(), data['fixed acidity'].kurt())","f241c001":"sns.kdeplot(data['volatile acidity'], shade=True);","24938902":"data['volatile acidity'] = np.log1p(data['volatile acidity'])\ndata[['volatile acidity']] = MinMaxScaler().fit_transform(data[['volatile acidity']])\nsns.distplot(data['volatile acidity'], fit = stats.norm)\nprint(data['volatile acidity'].skew(), data['volatile acidity'].kurt())","de756d93":"sns.kdeplot(data['citric acid'], shade=True);","fbf40095":"data['citric acid'] = np.log1p(data['citric acid'])\ndata[['citric acid']] = MinMaxScaler().fit_transform(data[['citric acid']])\nsns.distplot(data['citric acid'], fit = stats.norm)\nprint(data['citric acid'].skew(), data['citric acid'].kurt())","7741add0":"sns.kdeplot(data['residual sugar'], shade=True);","74fc0404":"data['residual sugar'] = np.log1p(data['residual sugar'])\ndata[['residual sugar']] = MinMaxScaler().fit_transform(data[['residual sugar']])\nsns.distplot(data['residual sugar'], fit = stats.norm)\nprint(data['residual sugar'].skew(), data['residual sugar'].kurt())","1ad3f810":"sns.kdeplot(data['chlorides'], shade=True);","e2e4f9ad":"data['chlorides'] = np.log1p(data['chlorides'])\ndata[['chlorides']] = MinMaxScaler().fit_transform(data[['chlorides']])\nsns.distplot(data['chlorides'], fit = stats.norm)\nprint(data['chlorides'].skew(), data['chlorides'].kurt())","4103cfb7":"sns.kdeplot(data['free sulfur dioxide'], shade=True);","7d74c4f6":"data['free sulfur dioxide'] = np.log1p(data['free sulfur dioxide'])\ndata[['free sulfur dioxide']] = MinMaxScaler().fit_transform(data[['free sulfur dioxide']])\nsns.distplot(data['free sulfur dioxide'], fit = stats.norm)\nprint(data['free sulfur dioxide'].skew(), data['free sulfur dioxide'].kurt())","028e1cb8":"sns.kdeplot(data['total sulfur dioxide'], shade=True);","144f0d3f":"data['total sulfur dioxide'] = np.log1p(data['total sulfur dioxide'])\ndata[['total sulfur dioxide']] = MinMaxScaler().fit_transform(data[['total sulfur dioxide']])\nsns.distplot(data['total sulfur dioxide'], fit = stats.norm)\nprint(data['total sulfur dioxide'].skew(), data['total sulfur dioxide'].kurt())","e91d2a90":"sns.kdeplot(data['density'], shade=True);","106a9b11":"data['density'] = np.log1p(data['density'])\ndata[['density']] = MinMaxScaler().fit_transform(data[['density']])\nsns.distplot(data['density'], fit = stats.norm)\nprint(data['density'].skew(), data['density'].kurt())","c972ba96":"sns.kdeplot(data['pH'], shade=True);","ba0106d0":"data['pH'] = np.log1p(data['pH'])\ndata[['pH']] = MinMaxScaler().fit_transform(data[['pH']])\nsns.distplot(data['pH'], fit = stats.norm)\nprint(data['pH'].skew(), data['pH'].kurt())","b3a2723d":"sns.kdeplot(data['sulphates'], shade=True);","58bae83c":"data['sulphates'] = np.log1p(data['sulphates'])\ndata[['sulphates']] = MinMaxScaler().fit_transform(data[['sulphates']])\nsns.distplot(data['sulphates'], fit = stats.norm)\nprint(data['sulphates'].skew(), data['sulphates'].kurt())","7bc1bab9":"sns.kdeplot(data['alcohol'], shade=True);","833c79c1":"data['alcohol'] = np.log1p(data['alcohol'])\ndata[['alcohol']] = MinMaxScaler().fit_transform(data[['alcohol']])\nsns.distplot(data['alcohol'], fit = stats.norm)\nprint(data['alcohol'].skew(), data['alcohol'].kurt())","7dcb0da0":"import seaborn as sns; \nsns.set(style=\"ticks\", color_codes=True)\nsns.pairplot(data, kind=\"reg\")","0cebbb08":"corrmat = data.corr(method='spearman')\nsns.clustermap(corrmat, cmap=\"YlGnBu\", linewidths=0.1);","cd33e1b0":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import pairwise_distances_argmin_min\n\nnp.random.seed(42)\ndata_k_means = data.copy()","d688c05c":"%matplotlib inline\nfrom mpl_toolkits.mplot3d import Axes3D\nplt.rcParams['figure.figsize'] = (16, 9)\nplt.style.use('ggplot')","1c708273":"Nc = range(1, 20)\nkmeans = [KMeans(n_clusters=i) for i in Nc]\nscore = [kmeans[i].fit(data_k_means).score(data_k_means) for i in range(len(kmeans))]\nplt.plot(Nc,score)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Score')\nplt.title('Elbow Curve')\nplt.show()","c974cd64":"kmeans = KMeans(n_clusters=5).fit(data_k_means)\ncentroids = kmeans.cluster_centers_\nprint(centroids)","dc469f96":"data_k_means['pred_cluster_labels'] = (kmeans.labels_ + 1).tolist()\ndata_k_means.head(3)","862106f1":"fig = px.pie(data_k_means, values='pred_cluster_labels', names='pred_cluster_labels')\nfig.show()","ad9e0ada":"from sklearn import metrics\nprint(f\"Homogeneity score: {metrics.homogeneity_score(data_k_means['original_labels'], data_k_means['pred_cluster_labels'])}\")\nprint(f\"Completeness score: {metrics.completeness_score(data_k_means['original_labels'], data_k_means['pred_cluster_labels'])}\")\nprint(f\"Adjusted rand score: {metrics.adjusted_rand_score(data_k_means['original_labels'], data_k_means['pred_cluster_labels'])}\")","aec6ea47":"print(f\"Silhouette score: {metrics.silhouette_score(data_k_means, data_k_means['pred_cluster_labels'] ,metric='euclidean', sample_size=4898)}\")","f50b2b21":"def select_k (data, k_range):\n    data_ = data.copy()\n    scores = dict()\n    for k in k_range:    \n        kmeans = KMeans(n_clusters=k).fit(data_)\n        labels = (kmeans.labels_ + 1).tolist()\n        data_[f'pred_{k}_cluster_labels'] = (kmeans.labels_ + 1).tolist()\n        residual_error = 0\n        print(f'# k = {k} =>')\n        homogeneity_residual_error = (1 - metrics.homogeneity_score(data_['original_labels'], labels))\n        print(f\"- Homogeneity score: {homogeneity_residual_error}\")\n        completeness_residual_error = (1 - metrics.completeness_score(data_['original_labels'], labels))\n        print(f\"- Completeness score: {completeness_residual_error}\")\n        adjusted_rand_residual_error = (1 - metrics.adjusted_rand_score(data_['original_labels'], labels))\n        print(f\"- Adjusted rand score: {adjusted_rand_residual_error}\")\n        silhouette_residual_error = (1 - metrics.silhouette_score(data, labels, metric='euclidean', sample_size=data.shape[0]))\n        print(f\"- Silhouette score: {silhouette_residual_error}\")\n        scores[f'k_{k}'] = silhouette_residual_error + adjusted_rand_residual_error + completeness_residual_error + homogeneity_residual_error\n        print(f'Total error = {scores[f\"k_{k}\"]}')\n    lists = sorted(scores.items()) \n    x, y = zip(*lists) \n    plt.plot(x, y)\n    plt.show()\n    return scores, data_\n\nscores_k_means, data_k_means = select_k(data_k_means, range(2,10))\n# Quitamos el que ya hab\u00eda de k = 5\nif 'pred_cluster_labels' in data.columns:\n    del data['pred_cluster_labels']\ndata_k_means.head()","4f385fb5":"d = pd.concat([data_k_means['fixed acidity'], data_k_means['pred_5_cluster_labels']], axis=1)\nf, ax = plt.subplots()\nfig = sns.boxplot(x='pred_5_cluster_labels', y=\"fixed acidity\", data=d)\nplt.xticks(rotation=90);","39e6615a":"d = pd.concat([data_k_means['citric acid'], data_k_means['pred_5_cluster_labels']], axis=1)\nf, ax = plt.subplots()\nfig = sns.boxplot(x='pred_5_cluster_labels', y=\"citric acid\", data=d)\nplt.xticks(rotation=90);","760ed187":"d = pd.concat([data_k_means['citric acid'], data_k_means['pred_5_cluster_labels']], axis=1)\nf, ax = plt.subplots()\nfig = sns.boxplot(x='pred_5_cluster_labels', y=\"citric acid\", data=d)\nplt.xticks(rotation=90);","ececfbd4":"d = pd.concat([data_k_means['chlorides'], data_k_means['pred_5_cluster_labels']], axis=1)\nf, ax = plt.subplots()\nfig = sns.boxplot(x='pred_5_cluster_labels', y=\"chlorides\", data=d)\nplt.xticks(rotation=90);","8e3d51cf":"d = pd.concat([data_k_means['free sulfur dioxide'], data_k_means['pred_5_cluster_labels']], axis=1)\nf, ax = plt.subplots()\nfig = sns.boxplot(x='pred_5_cluster_labels', y=\"free sulfur dioxide\", data=d)\nplt.xticks(rotation=90);","3dda7b21":"d = pd.concat([data_k_means['total sulfur dioxide'], data_k_means['pred_5_cluster_labels']], axis=1)\nf, ax = plt.subplots()\nfig = sns.boxplot(x='pred_5_cluster_labels', y=\"total sulfur dioxide\", data=d)\nplt.xticks(rotation=90);","a95b274b":"d = pd.concat([data_k_means['density'], data_k_means['pred_5_cluster_labels']], axis=1)\nf, ax = plt.subplots()\nfig = sns.boxplot(x='pred_5_cluster_labels', y=\"density\", data=d)\nplt.xticks(rotation=90);","ea2e4f7d":"d = pd.concat([data_k_means['pH'], data_k_means['pred_5_cluster_labels']], axis=1)\nf, ax = plt.subplots()\nfig = sns.boxplot(x='pred_5_cluster_labels', y=\"pH\", data=d)\nplt.xticks(rotation=90);","df0f90cd":"d = pd.concat([data_k_means['sulphates'], data_k_means['pred_5_cluster_labels']], axis=1)\nf, ax = plt.subplots()\nfig = sns.boxplot(x='pred_5_cluster_labels', y=\"sulphates\", data=d)\nplt.xticks(rotation=90);","600cb8fc":"d = pd.concat([data_k_means['alcohol'], data_k_means['pred_5_cluster_labels']], axis=1)\nf, ax = plt.subplots()\nfig = sns.boxplot(x='pred_5_cluster_labels', y=\"alcohol\", data=d)\nplt.xticks(rotation=90);","d2912dc3":"from scipy.cluster.hierarchy import dendrogram, linkage\ndata_aggl_hier = data.copy()\n\n# Creamos la matriz de linkage, pas\u00e1ndo como par\u00e1metros los datos y el criterio para escoger la distancia m\u00ednima\nH_cluster = linkage(data_aggl_hier,'ward') # ward es el caso c) Distancia entre los centroides de dos grupos.\n\n# Preparamos la representaci\u00f3n gr\u00e1fica\nplt.title('Hierarchical Clustering Dendrogram (truncated)')\nplt.xlabel('Cl\u00fasteres')\nplt.ylabel('Distancia')\n\n# Representamos la algomeraci\u00f3n en forma de digrama en \u00e1rbol\ndendrogram(\n    H_cluster, # jerarqu\u00eda de distancias m\u00ednimas y cl\u00fasteres calculado\n    leaf_rotation=90., # Dise\u00f1o representaci\u00f3n\n    leaf_font_size=12., # Dise\u00f1o represetanci\u00f3n\n    show_contracted=True, \n    orientation='right'\n)\nplt.show()","6a07dd3f":"from sklearn.cluster import AgglomerativeClustering\n\ncluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')\nlabels = cluster.fit_predict(data_aggl_hier)","a3473531":"data_aggl_hier['pred_cluster_labels'] = (labels + 1).tolist()\ndata_aggl_hier.head(3)","43da2f26":"fig = px.pie(data_aggl_hier, values='pred_cluster_labels', names='pred_cluster_labels')\nfig.show()","8ccea15c":"def select_n (data, n_range):\n    data_ = data.copy()\n    scores = dict()\n    for n in n_range:    \n        cluster = AgglomerativeClustering(n_clusters=n, affinity='euclidean', linkage='ward')\n        labels = cluster.fit_predict(data_aggl_hier)\n        data_[f'pred_{n}_cluster_labels'] = (labels + 1).tolist()\n        residual_error = 0\n        print(f'# n = {n} =>')\n        homogeneity_residual_error = (1 - metrics.homogeneity_score(data_['original_labels'], labels))\n        print(f\"- Homogeneity score: {homogeneity_residual_error}\")\n        completeness_residual_error = (1 - metrics.completeness_score(data_['original_labels'], labels))\n        print(f\"- Completeness score: {completeness_residual_error}\")\n        adjusted_rand_residual_error = (1 - metrics.adjusted_rand_score(data_['original_labels'], labels))\n        print(f\"- Adjusted rand score: {adjusted_rand_residual_error}\")\n        silhouette_residual_error = (1 - metrics.silhouette_score(data, labels, metric='euclidean', sample_size=data.shape[0]))\n        print(f\"- Silhouette score: {silhouette_residual_error}\")\n        scores[f'n_{n}'] = silhouette_residual_error + adjusted_rand_residual_error + completeness_residual_error + homogeneity_residual_error\n        print(f'Total error = {scores[f\"n_{n}\"]}')\n    lists = sorted(scores.items()) \n    x, y = zip(*lists) \n    plt.plot(x, y)\n    plt.show()\n    return scores, data_\n\nscores_aggl_hier, data_aggl_hier = select_n(data_aggl_hier, range(2,15))\n# Quitamos el que ya hab\u00eda de n = 5\nif 'pred_cluster_labels' in data.columns:\n    del data['pred_cluster_labels']\ndata_aggl_hier.head()","77619690":"algorithms = ['K-Means', 'Agglomeration']\nscores = [scores_k_means['k_5'], scores_aggl_hier['n_5']]\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.bar(algorithms, scores)\nax.set_ylabel('Error en la agrupaci\u00f3n')\nplt.show()","455e6680":"from sklearn.cluster import MeanShift, estimate_bandwidth\n\ndata_mean_shif = data.copy()\n\n# The following bandwidth can be automatically detected using\nbandwidth = estimate_bandwidth(data_mean_shif, quantile=0.1)\nmean_shif = MeanShift(bandwidth).fit(data_mean_shif)\n\ndata_mean_shif['pred_cluster_labels'] = (mean_shif.labels_ + 1).tolist()\ndata_mean_shif.head()","599c2d2f":"fig = px.pie(data_mean_shif, values='pred_cluster_labels', names='pred_cluster_labels')\nfig.show()","fd09b086":"def select_q (data, q_range):\n    data_ = data.copy()\n    scores = dict()\n    for q in q_range:    \n        bandwidth = estimate_bandwidth(data_mean_shif, quantile=q)\n        mean_shif = MeanShift(bandwidth).fit(data_mean_shif)\n        labels = (mean_shif.labels_ + 1).tolist()\n        data_['pred_cluster_labels'] = labels\n        residual_error = 0\n        print(f'# q = {q} =>')\n        homogeneity_residual_error = (1 - metrics.homogeneity_score(data_['original_labels'], labels))\n        print(f\"- Homogeneity score: {homogeneity_residual_error}\")\n        completeness_residual_error = (1 - metrics.completeness_score(data_['original_labels'], labels))\n        print(f\"- Completeness score: {completeness_residual_error}\")\n        adjusted_rand_residual_error = (1 - metrics.adjusted_rand_score(data_['original_labels'], labels))\n        print(f\"- Adjusted rand score: {adjusted_rand_residual_error}\")\n        silhouette_residual_error = (1 - metrics.silhouette_score(data, labels, metric='euclidean', sample_size=data.shape[0]))\n        print(f\"- Silhouette score: {silhouette_residual_error}\")\n        scores[f'q_{q}'] = silhouette_residual_error + adjusted_rand_residual_error + completeness_residual_error + homogeneity_residual_error\n        print(f'Total error = {scores[f\"q_{q}\"]}')\n    lists = sorted(scores.items()) \n    x, y = zip(*lists) \n    plt.plot(x, y)\n    plt.show()\n    return scores, data_\n\nscores_mean_shif, data_mean_shif = select_q(data_mean_shif, [0.1,0.2,0.3,0.4])\n# Quitamos el que ya hab\u00eda de q = 0.1\nif 'pred_cluster_labels' in data.columns:\n    del data['pred_cluster_labels']\ndata_mean_shif.head()","6da04f59":"algorithms = ['K-Means', 'Agglomeration', 'Mean Shift']\nscores = [scores_k_means['k_5'], scores_aggl_hier['n_5'], scores_mean_shif['q_0.2']]\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.bar(algorithms, scores)\nax.set_ylabel('Error en la agrupaci\u00f3n')\nplt.show()","04c48101":"* _Density_","6344af81":"### Mean shift","7bbf7058":"#### Multivariate visual analysis","81fe64d8":"Next, we extract the final k centroids of each inferred group. Each centroid is a vector with as many coordinates as there are attributes in the dataset. Logically, we cannot visualize more than 3 dimensions, only the coordinates in this case.","7188baaf":"All values are real numeric. No categorical variables (no discretization needed). No missing values. However, we see considerable gaps between the magnitudes of some attributes, let's standardized them.","61d318af":"####\u00a0Univariate visual analysis","08b64973":"* _Chlorides_ ","5c98c961":"* _Volatile acidicity_","f09b9eed":"* _Citric acid_ ","d77a01f2":"Now, let's extract the labels (for each K group) assigned to the observations and insert them in the original dataframe (_added 1 because plotly issues with 0 value_).","638a24f4":"* Distributions of the attributes have been normalized to mean 0 and standard deviation 1: (xi - xmin) \/ (xmax - xmin) to homogenize the their weights.\n* Variance has been smoothed by logarithmic function.","5c96a1c4":"###  Clustering algorithms:","b144a212":"* _Residual sugar_ ","912d59fa":"A first approach to know the most appropriate range of K values is the __Elbow method__: selects the range around the \"elbow\" of the graph. In this case the curvature is very smooth but we could deduce that the optimal values are around 5.","770fcef2":"#### Bivariate visual analysis","780d6f9b":"#### \u00bfHow do we measure these descriptive results?","5407f142":"Great! It is clearly seen how the total error of the metrics falls to k = 5, as the original labels pie graph.","c0a854df":"In addition, we see that there are only 5 significant groups, of which 2 are considerably smaller, therefore, we could approximate that there are 4 or 5 classes.","e643d4a7":"The vertical height represents the distance between the clusters. The longest height (the longest distance between clusters) will be the threshold at which all lower branches and leaves are cut, leaving us with only the top of the hierarchy. We see that this threshold is either on the distance = 15 with 3 groups, or on the distance = 25 with 5 groups. We are going to test with 5 and then we check the rest of n.\n\nNow that we know how many groups have to be divided to have the optimal grouping, we proceed with the agglomeration algorithm.","d597bc48":"* _Alcohol_","e38f2deb":"Before removing the label, let's see how many classes there would be and in what proportion, in order to know the optimal value of the k of the K-means. The classes are a score from 1 to 10, therefore there are 10 classes, however it seems that there are no observations in classes less than 3 nor greater than 9, therefore there are only 7 groups with similar characteristics.\n\n","49c005e7":"##### Evaluation without using the ground truth, instead by the intra-cluster cohesion (of a cluster with itself) or the inter-cluster separation (of a cluster with the rest of the clusters).","7fd56169":"* pH","fb9273d1":"#### Closer look to infered groups characteristics","f5273f03":"We can ratify these relationships in the scatter graphs above; when the color is more light yellow (inversely proportional relationship) the graph is linear with a negative slope and if the color is darker blue, vice versa. The most horizontal relationships (greenish colors) are the ones that have scored the least correlation and the ones that will give more information to the model.  ","86fbb60e":"# DESCRIPTIVE MODELS: \n## K-means, Hierarchical Agglomeration and Mean shift.\n\n[White Wine Dataset](https:\/\/www.kaggle.com\/ashishpatel26\/association-rule-mining-for-lastfm-using-python)\n* __Multivariate dataset__ (more than two attributes)\n* __Number of observations__: 4898\n* __Area__: Chemical composition of wine\n* __Attribute typology__: Real numbers\n* __Number of Attributes__: 11, all continuous\n\n__The label \"quality\" will be used for some evaluation purposes of the descriptive models__","44556785":"* _Sulphates_","ac726cbe":"* _Fixed acidicity_","567b860e":"#### Bibliography\n\n1. https:\/\/archive.ics.uci.edu\/ml\/datasets\/wine\n2. https:\/\/www.kaggle.com\/xvivancos\/tutorial-clustering-wines-with-k-means\nhttps:\/\/www.aprendemachinelearning.com\/k-means-en-python-paso-a-paso\/\n3. https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py\n4. https:\/\/jarroba.com\/k-means-python-scikit-learn-ejemplos\/\n5. https:\/\/towardsdatascience.com\/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a\n6. https:\/\/www.oreilly.com\/library\/view\/mastering-machine-learning\/9781788621113\/f4a7b48b-0021-4917-ad0d-e89cef3b4df6.xhtml\n7. https:\/\/learning.oreilly.com\/library\/view\/mastering-machine-learning\/9781788621113\/f11524df-733c-4062-a5bd-125e338c6760.xhtml\n8. https:\/\/stackabuse.com\/hierarchical-clustering-with-python-and-scikit-learn\/\n9. https:\/\/medium.com\/datadriveninvestor\/unsupervised-learning-with-python-k-means-and-hierarchical-clustering-f36ceeec919c\n10. https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_mean_shift.html#sphx-glr-auto-examples-cluster-plot-mean-shift-py\n11. https:\/\/www.kaggle.com\/fazilbtopal\/popular-unsupervised-clustering-algorithms","a9eea6f0":"The distribution is again quite similar to the previous two cases, however this algorithm seems to differentiate many more clusters.","38f103f1":"* __Silhouette score__: it is limited between -1 and 1. A value close to -1 indicates that the minimum inter-cluster distance is much less than the average intra-cluster distance, so the average distance within the group is greater than the average index of the closest group and sample xi is misallocated. Vice versa, a value close to 1 indicates that the algorithm achieved a very good level of internal cohesion and separation between groups (because the minimum inter-cluster distance is much higher than the average intra-cluster distance). Contrary to the previous measurements, the Silhouette score is not a cumulative function and must be calculated for each sample.","177a5699":"Dataset structure:","e7fd970d":"Let's take a look at the distributions of the attributes. ","e16f4956":"_Consulted sources at bottom *_","f5dd3d1d":"Let's look at the correlation between attributes. Most of them have a non-linear proportion; the model feeds best when there is greater variability in the data.","20550adc":"Performance is good for quantiles 0.1 and 0.2, from 0.4 the inclusive error is triggered.\n\n","0e4abbf4":"* _Free sulfur dioxide_ ","00e20aff":"##### Evaluation using the ground truth (supervised labeling relegated to the \"original data\" column at the beginning).\n\n* __Homogeneity score__: A cluster or group should contain only samples that belong to a single class. Bounded between 0 and 1. Being more homogeneous (better) the closer to 1 is.\n* __Completeness score__: The algorithm should assign the same cluster to the same observations that shared it in the actual labeling. Very similar to the previous one, only now taking into account the class to which those observations belong.\n* __Adjusted Rand Score__: The Rand measure is limited between -1 and 1. A value close to -1 indicates a prevalence of assignments to an incorrect cluster, while a value close to 1 indicates that the grouping algorithm is correctly reproducing the distribution of ground truth distribution.\n\n_NOTE: it is not necessary that the name of the labels be the same for \"original_labels\" and \"pred_cluster_labels\", it is done by means of uncertainties._","abfeb20a":"* _Total sulfur dioxide_ ","6955a0e0":"### Hierarchical clustering:\n\n1. We have as many groups as there are observations, we go to the most atomic level: 4897\n\n2. We selected the two observations (groups) with more similar characteristics (minimum Euclidean distance), generating k - 1 groups.\n\n3. We repeat step 2 until we have formed a single large cluster.\n\nNote: The link or linkage between clusters can follow different criteria:\n\na) Distance between the closing points of two groups.\n\nb) Distance between the farthest points of two groups.\n\nc) Distance between the centroids of two groups.\n\nd) Distance between all possible combinations of points between the two groups and take the mean.","3f26f0e6":"All the scores are close to 1, so they can be considered quite good. Let's try other values \u200b\u200bof k. Let's now verify the scores within a K values range araound the elbow curve.","08861a6a":"Indeed, in both 3 and 5 there is a local minimum and absolute minimum respectively.\n\n####\u00a0Let's compare the results of the kmeans and the other two tried methods\n\nThe one that has a minimum total error (its metrics are closer to 1) will be the algorithm that has the best performance:","e11fe39f":"### K-means","bb7dcee0":"The grouping is very similar to the one made at the beginning with the real labels, where the predominant classes were 3, 4, 5, 6 and 7. The only difference is that in this case groups 3 and 4, which correspond to 1 and 2, have a slight increase in samples."}}