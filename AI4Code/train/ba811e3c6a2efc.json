{"cell_type":{"d8ad9fbb":"code","6219b49d":"code","02443861":"code","8a22b551":"code","ff920b1a":"code","3c959d18":"code","857dc5b8":"code","11cb7f26":"code","20e3e73e":"code","d240f972":"code","38db27ac":"code","96bf3559":"code","282cd3ad":"code","d842b69a":"code","5b6b3ddb":"code","81f672aa":"code","883bf340":"code","7185f52f":"code","8f42cbc2":"code","ddc20e7a":"code","09a79177":"code","177b3e58":"code","99643e49":"code","9b38c46b":"code","2a807d5f":"code","a08758da":"code","64628623":"code","4854120f":"code","f0eb3d56":"code","8b9b4bee":"code","8b7cbf11":"markdown","7bb8a5ea":"markdown","64578540":"markdown","828210af":"markdown","366f7089":"markdown","f686951d":"markdown","a6beae64":"markdown","68c0f21c":"markdown","429f38be":"markdown","ed037de3":"markdown","1b41b7f7":"markdown","86afc42b":"markdown","3ef6de80":"markdown","556db255":"markdown","ecbea808":"markdown","80bd0afc":"markdown","883584dc":"markdown","973a78be":"markdown","c02c05c7":"markdown","dad26fab":"markdown","228f7171":"markdown","bc96c158":"markdown","11fa5a1c":"markdown"},"source":{"d8ad9fbb":"import os\nimport folium\nimport warnings\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom folium.plugins import HeatMap\n\nfolds  = 5\n\nsns.set(color_codes=True)\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nfrom sklearn.metrics import r2_score\nfrom ml_metrics import rmse\nfrom scipy import stats\nfrom sklearn.model_selection import GridSearchCV\nscore_calc = 'neg_mean_squared_error'","6219b49d":"house_price_data = pd.read_csv(\"\/kaggle\/input\/housesalesprediction\/kc_house_data.csv\")\nhouse_price_data = house_price_data.drop([\"date\",\"id\"], axis = 1)","02443861":"house_price_data.head()","8a22b551":"# Adding new features\nhouse_price_data[\"Home_Age\"] = 2020 - house_price_data[\"yr_built\"]\nhouse_price_data['is_renovated'] = house_price_data[\"yr_renovated\"].where(house_price_data[\"yr_renovated\"] == 0, 1)\nhouse_price_data['Total_Area'] = house_price_data['sqft_living'] + house_price_data['sqft_lot'] + house_price_data['sqft_above'] + house_price_data['sqft_basement'] \nhouse_price_data['Basement'] = house_price_data['sqft_basement'].where(house_price_data[\"sqft_basement\"] == 0, 1)","ff920b1a":"# House Age Distribution \nplt.figure(figsize=(15,5))\nsns.distplot(house_price_data[\"Home_Age\"], color=\"orange\")\nplt.title(\"House Age Distribution\")\nplt.ylabel(\"Density\")\nplt.xlabel(\"House Age\");","3c959d18":"# Price Distribution\nplt.figure(figsize=(15,5))\nsns.distplot(house_price_data[\"price\"], color=\"orange\")\nplt.title(\"Price Distribution\")\nplt.ylabel(\"Density\")\nplt.xlabel(\"Price\");\nsns.set(color_codes=True)","857dc5b8":"# Exploring correlation between features\nplt.figure(figsize=(10,8))\ncor = house_price_data.corr()\nsns.heatmap(cor, annot=False, cmap=plt.cm.Reds)\nplt.show()","11cb7f26":"# Distribution of all parameters wrt price.\nnr_rows = 3\nnr_cols = 6\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*4,nr_rows*4))\n\nnumerical_feats = house_price_data.dtypes[house_price_data.dtypes != \"object\"].index\n\nli_num_feats = list(numerical_feats)\nli_not_plot = ['id']\nli_plot_num_feats = [c for c in list(numerical_feats) if c not in li_not_plot]\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(li_plot_num_feats):\n            sns.regplot(house_price_data[li_plot_num_feats[i]], house_price_data['price'], ax = axs[r][c])\n            stp = stats.pearsonr(house_price_data[li_plot_num_feats[i]], house_price_data['price'])\n            str_title = \"r = \" + \"{0:.2f}\".format(stp[0]) + \"      \" \"p = \" + \"{0:.2f}\".format(stp[1])\n            axs[r][c].set_title(str_title,fontsize=11)\n            \nplt.tight_layout()    \nsns.set(color_codes=True)\nplt.show()   ","20e3e73e":"# Features having coorelation to price\nsns.set(color_codes=True)\nhouse_price_data.corrwith(house_price_data.price).plot.bar( figsize = (12, 5), title = \"Correlation with respect to Prices\", fontsize = 15, rot = 90, grid = True);","d240f972":"# Visualizing the surrounding \nmaxpr=house_price_data.loc[house_price_data['price'].idxmax()]\n\ndef generateBaseMap(default_location=[47.5112, -122.257], default_zoom_start=9.4):\n    base_map = folium.Map(location=default_location, control_scale=True, zoom_start=default_zoom_start)\n    return base_map\n\nhouse_price_data_copy = house_price_data.copy()\nhouse_price_data_copy['count'] = 1\nbasemap = generateBaseMap()\nfolium.TileLayer('cartodbpositron').add_to(basemap)\ns=folium.FeatureGroup(name='icon').add_to(basemap)\nfolium.Marker([maxpr['lat'], maxpr['long']],popup='Highest Price: $'+str(format(maxpr['price'],'.0f')),\n              icon=folium.Icon(color='green')).add_to(s)\n# add heatmap\nHeatMap(data=house_price_data_copy[['lat','long','count']].groupby(['lat','long']).sum().reset_index().values.tolist(),\n        radius=8,max_zoom=13,name='Heat Map').add_to(basemap)\nfolium.LayerControl(collapsed=False).add_to(basemap)\nbasemap","38db27ac":"# Correlation wrt price\ncorr = house_price_data.corr()\ncorr_abs = corr.abs()\n\nnr_num_cols = len(numerical_feats)\nser_corr = corr_abs.nlargest(nr_num_cols, 'price')['price']\nprint(ser_corr)","96bf3559":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\ntarget = house_price_data[\"price\"]\nfeatures = house_price_data.drop(\"price\", axis = 1)\n\nX_train, X_test, Y_train, Y_test = train_test_split(features, target, test_size = 0.3, random_state = 1)\n\nsc = StandardScaler()\nX_train_sc = pd.DataFrame(sc.fit_transform(X_train))\nX_test_sc = pd.DataFrame(sc.transform(X_test))","282cd3ad":"def get_best_score(grid):\n    \n    best_score = np.sqrt(-grid.best_score_)\n    print(best_score)    \n    print(grid.best_params_)\n    print(grid.best_estimator_)\n    \n    return best_score","d842b69a":"from sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nparameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\ngrid_linear = GridSearchCV(linreg, parameters, cv = folds, verbose = 1 , scoring = score_calc)\ngrid_linear.fit(X_train, Y_train)\n\nsc_linear = get_best_score(grid_linear)","5b6b3ddb":"LR = LinearRegression()\nLR.fit(X_train, Y_train)\npred_linreg_all = LR.predict(X_test)\npred_linreg_all[pred_linreg_all < 0] = pred_linreg_all.mean()","81f672aa":"# Rsquared Score for Linear Regression\nr2_score(Y_test, pred_linreg_all)","883bf340":"from sklearn.linear_model import SGDRegressor\n\nsgd = SGDRegressor()\nparameters = {'max_iter' :[10000], 'alpha':[1e-05], 'epsilon':[1e-02], 'fit_intercept' : [True]  }\ngrid_sgd = GridSearchCV(sgd, parameters, cv = folds, verbose = 0, scoring = score_calc)\ngrid_sgd.fit(X_train_sc, Y_train)\n\nsc_sgd = get_best_score(grid_sgd)\npred_sgd = grid_sgd.predict(X_test_sc)","7185f52f":"# Rsquared Score for SGD Regressor\nr2_score(Y_test, pred_sgd)","8f42cbc2":"from sklearn.tree import DecisionTreeRegressor\n\nparam_grid = { 'max_depth' : [7,8,9,10] , 'max_features' : [11,12,13,14] ,\n               'max_leaf_nodes' : [None, 12,15,18,20] ,'min_samples_split' : [20,25,30],\n                'presort': [False,True] , 'random_state': [5] }\n\ngrid_dtree = GridSearchCV(DecisionTreeRegressor(), param_grid, cv = folds, refit = True, verbose = 0, scoring = score_calc)\ngrid_dtree.fit(X_train, Y_train)\n\nsc_dtree = get_best_score(grid_dtree)\n\npred_dtree = grid_dtree.predict(X_test)","ddc20e7a":"# Rsquared Score for DTree Regression\nr2_score(Y_test, pred_dtree)","09a79177":"from sklearn.ensemble import RandomForestRegressor\n\nparam_grid = {'min_samples_split' : [3,4,6,10], 'n_estimators' : [70,100], 'random_state': [5] }\ngrid_rf = GridSearchCV(RandomForestRegressor(), param_grid, cv = folds, refit=True, verbose = 0, scoring = score_calc)\ngrid_rf.fit(X_train, Y_train)\n\nsc_rf = get_best_score(grid_rf)\npred_rf = grid_rf.predict(X_test)","177b3e58":"# Rsquared Score for RandomForest\nr2_score(Y_test, pred_rf)","99643e49":"from sklearn.neighbors import KNeighborsRegressor\n\nparam_grid = {'n_neighbors' : [3,4,5,6,7,10,15] ,              \n              'weights' : ['uniform','distance'] ,\n              'algorithm' : ['ball_tree', 'kd_tree', 'brute']}\n\ngrid_knn = GridSearchCV(KNeighborsRegressor(), param_grid, cv = folds, refit=True, verbose = 0, scoring = score_calc)\ngrid_knn.fit(X_train_sc, Y_train)\nsc_knn = get_best_score(grid_knn)\npred_knn = grid_knn.predict(X_test_sc)","9b38c46b":"# Rsquared Score for KNN\nr2_score(Y_test, pred_knn)","2a807d5f":"from xgboost import XGBRegressor\nparam_grid = {'learning_rate' : [0.005,0.01,0.001], 'n_estimators' : [40,200], 'random_state': [5],\n              'max_depth' : [4,9]}\ngrid_xgb = GridSearchCV(XGBRegressor(), param_grid, cv = folds, refit=True, verbose = 0, scoring = score_calc)\ngrid_xgb.fit(X_train, Y_train)\n\nsc_xgb = get_best_score(grid_xgb)\npred_xgb = grid_xgb.predict(X_test)","a08758da":"# Rsquared Score for XGBoost Regressor\nr2_score(Y_test, pred_xgb)","64628623":"list_scores = [sc_linear,sc_sgd, sc_dtree, sc_rf, sc_knn, sc_xgb]\nlist_predictions = [pred_linreg_all, pred_sgd, pred_dtree, pred_rf, pred_knn, pred_xgb]\nlist_regressors = ['Linear','SGD','DTr','RF','KNN','XGB']\nfig, ax = plt.subplots()\nfig.set_size_inches(10,7)\nsns.barplot(x=list_regressors, y=list_scores, ax=ax)\nplt.ylabel('RMSE Training')\nplt.show()","4854120f":"errors = []\nfor pred in list_predictions:\n    errors.append(rmse(Y_test, pred))\n    \nregressors = ['Linear','SGD','DTr','RF','KNN','XGB']\nfig, ax = plt.subplots()\nfig.set_size_inches(10,7)\nsns.barplot(x = regressors, y = errors, ax=ax)\nplt.ylabel('Actual Test RMSE')\nplt.xlabel('Algorithms')\nplt.show()","f0eb3d56":"predictions = {'Linear': pred_linreg_all,\n               'SGD': pred_sgd, 'DTr': pred_dtree, 'RF': pred_rf,\n               'KNN': pred_knn, 'XGB': pred_xgb}\ndf_predictions = pd.DataFrame(data=predictions) ","8b9b4bee":"plt.figure(figsize=(10,8))\ncor_pred = df_predictions.corr()\nsns.heatmap(cor_pred, annot=True, cmap=plt.cm.Reds)\nplt.show()","8b7cbf11":"**Distribution of all the parameters with respect to price. Features such as living area, lot area, grade and number bathroom are showing a linear trend with respect to price. Also the distribution below can be used to remove the possible outliers. For this analysis, we're not going to extract outliers but these plots are highly useful for removing outliers form the data**","7bb8a5ea":"**Prices of house are almost distributed normally with mean roughly centered around 45,000 USD. Also, We're going to ignore the impact of inflation over the years as  it is not mentioned in the data. But introducting inflation can be used to forecast the increment in prices of house for every subsequent year.**","64578540":"# Result Evaluation\n\n**Linear Regression models gave up high RMSE. Also, XGB, KNN and Dtree models performed similarly with roughly 162.5k RMS error in estimation. While RandomForest recorded the lowest RMS error.**","828210af":"# RandomForest Regressor","366f7089":"# 2. Feature Engineering\n\n**Now we'll add some new features to our house price data. When we actually look for a house, we keep all these features mentioned below in our mind:**\n\n* **Age of house** - Age of house extracted by choosing 2020 as a reference year.\n* **Is renovated(Categorical)** - 1 for renovated condition, 0 otherwise \n* **Total Area of house** \n* **Basement(Categorical)** - 1 if basement is available, 0 otherwise","f686951d":"** Significant evidence of high correlation between Price and Area of house**","a6beae64":"# KNN Regressor","68c0f21c":"**Creating Data Split for training and testing purpose. Also, using a scaled version of dataset for certain algorithms.**","429f38be":"# XGBoost Regressor","ed037de3":"**Let's make use of Lattitide, Longitude and Zipcode to explore the surrounded area. Thanks to Burhan for this neat heatmap!!**","1b41b7f7":"# Decision Tree Regressor","86afc42b":"# How closely the predictions are related from different models\n\n**Only for Random Forest, Decision Tree and XGBoost, the results are less correlated with the other Regressors.**","3ef6de80":"# **Conclusion from EDA:**\n\n**We see that for some features like 'sqft_living' and 'grade' there is a strong linear correlation of (0.70) and (0.67) to the target.\nFor this kernel I decided to use all features for prediction that have correlation with SalePrice.\nIn future work, we can try to drop some columns that have weak correlation with the target price. \nAlso, we can remove the outliers from the data for better model building practices. **","556db255":"# 4. Model Building\n\n**Scikit-learn offers a wide variety of regression algorithms. We'll build simple to complex regression models using different algorithms and will compare the results obtained over different models with our testing dataset which contains 30% of original dataset. For tuning the model and selecting optimum parameters. We'll use GridSearch cross validation.**\n\n**We test the following Regressors from scikit-learn:**\n* LinearRegression\n* Ridge\n* Lasso\n* Stochastic Gradient Descent\n* DecisionTreeRegressor\n* RandomForestRegressor\n* KNN Regressor\n* XGBoost Regressor","ecbea808":"# 5. Conclusion\n\nSo far, we explored the data and added some new features into it. We did some EDA to visualize the impact of features over the traget price variable. Then we built multiple regression models and evaluated their preformance over the test dataset. RandomForest outformed other models and gave a RMSE slightly lower than 150k. We also compared correlation from different predicted estimates over the testing dataset. That's all folks!!\n\n**Future Work**: I think by tuning the parameters of XGBoost, we can achieve lower RMSE than RandomForest. I'll update the kernel later by improving the RMSE from XGBoost Regressor.\n\nIf you like this kernel than I would appreciate an **upvote** from you. \n\nThanks!!\n","80bd0afc":"**Using Grid Search Cross Validation for choosing the best parameters of models based on obtained scores.**","883584dc":"# 3. Visualizing and Examining Data\n\n**This is not a very big data and we do not have too many features. Thus, we have chance to plot most of them and reach some useful analytical results. Drawing charts and examining the data before applying a model is a very good practice because we may detect some possible outliers or decide to do normalization. This is not a must but get know the data is always good. **\n","973a78be":"# Stochastic Gradient Descent Regressor","c02c05c7":" # 1. Introduction\n\n** Today, we're going to explore Washington House Price Data and will come up with a model to estimate the price of house given all the required features. We'll be doing some basic EDA followed by building multiple regression models from sckit-learn library and will evaluate their performance by comparing the predictions.**\n","dad26fab":"# **Linear Regression**","228f7171":"**Not all features are significant for regression task. We'll discard Latitude, Longitude and Zipcode to proceed with the regression models. **","bc96c158":"**Century old houses. Hope these houses are still in living condtion. Although distribution is very skewed.**","11fa5a1c":"# Moment of truth\n**Let's compare the overall RMS error secured by models over the testing dataset.**\n**RandomForest outformed all the models in the analysis and secured a rms value less than 150K which means we can actually use the RandomForest model in estimating the house price given all the other features.**"}}