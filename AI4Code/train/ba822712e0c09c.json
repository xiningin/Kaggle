{"cell_type":{"7b66ba7a":"code","3d50f967":"code","cc2b2451":"code","9fe94e57":"code","0f447378":"code","58d7668a":"code","0b04627e":"code","82588bec":"code","cdf20645":"code","3e29b26f":"code","983834ff":"code","08c891ad":"code","e3cc0f0b":"code","bd2cded6":"code","b46e1084":"code","e69bf845":"code","8d487fa3":"code","8d3ff1e9":"code","db83b9fc":"code","2d86f89a":"code","ed9344e7":"code","5c290a7f":"code","f29ce179":"code","8892eaff":"markdown","1ce5e4e8":"markdown"},"source":{"7b66ba7a":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords \n\nimport pandas as pd\nimport numpy as np # linear algebra\n\nfrom scipy.sparse import hstack, csr_matrix\nimport seaborn as sns\n# https:\/\/www.kaggle.com\/tunguz\/bow-meta-text-and-dense-features-lb-0-2241\n\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\n\nSEED = 23","3d50f967":"stopWords = set(stopwords.words('english'))\nstopWords","cc2b2451":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntraindex = train.index\n\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ntestdex = test.index","9fe94e57":"train.head(5)","0f447378":"test_id = test.id.values","58d7668a":"import gc\ny = train.target.values\ntrain.drop(['target'],axis=1, inplace=True)\ndf = pd.concat([train,test],axis=0)\n\nto_drop = ['id', 'location']\ndf.drop(to_drop, axis=1, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndel train, test\ngc.collect()","0b04627e":"from sklearn.model_selection import KFold\n\nkf = KFold(n_splits=5, shuffle=False)\ncols = ['keyword']\ntrain_new = df.loc[traindex,:].copy()\ntrain_new['target'] = y\n\nfor train_index, test_index in kf.split(train_new):\n    X_tr, X_val = train_new.iloc[train_index], train_new.iloc[test_index]\n    for col in cols:\n        item_id_target_mean = X_tr.groupby(col).target.mean()\n        X_val[col + 'mean_enc'] = X_val[col].map(item_id_target_mean)\n#         X_tr[col + 'mean_enc'] = X_tr[col].map(item_id_target_mean)\n    train_new.iloc[test_index] = X_val\n#     train_new.iloc[train_index] = X_tr\n\nprior = y.mean()\ntrain_new.fillna(prior, inplace=True)","82588bec":"# Calculate a mapping: {item_id: target_mean}\ntrain_new = df.loc[traindex,:].copy()\ntrain_new['target'] = y\nkeyword_target_mean = train_new.groupby('keyword').target.mean()\n\n# In our non-regularized case we just *map* the computed means to the `item_id`'s\ndf.loc[traindex,'keyword_target_enc'] = df.loc[testdex,'keyword'].map(keyword_target_mean)\n\n# Fill NaNs\ndf['keyword_target_enc'].fillna(y.mean(), inplace=True) \ndf.drop(['keyword'],axis=1, inplace=True)","cdf20645":"import matplotlib.pyplot as plt\nfig, axes = plt.subplots(ncols=2, figsize=(10, 4), dpi=100)\ndf_train = df.loc[traindex,:].copy()\ndf_test = df.loc[testdex,:].copy()\nDISASTER_TWEETS = y == 1\n\nfor i, feature in enumerate(['keyword_target_enc']):\n    sns.distplot(df_train.loc[~DISASTER_TWEETS][feature], label='Not Disaster', ax=axes[0], color='green')\n    sns.distplot(df_train.loc[DISASTER_TWEETS][feature], label='Disaster', ax=axes[0], color='red')\n# plt.legend()\n    sns.distplot(df_train[feature], label='Training', ax=axes[1])\n    sns.distplot(df_test[feature], label='Test', ax=axes[1])\n    axes[0].legend()\n    axes[1].legend()","3e29b26f":"# Meta Text Features\ntextfeats = [\"text\"]\nfor cols in textfeats:\n    df[cols] = df[cols].astype(str) \n    df[cols] = df[cols].astype(str).fillna('nicapotato') # FILL NA\n    df[cols] = df[cols].str.lower() # Lowercase all text, so that capitalized words dont get treated differently\n    df[cols + '_num_chars'] = df[cols].apply(len) # Count number of Characters\n    df[cols + '_num_words'] = df[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n    df[cols + '_num_unique_words'] = df[cols].apply(lambda comment: len(set(w for w in comment.split())))\n    df[cols + '_words_vs_unique'] = df[cols+'_num_unique_words'] \/ df[cols+'_num_words'] * 100 # Count Unique Words\n\nprint(\"\\n[TF-IDF] Term Frequency Inverse Document Frequency Stage\")\nstopWords = set(stopwords.words('english'))","983834ff":"import numpy as np\ntfidf_para = {\n    \"stop_words\": stopWords,\n    \"analyzer\": 'word',\n    \"token_pattern\": r'\\w{1,}',\n    \"sublinear_tf\": True,\n    \"dtype\": np.float32,\n    \"norm\": 'l2',\n    #\"min_df\":5,\n    #\"max_df\":.9,\n    \"smooth_idf\":False\n}","08c891ad":"from sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(ngram_range=(1,2), max_features=17000, **tfidf_para)\nvect.fit(df.loc[traindex,:].text.values)\nready_df = vect.transform(df.text.values)","e3cc0f0b":"df.head(4)","bd2cded6":"ready_df.shape","b46e1084":"# X = hstack([csr_matrix(df.drop(['id', 'keyword', 'location', 'text'],axis=1).values),ready_df]) # Sparse Matrix\nX = hstack([csr_matrix(df.iloc[traindex,1:].values),ready_df[0:traindex.shape[0]]])","e69bf845":"X_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.10, random_state=SEED)","8d487fa3":"lgbm_params =  {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': ['auc', 'f1'],\n    'max_depth': 16,\n    'num_leaves': 37,\n    'feature_fraction': 0.6,\n    'bagging_fraction': 0.8,\n    # 'bagging_freq': 5,\n    'learning_rate': 0.019,\n    'verbose': 0\n}  \n\n# LGBM Dataset Formatting \nlgtrain = lgb.Dataset(X_train, y_train)#,\n#                 feature_name=tfvocab,\n#                 categorical_feature = categorical)\nlgvalid = lgb.Dataset(X_valid, y_valid)#,\n#                 feature_name=tfvocab,\n#                 categorical_feature = categorical)","8d3ff1e9":"from sklearn.metrics import f1_score\n\ndef lgb_f1_score(y_hat, data):\n    y_true = data.get_label()\n    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n    return 'f1', f1_score(y_true, y_hat), True\n\nevals_result = {}\n\nlgb_clf = lgb.train(\n    lgbm_params,\n    lgtrain,\n    num_boost_round=16000,\n    valid_sets=[lgtrain, lgvalid],\n    valid_names=['train','valid'],\n    early_stopping_rounds=200,\n    verbose_eval=200,\n    feval=lgb_f1_score, evals_result=evals_result)\n\nlgb.plot_metric(evals_result, metric='f1')","db83b9fc":"testing = hstack([csr_matrix(df.iloc[testdex,1:].values),\\\n                  ready_df[traindex.shape[0]:]])\n\nlgpred = lgb_clf.predict(testing)\nlgsub = pd.DataFrame(lgpred,columns=[\"target\"],index=testdex)\nlgsub['target'].clip(0.0, 1.0, inplace=True) # Between 0 and 1\nlgsub['id'] = test_id\nlgsub.to_csv(\"submission.csv\",index=False,header=True)","2d86f89a":"import seaborn as sns\nsns.distplot(lgsub.target.values)","ed9344e7":"lgpred = lgb_clf.predict(X)\ntrain_new = df.loc[traindex,:].copy()\ntrain_new['target'] = y\ntrain_new['pred'] = lgpred","5c290a7f":"sns.distplot(train_new.target)","f29ce179":"train_new.sample(20)","8892eaff":"# logistic regr + tf idf","1ce5e4e8":"# mean-encoding"}}