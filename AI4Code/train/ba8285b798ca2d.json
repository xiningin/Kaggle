{"cell_type":{"899d885c":"code","575ac22a":"code","99f5f445":"code","9c20dfcf":"code","9cbf9e02":"code","181c01ec":"code","ab65da4d":"code","badf6942":"code","47b7ad90":"code","4366ce55":"code","911f0330":"code","a6158523":"code","2f0ae0b2":"code","54ebf4e3":"code","35b81208":"code","64d9a265":"code","03761ecd":"code","e6a1ed17":"code","386dd0b8":"code","453618b1":"code","9b8d88ed":"code","6d60fc7e":"code","aed16054":"code","ba7e804c":"code","3c27e07c":"code","cdb26ab0":"code","17f467fd":"code","c1a90cf6":"code","e20aa61f":"code","2c5cb8ff":"code","b3495b9d":"code","277a2994":"code","8f7de63e":"code","8f11ce46":"code","fa076e07":"code","1b0877d5":"code","8bdcf04e":"code","7339751d":"code","6b5341ef":"code","cd2840d8":"code","bbfb5420":"code","f390dae2":"code","70b3e119":"code","381fc01f":"code","e3eb5089":"code","207485cb":"code","43b771c9":"code","4c7daa16":"code","16295d1c":"code","542a0807":"code","8d35ac08":"code","11b76d2a":"code","7e26830c":"code","fa88f130":"code","06f06e9a":"code","f4ede698":"code","104aacfd":"code","eb6e441a":"code","7d0b2330":"code","dc016ebb":"code","1228fef6":"code","eca0b153":"code","8c86d55b":"code","779cf045":"code","0737e383":"code","8471ded4":"code","282d9afd":"code","210ab137":"code","db1efe39":"code","42b4d9b6":"code","9d1bb9e9":"code","b1827767":"code","7f0dbc22":"code","cac0b241":"code","a5473819":"code","da1ef34d":"code","d6826fbe":"code","d909e861":"markdown","5e59a1b4":"markdown","01738970":"markdown","6b8e56bb":"markdown","f51c1133":"markdown","e7448486":"markdown","ac3251b8":"markdown","a35edf4a":"markdown","29bdf1cc":"markdown","34413c41":"markdown","c49c3b2f":"markdown","a1483377":"markdown","0a52c1ed":"markdown","0ca12975":"markdown","567400a0":"markdown","45c0d0c1":"markdown","e97e8f44":"markdown","de8997bb":"markdown","5dc56086":"markdown","7a9a3dcf":"markdown","c6944ec9":"markdown","6240d160":"markdown","138bca7f":"markdown","102f260f":"markdown","3c55ea3e":"markdown","21594dc9":"markdown","f162b1e4":"markdown","ba1e4be7":"markdown","9535047c":"markdown","9aa35b93":"markdown","1b75c56c":"markdown","c5168ab7":"markdown","cc82c5da":"markdown","96180b96":"markdown","eb1a2d92":"markdown","da51eee5":"markdown","841d9e14":"markdown","12b6cfb0":"markdown","7d27242f":"markdown","181acd90":"markdown","363230fb":"markdown","d03f4c50":"markdown","7efc8ec0":"markdown","04b477ea":"markdown","1e372c22":"markdown","288a4a96":"markdown","a57569ed":"markdown","87f6c05e":"markdown","e18e99b7":"markdown","6edb7418":"markdown","73680b11":"markdown","a20ee60b":"markdown","82d33683":"markdown","cb9794bf":"markdown","be605bbc":"markdown","e897780a":"markdown","cfbc1fb5":"markdown","d8c2460d":"markdown","41276539":"markdown","0898d35c":"markdown","e91d6d4b":"markdown","d0277c5c":"markdown","65cc6e39":"markdown","33a94e33":"markdown","3b8e2144":"markdown","56c16998":"markdown","7dbeb3b3":"markdown","1128696f":"markdown","99b4d3d5":"markdown","042c5a70":"markdown","3356c8da":"markdown","b8f1296f":"markdown","711b4166":"markdown","ec5232b3":"markdown","24cb75e9":"markdown","3d33340a":"markdown","dbf88523":"markdown","e3750c4d":"markdown","9a71f8d6":"markdown","7aed3c69":"markdown","3bb4ce82":"markdown","f4aae3ab":"markdown","257acf3f":"markdown","f67c7db7":"markdown","344333c8":"markdown","66c4de94":"markdown","f76e479b":"markdown"},"source":{"899d885c":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport time\nimport warnings\nimport numpy as np\nwarnings.filterwarnings(\"ignore\")\n\nfrom nltk.corpus import stopwords\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import normalize\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics.classification import accuracy_score, log_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\nfrom scipy.sparse import hstack\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold \nfrom collections import Counter, defaultdict\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nimport math\nfrom sklearn.metrics import normalized_mutual_info_score\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom mlxtend.classifier import StackingClassifier\n\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\n","575ac22a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","99f5f445":"!unzip -o \/kaggle\/input\/msk-redefining-cancer-treatment\/training_variants.zip\n!unzip -o \/kaggle\/input\/msk-redefining-cancer-treatment\/training_text.zip\n\nimport py7zr\n\narchive = py7zr.SevenZipFile('\/kaggle\/input\/msk-redefining-cancer-treatment\/stage2_test_variants.csv.7z', mode='r')\narchive.extractall(path=\".\/\")\narchive.close()\n\narchive = py7zr.SevenZipFile('\/kaggle\/input\/msk-redefining-cancer-treatment\/stage2_test_text.csv.7z', mode='r')\narchive.extractall(path=\".\/\")\narchive.close()","9c20dfcf":"data = pd.read_csv('training_variants')\nprint('Number of data points : ', data.shape[0])\nprint('Number of features : ', data.shape[1])\nprint('Features : ', data.columns.values)\n\ndata.head()","9cbf9e02":"# note the seprator in this file\ndata_text =pd.read_csv(\"training_text\",sep=\"\\|\\|\",engine=\"python\",names=[\"ID\",\"TEXT\"],skiprows=1)\nprint('Number of data points : ', data_text.shape[0])\nprint('Number of features : ', data_text.shape[1])\nprint('Features : ', data_text.columns.values)\n\ndata_text.head()","181c01ec":"#data_text=data_text[:400]\n#data=data[:400]\n","ab65da4d":"from nltk.stem import PorterStemmer\nfrom nltk.stem import LancasterStemmer\n# loading stop words from nltk library\nstop_words = set(stopwords.words('english'))\n\ndef nlp_preprocessing(total_text):\n    if type(total_text) is str:\n        string = \"\"\n        # replace every special char with space\n        total_text = re.sub('[^a-zA-Z0-9\\n]', ' ', total_text)\n        # replace multiple spaces with single space\n        total_text = re.sub('\\s+',' ', total_text)\n        # converting all the chars into lower-case.\n        total_text = total_text.lower()\n        \n        string1=\"\"\n        for word in total_text.split():\n        # if the word is a not a stop word then retain that word from the data\n            if word not in stop_words:\n                string1 += word + \" \"\n        \n        string2=\"\"\n        for word in string1.split():\n        # if the length of the word is more than 3, retain that word from the data\n            if len(word) >3:\n                string2 += word + \" \"\n        \n        for word in string1.split():\n        # Stemming each word of TEXT field\n            lancaster=LancasterStemmer()\n            word= lancaster.stem(word)\n            string += word + \" \"\n            \n        return string","badf6942":"import swifter","47b7ad90":"import swifter\n#text processing stage.\nstart_time = time.clock()\n\ndata_text['TEXT']=data_text['TEXT'].swifter.apply(lambda x:nlp_preprocessing(x))\n\nprint('Time took for preprocessing the text :',time.clock() - start_time, \"seconds\")","4366ce55":"#merging both gene_variations and text data based on ID\nresult = pd.merge(data, data_text,on='ID', how='left')\n\nresult.head()","911f0330":"result[result.isnull().any(axis=1)]\n\nresult.loc[result['TEXT'].isnull(),'TEXT'] = result['Gene'] +' '+result['Variation']\n\nresult[result['ID']==1109]","a6158523":"y_true = result['Class'].values\nresult.Gene      = result.Gene.str.replace('\\s+', '_')\nresult.Variation = result.Variation.str.replace('\\s+', '_')\n\n# split the data into test and train by maintaining same distribution of output varaible 'y_true' [stratify=y_true]\nX_train, test_df, y_train, y_test = train_test_split(result, y_true, stratify=y_true, test_size=0.20, random_state=25 )\n# split the train data into train and cross validation by maintaining same distribution of output varaible 'y_train' [stratify=y_train]\ntrain_df, cv_df, y_train, y_cv = train_test_split(X_train, y_train, stratify=y_train, test_size=0.20, random_state=25)","2f0ae0b2":"print('Number of data points in train data:', train_df.shape[0])\nprint('Number of data points in test data:', test_df.shape[0])\nprint('Number of data points in cross validation data:', cv_df.shape[0])","54ebf4e3":"print(train_df.TEXT.str.split().str.len())","35b81208":"# Using the top 1000 words only\n\ntext_vectorizer = TfidfVectorizer(ngram_range=(1,1), max_df=0.80 ,max_features=1000)\n#text_vectorizer = TfidfVectorizer(ngram_range=(1,1),min_df=1)\ntext_vectorizer.fit(train_df['TEXT'])\n# getting all the feature names (words)\ntop1000_words =  text_vectorizer.get_feature_names()\nprint(len(top1000_words))\n\ngene_list = train_df['Gene'].tolist()\ngene_list = [x.lower() for x in gene_list]\n\nvariation_list = train_df['Variation'].tolist()\nvariation_list = [x.lower() for x in variation_list]\n   \ndef select_top1000words(text_str):\n    if type(text_str) is not int:\n        string = \"\"\n                \n        for word in text_str.split():\n        # if the word is within Top 1000 words then retain that word from the data\n            if (word in top1000_words) or (word in gene_list) or (word in variation_list):\n                string += word + \" \"\n        \n        return string\n","64d9a265":"#text processing stage.\nstart_time = time.clock()\n\ntrain_df['TEXT']=train_df['TEXT'].swifter.apply(lambda x:select_top1000words(x))\ntest_df['TEXT']=test_df['TEXT'].swifter.apply(lambda x:select_top1000words(x))\ncv_df['TEXT']=cv_df['TEXT'].swifter.apply(lambda x:select_top1000words(x))\n\nprint('Time took for preprocessing the text :',time.clock() - start_time, \"seconds\")","03761ecd":"print(\"No: of words in each row Text:\")\nprint(train_df.TEXT.str.split().str.len())","e6a1ed17":"# it returns a dict, keys as class labels and values as the number of data points in that class\ntrain_class_distribution = train_df['Class'].value_counts().sort_index()\ntest_class_distribution = test_df['Class'].value_counts().sort_index()\ncv_class_distribution = cv_df['Class'].value_counts().sort_index()\n\nmy_colors = 'rgbkymc'\ntrain_class_distribution.plot(kind='bar')\nplt.xlabel('Class')\nplt.ylabel('Data points per Class')\nplt.title('Distribution of yi in train data')\nplt.grid()\nplt.show()\n\n# ref: argsort https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.argsort.html\n# -(train_class_distribution.values): the minus sign will give us in decreasing order\nsorted_yi = np.argsort(-train_class_distribution.values)\nfor i in sorted_yi:\n    print('Number of data points in class', i+1, ':',train_class_distribution.values[i], '(', np.round((train_class_distribution.values[i]\/train_df.shape[0]*100), 3), '%)')\n\n    \nprint('-'*80)\nmy_colors = 'rgbkymc'\ntest_class_distribution.plot(kind='bar')\nplt.xlabel('Class')\nplt.ylabel('Data points per Class')\nplt.title('Distribution of yi in test data')\nplt.grid()\nplt.show()\n\n# ref: argsort https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.argsort.html\n# -(train_class_distribution.values): the minus sign will give us in decreasing order\nsorted_yi = np.argsort(-test_class_distribution.values)\nfor i in sorted_yi:\n    print('Number of data points in class', i+1, ':',test_class_distribution.values[i], '(', np.round((test_class_distribution.values[i]\/test_df.shape[0]*100), 3), '%)')\n\nprint('-'*80)\nmy_colors = 'rgbkymc'\ncv_class_distribution.plot(kind='bar')\nplt.xlabel('Class')\nplt.ylabel('Data points per Class')\nplt.title('Distribution of yi in cross validation data')\nplt.grid()\nplt.show()\n\n# ref: argsort https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.argsort.html\n# -(train_class_distribution.values): the minus sign will give us in decreasing order\nsorted_yi = np.argsort(-train_class_distribution.values)\nfor i in sorted_yi:\n    print('Number of data points in class', i+1, ':',cv_class_distribution.values[i], '(', np.round((cv_class_distribution.values[i]\/cv_df.shape[0]*100), 3), '%)')\n","386dd0b8":"# This function plots the confusion matrices given y_i, y_i_hat.\ndef plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n    \n    A =(((C.T)\/(C.sum(axis=1))).T)\n    #divid each element of the confusion matrix with the sum of elements in that column\n    \n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.T = [[1, 3],\n    #        [2, 4]]\n    # C.sum(axis = 1)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =1) = [[3, 7]]\n    # ((C.T)\/(C.sum(axis=1))) = [[1\/3, 3\/7]\n    #                           [2\/3, 4\/7]]\n\n    # ((C.T)\/(C.sum(axis=1))).T = [[1\/3, 2\/3]\n    #                           [3\/7, 4\/7]]\n    # sum of row elements = 1\n    \n    B =(C\/C.sum(axis=0))\n    #divid each element of the confusion matrix with the sum of elements in that row\n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.sum(axis = 0)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =0) = [[4, 6]]\n    # (C\/C.sum(axis=0)) = [[1\/4, 2\/6],\n    #                      [3\/4, 4\/6]] \n    \n    labels = [1,2,3,4,5,6,7,8,9]\n    # representing A in heatmap format\n    print(\"-\"*20, \"Confusion matrix\", \"-\"*20)\n    plt.figure(figsize=(20,7))\n    sns.heatmap(C, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n\n    print(\"-\"*20, \"Precision matrix (Columm Sum=1)\", \"-\"*20)\n    plt.figure(figsize=(20,7))\n    sns.heatmap(B, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n    \n    # representing B in heatmap format\n    print(\"-\"*20, \"Recall matrix (Row sum=1)\", \"-\"*20)\n    plt.figure(figsize=(20,7))\n    sns.heatmap(A, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()","453618b1":"# we need to generate 9 numbers and the sum of numbers should be 1\n# one solution is to genarate 9 numbers and divide each of the numbers by their sum\n# ref: https:\/\/stackoverflow.com\/a\/18662466\/4084039\ntest_data_len = test_df.shape[0]\ncv_data_len = cv_df.shape[0]\n\n# we create a output array that has exactly same size as the CV data\ncv_predicted_y = np.zeros((cv_data_len,9))\nfor i in range(cv_data_len):\n    rand_probs = np.random.rand(1,9)\n    cv_predicted_y[i] = ((rand_probs\/sum(sum(rand_probs)))[0])\nprint(\"Log loss on Cross Validation Data using Random Model\",log_loss(y_cv,cv_predicted_y, eps=1e-15))\n\n# Test-Set error.\n#we create a output array that has exactly same as the test data\ntest_predicted_y = np.zeros((test_data_len,9))\nfor i in range(test_data_len):\n    rand_probs = np.random.rand(1,9)\n    test_predicted_y[i] = ((rand_probs\/sum(sum(rand_probs)))[0])\nprint(\"Log loss on Test Data using Random Model\",log_loss(y_test,test_predicted_y, eps=1e-15))\n\npredicted_y =np.argmax(test_predicted_y, axis=1)\nplot_confusion_matrix(y_test, predicted_y+1)","9b8d88ed":"unique_genes = train_df['Gene'].value_counts()\nprint('Number of Unique Genes :', unique_genes.shape[0])\n# the top 10 genes that occured most\nprint(unique_genes.head(10))","6d60fc7e":"print(\"Ans: There are\", unique_genes.shape[0] ,\"different categories of genes in the train data, and they are distibuted as follows\",)","aed16054":"s = sum(unique_genes.values);\nh = unique_genes.values\/s;\nplt.plot(h, label=\"Histrogram of Genes\")\nplt.xlabel('Index of a Gene')\nplt.ylabel('Number of Occurances')\nplt.legend()\nplt.grid()\nplt.show()\n","ba7e804c":"c = np.cumsum(h)\nplt.plot(c,label='Cumulative distribution of Genes')\nplt.grid()\nplt.legend()\nplt.show()","3c27e07c":"# one-hot encoding of Gene feature.\ngene_vectorizer = TfidfVectorizer(ngram_range=(1,1))\ntrain_gene_feature_onehotCoding = gene_vectorizer.fit_transform(train_df['Gene'])\ntest_gene_feature_onehotCoding = gene_vectorizer.transform(test_df['Gene'])\ncv_gene_feature_onehotCoding = gene_vectorizer.transform(cv_df['Gene'])","cdb26ab0":"train_df['Gene'].head()","17f467fd":"print(\"train_gene_feature_onehotCoding is converted feature using one-hot encoding method. The shape of gene feature:\", train_gene_feature_onehotCoding.shape)","c1a90cf6":"alpha = [10 ** x for x in range(-5, 1)] # hyperparam for SGD classifier.\n\ncv_log_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=25)\n    clf.fit(train_gene_feature_onehotCoding, y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_gene_feature_onehotCoding, y_train)\n    predict_y = sig_clf.predict_proba(cv_gene_feature_onehotCoding)\n    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=25)\nclf.fit(train_gene_feature_onehotCoding, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_gene_feature_onehotCoding, y_train)\n\npredict_y = sig_clf.predict_proba(train_gene_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_gene_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_gene_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n","e20aa61f":"print(\"Q6. How many data points in Test and CV datasets are covered by the \", unique_genes.shape[0], \" genes in train dataset?\")\n\ntest_coverage=test_df[test_df['Gene'].isin(list(set(train_df['Gene'])))].shape[0]\ncv_coverage=cv_df[cv_df['Gene'].isin(list(set(train_df['Gene'])))].shape[0]\n\nprint('Ans\\n1. In test data',test_coverage, 'out of',test_df.shape[0], \":\",(test_coverage\/test_df.shape[0])*100)\nprint('2. In cross validation data',cv_coverage, 'out of ',cv_df.shape[0],\":\" ,(cv_coverage\/cv_df.shape[0])*100)","2c5cb8ff":"unique_variations = train_df['Variation'].value_counts()\nprint('Number of Unique Variations :', unique_variations.shape[0])\n# the top 10 variations that occured most\nprint(unique_variations.head(10))","b3495b9d":"print(\"Ans: There are\", unique_variations.shape[0] ,\"different categories of variations in the train data, and they are distibuted as follows\",)","277a2994":"s = sum(unique_variations.values);\nh = unique_variations.values\/s;\nplt.plot(h, label=\"Histrogram of Variations\")\nplt.xlabel('Index of a Variation')\nplt.ylabel('Number of Occurances')\nplt.legend()\nplt.grid()\nplt.show()","8f7de63e":"c = np.cumsum(h)\nplt.plot(c,label='Cumulative distribution of Variations')\nplt.grid()\nplt.legend()\nplt.show()","8f11ce46":"# one-hot encoding of variation feature.\nvariation_vectorizer = TfidfVectorizer(ngram_range=(1,1))\ntrain_variation_feature_onehotCoding = variation_vectorizer.fit_transform(train_df['Variation'])\ntest_variation_feature_onehotCoding = variation_vectorizer.transform(test_df['Variation'])\ncv_variation_feature_onehotCoding = variation_vectorizer.transform(cv_df['Variation'])\n\nvariation_vectorizer_test=variation_vectorizer","fa076e07":"print(\"train_variation_feature_onehotEncoded is converted feature using the onne-hot encoding method. The shape of Variation feature:\", train_variation_feature_onehotCoding.shape)","1b0877d5":"alpha = [10 ** x for x in range(-5, 1)]\n\ncv_log_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=25)\n    clf.fit(train_variation_feature_onehotCoding, y_train)\n    \n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_variation_feature_onehotCoding, y_train)\n    predict_y = sig_clf.predict_proba(cv_variation_feature_onehotCoding)\n    \n    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=25)\nclf.fit(train_variation_feature_onehotCoding, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_variation_feature_onehotCoding, y_train)\n\npredict_y = sig_clf.predict_proba(train_variation_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_variation_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_variation_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n","8bdcf04e":"print(\"Q12. How many data points are covered by total \", unique_variations.shape[0], \" genes in test and cross validation data sets?\")\ntest_coverage=test_df[test_df['Variation'].isin(list(set(train_df['Variation'])))].shape[0]\ncv_coverage=cv_df[cv_df['Variation'].isin(list(set(train_df['Variation'])))].shape[0]\nprint('Ans\\n1. In test data',test_coverage, 'out of',test_df.shape[0], \":\",(test_coverage\/test_df.shape[0])*100)\nprint('2. In cross validation data',cv_coverage, 'out of ',cv_df.shape[0],\":\" ,(cv_coverage\/cv_df.shape[0])*100)","7339751d":"# building a CountVectorizer with all the words that occured minimum 3 times in train data\ntext_vectorizer = TfidfVectorizer(ngram_range=(1,1))\ntrain_text_feature_onehotCoding = text_vectorizer.fit_transform(train_df['TEXT'])\n#Copying vectorizer to use it later with test data\ntext_vectorizer_test=text_vectorizer\n# getting all the feature names (words)\ntrain_text_features= text_vectorizer.get_feature_names()\n\n# train_text_feature_onehotCoding.sum(axis=0).A1 will sum every row and returns (1*number of features) vector\ntrain_text_fea_counts = train_text_feature_onehotCoding.sum(axis=0).A1\n\n# zip(list(text_features),text_fea_counts) will zip a word with its number of times it occured\ntext_fea_dict = dict(zip(list(train_text_features),train_text_fea_counts))\n\n\nprint(\"Total number of unique words in train data :\", len(train_text_features))\nprint(train_text_feature_onehotCoding.shape)","6b5341ef":"# don't forget to normalize every feature\ntrain_text_feature_onehotCoding = normalize(train_text_feature_onehotCoding, axis=0)\n\n# we use the same vectorizer that was trained on train data\ntest_text_feature_onehotCoding = text_vectorizer.transform(test_df['TEXT'])\n# don't forget to normalize every feature\ntest_text_feature_onehotCoding = normalize(test_text_feature_onehotCoding, axis=0)\n\n# we use the same vectorizer that was trained on train data\ncv_text_feature_onehotCoding = text_vectorizer.transform(cv_df['TEXT'])\n# don't forget to normalize every feature\ncv_text_feature_onehotCoding = normalize(cv_text_feature_onehotCoding, axis=0)","cd2840d8":"# Train a Logistic regression+Calibration model using text features which are one-hot encoded\nalpha = [10 ** x for x in range(-5, 1)]\n\ncv_log_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=25)\n    clf.fit(train_text_feature_onehotCoding, y_train)\n    \n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_text_feature_onehotCoding, y_train)\n    predict_y = sig_clf.predict_proba(cv_text_feature_onehotCoding)\n    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=25)\nclf.fit(train_text_feature_onehotCoding, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_text_feature_onehotCoding, y_train)\n\npredict_y = sig_clf.predict_proba(train_text_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_text_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_text_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n","bbfb5420":"def get_intersec_text(df):\n    df_text_vec = TfidfVectorizer(ngram_range=(1,1),min_df=3)\n    df_text_fea = df_text_vec.fit_transform(df['TEXT'])\n    df_text_features = df_text_vec.get_feature_names()\n\n    len1 = len(set(df_text_features))\n    len2 = len(set(train_text_features) & set(df_text_features))\n    return len1,len2","f390dae2":"len1,len2 = get_intersec_text(test_df)\nprint(np.round((len2\/len1)*100, 3), \"% of word of test data appeared in train data\")\nlen1,len2 = get_intersec_text(cv_df)\nprint(np.round((len2\/len1)*100, 3), \"% of word of Cross Validation appeared in train data\")","70b3e119":"#Data preparation for ML models.\n\n#Misc. functionns for ML models\n\n\ndef predict_and_plot_confusion_matrix(train_x, train_y,test_x, test_y, clf):\n    clf.fit(train_x, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x, train_y)\n    pred_y = sig_clf.predict(test_x)\n\n    # for calculating log_loss we willl provide the array of probabilities belongs to each class\n    print(\"Log loss :\",log_loss(test_y, sig_clf.predict_proba(test_x)))\n    # calculating the number of data points that are misclassified\n    print(\"Number of mis-classified points :\", np.count_nonzero((pred_y- test_y))\/test_y.shape[0])\n    plot_confusion_matrix(test_y, pred_y)","381fc01f":"# this function will be used just for naive bayes\n# for the given indices, we will print the name of the features\n# and we will check whether the feature present in the test point text or not\ndef get_impfeature_names(indices, text, gene, var, no_features):\n    gene_tfidf_vec = TfidfVectorizer(ngram_range=(1,1))\n    var_tfidf_vec = TfidfVectorizer(ngram_range=(1,1))\n    text_tfidf_vec = TfidfVectorizer(ngram_range=(1,1))\n    \n    gene_vec = gene_tfidf_vec.fit(train_df['Gene'])\n    var_vec  = var_tfidf_vec.fit(train_df['Variation'])\n    text_vec = text_tfidf_vec.fit(train_df['TEXT'])\n    \n    fea1_len = len(gene_vec.get_feature_names())\n    fea2_len = len(var_tfidf_vec.get_feature_names())\n        \n    word_present = 0\n    for i,v in enumerate(indices):\n        if (v < fea1_len):\n            word = gene_vec.get_feature_names()[v]\n            yes_no = True if word == gene else False\n            if yes_no:\n                word_present += 1\n                print(i, \"Gene feature [{}] present in test data point [{}]\".format(word,yes_no))\n        elif (v < fea1_len+fea2_len):\n            word = var_vec.get_feature_names()[v-(fea1_len)]\n            yes_no = True if word == var else False\n            if yes_no:\n                word_present += 1\n                print(i, \"variation feature [{}] present in test data point [{}]\".format(word,yes_no))\n        else:            \n            \n            word = text_vec.get_feature_names()[v-(fea1_len+fea2_len)]\n            yes_no = True if word in text.split() else False\n            if yes_no:\n                word_present += 1\n                print(i, \"Text feature [{}] present in test data point [{}]\".format(word,yes_no))\n\n    print(\"Out of the top \",no_features,\" features \", word_present, \"are present in query point\")","e3eb5089":"# merging gene, variance and text features\n\n# building train, test and cross validation data sets\n# a = [[1, 2], \n#      [3, 4]]\n# b = [[4, 5], \n#      [6, 7]]\n# hstack(a, b) = [[1, 2, 4, 5],\n#                [ 3, 4, 6, 7]]\n\ntrain_gene_var_onehotCoding = hstack((train_gene_feature_onehotCoding,train_variation_feature_onehotCoding))\ntest_gene_var_onehotCoding = hstack((test_gene_feature_onehotCoding,test_variation_feature_onehotCoding))\ncv_gene_var_onehotCoding = hstack((cv_gene_feature_onehotCoding,cv_variation_feature_onehotCoding))\n\ntrain_x_onehotCoding = hstack((train_gene_var_onehotCoding, train_text_feature_onehotCoding)).tocsr()\ntrain_y = np.array(list(train_df['Class']))\n\nprint(test_gene_feature_onehotCoding.shape)\nprint(train_variation_feature_onehotCoding.shape)\nprint(train_text_feature_onehotCoding.shape)\n\ntest_x_onehotCoding = hstack((test_gene_var_onehotCoding, test_text_feature_onehotCoding)).tocsr()\ntest_y = np.array(list(test_df['Class']))\n\ncv_x_onehotCoding = hstack((cv_gene_var_onehotCoding, cv_text_feature_onehotCoding)).tocsr()\ncv_y = np.array(list(cv_df['Class']))\n","207485cb":"print(\"One hot encoding features :\")\nprint(\"(number of data points * number of features) in train data = \", train_x_onehotCoding.shape)\nprint(\"(number of data points * number of features) in test data = \", test_x_onehotCoding.shape)\nprint(\"(number of data points * number of features) in cross validation data =\", cv_x_onehotCoding.shape)","43b771c9":"alpha = [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100,1000]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = MultinomialNB(alpha=i)\n    clf.fit(train_x_onehotCoding, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_onehotCoding, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(np.log10(alpha), cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (np.log10(alpha[i]),cv_log_error_array[i]))\nplt.grid()\nplt.xticks(np.log10(alpha))\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = MultinomialNB(alpha=alpha[best_alpha])\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\n\npredict_y = sig_clf.predict_proba(train_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n","4c7daa16":"clf = MultinomialNB(alpha=alpha[best_alpha])\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\nsig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n# to avoid rounding error while multiplying probabilites we use log-probability estimates\nprint(\"Log Loss :\",log_loss(cv_y, sig_clf_probs))\nprint(\"Number of missclassified point :\", np.count_nonzero((sig_clf.predict(cv_x_onehotCoding)- cv_y))\/cv_y.shape[0])\nplot_confusion_matrix(cv_y, sig_clf.predict(cv_x_onehotCoding.toarray()))","16295d1c":"alpha = [10 ** x for x in range(-6, 3)]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = SGDClassifier(class_weight='balanced', alpha=i, penalty='l2', loss='log', random_state=25)\n    clf.fit(train_x_onehotCoding, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_onehotCoding, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=25)\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","542a0807":"clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=25)\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y, cv_x_onehotCoding, cv_y, clf)","8d35ac08":"# from tabulate import tabulate\nclf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=25)\nclf.fit(train_x_onehotCoding,train_y)\ntest_point_index = 1\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","11b76d2a":"test_point_index = 50\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","7e26830c":"alpha = [10 ** x for x in range(-6, 1)]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=25)\n    clf.fit(train_x_onehotCoding, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_onehotCoding, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=25)\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","fa88f130":"clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=25)\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y, cv_x_onehotCoding, cv_y, clf)","06f06e9a":"clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=25)\nclf.fit(train_x_onehotCoding,train_y)\ntest_point_index = 1\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","f4ede698":"test_point_index = 50\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","104aacfd":"clf1 = SGDClassifier(alpha=0.001, penalty='l2', loss='log', class_weight='balanced', random_state=25)\nclf1.fit(train_x_onehotCoding, train_y)\nsig_clf1 = CalibratedClassifierCV(clf1, method=\"sigmoid\")\n\nclf2 = SGDClassifier(alpha=1, penalty='l2', loss='hinge', class_weight='balanced', random_state=25)\nclf2.fit(train_x_onehotCoding, train_y)\nsig_clf2 = CalibratedClassifierCV(clf2, method=\"sigmoid\")\n\n\nclf3 = MultinomialNB(alpha=0.001)\nclf3.fit(train_x_onehotCoding, train_y)\nsig_clf3 = CalibratedClassifierCV(clf3, method=\"sigmoid\")\n\nsig_clf1.fit(train_x_onehotCoding, train_y)\nprint(\"Logistic Regression :  Log Loss: %0.2f\" % (log_loss(cv_y, sig_clf1.predict_proba(cv_x_onehotCoding))))\nsig_clf2.fit(train_x_onehotCoding, train_y)\nprint(\"Support vector machines : Log Loss: %0.2f\" % (log_loss(cv_y, sig_clf2.predict_proba(cv_x_onehotCoding))))\nsig_clf3.fit(train_x_onehotCoding, train_y)\nprint(\"Naive Bayes : Log Loss: %0.2f\" % (log_loss(cv_y, sig_clf3.predict_proba(cv_x_onehotCoding))))\nprint(\"-\"*50)\nalpha = [0.0001,0.001,0.01,0.1,1,10] \nbest_alpha = 999\nfor i in alpha:\n    lr = LogisticRegression(C=i)\n    sclf = StackingClassifier(classifiers=[sig_clf1, sig_clf2, sig_clf3], meta_classifier=lr, use_probas=True)\n    sclf.fit(train_x_onehotCoding, train_y)\n    print(\"Stacking Classifer : for the value of alpha: %f Log Loss: %0.3f\" % (i, log_loss(cv_y, sclf.predict_proba(cv_x_onehotCoding))))\n    log_error =log_loss(cv_y, sclf.predict_proba(cv_x_onehotCoding))\n    if best_alpha > log_error:\n        best_alpha = log_error","eb6e441a":"lr = LogisticRegression(C=0.1)\nsclf = StackingClassifier(classifiers=[sig_clf1, sig_clf2, sig_clf3], meta_classifier=lr, use_probas=True)\nsclf.fit(train_x_onehotCoding, train_y)\n\nlog_error = log_loss(train_y, sclf.predict_proba(train_x_onehotCoding))\nprint(\"Log loss (train) on the stacking classifier :\",log_error)\n\nlog_error = log_loss(cv_y, sclf.predict_proba(cv_x_onehotCoding))\nprint(\"Log loss (CV) on the stacking classifier :\",log_error)\n\nlog_error = log_loss(test_y, sclf.predict_proba(test_x_onehotCoding))\nprint(\"Log loss (test) on the stacking classifier :\",log_error)\n\nprint(\"Number of missclassified point :\", np.count_nonzero((sclf.predict(test_x_onehotCoding)- test_y))\/test_y.shape[0])\nplot_confusion_matrix(test_y=test_y, predict_y=sclf.predict(test_x_onehotCoding))","7d0b2330":"#Refer:http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.VotingClassifier.html\nfrom sklearn.ensemble import VotingClassifier\nvclf = VotingClassifier(estimators=[('lr', sig_clf1), ('svc', sig_clf2), ('rf', sig_clf3)], voting='soft')\nvclf.fit(train_x_onehotCoding, train_y)\nprint(\"Log loss (train) on the VotingClassifier :\", log_loss(train_y, vclf.predict_proba(train_x_onehotCoding)))\nprint(\"Log loss (CV) on the VotingClassifier :\", log_loss(cv_y, vclf.predict_proba(cv_x_onehotCoding)))\nprint(\"Log loss (test) on the VotingClassifier :\", log_loss(test_y, vclf.predict_proba(test_x_onehotCoding)))\nprint(\"Number of missclassified point :\", np.count_nonzero((vclf.predict(test_x_onehotCoding)- test_y))\/test_y.shape[0])\nplot_confusion_matrix(test_y=test_y, predict_y=vclf.predict(test_x_onehotCoding))","dc016ebb":"from sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\ntrain_textWords=np.asarray(train_df['TEXT'].str.split().str.len())\ncv_textWords=np.asarray(cv_df['TEXT'].str.split().str.len())\ntest_textWords=np.asarray(test_df['TEXT'].str.split().str.len())\n\nscaler = preprocessing.MinMaxScaler()\ntrain_textWords1=scaler.fit_transform(train_textWords.reshape(-1,1))\ncv_textWords1=scaler.transform(cv_textWords.reshape(-1,1))\ntest_textWords1=scaler.transform(test_textWords.reshape(-1,1))\n","1228fef6":"train_textLength=np.asarray(train_df['TEXT'].apply(len))\ncv_textLength=np.asarray(cv_df['TEXT'].apply(len))\ntest_textLength=np.asarray(test_df['TEXT'].apply(len))","eca0b153":"train_word_density=train_textLength\/(train_textWords+1)\ncv_word_density=cv_textLength\/(cv_textWords+1)\ntest_word_density=test_textLength\/(test_textWords+1)\n\nscaler = preprocessing.MinMaxScaler()\ntrain_word_density=scaler.fit_transform(train_word_density.reshape(-1,1))\ncv_word_density=scaler.transform(cv_word_density.reshape(-1,1))\ntest_word_density=scaler.transform(test_word_density.reshape(-1,1))\n","8c86d55b":"#Count of Genes not useful, therefore ignoring as of now\ngene_list = train_df['Gene'].tolist()\ngene_list = [x.lower() for x in gene_list]\n\ntrain_geneCount = np.asarray(train_df['TEXT'].apply(lambda x: len([w for w in str(x).split() if w in gene_list]) ))\ncv_geneCount = np.asarray(cv_df['TEXT'].apply(lambda x: len([w for w in str(x).split() if w in gene_list]) ))\ntest_geneCount = np.asarray(test_df['TEXT'].apply(lambda x: len([w for w in str(x).split() if w in gene_list]) ))\n\nscaler = preprocessing.MinMaxScaler()\ntrain_geneCount=scaler.fit_transform(train_geneCount.reshape(-1,1))\ncv_geneCount=scaler.transform(cv_geneCount.reshape(-1,1))\ntest_geneCount=scaler.transform(test_geneCount.reshape(-1,1))\n","779cf045":"train_textFeatures = np.hstack((train_word_density,train_geneCount,train_textWords1))\ncv_textFeatures = np.hstack((cv_word_density,cv_geneCount,cv_textWords1))\ntest_textFeatures = np.hstack((test_word_density,test_geneCount,test_textWords1))\n","0737e383":"# merging gene, variance, text and Words features\n\ntrain_x = hstack((train_x_onehotCoding, train_textFeatures)).tocsr()\ncv_x = hstack((cv_x_onehotCoding, cv_textFeatures)).tocsr()\ntest_x = hstack((test_x_onehotCoding, test_textFeatures)).tocsr()\n\ntrain_y = np.array(list(train_df['Class']))\ncv_y = np.array(list(cv_df['Class']))\ntest_y = np.array(list(test_df['Class']))\n\n","8471ded4":"print(\"One hot encoding features :\")\nprint(\"(number of data points * number of features) in train data = \", train_x.shape)\nprint(\"(number of data points * number of features) in test data = \", test_x.shape)\nprint(\"(number of data points * number of features) in cross validation data =\", cv_x.shape)","282d9afd":"alpha = [10 ** x for x in range(-6, 1)]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=25)\n    clf.fit(train_x, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x)\n    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=25)\nclf.fit(train_x, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x, train_y)\n\npredict_y = sig_clf.predict_proba(train_x)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","210ab137":"clf = SGDClassifier( alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=25)\npredict_and_plot_confusion_matrix(train_x, train_y, cv_x, cv_y, clf)","db1efe39":"# Comparing all the models using Prettytable library\n\nfrom prettytable import PrettyTable\n    \nPT = PrettyTable()\n\nPT.field_names = [\"Model\",  \"CV Log Loss\", \"Test Log Loss\"]\n\nPT.add_row([\"\"\"Naive Bayes\"\"\", 1.158, 1.175])\nPT.add_row([\"\"\"Logistic Regression (w\/ Balancing)\"\"\", 1.034, 0.982])\nPT.add_row([\"\"\"Logistic Regression (w\/o Balancing)\"\"\", 1.049, 0.995])\nPT.add_row([\"\"\"Model Stacking\"\"\", 1.109, 1.085])\nPT.add_row([\"\"\"Logistic Regression (with added features)\"\"\", 0.964, 0.934])\n\nprint(PT)\nprint(\"=\"*100)\n\nprint(\"\\nFor the given data set, Logistic Regression (with added features) provides the best Test log loss of 0.934\")\nprint('Time took for preprocessing the text :',time.clock() - start_time, \"seconds\")","42b4d9b6":"\ntest_data = pd.read_csv('stage2_test_variants.csv')\nprint('Number of data points : ', test_data.shape[0])\nprint(test_data.head())\n\ntest_data_text =pd.read_csv(\"stage2_test_text.csv\",sep=\"\\|\\|\",engine=\"python\",names=[\"ID\",\"TEXT\"],skiprows=1)\nprint('Number of data points : ', test_data_text.shape[0])","9d1bb9e9":"test_data_text=test_data_text.fillna('xx')","b1827767":"test_data_text['TEXT']=test_data_text['TEXT'].swifter.apply(lambda x:nlp_preprocessing(x))\n\ntest_df = pd.merge(test_data, test_data_text,on='ID', how='left')\n\ntest_df['TEXT']=test_df['TEXT'].swifter.apply(lambda x:select_top1000words(x))","7f0dbc22":"test_gene_feature_onehotCoding = gene_vectorizer.transform(test_df['Gene'])\ntest_variation_feature_onehotCoding = variation_vectorizer_test.transform(test_df['Variation'])\n\n# we use the same vectorizer that was trained on train data\ntest_text_feature_onehotCoding = text_vectorizer_test.transform(test_df['TEXT'])\n# don't forget to normalize every feature\ntest_text_feature_onehotCoding = normalize(test_text_feature_onehotCoding, axis=0)\n","cac0b241":"test_gene_var_onehotCoding = hstack((test_gene_feature_onehotCoding,test_variation_feature_onehotCoding))\ntest_x_onehotCoding_LR = hstack((test_gene_var_onehotCoding,test_text_feature_onehotCoding))\n","a5473819":"scaler = preprocessing.MinMaxScaler()\n\ntest_textWords=np.asarray(test_df['TEXT'].str.split().str.len())\nscaler.fit(train_textWords.reshape(-1,1))\ntest_textWords1=scaler.transform(test_textWords.reshape(-1,1))\n\ntest_textLength=np.asarray(test_df['TEXT'].apply(len))\ntest_word_density=test_textLength\/(test_textWords+1)\nscaler.fit(train_word_density.reshape(-1,1))\ntest_word_density=scaler.transform(test_word_density.reshape(-1,1))\n\ntest_geneCount = np.asarray(test_df['TEXT'].apply(lambda x: len([w for w in str(x).split() if w in gene_list]) ))\nscaler.fit(train_geneCount.reshape(-1,1))\ntest_geneCount=scaler.transform(test_geneCount.reshape(-1,1))\n\ntest_textFeatures = np.hstack((test_word_density,test_geneCount,test_textWords1))\n\ntest_x = hstack((test_x_onehotCoding_LR, test_textFeatures)).tocsr()","da1ef34d":"predict_y = sig_clf.predict_proba(test_x)","d6826fbe":"submission_df = pd.DataFrame(predict_y, columns=['class'+str(c+1) for c in range(9)])\nsubmission_df.insert(0, 'ID', test_data['ID'])\nsubmission_df.to_csv('submission.csv', index=False)","d909e861":"<p> Objective: Predict the probability of each data-point belonging to each of the nine classes.\n<\/p>\n<p> Constraints:\n<\/p>\n* Interpretability\n* Class probabilities are needed.\n* Penalize the errors in class probabilites => Metric is Log-loss.\n* No Latency constraints.","5e59a1b4":"<h3>4.2.1 With Class balancing<\/h3>","01738970":" Split the dataset randomly into three parts train, cross validation and test with 64%,16%, 20% of data respectively","6b8e56bb":"### Feature Engineering performed by me to achieve a Test and CV log loss <1.0\n\n    1) While data preprocessing\/cleaning (Cell no.4) retained words in text column having length>3\n    2) Used Lancaster Stemming(most helpful in reducing the log loss)\n    3) Extracted features like Word count, Character Count, Word Density and Gene count and merged them \n       with pre-extracted features","f51c1133":"<h3>4.3.1 testing with hyper parameter tuning<\/h3>","e7448486":"<h2>2.1. Data<\/h2>","ac3251b8":"<h2>3.3 Univariate Analysis<\/h2>","a35edf4a":"<p style=\"font-size:18px;\"> <b>Q11.<\/b> Is the Variation feature stable across all the data sets (Test, Train, Cross validation)?<\/p>\n<p style=\"font-size:16px;\"> <b>Ans.<\/b> Not sure! But lets be very sure using the below analysis. <\/p>","29bdf1cc":"<h2>1.2. Source\/Useful Links<\/h2>","34413c41":"<h1>5. Assignments<\/h1>","c49c3b2f":"<h2>1.3. Real-world\/Business objectives and constraints.<\/h2>","a1483377":"<h3>3.1.1. Reading Gene and Variation Data<\/h3>","0a52c1ed":"* No low-latency requirement.\n* Interpretability is important.\n* Errors can be very costly.\n* Probability of a data-point belonging to each class is needed.","0ca12975":"## Gene Count","567400a0":"<h1>4. Machine Learning Models<\/h1>","45c0d0c1":"<h2>4.1. Base Line Model<\/h2>","e97e8f44":"<h3>4.1.1. Naive Bayes<\/h3>","de8997bb":"<h2>4.3 Stack the models <\/h2>","5dc56086":"<p>\n    training\/training_variants is a comma separated file containing the description of the genetic mutations used for training. <br>\n    Fields are \n    <ul>\n        <li><b>ID : <\/b>the id of the row used to link the mutation to the clinical evidence<\/li>\n        <li><b>Gene : <\/b>the gene where this genetic mutation is located <\/li>\n        <li><b>Variation : <\/b>the aminoacid change for this mutations <\/li>\n        <li><b>Class :<\/b> 1-9 the class this genetic mutation has been classified on<\/li>\n    <\/ul>","7a9a3dcf":"<p style=\"font-size:18px;\"> <b>Q.<\/b> Is the Text feature stable across all the data sets (Test, Train, Cross validation)?<\/p>\n<p style=\"font-size:16px;\"> <b>Ans.<\/b> Yes, it seems like! <\/p>","c6944ec9":"<h3>3.1.2. Reading Text Data<\/h3>","6240d160":"<h4>4.1.1.2. Testing the model with best hyper paramters<\/h4>","138bca7f":"<h1>2. Machine Learning Problem Formulation<\/h1>","102f260f":"## Merged Features","3c55ea3e":"<h4>4.2.1.2 Testing the model with best hyper paramters<\/h4>","21594dc9":"<h3>2.2.3. Machine Learing Objectives and Constraints<\/h3>","f162b1e4":"## Number of characters","ba1e4be7":"<h2>1.1. Description<\/h2>","9535047c":"<h4>4.2.2.3 Feature Importance, Correctly Classified point<\/h4>","9aa35b93":"<h1>3. Exploratory Data Analysis<\/h1>","1b75c56c":"<h3>4.2.2 Without Class balancing<\/h3>","c5168ab7":"## Word Density","cc82c5da":"<h3>2.2.1. Type of Machine Learning Problem<\/h3>","96180b96":"<p> Source: https:\/\/www.kaggle.com\/c\/msk-redefining-cancer-treatment\/ <\/p>\n<p> Data: Memorial Sloan Kettering Cancer Center (MSKCC)<\/p>\n<p> Download training_variants.zip and training_text.zip from Kaggle.<\/p> \n\n<h6> Context:<\/h6>\n<p> Source: https:\/\/www.kaggle.com\/c\/msk-redefining-cancer-treatment\/discussion\/35336#198462<\/p>\n\n<h6> Problem statement : <\/h6>\n<p> Classify the given genetic variations\/mutations based on evidence from text-based clinical literature. <\/p>","eb1a2d92":"1. https:\/\/www.forbes.com\/sites\/matthewherper\/2017\/06\/03\/a-new-cancer-drug-helped-almost-everyone-who-took-it-almost-heres-what-it-teaches-us\/#2a44ee2f6b25\n2. https:\/\/www.youtube.com\/watch?v=UwbuW7oK8rk \n3. https:\/\/www.youtube.com\/watch?v=qxXRKVompI8","da51eee5":"<h4>4.2.2.4 Feature Importance, Inorrectly Classified point<\/h4>","841d9e14":"<h3>3.1.4. Test, Train and Cross Validation Split<\/h3>","12b6cfb0":"<h3>3.2.2 Univariate Analysis on Variation Feature<\/h3>","7d27242f":"<h3>2.2.2. Performance Metric<\/h3>","181acd90":"<h2>3.2 Prediction using a 'Random' Model<\/h2>","363230fb":"<h2>4.2. Logistic Regression<\/h2>","d03f4c50":"<h4>4.1.1.1. Hyper parameter tuning<\/h4>","7efc8ec0":"<h4>5.1.2 Testing the model with best hyper paramters<\/h4>","04b477ea":"<p style=\"font-size:18px;\"> <b>Q9.<\/b> How to featurize this Variation feature ?<\/p>\n\n<p style=\"font-size:16px;\"><b>Ans.<\/b>There are two ways we can featurize this variable\ncheck out this video: https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/handling-categorical-and-numerical-features\/\n<ol><li>One hot Encoding<\/li><li>Response coding<\/li><\/ol><\/p>\n<p> We will be only using One Hot encoding to featurize the Variation Feature <\/p>","1e372c22":"<p style=\"font-size:16px\"> In a 'Random' Model, we generate the NINE class probabilites randomly such that they sum to 1. <\/p>","288a4a96":"<h3>2.1.2. Example Data Point<\/h3>","a57569ed":"1. How many unique words are present in train data?\n2. How are word frequencies distributed?\n3. How to featurize text field?\n4. Is the text feature useful in predicitng y_i?\n5. Is the text feature stable across train, test and CV datasets?","87f6c05e":"<ol>\n    <li> Apply All the models with tf-idf features (Replace CountVectorizer with tfidfVectorizer and run the same cells)<\/li>\n    <li> Instead of using all the words in the dataset, use only the top 1000 words based of tf-idf values<\/li>\n    <li>Apply Logistic regression with CountVectorizer Features, including both unigrams and bigrams<\/li>\n    <li> Try any of the feature engineering techniques discussed in the course to reduce the CV and test log-loss to a value less than 1.0<\/li>\n<\/ol>","e18e99b7":"Source: https:\/\/www.kaggle.com\/c\/msk-redefining-cancer-treatment#evaluation\n\nMetric(s): \n* Multi class log-loss \n* Confusion matrix \n","6edb7418":"<p style=\"font-size:36px;text-align:center\"> <b>Personalized cancer diagnosis<\/b> <\/p>","73680b11":"<h3>3.2.3 Univariate Analysis on Text Feature<\/h3>","a20ee60b":"<h6>training_variants<\/h6>\n<hr>\nID,Gene,Variation,Class<br>\n0,FAM58A,Truncating Mutations,1 <br>\n1,CBL,W802*,2 <br>\n2,CBL,Q249E,2 <br>\n...\n\n<h6> training_text<\/h6>\n<hr>\nID,Text <br>\n0||Cyclin-dependent kinases (CDKs) regulate a variety of fundamental cellular processes. CDK10 stands out as one of the last orphan CDKs for which no activating cyclin has been identified and no kinase activity revealed. Previous work has shown that CDK10 silencing increases ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2)-driven activation of the MAPK pathway, which confers tamoxifen resistance to breast cancer cells. The precise mechanisms by which CDK10 modulates ETS2 activity, and more generally the functions of CDK10, remain elusive. Here we demonstrate that CDK10 is a cyclin-dependent kinase by identifying cyclin M as an activating cyclin. Cyclin M, an orphan cyclin, is the product of FAM58A, whose mutations cause STAR syndrome, a human developmental anomaly whose features include toe syndactyly, telecanthus, and anogenital and renal malformations. We show that STAR syndrome-associated cyclin M mutants are unable to interact with CDK10. Cyclin M silencing phenocopies CDK10 silencing in increasing c-Raf and in conferring tamoxifen resistance to breast cancer cells. CDK10\/cyclin M phosphorylates ETS2 in vitro, and in cells it positively controls ETS2 degradation by the proteasome. ETS2 protein levels are increased in cells derived from a STAR patient, and this increase is attributable to decreased cyclin M levels. Altogether, our results reveal an additional regulatory mechanism for ETS2, which plays key roles in cancer and development. They also shed light on the molecular mechanisms underlying STAR syndrome.Cyclin-dependent kinases (CDKs) play a pivotal role in the control of a number of fundamental cellular processes (1). The human genome contains 21 genes encoding proteins that can be considered as members of the CDK family owing to their sequence similarity with bona fide CDKs, those known to be activated by cyclins (2). Although discovered almost 20 y ago (3, 4), CDK10 remains one of the two CDKs without an identified cyclin partner. This knowledge gap has largely impeded the exploration of its biological functions. CDK10 can act as a positive cell cycle regulator in some cells (5, 6) or as a tumor suppressor in others (7, 8). CDK10 interacts with the ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2) transcription factor and inhibits its transcriptional activity through an unknown mechanism (9). CDK10 knockdown derepresses ETS2, which increases the expression of the c-Raf protein kinase, activates the MAPK pathway, and induces resistance of MCF7 cells to tamoxifen (6). ... ","82d33683":"<h4>4.2.2.1 Hyper paramter tuning<\/h4>","cb9794bf":"<h3>4.3.3 Maximum Voting classifier <\/h3>","be605bbc":"<h3>3.1.3. Preprocessing of text<\/h3>","e897780a":"<p style=\"font-size:18px;\"> <b>Q1.<\/b> Gene, What type of feature it is ?<\/p>\n<p style=\"font-size:16px;\"><b>Ans.<\/b> Gene is a categorical variable <\/p>\n<p style=\"font-size:18px;\"> <b>Q2.<\/b> How many categories are there and How they are distributed?<\/p>","cfbc1fb5":"<p> We split the data into train, test and cross validation data sets, preserving the ratio of class distribution in the original data set  <\/p>","d8c2460d":"<h3>3.2.1 Univariate Analysis on Gene Feature<\/h3>","41276539":" Some articles and reference blogs about the problem statement","0898d35c":"<h2>2.3. Train, CV and Test Datasets<\/h2>","e91d6d4b":"<h4>5.1.1 Hyper paramter tuning<\/h4>","d0277c5c":"### Step by Step Procedure:\n\n#### a) Data Cleaning\n            1) Data loaded from 2 files into pandas dataframe and merged into single dataframe.\n            2) Performed data cleaning.\n            3) Split the data in 64:20:16 Train:Test:CV ratio.\n            4) Performed Data Cleaning.\n            5) Basic analysis before feature extraction.\n\n#### a) Featurization and EDA\n            1) Univariate analysis on Gene feature and feature extraction using One hot encoding\n            2) Univariate analysis on Variation feature and feature extraction using One Hot encoding.\n            3) Univariate analysis on Text feature and feature extraction using One hot encoding.\n            4) While faeture extraction for Test feature, used only the Top 1000 words.\n         \n#### c) Modeling \n            1) Built various models like Naive bayes, K-NN, Logistic Regression, Linear SVM, Random Forest, Model Stacking \n               and hypertune them\n            2) Plotted Confusion matrix for all the models\n            3) Also shown feature importance for correctly and incorrectly classified data point for each model.\n            3) Select model which provides best results as per our performance metric i.e Log-loss","65cc6e39":"There are many ways to estimate how good a feature is, in predicting y_i. One of the good methods is to build a proper ML model using just this feature. In this case, we will build a logistic regression model using only Gene feature (one hot encoded) to predict y_i.","33a94e33":"<p style=\"font-size:18px;\"> <b>Q7.<\/b> Variation, What type of feature is it ?<\/p>\n<p style=\"font-size:16px;\"><b>Ans.<\/b> Variation is a categorical variable <\/p>\n<p style=\"font-size:18px;\"> <b>Q8.<\/b> How many categories are there?<\/p>","3b8e2144":"<p style=\"font-size:18px;\"> <b>Q4.<\/b> How good is this gene feature  in predicting y_i?<\/p>","56c16998":"Let's build a model just like the earlier!","7dbeb3b3":"<h2>2.2. Mapping the real-world problem to an ML problem<\/h2>","1128696f":"<h4>4.2.1.1 Hyper paramter tuning<\/h4>","99b4d3d5":"## Number of Words","042c5a70":"<p style=\"font-size:18px;\"> <b>Q3.<\/b> How to featurize this Gene feature ?<\/p>\n\n<p style=\"font-size:16px;\"><b>Ans.<\/b>there are two ways we can featurize this variable\ncheck out this video: https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/handling-categorical-and-numerical-features\/\n<ol><li>One hot Encoding<\/li><li>Response coding<\/li><\/ol><\/p>\n<p> We will choose the appropriate featurization based on the ML model we use.  For this problem of multi-class classification with categorical features, one-hot encoding is better for Logistic regression  <\/p>","3356c8da":"<p style=\"font-size:18px;\"> <b>Q10.<\/b> How good is this Variation feature  in predicting y_i?<\/p>","b8f1296f":"<h4>4.2.2.2 Testing model with best hyper parameters<\/h4>","711b4166":"when we caculate the probability of a feature belongs to any particular class, we apply laplace smoothing\n<li>(numerator + 10\\*alpha) \/ (denominator + 90\\*alpha) <\/li>","ec5232b3":"<h3>2.1.1. Data Overview<\/h3>","24cb75e9":"<h3>4.3.2 testing the model with the best hyper parameters<\/h3>","3d33340a":"<h5>4.2.1.3.2 Incorrectly Classified point<\/h5>","dbf88523":"<p style=\"font-size:24px;text-align:Center\"> <b>Stacking the three types of features <\/b><p>","e3750c4d":"<h1>1. Business Problem<\/h1>","9a71f8d6":"<h5>4.2.1.3.1 Correctly Classified point<\/h5>","7aed3c69":"<h4>3.1.4.2. Distribution of y_i's in Train, Test and Cross Validation datasets<\/h4>","3bb4ce82":"- Source: https:\/\/www.kaggle.com\/c\/msk-redefining-cancer-treatment\/data\n- We have two data files: one conatins the information about the genetic mutations and the other contains the clinical evidence (text) that  human experts\/pathologists use to classify the genetic mutations. \n- Both these data files are have a common column called ID\n- <p> \n    Data file's information:\n    <ul> \n        <li>\n        training_variants (ID , Gene, Variations, Class)\n        <\/li>\n        <li>\n        training_text (ID, Text)\n        <\/li>\n    <\/ul>\n<\/p>","f4aae3ab":"<p>\n    \n            There are nine different classes a genetic mutation can be classified into => Multi class classification problem\n   \n      \n    \n<\/p>","257acf3f":"# 6. Conclusions","f67c7db7":"<h4>3.1.4.1. Splitting data into train, test and cross validation (64:20:16)<\/h4>","344333c8":"<h2> 5.1 Logistic Regression with TfidfVectorizer<\/h2>","66c4de94":"<p style=\"font-size:18px;\"> <b>Q5.<\/b> Is the Gene feature stable across all the data sets (Test, Train, Cross validation)?<\/p>\n<p style=\"font-size:16px;\"> <b>Ans.<\/b> Yes, it is. Otherwise, the CV and Test errors would be significantly more than train error. <\/p>","f76e479b":"<h2>3.1. Reading Data<\/h2>"}}