{"cell_type":{"88c0c2a6":"code","dcea3ade":"code","f98db545":"code","ebb52938":"code","308dd671":"code","fe6a6bca":"code","ecc70b01":"code","3a56f755":"code","150f88c6":"code","3de93aeb":"code","f3c73e87":"code","3821c393":"code","ca7784b3":"code","5c6463c7":"code","7981f46b":"code","3517ce3a":"code","fc530194":"code","a7a07515":"code","b966a2f2":"code","7bb93d39":"code","2211c96b":"code","dbbe4d22":"code","95462dd6":"code","921f391f":"code","f44e145a":"code","652f2d94":"code","e28b39f0":"code","547a2aab":"code","27e83cd8":"code","184642f6":"code","76155c9f":"code","098cddac":"code","afdb511b":"code","41be24f7":"code","2eeccdea":"code","35c1d001":"code","53561ddf":"code","a63c19af":"code","659b723b":"code","aeaf8ff4":"code","a79175dc":"code","d4ce3527":"code","6bc6b2b0":"code","0c10db01":"code","01296e4f":"code","a18aa3a0":"code","0ece6ec8":"code","0a515716":"code","efc3563c":"code","3b78ede6":"code","48da24c9":"code","5f540c12":"code","95bf8e98":"code","ba4a2256":"code","9560c693":"code","061d63aa":"code","146070c8":"code","4b8afd0f":"code","ac154c45":"code","33148f51":"code","816edfea":"code","6f56bd0a":"code","3d15e0c8":"code","36b64ac0":"code","ea95dff9":"code","77af7ecb":"code","041fa363":"code","7f15c924":"code","5c96ba2a":"code","15c6728d":"code","0226cc8b":"code","7a5b2e8e":"code","5b5f9565":"code","d4eaeaff":"code","7426b1cd":"code","b45e4ad8":"code","d3f546eb":"code","bbf6411d":"code","d9bd099b":"code","aa4ef79f":"code","b26317c1":"code","4cdefa24":"code","07213958":"code","6b333265":"code","ca69ce59":"code","1ee3253d":"code","e874704d":"code","dab5acf0":"code","57cfc9e7":"code","6db95e04":"code","8c8acfbd":"code","6644fac4":"code","190ddb26":"code","2c590abc":"code","8ca7485e":"code","ebced741":"code","8f060f2b":"markdown","e1b17fcd":"markdown","cab989df":"markdown","f3386316":"markdown","0585b32c":"markdown","d5637bfd":"markdown","e0e304c7":"markdown","4f788171":"markdown","e4e2deca":"markdown","71d59579":"markdown","eea73e7a":"markdown","024b7398":"markdown","ab335529":"markdown","e6d14934":"markdown","7aa9c3fb":"markdown","6cf1c35e":"markdown","8c9ca11e":"markdown","eb60877e":"markdown","c997d68b":"markdown","a5dd24a5":"markdown","fc720ac7":"markdown","218b4bcf":"markdown","b814eea1":"markdown","37713e89":"markdown","016ba145":"markdown","a236f45c":"markdown","81d30417":"markdown","6e40e2b9":"markdown","d7235c34":"markdown","f4fd92bc":"markdown","5d0176ae":"markdown","a84f1226":"markdown","58b3c96f":"markdown","2d8cf363":"markdown","8faa14cb":"markdown","586734c1":"markdown","83774052":"markdown","f64a36da":"markdown","4a9c36a4":"markdown","e79cdc4e":"markdown","4e44b273":"markdown","3a59a364":"markdown","60d8b67f":"markdown"},"source":{"88c0c2a6":"import numpy as np  # Librer\u00eda para aplicar \u00e1lgebra lineal\nimport pandas as pd # Para manejar los datos (datasets)\n\nimport matplotlib.pyplot as plt # Para las visualizaciones\nimport seaborn as sns # Visualizaciones m\u00e1s fancy\n\n#from sklearn import preprocessing # Para el procesador de los datos\nfrom sklearn.preprocessing import Imputer # Para adoptar una estrategia para los missing values\nfrom sklearn.preprocessing import LabelEncoder as Codificar # Para codificar variables categ\u00f3ricas\nfrom sklearn.preprocessing import OneHotEncoder # Para pasar el LaberEncoder vector a OneHot matriz\nfrom sklearn.preprocessing import MinMaxScaler # Para realizar el escalado en escala (0-1)\nfrom sklearn.model_selection import train_test_split as Separar # Para dividir en los 2 conjuntos\nfrom statsmodels.tools.eval_measures import rmse # Para calcular el error\nfrom sklearn.metrics import confusion_matrix as CM # Para construir la matriz de confusi\u00f3n\nfrom matplotlib.colors import ListedColormap as Colors # Para pintar las regiones en Clasificaci\u00f3n","dcea3ade":"# 1. Importar los datos y resumen\ndatos = pd.read_csv('..\/input\/datoscsv\/4.1.Salarios.csv')\ndatos.head(5)","f98db545":"# 2. Estrategia para los datos que faltan\ndatos.info()","ebb52938":"# 3. Crear los conjuntos\n# 3.1. Crear variables predictoras y variable respuesta\nX = datos['Experiencia']\ny = datos['Salario']\n# 3.2. Utilizar el Separador\nX_train, X_test, y_train, y_test = Separar(X, y, test_size=0.3)\nprint('X_train: ', X_train.shape)\nprint('X_test: ', X_test.shape)\nprint('y_train: ', y_train.shape)\nprint('y_test: ', y_test.shape)","308dd671":"X_train = X_train.values.reshape(-1,1)\nX_test = X_test.values.reshape(-1,1)\nprint('X_train: ', X_train.shape)\nprint('X_test: ', X_test.shape)","fe6a6bca":"# 4. Escalar\n'''\nNo es necesario para el algoritmo de reg. lineal de sklearn\n'''","ecc70b01":"# 5. Construcci\u00f3n del modelo\nfrom sklearn.linear_model import LinearRegression as LR\nregresor_lineal = LR()\nregresor_lineal.fit(X_train, y_train)","3a56f755":"# 6. Hacer las predicciones\ny_fit  = regresor_lineal.predict(X_train)\ny_pred = regresor_lineal.predict(X_test)","150f88c6":"# 7. Analizar los resultados\n\n# 7.1. Gr\u00e1fico\n# 7.1.1. Para el conjunto de entrenamiento\nplt.figure()\nplt.scatter(X_train, y_train, color='red')\nplt.plot(X_train, y_fit, color='blue')\nplt.title('Regresi\u00f3n Linear (Conjunto entrenamieto)')\nplt.xlabel('Experiencia (A\u00f1os)')\nplt.ylabel('Salario (\u20ac)')\nplt.plot()\n# 7.1.2. Para el conjunto de entrenamiento\nplt.figure()\nplt.scatter(X_test, y_test, color='red')\nplt.plot(X_test, y_pred, color='blue')\nplt.title('Regresi\u00f3n Linear (Conjunto entrenamieto)')\nplt.xlabel('Experiencia (A\u00f1os)')\nplt.ylabel('Salario (\u20ac)')\nplt.plot()\n\n# 7.2. Error - RMSE\nRMSE = np.sum(rmse(y_test, y_pred))\/len(y_test)\nprint('RMSE: ', RMSE)","3de93aeb":"# 1. Importar los datos y resumen\ndatos = pd.read_csv('..\/input\/datoscsv\/4.1.Empresas.csv')\n\ndatos.head()","f3c73e87":"# 1.2. Crear variables y respues\nX = datos.iloc[:,:-1].values\ny = datos.iloc[:,-1].values","3821c393":"# 2. Varibles categ\u00f3ricas\ncodificador = Codificar()\nX[:,3] = codificador.fit_transform(X[:,3])\ndataframe = pd.DataFrame(X, columns = (['Investigaci\u00f3n', 'Administraci\u00f3n', 'Marketing', 'Pais']))\ndataframe.head(3)","ca7784b3":"onehotcodificador = OneHotEncoder(categorical_features=[3])\nX = onehotcodificador.fit_transform(X).toarray()\ndataframe = pd.DataFrame(X, columns = (['Florida', 'Nueva York', 'California', 'Investigaci\u00f3n', 'Administraci\u00f3n', 'Marketing']))\ndataframe.head(3)","5c6463c7":"X = X[:,1:]","7981f46b":"# 3. Crear conjuntos\nX_train, X_test, y_train, y_test = Separar(X, y, test_size=0.3)","3517ce3a":"# 4. Escalar\n'''\nNo es necesario para el algoritmo de reg. lineal de sklearn\n'''","fc530194":"# 5. Crear y ajustar el modelo\nfrom sklearn.linear_model import LinearRegression as RL\nregresor_multiple = LR()\nregresor_multiple.fit(X_train, y_train)","a7a07515":"# 6. Hacer las predicciones\ny_fit  = regresor_multiple.predict(X_train)\ny_pred = regresor_multiple.predict(X_test)","b966a2f2":"# 7. Analizar los resultados\n\n# 7.1. Gr\u00e1fico\n''' No hay gr\u00e1fico porque estamos en espacio multidimensional'''\n# 7.2. Error - RMSE\nRMSE = np.sum(rmse(y_test, y_pred))\/len(y_test)\nprint('RMSE: ', RMSE)","7bb93d39":"# 1. Importar los datos y resumen\ndatos = pd.read_csv('..\/input\/datoscsv\/4.1.Salarios2.csv')\ndatos.head()","2211c96b":"datos = datos.iloc[:, 1:]\ndatos['Salario'].plot()\nplt.xlabel('A\u00f1os de Experiencia')\nplt.ylabel('Salario')\ndatos.head(2)","dbbe4d22":"# 2. Missing Values o Categ\u00f3ricas (Nada)","95462dd6":"# 3. Crear los conjuntos\n# 3.1. Crear variables predictoras y variable respuesta\nX = datos['Nivel'].values.reshape(-1,1) # Si solo hay 1 variable queremos que sea MATRIZ\ny = datos['Salario'].values\n# 3.2. Utilizar el Separador\n'''\nEn este escenario de problema no nos interesa hacer una separaci\u00f3n por 2 motivos:\n1 - Tenemos solamente un dataset de 10 muestras. Es demasiado peque\u00f1o\n2 - No estamos interesados en predecir algo remoto, sino que queremos hacer una predicci\u00f3n muy exacta \nde lo que cobraba este empleado para encarar bien la negocaci\u00f3n. Estamos solo interesados en el ajuste\nde nuestro modelo y por tanto en nuestro conjunto de entrenamiento. X_train = X.\n----\nX_train, X_test, y_train, y_test = Separar(X, y, test_size=0.3)\nX_train = X_train.reshape(-1,1) # Si solo hay 1 variable queremos que sea MATRIZ\nX_test = X_test.reshape(-1,1)\nprint('X_train: ', X_train.shape)\nprint('X_test: ', X_test.shape)\nprint('y_train: ', y_train.shape)\nprint('y_test: ', y_test.shape)\n'''","921f391f":"# 4. Escalar\n'''\nNo es necesario\n'''","f44e145a":"# 5. Crear y ajustar el modelo\nfrom sklearn.preprocessing import PolynomialFeatures as Polinomizar\npolinomizador = Polinomizar(degree=3)\nX = polinomizador.fit_transform(X)\ndataframe = pd.DataFrame(X)\ndataframe.head(3)","652f2d94":"regresor_lineal.fit(X, y)","e28b39f0":"# 6. Hacer las predicciones\ny_fit = regresor_lineal.predict(X)\n# y_pred = regresor_lineal.predict(np.array([[6.5]]))\nprint('El empleado deb\u00eda estar cobrando en torno a: ')","547a2aab":"# 7. Analizar los resultados\nx = X[:,1] # Cojemos la columna original para la representaci\u00f3n\n# 7.1. Gr\u00e1fico\nplt.figure()\nplt.scatter(x, y, color='red')\nplt.plot(x, y_fit, color='blue')\nplt.title('Regresi\u00f3n Polin\u00f3mica')\nplt.xlabel('Nivel')\nplt.ylabel('Salario (\u20ac)')\nplt.plot()\n\n# 7.2. Error - RMSE\nRMSE = np.sum(rmse(y, y_fit))\/len(y)\nprint('RMSE: ', RMSE)","27e83cd8":"# 1. Importar los datos y resumen\ndatos = pd.read_csv('..\/input\/datoscsv\/4.1.Salarios2.csv')","184642f6":"# 2. Missing Values o Categ\u00f3ricas (Nada)","76155c9f":"# 3. Crear los conjuntos\n# 3.1. Crear variables predictoras y variable respuesta\nX = datos['Nivel'].values.reshape(-1,1) # Si solo hay 1 variable queremos que sea MATRIZ\ny = datos['Salario'].values\n# 3.2. Utilizar el Separador\n'''\nEn este escenario de problema no nos interesa hacer una separaci\u00f3n\n'''","098cddac":"# 4. Escalar\n'''\nEl agoritmo SVR no es muy utilizado, y su librer\u00eda no implemente el escalado directamente\n\u00a1Es hora de hacerlo nosotros mismos!\n'''\nescaladorX = MinMaxScaler(feature_range=(0,1)) # Pondr\u00e1 todos los datos entre 0 y 1 -> NORMALIZAR\nescaladorY = MinMaxScaler(feature_range=(0,1))\nesc_X = escaladorX.fit_transform(X)\nesc_y = escaladorY.fit_transform(y.reshape(-1,1)) # Hay que convertir y a Matrix porque es lo que espera el objeto de la clase MinMaxScaler (nuestro 'escalador')","afdb511b":"# 5. Crear y ajustar el modelo\nfrom sklearn.svm import SVR\nregresor_svr = SVR(kernel='rbf')\nregresor_svr.fit(esc_X, esc_y)","41be24f7":"# 6. Hacer las predicciones\ny_fit = regresor_svr.predict(esc_X)\nx_pred = escaladorX.transform(np.array([[6.5]]))\ny_pred = regresor_svr.predict(x_pred)\n\n# Hay que desescalar de vuelta!\ny_fit = escaladorY.inverse_transform(y_fit.reshape(-1,1))\ny_pred = escaladorY.inverse_transform(y_pred.reshape(-1,1)) \n\nprint('El empleado deb\u00eda estar cobrando en torno a: ', y_pred)","2eeccdea":"# 7. Analizar los resultados\n\n# 7.1. Gr\u00e1fico\nplt.figure()\nplt.scatter(X, y, color='red')\nplt.plot(X, y_fit, color='blue')\nplt.scatter(6.5, y_pred, color='green')\nplt.title('Regresi\u00f3n SVR')\nplt.xlabel('Nivel')\nplt.ylabel('Salario (\u20ac)')\nplt.plot()\n\n# 7.2. Error - RMSE\nRMSE = np.sum(rmse(y, y_fit))\/len(y)\nprint('RMSE: ', RMSE)","35c1d001":"# 1. Importar los datos y resumen\ndatos = pd.read_csv('..\/input\/datoscsv\/4.1.Salarios2.csv')","53561ddf":"# 2. Missing Values o Categ\u00f3ricas (Nada)","a63c19af":"# 3. Crear los conjuntos\n# 3.1. Crear variables predictoras y variable respuesta\nX = datos['Nivel'].values.reshape(-1,1) # Si solo hay 1 variable queremos que sea MATRIZ\ny = datos['Salario'].values\n# 3.2. Utilizar el Separador\n'''\nEn este escenario de problema no nos interesa hacer una separaci\u00f3n\n'''","659b723b":"# 4. Escalar\n'''\nNo es necesario\n'''","aeaf8ff4":"# 5. Crear y ajustar el modelo\nfrom sklearn.tree import DecisionTreeRegressor as Arbol\narbol = Arbol(criterion='mse')\narbol.fit(X, y)","a79175dc":"# 6. Hacer las predicciones\ny_fit  = arbol.predict(X)\nx_pred = (np.array([[6.5]])) # Esto es porque predict espera un array (Matriz). Le pasamos as\u00ed una matrix de 1 fila y 1 columna (1 solo n\u00famero)\ny_pred = arbol.predict(x_pred)\n\nprint('El empleado deb\u00eda estar cobrando en torno a: ', y_pred)","d4ce3527":"# 7. Analizar los resultados\n\n# 7.1. Gr\u00e1fico\nplt.figure()\nplt.scatter(X, y, color='red')\nplt.plot(X, y_fit, color='blue')\nplt.scatter(x_pred, y_pred, color='green')\nplt.title('Regresi\u00f3n SVR')\nplt.xlabel('Nivel')\nplt.ylabel('Salario (\u20ac)')\nplt.plot()\n\n# 7.2. Error - RMSE\nRMSE = np.sum(rmse(y, y_fit))\/len(y)\nprint('RMSE: ', RMSE)","6bc6b2b0":"# 7.1.2. Gr\u00e1fico con m\u00e1s resoluci\u00f3n\nX_grid = np.arange(min(X), max(X), 0.001)\ny_grid = arbol.predict(X_grid.reshape(-1,1))\n\nplt.figure()\nplt.scatter(X, y, color='red')\nplt.plot(X_grid, y_grid, color='blue')\nplt.scatter(x_pred, y_pred, color='green')\nplt.title('Regresi\u00f3n SVR')\nplt.xlabel('Nivel')\nplt.ylabel('Salario (\u20ac)')\nplt.plot()","0c10db01":"# 1. Importar los datos y resumen\ndatos = pd.read_csv('..\/input\/datoscsv\/4.1.Salarios2.csv')","01296e4f":"# 2. Missing Values o Categ\u00f3ricas (Nada)","a18aa3a0":"# 3. Crear los conjuntos\n# 3.1. Crear variables predictoras y variable respuesta\nX = datos['Nivel'].values.reshape(-1,1) # Si solo hay 1 variable queremos que sea MATRIZ\ny = datos['Salario'].values\n# 3.2. Utilizar el Separador\n'''\nEn este escenario de problema no nos interesa hacer una separaci\u00f3n\n'''","0ece6ec8":"# 4. Escalar\n'''\nNo es necesario\n'''","0a515716":"# 5. Crear y ajustar el modelo\nfrom sklearn.ensemble import RandomForestRegressor as Bosque\nbosque = Bosque(n_estimators=10) \nbosque.fit(X,y)","efc3563c":"# 6. Hacer las predicciones\ny_fit  = bosque.predict(X)\nx_pred = (np.array([[6.5]])) # Esto es porque predict espera un array (Matriz). Le pasamos as\u00ed una matrix de 1 fila y 1 columna (1 solo n\u00famero)\ny_pred = bosque.predict(x_pred)\n\nprint('El empleado deb\u00eda estar cobrando en torno a: ', y_pred)","3b78ede6":"# 7. Analizar los resultados\n\n# 7.1.2. Gr\u00e1fico con m\u00e1s resoluci\u00f3n\nX_grid = np.arange(min(X), max(X), 0.001)\ny_grid = bosque.predict(X_grid.reshape(-1,1))\n\nplt.figure()\nplt.scatter(X, y, color='red')\nplt.plot(X_grid, y_grid, color='blue')\nplt.scatter(x_pred, y_pred, color='green')\nplt.title('Regresi\u00f3n SVR')\nplt.xlabel('Nivel')\nplt.ylabel('Salario (\u20ac)')\nplt.plot()\n\n# 7.2. Error - RMSE\nRMSE = np.sum(rmse(y, y_fit))\/len(y)\nprint('RMSE: ', RMSE)","48da24c9":"# 1. Importar los datos y resumen\ndatos = pd.read_csv('..\/input\/clasificacin\/4.2.Compras.csv')\ndatos.head(5)","5f540c12":"datos = datos[['Edad', 'Salario', 'Compra']] # Solo nos interesa esta informaci\u00f3n","95bf8e98":"# 2. Missing Values o Categ\u00f3ricas (Nada)","ba4a2256":"# 3. Crear los conjuntos\n# 3.1. Crear variables predictoras y variable respuesta\nX = datos[['Edad', 'Salario']].values\ny = datos['Compra'].values\n# 3.2. Utilizar el Separador\nX_train, X_test, y_train, y_test = Separar(X, y, test_size=0.25)\nprint('X_train: ', X_train.shape)\nprint('X_test: ', X_test.shape)\nprint('y_train: ', y_train.shape)\nprint('y_test: ', y_test.shape)","9560c693":"# 4. Escalar\nescalador = MinMaxScaler()\nsc_X_train = escalador.fit_transform(X_train)\nsc_X_test  = escalador.transform(X_test)","061d63aa":"# 5. Crear y ajustar el modelo\nfrom sklearn.linear_model import LogisticRegression as LR\nclasificador_log = LR()\nclasificador_log.fit(sc_X_train, y_train)","146070c8":"# 6. Hacer las predicciones\ny_pred = clasificador_log.predict(sc_X_test)","4b8afd0f":"print(X_train.shape)\nprint(sc_X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(sc_X_test.shape)\nprint(y_test.shape)","ac154c45":"# 7. Analizar los resultados\n\n# 7.1. Matrix de Confusi\u00f3n\ncm = CM(y_test, y_pred)\n\n# 7.2.1 Gr\u00e1fico de regiones y clasifiaci\u00f3n (Conjunto de entrenamiento)\nX_set, y_set = sc_X_train, y_train\n# 7.2.1.1. Creamos cuadr\u00edcula (grid) para hacer las regiones (tenemos que predecir todo el plano)\nplanoX, planoY = (np.arange(start=X_set[:,0].min()-1, stop=X_set[:,0].max()+1, step=0.01),\n                  np.arange(start=X_set[:,1].min()-1, stop=X_set[:,0].max()+1, step=0.01))\nX1, X2 = np.meshgrid(planoX, planoY)\n\nlimite = clasificador_log.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape)\nplt.contourf(X1, X2, limite, alpha = 0.35, cmap = Colors(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\n\n# 7.2.1.2. Pintamos las predicciones\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n               c = Colors(('red', 'green'))(i), label = j, s=10)\nplt.title('Clasificaci\u00f3n con Regresi\u00f3n Log\u00edstica (Conjunto de entrenamiento)')\nplt.xlabel('Edad')\nplt.ylabel('Salario')\nplt.show()\n\n# 7.2.2 Gr\u00e1fico de regiones y clasifiaci\u00f3n (Conjunto de validaci\u00f3n)\nX_set, y_set = sc_X_test, y_test\n# 7.2.2.1. Creamos cuadr\u00edcula (grid) para hacer las regiones (tenemos que predecir todo el plano)\nplanoX, planoY = (np.arange(start=X_set[:,0].min()-1, stop=X_set[:,0].max()+1, step=0.01),\n                  np.arange(start=X_set[:,1].min()-1, stop=X_set[:,0].max()+1, step=0.01))\nX1, X2 = np.meshgrid(planoX, planoY)\n\nlimite = clasificador_log.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape)\nplt.contourf(X1, X2, limite, alpha = 0.35, cmap = Colors(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\n\n# 7.2.2.2. Pintamos las predicciones\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n               c = Colors(('red', 'green'))(i), label = j, s=10)\nplt.title('Clasificaci\u00f3n con Regresi\u00f3n Log\u00edstica (Conjunto de entrenamiento)')\nplt.xlabel('Edad')\nplt.ylabel('Salario')\nplt.show()\n","33148f51":"# 1. Importar los datos y resumen\ndatos = pd.read_csv('..\/input\/clasificacin\/4.2.Compras.csv')","816edfea":"# 2. Missing Values o Categ\u00f3ricas (Nada)","6f56bd0a":"# 3. Crear los conjuntos\n# 3.1. Crear variables predictoras y variable respuesta\nX = datos[['Edad', 'Salario']].values\ny = datos['Compra'].values\n# 3.2. Utilizar el Separador\nX_train, X_test, y_train, y_test = Separar(X, y, test_size=0.25)\nprint('X_train: ', X_train.shape)\nprint('X_test: ', X_test.shape)\nprint('y_train: ', y_train.shape)\nprint('y_test: ', y_test.shape)","3d15e0c8":"# 4. Escalar\nescalador = MinMaxScaler()\nsc_X_train = escalador.fit_transform(X_train)\nsc_X_test  = escalador.transform(X_test)","36b64ac0":"# 5. Crear y ajustar el modelo\nfrom sklearn.svm import SVC\nclasificador_SVM = SVC(kernel='rbf')\nclasificador_SVM.fit(sc_X_train, y_train)","ea95dff9":"# 6. Hacer las predicciones\ny_pred = clasificador_SVM.predict(sc_X_test)","77af7ecb":"# 7. Analizar los resultados\n\n# 7.1. Matrix de Confusi\u00f3n\ncm = CM(y_test, y_pred)\n\n# 7.2.1 Gr\u00e1fico de regiones y clasifiaci\u00f3n (Conjunto de entrenamiento)\nX_set, y_set = sc_X_train, y_train\n# 7.2.1.1. Creamos cuadr\u00edcula (grid) para hacer las regiones (tenemos que predecir todo el plano)\nplanoX, planoY = (np.arange(start=X_set[:,0].min()-1, stop=X_set[:,0].max()+1, step=0.01),\n                  np.arange(start=X_set[:,1].min()-1, stop=X_set[:,0].max()+1, step=0.01))\nX1, X2 = np.meshgrid(planoX, planoY)\n\nlimite = clasificador_SVM.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape)\nplt.contourf(X1, X2, limite, alpha = 0.35, cmap = Colors(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\n\n# 7.2.1.2. Pintamos las predicciones\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n               c = Colors(('red', 'green'))(i), label = j, s=10)\nplt.title('Clasificaci\u00f3n con kNN (Conjunto de entrenamiento)')\nplt.xlabel('Edad')\nplt.ylabel('Salario')\nplt.show()\n\n# 7.2.2 Gr\u00e1fico de regiones y clasifiaci\u00f3n (Conjunto de validaci\u00f3n)\nX_set, y_set = sc_X_test, y_test\n# 7.2.2.1. Creamos cuadr\u00edcula (grid) para hacer las regiones (tenemos que predecir todo el plano)\nplanoX, planoY = (np.arange(start=X_set[:,0].min()-1, stop=X_set[:,0].max()+1, step=0.01),\n                  np.arange(start=X_set[:,1].min()-1, stop=X_set[:,0].max()+1, step=0.01))\nX1, X2 = np.meshgrid(planoX, planoY)\n\nlimite = clasificador_SVM.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape)\nplt.contourf(X1, X2, limite, alpha = 0.35, cmap = Colors(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\n\n# 7.2.2.2. Pintamos las predicciones\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n               c = Colors(('red', 'green'))(i), label = j, s=10)\nplt.title('Clasificaci\u00f3n con kNN (Conjunto de entrenamiento)')\nplt.xlabel('Edad')\nplt.ylabel('Salario')\nplt.show()","041fa363":"# 1. Importar los datos y resumen\ndatos = pd.read_csv('..\/input\/clasificacin\/4.2.Compras.csv')","7f15c924":"# 2. Missing Values o Categ\u00f3ricas (Nada)","5c96ba2a":"# 3. Crear los conjuntos\n# 3.1. Crear variables predictoras y variable respuesta\nX = datos[['Edad', 'Salario']].values\ny = datos['Compra'].values\n# 3.2. Utilizar el Separador\nX_train, X_test, y_train, y_test = Separar(X, y, test_size=0.25)","15c6728d":"# 4. Escalar\nescalador = MinMaxScaler()\nsc_X_train = escalador.fit_transform(X_train)\nsc_X_test  = escalador.transform(X_test)","0226cc8b":"# 5. Crear y ajustar el modelo\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nclasificador_knn = KNN()\nclasificador_knn2 = KNN()\nclasificador_knn.fit(X_train, y_train)\nclasificador_knn2.fit(sc_X_train, y_train)","7a5b2e8e":"# 6. Hacer las predicciones\ny_pred = clasificador_knn.predict(X_test)\ny_pred2 = clasificador_knn2.predict(sc_X_test)","5b5f9565":"y_pred == y_pred2","d4eaeaff":"# 7. Analizar los resultados\n\n# 7.1. Matrix de Confusi\u00f3n\ncm = CM(y_test, y_pred)\n\n# 7.2.1 Gr\u00e1fico de regiones y clasifiaci\u00f3n (Conjunto de entrenamiento)\nX_set, y_set = sc_X_train, y_train\n# 7.2.1.1. Creamos cuadr\u00edcula (grid) para hacer las regiones (tenemos que predecir todo el plano)\nplanoX, planoY = (np.arange(start=X_set[:,0].min()-1, stop=X_set[:,0].max()+1, step=0.01),\n                  np.arange(start=X_set[:,1].min()-1, stop=X_set[:,0].max()+1, step=0.01))\nX1, X2 = np.meshgrid(planoX, planoY)\n\nlimite = clasificador_knn2.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape)\nplt.contourf(X1, X2, limite, alpha = 0.35, cmap = Colors(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\n\n# 7.2.1.2. Pintamos las predicciones\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n               c = Colors(('red', 'green'))(i), label = j, s=10)\nplt.title('Clasificaci\u00f3n con SVM (Conjunto de entrenamiento)')\nplt.xlabel('Edad')\nplt.ylabel('Salario')\nplt.show()\n\n# 7.2.2 Gr\u00e1fico de regiones y clasifiaci\u00f3n (Conjunto de validaci\u00f3n)\nX_set, y_set = sc_X_test, y_test\n# 7.2.2.1. Creamos cuadr\u00edcula (grid) para hacer las regiones (tenemos que predecir todo el plano)\nplanoX, planoY = (np.arange(start=X_set[:,0].min()-1, stop=X_set[:,0].max()+1, step=0.01),\n                  np.arange(start=X_set[:,1].min()-1, stop=X_set[:,0].max()+1, step=0.01))\nX1, X2 = np.meshgrid(planoX, planoY)\n\nlimite = clasificador_knn2.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape)\nplt.contourf(X1, X2, limite, alpha = 0.35, cmap = Colors(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\n\n# 7.2.2.2. Pintamos las predicciones\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n               c = Colors(('red', 'green'))(i), label = j, s=10)\nplt.title('Clasificaci\u00f3n con SVM (Conjunto de entrenamiento)')\nplt.xlabel('Edad')\nplt.ylabel('Salario')\nplt.show()","7426b1cd":"# SIN ESCALAR\n# 7.2.2 Gr\u00e1fico de regiones y clasifiaci\u00f3n (Conjunto de validaci\u00f3n)\nX_set, y_set = X_test, y_test\n# 7.2.2.1. Creamos cuadr\u00edcula (grid) para hacer las regiones (tenemos que predecir todo el plano)\nplanoX, planoY = (np.arange(start=X_set[:,0].min()-1, stop=X_set[:,0].max()+1, step=(X_set[:,0].max()-X_set[:,0].min())\/100),\n                  np.arange(start=X_set[:,1].min()-1, stop=X_set[:,1].max()+1, step=(X_set[:,1].max()-X_set[:,1].min())\/100))\nX1, X2 = np.meshgrid(planoX, planoY)\n\nlimite = clasificador_knn.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape)\nplt.contourf(X1, X2, limite, alpha = 0.35, cmap = Colors(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\n\n# 7.2.2.2. Pintamos las predicciones\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n               c = Colors(('red', 'green'))(i), label = j, s=10)\nplt.title('Clasificaci\u00f3n con SVM (Conjunto de entrenamiento)')\nplt.xlabel('Edad')\nplt.ylabel('Salario')\nplt.show()","b45e4ad8":"# 1. Importar los datos y resumen\ndatos = pd.read_csv('..\/input\/clasificacin\/4.2.Compras.csv')","d3f546eb":"# 2. Missing Values o Categ\u00f3ricas (Nada)","bbf6411d":"# 3. Crear los conjuntos\n# 3.1. Crear variables predictoras y variable respuesta\nX = datos[['Edad', 'Salario']].values\ny = datos['Compra'].values\n# 3.2. Utilizar el Separador\nX_train, X_test, y_train, y_test = Separar(X, y, test_size=0.25)","d9bd099b":"# 4. Escalar\nescalador = MinMaxScaler()\nsc_X_train = escalador.fit_transform(X_train)\nsc_X_test  = escalador.transform(X_test)","aa4ef79f":"# 5. Construir y ajustar el modelo\nfrom sklearn.naive_bayes import GaussianNB as Bayesiano\nclasificador_bayes = Bayesiano()\nclasificador_bayes.fit(sc_X_train, y_train)","b26317c1":"# 6. Hacer las prediciones\ny_pred = clasificador_bayes.predict(sc_X_test)","4cdefa24":"# 7. Analizar los resultados\n\n# 7.1. Matrix de Confusi\u00f3n\ncm = CM(y_test, y_pred)\n\n# 7.2.1 Gr\u00e1fico de regiones y clasifiaci\u00f3n (Conjunto de entrenamiento)\nX_set, y_set = sc_X_train, y_train\n# 7.2.1.1. Creamos cuadr\u00edcula (grid) para hacer las regiones (tenemos que predecir todo el plano)\nplanoX, planoY = (np.arange(start=X_set[:,0].min()-1, stop=X_set[:,0].max()+1, step=0.01),\n                  np.arange(start=X_set[:,1].min()-1, stop=X_set[:,0].max()+1, step=0.01))\nX1, X2 = np.meshgrid(planoX, planoY)\n\nlimite = clasificador_bayes.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape)\nplt.contourf(X1, X2, limite, alpha = 0.35, cmap = Colors(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\n\n# 7.2.1.2. Pintamos las predicciones\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n               c = Colors(('red', 'green'))(i), label = j, s=10)\nplt.title('Clasificaci\u00f3n Naive Bayes (Conjunto de entrenamiento)')\nplt.xlabel('Edad')\nplt.ylabel('Salario')\nplt.show()\n\n# 7.2.2 Gr\u00e1fico de regiones y clasifiaci\u00f3n (Conjunto de validaci\u00f3n)\nX_set, y_set = sc_X_test, y_test\n# 7.2.2.1. Creamos cuadr\u00edcula (grid) para hacer las regiones (tenemos que predecir todo el plano)\nplanoX, planoY = (np.arange(start=X_set[:,0].min()-1, stop=X_set[:,0].max()+1, step=0.01),\n                  np.arange(start=X_set[:,1].min()-1, stop=X_set[:,0].max()+1, step=0.01))\nX1, X2 = np.meshgrid(planoX, planoY)\n\nlimite = clasificador_bayes.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape)\nplt.contourf(X1, X2, limite, alpha = 0.35, cmap = Colors(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\n\n# 7.2.2.2. Pintamos las predicciones\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n               c = Colors(('red', 'green'))(i), label = j, s=10)\nplt.title('Clasificaci\u00f3n Naive Bayes (Conjunto de validaci\u00f3n)')\nplt.xlabel('Edad')\nplt.ylabel('Salario')\nplt.show()","07213958":"# 1. Importar los datos y resumen\ndatos = pd.read_csv('..\/input\/clasificacin\/4.2.Compras.csv')","6b333265":"# 2. Missing Values o Categ\u00f3ricas (Nada)","ca69ce59":"# 3. Crear los conjuntos\n# 3.1. Crear variables predictoras y variable respuesta\nX = datos[['Edad', 'Salario']].values\ny = datos['Compra'].values\n# 3.2. Utilizar el Separador\nX_train, X_test, y_train, y_test = Separar(X, y, test_size=0.25)","1ee3253d":"# 4. Escalar\nescalador = MinMaxScaler()\nsc_X_train = escalador.fit_transform(X_train)\nsc_X_test  = escalador.transform(X_test)","e874704d":"# 5. Construir y ajustar el modelo\nfrom sklearn.tree import DecisionTreeClassifier as Arbol\narbol = Arbol()\narbol.fit(sc_X_train, y_train)","dab5acf0":"# 6. Hacer las prediciones\ny_pred = arbol.predict(sc_X_test)","57cfc9e7":"# 7. Analizar los resultados\n\n# 7.1. Matrix de Confusi\u00f3n\ncm = CM(y_test, y_pred)\n\n# 7.2.1 Gr\u00e1fico de regiones y clasifiaci\u00f3n (Conjunto de entrenamiento)\nX_set, y_set = sc_X_train, y_train\n# 7.2.1.1. Creamos cuadr\u00edcula (grid) para hacer las regiones (tenemos que predecir todo el plano)\nplanoX, planoY = (np.arange(start=X_set[:,0].min()-1, stop=X_set[:,0].max()+1, step=0.01),\n                  np.arange(start=X_set[:,1].min()-1, stop=X_set[:,0].max()+1, step=0.01))\nX1, X2 = np.meshgrid(planoX, planoY)\n\nlimite = arbol.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape)\nplt.contourf(X1, X2, limite, alpha = 0.35, cmap = Colors(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\n\n# 7.2.1.2. Pintamos las predicciones\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n               c = Colors(('red', 'green'))(i), label = j, s=10)\nplt.title('Clasificaci\u00f3n con \u00c1rbol de Decisi\u00f3n (Conjunto de entrenamiento)')\nplt.xlabel('Edad')\nplt.ylabel('Salario')\nplt.show()\n\n# 7.2.2 Gr\u00e1fico de regiones y clasifiaci\u00f3n (Conjunto de validaci\u00f3n)\nX_set, y_set = sc_X_test, y_test\n# 7.2.2.1. Creamos cuadr\u00edcula (grid) para hacer las regiones (tenemos que predecir todo el plano)\nplanoX, planoY = (np.arange(start=X_set[:,0].min()-1, stop=X_set[:,0].max()+1, step=0.01),\n                  np.arange(start=X_set[:,1].min()-1, stop=X_set[:,0].max()+1, step=0.01))\nX1, X2 = np.meshgrid(planoX, planoY)\n\nlimite = arbol.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape)\nplt.contourf(X1, X2, limite, alpha = 0.35, cmap = Colors(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\n\n# 7.2.2.2. Pintamos las predicciones\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n               c = Colors(('red', 'green'))(i), label = j, s=10)\nplt.title('Clasificaci\u00f3n con \u00c1rbol de Decisi\u00f3n (Conjunto de validaci\u00f3n)')\nplt.xlabel('Edad')\nplt.ylabel('Salario')\nplt.show()","6db95e04":"# 1. Importar los datos y resumen\ndatos = pd.read_csv('..\/input\/clasificacin\/4.2.Compras.csv')","8c8acfbd":"# 2. Missing Values o Categ\u00f3ricas (Nada)","6644fac4":"# 3. Crear los conjuntos\n# 3.1. Crear variables predictoras y variable respuesta\nX = datos[['Edad', 'Salario']].values\ny = datos['Compra'].values\n# 3.2. Utilizar el Separador\nX_train, X_test, y_train, y_test = Separar(X, y, test_size=0.25)","190ddb26":"# 4. Escalar\nescalador = MinMaxScaler()\nsc_X_train = escalador.fit_transform(X_train)\nsc_X_test  = escalador.transform(X_test)","2c590abc":"# 5. Construir y ajustar el modelo\nfrom sklearn.ensemble import RandomForestClassifier as Bosque\nbosque = Bosque()\nbosque.fit(sc_X_train, y_train)","8ca7485e":"# 6. Hacer las predicciones\ny_pred = bosque.predict(sc_X_test)","ebced741":"# 7. Analizar los resultados\n\n# 7.1. Matrix de Confusi\u00f3n\ncm = CM(y_test, y_pred)\n\n# 7.2.1 Gr\u00e1fico de regiones y clasifiaci\u00f3n (Conjunto de entrenamiento)\nX_set, y_set = sc_X_train, y_train\n# 7.2.1.1. Creamos cuadr\u00edcula (grid) para hacer las regiones (tenemos que predecir todo el plano)\nplanoX, planoY = (np.arange(start=X_set[:,0].min()-1, stop=X_set[:,0].max()+1, step=0.01),\n                  np.arange(start=X_set[:,1].min()-1, stop=X_set[:,0].max()+1, step=0.01))\nX1, X2 = np.meshgrid(planoX, planoY)\n\nlimite = bosque.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape)\nplt.contourf(X1, X2, limite, alpha = 0.35, cmap = Colors(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\n\n# 7.2.1.2. Pintamos las predicciones\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n               c = Colors(('red', 'green'))(i), label = j, s=10)\nplt.title('Clasificaci\u00f3n con \u00c1rbol de Decisi\u00f3n (Conjunto de entrenamiento)')\nplt.xlabel('Edad')\nplt.ylabel('Salario')\nplt.show()\n\n# 7.2.2 Gr\u00e1fico de regiones y clasifiaci\u00f3n (Conjunto de validaci\u00f3n)\nX_set, y_set = sc_X_test, y_test\n# 7.2.2.1. Creamos cuadr\u00edcula (grid) para hacer las regiones (tenemos que predecir todo el plano)\nplanoX, planoY = (np.arange(start=X_set[:,0].min()-1, stop=X_set[:,0].max()+1, step=0.01),\n                  np.arange(start=X_set[:,1].min()-1, stop=X_set[:,0].max()+1, step=0.01))\nX1, X2 = np.meshgrid(planoX, planoY)\n\nlimite = bosque.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape)\nplt.contourf(X1, X2, limite, alpha = 0.35, cmap = Colors(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\n\n# 7.2.2.2. Pintamos las predicciones\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n               c = Colors(('red', 'green'))(i), label = j, s=10)\nplt.title('Clasificaci\u00f3n con \u00c1rbol de Decisi\u00f3n (Conjunto de validaci\u00f3n)')\nplt.xlabel('Edad')\nplt.ylabel('Salario')\nplt.show()","8f060f2b":"# Machine Learning con Python\n---","e1b17fcd":"### Conclusi\u00f3n\nHemos visto como el \u00e1rbol de decisi\u00f3n ha ido realizando divisiones desde el nodo ra\u00edz generenado ramas para los diferentes valores que pueden tomar las variables independientes (edad y salario).\nLas salidas posibles de los nodos hoja son 0 o 1, es decir si comprar\u00e1 el coche o no, rojo o verde.","cab989df":"### Conclusi\u00f3n\n---\nLos SVR establecen utilizan una funci\u00f3n de transformaci\u00f3n para llevar los datos desde un espacion inicial a un nuevo espacio donde estos pueden ser m\u00e1s f\u00e1cilmente separables.","f3386316":"## Regresi\u00f3n Lineal M\u00faltiple\n---\nLa regresi\u00f3n linear utiliza el m\u00e9todo de m\u00ednimos cuadrados para encontrar la recta que resulta en la menor suma de errores al cuadrado (RMSE: Root Mean Square Error).\nLa palabra m\u00faltiple se refiere a que la variable respuesta depender\u00e1 de m\u00e1s de 1 variable independiente: Y = f(X1,...,Xn)\n\n### Escenario del Problema\n---\nQueremos encontrar cual es la manera de conseguir el mejor beneficio para nuestra empresa, en funci\u00f3n de en qu\u00e9 secciones decidamos invertir, as\u00ed como de la localizaci\u00f3n en la que la tengamos.","0585b32c":"X siempre queremos que sea una __matriz__, por tanto para los casos en los que es solo una variable, tenemos que aplicar un reshape para decirle a Python que esto no es un vector, sino que es una matrix de 1 sola columna","d5637bfd":"# Regresi\u00f3n\n---","e0e304c7":"**Nota** Hab\u00eda olvidado mencionar lo **importante que es escalar en estos gr\u00e1ficos**. Es una cuesti\u00f3n de interpretabilidad. Para que lo pod\u00e1is comprobar vosotros mismos, aqu\u00ed ten\u00e9is el mismo gr\u00e1fico si no escal\u00e1semos las variables (tener en cuenta que tienen **dimensiones muy dispares**# 7.2.2 Gr\u00e1fico de regiones y clasifiaci\u00f3n (Conjunto de validaci\u00f3n)","4f788171":"Tenemos una **variable categ\u00f3rica**. Habr\u00e1 que codificarla, para ello haremos uso de LabelEnconder, que dara un n\u00famero (0,1 o 2) para cada uno de las tres ciudades (Nueva York, California, Florida). Despu\u00e9s haremos uso de OneHotEnconder y eliminaremos 1 de las columnas para evitar la **Dummy Variable Trap**","e4e2deca":"## SVR\n---\nLos SVR utilizan una funci\u00f3n (denominada funci\u00f3n de N\u00facleo o kernel) para mapear los puntos del conjunto de datos disponibles a otra dimensi\u00f3n donde estos sean linealmente separables.\nSi imaginamos un espacion en 2D con puntos de 2 clases distintas, la funci\u00f3n de n\u00facleo podr\u00eda levantar a estos puntos a un espacio en 3D donde estos puntos de distintas clases podr\u00edan separarse por un plano\n\n### Escenario del problema\nVamos a contratar un nuevo empleado. Nos ha dicho que en su anterior empresa fue Manager Regional durante 2 a\u00f1os y que cobraba 170.000\u20ac al a\u00f1o. Queremos determinar hasta que punto nos dice la verdad para poder negociar con \u00e9l el salario que queremos ofrecerle en su nuevo puesto.","71d59579":"### Importar librer\u00edas generales\nEstas librer\u00edas son comunes y las utilizaremos en la gran mayor\u00eda de los algoritmos","eea73e7a":"Vaya, parece m\u00e1s de lo que nos hab\u00eda dicho! Vamos a comprobar que nuestro modelo lo hace bien.**","024b7398":"## Preprocesamiento de los datos - General\n---\nExiste un flujo que es com\u00fan para la mayor\u00eda de los algoritmos de Machine Learning   \n\n1 - Importar los datos y resumen   \n2 - Establecer una estrategia para los datos que faltan  (missing values)  y las variables categ\u00f3ricas  \n3 - Dividir el conjunto en entrenamiento (Train) y validaci\u00f3n (Test)  \n4 - Escalar (la mayor\u00eda de algoritmos de las librer\u00edas ya incluyen el escalado)  \n5 - Construir y ajustar (fit) el modelo que se vaya a usar al conjunto de entrenamiento  \n6 - Utilizar el modelo con nueva informaci\u00f3n del conjunto de validaci\u00f3n  \n7 - Validar el comportamiento (accuracy) que tiene el modelo  -> RMSE + Gr\u00e1fico  \n8 - Redefinir el modelo para conseguir mejores predicciones  \n9 - Guardar y desplegar el modelo en producci\u00f3n  ","ab335529":"Vaya, parece menos de lo que nos hab\u00eda dicho! Vamos a comprobar que nuestro modelo lo hace bien","e6d14934":"### Conclusi\u00f3n\nHemos visto como la regresi\u00f3n log\u00edstica establece una separaci\u00f3n lineal en funci\u00f3n de las probabilidades que tiene cada punto de pertenecer a una clase o a la otra.\nLa l\u00ednea de separaci\u00f3n corresponde por tanto a aquellos puntos donde la probabilidad de pertenecer a cualquiera de las dos clases es del 50%.","7aa9c3fb":"### Conclusi\u00f3n \nAtenci\u00f3n con el sobre-ajuste (overfitting). Puede que no nos interese querer aprozimar tan bien, ya que puede que tengamos ruido en los datos de entrenamiento y estamos creando zonas rojas que, lo m\u00e1s probable, es que cuando vengan datos del conjunto de validaci\u00f3n realmente deber\u00edan ser verdes.","6cf1c35e":"Removemos la primera columna (podr\u00eda ser cualquiera) para evitar la dummy trapX","8c9ca11e":"### Conclusi\u00f3n\n...","eb60877e":"## Regresi\u00f3n Log\u00edstica\n---\nLa regresi\u00f3n log\u00edsitica es un modelo que puede predecir la probabilidad que tiene una variable binaria (que puede aceptar 2 valores) de pertenecer a una clase o a otra.\nEs por tanto un m\u00e9todo utilizado para la clasificaci\u00f3n categ\u00f3rica de variables, especialmente \u00fatil por su simplicidad e interpretabilidad\n\n### Escenario del problema\nUna empresa de coches ha sacado un nuevo modelo al mercado. Le ha preguntado a una red social que quienes de sus usuarios han comprado el coche y cual es la edad y el salario de dichos usarios, para determinar el perfil de qui\u00e9n compra y qui\u00e9n no y actuar en consecuencia.\n","c997d68b":"## Naive Bayes - Clasificadores Bayesianos\nLos naive Bayes asumen que la presencia de una variable particular en una clase no est\u00e1 relacionada con la presencia de ninguna otra variable.\nClasifican a la nueva informaci\u00f3n basandonse en la probabilidad de pertenecer a cada una de ellas\n\n### Escenario del problema\nUna empresa de coches ha sacado un nuevo modelo al mercado. Le ha preguntado a una red social que quienes de sus usuarios han comprado el coche y cual es la edad y el salario de dichos usarios, para determinar el perfil de qui\u00e9n compra y qui\u00e9n no y actuar en consecuencia.","a5dd24a5":"## \u00c1rboles de Decisi\u00f3n\n---\nLos \u00e1rboles de decisi\u00f3n para la clasificaci\u00f3n funcionan igual que los vistos para regresi\u00f3n.\nLa \u00fanica diferencia es que el contenido de los nodos hojas son categor\u00edas en lugar de valores discretos\n\n### Escenario del problema\nUna empresa de coches ha sacado un nuevo modelo al mercado. Le ha preguntado a una red social que quienes de sus usuarios han comprado el coche y cual es la edad y el salario de dichos usarios, para determinar el perfil de qui\u00e9n compra y qui\u00e9n no y actuar en consecuencia.","fc720ac7":"Eliminamos la columna posici\u00f3n porque nos dice lo mismo que la columna nivel. Vemos de antemano que no hay una relaci\u00f3n lineal claramente.","218b4bcf":"## Bosques Aleatorios\nAl igual que en clasificaci\u00f3n, los bosques aleatorios consiten en n \u00e1rboles de decisi\u00f3n que promedian sus decisiones para 1 entrada, ofreciendo finalmente una \u00fanica categor\u00eda como respuesta\n\n### Escenario del problema\nUna empresa de coches ha sacado un nuevo modelo al mercado. Le ha preguntado a una red social que quienes de sus usuarios han comprado el coche y cual es la edad y el salario de dichos usarios, para determinar el perfil de qui\u00e9n compra y qui\u00e9n no y actuar en consecuencia.","b814eea1":"## Bosques Aleatorios\n---\nLos bosques aleatorios consisten simplemente en la generaci\u00f3n de un n\u00famero de \u00e1rboles de decisi\u00f3n cuyos nodos adoptan condiciones diferentes para decidir sobre el mismo conjunto de datos de entrada.\nEl resultado del bosque aleatorio ser\u00e1 el promedio del resultado de los n \u00e1rboles que conformen el bosque.\n\n### Escenario del problema\nVamos a contratar un nuevo empleado. Nos ha dicho que en su anterior empresa fue Manager Regional durante 2 a\u00f1os y que cobraba 170.000\u20ac al a\u00f1o. Queremos determinar hasta que punto nos dice la verdad para poder negociar con \u00e9l el salario que queremos ofrecerle en su nuevo puesto.","37713e89":"## \u00c1rboles de Decisi\u00f3n\n---\nLos \u00e1rboles de decisi\u00f3n son una de las t\u00e9cnicas m\u00e1s empleadas para el aprendizaje inductivo, siendo un m\u00e9todo bastante robusto frente a datos ruidosos. En esencia como todos los regresores, son una funci\u00f3n que dar\u00e1 salida a unas entradas. La representaci\u00f3n de esta funci\u00f3n toma dorma de \u00e1rbol y es interpretada como una serie de condiciones consecutivas que pueden ser f\u00e1cilmente mapeadas a reglas.\nExiten diferentes algoritmos para la creaci\u00f3n de los \u00e1rboles, en funci\u00f3n del m\u00e9todo de selecci\u00f3n de atributos. Los m\u00e9todos de seleci\u00f3n de atributos m\u00e1s comunes son: el \u00cdndice de Gini, la Ganancia de Informaci\u00f3n y la Proporci\u00f3n de Ganancia.\nEl resultado final ser\u00e1 un \u00e1rbol que empieza en un nodo ra\u00edz y termina en los nodos hojas. Los caminos desde el nodo ra\u00edz a cada uno de los nodos hojas son las ramas, que no son m\u00e1s que secuencias de reglas encadenadas. El resultado de haber llegado a un nodo hoja implica que se han cumplido una por una todas las condiciones en cada una de las ramificaciones de cada nodo intermedio.\n\n### Escenario del problema\nVamos a contratar un nuevo empleado. Nos ha dicho que en su anterior empresa fue Manager Regional durante 2 a\u00f1os y que cobraba 170.000\u20ac al a\u00f1o. Queremos determinar hasta que punto nos dice la verdad para poder negociar con \u00e9l el salario que queremos ofrecerle en su nuevo puesto.","016ba145":"Parece que no le faltan ning\u00fan dato (30 non-null en ambas columnas que tienen 30 datos cada una)","a236f45c":"Machine Learning es un conjunto de t\u00e9cnicas que permiten a los ordenadores realizar tareas de alta complejidas, propias de los seres humanos.  \nEsta forma de aprender se basa en el paradigma 'Aprender de la experiencia'.\nLos algoritmos de ML reciben datos, los cuales conforman esta experiencia; para identificar patrones en ellos, los cuales conforman el aprender.  \n\n**\u00bfC\u00f3mo generamos la experiencia y c\u00f3mo sabemos que aprende?**  \nVer\u00e9is que en los problemas de Machine Learning que entran dentro del aprendizaje supervisado (casi todos) el conjunto de datos del que se dispone se dividar\u00e1 en dos bloques:  \nEl conjunto de entrenamiento (train set): es la mayor\u00eda de los datos (en torno a un 80%) y son los datos que se utilizar\u00e1n para que nuestros modelos (regresores o clasificadores) aprendan a entender las relaciones entre nuestros datos.   \nEl cojunto de validaci\u00f3n (test set): es el conjunto restante. Este conjunto se utilizar\u00e1 para comprobar que con el entramiento realizado la m\u00e1quina es capaz de generar buenas conclusiones con informaci\u00f3n que no ha visto antes","81d30417":"## Regresi\u00f3n polin\u00f3mica\n---\nLa regresi\u00f3n polin\u00f3mica utiliza el m\u00e9todo de m\u00ednimos cuadrados para encontrar la curva polin\u00f3mica que resulta en la menor suma de errores al cuadrado (RMSE: Root Mean Square Error).\nLa forma de una regresi\u00f3n polin\u00f3mica tendr\u00e1 la forma: Y = C + aX + bX^2 + \u00b7\u00b7\u00b7  \n\n### Escenario del problema\n---\nVamos a contratar un nuevo empleado. Nos ha dicho que en su anterior empresa fue Manager Regional durante 2 a\u00f1os y que cobraba 170.000\u20ac al a\u00f1o. Queremos determinar hasta que punto nos dice la verdad para poder negociar con \u00e9l el salario que queremos ofrecerle en su nuevo puesto.","6e40e2b9":"## kNN - k Vecinos m\u00e1s cercanos\n---\nEl algortimo kNN categoriza los puntos en funci\u00f3n de la clase a la que pertenecen sus vecinos m\u00e1s cercanos.\nToma una distancia (lineal por ejemplo) a un conjunto de vecinos en un radio delimitado, y compara cual es la clase predominante para convertirse en uno m\u00e1s de esa clase\n\n### Escenario del problema\nUna empresa de coches ha sacado un nuevo modelo al mercado. Le ha preguntado a una red social que quienes de sus usuarios han comprado el coche y cual es la edad y el salario de dichos usarios, para determinar el perfil de qui\u00e9n compra y qui\u00e9n no y actuar en consecuencia.","d7235c34":"## Algoritmos de Machine Learning - Aprendizaje Supervisado\n---\n### Regresi\u00f3n\nRegresi\u00f3n Lineal Simple  \nRegresi\u00f3n Lineal M\u00faltiple  \nRegresi\u00f3n Polin\u00f3mica  \nSVR. \n\u00c1rboles de Decisi\u00f3n  \nBosques Aleatorios  \n\n### Clasificaci\u00f3n\nRegresi\u00f3n Log\u00edstica  \nSVM. \nkNN  \nNaive Bayes  \n\u00c1rboles de Decisi\u00f3n. \nBosques aleatorios. ","f4fd92bc":"\u00a1Esto tiene ya m\u00e1s pinta de haber el \u00e1rbol tomado decisiones de establecer l\u00edmites!","5d0176ae":"## SVM - Support Vector Machines\n---\nLos SVM utilizan la misma t\u00e9cnica de funci\u00f3n de mapeo para llevar los datos a un espacio de mayor dimensi\u00f3n donde los datos sean linealmente separables por un plano, que divida nuestros datos por categor\u00edas.\n\n### Escenario del problema\nUna empresa de coches ha sacado un nuevo modelo al mercado. Le ha preguntado a una red social que quienes de sus usuarios han comprado el coche y cual es la edad y el salario de dichos usarios, para determinar el perfil de qui\u00e9n compra y qui\u00e9n no y actuar en consecuencia.","a84f1226":"### Conclusion\nLos SVM mapean los datos a un espacio dimensional diferente para aplicar una separaci\u00f3n lineal en el nuevo espacio.\nLa l\u00ednea de separaci\u00f3n corresponde por tanto al proceso de invertir la funci\u00f3n de mapeo al plano generado en el espacio de mayor dimensiones de vuelta a nuestro espacio inicial.","58b3c96f":"Vemos como cada uno de los grados del polinomio de la variable X pasa a ser 1 nueva variable independiente","2d8cf363":"### Conclusi\u00f3n\nLos \u00e1rboles de decisi\u00f3n establecen divisiones entre el conjunto de datos como resultado de hacer divisiones en los nodos a cada nivel de profundidad del \u00e1rbol.\nDe esta manera cuando solicitamos por un valor, ir\u00e1 desviandose en cada desviaci\u00f3n hasta terminar la rama desde el nodo ra\u00edz hasta el nodo hoja en funci\u00f3n de las condiciones en cada nodo. Nos devolver\u00e1 el resultado del nodo hoja.\nPara el problema del sobreajuste, especificar par\u00e1metros en la construcci\u00f3n del \u00e1rbol o aplicar mecanismos de poda\n","8faa14cb":"Vaya nos estamos hacercando ya mucho!","586734c1":"### Conclusi\u00f3n\nLa RLM establece una relaci\u00f3n lineal entre todas las variables independientes (predictores) y la variable dependiente (respuesta) en funci\u00f3n de los datos disponibles en el conjunto de entrenamiento, y como aplica esta funci\u00f3n para obtener nuevas prediciones con la nueva informaci\u00f3n disponible en el conjunto de validaci\u00f3n.","83774052":"### Conclusi\u00f3n\nLa RP establece una relaci\u00f3n polin\u00f3mica entre la variable independiente (predictor) y la variable dependiente (respuesta) en funci\u00f3n de los datos disponibles en el conjunto de entrenamiento, y como aplica esta funci\u00f3n para obtener nuevas prediciones con la nueva informaci\u00f3n disponible en el conjunto de validaci\u00f3n.\nHemos visto como en ciertas ocasiones la relaciones entre dichas variables no es lineal, y por lo tanto no podemos confiar en un regresor lineal, y podremos encontrar un regresor polin\u00f3mico que se ajuste a los datos mucho mejor. \u00a1Pero mucho cuidado con el sobre ajuste!","f64a36da":"# Clasificaci\u00f3n\n---","4a9c36a4":"### Conclusi\u00f3n\nLa RLS establece una relaci\u00f3n lineal entre la variable independiente (predictor) y la variable dependiente (respuesta) en funci\u00f3n de los datos disponibles en el conjunto de entrenamiento, y como aplica esta funci\u00f3n para obtener nuevas prediciones con la nueva informaci\u00f3n disponible en el conjunto de validaci\u00f3n.","e79cdc4e":"**\u00a1Cuidado, esto es una trampa!**  \n\u00bfQu\u00e9 es lo que est\u00e1 pasando aqu\u00ed? \u00bfCre\u00e9is que estamos prediciendo perfecto lo que pasa? Est\u00e1 claro que no.  \n\nLo que pasa es que si estamos en 1 dimensi\u00f3n unicamente (Salario depende del Nivel \u00fanicamente) el \u00e1rbol de decisi\u00f3n est\u00e1 haciendo una divisi\u00f3n por cada punto y esta tomando la media que hay entre cada valor. Pero entonces, \u00bfpor qu\u00e9 vemos esto y no vemos una escalera, con un escal\u00f3n por cada divisi\u00f3n? Eso ser\u00eda lo l\u00f3gico, puesto que tomar\u00e1 el valor medio para todos el rango x que componga ese escal\u00f3n.  \n\nLa clave est\u00e1 en la resoluci\u00f3n de nuestro gr\u00e1fico. Nos estamos enfrente a un nuevo modelo de regresi\u00f3n, que es la discont\u00ednua. Para poder ver esta discontinuidad tenemos entonces que mejorar la resoluci\u00f3n.","4e44b273":"### Conclusi\u00f3n\nAntes hemos visto en el tema anterior como los \u00e1rboles de decisi\u00f3n os devolv\u00edan el resultado del nodo hoja resultante de cumplirse todas las condiciones en los nodos rama hasta \u00e9l. El bosque est\u00e1 creando n \u00e1rboles como este y promedia el resultado de los n nodos hoja resultantes.\nTiene sentido adem\u00e1s que no prediga de buena forma el \u00faltimo de los valores correspondiente al CEO ya que como vemos se sale del patr\u00f3n.","3a59a364":"## Regresi\u00f3n Lineal Simple\n---\nLa regresi\u00f3n linear utiliza el m\u00e9todo de m\u00ednimos cuadrados para encontrar la recta que resulta en la menor suma de errores al cuadrado (RMSE: Root Mean Square Error).\nLa palabra simple se refiere a que la variable respuesta solo depende de 1 variable independiente: Y = f(X)\n\n### Escenario del problema\nQueremos encontrar la relaci\u00f3n que existe entre los a\u00f1os de experiencia profesional y el salario que podemos esperar tener cuando lo hayamos conseguido.[](http:\/\/)","60d8b67f":"### Conclusi\u00f3n\nLos kNN buscan, en funci\u00f3n de las condiciones que establezcamos, los vecinos m\u00e1s pr\u00f3ximos a cada uno de los puntos de los que se quiere determinar la clase.\nEn funci\u00f3n de la clase de los vecinos cada uno de los puntos adoptar\u00e1 una clase."}}