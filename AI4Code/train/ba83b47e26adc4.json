{"cell_type":{"4186d197":"code","7e237346":"code","268f1b8f":"code","cddd7ecb":"code","8267e5f1":"code","5aabd2c5":"code","024b5adf":"code","3481e4af":"code","320bdfa9":"code","eb8ad281":"code","076eea57":"code","887a3a30":"code","ba609604":"code","bd256d9c":"code","47076a53":"code","ff32978a":"code","faada32f":"code","f6352850":"code","9aaa71b4":"code","dd92e7d2":"code","c95ed3b6":"code","fd8a4eec":"code","b38dbfd9":"code","c6f4a73e":"code","bd61a172":"code","d0326b6e":"code","0cd13105":"code","b283b4a7":"code","a69e420d":"code","2d747ea2":"code","15b3b029":"code","91085ae5":"code","2d0c42a7":"code","59666a87":"code","b6df1f6b":"code","47f63d7e":"code","d45aad6c":"code","a27c5205":"code","6c4eba71":"code","a31b5cc2":"code","4792e34a":"code","b6978236":"code","a5c5a62b":"code","afe36678":"code","36d28ac7":"code","90122468":"code","ae549de6":"markdown","494713af":"markdown","99895c41":"markdown","995a5e3e":"markdown","9021ee87":"markdown","5a164b23":"markdown","97578240":"markdown","b5c6014f":"markdown","fcb071ba":"markdown","e0a8db54":"markdown","771698d5":"markdown","e7e55029":"markdown","1153f607":"markdown","3cfcc5b6":"markdown","b992364f":"markdown","80530ba4":"markdown","8ae1222f":"markdown","9f6faa98":"markdown","4b72ca1d":"markdown","5b4cd444":"markdown","a086da9b":"markdown","8604e4f1":"markdown","6903cd06":"markdown","00a9c63e":"markdown","5bd5a43c":"markdown","5c32e3e7":"markdown","ae5e741c":"markdown","577b21ab":"markdown","1e15bbe6":"markdown","645e39b3":"markdown","d2a94451":"markdown","5d55fe8b":"markdown","fdc70728":"markdown","e311c80a":"markdown","d79edc5d":"markdown","ba1f4d75":"markdown","bc0d4f17":"markdown","59c600b0":"markdown","236ebf81":"markdown","51d0efd8":"markdown","189a1f9d":"markdown","e39c5b64":"markdown","64abca86":"markdown","ed23fab6":"markdown","1b886f68":"markdown","0f993134":"markdown","d42de1a8":"markdown","96af1a5f":"markdown","62ffae94":"markdown","f209b2a1":"markdown","27a927c0":"markdown","fe89154c":"markdown","247762d0":"markdown","52aa857e":"markdown","4329a235":"markdown","191867b8":"markdown","b4f31966":"markdown","45b9096c":"markdown","d6243a91":"markdown","85a6f3e7":"markdown","b644ee59":"markdown","65139967":"markdown","d2de4a32":"markdown","b11830b0":"markdown","9ffe6280":"markdown","90027299":"markdown","10104964":"markdown","ff04c35e":"markdown","11079c1a":"markdown","8e9cc286":"markdown","cc1f950e":"markdown","970d62b4":"markdown","51543b46":"markdown","a51af524":"markdown","389f3113":"markdown","79848457":"markdown","405b7494":"markdown","1c48230a":"markdown","f149040a":"markdown","75ba8c35":"markdown","af666595":"markdown","0729ef2d":"markdown","a1f822c6":"markdown","b8b4223b":"markdown","c50b871d":"markdown","e59bd96b":"markdown","9801edb9":"markdown","e0f23a7d":"markdown","8895f1dc":"markdown","bdbe1ffa":"markdown","ed52fa32":"markdown","be96b94c":"markdown","7e01c8f8":"markdown","07d49910":"markdown","248a2305":"markdown"},"source":{"4186d197":"import matplotlib.pyplot as plt\n\nplt.plot([1,2,3])","7e237346":"# Visualization packages\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.offline as pyo\nimport plotly.graph_objs as go\n# Set notebook mode to work in offline\npyo.init_notebook_mode()\n\n# Data packages\nimport pandas as pd\nimport numpy as np\n\n# Machine learning packages\nfrom sklearn.feature_selection import mutual_info_classif, f_classif, chi2, SelectKBest\nfrom sklearn.model_selection import ParameterGrid, GridSearchCV, train_test_split\nfrom sklearn.utils import shuffle\nfrom sklearn.decomposition import PCA, NMF, FactorAnalysis\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.metrics import (classification_report, confusion_matrix, \n                             plot_confusion_matrix, roc_curve, auc,\n                             roc_auc_score, f1_score)\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom catboost import EFstrType\n\n# Deep learning packages\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import *\nfrom tensorflow import keras\nimport tensorflow_probability as tfp\nfrom tensorflow.keras import Sequential\n\ntfpl = tfp.layers\ntfd = tfp.distributions\n\n# Optimization package\nfrom skopt.space import Real, Integer, Categorical\nfrom skopt.utils import use_named_args\nfrom skopt import gp_minimize\nfrom skopt.plots import plot_objective\n\n# Interpretability packages\nimport shap\nshap.initjs()\n\n# Other packages\nimport random\nfrom time import time\nimport os\nimport sys\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")","268f1b8f":"CLASS_LABELS = ['NORMAL', 'BACTERIA', 'VIRUS'] # we define our class labels\n\ndef process_path(nb_class):\n    \n    \"\"\"\n    As the function map from Dataset object is waiting a function, \n    we created process path to set the number of class desired. Then you can set nb_class to 2 \n    if you want to analyse only PNEUMONIA\/NORMAL set.\n    \n    parameters:\n    -----------\n        nb_class: int\n        \n    returns:\n    --------\n        f: Function used in map.\n    \"\"\"\n    \n    def f(file_path):\n        \n        \"\"\"\n        This function takes the files path as arguement and apply some preprocessing functions.\n        It also create labels according to name's file. Then NORMAL images become 0, BACTERA is 1 and VIRUS 2.\n\n        parameters:\n        -----------\n            file_path: str\n\n        returns:\n        --------\n            image, label: tensor, tensor.\n        \"\"\"\n        \n        label = 0 if tf.strings.split(file_path, os.path.sep)[-2]=='NORMAL' else 1\n\n        if label == 1 and nb_class == 3:\n            label = 1 if tf.strings.split(file_path, '_')[-2]=='bacteria' else 2\n            \n        image = tf.io.read_file(file_path)    \n        image = tf.image.decode_jpeg(image, channels=1)\n        image = tf.image.convert_image_dtype(image, tf.float32)\n         \n        # We resize images to (127, 127) because the smallest image in the dataset is 127.\n        image = tf.image.resize(image, [127, 127], method='area')\n        return image, label\n    \n    return f\n\ndef reader_image(path_file, batch_size, nb_class):\n    \n    \"\"\"\n    This function is containing the two previous one then that' why we arge getting the same parameters.\n\n    parameters:\n    -----------\n        file_path: str\n        batch_size: int\n        nb_class: int\n\n    returns:\n    --------\n        dataset object.\n    \"\"\"\n\n    list_ds = tf.data.Dataset.list_files(path_file)\n    labeled_ds = list_ds.map(process_path(nb_class))\n    \n    return labeled_ds.shuffle(100).batch(batch_size).prefetch(1)\n\ntrain_ds = reader_image('\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/train\/*\/*.jpeg', 16, 3)\ntest_ds = reader_image('\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/test\/*\/*.jpeg', 16, 3)\n\n# We display an example\nfor image, label in train_ds.take(1):\n    df = pd.DataFrame(image[0, :, :, 0].numpy())\n    \nprint(f'Outoupt : \\n image shape: {df.shape} and maximum\/minimum values: {round(df.max().max()), round(df.min().min())}')","cddd7ecb":"from sklearn.model_selection import train_test_split\n\ndef get_numpy(ds):\n    \n    \"\"\"\n    Function used to translate Dataset object into numpy array.\n    \n    parameters:\n    -----------\n        ds: Dataset\n        \n    returns:\n    --------\n        X: numpy array.\n        y: numpy array.\n    \"\"\"\n    \n    X, y = [], []\n    for images, labels in ds: \n        X.append(images)\n        y.append(labels)\n        \n    X = np.concatenate(X, axis=0)\n    X = X.reshape(X.shape[0], -1)\n    y = np.concatenate(y, axis=0)\n    \n    return X, y\n\nX, y = get_numpy(train_ds)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0)\nX_test, y_test = get_numpy(test_ds)","8267e5f1":"plt.figure(figsize=(20, 10))\n\nfor images, labels in train_ds.take(1):\n    for i in range(6):\n        ax = plt.subplot(3, 3, i+1)\n        plt.imshow(images[i, :, :, 0].numpy())\n        plt.title(f'Label: {CLASS_LABELS[labels[i].numpy()]}')\n        plt.axis(\"off\")","5aabd2c5":"data_train = pd.Series(y_train).value_counts() # Count the number of each class in train set\n\nnormal_train = data_train[0]\nbacteria_train = data_train[1]\nvirus_train = data_train[2]\ntotal_train = normal_train + bacteria_train + virus_train\n\ndata_valid = pd.Series(y_valid).value_counts() # Count the number of each class in validation set\n\nnormal_valid = data_valid[0]\nbacteria_valid = data_valid[1]\nvirus_valid = data_valid[2]\ntotal_valid = normal_valid + bacteria_valid + virus_valid\n\n# Trace the plot\nfig = make_subplots(\n    rows=1, cols=2,\n    specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"}]], column_titles=['Train proportions', 'Validation proportions'])\n\ncolors = ['gold', 'mediumturquoise', 'darkorange', 'lightgreen']\n\nfig.add_trace(go.Pie(labels=[\"Normal\", \"Bacteria\", \"Virus\"], values=[normal_train, bacteria_train, virus_train]), row=1, col=1)\nfig.add_trace(go.Pie(labels=[\"Normal\", \"Bacteria\", \"Virus\"], values=[normal_valid, bacteria_valid, virus_valid]), row=1, col=2)\n\nfig.update_traces(hoverinfo='label+percent', textinfo='value', textfont_size=20,\n                  marker=dict(colors=colors, line=dict(color='#000000', width=2)))\n\nfig.update(layout_title_text=f'Class proportions')\n\npyo.iplot(fig, filename='class proportion')","024b5adf":"# Definition of train values\nnormal_hist_train = pd.DataFrame(X_train[y_train==0][np.random.randint(sum(y_train==0), size=25)].flatten())\nbacteria_hist_train = pd.DataFrame(X_train[y_train==1][np.random.randint(sum(y_train==1), size=25)].flatten())\nvirus_hist_train = pd.DataFrame(X_train[y_train==2][np.random.randint(sum(y_train==2), size=25)].flatten())\n\n# Definition of valid values\nnormal_hist_valid = pd.DataFrame(X_valid[y_valid==0][np.random.randint(sum(y_valid==0), size=25)].flatten())\nbacteria_hist_valid = pd.DataFrame(X_valid[y_valid==1][np.random.randint(sum(y_valid==1), size=25)].flatten())\nvirus_hist_valid = pd.DataFrame(X_valid[y_valid==2][np.random.randint(sum(y_valid==2), size=25)].flatten())\n\n# Definition of train description dataframe\ndescr_normal_hist_train = normal_hist_train.describe()\ndescr_bacteria_hist_train = bacteria_hist_train.describe()\ndescr_virus_hist_train = virus_hist_train.describe()\n\n# Definition of valid description dataframe\ndescr_normal_hist_valid = normal_hist_valid.describe()\ndescr_bacteria_hist_valid = bacteria_hist_valid.describe()\ndescr_virus_hist_valid = virus_hist_valid.describe()\n\n# Definition of values to enter in table plotly\nvalues_train = [descr_normal_hist_train.index, #1st col\n          descr_normal_hist_train.round(3), \n          descr_bacteria_hist_train.round(3),\n          descr_virus_hist_train.round(3)]\n\nvalues_valid = [descr_normal_hist_valid.index, #1st col\n          descr_normal_hist_valid.round(3), \n          descr_bacteria_hist_valid.round(3),\n          descr_virus_hist_valid.round(3)]\n    \ndef create_table(fig, row, col, values):\n    \n    \"\"\"\n    function used to render description table.\n    \n    parameters:\n    -----------\n        fig: plotly figure\n        row: int\n        col: int\n        values: list of values to populate the table\n    \"\"\"\n\n    fig.add_trace(go.Table(\n      columnorder = [1, 2, 3, 4],\n      columnwidth = [160, 390],\n      header = dict(\n        values = [['Data'], ['<b>NORMAL<\/b><br> lungs'], \n                  ['<b>BACTERIA<\/b><br> lungs'], ['<b>Virus<\/b><br> lungs']],\n        line_color='darkslategray',\n        fill_color='royalblue',\n        align=['left', 'center'],\n        font=dict(color='white', size=12),\n        height=40\n      ),\n      cells=dict(\n        values=values,\n        line_color='darkslategray',\n        fill=dict(color=['paleturquoise', 'white']),\n        align=['left', 'center'],\n        font_size=12,\n        height=30)\n        )\n    , row=row, col=col)","3481e4af":"fig = make_subplots(\n    rows=2, cols=2,\n    specs=[[{\"type\": \"Histogram\"}, {\"type\": \"Histogram\"}],\n           [{\"type\": \"table\"}, {\"type\": \"table\"}]], column_titles=['Train histogram', 'validation histogram'],\n    )\n\n# Creation of the first histogram\nfig.add_trace(go.Histogram(x=normal_hist_train.values.flatten(),\n                           name='Normal',  marker=dict(color='gold')), row=1, col=1)\nfig.add_trace(go.Histogram(x=bacteria_hist_train.values.flatten(),\n                           name='Bacteria', marker=dict(color='mediumturquoise')), row=1, col=1)\nfig.add_trace(go.Histogram(x=virus_hist_train.values.flatten(),\n                           name='Virus', marker=dict(color='darkorange')), row=1, col=1)\n\n# Creation of the second histogram\nfig.add_trace(go.Histogram(x=normal_hist_valid.values.flatten(),\n                           name='Normal',  marker=dict(color='gold'), showlegend=False), row=1, col=2)\nfig.add_trace(go.Histogram(x=bacteria_hist_valid.values.flatten(),\n                           name='Bacteria', marker=dict(color='mediumturquoise'), showlegend=False), row=1, col=2)\nfig.add_trace(go.Histogram(x=virus_hist_valid.values.flatten(),\n                           name='Virus', marker=dict(color='darkorange'), showlegend=False), row=1, col=2)\n\nfig.update(layout_title_text=f'Class histograms')\n\n# Creation of the first table\ncreate_table(fig, 2, 1, values_train)\n\n# Creation of the second histogram\ncreate_table(fig, 2, 2, values_valid)\n\nfig.update_layout(height=1000, width=1000)\n\nfig.show()","320bdfa9":"plt.rcParams[\"figure.figsize\"] = (20,18)\nax = plt.subplot(1,3,1)\nax.imshow(pd.DataFrame(X_train[y_train==0]).sample(1000).median().values.reshape(127,127))\nplt.title('Normal Human body: mean over 500 images')\nplt.axis(\"off\")\n\nax2=plt.subplot(1,3,2)\nax2.imshow(pd.DataFrame(X_train[y_train==1]).sample(1000).median().values.reshape(127,127))\nplt.title('Infected by bacteria Human body: mean over 500 images')\nplt.axis(\"off\")\n\nax3 = plt.subplot(1,3,3)\nax3.imshow(pd.DataFrame(X_train[y_train==2]).sample(1000).median().values.reshape(127,127))\nplt.title('Infected by human virus Human body: mean over 500 images')\nplt.axis(\"off\")\nplt.show()","eb8ad281":"def print_classification_report(y_test, y_pred, target_names):\n    print('classification report:')\n    print(classification_report(y_test, y_pred, target_names=target_names))\n\ndef plot_roc_auc(X_test, y_test, y_pred, model, target_names):\n    # Plot Roc auc curve\n    y_score = model.predict_proba(X_test)\n    \n    # Compute ROC curve and ROC area for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    n_classes = len(target_names)\n    for i in range(n_classes):\n        y_hot = OneHotEncoder().fit_transform(y_test.reshape(-1,1)).toarray()\n        fpr[i], tpr[i], _ = roc_curve(y_hot[:, i], y_score[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    # Compute micro-average ROC curve and ROC area\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_hot.ravel(), y_score.ravel())\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    lw = 2\n\n    plt.plot(fpr['micro'], tpr['micro'], color='darkorange',\n             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n\ndef plot_matrix(model, X_test, y_test, target_names, ax):\n    # Plot confusion matrix\n    \n    plot_confusion_matrix(model, X_test, y_test, ax=ax)\n    \n    ax.set_xticklabels(target_names)\n    ax.set_yticklabels(target_names)\n    plt.title('Confusion matrix')\n    \ndef plot_matrix_dl(y_test, y_pred, target_names, ax):\n    # Plot confusion matrix for dl models\n    \n    confusion_mtx = tf.math.confusion_matrix(y_test, y_pred) \n    sns.heatmap(confusion_mtx, xticklabels=target_names, yticklabels=target_names, \n                annot=True, fmt='g', ax=ax)\n    plt.xlabel('Prediction')\n    plt.ylabel('Label')\n    \ndef plot_metrics(X_test, y_test, y_pred, model, target_names=['NORMAL', 'BACTERIA', 'VIRUS'], dl=False):\n    \n    print_classification_report(y_test, y_pred, target_names)\n    \n    plt.subplot(121)\n    plot_roc_auc(X_test, y_test, y_pred, model, target_names)\n    \n    ax = plt.subplot(122)\n    if dl:\n        plot_matrix_dl(y_test, y_pred, target_names, ax)\n    else:\n        plot_matrix(model, X_test, y_test, target_names, ax)","076eea57":"# We fit the first logistic regression without modifying the X\nt0 = time()\nclf = LogisticRegression()\nclf.fit (X_train, y_train)\nprint(f\"With {X_train.shape[1]} variables it is done in %0.3fs\" % (time() - t0))\nprint(f'score : {round(clf.score(X_test, y_test), 3)}')\n\n# We first applying PCA on X before fitting the logistic regression\ndim = PCA(400)\nX_train_PCA = dim.fit_transform(X_train)\nX_test_PCA = dim.transform(X_test)\n\nt0 = time()\nclf = LogisticRegression()\nclf.fit (X_train_PCA, y_train)\nprint(\"With 400 variables, it is done in %0.3fs\" % (time() - t0))\nprint(f\"score: {round(clf.score(X_test_PCA, y_test), 3)}\")","887a3a30":"plt.rcParams['figure.figsize'] = (15,8)\n\nn_components = 6 # Determine the number of components to display\nmethods = [PCA(n_components=n_components, svd_solver='randomized', whiten=True),\n           FactorAnalysis(n_components=n_components),\n           NMF(n_components)]\n    \nfor idx, method in enumerate(methods):\n    \n    t0 = time()\n    print(f'Method: {method}')\n    method.fit(X_train)\n    print(\"done in %0.3fs\" % (time() - t0))","ba609604":"for component in range(1, n_components+1):\n    axe = plt.subplot(1, n_components, component)\n    plt.title(f'Component n\u00b0{component}')\n    axe.xaxis.set_visible(False)\n    axe.yaxis.set_visible(False)\n    plt.imshow(methods[0].components_[component-1,:].reshape(127, 127))","bd256d9c":"for component in range(1, n_components+1):\n    axe = plt.subplot(1, n_components, component)\n    plt.title(f'Component n\u00b0{component}')\n    axe.xaxis.set_visible(False)\n    axe.yaxis.set_visible(False)\n    plt.imshow(methods[1].components_[component-1,:].reshape(127, 127))","47076a53":"for component in range(1, n_components+1):\n    axe = plt.subplot(1, n_components, component)\n    plt.title(f'Component n\u00b0{component}')\n    axe.xaxis.set_visible(False)\n    axe.yaxis.set_visible(False)\n    plt.imshow(methods[2].components_[component-1,:].reshape(127, 127))","ff32978a":"plt.rcParams['figure.figsize'] = (15,5)\n\nfor idx, method in enumerate(methods):\n    plt.subplot(1, 3, idx+1)\n    y = methods[idx].transform(X_train)[:, 0]\n    x = methods[idx].transform(X_train)[:, 1]\n    plt.scatter(x[y_train==0], y[y_train==0], c='g', alpha=0.5)\n    plt.scatter(x[y_train==1], y[y_train==1], c='r', alpha=0.5)\n    plt.scatter(x[y_train==2], y[y_train==2], c='b', alpha=0.5)\n    plt.title(method)","faada32f":"plt.rcParams['figure.figsize'] = (10, 5)\n\n# % of Explained variance with 358 components with PCA\n\nmethod = PCA(n_components=500, svd_solver='randomized', whiten=True)\nmethod.fit(X_train)\n\ny_fn = lambda x: round(np.sum(method.explained_variance_ratio_[:x]),3)\nx = np.linspace(0, 500, 100, dtype='int')\ny = np.array([y_fn(i) for i in x])\nx_95 = np.argmin(abs(y - 0.95), axis=0)\n\nplt.plot(x, y)\nplt.hlines(1, xmin=x.min(), xmax=x.max())\nplt.hlines(0.95, xmin=x.min(), xmax=x.max(), linestyles='--')\nplt.vlines(x[x_95], ymin=y.min(), ymax=1, color='r')\n\nplt.yticks([0.0, 0.2, 0.4, 0.6, 0.8, 0.95, 1.0])\nplt.xticks([i for i in range(0, 501, 100)] + [x[x_95]])\nplt.title('358 principal components explain 95% of the variance')\nplt.show()","f6352850":"t0 = time()\nranking = {'mutual_info_classif': pd.Series(mutual_info_classif(X_train, y_train)).sort_values(ascending=False).index,\n           'f_classif':pd.Series(f_classif(X_train, y_train)[0]).sort_values(ascending=False).index, \n           'chi2':pd.Series(chi2(X_train, y_train)[0]).sort_values(ascending=False).index,\n           'PCA': PCA}\nprint('Done in:', time() - t0)","9aaa71b4":"plt.rcParams['figure.figsize'] = (15,8)\n\nfor idx, algo in enumerate(ranking):\n    image = X_train[100].copy() # copy one image\n    plt.subplot(2,2, idx+1)\n    nfeats = 400 if algo=='PCA' else 3000 # We select 400 components in case of PCA else 3000 pixels\n    \n    if algo=='PCA':\n        new_image = image.reshape(1,-1) # PCA is waiting shape (n, 1)\n        pca = PCA(nfeats).fit(X_train)\n        new_image = pca.transform(new_image)[:,:nfeats] # We transform and select the 400 components\n        plt.imshow(new_image.reshape(20, 20))  \n        \n    else:\n        important_pixels = ranking[algo][:nfeats]\n        image[important_pixels] = 0.95  # We plot in yellow the selected pixels\n        plt.imshow(image.reshape(127, 127))\n        \n    plt.title(f'{algo}: {nfeats} pixels')","dd92e7d2":"def best_clf(X_train, y_train, X_valid, y_valid, X_test, y_test):\n    \n    \"\"\"\n    This function is used to find the best model based on roc auc score over the test set.\n    It is based also on gaussian process which is an optimization algorithm used to find \n    a minimum according a space of hyperparameters. Hence, we don't lost time.\n    \n    parameters:\n    -----------\n        X_train, y_train, X_valid, y_valid, X_test, y_test : Numpy arrays\n        \n    return:\n    -------\n        best_model: model trained\n        gp: gaussian process results see https:\/\/scikit-optimize.github.io\/stable\/modules\/generated\/skopt.gp_minimize.html \n            for more informations.\n    \"\"\"\n    \n    # We create clfs. It will gather the hyperparameters space\n    clfs = pd.DataFrame(columns=['spaces'])\n    \n    # ... hyperparameters space for each model\n    clfs['spaces'] = [[Real(0.5, 1.5, prior='log-uniform', name='C'),\n                       Categorical([\"balanced\", None], name='class_weight'),\n                       Integer(100, 3500, name='k')],\n                      \n                      [Real(0.5, 1, prior='log-uniform', name='subsample'),\n                      Real(0.1, 3, prior='log-uniform', name='lambda'),\n                      Integer(2, 6, name='max_depth'),\n                      Real(0.01, 0.04, prior='log-uniform', name='learning_rate'),\n                      Categorical(['multi:softmax'], name='objective'),\n                      Integer(100, 2000, name='k'),\n                      Integer(10, 30, name='early_stopping_rounds')],\n                      \n                      [Real(0.5, 1, prior='log-uniform', name='bagging_temperature'),\n                      Real(0.1, 3, prior='log-uniform', name='l2_leaf_reg'),\n                      Integer(2, 6, name='depth'),\n                      Integer(500, 1000, name='n_estimators'),\n                      Real(0.01, 0.04, prior='log-uniform', name='learning_rate'),\n                      Categorical(['MultiClass'], name='loss_function'),\n                      Categorical([\"Balanced\", \"None\"], name='auto_class_weights'),\n                      Integer(100, 2000, name='k'),\n                      Integer(10, 30, name='early_stopping_rounds')]]\n    \n    # We gather the models to test\n    clfs.index = [LogisticRegression,\n                  XGBClassifier,\n                  CatBoostClassifier]\n    \n    # Results will gather the roc auc score associated to each model\n    results = pd.DataFrame(columns=['model', 'roc_auc_score'])\n    \n    def find_best_params(model, space):\n        \"\"\"\n        Apply gaussian process optimization to a model.\n        \n        parameters:\n        -----------\n            model: classifier\n            space: list of hyperparameters space\n            \n        returns:\n        --------\n            best_params: dict\n            gp: gp results\n        \"\"\"\n        @use_named_args(space)\n        def objective(**params):\n            \n            \"\"\"\n            Objective function used in gp optimization.\n            \"\"\"\n            \n            k = params.pop('k', None) # We get k parameter used in SelectKBest and PCA\n            early_stopping_rounds = params.pop('early_stopping_rounds', None) # We get early_stopping_rounds parameter...\n                                                                        # ... used in fit method of XGB and Catboost.\n            \n            dim = SelectKBest(k=k)\n            new_X_train = dim.fit_transform(X_train, y_train) # Transform data\n            new_X_valid = dim.transform(X_valid) # Transform validation\n            \n            clf = model(random_state=0, verbose=False)\n            clf.set_params(**params)\n\n            try:\n                # Gradient boosting build their trees iteratively hence to prevent overfitting we can put evaluation set \n                clf = clf.fit(new_X_train, y_train,\n                  model__eval_set = [(new_X_valid[:int(len(X_valid)\/2)], y_valid[:int(len(X_valid)\/2)])],\n                  model__early_stopping_rounds = early_stopping_rounds, verbose=False)\n            except:\n                clf = clf.fit(new_X_train, y_train)\n\n            return log_loss(y_valid[int(len(X_valid)\/2):], clf.predict_proba(new_X_valid[int(len(X_valid)\/2):])) # Calculating crossentropy loss\n\n        gp = gp_minimize(objective, space, n_calls=20, random_state=0) # We use 20 points to minimize the function\n        best_params = {param.name: best_param  for best_param, param in zip(gp.x, space)}\n        return best_params, gp\n\n    for _, content in clfs.iterrows(): # We iterate over clfs items\n        \n        t0 = time()\n        print(f\"Fitting {content.name} to the training set\")\n        \n        model = content.name\n        space = content.iloc[0]\n        best_params, gp = find_best_params(model, space)\n\n        # assign hyperparameters\n        k = best_params.pop('k', None)\n        early_stopping_rounds = best_params.pop('early_stopping_rounds', None) \n        \n        # Create pipeline\n        clf = Pipeline([('dim', SelectKBest(k=k)),\n                        ('model', model(random_state=0, verbose=False, **best_params))])\n        \n        # logisticRegression don't take into account params early_stopping_rounds and eval set\n        try:\n            # We need to transform X_valid\n            clf[0].fit(X_train, y_train)\n            new_X_valid = clf[0].transform(X_valid, y_train)\n            \n            # We fit our classifier pipeline\n            clf = clf.fit(X_train, y_train,\n              model__eval_set = [(new_X_valid, y_valid)],\n              model__early_stopping_rounds = early_stopping_rounds, verbose=False)\n            \n        except:\n            print('no validation set used')\n            clf = clf.fit(X_train, y_train)\n            \n        print(\"done in %0.3fs\" % (time() - t0))\n\n        # #############################################################################\n        # Quantitative evaluation of the model quality on the test set\n\n        print(\"Predicting pneumonia\")\n        t0 = time()\n        y_pred = clf.predict_proba(X_test)\n        print(\"done in %0.3fs\" % (time() - t0))\n        \n        results.loc[str(clf), 'model'] = clf\n        results.loc[str(clf), 'roc_auc_score'] = roc_auc_score(y_test, y_pred, average='macro', multi_class='ovr')\n        \n    print(\"Best estimator found by bayesian optimization:\")\n    best_model = results.sort_values('roc_auc_score', ascending=False).model[0]\n\n    print(best_model)\n\n    return best_model, gp, results\n\nbest_model, gp, results = best_clf(X_train, y_train, X_valid, y_valid, X_test, y_test)","c95ed3b6":"display_results = results.copy()\ndisplay_results['models'] = ['Logistic Regression', 'XGBClassifier', 'CatBoostClassifier']\ndisplay_results.reset_index(drop=True).iloc[:,[2, 1]]","fd8a4eec":"_ = plot_objective(gp, n_points=10)","b38dbfd9":"y_pred = best_model.predict(X_test).flatten()\ny_proba = best_model.predict_proba(X_test)\nnew_X_test = best_model['dim'].transform(X_test) # transform X_train because pipeline raise error in shap function\n\nplt.rcParams['figure.figsize'] = (15, 5)\nplot_metrics(X_test, y_test, y_pred, best_model)","c6f4a73e":"# Function to make plots of the probabilities that the model estimates for an image\n\ndef analyse_model_prediction(data, true_labels, model, image_num, nb_class, ensemble_size=1):\n        \n    image = data[image_num]\n    true_label = true_labels[image_num]\n    predicted_probabilities = np.empty(shape=(ensemble_size, nb_class))\n    model_prediction = model.predict_proba(image[np.newaxis, :])\n    predicted_probabilities[:ensemble_size] = model.predict_proba(image.reshape(1, -1))\n    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 2),\n                                   gridspec_kw={'width_ratios': [2, 4]})\n    \n    # Show the image and the true label\n    ax1.imshow(image.reshape(127, 127), cmap='gray')\n    ax1.axis('off')\n    ax1.set_title('True label: {}'.format(CLASS_LABELS[true_label]))\n    \n    # Show a 95% prediction interval of model predicted probabilities\n    pct_2p5 = np.array([np.percentile(predicted_probabilities[:, i], 2.5) for i in range(nb_class)])\n    pct_97p5 = np.array([np.percentile(predicted_probabilities[:, i], 95) for i in range(nb_class)])    \n    bar = ax2.bar(np.arange(nb_class), pct_97p5, color='red')\n    bar[int(true_label)].set_color('green')\n    ax2.bar(np.arange(nb_class), pct_2p5-0.02, color='white', linewidth=1, edgecolor='white')\n    ax2.set_xticks(np.arange(3))\n    ax2.set_xticklabels(CLASS_LABELS)\n    ax2.set_ylim([0, 1])\n    ax2.set_ylabel('Probability')\n    ax2.set_title('Model estimated probabilities')\n    plt.show()\n\n\nfor i in [0, 300, 600]:\n    analyse_model_prediction(X_test, y_test, best_model, i, 3)","bd61a172":"k = best_model[0].get_params()['k']\npixels = ranking['f_classif'][:k] # We get the most important index pixels from ranking\n\n# CatBoost offers API to calculate the feature importances\nfeature_importances = np.zeros((127*127,))\nfeature_importances[pixels] = best_model[1].feature_importances_  \nfeature_importances = feature_importances.reshape(1, 127, 127, 1)\n\n# Pick an image \nimage = X_test[150, :]\n\nshap.image_plot([feature_importances], image.reshape(1, 127, 127), np.array([[f'Feature importances']]), show=False)\n\nax = plt.gca()        \n\n# Get the images on an axis\nim = ax.images \n# Assume colorbar was plotted last one plotted last\ncb = im[-1].colorbar   \n\ncb.set_label('Feature importances values')","d0326b6e":"plt.rcParams['figure.figsize'] = (15, 15)\n\ndef plot_shap(X, y, y_proba, image, id, model):\n    \"\"\"\n    function used to display for each class a picture with its score prediction associated. \n    \n    parameters:\n    -----------\n    \"\"\"\n    \n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(X)\n    nb_classes = len(shap_values)\n    y_true = y[id]\n    probs = y_proba[id]\n    \n    new_shaps = []\n    for cat, shap_category in enumerate(shap_values): \n        new_shap = np.zeros((127*127,))\n        new_shap[pixels] = shap_category[id, :]\n        new_shaps.append(new_shap.reshape(1, 127, 127, 1))\n        \n    indexes = np.array([[f'Normal: {probs[0]:.3f}%', f'Bacteria: {probs[1]:.3f}%', f'Virus: {probs[2]:.3f}%']])\n    \n    shap.image_plot(new_shaps, \n                    image[id].reshape(1, 127, 127),\n                    indexes, \n                    show=False)\n    \n    ax = plt.gcf()\n\n    # Get the images on an axis\n    im = ax.get_axes()\n    # Assume colorbar was plotted last one plotted last\n    \n    im[0].set_title(f'True label: {CLASS_LABELS[y_true]}')\n\nfor i in [10]:\n    plot_shap(new_X_test, y_test, y_proba, X_test, i, best_model['model'])","0cd13105":"class VotingClfs:  \n    def __init__(self, estimators):\n        \n        self.estimators = estimators\n        \n    def fit(self, X_train, y_train, X_valid, y_valid):\n        \n        for idx in range(len(self.estimators)):\n            self.estimators[idx].fit(X_train, y_train,\n                                     catboostclassifier__eval_set = [(X_valid, y_valid)],\n                                     catboostclassifier__early_stopping_rounds = 30)\n            \n        return self\n            \n    def predict_proba(self, X):\n        decisions = np.concatenate(list(map(lambda x: x.predict_proba(X), self.estimators)), axis=0)\n        self.std = np.std(decisions, axis=0)\n        self.mean = np.mean(decisions, axis=0)\n\n        return decisions\n\nbest_params = best_model[1].get_params()\nbest_params.pop('random_state', None)\npool_clfs = [make_pipeline(SelectKBest(k=k), CatBoostClassifier(**best_params, random_state=random.randint(0, 3000))) for _ in range(5)]\n\ndim = SelectKBest(k=k).fit(X_train, y_train)\nnew_X_valid = dim.transform(X_valid)\n\nvoting_clf = VotingClfs(pool_clfs).fit(X_train, y_train, new_X_valid, y_valid)","b283b4a7":"for i in [0, 400, 560]:\n    analyse_model_prediction(X_test, y_test, voting_clf, i, 3, 5)","a69e420d":"X_train_dl = X_train.reshape(-1, 127, 127, 1)\nX_valid_dl = X_valid.reshape(-1, 127, 127, 1)\nX_test_dl = X_test.reshape(-1, 127, 127, 1)","2d747ea2":"# Function to make plots of the probabilities that the model estimates for an image\n\ndef analyse_model_prediction(data, true_labels, model, image_num, run_ensemble=False):\n    if run_ensemble:\n        ensemble_size = 200\n    else:\n        ensemble_size = 1\n    image = data[image_num]\n    true_label = true_labels[image_num, 0]\n    predicted_probabilities = np.empty(shape=(ensemble_size, 3))\n    for i in range(ensemble_size):\n        predicted_probabilities[i] = model.predict(image[np.newaxis, :])[0]\n    model_prediction = model(image[np.newaxis, :])\n    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 2),\n                                   gridspec_kw={'width_ratios': [2, 4]})\n    \n    # Show the image and the true label\n    ax1.imshow(image[..., 0], cmap='gray')\n    ax1.axis('off')\n    ax1.set_title('True label: {}'.format(CLASS_LABELS[true_label]))\n    \n    # Show a 95% prediction interval of model predicted probabilities\n    pct_2p5 = np.array([np.percentile(predicted_probabilities[:, i], 5) for i in range(3)])\n    pct_97p5 = np.array([np.percentile(predicted_probabilities[:, i], 95) for i in range(3)])    \n    bar = ax2.bar(np.arange(3), pct_97p5, color='red')\n    bar[int(true_label)].set_color('green')\n    ax2.bar(np.arange(3), pct_2p5-0.02, color='white', linewidth=1, edgecolor='white')\n    ax2.set_xticks(np.arange(3))\n    ax2.set_xticklabels(CLASS_LABELS)\n    ax2.set_ylim([0, 1])\n    ax2.set_ylabel('Probability')\n    ax2.set_title('Model estimated probabilities')\n    plt.show()","15b3b029":"from tensorflow.keras.layers import *\nfrom tensorflow.keras import Model\n\ndef Network():\n    inputs = tf.keras.Input(shape=(127, 127, 1))\n    x = Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n    x = Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same')(x)\n    \n    x = BatchNormalization()(x)\n    x = MaxPool2D(pool_size=(2, 2))(x)\n    x = Dropout(0.2)(x)\n        \n    x = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(x)\n    x = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(x)\n    x = BatchNormalization()(x)\n    x = MaxPool2D(pool_size=(2, 2))(x)\n    x = Dropout(0.2)(x)\n        \n    x = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n    x = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n    x = BatchNormalization()(x)\n    x = MaxPool2D(pool_size=(2, 2))(x)\n    x = Dropout(0.2)(x)\n        \n    x = Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same')(x)\n    x = Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same')(x)\n    x = BatchNormalization()(x)\n    x = MaxPool2D(pool_size=(2, 2))(x)\n    x = Dropout(0.2)(x)\n        \n    x = Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same')(x)\n    x = Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same')(x)\n    x = BatchNormalization()(x)\n    x = MaxPool2D(pool_size=(2, 2))(x)\n    x = Dropout(0.2)(x)\n        \n    x = Flatten()(x)\n       \n    x = Dense(1024, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n        \n    x = Dense(512, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n        \n    x = Dense(256, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n        \n    x = Dense(64, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n        \n    output = Dense(3, activation = \"softmax\")(x)\n    \n    model = Model(inputs, output)\n    \n    return model\n\nmodel = Network()\nmodel.summary()","91085ae5":"reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.0001)\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n                    monitor='val_loss', min_delta=0, patience=4, verbose=0,\n                    mode='auto', baseline=None, restore_best_weights=True)\n\nmodel.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), optimizer='adam',\n              metrics=['accuracy'])\n\n# Scaling by total\/2 helps keep the loss to a similar magnitude.\n# The sum of the weights of all examples stays the same.\nweight_for_0 = (1\/normal_train) * (total_train) \/ 2.0 \nweight_for_1 = (1\/bacteria_train) * (total_train) \/ 2.0\nweight_for_2 = (1\/virus_train) * (total_train) \/ 2.0\n\nclass_weight = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2}\n\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))\nprint('Weight for class 2: {:.2f}'.format(weight_for_2))\n\nhistory = model.fit(X_train_dl, y_train, epochs=20, validation_data=(X_valid_dl, y_valid),\n                    callbacks = [reduce_lr, early_stopping], verbose=1, class_weight=class_weight) ","2d0c42a7":"fig = plt.figure(figsize=(20,5))\n\nax = plt.subplot(131)\n\nepochs = [i for i in range(len(history.history['loss']))]\nax.plot(epochs, history.history['loss'], label='loss')\nax.plot(epochs, history.history['val_loss'], label='val_loss')\nplt.legend()\n\nax = plt.subplot(132)\nax.plot(epochs, history.history['accuracy'], label='accuracy')\nax.plot(epochs, history.history['val_accuracy'], label='val_accuracy')\nplt.legend()\n\nplt.show()","59666a87":"plt.rcParams['figure.figsize'] = (15, 5)\n\ny_pred = model.predict(X_test_dl)\nmodel.predict_proba = lambda X: model.predict(X.reshape(-1, 127, 127, 1)) # Define predict_proba to use the plot_metrics function\n\nplot_metrics(X_test_dl, y_test, np.argmax(y_pred, axis=1), model, dl=True)\n","b6df1f6b":"def get_convolutional_reparameterization_layer(filters, kernel,\n                                               divergence_fn, dilation_rate, activation,\n                                               padding, strides):\n    \"\"\"\n    This function should create an instance of a Convolution2DReparameterization \n    layer according to the above specification. \n    The function takes the input_shape and divergence_fn as arguments, which should \n    be used to define the layer.\n    Your function should then return the layer instance.\n    \"\"\"\n    return tfpl.Convolution2DReparameterization(filters=filters, kernel_size=(kernel,kernel),\n                                                padding=padding, activation=activation, strides=strides,\n                                                dilation_rate=dilation_rate,\n                            kernel_prior_fn=tfpl.default_multivariate_normal_fn,\n                            kernel_posterior_fn=tfpl.default_mean_field_normal_fn(is_singular=False),\n                            kernel_divergence_fn=divergence_fn)\n\n# Function to define the spike and slab distribution\n\ndef spike_and_slab(event_shape, dtype):\n    distribution = tfd.Mixture(\n        cat=tfd.Categorical(probs=[0.5, 0.5]),\n        components=[\n            tfd.Independent(tfd.Normal(\n                loc=tf.zeros(event_shape, dtype=dtype), \n                scale=1.0*tf.ones(event_shape, dtype=dtype)),\n                            reinterpreted_batch_ndims=1),\n            tfd.Independent(tfd.Normal(\n                loc=tf.zeros(event_shape, dtype=dtype), \n                scale=10.0*tf.ones(event_shape, dtype=dtype)),\n                            reinterpreted_batch_ndims=1)],\n    name='spike_and_slab')\n    return distribution\n\n\ndef get_prior(kernel_size, bias_size, dtype=None):\n    n = kernel_size+bias_size\n    prior_model = Sequential([\n        tfpl.DistributionLambda(\n            lambda t: tfd.MultivariateNormalDiag(loc=tf.zeros(n), scale_diag=tf.ones(n))\n        )\n    ])\n    return prior_model\n\ndef get_posterior(kernel_size, bias_size, dtype=None):\n    \"\"\"\n    This function should create the posterior distribution as specified above.\n    The distribution should be created using the kernel_size, bias_size and dtype\n    function arguments above.\n    The function should then return a callable, that returns the posterior distribution.\n    \"\"\"\n    n = kernel_size + bias_size\n    return Sequential([\n        tfpl.VariableLayer(tfpl.IndependentNormal.params_size(n), dtype=dtype),\n        tfpl.IndependentNormal(n)])\n\ndef get_dense_variational_layer(units, activation, prior_fn, posterior_fn, kl_weight):\n    \"\"\"\n    This function should create an instance of a DenseVariational layer according \n    to the above specification. \n    The function takes the prior_fn, posterior_fn and kl_weight as arguments, which should \n    be used to define the layer.\n    Your function should then return the layer instance.\n    \"\"\"\n    \n    return tfpl.DenseVariational(\n                          units=units,\n                          activation=activation,\n                          make_prior_fn=prior_fn,\n                          make_posterior_fn=posterior_fn,\n                          kl_weight=kl_weight,\n                          kl_use_exact=False)\n\n# Create the layers\n\ntf.random.set_seed(0)\ndivergence_fn = lambda q, p, _ : tfd.kl_divergence(q, p) \/ X_train.shape[0]\n\ndef probabilistic_Network():\n    \n    inputs = tf.keras.Input(shape=(127, 127, 1))\n    x = get_convolutional_reparameterization_layer(16, kernel=3, activation='relu', padding='same', divergence_fn=divergence_fn, dilation_rate=1,\n                                               strides=1)(inputs)\n    x = get_convolutional_reparameterization_layer(16, kernel=3, activation='relu', padding='same', divergence_fn=divergence_fn, dilation_rate=1,\n                                               strides=1)(x)\n    x = BatchNormalization()(x)\n    x = MaxPool2D(pool_size=(2, 2))(x)\n    x = Dropout(0.2)(x)\n        \n    x = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(x)\n    x = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(x)\n    x = BatchNormalization()(x)\n    x = MaxPool2D(pool_size=(2, 2))(x)\n    x = Dropout(0.2)(x)\n        \n    x = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n    x = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n    x = BatchNormalization()(x)\n    x = MaxPool2D(pool_size=(2, 2))(x)\n    x = Dropout(0.2)(x)\n        \n    x = Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same')(x)\n    x = Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same')(x)\n    x = BatchNormalization()(x)\n    x = MaxPool2D(pool_size=(2, 2))(x)\n    x = Dropout(0.2)(x)\n        \n    x = Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same')(x)\n    x = Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same')(x)\n    x = BatchNormalization()(x)\n    x = MaxPool2D(pool_size=(2, 2))(x)\n    x = Dropout(0.2)(x)\n        \n    x = Flatten()(x)\n       \n    x = Dense(1024, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n        \n    x = Dense(512, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n        \n    x = Dense(256, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n        \n    x = Dense(64, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n        \n    output = Dense(3, activation = \"softmax\")(x)\n    \n    model = Model(inputs, output)\n    \n    return model\n\nprobabilistic_model = probabilistic_Network()\n\nprobabilistic_model.summary()\n\nprobabilistic_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), \n                            optimizer=tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, name='Adam'),\n              metrics=['accuracy'])\n\nhistory = probabilistic_model.fit(X_train_dl, y_train, epochs=20,\n                        validation_data=(X_valid_dl, y_valid),\n                        callbacks = [reduce_lr], class_weight=class_weight)","47f63d7e":"fig = plt.figure(figsize=(20,5))\n\nax = plt.subplot(131)\n\nepochs = [i for i in range(len(history.history['loss']))]\nax.plot(epochs, history.history['loss'], label='loss')\nax.plot(epochs, history.history['val_loss'], label='val_loss')\nplt.legend()\n\nax = plt.subplot(132)\nax.plot(epochs, history.history['accuracy'], label='accuracy')\nax.plot(epochs, history.history['val_accuracy'], label='val_accuracy')\nplt.legend()\n\nplt.show()","d45aad6c":"plt.rcParams['figure.figsize'] = (15, 5)\n\ny_pred = probabilistic_model.predict(X_test_dl)\nprobabilistic_model.predict_proba = lambda X: probabilistic_model.predict(X.reshape(-1, 127, 127, 1)) # Define predict_proba to use the plot_metrics function\n\nplot_metrics(X_test_dl, y_test, np.argmax(y_pred, axis=1), probabilistic_model, dl=True)","a27c5205":"for i in [492]:\n    analyse_model_prediction(X_test_dl, y_test.reshape(-1,1), probabilistic_model, i)","6c4eba71":"for i in [492]:\n    analyse_model_prediction(X_test_dl, y_test.reshape(-1,1), probabilistic_model, i, True)","a31b5cc2":"# since we have two inputs we pass a list of inputs to the explainer\nexplainer = shap.GradientExplainer(probabilistic_model, [X_train_dl[:200]])\n\n# we explain the model's predictions on the first three samples of the test set\nshap_values = explainer.shap_values(X_test_dl[[492]])\n\nshap.image_plot([shap_values[i] for i in range(3)], X_test_dl[[492]])","4792e34a":"IMG_SHAPE = (127, 127, 3)\nbase_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\n\ninputs = tf.keras.Input(shape=(127, 127, 1))\nx = keras.layers.Concatenate()((inputs,inputs,inputs))\nx = base_model(x, training=True)\nglobal_average_layer = tf.keras.layers.GlobalAveragePooling2D()\nfeature_batch_average = global_average_layer(x)\noutputs = keras.layers.Dense(3, activation=\"softmax\")(feature_batch_average)\nmodel = tf.keras.Model(inputs, outputs)\n\nbase_learning_rate = 0.001 # Little lr\nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n              metrics=['accuracy']) \n\n# Scaling by total\/2 helps keep the loss to a similar magnitude.\n# The sum of the weights of all examples stays the same.\nweight_for_0 = (1 \/ normal_train)*(total_train)\/2.0 \nweight_for_1 = (1 \/ bacteria_train)*(total_train)\/2.0\nweight_for_2 = (1 \/ virus_train)*(total_train)\/2.0\n\nclass_weight = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2}\n\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))\nprint('Weight for class 2: {:.2f}'.format(weight_for_2))\n\nhistory = model.fit(x=X_train_dl, \n                    y=y_train,\n                    epochs=3,\n                    validation_data=(X_valid_dl, y_valid), class_weight=class_weight)","b6978236":"fig = plt.figure(figsize=(20,5))\n\nax = plt.subplot(131)\n\nepochs = [i for i in range(len(history.history['loss']))]\nax.plot(epochs, history.history['loss'], label='loss')\nax.plot(epochs, history.history['val_loss'], label='val_loss')\nplt.legend()\n\nax = plt.subplot(132)\nax.plot(epochs, history.history['accuracy'], label='accuracy')\nax.plot(epochs, history.history['val_accuracy'], label='val_accuracy')\nplt.legend()\n\nplt.show()","a5c5a62b":"plt.rcParams['figure.figsize'] = (15, 5)\n\ny_pred = model.predict(X_test_dl)\nmodel.predict_proba = lambda X: model.predict(X.reshape(-1, 127, 127, 1)) # Define predict_proba to use the plot_metrics function\n\nplot_metrics(X_test, y_test, np.argmax(y_pred, axis=1), model, dl=True)","afe36678":"for i in [5]:\n    analyse_model_prediction(X_test_dl, y_train.reshape(-1, 1), model, i, 3)","36d28ac7":"# since we have two inputs we pass a list of inputs to the explainer\nexplainer = shap.GradientExplainer(model, [X_train_dl[:200]])\n\n# we explain the model's predictions on the first three samples of the test set\nshap_values = explainer.shap_values(X_test_dl[[5]])","90122468":"shap.image_plot([shap_values[i] for i in range(3)], X_test_dl[[5]])","ae549de6":"We don't forget to reshape our images.","494713af":"CNN model and XGBclassifier have close results. This model has better results on virus as it tends to favour this class.","99895c41":"<h2 style=\"background-color: #8db6cd; color:#FFFFFF\">2.1 - Train-validation-test <a id=ba><\/a><\/h2>","995a5e3e":" <a href=\"#a\" class=\"list-group-item\">Go to summary<\/a>","9021ee87":"We can saw that our model converge.","5a164b23":"Here is all packages used in this notebook.","97578240":"Let's display some examples.","b5c6014f":"<h1 style=\"background-color: #428bca; color:#FFFFFF\">6 - Neural Networks <a id=f><\/a><\/h1>","fcb071ba":"Dataset analysis is a necessary step before creating models.\n\n3 folders are provided to us: \n\n```\nchest_Xray\/\n\ttest\/\n\t\tPNEUMONIA\/\n\t\tNORMAL\/\n    valid\/\n\t\tPNEUMONIA\/\n\t\tNORMAL\/\n\ttrain\/\n\t\tPNEUMONIA\/\n\t\tNORMAL\/\n        \n```\n\nBy inspecting files names in PNEUMONIA folders we can distinguish two more classes: Bacteria and Virus.\n\n```\nperson1_bacteria_1.jpeg\n\nperson1180_virus_2012.jpeg\n\n```\n\nIt is then usefull to create a multiclass classifier instead of a binary one.\n\nNow, we need to get the data.\nThanks to tensorflow, which provide a clear and concise API, we create train_ds and test_ds. These are two objects that can act like generators thanks to the \"take\" method. The principal goal of these objects is also to resize images into a uniformised shape which is essential when feeding data to a model. You can also notify that we are not scaling data as pixels are already between 0 and 1. ","e0a8db54":" <a href=\"#a\" class=\"list-group-item\">Go to summary<\/a>","771698d5":"Machine learning algorithms such as gradient boosting are good tools as seen previously. Unfortunately, some actions are essential beforehand, such as reducing the number of inputs. Furthermore, these algorithms do not learn shapes or recognise particular patterns. So if we give it a 90 degree rotated or shifted image our results will be less good. Neural networks with their convolutional layers are an alternative to \"classical\" algorithms. They are suitable for very high dimensional inputs and are able to capture patterns anywhere on the images. Let's start with that!\n\n\n\n\n","e7e55029":"<h1 style=\"background-color: #428bca; color:#FFFFFF\">1 - Import packages<a id=a><\/a><\/h1>","1153f607":"In the previous images, row shows the same image associated with its prediction score for each class. Columns show pixels weights involved in the corresponding class probability. They are float between 0 and 1 and they handle the aleotoric uncertainty. Hence if scores are close we can conclude that the model is not very sure about its prediction. \n\nHowever, let introduce one more concept which is epistemic uncertainty. It can be referred to confidence interval, in other words, the score my model gives me is computed from a sample but where could be the real value if we had the entire population? This uncertainty is only function of the number of sample in the train set. But how can we introduce this interval?\n\nOne way to do it is to train multiple models of catboost with different random state like that they will be all different. Then, we can extract the mean and the variance to compute the confidence interval.","3cfcc5b6":"<h3 style=\"background-color: #8db6cd; color:#FFFFFF\">6.2.2 - Interpretability <a id=fd><\/a><\/h3>","b992364f":"It clearly denotes imbalanced data. Bacteria images represent 50 % of the set. It needs to be kept in mind and we will try to correct that during the model creation.\n    Validation set is representative of our train set in term of class proportions and It could be interesting to see their histograms to be sure.\n    \nThe following cells create histograms for train et validation set as well as their respective table which gather some statistical data.","80530ba4":"<h1 style=\"background-color: #428bca; color:#FFFFFF\">2 - Data exploration<a id=b><\/a><\/h1>","8ae1222f":"Here we go, the most interesting part. Here, we have to create a function, which on the basis of the roc auc score, returns the best model with the optimal hyperparameters. We will apply what we have just seen to maximise our performance.\n\nBefore diving into the code, 2 clarifications. We are talking about finding the best model with the optimal hyperparameters, we will not use the classical functions of scikit learn like gridsearchcv but rather a Bayesian optimization method. In 1 sentence, we will explore a space of hyperparameters and find the combinations that minimize our loss function (here the cross entropy). ","9f6faa98":" <a href=\"#a\" class=\"list-group-item\">Go to summary<\/a>","4b72ca1d":"<h2 style=\"background-color: #8db6cd; color:#FFFFFF\">6.3 - Transfer learning and fine tuning <a id=fc><\/a><\/h2>","5b4cd444":"<h2 style=\"background-color: #8db6cd; color:#FFFFFF\">2.3 - Conclusion <a id=bc><\/a><\/h2>","a086da9b":"We ran only one time the probabilistic model and see deterministic probabilities. We can see that our model is not sure of its answer but what is its confidence interval?  ","8604e4f1":"Data exploration is finish, it's time to define our metrics. They will be used to determine which model is the best one. We gathered multiple metrics:\n\n\n  <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\" class=\"list-group-item\">F1 score<\/a>\n  <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_auc_score.html?highlight=roc%20auc#sklearn.metrics.roc_auc_score\" class=\"list-group-item\">ROC AUC<\/a>\n  <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.recall_score.html?highlight=recall#sklearn.metrics.recall_score\" class=\"list-group-item\">Recall<\/a>\n  <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.precision_score.html?highlight=precision#sklearn.metrics.precision_score\" class=\"list-group-item\">Precision<\/a>\n\nA good understanding of these metrics can help us to choose the best model. But we need to choose one: The micro averaged roc auc score seems to be a good one as it take into account the uncertainties. The following cell shows how to use functions from scikit learn.","6903cd06":" <a href=\"#a\" class=\"list-group-item\">Go to summary<\/a>","00a9c63e":"<h1 style=\"background-color: #428bca; color:#FFFFFF\">5 - Best models <a id=e><\/a><\/h1>","5bd5a43c":"<h2 style=\"background-color: #8db6cd; color:#FFFFFF\">3.1 - Conclusion <a id=ca><\/a><\/h2>","5c32e3e7":"Here, no needs to reduce dimension neural networks work very well with high dimensions but we will introduce data augmentation. This is not a method to add more images, it aims to reduce overfitting by modifying some image caracteristics without deleting informations. We can rotate images or intensify the pixel values, even crop a little bit pixels as we saw in previous part that borders pixels could overfit our models.\n\nIf you are not comfortable with tensorflow API check their page they have lot of notebooks to learn.","ae5e741c":" <a href=\"#a\" class=\"list-group-item\">Go to summary<\/a>","577b21ab":"As explained in the introduction, interpetrability is at the heart of our project. CatBoost packages offers a simply way to calculate the feature importances to which is added the shap package which allows to visualize for each instance which pixel plays a more important role. Be carefull the shap values and feature importances have been multiplied by a factor in order to intensify the contrast, then the only thing we can conclude here is the pixels involvement on the decision and partially their values.\n","1e15bbe6":" <a href=\"#a\" class=\"list-group-item\">Go to summary<\/a>","645e39b3":"Pixel intensity is related to their importance but we don't know if it plays to its advantage or not in the decision. This is where shap value is usefull. It can associate a weight to each pixel. \n\n**For further read:**\n\n<a href=\"https:\/\/github.com\/slundberg\/shap\" class=\"list-group-item\">Shap package<\/a>\n<a href=\"https:\/\/christophm.github.io\/interpretable-ml-book\/pixel-attribution.html#grad-cam\" class=\"list-group-item\">Interpretable Machine Learning: A Guide for Making Black Box Models Explainable.\n*Christoph Molnar*<\/a>","d2a94451":" <a href=\"#a\" class=\"list-group-item\">Go to summary<\/a>","5d55fe8b":"One algorithm stand out for its speeds and robustness: ANOVA F-test.","fdc70728":"<h1 style=\"background-color: #428bca; color:#FFFFFF\">7 - Final conclusion <a id=g><\/a><\/h1>","e311c80a":"So far so good! catboost classifier is the best one with PCA. You probably answering what is the gp variable ? It includes lot of cool stuff about the optimization process check the scikit opt documentation for more precision. We can even visualize the partial dependence plot for each hyperparameters.","d79edc5d":"<h2 style=\"background-color: #8db6cd; color:#FFFFFF\">4.3 - Examples <a id=dc><\/a><\/h2>","ba1f4d75":"<h1 style=\"background-color: #428bca; color:#FFFFFF\">Summary<\/h1>","bc0d4f17":"There are a lot to say here. The first thing to notice is the difference in time executions. PCA is a way faster and this is a critic point here. Factor analysis and NMF takes so much time if we increase the number of components and that's the reason of using PCA. However, look at the first two components of NMF. It is similar to the picture we got on aggregated pixels. It means that it better decompose the data on the relevant pixel values. It worth to try NMF on few components like 6 before feeding our models.\n\n**a way to visualize the better decomposition of NFM it's to project data on the two first components.**","59c600b0":"<h2 style=\"background-color: #8db6cd; color:#FFFFFF\">5.3 - Conclusion <a id=ec><\/a><\/h2>","236ebf81":"We can easily see the difference. After reducing the inputs to 400 components, it takes less than 40 times than previous training for 1% accuracy loss. You would say that this is not a big gap but this algorithm is $O(nd)$ complexity. However, algorithms like xgboost with a complexity of $O(Kd\u2225x\u2225_0logn)$ are really accelerated.\n\nThen a question comes in mind. A lot of reducing algorithms exist but which one to use? We split them into 2 non exhaustive groups to try to understand their specificities. ","51d0efd8":" <a href=\"#a\" class=\"list-group-item\">Go to summary<\/a>","189a1f9d":"It seems that we constently overfit as the accuracy increase but not the validation accuracy.","e39c5b64":"**Then, let's display the results.**","64abca86":"As we will deal with models as gradient boosting, it is important to speak about curse of dimensionality, that's why this part is coming brutaly.\n\nLet's take an example to illustrate the high dimensionality problem in machine learning. We take a basic model like logistic regression which is quite quick and represent a good baseline.\n\nWe will first train it with the entire dataset. Then, we will try it on reducted data thanks to PCA.","ed23fab6":"<h2 style=\"background-color: #8db6cd; color:#FFFFFF\">2.2 - Sample <a id=bb><\/a><\/h2>","1b886f68":"<h2 style=\"background-color: #8db6cd; color:#FFFFFF\">4.2 - Feature selection <a id=db><\/a><\/h2>","0f993134":"In this part, we will try deep learning algorithms. First, we will go through deterministic model, then we will introduce a few probabilities in our values. We will use fine tuning to increase our results and finally, we will see how to interprete our results. Let's go!","d42de1a8":"Validation folders had not enough data to be usefull, so, we merged it into train folder and we will create another one.\n\nValidation set is essential for hyperparameters checking and to prevent some models of overfitting as gradient boosting and there are multiple techniques to split the data. As there are more than 5000 images, we will first try a simple split and finally a cross method.","96af1a5f":"We first turn Data into numpy arrays but why are we doing that? Dataset object is usefull for tensorflow models however as we are using other models from different packages it is easier to use arrays.\n\nFinally, this is a good practice to forgot the test set, like that, you prevent your model from data leakage.\n\nThe following cell is understandable by itself.","62ffae94":"<h1 style=\"background-color: #428bca; color:#FFFFFF\">3 - Metrics used <a id=c><\/a><\/h1>","f209b2a1":"Unfortunately, it is not clear which areas are involved in the decision.","27a927c0":"the results are very satisfactory.","fe89154c":"One last visualization for the road! By aggregating all the respective pixels for each class, we end up with these three images that say a lot. Indeed, the notable differences between normal lungs and sick ones are the intensity of the lungs and their blurred contours","247762d0":"Because we love graph, you can find below the data variance according number of components in PCA. We can see that 358 components explain 95% off the total variance.","52aa857e":"Even if our model doesn't give good enough results, we will try to build a bayesian model. I recommand you to follow the course \"Probabilistic Deep Learning with TensorFlow 2\" in coursera which is payant but really instructive. The following cell is quite complicated without previous explanations.","4329a235":"<div class=\"container-fluid\">\n    <img src=\"https:\/\/cdn.pixabay.com\/photo\/2014\/04\/03\/10\/21\/x-ray-310166_960_720.png\" class=\"center-block\">\n<\/div>\n            \n<h1 style=\"text-align: center; \n           font-size:50px;\n           background-color: #428bca;\n           border-radius: 8px;\n           line-height: 64px;\n           color:#FFFFFF;\"> X-RAY chests: Prediction, interpretability and uncertainty models <\/h1><a id=a><\/a>","191867b8":"<h2 style=\"background-color: #8db6cd; color:#FFFFFF\">5.1 - Examples <a id=ea><\/a><\/h2>","b4f31966":"**Pie chart seems to be an interesting graph to clearly see each proportion of images.**","45b9096c":"Feature selection is a technique of reducing dimension which eliminate variables that do not provide any information or those that seem independent, this way they do not change the current space.\n\nSee:\n\n<a href=\"https:\/\/en.wikipedia.org\/wiki\/Mutual_information\" class=\"list-group-item\">Mutual information<\/a>\n<a href=\"https:\/\/towardsdatascience.com\/anova-for-feature-selection-in-machine-learning-d9305e228476\" class=\"list-group-item\">ANOVA<\/a>\n<a href=\"https:\/\/en.wikipedia.org\/wiki\/Chi-squared_test#:~:text=A%20chi-squared%20test%2C%20also%20written%20as%20%CF%872%20test%2C,is%20used%20as%20short%20for%20Pearson%27s%20chi-squared%20test\" class=\"list-group-item\">Chi2 test<\/a>","d6243a91":" <a href=\"#a\" class=\"list-group-item\">Go to summary<\/a>","85a6f3e7":" <a href=\"#a\" class=\"list-group-item\">Go to summary<\/a>","b644ee59":"<div class=\"alert alert-info\" role=\"alert\"> <a style=\"font-weight: bold;\">Alert:<\/a> As the notebook is long, we placed at the beginning of each part this link to get back to the summary <a href=\"#a\" class=\"list-group-item\">Go to summary<\/a> <\/div>","65139967":"The results are really good! Bacteria class has the best statistics. We have a precision around 91% for normal class which is important, because classify a person as normal when it is false could be very dangerous. The inverse is also true and in this case we could do better, there are too much X-ray classified as bacteria or virus while they are normals.","d2de4a32":"<h2 style=\"background-color: #8db6cd; color:#FFFFFF\">4.4 - Conclusion <a id=dd><\/a><\/h2>","b11830b0":" <a href=\"#a\" class=\"list-group-item\">Go to summary<\/a>","9ffe6280":"Definition of 4 robusts metrics against imbalanced data","90027299":"<h3 style=\"background-color: #8db6cd; color:#FFFFFF\">6.2.1 - Examples <a id=fc><\/a><\/h3>","10104964":"Transfert learning seems to be the better model. However, bayesian model gives us lot of insights and can be more helpfull to doctors as the shap values gives clear areas. Moreover, it knows when it doesn't know. CatboostClassifier stands out for its interpretability. \n\nTo conclude, we have chosen the Catboost model as it gives real areas and insights. Also, its results were close to those of the MobileNetV2. This kind of model doesn't learn pattern but images are not expected to rotate, shift and everything else.\n\n# Thank you very much for having read my notebooks. Let's upvote it if you liked ","ff04c35e":"The following algorithms are known as factor methods as they are decomposing each matrix data into multiple ones. For example, PCA is using SVD decomposition and decompose the matrix \"train\" into $U\\Sigma V^*$ matrix where $V^*$ is the new space which maximize the variance. Therefore, the explained variance is concentrated on the firsts components of $V^*$. \n\n**For further read:**\n\n<a href=\"https:\/\/en.wikipedia.org\/wiki\/Singular_value_decomposition#:~:text=In%20linear%20algebra%2C%20the%20singular%20value%20decomposition%20%28SVD%29,matrix%20M%20is%20a%20factorization%20of%20the%20form\" class=\"list-group-item\">SVD decomposition<\/a>\n<a href=\"https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis\" class=\"list-group-item\">PCA<\/a>\n<a href=\"https:\/\/en.wikipedia.org\/wiki\/Factor_analysis\" class=\"list-group-item\">Factor analysis<\/a>\n<a href=\"https:\/\/en.wikipedia.org\/wiki\/Non-negative_matrix_factorization\" class=\"list-group-item\">Non negative matrix factorization<\/a>\n\nThe following code is understandable by itself. We are displaying the first 6 principals components of the new space for the three algorithm enumerate above.","11079c1a":"From the histograms, we can observe lot of pixels at 0 for normal lungs which could denote darker images, however in term of statistical data bacteria seems to have the lowest values.\n\nwe will not perform statistical tests here, given the values of count and std, we are fairly confident that the validation and training sets are identicals.","8e9cc286":" <a href=\"#a\" class=\"list-group-item\">Go to summary<\/a>","cc1f950e":"Informations from this part:\n\n - Imbalanced data\n - validation set ~ training set\n - Normal lungs -> pixels less intenses and better defined contours","970d62b4":"Ok, the results are not bad but less than CatboostClassifier. We could try an other model but this is not the goal here. We will show the history which gives good insight on the quality if the training. \n\n**Let's display the history**","51543b46":" <a href=\"#a\" class=\"list-group-item\">Go to summary<\/a>","a51af524":"Convinced ? Normal class and pneumonia class can be separated more easily in NMF projection.","389f3113":"<h1 style=\"background-color: #428bca; color:#FFFFFF\">4 - Curse of dimensionality: Eigenbodies or features importances <a id=d><\/a><\/h1>","79848457":"<h3 style=\"background-color: #8db6cd; color:#FFFFFF\">6.3.2 - Interpretability <a id=fg><\/a><\/h3>","405b7494":" <a href=\"#a\" class=\"list-group-item\">Go to summary<\/a>","1c48230a":" <a href=\"#a\" class=\"list-group-item\">Go to summary<\/a>","f149040a":"Here we can see that our model doubt. Previously, it classified the image because the associated probability was the higher but now this is not evident, two classes can be superposed. What does that mean ? We need more images like this one to prevent epistemic errors, in other terms it hesitates.","75ba8c35":" <a href=\"#a\" class=\"list-group-item\">Go to summary<\/a>","af666595":"Our implementation of CNN network doesn't convince me, the next step is pretrained model. In theory, they are faster to converge and give better results.","0729ef2d":"<h2 style=\"background-color: #8db6cd; color:#FFFFFF\">6.2 - Bayesian model <a id=fb><\/a><\/h2>","a1f822c6":"It makes more sense that the good one is bacteria class. Indeed, activated pixel for Virus class are located outside the body.","b8b4223b":"The developments brought about by artificial intelligence are gradually transforming various sectors, especially health, which represents a field with great potential. Indeed, diagnostic assistance using a machine learning algorithm can save many lives.\n\nFor this notebook, we will put ourselves in the shoes of engineers in charge of creating an intelligent algorithm that can diagnose a radiology of lungs as being healthy or sick. However, we are here to help doctors not to replace them, hence the interpretability and the uncertainty are as important as accuracy.\n\n**But what is pneumonia ?**\n\n> Pneumonia is an inflammatory condition of the lung primarily affecting the small air sacs known as alveoli.[3][14] Symptoms typically include some combination of productive or dry cough, chest pain, fever and difficulty breathing.[1] The severity of the condition is variable.[1]\n\n>Pneumonia is usually caused by infection with viruses or bacteria, and less commonly by other microorganisms.[a] Identifying the responsible pathogen can be difficult. Diagnosis is often based on symptoms and physical examination.[8] Chest X-rays, blood tests, and culture of the sputum may help confirm the diagnosis.[8] The disease may be classified by where it was acquired, such as community- or hospital-acquired or healthcare-associated pneumonia.[17]\n\n>Risk factors for pneumonia include cystic fibrosis, chronic obstructive pulmonary disease (COPD), sickle cell disease, asthma, diabetes, heart failure, a history of smoking, a poor ability to cough (such as following a stroke), and a weak immune system.[5][7]\n\n> from https:\/\/en.wikipedia.org\/wiki\/Pneumonia\n\nGood! After knowing that we propose you the following summary to dive into the dataset.","c50b871d":" <a href=\"#a\" class=\"list-group-item\">Go to summary<\/a>","e59bd96b":" <a href=\"#a\" class=\"list-group-item\">Go to summary<\/a>","9801edb9":"<h2 style=\"background-color: #8db6cd; color:#FFFFFF\">5.2 - Interpetability <a id=eb><\/a><\/h2>","e0f23a7d":"The results are a little worse than the previous model but that is not the point. Let's visualize.","8895f1dc":"As expected the PCA transformed data at the bottom right is not understandable by human. The three others shows in yellow their best 3000 features. However, we can see that pixels from chi2 and mutual_info are found at the periphery of the body, which can only lead to overfitting. PCA could also overfitting, moreover if we look at the 6 principal components above, border pixels are clearly more important in the new space. Therefore, f_classif seems to be a good algorithm and take the most important pixels into account.","bdbe1ffa":"<h1 style=\"background-color: #428bca; color:#FFFFFF\">Introduction<\/h1>","ed52fa32":"<h2 style=\"background-color: #8db6cd; color:#FFFFFF\">4.1 - Factor algorithms <a id=da><\/a><\/h2>","be96b94c":"<h2 style=\"background-color: #8db6cd; color:#FFFFFF\">6.1 - Deterministic model <a id=fa><\/a><\/h2>","7e01c8f8":"In the following cell, we are running each algorithms and sort index pixels by their score of informations or dependence. In case of PCA, we are doing nothing.","07d49910":"<h3><a href=\"#a\">1 - Import packages <\/a><\/h3>\n<h3><a href=\"#b\">2 - Data exploration<\/a><\/h3>\n<h4><a href=\"#ba\">2.1 - Train-validation-test<\/a><\/h4>\n<h4><a href=\"#bb\">2.2 - Sample<\/a><\/h4>\n<h4><a href=\"#bc\">2.3 - Conclusion<\/a><\/h4>\n<h3><a href=\"#c\">3 - Metrics used<\/a><\/h3>\n<h4><a href=\"#ca\">3.1 - Conclusion<\/a><\/h4>\n<h3><a href=\"#d\">4 - Curse of dimensionality: Eigenbodies or features importances<\/a><\/h3>\n<h4><a href=\"#da\">4.1 - Factor algorithms<\/a><\/h4>\n<h4><a href=\"#db\">4.2 - Feature selection<\/a><\/h4>\n<h4><a href=\"#dc\">4.3 - Examples<\/a><\/h4>\n<h4><a href=\"#dd\">4.4 - Conclusion<\/a><\/h4>\n<h3><a href=\"#e\">5 - Best models<\/a><\/h3>\n<h4><a href=\"#ea\">5.1 - Examples<\/a><\/h4>\n<h4><a href=\"#eb\">5.2 - Interpretability<\/a><\/h4>\n<h4><a href=\"#ec\">5.3 - Conclusion<\/a><\/h4>\n<h3><a href=\"#f\">6 - Neural Network<\/a><\/h3>\n<h4><a href=\"#fa\">6.1 - Deterministic model<\/a><\/h4>\n<h4><a href=\"#fb\">6.2 - Bayesian model<\/a><\/h4>\n<h4><a href=\"#fc\">6.4 - Examples<\/a><\/h4>\n<h4><a href=\"#fd\">6.5 - Interpretability<\/a><\/h4>\n<h4><a href=\"#fe\">6.3 - Transfer learning and fine tuning<\/a><\/h4>\n<h4><a href=\"#ff\">6.4 - Examples<\/a><\/h4>\n<h4><a href=\"#fg\">6.5 - Interpretability<\/a><\/h4>\n<h3><a href=\"#g\">7 - Final conclusion<\/a><\/h3>\n<hr>","248a2305":"<h3 style=\"background-color: #8db6cd; color:#FFFFFF\">6.3.1 - Examples <a id=ff><\/a><\/h3>"}}