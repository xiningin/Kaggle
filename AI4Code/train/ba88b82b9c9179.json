{"cell_type":{"3b1a55ba":"code","e9f6e00b":"code","cad55d19":"code","a63a794d":"code","b1ccb038":"code","1a350804":"code","c3c303bf":"code","18dbd4da":"markdown","37d156cd":"markdown","8ba4a9eb":"markdown"},"source":{"3b1a55ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e9f6e00b":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain_data.head()","cad55d19":"def feature_selection(data):\n    X = data[['Pclass', 'Age','SibSp', 'Parch', 'Fare']].to_numpy()\n\n    # map sex:\n    X_sex = data['Sex'].to_numpy()\n    smale = X_sex == 'male'\n    sfemale = X_sex == 'female'\n    X_sex = np.array(smale*1 + sfemale*-1)\n\n    X = np.append(X, X_sex[:,None], axis=1)\n\n    #map cabin:\n    X_cabin = pd.Series([i[0] if not pd.isnull(i) else '0' for i in data['Cabin'] ])\n    X_cabin = X_cabin.map({\"A\":1, \"B\" : 2 , \"C\":3, \"D\":4, \"E\":5, \"F\":6, \"G\":7,\"T\":8})\n    X = np.append(X, X_cabin[:,None], axis=1)\n\n    #map embarked:\n    X_emb = data['Embarked'].map({\"C\":1, \"S\" : 2 , \"Q\":3})\n    X = np.append(X, X_emb[:,None], axis=1)\n\n    # remove NaN:\n    sample_NaN = np.isnan(X)\n    X[sample_NaN] = 0\n\n    return(X)","a63a794d":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import BaggingClassifier\n\nX = feature_selection(train_data)\ny = train_data.Survived\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state=0)\n\n# pick samples:\n#n_samp = np.int8(np.ceil(Y_test.size*0.2))\n#X_train, Y_train = resample(X_train, Y_train, n_samples = n_samp, random_state=0, replace=False)\n\nclf = AdaBoostClassifier( n_estimators=100, random_state=0)\nclf.fit(X_train, Y_train)\nprint('Score:')\nprint(clf.score(X_test,Y_test))","b1ccb038":"from sklearn.metrics import plot_confusion_matrix\n\nparam_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"base_estimator__max_depth\": [None, 10,20,30,40,50,60]\n             }\n\nclf = AdaBoostClassifier(base_estimator = DecisionTreeClassifier() ,  n_estimators = 200, random_state=0)\n\n# run grid search\ngrid_search = GridSearchCV(clf, param_grid=param_grid, scoring = 'roc_auc')\n\ngrid_search.fit(X_train, Y_train)\nprint('Score with hyper parameter optimization:')\nprint(grid_search.score(X_test,Y_test))\nprint(grid_search.best_params_)","1a350804":"plot_confusion_matrix(grid_search, X_test, Y_test)","c3c303bf":"# compute submission:\nX = feature_selection(test_data)\nX_id = test_data[['PassengerId']]\npred =grid_search.predict(X)\noutput = np.array([X_id.to_numpy(),pred[:,None]]).T\ndf = pd.DataFrame(output[0,:,:],columns=['PassengerId', 'Survived'])\nprint(df)\ndf.to_csv('my_submission.csv', index=False)","18dbd4da":"Simple Feature Selection","37d156cd":"**Additional**: Grid Search Hyperparameter Optimization: \n\n(Boosts Score up to 84-87%)","8ba4a9eb":"**Idea:** Use a AdaBoost Ensemble binary classifier (least squares):"}}