{"cell_type":{"4bfabcad":"code","abddd5c3":"code","72e36c41":"code","fdad5da2":"code","d5375828":"code","20dda877":"code","5046a4fc":"code","6e87733f":"code","ff4a5ccc":"code","9f4fb52f":"code","dbea9a61":"code","6d2a1ca4":"code","0ff04665":"code","e9f37359":"code","adf18695":"code","3ce599c7":"code","7d0345df":"code","5cbadffc":"code","561ce5e8":"code","c7628099":"code","f41fc01e":"code","2e879046":"code","731c30c0":"code","1cba6b18":"code","5001cb19":"code","7b441f5c":"code","15c77506":"code","8b7a54d4":"code","d2a115b2":"code","850f5c08":"code","fb0eb112":"markdown","0abe73cf":"markdown","de1334f6":"markdown","58f773cf":"markdown","1716d88c":"markdown","349803ec":"markdown","a1a0b723":"markdown","e22b6ed0":"markdown","79c6e59f":"markdown","77a92f34":"markdown","39b6309e":"markdown","8ccde7a4":"markdown","c994c74c":"markdown","b67d35c6":"markdown","00ffea88":"markdown","3c40945f":"markdown","9f78080c":"markdown"},"source":{"4bfabcad":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","abddd5c3":"df = pd.read_csv('\/kaggle\/input\/dados-de-simulao-de-sep\/CB_166_1.txt', sep = ',', header = None, engine = 'python')\ndf = pd.DataFrame(data = df)\ndf","72e36c41":"#bibliotecas necess\u00e1rias\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\n#biblioteca para \"convers\u00e3o\" de variaveis categoricas (strings) em n\u00fameros (integer)\nfrom sklearn.preprocessing import LabelEncoder\n#biblioteca para criar tabela confus\u00e3o e medir precis\u00e3o\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier","fdad5da2":"previsores = df.iloc[:, 0:1114].values \nprevisores","d5375828":"from sklearn.preprocessing import MinMaxScaler","20dda877":"minmax = MinMaxScaler()\nprevisores = minmax.fit_transform(previsores)\nprevisores","5046a4fc":"classes = df.iloc[:, 1114].values\nclasses","6e87733f":"LEncoder = LabelEncoder()  #fun\u00e7\u00e3o label encoder \u00e9 o \"etiquetador\" das categorias\nclasses = LEncoder.fit_transform(classes)\nclasses ","ff4a5ccc":"x_train, x_test, y_train, y_test = train_test_split(previsores, classes, test_size = 0.3, random_state = 0)","9f4fb52f":"naiveb = GaussianNB()\nnaiveb.fit(x_train, y_train)","dbea9a61":"predicao = naiveb.predict(x_test)\npredicao","6d2a1ca4":"#comparando os dados previstos com dados anteriores por matriz de confus\u00e3o\nconfusion = confusion_matrix(predicao, y_test)\nconfusion","0ff04665":"accu = accuracy_score(predicao, y_test)\naccu","e9f37359":"from yellowbrick.classifier import ConfusionMatrix","adf18695":"viz = ConfusionMatrix(GaussianNB())\nviz.fit(x_train, y_train)\nviz.score(x_test, y_test)\nviz.poof()","3ce599c7":"from sklearn.tree import DecisionTreeClassifier, export_graphviz\nimport graphviz\n","7d0345df":"tree = DecisionTreeClassifier()\ntree.fit(x_train, y_train)","5cbadffc":"export_graphviz(tree, out_file ='tree.dot')","561ce5e8":"previsao = tree.predict(x_test)\nprevisao","c7628099":"conf = confusion_matrix(y_test, previsao)\nconf","f41fc01e":"acerto = accuracy_score(y_test, previsao)\nacerto","2e879046":"from sklearn.svm import SVC","731c30c0":"svm = SVC()\nsvm.fit(x_train, y_train)\nprev = svm.predict(x_test)\n","1cba6b18":"acerto_2 = accuracy_score(y_test, prev)\nacerto_2","5001cb19":"forest = ExtraTreesClassifier()\nforest.fit(x_train, y_train)\nimportance = forest.feature_importances_\nimportance","7b441f5c":"forest = RandomForestClassifier(n_estimators = 100)\nforest","15c77506":"forest.fit(x_train, y_train)","8b7a54d4":"predicties = forest.predict(x_test)\npredicties","d2a115b2":"confusion = confusion_matrix(predicties, y_test)\nconfusion","850f5c08":"accuracy_score(predicties, y_test)","fb0eb112":"### Dividindo o dataframe em previsores e classes para o treino (criar o modelo)","0abe73cf":"### O modelo criado por Random Forest \u00e9 o que apresentou maior percentual de acertos, atingindo at\u00e9 93.5%","de1334f6":"# Fazendo por \u00e1rvore de decis\u00e3o","58f773cf":"# Criando modelo de Naive Bayes para classifica\u00e7\u00f5es\n\n","1716d88c":"# Por Random Forest (m\u00faltiplas \u00e1rvores de decis\u00e3o)","349803ec":"### SVM: 71% de precis\u00e3o","a1a0b723":"## SVM e Sele\u00e7\u00e3o de atributos","e22b6ed0":"### Temos 1 para seguro e 0 para inseguro","79c6e59f":"### Naive Bayes: 90% de precis\u00e3o","77a92f34":"### Utilizando o train test split da scikit learn para dividir e organizar corretamente os dados para treino e teste (hold up, 70% dos dados s\u00e3o para treino, 30% s\u00e3o para testar a eficacia do modelo)","39b6309e":"## Visualizando a matriz confus\u00e3o","8ccde7a4":"#### Precis\u00e3o do modelo","c994c74c":"### Igualando as escalas dos atributos","b67d35c6":"### \u00c1rvore de decis\u00e3o: 77.4% de precis\u00e3o","00ffea88":"## Coleta de dados para manipula\u00e7\u00e3o","3c40945f":"### Transformando as vari\u00e1veis categ\u00f3ricas em num\u00e9ricas (uma vez que o classificador do Naive Bayes funciona apenas com n\u00fameros)","9f78080c":"## Aqui inicia-se o treino com Naive Bayes \n#### fun\u00e7\u00e3o GaussianNB da Scikit Learn"}}