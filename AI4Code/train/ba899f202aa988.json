{"cell_type":{"efa92be9":"code","ee3a95d3":"code","b8a81754":"code","33a697a3":"code","2e8f90d1":"code","601bb785":"code","2da29e81":"code","e9ab4247":"code","3fc1061e":"code","7e52d888":"code","3bb9bdb5":"code","db1f1eb1":"code","e4b3a037":"code","c78ce23c":"code","0f6c8e27":"code","9177dde7":"code","a978daac":"code","f957f3f0":"code","34f1d0ec":"code","3883db61":"code","2954eb0a":"code","742e5575":"code","0035e075":"code","a794939e":"code","ef71d263":"code","5ab35ce9":"code","bcf5f292":"code","b87691ba":"code","ebaef532":"code","aeaefe49":"code","2ce78ec5":"code","4703e03f":"code","73e89a6f":"code","b8db724c":"code","357fcedd":"markdown","854b475b":"markdown","17568253":"markdown","1c30aa81":"markdown","125e4950":"markdown","12603bfc":"markdown","82655db1":"markdown"},"source":{"efa92be9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ee3a95d3":"sample_submission = pd.read_csv(\"\/kaggle\/input\/analytics-vidhya-ltfs-2\/sample_submission.csv\", parse_dates=['application_date'])\ntrain = pd.read_csv(\"\/kaggle\/input\/analytics-vidhya-ltfs-2\/train.csv\",parse_dates=['application_date'])\ntest = pd.read_csv(\"\/kaggle\/input\/ltfs-2\/test_1eLl9Yf.csv\", parse_dates=['application_date'])\n","b8a81754":"train_1=train[train['segment']==1].groupby(['application_date']).sum().reset_index()[['application_date','case_count']].sort_values('application_date').set_index('application_date')\ntrain_2=train[train['segment']==2].groupby(['application_date']).sum().reset_index()[['application_date','case_count']].sort_values('application_date').set_index('application_date')\ntest_1=test[test['segment']==1][['application_date']].sort_values('application_date').set_index('application_date')\ntest_2=test[test['segment']==2][['application_date']].sort_values('application_date').set_index('application_date')","33a697a3":"train_1.loc[train_1.case_count>=8700,'case_count']=np.nan\n\n","2e8f90d1":"type(train_1.index)","601bb785":"train_1.index","2da29e81":"# setting 'application_date' as column \n\ntrain_1['application_date'] = train_1.index.get_level_values('application_date') \n\ntrain_2['application_date'] = train_2.index.get_level_values('application_date') \n\n\n# setting 'application_date' as column \n\ntest_1['application_date'] = test_1.index.get_level_values('application_date') \n\ntest_2['application_date'] = test_2.index.get_level_values('application_date')","e9ab4247":"train_1.columns, test_1.columns","3fc1061e":"#WE WILL CREATE FEATURES FROM \"APPLICATION_DATE\" COLUMN\n\n# THESE ARE THE STANDARDS FEATURES THAT WE ALWAYS MAKE IN CASE OF DATE COLUMN.\n\ndef create_features(df, label=None,seg=None):\n    \"\"\"\n    Creates time series features from datetime index.\n    \"\"\"\n    df = df.copy()\n    df['date'] = df.application_date\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['quarter'] = df['date'].dt.quarter\n    df['month'] = df['date'].dt.month\n    df['year'] = df['date'].dt.year\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['dayofmonth'] = df['date'].dt.day\n    df['week'] = df['date'].dt.week\n    df['is_month_start']=df['date'].dt.is_month_start\n    df['is_month_end']=df['date'].dt.is_month_end\n    df['is_quarter_start']=df['date'].dt.is_quarter_start\n    df['is_quarter_end']=df['date'].dt.is_quarter_end\n    df['is_year_start']=df['date'].dt.is_year_start\n    df['is_year_end']=df['date'].dt.is_year_end\n    df['Semester'] = np.where(df['quarter'].isin([1,2]),1,2)\n    \n    X = df.drop(['date','application_date'],axis=1)\n    if label:\n        y = df[label]\n        return X\n    return X\n\nXtrain_1 = create_features(train_1, label='case_count')\n\nXtest_1= create_features(test_1)","7e52d888":"Xtrain_2 = create_features(train_2, label='case_count')\n\nXtest_2= create_features(test_2 )","3bb9bdb5":"train_1['date'] = train_1.application_date","db1f1eb1":"Xtrain_1.head()\nprint(\"Shape of DataFrame: {}\".format(Xtrain_1.shape))\n\n#'Train shape: {}'.format(df.loc[df.train_or_test=='train',:].shape))","e4b3a037":"Xtrain_2.head()\nprint(\"Shape of Dataframe: {}\".format(Xtrain_2.shape))","c78ce23c":"Xtrain_1.columns, Xtrain_2.columns, Xtest_1.columns, Xtest_2.columns","0f6c8e27":"# Features constructed from previous case_count values\n\n# Creating case_count lag features\ndef create_case_count_lag_feats(df, gpby_cols, target_col, lags):\n    gpby = df.groupby(gpby_cols)\n    for i in lags:\n        df['_'.join([target_col, 'lag', str(i)])] = \\\n                gpby[target_col].shift(i).values + np.random.normal(scale=1.6, size=(len(df),))\n    return df\n\n# Creating case_count rolling mean features\ndef create_case_count_rmean_feats(df, gpby_cols, target_col, windows, min_periods=2, \n                             shift=1, win_type=None):\n    gpby = df.groupby(gpby_cols)\n    for w in windows:\n        df['_'.join([target_col, 'rmean',str(shift), str(w)])] = \\\n            gpby[target_col].shift(shift).rolling(window=w, \n                                                  min_periods=min_periods,\n                                                  win_type=win_type).mean().values + np.random.normal(scale=1.6, size=(len(df),))\n    return df\n\n# Creating case_count exponentially weighted mean features\ndef create_case_count_ewm_feats(df, gpby_cols, target_col, alpha=[0.9], shift=[1]):\n    gpby = df.groupby(gpby_cols)\n    for a in alpha:\n        for s in shift:\n            df['_'.join([target_col, 'lag', str(s), 'ewm', str(a)])] = \\\n                gpby[target_col].shift(s).ewm(alpha=a).mean().values + np.random.normal(scale=1.6, size=(len(df),))\n    return df","9177dde7":"# ONE HOT ENCODER OF CATEGORICAL FEATURES\n\ndef one_hot_encoder(df, ohe_cols=['dayofweek', 'quarter', 'month', 'dayofyear','dayofmonth', 'week']):\n    '''\n    One-Hot Encoder function\n    '''\n    print('Creating OHE features..\\nOld df shape:{}'.format(df.shape))\n    df = pd.get_dummies(df, columns=ohe_cols)\n    print('New df shape:{}'.format(df.shape))\n    return df\n","a978daac":"Xtrain_1.shape, Xtrain_2.shape","f957f3f0":"Xtrain_1['segment'] = 1\nXtrain_2['segment'] = 2\n\nXtest_1['segment'] = 1\nXtest_2['segment'] = 2","34f1d0ec":"train = pd.concat([Xtrain_1, Xtrain_2])\n\ntrain['train_or_test'] = 'train'\n\ntrain.shape","3883db61":"train.head()","2954eb0a":"train.tail()","742e5575":"test = pd.concat([Xtest_1, Xtest_2])\n\n\ntest['train_or_test'] = 'test'\n\ntest.shape\n","0035e075":"# Taking log of 'case_count' column \n\ntrain['case_count'] = np.log1p(train.case_count.values)\n","a794939e":"df = pd.concat([train,test], sort=False)\n","ef71d263":"df['application_date'] = df.index.get_level_values('application_date') ","5ab35ce9":"df.application_date.min(), df.application_date.max()","bcf5f292":"# Time-based Validation set\n\n# For validation to keep months also identical to test set we can choose period (same of 2018) as the validation set.\n\nmasked_series = (df['application_date'] >= '2018-07-06') & (df['application_date'] <= '2018-10-24')\nmasked_series2 = (df['application_date'] < '2018-07-06') & (df['application_date'] > '2018-10-24')\ndf.loc[(masked_series), 'train_or_test'] = 'val'\ndf.loc[(masked_series2), 'train_or_test'] = 'no_train'\nprint('Train shape: {}'.format(df.loc[df.train_or_test=='train',:].shape))\nprint('Validation shape: {}'.format(df.loc[df.train_or_test=='val',:].shape))\nprint('No train shape: {}'.format(df.loc[df.train_or_test=='no_train',:].shape))\nprint('Test shape: {}'.format(df.loc[df.train_or_test=='test',:].shape))","b87691ba":"## Creating case_count lag, rolling mean, rolling median, ohe features of the above train set\ntrain = create_case_count_lag_feats(train, gpby_cols=['segment'], target_col='case_count', \n                                    lags=[91,98,105,112,119,126,182,364,546,728])\n\ntrain = create_case_count_rmean_feats(train, gpby_cols=['segment'], \n                                 target_col='case_count', windows=[364,546], \n                                 min_periods=10, win_type='triang')\n\ntrain = create_case_count_ewm_feats(train, gpby_cols=['segment'], \n                               target_col='case_count', \n                               alpha=[0.95, 0.9, 0.8, 0.7, 0.6, 0.5], \n                               shift=[91,98,105,112,119,126,182,364,546,728])\n","ebaef532":"# Converting case_count of validation period to nan so as to resemble test period\ntrain = df.loc[df.train_or_test.isin(['train','val']), :]\nY_val = train.loc[train.train_or_test=='val', 'case_count'].values.reshape((-1))\nY_train = train.loc[train.train_or_test=='train', 'case_count'].values.reshape((-1))\ntrain.loc[train.train_or_test=='val', 'case_count'] = np.nan\n","aeaefe49":"# # Creating case_count lag, rolling mean, rolling median, ohe features of the above train set\ntrain = create_case_count_lag_feats(train, gpby_cols=['segment'], target_col='case_count', \n                                    lags=[91,98,105,112,119,126,182,364,546,728])\n\ntrain = create_case_count_rmean_feats(train, gpby_cols=['segment'], \n                                 target_col='case_count', windows=[364,546], \n                                 min_periods=10, win_type='triang')\n\ntrain = create_case_count_ewm_feats(train, gpby_cols=['segment'], \n                               target_col='case_count', \n                               alpha=[0.95, 0.9, 0.8, 0.7, 0.6, 0.5], \n                               shift=[91,98,105,112,119,126,182,364,546,728])\n\n# One-Hot Encoding\ntrain = one_hot_encoder(train) \n\n# Final train and val datasets\nval = train.loc[train.train_or_test=='val', :]\ntrain = train.loc[train.train_or_test=='train', :]\nprint('Train shape:{}, Val shape:{}'.format(train.shape, val.shape))","2ce78ec5":"import warnings\nwarnings.filterwarnings(\"ignore\")","4703e03f":"avoid_cols = ['application_date', 'case_count', 'train_or_test', 'id', 'year','is_month_start']\ncols = [col for col in train.columns if col not in avoid_cols]\nprint('No of training features: {} \\nAnd they are:{}'.format(len(cols), cols))","73e89a6f":"train_x=train[cols]\ntest_x=val[cols]\ntrain_y=Y_train\ntest_y=Y_val\ntrain_x.shape, test_x.shape, train_y.shape, test_y.shape","b8db724c":"train_x.fillna(0,inplace=True)\ntest_x.fillna(0,inplace=True)","357fcedd":"*THIS NOTEBOOK IS SECOND IN SERIES OF THREE NOTEBOOKS *\n*     *FIRST ONE IS FOR DATA VISUALIZATION *\n*     ### **SECOND ONE IS FOR FEATURE GENERATION **\n*     *THIRD ONE IS FOR MODELLING *\n    \n\n ","854b475b":"*WE WILL GENERATE NEW FEATURES AND THEN DO ONE HOT ENCODING *","17568253":"[https:\/\/www.analyticsvidhya.com\/blog\/2019\/12\/6-powerful-feature-engineering-techniques-time-series\/](http:\/\/)","1c30aa81":"### WE CAN CREATE FOLLOWING TYPE OF FETURES ON TIME SERIES DATA\n    1. TIME BASED FEATURES\n        *USING THE TIME OF DATETIME COLUMN*\n    2. DATE BASED FEATURES\n        *USING THE DATE OF DATETIME COLUMN*\n    3. TIME LAG BASED FEATURES\n        *BECAUSE TIME SERIES DATA DEPENDS ON PREVIOUS VALUES *\n    4. ROLLING MEAN FEATURES\n        *BASED ON VALUES ROLLING AT FIXED INTERVAL*\n    5. WEIGHTED MEAN FEATURES\n        *BASED ON WEIGHT DECAYS*","125e4950":"# FEATURE GENERATION","12603bfc":"# LTFS Data Science FinHack 2\nLTFS receives a lot of requests for its various finance offerings that include housing loan, two-wheeler loan, real estate financing and micro loans. The number of applications received is something that varies a lot with season. Going through these applications is a manual process and is tedious. Accurately forecasting the number of cases received can help with resource and manpower management resulting into quick response on applications and more efficient processing.\n    \n    \n # Problem Statement\n \n You have been appointed with the task of forecasting daily cases for next 3 months for 2 different business segments aggregated at the country level keeping in consideration the following major Indian festivals (inclusive but not exhaustive list): Diwali, Dussehra, Ganesh Chaturthi, Navratri, Holi etc. (You are free to use any publicly available open source external datasets). Some other examples could be:\n     Weather Macroeconomic variables Note that the external dataset must belong to a reliable source.\n    Data Dictionary The train data has been provided in the following way:\n    For business segment 1, historical data has been made available at branch ID level For business segment 2, historical data has been made available at State level.\n    Train File Variable Definition application_date Date of application segment Business Segment (1\/2) branch_id Anonymised id for branch at which application was received state State in which application was received (Karnataka, MP etc.) zone Zone of state in which application was received (Central, East etc.) case_count (Target) Number of cases\/applications received\n    Test File Forecasting needs to be done at country level for the dates provided in test set for each segment.\n    Variable Definition id Unique id for each sample in test set application_date Date of application segment Business Segment (1\/2)\n    \n    \n# Evaluation\n\n Evaluation Metric The evaluation metric for scoring the forecasts is *MAPE (Mean Absolute Percentage Error) M with the formula:\n Where At is the actual value and Ft is the forecast value.\n The Final score is calculated using MAPE for both the segments using the formula\n    \n","82655db1":"### ADDITIONAL FEATURE GENERATION"}}