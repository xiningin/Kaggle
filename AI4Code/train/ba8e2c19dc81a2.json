{"cell_type":{"ca3c6833":"code","bc9dad73":"code","afe8e037":"code","28644c1e":"code","8e49a2f2":"code","f2d96c94":"code","7aca6ea8":"code","b2e816dc":"code","825d0aa8":"code","fbe2d05a":"code","964af3c4":"code","70b6fd60":"code","6fb389d8":"code","8cb81970":"markdown","d3d59f14":"markdown","9dda50cf":"markdown","83680b20":"markdown"},"source":{"ca3c6833":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        data = os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bc9dad73":"!pip install transformers\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport re\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport transformers\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup","afe8e037":"BATCH_SIZE = 16\nMAX_LEN = 512\nEPOCHS = 10\n\nTOKENIZER = BertTokenizer.from_pretrained('bert-base-cased')\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","28644c1e":"df = pd.read_csv(data)\nprint(df.info())\n\nsns.countplot(x=df.sentiment)","8e49a2f2":"def remove_html_tags(string):\n    result = re.sub('<.*?>','',string)\n    return result\n\ndf.sentiment = df.sentiment.apply(lambda x: 1 if x == \"positive\" else 0)\ndf.review = df.review.apply(lambda x : remove_html_tags(x))\n\ntrain, test = train_test_split(df, test_size=0.2)\ntrain, test = train.reset_index(drop=True), test.reset_index(drop=True) \n\nprint(train)","f2d96c94":"class BERTDataset:\n    def __init__(self, review, sentiment):\n        self.review = review\n        self.sentiment = sentiment\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n\n    def __len__(self):\n        return len(self.review)\n\n    def __getitem__(self, item):\n        review = str(self.review[item])\n\n        encoding = self.tokenizer.encode_plus(review, None, add_special_tokens=True, \n            max_length=self.max_len, padding='max_length', truncation= True)\n        \n        return {   \n            \"ids\": torch.tensor(encoding[\"input_ids\"], dtype=torch.long),\n            \"mask\": torch.tensor(encoding[\"attention_mask\"], dtype=torch.long),\n            \"token_type_ids\": torch.tensor(encoding[\"token_type_ids\"], dtype=torch.long),\n            \"sentiments\": torch.tensor(self.sentiment[item], dtype=torch.float),\n        }\n\ndef create_data_loader(df):\n    ds = BERTDataset(review=df.review.values, sentiment=df.sentiment.values)\n    return DataLoader(ds, batch_size=BATCH_SIZE, num_workers=2)\n\ntrain_data_loader = create_data_loader(train)\ntest_data_loader = create_data_loader(test)\n\ndata = next(iter(train_data_loader))\nprint(data.keys())\nprint(data[\"ids\"].shape)","7aca6ea8":"class BERTSentiment(nn.Module):\n    def __init__(self):\n        super(BERTSentiment, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-cased', return_dict=False)\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size,1) \n        \n    def forward(self, ids, mask, token_type_ids):\n        _, pooled_output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids) \n        return self.out(self.drop(pooled_output))","b2e816dc":"model = BERTSentiment()\nmodel = model.to(device)","825d0aa8":"def train_model(data_loader, model, optimizer, scheduler, loss_fn):\n    model.train()\n    avg_losses = []\n    print_freq = 100\n    \n    for epoch in range(EPOCHS):\n        running_loss = 0.0\n        for i,d in enumerate(data_loader):\n            ids = d[\"ids\"].to(device, dtype=torch.long)\n            token_type_ids = d[\"token_type_ids\"].to(device, dtype=torch.long)\n            mask = d[\"mask\"].to(device, dtype=torch.long)\n            sentiments = d[\"sentiments\"].to(device, dtype=torch.float)\n\n            optimizer.zero_grad()\n\n            output = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n            loss = loss_fn(output, sentiments.view(-1,1))\n\n            loss.backward()\n\n            optimizer.step()\n            scheduler.step()\n            \n            # Print statistics.\n            running_loss += loss.item()\n            if i % print_freq == print_freq - 1: # Print every several mini-batches.\n                avg_loss = running_loss \/ print_freq\n                print('[epoch: {}, i: {:5d}] avg mini-batch loss: {:.3f}'.format(epoch+1, i+1, avg_loss))\n                avg_losses.append(avg_loss)\n                running_loss = 0.0\n    print('Finished Training.')\n    return avg_losses","fbe2d05a":"def eval_model(data_loader, model):\n    model.train()\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for d  in data_loader:\n            ids = d[\"ids\"].to(device, dtype=torch.long)\n            token_type_ids = d[\"token_type_ids\"].to(device, dtype=torch.long)\n            mask = d[\"mask\"].to(device, dtype=torch.long)\n            sentiments = d[\"sentiments\"].to(device, dtype=torch.float)\n\n            output = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n            \n            predicted = (torch.sigmoid(output)).cpu().numpy() >=0.5\n            predicted = np.concatenate(predicted).ravel()  #flaten a list of numpy array 1x16 to 16\n            sentiments = sentiments.cpu().numpy()\n            \n            total += len(sentiments)\n            correct += (predicted == sentiments).sum().item()\n            \n    print('Accuracy of the network on the test data: %d %%' % (100 * correct \/ total))","964af3c4":"total_step = len(train_data_loader)*EPOCHS\noptimizer = AdamW(model.parameters(), lr = 2e-5, correct_bias= False)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_step)\nloss_fn = nn.BCEWithLogitsLoss().to(device)","70b6fd60":"train_loss = train_model(train_data_loader, model, optimizer, scheduler,loss_fn)\n\nplt.plot(train_loss)\nplt.xlabel('mini-batch index \/ {}'.format(100))\nplt.ylabel('avg. mini-batch loss')\nplt.show()","6fb389d8":"eval_model(test_data_loader, model)","8cb81970":"# Preparing dataset for Transformer","d3d59f14":"# Data Cleaning\/ Preprocessing","9dda50cf":"# Model","83680b20":"# Traning\/ Testing"}}