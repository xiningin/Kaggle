{"cell_type":{"710020c5":"code","667d777e":"code","7e9342bb":"code","3aca9240":"code","ac444351":"code","735665fb":"code","781c1d22":"code","86274959":"code","a43f331e":"code","6f609fa6":"code","cde55a0c":"code","86d538fb":"code","8f3803b9":"code","bfb23533":"code","6487e877":"code","f64fc002":"code","b6c865b8":"markdown","8d5468c2":"markdown","2dc038b2":"markdown","33c48f83":"markdown","957e23d5":"markdown","b3f52880":"markdown","f410fa1b":"markdown","830d1b9b":"markdown","aa007f8c":"markdown","fe59edf7":"markdown","b60583e5":"markdown","de4c2bd6":"markdown","588e2ce9":"markdown","e477a8df":"markdown"},"source":{"710020c5":"import numpy as np\nimport pandas as pd\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_validate\n\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, f_regression\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.neural_network import MLPRegressor\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n\nimport random\nimport time\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","667d777e":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\nprint('train dataset size:', train.shape)\nprint('test dataset size:', test.shape)\ntrain.sample(4)","7e9342bb":"def proc_json(string, key):\n    try:\n        data = eval(string)\n        return \",\".join([d[key] for d in data])\n    except:\n        return ''\n\ndef proc_json_len(string):\n    try:\n        data = eval(string)\n        return len(data)\n    except:\n        return 0\n\n    \ndef feature_engineering(df):\n    # missing values\n    df.runtime.fillna(0, inplace=True)\n    df.status.fillna('Released', inplace=True)\n    df.release_date.fillna(df.release_date.mode()[0], inplace=True)\n    \n    # create count features\n    df['count_genre'] = df.genres.apply(proc_json_len)\n    df['count_country'] = df.production_countries.apply(proc_json_len)\n    df['count_company'] = df.production_companies.apply(proc_json_len)\n    df['count_splang'] = df.spoken_languages.apply(proc_json_len)\n    df['count_cast'] = df.cast.apply(proc_json_len)\n    df['count_crew'] = df.crew.apply(proc_json_len)\n    df['count_staff'] = df.count_cast + df.count_crew\n    df['count_keyword'] = df.Keywords.apply(proc_json_len)\n    \n    # convert json features\n    df.belongs_to_collection = df.belongs_to_collection.apply(lambda x: proc_json(x, 'name'))\n    df.genres = df.genres.apply(lambda x: proc_json(x, 'name'))\n    df.production_companies = df.production_companies.apply(lambda x: proc_json(x, 'name'))\n    df.production_countries = df.production_countries.apply(lambda x: proc_json(x, 'iso_3166_1'))\n    df.spoken_languages = df.spoken_languages.apply(lambda x: proc_json(x, 'iso_639_1'))\n    df.Keywords = df.Keywords.apply(lambda x: proc_json(x, 'name'))\n    \n    # create length of text features\n    df['len_title'] = df.title.str.len()\n    df.len_title.fillna(0, inplace=True)\n    df['len_overview'] = df.overview.str.len()\n    df.len_overview.fillna(0, inplace=True)\n    df['len_tagline'] = df.tagline.str.len()\n    df.len_tagline.fillna(0, inplace=True)\n    \n    # create category code features\n    df['code_origlang'] = df.original_language.astype('category').cat.codes\n    \n    # create date related features\n    df.release_date = pd.to_datetime(df.release_date)\n    df['release_year'] = df.release_date.dt.year\n    df['release_year'] = df.release_year.apply(lambda x: x-100 if x > 2020 else x)\n    df['release_month'] = df.release_date.dt.month\n    df['release_wday'] = df.release_date.dt.dayofweek\n\n    # create boolean features\n    df['in_collection'] = (df.belongs_to_collection != '').astype('uint8')\n    df['us_country'] = df.production_countries.str.contains('US').astype('uint8')\n    df['en_lang'] = (df.original_language == 'en').astype('uint8')\n    df['has_hompage'] = df.homepage.apply(lambda x: 1 if pd.isnull(x) == False else 0)\n\n    # log money values\n    if 'revenue' in df.columns:\n        df.revenue = np.log1p(df.revenue)\n    df.budget = np.log1p(df.budget)\n    df.popularity = np.log1p(df.popularity)\n    \n    return df\n","3aca9240":"train = feature_engineering(train)\ntest = feature_engineering(test)","ac444351":"train.info()","735665fb":"all_features = train.select_dtypes(include=['int64', 'float64', 'uint8', 'int8']).columns.tolist()\nall_features.remove('id')\n\nplt.figure(figsize=(18,18))\ncorrelations = train[all_features].corr()\nsns.heatmap(correlations, annot=True, fmt='.2', center=0.0, cmap='RdBu_r')\nplt.show()\n\ntarget = 'revenue'\nall_features.remove(target)","781c1d22":"def select_model(X, Y):\n\n    best_models = {}\n    models = [\n        {   'name': 'LinearRegression',\n            'estimator': LinearRegression() \n        },\n        {   'name': 'KNeighborsRegressor',\n            'estimator': KNeighborsRegressor(),\n        },\n        {   'name': 'RandomForestRegressor',\n            'estimator': RandomForestRegressor(),\n        },\n        {   'name': 'MLPRegressor',\n            'estimator': MLPRegressor(),\n        },\n        {   'name': 'GradientBoostingRegressor',\n            'estimator': GradientBoostingRegressor(),\n        },\n        {   'name': 'XGBoost',\n            'estimator': XGBRegressor(),\n        },\n        {   'name': 'LightGBM',\n            'estimator': LGBMRegressor(),\n        },\n        {   'name': 'CatBoost',\n            'estimator': CatBoostRegressor(verbose=False),\n        }\n        \n    ]\n    \n    for model in tqdm(models):\n        start = time.perf_counter()\n        grid = GridSearchCV(model['estimator'], param_grid={}, cv=5, scoring = \"neg_mean_squared_error\", verbose=False, n_jobs=-1)\n        grid.fit(X, Y)\n        best_models[model['name']] = {'score': grid.best_score_, 'params': grid.best_params_, 'model':model['estimator']}\n        run = time.perf_counter() - start\n        \n    return best_models\n\nmodels = select_model(train[all_features], train[target])\nmodels","86274959":"best_model = None\nmax_score = -100\nbest_model_name = ''\n\nfor m in models:\n    if models[m]['score'] > max_score:\n        max_score = models[m]['score']\n        best_model = models[m]['model']\n        best_model_name = m\n        \nprint(best_model_name, max_score)","a43f331e":"base_model = XGBRegressor()","6f609fa6":"corr_features = correlations.loc[correlations.revenue >= 0.1, 'revenue'].sort_values(ascending=False).index.tolist()\ncorr_features.remove('revenue')\ncorr_features","cde55a0c":"from sklearn.decomposition import PCA\npca = PCA(n_components=3)\nfit = pca.fit(train[all_features])\n\nprint(pca.explained_variance_ratio_)","86d538fb":"# the first PC is enough to show the variance.\nfeature_df = pd.DataFrame({'feature': all_features, 'importance': abs( pca.components_[0])})\nfeature_df.sort_values(by='importance', ascending=False, inplace=True)\n\npca_features = feature_df.feature[:15]\npca_features","8f3803b9":"from mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n\nX, y = train[all_features], train[target]\n\nsfs = SFS(estimator=base_model, \n           k_features=(3, 15),\n           forward=True, \n           floating=False, \n           scoring='neg_mean_squared_error',\n           cv=5)\n\nsfs.fit(X, y, custom_feature_names=all_features)\n\nprint('best combination (ACC: %.3f): %s\\n' % (sfs.k_score_, sfs.k_feature_names_))\n\nfig = plot_sfs(sfs.get_metric_dict(), kind='std_err')\nplt.title('Sequential Forward Selection (w. StdErr)')\nplt.grid()\nplt.show()\n\nsfs_features = list(sfs.k_feature_names_)","bfb23533":"def get_accuracy(features):\n    X, y = train[features], train['revenue']\n    \n    result = cross_validate(base_model, X, y, cv=5, scoring=\"neg_mean_squared_error\", verbose=False, n_jobs=-1)\n    return np.mean(result['test_score'])\n\n\nbest_features = None\nbest_accuracy = None\nbest_idx = -1\n\nfeature_candidates = [all_features, corr_features, pca_features, sfs_features]\nfor idx, flist in enumerate(feature_candidates):\n    acc = get_accuracy(flist)\n    if best_accuracy is None or acc > best_accuracy:\n        best_accuracy = acc\n        best_features = flist\n        best_idx = idx\n        \nprint(best_idx)\nprint(best_features)\nprint(best_accuracy)\n","6487e877":"hyperparameters = {\n    'max_depth': range(1, 12, 2),\n    'n_estimators': range(90, 201, 10),\n    'min_child_weight': range(1, 8, 2),\n    'learning_rate': [.05, .1, .15],\n}\n\ngrid = GridSearchCV(base_model, param_grid=hyperparameters, cv=5, scoring = \"neg_mean_squared_error\", verbose=True, n_jobs=-1)\ngrid.fit(train[best_features], train[target])\nprint('score = {}\\nparams={}'.format(grid.best_score_, grid.best_params_))","f64fc002":"opt_model = XGBRegressor(learning_rate=0.15, max_depth=3, min_child_weight=5, n_estimators=100)\nopt_model.fit(train[best_features], train[target], eval_metric='rmse')\npredict = opt_model.predict(test[best_features])\n\nsubmit = pd.DataFrame({'id': test.id, 'revenue':np.expm1(predict)})\nsubmit.to_csv('submission.csv', index=False)","b6c865b8":"\ud83d\udccd Feature candidate #4 : **sfs_features** - features selected by Sequential Feature Selection.","8d5468c2":"## Model Selection","2dc038b2":"<a id='2'><\/a>\n# [2. Feature Engineering](#999)\n","33c48f83":"#### CatBoost is the best but it takes too long, so I choose XGBoostRegressor.","957e23d5":"\ud83d\udccd Feature candidate #2 : **corr_features** - features that has high correlation(more than 0.1) with `revenue`.","b3f52880":"There are 8 JSON-style features, 4 numerical, 4 text, and 1 date feature.","f410fa1b":"<a id='1'><\/a>\n## 1. Loading Data","830d1b9b":"## Select the best features","aa007f8c":"<a id='3'><\/a>\n# [3. Feature & Model Selection](#999)\n\nModel selection, feature selection, and model tuning are closely inter-related and really confused process for us. The reason is:\n* Model selection depends on the feature selection:\n> Usually we cross-validate several models and select the best one. But models have their inherent workflow, so their CV scores depend largely on what kind of features we use. From this reason, it is hopeful to select model after we selected features to use.\n* Feature importance can be calculated by models.\n> Feature selection is the part of machine learning which needs domain knowledge the most. Many machine learning engineers do EDA and choose candidates by their experience and intuition. But the feature importances are scientifically proved only when we test them on models. And the feature importances vary according to the characteristics of model we use.\n> \n> Then, which is the first, hen or egg?\n* Model tuning makes things worse. \n> Suppose model A is worse than B at a point, but when we find a suitable set of parameters for A, it might be better than B with any set of parameters. Of course we can do model selection and tuning at the same time but it takes too long as every model has nearly unlimited set of parameters.\n \nThis is the main problem for us to build a model. \n\nFor now, we decided to do them in this order: \n1. Cross validate default models(without parameters) with all features and find the best one.\n1. Get cadidates of feature set using several methods(correlation based, PCA based, SFS based, and so on).\n1. Use the cadidates in the best model and choose the final feature set.\n1. Get the best parameters roughly using GridSearchCV.\n\n_If you have a better idea, please comment it and help us!_\n\n\ud83d\udccd Feature candidate #1 : **all_features** - all numerical features","fe59edf7":"SFS features were selected as the best one.","b60583e5":"<a id='999'><\/a>\n\n<img src='https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/10300\/logos\/thumb76_76.png' align='right' \/>\n\nIn this kernel we tried to show every corner of traditional machine learning from data loading to the final prediction. \n\nHere the main problem was [Chapter III. Feature & Model Selection](#3). I wish your valuable opinions about my work of this part.\n\nWe have a little data to predict TMDB movie's office box revenue: only 3,000 data items for the train and 4,400 for the test. It results in that it is not appropriate for this model to use modern ML techniques such as deep learning. We think it would be better to use traditional ML algorithms, where domain knowledge is needed for major steps of building model such as feature engineering and feature selection. \n\n\n1. [Loading Data](#1)\n1. [Feature Engineering](#2)\n1. [Feature & Model Selection](#3)\n1. [Parameter Tunning](#4)\n1. [Train and Predict](#5)\n","de4c2bd6":"\ud83d\udccd Feature candidate #3 : **pca_features** - features that has high weights for the primary component.","588e2ce9":"<a id='4'><\/a>\n# [4. Parameter Tunning](#999)","e477a8df":"<a id='5'><\/a>\n# [5. Train and Predict](#999)"}}