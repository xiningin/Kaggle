{"cell_type":{"f797fb1a":"code","66500797":"code","a5877158":"code","2ba9614a":"code","a67bfcc4":"code","20bc32e8":"code","7ac3b571":"code","c4173306":"code","ee50d1b7":"code","12981b2a":"code","d1a03401":"code","b5499efc":"code","1f3ecea7":"code","f9bfcbb8":"code","d35e3f6a":"code","ccd7ad90":"code","4b7b81e7":"code","4f4a7e42":"code","2a9c01f7":"code","43d449b2":"code","caef1bc0":"code","19264cea":"code","45df576d":"code","e4168e85":"code","b3d5c69a":"code","c60bed6d":"code","60aba0f8":"code","6a1afeef":"code","f2eb791a":"code","a6d2f869":"code","1fe01d2c":"code","27eaf871":"code","faf326d0":"code","8045da8b":"code","a38e5bbf":"code","26f2c86b":"code","a73ea956":"code","2387759c":"code","36e1b1b0":"code","ae259158":"code","92177593":"code","1567f9e4":"code","a276ec34":"code","27a96574":"code","c9d3cb01":"code","842a68f0":"code","529cab6e":"code","af03eb6a":"code","a5630bdc":"code","1bfb8585":"code","79690002":"code","35bef26d":"code","18227408":"code","1c269b9f":"code","bb9c63e8":"code","90d43ea5":"code","df58a291":"code","a96e3225":"code","36209cc3":"code","acc44eb2":"code","916486be":"code","50dc1763":"code","2e71e6e7":"code","e51af91a":"code","b5317bbf":"code","5bdeab23":"code","53518140":"code","ffc391e2":"code","0499bc2a":"code","2e1ce877":"code","5c50c1fd":"code","d41be287":"code","398db8de":"code","e33f4662":"code","d4760751":"code","fd301e54":"code","11b04d7a":"code","bb514881":"code","b28c32be":"code","f957cd83":"code","778e091b":"code","9d74b7f4":"code","c53acc37":"code","c4e70730":"code","f69013d4":"code","c879e855":"code","4cbb7e45":"code","8bde4b51":"code","f0aeb633":"code","629285f4":"code","031cec1e":"code","38f87481":"code","7979648c":"code","4ae10759":"code","105c141c":"code","fe50b554":"code","688946c3":"code","f6e52fcc":"code","b6fcbec6":"code","59bc8b52":"code","126f1159":"code","b196a84d":"code","8d8763b6":"code","9ced1bb4":"code","66a0ba01":"code","ad447038":"code","d5e6a8b8":"code","6cc08943":"code","71414947":"code","f3a6134e":"code","23a3bedc":"code","9f3e1d9a":"code","270e60cc":"code","39856fc9":"code","7023a428":"code","7de98a67":"code","ce989b05":"code","be1bec13":"code","b2677600":"code","4235e971":"code","942fafa7":"code","b32b182a":"code","2ae6be06":"code","6921f24f":"code","9729be2c":"code","2a287c57":"code","2b6bcb03":"markdown","600a9b8b":"markdown","7721d328":"markdown","86822b76":"markdown","32056c55":"markdown","fb212e34":"markdown","88439ec2":"markdown","04a84c5f":"markdown","fba0ce97":"markdown","58020224":"markdown","21a29fd9":"markdown","400af34f":"markdown","60bd9230":"markdown","717002f9":"markdown","901f0ece":"markdown","3da4caa1":"markdown","1ea78ee0":"markdown","2663a774":"markdown","bd507575":"markdown","8dfa5f49":"markdown","66b4fa2f":"markdown","0b44e415":"markdown","f8ad6573":"markdown","979538fe":"markdown","69f8862c":"markdown","65f38ca4":"markdown","0adfbc00":"markdown","678e4768":"markdown","79a83101":"markdown","c1074947":"markdown","8c5bfe44":"markdown","e71de158":"markdown","4c45acfc":"markdown","8a8f1b96":"markdown","1d3490c2":"markdown","55f46bab":"markdown","ce4fbe60":"markdown","6d8fe77d":"markdown","13bec6c8":"markdown","9aa1a8cc":"markdown","78d759e7":"markdown","8e0de7db":"markdown","0177a2f1":"markdown","3ea7065f":"markdown","dcd50260":"markdown","212e69b7":"markdown","3b41576e":"markdown","0c7e78b7":"markdown","cd0d10a8":"markdown","4df0befc":"markdown","de3acd35":"markdown","3d185af6":"markdown","739daec2":"markdown","fe8a3863":"markdown","634b0d48":"markdown","a35932f0":"markdown","3bbab997":"markdown","6c391fca":"markdown","68af8368":"markdown","ae21b6c5":"markdown","3f83b68f":"markdown","bb220526":"markdown","74712275":"markdown","740b78aa":"markdown","62d1cf29":"markdown","b3e92ba0":"markdown","25f75d8d":"markdown","0cc8ef2d":"markdown","db57221e":"markdown","145127ba":"markdown","a02a0501":"markdown","8efae57a":"markdown","81a549b9":"markdown","96cb0c1f":"markdown","81cb5611":"markdown","c4e1b175":"markdown","70ca11a2":"markdown","6ee30125":"markdown","17606cde":"markdown","0f99ac13":"markdown","5cc29b8a":"markdown","eaddcc0e":"markdown","8c96294a":"markdown","c47135a7":"markdown","080d2981":"markdown","ed1d877f":"markdown","01c3e6b6":"markdown","fd9f97d6":"markdown"},"source":{"f797fb1a":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\npd.set_option(\"display.max_columns\", None)\n# pd.set_option('display.max_rows', None)\npd.set_option(\"display.max_rows\", 200)\n\nimport statsmodels.api as sm","66500797":"data = '..\/input\/personal-loan-modeling\/Bank_Personal_Loan_Modelling.csv'\ndata_frame = pd.read_csv(data) #load and read the csv file\ndf= data_frame.copy() #making a copy to avoid changes to data\nprint(f\"There are {df.shape[0]} rows and {df.shape[1]} columns.\")\n#checking the shape of the dataset\nnp.random.seed(85) \ndf.sample(10) #loading random 10 rows","a5877158":"df.info() # looking at the structure of the data","2ba9614a":"#remove the spaces in the columns\ndf.rename(columns={\"ZIP Code\":\"ZIPCode\",\"Personal Loan\":\"Personal_Loan\",\n                        \"Securities Account\":\"Securities_Account\",\"CD Account\":'CD_Account'},inplace=True)","a67bfcc4":"# checking the number of uniques in the zip code\ndf['ZIPCode'].nunique()","20bc32e8":"df['ZIPCode'] = df['ZIPCode'].astype(str)\ndf['ZIPCode'] = df['ZIPCode'].str[0:2]\ndf['ZIPCode'].nunique()","7ac3b571":"df.drop(['ID'],axis=1,inplace=True)\n#Dropping ID as its not relevant","c4173306":"df['Education'] = df['Education'].astype('category')\ndf['Family'] = df['Family'].astype('category')\ndf['Personal_Loan'] = df['Personal_Loan'].astype('category')\ndf['Securities_Account'] = df['Securities_Account'].astype('category')\ndf['CD_Account'] = df['CD_Account'].astype('category')\ndf['Online'] = df['Online'].astype('category')\ndf['CreditCard'] = df['CreditCard'].astype('category')\ndf['ZIPCode'] = df['ZIPCode'].astype('category')\n\ndf.info() #rechecking the datatypes ","ee50d1b7":"df.describe().T ","12981b2a":"df[df['Experience'] < 0]['Experience'].count() #finding columns with -ve experience values","d1a03401":"df1=df[(df.Experience<0)] \nprint(f\"The unique Negative Experience Array= {df1['Experience'].unique()}\")\ndf1['Age'].value_counts(ascending=True)#finding the mean and median for w.r.t Age ","b5499efc":"#Let's check the actual experience distribution for the ages above\ndf2=df[(df.Experience>=0)][df.Age<30] #Since the age for -ve experience values is less than 30 yrs\ndf2.groupby(['Age']).agg([np.mean,np.median]).Experience","1f3ecea7":"multiplier = -1\nfor i in range(len(df)):\n    if df.Experience[i]<0:\n        df.Experience[i]=(df.Experience[i]*multiplier)\n(df.Experience<0).value_counts()         ","f9bfcbb8":"df.isna().sum()","d35e3f6a":"#Performing Univariate Analysis to study the central tendency and dispersion\n#Plotting histogram to study distribution\nUni_num = df.select_dtypes(include=np.number).columns.tolist()\nplt.figure(figsize=(17,75))\nfor i in range(len(Uni_num)):     #creating a loop that will show the plots for the columns in one plot\n    plt.subplot(18,3,i+1)\n    sns.histplot(df[Uni_num[i]],kde=False)\n    plt.tight_layout()\n    plt.title(Uni_num[i],fontsize=25)\n\nplt.show()","ccd7ad90":"plt.figure(figsize=(15,35))\nfor i in range(len(Uni_num)):\n    plt.subplot(10,3,i+1)\n    sns.boxplot(df[Uni_num[i]],showmeans=True, color='yellow')\n    plt.tight_layout()\n    plt.title(Uni_num[i],fontsize=25)\n\nplt.show()","4b7b81e7":"df3= df[(df.Mortgage>0)]","4f4a7e42":"fig,(ax_box,ax_hist) = plt.subplots(2,1,sharex=True ,\n                                        figsize=(10,9),\n                                        gridspec_kw = {\"height_ratios\": (.35, .65)})\nsns.boxplot(df3.Mortgage, ax=ax_box, showmeans=True, color='orange')\nsns.histplot(df3.Mortgage, ax=ax_hist,kde=True)","2a9c01f7":"categorical_val = df.select_dtypes(exclude=np.number).columns.tolist()","43d449b2":"plt.figure(figsize=(15,75))\nfor i in range(len(categorical_val)):     #creating a loop that will show the plots for the columns in one plot\n    plt.subplot(18,3,i+1)\n    ax=sns.countplot(df[categorical_val[i]],palette='Spectral')\n    plt.tight_layout()\n    plt.title(categorical_val[i],fontsize=25)\n    total = len (df[categorical_val[i]])\n    for p in ax.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_height()\/total) # percentage of each class of the category\n        x = p.get_x() + (p.get_width() \/ 2)-0.1  # width of the plot\n        y = p.get_y() + p.get_height()           # hieght of the plot\n        ax.annotate(percentage, (x, y), size = 12.5,color='black') # To annonate\nplt.show()","caef1bc0":"corr= df.corr()\nplt.figure(figsize=(10,7))\nsns.heatmap(corr,annot= True,vmin=0,vmax=1, cmap='RdYlGn_r',linewidths=0.75)\nplt.show()","19264cea":"sns.pairplot(data=df,hue='Personal_Loan')","45df576d":"# For all numerical variables with Personal_Loan\nplt.figure(figsize=(20,10))\nfor i, variable in enumerate(Uni_num):\n                     plt.subplot(3,2,i+1)\n                     sns.boxplot(df['Personal_Loan'],df[variable],palette=\"Dark2\")\n                     plt.tight_layout()\n                     plt.title(variable)\nplt.show()","e4168e85":"#Stacked plot of categorical variables with Personal Loans\ndef stacked_plot(x):\n    sns.set(palette='Accent')\n    tab1 = pd.crosstab(x,df['Personal_Loan'],margins=True)\n    print(tab1)\n    print('-'*120)\n    tab = pd.crosstab(x,df['Personal_Loan'],normalize='index')\n    tab.plot(kind='bar',stacked=True,figsize=(10,5))\n    plt.legend(loc='lower left', frameon=True)\n    plt.legend(loc=\"upper left\", bbox_to_anchor=(1,1))\n    plt.ylabel('Percentage')\n    plt.show()","b3d5c69a":"stacked_plot(df.ZIPCode)","c60bed6d":"stacked_plot(df.Family)","60aba0f8":"stacked_plot(df.Education)","6a1afeef":"stacked_plot(df.Securities_Account)","f2eb791a":"stacked_plot(df.CD_Account)","a6d2f869":"stacked_plot(df.Online)","1fe01d2c":"stacked_plot(df.CreditCard)","27eaf871":"#Income Vs Education Vs Personal_Loan\nplt.figure(figsize=(15,7))\nsns.boxplot(data=df,y='Income',x='Education',hue='Personal_Loan')\nplt.show()","faf326d0":"plt.figure(figsize=(15,7))\nsns.boxplot(data=df,y='Income',x='Family',hue='Personal_Loan')\nplt.show()","8045da8b":"plt.figure(figsize=(15,7))\nsns.boxplot(data=df,y='Mortgage',x='Family',hue='Personal_Loan')\nplt.show()","a38e5bbf":"plt.figure(figsize=(15,7))\nsns.boxplot(data=df,y='CCAvg',x='CreditCard',hue='Personal_Loan')\nplt.show()","26f2c86b":"plt.figure(figsize=(15,7))\nsns.scatterplot(data=df,y='Income',x='CCAvg',hue='Personal_Loan')\nplt.show()","a73ea956":"df1=df.copy() # new copy for Decision Tree model","2387759c":"# Lets treat outliers by flooring and capping\ndef treat_outliers(df,col):\n   \n    Q1=df[col].quantile(0.25) # 25th quantile\n    Q3=df[col].quantile(0.75)  # 75th quantile\n    IQR=Q3-Q1\n    Lower_Whisker = Q1 - 1.5*IQR \n    Upper_Whisker = Q3 + 1.5*IQR\n    df[col] = np.clip(df[col], Lower_Whisker, Upper_Whisker) # all the values samller than Lower_Whisker will be assigned value of Lower_whisker \n                                                            # and all the values above upper_whisker will be assigned value of upper_Whisker \n    return df\n\ndef treat_outliers_all(df, col_list): # treat outliers in numerical column of Dataframe\n    \n    for c in col_list:\n        df = treat_outliers(df,c)\n        \n        \n    return df ","36e1b1b0":"no_treatment = {'Age','Experience'} # These two variables dont have outliers\nnumerical_col = [ele for ele in Uni_num if ele not in no_treatment] \n#Applying outlier treatment\ndf = treat_outliers_all(df,numerical_col)","ae259158":"#Defining a function for Confusion matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nsns.set(font_scale=2.0) # to set font size for the matrix\ndef make_confusion_matrix(y_actual,y_predict):\n    '''\n    y_predict: prediction of class\n    y_actual : ground truth  \n    '''\n    cm=confusion_matrix(y_actual,y_predict)\n    group_names = ['True -ve','False +ve','False -ve','True +ve']\n    group_counts = [\"{0:0.0f}\".format(value) for value in\n                cm.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in\n                         cm.flatten()\/np.sum(cm)]\n    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2,v3 in\n              zip(group_names,group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    plt.figure(figsize = (10,7))\n    sns.heatmap(cm, annot=labels,fmt='',cmap='Blues')\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","92177593":"#Importing all necessary libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nimport statsmodels.api as sm\nfrom sklearn import metrics #accuracy,confusion metrics, etc\nfrom sklearn import datasets \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import datasets\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n","1567f9e4":"## Defining X and Y variables\nX = df.drop(['Personal_Loan'], axis=1) #dropping the dependent variable\nY = df[['Personal_Loan']]\n\n#Convert categorical variables to dummy variables\nX = pd.get_dummies(X, drop_first=True)\nX_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.30,random_state=29) # 70% train set and 30% test set\n","a276ec34":"logreg = LogisticRegression(solver='saga',max_iter=1000,penalty='none',verbose=True,n_jobs=1,random_state=29)\n\n# There arae several optimizer, we are using optimizer called as 'saga' with max_iter equal to 1000 \n# max_iter indicates number of iteration needed to converge\n\nlogreg.fit(X_train, y_train)\npred_train = logreg.predict(X_train)\npred_test = logreg.predict(X_test)\n\n#Checking the Accuracy of the model:\nprint('\\nAccuracy on train data:%.6f'%accuracy_score(y_train, pred_train) )\nprint('Accuracy on test data:%.6f' %accuracy_score(y_test, pred_test))\n#checking the Recall metrics of the model:\nprint('\\nRecall on train data:%.6f'%recall_score(y_train, pred_train) )\nprint('Recall on test data:%.6f'%recall_score(y_test, pred_test))\n#checking the Precision metrics of model:\nprint(\"\\nPrecision on training set : \",precision_score(y_train, pred_train))\nprint(\"Precision on test set : \",precision_score(y_test, pred_test))\n\nprint(\"\\nF1 Score on training set : \",f1_score(y_train, pred_train))\nprint(\"F1 Score on test set : \",f1_score(y_test, pred_test))","27a96574":"make_confusion_matrix(y_test,pred_test) #display confusion matrix for test set","c9d3cb01":"# adding constant to training and test set\nX_train = sm.add_constant(X_train)\nX_test = sm.add_constant(X_test)","842a68f0":"#Defining a funciton to call all the performance metrics scores\ndef metrics_score(model,train,test,train_y,test_y):\n    '''\n    Function to calculate different metric scores of the model - Accuracy, Recall, Precision, and F1 score\n    model: classifier to predict values of X\n    train, test: Independent features\n    train_y,test_y: Dependent variable '''\n     \n    pred = model.predict(train)\n    pred_train = list(map(round,pred))\n    pred1 = model.predict(test)\n    pred_test = list(map(round,pred1))\n   \n    print(\"Accuracy on training set : \",accuracy_score(train_y,pred_train))\n    print(\"Accuracy on test set : \",accuracy_score(test_y,pred_test))\n    print(\"Recall on training set : \",recall_score(train_y,pred_train))\n    print(\"Recall on test set : \",recall_score(test_y,pred_test))\n    print(\"Precision on training set : \",precision_score(train_y,pred_train))\n    print(\"Precision on test set : \",precision_score(test_y,pred_test))\n    print(\"F1 on training set : \",f1_score(train_y,pred_train))\n    print(\"F1 on test set : \",f1_score(test_y,pred_test))\n        \n  ","529cab6e":"logit = sm.Logit(y_train, X_train) #logistic regression\nlg = logit.fit(warn_convergence =False) \n#Checking model performance \nmetrics_score(lg,X_train,X_test,y_train,y_test)","af03eb6a":"cm_pred = lg.predict(X_test)\npred_test = list(map(round,cm_pred))\nmake_confusion_matrix(y_test,pred_test)","a5630bdc":"#checking the VIF scores for X_train set\nvif_series1 = pd.Series([variance_inflation_factor(X_train.values,i) for i in range(X_train.shape[1])],index=X_train.columns)\nprint('Series before feature selection: \\n\\n{}\\n'.format(vif_series1))","1bfb8585":"X_train1 = X_train.drop('Experience', axis=1)\nX_test1 = X_test.drop('Experience', axis=1)\nvif_series2 = pd.Series([variance_inflation_factor(X_train1.values,i) for i in range(X_train1.shape[1])],index=X_train1.columns)\nprint('Series before feature selection: \\n\\n{}\\n'.format(vif_series2))","79690002":"logit1=sm.Logit(y_train,X_train1)\nlg1=logit1.fit()\nmetrics_score(lg1,X_train1,X_test1,y_train,y_test)","35bef26d":"print(lg1.summary())","18227408":"#dropping all dummy variables of Region\nX_train2 = X_train1.drop(['ZIPCode_91','ZIPCode_92','ZIPCode_93','ZIPCode_94','ZIPCode_95','ZIPCode_96'], axis=1)\nX_test2 = X_test1.drop(['ZIPCode_91','ZIPCode_92','ZIPCode_93','ZIPCode_94','ZIPCode_95','ZIPCode_96'], axis=1)","1c269b9f":"logit2=sm.Logit(y_train,X_train2)\nlg2=logit2.fit()\n#print(lg2.summary())\n\n#Lets look at model performance \nmetrics_score(lg2,X_train2,X_test2,y_train,y_test)","bb9c63e8":"#Let's drop Age \nX_train3 = X_train2.drop(['Age'],axis=1)\nX_test3 = X_test2.drop(['Age'],axis=1)\nlogit3=sm.Logit(y_train,X_train3)\nlg3=logit3.fit()\n\nmetrics_score(lg3,X_train3,X_test3,y_train,y_test)","90d43ea5":"#Let's drop Mortgage \nX_train4 = X_train3.drop(['Mortgage'],axis=1)\nX_test4 = X_test3.drop(['Mortgage'],axis=1)\nlogit4=sm.Logit(y_train,X_train4)\nlg4=logit4.fit()\nmetrics_score(lg4,X_train4,X_test4,y_train,y_test)","df58a291":"print(lg4.summary())","a96e3225":"#Calculate Odds Ratio, probability\n##create a data frame to collate Odds ratio, probability and p-value of the coef\nlgcoef = pd.DataFrame(lg4.params, columns=['coef']) #getting the coefficent from lg4 model\nlgcoef.loc[:, \"Odds_ratio\"] = np.exp(lgcoef.coef) #calculate the odds ratio\n\nlgcoef['probability'] = lgcoef['Odds_ratio']\/(1+lgcoef['Odds_ratio']) #calculate the probability \nlgcoef['pval']=lg.pvalues\npd.options.display.float_format = '{:.2f}'.format","36209cc3":"# Filter by significant p-value (pval <0.005) and sort descending by Odds ratio\nlgcoef = lgcoef.sort_values(by=\"Odds_ratio\", ascending=False)\npval_filter = lgcoef['pval']<=0.005\nlgcoef[pval_filter]","acc44eb2":"pred1 = lg4.predict(X_test4)\npred_test1 = list(map(round,pred1))\nmake_confusion_matrix(y_test,pred_test1)","916486be":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\nlogit_roc_auc = roc_auc_score(y_test, lg4.predict(X_test4))\nfpr, tpr, thresholds = roc_curve(y_test, lg4.predict(X_test4))\nplt.figure(figsize=(13,8))\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","50dc1763":"# Optimal threshold as per AUC-ROC curve\n\noptimal_idx = np.argmax(tpr - fpr)\noptimal = thresholds[optimal_idx]\nprint(optimal)","2e71e6e7":"#Applying the optimal threshold to predict model for test data\ny_pred_train = (lg4.predict(X_train4)>optimal).astype(int)\ny_pred_test = (lg4.predict(X_test4)>optimal).astype(int)","e51af91a":"#Confusion matrix for test set for lg4 model\nmake_confusion_matrix(y_test,y_pred_test)","b5317bbf":"print(\"Accuracy on training set : \",accuracy_score(y_train,y_pred_train))\nprint(\"Accuracy on test set : \",accuracy_score(y_test,y_pred_test))\nprint(\"\\nRecall on training set : \",recall_score(y_train,y_pred_train))\nprint(\"Recall on test set : \",recall_score(y_test,y_pred_test))\nprint(\"\\nPrecision on training set : \",precision_score(y_train,y_pred_train))\nprint(\"Precision on test set : \",precision_score(y_test, y_pred_test))\n\nprint(\"\\nF1 Score on training set : \",f1_score(y_train,y_pred_train))\nprint(\"F1 Score on test set : \",f1_score(y_test, y_pred_test))","5bdeab23":"from sklearn.metrics import precision_recall_curve\ny_PresRec=lg4.predict(X_train4)\nprec, rec, tre = precision_recall_curve(y_train, y_PresRec)\n\ndef plot_prec_recall_vs_tresh(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], 'b--', label='precision')\n    plt.plot(thresholds, recalls[:-1], 'g--', label = 'recall')\n    plt.xlabel('Threshold')\n    plt.legend(loc='upper left')\n    plt.ylim([0,1])\nplt.figure(figsize=(10,7))\nplot_prec_recall_vs_tresh(prec, rec, tre)\nplt.show()","53518140":"#Applying the optimal threshold to predict model for test data\noptimal_threshold = 0.25 # we get a balanced recall and precision at this threshold\n\ny_pred_train1 = (lg4.predict(X_train4)>optimal_threshold).astype(int)\ny_pred_test1 = (lg4.predict(X_test4)>optimal_threshold).astype(int)","ffc391e2":"#Confusion matrix for test set for lg4 model\nmake_confusion_matrix(y_test,y_pred_test1)","0499bc2a":"print(\"Accuracy on training set : \",accuracy_score(y_train,y_pred_train1))\nprint(\"Accuracy on test set : \",accuracy_score(y_test,y_pred_test1))\nprint(\"\\nRecall on training set : \",recall_score(y_train,y_pred_train1))\nprint(\"Recall on test set : \",recall_score(y_test,y_pred_test1))\nprint(\"\\nPrecision on training set : \",precision_score(y_train,y_pred_train1))\nprint(\"Precision on test set : \",precision_score(y_test, y_pred_test1))\n\nprint(\"\\nF1 Score on training set : \",f1_score(y_train,y_pred_train1))\nprint(\"F1 Score on test set : \",f1_score(y_test, y_pred_test1))","2e1ce877":"from mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\nimport matplotlib.pyplot as plt ","5c50c1fd":"## Defining X and Y variables\nX = df.drop(['Personal_Loan'], axis=1)\nY = df[['Personal_Loan']]\n\n#Convert categorical variables to dummy variables\nX = pd.get_dummies(X, drop_first=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.30)\n# Fit the model on train\nm = LogisticRegression(solver='newton-cg',n_jobs=-1,random_state=0)","d41be287":"# we will first build model with all \nsfs = SFS(m, k_features=19, forward=True, floating=False, scoring='recall', verbose=2, cv=5)\nsfs = sfs.fit(X_train, y_train)\nfig = plot_sfs(sfs.get_metric_dict(),kind='std_dev')\nplt.ylim([0, 1])\nplt.title('Sequential Forward Selection (w. StdDev)')\nplt.grid()\nplt.show()","398db8de":"sfs1 = SFS(m,k_features=10, forward=True, floating=False, scoring='recall', verbose=2, cv=5)\n\nsfs1 = sfs1.fit(X_train, y_train)\nfig1 = plot_sfs(sfs1.get_metric_dict(),kind='std_dev')\nplt.ylim([0, 1])\nplt.title('Sequential Forward Selection (w. StdDev)')\nplt.grid()\nplt.show()\n","e33f4662":"#Which are the important features?\nfeat_cols = list(sfs1.k_feature_idx_)\nprint(feat_cols)","d4760751":"#Looking at the column names\nX_train.columns[feat_cols]","fd301e54":"#Creating new X_train and X_test with the selected columns\nX_train_final = X_train[X_train.columns[feat_cols]]\nX_test_final = X_test[X_train_final.columns]","11b04d7a":"#Fitting logistic regression model\nlogreg1 = LogisticRegression(solver='saga',max_iter=1000,penalty='none',verbose=True,n_jobs=1,random_state=29)\nlogreg1.fit(X_train_final, y_train)","bb514881":"#Lets check the model performance\nmetrics_score(logreg1,X_train_final,X_test_final,y_train,y_test)","b28c32be":"pred_test2 = logreg1.predict(X_test_final)\n\nprint(\"confusion matrix = \\n\")\nmake_confusion_matrix(y_test,pred_test2)","f957cd83":"#AOC-RUC Curve\n\nSFS_roc_auc = roc_auc_score(y_test, logreg1.predict_proba(X_test_final)[:,1])\nfpr, tpr, thresholds = roc_curve(y_test, logreg1.predict_proba(X_test_final)[:,1])\nplt.figure(figsize=(13,8))\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % SFS_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","778e091b":"optimal_idx1 = np.argmax(tpr - fpr)\noptimal_threshold1 = thresholds[optimal_idx1]\nprint(optimal_threshold1)","9d74b7f4":"y_pred_trn = (logreg1.predict(X_train_final)>optimal_threshold)\ny_pred_tst = (logreg1.predict(X_test_final)>optimal_threshold)","c53acc37":"# let us make confusion matrix after optimal threshold has been choosen\nmake_confusion_matrix(y_test,y_pred_tst)","c4e70730":"print('Accuracy on train data:',accuracy_score(y_train, y_pred_trn) )\nprint('Accuracy on test data:',accuracy_score(y_test, y_pred_tst))\n\nprint('\\nRcall on train data:',recall_score(y_train, y_pred_trn) )\nprint('Recall on test data:',recall_score(y_test, y_pred_tst))\n","f69013d4":"df1.info()","c879e855":"X= df1.drop(['Personal_Loan'],axis=1)\ny=df1['Personal_Loan']","4cbb7e45":"# encoding the categorical variables\nX = pd.get_dummies(X, drop_first=True)\n# Splitting data into training and test set:\nX_train,X_test, y_train, y_test =train_test_split(X,y, test_size=0.3, random_state=29)\nprint(X_train.shape,X_test.shape)","8bde4b51":"from sklearn import tree\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeClassifier\nDt = DecisionTreeClassifier(criterion='gini',class_weight={0:0.15,1:0.85},random_state=29)","f0aeb633":"Dt.fit(X_train,y_train)","629285f4":"y_predict = Dt.predict(X_test)\nmake_confusion_matrix(y_test,y_predict)","031cec1e":"y_train.value_counts(1)","38f87481":"def scores(model):\n    \"\"\" model : classifier to predict X values \"\"\"\n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n    \n    print(\"Accuracy on training set : \",metrics.accuracy_score(y_train,y_pred_train))\n    print(\"Accuracy on test set : \",metrics.accuracy_score(y_test,y_pred_test))\n\n    print(\"\\nRecall on training set : \",metrics.recall_score(y_train,y_pred_train))\n    print(\"Recall on test set : \",metrics.recall_score(y_test,y_pred_test))\n    \n    print(\"\\nPrecision on training set : \",metrics.precision_score(y_train,y_pred_train))\n    print(\"Precision on test set : \",metrics.precision_score(y_test,y_pred_test))\n    \n    print(\"\\nF1 on training set : \",metrics.f1_score(y_train,y_pred_train))\n    print(\"F1 on test set : \",metrics.f1_score(y_test,y_pred_test))","7979648c":"#Let's calculate the Accuracy and Recall Score of the model\nscores(Dt)","4ae10759":"column_names = list(X.columns)\nprint(column_names)","105c141c":"plt.figure(figsize=(20,30))\n\nout = tree.plot_tree(Dt,feature_names=column_names,filled=True,fontsize=8,node_ids=True,class_names=True)\nfor o in out:\n     arrow = o.arrow_patch\n     if arrow is not None:\n        arrow.set_edgecolor('black')\n        arrow.set_linewidth(1)\nplt.show()","fe50b554":"# Text report showing the rules of a decision tree -\n\nprint(tree.export_text(Dt,feature_names=column_names,show_weights=True))","688946c3":"importance = Dt.feature_importances_\nindices = np.argsort(importance)\n\nplt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importance[indices], color='green', align='center')\nplt.yticks(range(len(indices)), [column_names[i] for i in indices])\nplt.xlabel('Importance Value')\nplt.show()","f6e52fcc":"from sklearn.model_selection import GridSearchCV\n# Choose the type of classifier. \nclassifier = DecisionTreeClassifier(random_state=29,class_weight = {0:.15,1:.85}) #adding classweights \n\n#Defining the Hyperparameters\n\nparameters = {'max_depth': np.arange(1,11), \n            'criterion': ['gini'],\n            'splitter': ['best','random'],\n            'max_features': ['log2','sqrt']}\n\n# Type of scoring used to compare parameter combinations\nrecall_scorer = metrics.make_scorer(metrics.recall_score)\n\n# Run the grid search with the above parameters\ngrid_obj = GridSearchCV(classifier, parameters, scoring=recall_scorer,cv=5)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set to the best combination of parameters\nclassifier = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nclassifier.fit(X_train, y_train)","b6fcbec6":"pred_test2 = classifier.predict(X_test)\nmake_confusion_matrix(y_test,pred_test2)","59bc8b52":"scores(classifier)","126f1159":"plt.figure(figsize=(20,30))\n\nout = tree.plot_tree(classifier,feature_names=column_names,filled=True,fontsize=11,node_ids=True,class_names=True)\nfor o in out:\n     arrow = o.arrow_patch\n     if arrow is not None:\n        arrow.set_edgecolor('black')\n        arrow.set_linewidth(1)\nplt.show()","b196a84d":"# Text report showing the rules of a decision tree -\n\nprint(tree.export_text(classifier,feature_names=column_names,show_weights=True))","8d8763b6":"importances = classifier.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='green', align='center')\nplt.yticks(range(len(indices)), [column_names[i] for i in indices])\nplt.xlabel('Importance Value')\nplt.show()","9ced1bb4":"ccp = DecisionTreeClassifier(random_state=29,class_weight = {0:0.15,1:0.85})\nccp.fit(X_train,y_train)\npath = ccp.cost_complexity_pruning_path(X_train, y_train) #finding the alpha and impurity values\nccp_alphas, impurities = path.ccp_alphas, path.impurities","66a0ba01":"pd.DataFrame(path)\n #display as a dataframe","ad447038":"#plotting alpha vs impurities\nfig, ax = plt.subplots(figsize=(15,5))\nax.plot(ccp_alphas, impurities, marker='o', drawstyle=\"steps-post\")\nax.set_xlabel(\"Alpha\")\nax.set_ylabel(\"Total impurity of leaves\")\nax.set_title(\"Total Impurity vs Alpha for training set\")\nplt.show()","d5e6a8b8":"# Finding the number of nodes in the last tree and the corresponding alpha value\nclfs = [] #creating a empty list \nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=29, ccp_alpha=ccp_alpha,class_weight = {0:0.15,1:0.85})\n    clf.fit(X_train, y_train) #apply classifier model with alpha values  \n    clfs.append(clf)\nprint(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n      clfs[-1].tree_.node_count, ccp_alphas[-1])) #finding the last node and its corresponding alpha","6cc08943":"#Creating empty lists for train and test recall\nrecall_train=[]\nrecall_test=[]","71414947":"#run a loop to appead all recall scores for train and test at the alpha values\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=29, ccp_alpha=ccp_alpha,class_weight = {0:0.15,1:0.85})\n    clf.fit(X_train, y_train)\n    y_pred_train1 = clf.predict(X_train)\n    y_pred_test1 = clf.predict(X_test)\n    values_train = metrics.recall_score(y_train,y_pred_train1)\n    values_test= metrics.recall_score(y_test,y_pred_test1)\n    recall_train.append(values_train)\n    recall_test.append(values_test)","f3a6134e":"#plot the recall VS alpha \nfig, ax = plt.subplots(figsize=(9,10))\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"Recall\")\nax.set_title(\"Recall vs alpha for training and testing sets\")\nax.plot(ccp_alphas, recall_train, marker='o', label=\"train\",\n        drawstyle=\"steps-post\",)\nax.plot(ccp_alphas, recall_test, marker='o', label=\"test\",\n        drawstyle=\"steps-post\")\nax.legend(loc='lower left')\nplt.show()","23a3bedc":"#Let's find the best alpha threshold for max recall\nindex_best_alpha = np.argmax(recall_test)\nbest_model = clfs[index_best_alpha]\nprint(best_model)","9f3e1d9a":"#at alpha = 0.003\nbest_model2 = DecisionTreeClassifier(ccp_alpha=0.003,\n                       class_weight={0: 0.15, 1: 0.85}, random_state=29)\nbest_model2.fit(X_train, y_train)\n","270e60cc":"pred_test3=best_model2.predict(X_test)\nmake_confusion_matrix(y_test,pred_test3)","39856fc9":"scores(best_model2)","7023a428":"plt.figure(figsize=(10,10))\n\nout = tree.plot_tree(best_model2,feature_names=column_names,filled=True,fontsize=9,node_ids=True,class_names=True)\nfor o in out:\n     arrow = o.arrow_patch\n     if arrow is not None:\n        arrow.set_edgecolor('black')\n        arrow.set_linewidth(1)\nplt.show()\nplt.show()","7de98a67":"# Text report showing the rules of a decision tree -\n\nprint(tree.export_text(best_model2,feature_names=column_names,show_weights=True))","ce989b05":"importances2 = best_model2.feature_importances_\nindices = np.argsort(importances2)\n\nplt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances2[indices], color='green', align='center')\nplt.yticks(range(len(indices)), [column_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","be1bec13":"All_models = {'Model':['Logistic Regression Model-sklearn','Logistic Regression-Statsmodel-mutlicollinearity remvo','Logistic Regression-Optimal Threshold =0.2017','Logistic Regression-Optimal Threshold =0.25','Sequential Feature Selction Method','Initial Decision Tree','Decision treee- hyperparameter tuning(pre-pruning)',\n                                          'Decision tree- Cost Complexity post-pruning'],'Train_Accuracy':[0.9380,0.9640,0.9497,0.9560,0.944,1.0,0.8070,0.9749],'Test_Accuracy':[0.9387,0.9660,0.9400,0.9493,0.944,0.9880,0.80,0.972],'Train_Recall':[0.5245,0.7289,0.8484,0.8367,0.4817,1.0,0.9854,0.9417], 'Test_Recall':[0.4599,0.6788,0.8321,0.8102,0.5131,0.9051,0.9343,0.9343]}\ncomparison = pd.DataFrame(All_models)\n\ncomparison","b2677600":"df2=df.copy() # making a new copy from the dataset without outlier treatment\nA = df2.drop(['Personal_Loan'], axis=1) #dropping the dependent variable\nB = df2[['Personal_Loan']]","4235e971":"A = pd.get_dummies(A, drop_first=True) #creat dummy variables \n# Splitting data into training and test set:\nA_train,A_test, B_train, B_test =train_test_split(A,B, test_size=0.3,random_state=1)\n#split data\nprint(A_train.shape,A_test.shape)","942fafa7":"A_test.head()","b32b182a":"#apply the final model best_model2 to the train and test set\nfinal_pred_test = best_model2.predict(A_test)","2ae6be06":"data = df2.loc[A_test.index] #selecting rows with same index as test set\ndata['Predicted'] = final_pred_test\ndata.head()","6921f24f":"comparison_column = np.where(data[\"Predicted\"] == data[\"Personal_Loan\"], True, False) #identifying the misclassification\ndata['Misclassification'] = comparison_column\ndata['Misclassification'].value_counts()","9729be2c":"incorrect =data[data['Misclassification']== False] # Grouping only the misidentified rows \nincorrect.sample(5)","2a287c57":"#Crearting a Pandas Profile report to identify pattern\nfrom pandas_profiling import ProfileReport\nprofile  = ProfileReport(incorrect,title = 'Misclassification Pattern Profile',minimal=True) \nprofile.to_widgets()","2b6bcb03":"**Optimal Threshold from AUC-ROC**\n\n* The optimal threshold cut off will be where True Positive Rate is high and False Positive Rate is low","600a9b8b":"* There are no more insignificant variables \n\n**Hence, we will use lg4 as the final model**\n\n## Observations from Model:\n\n### Coefficient Interpretations:\n\n* Income, CCAvg, Mortgage, Family_3, Family_4, Both Education variables and CD_account1 have positive co-efficients; which indicate that an increase in their values will increase the probability of customers having Personal loans\n\n\n* Family2, Securities_Account_1,Online_1 and CreditCard_1 have a negative co-efficient; Which indicates that an increase in their value would decrease the probability  Customer's having Personal loans\n\n\n### Converting Coefficients to odds: \n* In a Logistic Regression model, the coefficients of the variable is the Log of odds. \n* We will calculate the  odds ration to quantify the strength of the assosiation between the dependent and independent variables\n\n**Odds ratio =  Exp(coef)**\n\n**Probability  = odds\/(1+odds)**","7721d328":"**Confusion matrix Prediction on lg4 model Test Data**","86822b76":"**Observations:**\n\n* The Logistic Regression model has good accuracy by poor Recall values.\n* This could be due to multi-collinearity and insignificant values in the model.\n* To check this we will build a model using statsmodels library","32056c55":"**Observations**:\n* 29.4% of customers are from region 94 followed by reion 92 at19.8%\n* 29.4 % of customers are of single-family household, with Family variable having four unique values.\n* Education has three unique values with 41.9% of at Undergrad level(1).\n* Personal_Loans is the Dependent variable and we see that there is heavy imbalance. Only 9.6% of customers in the data have accepted a loan from the previous campaign\n* 89.6% of customers dont have a Securities account whereas 94% of customers dont have a CD account.\n* We that 59.7 % of customers use the bank's online facilities and about 70.6% dont have credit cards issue by another bank.","fb212e34":"**Observations**:\n* The True positive has increased to 8.53% and the False negative has decreased to 0.6%.","88439ec2":"* The Recall for test set did not change.\n* Next let's drop Mortgage variable for this model and check performance again.\n* Even though Mortgage has a positive coefficient, it is compartitively low to the rest of the variables.","04a84c5f":"* The impurity values increases till ~ 0.03 of alpha value and remains constant till alpha ~ 0.22 before rising sharply","fba0ce97":"**Observations:**\n* Age and Experience have the highest VIF values.\n* We already suspected that these variables might have multicollinearity which is proven true with the above values.\n* We will remove Experience column to remove multi-collinearity","58020224":"* The True positive values('ie predicting customers who will purchase loan) is only 6.2% .\n* The False negative is at 2.93%. We have to check if we can bring this lower","21a29fd9":"**Observations**:\n* Age and Experience have the highest correlation at 0.99. We suspect multi-collinearity between these variables\n* Income and CC_Avg have the next highest positive correlation at 0.65. This suggests that customers with higher income have higher Credit card charges.\n* Income and CCAvg have a positive correlation with Mortgage.","400af34f":"## Exploratory Data Analysis:\n### Univariate Analysis - Numerical Columns:","60bd9230":"### Univariate Analysis - Categorical Columns:","717002f9":"# Importing Necessary Libraries","901f0ece":"* We now have 5000 rows and 13 columns and we see that the memory has also reduced\n\n## Summary of Numerical Columns","3da4caa1":"**Observations**:\n* The Accuracy for the test set is 0.96 which looks good\n* But the Recall for the test set is only 0.68\n* We must further analyse this model and check if the perfomance can be improved.","1ea78ee0":"## Load and Explore the Data","2663a774":"* Customers who have Personal loans have a higher credit card Average.\n* There are several outliers in customers who dont have personal loans. ","bd507575":"* There are 467 unique values in zip code.\n* In US, The first digit of a PIN indicates the zone or a region, the second indicates the sub-zone, and the third, combined with the first two, indicates the sorting district within that zone. The final three digits are assigned to individual post offices within the sorting district. \n* Hence we will group them based on the first two digits\n","8dfa5f49":"## Data Pre-Processing:\n\n### Outliers Treatment:\n* Income, CCAvg and Mortgage have very high outliers in the higher end and must be treated.\n* Since we will also be creating a Decision Tree model(Decision Trees are not influenced by outliers) \n  we will make a copy of the dataset before proceeding with outlier treatment.","66b4fa2f":"* All negative experience values have been corrected.","0b44e415":"### Split Data","f8ad6573":"* Since the recall value only stopped rising after 10th feature, we will proceed only with the best 10 features","979538fe":"# Misclassification of model:\n## Analysing predictions that were off the mark","69f8862c":"* Income level among all Family groups is significantly higher for customers who have a Personal Loan. ","65f38ca4":"* The Recall for test set has dropped to 0.686\n* Next let's drop Age variable and check model performance ","0adfbc00":"\n**Observations**\n* At 0.25 threshold, Recall has dropped to 0.810 for test set \n* But the Precision value has increased and Accuracy remains same.\n* Since Precision is not the defining metric; AUC-ROC threshold value has a better model ","678e4768":"**Maximum Recall value is at alpha 0.0042. But at this alpha we will lose valuable business information and the decision tree might have very less nodes.** \n\n**Hence we will use the point where the Recall values just begins to drop first; at alpha = 0.003.\nThis will ensure we are retaining information and also get a high recall value.**","79a83101":"* Now the unique ZIPCodes are reduces to seven groups","c1074947":"**Observations**\n\n* At Optimal Threshold, the Accuracy of the test set has reduced to 0.94 \n* But the Recall score for Test set has rised significantly to 0.8321\n\n\n### Percision-Recall Curve\n\n* This curve will plot the precision and Recall values for the lg4 model.","8c5bfe44":"* There are several outliers in Family size 1 and 2 for customers who dont have a Personal loan compares to the rest.\n* We also see that as Family size increases, the Mortgage value also rises and the customers have Personal Loans","e71de158":"* The VIF scores for all the variables is less than 5 and there is no more multi-collinearity in the model.\n* Let's check the model performance","4c45acfc":"* We have more customers who dont have Credit card with other banks\n* Again 10% of customers in both Credit Card classes have purchased loans","8a8f1b96":"### Logistic Regression Using Stats Model:\n* Using Stats Model in Python, we will get an list of statistical results for each estimator.\n* Stats Model is also used to further conduct tests and statistical data exploration","1d3490c2":"## Checking for Multicollinearity using VIF Scores:\n* Multicollinearity occurs when there is correlation between the predictor variables.\n* Variance Inflation factor: Variance inflation factors measure the inflation in the variances of the regression coefficients estimates due to collinearities that exist among the predictors.\n* If VIF is 1 then there is no correlation among the predictor variables. Whereas if VIF exceeds 5, we say there is moderate multi-collinearity and if it is 10 or exceeding 10, it shows signs of high multi-collinearity. \n* Alternatively we can check the significance of a variable to the model with the P-value","55f46bab":"**Observations**:\n* More Customers with higher income and CCAvg `>2.5(in thousand dollars)` have personal loans.","ce4fbe60":"\n- More Customers with larger family size(3&4) have Personal Loans. ","6d8fe77d":"## Multi-variate Analysis","13bec6c8":"* There are 49 misclassified data in the test set","9aa1a8cc":"* The Mean and Median for Age is almost equal ie approx 45 yrs \n* Experience Column has a min value -3, which is could be an error and needs to be checked.\n* Mean Income is greater than median income indicating skewness. We also see a very high Max value suggesting outliers.\n* CCavg minimum value is  0.0 dollars; suggesting that the customer may not have any credit cards. The Mean and median for this variable are fairly close.\n* Similarly, the Minimum value for Mortgage is 0.0 for atleast 50% of the customers; this could mean the customers dont own a home.\n","78d759e7":"## Comparison of all Models for Personal_Loan prediction","8e0de7db":"### Correlation Matrix","0177a2f1":"## Key Questions:\n1. What are the Key variables that have a strong relationships with the dependent variable?\n2. Which metric is right for model performance evaluation and why?\n3. How accurate are the Model predictions and can it be improved?\n","3ea7065f":"# Sequential Feature Selector method:\n* This method will begin with an empty model and will add in each forward step the one variable that gives the maximum improvement to the model.\n* The aim of this method is to discrad deceptive features and also speed training and testing process","dcd50260":"\n**Insights**\n- In the above model the following variables have p-value >0.05:\n    - Age, Mortgage, Family2, SecuritiesAccount_1 and all the Dummy variables of the Variable ZIPCode.\n- We know that Mortgage has a positive correlation to Personal Loan and Family2 despite having high p-value cannot be removed as its part of the Family category variable.\n- Let's remove all the dummy variables of Category Region and check model performance","212e69b7":"**OBSERVATIONS**:\n* About 3% of the data from the test set has been misclassified i.e The Predicted value of the model was not the same as Personal_loan variable in the dataset.\n\n* The miscalssifcation seeems to spread across all variables. But its significant on some\n* Income and CCAvg have high misclassifications. This is understandable as the model highlighted these two features as very important. Hence the model seems to have classified customers with high income and CCavg as potential loan borrowers\n\n* Among the categorical variables; again the misclassification is high for customer with CD_account; an important feature for the model. \n* The model has targeted all its important feature combinations as potential loan borrowing customers.","3b41576e":"## Feature Engineering:","0c7e78b7":"* 10% of customers in both classes of Online variable have purchased loans","cd0d10a8":"## Cost Complexity Pruning\n* This is another method to reduce and control the size of the Tree. This method is called Post-Pruning \n* Here, we use the Cost complexity Parameter `ccp_alpha` to prune the tree\n* We will remove each possible nodes based on the alpha value. The greater the `ccp_alpha`value, higher number of nodes will be pruned and the total impurity will also increase\n\n**Finding the `ccp_alpha` values**","4df0befc":"## Model Building - Decision Tree:\n### Approach\n1. Data preparation\n2. Partition the data into train and test set.\n3. Built a CART model on the train data.\n4. Tune the model and prune the tree, if required.\n5. Test the data on test set.","de3acd35":"**Observations**:\n* The Recall for test set has improved to 0.876 after the hyperparameter tuning.\n\n### Visualizing the Tree","3d185af6":"## Model Building\n* We will build the Decision Tree model using the default 'gini' criteria to split.\n* In our dataset, we know that there is an imbalance in the Dependent variable Personal_Loan. ie. 90.4% of frequency is for 0 and 9.6% is for 1.\n* This might cause the Decision Tree model to become biased towards the dominant class\n* Hence we will add a class_weight hyperparameter as a dictionary {0:0.15,1:0.85} to the model to specify the weight of each class and the decision tree will give more weightage to class 1","739daec2":"## Processing Columns","fe8a3863":"* The Accuracy values for both Train and Test set are very close.\n* But there is huge difference in the Recall Scores for train and test set. \n* This suggests that the model is overfitting.\n\n## Visualizing the Decision Tree","634b0d48":"**Insights**:\n* The distribution is again right-skewed with an increased Mean of around 183K dollars.\n* There are again several outliers in the higher end. We suspect this could be due to the location of the homes, as higher land value could mean higher mortgage price. Over 75% of the customers have Mortgageare below 230K dollars.","a35932f0":"* All the sub-regions show fairly equal distribution among customers who purchased a loan","3bbab997":"**Observations**:\n* Customers with higher Education level, ie, Graduate and Post-Graduate level and with a CD Account have a 98% probability of having personal loans\n* Customers with larger family size of 3 and 4 have higher probabilities 91% and 82% respectively of having a personal loan.\n* Other Significant Variables that have moderate to high probability are Income , Customers who use the Bank's online services and those who have addditional credit cards from Other Banks.\n\n\n### Identifying Key Variables:\n* The model indicates the following key variables to have a strong relationship with the dependent variable Personal_Loan\n    - Education \n    - CD_Account \n    - Family\n    - CCAvg\n    - Income\n    - Online and\n    - CreditCard","6c391fca":"* Income is the most important feature to predict if the customer bought a personal loan. \n* Education2, Family4 and CCAvg are the next most important predictor features.","68af8368":"**Observations**:\n* The order importance of features has changed.\n* The value for Income has increased and CCAvg is now the second most important feature.","ae21b6c5":"**Let's plot the Recall Vs Alpha values for both Train and Test set**","3f83b68f":"**Observations:**\n* The pair plot shows a more varying distribution in the variables between customers who took a loan and those who didnt. \n* We see again that the distribution for Age and Experience is very similar. This could suggest possible multicollinearity\n* There are overlaps that make it difficults to interpret who has personal loans and who doesnt, hence we will analyse futher with other plots","bb220526":"* There are no missing values in this dataset","74712275":"## Model Building \n\n### Model Evaluation Criterion \n#### Model can make two kinds of wrong predictions: \n1. Wrongly Identify customers as loan borrowers but they are not - False Positive\n2. Wrongly identifying customers as not borrowers but they actually buy loans - False Negative\n\n* Since the Banks wants to identify all potential customers who will purchase a loan, the False Negative value must be less.\n\n#### How to reduce losses\n* Recall is the Performance metric that must be improved.\n* The Recall score must be maximised and greater the score the less the chance of missing potential customers.\n\n\n#### Creating a Confusion Matrix","740b78aa":"## Bivariate Analysis","62d1cf29":"* Majority of the customers dont have a Securities Account out of which 420 have Personal loans\n* Remaining customers who do have an account; only 60 have loans. ","b3e92ba0":"**Observations**:\n* The mean values for Age is the same for both categories of Personal Loans\n* Similarly the mean values for Experience is also almost equal for both categories of Personal Loan. Both these variables dont have any outliers\n\n* Customers who have Personal Loans also have high Mean **Income and CreditCard Average** compared to customers who dont have a loan. Interesting we see several outliers in the higher end for both these variables in Class **0**. \n\n* The mean value for Mortgage at both levels in 0.0(in dollars). This is because majority of the customers dont have Mortgages. However, we see that customers with higher mortgages have Personal loans. But, we also see that there are several outliers in the high end again for customers who dont have a loan.\n\n* The above plot, suggests a correlation between Income,CCavg and Mortgage. Customers with high values for these variables have taken loans. This could suggests them as possible features of customers that can be targeted.","25f75d8d":"**Observations**:\n* There are close to a 100 nodes in the tree with the smallest sample size = 2 and the Gini value for the last node is 0.0\n* This is surely a overfitted Decision Tree model \n* Let's check the important features in the tree. This is also called the Gini importance","0cc8ef2d":"**Observations**:\n* Age and Experience are almost normally distributed and look quite similar. This suggests a correlation between the two.\n* There is skewness is the other three variables:\n* Income:\n    - Income shows the annual salary earned by the customer and its right-skewed in distribution.Majority of customers have income less than 100K, but there are several observation in the higher end.\n\n* Credit Card Average: \n    - CCAvg has several outliers in the higher end and is heavily right-skewed. Almost 75% of customers have an average of less than 2.5(in thousand dollars). This suggests that some customers have very high charges compared to the rest\n\n* Mortgage \n    - The distribution in Mortgage variable is also heavily skewed. Almost 50 % of customers dont have a mortgage,indicating they dont own a home. We will have to analyse the mortgage for customers who only own a home to understand the distributions","db57221e":"* Customers with higher Education levels have taken Personal Loans.","145127ba":"**Observations**:\n* The Accuracy and Recall values for test set has a slight increase\n\n\n**Variable Significance:**\n- The P-Value of the variable indicates if the predictor variable is significant or not.\n- The level of significance is 0.05 and any p-value less than 0.05 , then that variable would be considered significant.","a02a0501":"* All Outliers are treated","8efae57a":"**Observations**:\n* True Positive - 8.20%\n* False Positive - 0.27%\n* False Negative - 0.93%\n* True Negative - 90.6%\n\n* We also see that there are only 9.8% of the Class '1'.\n\n## Model Evaluation Criteria - Recall\n* As discussed earlier, the Bank wants to predict and identify all potential customers who will buy a Personal loan.\n* We want to maintain the False Negative ie. wrongly identifying customers as non-buyers but they actually purchase a loan as low as possible.\n\n* Hence Recall is the metric to be used\n","81a549b9":"**Observations**:\n- In the lg4 model:\n    - True positive is 6.33% \n    - True Negative is 90.33%\n    - False Positive is 0.53% \n    - False Negative is 2.8%\n- We need to improve the True Positive and reduce False Negative values.\n- Let's check for model improvement","96cb0c1f":"## Fixing DataTypes","81cb5611":"* In the 302 customers have a CD_account, almost 50% have a Personal Loan\n* This suggests that customers who have a CD_account are likely to buy loans and can be a possible target feature.","c4e1b175":"* All the variables are of numerical datatype and there are no missing values\n* The Dependent variable is Personal Loan. We see that it has only two values; `0 & 1`. As it is a binary class variable, we will convert to category for further processing.\n* Education and Family have numerical inputs label-coded from differnet categories and should be in category datatype\n* Securities_account, CD_Account, Online and CreditCard are int datatype but with Binary inputs\n    * {i.e 0 = No and 1 = Yes} \n* The inputs can also be considered as two categories and hence for better model and analysis, we will convert them to categorical datatype    \n* Zipcode signifies the area where the customer lives and we cannot use it in its Numerical Form. Hence we will extract relevant information from it and drop the column\n","70ca11a2":"* Checking the different values in the ZIPCode variable, we conclude that all the customers in this dataset are from the State of California. \n* We will extract the county where the customer is currently residing.","6ee30125":"* As Education level increases, Mean Income also increases.\n* Customers with Education level 2 and 3 who have personal loans have a much higher mean income than Education level 1 customers","17606cde":"# Personal Loan Campaign Modelling","0f99ac13":"**Observations**:\n* The top five important features are:\n    - Income\n    - Education_2\n    - Family_4\n    - CCAvg\n    - Family_3\n    \n* The above tree is complex to interpret\n* Since there is suspicions of over-fitting we must prune the tree(reduce overfit) for better model performance. \n\n\n## Reduce Over-Fitting:\n\n\n### GridSearch for Hyperparameter tuning of Tree Model\n* Hyperparameters are variables that control the network structure of the Decision tree.\n* As there is no direct way to calculate the effects of value change in hyperparamter has on the model, we will use a GridSearch\n* This is a tuning technique that will compute the optimum values of specific hyperparamters of the model\n* The parameters are optimized using a cross-validated GridSearch over a parameter grid\n","5cc29b8a":"## Missing Values :","eaddcc0e":"* We see that the mean and median for the above ages are approximately equal.\n* We are missing information for Age 23 yrs old, suggesting that the data has only -ve values for experience. \n* Replacing with either Mean or Median for corresponding ages will lose information for ages 23 & 24. \n* Hence we will consider this issue as an data entry error and remove the negative sign from the values, thus making them positive experience values","8c96294a":"## Objective :\n1. To predict whether a liability customer will buy a personal loan or not.\n2. Which variables are most significant.\n3. Which segment of customers should be targeted more.","c47135a7":"### Logistic Regression (with Sklearn library)","080d2981":"**Observations**:\n* The Accuracy is at 0.94 for the test set but the Recall is only 0.513 \n* The recall value is only slightly better than the Sklearn logistic regression model","ed1d877f":"* The Recall for test set improved to 0.6934\n* Let's check the model summary","01c3e6b6":"## Model Performance Improvement\n\n###  AUC-ROC curve:\n* This is a performance measurement for classification problems at various threshold settings","fd9f97d6":"**Observations**:\n\n* The overall results for recall has increased from the initial model and its also higher than the Hypertuned model.\n* The performance for both train(0.9417 and test (0.9343) recall is close and comparable\n\n\n### Visualizing Decision Tree for best_model2"}}