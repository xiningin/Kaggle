{"cell_type":{"fba090eb":"code","ec49a78a":"code","64be7569":"code","360325ea":"code","f03b8328":"code","9d5e5d40":"code","cce4e3c6":"code","784c0904":"code","1048a3c7":"code","feb0eda9":"code","0570466f":"code","6d193535":"code","79e5d3db":"code","01c6377a":"code","55df98b0":"code","439e4ac4":"code","7a0cc5dc":"code","2ad57f64":"code","5755a848":"code","942155d5":"code","f1315e10":"code","cdbeca29":"code","71061600":"markdown","715257de":"markdown","13d578c0":"markdown","f708d1a3":"markdown","0da2ba11":"markdown"},"source":{"fba090eb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec49a78a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport re\nfrom sklearn.utils import shuffle\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer","64be7569":"# Read data\ndf = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',\n                encoding='latin',\n                header=None)","360325ea":"df.head()","f03b8328":"# Change column names\n\ndf.columns = ['sentiment', 'id', 'date', 'query', 'user_id', 'text']\ndf.head()","9d5e5d40":"# Dropping all columns but text\n\ndf = df.drop(['id', 'date', 'query', 'user_id'], axis=1)","cce4e3c6":"# Replacing 4 with 1 for clarity\n# 0:Negative, 1:Positive\ndf = df.replace(4, 1)","784c0904":"df.head(-5)","1048a3c7":"# Confirming that the dataset is balanced\nsen_value = df['sentiment'].value_counts()\nprint(sen_value)\nplt.bar(sen_value.index, sen_value.values)","feb0eda9":"stop_words = stopwords.words('english')\nstemmer = SnowballStemmer('english')\ntext_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"","0570466f":"def preprocess(text):\n  text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n  tokens = []\n  for token in text.split():\n    if token not in stop_words:\n        tokens.append(token)\n  return \" \".join(tokens)","6d193535":"df['text'] = df['text'].apply(lambda x: preprocess(x))","79e5d3db":"df['text'][400000]","01c6377a":"df = shuffle(df)","55df98b0":"embed_dim = 100\nmax_length = 16\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"\ntraining_size= 160000 \ntest_portion=.1","439e4ac4":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(df['text'])\n\nword_index = tokenizer.word_index\nvocab_size = len(word_index)\n\nsequences = tokenizer.texts_to_sequences(df['text'])\npadded = pad_sequences(sequences,\n                      maxlen=max_length,\n                      padding=padding_type,\n                      truncating=trunc_type)\n\nsplit = int(test_portion * training_size)\n\ntest_seq = padded[:split]\ntrain_seq = padded[split:training_size]\ntest_labels = df['sentiment'][:split]\ntrain_labels = df['sentiment'][split:training_size]","7a0cc5dc":"print(vocab_size)\nprint(word_index['good'])","2ad57f64":"embed_index = {}\n\nwith open('\/kaggle\/input\/glove6b100dtxt\/glove.6B.100d.txt') as f:\n    for line in f:\n        values = line.split();\n        word = values[0];\n        coefs = np.asarray(values[1:], dtype='float32');\n        embed_index[word] = coefs;\n        \nembed_matrix = np.zeros((vocab_size+1, embed_dim));\nfor word, i in word_index.items():\n    embed_vector = embed_index.get(word);\n    if embed_vector is not None:\n        embed_matrix[i] = embed_vector;","5755a848":"print(len(embed_matrix))","942155d5":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embed_dim, input_length=max_length, weights=[embed_matrix], trainable=False),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy',\n             optimizer='adam',\n             metrics=['accuracy'])\n\nmodel.summary()","f1315e10":"num_epochs = 50\n\ntrain_padded = np.array(train_seq)\ntrain_labels = np.array(train_labels)\ntest_padded = np.array(test_seq)\ntest_labels = np.array(test_labels)\n\nhistory = model.fit(train_padded, train_labels, epochs=num_epochs, validation_data=(test_padded, test_labels), verbose=1)\n\nprint(\"Training Complete\")","cdbeca29":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\n# Plot training and validation accuracy per epoch\nplt.plot(epochs, acc, 'r')\nplt.plot(epochs, val_acc, 'b')\nplt.title('Training and validation accuracy')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend([\"Accuracy\", \"Validation Accuracy\"])\n\nplt.figure()\n\n\n# Plot training and validation loss per epoch\nplt.plot(epochs, loss, 'r')\nplt.plot(epochs, val_loss, 'b')\nplt.title('Training and validation loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Loss\", \"Validation Loss\"])\n\nplt.figure()","71061600":"### Creating embedding_matrix from glove","715257de":"### Modelling","13d578c0":"### Tokenizing","f708d1a3":"### Text Preprocessing","0da2ba11":"tfjhvutcybuyccsscs"}}