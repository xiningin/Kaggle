{"cell_type":{"6c4f7061":"code","e0962fa4":"code","c929806e":"code","74f95556":"code","1214ba70":"code","b98935a3":"code","23d1d19f":"code","713748dc":"code","f6fa941d":"code","1b3394fd":"code","587eb1f5":"code","af1abdb6":"code","691ad31a":"code","4b4c4211":"code","b87d3676":"code","69388ffe":"code","e0223ec9":"code","a379f0b1":"code","a53d83f5":"code","e57a7201":"code","300a46f6":"code","8c92afe3":"code","b4ccb13e":"code","c7b8118c":"code","a6404ca7":"code","3530b724":"markdown","ada8b904":"markdown","8844799f":"markdown","d2b15bbe":"markdown","31244299":"markdown","474c1d94":"markdown","66fc0e7b":"markdown","9e5d2685":"markdown","850c3f0d":"markdown","08b7422a":"markdown"},"source":{"6c4f7061":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e0962fa4":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')","c929806e":"df.shape","74f95556":"df.columns","1214ba70":"df.describe()","b98935a3":"df.Class.value_counts()","23d1d19f":"nb_fraud = (df.Class == 1).sum()\nnb_noFraud = (df.Class == 0).sum()\npct_fraud = nb_fraud\/(nb_fraud + nb_noFraud) * 100\n\nprint(\"Le pourcentage de transactions frauduleuses est de \", '%.3f' % pct_fraud)","713748dc":"null_values = df.isnull().values.sum()\nif null_values == 0 :\n    print('Aucune valeur manquante')","f6fa941d":"from sklearn.ensemble import RandomForestClassifier","1b3394fd":"from sklearn.model_selection import train_test_split\n\nX = df.drop(['Class'], axis=1)\ny = df.Class\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=1)","587eb1f5":"from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","af1abdb6":"def plot_roc_curve(est,X_test,y_test) :\n    probas = est.predict_proba(X_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,probas[:, 1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    plt.figure(figsize=(8,8))\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\n    plt.legend(loc='lower right')\n    plt.plot([0,1],[0,1],'r--')        # plus mauvaise courbe\n    plt.plot([0,0,1],[0,1,1],'g:')     # meilleure courbe\n    plt.xlim([-0.05,1.2])\n    plt.ylim([-0.05,1.2])\n    plt.ylabel('Taux de vrais positifs')\n    plt.xlabel('Taux de faux positifs')\n    plt.show","691ad31a":"rf = RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)","4b4c4211":"print(y_test.value_counts())\nprint()\nprint(\"Pct de fraudes : \", 87\/(87+56875)*100)","b87d3676":"print(classification_report(y_test, y_pred))","69388ffe":"cm = confusion_matrix(y_test, y_pred)\nprint(cm)","e0223ec9":"print(\"Accuracy : \", rf.score(X_test, y_test))","a379f0b1":"from imblearn.under_sampling import RandomUnderSampler \n\nrus = RandomUnderSampler()\nX_train, y_train = rus.fit_sample(X_train, y_train)","a53d83f5":"y_train.value_counts()","e57a7201":"rf = RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","300a46f6":"#on \"remet \u00e0 z\u00e9ro\" nos X et y car ils ont \u00e9t\u00e9 modifi\u00e9s lors du sous-\u00e9chantillonage\nX = df.drop(['Class'], axis=1)\ny = df.Class\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=1)","8c92afe3":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE()\nX_train, y_train = smote.fit_sample(X_train, y_train)","b4ccb13e":"y_train.value_counts()","c7b8118c":"rf = RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nprint()\n\ncm = confusion_matrix(y_test, y_pred)\nprint('Matrice de confusion : ')\nprint()\nprint(cm)","a6404ca7":"#affichage d\u00e9taill\u00e9 de l'accuracy qui est arrondie \u00e0 1 dans le classification report\nprint(\"Accuracy : \", rf.score(X_test, y_test))","3530b724":"### Sous-\u00e9chantillonage","ada8b904":"R\u00e9sultats beaucoup moins bons qu'avec le dataset complet. La pr\u00e9cision chute pour pr\u00e9dire \"1\".","8844799f":"31 colonnes ; 30 variables explicatives + 1 variable expliqu\u00e9e\n\nParmi les 30 explicatives, 28 variables issues d'une PCA, 1 variable temps et 1 variable montant.\n\nLa variable expliqu\u00e9e, *Class*, est un bool\u00e9en indiquant si la transaction est une fraude (1) ou non (0).","d2b15bbe":"## Exploration des donn\u00e9es","31244299":"*Dataset complet :* \n\n[[56871     4]\n\n [   21    **66**]]\n\n*Sur-\u00e9chantillonage :*\n\n[[56865    10]\n\n [   18    **69**]]","474c1d94":"### Comparaison\n\nLe sur-\u00e9chantillonage est la seule technique apportant des r\u00e9sultats probants. On va donc maintenant la comparer avec les r\u00e9sultats du mod\u00e8le entrain\u00e9 \u00e0 partir du dataset complet.","66fc0e7b":"### Dataset d\u00e9s\u00e9quilibr\u00e9","9e5d2685":"### Sur-\u00e9chantillonage","850c3f0d":"## Machine Learning\n\nLe dataset \u00e9tant tr\u00e8s d\u00e9s\u00e9quilibr\u00e9, nous allons devoir utiliser les techniques de **sous-\u00e9chantillonage** et **sur-\u00e9chantillonage** afin que notre mod\u00e8le ne soit pas influenc\u00e9 par la r\u00e9partition du dataset dans sa pr\u00e9diction.\n\nNous entrainerons \u00e9galement un mod\u00e8le sur le dataset tel quel afin de comparer nos r\u00e9sultats.\n\nPour la pr\u00e9diction, nous utiliserons le mod\u00e8le des **For\u00eats Al\u00e9atoires**.","08b7422a":"**Accuracy** tr\u00e8s l\u00e9g\u00e8rement sup\u00e9rieure sur le **dataset complet**.\n\n**Nombre de vrais positifs** (i.e fraudes d\u00e9tect\u00e9es) **plus \u00e9lev\u00e9 en sur-\u00e9chantillonage (69 VS 66).**\n\n***\n\nDataset complet :\n\n- 21 fois le mod\u00e8le a pr\u00e9dit non-fraude alors que c'\u00e9tait une fraude\n- 4 fois le mod\u00e8le a pr\u00e9dit fraude alors que \u00e7a n'en \u00e9tait pas une\n\nSur-\u00e9chantillonage :\n\n- 18 fois le mod\u00e8le a pr\u00e9dit non-fraude alors que c'\u00e9tait une fraude\n- 10 fois le mod\u00e8le a pr\u00e9dit fraude alors que \u00e7a n'en \u00e9tait pas une\n\n***\n\nDans le cas de la probl\u00e9matique de la fraude bancaire, **le sur-\u00e9chantillonage est plus adapt\u00e9**.\n\nEn effet, m\u00eame s'il perd en pr\u00e9cision, il est plus apte \u00e0 d\u00e9tecter les fraudes. Or, dans notre cas, il vaut mieux se tromper en cat\u00e9gorisant une transaction honn\u00eate \"fraude\" plut\u00f4t que de laisser passer une transaction frauduleuse. "}}