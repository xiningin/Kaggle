{"cell_type":{"c2830808":"code","d1c95b48":"code","5cdde671":"code","4fceb912":"code","33697b37":"code","af54e254":"code","68a2f6dc":"code","7d5f2b76":"code","c0d25eb8":"code","2f7627a3":"code","e55f7418":"code","3a77535b":"markdown","ec64f548":"markdown","ab14c0c2":"markdown"},"source":{"c2830808":"import numpy as np\nimport pandas as pd\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom textblob import TextBlob\nfrom textblob import Word","d1c95b48":"train=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntrain.head()","5cdde671":"train=train.drop(['id','keyword','location'],axis=1)\ntrain.info()","4fceb912":"test=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv').drop(['id','keyword','location'],axis=1)\ntest.head()","33697b37":"train.info(),test.info()","af54e254":"def processRow(row):\n    \n    tweet = row\n    #Lower case\n    tweet.lower()\n    #Removes unicode strings like \"\\u002c\" and \"x96\"\n    tweet = re.sub(r'(\\\\u[0-9A-Fa-f]+)',\" \", tweet)\n    tweet = re.sub(r'[^\\x00-\\x7f]',\" \",tweet)\n    #convert any url to URL\n    tweet = re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+))','URL',tweet)\n    #Convert any @Username to \"AT_USER\"\n    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n    #Remove additional white spaces\n    tweet = re.sub('[\\s]+', ' ', tweet)\n    tweet = re.sub('[\\n]+', ' ', tweet)\n    #Remove not alphanumeric symbols white spaces\n    tweet = re.sub(r'[^\\w]', ' ', tweet)\n    #Removes hastag in front of a word \"\"\"\n    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n    #Replace #word with word\n    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n    #Remove :( or :)\n    tweet = tweet.replace(':)',\" \")\n    tweet = tweet.replace(':(',\" \")\n    #remove numbers\n    tweet = \"\".join([i for i in tweet if not i.isdigit()])\n    #remove multiple exclamation\n    tweet = re.sub(r\"(\\!)\\1+\", ' ', tweet)\n    #remove multiple question marks\n    tweet = re.sub(r\"(\\?)\\1+\", ' ', tweet)\n    #remove multistop\n    tweet = re.sub(r\"(\\.)\\1+\", ' ', tweet)\n    #lemma\n    from textblob import Word\n    tweet =\" \".join([Word(word).lemmatize() for word in tweet.split()])\n    tweet = tweet.strip('\\'\"')\n    #lowercase\n    tweet = tweet.lower()\n    tweet = tweet.split()\n    ps = PorterStemmer()\n    all_stopwords = stopwords.words('english')\n    tweet = [ps.stem(word) for word in tweet if not word in set(all_stopwords)]\n    tweet = ' '.join(tweet)\n    \n    \n    row = tweet\n    return row\n\ndef processData(dataset,predictor):\n    \n    corpus=[]\n    for i in range(0,len(dataset)):\n        tweet = processRow(predictor[i])\n        corpus.append(tweet)\n    return corpus\n\ncorpus_train=processData(train,train['text'])\ncorpus_test=processData(test,test['text'])","68a2f6dc":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 3000)\n\nX = cv.fit_transform(corpus_train).toarray()\ny=train.iloc[:,-1].values\n\n\nXt=cv.transform(corpus_test).toarray()\n\nX.shape,y.shape,Xt.shape","7d5f2b76":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","c0d25eb8":"import xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nxgb1 = xgb.XGBRegressor()\nparameters = {'objective':['reg:squarederror'],\n              'booster':['gbtree'],\n              'learning_rate': [0.05,0.2,0.25,0.3], #so called `eta` value\n              'max_depth': [5, 8, 10,12],\n              'n_estimators': [100]}\n\n#Use gridsearch with 2 folds for finding optimal parameters and vlaues\nxgb_grid = GridSearchCV(xgb1,\n                        parameters,\n                        cv = 2,\n                        n_jobs = 2,\n                        verbose=True)\n\nxgb_grid.fit(X_train,y_train)\nprint(xgb_grid.best_params_)\n\n#output: booster= 'gbtree', learning_rate= 0.3, max_depth= 5, n_estimators= 100, objective= 'reg:squarederror' ","2f7627a3":"# Instantiate an XGBoost object with hyperparameters\nxgb_reg = xgb.XGBRegressor(booster= 'gbtree', learning_rate= 0.3, max_depth= 5, n_estimators= 100, objective= 'reg:squarederror')\n# Train the model with train data sets\nxgb_reg.fit(X_train, y_train)\n\ny_pred = xgb_reg.predict(X_test) # Predictions\n\ny_pred[y_pred<0.5]=0\ny_pred[y_pred>0.5]=1\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(accuracy_score(y_test, y_pred))","e55f7418":"y_pred_test=xgb_reg.predict(Xt)\n\ny_pred_test[y_pred_test<0.5]=0\ny_pred_test[y_pred_test>0.5]=1\n\ny_pred_test=y_pred_test.astype(int)\n\n\ntest_output=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\noutput = pd.DataFrame({'id': test_output.id, 'target': y_pred_test.astype('str')})\noutput.to_csv(\"nlp_submission.csv\", index=False)\nprint('Submission done!')","3a77535b":"**Import datasets and libraries**","ec64f548":"**Create two functions (one nested) that does the cleaning and processing the data**","ab14c0c2":"**Use CountVectorizer to create a matrix of token counts. One for training and one for testing**"}}