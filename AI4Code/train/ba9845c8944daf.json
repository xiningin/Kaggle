{"cell_type":{"37779fbf":"code","003f9aa2":"code","bd360894":"code","df2ca086":"code","f2899469":"code","4d54161a":"code","c30b85b2":"code","c96e53a2":"code","f0a1bc7a":"code","46e9daf0":"code","96a8f05c":"code","70fe82ad":"code","ed4f1ff5":"code","6aff6643":"code","151d8539":"code","ec77e73a":"code","fa5825bd":"code","947c85b7":"code","28e946b2":"code","e97159af":"code","7df0c195":"code","2b23293e":"code","87c667d1":"code","05d9019c":"code","f6e19f3a":"code","852908e8":"code","80e97fdf":"code","3be4358c":"code","0512c766":"code","7064fa2f":"code","d9a9a47c":"code","5ca80dd8":"code","e0a822fc":"code","d18090a2":"code","e6ca8938":"code","31e50345":"code","a69a51d3":"code","b923fb77":"code","2015d1d6":"code","4bf4ecba":"code","256067cf":"code","fc379085":"code","e8cd46b4":"code","6c64d0b0":"code","508c0b24":"code","5c506f22":"code","fceb6802":"markdown","6924d8db":"markdown","56a7b97b":"markdown","2e30a3c4":"markdown","523304f4":"markdown","04f13fb5":"markdown","6b2e630c":"markdown","3a030ca6":"markdown","5c475b4b":"markdown","471627bc":"markdown","761c1f9f":"markdown","f5cd6bd6":"markdown","7e9be38a":"markdown","84bdd103":"markdown","e74dc837":"markdown","6967c154":"markdown","99ac939b":"markdown","c493bda0":"markdown","aa2f9432":"markdown","6d6e6be9":"markdown","a3f76529":"markdown","d2430a76":"markdown","3b578667":"markdown","6fcb31c7":"markdown"},"source":{"37779fbf":"!pip install split_folders","003f9aa2":"#import required libraries \nimport os\nimport zipfile\nimport splitfolders \nimport time\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport cv2\n\nimport tensorflow as tf\nimport keras\nimport keras_preprocessing\nfrom keras_preprocessing import image\nfrom keras_preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications import  MobileNetV2, ResNet50\nfrom tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_input_resnet50\nfrom tensorflow.keras.applications.vgg16 import preprocess_input as preprocess_input_vgg16\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input as preprocess_input_mobilenetv2","bd360894":"!ls ..\/input\/flowers-recognition\/flowers\/","df2ca086":"# Split the Dataset folders into train test val folders\nsplitfolders.ratio(\"..\/input\/flowers-recognition\/flowers\/\", output=\"output\", seed=100, ratio=(.8, .1, .1), group_prefix=None) ","f2899469":"!ls  .\/output","4d54161a":"#create paths for folders \n\n\ndaisy_dir = os.path.join('.\/output\/train\/daisy')\ndandelion_dir = os.path.join('.\/output\/train\/dandelion')\nrose_dir = os.path.join('.\/output\/train\/rose')\nsunflower_dir = os.path.join('.\/output\/train\/sunflower')\ntulip_dir = os.path.join('.\/output\/train\/tulip')\n\n#count number of files in each directory\nprint('Total training Daisy images :',len(os.listdir(daisy_dir)))\nprint('Total training Dandelion images :',len(os.listdir(dandelion_dir)))\nprint('Total training Rose images :',len(os.listdir(rose_dir)))\nprint('Total training Sunflower images :',len(os.listdir(sunflower_dir)))\nprint('Total training Tulip images :',len(os.listdir(tulip_dir)))\n\nprint('\\n')\ndaisy_files = os.listdir(daisy_dir)\ndandelion_files = os.listdir(dandelion_dir)\nrose_files = os.listdir(rose_dir)\nsunflower_files = os.listdir(sunflower_dir)\ntulip_files = os.listdir(tulip_dir)\n\nprint(\"Total len of training images\",len(daisy_files+dandelion_files+rose_files+sunflower_files+tulip_files))","c30b85b2":"#count of images available in test dataset for each category\nprint('Total Test Daisy images :',len(os.listdir('output\/test\/daisy')))\nprint('Total Test Dandelion images :',len(os.listdir('output\/test\/dandelion')))\nprint('Total Test Rose images :',len(os.listdir('output\/test\/rose')))\nprint('Total Test Sunflower images :',len(os.listdir('output\/test\/sunflower')))\nprint('Total Test Tulip images :',len(os.listdir('output\/test\/tulip')))","c96e53a2":"#to plot images get complete paths for images\npic_index = 10\n\n#get paths for some of the immages from each folder\ndaisy_fewimg = [os.path.join(daisy_dir,fname) for fname in daisy_files[pic_index-10:pic_index]]\ndandelion_fewimg = [os.path.join(dandelion_dir,fname) for fname in dandelion_files[pic_index-10:pic_index]]\nrose_fewimg = [os.path.join(rose_dir,fname) for fname in rose_files[pic_index-10:pic_index]]\nsunflower_fewimg = [os.path.join(sunflower_dir,fname) for fname in sunflower_files[pic_index-10:pic_index]]\ntulip_fewimg = [os.path.join(tulip_dir,fname) for fname in tulip_files[pic_index-10:pic_index]]\n","f0a1bc7a":"#plot random image \nimport PIL.Image as Image\n\nprint(cv2.imread(daisy_fewimg[0]).shape)\nImage.open(daisy_fewimg[0])","46e9daf0":"Image.open(daisy_fewimg[0]).resize((224,224))","96a8f05c":"#function to convert BGR image to RGB\ndef cvtRGB(img):\n    return cv2.cvtColor(img.copy(), cv2.COLOR_BGR2RGB)","70fe82ad":"#plot smaple of images function\ndef plot_img(imgpath_list):\n\n  fig, ax = plt.subplots(3,3,figsize=(10,10))\n  k = 0\n  for j in range(3):\n    for i in range(3):\n        img = cv2.resize(cv2.imread(imgpath_list[k]),(224,224))\n        k=k+1\n        ax[i,j].imshow(cvtRGB(img));\n        ax[i,j].axis('off');","ed4f1ff5":"print('\\t\\t\\t\\tDaisy')\nplot_img(daisy_fewimg)","6aff6643":"print('\\t\\t\\t\\tDadelion')\nplot_img(dandelion_fewimg)","151d8539":"print('\\t\\t\\t\\tRose')\nplot_img(rose_fewimg)","ec77e73a":"print('\\t\\t\\t\\tTulip')\nplot_img(tulip_fewimg)","fa5825bd":"print('\\t\\t\\t\\tSunflower')\nplot_img(sunflower_fewimg)","947c85b7":"#image Augumentation using ImageDataGenerator on train data \n#Data augmentation is used to increase the size of training set and to get more different images\ntraining_dir = '.\/output\/train\/'\ntraining_datagen = ImageDataGenerator(\n    rescale = 1.\/255, #normalization\n    rotation_range = 40,\n    width_shift_range = 0.4,\n    height_shift_range = 0.4,\n    shear_range = 0.2,\n    zoom_range = 0.1,\n    fill_mode = 'nearest',\n    horizontal_flip = True)","28e946b2":"#do not perform augmentation on validation and test data\/validation set ,this might mislead the results\nvalidation_dir = '.\/output\/val'\nvalidation_datagen = ImageDataGenerator(\n    rescale=1.\/255\n)\n\ntest_dir = '.\/output\/test'\ntest_datagen = ImageDataGenerator(\n    rescale=1.\/255\n)","e97159af":"#flow_from_directory() method allows you to read the images directly from the directory and augment them while the neural network model is learning on the training data.\ntrain_generator = training_datagen.flow_from_directory(\n    training_dir,\n    target_size = (224,224), #rescale images to fixed size\n    class_mode = 'categorical',\n    batch_size = 32\n)\nvalidation_generator = validation_datagen.flow_from_directory(\n    validation_dir,\n    target_size = (224,224),\n    class_mode = 'categorical',\n    batch_size = 32\n)\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size = (224,224),\n    class_mode = 'categorical',\n    batch_size = 32\n)","7df0c195":"#tf.keras.backend.clear_session()","2b23293e":"# Build CNN sequential model and train from scratch \ndef model_from_scratch():\n  #Build model\n  model = tf.keras.models.Sequential([\n              tf.keras.layers.Conv2D(32, (3,3), activation = 'relu', input_shape = (224,224,3)),\n              tf.keras.layers.MaxPool2D(2,2),\n              tf.keras.layers.Dropout(0.5),\n              tf.keras.layers.Conv2D(64, (3,3), activation = 'relu'),\n              tf.keras.layers.MaxPool2D(2,2),\n              tf.keras.layers.Dropout(0.5),\n              tf.keras.layers.Conv2D(128, (3,3), activation = 'relu'),\n              tf.keras.layers.MaxPool2D(2,2),\n              tf.keras.layers.Dropout(0.5),\n              tf.keras.layers.Flatten(),\n              tf.keras.layers.Dropout(0.5),\n              tf.keras.layers.Dense(512, activation = 'relu'),\n              tf.keras.layers.Dense(5, activation = 'softmax')\n  ])\n\n  #model compilation\n  model.compile(loss='categorical_crossentropy',\n              optimizer = tf.keras.optimizers.Adam(),\n              metrics = ['accuracy']\n            )\n  \n  return model\n\nmodel_from_scratch().summary() #Model Summary","87c667d1":"#fit model to train dataset \nepochs = 25\nstart = time.time()\n\nmodel = model_from_scratch()\nhistory1 = model.fit(train_generator,\n                    epochs=epochs,\n                    validation_data=validation_generator,\n                    validation_steps=5,\n                    verbose=2\n                    )\n\nend = time.time()\nduration = end - start\nprint ('\\n Model built from scratch training took %0.2f seconds (%0.1f minutes) to train for %d epochs'%(duration, duration\/60, epochs) )","05d9019c":"#function to get accuracy and loss from history \ndef get_history_data(history):\n    \n  train_acc = history.history['accuracy']\n  val_acc = history.history['val_accuracy']\n\n  train_loss = history.history['loss']\n  val_loss = history.history['val_loss']\n\n  return train_acc,val_acc,train_loss,val_loss","f6e19f3a":"train_acc,val_acc,train_loss,val_loss = get_history_data(history1)","852908e8":"#plot Accuracy graph\nepochs = range(len(train_acc))\nplt.figure(figsize=(7,7));\nplt.plot(epochs,train_acc,label ='Train accuracy');\nplt.plot(epochs,val_acc,label ='Validation accuracy');\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend();\nplt.title('Training Vs Validation Accuracy');","80e97fdf":"#plot validation graph\nplt.figure(figsize=(7,7));\nplt.plot(epochs,train_loss,label ='Train Loss');\nplt.plot(epochs,val_loss,label ='Validation Loss');\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend();\nplt.title('Training Vs Validation Loss');","3be4358c":"model.save('.\/model_from_stratch.h5')","0512c766":"#check how model performs on test data(unseen data)\nmodel.evaluate(test_generator)","7064fa2f":"#tf.keras.backend.clear_session()","d9a9a47c":"#use VGG16 pre-trained model\ndef create_model_from_VGG16():\n   \n    model = VGG16(weights = \"imagenet\", include_top=False, input_shape = (224,224, 3))\n    \n    # don't train existing weights \n    for layer in model.layers:\n      layer.trainable = False\n\n    #Adding custom Layers \n    x = model.output\n    x = keras.layers.Flatten()(x)\n    x = keras.layers.Dense(128, activation=\"relu\")(x)\n    #x = keras.layers.Dropout(0.5)(x)\n    #x = keras.layers.Dense(1024, activation=\"relu\")(x)\n    predictions = keras.layers.Dense(5, activation=\"softmax\")(x)\n    \n    # creating the final model \n    final_model = keras.models.Model(inputs = model.input, outputs = predictions)\n    \n    final_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n    \n    return final_model\n\ncreate_model_from_VGG16().summary()","5ca80dd8":"epochs = 10\nstart = time.time()\n\nmodel_from_vgg16 = create_model_from_VGG16()\nhistory2 = model_from_vgg16.fit(train_generator,\n                    epochs=epochs,\n                    validation_data=validation_generator,\n                    validation_steps=5,\n                    verbose=2\n                     )\n\nend = time.time()\nduration = end - start\nprint ('\\n model from trained with VGG16 took %0.2f seconds (%0.1f minutes) to train for %d epochs'%(duration, duration\/60, epochs) )","e0a822fc":"pd.DataFrame(history2.history).plot(figsize=(8,5))\nplt.show()","d18090a2":"model.save('.\/model_from_VGG16.h5')","e6ca8938":"model_from_vgg16.evaluate(test_generator)","31e50345":"#tf.keras.backend.clear_session()","a69a51d3":"#use ResNet50 pre-trained model\ndef create_model_from_ResNet50():\n   \n    model = ResNet50(weights = \"imagenet\", include_top=False, input_shape = (224,224, 3))\n    \n    # don't train existing weights\n    for layer in model.layers:\n      layer.trainable = False\n\n    #Adding custom Dense Layers \n    x = model.output\n    x = keras.layers.Flatten()(x)\n    #x = keras.layers.Dense(128, activation=\"relu\")(x)\n    #x = keras.layers.Dropout(0.5)(x)\n    #x = keras.layers.Dense(512, activation=\"relu\")(x)\n    x = keras.layers.Dropout(0.5)(x)\n    x = keras.layers.Dense(256, activation=\"relu\")(x)\n    predictions = keras.layers.Dense(5, activation=\"softmax\")(x)\n    \n    # creating the final model \n    final_model = keras.models.Model(inputs = model.input, outputs = predictions)\n    \n    final_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n    \n    return final_model\n\ncreate_model_from_ResNet50().summary()","b923fb77":"#epochs = 10\n#start = time.time()\n\n#model_resNet50 = create_model_from_ResNet50()\n#history3 = model_resNet50.fit(train_generator,\n#                    epochs=epochs,\n#                    validation_data=validation_generator,\n#                    validation_steps=5,\n#                    verbose=2\n#                     )\n\n#end = time.time()\n#duration = end - start\n#print ('\\n model from trained from ResNet50 took %0.2f seconds (%0.1f minutes) to train for %d epochs'%(duration, duration\/60, epochs) )","2015d1d6":"#model_resNet50.evaluate(test_generator)","4bf4ecba":"#using preprocess function of pretarined model so that model is not overfitted and trained properly\ntraining_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_mobilenetv2)\ntest_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_mobilenetv2)\n\ntrain_generator = training_datagen.flow_from_directory(\n    training_dir,\n    target_size = (224,224),\n    class_mode = 'categorical',\n    batch_size = 32\n)\nvalidation_generator = validation_datagen.flow_from_directory(\n    validation_dir,\n    target_size = (224,224),\n    class_mode = 'categorical',\n    batch_size = 32\n)\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size = (224,224),\n    class_mode = 'categorical',\n    batch_size = 32\n)","256067cf":"#use Rmobilenetv2 pre-trained model\ndef create_model_from_mobilenetv2():\n   \n    model = MobileNetV2(weights = \"imagenet\", include_top=False, input_shape = (224,224, 3))\n    \n    # don't train existing weights\n    for layer in model.layers:\n      layer.trainable = False\n\n    #Adding custom Dense Layers \n    x = model.output\n    x = keras.layers.Flatten()(x)\n    x = keras.layers.Dense(128, activation=\"relu\")(x)\n    x = keras.layers.Dropout(0.5)(x)\n    x = keras.layers.Dense(512, activation=\"relu\")(x)\n    predictions = keras.layers.Dense(5, activation=\"softmax\")(x)\n    \n    # creating the final model \n    final_model = keras.models.Model(inputs = model.input, outputs = predictions)\n    \n    final_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n    \n    return final_model\n\ncreate_model_from_mobilenetv2().summary()","fc379085":"epochs = 10\nstart = time.time()\n\nmodel_mobilenetV2 = create_model_from_mobilenetv2()\nhistory4 = model_mobilenetV2.fit(train_generator,\n                    epochs=epochs,\n                    validation_data=validation_generator,\n                    validation_steps=5,\n                    verbose=2\n                     )\n\nend = time.time()\nduration = end - start\nprint ('\\n model from trained from MobileNetV2 took %0.2f seconds (%0.1f minutes) to train for %d epochs'%(duration, duration\/60, epochs) )","e8cd46b4":"model_mobilenetV2.evaluate(test_generator)","6c64d0b0":"!pip install -q gradio","508c0b24":"import gradio as gr \nimport tensorflow as tf\nimport numpy as np\nimport requests\nfrom keras.preprocessing import image","5c506f22":"#used model trained using vgg16 as it performed well on both train and test data\nlabels = ['Daisy','Dadelion','Rose','Sunflower','Tulip'] #classes\n\ndef classify_image(inp):\n  img = inp.reshape((-1,224,224,3)) #reshape input image\n  prediction = model_from_vgg16.predict(img).flatten() #prediction\n  return {labels[i]: float(prediction[i]) for i in range(5)} #return classes\n\nimage = gr.inputs.Image(shape=(224, 224))\nlabel = gr.outputs.Label(num_top_classes=1)\n\ngr.Interface(fn=classify_image, inputs=image, outputs=label, capture_session=True).launch(debug=True,share=True)","fceb6802":"#### MobileNetV2","6924d8db":"#### Visualize model performance","56a7b97b":"**model performance on test data:**\n**loss: 0.5357 - accuracy: 0.8276**","2e30a3c4":"### Import required libraries and Load the dataset","523304f4":"##### Performance on Test Data:\n**loss: 0.7838 - accuracy: 0.8483**","04f13fb5":"Few images has other objects present","6b2e630c":"### ResNet50","3a030ca6":"###### the flucation on validation accuarcy is nothing but overfitting , which can be probaly because of the noise present in the data","5c475b4b":"## Transfer Learning : Using Pre-trained model to improve model performance","471627bc":"![image.png](attachment:ef0681e9-c2bc-4871-8347-7ef7cb99426b.png)","761c1f9f":"**loss: 0.8467 - accuracy: 0.7034** \n\n**Model has performed well on test data**","f5cd6bd6":"##### Model Performance :\n**loss: 0.4698 - accuracy: 0.8473 - val_loss: 0.9422 - val_accuracy: 0.7875**","7e9be38a":"![screenshot (4).png](attachment:7b23bbea-341c-4caa-93c2-620549e514f6.png)\n\n**Actual : Daisy**\n\n**Prediction: Daisy**","84bdd103":"### Test model by uploading random images and check results","e74dc837":"##### val_accuracy:0.6625\n##### val_loss : 0.8049 \n##### Model has been trained well with 66% accuracy , but will try increasing it further with Transfer learning","6967c154":"![screenshot (3).png](attachment:4fce91a7-a8c4-4f0a-9e5a-d667f5ff4612.png)\n\n**Actual : Dadelion**\n\n**Prediction : Daisy**\n\n\n![screenshot (5).png](attachment:d8ac75d3-1ca8-467f-bf42-b4e4241ca7e3.png)\n\n**Actual : Tulip**\n\n**Prediction : Tulip**\n\n\n![screenshot (6).png](attachment:4f7b953a-58b3-43ea-b637-372e2bed63d1.png)\n\n**Actual : Daisy**\n\n**Prediction : Daisy**","99ac939b":"### Visualization","c493bda0":"### VGG16 : ","aa2f9432":"**Please upvote my notebook if you found it informative and helpful. If you want to give any suggestions and queries regarding notebook please feel free to mention it in comments**","6d6e6be9":"##### results\n**loss: 0.5707 - accuracy: 0.7871 - val_loss: 0.6541 - val_accuracy: 0.7875**\nBetter than the perevious model (model built from scratch)","a3f76529":"### Model Building","d2430a76":"### Pre-Processing ","3b578667":"\n### Author : Kalyani Avhale\n# Flower Recognition \ud83c\udf38 \n  - Task : Recognize what kind of flower is that?\n  - This dataset contains 4242 images of flowers.\n  - The pictures are divided into five classes: chamomile, tulip, rose, sunflower, dandelion.\n  - For each class there are about 800 photos. Photos are not high resolution, about 320x240 pixels. Photos are not reduced to a single size, they have different proportions!\n  \n  ###### Dataset : https:\/\/www.kaggle.com\/alxmamaev\/flowers-recognition","6fcb31c7":"![screenshot.png](attachment:e6b1321f-1f0c-462f-a1c3-ae65114442a5.png)\n\n**Actual : Rose**\n\n**Prediction : Rose**"}}