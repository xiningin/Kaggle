{"cell_type":{"01bf25db":"code","4ef611f1":"code","86aec48c":"code","f2c0576a":"code","e54c598d":"code","f012dc81":"code","fbe045e4":"code","9a13a148":"code","bb485380":"code","55071027":"code","67fc1d36":"code","cdfa0c4f":"code","b49ee22a":"code","354fd5aa":"code","6ce39457":"markdown","78fe9cfc":"markdown","2b1ac1b7":"markdown","f47b9f40":"markdown","5f1ab410":"markdown","669f8615":"markdown","580f6ca4":"markdown","63e80707":"markdown","1f1fbfdf":"markdown","a087183d":"markdown","04e48796":"markdown","2530565c":"markdown"},"source":{"01bf25db":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfrom fastai.vision import *\nimport fastai\nimport pathlib\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.tabular import *\nfrom torchvision.transforms import *\n\nfrom itertools import product\nimport cv2\nfrom tqdm import tqdm_notebook as tqdm\n\ndef dense_patchify(img, patch_size,min_overlap=0.1,scale=1 ):\n\n    i_h, i_w, channel = np.shape(img)\n    p_h, p_w = patch_size\n\n    if i_h < p_h | i_w < p_w:\n        print(\"image is too small for this patch size. Please add black pixels or resize images\")\n\n    if scale != 1:\n        img = cv2.resize(img, (round(i_w*scale),round(i_h*scale)))\n\n\n\n    n_h = 1\n    n_w =1\n    c_h, c_w = (i_h-p_h),(i_w-p_h)\n\n    while c_h > p_h*(1-min_overlap):\n        n_h += 1\n        c_h = round((i_h-p_h)\/n_h)\n\n    while c_w > p_w*(1-min_overlap):\n        n_w += 1\n        c_w = round((i_w-p_w)\/n_w)\n\n\n    new_img = np.zeros(((c_h*n_h)+p_h+n_h,(c_w*n_w)+p_w+n_w,channel))\n    new_img[:i_h,:i_w]=img\n    \n    possible_h = range(0,(c_h*(n_h+1)), c_h )\n    possible_w = range(0,(c_w*(n_w+1)), c_w )\n\n    origins_array = list(product(possible_h, possible_w))\n\n    patches  = np.zeros((len(origins_array), p_h, p_w,3))\n    \n    for idx,origin in enumerate(origins_array):\n        patches[idx,:,:,:] = cv2.resize(new_img[origin[0]:origin[0]+p_h,origin[1]:origin[1]+p_w,:], (p_h,p_w),  interpolation = cv2.INTER_AREA) \n            \n    return patches, np.array(origins_array)\n","4ef611f1":"np.random.seed(84) # Here we fix the randomness to have replicable results\n\n\n# Here are the path to dataset\ndata_dir = \"\/kaggle\/input\/v2-plant-seedlings-dataset\/nonsegmentedv2\/\"\ndata_dir = pathlib.Path(data_dir)\n\n\ndata = ImageDataBunch.from_folder(data_dir, train='.', valid_pct=0.2,\n                                  ds_tfms=(), size=224).normalize(imagenet_stats)\n\n\n","86aec48c":"data.show_batch(rows=5, figsize=(15, 15))\n\n","f2c0576a":"# Link to doc : https:\/\/docs.fast.ai\/vision.transform.html#get_transforms\n\ntfms = get_transforms(do_flip=True, flip_vert=True, max_rotate=180.0, max_zoom=1.3, max_lighting=0.5, max_warp=0.1, p_affine=0.75, p_lighting=0.75)\ndata = ImageDataBunch.from_folder(data_dir, train='.', valid_pct=0.2,\n                                  ds_tfms=tfms, size=224, bs=6).normalize(imagenet_stats)\ndata.show_batch(rows=5, figsize=(15, 15))","e54c598d":"learn = cnn_learner(data,\n                    models.resnet18,\n                    loss_func =CrossEntropyFlat(),\n                    opt_func=optim.Adam,\n                    metrics=accuracy,\n                    callback_fns=ShowGraph)\n\nlearn.model_dir = \"\/kaggle\/models\"","f012dc81":"defaults.device = torch.device('cuda') # makes sure the gpu is used\nlearn.fit_one_cycle(4) # Here we fit the head for 4 epochs","fbe045e4":"learn.unfreeze() # must be done before calling lr_find\nlearn.lr_find()\nlearn.recorder.plot()\n\n","9a13a148":"learn.fit_one_cycle(4, max_lr=slice(3e-6, 3e-5))\nlearn.save(\"plant-model\")","bb485380":"\npreds,y,losses = learn.get_preds(with_loss=True)\ninterp = ClassificationInterpretation(learn, preds, y, losses)","55071027":"interp.plot_confusion_matrix(figsize=(10,10))","67fc1d36":"for i in interp.top_losses(10).indices:\n    data.open(data.items[i]).show(title=data.items[i])\n","cdfa0c4f":"\nwindow_shape = (512, 512)\n#B = extract_patches_2d(A, window_shape)\n\n\n\ndata_dir = \"\/kaggle\/input\/plant-segmentation\/plant_segmentation\/dataset\/arabidopsis\"\ndata_dir = pathlib.Path(data_dir)\n\npath_img = data_dir\/'images'\n\npath_lbl = data_dir\/'masks'\n\nnew_train_images = pathlib.Path() \/ \"\/kaggle\/input\/images\"\nnew_train_masks = pathlib.Path() \/ \"\/kaggle\/input\/masks\"\n\nnew_train_images.mkdir(exist_ok=True)\nnew_train_masks.mkdir(exist_ok=True)\n\nfor i,j  in tqdm(zip(path_img.glob(\"*\"),path_lbl.glob(\"*\"))):\n    img = cv2.imread(str(i))\n    patches,_ = dense_patchify(img,(512,512))\n    \n    for idx, p in enumerate(patches):\n        cv2.imwrite(str( new_train_images \/ f\"{str(idx)}_{i.name}\"),p)\n\n    img = cv2.imread(str(j))\n    patches,_ = dense_patchify(img,(512,512))\n    \n    for idx, p in enumerate(patches):\n        cv2.imwrite(str( new_train_masks \/ f\"{str(idx)}_{j.name}\"),p)\n","b49ee22a":"tfms = get_transforms(do_flip=True, flip_vert=True, max_rotate=180.0, max_zoom=1.3, max_lighting=0.5, max_warp=0.1, p_affine=0.75, p_lighting=0.75) \n\nget_y_fn = lambda x: new_train_masks\/f'{x.name}'\ncodes = [\"Background\",\"hypocotyl\",\"non-hypocotyl\"]\n\ndata = (SegmentationItemList.from_folder(new_train_images)\n        .split_by_rand_pct()\n        .label_from_func(get_y_fn, classes=codes)\n        .transform(tfms,tfm_y=True)\n        .databunch(bs=4, path=data_dir)\n        .normalize(imagenet_stats))\n\ndata.show_batch(rows=2, figsize=(10, 10))\n\n# https:\/\/towardsdatascience.com\/introduction-to-image-augmentations-using-the-fastai-library-692dfaa2da42","354fd5aa":"wd=1e-2\nlearn = unet_learner(data, models.resnet18, metrics=dice, wd=wd)\nlearn.model_dir = \"\/kaggle\/models\"","6ce39457":"### Training\n\nOur dataset is already ready !\nPlease note that in real life it can take much more time to obtain a homogeneous dataset and it's usually the first source of error. We will speak about it later in the workshop.\n\nFastai provides a one-line command to:\n- download a pretrained model\n- set up loss and optimizer\n- launch training\n\nThe network we will use is resnet 18, which has 18 convolutions layers !\n\n![](https:\/\/www.researchgate.net\/profile\/Chiman_Kwan\/publication\/332303940\/figure\/fig1\/AS:745864146452480@1554839278101\/Architecture-of-ResNet-18-Figure-from-reference-18.jpg)\n\nBy default the cnn_learner is in a frozen mode, which mean only the last layer is learning.\n\n#### Loss and optimizer\nThe negative log likelihood loss or CrossEntropy. It is useful to train a classification problem with C classes.\n\n![](https:\/\/i.imgur.com\/hU252jE.jpg)\n\nThe optimizer is Adam. We will pass on optimizer today.\n\n#### Metrics\n\nAccuracy \n\n![](https:\/\/miro.medium.com\/max\/2868\/1*WGK_3mj_KBZh9yTiLXGh-A.png)","78fe9cfc":"### Data augmentation\n\nOne very usual way to improve generalization of a model and increase the size of the training set is data augmentation. \n\nData Augmentation consists on applying more or less realistic transformation on the image.\n\nWe will use transformations included in fastai package. \n\nYou can always use Data Augmentation for training so never, never hesitate to use plenty of them !\n\n<img src=\"https:\/\/camo.githubusercontent.com\/fd2405ab170ab4739c029d7251f5f7b4fac3b41c\/68747470733a2f2f686162726173746f726167652e6f72672f776562742f62642f6e652f72762f62646e6572763563746b75646d73617a6e687734637273646669772e6a706567\" \/>","2b1ac1b7":"### Segmentation\n\nThe same process can be repeated for segmentation purposes. The only difference is that instead of a unique label, we expect a matrix of label.\n\nWe will use a simple dataset with three classes: background, non-hypocothyl, hypocothyl\n\nWe will use U-Net\n\n![](https:\/\/www.researchgate.net\/profile\/Alan_Jackson9\/publication\/323597886\/figure\/fig2\/AS:601386504957959@1520393124691\/Convolutional-neural-network-CNN-architecture-based-on-UNET-Ronneberger-et-al.png)","f47b9f40":"### Data Augmentation\n\nLet's take a look on our data\nRemember, we haven't apply any transformation.","5f1ab410":"## Building a CNN classifier\n\n### Workflow\n3 steps:\n1. Find labeled data\n2. Train a classifier with transfer learning. We will use a CNN classifier that has been trained on imagenet\n3. Evaluation of performance\n\n### Presentation of imagenet\n\n- ImageNet is a very large scale image. Usually people share networks which have been trained on this dataset. We usually consider that the weights learned are a better initialization than randomly initialized weights. \n\n\nlink: http:\/\/www.image-net.org\/\n\n\n\n___ \n\n### Dataset\n\nAarhus University: Plant Seedlings dataset. A competition was held on Kaggle few years ago. It contains seedlings RGB images.\n\nhttps:\/\/vision.eng.au.dk\/plant-seedlings-dataset\/\n\n\n \n ___\n\n### Theory on transfer learning \n\n<center><img src=\"https:\/\/miro.medium.com\/max\/1920\/1*qfQ3hmHLwApXZBN-A85r8g.png\" alt=\"drawing\"\/><\/center>\n\n- Using a network trained on Imagenet improve convergence, generalization and performance with small dataset\n- Libraries usually provided the needed pretrained models\n","669f8615":"### Imports","580f6ca4":"Explications :\n- ImageDataBunch is a python object provided by fastai to facilitate the loading of data\n- data_dir is the path to our data\n- valid_pct is the percentage of images we hold for evaluation. Here we fix this percentage to 20%\n- ds_tfms is the transformation we apply to images before entering the network. Here it's empty\n- size is the target size before entering the network. Here we resize all images to 224\n- normalize: in transfer learning, the images need to be normalized with the mean and variance from the original dataset. imagenet_stats provides directly such metrics","63e80707":"# Plant phenotyping Training Workshop - Nanjing agricultural university - 2019\nEtienne David - UMT CAPTE - 20\/10\/2019\n\n___\n<center><img src=\"https:\/\/umt-capte.fr\/wp-content\/uploads\/2019\/08\/logo_capte_hd_fond_sombre.png\" alt=\"drawing\" style=\"width:400px;\"\/><\/center>\n\n<br>\n ___\n\nThis notebook is intended to be used as material for the Plant phenotyping Training Workshop.\n\n## Goals\n- Build your first classifier and understand all the steps of training\n- Understand transfer learning strategy\n- Work on building your first segmentation model !\n\n## Tools\n- We will used kaggle notebook as they provide 30h of free GPU utilization and an environment with all necessary python libraries\n- The tutorial is based on Pytorch and fastai, a wrapper built for learning Deep Learning, providing tons of utilities (www.fast.ai)\n\n## Data\n- We will use freely available data on Kaggle","1f1fbfdf":"#### Unfreezing\n\nAfter the head is trained, we can unfreeze the whole network for better accuracy.\n\nHere we use lr_find which help us to find the right learning rate","a087183d":"#### Interpretation\n\nFastai provides useful function to explore if our model did right or not","04e48796":"#### Loading data","2530565c":"#### Can you write the rest of the code ?"}}