{"cell_type":{"6593ee09":"code","a7cd0137":"code","c73e0601":"code","f7dafe3a":"code","4c667c76":"code","b5bc921d":"code","dae3bdd1":"code","a7496ea5":"code","8eeea5ce":"code","ce6912e4":"code","d3a14109":"code","1a1f6b44":"code","51ce56f8":"code","24e06268":"code","273bd2fa":"code","6fd72fe8":"code","251ead6e":"code","fbc735aa":"code","e05aabce":"code","8de19a19":"markdown","3ae05c5a":"markdown","10fbaa09":"markdown","480125ec":"markdown","df527128":"markdown","0109b939":"markdown","665cd808":"markdown","fdbd029b":"markdown","4ca76b3f":"markdown","ff40fc06":"markdown","5daf00aa":"markdown","f0928cfa":"markdown","f28e163f":"markdown","5d1ea854":"markdown","b71ab03e":"markdown","c8c6515e":"markdown","dfa613c9":"markdown","c50684fe":"markdown"},"source":{"6593ee09":"!pip install beautifulsoup4\nimport numpy as np\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport sys\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk import FreqDist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM, Embedding\nfrom keras.utils import np_utils, to_categorical\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport re\nimport seaborn as sns\nimport random","a7cd0137":"prefix = '\/kaggle\/input\/wikibooks-dataset\/'\n\ndf_en = pd.read_csv(prefix + 'english-wikibooks\/en-books-dataset.csv')","c73e0601":"df_en","f7dafe3a":"length = []\nfor i in range(10000):\n    soup = BeautifulSoup(df_en.iloc[i]['body_html'], 'html.parser')\n    length.append(len(soup.findAll('p')))","4c667c76":"print(length.index(max(length)))\nmax(length)","b5bc921d":"df_en.loc[length.index(max(length))]","dae3bdd1":"soup = BeautifulSoup(df_en.iloc\n                     [length.index(max(length))]['body_html']\n                     , 'html.parser').get_text(separator='\\n', strip=True).lower()","a7496ea5":"wordcloud = WordCloud().generate(soup)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","8eeea5ce":"# instantiate the tokenizer\ntokenizer = RegexpTokenizer(r'\\w+')\ntokens = tokenizer.tokenize(soup)\n\ntokens = [re.sub(r'[^a-z]', '', x) for x in tokens]\ntokens = [x for x in tokens if x.strip()]","ce6912e4":"# Figures inline and set visualization style\n%matplotlib inline\nsns.set()\n\nsw = stopwords.words('english')\n\nwords_ns = []\n\n# Add to words_ns all words that are in words but not in sw\nfor word in tokens:\n    if word not in sw:\n        words_ns.append(word)\n\n# Create freq dist and plot\nfreqdist1 = FreqDist(words_ns)\nfreqdist1.plot(20)","d3a14109":"length = 51\nlines = []\nfor i in range(length, len(tokens)):\n    # select sequence of tokens\n    seq = tokens[i-length:i]\n    # convert into a line\n    line = ' '.join(seq)\n    # store\n    lines.append(line)","1a1f6b44":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(lines)\nsequences = tokenizer.texts_to_sequences(lines)","51ce56f8":"# vocabulary size\nvocab_size = len(tokenizer.word_index) + 1","24e06268":"# separate into input and output\nsequences = np.array(sequences)\nX, y = sequences[:,:-1], sequences[:,-1]\ny = to_categorical(y, num_classes=vocab_size)\nseq_length = X.shape[1]","273bd2fa":"# define model\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 50, input_length=seq_length))\nmodel.add(LSTM(100, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(vocab_size, activation='softmax'))\nprint(model.summary())","6fd72fe8":"# compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit model\nmodel.fit(X, y, batch_size=64, epochs=100)","251ead6e":"def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n    result = list()\n    in_text = seed_text\n    # generate a fixed number of words\n    for _ in range(n_words):\n        # encode the text as integer\n        encoded = tokenizer.texts_to_sequences([in_text])[0]\n        # truncate sequences to a fixed length\n        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n        # predict probabilities for each word\n        yhat = model.predict_classes(encoded, verbose=0)\n        # map predicted word index to word\n        out_word = ''\n        for word, index in tokenizer.word_index.items():\n            if index == yhat:\n                out_word = word\n                break\n        # append to input\n        in_text += ' ' + out_word\n        result.append(out_word)\n    return ' '.join(result)","fbc735aa":"# select a seed text\nseed_text = lines[random.randint(0,len(lines))]\nprint(seed_text + '\\n')","e05aabce":"generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\nprint(generated)","8de19a19":"# A.9 Tokenizing the sentences\nThe individual lines are tokenized using Keras.","3ae05c5a":"# A.5 Wordcloud to view words\nWe've got quite a large emphasis in the book relating to files and users\/uses - as expected in a book about Linux! Perhaps generated sentences may be along the lines of a user guide.","10fbaa09":"# A.10 One-hot encoding the sequence\nThe output vector is one-hot encoded, which facilitates the training. However, meaning may be lost as it removes causal linkage with surrounding words. Embedding may be better here...\n\n","480125ec":"Getting a view on the data. There's quite a few Wikibooks to choose from.","df527128":"# Section A: Process the data","0109b939":"# A.4 Extracting the HTML contents\nBeautifulSoup extracts the text from the chosen book, and parses it. I set all characters to lower-case for consistency and use \\n as a separator, also applying a strip to remove whitespace.","665cd808":"# A.3 Choosing a book\n\nThe aim here is to choose a particularly long book. Ideally, each book could be scanned using BeautifulSoup for its length (using the 'p' marker as a delimiter to potentially avoid long strings of nonsense text padding out the list). However, due to the process being computationally intensive, I opt to just search the first 10000 books for the longest length book.\n\nAnother method could have been to iterate through the books and check the length with a threshold value (say $1000 \\leq l \\leq 10000$ where $l$ is the length of the book). This could still be computationally intensive, and the bounds are still arbitrarily designed. Yet another method could be to do a search where you check if the $i$th book's length is larger than the $i+1$th book, and if so, store that index else don't and move to the next. A recursive search like this would find the very longest book. Is this necessary? Perhaps not.\n\n","fdbd029b":"# B. Building the model\nHere the model is built. From this point I follow the following link:\nhttps:\/\/machinelearningmastery.com\/how-to-develop-a-word-level-neural-language-model-in-keras\/","4ca76b3f":"# Looking at the book itself\n\nI was just curious which book we're working with... I may even read it :)","ff40fc06":"Great, so in the search, the longest book is of length 1567 (that many paragraphs). We can work with that, it's likely to have ample vocabulary present!","5daf00aa":"# A.2 Loading the English wikibooks dataset","f0928cfa":"# A.1 Importing all necessary packages","f28e163f":"# B.2 Fitting the model\nThe model is fed the data. A batch size of 64 is used to improve accuracy - the smaller the batch, the less data is needed for the machine to learn (and thus improved generalisation). However it causes the slowdown of the model. Larger values speed up the model, but may not generalise well due to seeing too much data.\n\nNote that the accuracy grows slowly, so more epochs can improve accuracy. However, once again it slows the model down.\n\nI note here that my aim was to get a model to run, **not** to get the highest accuracy. Hyperparameters being tuned (and potentially choosing a different text) can improve the ultimate accuracy. This will also lower the loss which is high with this particular text\/model to start with!","5d1ea854":"# A.6 Tokenizing the data\nThis step translates the sentences into words (the tokens). I apply regex to remove numerical values and then empty strings. Maybe there is a better way to do this to avoid regex, but I don't know it! Comments welcome here :)\n\nI opt not to remove stop words, as these may be relevant when constructing meaningful sentences.","b71ab03e":"# A.7 Plotting the frequency distribution\n\nFor the sake of a frequency distribution to get a general sentiment behind this text, I remove stop words.\n\nAs observed from the wordcloud, we do have quite the emphasis on file\/edit operations. This is expected from a book on Linux. Perhaps choosing a non-technical book will diversify the vocab. But for now it works for me!","c8c6515e":"# A.8 Breaking up the word list into sequences\nI split the sentence up into lines of length 50. This quantity may be tweaked!\n\nNote that from this point onwards, I am using: https:\/\/machinelearningmastery.com\/how-to-develop-a-word-level-neural-language-model-in-keras\/","dfa613c9":"# B.1 Defining the model\nThe model is being built sequentially, so the starting point is defining a sequential model. An embedding layer is defined, based on the input sequences and vocab size. A two-layer LSTM (each with 100 memory cells) is defined with drop outs to prevent overfitting. A dense layer connecting the cells is added to interpret the features, with a softmax to normalize the probability distribution of outputs.","c50684fe":"# The aim here is to attempt word generation with an English Wikibook as the source.\n\n"}}