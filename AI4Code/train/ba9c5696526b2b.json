{"cell_type":{"89b17a62":"code","7edefbc7":"code","37965c78":"code","d67e9ac6":"code","ef870062":"code","77f2feca":"code","21e0b1b3":"code","4725b25e":"code","22ecc0f5":"code","4ceec1eb":"code","9ae94811":"code","27ee5be0":"code","dcbf3d87":"code","8731bd11":"code","c8dd2a15":"code","76f23992":"code","3bb250ea":"code","950575e7":"code","dc098bb1":"code","b0eab079":"code","fb14a585":"code","f3a1a248":"code","0f003a20":"code","41e6dee3":"code","76d52777":"code","3f7a66da":"markdown","3b593f5e":"markdown","18844a1a":"markdown","db261b2b":"markdown","5a2e1e1f":"markdown","3512cf51":"markdown","2c28a57d":"markdown","3eb5b52f":"markdown","b0a131a5":"markdown","b47c9042":"markdown","5201f57d":"markdown","5b8f9384":"markdown","1c4ca169":"markdown","5315ab2a":"markdown","11a710ca":"markdown","9a3758da":"markdown","8c3e96c5":"markdown","5fb4a85b":"markdown","e3ab911b":"markdown","f5b23e5c":"markdown","c93aedb0":"markdown","526b9b2f":"markdown","508189b7":"markdown","dd86baac":"markdown","625efa76":"markdown","1119ee57":"markdown","7a99df90":"markdown","5e0c3457":"markdown","34169616":"markdown","cf17341e":"markdown","f5e8d012":"markdown","26bcb7ee":"markdown","ad04ebc3":"markdown","bbf46b19":"markdown","12be4a24":"markdown","5b76131e":"markdown","66f64f84":"markdown"},"source":{"89b17a62":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport folium\nfrom IPython.display import HTML\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, accuracy_score","7edefbc7":"data = pd.read_csv('..\/input\/online-food-delivery-preferencesbangalore-region\/onlinedeliverydata.csv')","37965c78":"data.head()","d67e9ac6":"data.describe()","ef870062":"data.info()","77f2feca":"%matplotlib inline\n\n_ = [0, 6]\n_ = list(enumerate([list(data.columns)[i] for i in _], start=1))\n\nfig = plt.figure(figsize=[16,24])\nfor index, col_name in _:\n    ax = fig.add_subplot(3, 1, index)    \n    sns.countplot(x=col_name, data=data, hue='Output', palette='viridis')","21e0b1b3":"# Creating a class for grouping categorical variables into frequency tables\nclass CategoricalGrouping():\n    \n    def __init__(self, data, col1, col2):\n        self.data = data  # Pandas dataframe\n        self.col1 = col1  # Column with categories for analysis\n        self.col2 = col2  # Output variable\n        \n    @property\n    def table(self):\n        return self.data.groupby([self.col1, self.col2]).size().reset_index().pivot(\n            columns=self.col1, index=self.col2, values=0).fillna(0)\n\n\n# Defining a function to plot a nested pie chart\ndef nested_piechart(data, axis, wedge_width, pie_colors, chart_title):\n    \"\"\"This function takes the following arguments:\n        \n        data: a pandas dataframe of dimension greater than 2x1 (row x column)\n        axis: matplotlib.axes.Axes object for plotting\n        wedge_width: float, should be <=1\n        pie_colors: list, color codes in hex, should be >= maximum # of categories\n        chart_title: str, chart title to display\n        \n    \"\"\"\n\n    # Outer wedges\n    wedges_outer, texts_outer = axis.pie(data.iloc[1], radius=1, wedgeprops=dict(width=wedge_width, edgecolor='w'), \n           startangle=90, colors=pie_colors)\n\n    # Inner wedges\n    axis.pie(data.iloc[0], radius=(1-wedge_width), wedgeprops=dict(width=wedge_width, edgecolor='w', alpha=0.7), \n           startangle=90, colors=pie_colors)\n\n    axis.set(aspect=\"equal\", title=chart_title)\n\n    axis.legend(wedges_outer, list(data.columns),\n              title=chart_title,\n              loc=\"lower center\",\n              bbox_to_anchor=(0.85, -0.1, 0.5, 1))\n\n    # Defining properties for annotations\n    bbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\n    kw = dict(arrowprops=dict(arrowstyle=\"-\"),\n              bbox=bbox_props, zorder=0, va=\"center\")\n\n    y = np.sin(np.deg2rad(120))  # Converting degrees to radians\n    x = np.cos(np.deg2rad(120))  # Converting degrees to radians\n\n    horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]  # Depending on the radians of x, will give -1 or 1\n    connectionstyle = \"angle,angleA=0,angleB={}\".format(120)\n    kw[\"arrowprops\"].update({\"connectionstyle\": connectionstyle})  # adding connection style args to kw dict\n    axis.annotate(data.index[1], xy=(x, y), xytext=(1*np.sign(x), 1.2*y), \n                horizontalalignment=horizontalalignment, **kw)\n\n    y = np.sin(np.deg2rad(140)) - 0.60  # Converting degrees to radians\n    x = np.cos(np.deg2rad(140)) + 0.37  # Converting degrees to radians\n\n    horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]  # Depending on the radians of x, will give -1 or 1\n    connectionstyle = \"angle,angleA=0,angleB={}\".format(140)\n    kw[\"arrowprops\"].update({\"connectionstyle\": connectionstyle})  # adding connection style args to kw dict\n    axis.annotate(data.index[0], xy=(x, y), xytext=(0.01*np.sign(x), -2*y), \n                horizontalalignment=horizontalalignment, **kw)\n","4725b25e":"%matplotlib inline\n\nfig = plt.figure(figsize=[16,40])\n\nsize = 0.3\nc2 = 'Output'\nc_palette = ['#003f5c', '#58508d', '#bc5090', '#ff6361', '#ffa600']\n\ncat_var = ['Gender', 'Marital Status', 'Occupation', 'Monthly Income', 'Educational Qualifications', 'Medium (P1)', \n           'Medium (P2)', 'Meal(P1)', 'Meal(P2)', 'Perference(P1)', 'Perference(P2)']\n\nax_list = []\n\nfor ind, var in enumerate(cat_var):\n    ax_list.append(fig.add_subplot(6, 2, (ind+1)))\n    nested_piechart(CategoricalGrouping(data, var, c2).table, ax_list[ind], size, c_palette, var)\n    \n","22ecc0f5":"data.drop(['Medium (P1)', 'Medium (P2)', 'Meal(P1)', 'Meal(P2)'], axis=1, inplace=True)","4ceec1eb":"%matplotlib inline\n\nx = data.groupby(['latitude', 'longitude', 'Pin code']).size().reset_index()\nx.columns = ['latitude', 'longitude', 'pincode', 'frequency']\nx.sort_values(by=['frequency'], ascending=False, inplace=True)\n\nlatitude = 12.972442\nlongitude = 77.580643\ndelivery_map = folium.Map(location=[latitude, longitude], zoom_start=11)\n\nfor lat, lon, freq, pin in zip(x['latitude'], x['longitude'], x['frequency'], x['pincode']):\n    folium.CircleMarker([lat, lon], radius=freq, \n                        popup = ('Pincode: ' + str(pin) + '<br>' \n                                 '# of customers: ' + str(freq)\n                                ), \n                        tooltip='Click to expand',\n                        color='b', \n                        fill_color='red', \n                        fill=True, \n                        fill_opacity=0.6).add_to(delivery_map)","9ae94811":"delivery_map","27ee5be0":"data.drop(['latitude', 'longitude', 'Pin code'], axis=1, inplace=True)","dcbf3d87":"# Finding out all unique sets of categorical variables within the features\ndata_list = []\nfor r in data.iloc[:, np.r_[1, 4, 5, 9:47]].columns:\n    df_row = list(data[r].unique())\n    df_row = pd.Series(dict(enumerate(sorted(df_row))))\n    data_list.append(df_row)\n\ndf_1 = pd.DataFrame(data=data_list, index=data.iloc[:,np.r_[1, 4, 5, 9:47]].columns)\n\ndf_1['combined'] = df_1.apply(lambda x: ', '.join(x.dropna().values.tolist()), axis=1)\ndf_1.drop([0, 1, 2, 3, 4], inplace=True, axis=1)\ndf_1 = df_1.reset_index()\n\ndf_1['index'] = df_1.groupby(['combined'])['index'].transform(lambda x : ','.join(x))\ndf_1 = df_1.drop_duplicates()\ndf_1.columns = ['features', 'data_categories']\ndf_1.drop([23, 24], axis=0, inplace=True)\ndf_1 = df_1.reset_index()\ndf_1.drop('index', axis=1, inplace=True)\n\nx1 = list(enumerate([[0, 1], [2, 3, 1, 4, 0], \n                     [2, 4, 3, 1, 0], [4, 2, 3, 5, 1], \n                     [1, 2, 3, 4, 5], [4, 2, 3, 5, 1], \n                     [4, 3, 2, 1, 5], [1, 0]]\n                   ))\n\nfor i, l in x1:\n    df_1.at[i, 'data_categories'] = dict(zip(df_1.iloc[i, 1].split(', '), l))\n","8731bd11":"for i in df_1.index:\n    for j in df_1.iloc[i, 0].split(','):\n        data[j] = data[j].apply(lambda x: df_1.iloc[i, 1][x])","c8dd2a15":"%matplotlib inline\n\n\nfig = plt.figure(figsize=[32, 18])\nsns.heatmap(data.corr(method='spearman'), annot=False, mask=np.triu(data.corr(method='spearman')), cmap='Spectral', \n            linewidths=0.1, linecolor='white')","76f23992":"_ = pd.get_dummies(data.iloc[:, [2, 3, 7, 8, 29, 30, 37]], drop_first=True)\ndata.drop([data.columns[i] for i in [2, 3, 7, 8, 29, 30, 37, 47]], axis=1, inplace=True)\ndata = data.join(_)\ndata","3bb250ea":"X = data.drop('Output', axis=1) # input categorical features\ny = data['Output'] # target variable","950575e7":"sf = SelectKBest(chi2, k='all')\nsf_fit = sf.fit(X, y)\n\nchi2_scores = pd.DataFrame([sf_fit.scores_, X.columns], index=['feature_score', 'feature_name']).transpose()\nchi2_scores = chi2_scores.sort_values(by=['feature_score'], ascending=True).reset_index().drop('index', axis=1)","dc098bb1":"%matplotlib inline\n\nfig = plt.figure(figsize=[10, 20])\nplt.barh(chi2_scores['feature_name'], chi2_scores['feature_score'], align='center', alpha=0.5)\nplt.xlabel('Score')","b0eab079":"X = X[list(chi2_scores.iloc[-20:,1])]\nX","fb14a585":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)","f3a1a248":"log_model = LogisticRegression(max_iter=10000)\nlog_model.fit(X_train, y_train)\n\nlog_pred = log_model.predict(X_test)\n\nprint(classification_report(y_test, log_pred))\nprint(confusion_matrix(y_test, log_pred))","0f003a20":"rfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\n\nrfc_pred = rfc.predict(X_test)\n\nprint(classification_report(y_test, rfc_pred))\nprint(confusion_matrix(y_test, rfc_pred))","41e6dee3":"error_rate = []\n\nfor i in range(1,11):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append([accuracy_score(y_test, pred_i), precision_score(y_test, pred_i), \n                       recall_score(y_test, pred_i)])\n\nerror_rate = pd.DataFrame(error_rate, columns=['accuracy_score', 'precision_score', 'recall_score'])\nerror_rate.plot()","76d52777":"knn_model = KNeighborsClassifier(n_neighbors=7)\nknn_model.fit(X_train, y_train)\n\nknn_pred = knn_model.predict(X_test)\n\nprint(classification_report(y_test, knn_pred))\nprint(confusion_matrix(y_test, knn_pred))","3f7a66da":"### Categorical Variables","3b593f5e":"***","18844a1a":"### 1. Logistic Regression","db261b2b":"<font size=\"3\">We would drop the lat, lon and pincode variables from the data set. This is due to the fact that the data is comprised of responses from various individuals, and these set of variables provide no information other than where these individuals are situated in Bangalore.<\/font>","5a2e1e1f":"We get best performance at k = 7, and hence we'll train the model as such","3512cf51":"<font size=\"3\">Data Processing and Visualization Modules<\/font>\n\n1. Numpy\n2. Pandas\n3. Seaborn\n4. Matplotlib\n5. Folium\n\n<font size=\"3\">Machine Learning Modules<\/font>\n\n1. Train test split\n2. Logistic Regression\n3. Random Forest Classifier\n4. K-Neighbors Classifier\n5. SelectKBest\n6. chi2\n7. Confusion Matrix\n8. Classification Report\n9. Precision Score\n10. Recall Score\n11. Accuracy Score","2c28a57d":"# Data Visualizations","3eb5b52f":"***","b0a131a5":"### 3. K Nearest Neighbors","b47c9042":"# Conclusion","5201f57d":"### 3. Geospatial analysis","5b8f9384":"### 2. Random Forest","1c4ca169":"<font size=\"3\">Below is an array of pie charts created for each categorical variable, with the hue set to the Output variable<\/font>","5315ab2a":"<font size=\"3\">There are no extreme values in both the continuous variables (age and family size), hence we do not need to treat the data for outliers. We can also see that reordering is more prevalent in age groups less than 25, and family size of less than 4<\/font>","11a710ca":"Next Steps:\n\n1. To extract even more out of the data set, we could further explore other dimensionality reduction methods, such as multiple correspondence analysis which solves the issue of principal component analysis not being suited to categorical data\n\n2. We ignored the text data, i.e. reviews in this analysis, and analysing with NLP methods could also add to the analysis\n\n3. Could we tune the model further by adding\/subtracting variables?","9a3758da":"# Modeling","8c3e96c5":"<font size=\"3\">The variables Medium (P1), Medium (P2), Meal (P1) and Meal (P2) contribute little to no information which may help in predicting customer churn. In Medium (P1) for example, a customer who did not order again was using a food delivery app, which is redundant. Other variables also do not offer any more information, as the distribution of the classes seems the same.\n\nWe will now proceed to drop these variables<\/font>","5fb4a85b":"***","e3ab911b":"<font size=\"3\">As a starting point, we will select the top 20 features with the highest feature scores and see if we need to drop any variables later due to overfitting<\/font>","f5b23e5c":"***","c93aedb0":"# Setting Up","526b9b2f":"<font size=\"3\">We notice that not only are features from 'Ease and convenient' to 'Good Tracking system' correlated with our output variable, but also with each other. In fact, if we observe the diagonals, we can see which features are correlated with each other, which gives us an insight as to how we can compress the multiple dimensions in our variables. One of the popular dimensionality reduction methods, principal component analysis will not help in this case because the variables are of ordinal categorical nature rather than continuous. We will use other feature selection methods later in the data pre-processing stage<\/font>","508189b7":"<font size=\"3\">We have 388 rows of data, but only 77 unique coordinates, which indicates that either there are only 77 customers in this data set, or the lat-lon location is not exact and adheres to a certain area. Since this dataset is derived from a survey, we can assume the latter. This is also substantiated by the fact that there are 77 unique pincodes in the dataset. The above map visualization shows the density of customers across various pincodes of Bangalore. <\/font>","dd86baac":"Checking metrics for different values of k","625efa76":"<font size=\"3\">Out of all models created, Random Forest and K Nearest Neighbors give the best performance. K Nearest Neighbors has the added advantage of being relatively simpler to interpret, without any sacrifice in accuracy.\n\nAnother consideration is about the data itself. We see that most of the variables that have a material impact on customer churn relate to broad aspects of ease and convenience. In fact, the subset of features selected in the model could inform business decisions, regarding which aspects of the service could be focused on for minimizing churn, and what user segments should the marketing dollars be spent on.<\/font>","1119ee57":"<font size=\"3\">Before plotting the correlation matrix, we need to account for variables which are in the form of likert scales, and express them in the form of ordinal rank order scale. Then we will use Spearman's rank correlation to compute the correlation matrix for these variables in order to get a better understanding of our features<\/font>","7a99df90":"### 4. Correlation Matrix","5e0c3457":"Some of the pre-processing has been done in the visualization step itself, where we converted likert features to an ordinal scale. There are a few features remaining which are still categorical. We will convert these to dummy variables now","34169616":"<font size=\"3\">Even before moving on to the data visualization step, we can see that some form of dimensionality reduction will be required to avoid overfitting when we fit the model on this data. In the next steps we will see how we can do it.<\/font>","cf17341e":"![delivery-ecommerce-shipment-fast-free-box-edited.com.jpg](attachment:delivery-ecommerce-shipment-fast-free-box-edited.com.jpg)","f5e8d012":"<b><font size=\"10\"><center>Online Food Delivery Preferences<\/center><\/font><\/b>","26bcb7ee":"### Continuous Variables","ad04ebc3":"***","bbf46b19":"# Data Pre-processing","12be4a24":"<font size=\"4\">This database has been sourced from kaggle (link: https:\/\/www.kaggle.com\/benroshan\/online-food-delivery-preferencesbangalore-region). The goal is to see if we can predict customer churn. We will perform some visualizations before we go on to pre-process the data, after which we will implement classification models<\/font>","5b76131e":"<font size=\"3\">Structure of our data<\/font>","66f64f84":"<font size=\"3\">Importing data<\/font>"}}