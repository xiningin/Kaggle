{"cell_type":{"4cb587ff":"code","ae87a44b":"code","f8359c6f":"code","c19547a1":"code","b108ba70":"code","31171ae1":"code","e142a858":"code","ab337cd3":"code","963e9f1f":"code","7d18b88a":"code","0acb910f":"code","b334af28":"code","2cc9ba05":"code","0c8b3a9d":"code","d8fcb96d":"code","820782bc":"code","77b751d0":"code","3156e9e3":"code","dbb3868b":"code","47b4e1b2":"code","0543238e":"code","5d1b4374":"code","124804e0":"code","46580861":"code","46c3b2fb":"code","b246ce95":"code","ac0c2028":"code","c05f2e8c":"code","0d86e21a":"code","3b0ccf08":"code","193bef41":"code","dbbfc996":"code","91eb5a3b":"code","ee65544b":"code","d78e9b57":"code","db2ab376":"code","ebcf679d":"code","d1ab84b2":"code","795da162":"code","21571da9":"code","895c7965":"code","304d2253":"code","75103734":"code","87be12d8":"code","f0fe2a5e":"code","fa4bd8c3":"code","cb8f1f1f":"code","cd5c47c0":"code","a3a48b40":"code","5de384f5":"code","4733616f":"code","82d62c7c":"code","1174c095":"code","820204c4":"code","06c5b8ea":"code","9fef170a":"code","7488a659":"code","ea8fe103":"code","90dd0930":"code","ffaa787b":"code","e0ac866b":"code","fc9e26d6":"code","689dbfc7":"code","f7cf268e":"code","b85b26ef":"code","4e26e36b":"code","09e8e9ca":"code","ac48aab2":"code","99614f83":"code","2b1150d5":"code","b7fa580c":"code","aaa12330":"code","344bdb8d":"code","c64a4252":"code","d5a59d88":"code","52b11757":"code","12dceb06":"code","e0189cd0":"code","0327c750":"code","680e7c8c":"code","8487fa75":"code","d2f8be20":"code","fb98aed0":"code","47712fa9":"code","0ba2d0a2":"code","66584059":"code","8859edb5":"code","473f7107":"code","676d8100":"code","20248be0":"code","07d15475":"code","0d883fac":"code","b669d175":"code","1fd04f60":"code","9762cf9e":"code","6b26fd17":"code","f08b2102":"code","bedf1d12":"code","59dfb0fa":"code","0d48bd59":"code","9a453d39":"code","6e6c591a":"code","711ff607":"code","0aa8db95":"code","a03df45c":"code","39ac514d":"code","dad48d2c":"code","8f365291":"code","03a42464":"code","3540ab1b":"code","b0e62a2f":"code","6f491370":"code","d4220173":"code","38ae5cb0":"code","7f3a6b9a":"code","c7eec2a5":"code","623a5ee8":"code","874d00d2":"code","6a4dae6c":"code","d08cece5":"code","18b1a482":"code","9e18f41d":"code","5236fb4f":"code","6eec3897":"code","4043c69d":"code","98158cde":"code","849580f2":"code","882b63e2":"code","b50c6dff":"code","6e3c278a":"code","16569563":"code","5febdfaf":"code","1bd8bc8a":"code","3d2ba56b":"code","9c0d85f7":"code","2f579b13":"code","5bc9dfee":"code","0ed4641e":"code","c3192861":"code","09c72168":"code","4d17a078":"code","513b3e3d":"code","5a7f7159":"code","02dccee7":"code","005169ec":"code","664e73c6":"code","9deaf6f6":"code","19b6c666":"code","ed2b15b6":"code","6c49bb95":"code","b99c9f25":"code","ff83cb7a":"code","fddd8f72":"code","15d7751c":"code","cd7068ad":"code","acfdedb2":"code","ae6154d3":"code","73eb3ddc":"code","9308e3fe":"code","881aef1b":"code","c3d9bcee":"code","733ab5c4":"code","82010317":"code","a8244fef":"code","1e41cc4a":"code","4c31ac39":"code","d774ab8a":"code","6de6d4f5":"code","bc4c6dd2":"code","3cb3a815":"code","96f04d87":"code","69580e4f":"code","0d9929b1":"code","08c9597f":"code","2e0e2839":"code","48c701b5":"code","03392f46":"code","51b4fac4":"code","1a9950e0":"code","41a16bfb":"code","83112bc6":"code","f61517fe":"code","eb2f391d":"markdown","eba752a5":"markdown","a49300c3":"markdown","ed17b758":"markdown","ecacf348":"markdown","be52d260":"markdown","f34c8c0c":"markdown","32afded0":"markdown","3e4aceef":"markdown","25e3390c":"markdown","549abced":"markdown","6fa92f21":"markdown","13b039b3":"markdown","b99cf992":"markdown","e0cb3c68":"markdown","dd616b09":"markdown","f6c90395":"markdown","84e6c6ea":"markdown","2c1088b2":"markdown","52ab4d3d":"markdown","3a4a77a5":"markdown","783e0d7e":"markdown","c0949aad":"markdown","41444d9a":"markdown","cc8e35cf":"markdown","6e7a748d":"markdown","f5779bf5":"markdown","f12ed008":"markdown","0f84f3fe":"markdown","ca84d95d":"markdown","8e80cdc5":"markdown","9e5178ea":"markdown","70ccae3b":"markdown","518c0fc2":"markdown","c1855ea8":"markdown","d040ab32":"markdown","d0595de6":"markdown","31da622c":"markdown","8eac3342":"markdown","1433dc81":"markdown","12ff185b":"markdown","3feef8ff":"markdown","180f6cd3":"markdown","49111037":"markdown","5cf14ea2":"markdown","39a8cb11":"markdown","e6b8aea9":"markdown","c4d1ab03":"markdown","911c31f5":"markdown","70ef54cf":"markdown","fe783449":"markdown","3fc59bda":"markdown","7de6f9d6":"markdown","86b67819":"markdown","1c0d1ff0":"markdown","06fb074b":"markdown","3f171fb2":"markdown","cec80c7c":"markdown","ae51dc58":"markdown","5bf8afc5":"markdown","0e88a607":"markdown","4c9d82df":"markdown","df2445fa":"markdown","86098be4":"markdown","5af6ed78":"markdown","e54e1e3c":"markdown","5ab778d8":"markdown","de41456f":"markdown","bcf06676":"markdown","e58d11fa":"markdown","07219e6c":"markdown"},"source":{"4cb587ff":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","ae87a44b":"path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)#same  data as before","f8359c6f":"tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor]\nbs = 64\n\nil = ImageList.from_files(path, tfms=tfms)\nsd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='val'))\nll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor())\ndata = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=4)","c19547a1":"\nimg1 = PIL.Image.open(ll.train.x.items[0])\nimg1","b108ba70":"img2 = PIL.Image.open(ll.train.x.items[4000])\nimg2","31171ae1":"mixed_up = ll.train.x[0] * 0.3 + ll.train.x[4000] * 0.7 #so we are mixing two images together where we take 30 procent of the one image and 70 of the other image\nplt.imshow(mixed_up.permute(1,2,0)); ","e142a858":"\n# PyTorch has a log-gamma but not a gamma, so we'll create one\n\u0393 = lambda x: x.lgamma().exp()\n","ab337cd3":"\nfacts = [math.factorial(i) for i in range(7)]","963e9f1f":"plt.plot(range(7), facts, 'ro')\nplt.plot(torch.linspace(0,6), \u0393(torch.linspace(0,6)+1))\nplt.legend(['factorial','\u0393']);","7d18b88a":"\ntorch.linspace(0,0.9,10)","0acb910f":"\n_,axs = plt.subplots(1,2, figsize=(12,4))\nx = torch.linspace(0,1, 100)\nfor \u03b1,ax in zip([0.1,0.8], axs):\n    \u03b1 = tensor(\u03b1)\n#     y = (x.pow(\u03b1-1) * (1-x).pow(\u03b1-1)) \/ (gamma_func(\u03b1 ** 2) \/ gamma_func(\u03b1)) #so this is in programming 'sprog'\n    y = (x**(\u03b1-1) * (1-x)**(\u03b1-1)) \/ (\u0393(\u03b1)**2 \/ \u0393(2*\u03b1)) #and this is like if we did it in math \n    ax.plot(x,y)\n    ax.set_title(f\"\u03b1={\u03b1:.1}\")\n    #so we get to pick alfa and if it is high it is very likely we get a equarel mix (procentage of each image, combined in one)\n    #and if it is low unlikely, so more likely that one or the other will be more dominant.","b334af28":"#export\nclass NoneReduce():\n    def __init__(self, loss_func): \n        self.loss_func,self.old_red = loss_func,None\n        \n    def __enter__(self):\n        if hasattr(self.loss_func, 'reduction'):\n            self.old_red = getattr(self.loss_func, 'reduction')\n            setattr(self.loss_func, 'reduction', 'none')\n            return self.loss_func\n        else: return partial(self.loss_func, reduction='none')\n        \n    def __exit__(self, type, value, traceback):\n        if self.old_red is not None: setattr(self.loss_func, 'reduction', self.old_red)","2cc9ba05":"#export\nfrom torch.distributions.beta import Beta\n\ndef unsqueeze(input, dims):\n    for dim in listify(dims): input = torch.unsqueeze(input, dim)\n    return input\n\ndef reduce_loss(loss, reduction='mean'):\n    return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss #so takes the sum of all the losses in a minibtach and take the mean to it\n#note we ","0c8b3a9d":"#so mixup is gonna change our loss function. So we need to know what loss function to change \nclass MixUp(Callback): \n    _order = 90 #Runs after normalization and cuda\n    def __init__(self, \u03b1:float=0.4): self.distrib = Beta(tensor([\u03b1]), tensor([\u03b1]))\n    \n    def begin_fit(self): self.old_loss_func,self.run.loss_func = self.run.loss_func,self.loss_func #when you start fitting we find out what the old loss function on the learner was and tore it away\n    \n    def begin_batch(self):\n        if not self.in_train: return #Only mixup things during training\n        \u03bb = self.distrib.sample((self.yb.size(0),)).squeeze().to(self.xb.device)\n        \u03bb = torch.stack([\u03bb, 1-\u03bb], 1)\n        self.\u03bb = unsqueeze(\u03bb.max(1)[0], (1,2,3))\n        shuffle = torch.randperm(self.yb.size(0)).to(self.xb.device) #grap all the images and shuffle them \n        xb1,self.yb1 = self.xb[shuffle],self.yb[shuffle] #randomly picked one \n        self.run.xb = lin_comb(self.xb, xb1, self.\u03bb)#linear combination (lin_comb) of our actual images(xb) and some randomly picked images(xb1) in that imibatch\n        \n    def after_fit(self): self.run.loss_func = self.old_loss_func\n    \n    def loss_func(self, pred, yb):\n        if not self.in_train: return self.old_loss_func(pred, yb) #if it is in validation there is no mixup involed \n        with NoneReduce(self.old_loss_func) as loss_func: #and when we are training we will calulate the loss on to different set of images ...\n            loss1 = loss_func(pred, yb) # ... one is just the regular set from training data ...\n            loss2 = loss_func(pred, self.yb1)#... and the other one is randomly picked \n        loss = lin_comb(loss1, loss2, self.\u03bb) #and our loss is a linear combination of our the loss of our normaly batched loss and our randomly batched loss\n        return reduce_loss(loss, getattr(self.old_loss_func, 'reduction', 'mean'))","d8fcb96d":"nfs = [32,64,128,256,512]","820782bc":"\ndef get_learner(nfs, data, lr, layer, loss_func=F.cross_entropy,\n                cb_funcs=None, opt_func=optim.SGD, **kwargs):\n    model = get_cnn_model(data, nfs, layer, **kwargs)\n    init_cnn(model)\n    return Learner(model, data, loss_func, lr=lr, cb_funcs=cb_funcs, opt_func=opt_func)","77b751d0":"cbfs = [partial(AvgStatsCallback,accuracy),\n        CudaCallback, \n        ProgressCallback,\n        partial(BatchTransformXCallback, norm_imagenette),\n        MixUp]","3156e9e3":"\nlearn = get_learner(nfs, data, 0.4, conv_layer, cb_funcs=cbfs)","dbb3868b":"\nlearn.fit(1)","47b4e1b2":"#if there is a lot of nose because of wrong or no labels this can be realy good \nclass LabelSmoothingCrossEntropy(nn.Module):\n    def __init__(self, \u03b5:float=0.1, reduction='mean'):\n        super().__init__()\n        self.\u03b5,self.reduction = \u03b5,reduction\n    \n    def forward(self, output, target):\n        c = output.size()[-1]\n        log_preds = F.log_softmax(output, dim=-1)\n        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n        nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n        return lin_comb(loss\/c, nll, self.\u03b5)","0543238e":"cbfs = [partial(AvgStatsCallback,accuracy),\n        CudaCallback,\n        ProgressCallback,\n        partial(BatchTransformXCallback, norm_imagenette)]","5d1b4374":"learn = get_learner(nfs, data, 0.4, conv_layer, cb_funcs=cbfs, loss_func=LabelSmoothingCrossEntropy())","124804e0":"learn.fit(1)","46580861":"assert learn.loss_func.reduction == 'mean'","46c3b2fb":"\n%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","b246ce95":"\n# export \nimport apex.fp16_utils as fp16","ac0c2028":"bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)","c05f2e8c":"def bn_to_float(model):\n    if isinstance(model, bn_types): model.float()#batch norm goes to float \n    for child in model.children():  bn_to_float(child)\n    return model","0d86e21a":"def model_to_half(model):\n    model = model.half() #turns model in half \n    return bn_to_float(model)","3b0ccf08":"\nmodel = nn.Sequential(nn.Linear(10,30), nn.BatchNorm1d(30), nn.Linear(30,2)).cuda()\nmodel = model_to_half(model)","193bef41":"def check_weights(model):\n    for i,t in enumerate([torch.float16, torch.float32, torch.float16]):\n        assert model[i].weight.dtype == t\n        assert model[i].bias.dtype   == t","dbbfc996":"\ncheck_weights(model)","91eb5a3b":"model = nn.Sequential(nn.Linear(10,30), nn.BatchNorm1d(30), nn.Linear(30,2)).cuda()\nmodel = fp16.convert_network(model, torch.float16)\ncheck_weights(model)","ee65544b":"from torch.nn.utils import parameters_to_vector\n\ndef get_master(model, flat_master=False):\n    model_params = [param for param in model.parameters() if param.requires_grad]\n    if flat_master:\n        master_param = parameters_to_vector([param.data.float() for param in model_params])\n        master_param = torch.nn.Parameter(master_param, requires_grad=True)\n        if master_param.grad is None: master_param.grad = master_param.new(*master_param.size())\n        return model_params, [master_param]\n    else:\n        master_params = [param.clone().float().detach() for param in model_params]\n        for param in master_params: param.requires_grad_(True)\n        return model_params, master_params","d78e9b57":"model_p,master_p = get_master(model)\nmodel_p1,master_p1 = fp16.prep_param_lists(model)","db2ab376":"def same_lists(ps1, ps2):\n    assert len(ps1) == len(ps2)\n    for (p1,p2) in zip(ps1,ps2): \n        assert p1.requires_grad == p2.requires_grad\n        assert torch.allclose(p1.data.float(), p2.data.float())","ebcf679d":"same_lists(model_p,model_p1)\nsame_lists(model_p,master_p)\nsame_lists(master_p,master_p1)\nsame_lists(model_p1,master_p1)","d1ab84b2":"model1 = nn.Sequential(nn.Linear(10,30), nn.Linear(30,2)).cuda()\nmodel1 = fp16.convert_network(model1, torch.float16)","795da162":"model_p,master_p = get_master(model1, flat_master=True)\nmodel_p1,master_p1 = fp16.prep_param_lists(model1, flat_master=True)","21571da9":"same_lists(model_p,model_p1)\nsame_lists(master_p,master_p1)","895c7965":"assert len(master_p[0]) == 10*30 + 30 + 30*2 + 2\nassert len(master_p1[0]) == 10*30 + 30 + 30*2 + 2","304d2253":"def get_master(opt, flat_master=False):\n    model_params = [[param for param in pg if param.requires_grad] for pg in opt.param_groups]\n    if flat_master:\n        master_params = []\n        for pg in model_params:\n            mp = parameters_to_vector([param.data.float() for param in pg])\n            mp = torch.nn.Parameter(mp, requires_grad=True)\n            if mp.grad is None: mp.grad = mp.new(*mp.size())\n            master_params.append(mp)\n    else:\n        master_params = [[param.clone().float().detach() for param in pg] for pg in model_params]\n        for pg in master_params:\n            for param in pg: param.requires_grad_(True)\n    return model_params, master_params","75103734":"def to_master_grads(model_params, master_params, flat_master:bool=False)->None:\n    if flat_master:\n        if master_params[0].grad is None: master_params[0].grad = master_params[0].data.new(*master_params[0].data.size())\n        master_params[0].grad.data.copy_(parameters_to_vector([p.grad.data.float() for p in model_params]))\n    else:\n        for model, master in zip(model_params, master_params):\n            if model.grad is not None:\n                if master.grad is None: master.grad = master.data.new(*master.data.size())\n                master.grad.data.copy_(model.grad.data)\n            else: master.grad = None","87be12d8":"\nx = torch.randn(20,10).half().cuda()\nz = model(x)\nloss = F.cross_entropy(z, torch.randint(0, 2, (20,)).cuda())\nloss.backward()","f0fe2a5e":"to_master_grads(model_p, master_p)","fa4bd8c3":"def check_grads(m1, m2):\n    for p1,p2 in zip(m1,m2): \n        if p1.grad is None: assert p2.grad is None\n        else: assert torch.allclose(p1.grad.data, p2.grad.data)","cb8f1f1f":"check_grads(model_p, master_p)","cd5c47c0":"fp16.model_grads_to_master_grads(model_p, master_p)","a3a48b40":"check_grads(model_p, master_p)","5de384f5":"\nfrom torch._utils import _unflatten_dense_tensors\n\ndef to_model_params(model_params, master_params, flat_master:bool=False)->None:\n    if flat_master:\n        for model, master in zip(model_params, _unflatten_dense_tensors(master_params[0].data, model_params)):\n            model.data.copy_(master)\n    else:\n        for model, master in zip(model_params, master_params): model.data.copy_(master.data)","4733616f":"def get_master(opt, flat_master=False):\n    model_pgs = [[param for param in pg if param.requires_grad] for pg in opt.param_groups]\n    if flat_master:\n        master_pgs = []\n        for pg in model_pgs:\n            mp = parameters_to_vector([param.data.float() for param in pg])\n            mp = torch.nn.Parameter(mp, requires_grad=True)\n            if mp.grad is None: mp.grad = mp.new(*mp.size())\n            master_pgs.append([mp])\n    else:\n        master_pgs = [[param.clone().float().detach() for param in pg] for pg in model_pgs]\n        for pg in master_pgs:\n            for param in pg: param.requires_grad_(True)\n    return model_pgs, master_pgs","82d62c7c":"# export \ndef to_master_grads(model_pgs, master_pgs, flat_master:bool=False)->None:\n    for (model_params,master_params) in zip(model_pgs,master_pgs):\n        fp16.model_grads_to_master_grads(model_params, master_params, flat_master=flat_master)","1174c095":"\n# export \ndef to_model_params(model_pgs, master_pgs, flat_master:bool=False)->None:\n    for (model_params,master_params) in zip(model_pgs,master_pgs):\n        fp16.master_params_to_model_params(model_params, master_params, flat_master=flat_master)","820204c4":"class MixedPrecision(Callback):\n    _order = 99\n    def __init__(self, loss_scale=512, flat_master=False): #multiply by loss_scale and then divide by it to get the right scaleing \n        assert torch.backends.cudnn.enabled, \"Mixed precision training requires cudnn.\"\n        self.loss_scale,self.flat_master = loss_scale,flat_master\n\n    def begin_fit(self):\n        self.run.model = fp16.convert_network(self.model, dtype=torch.float16)\n        self.model_pgs, self.master_pgs = get_master(self.opt, self.flat_master)\n        #Changes the optimizer so that the optimization step is done in FP32.\n        self.run.opt.param_groups = self.master_pgs #Put those param groups inside our runner.\n        \n    def after_fit(self): self.model.float()\n\n    def begin_batch(self): self.run.xb = self.run.xb.half() #Put the inputs to half precision\n    def after_pred(self):  self.run.pred = self.run.pred.float() #Compute the loss in FP32\n    def after_loss(self):  self.run.loss *= self.loss_scale #Loss scaling to avoid gradient underflow\n\n    def after_backward(self):\n        #Copy the gradients to master and unscale\n        to_master_grads(self.model_pgs, self.master_pgs, self.flat_master)\n        for master_params in self.master_pgs:\n            for param in master_params:\n                if param.grad is not None: param.grad.div_(self.loss_scale)\n\n    def after_step(self):\n        #Zero the gradients of the model since the optimizer is disconnected.\n        self.model.zero_grad()\n        #Update the params from master to model.\n        to_model_params(self.model_pgs, self.master_pgs, self.flat_master)","06c5b8ea":"path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)","9fef170a":"\ntfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor]\nbs = 64\n\nil = ImageList.from_files(path, tfms=tfms)\nsd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='val'))\nll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor())\ndata = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=4)","7488a659":"nfs = [32,64,128,256,512]","ea8fe103":"\ndef get_learner(nfs, data, lr, layer, loss_func=F.cross_entropy,\n                cb_funcs=None, opt_func=adam_opt(), **kwargs):\n    model = get_cnn_model(data, nfs, layer, **kwargs)\n    init_cnn(model)\n    return Learner(model, data, loss_func, lr=lr, cb_funcs=cb_funcs, opt_func=opt_func)","90dd0930":"\ncbfs = [partial(AvgStatsCallback,accuracy),\n        ProgressCallback,\n        CudaCallback,\n        partial(BatchTransformXCallback, norm_imagenette)]","ffaa787b":"\nlearn = get_learner(nfs, data, 1e-2, conv_layer, cb_funcs=cbfs)","e0ac866b":"\nlearn.fit(1)","fc9e26d6":"cbfs = [partial(AvgStatsCallback,accuracy),\n        CudaCallback,\n        ProgressCallback,\n        partial(BatchTransformXCallback, norm_imagenette),\n        MixedPrecision]","689dbfc7":"\nlearn = get_learner(nfs, data, 1e-2, conv_layer, cb_funcs=cbfs)","f7cf268e":"learn.fit(1)","b85b26ef":"test_eq(next(learn.model.parameters()).type(), 'torch.cuda.FloatTensor')","4e26e36b":"# export \ndef test_overflow(x):\n    s = float(x.float().sum())\n    return (s == float('inf') or s == float('-inf') or s != s)","09e8e9ca":"x = torch.randn(512,1024).cuda()","ac48aab2":"test_overflow(x)","99614f83":"x[123,145] = float('inf')\ntest_overflow(x)","2b1150d5":"%timeit test_overflow(x)","b7fa580c":"\n%timeit torch.isnan(x).any().item()","aaa12330":"\n# export \ndef grad_overflow(param_groups):\n    for group in param_groups:\n        for p in group:\n            if p.grad is not None:\n                s = float(p.grad.data.float().sum())\n                if s == float('inf') or s == float('-inf') or s != s: return True\n    return False","344bdb8d":"# export \nclass MixedPrecision(Callback):\n    _order = 99\n    def __init__(self, loss_scale=512, flat_master=False, dynamic=True, max_loss_scale=2.**24, div_factor=2.,\n                 scale_wait=500):\n        assert torch.backends.cudnn.enabled, \"Mixed precision training requires cudnn.\"\n        self.flat_master,self.dynamic,self.max_loss_scale = flat_master,dynamic,max_loss_scale\n        self.div_factor,self.scale_wait = div_factor,scale_wait\n        self.loss_scale = max_loss_scale if dynamic else loss_scale\n\n    def begin_fit(self):\n        self.run.model = fp16.convert_network(self.model, dtype=torch.float16)\n        self.model_pgs, self.master_pgs = get_master(self.opt, self.flat_master)\n        #Changes the optimizer so that the optimization step is done in FP32.\n        self.run.opt.param_groups = self.master_pgs #Put those param groups inside our runner.\n        if self.dynamic: self.count = 0\n\n    def begin_batch(self): self.run.xb = self.run.xb.half() #Put the inputs to half precision\n    def after_pred(self):  self.run.pred = self.run.pred.float() #Compute the loss in FP32\n    def after_loss(self):  \n        if self.in_train: self.run.loss *= self.loss_scale #Loss scaling to avoid gradient underflow\n\n    def after_backward(self):\n        #First, check for an overflow\n        if self.dynamic and grad_overflow(self.model_pgs):\n            #Divide the loss scale by div_factor, zero the grad (after_step will be skipped)\n            self.loss_scale \/= self.div_factor\n            self.model.zero_grad()\n            return True #skip step and zero_grad\n        #Copy the gradients to master and unscale\n        to_master_grads(self.model_pgs, self.master_pgs, self.flat_master)\n        for master_params in self.master_pgs:\n            for param in master_params:\n                if param.grad is not None: param.grad.div_(self.loss_scale)\n        #Check if it's been long enough without overflow\n        if self.dynamic:\n            self.count += 1\n            if self.count == self.scale_wait:\n                self.count = 0\n                self.loss_scale *= self.div_factor\n\n    def after_step(self):\n        #Zero the gradients of the model since the optimizer is disconnected.\n        self.model.zero_grad()\n        #Update the params from master to model.\n        to_model_params(self.model_pgs, self.master_pgs, self.flat_master)","c64a4252":"cbfs = [partial(AvgStatsCallback,accuracy),\n        CudaCallback,\n        ProgressCallback,\n        partial(BatchTransformXCallback, norm_imagenette),\n        MixedPrecision]","d5a59d88":"learn = get_learner(nfs, data, 1e-2, conv_layer, cb_funcs=cbfs)","52b11757":"learn.fit(1)","12dceb06":"learn.cbs[-1].loss_scale","e0189cd0":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline\n","0327c750":"path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)","680e7c8c":"\nsize = 128\ntfms = [make_rgb, RandomResizedCrop(size, scale=(0.35,1)), np_to_float, PilRandomFlip()] #a minimum scale of 0.35 to work \"virker til\" good. And we are not gonna use an other argutation then fliping \n\nbs = 64\n\nil = ImageList.from_files(path, tfms=tfms)\nsd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='val'))\nll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor())\n\nll.valid.x.tfms = [make_rgb, CenterCrop(size), np_to_float]\n\ndata = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=8)","8487fa75":"\n#export\ndef noop(x): return x\n\nclass Flatten(nn.Module):\n    def forward(self, x): return x.view(x.size(0), -1)\n\ndef conv(ni, nf, ks=3, stride=1, bias=False):\n    return nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks\/\/2, bias=bias)","d2f8be20":"#export\nact_fn = nn.ReLU(inplace=True) #and for our activation function we are just gonna use Relu for now \n\ndef init_cnn(m):\n    if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)\n    if isinstance(m, (nn.Conv2d,nn.Linear)): nn.init.kaiming_normal_(m.weight)\n    for l in m.children(): init_cnn(l)\n\ndef conv_layer(ni, nf, ks=3, stride=1, zero_bn=False, act=True):\n    bn = nn.BatchNorm2d(nf)\n    nn.init.constant_(bn.weight, 0. if zero_bn else 1.) #we initizise the weight to sometimes have weight of 1 and sometimes to have weights of 1 \n    layers = [conv(ni, nf, ks, stride=stride), bn] #the layers start with a conv with some stide followed by a batch norm(bn)\n    if act: layers.append(act_fn) #and optionaly we can add an activation function (act_fn)\n    return nn.Sequential(*layers) #a conv layer is sequential containing a bunch of layers ","fb98aed0":"#export\nclass ResBlock(nn.Module):\n    def __init__(self, expansion, ni, nh, stride=1): #so expansion is 1 for resnet 18 or 34 and and 4 if it is bigger \n        super().__init__()\n        nf,ni = nh*expansion,ni*expansion\n        layers  = [conv_layer(ni, nh, 3, stride=stride),\n                   conv_layer(nh, nf, 3, zero_bn=True, act=False) #if expansion is 1 we just we just add one exstra conv layer but...\n        ] if expansion == 1 else [ #...if expansion is bigger then 1 we add two exxtra conv layers \n                   conv_layer(ni, nh, 1),\n                   conv_layer(nh, nh, 3, stride=stride),\n                   conv_layer(nh, nf, 1, zero_bn=True, act=False)\n        ]\n        self.convs = nn.Sequential(*layers)\n        self.idconv = noop if ni==nf else conv_layer(ni, nf, 1, act=False) #Conv(1x1) if the number of inputs is different to the numbers of filters we add a conv layer\n        self.pool = noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True) #AvgPool if the stride is other then one we add an Averge pooling \n\n    def forward(self, x): return act_fn(self.convs(x) + self.idconv(self.pool(x)))","47712fa9":"#export\nclass XResNet(nn.Sequential):\n    @classmethod\n    def create(cls, expansion, layers, c_in=3, c_out=1000):\n        nfs = [c_in, (c_in+1)*8, 64, 64] #start wift setting up how many filters there is gonna be for the first 3 layer. and the first 3 layers will start with 3 channels(c_in)\n        #and the output to the first layer will be the c_in plus 1 times 8, and the reason for this is that this is 32 to the secound layer which is the same as the aticle \"the bag of thrich uses\".\n        #the reason we multiply by 8 is because invidia grafic card like everything to be a multitude of 8 \n        stem = [conv_layer(nfs[i], nfs[i+1], stride=2 if i==0 else 1) #the stem is the very start of a of a CNN an it is just the 3 conv layers (nfs[i], nfs[i+1], stride=2)\n            for i in range(3)]\n\n        nfs = [64\/\/expansion,64,128,256,512]\n        res_layers = [cls._make_layer(expansion, nfs[i], nfs[i+1], #now we are gonna create a bunch of resbloks. #Gonna create a resnet blok for every res layer \n                                      n_blocks=l, stride=1 if i==0 else 2)\n                  for i,l in enumerate(layers)]\n        res = cls(\n            *stem,\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            *res_layers,\n            nn.AdaptiveAvgPool2d(1), Flatten(),\n            nn.Linear(nfs[-1]*expansion, c_out),\n        )\n        init_cnn(res)\n        return res\n\n    @staticmethod\n    def _make_layer(expansion, ni, nf, n_blocks, stride):\n        return nn.Sequential(\n            *[ResBlock(expansion, ni if i==0 else nf, nf, stride if i==0 else 1) #createing the resblok \n              for i in range(n_blocks)])","0ba2d0a2":"\n#create all of our resnet \ndef xresnet18 (**kwargs): return XResNet.create(1, [2, 2,  2, 2], **kwargs) #exsampel here is the [2, 2,  2, 2] how many bloks we want in each layer and \n#expansion is 1 \ndef xresnet34 (**kwargs): return XResNet.create(1, [3, 4,  6, 3], **kwargs)\ndef xresnet50 (**kwargs): return XResNet.create(4, [3, 4,  6, 3], **kwargs)\ndef xresnet101(**kwargs): return XResNet.create(4, [3, 4, 23, 3], **kwargs)\ndef xresnet152(**kwargs): return XResNet.create(4, [3, 8, 36, 3], **kwargs)","66584059":"cbfs = [partial(AvgStatsCallback,accuracy), ProgressCallback, CudaCallback,\n        partial(BatchTransformXCallback, norm_imagenette),\n#         partial(MixUp, alpha=0.2)\n       ]","8859edb5":"loss_func = LabelSmoothingCrossEntropy()\narch = partial(xresnet18, c_out=10)\nopt_func = adam_opt(mom=0.9, mom_sqr=0.99, eps=1e-6, wd=1e-2)","473f7107":"#export\ndef get_batch(dl, learn):\n    learn.xb,learn.yb = next(iter(dl))\n    learn.do_begin_fit(0)\n    learn('begin_batch')\n    learn('after_fit')\n    return learn.xb,learn.yb","676d8100":"# export\ndef model_summary(model, data, find_all=False, print_mod=False):\n    xb,yb = get_batch(data.valid_dl, learn)\n    mods = find_modules(model, is_lin_layer) if find_all else model.children()\n    f = lambda hook,mod,inp,out: print(f\"====\\n{mod}\\n\" if print_mod else \"\", out.shape)\n    with Hooks(mods, f) as hooks: learn.model(xb)","20248be0":"learn = Learner(arch(), data, loss_func, lr=1, cb_funcs=cbfs, opt_func=opt_func)","07d15475":"learn.model = learn.model.cuda()\nmodel_summary(learn.model, data, print_mod=False)","0d883fac":"arch = partial(xresnet34, c_out=10)","b669d175":"\nlearn = Learner(arch(), data, loss_func, lr=1, cb_funcs=cbfs, opt_func=opt_func)","1fd04f60":"\nlearn.fit(1, cbs=[LR_Find(), Recorder()])","9762cf9e":"learn.recorder.plot(3)","6b26fd17":"#export\ndef create_phases(phases):\n    phases = listify(phases)\n    return phases + [1-sum(phases)]","f08b2102":"print(create_phases(0.3))\nprint(create_phases([0.3,0.2]))","bedf1d12":"\nlr = 1e-2\npct_start = 0.5\nphases = create_phases(pct_start)\nsched_lr  = combine_scheds(phases, cos_1cycle_anneal(lr\/10., lr, lr\/1e5))\nsched_mom = combine_scheds(phases, cos_1cycle_anneal(0.95, 0.85, 0.95))","59dfb0fa":"cbsched = [\n    ParamScheduler('lr', sched_lr),\n    ParamScheduler('mom', sched_mom)]","0d48bd59":"learn = Learner(arch(), data, loss_func, lr=lr, cb_funcs=cbfs, opt_func=opt_func)","9a453d39":"learn.fit(5, cbs=cbsched)","6e6c591a":"def cnn_learner(arch, data, loss_func, opt_func, c_in=None, c_out=None,\n                lr=1e-2, cuda=True, norm=None, progress=True, mixup=0, xtra_cb=None, **kwargs):\n    cbfs = [partial(AvgStatsCallback,accuracy)]+listify(xtra_cb)\n    if progress: cbfs.append(ProgressCallback)\n    if cuda:     cbfs.append(CudaCallback)\n    if norm:     cbfs.append(partial(BatchTransformXCallback, norm))\n    if mixup:    cbfs.append(partial(MixUp, mixup))\n    arch_args = {}\n    if not c_in : c_in  = data.c_in\n    if not c_out: c_out = data.c_out\n    if c_in:  arch_args['c_in' ]=c_in\n    if c_out: arch_args['c_out']=c_out\n    return Learner(arch(**arch_args), data, loss_func, opt_func=opt_func, lr=lr, cb_funcs=cbfs, **kwargs)","711ff607":"learn = cnn_learner(xresnet34, data, loss_func, opt_func, norm=norm_imagenette)","0aa8db95":"learn.fit(5, cbsched)","a03df45c":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","39ac514d":"path = datasets.untar_data(datasets.URLs.IMAGEWOOF_160) ","dad48d2c":"\nsize = 128\nbs = 64\n\n#data block API\ntfms = [make_rgb, RandomResizedCrop(size, scale=(0.35,1)), np_to_float, PilRandomFlip()]\nval_tfms = [make_rgb, CenterCrop(size), np_to_float]\nil = ImageList.from_files(path, tfms=tfms)\nsd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='val'))\nll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor())\nll.valid.x.tfms = val_tfms\ndata = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=8)","8f365291":"len(il)","03a42464":"loss_func = LabelSmoothingCrossEntropy()\nopt_func = adam_opt(mom=0.9, mom_sqr=0.99, eps=1e-6, wd=1e-2)","3540ab1b":"learn = cnn_learner(xresnet18, data, loss_func, opt_func, norm=norm_imagenette)","b0e62a2f":"\ndef sched_1cycle(lr, pct_start=0.3, mom_start=0.95, mom_mid=0.85, mom_end=0.95): #schedular one cycle \n    phases = create_phases(pct_start) #creae our ohases \n    sched_lr  = combine_scheds(phases, cos_1cycle_anneal(lr\/10., lr, lr\/1e5)) #create learning rate \n    sched_mom = combine_scheds(phases, cos_1cycle_anneal(mom_start, mom_mid, mom_end)) #cetate momentum \n    return [ParamScheduler('lr', sched_lr),\n            ParamScheduler('mom', sched_mom)]","6f491370":"lr = 3e-3\npct_start = 0.5\ncbsched = sched_1cycle(lr, pct_start)","d4220173":"\nlearn.fit(40, cbsched) #we are gonna use this model to do transfer learning on the data below so we need to save the model ","38ae5cb0":"st = learn.model.state_dict() #so when we save a model we grap the state dictornary ","7f3a6b9a":"\ntype(st) #so it created a dictornary ","c7eec2a5":"\n', '.join(st.keys()) #where the keys is just the layers ","623a5ee8":"st['10.bias'] #so we can look up fx 10.bias from above and it just retrns the weights ","874d00d2":"mdl_path = path\/'models' #create somewhere to...\nmdl_path.mkdir(exist_ok=True)#... save our model ","6a4dae6c":"torch.save(st, mdl_path\/'iw5') #and torch.save will save that dictornary ","d08cece5":"pets = datasets.untar_data(datasets.URLs.PETS)","18b1a482":"pets.ls()","9e18f41d":"pets_path = pets\/'images'","5236fb4f":"il = ImageList.from_files(pets_path, tfms=tfms)","6eec3897":"il","4043c69d":"#so since there isnt a validation set we are gonna create a random splitter\ndef random_splitter(fn, p_valid): return random.random() < p_valid","98158cde":"random.seed(42)","849580f2":"sd = SplitData.split_by_func(il, partial(random_splitter, p_valid=0.1)) #pass the random splitter to split_by_func","882b63e2":"\nsd#and we are done the validationset is created ","b50c6dff":"n = il.items[0].name; n# so lets grap one file name ","6e3c278a":"re.findall(r'^(.*)_\\d+.jpg$', n)[0] #and let just get the dogs name ","16569563":"\ndef pet_labeler(fn): return re.findall(r'^(.*)_\\d+.jpg$', fn.name)[0] #find all the dogs and cats names ","5febdfaf":"proc = CategoryProcessor()","1bd8bc8a":"\nll = label_by_func(sd, pet_labeler, proc_y=proc)#and label them all","3d2ba56b":"\n', '.join(proc.vocab) #so this is how it looks like ","9c0d85f7":"ll.valid.x.tfms = val_tfms","2f579b13":"c_out = len(proc.vocab)","5bc9dfee":"data = ll.to_databunch(bs, c_in=3, c_out=c_out, num_workers=8)","0ed4641e":"learn = cnn_learner(xresnet18, data, loss_func, opt_func, norm=norm_imagenette)","c3192861":"\nlearn.fit(5, cbsched) #and now we can train without the transfer and it dose not look good so lets try with transfer learning ","09c72168":"learn = cnn_learner(xresnet18, data, loss_func, opt_func, c_out=10, norm=norm_imagenette) #data is the pets databuch but lets tell it to have 10 channels (c_out) output so\n#so 10 activations at the end becouse the data from the model from the transfer model only have 10 dogs labels so we are gonna use that ","4d17a078":"\nst = torch.load(mdl_path\/'iw5')#grap our state dictornary we saved above ","513b3e3d":"\nm = learn.model","5a7f7159":"m.load_state_dict(st) #and we load it into our model ","02dccee7":"cut = next(i for i,o in enumerate(m.children()) if isinstance(o,nn.AdaptiveAvgPool2d))#so we look through all the children of the model and we try to find the adaptive averge pooling layer \nm_cut = m[:cut] #so lets create a new model that take all up to the adaptive averge pooling layer. so this is the body of the model ","005169ec":"\nxb,yb = get_batch(data.valid_dl, learn)","664e73c6":"pred = m_cut(xb)#so now we give the body a new head ","9deaf6f6":"pred.shape #so we need how many outputs there are from the m_cut as the input to the new model, so we print it and it is 512 ","19b6c666":"\nni = pred.shape[1]","ed2b15b6":"class AdaptiveConcatPool2d(nn.Module):\n    def __init__(self, sz=1):\n        super().__init__()\n        self.output_size = sz\n        self.ap = nn.AdaptiveAvgPool2d(sz) #do averge pool\n        self.mp = nn.AdaptiveMaxPool2d(sz)#do max pool\n    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1) #and catilate them together ","6c49bb95":"\nnh = 40\n\nm_new = nn.Sequential( #so our need model..\n    m_cut, AdaptiveConcatPool2d(), Flatten(), #.. contain the whole body with the adaptive pooling, and flatten and linear \n    nn.Linear(ni*2, data.c_out)) #our linear layer needs twice as many inout since we got both averge and max pooling ","b99c9f25":"learn.model = m_new #lets replace the old model with the new model we just created ","ff83cb7a":"learn.fit(5, cbsched) #and look at that it is much better then before ","fddd8f72":"#now we just refactor the code above into a function \ndef adapt_model(learn, data):\n    cut = next(i for i,o in enumerate(learn.model.children())\n               if isinstance(o,nn.AdaptiveAvgPool2d))\n    m_cut = learn.model[:cut]\n    xb,yb = get_batch(data.valid_dl, learn)\n    pred = m_cut(xb)\n    ni = pred.shape[1]\n    m_new = nn.Sequential(\n        m_cut, AdaptiveConcatPool2d(), Flatten(),\n        nn.Linear(ni*2, data.c_out))\n    learn.model = m_new","15d7751c":"\nlearn = cnn_learner(xresnet18, data, loss_func, opt_func, c_out=10, norm=norm_imagenette)\nlearn.model.load_state_dict(torch.load(mdl_path\/'iw5'))","cd7068ad":"\nadapt_model(learn, data)","acfdedb2":"#so lets just train the head so we take all the paramters in the body (m_cut) and freeze them like so \nfor p in learn.model[0].parameters(): p.requires_grad_(False)","ae6154d3":"#so now are training just the head and we get 54 which is ifne \nlearn.fit(3, sched_1cycle(1e-2, 0.5))","73eb3ddc":"#so we unfreeze the whole body so we get the whole model for training \nfor p in learn.model[0].parameters(): p.requires_grad_(True)","9308e3fe":"#so we train again but now we hit a issue since we get the same acc. as the head trainig. note when something wried is happing it is almost surtant it is bacthnorm. and it is true here to \n#so what happened is that our frozen part of our model #so the inside the head model had a different avg and std then the body so everything just tried to catch up when we unfroze it \nlearn.fit(5, cbsched, reset_opt=True)","881aef1b":"learn = cnn_learner(xresnet18, data, loss_func, opt_func, c_out=10, norm=norm_imagenette)\nlearn.model.load_state_dict(torch.load(mdl_path\/'iw5'))\nadapt_model(learn, data)","c3d9bcee":"\ndef apply_mod(m, f): #apply any function you pass to it recursively to all the children of the module #note pytorch has its own called model.apply(see below when used)\n    f(m)\n    for l in m.children(): apply_mod(l, f)\n\ndef set_grad(m, b): #set gradient \n    if isinstance(m, (nn.Linear,nn.BatchNorm2d)): return #if it s a linear layer or batch norm layer at the middel return, so dont change the gradient otherwise\n    if hasattr(m, 'weight'):#if it got weights ...\n        for p in m.parameters(): p.requires_grad_(b) #...set required gradient to whatever you asked for and we set it to false below","733ab5c4":"apply_mod(learn.model, partial(set_grad, b=False)) #so we freeze just the non-batchnorm layers and the last layer ","82010317":"learn.fit(3, sched_1cycle(1e-2, 0.5))","a8244fef":"apply_mod(learn.model, partial(set_grad, b=True)) #unfreeze","1e41cc4a":"learn.fit(5, cbsched, reset_opt=True) #and now we see a mush better result again ","4c31ac39":"#same thing as the function apply_mod\nlearn.model.apply(partial(set_grad, b=False));","d774ab8a":"learn = cnn_learner(xresnet18, data, loss_func, opt_func, c_out=10, norm=norm_imagenette)","6de6d4f5":"learn.model.load_state_dict(torch.load(mdl_path\/'iw5'))\nadapt_model(learn, data)","bc4c6dd2":"def bn_splitter(m):\n    def _bn_splitter(l, g1, g2):\n        if isinstance(l, nn.BatchNorm2d): g2 += l.parameters() #which will recursely look for for batchnorm layers and put htem intor the seound group(g2)\n        elif hasattr(l, 'weight'): g1 += l.parameters() #or anything else with a weight goes into the first group \n        for ll in l.children(): _bn_splitter(ll, g1, g2) #and do it recursely \n        \n    g1,g2 = [],[] #create two emty arrays for the two groups of paramters \n    _bn_splitter(m[0], g1, g2) #and its gonna pass the body to _bn_splitter...\n    \n    g2 += m[1:].parameters() #and also everyting in the secound group will add everything after the head \n    return g1,g2","3cb3a815":"#set veribles for the 2 groups we split our paramters up into\na,b = bn_splitter(learn.model)","96f04d87":"test_eq(len(a)+len(b), len(list(m.parameters()))) #check that the total lenght of a+b is the same as all the paramters in the total model ","69580e4f":"Learner.ALL_CBS ","0d9929b1":"#export\nfrom types import SimpleNamespace\ncb_types = SimpleNamespace(**{o:o for o in Learner.ALL_CBS})","08c9597f":"\ncb_types.after_backward","2e0e2839":"class DebugCallback(Callback): #for debuging\n    _order = 999\n    def __init__(self, cb_name, f=None): self.cb_name,self.f = cb_name,f\n    def __call__(self, cb_name): #overwrite dounder call itself \n        if cb_name==self.cb_name:\n            if self.f: self.f(self.run)\n            else:      set_trace()","48c701b5":"def sched_1cycle(lrs, pct_start=0.3, mom_start=0.95, mom_mid=0.85, mom_end=0.95):\n    phases = create_phases(pct_start)\n    sched_lr  = [combine_scheds(phases, cos_1cycle_anneal(lr\/10., lr, lr\/1e5))\n                 for lr in lrs]\n    sched_mom = combine_scheds(phases, cos_1cycle_anneal(mom_start, mom_mid, mom_end))\n    return [ParamScheduler('lr', sched_lr),\n            ParamScheduler('mom', sched_mom)]","03392f46":"disc_lr_sched = sched_1cycle([0,3e-2], 0.5) #so no learning rate for the body (0) but a learning rate of 3e-2 for the head and the batch norm","51b4fac4":"learn = cnn_learner(xresnet18, data, loss_func, opt_func,\n                    c_out=10, norm=norm_imagenette, splitter=bn_splitter)\n\nlearn.model.load_state_dict(torch.load(mdl_path\/'iw5'))\nadapt_model(learn, data)","1a9950e0":"def _print_det(o): \n    print (len(o.opt.param_groups), o.opt.hypers)\n    raise CancelTrainException()\n\nlearn.fit(1, disc_lr_sched + [DebugCallback(cb_types.after_batch, _print_det)])","41a16bfb":"learn.fit(3, disc_lr_sched)","83112bc6":"disc_lr_sched = sched_1cycle([1e-3,1e-2], 0.3)","f61517fe":"\nlearn.fit(5, disc_lr_sched)","eb2f391d":"## Training in mixed precision","eba752a5":"\n# Mixup \/ Label smoothing","a49300c3":"## Discriminative LR and param groups","ed17b758":"\nWith a low \u03b1, we pick values close to 0. and 1. with a high probability, and the values in the middle all have the same kind of probability. With a greater \u03b1, 0. and 1. get a lower probability .\n\nWhile the approach above works very well, it's not the fastest way we can do this. The main point that slows down this process is wanting two different batches at every iteration (which means loading twice the amount of images and applying to them the other data augmentation function). To avoid this slow down, we can be a little smarter and mixup a batch with a shuffled version of itself (this way the images mixed up are still different). This was a trick suggested in the MixUp paper.\n\nThen pytorch was very careful to avoid one-hot encoding targets when it could, so it seems a bit of a drag to undo this. Fortunately for us, if the loss is a classic cross-entropy, we have\n\n    loss(output, new_target) = t * loss(output, target1) + (1-t) * loss(output, target2)\nso we won't one-hot encode anything and just compute those two losses then do the linear combination.\n\nUsing the same parameter t for the whole batch also seemed a bit inefficient. In our experiments, we noticed that the model can train faster if we draw a different t for every image in the batch (both options get to the same result in terms of accuracy, it's just that one arrives there more slowly). The last trick we have to apply with this is that there can be some duplicates with this strategy: let's say or shuffle say to mix image0 with image1 then image1 with image0, and that we draw t=0.1 for the first, and t=0.9 for the second. Then\n\n    image0 * 0.1 + shuffle0 * (1-0.1) = image0 * 0.1 + image1 * 0.9\n    image1 * 0.9 + shuffle1 * (1-0.9) = image1 * 0.9 + image0 * 0.1\nwill be the same. Of course, we have to be a bit unlucky but in practice, we saw there was a drop in accuracy by using this without removing those near-duplicates. To avoid them, the tricks is to replace the vector of parameters we drew by\n\n    t = max(t, 1-t)\nThe beta distribution with the two parameters equal is symmetric in any case, and this way we insure that the biggest coefficient is always near the first image (the non-shuffled batch).\n\nIn Mixup we have handle loss functions that have an attribute reduction (like nn.CrossEntropy()). To deal with the reduction=None with various types of loss function without modifying the actual loss function outside of the scope we need to perform those operations with no reduction, we create a context manager:","ecacf348":"\nThe corresponding function in the Apex utils is model_grads_to_master_grads.","be52d260":"So we can use it in the following function that checks for gradient overflow:","f34c8c0c":"-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","32afded0":"## Train","3e4aceef":"And we can check our loss function reduction attribute hasn't changed outside of the training loop:","25e3390c":"In Apex, the function that does this for us is convert_network. We can use it to put the model in FP16 or back to FP32.","549abced":"\nThe corresponding function in Apex is master_params_to_model_params.\n\n## But we need to handle param groups\nThe thing is that we don't always want all the parameters of our model in the same parameter group, because we might:\n\n* want to do transfer learning and freeze some layers\n* apply discriminative learning rates\n* don't apply weight decay to some layers (like BatchNorm) or the bias terms\n\nSo we actually need a function that splits the parameters of an optimizer (and not a model) according to the right parameter groups and the following functions need to handle lists of lists of parameters (one list of each param group in model_pgs and master_pgs)","6fa92f21":"# Part 4 11a_transfer_learning","13b039b3":"\nThe util function from Apex to do this is prep_param_lists.","b99cf992":"## Implementation\n\nThe implementation relies on something called the beta distribution which in turns uses something which Jeremy still finds mildly terrifying called the gamma function. To get over his fears, Jeremy reminds himself that gamma is just a factorial function that (kinda) interpolates nice and smoothly to non-integers too. How it does that exactly isn't important...","e0cb3c68":"\nThe loss scale used is way higher than our previous number:","dd616b09":"## Serializing the model","f6c90395":"French horn or tench? The right answer is 70% french horn and 30% tench ;)","84e6c6ea":"-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","2c1088b2":"\nNB: If you see math symbols you don't know you can google them like this: \u0393 function.\n\nIf you're not used to typing unicode symbols, on Mac type ctrl-cmd-space to bring up a searchable emoji box. On Linux you can use the compose key. On Windows you can also use a compose key, but you first need to install WinCompose. By default the compose key is the right-hand Alt key.\n\nYou can search for symbol names in WinCompose. The greek letters are generally compose-\\*-letter (where letter is, for instance, a to get greek \u03b1 alpha).","52ab4d3d":"The sign bit gives us +1 or -1, then we have 5 bits to code an exponent between -14 and 15, while the fraction part has the remaining 10 bits. Compared to FP32, we have a smaller range of possible values (2e-14 to 2e15 roughly, compared to 2e-126 to 2e127 for FP32) but also a smaller offset.\n\nFor instance, between 1 and 2, the FP16 format only represents the number 1, 1+2e-10, 1+2*2e-10... which means that 1 + 0.0001 = 1 in half precision. That's what will cause a certain numbers of problems, specifically three that can occur and mess up your training.\n\n1. The weight update is imprecise: inside your optimizer, you basically do w = w - lr w.grad for each weight of your network. The problem in performing this operation in half precision is that very often, w.grad is several orders of magnitude below w, and the learning rate is also small. The situation where w=1 and lrw.grad is 0.0001 (or lower) is therefore very common, but the update doesn't do anything in those cases.\n2. Your gradients can underflow. In FP16, your gradients can easily be replaced by 0 because they are too low.\n3. Your activations or loss can overflow. The opposite problem from the gradients: it's easier to hit nan (or infinity) in FP16 precision, and your training might more easily diverge.","3a4a77a5":"## Converting the model to FP16\n\n\nWe will need a function to convert all the layers of the model to FP16 precision except the BatchNorm-like layers (since those need to be done in FP32 precision to be stable). We do this in two steps: first we convert the model to FP16, then we loop over all the layers and put them back to FP32 if they are a BatchNorm layer.","783e0d7e":"## XResNet","c0949aad":"## Imagenet(te) training","41444d9a":"Let's test this:","cc8e35cf":"-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","6e7a748d":"Training without mixed precision","f5779bf5":"\n## Creating the master copy of the parameters\nFrom our model parameters (mostly in FP16), we'll want to create a copy in FP32 (master parameters) that we will use for the step in the optimizer. Optionally, we concatenate all the parameters to do one flat big tensor, which can make that step a little bit faster.","f12ed008":"-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","0f84f3fe":"## cnn_learner","ca84d95d":"## The solution: mixed precision training\u00b6\nTo address those three problems, we don't fully train in FP16 precision. As the name mixed training implies, some of the operations will be done in FP16, others in FP32. This is mainly to take care of the first problem listed above. For the next two there are additional tricks.\n\nThe main idea is that we want to do the forward pass and the gradient computation in half precision (to go fast) but the update in single precision (to be more precise). It's okay if w and grad are both half floats, but when we do the operation w = w - lr * grad, we need to compute it in FP32. That way our 1 + 0.0001 is going to be 1.0001.\n\nThis is why we keep a copy of the weights in FP32 (called master model). Then, our training loop will look like:\n\n1. compute the output with the FP16 model, then the loss\n2. back-propagate the gradients in half-precision.\n3. copy the gradients in FP32 precision\n4. do the update on the master model (in FP32 precision)\n5. copy the master model in the FP16 model.\n\nNote that we lose precision during step 5, and that the 1.0001 in one of the weights will go back to 1. But if the next update corresponds to add 0.0001 again, since the optimizer step is done on the master model, the 1.0001 will become 1.0002 and if we eventually go like this up to 1.0005, the FP16 model will be able to tell the difference.\n\nThat takes care of problem 1. For the second problem, we use something called gradient scaling: to avoid the gradients getting zeroed by the FP16 precision, we multiply the loss by a scale factor (scale=512 for instance). That way we can push the gradients to the right in the next figure, and have them not become zero.","8e80cdc5":"\nThe thing is that we don't always want all the parameters of our model in the same parameter group, because we might:\n\n* want to do transfer learning and freeze some layers\n* apply discriminative learning rates\n* don't apply weight decay to some layers (like BatchNorm) or the bias terms\n\nSo we actually need a function that splits the parameters of an optimizer (and not a model) according to the right parameter groups.","9e5178ea":"## Problems with half-precision:\nTo understand the problems with half precision, let's look briefly at what an FP16 looks like (more information here).","70ccae3b":"\n## Label smoothing\nAnother regularization technique that's often used is label smoothing. It's designed to make the model a little bit less certain of it's decision by changing a little bit its target: instead of wanting to predict 1 for the correct class and 0 for all the others, we ask it to predict 1-\u03b5 for the correct class and \u03b5 for all the others, with \u03b5 a (small) positive number and N the number of classes. This can be written as:\n\n$$loss = (1-\u03b5) ce(i) + \u03b5 \\sum ce(j) \/ N$$\nwhere ce(x) is cross-entropy of x (i.e. $-\\log(p_{x})$), and i is the correct class. This can be coded in a loss function:","518c0fc2":"\nAnd now we can write a new version of the Callback that handles dynamic loss scaling.","c1855ea8":"It's also possible to save the whole model, including the architecture, but it gets quite fiddly and we don't recommend it. Instead, just save the parameters, and recreate the model directly.","d040ab32":"Training with mixed precision","d0595de6":"Then we can use it in MixUp:","31da622c":"\nIn the original article, the authors suggested three things:\n\n1. Create two separate dataloaders and draw a batch from each at every iteration to mix them up\n2. Draw a t value following a beta distribution with a parameter \u03b1 (0.4 is suggested in their article)\n3. Mix up the two batches with the same value t.\n4. Use one-hot encoded targets\n\nWhy the beta distribution with the same parameters \u03b1? Well it looks like this:","8eac3342":"![image.png](attachment:image.png)","1433dc81":"We need to replace the old model_summary since it used to take a Runner.","12ff185b":"\nPytorch already has an apply method we can use:","3feef8ff":"Now let's test this on Imagenette","180f6cd3":"### Copy the gradients from model params to master params\nAfter the backward pass, all gradients must be copied to the master params before the optimizer step can be done in FP32. We need a function for that (with a bit of adjustement if we have flat master).","49111037":"![image.png](attachment:image.png)\n\nthis is a resblok with the indenty path ","5cf14ea2":"Note: we implement the various reduction attributes so that it plays nicely with MixUp after. ","39a8cb11":"-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","e6b8aea9":"so instead of freezing the weights we can just freeze the learning rate by setting it to 0 for some layers. so we start by spltting our layers into 2 or more groups with a function (bn_splitter)","c4d1ab03":"# Part 3 11_train_imagenette","911c31f5":"\n## A little bit of theory\n\n\nContinuing the documentation on the fastai_v1 development here is a brief piece about mixed precision training. A very nice and clear introduction to it is this video from NVIDIA.\n\n### What's half precision?\nIn neural nets, all the computations are usually done in single precision, which means all the floats in all the arrays that represent inputs, activations, weights... are 32-bit floats (FP32 in the rest of this post). An idea to reduce memory usage (and avoid those annoying cuda errors) has been to try and do the same thing in half-precision, which means using 16-bits floats (or FP16 in the rest of this post). By definition, they take half the space in RAM, and in theory could allow you to double the size of your model and double your batch size.\n\nAnother very nice feature is that NVIDIA developed its latest GPUs (the Volta generation) to take fully advantage of half-precision tensors. Basically, if you give half-precision tensors to those, they'll stack them so that each core can do more operations at the same time, and theoretically gives an 8x speed-up (sadly, just in theory).\n\nSo training at half precision is better for your memory usage, way faster if you have a Volta GPU (still a tiny bit faster if you don't since the computations are easiest). How do we do it? Super easily in pytorch, we just have to put .half() everywhere: on the inputs of our model and all the parameters. Problem is that you usually won't see the same accuracy in the end (so it happens sometimes) because half-precision is... well... not as precise ;).","70ef54cf":"## Imagenet\nYou can see all this put together in the fastai imagenet training script. It's the same as what we've seen so far, except it also handles multi-GPU training. So how well does this work?\n\nWe trained for 60 epochs, and got an error of 5.9%, compared to the official PyTorch resnet which gets 7.5% error in 90 epochs! Our xresnet 50 training even surpasses standard resnet 152, which trains for 50% more epochs and has 3x as many layers.","fe783449":"## The main Callback","3fc59bda":"We can't use flat_master when there is a mix of FP32 and FP16 parameters (like batchnorm here).","7de6f9d6":"# Part 2 10c_fp16","86b67819":"\n## Util functions\nBefore going in the main Callback we will need some helper functions. We will refactor using the APEX library util functions. The python-only build is enough for what we will use here if you don't manage to do the CUDA\/C++ installation.","1c0d1ff0":"![image.png](attachment:image.png)","06fb074b":"Do nnote that with mixup there is not realy needed the orther types of transform like flipe since this is so good ","3f171fb2":"## Batch norm transfer","cec80c7c":"## adapt_model and gradual unfreezing","ae51dc58":"Of course we don't want those 512-scaled gradients to be in the weight update, so after converting them into FP32, we can divide them by this scale factor (once they have no risks of becoming 0). This changes the loop to:\n\n1. compute the output with the FP16 model, then the loss.\n2. multiply the loss by scale then back-propagate the gradients in half-precision.\n3. copy the gradients in FP32 precision then divide them by scale.\n4. do the update on the master model (in FP32 precision).\n5. copy the master model in the FP16 model.\n\nFor the last problem, the tricks offered by NVIDIA are to leave the batchnorm layers in single precision (they don't have many weights so it's not a big memory challenge) and compute the loss in single precision (which means converting the last output of the model in single precision before passing it to the loss).","5bf8afc5":"-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","0e88a607":"## Custom head","4c9d82df":"so to fix this we should only freeze all of the body paramters that are not in the batch norm layers ","df2445fa":"so now we have to label it but since it is nit in folders we have to look at the file name ","86098be4":"## Copy the master params to the model params\nAfter the step, we need to copy back the master parameters to the model parameters for the next update","5af6ed78":"![image.png](attachment:image.png)","e54e1e3c":"Questions: How does softmax interact with all this? Should we jump straight from mixup to inference?\nA: dont use it do the following-->","5ab778d8":"## Pets","de41456f":"so this is now a dog model predicter from above but its learner for it is looking at the pets databunch.\nso now we have to remove the last linear layers and replace it with one that has the right numbers of activation(channels) for the output wich is 37 different labels ","bcf06676":"# Mixup","e58d11fa":"\n# Dynamic loss scaling\nThe only annoying thing with the previous implementation of mixed precision training is that it introduces one new hyper-parameter to tune, the value of the loss scaling. Fortunately for us, there is a way around this. We want the loss scaling to be as high as possible so that our gradients can use the whole range of representation, so let's first try a really high value. In all likelihood, this will cause our gradients or our loss to overflow, and we will try again with half that big value, and again, until we get to the largest loss scale possible that doesn't make our gradients overflow.\n\nThis value will be perfectly fitted to our model and can continue to be dynamically adjusted as the training goes, if it's still too high, by just halving it each time we overflow. After a while though, training will converge and gradients will start to get smaller, so we also need a mechanism to get this dynamic loss scale larger if it's safe to do so. The strategy used in the Apex library is to multiply the loss scale by 2 each time we had a given number of iterations without overflowing.\n\nTo check if the gradients have overflowed, we check their sum (computed in FP32). If one term is nan, the sum will be nan. Interestingly, on the GPU, it's faster than checking torch.isnan:","07219e6c":"## What is mixup?\nAs the name kind of suggests, the authors of the mixup article propose to train the model on a mix of the pictures of the training set. Let's say we're on CIFAR10 for instance, then instead of feeding the model the raw images, we take two (which could be in the same class or not) and do a linear combination of them: in terms of tensor it's\n\n    new_image = t * image1 + (1-t) * image2\nwhere t is a float between 0 and 1. Then the target we assign to that image is the same combination of the original targets:\n\n    new_target = t * target1 + (1-t) * target2\nassuming your targets are one-hot encoded (which isn't the case in pytorch usually). And that's as simple as this."}}