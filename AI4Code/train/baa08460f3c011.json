{"cell_type":{"7af899cf":"code","0c70cf05":"code","81078193":"code","f2d65801":"code","6fca0331":"code","e3bf47f0":"code","8861c902":"code","4cc744a2":"code","03fdac03":"code","2fdc5bd0":"code","568ca8fa":"code","c0044fac":"code","9ecf536c":"code","c3a7b872":"code","4b1e4d19":"code","308e214f":"code","c349fca2":"code","cd066dc1":"code","67fd76f0":"code","9d280f1d":"code","195eacd9":"code","ee4862d4":"code","881a2563":"code","29485d5c":"code","24ab4b46":"code","4f02ab6c":"code","f38b58a0":"code","2b8240aa":"code","320d934e":"code","fe5c8c03":"code","c5c472fc":"code","e17562b6":"code","7bae4ffb":"code","fe12ccb4":"code","c23c2b43":"code","c3c2279c":"code","7dad3725":"code","4a70f629":"code","90aa8abe":"code","b22247c4":"code","108441ea":"code","8ae02aef":"code","756102be":"code","c182597a":"code","0cde8c3a":"code","14151f47":"code","4b5a9f09":"code","629de507":"code","803f8edb":"code","378017b7":"code","fd43ca4f":"markdown","4becc178":"markdown","bef4fbbb":"markdown","a5b7d80c":"markdown","753e872b":"markdown","313e4239":"markdown","40ad6c63":"markdown","9816070b":"markdown","7b30d285":"markdown","d5a0cdf6":"markdown","69e05666":"markdown","9cc695f5":"markdown","e2955253":"markdown","e9c83a34":"markdown","3cdc8e73":"markdown","48f724ea":"markdown","a6e6205b":"markdown","05f798c5":"markdown","e0cdb7a9":"markdown","e541802f":"markdown","ff970620":"markdown","5719d2e7":"markdown","7e19bc8f":"markdown","51d1086c":"markdown","6b275148":"markdown","90015567":"markdown","787f51a3":"markdown","95621d7c":"markdown","35e25225":"markdown","5945ea9b":"markdown","6a21ed76":"markdown","c4868dca":"markdown","784d1696":"markdown","28a8b285":"markdown","d8696b36":"markdown","67b2a100":"markdown","c62bf51c":"markdown","accbe9eb":"markdown","e022f808":"markdown","3441554f":"markdown","eea3ed0a":"markdown","6440a062":"markdown","72e6ba80":"markdown","f38579de":"markdown","1eefd995":"markdown","4c61aa2e":"markdown","ac247645":"markdown","2509454f":"markdown","d0c0bef4":"markdown","35f2e98e":"markdown","817bd803":"markdown","acbca36f":"markdown","84402e59":"markdown","12c33b5e":"markdown","c5b86448":"markdown","9e88a442":"markdown","235cdc8a":"markdown","4504cb3a":"markdown","4895d143":"markdown","ecfe86fc":"markdown","54b700ab":"markdown","6b420340":"markdown","08b1a419":"markdown","dce76b8f":"markdown","3f9ee24b":"markdown","02fe560d":"markdown","a60d01da":"markdown","3994ae82":"markdown","f0ddf4e3":"markdown","c132b155":"markdown","4dc206df":"markdown","850b273d":"markdown","46131665":"markdown","138913f3":"markdown","3576cea8":"markdown","714925a2":"markdown","2a515fd1":"markdown","8101fc0e":"markdown","c363fa93":"markdown","cceb077c":"markdown","d8fad5cc":"markdown","deb6981c":"markdown","67330f08":"markdown","a40d5fba":"markdown","0c6f78b1":"markdown","9c474f93":"markdown","5a2b6c6a":"markdown","14fdadba":"markdown","6dd4ceb6":"markdown","b8632295":"markdown","205a0b17":"markdown","e496213a":"markdown","cffb8dd7":"markdown","65d39532":"markdown","7c14495f":"markdown","d9d33547":"markdown","ad2a028f":"markdown","07380b74":"markdown","52068192":"markdown","bc91c14d":"markdown","5b9053e0":"markdown","d9e9633e":"markdown","77e6806a":"markdown","102a3f51":"markdown","5fc57855":"markdown","45571e5f":"markdown","27638b5b":"markdown","a7315651":"markdown","42b0dcbe":"markdown","7281f17a":"markdown","24452667":"markdown","6bc3610e":"markdown","7d81a1f1":"markdown"},"source":{"7af899cf":"# Importing necessary libraries\n\nimport pandas as pd                                                  # DataFrames\nimport numpy as np                                                   # Mathematical operations\nimport seaborn as sns                                                # Visualisations\nimport matplotlib as mplt                                            # Visualisations\nimport matplotlib.pyplot as plt                                      # Visualisations\n\nimport scipy.stats as s                                              # Statistics functions\nfrom scipy.stats import norm, pareto, expon, normaltest, chi2        # Statistics functions\nimport statsmodels.api as sm                                         # Statistics functions\nimport statsmodels.discrete.discrete_model as log                    # LogisticRegression\nfrom sklearn.linear_model import LogisticRegression                  # LogisticRegression\nfrom sklearn import svm                                              # SVM\n\n\nfrom patsy import dmatrices                                          # Matrix with interactions\nimport random\nimport warnings\nimport itertools\n\n\n# To speed up run of the kernel, we'll hush down all warnings (it's not advisable to do it at the design stage)\nwarnings.filterwarnings(\"ignore\")\n\n\n# Setting seed for repropructive purposes\nrandom.seed(10)","0c70cf05":"df = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")","81078193":"df.shape","f2d65801":"df.head()","6fca0331":"pd.isna(df).any()","e3bf47f0":"df.describe()","8861c902":"((np.sum(df.loc[df['age']>50, 'target']))\/(np.sum(df.loc[df['age']<50, 'target'])))*(np.sum(df['age']<50)\/np.sum(df['age']>50))","4cc744a2":"# Target\ndf['target'] = df['target']+1\ndf.loc[df['target']==2, 'target'] = 0\n\n# Thal\ndf.loc[df['thal']==0, 'thal'] = np.nan","03fdac03":"df = df.rename(columns={'cp':'chest_pain', 'trestbps':'blood_pressure', 'fbs':'blood_sugar', 'restecg':'cardio', 'thalach':'heart_rate', 'exang':'ex_angina', 'oldpeak':'ST_depression', 'ca':'vessels_coloured'})\ndf2 = df.copy()\ndf3 = df.copy()","2fdc5bd0":"# CHEST PAIN\ndf.loc[df['chest_pain']==0, 'chest_pain'] = 'asymptomatic'\ndf.loc[df['chest_pain']==1, 'chest_pain'] = 'typical'\ndf.loc[df['chest_pain']==2, 'chest_pain'] = 'atypical'\ndf.loc[df['chest_pain']==3, 'chest_pain'] = 'non_anginal'\n\n\n# CARDIO\ndf.loc[df['cardio']==0, 'cardio'] = 'left_ventricular_hypertrophy'\ndf.loc[df['cardio']==1, 'cardio'] = 'normal'\ndf.loc[df['cardio']==2, 'cardio'] = 'wave_abnormality'\n\n\n#SLOPE\ndf.loc[df['slope']==0, 'slope'] = 'downsloping'\ndf.loc[df['slope']==1, 'slope'] = 'flat'\ndf.loc[df['slope']==2, 'slope'] = 'upsloping'\n\n\n#THAL\ndf.loc[df['thal']==1, 'thal'] = 'fixed_defect'\ndf.loc[df['thal']==2, 'thal'] = 'normal'\ndf.loc[df['thal']==3, 'thal'] = 'reversable_defect'","568ca8fa":"for i in ['chest_pain', 'cardio', 'slope', 'thal']:\n    df = df.merge(pd.get_dummies(df[i], prefix=str(i)), right_index=True, left_index=True)\n    df2 = df2.merge(pd.get_dummies(df[i], prefix=str(i)), right_index=True, left_index=True)","c0044fac":"sns.set(style=\"whitegrid\", palette=\"pastel\", color_codes=True)\nsns.heatmap(df2.corr())","9ecf536c":"sns.pairplot(df3, hue='target')","c3a7b872":"sns.pairplot(df3, hue='target', x_vars=['age', 'sex', 'heart_rate', 'slope', 'ST_depression', 'target'], y_vars=['age', 'sex', 'heart_rate', 'slope', 'ST_depression', 'target'])","4b1e4d19":"ax = sns.lmplot('age', 'heart_rate', hue='target', col='target',ci=95, data=df, order=1).set(ylabel=\"Heart rate\", xlabel='Age').fig.suptitle(\"Effects of age on heart rate\", fontsize=25, x=0.53, y=1.05, fontstyle='oblique')","308e214f":"sns.boxplot('slope', 'ST_depression', data=df).set(ylabel=\"St depression\", xlabel='Slope')\nplt.title(\"Differences in means of 'St_depression' in relation the slope\", y=1.05, fontsize = 16, fontstyle='oblique')\n","c349fca2":"# Calculating mean of age for each level of slope\nmean_up = np.mean(df.loc[df['slope']=='upsloping', 'age'])\nmean_flat = np.mean(df.loc[df['slope']=='flat', 'age'])\nmean_downsloaping = np.mean(df.loc[df['slope']=='downsloping', 'age'])\n\n# Grand mean\ngrand_mean = np.mean([mean_up, mean_flat, mean_downsloaping])\nvariab_y = df['age']-grand_mean\ndiff_blood_pressure = df['blood_pressure'] - np.mean(df['blood_pressure'])\ndiff_blood_sugar = df['blood_sugar'] - np.mean(df['blood_sugar'])\ndiff_heart_rate = df['heart_rate'] - np.mean(df['heart_rate'])\n\n\nX = pd.DataFrame(np.ones(303), columns=['tau']).merge(diff_blood_pressure, left_index=True, right_index=True).merge(diff_blood_sugar, left_index=True, right_index=True).merge(diff_heart_rate, left_index=True, right_index=True)\nfrom statsmodels.graphics.gofplots import qqplot\n\n\nest = sm.OLS(variab_y, X)\nest2 = est.fit()\nprint(est2.summary())","cd066dc1":"sns.despine(left=True)\nsns.kdeplot(df.loc[df['target']==1,'age' ], bw=1.5, label=\"target - 1\")\nsns.kdeplot(df.loc[df['target']==0,'age'], bw=1.5,label=\"target - 0\")\nplt.title(\"Distributions of age by presence of heart disease\", y=1.05, fontsize = 16, fontstyle='oblique')","67fd76f0":"g = sns.catplot(y='age', x='sex', data=df, hue='target', kind='violin', inner=\"quart\", split=True, palette={0: \"y\", 1: \"b\"}).set(ylabel=\"Age\", xlabel='Gender', xticklabels=['female', 'male']).fig.suptitle(\"Importance of sex and age on heart disease\", fontsize=20, x=0.53, y=1.05, fontstyle='oblique')","9d280f1d":"df.groupby(['sex'])['target'].value_counts()","195eacd9":"sns.countplot(x='chest_pain', hue='target', data=df, dodge=True).set_xlabel('Chest pain')\nplt.title(\"Heart disease by chest pain\", y=1.05, fontsize = 16, fontstyle='oblique')","ee4862d4":"chi = np.round(s.chi2_contingency(pd.crosstab(df['chest_pain'], df['target']))[1], 4)\nprint(\"The p-value for chi_squared test of independence equals to \",chi, '.')","881a2563":"sns.distplot(df.loc[(df['target']==0), 'ST_depression'], kde_kws={'color':'b', 'label':'target - 0', 'lw':5, 'alpha':0.4})\nsns.distplot(df.loc[(df['target']==1), 'ST_depression'], kde_kws={'color':'r', 'label':'target - 1', 'lw':5, 'alpha':0.4})\nplt.title(\"Distribution of ST depression\", y=1.05, fontsize = 16, fontstyle='oblique')","29485d5c":"b = np.arange(1000)*0.02\nplt.plot(b, (-0.4974 - np.log(1.0396+b) + 0.5796\/(1.0808+2.3184*b+np.square(b))))","24ab4b46":"random =  np.random.normal(0, 0.25, 303)\ndf['sqrt_ST_depression'] = np.sqrt(df['ST_depression'])\nsns.distplot(df['sqrt_ST_depression'])","4f02ab6c":"means = lambda x: x.mean()\ndf_cov = df2.copy()\ndf_cov['thal'] =  np.where(np.isnan(df2['thal']), 2, df2['thal'])\nmean = means(df_cov)\ncov = np.linalg.inv(np.cov(df_cov.T)) \nmachalonobis = np.diag(np.sqrt(np.dot((df_cov-mean) @ cov, (df_cov-mean).T)))\n\n# Assuming that the test statistic follows chi-square distributed with \u2018k\u2019 degree of freedom (where k is the number of predictor variables in the model), we choose the critical value to be 0.025\nchi2.ppf((1-0.05), df=14)\nnp.sum(machalonobis>23.11)\n\ndf_no_outliers = df2.copy()\ndf_no_outliers = df_no_outliers[np.isin(machalonobis, np.sort(machalonobis)[:-7])]\n\n-(df_no_outliers.shape[0] - df.shape[0])","f38b58a0":"random_numbers_train = np.random.choice(len(df2['age']), size=250, replace=False)\nrandom_numbers_test = np.arange(303)[(np.isin(np.arange(303), random_numbers_train))==False]\ntrain = df2.iloc[random_numbers_train, ]\ntest = df2.iloc[random_numbers_test, ]\n\n\n# Outliers (test set - 51, train set - 250, validation set - 48)\n\nrandom_numbers_train_out = np.random.choice(df_no_outliers.index.values, size=250, replace=False)\nrandom_numbers_test_out = df_no_outliers.index.values[(np.isin(df_no_outliers.index.values, random_numbers_train_out))==False]\ntrain_out = df_no_outliers.loc[random_numbers_train_out, ]\ntest_out = df_no_outliers.loc[random_numbers_test_out, ]\n","2b8240aa":"def standardise(train, test):\n    train_target = train['target']\n    test_target = test['target']\n    train.drop(['target'], axis=1, inplace=True)\n    test.drop(['target'], axis=1, inplace=True)\n    means = lambda x: x.mean()\n    std = lambda x: x.std()\n    means_train = means(train)\n    std_train = std(train)\n    standard = lambda x: (x-means_train)\/std_train\n    train = standard(train)\n    test = standard(test)\n    train = pd.merge(train, train_target, left_index=True, right_index=True)\n    test = pd.merge(test, test_target, left_index=True, right_index=True)\n    return train, test\n\ntest.groupby(['cardio_wave_abnormality']).size()\ntrain.groupby(['cardio_wave_abnormality']).size()\n# There are only 0 instances of 'cardio_wave_abnormality' in the version without \"outliers\" and lambda expression calculates standard deviation to equal 0, in which case it divides the feature by 0 and generates NANs.\n# Therefore, we need to remove this feature from the version without \"outliers\"\ntrain_out.drop(['cardio_wave_abnormality'], axis=1, inplace=True)\ntest_out.drop(['cardio_wave_abnormality'], axis=1, inplace=True)\n\ntrain, test = standardise(train, test)\ntrain_out, test_out = standardise(train_out, test_out)","320d934e":"def negatives_positives_ratio(y, size, desired_lowest_ratio, desired_highest_ratio):\n    y_array = np.array(y).reshape((len(y)))\n    all_positions = np.arange(len(y))\n    positives_position = all_positions[y_array==1]\n    negatives_position = all_positions[y_array==0]\n    positives_amount = int(np.sum(y==1))\n    negatives_amount = int(np.sum(y==0))\n    positiv_max_ratio = np.round(positives_amount\/size, 2)\n    if positives_amount<desired_highest_ratio*size:\n        upper_bound = positiv_max_ratio\n    else:\n        upper_bound = desired_highest_ratio*size\n    if 0.25*size<size-negatives_amount:\n        lower_bound = np.round((size-negatives_amount)\/size, 2)\n    else:\n        lower_bound =  desired_lowest_ratio*size\n    print(lower_bound, upper_bound)\n    positives_number_to_choose = np.round(size*np.arange(lower_bound+0.005, upper_bound, 0.005))\n    return positives_number_to_choose,positives_position, negatives_position, positives_amount, negatives_amount, y_array, all_positions\n\n\n\ndef train_val_split(X, y, k, size, desired_lowest_ratio, desired_highest_ratio):\n    preds=[]\n    labels=[]\n    accuracies=[]\n    recalls=[]\n    precisions=[]\n    trainings_Xs=[]\n    trainings_ys=[]\n    val_Xs=[]\n    val_ys=[]\n    X = pd.DataFrame(X)\n    y = pd.DataFrame(y)\n    positives_number_to_choose,positives_position, negatives_position, positives_amount, negatives_amount, y_array, all_positions = negatives_positives_ratio(y, size, desired_lowest_ratio, desired_highest_ratio)\n    for i in range(k):\n        positives_number = int(np.random.choice(positives_number_to_choose, 1, replace=False))\n        negatives_number = size - positives_number\n        positions_train = np.concatenate((np.random.choice(positives_position, positives_number, replace=False), np.random.choice(negatives_position, negatives_number, replace=False)))\n        position_val = all_positions[np.isin(all_positions, positions_train)==False]\n        training_x = X.iloc[positions_train, ]\n        training_y = y_array[positions_train]\n        val_x = X.iloc[position_val, ]\n        val_y = y_array[position_val]\n        trainings_Xs.append(training_x)\n        trainings_ys.append(training_y)\n        val_Xs.append(val_x)\n        val_ys.append(val_y)\n    return trainings_Xs, trainings_ys, val_Xs, val_ys\n\n\ndef model_training_prediction(trainings_Xs, trainings_ys, val_Xs):\n    preds=[]\n    for training_x, training_y, val_x in zip(trainings_Xs, trainings_ys, val_Xs):\n        model = LogisticRegression(solver='liblinear').fit(training_x, training_y)\n        pred = (model.predict_proba(val_x)[:, 1]).ravel()\n        preds.append(pred)\n    return preds\n\n\n\ndef scores_reg(preds, labels, k, threshold, lower_bound_IC, upper_bound_IC):\n    accuracy_final = {}\n    precision_final = {}\n    recall_final = {}\n    for cutoff in threshold:\n        recalls=[]\n        accuracies=[]\n        precisions=[]\n        for counter in range(k):\n            pred = preds[counter]\n            label = labels[counter]\n            pred = np.where(pred>cutoff, 1, 0)\n            pred = np.array(pred.ravel())\n#           val_y = val_y.values.reshape((np.sum(np.isin(count, a)==False)))\n            TP = np.sum(np.logical_and(pred==1, label==1))\n            TN = np.sum(np.logical_and(pred==0, label==0))\n            FP = np.sum(np.logical_and(pred==1, label==0))\n            FN = np.sum(np.logical_and(pred==0, label==1))\n            accuracy = (TP + TN)\/(TP + TN + FP + FN)\n            recall = TP\/(TP+FN)\n            precision = TP\/(TP+FP)\n            recalls.append(recall)\n            accuracies.append(accuracy)\n            precisions.append(precision)\n        recall_mean = np.nanmean(recalls)\n        accuracy_mean = np.nanmean(accuracies)\n        precision_mean = np.nanmean(precisions)\n        recall_lower, recall_upper = np.nanquantile(recalls, np.array([lower_bound_IC, upper_bound_IC]))\n        precision_lower, precision_upper = np.nanquantile(precisions,np.array([lower_bound_IC, upper_bound_IC]))\n        accuracy_lower, accuracy_upper = np.nanquantile(accuracies,np.array([lower_bound_IC, upper_bound_IC]))\n        for scores in [accuracy_mean, recall_mean, precision_mean, recall_lower, recall_upper, precision_lower, precision_upper, accuracy_lower, accuracy_upper]:\n            scores = np.round(scores, 3)\n        accuracy_final[cutoff] = [accuracy_lower,accuracy_mean, accuracy_upper]\n        precision_final[cutoff] = [precision_lower, precision_mean, precision_upper]\n        recall_final[cutoff] = [recall_lower, recall_mean, recall_upper]\n    return accuracy_final, precision_final, recall_final\n\n\ndef K_fold_regr(X, y, k, size, threshold, lower_bound_IC=0.025, upper_bound_IC=0.975,  desired_lowest_ratio=0.25, desired_highest_ratio=0.75):\n    trainings_Xs, trainings_ys, val_Xs, val_ys = train_val_split(X, y, k, size, desired_lowest_ratio, desired_highest_ratio)\n    preds = model_training_prediction(trainings_Xs, trainings_ys, val_Xs)\n    accuracy_final, precision_final, recall_final = scores_reg(preds, val_ys, k, threshold, lower_bound_IC, upper_bound_IC)\n    return accuracy_final, precision_final, recall_final","fe5c8c03":"def negatives_positives_ratio(y, size, desired_lowest_ratio, desired_highest_ratio):\n    y_array = np.array(y).reshape((len(y)))\n    all_positions = np.arange(len(y))\n    positives_position = all_positions[y_array==1]\n    negatives_position = all_positions[y_array==0]\n    positives_amount = int(np.sum(y==1))\n    negatives_amount = int(np.sum(y==0))\n    positiv_max_ratio = np.round(positives_amount\/size, 2)\n    if positives_amount<desired_highest_ratio*size:\n        upper_bound = positiv_max_ratio\n    else:\n        upper_bound = desired_highest_ratio*size\n    if 0.25*size<size-negatives_amount:\n        lower_bound = np.round((size-negatives_amount)\/size, 2)\n    else:\n        lower_bound =  desired_lowest_ratio*size\n    print(lower_bound, upper_bound)\n    positives_number_to_choose = np.round(size*np.arange(lower_bound+0.005, upper_bound, 0.005))\n    return positives_number_to_choose,positives_position, negatives_position, positives_amount, negatives_amount, y_array, all_positions\n\n\n\ndef train_val_split(X, y, k, size, desired_lowest_ratio, desired_highest_ratio):\n    preds=[]\n    labels=[]\n    accuracies=[]\n    recalls=[]\n    precisions=[]\n    trainings_Xs=[]\n    trainings_ys=[]\n    val_Xs=[]\n    val_ys=[]\n    X = pd.DataFrame(X)\n    y = pd.DataFrame(y)\n    positives_number_to_choose,positives_position, negatives_position, positives_amount, negatives_amount, y_array, all_positions = negatives_positives_ratio(y, size, desired_lowest_ratio, desired_highest_ratio)\n    for i in range(k):\n# It's possible that the subsample might have deterministically collinear variables, therefore, we include try\/except expression to make the function draw another subsample if such a situation occurs \n        positives_number = int(np.random.choice(positives_number_to_choose, 1, replace=False))\n        negatives_number = size - positives_number\n        positions_train = np.concatenate((np.random.choice(positives_position, positives_number, replace=False), np.random.choice(negatives_position, negatives_number, replace=False)))\n        position_val = all_positions[np.isin(all_positions, positions_train)==False]\n        training_x = X.iloc[positions_train, ]\n        training_y = y_array[positions_train]\n        val_x = X.iloc[position_val, ]\n        val_y = y_array[position_val]\n        trainings_Xs.append(training_x)\n        trainings_ys.append(training_y)\n        val_Xs.append(val_x)\n        val_ys.append(val_y)\n    return trainings_Xs, trainings_ys, val_Xs, val_ys\n\n    \ndef training_model(X, y, k, size,  classifier, param_name1, param_value1, param_name2=0, param_value2=0, param_name3=0, param_value3=0,param_add_name1='coef0', param_add_value1=0, param_add_name2='gamma', param_add_value2='auto', lower_bound_IC=0.025, upper_bound_IC=0.975,  desired_lowest_ratio=0.25, desired_highest_ratio=0.75):\n    accuracies_train = []\n    recalls_train = []\n    precisions_train = []\n    accuracies_out = []\n    recalls_out = []\n    precisions_out = []\n    trainings_Xs, trainings_ys, val_Xs, val_ys = train_val_split(X, y, k, size, desired_lowest_ratio, desired_highest_ratio)\n    for training_x, training_y, val_x, val_y in zip(trainings_Xs, trainings_ys, val_Xs, val_ys):\n        if isinstance(param_name1, str) & isinstance(param_name2, str)==False:\n            param = {param_name1:param_value1, param_add_name1:param_add_value1, param_add_name2:param_add_value2}\n            model = classifier(**param).fit(training_x, training_y.ravel())\n        elif isinstance(param_name1, str) & isinstance(param_name2, str) & isinstance(param_name3, str)==False:\n            param = {param_name1:param_value1, param_name2:param_value2, param_add_name1:param_add_value1, param_add_name2:param_add_value2}\n            model = classifier(**param).fit(training_x, training_y.ravel())\n        elif isinstance(param_name1, str) & isinstance(param_name2, str) & isinstance(param_name3, str):\n            param = {param_name1:param_value1, param_name2:param_value2, param_name3:param_value3,  param_add_name1:param_add_value1, param_add_name2:param_add_value2}\n            model = classifier(**param).fit(training_x, training_y.ravel())\n        else:\n            raise ValueError(\"Parameter's name must be string\")\n        for error in ['out-of-sample', 'training']:\n            if error=='out-of-sample':\n                pred = np.array(model.predict(val_x))\n                label = np.array(val_y)\n            elif error=='training':\n                pred = np.array(model.predict(training_x))\n                label = training_y\n            TP = np.sum(np.logical_and(pred==1, label==1))\n            TN = np.sum(np.logical_and(pred==0, label==0))\n            FP = np.sum(np.logical_and(pred==1, label==0))\n            FN = np.sum(np.logical_and(pred==0, label==1))         \n            accuracy = (TP + TN)\/(TP + TN + FP + FN)\n            recall = TP\/(TP+FN)\n            precision = TP\/(TP+FP)\n            if error=='out-of-sample': \n                recalls_out.append(recall)\n                accuracies_out.append(accuracy)\n                precisions_out.append(precision)\n            elif error=='training':\n                recalls_train.append(recall)\n                accuracies_train.append(accuracy)\n                precisions_train.append(precision)\n    return recalls_out,  accuracies_out, precisions_out, recalls_train, accuracies_train, precisions_train\n\ndef lower_mean_upper(recalls_out, accuracies_out, precisions_out, recalls_train, accuracies_train, precisions_train, lower_bound_IC, upper_bound_IC):\n    recall_mean_out = np.nanmean(recalls_out)\n    accuracy_mean_out = np.nanmean(accuracies_out)\n    precision_mean_out = np.nanmean(precisions_out)\n    recall_mean_train = np.nanmean(recalls_train)\n    accuracy_mean_train = np.nanmean(accuracies_train)\n    precision_mean_train = np.nanmean(precisions_train)\n    recall_lower_out, recall_upper_out = np.nanquantile(recalls_out, np.array([lower_bound_IC, upper_bound_IC]))\n    precision_lower_out, precision_upper_out = np.nanquantile(precisions_out,np.array([lower_bound_IC, upper_bound_IC]))\n    accuracy_lower_out, accuracy_upper_out = np.nanquantile(accuracies_out,np.array([lower_bound_IC, upper_bound_IC]))\n    recall_lower_train, recall_upper_train = np.nanquantile(recalls_train, np.array([lower_bound_IC, upper_bound_IC]))\n    precision_lower_train, precision_upper_train = np.nanquantile(precisions_train,np.array([lower_bound_IC, upper_bound_IC]))\n    accuracy_lower_train, accuracy_upper_train = np.nanquantile(accuracies_train,np.array([lower_bound_IC, upper_bound_IC])) \n    return recall_mean_out, accuracy_mean_out, precision_mean_out, recall_mean_train, accuracy_mean_train,precision_mean_train, recall_lower_out,recall_upper_out, precision_lower_out, precision_upper_out, accuracy_lower_out, accuracy_upper_out, recall_lower_train, recall_upper_train, precision_lower_train, precision_upper_train, accuracy_lower_train, accuracy_upper_train    \n\n    \ndef scores(X, y, k, size,  classifier, param_name1, param_value1, param_name2, param_value2, param_name3, param_value3, param_add_name1, param_add_value1, param_add_name2, param_add_value2, lower_bound_IC, upper_bound_IC,  desired_lowest_ratio, desired_highest_ratio):\n    recalls_out,  accuracies_out, precisions_out, recalls_train, accuracies_train, precisions_train = training_model(X, y, k, size,  classifier, param_name1, param_value1, param_name2, param_value2, param_name3, param_value3, param_add_name1='coef0', param_add_value1=0, param_add_name2='gamma', param_add_value2='auto', lower_bound_IC=0.025, upper_bound_IC=0.975,  desired_lowest_ratio=0.25, desired_highest_ratio=0.75)\n    recall_mean_out, accuracy_mean_out, precision_mean_out, recall_mean_train, accuracy_mean_train,precision_mean_train, recall_lower_out,recall_upper_out, precision_lower_out, precision_upper_out, accuracy_lower_out, accuracy_upper_out, recall_lower_train, recall_upper_train, precision_lower_train, precision_upper_train, accuracy_lower_train, accuracy_upper_train = lower_mean_upper(recalls_out, accuracies_out, precisions_out, recalls_train, accuracies_train, precisions_train, lower_bound_IC, upper_bound_IC)\n    return recall_mean_out, accuracy_mean_out, precision_mean_out, recall_mean_train, accuracy_mean_train,precision_mean_train, recall_lower_out,recall_upper_out, precision_lower_out, precision_upper_out, accuracy_lower_out, accuracy_upper_out, recall_lower_train, recall_upper_train, precision_lower_train, precision_upper_train, accuracy_lower_train, accuracy_upper_train\n\ndef Hypertuning(X, y, k, size, classifier, params1=0, params2=0, params3=0, param_add_name1='coef0', param_add_value1=0, param_add_name2='gamma', param_add_value2='auto', lower_bound_IC=0.25, upper_bound_IC=0.975,  desired_lowest_ratio=0.25, desired_highest_ratio=0.75):\n    accuracies_final_dic_out= {}\n    recalls_final_dic_out = {}\n    precisions_final_dic_out = {}\n    accuracies_final_dic_train= {}\n    recalls_final_dic_train = {}\n    precisions_final_dic_train = {}\n    #\n    param_name1 = params1[0]\n    for w in range(len(params1[1])):\n        param_value1 = params1[1][w]\n        if params2!=0:\n            param_name2 = params2[0]\n            accuracies_final_dic_out[param_value1]= {}\n            recalls_final_dic_out[param_value1] = {}\n            precisions_final_dic_out[param_value1] = {}\n            accuracies_final_dic_train[param_value1]= {}\n            recalls_final_dic_train[param_value1] = {}\n            precisions_final_dic_train[param_value1] = {}\n            for ww in range(len(params2[1])):\n                param_value2 = params2[1][ww]\n                if params3!=0:\n                    param_name3 = params3[0]\n                    accuracies_final_dic_out[param_value1][param_value2]= {}\n                    recalls_final_dic_out[param_value1][param_value2] = {}\n                    precisions_final_dic_out[param_value1][param_value2] = {}\n                    accuracies_final_dic_train[param_value1][param_value2]= {}\n                    recalls_final_dic_train[param_value1][param_value2] = {}\n                    precisions_final_dic_train[param_value1][param_value2] = {}\n                    for www in range(len(params3[1])):\n                        param_value3 = params3[1][www]\n                        recall_mean_out, accuracy_mean_out, precision_mean_out, recall_mean_train, accuracy_mean_train,precision_mean_train, recall_lower_out,recall_upper_out, precision_lower_out, precision_upper_out, accuracy_lower_out, accuracy_upper_out, recall_lower_train, recall_upper_train, precision_lower_train, precision_upper_train, accuracy_lower_train, accuracy_upper_train = scores(X, y, k, size,  classifier, param_name1, param_value1, param_name2, param_value2, param_name3, param_value3, param_add_name1, param_add_value1, param_add_name2, param_add_value2, lower_bound_IC, upper_bound_IC,  desired_lowest_ratio, desired_highest_ratio)\n                        accuracies_final_dic_out[param_value1][param_value2][param_value3] = [accuracy_lower_out, accuracy_mean_out, accuracy_upper_out]\n                        recalls_final_dic_out[param_value1][param_value2][param_value3] = [recall_lower_out, recall_mean_out, recall_upper_out] \n                        precisions_final_dic_out[param_value1][param_value2][param_value3] = [precision_lower_out, precision_mean_out, precision_upper_out]\n                        accuracies_final_dic_train[param_value1][param_value2][param_value3] = [accuracy_lower_train, accuracy_mean_train, accuracy_upper_train]\n                        recalls_final_dic_train[param_value1][param_value2][param_value3] = [recall_lower_train, recall_mean_train, recall_upper_train] \n                        precisions_final_dic_train[param_value1][param_value2][param_value3] = [precision_lower_train, precision_mean_train, precision_upper_train]\n                else:\n                    recall_mean_out, accuracy_mean_out, precision_mean_out, recall_mean_train, accuracy_mean_train,precision_mean_train, recall_lower_out,recall_upper_out, precision_lower_out, precision_upper_out, accuracy_lower_out, accuracy_upper_out, recall_lower_train, recall_upper_train, precision_lower_train, precision_upper_train, accuracy_lower_train, accuracy_upper_train= scores(X, y, k, size,  classifier, param_name1, param_value1, param_name2, param_value2, 0, 0, param_add_name1, param_add_value1, param_add_name2, param_add_value2, lower_bound_IC, upper_bound_IC,  desired_lowest_ratio, desired_highest_ratio)\n                    print(param_value1, param_value2, accuracy_mean_out, accuracy_mean_train)\n                    accuracies_final_dic_out[param_value1][param_value2] = [accuracy_lower_out, accuracy_mean_out, accuracy_upper_out]\n                    recalls_final_dic_out[param_value1][param_value2] = [recall_lower_out, recall_mean_out, recall_upper_out] \n                    precisions_final_dic_out[param_value1][param_value2]= [precision_lower_out, precision_mean_out, precision_upper_out]\n                    accuracies_final_dic_train[param_value1][param_value2]= [accuracy_lower_train, accuracy_mean_train, accuracy_upper_train]\n                    recalls_final_dic_train[param_value1][param_value2] = [recall_lower_train, recall_mean_train, recall_upper_train] \n                    precisions_final_dic_train[param_value1][param_value2] = [precision_lower_train, precision_mean_train, precision_upper_train]\n        else:\n            recall_mean_out, accuracy_mean_out, precision_mean_out, recall_mean_train, accuracy_mean_train,precision_mean_train, recall_lower_out,recall_upper_out, precision_lower_out, precision_upper_out, accuracy_lower_out, accuracy_upper_out, recall_lower_train, recall_upper_train, precision_lower_train, precision_upper_train, accuracy_lower_train, accuracy_upper_train= scores(X, y, k, size,  classifier, param_name1, param_value1, 0, 0, 0, 0, param_add_name1, param_add_value1, param_add_name2, param_add_value2, lower_bound_IC, upper_bound_IC,  desired_lowest_ratio, desired_highest_ratio)\n            accuracies_final_dic_out[param_value1]= [accuracy_lower_out, accuracy_mean_out, accuracy_upper_out]\n            recalls_final_dic_out[param_value1] = [recall_lower_out, recall_mean_out, recall_upper_out] \n            precisions_final_dic_out[param_value1] = [precision_lower_out, precision_mean_out, precision_upper_out]\n            accuracies_final_dic_train[param_value1] = [accuracy_lower_train, accuracy_mean_train, accuracy_upper_train]\n            recalls_final_dic_train[param_value1] = [recall_lower_train, recall_mean_train, recall_upper_train] \n            precisions_final_dic_train[param_value1] = [precision_lower_train, precision_mean_train, precision_upper_train]\n    return  recalls_final_dic_out, precisions_final_dic_out, accuracies_final_dic_out,recalls_final_dic_train, precisions_final_dic_train, accuracies_final_dic_train","c5c472fc":"# First model: full with all variable and their interaction (of course without one variable per class of variables to avoid multicollinearity)\ny, X = dmatrices('target ~ age + sex + age:sex + blood_pressure+ chol+blood_sugar+heart_rate+heart_rate:age+vessels_coloured+ex_angina+ chest_pain_non_anginal + chest_pain_typical + chest_pain_asymptomatic + cardio_left_ventricular_hypertrophy+ cardio_wave_abnormality+slope_flat+slope_upsloping+slope_upsloping:ST_depression+slope_flat:ST_depression+ST_depression+thal_fixed_defect+thal_reversable_defect', data=train)\nlr = log.Logit(y, X).fit()\nprint(lr.summary())\n\n\n# Second model: with selected variables\ny2, X2 = dmatrices('target ~ age + sex + blood_pressure+heart_rate+heart_rate:age+vessels_coloured+chest_pain_atypical + chest_pain_non_anginal + chest_pain_typical + cardio_normal+slope_upsloping+slope_flat+slope_upsloping+slope_flat:ST_depression+thal_reversable_defect', data=train)\nlr = log.Logit(y2, X2).fit()\nprint(lr.summary())","e17562b6":"# Creating dataset with interactions also for the dataset without \"outliers\"\ny_out, X_out = dmatrices('target ~ age + sex + age:sex + blood_pressure+ chol+blood_sugar+heart_rate+heart_rate:age+vessels_coloured+ex_angina+ chest_pain_non_anginal + chest_pain_typical + chest_pain_asymptomatic + cardio_left_ventricular_hypertrophy+slope_flat+slope_upsloping+slope_upsloping:ST_depression+slope_flat:ST_depression+ST_depression+thal_fixed_defect+thal_reversable_defect', data=train_out)\n\ny2_out, X2_out = dmatrices('target ~ age + sex + blood_pressure+heart_rate+heart_rate:age+vessels_coloured+chest_pain_atypical + chest_pain_non_anginal + chest_pain_typical + cardio_normal+slope_upsloping+slope_flat+slope_upsloping+slope_flat:ST_depression+thal_reversable_defect', data=train_out)\n\n\naccuracies_reg, recalls_reg, precisions_reg= K_fold_regr(X, y, k=200, size=200, threshold=np.arange(0, 100, 0.5)*0.01)\n\naccuracies_reg2, recalls_reg2, precisions_reg2 = K_fold_regr(X2, y2, k=200, size=200, threshold=np.arange(0, 100, 0.5)*0.01)\n\naccuracies_reg_out, recalls_reg_out, precisions_reg_out = K_fold_regr(X_out, y_out, k=200, size=200, threshold=np.arange(0, 100, 0.5)*0.01)\n\naccuracies_reg2_out, recalls_reg2_out, precisions_reg2_out = K_fold_regr(X2_out, y2_out, k=200, size=200, threshold=np.arange(0, 100, 0.5)*0.01)","7bae4ffb":"# Function to dismantle lower_bound, mean, upper_bound for each threshold\ndef lower_mean_upper_score(score):\n    threshs, vals = zip(*score.items())\n    means_ac = []\n    lower_bound_ac=[]\n    upper_bound_ac=[]\n    for i in range(len(threshs)):\n        means_ac.append(vals[i][1])\n        lower_bound_ac.append(vals[i][0])\n        upper_bound_ac.append(vals[i][2])\n    return lower_bound_ac, means_ac, upper_bound_ac\n\n\n\nlower_bound_ac, means_ac, upper_bound_ac = lower_mean_upper_score(accuracies_reg)\nlower_bound_rc , means_rc , upper_bound_rc = lower_mean_upper_score(recalls_reg)\nlower_bound_pr, means_pr, upper_bound_pr = lower_mean_upper_score(precisions_reg)\nlower_bound_ac2, means_ac2, upper_bound_ac2 = lower_mean_upper_score(accuracies_reg2)\nlower_bound_rc2 , means_rc2 , upper_bound_rc2 = lower_mean_upper_score(recalls_reg2)\nlower_bound_pr2, means_pr2, upper_bound_pr2 = lower_mean_upper_score(precisions_reg2)\nlower_bound_ac_out, means_ac_out, upper_bound_ac_out = lower_mean_upper_score(accuracies_reg_out)\nlower_bound_rc_out , means_rc_out , upper_bound_rc_out = lower_mean_upper_score(recalls_reg_out)\nlower_bound_pr_out, means_pr_out, upper_bound_pr_out = lower_mean_upper_score(precisions_reg_out)\nlower_bound_ac_out2, means_ac_out2, upper_bound_ac_out2 = lower_mean_upper_score(accuracies_reg2_out)\nlower_bound_rc_out2 , means_rc_out2 , upper_bound_rc_out2 = lower_mean_upper_score(recalls_reg2_out)\nlower_bound_pr_out2, means_pr_out2, upper_bound_pr_out2 = lower_mean_upper_score(precisions_reg2_out)\n\n\nlower_bound_acs = [lower_bound_ac, lower_bound_ac2, lower_bound_ac_out, lower_bound_ac_out2]\nmean_acs = [means_ac, means_ac2, means_ac_out, means_ac_out2]\nupper_bounds_acs = [upper_bound_ac, upper_bound_ac2, upper_bound_ac_out, upper_bound_ac_out2]","fe12ccb4":"colors=['#003f5c', '#f95d6a' ,'#665191', '#ffa600', '#d45087', '#f95d6a', '#820401', '#ff7c43', '#f0a58f']\nlegends = ['full model', 'reduced model', 'full model no-outlier', 'reduced model no-outliers']\n\nfig, ax = plt.subplots(1, 1, figsize=(12,10))\nfor score_lower, score_mean, score_upper, color, legend in zip(lower_bound_acs, mean_acs, upper_bounds_acs, colors, legends):\n    ax.plot(np.arange(0, 100, 0.5)*0.01, np.round(score_mean, 3), label=legend, color=color)\n    ax.legend(loc='upper right')\n    ax.fill_between(np.arange(0, 100, 0.5)*0.01, np.round(score_lower, 3), np.round(score_upper, 3), alpha=.1, color=color)\nax.set_xlabel('Threshold', size=15)\nax.set_ylabel('Score', size=15)\nax.set_title('The comparison of accuracy between models \\n With confidence interval 95%', fontsize=25, x=0.53, y=1, fontfamily='sans-serif', fontweight=\"bold\")","c23c2b43":"fig, ax = plt.subplots(1, 1, figsize=(12, 10))\nax.plot(np.arange(0, 100, 0.5)*0.01, means_rc, color='#ff7c43', label='Recall')\nax.set_ylabel('Score', size=15)\nax.set_xlabel('Threshold', size=15)\nax.legend(loc='upper right')\nax.fill_between(np.arange(0, 100, 0.5)*0.01, lower_bound_rc, upper_bound_rc, alpha=.3, color='#ff7c43')\nax.plot(np.arange(0, 100, 0.5)*0.01, means_pr, color='#820401', label='Precision')   \nax.legend(loc='upper right')\nax.fill_between(np.arange(0, 100, 0.5)*0.01, lower_bound_pr, upper_bound_pr, alpha=.3, color='#820401')\nax.set_title('The comparison between Recall and Precision \\n With confidence interval 95%', fontsize=25, x=0.53, y=1,  fontfamily='sans-serif', fontweight=\"bold\")","c3c2279c":"def mean_bounds(score):\n    means=[]\n    lower_bound=[]\n    upper_bound=[]\n    for i in list(score.keys()):\n        lower_bound.append(score[i][0])\n        means.append(score[i][1])\n        upper_bound.append(score[i][2])\n    return means,lower_bound, upper_bound\n\n\ndef graph_auc(xs, axs, upper, lower, mean, row, col, algorithm_name, method, color):\n    axs[col, row].plot(xs, mean, color=color)\n    axs[col, row].fill_between(np.arange(len(lower)), lower, upper, alpha=.1, color=color)\n    axs[col, row].set_title('{} {}'.format(algorithm_name, method), size=25, y=1.02, fontfamily='serif')\n\n                                                                                      \n                                                                                      \ndef graph_pr_rc(xs, axs, upper_recall, lower_recall, upper_precision, lower_precision,  mean_recall, mean_precision, row, col, algorithm_name, method, color):\n    axs[col, row].plot(xs, mean_recall, linestyle='-', color=color)\n    axs[col, row].plot(xs, mean_precision, linestyle='-.', color=color)\n    axs[col, row].fill_between(np.arange(len(lower_recall)), lower_recall, upper_recall, alpha=.1, color=color)\n    axs[col, row].fill_between(np.arange(len(lower_precision)), lower_precision, upper_precision, alpha=.1, color=color)\n    axs[col, row].set_title('{} {}'.format(algorithm_name, method), size=25, y=1.02, fontfamily='serif')\n                                                                              \n                                                                                      \n                                                                                                     \n                                                                                      \ndef drawing_ac_pr_rc(ac_score_values_out, pr_score_values_out, rc_score_values_out, ac_score_values_train, pr_score_values_train, rc_score_values_train, algorithm_name, additional_param_name1, additional_param_name2, palette_colours=['#003f5c', '#ff7c43', '#665191', '#ffa600', '#d45087', '#f95d6a', '#820401', '#f0a58f']):\n    methods = list(ac_score_values_out.keys())\n    method = list(ac_score_values_out.keys())[0]\n    method2 = list(ac_score_values_out[method].keys())[0]\n    if isinstance(ac_score_values_out[method][method2], list):\n        dic_levels=2\n        xticks = np.arange(0, len(list(ac_score_values_out[method].keys())), 1)\n        xtick_labels = [str(i) for i  in list(ac_score_values_out[method].keys())]\n    else:\n        dic_levels=3\n        xticks = np.arange(0, len(list(ac_score_values_out[method][method2].keys())), 1)\n        xtick_labels = [str(i) for i  in list(ac_score_values_out[method][method2].keys())]\n    fig, axs = plt.subplots(2, len(methods), figsize=(8*len(methods), 20), sharey=True, squeeze=False)\n    fig.suptitle('The Precisions\/Recall\/Accuracy \\n in relation to threshold (95% confidence interval)', fontsize=30,y=0.95, fontfamily='sans-serif', fontweight=\"bold\")\n    count=0\n    for ax in fig.axes:\n        plt.sca(ax)\n        plt.subplots_adjust(left=0.1, right=0.9, top=0.85, bottom=0.1, hspace=0.3)\n        plt.ylim(-0.05, 1.05)\n        plt.xlabel('Parameter - C',labelpad=0.25, size=15, fontfamily='serif')\n        plt.ylabel('Value of the score', labelpad=15, size=15, fontfamily='serif')\n        plt.xticks(xticks, labels=xtick_labels, size=12, fontfamily='serif', rotation=45)\n        plt.yticks(np.arange(0, 11)*0.1, label=np.round(np.arange(0, 11)*0.1, 1), size=12, fontfamily='serif')\n        plt.grid(axis='y', linestyle='-.', alpha=0.7)\n    for score in ['accuracy', 'recall\/precision']:\n        for counter, method in enumerate(methods):\n            if score=='accuracy':\n                if dic_levels==3:\n                    methods2 = list(ac_score_values_out[method].keys())\n                    for color_type, method2 in enumerate(methods2):\n                        color = palette_colours[color_type]\n                        means, lower_bounds, upper_bounds = mean_bounds(ac_score_values_out[method][method2])\n                        xs = np.arange(0, len(list(ac_score_values_out[method][method2].keys())))\n                        graph_auc(xs, axs, upper_bounds, lower_bounds, means, counter, 0, algorithm_name, additional_param_name1, color)\n                    axs[0, counter].legend(['Accuracy for {}; {}: {}'.format(additional_param_name1, additional_param_name2, method2) for method2 in methods2], fontsize='medium')\n                elif dic_levels==2:   \n                    means_out, lower_bounds_out, upper_bounds_out = mean_bounds(ac_score_values_out[method])\n                    means_train, lower_bounds_train, upper_bounds_train = mean_bounds(ac_score_values_train[method])\n                    xs = np.arange(0, len(list(ac_score_values_out[method].keys())))\n                    graph_auc(xs, axs, upper_bounds_out, lower_bounds_out, means_out, counter, 0, algorithm_name, method, '#003f5c')\n                    graph_auc(xs, axs, upper_bounds_train, lower_bounds_train, means_train, counter, 0, algorithm_name, method, '#f95d6a')\n                    axs[0, counter].legend(['Accuracy on validation set', 'Accuracy on training set'], fontsize='large')\n            elif score=='recall\/precision':\n                if dic_levels==3:\n                    methods2 = list(ac_score_values_out[method].keys())\n                    for color_type, method2 in enumerate(methods2):\n                        color = palette_colours[color_type]\n                        means_pr, lower_bounds_pr, upper_bounds_pr = mean_bounds(pr_score_values_out[method][method2])\n                        means_rc, lower_bounds_rc, upper_bounds_rc = mean_bounds(rc_score_values_out[method][method2])\n                        xs = np.arange(0, len(list(ac_score_values_out[method][method2].keys())))\n                        graph_pr_rc(xs, axs, upper_bounds_rc, lower_bounds_rc, upper_bounds_pr, lower_bounds_pr,  means_rc, means_pr, counter, 1, algorithm_name, additional_param_name1, color)\n                    lg=()\n                    for c in range(len(methods2)):\n                        lg += ('Recall', 'Precision', )\n                    methods2_double = list(itertools.chain.from_iterable(itertools.repeat(x, 2) for x in methods2))\n                    axs[1,counter].legend(['{} for {}; {} : {}'.format(l, additional_param_name1,additional_param_name2, method2) for l, method2  in zip(lg, methods2_double)], fontsize='medium')\n                elif dic_levels==2:   \n                    means_pr, lower_bounds_pr, upper_bounds_pr = mean_bounds(pr_score_values_out[method])\n                    means_rc, lower_bounds_rc, upper_bounds_rc = mean_bounds(rc_score_values_out[method])\n                    xs = np.arange(0, len(list(ac_score_values_out[method].keys())))\n                    graph_pr_rc(xs, axs, upper_bounds_rc, lower_bounds_rc, upper_bounds_pr, lower_bounds_pr,  means_rc, means_pr, counter, 1, algorithm_name, method, '#ff7c43')\n                    axs[1, counter].legend(['Recall', 'Precision'],fontsize='large')","7dad3725":"X = train[['age', 'blood_pressure', 'chol', 'blood_sugar','heart_rate', 'ex_angina', 'ST_depression',  'vessels_coloured', 'chest_pain_atypical', 'chest_pain_non_anginal', 'chest_pain_typical', \n        'cardio_left_ventricular_hypertrophy', 'cardio_normal', 'cardio_wave_abnormality', 'slope_downsloping', 'slope_flat','slope_upsloping', 'thal_fixed_defect', 'thal_normal','thal_reversable_defect']]\n\n\ny = train['target']","4a70f629":"recalls_final_dic_out, precisions_final_dic_out, accuracies_final_dic_out,recalls_final_dic_train, precisions_final_dic_train, accuracies_final_dic_train = Hypertuning(X, y, k=200, size=200, classifier=svm.SVC, params1=(\"kernel\", ('linear', 'sigmoid', 'rbf')),\n                                                            params2=(\"C\", (0.0001,0.0002, 0.001, 0.002, 0.01,0.02,0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 30, 50)), params3=0)","90aa8abe":"drawing_ac_pr_rc(accuracies_final_dic_out, precisions_final_dic_out, recalls_final_dic_out, accuracies_final_dic_train,precisions_final_dic_train,  recalls_final_dic_train,'svm', 0, 0)","b22247c4":"recalls_final_dic_poly_out, precisions_final_dic_poly_out, accuracies_final_dic_poly_out, recalls_final_dic_poly_train, precisions_final_dic_poly_train, accuracies_final_dic_poly_train = Hypertuning(X, y, k=200, size=200, classifier=svm.SVC, params1=(\"kernel\", ['poly']),\n                                                            params2=(\"degree\", (1, 2, 3, 4, 5, 6)), params3=(\"C\", (0.0001,0.0002, 0.001, 0.002, 0.01,0.02,0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 30.0, 50.0)))","108441ea":"drawing_ac_pr_rc(accuracies_final_dic_poly_out, precisions_final_dic_poly_out, recalls_final_dic_poly_out,  accuracies_final_dic_poly_train, precisions_final_dic_poly_train,  recalls_final_dic_poly_train, 'svm','poly','degree')","8ae02aef":"recalls_final_dic_poly_1_out, precisions_final_dic_poly_1_out, accuracies_final_dic_poly_1_out, recalls_final_dic_poly_1_train, precisions_final_dic_poly_1_train, accuracies_final_dic_poly_1_train = Hypertuning(X, y, k=200, size=200, classifier=svm.SVC, params1=(\"kernel\", ['poly']),\n                                                            params3=(\"C\", (0.0001,0.0002, 0.001, 0.002, 0.01,0.02,0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 30, 50)), params2=('coef0', (0, 0.5, 1, 3, 5, 20, 30)), param_add_name1=\"degree\", param_add_value1=1)","756102be":"drawing_ac_pr_rc(accuracies_final_dic_poly_1_out, precisions_final_dic_poly_1_out, recalls_final_dic_poly_1_out, accuracies_final_dic_poly_1_train, precisions_final_dic_poly_1_train, recalls_final_dic_poly_1_train, 'Svm', 'linear', 'coeff')","c182597a":"recalls_final_dic_rbf_out, precisions_final_dic_rbf_out, accuracies_final_dic_rbf_out, recalls_final_dic_rbf_train, precisions_final_dic_rbf_train, accuracies_final_dic_rbf_train = Hypertuning(X, y, 200, 200, svm.SVC, (\"kernel\", ['sigmoid']),\n                                                            ('coef0', (0, 0.5, 1, 3, 5, 20, 30)), (\"C\", (0.0001,0.0002, 0.001, 0.002, 0.01,0.02, 0.1, 0.2, 0.5,1.0, 2.0, 5.0, 10.0, 20.0, 30.0, 50.0)), 'gamma', 'auto')","0cde8c3a":"drawing_ac_pr_rc(accuracies_final_dic_rbf_out, precisions_final_dic_rbf_out, recalls_final_dic_rbf_out, accuracies_final_dic_rbf_out, precisions_final_dic_rbf_out, recalls_final_dic_rbf_out,  'Svm', 'sigmoid', 'gamma')","14151f47":"recalls_final_dic_rbf_gamma_out, precisions_final_dic_rbf_gamma_out, accuracies_final_dic_rbf_gamma_out, recalls_final_dic_rbf_gamma_train, precisions_final_dic_rbf_gamma_train, accuracies_final_dic_rbf_gamma_train= Hypertuning(X, y, k=200, size=200, classifier=svm.SVC, params1=(\"kernel\", ['rbf']),\n                                                            params3=(\"C\", (0.0001,0.0002, 0.001, 0.002, 0.01,0.02,0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 30, 50)), params2=('gamma', (0.001, 0.002, 0.01, 0.02, 0.1, 0.2, 1, 2)), param_add_name1=\"coef0\", param_add_value1=0)","4b5a9f09":"drawing_ac_pr_rc(accuracies_final_dic_rbf_gamma_out, precisions_final_dic_rbf_gamma_out, recalls_final_dic_rbf_gamma_out, accuracies_final_dic_rbf_gamma_out, precisions_final_dic_rbf_gamma_out, recalls_final_dic_rbf_gamma_out,  'Svm', 'rbf', 'gamma')","629de507":"train_y = train['target']\ntrain_x =train.drop(['target'], axis=1)\ntest_y = test['target']\ntest_x = test.drop(['target'], axis=1)\n\n#\ntrain['thal'] = np.where(np.isnan(train['thal']), np.nanmean(train['thal']), train['thal'])\nsv = svm.SVC(C=0.01, kernel='linear').fit(train_x, train_y)\nlr = LogisticRegression().fit(train_x, train_y)\npred_sv_train = sv.predict(train_x)\npred_sv_test = sv.predict(test_x)\n\naccuracy_train_sv = np.sum(pred_sv_train==train_y)\/len(pred_sv_train)\npred_lr = lr.predict(test_x)\n\naccuracy_sv = np.sum(pred_sv_test==np.array(test_y))\/len(pred_sv_test)\naccuracy_lr = np.sum(pred_lr==np.array(test_y))\/len(pred_sv_test)","803f8edb":"accuracy_sv","378017b7":"accuracy_lr","fd43ca4f":"As such, we'd like to maximize the accuracy of the model but not necessarily at the cost of the recall. Our aim is to build an accurate predictive model, yet, we attach great importance on classifying the ill as ill, and simultaneously, can tolerate diagnosing the healthy as ones with a heart disease. The cost of falsely admitting someone to hospital is lower than the cost of \"missing out\" a potential patient with a heart disease (at least the social cost...). Having said that, we cannot label everyone as sick either. Choosing a right balance between recall and precision is difficult, and often, it's set based not on *a priori* known\/set ratio between recall and precision but on the value of a threshold at which there is an abrupt improvement in both scores (or drastic improvement in the one of our special interest, and a moderate deterioration in the other). Accordingly, I'm not going to set a rigid ratio that will indicate the equivalency of trade-off between recall and precision, and in return we'll visualise the both scores in relation to the threshold, which should give us a clue of a perfect one.","4becc178":"We ought to rename the columns to make their names more intuitive.","bef4fbbb":"There are more (by 1\/3) older people with the target variable equal to 0. Therefore, 0 should denote a heart condition.","a5b7d80c":"At this stage we'll check performance of the SVM on test set.","753e872b":"Since we have quite a small dataset at out disposable, we cannot be overly confident that the point estimate of the scores (we should remember that - the scores such as accuracy, recall, precision of a sample are estimators of such scores in the population) such as recall\/precision\/accuracy will be correct\/accurate. It's better to draw on the interval estimates called also the confidence intervals if we don't believe the point estimates to be stable.\n\nThere are two approaches towards constructing confidence intervals: parametric and non-parametric ones. As for the former, one has to assume distribution of a statistic of which we want to calculate the confidence interval (as we assume the normal distribution of means of some variable, according to the central theorem). The non-parametric approach, however, assumes that by drawing numerous sub-samples from a given sample we can calculate statistic on each subsample, which will give us a distribution of a given statistic with regard to subsamples. Then we make an assumption that the distribution of the statistic constructed by drawing samples from the population is the same as one constructed by drawing subsamples from our sample. Having computed the distribution, we calculate $2.5^{th}$ and $97.5^{th}$ percentiles which will be bounds of an 95%-confidence interval of a statistic. The advantage of this technique is that we don't have to assume the normal distribution or any other, we \"just create a distribution of our own\".\n","313e4239":"# 3.5 Models","40ad6c63":"### Feature selection","9816070b":"However, we can allow some points to be missclassified and the degree to which we tolerate it is expressed in the penalty term C. \n\nTherefore, our final constraint looks as follows:\n\n$$ y_i(w^Tx_i + b) \u2265 1 - \\xi_i $$\n\n, where for some $ i^{th} $ points, for which $ \\xi_i \\not= 0 $, we allow the constraint to be altered. \n\nErgo,    \n$  y_i*f(x_i)\u2265 1 - \\xi_i \\implies \\xi_i = max(0, 1-y_i*(f(x_i)) $, where hinge_loss is exactly $ max(0, 1-y_i(f(x_i)) $.\n\nIn conclusion, the aim of the SVM is to minimize: sum of distances of the misclassified points from the support vectors (hinge loss) and the margin between support vectors.","7b30d285":"The logistic regression isn't the classification algorithm per se, since it models probabilities (actually log-odds) of a point belonging to a given class. Therefore, it takes a less stringent\/safer approach by assigning only probabilities and not deciding definetly whether a point should belong to a given class. It does it, though, at the cost of accuracy and, therefore, for only classification's purposes the SVM performs usually better. On the other hand, the logistic regression can provide us with both: the significance of a variable's influence on the outcome (expressed in the so-called \"p-value\"), and the confidence with which a point\/observation can be assigned to a class (the probability expressed in the coefficients of the log-odds ratio). Moreover, the logistic regression allows also for another kind of flexibility, namely by classifying points on the basis of the logistic regression, you can control the value of the \"cutoff\"-threshold of probability above which point is assigned to a class - (that is, our tolerance towards misclassification of **either** of the two classes, which is expressed in the precision\/recall scores)*. There's also possibility to determine an interval of variables deemed hard to classify (the ones whose probability ranges 40% - 60%). Last but not least, the logistic regression apart from classification yields also coefficients, which are interepretable as odds ratios in contrast to the lack thereof in the of uninterpretable SVM. Yet, the logistic regression requires no collinearly among the independent variables and linearity of the independent variables with the logit, even so while performing cross-validation those assumption are sometime ignored.","d5a0cdf6":"Variable name | Description \n--------------|-------------\nage      | Age in years\nsex |  Female or male\ncp | Chest pain type (typical angina, atypical angina, non-angina, or asymptomatic angina)\ntrestbps | Resting blood pressure (mm Hg)\nchol | Serum cholesterol (mg\/dl)\nfbs | \tFasting blood sugar (< 120 mg\/dl or > 120 mg\/dl)\nresetcg | Resting electrocardiography results (normal, ST-T wave abnormality, or left ventricular hypertrophy)\nthalach | \tMax. heart rate achieved during thalium stress test\nexang | Exercise induced angina (yes or no)\noldpeak | \tST depression induced by exercise relative to rest\nslope | \tSlope of peak exercise ST segment (upsloping, flat, or downsloping)\nca | Number of major vessels colored by fluoroscopy\nthal | Thalium stress test result (normal, fixed defect, or reversible defect)\ntarget | Heart disease status: number of major vessels with >50% narrowing (0,1,2,3, or 4)","69e05666":"### Writing function to visualise accuracy\/recall\/precision","9cc695f5":"The dataset is quite small, having 13 explanatory variables across 303 observations. At first sight, it seems that unless the features are solid predictors for heart disease, we may face difficulties identifying the pattern of ill patients, and even if we score high on any measure of accuracy, there still might a lurking possibility that we'll overfit. Therefore, as it is the case with small dataset, we need to carefully consider the appropriate way for train\/test split (especially the proportions).","e2955253":"# 1. Introduction   ","e9c83a34":"![1_zfH9946AssCx4vzjaizWeg.png](attachment:1_zfH9946AssCx4vzjaizWeg.png)","3cdc8e73":"*As a matter of fact, you can calculate a probability, with which a given point is assigned to a class in the case of SVM, as well. It's called the confidence score (it's a ratio of distances between a point and both of two classes). All the same, it's not as interpretable as probabilites that we get from the logistic regression.","48f724ea":"# 1.3. Data wrangling","a6e6205b":"A well-performing logistic regression is that with a low function $ \\ell(\\beta) $, therefore, ultimately the aim of the logistic regression is to come up with such parameters $ \\beta $ that for $ \\{X_0 : y_i = 0 \\} \\implies X_0\\beta  \\rightarrow -\\infty $ and when $ \\{X_1 : y_i = 1 \\} \\implies  X_1\\beta  \\rightarrow \\infty $. Then the logit performs non-linear mapping of $ X\\beta $ in a following manner $ logit([-\\infty, \\infty]) \\rightarrow [0, 1] $ to meet the condition of bounded probability.\n","05f798c5":"The accuracy on training equals 81% and on test set 83%.","e0cdb7a9":"The number of 0's is still too considerable, therefore, the square-root transformation isn't the right transformation.","e541802f":"As has been said already, the mere fact that a set of points has extreme values, doesn't necessarily mean that we should treat those values as outliers, especially when we have a small dataset, then every observation is of value. Our main models will be logistic regression and SVM; the former is sensitive to outliers in its standard (nor regularised) form, the other while not being that sensitive to outliers (thanks to its tolerance to misclassification), still by penalizing the misclassification of outliers can influecne the objective function.  Some people may indeed have extreme values of some features (like cholesterol level), which by including in our model will acertain it that such values are indicative of the disease. Whether \"outliers\" have adverse effect on performance or not, will be checked by training model on two datasets.","ff970620":"# 2. Exploratory analysis","5719d2e7":"# 4.0 Conclusions and recommendations","7e19bc8f":"### Corections","51d1086c":"**For classification, wel'll use: SVM and Logisitc Regression, and then decide for a one on the basis of scores such as: accuracy, recall, precision. The choice of those algorithms over others will be justified later after having introduced the chosen algorithms. It's crucial that we take into consideration information that we found during the exploratory analysis. After splitting the data into appropriate sets and going through differences between SVM and logistic regression, we'll dive into tuning hyperparameters of aforementioned algorithms.  **","6b275148":"Now, we will try to capture some dependencies between variables, and then use acquired knowledge by contructing our model. \n\nTo gather basic information, it's advisable to visualise relations between all variables by plotting them (each variable versus each). The preliminary overview can also be presented with an aid of the correlation matrix, however, it doesn't convey as much information as a graph. Besides, correlation matrix assumes that the interaction between two variables is linear, which doesn't often hold. Lastly, it fails to show association between nominal variables.","90015567":"A great obstacle in a successful analysis of the dataset is the lack of deep domain expertise (in biology\/medicine) on my part, which cripples my ability to, in a corect manner, exploit variables or let alone interpret their effects on the outcome. ","787f51a3":"For a Bernoulli trial the standard devaition is given as:\n\n$$ sd = \\sqrt{p(1-p)} $$\n\nIn our case, $p$ is the probability of a correct classification (in the case of accuracy a set of $\\{(1, 1), (0, 0)\\} $ indicates success of a correct classification and the other set of $\\{(0, 1), (1, 0)\\} $ indicates failure of a correct classification - for the scores recall\/precision one can construct similar binary outcomes). Obviously, we don't know the true $p$, we can only assume the worst case scenario, when $p=1\/2$, because then then $sd$ is the greatest. Therefore,\n\n$$ \\sqrt{n}>\\frac{\\sqrt{0.5*0.5}}{0.05} \\implies n>100  $$ \n\n \nIn the worst case scenario, a sample has to have more than 100 . Actually glancing through other kernels, I saw that the average accuracy was in the region of 85%.\n\nWith prior probability of correct classification equal to 0.85, we calculate the size of our validation set.\n\n$$ \\sqrt{n}>\\frac{\\sqrt{0.85*0.15}}{0.05} \\implies  n>49 $$\n\n, which in fact is pretty similar if we had gone with the \"20\/80 rule\" that is often used by splitting dataset into training and test sets.\n\nIn conclusion,\n \n**train set - 200**  \n**validation set - 250\/5 = 50**   \n**test set - 53 ** ","95621d7c":"![](https:\/\/www.health.harvard.edu\/media\/content\/images\/p2_QA_MLJuly19_gi484761271.jpg)","35e25225":"**At the beginning let's have a quick look through perfomance of the SVM using: linear, sigmoid, rbf kernel.**","5945ea9b":"# 3.6 Comparison of models on the test set","6a21ed76":"We see that the difference between two models: full and restricted, is rather not significant (the pseudo R-sqaured is decreased by only two percentages points). Nonetheless, we'll cross-validate performance of two models on validation sets. \n\nIn total, we have 4 datasets to test, since for each of the two we've got a version with presumed outliers and without.","c4868dca":"Logisitc loss has some intuitive properties. \n\n\n\nIf $ y_i = 1 \\implies \\ell(\\beta_i) = 1*log(\\pi_i) + 0*\\log(1-\\pi_i) = \\log(\\pi_i) = \\log(\\frac{1}{1+e^{-X\\beta}}) $   \nIf $ y_i = 0 \\implies  \\ell(\\beta_i)  = 0*log(\\pi_i) + 1*\\log(1-\\pi_i)\\log(1 - \\pi_i) = \\log(1 - \\frac{1}{1+e^{-X\\beta}}) =  \\log(1 - \\frac{e^{X\\beta}}{1+e^{X\\beta}}) = \\log(\\frac{1}{1+e^{X\\beta}})$.\n","784d1696":"# 1.2. Quick look at the data","28a8b285":"As we remember the standard error is calculated according to the formula:\n\n$$ se=\\frac{sd}{\\sqrt{n}} $$\n, where $se$ - standard error, $sd$ standard deviation, $n$ - the size of a sample\n","d8696b36":"![Bez%C2%A0tytu%C5%82u.png](attachment:Bez%C2%A0tytu%C5%82u.png)","67b2a100":"After having scanned through the aggregated graph above, we can conclude that there is a negative relationship between variables: **age** and **heart rate**, and between **slope** and **depression**. In addition, variables: **thal**, **vessels_coloured**, **cardio** appear to have values which are underrepresented (some values of those variables make up small percentage of all values of the given variable). Therefore, we're going to have to decide whether or not to merge underrepresented values with other values. The dataset also seems to include some outliers (esp. in **depression**, **chol**, **blood_pressure**). While excluding outliers from an analysis, one should be extremely cautious, since excluding them may deteriorate the performance of an algorithm, nonetheless. ","c62bf51c":"Every, roughly speaking, classification\/regression algorithm consists of two components, which are to be minimized.\n\n$$ \\min_{w} \\sum_{x, y}L(w^\u22a4x,y)+\u03bbh(w) $$\n\nThe $ L() $ function is a loss function, which expresses \"unfit\" of the model on the training set, the other, $ h(w) $, is a regularisation term, which is typically chosen to penalise the complexity of the model (the degree to which we want to penalise is controlled by the $ \u03bb $ parameter) in order to prevent a model from overfitting. Of course, the SVM algorithm also has a constraint aside from the minimisation of the objective function.\n\nSurprsingly enough, even though the logistic regression comes from the probabilistic background, whereas the SVM classifier takes more geometrical approach, they differ only with the loss function (and the SVM has already a regularisation term while the logistic regression only can be regularised).  The logistic regression minimizes the logistic loss, while the SVM classifier minimizes the hinge loss (which while being convex is bound to converge to a global minimum than a standard zero-one loss).\n\n![main-qimg-a91f68bfa260b30d437ef9a511153e64.png](attachment:main-qimg-a91f68bfa260b30d437ef9a511153e64.png)","accbe9eb":"The linear kernel\n$$ K(x,y)=x^Ty $$\n\nIf one suspects that data aren't linearly separable (or checked it empirically), another recommendable kernel is the radial basis function kernel.\n\n$$ K(x,y)=e^{\u2212\u03b3\u2225x\u2212y\u2225^2} $$\n\nLooking at its formula, it's easy to notice this kernel yield the higher values the closer points are to each other. Therefore, in contrast with other kernels, this one has a property of  *translation invariance*: (a property holding that $ K(x,y)=K(x+a,y+a) $), which means that it's sensitive to relative distances but not to absolute values (magnitude). In addition, the rbf is based on the Euclidean distance as opposed to the dot product as in other kernels. The most important difference between these two measures of similarity, is that the dot product measures the distance **from the origin** (actually to be precise it projects one vector onto another, the distance of which, with the aid of imagination, is clearly dependant on the angle between the two vectors but also on their **absolute magnitude**)  whereas the euclidean distance **from each other**. The distinction is especially visible when considering the following example, let's suppose $ x = (1, 1)$ and $ y = (-1, 1) $. Then the dot product $ <(x) | (y)> $ is equal to 0 (which indicates orthogonality\/perpendicularity), which suggest that vector are completely different, whereas the Euclidean distance is small, indicating similarity. In conclusion, if, in a given dataset, the magnitude of vectors isn't crucial (just the proportion of features in a vector) then rbf should be chosen.\n\nApart from different similarity measure, the rbf kernel allows us also to tweak $\\gamma$ paramater, which for high values classify only close points to the same class (therefore ovefit), for low values underfit.\n\n\nAs a general rule, one chooses either the linear or rbf kernel, however, if the dataset in question isn't to big, it's always a good practice to hyperparameterize the choice of the kernel through cross-validation. One of the most known advantage of the SVM algorithm is that it handles dataset with many features well, however, when the number of features is considerably large, it may be advisable not to use the rbf due to computation time.\n\nOther popular kernels are as follows:\n\nThe polynomial kernel\n$$ K(x,y)=(\\gamma*x^Ty+c)^d $$\n\nThe sigmoid kernel\n\n$$ K(x,y)=tanh(\\gamma*x^Ty+c) $$\n\n, where $c$ and $\\gamma$ are additional parameters.","e022f808":"# 3.3. Choice of a model","3441554f":"Alas, I couldn't find an already written function in Python, for hyperparameterizing with monte-carlo fold validation option, which will, hopefully, minimise the loss of information for training that is occuring from creation of a validation set and reduction of the training set size. Taking advantage of writing the function by myself, I also included in it not only accuracy of models but scores such as: recall and precision, and other needed parameters.","eea3ed0a":"It seems that we have imbalanced data, which has a twofold nature. Firstly, women make up 32% of the sample, secondly, the diagnosed women acount for 27% of the total number of women, whereas as far as men are concerned, this fraction equals 45%. The first question that comes to our minds is why women are twice less likely to appear in our sample. Possibly, women are, in fact, less probable to have a heart conditions, proof of which can be seen in the target ratio within genders, but also in the sheer number of women, more healthy on average, thus, subjected to examination equally more seldom. To tell the truth, we are in no position to check the plausibility of such scenarios, but all the same, we'll assume that the gender of a patient is random and there's no hidden bias in choosing patient with a given gender. \n\nFurthermore, when it comes to the men, the distribution of diagnosed are pretty much similar to one of the healthy, except for the fact that it's shifted towards the greater age for the former. Other than that the both distribution share common characteristics. Quite differently, the distribution of the ill women has a considerably lower variance than that of the healthy. This implies that, in reality, there are no women with heart conditions below 50, and the age, for women, is a better predictor of being healthy than for men.","6440a062":"# 3. Predictive models","72e6ba80":"The chest paint seems to be an important variable for prediciting the heart disease.","f38579de":"### Writing functions for hyperparameterizing","1eefd995":"With the presence of the heart disease, values of **ST_depression**, on average, increase, but the contrast between values of **ST_depression** in relation to the target variable is especially noticeable at the value of 0, which equals the absence of **ST_depression** - that is, for healthy patients it is substantialy more common to have the **ST_depression** equal to 0. Therefore, we'll create a dummy variable informing whether a patient's **ST_depression** equals 0, as we suspect that it might carry additional information for our model apart from being a value at the scale. Hopefully, this will improve the performance of our algorithms.","4c61aa2e":"**The SVM algorithm tries to find such parameters $ w $ (often called weights) ensuring that classification of both classes is the most \"confident\" (i.e., the closest points to being missclassified placing themselves on the suport vectors are the most distinct - separated by a possibly largest margin-, allowing at the same time for some percentage of misclassification, the rigorousness of which is controlled by the C parameter), whereas the logistic regression tries to find such parameters $ \\beta $ ensuring that observations of a chosen class have been assigned higher probabilities (and observations of the other class possibly lowest). The SVM takes into consideration only points lying on the support vectors, whereas the logistic regression the whole set of points (therefore, it's possible that in exluding a lot of points from the SVM, the decision boundary may not even flinch, whereas the exclusion of every point has some marginal effect on the parameters of the logistic regression). If we're interested only in classifying points then the SVM is rather recommended, when the emphasis is shifted more to knowing the confidence (probability) of the classification then probably the logistic regression would be the best choice. Last but not least, the SVM has a regularisation term already incorporated in the algorithm, consequently it overfits less frequently (logistic regression doesn't have, by deafult, a regularisation term).**\n\n**Having said that, it's often extremely hard to say *a prior* which one will be recommended for a given scenario.**","ac247645":"# 2.2. Age","2509454f":"## SVM","d0c0bef4":"### Summing up the differences between SVM and logistic regression","35f2e98e":"Almost all machine learning algorithms (beside LDA or QDA) don't require normality from variables, yet the severe skewness of variables should attract our attention and make us think of a possible origin of such a distribution. The most common plausible explanations are as follows:\n\n1) variable (consequently entire sample) comes from a non-homogenous population (especiably visible when the distribution is evidently non-unimodal, then it may suggest that you sample from two or more quite distinct population)     \n2) sample is not representative (each individual didnt't have equal chance to be picked from population) due to the sample bias, thus, not enabling us to infer about the population         \n3) a variable, indeed, has a different (non-normal) distribution\n\nIn the case of the last option, if the data is severely skewed, it's reasonable to assume that there'll be no linear relationship with a given variable and the outcome. Therefore, if we are going to use algorithms such as the logistic regression that assumes linearity (of independent variable and log-odds), then if the relationship isn't linear then the estimates will be baised. Besides, in transforming the data so that their distribution is more symmetric, you can often improve the performance of algorithms. Whether it'll do so, we'll find out soon.","817bd803":"**The distribution of ST_depression differs significantly for patients with disease and healthy ones**","acbca36f":"We need to unpack the output of the previous function so that we have the mean of the score and $ 2.5^{th} $ and $ 97.5^{th} $ percentiles to compute confidence intervals.","84402e59":"### Determining meanings of the target variable","12c33b5e":"It seem that according to the Machalonobis distance with the critical value of 0.05 and 14 degrees of freedom (statistic chi sqaure equal to 23), there aren't any outliers in our dataset. Yet to be perfectly certain, we'll create a copy dataset without 7 variables with highest distance and check how excluding them from the dataset will change the performance (sensitive analysis).","c5b86448":"**There are no missing values.**","9e88a442":"### Comment to the graphs","235cdc8a":"** We want to log-transform the variable, however, we need to resolve the issue of having 0's in our variable, which obviously cannot be transformed with logarithm. To shed a light of this problem, we need to understand a couple of useful concepts.**","4504cb3a":"**Distribution of sex conditioned on age**","4895d143":"### Train-test split","ecfe86fc":"Creating dummy variables for nominal variables - some algorithms require the nominal variables to be split into dummy variables.","54b700ab":"**Below you can find the description of the data.**","6b420340":"This kernel is devoted to finding best-performing classification algorithms with use of hyperparameter tuning, which will be preceded by a comprehensive exploratory analysis with data visualisation. Having found the best algorithm, we'll try to recognise patients with heart diseases given a couple of features describing their health condition. I hope that everyone reading this will find this kernel informative. ","08b1a419":"Therefore, in the case of a model when we don't treat recall and precision scores as equally important, we cannot use scores such as F-score (being the harmonic mean of precision and recall whose formula is given by $ F1 = 2\\frac{Precision*Recal}{Precision+Recall} $) or auxilliary graphs such as ROC\/AUC, which can be seen below.","dce76b8f":"### Logistic loss\n\nThe logistic loss is a log-likelihood function (for either $y_i = 0 $ or $y_i = 1 $) derived from the Bernoulli mass function. The logistic loss is as follows:\n\n$$ \\newcommand{\\Lagr}{\\mathcal{L}} $$\n$$ Log \\Lagr(\\beta) = \\ell(\\beta) = \\sum_{i}^{n} [y_i\\log(\\pi_i) + (1-y_i)\\log(1-\\pi_i)]  $$,\n\nwhere $ \\ell $ is the log-likelihood function, $ n $ is the total number of observations, $ \\pi $ is the probability of correctly assigning a point to a given class and $ y $ is a class indicator (either 1 or 0).","3f9ee24b":"**Plotting biased arisen in relation to the value of the constant.**","02fe560d":"We've managed to capture some dependency between the slope of the peak exercise ST segment and ST depression. Due to lack of the domain expertise in medicine on my part, I find it particularly hard to find any interpretation of this phenomenon, let alone determine whether it can be pinpointed to the causal effect or it should be regarded as a spurious correlation.\n\nAside from **St_depression**, **age** seems to vary according to **slope**. To measure the effect of **age** on the **slope** we can draw on the analysis of covariance (*ANCOVA*). It'll check if the means of **age** for each slope are significantly different. More formally, the aim of the ANOVA is to see whether the variance between groups is significantly larger than the variance within group (if it is then the grouping variable recognizes some distinct clusters). In the ANCOVA case, we take into consideration also the covariates in order to increase the statistical power of the test (increase the probability of rejecting the null hypothesis, stating that there are no significant differences).\n\nTo show inuition behind the ANCOVA, we can break down the variable into the following components.\n\n\n\n$${\\displaystyle y_{ij} =\\mu +\\tau _{i}+\\mathrm {B} (x_{ij}-{\\overline {x}})+\\epsilon _{ij}.}$$,\n\n\n\nwhere $y_{ij}$ is the $j^{th}$ observation of the variable of interest under the  $i^{th}$ categorical group,  $x_{ij}$ is the $j^{th}$ observation of the covariate under the $i^{th}$  group, $\\mu$ is the grand mean and $\\overline {x}$  the global mean.\nThe variables to be fitted are $\\tau _{i}$ (the effect of the $i^{th}$ level of the IV), *B* (the slope of the line) and $\\epsilon _{ij}$ (the associated unobserved error term for the $j^{th}$ observation in the $j^{th}$ group). The equation can be transformed as:\n\n\n\n$$ {\\displaystyle y_{ij} -\\mu  =\\tau _{i}+\\mathrm {B} (x_{ij}-{\\overline {x}})+\\epsilon _{ij}.} $$\n\n\n\n$$ {\\displaystyle Total\\_variability = Between\\_variability +\\mathrm {B} * Covariates\\_variability + Random\\_variability.} $$\n\n\n\n\nThe goal is to check whether the *tau* is statistically significant - statistically significance differences in **age** grouped by **slope**.","a60d01da":"The log-transformation (as well as the square-transformation) with 0's doesn't yield satisfactory results, consequently it should only be recommended when we have a strong evidence to support such a transformations. To allow for the presence of the 0 values, we could consider another similar-looking distribution which allows for existence of 0's, namely the exponential distribution, whereby we could grasp the idea of **ST_depression** as an interval with length according to how long we must wait to an end of the stress test depression, yet because of the lack of theoretical background, we'll leave the variable untransformed.","3994ae82":"To find points that suspected to be outliers, the mechalonobis distance is an oft-used similarity metrics. It's similar to Euclidean distance but it takes into account also the variability of the data (standard deviation) and correlation. By way of illustration, it can be thought of as a measure from a given vector to the distribution of points, consequently it well fits into explaining if a given point was drawn from the same population as the rest of points (which is in fact checking if some point\/observation is outlier). Other applications include checking to which cluster of points should a point belong. Below you can find the formula of the Machalonobis distance.\n\n$$ D_M = \\sqrt{(X-\\bar{X})S^{-1}(X-\\bar{X})^{T}} $$\n\n,where $ X $ is a vector in a feature space, $\\bar{X}$ is vectors of means of each feature and $ S $ is a covariance matrix between features.\n","f0ddf4e3":"As suspected the optimal \"cutoff\" of the logistic regression lies in the region of 0.5. There isn't any significant difference between the four models, but all the same it seems that the full model performs marginally better, and besides, when in doubt choosing model, we should aim for the one that makes the least assumptions.","c132b155":"There's a heated debate whether we should select variables for the classification algorithms like SVM. Some argue that the SVM algorithm has a regularisation term (a quasi squared l2-norm penalty), which will force a coefficient (weight) of an irrelevant variable to be close to 0 if it has no effect on the outcome (as a matter of fact, if someone is certain that some of the variables are insignificant and including them in the equation may introduce noise, then it's recommended to use l1-norm, which diverges to 0 faster). \n\nBy and large, if the feature selection is performed, it's: for generalisation purposes (e.g., understanding which genes are relevant in relation to the disease), running time requirements (e.g., pedestrain detection), computation expensiveness (training SVM takes up a lot of RAM), constraint (only some variables has to be chosen) and interpretation imposed by itself, yet, it's believed that the abundance of irrelevant features shouldn't worsen performance (although it may introduce a lot of noise when the number of irrelevant features is large, for reference look:  J. Weston et al.: Feature Selection for SVMs http:\/\/www.cs.columbia.edu\/~jebara\/6772\/papers\/weston01feature.pdf or I. Guyon and A. Elisseef, 2003). One way to deal with this conundrum is as proposed by George Forman (et al.), namely after having scaled all variables to be in an interval [0,1], to rescale them on the basis of inluence they have on the outcome (similar approach can be found in Yves Grandvalet, Stephane Canu: Adaptive Scaling for Feature Selection in SVMs: https:\/\/www.utc.fr\/~grandval\/nips02.pdf). Since the objective function is:\n\n$$ \\min_{w} w^Tw + \\sum_{i=1}^m \\xi_i $$\n\nUnder the constraints: $ w^Tx + b \\geq 1 - \\xi_i $ and $ \\xi_i \\geq 0 $.\n\nTherefore, it's easy to notice that with larger values of some features (through rescaling them to be outside bounds of [0 ,1]), less of the regularised weight $ w_i $ is required to achieve the same effect (that is minimisation of the objective function). Also, from the geometrical perspective, it's quite intuitive that if we inflate one of the dimensions of the feature space,  points will be \"stretched out\" in this \"direction\" creating, relatively to other dimensions, vaster space allowing to fit a large-margin decision boundary. To capture relevance of variable, often the Information Gain (or other forms of measuering influence on the outcome) is drawn on (although it should be pointed that there are also other techniques, laid out by Olivier Chapelle, S. Sathiya Keerthi: http:\/\/olivier.chapelle.cc\/pub\/jsm_fsel.pdf, which are, I'm afraid, beyond the scope of my understanding).\n\nEven so, the \"weighted\" SVM is recommended only for a large pool of features, in our case we can fall back on the logistic regression, which can be used to gauge effect of the independent variables on the outcome. Then we'll see whether the dataset with restricted features performs better.","4dc206df":"Healthy patients are, more or less, uniformly distributed in age ranging 40-60, in contrast to those diagnosed with a heart condition which center around the age of 60 and are relatively underrepresented in the age band 30-50. It doesn't come as surprise to us that older people are more prone to medical conditions. What we might be interested in, is a potential  \"moderating\" effect of the gender on the distribution of age in relation to the target. ","850b273d":"# Classification of Heart Diseases","46131665":"# 3.2. Scaling","138913f3":"# 2.4. Outliers","3576cea8":"### Logistic regression vs SVM","714925a2":"There doesn't seem to be relevant differences in **age** in relation to **slope**.","2a515fd1":"We need to calculate how much the probability of a heart disease is higher for patients older than 50. The probability (here, accordin to the Bernoulli scheme) is derived simply through maximisation of the log-likelihood function (I recommend anyone reading it to do it themselves).\n\n$$ p = \\frac{\\sum_{i} x_i}{n} $$, \nwhere $p$ is the probability of a disease, $\\sum_{i} x_i$ the number of a patients (older than 50) with a heart disease, $n$ is the number of patients older than 50.","8101fc0e":"In this kernel, we faced a problem of a small sample. It presents difficulties especially at the stage of splitting the data set into training and testing sets, since if we leave too few observations for training, the model won't be fed with much information, if we leave too few observations in the test set (as in our example), we can score low, although the model fit the data well. This conundrum becomes especially acute if we want to tune hyperparameters, and the monte-carlo cross validation alleviate this probem to some extent. \n\nWhen it comes to data, having no expertise in a given field, the feature selection may become cumbersome. The aid of techniques such as logistic regression or correlation isn't often helpful either, since the relationship between independent and dependent variables may be way more complicates than linear one. In such cases, the information gain might come handy. \n\nIn conclusion, the right split of the data in case of a small dataset might be equally important to the choice of a algorithm.","c363fa93":"We have to scale (standardise) the test set with the same standard deviation and mean (of the training set).","cceb077c":"# 3.3.1 Logistic regression and SVM","d8fad5cc":"Assigning names to factor values for visualisation purposes.","deb6981c":"There is a clear surge in the accuracy score at the value of the C parameter equal to 1.0 as far as the rbf and sigmoid is concerned. It doesn't come as a surprise to us that for tiny values of the C parameter we get very low accuracy on both sets, because with such levels of this parameter the SVM tries to look for a large-margin separating hyperplane that is too tolerative of misclassifying points and consequently completely fails to recognize pattern in the data (because it treats the great deal of points as \"exceptions\" that can be misclassified). In other words, the low values of the C parameter introduces low variance but high bias to the model (thus leading to underfitting). The jump in accuracy can also be seen in the case of the linear kernel - the threshold is at 0.01. On a side note, the rbf kernel has an interesting feature that can be seen on the graph, namely it can perfectly overfits with large values of C (and with high valus of gamma). As for the recall and precision, there aren't many differences between kernels - the recall, which is of greater interest for us than precision, follows the accuracy score line.","67330f08":"### Hinge loss","a40d5fba":"**Having only 303 instances, the matter of skillful split of the data into: train, test, validation sets, is of paramount importance. With less training data, our parameter estimates will have greater variance. With less testing data, our performance statistic will have greater variance.**\n\nWith regard to the binary classification, we can estimate how large the validation set should be so that the standard error of the scores based on two outcomes (fit or not fit - such as accuracy, recall, precision) does not exceed 5%.\n","0c6f78b1":"![output-onlinepngtools%20%281%29.png](attachment:output-onlinepngtools%20%281%29.png)","9c474f93":"# 3.1. Train\/test\/validation set","5a2b6c6a":"We're fitting the logistic regression as the first model, since, as said before, it allows us to easily select variables that siginficantly improve performance of the classification (or ones that deteriorate it), which the SVM classifier can't do. If we deem some variable insignificant (given some fixed p-value), it doesn't, of course, necessarily mean that they don't influence the outcome, only that by stating that their coefficients are different than 0 we'll be mistaken with a low probability (p-value). But even if they have some effect on the outcome, it should be nonetheless negligible.  \n\nGenerally, there are two most common approaches to the variable selection in the case of logistic regression (just like linear regression, etc...). The one being the forward selection which looks at each explanatory variable individually and selects the single explanatory variable that \ufb01ts the data the best - this one is included in the equation first. This is repeated until none of the remaining variables will improve performance. Backward selection starts with a model that contains all explanatory variables,and then with each step one variable deemed unsignificant is removed. This continues until all variables in the model are signi\ufb01cant. We'll opt for the latter for the sake of simplicity. To gauge the \"significance\" of the variables, we can look at their p-value. The problem with this approach is that with quite a small sample that we have, we cannot blindly follow *a priori* chosen threshold (say 5%); if we believe that some variable (such as gender, age) influence the outcome, nonetheless, then we'll include them in the equation. Apart from looking at the \"p-value\" one can also pay attention to the decrease\/increase in the value of the likelihood function upon excluding\/including a variable. Both should approaches yield comparable results.","14fdadba":"### Benefits of using the logistic regression for classification purposes","6dd4ceb6":"### Choosing suitable transformation","b8632295":"**Two points worth noting:**\n\n* The target variable is binary instead of having 4 values (as written in the description). It's reasonable to assume that one value indicates a heart disease (let's say, at least one major vessel with >50% narrowing) and the other absence of the heart disease. Now, all we have to do is determine which value correspond to the disease or lack thereof. From our layman's perspective, we know that the probability of a poor heart conditions should surge as age increases (*ceteris paribus*). According to the calculations below, 0 should denote a heart conditions.\n\n* As suggested by some Kaggle's user, there are two instances when 'thal' is equal to 0 (which isn't in accordance to the description), instead there should be NaN.","205a0b17":"Without doubt, the linear kernel (that is the polynomial with the degree equal to 1) is superior over the polynomial kernels with degrees greater than 1. Having said that, let's look at the formulas of these two kernels.\n\nIf $ d=1 \\implies Poly(x,y)=(x^Ty+a)^1 = (x^Ty+a) = Linear(x, y) + a $, where a is another parameter.\n\nTherefore, there's a possibility of tweaking even further the linear kernel by the a coefficient. ","e496213a":"There are reasonable grounds to believe that the **ST_depression** variable is log-normally distributed, the empirical one - the distribution of the sample -, and the theoretical one -  as many processes in biology\/medicine\/chemistry do have such a distribution. It arises from the fact that processes, in those areas, are the statistical realization of the multiplicative product of many independent random variables, rather than additive one (as in the case of the normal distribution).","cffb8dd7":"### Comment to the graphs","65d39532":"At the very beginning, let's remind the elementary properties of the expected value - $E()$ and the variance - $Var()$:\n\n$$ E[X+c] = E[X]+E[c] = E[X] + c $$\n\n$$ Var[X+c] = E[(X+c)^2] -E[X+c]^2 = E[X^2] + 2E[X]c + E[c^2] - E[X]^2 - 2E[X]c - E[c]^2 = E[X^2] - E[X]^2 + c^2 - c^2 = Var(X)  $$\n\nHowever, when it comes to the expected value of logairthm, we can only estimate its value using Taylor expansion around $ x_0=E[x]$.\n\n$$E[log(x)]\u2248log(E[x])\u2212\\frac{Var[x]}{2E[x]^2} $$\n\nWe would like to add such a constant (becauase we want to shift the distribution a little bit so that there are no 0 values) that won't much bias the estimates of the expected value and the variance (which we assume to be the main characteristics of the normal distribution to which through log-transformation we want the variable to transform). In order to achieve it, we'd like to minimize the difference in expected value ($E[log(x)] - E[log(x+b)]$) and variance between the log-transformed variable and the log-transformed variable with a constant shift.\n\n$$ E[log(x)] - E[log(x+b)] = log(E[x]) - log(E[x+b])+\\frac{Var[x+b]}{2E[x+b]^2}-\\frac{Var[x]}{2E[x]^2} = log(E[x]) - log(E[x]+b) + \\frac{Var(x)}{2(E[x]^2+2bE[x]+b^2} - \\frac{Var(x)}{2E[X]^2} = -0.4974 - log(1.0396+b) + \\frac{0.5796}{1.0808+2.3184*b+b^2} $$\n\nProvided that $$ E[x] = 1.0396 $$ $$ Var[x] = 1.1592 $$\n","7c14495f":"**The Support Vector Machine is believed, by the researchers, to be one of the most effective classification algorithm.**","d9d33547":"# 1.1 Loading the data set and libraries","ad2a028f":"The performance of the models doesn't vary with the choice of gamma and coef coefficients. Therefore, we assume that the svm with linear kernel and C equal to 0.1 is the best choice.","07380b74":"The **heart_rate** variable refers to the maximum heart rate achieved during thalium stress test. Intuitively, it seems logical to assume that the higher rate indicates the satisfactory condition of heart, since it managed to increase its rate to such a level during the stress test. In accordance with our presupposition, the maximum heart rate is lower for those diagnosed with heart diseases and the difference is the larger, the younger a patient is. Given this finding, one can also presume that the poor heart condition can be easier noticed\/recognised in younger patients, since the older patients are more prone to have a heart disease. Nevertheless, this is still to be proven.","52068192":"We assume that $$ X \\sim N(\\mu, \\sigma)$$  $$ Y = e^X $$  $$ Y \\sim logN(\\mu, \\sigma) $$. \n\n\n\nThus,\n\n$$ Y*e^c = e^X*e^c = e^{X+c} = A $$\n$$ log(A) \\sim N(\\mu+c, \\sigma)  $$\n\n\n\nIn conclusion, if $ (X+c) \\sim N(\\mu+c, \\sigma)$ then $ (Y*e^c) \\sim logN(\\mu+c, \\sigma)$.\n\nThe results above are in line with our intuition, if we suspect that data are a result of a multiplicative process then adding a constant will be \"corrupting\" the ratio inherent to the data, Therefore, it poses a problem for us since by mutliplication we won't get rid of zeros from the variable. As a last resort, we could  propose to replace the 0 values by the log-normally distributed noise in interval (0, 0.5] that would be censored in a way preventing it be equal to 0, and in addition to that, include a dummy variable, about which we spoke already. Yet, that would be a viable solution if the number of 0's wasn't considerable as it is in our case. , we'll try the square-root transforation.","bc91c14d":"# 2.1. General overview","5b9053e0":"# 3.4 Assessing performance of the models","d9e9633e":"As illustrated by the graph, the smaller a constant we add, the smaller the bias becomes, which in fact is quite intuitive. However, that's only bias on the expected value, the whole distribution will inevitably change. If we want the distribution not to change the shape, we'd need to multiply it by the exponent, which will be clear after considering the following example.","77e6806a":"### Comment to the graphs","102a3f51":"We should also check the correctness of coding in relation to the description.","5fc57855":"Corrections as to the **target** variable and the **thal** variables.","45571e5f":"After having decided for the support vector machine classification, we have to choose two main parameters: C parameter and kernel function. Starting with the latter, hardly ever will you know which would be the most suitable for a given dataset and problem, only on the basis of the theoretical knowledge. Presumably, the linear kernel may sometimes be chosen just on the belief that the points are linearly separable, especially when there are no complicated interaction effects of independent variables on the outcome. Although, intuitively, the linear kernel may appear not to perform well for its lack of flexibility, it turns out to be quite commonly used especially for the fact that it generalizes well with small training sets. \n","27638b5b":"# 2.3. Chest pain","a7315651":"# 2.3. St depression","42b0dcbe":"## Comment to graphs","7281f17a":"The hinge loss is derived as a result of the maxisimisation of the margin between the support vectors constrained by a condition that points must be correctly classified.\nMore formally:\n\n$$ \\max_{w} \\frac{b}{||w||} \\implies \\min_{w} ||w|| $$ subject to $$ y_i(w^Tx_i + b) \u2265 1$$ for $ i = 1, ..., n $ and $ y \\in \\{ -1, 1\\} $ depending on a class.\n\n, where $ w $ is a parameter of $ f(x) = w^Tx + b $, which, when f(x)=0, is a decision boundary, when f(x)=1 or f(x)=-1, corresponds to a support vector.","24452667":"### Evaluating model with common statistics and scores","6bc3610e":"######  Adam Bielski - December 2019","7d81a1f1":"if $y_i = 1 \\land X\\beta \\rightarrow \\infty$  $\\implies \\lim_{X\\beta \\to \\infty} \\ell(\\beta_i) = \\lim_{X\\beta \\to \\infty}  \\log(\\pi_i) = \\log(1) = 0$     \nif $y_i = 0 \\land X\\beta \\rightarrow -\\infty$  $\\implies \\lim_{X\\beta \\to -\\infty} \\ell(\\beta_i) = \\lim_{X\\beta \\to -\\infty}  \\log(1-\\pi_i) = \\log(1) = 0$"}}