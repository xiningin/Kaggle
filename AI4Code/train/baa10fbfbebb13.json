{"cell_type":{"e9b43c66":"code","6764ea7e":"code","26131aa7":"code","fd50f0cb":"code","d134d8b3":"code","ff924213":"code","de466f66":"code","ac4b91f7":"code","cd722624":"code","4d16a528":"code","ba704063":"code","81615244":"code","171f753c":"code","5b5a7717":"code","59958a30":"code","b2b999f0":"code","456ba868":"code","e832cf21":"code","80d0f05f":"code","ac4b5417":"code","ffedb2b7":"code","af7d1877":"code","1896f3e0":"code","952f78b4":"code","249f4960":"code","ac06a41e":"code","0d084728":"code","c2f8600c":"code","23c7a964":"code","508c73e3":"code","cc64e7c1":"code","8a769fec":"code","7418cd07":"code","a4a93146":"code","07f6ae9f":"code","ee6d5d2c":"code","b4cf2ae5":"code","52230637":"code","e58d1b13":"code","cfac5d63":"code","06610b83":"markdown","aaca4215":"markdown","c2dac0bc":"markdown","0aea529e":"markdown","43e64440":"markdown","70983f30":"markdown","8a8ac669":"markdown","d50d8073":"markdown","16832d2b":"markdown","7aa60e7c":"markdown"},"source":{"e9b43c66":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6764ea7e":"data = pd.read_csv('\/kaggle\/input\/santander-customer-transaction-prediction\/train.csv')\ndata.info()","26131aa7":"data.head()","fd50f0cb":"data.drop('ID_code', 1, inplace=True)\ndata.describe()","d134d8b3":"missing = False\nfor i in data.columns:\n    if len(pd.notnull(data[i])==True)!=len(data):\n        print (i, \": \",len(pd.notnull(data[i])))\n        missing = True\nif not missing:\n    print(\"No missing Data in train data.\")","ff924213":"test = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/test.csv')\ntest.head()","de466f66":"test.info()","ac4b91f7":"missing = False\nfor i in test.columns:\n    if len(pd.notnull(test[i])==True)!=len(test):\n        print (i, \": \",len(pd.notnull(test[i])))\n        missing = True\nif not missing:\n    print(\"No missing Data in test.\")","cd722624":"sns.countplot('target', data=data)","4d16a528":"str( np.round(len(data[data['target']==1]) \/(len(data[data['target']==0]) + len(data[data['target']==0]))*100,2)) +' %, ratio of positive class' ","ba704063":"plt.figure(figsize=(15,15))\n\nsns.heatmap(data.corr(), cmap='coolwarm')","81615244":"init_cols = data.columns[1:]","171f753c":"stat_table = pd.DataFrame()\nmeans_0 = []\nmeans_1 = []\nfor i in init_cols:\n    means_0.append(data[data['target']==0][i].mean())\n    means_1.append(data[data['target']==1][i].mean())\n\nstat_table[\"feat\"] = init_cols\nstat_table[\"0\"] = means_0\nstat_table[\"1\"] = means_1\n\nstat_table[\"Class 0\"] = stat_table.apply(lambda x: x[\"0\"]\/(x[\"0\"]+x[\"1\"]), 1)\nstat_table[\"Class 1\"] = stat_table.apply(lambda x: x[\"1\"]\/(x[\"0\"]+x[\"1\"]), 1)\nstat_table = stat_table[[\"feat\", \"Class 0\", \"Class 1\"]]\n\nplt.figure(figsize=(15,15))\nsns.heatmap(stat_table[[\"Class 0\", \"Class 1\"]], cmap='coolwarm')\n\n","5b5a7717":"stat_table[\"var\"] = stat_table.apply(lambda x: np.abs(x[\"Class 0\"] - x[\"Class 1\"])>0.05  , 1)\nstat_table = stat_table[stat_table['var']==True]\n# sns.heatmap(stat_table[[\"Class 0\", \"Class 1\"]], cmap='coolwarm')\n#stat_table","59958a30":"n_rows = 5\nfig , axs = plt.subplots (n_rows, len(stat_table)\/\/n_rows, figsize=(25,15))\ncols = stat_table['feat'].values\nfor i,ax in enumerate(axs.flatten()):\n    \n    sns.distplot(data[data['target']==0][cols[i]] , ax=ax)\n    sns.distplot(data[data['target']==1][cols[i]] , ax=ax)\n    \nplt.tight_layout()\nplt.show()","b2b999f0":"data.head()","456ba868":"from sklearn import mixture\n\nn_rows = 5\nn_components = 2 \nfig , axs = plt.subplots (n_rows, len(stat_table)\/\/n_rows, figsize=(25,15))\ncols = stat_table['feat'].values\nall_data = pd.concat([data, test])\nfor i,ax in enumerate(axs.flatten()):\n    \n    x = all_data[cols[i]].values\n\n    \n    clf = mixture.GaussianMixture(n_components=n_components, )\n    clf.fit(x.reshape(-1,1))\n    samples = clf.sample(len(x))\n    samples_cp1 = samples[0][samples[1] ==0]\n    samples_cp2 = samples[0][samples[1] ==1]\n    sns.distplot(samples_cp1, kde=False, ax=ax)\n    sns.distplot(samples_cp2, kde=False, ax=ax)\n    \n    all_data[cols[i]+'prob'] = clf.predict_proba(x.reshape(-1,1))[:,1]\n        \nplt.tight_layout()\nplt.show()","e832cf21":"n_rows = 30\n\nfig , axs = plt.subplots (n_rows, len(init_cols)\/\/n_rows, figsize=(25,80))\n\nfor i,ax in enumerate(axs.flatten()):\n    tmp = data[[init_cols[i], 'target']].copy()\n    x = tmp[[init_cols[i]]].values\n    bins = np.histogram(x)[1]\n    bins_indexes =  np.digitize(x, bins)\n    tmp['index'] = bins_indexes\n    \n    tmp = tmp.groupby('index')['target'].mean()\n    tmp.plot(ax=ax)\n    ax.set_title(init_cols[i])\n        \nplt.tight_layout()\nplt.show()","80d0f05f":"data = all_data.iloc[:len(data),:].drop('ID_code', 1)\ntest = all_data.iloc[len(data):,:].drop('target', 1)","ac4b5417":"data['average_row'] = data[init_cols] .apply(lambda x: np.round(np.mean(x),5), 1)\ntest['average_row'] = test[init_cols] .apply(lambda x: np.round(np.mean(x),5), 1)","ffedb2b7":"data['std_row'] = data[init_cols] .apply(lambda x: np.round(np.std(x),5), 1)\ntest['std_row'] = test[init_cols] .apply(lambda x: np.round(np.std(x),5), 1)","af7d1877":"fig , axs = plt.subplots (1, 2, figsize=(15,5)) \n\nsns.distplot(data['average_row'] , ax=axs[0], label='Train Data' )\nsns.distplot(test['average_row'] , ax=axs[0], label='Tets Data')\naxs[0].legend()\nsns.distplot(data[data['target']==0]['average_row'] , ax=axs[1], label='Class 0' )\nsns.distplot(data[data['target']==1]['average_row'] , ax=axs[1], label='Class 1')\naxs[1].legend()\nplt.tight_layout()\nplt.show()","1896f3e0":"fig , axs = plt.subplots (1, 2, figsize=(15,5)) \n\nsns.distplot(data['std_row'] , ax=axs[0], label='Train Data' )\nsns.distplot(test['std_row'] , ax=axs[0], label='Tets Data')\naxs[0].legend()\nsns.distplot(data[data['target']==0]['std_row'] , ax=axs[1], label='Class 0' )\nsns.distplot(data[data['target']==1]['std_row'] , ax=axs[1], label='Class 1')\naxs[1].legend()\nplt.tight_layout()\nplt.show()","952f78b4":"fig , axs = plt.subplots (1, 2, figsize=(15,5)) \n\nsns.distplot(np.mean(data[init_cols]) , ax=axs[0], label='Train Data' )\nsns.distplot(np.mean(test[init_cols]) , ax=axs[0], label='Tets Data')\naxs[0].legend()\nsns.distplot(np.mean(data[data['target']==0][init_cols]) , ax=axs[1], label='Class 0' )\nsns.distplot(np.mean(data[data['target']==1][init_cols]) , ax=axs[1], label='Class 1')\naxs[1].legend()\nplt.tight_layout()\nplt.show()","249f4960":"plt.figure(figsize=(35,8))\n\nn_unique_data = {i:len(np.unique(data[i])) for i in init_cols}\nn_unique_data = {k: v for k, v in sorted(n_unique_data.items(), key=lambda item: item[1])}\nsns.barplot(list(n_unique_data.keys()), list(n_unique_data.values()))\nplt.xticks(rotation=90)\nplt.show()\n","ac06a41e":"plt.figure(figsize=(35,8))\n\nn_unique_test = {i:len(np.unique(test[i])) for i in init_cols}\nn_unique_test = {k: v for k, v in sorted(n_unique_test.items(), key=lambda item: item[1])}\nsns.barplot(list(n_unique_test.keys()), list(n_unique_test.values()))\nplt.xticks(rotation=90)\nplt.show()\n","0d084728":"all_data = pd.concat([data, test])\n\nfor i in init_cols:\n    counts = all_data[i].value_counts()\n    all_data[i+\"_count\"] =  all_data[i].map(counts)","c2f8600c":"powers = [2, 3]\nfor p in powers:\n    for i in init_cols:\n        all_data[i+\"_\"+str(p)] = np.power(all_data[i], p)","23c7a964":"data = all_data.iloc[:len(data),:].drop('ID_code', 1)\ntest = all_data.iloc[len(data):,:].drop('target', 1)","508c73e3":"import gc\ndel all_data\ngc.collect()","cc64e7c1":"from sklearn.manifold import TSNE\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nX = subsampled_data[cols].values\ny = subsampled_data['target'].values\n\n\nscaler = MinMaxScaler()\nscaler.fit(X)\nX = scaler.transform(X)\n\nX = TSNE(2, n_jobs=-1).fit_transform(X)\n\ndf = pd.DataFrame()\n\ndf['target'] = y\ndf[['x', 'y']] = X\nsns.scatterplot('x', 'y', 'target', data=df)\n\n","8a769fec":"submission = test.copy()\nsubmission['target'] = 1\nsubmission = submission [['ID_code', 'target']]\n# submission.to_csv('submission.csv', index=False)","7418cd07":"from sklearn.preprocessing import StandardScaler\nfrom collections import Counter\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score, roc_auc_score\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import GridSearchCV","a4a93146":"from imblearn.over_sampling import RandomOverSampler\n\n# cols_train = stat_table['feat']\nX = data.drop('target', 1).values\ny = data['target'].values\nprint ('Original Dataset', Counter(y))\n\nscaler = StandardScaler()\nscaler.fit(X)\nX = scaler.transform(X)\n\n\nnm = RandomOverSampler()\nX, y = nm.fit_resample(X, y)\nprint('Resampled dataset shape %s' % Counter(y))\n\n\ngrid={\"C\":np.logspace(-3,3,4)}\n     #\"class_weight\": ['balanced', None]}# l1 lasso l2 ridge\nlogreg=LogisticRegression()\nlogreg_cv=GridSearchCV(logreg,grid,cv=10)\nlogreg_cv.fit(X,y)\n\nprint(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\nprint(\"accuracy :\",logreg_cv.best_score_)\n\n\nscores_f1 = []\nscores_roc = []\n\nC = logreg_cv.best_params_['C']\n#class_weight = logreg_cv.best_params_['class_weight']\n\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n\nfor train_ix, test_ix in kfold.split(X, y):\n    train_X, test_X = X[train_ix], X[test_ix]\n    train_y, test_y = y[train_ix], y[test_ix]\n    \n    clf = LogisticRegression(C=C, )\n    clf.fit(train_X, train_y)\n    y_hat = clf.predict(train_X)\n    scores_f1.append(f1_score(train_y, y_hat))\n    \n    y_hat = clf.predict_proba(train_X)[:,1]\n    scores_roc.append(roc_auc_score(train_y, y_hat))\n\nprint ('F1 Score: %f, +\/-%f'%(np.mean(scores_f1), np.std(scores_f1)))\nprint ('ROC AUC Score: %f, +\/-%f'%(np.mean(scores_roc), np.std(scores_roc)))\n\nclf = LogisticRegression(C=C)\nclf.fit(X, y)\n    \ncoefs = np.abs(clf.coef_[0])\nindices = np.argsort(coefs)[::-1]\n\nplt.figure()\nplt.title(\"Feature importances (Logistic Regression)\")\nplt.bar(range(15), coefs[indices[:15]],\n       color=\"r\", align=\"center\")\nplt.xticks(range(15), data.columns[indices[:15]], rotation=45, ha='right')\nplt.subplots_adjust(bottom=0.3)","07f6ae9f":"from sklearn.naive_bayes import GaussianNB\n\n\n# cols_train = stat_table['feat']\nX = data.drop('target', 1).values\ny = data['target'].values\nprint ('Original Dataset', Counter(y))\n\nnm = RandomOverSampler()\nX, y = nm.fit_resample(X, y)\n\nscores_f1 = []\nscores_roc = []\n\n\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n\nfor train_ix, test_ix in kfold.split(X, y):\n    train_X, test_X = X[train_ix], X[test_ix]\n    train_y, test_y = y[train_ix], y[test_ix]\n    \n    clf = GaussianNB()\n    clf.fit(train_X, train_y)\n    \n    y_hat = clf.predict(train_X)\n    scores_f1.append(f1_score(train_y, y_hat))\n    \n    y_hat = clf.predict_proba(train_X)[:,1]\n    scores_roc.append(roc_auc_score(train_y, y_hat))\n\nprint ('F1 Score: %f, +\/-%f'%(np.mean(scores_f1), np.std(scores_f1)))\nprint ('ROC AUC Score: %f, +\/-%f'%(np.mean(scores_roc), np.std(scores_roc)))\n\nclf = GaussianNB()\nclf.fit(X, y)\ny_hat = clf.predict_proba(X)[:,1]\nprint ('ROC AUC Score: %f'%(roc_auc_score(y, y_hat)))\n","ee6d5d2c":"import lightgbm as lgb\n\ngbdt_param = {\n    # Core Parameters\n    'objective': 'binary',\n    'boosting': 'gbdt',\n    'learning_rate': 0.01,\n    'num_leaves': 15,\n    'tree_learner': 'serial',\n    'num_threads': 8,\n    'seed': 42,\n    \n    # Learning Control Parameters\n    'max_depth': -1,\n    'min_data_in_leaf': 50,\n    'min_sum_hessian_in_leaf': 10,  \n    'bagging_fraction': 0.6,\n    'bagging_freq': 5,\n    'feature_fraction': 0.05,\n    'lambda_l1': 1.,\n    'bagging_seed': 42,\n    \n    # Others\n    'verbosity ': 1,\n    'boost_from_average': False,\n    'metric': 'auc',\n    #'device' : 'gpu'\n}\n\n\n# cols_train = stat_table['feat']\nX = data.drop('target', 1).values\ny = data['target'].values\nprint ('Original Dataset', Counter(y))\nnm = RandomOverSampler()\nX, y = nm.fit_resample(X, y)\n\nscores_f1 = []\nscores_roc = []\n\n\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n\nfor train_ix, test_ix in kfold.split(X, y):\n    train_X, test_X = X[train_ix], X[test_ix]\n    train_y, test_y = y[train_ix], y[test_ix]\n    evals_result = {}\n    trn_data = lgb.Dataset(train_X, label=train_y)\n    val_data = lgb.Dataset(test_X, label=test_y)\n    \n    lgb_clf = lgb.train(gbdt_param, trn_data, 10000,\n             valid_sets=[trn_data, val_data], early_stopping_rounds=5000,\n                        verbose_eval=1000, evals_result=evals_result)\n","b4cf2ae5":"import lightgbm as lgb\n\ngbdt_param = {\n    # Core Parameters\n    'objective': 'binary',\n    'boosting': 'gbdt',\n    'learning_rate': 0.01,\n    'num_leaves': 15,\n    'tree_learner': 'serial',\n    'num_threads': 8,\n    'seed': 42,\n    \n    # Learning Control Parameters\n    'max_depth': -1,\n    'min_data_in_leaf': 50,\n    'min_sum_hessian_in_leaf': 10,  \n    'bagging_fraction': 0.6,\n    'bagging_freq': 5,\n    'feature_fraction': 0.05,\n    'lambda_l1': 1.,\n    'bagging_seed': 42,\n    \n    # Others\n    'verbosity ': 1,\n    'boost_from_average': False,\n    'metric': 'auc',\n#    'device' : 'gpu'\n}\n\n\n\n# cols_train = stat_table['feat']\nX = data.drop('target', 1).values\ny = data['target'].values\nnm = RandomOverSampler()\nX, y = nm.fit_resample(X, y)\n\ntrn_data = lgb.Dataset(X, label=y)\n\nlgb_clf = lgb.train(gbdt_param, trn_data, 100000)\n","52230637":"submission = test.copy()\nsubmission['target'] = lgb_clf.predict(submission.drop('ID_code', 1).values)\nsubmission = submission [['ID_code', 'target']]\n\nsubmission.to_csv('submission.csv', index=False)","e58d1b13":"submission.info()","cfac5d63":"sns.distplot(submission['target'])","06610b83":"Train and test dets are balanced. Difference in mean between the two classes","aaca4215":"## Base Model ","c2dac0bc":"Totally uncorellated Features","0aea529e":"Always prediticting 1 leads to a score of 50%, it means that the test set is balanced.","43e64440":"Spotted several features where their class means differ from each other","70983f30":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nX = data[cols].values\ny = data['target'].values\n\n\nscaler = MinMaxScaler()\nscaler.fit(X)\nX = scaler.transform(X)\n\npca = PCA(2).fit(X)\nX = pca.transform(X)\n\ndf = pd.DataFrame()\n\ndf['target'] = y\ndf[['x', 'y']] = X\nsns.scatterplot('x', 'y', 'target', data=df)\n\nprint(pca.explained_variance_ratio_)","8a8ac669":"{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 0.32888326498603004, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 0.01363936719149417, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 80, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}","d50d8073":"## Submissions","16832d2b":"### EDA","7aa60e7c":"No Patterns in data"}}