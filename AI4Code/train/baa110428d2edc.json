{"cell_type":{"4982b81e":"code","f2e49002":"code","9fc71ad9":"code","0d237983":"code","5f442312":"code","72683c91":"code","43af9192":"code","3e8f34d8":"code","d16045ba":"code","ca829908":"code","47b16430":"code","cfd879a8":"code","0a346966":"code","10d8f989":"code","c7f45d62":"code","171c63e3":"code","19774edb":"code","a69ab6f7":"code","7f62d65e":"code","010a0bc6":"code","6aebdb5f":"code","168efd12":"code","951bf781":"code","8afc59d2":"code","3d8356d3":"code","7de308e3":"code","2dc0ba40":"code","f7ac641e":"code","2e19701d":"code","2e40bca0":"code","06796e4e":"code","def61a0f":"code","b22eb873":"code","8a880c2e":"code","e7dccb48":"code","5b71c0b7":"code","0c8d3351":"code","8c84eb83":"code","f3bb2755":"markdown","cb917709":"markdown","6463d582":"markdown","ee8afbf6":"markdown","267383a1":"markdown"},"source":{"4982b81e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f2e49002":"raw_csv = pd.read_csv(\"..\/input\/3d-object-detection-for-autonomous-vehicles\/train.csv\")","9fc71ad9":"raw_csv.head()","0d237983":"raw_csv.sample(5)","5f442312":"raw_csv[['PredictionString']]","72683c91":"raw_csv.shape","43af9192":"%matplotlib inline\nimport matplotlib.pyplot as plt","3e8f34d8":"image=plt.imread(\"\/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/train_images\/host-a101_cam3_1242749262599200006.jpeg\", format=None)","d16045ba":"test_image=plt.imread(\"\/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/test_images\/host-a011_cam3_1232841055800995006.jpeg\", format=None)","ca829908":"plt.imshow(image,cmap='gray')","47b16430":"plt.imshow(test_image,cmap='gray')","cfd879a8":"image.shape","0a346966":"test_image.shape","10d8f989":"pip install -U lyft_dataset_sdk","c7f45d62":"pip install -U git+https:\/\/github.com\/lyft\/nuscenes-devkit","171c63e3":"!pip install -U git+https:\/\/github.com\/lyft\/nuscenes-devkit moviepy >> \/dev\/tmp","19774edb":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom IPython.display import HTML","a69ab6f7":"import pdb\nimport cv2\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom pathlib import Path\nfrom matplotlib import pyplot as plt\nfrom mpl_toolkits.mplot3d import axes3d, Axes3D\n\n# Load the SDK\nfrom lyft_dataset_sdk.lyftdataset import LyftDataset, LyftDatasetExplorer, Quaternion, view_points\nfrom lyft_dataset_sdk.utils.data_classes import LidarPointCloud\n\nfrom moviepy.editor import ImageSequenceClip\nfrom tqdm import tqdm_notebook as tqdm\n\n%matplotlib inline","7f62d65e":"# gotta do this for LyftDataset SDK, it expects folders to be named as `images`, `maps`, `lidar`\n\n!ln -s \/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/train_images images\n!ln -s \/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/train_maps maps\n!ln -s \/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/train_lidar lidar\n!ln -s \/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/train_data data","010a0bc6":"lyftdata = LyftDataset(data_path='.', json_path='data\/', verbose=True)","6aebdb5f":"lyftdata.category[0]","168efd12":"car_token = lyftdata.category[0]['token']\ncar_token","951bf781":"lyftdata.get('category',car_token)","8afc59d2":"lyftdata.sample_annotation[0]","3d8356d3":"#sample_annotation\n#sample_annotation refers to any bounding box defining the position of an object seen in a sample. \n#All location data is given with respect to the global coordinate system. Let's examine an example from our sample above.\nmy_annotation_token = my_sample['anns'][16]\nmy_annotation =  my_sample_data.get('sample_annotation', my_annotation_token)\nmy_annotation","7de308e3":"#We can also render an annotation to have a closer look.\nlyftdata.render_annotation(my_annotation_token)","2dc0ba40":"#The attribute record indicates about what was the state of the concerned object when it was annotated\nlyftdata.get('attribute', lyftdata.sample_annotation[0]['attribute_tokens'][0])","f7ac641e":"lyftdata.scene[0]","2e19701d":"my_scene = lyftdata.scene[1]\nmy_sample_token= my_scene['first_sample_token']\nlyftdata.render_sample(my_sample_token)","2e40bca0":"train = pd.read_csv('..\/input\/3d-object-detection-for-autonomous-vehicles\/train.csv')\ntrain.head()","06796e4e":"#We'll be using token0 to as our reference sample token\ntoken0 = train.iloc[0]['Id']\ntoken0","def61a0f":"my_sample = lyftdata.get('sample', my_sample_token)\nmy_sample","b22eb873":"#A useful method is list_sample() which lists all related sample_data keyframes and sample_annotation associated with a sample which we will discuss in detail in the subsequent parts.\n\nlyftdata.list_sample(my_sample['token'])","8a880c2e":"lyftdata.render_sample_3d_interactive(my_sample['token'], render_sample=False)","e7dccb48":"#Instead of looking at camera and lidar data separately, we can also project the lidar pointcloud into camera images\nlyftdata.render_pointcloud_in_image(sample_token = my_sample[\"token\"],\n                                      dot_size = 1,\n                                      camera_channel = 'CAM_FRONT')","5b71c0b7":"#The dataset contains data that is collected from a full sensor suite. Hence, for each snapshot of a scene, we provide references to a family of data that is collected from these sensors.\n\n#We provide a data key to access these:\nmy_sample['data']","0c8d3351":"#Notice that the keys are referring to the different sensors that form our sensor suite. Let's take a look at the metadata of a sample_data taken from CAM_FRONT.\nsensor_channel = 'CAM_FRONT' \nmy_sample_data = lyftdata.get('sample_data', my_sample['data'][sensor_channel])\nmy_sample_data","8c84eb83":" # also try this e.g. with 'LIDAR_TOP'\nsensor_channel = 'LIDAR_TOP'  \nmy_sample_data_lidar = lyftdata.get('sample_data', my_sample['data'][sensor_channel])\nmy_sample_data_lidar","f3bb2755":"Each scene provides the first sample token and the last sample token, we can see there are 126 sample records (nbr_samples) in between these two.","cb917709":"**Scenes**\nA scene is a 25-45s long sequence of consecutive frames extracted from a log. A frame (also called a sample) is a collection of sensor outputs (images, lidar points) at a given timestamp\n\nscene {\n   \"token\":                   <str> -- Unique record identifier.\n   \"name\":                    <str> -- Short string identifier.\n   \"description\":             <str> -- Longer description of the scene.\n   \"log_token\":               <str> -- Foreign key. Points to log from where the data was extracted.\n   \"nbr_samples\":             <int> -- Number of samples in this scene.\n   \"first_sample_token\":      <str> -- Foreign key. Points to the first sample in scene.\n   \"last_sample_token\":       <str> -- Foreign key. Points to the last sample in scene.\n}","6463d582":"**3D interactive visualization of a sample**\n\n\nWe can visualize a sample interactively using lyft SDK's inbuilt render_sample_3d_interactive functionality","ee8afbf6":"**Sample**\nA sample is defined as an annotated keyframe of a scene at a given timestamp. A sample is data collected at (approximately) the same timestamp as part of a single LIDAR sweep.\n\nsample {\n   \"token\":                   <str> -- Unique record identifier.\n   \"timestamp\":               <int> -- Unix time stamp.\n   \"scene_token\":             <str> -- Foreign key pointing to the scene.\n   \"next\":                    <str> -- Foreign key. Sample that follows this in time. Empty if end of scene.\n   \"prev\":                    <str> -- Foreign key. Sample that precedes this in time. Empty if start of scene.\n}\n    \n  Remember, token0 is a token to a particular sample record in sample data table (sample.json), let's look at that sample using lyft SDK's inbuilt .get function","267383a1":"train dataframe's Id column contains tokens (unique identifiers) of train sample records present in sample table and PredictionString contains corresponding ground truth annotations (bounding boxes) for different object categories"}}