{"cell_type":{"6581e505":"code","408bd0cb":"code","1fff997b":"code","0bb19f22":"code","515e74f9":"code","72b360c6":"code","c004e6df":"code","c358ae0b":"code","d8fa26f3":"code","1d74ecb2":"code","9fcf548b":"code","35f459dc":"code","b76a8c59":"code","2fcee6c6":"code","2b55a6f9":"code","28617e2b":"markdown","da8df080":"markdown","0735f5c8":"markdown","4ee145fc":"markdown","b94fcd2a":"markdown","a7e01d8e":"markdown","bcba5705":"markdown","24fc99f0":"markdown","9ac89160":"markdown","be7f6dbb":"markdown","ee2fcb52":"markdown","82678c15":"markdown","b16bf24d":"markdown"},"source":{"6581e505":"import pandas as pd\nimport numpy as np\ntest = pd.read_csv(\"..\/input\/cat-in-the-dat-ii\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/cat-in-the-dat-ii\/train.csv\")\ntest_encoded = pd.read_csv(\"..\/input\/catindat2encoded\/test_encoded.csv\")\ntrain_encoded = pd.read_csv(\"..\/input\/catindat2encoded\/train_encoded.csv\")\ntest_id = pd.read_csv(\"..\/input\/cat-in-the-dat-ii\/sample_submission.csv\")['id']","408bd0cb":"target = train['target']\ntest.drop(['id'], axis =1 , inplace = True)\ntrain.drop(['id','target'], axis =1 , inplace = True)\ndata = pd.concat([train,test])\nfor i in data.columns:\n    data[i] = data[i].fillna(data[i].mode()[0])","1fff997b":"def bin_encoder(integer):\n    if integer == 0 or integer == 'N' or integer == 'F':\n        return False\n    elif integer ==1 or integer == 'Y' or integer== 'T' :\n        return True\nbin = ['bin_0','bin_1','bin_2','bin_3','bin_4']\nfor i in bin:\n    data[i] = data[i].apply(bin_encoder)","0bb19f22":"from sklearn.preprocessing import LabelEncoder\nfor i in range(0,5):\n    l_encoder = LabelEncoder()\n    key = \"nom_\" + str(i)\n    data[key] = l_encoder.fit_transform(data[key].fillna(\"NULL\").astype(str).values) ","515e74f9":"from category_encoders import TargetEncoder\nfor i in range(5,10):\n    target_encoder = TargetEncoder()\n    key = \"nom_\" + str(i)\n    train_te = target_encoder.fit_transform(train[key].fillna(\"NULL\").astype(str).values,target).values\n    test_te = target_encoder.transform(test[key].fillna(\"NULL\").astype(str).values).values\n    data[key] =np.concatenate((train_te,test_te), axis=0)","72b360c6":"ordinal = [\n    [1.0, 2.0, 3.0],\n    ['Novice', 'Contributor', 'Expert', 'Master', 'Grandmaster'],\n    ['Freezing', 'Cold', 'Warm', 'Hot', 'Boiling Hot', 'Lava Hot']\n]\n\nfor i in range(1, 3):\n    ordinal_dict = {i : j for j, i in enumerate(ordinal[i])}\n    key = \"ord_\" + str(i)\n    data[key] = (data[key]).map(ordinal_dict)","c004e6df":"from sklearn.preprocessing import OrdinalEncoder\noencoder = OrdinalEncoder(dtype=np.int16)\nfor enc in [\"ord_3\",\"ord_4\",\"ord_5\"]:\n    data[enc] = oencoder.fit_transform(np.array(data[enc]).reshape(-1,1))","c358ae0b":"data['month_sin'] = np.sin((data['month'] - 1) * (2.0 * np.pi \/ 12))\ndata['month_cos'] = np.cos((data['month'] - 1) * (2.0 * np.pi \/ 12))\n\ndata['day_sin'] = np.sin((data['day'] - 1) * (2.0 * np.pi \/ 7))\ndata['day_cos'] = np.cos((data['day'] - 1) * (2.0 * np.pi \/ 7))","d8fa26f3":"from sklearn.preprocessing import MinMaxScaler\nscaler= MinMaxScaler()\ndata = scaler.fit_transform(data)\ndata = pd.DataFrame(data)","1d74ecb2":"train = pd.concat([data[:600000],target],axis =1)\ntrain.to_csv(\"train_encoded.csv\",index = False)\ntest = data[600000:]\ndata[600000:].to_csv(\"test_encoded.csv\",index = False)","9fcf548b":"from tqdm import tqdm\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nclass average_stacking(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self,models):\n        self.models = models\n    def fit(self, x,y):\n        self.model_clones = [clone(x) for x in self.models]\n        \n        for model in tqdm(self.model_clones):\n            model.fit(x,y)\n        return self\n    def predict(self, x):\n        preds = np.column_stack([\n            model.predict(x) for model in tqdm(self.model_clones)\n        ])\n        return np.mean(preds, axis = 1)","35f459dc":"from sklearn.ensemble import GradientBoostingRegressor, AdaBoostClassifier\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn import linear_model\nimport xgboost as xgb\nimport lightgbm as lgb","b76a8c59":"glm = linear_model.LogisticRegression( random_state=1, solver='lbfgs', max_iter=2020, fit_intercept=True, penalty='none', verbose=0)\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=920,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nGBoost = GradientBoostingRegressor(n_estimators=1500, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   verbose = True,\n                                   loss='huber', random_state =5)\nmodel_ada = AdaBoostClassifier(n_estimators= 2200, learning_rate= 0.75)","2fcee6c6":"averaged_models = average_stacking(models = (model_lgb, glm, GBoost, model_ada))\ntrain_y = train_encoded['target']\ntrain_encoded.drop(['target'], axis =1 , inplace = True)\n# averaged_models.fit(train_encoded, train_y)\n# avg_pred = averaged_models.predict(test_encoded)","2b55a6f9":"# pd.DataFrame({'id': test_id, 'target': avg_preds}).to_csv('submission.csv', index=False)","28617e2b":"* The alphabetical ordinal variables are encoded using Ordinal Encoder","da8df080":"* Scaling data to get normalized values","0735f5c8":"* ordinal variables 0 to 3 were mapped using dicts and assigned values corresponding to their interpreted values","4ee145fc":"* Binary data to boolean values using the function below","b94fcd2a":"* Exporting our newly made dataset to csv","a7e01d8e":"**Feature generation from cyclic variables**\n* After reading a few notebooks I thought we should generate a few more features to extract information from day and month","bcba5705":"* nom_5 to nom_9 were target encoded due their high cardinality value","24fc99f0":"* nom_0 to nom_4 were Label Encoded\n","9ac89160":"# 1. Data\nI will be using a custom dataset which I created. This contains the contest data which is cleaned, preprocessed and encoded. The different types of encoding used are as follows:\n","be7f6dbb":"# \ud83c\udf86Encode like there's no tomorrow\n\n**This kernel will be a combination of a variety of encoding techniques, pipelines and models with the aim of achieving a better score**\nI have exported the encoded dataset which I made in this notebook, feel free to use it in your pipelines and dont forget to credit this notebook. \ud83d\ude09\nHope this notebook is as fun to read as it was for me to write.","ee2fcb52":"### <span style=\"color:red\">Upvote and share if this notebook helped you in any way<\/span> \ud83d\ude01","82678c15":"**Borrowing a bit of code from my [previous notebook](https:\/\/www.kaggle.com\/amoghjrules\/intro-to-stacking-averaging-base-models)**","b16bf24d":"* Empty data was filled with mode for respective columns"}}