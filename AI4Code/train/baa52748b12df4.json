{"cell_type":{"5ca71ebb":"code","dcbbce37":"code","847616cc":"code","586489a6":"code","42496eaf":"code","cbee69d3":"code","e7bbdf4f":"code","02385ae8":"code","4d16b217":"code","06160acb":"code","30b1b15e":"code","08978edb":"code","6b82336b":"code","0252213c":"code","b79f4e51":"code","e3bdb803":"code","c2f4e366":"code","48977d81":"code","32aeff03":"code","de77998f":"code","0a8100cf":"code","a56d0d4c":"code","5065ea71":"code","56b74b97":"code","3b6b10ac":"code","11d2946f":"code","c9700fcc":"code","070db08c":"code","831f9170":"code","4ec3a63f":"code","0f9ea150":"code","9cc819dd":"code","140c7a8c":"code","3e9744ab":"code","f347ffcf":"code","850d0405":"code","a9444fef":"code","8f6421c0":"code","2663de7f":"code","fc3a5104":"code","541e71b2":"code","cb7ee0c9":"code","0ef9a26c":"code","c55d2d3d":"code","a848d6ec":"code","fff9aea5":"code","f63b876d":"code","77794e14":"code","73d11b86":"code","e723b7a3":"code","a78f1c04":"code","1c08f8fa":"code","9c418d29":"code","058860fc":"code","c5bcff6a":"code","832c1b72":"code","ee71393d":"code","526153b5":"code","aa6dac3d":"code","37f653aa":"code","888e2af5":"code","384d3eb8":"code","5f775f43":"code","50ed51a2":"code","dfc12b78":"code","7ab99246":"code","32751e32":"code","19043d88":"code","b81a9e4e":"markdown","47e02791":"markdown","49280f44":"markdown","04e4a014":"markdown","4ee8fb82":"markdown","57c4305c":"markdown","90c2c4b2":"markdown","9ec0ee6a":"markdown","c7ba9aa9":"markdown","fba8dc97":"markdown","6d0407de":"markdown","3d4564d8":"markdown","b7bd2ae1":"markdown","3d02f2be":"markdown","62e76037":"markdown","e8c4397a":"markdown","62f582b5":"markdown","238fb5e2":"markdown","aac9d229":"markdown","3e2bee2a":"markdown","a68c0a21":"markdown","8997aefb":"markdown","69424102":"markdown","5fabf0a9":"markdown","5599bab2":"markdown","32e947ce":"markdown","913435ad":"markdown","bf41719b":"markdown"},"source":{"5ca71ebb":"import fastai; fastai.__version__","dcbbce37":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport pandas as pd\nimport re\nimport numpy as np\nfrom fastai import * #\u00a0notebook was run with fastai 1.0.51\nfrom fastai.text import *\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\nfrom sklearn.metrics import classification_report\n\npd.set_option('display.max_colwidth', -1)\n\n# by setting a random seed number, we'll ensure that when doing language model, same training-validation split is used.\nnp.random.seed(42) \n","847616cc":"path = Path('..\/input')","586489a6":"df = pd.read_csv(path\/\"trainset.csv\")\ndf_test = pd.read_csv(path\/\"testset_w_refs.csv\")\ndf_dev = pd.read_csv(path\/\"devset.csv\")\nprint(df.shape)\nprint(df_dev.shape)\nprint(df_test.shape)\ndf.head()","42496eaf":"import unicodedata\ndef strip_accents(s):\n   return ''.join(c for c in unicodedata.normalize('NFD', s)\n                  if unicodedata.category(c) != 'Mn')","cbee69d3":"def delexicalize(attribute,value,new_value,new_row,row):\n    new_row[\"ref\"] = re.sub(value,new_value,new_row[\"ref\"])\n    new_row[\"ref\"] = re.sub(value.lower(),new_value.lower(),new_row[\"ref\"])\n    new_row[\"ref\"] = re.sub(strip_accents(value.lower()),new_value.lower(),new_row[\"ref\"])\n    new_row[\"ref\"] = re.sub(strip_accents(value),new_value,new_row[\"ref\"])\n    value0=value[0]+value[1:].lower()\n    new_row[\"ref\"] = re.sub(value0,new_value,new_row[\"ref\"])\n    new_row[\"ref\"] = re.sub(strip_accents(value0),new_value,new_row[\"ref\"])\n    value0=value[0].lower()+value[1:]\n    new_row[\"ref\"] = re.sub(value0,new_value,new_row[\"ref\"])\n    new_row[\"ref\"] = re.sub(strip_accents(value0),new_value,new_row[\"ref\"])\n    return new_row","e7bbdf4f":"from nltk import sent_tokenize\ndef process_features(df):\n    rows = []\n    for i,row in df.iterrows():\n        row0 = row.to_dict()\n        row0[\"old_mr\"] = row0[\"mr\"]\n        row0[\"mr\"] = re.sub(\"  +\",\" \",row0[\"mr\"])\n        name = re.sub(r\"^.*name\\[([^\\]]+)\\].*$\",r\"\\1\",row0[\"mr\"].strip())\n        near = re.sub(r\"^.*near\\[([^\\]]+)\\].*$\",r\"\\1\",row0[\"mr\"].strip())\n        name = re.sub(\"  +\",\" \",name)\n        near = re.sub(\"  +\",\" \",near)\n        row0 = delexicalize(\"name\",name,\"XXX\",row0,row)\n        row0 = delexicalize(\"near\",near,\"YYY\",row0,row)\n        row0[\"mr\"] = re.sub(r\"name\\[[^\\]]+\\](, *| *$)\",\"\",row0[\"mr\"].strip())\n        row0[\"mr\"] = re.sub(r\"near\\[[^\\]]+\\](, *| *$)\",r\"near[yes]\\1\",row0[\"mr\"].strip())\n        row0[\"mr\"] = re.sub(r\", *$\",\"\",row0[\"mr\"].strip())\n        row0[\"mr\"] = re.sub(r\" *, *\",\",\",row0[\"mr\"].strip())\n        row0[\"mr\"] = row0[\"mr\"].strip()\n        if row[\"ref\"]==row0[\"ref\"]:\n            continue\n        rows.append(row0)\n    return pd.DataFrame(rows)","02385ae8":"df=process_features(df)\ndf_dev=process_features(df_dev)\ndf_test=process_features(df_test)\nprint(df.shape)\nprint(df_dev.shape)\nprint(df_test.shape)\ndf.head()","4d16b217":"from nltk.tokenize import sent_tokenize\nrows=[]\nfor i,row in df.iterrows():\n    mrs = row[\"mr\"].split(\",\")\n    sents = sent_tokenize(row[\"ref\"])\n    for mr in mrs:\n        row[mr]=1\n        if not mr.startswith(\"near\") and not mr.startswith(\"name\"):\n            feature_name = re.sub(r\"^([^\\[]+)\\[.*$\",r\"\\1\",mr.strip())\n            row[feature_name]=1\n    row[\"num_mrs\"]=len(mrs)\n    row[\"num_sents\"]=len(sents)\n    rows.append(row)","06160acb":"df_stats = pd.DataFrame(rows)\ndf_stats = df_stats.fillna(0)\ndf_stats.head(5)","30b1b15e":"stats = {}\ndf_sample = df_stats\nrows =[]\nfor col in df_sample.columns:\n    row={}\n    if df_stats[col].dtype == np.float64:\n        if \"[\" not in col:\n            row[\"feature\"]=\"_\"+col\n        else:\n            row[\"feature\"]=col\n        row[\"num\"]=df_sample[col].sum()\n        row[\"mean\"]=df_sample[col].mean()\n        row[\"std\"]=df_sample[col].std()\n        rows.append(row)\n    elif df_sample[col].dtype == np.int64:\n        row[\"feature\"]=\"__\"+col\n        row[\"num_1\"] = (df_sample.loc[df_sample[col]==1]).shape[0]\n        row[\"num\"]=df_sample[col].sum()\n        row[\"mean\"]=df_sample[col].mean()\n        row[\"min\"]=df_sample[col].min()\n        row[\"max\"]=df_sample[col].max()\n        row[\"std\"]=df_sample[col].std()\n        row[\"median\"]=df_sample[col].median()\n        rows.append(row)\ndf_stats0 = pd.DataFrame(rows)\ndf_stats0 = df_stats0.sort_values(by=\"feature\")\ndf_stats0","08978edb":"df_all = pd.concat([df, df_dev,df_test], ignore_index=True)\ndf_all.shape","6b82336b":"bs = 56","0252213c":"df_all.sample(5)","b79f4e51":"data_lm = (TextList.from_df(df_all, \".\", cols='ref')\n                .split_by_rand_pct(0.1)\n                .label_for_lm()\n                .databunch(bs=bs))","e3bdb803":"data_lm.export('data_lm.pkl')","c2f4e366":"data_lm.show_batch()","48977d81":"learn_lm = language_model_learner(data_lm, arch=AWD_LSTM, drop_mult=1e-7)","32aeff03":"learn_lm.freeze()","de77998f":"learn_lm.lr_find()\nlearn_lm.recorder.plot()","0a8100cf":"learn_lm.fit_one_cycle(1, 5e-02, moms=(0.8,0.7))","a56d0d4c":"learn_lm.unfreeze()","5065ea71":"learn_lm.lr_find()\nlearn_lm.recorder.plot(suggestion=True)","56b74b97":"learn_lm.fit_one_cycle(4, 1e-03, moms=(0.8,0.7),wd=0.3)","3b6b10ac":"learn_lm.recorder.plot_losses()","11d2946f":"learn_lm.save('fine_tuned')\nlearn_lm.save_encoder('fine_tuned_enc')","c9700fcc":"TEXT = \"Near\"\nN_WORDS = 50\nN_TEXTS = 2","070db08c":"print(\"\\n\".join(learn_lm.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_TEXTS)))","831f9170":"with open('vocab.pkl', 'wb') as f:\n    pickle.dump(data_lm.vocab, f)","4ec3a63f":"bs = 56","0f9ea150":"def precision(log_preds, targs, thresh=0.5, epsilon=1e-8):\n    pred_pos = (log_preds > thresh).float()\n    tpos = torch.mul((targs == pred_pos).float(), targs.float())\n    return (tpos.sum()\/(pred_pos.sum() + epsilon))#.item()","9cc819dd":"def recall(log_preds, targs, thresh=0.5, epsilon=1e-8):\n    pred_pos = (log_preds > thresh).float()\n    tpos = torch.mul((targs == pred_pos).float(), targs.float())\n    return (tpos.sum()\/(targs.sum() + epsilon))","140c7a8c":"data_clas = TextClasDataBunch.from_df(\".\", train_df=df, valid_df=df_dev, \n                                  vocab=data_lm.vocab, \n                                  text_cols='ref', \n                                  label_cols='mr',\n                                  label_delim=',',\n                                  bs=bs)","3e9744ab":"data_clas.show_batch()","f347ffcf":"print(len(data_clas.valid_ds.classes))\ndata_clas.valid_ds.classes","850d0405":"learn = text_classifier_learner(data_clas, arch=AWD_LSTM,drop_mult=1e-7)\nlearn.metrics = [accuracy_thresh, precision, recall]\nlearn.load_encoder('fine_tuned_enc')","a9444fef":"learn.freeze()","8f6421c0":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","2663de7f":"learn.fit_one_cycle(10, 6.92E-02, moms=(0.8,0.7),wd=1e-6)","fc3a5104":"learn.recorder.plot_losses()","541e71b2":"learn.save(\"stage1\")","cb7ee0c9":"learn = text_classifier_learner(data_clas, arch=AWD_LSTM,drop_mult=1e-7)\nlearn = learn.load(\"stage1\")\nlearn.metrics = [accuracy_thresh, precision, recall]","0ef9a26c":"learn.unfreeze()","c55d2d3d":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","a848d6ec":"learn.fit_one_cycle(5, slice(1E-03\/(2.6**4),1E-03), moms=(0.8,0.7), wd=0.5)","fff9aea5":"learn.save(\"classifier_model\",return_path=True, with_opt=True)","f63b876d":"from IPython.display import FileLinks\nFileLinks('.') # input argument is specified folder","77794e14":"learn.recorder.plot_losses()","73d11b86":"with open('vocab.pkl', 'rb') as fp:\n    vocab = pickle.load(fp)","e723b7a3":"def make_predictions(path,voc,model_name,df_train,df_valid,vocab,bs):\n    data_clas = TextClasDataBunch.from_df(path, train_df=df_train, valid_df=df_valid, \n                                          vocab=voc,\n                                      text_cols='ref', \n                                      label_cols='mr',\n                                      label_delim=',',\n                                      bs=bs)\n    learn = text_classifier_learner(data_clas, arch=AWD_LSTM)\n    learn.load(model_name)\n    learn.data = data_clas\n    preds, y = learn.get_preds(ordered=True)\n    return learn,preds,y","a78f1c04":"path=\".\"","1c08f8fa":"learn_train,preds_train,y_train = make_predictions(path,vocab,\"classifier_model\",df,df,None,bs)\nlearn_valid,preds_valid,y_valid = make_predictions(path,vocab,\"classifier_model\",df,df_dev,None,bs)\nlearn_valid,preds_test,y_test = make_predictions(path,vocab,\"classifier_model\",df,df_test,None,bs)","9c418d29":"f1_train = f1_score(y_train, preds_train>0.5, average='micro')\nf1_valid = f1_score(y_valid, preds_valid>0.5, average='micro')\nf1_test = f1_score(y_test, preds_test>0.5, average='micro')\nf1_train,f1_valid,f1_test","058860fc":"y_true_train = y_train.numpy()\nscores_train = preds_train.numpy()\nreport = classification_report(y_true_train, scores_train>0.5, target_names=data_clas.valid_ds.classes)\nprint(report)","c5bcff6a":"y_true_valid = y_valid.numpy()\nscores_valid = preds_valid.numpy()\nreport = classification_report(y_true_valid, scores_valid>0.5, target_names=data_clas.valid_ds.classes)\nprint(report)","832c1b72":"y_true_test = y_test.numpy()\nscores_test = preds_test.numpy()\nreport = classification_report(y_true_test, scores_test>0.5, target_names=data_clas.valid_ds.classes)\nprint(report)","ee71393d":"learn,preds,y = make_predictions(path,vocab,\"classifier_model\",df,df,None,bs)","526153b5":"f1_score(y, preds>0.5, average='micro')","aa6dac3d":"def set_row_metrics(row,true_mrs,predicted_mrs):\n        tp=0\n        fp=0\n        tn=0\n        fn=0\n        for mr in predicted_mrs:\n            if mr in true_mrs:\n                tp+=1\n            else:\n                fp+=1\n        for mr in true_mrs:\n            if mr not in predicted_mrs:\n                fn+=1\n            else:\n                tn+=1\n        row[\"tp\"]=tp\n        row[\"fp\"]=fp\n        row[\"fn\"]=fn\n        row[\"tn\"]=tn\n        row[\"precision\"]=0\n        row[\"recall\"]=0\n        row[\"fscore\"]=0\n        if tp+fp>0:\n            row[\"precision\"]=float(tp)\/(tp+fp)\n        if tp+fn>0:\n            row[\"recall\"]=float(tp)\/(tp+fn)\n        if row[\"precision\"]+row[\"recall\"]>0:\n            row[\"fscore\"]= 2*((row[\"precision\"]*row[\"recall\"])\/(row[\"precision\"]+row[\"recall\"]))\n        return row","37f653aa":"def set_labels(df,preds,classes):\n    preds_true = (preds>0.5)\n    counter=0\n    rows=[]\n    for i,row in df.iterrows():\n        row_preds = preds[counter]\n        indices = [j for j in range(len(preds_true[counter])) if preds_true[counter][j]==True]\n        row_labels = [classes[j] for j in indices]\n        row[\"mr_predict\"]=\",\".join(sorted(row_labels))\n        predicted_mrs = row[\"mr\"].split(\",\")\n        row[\"mr\"]=\",\".join(sorted(predicted_mrs))\n        row = set_row_metrics(row,row_labels,predicted_mrs)\n        rows.append(row)\n        counter=counter+1\n    return pd.DataFrame(rows)","888e2af5":"learn.data.valid_ds.classes","384d3eb8":"df_preds = set_labels(df,preds,learn.data.valid_ds.classes)","5f775f43":"df_preds[df_preds[\"fscore\"]==1].shape[0]\/df_preds.shape[0]","50ed51a2":"df_preds = df_preds.sort_values(by=[\"fscore\",\"precision\",\"recall\"],ascending=True)","dfc12b78":"df_preds.to_csv(\"training_preds.csv\",sep=\"\\t\",index=False)","7ab99246":"df_preds[df_preds[\"fscore\"]<1][df_preds[\"fscore\"]>0].head(5)","32751e32":"def convert_to_dict(features):\n    d = {}\n    features = features.split(\",\")\n    for f in features:\n        name = (re.sub(r\"^([^\\[]+)\\[([^\\]]+)\\]$\",r\"\\1\",f)).strip()\n        value = (re.sub(r\"^([^\\[]+)\\[([^\\]]+)\\]$\",r\"\\2\",f)).strip()\n        if name not in d.keys():\n            d[name]=set()\n        d[name].add(value)\n    return d","19043d88":"rows=[]\nfor i,row in df_preds.iterrows():\n    row0=row\n    mrs = convert_to_dict(row[\"mr\"])\n    mrs_predict = convert_to_dict(row[\"mr_predict\"])\n    missing=0\n    mismatch=0\n    added=0\n    for feature in mrs.keys():\n        if feature not in mrs_predict.keys():\n            missing+=1\n        else:\n            for value in mrs[feature]:\n                if value not in mrs_predict[feature]:\n                    mismatch+=1\n                    break\n    for feature in mrs_predict.keys():\n        if feature not in mrs.keys():\n            added+=1\n    row0[\"missing\"]=missing\n    row0[\"mismatch\"]=mismatch\n    row0[\"added\"]=added\n    rows.append(row0)\n    \npd_preds0 = pd.DataFrame(rows)\npd_preds0.sample(5)","b81a9e4e":"According to the e2e NLG challenge organizers (see [that paper](https:\/\/aclweb.org\/anthology\/W18-6539)), the NLG model with best results is achieved by [this system](https:\/\/aclweb.org\/anthology\/N18-1014). To improve their results, the authors of that approach use reranking of the NLG outputs by looking at slots in the MR that are missing in the output text (false negative) and slots in the output text that are missing in the input MR (false positive). \n\nThis is done by a slot aligner that aligns each sentence in the output with a subset of MR slots.  This slot alignment approach uses heuristics based on a gazetteer, a set of hand written rules and access to Wordnet to augment the gazetteer with related terms (e.g., \"italian\" and \"pasta\"). The slot alignment is also used by the authors to generate new data by taking individual sentences and their aligned slots as new input pairs.\n\nIn this notebook, we align the texts with the MRs, not individual sentences. That is left for future work.","47e02791":"## The dataset","49280f44":"The approach in this notebook relies on [fastai](https:\/\/www.fast.ai\/) approach and library for classification using transfer-based NLP and gets an **f-score of 89-90% over all the labels on the test set**.\n\nThe texts were delexicalized for venue names and this raised the test set f-score (from 85% to 90%) since we don't want labels to depend on names of venues. On the other hand, types of venue `restaurant` have a very low f-score whilst being high in both training and validation set. It is not clear why this happens.\n\nQualitative analysis of texts and MR pairs with most incorrect predictions in the training set reveals 3 main issues with the dataset: some MRs are not verbalized, some texts verbalize different MRs, and some MRs are so close in meaning that they are undistinguishable in the text.\n\nNote: This notebook is based on [that github](https:\/\/github.com\/krasing\/multilabel-ULMFiT\/blob\/master\/asrs_new-factors-clean.ipynb) which does multi-label classification.","04e4a014":"In this notebook, we train a  classifier to label a text with the MR attribute-values. So given the text above as input, the classifier would tell us that it is `eatType[coffee shop]`, `food[French]`, `priceRange[moderate]`, `customerRating[3\/5]`, `kidsFriendly[yes]`. `area[riverside]` should not be output as it is not verbalized (the human writers sometimes omitted some information).","4ee8fb82":"We merge training examples with their predictions in the same dataframe and order rows in ascending order for f-score so we can view the worst predictions first:","57c4305c":"## Classification","90c2c4b2":"## Aligning a text with its meaning representation in Natural Language Generation: what for?","9ec0ee6a":"For fine tuning the language model, we use training, validation and test set, as we're not using the labels.","c7ba9aa9":"## Quantitative evaluation","fba8dc97":"We get the generative language model to work, giving it some beginning of text for it to complete:","6d0407de":"## Getting the data","3d4564d8":"Next, we unfreeze the whole model and train some more. I did not find that unfreezing the last 2 layers first made any improvement.","b7bd2ae1":"## Qualitative evaluation","3d02f2be":"For each alignment, we can mark how many of the original it is missing, for how many it is a mismatch (original says Italian food and alignment says French food), and for how many the alignment added information:","62e76037":"The [E2E NLG challenge dataset](https:\/\/github.com\/tuetschek\/e2e-dataset) consists of 50k pairs of natural language texts (NLs) and their meaning representations (MRs). For example:\n\n- MR:\n\n```\nname[The Eagle],\neatType[coffee shop],\nfood[French],\npriceRange[moderate],\ncustomerRating[3\/5],\narea[riverside],\nkidsFriendly[yes],\nnear[Burger King]\n```\n\n- NL:\n\n\n```\nThe three star coffee shop, The Eagle, gives families a mid-priced dining experience featuring a variety of wines and cheeses. Find The Eagle near Burger King.\n```\n\nThis example is taken from the page on the [E2E NLG challenge](http:\/\/www.macs.hw.ac.uk\/InteractionLab\/E2E\/). The objective of this challenge is to generate a text given its meaning representation. ","e8c4397a":"We remove name and near from the features as they are open ended + they are normally a strict match.","62f582b5":"## Fine tuning the language model","238fb5e2":"We sort dataframe containing real MRs, predicted MRs and corresponding texts, together with fscore and other metrics, in ascending order of fscore, so as to do some error analysis:","aac9d229":"## Objective of this notebook","3e2bee2a":"## Results and discussion","a68c0a21":"The f-score with test set is 90%. For some reason the classifier is no good at finding `eatType[restaurant]` mentions, with an f-score of 1%. \n\nWith the validation set, there are no instances of `eatType[restaurant]` and `eatType[pub]` or `food[Fast food],food[French],food[Indian],food[Italian],food[Japanese]`. \n\nWith the training set, the f-score is 94% with high f-scores on individual labels.\n\nWhy is detection of eating venue type so poor in the test set?","8997aefb":"## Some statistics about the data","69424102":"We save the vocabulary, to use when doing predictions:","5fabf0a9":"We find that over two thirds of training instances are a perfect match:","5599bab2":"Given the e2e NLG challenge dataset, we want to detect content given text.","32e947ce":"We can also look into more details at each feature performance:","913435ad":"We remove the name feature-value as it is in every MR and string-based. We replace the name feature value with `near[yes]` to indicate when it is verbalized.","bf41719b":"When looking at the texts with low f-score prediction, the following problems appear:\n\n1. Some MRs are altogether incorrectly verbalized or sloppily verbalized. For example we have the following true MRs:\n```\narea[riverside],customer rating[1 out of 5],food[Fast food],near[yes],priceRange[high]\t\n```\nIt gets verbalized as:\n```\nAlimentum is a one star restaurant near the Yippee Noodle Bar\n```\nSo `fast food` has been verbalized as `restaurant` which is technically true (we can say \"a fast food restaurant\").\n\n2. Some MRs are not verbalized like in the example above where the fact that it is by the riverside is not mentioned.\n3. There is a problem in that quantiative customer rating and price range like `customerRating[1 to 5]` are sometimes verbalized quantitatively, which is then analysed as `customerRating[low]`. So it seems that one can generate from those MRs but for evaluation, more flexible MRs should be considered: if text says 'low customer rating' then MR can either be `customerRating[low]` or `customerRating[1 to 5]`. For example we have the following true MRs:\n```\neatType[restaurant],familyFriendly[yes],food[Japanese],priceRange[less than \u00a320]\n```\nIt gets verbalized as:\n```\nLoch Fyne is a cheap family friendly Japanese restaurant.\n```\nwhich gets classified as:\n````\neatType[restaurant],familyFriendly[yes],food[Japanese],priceRange[cheap]\n```\n"}}