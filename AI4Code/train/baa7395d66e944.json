{"cell_type":{"7164273c":"code","348df35d":"code","6a9f2616":"code","86a613cb":"code","49131eb4":"code","69a9d12e":"code","ac946fce":"code","0410c5cf":"markdown","6c3efca8":"markdown","d6e0cb66":"markdown","bfa56682":"markdown","c85bcc7d":"markdown","ba45e3a9":"markdown","fbdd618f":"markdown"},"source":{"7164273c":"import numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt","348df35d":"# hyperparameters for prior losses generating (some estimation from leaderboard)\nL0, SIGMA0 = 6.8, 2\n\n# small noise deviation\nEPS = 0.1","6a9f2616":"def plot_solution(n_cv, n_test, sigma, ax):\n    ## hyperparameters\n    n_pub, n_priv = 0.15 * n_test, 0.85 * n_test\n\n    ## prior losses\n    l0_CV_samples = np.random.normal(loc=L0, scale=SIGMA0, size=int(n_cv))\n    l0_pub_samples = np.random.normal(loc=L0, scale=SIGMA0, size=int(n_pub))\n    l0_priv_samples = np.random.normal(loc=L0, scale=SIGMA0, size=int(n_priv))\n\n    ## mean losess \n    l0_CV = l0_CV_samples.mean()\n    l0_pub = l0_pub_samples.mean()\n    l0_priv = l0_priv_samples.mean()\n    \n    ## distributions\n    l_CV_samples = np.random.normal(loc=l0_CV, scale=EPS, size=5)\n    l_pub_samples = np.random.normal(loc=l0_pub, scale=sigma, size=5)\n    l_priv_samples = np.random.normal(loc=l0_priv, scale=sigma, size=5)\n    \n    ## ## distributions distributions & samples\n    x = np.linspace(L0 - 1*SIGMA0, L0 + 1*SIGMA0, 100)\n\n    ax.plot(x, stats.norm.pdf(x, l0_CV, EPS), label=\"CV\")\n    _ = ax.scatter(l_CV_samples, 0.2 * np.random.rand(5), marker='x')\n\n    ax.plot(x, stats.norm.pdf(x, l0_pub, sigma), label=\"public\")\n    _ = ax.scatter(l_pub_samples, 0.2 * np.random.rand(5), marker='x')\n\n    ax.plot(x, stats.norm.pdf(x, l0_priv, sigma), label=\"private\")\n    _ = ax.scatter(l_priv_samples, 0.2 * np.random.rand(5), marker='x')\n\n    _ = ax.legend()\n    \ndef plot_solutions(n_cv, n_test, sigma):\n    # repeat plot_solution\n    nrows, ncols = 3, 2\n    fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, 12))\n\n    for i in range(nrows):\n        for j in range(ncols):\n            plot_solution(n_cv=n_cv, n_test=n_test, sigma=sigma, ax=ax[i][j])","86a613cb":"plot_solutions(n_cv=1e3, n_test=1e3, sigma=EPS)","49131eb4":"plot_solutions(n_cv=1e3, n_test=1e3, sigma=0.3)","69a9d12e":"plot_solutions(n_cv=1e3, n_test=1e3, sigma=1)","ac946fce":"plot_solutions(n_cv=2 * 176, n_test=200, sigma=0.3)","0410c5cf":"## 3. Small Data & Domain Shifting","6c3efca8":"## 2. Big Data & Domain Shifting","d6e0cb66":"# Hyperparameters","bfa56682":"This notebook corresponds [this](https:\/\/www.kaggle.com\/c\/osic-pulmonary-fibrosis-progression\/discussion\/183086) topic and I use it for drawing distributions and samples from it. Steps:\n- get hyperparameters $n_{cv}$, $n_{test}$ (amount of samples in the train and test set respectively) and $\\sigma$ (domain shifting factor).\n- generate some prior losses $l_{CV}^0, l_{pub}^0, l_{priv}^0$. We dont use some particular solution *S* for getting losses (in this case we would get only posterior losses), instead we just generate solution *S* by generating prior losses for each sample.\n- count mean losses and drawing distributions with some samples.","c85bcc7d":"## 1. Big Data & No Domain Shifting","ba45e3a9":"# Code","fbdd618f":"# Results"}}