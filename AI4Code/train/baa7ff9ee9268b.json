{"cell_type":{"99df70b2":"code","e564b021":"code","5e9deeb3":"code","42cd7049":"code","6f9212bb":"code","3433e228":"code","415a1729":"code","05e3b313":"code","4cac8ccc":"code","a71b9c74":"code","d220a0cc":"code","86b76b45":"code","df630725":"code","58c2e9e0":"code","7c7f5fee":"code","353b79b0":"code","f5cd4680":"code","1bebcda8":"code","64cf7432":"code","0ca6895f":"code","6e3690e1":"code","e58b8067":"code","d0245264":"code","de04f2d0":"code","324d56c2":"code","3aaa0b1b":"code","a9bff472":"code","e66d53fd":"code","7cc8f2ac":"code","6ae16202":"code","f9a82786":"code","9474b92a":"code","ac95fb26":"code","a1bbdea2":"code","23612d8f":"code","a5708de0":"code","c996dd17":"code","4f444295":"code","62dd81ee":"code","854b8b7a":"code","ebb09fb5":"code","a18dca2e":"code","f77dd853":"code","7541f7e7":"code","37f739b7":"code","3493a77c":"code","91e16b50":"code","b9797c49":"code","13923354":"code","552bc3d9":"code","3f507462":"code","249cc5cb":"code","6c4f4f3a":"code","471778d0":"code","efad8898":"code","c5a5b15d":"code","fd907192":"code","b204b753":"code","69faa0a9":"code","b5179837":"code","86306801":"code","d0d3386e":"code","9525f628":"code","e1392353":"code","9aced4d7":"code","f930ceeb":"code","7494811f":"code","c555b3ce":"code","c56bcd46":"code","89250f98":"code","596506d0":"code","34e40a62":"code","8b17f1a4":"code","179bd116":"code","6ab3482b":"code","b7950a78":"code","a50f3f91":"code","3f9ceefd":"code","79b986a4":"code","4d48a15c":"code","06232a4c":"code","4100c25d":"code","631a57df":"code","943fde0b":"code","88cf4fa9":"code","2d1456f0":"code","64669584":"code","792845e2":"code","7dd2dab6":"code","7fb3c2fc":"code","ef54b6a3":"code","1cf56e31":"code","bfc53350":"code","fa8af7f5":"code","b24e84cf":"code","d2815b9d":"code","180c0c2b":"code","c8b9f559":"code","fac9f757":"code","4b28188c":"code","42997498":"code","0f0a4f87":"code","d6491e76":"code","79b37ab6":"code","f790e550":"code","cc0a642b":"code","67f746aa":"code","8b77a972":"code","882603b1":"code","7f15a2a2":"code","1a77ccb3":"code","1d18e350":"code","d409e033":"code","799a808d":"code","1de7bfba":"code","41f02d67":"code","5110a36c":"code","f7b0b831":"code","78e50d46":"code","8ca8147b":"code","d17d9d1a":"code","98529dd0":"code","6331782d":"code","7b0c3d55":"code","ac3053e4":"code","b2c57af2":"code","2071d9f6":"code","dd699546":"code","c32a5ae4":"code","f5355a28":"code","f1ed8f57":"code","82f53a8b":"code","b146e865":"code","544805db":"code","f2ff6abc":"code","b7d8f0c2":"code","698a569e":"markdown","262bf33e":"markdown","1e75bbe2":"markdown","680bc37f":"markdown","ab323072":"markdown","4f1281ab":"markdown","7acfec0a":"markdown","383d5c1c":"markdown","923a92df":"markdown","e214c34c":"markdown","055406dc":"markdown","793219a8":"markdown","e54b877e":"markdown","6c3650e5":"markdown","c8b6d22b":"markdown","4650985e":"markdown","fc39a2ef":"markdown","79bd7dc9":"markdown","3e43d870":"markdown","c8cf950d":"markdown","4c2109b0":"markdown","ea8f6547":"markdown","b2e40c5c":"markdown","9df55e3d":"markdown","e77ce566":"markdown","fc0c4a38":"markdown","46ea97bc":"markdown","0e0d3a68":"markdown","9463affe":"markdown","b694ee38":"markdown","4cb14aef":"markdown","c84bca6e":"markdown","af878f63":"markdown","d92b64b4":"markdown","4cef4da0":"markdown","4f7a8196":"markdown","038c93e7":"markdown","7aa1f880":"markdown","04c9b463":"markdown","00a0b830":"markdown","53be3070":"markdown","eb230928":"markdown","88e30f97":"markdown","f7e45c33":"markdown","acd59391":"markdown","ec22cae6":"markdown","302e9778":"markdown","e3eec25f":"markdown","9ef2ae1f":"markdown","bd1edbb2":"markdown","5ce8fa5b":"markdown","fd3958b4":"markdown","8abf4e3c":"markdown","4d3c19cc":"markdown","a48b1adc":"markdown","3bc88bfa":"markdown","7ce6ac29":"markdown","cd36c1c5":"markdown","f4c3113b":"markdown","44fa2c84":"markdown","f11defc9":"markdown","74f0ccba":"markdown","f110cd34":"markdown","88b78b9f":"markdown","6c185e02":"markdown","c1470323":"markdown","0cc34869":"markdown","5c67980b":"markdown","10a61779":"markdown","eebc31a3":"markdown","1f10f205":"markdown","cc5b7fda":"markdown","8e2a639d":"markdown","b0edc1d0":"markdown","0b1ee281":"markdown","e0576a65":"markdown","083aa67d":"markdown","13539b02":"markdown","3356e6d2":"markdown","ddba177b":"markdown","3af588dc":"markdown","da3c18fc":"markdown","a17934bb":"markdown","9c00f9db":"markdown"},"source":{"99df70b2":"import numpy as np \n# large, multi-dimensional arrays and matrices, \n# along with a large collection of high-level mathematical functions to operate on these arrays.\nimport pandas as pd\n# data structures and operations for manipulating numerical tables and time series\nimport matplotlib.pyplot as plt\n# plotting\nimport plotly.express as px\n# graph\nimport plotly.graph_objects as go\n# graph\nimport seaborn as sns\n# t-test\nfrom scipy import stats\n# regression\nfrom sklearn import datasets, linear_model\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n# Word Cloud\nfrom wordcloud import WordCloud","e564b021":"data=pd.read_csv('..\/input\/data-analyst-jobs\/DataAnalyst.csv')","5e9deeb3":"data.head(2)","42cd7049":"data.describe(include='all')","6f9212bb":"# Check for missing values\ndef missing_values_table(df):\n    # number of missing values\n    mis_val = df.isnull().sum()\n    # % of missing values\n    mis_val_percent = 100 * mis_val \/ len(df)\n    # make table # axis '0' concat along index, '1' column\n    mis_val_table = pd.concat([mis_val,mis_val_percent],axis=1) \n    # rename columns\n    mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0:'Missing Values',1:'% of Total Values'})\n    # sort by column\n    mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1]!=0].sort_values(\n        '% of Total Values',ascending=False).round(1) #Review\n    print(\"Your selected datset has \"+str(df.shape[1])+\" columns and \"+str(len(df))+\" observations.\\n\"\n         \"There are \"+str(mis_val_table_ren_columns.shape[0])+\" columns that have missing values.\")\n    # return the dataframe with missing info\n    return mis_val_table_ren_columns\n\nmissing_values_table(data)","3433e228":"data['Easy Apply'].value_counts()","415a1729":"data['Competitors'].value_counts()","05e3b313":"# Replace -1 or -1.0 or '-1' to NaN\ndata=data.replace(-1,np.nan)\ndata=data.replace(-1.0,np.nan)\ndata=data.replace('-1',np.nan)","4cac8ccc":"missing_values_table(data)","a71b9c74":"#Remove '\\n' from Company Name. \ndata['Company Name'],_=data['Company Name'].str.split('\\n', 1).str\n# 1st column after split, 2nd column after split (delete when '_')\n# string.split(separator, maxsplit) maxsplit default -1, which means all occurrances","d220a0cc":"# Split salary into two columns min salary and max salary.\ndata['Salary Estimate'],_=data['Salary Estimate'].str.split('(', 1).str","86b76b45":"# Split salary into two columns min salary and max salary.\ndata['Min_Salary'],data['Max_Salary']=data['Salary Estimate'].str.split('-').str\ndata['Min_Salary']=data['Min_Salary'].str.strip(' ').str.lstrip('$').str.rstrip('K').fillna(0).astype('int')\ndata['Max_Salary']=data['Max_Salary'].str.strip(' ').str.lstrip('$').str.rstrip('K').fillna(0).astype('int')\n# lstrip is for removing leading characters\n# rstrip is for removing rear characters","df630725":"#Drop the original Salary Estimate column\ndata.drop(['Salary Estimate'],axis=1,inplace=True)","58c2e9e0":"# To estimate the salary with regression and other analysis, better come up with one number: Est_Salary = (Min_Salary+Max_Salary)\/2\ndata['Est_Salary']=(data['Min_Salary']+data['Max_Salary'])\/2","7c7f5fee":"# Create a variable for how many years a firm has been founded\ndata['Years_Founded'] = 2020 - data['Founded']","353b79b0":"# A final look at the data before analysis\ndata.head(2)","f5cd4680":"plt.figure(figsize=(13,5))\nsns.set() #style==background\nsns.distplot(data['Min_Salary'], color=\"b\")\nsns.distplot(data['Max_Salary'], color=\"r\")\n\nplt.xlabel(\"Salary ($'000)\")\nplt.legend({'Min_Salary':data['Min_Salary'],'Max_Salary':data['Max_Salary']})\nplt.title(\"Distribution of Min & Max Salary\",fontsize=19)\nplt.xlim(0,210)\nplt.xticks(np.arange(0, 210, step=10))\nplt.tight_layout()\nplt.show()","1bebcda8":"min_max_view = data.sort_values(['Min_Salary','Max_Salary'],ascending=True).reset_index(drop=True).reset_index()\nmin_max_view = min_max_view.drop([0])","64cf7432":"f, (ax_box, ax_line) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\": (0.05,1)},figsize=(13,5))\nmean=min_max_view['Est_Salary'].mean()\nmedian=min_max_view['Est_Salary'].median()\n\nbpv = sns.boxplot(y='Est_Salary',data=min_max_view, ax=ax_box).set(ylabel=\"Est. Salary ($'000)\")\nax_box.axhline(mean, color='k', linestyle='--')\nax_box.axhline(median, color='y', linestyle='-')\n\nlp1 = sns.lineplot(x='index',y='Min_Salary',data=min_max_view, color='b')\nlp2 = sns.lineplot(x='index',y='Max_Salary',ax=ax_line,data=min_max_view, color='r')\nax_line.axhline(mean, color='k', linestyle='--')\nax_line.axhline(median, color='y', linestyle='-')\n\nplt.legend({'Min_Salary':data['Min_Salary'],'Max_Salary':data['Max_Salary'],'Mean':mean,'Median':median})\nplt.title(\"Salary Estimates of Each Analyst\",fontsize=19)\nplt.xlabel(\"Observations\")\nplt.tight_layout()\nplt.show()","0ca6895f":"sns.set(style='white')\n\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw= {\"height_ratios\": (0.2, 1)},figsize=(13,5))\nmean=data['Est_Salary'].mean()\nmedian=data['Est_Salary'].median()\n\nbph = sns.boxplot(data['Est_Salary'], ax=ax_box).set(xlabel=\"\")\nax_box.axvline(mean, color='k', linestyle='--')\nax_box.axvline(median, color='y', linestyle='-')\n\ndp = sns.distplot(data['Est_Salary'],ax=ax_hist, color=\"g\").set(xlabel=\"Est. Salary ($'000)\")\nax_hist.axvline(mean, color='k', linestyle='--')\nax_hist.axvline(median, color='y', linestyle='-')\n\nplt.legend({'Mean':mean,'Median':median})\nplt.xlim(0,210)\nplt.xticks(np.arange(0,210,step=10))\nplt.tight_layout() #Adjust the padding between and around subplots\nplt.show()","6e3690e1":"sns.set(style='white')\n\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw= {\"height_ratios\": (0.2, 1)},figsize=(13,5))\nmean=data['Years_Founded'].mean()\nmedian=data['Years_Founded'].median()\n\nbph = sns.boxplot(data['Years_Founded'], ax=ax_box).set(xlabel=\"\")\nax_box.axvline(mean, color='k', linestyle='--')\nax_box.axvline(median, color='y', linestyle='-')\n\ndp = sns.distplot(data['Years_Founded'],ax=ax_hist, color=\"g\").set(xlabel=\"Years_Founded\")\nax_hist.axvline(mean, color='k', linestyle='--')\nax_hist.axvline(median, color='y', linestyle='-')\n\nplt.legend({'Mean':mean,'Median':median})\nplt.xlim(0,240)\nplt.xticks(np.arange(0,240,step=10))\nplt.tight_layout() #Adjust the padding between and around subplots\nplt.show()","e58b8067":"sns.set(style='white')\n\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw= {\"height_ratios\": (0.2, 1)},figsize=(13,5))\nmean=data['Rating'].mean()\nmedian=data['Rating'].median()\n\nbph = sns.boxplot(data['Rating'], ax=ax_box).set(xlabel=\"\")\nax_box.axvline(mean, color='k', linestyle='--')\nax_box.axvline(median, color='y', linestyle='-')\n\ndp = sns.distplot(data['Rating'],ax=ax_hist, color=\"g\").set(xlabel=\"Ratings\")\nax_hist.axvline(mean, color='k', linestyle='--')\nax_hist.axvline(median, color='y', linestyle='-')\n\nplt.legend({'Mean':mean,'Median':median})\nplt.xlim(0,6)\nplt.xticks(np.arange(0,6,step=1))\nplt.tight_layout() #Adjust the padding between and around subplots\nplt.show()","d0245264":"# First I count the positions opened by the companies.\ndf_by_firm=data.groupby('Company Name')['Job Title'].count().reset_index().sort_values(\n    'Job Title',ascending=False).head(20).rename(columns={'Job Title':'Hires'})\n# When we reset the index, the old index is added as a column, and a new sequential index is used","de04f2d0":"# Merge with original data to get salary estimates.\nSal_by_firm = df_by_firm.merge(data,on='Company Name',how='left')","324d56c2":"sns.set(style=\"white\")\nf, (ax_bar, ax_point) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\":(0.6,1)},figsize=(13,7))\nsns.barplot(x='Hires',y='Company Name',data=Sal_by_firm,ax=ax_bar, palette='Set2').set(ylabel=\"\")\nsns.pointplot(x='Est_Salary',y='Company Name',data=Sal_by_firm, join=False,ax=ax_point).set(\n    ylabel=\"\",xlabel=\"Salary ($'000)\")\n\nplt.tight_layout()","3aaa0b1b":"df_by_city=data.groupby('Location')['Job Title'].count().reset_index().sort_values(\n    'Job Title',ascending=False).head(20).rename(columns={'Job Title':'Hires'})\nSal_by_city = df_by_city.merge(data,on='Location',how='left')","a9bff472":"sns.set(style=\"white\")\nf, (ax_bar, ax_point) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\":(0.6,1)},figsize=(13,7))\nsns.barplot(x='Hires',y='Location',data=Sal_by_city,ax=ax_bar, palette='Set2').set(ylabel=\"\")\nsns.pointplot(x='Est_Salary',y='Location',data=Sal_by_city, join=False,ax=ax_point).set(\n    ylabel=\"\",xlabel=\"Salary ($'000)\")\n\nplt.tight_layout()","e66d53fd":"data['City'],data['State'] = data['Location'].str.split(', ',1).str","7cc8f2ac":"data['State']=data['State'].replace('Arapahoe, CO','CO')","6ae16202":"stateCount = data.groupby('State')[['Job Title']].count().reset_index().rename(columns={'Job Title':'Hires'}).sort_values(\n    'Hires', ascending=False).reset_index(drop=True)\nstateCount = stateCount.merge(data, on='State',how='left')","f9a82786":"sns.set(style=\"whitegrid\")\nf, (ax_bar, ax_point) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\":(0.6,1)},figsize=(13,7))\nsns.barplot(x='Hires',y='State',data=stateCount,ax=ax_bar)\nsns.pointplot(x='Est_Salary',y='State',data=stateCount, join=False,ax=ax_point).set(ylabel=\"\",xlabel=\"Salary ($'000)\")\n\nplt.tight_layout()","9474b92a":"data['HQCity'],data['HQState'] = data['Headquarters'].str.split(', ',1).str","ac95fb26":"data['HQState']=data['HQState'].replace('NY (US), NY','NY')","a1bbdea2":"HQCount = data.groupby('HQState')[['Job Title']].count().reset_index().rename(columns={'Job Title':'Hires'}).sort_values(\n    'Hires', ascending=False).head(20).reset_index(drop=True)\nHQCount = HQCount.merge(data, on='HQState',how='left')","23612d8f":"sns.set(style=\"whitegrid\")\nf, (ax_bar, ax_point) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\":(0.6,1)},figsize=(13,7))\nsns.barplot(x='Hires',y='HQState',data=HQCount,ax=ax_bar)\nsns.pointplot(x='Est_Salary',y='HQState',data=HQCount, join=False,ax=ax_point).set(ylabel=\"\",xlabel=\"Salary ($'000)\")\n\nplt.tight_layout()","a5708de0":"RevCount = data.groupby('Revenue')[['Job Title']].count().reset_index().rename(columns={'Job Title':'Hires'}).sort_values(\n    'Hires', ascending=False).reset_index(drop=True)","c996dd17":"#Make the Revenue column clean\nRevCount[\"Revenue_USD\"]=['Unknown','100-500 million','50-100 million','10+ billion','10-25 million','2-5 billion','1-5 million','25-50 million','<1 million','1-2 billion','0.5-1 billion','5-10 million','5-10 billion']\n#Merge the new Revenue back to data\nRevCount2 = RevCount[['Revenue','Revenue_USD']]\nRevCount = RevCount.merge(data, on='Revenue',how='left')","4f444295":"data=data.merge(RevCount2,on='Revenue',how='left')","62dd81ee":"sns.set(style=\"whitegrid\")\nf, (ax_bar, ax_point) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\":(0.6,1)},figsize=(13,7))\nsns.barplot(x='Hires',y='Revenue_USD',data=RevCount,ax=ax_bar)\nsns.pointplot(x='Est_Salary',y='Revenue_USD',data=RevCount, join=False,ax=ax_point).set(ylabel=\"\",xlabel=\"Salary ($'000)\")\n\nplt.tight_layout()","854b8b7a":"SizeCount = data.groupby('Size')[['Job Title']].count().reset_index().rename(columns={'Job Title':'Hires'}).sort_values(\n    'Hires', ascending=False).reset_index(drop=True)\nSizeCount = SizeCount.merge(data, on='Size',how='left')","ebb09fb5":"sns.set(style=\"whitegrid\")\nf, (ax_bar, ax_point) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\":(0.6,1)},figsize=(13,7))\nsns.barplot(x='Hires',y='Size',data=SizeCount,ax=ax_bar)\nsns.pointplot(x='Est_Salary',y='Size',data=SizeCount, join=False,ax=ax_point).set(ylabel=\"\",xlabel=\"Salary ($'000)\")\n\nplt.tight_layout()","a18dca2e":"SecCount = data.groupby('Sector')[['Job Title']].count().reset_index().rename(columns={'Job Title':'Hires'}).sort_values(\n    'Hires', ascending=False).reset_index(drop=True)\nSecCount = SecCount.merge(data, on='Sector',how='left')\nSecCount = SecCount[SecCount['Hires']>29]","f77dd853":"sns.set(style=\"whitegrid\")\nf, (ax_bar, ax_point) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\":(0.6,1)},figsize=(13,7))\nsns.barplot(x='Hires',y='Sector',data=SecCount,ax=ax_bar)\nsns.pointplot(x='Est_Salary',y='Sector',data=SecCount, join=False,ax=ax_point).set(ylabel=\"\",xlabel=\"Salary ($'000)\")\n\nplt.tight_layout()","7541f7e7":"OwnCount = data.groupby('Type of ownership')[['Job Title']].count().reset_index().rename(columns={'Job Title':'Hires'}).sort_values(\n    'Hires', ascending=False).reset_index(drop=True)\nOwnCount = OwnCount.merge(data, on='Type of ownership',how='left')","37f739b7":"sns.set(style=\"whitegrid\")\nf, (ax_bar, ax_point) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\":(0.6,1)},figsize=(13,7))\nsns.barplot(x='Hires',y='Type of ownership',data=OwnCount,ax=ax_bar)\nsns.pointplot(x='Est_Salary',y='Type of ownership',data=OwnCount, join=False,ax=ax_point).set(ylabel=\"\",xlabel=\"Salary ($'000)\")\n\nplt.tight_layout()","3493a77c":"# create a new dataset from original data\ntext_Analysis = data[['Job Title','Job Description','Est_Salary','Max_Salary','Min_Salary','State','Easy Apply','Revenue_USD','Rating','Size','Industry','Sector','Type of ownership','Years_Founded','Company Name','HQState']]\n# remove special characters and unify some word use\ntext_Analysis['Job_title_2']= text_Analysis['Job Title'].str.upper().replace(\n    [',','\u00c2','\/','\\t','\\n','-','AND ','&','\\(','\\)','WITH ','SYSTEMS','OPERATIONS','ANALYTICS','SERVICES','\\[','\\]','ENGINEERS','NETWORKS','GAMES','MUSICS','INSIGHTS','SOLUTIONS','JR.','MARKETS','STANDARDS','FINANCE','ENGINEERING','PRODUCTS','DEVELOPERS','SR. ','SR ','JR. ','JR '],\n    ['','',' ',' ',' ',' ','',' ',' ',' ','','SYSTEM','OPERATION','ANALYTIC','SERVICE','','','ENGINEER','NETWORK','GAME','MUSIC','INSIGHT','SOLUTION','JUNIOR','MARKET','STANDARD','FINANCIAL','ENGINEER','PRODUCT','DEVELOPER','SENIOR ','SENIOR ','JUNIOR ','JUNIOR '],regex=True)\n# later found out replace('[^A-Za-z0-9]+', ' ',regex=True) is a simpler way to remove non-text characters","91e16b50":"text_Analysis['Job_title_2']= text_Analysis['Job_title_2'].str.upper().replace(\n    ['  ','   ','    '],\n    [' ',' ',' '],regex=True)\n# later found out replace('[^A-Za-z0-9]+', ' ',regex=True) is a simpler way to remove non-text characters","b9797c49":"# unify some word use\ntext_Analysis['Job_title_2']= text_Analysis['Job_title_2'].str.upper().replace(\n    ['BUSINESS INTELLIGENCE','INFORMATION TECHNOLOGY','QUALITY ASSURANCE','USER EXPERIENCE','USER INTERFACE','DATA WAREHOUSE','DATA ANALYST','DATA BASE','DATA QUALITY','DATA GOVERNANCE','BUSINESS ANALYST','DATA MANAGEMENT','REPORTING ANALYST','BUSINESS DATA','SYSTEM ANALYST','DATA REPORTING','QUALITY ANALYST'],\n    ['BI','IT','QA','UX','UI','DATA_WAREHOUSE','DATA_ANALYST','DATABASE','DATA_QUALITY','DATA_GOVERNANCE','BUSINESS_ANALYST','DATA_MANAGEMENT','REPORTING_ANALYST','BUSINESS_DATA','SYSTEM_ANALYST','DATA_REPORTING','QUALITY_ANALYST'],regex=True)","13923354":"# unify some word use\ntext_Analysis['Job_title_2']= text_Analysis['Job_title_2'].str.upper().replace(\n    ['DATA_ANALYST JUNIOR','DATA_ANALYST SENIOR','DATA  REPORTING_ANALYST'],\n    ['JUNIOR DATA_ANALYST','SENIOR DATA_ANALYST','DATA_REPORTING_ANALYST'],regex=True)","552bc3d9":"jobCount=text_Analysis.groupby('Job_title_2')[['Job Title']].count().reset_index().rename(\n    columns={'Job Title':'Count'}).sort_values('Count',ascending=False)\njobSalary = text_Analysis.groupby('Job_title_2')[['Max_Salary','Est_Salary','Min_Salary']].mean().sort_values(\n    ['Max_Salary','Est_Salary','Min_Salary'],ascending=False)\njobSalary['Spread']=jobSalary['Max_Salary']-jobSalary['Est_Salary']\njobSalary=jobSalary.merge(jobCount,on='Job_title_2',how='left').sort_values('Count',ascending=False).head(20)","3f507462":"f, axs = plt.subplots(2, sharex=True, gridspec_kw= {\"height_ratios\":(1,0.5)},figsize=(13,8))\n\nax = axs[0]\nax.errorbar(x='Job_title_2',y='Est_Salary',data=jobSalary,yerr=jobSalary['Spread'],fmt='o')\nax.set_ylabel('Est. Salary ($\\'000)')\n\nax = axs[1]\nsns.barplot(x=jobSalary['Job_title_2'],y=jobSalary['Count']).set(xlabel=\"\")\n\nplt.xticks(rotation=65,horizontalalignment='right')\nplt.tight_layout()","249cc5cb":"# get top keywords\ns = text_Analysis['Job_title_2'].str.split(expand=True).stack().value_counts().reset_index().rename(\n    columns={'index':'KW',0:'Count'})\nS = s[s['Count']>29]\nS","6c4f4f3a":"# write get_keyword method\ndef get_keyword(x):\n   x_ = x.split(\" \")\n   keywords = []\n   try:\n      for word in x_:\n         if word in np.asarray(S['KW']):\n            keywords.append(word)\n   except:\n      return -1\n\n   return keywords","471778d0":"# get keywords from each row\ntext_Analysis['KW'] = text_Analysis['Job_title_2'].apply(lambda x: get_keyword(x))","efad8898":"# create dummy columns by keywords\nkwdummy = pd.get_dummies(text_Analysis['KW'].apply(pd.Series).stack()).sum(level=0).replace(2,1)\ntext_Analysis = text_Analysis.merge(kwdummy,left_index=True,right_index=True).replace(np.nan,0)","c5a5b15d":"# drop 2149 because unpaid analyst is not usual\ntext_Analysis = text_Analysis.drop([2149])","fd907192":"# run t-test for top keywords to see their correlation with salaries\ntext_columns = list(text_Analysis.columns)\nttests=[]\nfor word in text_columns:\n    if word in set(S['KW']):\n        ttest = stats.ttest_ind(text_Analysis[text_Analysis[word]==1]['Est_Salary'],\n                                     text_Analysis[text_Analysis[word]==0]['Est_Salary'])\n        ttests.append([word,ttest])\n        \nttests = pd.DataFrame(ttests,columns=['KW','R'])\nttests['R']=ttests['R'].astype(str).replace(['Ttest_indResult\\(statistic=','pvalue=','\\)'],['','',''],regex=True)\nttests['Statistic'],ttests['P-value']=ttests['R'].str.split(', ',1).str\nttests=ttests.drop(['R'],axis=1).sort_values('P-value',ascending=True)\nttests","b204b753":"# Selecting keywords with p-value <0.1 into multiple regression model.\nttest_pass = list(ttests[ttests['P-value'].astype(float)<0.1]['KW'])\nprint(*ttest_pass,sep=' + ')","69faa0a9":"# Run initial regression model.\ntitleMod = ols(\"Est_Salary ~ JUNIOR + SENIOR + II + BUSINESS_ANALYST + DATA_GOVERNANCE + MARKETING + FINANCIAL\",\n               data=text_Analysis).fit()\nprint(titleMod.summary())","b5179837":"# Remove variables with p-value >0.05 one by one until all <0.05\ntitleMod_final = ols(\"Est_Salary ~ JUNIOR + SENIOR + II + BUSINESS_ANALYST\",\n               data=text_Analysis).fit()\nprint(titleMod_final.summary())","86306801":"# Plot with scatterplots\nfig = plt.figure(figsize=(13, 13))\nfig = sm.graphics.plot_partregress_grid(titleMod_final,fig=fig)\nfig.tight_layout(pad=1.0)\n# Sorry somebody tell me how to remove that \"Partial Regression Plot\"","d0d3386e":"text_Analysis['Job_Desc2'] = text_Analysis['Job Description'].replace('[^A-Za-z0-9]+', ' ',regex=True)","9525f628":"text_Analysis['Job_Desc2'] = text_Analysis['Job_Desc2'].str.upper().replace(\n    ['COMPUTER SCIENCE','ENGINEERING DEGREE',' MS ','BUSINESS ANALYTICS','SCRUM MASTER','MACHINE LEARNING',' ML ','POWER BI','ARTIFICIAL INTELLIGENCE',' AI ','ALGORITHMS','DEEP LEARNING','NEURAL NETWORK','NATURAL LANGUAGE PROCESSING','DECISION TREE','CLUSTERING','PL SQL'],\n    ['COMPUTER_SCIENCE','ENGINEERING_DEGREE',' MASTER ','BUSINESS_ANALYTICS','SCRUM_MASTER','MACHINE_LEARNING',' MACHINE_LEARNING ','POWER_BI','ARTIFICIAL_INTELLIGENCE',' ARTIFICIAL_INTELLIGENCE ','ALGORITHM','DEEP_LEARNING','NEURAL_NETWORK','NATURAL_LANGUAGE_PROCESSING','DECISION_TREE','CLUSTER','PLSQL'],regex=True)","e1392353":"# Create a list of big data buzzwords to see if those words in JD would influence the salary\nbuzzwords = ['COMPUTER_SCIENCE','MASTER','MBA','SQL','PYTHON','R','PHD','BUSINESS_ANALYTICS','SAS','PMP','SCRUM_MASTER','STATISTICS','MATHEMATICS','MACHINE_LEARNING','ARTIFICIAL_INTELLIGENCE','ECONOMICS','TABEAU','AWS','AZURE','POWER_BI','ALGORITHM','DEEP_LEARNING','NEURAL_NETWORK','NATURAL_LANGUAGE_PROCESSING','DECISION_TREE','REGRESSION','CLUSTER','ORACLE','EXCEL','TENSORFLOW','HADOOP','SPARK','NOSQL','SAP','ETL','API','PLSQL','MONGODB','POSTGRESQL','ELASTICSEARCH','REDIS','MYSQL','FIREBASE','SQLITE','CASSANDRA','DYNAMODB','OLTP','OLAP']","9aced4d7":"# Count the JD keywords.\nS2 = text_Analysis['Job_Desc2'].str.split(expand=True).stack().value_counts().reset_index().rename(\n    columns={'index':'KW',0:'Count'})\nS2 = S2[S2['KW'].isin(buzzwords)].reset_index(drop=True)\n# .sort_values('Count',ascending=False)\nS2_TOP = S2[S2['Count']>29]\nS2_TOP_JD = S2_TOP\nS2_TOP_JD['KW'] = S2_TOP_JD['KW'] +'_JD'","f930ceeb":"wordCloud = WordCloud(width=450,height= 300).generate(' '.join(S2['KW']))\nplt.figure(figsize=(19,9))\nplt.axis('off')\nplt.title(\"Keywords in Data Analyst Job Descriptions\",fontsize=20)\nplt.imshow(wordCloud)\nplt.show()","7494811f":"# write get_keyword method\ndef get_keyword(x):\n   x_ = x.split(\" \")\n   keywords = []\n   try:\n      for word in x_:\n         if word + '_JD' in np.asarray(S2_TOP_JD['KW']):\n            keywords.append(word + '_JD')\n   except:\n      return -1\n\n   return keywords","c555b3ce":"# get keywords from each row\ntext_Analysis['JDKW'] = text_Analysis['Job_Desc2'].apply(lambda x: get_keyword(x))","c56bcd46":"# create dummy columns by keywords\nkwdummy = pd.get_dummies(text_Analysis['JDKW'].apply(pd.Series).stack()).sum(level=0)\n# Since a JD sometimes repeat a keyword, the value may >1\n# But what we want to know is whether the appearance of the keyword impact the salary, not frequency\n# So values >1 have to be replaced by 1, but there must be a better way than coding like this \u2193\nkwdummy = kwdummy.replace([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18],\n                         [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1])","89250f98":"# merge back the dummy columns to the main dataset\ntext_Analysis = text_Analysis.merge(kwdummy,left_index=True,right_index=True,how='left').replace(np.nan,0)","596506d0":"# let's see if number of buzzwords contained or how wordy the JD is would have impact.\ntext_Analysis['JDKWlen']=text_Analysis['JDKW'].str.len()\ntext_Analysis['JDlen']=text_Analysis['Job Description'].str.len()","34e40a62":"# run t-test for top keywords to see their correlation with salaries\ntext_columns = list(text_Analysis.columns)\nttests_JD=[]\nfor word in text_columns:\n    if word in set(S2_TOP_JD['KW']):\n        ttest2 = stats.ttest_ind(text_Analysis[text_Analysis[word]>0]['Est_Salary'],\n                                 text_Analysis[text_Analysis[word]==0]['Est_Salary'])\n        ttests_JD.append([word,ttest2])\n\nttests_JD = pd.DataFrame(ttests_JD,columns=['KW','R'])\nttests_JD['R']=ttests_JD['R'].astype(str).replace(['Ttest_indResult\\(statistic=','pvalue=','\\)'],['','',''],regex=True)\nttests_JD['Statistic'],ttests_JD['P-value']=ttests_JD['R'].str.split(', ',1).str\nttests_JD=ttests_JD.drop(['R'],axis=1).sort_values('P-value',ascending=True)\nttests_JD","8b17f1a4":"#Selecting keywords with p-value <0.1 into multiple regression model.\nttest_JD_pass1 = list(ttests_JD[ttests_JD['P-value'].astype(float)<0.1]['KW'])\nprint(*ttest_JD_pass1,sep=' + ')","179bd116":"#Run regression and remove variables with p-value >0.05 one by one until all <0.05\nJDMod = ols(\"Est_Salary ~ PYTHON_JD + MYSQL_JD + PHD_JD + SAS_JD\",\n               data=text_Analysis).fit()\nprint(JDMod.summary())","6ab3482b":"fig = plt.figure(figsize=(13, 13))\nfig = sm.graphics.plot_partregress_grid(JDMod,fig=fig)\nfig.tight_layout(pad=1.0)","b7950a78":"# create dummy columns by State\nkwdummy = pd.get_dummies(text_Analysis['State'].apply(pd.Series).stack()).sum(level=0)\ntext_Analysis = text_Analysis.merge(kwdummy,left_index=True,right_index=True,how='left').replace(np.nan,0)","a50f3f91":"S3 = text_Analysis['State'].value_counts().reset_index().rename(\n    columns={'index':'State','State':'Count'})\nS3_Top = S3[S3['Count']>29]\nS3_Top","3f9ceefd":"#run t-test for top states hring analysts to see their correlation with salaries\ntext_columns = list(text_Analysis.columns)\nttests_state=[]\nfor word in text_columns:\n    if word in set(S3_Top['State']):\n        ttest3 = stats.ttest_ind(text_Analysis[text_Analysis[word]>0]['Est_Salary'],\n                                 text_Analysis[text_Analysis[word]==0]['Est_Salary'])\n        ttests_state.append([word,ttest3])\n\nttests_state = pd.DataFrame(ttests_state,columns=['State','R'])\nttests_state['R']=ttests_state['R'].astype(str).replace(['Ttest_indResult\\(statistic=','pvalue=','\\)'],['','',''],regex=True)\nttests_state['Statistic'],ttests_state['P-value']=ttests_state['R'].str.split(', ',1).str\nttests_state=ttests_state.drop(['R'],axis=1).sort_values('P-value',ascending=True)\nttests_state","79b986a4":"#Selecting states with p-value <0.1 into multiple regression model.\nttest_state_pass = list(ttests_state[ttests_state['P-value'].astype(float)<0.1]['State'])\nprint(*ttest_state_pass,sep=' + ')","4d48a15c":"StateMod = ols(\"Est_Salary ~ IL + UT + TX + OH + PA + CA\",\n               data=text_Analysis).fit()\nprint(StateMod.summary())","06232a4c":"fig = plt.figure(figsize=(13, 13))\nfig = sm.graphics.plot_partregress_grid(StateMod,fig=fig)\nfig.tight_layout(pad=1.0)","4100c25d":"S31 = text_Analysis['HQState'].value_counts().reset_index().rename(\n    columns={'index':'HQState','HQState':'Count'}).replace(0,'Unknown_State')\nS31_Top = S31[S31['Count']>29]\nS31_Top['HQState_HQ'] = [s + '_HQ' for s in S31_Top['HQState']]","631a57df":"# create dummy columns by HQ State\nkwdummy = pd.get_dummies(S31_Top['HQState_HQ'].apply(pd.Series).stack()).sum(level=0)\nS31_Top2 = S31_Top.merge(kwdummy,left_index=True,right_index=True,how='left').drop(['Count'],axis=1)\ntext_Analysis = text_Analysis.merge(S31_Top2,on='HQState',how='left').replace(np.nan,0)","943fde0b":"text_columns = list(text_Analysis.columns)\nttests_HQstate=[]\nfor word in text_columns:\n    if word in set(S31_Top['HQState_HQ']):\n        ttest31 = stats.ttest_ind(text_Analysis[text_Analysis[word]>0]['Est_Salary'],\n                                 text_Analysis[text_Analysis[word]==0]['Est_Salary'])\n        ttests_HQstate.append([word,ttest31])\n\nttests_HQstate = pd.DataFrame(ttests_HQstate,columns=['HQState_HQ','R'])\nttests_HQstate['R']=ttests_HQstate['R'].astype(str).replace(['Ttest_indResult\\(statistic=','pvalue=','\\)'],['','',''],regex=True)\nttests_HQstate['Statistic'],ttests_HQstate['P-value']=ttests_HQstate['R'].str.split(', ',1).str\nttests_HQstate=ttests_HQstate.drop(['R'],axis=1).sort_values('P-value',ascending=True)\nttests_HQstate","88cf4fa9":"ttest_HQstate_pass = list(ttests_HQstate[ttests_HQstate['P-value'].astype(float)<0.1]['HQState_HQ'])\nprint(*ttest_HQstate_pass,sep=' + ')","2d1456f0":"HQStateMod = ols(\"Est_Salary ~ PA_HQ + OH_HQ + NJ_HQ + CA_HQ + TX_HQ\",\n               data=text_Analysis).fit()\nprint(HQStateMod.summary())","64669584":"#Remove special characters.\ntext_Analysis['Revenue_USD'] = text_Analysis['Revenue_USD'].replace('[^A-Za-z0-9]+', '_',regex=True).replace('1_5_million','Small_Business').replace('Unknown','RevUnknown')\ntext_Analysis['Size'] = text_Analysis['Size'].replace('[^A-Za-z0-9]+', '_',regex=True).replace(\n    '10000_employees','Large_Firm').replace('Unknown','SizeUnknown')\ntext_Analysis['Sector'] = text_Analysis['Sector'].replace('[^A-Za-z0-9]+', '_',regex=True).replace(['Government','Unknown'],['GovSec','SectorUnknown'])\ntext_Analysis['Industry'] = text_Analysis['Industry'].replace('[^A-Za-z0-9]+', '_',regex=True).replace('Unknown','IndUnknown')\ntext_Analysis['Type of ownership'] = text_Analysis['Type of ownership'].replace('[^A-Za-z0-9]+', '_',regex=True).replace('Unknown','OwnUnknown')","792845e2":"#Rename column name for running regression later.\ntext_Analysis = text_Analysis.rename(columns={\"Easy Apply\":\"Easy_Apply\"})","7dd2dab6":"# create dummy columns by Revenue\nkwdummy = pd.get_dummies(text_Analysis['Revenue_USD'].apply(pd.Series).stack()).sum(level=0)\ntext_Analysis = text_Analysis.merge(kwdummy,left_index=True,right_index=True,how='left').replace(np.nan,0)","7fb3c2fc":"S4 = text_Analysis['Revenue_USD'].value_counts().reset_index().rename(\n    columns={'index':'Revenue_USD','Revenue_USD':'Count'})\nS4_Top = S4[S4['Count']>29]\nS4_Top","ef54b6a3":"#run t-test to see the salary differences by companies' revenue.\ntext_columns = list(text_Analysis.columns)\nttests_rev=[]\nfor word in text_columns:\n    if word in set(S4_Top['Revenue_USD']):\n        ttest4 = stats.ttest_ind(text_Analysis[text_Analysis[word]>0]['Est_Salary'],\n                                 text_Analysis[text_Analysis[word]==0]['Est_Salary'])\n        ttests_rev.append([word,ttest4])\n\nttests_rev = pd.DataFrame(ttests_rev,columns=['Revenue_USD','R'])\nttests_rev['R']=ttests_rev['R'].astype(str).replace(['Ttest_indResult\\(statistic=','pvalue=','\\)'],['','',''],regex=True)\nttests_rev['Statistic'],ttests_rev['P-value']=ttests_rev['R'].str.split(', ',1).str\nttests_rev=ttests_rev.drop(['R'],axis=1).sort_values('P-value',ascending=True)\nttests_rev","1cf56e31":"#Selecting revenues with p-value <0.1 into multiple regression model.\nttest_rev_pass = list(ttests_rev[ttests_rev['P-value'].astype(float)<0.1]['Revenue_USD'])\nprint(*ttest_rev_pass,sep=' + ')","bfc53350":"kwdummy = pd.get_dummies(text_Analysis['Size'].apply(pd.Series).stack()).sum(level=0)\ntext_Analysis = text_Analysis.merge(kwdummy,left_index=True,right_index=True,how='left').replace(np.nan,0)","fa8af7f5":"S5 = text_Analysis['Size'].value_counts().reset_index().rename(\n    columns={'index':'Size','Size':'Count'})\nS5_Top = S5[S5['Count']>29]\nS5_Top","b24e84cf":"text_columns = list(text_Analysis.columns)\nttests_size=[]\nfor word in text_columns:\n    if word in set(S5_Top['Size']):\n        ttest5 = stats.ttest_ind(text_Analysis[text_Analysis[word]>0]['Est_Salary'],\n                                 text_Analysis[text_Analysis[word]==0]['Est_Salary'])\n        ttests_size.append([word,ttest5])\n\nttests_size = pd.DataFrame(ttests_size,columns=['Size','R'])\nttests_size['R']=ttests_size['R'].astype(str).replace(['Ttest_indResult\\(statistic=','pvalue=','\\)'],['','',''],regex=True)\nttests_size['Statistic'],ttests_size['P-value']=ttests_size['R'].str.split(', ',1).str\nttests_size=ttests_size.drop(['R'],axis=1).sort_values('P-value',ascending=True)\nttests_size","d2815b9d":"ttest_size_pass = list(ttests_size[ttests_size['P-value'].astype(float)<0.1]['Size'])\nprint(*ttest_size_pass,sep=' + ')","180c0c2b":"kwdummy = pd.get_dummies(text_Analysis['Sector'].apply(pd.Series).stack()).sum(level=0)\ntext_Analysis = text_Analysis.merge(kwdummy,left_index=True,right_index=True,how='left').replace(np.nan,0)","c8b9f559":"S6 = text_Analysis['Sector'].value_counts().reset_index().rename(\n    columns={'index':'Sector','Sector':'Count'})\nS6_Top = S6[S6['Count']>29]\nS6_Top","fac9f757":"text_columns = list(text_Analysis.columns)\nttests_sec=[]\nfor word in text_columns:\n    if word in set(S6_Top['Sector']):\n        ttest6 = stats.ttest_ind(text_Analysis[text_Analysis[word]>0]['Est_Salary'],\n                                 text_Analysis[text_Analysis[word]==0]['Est_Salary'])\n        ttests_sec.append([word,ttest6])\n\nttests_sec = pd.DataFrame(ttests_sec,columns=['Sector','R'])\nttests_sec['R']=ttests_sec['R'].astype(str).replace(['Ttest_indResult\\(statistic=','pvalue=','\\)'],['','',''],regex=True)\nttests_sec['Statistic'],ttests_sec['P-value']=ttests_sec['R'].str.split(', ',1).str\nttests_sec=ttests_sec.drop(['R'],axis=1).sort_values('P-value',ascending=True)\nttests_sec","4b28188c":"ttest_sec_pass = list(ttests_sec[ttests_sec['P-value'].astype(float)<0.1]['Sector'])\nprint(*ttest_sec_pass,sep=' + ')","42997498":"kwdummy = pd.get_dummies(text_Analysis[text_Analysis['Sector']=='Information_Technology']['Industry'].apply(pd.Series).stack()).sum(level=0)\ntext_Analysis = text_Analysis.merge(kwdummy,left_index=True,right_index=True,how='left').replace(np.nan,0)","0f0a4f87":"S7 = text_Analysis[text_Analysis['Sector']=='Information_Technology']['Industry'].value_counts().reset_index().rename(\n    columns={'index':'Industry','Industry':'Count'})\nS7_Top = S7[S7['Count']>29]\nS7_Top","d6491e76":"text_columns = list(text_Analysis.columns)\nttests_ind=[]\nfor word in text_columns:\n    if word in set(S7_Top['Industry']):\n        ttest7 = stats.ttest_ind(text_Analysis[text_Analysis[word]>0]['Est_Salary'],\n                                 text_Analysis[text_Analysis[word]==0]['Est_Salary'])\n        ttests_ind.append([word,ttest7])\n\nttests_ind = pd.DataFrame(ttests_ind,columns=['Industry','R'])\nttests_ind['R']=ttests_ind['R'].astype(str).replace(['Ttest_indResult\\(statistic=','pvalue=','\\)'],['','',''],regex=True)\nttests_ind['Statistic'],ttests_ind['P-value']=ttests_ind['R'].str.split(', ',1).str\nttests_ind=ttests_ind.drop(['R'],axis=1).sort_values('P-value',ascending=True)\nttests_ind","79b37ab6":"ttest_ind_pass = list(ttests_ind[ttests_ind['P-value'].astype(float)<0.1]['Industry'])\nprint(*ttest_ind_pass,sep=' + ')","f790e550":"kwdummy = pd.get_dummies(text_Analysis['Type of ownership'].apply(pd.Series).stack()).sum(level=0)\ntext_Analysis = text_Analysis.merge(kwdummy,left_index=True,right_index=True,how='left').replace(np.nan,0)","cc0a642b":"S8 = text_Analysis['Type of ownership'].value_counts().reset_index().rename(\n    columns={'index':'Type_of_ownership','Type of ownership':'Count'})\nS8_Top = S8[S8['Count']>29]\nS8_Top","67f746aa":"text_columns = list(text_Analysis.columns)\nttests_own=[]\nfor word in text_columns:\n    if word in set(S8_Top['Type_of_ownership']):\n        ttest8 = stats.ttest_ind(text_Analysis[text_Analysis[word]>0]['Est_Salary'],\n                                 text_Analysis[text_Analysis[word]==0]['Est_Salary'])\n        ttests_own.append([word,ttest8])\n\nttests_own = pd.DataFrame(ttests_own,columns=['Type_of_ownership','R'])\nttests_own['R']=ttests_own['R'].astype(str).replace(['Ttest_indResult\\(statistic=','pvalue=','\\)'],['','',''],regex=True)\nttests_own['Statistic'],ttests_own['P-value']=ttests_own['R'].str.split(', ',1).str\nttests_own=ttests_own.drop(['R'],axis=1).sort_values('P-value',ascending=True)\nttests_own","8b77a972":"ttest_own_pass = list(ttests_own[ttests_own['P-value'].astype(float)<0.1]['Type_of_ownership'])\nprint(*ttest_own_pass,sep=' + ')","882603b1":"ModC = ols(\"Est_Salary ~ JUNIOR + IL + UT + TX + OH + PA + CA + Small_Business + Information_Technology + NJ_HQ + MYSQL_JD\",\n               data=text_Analysis).fit()\n# Rating, Years_Founded, Easy_Apply, PHD, Sector, Size, Type_of_ownership not significant\nprint(ModC.summary())","7f15a2a2":"# Trying different interaction terms.\ntext_Analysis['CA_SB']=text_Analysis['CA']*text_Analysis['Small_Business']\ntext_Analysis['CA_IT']=text_Analysis['CA']*text_Analysis['Information_Technology']\ntext_Analysis['IT_SB']=text_Analysis['Information_Technology']*text_Analysis['Small_Business']\ntext_Analysis['CA_IT_SB']=text_Analysis['Information_Technology']*text_Analysis['Small_Business']*text_Analysis['CA']\ntext_Analysis['CA_NJ_HQ']=text_Analysis['CA']*text_Analysis['NJ_HQ']\ntext_Analysis['SB_NJ_HQ']=text_Analysis['Small_Business']*text_Analysis['NJ_HQ']\ntext_Analysis['IT_NJ_HQ']=text_Analysis['Information_Technology']*text_Analysis['NJ_HQ']\ntext_Analysis['CA_PHD']=text_Analysis['CA']*text_Analysis['PHD_JD']\ntext_Analysis['CA_CA_HQ']=text_Analysis['CA']*text_Analysis['CA_HQ']","1a77ccb3":"ModS = ols(\"Est_Salary ~ CA + CA_PHD + PHD_JD\",\n               data=text_Analysis).fit()\nprint(ModS.summary())","1d18e350":"# Final model considering interaction terms.\nModC = ols(\"Est_Salary ~ JUNIOR + MYSQL_JD + IL + UT + TX + OH + PA + CA + Small_Business + Information_Technology + CA_IT + NJ_HQ\",\n               data=text_Analysis).fit()\n# Rating, Years_Founded, Easy_Apply, PHD, Sector, Size, Type_of_ownership not significant\nprint(ModC.summary())","d409e033":"fig = plt.figure(figsize=(13, 26))\nfig = sm.graphics.plot_partregress_grid(ModC,fig=fig)\nfig.tight_layout(pad=1.0)","799a808d":"# create a separate dataset for CA\ndata_CA = data[data['State']=='CA']","1de7bfba":"pd.set_option('display.max_columns', None)\ndata_CA.describe(include='all')","41f02d67":"sns.set(style='white')\n\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw= {\"height_ratios\": (0.2, 1)},figsize=(13,5))\nmean=data['Est_Salary'].mean()\nmedian=data['Est_Salary'].median()\n\nbph = sns.boxplot(data['Est_Salary'], ax=ax_box).set(xlabel=\"\")\nax_box.axvline(mean, color='k', linestyle='--')\nax_box.axvline(median, color='y', linestyle='-')\n\ndp1 = sns.distplot(data_CA['Est_Salary'],ax=ax_hist, color=\"r\").set(xlabel=\"Est. Salary ($'000)\")\ndp2 = sns.distplot(data['Est_Salary'],ax=ax_hist, color=\"g\").set(xlabel=\"Est. Salary ($'000)\")\nax_hist.axvline(mean, color='k', linestyle='--')\nax_hist.axvline(median, color='y', linestyle='-')\n\nplt.legend({'Mean (All)':mean,'Median (All)':median,'CA':data_CA['Est_Salary'],'All':data['Est_Salary']})\nplt.xlim(0,210)\nplt.xticks(np.arange(0,210,step=10))\nplt.tight_layout() #Adjust the padding between and around subplots\nplt.show()","5110a36c":"# Create a table for heatmap of number of companies with different sizes and revenues\nFirm_Size = data.pivot_table(columns=\"Size\",index=\"Revenue_USD\",values=\"Company Name\",aggfunc=pd.Series.nunique).reset_index()\nFirm_Size = Firm_Size[['Revenue_USD','1 to 50 employees','51 to 200 employees','201 to 500 employees','501 to 1000 employees','1001 to 5000 employees','5001 to 10000 employees','10000+ employees']]\nFirm_Size = Firm_Size.reindex([11,2,9,4,7,10,5,0,1,6,8,3,12])\nFirm_Size = Firm_Size.set_index('Revenue_USD').replace(np.nan,0)\n\n# Create a table for heatmap of number of companies with different sizes and revenues in CA\nFirm_Size_CA = data_CA.pivot_table(columns=\"Size\",index=\"Revenue_USD\",values=\"Company Name\",aggfunc=pd.Series.nunique).reset_index()\nFirm_Size_CA = Firm_Size_CA[['Revenue_USD','1 to 50 employees','51 to 200 employees','201 to 500 employees','501 to 1000 employees','1001 to 5000 employees','5001 to 10000 employees','10000+ employees']]\nFirm_Size_CA = Firm_Size_CA.reindex([11,2,9,4,7,10,5,0,1,6,8,3,12])\nFirm_Size_CA = Firm_Size_CA.set_index('Revenue_USD').replace(np.nan,0)\n\n# Create table for heatmap of salaries by companies with different sizes and revenues\nFirm_Size_Sal = data.pivot_table(columns=\"Size\",index=\"Revenue_USD\",values=\"Est_Salary\",aggfunc=np.mean).reset_index()\nFirm_Size_Sal = Firm_Size_Sal[['Revenue_USD','1 to 50 employees','51 to 200 employees','201 to 500 employees','501 to 1000 employees','1001 to 5000 employees','5001 to 10000 employees','10000+ employees']]\nFirm_Size_Sal = Firm_Size_Sal.reindex([11,2,9,4,7,10,5,0,1,6,8,3,12])\nFirm_Size_Sal = Firm_Size_Sal.set_index('Revenue_USD').replace(np.nan,0)\n\n# Create table for heatmap of salaries by companies with different sizes and revenues in CA\nFirm_Size_CA_Sal = data_CA.pivot_table(columns=\"Size\",index=\"Revenue_USD\",values=\"Est_Salary\",aggfunc=np.mean).reset_index()\nFirm_Size_CA_Sal = Firm_Size_CA_Sal[['Revenue_USD','1 to 50 employees','51 to 200 employees','201 to 500 employees','501 to 1000 employees','1001 to 5000 employees','5001 to 10000 employees','10000+ employees']]\nFirm_Size_CA_Sal = Firm_Size_CA_Sal.reindex([11,2,9,4,7,10,5,0,1,6,8,3,12])\nFirm_Size_CA_Sal = Firm_Size_CA_Sal.set_index('Revenue_USD').replace(np.nan,0)","f7b0b831":"f, axs = plt.subplots(nrows=2,ncols=2, sharey=True,sharex=True, figsize=(13,9))\n\nfs = sns.heatmap(Firm_Size,annot=True,fmt='.0f',annot_kws={\"size\": 12},cmap=\"YlGnBu\", ax=axs[0,0]).set(title=\"Number of Firms Hiring Data Analysts (US)\",xlabel=\"\")\nfsc = sns.heatmap(Firm_Size_CA,annot=True,fmt='.0f',annot_kws={\"size\": 12},cmap=\"YlGnBu\", ax=axs[0,1]).set(title=\"Number of Firms Hiring Data Analysts (California)\",xlabel=\"\",ylabel=\"\")\nfss = sns.heatmap(Firm_Size_Sal,annot=True,fmt='.0f',annot_kws={\"size\": 12},cmap=\"Oranges\",ax=axs[1,0]).set(title=\"Avg. Salaries of Data Analyst Hires (US)\")\nfscs = sns.heatmap(Firm_Size_CA_Sal,annot=True,fmt='.0f',annot_kws={\"size\": 12},cmap=\"Oranges\",ax=axs[1,1]).set(title=\"Avg. Salaries of Data Analyst Hires (California)\",ylabel=\"\")\n\nplt.setp([a.get_xticklabels() for a in axs[1,:]],rotation=45,ha='right')\nplt.tight_layout()\nplt.show()","78e50d46":"ca_sal_by_firm = data_CA.groupby('Company Name')[['Est_Salary']].mean().reset_index()","8ca8147b":"MLHighPay = data_CA[(data_CA['Revenue_USD']=='100-500 million')&(\n    data_CA['Size']=='1001 to 5000 employees')]['Company Name'].value_counts().reset_index().rename(\n    columns={'index':'Company Name','Company Name':'Hires'})","d17d9d1a":"MLHighPay = MLHighPay.merge(ca_sal_by_firm, on='Company Name',how='left')\nMLHighPay = MLHighPay.merge(data_CA[['Company Name','Rating','Headquarters','Type of ownership','Industry','Sector','Years_Founded','Competitors']], on='Company Name',how='left')\nMLHighPay = MLHighPay.drop_duplicates().reset_index(drop=True)\nMLHighPay","98529dd0":"MLHighPay.describe(include='all')","6331782d":"smallHighPay = data_CA[((data_CA['Revenue_USD']=='<1 million')|(\n    data_CA['Revenue_USD']=='1-5 million'))&(\n    data_CA['Size']=='1 to 50 employees')]['Company Name'].value_counts().reset_index().rename(\n    columns={'index':'Company Name','Company Name':'Hires'})","7b0c3d55":"smallHighPay = smallHighPay.merge(ca_sal_by_firm, on='Company Name',how='left')\nsmallHighPay = smallHighPay.merge(data_CA[['Company Name','Rating','Headquarters','Type of ownership','Industry','Sector','Years_Founded','Competitors']], on='Company Name',how='left')\nsmallHighPay = smallHighPay.drop_duplicates().reset_index(drop=True)\nsmallHighPay","ac3053e4":"smallHighPay.describe(include='all')","b2c57af2":"SMHighPay = data_CA[(data_CA['Revenue_USD']=='Unknown')&(\n    data_CA['Size']=='51 to 200 employees')]['Company Name'].value_counts().reset_index().rename(\n    columns={'index':'Company Name','Company Name':'Hires'})","2071d9f6":"SMHighPay = SMHighPay.merge(ca_sal_by_firm, on='Company Name',how='left')\nSMHighPay = SMHighPay.merge(data_CA[['Company Name','Rating','Headquarters','Type of ownership','Industry','Sector','Years_Founded','Competitors']], on='Company Name',how='left')\nSMHighPay = SMHighPay.drop_duplicates().reset_index(drop=True)\nSMHighPay","dd699546":"SMHighPay.describe(include='all')","c32a5ae4":"RevCountCA = data_CA.groupby('Revenue_USD')[['Job Title']].count().reset_index().rename(columns={'Job Title':'Count'}).sort_values(\n    'Count', ascending=False).reset_index(drop=True)\nRevCountCA = RevCountCA.merge(data_CA, on='Revenue_USD',how='left')","f5355a28":"sns.set(style=\"whitegrid\")\nf, (ax_bar, ax_point) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\":(0.6,1)},figsize=(13,7))\nsns.barplot(x='Count',y='Revenue_USD',data=RevCountCA,ax=ax_bar)\nsns.pointplot(x='Est_Salary',y='Revenue_USD',data=RevCountCA, join=False,ax=ax_point).set(ylabel=\"\",xlabel=\"Salary ($'000)\")\n\nplt.tight_layout()","f1ed8f57":"SizeCountCA = data_CA.groupby('Size')[['Job Title']].count().reset_index().rename(columns={'Job Title':'Count'}).sort_values(\n    'Count', ascending=False).reset_index(drop=True)\nSizeCountCA = SizeCountCA.merge(data_CA, on='Size',how='left')","82f53a8b":"sns.set(style=\"whitegrid\")\nf, (ax_bar, ax_point) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\":(0.6,1)},figsize=(13,7))\nsns.barplot(x='Count',y='Size',data=SizeCountCA,ax=ax_bar)\nsns.pointplot(x='Est_Salary',y='Size',data=SizeCountCA, join=False,ax=ax_point).set(ylabel=\"\",xlabel=\"Salary ($'000)\")\n\nplt.tight_layout()","b146e865":"SecCountCA = data_CA.groupby('Sector')[['Job Title']].count().reset_index().rename(columns={'Job Title':'Count'}).sort_values(\n    'Count', ascending=False).head(12).reset_index(drop=True)\nSecCountCA = SecCountCA.merge(data_CA, on='Sector',how='left')","544805db":"sns.set(style=\"whitegrid\")\nf, (ax_bar, ax_point) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\":(0.6,1)},figsize=(13,7))\nsns.barplot(x='Count',y='Sector',data=SecCountCA,ax=ax_bar)\nsns.pointplot(x='Est_Salary',y='Sector', join=False,data=SecCountCA,ax=ax_point).set(ylabel=\"\",xlabel=\"Salary ($'000)\")\n\nplt.tight_layout()","f2ff6abc":"OwnCountCA = data_CA.groupby('Type of ownership')[['Job Title']].count().reset_index().rename(columns={'Job Title':'Count'}).sort_values(\n    'Count', ascending=False).reset_index(drop=True)\nOwnCountCA = OwnCountCA.merge(data_CA, on='Type of ownership',how='left')","b7d8f0c2":"sns.set(style=\"whitegrid\")\nf, (ax_bar, ax_point) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\":(0.6,1)},figsize=(13,7))\nsns.barplot(x='Count',y='Type of ownership',data=OwnCountCA,ax=ax_bar)\nsns.pointplot(x='Est_Salary',y='Type of ownership',data=OwnCountCA, join=False,ax=ax_point).set(ylabel=\"\",xlabel=\"Salary ($'000)\")\n\nplt.tight_layout()","698a569e":"# Regression Analysis","262bf33e":"* From the graph, at that time New York was hiring the most analysts, with est. salary about 70K.\n* San Jose, CA is with the highest est. salary and the largest variance.","1e75bbe2":"## Explore the Data","680bc37f":"It's statistically significant that larger firms tend to pay 2K less than average companies.","ab323072":"* A big portion of source of analysts hiring are from giants (10K+ employees & USD10B+ revenues), but they don't necessarily pay more.\n* There's high demand among financially unpublic firms (Revenue 'Unknown'). And they pay similar or higher salaries to analysts than giants.\n* Medium-large firms (especially 1K-5K employees & USD100-500M revenues), small businesses (especially <50 employees & < USD5M revenues) and small-medium businesses (51-200 employees with revenues unknown) pay more.\n* Companies in CA obviously pay more.","4f1281ab":"### Characters of those high-paying small-medium businesses in CA\n* Most are private companies (51-200 employees with revenues unknown). \n* Almost Half are IT-related.\n* Avg. hires: 1.1; Avg. rating 4.2 (higher than ttl. avg. 3.1); Avg. company age: **9.9** (ttl. avg. 40)","7acfec0a":"# Exploratory Analysis","383d5c1c":"## Who are those high-paying small-medium businesses in CA?","923a92df":"Only NGOs tend to pay a bit less.","e214c34c":"Another view the see the distribution of salary min, max, mean and median: \n* X-axis: the id(index) of all observations sorted by ascending order of min salaries.\n* The min-max range estimates are more stable when min_salary ranges from 37K-50K.\n* Est. Salaries above 112K are outliers.","055406dc":"Revenue '0' are those NaN values replaced for making dummy columns and are to be ignored.","793219a8":"## Distribution of Company Ratings","e54b877e":"## Hires and Salary Estimates by Sizes (CA)","6c3650e5":"# [Heatmap] Number, Size and Salary of Hiring Companies (CA vs All)","c8b6d22b":"## Hires and Salary Estimates by Revenues (CA)","4650985e":"### Hires and Salary Estimates by Types of Ownership (CA)","fc39a2ef":"Again created a bar chart & error bar combo intended to see the salaries and counts, but not very effective (most sample sizes are under 30) as there are too many ways in presenting a position's name even some wordings are standardized. \n\n### Regression model may be a better approach: some certain keywords in job title\/desc may be correlated with salary pay.","79bd7dc9":"## Data Cleaning","3e43d870":"## Correlation: Job Title Keywords vs Salary","c8cf950d":"In CA, a dominant portion of analysts positions are largely released by companies with revenues information unknown.","4c2109b0":"## Hires and Salary Estimates by Job Titles","ea8f6547":"Now you can see there are lots of missing values in the dataset. Most positions don't support the Easy Apply function. Competitors are not identified for majority of the companies.","b2e40c5c":"Data includes job title , salary estimation , job description , rating ,company name , location and many more ...\n\"Easy Apply\" should be the function that applicants can directly apply a job directly through 3rd party jobboard (e.g. Glassdoor, LinkedIn...) without logging into the hiring company's career site.","9df55e3d":"### Characters of those high-paying medium-large businesses in CA\n* Most are private companies (1K-5K employees & USD100-500M revenues). \n* Half are Staffing & Outsourcing businesses.\n* Avg. hires: 1.3; Avg. rating 3.7 (higher than ttl. avg. 3.1); Avg. company age: 22.6 (ttl. avg. 40)","e77ce566":"### [Create Size Variables for Multiple Regression]","fc0c4a38":"### Characters of those high-paying small businesses in CA \n* Most are private companies (<50 employees & < USD5M revenues). \n* Industry & sector are more fregmented, but IT-related is still the top.\n* Avg. hires: 1.3; Avg. rating 4.1 (higher than ttl. avg. 3.1); Avg. company age: **13.17** (ttl. avg. 40)","46ea97bc":"## Hires and Salary Estimates by Firms (Top 20)","0e0d3a68":"## Hires and Salary Estimates by Job Location Cities (Top 20)","9463affe":"# Upvote if you like my work!","b694ee38":"## Hires and Salary Estimates by Size","4cb14aef":"## Hires and Salary Estimates by Sector (Top 12)","c84bca6e":"* The seniorities seem to be more relevant than functional keywords such as marketing or finance.\n* Interestingly, \"business analysts\" are paid less than \"data analysts\", \"analysts\" or other forms of analysts. That such positions are less based in CA in this dataset could be the cause (15% versus 28% overall average). Indeed it didn't make it to the final model where job location is controlled.\n* This model can only explain less than 1% of variations in salaries.\n\n*Also tried interaction terms between business_analyst and seniorities but nothing new found.*","af878f63":"To have an overview on the number, size and salary of those hiring companies and compare the outcomes between CA and all US, I want to create a heatmap.","d92b64b4":"The regional difference (job location) is the most crucial factor to the salary variation.","4cef4da0":"Size '0' are those NaN values replaced for making dummy columns and are to be ignored.","4f7a8196":"### Hires and Salary Estimates by Sectors (CA)","038c93e7":"* Regional difference (job location) is still the most deciding factor to salary variations.\n* IT companies don't necessarily pay more, but California's IT companies do! (CA_IT)\n* Small Businesses (with revenues from 1 to 5 million USD) tend to pay more.\n* Companies headquartered in NJ tend to pay more (NJ_HQ).\n* Analysts with MYSQL experience get higer pay.\n* Intriguingly PHD didn't make it to the final model, nor did the interaction term 'CA_PHD'.","7aa1f880":"# Data Preparation","04c9b463":"SAS analysts get paid lower? Data shows these are more probable to be junior positions and\/or at older (avg. company age 38) firms in Texas, one of the most low-paying states. It didn't make it to the final model, where job location is controlled.\n\n*Also tried interaction terms between keywords but nothing new found.*","00a0b830":"### Larger firms do not necessarily pay more to data analysts.\nInstead, high demands with higher salaries are found in clusters such as follows. These high-paying firms are mostly private companies with a lot higher ratings on Glassdoor and are much younger.\n\n* **Mid-large firms (employees 1K-5K  & revenues USD100-500M)** \n* **Small businesses (employees <50 & < revenues USD5M)**\n* **Small-medium businesses (51-200 employees with revenues unknown)**\n![image.png](attachment:image.png)","53be3070":"* From the graph, at that time Staffigo was hiring nearly 60 analysts but with 4th lowest est. salary.\n* Apple is with the highest est. salary but the variance is large (the lines represent 95% confidence interval)\n* Most sample sizes are lower than 30, so we'd better be conservative about the est. salaries by firms.","eb230928":"When viewing by states, the positive correlation between demand and analysts' salary becomes more clear. Companies at CA, where the highest demand is, are the most generous.","88e30f97":"## Correlation: Job Description vs Salary","f7e45c33":"## Hires and Salary Estimates by Type of Ownership","acd59391":"## Who are those high-paying small businesses in CA?","ec22cae6":"As some of the columns contains -1 or '-1.0' or '-1' etc . We need to clean this(This is kind of null values)","302e9778":"\u2193Preparing for visualisation","e3eec25f":"# More on California Analyst Salary (Sector & Type of Ownership)","9ef2ae1f":"![image.png](attachment:image.png)","bd1edbb2":"## Hires and Salary Estimates by Headquarters Location (Top 20)","5ce8fa5b":"\u2193Spoiler: later found out that only IT is significant in the final multiple regression model, so dig deeper into the subcategory \"Industry\" under the IT sector.","fd3958b4":"## Who are those high-paying medium-large businesses in CA?","8abf4e3c":"### SQL & Excel - basics; Python & technical degrees - nice to have; MYSQL - bonused in the IT & Consulting firms niche in California.\n60% of analyst jobs require\/prefer SQL experience; 40% require\/prefer intensive Excel skills; 30% require\/prefer Python experience, Master degrees or Computer Science degrees. **Less than 4% of positions require\/prefer experience with MYSQL, but they come with higher pay (+USD 5K)**.\n![image.png](attachment:image.png)","4d3c19cc":"Private companies' demand are higher, and the salary offers are comparable with public firms.","a48b1adc":"Most industries under the Information Technology sector pay more.","3bc88bfa":"The variances in salaries by headquarters locations is that larger than job location, implying that other factors such as job location might have greater impact on salary variations. We'll see that in the regression model.","7ce6ac29":"## Hires and Salary Estimates by Job Location States","cd36c1c5":"### [Create IT Industries Variables for Multiple Regression]","f4c3113b":"## Distribution of Company Ages","44fa2c84":"### [Create Sector Variables for Multiple Regression]","f11defc9":"Compared with that of the US as a whole, the salary distribution in CA shifts to the right, indicating overall higher salary payments.","74f0ccba":"I want to know the companies actively hiring Data Analysts and the estimated salaries they offer.","f110cd34":"# Deeper Look at California - Salary Distribution","88b78b9f":"# Final Regression Model (California rocks!)","6c185e02":"## More Variables: Revenue, Size, Sector, Industry and Type of Ownership","c1470323":"\u2193The combined regression model before considering interaction terms.","0cc34869":"### [Create Revenue Variables for Multiple Regression]","5c67980b":"* By the modes of distribution, we can say Data Analysts's minimum salary is 45K and maximum 75K.\n* However, Max Salary's distribution is more spread.","10a61779":"## Correlation: HQ Location (State) vs Salary","eebc31a3":"## Salary Distribution of All Data Analysts","1f10f205":"Focus only on Est. Salary and disregarding Min & Max. Both mean and median are around 70K.","cc5b7fda":"## Insights from Regression\n### While salaries for PHD analysts in other states stand out, it is not the case in California. \nThe final regression model shows simply being in California can raise salary. It does not matter whether the position require\/prefer a PHD degree. Possible explanations I came up as follows. Your thought? \n1. Too many job applicants in CA have PHD degrees, so HR can easily hire PHD without requiring it; and PHD's are hired with premium salaries regardless if the job description requires so.\n2. Non-PHD job applicants in CA possess other expertises as valuable as PHD degrees.\n\n### An IT job is not a high-pay guarantee unless it is based in California.\nIn terms of industries and sectors, top demands come from IT, business services (62% are staffing & outsourcing firms) , finance and healthcare (including biotech & pharmaceuticals). However, only positions in California from IT companies are significantly paid higher (USD 25K higher holding other variables constant).\n\n### New Jersey based companies are smaller but more generous.\nWhile headquarters' locations are not so much correlated with salaries as job locations are, those 74 **companies headquartered in New Jersey pay USD 5K higher** holding other variables constant. The small businesses (employees <50 & < revenues USD5M) share among NJ-based companies is 3 times higher than the average.\n\n### Other than a junior position, the job title does not matter.\nWhile a 'Junior' position is well expected to receive lower salaries (3K-8K lower), the correlations with other seniority wordings such as 'Lead', 'Senior', 'I', 'II' and 'III' are seen and expected in initial models but not significant after controlling job locations. The correlation with functional keywords such as marketing and financial are not significant as well.","8e2a639d":"Again, the correlation between firm size and salary doesn't seem to be positive.","b0edc1d0":"## Hires and Salary Estimates by Revenue","0b1ee281":"Weird to see Medium_Business (50_100_million) pays 2K less than average firms as Small_Business pays more, but we can check if it's still valid in multiple regression later.","e0576a65":"## Correlation: Job Location (State) vs Salary","083aa67d":"# About\n## Dataset\nThis dataset was created by [picklesueat](https:\/\/github.com\/picklesueat\/data_jobs_data) and contains more than 2000 job listing for data analyst positions (all assumed to be open positions at the time the dataset was published in July 2020), with features such as:\n\n* Salary Estimate\n* Location\n* Company Rating\n* Job Description\n  and more.\n  \n## The Work\nThis is my first Python data visualisation project on Kaggle. I began with adopting Yashvi Patel's library and dataset import, data exploration and cleaning and some of the visualisations. And then I incorporated different visualisation techniques and regressions from various sources.\n\n## Objectives\n* What kind of jobs get higher salaries? (Job Title, Job Description, EasyApply)\n* What kind of companies pay more? (Rating, Company, Size, *Years established (now - Founded)*, Type of ownership, Industry & Sector, Revenue)\n* Does job\/headquarters location matter to salaries?\n\n## Methodologies\n\n1. Exploratory Data Analysis (distribution, boxplot, barcharts, errorbars, heatmaps, scatterplots...etc.)\n2. T-test\n3. Multiple Regression\n\n## Limitations and Assumptions\n\n* The results only reflet the outcome at the time the dataset was published, which is pressumed to be July 2020. Seasonal variation is disregarded (not a time-series data).\n* Somehow only 27 positions are found 'remote' in this dataset. The impact of pandemic (more jobs becoming remote) is unclear.\n* The salary estimates come from Glassdoor, which may not reflect the actual salaries.\n* The dataset is assumed to reflect the traits of the actual job market.\n* The salaries are nominal, not adjusted by living costs or consumer price index.","13539b02":"# Key Insights\n\n### Job location is the top deciding factor of data analysts' salary variation in this dataset (R-Square 26%). \nOverall average estimated salary of a data analyst is USD 72K, and median is USD 69K, whereas a data analyst in California earns USD 88K on average, USD 81K as median. Salaries in San Jose and San Francisco are proven by graphs to be even higher.\n![image.png](attachment:image.png)","3356e6d2":"### [Create Type of Ownership Variables for Multiple Regression]","ddba177b":"## Import Libraries and Dataset","3af588dc":"Companies headquartered in CA or NJ pay more.","da3c18fc":"This graph shows that big firms and finanically public firms do not seem to pay more than small and medium businesses for analysts.","a17934bb":"\u2193Here's a series operations to create and clean a dataset to dissect texts in Job Titles and Descriptions","9c00f9db":"Biotech, Pharmaceuticals and IT sectors obvious pay more, whereas Finance and Government sectors pay less."}}