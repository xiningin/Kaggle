{"cell_type":{"2c5555f9":"code","d62c13e2":"code","62da5300":"code","df6f804c":"code","b954c89f":"code","61d7422d":"code","f7fb27b6":"code","1dae51fb":"code","f307d589":"code","96e02382":"code","dbf42e0d":"code","85e2b4d5":"code","c27211dc":"code","16736473":"code","c08583d4":"code","77123ed6":"code","3478e19d":"code","001089cd":"code","23322a7b":"code","d617bfae":"code","8af36f60":"code","001662af":"code","30faa7ee":"code","f422e2fb":"code","18908342":"code","bb3b23bf":"code","da100584":"code","3103952d":"code","e7f53f08":"code","3f1ac89d":"code","bce4866b":"markdown","dfc1da98":"markdown","737ab3ff":"markdown"},"source":{"2c5555f9":"#From the discussion board (https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/291823)\n\n!pip install scikit-learn-intelex\nfrom sklearnex import patch_sklearn\npatch_sklearn()","d62c13e2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom functools import partial\nimport optuna\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.model_selection import StratifiedKFold\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport gc\nfrom scipy import stats","62da5300":"df_train = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv')","df6f804c":"# from the discussion board (https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/291844)\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","b954c89f":"#df_train = reduce_mem_usage(df_train)\n#df_test = reduce_mem_usage(df_test)","61d7422d":"df_train.head(5)","f7fb27b6":"#nan values in dataframe?\ndf_train.isna().sum()","1dae51fb":"df_train.dtypes","f307d589":"def is_categorical(data,column):\n    if len(data[column].unique()) <= 15:\n        print(str(column) + \": \" + str(data[column].unique()))\n    return None\n    \n    ","96e02382":"columns = df_train.columns.to_list()\nfor column in columns:\n    is_categorical(df_train,column)\n    ","dbf42e0d":"#Soil_Type7 and SoilType15 has only zero values. Need to delete those two columns.\n\ndf_train = df_train.drop(['Soil_Type7','Soil_Type15'],axis=1)\ndf_test = df_test.drop(['Soil_Type7','Soil_Type15'],axis=1)\n","85e2b4d5":"df_train.tail(10)","c27211dc":"# Cover_Type 5 was only one sample in this data.  \ndf_train = df_train[df_train.Cover_Type != 5]","16736473":"targets = df_train.Cover_Type\nfeatures = df_train.drop(['Cover_Type'],axis=1)\n\n","c08583d4":"#Extra feature engineering from the discussion board https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/293373\n\nfeatures[\"Aspect\"][features[\"Aspect\"] <0] +=360\nfeatures[\"Aspect\"][features[\"Aspect\"] >359]-=360\n\ndf_test[\"Aspect\"][df_test[\"Aspect\"] <0] +=360\ndf_test[\"Aspect\"][df_test[\"Aspect\"] >359] -=360\n\n\nfeatures.loc[df_train[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\ndf_test.loc[df_test[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n\nfeatures.loc[df_train[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\ndf_test.loc[df_test[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n\nfeatures.loc[df_train[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\ndf_test.loc[df_test[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n\nfeatures.loc[df_train[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\ndf_test.loc[df_test[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n\nfeatures.loc[df_train[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\ndf_test.loc[df_test[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n\nfeatures.loc[df_train[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\ndf_test.loc[df_test[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\n\n","77123ed6":"#extra feature engineering from the discussion board. \n#sum of soil_type and wilderness_ares https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/292823\n\n\nfeature_list = features.columns\nsoil_features = [x for x in feature_list if x.startswith(\"Soil_Type\")]\nfeatures['soil_type_count'] = features[soil_features].sum(axis=1)\ndf_test['soil_type_count'] =df_test[soil_features].sum(axis=1)\n\nwilderness_features= [x for x in feature_list if x.startswith('Wilderness')]\nfeatures['wilderness_area_count']=features[wilderness_features].sum(axis=1)\ndf_test['wilderness_area_count'] = df_test[wilderness_features].sum(axis=1)","3478e19d":"#some more features engineering\n# from this discussion https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/293612\n\n\nfeatures['Euclidean_Distance_to_Hydrology'] =  ((features['Horizontal_Distance_To_Hydrology'])**2 + (features['Vertical_Distance_To_Hydrology'])**2)**0.5\n\n\nfeatures['Manhattan_Distance_to_Hydrology'] = np.abs(features['Horizontal_Distance_To_Hydrology']) + np.abs(features['Vertical_Distance_To_Hydrology'])\n\n\ndf_test['Euclidean_Distance_to_Hydrology'] =  ((df_test['Horizontal_Distance_To_Hydrology'])**2 + (df_test['Vertical_Distance_To_Hydrology'])**2)**0.5\n\ndf_test['Manhattan_Distance_to_Hydrology'] = np.abs(df_test['Horizontal_Distance_To_Hydrology']) + np.abs(df_test['Vertical_Distance_To_Hydrology'])","001089cd":"#Scaling the values.\n\n\nfrom sklearn import preprocessing\nscaler = preprocessing.MinMaxScaler()\n#scaler = preprocessing.StandardScaler()\n#scaler = preprocessing.RobustScaler()\n\nnumeric_features = features.columns[1:11].to_list()\n\nfor i in ['soil_type_count',\n       'wilderness_area_count', 'Euclidean_Distance_to_Hydrology',\n       'Manhattan_Distance_to_Hydrology']:\n    numeric_features.append(i)\n     \n\n\nfeatures[numeric_features] = scaler.fit_transform(features[numeric_features])\ndf_test[numeric_features] = scaler.transform(df_test[numeric_features])","23322a7b":"\nencoder = LabelEncoder()\ntargets[:] = encoder.fit_transform(targets[:])","d617bfae":"del df_train","8af36f60":"#train_X,val_X,train_y,val_y = train_test_split(features,targets,random_state=1)\n","001662af":"from sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\n\ndef objective(trial,X,y, name='xgb'):\n    params = param = {\n        'objective':'multi:softmax',\n        'tree_method':'gpu_hist',  \n        'lambda': trial.suggest_loguniform(\n            'lambda', 1e-3, 10.0\n        ),\n        'alpha': trial.suggest_loguniform(\n            'alpha', 1e-3, 10.0\n        ),\n        'colsample_bytree': trial.suggest_categorical(\n            'colsample_bytree', [0.5,0.6,0.7,0.8,0.9,1.0]\n        ),\n        'subsample': trial.suggest_categorical(\n            'subsample', [0.6,0.7,0.8,1.0]\n        ),\n        'learning_rate': trial.suggest_categorical(\n            'learning_rate', [0.25,0.3,0.35,0.4,0.2,0.1,0.09, 0.01]\n        ),\n        'n_estimators': trial.suggest_categorical(\n            \"n_estimators\", [150, 200, 300, 3000]\n        ),\n        'max_depth': trial.suggest_categorical(\n            'max_depth', [4,5,7,9,11,13,15,17]\n        ),\n        'random_state': 42,\n        'min_child_weight': trial.suggest_int(\n            'min_child_weight', 1, 300\n        ),\n        'eval_metric':'auc',\n        #'num_of_classes':4,\n        \n    }\n\n    model =  XGBClassifier(**params)\n    model.fit(train_X,train_y,eval_set=[(val_X,val_y)],early_stopping_rounds=50,verbose=False)\n\n\n    train_score = np.round(accuracy_score(train_y, model.predict(train_X)), 5)\n    test_score = np.round(accuracy_score(val_y, model.predict(val_X)), 5)\n                  \n    print(f'TRAIN ROC : {train_score} || TEST ROC : {test_score}')\n                  \n    return test_score","30faa7ee":"#%%time\n#optimize = partial(objective,X=train_X,y=train_y)\n\n#study_lgbm = optuna.create_study(direction ='maximize')\n#study_lgbm.optimize(optimize,n_trials=30)\n","f422e2fb":"#print(f\"\\tBest value (roc): {study_lgbm.best_value:.5f}\")\n#print(f\"\\tBest params:\")\n\n#for key, value in study_lgbm.best_params.items():\n#   print(f\"\\t\\t{key}: {value}\")\n\n\n#Best value (roc): 0.96218\n#\tBest params:\n#\t\tlambda: 1.582390097702773\n#\t\talpha: 0.00641014056102776\n#\t\tcolsample_bytree: 0.8\n#\t\tsubsample: 1.0\n#\t\tlearning_rate: 0.09\n#\t\tn_estimators: 3000\n#\t\tmax_depth: 13\n#\t\tmin_child_weight: 39\n\n","18908342":"params = {\n'lambda': 1.582390097702773,\n'alpha': 0.00641014056102776,\n'colsample_bytree': 0.8,\n'subsample': 1.0,\n'learning_rate': 0.09,\n'n_estimators': 3000,\n'max_depth': 13,\n'min_child_weight': 39\n}","bb3b23bf":"#referred to this note book (6)\n\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\npreds = []\n\nkf = StratifiedKFold(n_splits=20,random_state=48,shuffle=True)\nacc =[]\nn=0 \n\nfor trn_idx, test_idx in kf.split(features, targets):\n    X_tr, X_val = features.iloc[trn_idx], features.iloc[test_idx]\n    y_tr,y_val= targets.iloc[trn_idx] , targets.iloc[test_idx]\n    \n    model = XGBClassifier(**params,objective= 'multi:softmax', tree_method='gpu_hist')\n    model.fit(X_tr,y_tr,eval_set = [(X_val,y_val)],early_stopping_rounds =100,verbose =False)\n    \n    preds.append(model.predict(df_test))\n    acc.append(accuracy_score(y_val,model.predict(X_val)))\n    \n    \n    print(f\"fold: {n+1} , accuracy: {round(acc[n]*100,3)}\")\n    n+=1\n    \n    del X_tr,X_val,y_tr,y_val\n    gc.collect()\n","da100584":"print(f\"the mean Accuracy is : {round(np.mean(acc)*100,3)} \")","3103952d":"prediction = stats.mode(preds)[0][0]","e7f53f08":"predictions = encoder.inverse_transform(prediction)","3f1ac89d":"index = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\")\nindex['Cover_Type'] = predictions\nindex.to_csv('submission.csv',index=False)","bce4866b":"# 1. Data Explanatory Analysis and Cleaning\n\nThis is my third time to join Tabular Play Ground Series. I did not put much time for data explanatory analysis in the last two competitions. I will put some more effort on it this time to do effective feature engineering later on. To do so, I referred to [Machine Learning Explainability](https:\/\/www.kaggle.com\/learn\/machine-learning-explainability) course on Kaggle. \n","dfc1da98":"# Making Model and Predict \n","737ab3ff":"# Data Description\n\nFor this competition, you will be predicting a categorical target based on a number of feature columns given in the data. The data is synthetically generated by a GAN that was trained on a the data from the Forest Cover Type Prediction. This dataset is (a) much larger, and (b) may or may not have the same relationship to the target as the original data.\n\nPlease refer to this data page for a detailed explanation of the features.\n\nFiles\n* train.csv - the training data with the target Cover_Type column\n* test.csv - the test set; you will be predicting the Cover_Type for each row in this file (the target integer class)\n* sample_submission.csv - a sample submission file in the correct format\n\n\n*******From the competition data page.***************\n"}}