{"cell_type":{"1c521c4d":"code","95207ca0":"code","40d04286":"code","990dd570":"code","4c91a571":"code","ee595808":"code","b60dca59":"code","b1fd67dc":"code","324b105e":"code","2ceb5ba7":"code","8df9f6fc":"code","c7d3bf05":"code","45073ab2":"code","c416f9aa":"code","29105540":"code","c849be9a":"code","ab0ed372":"code","4be36039":"code","5d6911df":"code","8bc9057d":"code","dd4e20f8":"code","d0144190":"code","dc627084":"code","a7e14482":"code","df6e5582":"code","2724ea56":"code","70a31569":"code","95f89a30":"code","989a6ba6":"code","6a547165":"code","c5611a19":"code","de1b66a9":"code","f456addf":"code","03c3947e":"code","a3ed51db":"code","cfdbe897":"markdown","b342cce5":"markdown","3b4002a8":"markdown","f79b4909":"markdown","c084288e":"markdown","01d2ba3b":"markdown","7cb3be05":"markdown","02ed384a":"markdown","020130be":"markdown","c2ded5b7":"markdown","9d1ee35f":"markdown","7fce33ec":"markdown","bbffcb37":"markdown","35a7e9b7":"markdown","edbaa881":"markdown"},"source":{"1c521c4d":"%%time\n!pip install ..\/input\/lama-whl\/efficientnet_pytorch-0.7.0\/dist\/efficientnet_pytorch-0.7.0.tar ..\/input\/lama-whl\/log_calls-0.3.2\/log_calls-0.3.2\/ ..\/input\/lama-whl\/sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl ..\/input\/lama-whl\/sphinxcontrib_htmlhelp-1.0.3-py2.py3-none-any.whl ..\/input\/lama-whl\/sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl ..\/input\/lama-whl\/sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl ..\/input\/lama-whl\/sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl ..\/input\/lama-whl\/sphinxcontrib_serializinghtml-1.1.4-py2.py3-none-any.whl ..\/input\/lama-whl\/importlib_metadata-1.7.0-py2.py3-none-any.whl ..\/input\/lama-whl\/poetry_core-1.0.3-py2.py3-none-any.whl ..\/input\/lama-whl\/imagesize-1.2.0-py2.py3-none-any.whl ..\/input\/lama-whl\/docutils-0.16-py2.py3-none-any.whl ..\/input\/lama-whl\/alabaster-0.7.12-py2.py3-none-any.whl ..\/input\/lama-whl\/snowballstemmer-2.1.0-py2.py3-none-any.whl ..\/input\/lama-whl\/Sphinx-3.5.4-py3-none-any.whl ..\/input\/lama-whl\/sphinx_autodoc_typehints-1.11.1-py3-none-any.whl ..\/input\/lama-whl\/nbsphinx-0.8.0-py3-none-any.whl ..\/input\/lama-whl\/nbsphinx_link-1.3.0-py2.py3-none-any.whl ..\/input\/lama-whl\/cssselect-1.1.0-py2.py3-none-any.whl ..\/input\/lama-whl\/pyquery-1.4.3-py3-none-any.whl ..\/input\/lama-whl\/chuanconggao-html2json-0.2.4.1-0-g99d7fbb\/chuanconggao-html2json-99d7fbb\/ ..\/input\/lama-whl\/json2html-1.3.0\/json2html-1.3.0 ..\/input\/lama-whl\/lightgbm-2.3.1-py2.py3-none-manylinux1_x86_64.whl ..\/input\/lama-whl\/AutoWoE-1.2.1-py3-none-any.whl ..\/input\/lama-whl\/LightAutoML-0.2.14-py3-none-any.whl","95207ca0":"%%time\n!pip install \/kaggle\/input\/nvidia-dali\/nvidia_dali_cuda100-1.1.0-2239998-py3-none-manylinux2014_x86_64.whl","40d04286":"%%time\nimport sys\n!cp -r ..\/input\/clip-pretrained\/CLIP\/CLIP-main \/tmp\/\n\n# Kaggle likes to unpack .gz files in datasets... so we have to pack it back\n!gzip -c \/tmp\/CLIP-main\/clip\/bpe_simple_vocab_16e6.txt > \/tmp\/CLIP-main\/clip\/bpe_simple_vocab_16e6.txt.gz\nsys.path.append('\/tmp\/CLIP-main')\n!cp -r \/tmp\/CLIP-main\/clip \/opt\/conda\/lib\/python3.7\/site-packages\/\n\n!pip install ..\/input\/clip-pretrained\/ftfy-5.9\/ftfy-5.9 \\\n             ..\/input\/clip-pretrained\/torchvision-0.8.2+cu110-cp37-cp37m-linux_x86_64.whl \\\n             ..\/input\/clip-pretrained\/torch-1.7.1+cu110-cp37-cp37m-linux_x86_64.whl ","990dd570":"EFFNET_PATH = '..\/input\/shopee-effnet\/effnet_b2.pth'\nID_BERT_PATH = '..\/input\/shopee-id-bert\/'\nML_PATH = '..\/input\/shopee-ml\/'\n\nDEBUG = False\n\nif DEBUG:\n    test = 'train'\nelse:\n    test = 'test'","4c91a571":"import sys\n\nsys.path.append('..\/input\/shopee-effnet\/efficientnet_pytorch-0.7.0\/')\n\nimport numpy as np\nimport pandas as pd\nimport joblib\n\nimport tqdm\n\nimport torch\nimport os\nimport gc\nimport networkx as nx\nimport treelite\nimport json\nimport cuml\nimport clip\nimport catboost as cb\n\nimport nvidia.dali.ops as ops\nimport nvidia.dali.types as types\n\nfrom pandas import Series, DataFrame\nfrom efficientnet_pytorch import EfficientNet\nfrom transformers import AlbertTokenizer, AlbertModel, BertTokenizer, BertModel, \\\n        AutoModel, AutoTokenizer, BertTokenizerFast\n\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML\nfrom lightautoml.tasks import Task\n\nfrom copy import deepcopy\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\n\nfrom nvidia.dali.pipeline import Pipeline\nfrom nvidia.dali.plugin.pytorch import DALIGenericIterator\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.cuda.amp import autocast","ee595808":"def get_validation_folds(df, nfolds=5, random_state=42):\n    \"\"\"\n    Function to create validation folds. Split not only by label group, but also by title, image, phash\n    \"\"\"\n    np.random.seed(random_state)\n    G = nx.Graph()\n\n    for col in ['label_group', 'title', 'image_phash', 'image']:\n\n        agg = df.groupby(col)['posting_id'].agg(list).tolist()\n        for p in agg:\n            nx.add_path(G, p)\n\n    cc = {}\n    for n, c in enumerate(nx.connected_components(G)):\n        val = min(c)\n        for x in c:\n            cc[x] = val\n\n    group_idx = df['posting_id'].map(cc).values\n    groups = np.unique(group_idx)\n    np.random.shuffle(groups)\n\n    split = np.array_split(groups, nfolds)\n\n    folds = np.zeros(df.shape[0], dtype=np.int32)\n\n    for n, s in enumerate(split):\n        folds[np.isin(group_idx, s)] = n\n\n    return folds\n\n\ndef phash_to_bag(x):\n    \"\"\"\n    Transform single phash to OHE representation\n    \"\"\"\n    res = np.zeros(16 * 16, dtype=np.int32)\n\n    for n, i in enumerate(x):\n        res[int(i, 16) + n * 16] += 1\n\n    return res \/ ((res ** 2).sum() ** .5)\n\n\ndef get_phash_embed(df):\n    \"\"\"\n    Transform df phash to OHE representation\n    \"\"\"\n    embed = np.stack(df['image_phash'].map(phash_to_bag).tolist())\n    return embed.astype(np.float32)\n\n\ndef union_pred(*preds):\n    \"\"\"\n    Union preds from different embeds\n    \"\"\"\n    res = []\n\n    for pp in zip(*preds):\n        row = []\n        for p in pp:\n            row.extend(list(p))\n\n        row = list(set(row))\n        res.append(row)\n\n    return res\n\ndef get_dist_features(D):\n    \"\"\"\n    Get density features for embed point\n    \"\"\"\n    features = []\n    for i in [2, 3, 5, 10, 20, 50]:\n        features.append(D[:, 1: i].mean(axis=1))\n\n    for i in [.5, .6, .7, .8, .9, .95, .97, .99]:\n        features.append((D >= i).sum(axis=1))\n\n    return np.stack(features, axis=1).astype(np.float32)\n\n\ndef get_paired_indexes(pred, folds=None):\n    \"\"\"\n    Create points pairs candidates. First half of pairs is left\/right pairs, second part is reflection - right\/left\n    \"\"\"\n    left, right, fold = [], [], []\n\n    if folds is None:\n        folds = np.zeros(len(pred))\n\n    added_pairs = set(zip(left, right))\n\n    for n, (pp, f) in enumerate(zip(pred, folds)):\n        for p in pp:\n            if n != p and (p, n) not in added_pairs:\n                left.append(n)\n                right.append(p)\n                fold.append(f)\n                added_pairs.add((n, p))\n\n    # add reversed pairs\n    lc, rc = left.copy(), right.copy()\n    left.extend(rc)\n    right.extend(lc)\n    fold.extend(fold.copy())\n\n    return np.array(left), np.array(right), np.array(fold)\n\n\ndef get_pairwise_dist(left, right, embed, folds=None, batch_size=20000):\n    \"\"\"\n    Compute pairwise distance features\n    \"\"\"\n    res = np.zeros((left.shape[0], 6), dtype=np.float32)\n    embed = torch.from_numpy(embed).cuda()\n\n    q_int = (np.array([0.25, 0.975]) * (embed.shape[1] - 1)).astype(np.int32)\n    f = None\n\n    for i in range(0, left.shape[0], batch_size):\n        left_embed, right_embed = embed[left[i: i + batch_size]], embed[right[i: i + batch_size]]\n        # in case of out-of-fold embeddings\n        if folds is not None and len(embed.shape) == 3:\n            f = torch.from_numpy(folds[i: i + batch_size].astype(np.int64)\n                                 ).view(-1, 1, 1).cuda().repeat(1, embed.shape[1], 1)\n            left_embed = torch.gather(left_embed, dim=2, index=f).squeeze(dim=2)\n            right_embed = torch.gather(right_embed, dim=2, index=f).squeeze(dim=2)\n\n        for n, fn in enumerate([lambda x, y: x * y, lambda x, y: (x - y) ** 2]):\n            coords = fn(left_embed, right_embed)\n            # distance\n            Dist = coords.sum(dim=1)\n            if len(Dist.shape) == 2:\n                Dist = Dist.mean(dim=1)\n\n            res[i: i + batch_size, n * 3] = Dist.cpu().numpy()\n            # quantiles\n            idx = coords.argsort(dim=1)[:, q_int]\n            Qs = torch.gather(coords, dim=1, index=idx)\n            if len(Qs.shape) == 3:\n                Qs = Qs.mean(dim=2)\n            res[i: i + batch_size, (n * 3) + 1: (n * 3) + 3] = Qs.cpu().numpy()\n\n    del embed, left_embed, right_embed, coords, f, Dist, Qs, idx\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    return res\n\n\ndef get_cross_pairwise_dist(left, right, embed_pair, batch_size=20000):\n    \"\"\"\n    Compute text to image\/image to text pairwise features (for CLIP embedding)\n    \"\"\"\n    res = np.zeros((left.shape[0], 4), dtype=np.float32)\n    embed_pair = [torch.from_numpy(x).cuda() for x in embed_pair]\n\n    for n, emb0 in enumerate(embed_pair):\n        for k, emb1 in enumerate(embed_pair):\n            for i in range(0, left.shape[0], batch_size):\n                left_embed, right_embed = emb0[left[i: i + batch_size]], emb1[right[i: i + batch_size]]\n                res[i: i + batch_size, n * 2 + k] = (left_embed * right_embed).sum(dim=1).cpu().numpy()\n\n    del embed_pair, emb0, emb1, left_embed, right_embed\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    return res\n\n\ndef get_sparse_pairwise_dist(left, right, embed):\n    \"\"\"\n    Get pairwise features for sparse embedding (tfidf)\n    \"\"\"\n    left = embed[left]\n    right = embed[right]\n\n    return np.array(left.multiply(right).sum(axis=1))\n\n\ndef get_pairwise(left, right, *embeds, folds=None, scores_only=False):\n    \"\"\"\n    Get pairwise features for embeddings list\n    \"\"\"\n    feats = []\n\n    for embed in embeds:\n        # for inference - not to keep embeddings in memory\n        if type(embed) is str:\n            embed = joblib.load(embed)\n\n        if isinstance(embed, np.ndarray):\n            pairw = get_pairwise_dist(left, right, embed, folds=folds, batch_size=10000)\n        elif isinstance(embed, list):\n            pairw = get_cross_pairwise_dist(left, right, embed, batch_size=10000)\n        else:\n            pairw = get_sparse_pairwise_dist(left, right, embed)\n\n        if scores_only:\n            pairw = pairw[:, [0]]\n\n        feats.append(pairw)\n\n    return np.concatenate(feats, axis=1)\n\n\ndef get_features(left, right, texts, points_feats_list, embed_list, count_params, bypass_feats=None, folds=None):\n    \"\"\"\n    Get GBM features from left\/right pairs idx\n    \"\"\"\n    feats = []\n\n    for points_feats in points_feats_list:\n        for index in [left, right]:\n            feats.append(points_feats[index])\n\n    feats.append(get_pairwise(left, right, *embed_list, folds=folds))\n\n    for param in count_params:\n        feats.append(get_length_features(left, right, texts, param))\n\n    if bypass_feats is not None:\n        feats.extend(bypass_feats)\n\n    return np.concatenate(feats, axis=1).astype(np.float32)\n\n\ndef get_prediction_index(left, right, prob, k, cutoff=0.3, hard_cutoff=False, exact_add=2):\n    \"\"\"\n    Transform paired prediction to list of product predictions\n    \"\"\"\n    res = [[x] for x in range(k)]\n    probs = [[1] for x in range(k)]\n    sl = prob > cutoff\n\n    for n, (l, r, flg) in enumerate(zip(left, right, sl)):\n        res[l].append(r)\n        probs[l].append(prob[n])\n\n    # calc proxy f1 score and decide - if it worth to add next point\n    # TODO: Check it later !!! small score decrease\n    for i in range(len(res)):\n        arr = res[i]\n\n        orders = np.array(probs[i]).argsort()[::-1][:50]\n        indexes = np.array(arr)[orders]\n        ps = np.array(probs[i])[orders]\n\n        result = []\n        prev_proxy_f1 = 0\n        proxy_total_yt = ps[ps > cutoff].sum()\n        proxy_total_inter = 0\n        len_yp = 0\n        for n, (idx, p) in enumerate(zip(indexes, ps[ps > cutoff])):\n\n            len_yp += 1\n            proxy_total_inter += 2 * p\n            proxy_f1 = proxy_total_inter \/ (len_yp + proxy_total_yt)\n\n            if (proxy_f1 >= prev_proxy_f1) or hard_cutoff:\n                result.append(idx)\n                prev_proxy_f1 = proxy_f1\n            else:\n                break\n\n        if len(result) < exact_add:\n            result = list(indexes[:exact_add])\n\n        res[i] = result\n        probs[i] = ps[:len(result)]\n\n    return res, probs\n\n\ndef cutoff_prediction(D, I, cutoff, exact_add=2):\n    \"\"\"\n    Cutoff prediction of distances\/indices matrices\n    \"\"\"\n    res = []\n\n    ranger = np.arange(D.shape[1])\n\n    for d, i in zip(D, I):\n        res.append(i[(d > cutoff) | (ranger < exact_add)])\n    return res\n\n\ndef get_y_true(df):\n    \"\"\"\n    Get true prediction indices\n    \"\"\"\n    index = Series(np.arange(df.shape[0]))\n    grp = index.groupby(df['label_group'].values).agg(list)\n\n    return df['label_group'].map(grp).tolist()\n\n\ndef f1_score(y_true, y_pred):\n    \"\"\"\n    F1 score\n    \"\"\"\n    metric = 0\n\n    for yt, yp in zip(y_true, y_pred):\n        inter = np.intersect1d(yt, yp)\n        metric += 2 * len(inter) \/ (len(yt) + len(yp))\n\n    return metric \/ len(y_true)\n\n\ndef f1_score_co_search(y_true, D, I, cutoffs=None):\n    \"\"\"\n    F1 score with cutoff search for distances\/indices matrices\n    \"\"\"\n    if cutoffs is None:\n        cutoffs = np.linspace(0.5, 1, 20)[:-1]\n\n    else:\n        cutoffs = np.sort(cutoffs)\n\n    metric = np.zeros_like(cutoffs)\n\n    for yt, yp, d in zip(y_true, I, D):\n        if not isinstance(yp, np.ndarray):\n            yp = np.array(yp)\n\n        sls = cutoffs[:, np.newaxis] <= d[np.newaxis, :]\n        sls_sum = sls.sum(axis=1)\n\n        prev_s = -np.inf\n        for n, (sl, s) in enumerate(zip(sls, sls_sum)):\n            # if we pass cutoff - switch slice and recalc metric\n            if s != prev_s:\n                yp_ = yp[sl]\n                inter = np.intersect1d(yt, yp_)\n                met_ = 2 * len(inter) \/ (len(yt) + len(yp_))\n                prev_s = s\n\n            metric[n] += met_\n\n    metric = metric \/ len(y_true)\n\n    best_val = metric.argmax()\n\n    return metric[best_val], cutoffs[best_val]\n\n\ndef get_di_torch(embed, n_candidates=50, batch_size=1000):\n    \"\"\"\n    Calc distances\/indices matrices from embeddings\n    \"\"\"\n    D = np.zeros((embed.shape[0], n_candidates), dtype=np.float32)\n    I = np.zeros((embed.shape[0], n_candidates), dtype=np.int32)\n\n    flg_dense = isinstance(embed, np.ndarray)\n\n    if flg_dense:\n        embed_cuda = torch.from_numpy(embed).cuda()\n    else:\n        embed_cuda = csr_to_torch_sparse(embed).cuda()\n\n    for i in range(0, embed.shape[0], batch_size):\n\n        if flg_dense:\n            embed_batch = embed_cuda[i: i + batch_size]\n            d = torch.matmul(embed_cuda, embed_batch.T).T\n        else:\n            embed_batch = torch.from_numpy(embed[i: i + batch_size].toarray().T).cuda()\n            d = torch.matmul(embed_cuda, embed_batch).T\n\n        idx = torch.argsort(d, dim=1, descending=True)[:, :n_candidates]\n        I[i: i + batch_size, :idx.shape[1]] = idx.cpu().numpy()\n        D[i: i + batch_size, :idx.shape[1]] = torch.gather(d, 1, idx).cpu().numpy()\n\n    del d, idx, embed_cuda, embed_batch\n    torch.cuda.empty_cache()\n\n    return D, I\n\n\ndef get_cross_di_torch(embed_x, embed_y, n_candidates=50, batch_size=1000):\n    \"\"\"\n    Calc cross distances\/indices matrices (for CLIP)\n    \"\"\"\n    D = np.zeros((embed_y.shape[0], n_candidates), dtype=np.float32)\n    I = np.zeros((embed_y.shape[0], n_candidates), dtype=np.int32)\n\n    flg_dense = isinstance(embed_x, np.ndarray)\n\n    if flg_dense:\n        embed_x = torch.from_numpy(embed_x).T.cuda()\n        embed_y = torch.from_numpy(embed_y).cuda()\n    else:\n        embed_x = csr_to_torch_sparse(embed_x).cuda()\n\n    for i in range(0, embed_y.shape[0], batch_size):\n\n        if flg_dense:\n            embed_batch = embed_y[i: i + batch_size]\n            # d = torch.matmul(embed_x, embed_batch.T).T\n            d = torch.matmul(embed_batch, embed_x)\n        else:\n            embed_batch = torch.from_numpy(embed_y[i: i + batch_size].toarray().T).cuda()\n            d = torch.matmul(embed_x, embed_batch).T\n\n        idx = torch.argsort(d, dim=1, descending=True)[:, :n_candidates]\n        I[i: i + batch_size, :idx.shape[1]] = idx.cpu().numpy()\n        D[i: i + batch_size, :idx.shape[1]] = torch.gather(d, 1, idx).cpu().numpy()\n\n    del d, idx, embed_x, embed_y, embed_batch\n    torch.cuda.empty_cache()\n\n    return D, I\n\n\n\ndef csr_to_torch_sparse(csr_mat):\n    \"\"\"\n    Transform csr matrix to torch Sparse format\n    \"\"\"\n    coo_mat = csr_mat.astype(np.float32).tocoo()\n\n    row = torch.from_numpy(coo_mat.row).type(torch.int64)\n    col = torch.from_numpy(coo_mat.col).type(torch.int64)\n    edge_index = torch.stack([row, col], dim=0)\n\n    val = torch.from_numpy(coo_mat.data)\n    out = torch.sparse.FloatTensor(edge_index, val, torch.Size(coo_mat.shape))\n\n    return out\n\n\nclass ReadPipeline(Pipeline):\n    \"\"\"\n    DALI Image read pipeline for torch\n    \"\"\"\n    def __init__(self, img_list, batch_size, img_size=300, num_threads=2, device_id=0, num_gpus=1, shuffle=False,\n                 name='Reader', hflip_p=0, scale=1, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n        super().__init__(batch_size, num_threads, device_id)\n\n        self.input = ops.FileReader(files=img_list, random_shuffle=False,\n                                    shard_id=device_id, shuffle_after_epoch=shuffle, num_shards=num_gpus)\n\n        self.decode = ops.ImageDecoder(device=\"mixed\", output_type=types.RGB)\n        self.resize = ops.Resize(device=\"gpu\", resize_shorter=img_size,\n                                 interp_type=types.INTERP_LINEAR)\n        self.cmn = ops.CropMirrorNormalize(device=\"gpu\",\n                                           dtype=types.FLOAT,\n                                           crop=(img_size, img_size),\n                                           mean=[255 * x for x in mean],\n                                           std=[255 * x for x in std])\n\n        self.uniform = ops.random.Uniform(range=(0.0, 1.0))\n        self.resize_rng = ops.random.Uniform(range=(300, int(img_size * scale) + 1))\n        self.coin = ops.random.CoinFlip(probability=hflip_p)\n\n        self.name = 'Reader'\n\n    def define_graph(self):\n        inputs, labels = self.input(name=self.name)\n        images = self.decode(inputs)\n        images = self.resize(images  # , resize_shorter=self.resize_rng()\n                             )\n        output = self.cmn(images, mirror=self.coin()\n                          # , out_of_bounds_policy='trim_to_shape', crop_pos_x=self.uniform(),\n                          # crop_pos_y=self.uniform()\n                          )\n        return (output, labels)\n\n\nclass DaliTorchLoader(DataLoader):\n    \"\"\"\n    Torch DataLoader with DALI ReadPipeline\n    \"\"\"\n    def __init__(self, image_list, image_size=300, batch_size=128, num_threads=2, n_gpus=1, shuffle=False,\n                 drop_last=False, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), name='Reader'):\n\n        self._initialize(image_list, image_size=image_size, batch_size=batch_size, num_threads=num_threads,\n                         n_gpus=n_gpus, shuffle=shuffle,\n                         drop_last=drop_last, mean=mean, std=std, name=name)\n\n    def _initialize(self, image_list, image_size=300, batch_size=128, num_threads=2, n_gpus=1, shuffle=False,\n                    drop_last=False, name='Reader', hflip_p=0, scale=1,\n                    mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n\n        assert n_gpus == 1, 'For now only 1 GPU'\n\n        pipes = [ReadPipeline(image_list, batch_size=batch_size, img_size=image_size, num_threads=num_threads,\n                              device_id=device_id, num_gpus=n_gpus, shuffle=shuffle, name=name, hflip_p=hflip_p,\n                              scale=scale, mean=mean, std=std)\n                 for device_id in range(n_gpus)]\n\n        self.length = len(image_list) \/\/ batch_size\n        self.last_batch = len(image_list) % batch_size\n\n        if self.last_batch == 0 or drop_last:\n            self.last_batch = batch_size\n            self.drop_last = True\n        else:\n            self.length += 1\n            self.drop_last = drop_last\n\n        pipes[0].build()\n        self.dali_iter = DALIGenericIterator(pipes, ['data', 'label'], reader_name=name)\n\n    def __len__(self):\n\n        return self.length\n\n    def _process_batch(self, batch):\n        # assume single GPU\n        batch = batch[0]\n        return batch\n\n    def __iter__(self):\n\n        for n, batch in enumerate(self.dali_iter):\n            batch = self._process_batch(batch)\n            # if not drop last - cut last batch\n            if n == (len(self) - 1) and not self.drop_last:\n                for k in batch:\n                    batch[k] = batch[k][:self.last_batch]\n                yield batch\n            else:\n                yield batch\n                # if drop last and last full batch - raise stop iteration\n                if n == (len(self) - 1) and self.drop_last:\n                    return\n                \n                \n\ndef score_with_image_model_dali(df, model, image_path, image_size=300, batch_size=32, device='cuda:0', n_jobs=10,\n                                mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n    \"\"\"\n    Get image embedding\n    \"\"\"\n    dl = DaliTorchLoader(df['image'].map(lambda x: os.path.join(image_path, x)).tolist(), \\\n                         image_size=image_size, batch_size=batch_size, num_threads=n_jobs, mean=mean, std=std)\n\n    model.eval()\n\n    res = []\n\n    with torch.set_grad_enabled(False):\n        for batch in tqdm.tqdm(dl):\n            with autocast():\n                pred = model.extract_features(batch['data'].to(device))\n            pred = pred.view(*pred.shape[:2], -1).mean(dim=-1).detach().cpu().numpy().astype(np.float32)\n            res.append(pred)\n\n    res = np.concatenate(res, axis=0)\n    res = res \/ ((res ** 2).sum(axis=1, keepdims=True) ** .5)\n    return res\n\n\nclass MatchingTextTest(Dataset):\n    \"\"\"\n    Torch Dataset for text models\n    \"\"\"\n    def __init__(self, data, random_state=42):\n        self.data = data\n\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        text = row['title']\n\n        return text\n\n    def __len__(self):\n        return self.data.shape[0]\n\n    \ndef score_with_text_model(df, text_model, tokenizer=None, batch_size=32, device='cuda:0',\n                          max_length=128, n_jobs=10, standartize=True, normalize=True):\n    \"\"\"\n    Get text embedding\n    \"\"\"\n    ds = MatchingTextTest(df)\n    dl = DataLoader(ds, batch_size=batch_size, num_workers=n_jobs, drop_last=False, shuffle=False)\n\n    if tokenizer is None:\n        tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n    text_model.eval()\n\n    res = []\n\n    with torch.set_grad_enabled(False):\n        for batch in tqdm.tqdm(dl):\n            texts = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n            with autocast():\n                texts = {x: texts[x].to(device) for x in texts}\n                texts = text_model(**texts).last_hidden_state[:, 0, :]\n\n            pred = texts.detach().cpu().numpy().astype(np.float32)\n            res.append(pred)\n\n    res = np.concatenate(res, axis=0)\n    if standartize:\n        res = (res - res.mean(axis=0)) \/ res.std(axis=0)\n\n    if normalize:\n        res = res \/ ((res ** 2).sum(axis=1, keepdims=True) ** .5)\n    return res\n\n\n\nclass TokenizeWrapper:\n    \"\"\"\n    Tokenizer for CLIP\n    \"\"\"\n    def __init__(self, max_len=77):\n        self.tokenizer = clip.simple_tokenizer.SimpleTokenizer()\n        self.max_len = max_len\n\n    def __call__(self, texts):\n        res = torch.zeros((len(texts), self.max_len), dtype=torch.long)\n        res[:, 0] = 49406\n\n        for n, tx in enumerate(texts):\n            enc = self.tokenizer.encode(tx)\n            enc = enc[:self.max_len - 2] + [49407]\n            res[n, 1:len(enc) + 1] = torch.tensor(enc)\n\n        return res\n\n\nclass DaliClipLoader(DaliTorchLoader):\n    \"\"\"\n    DataLodaer for CLIP\n    \"\"\"\n    def __init__(self, train, image_path='train_images', batch_size=128, num_threads=2, n_gpus=1,\n                 name='Reader', hflip_p=0.5, scale=1.3):\n        self.train = train\n        self.params = {\n\n            'image_size': 244,\n            'batch_size': batch_size,\n            'num_threads': num_threads,\n            'n_gpus': n_gpus,\n            'name': name,\n            'shuffle': False,\n            'drop_last': False,\n            'hflip_p': hflip_p,\n            'scale': scale,\n            'mean': (0.48145466, 0.4578275, 0.40821073),\n            'std': (0.26862954, 0.26130258, 0.27577711)\n\n        }\n        self.texts = TokenizeWrapper()(self.train['title'])\n        self.image_path = image_path\n        self._initialize(train['image'].map(lambda x: os.path.join(image_path, x)).tolist(), **self.params)\n\n    def _process_batch(self, batch):\n        # assume single GPU\n        batch = super()._process_batch(batch)\n        batch['text'] = self.texts[batch['label'].type(torch.long)[:, 0]]\n        return batch\n\n\ndef score_with_clip_model_dali(df, model, image_path, batch_size=32, device='cuda:0', n_jobs=10,\n                               ):\n    \"\"\"\n    Get CLIP embedding\n    \"\"\"\n    dl = DaliClipLoader(df, image_path=image_path, batch_size=batch_size, num_threads=n_jobs)\n\n    model.eval()\n\n    res_img, res_text = [], []\n\n    with torch.set_grad_enabled(False):\n        for batch in tqdm.tqdm(dl):\n            with autocast():\n                res_img.append(model.encode_image(batch['data'].to(device)).detach().cpu().numpy().astype(np.float32))\n                res_text.append(model.encode_text(batch['text'].to(device)).detach().cpu().numpy().astype(np.float32))\n\n    res_img = np.concatenate(res_img, axis=0)\n    res_img = res_img \/ ((res_img ** 2).sum(axis=1, keepdims=True) ** .5)\n\n    res_text = np.concatenate(res_text, axis=0)\n    res_text = res_text \/ ((res_text ** 2).sum(axis=1, keepdims=True) ** .5)\n\n    return res_img, res_text\n\n\ndef get_length_features(left, right, texts, vect_params):\n    \"\"\"\n    Get additional features from text\n    \"\"\"\n    res = np.empty((len(left), 3), dtype=np.float32)\n\n    token_counts = CountVectorizer(**vect_params, dtype=np.bool, binary=True).fit_transform(texts)\n    left, right = token_counts[left], token_counts[right]\n\n    inter = left.multiply(right)\n    res[:, 0] = inter.sum(axis=1).ravel()\n\n    diff = left + right - inter\n    res[:, 1] = diff.sum(axis=1).ravel()\n\n    res[:, 2] = np.abs(left.sum(axis=1).ravel() - right.sum(axis=1).ravel())\n\n    return res\n\n\nclass FeaturesGenerator:\n    \"\"\"\n    Features generator (for batch inference)\n    \"\"\"\n    def __init__(self, feature_fn, cache_dir=None, **kwargs):\n\n        self.kwargs = kwargs\n        self.cache_dir = cache_dir\n\n        if cache_dir is not None:\n\n            self.batch = os.path.join(cache_dir, 'batch_{0}.pkl')\n            os.makedirs(cache_dir, exist_ok=True)\n\n            for f in [x for x in os.listdir(cache_dir) if x[:5] == 'batch']:\n\n                path = os.path.join(cache_dir, f)\n\n                if os.path.exists(path):\n                    os.remove(path)\n\n        self.feature_fn = feature_fn\n\n    def features(self, left, right, folds=None, bypass_feats=None):\n\n        if bypass_feats is None:\n            bypass_feats = []\n\n        X = self.feature_fn(left=left, right=right, folds=folds,\n                            bypass_feats=bypass_feats, **self.kwargs)\n        \n        X = DataFrame(X, columns=['feat_{0}'.format(x) for x in range(X.shape[1])])\n\n        return X\n\n    def features_generator(self, left, right, bypass_feats=None, batch_size=500000):\n\n        if bypass_feats is None:\n            bypass_feats = []\n\n        self.cached_files = []\n\n        for n, i in enumerate(range(0, len(left), batch_size)):\n\n            X = self.feature_fn(left=left[i: i + batch_size], right=right[i: i + batch_size], folds=None,\n                                bypass_feats=[x[i: i + batch_size] for x in bypass_feats], **self.kwargs)\n\n            if self.cache_dir is not None:\n                joblib.dump(X, self.batch.format(n))\n                self.cached_files.append(self.batch.format(n))\n            \n            X = DataFrame(X, columns=['feat_{0}'.format(x) for x in range(X.shape[1])])\n\n            yield X\n\n    def cached_generator(self):\n\n        assert len(self.cached_files) > 0, 'No cached files'\n\n        for fname in self.cached_files:\n            yield joblib.load(fname)\n\ndef reflect_prediction(pred):\n    \"\"\"\n    Average left\/right and right\/left prediction\n    \"\"\"\n    L = pred.shape[0] \/\/ 2\n    reflected = pred.copy()\n\n    reflected[:L] += pred[L:]\n    reflected[L:] += pred[:L]\n\n    reflected \/= 2\n\n    return reflected\n\n\n\n### Clustering functions\n\n\ndef calc_components_dist(comp0, comp1, orig_dist):\n    res = []\n\n    for c0 in comp0:\n        for c1 in comp1:\n            d = orig_dist[c0].get(c1)\n            if d is None:\n                d = orig_dist[c1].get(c0, 0)\n            res.append(d)\n\n    res = np.array(res)\n\n    return np.mean(res) * 0.75 + res.max() * 0.25\n\n\ndef upd_cutoff(co, cl_s0, cl_s1):\n    cl_size = max(cl_s0, cl_s1)\n\n    if cl_size < 2:\n        co = max(co, 0.55)\n    elif cl_size < 5:\n        co = max(co, 0.45)\n    elif cl_size < 10:\n        co = max(co, 0.35)\n    else:\n        co = max(co, 0.25)\n\n    return co\n\n\ndef _default_cutoff_fn(co, *args, **kwargs):\n    return co\n\n\ndef update_clusters(clusters, cluster_candidates, cluster_distances, cutoff=0.99, max_add=2, max_cl_size=50,\n                    cutoff_fn=None):\n    if cutoff_fn is None:\n        cutoff_fn = _default_cutoff_fn\n\n    G = nx.Graph()\n\n    for cname in clusters:\n\n        cand = cluster_candidates[cname]\n        dist = cluster_distances[cname]\n        valid_candidates = [x for (x, y) in zip(cand, dist)\n                            if y > cutoff_fn(cutoff, len(clusters[x]), len(clusters[cname]))\n                            ][:max_add]\n\n        path = clusters[cname].copy()\n        for cand in valid_candidates:\n\n            valid = clusters[cand]\n            if ((len(valid) + len(path)) <= max_cl_size):\n                path.extend(clusters[cand])\n\n        nx.add_path(G, path)\n\n    clusters = {}\n    backmap = {}\n\n    for comp in nx.connected_components(G):\n\n        comp = list(comp)\n        cname = min(comp)\n\n        clusters[cname] = comp\n\n        for c in comp:\n            backmap[c] = cname\n\n    for c in range(len(backmap)):\n        if c not in backmap:\n            clusters[c] = [c]\n            backmap[c] = c\n\n    return clusters, backmap\n\n\ndef update_distances(clusters, backmap, orig_dist):\n    cluster_candidates = {}\n    cluster_distances = {}\n\n    for cname in clusters:\n        comp = set(clusters[cname])\n        candidates = []\n        for c in comp:\n            candidates.extend([backmap[x] for x in orig_dist[c] if x not in comp])\n        candidates = list(set(candidates))\n\n        distances = np.array([calc_components_dist(clusters[x], comp, orig_dist) for x in candidates])\n        order = distances.argsort()[::-1]\n        cluster_candidates[cname] = list(np.array(candidates)[order])\n        cluster_distances[cname] = distances[order]\n\n    return cluster_candidates, cluster_distances\n\n\ndef init_clusters(oof_pred, oof_probs):\n    orig_dist = []\n\n    for cc, pp in zip(oof_pred, oof_probs):\n        orig_dist.append({x: y for (x, y) in zip(cc[1:], pp[1:])})\n\n    clusters, cluster_candidates, cluster_distances = {}, {}, {}\n\n    for c in range(len(oof_pred)):\n        clusters[c] = [c]\n        cluster_candidates[c] = oof_pred[c]\n        cluster_distances[c] = oof_probs[c]\n\n    return orig_dist, clusters, cluster_candidates, cluster_distances\n\n\ndef get_pred_from_cluster(clusters, orig_dist, cluster_candidates, cluster_distances,\n                          cl_co=0.5, co=0.6, exact_add=1):\n    pred = [None for _ in range(len(orig_dist))]\n\n    for cname in clusters:\n        comp = clusters[cname]\n\n        to_merge = []\n        cand = cluster_candidates[cname]\n        dist = cluster_distances[cname]\n\n        for n, (c, d) in enumerate(zip(cand, dist)):\n            if (d > cl_co) or ((n < exact_add) and len(comp) == 1):\n                to_merge.extend(clusters[c])\n\n        for c in comp:\n\n            pp = [c] + [x for x in comp if x != c] + to_merge\n\n            dist = [1]\n            for p in pp[1:]:\n                dist.append(orig_dist[c].get(p, 0))\n\n            dist = np.array(dist)\n            orders = dist.argsort()\n            pp = np.array(pp)[orders][:50]\n\n            s_pp = set(pp)\n\n            additional = [x for x in orig_dist[c] if x not in s_pp and orig_dist[c][x] > co]\n            pred[c] = list(pp) + additional[:50 - len(pp)]\n\n    return pred","b60dca59":"data = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\ndevice = 'cuda:0'\ny_true = get_y_true(data)\nfolds = get_validation_folds(data, 5, 42)","b1fd67dc":"def extract_feats_and_preds(embed, co):\n    \"\"\"\n    Calculate points frequencies (features for meta model) and prediction from embedding\n    \"\"\"\n    D, I = get_di_torch(embed)\n    points = get_dist_features(D)\n    pred = cutoff_prediction(D, I, co)\n    \n    return points, pred \n","324b105e":"%%time\ndef get_effnet_embed(data, co=0.7, images_path='train'):\n    \"\"\"\n    Get embeddings, predctions and frequency stats from Efficient Net\n    \"\"\"\n    model = EfficientNet.from_name('efficientnet-b2')\n    model.load_state_dict(joblib.load(EFFNET_PATH))\n    model = model.to(device)\n    embed = score_with_image_model_dali(data, model, \n                '..\/input\/shopee-product-matching\/{0}_images\/'.format(images_path), \n                380, batch_size=128, device=device, n_jobs=2)\n    \n    points, pred = extract_feats_and_preds(embed, co)\n\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return embed, pred, points\n\nimage_embed, image_pred, img_points = get_effnet_embed(data)","2ceb5ba7":"%%time\ndef get_multilang_embed(data, co=0.6):\n    \"\"\"\n    Get embeddings, predctions and frequency stats from Multi language model setu4993\/LaBSE\n    \"\"\"    \n    model = AutoModel.from_pretrained('{0}\/ml.model'.format(ML_PATH), \n                                  return_dict=True).to(device)\n    tokenizer = BertTokenizerFast.from_pretrained('{0}\/ml.token'.format(ML_PATH))\n\n    embed = score_with_text_model(data, model, tokenizer=tokenizer, \n                                   batch_size=128, device=device, max_length=160, n_jobs=2)\n    \n    points, pred = extract_feats_and_preds(embed, co)\n\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return embed, pred, points\n\n\nml_embed, ml_pred, ml_points = get_multilang_embed(data, co=0.6)","8df9f6fc":"%%time\ndef get_id_embed(data, co=0.6):\n    \n    \"\"\"\n    Get embeddings, predctions and frequency stats from Indonesian model cahya\/bert-base-indonesian-522M\n    \"\"\"    \n\n    model = BertModel.from_pretrained('{0}\/id.model'.format(ID_BERT_PATH), \n                                return_dict=True).to(device)\n    tokenizer = BertTokenizer.from_pretrained('{0}\/id.token'.format(ID_BERT_PATH))\n\n    embed = score_with_text_model(data, model, tokenizer=tokenizer, \n                                batch_size=128, device=device, max_length=160, n_jobs=2)\n    \n    points, pred = extract_feats_and_preds(embed, co)\n\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return embed, pred, points\n\n\nid_embed, id_pred, id_points = get_id_embed(data, co=0.6)","c7d3bf05":"%%time\ndef get_clip_embed(data, co_img, co_text, images_path='train'):\n    \"\"\"\n    Get embeddings from CLIP\n    \"\"\"\n    clip_model, _ = clip.load(\"..\/input\/clip-pretrained\/ViT-B-32.pt\")\n    clip_model = clip_model.to(device)\n    clip_img_embed, clip_text_embed = score_with_clip_model_dali(data, clip_model,\n            '..\/input\/shopee-product-matching\/{0}_images\/'.format(images_path))\n\n\n    clip_img_D, clip_img_I = get_cross_di_torch(clip_img_embed, clip_text_embed)\n    clip_img_points = get_dist_features(clip_img_D)\n\n    clip_text_D, clip_text_I = get_cross_di_torch(clip_text_embed, clip_img_embed)\n    clip_text_points = get_dist_features(clip_text_D)\n    \n\n    clip_img_pred = cutoff_prediction(clip_img_D, clip_img_I, co_img)\n    clip_text_pred = cutoff_prediction(clip_text_D, clip_text_I, co_text)\n    \n    del clip_text_D, clip_text_I, clip_img_D, clip_img_I, clip_model\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return clip_img_embed, clip_text_embed, clip_img_pred, clip_text_pred, clip_img_points, clip_text_points\n\nclip_img_embed, clip_text_embed, clip_img_pred, \\\n    clip_text_pred, clip_img_points, clip_text_points = get_clip_embed(data, 0.34, 0.34)","45073ab2":"%%time\ndef get_tfidf_embed(data, param_list, cutoffs):\n    \"\"\"\n    Get TfIdf embeddings with different tokenize params\n    \"\"\"\n    tfidf_embed, tfidf_D, tfidf_I, tfidf_points = [], [], [], []\n\n    for params in param_list:\n        vect = TfidfVectorizer(**params, dtype=np.float32)\n        tfidf_embed.append(vect.fit_transform(data['title']))\n\n        _d, _i = get_di_torch(tfidf_embed[-1])\n        tfidf_D.append(_d)\n        tfidf_I.append(_i)\n\n        tfidf_points.append(get_dist_features(tfidf_D[-1]))\n        print(params)\n        \n    tfidf_preds = []\n\n    for d, i, co in zip(tfidf_D, tfidf_I, cutoffs):\n\n        tfidf_preds.append(cutoff_prediction(d, i, co))\n        print(sum(map(len, tfidf_preds[-1])) \/ len(data))\n\n    del tfidf_I, tfidf_D\n    gc.collect()\n        \n    return tfidf_embed, tfidf_preds, tfidf_points\n    \n\ntfidf_embed, tfidf_preds, tfidf_points = get_tfidf_embed(data,                  \n                                        param_list = [\n                                            {'lowercase': True, 'ngram_range': (1, 1)}, \n                                            {'lowercase': True, 'ngram_range': (3, 3),\n                                             'analyzer': 'char'},  \n                                        ], \n                                        cutoffs=[0.45, 0.45])","c416f9aa":"%%time\ntotal = union_pred(image_pred, \n                   ml_pred, \n                   id_pred, \n                   clip_img_pred, clip_text_pred,\n                   *tfidf_preds)\nleft, right, fold = get_paired_indexes(total, folds)","29105540":"left.shape","c849be9a":"# del image_pred, ml_pred, id_pred, tfidf_preds\n# gc.collect()","ab0ed372":"xgen = FeaturesGenerator(get_features, texts=data['title'], \n                         \n                         points_feats_list=[img_points, id_points, ml_points] + tfidf_points, \n                         \n                         embed_list=[image_embed, id_embed, ml_embed, \n                                     [clip_img_embed, clip_text_embed]] + tfidf_embed,\n                         \n                         count_params=[\n                            {'lowercase': True, 'ngram_range': (1, 1)}, \n                            {'lowercase': True, 'ngram_range': (3, 3), \n                             'analyzer': 'char'}, \n                        ])","4be36039":"%%time\nX = xgen.features(left, right, fold)\nX['target'] = (data['label_group'].values[left] == data['label_group'].values[right]).astype(np.float32)\nX['fold'] = fold","5d6911df":"# del xgen, img_points, id_points, ml_points,  tfidf_points, \\\n#     image_embed, id_embed, ml_embed, tfidf_embed, total\n\n# gc.collect()","8bc9057d":"%%time\n\ntask = Task('binary')\nroles = {'target': 'target', 'group': 'fold'}\n\nautoml = TabularAutoML(task = task, \n                       timeout = 3600,\n                       cpu_limit = 2,\n                       general_params = {'nested_cv': False, 'use_algos': [['linear_l2', 'cb']]},\n                       reader_params = {'cv': 5, 'random_state': 42, 'advanced_roles': False}, \n                       cb_params = {'default_params': {'learning_rate': 0.03, \"od_wait\": 300, \"max_bin\": 128, \n                                                       \"min_data_in_leaf\": 10, \"max_depth\": 8}, 'freeze_defaults': True},\n                       selection_params = {'mode': 0},\n                       verbose=2)\n\npreds = automl.fit_predict(X, roles=roles).data[:, 0]","dd4e20f8":"del X\ngc.collect()","d0144190":"%%time\noof_pred, oof_probs = get_prediction_index(left, right, \n                                           prob=preds,\n                                           k=data.shape[0], cutoff = .4, hard_cutoff=True, \n                                           exact_add=2)","dc627084":"print('OOF score with no post processing {0}'.format(f1_score(y_true, oof_pred)))","a7e14482":"oof_pred, oof_probs = get_prediction_index(left, right, \n                                           prob=preds,\n                                           k=data.shape[0], cutoff = .1, hard_cutoff=True, \n                                           exact_add=2)\n\norig_dist, clusters, cluster_candidates, cluster_distances = init_clusters(oof_pred, oof_probs)\n\ncutoff = 1.00\nfor i in range(90 ):\n    \n    cutoff -= 0.01\n        \n    clusters, backmap = update_clusters(clusters, cluster_candidates, \n                                        cluster_distances, cutoff=cutoff, \n                                        max_add=2, max_cl_size=50, \n                                        cutoff_fn=upd_cutoff)\n    \n    cluster_candidates, cluster_distances = update_distances(clusters, backmap, orig_dist)\n    \n    print('Cutoff {0} done. N clusters = {1}'.format(round(cutoff, 3), len(clusters)))\n    \n    new_oof_pred = get_pred_from_cluster(clusters, orig_dist, \n                                         cluster_candidates, cluster_distances, \n                                         cl_co=1, co=1, exact_add=1)\n    sc = f1_score(y_true, new_oof_pred)\n    \n    print(' Score {0}'.format(sc))\n    \n    if (len(data) \/ len(clusters)) >= 2.82:\n        break","df6e5582":"print('OOF score with post processing {0}'.format(f1_score(y_true, new_oof_pred)))","2724ea56":"del y_true, oof_pred, new_oof_pred, oof_probs, clusters, backmap, cluster_candidates, cluster_distances, orig_dist, data\ngc.collect()","70a31569":"data = pd.read_csv('..\/input\/shopee-product-matching\/{0}.csv'.format(test))","95f89a30":"%%time\nimage_embed, image_pred, img_points = get_effnet_embed(data, co=0.7, images_path=test)\nml_embed, ml_pred, ml_points = get_multilang_embed(data, co=0.6)\nid_embed, id_pred, id_points = get_id_embed(data, co=0.6)\n\nclip_img_embed, clip_text_embed, clip_img_pred, \\\n    clip_text_pred, clip_img_points, clip_text_points = get_clip_embed(data, 0.34, 0.34, test)\n\ntfidf_embed, tfidf_preds, tfidf_points = get_tfidf_embed(data,                  \n                                        param_list = [\n                                            {'lowercase': True, 'ngram_range': (1, 1)}, \n                                            {'lowercase': True, 'ngram_range': (3, 3), \n                                             'analyzer': 'char'},  \n                                        ], \n                                        cutoffs=[0.45, 0.45])\n\n\ntotal = union_pred(image_pred, \n                   ml_pred, \n                   id_pred, \n                   clip_img_pred, clip_text_pred,\n                   *tfidf_preds)\n\nleft, right, _ = get_paired_indexes(total)\n\nxgen = FeaturesGenerator(get_features, texts=data['title'], \n                         \n                         points_feats_list=[img_points, id_points, ml_points] + tfidf_points, \n                         \n                         embed_list=[image_embed, id_embed, ml_embed, \n                                     [clip_img_embed, clip_text_embed]] + tfidf_embed,\n                         \n                         count_params=[\n                            {'lowercase': True, 'ngram_range': (1, 1)}, \n                            {'lowercase': True, 'ngram_range': (3, 3), \n                             'analyzer': 'char'}, \n                        ])","989a6ba6":"del image_pred,  ml_pred, id_pred, tfidf_preds\ngc.collect()","6a547165":"%%time\nif len(left) > 0:\n    \n    prediction = []\n    \n    for batch in xgen.features_generator(left, right, batch_size=250000):\n        \n        print('Batch shape {0}'.format(batch.shape))\n        \n        # automl predict\n        prediction.append(automl.predict(batch).data[:, 0])\n        \n        del batch\n        gc.collect()\n        torch.cuda.empty_cache()\n    \n    prediction = reflect_prediction(np.concatenate(prediction))\n    # calibrate predictions (assume we have same amount of positive and twice amount of negative)\n    prediction = prediction \/ (prediction + (1 - prediction) * 2)","c5611a19":"if len(left) > 0:\n    pred_index, probs = get_prediction_index(left, right, prob=prediction,\n                                             k=data.shape[0], \n                                             cutoff = .1, hard_cutoff=True, exact_add=2)\n\n    orig_dist, clusters, cluster_candidates, cluster_distances = init_clusters(pred_index, probs)\n\n    cutoff = 1.00\n    for i in range(90):\n        cutoff -= 0.01\n\n        clusters, backmap = update_clusters(clusters, cluster_candidates, \n                                            cluster_distances, cutoff=cutoff, \n                                            max_add=2, max_cl_size=50, \n                                            cutoff_fn=upd_cutoff)\n        \n        cluster_candidates, cluster_distances = update_distances(clusters, backmap, orig_dist)\n\n        print('Cutoff {0} done. N clusters = {1}'.format(round(cutoff, 3), len(clusters)))\n\n        if (len(data) \/ len(clusters)) >= 2.6:\n            break\n            \n    pred_index = get_pred_from_cluster(clusters, orig_dist, \n                                       cluster_candidates, cluster_distances, \n                                       cl_co=1, co=1, exact_add=1)\n\nelse:\n    pred_index, probs = [[x] for x in range(data.shape[0])], [[1.0]] * data.shape[0]","de1b66a9":"%%time\nposting_dict = data['posting_id'].reset_index(drop=True).to_dict()\n\npred_posting = []\nfor pp in pred_index:\n    pred_posting.append(' '.join([posting_dict[x] for x in pp]))","f456addf":"prediction = data[['posting_id']].copy()\nprediction['matches'] = pred_posting\n\nprediction","03c3947e":"prediction.to_csv('submission.csv',index=False)\nsub = pd.read_csv('submission.csv')\nsub.head()","a3ed51db":"%%time\nif DEBUG:   \n    y_true = get_y_true(data)\n    metric_val = f1_score(y_true, pred_index)\n    \n    print(metric_val)","cfdbe897":"### Train part","b342cce5":"### Make clustering for train data","3b4002a8":"### Multilang BERT","f79b4909":"### LightAutoML fit predict","c084288e":"# Parts of 3rd Place solution\n\nThis is simplified version of my solution on 3rd place (train + inference) without trained embeddings, but here I use LightAutoML instead of catboost.  \nAlso I increase individual embeddings cutoffs to make models trainable with Kaggle Kernels memory limit.\n\n\nGeneral approach is described here https:\/\/www.kaggle.com\/c\/shopee-product-matching\/discussion\/238515\n\n\nEven simplified, it steel scores in Gold range) Good luck!","01d2ba3b":"### Make prediction","7cb3be05":"### CLIP","02ed384a":"### Image embedding","020130be":"### Define utils","c2ded5b7":"### Create submission","9d1ee35f":"### TF IDFs","7fce33ec":"### Union candidates","bbffcb37":"### Inference for test","35a7e9b7":"### Indonesian BERT","edbaa881":"### Get features"}}