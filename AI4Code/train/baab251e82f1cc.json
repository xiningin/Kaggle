{"cell_type":{"1c86510d":"code","6bdb3c53":"code","70645e83":"code","364e29ad":"code","3d707264":"code","2aae706f":"code","31b855f4":"code","14cfde6e":"code","8105018e":"code","e988130c":"code","13732fd3":"code","1ca8f129":"code","0de8f219":"code","7d2a2a49":"code","550dc4ae":"code","2625d082":"code","c9ed883e":"code","9066a149":"code","1ed7abe9":"code","1d8486ad":"code","9b3ca6a3":"code","9a2f9306":"code","69f94078":"code","71dfce38":"code","fb957e1a":"code","c3702f8c":"code","69f984b7":"code","a5f88898":"code","68c56fcc":"code","c10b6a07":"code","4d4f1b1b":"code","db2ac3b2":"code","2efc0309":"code","6b1c2d11":"code","f72b03c3":"code","41f0ab2d":"code","5941d4ec":"code","97ca2321":"code","6aae2f24":"code","dc881f31":"code","90bf9017":"code","3eb4e5ea":"code","0f5cd41d":"code","4a3ae00a":"code","6de39c6d":"code","b0c58d1b":"code","70ef4e0c":"code","246700dc":"code","fa94d327":"code","91e8d616":"code","e091a581":"code","f8c63de5":"code","bbfee056":"code","d5fe14c2":"code","2cee2f71":"markdown","1565a5f9":"markdown","10bb90a4":"markdown","f37554ca":"markdown","2ae2aebc":"markdown","9bdd4254":"markdown","12f1c876":"markdown","da9bbdc7":"markdown","29e00b98":"markdown","2fe8da67":"markdown","e7c64822":"markdown","f3b29b4e":"markdown","c0fee3a3":"markdown","77626e66":"markdown","77f3b08e":"markdown","960e956f":"markdown","d33414a8":"markdown","a8e37a76":"markdown","5dd29db9":"markdown","618f3a75":"markdown","a46a3a25":"markdown","318a6590":"markdown","725cd4ae":"markdown","baade546":"markdown","920a03d7":"markdown","7a29c27d":"markdown"},"source":{"1c86510d":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6bdb3c53":"## Importing libraries\n\nimport numpy as np\nimport pandas as pd\npd.set_option('max_columns', None)\n\nimport plotly.express as px\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.metrics import (confusion_matrix, precision_recall_curve, auc, roc_curve, accuracy_score, recall_score, classification_report, f1_score, average_precision_score, precision_recall_fscore_support, roc_auc_score)\nfrom sklearn.decomposition import PCA\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier \n\nimport xgboost as XGB\nimport lightgbm as lgb\n\nfrom imblearn.over_sampling import SMOTE\n\nimport warnings\n\n#Ignore warnings\nwarnings.filterwarnings(action='ignore')","70645e83":"#Import data set\ndata = pd.read_csv('..\/input\/company-bankruptcy-prediction\/data.csv')","364e29ad":"data.head(20)","3d707264":"#Shape of the data set\ndata.shape","2aae706f":"#Now that we have an idea of our data, we need to obtain more information possible on them. The first thing that we want to understand is the nature of our data, namely if the data are numerical or categorical and if we have missing information among them. It is possible to check both these points using the .info() pandas method.\ndata.info()","31b855f4":"# Computing the descriptive statistics of our numerical features (Statistical summary of our data)\ndata.describe()","14cfde6e":"# Checking any NaN value presence\ndata.isna().sum().max()","8105018e":"# Checking for duplicates\ndata.duplicated().sum()","e988130c":"print(data['Bankrupt?'].value_counts())\nprint('_'* 30)\nprint('Companies that went bankrupt: ', round(data['Bankrupt?'].value_counts()[0]\/len(data) * 100,1), '% of whole data set')\nprint('Companies that did not go bankrupt: ', round(data['Bankrupt?'].value_counts()[1]\/len(data) * 100,1), '% of whole data set')","13732fd3":"# Checking labels distributions\n\nsns.set_theme(context = 'talk', style='darkgrid', palette='deep', font='sans-serif', font_scale = 0.8, rc={\"grid.linewidth\": 4})\n\nplt.figure(figsize = (16,9))\nsns.countplot(data['Bankrupt?'])\nplt.title('Class Distributions \\n (0: Failed to go bankrupt || 1: Went bankrupt)', fontsize=16)\nplt.show()","1ca8f129":"#Cardinality check\n{column: len(data[column].unique()) for column in data.columns}","0de8f219":"def preprocess_inputs(df):\n    df = df.copy()\n    \n    # Drop single-value column\n    df = df.drop(' Net Income Flag', axis=1)\n\n    #We will remove the column Net Income Flag since it has only single value\n\n    # Split df into X and y\n    y = df['Bankrupt?']\n    X = df.drop('Bankrupt?', axis=1)\n    \n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, test_size = 0.2, shuffle=True, random_state=1, stratify = y)\n    \n    # Scale X\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n    X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n    \n    return X_train, X_test, y_train, y_test","7d2a2a49":"X_train, X_test, y_train, y_test = preprocess_inputs(data)","550dc4ae":"#Splitted Data\nprint('X_train shape is ' , X_train.shape)\nprint('X_test shape is ' , X_test.shape)\nprint('y_train shape is ' , y_train.shape)\nprint('y_test shape is ' , y_test.shape)","2625d082":" y_test.value_counts() \/ len(y_test)","c9ed883e":"original_models = {\n    \"Logistic Regression\": LogisticRegression(solver = \"liblinear\", l1_ratio = 0.5),\n    \"K-Nearest Neighbors\": KNeighborsClassifier(weights='distance', metric='euclidean'),\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Support Vector Machine (Linear Kernel)\": LinearSVC(C = 0.5),\n    \"Support Vector Machine (RBF Kernel)\": SVC(),\n    \"Neural Network\": MLPClassifier(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"Gradient Boosting\": GradientBoostingClassifier(loss = \"exponential\"),\n    \"XgBoost Classifier\": XGB.XGBClassifier(),\n    \"AdaBoost Classifier\": AdaBoostClassifier(n_estimators = 75, learning_rate = 0.3),\n    \"CatBoost Classifier\": CatBoostClassifier(verbose = False),\n    \"LightGBM\": LGBMClassifier()\n}\n\nfor name, model in original_models.items():\n  model.fit(X_train, y_train)\n  print(name + ' trained.')","9066a149":"original_results = []\n\nfor name, model in original_models.items():\n  result = model.score(X_test, y_test)\n  original_results.append(result)\n\n  print(\"\"\"\n  __________________________\"\"\"+name+\"\"\"__________________________\n  \"\"\")\n\n  model = original_models[name]\n  y_test_pred = model.predict(X_test)\n  arg_test = {'y_true':y_test, 'y_pred':y_test_pred}\n  print(confusion_matrix(**arg_test))\n  print(classification_report(**arg_test))\n\n  print(name + ': {:.5f}%'.format(result * 100))","1ed7abe9":"# Plotting confusion matrix for each classifier\n\na = 3  # number of rows\nb = 4  # number of columns\nc = 1  # initialize plot counter\n\nfig = plt.figure(figsize=(30, 18))\n\nfor name, model in original_models.items():\n    original_results_smote = model.score(X_test, y_test)\n    model = original_models[name]\n    y_test_pred_smote = model.predict(X_test)\n    arg_test = {'y_true':y_test, 'y_pred':y_test_pred_smote}\n\n    conf_mx0 = confusion_matrix(y_test, y_test_pred_smote)\n\n    heat_cm0 = pd.DataFrame(conf_mx0, columns=np.unique(y_test), index = np.unique(y_test))\n    heat_cm0.index.name = 'Actual'\n    heat_cm0.columns.name = 'Predicted'\n\n    plt.subplot(a, b, c)\n    #plt.title(name)\n    fig.subplots_adjust(left=None, bottom=None, right= None, top=None, wspace=0.4, hspace= 0.4)\n    sns.heatmap(heat_cm0, annot=True, fmt='.2f', square=True, annot_kws={\"size\": 16}, cmap = 'Purples').set_title(name, fontsize = 20)\n    c = c + 1\n\nplt.show()","1d8486ad":"correlation_matrix = data.corr()\ncorrelation_matrix.style.background_gradient(sns.light_palette('red', as_cmap=True))","9b3ca6a3":"correlation_matrix = correlation_matrix.iloc[1:,1:]","9a2f9306":"drop_correlated_fea = []\nfor i in range(len(correlation_matrix.columns)):\n    for j in range(i):\n        if(correlation_matrix.iloc[i,j] >= 0.894 or correlation_matrix.iloc[i,j] <= -0.894):\n            if correlation_matrix.columns[j] not in drop_correlated_fea:\n                drop_correlated_fea.append(correlation_matrix.columns[j])  ","69f94078":"len(drop_correlated_fea)","71dfce38":"data = data.drop(drop_correlated_fea, axis = 1)","fb957e1a":"data","c3702f8c":"def preprocess_inputs(df):\n    df = df.copy()\n    \n    # Drop single-value column\n    df = df.drop(' Net Income Flag', axis=1)\n\n    #We will remove the column Net Income Flag since it has only single value\n\n    # Split df into X and y\n    y = df['Bankrupt?']\n    X = df.drop('Bankrupt?', axis=1)\n    \n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, test_size = 0.2, shuffle=True, random_state=1, stratify = y)\n    \n    # Scale X\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n    X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n    \n    return X_train, X_test, y_train, y_test","69f984b7":"X_train, X_test, y_train, y_test = preprocess_inputs(data)","a5f88898":"def preprocess_inputs(df_):\n    df_ = df_.copy()\n    \n    # Drop single-value column\n    df_ = df_.drop(' Net Income Flag', axis=1)\n\n    #We will remove the column Net Income Flag since it has only single value\n\n    # Split df into X and y\n    y_ = df_['Bankrupt?']\n    X_ = df_.drop('Bankrupt?', axis=1)\n\n    #Initializing SMOTE\n\n    sm = SMOTE(random_state = 42)\n    X_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\n\n    sm = SMOTE(random_state = 42)\n    X_test_oversampled, y_test_oversampled = sm.fit_resample(X_train_smote, y_train_smote)\n    X_train_smote = pd.DataFrame(X_test_oversampled, columns=X_.columns)\n\n    return X_train_smote, y_train_smote","68c56fcc":"X_train_smote, y_train_smote = preprocess_inputs(data)","c10b6a07":"print('X_train_smote shape is ' , X_train_smote.shape) # Train data after SMOTE\nprint('y_train_smote shape is ' , y_train_smote.shape) # Train data after SMOTE\nprint('X_test_smote shape is ' , X_test.shape) # Original test data\nprint('y_test_smote shape is ' , y_test.shape) # Original test data","4d4f1b1b":"sns.set_theme(context = 'talk', style='darkgrid', palette='deep', font='sans-serif', font_scale = 0.8, rc={\"grid.linewidth\": 4})\n\nplt.figure(figsize = (16,9))\nsns.countplot(y_train_smote)\nplt.title('Class Distributions \\n (0: Failed to go bankrupt || 1: Went bankrupt)', fontsize=16)\nplt.show()","db2ac3b2":"original_models_smote_ot = {\n    \"Logistic Regression\": LogisticRegression(solver = \"liblinear\", l1_ratio = 0.5),\n    \"K-Nearest Neighbors\": KNeighborsClassifier(weights='distance', metric='euclidean'),\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Support Vector Machine (Linear Kernel)\": LinearSVC(C = 0.5),\n    \"Support Vector Machine (RBF Kernel)\": SVC(),\n    \"Neural Network\": MLPClassifier(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"Gradient Boosting\": GradientBoostingClassifier(loss = \"exponential\"),\n    \"XgBoost Classifier\": XGB.XGBClassifier(),\n    \"AdaBoost Classifier\": AdaBoostClassifier(n_estimators = 75, learning_rate = 0.3),\n    \"CatBoost Classifier\": CatBoostClassifier(verbose = False),\n    \"LightGBM\": LGBMClassifier()\n}\n\nfor name, model in original_models_smote_ot.items():\n  model.fit(X_train_smote, y_train_smote)\n  print(name + ' trained.')","2efc0309":"original_results_smote_ot = []\n\nfor name, model in original_models_smote_ot.items():\n  result_smote = model.score(X_test, y_test)\n  original_results_smote_ot.append(result_smote)\n\n  print(\"\"\"\n  __________________________\"\"\"+name+\"\"\"__________________________\n  \"\"\")\n\n  model = original_models_smote_ot[name]\n  y_test_pred_smote = model.predict(X_test)\n  arg_test = {'y_true':y_test, 'y_pred':y_test_pred_smote}\n  print(confusion_matrix(**arg_test))\n  print(classification_report(**arg_test))\n\n  print(name + ': {:.5f}%'.format(result_smote * 100))","6b1c2d11":"fig, ax = plt.subplots()\nfig.set_size_inches(25,12)\n\nfor m in original_models_smote_ot:\n    y_pred = original_models_smote_ot[m].predict(X_test)\n    fpr, tpr, thresholds_nb = roc_curve(y_test, y_pred, pos_label=1)\n    roc_auc = auc(fpr, tpr)\n    precision_nb, recall_nb, th_nb = precision_recall_curve(y_test, y_pred,pos_label=1)\n    plt.plot(fpr, tpr, label= m + ': {:.5f}'.format(roc_auc_score(y_test, y_pred)))\nplt.plot([0, 1], [0, 1], 'r--')\nplt.axis([-0.01, 1, 0, 1])\nplt.xlabel('False Positive Rate', fontweight='bold', fontsize=14)\nplt.ylabel('True Positive Rate', fontweight='bold', fontsize=14)\nplt.title('ROC Curve', fontweight='bold', fontsize=18)\nplt.legend(loc='best')\nplt.show()\n","f72b03c3":"# Plotting confusion matrix for each classifier\n\na = 3  # number of rows\nb = 4  # number of columns\nc = 1  # initialize plot counter\n\nfig = plt.figure(figsize=(30, 18))\n\nfor name, model in original_models_smote_ot.items():\n    original_results_smote_ot = model.score(X_test, y_test)\n    model = original_models_smote_ot[name]\n    y_test_pred_smote = model.predict(X_test)\n    arg_test = {'y_true':y_test, 'y_pred':y_test_pred_smote}\n\n    conf_mx0 = confusion_matrix(y_test, y_test_pred_smote)\n\n    heat_cm0 = pd.DataFrame(conf_mx0, columns=np.unique(y_test), index = np.unique(y_test))\n    heat_cm0.index.name = 'Actual'\n    heat_cm0.columns.name = 'Predicted'\n\n    plt.subplot(a, b, c)\n    #plt.title(name)\n    fig.subplots_adjust(left=None, bottom=None, right= None, top=None, wspace=0.4, hspace= 0.4)\n    sns.heatmap(heat_cm0, annot=True, fmt='.2f', square=True, annot_kws={\"size\": 16}, cmap = 'Purples').set_title(name, fontsize = 20)\n    c = c + 1\n\nplt.show()","41f0ab2d":"def preprocess_inputs(df_):\n    df_ = df_.copy()\n    \n    # Drop single-value column\n    df_ = df_.drop(' Net Income Flag', axis=1)\n\n    #We will remove the column Net Income Flag since it has only single value\n\n    # Split df into X and y\n    y_ = df_['Bankrupt?']\n    X_ = df_.drop('Bankrupt?', axis=1)\n\n    #Initializing SMOTE\n\n    sm = SMOTE(random_state = 42)\n    X_smote, y_smote = sm.fit_resample(X_, y_)\n\n    sm = SMOTE(random_state = 42)\n    X_test_oversampled, y_test_oversampled = sm.fit_resample(X_smote, y_smote)\n    X_smote = pd.DataFrame(X_test_oversampled, columns=X_.columns)\n    \n    # Train-test split\n    X_train_smote, X_test_smote, y_train_smote, y_test_smote = train_test_split(X_smote, y_smote, train_size = 0.8, test_size = 0.2, random_state=1, shuffle=True, stratify = y_smote)\n    \n    # Scale X\n    scaler = StandardScaler()\n    scaler.fit(X_train_smote)\n    X_train_smote = pd.DataFrame(scaler.transform(X_train_smote), index=X_train_smote.index, columns=X_train_smote.columns)\n    X_test_smote = pd.DataFrame(scaler.transform(X_test_smote), index=X_test_smote.index, columns=X_test_smote.columns)\n    \n    return X_train_smote, X_test_smote, y_train_smote, y_test_smote, y_smote","5941d4ec":"X_train_smote, X_test_smote, y_train_smote, y_test_smote, y_smote = preprocess_inputs(data)","97ca2321":"print('X_train_smote shape is ' , X_train_smote.shape) # Train data after SMOTE\nprint('y_train_smote shape is ' , y_train_smote.shape) # Train data after SMOTE\nprint('X_test_smote shape is ' , X_test_smote.shape) # Test data after SMOTE\nprint('y_test_smote shape is ' , y_test_smote.shape) # Test data after SMOTE","6aae2f24":"sns.set_theme(context = 'talk', style='darkgrid', palette='deep', font='sans-serif', font_scale = 0.8, rc={\"grid.linewidth\": 4})\n\nplt.figure(figsize = (16,9))\nsns.countplot(y_smote)\nplt.title('Class Distributions \\n (0: Failed to go bankrupt || 1: Went bankrupt)', fontsize=16)\nplt.show()","dc881f31":"original_models_smote_st = {\n    \"Logistic Regression\": LogisticRegression(solver = \"liblinear\", l1_ratio = 0.5),\n    \"K-Nearest Neighbors\": KNeighborsClassifier(weights='distance', metric='euclidean'),\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Support Vector Machine (Linear Kernel)\": LinearSVC(C = 0.5),\n    \"Support Vector Machine (RBF Kernel)\": SVC(),\n    \"Neural Network\": MLPClassifier(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"Gradient Boosting\": GradientBoostingClassifier(loss = \"exponential\"),\n    \"XgBoost Classifier\": XGB.XGBClassifier(),\n    \"AdaBoost Classifier\": AdaBoostClassifier(n_estimators = 75, learning_rate = 0.3),\n    \"CatBoost Classifier\": CatBoostClassifier(verbose = False),\n    \"LightGBM\": LGBMClassifier()\n}\n\nfor name, model in original_models_smote_st.items():\n  model.fit(X_train_smote, y_train_smote)\n  print(name + ' trained.')","90bf9017":"original_results_smote_st = []\n\nfor name, model in original_models_smote_st.items():\n  result_smote = model.score(X_test_smote, y_test_smote)\n  original_results_smote_st.append(result_smote)\n\n  print(\"\"\"\n  __________________________\"\"\"+name+\"\"\"__________________________\n  \"\"\")\n\n  model = original_models_smote_st[name]\n  y_test_pred_smote = model.predict(X_test_smote)\n  arg_test = {'y_true':y_test_smote, 'y_pred':y_test_pred_smote}\n  print(confusion_matrix(**arg_test))\n  print(classification_report(**arg_test))\n\n  print(name + ': {:.5f}%'.format(result_smote * 100))","3eb4e5ea":"fig, ax = plt.subplots()\nfig.set_size_inches(25,12)\n\nfor m in original_models_smote_st:\n    y_pred = original_models_smote_st[m].predict(X_test_smote)\n    fpr, tpr, thresholds_nb = roc_curve(y_test_smote, y_pred, pos_label=1)\n    roc_auc = auc(fpr, tpr)\n    precision_nb, recall_nb, th_nb = precision_recall_curve(y_test_smote, y_pred,pos_label=1)\n    plt.plot(fpr, tpr, label= m + ': {:.5f}'.format(roc_auc_score(y_test_smote, y_pred)))\nplt.plot([0, 1], [0, 1], 'r--')\nplt.axis([-0.01, 1, 0, 1])\nplt.xlabel('False Positive Rate', fontweight='bold', fontsize=14)\nplt.ylabel('True Positive Rate', fontweight='bold', fontsize=14)\nplt.title('ROC Curve', fontweight='bold', fontsize=18)\nplt.legend(loc='best')\nplt.show()","0f5cd41d":"# Plotting confusion matrix for each classifier\n\na = 3  # number of rows\nb = 4  # number of columns\nc = 1  # initialize plot counter\n\nfig = plt.figure(figsize=(30, 18))\n\nfor name, model in original_models_smote_st.items():\n    original_results_smote_st = model.score(X_test_smote, y_test_smote)\n    model = original_models_smote_st[name]\n    y_test_pred_smote = model.predict(X_test_smote)\n    arg_test = {'y_true':y_test_smote, 'y_pred':y_test_pred_smote}\n\n    conf_mx0 = confusion_matrix(y_test_smote, y_test_pred_smote)\n\n    heat_cm0 = pd.DataFrame(conf_mx0, columns=np.unique(y_test_smote), index = np.unique(y_test_smote))\n    heat_cm0.index.name = 'Actual'\n    heat_cm0.columns.name = 'Predicted'\n\n    plt.subplot(a, b, c)\n    #plt.title(name)\n    fig.subplots_adjust(left=None, bottom=None, right= None, top=None, wspace=0.4, hspace= 0.4)\n    sns.heatmap(heat_cm0, annot=True, fmt='.2f', square=True, annot_kws={\"size\": 16}, cmap = 'Purples').set_title(name, fontsize = 20)\n    c = c + 1\n\nplt.show()","4a3ae00a":"n_components = 74\n\npca = PCA(n_components)\npca.fit(X_train_smote)\n\nX_train_reduced = pd.DataFrame(pca.transform(X_train_smote), index = X_train_smote.index, columns = ['PC' + str(i) for i in range(1, n_components + 1)])\nX_test_reduced = pd.DataFrame(pca.transform(X_test_smote), index = X_test_smote.index, columns = ['PC' + str(i) for i in range(1, n_components + 1)])","6de39c6d":"X_train_reduced.describe()","b0c58d1b":"pca.explained_variance_ratio_","70ef4e0c":"fig = px.bar(\n    x = ['PC' + str(i) for i in range(1, n_components + 1)],\n    y = pca.explained_variance_ratio_,\n    labels = {'x': \"Principal Component\", 'y': 'Variance Ratio'},\n    color = pca.explained_variance_ratio_,\n    color_continuous_scale = [(0,'darkblue'), (1, 'lightblue')],\n    title = 'Proportion of variance in Principal Components'\n        \n)\n         \nfig.show()","246700dc":"n_components = 7\n\npca = PCA(n_components)\npca.fit(X_train_smote)\n\nX_train_reduced = pd.DataFrame(pca.transform(X_train_smote), index = X_train_smote.index, columns = ['PC' + str(i) for i in range(1, n_components + 1)])\nX_test_reduced = pd.DataFrame(pca.transform(X_test_smote), index = X_test_smote.index, columns = ['PC' + str(i) for i in range(1, n_components + 1)])","fa94d327":"pca.explained_variance_ratio_","91e8d616":"reduced_models_smote = {\n    \"Logistic Regression\": LogisticRegression(solver = \"liblinear\", l1_ratio = 0.5),\n    \"K-Nearest Neighbors\": KNeighborsClassifier(weights='distance', metric='euclidean'),\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Support Vector Machine (Linear Kernel)\": LinearSVC(C = 0.5),\n    \"Support Vector Machine (RBF Kernel)\": SVC(),\n    \"Neural Network\": MLPClassifier(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"Gradient Boosting\": GradientBoostingClassifier(loss = \"exponential\"),\n    \"XgBoost Classifier\": XGB.XGBClassifier(),\n    \"AdaBoost Classifier\": AdaBoostClassifier(n_estimators = 75, learning_rate = 0.3),\n    \"CatBoost Classifier\": CatBoostClassifier(verbose = False),\n    \"LightGBM\": LGBMClassifier()\n}\n\nfor name, model in reduced_models_smote.items():\n  model.fit(X_train_reduced, y_train_smote)\n  print(name + ' trained.')","e091a581":"reduced_results_smote = []\n\nfor name, model in reduced_models_smote.items():\n  reduced_result_smote = model.score(X_test_reduced, y_test_smote)\n  reduced_results_smote.append(reduced_result_smote)\n\n  print(\"\"\"\n  __________________________\"\"\"+name+\"\"\"__________________________\n  \"\"\")\n\n  model = reduced_models_smote[name]\n  y_test_pred_smote = model.predict(X_test_reduced)\n  arg_test = {'y_true':y_test_smote, 'y_pred':y_test_pred_smote}\n  print(confusion_matrix(**arg_test))\n  print(classification_report(**arg_test))\n\n  print(name + ': {:.5f}%'.format(reduced_result_smote * 100))","f8c63de5":"from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc, roc_curve, recall_score, \n                             classification_report, f1_score, average_precision_score, precision_recall_fscore_support)\n\nfig, ax = plt.subplots()\nfig.set_size_inches(25,12)\n\nfor m in reduced_models_smote:\n    y_pred = reduced_models_smote[m].predict(X_test_reduced)\n    fpr, tpr, thresholds_nb = roc_curve(y_test_smote, y_pred, pos_label=1)\n    roc_auc = auc(fpr, tpr)\n    precision_nb, recall_nb, th_nb = precision_recall_curve(y_test_smote, y_pred,pos_label=1)\n    plt.plot(fpr, tpr, label= m + ': {:.5f}'.format(roc_auc_score(y_test_smote, y_pred)))\nplt.plot([0, 1], [0, 1], 'b--')\nplt.axis([-0.01, 1, 0, 1])\nplt.xlabel('False Positive Rate', fontweight='bold', fontsize=14)\nplt.ylabel('True Positive Rate', fontweight='bold', fontsize=14)\nplt.title('ROC Curve', fontweight='bold', fontsize=18)\nplt.legend(loc='best')\nplt.show()\n","bbfee056":"# Plotting confusion matrix for each classifier\n\na = 3  \nb = 4  \nc = 1\n\nfig = plt.figure(figsize=(30, 18))\n\nfor name, model in reduced_models_smote.items():\n    reduced_result_smote = model.score(X_test_reduced, y_test_smote)\n    model = reduced_models_smote[name]\n    y_test_pred_smote = model.predict(X_test_reduced)\n    arg_test = {'y_true':y_test_smote, 'y_pred':y_test_pred_smote}\n\n    conf_mx0 = confusion_matrix(y_test_smote, y_test_pred_smote)\n\n    heat_cm0 = pd.DataFrame(conf_mx0, columns=np.unique(y_test_smote), index = np.unique(y_test_smote))\n    heat_cm0.index.name = 'Actual'\n    heat_cm0.columns.name = 'Predicted'\n\n    plt.subplot(a, b, c)\n    #plt.title(name)\n    fig.subplots_adjust(left=None, bottom=None, right= None, top=None, wspace=0.4, hspace= 0.4)\n    sns.heatmap(heat_cm0, annot=True, fmt='.2f', square=True, annot_kws={\"size\": 16}, cmap = 'Purples').set_title(name, fontsize = 20)\n    c = c + 1\n\nplt.show()","d5fe14c2":"fig_dim_red = px.bar(\n    x=np.subtract(reduced_results_smote, original_results_smote_st),\n    y=original_models_smote_st.keys(),\n    orientation='h',\n    labels={'x': \"<b>Change in Performance<b>\", 'y': \"<b>Model<b>\"},\n    color=np.subtract(reduced_results_smote, original_results_smote_st),\n    color_continuous_scale=[(0, 'yellow'), (1, 'red')],\n    title=\"<b>Change in Model Performance After Dimensionality Reduction<b>\"\n)\n\nfig_dim_red.show()","2cee2f71":"### **Oversampling with SMOTE (Original Test Data)**","1565a5f9":"![resim.png](attachment:e5111751-cca5-445d-a294-3095db057337.png)\n\nData pre-processing, which is essentially a technique used in data mining, is a very frequently applied method that should be adopted before the development of machine learning models. Often real-world data is inconsistent, incomplete, and contains some errors, so they are unlikely to be directly analysed. Therefore, the raw data is transformed by pre-processing and put into a useful and an effective format before analysis. Data pre-processing, in its simplest form, includes the steps of cleaning the data (management of incomplete and noisy data), transforming the data (normalization, etc.) and reducing the data (methods such as dimension reduction, etc.). In machine learning processes, data pre-processing is simply done with the following steps:\n\n1. Installing open-source libraries required for data manipulation and analysis, especially Pandas and NumPy\n1. Uploading the dataset in the appropriate format\n1. Observing the features such as missing data and data type in the data set and eliminating this problem if they are problematic\n1. Splitting the data as train and test data in order to apply machine learning algorithms\n\nIn order to detect machine learning model behaviours, it is important to divide the data into two as train and test data because machine learning methods are primarily trained through updating various parameters with train data. After the train phase, the machine learning model is tested with a different data set (test da-ta). Thus, how the established model responds to new data (observations) is measured. There are various opinions about how much of the total data should be allocated to train and how much to test data. Although it is necessary to keep the train data as large as possible for very large data, the general opinion is to separate the train and test data with a rate of 80-20%.\n","10bb90a4":"**ROC Curve**\n\nFigure shows the ROC curve of the model. As can be seen from both performance metrics, many models predicted classes with around 95% success. Especially the success of models such as Neural network, Random Forest, CatBoost and LightGBM is very high.","f37554ca":"First of all, the distribution of the data for 6819 companies in the data is shown in this figure. As can be seen from the figure, there is a serious imbalance between classes.","2ae2aebc":"SMOTE technique has been applied directly to the raw data (before train-test data split). The data distribution obtained is as shown in figure.","9bdd4254":"### **Change in Performance After PCA**","12f1c876":"Dimension reduction is essentially reducing the number of features in the data. Since the presence of many features can make the model difficult to understand and manage, dimension reduction can be used to make the model less complex. In addition, it can be said that the use of too many features shows poor performance in machine learning algorithms, and models run longer by consuming more memory. Although no \"best\" method can be directly suggested for dimension reduction, this can be achieved with various techniques such as missing value ratio in features, low-high correlation filter, random forest algorithm, factor analysis, principal component analysis. In short, the model is simplified with dimension reduction, and it is ensured that the model is created more easily with basic variables.","da9bbdc7":"# **Data Description**","29e00b98":"### **Reduced Model**","2fe8da67":"# **Data Preprocessing and Train-Test Data Split**","e7c64822":"### **Multicollinearity Check**","f3b29b4e":"# **Models**","c0fee3a3":"Multicollinearity means that there is a strong correlation between the independent variables that predict the dependent variable in a regression model. In such a case, it cannot be determined how much effect the independent variables have on the dependent variables individually. Multicollinearity is determined by the 'Variance Inflation Factor (VIF)' and this ratio comes from the correlation between independent variables. Generally, a VIF score above 5 indicates the presence of multicollinearity. In this case, it is suggested as a solution to remove one of the variables that is related to each other from the model. Due to the use of many features in machine learning methods, there may be some overlapping features. Features that overlap with this analysis should be identified and excluded from the analysis.","77626e66":"In this analysis, SMOTE technique has been applied to the raw data. Afterwards, train and test data have been created. Thus, the trained models were validated with the test data, which has a more balanced class distribution. Nevertheless, due to the loss of originality of the test data, there was a problem of overfitting in the models. ","77f3b08e":"### **Oversampling with SMOTE (SMOTE Test Data)**","960e956f":"Unbalanced dataset is a problem that is frequently seen in classification problems and occurs when the class distributions are quite far from each other. This problem arises because the majority class dominates the minority class in machine learning algorithms. Owing to this, algorithms often predict the entire data set very poorly for the minority class, showing proximity to the majority class. Even though there are different metric selection and resampling methods to solve such problems, the SMOTE sampling method is the easiest and most useful method to apply.\n\nSMOTE oversampling technique starts from the samples of the minority class and generates synthetic new observations in the feature space randomly by interpolation method. Thus, it balances the majority class with the number of observations. However, it does not interfere with the model other than increasing the number of samples and does not provide extra information to the model. According to some sources, the point to be considered while applying SMOTE is that the method should be applied only to the train data set and the original test data should be used while testing the data. Nevertheless, in some models, after all the data is balanced with the SMOTE method, the split of train and test data and the application of algorithms in this way also stand out in practice.\n","d33414a8":"Afterwards, dimension reduction has been applied in order to express the model with fewer attributes. In this way, it is aimed to establish a decent enough model with fewer variables.","a8e37a76":"**ROC Curve**\n\nThen, all data have been trained with 12 models using the train data. The point to note in this regard is that SMOTE is applied only to the train data in the initial data. That is, the test data is kept as the original data at this stage. The sample has been expanded with SMOTE and then the data has been trained with a larger sample, but the models have been tested with the original test data. Since the models were trained with more data, the test data has been predicted to be better than the initial model. Figure below gives the ROC curve of the models. As can be seen from the ROC curve and confusion matrices, while logistic regression, SVM methods (especially RBF kernel) and AdaBoost classifier give relatively good results, the results of other models are only mediocre.","5dd29db9":"## Bankruptcy Prediction\n\n![resim.png](attachment:29fd600f-fcfb-4dd9-8ee0-5488ee35b3f9.png)\n\nThe term bankruptcy is expressed as the inability of a company to pay its debts to its creditors. The bankruptcy of a company and even the possibility of going bankrupt is important for the company's investors and society. Therefore, bankruptcy prediction should be made before the bankruptcy of a company and necessary and appropriate models should be built. In this part of the model, we created machine learning algorithms that can predict whether companies will go bankrupt. In this way, it will be possible to predict the bankruptcy of companies with their financial statements and financial ratios.\n\n\n**INTRODUCTION**\n\nThere are more than 6800 companies in the data used in the bankruptcy prediction model. The bankruptcy cases of these companies in the data are shown as 1 (bankrupted) and 0 (failed to go bankrupt) and it is tried to predict whether they will go bankrupt with 95 financial ratios. \n\n* **95 features (X1-X95)**\n\nOur goal is to use these features to have clearer information about the future and legitimacy of the companies.\n\nP.s: If you like this notebook don't forget to ****UPVOTE****!","618f3a75":"### **Dimensionality Reduction (PCA)**","a46a3a25":"![resim.png](attachment:6633023c-91ac-4bf6-bc82-e83a6b715088.png)\n\nFor this prediction, 12 different machine learning algorithms have been used in this case study. The algorithms used are as follows:\n\n**1.\tLogistic regression:** It is a machine learning classification method and tries to predict the categorical dependent variable coded as binary (1 \u2013 yes, successful etc., 0 \u2013 no, unsuccessful etc.).\n\n**2.K-Nearest Neighbors:** It is one of the most commonly used supervised machine learning algorithms. It is mostly used in solving classification problems and is based on the nearest neighbour principle.\n\n**3.\tDecision Trees:** It is a tree-based learning algorithm and one of the most used supervised learning algorithms. It has a pre-defined target variable and has a structure used to divide a dataset containing many observations into smaller sets within the framework of various rules.\n\n**4.\tSupport Vector Machine (Linear Kernel):** It is one of the supervised learning methods generally used in classification problems. SVMs with a linear kernel operate using a linear line separating the two classes.\n\n**5.\tSupport Vector Machine (Non-Linear Kernel):** It is generally preferred when the data set is not very large. Compared to linear core SVM, it uses non-linear classification method with gamma hyperparameter.\n\n**6.\tArtificial Neural Networks:** It can offer a single-layer or multi-layer learning method according to the number of layers. It consists of input, aggregation function, activation function, and outputs.\n\n**7.\tRandom Forest:** It is a popular machine learning model that can be applied to both regression and classification problems without the need for hyperparameter estimation.\n\n**8.\tGradient Boosting:** It is a method of transforming weak learners into strong learners. First, it creates a starting leaf and then creates new trees by taking into account the errors that occur. This process is continued until a better result cannot be obtained.\n\n**9.\tXgBoost Classifier:** It is the optimized and higher performance version of the gradient boosting algorithm in various ways. Possibly overfitting in gradient boosting is avoided by this algorithm.\n\n**10.\tAdaBoost Classifier:** It is an iterative ensemble method. It combines very low-performing classifiers to form a powerful classifier.\n\n**11.\tCatboost Classifier:** Another classification algorithm based on gradient boosting. Its learning speed is higher than other classifiers and it can work with both numerical, categorical and text data.\n\n**12.\tLightGBM Classifier:** It is a histogram-based method that reduces memory usage and speeds up the train of the model.\n","318a6590":"As a result of the analysis depicted in Figure above, the contribution of the variables to the model has been shown and a \"reduced model\" has been established with 7 attributes according to the graph. Since the marginal contribution of the features after the 7 features to the model has decreased noticeably, it is enough to select 7 features.","725cd4ae":"**Confusion Matrix**\n\nConfusion matrix, which is one of the easiest metrics to measure the accuracy of the model, includes its actual and predicted dimensions. The data, which is actually 1 and estimated as 1 by the model, and is actually 0 and estimated as 0 by the model, shows the number of correct predictions of the model.","baade546":"Figure shows how the performance of the 12 models changed after dimension reduction. In spite of the decrease of roughly 10% in some models, it can be said that it is quite satisfactory to establish models with only 7 features.","920a03d7":"# **Initial Models**","7a29c27d":"After the dimension reduction was performed, the new data with 7 variables was rerun with the SMOTE model (in which the test data was also SMOTE). ROC curve and confusion matrices gave relatively poor results compared to the original model, but despite the fact that the model could be expressed with only 7 variables instead of 74, no tremendous performance degradation was observed. Figures below shows the ROC curves and confusion matrices of the \u201creduced model\u201d. "}}