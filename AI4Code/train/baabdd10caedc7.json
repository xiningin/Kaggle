{"cell_type":{"7c71c7da":"code","caadfda6":"code","a5696551":"code","c835fb92":"code","e406df4f":"code","9236f0c2":"code","c4ed0f94":"code","9c093c40":"code","1b84ac1b":"code","6ea8ea5d":"code","3976e38b":"code","771e84fd":"code","0ce2a565":"code","ecb1635c":"code","5ea0a5d7":"code","90cabbbf":"code","b4f1a7c8":"code","444e8cdc":"code","a95000b6":"code","243523e7":"code","dde2511a":"code","fa56a14a":"code","b0506676":"code","66bb73ba":"code","304bf390":"code","b7a11166":"code","81ada386":"code","4e571304":"code","f194ae0e":"code","2f0d3d7f":"code","4268d8e4":"code","c0066300":"code","c5f039ca":"code","26e06186":"code","1f09de67":"code","4e458cfa":"code","8e0e0f3d":"code","cbb8862c":"code","df119277":"code","61c2acaa":"code","dfe1e9ce":"code","6a2a9c32":"code","47df992a":"code","6e2d54f9":"code","4102e1b6":"code","7aead9f1":"code","8a0895e7":"code","084748d7":"code","14ce364a":"code","d93784f9":"code","066706d4":"code","bc5161fb":"markdown","dfdc3d71":"markdown","b0dfa471":"markdown","32d3b985":"markdown","3ff60108":"markdown","6b5fe8e9":"markdown","92849a56":"markdown","299f7b2a":"markdown","5c8a9f1c":"markdown","949b5ae0":"markdown","98ef9b69":"markdown","dc3bacf2":"markdown","ddb18f10":"markdown","7b78ad72":"markdown","8848c932":"markdown","75370b4f":"markdown","6bcb5e27":"markdown","c513a409":"markdown","2ecde75a":"markdown","2dbdef35":"markdown","ffd8d1d9":"markdown","032cad64":"markdown","ceafa6f1":"markdown","4a27e802":"markdown","0096eca8":"markdown","1eb57053":"markdown","d4c8781e":"markdown","74868256":"markdown","8328604b":"markdown","494a748a":"markdown","ceb7fe2b":"markdown","3407ea84":"markdown","cc61c495":"markdown","bae03def":"markdown","9bd06d5d":"markdown","3cab74b1":"markdown","189ca99a":"markdown","0d570670":"markdown","d065ec1a":"markdown","c88bd9d3":"markdown","d6ba6303":"markdown"},"source":{"7c71c7da":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom wordcloud import WordCloud","caadfda6":"df = pd.read_csv(\"..\/input\/fake-news-dataset\/train.csv\")","a5696551":"df.head()","c835fb92":"df.isnull().sum()","e406df4f":"df = df.dropna() #  removing the missing values rows from dataset","9236f0c2":"df.info()","c4ed0f94":"\nduplicateRowsDF = df[df.duplicated([\"id\",\"title\",\"author\",\"text\"], keep=\"last\")]\nprint(\"No of duplicates found:\", duplicateRowsDF.shape[0])\n\n\n\n#removing duplicates\n# final_data = df.drop_duplicates([\"id\",\"title\",\"author\",\"text\"], keep=\"first\", inplace=False)","9c093c40":"import nltk\nimport re\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')","1b84ac1b":"stop_words = set(stopwords.words(\"english\"))","6ea8ea5d":"text = \" \".join([x for x in df.text])\nwordcloud = WordCloud(background_color = 'white', stopwords = stop_words).generate(text)\nplt.figure(figsize = (10,8))\nplt.imshow(wordcloud,interpolation = 'bilinear')\nplt.axis('off')\nplt.show()","3976e38b":"### for fake\ntext = \" \".join([x for x in df.text[df.label == 1]])\nwordcloud = WordCloud(background_color = 'white', stopwords = stop_words).generate(text)\nplt.figure(figsize = (10,8))\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.show()","771e84fd":"### for Not fake (real)\n\ntext = \" \".join([x for x in df.text[df.label == 0]])\nwordcloud = WordCloud(background_color = 'white',  stopwords = stop_words).generate(text)\nplt.figure(figsize = (10,8))\nplt.imshow(wordcloud)","0ce2a565":"X = df.drop('label',axis = 1)","ecb1635c":"### dependent Features\ny = df['label']","5ea0a5d7":"X.shape","90cabbbf":"y.shape","b4f1a7c8":"print('Number of 0 (not fake) :', df['label'].value_counts()[0])\nprint('Number of 1 (fake) :', df['label'].value_counts()[1])","444e8cdc":"sns.set_style('darkgrid')\nplt.figure(figsize=(7,5))\nsns.countplot(x='label',data=df)","a95000b6":"from tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense","243523e7":"### voc size\nvoc_size  = 5000","dde2511a":"messages = X.copy()","fa56a14a":"messages.reset_index(inplace = True)","b0506676":"\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\ncorpus = []\nfor i in range(0, len(messages)):\n\n    review = re.sub('[^a-zA-Z]', ' ', messages['title'][i])  ### apart form a-z&A-Z replace(:,*:;) with ' '\n    review = review.lower()\n    review = review.split()\n    \n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus.append(review)\n","66bb73ba":"corpus[0] # list of sentences after stemming","304bf390":"onehot_repr = [one_hot(words, voc_size) for words in corpus]\nonehot_repr[0]","b7a11166":"# pre padding to create same size of sentences.\n\nsent_length = 25\nembedded_doc = pad_sequences(onehot_repr,padding = 'pre', maxlen= sent_length)\n","81ada386":"embedded_doc[0]","4e571304":"from tensorflow.keras.layers import Dropout\n### creating model\nembedding_vector_features = 40\nmodel = Sequential()\nmodel.add(Embedding(voc_size,embedding_vector_features, input_length = sent_length))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(200))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1,activation = 'sigmoid'))\n\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nmodel.summary()","f194ae0e":"X_final = np.array(embedded_doc)\ny_final = np.array(y)","2f0d3d7f":"X_final.shape","4268d8e4":"from sklearn.model_selection import train_test_split\nX_train,X_test, y_train, y_test = train_test_split(X_final, y_final, test_size = 0.20, random_state = 0)","c0066300":"model_history = model.fit(X_train,y_train, validation_data = (X_test,y_test), epochs = 10, batch_size  =64)","c5f039ca":"y_pred = model.predict_classes(X_test)\n","26e06186":"from sklearn.metrics import confusion_matrix, accuracy_score,classification_report\ncm  = confusion_matrix(y_test, y_pred)\n\nprint(accuracy_score(y_test, y_pred),'\\n')\nprint(classification_report(y_test, y_pred))","1f09de67":"plt.title('LSTM Confusion Matrix')\nsns.heatmap(cm,annot = True, cbar = False, fmt=\"d\",cmap=\"Blues\")","4e458cfa":"test_df = pd.read_csv('..\/input\/fake-news-dataset\/test.csv')\n","8e0e0f3d":"test_df.head()","cbb8862c":"\n\ntest_df.isnull().sum()","df119277":"test_df.shape","61c2acaa":"#the solution file that can be submitted in kaggle expects it to have 5200 rows so we can't drop rows in the test dataset\ntest_df.fillna('fake fake fake',inplace=True)\n","dfe1e9ce":"\n\ncorpus_test = []\nfor i in range(0, len(test_df)):\n    review = re.sub('[^a-zA-Z]', ' ',test_df['title'][i])\n    review = review.lower()\n    review = review.split()\n    \n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus_test.append(review)","6a2a9c32":"\nonehot_repr_test=[one_hot(words,voc_size)for words in corpus_test] ","47df992a":"\nsent_length=25\n\nembedded_docs_test=pad_sequences(onehot_repr_test,padding='pre',maxlen=sent_length)\nprint(embedded_docs_test)","6e2d54f9":"X_test=np.array(embedded_docs_test)","4102e1b6":"\n\ncheck = model.predict_classes(X_test)","7aead9f1":"type(check)","8a0895e7":"check[0]","084748d7":"val = []\nfor i in check:\n    val.append(i[0])","14ce364a":"print('Loading the submissiion file')\nsubmit_df = pd.read_csv('..\/input\/fake-news-dataset\/submit.csv')\nprint(submit_df.columns)\n","d93784f9":"submit_df['label'] = val","066706d4":"#saving the submission file\n\nsubmit_df.to_csv('submission.csv',index=False)","bc5161fb":"## Data preprocessing","dfdc3d71":"#### checking the missing values","b0dfa471":"#### One hot representation","32d3b985":"#### Loading the dataset","3ff60108":"### importing libraries","6b5fe8e9":"#### splitting data into train and validate set","92849a56":"#### Independent Features & dependent Features","299f7b2a":"#### checking duplicates","5c8a9f1c":" #### wordcloud","949b5ae0":"#### Model Building","98ef9b69":"#### model training","dc3bacf2":"# about dataset :\ntrain.csv: A full training dataset with the following attributes:\n\n1. id: unique id for a news article\n2. title: the title of a news article\n3. author: author of the news article\n4. text: the text of the article; could be incomplete\n5. label: a label that marks the article as potentially unreliable. Where 1: unreliable and 0: reliable.\n\n\n\n","ddb18f10":"#### checking  null values in the test_df ","7b78ad72":"- here clearly we can see that in that above sentence we added the 0 in the starting of that particular sentence by using __pre padding__.similarly for all \nsentences.Now the sentences have same size.","8848c932":"## Applying Model on test dataset","75370b4f":"#### padding for the test dataset","6bcb5e27":"- Now we don't have any null values.So we can go for the next step.","c513a409":"#### Word Embedding","2ecde75a":"#### making predictions for the test dataset","2dbdef35":"# if this notebook was helpful please upvote & If you have any questions or suggestions, feel free to write them down in the comment section.","ffd8d1d9":"#### Stemming","032cad64":"####  creating corpus for the test_df exactly the same as we created for the training dataset****","ceafa6f1":"#### loading test dataset for prediction\n\n","4a27e802":"- Here we created the one_hot representation converted all sentence into vector.\n-  But they are not fixed size.For that we have to apply  __pre padding__.","0096eca8":"When we are dealing with text data, first we need to preprocess the text and then convert it into vectors.\n\n- Stemming is actually removing the suffix from a word and reducing it to its root word. First use stemming technique on text to convert into its root word.\n\n- We generally get text mixed up with a lot of special characters,numerical, etc. we need to take care of removing unwanted text from data. Use regular expressions to replace all the unnecessary data with spaces\n\n- Convert all the text into lowercase to avoid getting different vectors for the same word . Eg: and, And ------------> and\n\n- Remove stopWords - \u201cstop words\u201d typically refers to the most common words in a language, Eg: he, is, at etc. We need to filter stopwords.\n  1. Split the sentence into words\n  2. Extract the text except for stopwords\n  3. Again join them into sentences\n  4. Append the cleaned text into a list (corpus)\n  \n  \nNow our text is ready , convert the text into vectors \n\n","1eb57053":"- here we have null values in test_df. We have to handle the null values.","d4c8781e":"## Modeling","74868256":"![Fake-news.jpg](attachment:Fake-news.jpg)","8328604b":"## Performance Metrics and Accuracy","494a748a":"- Here we don't have predicted values(label) in test_df","ceb7fe2b":"<center><h1 style=\"color:green;\"> LSTM Architecture<\/h1><\/center>\n","3407ea84":"#### Dropping the missing values","cc61c495":"In the following analysis, we will talk about how one can create an NLP to detect whether the news is real or fake. Nowadays, fake news has become a common trend. Even trusted media houses are known to spread fake news and are losing their credibility. So, how can we trust any news to be real or fake?\n\n","bae03def":"# Fake News Classifier using LSTM","9bd06d5d":"- It will show the most frequent word in the given text.\n- Most frequent word becomes larger & darker.","3cab74b1":"#### creating one hot representation for the test corpus\n","189ca99a":"# Introduction","0d570670":"Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems. it resolve Gradient Vanishing problem & we can regulate the  sequence information by using different gates in LSTM architecture.\n","d065ec1a":"#### checking balance or imbalance dataset","c88bd9d3":"> [](http:\/\/)![Lstm.png](attachment:Lstm.png)","d6ba6303":"- In given data there are some missing values . Firstly we have to avoid the missing values."}}