{"cell_type":{"06addef3":"code","8c240700":"code","b769d40e":"code","642dd8ff":"code","591e2e12":"code","5ff48fb9":"code","711b241c":"code","060086ed":"code","bf6a21fb":"code","ec1ee58d":"code","e0a574ba":"code","b838d817":"code","b8fa61c5":"code","cc1d3b54":"code","a0111e40":"code","c8f77217":"code","7066bda0":"code","c1e1ab2c":"code","c89ebbf3":"code","410e8609":"code","de6a38f5":"code","a1e679c7":"code","ccc3045c":"code","c86d3dc8":"code","c5bcf421":"code","275e76c1":"code","ad30b99d":"code","8ee50704":"code","32fa9e4f":"code","443d388f":"code","53dfe4a1":"code","1a0cbfcb":"code","76c8901b":"code","39b76139":"code","c867c72b":"code","fa96ab0c":"code","f4d32245":"code","c898a18f":"code","a9485abf":"code","fb7a41a9":"code","5031e043":"code","12ddcabc":"code","1051859d":"code","b3b8b884":"code","4026bdcc":"code","080dd799":"code","0b213396":"code","d707a8fc":"code","b2f4e69c":"code","75e91092":"code","d48ad491":"code","fcbcc5e5":"code","1387de40":"code","3955a957":"code","5745857b":"code","dfc0efe3":"code","305c9f3f":"code","ac1d67c3":"code","d4651d1e":"code","165cc026":"code","b832d49c":"code","71b4c1f9":"code","b58d9845":"code","b57fb472":"code","a1fd6704":"code","ebfa4dfc":"code","5c6518c6":"code","74701fb3":"code","d9e41a51":"code","7d1173a3":"code","103055ba":"markdown","2ae3aea4":"markdown","288dd052":"markdown","27a2c4df":"markdown","4868ef53":"markdown","49fc5c87":"markdown","25ee7706":"markdown","f48290a0":"markdown","a1fd74d8":"markdown","510eb5a0":"markdown","1238392f":"markdown","d767b7d6":"markdown","ae50ba8c":"markdown","4a2cdfc0":"markdown","2121ca02":"markdown","abd56798":"markdown","dff732f5":"markdown","6797503c":"markdown","dd3d0eb3":"markdown","fa3a6e2e":"markdown","115833f7":"markdown","05b7272a":"markdown","4beee2e8":"markdown","ec7e5b15":"markdown","94b1c5d0":"markdown","ee6ccad1":"markdown","f1f2024f":"markdown","fb1f6d79":"markdown","6749750d":"markdown","4c1a6107":"markdown","11e5a9bd":"markdown","1c1a6bbd":"markdown"},"source":{"06addef3":"import numpy as np\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport geopandas as gpd\nfrom shapely.geometry import Point, Polygon\n\nimport ssl\n\ncontext = ssl._create_unverified_context()\nplt.style.use('fivethirtyeight')","8c240700":"train_df = pd.read_csv('..\/input\/new-york-city-taxi-fare-prediction\/train.csv', nrows=500_000)\ntest_df = pd.read_csv('..\/input\/new-york-city-taxi-fare-prediction\/test.csv', nrows=500_000)\n\nprint('Number of train: {}'.format(train_df.shape))\nprint('Number of test: {}'.format(test_df.shape))","b769d40e":"train_df.head(5)","642dd8ff":"test_df.head(5)","591e2e12":"train_df['key'] = pd.to_datetime(train_df['key'])\ntrain_df['pickup_datetime'] = pd.to_datetime(train_df['pickup_datetime'])\n\ntest_df['key'] = pd.to_datetime(test_df['key'])\ntest_df['pickup_datetime'] = pd.to_datetime(test_df['pickup_datetime'])","5ff48fb9":"train_df.isnull().sum().sort_values(ascending=False)","711b241c":"test_df.isnull().sum().sort_values(ascending=False)","060086ed":"train_df = train_df.dropna()\ntest_df = test_df.dropna() # Although there is no missing value in test dataset, perform the dropna","bf6a21fb":"print('Number of Missing values in train: {}'.format(train_df.isnull().sum().sum()))\nprint('Number of Missing values in test: {}'.format(test_df.isnull().sum().sum()))","ec1ee58d":"print('Previous dataset: {}'.format(len(train_df)))\ntrain_df = train_df[train_df.fare_amount>0]\nprint('Corrected dataset: {}'.format(len(train_df)))","e0a574ba":"fig, axs = plt.subplots(1, 2, figsize=(10,4))\nsns.kdeplot(train_df['fare_amount'].values, ax=axs[0]).set_title(\"distribution of fare amount\")\nsns.kdeplot(np.log(train_df['fare_amount'].values), ax=axs[1]).set_title(\"Distribution of log-scaled fare_amount\")","b838d817":"fig, axs = plt.subplots(figsize=(8,4))\nplt.hist(train_df['passenger_count'].values)\nplt.title(\"distribution of passenger\")","b8fa61c5":"print('Previous dataset: {}'.format(len(train_df)))\ntrain_df = train_df[train_df.passenger_count>0]\ntrain_df = train_df[train_df.passenger_count<13]\nprint('Corrected dataset: {}'.format(len(train_df)))","cc1d3b54":"fig, axs = plt.subplots(figsize=(8,4))\nplt.hist(train_df['passenger_count'].values)\nplt.title(\"Corrected distribution of passenger\")","a0111e40":"print('Max and Min pickup longitude: {} and {}'.format(max(train_df.pickup_longitude), min(train_df.pickup_longitude)))\nprint('Max and Min dropout longitude: {} and {}'.format(max(train_df.dropoff_longitude), min(train_df.dropoff_longitude)))\n\nprint('Max and Min pickup latitude: {} and {}'.format(max(train_df.pickup_latitude), min(train_df.pickup_latitude)))\nprint('Max and Min dropout latitude: {} and {}'.format(max(train_df.dropoff_latitude), min(train_df.dropoff_latitude)))\n\nprint('Mean pickup latitude: {}'.format(np.mean(train_df.pickup_latitude)))\nprint('Mean dropout latitude: {}'.format(np.mean(train_df.dropoff_latitude)))","c8f77217":"fig, ax = plt.subplots()\nax.scatter(x = train_df.pickup_longitude, y = train_df.pickup_latitude, color='blue')\nax.scatter(train_df.dropoff_longitude, train_df.dropoff_longitude, color='red')\nplt.show()","7066bda0":"def select_within_boundingbox(df, BB):\n    return (df.pickup_longitude >= BB[0]) & (df.pickup_longitude <= BB[1]) & \\\n           (df.pickup_latitude >= BB[2]) & (df.pickup_latitude <= BB[3]) & \\\n           (df.dropoff_longitude >= BB[0]) & (df.dropoff_longitude <= BB[1]) & \\\n           (df.dropoff_latitude >= BB[2]) & (df.dropoff_latitude <= BB[3])","c1e1ab2c":"min(test_df.pickup_longitude.min(), test_df.dropoff_longitude.min()), \\\nmax(test_df.pickup_longitude.max(), test_df.dropoff_longitude.max())","c89ebbf3":"min(test_df.pickup_latitude.min(), test_df.dropoff_latitude.min()), \\\nmax(test_df.pickup_latitude.max(), test_df.dropoff_latitude.max())","410e8609":"BB = (-74.3, -72.9, 40.5, 41.7)\nBB_zoom = (-74.1, -73.75, 40.6, 40.9)","de6a38f5":"print('Old size: %d' % len(train_df))\ntrain_df = train_df[select_within_boundingbox(train_df, BB)]\nprint('New size: %d' % len(train_df))","a1e679c7":"def plot_on_map(df, BB, s=10, alpha=0.2):\n    fig, axs = plt.subplots(1, 3, figsize=(20,5))\n    axs[0].scatter(df.pickup_longitude, df.pickup_latitude, zorder=1, alpha=alpha, c='r', s=s)\n    axs[0].set_xlim((BB[0], BB[1]))\n    axs[0].set_ylim((BB[2], BB[3]))\n    axs[0].set_title('Pickup locations')\n    #axs[0].imshow(extend=BB)\n\n    axs[1].scatter(df.dropoff_longitude, df.dropoff_latitude, zorder=1, alpha=alpha, c='b', s=s)\n    axs[1].set_xlim((BB[0], BB[1]))\n    axs[1].set_ylim((BB[2], BB[3]))\n    axs[1].set_title('Dropoff locations')\n    #axs[1].imshow()\n    \n    axs[2].scatter(df.pickup_longitude, df.pickup_latitude, zorder=1, alpha=alpha, c='r', s=s, label='pickup')\n    axs[2].scatter(df.dropoff_longitude, df.dropoff_latitude, zorder=1, alpha=alpha, c='b', s=s, label='dropoff')\n    axs[2].set_xlim((BB[0], BB[1]))\n    axs[2].set_ylim((BB[2], BB[3]))\n    plt.legend(loc='upper left')","ccc3045c":"plot_on_map(train_df, BB, s=1, alpha=0.3)","c86d3dc8":"plot_on_map(train_df, BB_zoom, s=1, alpha=0.3)","c5bcf421":"print('Previous dataset: {}'.format(len(train_df)))\ntrain_df = train_df[train_df.pickup_longitude>-75]\ntrain_df = train_df[train_df.pickup_longitude<-73]\ntrain_df = train_df[train_df.pickup_latitude>40]\ntrain_df = train_df[train_df.pickup_latitude<42]\n\ntrain_df = train_df[train_df.dropoff_longitude>-75]\ntrain_df = train_df[train_df.dropoff_longitude<-73]\ntrain_df = train_df[train_df.dropoff_latitude>40]\ntrain_df = train_df[train_df.dropoff_latitude<42]\nprint('Corrected dataset: {}'.format(len(train_df)))","275e76c1":"# Accurate form\ndef cal_dist(plo, pla, dlo, dla):\n    # plo = pickup longtitude\n    # pla = pickup latitude\n    # dlo = dropoff longitude\n    # dla = dropoff latitude\n    data = [train_df, test_df]\n    R = 6373.0\n    \n    for i in data:\n    \n        lat1 = np.radians(i[pla])\n        lat2 = np.radians(i[dla])\n        lon1 = np.radians(i[plo])\n        lon2 = np.radians(i[dlo])\n    \n        dlon = abs(lon2-lon1)\n        dlat = abs(lat2-lat1)\n    \n        a = np.sin(dlat \/ 2) **2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon \/ 2) **2\n        c = 2*np.arctan2(np.sqrt(a), np.sqrt(1-a))\n    \n        dist = R * c\n        i['dist'] = dist\n    return dist\n\n# Simple form\ndef dist(plo, pla, dlo, dla):\n    dist = np.abs(dlo - plo) + np.abs(dla - pla)\n    return dist","ad30b99d":"train_df['dist'] = 0.0\ncal_dist('pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude')\ncal_dist('pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude')","8ee50704":"train_df.head()","32fa9e4f":"test_df.head()","443d388f":"fig, axs = plt.subplots(1,2, figsize=(15,4))\nsns.kdeplot(train_df['dist'].values, ax=axs[0]).set_title(\"Distance distribution of Train data\")\nsns.kdeplot(test_df['dist'].values, ax=axs[1]).set_title(\"Distance distribution of Test data\")","53dfe4a1":"nyc = (-74.0063889, 40.7141667)\njfk = (-73.7822222222, 40.6441666667)\newr = (-74.175, 40.69)\nlgr = (-73.87, 40.77)","1a0cbfcb":"#def dist(plo, pla, dlo, dla):\n# pla plo dla dlo\ndef cal_airport(plo, pla, dlo, dla):\n    data = [train_df, test_df]\n    for i in data:\n        i['pickup_nyc'] = dist(nyc[0], nyc[1], i['pickup_longitude'], i['pickup_latitude'])\n        i['dropoff_nyc'] = dist(nyc[0], nyc[1], i['dropoff_longitude'], i['dropoff_latitude'])\n        \n        i['pickup_jfk'] = dist(jfk[0], jfk[1], i['pickup_longitude'], i['pickup_latitude'])\n        i['dropoff_jfk'] = dist(jfk[0], jfk[1], i['dropoff_longitude'], i['dropoff_latitude'])\n        \n        i['pickup_ewr'] = dist(ewr[0], ewr[1], i['pickup_longitude'], i['pickup_latitude'])\n        i['dropoff_ewr'] = dist(ewr[0], ewr[1], i['dropoff_longitude'], i['dropoff_latitude'])\n        \n        i['pickup_lgr'] = dist(lgr[0], lgr[1], i['pickup_longitude'], i['pickup_latitude'])\n        i['dropoff_lgr'] = dist(lgr[0], lgr[1], i['dropoff_longitude'], i['dropoff_latitude'])","76c8901b":"train_df[['pickup_nyc', 'dropoff_nyc', 'pickup_jfk', 'dropoff_jfk', 'pickup_ewr', 'dropoff_ewr', 'pickup_lgr', 'dropoff_lgr']] = 0.0\ntest_df[['pickup_nyc', 'dropoff_nyc', 'pickup_jfk', 'dropoff_jfk', 'pickup_ewr', 'dropoff_ewr', 'pickup_lgr', 'dropoff_lgr']] = 0.0\n\ncal_airport('pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude')\ntrain_df.head()","39b76139":"test_df.head()","c867c72b":"fig, axs = plt.subplots(1,4, figsize=(20,4))\nsns.kdeplot(train_df['pickup_nyc'].values, ax=axs[0]).set_title(\"Pickup from NYC\")\nsns.kdeplot(train_df['pickup_jfk'].values, ax=axs[1]).set_title(\"Pickup from JFK\")\nsns.kdeplot(train_df['pickup_ewr'].values, ax=axs[2]).set_title(\"Pickup from EWR\")\nsns.kdeplot(train_df['pickup_lgr'].values, ax=axs[3]).set_title(\"Pickup from LGR\")","fa96ab0c":"fig, axs = plt.subplots(1,4, figsize=(20,4))\nsns.kdeplot(np.log(train_df['pickup_nyc'].values), ax=axs[0]).set_title(\"Pickup from NYC(log)\")\nsns.kdeplot(np.log(train_df['pickup_jfk'].values), ax=axs[1]).set_title(\"Pickup from JFK(log)\")\nsns.kdeplot(np.log(train_df['pickup_ewr'].values), ax=axs[2]).set_title(\"Pickup from EWR(log)\")\nsns.kdeplot(np.log(train_df['pickup_lgr'].values), ax=axs[3]).set_title(\"Pickup from LGR(log)\")","f4d32245":"train_df['hour'] = train_df['pickup_datetime'].dt.hour\ntrain_df['day'] = train_df['pickup_datetime'].dt.day\ntrain_df['month'] = train_df['pickup_datetime'].dt.month\ntrain_df['year'] = train_df['pickup_datetime'].dt.year","c898a18f":"train_df.head()","a9485abf":"test_df['hour'] = test_df['pickup_datetime'].dt.hour\ntest_df['day'] = test_df['pickup_datetime'].dt.day\ntest_df['month'] = test_df['pickup_datetime'].dt.month\ntest_df['year'] = test_df['pickup_datetime'].dt.year","fb7a41a9":"test_df.head()","5031e043":"import lightgbm as lgbm\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split","12ddcabc":"train = train_df.drop(['key','pickup_datetime'], axis=1)\ntest = test_df.drop(['key', 'pickup_datetime'], axis=1)","1051859d":"X_train, X_test, y_train, y_test = train_test_split(train.drop('fare_amount', axis=1), train['fare_amount'], test_size=0.3)","b3b8b884":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","4026bdcc":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","080dd799":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","0b213396":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","d707a8fc":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =42, nthread = -1)","b2f4e69c":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.005, n_estimators=100,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","75e91092":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) # mean & the standard deviation","d48ad491":"score = rmsle_cv(ENet)\nprint(\"\\nENet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) # mean & the standard deviation","fcbcc5e5":"#score = rmsle_cv(KRR)\n#print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","1387de40":"#score = rmsle_cv(GBoost)\n#print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","3955a957":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","5745857b":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","dfc0efe3":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1) ","305c9f3f":"averaged_models = AveragingModels(models = (ENet,lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","ac1d67c3":"averaged_models.fit(X_train.values, y_train)\nstacked_train_pred = averaged_models.predict(X_train.values)\nstacked_pred = np.expm1(averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","d4651d1e":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","165cc026":"params = {'max_depth':7,\n          'eta':1,\n          'objective':'reg:linear',\n          'eval_metric':'rmse',\n          'learning_rate':0.05\n         }\nnum_rounds = 50","b832d49c":"dtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(test)\nmodel_xgbm = xgb.train(params, dtrain, num_rounds)","71b4c1f9":"xgb_train_pred = model_xgbm.predict(dtest)\nxgb_pred = np.expm1(model_xgbm.predict(dtest))","b58d9845":"params = {\n    'boosting_type':'gbdt',\n    'objective': 'regression',\n    'nthread': -1,\n    'verbose': 0,\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'max_depth': -1,\n    'subsample': 0.8,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.6,\n    'reg_lambda': 0.001,\n    'metric': 'rmse',\n    'min_split_gain': 0.5,\n    'min_child_weight': 1,\n    'min_child_samples': 10,\n    'scale_pos_weight':1,\n    'force_col_wise':True\n    }\ntrain_set = lgb.Dataset(X_train, y_train, silent=True)\nmodel_lgbm = lgb.train(params, train_set = train_set)","b57fb472":"lgb_train_pred = model_lgbm.predict(X_train)\nlgb_pred = np.expm1(model_lgbm.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","a1fd6704":"print('Number of test: {}'.format(lgb_pred.shape))","ebfa4dfc":"'''RMSE on the entire Train data when averaging'''\n\n#print('RMSLE score on train data:')\n#print(rmsle(y_train,stacked_train_pred*0.70 +\n#               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","5c6518c6":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15\nprint(len(ensemble))\nensemble","74701fb3":"sub=pd.read_csv('..\/input\/new-york-city-taxi-fare-prediction\/sample_submission.csv')","d9e41a51":"sub.head()","7d1173a3":"sub['fare_amount'] = ensemble\nsub.to_csv('submission.csv',index=False)\nsub.head()","103055ba":"### Distance from Airports\nMain airports in NYC. <br>\n1. NYC - city center\n2. JFK\n3. EWR\n4. LGR","2ae3aea4":"## 4. Submission","288dd052":"## 1. Data Cleaning\nBasic level of EDA is performed using the train dataset above.","27a2c4df":"#### XGBoost","4868ef53":"## 0. Data Load","49fc5c87":"## 3. Modeling\n#### Reference: [Stacked Regressions: Top 4% on LeaderBoard - House Prices](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard)","25ee7706":"### Passenger_count","f48290a0":"## 2. Data creating","a1fd74d8":"#### Elastic Net Regression","510eb5a0":"#### LASSO Regression","1238392f":"In conclusion, there is no missing values or null values in the dataset.","d767b7d6":"### Base models scores","ae50ba8c":"#### LightGBM","4a2cdfc0":"The number of missing value can be changed according to the data size which is selected on the data loading step. <br>\nPortion of the missing value is quite small to neglectable. <\/br>\nTherefore just <code>dropna<\/code> the missing values.","2121ca02":"Since there are some outliers, we should remove them.<br>\n##### Reference -  [NYC Taxi Fare - Data Exploration](https:\/\/www.kaggle.com\/breemen\/nyc-taxi-fare-data-exploration)","abd56798":"### Match the datetime format\nThis process is time-consuming. Therefore matching the datetime format after data cleaning is better.<br>\nHowever, I perform it previously to check the datetime format during the EDA and claning.","dff732f5":"#### Ensemble","6797503c":"#### Import librairies","dd3d0eb3":"#### Stacking models","fa3a6e2e":"### Fare_amount","115833f7":"#### NEW XGB and LGBM Models for Ensemble\nThe parameters for model are refered from [this notebook](https:\/\/www.kaggle.com\/madhurisivalenka\/cleansing-eda-modelling-lgbm-xgboost-starters))","05b7272a":"### Distance parameter","4beee2e8":"Max and Min <b>latitude<\/b> of test dataset","ec7e5b15":"### Load data\nAs the entire dataset is about 55M rows, only part of the dataset is used for EDA","94b1c5d0":"Load NYC map","ee6ccad1":"Based on the coordinate of the test dataset, bounding box can be created. <\/br>\nMax and Min <b>longitude<\/b> of test dataset","f1f2024f":"## Cross validate model","fb1f6d79":"# TAXI Fare Prediction - EDA Parts","6749750d":"### Location of pickup and dropoff\nThe location of New York city is -74.0063889 (longitude) and 40.7141667 (latitude).","4c1a6107":"### Datetime","11e5a9bd":"Location Data cleaning","1c1a6bbd":"### Check missing value\nBefore checking the data, the basic level of data cleaning is performed. <br>\nFirst of all, we should check the missing value."}}