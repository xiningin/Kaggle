{"cell_type":{"fa180c73":"code","7fed2a4f":"code","d93b2d0b":"code","3b743b0a":"code","0dbfd5b3":"code","bef276dd":"code","b9c39446":"code","1cdb5b96":"code","3037c946":"code","90327dcd":"code","b9ddab43":"code","3748565d":"code","457e449f":"code","0446b067":"code","0153bcc7":"code","ae35cacd":"code","53ffc538":"code","38af53d9":"markdown","199ee636":"markdown","f3ce0c24":"markdown","b2c7faaa":"markdown"},"source":{"fa180c73":"!pip install clean-text","7fed2a4f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport re\nfrom cleantext import clean\n\nfrom tqdm.auto import tqdm\ntqdm.pandas()\n\npd.options.display.max_colwidth = None","d93b2d0b":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers as L\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","3b743b0a":"with open('..\/input\/bhagwat-gita-in-english\/gita.txt') as f:\n    data = f.read()\ndf = pd.DataFrame({'text': data.split('\\n')})\nprint(len(df))\ndf.head()  ","0dbfd5b3":"clean_text = lambda x: clean(x,\n    fix_unicode=True,               # fix various unicode errors\n    to_ascii=True,                  # transliterate to closest ASCII representation\n    lower=True,                     # lowercase text\n    no_line_breaks=False,           # fully strip line breaks as opposed to only normalizing them\n    no_urls=True,                  # replace all URLs with a special token\n    no_emails=True,                # replace all email addresses with a special token\n    no_phone_numbers=True,         # replace all phone numbers with a special token\n    no_numbers=True,               # replace all numbers with a special token\n    no_digits=True,                # replace all digits with a special token\n    no_currency_symbols=False,      # replace all currency symbols with a special token\n    no_punct=False,                 # remove punctuations\n    replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n    replace_with_url=\"\",\n    replace_with_email=\"\",\n    replace_with_phone_number=\"\",\n    replace_with_number=\"\",\n    replace_with_digit=\"\",\n    replace_with_currency_symbol=\"<CUR>\",\n    lang=\"en\"                       # set to 'de' for German special handling\n)","bef276dd":"df['cleaned'] = df['text'].progress_apply(clean_text)\ndf['cleaned'] = df['cleaned'].apply(lambda x: re.sub(r\"\\s\\W*\\s\", \"\", x))\ndf['len'] = df['cleaned'].apply(lambda x: len(x.split()))","b9c39446":"fig = plt.figure(figsize=(12, 5))\nfig.suptitle('Length of Sentences', fontsize=12)\ndf['len'].plot.hist(bins=15);","1cdb5b96":"df = df[df['len'] > 1].drop_duplicates(subset=['cleaned']).reset_index(drop=True)","3037c946":"y = df.cleaned.values.tolist()\n\ntokenizer = Tokenizer(filters='!\"#$%&()*+,-.\/:;<=>?@^_`{|}~\\t\\n')\ntokenizer.fit_on_texts(y)\nmask_token_id = len(tokenizer.word_index)+1\ntokenizer.word_index.update({'[mask]': mask_token_id})","90327dcd":"def get_masked_input(sequence):\n    mask = np.random.randint(low=0, high=len(sequence))\n    return [token if i != mask else mask_token_id for i, token in enumerate(sequence)]","b9ddab43":"VOCAB = len(tokenizer.word_index)\nMAX_SEQ_LEN = 15\nEMBEDDING_VECTOR_LENGTH = 32\nDENSE_DIM = 32\nNUM_HEADS = 2\nRECURRENT_DROPOUT = 0.5","3748565d":"y = tokenizer.texts_to_sequences(y)\nx = [get_masked_input(seq) for seq in y]\nx = pad_sequences(x, maxlen=MAX_SEQ_LEN, padding='post')\ny = pad_sequences(y, maxlen=MAX_SEQ_LEN, padding='post')\ny = keras.utils.to_categorical(y)\n\nx.shape, y.shape","457e449f":"class PositionalEmbedding(L.Layer):\n    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.token_embeddings = L.Embedding(input_dim, output_dim)\n        self.position_embeddings = L.Embedding(sequence_length, output_dim)\n        self.sequence_length = sequence_length\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        \n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n        \n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"output_dim\": self.output_dim,\n            \"sequence_length\": self.sequence_length,\n            \"input_dim\": self.input_dim,\n        })\n        return config\n    \nclass TransformerEncoder(L.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.attention = L.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.dense_proj = keras.Sequential([L.Dense(dense_dim, activation='relu'), L.Dense(embed_dim)])\n        self.layernorm1 = L.LayerNormalization()\n        self.layernorm2 = L.LayerNormalization()\n    \n    def call(self, inputs, mask=None):\n        if mask is not None:\n            mask = mask[: tf.newaxis, :]\n        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n        proj_input = self.layernorm1(inputs + attention_output)\n        proj_output = self.dense_proj(proj_input)\n        return self.layernorm2(proj_input + proj_output)\n    \n    def get_config(self):\n        config = super().get_confog()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"num_heads\": self.num_heads,\n            \"dense_dim\": self.dense_dim\n        })\n        return config    ","0446b067":"model = keras.Sequential([\n    keras.Input(shape=(None, ), dtype=\"int64\"),\n    PositionalEmbedding(MAX_SEQ_LEN, VOCAB, EMBEDDING_VECTOR_LENGTH),\n    TransformerEncoder(EMBEDDING_VECTOR_LENGTH, DENSE_DIM, NUM_HEADS),\n    L.Dropout(0.5),\n    L.Dense(VOCAB, activation=\"softmax\")\n])\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n\nmodel.summary()\nkeras.utils.plot_model(model, show_shapes=True)","0153bcc7":"es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)\nrlp = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1)\nhistory = model.fit(x=x, y=y, validation_split=0.1, callbacks=[es, rlp], epochs=100)","ae35cacd":"fig, ax = plt.subplots(2, 1, figsize=(20, 8))\nmetrics = pd.DataFrame(history.history)\nmetrics[['acc', 'val_acc']].plot(ax=ax[0])\nmetrics[['loss', 'val_loss']].plot(ax=ax[1])\nax[0].set_title('Model Accuracy', fontsize=12)\nax[1].set_title('Model Loss', fontsize=12)\nfig.suptitle('Model Metrics', fontsize=18);","53ffc538":"query = \"to die performing [mask] is no ill;\"\nquery_token_ids = tokenizer.texts_to_sequences([query])\nquery_token_ids = pad_sequences(query_token_ids, maxlen=MAX_SEQ_LEN, padding='post')\n\npred = model(query_token_ids)\npred_seq = np.ravel(pred.numpy().argmax(axis=-1))\npred_text = ' '.join(tokenizer.index_word[token] for token in pred_seq if token != 0)\npred_text","38af53d9":"# Training","199ee636":"# Data Preparation","f3ce0c24":"# Inference","b2c7faaa":"# Model"}}