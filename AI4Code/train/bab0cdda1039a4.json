{"cell_type":{"768efa68":"code","777d58c8":"code","9d799db6":"code","f47a1476":"code","74d8ff84":"code","13084aa8":"code","143641ed":"code","c2c006be":"code","5a3b6e69":"code","d47c1004":"code","74d6e52e":"code","454a8432":"code","a9e25635":"code","58502642":"code","7dedd07d":"code","d104ca93":"code","0b9de85b":"code","607f380b":"code","e36a4004":"code","9047ace1":"code","25cc7155":"code","cd7e27dc":"code","e3e32207":"code","9e90d0e5":"code","529ab3ed":"code","71e54c87":"code","1a2ae403":"code","40aaebbf":"code","7b2ffa75":"code","5785a1ff":"code","eda52862":"code","be1d17c7":"code","0fe0a820":"code","d7769c98":"code","7e17a3e7":"code","5448862a":"code","4830f564":"code","e659ab6b":"code","5a6c37b5":"code","f57d37a5":"code","4d2eb17a":"code","958fe9b1":"code","972380a3":"markdown","5eb29f4d":"markdown","54a710d2":"markdown","8eb5648b":"markdown","861baefe":"markdown","95403454":"markdown","42f391e1":"markdown","3fb60dd8":"markdown","5fa349b5":"markdown","0b9e9577":"markdown","8c09efbd":"markdown","a46fd3b5":"markdown","59c4e0ca":"markdown"},"source":{"768efa68":"# Basic Operation\nimport pandas as pd\nimport numpy as np\n\n# Text Preprocessing & Cleaning\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nimport re\n\n\nfrom sklearn.model_selection import train_test_split # Split Data \nfrom imblearn.over_sampling import SMOTE # Handling Imbalanced\n\n# Model Building\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\n\n\nfrom sklearn.metrics import classification_report , confusion_matrix , accuracy_score # Performance Metrics  \n\n\n# Data Visualization \nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom termcolor import cprint\nimport seaborn as sns\nimport warnings   \n\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","777d58c8":"df = pd.read_csv('..\/input\/twitter-airline-sentiment\/Tweets.csv')\n","9d799db6":"df.head()","f47a1476":"df.shape","74d8ff84":"cprint('Total Null Values in Dataset :','green')\nprint(df.isnull().sum()) # showing null values of train data\nplt.figure(figsize=(14,8))\n\n\n# visualize null values\nsns.heatmap(df.isnull(), yticklabels = False, cmap='magma')\nplt.title('Total null values of dataset',size=20);\n","13084aa8":"cprint(\"Total number of sentiments of tweets :\",'green')\nprint(df.airline_sentiment.value_counts())\nplt.figure(figsize = (10, 8))\nax = sns.countplot(x = 'airline_sentiment', data = df, palette = 'pastel')\nax.set_title(label = 'Total number of sentiments of tweets', fontsize = 20)\nplt.show()","143641ed":"cprint(\"Total number of tweets for each airline :\",'green')\nprint(df.groupby('airline')['airline_sentiment'].count())\n\n\n\nplt.figure(figsize = (10, 8))\nax = sns.countplot(x = 'airline', data = df, palette = 'pastel')\nax.set_title(label = 'Total number of tweets for each airline', fontsize = 20)\nplt.show()\n\n\ncprint(\"Total number of sentiment tweets for each airline :\",'green')\nairlines= ['US Airways','United','American','Southwest','Delta','Virgin America']\nfor i in airlines :\n    print('{} : \\n'.format(i),df.loc[df.airline == i].airline_sentiment.value_counts())","c2c006be":"cprint('Reasons Of Negative Tweets :','green')\nprint(df.negativereason.value_counts())\n\nplt.figure(figsize = (24, 10))\nsns.countplot(x = 'negativereason', data = df, palette = 'hls')\nplt.title('Reasons Of Negative Tweets About Airlines', fontsize = 20)\nplt.show()","5a3b6e69":"NR_Count=df['negativereason'].value_counts()\ndef NCount(Airline):\n    airlineName =df[df['airline']==Airline]\n    count= airlineName['negativereason'].value_counts()\n    Unique_reason= df['negativereason'].unique()\n    Unique_reason=[x for x in Unique_reason if str(x) != 'nan']\n    Reason_frame=pd.DataFrame({'Reasons':Unique_reason})\n    Reason_frame['count']=Reason_frame['Reasons'].apply(lambda x: count[x])\n    return Reason_frame\n\ndef plot_reason(airline):\n    a= NCount(airline)\n    count=a['count']\n    Id = range(1,(len(a)+1))\n    plt.bar(Id,count, color=['darkviolet','yellow','blue','lime','pink','crimson','gold','cyan','orange','purple'])\n    plt.xticks(Id,a['Reasons'],rotation=90)\n    plt.title('Count of Reasons for '+ airline)\n    \nplt.figure(2,figsize=(13, 13))\nfor i in airlines:\n    indices= airlines.index(i)\n    plt.subplot(2,3,indices+1)\n    plt.subplots_adjust(hspace=0.9)\n    plot_reason(i)","d47c1004":"# Split text of Sentiments    \npositive = df[df['airline_sentiment'] == 'positive'].text\nneutral  = df[df['airline_sentiment'] == 'neutral'].text\nnegative  = df[df['airline_sentiment'] == 'negative'].text","74d6e52e":"# world could of positive sentiments\nplt.figure(figsize = (20,20)) \nworldcould_pos = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 680).generate(\" \".join(positive))\nplt.imshow(worldcould_pos,interpolation = 'bilinear')\nax.grid(False)","454a8432":"# world could of neutral sentiments\nplt.figure(figsize = (20,20)) \nworldcould_neutral = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 680).generate(\" \".join(neutral))\nplt.imshow(worldcould_neutral,interpolation = 'bilinear')\nax.grid(False)","a9e25635":"# world could of negative sentiments\nplt.figure(figsize = (20,20)) \nworldcould_neg = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 680).generate(\" \".join(negative))\nplt.imshow(worldcould_neg,interpolation = 'bilinear')\nax.grid(False)","58502642":"# convert Sentiments to 0,1,2\ndef convert_Sentiment(sentiment):\n    if  sentiment == \"positive\":\n        return 2\n    elif sentiment == \"neutral\":\n        return 1\n    elif sentiment == \"negative\":\n        return 0","7dedd07d":"# Apply convert_Sentiment function\ndf.airline_sentiment = df.airline_sentiment.apply(lambda x : convert_Sentiment(x))","d104ca93":"df.airline_sentiment","0b9de85b":"# Remove stop words\ndef remove_stopwords(text):\n    text = ' '.join([word for word in text.split() if word not in (stopwords.words('english'))])\n    return text\n\n# Remove url  \ndef remove_url(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\n# Remove punct\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\n# Remove html \ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\n# Remove @username\ndef remove_username(text):\n    return re.sub('@[^\\s]+','',text)\n\n# Remove emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\n# Decontraction text\ndef decontraction(text):\n    text = re.sub(r\"won\\'t\", \" will not\", text)\n    text = re.sub(r\"won\\'t've\", \" will not have\", text)\n    text = re.sub(r\"can\\'t\", \" can not\", text)\n    text = re.sub(r\"don\\'t\", \" do not\", text)\n    \n    text = re.sub(r\"can\\'t've\", \" can not have\", text)\n    text = re.sub(r\"ma\\'am\", \" madam\", text)\n    text = re.sub(r\"let\\'s\", \" let us\", text)\n    text = re.sub(r\"ain\\'t\", \" am not\", text)\n    text = re.sub(r\"shan\\'t\", \" shall not\", text)\n    text = re.sub(r\"sha\\n't\", \" shall not\", text)\n    text = re.sub(r\"o\\'clock\", \" of the clock\", text)\n    text = re.sub(r\"y\\'all\", \" you all\", text)\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"n\\'t've\", \" not have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'d've\", \" would have\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ll've\", \" will have\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    return text  \n\n# Seperate alphanumeric\ndef seperate_alphanumeric(text):\n    words = text\n    words = re.findall(r\"[^\\W\\d_]+|\\d+\", words)\n    return \" \".join(words)\n\ndef cont_rep_char(text):\n    tchr = text.group(0) \n    \n    if len(tchr) > 1:\n        return tchr[0:2] \n\ndef unique_char(rep, text):\n    substitute = re.sub(r'(\\w)\\1+', rep, text)\n    return substitute\n\ndef char(text):\n    substitute = re.sub(r'[^a-zA-Z]',' ',text)\n    return substitute\n\n# combaine negative reason with  tweet (if exsist)\ndf['final_text'] = df['negativereason'].fillna('') + ' ' + df['text'] \n\n\n# Apply functions on tweets\ndf['final_text'] = df['final_text'].apply(lambda x : remove_username(x))\ndf['final_text'] = df['final_text'].apply(lambda x : remove_url(x))\ndf['final_text'] = df['final_text'].apply(lambda x : remove_emoji(x))\ndf['final_text'] = df['final_text'].apply(lambda x : decontraction(x))\ndf['final_text'] = df['final_text'].apply(lambda x : seperate_alphanumeric(x))\ndf['final_text'] = df['final_text'].apply(lambda x : unique_char(cont_rep_char,x))\ndf['final_text'] = df['final_text'].apply(lambda x : char(x))\ndf['final_text'] = df['final_text'].apply(lambda x : x.lower())\ndf['final_text'] = df['final_text'].apply(lambda x : remove_stopwords(x))","607f380b":"# result\ndf['final_text']","e36a4004":"X = df['final_text']\ny = df['airline_sentiment']","9047ace1":"# Apply TFIDF on cleaned tweets\ntfid = TfidfVectorizer()\nX_final =  tfid.fit_transform(X)","25cc7155":"# Handling imbalanced using SMOTE\nsmote = SMOTE()\nx_sm,y_sm = smote.fit_resample(X_final,y)","cd7e27dc":"# Split Data into train & test \nX_train , X_test , y_train , y_test = train_test_split(x_sm , y_sm , test_size=0.2)","e3e32207":"rf = RandomForestClassifier()\nrf.fit(X_train,y_train)","9e90d0e5":"rf_prediction =  rf.predict(X_test)","529ab3ed":"accuracy_score(rf_prediction,y_test)","71e54c87":"xgb = XGBClassifier()\nxgb.fit(X_train,y_train)","1a2ae403":"xgb_prediction =  xgb.predict(X_test)","40aaebbf":"accuracy_score(xgb_prediction,y_test)","7b2ffa75":"gbc = GradientBoostingClassifier()\ngbc.fit(X_train,y_train)","5785a1ff":"gbc_prediction =  gbc.predict(X_test)","eda52862":"accuracy_score(gbc_prediction,y_test)","be1d17c7":"svm = SVC()\nsvm.fit(X_train,y_train)","0fe0a820":"svm_prediction =  svm.predict(X_test)","d7769c98":"accuracy_score(svm_prediction,y_test)","7e17a3e7":"nb = MultinomialNB()\nnb.fit(X_train,y_train)","5448862a":"nb_prediction =  nb.predict(X_test)","4830f564":"accuracy_score(nb_prediction,y_test)","e659ab6b":"ds = DecisionTreeClassifier()\nds.fit(X_train,y_train)","5a6c37b5":"ds_prediction =  ds.predict(X_test)","f57d37a5":"accuracy_score(ds_prediction,y_test)","4d2eb17a":"cr = classification_report(y_test, rf_prediction)","958fe9b1":"print(\"Classification Report:\\n----------------------\\n\", cr)\n\ncm = confusion_matrix(y_test,rf_prediction)\n\n\n# plot confusion matrix \nplt.figure(figsize=(8,6))\nsentiment_classes = ['Negative', 'Neutral', 'Positive']\nsns.heatmap(cm, cmap=plt.cm.Blues, annot=True, fmt='d', \n            xticklabels=sentiment_classes,\n            yticklabels=sentiment_classes)\nplt.title('Confusion matrix', fontsize=16)\nplt.xlabel('Actual label', fontsize=12)\nplt.ylabel('Predicted label', fontsize=12)\nplt.show()","972380a3":"## [Decision Tree](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html)","5eb29f4d":"## [RandomForest](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html)","54a710d2":"<a id=\"6\"><\/a>\n# <p style=\"background-color:#ffb037;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:8px 10px;\">VISUALIZE  MODEL PERFORMENCE<\/p>  ","8eb5648b":"<a id=\"1\"><\/a>\n# <p style=\"background-color:#ffb037;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:8px 10px;\">IMPORT LIBRARIES<\/p>    ","861baefe":"<a id=\"2\"><\/a>\n# <p style=\"background-color:#ffb037;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:8px 10px;\">LOAD DATA<\/p>    ","95403454":"<a id=\"4\"><\/a>\n# <p style=\"background-color:#ffb037;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:8px 10px;\"> TEXT PREPROCESSING AND CLEANING<\/p>  ","42f391e1":"## [Naive Bayes](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.GaussianNB.html)","3fb60dd8":"## [Support vector machine](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html)","5fa349b5":"<a id=\"5\"><\/a>\n# <p style=\"background-color:#ffb037;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:8px 10px;\">HANDLING IMBALANCE <\/p>  ","0b9e9577":"<p style = \"font-size : 50px; color : #ffff ; text-align : center; background-color : #ffb037; border-radius: 4px 4px;\"><strong>US Airline Tweets Sentiment Analysis <\/strong><\/p>\n\n\n\n\n<img  style=\"float: center;  border:5px solid #ffb037;\"  src=\"https:\/\/cdn.dribbble.com\/users\/846207\/screenshots\/7617197\/media\/e87a923768846bc12f00539d66e80931.gif\">\n\nIn this project, I will analyze the user tweets about the airlines performance in the US which was scraped from Twitter in 2015 , I will clean and preprocess the data try to handle the imblance of the data and finally create a model to analyze Sentiment of the tweets and returen whether is positive, neutral or negative. \n\n\n\n    \n<a id='top'><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<p style=\"background-color:#ffb037;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:8px 10px;\">Overview of the project<\/p>   \n\n    \n### 1) Load The Data \n\n### 2) Data Visualization \n\n### 3) Text Preprocessing and Cleaning\n    \n### 4) Handling Imbalnce         \n\n### 5) Model Building  ","8c09efbd":"## [XGBClassifier](https:\/\/xgboost.readthedocs.io\/en\/latest\/index.html)","a46fd3b5":"<a id=\"3\"><\/a>\n# <p style=\"background-color:#ffb037;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:8px 10px;\">DATA VISUALIZATION<\/p>      ","59c4e0ca":"## [GradientBoostingClassifier](http:\/\/scikitlearn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html)"}}