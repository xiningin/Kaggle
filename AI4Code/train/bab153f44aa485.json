{"cell_type":{"208f1042":"code","eacf5245":"code","0668a39e":"code","45524c0f":"code","2c8cae58":"code","62ef2070":"code","a51e5639":"code","4bd5796d":"code","64a1ee22":"code","9a55937d":"code","2339af6c":"code","a012b3e0":"code","61ae7a45":"code","7183d80f":"code","fb8eb452":"code","f06c0737":"code","31b5c852":"code","517439d1":"code","9e222f7c":"code","e760b9e9":"code","62b3b239":"code","0e604e02":"code","f0489aac":"code","e330b8f6":"code","5b4f77e1":"code","bcaa9fc1":"code","ddcf47a0":"code","c4cba58a":"code","82dbf451":"code","deac126e":"code","6623eefd":"code","1e142cff":"code","80b36053":"code","665ca754":"code","ac859d87":"code","3827ba03":"code","27d8b443":"code","20b6aa63":"code","7c31afb1":"code","78b843da":"code","7e6b7d0b":"code","81865ba9":"code","7250a874":"code","9ff5ee5a":"code","1df2ecc0":"code","c29dbbc0":"code","24205e6f":"code","18786cc9":"code","c3d4f6f0":"code","a1e6fe42":"code","61963f0c":"code","64bca998":"markdown","f3fa0896":"markdown","4c52a7fa":"markdown","226182b5":"markdown","aca34bc4":"markdown","53417c53":"markdown","84cbf1bd":"markdown","91c1da43":"markdown","163efcc5":"markdown","55879f41":"markdown","de73d914":"markdown","7432e53f":"markdown","bac8fe45":"markdown","d5977d99":"markdown","0e2885ff":"markdown"},"source":{"208f1042":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eacf5245":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set()\n","0668a39e":"!pip install openpyxl","45524c0f":"# this is an xlsx file\ntrain=pd.read_excel(r'\/kaggle\/input\/flight-fare-prediction-mh\/Data_Train.xlsx',engine='openpyxl')\ntest=pd.read_excel(r'\/kaggle\/input\/flight-fare-prediction-mh\/Test_set.xlsx',engine='openpyxl')","2c8cae58":"train.head(3)","62ef2070":"pd.set_option('display.max_columns', None)","a51e5639":"train.info()","4bd5796d":"train.dropna(inplace=True)\ntrain.isnull().sum()","64a1ee22":"train['Duration'].value_counts()","9a55937d":"#converting the object data tyoe to date\n#Year is constant throughout the data\ntrain['day']=pd.to_datetime(train.Date_of_Journey, format = '%d\/%m\/%Y').dt.day\ntrain['month']=pd.to_datetime(train.Date_of_Journey, format = '%d\/%m\/%Y').dt.month\ntrain.drop(['Date_of_Journey'], axis=1, inplace=True)\n#Year is constant throughout the data\ntrain.head(2)","2339af6c":"#now lets move to Dep_Time column of the dataset\n\ntrain['D_hour']=pd.to_datetime(train.Dep_Time).dt.hour\ntrain['D_min']=pd.to_datetime(train.Dep_Time).dt.minute\ntrain.drop(['Dep_Time'], axis=1, inplace=True)\ntrain.head(2)","a012b3e0":"#In the column Arrival_Time, we are not obtaining month and day because that will be estimated during duration of flight\ntrain['A_hour']=pd.to_datetime(train.Arrival_Time).dt.hour\ntrain['A_min']=pd.to_datetime(train.Arrival_Time).dt.minute\ntrain.drop(['Arrival_Time'], axis=1, inplace=True)\ntrain.head(2)","61ae7a45":"#as we can see the format of duration is not same \n\nlist_duration= list(train[\"Duration\"])\n\nfor i in range(len(list_duration)):\n    if len(list_duration[i].split()) !=2:\n        if \"h\" in list_duration[i]:\n            list_duration[i] = list_duration[i].strip() + \" 0m\"  \n        else:\n            list_duration[i] = \"0h \" + list_duration[i] \n            \nhours = []\nmins = []\nfor i in range(len(list_duration)):\n    hours.append(int(list_duration[i].split(sep = \"h\")[0]))   \n    mins.append(int(list_duration[i].split(sep = \"m\")[0].split()[-1])) \ntrain[\"Du_hours\"] =hours\ntrain[\"Du_mins\"] =mins\n\ntrain.drop([\"Duration\"], axis = 1, inplace = True)","7183d80f":"train.head(2)","fb8eb452":"train['Airline'].value_counts()","f06c0737":"dims = (40, 8) \n#change size accordingly, here I have used x axis as 40 to avoid overlapping of lables\nfig, ax = plt.subplots(figsize= dims)\nsns.barplot(data=train, x='Airline', y='Price')","31b5c852":"#Here Airline is Nominal Categorical data, so we perform OneHotEncoding\n\nAirline = train[[\"Airline\"]]\nAirline = pd.get_dummies(Airline, drop_first= True)\nAirline.head()","517439d1":"#Source\n\nprint(train['Source'].value_counts())\n\ndims = (20, 8) \n#change size accordingly, here i have used x axis as 40 to avoid overlapping of lables\nfig, ax = plt.subplots(figsize= dims)\nsns.barplot(data=train, x='Source', y='Price')\n\n#for nominal category\nSource = train[[\"Source\"]]\nSource = pd.get_dummies(Source, drop_first= True)\nSource.head()","9e222f7c":"#Destination\n\n#destination will be same as source\nprint(train['Destination'].value_counts())\nDestination = train[[\"Destination\"]]\nDestination = pd.get_dummies(Destination, drop_first=True)\nDestination.head()","e760b9e9":"train.head(2)\n#here we can see that Route is connected to Total_stops and the column Additional_info is almost empty\n\ntrain.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)","62b3b239":"#Total_Stops\n\nprint(train[\"Total_Stops\"].value_counts())\n\n#It's a case of Ordinal Categorical data type so we will implememnt LableEncoder\ntrain.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)\ntrain.head(3)","0e604e02":"data_train = pd.concat([train, Airline, Source, Destination], axis = 1)\ndata_train.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)","f0489aac":"print(data_train.shape)\ndata_train.head(3)","e330b8f6":"test.head(3)","5b4f77e1":"test.info()","bcaa9fc1":"test.dropna(inplace = True)\nprint(test.isnull().sum())","ddcf47a0":"#Date_of_Journey\ntest['day']=pd.to_datetime(test.Date_of_Journey, format = '%d\/%m\/%Y').dt.day\ntest['month']=pd.to_datetime(test.Date_of_Journey, format = '%d\/%m\/%Y').dt.month\ntest.drop(['Date_of_Journey'], axis=1, inplace=True)\n#Year is constant throughout the data\n\n\ntest['D_hour']=pd.to_datetime(test.Dep_Time).dt.hour\ntest['D_min']=pd.to_datetime(test.Dep_Time).dt.minute\ntest.drop(['Dep_Time'], axis=1, inplace=True)\n\ntest['A_hour']=pd.to_datetime(test.Arrival_Time).dt.hour\ntest['A_min']=pd.to_datetime(test.Arrival_Time).dt.minute\ntest.drop(['Arrival_Time'], axis=1, inplace=True)\n\nlist_duration= list(test[\"Duration\"])\n\nfor i in range(len(list_duration)):\n    if len(list_duration[i].split()) !=2:\n        if \"h\" in list_duration[i]:\n            list_duration[i] = list_duration[i].strip() + \" 0m\"  \n        else:\n            list_duration[i] = \"0h \" + list_duration[i] \n            \nhours = []\nmins = []\nfor i in range(len(list_duration)):\n    hours.append(int(list_duration[i].split(sep = \"h\")[0]))   \n    mins.append(int(list_duration[i].split(sep = \"m\")[0].split()[-1])) \ntest[\"Du_hours\"] =hours\ntest[\"Du_mins\"] =mins\n\ntest.drop([\"Duration\"], axis = 1, inplace = True)","c4cba58a":"Airline = test[[\"Airline\"]]\nAirline = pd.get_dummies(Airline, drop_first= True)\n\nSource = test[[\"Source\"]]\nSource = pd.get_dummies(Source, drop_first= True)\n\nDestination = test[[\"Destination\"]]\nDestination = pd.get_dummies(Destination, drop_first=True)\n\ntest.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)\n\ntest.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)\n\ndata_test = pd.concat([train, Airline, Source, Destination], axis = 1)\ndata_test.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)","82dbf451":"data_test.head(2)","deac126e":"data_train.columns","6623eefd":"X = data_train.loc[:,['Total_Stops','day', 'month', 'D_hour', 'D_min', 'A_hour',\n       'A_min', 'Du_hours', 'Du_mins', 'Airline_Air India', 'Airline_GoAir',\n       'Airline_IndiGo', 'Airline_Jet Airways', 'Airline_Jet Airways Business',\n       'Airline_Multiple carriers',\n       'Airline_Multiple carriers Premium economy', 'Airline_SpiceJet',\n       'Airline_Trujet', 'Airline_Vistara', 'Airline_Vistara Premium economy',\n       'Source_Chennai', 'Source_Delhi', 'Source_Kolkata', 'Source_Mumbai',\n       'Destination_Cochin', 'Destination_Delhi', 'Destination_Hyderabad',\n       'Destination_Kolkata', 'Destination_New Delhi']]\nX.head(3)\n\n#try obtaining X by dropping the 'Price' column ","1e142cff":"y = data_train.iloc[:, 1]\ny.head()","80b36053":"train.corr()\n#it's difficult to read the table so we can implement heatmap which will be visual display for the exact same table of correlational values","665ca754":"plt.figure(figsize = (20,20))\nsns.heatmap(train.corr(), annot = True, cmap = \"Blues\")\nplt.show()","ac859d87":"from sklearn.ensemble import ExtraTreesRegressor\nselection = ExtraTreesRegressor()\nselection.fit(X, y)","3827ba03":"print(selection.feature_importances_)","27d8b443":"#plot graph of feature importances for better visualization\nplt.figure(figsize = (20,8))\nfeat_importances = pd.Series(selection.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()","20b6aa63":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\nfrom sklearn.model_selection import RandomizedSearchCV","7c31afb1":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 32)\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train)","78b843da":"y_pred = model.predict(X_test)","7e6b7d0b":"model.score(X_train, y_train)","81865ba9":"model.score(X_test, y_test)","7250a874":"plt.scatter(y_test, y_pred, alpha = 0.3)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","9ff5ee5a":"#RMSE\nrmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n\n# RMSE\/(max(DV)-min(DV))\nprint(rmse\/(max(y)-min(y)))\n\nmetrics.r2_score(y_test, y_pred)","1df2ecc0":"n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\nmin_samples_split = [2, 5, 10, 15, 100]\nmin_samples_leaf = [1, 2, 5, 10]","c29dbbc0":"random_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}","24205e6f":"random = RandomizedSearchCV(estimator = model, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)","18786cc9":"random.fit(X_train,y_train)","c3d4f6f0":"random.best_params_","a1e6fe42":"final_prediction = random.predict(X_test)","61963f0c":"#please upvote to support","64bca998":"<a id='tc'><h2>Section 1<\/h2><\/a>\n****\n<h1 style=\"background:#ffffe6;color:#ff4d4d\"><center>Table of Contents and Loading the Data<\/center><\/h1>","f3fa0896":"<a id='eda'><h2>Section 2<\/h2><\/a>\n****\n<h3 style=\"background:#e6ffee;color:#004d1a\"><center>We have loaded our dataset and now we are all set for EDA (Exploratory Data Analysis)<\/center><\/h3>\n\n[back to table of contents](#tc)","4c52a7fa":"<h3 style= \"color:#004d4d\">Handling Categorical Data<\/h3>","226182b5":"* ***ExtraTreesRegressor***\nThis class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.*","aca34bc4":"* Split dataset into train and test set in order to prediction w.r.t X_test\n* Import model\n* Fit the data\n* Predict w.r.t X_test\n* In regression check RSME Score\n* Plot graph","53417c53":"* **While making changes in train data, we can use for loop for datasets: where datasets= [train,test]**\n* **But to avoid confusion I have separately edited both train and test**","84cbf1bd":"for beginners- [dataframe.loc](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.loc.html),\n[dataframe.iloc](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.iloc.html#pandas.DataFrame.iloc)","91c1da43":"Create Random Grid","163efcc5":"<a id='fm'><h2>Section 6<\/h2><\/a>\n****\n\n<h3 style=\"background:#ffbf80;color:#663300\"><center>Fitting Model<\/center><\/h3>\n\n![Random Forest](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/02\/rfc_vs_dt1.png)\n\nImage by Analystics Vidhya*\n\n\n[back to table of contents](#tc)","55879f41":"<h3 style= \"color:#004d1a\">EDA (Exploratory Data Analysis)<\/h3>","de73d914":"<a id='test'><h2>Section 4<\/h2><\/a>\n****\n<h3 style=\"background:#fff9e6;color:#4d3900\"><center>Test Set (Preprocessing)<\/center><\/h3>\n\n[back to table of contents](#tc)","7432e53f":"* [Section 2](#eda) **EDA**\n* [Section 3](#hcd) **Handling Categorical Data**\n* [Section 4](#test) **Test Set**\n* [Section 5](#fs) **Feature Selection**\n* [Section 6](#fm) **Fitting Model**\n* [Section 7](#ht) **Hyperparameter Tuning**","bac8fe45":"<a id='hcd'><h2>Section 3<\/h2><\/a>\n****\n<h3 style=\"background:#e6ffff;color:#004d4d\"><center>Handling Categorical Data<\/center><\/h3>\n\n[back to table of contents](#tc)","d5977d99":"<a id='fs'><h2>Section 5<\/h2><\/a>\n****\n<h3 style=\"background:#ffe6ff;color:#4d004d\"><center>Feature Selection<\/center><\/h3>\n\n[back to table of contents](#tc)","0e2885ff":"<a id='ht'><h2>Section 7<\/h2><\/a>\n****\n\n<h3 style=\"background:#e6ffe6;color:#003300\"><center>Hyperparameter Tuning<\/center><\/h3>\n\n* RandomizedSearchCV \n* Assign hyperparameters in form of dictionery\n* Fit the model\n* Check best paramters and best score\n\n[back to table of contents](#tc)"}}