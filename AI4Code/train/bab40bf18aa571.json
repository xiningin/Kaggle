{"cell_type":{"ab895fb5":"code","9f6cf2c1":"code","715b2bf9":"code","7dc42723":"code","b14b72ba":"code","ad34fc9c":"code","b016f987":"code","2a277fcc":"code","42bad685":"code","877eb97d":"code","56fe4e9e":"code","bfa65b52":"code","48c3c3b9":"code","5e4f5142":"code","b9836455":"code","4f10b63b":"code","c36127f4":"code","f00c77c1":"code","ef7d2989":"code","a8b59fec":"code","ccb97310":"code","9fac6017":"code","c1aa4636":"code","37a29248":"code","95498213":"code","41ff9db0":"code","90e29b70":"code","6b4a6a89":"code","f468f866":"code","b7818967":"code","6f4aca48":"code","c190cfd4":"code","2b4efb5b":"code","a6431069":"code","d8a2ef98":"code","6651d44b":"code","c1bd295f":"code","4f27cda7":"code","ed9b416f":"code","6fe04cbb":"code","6d59c838":"code","cc730eda":"code","ee636c29":"code","e1240e14":"code","4b7bcedf":"code","71762282":"code","8c2b75b6":"code","203fd615":"code","6c8febc9":"code","322a33dc":"code","54283478":"code","1fff8190":"code","6d0bbab0":"code","055f1104":"code","5ab0d2a5":"code","7d3ac108":"code","b9c1d076":"code","37319cc0":"code","fb163a0e":"code","768a6125":"code","8152945b":"code","c98ad5ae":"code","247d518e":"code","822a7795":"code","292a225b":"code","9e7a2ff8":"code","d766addd":"code","461a4a1d":"code","9ad8143a":"code","a7bd78a7":"code","e12da3be":"code","b8d7f500":"markdown","69c5429c":"markdown","62b94028":"markdown","bdef7c08":"markdown","24c925c0":"markdown","6c698020":"markdown","2339cef3":"markdown","dbfb6e81":"markdown","1dab17ff":"markdown","4400d335":"markdown","7c8dc5d0":"markdown","54492a91":"markdown","741ce394":"markdown","254a97a5":"markdown","ed0e8fe4":"markdown","94b6d1f2":"markdown","34c2f2b1":"markdown","45b5bc97":"markdown","a5ff45df":"markdown","6d46506c":"markdown","cef79739":"markdown","0e3e4053":"markdown","d2dd9ab4":"markdown","a9c71501":"markdown","5daf60de":"markdown","2fa30ab1":"markdown","e7d9b804":"markdown","9d29fc5a":"markdown","73f31105":"markdown","28e76b6d":"markdown","5ef3e42d":"markdown","121f8fef":"markdown","abf57d29":"markdown","74f4608f":"markdown","c43e0b6f":"markdown","2f377eab":"markdown","e82e1383":"markdown","cb28eb75":"markdown","d9d8bc04":"markdown","1c5d4ce9":"markdown","d217df94":"markdown","c1c21b17":"markdown","b2f04f4f":"markdown","9b3ea12c":"markdown","be46a55a":"markdown","2c5e82f9":"markdown","a0bf6b0d":"markdown","5fc9682a":"markdown","d789d576":"markdown","0f69220d":"markdown","835d93fa":"markdown","524391be":"markdown","fe6d2312":"markdown","7232fdc8":"markdown","53acaf52":"markdown","39e865ba":"markdown","613311f5":"markdown","5db595ca":"markdown","ff9310e1":"markdown","6df8fbd8":"markdown","7d12cdf4":"markdown","e86f998b":"markdown","9c99a94e":"markdown","ec17d3ea":"markdown"},"source":{"ab895fb5":"# Install openyxl for xlsx files since Pandas no longer supports them natively\n!pip install openpyxl\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport random\nimport itertools\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport math\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.image import load_img,img_to_array\n\nfrom keras.utils import np_utils\nfrom keras.models import Model, Sequential, load_model\nfrom keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Reshape, Dropout\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\n\n\n## models to import\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input\n\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle","9f6cf2c1":"## for VGG pre-trained model it only accepts a 224 x 224 image size\nimg_size = 224","715b2bf9":"# open xlsx file using the openpyxl engine\n\n#df_data = pd.read_excel(\"..\/input\/ocular-disease-recognition-odir5k\/ODIR-5K\/ODIR-5K\/data.xlsx\", engine = 'openpyxl')\n#df_data.head()\n\n# open csv file (note - target and filename only references one single eye)\n\ndf_data = pd.read_csv(\"..\/input\/ocular-disease-recognition-odir5k\/full_df.csv\")\ndf_data.head()","7dc42723":"df_data[df_data.C==1].head()","b14b72ba":"df_data.info()","ad34fc9c":"df_data[df_data == 1].sum(axis=0)","b016f987":"df_data2 = df_data.iloc[:, 1:7]\n#df_data2['filepath'] = pd.Series(df_data['filepath'])\ndf_data2.head()","2a277fcc":"img_dir = \"\/kaggle\/input\/ocular-disease-recognition-odir5k\/preprocessed_images\"","42bad685":"# Left diagnosis with 'cataract' keyword\n\ndf_data2[df_data2['Left-Diagnostic Keywords'].str.match('cataract')].head()","877eb97d":"## let's place the left cataract data into its own dataframe and print the number of rows\n\ndf_left_cat = df_data2[df_data2['Left-Diagnostic Keywords'].str.match('cataract')]\nprint(len(df_left_cat))","56fe4e9e":"# Right diagnosis with 'cataract' keyword\n\ndf_data[df_data['Right-Diagnostic Keywords'].str.match('cataract')].head()","bfa65b52":"## let's place the right cataract data into its own dataframe and print the number of rows\n\ndf_rt_cat = df_data2[df_data2['Right-Diagnostic Keywords'].str.match('cataract')]\nprint(len(df_rt_cat))","48c3c3b9":"df_cat_filenames = df_left_cat['Left-Fundus'].append(df_rt_cat['Right-Fundus'], ignore_index=True)\ndf_cat_filenames.head()\n\n","5e4f5142":"df_cat_filenames.tail()","b9836455":"len(df_cat_filenames)","4f10b63b":"img = df_cat_filenames[342]\nimage = cv2.imread(os.path.join(img_dir, img))\nplt.imshow(image)\nprint(image.shape)\nprint(img)","c36127f4":"plt.figure(figsize=(8,8))\nfor i in range(9):\n    img = df_cat_filenames[i]\n    image = cv2.imread(os.path.join(img_dir, img))\n\n    ## convert image to RGB\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    ## subplot variables - (# of rows, # of columns, iterate through locations on grid)\n    plt.subplot(3,3,i+1)\n    plt.imshow(image_rgb)\n    \n    ## label with filename and diagnosis\n    plt.xlabel('Filename: {}\\n''Cataract'.format(df_cat_filenames[i]))\n\nplt.tight_layout() ","f00c77c1":"df_data[df_data == 1].sum(axis=0)","ef7d2989":"df_data2.head()","a8b59fec":"## let's place the Normal data into its own dataframe and print the number of rows\n\ndf_left_norm = df_data2[df_data2['Left-Diagnostic Keywords'].str.match('normal')]\nprint(len(df_left_norm))","ccb97310":"## let's place the Normal data into its own dataframe and print the number of rows\n\ndf_rt_norm = df_data2[df_data2['Right-Diagnostic Keywords'].str.match('normal')]\nprint(len(df_rt_norm))","9fac6017":"df_norm_filenames = df_left_norm['Left-Fundus'].append(df_rt_norm['Right-Fundus'], ignore_index=True)\ndf_norm_filenames","c1aa4636":"## remember we're choosing 572 to equal the number of cataract images we pulled\n\ndf_norm_filenames_random = df_norm_filenames.sample(n = 572)\ndf_norm_filenames_random.head()\n\n## looks like the index is out of order now that we got all the randos","37a29248":"## we will reset the index as well\ndf_norm_filenames_random = df_norm_filenames_random.reset_index(drop=True)\ndf_norm_filenames_random","95498213":"plt.figure(figsize=(8,8))\nfor i in range(9):\n    img = df_norm_filenames_random[i]\n    image = cv2.imread(os.path.join(img_dir, img))\n\n    ## convert image to RGB\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    ## subplot variables - (# of rows, # of columns, iterate through locations on grid)\n    plt.subplot(3,3,i+1)\n    plt.imshow(image_rgb)\n    \n    ## label with filename and diagnosis\n    plt.xlabel('Filename: {}\\n''Normal'.format(df_norm_filenames_random[i]))\n\nplt.tight_layout() ","41ff9db0":"df_cat_filenames = pd.DataFrame(df_cat_filenames, columns = [\"filename\"])\n#df_cat_filenames.set_index(\"filename\", inplace = True)\n\n# add a new column of '1' to the dataframe\ndf_cat_filenames[\"label\"] = \"cataract\"\n\ndf_cat_filenames.head()","90e29b70":"df_norm_filenames_random = pd.DataFrame(df_norm_filenames_random, columns = [\"filename\"])\n#df_cat_filenames.set_index(\"filename\", inplace = True)\n\n# add a new column of '1' to the dataframe\ndf_norm_filenames_random[\"label\"] = \"normal\"\n\ndf_norm_filenames_random.head()","6b4a6a89":"df_combined = df_cat_filenames.append(df_norm_filenames_random, ignore_index=True)\ndf_combined","f468f866":"df_combined_random = df_combined.sample(frac=1).reset_index(drop=True)\ndf_combined_random","b7818967":"# pull 80% of the combined dataset and reserve it for the training data\n# the data generator will automatically create a validation set for us later\n\ndf_train = df_combined_random.sample(frac=0.8,random_state=42)\ndf_train.reset_index(drop=True)\n\n# exclude the 80% that was already chosen, the remaining 20% will go into testing\ndf_test = df_combined_random.drop(df_train.index)\ndf_test.reset_index(drop=True)\n\nprint(len(df_combined_random))\nprint(len(df_train))\nprint(len(df_test))","6f4aca48":"train_datagen=tf.keras.preprocessing.image.ImageDataGenerator(\n            rescale=1.\/255.,\n            validation_split=0.20,\n            rotation_range=90,\n#            width_shift_range=0.2,\n#            height_shift_range=0.2,\n            horizontal_flip=True,\n            vertical_flip=True,\n            shear_range=0.2,\n            brightness_range=[0.3,1]    \n#            zoom_range=0.2\n            )\n\n## for testing we don't want to do too much augmentation, we'll just scale it.\n\ntest_datagen=ImageDataGenerator(rescale=1.\/255.)","c190cfd4":"df_train['label'] = df_train['label'].astype(str)\ndf_test['label'] = df_test['label'].astype(str)","2b4efb5b":"train_generator=train_datagen.flow_from_dataframe(\ndataframe=df_train,\ndirectory=img_dir,\nx_col=\"filename\",\ny_col=\"label\",\nsubset=\"training\",\nbatch_size=32,\nseed=42,\nshuffle=True,\nclass_mode=\"categorical\",\ntarget_size=(img_size,img_size))\n\n## validation set is created from the training set, \n## we set it at 20% of the training data in the previous code\n\nvalid_generator=train_datagen.flow_from_dataframe(\ndataframe=df_train,\ndirectory=img_dir,\nx_col=\"filename\",\ny_col=\"label\",\nsubset=\"validation\",\nbatch_size=32,\nseed=42,\nshuffle=True,\nclass_mode=\"categorical\",\ntarget_size=(img_size,img_size))\n\n\n\ntest_generator=test_datagen.flow_from_dataframe(\ndataframe=df_test,\ndirectory=img_dir,\nx_col=\"filename\",\ny_col=\"label\",\nbatch_size=32,\n#seed=42,\nshuffle=False,\nclass_mode=\"categorical\",\ntarget_size=(img_size,img_size))","a6431069":"train_image_data, train_labels = train_generator.next()\n#train_image_data[0]\ntrain_image_data.shape","d8a2ef98":"train_labels[0]","6651d44b":"plt.imshow(train_image_data[0], interpolation='nearest')\nplt.show()\n#images = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))","c1bd295f":"plt.figure(figsize=(8,8))\n\nfor i in range(9):\n    plt.subplot(3, 3, i + 1) \n    plt.imshow(train_image_data[i], interpolation='nearest')\n    \nplt.show() ","4f27cda7":"# get VGG16 base model\nvgg16 = keras.applications.vgg16.VGG16(input_shape=(224, 224, 3),\n                                       weights='imagenet',\n                                       include_top=False)\n\n# add new dense layers at the top\nx = keras.layers.Flatten()(vgg16.output)\nx = keras.layers.Dense(1024, activation='relu')(x)\nx = keras.layers.Dropout(0.5)(x)\nx = keras.layers.Dense(128, activation='relu')(x)\n\n## remember we are using 2 outputs only\npredictions = keras.layers.Dense(2, activation='softmax')(x)\n\n# define and compile model\nmodel = keras.Model(inputs=vgg16.inputs, outputs=predictions)\nfor layer in vgg16.layers:\n    layer.trainable = False\n    \nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])","ed9b416f":"checkpoint = ModelCheckpoint(\"vgg16_1.h5\", \n                             monitor='val_accuracy', \n                             verbose=1, \n                             save_best_only=True, \n                             save_weights_only=False, \n                             mode='auto', \n                             period=1)\n\nearly = EarlyStopping(monitor='val_accuracy', \n                      min_delta=0, \n                      patience=3, \n                      verbose=1, \n                      mode='auto')\n","6fe04cbb":"batch_size = 32\nn_spe = train_generator.samples \/\/ batch_size\nn_val_steps = valid_generator.samples \/\/ batch_size\nn_epochs = 30\n\nprint(n_spe,n_val_steps)","6d59c838":"hist = model.fit(train_generator,\n                        steps_per_epoch=n_spe,\n                        validation_data=valid_generator,\n                        validation_steps=n_val_steps,\n                        epochs=n_epochs,\n                        shuffle=True,\n                        workers=5,\n                        use_multiprocessing=True,\n                        callbacks=[checkpoint,early])","cc730eda":"from keras.models import load_model\nmodel = load_model('.\/vgg16_1.h5')\n","ee636c29":"plt.plot(hist.history[\"accuracy\"])\nplt.plot(hist.history['val_accuracy'])\nplt.title(\"Model Accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"Accuracy\",\"Validation Accuracy\"])\nplt.show()\n\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title(\"Model Loss\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"loss\",\"Validation Loss\"])\nplt.show()","e1240e14":"test_generator.reset()","4b7bcedf":"pred = model.predict_generator(test_generator,verbose=1,steps=test_generator.samples\/batch_size)\n","71762282":"## let's get the first 10 rows\nprint(pred[0:10])","8c2b75b6":"predicted_class_idx=np.argmax(pred,axis=1)\n\n## print the same 10 rows\n\nprint(predicted_class_idx[0:10])","203fd615":"print(len(predicted_class_idx))","6c8febc9":"model.evaluate(test_generator,use_multiprocessing=True,workers=10)\n","322a33dc":"valid_generator.class_indices.items()","54283478":"valid_labels = dict((value,key) for key,value in valid_generator.class_indices.items())\npred_labels = [valid_labels[key] for key in predicted_class_idx]","1fff8190":"pred_labels[1:10]","6d0bbab0":"filenames = test_generator.filenames\nprediction_df = pd.DataFrame({'Filename': filenames,'Prediction': pred_labels})\nprediction_df.head()","055f1104":"prediction_df.iloc[35]","5ab0d2a5":"print(test_generator.filenames[35])\nprint(test_generator.labels[35])","7d3ac108":"test_file_names=test_generator.filenames  # sequential list of name of test files of each sample\ntest_labels=test_generator.labels # is a sequential list  of test labels for each image sample\nclass_dict= test_generator.class_indices # a dictionary where key is the class name and value is the label for the class\n\nprint (class_dict) # have a look at the dictionary\n\nnew_dict={} \n\nfor key in class_dict: # set key in new_dict to value in class_dict and value in new_dict to key in class_dict\n    value = class_dict[key]\n    new_dict[value] = key\n\nprint('  RESULT  PREDICT      TRUE CLASS       FILENAME ' ) # adjust spacing based on your class names\n\nfor i, p in enumerate(pred):\n    pred_index=np.argmax(p) # get the index that has the highest probability\n    pred_class=new_dict[pred_index]  # find the predicted class based on the index\n    true_class=new_dict[test_labels[i]] # use the test label to get the true class of the test file\n    file=test_file_names[i]\n    \n    if true_class == pred_class:\n        result = \"Correct\"\n    else:\n        result = \"Wrong  \"\n    \n    \n    print(f' {result}   {pred_class}    {true_class}      {file}')","b9c1d076":"x_test, y_test = test_generator.next()","37319cc0":"print(len(x_test))\nprint(len(y_test))\n\n\n## compare this length to our prediction data and notice the difference.  \n\nprint(len(pred))","fb163a0e":"loss,accuracy = model.evaluate(x_test,y_test)\nprint(\"loss:\",loss)\nprint(\"Accuracy:\",accuracy)","768a6125":"test_image_data, test_labels = test_generator.next()","8152945b":"print(test_image_data.shape)\nprint(test_labels.shape)","c98ad5ae":"z = 0\n#test filename\n\ntest_file_names=test_generator.filenames[z]\nprint(test_file_names)\n\ntest_labels_example=test_generator.labels[z]\nprint(test_labels_example)\n\npred_labels[z]","247d518e":"### keep\n### show all test labels\n#test_generator.labels","822a7795":"test_labels[5]","292a225b":"test_class_idx=np.argmax(test_labels,axis=1)\n#test_class_idx[4]\ntest_class_idx","9e7a2ff8":"plt.figure(figsize=(12,6))\nfor i in range(18):\n    sample = random.choice(range(test_generator.samples))\n#    print(str(sample))\n    img = test_generator.filenames[sample]\n    image = cv2.imread(os.path.join(img_dir, img))\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    category = test_generator.labels[sample]\n    pred_category = pred_labels[sample]\n\n#    sample = random.choice(range(0,test_generator.samples))    \n#    image = test_image_data[sample]\n#    category = test_class_idx[sample]\n#    pred_category = pred_labels[sample]\n           \n    \n    if category== 1:\n        label = \"Normal\"\n    else:\n        label = \"Cataract\"\n        \n    if pred_category== \"normal\":\n        pred_label = \"Normal\"\n    else:\n        pred_label = \"Cataract\"\n\n    if label == pred_label:\n        result = \"Correct\"\n    else:\n        result = \"Wrong\"\n\n        \n    plt.subplot(3,6,i+1)\n    plt.imshow(image_rgb, interpolation='nearest')\n    plt.xlabel(\"Actual:{}\\nPrediction:{}\\nResult:{}\\nF:{}\\nRow:{}\".format(label,\n                                                                          pred_label,\n                                                                          result,\n                                                                          test_generator.filenames[sample],\n                                                                          sample))\nplt.tight_layout() ","d766addd":"print('Classification Report')\ntarget_names = ['Cataract', 'Normal']\nprint(classification_report(test_generator.classes, predicted_class_idx, target_names=target_names))","461a4a1d":"cm = confusion_matrix(test_generator.labels, predicted_class_idx)\nprint('Confusion Matrix')\ncm","9ad8143a":"def plot_confusion_matrix(cm, classes,\n                        normalize=False,\n                        title='Confusion matrix',\n                        cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","a7bd78a7":"cm_plot_labels = ['cataract','normal']","e12da3be":"plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')","b8d7f500":"## Plot accuracy versus loss","69c5429c":"### Ocular disease map\n\n* Normal (N),\n* Diabetes (D),\n* Glaucoma (G),\n* Cataract (C),\n* Age related Macular Degeneration (A),\n* Hypertension (H),\n* Pathological Myopia (M),\n* Other diseases\/abnormalities (O)","62b94028":"## Classification report","bdef7c08":"## Create check point and early stop\n\nA check point allows us to monitor the acuraccy and perform some task, in this case we will save the model as a file.\n\nThe early stop will monitor accuracy for a 'patience' parameter and if there is no improvement it will stop.","24c925c0":"## Append dataframes into a single dataset\n\nWe have both cataract and normal dataframes with labels, we need to combine them to form a single dataframe to feed into our image generator.\n","6c698020":"## Convert test labels\n\nLet's look at a single example of our test label data","2339cef3":"## Randomize our final combined dataset\n\nPrior to feeding this organized set of cataract and normal images we need to randomize the rows within so that when we train we will train from a random pool of samples.\n\nIf we append, we end up adding a column for the old index values, which we don't want any new columns, so we'll drop the old index and allow the new dataframe to have a new one.","dbfb6e81":"## Image generator results\n\nLet's look at the actual data that gets created from the image generator.  The \"next\" command splits the generator data into two numpy arrays - one with image data and one with label data.","1dab17ff":"## Select only Cataract related data\n\nSince we can't rely on the encoded categories, we will search for terms in the keywords columns for each eye.","4400d335":"So if we add these up we have over 5000 images to choose from (comprised of both left and right side eye images) that are considered \"normal\".  This is far more than we have for our 'cataract' data that had only 572 images.  So, in that case let's pull an equal amount of randomly selected \"normal\" images.","7c8dc5d0":"## Plot confusion matrix","54492a91":"## The VGG16 model diagram","741ce394":"## Create test, train and validation image data generators","254a97a5":"## Import image description data","ed0e8fe4":"It seems safe to say that if either eye has a cataract, then it is properly labeled in the keywords column of that eye with the word \"cataract\".  At this point, it's safe to go ahead and ignore the encoded columns since they simply don't provide accurate mapping to our left\/right eye requirements when we process the left\/right images.","94b6d1f2":"## Correct vs. Incorrect list\n\nNow that we have lightly verified our test and prediction data is aligned, we can list all of our predictions and compare them to our test data.  This is only an example using a small test dataset (under 1000).  I can't possibly recommend this for a large dataset.","34c2f2b1":"Use the training label string name keys (cataract and normal) and convert the one-hot encoded predicted class labels into string names.  What these lines do is grab the name of the category (the value) and place it into the predicted labels variable where it meets the criteria of the value.","45b5bc97":"## Convert row objects to string type\n\nAlthough flow from dataframe will work with other datatypes, I kept getting errors that were requesting to convert the dataframe rows into string type.","a5ff45df":"## Another approach to generator data\n\nYou can split your test generator data another way by using 'next'.  This will trigger the generator to only give you the amount of data in your batch size variable, so don't expect this is exactly like the previous method.  Each time you trigger the test generator, or any generator, you are only going to get a limited amount.\n\nHere we are splitting the test generator object data into two numpy arrays, one with actual image data and one with label data.","6d46506c":"flow() returns an iterator yielding tuples of (x, y), you can access elements using test_iterator.next().","cef79739":"## Prediction vs Test image grid\n\nWe're working with images, so let's actually look at the results of our prediction vs test images.\n\nLet's grab our test data items, the image array and the encoded image category.  Let's start by renaming them to something more friendly.","0e3e4053":"## Set up file paths\n\n","d2dd9ab4":"Looks like the random sampling ","a9c71501":"## Split our dataframe into test, train, validation dataframes\n\nSince we're going to continue to use a dataframe with a labels column, we can't use the common sklearn test-train-split function.  we will instead simplify things and create three data frames by pulling random samples out and then excluding those samples from another set by using the drop method.","5daf60de":"## Convert raw prediction data\n\nLet's convert the raw data into something more friendly, 1's and 0's.  Argmax will do this for us!","2fa30ab1":"### Find the number of positive \"1\" results in the dataset.  \n\nWe will look at the columns (this is the axis=0 flag) and count all the '1' in it.","e7d9b804":"We see 2101 \"normal\" encoded results, but we can't really trust this since this may not take into account both eyes.  Let's check out the keyword results that contain \"normal\", separately for both eyes:","9d29fc5a":"we're still working with the full set of labels from our generator","73f31105":"## Make our predictions with predict.generator\n\nWe have several generators that feed a set batch size of image data into whatever we want, whenever we call the generator iterator.  We need to reset the test generator so that we aren't feeding any old image data further down the line.\n\nThe prediction output is only a set of categorical labels of 0 and 1 - in this case Cataract is 0 and normal is 1.  The prediction output is NOT image data.","28e76b6d":"## View example images\n\nNow we have a dataframe of images, let's take a look at a few.  Note that when the image file is read with the OpenCV function imread(), the order of colors is BGR (blue, green, red).  Let's select a random image from our dataframe.","5ef3e42d":"In our grid we'd like to see a few parameters such as filename, label and categorical value (1, 0).\n\nLet's test the output of those parameters from the generators","121f8fef":"## Load our saved model","abf57d29":"## Verify prediction dataframe results\n\nSince we want to prove that our test data is indeed being tested, let's verify that our prediction data is indeed aligned with the test filename, row and diagnosis.","74f4608f":"## Select and create a random set","c43e0b6f":"## Constants for our training run\n\nSPE is 'steps per epoch'\n\nn_val_steps is 'number of validation steps'","2f377eab":"![](https:\/\/neurohive.io\/wp-content\/uploads\/2018\/11\/vgg16-1-e1542731207177.png)\n\nhttps:\/\/neurohive.io\/en\/popular-networks\/vgg16\/","e82e1383":"## Add category label to list\n\nPreviously we created these two objects that weren't true dataframes, they became lists after we did an append of columns.  For convenience, we need to turn them back into dataframes:\n\n- df_norm_filenames_random\n- df_cat_filenames\n\nImages will be labeled normal or cataract, in this case we will be using an image data generator that will do the one-hot-encoding for us, so we can preserve the actual \"word\" string labels.","cb28eb75":"## Build a new dataframe\n\nWe don't actually need to do this, but, building a new dataframe can be beneficial if you need to do any prediction lookups that require a dataframe, or you could export this out into a CSV.  Pretty handy to have around.","d9d8bc04":"Compare that same row to our test generator data","1c5d4ce9":"## Combine cataract filename data\n\nTo view, parse and run loops on images it is much more convenient to have all the cataract images in one dataframe as a single column.  We will append only the image filenames into a new dataframe.","d217df94":"## Create grid of example images\n\nFrom our cataract set of image file paths, let's create a comparitive grid to see what cataracts actually looks like.  We'll convert them to a more realistic RGB pallete as well.","c1c21b17":"From:  https:\/\/stackoverflow.com\/questions\/46820609\/how-the-keras-steps-per-epoch-in-fit-generator-works\n\nThe steps_per_epoch parameter is the number of batches of samples it will take to complete one full epoch. This is dependent on your batch size. The batch size is set where you initialize your training data. For example, if you're doing this with ImageDataGenerator.flow() or ImageDataGenerator.flow_from_directory(), the batch size is specified with the batch_size parameter in each of these.\n\nWith 3000 samples for example:\n\n```\nIf your batch size was 100, then steps_per_epoch would be 30.\nIf your batch size was 10, then steps_per_epoch would be 300.\nIf your batch size was 1, then steps_per_epoch would be 3000.\n```\n\nThis is because steps_per_epoch should be equivalent to the total number of samples divided by the batch size. The process of implementing this in Keras is available in the two videos below.","b2f04f4f":"## Create a grid of \"normal\" images","9b3ea12c":"## Evaluate our model's accuracy","be46a55a":"## Image Generator flow from Dataframe\n\nThis was developed by:  https:\/\/vijayabhaskar96.medium.com\/tutorial-on-keras-flow-from-dataframe-1fd4493d237c\n\nWe have a nice dataframe that we have created and Keras combined with this handy function will allow us to generate augmented and scaled images with minimal effort.  \n\nHere we will add some parameters that will perform some adjustment to the images.","2c5e82f9":"## Augmented image examples\n\nLet's make a grid of data augmented images","a0bf6b0d":"We can see that the y-axis is the true (test) labels and the x-axis is the predicted output.  \n\nCorrect items are where both the true and predicted cells meet - the categories are similar, in this case, a darker color.\n\nIncorrect items are lighter in color and are where the true and predicted cells have disimilar categories.","5fc9682a":"We can see that the left images were first and then let's check to make sure the tail end of the dataframe has the right eye images.","d789d576":"Let's find out how many rows of cataract images we're working with\n","0f69220d":"Let's look at the label format","835d93fa":"## Constants for data preparation","524391be":"Convert our test label tuples into a single one dimensional array with argmax.\n\nIt's clear to see the entire test labels array is only 32 elements since this is the batch size limit that was set in the generator parameters.","fe6d2312":"Take a look at the shape difference in these arrays.  The shapes should clue you in and make sense about the data within them.  \n\nThe shape of the image data structured like this:\n\n( NUMBER OF ROWS, DIMENSION, DIMENSION, DEPTH)\n\nLet's unpack the shape:\n\n- For images this is pretty clear, we have 32 samples gathered by our test generator\n- 224 in the horizontal dimensions of the image\n- 224 in the vertical dimension of the image\n- 3 depth since this is a color image\n\nWhile the label data is structured like this:\n\n- 32 samples\n- 2 depth since we have 2 outputs, the numerical probablity of each of our categories","7232fdc8":"## Gather the normal images\nWe now need to gather a set of normal images.  First let's see how many normal images there are available to us - we did this earlier but let's take a look again.  Remember, we're working with two eyes and we noticed that some of the encoded columns don't have data for both eyes, so the whole story is not truly told.  We need to look at the labeling in the keywords as that seems to be the source of truth when considering both eyes.","53acaf52":"## Confusion matrix\n\nCode from:  https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n\nA confusion matrix is an EXTREMELY HANDY WAY of displaying the amount of correct vs. incorrect predictions.  But the raw confusion matrix output is a pretty basic array.  This can be plotted to be much more friendly to read.","39e865ba":"## Problems with the encoded categories\n\nWe found that relying on the encoded diagnosis categories \"N\tD\tG\tC\tA\tH\tM\tO\" could be problematic in the sense that some of the values were positive for both normal and disease eyes, but didn't specify which eye. The problem with this is that the files and diagnosis are orientation specific and the row of data contains both eyes information.  If we use the categories exclusively, we run into the problem later on where pulling specific eye orientation data is impossible as it is lost in the encoded categories.  \n\nTherefore we will exclude those categories for this notebook although they could certainly be used for other investigations.","613311f5":"## Prediction raw data\n\nLet's look at what the predict generator outputs to get some better insight into what the mechanics of the model output really are!","5db595ca":"## Plot our comparison grid\n\nThe data generator encoded our categories for us, whereby cataract = 0 and normal = 1.  Since we are using image generators we will only be able to gather 32 images at a time since this is the batch size we set.  If we increase our batch size, the generator will gather more images and send them through the fit function during training","ff9310e1":"We can use this also as a gauge for evaluating our model's accuracy, this of course should be relatively close to our accuracy from the previous model predict function.","6df8fbd8":"## Build our transfer learning model\n\nHere we load the vgg16 model, but since we are doing transfer learning, we don't want the fully connected portion, so we will rip it off with the include_top FALSE option.  We don't want this because we have a custom dataset that has a custom number of outputs, 2 in this case. \n\nWe will use the 'imagenet' weights since those filters \/ weights \/ kernels will help us find edges, corners, rounded areas of our images.  Those are already baked in and we can speed this process up by using some that are proven to be effective for thousands of images.\n\nWe will drop 50% of the perceptrons during the final layer to avoid overfitting.\n\nWe will use the ADAM gradient descent optimizer as well.","7d12cdf4":"Let's grab a single augmented image","e86f998b":"## Combine normal filename data","9c99a94e":"### Exploratory data analysis\n\nOne big issue with this data set is that it has images for BOTH left and right eyes in separate files, which totally makes sense since as humans we have two eyes.  However, in the numeric encoded diagnosis (the columns labeled C, D, G...) it doesn't specify the left or right eye, the logic is that if there was any eye with an issue, then a 'true' '1' would result.  '1' doesn't tell us if this was a left or right eye, and in some queries this doesn't matter, but when we pull our image files, this becomes and issue.\n\nWe really need to go by the data from the keyword diagnosis, if there is a \n\nLet's take a look at the keywords of images that have a cataract diagnosis","ec17d3ea":"## Convert one-hot category labels to text labels\n\n1's and 0's are great, but we are going to eventually need the actual text names.\n\nThe generators have lots and lots of parameters for you to use."}}