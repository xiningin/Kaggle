{"cell_type":{"92eca56d":"code","c8593868":"code","f8e80cf4":"code","66257c41":"code","41bce9a0":"code","62f312c0":"code","f6177e48":"code","baa079c2":"code","dce15b25":"code","62db3680":"code","188b1466":"code","929dcb94":"code","fc9adfa9":"code","63eb34d4":"code","1b015f22":"code","86c9256b":"code","8191fa45":"code","4c4d5d74":"code","c9133a7a":"code","c3b6c5ee":"code","b73cbcfa":"code","b3035477":"code","bc9791f4":"code","60a907fb":"code","2bf625b9":"code","279ed976":"code","2f556eeb":"code","0f449730":"code","9b83987f":"code","90ea24f4":"code","1e8f3433":"code","07fbe44a":"code","1a7b7c15":"code","7e3ffb57":"code","b75d3de2":"code","fc08b3cd":"code","acf307d0":"code","e94b5137":"code","3426e168":"markdown","1290ff98":"markdown","69e702e0":"markdown","643a498f":"markdown","6767fa69":"markdown","b9bb3db6":"markdown","18a37a5a":"markdown","dcfef043":"markdown","697a46c2":"markdown","2655fb1e":"markdown","84c9565b":"markdown","571e0b09":"markdown","9275a84a":"markdown","4109f892":"markdown","d3967a86":"markdown","20d97b6e":"markdown","5267b7b5":"markdown","bc294d1f":"markdown","afebbf5b":"markdown","c9813d48":"markdown","b07e7c36":"markdown","30a871d5":"markdown","279cb19d":"markdown"},"source":{"92eca56d":"import os\nimport re\nimport html as ihtml\nimport warnings\nimport random\nwarnings.filterwarnings('ignore')\n\nos.environ[\"TFHUB_CACHE_DIR\"] = \"\/tmp\/\"\n\nimport spacy\nnlp = spacy.load('en_core_web_sm')\nnlp.remove_pipe('parser')\nnlp.remove_pipe('ner')\n#nlp.remove_pipe('tagger')\n\nimport pyLDAvis\nimport pyLDAvis.gensim\npyLDAvis.enable_notebook()\n\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport numpy as np\nimport gensim\nimport scipy\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport umap\n\npd.set_option('display.max_colwidth', -1)\n\nSEED = 13\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\n%matplotlib inline","c8593868":"input_dir = '..\/input\/'\n\nquestions = pd.read_csv(os.path.join(input_dir, 'questions.csv'))","f8e80cf4":"# Spacy Tokenfilter for part-of-speech tagging\ntoken_pos = ['NOUN', 'VERB', 'PROPN', 'ADJ', 'INTJ', 'X']\n\ndef clean_text(text, remove_hashtags=True):\n    text = BeautifulSoup(ihtml.unescape(text), \"lxml\").text\n    text = re.sub(r\"http[s]?:\/\/\\S+\", \"\", text)\n    if remove_hashtags:\n        text = re.sub(r\"#[a-zA-Z\\-]+\", \"\", text)\n    text = re.sub(r\"\\s+\", \" \", text)        \n    return text\n\ndef nlp_preprocessing(data):\n    \"\"\" Use NLP to transform the text corpus to cleaned sentences and word tokens\n\n    \"\"\"    \n    def token_filter(token):\n        \"\"\" Keep tokens who are alphapetic, in the pos (part-of-speech) list and not in stop list\n\n        \"\"\"    \n        return not token.is_stop and token.is_alpha and token.pos_ in token_pos\n    \n    processed_tokens = []\n    data_pipe = nlp.pipe(data, n_threads=4)\n    for doc in data_pipe:\n        filtered_tokens = [token.lemma_.lower() for token in doc if token_filter(token)]\n        processed_tokens.append(filtered_tokens)\n    return processed_tokens","66257c41":"questions['questions_full_text'] = questions['questions_title'] + ' '+ questions['questions_body']","41bce9a0":"sample_text = questions[questions['questions_full_text'].str.contains(\"&a\")][\"questions_full_text\"].iloc[0]\nsample_text","62f312c0":"sample_text = clean_text(sample_text)\nsample_text","f6177e48":"sample = nlp_preprocessing([sample_text])\n\" \".join(sample[0])","baa079c2":"%%time\nquestions['questions_full_text'] = questions['questions_full_text'].apply(clean_text)","dce15b25":"questions['questions_full_text'].sample(2)","62db3680":"%%time\nquestions['nlp_tokens'] = nlp_preprocessing(questions['questions_full_text'])","188b1466":"questions['nlp_tokens'].sample(2)","929dcb94":"# Gensim Dictionary\nextremes_no_below = 10\nextremes_no_above = 0.6\nextremes_keep_n = 8000\n\n# LDA\nnum_topics = 10\npasses = 20\nchunksize = 1000\nalpha = 1\/50","fc9adfa9":"def get_model_results(ldamodel, corpus, dictionary):\n    \"\"\" Create doc-topic probabilities table and visualization for the LDA model\n\n    \"\"\"  \n    vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=False)\n    transformed = ldamodel.get_document_topics(corpus)\n    df = pd.DataFrame.from_records([{v:k for v, k in row} for row in transformed])\n    return vis, df  ","63eb34d4":"%%time\nlda_tokens = questions['nlp_tokens']\n\n# Gensim Dictionary\nlda_dic = gensim.corpora.Dictionary(lda_tokens)\nlda_dic.filter_extremes(no_below=extremes_no_below, no_above=extremes_no_above, keep_n=extremes_keep_n)\nlda_corpus = [lda_dic.doc2bow(doc) for doc in lda_tokens]\n\nlda_tfidf = gensim.models.TfidfModel(lda_corpus)\nlda_corpus = lda_tfidf[lda_corpus]\n\n# Create LDA Model\nlda_model = gensim.models.ldamodel.LdaModel(lda_corpus, num_topics=num_topics, \n                                            id2word = lda_dic, passes=passes,\n                                            chunksize=chunksize,update_every=0,\n                                            alpha=alpha, random_state=SEED)","1b015f22":"# Create Visualization and Doc-Topic Probapilities\nlda_vis, lda_result = get_model_results(lda_model, lda_corpus, lda_dic)\nlda_questions = questions[['questions_id', 'questions_title', 'questions_body']]\nlda_questions = pd.concat([lda_questions, lda_result.add_prefix('Topic_')], axis=1)","86c9256b":"# Disabled for compatibility issue\n# lda_vis","8191fa45":"print(\"\\n\\n\".join([\"Topic{}:\\n {}\".format(i, j) for i, j in lda_model.print_topics()]))","4c4d5d74":"corpus_csr = gensim.matutils.corpus2csc(lda_corpus).T","c9133a7a":"# There exist some zero rows:\nnon_zeros = np.where(corpus_csr.sum(1) != 0)[0]\nprint(corpus_csr.shape[0])\ncorpus_csr = corpus_csr[non_zeros, :]\nprint(corpus_csr.shape[0])","c3b6c5ee":"# Normalize by row\ncorpus_csr = corpus_csr.multiply(\n    scipy.sparse.csr_matrix(1\/np.sqrt(corpus_csr.multiply(corpus_csr).sum(1))))","b73cbcfa":"# Double check the norms\nnp.sum(np.abs(corpus_csr.multiply(corpus_csr).sum(1) - 1) > 0.001)","b3035477":"%%time\nembedding = umap.UMAP(metric=\"cosine\", n_components=2).fit_transform(corpus_csr)","bc9791f4":"df_emb = pd.DataFrame(embedding, columns=[\"x\", \"y\"])\ndf_emb[\"label\"] = np.argmax(lda_result.iloc[non_zeros].fillna(0).values, axis=1)","60a907fb":"df_emb_sample = df_emb.sample(5000)\nfig, ax = plt.subplots(figsize=(12, 10))\nplt.scatter(\n    df_emb_sample[\"x\"].values, df_emb_sample[\"y\"].values, s=2, c=df_emb_sample[\"label\"].values# , cmap=\"Spectral\"\n)\nplt.setp(ax, xticks=[], yticks=[])\ncbar = plt.colorbar(boundaries=np.arange(11)-0.5)\ncbar.set_ticks(np.arange(10))\nplt.title(\"TF-IDF matrix embedded into two dimensions by UMAP\", fontsize=18)\nplt.show()","2bf625b9":"g = sns.FacetGrid(df_emb, col=\"label\", col_wrap=2, height=5, aspect=1)\ng.map(plt.scatter, \"x\", \"y\", s=0.2).fig.subplots_adjust(wspace=.05, hspace=.5)","279ed976":"# keep well separated points\ndf_emb_sample = df_emb[np.amax(lda_result.iloc[non_zeros].fillna(0).values, axis=1) > 0.7]\nprint(\"Before:\", df_emb.shape[0], \"After:\", df_emb_sample.shape[0])\ng = sns.FacetGrid(df_emb_sample, col=\"label\", col_wrap=2, height=5, aspect=1)\ng.map(plt.scatter, \"x\", \"y\", s=0.3).fig.subplots_adjust(wspace=.05, hspace=.5)","2f556eeb":"embed = hub.Module(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/3\")","0f449730":"import logging\nfrom tqdm import tqdm_notebook\ntf.logging.set_verbosity(logging.WARNING)\nBATCH_SIZE = 128\n\nsentence_input = tf.placeholder(tf.string, shape=(None))\n# For evaluation we use exactly normalized rather than\n# approximately normalized.\nsentence_emb = tf.nn.l2_normalize(embed(sentence_input), axis=1)\n\nsentence_embeddings = []       \nwith tf.Session() as session:\n    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n    for i in tqdm_notebook(range(0, len(questions), BATCH_SIZE)):\n        sentence_embeddings.append(\n            session.run(\n                sentence_emb, \n                feed_dict={\n                    sentence_input: questions[\"questions_full_text\"].iloc[i:(i+BATCH_SIZE)].values\n                }\n            )\n        )","9b83987f":"sentence_embeddings = np.concatenate(sentence_embeddings, axis=0)\nsentence_embeddings.shape","90ea24f4":"%%time\nimport umap\nembedding = umap.UMAP(metric=\"cosine\", n_components=2).fit_transform(sentence_embeddings)","1e8f3433":"df_se_emb = pd.DataFrame(embedding, columns=[\"x\", \"y\"])\ndf_se_emb[\"label\"] = np.argmax(lda_result.fillna(0).values, axis=1)\ndf_se_emb[\"label\"] = df_se_emb[\"label\"].astype(\"category\")","07fbe44a":"df_emb_sample = df_se_emb.sample(5000)\nfig, ax = plt.subplots(figsize=(12, 10))\nplt.scatter(\n    df_emb_sample[\"x\"].values, df_emb_sample[\"y\"].values, s=2, c=df_emb_sample[\"label\"].values# , cmap=\"Spectral\"\n)\nplt.setp(ax, xticks=[], yticks=[])\ncbar = plt.colorbar(boundaries=np.arange(11)-0.5)\ncbar.set_ticks(np.arange(10))\nplt.title(\"Sentence embeddings embedded into two dimensions by UMAP\", fontsize=18)\nplt.show()","1a7b7c15":"g = sns.FacetGrid(df_se_emb, col=\"label\", col_wrap=2, height=5, aspect=1)\ng.map(plt.scatter, \"x\", \"y\", s=0.2).fig.subplots_adjust(wspace=.05, hspace=.5)","7e3ffb57":"def find_similar(idx, top_k):\n    cosine_similarities = sentence_embeddings @ sentence_embeddings[idx][:, np.newaxis]\n    return np.argsort(cosine_similarities[:, 0])[::-1][1:(top_k+1)]","b75d3de2":"IDX = 0\nsimilar_ids = find_similar(IDX, top_k=3).tolist()\nfor idx in [IDX] + similar_ids:\n    print(questions[\"questions_full_text\"].iloc[idx], \"\\n\")","fc08b3cd":"IDX = 5\nsimilar_ids = find_similar(IDX, top_k=3).tolist()\nfor idx in [IDX] + similar_ids:\n    print(questions[\"questions_full_text\"].iloc[idx], \"\\n\")","acf307d0":"IDX = 522\nsimilar_ids = find_similar(IDX, top_k=3).tolist()\nfor idx in [IDX] + similar_ids:\n    print(questions[\"questions_full_text\"].iloc[idx], \"\\n\")","e94b5137":"IDX = 13331\nsimilar_ids = find_similar(IDX, top_k=3).tolist()\nfor idx in [IDX] + similar_ids:\n    print(questions[\"questions_full_text\"].iloc[idx], \"\\n\")","3426e168":"### Dimension Reduction using UMAP","1290ff98":"Looks reasonable. We can actually build a recommendation upon these embeddings directly.","69e702e0":"## Sentence Embeddings Space\n\nThe model used is the universal sentence encoder (large\/transformer) version 3. The extracted sentence embeddings will have a dimension of 512. Here we also use cosine similarity.","643a498f":"### Extract the embeddings","6767fa69":"### Examine the sentence embeddings","b9bb3db6":"# An Attemplt to Visualize Topic Model (LDA)\n\nI'm curious how well topic models (latent Dirichlet allocation algorithm, specifically) performs in terms of seperating the corpus into semantically similar groups. This is an attempt of me to visually figure it out.\n\nThe topic model is taken from [CareerVillage.org Recommendation Engine](https:\/\/www.kaggle.com\/danielbecker\/careervillage-org-recommendation-engine) by [Daniel Becker](https:\/\/www.kaggle.com\/danielbecker) (it's fantastic; go upvote) with a few modifications:\n\n1. Slightly different text cleaning\n2. Hashtags are removed. I want to separate natural language understanding from (implicit) tag grouping in this task, that is, only focus on the questions, not tags.\n3. The number of topics is reduced from 18 to 10. (This is just to make life easier for me. You can use whatever number you like.)\n\nYou can skip all the preprocessing and model fitting using the links below. For visualization, first I try to plot the tf-idf feature space using [UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction](https:\/\/umap-learn.readthedocs.io\/en\/latest\/). Then I use the [Universal Sentence Encoder](https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/3) from Google and plot the sentence embeddings. (The sentence embeddings are more likely to capture semantic similarities than tf-idf).","18a37a5a":"We removed ~3,000 questions (the number is relative low, so in this model topics are quite distinguishable). The results are not much different, though.","dcfef043":"## Imports","697a46c2":"## TF-IDF Space\n\nHere we use cosine similarities. Questions with similar term frequency distribution will have higher similarity scores. It requires the TF-IDF vectors to have the same L2-norms. (I'm not sure if UMAP does the normalization for us or not. The safer way is to do it ourselves.)","2655fb1e":"### Prepare TF-IDF Matrix","84c9565b":"Arguably the pattern is slightly stronger here than the TF-IDF one. We can see more obvious small clusters(higher density areas) here. But still a topic can contains some very semantically different questions (large distance).","571e0b09":"Questions from different topics all got mixed together. Not much can be seen in the above plot. Let's try to plot each topic separately:","9275a84a":"### The Real Deal","4109f892":"We're able to see some patterns now. Note since LDA is a mixture model, a question can be assigned multiple topics. What if we only plot questions with only one dominant topic?","d3967a86":"## Topic Modeling","20d97b6e":"### Checking","5267b7b5":"### Visualization","bc294d1f":"Still the topics are not very separable from each other. Let's check the topics one by one:","afebbf5b":"### Show Topics","c9813d48":"Let's check if the sentence embeddings are doing a good job by finding similar questions to some random samples of questions:","b07e7c36":"## Summary\n\nTopic model provides unsupervised clustering\/classification that is (somewhat) interpretable. Most of the time we can tell what a topic is about once we see its most distinguishable words. However, as we found out in the visualization, the sensitivity of the model is quite high. A topic can contain a very diverse set of questions. One way to solve this is to increase the number of topics, but it is hard to tell how many is enough.\n\nWe also explores the topic model visualized under sentence embeddings space. The word\/term-based model seems to successfully captured some semantic information as we can see some obvious clusters.\n\nBut if we just want to find questions in the close vicinity in the semantic space of a question, directly using the sentence embeddings and cosing similarity might be enough. ","30a871d5":"## Contents\n\n1. [Imports](#Imports)\n2. [Preprocessing](#Preprocessing)\n  * [Checking](#Checking)\n  * [The Real Deal](#The-Real-Deal)\n3. [Topic Modeling](#Topic-Modeling)\n  * [Show Topics](#Show-Topics)\n4. [TF-IDF Space](#TF-IDF-Sace)\n  * [Prepare TF-IDF Matrix](#Prepare-TF-IDF-Matrix)\n  * [Dimension Reduction using UMAP](#Dimension-Reduction-using-UMAP)\n5. [Sentence Embeddings Space](#Sentence-Embeddings-Space)\n  * [Extract the embeddings](#Extract-the-embeddings)\n  * [Visualization](#Visualization)\n  * [Examine the sentence embeddings](#Examine-the-sentence-embeddings)\n6. [Summary](#Summary)","279cb19d":"## Preprocessing"}}