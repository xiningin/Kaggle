{"cell_type":{"624a7f4a":"code","428a42ae":"code","b778e215":"code","3e67136c":"code","f7bd13a7":"code","eab31d9f":"code","5d5c62c8":"code","e5aba97c":"code","1779937a":"code","d7e2c786":"code","c3d27693":"code","38f1b2e1":"code","b1211af1":"code","218968b3":"code","aba2c9a5":"code","a86124af":"code","430b176f":"code","2a4efe09":"code","9ca85be5":"markdown","7498f6d5":"markdown","fdcab14c":"markdown","c86816b5":"markdown","9a1f23d8":"markdown","112b2b5a":"markdown","870d098a":"markdown","86c52c52":"markdown","df7c9f56":"markdown","5584832c":"markdown","857fe4c6":"markdown","9c5e27fd":"markdown","d71efa02":"markdown","cd55721a":"markdown"},"source":{"624a7f4a":"import numpy as np \nimport pandas as pd \nimport seaborn as sn\nimport sklearn\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn import linear_model\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import cross_val_score","428a42ae":"df_treino = pd.read_csv(\"\/kaggle\/input\/atividade-regressao-PMR3508\/train.csv\", na_values = '?')\ndf_treino.set_index('Id',inplace=True)\ndf_treino.head()\n","b778e215":"df_teste = pd.read_csv(\"\/kaggle\/input\/atividade-regressao-PMR3508\/test.csv\", na_values = '?')\ndf_teste.set_index('Id',inplace=True)\ndf_teste.head()","3e67136c":"print(\"DADOS DE TREINO \")\n\ndf_treino.describe()","f7bd13a7":"df_treino.isnull().sum()","eab31d9f":"print(\"DADOS DE TESTE \")\n\ndf_teste.describe()","5d5c62c8":"df_teste.isnull().sum()","e5aba97c":"correlacao = df_treino.corr()\nplt.figure(figsize=(12, 8))\nsn.heatmap(correlacao, annot=True)\nplt.show()","1779937a":"df_treino[\"position\"] = df_treino[\"latitude\"]\/df_treino[\"longitude\"]\ndf_treino[\"bedrooms\/rooms\"] = df_treino[\"total_bedrooms\"]\/df_treino[\"total_rooms\"]\ndf_treino[\"density\"] = df_treino[\"households\"]\/df_treino[\"population\"]\ndf_treino = df_treino.drop(['median_age'], axis = 1)\ncorrelacao_nova = df_treino.corr()\nplt.figure(figsize=(18, 8))\nsn.heatmap(correlacao_nova, annot=True)\nplt.show()","d7e2c786":"df_treino = df_treino.drop(['latitude', 'longitude', 'total_rooms', 'total_bedrooms', 'population','households', 'bedrooms\/rooms'], axis = 1)\ncorrelacao_final = df_treino.corr()\nplt.figure(figsize=(10, 8))\nsn.heatmap(correlacao_final, annot=True)\nplt.show()","c3d27693":"Y_treino = np.array(df_treino['median_house_value']) \nX_treino = df_treino.drop(['median_house_value'], axis = 1)\nX_treino.head()","38f1b2e1":"X_treino.hist(bins=50,figsize=(20,15))\nplt.show()","b1211af1":"scaler = sklearn.preprocessing.MinMaxScaler()\n\nX_treino = pd.DataFrame(scaler.fit_transform(X_treino))\n\nX_treino.hist(bins=50,figsize=(20,15))\nplt.show()","218968b3":"Linear = linear_model.LinearRegression()\nR2_Linear = cross_val_score(Linear, X_treino, Y_treino, cv=10, scoring=\"r2\")\n\nLasso = linear_model.Lasso(alpha=10)\nR2_Lasso = cross_val_score(Lasso, X_treino, Y_treino, cv=10, scoring=\"r2\")\n\nRidge = sklearn.linear_model.Ridge(alpha=10)\nR2_Ridge = cross_val_score(Ridge, X_treino, Y_treino, cv=10, scoring=\"r2\")\n\nKNN = KNeighborsRegressor(n_neighbors=25, p=1)\nR2_KNN = cross_val_score(KNN, X_treino, Y_treino, cv=10, scoring=\"r2\")","aba2c9a5":"print(\"R2 Linear Regression: \", R2_Linear.mean())\nprint(\"R2 Lasso Regression: \", R2_Lasso.mean())\nprint(\"R2 Ridge Regression: \", R2_Ridge.mean())\nprint(\"R2 KNN: \", R2_KNN.mean())","a86124af":"KNN.fit(X_treino,Y_treino)","430b176f":"df_teste[\"position\"] = df_teste[\"latitude\"]\/df_teste[\"longitude\"]\ndf_teste[\"bedrooms\/rooms\"] = df_teste[\"total_bedrooms\"]\/df_teste[\"total_rooms\"]\ndf_teste[\"density\"] = df_teste[\"households\"]\/df_teste[\"population\"]\ndf_teste = df_teste.drop(['median_age'], axis = 1)\ndf_teste = df_teste.drop(['latitude', 'longitude', 'total_rooms', 'total_bedrooms', 'population','households', 'bedrooms\/rooms'], axis = 1)\n\nX_teste = pd.DataFrame(scaler.fit_transform(df_teste))","2a4efe09":"Y_teste = KNN.predict(X_teste)\noutput = pd.DataFrame()\noutput[0] = df_teste.index\noutput[1] = Y_teste\noutput.columns = ['Id', 'median_house_value']\noutput.to_csv('submission.csv',index = False)","9ca85be5":"Agora podemos importar os data frame California Housing, tanto de treino quanto de teste, e visualizar quais s\u00e3o seus atributos e suas caracter\u00edsticas b\u00e1sicas : ","7498f6d5":"### Compara\u00e7\u00e3o dos resultados de R2 para os diferentes modelos","fdcab14c":"# PMR3508 - Aprendizado de M\u00e1quina e Reconhcimento de Padr\u00f5es\n                            Atividade Regress\u00e3o Linear : California Housing\n\n                                        Paolla Furquim Daud - 9345533\n\nO objetivo deste Exerc\u00edcio Programa \u00e9 utilizar os dados da base California Housing para construir um regressor que preveja o pre\u00e7o de um im\u00f3vel na Calif\u00f3rnia. Para isso, vamos seguir os seguintes passos :\n\n1) Importa\u00e7\u00e3o e Visualiza\u00e7\u00e3o Geral dos Dados Dispon\u00edveis\n\n2) An\u00e1lise Explorat\u00f3ria dos Dados \n\n3) Sele\u00e7\u00e3o e Manipula\u00e7\u00e3o de Features\n \n4) Treinamento e Valida\u00e7\u00e3o de Diferentes Regressores e Sele\u00e7\u00e3o do Melhor Modelo\n\n5) Aplica\u00e7\u00e3o na Base de Teste\n\n## 1) Importa\u00e7\u00e3o e Visualiza\u00e7\u00e3o Geral dos Dados Dispon\u00edveis\n\nPrimeiramente devemos importar as bibliotecas necess\u00e1rias para lidar com a base de dados \n","c86816b5":"Por fim, podemos realizar a predi\u00e7\u00e3o dos valores da target usando a KNN treinada anteriormente e export\u00e1-los.","9a1f23d8":"J\u00e1 podemos ver que a base de dados \u00e9 composta de 8 features e a target, median_house_value. Todos os dados envolvidos s\u00e3o num\u00e9ricos, o que elimina a necessidade de tratamentos do tipo encoding. Agora podemos checar estat\u00edsticas b\u00e1sicas sobre a distribui\u00e7\u00e3o de cada uma destas colunas.","112b2b5a":"Agora podemos aplicar um Scaler do tipo minMax e plotar novamente os histogramas. Podemos ver que as distribui\u00e7\u00f5es mant\u00e9m o mesmo formato, por\u00e9m agora todas s\u00e3o limitadas a valores entre 0 e 1.","870d098a":"Por fim, o \u00faltimo passo necess\u00e1rio para preparar os dados antes de treinar os regressores \u00e9 a normaliza\u00e7\u00e3o ou padroniz\u00e3o. Dessa forma, as diferentes ordens de grandeza das features n\u00e3o ter\u00e3o influ\u00eancia nos seus pesos.\n\nPara observar o efeito da reescala dos dados, vamos primeiro observar as distribui\u00e7\u00f5es originais dos atributos.","86c52c52":"Observando a matriz acima podemos tirar algumas conclus\u00f5es sobre os dados dispon\u00edveis :\n\n**A)** H\u00e1 uma correla\u00e7\u00e3o alt\u00edssima entre os atributos latitude e longitude. Esse resultado faz sentido, considerando a forma esguia e inclinada do estado da Calif\u00f3rnia. Por\u00e9m, isso n\u00e3o \u00e9 ideal para o treinamento do regressor, pois aumentar\u00e1 o peso dessa informa\u00e7\u00e3o. Dessa forma, podemos combinar os dois atributos em um novo atributo, latitude\/longitude, e observar qual sua correla\u00e7\u00e3o com o valor da target\n\n**B)** A grande maioria dos atributos dispon\u00edveis (com exce\u00e7\u00e3o de median_income, total_rooms e latitude) tem uma correla\u00e7\u00e3o consideravelmente baixa com o valor da target e portanto n\u00e3o adicionam informa\u00e7\u00f5es valiosas ao regressor. Podemos tentar combin\u00e1-las para obter novas features que apresentem maior influ\u00eancia no valor de interesse, ou simplesmente desconsider\u00e1-las.\n\n**C)** Al\u00e9m disso, vemos correla\u00e7\u00f5es alt\u00edssimas tamb\u00e9m entre os atributos total_rooms, total_bedrooms, population e households, e tamb\u00e9m valores altos para as correla\u00e7\u00f5es destas com median_age. Isso indica que, como no caso de latitude e longitude, elas podem ser combinadas de modo a criar novos atributos, eliminando essa redund\u00e2ncia e poss\u00edvelmente atingindo uma maior correla\u00e7\u00e3o com o valor da target.\n\n**D)** A feature median_age n\u00e3o apresenta correla\u00e7\u00e3o relevante com nenhuma das outras features. Considerando que temos 5 features dispon\u00edveis para criar combina\u00e7\u00f5es novas e que n\u00e3o podemos usar uma mesma feature mais de uma vez (pois as novas features criadas com base numa mesma original teriam correla\u00e7\u00e3o entre si, enviesando o modelo) e que as outras 4 apresentam correla\u00e7\u00f5es mais altas entre si, n\u00e3o faz sentido tentar combin\u00e1-la. Assim, median_age ser\u00e1 desconsiderada. \n\nDe posse destas informa\u00e7\u00f5es podemos avan\u00e7ar para a Sele\u00e7\u00e3o e Manipula\u00e7\u00e3o de Features, buscando obter maiores correla\u00e7\u00f5es com a target.\n\n## 3) Sele\u00e7\u00e3o e Manipula\u00e7\u00e3o de Features\n\nCom isso em mente, vamos deletar a coluna median_age do nosso dataframe, e criar as seguintes novas features :\n\n**1)** Position = Latitude\/Longitude\n\n**2)** Bedrooms\/Rooms = total_bedrooms\/total_rooms\n\n**3)** Density = households\/population","df7c9f56":"Olhando novamente para o heatmap, podemos ver que o atributo position apresenta uma melhora em rela\u00e7\u00e3o aos atributos latitude e longitude, pois sua correla\u00e7\u00e3o com o valor da target \u00e9 de 0.21, maior do que a de ambos separadamente (-0.044 para longitude e -0.15 para latitude).\n\nO mesmo vale para a feature bedrooms\/rooms, que apresenta correla\u00e7\u00e3o de -0.25 com a target, melhor do que as originais (0.14 e 0.056). Isso \u00e9 um bom argumento para utiliz\u00e1-la no treinamento dos nossos regressores. Entretanto, tamb\u00e9m podemos observar uma correla\u00e7\u00e3o razo\u00e1vel desse valor com a median_income, que n\u00e3o era vis\u00edvel antes da combina\u00e7\u00e3o dos par\u00e2metros. Dessa forma, vamos elimin\u00e1-la do nosso dataframe, para evitar enviesar o modelo.\n\nPor \u00faltimo, a feature Density tem correla\u00e7\u00e3o de 0.27 com a median_house_value, consideravelmente melhor do que os valores de 0.072 e -0.02 dos atributos que \u00e0 comp\u00f5e.\n\nDessa forma, podemos eliminar todas as features originais mencionadas acima e reavaliar o heatmap dos nossos dados.\n","5584832c":"Vemos que ambas as bases n\u00e3o tem dados faltantes, portanto n\u00e3o precisamos nos preocupar com o tratamento destes. Tamb\u00e9m podemos ver que as distribui\u00e7\u00f5es dos valores nas duas bases s\u00e3o condizentes, o que significa que n\u00e3o temos problemas de dados desbalanceados. Ao mesmo tempo, \u00e9 poss\u00edvel observar que h\u00e1 uma grande variedade na ordem de grandeza m\u00e9dia de cada atributo, portanto uma normaliza\u00e7\u00e3o pode ser adequada para garantir que isso n\u00e3o interfira nos pesos dos mesmos. A partir de agora faremos a an\u00e1lise dos dados utilizando somente a base de treino \n\n## 2) An\u00e1lise Explorat\u00f3ria dos Dados\n\nO heatmap \u00e9 uma \u00f3tima ferramenta para analisar a relev\u00e2ncia dos atributos dispon\u00edveis. As features idealmente devem ter alta correla\u00e7\u00e3o com o valor da target, e baixa correla\u00e7\u00e3o entre si.","857fe4c6":"Avaliando os regressores pela m\u00e9trica escolhida, o KNN \u00e9 o que apresenta melhor performance. Dessa forma, usaremos ele na fase seguinte, para predizer os valores da target na base de dados de teste.","9c5e27fd":"Todas as features selecionadas apresentam correla\u00e7\u00e3o de ao menos 20% com o valor da target, al\u00e9m de nao apresentarem altas correla\u00e7\u00f5es umas com as outras. Portanto, esse formato do dataframe \u00e9 apropriado para o treinamento dos nossos regressores.\n\nAntes de realizar o \u00faltimo passo na prepara\u00e7\u00e3o dos dados, podemos agora separar o nosso dataframe de treino entre X e Y, sendo X os atributos usados na regress\u00e3o e Y a vari\u00e1vel de interesse.","d71efa02":"Com os dados prontos, podemos iniciar o treinamento e valida\u00e7\u00e3o dos regressores.\n\n## 4) Treinamento e Valida\u00e7\u00e3o de Diferentes Regressores e Sele\u00e7\u00e3o do Melhor Modelo\n\n\nOs seguintes modelos de regressores ser\u00e3o treinados e validados atrav\u00e9s de valida\u00e7\u00e3o cruzada, obtendo a m\u00e9trica R2. O que apresentar melhor performance nos dados de treino ser\u00e1 usado para gerar os valores estimados para os dados de teste.\n\n**A)** Linear Regression\n\n**B)** Lasso Regression\n\n**C)** Ridge Regression\n\n**D)** KNN\n","cd55721a":"## 5) Aplica\u00e7\u00e3o na Base de Teste\n\nPara realizar a predi\u00e7\u00e3o na base de teste, primeiramente devemos aplicar nela as mesmas altera\u00e7\u00f5es que fizemos na base de treino durante a prepara\u00e7\u00e3o dos dados. Ou seja, criar as features novas, eliminar as irrelevantes e aplicar o minMaxScaler. "}}