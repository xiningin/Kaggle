{"cell_type":{"4bbd9489":"code","c067535a":"code","8d0c4d4a":"code","25e9a07f":"code","b5d8dc79":"code","3ba23176":"code","9cb15707":"code","23d54f8b":"code","8f72ab67":"code","47a38d98":"code","bedf0eb2":"code","d3eebdde":"code","1ad59742":"code","0bb44ee1":"code","219608a9":"code","1c7c9e5b":"code","30fd73ae":"code","3aa6ba22":"code","8ec04f78":"code","25fdc40e":"code","e0b93c2c":"code","1fd18d62":"code","879b96f5":"code","91ea793f":"code","bd447fb0":"markdown","60771efc":"markdown","5f09b8b9":"markdown","82eca086":"markdown","1881d77a":"markdown","13ca04c4":"markdown","ab706a07":"markdown","dfbf48b5":"markdown","b4cc8bc7":"markdown","ea333895":"markdown"},"source":{"4bbd9489":"!pip install -qq git+https:\/\/www.github.com\/keras-team\/keras-contrib.git","c067535a":"import scipy\nfrom keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\nfrom keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\nfrom keras.layers import BatchNormalization, Activation, ZeroPadding2D\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.convolutional import UpSampling2D, Conv2D\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam\nimport datetime\nimport matplotlib.pyplot as plt\nimport sys\nimport numpy as np\nimport os\nfrom IPython.display import Image\nfrom keras.utils.vis_utils import model_to_dot\nfrom tqdm import tqdm_notebook","8d0c4d4a":"from keras import backend as K\nfrom keras import layers\n# reparameterization trick\n# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n# z = z_mean + sqrt(var) * epsilon\ndef conv2d(layer_input, filters, f_size=4, strides=2, **kwargs):\n    \"\"\"Layers used during downsampling\"\"\"\n    d = Conv2D(filters, kernel_size=f_size, strides=strides, padding='same', **kwargs)(layer_input)\n    d = LeakyReLU(alpha=0.2)(d)\n    d = InstanceNormalization()(d)\n    return d\n        \ndef sampling_2d(args):\n    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n    # Arguments\n        args (tensor): mean and log of variance of Q(z|X)\n    # Returns\n        z (tensor): sampled latent vector\n    \"\"\"\n\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim_x = K.int_shape(z_mean)[1]\n    dim_y = K.int_shape(z_mean)[2]\n    dim_c = K.int_shape(z_mean)[3]\n    # by default, random_normal has mean = 0 and std = 1.0\n    epsilon = K.random_normal(shape=(batch, dim_x, dim_y, dim_c))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\n\ndef z_log_loss(x):\n    return 0.5*K.mean(K.exp(x)-x)\n\n\ndef z_mean_loss(x):\n    return 0.5*K.mean(K.square(x))\n\n\n\ndef make_vae_layer(x, prefix=''):\n    in_shape = x._keras_shape[1:]\n    \n    z_mean = conv2d(x, filters=in_shape[2], f_size=1, strides=1, activity_regularizer=z_mean_loss)\n    z_log_var = conv2d(x, filters=in_shape[2], f_size=1, strides=1, activity_regularizer=z_log_loss)\n    z = layers.Lambda(sampling_2d, output_shape=in_shape, name='z_{}'.format(prefix))([z_mean, z_log_var])\n    return z","25e9a07f":"def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n    \"\"\"Layers used during upsampling\"\"\"\n    u = UpSampling2D(size=2)(layer_input)\n    u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n    if dropout_rate:\n        u = Dropout(dropout_rate)(u)\n    u = InstanceNormalization()(u)\n    u = Concatenate()([u, skip_input])\n    return u\n\nclass CycleGAN():\n    def __init__(self, img_rows, img_cols, channels_A, channels_B):\n        # Input shape\n        self.img_rows = img_rows\n        self.img_cols = img_cols\n        self.channels_A = channels_A\n        self.channels_B = channels_B\n        self.img_shape_A = (self.img_rows, self.img_cols, self.channels_A)\n        self.img_shape_B = (self.img_rows, self.img_cols, self.channels_B)\n        # Calculate output shape of D (PatchGAN)\n        patch_r = int(self.img_rows \/ 2**4)\n        patch_c = int(self.img_cols \/ 2**4)\n        self.disc_patch = (patch_r, patch_c, 1)\n\n        # Number of filters in the first layer of G and D\n        self.gf = 32\n        self.df = 64\n\n        # Loss weights\n        self.lambda_cycle = 10.0                    # Cycle-consistency loss\n        self.lambda_id = 0.1 * self.lambda_cycle    # Identity loss\n\n        optimizer = Adam(0.0002, 0.5)\n\n        # Build and compile the discriminators\n        self.d_A = self.build_discriminator(self.img_shape_A)\n        self.d_B = self.build_discriminator(self.img_shape_B)\n        self.d_A.compile(loss='mse',\n            optimizer=optimizer,\n            metrics=['accuracy'])\n        self.d_B.compile(loss='mse',\n            optimizer=optimizer,\n            metrics=['accuracy'])\n\n        #-------------------------\n        # Construct Computational\n        #   Graph of Generators\n        #-------------------------\n\n        # Build the generators\n        self.g_AB = self.build_generator(self.img_shape_A, self.img_shape_B)\n        self.g_BA = self.build_generator(self.img_shape_B, self.img_shape_A)\n\n        # Input images from both domains\n        img_A = Input(shape=self.img_shape_A, name='ImageA')\n        img_B = Input(shape=self.img_shape_B, name='ImageB')\n\n        # Translate images to the other domain\n        fake_B = self.g_AB(img_A)\n        fake_A = self.g_BA(img_B)\n        # Translate images back to original domain\n        reconstr_A = self.g_BA(fake_B)\n        reconstr_B = self.g_AB(fake_A)\n        # Identity mapping of images\n        img_A_id = self.g_BA(img_B)\n        img_B_id = self.g_AB(img_A)\n\n        # For the combined model we will only train the generators\n        self.d_A.trainable = False\n        self.d_B.trainable = False\n\n        # Discriminators determines validity of translated images\n        valid_A = self.d_A(fake_A)\n        valid_B = self.d_B(fake_B)\n\n        # Combined model trains generators to fool discriminators\n        self.combined = Model(inputs=[img_A, img_B],\n                              outputs=[ valid_A, valid_B,\n                                        reconstr_A, reconstr_B,\n                                        img_A_id, img_B_id ])\n        self.combined.compile(loss=['mse', 'mse',\n                                    'mae', 'mae',\n                                    'mae', 'mae'],\n                            loss_weights=[  1, 1,\n                                            self.lambda_cycle, self.lambda_cycle,\n                                            self.lambda_id, self.lambda_id ],\n                            optimizer=optimizer)\n\n    def build_generator(self, in_img_shape, out_img_shape):\n        \"\"\"U-Net Generator\"\"\"\n        # Image input\n        d0 = Input(shape=in_img_shape)\n\n        # Downsampling\n        d1 = conv2d(d0, self.gf)\n        d2 = conv2d(d1, self.gf*2)\n        d3 = conv2d(d2, self.gf*4)\n        d4 = conv2d(d3, self.gf*8)\n        \n        d4 = make_vae_layer(d4, prefix='{}-{}'.format(in_img_shape[-1], out_img_shape[-1]))\n        \n        # Upsampling\n        u1 = deconv2d(d4, d3, self.gf*4)\n        u2 = deconv2d(u1, d2, self.gf*2)\n        u3 = deconv2d(u2, d1, self.gf)\n\n        u4 = UpSampling2D(size=2)(u3)\n        output_img = Conv2D(out_img_shape[-1], kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n\n        return Model(d0, output_img, name='Gen_{}_{}_{}-{}'.format(*in_img_shape, out_img_shape[-1]))\n\n    def build_discriminator(self, img_shape):\n\n        def d_layer(layer_input, filters, f_size=4, normalization=True):\n            \"\"\"Discriminator layer\"\"\"\n            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n            d = LeakyReLU(alpha=0.2)(d)\n            if normalization:\n                d = InstanceNormalization()(d)\n            return d\n\n        img = Input(shape=img_shape)\n\n        d1 = d_layer(img, self.df, normalization=False)\n        d2 = d_layer(d1, self.df*2)\n        d3 = d_layer(d2, self.df*4)\n        d4 = d_layer(d3, self.df*8)\n\n        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n\n        return Model(img, validity, name='Disc_{}_{}_{}'.format(*img_shape))","b5d8dc79":"cg = CycleGAN(32, 64, 3, 1)","3ba23176":"Image(model_to_dot(cg.combined, show_shapes=True).create_png())","9cb15707":"Image(model_to_dot(cg.g_AB, show_shapes=True).create_png())","23d54f8b":"Image(model_to_dot(cg.d_A, show_shapes=True).create_png())","8f72ab67":"import h5py\nfrom skimage.util import montage as montage2d\nnorm_stack = lambda x: np.clip((x-127.0)\/127.0, -1, 1)\ndef norm_stack(x):\n    # calculate statistics on first 20 points\n    mean = np.mean(x[:20])\n    std = np.std(x[:20])\n    return (1.0*x-mean)\/(2*std)","47a38d98":"data_dir = os.path.join('..', 'input', 'eye-gaze')\nhelen_eye_dir = '..\/input\/getting-all-the-eye-balls\/'","bedf0eb2":"# load the data file and extract dimensions\nwith h5py.File(os.path.join(data_dir,'gaze.h5'),'r') as t_file:\n    print(list(t_file.keys()))\n    assert 'image' in t_file, \"Images are missing\"\n    assert 'look_vec' in t_file, \"Look vector is missing\"\n    look_vec = t_file['look_vec'][()]\n    assert 'path' in t_file, \"Paths are missing\"\n    print('Images found:',len(t_file['image']))\n    for _, (ikey, ival) in zip(range(1), t_file['image'].items()):\n        print('image',ikey,'shape:',ival.shape)\n        img_width, img_height = ival.shape\n    syn_image_stack = norm_stack(np.expand_dims(np.stack([a for a in t_file['image'].values()],0), -1))\n    print(syn_image_stack.shape, 'loaded')\nplt.matshow(montage2d(syn_image_stack[0:9, :, :, 0]), cmap = 'gray')","d3eebdde":"# load the data file and extract dimensions\nmontage_rgb = lambda x: np.clip(0.5*np.stack([montage2d(x[..., i]) for i in range(x.shape[-1])], -1)+0.5, 0, 1) \nwith h5py.File(os.path.join(helen_eye_dir,'eye_balls_rgb.h5'),'r') as t_file:\n    real_image_stack = norm_stack(t_file['image'][()])\nplt.imshow(montage_rgb(real_image_stack[0:16, :, :]))","1ad59742":"from sklearn.model_selection import train_test_split\nfrom scipy.ndimage import zoom\nclass loader_class():\n    def __init__(self, a_stack, b_stack, goal_size=None):\n        if goal_size is not None:\n            a_stack = zoom(a_stack, (1, goal_size[0]\/a_stack.shape[1], goal_size[1]\/a_stack.shape[2], 1), order=0)\n            b_stack = zoom(b_stack, (1, goal_size[0]\/b_stack.shape[1], goal_size[1]\/b_stack.shape[2], 1), order=0)\n        self.a_stack = train_test_split(a_stack, test_size=0.25, random_state=2019)\n        self.b_stack = train_test_split(b_stack, test_size=0.25, random_state=2019)\n        self.n_batches = 0\n    def load_batch(self, batch_size, repeats=5):\n        train_a = self.a_stack[0]\n        train_b = self.b_stack[0]\n        for _ in range(repeats): # make sure we go through all of both datasets (-ish)\n            a_idx = np.random.permutation(np.arange(train_a.shape[0]))\n            b_idx = np.random.permutation(np.arange(train_b.shape[0]))\n            seq_len = min(a_idx.shape[0], b_idx.shape[0])\/\/batch_size*batch_size\n            for i in range(0, seq_len, batch_size):\n                self.n_batches+=1\n                c_len = min(batch_size, seq_len-i)\n                yield train_a[a_idx[i:i+c_len]], train_b[b_idx[i:i+c_len]]\n    def load_data(self, domain=\"A\", batch_size=1, is_testing=False):\n        if domain==\"A\":\n            train_x, test_x = self.a_stack\n        elif domain==\"B\":\n            train_x, test_x = self.b_stack\n        else:\n            raise ValueError(\"Unknown domain\")\n        if is_testing:\n            train_x = test_x\n        out_idx = np.random.choice(range(train_x.shape[0]), size=batch_size)\n        \n        return train_x[out_idx]\nloader_obj = loader_class(a_stack=real_image_stack, b_stack=syn_image_stack, goal_size=(32, 64))\nloader_obj.load_data(domain=\"A\", batch_size=1, is_testing=True).shape","0bb44ee1":"# sanity check on the tool\nfor _, (a, b) in zip(range(2), loader_obj.load_batch(8)):\n    print(a.shape, b.shape)","219608a9":"def sample_images(cyc_gan, data_loader, epoch, batch_i):\n    plt.close('all')\n    r, c = 2, 3\n    np.random.seed(batch_i)\n    imgs_A = data_loader.load_data(domain=\"A\", batch_size=1, is_testing=True)\n    imgs_B = data_loader.load_data(domain=\"B\", batch_size=1, is_testing=True)\n\n    # Translate images to the other domain\n    fake_B = cyc_gan.g_AB.predict(imgs_A)\n    fake_A = cyc_gan.g_BA.predict(imgs_B)\n    # Translate back to original domain\n    reconstr_A = cyc_gan.g_BA.predict(fake_B)\n    reconstr_B = cyc_gan.g_AB.predict(fake_A)\n\n    gen_imgs = [imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B]\n\n    titles = ['Original', 'Translated', 'Reconstructed']\n    fig, axs = plt.subplots(r, c, figsize=(10, 5))\n    cnt = 0\n    for i in range(r):\n        for j in range(c):\n            c_img = np.clip(0.5 * gen_imgs[cnt][0]+0.5, 0, 1)\n            if c_img.shape[-1]==1:\n                c_img = c_img[:, :, 0]\n            axs[i,j].imshow(c_img, cmap='gray', vmin=0, vmax=1)\n            axs[i, j].set_title('{} {}'.format(titles[j], 'A' if i==0 else 'B'))\n            axs[i,j].axis('off')\n            cnt += 1\n    fig.savefig(\"{:03d}_{:03d}.png\".format(epoch, batch_i))\nsample_images(cg, loader_obj, 0, 0)","1c7c9e5b":"sample_images(cg, loader_obj, 0, 0) # show if there is some variation","30fd73ae":"BATCH_SIZE = 256\nEPOCHS = 30","3aa6ba22":"start_time = datetime.datetime.now()\n\n# Adversarial loss ground truths\nvalid = np.ones((BATCH_SIZE,) + cg.disc_patch)\nfake = np.zeros((BATCH_SIZE,) + cg.disc_patch)\n\nfor epoch in tqdm_notebook(range(EPOCHS), desc='Epochs'):\n    for batch_i, (imgs_A, imgs_B) in tqdm_notebook(enumerate(loader_obj.load_batch(BATCH_SIZE)), desc='Batch'):\n        #  Train Discriminators\n\n        # Translate images to opposite domain\n        fake_B = cg.g_AB.predict(imgs_A)\n        fake_A = cg.g_BA.predict(imgs_B)\n\n        # Train the discriminators (original images = real \/ translated = Fake)\n        dA_loss_real = cg.d_A.train_on_batch(imgs_A, valid)\n        dA_loss_fake = cg.d_A.train_on_batch(fake_A, fake)\n        dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n\n        dB_loss_real = cg.d_B.train_on_batch(imgs_B, valid)\n        dB_loss_fake = cg.d_B.train_on_batch(fake_B, fake)\n        dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n\n        # Total disciminator loss\n        d_loss = 0.5 * np.add(dA_loss, dB_loss)\n        \n        #  Train Generators\n        # Train the generators\n        g_loss = cg.combined.train_on_batch([imgs_A, imgs_B],\n                                                [valid, valid,\n                                                imgs_A, imgs_B,\n                                                imgs_A, imgs_B])\n\n        elapsed_time = datetime.datetime.now() - start_time\n    \n    # Plot the progress at each epoch\n    print (\"[Epoch %d\/%d] [Batch %d\/%d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s \" \\\n                                                            % ( epoch, EPOCHS,\n                                                                batch_i, loader_obj.n_batches,\n                                                                d_loss[0], 100*d_loss[1],\n                                                                g_loss[0],\n                                                                np.mean(g_loss[1:3]),\n                                                                np.mean(g_loss[3:5]),\n                                                                np.mean(g_loss[5:6]),\n                                                                elapsed_time))\n    \n    sample_images(cg, loader_obj, epoch, 0)","8ec04f78":"sample_images(cg, loader_obj, EPOCHS, 1)","25fdc40e":"sample_images(cg, loader_obj, EPOCHS, 2)","e0b93c2c":"sample_images(cg, loader_obj, EPOCHS, 3)","1fd18d62":"sample_images(cg, loader_obj, EPOCHS, 4)","879b96f5":"sample_images(cg, loader_obj, EPOCHS, 5)","91ea793f":"sample_images(cg, loader_obj, EPOCHS, 5)","bd447fb0":"## High-res RGB images\nHere we take images from the [Helen-Eye dataset](https:\/\/www.kaggle.com\/kmader\/helen-eye-dataset) as the B-stack where we want to convert to.","60771efc":"# Train Model","5f09b8b9":"## Low-Res Unity Eyes\nHere we take the low-resolution unity eyes as the $A$ input to the models","82eca086":"# Data Loaders\nHere we setup the data-loaders","1881d77a":"# Build Models","13ca04c4":"## Generators\nWe just show one and the other can be inferred","ab706a07":"## Preview Model Output","dfbf48b5":"# Goal\nHere the goal is to use cyclegan for going back and forth between low-resolution grey-scale eyes (generated in Unity) and high-resolution color eyes from real-images. It will have to learn color-ification as well as change of perspective. The idea is basically to see where these models work well and what problems frequently come up.\n\n## VAE?\nHere we add a variational component to the generators to make them possibly more diverse.","b4cc8bc7":"## Discriminator","ea333895":"# Setup CycleGAN Code\nThe code below has been lightly adapted from the code at https:\/\/github.com\/eriklindernoren\/Keras-GAN\/tree\/master\/cyclegan by @eriklindernoren"}}