{"cell_type":{"ec7b5595":"code","4779a280":"code","db5f2e94":"code","8f600e29":"code","0ae39cb5":"code","37edd0e3":"code","61a9240c":"code","6425efe9":"code","4c0934f4":"code","7f5275a2":"code","bb20ed6f":"code","acb94d28":"code","d5a33546":"code","7a12d344":"code","eb21aee3":"code","1f0ec4ca":"code","ef00c3c3":"code","57850034":"code","d940179f":"code","f35480c0":"code","eadfd24e":"code","68e6db74":"code","9d0f7807":"code","1dd6edee":"code","babb6e9c":"code","0be35057":"code","b25ad4f4":"markdown","67828785":"markdown","e1002d44":"markdown","d7955ae5":"markdown","11574d51":"markdown","ebd6ba3a":"markdown","e5bea9fa":"markdown","d8897a84":"markdown","68d7c26e":"markdown","7af89289":"markdown","6adc2234":"markdown","30678c0a":"markdown","7c2249ab":"markdown","c22b21c4":"markdown","2bf30ff4":"markdown","a5401a69":"markdown","3192b7c7":"markdown","4d6a44aa":"markdown","6f738822":"markdown","ebe1ad3f":"markdown","6043261e":"markdown","be508c7b":"markdown","47efcc6f":"markdown","3cdb7e7d":"markdown","c10757e2":"markdown"},"source":{"ec7b5595":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport tensorflow as tf\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error\n\nimport time\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","4779a280":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","db5f2e94":"df = pd.read_csv('\/kaggle\/input\/intel-stock-prices-historical-data-intc\/INTC.csv')\ndf.head()","8f600e29":"# check missing value\ndf.isnull().sum()","0ae39cb5":"# respon variable ('close' column)\ny = df.iloc[:,4:5].astype(float).values","37edd0e3":"y_max = max(y)[0]\ny_min = min(y)[0]\n\ninterval = y_max-y_min\ninterval","61a9240c":"scaler = MinMaxScaler(feature_range=(0, 1))\nscaled_y = scaler.fit_transform(y)","6425efe9":"# Plot of Original Data\nplt.plot(y)\nplt.show()","4c0934f4":"# Plot of Scaled Data\nplt.plot(scaled_y)\nplt.show()","7f5275a2":"# acf\/pacf plot\nfig,ax = plt.subplots(figsize=(20,15))\nsm.graphics.tsa.plot_acf(scaled_y,lags=100,ax=ax);","bb20ed6f":"# create function of timewindow\ndef timewindow(y, window):\n    obs  = len(y)-window\n    yt   = y[:obs,:]\n    for i in np.arange(1,window+1):\n        yt = np.hstack((yt, y[i:obs+i,:]))\n    return yt","acb94d28":"scaled_data = timewindow(scaled_y,1)","d5a33546":"scaled_data","7a12d344":"# the shape of data\nn_data,n_var = scaled_data.shape\nn_data,n_var","eb21aee3":"n_test = int(0.2*n_data)\nn_test","1f0ec4ca":"X_scaled_train = scaled_data[:-n_test,:-1]\ny_scaled_train = scaled_data[:-n_test,-1]\n\nX_scaled_test = scaled_data[-n_test:,:-1]\ny_scaled_test = scaled_data[-n_test:,-1]","ef00c3c3":"# reshape X_train and X_test to ndim = 3\nX_scaled_train = np.reshape(X_scaled_train, (X_scaled_train.shape[0], 1, X_scaled_train.shape[1]))\nX_scaled_test  = np.reshape(X_scaled_test,  (X_scaled_test.shape[0], 1, X_scaled_test.shape[1]))","57850034":"model = tf.keras.models.Sequential([tf.keras.layers.LSTM(4, input_shape=(1,1)),\n                                    tf.keras.layers.Dense(1),])\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-04)\nmodel.compile(loss=tf.keras.losses.Huber(),optimizer=optimizer,metrics=[\"mae\"])\n\nmodel.summary()","d940179f":"max_mae = 0.005 ; # 0.5% from scaled data\n\nclass StopCond(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('mae')<max_mae):\n            print(\"MAE threshold condition has been satisfied.\")\n            self.model.stop_training = True\n\n\nearly_stopping    = StopCond()","f35480c0":"start_time = time.time()","eadfd24e":"history = model.fit(X_scaled_train,y_scaled_train,epochs=100,batch_size = 1,callbacks = [early_stopping])","68e6db74":"end_time = time.time()\ndurasi = (end_time - start_time)\/60\n\nprint(\"Time elapsed to train model :\",durasi,\"minutes.\")","9d0f7807":"y_scaled_test_predict = model.predict(X_scaled_test)\n\nmae = mean_absolute_error(y_scaled_test_predict,y_scaled_test)\nprint(\"MAE : \" + str(mae) + \" --> \" + str(round(mae,3)*100) + \"% of scaled data.\")","1dd6edee":"plt.plot(y_scaled_test, label = 'Actual Serries')\nplt.plot(y_scaled_test_predict, label = 'Predicted Series')\nplt.title('Plot of Scaled Data Test')\nplt.legend()\nplt.show()","babb6e9c":"y_test = y[-n_test:]\ny_test_predict = scaler.inverse_transform(y_scaled_test_predict)\n\nmae = mean_absolute_error(y_test_predict,y_test)\nprint(\"MAE : \" + str(mae) + \" --> \" + str(round(mae,3)*100\/interval) + \"% of original data.\")","0be35057":"plt.plot(y_test, label = 'Actual Serries')\nplt.plot(y_test_predict, label = 'Predicted Series')\nplt.title('Plot of Original Data Test')\nplt.legend()\nplt.show()","b25ad4f4":"From the training history above, the model has matched the maximum MAE condition which is 0.5% of scaled data.","67828785":"It seems that there are k (which is many) steps of time that have acceptable correlation to current condition ($t^{th}$). However, because this is a daily data, we just choose one periode (e.g.) for time window.","e1002d44":"In this notebook, we will predict the close price this stock.","d7955ae5":"## Training Model","11574d51":"# Import Data","ebd6ba3a":"Therefore, we have 10360 data with 3 variable (2 predictor, and other as target).","e5bea9fa":"## Reshape","d8897a84":"## Interval of Original Data","68d7c26e":"## Create new Dataset based on Determined Time Window","7af89289":"## Callback","6adc2234":"# Data Preprocessing","30678c0a":"## Normalization","7c2249ab":"## Model Evaluation on Validation\/Test of Scaled Data","c22b21c4":"## Data Splitting\nSplit data to 20% data test, and the rest as data train.","2bf30ff4":"## Model Evaluation on Validation\/Test of Original Data","a5401a69":"## Data Visualization","3192b7c7":"# Modelling\n## Model Architecture, Optimizer, and Loss","4d6a44aa":"## Data Preview after Adding Time Window","6f738822":"## Plot of  Validation\/Test of Original Data between Actual vs Predicted","ebe1ad3f":"# Import Necessary Libary\n","6043261e":"From both plots above, it seems that the predicted data (by model) have good approximation to the original and scaled data test.","be508c7b":"By this normalization, thus we can say that this data have interval in [0,1].","47efcc6f":"## Plot of  Validation\/Test of Scaled Data between Actual vs Predicted","3cdb7e7d":"## ACF Plot to Determine The Number of Time Window","c10757e2":"# Model Evaluation on Test Data"}}