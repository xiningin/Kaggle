{"cell_type":{"34a036da":"code","b4521f6e":"code","a657d3e7":"code","6289f672":"code","a94f6d5c":"code","ba9dbfb4":"code","69b72e5b":"code","b49f9565":"code","7775ed91":"code","3ac512c7":"code","ca8d4bd6":"code","8db5ce3f":"code","b445fe7f":"code","3151bf15":"code","76bfc678":"code","bd5ff6c8":"code","9d52085c":"code","189e75e9":"code","4663ccab":"code","beb47254":"markdown","137adb14":"markdown","d3f0db96":"markdown","578a3f55":"markdown","fcbd904d":"markdown","9dda09af":"markdown","7454b73e":"markdown","e86fa282":"markdown","a37d4812":"markdown","3b57eb51":"markdown","cb4a36c3":"markdown","a5471493":"markdown","1bf0a537":"markdown","a495d8b4":"markdown","947ff903":"markdown","18df1911":"markdown","43034b9b":"markdown","5ab2662a":"markdown","a9145b46":"markdown","664b6a13":"markdown","af666f34":"markdown","ab34e235":"markdown","d42e876b":"markdown","1f3af1f4":"markdown","0ea37163":"markdown","c9726c35":"markdown","1ae0aae6":"markdown","618f8d29":"markdown","0980fb4c":"markdown","782b9fd0":"markdown"},"source":{"34a036da":"!pip install -q fastai2","b4521f6e":"!pip install -q iterative-stratification","a657d3e7":"from fastai2.vision.all import *\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","6289f672":"path = Path('..\/input\/jovian-pytorch-z2g\/Human protein atlas')\n\ntrain_df = pd.read_csv(path\/'train.csv')\n\ntrain_df['Image'] = train_df['Image'].apply(str) + \".png\"\n\ntrain_df['Image'] = \"..\/input\/jovian-pytorch-z2g\/Human protein atlas\/train\/\" + train_df['Image']\n\ntrain_df.head()","a94f6d5c":"strat_kfold = MultilabelStratifiedKFold(n_splits=3, random_state=42, shuffle=True)\ntrain_df['fold'] = -1\nfor i, (_, test_index) in enumerate(strat_kfold.split(train_df.Image.values, train_df.iloc[:,1:].values)):\n    train_df.iloc[test_index, -1] = i\ntrain_df.head()","ba9dbfb4":"train_df.fold.value_counts().plot.bar();","69b72e5b":"def get_data(fold=0, size=224,bs=32):\n    return DataBlock(blocks=(ImageBlock,MultiCategoryBlock),\n                       get_x=ColReader(0),\n                       get_y=ColReader(1, label_delim=' '),\n                       splitter=IndexSplitter(train_df[train_df.fold == fold].index),\n                       item_tfms=[FlipItem(p=0.5),Resize(512,method='pad')],\n                   batch_tfms=[*aug_transforms(size=size,do_flip=True, flip_vert=True, max_rotate=180.0, max_lighting=0.6,max_warp=0.1, p_affine=0.75, p_lighting=0.75,xtra_tfms=[RandomErasing(p=0.5,sh=0.1, min_aspect=0.2,max_count=2)]),Normalize],\n                      ).dataloaders(train_df, bs=bs)","b49f9565":"def accuracy_multi(inp, targ, thresh=0.5, sigmoid=True):\n    \"Compute accuracy when `inp` and `targ` are the same size.\"\n    if sigmoid: inp = inp.sigmoid()\n    return ((inp>thresh)==targ.bool()).float().mean()","7775ed91":"def F_score(output, label, threshold=0.2, beta=1):\n    prob = output > threshold\n    label = label > threshold\n\n    TP = (prob & label).sum(1).float()\n    TN = ((~prob) & (~label)).sum(1).float()\n    FP = (prob & (~label)).sum(1).float()\n    FN = ((~prob) & label).sum(1).float()\n\n    precision = torch.mean(TP \/ (TP + FP + 1e-12))\n    recall = torch.mean(TP \/ (TP + FN + 1e-12))\n    F2 = (1 + beta**2) * precision * recall \/ (beta**2 * precision + recall + 1e-12)\n    return F2.mean(0)","3ac512c7":"test_df = pd.read_csv('..\/input\/jovian-pytorch-z2g\/submission.csv')\ntstpng = test_df.copy()\ntstpng['Image'] = tstpng['Image'].apply(str) + \".png\"\ntstpng['Image'] = \"..\/input\/jovian-pytorch-z2g\/Human protein atlas\/test\/\" + tstpng['Image']\ntstpng.head()","ca8d4bd6":"mixup = MixUp(0.3)","8db5ce3f":"import gc","b445fe7f":"all_preds = []\n\nfor i in range(3):\n    dls = get_data(i,256,64)\n    learn = cnn_learner(dls, resnet34, metrics=[partial(accuracy_multi, thresh=0.2),partial(F_score, threshold=0.2)],cbs=mixup).to_fp16()\n    learn.fit_one_cycle(10, cbs=EarlyStoppingCallback(monitor='valid_loss'))\n    learn.dls = get_data(i,512,32)\n    learn.fine_tune(10,cbs=EarlyStoppingCallback(monitor='valid_loss'))\n    tst_dl = learn.dls.test_dl(tstpng)\n    preds, _ = learn.get_preds(dl=tst_dl)\n    all_preds.append(preds)\n    del learn\n    torch.cuda.empty_cache()\n    gc.collect()","3151bf15":"subm = pd.read_csv(\"..\/input\/jovian-pytorch-z2g\/submission.csv\")\npreds = np.mean(np.stack(all_preds), axis=0)","76bfc678":"k = dls.vocab","bd5ff6c8":"preds[0]","9d52085c":"thresh=0.2\nlabelled_preds = [' '.join([k[i] for i,p in enumerate(pred) if p > thresh]) for pred in preds]","189e75e9":"test_df['Label']=labelled_preds","4663ccab":"test_df.to_csv('submission.csv',index=False)","beb47254":"## Training","137adb14":"F_score is way of evaluation for this competition so used this.","d3f0db96":"Cross-validation, how I see it, is the idea of minimizing randomness from one split by makings n folds, each fold containing train and validation splits. You train the model on each fold, so you have n models. Then you take average predictions from all models, which supposedly give us more confidence in results.\nThese we will see in following code. I found iterative-stratification package that provides scikit-learn compatible cross validators with stratification for multilabel data.","578a3f55":"## Install all the necessary libraries\n\nI am using fastai2 so import that. \n","fcbd904d":"I have tried this first time, but this technique didnot improve my result in this problem. It usually improves accuracy after 80 epochs but I have trained for 20 epoches. so there was no difference in accuracy without it. so you can ignore this. \n\nBut to know about how mixup works is good, I will separate blog on this, so follow my twitter for updates.","9dda09af":"I have used technique called mixup, its a data augmentation technique. \n\nIn fastai Mixup is callback, and\nthis Callback is used to apply MixUp data augmentation to your training.\nto know more read [this](http:\/\/dev.fast.ai\/callback.mixup)","7454b73e":"If GPU is out of memory delete learner and empty cuda cache done in last line of code.","e86fa282":"### Cross Validation","a37d4812":"The method I use here is if we have column called fold and with fold number it would be helpfull to split data using that.\n\nfastai has IndexSplitter in datablock api so this would be helpful.\n\n","3b57eb51":"so what is **stratification**??\n\nThe splitting of data into folds may be governed by criteria such as ensuring that each fold has the same proportion of observations with a given categorical value, such as the class outcome value. This is called stratified cross-validation","cb4a36c3":"I found threshold of 0.2 works good for my code.\n\nthen all the labels predicted above 0.2 are labels of that image using vocab. ","a5471493":"## DataBlock \n\nnow that data is in dataframe and also folds are also defined for cross validation, we will build dataloaders, for which we will use datablock.\n\nIf you want to learn how fastai datablock see my blog series [Make code Simple with DataBlock api](https:\/\/kirankamath.netlify.app\/blog\/fastais-datablock-api\/)","1bf0a537":"put them in Labels column","a495d8b4":"append all prediction to list so that we use it later.\n\nI have run the model for less epochs to see code works and show result, or stopped model in between(it took so much time)\n\nThis method gave me F_score of `.77` and accuracy of `>91%` so you can try.\n\nMy Purpose here is to write blog and explain how to approach and how code works.","947ff903":"stack all the prediction stored in list and average the values.","18df1911":"**My opinion**: \n\n---\n\nIn my opinion it's more important to make one right split, especially because CV takes n times more to train. Then why did I do it??\n\nI wanted to explore classification using cross validation using fastai, which I didn't find many resources to learn. So if I write this blog it may help people.\n\nfastai has no cross validation split(may be) in their library to work like other functions they provide. It may be because cross validation takes time, so may be it not that useful.\n\nBut still in this condition I feel its worth exploring using fastai.\n\n\n\n\n\n\n\n","43034b9b":"we will create a function get_data to create dataloader.\n\nget_data uses fold to split data to be used for cross validation using IndexSplitter. \nfor multiLabel problem compared to single only extra thing to be done is to add MultiCategoryBlock in blocks, this is how fastai makes it easy to work.","5ab2662a":"The problem I have considered is Multi Label classification. In addition to having multiple labels in each image, the other challenge in this problem is the existence of rare classes and combinations of different classes. So in this situation normal split or random split doesnt work because you can end up putting rare cases in the validation set and your model will never learn about them. The stratification present in the scikit-learn is also not equipped to deal with multilabel targets. ","a9145b46":"this step is to submit result to kaggle.","664b6a13":"# fastai MultiLabel Classification using Kfold Cross Validation","af666f34":"gc is for garbage collection","ab34e235":"## Gathering test set","d42e876b":"Since this is multi label problem normal accuracy function wont work, so we have accuracy_multi. fastai has this which we can directly use in metrics but I wanted to know how that works so took code of it.","1f3af1f4":"## metrics","0ea37163":"I have used a technique called progressive resizing. \n\nthis is very simple: start training using small images, and end training using large images. Spending most of the epochs training with small images, helps training complete much faster. Completing training using large images makes the final accuracy much higher. this approach is called progressive resizing.\n\nwe should use the `fine_tune` method after we resize our images to get our model to learn to do something a little bit different from what it has learned to do before. ","c9726c35":"I have specifically choosen this problem because we may learn some techniques on the way, which we otherwise would not have thought of.\n\n**There may be better or easy way of doing kfold cross validation but I have done it keeping in mind how to implement using fastai**, so if you know some better way so please mail or tweet the idea, i will try to implement and give you credit.","1ae0aae6":"I have created 3 folds where I simply get the data from a particular fold, create a model, add metrics, I have used resnet34.\nAnd that's the whole training process. I just trained model on each fold and saved predictions for the test set.","618f8d29":"Here dataset is of Zero to GANs - Human Protein Classification inclass jovian.ml hosted competition","0980fb4c":"You should have list of labels which we get using vocab.","782b9fd0":"I have used `cbs=EarlyStoppingCallback(monitor='valid_loss')` so that model doesnot overfit."}}