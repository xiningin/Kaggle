{"cell_type":{"de10072f":"code","cfdceb5a":"code","140fa6c3":"code","933ea072":"code","e5a5dea2":"code","6e28a01e":"code","f36fbdc8":"code","8f052e31":"code","84246289":"code","346ffcaa":"code","21caa569":"code","c51c9d16":"code","e16f6094":"code","aea787b0":"code","5623193c":"code","0e1e662e":"code","ccd9d0d9":"code","34595ace":"code","b355a215":"code","96814a2a":"code","90eaae23":"code","8635c833":"code","83bc7693":"code","6dce5ac4":"code","ef9bb4f8":"code","2b441f92":"code","2b613d4c":"code","72027fba":"code","82fa3b16":"code","b0462f12":"code","67bd201a":"code","ff403b82":"code","a70d1dcb":"code","b17ab1ab":"code","1cc384b0":"code","75158701":"code","6a5971d5":"code","1e958678":"code","098efda4":"code","c0199547":"code","e1390746":"code","bf779916":"code","2537f22c":"code","466968ac":"code","2e405143":"code","f0cb8f26":"code","758175fd":"code","5d450dc0":"code","6a796e20":"code","ce3050e2":"code","0f1f9c25":"code","e88422bd":"code","12b28588":"code","4ea01d54":"code","7d33c72a":"code","10b79a75":"code","c49c5399":"code","6714f5be":"code","6a2cf1d2":"code","26b116b5":"code","83de7d9b":"code","f75b6dd2":"code","9765b413":"code","702ecd5c":"code","6bc63f7e":"code","b5bbd9be":"code","3b4faa93":"code","7d21ad21":"code","08b5ef50":"code","2b860a4e":"code","32db93f7":"markdown","b4199a66":"markdown","6772d620":"markdown","62711a8a":"markdown","c9ea640b":"markdown","322794c3":"markdown","cbd1e832":"markdown","d997e0bb":"markdown","49c989cb":"markdown","06fd9b1f":"markdown","8f9112b0":"markdown","d519be61":"markdown","a2819aa9":"markdown","c97b2700":"markdown","6aee80b3":"markdown","928d17ec":"markdown","c6c3cf7b":"markdown","eeaeab31":"markdown","cb7017c7":"markdown","b4695fc9":"markdown","9a3efa2d":"markdown","8695ee56":"markdown","4e3af4fc":"markdown","461f4946":"markdown","636850fa":"markdown","3ffe4cda":"markdown","f338a03e":"markdown","381a312b":"markdown","d55402a8":"markdown","24d907f2":"markdown","36cc0726":"markdown"},"source":{"de10072f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cfdceb5a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom sklearn import preprocessing \nfrom category_encoders import *\nfrom sklearn.preprocessing import LabelEncoder\n%matplotlib inline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import datasets, linear_model, metrics\nfrom sklearn.metrics import  confusion_matrix\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix","140fa6c3":"df = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ndf","933ea072":"# Exploratory Data Analysis\ndef libraries():\n    global pd,np\n    import pandas as pd\n    import numpy as np\ndef load():\n    global df\n    df=pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\n    \ndef top_rows(value):\n    print('\\033[1m'+ 'displaying the', value, 'rows from top'+'\\033[0m')\n    a=df.head(value)\n    print(a,'\\n')\n    \ndef bottom_rows(value):\n    print('\\033[1m'+'displaying the', value, 'rows from bottom'+'\\033[0m')\n    b=df.tail(value)\n    print(b,'\\n')\n    \ndef rows_columns():\n    print('\\033[1m'+'Shape of the Data set'+'\\033[0m')\n    c=df.shape\n    print(c,'\\n')\n    \ndef col_names():\n    print('\\033[1m'+'Column Names in the Data set'+'\\033[0m')\n    d=df.columns\n    print(d,'\\n')\n    \ndef information():\n    print('\\033[1m'+'Quick Overview of DataSet(info)'+'\\033[0m')\n    e = df.info()\n    print(e,'\\n')\n\ndef sizee():\n    print('\\033[1m'+'No.of Elements in the DataSet'+'\\033[0m')\n    f = df.size\n    print(f,'\\n')\n\ndef ndimension():\n    print('\\033[1m'+'Dimensions in your dataframe'+'\\033[0m')\n    g = df.ndim\n    print(g,'\\n')\n    \ndef stats_summary():\n    print('\\033[1m'+'Staistical Summary of DataSet'+'\\033[0m')\n    h = df.describe()\n    print(h,'\\n')\n    \ndef null_values():\n    print('\\033[1m'+'Number of Missing values in each column'+'\\033[0m')\n    i = df.isnull().sum()\n    print(i,'\\n')\n    \ndef n_unique():\n    print('\\033[1m'+'Number of unique elements'+'\\033[0m')\n    j = df.nunique()\n    print(j,'\\n')\n    \ndef memory_use():\n    print('\\033[1m'+'Memory used by all colomns in bytes'+'\\033[0m')\n    k = df.memory_usage()\n    print(k,'\\n')\n    \ndef is_na(value):\n    print('\\033[1m'+'Dataframe filled with boolean values with true indicating missing values'+'\\033[0m')\n    l = df.isna().head(value)\n    print(l,'\\n')\n    \ndef duplicate():\n    print('\\033[1m'+'Boolean Series denoting duplicate rows'+'\\033[0m')\n    m = df.duplicated().sum()\n    print(m,'\\n')\n    \ndef valuecounts():\n    print('\\033[1m'+'Series containing count of unique values'+'\\033[0m')\n    n = df.value_counts()\n    print(n,'\\n')\n\ndef datatypes():\n    print('\\033[1m'+'Datatype of each column'+'\\033[0m')\n    o = df.dtypes\n    print(o,'\\n')\n    \ndef correlation():\n    print('\\033[1m'+'Correalation between all columns in DataFrame'+'\\033[0m')\n    p = df.corr()\n    print(p,'\\n')\n    \ndef nonnull_count():\n    print('\\033[1m'+'Count of non-null values'+'\\033[0m')\n    q = df.count()\n    print(q,'\\n')\n    \ndef eda():\n    load()\n    value= 5\n    datatypes()\n    top_rows(value)\n    bottom_rows(value)\n    rows_columns()\n    col_names()\n    information()\n    sizee()\n    ndimension()\n    stats_summary()\n    null_values()\n    n_unique()\n    memory_use()\n    is_na(value)\n    nonnull_count()\n    duplicate()\n    valuecounts()\n    correlation()\n    \n    \n    \n        \ndef stats_u(data,col):\n    if data[col].dtype == \"float64\":\n        print(col,\"has Quantitative data\")\n        mean_value=data[col].mean()\n        print('mean of',col,'column',mean_value)\n        max_value = data[col].max()\n        print('Maximum value of',col,'column',max_value)\n        min_value = data[col].min()\n        print('Minimum value of',col,'column',min_value)\n        median_value = data[col].median(skipna = True)\n        print('median of',col,'column',median_value)\n        std_value = data[col].std()\n        print('standard deviation of',col,'column',std_value)\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        print('quartile 1 of',col,'column is',q1)\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        print('quartile 2 of',col,'column is',q2)\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        print('quartile 3 of',col,'column is',q3)\n        q4 = data[col].quantile(1,interpolation='nearest')\n        print('quartile 4 of',col,'column is',q4)\n        IQR = q3 -q1\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        print('Lower Limit Point:',LLP)\n        print('Upper Limit Point:',ULP)\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers\")\n        else:\n            print(\"There are outliers\")\n            print(data[data[col]<LLP][col])\n            print(data[data[col]>ULP][col])\n            \n    elif data[col].dtype == \"int64\":\n        print(col,\"has Quantitative data\")\n        mean_value=data[col].mean()\n        print('mean of',col,'column',mean_value)\n        median_value = data[col].median(skipna = True)\n        print('median of',col,'column',median_value)\n        std_value = data[col].std()\n        print('standard deviation of',col,'column',std_value)\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        print('quartile 1 of',col,'column is',q1)\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        print('quartile 2 of',col,'column is',q2)\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        print('quartile 3 of',col,'column is',q3)\n        q4 = data[col].quantile(1,interpolation='nearest')\n        print('quartile 4 of',col,'column is',q4)\n        IQR = q3 -q1\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        print('Lower Limit Point:',LLP)\n        print('Upper Limit Point:',ULP)\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers\")\n        else:\n            print(\"There are outliers\")\n            print(\"Outliers are:\")\n            print(data[data[col]<LLP][col])\n            print(data[data[col]>ULP][col])\n    else:\n        print(col,'has Qualitative Data')\n        z = df[col].mode()\n        print('mode of',col,'column:\\n',z)\n        print('Count of mode is:\\n',df[col].value_counts())\n        print('Unique strings in',col,'are',data[col].nunique())\n        if(data[col].nunique() == 1):\n            print(col,'has same string')\n        elif(data[col].nunique() == 2):\n            print(col,'has binary strings')\n        else:\n            print(col,'has multi stings')\n\n\nlibraries()\neda()\n\nprint(\"----------------------------------------------------------------------------------------------------------------------\")\nprint('\\033[1m'+'Summary Of DataSet'+'\\033[0m')\nprint('\\033[1m'+'Name of the DataSet:'+'\\033[0m','Heart Failure')\nprint('\\033[1m'+'DataTypes in the DataSet:\\n'+'\\033[0m',df.dtypes)\nprint('\\033[1m'+'Columns in DataSet:'+'\\033[0m',df.columns)\nprint('\\033[1m'+'Shape of DataSet:'+'\\033[0m',df.shape)\nprint('\\033[1m'+'Size of DataSet:'+'\\033[0m',df.size)\nprint('\\033[1m'+'Dimension of DataSet:'+'\\033[0m',df.ndim)\nprint('\\033[1m'+'Total Memory used in DataSet:'+'\\033[0m',df.memory_usage().sum())\nprint('\\033[1m'+'Total Number of missing values in DataSet:'+'\\033[0m',df.isnull().sum().sum())\nprint('\\033[1m'+'Total Number of Unique values in DataSet:'+'\\033[0m',df.nunique().sum())\nprint('\\033[1m'+'Total Number of non null values in DataSet:'+'\\033[0m',df.count().sum())\nprint('\\033[1m'+'Total Number of duplicate rows in DataSet:'+'\\033[0m',df.duplicated().sum())\nprint(\"----------------------------------------------------------------------------------------------------------------------\")\nprint('\\033[1m'+'Summary Of Each Colomn'+'\\033[0m')\nprint(\"\\n\")\ncols=df.columns\ncols\nfor i in cols:\n    print('\\033[1m'+i+'\\033[0m')\n    stats_u(df,i)\n    print(\"\\n\")\n            \n","e5a5dea2":"df.head()","6e28a01e":"df.tail()","f36fbdc8":"df.dtypes","8f052e31":"df.columns","84246289":"df.shape","346ffcaa":"df.size","21caa569":"df.info()","c51c9d16":"df.describe()","e16f6094":"df.skew()","aea787b0":"df.corr()","5623193c":"df.duplicated().sum()","0e1e662e":"df.isnull().sum()","ccd9d0d9":"! pip install Autoviz","34595ace":"! pip install xlrd","b355a215":"from autoviz.AutoViz_Class import AutoViz_Class\nAV = AutoViz_Class()\ndf_av = AV.AutoViz('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","96814a2a":"df['anaemia'].value_counts()","90eaae23":"sns.countplot(x = 'anaemia',data = df)\nplt.show()","8635c833":"df['diabetes'].value_counts()","83bc7693":"sns.countplot(x = 'diabetes',data = df)\nplt.show()","6dce5ac4":"df['high_blood_pressure'].value_counts()","ef9bb4f8":"sns.countplot(x = 'high_blood_pressure',data = df)\nplt.show()","2b441f92":"df['sex'].value_counts()","2b613d4c":"sns.countplot(x = 'sex',data = df)\nplt.show()","72027fba":"df['smoking'].value_counts()","82fa3b16":"sns.countplot(x = 'smoking',data = df)\nplt.show()","b0462f12":"df['DEATH_EVENT'].value_counts()","67bd201a":"sns.countplot(x = 'DEATH_EVENT',data = df)\nplt.show()","ff403b82":"obj = ['anaemia','diabetes','high_blood_pressure','sex','smoking','DEATH_EVENT']\n# list containing all binary categorical columns","a70d1dcb":"for i in obj:\n    data=df.copy()\n    data.groupby(i)['age'].median().plot.bar()\n    plt.xlabel(i)\n    plt.ylabel('age')\n    plt.title(i)\n    plt.show()","b17ab1ab":"for i in obj:\n    data=df.copy()\n    data.groupby(i)['creatinine_phosphokinase'].median().plot.bar()\n    plt.xlabel(i)\n    plt.ylabel('creatinine_phosphokinase')\n    plt.title(i)\n    plt.show()","1cc384b0":"for i in obj:\n    data=df.copy()\n    data.groupby(i)['ejection_fraction'].median().plot.bar()\n    plt.xlabel(i)\n    plt.ylabel('ejection_fraction')\n    plt.title(i)\n    plt.show()","75158701":"for i in obj:\n    data=df.copy()\n    data.groupby(i)['platelets'].median().plot.bar()\n    plt.xlabel(i)\n    plt.ylabel('platelets')\n    plt.title(i)\n    plt.show()","6a5971d5":"for i in obj:\n    data=df.copy()\n    data.groupby(i)['serum_creatinine'].median().plot.bar()\n    plt.xlabel(i)\n    plt.ylabel('serum_creatinine')\n    plt.title(i)\n    plt.show()","1e958678":"num = []\nfor i in df.columns:\n    if i not in obj:\n        num.append(i)\nnum# all numerical columns","098efda4":"for i in range(len(obj)):\n    x='anaemia'\n    for j in range(len(num)):\n        if obj[i] != x:\n            sns.barplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","c0199547":"for i in range(len(obj)):\n    x='diabetes'\n    for j in range(len(num)):\n        if obj[i] != x:\n            sns.barplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","e1390746":"for i in range(len(obj)):\n    x='high_blood_pressure'\n    for j in range(len(num)):\n        if obj[i] != x:\n            sns.barplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","bf779916":"for i in range(len(obj)):\n    x='sex'\n    for j in range(len(num)):\n        if obj[i] != x:\n            sns.barplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","2537f22c":"for i in range(len(obj)):\n    x='smoking'\n    for j in range(len(num)):\n        if obj[i] != x:\n            sns.barplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","466968ac":"for i in range(len(obj)):\n    x='DEATH_EVENT'\n    for j in range(len(num)):\n        if obj[i] != x:\n            sns.barplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","2e405143":"for i in range(len(obj)):\n    x='age'\n    for j in range(len(num)):\n        if num[j] != x:\n            sns.scatterplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","f0cb8f26":"for i in range(len(obj)):\n    x='creatinine_phosphokinase'\n    for j in range(len(num)):\n        if num[j] != x:\n            sns.scatterplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","758175fd":"for i in range(len(obj)):\n    x='platelets'\n    for j in range(len(num)):\n        if num[j] != x:\n            sns.scatterplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","5d450dc0":"for i in range(len(obj)):\n    x='serum_creatinine'\n    for j in range(len(num)):\n        if num[j] != x:\n            sns.scatterplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","6a796e20":"for i in range(len(obj)):\n    x='serum_sodium'\n    for j in range(len(num)):\n        if num[j] != x:\n            sns.scatterplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","ce3050e2":"for i in range(len(obj)):\n    x='time'\n    for j in range(len(num)):\n        if num[j] != x:\n            sns.scatterplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","0f1f9c25":"plt.figure(figsize=(6,8))\nx = df.drop(obj,axis = 1)\nfor i in x.columns:\n    sns.histplot(x[i],kde = True)\n    plt.show()","e88422bd":"x = df.drop(obj,axis = 1)\nfor i in x.columns:\n    sns.boxplot(x = i, data = x,color = 'yellowgreen')   \n    plt.xlabel(i)\n    plt.show()","12b28588":"x = df.drop(obj,axis = 1)\nfor i in x.columns:\n    sns.violinplot(x = i, data = x,color = 'yellowgreen')   \n    plt.xlabel(i)\n    plt.show()","4ea01d54":"sns.pairplot(df)","7d33c72a":"def count_outliers(data,col):\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        q4 = data[col].quantile(1,interpolation='nearest')\n        IQR = q3 -q1\n        global LLP\n        global ULP\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers in\",i)\n        else:\n            print(\"There are outliers in\",i)\n            x = data[data[col]<LLP][col].size\n            y = data[data[col]>ULP][col].size\n            a.append(i)\n            print('Count of outliers are:',x+y)\nglobal a\na = []\nfor i in x.columns:\n    count_outliers(df,i)","10b79a75":"df.isnull().sum()# no null values treatment","c49c5399":"scaler = StandardScaler()\nscaler.fit(df.drop('DEATH_EVENT',axis = 1))","6714f5be":"scaled_features = scaler.transform(df.drop('DEATH_EVENT',axis = 1))\ndf_feat = pd.DataFrame(scaled_features,columns = df.columns[:-1])\ndf_feat.head()","6a2cf1d2":"X = df_feat\ny = df['DEATH_EVENT']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n","26b116b5":"knn = KNeighborsClassifier(n_neighbors = 29)\nknn.fit(X_train,y_train)","83de7d9b":"pred = knn.predict(X_test)\npred","f75b6dd2":"print(confusion_matrix(y_test,pred))","9765b413":"print(classification_report(y_test,pred))","702ecd5c":"error_rate= []\nfor i in range(1,40):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","6bc63f7e":"plt.figure(figsize = (10,6))\nplt.plot(range(1,40),error_rate,color = 'blue',linestyle = '--',marker = 'o',markerfacecolor='red',markersize = 10)\nplt.title('Error Rate vs K')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","b5bbd9be":"print(metrics.accuracy_score(y_test, pred))","3b4faa93":"forest= RandomForestClassifier(n_estimators =40, random_state = 0)\nforest.fit(X_train,y_train)  \ny_pred = forest.predict(X_test)\nforest.score(X_test,y_test)","7d21ad21":"logmodel = LogisticRegression()\nlogmodel.fit(X_train,y_train)","08b5ef50":"pred = logmodel.predict(X_test)\nprint(classification_report(y_test,pred))\nprint(confusion_matrix(y_test,pred))","2b860a4e":"logmodel.score(X_test,y_test)","32db93f7":"## Now we will plot scatter plot with x and y axis as numerical columns and categorical columns as hue","b4199a66":"## platelets vs all categorical columns","6772d620":"# Exploratory Data analysis using my own buitin function","62711a8a":"# Exploratory Data Analysis","c9ea640b":"## creatinine_phosphokinase vs all categorical columns","322794c3":"# Loading dataset","cbd1e832":"## Finding least value of K","d997e0bb":"## Fixing diabetes to x axis and numerical columns in y axis with hue as categorical columns","49c989cb":"## ejection_fraction vs all categorical columns","06fd9b1f":"## predicting death event","8f9112b0":"# Data Preprocessing","d519be61":"## pairplot","a2819aa9":"## serum_creatinine vs all categorical columns","c97b2700":"## Boxplot for outliers","6aee80b3":"## Fixing sex to x axis and numerical columns in y axis with hue as categorical columns","928d17ec":"## Fixing anaemia to x axis and numerical columns in y axis with hue as categorical columns","c6c3cf7b":"## Fixing smoking to x axis and numerical columns in y axis with hue as categorical columns","eeaeab31":"# Importing Libraries","cb7017c7":"# Feature Selection","b4695fc9":"# Prediction using Random Forest ","9a3efa2d":"# Prediction using KNN","8695ee56":"## Fixing DEATH_EVENT to x axis and numerical columns in y axis with hue as categorical columns","4e3af4fc":"# Feature scaling","461f4946":"## Fixing high_blood_pressure to x axis and numerical columns in y axis with hue as categorical columns","636850fa":"## Count of outiers","3ffe4cda":"# Prediction using logistic regression","f338a03e":"# Data Visualisation using Autoviz","381a312b":"## histplots for numerical columns","d55402a8":"# Data Visualisation","24d907f2":"## violin plot","36cc0726":"## age vs all categorical columns"}}