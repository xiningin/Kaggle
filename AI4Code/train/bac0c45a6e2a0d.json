{"cell_type":{"cb1d48e8":"code","5dc32c27":"code","b76577ad":"code","40455f42":"code","fac2ba55":"code","f63975a5":"code","ac34fdf1":"code","9fe5d027":"code","3564b1cc":"code","ef08416a":"code","4dc25a6c":"code","a1739b7f":"code","8db5e2d8":"code","5c225777":"code","4291f4de":"code","fb2bb50f":"code","5d033b8a":"code","99b586df":"code","4856103f":"code","3a0db634":"code","71922e26":"code","b00173ad":"code","329273dd":"code","67d4bd78":"code","bd2c8ebd":"code","ac0c917f":"code","31279f6b":"markdown","e3013cde":"markdown","14ae188d":"markdown","4570de54":"markdown","f1940307":"markdown","a64cdc19":"markdown","2e9b5a69":"markdown"},"source":{"cb1d48e8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5dc32c27":"import string \nstring.punctuation\nimport nltk\nfrom nltk.corpus import stopwords\nimport re\nimport unicodedata","b76577ad":"#Downloading pre-stored stopwords in English language from NLTK Library\n\nstop_words = set(stopwords.words('english'))","40455f42":"data = pd.read_csv(\"\/kaggle\/input\/joe-biden-tweets\/JoeBidenTweets.csv\")\ndata.head()","fac2ba55":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nsentences = data['tweet'].tolist()\njoined_sentences = ''.join(sentences)\n\nplt.figure(figsize = (15,15))\nplt.imshow(WordCloud(colormap='Dark2').generate(joined_sentences) )","f63975a5":"#Dropping irrelevant columns from the data\ndata = data.drop(columns=['id', 'url', 'timestamp', 'replies', 'retweets', 'likes', 'quotes'])","ac34fdf1":"#Function to perform data pre-processing \n\ndef preprocessing(text):\n    lowercase = text.lower()\n    punc_removal = [char for char in lowercase if char not in string.punctuation]\n    punc_removal_joined = ''.join(punc_removal)\n    url_removal = re.sub(r'(https|http)?:\\\/\\\/(\\w|\\.|\\\/|\\?|\\=|\\&|\\%)*\\b', '', punc_removal_joined, flags=re.MULTILINE)\n    emoji_removal = url_removal.encode('ascii', 'ignore').decode('ascii')\n    rt_removal = re.sub(\"RT\", \"\", emoji_removal)\n    email_removal = re.sub(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)', '', rt_removal)\n    numbers_removal = re.sub(r'[0-9]', \"\", email_removal)\n    stopwords_removal = [word for word in numbers_removal.split() if word not in stopwords.words('english')]\n    return stopwords_removal","9fe5d027":"data['processed_tweet'] = data['tweet'].apply(preprocessing).astype(str)","3564b1cc":"#Assigning polarity scores using TextBlob \n\nfrom textblob import TextBlob\ndata['polarity'] = data['processed_tweet'].apply(lambda x: TextBlob(x).sentiment.polarity)","ef08416a":"#Applying conditions to the polarity score and assigning target 0 and 1 (Negative & Positive) respectively\n\nconditionList = [\n                 data['polarity'] > 0,\n                 data['polarity'] <= 0\n                 ]\nchoiceList = ['1', '0']\ndata['target'] = np.select(conditionList, choiceList, default='no_label')","4dc25a6c":"data.head()","a1739b7f":"import tensorflow as tf\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, Dropout, Activation, Bidirectional, SimpleRNN\nfrom sklearn.model_selection import train_test_split","8db5e2d8":"#Tokenizing the words and sentences.. Followed by padding to set a fixed length for all tweets \n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(data['processed_tweet'].values)\n\nword_index = tokenizer.word_index\n\nsentence = tokenizer.texts_to_sequences(data['processed_tweet'].values)\npadding = pad_sequences(sentence, padding='post', maxlen = 22)","5c225777":"#Defining the size of vocabulary \n\nvocab_size = len(word_index) + 1","4291f4de":"#Assigning X, y features and splitting them for training and testing \n\nX = padding\ny = pd.get_dummies(data['target']).values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.22, random_state = 42)","fb2bb50f":"#Initializing GloVe word embedding \n\nembeddings_index = dict()\nf = open('..\/input\/glove-twitter\/glove.twitter.27B.200d.txt')\nfor line in f:\n\tvalues = line.split()\n\tword = values[0]\n\tcoefs = np.asarray(values[1:], dtype='float32')\n\tembeddings_index[word] = coefs\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))","5d033b8a":"embedding_matrix = np.zeros((vocab_size, 200))\nfor word, i in tokenizer.word_index.items():\n\tembedding_vector = embeddings_index.get(word)\n\tif embedding_vector is not None:\n\t\tembedding_matrix[i] = embedding_vector","99b586df":"embed_dim = 200\nlstm_out = 128\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embed_dim,input_length = X.shape[1], weights=[embedding_matrix],trainable=False))\nmodel.add(LSTM(lstm_out, return_sequences=True))\nmodel.add(LSTM(lstm_out))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","4856103f":"model.fit(X_train, y_train, epochs = 10, verbose = 1, validation_data=(X_test, y_test))","3a0db634":"#Train Test split for Logistic Regression\n\nX = padding\ny = data['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.22, random_state = 42)","71922e26":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train, y_train)","b00173ad":"print(\"LR Score of Training Set\" ,lr.score(X_train, y_train))\nprint(\"LR Score of Test Set\" ,lr.score(X_test, y_test))","329273dd":"expected = y_test\npredicted = lr.predict(X_test)","67d4bd78":"from sklearn.metrics import plot_confusion_matrix\nfrom sklearn import metrics\ncm = metrics.confusion_matrix(expected, predicted, labels = ['1','0'])\nprint(cm)","bd2c8ebd":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=[1,0])\ndisp = disp.plot()\n\nplt.show()","ac0c917f":"from sklearn import metrics\nprint(metrics.classification_report(expected, predicted))","31279f6b":"# **Data Pre-Processing**","e3013cde":"# **Logisitic Regression**","14ae188d":"# **GloVe Word Embedding**","4570de54":"# **Tokeinzing**","f1940307":"# **Loading Data**","a64cdc19":"# **Deep Learning \/ Neural Network Model**","2e9b5a69":"# **Visualization**"}}