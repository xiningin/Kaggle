{"cell_type":{"87073af9":"code","a9b414bf":"code","1ddc30f8":"code","bbbb4824":"code","1618235c":"code","598dd2dd":"code","05c0fb0a":"code","501f442f":"code","2f8a3c8e":"code","e3988de3":"code","9b4e74cf":"code","7c53773c":"code","88802d55":"code","71828461":"code","dc8ea417":"code","4784eeee":"code","f8fa71d7":"code","67e6e119":"code","40d4dae8":"code","9d29131e":"code","3f0070f9":"code","49cbda4f":"code","25390f99":"code","b8da30ea":"code","2a8ec5e9":"code","619ae05a":"code","82bff0f7":"code","be8b4aed":"code","8fc16cfd":"code","6adc38b3":"code","7e353a99":"code","0681549f":"code","484f73e1":"code","644bb4d0":"code","c9d23d5b":"markdown","dfc00fbb":"markdown","eea7b0d6":"markdown","1268f887":"markdown","32e7bf9f":"markdown","5db9c032":"markdown","f96ec6a3":"markdown","8bc4d7c2":"markdown"},"source":{"87073af9":"#ignoring warnings\nimport warnings\nwarnings.simplefilter('ignore')\n\n#importing neccesary modules\nimport sys\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-whitegrid')\nimport seaborn as sns\n\nimport sklearn\nfrom sklearn.linear_model import LinearRegression, SGDRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import BaggingRegressor, RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.externals import joblib\nfrom sklearn.model_selection import GridSearchCV, train_test_split\n\nimport xgboost\nfrom xgboost import XGBRegressor, DMatrix","a9b414bf":"#python package version information\nprint('Python version:{}'.format(sys.version))\nprint('Numpy version:{}'.format(np.__version__))\nprint('Pandas version:{}'.format(pd.__version__))\nprint('MatlpotLib version:{}'.format(matplotlib.__version__))\nprint('Seaborn version:{}'.format(sns.__version__))\nprint('Sci-Kit Learn version:{}'.format(sklearn.__version__))\nprint('XGBoost version:{}'.format(xgboost.__version__))","1ddc30f8":"eu = pd.read_csv('..\/input\/eurusd-daily\/eu.csv', index_col=0, parse_dates=True, skipinitialspace=True)\neu.drop('date', axis='columns', inplace=True)\neu.head(2)","bbbb4824":"eu.isna().sum()","1618235c":"plt.figure(figsize=(15,6))\nplt.plot(eu.close)\nplt.title('Euro vs USD')\nplt.legend()\nplt.show()","598dd2dd":"#box plot for open, high, low , close\neu.drop('volume', axis=1).boxplot()","05c0fb0a":"#removing outliers above\nvol_cut_off = eu.volume.std()*3 + eu.volume.mean()\neu.volume[eu.volume > vol_cut_off] = vol_cut_off","501f442f":"#removing ouliers below\nvol_cut_off =  eu.volume.mean() - eu.volume.std()*3\neu.volume[eu.volume < vol_cut_off] = vol_cut_off","2f8a3c8e":"#boxplot for volume\neu[['volume']].boxplot()","e3988de3":"def generate_features(df):\n    \"\"\" Generate features for a stock\/index\/currency\/commodity based on historical price and performance\n    Args:\n        df (dataframe with columns \"open\", \"close\", \"high\", \"low\", \"volume\")\n    Returns:\n        dataframe, data set with new features\n    \"\"\"\n    df_new = pd.DataFrame()\n    \n    # 6 original features\n    df_new['open'] = df['open']\n    df_new['open_1'] = df['open'].shift(1)\n    df_new['close_1'] = df['close'].shift(1)\n    df_new['high_1'] = df['high'].shift(1)\n    df_new['low_1'] = df['low'].shift(1)\n    df_new['volume_1'] = df['volume'].shift(1)\n    \n    # 50 original features\n    # average price\n    df_new['avg_price_5'] = df['close'].rolling(window=5).mean().shift(1)\n    df_new['avg_price_30'] = df['close'].rolling(window=21).mean().shift(1)\n    df_new['avg_price_90'] = df['close'].rolling(window=63).mean().shift(1)\n    df_new['avg_price_365'] = df['close'].rolling(window=252).mean().shift(1)\n    \n    # average price ratio\n    df_new['ratio_avg_price_5_30'] = df_new['avg_price_5'] \/ df_new['avg_price_30']\n    df_new['ratio_avg_price_905_'] = df_new['avg_price_5'] \/ df_new['avg_price_90']\n    df_new['ratio_avg_price_5_365'] = df_new['avg_price_5'] \/ df_new['avg_price_365']\n    df_new['ratio_avg_price_30_90'] = df_new['avg_price_30'] \/ df_new['avg_price_90']\n    df_new['ratio_avg_price_30_365'] = df_new['avg_price_30'] \/ df_new['avg_price_365']\n    df_new['ratio_avg_price_90_365'] = df_new['avg_price_90'] \/ df_new['avg_price_365']                                            \n    \n    \n    # average volume\n    df_new['avg_volume_5'] = df['volume'].rolling(window=5).mean().shift(1)\n    df_new['avg_volume_30'] = df['volume'].rolling(window=21).mean().shift(1)\n    df_new['avg_volume_90'] = df['volume'].rolling(window=63).mean().shift(1)\n    df_new['avg_volume_365'] = df['volume'].rolling(window=252).mean().shift(1)\n    \n    #average volume ratio\n    df_new['ratio_avg_volume_5_30'] = df_new['avg_volume_5'] \/ df_new['avg_volume_30']\n    df_new['ratio_avg_volumee_5_90'] = df_new['avg_volume_5'] \/ df_new['avg_volume_90']                                                   \n    df_new['ratio_avg_volume_5_365'] = df_new['avg_volume_5'] \/ df_new['avg_volume_365']\n    df_new['ratio_avg_volume_30_90'] = df_new['avg_volume_30'] \/ df_new['avg_volume_90']\n    df_new['ratio_avg_volume_30_365'] = df_new['avg_volume_30'] \/ df_new['avg_volume_365']\n    df_new['ratio_avg_volume_90_365'] = df_new['avg_volume_90'] \/ df_new['avg_volume_365']                                                 \n    \n    \n    # standard deviation of prices\n    df_new['std_price_5'] = df['close'].rolling(window=5).std().shift(1)\n    df_new['std_price_30'] = df['close'].rolling(window=21).std().shift(1)\n    df_new['std_price_90'] = df['close'].rolling(window=63).std().shift(1)                                               \n    df_new['std_price_365'] = df['close'].rolling(window=252).std().shift(1)\n    \n    # standard deviation ratio of prices \n    df_new['ratio_std_price_5_30'] = df_new['std_price_5'] \/ df_new['std_price_30']\n    df_new['ratio_std_price_5_90'] = df_new['std_price_5'] \/ df_new['std_price_90']\n    df_new['ratio_std_price_5_365'] = df_new['std_price_5'] \/ df_new['std_price_365']\n    df_new['ratio_std_price_30_90'] = df_new['std_price_30'] \/ df_new['std_price_90'] \n    df_new['ratio_std_price_30_365'] = df_new['std_price_30'] \/ df_new['std_price_365']                                               \n    df_new['ratio_std_price_90_365'] = df_new['std_price_90'] \/ df_new['std_price_365']                                                \n    \n    \n    # standard deviation of volumes\n    df_new['std_volume_5'] = df['volume'].rolling(window=5).std().shift(1)\n    df_new['std_volume_30'] = df['volume'].rolling(window=21).std().shift(1)\n    df_new['std_volume_90'] = df['volume'].rolling(window=63).std().shift(1)\n    df_new['std_volume_365'] = df['volume'].rolling(window=252).std().shift(1)\n    \n    #standard deviation ratio of volumes\n    df_new['ratio_std_volume_5_30'] = df_new['std_volume_5'] \/ df_new['std_volume_30']\n    df_new['ratio_std_volume_5_90'] = df_new['std_volume_5'] \/ df_new['std_volume_90']\n    df_new['ratio_std_volume_5_365'] = df_new['std_volume_5'] \/ df_new['std_volume_365']                                               \n    df_new['ratio_std_volume_30_90'] = df_new['std_volume_30'] \/ df_new['std_volume_90']\n    df_new['ratio_std_volume_30_365'] = df_new['std_volume_30'] \/ df_new['std_volume_365']\n    df_new['ratio_std_volume_90_365'] = df_new['std_volume_90'] \/ df_new['std_volume_365']                                               \n                                                   \n    # return\n    df_new['return_1'] = ((df['close'] - df['close'].shift(1)) \/ df['close'].shift(1)).shift(1)\n    df_new['return_5'] = ((df['close'] - df['close'].shift(5)) \/ df['close'].shift(5)).shift(1)\n    df_new['return_30'] = ((df['close'] - df['close'].shift(21)) \/ df['close'].shift(21)).shift(1)\n    df_new['return_90'] = ((df['close'] - df['close'].shift(63)) \/ df['close'].shift(63)).shift(1)                                                \n    df_new['return_365'] = ((df['close'] - df['close'].shift(252)) \/ df['close'].shift(252)).shift(1)\n    \n    #average of return\n    df_new['moving_avg_5'] = df_new['return_1'].rolling(window=5).mean()\n    df_new['moving_avg_30'] = df_new['return_1'].rolling(window=21).mean()\n    df_new['moving_avg_30'] = df_new['return_1'].rolling(window=63).mean()\n    df_new['moving_avg_365'] = df_new['return_1'].rolling(window=252).mean()\n    \n    # the target\n    df_new['close'] = df['close']\n    df_new = df_new.dropna(axis=0)\n    return df_new\n\ndata = generate_features(eu)","9b4e74cf":"data.head()","7c53773c":"data.describe()","88802d55":"data.info()","71828461":"sns.heatmap(data.corr())","dc8ea417":"#import datetime module\nimport datetime\n\n#segregate data for training\nstart_train = datetime.datetime(1999, 1, 1,0,0)\nend_train = datetime.datetime(2017, 12, 31, 0, 0)\ndata_train = data.loc[start_train:end_train]\ndata_train.describe()","4784eeee":"#segregate data for validation\nstart_test = datetime.datetime(2018, 1, 1, 0, 0)\nend_test = datetime.datetime(2019, 6, 7, 0, 0)\ndata_test = data.loc[start_test:end_test]\ndata_test.describe()","f8fa71d7":"X_train = data_train.drop('close', axis='columns')\ny_train = data_train.close\n\nX_test = data_test.drop('close', axis='columns')\ny_test = data_test.close\n\n#checking the shape of the train and test data\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","67e6e119":"#initiating standard scaler\nscaler = StandardScaler()\n\n#fit the scaler in training features\nscaler.fit(X_train)\n\n#Rescale both sets using the trained scaler\nX_scaled_train = scaler.transform(X_train)\nX_scaled_test = scaler.transform(X_test)","40d4dae8":"from sklearn.linear_model import LinearRegression\nlin = LinearRegression()\n\nlin.fit(X_scaled_train, y_train)\npredictions_lin = lin.predict(X_scaled_test)\n\nprint('RMSE: {0:.3f}'.format(mean_squared_error(y_test, predictions_lin)**0.5))\nprint('MAE: {0:.3f}'.format(mean_absolute_error(y_test, predictions_lin)))\nprint('R^2: {0:.3f}'.format(r2_score(y_test, predictions_lin)))","9d29131e":"dates = data_test.index.values\nplt.figure(figsize = (18,9))\nplt.style.use('seaborn-whitegrid')\nplot_truth, = plt.plot(dates, y_test)\nplot_lin, = plt.plot(dates, predictions_lin)\nplt.legend([plot_truth, plot_lin], ['Truth', 'Linear Regression'])\nplt.title('Gold price : Prediction vs Truth - Linear Regression')\nplt.show()","3f0070f9":"# First experiment with linear regression\n\n# SGD is very sensitive to data with features at different scales. Hence we need to do feature scaling before training.\n#search for the SGD-based linear regression with the optimal set of parameters. \nfrom sklearn.linear_model import SGDRegressor\n\nparam_grid = {\n    'penalty':['l1', 'l2', 'elasticnet'],\n    \"alpha\": [1e-5, 3e-5, 1e-4],\n    \"eta0\": [0.01, 0.03, 0.1],\n}\n\nsgd = SGDRegressor()\ngrid_search = GridSearchCV(sgd, param_grid, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\ngrid_search.fit(X_scaled_train, y_train)\n\nprint(grid_search.best_params_)\n\nsgd_best = grid_search.best_estimator_\n#print(grid_search.best_score_)\n\npredictions_sgd = sgd_best.predict(X_scaled_test)\n\n#evaluating the predictions\nprint('RMSE: {0:.3f}'.format(mean_squared_error(y_test, predictions_sgd)**0.5))\nprint('MAE: {0:.3f}'.format(mean_absolute_error(y_test, predictions_sgd)))\nprint('R^2: {0:.3f}'.format(r2_score(y_test, predictions_sgd)))","49cbda4f":"dates = data_test.index.values\nplt.figure(figsize = (18,9))\nplot_truth, = plt.plot(dates, y_test)\nplot_sgd, = plt.plot(dates, predictions_sgd)\nplt.legend([plot_truth, plot_sgd], ['Truth', 'SGD'])\nplt.title('Gold price : Prediction vs Truth - SGD Regressor')\nplt.style.use('seaborn-whitegrid')\nplt.show()","25390f99":"xgb = XGBRegressor()\n\ndata_dmatrix = DMatrix(data=X_train,\n                           label=y_train)\n\nxgb_param_grid = {'learning_rate': [0.001, 0.01, 0.1, 1],\n                  'n_estimators': [50, 100, 200, 300],\n                  'subsample': [0.3, 0.5, 0.7, 1]}\n\ngrid_search = GridSearchCV(estimator=xgb,    \n                        param_grid=xgb_param_grid,\n                        scoring='neg_mean_squared_error', \n                        cv=4, \n                        verbose=1,\n                       n_jobs=-1)\n\ngrid_search.fit(X_train, y_train) \n\nprint(\"Best parameters found: \",grid_search.best_params_)\n\nxgb_best = grid_search.best_estimator_\n\nxgb_best.fit(X_train,y_train)\npredictions_xgb = xgb_best.predict(X_test)\n\nprint('RMSE: {0:.3f}'.format(mean_squared_error(y_test, predictions_xgb)**0.5))\nprint('MAE: {0:.3f}'.format(mean_absolute_error(y_test, predictions_xgb)))\nprint('R^2: {0:.3f}'.format(r2_score(y_test, predictions_xgb)))","b8da30ea":"dates = data_test.index.values\nplt.figure(figsize = (18,9))\nplot_truth, = plt.plot(dates, y_test)\nplot_xgb, = plt.plot(dates, predictions_xgb)\nplt.legend([plot_truth, plot_xgb], ['Truth', 'xgb'])\nplt.title('Gold price : Prediction vs Truth - XGB Regressor')\nplt.show()","2a8ec5e9":"BaggingRegressor?","619ae05a":"bgr = BaggingRegressor(base_estimator=lin, n_estimators=100, oob_score=True, n_jobs=-1)\n\nbgr.fit(X_scaled_train, y_train)\npredictions_bgr = bgr.predict(X_scaled_test)\n\nprint('OOB: {0:.3f}'.format(bgr.oob_score))\nprint('RMSE: {0:.3f}'.format(mean_squared_error(y_test, predictions_bgr)**0.5))\nprint('MAE: {0:.3f}'.format(mean_absolute_error(y_test, predictions_bgr)))\nprint('R^2: {0:.3f}'.format(r2_score(y_test, predictions_bgr)))","82bff0f7":"dates = data_test.index.values\nplt.figure(figsize = (18,9))\nplot_truth, = plt.plot(dates, y_test)\nplot_bgr, = plt.plot(dates, predictions_bgr)\nplt.legend([plot_truth, plot_bgr], ['Truth', 'bgr'])\nplt.title('Gold price : Prediction vs Truth - BGR')\nplt.show()","be8b4aed":"param_grid = {\n    \"max_depth\": [30, 50],\n    \"min_samples_split\": [5, 10, 20],\n\n}\n\nrf = RandomForestRegressor(n_estimators=100)\ngrid_search = GridSearchCV(rf, param_grid, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\nprint(grid_search.best_params_)\n# print(grid_search.best_score_)\n\nrf_best = grid_search.best_estimator_\npredictions_rf = rf_best.predict(X_test)\n\nprint('RMSE: {0:.3f}'.format(mean_squared_error(y_test, predictions_rf)**0.5))\nprint('MAE: {0:.3f}'.format(mean_absolute_error(y_test, predictions_rf)))\nprint('R^2: {0:.3f}'.format(r2_score(y_test, predictions_rf)))\n","8fc16cfd":"dates = data_test.index.values\nplt.figure(figsize = (18,9))\nplot_truth, = plt.plot(dates, y_test)\nplot_rf, = plt.plot(dates, predictions_rf)\nplt.legend([plot_truth, plot_rf], ['Truth', 'RF'])\nplt.title('Gold price : Prediction vs Truth - Random Forest')\nplt.show()","6adc38b3":"mae_scoring = pd.Series({'LIN':mean_absolute_error(y_test, predictions_lin),\n                'SGD':mean_absolute_error(y_test, predictions_sgd),\n                'XGB':mean_absolute_error(y_test, predictions_xgb),\n                #'VTR':mean_absolute_error(y_test, predictions_vtr),\n                'BGR':mean_absolute_error(y_test, predictions_bgr),\n                'RFR':mean_absolute_error(y_test, predictions_rf)})\n\n#filtering the regressor with the least mean_absolute_error value\nfilter = mae_scoring.min()\nmae_min = mae_scoring[mae_scoring == mae_scoring.min()]\nprint('The model with the least mean_absolute_error:\\n',mae_min)\n\nplt.plot(mae_scoring, 'r')","7e353a99":"#saving the BGR model with sklearn:joblib\njoblib.dump(bgr, 'bgr_eurusd_10062019.pkl')","0681549f":"#loading the saved model\nmodel = joblib.load('bgr_eurusd_10062019.pkl')","484f73e1":"#Visualizing the predictions and truth values\npred = model.predict(X_scaled_test)\nplt.figure(figsize=(15,7))\nplt.plot(y_test, 'r', label='Truth')\nplt.plot(y_test.index, pred, 'b', label='Predicted')\nplt.title(\"Bagging Regressor Model\")\nplt.xlabel('Date')\nplt.ylabel('Price')\nplt.legend()\nplt.show()","644bb4d0":"#plotting regression line\nplt.style.use('seaborn-whitegrid')\nplt.scatter(y_test, pred,color='blue')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', lw=3, label='Regression fit')\nfig = plt.gcf()\nfig.set_size_inches(10,5)\nplt.title(\"Regression Line for EURUSD\")\nplt.legend()\nplt.show()","c9d23d5b":"### Saving, Loading and Predicting with the BGR Model","dfc00fbb":"### Bagging Regressor (BGR)","eea7b0d6":"### SGD REGRESSOR (SGD)","1268f887":"### Random Forest Regressor (RF)","32e7bf9f":"### Extreme Gradient Boosting Regressor (XGB)","5db9c032":"### SCALING THE PREDICTOR DATA","f96ec6a3":"### SEGREGATING TRAIN AND TEST DATA","8bc4d7c2":"### LINEAR REGRESSION (LIN)"}}