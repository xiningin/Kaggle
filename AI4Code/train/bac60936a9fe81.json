{"cell_type":{"7e250fa9":"code","609f2142":"code","63f44f4c":"code","4b4e50fa":"code","7d478fa9":"code","748545a2":"code","4dcfdad2":"code","0652cd3e":"code","0ada1a82":"code","e5e43135":"code","5402bebe":"code","65a1e97b":"code","19d069de":"code","ee99d479":"code","3277ace0":"code","cfa00e1a":"code","0d4144ad":"code","3112928d":"code","de3117cf":"code","cdf35dd3":"code","ca270a9e":"code","de42449f":"code","b946461b":"code","7d7c639e":"code","c5c359e6":"code","d2e1c39f":"code","f4e93bb5":"code","be448696":"code","e3039da0":"code","ccf9c8c3":"code","db9f3f84":"code","636c832c":"code","219a14ff":"code","ebe2ba6e":"code","5d23bd9e":"code","a99d1e28":"code","6de40da8":"code","a34cf50d":"code","2e81fdf7":"code","ba2d8a71":"code","4e647c5b":"code","f5705fc1":"code","905f248f":"code","aeea7669":"code","c840fe6d":"code","5ede11b2":"code","60df83dc":"code","e4964889":"code","b7c0bd52":"code","2e8e3b92":"code","30759078":"code","f39f1e84":"code","fdc515a0":"code","7187c7d9":"code","1e02ea14":"code","141e7da1":"code","a85caa79":"code","9ff3828c":"code","91b7026b":"code","f5cd7eee":"code","cc47b0f6":"code","8fd3f8cc":"code","6fd0bd8c":"code","3be9aa19":"code","855a4238":"markdown","1db434df":"markdown","fbc51798":"markdown","91686da9":"markdown","f9f2c2e6":"markdown","59b30bd7":"markdown","c4bf20b0":"markdown","037968f9":"markdown","ea704051":"markdown","4226b102":"markdown","61b772e1":"markdown","2e07d186":"markdown","9764ad8e":"markdown","37ff14e4":"markdown","42869fe8":"markdown","5ee743f7":"markdown","22d52b76":"markdown","e8a89898":"markdown","c8bc0ffc":"markdown","544a78d6":"markdown","6bb3d44d":"markdown","917a2d57":"markdown","d5af3b37":"markdown","bccd6635":"markdown","3246b332":"markdown"},"source":{"7e250fa9":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"seaborn-whitegrid\")\nimport warnings\nwarnings.filterwarnings(\"ignore\")","609f2142":"data = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndata.head(10)","63f44f4c":"data","4b4e50fa":"data.columns.values.reshape(-1,1)","7d478fa9":"data.describe().T","748545a2":"data.info()","4dcfdad2":"def plot_hist(variable):\n    plt.figure(figsize=(9,3))\n    plt.hist(data[variable],bins=50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.show()\n    \n    \ncolumns = ['Pregnancies','Glucose','BloodPressure','SkinThickness',\n       'Insulin','BMI', 'DiabetesPedigreeFunction','Age','Outcome']\n\nfor col in columns:\n    plot_hist(col)","0652cd3e":"data.isnull().sum()","0ada1a82":"# Outlier Detection \nfor col in columns:\n    sns.boxplot(x = data[col])\n    plt.show()","e5e43135":"data['Glucose'].fillna(data['Glucose'].mean(), inplace = True)\ndata['BloodPressure'].fillna(data['BloodPressure'].mean(), inplace = True)\ndata['SkinThickness'].fillna(data['SkinThickness'].median(), inplace = True)\ndata['Insulin'].fillna(data['Insulin'].median(), inplace = True)\ndata['BMI'].fillna(data['BMI'].median(), inplace = True)","5402bebe":"data.shape","65a1e97b":"import missingno as msno\np=msno.bar(data)","19d069de":"sns.countplot(x=data.Outcome,data=data)\nplt.show()","ee99d479":"g = sns.factorplot(x=\"Outcome\",y=\"BMI\",data=data,kind=\"bar\")\ng.set_ylabels(\"BMI\")\nplt.show()","3277ace0":"g = sns.factorplot(x=\"Outcome\",y=\"Pregnancies\",data=data,kind=\"bar\")\ng.set_ylabels(\"Pregnancies\")\nplt.show()","cfa00e1a":"g = sns.factorplot(x=\"Outcome\",y=\"Glucose\",data=data,kind=\"bar\")\ng.set_ylabels(\"Glucose\")\nplt.show()","0d4144ad":"g = sns.factorplot(x=\"Outcome\",y=\"BloodPressure\",data=data,kind=\"bar\")\ng.set_ylabels(\"BloodPressure\")\nplt.show()","3112928d":"g = sns.factorplot(x=\"Outcome\",y=\"SkinThickness\",data=data,kind=\"bar\")\ng.set_ylabels(\"SkinThickness\")\nplt.show()","de3117cf":"g = sns.factorplot(x=\"Outcome\",y=\"Insulin\",data=data,kind=\"bar\")\ng.set_ylabels(\"Insulin\")\nplt.show()","cdf35dd3":"g = sns.factorplot(x=\"Outcome\",y=\"BMI\",data=data,kind=\"bar\")\ng.set_ylabels(\"BMI\")\nplt.show()","ca270a9e":"g = sns.factorplot(x=\"Outcome\",y=\"DiabetesPedigreeFunction\",data=data,kind=\"bar\")\ng.set_ylabels(\"DiabetesPedigreeFunction\")\nplt.show()","de42449f":"g = sns.factorplot(x=\"Outcome\",y=\"Age\",data=data,kind=\"bar\")\ng.set_ylabels(\"Age\")\nplt.show()","b946461b":"f,ax = plt.subplots(figsize=(10,8))\ncorr = data.corr()\nsns.heatmap(\n    corr,\n    mask= np.zeros_like(corr, dtype=np.bool),\n    cmap= sns.diverging_palette(240,10,as_cmap=True),\n    square= True,\n    ax=ax\n    )\nplt.show()","7d7c639e":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import neighbors\nfrom sklearn import metrics\nfrom sklearn.svm import SVR","c5c359e6":"y = data[\"Outcome\"].values\nx_data = data.drop([\"Outcome\"],axis=1)\nX = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n\nprint(\"X_train: \", len(X_train))\nprint(\"X_test: \", len(X_test))\nprint(\"y_train: \", len(y_train))\nprint(\"y_test: \", len(y_test))","d2e1c39f":"knn_model = KNeighborsClassifier(n_neighbors=3).fit(X_train,y_train)\ny_pred = knn_model.predict(X_test)\ny_pred","f4e93bb5":"print(\"mean of error squares: \", np.sqrt(mean_squared_error(y_test,y_pred)))\nprint(\"Score: \", knn_model.score(X_test,y_test)*100)","be448696":"# find k value\nscore_list = []\n\nfor each in range(1,100):\n    knn2 = KNeighborsClassifier(n_neighbors=each)\n    knn2.fit(X_train,y_train)\n    score_list.append(knn2.score(X_test,y_test))\n    \nplt.plot(range(1,100),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","e3039da0":"knn_model = KNeighborsClassifier(n_neighbors=43).fit(X_train,y_train)\ny_pred = knn_model.predict(X_test)\nprint(\"mean of error squares: \", np.sqrt(mean_squared_error(y_test,y_pred)))\nprint(\"Accuracy: \", knn_model.score(X_test,y_test)*100)","ccf9c8c3":"# Another way\nparam_grid = {'n_neighbors':np.arange(1,50)}\nknn = KNeighborsClassifier()\nknn_cv= GridSearchCV(knn,param_grid,cv=5,n_jobs=-1,verbose=2).fit(X_train,y_train)\ny_pred = knn_cv.predict(X_test)","db9f3f84":"print(\"Accuracy: \", knn_cv.best_score_*100)","636c832c":"zero = data[data[\"Outcome\"] == 0]\none = data[data[\"Outcome\"] == 1]\n# scatter plot\nplt.scatter(zero.BMI,zero.DiabetesPedigreeFunction,color=\"red\",label=\"zero\",alpha= 0.3)\nplt.scatter(one.BMI,one.DiabetesPedigreeFunction,color=\"green\",label=\"one\",alpha= 0.3)\nplt.legend()\nplt.show()","219a14ff":"y = data[\"Outcome\"].values\nx_data = data.drop([\"Outcome\"],axis=1)\nX = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n\nsvr_model = SVR(\"linear\").fit(X_train,y_train)\nsvr_model","ebe2ba6e":"y_pred = svr_model.predict(X_test)\nprint(\"mean of error squares: \", np.sqrt(mean_squared_error(y_test,y_pred)))\nprint(\"Accuracy: \", svr_model.score(X_test,y_test)*100)","5d23bd9e":"# GRID SEARCH METHOD\nsvr_model = SVR('linear')\nsvr_params = {\n    \"C\": [0.1, 0.5, 1, 3]  # penalty coefficient values\n}\nsvr_cv_model = GridSearchCV(svr_model, svr_params, cv=5, verbose=2, n_jobs=-1).fit(X_train,y_train)","a99d1e28":"svr_cv_model.best_params_","6de40da8":"print(\"Accuracy: \",svr_cv_model.best_score_*100)","a34cf50d":"data.head()","2e81fdf7":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","ba2d8a71":"y = data[\"Outcome\"].values\nx_data = data.drop([\"Outcome\"],axis=1)\nX = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n\nmlp_model = MLPClassifier().fit(X_train,y_train)\nmlp_model","4e647c5b":"y_pred = mlp_model.predict(X_test)\nprint(\"mean of error squares: \", np.sqrt(mean_squared_error(y_test,y_pred)))\nprint(\"Accuracy: \", mlp_model.score(X_test,y_test)*100)","f5705fc1":"# another way\nmlp_params = {\n    \"alpha\": [0.1, 0.01, 0.02, 0.001, 0.0001],\n    \"hidden_layer_sizes\": [(10,20),(5,5),(100,100)]\n}\n\nmlp_cv_model = GridSearchCV(mlp_model,mlp_params,cv=10,verbose=2,n_jobs=-1).fit(X_train,y_train) ","905f248f":"print(\"Accuracy: \", mlp_cv_model.best_score_*100)","aeea7669":"y = data[\"Outcome\"].values\nx_data = data.drop([\"Outcome\"],axis=1)\nX = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n\ncard_model = DecisionTreeClassifier(random_state=42)\ncard_model.fit(X_train,y_train)\ny_pred = card_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","c840fe6d":"print(\"mean of error squares: \", np.sqrt(mean_squared_error(y_test,y_pred)))\nprint(\"Accuracy: \",card_model.score(X_test,y_test)*100)","5ede11b2":"# another way\ncard_params = {\n    \"max_depth\": [2,3,4,5,10,20],\n    \"min_samples_split\": [2,10,5,30,50,10]\n}\ncard_model = DecisionTreeClassifier()\ncard_cv_model = GridSearchCV(card_model,card_params,cv=10,n_jobs=-1,verbose=2).fit(X_train,y_train)","60df83dc":"print(\"Accuracy: \",card_cv_model.best_score_*100)","e4964889":"y = data[\"Outcome\"].values\nx_data = data.drop([\"Outcome\"],axis=1)\nX = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n\nrf_model = RandomForestClassifier(random_state=42).fit(X_train,y_train)\ny_pred = rf_model.predict(X_test)\nprint(\"Accuracy: \", rf_model.score(X_test,y_test))","b7c0bd52":"# another way\nrf_params = {\n    \"max_depth\": [5,8,10],\n    \"max_features\": [2,5,10],\n    \"n_estimators\": [200,500,1000,2000],\n    \"min_samples_split\": [2,10,80,100]\n}\n\nrf_cv_model = GridSearchCV(rf_model,rf_params,cv=10,n_jobs=-1,verbose=2).fit(X_train,y_train)","2e8e3b92":"print(\"Accuracy: \", rf_cv_model.best_score_*100)","30759078":"y = data[\"Outcome\"].values\nx_data = data.drop([\"Outcome\"],axis=1)\nX = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n\ngbm_model = GradientBoostingClassifier().fit(X_train,y_train)\ny_pred = gbm_model.predict(X_test)\nprint(\"Accuracy: \", gbm_model.score(X_test,y_test)*100)","f39f1e84":"# another way\ngbm_model = GradientBoostingClassifier().fit(X_train,y_train)\ngbm_params = {\n    \"learning_rate\": [0.001,0.1,0.001],\n    \"max_depth\": [3,5,8],\n    \"n_estimators\": [100,200,500],\n    \"min_samples_split\": [1,0.5,0.8],\n}\n\ngbm_cv_tuned = GridSearchCV(gbm_model,gbm_params,cv=10,n_jobs=-1,verbose=2).fit(X_train,y_train)","fdc515a0":"print(\"Accuracy: \", gbm_cv_tuned.best_score_*100)","7187c7d9":"import xgboost\nfrom xgboost import XGBClassifier\n\ny = data[\"Outcome\"].values\nx_data = data.drop([\"Outcome\"],axis=1)\nX = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n\nxgb = XGBClassifier().fit(X_train,y_train)\nprint(\"Accuracy: \", xgb.score(X_test,y_test)*100)","1e02ea14":"xgb = XGBClassifier()\n\nxgb_params = {\n    \"learning_rate\": [0.01,0.01,0.5],\n    \"max_depth\": [2,3,4,5,8],\n    \"n_estimators\": [100,200,500,1000],\n    \"colsample_bytree\": [0.4,0.7,1]\n}\n\nxgb_cv_model = GridSearchCV(xgb,xgb_params,cv=10,n_jobs=-1,verbose=1).fit(X_train,y_train)","141e7da1":"print(\"Accuracy: \", xgb_cv_model.best_score_*100)","a85caa79":"y = data[\"Outcome\"].values\nx_data = data.drop([\"Outcome\"],axis=1)\nX = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n\nfrom lightgbm import LGBMClassifier\n\nlgb_model = LGBMClassifier().fit(X_train,y_train)\nprint(\"Accuracy: \", lgb_model.score(X_test,y_test)*100)","9ff3828c":"lgbm_params = {\n    \"learning_rate\": [0.01,0.1,0.5,1],\n    \"n_estimators\": [20,40,100,200,500,1000],\n    \"max_depth\": [1,2,3,4,5,6,7,8,9,10]\n}\n\nlgbm_cv_model = GridSearchCV(lgb_model,lgbm_params,cv=10,n_jobs=-1,verbose=2).fit(X_train,y_train)","91b7026b":"print(\"Accuracy: \", lgbm_cv_model.best_score_*100)","f5cd7eee":"y = data[\"Outcome\"].values\nx_data = data.drop([\"Outcome\"],axis=1)\nX = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n\nfrom catboost import CatBoostClassifier\ncatb_model = CatBoostClassifier(random_state=42).fit(X_train,y_train)\nprint(\"Accuracy: \", catb_model.score(X_test,y_test)*100)","cc47b0f6":"# another way\ncatb_params = {\n    \"iterations\": [200,500,100],\n    \"learning_rate\": [0.01,0.1],\n    \"depth\": [3,6,8]\n}\ncatb_model = CatBoostClassifier()\ncatb_cv_model = GridSearchCV(catb_model,catb_params,cv=5,n_jobs=-1,verbose=2).fit(X_train,y_train)","8fd3f8cc":"print(\"Accuracy: \", catb_cv_model.best_score_*100)","6fd0bd8c":"def compML(df,alg):\n    y = data[\"Outcome\"].values\n    x_data = data.drop([\"Outcome\"],axis=1)\n    X = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\n    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n\n    model = alg().fit(X_train,y_train)\n    model_name = alg.__name__\n    print(model_name,\": \", model.score(X_test,y_test)*100)","3be9aa19":"models = [\n    LGBMClassifier,\n    XGBClassifier,\n    GradientBoostingClassifier,\n    RandomForestClassifier,\n    DecisionTreeClassifier,\n    MLPClassifier,\n    KNeighborsClassifier,\n    SVR\n]\n\nfor i in models:\n    compML(data,i)","855a4238":"# Automation of Machine Learning Tasks","1db434df":"### Age","fbc51798":"### Insulin","91686da9":"### BloodPressure","f9f2c2e6":"# Data Preparation & Outlier Detection <a id=\"2\"><\/a>","59b30bd7":"There are better results found with GridSearch above. I do not repeat because the procedures are long.","c4bf20b0":"# XGBoost\n* XGBoost is the scalable and scalable version of GBM optimized to increase speed and forecast performance.\n* It is scalable.\n* It is fast.\n* Prediction success is high.\n* It proved its success in many kaggle competitions.","037968f9":"### Glucose","ea704051":"### BMI","4226b102":"# Random Forests\n* The basis is based on gathering and evaluating the estimates produced by more than one decision tree created by the bootstrap method.\n- Lowers the mean square root value of the error squares.\n- Increases the correct classification rate.\n- It reduces variance and is resistant to memorization.\n- Combining the predictions produced by multiple decision trees, the basis of\n- Observations for trees are selected using the random sample selection method of bootstrap, and random subspace method.\n- In each node of the decision tree, the best branching variable is chosen among the less number of variables randomly selected from all variables.\n- 2\/3 of the data set is used in tree building. Outside data is used for performance evaluation of trees and determination of variable significance.\n- Random variables are selected at each joint.","61b772e1":"# Support Vector Classification\n* It is one of the strong and flexible modeling techniques. It can be used for classification and regression. Robust is a regression modeling technique.\n* But it is to determine a line or curve to a margin range so that it can get the maximum point with the smallest error.","2e07d186":"![](:\/\/media3.giphy.com\/media\/MVDPX3gaKFPuo\/giphy.gif?cid=ecf05e47486a445c592a49fb8fca7f525cdd691411c8d66a&rid=giphy.gif)\nYesss. All data is full. We should celebrate this.","9764ad8e":"# K-Nearest Neighbors\n* Estimation is made on the similarities of the observations.\n* In order to estimate the \"Y\", which is the dependent variable value of the observation unit whose independent variable values \u200b\u200bare given, the decoration of the relevant observation units with the other observation units in the table will be calculated.\n<br><br>\n- Determine the number of neighbors (K).\n- Calculate distances from unknown point to all other points.\n- Select the closest k-observation, with distances in order and according to the specified k number.\n- Classification is the most common class, and regression is the average value as the estimated value.","37ff14e4":"### Pregnancies","42869fe8":"# Gradient Boosting Machines (GBM)\n* AdaBoost is a generalized version that can be easily adapted to classification and regression problems.\n* A series of models in the form of a single predictive model is established on the residuals.\n* Finding coefficients or decision rules to minimize the mean of error squares.\n* Gradient Boosting creates a series of models in the form of a single predictive model.\n* A model in the series is created by overwriting the prediction residues \/ errors of the previous model in the series (feet).\n* GBM can use many basic learner types.\n* COST functions and link functions can be modified.\n* Boosting + Gradient Descent\n\n### Introduction to Boosting Methods\n* Weak students are based on the idea of \u200b\u200bcoming together and revealing a strong student.\n* Bad estimation is the big value that results from squaring the difference between the real and the estimated values. Trees that make bad predictions are also weak estimators.\n\n### AdaBoost (Adaptive Boosting)\n* It is the algorithm that brings the idea of \u200b\u200bweak classifiers to come together and form a strong classifier.\n\n![](https:\/\/miro.medium.com\/proxy\/1*m2UHkzWWJ0kfQyL5tBFNsQ.png)\n\n* Classification process has been made in the 1st picture\n* + Symbols in the box represent blue classification, - symbols represent red classification. In this case, some symbols are in the wrong box.\n* In the second picture, a classification was made again considering the situations in the first picture.\n* In the third picture, a new classification was made by taking the second picture into consideration.\n* The 4th image classification process has been provided successfully.","5ee743f7":"# LightGBM\n* Light GBM is another type of GBM developed to increase XGBoost's training time performance.\n* More performance.\n* Leaf-wise growth strategy instead of level-wise growth strategy.\n* Depth-first search (DFS) instead of Breadth-first search (BFS).","22d52b76":"# Univariate Variable Analysis","e8a89898":"### DiabetesPedigreeFunction","c8bc0ffc":"# Artificial Neural Network\n* It is one of the powerful machine learning algorithms that can be used for classification and regression problems that refer to the way the human brain processes information.\n* The aim is to reach the coefficients that can make estimates with the smallest error.\n![](https:\/\/www.researchgate.net\/profile\/Facundo_Bre\/publication\/321259051\/figure\/fig1\/AS:614329250496529@1523478915726\/Artificial-neural-network-architecture-ANN-i-h-1-h-2-h-n-o.png)\n\n\n## Artificial Neural Network - Model & Predict","544a78d6":"# Modelling","6bb3d44d":"# Classification Tree (CART) \n* The aim is to transform the complex structures in the dataset into simple decision structures.\n* Heterogeneous data sets are divided into homogeneous subgroups according to a specified target variable.","917a2d57":"# Veriable Description \n1. Pregnancies: <br>\n   Number of times pregnant\n2. Glucose: <br>\n   Plasma glucose concentration over 2 hours in an oral glucose tolerance test.\n3. BloodPressure: <br>\n   Diastolic blood pressure (mm Hg)\n4. Skin Thickness: <br>\n   Triceps skin fold thickness (mm)\n5. Insulin: <br>\n   2-Hour serum insulin (mu U\/ml)\n6. BMI: <br>\n   This is a numerical value of your weight in relation to your height. A BMI between 18.5 and 25 kg\/m\u00b2 indicates a normal weight. A BMI of less than 18.5 kg\/m\u00b2 is considered underweight. A BMI between 25 kg\/m\u00b2 and 29.9 kg\/m\u00b2 is considered overweight.\n7. DiabetesPedigreeFunction: <br>\n   Diabetes pedigree function (a function which scores likelihood of diabetes based on family history)\n8. Age: <br>\n   Age (years)\n9. Outcome: <br>\n   Class variable (0 if non-diabetic, 1 if diabetic)  ","d5af3b37":"# Load and Check Data\nLet's upload our data set and make small reviews. We will do a detailed review later.","bccd6635":"### SkinThickness","3246b332":"# CatBoost (Category Boosting)\n* Another fast, successful GBM derivative that can automatically combat categorical variables.\n* Categorical variable support\n* Fast and scalable GPU support\n* More successful predictions\n* Fast train and fast prediction\n* Russia's first open source, successful ML study"}}