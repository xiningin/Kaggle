{"cell_type":{"e407bcdf":"code","a58e9fe8":"code","253416a8":"code","00c71a0d":"code","34996c30":"code","239ca974":"code","33341b0b":"code","70e8b5ba":"code","aee93072":"code","a9bcec72":"code","4c95befc":"code","ba6d922c":"code","8da3902c":"code","34dfb9b8":"code","646b658b":"code","6e63c05f":"code","47e5dafa":"code","48ff006f":"code","0c50666e":"code","e9d6d36b":"code","2c41edfe":"code","861dc596":"code","3aec1f15":"code","7e14613a":"code","94bc01b0":"code","e284a48a":"code","f1676ee8":"code","82c89205":"code","f6373669":"code","d320be9b":"markdown","1cba0d71":"markdown","67168435":"markdown"},"source":{"e407bcdf":"# Data Cleaning Tools\nimport pandas as pd\nimport numpy as np\nimport datetime\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a58e9fe8":"# Import data\nNBA_Shot_Logs = pd.read_csv('..\/input\/nba-shot-logs\/shot_logs.csv')\n# Drop the features that has extremely low correlation with the predicting feature\nNBA_Shot_Logs = NBA_Shot_Logs.drop(['player_id','player_name','FGM','W','GAME_ID','MATCHUP','FINAL_MARGIN','CLOSEST_DEFENDER_PLAYER_ID','CLOSEST_DEFENDER','PTS'], axis=1)\n# Display result data\nNBA_Shot_Logs.head()","253416a8":"# Changing the type of GAME_CLOCK so that it become an numerical feature  and can be trained in the future.\nNBA_Shot_Logs['GAME_CLOCK'].str.split(':')\nNBA_Shot_Logs['GAME_CLOCK'] = NBA_Shot_Logs['GAME_CLOCK'].str.split(':').apply(lambda x: int(x[0]) * 60 + int(x[1]))\nNBA_Shot_Logs.rename(columns={'GAME_CLOCK':'GAME_CLOCK_SEC'}, inplace=True)","00c71a0d":"NBA_Shot_Logs","34996c30":"# Check for any missing value\nNBA_Shot_Logs.isnull().any()","239ca974":"# Imputing shot clock is not fair for the data, so we drop all missing rows\nNBA_Shot_Logs = NBA_Shot_Logs.dropna(how = 'any', axis = 0)","33341b0b":"# Binary encoding for LOCATION and SHOT_RESULT\nNBA_Shot_Logs['LOCATION'][NBA_Shot_Logs.LOCATION == 'H'] = 1\nNBA_Shot_Logs['LOCATION'][NBA_Shot_Logs.LOCATION == 'A'] = 0\nNBA_Shot_Logs['LOCATION'] = NBA_Shot_Logs['LOCATION'].astype('int32')\n\nNBA_Shot_Logs['SHOT_RESULT'][NBA_Shot_Logs.SHOT_RESULT == 'made'] = 1\nNBA_Shot_Logs['SHOT_RESULT'][NBA_Shot_Logs.SHOT_RESULT == 'missed'] = 0\nNBA_Shot_Logs['SHOT_RESULT'] = NBA_Shot_Logs['SHOT_RESULT'].astype('int32')","70e8b5ba":"#Checking the correlation between numerical features\nnumerical_col = NBA_Shot_Logs.select_dtypes(include = ['float64', 'int64','int32'])\ncorr = numerical_col.corr()\ncorr","aee93072":"# Collecting all variables except the one we are predicting into one list\nx_variables = NBA_Shot_Logs.columns.values.tolist()\nx_variables","a9bcec72":"#Check if we have all the features needed in the list\nx_variables.pop(9)\nx_variables","4c95befc":"X = NBA_Shot_Logs[x_variables]\nX.head()","ba6d922c":"# Assign SHOT_RESULT as Y which is the target feature that we are trying to predict.\nY = NBA_Shot_Logs['SHOT_RESULT']\nY.head()","8da3902c":"#Import the train_test_split function to split 80% of the data into train set and 20% into test set.\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, \n                                                    test_size = 0.2,\n                                                    random_state = 33)","34dfb9b8":"X_train.dtypes\nX_train.head()","646b658b":"from sklearn.impute import SimpleImputer\n# Replace the missing values in X_train and X_test\nmyimp = SimpleImputer()\n\nimputed_X_train = pd.DataFrame(myimp.fit_transform(X_train))\nimputed_X_test = pd.DataFrame(myimp.transform(X_test))\n\nimputed_X_train.columns = X_train.columns\nimputed_X_test.columns = X_test.columns\n\nX_train = imputed_X_train\nX_test = imputed_X_test","6e63c05f":"#Check the first five rows\nX_train.head()","47e5dafa":"import xgboost as xgb\nfrom sklearn import metrics\n# Changing the format\ndtest = xgb.DMatrix(X_test)\nd_train_xgboost = xgb.DMatrix(X_train,label = Y_train)\n\n# Parameter of boosting\nparameters={'max_depth':10, \n            'objective':'binary:logistic',\n            'eval_metric':'auc',\n            'learning_rate':.05}\n\nplst = list(parameters.items())","48ff006f":"# Training model\nxgb_model = xgb.train(parameters, d_train_xgboost, 50)  # train model","0c50666e":"y_pred_xgb = xgb_model.predict(dtest)\ny_pred_xgb","e9d6d36b":"# We here will assign values of y_pred_xgb to three values(0.5,1,0) because we do not want too many decimals and less distinct values will help us to predict.\nfor i in range(0, X_test.shape[0]): \n    if y_pred_xgb[i]>=.5:       # setting threshold to .5 \n       y_pred_xgb[i]=1 \n    else: \n       y_pred_xgb[i]=0  ","2c41edfe":"# Print out the accuracy score.\nprint (\"Accuracy with XGBoost= \", metrics.accuracy_score(y_pred_xgb, Y_test))","861dc596":"# Plot model's features importance so we can see which features are more important for predicting SHOT_RESULT and which features are less important.\n# We can observe that GMAE_CLOCK_SEC is the most imortant feature for predicting SHOT_RESULT and PTS_TYPE is the least important feature.\nxgb.plot_importance(xgb_model)","3aec1f15":"#lightgbm\nimport lightgbm as lgb\n\nd_train_lgbm = lgb.Dataset(X_train, label=Y_train)\n\n# Here we set a relatively high value of 0.1 to learning_rate but we will reduce it later to verify.\nparams = {    \n          'boosting_type': 'gbdt',\n          'objective': 'binary',\n          'metric': 'auc',\n          'nthread':4,\n          'learning_rate':0.1,\n          'num_leaves':30, \n          'max_depth': 10,   \n          'subsample': 0.65, \n          'colsample_bytree': 0.65, \n    }\n# Here we perform cross-validation with the parameters that we choose so that the length of it will be the most accurate n_estimators' value we can get.\ncv_results = lgb.cv(params, d_train_lgbm, num_boost_round=1000, nfold=5, stratified=False, shuffle=True, metrics='auc',early_stopping_rounds=50,seed=0)\nprint('best n_estimators:', len(cv_results['auc-mean']))\nprint('best cv score:', pd.Series(cv_results['auc-mean']).max())\n","7e14613a":"# Here, we first choose the range of parameter max_depth and num_leaves that we want to test in order to find the most effective value. \n# Then we will use GridSearchCV with the chosen parameters to determine the most effective value of max_depth and num_leaves.\n# We find that the best value for max_depth and num_leaves are 4 and 20, we will need these value for finding other paremeter's best value.\nfrom sklearn.model_selection import GridSearchCV\nparams_test1={'max_depth': range(4,9,1), 'num_leaves':range(5, 100, 5)}\n\ngsearch1 = GridSearchCV(estimator = lgb.LGBMClassifier(boosting_type='gbdt',objective='binary',metrics='auc',learning_rate=0.1, n_estimators=42, max_depth=10, bagging_fraction = 0.8,feature_fraction = 0.8), \n                       param_grid = params_test1, scoring='roc_auc',cv=5,n_jobs=-1)\ngsearch1.fit(X_train,Y_train)\ngsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_","94bc01b0":"# Here, we first choose the range of parameter max_bin and min_data_in_leaf that we want to test in order to find the most effective value. \n# Then we will use GridSearchCV with the chosen parameters to determine the most effective value of max_bin and min_data_in_leaf. Note that we will use the value of max_depth and num_leaves that we achieved from last part.\n# We find that the best value for max_bin and min_data_in_leaf are 215 and 41, we will need these value for finding other paremeter's best value.\nparams_test2={'max_bin': range(5,256,10), 'min_data_in_leaf':range(1,102,10)}\n              \ngsearch2 = GridSearchCV(estimator = lgb.LGBMClassifier(boosting_type='gbdt',objective='binary',metrics='auc',learning_rate=0.1, n_estimators=42, max_depth=4, num_leaves=20,bagging_fraction = 0.8,feature_fraction = 0.8), \n                       param_grid = params_test2, scoring='roc_auc',cv=5,n_jobs=-1)\ngsearch2.fit(X_train,Y_train)\ngsearch2.cv_results_, gsearch2.best_params_, gsearch2.best_score_","e284a48a":"# Here, we first choose the range of values of parameter feature_fraction, bagging_fraction and bagging_freq that we want to test in order to find the most effective value. \n# Then we will use GridSearchCV with the chosen parameters to determine the most effective value of feature_fraction, bagging_fraction and bagging_freq. Note that we will use the value of max_bin and min_data_in_leaf that we achieved from last part.\n# We find that the best value for feature_fraction, bagging_fraction and bagging_freq are 0.8,0.6 and 0, we will need these value for finding other paremeter's best value.\nparams_test3={'feature_fraction': [0.6,0.7,0.8,0.9,1.0],\n              'bagging_fraction': [0.6,0.7,0.8,0.9,1.0],\n              'bagging_freq': range(0,81,10)\n}\n              \ngsearch3 = GridSearchCV(estimator = lgb.LGBMClassifier(boosting_type='gbdt',objective='binary',metrics='auc',learning_rate=0.1, n_estimators=42, max_depth=4, num_leaves=20,max_bin=215,min_data_in_leaf=41), \n                       param_grid = params_test3, scoring='roc_auc',cv=5,n_jobs=-1)\ngsearch3.fit(X_train,Y_train)\ngsearch3.cv_results_, gsearch3.best_params_, gsearch3.best_score_","f1676ee8":"# Here, we first choose the range of values of parameter lambda_l1 and lambda_l2 that we want to test in order to find the most effective value. \n# Then we will use GridSearchCV with the chosen parameters to determine the most effective value of lambda_l1 and lambda_l2. Note that we will use the value of feature_fraction, bagging_fraction and bagging_freq that we achieved from last part.\n# We find that the best value for lambda_l1 and lambda_l2 are 0 and 0, we will need these value for finding other paremeter's best value.\nparams_test4={'lambda_l1': [1e-5,1e-3,1e-1,0.0,0.1,0.3,0.5,0.7,0.9,1.0],\n              'lambda_l2': [1e-5,1e-3,1e-1,0.0,0.1,0.3,0.5,0.7,0.9,1.0]\n}\n              \ngsearch4 = GridSearchCV(estimator = lgb.LGBMClassifier(boosting_type='gbdt',objective='binary',metrics='auc',learning_rate=0.1, n_estimators=42, max_depth=4, num_leaves=20,max_bin=215,min_data_in_leaf=41,bagging_fraction=0.6,bagging_freq= 0, feature_fraction= 0.8), \n                       param_grid = params_test4, scoring='roc_auc',cv=5,n_jobs=-1)\ngsearch4.fit(X_train,Y_train)\ngsearch4.cv_results_, gsearch4.best_params_, gsearch4.best_score_","82c89205":"# Here, we first choose the range of values of parameter min_split_gain that we want to test in order to find the most effective value. \n# Then we will use GridSearchCV with the chosen parameters to determine the most effective value of min_split_gain. Note that we will use the value of lambda_l1 and lambda_l2 that we achieved from last part.\n# We find that the best value for min_split_gain is 0.\nparams_test5={'min_split_gain':[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]}\n              \ngsearch5 = GridSearchCV(estimator = lgb.LGBMClassifier(boosting_type='gbdt',objective='binary',metrics='auc',learning_rate=0.1, n_estimators=42, max_depth=4, num_leaves=20,max_bin=215,min_data_in_leaf=41,bagging_fraction=0.6,bagging_freq= 0, feature_fraction= 0.8,\nlambda_l1=0.0,lambda_l2=0.0), \n                       param_grid = params_test5, scoring='roc_auc',cv=5,n_jobs=-1)\ngsearch5.fit(X_train,Y_train)\ngsearch5.cv_results_, gsearch5.best_params_, gsearch5.best_score_","f6373669":"# We will increase n_estimators in order to let the model more accurate and decrease learning_rate to 0.01, the initial learning_rate of 0.1 is too high and 0.01 is more appropriate.\n# Then, we will do another preditction based on the train set and observe an accuracy score.\nmodel=lgb.LGBMClassifier(boosting_type='gbdt',objective='binary',metrics='auc',learning_rate=0.01, n_estimators=1000, max_depth=4, num_leaves=20,max_bin=215,min_data_in_leaf=41,bagging_fraction=0.6,bagging_freq= 0, feature_fraction= 0.8,\nlambda_l1=0.0,lambda_l2=0.0,min_split_gain=0)\nmodel.fit(X_train,Y_train)\ny_pre=model.predict(X_test)\n\nimport sklearn.metrics as metrics\n\nprint(\"acc:\",metrics.accuracy_score(Y_test,y_pre))\nprint(\"auc:\",metrics.roc_auc_score(Y_test,y_pre))","d320be9b":"# Apply boosting method LightBGM\n* Change the format of X_train and Y_train\n* Assign initial values to the parameters of LightBGM. \n* Apply gridsearch to look for the value of each feature that gives the best score one by one.","1cba0d71":"# Apply boosting method XGBOOST\n* Change the format of train and test datasets\n* Set up the parameters for boosting method\n* Train the model\n* Achieve the accuracy score","67168435":"# Binary encoding for LOCATION and SHOT_RESULT\n* Convert categorical values of LOCATION into numerical values\n* Convert categorical values of SHOT_RESULT into numerical values"}}