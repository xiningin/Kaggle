{"cell_type":{"3c072ef9":"code","fc18ffca":"code","1c278792":"code","2a354ac3":"code","c634ccb9":"code","80400615":"code","78225979":"code","50f87b4a":"code","71b5e3e2":"code","ad8aa4f1":"code","d14aff1c":"code","cd55bad7":"code","f01e4b22":"code","b6a2e291":"code","51ad2ea8":"code","87903184":"code","7a86521a":"code","e11a9eef":"code","f94a3f48":"code","a9f4f35d":"code","b2cf24e7":"code","b6e85dd8":"code","c1167d6c":"code","f0d7f4ec":"code","70ccce9f":"code","a0f6d45e":"code","ff437aa0":"code","6861a0be":"code","15fd2675":"code","1af7293c":"code","53b6e414":"code","071996df":"code","532b2cbc":"markdown","f2a5df93":"markdown","0ddef56c":"markdown","5d7fd976":"markdown","6804b040":"markdown","31ac1691":"markdown","93fc9029":"markdown","235cdbd2":"markdown","9b8fdabc":"markdown","0222af80":"markdown","8e0db00a":"markdown","78c9359a":"markdown","f2f6255f":"markdown","e7feef49":"markdown","64f392c6":"markdown","38b06cbb":"markdown","7e516606":"markdown","b6146e01":"markdown","6ff1106a":"markdown","99ac9a62":"markdown","d8510833":"markdown","73643da0":"markdown","ce1b9b50":"markdown","c474d27a":"markdown","6f47405d":"markdown"},"source":{"3c072ef9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fc18ffca":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier,VotingClassifier\nimport xgboost as xgb\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.feature_selection import RFE\nfrom sklearn.svm import SVC,LinearSVC","1c278792":"train=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain.head()","2a354ac3":"print(\"Train data shape:\",train.shape)\nprint(\"Test data shape\",test.shape)","c634ccb9":"train.info()","80400615":"test_id=test['PassengerId']\ndf=pd.concat([train,test],axis=0)\ndf.head()\nprint(df.info())","78225979":"df=df.drop(['PassengerId','Cabin','Ticket'],axis=1)\ndf['Age'].fillna(df['Age'].median(),inplace=True)\ndf['Fare'].fillna(df['Fare'].median(),inplace=True)\ndf['Embarked'].fillna(df['Embarked'].mode()[0],inplace=True)","50f87b4a":"df['Familysize']=df['SibSp']+df['Parch']\ndf['IsAlone']=1\ndf['IsAlone'].loc[df['Familysize']>=1]=0\n\ndf['FareBin']=pd.cut(df['Fare'],4)\ndf['AgeBin']=pd.cut(df['Age'].astype(int),5)\n\ndf['Title']=df['Name'].str.split(\",\",expand=True)[1].str.split('.',expand=True)[0]\nmin=10\ntitle_names = df['Title'].value_counts() < min\ndf['Title']=df['Title'].apply(lambda x: 'Misc' if title_names[x]==True else x)\n\ndf.head()","71b5e3e2":"label=LabelEncoder()\ndf['Sex_Code']=label.fit_transform(df['Sex'])\ndf['Embarked_Code']=label.fit_transform(df['Embarked'])\ndf['Title_Code']=label.fit_transform(df['Title'])\ndf['FareBin_Code']=label.fit_transform(df['FareBin'])\ndf['Age_Code']=label.fit_transform(df['AgeBin'])\n\ndf.head()","ad8aa4f1":"df=df.drop(['Name'],axis=1)\ndf=pd.get_dummies(df)\ndf.head()","d14aff1c":"ctest=df[df.Survived.isna()]\nctest=ctest.drop(['Survived'],axis=1)\nctrain=df[df.Survived.notna()]\nprint(ctrain.shape)\nprint(ctest.shape)","cd55bad7":"cX=ctrain.drop(['Survived'],axis=1)\ncy=ctrain[['Survived']]","f01e4b22":"cX_train, cX_test, cy_train, cy_test = train_test_split(cX,cy,stratify=cy,test_size=0.2,random_state=1)","b6a2e291":"lcv=LassoCV()\nlcv.fit(cX_train,cy_train)\nlcv_mask=lcv.coef_!=0\nprint(sum(lcv_mask))","51ad2ea8":"rfe_rf=RFE(estimator=RandomForestClassifier(),n_features_to_select=12,verbose=1)\nrfe_rf.fit(cX_train,cy_train)\nrf_mask=rfe_rf.support_","87903184":"rfe_gb=RFE(estimator=GradientBoostingClassifier(),n_features_to_select=12,verbose=1)\nrfe_gb.fit(cX_train,cy_train)\ngb_mask=rfe_gb.support_","7a86521a":"votes=np.sum([lcv_mask,rf_mask,gb_mask],axis=0)\nprint(votes)\nmask=votes>1","e11a9eef":"print(cX_train.shape)\nccX_train=cX_train.loc[:,mask]\nprint(ccX_train.shape)\n\nprint(cX_test.shape)\nccX_test=cX_test.loc[:,mask]\nprint(ccX_test.shape)\n\nprint(ctest.shape)\ncctest=ctest.loc[:,mask]\nprint(cctest.shape)\n","f94a3f48":"ccX_train.head()","a9f4f35d":"lr=LogisticRegression()\nlr.fit(ccX_train,cy_train)\ny_pred=lr.predict(ccX_test)\nprint(accuracy_score(cy_test,y_pred))","b2cf24e7":"steps=[('scaler',StandardScaler()),('lr',LogisticRegression())]\nlr_pipe=Pipeline(steps)\nlr_pipe.fit(ccX_train,cy_train)\ny_pred=lr_pipe.predict(ccX_test)\nprint(accuracy_score(cy_test,y_pred))","b6e85dd8":"knn=KNeighborsClassifier(n_neighbors=9)\nknn.fit(cX_train,cy_train)\ny_pred=knn.predict(cX_test)\nprint(accuracy_score(cy_test,y_pred))","c1167d6c":"param={'knn__n_neighbors':np.arange(1,20)}\nsteps=[('scaler',StandardScaler()),('knn',KNeighborsClassifier())]\nknn_pipe=Pipeline(steps)\ngrid_knn=GridSearchCV(estimator=knn_pipe,param_grid=param,cv=10,n_jobs=-1)\ngrid_knn.fit(ccX_train,cy_train)\ny_pred=grid_knn.predict(ccX_test)\nprint(accuracy_score(cy_test,y_pred))\nprint(grid_knn.best_estimator_)","f0d7f4ec":"param={'max_depth':np.arange(3,8),'min_samples_leaf':[0.04,0.06,0.08],'max_features':[0.2,0.4,0.6,0.8]}\ndt=DecisionTreeClassifier(random_state=12)\ngrid_dt=GridSearchCV(estimator=dt,param_grid=param,cv=10,n_jobs=-1)\ngrid_dt.fit(ccX_train,cy_train)\ny_pred=grid_dt.predict(ccX_test)\nprint(accuracy_score(cy_test,y_pred))\nprint(grid_dt.best_params_)","70ccce9f":"param={'n_estimators':[200],'max_depth':np.arange(3,6),'min_samples_leaf':[0.04,0.06,0.08],'max_features':[0.2,0.4,0.6,0.8]}\nrf=RandomForestClassifier(random_state=12)\ngrid_rf=GridSearchCV(estimator=rf,param_grid=param,cv=10,n_jobs=-1)\ngrid_rf.fit(ccX_train,cy_train)\ny_pred=grid_rf.predict(ccX_test)\nprint(accuracy_score(cy_test,y_pred))\nprint(grid_rf.best_params_)","a0f6d45e":"xg_cl=xgb.XGBClassifier(objective='binary:logistic',n_estimators=4,seed=123)\nxg_cl.fit(ccX_train,cy_train)\ny_pred=xg_cl.predict(ccX_test)\nprint(accuracy_score(cy_test,y_pred))","ff437aa0":"xg=xgb.XGBClassifier(objective='reg:logistic',seed=123)\nparams={'n_estimators':[100,200],'max_depth':np.arange(2,6),'alpha':[0.01,0.1,1,10]}\ngrid_xg=GridSearchCV(estimator=xg,param_grid=params,cv=10,n_jobs=-1)\ngrid_xg.fit(ccX_train,cy_train)\ny_pred=grid_xg.predict(ccX_test)\nprint(accuracy_score(cy_test,y_pred))\nprint(grid_xg.best_params_)","6861a0be":"dt=DecisionTreeClassifier(max_depth=1,random_state=1)\nada=AdaBoostClassifier(base_estimator=dt,n_estimators=300,learning_rate=0.05)\nada.fit(ccX_train,cy_train)\ny_pred=ada.predict(ccX_test)\nprint(accuracy_score(cy_test,y_pred))","15fd2675":"\ngrad=GradientBoostingClassifier(n_estimators=500,learning_rate=0.01)\ngrad.fit(ccX_train,cy_train)\ny_pred=grad.predict(ccX_test)\nprint(accuracy_score(cy_test,y_pred))","1af7293c":"svc=SVC(C=100,random_state=12)\nsvc.fit(ccX_train,cy_train)\ny_pred=svc.predict(ccX_test)\nprint(accuracy_score(cy_test,y_pred))","53b6e414":"lr=LogisticRegression(random_state=12)\nknn=KNeighborsClassifier()\ndt=DecisionTreeClassifier(random_state=12)\nclassifiers=[('Logistic',lr_pipe),\n            ('knn',grid_knn),\n            ('dt',grid_dt),\n            ('gradient',grad),\n            ('RF',grid_rf),\n            ('Ada',ada),\n            ('XGb',xg_cl),\n            ('XgbGrid',grid_xg)]\nvc=VotingClassifier(estimators=classifiers)\nvc.fit(ccX_train,cy_train)\ny_pred=vc.predict(ccX_test)\nprint(accuracy_score(cy_test,y_pred))","071996df":"#test_1=test.drop(['PassengerId'],axis=1)\ntest_1=cctest\ntest_2=pd.DataFrame(test_1,columns=test_1.columns)\nans=vc.predict(test_2)\n\nsub=pd.DataFrame({\n    'PassengerId':test_id.astype(int),\n    'Survived':ans.astype(int)\n})\nprint(sub.head())\nsub.to_csv('submissions.csv',index=False)","532b2cbc":"Logistic Regression","f2a5df93":"Apply mask to train and test data","0ddef56c":"Using GridSearch to select best parameters for RandomForest","5d7fd976":"Feature Selection:\nHere we will use 3 different techniques for feature selection\n* LassoCV\n* RandomForestClassifier\n* GradientBoostingCLassifier","6804b040":"Import all required libraries","31ac1691":"Now separate the data into train and test data depending on Survived column","93fc9029":"Now drop the Name column and use pandas get_dummies function to convert it into dummy variables","235cdbd2":"Using GridSearch to select best parameters for XGBosst Classifier","9b8fdabc":"RFE is RandomForestEstimator which will be usefull to select features from RandomForestand GradientBoost","0222af80":"Split the train data into training and testing data","8e0db00a":"Select the columns with minimum one vote","78c9359a":"Logistic Regression with Standard scaler","f2f6255f":"Add new columns with necessary information","e7feef49":"AdaBoost Classifier","64f392c6":"Use label encoder to transform the category columns","38b06cbb":"Using GridSearch to select best parameters for KNN","7e516606":"Load the data required","b6146e01":"Remove unnecessary columns from the data like Id, Cabin, Ticket","6ff1106a":"Using GridSearch to select best parameters for DecisionTree","99ac9a62":"Combine train and test data for data preprocessing","d8510833":"Support Vector Classifier","73643da0":"XGBoost Classifier","ce1b9b50":"Using Voting Classifier to combine all the above techniques","c474d27a":"KNN Neighbors Classifier","6f47405d":"Gradient Bosst Classifier"}}