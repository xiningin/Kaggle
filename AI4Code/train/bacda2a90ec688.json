{"cell_type":{"ed967ad0":"code","32003201":"code","cb86aeae":"code","77ae0d5a":"code","7dc0f872":"code","497ff814":"code","06abdd45":"code","c6e8c20f":"code","133dbcba":"code","5bc131a0":"code","b40f4234":"code","71a1f9d2":"code","c854af7f":"markdown","61b21544":"markdown","8ae9d10a":"markdown","aba24a3b":"markdown","822848d5":"markdown","a99be403":"markdown","4a4e1f77":"markdown","611109e5":"markdown","a64f1b8f":"markdown"},"source":{"ed967ad0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","32003201":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import DBSCAN\nimport sklearn.utils\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nfrom scipy.spatial.distance import cdist\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\ndf = pd.read_csv('\/kaggle\/input\/classroommarks\/marks.csv', header= None)","cb86aeae":"df.columns = 'to_drop marks'.split()\ndf.drop('to_drop', axis = 1, inplace = True)\nx = df.marks.values\nplt.figure(figsize = (12,6))\nsns.distplot(x, bins = 50)","77ae0d5a":"plt.figure(figsize = (12,6))\nsns.boxplot(x)","7dc0f872":"def conv100(x):\n    scaler = MinMaxScaler\n    x_fitted= scaler.fit_trainsform(x.reshape(-1,1))\n    x_fitted = np.array([i*100 for i in x_fitted])\n    return x_fitted\nscaler = MinMaxScaler()\nx_fitted = scaler.fit_transform(x.reshape(-1,1))\nx_fitted = np.array([i*100 for i in x_fitted])","497ff814":"\ndb = DBSCAN(eps = 4,min_samples = 4)\ndb.fit(x_fitted)\ndf = pd.DataFrame()\ndf['marks'] = [i[0] for i in x_fitted]\ndf['labels'] = [i[0] for i in (db.labels_.reshape(-1,1))]\ndf['ax'] = 1\nplt.figure(figsize = (25,10))\nax = sns.scatterplot(x= df.marks, y = df.ax, hue = df.labels)\nplt.setp(ax.get_legend().get_texts(), fontsize='22') \nplt.setp(ax.get_legend().get_title(), fontsize='22') \n","06abdd45":"df = df[df.labels!= -1]","c6e8c20f":"n_grades =5\n# Input the value of grades here\n\n\nkm = KMeans(\n    n_clusters=n_grades, init='random',\n    n_init=10, max_iter=300, \n    tol=1e-04, random_state=0\n)\ny_km = km.fit_predict(df['marks ax'.split()])\ndf['labels'] = y_km\n\nplt.figure(figsize = (25,10))\nsns.scatterplot(x= df.marks, y = df.ax, hue = df.labels)","133dbcba":"marks_range = df.groupby('labels').min().sort_values('marks').reset_index(drop = True)\nplt.figure(figsize = (12,8))\nsns.barplot(marks_range.index, marks_range.marks)\nprint(\"These are the detected minimum values for each grade\")\nmarks_range.head()","5bc131a0":"\n\ndistortions = []\ninertias = []\nmapping1 = {}\nmapping2 = {}\nK = range(1, 10)\nfor k in K:\n    # Building and fitting the model\n    kmeanModel = KMeans(n_clusters=k).fit(df['marks ax'.split()])\n    kmeanModel.fit(df['marks ax'.split()])\n \n    distortions.append(sum(np.min(cdist(df['marks ax'.split()], kmeanModel.cluster_centers_,\n                                        'euclidean'), axis=1)) \/ df['marks ax'.split()].shape[0])\n    inertias.append(kmeanModel.inertia_)\n \n    mapping1[k] = sum(np.min(cdist(df['marks ax'.split()], kmeanModel.cluster_centers_,\n                                   'euclidean'), axis=1)) \/ df['marks ax'.split()].shape[0]\n    mapping2[k] = kmeanModel.inertia_","b40f4234":"plt.figure(figsize= (10,5))\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('Values of K')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method using Distortion')\nplt.show()","71a1f9d2":"plt.figure(figsize= (10,5))\nplt.plot(K, inertias, 'bx-')\nplt.xlabel('Values of K')\nplt.ylabel('Inertia')\nplt.title('The Elbow Method using Inertia')\nplt.show()","c854af7f":"*We observe outliers at the lower end of the spectrum*","61b21544":"### This shows that a cluster of 3 or 4 better represents our distribution\nFeel free to share your views in the discussions for critical constructive reviews and improvements. You can reach me at https:\/\/www.linkedin.com\/in\/jay-dhanwant <br>\n\nThanks a lot and have a better day than me!","8ae9d10a":"*The data has does not have normal distribution, and is negatively skewed. It already depicts the loop holes in the approaches that models it as normal distributions.*","aba24a3b":"\n# Data Preprocessing","822848d5":"# Generating optimum number of grades\nWhile we modelled the data using user specified grades, the question remains whether we can detect if the distribution is explained better by lower or higher number of grades.  \nWe can use the elbow method on distortion as well as the inertial. Inertial represents the total sum of squared distances to the respective cluster point, while distortion represeents the average the average of the squared distances. We chose the number of clusters from where the loss starts decreasing in a linear fashion.","a99be403":"### The Dataset\nThe dataset consist of the classroom marks of a specific subjects. Our objective is to model the data into sensible cluster for generating grades.","4a4e1f77":"# Removing the outliers\n### Eps and minimum sample is selected by intelligent guess to optimise outlier predictions","611109e5":"# Objective\nThe purpose of this kernel is to propose a machine learing based approach for grading students in any course. <br> \nLet's first look at the current techniques that's being used by universites to grade students. <br> \n### I. Absolute Grading System\nThis is one of the most common form of grading, where the criteria for grading is set before students is yet to start writing the paper. <br> \nPros: This sets a standard for excellence and failing and is good for the courses with predefined difficulty of the examination.\nCons: This reduces the flexibility of the examination grately. Now the examiners are compiled to set predefined level of difficulty, which in it's nature is a subjective concept. A slight difficult question could result in unfair grades of the batch. <br> \nOne example could be:\n1. Assign marks > 90 --> A\n2. Assign the next 70+ --> B\n3. Assign the next 30+ --> C\n<br>\n\n### II. Relative Grading system\nThis is commonly adopted in universities. According this system, the grade vs marks distribution is decided by the the collective performance for a particular batch.<br> One example could be:\n1. Assign top 10% students --> A\n2. Assign the next 20% students --> B\n3. Assign the next 40% students --> C\n4. Assign the remaining 30% --> D\n\n<br> \nThis method brings in compeitition among students. While this is better or not is a topic of another discussion, but this method certainly grades few students low regardless their absolute marks. \nPros: Difficulty of the paper no longer plays a role in the grade assignment.\nCons: Considering that the marks are assinged by merit, this method does not guarantees that the grades would be assigned on the basis of students' merit. <br>\n<br>\n\n### III. Applying ML based clustering for sound grading\nLets consider a distribution {1,1,2,2,3,4,5,5,6,10}. If we assign top 20% of the students a grade of A, this wouldn't be fair to the 8th student, since it's more logical for 6 to fall into a grade with 4s and 5s than with 10. <br> \nNow we can start forming the problem statement: For a student to feel that the grades are fair, other students having the same grade should have their marks as close as possible to the student. Hence the problem becomes a minimization problem of inter-cluster distances.\n<br>\n\n![Image](https:\/\/i.ytimg.com\/vi\/fGkGRoiBtKg\/hqdefault.jpg)\n<br>\n\nImage above is a rough representation of a linear data seperated into different clusters. For each set of marks, we effectively want to minimize this cost function\n<br>\n\n![Image](https:\/\/www.saedsayad.com\/images\/Clustering_kmeans_c.png)<br>\n(img source: https:\/\/www.saedsayad.com\/)\n<br>\nK-Means algorithm exactly matches with this problem statement. Here is how it works:\n1. We select the number of clusters k (number of grades we want to allot)\n2. We group each data-point to the closest chosen centroid\n3. We take centroid of newly formed clusters\n4. We repeat 2 and 3 until the change in cluster points reduces below desired errors\n\n### IV. Removing the outliers using DBSCAN\nDBSCAN or Density-based spatial clustering of applications with noise, is an excellent algorithm when we need to detect the outlier in a spatial data. \nIf we consider BiDirectional space, DBSCAN groups together points based on distances (usually euclidean distances) and a minimum density (number of points in this circle or hypersphere for multi-dimiensional data). \nIt classifies the points in density regions below a specific thresholds as noise. Here is a depiction:\n\n![Image](https:\/\/3.bp.blogspot.com\/-rDYuyg00Z0w\/WXA-OQpkAfI\/AAAAAAAAI_I\/QshfNVNHD_wXJwXEipRIVzDSX5iOEAy2wCEwYBhgL\/s1600\/DBSCAN_Points.PNG)\n<br> \nIf a data point doesn't have minimum number of samples within set threshold, it will be considered as an outlier. These two values are hyperparameters to DBSCAN. A value of minimum sample = 4 and a distance threshold = 4 is found to be working in most cases, however you can tune it as per your need. \n\n**Recommended Number of grades:**\nIt might happen that it will make more sense to assign a lower number of grades to class than maximum. Imagine a distribution of {3,3,4,9,9,10} we can easily segregate the bunch into 4-5 grades, but to a human eye, it makes more sense to assign 2 grades instead of 4. How does our model handles these values? \n<br> \n\nTo tackle this issue, we use elbow method to figure out the sutiable numer of clusters for a distribution. The intution here is that, as we increase the number of clusters, J(cost function) which naturally reduce and become 0 for number of clusters = number of datapoints. We will increase the number of clusters only when it results in significant decline in the J value compared to previous k values. Here, we follow following steps:\n1. Run the algorithms for different values of k\n2. For each k, calculate the total loss\n3. Plot total loss against the number of clusters \n4. The value representing the most significant bent is considered as the best fit for clustering\n<br> \nThe plot would look something like this:\n<br>\n\n![Image](https:\/\/miro.medium.com\/max\/832\/1*8wV1j-klQA1xFvfaNXuVzg.png)<br>\n(img source: kdnuggets)","a64f1b8f":"# Performing K means clustering on the sample"}}