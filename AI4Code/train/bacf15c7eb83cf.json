{"cell_type":{"342d0bf9":"code","ce37cc7f":"code","490c13c2":"code","e04d400f":"code","efc223bf":"code","71eb7c56":"code","a6cf612c":"code","0c45de2d":"code","ec61caed":"code","56e8ec6e":"code","107bd3b4":"code","14240391":"code","3a967eca":"code","e66793d8":"code","4e9698ee":"code","a7e57117":"code","fe303f35":"code","3a922e86":"code","d6bb4252":"code","a1b0a85e":"code","68adccea":"code","4b860ae7":"markdown","2ab66ea3":"markdown","880f8622":"markdown","46d4cb4b":"markdown","275b8bd4":"markdown"},"source":{"342d0bf9":"import pandas as pd\nimport numpy as np\nimport gc\ngc.enable()","ce37cc7f":"# Thanks and credited to https:\/\/www.kaggle.com\/gemartin who created this wonderful mem reducer\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n\n\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n    df = reduce_mem_usage(df)\n    return df","490c13c2":"print('-' * 80)\nprint('train')\ntrain = import_data('..\/input\/train_V2.csv')\n# train = train[train['maxPlace'] > 1]\n","e04d400f":"train['totalDistance'] = train['rideDistance'] + train[\"walkDistance\"] + train[\"swimDistance\"]","efc223bf":"train.head()","71eb7c56":"train.info()","a6cf612c":"train.isnull().sum().sum()","0c45de2d":"# y = np.array(train.groupby(['matchId','groupId'])['winPlacePerc'].agg('mean'), dtype=np.float64)\ny = train['winPlacePerc']\ntrain.drop('winPlacePerc', axis=1, inplace=True)","ec61caed":"\"\"\"\nit is a team game, scores within the same group is same, so let's get the feature of each group\n\"\"\"\ntrain_size = train.groupby(['matchId','groupId']).size().reset_index(name='group_size')\ntrain_mean = train.groupby(['matchId','groupId']).mean().reset_index()\ntrain_max = train.groupby(['matchId','groupId']).max().reset_index()\ntrain_min = train.groupby(['matchId','groupId']).min().reset_index()","56e8ec6e":"\"\"\"\nalthough you are a good game player, \nbut if other players of other groups in the same match is better than you, you will still get little score\nso let's add the feature of each match\n\"\"\"\ntrain_match_mean = train.groupby(['matchId']).mean().reset_index()\n\ntrain = pd.merge(train, train_mean, suffixes=[\"\", \"_mean\"], how='left', on=['matchId', 'groupId'])\ndel train_mean\n\ntrain = pd.merge(train, train_max, suffixes=[\"\", \"_max\"], how='left', on=['matchId', 'groupId'])\ndel train_max\n\ntrain = pd.merge(train, train_min, suffixes=[\"\", \"_min\"], how='left', on=['matchId', 'groupId'])\ndel train_min\n\ntrain = pd.merge(train, train_match_mean, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\ndel train_match_mean\n\ntrain = pd.merge(train, train_size, how='left', on=['matchId', 'groupId'])\ndel train_size\n\ntrain_columns = list(train.columns)","107bd3b4":"\"\"\" remove some columns \"\"\"\ntrain_columns.remove(\"Id\")\ntrain_columns.remove(\"matchId\")\ntrain_columns.remove(\"groupId\")\ntrain_columns.remove(\"Id_mean\")\ntrain_columns.remove(\"Id_max\")\ntrain_columns.remove(\"Id_min\")\ntrain_columns.remove(\"Id_match_mean\")\ntrain_columns.remove(\"matchType\")","14240391":"\"\"\"\nin this game, team skill level is more important than personal skill level \nmaybe you are a good player, but if your teammates is bad, you will still lose\nso let's remove the features of each player, just select the features of group and match\n\"\"\"\ntrain_columns_new = []\nfor name in train_columns:\n    if '_' in name:\n        train_columns_new.append(name)\ntrain_columns = train_columns_new    \nprint(train_columns)","3a967eca":"# train_columns = ['assists', 'boosts', 'damageDealt', 'DBNOs', 'headshotKills', \n#                 'heals', 'killPlace', 'killPoints', 'kills', 'killStreaks', \n#                 'longestKill', 'maxPlace', 'numGroups', 'revives','rideDistance', \n#                 'roadKills', 'swimDistance', 'teamKills', 'vehicleDestroys', 'walkDistance', \n#                 'weaponsAcquired', 'winPoints']\n\nX_train = train[train_columns]","e66793d8":"X_train['winPlacePerc'] = y\nx_train = X_train.sample(frac=0.8)\nX_val = X_train.loc[~X_train.index.isin(x_train.index)]\ny_train = x_train['winPlacePerc']\nx_train.drop('winPlacePerc', axis=1, inplace=True)\ny_val = X_val['winPlacePerc']\nX_val.drop('winPlacePerc', axis=1, inplace=True)\n\ndel X_train, train\ngc.collect()\nprint(x_train.info(memory_usage='deep', verbose=False))\nprint(X_val.info(memory_usage='deep', verbose=False))","4e9698ee":"# from sklearn import preprocessing\n# scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1), copy=False).fit(x_train)\n# # scaler = preprocessing.QuantileTransformer().fit(x_train)\n\n# x_train = scaler.transform(x_train)\n# X_val = scaler.transform(X_val)\n# X_test = scaler.transform(X_test)\n# y_train = y_train*2 - 1\n\n# print(\"x_train\", x_train.shape, x_train.min(), x_train.max())\n# print(\"X_val\", X_val.shape, X_val.min(), X_val.max())\n# print(\"X_test\", X_test.shape, X_test.min(), X_test.max())\n# print(\"y_train\", y_train.shape, y_train.max(), y_train.min())\n\n# X_val = np.clip(X_val, a_min=-1, a_max=1)\n# print(\"X_val\", X_val.shape, X_val.min(), X_val.max())\n# X_test = np.clip(X_test, a_min=-1, a_max=1)\n# print(\"x_test\", X_test.shape, X_test.min(), X_test.max())\n# gc.collect()","a7e57117":"import catboost\nimport time\nfrom catboost import CatBoostRegressor\n\nstart_time = time.time()\nmodel = CatBoostRegressor(iterations=500, learning_rate=0.05, loss_function='MAE',eval_metric='MAE', depth = 15,\n#                           task_type = \"GPU\", \n#                           one_hot_max_size = 64,\n                          use_best_model=True, od_type=\"Iter\", od_wait=20, thread_count=128, random_seed = 123)\nmodel.fit(x_train, y_train, eval_set=(X_val, y_val))\nend_time = time.time()\nprint('The training time = {}'.format(end_time - start_time))","fe303f35":"print('-' * 80)\nprint('test')\ntest = import_data('..\/input\/test_V2.csv')\ntest_new = test[['Id', 'matchId', 'groupId']]\n\ntest['totalDistance'] = test['rideDistance'] + test[\"walkDistance\"] + test[\"swimDistance\"]\n\ntest.info(memory_usage='deep', verbose=False)\n\ntest_size = test.groupby(['matchId','groupId']).size().reset_index(name='group_size')\ntest_mean = test.groupby(['matchId','groupId']).mean().reset_index()\ntest_max = test.groupby(['matchId','groupId']).max().reset_index()\ntest_min = test.groupby(['matchId','groupId']).min().reset_index()\n\ntest_match_mean = test.groupby(['matchId']).mean().reset_index()\ntest = pd.merge(test, test_mean, suffixes=[\"\", \"_mean\"], how='left', on=['matchId', 'groupId'])\ndel test_mean\ntest = pd.merge(test, test_max, suffixes=[\"\", \"_max\"], how='left', on=['matchId', 'groupId'])\ndel test_max\ntest = pd.merge(test, test_min, suffixes=[\"\", \"_min\"], how='left', on=['matchId', 'groupId'])\ndel test_min\ntest = pd.merge(test, test_match_mean, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\ndel test_match_mean\ntest = pd.merge(test, test_size, how='left', on=['matchId', 'groupId'])\ndel test_size","3a922e86":"X_test = test[train_columns]\ndel test\ngc.collect()","d6bb4252":"pred = model.predict(X_test)","a1b0a85e":"test_new['winPlacePercPred'] = pred\n\nprint('Correcting predictions')\n        \ntest_new['prediction_mod'] = -1.0\nmatchId = test_new['matchId'].unique()\n\nfor match in matchId:\n    df_match = test_new[test_new['matchId']==match]\n\n    df_max = df_match.groupby(['groupId']).max()\n    pred_sort = sorted(df_max['winPlacePercPred'])\n\n    for i in df_max.index:\n        groupPlace = pred_sort.index(df_max.loc[i]['winPlacePercPred'])\n        if len(pred_sort) > 1:\n            df_max.at[i,'prediction_mod'] = groupPlace\/(len(pred_sort)-1)\n        else:\n            df_max.at[i,'prediction_mod'] = 1.0\n\n    for i in df_match.index:\n        test_new.at[i, 'prediction_mod'] = df_max['prediction_mod'].loc[df_match['groupId'].loc[i]]\n\ny_submit_cor = test_new['prediction_mod']\nprint('Submission scores corrected')\n\ntest_new.head()","68adccea":"df_test = import_data('..\/input\/sample_submission_V2.csv')\ndf_test['winPlacePerc'] = y_submit_cor\ndf_test['Id'] = df_test['Id'].astype(str)\nsubmission = df_test[['Id', 'winPlacePerc']]\n\nsubmission.to_csv('submission.csv', index=False) \nprint('Submission file made\\n')","4b860ae7":"**3. Prepare Data**","2ab66ea3":"**1. Data Structure**","880f8622":"**2. MissingValue ?**","46d4cb4b":"**4. Training data**","275b8bd4":"**5. Submission**"}}