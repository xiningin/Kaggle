{"cell_type":{"5815d1e1":"code","91b7595f":"code","87a3be00":"code","ae1224f3":"code","45b8f635":"code","b07dd435":"code","37bd32b4":"code","7082188d":"code","7f44d3eb":"code","5cbc5e74":"code","2f6ac292":"code","1e4a93b8":"code","da7a1ec3":"code","db46a73f":"code","411f12de":"code","a7dce3b3":"code","f48a8913":"code","cc107c1d":"code","5d789175":"code","fe09894e":"code","346dd2fd":"code","84774c07":"code","1116b813":"code","a17b2472":"code","e38356ae":"code","ce0ecd49":"code","ddfbb9eb":"code","808e9524":"code","69507ded":"code","88a4873e":"code","63c723b9":"code","bbcf31c4":"code","b3ffc2ef":"markdown","e3e02a82":"markdown","1423f484":"markdown","c4262694":"markdown","701cc61c":"markdown","82d99aa9":"markdown","4f123a15":"markdown","5d346424":"markdown","35d3642f":"markdown","9a761e63":"markdown","1cd706b3":"markdown","f418f195":"markdown","8d2289e9":"markdown","bf5c6cdf":"markdown","d861b667":"markdown","cc343812":"markdown","2ee7e594":"markdown","9b6ed306":"markdown","4167029f":"markdown","35959b5b":"markdown","11439038":"markdown","8932d71c":"markdown","66af6d00":"markdown","06c37867":"markdown","e1e254e8":"markdown","d5257b94":"markdown"},"source":{"5815d1e1":"from keras.layers import Conv2D, UpSampling2D, Dropout\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\nfrom skimage.io import imshow\nfrom skimage.color import rgb2lab, lab2rgb, gray2rgb\nfrom skimage.transform import resize\nimport skimage.io\nfrom skimage.io import imsave\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport keras\nimport os\nimport glob\nimport seaborn as sns\nimport pandas as pd\nimport cv2","91b7595f":"#for train image pixel \ncy = []\ncx = []\nfor i in (glob.glob(\"..\/input\/flickr-image-dataset\/flickr30k_images\/flickr30k_images\/*.jpg\")):\n    img = plt.imread(i)\n    a = np.shape(img)\n    c = np.reshape(img,(a[0]*a[1],a[2]))\n    cy.append(np.shape(c)[0])\n    cx.append(i)\ncolumns = ['Images','pixels']\ndt = np.array([cx,cy])\ndf = pd.DataFrame(dt.T, columns = columns)\ndf['pixels'] = df['pixels'].astype('int')\ndf = df.sort_values('pixels')\ndf.head()","87a3be00":"# sns.set(style=\"darkgrid\")\n# mortality_age_plot = sns.barplot(x=df['Images'],\n#                                  y=df['pixels'],\n#                                  palette = 'muted',\n#                                  order=df['Images'].tolist())\n\n# plt.xticks(rotation=90)\n# plt.show()","ae1224f3":"new_style = {'grid': False}\nplt.rc('axes', **new_style)\n_, ax = plt.subplots(3, 3, sharex='col', sharey='row', figsize=(12, 12))\n\nfor i in range(9):\n    img = cv2.imread(df['Images'][i])\n    ax[i \/\/ 3, i % 3].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    print(cat_train_df['Images'][i])\n  ","45b8f635":"new_style = {'grid': False}\nplt.rc('axes', **new_style)\n_, ax = plt.subplots(3, 3, sharex='col', sharey='row', figsize=(12, 12))\n\nfor i in range(9):\n    img = cv2.imread(df['Images'][i])\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    edges = cv2.Canny(gray,200,100)\n    ax[i \/\/ 3, i % 3].imshow(edges)\n    print(df['Images'][i])","b07dd435":"new_style = {'grid': False}\nplt.rc('axes', **new_style)\n_, ax = plt.subplots(3, 3, sharex='col', sharey='row', figsize=(12, 12))\nj = 0\nfor i in range((len(df['Images'])-1),(len(cat_train_df['Images'])-10),-1):\n    img = cv2.imread(df['Images'][i])\n    ax[j \/\/ 3, j % 3].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    print(df['Images'][i])\n    j += 1","37bd32b4":"new_style = {'grid': False}\nplt.rc('axes', **new_style)\n_, ax = plt.subplots(3, 3, sharex='col', sharey='row', figsize=(12, 12))\nj = 0\nfor i in range((len(df['Images'])-1),(len(df['Images'])-10),-1):\n    img = cv2.imread(df['Images'][i])\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    edges = cv2.Canny(gray,200,100)\n    ax[j \/\/ 3, j % 3].imshow(edges)\n    print(df['Images'][i])\n    j += 1","7082188d":"def pixel_matrix(path):\n    image = plt.imread(path)\n    dims = np.shape(image)\n    return np.reshape(image, (dims[0] * dims[1], dims[2]))# changing shape","7f44d3eb":"def variance_of_laplacian(image):\n    # compute the Laplacian of the image and then return the focus\n    # measure, which is simply the variance of the Laplacian\n    return cv2.Laplacian(image, cv2.CV_64F).var()\n\ncount = 0\nfor imagePath in df['Images']:\n    # load the image, convert it to grayscale, and compute the\n    # focus measure of the image using the Variance of Laplacian\n    # method\n    image = cv2.imread(imagePath)\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    fm = variance_of_laplacian(gray)\n\n    # if the focus measure is less than the supplied threshold,\n    # then the image should be considered \"blurry\"\n    \n    if fm < 110.0:\n        count += 1\n        \nprint(\"Total blur image is \",count)","5cbc5e74":"path = '..\/input\/flickr-image-dataset'","2f6ac292":"#Normalize images - divide by 255\ntrain_datagen = ImageDataGenerator(rescale=1. \/ 255)","1e4a93b8":"#Resize images, if needed\ntrain = train_datagen.flow_from_directory(path, \n                                          target_size=(256, 256), \n                                          batch_size=340, \n                                          class_mode=None)","da7a1ec3":"#iterating on each image and covert the RGB to Lab.\nX =[]\nY =[]\nfor img in train[0]:\n    try:\n        lab = rgb2lab(img)\n        X.append(lab[:,:,0]) \n        Y.append(lab[:,:,1:] \/ 128) #A and B values range from -127 to 128, \n      #so we divide the values by 128 to restrict values to between -1 and 1.\n    except:\n        print('error')","db46a73f":"X = np.array(X)\nY = np.array(Y)\nX = X.reshape(X.shape+(1,)) #dimensions to be the same for X and Y\nprint(X.shape)\nprint(Y.shape)","411f12de":"#Encoder\nmodel = Sequential()\nmodel.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=2, input_shape=(256, 256, 1)))\nmodel.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\nmodel.add(Conv2D(128, (3,3), activation='relu', padding='same', strides=2))\nmodel.add(Conv2D(256, (3,3), activation='relu', padding='same'))\nmodel.add(Conv2D(256, (3,3), activation='relu', padding='same', strides=2))\nmodel.add(Conv2D(512, (3,3), activation='relu', padding='same'))\nmodel.add(Conv2D(512, (3,3), activation='relu', padding='same'))\nmodel.add(Conv2D(256, (3,3), activation='relu', padding='same'))","a7dce3b3":"#Decoder\n#Decoder\n#Note: For the last layer we use tanh instead of Relu. \n#This is because we are colorizing the image in this layer using 2 filters, A and B.\n#A and B values range between -1 and 1 so tanh (or hyperbolic tangent) is used\n#as it also has the range between -1 and 1. \n#Other functions go from 0 to 1.\nmodel.add(Conv2D(128, (3,3), activation='relu', padding='same'))\nmodel.add(UpSampling2D((2, 2)))\nmodel.add(Conv2D(64, (3,3), activation='relu', padding='same'))\nmodel.add(UpSampling2D((2, 2)))\nmodel.add(Conv2D(32, (3,3), activation='relu', padding='same'))\nmodel.add(Conv2D(16, (3,3), activation='relu', padding='same'))\nmodel.add(Conv2D(2, (3, 3), activation='tanh', padding='same'))\nmodel.add(UpSampling2D((2, 2)))","f48a8913":"model.compile(optimizer='adam', loss='mse' , metrics=['accuracy'])\nmodel.summary()\nhistory=model.fit(X,Y,validation_split=0.1, epochs=150, batch_size=16)\nmodel.save('other_files\/colorize_autoencoder.model')","cc107c1d":"accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(accuracy))\nplt.plot(epochs, accuracy, 'b', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'r', label='Test accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.title('Accuracy Model')\nplt.legend()\nplt.show()","5d789175":"plt.figure()\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Test loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.title('Loss Model')\nplt.legend()\nplt.show()","fe09894e":"tf.keras.models.load_model(\n    'other_files\/colorize_autoencoder.model',\n    custom_objects=None,\n    compile=True)\nimg1_color=[]\nimg1=img_to_array(load_img('..\/input\/flickr-image-dataset\/flickr30k_images\/flickr30k_images\/1000268201.jpg'))\nimg1 = resize(img1 ,(256,256))\nimg1_color.append(img1)\nimg1_color = np.array(img1_color, dtype=float)\nimg1_color = rgb2lab(1.0\/255*img1_color)[:,:,:,0]\nimg1_color = img1_color.reshape(img1_color.shape+(1,))\noutput1 = model.predict(img1_color)\noutput1 = output1*128\nresult = np.zeros((256, 256, 3))\nresult[:,:,0] = img1_color[0][:,:,0]\nresult[:,:,1:] = output1[0]\nimshow(lab2rgb(result))\nimsave(\"result.png\", lab2rgb(result))","346dd2fd":"vggmodel = VGG16()\nnewmodel = Sequential()","84774c07":"#num = 0\nfor i, layer in enumerate(vggmodel.layers):\n    if i<19:          #Only up to 19th layer to include feature extraction only\n        newmodel.add(layer)\nnewmodel.summary()\nfor layer in newmodel.layers:\n    layer.trainable=False   #We don't want to train these layers again, so False. ","1116b813":"#Normalize images - divide by 255\ntrain_datagen = ImageDataGenerator(rescale=1. \/ 255)","a17b2472":"train = train_datagen.flow_from_directory(path, target_size=(224, 224), batch_size=32, class_mode=None)","e38356ae":"#Convert from RGB to Lab\nX =[]\nY =[]\nfor img in train[0]:\n    try:\n        lab = rgb2lab(img)\n        X.append(lab[:,:,0]) \n        Y.append(lab[:,:,1:] \/ 128) #A and B values range from -127 to 128, \n      #so we divide the values by 128 to restrict values to between -1 and 1.\n    except:\n        print('error')","ce0ecd49":"X = np.array(X)\nY = np.array(Y)\nX = X.reshape(X.shape+(1,)) #dimensions to be the same for X and Y\nprint(X.shape)\nprint(Y.shape)","ddfbb9eb":"#now we have one channel of L in each layer but, VGG16 is expecting 3 dimension, \n#so we repeated the L channel two times to get 3 dimensions of the same L channel\n\nvggfeatures = []\nfor i, sample in enumerate(X):\n    sample = gray2rgb(sample)\n    sample = sample.reshape((1,224,224,3))\n    prediction = newmodel.predict(sample)\n    prediction = prediction.reshape((7,7,512))\n    vggfeatures.append(prediction)\nvggfeatures = np.array(vggfeatures)\nprint(vggfeatures.shape)","808e9524":"#Decoder\nmodel = Sequential()\n\nmodel.add(Conv2D(256, (3,3), activation='relu', padding='same', input_shape=(7,7,512)))\nmodel.add(Conv2D(128, (3,3), activation='relu', padding='same'))\nmodel.add(UpSampling2D((2, 2)))\nmodel.add(Conv2D(64, (3,3), activation='relu', padding='same'))\nmodel.add(UpSampling2D((2, 2)))\nmodel.add(Conv2D(32, (3,3), activation='relu', padding='same'))\nmodel.add(UpSampling2D((2, 2)))\nmodel.add(Conv2D(16, (3,3), activation='relu', padding='same'))\nmodel.add(UpSampling2D((2, 2)))\nmodel.add(Conv2D(2, (3, 3), activation='tanh', padding='same'))\nmodel.add(UpSampling2D((2, 2)))\nmodel.summary()","69507ded":"model.compile(optimizer='Adam', loss='mse' , metrics=['accuracy'])\nhistory=model.fit(vggfeatures, Y,validation_split=0.1 ,verbose=1, epochs=1000, batch_size=128)\n\nmodel.save('colorize_autoencoder_VGG16.model')","88a4873e":"accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(accuracy))\nplt.plot(epochs, accuracy, 'b', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'r', label='Test accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.title('Accuracy Model')\nplt.legend()\nplt.show()","63c723b9":"plt.figure()\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Test loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.title('Loss Model')\nplt.legend()\nplt.show()","bbcf31c4":"#Predicting using saved model.\nmodel = tf.keras.models.load_model('..\/input\/vgg16-colorize-autoencoder\/colorize_autoencoder_VGG16_10000.model',\n                                   custom_objects=None,\n                                   compile=True)\n# testpath = '\/kaggle\/input'\n# files = os.listdir(testpath)\n# for idx, file in enumerate(testpath):\ntest = img_to_array(load_img('..\/input\/flickr-image-dataset\/flickr30k_images\/flickr30k_images\/1000268201.jpg'))\ntest = resize(test, (224,224), anti_aliasing=True)\ntest*= 1.0\/255\nlab = rgb2lab(test)\nl = lab[:,:,0]\nL = gray2rgb(l)\nL = L.reshape((1,224,224,3))\n#print(L.shape)\nvggpred = newmodel.predict(L)\nab = model.predict(vggpred)\n#print(ab.shape)\nab = ab*128\ncur = np.zeros((224, 224, 3))\ncur[:,:,0] = l\ncur[:,:,1:] = ab\nimshow(lab2rgb(cur))\n# imsave('images\/colorization2\/vgg_result\/result'+str(idx)+\".jpg\", lab2rgb(cur))","b3ffc2ef":"The objective of this project is to give color to black and white image.","e3e02a82":"# 1.1 Introduction","1423f484":"# 1.ML Project 3","c4262694":"![](https:\/\/cdn.shortpixel.ai\/client\/q_lossy,ret_img,w_841\/https:\/\/kharpann.com\/wp-content\/uploads\/2020\/06\/Autoencoder-for-Image-Colorization.png)","701cc61c":"# 5 Reference","82d99aa9":"# Cleaning Data","4f123a15":"# Compile and Train Model","5d346424":"# Design a Model","35d3642f":"# Testing with black and white Image","9a761e63":"# Ploting Curve","1cd706b3":"# 3 Autoencoder","f418f195":"# Import Libraries","8d2289e9":"# Data Visualization","bf5c6cdf":"# 2 Data Set Selection And EDA","d861b667":"![](https:\/\/tech.showmax.com\/2017\/10\/convnet-architectures\/image_0-8fa3b810.png)","cc343812":"# Ploting Curve","2ee7e594":"# 4 VGG16","9b6ed306":"\n* https:\/\/tech.showmax.com\/2017\/10\/convnet-architectures\/image_0-8fa3b810.png\n* https:\/\/www.kaggle.com\/hsankesara\/flickr-image-dataset\n* https:\/\/sanjayasubedi.com.np\/deeplearning\/black-and-white-to-color-using-deep-learning\/\n* https:\/\/github.com\/PacktPublishing\/Advanced-Deep-Learning-with-Keras\/blob\/master\/chapter3-autoencoders\/colorization-autoencoder-cifar10-3.4.1.py\n* https:\/\/github.com\/bnsreenu\/python_for_microscopists\/blob\/master\/090a-autoencoder_colorize_V0.2.py\n* https:\/\/github.com\/bnsreenu\/python_for_microscopists\/blob\/master\/092-autoencoder_colorize_transfer_learning_VGG16_V0.1.py\n* https:\/\/cdn.shortpixel.ai\/client\/q_lossy,ret_img,w_841\/https:\/\/kharpann.com\/wp-content\/uploads\/2020\/06\/Autoencoder-for-Image-Colorization.png\n* https:\/\/github.com\/bnsreenu\/python_for_microscopists","4167029f":"1. Eden Zere\n2. Mario Arismendi Matos\n3. Essey Abraham Tezare","35959b5b":"# 1.2 Agenda","11439038":"1. Data Set Selection\n2. EDA\n3. Autoencoder\n4. VGG16\n5. Reference","8932d71c":"# Reading the data","66af6d00":"# 1.3 Team Members","06c37867":"# Design a Model","e1e254e8":"# Testing with black and white Image","d5257b94":"# Compile and Train Model"}}