{"cell_type":{"13f9ec92":"code","e4429df3":"code","e1936642":"code","0bf7502c":"code","0c1761ff":"code","4e59f5ec":"code","b2caf244":"code","483ebbf2":"code","98bd4d4b":"code","bb7baf38":"code","13a8e675":"code","9caaa2d4":"code","aa0dcd82":"code","c14e1a3f":"code","0a6f496c":"code","7a08a8eb":"code","408eaf91":"code","1e0256fa":"code","897736a2":"code","4a58e91d":"markdown","9ad22296":"markdown","da5c0b50":"markdown","8fdc8439":"markdown","bd6c0d91":"markdown","9ec9b75a":"markdown","87b0b91f":"markdown","1edd3dc0":"markdown","6428389d":"markdown"},"source":{"13f9ec92":"import time\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport os\nfrom tqdm import tqdm, tqdm_notebook\nfrom PIL import Image\nfrom os.path import isfile, join, abspath, exists, isdir, expanduser\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torchvision.models.inception import *\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torch.optim import lr_scheduler\n\nimport random\n\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","e4429df3":"img_size = 299 # for Incerption V3\nBATCH_SIZE = 128\nN_EPOCHS = 5","e1936642":"train_path = \"..\/input\/petfinder-adoption-prediction\/train_images\/\"\ntest_path = \"..\/input\/petfinder-adoption-prediction\/test_images\/\"\ntrain_df = pd.read_csv('..\/input\/petfinder-adoption-prediction\/train\/train.csv')\ntest_df = pd.read_csv('..\/input\/petfinder-adoption-prediction\/test\/test.csv')","0bf7502c":"train_df.Name = train_df.Name.fillna('GOTNONAME')\ntest_df.Name = test_df.Name.fillna('GOTNONAME')\n\ntrain_df.Description = train_df.Description.fillna('GOTNODESC')\ntest_df.Description = test_df.Description.fillna('GOTNODESC')","0c1761ff":"# https:\/\/www.kaggle.com\/pvlima\/use-pretrained-pytorch-models\n\ncache_dir = expanduser(join('~', '.torch'))\nif not exists(cache_dir):\n    os.makedirs(cache_dir)\nmodels_dir = join(cache_dir, 'models')\nif not exists(models_dir):\n    os.makedirs(models_dir)","4e59f5ec":"!ls ..\/input\/","b2caf244":"!cp ..\/input\/pytorch-pretrained-models\/* ~\/.torch\/models\/","483ebbf2":"!ls ~\/.torch\/models","98bd4d4b":"# https:\/\/www.kaggle.com\/bminixhofer\/deterministic-neural-networks-using-pytorch\ndef seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()","bb7baf38":"# https:\/\/www.kaggle.com\/christofhenkel\/extract-image-features-from-pretrained-nn\ndef resize_to_square(im):\n    old_size = im.shape[:2] # old_size is in (height, width) format\n    ratio = float(img_size)\/max(old_size)\n    new_size = tuple([int(x*ratio) for x in old_size])\n    # new_size should be in (width, height) format\n    im = cv2.resize(im, (new_size[1], new_size[0]))\n    delta_w = img_size - new_size[1]\n    delta_h = img_size - new_size[0]\n    top, bottom = delta_h\/\/2, delta_h-(delta_h\/\/2)\n    left, right = delta_w\/\/2, delta_w-(delta_w\/\/2)\n    color = [0, 0, 0]\n    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n    return new_im\n\ndef load_image(path, pet_id):\n    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n    new_image = resize_to_square(image)\n    new_image = cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)\n    #new_image = preprocess_input(new_image)\n    return new_image","13a8e675":"class ImageLoader(Dataset):\n\n    def __init__(self, list_IDs, labels=None, dir_name=None, transform=None, return_id=False):\n        'Initialization'\n        self.labels = labels\n        self.list_IDs = list_IDs\n        self.dir_name = dir_name\n        self.transform = transform\n        self.return_id = return_id\n    \n    def __len__(self):\n        'Denotes the total number of samples'\n        return len(self.list_IDs)\n    \n    def __getitem__(self, index):\n        'Generates one sample of data'\n        ID = self.list_IDs[index]\n\n        try: \n            X = load_image(self.dir_name, ID)\n        except:\n            X = np.zeros((img_size, img_size, 3))\n            \n        if self.transform:\n            X = self.transform(X)\n        \n        if self.labels and self.return_id:\n            return X, y, ID\n        elif self.labels:\n            return X, y\n        elif self.return_id:\n            return X, ID\n        else:\n            return X","9caaa2d4":"normalize = transforms.Normalize(\n   mean=[0.485, 0.456, 0.406],\n   std=[0.229, 0.224, 0.225]\n)\nds_trans = transforms.Compose([\n                               #transforms.Scale(224),\n                               #transforms.CenterCrop(224),\n                               transforms.ToTensor(),\n                               normalize])","aa0dcd82":"class CustomInception3(Inception3):\n    def __init__(self, num_classes=1000, aux_logits=False, transform_input=False, final_pooling=None):\n        self.final_pooling = final_pooling\n        super(CustomInception3, self).__init__()\n        \n    def forward(self, x):\n        if self.transform_input:\n            x = x.clone()\n            x[:, 0] = x[:, 0] * (0.229 \/ 0.5) + (0.485 - 0.5) \/ 0.5\n            x[:, 1] = x[:, 1] * (0.224 \/ 0.5) + (0.456 - 0.5) \/ 0.5\n            x[:, 2] = x[:, 2] * (0.225 \/ 0.5) + (0.406 - 0.5) \/ 0.5\n        # 299 x 299 x 3\n        x = self.Conv2d_1a_3x3(x)\n        # 149 x 149 x 32\n        x = self.Conv2d_2a_3x3(x)\n        # 147 x 147 x 32\n        x = self.Conv2d_2b_3x3(x)\n        # 147 x 147 x 64\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        # 73 x 73 x 64\n        x = self.Conv2d_3b_1x1(x)\n        # 73 x 73 x 80\n        x = self.Conv2d_4a_3x3(x)\n        # 71 x 71 x 192\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        # 35 x 35 x 192\n        x = self.Mixed_5b(x)\n        # 35 x 35 x 256\n        x = self.Mixed_5c(x)\n        # 35 x 35 x 288\n        x = self.Mixed_5d(x)\n        # 35 x 35 x 288\n        x = self.Mixed_6a(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6b(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6c(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6d(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6e(x)\n        # 17 x 17 x 768\n        \n        ## we turn off auxiliary\n        #if self.training and self.aux_logits:\n        #    aux = self.AuxLogits(x)\n        \n        # 17 x 17 x 768\n        x = self.Mixed_7a(x)\n        # 8 x 8 x 1280\n        x = self.Mixed_7b(x)\n        # 8 x 8 x 2048\n        x = self.Mixed_7c(x)\n        # 8 x 8 x 2048\n        x = F.avg_pool2d(x, kernel_size=8) # size (batch_size, 2048, 1, 1)\n        # 1 x 1 x 2048\n        \n        ## We'll save average pooling over the last conv output, but turn off the last FC layer\n       \n        #x = F.dropout(x, training=self.training)\n        # 1 x 1 x 2048\n        #x = x.view(x.size(0), -1)\n        # 2048\n        #x = self.fc(x)\n        \n        ## turn off aux output\n        # 1000 (num_classes)\n        #if self.training and self.aux_logits:\n        #    return x, aux\n        \n        if self.final_pooling:\n            x = F.avg_pool1d(x.view(x.size(0), 2048, 1).permute(0, 2, 1), kernel_size=self.final_pooling)\n        \n        return x","c14e1a3f":"inception_weights = \"\/tmp\/.torch\/models\/inception_v3_google-1a9a5a14.pth\"","0a6f496c":"InceptionModel = CustomInception3(final_pooling=8)\n\nInceptionModel.load_state_dict(torch.load(inception_weights))","7a08a8eb":"def find_ids_w_images(df, image_folder):\n    pet_ids = [s.split(\"-1.jpg\")[0] for s in os.listdir(image_folder) if s.endswith(\"-1.jpg\")]\n    #df_img = df[train_df.PetID.isin(pet_ids)]\n    #pet_ids = df_img.PetID.values\n    return pet_ids","408eaf91":"def extract_features(df, image_folder, model):\n    model.eval() \n    model = model.cuda()\n    img_ids = find_ids_w_images(df, image_folder)\n    \n    data_ds = ImageLoader(img_ids, dir_name=image_folder, transform=ds_trans)\n    data_dl = DataLoader(data_ds, batch_size=BATCH_SIZE, shuffle=False)\n    \n    with torch.no_grad():\n        features = None\n        for x in tqdm(data_dl, disable=True):\n            x = x.cuda()\n            output = model(x)\n\n            if features is not None:\n                features = torch.cat((features, output), 0)\n            else:\n                features = output\n        \n        features = features.view(features.size(0), -1)\n        feat_df = pd.DataFrame(features.cpu().numpy(), columns=[f'img_{n}' for n in range(features.size(-1))])\n        feat_df = pd.concat((pd.DataFrame({'PetID': img_ids}), feat_df), axis=1)\n        \n        feat_df = df.merge(feat_df, on='PetID', how='outer')\n        \n        feat_df = feat_df.fillna(0)\n    \n    return feat_df","1e0256fa":"train_df = extract_features(train_df, train_path, InceptionModel)","897736a2":"test_df = extract_features(train_df, train_path, InceptionModel)","4a58e91d":"### Create Custom Inception instance and load weights","9ad22296":"### Define DataLoader that uploads and preprocess images","da5c0b50":"We apply an extra 1D average pooling to the final 1 x 1 x 2048 activations. <br>\nHere we use kernel of size 8, so the final output will have 256 features.","8fdc8439":"@christofhenkel kernel on image feature extraction with pretrained models in Keras (https:\/\/www.kaggle.com\/christofhenkel\/extract-image-features-from-pretrained-nn) inspired me to do the same thing with PyTorch. In this kernel I demonstrate how to extract features with pretrained Inception_v3 model in PyTorch. Previously, @pvlima posted a great kernel on image classification with pretrained models in PyTorch (https:\/\/www.kaggle.com\/pvlima\/use-pretrained-pytorch-models). But, unlike in Keras, pretrained models in PyTorch contain the last FC layer which, in the case of feature extraction, is unnecessary. Also, the trick with Inception_v3 is that it has two outputs: there's an auxiliary branch in the network (https:\/\/arxiv.org\/abs\/1409.4842, https:\/\/arxiv.org\/abs\/1512.00567) which helps with the classification task, but useless for our goal. So, in order to get rid of all unnecessary parts, I inherited Inception_v3 class and overrode the forward method. Also, I added extra 1D average pooling layer to reduce the number of features (originally, it's a 2048D vector). ","bd6c0d91":"We retain Inception constructor as is, so that we initiate all the layers in the network, even those that we won't use later. We add an extra field *final_pooling* that equals to the kernel size of the last 1d average pooling layer","9ec9b75a":"### Seed everything for deterministic result","87b0b91f":"### Image preprocessing","1edd3dc0":"### Extract features with CustomInception and concat them to the original dataframe","6428389d":"### Copy pretrained models weights into ~\/.torch - default directory for model weights"}}