{"cell_type":{"bb5fd91f":"code","6e067905":"code","92b21e64":"code","4328c32d":"code","d96ebe15":"code","80c69e27":"code","9eda543f":"code","bdb4088b":"code","9c025ae3":"code","6bcde705":"code","b0bfdee8":"code","e3ca5d46":"code","d15dc117":"code","5f37ae94":"code","b7ba91b2":"code","ca2cbcb2":"code","dcb60e66":"code","8ad5f8da":"code","8fc14834":"code","7015bb72":"code","ae59a039":"code","2f60938a":"code","77701d4e":"code","4542a3ca":"code","9c89e0e2":"code","4736de55":"code","7893512c":"code","32ad00b6":"code","941603a0":"code","743c5b43":"code","ba64559a":"code","3e65a422":"code","f862e82e":"code","910f3aa0":"code","fac1e1eb":"code","eec2cfcd":"code","94f868a6":"code","09f4b1b4":"code","39c02c09":"code","75aec005":"code","321a8b53":"code","54810712":"code","f677dc94":"code","b39636c7":"code","75757938":"code","745a2b85":"code","b903d95b":"code","6e261a7f":"code","7027b877":"code","3561d327":"code","cce534a0":"code","dc744daa":"code","c6fceab5":"code","44ac11e0":"code","83fb6f9f":"code","2eb6023a":"code","3727e44d":"code","514512fe":"code","311c9e3b":"code","9e68b949":"code","c4abea8b":"code","82e7c855":"code","9fb2274f":"code","94840899":"code","32b54ed6":"code","5d365a8f":"code","945d2b2e":"code","28bfbdd2":"code","b3ded9f1":"code","6ec94c79":"code","85be70c0":"code","702065c9":"code","ad16508b":"code","7436d293":"code","c895a794":"code","8de9c0bf":"code","f8270315":"code","a430560f":"code","0e04b8b1":"code","59f9b021":"code","944d57aa":"code","91d7b3b0":"code","858a8013":"code","d1061cfd":"code","90988551":"code","68ad887f":"code","8df1c0b3":"code","67c5bc48":"markdown","20bbc09a":"markdown","4b6045f1":"markdown","b7816f06":"markdown","1b99d3ad":"markdown","364e2d50":"markdown","7b68aff0":"markdown","228a9f09":"markdown","6d2cfbcf":"markdown","897140c3":"markdown","03b00db8":"markdown","78b9a318":"markdown","27924750":"markdown","0c746ab8":"markdown","91451ed9":"markdown","39671c94":"markdown","f8e1436b":"markdown","12862ff6":"markdown","b54edf8f":"markdown","19f8111c":"markdown","7968f02a":"markdown","8756eeb2":"markdown","944935c2":"markdown","0d312074":"markdown"},"source":{"bb5fd91f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6e067905":"!pip install feature_engine","92b21e64":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as snb\nimport scipy.stats as stats\nimport statsmodels.api as sm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n\nfrom feature_engine.missing_data_imputers import MeanMedianImputer, CategoricalVariableImputer, AddMissingIndicator\nfrom feature_engine.categorical_encoders import OneHotCategoricalEncoder, RareLabelCategoricalEncoder,OrdinalCategoricalEncoder\nfrom feature_engine.outlier_removers import OutlierTrimmer\nfrom feature_engine import variable_transformers\nfrom feature_engine.discretisers import EqualWidthDiscretiser","4328c32d":"#Getting data \n\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntrain.head(1)","d96ebe15":"#removing ID Column\ntrain = train.drop(columns=[\"Id\"])\ntest = test.drop(columns=[\"Id\"])\ntrain.head(1)","80c69e27":"# Function for plot and check Data Distribution\n\ndef diagnostic_plots(df, variable):\n    for i in variable: \n        plt.figure(figsize=(16, 4))\n\n        # histogram\n        plt.subplot(1, 3, 1)\n    \n        snb.distplot(df[i], bins=30)\n        plt.title('Histogram')\n\n        # Q-Q plot\n        plt.subplot(1, 3, 2)\n        stats.probplot(df[i], dist=\"norm\", plot=plt)\n        plt.ylabel('RM quantiles')\n\n        # boxplot\n        plt.subplot(1, 3, 3)\n        snb.boxplot(y=df[i])\n        plt.title('Boxplot')\n        \n        print(\"\\n***********{}**********\\n\".format(i))\n        print(\"**Skewness: \",df[i].skew())\n        plt.show()","9eda543f":"# Function to plot relationship between Categorical Variable and Target\n#~ Function to plot relationship between Categorical Variable and Target\n\ndef explore_relation_catTotar(dataX,y,col_category):\n    temp = dataX.copy()\n    temp[\"target\"]=y\n    for i in col_category:\n        fig = plt.figure()\n        fig = temp.groupby([i])[\"target\"].mean().plot()\n        fig.set_title('Relationship between {} and Item_Outlet_Sales'.format(i))\n        fig.set_ylabel('Mean Item_Outlet_Sales')\n        plt.show()","bdb4088b":"# Basic Functions for Outlier Detection - Skewed Distribution & Gaussian Distribution\n\n#Skewed Distribution\n\ndef skewed_outlier(df, variable):\n    \n    IQR = df[variable].quantile(0.75) - df[variable].quantile(0.25)\n\n    lower_boundary = df[variable].quantile(0.25) - (IQR * 3)\n    upper_boundary = df[variable].quantile(0.75) + (IQR * 3)\n\n    return upper_boundary, lower_boundary\n\ndef calculating_outlier_skewed(df,variables):\n    dic = {}\n    for i in variables:\n        ub, lb = skewed_outlier(df,i)\n        ub_len = len(df[df[i]>ub])\n        lb_len = len(df[df[i]<lb])\n        per_total = ((ub_len + lb_len)\/len(df))*100\n        if per_total != 0:\n            dic[i]=per_total\n    return dic\n\n\n# Gaussian Distribution\n\ndef gaussian_outlier(df,variable):\n    \n    upper_boundary = df[variable].mean() + 3 * df[variable].std()\n    lower_boundary = df[variable].mean() - 3 * df[variable].std()\n\n    return upper_boundary, lower_boundary\n\n\ndef calculating_outlier_gaussian(df,variables):\n    dic = {}\n    for i in variables:\n        ub, lb = gaussian_outlier(df,i)\n        ub_len = len(df[df[i]>ub])\n        lb_len = len(df[df[i]<lb])\n        per_total = ((ub_len + lb_len)\/len(df))*100\n        if per_total != 0:\n            dic[i]=per_total\n    return dic","9c025ae3":"#calculating number of years after house is sold\ntrain['SoldAfterYears'] = train['YrSold'] - train['YearBuilt']\ntest['SoldAfterYears'] = test['YrSold'] - test['YearBuilt']\n\n#Calculating Number of Months after house was sold\ntrain['SoldAfterMonths'] = ((train['YrSold'] - train['YearBuilt'])*12)  + train['MoSold']\ntest['SoldAfterMonths'] = ((test['YrSold'] - test['YearBuilt'])*12)  + test['MoSold']\n\n\n#calculating Renovation after years and difference in the renovations year and build year\ntrain['RenovationCalculation']=train['SoldAfterYears'] - (train['YearRemodAdd'] - train['YearBuilt'])\ntest['RenovationCalculation']=test['SoldAfterYears'] - (test['YearRemodAdd'] - test['YearBuilt'])\n\n\n\n#Adding New Renovation Column\n# 0 - No renovation\n# 1 - Renovation done\nrenovation_train = list(map(lambda x, y : 0 if x-y==0 else 1,train['YearRemodAdd'],train['YearBuilt']))\nrenovation_test = list(map(lambda x, y : 0 if x-y==0 else 1,test['YearRemodAdd'],test['YearBuilt']))\ntrain['Renovation_Done'] = renovation_train\ntest['Renovation_Done'] = renovation_test\n\n\n#updating garage status as per the Garage_built_year\ntrain[\"Garage_status\"] = train[\"GarageYrBlt\"] - train['YearBuilt']\ntrain['Garage_status'] = train.Garage_status.fillna(-1)\ntrain['Garage_status'] = train['Garage_status'].replace([i for i in range(-20,0)],'No_Garag')\ntrain['Garage_status'] = train['Garage_status'].replace(0,'Built-in')\ntrain['Garage_status'] = train['Garage_status'].replace([i for i in range (1,210)],'Built-Later')\n\ntest[\"Garage_status\"] = test[\"GarageYrBlt\"] - test['YearBuilt']\ntest['Garage_status'] = test.Garage_status.fillna(-1)\ntest['Garage_status'] = test['Garage_status'].replace([i for i in range(-20,0)],'No_Garag')\ntest['Garage_status'] = test['Garage_status'].replace(0,'Built-in')\ntest['Garage_status'] = test['Garage_status'].replace([i for i in range (1,210)],'Built-Later')\n","6bcde705":"#due to wrong values some of the output getting in -ve. replacing these with 0\ntrain['RenovationCalculation'] = train['RenovationCalculation'].replace([-1],0)\ntest['RenovationCalculation'] = test['RenovationCalculation'].replace([-1,-2],0)","b0bfdee8":"#Dropping the columns which are related to Date\/Years after extracting the usefull informations\ntrain = train.drop(columns=['YearBuilt','YearRemodAdd','YrSold','GarageYrBlt','SoldAfterMonths'],axis=1)\ntest = test.drop(columns=['YearBuilt','YearRemodAdd','YrSold','GarageYrBlt','SoldAfterMonths'],axis=1)","e3ca5d46":"#Separating Numerical and Categorical Columns\ncol_number = [i for i in train.columns if train[i].dtype!='O']\ncol_category =[ i for i in train.columns if train[i].dtype=='O']","d15dc117":"#checking the Numerical columns which has less than 20 unique values\n# let's visualise the values of the discrete variables\ndiscrete = []\n\nfor var in col_number:\n    if len(train[var].unique()) < 20:\n        print(var, ' values: ', train[var].unique())\n        discrete.append(var)\nprint('\\nThere are {} discrete variables'.format(len(discrete)))","5f37ae94":"cols_to_remove = ['OverallQual','FullBath','TotRmsAbvGrd','GarageCars']\nfor i in cols_to_remove:\n    discrete.remove(i)","b7ba91b2":"#Convert rest of discrete variables into Categorical.\ntrain[discrete] = train[discrete].astype(str)\ntest[discrete] = test[discrete].astype(str)","ca2cbcb2":"train[['1stFlrSF','BsmtFinSF2','LowQualFinSF','3SsnPorch','MiscVal','EnclosedPorch','ScreenPorch','SalePrice']].corr()","dcb60e66":"train = train.drop(columns=['1stFlrSF','BsmtFinSF2','LowQualFinSF','3SsnPorch','MiscVal','EnclosedPorch','ScreenPorch'],axis=1)\ntest = test.drop(columns=['1stFlrSF','BsmtFinSF2','LowQualFinSF','3SsnPorch','MiscVal','EnclosedPorch','ScreenPorch'],axis=1)","8ad5f8da":"#Separating Numerical and Categorical Columns\ncol_number = [i for i in train.columns if train[i].dtype!='O']\ncol_category =[ i for i in train.columns if train[i].dtype=='O']","8fc14834":"discrete1 = []\n\nfor var in col_number:\n    if len(train[var].unique()) < 20:\n        print(var, ' values: ', train[var].unique())\n        discrete1.append(var)\nprint()\nprint('There are {} discrete variables'.format(len(discrete1)))","7015bb72":"#removing them from separated col_number list\n\nfor i in discrete1:\n    col_number.remove(i)\n","ae59a039":"#As per the skewness, seperating the \"Skewed Features\" & \"Gaussian Features\"\nskewnewss = dict(train[col_number].skew())\ngaussian_features =[]\nskewed_features =[]\nfor i,j in skewnewss.items():\n    if (skewnewss[i]<0.75) and (skewnewss[i]> -0.75):\n        gaussian_features.append(i)\n    else:\n        skewed_features.append(i)","2f60938a":"#calculating the outlier present in each column in (%) ~ functions are defined above\ngaussian_outlier_percentage = calculating_outlier_gaussian(train,gaussian_features)\nskewed_outlier_percentage = calculating_outlier_skewed(train,skewed_features)","77701d4e":"#getting all those column names in the list\nskewed_outlier_cols_list = list(skewed_outlier_percentage.keys())\ngaussian_outlier_cols_list = list(gaussian_outlier_percentage.keys())","4542a3ca":"for i in ['LotFrontage','MasVnrArea']:\n    skewed_outlier_cols_list.remove(i)","9c89e0e2":"#before removing outliers\ntrain.shape","4736de55":"#Creating Pipeline to remove the outliers\n\noutlier_trimmer_pipe = Pipeline(steps=[\n    ('Gaussian_Outliers',OutlierTrimmer(distribution='gaussian',tail='both',\n                                       fold=3, variables=gaussian_outlier_cols_list)),\n    (\"Skewed_Outlier\",OutlierTrimmer(distribution='skewed',tail='both',\n                                    fold=3, variables=skewed_outlier_cols_list))\n])\n\noutlier_trimmer_pipe.fit(train)\ntrain = outlier_trimmer_pipe.transform(train)","7893512c":"train.shape","32ad00b6":"use_cols=list(train.columns)\nuse_cols.remove(\"SalePrice\")\n\nX_train, X_test, y_train, y_test = train_test_split(train[use_cols],train[\"SalePrice\"],\n                                                    test_size=0.25,random_state=2)\n\nX_train.shape, X_test.shape","941603a0":"#finding those variable which has missing values \nmissing_data_var_Xtrain = [c for c in X_train.columns if X_train[c].isnull().mean() != 0]","743c5b43":"#Variable \"GarageCars\" from test set is not matching X_train\ntest['GarageCars'] = test['GarageCars'].replace(['nan'],'0')\ntest['GarageCars'] = test['GarageCars'].replace(['5.0'],'4')\ntest['GarageCars'] = test['GarageCars'].replace(['0.0'],'0')\ntest['GarageCars'] = test['GarageCars'].replace(['1.0'],'1')\ntest['GarageCars'] = test['GarageCars'].replace(['2.0'],'2')\ntest['GarageCars'] = test['GarageCars'].replace(['3.0'],'3')\ntest['GarageCars'] = test['GarageCars'].replace(['4.0'],'4')","ba64559a":"#Separating Numerical and Categorical Columns from X_train missing_values columns\nmissing_number = [i for i in missing_data_var_Xtrain if X_train[i].dtype!='O']\nmissing_category =[ i for i in missing_data_var_Xtrain if X_train[i].dtype=='O']","3e65a422":"missing_number","f862e82e":"num_impute_pipeline = Pipeline(steps=[\n    (\"Missing Indicator\",AddMissingIndicator(variables=[\"LotFrontage\"])), #Addming missing indicator\n    (\"Median_imputation\",MeanMedianImputer(imputation_method=\"median\",variables=['LotFrontage',\"MasVnrArea\"]))\n])\n\nnum_impute_pipeline.fit(X_train)\nX_train = num_impute_pipeline.transform(X_train)\nX_test = num_impute_pipeline.transform(X_test)\ntest = num_impute_pipeline.transform(test)","910f3aa0":"cat_missing_data = ['MasVnrType','Electrical']\ncat_missing_labels = ['Alley','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n                  'FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PoolQC',\n                  'Fence', 'MiscFeature']","fac1e1eb":"#creating a Pipeline to impute the missing_values in categorical variables\ncategory_missing_pipe = Pipeline(steps=[\n    ('Frequent_category',CategoricalVariableImputer(imputation_method='frequent',\n                                                   variables=cat_missing_data)),\n    ('Missing_Labels',CategoricalVariableImputer(imputation_method='missing',\n                                                variables=cat_missing_labels))\n])\n\n\n#imputing the missing value\ncategory_missing_pipe.fit(X_train)\nX_train = category_missing_pipe.transform(X_train)\nX_test = category_missing_pipe.transform(X_test)\ntest = category_missing_pipe.transform(test)","eec2cfcd":"#Checking if there are any missing date in Test Set now\nmissing_data_var_test = [c for c in test.columns if test[c].isnull().mean() != 0]\ntest[missing_data_var_test].isnull().mean()*100","94f868a6":"#Separating Numerical and Categorical Columns from test missing_values columns\ntest_missing_number = [i for i in missing_data_var_test if test[i].dtype!='O']\ntest_missing_category =[ i for i in missing_data_var_test if test[i].dtype=='O']","09f4b1b4":"test[test_missing_number].isnull().mean()","39c02c09":"test_imputer_remaining = MeanMedianImputer(imputation_method=\"median\",\n                                          variables=test_missing_number)\n\ntest_imputer_remaining.fit(X_train)\ntest = test_imputer_remaining.transform(test)","75aec005":"test[test_missing_category].isnull().mean()","321a8b53":"test_imputer_cat = CategoricalVariableImputer(imputation_method='frequent',\n                                             variables=test_missing_category)\n\ntest_imputer_cat.fit(X_train)\ntest = test_imputer_cat.transform(test)","54810712":"#Separating Numerical and Categorical Columns\ncol_number = [i for i in X_train.columns if X_train[i].dtype!='O']\ncol_category =[ i for i in X_train.columns if X_train[i].dtype=='O']","f677dc94":"X_train[col_number].describe().transpose()","b39636c7":"log_transform = ['LotFrontage','LotArea','GrLivArea'] #does not have 0\nbox_cox_transform = ['MasVnrArea','BsmtFinSF1','BsmtUnfSF','TotalBsmtSF',\n                     '2ndFlrSF','GarageArea','WoodDeckSF','OpenPorchSF']\navoid = ['OverallQual','FullBath','TotRmsAbvGrd','GarageCars','LotFrontage_na']\nscalar_PCA = []\n\nfor i in col_number:\n    if i not in avoid:\n        scalar_PCA.append(i)\n","75757938":"\nX_train[['PoolArea','MSSubClass']]=X_train[['PoolArea','MSSubClass']].astype(int)\nX_test[['PoolArea','MSSubClass']]=X_test[['PoolArea','MSSubClass']].astype(int)\ntest[['PoolArea','MSSubClass']]=test[['PoolArea','MSSubClass']].astype(int)\n","745a2b85":"disc_pipe = Pipeline(steps=[\n    ('PoolArea_disc',EqualWidthDiscretiser(bins = 2, variables=['PoolArea'])),\n    ('MSSubClass_disc',EqualWidthDiscretiser(bins=8,variables=['MSSubClass'])),   \n])\n\ndisc_pipe.fit(X_train)\nX_train = disc_pipe.transform(X_train)\nX_test = disc_pipe.transform(X_test)\ntest = disc_pipe.transform(test)","b903d95b":"explore_relation_catTotar(X_train,y_train,['MSSubClass'])","6e261a7f":"X_train[['MSSubClass']]=X_train[['MSSubClass']].astype(str)\nX_test[['MSSubClass']]=X_test[['MSSubClass']].astype(str)\ntest[['MSSubClass']]=test[['MSSubClass']].astype(str)","7027b877":"X_train1 = X_train.copy()\nX_test1 = X_test.copy()\ntest1 = test.copy()","3561d327":"#Separating Numerical and Categorical Columns\ncol_number = [i for i in X_train1.columns if X_train1[i].dtype!='O']\ncol_category =[ i for i in X_train1.columns if X_train1[i].dtype=='O']","cce534a0":"rare_encoder = RareLabelCategoricalEncoder(tol=0.05,\n                                                   n_categories=1,\n                                                    variables=col_category)\n\nrare_encoder.fit(X_train1)\nX_train1 = rare_encoder.transform(X_train1)\nX_test1 = rare_encoder.transform(X_test1)\ntest1 = rare_encoder.transform(test1)","dc744daa":"ordered =[]\none_hot=[]\nfor i in col_category:\n    if len(X_train1[i].unique())>=5:\n        ordered.append(i)\n    else:\n        one_hot.append(i)\n        ","c6fceab5":"target_encoding = OrdinalCategoricalEncoder(encoding_method=\"ordered\",\n                                           variables=ordered)\n\ntarget_encoding.fit(X_train1,y_train)\n","44ac11e0":"X_train1 = target_encoding.transform(X_train1)\nX_test1 = target_encoding.transform(X_test1)\ntest1 = target_encoding.transform(test1)","83fb6f9f":"one_hot = OneHotCategoricalEncoder(top_categories=None,\n                                  variables=one_hot,\n                                  drop_last=True)\n\none_hot.fit(X_train1)","2eb6023a":"X_train1 = one_hot.transform(X_train1)\nX_test1 = one_hot.transform(X_test1)\ntest1 = one_hot.transform(test1)","3727e44d":"X_train1.shape, X_test1.shape,test1.shape","514512fe":"X_train2 = X_train1.copy()\nX_test2 = X_test1.copy()\ntest2 = test1.copy()","311c9e3b":"X_train2_scalar_pca = X_train2[scalar_PCA].copy()\nX_test2_scalar_pca = X_test1[scalar_PCA].copy()\ntest2_scalar_pca = test2[scalar_PCA].copy()\n\n\nX_train2 = X_train2.drop(columns=scalar_PCA,axis=1)\nX_test2 = X_test2.drop(columns=scalar_PCA,axis=1)\ntest2 = test2.drop(columns=scalar_PCA,axis=1)","9e68b949":"scalar = StandardScaler()\n\nscalar.fit(X_train2_scalar_pca)\nX_train2_scalar_pca = scalar.transform(X_train2_scalar_pca)\nX_test2_scalar_pca = scalar.transform(X_test2_scalar_pca)\ntest2_scalar_pca = scalar.transform(test2_scalar_pca)","c4abea8b":"pca = PCA(n_components=11)\npca.fit(X_train2_scalar_pca)","82e7c855":"X_train2_scalar_pca = pca.transform(X_train2_scalar_pca)\nX_test2_scalar_pca = pca.transform(X_test2_scalar_pca)\ntest2_scalar_pca = pca.transform(test2_scalar_pca)","9fb2274f":"np.cumsum(pca.explained_variance_ratio_)","94840899":"pca.explained_variance_ratio_","32b54ed6":"cols = []\nfor i in range (0,len(pca.explained_variance_ratio_)):\n    var = 'col'+ str(i)\n    cols.append(var)\n\nX_train_pca=pd.DataFrame(data=X_train2_scalar_pca,columns=cols)\nX_test_pca = pd.DataFrame(data=X_test2_scalar_pca,columns=cols)\ntest_pca = pd.DataFrame(data=test2_scalar_pca,columns=cols)\n","5d365a8f":"X_train_pca.head()","945d2b2e":"X_train2 = X_train2.reset_index(drop=True)\nX_train2.head()","28bfbdd2":"X_train_model = X_train2.merge(X_train_pca,left_index=True, right_index=True)\nX_train_model.head()","b3ded9f1":"X_test2 = X_test2.reset_index(drop=True)\nX_test_model = X_test2.merge(X_test_pca,left_index=True, right_index=True)\nX_test_model.head()","6ec94c79":"test2 = test2.reset_index(drop=True)\ntest_model = test2.merge(test_pca,left_index=True, right_index=True)\ntest_model.head()","85be70c0":"X_train_model.shape, X_test_model.shape,test_model.shape","702065c9":"y_train_log = np.log(y_train)\ny_test_log = np.log(y_test)","ad16508b":"lr = LinearRegression()\nlr.fit(X_train_model,y_train_log)","7436d293":"y_predict_train = lr.predict(X_train_model)\ny_predict_test = lr.predict(X_test_model)","c895a794":"print(\"\\nTraining RMSE and R-Square: \")\nprint(\"Root Mean Square Error :\",mean_squared_error(y_train_log,y_predict_train)**0.5)\nprint(\"R-square :\",lr.score(X_train_model,y_train_log))\n\nprint(\"\\nTesting RMSE and R-Square: \")\nprint(\"Root Mean Square Error :\",mean_squared_error(y_test_log,y_predict_test)**0.5)\nprint(\"R-square :\",lr.score(X_test_model,y_test_log))","8de9c0bf":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import Lasso","f8270315":"model = Lasso(max_iter=20000)\n\n#define Model evealuation method\ncv = RepeatedKFold(n_splits=10,n_repeats=3, random_state=1)\n\n#define Grid\ngrid = {'alpha': [0.001,0.01,0.1,0.0012]}\n\nsearch = GridSearchCV(model, grid, scoring='neg_mean_squared_error', cv=6)\n\nresults=search.fit(X_train_model,y_train_log)\n\nprint('Config: %s' % results.best_params_)","a430560f":"y_predict_train = search.predict(X_train_model)\ny_predict_test = search.predict(X_test_model)","0e04b8b1":"print(\"\\nTraining RMSE and R-Square: \")\nprint(\"Root Mean Square Error :\",mean_squared_error(y_train_log,y_predict_train)**0.5)\n\n\nprint(\"\\nTesting RMSE and R-Square: \")\nprint(\"Root Mean Square Error :\",mean_squared_error(y_test_log,y_predict_test)**0.5)\n","59f9b021":"import xgboost as xgb\nfrom xgboost.sklearn import XGBRegressor","944d57aa":"param = {'objective':['reg:squarederror'],\n         'learning_rate': [0.1,0.11,0.12,0.13,0.15,0.2],\n        'max_depth': [1,2,3,4,5]}\n\nxgb1 = XGBRegressor()\n\nxgb_grid = GridSearchCV(xgb1, param, cv = 3)\n\nxgb_grid.fit(X_train_model, y_train_log)","91d7b3b0":"print(xgb_grid.best_score_)\nprint(xgb_grid.best_params_)","858a8013":"y_predict_train = xgb_grid.predict(X_train_model)\ny_predict_test =  xgb_grid.predict(X_test_model)","d1061cfd":"print(\"\\nTraining RMSE and R-Square: \")\nprint(\"Root Mean Square Error :\",mean_squared_error(y_train_log,y_predict_train)**0.5)\n\n\nprint(\"\\nTesting RMSE and R-Square: \")\nprint(\"Root Mean Square Error :\",mean_squared_error(y_test_log,y_predict_test)**0.5)","90988551":"y_predict_test_kaggle =  xgb_grid.predict(test_model)","68ad887f":"output = list(np.exp(y_predict_test_kaggle))","8df1c0b3":"# Importing library \nimport csv \n\nl=[output]\n  \n# data to be written row-wise in csv fil \ndata = l\n  \n# opening the csv file in 'w+' mode \nfile = open('out.csv', 'w+', newline ='') \n  \n# writing the data into the file \nwith file:     \n    write = csv.writer(file) \n    write.writerows(data) ","67c5bc48":"**Categorical variables which actually has missing data -** \n\n'MasVnrType' & ''Electrical'\n\n\n**Categorical variables which actually has missing Labels-**\n\n'Alley','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PoolQC','Fence', 'MiscFeature'\n\n","20bbc09a":"#### Handling Data\/Year Columns","4b6045f1":"#### Exploring the dataset","b7816f06":"### Part 4 : Missing Data Imputation","1b99d3ad":"Columns will not be considered for outlier removal = ['OverallQual', 'FullBath', 'TotRmsAbvGrd', 'GarageCars']","364e2d50":"Columns 'LotFrontage' and 'MasVnrArea' has got missing data. We will handle the outlier for this later","7b68aff0":"### Defining the functions for various usage","228a9f09":"#### Linear Regression","6d2cfbcf":"### Lasso","897140c3":"### Part 3 : Splitting the data into train and test","03b00db8":"We have removed 67 rows from the dataset in which outlier were present. THis has helped to increase the correlation of few of the columns as well with target ex - 'LoatArea","78b9a318":"### Part 7 - Variable Transformation","27924750":"**One-Hot Encoding**","0c746ab8":"### Part 8 : Model Building","91451ed9":"### Part 2 : Analysing and Removing Outlier","39671c94":"There are total 15 discrete variables out of that most of them are \"Ordinal Variables\" and some \"Nominal Data\"\n\nOut of those 15 variables below variables have good correlation with target so we keep them Numerical only and rest we will convert to string\/catergorical\n\n'OverallQual','FullBath','TotRmsAbvGrd','GarageCars'","f8e1436b":"### Part 1: Data Exploration & Data Cleaning","12862ff6":"### Part 6 - Categorical Data Encoding","b54edf8f":"#### XGBRegressor","19f8111c":"Below columns has vary low correlation with the target. Removing them..\n","7968f02a":"### Part 5 : Collecting Continuous Variables for further transformation","8756eeb2":"**Target Incoding**","944935c2":"'MSSubClass' is not following any monotonic relationship with the target so converting it back to categorical now and then perform target encoding on the same","0d312074":"Variables 'PoolArea','MSSubClass' has low cardinality & different values. We will use Discretiser on them."}}