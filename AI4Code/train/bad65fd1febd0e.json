{"cell_type":{"def7b733":"code","2b2a58b0":"code","b4298de4":"code","0b942c80":"code","ad0cf101":"code","ed989a7d":"code","40eb0e96":"code","c265423a":"code","3c66bb2d":"code","e8e4f15a":"code","de8f1f70":"code","63043bf0":"code","4d6b4c40":"code","1dc9c9d3":"code","9199ad82":"code","f6bf33de":"code","35a28bbc":"code","8fb87ed7":"code","168affc0":"code","f7ec4fbd":"code","e14a1271":"code","85f7d001":"code","ff28a652":"code","26d660c5":"code","235e3e21":"code","4018582a":"code","3bbb4960":"code","a39c069c":"code","45ce2226":"code","bbb504a3":"markdown","635a6f4b":"markdown","e013b16a":"markdown","db7a2de2":"markdown","8ee4d408":"markdown","72452d49":"markdown","fff84c31":"markdown","2115e0ff":"markdown","8b947da6":"markdown","22a2432a":"markdown","e810a727":"markdown","2276e45a":"markdown","3ba5eb4a":"markdown","b787fff5":"markdown","fbff85b5":"markdown","7090a2c4":"markdown"},"source":{"def7b733":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2b2a58b0":"# Type declairation to minimize RAM consumption\ncol_list = ['minutes_past', 'radardist_km', 'Ref', 'Ref_5x5_10th',\n       'Ref_5x5_50th', 'Ref_5x5_90th', 'RefComposite', 'RefComposite_5x5_10th',\n       'RefComposite_5x5_50th', 'RefComposite_5x5_90th', 'RhoHV',\n       'RhoHV_5x5_10th', 'RhoHV_5x5_50th', 'RhoHV_5x5_90th', 'Zdr',\n       'Zdr_5x5_10th', 'Zdr_5x5_50th', 'Zdr_5x5_90th', 'Kdp', 'Kdp_5x5_10th',\n       'Kdp_5x5_50th', 'Kdp_5x5_90th', 'Expected']\nd = {c : np.float32 for c in col_list}","b4298de4":"train = pd.read_csv(\"..\/input\/how-much-did-it-rain-ii\/train.zip\", dtype=d)\ntrain","0b942c80":"train.keys()","ad0cf101":"train.loc[train[\"Id\"] == 862571]","ed989a7d":"train.loc[train[\"Id\"] == 5]","40eb0e96":"train.isna().sum()","c265423a":"pd.set_option('display.float_format', lambda x: '%.3f' % x)","3c66bb2d":"train.fillna(0, inplace=True)\ntrain[[\"minutes_past\", \"radardist_km\", \"Expected\"]].describe()","e8e4f15a":"corr_mat = train.corr()\ncorr_mat.style.background_gradient(cmap='coolwarm')","de8f1f70":"import matplotlib.pyplot as plt\n\nf = plt.figure(figsize=(10, 10))\nplt.matshow(corr_mat, fignum=f.number)\nplt.colorbar()","63043bf0":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(15, 10))\nplt.scatter(np.arange(len(train[\"Expected\"].unique())), train[\"Expected\"].unique())","4d6b4c40":"plt.figure(figsize=(15, 10))\nplt.hist(train[\"Expected\"].unique())","1dc9c9d3":"from scipy import stats\n\nprint(stats.percentileofscore(train[\"Expected\"], 106))","9199ad82":"train.drop(train[train[\"Expected\"] >= 106].index, inplace=True)\ntrain","f6bf33de":"plt.figure(figsize=(15, 10))\nplt.scatter(np.arange(len(train[\"Expected\"].unique())), train[\"Expected\"].unique())","35a28bbc":"plt.figure(figsize=(15, 10))\nplt.hist(train[\"Expected\"].unique())","8fb87ed7":"train_grouped = train.groupby('Id')\ntarget = pd.DataFrame(train_grouped['Expected'].mean()) # mean, or any value, since they are the same in a group","168affc0":"target.reset_index(inplace=True)\ntarget = target[\"Expected\"]\ntarget","f7ec4fbd":"def pad_series(X, target_len=19):\n    seq_len = X.shape[0]\n    pad_size = target_len-seq_len\n    if (pad_size > 0):\n        X = np.pad(X, ((0,pad_size), (0,0)), 'constant', constant_values=0.)\n    return X, seq_len","e14a1271":"INPUT_WIDTH = 19\ndata_size = len(train_grouped)\nX_train = np.empty((data_size, INPUT_WIDTH, 22))\nseq_lengths = np.zeros(data_size)\ny_train = np.zeros(data_size)\n\ni = 0\nfor _, group in train_grouped:\n    X = group.values\n    seq_len = X.shape[0]\n    X_train[i,:seq_len,:] = X[:,1:23]\n    y_train[i] = X[0,23]\n    i += 1\n    del X\n    \ndel train_grouped","85f7d001":"X_train.shape","ff28a652":"from sklearn.model_selection import train_test_split\n\nx_train, x_valid, y_train, y_valid = train_test_split(X_train, target, random_state=42, shuffle=True)","26d660c5":"import tensorflow as tf\n\ndel X_train\ndel target\n\ntrain_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\nvalid_data = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\ntrain_data = train_data.batch(32)\nvalid_data = valid_data.batch(32)\n\ndel x_train\ndel x_valid\ndel y_train\ndel y_valid","235e3e21":"import tensorflow as tf\n\ndef create_model(shape=(19, 22)):\n    tfkl = tf.keras.layers\n    model = tf.keras.Sequential([\n        tfkl.Bidirectional((tfkl.LSTM(128, return_sequences=True)), input_shape=shape),\n        tfkl.Bidirectional(tfkl.LSTM(64)),\n        tfkl.Dense(64, activation=\"linear\"),\n        tfkl.Dense(1, activation=\"linear\")\n    ])\n    \n    model.compile(loss='mean_absolute_error', optimizer=\"adam\")\n    return model\n\nmodel = create_model()\nmodel.summary()","4018582a":"model.fit(train_data, epochs=100, validation_data=valid_data,\n          callbacks=[tf.keras.callbacks.ReduceLROnPlateau(), \n                    tf.keras.callbacks.EarlyStopping(patience = 10),\n                    tf.keras.callbacks.ModelCheckpoint(\"model.h5\", save_best_only=True)])","3bbb4960":"col_list.pop()\nd = {c: np.float32 for c in col_list}\n\ntest = pd.read_csv(\"..\/input\/how-much-did-it-rain-ii\/test.zip\", dtype=d)\ntest[test.columns[1:]] = test[test.columns[1:]].astype(np.float32)\ntest_ids = test['Id'].unique()\n\n# Convert all NaNs to zero\ntest = test.reset_index(drop=True)\ntest.fillna(0.0, inplace=True)\ntest_groups = test.groupby(\"Id\")\ntest_size = len(test_groups)\n\nX_test = np.zeros((test_size, INPUT_WIDTH, 22), dtype=np.float32)\n\ni = 0\nfor _, group in test_groups:\n    X = group.values\n    seq_len = X.shape[0]\n    X_test[i,:seq_len,:] = X[:,1:23]\n    i += 1\n    del X\n    \ndel test_groups\nX_test.shape","a39c069c":"submission = pd.read_csv(\"..\/input\/how-much-did-it-rain-ii\/sample_solution.csv.zip\")\nsubmission","45ce2226":"model.load_weights(\"model.h5\")\npredictions = model.predict(X_test, batch_size=32)\nsubmission[\"Expected\"] = predictions\nsubmission.to_csv(\"submission.csv\", index=False)","bbb504a3":"Base on the score, I could have gotten a silver medal! Sadly I came too late!","635a6f4b":"Split the model into Train and Valid set","e013b16a":"I think because I assume the `Nan` values are `0` so there is near zero correlation with `Expected`. ","db7a2de2":"Our data looks good now. I think someone measured the outliers on stormy days.  \n  \nNow let's use Seaborn's `pairplot` to see the relations between variables, but let's select only specific values.","8ee4d408":"Let's calculate the correlation matrix","72452d49":"In the `Expected` column, the mean is about `106`, but 75% of the data is lower than `3.8`, and the max is `33017`. Our data has a lot of outliers. Let's plot them~!","fff84c31":"# EDA and Preprocessing","2115e0ff":"Convert into `tf.data.Dataset` to avoid Out of memory while training, and delete unused variables.","8b947da6":"Now let's exclude the outliers. We will use `scipy.stats.precentileofscore` to calculate the percentile of a given number in the column. Let's try with `mean`.","22a2432a":"# Make predictions","e810a727":"# Create and train the Model","2276e45a":"# Data description\nTo understand the data, you have to realize that there are multiple radar observations over the course of an hour, and only one gauge observation (the 'Expected'). That is why there are multiple rows with the same 'Id'.  \n\nThe columns in the datasets are:  \n\nId:  A unique number for the set of observations over an hour at a gauge.  \nminutes_past:  For each set of radar observations, the minutes past the top of the hour that the radar observations were carried out.  Radar observations are snapshots at that point in time.  \nradardist_km:  Distance of gauge from the radar whose observations are being reported.  \nRef:  Radar reflectivity in km  \nRef_5x5_10th:   10th percentile of reflectivity values in 5x5 neighborhood around the gauge.  \nRef_5x5_50th:   50th percentile  \nRef_5x5_90th:   90th percentile  \nRefComposite:  Maximum reflectivity in the vertical column above gauge.  In dBZ.  \nRefComposite_5x5_10th  \nRefComposite_5x5_50th  \nRefComposite_5x5_90th  \nRhoHV:  Correlation coefficient (unitless)  \nRhoHV_5x5_10th  \nRhoHV_5x5_50th  \nRhoHV_5x5_90th  \nZdr:    Differential reflectivity in dB  \nZdr_5x5_10th  \nZdr_5x5_50th  \nZdr_5x5_90th  \nKdp:  Specific differential phase (deg\/km)  \nKdp_5x5_10th  \nKdp_5x5_50th  \nKdp_5x5_90th  \nExpected:  Actual gauge observation in mm at the end of the hour.","3ba5eb4a":"Now we begin processing the data into time series data. The time series is the `minutes_past` column.  \nThanks to the notebook: https:\/\/www.kaggle.com\/andkul\/deep-lstm-to-predict-rainfall","b787fff5":"92% of the data is lower than `106`, which is about 1.279M rows, while our data has `13765201` rows. Let's exlude the outliers","fbff85b5":"Let's look at some samples","7090a2c4":"We can see that most of the data is `NaN`, which indicates there is no data. However we can assume that if there is no data, these values can be zero. It is not true in most cases, since `0 dB` does not mean no sound: it only means the pressure of the measured data compares to the pressure of the reference point is equal. Source: https:\/\/www.animations.physics.unsw.edu.au\/jw\/dB.htm#definition"}}