{"cell_type":{"61a8594f":"code","54de343a":"code","763ecab7":"code","5e5378c2":"code","2c80a87d":"code","cb5ea213":"code","40defa64":"code","c30d5e27":"code","82a57490":"code","85f7649d":"code","5b1b2e39":"code","d64129fc":"code","91b399c4":"code","1c0c414a":"code","42fdd125":"code","abaf7660":"code","053899a9":"code","083ee861":"code","5fd11ffa":"code","04bd58a2":"code","e02aea44":"code","60bc52ad":"code","44adfdba":"code","0fe37c46":"code","c9328055":"code","fa2c9e66":"code","6779d59d":"code","88d3880d":"code","c76aa407":"code","f735bae0":"code","965b3064":"code","9de3852d":"code","9905bba9":"code","ddda2869":"code","81fc3a27":"code","548b398d":"code","c8456b36":"code","2b3424c1":"code","bfe00234":"code","15953476":"code","a29f91e0":"code","1a0c5bba":"code","aad0c70c":"code","a39dfb80":"code","2438d3c3":"code","9c13885c":"code","f2fc2a8c":"code","863f07b5":"code","749acf88":"code","9d3f21fa":"code","f9e2cc8d":"code","59d8b99e":"code","4b64a709":"code","8f1c3738":"code","57ece597":"code","b526c912":"code","c5199786":"code","739fb2a8":"code","e725b903":"code","ffadc536":"code","56cbfa19":"code","86f97a33":"code","6aec91ae":"code","87c7186b":"code","38473054":"code","4da3f0af":"code","e60b8e65":"code","4af5cdc6":"code","0dd4ca7c":"code","9c146258":"code","5a1ef3a4":"code","924e4c72":"code","dedfaa4b":"code","0d156bd0":"code","21a59d1e":"code","2cf3ca0a":"code","d0b47a44":"code","24198a03":"code","039c67c5":"code","b893a980":"code","c4b395c0":"code","92c43e48":"code","fdaf3f0e":"markdown","f175bac2":"markdown","129b3062":"markdown","3b041447":"markdown","c0d5d589":"markdown","9d88d611":"markdown","cda2dab9":"markdown","ea004e82":"markdown","44845c5b":"markdown","3befbac6":"markdown","fe976cb5":"markdown","464c1347":"markdown","aeba8968":"markdown","2242b01d":"markdown","2f0fa58e":"markdown","368b390c":"markdown","bcaa3f8e":"markdown","e14bc124":"markdown","58bd0f17":"markdown","78dc1fe5":"markdown","bf0b2135":"markdown","a4b741a9":"markdown"},"source":{"61a8594f":"# This notebook is based on https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard","54de343a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","763ecab7":"import matplotlib.pyplot as plt\nimport matplotlib_inline\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm, skew","5e5378c2":"train = pd.read_csv('..\/input\/house-prices-data\/train.csv')","2c80a87d":"train.head()","cb5ea213":"print('The shape of the train dataset is {}'.format(train.shape))","40defa64":"train.isnull().sum()","c30d5e27":"train.isnull().sum()\/len(train)*100","82a57490":"###Drop the 'Id' column\n\ntrain_Id = train['Id']\nprint('The original shape of the train dataset is {}'.format(train.shape))\ntrain.drop('Id', axis = 1, inplace = True)\nprint('The new shape of the train dataset is {}'.format(train.shape))","85f7649d":"#Determine the number of missing values\ntrain_na = train.isnull().sum()\/len(train) * 100\ntrain_na = train_na.drop(train_na[train_na == 0].index).sort_values(ascending=False)\ntrain_na","5b1b2e39":"#Let's explore the outliers\n\nsns.set_style('darkgrid')\nfig,ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.xlabel('GrLivArea')\nplt.ylabel('SalePrice')\nplt.show()","d64129fc":"train = train.drop(train[train['GrLivArea'] > 4000].index)","91b399c4":"sns.set_style('darkgrid')\nfig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.xlabel('GrLivArea')\nplt.ylabel('SalePrice')\nplt.show()","1c0c414a":"#Let's explore the target variable 'SalePrice'\n\nsns.histplot(train['SalePrice'], stat = 'probability', bins = 30, kde = True)\nplt.show()","42fdd125":"prob = stats.probplot(train['SalePrice'], plot = plt)\nplt.show()","abaf7660":"#We'll use the numpy log1p function for the log transformation of the target variable\n\ntrain['SalePrice'] = np.log1p(train['SalePrice'])","053899a9":"sns.histplot(train['SalePrice'], bins = 30, kde = True)\nplt.show()","083ee861":"prob = stats.probplot(train['SalePrice'], plot = plt)\nplt.show()","5fd11ffa":"plt.figure(figsize = (9,9))\nsns.heatmap(train.corr())\nplt.show()","04bd58a2":"train.corr()['SalePrice'].sort_values(ascending=False)","e02aea44":"for col in ('PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu'):\n  train[col] = train[col].fillna('None')","60bc52ad":"train['LotFrontage'] = train.groupby('Neighborhood')['LotFrontage'].transform(lambda x:x.fillna(x.median()))","44adfdba":"train_na = train.isnull().sum()\/len(train)\ntrain_na = train_na.drop(train_na[train_na == 0].index).sort_values(ascending=False)\ntrain_na","0fe37c46":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n  train[col] = train[col].fillna('None')","c9328055":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n  train[col] = train[col].fillna(0)","fa2c9e66":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n  train[col] = train[col].fillna(0)","6779d59d":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n  train[col] = train[col].fillna('None')","88d3880d":"train['MasVnrType'] = train['MasVnrType'].fillna('None')\ntrain['MasVnrArea'] = train['MasVnrArea'].fillna(0)","c76aa407":"train['MSZoning'] = train['MSZoning'].fillna(train['MSZoning'].mode()[0])","f735bae0":"train = train.drop(['Utilities'], axis = 1)","965b3064":"train['Functional'] = train['Functional'].fillna('Typ')","9de3852d":"for col in ('Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType'):\n  train[col] = train[col].fillna(train[col].mode()[0])","9905bba9":"train['MSSubClass'] = train['MSSubClass'].fillna('None')","ddda2869":"#Check for remaining missing values\ntrain_na = train.isnull().sum()\/len(train)\ntrain_na = train_na.drop(train_na[train_na == 0].index).sort_values(ascending=False)\nprint('There are {} missing values'.format(len(train_na)))","81fc3a27":"from sklearn.preprocessing import LabelEncoder\n\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\nfor c in cols:\n  le = LabelEncoder()\n  le.fit(list(train[c].values))\n  train[c] = le.transform(list(train[c].values))\n\nprint('The shape of train: {}'.format(train.shape))","548b398d":"###Assign 'SalePrice' to target\ntarget = train['SalePrice']\n\n###Drop the 'SalePrice' from data\ntrain.drop(['SalePrice'], axis = 1, inplace = True)\nprint('Shape of train dataset: {}'.format(train.shape))","c8456b36":"import numpy as np\n\nfor f in ('OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea'):\n  train[f+'_p2'] = train[f] **2\n  train[f+'_p3'] = train[f] **3\n  train[f+'_psqrt'] = np.sqrt(train[f])\n\n#Adding total sqfootage feature\ntrain['TotalSF'] = train['TotalBsmtSF'] + train['1stFlrSF'] + train['2ndFlrSF']\n# train['OverArea'] = train['OverallQual'] * train['GrLivArea']\n# train['OverArea'] = train['GarageCars'] * train['GarageArea']","2b3424c1":"num_feats = train.dtypes[train.dtypes != 'object'].index\n\nskew_feats = train[num_feats].apply(lambda x:skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew':skew_feats})\nskewness.head(10)","bfe00234":"skewness = skewness[abs(skewness) > 0.75]\nprint('There are {} skewed numerical features to Box Cox transform'.format(skewness.shape[0]))","15953476":"from scipy.special import boxcox1p\nskew_feats = skewness.index\nlam = 0.15\nfor s in skew_feats:\n  train[s] = boxcox1p(train[s],lam)","a29f91e0":"test = pd.read_csv('..\/input\/house-prices-data\/test.csv')","1a0c5bba":"test.head()","aad0c70c":"test_na = test.isnull().sum()\/len(test) * 100\ntest_na = test_na.drop(test_na[test_na == 0].index).sort_values(ascending=False)\ntest_na","a39dfb80":"#Similar to the train dataset, let's save the Id column to test_Id and then drop it from the test dataset\ntest_Id = test['Id']\ntest.drop('Id',axis=1,inplace=True)","2438d3c3":"for col in ('PoolQC', 'MiscFeature', 'Alley', 'Fence','FireplaceQu'):\n  test[col] = test[col].fillna('None')","9c13885c":"test['LotFrontage'] = test.groupby('Neighborhood')['LotFrontage'].transform(lambda x:x.fillna(x.median()))","f2fc2a8c":"for col in ('GarageType','GarageFinish','GarageQual','GarageCond'):\n  test[col] = test[col].fillna('None')","863f07b5":"for col in ('GarageYrBlt','GarageArea','GarageCars'):\n  test[col] = test[col].fillna(0)","749acf88":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n  test[col] = test[col].fillna(0)","9d3f21fa":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n  test[col] = test[col].fillna('None')","f9e2cc8d":"test['MasVnrType'] = test['MasVnrType'].fillna('None')\ntest['MasVnrArea'] = test['MasVnrArea'].fillna(0)","59d8b99e":"test['MSZoning'] = test['MSZoning'].fillna(test['MSZoning'].mode()[0])","4b64a709":"test = test.drop(['Utilities'], axis =1)","8f1c3738":"test['Functional'] = test['Functional'].fillna('Typ')\ntest['MSSubClass'] = test['MSSubClass'].fillna('None')","57ece597":"for col in ('Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType'):\n  test[col] = test[col].fillna(test[col].mode()[0])","b526c912":"cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\nfor c in cols:\n  le.fit(list(test[c].values))\n  test[c] = le.transform(list(test[c].values))\n","c5199786":"for f in ('OverallQual', 'GrLivArea','GarageCars', 'GarageArea'):\n  test[f+'_p2'] = test[f] **2\n  test[f+'_p3'] = test[f] **3\n  test[f+'_psqrt'] = np.sqrt(test[f])\n\n#Adding total sqfootage feature\ntest['TotalSF'] = test['TotalBsmtSF'] + test['1stFlrSF'] + test['2ndFlrSF']\n# test['OverArea'] = test['OverallQual'] * test['GrLivArea']\n# test['OverArea'] = test['GarageCars'] * test['GarageArea']","739fb2a8":"numeric_feats = test.dtypes[test.dtypes != 'object'].index\n\nskewed_feats = test[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew': skewed_feats})\nskewness.head(10)","e725b903":"skewness = skewness[abs(skewness) > 0.75]\nprint('There are {} skewed numerical features to Box Cox transform'.format(skewness.shape[0]))\n\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n  test[feat] = boxcox1p(test[feat], lam)","ffadc536":"print('Test data shape:{}'.format(test.shape))","56cbfa19":"print('Train data shape: {}'.format(train.shape))","86f97a33":"#Save the number of rows for each dataset\nlen_train = train.shape[0]\nlen_test = test.shape[0]","6aec91ae":"#Let's combine the train and test datasets prior to OHE\ndata = pd.concat((train,test)).reset_index(drop=True)","87c7186b":"#OHE\ndata = pd.get_dummies(data)\nprint(data.shape)","38473054":"#Let's split 'data' back to train and test\ntrain = data[:len_train]\ntest = data[len_train:]\n\nprint('Train dataset shape: {}'.format(train.shape))\nprint('Test dataset shape: {}'.format(test.shape))","4da3f0af":"pip install scikit-optimize","e60b8e65":"import lightgbm as lgb\nfrom sklearn import metrics\n\ndef optimize(params, param_names, x, y):\n  params = dict(zip(param_names, params))\n  model = lgb.LGBMRegressor(**params)\n  kf = model_selection.KFold(n_splits = 5)\n  accuracies = []\n  for idx in kf.split(X=x, y=y):\n    train_idx, test_idx = idx[0], idx[1]\n    xtrain = x.iloc[train_idx]\n    ytrain = y.iloc[train_idx]\n\n    xtest = x.iloc[test_idx]\n    ytest = y.iloc[test_idx]\n\n    model.fit(xtrain, ytrain)\n    preds = model.predict(xtest)\n    fold_acc = metrics.mean_squared_error(ytest, preds)\n    accuracies.append(fold_acc)\n\n  return np.mean(accuracies)\n\n","4af5cdc6":"# from functools import partial\n# from skopt import space\n# from skopt import gp_minimize\n# from sklearn import model_selection\n# from skopt import callbacks\n# from skopt.callbacks import CheckpointSaver\n# from scipy.stats import uniform as sp_uniform\n\n# checkpoint_saver = CheckpointSaver(\".\/checkpoint.pkl\", compress = 9)\n\n# param_space = [\n#     space.Categorical(['regression'], name = 'objective'),\n#     space.Integer(100, 5000, name = 'n_estimators'),\n#     space.Integer(2, 100, name = 'num_leaves'),\n#     space.Integer(1, 50, name = 'min_data_in_leaf'),\n#     space.Integer(1, 25, name = 'max_depth'),\n#     space.Real(0.0001, 0.1, name = 'learning_rate'),\n#     space.Real(0.01, 0.99, name = 'bagging_fraction'),\n#     space.Integer(1, 20, name = 'bagging_freq'),\n#     space.Integer(1, 10, name = 'bagging_seed'),\n#     space.Integer(2, 100, name = 'max_bin'),\n#     space.Real(0.01, 0.99, name = 'feature_fraction'),\n#     space.Integer(1, 10, name = 'feature_fraction_seed'),\n#     space.Integer(1, 20, name = 'min_sum_hessian_in_leaf')\n\n# ]\n\n\n# param_names = [\n#             'objective',\n#             'n_estimators',\n#             'num_leaves',\n#             'min_data_in_leaf',\n#             'max_depth',\n#             'learning_rate',\n#             'bagging_fraction',\n#             'bagging_freq',\n#             'bagging_seed',\n#             'max_bin',\n#             'feature_fraction',\n#             'feature_fraction_seed',\n#             'min_sum_hessian_in_leaf'\n\n# ]\n\n# optimization_function = partial(\n#     optimize,\n#     param_names = param_names,\n#     x=train,\n#     y=target\n# )\n\n# result = gp_minimize(\n#     optimization_function,\n#     dimensions = param_space,\n#     n_calls = 50,\n#     n_random_starts = 10,\n#     n_jobs = -1,\n#     callback = [checkpoint_saver],\n#     random_state = 123,\n#     verbose = True,\n# )\n","0dd4ca7c":"# from skopt import load\n\n# result = load('.\/checkpoint.pkl')\n\n# print(\"\"\"Best parameters:\n# objective=%s,\n# n_estimators=%d,\n# num_leaves=%d,\n# min_data_in_leaf=%d,\n# max_depth=%d,\n# learning_rate=%.6f,\n# bagging_fraction=%f,\n# bagging_freq=%d,\n# bagging_seed=%d,\n# max_bin=%d,\n# feature_fraction=%f,\n# feature_fraction_seed=%d,\n# min_sum_hessian_in_leaf=%d\n# \"\"\"  % (result.x[0], result.x[1],result.x[2], result.x[3],result.x[4],result.x[5],result.x[6],\n#         result.x[7],result.x[8],result.x[9],result.x[10],result.x[11],result.x[12]\n#                       ))","9c146258":"reg_lgb = lgb.LGBMRegressor(objective='regression',\nn_estimators=2355,\nnum_leaves=35,\nmin_data_in_leaf=2,\nmax_depth=3,\nlearning_rate=0.047790,\nbagging_fraction=0.507117,\nbagging_freq=11,\nbagging_seed=10,\nmax_bin=68,\nfeature_fraction=0.031244,\nfeature_fraction_seed=7,\nmin_sum_hessian_in_leaf=1)\nreg_lgb.fit(train,target)","5a1ef3a4":"lgb_preds = np.expm1(reg_lgb.predict(test))","924e4c72":"# from sklearn.ensemble import GradientBoostingRegressor \n\n# def optimize(params, param_names, x, y):\n#   params = dict(zip(param_names, params))\n#   model = GradientBoostingRegressor(**params)\n#   kf = model_selection.KFold(n_splits = 5)\n#   accuracies = []\n#   for idx in kf.split(X=x, y=y):\n#     train_idx, test_idx = idx[0], idx[1]\n#     xtrain = x.iloc[train_idx]\n#     ytrain = y.iloc[train_idx]\n\n#     xtest = x.iloc[test_idx]\n#     ytest = y.iloc[test_idx]\n\n#     model.fit(xtrain, ytrain)\n#     preds = model.predict(xtest)\n#     fold_acc = metrics.mean_squared_error(ytest, preds)\n#     accuracies.append(fold_acc)\n\n#   return np.mean(accuracies)","dedfaa4b":"# from functools import partial\n# from skopt import space\n# from skopt import gp_minimize\n# from sklearn import model_selection\n# from skopt import callbacks\n# from skopt.callbacks import CheckpointSaver\n\n# checkpoint_saver = CheckpointSaver(\".\/checkpoint.pkl\", compress = 9)\n\n# param_space = [\n#     space.Integer(3, 15, name = 'max_depth'),\n#     space.Real(0.01, 0.1, name = 'learning_rate'),\n#     space.Integer(1000, 4000, name = 'n_estimators'),\n#     space.Integer(2, 10, name = 'min_samples_split'),\n#     space.Integer(2, 10, name = 'min_samples_leaf'),\n#     space.Categorical(['ls', 'lad','huber'], name = 'loss'),\n#     space.Categorical(['auto', 'sqrt','log2'], name = 'max_features'),\n#     space.Categorical(['friedman_mse', 'mse','mae'], name = 'criterion')\n# ]\n\n# param_names = [\n#             'max_depth',\n#             'learning_rate',\n#             'n_estimators',\n#             'min_samples_split',\n#             'min_samples_leaf',\n#             'loss',\n#             'max_features',\n#             'criterion'\n\n# ]\n\n# optimization_function = partial(\n#     optimize,\n#     param_names = param_names,\n#     x=train,\n#     y=target\n# )\n\n# result = gp_minimize(\n#     optimization_function,\n#     dimensions = param_space,\n#     n_calls = 10,\n#     n_random_starts = 10,\n#     n_jobs = -1,\n#     callback = [checkpoint_saver],\n#     random_state = 123,\n#     verbose = True,\n# )\n","0d156bd0":"# from skopt import load\n\n# result = load('.\/checkpoint.pkl')\n\n# print(\"\"\"Best parameters:\n# max_depth=%d,\n# learning_rate=%.6f,\n# n_estimators=%d,\n# min_samples_split=%d,\n# min_samples_leaf=%d,\n# loss=%s,\n# max_features=%s,\n# criterion=%s\n# \"\"\"  % (result.x[0], result.x[1],result.x[2], result.x[3],result.x[4],result.x[5],result.x[6],\n#         result.x[7],\n#                       ))","21a59d1e":"from sklearn.ensemble import GradientBoostingRegressor \nreg_gbr = GradientBoostingRegressor(max_depth=5,\nlearning_rate=0.046092,\nn_estimators=2882,\nmin_samples_split=5,\nmin_samples_leaf=4,\nloss='lad',\nmax_features='log2',\ncriterion='mae')\nreg_gbr.fit(train,target)","2cf3ca0a":"gbr_preds = np.expm1(reg_gbr.predict(test))","d0b47a44":"estimators = [\n              ('gbr',GradientBoostingRegressor(max_depth=5,\nlearning_rate=0.046092,\nn_estimators=2882,\nmin_samples_split=5,\nmin_samples_leaf=4,\nloss='lad',\nmax_features='log2',\ncriterion='mae')),\n              ('lgb',lgb.LGBMRegressor(objective='regression',\nn_estimators=2355,\nnum_leaves=35,\nmin_data_in_leaf=2,\nmax_depth=3,\nlearning_rate=0.047790,\nbagging_fraction=0.507117,\nbagging_freq=11,\nbagging_seed=10,\nmax_bin=68,\nfeature_fraction=0.031244,\nfeature_fraction_seed=7,\nmin_sum_hessian_in_leaf=1))\n]","24198a03":"from sklearn.ensemble import StackingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nreg = StackingRegressor(\n    estimators = estimators,\n    final_estimator=RandomForestRegressor(n_estimators=10,\n                                          random_state=123)\n)","039c67c5":"reg.fit(train,target)","b893a980":"stack_preds = np.expm1(reg.predict(test))","c4b395c0":"#Assign weights to the 3 models with gbr having the heaviest weight as it is the most accurate of the three\nblend = lgb_preds*(0.15) + gbr_preds*(0.7) + stack_preds*(0.15)","92c43e48":"#Save the predictions for 'SalePrice' to 'submission.csv'\noutput = pd.DataFrame({'Id':test_Id,'SalePrice':blend})\noutput.to_csv('submission.csv',index=False)","fdaf3f0e":"Now it's time to fill the missing values. As I've stated earlier, for some features, NA means None so we'll fill those features with None","f175bac2":"<a id=\"LightGBM with Hyperparameter Tuning using skopt\"><\/a>\n### Training a Model\/Hyperparameter Tuning","129b3062":"<a id=\"Load Train Data\"><\/a>\n### Read Train Data","3b041447":"### Import libraries","c0d5d589":"<a id=\"Log Transformation of Target Variable\"><\/a>\n### Log Transformation of Target Variable","9d88d611":"Uncomment the next two cells below to search for the best hyperparameters of LightGBM model","cda2dab9":"<a id=\"Model Blending\"><\/a>\n### Model Blending","ea004e82":"We can see that the data is skewed to the right, meaning that the tail is on the right side. To create a good model, at least for regression, we'd like our data to follow a normal distribution","44845c5b":"Uncomment the next two cells below to search for the best hyperparameters for the GradientBoostingRegressor model","3befbac6":"<a id=\"Model Stacking\"><\/a>\n### Model Stacking","fe976cb5":"Now it's time to fill the missing values. As I've stated earlier, for some features, NA means None so we'll fill those features with None","464c1347":"<a id=\"Exploratory Data Analysis\"><\/a>\n### Exploratory Data Analysis","aeba8968":"To address the skewness of the target variable, we'll perform a log transformation","2242b01d":"The author recommends removing houses with more than 4000 square feet so let's do that","2f0fa58e":"<a id=\"Data Cleaning\"><\/a>\n### Data Cleaning","368b390c":"### Test dataset - Feature Engineering","bcaa3f8e":"We can see that OverallQual and GrLivArea have strong correlations with SalePrice","e14bc124":"<a id=\"Feature Engineering\"><\/a>\n### Feature Engineering","58bd0f17":"The QQ (Quantile-Quantile) plot above confirms that we don't have a normal distribution","78dc1fe5":"The author of the dataset, Dean De Cock of Truman State University, mentioned that there are outliers in the dataset particularly when we plot GrLivArea vs SalePrice","bf0b2135":"We can see that PoolQC, MiscFeature and Alley have more than 90% missing values. The natural tendency is to drop these columns. However, if we study the data_description.txt file, the label 'NA' for these features mean the absence of a pool, miscellaneous feature and an alley. Hence, we will not drop these columns but instead, we'll replace those with 'None' in the data cleaning section","a4b741a9":"### Read the Test Data"}}