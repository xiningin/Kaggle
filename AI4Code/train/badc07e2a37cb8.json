{"cell_type":{"f68e7426":"code","ec549f1c":"code","99430187":"code","5b65a3d8":"code","ab19e463":"code","1f8b5bbc":"code","fef1f227":"code","6876fb90":"code","6c012db8":"code","69f26662":"code","9685f1d8":"code","80c85b1f":"code","e60fd87b":"code","70750273":"code","afe325c0":"code","7f2be059":"code","8ea80a10":"code","ee6e250f":"code","8f3c5792":"code","9d89d925":"code","36929fe0":"code","1ad574c1":"code","ec7dfd79":"code","3d045ccf":"code","e36c776b":"code","92ff8aac":"code","c2f0351b":"code","90301d5e":"code","f8954a85":"code","79bbb1d2":"code","05657619":"code","893902f9":"code","9e9cf363":"code","2b3faa82":"code","48502a18":"code","391c5930":"code","dfe9f11b":"code","6bcd2ef3":"code","0042b6e2":"code","730c4b2d":"code","1b71ae40":"code","bdfe508b":"code","da403d92":"code","9b409f4a":"code","91effa26":"code","0a6132e0":"code","45c7a47a":"code","b8340b77":"code","fea18381":"code","bddfb7cc":"code","fcce17d7":"code","bb305d2b":"code","7c1b1f88":"code","9703419b":"code","a21ae83b":"code","321cc33f":"code","0831de84":"code","74b76a44":"code","82fefb77":"code","5f6301c5":"code","e244a684":"code","66aab700":"code","d72059b5":"code","c441ab56":"code","ecb0a00e":"code","e1018274":"code","c8980d4e":"code","9acd460b":"code","051dd7d4":"code","97e8a1ba":"code","f51f7bd6":"code","dbf1ddf3":"code","4c2e430e":"code","58c61328":"markdown","4788f4f1":"markdown","6a67030b":"markdown","3ac1065b":"markdown","9479e646":"markdown","2ee9816d":"markdown","710612d9":"markdown","20b970d5":"markdown","89144abf":"markdown","c3e0e36d":"markdown","3531280f":"markdown","7144bed6":"markdown","1bdb1314":"markdown","9b4cd553":"markdown","1ea51636":"markdown","94eddc97":"markdown","8c50de58":"markdown","417c4cbd":"markdown","c296cd50":"markdown","274548f9":"markdown","522cd7ac":"markdown","191b07b1":"markdown","4397eedf":"markdown","a3438ee4":"markdown"},"source":{"f68e7426":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom scipy.stats import ttest_rel\n\nimport datetime\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","ec549f1c":"df=pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/train.csv')\n","99430187":"df","5b65a3d8":"# Parsing Date\n# create a new column, date_parsed, with the parsed dates\n\ndf['date_time'] = pd.to_datetime(df['date_time'])\n\n'''df['Date'] = df['date_parsed'].dt.strftime('%m\/%d\/%y\/')\ndf['Time'] = df['date_parsed'].dt.strftime('%H:%M')'''\n\ndf.head(2)","ab19e463":"df.index=df['date_time']\ndf.drop(columns= ['date_time'], axis = 1, inplace=True)\n","1f8b5bbc":"df.describe()","fef1f227":"df.info()","6876fb90":"df.hist(figsize=(20,20), bins=25);","6c012db8":"# Checking for outlier\n# df.boxplot()\n\nplt.figure(figsize=(20,10))\nsns.boxplot(data=df)","69f26662":"Zscore=pd.DataFrame()\n\nZscore['sensor1']=( df.sensor_1 -df.sensor_1.mean())\/df.sensor_1.std()\nZscore['sensor2']=( df.sensor_2 -df.sensor_2.mean())\/df.sensor_2.std()\nZscore['sensor3']=( df.sensor_3 -df.sensor_3.mean())\/df.sensor_3.std()\nZscore['sensor4']=( df.sensor_4 -df.sensor_4.mean())\/df.sensor_4.std()\nZscore['sensor5']=( df.sensor_5 -df.sensor_5.mean())\/df.sensor_5.std()\n\nZscore['temp']=( df.deg_C -df.deg_C.mean())\/df.deg_C.std()\nZscore['absolute_humidity']=( df.absolute_humidity -df.absolute_humidity.mean())\/df.absolute_humidity.std()\nZscore['relative_humidity']=( df.relative_humidity -df.relative_humidity.mean())\/df.relative_humidity.std()\n\nZscore['target_carbon_monoxide']=( df.target_carbon_monoxide -df.target_carbon_monoxide.mean())\/df.target_carbon_monoxide.std()\nZscore['target_benzene']=( df.target_benzene -df.target_benzene.mean())\/df.target_benzene.std()\nZscore['target_nitrogen_oxides']=( df.target_nitrogen_oxides -df.target_nitrogen_oxides.mean())\/df.target_nitrogen_oxides.std()\n\n","9685f1d8":"plt.figure(figsize=(15,6))\n\n\nplt.subplot(1,2,1)\nplt.bar(Zscore.columns, (Zscore>3).sum())\nplt.xticks(rotation=90,fontsize = 15);\nplt.title('ZScore > 3' , fontsize = 18,fontweight = 'bold');\n\nplt.subplot(1,2,2)\nplt.bar(Zscore.columns, (Zscore<-2.5).sum())\nplt.xticks(rotation=90,fontsize = 15);\nplt.title('ZScore < -2.5', fontsize = 18,fontweight = 'bold');","80c85b1f":"print('No of Rows having ZScore greater than 3 is' ,(Zscore>3).sum().sum())\n\nprint('No of Rows having ZScore less than -2.5 is' ,(Zscore<-2.5).sum().sum())","e60fd87b":"# Check for correlation\n\nplt.figure(figsize=(10,10))\nsns.heatmap(df.corr(), annot= True)\n","70750273":"# Now Checking the coveriance\n# to check how the model is related\ndf.cov()","afe325c0":"# Checking the p value\npvalue= ttest_rel(df['relative_humidity'] , df['absolute_humidity'])\nprint(pvalue)","7f2be059":"from sklearn.feature_selection import mutual_info_regression","8ea80a10":"df_benz=pd.DataFrame()\ndf_no2=pd.DataFrame()\ndf_co = pd.DataFrame()\n\ndf_benz=df.drop(columns={'target_carbon_monoxide', 'target_nitrogen_oxides'}, axis=1)\ndf_co=df.drop(columns={ 'target_benzene', 'target_nitrogen_oxides'}, axis=1)\ndf_no2=df.drop(columns={'target_carbon_monoxide', 'target_benzene'}, axis=1)\n","ee6e250f":"# Mutual Information for benzene\n\n\nX = df_benz.copy()\ny = X.pop(\"target_benzene\")\n\n# Label encoding for categoricals\nfor colname in X.select_dtypes(\"object\"):\n    X[colname], _ = X[colname].factorize()\n\n# All discrete features should now have integer dtypes (double-check this before using MI!)\ndiscrete_features = X.dtypes == int\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(X, y, discrete_features)\nms_benz =mi_scores # Created a mi score card for benz\n\n# Mutual Information for nitrogen Oxides\n\n\nX = df_no2.copy()\ny = X.pop(\"target_nitrogen_oxides\")\n\n# Label encoding for categoricals\nfor colname in X.select_dtypes(\"object\"):\n    X[colname], _ = X[colname].factorize()\n\n# All discrete features should now have integer dtypes (double-check this before using MI!)\ndiscrete_features = X.dtypes == int\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(X, y, discrete_features)\nms_no2 =mi_scores # Created a mi score card for no2\n\n\n# Mutual Information for Carbon Monoxide\n\nX = df_co.copy()\ny = X.pop(\"target_carbon_monoxide\")\n\n# Label encoding for categoricals\nfor colname in X.select_dtypes(\"object\"):\n    X[colname], _ = X[colname].factorize()\n\n# All discrete features should now have integer dtypes (double-check this before using MI!)\ndiscrete_features = X.dtypes == int\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(X, y, discrete_features)\nms_co= mi_scores # Created a mi score card for co\n\n","8f3c5792":"def plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.grid(True)\n\n\nplt.figure(dpi=100, figsize=(16, 5))\nplt.suptitle('Mutual Information Score', fontsize=24)\n\nplt.subplot(1,3,1)\nplot_mi_scores(ms_co)\nplt.title('Carbon Monoxide')\n\nplt.subplot(1,3,2)\nplot_mi_scores(ms_no2)\nplt.title('Nitrogen oxide')\n\nplt.subplot(1,3,3)\nplot_mi_scores(ms_benz)\nplt.title('Benzene')\n\nplt.tight_layout()","9d89d925":"plt.figure(figsize=(20,20))\n\nplt.subplot(5, 1, 1)\ndf['sensor_1'].plot( color='orange' , title='Sensor1')\n\nplt.subplot(5, 1, 2)\ndf['sensor_2'].plot(color='red' , title=\"Sensor2\")\n\nplt.subplot(5, 1, 3)\ndf['sensor_3'].plot(color='purple' , title=\"Sensor3\")\n\nplt.subplot(5, 1, 4)\ndf['sensor_4'].plot(color='green' , title=\"Sensor4\")\n\nplt.subplot(5, 1, 5)\ndf['sensor_5'].plot(color='blue' , title=\"Sensor5\")\n\nplt.tight_layout()","36929fe0":"plt.figure(figsize=(20,13))\n\nplt.subplot(3, 1, 1)\ndf['target_carbon_monoxide'].plot( color='orange' , title='target_carbon_monoxide')\n\nplt.subplot(3, 1, 2)\ndf['target_benzene'].plot(color='red' , title=\"target_benzene\")\n\nplt.subplot(3, 1, 3)\ndf['target_nitrogen_oxides'].plot(color='purple' , title=\"target_nitrogen_oxides\")\n\nplt.tight_layout()","1ad574c1":"plt.figure(figsize=(20,10))\n\nplt.subplot(3,1,1)\ndf['relative_humidity'].plot(title= 'Relative_humidity')\n\nplt.subplot(3,1,2)\ndf['absolute_humidity'].plot(title= 'absolute_humidity')\n\nplt.subplot(3,1,3)\ndf['deg_C'].plot(title= 'Temperature in C')\nplt.tight_layout()","ec7dfd79":"#  Preserving the original df\ndf_original=df","3d045ccf":"# extracting features using datetime\ndef datetime2features(df):\n    time_col = \"date_time\"\n    df[\"year\"] = df[time_col].dt.year\n    df[\"month\"] = df[time_col].dt.month\n    df[\"day\"] = df[time_col].dt.day\n    df[\"hour\"] = df[time_col].dt.hour\n    df[\"dayofweek\"] = df[time_col].dt.dayofweek\n    df[\"year\"] = df[time_col].dt.year\n    df['weekend'] = df[time_col].dt.dayofweek.apply(lambda x: 1 if (x>4)  else 0)","e36c776b":"'''\n    which phase of day the time denotes [morning, afternoon, evening, night] \n'''\ndef time_phase(df):\n    def which_phase(hour):\n        if hour >= 0 and hour <= 5:\n            return 1\n        elif hour >=6 and hour <= 11:\n            return 2\n        elif hour >=12  and hour <= 17:\n            return 3\n        elif hour >=18 and hour <= 23:\n            return 4\n        return NaN \n    time_col = \"date_time\"\n    df[\"phase\"] = df[time_col].dt.hour.apply(lambda x : which_phase(x))","92ff8aac":"'''\n    which season of year the time denotes [summer, rainy, winter] \n'''\ndef season(df):\n    def which_season(month):\n        if month >= 3 and month <= 6:\n            return 1\n        elif month >= 7 and month <= 9:\n            return 2\n        elif month >= 10  and month <= 12:\n            return 3\n        elif month < 3:\n            return 3\n        return NaN\n    time_col = \"date_time\"\n    df[\"season\"] = df[time_col].dt.month.apply(lambda x : which_season(x))","c2f0351b":"'''\nratio between absolutr_humidity and relative humidity\n'''\ndef SMC(df):\n    df['SMC'] = (df['absolute_humidity'] * 100) \/ df['relative_humidity']\n\n\n","90301d5e":"'''\nratio between relative humidity and temperature\n'''\ndef ratio_rh_temp(df):\n    df[\"r_rh_temp\"] = df[\"relative_humidity\"]\/(df[\"deg_C\"]+1e-9)","f8954a85":"'''\nDew Point\n'''\ndef Dew_Point(df):\n    df['Dew_Point'] = 243.12*(np.log(df['relative_humidity'] * 0.01) + (17.62 * df['deg_C'])\/(243.12+df['deg_C']))\/(17.62-(np.log(df['relative_humidity'] * 0.01)+17.62*df['deg_C']\/(243.12+df['deg_C'])))","79bbb1d2":"# Doing feature engineering to df's copy\ndff=df.reset_index()\n\ndatetime2features(dff)\ntime_phase(dff)\nseason(dff)\nSMC(dff)\nratio_rh_temp(dff)\nDew_Point(dff)\ndff.head(3)","05657619":"def date_time_encoding(f_name, f_itself, max_val, key):\n    if (key == 'test'):\n        test_data['sin_' + f_name] = np.sin(2 * np.pi * (f_itself\/max_val))\n        test_data['cos_' + f_name] = np.cos(2 * np.pi * (f_itself\/max_val))\n        test_data['tan_' + f_name] = np.tan(2 * np.pi * (f_itself\/max_val))\n        #test_data['sinh_' + f_name]= np.sinh(2 * np.pi * (f_itself\/max_val))\n        #test_data['cosh_' + f_name]= np.cosh(2 * np.pi * (f_itself\/max_val))\n    if (key == 'train'):\n        dff['sin_' + f_name] = np.sin(2 * np.pi * (f_itself\/max_val))\n        dff['cos_' + f_name] = np.cos(2 * np.pi * (f_itself\/max_val))\n        dff['tan_' + f_name] = np.tan(2 * np.pi * (f_itself\/max_val))\n        #X_train['sinh_' + f_name]= np.sinh(2 * np.pi * (f_itself\/max_val))\n        #X_train['cosh_' + f_name]= np.cosh(2 * np.pi * (f_itself\/max_val))\n    return 0\n","893902f9":"date_time = pd.to_datetime(dff['date_time'])\nmonth = pd.DataFrame(date_time.dt.month)\nday  = pd.DataFrame(date_time.dt.day)\nhour = pd.DataFrame(date_time.dt.hour)\n","9e9cf363":"date_time_encoding('time', hour, 24, 'train')\ndate_time_encoding('month', month, 12, 'train')\ndate_time_encoding('day' , day,  31, 'train')","2b3faa82":"plt.figure(figsize=(25,20))\nsns.heatmap(dff.corr(), annot=True);","48502a18":"df1=dff.set_index('date_time')\ndf1=dff.drop(columns={'date_time'}, axis=1)\ndf_benz=df1.drop(columns={'target_carbon_monoxide', 'target_nitrogen_oxides'}, axis=1)\ndf_co=df1.drop(columns={ 'target_benzene', 'target_nitrogen_oxides'}, axis=1)\ndf_no2=df1.drop(columns={'target_carbon_monoxide', 'target_benzene'}, axis=1)\n","391c5930":"# Mutual Information for benzene\n\n\nX = df_benz.copy()\ny = X.pop(\"target_benzene\")\n\n# Label encoding for categoricals\nfor colname in X.select_dtypes(\"object\"):\n    X[colname], _ = X[colname].factorize()\n\n# All discrete features should now have integer dtypes (double-check this before using MI!)\ndiscrete_features = X.dtypes == int\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(X, y, discrete_features)\nms_benz =mi_scores # Created a mi score card for benz\n\n# Mutual Information for nitrogen Oxides\n\n\nX = df_no2.copy()\ny = X.pop(\"target_nitrogen_oxides\")\n\n# Label encoding for categoricals\nfor colname in X.select_dtypes(\"object\"):\n    X[colname], _ = X[colname].factorize()\n\n# All discrete features should now have integer dtypes (double-check this before using MI!)\ndiscrete_features = X.dtypes == int\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(X, y, discrete_features)\nms_no2 =mi_scores # Created a mi score card for no2\n\n\n# Mutual Information for Carbon Monoxide\n\nX = df_co.copy()\ny = X.pop(\"target_carbon_monoxide\")\n\n# Label encoding for categoricals\nfor colname in X.select_dtypes(\"object\"):\n    X[colname], _ = X[colname].factorize()\n\n# All discrete features should now have integer dtypes (double-check this before using MI!)\ndiscrete_features = X.dtypes == int\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(X, y, discrete_features)\nms_co= mi_scores # Created a mi score card for co\n\n","dfe9f11b":"def plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores, color='blue')\n    plt.yticks(width, ticks)\n    plt.grid(True)\n\n\nplt.figure(dpi=100, figsize=(16, 5))\nplt.suptitle('Mutual Information Score after Feature Engineering', fontsize=24)\n\nplt.subplot(1,3,1)\nplot_mi_scores(ms_co)\nplt.title('Carbon Monoxide')\n\nplt.subplot(1,3,2)\nplot_mi_scores(ms_no2)\nplt.title('Nitrogen oxide')\n\nplt.subplot(1,3,3)\nplot_mi_scores(ms_benz)\nplt.title('Benzene')\n\nplt.tight_layout()","6bcd2ef3":"from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, BayesianRidge, LogisticRegression\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cbt\nimport sklearn.metrics as metrics\nimport sklearn.model_selection as ms\nimport pickle","0042b6e2":"# Cross validation utility\nclass CrossValidation:\n    def __init__(self, df, shuffle,random_state=None):\n        self.df = df\n        self.random_state = random_state\n        self.shuffle = shuffle\n        if shuffle is True:\n            self.df = df.sample(frac=1,\n                random_state=self.random_state).reset_index(drop=True)\n        if not shuffle:\n            self.random_state = None\n\n    def hold_out_split(self,percent,stratify=None):\n        if stratify is not None:\n            y = self.df[stratify]\n            train,val = ms.train_test_split(self.df, test_size=percent\/100,\n                stratify=y, random_state=self.random_state)\n            return train,val\n        size = len(self.df) - int(len(self.df)*(percent\/100))\n        train = self.df.iloc[:size,:]\n        val = self.df.iloc[size:,:]\n        return train,val\n\n    def kfold_split(self, splits, stratify=None):\n        if stratify is not None:\n            kf = ms.StratifiedKFold(n_splits=splits, \n                random_state=self.random_state)\n            y = self.df[stratify]\n            for train, val in kf.split(X=self.df,y=y):\n                t = self.df.iloc[train,:]\n                v = self.df.iloc[val, :]\n                yield t,v\n        else:\n            kf = ms.KFold(n_splits=splits, shuffle=self.shuffle,\n                random_state=self.random_state)\n            for train, val in kf.split(X=self.df):\n                t = self.df.iloc[train,:]\n                v = self.df.iloc[val, :]\n                yield t,v","730c4b2d":"# calculate rmsle of predicted data \ndef mse(y_true, y_pred):\n    return metrics.mean_squared_error(y_true, y_pred)","1b71ae40":"folds = 5\nseed = 48","bdfe508b":"features_exclude = [\"date_time\"]\ntargets = ['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']\n","da403d92":"features = [col for col in dff.columns if col not in features_exclude+targets]\nprint(features)","9b409f4a":"cv = CrossValidation(dff, shuffle=True, random_state=seed)","91effa26":"fold_models = {tar:[] for tar in targets}\nprint(fold_models)","0a6132e0":"m_rfg = RandomForestRegressor(n_estimators=100)\nm_gb = GradientBoostingRegressor(n_estimators=100)\nm_lgb = lgb.LGBMRegressor(seed=seed)\nm_ctb = cbt.CatBoostRegressor(random_seed=seed, verbose=False)\nm_xgb = xgb.XGBRegressor(random_state=seed)\n\nlearners = (m_rfg, m_gb ,m_lgb, m_ctb, m_xgb)\n\nmeta_model = BayesianRidge(normalize=True)","45c7a47a":"def train_step(X, Y, evalX, evalY, learners, meta_model, verbose=True):\n    reg = StackingCVRegressor(regressors=learners, \n                            meta_regressor = meta_model,\n                            n_jobs = -1,\n                            verbose = int(verbose)\n                           )\n    trainX = X.values\n    trainY = Y.values\n    \n    model = reg.fit(trainX, trainY)\n    \n    predY_train = model.predict(trainX)\n    \n    train_rmsle = mse(trainY, predY_train)\n    train_r2 = metrics.r2_score(trainY, predY_train)\n    \n    if verbose:\n        print(\"Training mse: \", train_rmsle)\n        print(\"Training r2: \", train_r2)\n\n    valX = evalX.values\n    valY = evalY.values\n    \n    predY_val = model.predict(valX)\n    \n    val_rmsle = mse(valY, predY_val)\n    val_r2 = metrics.r2_score(valY, predY_val)\n    \n    if verbose:\n        print(\"Validation mse: \", val_rmsle)\n        print(\"Validation r2: \", val_r2)\n        \n    return {\"model\": model,\n            \"train_scores\":{\"r2\": train_r2, \"mse\": train_rmsle},\n            \"val_scores\":{\"r2\": val_r2, \"mse\": val_rmsle}\n           }\n","b8340b77":"def train_folds(cv, feature_cols, target_col, num_folds, learners, meta_model,\n                verbose=True):\n    fold_train_rmsle = []\n    fold_train_r2 = []\n    fold_val_rmsle = [] \n    fold_val_r2 = []\n    fold_models = []\n\n    for fold, (train_, val_) in enumerate(cv.kfold_split(splits=num_folds)):\n        result = train_step(X=train_[feature_cols],\n                            Y=train_[target_col],\n                            evalX=val_[feature_cols],\n                            evalY=val_[target_col],\n                            learners=learners,\n                            meta_model=meta_model,\n                            verbose=verbose\n                           )\n        fold_train_rmsle.append(result[\"train_scores\"][\"mse\"])\n        \n        fold_train_r2.append(result[\"train_scores\"][\"r2\"])\n\n        fold_val_rmsle.append(result[\"val_scores\"][\"mse\"])\n        fold_val_r2.append(result[\"val_scores\"][\"r2\"])\n\n        fold_models.append(result[\"model\"])\n        \n    return {\"models\":fold_models,\n            \"train_scores\":{\"r2\": np.mean(fold_train_r2), \"mse\": np.mean(fold_train_rmsle)},\n            \"val_scores\":{\"r2\":np.mean(fold_val_r2), \"mse\":np.mean(fold_val_rmsle)}\n           }\n","fea18381":"target = \"target_carbon_monoxide\"\nresults = train_folds(cv, features, target, folds, learners, meta_model)\nfold_models[target] = results[\"models\"]\n\nprint(\"=\"*50)\nprint(\"Training MSE: \", results[\"train_scores\"][\"mse\"])\nprint(\"Training R2: \", results[\"train_scores\"][\"r2\"])\nprint(\"Validation MSE: \", results[\"val_scores\"][\"mse\"])\nprint(\"Validation R2: \", results[\"val_scores\"][\"r2\"])","bddfb7cc":"target = \"target_benzene\"\nresults = train_folds(cv, features, target, folds, learners, meta_model)\nfold_models[target] = results[\"models\"]\nprint(\"=\"*50)\nprint(\"Training MSE: \", results[\"train_scores\"][\"mse\"])\nprint(\"Training R2: \", results[\"train_scores\"][\"r2\"])\nprint(\"Validation MSE: \", results[\"val_scores\"][\"mse\"])\nprint(\"Validation R2: \", results[\"val_scores\"][\"r2\"])","fcce17d7":"target = \"target_nitrogen_oxides\"\nresults = train_folds(cv, features, target, folds, learners, meta_model)\nfold_models[target] = results[\"models\"]\nprint(\"=\"*50)\nprint(\"Training MSE: \", results[\"train_scores\"][\"mse\"])\nprint(\"Training R2: \", results[\"train_scores\"][\"r2\"])\nprint(\"Validation MSE: \", results[\"val_scores\"][\"mse\"])\nprint(\"Validation R2: \", results[\"val_scores\"][\"r2\"])","bb305d2b":"def get_weights(predictions, targets, apply_softmax=True):\n    def softmax(x):\n        f_x = np.exp(x) \/ np.sum(np.exp(x))\n        return f_x\n    lnr = LinearRegression()\n    lnr_model = lnr.fit(predictions, targets)\n    if apply_softmax:\n        return softmax(lnr_model.coef_)\n    return lnr_model.coef_","7c1b1f88":"def weighted_sum(predictions, weights):\n    return np.dot(predictions, weights)\n","9703419b":"trainX = dff[features].values\ntrainY = dff[targets]","a21ae83b":"predictions = []","321cc33f":"preds = []\nfor model in fold_models[targets[0]]:\n    preds.append(model.predict(trainX))\n\npreds = np.array(preds)\n\nweights_0 = get_weights(preds.transpose(), trainY[targets[0]].values)\nprint(\"Fold Predictions Weightings\")\nprint(weights_0)\n\npreds = weighted_sum(preds.transpose(), weights_0)\npredictions.append(preds)","0831de84":"preds = []\nfor model in fold_models[targets[1]]:\n    preds.append(model.predict(trainX))\n\npreds = np.array(preds)\nweights_1 = get_weights(preds.transpose(), trainY[targets[1]].values)\nprint(\"Fold Predictions Weightings\")\nprint(weights_1)\n\npreds = weighted_sum(preds.transpose(), weights_1)\npredictions.append(preds)","74b76a44":"preds = []\nfor model in fold_models[targets[2]]:\n    preds.append(model.predict(trainX))\n\npreds = np.array(preds)\nweights_2 = get_weights(preds.transpose(), trainY[targets[2]].values)\nprint(\"Fold Predictions Weightings\")\nprint(weights_2)\n\npreds = weighted_sum(preds.transpose(), weights_2)\npredictions.append(preds)","82fefb77":"predictions = np.array(predictions).transpose()\nprint(predictions.shape)\nprint(trainY.shape)","5f6301c5":"predictions = np.where(predictions<0, 0, predictions)","e244a684":"print(\"R2 score: \", metrics.r2_score(trainY, predictions))\nprint(\"RMSLE score: \", np.sqrt(metrics.mean_squared_log_error(trainY, predictions)))","66aab700":"# import test data\ndf_test= pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/test.csv')\ndf_test['date_time'] = pd.to_datetime(df_test[\"date_time\"])\ndf_test.head(2)","d72059b5":"# Feature engineering on test data\n\ndatetime2features(df_test)\ntime_phase(df_test)\nseason(df_test)\nSMC(df_test)\nratio_rh_temp(df_test)\nDew_Point(df_test)\ndff.head(2)","c441ab56":"testX = df_test[features].values","ecb0a00e":"predictions = []","e1018274":"preds = []\nfor model in fold_models[targets[0]]:\n    preds.append(model.predict(testX))\n\npreds = np.array(preds)\n\npreds = weighted_sum(preds.transpose(), weights_0)\npredictions.append(preds)","c8980d4e":"preds = []\nfor model in fold_models[targets[1]]:\n    preds.append(model.predict(testX))\n\npreds = np.array(preds)\n\npreds = weighted_sum(preds.transpose(), weights_1)\npredictions.append(preds)","9acd460b":"preds = []\nfor model in fold_models[targets[2]]:\n    preds.append(model.predict(testX))\n\npreds = np.array(preds)\n\npreds = weighted_sum(preds.transpose(), weights_2)\npredictions.append(preds)","051dd7d4":"predictions = np.array(predictions).transpose()","97e8a1ba":"predictions.shape","f51f7bd6":"predictions = np.where(predictions<0, 0, predictions)","dbf1ddf3":"df_submission=pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/sample_submission.csv')\ndf_submission.head(2)","4c2e430e":"\ndf_submission[targets] = predictions\ndf_submission.to_csv(\"submission.csv\", index=False)\ndf_submission.head()","58c61328":"<a id=\"8\"><\/a>\n<p style = \"font-size : 20px; color : black ; font-family : 'Comic Sans MS'; text-align : center; background-color : #ff6666; border-radius: 5px 5px;\"><strong>Model Creation<\/strong><\/p>","4788f4f1":"### Splitting data into folds for cross validation","6a67030b":"If we remove all rows with Z Score less than -2.5 and greater than 3 we will loss somewhere around 12% of our Data.","3ac1065b":"<a id='20'><\/a>\n<font color = '#F08841'>\n    Content:\n\n1. [Import library and data](#0)    \n1. [Data Processing](#1)\n1. [Data Visualization](#2)\n    *          [Check for Outliers](#3)\n    *          [Checking Relationship Amoung Features](#4)\n    *          [Plotting Columns against Date_Time](#5)\n1.  [Conclusion After EDA](#6)\n1.  [Feature Engineering](#7)\n2.  [Model Creation](#8)\n3.  [Prediction](#9)\n4.  [Submittion](#10)   \n5.  [Glossery](#15)","9479e646":"<p style = \"font-size : 15px; color : black ; font-family : 'Comic Sans MS'; text-align : center; background-color : #ffb84d; border-radius: 5px 5px;\"><strong>Mutual Information<\/strong><\/p>","2ee9816d":"<a id=\"15\"><\/a>\n<p style = \"font-size : 20px; color : black ; font-family : 'Comic Sans MS'; text-align : center; background-color : #ff6666; border-radius: 5px 5px;\"><strong>Glossery<\/strong><\/p>\n# Glossery\n\n* Absolute humidity is the measure of water vapor (moisture) in the air, regardless of temperature. It is expressed as grams of moisture per cubic meter of air (g\/m3).\n\n* Relative humidity also measures water vapor but RELATIVE to the temperature of the air. It is expressed as the amount of water vapor in the air as a percentage of the total amount that could be held at its current temperature.Warm air can hold far more moisture than cold air meaning that the relative humidity of cold air would be far higher than warm air if their absolute humidity levels were equal.Relative humidity is cited in weather forecasts as it affects how we \u201cfeel\u201d temperature.\n\n[top](#20)","710612d9":"### Carbob Monoxide","20b970d5":"<a id=\"5\"><\/a>\n<p style = \"font-size : 15px; color : black ; font-family : 'Comic Sans MS'; text-align : center; background-color : #ffb84d; border-radius: 5px 5px;\"><strong>Plotting Features vs Date_Time<\/strong><\/p>\n","89144abf":"<a id=\"7\"><\/a>\n<p style = \"font-size : 20px; color : black ; font-family : 'Comic Sans MS'; text-align : center; background-color : #ff6666; border-radius: 5px 5px;\"><strong>Feature Engineering<\/strong><\/p>\n\n<p style = \"font-size : 15px; color : black ; font-family : 'Comic Sans MS'; text-align : left; background-color : #ffe0b3; border-radius: 5px 5px;\"><b>Through Mutual Information we have noticed that data and time have some effect on Target value. Thus lets create few features using date and time<\/b> <ul style = \"font-size : 15px; color : black ; font-family : 'Comic Sans MS'; text-align : left; background-color : #ffe0b3; border-radius: 5px 5px;\">\n    <li><b>Season<\/b> (As season have effect on air pollution<\/li>\n    <li><b>Parts of day {Phase}<\/b> ( pollution may increase or decrease depending on part of day, generally in morning and evening pollution due to heavy transportation rises)<\/li>\n    <li><b>Ratio<\/b> <li>between relative humidity and temperature<\/li> <li> between absolute humidity and relative humidity <\/li>\n    ","c3e0e36d":"### Benzene","3531280f":"<a id=\"4\"><\/a>\n<p style = \"font-size : 15px; color : black ; font-family : 'Comic Sans MS'; text-align : center; background-color : #ffb84d; border-radius: 5px 5px;\"><i>Checking for Relation Amoung Features<\/i><\/p>\n","7144bed6":"## Inference on test dataset","1bdb1314":"<a id=\"1\"><\/a>\n<p style = \"font-size : 20px; color : black ; font-family : 'Comic Sans MS'; text-align : center; background-color : #ff6666; border-radius: 5px 5px;\"><strong> Data Processing<\/strong><\/p>\n","9b4cd553":"# Reference \n[Feature Engineering Idea Collection](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jul-2021\/discussion\/250931)\n[one of the main note book for which above collection borrowed the idea](https:\/\/www.kaggle.com\/alexryzhkov\/tps-lightautoml-baseline-with-pseudolabels)","1ea51636":"<a id=\"6\"><\/a>\n<p style = \"font-size : 20px; color : black ; font-family : 'Comic Sans MS'; text-align : center; background-color : #ff6666; border-radius: 5px 5px;\"><strong>Conclusion of EDA <\/strong><\/p>\n\t\n<p><ul style = \"font-size : 15px; color : black ; font-family : 'Comic Sans MS'; text-align : left; background-color : #ffe0b3; border-radius: 5px 5px;\"><li>There is around 10% outliers in the data frame<\/li><li>Sensor1,2 & 5 are +vely correlated with all 3 Target Features<\/li><li>While Sensor 3 -vely correlated with all 3 Target Features<\/li><li>Mutual Information of <b>Target Benzene<\/b> sensor_2 :1.998199 sensor_3 : 0.909053\nsensor_5 :            0.834663\nsensor_1 :            0.778104\nsensor_4 :            0.673297<\/li>\n    <li>Mutual Information of <b>Target CO<\/b> sensor_2             1.040071\nsensor_1             0.745881\nsensor_5             0.683925\nsensor_3             0.616273\nsensor_4             0.386446<\/li>\n    <li> Mutual Information of <b>Target NO2<\/b> sensor_2             0.551770\nDate                 0.533851\nsensor_5             0.529259\nsensor_3             0.486198\nsensor_1             0.374835\nTime                 0.193417\nsensor_4             0.190438\n    <\/ul><\/p>\n","94eddc97":"<a id=\"3\"><\/a>\n<p style = \"font-size : 15px; color : black ; font-family : 'Comic Sans MS'; text-align : center; background-color : #ffb84d; border-radius: 5px 5px;\"><i>Checking for outliers<\/i><\/p>\n","8c50de58":"<a id=\"0\"><\/a>\n<p style = \"font-size : 20px; color : black ; font-family : 'Comic Sans MS'; text-align : center; background-color : #ff6666; border-radius: 5px 5px;\"><strong>Importing Required Libraries and Data <\/strong><\/p>\n","417c4cbd":"<a id=\"2\"><\/a>\n<p style = \"font-size : 20px; color : black ; font-family : 'Comic Sans MS'; text-align : center; background-color : #ff6666; border-radius: 5px 5px;\"><strong>Data Visualization<\/strong><\/p>\n","c296cd50":"## Checking correlation and mi for new df","274548f9":"<a id=\"15\"><\/a>\n<p style = \"font-size : 20px; color : black ; font-family : 'Comic Sans MS'; text-align : center; background-color : #ff6666; border-radius: 5px 5px;\"><strong>Submission<\/strong><\/p>","522cd7ac":"### Nitrogen Oxide","191b07b1":"# Training","4397eedf":"* First thing which we observe is it has hourly data if air quality. which we might need to parse","a3438ee4":"### Prediction blending from folds"}}