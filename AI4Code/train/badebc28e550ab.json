{"cell_type":{"6bcbe38b":"code","902eedf1":"code","d67b1254":"code","1e50c538":"code","be111cda":"code","2291ec45":"code","03e2d6fa":"code","bc1f85d7":"code","9e7d4802":"code","a78cfc95":"code","2acbc2b6":"code","eb008d40":"code","02b44060":"code","4bb44eda":"code","486eb94c":"code","63d4fff5":"code","2bf6daed":"code","17797f28":"code","9c9e8196":"code","d8961c6a":"code","380ba426":"code","bc84e351":"code","644340b8":"code","314c7987":"markdown","d5a82330":"markdown","92ae6d5b":"markdown","95a6253a":"markdown","221f9d5c":"markdown","86ea73bd":"markdown","93533dbe":"markdown","2a769326":"markdown","3f665778":"markdown","598c00e2":"markdown","feab6af9":"markdown","89ca6ffe":"markdown","9905aed0":"markdown","4d1ee0cf":"markdown","f47d75dd":"markdown","7edc9594":"markdown","41f667a7":"markdown","088b56a4":"markdown","2baf2a15":"markdown"},"source":{"6bcbe38b":"# This kernel borrows heavily from https:\/\/www.kaggle.com\/nanji200\/titanic-neural-networks-keras-81-8-e46e71\/edit\n# linear algebra\nimport numpy as np \n\n# data processing\nimport pandas as pd \n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Data processing, metrics and modeling\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n\n# Model evaluations\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\npd.set_option('display.max_colwidth', 0)\n\nhypertuning = 0","902eedf1":"test_df = pd.read_csv(\"..\/input\/test.csv\")\ntrain_df = pd.read_csv(\"..\/input\/train.csv\")","d67b1254":"def exploreData(df,dfName):\n    #Set display\n    #pd.options.display.max_columns=15\n    #pd.options.display.max_rows=892\n\n    print('SNAPSHOT OF '+ dfName)\n    df.info()\n    print('\\n')\n\n    print('BASIC DESCRIPTION')\n    print(df.describe())\n    print('\\n')\n\n    print('SNAPSHOT OF FIRST 8 RECORDS')\n    print(df.head(8))\n    print('\\n')\n\n    # List missing values\n    print('MISSING VALUE SUMMARY')\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent_1 = df.isnull().sum()\/df.isnull().count()*100\n    percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n    print(missing_data.head(5))   ","1e50c538":" # Describe features\ndes='''DESCRIPTION OF FEATURES:\nsurvival:    Survival \nPassengerId: Unique Id of a passenger. \npclass:    Ticket class     \nsex:    Sex     \nAge:    Age in years     \nsibsp:    # of siblings \/ spouses aboard the Titanic     \nparch:    # of parents \/ children aboard the Titanic     \nticket:    Ticket number     \nfare:    Passenger fare     \ncabin:    Cabin number     \nembarked:    Port of Embarkation'''\nprint(des)","be111cda":"exploreData(train_df,'TRAIN Dataset')","2291ec45":"exploreData(test_df,'TEST Dataset')","03e2d6fa":"import re\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\ngenders = {\"male\": 0, \"female\": 1}\nports = {\"S\": 0, \"C\": 1, \"Q\": 2}\ndata = [train_df, test_df]\n\nfor dataset in data:\n#add new column \"relatives\"\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']   \n#Since the Embarked feature has only 2 missing values, we will just fill these with the most common one\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n#Create new column IsThereCabinData\n    dataset.loc[dataset['Cabin'].isna(), 'IsThereCabinData'] = 0\n    dataset.loc[dataset['Cabin'].notna(), 'IsThereCabinData'] = 1\n    dataset['IsThereCabinData'] = dataset['IsThereCabinData'].astype(int)\n#create new column \"deck\"\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset['Deck'] = dataset['Deck'].fillna('U')\n# Title\n    dataset['Title'] = dataset['Name']\n# Cleaning name and extracting Title\n    for name_string in dataset['Name']:\n        dataset['Title'] = dataset['Name'].str.extract('([A-Za-z]+)\\.', expand=True)    \n# Replacing rare titles \n    mapping = {'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs', 'Major': 'Other', \n               'Col': 'Other', 'Dr' : 'Other', 'Rev' : 'Other', 'Capt': 'Other', \n               'Jonkheer': 'Royal', 'Sir': 'Royal', 'Lady': 'Royal', \n               'Don': 'Royal', 'Countess': 'Royal', 'Dona': 'Royal'}           \n    dataset.replace({'Title': mapping}, inplace=True)\n    titles = ['Miss', 'Mr', 'Mrs', 'Royal', 'Other', 'Master']\n# Replacing missing age by median\/title \n    for title in titles:\n        age_to_impute = dataset.groupby('Title')['Age'].median()[titles.index(title)]\n        dataset.loc[(dataset['Age'].isnull()) & (dataset['Title'] == title), 'Age'] = age_to_impute    \n# New feature : Family_size\n    dataset['Family_Size'] = dataset['Parch'] + dataset['SibSp'] + 1\n    dataset.loc[:,'FsizeD'] = 'Alone'\n    dataset.loc[(dataset['Family_Size'] > 1),'FsizeD'] = 'Small'\n    dataset.loc[(dataset['Family_Size'] > 4),'FsizeD'] = 'Big'\n# Replacing missing Fare by median\/Pclass \n    fa = dataset[dataset[\"Pclass\"] == 3]\n    dataset['Fare'].fillna(fa['Fare'].median(), inplace = True)\n#  New feature : Child\n    dataset.loc[:,'Child'] = 1\n    dataset.loc[(dataset['Age'] >= 18),'Child'] =0","bc1f85d7":"#drop columns\ncol_list = ['Cabin','Name','Parch', 'SibSp','Ticket', 'Family_Size', 'Embarked', 'Deck']\ntrain_df = train_df.drop(columns = col_list)\ntest_df = test_df.drop(columns = col_list)","9e7d4802":"le = LabelEncoder()\ndata = [train_df, test_df]\nfor dataset in data:\n    # Binary columns with 2 values like Sex\n    bin_cols = dataset.nunique()[dataset.nunique() == 2].keys().tolist()\n    # Multi value columns with 3 to 11 values. i.e columns with category data like Pclass\n    multi_cols = dataset.nunique()[(dataset.nunique() > 2) & (dataset.nunique() < 12)].keys().tolist()  \n    # numerical columns with > 11 values like Fare, Age etc.\n    num_cols = dataset.nunique()[dataset.nunique() > 11].keys().tolist() \n    \nstd = StandardScaler()\n\n# TRAIN\n# Binary columns - perform label encoding\nfor i in bin_cols :\n    train_df[i] = le.fit_transform(train_df[i])\n# Multi value columns - perform one hot encoding\ntrain_df = pd.get_dummies(train_df,columns = multi_cols)  \n# Numerical columns - perfrom scaling\/normalization\nscaled = std.fit_transform(train_df[num_cols])\nscaled = pd.DataFrame(scaled,columns = num_cols)\n# dropping original values merging scaled values for numerical columns\ntrain_df = train_df.drop(columns = num_cols,axis = 1)\ntrain_df = train_df.merge(scaled,left_index = True,right_index = True,how = \"left\")\ntrain_df = train_df.drop(columns = ['PassengerId'],axis = 1)\n\n# TEST\n# Binary columns - perform label encoding\nfor i in bin_cols :\n    test_df[i] = le.fit_transform(test_df[i])\n# Multi value columns - perform one hot encoding\ntest_df = pd.get_dummies(test_df,columns = multi_cols)   \n# Numerical columns - perfrom scaling\/normalization\nscaled = std.fit_transform(test_df[num_cols])\nscaled = pd.DataFrame(scaled,columns = num_cols)\n# dropping original values merging scaled values for numerical columns\ntest_df = test_df.drop(columns = num_cols,axis = 1)\ntest_df = test_df.merge(scaled,left_index = True,right_index = True,how = \"left\")\ntest_df = test_df.drop(columns = ['PassengerId'],axis = 1)","a78cfc95":"# Deck_T feature is in train but not in test, let us remove it from train\n#train_df = train_df.drop(columns = ['Deck_T'],axis = 1)\n# X and Y\nX_train = train_df.iloc[:, 1:39].as_matrix()\ny_train = train_df.iloc[:,0].as_matrix()","2acbc2b6":"# baseline model\ndef create_baseline():\n    # create model\n    model = Sequential()\n    model.add(Dense(13, input_dim = 26, activation = 'relu')) # 26: number of input columns in train\/test\n    model.add(Dropout(0.2))\n    model.add(Dense(8, activation = 'relu'))\n    model.add(Dense(1, activation = 'sigmoid'))\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n    return model\n\nestimator = KerasClassifier(build_fn = create_baseline, epochs = 20, batch_size = 10, verbose = 1)\nkfold = StratifiedKFold(n_splits = 5, random_state = 42, shuffle = False)\nresults = cross_val_score(estimator, X_train, y_train, cv = kfold)\nprint(\"Results: Mean score: %.2f%% (STD score: %.2f%%)\" % (results.mean()*100, results.std()*100))","eb008d40":"# define the grid search parameters\nbatch_size = [20, 40, 60, 80]\nepochs = [20, 50, 100, 150, 200]\nparam_grid = dict(batch_size=batch_size, epochs=epochs)\ngrid = GridSearchCV(estimator=estimator, param_grid=param_grid, n_jobs=-1)\ngrid_result = grid.fit(X_train, y_train)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","02b44060":"# X Test\nX_test = test_df.as_matrix()\n#estimator.fit(X_train, y_train, epochs = 20, batch_size = 10)\nestimator.fit(X_train, y_train, epochs = 150, batch_size = 40)\nscore = estimator.model.evaluate(X_train, y_train, batch_size=40)\nprint(\"Score=\", score)\n# Predicting y_test\nprediction = estimator.predict(X_test).tolist()","4bb44eda":"se = pd.Series(prediction)\n# Creating new column of predictions in data_check dataframe\ndata_check =  pd.read_csv(\"..\/input\/test.csv\")\ndata_check['check'] = se\ndata_check['check'] = data_check['check'].str.get(0)\nseries = []\nfor val in data_check.check:\n    if val >= 0.5:\n        series.append(1)\n    else:\n        series.append(0)\ndata_check['final'] = series\ntemp = pd.DataFrame(pd.read_csv(\"..\/input\/test.csv\")['PassengerId'])\ntemp['Survived'] = data_check['final']\ntemp.to_csv(\"submission.csv\", index = False)","486eb94c":"train_df.columns.values","63d4fff5":"survived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nwomen = train_df[train_df['Sex']=='female']\nmen = train_df[train_df['Sex']=='male']\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\nax.legend()\n_ = ax.set_title('Male')","2bf6daed":"FacetGrid = sns.FacetGrid(train_df, row='Embarked', size=4.5, aspect=1.6)\nFacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )\nFacetGrid.add_legend()","17797f28":"sns.barplot(x='Pclass', y='Survived', data=train_df)","9c9e8196":"grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","d8961c6a":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n    dataset['not_alone'] = dataset['not_alone'].astype(int)","380ba426":"train_df","bc84e351":"train_df['not_alone'].value_counts()","644340b8":"axes = sns.factorplot('relatives','Survived', \n                      data=train_df, aspect = 2.5, )","314c7987":"Here we can see that you had a high probabilty of survival with 1 to 3 realitves, but a lower one if you had less than 1 or more than 3 (except for some cases with 6 relatives).","d5a82330":"The plot above confirms our assumption about pclass 1, but we can also spot a high probability that a person in pclass 3 will not survive.","92ae6d5b":"# **More detailed data analysis**\nFrom the table above, we can note a few things. First of all, that we **need to convert a lot of features into numeric** ones later on, so that the machine learning algorithms can process them. Furthermore, we can see that the **features have widely different ranges**, that we will need to convert into roughly the same scale. We can also spot some more features, that contain missing values (NaN = not a number), that wee need to deal with.\n\n","95a6253a":"# **Data Exploration\/Analysis**","221f9d5c":"**5.  SibSp and Parch:**\n\nSibSp and Parch would make more sense as a combined feature, that shows the total number of relatives, a person has on the Titanic. I will create it below and also a feature that sows if someone is not alone.","86ea73bd":"# **Getting the Data**","93533dbe":"You can see that men have a high probability of survival when they are between 18 and 30 years old, which is also a little bit true for women but not fully. For women the survival chances are higher between 14 and 40.\n\nFor men the probability of survival is very low between the age of 5 and 18, but that isn't true for women. Another thing to note is that infants also have a little bit higher probability of survival.\n\nSince there seem to be **certain ages, which have increased odds of survival** and because I want every feature to be roughly on the same scale, I will create age groups later on.","2a769326":"Here we see clearly, that Pclass is contributing to a persons chance of survival, especially if this person is in class 1. We will create another pclass plot below.","3f665778":"The Embarked feature has only 2 missing values, which can easily be filled. It will be much more tricky, to deal with the 'Age' feature, which has 177 missing values. The 'Cabin' feature needs further investigation, but it looks like that we might want to drop it from the dataset, since 77 % of it are missing.","598c00e2":"# **Hypertuning**","feab6af9":"# **Prepare data**","89ca6ffe":"Embarked seems to be correlated with survival, depending on the gender. \n\nWomen on port Q and on port S have a higher chance of survival. The inverse is true, if they are at port C. Men have a high survival probability if they are on port C, but a low probability if they are on port Q or S. \n\nPclass also seems to be correlated with survival. We will generate another plot of it below.","9905aed0":"**3. Embarked, Pclass  and Sex:**","4d1ee0cf":"**4. Pclass:**","f47d75dd":"**1. Age and Sex:**","7edc9594":"Above you can see the 11 features + the target variable (survived). **What features could contribute to a high survival rate ?** \n\nTo me it would make sense if everything except 'PassengerId', 'Ticket' and 'Name'  would be correlated with a high survival rate. ","41f667a7":"# **END**","088b56a4":"# **Summary**\n\nThis project deepened my machine learning knowledge significantly and I strengthened my ability to apply concepts that I learned from textbooks, blogs and various other sources, on a different type of problem. This project had a heavy focus on the data preparation part, since this is what data scientists work on most of their time. \n\nI started with the data exploration where I got a feeling for the dataset, checked about missing data and learned which features are important. During this process I used seaborn and matplotlib to do the visualizations. During the data preprocessing part, I computed missing values, converted features into numeric ones, grouped values into categories and created a few new features. Afterwards I started training 8 different machine learning models, picked one of them (random forest) and applied cross validation on it. Then I explained how random forest works, took a look at the importance it assigns to the different features and tuned it's performace through optimizing it's hyperparameter values.  Lastly I took a look at it's confusion matrix and computed the models precision, recall and f-score, before submitting my predictions on the test-set to the Kaggle leaderboard.\n\nBelow you can see a before and after picture of the train_df dataframe:\n\n![Titanic](https:\/\/img1.picload.org\/image\/dagldoor\/before_after.png)\n\n\nOf course there is still room for improvement, like doing a more extensive feature engineering, by comparing and plotting the features against each other and identifying and removing the noisy features. Another thing that can improve the overall result on the kaggle leaderboard would be a more extensive hyperparameter tuning on several machine learning models. Of course you could also do some ensemble learning.","2baf2a15":"# ****I have moved more detailed data analysis to the bottom. Didn't want to scroll up and down :)****"}}