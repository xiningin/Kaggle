{"cell_type":{"305d212d":"code","eb9937ae":"code","9b1731d1":"code","591bf370":"code","0768e593":"code","27858319":"code","0e54d45f":"code","03669ea0":"code","fc916806":"code","989f7016":"code","7597dca7":"code","ebcd3444":"code","4368a058":"code","88f745d6":"code","7edb9bf7":"code","ebf5efdd":"code","a57a1555":"code","7ef446a1":"markdown","e3d4dbc0":"markdown","9e8e6a43":"markdown","6baec374":"markdown","a01af126":"markdown","6af715e3":"markdown","32547aca":"markdown","647db4a6":"markdown"},"source":{"305d212d":"#LOADING MODULES\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style(\"whitegrid\")\nsns.set_palette(\"deep\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eb9937ae":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","9b1731d1":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","591bf370":"submission = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")","0768e593":"women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint(\"% of women who survived:\", rate_women)","27858319":"men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"% of men who survived:\", rate_men)","0e54d45f":"from sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","03669ea0":"train_data.info() ","fc916806":"tmp = train_data.drop(columns=\"Cabin\").drop(index=train_data.loc[train_data[\"Embarked\"].isna()].index) # i delete the uninsightful columns and indices temporarily\nmean_tmp = tmp[[\"Pclass\", \"Sex\", \"Survived\"]].groupby([\"Pclass\", \"Sex\"], as_index=False).mean()\n\nfig = plt.figure(figsize=(20, 15))\nsubfig = fig.subfigures(2, 1)\n\ng1 = sns.FacetGrid(mean_tmp, col=\"Pclass\", aspect=1, height=4)\ng1.map(sns.barplot, \"Sex\", \"Survived\", order=[\"male\", \"female\"])\n\ng2 = sns.FacetGrid(tmp, col=\"Pclass\", aspect=1, height=4)\ng2.map(sns.countplot, \"Sex\", order=[\"male\", \"female\"])\n\nsubfig[0] = g1.figure\nsubfig[0].suptitle(\"Count and Ratio of Survivers\", y=1.1, fontsize=16)\nsubfig[1] = g2.figure","989f7016":"tmp = train_data.drop(columns=\"Cabin\").drop(index=train_data.loc[train_data[\"Embarked\"].isna()].index) # i delete the uninsightful columns and indices temporarily\n\nfig, (ax1, ax2) =plt.subplots(2, 1, figsize=(20,10))\n\nsns.kdeplot(data=tmp, x=\"Age\", shade=True, hue=\"Sex\", ax=ax1)\nsns.kdeplot(data=tmp, x=\"Age\", shade=True, hue=\"Survived\", ax=ax2)","7597dca7":"tmp_mean = tmp[[\"Sex\", \"Age\", \"Survived\"]].groupby([\"Age\", \"Sex\"], as_index=False).mean()\nsns.lmplot(data=tmp_mean, x=\"Age\", y=\"Survived\", hue=\"Sex\", aspect=6, height=4)","ebcd3444":"from sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom sklearn.linear_model import LinearRegression\n\ndrop_labels = [\"PassengerId\", \"Cabin\", \"Ticket\", \"Name\"]\nclass FullPrep(TransformerMixin, BaseEstimator):\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return X.drop(columns=drop_labels)\n\ndrop_na_labels = list(train_data) # I know that this isn't nececary\nnum_labels = [\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n\nnum_pipeline = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"median\"))\n])\n\nembarked_pipeline = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"cat_enc\", OneHotEncoder())\n])\n\ncolumn_t = ColumnTransformer([\n    (\"num\", num_pipeline, num_labels),\n    (\"embarked\", embarked_pipeline, [\"Embarked\"]),\n    (\"sex\", OrdinalEncoder(), [\"Sex\"])\n])\n\nfull_pipeline = Pipeline([\n    (\"dropna\", FullPrep()),\n    (\"columns\", column_t),\n])\n\ntrain_X, test_X = full_pipeline.fit_transform(train_data.drop(columns=\"Survived\")), full_pipeline.fit_transform(test_data)\ntrain_y = train_data[\"Survived\"]\ntrain_X[0], train_y[0]","4368a058":"from sklearn.model_selection import cross_val_score\n\nlin_reg = LinearRegression()\nresult = cross_val_score(lin_reg, train_X, train_y, cv=10)\nresult.mean()","88f745d6":"from sklearn.ensemble import RandomForestClassifier\n\nreg = RandomForestClassifier(n_estimators=100)\nscore = cross_val_score(reg, train_X, train_y, cv=10)\nscore.mean()","7edb9bf7":"# Final training\nreg.fit(train_X, train_y)","ebf5efdd":"submission = pd.DataFrame({\n    \"PassengerId\": test_data[\"PassengerId\"],\n    \"Survived\": reg.predict(test_X)\n})\nsubmission","a57a1555":"submission.to_csv(\"submission.csv\", index=False)","7ef446a1":"Loading datasets\nreading train and test datasets","e3d4dbc0":"Obviously the classifier works much better. At this point I could test some other models, but not for now.","9e8e6a43":"Looking into the training dataset\nPrinting first 5 rows of the train dataset.","6baec374":"Below is a brief information about each columns of the dataset:\n\nPassengerId: An unique index for passenger rows. It starts from 1 for first row and increments by 1 for every new rows.\n\nSurvived: Shows if the passenger survived or not. 1 stands for survived and 0 stands for not survived.\n\nPclass: Ticket class. 1 stands for First class ticket. 2 stands for Second class ticket. 3 stands for Third class ticket.\n\nName: Passenger's name. Name also contain title. \"Mr\" for man. \"Mrs\" for woman. \"Miss\" for girl. \"Master\" for boy.\n\nSex: Passenger's sex. It's either Male or Female.\n\nAge: Passenger's age. \"NaN\" values in this column indicates that the age of that particular passenger has not been recorded.\n\nSibSp: Number of siblings or spouses travelling with each passenger.\n\nParch: Number of parents of children travelling with each passenger.\nTicket: Ticket number.\nFare: How much money the passenger has paid for the travel journey.\nCabin: Cabin number of the passenger. \"NaN\" values in this column indicates that the cabin number of that particular passenger has not been recorded.\nEmbarked: Port from where the particular passenger was embarked\/boarded.\n\n\nTotal rows and columns\n\nWe can see that there are 891 rows and 12 columns in our training dataset.","a01af126":"The last plot shows a linear correlation between age, sex and probability of survival. As you can see, not only were women much more likely to survive but the age didn't play as big of a role as for male passengers.\n\nAs you can see, the Cabin attribute is too incomplete as to being useful. Which is unfortunate, because the attribute might have given some insight. One possibility would be to transform the cabins into areas and then derive the area from the ticket price for the missing values. And I could look at surnames of the passengers to find the missing Embarked values. But I decided not to bother. The missing values in Age can be easily replaced by the sklearn SimpleImputer class with strategy=\"median\". The PassengerID doesn't have any meaning too. And some Columns are dropped for simplicity. Sex is converted to a binary numeric attribute.","6af715e3":"There is a strong correlation between the passengerclass, the sex and the rate of survival. In the following we will look at the correlation between the age and survival rate.","32547aca":"First I tried LinearRegression but then I noticed that I need a classifier.","647db4a6":"These plots show, that children are much more likely to survive than middle-aged people. The first plot is just for basic visualization of the age distribtion on the titanic."}}