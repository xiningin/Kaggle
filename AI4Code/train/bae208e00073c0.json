{"cell_type":{"978a5d56":"code","b18c77d1":"code","1b147668":"code","b1560eb9":"code","38acb723":"code","2bc895e9":"code","d6d9f7ae":"code","7036cdee":"code","89a04a9f":"markdown","ffdb4db0":"markdown","cd61663c":"markdown","fbc49beb":"markdown","3a3b0d09":"markdown","7cf93656":"markdown","cba6a4dc":"markdown","69daff6a":"markdown","09bf48ee":"markdown","c14e445d":"markdown"},"source":{"978a5d56":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nimport tensorflow as tf\nimport random\nimport os\n\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\n\nSEED=42\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n    \ntrain = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv')\n","b18c77d1":"from sklearn.preprocessing import MinMaxScaler\n\nlabel_map = {\n    'Class_1' : 0,\n    'Class_2' : 1,\n    'Class_3' : 2,\n    'Class_4' : 3,\n}\ntrain['target'] = train['target'].map(label_map)","1b147668":"features = ['feature_{}'.format(x) for x in range(50)]\nqt = train[features].quantile(np.arange(0,1,0.002))\n\ndef clip(df):\n    df = df.copy()\n    for feature in features:\n        df[feature] = df[feature].clip(lower=0, upper=qt.loc[0.998][feature])\n    return df\n","b1560eb9":"values=[]\nlabels=[0,1,2,3,]\nfor feature in features:\n    grouped = clip(train).groupby(feature)\n    for value, group in grouped:\n        value=[feature, value]\n        for label in labels:\n            p =  (group['target'] == label).mean()\n            p = np.clip(p, 1e-06, 1 - 1e-06)\n            value.append(np.log(p+0.5))\n            value.append(np.log(p\/(1-p)))\n        values.append(value)\ndf_proba = pd.DataFrame(values,\n                        columns=['feature', 'value',\n                                 'Class_1_proba1',\n                                 'Class_1_proba2',\n                                 'Class_2_proba1',\n                                 'Class_2_proba2',\n                                 'Class_3_proba1',\n                                 'Class_3_proba2',\n                                 'Class_4_proba1',\n                                 'Class_4_proba2',\n                                ])\nproba_dict_1={}\nproba_dict_2={}\n\nfor i in range(len(df_proba)):\n    feature = df_proba.iloc[i]['feature']\n    value = df_proba.iloc[i]['value']\n    proba_dict_1[feature, value] = df_proba.iloc[i][['Class_1_proba1','Class_2_proba1','Class_3_proba1','Class_4_proba1',]].values.astype(float)\n    proba_dict_2[feature, value] = df_proba.iloc[i][['Class_1_proba2','Class_2_proba2','Class_3_proba2','Class_4_proba2',]].values.astype(float)\n    ","38acb723":"from sklearn.base import TransformerMixin\n\ndef reshape(df):\n    values=[]\n    for value in df.values:\n        values.append([_ for _ in value])\n    return np.array(values)\n\nclass MyTransformer1(TransformerMixin):\n    def fit_transform(self, X, y=None,**fit_params):\n        return self.transform(X)\n    def transform(self, X):\n        newX = pd.DataFrame()\n        for feature in features:\n            newX[feature] = X[feature].clip(lower=qt.loc[0.002][feature], upper=qt.loc[0.998][feature])\n            newX[feature] = 1 \/ (newX[feature] - newX[feature].min() + 1)\n        return newX\n\nclass MyTransformer2(TransformerMixin):\n    def fit_transform(self, X, y=None,**fit_params):\n        return self.transform(X)\n    def transform(self, X):\n        newX = pd.DataFrame()\n        X = clip(X)\n        for feature in features:\n            newX[feature] = X[feature].apply(lambda x:proba_dict_1[feature, x])\n        return reshape(newX).reshape((-1,200))\n\nclass MyTransformer3(MyTransformer2):\n    def fit_transform(self, X, y=None,**fit_params):\n        return self.transform(X)\n    def transform(self, X):\n        newX = pd.DataFrame()\n        X = clip(X)\n        for feature in features:\n            newX[feature] = X[feature].apply(lambda x:proba_dict_1[feature, x])\n        return reshape(newX)\n\ndef normalize(df, columns):\n    for column in columns:\n        min_val, max_val = df[column].agg([min,max])\n        df[column] = (df[column] - min_val) \/ (max_val - min_val)\n    return df\n\nclass MyTransformer4(TransformerMixin):\n    def fit_transform(self, X, y=None,**fit_params):\n        return self.transform(X)\n    def transform(self, X):\n        newX = pd.DataFrame()\n        for feature in features:\n            newX[feature] = X[feature].clip(lower=0).apply(lambda x:1\/(x+1))\n        return normalize(newX, features)\n","2bc895e9":"from sklearn.base import ClassifierMixin\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.pipeline import make_pipeline\n\nmy_model1 = CatBoostClassifier(\n    iterations=887,\n    min_child_samples=200,\n    random_state=SEED,\n    max_depth=6,\n    verbose=0)\n\nmy_model2 = CatBoostClassifier(\n    iterations=160,\n    min_child_samples=200,\n    max_depth=2,\n    eval_metric='MultiClass',\n    random_state=SEED,\n    verbose=0)\n\ninitial_learning_rate = 0.001\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate,\n    decay_steps=10000,\n    decay_rate=0.96,\n    staircase=True)\noptimizer = tf.keras.optimizers.Adam(\n    learning_rate=lr_schedule, \n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    name='Adam'\n)\nclass TensorflowClassifier(ClassifierMixin):\n    def __init__(self):\n        self.histories=[]\n        self.classes_ = [0,1,2,3]\n        self.model = tf.keras.Sequential([\n            tf.keras.layers.Flatten(input_shape=(50,4)),\n            tf.keras.layers.Dense(64, activation='relu'),\n            tf.keras.layers.Dense(64, activation='relu'),\n            tf.keras.layers.Dense(32, activation='relu'),\n            tf.keras.layers.Dense(4, activation='softmax')\n        ])\n        self.model.compile(\n            optimizer=optimizer, \n            loss='sparse_categorical_crossentropy',\n            metrics=['sparse_categorical_crossentropy',])\n    def get_params(self,deep):\n        return {}\n    def fit(self, X, y):\n        callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n        history = self.model.fit(X, y, epochs=300, batch_size = 200, validation_split=0.1, callbacks=[callback],verbose=0)\n        self.histories.append(history)\n        return self\n    def predict_proba(self, X):\n        return self.model.predict(X).reshape((-1, 4))\n\nmy_model3 = TensorflowClassifier()\n\nmy_model4 = LGBMClassifier(\n    random_state=SEED,\n    min_child_samples=150,\n    n_estimators=74,\n)\n\npipeline1 = make_pipeline(MyTransformer1(),my_model1)\npipeline2 = make_pipeline(MyTransformer2(),my_model2)\npipeline3 = make_pipeline(MyTransformer3(),my_model3)\npipeline4 = make_pipeline(MyTransformer4(),my_model4)\n","d6d9f7ae":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss,accuracy_score\n\nestimators = [\n    ('mod1', pipeline1),\n    ('mod2', pipeline2),\n    ('mod3', pipeline3),\n    ('mod4', pipeline4),\n]\n\nskf = StratifiedKFold(shuffle=True, random_state=SEED)\n\nX = train[features]\ny = train['target']\nscores=[]\nfor train_index, valid_index in skf.split(X, y):\n    X_train, X_val = X.iloc[train_index],X.iloc[valid_index]\n    y_train, y_val = y.iloc[train_index],y.iloc[valid_index]\n    mod = VotingClassifier(\n        estimators=estimators,\n        voting = 'soft',\n    ).fit(X_train, y_train)\n    y_pred = mod.predict_proba(X_val)\n    print('log_loss={}, accuracy={}'.format(log_loss(y_val, y_pred),\n                                            accuracy_score(y_val, np.argmax(y_pred,axis=1))))\n    scores.append([mod, log_loss(y_val, y_pred),accuracy_score(y_val, np.argmax(y_pred,axis=1))])\nscores = pd.DataFrame(scores, columns=['mod','log_loss','accuracy'])\nprint(scores.describe())","7036cdee":"y_pred_test = np.zeros((len(test), 4))\n\nfor mod in scores['mod']:\n    y_pred_test += mod.predict_proba(test[features])\n\ny_pred_test \/= len(scores)\n\nsubmission = test[['id']].copy()\nsubmission['Class_1'] = y_pred_test[:,0]\nsubmission['Class_2'] = y_pred_test[:,1]\nsubmission['Class_3'] = y_pred_test[:,2]\nsubmission['Class_4'] = y_pred_test[:,3]\nsubmission.to_csv('submission.csv', index=False)","89a04a9f":"## 2. preprocessing datasets.","ffdb4db0":"## 6. create my submission file","cd61663c":"## 1. read datasets.","fbc49beb":"## 3. define my transformer","3a3b0d09":"## 4. define my classifier","7cf93656":"### 2.3. prepare feature's value based on the probability of target's rate. ","cba6a4dc":"# VotingClassifier (lightgbm, catboost, tensorflow) ","69daff6a":"### 2.1. convert 'target' value to 0,1,2,3. ","09bf48ee":"### 2.2. clip datasets.","c14e445d":"## 5. train VotingClassifier"}}