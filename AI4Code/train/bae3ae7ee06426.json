{"cell_type":{"89c47457":"code","bd966f3e":"code","86125287":"code","7746cf5b":"code","99c48fe0":"code","d4ada680":"code","387abea8":"code","b4b274b9":"code","734f5fde":"code","d4229595":"code","1b3793ec":"code","b1f8c0e2":"code","dd0f26fe":"code","3dfe4af7":"code","fb8a3880":"code","a613e1bd":"code","97d3258e":"code","a13b9ebc":"code","90721be7":"code","89797206":"code","7cf47e99":"code","65cdd907":"code","2d92332c":"code","94370361":"code","d98f6794":"code","7a2bd66f":"code","ed75e783":"code","7c009726":"code","eae9640a":"markdown","ddc32cf3":"markdown","abfcf3eb":"markdown","fef3455b":"markdown","49b1a3d3":"markdown","55ac29ac":"markdown","e946b8ef":"markdown","f19c28d6":"markdown","97348720":"markdown","5075eefd":"markdown","10fa05d4":"markdown","3a89c637":"markdown","3ba0c74b":"markdown","a7fec62e":"markdown","a4c661e3":"markdown"},"source":{"89c47457":"!pip install xlrd","bd966f3e":"!pip install nltk","86125287":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame\npd.options.mode.chained_assignment = None\n\n# a library for cross-validation, hold out data split\nfrom sklearn.model_selection import KFold, train_test_split\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\n\n# a library for reading data and formatting information from Excel files\nimport xlrd\n\n# libraries for text processing\nimport re\nimport string\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\nfrom nltk.collocations import TrigramAssocMeasures, TrigramCollocationFinder\n\n# a data visualization library\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nRANDOM_STATE = 21","7746cf5b":"# read data from excel\nraw_df = pd.read_excel('..\/input\/aeronautics-astronautics-journal-abstracts\/aiaa_dataset.xls')\nraw_df.head()","99c48fe0":"raw_df.shape","d4ada680":"# check columns with nulls\nraw_df.isnull().any()","387abea8":"# check duplicates and drop if exist\nduplicated_rows = raw_df[raw_df.duplicated().values == True]\nprint(f'Number of duplicates: {len(duplicated_rows)}')\n\nraw_df.drop_duplicates(inplace=True, ignore_index=False)\nprint(f'Shape of the data after removing duplicates {raw_df.shape}')","b4b274b9":"# check unique values in columns\nprint('Number of unique values per column')\nprint(raw_df.nunique())\n    \na = raw_df['volume'].unique()\na.sort()\nprint(f'\\nUnique values in the volume column: {a}')\n\nb = raw_df['journal'].unique()\nprint(f'\\nUnique values in the journal column: {b}')","734f5fde":"similar_rows = raw_df[\n    np.logical_and(\n        (raw_df['title'].duplicated().values == True), \n        (raw_df['abstract'].duplicated().values == True),\n        (raw_df['journal'].duplicated().values == True)\n    )\n]\n\nraw_df[raw_df['title'].isin(similar_rows['title'].tolist())].sort_values(by='title')","d4229595":"raw_df.drop(similar_rows.index, inplace=True)\nraw_df[raw_df['title'].isin(similar_rows['title'].tolist())].sort_values(by='title')","1b3793ec":"raw_df_train, raw_df_test = train_test_split(\n    raw_df, test_size=0.1, random_state=RANDOM_STATE, stratify=raw_df['journal']\n)\nraw_df_test.shape","b1f8c0e2":"# retrieve combined text as well as text per each journal\nraw_df_train.loc[:, 'text'] = raw_df_train.loc[:, 'title'] + ' ' + raw_df_train.loc[:, 'abstract']\ntext_data = raw_df_train.loc[:, 'text'].tolist()\n\ntext_jpp = raw_df_train.loc[raw_df_train['journal'] == 'JPP', 'text'].tolist()\ntext_jtht = raw_df_train.loc[raw_df_train['journal'] == 'JTHT', 'text'].tolist()","dd0f26fe":"# we transform text into lowercase, split it into sentences and then into word tokens\ntokens_all = nltk.word_tokenize(' '.join(nltk.sent_tokenize(' '.join(text_data).lower())))\ntokens_jpp = nltk.word_tokenize(' '.join(nltk.sent_tokenize(' '.join(text_jpp).lower())))\ntokens_jtht = nltk.word_tokenize(' '.join(nltk.sent_tokenize(' '.join(text_jtht).lower())))\n\nvocabulary_all = sorted(set(tokens_all))\nvocabulary_jpp = sorted(set(tokens_jpp))\nvocabulary_jtht = sorted(set(tokens_jtht))\n\nprint(f'Size of full vocabulary: {len(vocabulary_all)}')\nprint(f'Size of JPP vocabulary: {len(vocabulary_jpp)}')\nprint(f'Size of JTHT vocabulary: {len(vocabulary_jtht)}')","3dfe4af7":"# select vocabulary common for both journals \nvocabulary_common = [x for x in vocabulary_jpp if x in vocabulary_jtht]\n\n# as well as the specific vocabulary per each journal\nvocabulary_jpp_unique = [x for x in vocabulary_jpp if x not in vocabulary_common]\nvocabulary_jtht_unique = [x for x in vocabulary_jtht if x not in vocabulary_common]\n\nprint(f'Number of common tokens: {len(vocabulary_common)}')\nprint(f'Number of JPP specific tokens: {len(vocabulary_jpp_unique)}')\nprint(f'Number of JTHT specific tokens:  {len(vocabulary_jtht_unique)}')","fb8a3880":"word_freq_jpp = nltk.FreqDist(w for w in tokens_jpp if w not in vocabulary_common)\nword_freq_jtht = nltk.FreqDist(w for w in tokens_jtht if w not in vocabulary_common)\n\ndef print_most_frequent_words(n_words = 30):\n    \n    print(f'The most frequent {n_words} words unique per type of journal\\n')\n    print(f'JPP journal: \\n{list(word_freq_jpp)[:30]}\\n')\n    print(f'JTHT journal: \\n{list(word_freq_jtht)[:30]}')\n    \nprint_most_frequent_words()","a613e1bd":"# remove stop words, punctuation\nwords_to_remove = set(stopwords.words('english') + list(string.punctuation))\n\nfiltered_vocabulary_all = [\n    word for word in vocabulary_all if word not in words_to_remove\n]\n\n# remove numbers and arithmetic expressions\npattern_numbers_and_expressions = re.compile('[\u00b1\u00d7\u2212\u223c\u2013-]?[0-9]\\d*[\u00d7\u00b1=.,\/\u2212\u223c\u2013-\u2236]?\\d*[\u00d7\u00b1=.,\/\u2212\u223c\u2013-\u2236]?')\npattern_letters_and_numbers = re.compile('[A-Za-z]+[\\d@]+[\\w@]*|[\\d@]+[A-Za-z]+[\\w@]*')\n\nfiltered_vocabulary_all =[\n    word for word in filtered_vocabulary_all if not (\n        pattern_numbers_and_expressions.match(word)\n        or pattern_letters_and_numbers.match(word)\n        or '=' in word\n    )\n]\n\n# remove leading and trailing special characters\nfiltered_vocabulary_all = [w.strip(\"'.\/*\u2217-\u2032+\u00ae`\u2019\") for w in filtered_vocabulary_all]\n\n# remove words with only one symbol that are not alpha\nfiltered_vocabulary_all = list(filter(\n    lambda x: (len(x) > 1) or (len(x) == 1 and x.isalpha()), \n    filtered_vocabulary_all\n)) \n\nfiltered_tokens_all = [t for t in tokens_all if t in filtered_vocabulary_all]\nfiltered_tokens_jpp = [t for t in tokens_jpp if t in filtered_vocabulary_all]\nfiltered_tokens_jtht = [t for t in tokens_jtht if t in filtered_vocabulary_all]","97d3258e":"freq_tokens_all = nltk.FreqDist(filtered_tokens_all)\n\nplt.figure(figsize=(17, 7))\nfreq_tokens_all.plot(50, title='The most frequent words in Aeronautics & Astronautics Abstracts')","a13b9ebc":"bigram_frequency_creteria = 5\nmax_number_allocations = 1000\n\nbigram_measures = BigramAssocMeasures()\nfinder = BigramCollocationFinder.from_words(filtered_tokens_all)\nfinder.apply_freq_filter(bigram_frequency_creteria)\n\nbigram_collocation = finder.nbest(bigram_measures.likelihood_ratio, max_number_allocations)\n\nprint(len(bigram_collocation))\nprint(bigram_collocation[0:10])","90721be7":"trigram_frequency_creteria = 3\nmax_number_allocations = 500\n\ntrigram_measures = TrigramAssocMeasures()\n\nfinder = TrigramCollocationFinder.from_words(filtered_tokens_all)\nfinder.apply_freq_filter(trigram_frequency_creteria)\n\ntrigram_collocation = finder.nbest(trigram_measures.likelihood_ratio, max_number_allocations)\n\nprint(len(trigram_collocation))\nprint(trigram_collocation[:10])","89797206":"# transform collocations from tuples to string units and store them in lists\nbigram_collocation = [' '.join(collocation) for collocation in bigram_collocation]\ntrigram_collocation = [' '.join(collocation) for collocation in trigram_collocation]\n\n# create vocabulary of collocations\ncollocations_vocabulary = bigram_collocation\ncollocations_vocabulary.extend(trigram_collocation)\n\nprint(bigram_collocation[:5])\nprint(trigram_collocation[:5])","7cf47e99":"# get n most frequent vocabulary tokens\ndef get_vocabulary(tokens: list, n = 2000):\n\n    vocabulary = tokens[:n] if len(tokens) >= n else tokens\n    return vocabulary\n\n# create vocabulary of tokens\nordered_tokens = list(freq_tokens_all)\ntokens_vocabulary = get_vocabulary(ordered_tokens, len(ordered_tokens))\n\nprint(f'Size of vocabulary of tokens: {len(tokens_vocabulary)}')\nprint(f'Size of vocabulary of collocations: {len(collocations_vocabulary)}')","65cdd907":"# get document features in a form of dictionary \n# where key contains a token name and value tells whether the token is present in the document\ndef get_document_features(\n    document_tokens, \n    document_text,\n    tokens = tokens_vocabulary, \n    collocations = collocations_vocabulary\n):\n    unique_document_tokens = set(document_tokens)\n    \n    features = {}\n    for word in tokens:\n        features['contains({})'.format(word)] = (word in unique_document_tokens)\n        \n    for collocation in collocations:\n        features['contains({})'.format(collocation)] = (collocation in document_text)\n        \n    return features\n\n# create features for each type of journals of the TRAINING set\nfeatures_jpp_train = [\n    (\n        get_document_features(\n            nltk.word_tokenize(' '.join(nltk.sent_tokenize(d.lower()))), d\n        ), 'JPP'\n    ) for d in text_jpp\n]\n\nfeatures_jtht_train = [\n    (\n        get_document_features(\n            nltk.word_tokenize(' '.join(nltk.sent_tokenize(d.lower()))), d\n        ), 'JTHT'\n    ) for d in text_jtht\n]\n\ndocuments_features_train = features_jpp_train + features_jtht_train\nrandom.shuffle(documents_features_train)\n\nprint(f'Check number of documents features in the training set: {len(documents_features_train)}')","2d92332c":"# create features for each type of journals of the TESTING set\nraw_df_test.loc[:, 'text'] = raw_df_test.loc[:, 'title'] + ' ' + raw_df_test.loc[:, 'abstract']\n\ntext_jpp_test = raw_df_test.loc[raw_df_test['journal'] == 'JPP', 'text'].tolist()\ntext_jtht_test = raw_df_test.loc[raw_df_test['journal'] == 'JTHT', 'text'].tolist()\n\n\nfeatures_jpp_test = [\n    (\n        get_document_features(\n            nltk.word_tokenize(' '.join(nltk.sent_tokenize(d.lower()))), d\n        ), 'JPP'\n    ) for d in text_jpp_test\n]\n\nfeatures_jtht_test = [\n    (\n        get_document_features(\n            nltk.word_tokenize(' '.join(nltk.sent_tokenize(d.lower()))), d\n        ), 'JTHT'\n    ) for d in text_jtht_test\n]\n\ndocuments_features_test = features_jpp_test + features_jtht_test\nrandom.shuffle(documents_features_test)\n\nprint(f'Check number of documents features in the testing set: {len(documents_features_test)}')","94370361":"num_folds = 10\n\nclassifier = nltk.NaiveBayesClassifier\n\ntraining_accuracy = []\nvalidation_accuracy = []\n\nskf = KFold(n_splits=num_folds, shuffle=True, random_state=RANDOM_STATE)\n\nfor train_index, validation_index in skf.split(documents_features_train):\n    \n    train_set = [documents_features_train[i] for i in train_index]\n    validation_set = [documents_features_train[i] for i in validation_index]\n    model = classifier.train(train_set)\n    \n    training_accuracy.append(nltk.classify.accuracy(model, train_set))\n    validation_accuracy.append(nltk.classify.accuracy(model, validation_set))","d98f6794":"print(f'Accuracy of train data: {sum(training_accuracy)\/len(training_accuracy)}')\nprint(f'Accuracy of validation data: {sum(validation_accuracy)\/len(validation_accuracy)}')","7a2bd66f":"model = classifier.train(documents_features_train)\n\ntesting_accuracy = nltk.classify.accuracy(model, documents_features_test)\nprint(f'Accuracy of test data: {testing_accuracy}')","ed75e783":"# get document features for test in the initial order\ndocuments_features_test = features_jpp_test + features_jtht_test\n\nwrong_predictions_indexes = []\n\nfor index, document in enumerate(documents_features_test):\n    features, journal_type = document\n    journal_type_predicted = model.classify(features)\n    if journal_type != journal_type_predicted:\n        wrong_predictions_indexes.append(index)\n\nn = len(text_jpp_test)\nfor index in wrong_predictions_indexes:\n    if index < n:\n        print(f'JPP document: {text_jpp_test[index]}\\n')\n    else:\n        print(f'JTHT document: {text_jtht_test[index - n]}\\n')","7c009726":"model.show_most_informative_features(50)","eae9640a":"## Most Informative Features\nBoth tokens and collocations play important role in predicting the domain","ddc32cf3":"## Frequency distribution of tokens","abfcf3eb":"## Find collocations\nA collocation is a series of words or terms that co-occur more often than would be expected by chance. They can be play an important role in distinguishing between domains","fef3455b":"## Feature engineering\n<b>For Naive Bayes Classifier<\/b>","49b1a3d3":"Find the most frequent tokens unique for the each type of journal","55ac29ac":"Values of the title, abstract and journal are repeated in the subset above. The only difference is volume (35.1\/34.5). We can remove the similar rows because the information important for the task is in the first 3 columns","e946b8ef":"## Retrieve vocabulary from text data\n<b>We use Natural Language Toolkit<\/b><\/br>\n@ Steven Bird, Ewan Klein, and Edward Loper (2009). Natural Language Processing with Python. O\u2019Reilly Media Inc. https:\/\/www.nltk.org\/book\n\nWe will explore vocabulary of each type of journals","f19c28d6":"## Text Data Cleaning\nLet's remove stop words, punctuation, numbers, single sybmol words and arithmetic expressions from the vocabulary","97348720":"## Test on the testing dataset","5075eefd":"## Classify and validate entries \n<b>With NaiveBayesClassifier and KFold cross-validation<\/b><\/br><\/br>\nWe use cross-validation to try different parameters for bigram and trigram_frequency_creteria, max_number_allocations and tokens vocabulary size. Then we test the model on unseen data","10fa05d4":"## Explore and preprocess the data","3a89c637":"## Split data\nLater we will retrieve a text vocabulary only from the training data. The vocabulary of the testing data will remain unseen for our model. \nWe split the data to keep proportation of both classes the same as in initial dataset","3ba0c74b":"We see that title and abstract columns have 474 unique values while the total number of rows is 492. We look at the repeated values of titles and abstracts","a7fec62e":"## Import the required libraries","a4c661e3":"## Error Analysis\nAnalyse documents with wrong predictions"}}