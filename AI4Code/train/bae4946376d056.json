{"cell_type":{"bd8f4883":"code","75cb447d":"code","b666230d":"code","8a09afdc":"code","17eba931":"code","886df5e9":"code","96876763":"code","15ead985":"code","96606b74":"code","c6895e5d":"code","41d15027":"code","3c098969":"code","f6d69c1b":"code","bf5c84c6":"code","ac4c8a27":"code","505dee12":"code","6398daa2":"code","2b2a3a72":"code","31fbaa33":"code","fe1e49e4":"code","7edea2be":"code","c9fd3c96":"markdown","6e8e6139":"markdown","63a4edc8":"markdown","76866e8a":"markdown","c14179b1":"markdown","b46d2d69":"markdown","540ccc10":"markdown","01b6b7a2":"markdown","0ee06428":"markdown","58af54c4":"markdown","ff363a61":"markdown","0c69bfaf":"markdown","0baeb80c":"markdown","b51358d6":"markdown","95c15218":"markdown","9f6dfa4d":"markdown","0fdb92ae":"markdown","ea84941a":"markdown","e4109cc1":"markdown"},"source":{"bd8f4883":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sbn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten\nfrom keras.optimizers import Adam\nfrom keras.callbacks import TensorBoard\nfrom keras.utils import to_categorical","75cb447d":"fashion_train_df = pd.read_csv('..\/input\/fashion-mnist-datasets\/fashion-mnist_train.csv', sep=',')\nfashion_test_df = pd.read_csv('..\/input\/fashion-mnist-datasets\/fashion-mnist_test.csv', sep=',')","b666230d":"fashion_train_df.shape   # Shape of the dataset","8a09afdc":"fashion_train_df.columns   # Name of the columns of the DataSet.","17eba931":"print(set(fashion_train_df['label']))","886df5e9":"print([fashion_train_df.drop(labels='label', axis=1).min(axis=1).min(), \n      fashion_train_df.drop(labels='label', axis=1).max(axis=1).max()])","96876763":"fashion_train_df.head()","15ead985":"fashion_test_df.shape","96606b74":"fashion_test_df.head()","c6895e5d":"# Convert the dataframe ti numpy array\ntraining = np.asarray(fashion_train_df, dtype='float32')\n\n# Lets show multiple images in a 15x15 grid\nheight = 10\nwidth = 10\n\nfig, axes = plt.subplots(nrows=width, ncols=height, figsize=(17,17))\naxes = axes.ravel()  # this flattens the 15x15 matrix into 225\nn_train = len(training)\n\nfor i in range(0, height*width):\n    index = np.random.randint(0, n_train)\n    axes[i].imshow(training[index, 1:].reshape(28,28))\n    axes[i].set_title(int(training[index, 0]), fontsize=8)\n    axes[i].axis('off')\n    \nplt.subplots_adjust(hspace=0.5)","41d15027":"# convert to numpy arrays and reshape\ntraining = np.asarray(fashion_train_df, dtype='float32')\nX_train = training[:, 1:].reshape([-1,28,28,1])\nX_train = X_train\/255   # Normalizing the data\ny_train = training[:, 0]\n\ntesting = np.asarray(fashion_test_df, dtype='float32')\nX_test = testing[:, 1:].reshape([-1,28,28,1])\nX_test = X_test\/255    # Normalizing the data\ny_test = testing[:, 0]","3c098969":"# Split the training set into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=12345)    # TODO : change the random state to 5","f6d69c1b":"# Lets check the shape of all three datasets\nprint(X_train.shape, X_val.shape, X_test.shape)\nprint(y_train.shape, y_val.shape, y_test.shape)","bf5c84c6":"cnn_model = Sequential()\ncnn_model.add(Conv2D(filters=64, kernel_size=(3,3), input_shape=(28,28,1), activation='relu'))\ncnn_model.add(MaxPooling2D(pool_size = (2,2)))\ncnn_model.add(Dropout(rate=0.3))\ncnn_model.add(Flatten())\ncnn_model.add(Dense(units=32, activation='relu'))\ncnn_model.add(Dense(units=10, activation='sigmoid'))","ac4c8a27":"cnn_model.compile(optimizer=Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\ncnn_model.summary()","505dee12":"cnn_model.fit(x=X_train, y=y_train, batch_size=512, epochs=50, validation_data=(X_val, y_val))","6398daa2":"eval_result = cnn_model.evaluate(X_test, y_test)\nprint(\"Accuracy : {:.3f}\".format(eval_result[1]))","2b2a3a72":"y_pred = cnn_model.predict_classes(x=X_test)","31fbaa33":"height = 10\nwidth = 10\n\nfig, axes = plt.subplots(nrows=width, ncols=height, figsize=(20,20))\naxes = axes.ravel()\nfor i in range(0, height*width):\n    index = np.random.randint(len(y_pred))\n    axes[i].imshow(X_test[index].reshape((28,28)))\n    axes[i].set_title(\"True Class : {:0.0f}\\nPrediction : {:d}\".format(y_test[index],y_pred[index]))\n    axes[i].axis('off')\nplt.subplots_adjust(hspace=0.9, wspace=0.5)","fe1e49e4":"cm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(10,5))\nsbn.heatmap(cm, annot=True)","7edea2be":"num_classes = 10\nclass_names = [\"class {}\".format(i) for i in range(num_classes)]\ncr = classification_report(y_test, y_pred, target_names=class_names)\nprint(cr)","c9fd3c96":"**Plot Confusin Matrix**","6e8e6139":"### Step # 5 - Create and Train the Model\n---\n**Create the model**","63a4edc8":"# Fashion MNIST Classification\n---\nFashion-MNIST is a dataset of Zalando's article images\u2014consisting of a **training set of 60,000** examples and a **test set of 10,000 examples**. Each example is a **28x28 grayscale** image, associated with a label from **10 classes**. We intend Fashion-MNIST to serve as a direct drop-in **replacement for the original MNIST** dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\n\nHere's an example how the data looks (each class takes three-rows):\n![Fashion MNIST](https:\/\/github.com\/zalandoresearch\/fashion-mnist\/raw\/master\/doc\/img\/fashion-mnist-sprite.png)","76866e8a":"### Step # 2 - Load Data\n---\nNow lets use **pandas** library to read the train and test datasets in the respective csv files. We are going to use the **read_csv** function which reads a csv file and returns a pandas **DataFrame** object.","c14179b1":"### Step # 5 - Evaluate the Model\n---\n**Get the accuracy of the model**","b46d2d69":"### Step # 3 - Visualization\n---\nNow that we have loaded the data and also got somewhat acquainted with it lets visualize the actual images. We are going to use **Matplotlib** library for this. ","540ccc10":"**Classification Report**","01b6b7a2":"Now that we have loaded the datasets, lets check some parameters about the datasets.","0ee06428":"So we can see that the 1st column is the label or target value for each row.\n\nNow Lets find out how many distinct lables we have.","58af54c4":"### Step # 4 - Preprocess Data\n---\nGreat! We have visualized the images. So now we can start preparing for creating our model. But before that we need to preprocess our data so that we can fit our model easily. Lets do that first.\n\nSince we are dealing with image data and our task is to recognize and classify images our model should be a Convolutional Neural Network. For that our images should have atleast 3 dimensions (**height** x **width** x **color_channels**). But our images are flattened in one dimension, **784 pixel (28x28x1)** values per row. So we need to reshape the data into its original format.","ff363a61":"Also we need to have three different sets of data for **training, validatin** and **testing**. We already have different sets for training and testing. So we are going to split the training dataset further into two sets and will use one set of training and the other for validation.","0c69bfaf":"**Visualize the model's predictions**","0baeb80c":"So here we have 10000 images instead of 60000 as in the train dataset.\n\nLets check first few rows.","b51358d6":"**compile the model**","95c15218":"So we have 0 to 255 which is the color values for grayscale. 0 being white and 255 being black.\n\nNow lets check some of the rows in tabular format","9f6dfa4d":"### Step # 1 - Import Libraries\n---\nLets import all the libraries we are going to require for this classification project. It is always good to put all the import statements at the begining of the file.","0fdb92ae":"So evry other things of the test dataset are going to be the same as the train dataset except the shape.","ea84941a":"**Train the model**","e4109cc1":"So we have 10 different lables. from 0 to 9. \n\nNow lets find out what is the min and max of values of in the other columns."}}