{"cell_type":{"a3346a11":"code","b6136d62":"code","79e0fefb":"code","401e1f8e":"code","e138cb18":"code","fc2a3c5c":"code","0e95e8b7":"code","75274a99":"code","af51fe4a":"code","3dd6b44f":"code","2d535edb":"code","b93c5591":"code","4263d77d":"code","5b634a65":"code","a01814b0":"code","ca05c044":"code","ff9e58b7":"code","bcdba96c":"code","acd48f02":"code","4d3f8adc":"code","27e1c641":"code","a28ed71d":"code","8c0c0c5a":"code","ff3a2920":"markdown","0b670099":"markdown","8b79fa53":"markdown","3181aede":"markdown","4b598d4e":"markdown","6e628baa":"markdown","44768e88":"markdown","ff04c4a7":"markdown","7741c91b":"markdown","d3bbfa2a":"markdown","ab3ec463":"markdown","8f8eb27b":"markdown","ecbc9347":"markdown","7a111609":"markdown","622e5e63":"markdown","c53b836e":"markdown","f2228c37":"markdown","ff772e49":"markdown","76562c75":"markdown","6584f49e":"markdown"},"source":{"a3346a11":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom catboost import CatBoostClassifier","b6136d62":"data_bc = pd.read_csv('..\/input\/credit-card-customers\/BankChurners.csv')","79e0fefb":"data_desc = data_bc.describe()","401e1f8e":"data_bc.head()","e138cb18":"data_bc.drop(['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n             'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'],\n                      axis='columns', inplace=True)\n\ndata_desc = data_bc.describe()","fc2a3c5c":"data_desc.columns","0e95e8b7":"data_bc.columns","75274a99":"data_bc = pd.get_dummies(data_bc, columns=['Attrition_Flag', 'Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category'])\n\ndata_bc.describe()","af51fe4a":"data_bc.isnull().sum()","3dd6b44f":"data_bc1 = data_bc.drop(columns=['Attrition_Flag_Attrited Customer', 'Gender_F', 'Education_Level_College', \n                                 'Marital_Status_Divorced', 'Income_Category_Unknown', 'Card_Category_Blue'])","2d535edb":"data_bc1.columns","b93c5591":"X=data_bc1[['CLIENTNUM', 'Customer_Age', 'Dependent_count', 'Months_on_book',\n       'Total_Relationship_Count', 'Months_Inactive_12_mon',\n       'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal',\n       'Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt',\n       'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio', 'Gender_M',\n       'Education_Level_Doctorate', 'Education_Level_Graduate',\n       'Education_Level_High School', 'Education_Level_Post-Graduate',\n       'Education_Level_Uneducated', 'Education_Level_Unknown',\n       'Marital_Status_Married', 'Marital_Status_Single',\n       'Marital_Status_Unknown', 'Income_Category_$120K +',\n       'Income_Category_$40K - $60K', 'Income_Category_$60K - $80K',\n       'Income_Category_$80K - $120K', 'Income_Category_Less than $40K',\n       'Card_Category_Gold', 'Card_Category_Platinum', 'Card_Category_Silver']]\nY=data_bc1[['Attrition_Flag_Existing Customer']]\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,random_state=0)\n\nX_train.to_csv('X_train.csv')\nX_test.to_csv('X_test.csv')\n\nY_train.to_csv('Y_train.csv')\nY_test.to_csv('Y_test.csv')","4263d77d":"X_train = pd.read_csv('.\/X_train.csv')\nX_test = pd.read_csv('.\/X_test.csv')\nY_train = pd.read_csv('.\/Y_train.csv').to_numpy()[:,1]\nY_test = pd.read_csv('.\/Y_test.csv').to_numpy()[:,1]","5b634a65":"log_reg=LogisticRegression(C=1000,max_iter=50000)\nlog_reg.fit(X_train, Y_train)\n\n\nprint('--------------------------------------------------------------------------')\nprint('Logistic Regression:')\nprint('Traning Model accruracy scores: {:.3f}'.format(log_reg.score(X_train,Y_train)))\nprint('Test Model accruracy scores: {:.3f}'.format(log_reg.score(X_test,Y_test)))\nprint('--------------------------------------------------------------------------')","a01814b0":"KNN=KNeighborsClassifier(n_neighbors=20)\nKNN.fit(X_train, Y_train)\nY_pred=KNN.predict(X_test) #here we make our predictions\n\nActVPred = pd.DataFrame({'Actual': Y_test, 'Predicted': Y_pred})\nprint(ActVPred)\n\n#Checking the accuracy \nprint('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, Y_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(Y_test, Y_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))\n\nCount_row = []\nindex = 0\n\nfor i, row in ActVPred.iterrows():\n    if (row['Actual'] < 1):\n        if (row['Predicted'] < 0.5):\n            Count_row.append(1)\n        else:\n            Count_row.append(0)\n    else:\n        if (row['Predicted'] >= 0.5):\n            Count_row.append(1)\n        else:\n            Count_row.append(0)\n    index = index + 1\n    \nprint('--------------------------------------------------------------------------')\nprint('KNN:')\nprint('Model accruracy scores: {:.3f}'.format(Count_row.count(1)\/index))","ca05c044":"Clf =  RandomForestClassifier(n_estimators = 500, n_jobs = -1)\nClf.fit(X_train, Y_train)\nY_pred=Clf.predict(X_test) \n\n\nActVPred = pd.DataFrame({'Actual': Y_test, 'Predicted': Y_pred})\nprint(ActVPred)\n\n#Checking the accuracy \nprint('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, Y_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(Y_test, Y_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))\n\nCount_row = []\nindex = 0\n\nfor i, row in ActVPred.iterrows():\n    if (row['Actual'] < 1):\n        if (row['Predicted'] < 0.5):\n            Count_row.append(1)\n        else:\n            Count_row.append(0)\n    else:\n        if (row['Predicted'] >= 0.5):\n            Count_row.append(1)\n        else:\n            Count_row.append(0)\n    index = index + 1\n    \n    \nprint('--------------------------------------------------------------------------')\nprint('Random Forest Classifier:')\nprint('Model accruracy scores: {:.3f}'.format(Count_row.count(1)\/index))\n\ncompare1 = pd.DataFrame()\ncompare1[0] = Clf.feature_importances_\ncompare1[1] = X_test.columns\n\nprint('Feature importance: ')\nprint(compare1.sort_values(by=0,ascending= False))","ff9e58b7":"NN = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1, max_iter=1000)\nNN.fit(X_train, Y_train)\n\nY_pred = NN.predict(X_test)\n\nActVPred = pd.DataFrame({'Actual': Y_test, 'Predicted': Y_pred})\nprint(ActVPred)\n\n#Checking the accuracy \nprint('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, Y_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(Y_test, Y_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))\n\nCount_row = []\nindex = 0\n\nfor i, row in ActVPred.iterrows():\n    if (row['Actual'] < 1):\n        if (row['Predicted'] < 0.5):\n            Count_row.append(1)\n        else:\n            Count_row.append(0)\n    else:\n        if (row['Predicted'] >= 0.5):\n            Count_row.append(1)\n        else:\n            Count_row.append(0)\n    index = index + 1\n    \n    \nprint('--------------------------------------------------------------------------')\nprint('Random Forest Classifier:')\nprint('Model accruracy scores: {:.3f}'.format(Count_row.count(1)\/index))","bcdba96c":"# Using the best model from Grid Serach CV\nmodel = RandomForestRegressor(max_depth=15, random_state=42) \n\nmodel.fit(X_train, Y_train)\n\nY_pred = model.predict(X_test)\n\nActVPred = pd.DataFrame({'Actual': Y_test, 'Predicted': Y_pred})\nprint(ActVPred)\n\n#Checking the accuracy \nprint('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, Y_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(Y_test, Y_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))\n\nCount_row = []\nindex = 0\n\nfor i, row in ActVPred.iterrows():\n    if (row['Actual'] < 1):\n        if (row['Predicted'] < 0.5):\n            Count_row.append(1)\n        else:\n            Count_row.append(0)\n    else:\n        if (row['Predicted'] >= 0.5):\n            Count_row.append(1)\n        else:\n            Count_row.append(0)\n    index = index + 1\n    \nprint('--------------------------------------------------------------------------')\nprint('RandomForestRegressor:')\nprint('Model accruracy scores: {:.3f}'.format(Count_row.count(1)\/index))","acd48f02":"from sklearn.naive_bayes import GaussianNB\n\nGNB = GaussianNB()\n\nGNB.fit(X_train, Y_train)\nY_pred = GNB.predict(X_test)\n\nActVPred = pd.DataFrame({'Actual': Y_test, 'Predicted': Y_pred})\nprint(ActVPred)\n\n#Checking the accuracy \nprint('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, Y_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(Y_test, Y_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))\n\nCount_row = []\nVisual_rep = []\nindex = 0\n\nfor i, row in ActVPred.iterrows():\n    if (row['Predicted'] < 0.5):\n        Visual_rep.append(0)\n    else:\n        Visual_rep.append(1)\n            \n    if (row['Actual'] < 1):\n        if (row['Predicted'] < 0.5):\n            Count_row.append(1)\n        else:\n            Count_row.append(0)\n    else:\n        if (row['Predicted'] >= 0.5):\n            Count_row.append(1)\n        else:\n            Count_row.append(0)\n    \n    index = index + 1\n    \nprint('--------------------------------------------------------------------------')\nprint('Naive Bayes:')\nprint('Model accruracy scores: {:.3f}'.format(Count_row.count(1)\/index))\n","4d3f8adc":"model = XGBRegressor()\nmodel.fit(X_train, Y_train)\nY_pred = model.predict(X_test)\n\nActVPred = pd.DataFrame({'Actual': Y_test, 'Predicted': Y_pred})\nprint(ActVPred)\n\n#Checking the accuracy \nprint('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, Y_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(Y_test, Y_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))\n\nCount_row = []\nVisual_rep = []\nindex = 0\n\nfor i, row in ActVPred.iterrows():\n    if (row['Predicted'] < 0.5):\n        Visual_rep.append(0)\n    else:\n        Visual_rep.append(1)\n        \n    if (row['Actual'] < 1):\n        if (row['Predicted'] < 0.5):\n            Count_row.append(1)\n        else:\n            Count_row.append(0)\n    else:\n        if (row['Predicted'] >= 0.5):\n            Count_row.append(1)\n        else:\n            Count_row.append(0)\n    index = index + 1\n    \nprint('--------------------------------------------------------------------------')\nprint('XGBoost:')\nprint('Model accruracy scores: {:.3f}'.format(Count_row.count(1)\/index))\n","27e1c641":"ax = plt.subplots(figsize=(10, 10))\nax = sns.heatmap(confusion_matrix(Visual_rep,Y_test),annot=True,cmap='coolwarm',fmt='d')\nax.set_title('Prediction On Original Data With XGBoost Confusion Matrix',fontsize=18)\nax.set_xticklabels(['Churn','Not Churn'],fontsize=18)\nax.set_yticklabels(['Predicted Churn','Predicted Not Churn'],fontsize=18)\n\nplt.show()","a28ed71d":"Cat = CatBoostClassifier(silent = True)\n\ndetails = Cat.fit(X_train, Y_train)\nY_pred = Cat.predict(X_test)\n\nActVPred = pd.DataFrame({'Actual': Y_test, 'Predicted': Y_pred})\nprint(ActVPred)\n\n#Checking the accuracy \nprint('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, Y_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(Y_test, Y_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))\n\nCount_row = []\nVisual_rep = []\nindex = 0\n\nfor i, row in ActVPred.iterrows():\n    if (row['Predicted'] < 0.5):\n        Visual_rep.append(0)\n    else:\n        Visual_rep.append(1)\n            \n    if (row['Actual'] < 1):\n        if (row['Predicted'] < 0.5):\n            Count_row.append(1)\n        else:\n            Count_row.append(0)\n    else:\n        if (row['Predicted'] >= 0.5):\n            Count_row.append(1)\n        else:\n            Count_row.append(0)\n    \n    index = index + 1\n    \nprint('--------------------------------------------------------------------------')\nprint('CatBoost:')\nprint('Model accruracy scores: {:.3f}'.format(Count_row.count(1)\/index))\n","8c0c0c5a":"ax = plt.subplots(figsize=(10, 10))\nax = sns.heatmap(confusion_matrix(Visual_rep,Y_test),annot=True,cmap='coolwarm',fmt='d')\nax.set_title('Prediction On Original Data With CatBoost Confusion Matrix',fontsize=18)\nax.set_xticklabels(['Churn','Not Churn'],fontsize=18)\nax.set_yticklabels(['Predicted Churn','Predicted Not Churn'],fontsize=18)\n\nplt.show()","ff3a2920":"# Logistic Regression","0b670099":"# Preparing Test and Train data","8b79fa53":"Dropping one column each for one hot encoded columns","3181aede":"# RandomForestRegressor","4b598d4e":"# Naive Bayes","6e628baa":"# Loading saved data","44768e88":"# Getting view of data","ff04c4a7":"# Representation of the prediction","7741c91b":"# CatBoost","d3bbfa2a":"# Representation of the prediction","ab3ec463":"# RandomForest Classifier","8f8eb27b":"# Loading the dataset","ecbc9347":"# Finding out Columns that have numerical and non-numerical values","7a111609":"From the above two output we can identify non numerical columns","622e5e63":"# One hot encoding for non numerical columns","c53b836e":"# Neural Network\n\nsolver{\u2018lbfgs\u2019, \u2018sgd\u2019, \u2018adam\u2019}, default=\u2019adam\u2019\nThe solver for weight optimization. \n\n-\u2018lbfgs\u2019 is an optimizer in the family of quasi-Newton methods.\n\n-\u2018sgd\u2019 refers to stochastic gradient descent.\n\n-\u2018adam\u2019 refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba\n\nNote: The default solver \u2018adam\u2019 works pretty well on relatively large datasets (with thousands of training samples or more) in terms of both training time and validation score. For small datasets, however, \u2018lbfgs\u2019 can converge faster and perform better.","f2228c37":"# KNN method","ff772e49":"# Viewing all Columns in play","76562c75":"# XGBoost","6584f49e":"# Loading Libraries"}}