{"cell_type":{"3f46622a":"code","2fb98d87":"code","ce635429":"code","fa97d15f":"code","89297099":"code","11f34c17":"code","eec3d55f":"code","8701ecef":"code","5f061722":"markdown","73a7b58e":"markdown","ea63dd4e":"markdown","a51a20d9":"markdown","ca69dda2":"markdown","8c6f1927":"markdown","a5e6f628":"markdown","69ea9bea":"markdown","ac3baa2d":"markdown","ea925d5d":"markdown"},"source":{"3f46622a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2fb98d87":"churn = pd.read_csv('..\/input\/Churn_Modelling.csv')\n\ndef filter_by_dtype(dataframe, data_type):\n    \"\"\"filter a dataframe by columns with a certain data_type\"\"\"\n    col_names = dataframe.dtypes[dataframe.dtypes == data_type].index\n    return dataframe[col_names]\n\nchurn_numerical = pd.concat([filter_by_dtype(churn, int), filter_by_dtype(churn, float)], axis=1)\n\n### Plot Distibutions ###\nfig = plt.figure(figsize = (15,20))\nax = fig.gca()\nchurn_numerical.hist(ax = ax, bins = 15)\nplt.show()","ce635429":"def print_column_counts(dataframe, list_of_columns):\n    \"\"\"print the percentages of all the unique values in a column\"\"\"\n    df_len = len(dataframe)\n    for name in list_of_columns:\n        counts = dataframe[name].value_counts() \/ df_len\n        percents = list(counts); values = list(counts.index)\n        values_and_percents = list(zip(values, percents))\n        [print(name + ' value ' + str(value) + ' represents ' + str(round(percent * 100, 4)) + '% percent of the field \\n') for value, percent in values_and_percents]\n        \nprint_column_counts(churn, ['Exited','HasCrCard','Gender', 'Geography','IsActiveMember','NumOfProducts'])","fa97d15f":"fig = plt.figure(figsize = (18,18)); ax = fig.gca()\none_hot_churn = pd.get_dummies(churn, columns = ['Gender', 'Geography', 'HasCrCard', 'IsActiveMember'])\nsns.heatmap(one_hot_churn.corr(), annot = True, vmin= -0.5, vmax = 0.5, ax=ax)","89297099":"one_hot_churn.columns","11f34c17":"np.random.seed(123)\n\nkeep_columns = ['CreditScore', 'Age', 'Tenure',\n       'Balance', 'NumOfProducts', 'EstimatedSalary',\n       'Gender_Female', 'Gender_Male', 'Geography_France', 'Geography_Germany',\n       'Geography_Spain', 'HasCrCard_0', 'HasCrCard_1', 'IsActiveMember_0',\n       'IsActiveMember_1']\n\nlabel = ['Exited']\n\nshuffled_churn = one_hot_churn.sample(frac=1).reset_index(drop=True)\nchurn_train = shuffled_churn[keep_columns]\nchurn_label = shuffled_churn[label]\nsplit_num = int(len(shuffled_churn) * 0.8)\n\ntrain_x = churn_train.iloc[:split_num,:]\ntrain_y = churn_label[:split_num]\ntest_x = churn_train.iloc[split_num:,:]\ntest_y = churn_label[split_num:]","eec3d55f":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nneigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(train_x, train_y)\npred_y = neigh.predict(test_x)\nprint(neigh.score(train_x, train_y))\nconf = confusion_matrix(test_y, pred_y)\nconf","8701ecef":"sns.heatmap(conf, annot = True, fmt='g')","5f061722":"# 2). Train a 'Dumb' Model as a Baseline\n\nA dumb model could turn out to preform quite well. What makes it 'dumb' is that I'll use a simple classification algorithm, off-the-shelf, with no engineered features or feature scaling. I will use this algorithm to rate future algorithms against.","73a7b58e":"**'Age'**, **'CreditScore'**, and **'Balance'** seem to follow nearly normal distribution. **'Balance'** has a large skew at a low balance. This will throw off averages or medians if calculated using this field. \n\nThe other fields are mostlty uniform in distribution. **'Exited'** and **'NumOfProducts'** are different thought, they look like decaying exponential functions.","ea63dd4e":"## 1c). Plot Correlations\nNow I'm going to check correlations to see which features most strongly correlate with **'Exited'**. I also want to check if any of the  have any co-linearity or if there are any other interesting correlations in the dataset.","a51a20d9":"# 1). Exploratory Work","ca69dda2":"## 1b). Value Counts by Column\nThe following cell prints the unique value counts for each categorical column in the dataset. I use a custom function print_column_counts to find these percentages","8c6f1927":"Here we see the field we want to use to train our predictive model **'Exited'** is imbalanced, with almost 80% of our data containing individuals that did not churn (0). \n\nWe also notice other categories are disproportionate. For example, within the **'Gender'** field over half of the data are males (55%) and roughly half of the **'Geography'** field is represented by France with Spain and Germany representing the other half. These percertages will be helpful to keep in mind as we move forward","a5e6f628":"## 2a). Train-Test Split ","69ea9bea":"## 1a). Plot Distributions of Numerical Variables\nI define a custom function to filter a dataframe by a certain data type. That function is defined as filter_by_dataframe. Next I use the filter and plot the distributions of int and float vairables using pandas in-built distibution function dist()","ac3baa2d":"Here we see that **'Age', 'Balance', 'IsActiveMember'**, **'Gender'** and **'NumOfProducts'** are most correlated with **'Exited'** (all still under +\/- 0.3 so they are *light* correlations). \n\nAn interesting correlational group is between the country locations **'Geography'** and **'Balance'**. Germany is positively correlated with **'Balance'**, while Spain and France are negatively correlated with **'Balance'**. Meaning in this dataset one is more likely to have a higher balance if they live in Germany than if they live in France or Spain. \n\nAlso worth noting is that the negative correlation between **'Balance' ** and **'NumOfProducts'**. This is a weak negative correlation coming in at -0.3, but it shows that the more products one has, generally, the more likely they are to have a lower balance. \n\nThere aren't many stong correlations in this dataset (outside of one hot groups) so we can conclude that there's no colinearity.","ea925d5d":"At 84% the model has learned *something*, but it's not as good as it may sound. For instance, If the model were to just guess '0' everytime, it'd end up with 80% accuracy (remember the percentages from section 1b?) because '0's make up 80% of the labeled values. \n\nThis is a good first pass though. \n\n### Thank you for reading, More to come!"}}