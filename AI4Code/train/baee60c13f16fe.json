{"cell_type":{"f1d94138":"code","0381286e":"code","5e4a74f8":"code","39f41195":"code","6006dc34":"code","d37686d3":"code","ee7ed264":"code","64b06c57":"code","a813551c":"code","dbe1516a":"code","18adbb12":"code","dfeef67f":"code","f30521ac":"code","f3bc1f37":"code","52db369a":"code","69553c1a":"code","004c6336":"code","d9cfc04d":"code","3cad39f6":"code","7eb23ec1":"code","9336f9dc":"code","647538b8":"code","472fb21a":"code","84d761e7":"code","1f531578":"code","9342a4b1":"code","814b5b75":"code","5b459d9a":"code","488f69cf":"code","f7a33fc6":"code","d5575ec0":"code","4d40a85a":"code","a26b0d4d":"code","9cf43fe7":"code","536d07bc":"code","2345c5b7":"code","b492ba73":"code","7be65b17":"code","d4675dcc":"code","63f99373":"code","043a1f16":"code","73b144ac":"code","fec22602":"code","71785650":"code","8ded8467":"code","9238e3a3":"code","3574051c":"code","f4f1a127":"code","427fab95":"code","44d92c1e":"code","c09664ed":"code","d3b1559a":"code","668cdfd9":"code","1c8714c6":"code","4df95178":"code","7868091b":"code","c2929c49":"code","15bbec3b":"code","9c418d8e":"code","bd7bdf0c":"code","2ca0472b":"code","fd4c4d40":"code","39f7d4f3":"code","7cfa4e72":"code","a210ee32":"code","ba08cfff":"code","27a518c5":"code","ea9d881f":"code","189cb94d":"code","606277b0":"code","15ea8338":"code","11b7d7e8":"code","fe04653f":"code","48060c4f":"code","09854afa":"code","1b2c3278":"code","21f87e70":"code","165758d9":"code","8e717a63":"code","b86511b0":"markdown","1454dc72":"markdown","7ee7e1e3":"markdown","2e97ce57":"markdown","4819c32c":"markdown","84e8a899":"markdown","09971429":"markdown","33de2b04":"markdown","f7b1f3c6":"markdown","317e406f":"markdown","cb156244":"markdown","d00f45ea":"markdown","b100a433":"markdown","d1ce9ca6":"markdown","5601c4ae":"markdown","e933e76f":"markdown","c0fbd49f":"markdown","c1bf4cbe":"markdown","f1d2f79a":"markdown","a6707656":"markdown","e5e60235":"markdown","b334aa9c":"markdown","61126bbc":"markdown","2256d09f":"markdown","f4c83442":"markdown","6b59f6b4":"markdown","54335b47":"markdown","9baf921f":"markdown","8af1cdc4":"markdown","e8b33ad5":"markdown","f5b386dc":"markdown","44cf3080":"markdown","8790a131":"markdown","b2f6d9be":"markdown","17d8eb93":"markdown","8d7c4c22":"markdown","237cbb3f":"markdown","d3c53199":"markdown","fe9cc68c":"markdown","89f260f5":"markdown","4a0c3028":"markdown","8c38a177":"markdown","07e79f7b":"markdown","562bb261":"markdown","0a446d69":"markdown","7167ca86":"markdown","d07bf8f1":"markdown","a6692582":"markdown","89ee1b5c":"markdown","6b38d1dc":"markdown","4585eb7a":"markdown","8240d2b6":"markdown","dc505c9b":"markdown","a054806b":"markdown","606cd631":"markdown","5b8eead6":"markdown","e6839c56":"markdown","1b5ac70f":"markdown","bff23c23":"markdown","15f3629c":"markdown","ddd94a92":"markdown","c0239260":"markdown","3de066fb":"markdown","f0f881bc":"markdown","28e11b51":"markdown","c9bdf48e":"markdown","444aa18b":"markdown","e78af176":"markdown","3075a7b6":"markdown","4d650717":"markdown","eabe28dd":"markdown","e55a029f":"markdown","4ee50352":"markdown","25a8db25":"markdown","51b631a7":"markdown","bbb588ea":"markdown","5db57789":"markdown","e8cbca5a":"markdown","ddfb9570":"markdown","cc4e85b5":"markdown","f6e0fd91":"markdown","62361fc2":"markdown","4d198502":"markdown","4ab3c997":"markdown","4439f097":"markdown","40f5b0f9":"markdown","34efb96f":"markdown","f298b06f":"markdown","83f24648":"markdown","261ad9c8":"markdown","a492de91":"markdown"},"source":{"f1d94138":"import os\nimport gc\nimport sys\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', 140)\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n%matplotlib inline\nsns.set_style('darkgrid')\n\nfrom sklearn.preprocessing import StandardScaler as scale\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import k_means\nfrom sklearn.model_selection import train_test_split as split \nfrom sklearn.model_selection import GridSearchCV as Grid\nimport xgboost as xgb\nfrom sklearn.metrics import (roc_auc_score, precision_score, recall_score, f1_score,\n                             confusion_matrix, accuracy_score, roc_curve, auc)","0381286e":"df = pd.read_csv('..\/input\/jane-street-market-prediction\/train.csv')","5e4a74f8":"#saving the original dataset length\norg_len = len(df)\ndf.head(2)","39f41195":"df.info()","6006dc34":"df.feature_0 = df.feature_0.astype(np.int8)\ndf.date= df.date.astype(np.int16)\ndf.ts_id = df.ts_id.astype(np.int32)","d37686d3":"for i in df:\n    if df[i].dtype == np.float64:\n        if (((df[i] < .0001) & (df[i] > -.0001)).mean()) > .001:\n            print(i)","ee7ed264":"for i in df:\n    if df[i].dtype == np.float64:\n        if (((df[i] < .0001) & (df[i] > -.0001)).mean()) < .001:\n            df[i] = df[i].astype(np.float32)\n            gc.collect();","64b06c57":"df.info()","a813551c":"df.sort_values(by= ['date','ts_id'],inplace=True)","dbe1516a":"df['action'] = np.where(df['resp'] > 0,1,0)\ndf.action = df.action.astype('category')","18adbb12":"fig = plt.figure(figsize=(16,6))\nax = plt.subplot(1,1,1)\ndf.groupby('date')[['resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp']].sum().cumsum().plot(ax=ax)\nplt.title('Cumulative Sum of Different RESP\\'s',fontsize=18)\nplt.xlabel('Date',fontsize=14)\nplt.axvspan(0,92,linestyle=':',linewidth=2,label='first 92 days',color='darkorange',alpha=.2)\nplt.legend(fontsize=12,ncol=3,loc=2);","dfeef67f":"fig = px.line(df.groupby('date')[['resp_1', 'resp_2', 'resp_3', 'resp_4','resp']].mean(),\n              x= df.groupby('date')[['resp_1', 'resp_2', 'resp_3', 'resp_4','resp']].mean().index,\n              y= ['resp_1', 'resp_2', 'resp_3', 'resp_4','resp'],\n              title= '\\naverage Resp per day')\nfig.layout.xaxis.title = 'Day' \nfig.layout.yaxis.title = 'Avg Resp'\nfig.show()","f30521ac":"fig,((ax11,ax12,ax13),(ax21,ax22,ax23),(ax31,ax32,ax33),(ax41,ax42,ax43),(ax51,ax52,ax53)) = plt.subplots(5,3,figsize=(18,24))\nplt.subplots_adjust(hspace=0.35)\nax11.hist(df.resp,bins=150, color='darkblue',alpha=.6)\nax11.axvline(df.resp.mean()+df.resp.std(),color='darkorange',linestyle=':',linewidth=2)\nax11.axvline(df.resp.mean()-df.resp.std(),color='darkorange',linestyle=':',linewidth=2)\ndf.resp.plot.hist(bins= 150,ax=ax12,color='darkblue',alpha=.6)\nax12.axvline(df.resp.mean()+df.resp.std(),color='darkorange',linestyle=':',linewidth=2)\nax12.axvline(df.resp.mean()-df.resp.std(),color='darkorange',linestyle=':',linewidth=2)\nax12.set_xlim(-.08,.08)\nax13.hist(df.resp,bins=150, color='darkblue',alpha=.6)\nax13.set_yscale('log')\nskew= round(df.resp.skew(),4)\nkurt= round(df.resp.kurtosis())\nstd1= round((((df.resp.mean()-df.resp.std()) < df.resp ) & (df.resp < (df.resp.mean()+df.resp.std()))).mean()*100,2)\nprops = dict(boxstyle='round', facecolor='white', alpha=0.5)\nax11.text(.02,.96,'\u03bc = {}\\nstd = {}\\nskewness = {}\\nkurtosis = {}\\n% values in 1 std = {}%'.format(round(df.resp.mean(),4),round(df.resp.std(),4),skew,kurt,std1),\n         transform=ax11.transAxes, verticalalignment='top',bbox=props,fontsize=10)\nax11.set_title('Resp Hist Normal scale',fontsize=14)\nax12.set_title('Resp Hist normal scale zoomed',fontsize=14)\nax13.set_title('Resp Hist with freq on a log scale',fontsize=14);\nax11.set_xlabel('')\nax11.set_ylabel('')\nax12.set_xlabel('')\nax12.set_ylabel('')\nax13.set_xlabel('')\nax13.set_ylabel('')\nax21.hist(df.resp_1,bins=150,color='darkblue',alpha=.6)\nax21.axvline(df.resp_1.mean()+df.resp_1.std(),color='darkorange',linestyle=':',linewidth=2)\nax21.axvline(df.resp_1.mean()-df.resp_1.std(),color='darkorange',linestyle=':',linewidth=2)\ndf.resp_1.plot.hist(bins= 150,ax=ax22,color='darkblue',alpha=.6)\nax22.axvline(df.resp_1.mean()+df.resp_1.std(),color='darkorange',linestyle=':',linewidth=2)\nax22.axvline(df.resp_1.mean()-df.resp_1.std(),color='darkorange',linestyle=':',linewidth=2)\nax22.set_xlim(-.08,.08)\nax23.hist(df.resp_1,bins=150,color='darkblue',alpha=.6)\nax23.set_yscale('log')\nskew= round(df.resp_1.skew(),4)\nkurt= round(df.resp_1.kurtosis())\nstd1= round((((df.resp_1.mean()-df.resp_1.std()) < df.resp_1 ) & (df.resp_1 < (df.resp_1.mean()+df.resp_1.std()))).mean()*100,2)\nax21.text(.02,.96,'\u03bc = {}\\nstd = {}\\nskewness = {}\\nkurtosis = {}\\n% values in 1 std = {}%'.format(round(df.resp_1.mean(),4),round(df.resp_1.std(),4),skew,kurt,std1),\n         transform=ax21.transAxes, verticalalignment='top',bbox=props,fontsize=10)\nax21.set_title('Resp_1 Hist Normal scale',fontsize=14)\nax22.set_title('Resp_1 Hist normal scale zoomed',fontsize=14)\nax23.set_title('Resp_1 Hist with freq on a log scale',fontsize=14);\nax21.set_xlabel('')\nax21.set_ylabel('')\nax22.set_xlabel('')\nax22.set_ylabel('')\nax23.set_xlabel('')\nax23.set_ylabel('')\nax31.hist(df.resp_2,bins=150,color='darkblue',alpha=.6)\nax31.axvline(df.resp_2.mean()+df.resp_2.std(),color='darkorange',linestyle=':',linewidth=2)\nax31.axvline(df.resp_2.mean()-df.resp_2.std(),color='darkorange',linestyle=':',linewidth=2)\ndf.resp_2.plot.hist(bins= 150,ax=ax32,color='darkblue',alpha=.6)\nax32.axvline(df.resp_2.mean()+df.resp_2.std(),color='darkorange',linestyle=':',linewidth=2)\nax32.axvline(df.resp_2.mean()-df.resp_2.std(),color='darkorange',linestyle=':',linewidth=2)\nax32.set_xlim(-.08,.08)\nax33.hist(df.resp_2,bins=150, color='darkblue',alpha=.6)\nax33.set_yscale('log')\nskew= round(df.resp_2.skew(),4)\nkurt= round(df.resp_2.kurtosis())\nstd1= round((((df.resp_2.mean()-df.resp_2.std()) < df.resp_2 ) & (df.resp_2 < (df.resp_2.mean()+df.resp_2.std()))).mean()*100,2)\nax31.text(.02,.96,'\u03bc = {}\\nstd = {}\\nskewness = {}\\nkurtosis = {}\\n% values in 1 std = {}%'.format(round(df.resp_2.mean(),4),round(df.resp_2.std(),4),skew,kurt,std1),\n         transform=ax31.transAxes, verticalalignment='top',bbox=props,fontsize=10)\nax31.set_title('Resp_2 Hist Normal scale',fontsize=14)\nax32.set_title('Resp_2 Hist normal scale zoomed',fontsize=14)\nax33.set_title('Resp_2 Hist with freq on a log scale',fontsize=14);\nax31.set_xlabel('')\nax31.set_ylabel('')\nax32.set_xlabel('')\nax32.set_ylabel('')\nax33.set_xlabel('')\nax33.set_ylabel('')\nax41.hist(df.resp_3, color='darkblue',alpha=.6,bins=150)\nax41.axvline(df.resp_3.mean()+df.resp_3.std(),color='darkorange',linestyle=':',linewidth=2)\nax41.axvline(df.resp_3.mean()-df.resp_3.std(),color='darkorange',linestyle=':',linewidth=2)\ndf.resp_3.plot.hist(bins=150, color='darkblue',alpha=.6,ax=ax42)\nax42.axvline(df.resp_3.mean()+df.resp_3.std(),color='darkorange',linestyle=':',linewidth=2)\nax42.axvline(df.resp_3.mean()-df.resp_3.std(),color='darkorange',linestyle=':',linewidth=2)\nax42.set_xlim(-.08,.08)\nax43.hist(df.resp_3, color='darkblue',alpha=.6,bins=150)\nax43.set_yscale('log')\nskew= round(df.resp_3.skew(),4)\nkurt= round(df.resp_3.kurtosis())\nstd1= round((((df.resp_3.mean()-df.resp_3.std()) < df.resp_3 ) & (df.resp_3 < (df.resp_3.mean()+df.resp_3.std()))).mean()*100,2)\nax41.text(.02,.96,'\u03bc = {}\\nstd = {}\\nskewness = {}\\nkurtosis = {}\\n% values in 1 std = {}%'.format(round(df.resp_3.mean(),4),round(df.resp_3.std(),4),skew,kurt,std1),\n         transform=ax41.transAxes, verticalalignment='top',bbox=props,fontsize=10)\nax41.set_title('Resp_3 Hist Normal scale',fontsize=14)\nax42.set_title('Resp_3 Hist normal scale zoomed',fontsize=14)\nax43.set_title('Resp_3 Hist with freq on a log scale',fontsize=14);\nax41.set_xlabel('')\nax41.set_ylabel('')\nax42.set_xlabel('')\nax42.set_ylabel('')\nax43.set_xlabel('')\nax43.set_ylabel('')\nax51.hist(df.resp_4,bins=150, color='darkblue',alpha=.6)\nax51.axvline(df.resp_4.mean()+df.resp_4.std(),color='darkorange',linestyle=':',linewidth=2)\nax51.axvline(df.resp_4.mean()-df.resp_4.std(),color='darkorange',linestyle=':',linewidth=2)\ndf.resp_4.plot.hist(bins= 150,color='darkblue',alpha=.6,ax=ax52)\nax52.axvline(df.resp_4.mean()+df.resp_4.std(),color='darkorange',linestyle=':',linewidth=2)\nax52.axvline(df.resp_4.mean()-df.resp_4.std(),color='darkorange',linestyle=':',linewidth=2)\nax52.set_xlim(-.08,.08)\nax53.hist(df.resp_4,bins=150,color='darkblue',alpha=.6)\nax53.set_yscale('log')\nskew= round(df.resp_4.skew(),4)\nkurt= round(df.resp_4.kurtosis())\nstd1= round((((df.resp_4.mean()-df.resp_4.std()) < df.resp_4 ) & (df.resp_4 < (df.resp_4.mean()+df.resp_4.std()))).mean()*100,2)\nax51.text(.02,.96,'\u03bc = {}\\nstd = {}\\nskewness = {}\\nkurtosis = {}\\n% values in 1 std = {}%'.format(round(df.resp_4.mean(),4),round(df.resp_4.std(),4),skew,kurt,std1),\n         transform=ax51.transAxes, verticalalignment='top',bbox=props,fontsize=10)\nax51.set_title('Resp_4 Hist Normal scale',fontsize=14)\nax52.set_title('Resp_4 Hist normal scale zoomed',fontsize=14)\nax53.set_title('Resp_4 Hist with freq on a log scale',fontsize=14)\nax51.set_xlabel('')\nax51.set_ylabel('')\nax52.set_xlabel('')\nax52.set_ylabel('')\nax53.set_xlabel('')\nax53.set_ylabel('')\nfig.suptitle('RESPs Historgrams on Different Scales',fontsize=18,y=.92);","f3bc1f37":"sns.pairplot(df[['resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp']],corner=True);","52db369a":"df.date.unique()","69553c1a":"fig = px.area(data_frame= df.groupby('date')[['resp']].count(),title='Number of operation per day')\nfig.update_traces( showlegend = False)\nfig.layout.xaxis.title = 'Day' \nfig.layout.yaxis.title = 'Number of operations'\nfig.show()","004c6336":"fig = px.area(data_frame= df.groupby('date')[['resp']].sum(),title='Resp sum of operation per day')\nfig.update_traces( showlegend = False)\nfig.layout.xaxis.title = 'Day' \nfig.layout.yaxis.title = 'Resp sum'\nfig.show()","d9cfc04d":"date_df = df.groupby('date')[['resp']].mean()\nstd20 = []\nfor i in range(len(date_df)):\n    if i <20:\n        std20.append(np.nan)\n    else:\n        moving_std = date_df['resp'][i-20:i].std()\n        std20.append(moving_std)\ndate_df['moving_std'] = std20\ndate_df.tail(2)","3cad39f6":"fig = px.line(data_frame=date_df,y=['resp','moving_std'],title='Average Resp & 20 day moving standard deviation')\nfig.layout.xaxis.title = 'Day' \nfig.layout.yaxis.title = 'Avg Resp'\nfig.show()","7eb23ec1":"fig, (ax1,ax2) = plt.subplots(2,1,figsize=(22,14))\ndf.groupby('date')[['resp_1', 'resp_2', 'resp_3', 'resp_4']].std().plot(ax=ax1,color=['steelblue','darkorange','red','green'],alpha=.8)\nax1.axvspan(0,92,linestyle=':',linewidth=2,label='first 92 days',color='yellow',alpha=.1)\ndf.groupby('date')[['resp_1', 'resp_2', 'resp_3', 'resp_4']].std().plot.kde(ax=ax2)\nfig.suptitle('Resp\\'s Std',fontsize=18,y=.96)\nax2.set_xlabel('')\nax1.set_xlabel('')\nax2.set_title('kde of each resp std', fontsize=14)\nax1.set_title('std of Resp\\'s for each trading day',fontsize=14);","9336f9dc":"fig = plt.figure(figsize=(18,7))\ngrid =  gridspec.GridSpec(2,3,figure=fig,hspace=.3,wspace=.2)\nax1 = fig.add_subplot(grid[0, 0])\nax2 = fig.add_subplot(grid[0, 1])\nax3 = fig.add_subplot(grid[1, 0])\nax4 = fig.add_subplot(grid[1, 1])\nax5 = fig.add_subplot(grid[:, 2])\nsns.boxplot(x = df.weight,width=.5,ax=ax1)\nax2.hist(df.weight, color='#404788ff',alpha=.6, bins= list([-.05] + list(10**np.arange(-2,2.24,.05))))\nax2.set_xscale('symlog')\nax2.set_xlim(-.05,227)\nsns.boxplot(x = df.weight[df.weight != 0],width=.5,ax=ax3)\nax1.set_title('Weights including zero weights',fontsize=14)\nax3.set_title('Weights not including zero weights',fontsize=14)\nax2.set_title('Weights including zero weights (log)',fontsize=14)\nax4.set_title('Weights not including zero weights (log)',fontsize=14)\nprops = dict(boxstyle='round', facecolor='white', alpha=0.4)\nax1.text(.2,.9,'\u03bc = {}    std = {}\\nmin = {}    max = {}'.format(round(df.weight.mean(),3),round(df.weight.std(),3),round(df.weight.min(),3),round(df.weight.max(),3)),\n         transform=ax1.transAxes, verticalalignment='top',bbox=props,fontsize=12)\nax3.text(.2,.9,'\u03bc = {}        std = {}\\nmin = {}    max = {}'.format(round(df.weight[df.weight != 0].mean(),3),round(df.weight[df.weight != 0].std(),3),\n                                                              round(df.weight[df.weight != 0].min(),3),round(df.weight[df.weight != 0].max(),3)),\n         transform=ax3.transAxes, verticalalignment='top',bbox=props,fontsize=12)\nax4.hist(df.weight[df.weight !=0],color='#404788ff',alpha=.6,bins=10**np.arange(-2.16,2.24,.05))\nax4.set_xscale('log')\nax4.set_xticks((.01,.03,.1,.3,1,3,10,30,100))\nax4.set_xticklabels((.01,.03,.1,.3,1,3,10,30,100))\nax5.pie(((df.weight==0).mean(),(1-(df.weight==0).mean())),startangle=300,wedgeprops=dict(width=0.5),\n        labels=('Zeros\\n{}%'.format(round((df.weight==0).mean()*100,2)),'Nonzeros\\n{}%'.format(round((1-(df.weight==0).mean())*100,2))),\n        textprops={'fontsize': 12},colors=['#404788ff','#55c667ff'])\nax5.set_title('Zeros vs non-zero weights',fontsize=14)\nax1.set_xlabel('')\nax2.set_xlabel('')\nax3.set_xlabel('')\nax2.set_ylabel('')\nax5.set_ylabel('')\nax4.set_xlabel('');","647538b8":"fig = plt.figure(figsize=(15,10))\nfig.suptitle('Nonzero weights histogram in different scales',fontsize=18)\nax1 = plt.subplot(3,1,1)\nax1.hist(df.weight[df.weight !=0],color='darkblue',alpha=.7, bins=10**np.arange(-2.16,2.23,.05))\nplt.xscale('log')\nplt.xticks((.01,.03,.1,.3,1,3,10,30,100),(.01,.03,.1,.3,1,3,10,30,100))\nax2 = plt.subplot(3,1,2)\nsns.distplot(df.weight[df.weight != 0], color='darkblue', bins=400, ax=ax2) \nax3 = plt.subplot(3,1,3)\nax3.hist(df.weight[(df.weight !=0) & (df.weight < 3.197 )],color='darkblue',alpha=.7, bins=200)\nax3.set_xlim(0,3.3)\nax2.set_xlabel('') \nax1.set_title('All values (log-scale)',fontsize=14)\nax2.set_title('kde of the distribution',fontsize=14)\nax3.set_title('75% of the Values',fontsize=14)\nplt.subplots_adjust(hspace=.4);","472fb21a":"fig, (ax1,ax2) = plt.subplots(2,1,figsize=(16,8))\nfig.suptitle('Weight outliers',fontsize=18)\nsns.boxplot(df.weight,width=.5, ax=ax1)\nax1.axvline(np.percentile(df.weight,95), color= 'green',label='95.0%',linestyle=':',linewidth=3)\nax1.axvline(np.percentile(df.weight,99), color= 'darkblue',label='99.0%',linestyle=':',linewidth=3)\nax1.axvline(np.percentile(df.weight,99.9), color= 'darkorange',label='99.9%',linestyle=':',linewidth=3)\nax1.axvline(np.percentile(df.weight,99.99), color= 'magenta',label='99.99%',linestyle=':',linewidth=3)\nax1.legend(fontsize=13)\nsns.boxplot(df.weight[df.weight !=0],width=.5, ax=ax2)\nax2.axvline(np.percentile(df.weight[df.weight !=0],95), color= 'green',label='95.0%',linestyle=':',linewidth=3)\nax2.axvline(np.percentile(df.weight[df.weight !=0],99), color= 'darkblue',label='99.0%',linestyle=':',linewidth=3)\nax2.axvline(np.percentile(df.weight[df.weight !=0],99.9), color= 'darkorange',label='99.9%',linestyle=':',linewidth=3)\nax2.axvline(np.percentile(df.weight[df.weight !=0],99.99), color= 'magenta',label='99.99%',linestyle=':',linewidth=3)\nax2.legend(fontsize=13)\nax1.set_title('All weights', fontsize= 14)\nax2.set_title('Non-zero weights', fontsize= 14)\nax1.set_xlabel('')\nax2.set_xlabel('');","84d761e7":"sns.scatterplot(data=df, x='resp',y='weight', color= 'blue', alpha=.3)\nplt.title('Resp vs Weight\\ncorrelation={}'.format(round(df.weight.corr(df.resp),4)));","1f531578":"df_f =  pd.read_csv('..\/input\/jane-street-market-prediction\/features.csv')\ndf_f.head(5)","9342a4b1":"fig = px.bar(df_f.set_index('feature').T.sum(), title='Number of tags for each feature')\nfig.layout.xaxis.tickangle = 300\nfig.update_traces( showlegend = False)\nfig.layout.xaxis. dtick = 5\nfig.layout.xaxis.title = ''\nfig.layout.yaxis.title = ''\nfig.show()","814b5b75":"fig = px.bar(x = df.isnull().sum().index,y= df.isnull().sum().values,title= 'Number of Null values')\nfig.layout.xaxis.tickangle = 300\nfig.layout.xaxis. dtick = 5\nfig.layout.yaxis. dtick = 100000\nfig.layout.xaxis.title = ''\nfig.layout.yaxis.title = ''\nfig.layout.xaxis.showgrid = True\nfig.show()","5b459d9a":"nulls = df.isnull().sum()\nnulls_list = list(nulls[nulls >(0.1 * len(df))].index)\nnulls_list","488f69cf":"df[['resp','resp_1','resp_2','resp_3','resp_4','weight']+nulls_list].corr().style.background_gradient(cmap='coolwarm')","f7a33fc6":"df.drop(columns=nulls_list,inplace=True)","d5575ec0":"(df.iloc[:,7:-2].std() \/ df.iloc[:,7:-2].mean()).head(5)","4d40a85a":"df.iloc[:,7:-2].hist(bins=100,figsize=(20,74),layout=(29,4));","a26b0d4d":"fig = plt.figure(figsize=(20,80))\nfig.suptitle('Features Box plot with 0.1% 99.9% whiskers',fontsize=22, y=.89)\ngrid =  gridspec.GridSpec(29,4,figure=fig,hspace=.5,wspace=.05)\nfeatstr = [i for i in df.columns[7:-2]]\ncounter = 0\nfor i in range(29):\n    for j in range(4):\n        subf = fig.add_subplot(grid[i, j]);\n        sns.boxplot(x= df[featstr[counter]],saturation=.5,color= 'blue', ax= subf,width=.5,whis=(.1,99.9));\n        subf.axvline(df[featstr[counter]].mean(),color= 'darkorange', label='Mean', linestyle=':',linewidth=3)\n        subf.set_xlabel('')\n        subf.set_title('{}'.format(featstr[counter]),fontsize=16)\n        counter += 1\n        gc.collect()\nplt.show();","9cf43fe7":"df.fillna(df.mean(axis=0),inplace=True)","536d07bc":"df.groupby('date')[featstr].mean().cumsum().plot(layout=(29,4),subplots=True,figsize=(20,82),xlabel='')\nfig = plt.gcf()\nfig.text(0.5, 0.19, 'Date',ha='center', fontsize = 24)\nfig.suptitle('Cumulative features means per day',fontsize=24,y=.886);","2345c5b7":"corr = df.iloc[:,7:-2].corr()","b492ba73":"corr.style.background_gradient(cmap='coolwarm')","7be65b17":"fig = plt.figure(figsize=(18,12))\nax = plt.subplot(1,1,1)\nsns.heatmap(corr,ax= ax, cmap='coolwarm');","d4675dcc":"featstr2 = [ i for i in featstr if i not in ['feature_41','feature_64']]\nlen(featstr)","63f99373":"fig = plt.figure(figsize=(22,44))\ngrid =  gridspec.GridSpec(12,5,figure=fig,hspace=.5,wspace=.2)\ncounter = 1\nfor i in range(12):\n    for j in range(5):\n        if counter == 113:\n            break\n        subf = fig.add_subplot(grid[i, j]);\n        sns.scatterplot(x= df[featstr2[counter]], y = df[featstr2[counter+1]], ax= subf);\n        cor = round(df[featstr2[counter]].corr(df[featstr2[counter+1]]) * 100,2)\n        subf.set_xlabel('')\n        subf.set_ylabel('')\n        subf.set_title('{} & {}\\nCorrelation = {}%'.format(featstr2[counter],featstr2[counter+1],cor),fontsize=14)\n        counter += 2\n        gc.collect();  ","043a1f16":"plt.figure(figsize=(12,6)) \nsns.heatmap(df[featstr2[15:23]].corr(),center=0,cmap='coolwarm',annot=True,cbar=False);","73b144ac":"sns.pairplot(df[featstr2[15:23]],corner=True);","fec22602":"plt.figure(figsize=(12,6)) \nsns.heatmap(df[featstr2[23:31]].corr(),center=0,cmap='coolwarm',annot=True,cbar=False);","71785650":"sns.pairplot(df[featstr2[23:31]],corner=True);","8ded8467":"plt.figure(figsize=(18,6)) \nsns.heatmap(df[featstr2[15:31]].corr(),center=0,cmap='coolwarm',annot=True,cbar=False);","9238e3a3":"fig = px.bar(df[featstr].mean(), title='Features mean values')\nfig.layout.xaxis.tickangle = 300\nfig.update_traces(showlegend = False)\nfig.layout.xaxis. dtick = 5\nfig.layout.xaxis.title = ''\nfig.layout.yaxis.title = ''\nfig.show()","3574051c":"fig = px.bar(df[featstr].max(), title='Features Max Values')\nfig.layout.xaxis.tickangle = 300\nfig.update_traces(showlegend = False)\nfig.layout.xaxis. dtick = 5\nfig.layout.xaxis.title = ''\nfig.layout.yaxis.title = ''\nfig.show()","f4f1a127":"fig = px.bar(df[featstr].min(), title='Features Min Values')\nfig.layout.xaxis.tickangle = 300\nfig.update_traces(showlegend = False)\nfig.layout.xaxis. dtick = 5\nfig.layout.xaxis.title = ''\nfig.layout.yaxis.title = ''\nfig.show()","427fab95":"fig, (ax1,ax2,ax3)= plt.subplots(3,1,figsize=(10,12))\nplt.subplots_adjust(hspace=.3)\nsns.distplot(df[featstr].max(),ax= ax1 )\nsns.distplot(df[featstr].min(),ax= ax2)\nsns.distplot(df[featstr].mean(),ax= ax3)\nfig.suptitle('distribution of mean max and min for features',fontsize=16)\nax1.set_title('distribution  of features max values',fontsize=14)\nax1.text(.82,.56,'std = {}'.format(round(df[featstr].max().std(),2)),transform=ax1.transAxes, verticalalignment='top',bbox=props,fontsize=12)\nax2.set_title('distribution  of features min values',fontsize=14)\nax2.text(.82,.56,'std = {}'.format(round(df[featstr].min().std(),2)),transform=ax2.transAxes, verticalalignment='top',bbox=props,fontsize=12)\nax3.set_title('distribution  of features mean values',fontsize=14)\nax3.text(.82,.56,'std = {}'.format(round(df[featstr].mean().std(),2)),transform=ax3.transAxes, verticalalignment='top',bbox=props,fontsize=12);","44d92c1e":"for i in featstr[1:]:\n    print('{}\\n0.1%:99.9% are between: {}\\nmax: {}\\nmin: {}\\n75% are under: {}'.format(i,np.percentile(df[i],(.1,99.9)), df[i].max(),df[i].min(),np.percentile(df[i],75)),\n         '\\n===============================')","c09664ed":"df[(df.feature_56== df.feature_56.max())|(df.feature_57== df.feature_57.max())|(df.feature_58== df.feature_58.max()) | (df.feature_59== df.feature_59.max())]","d3b1559a":"n999 = [ np.percentile(df[i],99.9) for i in featstr[1:]]\nn001 = [ np.percentile(df[i],.1) for i in featstr[1:]]","668cdfd9":"for i, j in enumerate(featstr[1:]):\n    df = df[df[j] < n999[i]]\n    gc.collect()","1c8714c6":"str(round(((org_len - len(df))\/org_len)*100,2))+'%'","4df95178":"fig = px.bar(df[featstr].max(), title='Features Max Values')\nfig.layout.xaxis.tickangle = 300\nfig.update_traces(showlegend = False)\nfig.layout.xaxis. dtick = 5\nfig.layout.xaxis.title = ''\nfig.layout.yaxis.title = ''\nfig.show()","7868091b":"fig = plt.figure(figsize=(20,80))\nfig.suptitle('Features Box plot with 0.1% 99.9% whiskers',fontsize=22, y=.89)\ngrid =  gridspec.GridSpec(29,4,figure=fig,hspace=.5,wspace=.05)\ncounter = 0\nfor i in range(29):\n    for j in range(4):\n        subf = fig.add_subplot(grid[i, j]);\n        sns.boxplot(x= df[featstr[counter]],saturation=.5,color= 'blue', ax= subf,width=.5,whis=(.1,99.9));\n        subf.set_xlabel('')\n        subf.set_title('{}'.format(featstr[counter]),fontsize=16)\n        counter += 1\n        gc.collect();","c2929c49":"for i,j in zip(featstr[1:][2:34],n001[2:34]):\n    df = df[df[i] > j]\n    gc.collect();","15bbec3b":"str(round(((org_len - len(df))\/org_len)*100,2))+'%'","9c418d8e":"fig = plt.figure(figsize=(20,80))\nfig.suptitle('KDE plot of Features',fontsize=24,transform =fig.transFigure, y=.89)\ngrid =  gridspec.GridSpec(29,4,figure=fig,hspace=.5,wspace=.01)\ncounter = 0\nfor i in range(29):\n    for j in range(4):\n        subf = fig.add_subplot(grid[i, j]);\n        sns.distplot(df[df.action==0][featstr[counter]],bins= 100,label='Negative',\n                     color='darkorange', kde_kws={'linewidth':4},ax=subf)\n        sns.distplot(df[df.action!=0][featstr[counter]],bins= 100,label='Positive',\n                     color='blue', kde_kws={'alpha':.9,'linewidth':2},hist_kws={'alpha':.3},ax=subf)\n        subf.axvline(np.percentile(df[featstr[counter]],99.5),color= 'darkblue', label='99.5%', linestyle=':',linewidth=2)\n        subf.axvline(np.percentile(df[featstr[counter]],.5),color= 'red', label='0.5%', linestyle=':',linewidth=2)\n        subf.legend().set_visible(False)\n        subf.set_xlabel('')\n        subf.set_title('{}'.format(featstr[counter]),fontsize=16)\n        kurt=round(df[featstr[counter]].kurt(),2)\n        skew=round(df[featstr[counter]].skew(),2)\n        subf.text(.6,.92,'Kurt = {:.2f}\\nSkew = {:.2f}'.format(kurt ,skew),\n         transform=subf.transAxes, verticalalignment='top',bbox=props,fontsize=10)\n        counter += 1\n        gc.collect();\nhandles, labels = subf.get_legend_handles_labels()\nfig.legend(handles, labels,ncol=4, bbox_to_anchor=(0.86, 0.893),fontsize=10,\n           title= 'Resp',title_fontsize=14,bbox_transform =fig.transFigure);","bd7bdf0c":"respcorr =  pd.Series([ df.resp.corr(df[i]) for i in featstr],index=featstr)","2ca0472b":"fig = px.bar(respcorr,color = respcorr, color_continuous_scale=['red','blue'], title= 'Features Correlation with Resp')\nfig.layout.xaxis.tickangle = 300\nfig.layout.xaxis. dtick = 5\nfig.layout.xaxis.title = ''\nfig.layout.yaxis.title = 'pearson correlation'\nfig.update(layout_coloraxis_showscale=False)\nfig.show();","fd4c4d40":"wecorr = pd.Series([df[df.weight != 0].weight.corr(df[df.weight != 0][i]) for i in featstr],index=featstr)","39f7d4f3":"wecorr.head(10)","7cfa4e72":"fig = px.bar(wecorr,title= 'Features Correlation with Weight (not including zero weights)')\nfig.layout.xaxis.tickangle = 300\nfig.layout.xaxis. dtick = 5\nfig.layout.xaxis.title = ''\nfig.layout.yaxis.title = 'pearson correlation'\nfig.update(layout_coloraxis_showscale=False)\nfig.update_layout(showlegend=False)\nfig.show()","a210ee32":"fig = plt.figure(figsize=(8,6))\nsns.scatterplot(df[df.weight != 0].weight,df[df.weight != 0].feature_51, color = 'darkblue', alpha=.3)\nplt.xlabel('Weight',fontsize=14)\nplt.ylabel('Featre_51',fontsize=14)\nplt.title('Feature_51 vs Weight\\nCorrelation = {}%'.format(round(df[df.weight != 0].weight.corr(df[df.weight != 0].feature_51),4)*100),fontsize=16);","ba08cfff":"fig = plt.figure(figsize=(8,6))\nsns.scatterplot(df[df.weight != 0].weight,df[df.weight != 0].feature_126, color = 'darkblue', alpha=.3)\nplt.xlabel('Weight',fontsize=14)\nplt.ylabel('Featre_126',fontsize=14)\nplt.title('Feature_126 vs Weight\\nCorrelation{}%'.format(round(df[df.weight != 0].weight.corr(df[df.weight != 0].feature_126),4)*100),fontsize=16);","27a518c5":"plt.figure(figsize=(7,5)) \ndf.feature_0.value_counts().plot.bar(color='darkblue',alpha=.6,width=.5)\nplt.title('Feature_0',fontsize=18) \nplt.xticks(rotation=0,fontsize=14);","ea9d881f":"plt.figure(figsize=(8,6)) \nsns.countplot(data=df, x='feature_0', hue='action',palette='viridis')\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=13)\nplt.xlabel('Feature 0',fontsize=12)\nplt.title('Feature 0 and Resp', fontsize=18)\nplt.ylabel('')\nplt.xlim(-1,2)\nh, l = plt.gca().get_legend_handles_labels()\nplt.legend(h,['Negative','Positive'],ncol=1, fontsize=12, loc=3,title= 'Resp',title_fontsize=14);","189cb94d":"scaler = scale()","606277b0":"scaler.fit(df[featstr[1:]])","15ea8338":"df_pca = pd.DataFrame(scaler.transform(df[featstr[1:]]))\ndf_pca.columns = featstr[1:]\ngc.collect()\ndf_pca.head()","11b7d7e8":"pca =  PCA(n_components= 8).fit(df_pca)","fe04653f":"df_pca = pd.DataFrame(pca.transform(df_pca))","48060c4f":"pcs = ['pc'+str(i+1) for i in range(8)]","09854afa":"df_pca.columns = pcs\ndf_pca['action'] = df.action.values\ndf_pca['weight'] = df.weight.values\ndf_pca['resp'] = df.resp.values\ndf_pca.head()","1b2c3278":"df_pca.corr().style.background_gradient(cmap='coolwarm')","21f87e70":"kmeans = k_means(n_clusters= 8, max_iter= 400, random_state= 0,X=df_pca[pcs])","165758d9":"df_pca['cluster'] = kmeans[1]\ndf_pca['cluster'] = df_pca['cluster'].astype('category')\ndf_pca.head(8)","8e717a63":"fig = plt.figure(figsize=(12,6))\nax = plt.subplot(1,1,1)\nsns.countplot(data=df_pca,x='cluster',hue='action',ax=ax,palette='viridis')\nh, l = plt.gca().get_legend_handles_labels()\nplt.legend(h,['Negative','Positive'],ncol= 1, fontsize= 12, loc= 1,title= 'Resp',title_fontsize=14)\nplt.xlabel('Clusters',fontsize=18)\nplt.ylabel('')\nplt.xticks(fontsize=14) \nplt.title('PCA Clusters and Resp', fontsize=22);","b86511b0":"### Importing Libraries ","1454dc72":"### Adding the clusters to the PCA dataframe","7ee7e1e3":"### Again we make a pandas series of weight correlation with feature but only taking in consideration weights larger than 0","2e97ce57":"we can see that Resp is highly related to Resp_4\nalso Resp_1 and Resp_2 are highly relted to each other \nfrom the relation shown in this figure and the standard deviation and distribution shown in the figure before this one we can assume that Resp is more related to longer time horizon invest as longer time horizon are associated with more return and higher risk\n\n### [The Basics of Investment Time Horizons](https:\/\/www.investopedia.com\/terms\/t\/timehorizon.asp)\n> ####  An Investment Time Horizon is the period where one expects to hold an investment for a specific goal. Investments are generally broken down into two main categories: stocks (riskier) and bonds (less risky). The longer the Time Horizon, the more aggressive, or riskier portfolio, an investor can build. The shorter the Time Horizon, the more conservative, or less risky, the investor may want to adopt. \n\n[source: investopedia.com](https:\/\/www.investopedia.com\/terms\/t\/timehorizon.asp)","4819c32c":"#### **Looking into the relationship between these features since there is kind of pattern in the number of null values**","84e8a899":"### The following code shows the ratio of observations lost because of cleaning so far","09971429":"<a id='features'><\/a>\n# Features data analysis","33de2b04":"<a id='pca'><\/a>\n## PCA Starter","f7b1f3c6":"### Finding out the features with missing values more than 10 %","317e406f":"<a id='nulls'><\/a>\n## **Exploring the Null values**","cb156244":"<a id='growth'><\/a>\n## Features growth ","d00f45ea":"### We can see that there is a lot of outliers affecting the distribution of each feature.\n### Also since the majority of values are heavily centerd around the mean, we will fill null values using the mean.","b100a433":"### Adding target","d1ce9ca6":"### The following code loop through the dataframe to change float64 columns to float32 only if there is a really few amount of data (a very conservative threshold of 0.1%) between -.0001:.0001 to avoid hurting accuracy of small values columns","5601c4ae":"### Now we explore highly correlated groups of features","e933e76f":"### Plotting a heatmap for features correlation","c0fbd49f":"### Now we check the standard deviation of each resp for each day","c1bf4cbe":"> ### We can see that weight is not linearly correlated with Resp but it's clear that higher weight are only associated with low Resp values","f1d2f79a":"<a id=\"eda\"><\/a>\n# **EDA**","a6707656":"### Plotting the relation between clusters and Resp","e5e60235":"<a id='weight'><\/a>\n## **Weight**","b334aa9c":"### A more statistically oriented exploring to outliers","61126bbc":"<h1><center>Let's dive right in!<\/center><\/h1>\n\n![](https:\/\/media1.tenor.com\/images\/ed3ccde29b0efef4a88e13353f6923ba\/tenor.gif)\n\n","2256d09f":"> ### PCA did't really help as principal components still don't have a clear relation with resp","f4c83442":"> ### Clusters seem to be also scattered with no clear relation between a cluster and resp","6b59f6b4":"> ### we can see a noticeable difference in memory usage","54335b47":"### Finding the ratio of the data lost in removing the outliers","9baf921f":"### Adding weight, Resp and action to the new dataframe","8af1cdc4":"> ### It can be inferred that the dataset has extreme outliers it also worth mentioning that some of outliers are accompanied with large values from neighbor columns which is a result of high multicollinearity between features","e8b33ad5":"> ### The Date seem to contain 2 years of trading data since the trading days of the year are approximately 252 : 253 days\nhttps:\/\/en.wikipedia.org\/wiki\/Trading_day","f5b386dc":"> ### Feature_0 seem to be some kind of binary feature","44cf3080":"### Checking the unique values of date","8790a131":"> ### things looks almost the same with the other cluster, it  also worth mentioning that both of these features clusters are negatively correlated with each other.","b2f6d9be":">### **It looks like there is a lot of multicollinearity between features and also it looks like there is a pattern of couples in the features space and this pattern is kind of broke at some features  like feature_41**","17d8eb93":"#### the coefficient of variation seems unreliable due to the value of the mean being near to zero","8d7c4c22":"### Exploring correlation in the PCA dataframe","237cbb3f":"### We will create a moving standard deviation of 20 days (which is a month of trading) for the average of resp","d3c53199":"<a id='multicollinearity'><\/a>\n## Correlation between features","fe9cc68c":"### Reduce the dimensionality of the data to 8 principal components","89f260f5":"### A deeper look at outliers","4a0c3028":"> <a id='outlier'><\/a>\n## Outliers","8c38a177":"### First we make a correlation pandas series for the relationship between resp and each feature","07e79f7b":"### Now we will make a bold move by removing these extreme outliers that are above 99.9% of feature data\n#### To avoid removing more data while looping through the data set we will make a list of 99.9% mark for each and every single feature, We will also create a list for negative outliers values \"using .1 % mark\" to be explored later ","562bb261":"### **The following code will make a grid of horizontal box plot with the mean ploted too to get a comprehensive solid understanding of the features distributions**\n> #### please note that I used customized 0.1%:99.9% whisker to show extreme outliers since the data is strongly centered.","0a446d69":"<a id='resp'><\/a>\n## **Resp Data Analysis** ","7167ca86":"### now concerning the remaining nulls we will look firstly to the coefficient of variation","d07bf8f1":"#### We start off with featrues: **[feature_19, feature_20, feature_21, feature_22, feature_23, feature_24, feature_25, feature_26, feature_29]** since there is kind of multicollinearity cluster","a6692582":"# <font color='green'>Work in progress  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2592<\/font> ","89ee1b5c":"### First we take a look at the mean of features.","6b38d1dc":"> ### As expected since these are finance related features a lot of features are highly correlate with each other","4585eb7a":"### Now we can take a bird's-eye view  of features distributions","8240d2b6":"<a id='general'><\/a>\n# **General Data Analysis**","dc505c9b":"### Now we make a boxplot grid again with customized .1% : 99.9% whiskers.","a054806b":"> ### By Adding Resp value to the features distributions can now see the following:\n- The histograms of features now have way less outliers and more formal distribution.\n- We can also see that some features like 1, 2, 85, 87, 88 and 91 have many negative outliers values.\n- Some features like 49, 50, 51, 55, 56, 57, 58, and 59 still have many positive outliers.\n- Features distributions are not affected by the resp value.","606cd631":"<a id='f0'><\/a>\n## **Feature 0**","5b8eead6":"### Now we check the lowest correlation","e6839c56":"### first we start with highest correlation which belongs to features 51 ","1b5ac70f":"> ### The dataset still has many obvious outliers especially negative values since we only removed positive outliers so features which used to have symmetrical outliers now have some kind of left skewed outliers.\n>### comparing both of the boxplots (before and after removing positive outliers) we can notice features from feature_3 to feature_40 which used to have symmetrical outliers now changed to have extreme negative outliers after trimming\n\n","bff23c23":"### Features and Weight","15f3629c":"### Since the number of null values in these columns are huge (larger than one quarter of a million!) we will be dropping features with more than 10% null values since there is no correlation with any of these features and Resp's and weight.","ddd94a92":"> ### Many features cumulative mean seem to be linearly growing but some features like 81, 82, and 83 are actually decreasing, and there are some features that fluctuate like feature 3","c0239260":"<a id='date'><\/a>\n## **Date**","3de066fb":"> ### seems like there is not an obvious relation between feature_0 and resp being negative or positive","f0f881bc":"### Now we check the Resp Amount and Number of operations for each day","28e11b51":"### Taking Resp value in consideration","c9bdf48e":"## **Loading Data**","444aa18b":"### Now we plot the average of each Resp","e78af176":"### Assuring that Data is stored by date","3075a7b6":"### We explore largest and lowest correlation coefficients with a scatter plots","4d650717":"### Exploring the possible clusters in the PCA dataframe","eabe28dd":"> ### It can be noticed the there were more gains in the first 92 days which leads to the insight mentioned by many kaggler that it may be a good idea to drop the observations before this point,\n> ### We can also notice that resp_4 has the highest cumulative sum on the other hand resp_1 has the smallest cumulative sum.","e55a029f":"<h3><center>Upvote this notebook and Michelangelo will get his favorite pizza\ud83c\udf55<\/center><\/h3>\n\n![](http:\/\/i.imgflip.com\/1ydu71.gif)","4ee50352":"### Trying to cut corners to save some memory","25a8db25":"### Features and resp Correlation","51b631a7":"> ### We can see that despite the fact the mean values are not that different from each other the min and max values are very deviated with highly skewed distribution","bbb588ea":"# <font color='darkblue'>Jane Street Market Prediction<\/font>\n### This is an extensive data analysis for the jane street market dataset, this notebook will go through the train and features csv's for an extensive exploratory data analysis, Also some data cleaning and preprocessing will be done along the way.\n### <font color='darkred'>Please note the following:<\/font> \n- this is a relatively large notebook with a lot of exhaustive analytics as it meant to be that way to give a comprehensive understanding of the dataset so if you are to copy and run it please note that it will take a considerable amount of time to run ( around 50 minutes) so be patient.\n- this is an exploratory data analysis and not explanatory one so some figures will be kind of complicated, long and full of data as this notebook meant to help data scientists here on kaggle in their model building process.\n- I didn't discuss the hypothesis of the data being synthetic as this point was addressed by other kaggler in detail, and personally I think the pattern in features are because the fact that many financial metrics are correlated with each other.\n- The PCA \/ Clustering part is just a starter for exploring the possible alternatives to reduce the features space size and find some patterns in the dadataset. \n\n\n## [The EDA](#eda) will be devided to three main parts:\n      \n\n## 1- [General EDA](#general)\n  >- [Resp Data Analysis](#resp)\n  >- [Date](#date)\n  >- [Weight](#weight)\n\n## 2 - [Features Data Analysis](#features)\n  >- [Null Values](#nulls)\n  >- [Cumulative growth](#growth)\n  >- [Multicollinearity](#multicollinearity)\n  >- [Outliers](#outlier)\n  >- [Feature 0](#f0)\n\n\n## 3 - [PCA & Clustering](#pcacls)\n  >- [PCA](#pca)\n  >- [Clustering](#cluster)","5db57789":"### The kde's and histograms of features after dropping outliers and taking in consideration Resp value ","e8cbca5a":"> ### It can be noticed that Resp has many fluctuation","ddfb9570":"> ### We can see that features are not really correlated to Resp","cc4e85b5":"> ### It seems that weight is highly correlated to feature 51","f6e0fd91":"> ### While there is some kind negative correlation between weight and feature 126 the relation seem to be weak","62361fc2":"### First we scale the data","4d198502":"> ### Despite the fact that Pearson coefficients of correlation are really high between these features the relationships are not completely linear, also it can be noticed that the outliers affect the shape of scatter plots.","4ab3c997":"### Finding the unique values of Feature 0","4439f097":"### First we make a correlation dataframe","40f5b0f9":"<a id='pcacls'><\/a>\n# PCA & Clustering","34efb96f":"> ### As it was mentioned before the standard deviation seems to increase with resp mostly related to longer time horizon investments\n> ### It can also be noticed that the deviation was kind of higher in the first 100 days as it was mentioned by many kagglers that there may was some kind of trading model adjustment done after the 80th day.","f298b06f":"### Now we check the correlation again but between other group ","83f24648":"### Manual outlier trimming to these features","261ad9c8":"### Loading the features csv","a492de91":"<a id='cluster'><\/a>\n## Clustering"}}