{"cell_type":{"d034fcef":"code","77993453":"code","62c7f689":"code","799439e8":"code","f6602abd":"code","c724b22e":"code","ee4bf99b":"code","193ad4db":"code","26702e40":"code","3e99f249":"code","f69e02be":"code","37e98a20":"code","b1adbbd5":"code","c613a6a2":"code","776e1457":"code","1459e346":"code","5178498f":"code","76581dfb":"code","b8ed0d37":"code","48bf22dc":"code","dff3c78e":"code","b84a570a":"code","b8976233":"code","ddefe182":"code","5b18ed4d":"code","2e5bcc88":"code","7b90e7e9":"code","bbc2286b":"code","7865e222":"code","45ea4f78":"code","3e32d697":"code","f37a642f":"code","04a96dab":"code","b37108f8":"code","dd94bd8f":"code","a53bf5be":"code","0c737aa3":"code","8ab31916":"code","03134ee9":"code","2c89c05c":"code","3a91feaa":"code","385110f7":"code","e6a7c3f1":"code","c3826a84":"code","fe04bf4e":"code","6cf81dfc":"code","70c4353a":"code","0bfa5313":"code","43a404ce":"code","17404beb":"code","03c9eb38":"code","7a821748":"code","395eb82c":"code","8a0746e4":"code","ca620637":"code","8865a55e":"code","6077eea6":"code","bbc6d023":"code","224a8f76":"code","cbb85e6e":"code","473fc929":"code","decfd305":"code","abff67db":"code","881e312e":"code","7fdead3d":"code","5c1cbaa9":"code","c92fabae":"code","947baf85":"code","53c64896":"code","7c4b6ee0":"code","ca81f853":"code","29542d07":"code","ef8d5f97":"code","4815599c":"code","f43bdcc4":"code","03a258c3":"code","9f432ec5":"code","49a8425a":"code","a4cac5c0":"code","4c9290e2":"code","3a5c9670":"markdown","61c09650":"markdown","7dba89f4":"markdown","117782ae":"markdown","6704761d":"markdown","6db4e595":"markdown","5b1f7658":"markdown","289937ed":"markdown","8870c1d7":"markdown","2fb18f93":"markdown","119a7265":"markdown","648cfcc3":"markdown","c67dac2c":"markdown","85ade0e0":"markdown","3be943f0":"markdown"},"source":{"d034fcef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        continue\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","77993453":"from urllib import request\nfrom itertools import product\nimport pickle\n\nimport scipy.stats as ss\nimport missingno as msno\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom IPython.display import HTML\nimport seaborn as sns\nimport cufflinks as cf\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n\nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\ninit_notebook_mode(connected=True)\n\n# import plotly.offline\n# cf.go_offline()\n# cf.set_config_file(offline=False, world_readable=True)\npd.options.plotting.backend =  'matplotlib'#\"plotly\"\ndef plot(fig):\n    return HTML(fig.to_html())\n\n\nimport IPython\ndef display(*dfs):\n    for df in dfs:\n        IPython.display.display(df)","62c7f689":"!python -m pip install --upgrade pip\n!pip install -U scikit-learn","799439e8":"auser = pd.read_csv('\/kaggle\/input\/acea-water-prediction\/Aquifer_Auser.csv', parse_dates=['Date'], dayfirst=True,)\\\n                    .rename(columns={'Date':'date'}).set_index('date').sort_index()\npetrignano = pd.read_csv('\/kaggle\/input\/acea-water-prediction\/Aquifer_Petrignano.csv', parse_dates=['Date'], dayfirst=True,)\\\n                    .rename(columns={'Date':'date'}).set_index('date').sort_index()\ndoganella = pd.read_csv('\/kaggle\/input\/acea-water-prediction\/Aquifer_Doganella.csv', parse_dates=['Date'], dayfirst=True,)\\\n                    .rename(columns={'Date':'date'}).set_index('date').sort_index()\nluco = pd.read_csv('\/kaggle\/input\/acea-water-prediction\/Aquifer_Luco.csv', parse_dates=['Date'], dayfirst=True,)\\\n                    .rename(columns={'Date':'date'}).set_index('date').sort_index()","f6602abd":"rainfall_windows = pickle.load(open('\/kaggle\/input\/water-italy-aquifers-windows-for-ewm\/rainfall_windows.pkl', 'rb'))\nvolume_windows = pickle.load(open('\/kaggle\/input\/water-italy-aquifers-windows-for-ewm\/volume_windows.pkl', 'rb'))\ntemperature_windows = pickle.load(open('\/kaggle\/input\/water-italy-aquifers-windows-for-ewm\/temperature_windows.pkl', 'rb'))\nhydrometry_windows = pickle.load(open('\/kaggle\/input\/water-italy-aquifers-windows-for-ewm\/hydro_windows.pkl', 'rb'))\ndepth_windows = pickle.load(open('\/kaggle\/input\/water-italy-aquifers-windows-for-ewm\/depth_windows.pkl', 'rb'))\n\n\ndepth = []\nfor df, name in zip([auser, doganella, luco, petrignano],\n                       ['auser', 'doganella', 'luco', 'petrignano']):\n    depth.extend([f for f in df.columns if 'Depth' in f ])\n    \nassert len(rainfall_windows) == len(depth)\nassert len(volume_windows) == len(depth)\nassert len(temperature_windows) == len(depth)\nassert len(hydrometry_windows) == 7\nassert len(depth_windows) == 4","c724b22e":"def get_column_category(x):\n    if 'Date' in x:\n        return 'Date'\n    elif 'Rainfall' in x:\n        return 'Rainfall'\n    elif 'Depth' in x:\n        return 'Depth to Groundwater'\n    elif 'Temperature' in x:\n        return 'Temperature'\n    elif 'Volume' in x:\n        return 'Volume'\n    elif 'Hydrometry' in x:\n        return 'Hydrometry'\n    elif 'Lake_Level' in x:\n        return 'Lake Level'\n    elif 'Flow_Rate' in x:\n        return 'Flow Rate'\n    else:\n        return x\n\ntemp_df = pd.DataFrame({'column_name' : auser.columns, 'waterbody_type':'auser'})\ntemp_df = temp_df.append(pd.DataFrame({'column_name' : doganella.columns, 'waterbody_type':'doganella'}))\ntemp_df = temp_df.append(pd.DataFrame({'column_name' : luco.columns, 'waterbody_type':'luco'}))\ntemp_df = temp_df.append(pd.DataFrame({'column_name' : petrignano.columns, 'waterbody_type':'petrignano'}))\n# temp_df = temp_df.append(pd.DataFrame({'column_name' : lake_biliancino_df.columns, 'waterbody_type':'Lake Biliancino'}))\n# temp_df = temp_df.append(pd.DataFrame({'column_name' : river_arno_df.columns, 'waterbody_type':'River Arno'}))\n# temp_df = temp_df.append(pd.DataFrame({'column_name' : water_spring_amiata_df.columns, 'waterbody_type':'Water Spring Amiata'}))\n# temp_df = temp_df.append(pd.DataFrame({'column_name' : water_spring_lupa_df.columns, 'waterbody_type':'Water Spring Lupa'}))\n# temp_df = temp_df.append(pd.DataFrame({'column_name' : water_spring_madonna_df.columns, 'waterbody_type':'Water Spring Madonna'}))\n\ntemp_df['column_category'] = temp_df.column_name.apply(lambda x: get_column_category(x))\n\ntemp_df = temp_df.groupby('waterbody_type').column_category.value_counts().to_frame()\ntemp_df.columns = ['counts']\ntemp_df = temp_df.reset_index(drop=False)\ntemp_df = temp_df.pivot(index='waterbody_type', columns='column_category')['counts']\ntemp_df['n_features'] = temp_df.sum(axis=1)\ntemp_df['n_predict'] = [3, 9, 1, 2]\ntemp_df['fed'] = [None, 'meteoric infiltration', 'meteoric infiltration', 'Chiascio river']\n\nf, ax = plt.subplots(1,1, figsize=(12, 5))\nsns.heatmap(temp_df.drop('fed', axis=1), cmap='Blues', linewidth=1, ax=ax, vmin=0, vmax=10, annot=True)\nax.set_ylabel('')\nax.set_xlabel('')\nax.set_title('Features, Number of Columns and Target Variables', fontsize=16)\nfor tick in ax.xaxis.get_major_ticks():\n    tick.label.set_fontsize(14) \nfor tick in ax.yaxis.get_major_ticks():\n    tick.label.set_fontsize(14)\n    tick.label.set_rotation(45)","ee4bf99b":"# type and whit's fed for each aquifer - from datasets info\n\naquifer_type = pd.DataFrame([{'aquifer': 'auser', 'name': 'SAL', 'type': 'unconfined'}, # 1\n                {'aquifer': 'auser', 'name': 'PAG', 'type': 'unconfined'}, # 2\n                {'aquifer': 'auser', 'name': 'COS', 'type': 'unconfined'}, # 3\n                {'aquifer': 'auser', 'name': 'DIEC', 'type': 'unconfined'}, # 4\n                {'aquifer': 'auser', 'name': 'LT2', 'type': 'confined'},   # 5\n                \n                {'aquifer': 'petrignano', 'name': 'P24', 'type': 'unconfined'}, \n                {'aquifer': 'petrignano', 'name': 'P25', 'type': 'unconfined'}, \n                \n                {'aquifer': 'doganella', 'name': 'Pozzo_1', 'type': 'semi-confined'},\n                {'aquifer': 'doganella', 'name': 'Pozzo_2', 'type': 'semi-confined'},\n                {'aquifer': 'doganella', 'name': 'Pozzo_3', 'type': 'semi-confined'},\n                {'aquifer': 'doganella', 'name': 'Pozzo_4', 'type': 'semi-confined'},\n                {'aquifer': 'doganella', 'name': 'Pozzo_5', 'type': 'semi-confined'},\n                {'aquifer': 'doganella', 'name': 'Pozzo_6', 'type': 'semi-confined'},\n                {'aquifer': 'doganella', 'name': 'Pozzo_7', 'type': 'semi-confined'},\n                {'aquifer': 'doganella', 'name': 'Pozzo_8', 'type': 'semi-confined'},\n                {'aquifer': 'doganella', 'name': 'Pozzo_9', 'type': 'semi-confined'},\n                \n                {'aquifer': 'luco', 'name': 'Podere_Casetta', 'type': None},\n                {'aquifer': 'luco', 'name': 'Pozzo_1', 'type': None},\n                {'aquifer': 'luco', 'name': 'Pozzo_3', 'type': None},\n                {'aquifer': 'luco', 'name': 'Pozzo_4', 'type': None}\n               ]\n                           )\naquifer_type = aquifer_type.merge(temp_df.fed, left_on='aquifer',right_index=True, how='outer')\naquifer_type['features'] = aquifer_type.apply(lambda x: [df.columns for (df,name) in zip([auser, doganella, luco, petrignano],\n                                                                                         ['auser', 'doganella', 'luco', 'petrignano'])\n                                                         if x.aquifer == name][0],\n                                              axis=1)\n\ndef color_none_red(val):\n    color = 'red' if val is None else 'black'\n    return 'color: %s' % color\n\naquifer_type.iloc[:, :-1].style.applymap(color_none_red)","193ad4db":"aquifer_type.drop_duplicates(['type', 'fed'])","26702e40":"from sklearn.preprocessing import LabelEncoder\nfrom geopy.geocoders import Nominatim\nimport folium\n\nlocations = {}\n\nlocations['Settefrati'] = {'lat' : 41.669624, 'lon' : 13.850011 }\nlocations['Velletri'] = {'lat' : 41.6867015, 'lon' : 12.7770433 }\nlocations['Petrignano'] = {'lat' : 43.1029282, 'lon' : 12.5237369 }\nlocations['Piaggione'] = {'lat' : 43.936794, 'lon' : 10.5040929 }\nlocations['S_Fiora'] = {'lat' : 42.854, 'lon' : 11.556 }\nlocations['Abbadia_S_Salvatore'] = {'lat' : 42.8809724, 'lon' : 11.6724203 }\nlocations['Vetta_Amiata'] = {'lat' : 42.8908958, 'lon' : 11.6264863 }\nlocations['Castel_del_Piano'] = {'lat' : 42.8932352, 'lon' : 11.5383804 }\nlocations['Terni'] = {'lat' : 42.6537515, 'lon' : 12.43981163 }\nlocations['Bastia_Umbra'] = {'lat' : 43.0677554, 'lon' : 12.5495816  }\nlocations['S_Savino'] = {'lat' : 43.339, 'lon' : 11.742 }\nlocations['Monteroni_Arbia_Biena'] = {'lat' : 43.228279, 'lon' : 11.4021433 }\nlocations['Monticiano_la_Pineta'] = {'lat' : 43.1335066 , 'lon' : 11.2408464 }\nlocations['Montalcinello'] = {'lat' : 43.1978783, 'lon' : 11.0787906 }\nlocations['Sovicille'] = {'lat' : 43.2806018, 'lon' : 11.2281756 }\nlocations['Simignano'] = {'lat' : 43.2921965, 'lon' : 11.1680079 }\nlocations['Mensano'] = {'lat' : 43.3009594 , 'lon' : 11.0548528 }\nlocations['Siena_Poggio_al_Vento'] = {'lat' : 43.1399762, 'lon' : 11.3832092 }\nlocations['Scorgiano'] = {'lat' : 43.3521445 , 'lon' : 11.15867 }\nlocations['Ponte_Orgia'] = {'lat' : 43.2074581 , 'lon' : 11.2504416 }\nlocations['Pentolina'] = {'lat' : 43.1968029, 'lon' : 11.1754672 }\nlocations['Montevarchi'] = {'lat' : 43.5234999, 'lon' : 11.5675911 }\nlocations['Incisa'] = {'lat' : 43.6558723, 'lon' : 11.4526838 }\nlocations['Camaldoli'] = {'lat' : 43.7943293, 'lon' : 11.8199481 }\nlocations['Bibbiena'] = {'lat' : 43.6955475, 'lon' : 11.817341 }\nlocations['Stia'] = {'lat' : 43.801537, 'lon' : 11.7067347 }\nlocations['Laterina'] = {'lat' : 43.5081823, 'lon' : 11.7102588 }\nlocations['Monteporzio'] = {'lat' : 41.817251, 'lon' : 12.7050839 }\nlocations['Pontetetto'] = {'lat' : 43.8226294, 'lon' : 10.4940843 }\nlocations['Ponte_a_Moriano'] = {'lat' : 43.9083609 , 'lon' : 10.5342488 }\nlocations['Calavorno'] = {'lat' : 44.0217216, 'lon' : 10.5297323 }\nlocations['Borgo_a_Mozzano'] = {'lat' : 43.978948, 'lon' : 10.545703  }\nlocations['Gallicano'] = {'lat' : 44.0606512, 'lon' : 10.435668  }\nlocations['Tereglio_Coreglia_Antelminelli'] = {'lat' : 44.0550548 , 'lon' : 10.5623594 }\nlocations['Lucca_Orto_Botanico'] = {'lat' : 43.84149865, 'lon' : 10.51169066 }\nlocations['Orentano'] = {'lat' : 43.7796506, 'lon' : 10.6583892 }\nlocations['Fabbriche_di_Vallico'] = {'lat' : 43.997647, 'lon' : 10.4279  }\nlocations['Monte_Serra'] = {'lat' : 43.750833, 'lon' : 10.555278 }\nlocations['Mangona'] = {'lat' : 44.0496863, 'lon' : 11.1958797 }\nlocations['Le_Croci'] = {'lat' : 44.0360503, 'lon' : 11.2675661 }\nlocations['Cavallina'] = {'lat' : 43.9833515, 'lon' : 11.2323312 }\nlocations['S_Agata'] = {'lat' : 43.9438247, 'lon' : 11.3089835 }\nlocations['Firenze'] = {'lat' : 43.7698712, 'lon' : 11.2555757 }\nlocations['S_Piero'] = {'lat' : 43.9637372, 'lon' : 11.3182991 }\nlocations['Vernio'] = {'lat' : 44.0440508 , 'lon' : 11.1498804  }\nlocations['Consuma'] = {'lat' : 43.784, 'lon' : 11.585 }\nlocations['Croce_Arcana']  = {'lat' : 44.1323056, 'lon' : 10.7689152 }\nlocations['Laghetto_Verde']  = {'lat' :   42.883, 'lon' : 11.662  }\n\nlocations_df = pd.DataFrame(columns=['city', 'lat', 'lon'] )\n\ndef get_location_coordinates(df, column_type, cluster, target_df):\n    for location in df.columns[df.columns.str.startswith(column_type)]:\n        location = location.split(column_type)[1]\n\n        loc_dict = {}\n        loc_dict['city'] = location\n        loc_dict['cluster'] = cluster\n        loc_dict['type'] = column_type[:-1]\n        loc_dict['lat'] = locations[location]['lat']\n        loc_dict['lon'] = locations[location]['lon']\n\n        target_df = target_df.append(loc_dict, ignore_index=True)\n\n    return target_df\n\nlocations_df = get_location_coordinates(auser, 'Temperature_', 'auser_df', locations_df)\nlocations_df = get_location_coordinates(auser, 'Rainfall_', 'auser_df', locations_df)\n\nlocations_df = get_location_coordinates(doganella, 'Temperature_', 'doganella_df', locations_df)\nlocations_df = get_location_coordinates(doganella, 'Rainfall_', 'doganella_df', locations_df)\n\nlocations_df = get_location_coordinates(luco, 'Temperature_', 'luco_df', locations_df)\nlocations_df = get_location_coordinates(luco, 'Rainfall_', 'luco_df', locations_df)\n\nlocations_df = get_location_coordinates(petrignano, 'Temperature_', 'petrignano_df', locations_df)\nlocations_df = get_location_coordinates(petrignano, 'Rainfall_', 'petrignano_df', locations_df)\n\n# locations_df = get_location_coordinates(lake_biliancino_df, 'Temperature_', 'lake_biliancino_df', locations_df)\n# locations_df = get_location_coordinates(lake_biliancino_df, 'Rainfall_', 'lake_biliancino_df', locations_df)\n\n# locations_df = get_location_coordinates(river_arno_df, 'Temperature_', 'river_arno_df', locations_df)\n# locations_df = get_location_coordinates(river_arno_df, 'Rainfall_', 'river_arno_df', locations_df)\n\n# locations_df = get_location_coordinates(water_spring_amiata_df, 'Temperature_', 'water_spring_amiata_df', locations_df)\n# locations_df = get_location_coordinates(water_spring_amiata_df, 'Rainfall_', 'water_spring_amiata_df', locations_df)\n\n# locations_df = get_location_coordinates(water_spring_lupa_df, 'Temperature_', 'water_spring_lupa_df', locations_df)\n# locations_df = get_location_coordinates(water_spring_lupa_df, 'Rainfall_', 'water_spring_lupa_df', locations_df)\n\n# locations_df = get_location_coordinates(water_spring_madonna_df, 'Temperature_', 'water_spring_madonna_df', locations_df)\n# locations_df = get_location_coordinates(water_spring_madonna_df, 'Rainfall_', 'water_spring_madonna_df', locations_df)\n\n# Drop duplicates\nlocations_df = locations_df.sort_values(by='city').drop_duplicates().reset_index(drop=True)\n\n# Label Encode cluster feature for visualization puposes\nle = LabelEncoder()\nle.fit(locations_df.cluster)\nlocations_df['cluster_enc'] = le.transform(locations_df.cluster)","3e99f249":"m = folium.Map(location=[42.6, 12.4], tiles='cartodbpositron',zoom_start=7)\n\ncolors = ['purple','lightred','green', 'lightblue', 'red', 'blue', 'darkblue','lightgreen', 'orange',  'darkgreen', 'beige',  'pink', 'darkred', 'darkpurple', 'cadetblue',]\nicons = {'Temperature': 'certificate',\n        'Rainfall': 'cloud'}\n\ngeolocator = Nominatim(user_agent='myapplication')\nfor city, gr in locations_df.groupby('city'):\n    if gr.shape[0] > 1: icon = 'th-list' \n    else: icon = icons[gr.iloc[0]['type']]\n    folium.Marker([gr.iloc[0].lat, \n                  gr.iloc[0].lon],\n                  popup=city, \n                  icon=folium.Icon(color=colors[gr.iloc[0].cluster_enc], icon=icon)).add_to(m)\n    \nm","f69e02be":"import json\ngeo_file = json.load(open('\/kaggle\/input\/geo-data-water-italia\/geo_data.json', 'rb'))['features']\n\ngeo_dict = {}\nfor el in geo_file:\n    sea_level = el['description'].split()\n    try:\n        sea_level = float(sea_level[sea_level.index('[m]<\/b>')+1].replace('<br', ''))\n    except:\n        sea_level = None\n    geo_dict[el['name']] = dict(ids=el['id'], lat=el['lat'], lon=el['lon'], latlon=(el['lat'], el['lon']), sea_level=sea_level)    \n\ngeo_data = []\nfor df, name in zip([auser, doganella, luco, petrignano],\n                    ['auser', 'doganella', 'luco', 'petrignano']):\n    features = df.columns\n    features = features.str.replace('Rainfall_', '').str.replace('Depth_to_Groundwater_', '')\\\n    .str.replace('Temperature_', '').str.replace('Volume_', '').str.replace('Hydrometry_', '')\\\n    .str.replace('_', ' ')\n    features = features.str.replace('Tereglio Coreglia Antelminelli', 'Tereglio')\\\n                        .str.replace('Lucca Orto Botanico', 'Lucca (Orto Botanico)')\\\n                        .str.replace('Monte S Quirico', 'Monte S.Quirico')\\\n                        .str.replace('Rainfall_Monticiano_la_Pineta', 'Rainfall_Monticiano_La_Pineta')\\\n    \n    \n    for f in features:\n        try:\n            geo_data.append(dict(**dict(aquifer=name, name=f), **geo_dict[f]))\n        except:\n            continue\n        \ngeo_data = pd.DataFrame(geo_data)\ngeo_data['name'] = geo_data['name'].str.replace('(','').str.replace(')', '').str.replace('.',' ')\n\nfor f in geo_data['name']:\n    geo_data.loc[geo_data['name']==f, 'feature_name'] = [x for x in auser.columns.tolist() + doganella.columns.tolist() + luco.columns.tolist() + petrignano.columns.tolist() \n                                                         if f.replace(' ', '_') in x ]\n\ngeo_data['type'] = geo_data.feature_name.str.split('_').apply(lambda x: x[0])\ngeo_data","37e98a20":"# def autolabel(xx, yy, names):\n#     \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n#     for x, y, ann in zip(xx, yy, names):\n#         ax.annotate('{}'.format(ann),\n#                     xy=(x, y),\n#                     xytext=(0, 3),  # 3 points vertical offset\n#                     textcoords=\"offset points\",\n#                     ha='center', va='bottom')\n        \n        \n# fig, ax = plt.subplots(figsize = (8,8))\n# borders = (10.4, 10.8, 43.7, 44.2)\n# ax.set_xlim(borders[0],borders[1])\n# ax.set_ylim(borders[2],borders[3])\n# for (i, gr), c, add in zip(geo_data.groupby('type'), ('black', 'r', 'b'), (0, 0.0025, 0.005)):\n#     ax.scatter(gr.lon+add, gr.lat, zorder=1, alpha=0.5, c=c, s=30, label=i)\n    \n    \n# autolabel(geo_data.lon, geo_data.lat, geo_data['name'])\n# plt.imshow(plt.imread('\/kaggle\/input\/geo-data-water-italia\/map.png'), \n#            zorder=0, extent=borders, aspect='equal',\n#            alpha=0.75)\n# ax.legend(loc='upper left')","b1adbbd5":"for df in [auser, doganella, luco, petrignano]:\n    shift = df.reset_index().date - auser.reset_index().date.shift(1)\n    assert shift.value_counts().shape[0] == 1","c613a6a2":"def plot_corr_all(features_pattern):\n    if isinstance(features_pattern, str): features_pattern = [features_pattern]\n    \n    fig, axes = plt.subplots(2, 2, figsize=(15*2, 12*2))\n    fig.subplots_adjust(wspace = 0.3, hspace=0.3)\n\n    for ax, df, name in zip(axes.ravel(), [auser, doganella, luco, petrignano],\n                           ['auser', 'doganella', 'luco', 'petrignano']):\n        features = [f for f in df.columns if 'Depth' in f]\n        for pattern in features_pattern:\n            features.extend([f for f in df.columns if pattern in f])\n            \n        if features_pattern[0] == 'all':\n            features = df.columns\n        if name=='petrignano': annot, cbar = True, True\n        else: annot, cbar = True, False\n        sns.heatmap(df[features].corr(), annot=annot, ax=ax, cmap='coolwarm', cbar=cbar, vmin=-1, vmax=1)\n        ax.set_title(name)\n        \nplot_corr_all('all')","776e1457":"fig, axes = plt.subplots(1, 4, figsize=(8*4, 5))\n\nfor ax, df, name in zip(axes.ravel(), [auser, doganella, luco, petrignano],\n                       ['auser', 'doganella', 'luco', 'petrignano']):\n    ax.set_title(name)\n    msno.matrix(df, figsize=(10,8), fontsize=10, ax=ax)","1459e346":"for df, name in zip([auser, doganella, luco, petrignano],\n                    ['auser', 'doganella', 'luco', 'petrignano']):\n    print(f'{name}:\\t{df.index.min().date()} - {df.index.max().date()}')","5178498f":"# features_rainfall = [f for f in auser.columns if 'Rainfall' in f]\n# features_depth = [f for f in auser.columns if 'Depth' in f]\n# features_volume = [f for f in auser.columns if 'Volume' in f]\n# features_temp = [f for f in auser.columns if 'Temperature' in f]\n# features_hydro = [f for f in auser.columns if 'Hydrometry' in f]\n\n# start_date = auser[features_rainfall][auser[features_rainfall].isna().all(axis=1)]\\\n#                     .index[-1]\n# auser = auser[auser.index > start_date]","76581dfb":"def get_time_features(df):\n    df['year'] = df.index.year\n    df['month'] = df.index.month.astype('category')\n    df['weekofyear'] = df.index.isocalendar().week.astype('category')\n    df['day'] = df.index.day.astype('category')\n    df['dayofyear'] = df.index.dayofyear.astype('category')\n    df['quarter'] = df.index.quarter.astype('category')\n    ","b8ed0d37":"get_time_features(auser)\nget_time_features(doganella)\nget_time_features(luco)\nget_time_features(petrignano)","48bf22dc":"def plot_data_by_year(auser, f, tresholds):\n    col = 2\n    row = auser.year.nunique() \/\/ 2 + 1\n    fig, axes = plt.subplots(row, col ,figsize=(10*col,2*row),)\n    for (y,gr), ax in zip(auser.groupby('year')[f], axes.ravel()):\n        gr.plot(ax=ax, label=y)\n        ax.legend()\n        \n    for th in tresholds:\n        for (y,gr), ax in zip(auser.groupby('year')[f], axes.ravel()):\n            th_full = pd.Timestamp( f'{y}-{th}')\n            ax.axvline(x=th_full, color='black', linestyle='--')\n    fig.suptitle(f)\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.95)","dff3c78e":"# auser dataset\nfeatures_depth = [f for f in auser.columns if 'Depth' in f]\nauser[auser.index.year >= 2005][features_depth].plot(subplots=True, layout=(3,2), figsize=(10*2,3*3));","b84a570a":"(auser[features_depth] == 0).sum()","b8976233":"auser[features_depth] = auser[features_depth].replace(0, np.nan)","ddefe182":"features_depth = [f for f in auser.columns if 'Depth' in f]\nauser[auser.index.year >= 2005][features_depth].plot(subplots=True, layout=(3,2), figsize=(10*2,3*3));","5b18ed4d":"# doganella dataset\nfeatures_depth = [f for f in doganella.columns if 'Depth' in f]\ndoganella[doganella.index.year >= 2012][features_depth].plot(subplots=True, layout=(3,3), figsize=(10*3,3*3));","2e5bcc88":"features_depth = [f for f in doganella.columns if 'Depth' in f]\ndoganella[doganella.index.year >= 2020][features_depth[1]].iloc[100:].plot(subplots=True, layout=(3,3), figsize=(10*3,3*3), style='-o');","7b90e7e9":"features_depth = [f for f in doganella.columns if 'Depth' in f]\ndoganella[doganella.index.year >= 2020][features_depth[-2]].iloc[100:].plot(subplots=True, layout=(3,3), figsize=(10*3,3*3), style='-o');","bbc2286b":"luco.rename(columns=lambda x: str(x).replace('Groundwater_Pozzo', 'Groundwater_Pozzo_luco')).columns","7865e222":"# luco dataset\nluco.columns = luco.rename(columns=lambda x: str(x).replace('Groundwater_Pozzo', 'Groundwater_Pozzo_luco')).columns\nfeatures_depth = [f for f in luco.columns if 'Depth' in f]\nluco[luco.index.year >= 2008][features_depth].plot(subplots=True, layout=(2, 2), figsize=(10*2,2*3));","45ea4f78":"(luco[features_depth] == 0).sum()","3e32d697":"luco[features_depth] = luco[features_depth].replace(0, np.nan)\nfeatures_depth = [f for f in luco.columns if 'Depth' in f]\nluco[luco.index.year >= 2008][features_depth].plot(subplots=True, layout=(2, 2), figsize=(10*2,2*3));","f37a642f":"# petrignano dataset\nfeatures_depth = [f for f in petrignano.columns if 'Depth' in f]\npetrignano[petrignano.index.year >= 1800][features_depth].plot(subplots=True, layout=(1, 2), figsize=(10*2,1*3));","04a96dab":"# for 0-24 hours range\nfeatures = ['Rainfall_Scorgiano', 'Rainfall_Pentolina']\n\ndownloaded_features0_24 = pd.DataFrame(columns=['date'])\nfor ids in geo_data[geo_data.feature_name.isin(features)].ids:\n#                                 https:\/\/www.sir.toscana.it\/archivio\/download.php?IDST=pluvio0_24&IDS=TOS03002742\n    context = request.urlopen(f'https:\/\/www.sir.toscana.it\/archivio\/download.php?IDST=pluvio0_24&IDS={ids}').read().decode('utf-8')\n    to_add = pd.DataFrame([x.split(';') for x in context[context.find(\"gg\/mm\/aaaa\") -1:].replace('@', '').replace(',','.').split('\\r\\n')])\n    to_add = to_add.iloc[1:, :-1]\n    to_add.columns = ['date', f'{geo_data.loc[geo_data.ids==ids, \"feature_name\"].iloc[0]}']\n    \n    downloaded_features0_24 = downloaded_features0_24.merge(to_add, on='date', how='outer')\n    \ndownloaded_features0_24.date = pd.to_datetime(downloaded_features0_24.date, dayfirst=True)\ndownloaded_features0_24.set_index('date', inplace=True)\ndownloaded_features0_24.sort_index(inplace=True)\ndownloaded_features0_24 = downloaded_features0_24.iloc[:-1]\ndownloaded_features0_24 = downloaded_features0_24.replace('', np.nan).astype('float')\n    \n# replace our data with outliers on downloaded one\nfor f in features:\n    luco = luco.merge(downloaded_features0_24[f], left_index=True, right_index=True, how='left', suffixes=['_drop', ''])\n    luco.drop(f'{f}_drop', axis=1, inplace=True)","b37108f8":"# replace 0 on nan\nfeatures_volume = [f for f in auser.columns if 'Volume' in f]\nauser[features_volume] = auser[features_volume].replace(0, np.nan)\n\n# doganella volumes are positive. Make them negative, like other volumes in datasets\nfeatures_volume = [f for f in doganella.columns if 'Volume' in f]\ndoganella[features_volume] = -doganella[features_volume]","dd94bd8f":"# replace 0 on nans, if two or more 0 follow to each other\nfor df, name in zip([auser, doganella, luco, petrignano],\n                    ['auser', 'doganella', 'luco', 'petrignano']):\n    features_temp = [f for f in df.columns if 'Temperature' in f]\n    for f in features_temp:\n        temp = df[f].copy().to_frame()\n        temp['is0'] = False\n        temp['is0'][temp[f].notna()] = temp[f].dropna().rolling(2).mean() == 0\n        df[f][temp.is0] = np.nan","a53bf5be":"features_hydrometry = [f for f in df.columns if 'Hydrometry' in f]\npetrignano[features_hydrometry] = petrignano[features_hydrometry].replace(0, np.nan)","0c737aa3":"def get_ewm_dataset(df, name, delta_win=0):\n    df_total = pd.DataFrame()\n\n    features_depth = [f for f in df.columns if 'Depth' in f]\n    for f_main in features_depth:\n        temp = df[f_main].rename('target').to_frame()\n        temp['dataset'] = name\n        temp['target_name'] = f_main\n        for win_for_features in [rainfall_windows, volume_windows, temperature_windows, hydrometry_windows, depth_windows]:\n            if f_main not in win_for_features.keys(): # if feature (like hydropmetry) not in dataset\n                continue\n\n            for f_ewm, window in win_for_features[f_main].items():\n                if f_ewm not in df.columns:\n                    continue\n                if delta_win < 0 and window < np.abs(delta_win):\n                    delta_win = -window\n                temp[f_ewm] = df[f_ewm].ewm(window+delta_win,  min_periods=65).mean()\n\n        df_total = pd.concat([df_total, temp])\n\n    return df_total\n\n\ndef plot_all_features_grouped_target(df_total):\n    for i, gr in df_total.groupby('target_name'):\n#         display(gr)\n        ncols = 3\n        nrows = (gr.shape[1] - 2) \/\/ ncols\n        nrows = nrows + 1 if  (gr.shape[1]-2) % ncols != 0 else nrows\n        fig, ax = plt.subplots(figsize=(ncols*7,nrows*1.5))\n        gr.plot(subplots=True, layout=(nrows,ncols), ax=ax)","8ab31916":"temp = {'Depth_to_Groundwater_PAG': 'Groundwater_PAG',\n              'Depth_to_Groundwater_DIEC': 'Groundwater_DIEC'}\nauser.rename(columns=temp, inplace=True)\nfor k,v in depth_windows.items():\n    for k2,v2 in temp.items():\n        try:\n            depth_windows[k][v2] =  depth_windows[k][k2]\n            depth_windows[k].pop(k2)\n        except KeyError:\n            continue\n            \n            \ntemp = {'Depth_to_Groundwater_Pozzo_luco_1':'Groundwater_Pozzo_luco_1',\n                     'Depth_to_Groundwater_Pozzo_luco_3':'Groundwater_Pozzo_luco_3',\n                     'Depth_to_Groundwater_Pozzo_luco_4': 'Groundwater_Pozzo_luco_4'}\nluco.rename(columns=temp, inplace=True)\nfor k,v in depth_windows.items():\n    for k2,v2 in temp.items():\n        try:\n            depth_windows[k][v2] =  depth_windows[k][k2.replace('_luco', '')]\n            depth_windows[k].pop(k2.replace('_luco', ''))\n        except KeyError:\n            continue","03134ee9":"auser_df = get_ewm_dataset(auser, 'auser')\nplot_all_features_grouped_target(auser_df)","2c89c05c":"doganella_df = get_ewm_dataset(doganella, 'doganella')\nplot_all_features_grouped_target(doganella_df)","3a91feaa":"luco_df = get_ewm_dataset(luco, 'luco')\nplot_all_features_grouped_target(luco_df)","385110f7":"petrignano_df = get_ewm_dataset(petrignano, 'petrignano')\nplot_all_features_grouped_target(petrignano_df)","e6a7c3f1":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit","c3826a84":"auser_df.sort_index(inplace=True)\ndoganella_df.sort_index(inplace=True)\nluco_df.sort_index(inplace=True)\npetrignano_df.sort_index(inplace=True)","fe04bf4e":"fig, ax = plt.subplots(figsize=(15,5))\nfor df in [auser_df, doganella_df, luco_df, petrignano_df]:\n    for i, gr in df.groupby('target_name'):\n        temp = gr.target.dropna()\n        ax.plot([temp.index.min(), temp.index.max()], [i,i], marker='|')","6cf81dfc":"print(luco_df.groupby('target_name').apply(lambda x: x.target.dropna().index.min())[-1], luco_df[luco_df.target.notna()].index.max())\n(luco_df[luco_df.target.notna()].index.max() - luco_df.groupby('target_name').apply(lambda x: x.target.dropna().index.min())[-1])","70c4353a":"1033\/365","0bfa5313":"(1033-181)\/365","43a404ce":"def split_by_period(df, test_period = 181):\n    train, test = pd.DataFrame() ,pd.DataFrame()\n    for i,gr in df.groupby('target_name', as_index=False):\n        test_start = gr.index.max() - pd.Timedelta(test_period, unit='d')\n        train = pd.concat([train, gr[gr.index < test_start]])\n        test = pd.concat([test,  gr[gr.index >= test_start]])\n    return train, test\n        \nauser_train, auser_test = split_by_period(auser_df)\ndoganella_train, doganella_test = split_by_period(doganella_df)\nluco_train, luco_test = split_by_period(luco_df)\npetrignano_train, petrignano_test = split_by_period(petrignano_df)","17404beb":"!pip install neptune-client","03c9eb38":"from sklearn.model_selection import cross_validate\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n\nimport lightgbm as lgb\n\nfrom statsmodels.stats.weightstats import _tconfint_generic\n\nimport neptune\n# from neptunecontrib.monitoring.lightgbm import neptune_monitor\nimport pickle\nneptune_token = pickle.load(open('\/kaggle\/input\/tokens\/neptune_token.pkl', 'rb'))\nneptune.init(project_qualified_name='declot\/Water-Italy-aquifer', # change this to your `workspace_name\/project_name`\n             api_token=neptune_token, # change this to your api token\n            )\n# neptune.init(project_qualified_name='declot\/Water-Italy-aquifer', # change this to your `workspace_name\/project_name`\n#              api_token='ANONYMOUS'#neptune_token, # change this to your api token\n#             )","7a821748":"def plot_eval_hist(ax, metric_by_iter_val, metric_by_iter_train, n_estim):\n    ax.plot(range(n_estim), metric_by_iter_train, label='train')\n    ax.plot(range(n_estim), metric_by_iter_val, label='val')\n    ax.set_xlabel('n_estimators')\n    ax.set_ylabel('rmse')\n    ax.legend() \n                   \ndef plot_eval_history_rf(ax, model, n_estim, X_train, y_train ,X_val, y_val):\n    metric_by_iter_val, metric_by_iter_train = [], []\n    for i in range(n_estim):\n        pred = model.estimators_[i].predict(X_val)\n        metric_by_iter_val.append(mean_absolute_percentage_error(y_val, pred))\n        pred = model.estimators_[i].predict(X_train)\n        metric_by_iter_train.append(mean_absolute_percentage_error(y_train, pred))\n    plot_eval_hist(ax, metric_by_iter_val, metric_by_iter_train, n_estim)\n    \ndef plot_eval_history_lgb(ax, eval_hist):\n    metric_by_iter_val, metric_by_iter_train = eval_hist['val']['rmse'], eval_hist['train']['rmse']\n    n_estim = len(metric_by_iter_val)\n    plot_eval_hist(ax, metric_by_iter_val, metric_by_iter_train, n_estim)\n       \n\n            \n            \n# def plot_eval_history_lgb(ax, model, n_estim, X_train, y_train ,X_val, y_val):\n    \n#     ax.annotate(f_name, xy=(0, 0.5), xytext=(-ax.yaxis.labelpad - 5, 0),\n#         xycoords=ax.yaxis.label, textcoords='offset points',\n#         size='large', ha='right', va='center')\n#     ax.plot(range(n_estim, metric_by_iter_train, label='train')\n#     ax.plot(range(n_estim), metric_by_iter_val, label='val')\n#     ax.set_xlabel('n_estimators')\n#     ax.set_ylabel('mape')\n#     ax.legend()\n            \ndef plot_res_cv_model(res_cv, cv, gr, axes, tags, f_name):\n    train_idx, val_idx = list(cv.split(gr))[-1]\n    train_idx, val_idx = list(cv.split(gr))[-1]\n    X_val = gr.drop(['target', 'dataset', 'target_name'], axis=1).iloc[val_idx]\n    y_val, y_train = gr.target.iloc[val_idx], gr.target.iloc[train_idx]\n    X_train = gr.drop(['target', 'dataset', 'target_name'], axis=1).iloc[train_idx]\n\n    if 'rf' in tags:\n        model = res_cv['estimator'][-1].fit(X_train, y_train)\n        plot_eval_history_rf(axes[0], model, params_rf['n_estimators'],\n                             X_train, y_train ,X_val, y_val)\n    elif 'lgb' in tags:\n        eval_hist = {}\n        model = res_cv['estimator'][-1].fit(X_train, y_train, \n                                            eval_set=[(X_val, y_val), (X_train, y_train)],\n                                            eval_names=('val','train' ), verbose=False,\n                                            callbacks=[lgb.record_evaluation(eval_hist)])\n        plot_eval_history_lgb(axes[0], eval_hist)\n        m = -np.mean(res_cv['test_score'])\n        axes[0].plot([0, len(eval_hist['val']['rmse'])],\n                     [m]*2, color='black', linestyle='--')\n        ci = _tconfint_generic(m, np.std(res_cv['test_score'])\/np.sqrt(cv.n_splits), cv.n_splits-1, 0.05, '2s')\n        axes[0].fill_between([0, len(eval_hist['val']['rmse'])], [ci[0]]*2, [ci[1]]*2, \n                             color='black', alpha=0.25)\n        \n    y_pred = model.predict(X_val)\n    resid = y_val - y_pred\n    \n    m = -np.mean(res_cv['test_score'])\n    axes[0].annotate(f_name, xy=(0, 0.5), xytext=(-axes[0].yaxis.labelpad-5, 0),\n            xycoords=axes[0].yaxis.label, textcoords='offset points',\n            size='large', ha='right', va='center')\n    \n    axes[0].annotate(f'{round(m, 3)}', \n                     xy=(0, 0.), xytext=(-axes[0].yaxis.labelpad-5, 0),\n            xycoords=axes[0].yaxis.label, textcoords='offset points',\n            size='large', ha='right', va='center')\n    axes[1].plot(y_val, label='val true')\n    axes[1].plot(pd.Series(y_pred, index=y_val.index), label='val pred')\n    axes[1].legend()\n\n    axes[2].scatter(y_val, y_pred)\n    axes[2].plot(y_val, y_val, linestyle='--', c='black')\n    axes[2].set_xlabel('y_true')\n    axes[2].set_title('y_pred')\n\n    axes[3].scatter(y_val, resid)\n    axes[3].plot(y_val, [0]*y_val.shape[0], linestyle='--', c='black')\n    axes[3].set_xlabel('y_ture')\n    axes[3].set_title('residuals')\n            \n\n#     return to_return, list(df.groupby('target_name').indices.keys())\n\n\n\n#             for i, (train_idx, val_idx) in enumerate(cv.split(gr), 1):\n#                 temp = gr.drop(['target', 'dataset', 'target_name'], axis=1)\n#                 X_train, y_train = temp.iloc[train_idx], gr.target.iloc[train_idx]\n#                 X_val, y_val = temp.iloc[val_idx], gr.target.iloc[val_idx]\n                \n#                 train, val = lgb.Dataset(X_train, y_train), lgb.Dataset(X_val, y_val)\n#                 res_cv = lgb.train(params, train, 100, verbose_eval=10, \n#                                    valid_sets=[train, val], valid_names=['train','valid'],\n# #                                    callbacks=[neptune_monitor(prefix=f'cv{i}_')]\n#                                     )\n\n\n#             train_dataset = lgb.Dataset(gr.drop(['target', 'dataset', 'target_name'], axis=1).iloc[train_idx], \n#                                   gr.target.iloc[train_idx])\n#             val_dataset = lgb.Dataset(gr.drop(['target', 'dataset', 'target_name'], axis=1).iloc[val_idx], \n#                                   gr.target.iloc[val_idx], free_raw_data=False)\n#             model = lgb.train(params, train_dataset, params['num_iter'], (val_dataset), verbose_eval=0)\n#             y_pred = model.predict(val_dataset.get_data())\n#             y_true = val_dataset.get_label()\n#             resid = y_true - y_pred\n    \n    \n\ndef model_lgb_cv(temp, params, n, cv, categorical='auto'):\n    dataset = lgb.Dataset(temp.drop(['target', 'dataset', 'target_name'], axis=1), temp.target)\n    return lgb.cv(params, dataset, n, folds=cv, shuffle=False, eval_train_metric=True,\n        verbose_eval=0)\n\n\ndef model_lgb_sklearn_cv(temp, params, cv):\n    model = lgb.LGBMRegressor(**params)\n    return cross_validate(model, temp.drop(['target', 'dataset', 'target_name'], axis=1), temp.target,\n                            scoring='neg_root_mean_squared_error', cv=cv, \n                            return_train_score=True, return_estimator=True, n_jobs=-1)\n\ndef plot_iter_score_cv(ax, res_cv, dataset='train', metric='rmse'):\n    template = f'{dataset} {metric}'\n    x = range(len(res_cv[f'{template}-mean']))\n    ax.plot(x, res_cv[f'{template}-mean'], label=dataset)\n    ci = _tconfint_generic(res_cv[f'{template}-mean'], \n                       np.array(res_cv[f'{template}-stdv'])\/np.sqrt(cv.get_n_splits()),\n                       dof=cv.get_n_splits()-1, alpha=0.05, alternative='2s')\n    ax.fill_between(x, ci[0], ci[1], alpha=0.25)\n    ax.set_title(metric)\n    ax.legend()\n\n    if dataset == 'valid':\n        y = res_cv[f'{template}-mean'][-1]\n        ax.plot(x, [y]*len(x), '--')\n        ax.text(x[-1]-10, y+0.1*y, f'{round(y, 3)}')","395eb82c":"# for df, df_name in zip([auser_train, doganella_train, luco_train, petrignano_train],\n#                         ['auser', 'doganella', 'luco', 'petrignano']):\n\ndef train(df, dataset_name, params, params_exp,\n          name_exp='lgb', tags=['lgb'], plot_res=True):\n    if name_exp is not None:\n        tags = tags + [dataset_name]\n        neptune_exp = neptune.create_experiment(params=dict(**params, **params_exp),\n                                                name=name_exp, tags=tags)\n    ncols, nrows = 4, df.target_name.nunique()\n    if plot_res:\n        fig, axes_all = plt.subplots(nrows, ncols, figsize=(ncols*4, nrows*3))\n    else: axes_all = np.zeros((nrows,))   \n\n    to_return = [[],[], []]\n    for (f_name, gr), axes in zip(df.groupby('target_name', sort=False), axes_all):  \n        f_name = f_name.replace('Depth_to_Groundwater_', '')\n        gr = preprocess(gr) # common preprocces data\n        n_splits = 10 if (gr.shape[0]-1) \/\/ 181 > 10 else (gr.shape[0]-1) \/\/ 181\n        cv = TimeSeriesSplit(n_splits=n_splits, test_size=181)\n\n        if 'rf' in tags:\n            pipe = RandomForestRegressor(**params, n_jobs=-1)\n            res_cv = cross_validate(pipe, gr.drop(['target', 'dataset', 'target_name'], axis=1), gr.target,\n                                   scoring='neg_root_mean_squared_error', cv=cv, \n                                   return_train_score=True, return_estimator=True, n_jobs=-1)\n        elif 'lgb' in tags:\n#                 res_cv = model_lgb_sklearn_cv(gr, params, cv) # fit cv\n            pipe = lgb.LGBMRegressor(**params)\n            res_cv = cross_validate(pipe, gr.drop(['target', 'dataset', 'target_name'], axis=1), gr.target,\n                        scoring='neg_root_mean_squared_error', cv=cv, \n                        return_train_score=True, return_estimator=True, n_jobs=-1)\n#         print(res_cv['test_score'])\n        to_return[0].append(-np.mean(res_cv['test_score']))\n        to_return[1].append(np.std(res_cv['test_score'])\/np.sqrt(n_splits))\n        to_return[2].append(n_splits)\n        if name_exp is not None: \n            neptune.log_metric(f'{dataset_name}_{f_name}_rmse',\n                               -np.mean(res_cv['test_score'])) \n\n        if plot_res:\n            print('---------------------------')\n            print(f_name)\n            print('cv nsplit', n_splits)\n            print(-np.mean(res_cv['test_score']))\n\n            plot_res_cv_model(res_cv, cv, gr, axes, tags=tags, f_name=f_name)\n\n    if name_exp is not None:\n        neptune.log_image(f'{dataset_name}', fig)\n        neptune.stop()\n        \n    return to_return","8a0746e4":"# neptune.stop()","ca620637":"params_exp = dict(future_days=0)\nname = 'auser dropall'\nparams_lgb = dict(n_estimators=100,\n                  objective='regression_l2',\n                  lambda_l2=2,\n                  metrics=['rmse', 'mape'],\n                 #linear_tree,\n                 learning_rate=0.1,\n                 num_leaves=31,\n                 extra_trees=False,\n#                  early_stopping_rounds=5\n                 )\n\nparams_rf = dict(n_estimators=100, max_depth=8, max_features=0.3,\n                min_samples_leaf=5, ccp_alpha=0)\n\ndef preprocess(gr):\n    assert gr.shape == gr.resample('1d').last().shape\n#     temp = gr[gr.target.notna()]\n    temp = gr.dropna()\n    get_time_features(temp)\n    temp.drop('year', axis=1, inplace=True)\n#     print(f'data shape:{temp.shape}')\n    return temp\n    \n    \ntrain(doganella_df, 'doganella', params_lgb, params_exp,\n      name_exp='doganella_ini', tags=['lgb'], plot_res=True)\n","8865a55e":"params_rf = dict(n_estimators=100, max_depth=4, max_features=0.3,\n                min_samples_leaf=5, ccp_alpha=0)\n  \n    \ntrain(doganella_df, 'doganella', params_rf, params_exp,\n      name_exp='doganella ini rmse', tags=['rf'], plot_res=True)\n","6077eea6":"from tqdm.notebook import tqdm\nfrom matplotlib import cm","bbc6d023":"def model_metrics_by_lag_delta_windows(df, dataset_name, lags, delta_windows,\n                                      params, params_exp):\n    n = len([f for f in df.columns if 'Depth' in f])\n    results_mean = np.zeros((n, lags.shape[0], delta_wins.shape[0]))\n    results_std = np.zeros((n, lags.shape[0], delta_wins.shape[0]))\n    cvs = np.zeros((n, lags.shape[0], delta_wins.shape[0]))\n    for (i,l),(j,w) in tqdm(product(enumerate(lags), enumerate(delta_wins)), \n                            total=lags.shape[0] * delta_wins.shape[0]):\n        temp_df = get_ewm_dataset(df, dataset_name, delta_win=w)\n        temp_df.target = temp_df.groupby('target_name', sort=False).target.shift(-l)\n        res = train(temp_df, dataset_name, params, params_exp,\n                    name_exp=None, tags=['rf'], plot_res=False)\n\n        for f in range(n):\n            results_mean[f,i,j], results_std[f,i,j] = res[0][f], res[1][f]\n            cvs[f,i,j] = res[2][f]\n    \n    pickle.dump(results_mean, open(f'results_mean_{dataset_name}.pkl', 'wb'))\n    pickle.dump(results_std, open(f'results_std_{dataset_name}.pkl', 'wb'))\n    pickle.dump(cvs, open(f'cvs_{dataset_name}.pkl', 'wb'))\n    \n    return results_mean, results_std, cvs","224a8f76":"def plot3d_metrics_by_lag_delta_window(results_mean, lags, delta_wins, f_names):\n    col = 5 if len(f_names) >5 else len(f_names)\n    row = results_mean.shape[0] \/\/ col +1\n    fig = plt.figure(figsize=(col*5, row*5))\n                 \n    for i in range(results_mean.shape[0]):\n        ax = fig.add_subplot(row, col,i+1, projection='3d')\n        ax.set_title(f_names[i].replace('_to_Groundwater_','_'))\n        X, Y = lags, delta_wins\n        Y, X = np.meshgrid(Y, X)\n        Z = -results_mean[i]\n        surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\n                           linewidth=0, antialiased=False)\n    #         coor = np.unravel_index(Z.argmax(), Z.shape)\n    #         label = f'lag{lags[coor[0]]} wind{ewms[coor[1]]} coor{round(Z.max(), 2)}'\n    #         ax.text(lags[coor[0]], ewms[coor[1]], Z.max(), label)\n\n        ax.set_xlabel('lag')\n        ax.set_ylabel('window')\n        ax.set_zlabel('rmse')        \n        \n        \ndef plot2d_metrics_by_lag_delta_window(results_mean, results_std, cvs,\n                                       lags, delta_wins, f_names):\n    col=4\n    row = lags.shape[0] \/\/ col +1\n    for f in range(len(f_names)):\n        fig, axes = plt.subplots(row, col, figsize=(col*5, row*4), sharey=True)\n        fig.suptitle(f_names[f])\n        fig.subplots_adjust(top=0.95)\n        for (i, l), ax in zip(enumerate(lags), axes.ravel()):\n            ax.plot(delta_wins, results_mean[f][i])\n            ci = _tconfint_generic(results_mean[f][i], results_std[f][i], cvs[f][i], 0.05, '2s')\n    #         print(ci)\n            ax.fill_between(delta_wins,ci[0], ci[1], color='black', alpha=0.25 )\n            ax.set_title(f'lag={l}')\n","cbb85e6e":"lags = np.array(list(range(0,8)) + list(range(10, 35, 5)))\ndelta_wins = np.arange(-25, 25, 5)\n# results_mean_auser,results_std_auser, cvs_auser  = model_metrics_by_lag_delta_windows(auser, 'auser', \n#                                                         lags, delta_wins,\n#                                                         params_rf, params_exp)\n\nresults_mean_auser = pickle.load(open('\/kaggle\/input\/geo-data-water-italia\/results_mean_auser.pkl', 'rb'))\nresults_std_auser = pickle.load(open('\/kaggle\/input\/geo-data-water-italia\/results_std_auser.pkl', 'rb'))\ncvs_auser = pickle.load(open('\/kaggle\/input\/geo-data-water-italia\/cvs_auser.pkl', 'rb'))","473fc929":"f_names = auser_df.target_name.unique()\nplot3d_metrics_by_lag_delta_window(results_mean_auser, lags, delta_wins, f_names)","decfd305":"plot2d_metrics_by_lag_delta_window(results_mean_auser, results_std_auser, cvs_auser,\n                                   lags, delta_wins, f_names)","abff67db":"lags = np.array(list(range(0, 8)) + list(range(10, 35, 5)))\ndelta_wins = np.arange(-25, 25, 5)\n# results_mean_doganella,results_std_doganella, cvs_doganella = model_metrics_by_lag_delta_windows(doganella, 'doganella', \n#                                                         lags, delta_wins,\n#                                                         params_rf, params_exp)\n\nresults_mean_doganella = pickle.load(open('\/kaggle\/input\/geo-data-water-italia\/results_mean_doganella.pkl', 'rb'))\nresults_std_doganella = pickle.load(open('\/kaggle\/input\/geo-data-water-italia\/results_std_doganella.pkl', 'rb'))\ncvs_doganella = pickle.load(open('\/kaggle\/input\/geo-data-water-italia\/cvs_doganella.pkl', 'rb'))\n\nf_names = doganella_df.target_name.unique()\nplot3d_metrics_by_lag_delta_window(results_mean_doganella, lags, delta_wins, f_names)\nplot2d_metrics_by_lag_delta_window(results_mean_doganella, results_std_doganella, cvs_doganella,\n                                   lags, delta_wins, f_names)","881e312e":"(luco_df.isna().sum()\/luco_df.shape[0]).sort_values()","7fdead3d":"to_drop = luco_df.columns[(luco_df.isna().sum()\/luco_df.shape[0]) > 0.8]\nto_drop","5c1cbaa9":"lags = np.array(list(range(0, 8)) + list(range(10, 35, 5)))\ndelta_wins = np.arange(-25, 25, 5)\n\n# results_mean_luco,results_std_luco, cvs_luco = \\\n#         model_metrics_by_lag_delta_windows(luco.drop(columns=to_drop), 'luco', \n#                                            lags, delta_wins,\n#                                            params_rf, params_exp)\n\nresults_mean_luco = pickle.load(open('\/kaggle\/input\/geo-data-water-italia\/results_mean_luco.pkl', 'rb'))\nresults_std_luco = pickle.load(open('\/kaggle\/input\/geo-data-water-italia\/results_std_luco.pkl', 'rb'))\ncvs_luco = pickle.load(open('\/kaggle\/input\/geo-data-water-italia\/cvs_luco.pkl', 'rb'))\n\nf_names = luco_df.target_name.unique()\nplot3d_metrics_by_lag_delta_window(results_mean_luco, lags, delta_wins, f_names)\nplot2d_metrics_by_lag_delta_window(results_mean_luco, results_std_luco, cvs_luco,\n                                   lags, delta_wins, f_names)\n","c92fabae":"lags = np.array(list(range(0, 8)) + list(range(10, 35, 5)))\ndelta_wins = np.arange(-25, 25, 5)\n# results_mean_petrignano,results_std_petrignano, cvs_petrignano  = model_metrics_by_lag_delta_windows(petrignano, 'petrignano', \n#                                                         lags, delta_wins,\n#                                                         params_rf, params_exp)\n\nresults_mean_petrignano = pickle.load(open('\/kaggle\/input\/geo-data-water-italia\/results_mean_petrignano.pkl', 'rb'))\nresults_std_petrignano = pickle.load(open('\/kaggle\/input\/geo-data-water-italia\/results_std_petrignano.pkl', 'rb'))\ncvs_petrignano = pickle.load(open('\/kaggle\/input\/geo-data-water-italia\/cvs_petrignano.pkl', 'rb'))\n\nf_names = petrignano_df.target_name.unique()\nplot3d_metrics_by_lag_delta_window(results_mean_petrignano, lags, delta_wins, f_names)\nplot2d_metrics_by_lag_delta_window(results_mean_petrignano, results_std_petrignano, cvs_petrignano,\n                                   lags, delta_wins, f_names)","947baf85":"def highlight_min(s):\n    is_min = s == s.min()\n    return ['background-color: yellow' if v else '' for v in is_min]\n\n\nres_mean = pd.DataFrame()\nres_std = pd.DataFrame()\nfor m, st, name in zip((results_mean_auser, results_mean_doganella, results_mean_luco, results_mean_petrignano),\n                       (results_std_auser, results_std_doganella, results_std_luco, results_std_petrignano),\n                       ('auser', 'doganella', 'luco', 'petrignano')):\n    idx = [0,7,9,10,12] \n    \n    for target in range(m.shape[0]):\n        res_mean = pd.concat([res_mean,\n                              pd.Series(m[target].mean(axis=1)[idx], \n                                        index=[0,7,15,20,30], name=f'{name}_mean{target}')],\n                             axis=1)\n        res_std = pd.concat([res_std, \n                             pd.Series(st[target].mean(axis=1)[idx], \n                                       index=[0,7,15,20,30], name=f'{name}_std{target}')],\n                            axis=1)\n        \nres_mean_diff = (res_mean - res_mean.iloc[0]).drop(index=0)\n","53c64896":"display(res_mean_diff.style.apply(highlight_min),\n        res_std.drop(index=0) * t.ppf(0.975, res_cvs-1))","7c4b6ee0":"res_cvs = np.concatenate([f.mean(axis=(1,2)).round(0) for f in [cvs_auser, cvs_doganella, cvs_luco, cvs_petrignano]])\ntemp = res_mean_diff.copy()\ntemp[pd.DataFrame(res_mean_diff.abs() >= \\\n                           (res_std.drop(index=0).values * t.ppf(0.975, res_cvs-1)))] = np.nan\ntemp.style.apply(highlight_min)\n","ca81f853":"res_cvs = np.concatenate([f.mean(axis=(1,2)).round(0) for f in [cvs_auser, cvs_doganella, cvs_luco, cvs_petrignano]])\ntemp = res_mean_diff.copy()\ntemp[res_mean_diff.abs() >= res_std.drop(index=0).values] = np.nan\ntemp.style.apply(highlight_min)\n","29542d07":"\ndef preprocess(gr):\n    assert gr.shape == gr.resample('1d').last().shape\n    gr.target = gr.target.shift(params_exp['future_days'])\n    temp = gr[gr.target.notna()]\n    get_time_features(temp)\n    temp.drop('year', axis=1, inplace=True)\n    \n    \n# #     temp = gr.dropna()\n#     features_rainfall = [f for f in temp.columns if 'Rainfall' in f]\n# #     display(temp[features_rainfall].isna().sum().sort_values())\n#     temp['rainfall_mean'] = temp[features_rainfall].mean(axis=1)\n# #     th = temp.shape[0] * 0.5\n# #     for f in features_rainfall:\n# #         if temp[f].isna().sum() >= th:\n# #             temp.drop(f, axis=1, inplace=True)\n    \n# #     temp.drop([f for f in temp.columns if 'Groundwater' in f], axis=1, inplace=True)\n    \n# #     features_volume = [f for  f in temp.columns if 'Volume' in f]\n# #     temp = temp[temp[features_volume].notna().any(axis=1)]\n#     display(temp.isna().sum())\n","ef8d5f97":"def plot_df_with_na(df):\n    ncols = 3\n    n = df.select_dtypes('float').shape[1]\n    nrows = n \/\/ ncols\n    nrows = nrows + 1 if n % ncols != 0 else nrows\n    fig, axes = plt.subplots(nrows,ncols, figsize=(ncols*10, nrows*3))\n    for f, ax in zip(df.select_dtypes('float').columns, axes.ravel()):\n        temp = df[f].copy().to_frame()\n        temp['na'] = temp[f].isna()\n#         temp['isna'][temp[f].notna()] = temp[f].dropna().rolling(2).mean() == 0\n        temp[f].plot(ax=ax, title=f)\n        ax.scatter(temp[temp.na].index, [0]*temp[temp.na].shape[0], c='r')        \n# luco_df[luco_df.target.notna()].plot(subplots=True, layout=(20,3), ax=ax);","4815599c":"plot_df_with_na(luco_df[luco_df.target.notna()])","f43bdcc4":"fig, axes = plt.subplots(9,1, figsize=(20,9*4))\n\nfor (i, gr), ax in zip(doganella_df[doganella_df.index.year >= 2012].groupby('target_name'),\n                      axes.ravel()):\n    gr.target.plot(ax=ax)\n    ax.scatter(gr[gr.target.isna()].index, \n               gr.groupby('target_name').target.transform('max')[gr.target.isna()], c='r')\n    ","03a258c3":"fig, axes = plt.subplots(9,1, figsize=(25,9*5))\n\nfor (i, gr), ax in zip(doganella_df[doganella_df.index.year >= 2012].groupby('target_name'),\n                      axes.ravel()):\n    temp = gr.target.rolling(5).mean()\n    gr.target.plot(ax=ax, c='black', alpha=0.5)\n    temp.plot(ax=ax)\n    ax.scatter(temp[temp.isna()].index, \n               gr.groupby('target_name').target.transform('max')[temp.isna()], c='r')\n    ","9f432ec5":"features_depth = [f for f in doganella.columns if 'Depth' in f][2:6:3]\n# doganella_df\ndoganella[doganella.index.year >= 2019][features_depth].plot()","49a8425a":"fig, ax = plt.subplots(figsize=(30, 40))\ndoganella[doganella.index.year >= 2019].plot(subplots=True, layout=(20,2), ax=ax);","a4cac5c0":"fig, ax = plt.subplots(figsize=(30, 40))\ndoganella.plot(subplots=True, layout=(20,2), ax=ax);","4c9290e2":"plot_all_features_grouped_target(doganella_df[(doganella_df.index.year >= 2019)&(doganella_df.target_name.isin(features_depth))])","3a5c9670":"# Corr matrix","61c09650":"# Targets","7dba89f4":"# Models","117782ae":"# Datasets","6704761d":"# Location features map","6db4e595":"# Missings","5b1f7658":"**Exp smoothing**\n\nAll analysis for current theme is [here](https:\/\/www.kaggle.com\/declot\/water-italy-aquifers-windows-for-ewm#Common-rainfalls-analysis)","289937ed":"**Hydrometry**","8870c1d7":"# Get additional geo data\njson file was  downloaded from official website https:\/\/www.sir.toscana.it\/consistenza-rete","2fb18f93":"There are some outliers with depgth going up to 0. Find these points and replace with nans","119a7265":"**Temperature**","648cfcc3":"**Volume**","c67dac2c":"**Rainfall**\n\nFor two rainfall features in auser dataset wrong data are used in the initial dataset. Let's download the new rainfall time-series and replace features on new. More explanation is [here](https:\/\/www.kaggle.com\/declot\/water-italy-aquifers-windows-for-ewm#Common-rainfalls-analysis)","85ade0e0":"# Features preprocessing","3be943f0":"# Time"}}