{"cell_type":{"4f8d2c10":"code","9ad56544":"code","e84c52c6":"code","b6c60dda":"code","120dc0c2":"code","475e10e8":"code","90de7068":"code","40cdb5a5":"code","b39fa088":"code","8d0d5c0d":"code","e8ae2da8":"code","5ae5fe95":"code","a3adce51":"code","c931089b":"code","98993955":"code","939bc9a4":"code","fd4a76d1":"code","91143894":"code","062e8724":"code","16accda5":"code","3542f7ee":"code","b5e4c9b5":"code","3fe53165":"code","8636d68c":"code","0f8c7b8f":"code","84e4c783":"code","b1249be9":"code","2d9225b5":"code","572965fb":"code","a9234035":"code","f006aa96":"code","728862da":"code","81cca709":"code","15e3077d":"code","f726d2bb":"code","426eb27f":"code","981094df":"code","3974829d":"code","63935e71":"code","5bf9c90d":"code","265f19c4":"code","80110128":"code","6e24c37c":"code","3f5c623e":"code","28c4e42d":"code","a8a399fa":"code","596fb75d":"code","037b1f8c":"code","68647e9e":"code","d54c9e71":"code","97ae1055":"code","155afaed":"code","e2e616a5":"markdown","6b157dc7":"markdown","203dbb0e":"markdown","64d05723":"markdown","7e7ef0b3":"markdown","3132c350":"markdown","2749cd78":"markdown","5a1c202f":"markdown","51ae2338":"markdown","58291973":"markdown","9c2ea64d":"markdown","063574ab":"markdown","26a5a8a2":"markdown","a209873f":"markdown","bd4f82b2":"markdown","2a2c5eec":"markdown","d2e3d459":"markdown","898b0ba0":"markdown","4087b127":"markdown","2108a4a1":"markdown","c1144758":"markdown","fa2eb3bd":"markdown","a122cc46":"markdown","d361444c":"markdown","ba91419f":"markdown","4406aa65":"markdown","f19c7efe":"markdown"},"source":{"4f8d2c10":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9ad56544":"# Importing required libraries\nimport numpy as np\nimport pandas as pd, datetime\nimport seaborn as sns\nfrom statsmodels.tsa.stattools import adfuller\nimport matplotlib.pyplot as plt\nget_ipython().run_line_magic('matplotlib', 'inline')\nfrom time import time\nimport os\nfrom math import sqrt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport itertools\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import acf,pacf\nfrom statsmodels.tsa.arima_model import  ARIMA\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom pandas import DataFrame\nimport xgboost as xgb\nfrom fbprophet import Prophet\nimport warnings\nwarnings.filterwarnings('ignore')","e84c52c6":"# Importing store data\nstore = pd.read_csv('..\/input\/rossmann-store-sales\/store.csv')\nstore.head()","b6c60dda":"# Importing train data\ntrain = pd.read_csv('..\/input\/rossmann-store-sales\/train.csv', index_col='Date', parse_dates = True)\ntrain.head()","120dc0c2":"\n# Importing test data\ntest = pd.read_csv('..\/input\/rossmann-store-sales\/test.csv')\ntest.head()","475e10e8":"# Checking train data\ntrain.head(5).append(train.tail(5))","90de7068":"train.shape","40cdb5a5":"\n# Extracting year, month, day and week, and making new column\ntrain['Year'] = train.index.year\ntrain['Month'] = train.index.month\ntrain['Day'] = train.index.day\ntrain['WeekOfYear'] = train.index.weekofyear\n\ntrain['SalePerCustomer'] = train['Sales']\/train['Customers']","b39fa088":"# Checking train data again\ntrain.head()","8d0d5c0d":"# Checking data when the stores were closed\ntrain_store_closed = train[(train.Open == 0)]\ntrain_store_closed.head()","e8ae2da8":"# Checking days when the stores were closed\ntrain_store_closed.hist('DayOfWeek');","5ae5fe95":"# Checking whether there was a school holiday when the store was closed\ntrain_store_closed['SchoolHoliday'].value_counts().plot(kind='bar');","a3adce51":"# Checking whether there was a state holiday when the store was closed\ntrain_store_closed['StateHoliday'].value_counts().plot(kind='bar');","c931089b":"# Checking missing values in train set - no missing value\ntrain.isnull().sum()","98993955":"# No. of days with closed stores\ntrain[(train.Open == 0)].shape[0]","939bc9a4":"# No. of days when store was opened but zero sales - might be because of external factors or refurbishmnent\ntrain[(train.Open == 1) & (train.Sales == 0)].shape[0]","fd4a76d1":"# Checking store data\nstore.head()","91143894":"# Checking missing values in store data \nstore.isnull().sum()","062e8724":"\n# Replacing missing values for Competiton distance with median\nstore['CompetitionDistance'].fillna(store['CompetitionDistance'].median(), inplace=True)","16accda5":"\n# No info about other columns - so replcae by 0\nstore.fillna(0, inplace=True)","3542f7ee":"# Checking test data\ntest.head()","b5e4c9b5":"\n# Checking missing values\ntest.isnull().sum()","3fe53165":"# Assuming stores open in test\ntest.fillna(1, inplace=True)","8636d68c":"# Joining the tables\ntrain_store_joined = pd.merge(train, store, on='Store', how='inner')\ntrain_store_joined.head()","0f8c7b8f":"# Distribution of sales and customers across store types\ntrain_store_joined.groupby('StoreType')['Customers', 'Sales', 'SalePerCustomer'].sum().sort_values('Sales', ascending=False)","84e4c783":"# Closed and zero-sales obseravtions\ntrain_store_joined[(train_store_joined.Open ==0) | (train_store_joined.Sales==0)].shape","b1249be9":"# Open & Sales >0 stores\ntrain_store_joined_open = train_store_joined[~((train_store_joined.Open ==0) | (train_store_joined.Sales==0))]","2d9225b5":"# Correlation\nplt.figure(figsize = (20, 10))\nsns.heatmap(train_store_joined.corr(), annot = True);\n","572965fb":"# Sales trend over the months\nsns.factorplot(data = train_store_joined_open, x =\"Month\", y = \"Sales\", \n               col = 'Promo', # per store type in cols\n               hue = 'Promo2',\n               row = \"Year\"\n             );","a9234035":"# Sales trend over days\nsns.factorplot(data = train_store_joined_open, x = \"DayOfWeek\", y = \"Sales\", hue = \"Promo\");","f006aa96":"pd.plotting.register_matplotlib_converters()","728862da":"# Data Preparation: input should be float type\ntrain['Sales'] = train['Sales'] * 1.0\n\n# Assigning one store from each category\nsales_a = train[train.Store == 2]['Sales']\nsales_b = train[train.Store == 85]['Sales'].sort_index(ascending = True) \nsales_c = train[train.Store == 1]['Sales']\nsales_d = train[train.Store == 13]['Sales']\n\nf, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize = (12, 13))\n\n# Trend\nsales_a.resample('W').sum().plot(ax = ax1)\nsales_b.resample('W').sum().plot(ax = ax2)\nsales_c.resample('W').sum().plot(ax = ax3)\nsales_d.resample('W').sum().plot(ax = ax4);","81cca709":"# Function to test the stationarity\ndef test_stationarity(timeseries):\n    \n    # Determing rolling statistics\n    roll_mean = timeseries.rolling(window=7).mean()\n    roll_std = timeseries.rolling(window=7).std()\n\n    # Plotting rolling statistics:\n    orig = plt.plot(timeseries.resample('W').mean(), color='blue',label='Original')\n    mean = plt.plot(roll_mean.resample('W').mean(), color='red', label='Rolling Mean')\n    std = plt.plot(roll_std.resample('W').mean(), color='green', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.show(block=False)\n    \n    # Performing Dickey-Fuller test:\n    print('Results of Dickey-Fuller Test:')\n    result = adfuller(timeseries, autolag='AIC')\n    print('ADF Statistic: %f' % result[0])\n    print('p-value: %f' % result[1])\n    print('Critical Values:')\n    for key, value in result[4].items():\n           print(key, value)","15e3077d":"# Testing stationarity of store type a\ntest_stationarity(sales_a)","f726d2bb":"#Testing stationarity of store type b\ntest_stationarity(sales_b)","426eb27f":"#Testing stationarity of store type b\ntest_stationarity(sales_c)","981094df":"#Testing stationarity of store type d\ntest_stationarity(sales_d)","3974829d":"# Plotting seasonality and trend\ndef plot_timeseries(sales,StoreType):\n\n    fig, axes = plt.subplots(2, 1, sharex=True, sharey=False)\n    fig.set_figheight(10)\n    fig.set_figwidth(15)\n\n    decomposition= seasonal_decompose(sales, model = 'additive',freq=365)\n\n    estimated_trend = decomposition.trend\n    estimated_seasonal = decomposition.seasonal\n    estimated_residual = decomposition.resid\n    \n    axes[1].plot(estimated_seasonal, 'g', label='Seasonality')\n    axes[1].legend(loc='upper left');\n    \n    axes[0].plot(estimated_trend, label='Trend')\n    axes[0].legend(loc='upper left');\n\n    plt.title('Decomposition Plots')","63935e71":"# Plotting seasonality and trend for store type a\nplot_timeseries(sales_a,'a')","5bf9c90d":"# Plotting seasonality and trend for store type b\nplot_timeseries(sales_b,'b')","265f19c4":"# Plotting seasonality and trend for store type c\nplot_timeseries(sales_c,'c')","80110128":"# Plotting seasonality and trend for store type d\nplot_timeseries(sales_d,'d')","6e24c37c":"# Autocorrelation function to make ACF and PACF graphs\ndef auto_corr(sales):\n    lag_acf = acf(sales,nlags=30)\n    lag_pacf = pacf(sales,nlags=20,method='ols')\n  \n    plt.subplot(121)\n    plt.plot(lag_acf)\n    plt.axhline(y=0,linestyle='--',color ='red')\n    plt.axhline(y=1.96\/np.sqrt(len(sales_a)),linestyle='--',color ='red')\n    plt.axhline(y=-1.96\/np.sqrt(len(sales_a)),linestyle='--',color ='red')\n    plt.title('ACF')\n    \n    plt.subplot(122)\n    plt.plot(lag_pacf)\n    plt.axhline(y=0,linestyle='--',color ='red')\n    plt.axhline(y=1.96\/np.sqrt(len(sales_a)),linestyle='--',color ='red')\n    plt.axhline(y=-1.96\/np.sqrt(len(sales_a)),linestyle='--',color ='red')\n    plt.title('PACF')","3f5c623e":"# ACF and PACF for store type a\nauto_corr(sales_a)","28c4e42d":"# ACF and PACF for store type c\nauto_corr(sales_c)","a8a399fa":"# ACF and PACF for store type d\nauto_corr(sales_d)","596fb75d":"# Summing sales on per week basis\ntrain_arima = train.resample(\"W\").mean() \ntrain_arima = train_arima[[\"Sales\"]]\ntrain_arima.plot();","037b1f8c":"# Define the p, d and q parameters to take any value between 0 and 3\np = d = q = range(0, 2)\n\n# Generate all different combinations of p, q and q triplets\npdq = list(itertools.product(p, d, q))\n\n# Generate all different combinations of seasonal p, q and q triplets\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n\nprint('Examples of parameter combinations for Seasonal ARIMA: ')\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))","68647e9e":"# Determing p,d,q combinations with AIC scores.\nfor param in pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(train_arima,\n                                            order=param,\n                                            seasonal_order=param_seasonal,\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False)\n\n            results = mod.fit()\n\n            print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n        except:\n            continue","d54c9e71":"# Fitting the data to SARIMA model \nmodel_sarima = sm.tsa.statespace.SARIMAX(train_arima,\n                                order=(1, 1, 1),\n                                seasonal_order=(0, 1, 1, 12),\n                                enforce_stationarity=False,\n                                enforce_invertibility=False)\n\nresults_sarima = model_sarima.fit()\n\nprint(results_sarima.summary().tables[1])","97ae1055":"# Checking diagnostic plots\nresults_sarima.plot_diagnostics(figsize=(10, 10))\nplt.show()","155afaed":"# Model Prediction and validation \n# Predictions are performed for the 11th Jan' 2015 onwards of the train data.\n\npred = results_sarima.get_prediction(start=pd.to_datetime('2015-01-11'), dynamic = False) \n\n# Get confidence intervals of forecasts\npred_ci = pred.conf_int() \n\nax = train_arima[\"2014\":].plot(label = \"observed\", figsize=(15, 7))\npred.predicted_mean.plot(ax = ax, label = \"One-step ahead Forecast\", alpha = 1)\nax.fill_between(pred_ci.index, \n                pred_ci.iloc[:, 0], \n                pred_ci.iloc[:, 1], \n                color = \"k\", alpha = 0.05)\n\nax.set_xlabel(\"Date\")\nax.set_ylabel(\"Sales\")\n\nplt.legend\nplt.show()\n\ntrain_arima_forecasted = pred.predicted_mean\ntrain_arima_truth = train_arima[\"2015-01-11\":]\n\n# Calculating the error\nrms_arima = sqrt(mean_squared_error(train_arima_truth, train_arima_forecasted))\nprint(\"Root Mean Squared Error: \", rms_arima)","e2e616a5":"Stationarity of Time Series\n\nIn order to use time series forecasting models, we need to ensure that our time series data is stationary i.e constant mean, constant variance and constant covariance with time.\n\nThere are 2 ways to test the stationarity of time series\na) Rolling Mean: Visualization b) Dicky - Fuller test: Statistical test\n\na) Rolling Mean: A rolling analysis of a time series model is often used to assess the model's stability over time. The window is rolled (slid across the data) on a weekly basis, in which the average is taken on a weekly basis. Rolling Statistics is a visualization test, where we can compare the original data with the rolled data and check if the data is stationary or not.\n\nb) Dicky -Fuller test: This test provides us the statistical data such as p-value to understand whether we can reject the null hypothesis. The null hypothesis is that data is not stationary and the alternative hypothesis says that data is stationary. If p-value is less than the critical value (say 0.5), we will reject the null hypothesis and say that data is stationary.","6b157dc7":"We can see from the above plots and statistical tests that mean and variation doesn't change much with time, i.e they are constant. Thus, we don't need to perform any transformation (needed when time series is not stationary).\n\nNow, let's check the trend and seasonality in our data.","203dbb0e":"# **Exploratory Data Analysis (EDA)**","64d05723":"From the above plots, we can see that there is seasonality and trend present in our data. So, we'll use forecasting models that take both of these factors into consideration. For example, SARIMAX and Prophet.","7e7ef0b3":"**Correlation analysis**","3132c350":"Now, let's iterate through these combinations to see which one gives the lowest AIC score.\n\n","2749cd78":"\nWe can see from the trend that there are no promotions on the weekends i.e Saturday and Sunday, which makes sense as stores want to earn a maximum profit during the time when people do their house chores. The sales tend to increase on Sunday because people shop during the weekend. We can also see that the maximum sale happens on Mondays when there are promotional offers.","5a1c202f":"Let's see the stores which are closed or have zero sales.\n\n","51ae2338":"The above graphs suggest that the p = 2 and q = 2 but let's do a grid search and see which combination of p, q and d gives the lowest Akaike information criterion (AIC, which tells us the quality of statistical models for a given set of data. Best model uses the lowest number of features to fit the data.\n\nIf we are to predict the sales of each store, we need to consider the whole data set rather than one store of each category. We took one store of each category to understand the tiem series data but from now on, we'll use the whole dataset for modelling.","58291973":"We can see from the above 'Histogram plus estimated density' plot that our KDE (Kernel Desnity Estimator) plot closely follows the N(0,1) normal distribution plot. The Normal Q-Q plot shows that the ordered distribution of residuals follows the distribution similar to normal distribution. Thus, our model seems to be pretty good.\n\nStandardized residual plot tells us that there is no major seasonality trend, which is confirmed by Correlogram (autocorrelation) plot. Autocorrelation plot tells us that the time series residuals have low correlation with lagged versions of itself.","9c2ea64d":"**Model 1 - SARIMA (Seasonal Autoregressive Integrated Moving Average):**\n\nIn order to use this model, we need to first find out values of p, d and q. p represents number of Autoregressive terms - lags of dependent variable. q represents number of Moving Average terms - lagged forecast errors in prediction equation. d represents number of non-seasonal differences.\n\nTo find the values of p, d and q - we use Autocorrelation function (ACF) and Partial Autocorrelation (PACF) plots.\n\nACF - measure of correlation between time series with a lagged version of itself. PACF - measure of correlation between time series with a lagged version of itself but after eliminating the variations already explained by the intervening comparison.\n\np value is the value on x-axis of PACF where the plot crosses the upper Confidence Interval for the first time. q value is the value on x-axis of ACF where the plot crosses the upper Confidence Interval for the first time.\n\nNow, let's plot these graphs.","063574ab":"**Stationary time series**","26a5a8a2":"Conclusions of EDA\n\na) The most selling and crowded StoreType is A.\n\nb) StoreType B has the highest Sale per Customer.\n\nc) Customers tends to buy more on Mondays when there are ongoing promotional offers and on Thursdays\/Fridays when there is no promotion at all.\n\nd) Second promotion (Promo2) doesn't seem to contribute in the increase of sales.","a209873f":"Only 3 observations have 'Competition Distance' missing. This is probably because someone didn't enter the information in the system. It's safe to replace these missing values with the median. We can't use the same approach with competition's existence (month and year) as it doesn't make sense. It is better to replace it with 0 (i.e launched recently). We'll also impute the missing values in promo with 0 as no information about promo is avaialable.","bd4f82b2":"Evaluation Metrics:\n\nThere are two popular metrics used in measuring the performance of regression (continuous variable) models i.e MAE & RMSE.\n\nMAE - Mean Absolute Error: It is the average of the absolute difference between the predicted values and observed values.\n\nRMSE - Root Mean Square Error: It is the square root of the average of squared differences between the predicted values and observed values.\n\nMAE is easier to understand and interpret but RMSE works well in situations where large errors are undesirable. This is because the errors are squared before they are averaged, thus penalizing large errors. In our case, RMSE suits well because we want to predict the sales with minimum error (i.e penalize high errors) so that inventory can be managed properly.\n\nSo, let's choose RMSE as a metric to measure the performance of our models.","2a2c5eec":"**Forecasting a Time Series**","d2e3d459":"So, we have 172,871 observations when the stores were closed or have zero sales. We can drop these rows in order to do data analysis but we can still keep them for predictive modelling because our models will be able to understand the trend behind it","898b0ba0":"**Hyperparamter tuning ARIMA model**\n\nAs discussed above, we have three parameters (p, d and q) for SARIMA model. So, in order to choose the best combination of these parameter, we'll use a grid search. The best combination of parameters will give the lowest AIC score.","4087b127":"From above table, we can see that Store of type 'a' and 'd' have the highest total sales but stores of type 'c' and 'b' have the highest sale per customer.\n\n","2108a4a1":"**Final ARIMA Solution:**\n\nWe tried different combinations of parameters using Grid search and foundoptimal parameter: ARIMA(1, 1, 1)x(0, 1, 1, 12)12 - AIC:1806.29. Let's try another model to see if we can reduce the error.","c1144758":"Fitting the model - using hyperparamters tuned above\n","fa2eb3bd":"Time Series Analysis\n\nFor Time Series Anlaysis , we will consider one store from each store type a , b , c , d that will represent their respective group. It also makes sense to downsample the data from days to weeks using the resample method to see the present trends more clearly.","a122cc46":"We can see from the above trends that sales tend to spike in November and December. So, there is a seasonality factor present in the data.\n\n","d361444c":"# **Time Series Analysis & Predictive Modelling**","ba91419f":"We can see from above plots that sales for StoreType A and C tend to peak in the end of year (Christmas season) and then decline after the holidays. We are not able to see a similar trend in StoreType D because no data is available for that time period (stores closed).","4406aa65":"We can see from the above grid search that our optimal parameter combination is ARIMA(1, 1, 1)x(0, 1, 1, 12)12 - AIC:1806.2981906705384. So, let's use this in our model.","f19c7efe":"\nWe can see a strong positive correlation between the amount of Sales and Customers visiting the store. We can also observe a positive correlation between a running promotion (Promo = 1) and number of customers."}}