{"cell_type":{"5cd03f80":"code","584bdb83":"code","6be30c6a":"code","4cdcba0c":"code","77a78629":"code","25320096":"code","4ced61e2":"code","edab494f":"code","5efa0cba":"code","7a2d63f1":"code","7f99ecc1":"code","12add91c":"code","1e702580":"code","9bbf1a53":"code","81d06261":"code","1b9c60d2":"code","54aa7524":"code","fc9da02d":"code","6dc40d56":"code","b241d211":"code","1db1e761":"code","746c4265":"code","6ee71f79":"code","12aac107":"code","29d02354":"markdown","b8952d60":"markdown","75f3d0d8":"markdown","cf96257a":"markdown","ae5ff2f0":"markdown","78705dc7":"markdown","7bdc1de5":"markdown","6842ac2a":"markdown","9cd2b82b":"markdown","e1241da7":"markdown","95c6c506":"markdown","03347528":"markdown","356bbaf9":"markdown","7248572a":"markdown","2229ea56":"markdown","86455886":"markdown","341fc150":"markdown","4191f6ee":"markdown","10e51c86":"markdown","2c93d963":"markdown","762005b5":"markdown","8ce539e2":"markdown","f0197b0d":"markdown","9b9eb893":"markdown","d04aa386":"markdown"},"source":{"5cd03f80":"import pandas as pd\nfrom pandas.plotting import autocorrelation_plot\nfrom pandas import DataFrame\nfrom pandas import concat\nimport numpy as np\nfrom math import sqrt\n\nfrom sklearn.metrics import mean_squared_error\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.stattools import acf\nfrom statsmodels.tsa.stattools import pacf\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom scipy.stats import boxcox\n\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nimport matplotlib.pyplot as plt\nfrom matplotlib.pylab import rcParams\nfrom matplotlib import colors\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","584bdb83":"col_names = [\"date\", \"value\"]\ndf = pd.read_csv(\"\/kaggle\/input\/time-series-datasets\/Electric_Production.csv\",\n                 names = col_names, header = 0, parse_dates = [0])\ndf['date'] = pd.to_datetime(df['date'],infer_datetime_format=True)\ndf = df.set_index(['date'])\ndf.head()","6be30c6a":"rolling_mean = df.rolling(window=12).mean()\nrolling_std = df.rolling(window=12).std()\nplt.figure(figsize = (10,6))\nplt.plot(df, color='cornflowerblue', label='Original')\nplt.plot(rolling_mean, color='firebrick', label='Rolling Mean')\nplt.plot(rolling_std, color='limegreen', label='Rolling Std')\nplt.xlabel('Date', size = 12)\nplt.ylabel('Electric Production', size  = 12)\nplt.legend(loc = 'upper left')\nplt.title('Rolling Statistics', size = 14)\nplt.show()","4cdcba0c":"plt.figure(figsize = (10,6))\nplt.plot(df['value'], color = 'cornflowerblue')\nplt.xlabel('Date', size = 12)\nplt.ylabel('Electric Production', size = 12)\nplt.show()","77a78629":"plt.figure(figsize = (10,6))\nplt.hist(df['value'], color = 'cornflowerblue')\nplt.xlabel('Electric Production', size = 12)\nplt.ylabel('Frequency', size = 12)\nplt.show()","25320096":"print(\"Data Shape: {}\".format(df.shape))\nvalue_1 = df[0:199]\nvalue_2 = df[200:397]","4ced61e2":"print(\"Mean of value_1: {}\".format(round(value_1.mean()[0],3)))\nprint(\"Mean of value_2: {}\".format(round(value_2.mean()[0],3)))","edab494f":"print(\"Variance of value_1: {}\".format(round(value_1.var()[0],3)))\nprint(\"Variance of value_2: {}\".format(round(value_2.var()[0],3)))","5efa0cba":"def adfuller_test(ts, window = 12):\n    \n    movingAverage = ts.rolling(window).mean()\n    movingSTD = ts.rolling(window).std()\n    \n    plt.figure(figsize = (10,6))\n    orig = plt.plot(ts, color='cornflowerblue',\n                    label='Original')\n    mean = plt.plot(movingAverage, color='firebrick',\n                    label='Rolling Mean')\n    std = plt.plot(movingSTD, color='limegreen',\n                   label='Rolling Std')\n    plt.legend(loc = 'upper left')\n    plt.title('Rolling Statistics', size = 14)\n    plt.show(block=False)\n    \n    adf = adfuller(ts, autolag='AIC')\n    \n    print('ADF Statistic: {}'.format(round(adf[0],3)))\n    print('p-value: {}'.format(round(adf[1],3)))\n    print(\"##################################\")  \n    print('Critical Values:')\n    \n    for key, ts in adf[4].items():\n         print('{}: {}'.format(key, round(ts,3)))\n    print(\"##################################\")\n    \n    if adf[0] > adf[4][\"5%\"]:\n        print(\"ADF > Critical Values\")\n        print (\"Failed to reject null hypothesis, time series is non-stationary.\")\n    else:\n        print(\"ADF < Critical Values\")\n        print (\"Reject null hypothesis, time series is stationary.\")\n        \nadfuller_test(df, window = 12)","7a2d63f1":"df_log_scaled = df\ndf_log_scaled['value'] = boxcox(df_log_scaled['value'], lmbda=0.0)\nplt.figure(figsize = (10,6))\nplt.plot(df_log_scaled, color = 'cornflowerblue')\nplt.xlabel('Date', size = 12)\nplt.ylabel('Electric Production', size = 12)\nplt.title(\"After Logarithmic Transformation\", size = 14)\nplt.show()","7f99ecc1":"moving_avg = df_log_scaled.rolling(window=12).mean()\ndf_log_scaled_ma = df_log_scaled - moving_avg\ndf_log_scaled_ma.dropna(inplace=True)\nplt.figure(figsize = (10,6))\nplt.plot(df_log_scaled_ma, color = 'cornflowerblue')\nplt.xlabel('Date', size = 12)\nplt.ylabel('Electric Production', size = 12)\nplt.title(\"After Moving Average\", size = 14)\nplt.show()","12add91c":"df_log_scaled_ma_ed = df_log_scaled_ma.ewm(halflife=12, min_periods=0, adjust=True).mean()\ndf_lsma_sub_df_lsma_ed = df_log_scaled_ma - df_log_scaled_ma_ed\nplt.figure(figsize = (10,6))\nplt.plot(df_lsma_sub_df_lsma_ed - df_log_scaled_ma_ed, color='cornflowerblue')\nplt.xlabel('Date', size = 12)\nplt.ylabel('Electric Production', size = 12)\nplt.title(\"After Exponential Decay Transformation\", size = 14)\nplt.show()","1e702580":"adfuller_test(df_lsma_sub_df_lsma_ed, window = 12)","9bbf1a53":"rcParams['figure.figsize']=10,8\ndf_seasonal_decompose = seasonal_decompose(df_lsma_sub_df_lsma_ed, \n                                           model='duplicative')\ndf_seasonal_decompose.plot()\nplt.show()","81d06261":"auto_c_f = acf(df_lsma_sub_df_lsma_ed, nlags=20)\npartial_auto_c_f = pacf(df_lsma_sub_df_lsma_ed, nlags=20, method='ols')\n\nfig, axs = plt.subplots(1, 2, figsize =(12,5))\n\nplt.subplot(121)\nplt.plot(auto_c_f)\nplt.axhline(y=0, linestyle='--', color='limegreen')\nplt.axhline(y=-1.96\/np.sqrt(len(df_lsma_sub_df_lsma_ed)),\n            linestyle='--', color='firebrick')\nplt.axhline(y=1.96\/np.sqrt(len(df_lsma_sub_df_lsma_ed)),\n            linestyle='--', color='firebrick')\nplt.title('Autocorrelation Function', size = 14)            \n\nplt.subplot(122)\nplt.plot(partial_auto_c_f)\nplt.axhline(y=0, linestyle='--', color='limegreen')\nplt.axhline(y=-1.96\/np.sqrt(len(df_lsma_sub_df_lsma_ed)),\n            linestyle='--', color='firebrick')\nplt.axhline(y=1.96\/np.sqrt(len(df_lsma_sub_df_lsma_ed)),\n            linestyle='--', color='firebrick')\nplt.title('Partial Autocorrelation Function', size = 14)\n            \nplt.tight_layout() ","1b9c60d2":"values = DataFrame(df_lsma_sub_df_lsma_ed.values)\npersistence_df = concat([values.shift(1), values], axis=1)\npersistence_df.columns = ['t-1', 't+1']\nper_values = persistence_df.values\n\ntrain = per_values[1:len(per_values)-10] \ntest = per_values[len(per_values)-10:]\n\nX_train, y_train = train[:,0], train[:,1]\nX_test, y_test = test[:,0], test[:,1]\n\ndef persistence(x):\n    return x\n\npredictions = []\nfor i in X_test:\n    y_pred = persistence(i)\n    predictions.append(y_pred)\n\npersistence_score = mean_squared_error(y_test, predictions)\nprint('Persistence MSE: {}'.format(round(persistence_score,4)))","54aa7524":"plt.figure(figsize = (10,6))\nplt.plot(y_test, label = \"true values\", color = \"cornflowerblue\")\nplt.plot(predictions,label = \"forecasts\", color='darkorange')\nplt.title(\"Persistence Model\", size = 14)\nplt.legend(loc = 'upper left')\nplt.show()","fc9da02d":"ar_values = df_lsma_sub_df_lsma_ed.values\ntrain = ar_values[1:len(ar_values)-10] \ntest = ar_values[len(ar_values)-10:]\nmodel = ARIMA(train, order=(2,1,0))\nAR_model = model.fit()\n\npredictions = AR_model.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\nar_score = mean_squared_error(test, predictions)\nprint('AR MSE: {}'.format(round(ar_score,4)))","6dc40d56":"plt.figure(figsize = (10,6))\nplt.plot(test, label = \"true values\", color = \"cornflowerblue\")\nplt.plot(predictions,label = \"forecasts\", color='darkorange')\nplt.title(\"AR Model\", size = 14)\nplt.legend(loc = 'upper left')\nplt.show()","b241d211":"model = ARIMA(train, order=(0,1,2))\nMA_model = model.fit()\n\npredictions = MA_model.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\nma_score = mean_squared_error(test, predictions)\nprint('MA MSE: {}'.format(round(ma_score,4)))","1db1e761":"plt.figure(figsize = (10,6))\nplt.plot(test, label = \"true values\", color = \"cornflowerblue\")\nplt.plot(predictions,label = \"forecasts\", color='darkorange')\nplt.title(\"MA Model\", size = 14)\nplt.legend(loc = 'upper left')\nplt.show()","746c4265":"model = ARIMA(train, order=(2,1,2))\nARIMA_model = model.fit()\n\npredictions = ARIMA_model.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\narima_score = mean_squared_error(test, predictions)\nprint('ARIMA MSE: {}'.format(round(arima_score,4)))","6ee71f79":"plt.figure(figsize = (10,6))\nplt.plot(test, label = \"true values\", color = \"cornflowerblue\")\nplt.plot(predictions,label = \"forecasts\", color='darkorange')\nplt.title(\"ARIMA Model\", size = 14)\nplt.legend(loc = 'upper left')\nplt.show()","12aac107":"errors = pd.DataFrame()\nerrors[\"Model\"] = [\"Persistence\", \"Autoregression\", \"Moving Average\", \"ARIMA\"]\nerrors[\"MSE\"] = [persistence_score, ar_score, ma_score, arima_score]\nerrors = errors.sort_values(\"MSE\", ascending = True, ignore_index = True)\nerrors.index = errors.Model\ndel errors[\"Model\"]\n\ndef coloring_bg(s, min_, max_, cmap='Reds', low=0, high=0):\n    color_range = max_ - min_\n    norm = colors.Normalize(min_ - (color_range * low), max_ + (color_range * high))\n    normed = norm(s.values)\n    c = [colors.rgb2hex(x) for x in plt.cm.get_cmap(cmap)(normed)]\n    return ['background-color: %s' % color for color in c]\n\nerrors.style.apply(coloring_bg,min_ =errors.min().min(),\n               max_ = errors.max().max(), low = 0.1, high = 0.85)","29d02354":"<a id = \"17\"><\/a><h1 id=\"Moving Average Model\"><p style=\"font-weight: bold; font-family:Verdana; font-size:18px; color:#47A8EC; \">Moving Average Model<\/p><\/h1>","b8952d60":"Looking at the plot we can observe there is an **upward trend** over the period of time.","75f3d0d8":"<a id = \"13\"><\/a><h1 id=\"ACF & PACF\"><span class=\"label label-default\" style=\"background-color:#47A8EC; border-radius:6px; font-weight: bold; font-family:Verdana; font-size:22px; color:#FBFAFC; \">ACF & PACF<\/span><\/h1>\n\nAutocorrelation and partial autocorrelation plots are heavily used in time series analysis and forecasting.\n\nThese are plots that graphically summarize the strength of a relationship with an observation in a time series with observations at prior time steps.","cf96257a":"The p-value is very less than the significance level of 0.05 and hence we can reject the null hypothesis and take that the series is stationary.","ae5ff2f0":"**Reference:**   \nhttps:\/\/machinelearningmastery.com\/    \nhttps:\/\/otexts.com\/fpp2\/index.html","78705dc7":"<a id = \"12\"><\/a><h1 id=\"Decomposition\"><span class=\"label label-default\" style=\"background-color:#47A8EC; border-radius:6px; font-weight: bold; font-family:Verdana; font-size:22px; color:#FBFAFC; \">Decomposition<\/span><\/h1>","7bdc1de5":"<span class=\"label label-default\" style=\"background-color:#47A8EC; border-radius:6px; font-weight: bold; font-family:Verdana; font-size:28px; color:#FBFAFC; \">\u26a1 Electricity Production Forecasting \u26a1<\/span>","6842ac2a":"<a id = \"1\"><\/a><h1 id=\"Libraries and Utilities\"><span class=\"label label-default\" style=\"background-color:#47A8EC; border-radius:6px; font-weight: bold; font-family:Verdana; font-size:22px; color:#FBFAFC; \">Libraries and Utilities<\/span><\/h1>","9cd2b82b":"<a id = \"2\"><\/a><h1 id=\"Loading Data\"><span class=\"label label-default\" style=\"background-color:#47A8EC; border-radius:6px; font-weight: bold; font-family:Verdana; font-size:22px; color:#FBFAFC; \">Loading Data<\/span><\/h1>","e1241da7":"<a id = \"11\"><\/a><h1 id=\"Exponential Decay Transformation\"><p style=\"font-weight: bold; font-family:Verdana; font-size:18px; color:#47A8EC; \">Exponential Decay Transformation<\/p><\/h1>","95c6c506":"1. [Libraries and Utilities](#1)\n2. [Loading Data](#2)\n3. [Rolling Statistics](#3)\n4. [Checking Stationarity](#4)\n    - [Mean of Data](#5)\n    - [Variance of Data](#6)\n    - [Augmented Dickey-Fuller Test](#7)\n5. [Converting Data to Stationary](#8)\n    - [Logarithmic Transformation with Box-Cox](#9)\n    - [Removing Trend with Moving Average](#10)\n    - [Exponential Decay Transformation](#11)\n6. [Decomposition](#12)\n7. [ACF & PACF](#13)\n8. [Forecasting Models](#14)\n    - [Persistence Model](#15)\n    - [Autoregression Model](#16)\n    - [Moving Average Model](#17)\n    - [ARIMA Model](#18)\n    - [Mean Squared Errors](#19)","03347528":"<a id = \"14\"><\/a><h1 id=\"Forecasting Models\"><span class=\"label label-default\" style=\"background-color:#47A8EC; border-radius:6px; font-weight: bold; font-family:Verdana; font-size:22px; color:#FBFAFC; \">Forecasting Models<\/span><\/h1>\n\n<a id = \"15\"><\/a><h1 id=\"Persistence Model\"><p style=\"font-weight: bold; font-family:Verdana; font-size:18px; color:#47A8EC; \">Persistence Model<\/p><\/h1>\nLet\u2019s say that we want to develop a model to predict the last 10 days of electric production in the dataset given all prior observations. The simplest model that we could use to make predictions would be to persist the last observation. We can call this a persistence model and it provides a baseline of performance for the problem that we can use for comparison with an autoregression model.","356bbaf9":"<a id = \"6\"><\/a><h1 id=\"Variance\"><p style=\"font-weight: bold; font-family:Verdana; font-size:18px; color:#47A8EC; \">Variance of Data<\/p><\/h1>","7248572a":"<a id = \"4\"><\/a><h1 id=\"Checking Stationarity\"><span class=\"label label-default\" style=\"background-color:#47A8EC; border-radius:6px; font-weight: bold; font-family:Verdana; font-size:22px; color:#FBFAFC; \">Checking Stationarity<\/span><\/h1>","2229ea56":"We will proceed by splitting the data into two parts so that we can then check the **mean** and **variance** of the data.","86455886":"<a id = \"5\"><\/a><h1 id=\"Mean of Data\"><p style=\"font-weight: bold; font-family:Verdana; font-size:18px; color:#47A8EC; \">Mean of Data<\/p><\/h1>","341fc150":"<a id = \"10\"><\/a><h1 id=\"Removing Trend with Moving Average\"><p style=\"font-weight: bold; font-family:Verdana; font-size:18px; color:#47A8EC; \">Removing Trend with Moving Average<\/p><\/h1>\n\nA time series with a trend is called **non-stationary**.\n\nAn identified trend can be modeled. Once modeled, it can be removed from the time series dataset. This is called detrending the time series.\n\nIf a dataset does not have a trend or we successfully remove the trend, the dataset is said to be trend stationary.","4191f6ee":"<a id = \"16\"><\/a><h1 id=\"Autoregression Model\"><p  style=\"font-weight: bold; font-family:Verdana; font-size:18px; color:#47A8EC; \">Autoregression Model<\/p><\/h1>\nAn autoregression model is a linear regression model that uses lagged variables as input variables.\n\nWe could calculate the linear regression model manually using the LinearRegession class in scikit-learn and manually specify the lag input variables to use.\n\nAlternately, the statsmodels library provides an autoregression model where you must specify an appropriate lag value and trains a linear regression model. It is provided in the AutoReg class.","10e51c86":"<a id = \"8\"><\/a><h1 id=\"Converting Data to Stationary\"><span class=\"label label-default\" style=\"background-color:#47A8EC; border-radius:6px; font-weight: bold; font-family:Verdana; font-size:22px; color:#FBFAFC; \">Converting Data to Stationary<\/span><\/h1>\n<a id = \"9\"><\/a><h1 id=\"Logarithmic Transformation with Box-Cox\"><p style=\"font-weight: bold; font-family:Verdana; font-size:18px; color:#47A8EC; \">Logarithmic Transformation with Box-Cox<\/p><\/h1>\nThe Box-Cox transform is a configurable data transform method that supports both square root and log transform, as well as a suite of related transforms.","2c93d963":"<a id = \"7\"><\/a><h1 id=\"Augmented Dickey-Fuller Test\"><p style=\"font-weight: bold; font-family:Verdana; font-size:18px; color:#47A8EC; \">Augmented Dickey-Fuller Test<\/p><\/h1>\n\nAugmented Dickey-Fuller Test is a common statistical test used to test whether a given time series is stationary or not. We can achieve this by defining the null and alternate hypothesis.\n\n**Null Hypothesis:** Time Series is non-stationary. It gives a time-dependent trend.   \n**Alternate Hypothesis:** Time Series is stationary. In another term, the series doesn\u2019t depend on time.   \n\n**ADF or t Statistic < critical values:** Reject the null hypothesis, time series is stationary.   \n**ADF or t Statistic > critical values:** Failed to reject the null hypothesis, time series is non-stationary.","762005b5":"<a id = \"3\"><\/a><h1 id=\"Rolling Statistics\"><span class=\"label label-default\" style=\"background-color:#47A8EC; border-radius:6px; font-weight: bold; font-family:Verdana; font-size:22px; color:#FBFAFC; \">Rolling Statistics<\/span><\/h1>\nThe first thing to do in any data analysis task is to plot the data. Graphs enable many features of the data to be visualised, including patterns, unusual observations, changes over time, and relationships between variables. The features that are seen in plots of the data must then be incorporated, as much as possible, into the forecasting methods to be used. ","8ce539e2":"<a id = \"18\"><\/a><h1 id=\"ARIMA Model\"><p style=\"font-weight: bold; font-family:Verdana; font-size:18px; color:#47A8EC; \">ARIMA Model<\/p><\/h1>\nThe statsmodels library provides the capability to fit an ARIMA model.\n\nAn ARIMA model can be created using the statsmodels library as follows:\n\n**1.** Define the model by calling ARIMA() and passing in the **p**, **d**, and **q** parameters.   \n**2.** The model is prepared on the training data by calling the fit() function.   \n**3.** Predictions can be made by calling the predict() function and specifying the index of the time or times to be predicted.\n\nLet\u2019s start off with something simple. We will fit an ARIMA model to the entire Electric Production dataset.","f0197b0d":"The p-value is obtained is greater than significance level of 0.05 and the ADF statistic is higher than any of the critical values.\n\nClearly, there is no reason to reject the null hypothesis. So, the time series is in fact **non-stationary**.","9b9eb893":"Let's test stationarity again.","d04aa386":"<a id = \"19\"><\/a><h1 id=\"Mean Squared Errors\"><p style=\"font-weight: bold; font-family:Verdana; font-size:18px; color:#47A8EC; \">Mean Squared Errors<\/p><\/h1>"}}