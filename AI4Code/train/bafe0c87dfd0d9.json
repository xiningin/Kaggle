{"cell_type":{"924d2b0b":"code","318c75a0":"code","22fb9d7e":"code","690c4e6e":"code","5a673d78":"code","176509bf":"code","0a9a84b4":"code","5238933f":"code","0d1cde10":"code","b8ff9bce":"code","6d19d659":"code","3fdc069e":"code","07fd63f4":"code","73d35d53":"code","bf07c861":"code","72108537":"code","12b847bb":"code","1fd27cff":"code","a771c313":"code","693f0207":"code","5bf8aa99":"code","2df0758a":"code","4cbb0b07":"code","b1a97668":"code","9731fd4d":"code","5a211e14":"code","a1a0b1c0":"code","3e94e897":"code","69bf9f5a":"code","ea97dc37":"code","6f4ceb1f":"code","25a68142":"code","aa9f39f0":"code","b99e737e":"code","6f1044c9":"code","02ba27d8":"code","9add63e7":"code","1bdd084b":"code","a6f8209f":"code","d9d57e84":"code","d87bd285":"code","295303ca":"code","be80e5df":"markdown","c1754c87":"markdown","289d2699":"markdown","aa30c510":"markdown","a43cc878":"markdown","499dddc4":"markdown","19686ea5":"markdown","69a2b8ba":"markdown","414b2bac":"markdown","58d105f5":"markdown","d8d00333":"markdown"},"source":{"924d2b0b":"import numpy as np\nimport pandas as pd\nimport os\nimport json\nfrom pandas.io.json import json_normalize\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nprint(os.listdir(\"..\/input\"))","318c75a0":"# shared from this kernel\n# https:\/\/www.kaggle.com\/julian3833\/1-quick-start-read-csv-and-flatten-json-fields\n# def load_df(csv_path='..\/input\/train.csv', nrows=None):\n#     JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n#     df = pd.read_csv(csv_path, dtype={'fullVisitorId': 'str'}, nrows=nrows)\n#     for column in JSON_COLUMNS:\n#         df = df.join(pd.DataFrame(df.pop(column).apply(pd.io.json.loads).values.tolist(), index=df.index))\n\n#     return df\n\ndef load_df(csv_path='..\/input\/train.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': 'str'}, # Important!!\n                     nrows=nrows)\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n    return df","22fb9d7e":"%%time\ndf_train = load_df(nrows=100000)","690c4e6e":"df_test = load_df(nrows=100000)","5a673d78":"import missingno as msno\nmsno.matrix(df_train);\nmsno.dendrogram(df_train);","176509bf":"# remove constant cols\nconstant_cols = []\nall_cols = df_train.columns\nfor col in all_cols:\n    if df_train[col].nunique() == 1:\n        constant_cols.append(col)\n        \ndf_train.drop(constant_cols, axis=1, inplace=True)","0a9a84b4":"# convert numericals\ndef convert_to_numerical(df, cols):\n    for col in cols:\n        df[col] = pd.to_numeric(df[col], errors='coerce')\n    return df\n\ntotal_cols = df_train.filter(like=\"totals\")\ndf_train = convert_to_numerical(df_train, total_cols)\ndf_train = convert_to_numerical(df_train, total_cols)\n\nother_numerical_cols = [\"trafficSource.adwordsClickInfo.page\", ]\ndf_train = convert_to_numerical(df_train, other_numerical_cols)\ndf_test = convert_to_numerical(df_test, other_numerical_cols)","5238933f":"# format dates\ndf_train[\"date\"] = pd.to_datetime(df_train[\"date\"], errors='coerce', format=\"%Y%m%d\")\ndf_train['visitStartTime'] = pd.to_datetime(df_train['visitStartTime'],unit='s')","0d1cde10":"# These values are considered NONE\nDEFAULT_VALUE = [\"not available in demo dataset\", \"(none)\"]\n\n# then all categoricals default values will be \"NA\"\nFILLNA_VALUE_CAT = \"NA\"\nfor col in df_train.select_dtypes(np.object).columns:\n    df_train[col].replace(DEFAULT_VALUE, value=None, inplace=True)\n    df_train[col].fillna(FILLNA_VALUE_CAT, inplace=True)","b8ff9bce":"FULL_ID = \"fullVisitorId\"\nlist_feature_dfs = []","6d19d659":"# basic stats\nnumerical_cols = [\"totals.hits\", \"totals.pageviews\"]\ndf_feat_num = df_train.groupby(FULL_ID)[numerical_cols].agg([\"mean\", \"std\", \"sem\"])\ndf_feat_num.columns = [' '.join(col).strip() for col in df_feat_num.columns.values]\n\n# number of visits\ndf_num_visits = df_train.groupby(FULL_ID).size().to_frame('num_visits')\n\n# number of sessions\ndf_num_sessions = df_train.groupby(FULL_ID)[\"sessionId\"].nunique().to_frame(\"num_sessions\")\n\nlist_feature_dfs.append(df_feat_num)\nlist_feature_dfs.append(df_num_visits)\nlist_feature_dfs.append(df_num_sessions)","3fdc069e":"# consider as categorical all columns that have unique values <= 100\n# this is a feature engineering parameter\nNUM_UNIQUE_VALS_TO_BE_CAT = 100\nEXEMPT_COLS = [FULL_ID, \"sessionId\", \"trafficSource.adwordsClickInfo.gclId\"]\ncategorical_cols = []\ntext_cols = []\n# all categorical-like columns\nfor col in df_train.select_dtypes(np.object).drop(EXEMPT_COLS, axis=1).columns:\n    if len(df_train[col].value_counts()) <= NUM_UNIQUE_VALS_TO_BE_CAT:\n        categorical_cols.append(col)\n    else:\n        text_cols.append(col)","07fd63f4":"categorical_cols","73d35d53":"text_cols","bf07c861":"# this can be considered as a regularization parameter. \n# All values outside of the top THRESH_CAT are considered \"others\"\nTHRESH_CAT = 20\nOTHERS = \"OTHERS\"\n\ndef group_id_categorical(df, col, threshold_categorical = THRESH_CAT, others_value = OTHERS):    \n    vc = df[col].value_counts()\n\n    list_top = vc[:threshold_categorical].index\n    \n    col_series = np.where(df[col].isin(list_top), df[col], others_value)\n    id_series = df[FULL_ID]\n\n    df_group_col = pd.DataFrame({FULL_ID : id_series, col : col_series})\n    df_group_col = df_group_col.groupby([FULL_ID, col]).size().unstack()\n    \n    # naming: origin_variable.value\n    df_group_col.columns = col + \"=\" + df_group_col.columns\n    \n    return df_group_col.fillna(0)","72108537":"for col in categorical_cols:\n    df_group_col = group_id_categorical(df_train, col)\n    list_feature_dfs.append(df_group_col)","12b847bb":"df_train[\"date-month\"] = \"month-\" + df_train[\"date\"].dt.month.apply(str)\ndf_train[\"date-dayofweek\"] = \"dayofweek-\" + df_train[\"date\"].dt.day_name()\ndf_train['date-weekofmonth'] = \"weekofmonth-\" + df_train[\"date\"].apply(lambda d: str((d.day-1) \/\/ 7 + 1))\n\ndf_date_month = df_train.groupby([FULL_ID, \"date-month\"]).size().unstack().fillna(0)\ndf_date_dayofweek = df_train.groupby([FULL_ID, \"date-dayofweek\"]).size().unstack().fillna(0)\ndf_date_weekofmonth = df_train.groupby([FULL_ID, \"date-weekofmonth\"]).size().unstack().fillna(0)\n\nlist_feature_dfs.append(df_date_month)\nlist_feature_dfs.append(df_date_dayofweek)\nlist_feature_dfs.append(df_date_weekofmonth)","1fd27cff":"df_train[\"visit-month\"] = \"visit-month-\" + df_train[\"visitStartTime\"].dt.month.apply(str)\ndf_train[\"visit-dayofweek\"] = \"visit-dayofweek-\" + df_train[\"visitStartTime\"].dt.day_name()\ndf_train['visit-weekofmonth'] = \"visit- weekofmonth-\" + df_train[\"visitStartTime\"].apply(\n    lambda d: str((d.day-1) \/\/ 7 + 1))\ndf_train[\"time-hour\"] = \"hour-\" + df_train[\"visitStartTime\"].dt.hour.apply(str)\n\ndf_visit_month = df_train.groupby([FULL_ID, \"visit-month\"]).size().unstack().fillna(0)\ndf_visit_dayofweek = df_train.groupby([FULL_ID, \"visit-dayofweek\"]).size().unstack().fillna(0)\ndf_visit_weekofmonth = df_train.groupby([FULL_ID, \"visit-weekofmonth\"]).size().unstack().fillna(0)\ndf_time_hour = df_train.groupby([FULL_ID, \"time-hour\"]).size().unstack().fillna(0)\n\nlist_feature_dfs.append(df_visit_month)\nlist_feature_dfs.append(df_visit_dayofweek)\nlist_feature_dfs.append(df_visit_weekofmonth)\nlist_feature_dfs.append(df_time_hour)","a771c313":"from sklearn.feature_extraction.text import CountVectorizer\ncol = \"trafficSource.source\"\ndf = df_train\n\ndef group_id_text(df, col, min_df=3):\n    df_text = df.groupby(FULL_ID)[col].apply(lambda x: \"%s\" % ' '.join(x))\n    df_text_index = df_text.index\n\n    count_vec = CountVectorizer(binary=True, min_df=min_df)\n    df_text = pd.DataFrame(count_vec.fit_transform(df_text).toarray(), index=df_text_index, \n                 columns=count_vec.get_feature_names())\n    df_text.columns = col + \"=\" + df_text.columns\n    return df_text","693f0207":"for col in text_cols:\n    list_feature_dfs.append(group_id_text(df_train, col))","5bf8aa99":"from functools import reduce\ndef join_dfs(ldf, rdf):\n    return ldf.join(rdf, how='left')\n\ndf_features = reduce(join_dfs, list_feature_dfs)\n\ndf_features = df_features.fillna(0)\ndf_features.head()","2df0758a":"target_data = df_train.groupby(FULL_ID)[\"totals.transactionRevenue\"].sum()\n\n# log my target data\ntarget_data = np.log(1+ target_data)\n\n# reindex target data so features are aligned\ntarget_data = target_data.reindex(df_features.index)","4cbb0b07":"df_features.shape","b1a97668":"from lightgbm.sklearn import LGBMRegressor\n\nmodel=LGBMRegressor(boosting_type='gbdt', max_depth=5, n_estimators=1000, \n                    max_bin=255,subsample_for_bin=50000,\n                    min_child_weight=3,min_child_samples=10,subsample=0.6,subsample_freq=1,colsample_bytree=0.6,\n                    seed=23,silent=False,nthread=-1,n_jobs=-1)","9731fd4d":"# from sklearn.ensemble import RandomForestRegressor\n# params = {\"n_estimators\" : 100, \"max_depth\" : 7, \"min_samples_split\" : 0.001, \"max_features\" : \"sqrt\"}\n# rfr = RandomForestRegressor(verbose=1, n_jobs=4)\n# rfr.set_params(**params)","5a211e14":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error\n\nrandom_seed = 1234\nX_train, X_test, y_train, y_test = train_test_split(df_features, target_data, test_size=0.2, \n                                                    random_state = random_seed)","a1a0b1c0":"PATIENCE = 3\n# evaluation set\nX_train1, X_val, y_train1, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state = random_seed)\n\neval_params = {\"eval_set\": (X_val, y_val), \"eval_metric\":'l2_loss', \"early_stopping_rounds\":PATIENCE}","3e94e897":"model.fit(X_train, y_train, **eval_params)\n\ny_preds = model.predict(X_test)","69bf9f5a":"print(\"MSE:\", mean_squared_error(y_test, y_preds))","ea97dc37":"list_test_feature_dfs = []","6f4ceb1f":"df_test = convert_to_numerical(df_test, total_cols)\ndf_test = convert_to_numerical(df_test, other_numerical_cols)","25a68142":"for col in df_test.select_dtypes(np.object).columns:\n    df_test[col].replace(DEFAULT_VALUE, value=None, inplace=True)\n    df_test[col].fillna(FILLNA_VALUE_CAT, inplace=True)","aa9f39f0":"# format dates\ndf_test[\"date\"] = pd.to_datetime(df_test[\"date\"], errors='coerce', format=\"%Y%m%d\")\ndf_test['visitStartTime'] = pd.to_datetime(df_test['visitStartTime'],unit='s')","b99e737e":"# basic stats\nnumerical_cols = [\"totals.hits\", \"totals.pageviews\"]\ndf_feat_num = df_test.groupby(FULL_ID)[numerical_cols].agg([\"mean\", \"std\", \"sem\"])\ndf_feat_num.columns = [' '.join(col).strip() for col in df_feat_num.columns.values]\n\n# number of visits\ndf_num_visits = df_test.groupby(FULL_ID).size().to_frame('num_visits')\n\n# number of sessions\ndf_num_sessions = df_train.groupby(FULL_ID)[\"sessionId\"].nunique().to_frame(\"num_sessions\")\n\nlist_test_feature_dfs.append(df_feat_num)\nlist_test_feature_dfs.append(df_num_visits)\nlist_test_feature_dfs.append(df_num_sessions)","6f1044c9":"for col in categorical_cols:\n    df_group_col = group_id_categorical(df_test, col)\n    list_test_feature_dfs.append(df_group_col)\n    \nfor col in text_cols:\n    list_test_feature_dfs.append(group_id_text(df_test, col))","02ba27d8":"df_test[\"date-month\"] = \"month-\" + df_test[\"date\"].dt.month.apply(str)\ndf_test[\"date-dayofweek\"] = \"dayofweek-\" + df_test[\"date\"].dt.day_name()\ndf_test['date-weekofmonth'] = \"weekofmonth-\" + df_test[\"date\"].apply(lambda d: str((d.day-1) \/\/ 7 + 1))\n\ndf_date_month = df_test.groupby([FULL_ID, \"date-month\"]).size().unstack().fillna(0)\ndf_date_dayofweek = df_test.groupby([FULL_ID, \"date-dayofweek\"]).size().unstack().fillna(0)\ndf_date_weekofmonth = df_test.groupby([FULL_ID, \"date-weekofmonth\"]).size().unstack().fillna(0)\n\nlist_test_feature_dfs.append(df_date_month)\nlist_test_feature_dfs.append(df_date_dayofweek)\nlist_test_feature_dfs.append(df_date_weekofmonth)","9add63e7":"df_test[\"visit-month\"] = \"visit-month-\" + df_test[\"visitStartTime\"].dt.month.apply(str)\ndf_test[\"visit-dayofweek\"] = \"visit-dayofweek-\" + df_test[\"visitStartTime\"].dt.day_name()\ndf_test['visit-weekofmonth'] = \"visit- weekofmonth-\" + df_test[\"visitStartTime\"].apply(\n    lambda d: str((d.day-1) \/\/ 7 + 1))\ndf_test[\"time-hour\"] = \"hour-\" + df_test[\"visitStartTime\"].dt.hour.apply(str)\n\ndf_visit_month = df_test.groupby([FULL_ID, \"visit-month\"]).size().unstack().fillna(0)\ndf_visit_dayofweek = df_test.groupby([FULL_ID, \"visit-dayofweek\"]).size().unstack().fillna(0)\ndf_visit_weekofmonth = df_test.groupby([FULL_ID, \"visit-weekofmonth\"]).size().unstack().fillna(0)\ndf_time_hour = df_test.groupby([FULL_ID, \"time-hour\"]).size().unstack().fillna(0)\n\nlist_test_feature_dfs.append(df_visit_month)\nlist_test_feature_dfs.append(df_visit_dayofweek)\nlist_test_feature_dfs.append(df_visit_weekofmonth)\nlist_test_feature_dfs.append(df_time_hour)","1bdd084b":"df_test_features = reduce(join_dfs, list_test_feature_dfs)\n\n# add columns that are not in the test set\nmissing_columns = df_features.columns.difference(df_test_features.columns)\ndf_test_features = df_test_features.reindex(columns = df_test_features.columns.tolist() + missing_columns.tolist())\n\n# fill with 0\ndf_test_features = df_test_features.fillna(0)\n\n# reorder columns\ndf_test_features = df_test_features[df_features.columns.tolist()]\n\ndf_test_features.head()","a6f8209f":"y_test_preds = model.predict(df_test_features)","d9d57e84":"df_submission = pd.DataFrame({\"PredictedLogRevenue\" : \n                             y_test_preds}, index=df_test_features.index)","d87bd285":"df_sample_submission = pd.read_csv(\"..\/input\/sample_submission.csv\", index_col=0)\ndf_sample_submission.update(df_submission)\ndf_sample_submission.reset_index(inplace=True)","295303ca":"df_sample_submission.to_csv('submission.csv', index=False)","be80e5df":"# Preprocessing\n1. Remove columns with only one value\n2. Convert numericals\n3. Convert dates\n4. Convert \"not available in demo dataset\" to null","c1754c87":"# Lag Variables - TODO","289d2699":"# Feature Engineering\n1. Compile numerical variables\n2. Categoricals (top 100 categoricals count per person)\n3. Text features\n4. Date variables\n5. Lag variables - TODO","aa30c510":"# RandomForest Regressor\n1. Tuning = TODO","a43cc878":"# Date Variables","499dddc4":"# Join them all","19686ea5":"## Text Column","69a2b8ba":"## Categorical group","414b2bac":"- Very manual tuning. <b> You should do random search!<\/b>\n    - 3.1501, max_depth=7, subsample=1, colsample_bytree=1\n    - 3.3772, max_depth=7, subsample=0.8, colsample_bytree=0.8\n    - 3.2647, max_depth=7, subsample=0.6, colsample_bytree=0.6\n    \n    - 3.2742, max_depth=5, subsample=1, colsample_bytree=1\n    - 3.1380, max_depth=5, subsample=0.8, colsample_bytree=0.8\n    - **2.9920, max_depth=5, subsample=0.6, colsample_bytree=0.6**\n    \n    - 3.2010, max_depth=5, subsample=1, colsample_bytree=1\n    - 3.1321, max_depth=5, subsample=0.8, colsample_bytree=0.8\n    - 3.1340 max_depth=5, subsample=0.6, colsample_bytree=0.6","58d105f5":"# Missing Plots\nA dendrogram plot from missingno clusters features based on their \"null correlation\". Transaction revenue seems to be related to the traffic sources -- if they are not null.","d8d00333":" # Submission\n TODO: create functions from the above\n \n TODO: full dataset prediction and submission"}}