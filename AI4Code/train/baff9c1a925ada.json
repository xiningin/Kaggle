{"cell_type":{"5d3066a1":"code","a07f96d9":"code","01521eff":"code","42411563":"code","61a467a5":"code","e25d26f7":"code","673ee7d1":"code","ace76844":"code","8d804cc9":"code","9bd7fc27":"code","a422ed31":"code","7af1d9bf":"code","64badd44":"code","074899f8":"code","65db66e2":"code","c27f55c2":"code","9f6557ad":"code","0ca688f1":"code","e63ff0c0":"code","16698b77":"code","fde9bb76":"code","51286ce6":"code","23ac1539":"code","1214f104":"code","9f50cc82":"code","d7103482":"code","0d9122dd":"code","cc4314cd":"code","5346e3e2":"code","f4f27b38":"code","e8e4562e":"code","16f0e731":"code","c1af5595":"code","e919dcad":"code","bc5458d6":"code","dd6164c3":"code","6ef22de2":"code","370e37ac":"code","2986f6a0":"code","f45e5617":"code","1e52f66c":"code","4f849870":"code","02b7c540":"code","5129c108":"code","309528d7":"code","6846e189":"code","c06f0626":"code","b00e3395":"code","e4299adc":"code","f25dbda9":"code","76fa8e1d":"code","dac1c9ed":"markdown","ee8ddf2a":"markdown","537dbb14":"markdown","7156c015":"markdown","6e1eeb23":"markdown","551de831":"markdown","b1e77dbc":"markdown","4aaf2630":"markdown","b56dd668":"markdown","76162694":"markdown","baee1b5a":"markdown","88cec23e":"markdown","b65fa5ec":"markdown"},"source":{"5d3066a1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set_style('darkgrid')\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\n#from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n#from sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\n#from sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, validation_curve, cross_val_predict\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, roc_auc_score, make_scorer\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\nimport time\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a07f96d9":"submission = pd.read_csv(\"..\/input\/santander-customer-transaction-prediction\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/santander-customer-transaction-prediction\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/santander-customer-transaction-prediction\/train.csv\")","01521eff":"submission.head(10)","42411563":"train.head(10)","61a467a5":"test.head(10)","e25d26f7":"train.describe()","673ee7d1":"train.info()","ace76844":"test.info()","8d804cc9":"train.columns","9bd7fc27":"test.columns","a422ed31":"train.isna().sum()","7af1d9bf":"test.isna().sum()","64badd44":"columns = train.columns\npercent_missing = train.isnull().sum() * 100 \/ len(train)\nmissing_value_data = pd.DataFrame({'column_name': columns,\n                                 'percent_missing': percent_missing})\nmissing_value_data.sort_values('percent_missing')","074899f8":"columnss = test.columns\npercent_missing = test.isnull().sum() * 100 \/ len(test)\nmissing_value_data = pd.DataFrame({'column_name': columnss,\n                                 'percent_missing': percent_missing})\nmissing_value_data.sort_values('percent_missing')","65db66e2":"sns.heatmap(train.corr())\ntrain.corr()","c27f55c2":"train_correlations = train.drop([\"target\"], axis=1).corr()\ntrain_correlations = train_correlations.values.flatten()\ntrain_correlations = train_correlations[train_correlations != 1]\n\ntest_correlations = test.corr()\ntest_correlations = test_correlations.values.flatten()\ntest_correlations = test_correlations[test_correlations != 1]\n\nplt.figure(figsize=(20,5))\nsns.distplot(train_correlations, color=\"Red\", label=\"train\")\nsns.distplot(test_correlations, color=\"Yellow\", label=\"test\")\nplt.xlabel(\"Correlation values found in train (except 1)\")\nplt.ylabel(\"Density\")\nplt.title(\"Are there correlations between features?\"); \nplt.legend();","9f6557ad":"features = train.columns.values[2:202]\nunique_max_train = []\nunique_max_test = []\nfor feature in features:\n    values = train[feature].value_counts()\n    unique_max_train.append([feature, values.max(), values.idxmax()])\n    values = test[feature].value_counts()\n    unique_max_test.append([feature, values.max(), values.idxmax()])","0ca688f1":"np.transpose((pd.DataFrame(unique_max_train, columns=['Feature', 'Max duplicates', 'Value'])).\\\n            sort_values(by = 'Max duplicates', ascending=False).head(15))","e63ff0c0":"train.shape, test.shape, submission.shape","16698b77":"train.target.dtype","fde9bb76":"train.target = train.target.astype(np.int32)\ntrain.target.dtype","51286ce6":"f,ax=plt.subplots(1,2, figsize=(12,4))\ntrain.target.value_counts().plot.pie(explode=[0,0.12],autopct='%1.3f%%',ax=ax[0])\nsns.countplot(train['target'])\nplt.show()","23ac1539":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\nax = sns.distplot(train[train['target']==0].var_10, bins = 30, ax = axes[0], kde = False)\nax.set_title('0')\nax = sns.distplot(train[train['target']==1].var_10, bins = 30, ax = axes[1], kde = False)\nax.set_title('1')","1214f104":"f,ax = plt.subplots(1,2,figsize=(18,10))\nzero = train.loc[train[\"target\"] == 0]\none = train.loc[train[\"target\"] == 1]\nk = sns.kdeplot(zero.var_10,zero.var_100,shade=True,shade_lowest=True,cmap=\"magma\",ax=ax[0])\nk = sns.kdeplot(one.var_10,one.var_100,shade=True,shade_lowest=True,cmap=\"plasma_r\",ax=ax[1])\nax[0].set_xlabel(\"var_10\", fontsize=18)\nax[1].set_xlabel(\"var_10\", fontsize=18)\nax[0].set_ylabel(\"var_100\", fontsize=18)\nax[1].set_ylabel(\"var_100\", fontsize=18)\nax[0].set_title(\"Zero\", fontsize=18)\nax[1].set_title(\"One\", fontsize=18)\nplt.show()","9f50cc82":"f,ax = plt.subplots(1,2,figsize=(12,12))\nn = sns.scatterplot(train[\"target\"],train[\"var_20\"],hue=train[\"target\"],s=100,ax=ax[0])\nn = sns.barplot(train[\"target\"],train[\"var_20\"],ax=ax[1])\nplt.show()","d7103482":"plt.figure(figsize=(18,18))\nn = sns.pairplot(train[[\"var_20\",\"var_40\",\"var_60\",\"var_80\",\"var_100\",\"var_150\",\"target\"]],hue=\"target\",palette=\"husl\")\nplt.show()","0d9122dd":"plt.figure(figsize=(18,18))\nn = sns.pairplot(train[[\"var_10\",\"var_30\",\"var_50\",\"var_70\",\"var_90\",\"var_180\",\"target\"]],hue=\"target\",palette=\"husl\")\nplt.show()","cc4314cd":"X, y = train.iloc[:,2:], train.iloc[:,1]","5346e3e2":"scale = MinMaxScaler()\nscale.fit(X)\nfeatures = pd.DataFrame(scale.transform(X))","f4f27b38":"X","e8e4562e":"y","16f0e731":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","c1af5595":"print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)","e919dcad":"# 1-Nearest Neighbor Classifier\n#NeNeClassifier = KNeighborsClassifier(n_neighbors=3)\n#-------------------------------------------------------------------\n# 2-Logistic Regression\nLRClassifer = LogisticRegression()\n#-------------------------------------------------------------------\n# 3-SVM Sigmoid Classifier\n#SVMClassifer = SVC(kernel='sigmoid')\n#-------------------------------------------------------------------\n# 4-Naive Bayes Classifier\nNBClassifer = GaussianNB()\n#-------------------------------------------------------------------\n# 5-Decision Tree Classifier\nDTClassifer = DecisionTreeClassifier(min_impurity_split=2, min_samples_leaf=9, random_state=25)\n#-------------------------------------------------------------------\n# 6-Random Forest Classifier\n#RFClassifer = RandomForestClassifier(n_estimators=300, random_state=10)\n#-------------------------------------------------------------------\n# 6-XGBoost Classifier\nXGBClassifer = XGBClassifier(n_estimators=100)","bc5458d6":"#NeNeClassifier.fit(X_train, y_train)\nLRClassifer.fit(X_train, y_train)\n#SVMClassifer.fit(X_train, y_train)","dd6164c3":"NBClassifer.fit(X_train, y_train)","6ef22de2":"DTClassifer.fit(X_train, y_train)","370e37ac":"#RFClassifer.fit(X_train, y_train)","2986f6a0":"XGBClassifer.fit(X_train, y_train)","f45e5617":"#y_preds = NeNeClassifier.predict(X_test)\ny_preds1 = LRClassifer.predict(X_test)\n#y_preds2 = SVMClassifer.predict(X_test)","1e52f66c":"y_preds3 = NBClassifer.predict(X_test)","4f849870":"y_preds4 = DTClassifer.predict(X_test)","02b7c540":"#y_preds5 = RFClassifer.predict(X_test)","5129c108":"y_preds6 = XGBClassifer.predict(X_test)","309528d7":"print(y_preds1[:10],'\\n',y_test[:10])\nprint(\"*******************************************************\")\nprint(y_preds3[:10],'\\n',y_test[:10])\nprint(\"*******************************************************\")\nprint(y_preds4[:10],'\\n',y_test[:10])\nprint(\"*******************************************************\")\nprint(y_preds6[:10],'\\n',y_test[:10])","6846e189":"print(\"Accuracy of Logistic Regression\",accuracy_score(y_test, y_preds1))\nprint(\"Accuracy of Naive Bayes\",accuracy_score(y_test, y_preds3))\nprint(\"Accuracy of Decision Tree\",accuracy_score(y_test, y_preds4))\nprint(\"Accuracy of XGBoost\",accuracy_score(y_test, y_preds6))","c06f0626":"print(\"\\nLogistic Regression\\n\",classification_report(y_test, y_preds1))\nprint(\"\\nNaive Bayes\\n\",classification_report(y_test, y_preds3))\nprint(\"\\nDecision Tree\\n\",classification_report(y_test, y_preds4))\nprint(\"\\nXGBoost\\n\",classification_report(y_test, y_preds6))","b00e3395":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nlogit_roc_auc = roc_auc_score(y_test, LRClassifer.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, LRClassifer.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","e4299adc":"nb_roc_auc = roc_auc_score(y_test, NBClassifer.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, NBClassifer.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Naive Bayes (area = %0.2f)' % nb_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('')\nplt.legend(loc=\"lower right\")\nplt.savefig('NB_ROC')\nplt.show()","f25dbda9":"dt_roc_auc = roc_auc_score(y_test, DTClassifer.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, DTClassifer.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Decision Tree (area = %0.2f)' % dt_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('')\nplt.legend(loc=\"lower right\")\nplt.savefig('DT_ROC')\nplt.show()","76fa8e1d":"xgboost_roc_auc = roc_auc_score(y_test, XGBClassifer.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, XGBClassifer.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='XGBoost (area = %0.2f)' % xgboost_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('')\nplt.legend(loc=\"lower right\")\nplt.savefig('XG_ROC')\nplt.show()","dac1c9ed":"So, first part is an EDA of the data","ee8ddf2a":"****KNN****, ****Logistic Regression****, ****Naive Baeys****, ****Decision Tree****, ****Random Forest****, ****XGBoost****","537dbb14":"No need to encode data. Dividing into Features and Labels.","7156c015":"The correlation between the features is very small","6e1eeb23":"Both train and test data have 200,000 entries and 202, respectivelly 201 (2 in submission) columns","551de831":"Train contains:\n\n* ID_code (string);\n* target;\n* 200 numerical variables, named from var_0 to var_199;\n\nTest contains:\n\n* ID_code (string);\n* 200 numerical variables, named from var_0 to var_199;","b1e77dbc":"Divide Data into Train and Test","4aaf2630":"No linear correlation","b56dd668":"Scale the Features","76162694":"No missing values, cool","baee1b5a":"The data is unbalanced with respect with target value","88cec23e":"# Santander Customer Transaction Prediction\n![Santander Customer Transaction Prediction](https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/santander\/atm_image.png)","b65fa5ec":"****Duplicate values****"}}