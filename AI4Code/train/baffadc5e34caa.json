{"cell_type":{"425a3183":"code","9d3d35bb":"code","dfac712c":"code","85c8804b":"code","5e7f9eb1":"code","4ca10b90":"code","7d514524":"code","31eb9652":"code","32683c27":"code","f23e1cf1":"code","c2e582b8":"code","7c715c87":"code","ea4c4ff9":"code","71c2d001":"code","89388221":"code","19e02559":"code","2fe1575b":"markdown","afcab795":"markdown","e4b7de59":"markdown","91c5f56b":"markdown","47cad558":"markdown","821366c4":"markdown","28695d21":"markdown","67a059bc":"markdown","5e202978":"markdown","d6259a58":"markdown","8be7fb12":"markdown","83f14003":"markdown"},"source":{"425a3183":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom collections import Counter\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n%matplotlib inline","9d3d35bb":"train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\nprint(train.shape)\ntrain.head()","dfac712c":"sns.countplot(train['label'])","85c8804b":"x_train = (train.iloc[:,1:].values).astype('float32') # all pixels\ny_train = train.iloc[:,0].values.astype('int32') # all labels\nx_test = test.values.astype('float32') # all pixels","5e7f9eb1":"%matplotlib inline\n# preview the images first\nplt.figure(figsize=(12,10))\nx, y = 10, 4\nfor i in range(40):  \n    plt.subplot(y, x, i+1)\n    plt.imshow(x_train[i].reshape((28,28)),interpolation='nearest')\nplt.show()","4ca10b90":"x_train = x_train\/255.0\nx_test = x_test\/255.0","7d514524":"print(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')","31eb9652":"X_train = x_train.reshape(x_train.shape[0], 28, 28,1)\nX_test = x_test.reshape(x_test.shape[0], 28, 28,1)","32683c27":"y_train = keras.utils.to_categorical(y_train, 10)\n\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, y_train, test_size = 0.1, random_state = 42)","f23e1cf1":"batch_size = 64\nepochs = 20\ninput_shape = (28, 28, 1)","c2e582b8":"model = Sequential()\n\nmodel.add(Conv2D(32, kernel_size=(3, 3),activation='relu',kernel_initializer='he_normal',input_shape=input_shape))\nmodel.add(Conv2D(32, kernel_size=(3, 3),activation='relu',kernel_initializer='he_normal'))\nmodel.add(MaxPool2D((2, 2)))\nmodel.add(Dropout(0.20))\n\nmodel.add(Conv2D(64, (3, 3), activation='relu',padding='same',kernel_initializer='he_normal'))\nmodel.add(Conv2D(64, (3, 3), activation='relu',padding='same',kernel_initializer='he_normal'))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(128, (3, 3), activation='relu',padding='same',kernel_initializer='he_normal'))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\n\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(10, activation='softmax'))","7c715c87":"model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics = ['accuracy'])","ea4c4ff9":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=15, # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images","71c2d001":"datagen.fit(X_train)\nmodel.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size), \n                    epochs = epochs, validation_data = (X_val,Y_val),\n                    verbose = 1, \n                    steps_per_epoch = X_train.shape[0] \/\/ batch_size\n                   )","89388221":"predictions = model.predict(X_test)\nresults = np.argmax(predictions, axis = 1)","19e02559":"submissions = pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\")\nsubmissions['Label'] = results\nsubmissions.to_csv('submission.csv', index = False)","2fe1575b":"### Predictions","afcab795":"### Fit Model","e4b7de59":"#### **Reshape**","91c5f56b":"### Define Model","47cad558":"### Submission","821366c4":"### Let\u2019s have a look at our data","28695d21":"#### **Number of Labels**","67a059bc":"#### Splitting our train data into train and validation data","5e202978":"### Description\nMNIST (\u201cModified National Institute of Standards and Technology\u201d) is the de facto \u201chello world\u201d dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\n\nIn this competition, your goal is to correctly identify digits from a dataset of tens of thousands of handwritten images. We\u2019ve curated a set of tutorial-style kernels which cover everything from regression to neural networks. We encourage you to experiment with different algorithms to learn first-hand what works well and how techniques compare.\n\n### Practice Skills\n* Computer vision fundamentals including simple neural networks\n* Classification methods such as SVM and K-nearest neighbors\n\n### Acknowledgements\nMore details about the dataset, including algorithms that have been tried on it and their levels of success, can be found at http:\/\/yann.lecun.com\/exdb\/mnist\/index.html. The dataset is made available under a Creative Commons Attribution-Share Alike 3.0 license.\n\n### Data Description\nThe data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine.\n\nEach image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n\nThe training data set, (train.csv), has 785 columns. The first column, called \u201clabel\u201d, is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n\nEach pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).\n\nFor example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below.\n\nVisually, if we omit the \u201cpixel\u201d prefix, the pixels make up the image like this:\n\n000 001 002 003 \u2026 026 027\n028 029 030 031 \u2026 054 055\n056 057 058 059 \u2026 082 083\n| | | | \u2026 | |\n728 729 730 731 \u2026 754 755\n756 757 758 759 \u2026 782 783\n\nThe test data set, (test.csv), is the same as the training set, except that it does not contain the \u201clabel\u201d column.\n\nYour submission file should be in the following format: For each of the 28000 images in the test set, output a single line containing the ImageId and the digit you predict. For example, if you predict that the first image is of a 3, the second image is of a 7, and the third image is of a 8, then your submission file would look like:\n\nImageId,Label\n1,3\n2,7\n3,8\n(27997 more lines)\n\nThe evaluation metric for this contest is the categorization accuracy, or the proportion of test images that are correctly classified. For example, a categorization accuracy of 0.97 indicates that you have correctly classified all but 3% of the images.\n\n### So let\u2019s begin here\u2026","d6259a58":"### Compile Model","8be7fb12":"### Load Data","83f14003":"Setting datatypes for pixels and labels"}}