{"cell_type":{"6376a390":"code","f6b4d512":"code","cad0fd7d":"code","32d3caf3":"code","d256bb90":"code","0787fa79":"code","6252ae7b":"code","78a29d61":"code","9e1cbef8":"code","603f1915":"code","c2478426":"code","0eb0df27":"code","baa5605a":"code","91b4a1e9":"code","58a2de03":"code","34a86630":"code","d35619a6":"code","16bdd00b":"code","a8db8fe7":"markdown","047910b5":"markdown","2282366f":"markdown"},"source":{"6376a390":"!apt-get -y install libenchant-dev","f6b4d512":"!pip install pyenchant","cad0fd7d":"import re\nimport enchant\nimport nltk\n\nfrom nltk.corpus import stopwords, words\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')","32d3caf3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d256bb90":"url = \"\/kaggle\/input\/formspring-data-for-cyberbullying-detection\/formspring_data.csv\"\ndf = pd.read_csv(url, sep='\\t')\ndf = df[df['ans'].notna()]","0787fa79":"df.columns","6252ae7b":"sum(df.ans1 == df.ans2) \/ len(df)","78a29d61":"sum(df.ans2 == df.ans3) \/ len(df)","9e1cbef8":"sum(df.ans1 == df.ans3) \/ len(df)","603f1915":"index = (df.ans1 == df.ans2) & (df.ans2 == df.ans3) & (df.ans1 == df.ans3)","c2478426":"df = df[index]","0eb0df27":"df.head()","baa5605a":"# Putting back apostrophe's may help\ndf.ques = df.ques.str.replace(\"&#039;\", \"'\") \n\n# Removing may not be effective since there is a word check implemented\ndf.ques = df.ques.str.replace(\"<br>\", \"\") \ndf.ques = df.ques.str.replace(\"&quot;\", \"\") \ndf.ques = df.ques.str.replace(\"<3\", \"\") ","91b4a1e9":"bully_df = df[df.ans1 == \"Yes\"].reset_index(drop=True)\n\ndf[df.ans1 == \"Yes\"].head()","58a2de03":"stop_words = stopwords.words(\"english\")\nlemmatizer = WordNetLemmatizer()\nenchant_dict = enchant.Dict(\"en_US\")\n\ndef tokenize(text):\n    # normalize case and remove punctuation\n    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n    \n    # tokenize text\n    tokens = word_tokenize(text)\n    \n    # lemmatize andremove stop words\n    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n    \n    #tokens = [word for word in tokens if enchant_dict.check(word)]\n\n    return tokens","34a86630":"for i in range(0,6):\n    print(bully_df.ques[i])\n    print(tokenize(bully_df.ques[i]),\"\\n\")","d35619a6":"import nltk\nnltk.download(['punkt', 'wordnet'])\n\nimport re\nimport numpy as np\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer","16bdd00b":"def load_data(path):\n    df = pd.read_csv(path, sep='\\t')\n    df = df[df['ans'].notna()]\n    \n    index = (df.ans1 == df.ans2) & (df.ans2 == df.ans3) & (df.ans1 == df.ans3)\n    df = df[index].reset_index(drop=True)\n    df = df[[\"ques\",\"ans\",\"ans1\"]]\n    \n    X = df.ans.values\n    y = df.ans1.values\n    return X, y\n\ndef tokenize(text):\n\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n\n    clean_tokens = []\n    for tok in tokens:\n        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n        clean_tokens.append(clean_tok)\n\n    return clean_tokens\n\ndef display_results(y_test, y_pred):\n    labels = np.unique(y_pred)\n    confusion_mat = confusion_matrix(y_test, y_pred, labels=labels)\n    accuracy = (y_pred == y_test).mean()\n\n    print(\"Labels:\", labels)\n    print(\"Confusion Matrix:\\n\", confusion_mat)\n    print(\"Accuracy:\", accuracy)\n    \ndef main():\n    url = \"\/kaggle\/input\/formspring-data-for-cyberbullying-detection\/formspring_data.csv\"\n    X, y = load_data(url)\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n\n    vect = CountVectorizer(tokenizer=tokenize)\n    tfidf = TfidfTransformer()\n    clf = RandomForestClassifier()\n\n    # train classifier\n    X_train_counts = vect.fit_transform(X_train)\n    X_train_tfidf = tfidf.fit_transform(X_train_counts)\n    clf.fit(X_train_tfidf, y_train)\n\n    # predict on test data\n    X_test_counts = vect.transform(X_test)\n    X_test_tfidf = tfidf.transform(X_test_counts)\n    y_pred = clf.predict(X_test_tfidf)\n    \n    # predict on test data\n    X_test_counts = vect.transform([\"whoa stop you stupid bitch\"])\n    X_test_tfidf = tfidf.transform(X_test_counts)\n    print(\"Given text: 'whoa stop you stupid bitch' \")\n    print(\"Prediction: {}\\n\".format(clf.predict(X_test_tfidf)))\n    \n\n    # display results\n    display_results(y_test, y_pred)\n\n\nmain()","a8db8fe7":"### Whole Process\nThe process includes tokenizing the input strings and creating pipelines for the ML model.","047910b5":"### Three Results\nThe data has three results, and I would like to only view data where all three results match.","2282366f":"### Tokenizer\nBuild a custom function that can gather actual words from a give string."}}