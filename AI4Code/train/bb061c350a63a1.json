{"cell_type":{"b6ca5b22":"code","cb7c3c16":"code","62e0abf8":"code","01a56b3e":"code","4201a44f":"code","028757f7":"code","c524ffbe":"code","5cb308f4":"code","0910982a":"code","5cc703d4":"code","693a4498":"code","070c120c":"code","20be1c0e":"code","e1ed87ea":"code","147415fb":"code","ea0e1acf":"code","0aa5a9a2":"code","3285955f":"code","769fb848":"code","c2bd0f6d":"code","abe3e12e":"code","22761e5e":"code","9a0f2406":"code","667841f1":"code","614d5ccf":"code","24aa7cbb":"code","874f3027":"markdown","fd17b3ee":"markdown","d8a30b82":"markdown","89b56ab8":"markdown","bba2aaed":"markdown","3ed4a977":"markdown","c172b118":"markdown","e3bf0c38":"markdown","f095270e":"markdown","610f1588":"markdown","90a7a0d8":"markdown","bca9fe48":"markdown","0b4e99d9":"markdown"},"source":{"b6ca5b22":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport pylab as pl\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","cb7c3c16":"test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ndata = pd.read_csv(\"..\/input\/titanic\/train.csv\")","62e0abf8":"data.head()","01a56b3e":"data_train_x = ['Sex']\n\ndata_train = pd.get_dummies(data[data_train_x])","4201a44f":"sns.violinplot(x = \"Sex\", y = \"Survived\", data = data, ci = False)","028757f7":"#setting the matrixes\nX = data_train.iloc[:].values","c524ffbe":"noOfTrainEx = X.shape[0] # no of training examples\nprint(\"noOfTrainEx: \",noOfTrainEx)\nnoOfWeights = X.shape[1]+1 # no of features+1 => weights\nprint(\"noOfWeights: \", noOfWeights)","5cb308f4":"ones = np.ones([noOfTrainEx, 1]) # create a array containing only ones \nX = np.concatenate([ones, X],1) # cocatenate the ones to X matrix\ntheta = np.ones((1, noOfWeights)) #np.array([[1.0, 1.0]])","0910982a":"y = data['Survived'].values.reshape(-1,1) # create the y matrix","5cc703d4":"print(X.shape)\nprint(theta.shape)\nprint(y.shape)","693a4498":"X[0]","070c120c":"def sigmoid(z):\n        return 1 \/ (1 + np.exp(-z))\ndef computeCost(h, y):\n    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()","20be1c0e":"#set hyper parameters\nalpha = 0.01\niters = 5000","e1ed87ea":"## Gradient Descent funtion\ndef gradientDescent(X, y, theta, alpha, iters):\n    cost = np.zeros(iters)\n    for i in range(iters):\n        z = X @ theta.T\n        h = sigmoid(z)\n        theta = theta - (alpha\/len(X)) * np.sum((h - y) * X, axis=0)\n        cost[i] = computeCost(h, y)\n        if i % 100 == 0: # just look at cost every ten loops for debugging\n            print(i, 'iteration, cost:', cost[i])\n    return (theta, cost)","147415fb":"g, cost = gradientDescent(X, y, theta, alpha, iters)  ","ea0e1acf":"print(g)","0aa5a9a2":"#plot the cost\nfig, ax = plt.subplots()  \nax.plot(np.arange(iters), cost, 'r')  \nax.set_xlabel('Iterations')  \nax.set_ylabel('Cost')  \nax.set_title('Error vs. Training Epoch')","3285955f":"axes = sns.violinplot(x = \"Sex\", y = \"Survived\", data = data, ci = False)\nx_vals = np.array(axes.get_xlim()) \ny_vals = g[0][0] + g[0][1]* x_vals #the line equation\nplt.plot(x_vals, y_vals, '--')","769fb848":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(C=1e20)\nmodel.fit(X, y)\nprint(model.coef_)\nprint(g)","c2bd0f6d":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(model, X, y, cv=3)\nC = confusion_matrix(y, predictions)\nprint(C)\n\n","abe3e12e":"TP, FP, FN, TN = C[0][0], C[0][1], C[1][0], C[1][1]\ndf_cm = pd.DataFrame(C, columns=np.unique(y), index = np.unique(y))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nplt.figure(figsize = (10,7))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(df_cm, cmap=\"Blues\", annot=True, fmt='d', annot_kws={\"size\": 16})","22761e5e":"accuracy = (TP+TN)\/noOfTrainEx\nprint(accuracy)","9a0f2406":"precision = TP\/(TP+FP)\nprint(precision)","667841f1":"recall = TP\/(TP+FN)\nprint(recall)","614d5ccf":"specificity = TN\/(TN+FP)\nprint(specificity)","24aa7cbb":"from scipy import stats\nF1 = stats.hmean([precision, recall])\nprint(F1)","874f3027":"## Specificity or TNR (True Negative Rate)\n\n![](https:\/\/miro.medium.com\/max\/1000\/1*8BqJnTfcbE7RCq3ZXUnkhg.png)","fd17b3ee":"## Recall or Sensitivity or TPR (True Positive Rate)\n\n![](https:\/\/miro.medium.com\/max\/1144\/1*Tmrc87X3hakNhpnc6XEZZg.png)","d8a30b82":"## AUC - Receiver Operating Characteristic (ROC) Curve \n\nAn ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n\nTrue Positive Rate = Recall \/Sensitivity\n\nFalse Positive Rate = 1 - Specificity\n\nAUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s.\n\n![](https:\/\/miro.medium.com\/max\/361\/1*pk05QGzoWhCgRiiFbz-oKQ.png)\n\nAn excellent model has AUC near to the 1 which means it has good measure of separability. A poor model has AUC near to the 0 which means it has worst measure of separability. In fact it means it is reciprocating the result. It is predicting 0s as 1s and 1s as 0s. And when AUC is 0.5, it means model has no class separation capacity whatsoever.","89b56ab8":"Few other metrics are: https:\/\/www.analyticsvidhya.com\/blog\/2019\/08\/11-important-model-evaluation-error-metrics\/","bba2aaed":"AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.\n\nAUC is desirable for the following two reasons:\n\n1. AUC is scale-invariant. It measures how well predictions are ranked, rather than their absolute values.\n1. AUC is classification-threshold-invariant. It measures the quality of the model's predictions irrespective of what classification threshold is chosen.\n\nHowever, both these reasons come with caveats, which may limit the usefulness of AUC in certain use cases:\n\n1. Scale invariance is not always desirable. For example, sometimes we really do need well calibrated probability outputs, and AUC won\u2019t tell us about that.\n\n1. Classification-threshold invariance is not always desirable. In cases where there are wide disparities in the cost of false negatives vs. false positives, it may be critical to minimize one type of classification error. For example, when doing email spam detection, you likely want to prioritize minimizing false positives (even if that results in a significant increase of false negatives). AUC isn't a useful metric for this type of optimization.","3ed4a977":"## AIC (Akaike Information Criteria) \nThe analogous metric of adjusted R\u00b2 in logistic regression is AIC. AIC is the measure of fit which penalizes model for the number of model coefficients. Therefore, we always prefer model with minimum AIC value.","c172b118":"## Null Deviance and Residual Deviance \n\nNull Deviance indicates the response predicted by a model with nothing but an intercept. Lower the value, better the model. Residual deviance indicates the response predicted by a model on adding independent variables. Lower the value, better the model.","e3bf0c38":"## Confusion Matrix\n\n![](https:\/\/miro.medium.com\/proxy\/1*7SgzmX05T81Ojaor9s5HWQ.png)\n\n1. True Positives (TP): when the actual class of the data point was 1(True) and the predicted is also 1(True)\n1. True Negatives (TN): when the actual class of the data point was 0(False) and the predicted is also 0(False)\n1. False Positives (FP):when the actual class of the data point was 0(False) and the predicted is 1(True)\n1. False Negatives (FN): When the actual class of the data point was 1(True) and the predicted is 0(False)\n\n","f095270e":"References taken from \nhttps:\/\/towardsdatascience.com\/how-to-select-the-right-evaluation-metric-for-machine-learning-models-part-3-classification-3eac420ec991","610f1588":"## F1 Score\n\n> F1 Score = Harmonic Mean(Precision, Recall)\n\nHarmonic mean is kind of an average when x and y are equal. But when x and y are different, then it\u2019s closer to the smaller number as compared to the larger number.\n\n","90a7a0d8":"## Precision\n\n![](https:\/\/miro.medium.com\/max\/1146\/1*1ibi70q9kZTTLibiqjE_Rg.png)","bca9fe48":"## Accuracy\n\n![](https:\/\/miro.medium.com\/max\/1064\/1*5XuZ_86Rfce3qyLt7XMlhw.png)\n\nDrawback:\n","0b4e99d9":"The first row is about the not-survived-predictions: \n- 468 passengers were correctly classified as not survived (called true negatives) and \n- 81 where wrongly classified as not survived (false positives).\n\nThe second row is about the survived-predictions: \n- 109 passengers where wrongly classified as survived (false negatives) and \n- 233 where correctly classified as survived (true positives)."}}