{"cell_type":{"022e8a26":"code","a4bb07be":"code","99336988":"code","2d6a2a0d":"code","942d482c":"code","65031740":"code","3b10253a":"code","e6263704":"code","c447b212":"code","7c05e9a3":"code","10975032":"code","90200322":"code","cf626835":"code","4e2afd5a":"code","a53f3483":"code","1b15544b":"code","13979f28":"code","14a58edb":"code","eaa7decd":"code","9acae1c1":"code","6e6f48a4":"code","e6ccbb9d":"code","1df40b65":"code","aa3ff027":"code","a5d15542":"code","5e94000d":"code","cedeb90a":"code","9313013d":"code","cd86c781":"code","93acd919":"code","9de43a15":"code","d51ad645":"code","7737a533":"code","da238439":"code","6e0565d4":"code","da65d941":"markdown","c9122a23":"markdown","6b4fafdd":"markdown","e502a471":"markdown","492d1cfd":"markdown","42b841c5":"markdown","02e62f96":"markdown","094a6544":"markdown","11b9ec98":"markdown","fb43c0d3":"markdown","9fb3319a":"markdown","d89761cc":"markdown","cf306829":"markdown","7be7dcd6":"markdown","e47df94b":"markdown","4da30b33":"markdown","c133ffa3":"markdown","f157ac2a":"markdown","ee09bffd":"markdown","66acc663":"markdown","63b6e308":"markdown","09c1d3a0":"markdown","b76d7391":"markdown","f9b61a25":"markdown"},"source":{"022e8a26":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","a4bb07be":"from fastai import *\nfrom fastai.vision import *\nfrom torchvision.models import * \n\nimport os\nimport matplotlib.pyplot as plt","99336988":"path = Path(\"..\/input\")\nlabels = pd.read_csv(path\/\"train_labels.csv\")\nlabels.head()","2d6a2a0d":"print(labels[\"label\"].nunique()); classes = list(set(labels[\"label\"])); classes","942d482c":"for i in classes:\n    print(\"Number of items in class {} is {}\".format(i,len(labels[labels[\"label\"] == i])))","65031740":"tfms = get_transforms(do_flip = True,flip_vert = True,max_zoom = 1.1)","3b10253a":"np.random.seed(123)\nsz = 32\ndata = ImageDataBunch.from_csv(path, folder = 'train', csv_labels = \"train_labels.csv\",\n                               test = 'test',suffix=\".tif\", size = sz,bs = 256,\n                               ds_tfms = tfms)\ndata.path = pathlib.Path('.')\ndata.normalize(imagenet_stats)","e6263704":"print(data.classes); data.c","c447b212":"data.show_batch(rows = 3)","7c05e9a3":"from sklearn.metrics import roc_auc_score\n\ndef auc_score(y_pred,y_true,tens=True):\n    score = roc_auc_score(y_true,torch.sigmoid(y_pred)[:,1])\n    if tens:\n        score = tensor(score)\n    return score","10975032":"arch = models.densenet121\nlearn = cnn_learner(data,arch,pretrained = True,ps = 0.45,metrics = [auc_score,accuracy])","90200322":"learn.lr_find()\nlearn.recorder.plot()","cf626835":"learn.fit_one_cycle(8,1e-2)","4e2afd5a":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot()","a53f3483":"learn.fit_one_cycle(2,max_lr = slice(1e-5,1e-3))","1b15544b":"newTfms = get_transforms(do_flip = True,flip_vert = True,max_zoom = 1.25)\nnewSz = 64\nnewData = ImageDataBunch.from_csv(path, folder = 'train', csv_labels = \"train_labels.csv\",\n                               test = 'test',suffix=\".tif\", size = newSz, ds_tfms = newTfms)\nnewData.path = pathlib.Path('.')\nnewData.normalize(imagenet_stats)\nlearn.data = newData","13979f28":"learn.freeze()\nlearn.lr_find()\nlearn.recorder.plot()","14a58edb":"learn.fit_one_cycle(8,1e-2)","eaa7decd":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot()","9acae1c1":"learn.fit_one_cycle(2,max_lr = slice(1e-3\/3,1e-3))","6e6f48a4":"learn.save('stage-2')","e6ccbb9d":"preds,y = learn.TTA()\nacc = accuracy(preds, y)\nprint('The validation accuracy is {} %.'.format(acc * 100))\npred_score = auc_score(preds,y).item()\nprint('The validation AUC is {}.'.format(pred_score))","1df40b65":"newTfms = get_transforms(do_flip = True,flip_vert = True,max_zoom = 1.5)\nnewSz = 96\nnewData = ImageDataBunch.from_csv(path, folder = 'train', csv_labels = \"train_labels.csv\",\n                               test = 'test',suffix=\".tif\", size = newSz, ds_tfms = newTfms)\nnewData.path = pathlib.Path('.')\nnewData.normalize(imagenet_stats)\nlearn.data = newData","aa3ff027":"learn.freeze()\nlearn.lr_find()\nlearn.recorder.plot()","a5d15542":"learn.fit_one_cycle(4,1e-4)","5e94000d":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot()","cedeb90a":"learn.fit_one_cycle(2,max_lr = slice(1e-5\/3,1e-5))","9313013d":"learn.save('stage-3')","cd86c781":"preds,y = learn.TTA()\nacc = accuracy(preds, y)\nprint('The validation accuracy is {} %.'.format(acc * 100))\npred_score = auc_score(preds,y).item()\nprint('The validation AUC is {}.'.format(pred_score))","93acd919":"interp = ClassificationInterpretation.from_learner(learn)","9de43a15":"interp.plot_top_losses(6)","d51ad645":"interp.plot_confusion_matrix()","7737a533":"preds,y = learn.TTA()\nacc = accuracy(preds, y)\nprint('The validation accuracy is {} %.'.format(acc * 100))\npred_score = auc_score(preds,y).item()\nprint('The validation AUC is {}.'.format(pred_score))","da238439":"def generateSubmission(learner):\n    submissions = pd.read_csv('..\/input\/sample_submission.csv')\n    id_list = list(submissions.id)\n    preds,y = learner.TTA(ds_type=DatasetType.Test)\n    pred_list = list(preds[:,1])\n    pred_dict = dict((key, value.item()) for (key, value) in zip(learner.data.test_ds.items,pred_list))\n    pred_ordered = [pred_dict[Path('..\/input\/test\/' + id + '.tif')] for id in id_list]\n    submissions = pd.DataFrame({'id':id_list,'label':pred_ordered})\n    submissions.to_csv(\"submission_transferLearning_{}.csv\".format(pred_score),index = False)","6e0565d4":"generateSubmission(learn)","da65d941":"Exploratory data analysis should be the first step of every data science task. Due to the lack of domain knowledge, I will only check for the number of classes and the number of items per class. Imbalanced datasets may require resampling of the data to ensure proper training.","c9122a23":"# Future work\n\n1. [DONE] Generate sample submission to ensure functional code. (0.6007)\n2. [DONE] Implement skeleton model for baseline (0.9106).\n3. [DONE] Prepare AUC metric (0.9233).\n4. [DONE] Deploy reasoned data augmentation (0.9241).\n5. [DONE] Deploy test-time augmentation (0.9364).\n6. Test other architectures (which are found [here in the Pytorch docs](https:\/\/pytorch.org\/docs\/stable\/torchvision\/models.html))\n    1. [DONE] ResNet-34 (0.9362)\n    2. [DONE] DenseNet-169 (0.9370)\n7. [DONE] Implement weight decay. (0.9368)\n8. [DONE] Retrain model with higher resolution images. (0.9573)","6b4fafdd":"We will now train the model on the same dataset, except we are using images of higher resolution. Intuitively, the 'concepts' learnt by the neural network will continue to be applied in training with the new set of images.","e502a471":"# Introduction\n`fastai` is a free deep learning API built on [PyTorch V1](https:\/\/pytorch.org\/). The [fast.ai team](https:\/\/www.fast.ai\/2018\/10\/02\/fastai-ai\/) incorporates their reseach breakthroughs into the software, enabling users to achieve more accurate results faster and with fewer lines of code.\n\nThis kernel illustrates the simplicity of deploying the `fastai.vision` package for image classification tasks. I am in no way a domain expert in this topic, in fact having no domain knowledge at all before this competition! I will heavily rely on published kernels (which are all referenced under [Acknowledgements](#Acknowledgements)) in guidance for setting hyperparameters in this task.\n\nI will be deploying standard techniques taught in the fast.ai course to see how well these techniques can perform without needing expert knowledge. The techniques are:\n1. Learning rate finder\n2. 1-cycle learning\n3. Differential learning rates for model finetuning\n4. Data augmentation\n5. Test time augmentation\n6. Transfer learning via low-resolution images\n\nThis kernel had the previous name of **Minimal fast.ai kit for image classification**, which is a slight misnomer now, considering the detailed techniques being deployed in this image classification task.","492d1cfd":"`fit_one_cycle`is a method implemented by the library and proposed in [this paper](https:\/\/arxiv.org\/pdf\/1803.09820.pdf) to produce more accurate results and faster convergence. [This post](https:\/\/sgugger.github.io\/the-1cycle-policy.html) is a great explanation of why `fit_one_cycle`works over the standard `fit`.","42b841c5":"# Data loading and preparation","02e62f96":"# Model training","094a6544":"# Model interpretation","11b9ec98":"At this stage, we would like to check the effectiveness of the `learn` model against our validation set (which is automatically generated by the `ImageDataBunch` object). We will use the following methods to evaluate the effectiveness.\n1. Confusion matrix.\n2. Accuracy.\n3. ROC-AUC, as dictated in the competition evaluation.","fb43c0d3":"# Contents\n1. [Skeleton code](#Skeleton-code)\n* [Import packages](#Import-packages)\n* [Exploratory data analysis](#Exploratory-data-analysis)\n* [Data loading and preparation](#Data-loading-and-preparation)\n* [Model creation](#Model-creation)\n* [Model training](#Model-training).\n* [Model interpretation](#Model-interpretation)\n* [Transfer learning](#Transfer-learning)\n* [Generating submission](#Generating-submission)\n* [Future work](#Future-work)\n* [Acknowledgements](#Acknowledgements)","9fb3319a":"# Transfer learning","d89761cc":"# Import packages","cf306829":"Following from the initial idea of showing the simplicity of using the `fastai` library, below is a code snippet containing 27 lines of code using default settings for a base model generation. Of these 27 lines, 10 lines are used to generate the submission file for the required format.\n```\nfrom fastai import *\nfrom fastai.vision import *\n\ndata = ImageDataBunch.from_csv(path, folder = 'train', csv_labels = \"train_labels.csv\",\n                               test = 'test',suffix=\".tif\", size = 36, ds_tfms = get_transforms())\ndata.path = pathlib.Path('.')\ndata.normalize(imagenet_stats)\n\nlearn = create_cnn(data,resnet50,pretrained = True,metrics = accuracy)\nlearn.fit_one_cycle(5)\n\nlearn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot()\nlearn.fit_one_cycle(3,max_lr = slice(1e-6,3e-4))\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_top_losses(9)\ninterp.plot_confusion_matrix()\npreds,y = learn.TTA()\nacc = accuracy(preds, y)\nprint('The validation accuracy is {} %.'.format(acc * 100))\n\ndef generateSubmission(learner):\n    submissions = pd.read_csv('..\/input\/sample_submission.csv')\n    id_list = list(submissions.id)\n    preds,y = learner.TTA(ds_type=DatasetType.Test)\n    pred_list = list(preds[:,1])\n    pred_dict = dict((key, value.item()) for (key, value) in zip(learner.data.test_ds.items,pred_list))\n    pred_ordered = [pred_dict[Path('..\/input\/test\/' + id + '.tif')] for id in id_list]\n    submissions = pd.DataFrame({'id':id_list,'label':pred_ordered})\n    submissions.to_csv(\"submission_{}.csv\".format(pred_score),index = False)\n \n generateSubmission(learn)\n```","7be7dcd6":"# Model creation","e47df94b":"# Skeleton code","4da30b33":"# Acknowledgements\n\n* [qitvision](https:\/\/www.kaggle.com\/qitvision\/) for his [extremely well-explained kernel](https:\/\/www.kaggle.com\/qitvision\/a-complete-ml-pipeline-fast-ai) on the same competition and for answering my questions on data loading.\n* [Gunther](https:\/\/www.kaggle.com\/guntherthepenguin) for providing the implementation of the AUC metric in [his kernel in the same competition.](https:\/\/www.kaggle.com\/guntherthepenguin\/fastai-v1-densenet169)\n* The [fast.ai team](https:\/\/www.fast.ai\/about\/) for creating the [library](https:\/\/docs.fast.ai\/index.html) and [the v3 course](https:\/\/course.fast.ai\/index.html) for teaching deep learning in a very accessible manner.","c133ffa3":"The most important hyperparameter in training neural networks in general is the **learning rate**. Unfortunately as of now, there is no way of finding a good learning rate without trial-and-error. \n\nThe library has made it convenient to test different learning rates. We find a good learning rate using the method `lr_find`, then plotting the graph of learning rates against losses. As a rule of thumb, the learning rate is chosen from a part of the graph where it is **steepest** and **most consistent**.","f157ac2a":"# Exploratory data analysis","ee09bffd":"## 64x64 images","66acc663":"# Generating submission","63b6e308":"Submissions into the competition are [evaluated on the area under the ROC curve](https:\/\/www.kaggle.com\/c\/histopathologic-cancer-detection#evaluation) between the predicted probability and the observed target. Since we have a limited number of submissions per day, implementing a metric for the ROC AUC (which is non-standard in the fast.ai v1 library) allows us to run as many experiments we want.\n\nAt this point, I am not sure if changing the metric changes the loss function in the `Learner` to optimize the metric. I will be doing more reading up in that area. If anyone knows the answer to this, leave something in the comments below!","09c1d3a0":"## 96x96 images","b76d7391":"As the order of the images loaded into `data` is not necessarily the same order as that in the required submission, we will need to rearrange the predictions on the test set.\n\nThe desired order for submission is that of `sample_submission.csv`. while the order of the test set loaded into `data` can be accessed by calling `learn.data.test_ds.items`. We will first create a dictionary assigning each image in the test to its prediction by the model, then call the keys in the order in  `sample_submission.csv`.","f9b61a25":"A [simple rule of thumb suggested](https:\/\/github.com\/hiromis\/notes\/blob\/master\/Lesson1.md) by Jeremy when selecting differential learning rates is to:\n1. Set the first part of the slice (corresponding to the earlier layers of the model) to a learning rate much smaller than where the loss starts increasing.\n2. Set the final slice to a learning rate 0.1x of that used when training the frozen model."}}