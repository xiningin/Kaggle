{"cell_type":{"3c85629a":"code","af0d3455":"code","f7f9eac3":"code","fc9d77d6":"code","2cf077c9":"code","3feab7df":"code","3f821543":"code","64adde92":"code","48beec33":"code","e5f46d8f":"code","c006fc92":"code","abd4a7ca":"code","fb60df99":"code","e7bf883b":"code","d20f72e2":"code","d4fae220":"code","95e3734d":"markdown","f481be0c":"markdown","b9423fcd":"markdown","64c2b89e":"markdown","2d1855b6":"markdown","54dad0b3":"markdown","0c69707f":"markdown","ee751e0d":"markdown","8951b8cb":"markdown","3e9baa03":"markdown","79c81bc5":"markdown","d00139db":"markdown","4c479883":"markdown","90b4abf3":"markdown","f2b85625":"markdown","c2f7d02c":"markdown","a7e0a9d3":"markdown","2ccd8888":"markdown","b54a4e17":"markdown","455bd804":"markdown","3e9b21b0":"markdown","e450293e":"markdown","735fd12c":"markdown"},"source":{"3c85629a":"import sys\nsys.path.append('..\/input\/timm-pytorch-image-models\/pytorch-image-models-master')","af0d3455":"!pip install --upgrade -q wandb\n!pip install -q pytorch-lightning","f7f9eac3":"# Hide Warning\nimport warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=UserWarning)\n\n# Python Libraries\nfrom tqdm import tqdm\nfrom collections import defaultdict\nimport pandas as pd\nimport numpy as np\nimport os\nimport random\nimport glob\nimport math\n\n\n# Visualizations\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nimport plotly.express as px\n%matplotlib inline\nsns.set(style=\"whitegrid\")\n\n# Utilities and Metrics\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\n# Pytorch \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.optim.optimizer import Optimizer, required \n\n\n# Pytorch Lightning\nimport pytorch_lightning as pl\nfrom pytorch_lightning import seed_everything\nfrom pytorch_lightning import Callback\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom pytorch_lightning.loggers import WandbLogger\n\n# Pytorch Image Models\nimport timm\n\n# Image Augmentation Library\nimport albumentations\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations.core.transforms_interface import DualTransform\nfrom albumentations.augmentations import functional as AF\n\n# Weights and Biases Tool\nimport wandb\nwandb.login()","fc9d77d6":"params = {\n    'seed': 42,\n    'model': 'efficientnet_b0',\n    'size' : 480,\n    'inp_channels': 1,\n    'lr': 1e-3,\n    'weight_decay': 1e-6,\n    'batch_size': 32,\n    'num_workers' : 8,\n    'epochs': 5,\n    'out_features': 1,\n    'name': 'CosineAnnealingLR',\n    'T_max': 10,\n    'min_lr': 1e-6,\n    'nfolds': 5,\n    'precision': 16\n}","2cf077c9":"def seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(params['seed'])","3feab7df":"train_dir = ('..\/input\/seti-breakthrough-listen\/train')\ntest_dir = ('..\/input\/seti-breakthrough-listen\/test')\ntrain_df = pd.read_csv('..\/input\/seti-breakthrough-listen\/train_labels.csv')\ntest_df = pd.read_csv('..\/input\/seti-breakthrough-listen\/sample_submission.csv')","3f821543":"def return_filpath(name, folder=train_dir):\n    path = os.path.join(folder, name[0], f'{name}.npy')\n    return path","64adde92":"train_df['image_path'] = train_df['id'].apply(lambda x: return_filpath(x))\ntest_df['image_path'] = test_df['id'].apply(lambda x: return_filpath(x, folder=test_dir))\ntrain_df.head()","48beec33":"class GridMask(DualTransform):\n\n    def __init__(self, num_grid=3, fill_value=0, rotate=0, mode=0, always_apply=False, p=0.5):\n        super(GridMask, self).__init__(always_apply, p)\n        if isinstance(num_grid, int):\n            num_grid = (num_grid, num_grid)\n        if isinstance(rotate, int):\n            rotate = (-rotate, rotate)\n        self.num_grid = num_grid\n        self.fill_value = fill_value\n        self.rotate = rotate\n        self.mode = mode\n        self.masks = None\n        self.rand_h_max = []\n        self.rand_w_max = []\n\n    def init_masks(self, height, width):\n        if self.masks is None:\n            self.masks = []\n            n_masks = self.num_grid[1] - self.num_grid[0] + 1\n            for n, n_g in enumerate(range(self.num_grid[0], self.num_grid[1] + 1, 1)):\n                grid_h = height \/ n_g\n                grid_w = width \/ n_g\n                this_mask = np.ones((int((n_g + 1) * grid_h), int((n_g + 1) * grid_w))).astype(np.uint8)\n                for i in range(n_g + 1):\n                    for j in range(n_g + 1):\n                        this_mask[\n                             int(i * grid_h) : int(i * grid_h + grid_h \/ 2),\n                             int(j * grid_w) : int(j * grid_w + grid_w \/ 2)\n                        ] = self.fill_value\n                        if self.mode == 2:\n                            this_mask[\n                                 int(i * grid_h + grid_h \/ 2) : int(i * grid_h + grid_h),\n                                 int(j * grid_w + grid_w \/ 2) : int(j * grid_w + grid_w)\n                            ] = self.fill_value\n                \n                if self.mode == 1:\n                    this_mask = 1 - this_mask\n\n                self.masks.append(this_mask)\n                self.rand_h_max.append(grid_h)\n                self.rand_w_max.append(grid_w)\n\n    def apply(self, image, mask, rand_h, rand_w, angle, **params):\n        h, w = image.shape[:2]\n        mask = AF.rotate(mask, angle) if self.rotate[1] > 0 else mask\n        mask = mask[:,:,np.newaxis] if image.ndim == 3 else mask\n        image *= mask[rand_h:rand_h+h, rand_w:rand_w+w].astype(image.dtype)\n        return image\n\n    def get_params_dependent_on_targets(self, params):\n        img = params['image']\n        height, width = img.shape[:2]\n        self.init_masks(height, width)\n\n        mid = np.random.randint(len(self.masks))\n        mask = self.masks[mid]\n        rand_h = np.random.randint(self.rand_h_max[mid])\n        rand_w = np.random.randint(self.rand_w_max[mid])\n        angle = np.random.randint(self.rotate[0], self.rotate[1]) if self.rotate[1] > 0 else 0\n\n        return {'mask': mask, 'rand_h': rand_h, 'rand_w': rand_w, 'angle': angle}\n\n    @property\n    def targets_as_params(self):\n        return ['image']\n\n    def get_transform_init_args_names(self):\n        return ('num_grid', 'fill_value', 'rotate', 'mode')","e5f46d8f":"def get_train_transforms():\n    return albumentations.Compose(\n        [\n            albumentations.Resize(params['size'],params['size']),\n            albumentations.HorizontalFlip(p=0.5),\n            albumentations.VerticalFlip(p=0.5),\n            albumentations.Rotate(limit=180, p=0.7),\n            albumentations.RandomBrightness(limit=0.6, p=0.5),\n            albumentations.Cutout(\n                num_holes=10, max_h_size=12, max_w_size=12,\n                fill_value=0, always_apply=False, p=0.5\n            ),\n            albumentations.ShiftScaleRotate(\n                shift_limit=0.25, scale_limit=0.1, rotate_limit=0\n            ),\n            albumentations.OneOf([\n                GridMask(num_grid=3, mode=0, rotate=15),\n                GridMask(num_grid=3, mode=2, rotate=15),\n                                ], p=0.7),\n            ToTensorV2(p=1.0),\n        ]\n    )\n\ndef get_valid_transforms():\n    return albumentations.Compose(\n        [\n            albumentations.Resize(params['size'],params['size']),\n            ToTensorV2(p=1.0)\n        ]\n    )\n\ndef get_test_transforms():\n        return albumentations.Compose(\n            [\n                albumentations.Resize(params['size'],params['size']),\n                ToTensorV2(p=1.0)\n            ]\n        )","c006fc92":"class SETIDataset(Dataset):\n    def __init__(self, images_filepaths, targets, transform=None):\n        self.images_filepaths = images_filepaths\n        self.targets = targets\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images_filepaths)\n\n    def __getitem__(self, idx):\n        image_filepath = self.images_filepaths[idx]\n        image = np.load(image_filepath).astype(np.float32)\n        image = np.vstack(image).transpose((1, 0))\n            \n        if self.transform is not None:\n            image = self.transform(image=image)[\"image\"]\n        else:\n            image = image[np.newaxis,:,:]\n            image = torch.from_numpy(image).float()\n        \n        label = torch.tensor(self.targets[idx]).float()\n        return image, label","abd4a7ca":"class ImagePredictionLogger(Callback):\n    def __init__(self, val_samples, num_samples=32):\n        super().__init__()\n        self.num_samples = num_samples\n        self.val_imgs, self.val_labels = val_samples\n        \n    def on_validation_epoch_end(self, trainer, pl_module):\n        val_imgs = self.val_imgs.to(device=pl_module.device)\n        val_labels = self.val_labels.to(device=pl_module.device)\n        logits = pl_module(val_imgs)\n        preds = torch.argmax(logits, -1)\n        trainer.logger.experiment.log({\n            \"examples\":[wandb.Image(x, caption=f\"Pred:{pred}, Target:{y}\") \n                           for x, pred, y in zip(val_imgs[:self.num_samples], \n                                                 preds[:self.num_samples], \n                                                 val_labels[:self.num_samples])]\n            }, commit=False)","fb60df99":"# https:\/\/github.com\/lessw2020\/Ranger-Deep-Learning-Optimizer\n\nclass Ranger(Optimizer):\n\n    def __init__(self, params, lr=1e-3,                       # lr\n                 alpha=0.5, k=6, N_sma_threshhold=5,           # Ranger options\n                 betas=(.95, 0.999), eps=1e-5, weight_decay=0,  # Adam options\n                 # Gradient centralization on or off, applied to conv layers only or conv + fc layers\n                 use_gc=True, gc_conv_only=False\n                 ):\n\n        # parameter checks\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f'Invalid slow update rate: {alpha}')\n        if not 1 <= k:\n            raise ValueError(f'Invalid lookahead steps: {k}')\n        if not lr > 0:\n            raise ValueError(f'Invalid Learning Rate: {lr}')\n        if not eps > 0:\n            raise ValueError(f'Invalid eps: {eps}')\n\n\n        # prep defaults and init torch.optim base\n        defaults = dict(lr=lr, alpha=alpha, k=k, step_counter=0, betas=betas,\n                        N_sma_threshhold=N_sma_threshhold, eps=eps, weight_decay=weight_decay)\n        super().__init__(params, defaults)\n\n        # adjustable threshold\n        self.N_sma_threshhold = N_sma_threshhold\n\n        # look ahead params\n\n        self.alpha = alpha\n        self.k = k\n\n        # radam buffer for state\n        self.radam_buffer = [[None, None, None] for ind in range(10)]\n\n        # gc on or off\n        self.use_gc = use_gc\n\n        # level of gradient centralization\n        self.gc_gradient_threshold = 3 if gc_conv_only else 1\n\n        print(\n            f\"Ranger optimizer loaded. \\nGradient Centralization usage = {self.use_gc}\")\n        if (self.use_gc and self.gc_gradient_threshold == 1):\n            print(f\"GC applied to both conv and fc layers\")\n        elif (self.use_gc and self.gc_gradient_threshold == 3):\n            print(f\"GC applied to conv layers only\")\n\n    def __setstate__(self, state):\n        print(\"set state called\")\n        super(Ranger, self).__setstate__(state)\n\n    def step(self, closure=None):\n        loss = None\n\n        # Evaluate averages and grad, update param tensors\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        'Ranger optimizer does not support sparse gradients')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]  # get state dict for this param\n\n                if len(state) == 0:  \n\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n\n                    # look ahead weight storage now in state dict\n                    state['slow_buffer'] = torch.empty_like(p.data)\n                    state['slow_buffer'].copy_(p.data)\n\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(\n                        p_data_fp32)\n\n                # begin computations\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                # GC operation for Conv layers and FC layers\n                if grad.dim() > self.gc_gradient_threshold:\n                    grad.add_(-grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True))\n\n                state['step'] += 1\n\n                # compute variance mov avg\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                # compute mean moving avg\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                buffered = self.radam_buffer[int(state['step'] % 10)]\n\n                if state['step'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2 ** state['step']\n                    N_sma_max = 2 \/ (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * \\\n                        state['step'] * beta2_t \/ (1 - beta2_t)\n                    buffered[1] = N_sma\n                    if N_sma > self.N_sma_threshhold:\n                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) \/ (N_sma_max - 4) * (\n                            N_sma - 2) \/ N_sma * N_sma_max \/ (N_sma_max - 2)) \/ (1 - beta1 ** state['step'])\n                    else:\n                        step_size = 1.0 \/ (1 - beta1 ** state['step'])\n                    buffered[2] = step_size\n\n                if group['weight_decay'] != 0:\n                    p_data_fp32.add_(-group['weight_decay']\n                                     * group['lr'], p_data_fp32)\n\n                # apply lr\n                if N_sma > self.N_sma_threshhold:\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    p_data_fp32.addcdiv_(-step_size *\n                                         group['lr'], exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n                # integrated look ahead...\n                # we do it at the param level instead of group level\n                if state['step'] % group['k'] == 0:\n                    # get access to slow param tensor\n                    slow_p = state['slow_buffer']\n                    # (fast weights - slow weights) * alpha\n                    slow_p.add_(self.alpha, p.data - slow_p)\n                    # copy interpolated weights to RAdam param tensor\n                    p.data.copy_(slow_p)\n\n        return loss","e7bf883b":"class DataModule(pl.LightningDataModule):\n\n    def __init__(self, train_data, valid_data, test_data):\n        super().__init__()\n        self.train_data = train_data\n        self.valid_data = valid_data\n        self.test_data = test_data\n        \n    def setup(self, stage=None):\n        self.train_dataset = SETIDataset(\n        images_filepaths=self.train_data['image_path'].values,\n        targets=self.train_data['target'].values,\n        transform=get_train_transforms()\n            )\n        \n        self.val_dataset = SETIDataset(\n        images_filepaths=self.valid_data['image_path'].values,\n        targets=self.valid_data['target'].values,\n        transform=get_valid_transforms()\n            )\n\n        self.test_dataset = SETIDataset(\n        images_filepaths = self.test_data['image_path'].values,\n        targets = self.test_data['target'].values,\n        transform = get_test_transforms()\n            )\n\n    def train_dataloader(self):\n        return DataLoader(\n        self.train_dataset,\n        batch_size=params['batch_size'],\n        shuffle=True,\n        num_workers=params['num_workers'],\n        pin_memory=True\n            )\n\n    def val_dataloader(self):\n        return DataLoader(\n        self.val_dataset,\n        batch_size=params['batch_size'],\n        shuffle=False,\n        num_workers=params['num_workers'],\n        pin_memory=True\n            )\n\n    def test_dataloader(self):\n        return DataLoader(\n        self.test_dataset, batch_size=params['batch_size'],\n        shuffle=False, num_workers=params['num_workers'],\n        pin_memory=True\n            )","d20f72e2":"class Trainer(pl.LightningModule):\n\n    def __init__(self, model_name=params['model'],out_features=params['out_features'],\n                 inp_channels=params['inp_channels'],pretrained=True):\n        super().__init__()\n        \n        self.model = timm.create_model(model_name, pretrained=pretrained,\n                                       in_chans=inp_channels)\n        n_features = self.model.classifier.in_features\n        self.model.classifier = nn.Linear(n_features, out_features, bias=True) \n        \n        self.criterion = nn.BCEWithLogitsLoss()\n                \n    def forward(self, x):\n        output = self.model(x)\n        return output\n\n\n    def training_step(self, batch, batch_idx):\n\n        x, y = batch\n        output = self(x)\n        labels = y.unsqueeze(1)\n        loss = self.criterion(output, labels)\n        \n        try:\n            auc=roc_auc_score(labels.detach().cpu(), output.sigmoid().detach().cpu()) \n\n            self.log(\"auc\", auc, on_step= True, prog_bar=True, logger=True)\n            self.log(\"Train Loss\", loss, on_step= True,prog_bar=True, logger=True)\n        except:\n            pass\n\n        return {\"loss\": loss, \"predictions\": output, \"labels\": labels}\n\n    def training_epoch_end(self, outputs):\n\n        preds = []\n        labels = []\n        \n        for output in outputs:\n            \n            preds += output['predictions']\n            labels += output['labels']\n\n        labels = torch.stack(labels)\n        preds = torch.stack(preds)\n\n        train_auc=roc_auc_score(labels.detach().cpu(), preds.sigmoid().detach().cpu())\n        self.log(\"mean_train_auc\", train_auc, prog_bar=True, logger=True)\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        output = self(x)\n        labels = y.unsqueeze(1)\n        loss = self.criterion(output, labels)\n        self.log('val_loss', loss, on_step= True, prog_bar=True, logger=True)\n        return {\"predictions\": output, \"labels\": labels}\n      \n\n    def validation_epoch_end(self, outputs):\n\n        preds = []\n        labels = []\n        \n        for output in outputs:\n            preds += output['predictions']\n            labels += output['labels']\n\n        labels = torch.stack(labels)\n        preds = torch.stack(preds)\n\n        val_auc=roc_auc_score(labels.detach().cpu(), preds.sigmoid().detach().cpu())\n        self.log(\"val_auc\", val_auc, prog_bar=True, logger=True)\n        \n\n    def test_step(self, batch, batch_idx):\n        x = batch        \n        output = self(x).sigmoid()\n        return output   \n\n    def configure_optimizers(self):\n\n        optimizer = Ranger(self.parameters(), lr = params['lr'])\n\n        scheduler = CosineAnnealingLR(optimizer,\n                              T_max=params['T_max'],\n                              eta_min=params['min_lr'],\n                              last_epoch=-1)\n\n        return dict(\n          optimizer=optimizer,\n          lr_scheduler=scheduler\n        )","d4fae220":"skf = StratifiedKFold(n_splits=params['nfolds'], shuffle=True, random_state=params['seed'])\ntrain_df[\"fold\"] = -1\n\nmodel = Trainer()\n\nfor fold, (_, val_idx) in enumerate(skf.split(train_df[\"id\"], train_df[\"target\"])):\n    train_df.loc[val_idx, \"fold\"] = fold\n    \n    if fold != 0:\n        continue\n    \n    wandb_logger = WandbLogger(project='SETI-Lightning', \n                           config={'competetion': 'SETI', '_wandb_kernel':'tang'},\n                           group='EffNet', \n                           job_type='train',\n                           name = f'Fold{fold}')\n    \n    print(f\"{'='*38} Fold: {fold} {'='*38}\")\n    \n    \n    data_module = DataModule(\n      train_df[train_df['fold']!=fold],\n      train_df[train_df['fold']==fold], \n      train_df[train_df['fold']==fold], \n    )\n    \n    data_module.setup()\n    val_samples = next(iter(data_module.val_dataloader()))\n    \n    early_stopping_callback = EarlyStopping(monitor='val_auc',mode=\"max\", patience=2)\n    \n    checkpoint_callback = ModelCheckpoint(\n      dirpath=\"checkpoints\",\n      filename=\"best-checkpoint-fold{fold}-val_auc{val_auc:.3f}\",\n      save_top_k=params['epochs'],\n      verbose=True,\n      monitor=\"val_auc\",\n      mode=\"max\"\n    )\n    \n    trainer = pl.Trainer(\n      gpus= 1,\n      checkpoint_callback=checkpoint_callback,\n      callbacks=[early_stopping_callback,\n                       ImagePredictionLogger(val_samples)],\n      max_epochs=params['epochs'],\n      precision=params['precision'],\n      progress_bar_refresh_rate=1,\n      logger=wandb_logger\n    )\n    \n    trainer.fit(model, data_module)","95e3734d":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Import Libraries<\/p>","f481be0c":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Single Fold Metrics<\/p><br>\n\n<center><img src=\"https:\/\/i.imgur.com\/Wgxk9rf.png\" width=\"1500\" alt=\"metrics\" \/><\/center>","b9423fcd":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Define Configurations\/Parameters<\/p>","64c2b89e":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Pytorch Lightning Module<\/p>\n<center><img src=\"data:image\/png;base64,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\" width=\"400\" alt=\"Weights & Biases\" \/><\/center><br>\n\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\"> A LightningModule organizes your PyTorch code into 5 sections<br>1. Computations (init).<br>2. Train loop (training_step)<br>3. Validation loop (validation_step)<br>4. Test loop (test_step)<br>5. Optimizers (configure_optimizers)<br><br>\nA LightningModule is a torch.nn.Module but with added functionality. ","2d1855b6":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Define Seed for Reproducibility<\/p>","54dad0b3":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Validation Loss per Epoch<\/p><br>\n\n<center><img src=\"https:\/\/i.imgur.com\/RsfAt2g.png\" width=\"1500\" alt=\"metrics\" \/><\/center>","0c69707f":"<p p style = \"font-family: garamond; font-size:40px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">What are we discussing today? <\/p>\n <p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#006699; border-radius: 10px 10px; text-align:center\">Efficient Nets<br>\n Ranger optimizer <br>\n GridMask Augmentation <br>\n Pytorch Lightning <br>\n Stratified K-Fold with PL \u26a1\ufe0f <br>\n Weights and Biases for Experiment Tracking","ee751e0d":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Single Fold Results<\/p>\n\n<p style = \"font-family: garamond; font-size: 25px; font-style: normal; border-radius: 10px 10px; text-align:center\">We are able to achieve a Validation AUC score of 97.50! in just 5 Epochs<br><br> Weights & Biases provides us with such easy to use interface and tools to keep a track of our Evaluation metrics like training and validation loss and Roc along with other resources like Gpu usage<br><br> Let's take a look at some of our training and GPU Utilization graphs along with Predictions<\/p>","8951b8cb":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Load Train and Test<\/p>","3e9baa03":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\"><a href = 'https:\/\/wandb.ai\/tanishqgautam\/SETI-Lightning?workspace=user-tanishqgautam'>Check out the Weights and Biases Dashboard here $\\rightarrow$ <\/a><\/p>","79c81bc5":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Image Augmentation<\/p>","d00139db":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Pytorch Lightning Trainer<\/p>\n\n# <center><img src=\"data:image\/png;base64,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\" width=\"400\" alt=\"Weights & Biases\" \/><\/center>\n\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\"> Once you\u2019ve organized your PyTorch code into a LightningModule, the Trainer automates everything else.<br><br>\ud83d\udccd This abstraction achieves the following:<br><br>1. You maintain control over all aspects via PyTorch code without an added abstraction.<br>2. The trainer uses best practices embedded by contributors and users from top AI labs such as Facebook AI Research, NYU, MIT, Stanford, etc\u2026<br>3. The trainer allows overriding any key part that you don\u2019t want automated.<br><br>\n\ud83d\udccd Under the hood<br>\nUnder the hood, the Lightning Trainer handles the training loop details for you, some examples include:<br>1. Automatically enabling\/disabling grads<br>2. Running the training, validation and test dataloaders<br>3. Calling the Callbacks at the appropriate times<br>4. Putting batches and computations on the correct devices<br><br>\n\ud83d\udccd The code below applies Stratified K-Fold with PL \u26a1\ufe0f\n\n","4c479883":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Custom Callback for Viewing Predictions in W&B <br> Code Credit <a href = 'https:\/\/www.kaggle.com\/ayuraj\/use-pytorch-lightning-with-weights-and-biases'>Ayush Thakur <\/a><\/p>\n","90b4abf3":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">GPU Utilization<\/p><br>\n\n<center><img src=\"https:\/\/i.imgur.com\/nHhgP1e.png\" width=\"1500\" alt=\"metrics\" \/><\/center>","f2b85625":"# <center><img src=\"https:\/\/i.imgur.com\/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" \/><\/center><br>\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">Wandb is a developer tool for companies turn deep learning research projects into deployed software by helping teams track their models, visualize model performance and easily automate training and improving models.\nWe will use their tools to log hyperparameters and output metrics from your runs, then visualize and compare results and quickly share findings with your colleagues.<br><br>We'll be using this to train our K Fold Cross Validation and gain better insights about our training. <br><br><\/p>\n\n![img](https:\/\/i.imgur.com\/BGgfZj3.png)","c2f7d02c":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">GridMask Data Augmentation<\/p>\n\n<center><img src=\"https:\/\/pic3.zhimg.com\/v2-2fbfeb0fd08f902de0fabe11ca5edac9_1440w.jpg?source=172ae18b\" width=\"1000\" alt=\"GridMAsk\" \/><\/center><br>\n\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">GridMask Augmentation utilizes information removal to achieve state-of-the-art results in a variety of computer vision tasks. There are limitations to existing information dropping algorithms whereas GridMask is simple and yet very effective.<br><br> It is based on the deletion of regions of the input image. Extensive experiments show that this method outperforms the latest AutoAugment,which is way more computationally expensive due to the use of reinforcement learning to find the best policies. On the\nImageNet dataset for recognition, COCO2017 object detection, and on Cityscapes dataset for semantic segmentation, GridMask improves performance over baselines. <br><br>The extensive experiments manifest the effectiveness and generality of the new method.<br><br> Avoiding excessive deletion and reservation\nof continuous regions is the core requirement for information dropping methods. <br><br>The reason is twofold intuitively.<br>1. On the one hand, excessively deleting one or a few regions may lead to complete object removal and context information be removed as well. Thus remaining information is not enough to be classified and the image is more like noisy data.<br>2. On the other hand, excessive preserving regions could make some objects untouched. They are trivial images that may lead to a reduction of the networks robustness. <br><br>\nThus designing a simple method that reduces the\nchance of causing these two problems becomes essential.","a7e0a9d3":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Image Predictions<\/p><br>\n\n<center><img src=\"https:\/\/i.imgur.com\/FYMjXSS.png\" width=\"1500\" alt=\"metrics\" \/><\/center>","2ccd8888":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Ranger Optimizer with Gradient Centralization<\/p>\n\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\"> The Ranger optimizer combines two very new Optimizers into a single optimizer:- <br><br>1. RAdam - which is a variant of the Adam stochastic optimizer that introduces a term to rectify the variance of the adaptive learning rate. <br>2.Lookahead - which iteratively updates two sets of weights. Intuitively, the algorithm chooses a search direction by looking ahead at the sequence of fast weights generated by another optimizer.<br><br>RAdam provides the best base for an optimizer to build on as it leverages a dynamic rectifier to adjust the adaptive momentum of Adam based on the variance. LookAhead provides a breakthrough in robust and stable exploration during the entirety of training. <br><br>\n\ud83d\udccd What is Gradient Centralization? <br><br> GC is a new optimization technique, namely gradient centralization which operates directly on gradients by centralizing the gradient vectors to have zero mean. GC can be viewed as a projected gradient descent method with a constrained loss function. We show that GC can regularize both the weight space and output feature space so that it can boost the generalization performance of DNNs.<br><br> Moreover, GC improves the Lipschitzness of the loss function and its gradient so that the training process becomes more efficient and stable. GC is very simple to implement and can be easily embedded into existing gradient based DNN optimizers with only one line of code. It can also be directly used to fine-tune the pre-trained DNNs.","b54a4e17":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">TIMM Pytorch Models<\/p>\n\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">PyTorch Image Models (timm) is a collection of image models, layers, utilities, optimizers, schedulers, data-loaders \/ augmentations, and reference training \/ validation scripts that aim to pull together a wide variety of SOTA models with ability to reproduce ImageNet training results.<br><br>\nUsing timm we will create the Efficient Net model for our problem statement. We will be using the Efficient Net B0 variant. <\/p>","455bd804":"<p p style = \"font-family: garamond; font-size:35px; font-style: normal;background-color: #f6f5f5; color :#ff0066; border-radius: 10px 10px; text-align:center\">Upvote the kernel if you find it insightful!<\/p>","3e9b21b0":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spacing: 1px; background-color: #f6f5f5; color :#6666ff; border-radius: 200px 200px; text-align:center\">Efficient Networks: SOTA Image Classification<\/h1>\n\n<center><img src=\"https:\/\/1.bp.blogspot.com\/-oNSfIOzO8ko\/XO3BtHnUx0I\/AAAAAAAAEKk\/rJ2tHovGkzsyZnCbwVad-Q3ZBnwQmCFsgCEwYBhgL\/s640\/image3.png\" width=\"500\" alt=\"Weights & Biases\" \/><\/center><br>\n\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">Convolutional Neural Networks are commonly developed at a fixed resource budget,and then scaled up for better accuracy if more resources are available. Efficient Nets propose a new scaling method that uniformly scales all dimensions of depth\/width\/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models,called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets.<\/p>","e450293e":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Custom Dataset<\/p>","735fd12c":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Pytorch Lightning DataModule<\/p>\n\n# <center><img src=\"data:image\/png;base64,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\" width=\"400\" alt=\"Weights & Biases\" \/><\/center>\n\n\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\"> A datamodule is a shareable, reusable class that encapsulates all the steps needed to process data. A datamodule encapsulates the five steps involved in data processing in PyTorch:<br>1. Download \/ tokenize \/ process.<br>2. Clean and (maybe) save to disk.<br>3. Load inside Dataset.<br>4. Apply transforms (rotate, tokenize, etc\u2026).<br>5. Wrap inside a DataLoader.<br><br>\n\ud83d\udccd Why do I need a DataModule?<br>In normal PyTorch code, the data cleaning\/preparation is usually scattered across many files. This makes sharing and reusing the exact splits and transforms across projects impossible.<br>Datamodules are for you if you ever asked the questions:<br>1. what splits did you use?<br>2. what transforms did you use?<br>3. what normalization did you use?<br>4. how did you prepare\/tokenize the data?<br><br> To summarize: A DataModule is simply a collection of a train_dataloader, val_dataloader(s), test_dataloader(s) along with the matching transforms and data processing\/downloads steps required."}}